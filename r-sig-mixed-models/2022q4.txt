From v@np@r|don @end|ng |rom w|@c@edu  Tue Oct  4 18:08:19 2022
From: v@np@r|don @end|ng |rom w|@c@edu (Jeroen van Paridon)
Date: Tue, 4 Oct 2022 16:08:19 +0000
Subject: [R-sig-ME] New: lmerMultiMember adds support for multiple
 membership models to lme4
Message-ID: <DM6PR06MB4171281AA01E0CEF9E29D501B85A9@DM6PR06MB4171.namprd06.prod.outlook.com>

Hi all,

This is just to inform anyone who might be interested that Phillip Alday and I have developed a small R package that adds support for models with multiple membership random effects to lme4.

Multiple membership models have been discussed on this mailing list several times over the years, but they are not supported in lme4. A few years ago, Ben Bolker very helpfully provided an example of how to ?hack? lme4 to insert a multiple membership random effect; we have now reworked that example into a more user-friendly R package that also includes helper functions for setting up multiple membership indicator matrices, visualizing those matrices, and for creating nested multiple membership groupings.

Specifying multiple membership groupings is not difficult, but we recommend people check out the README and vignettes at https://github.com/jvparidon/lmerMultiMember before diving in. Other than that, lmerMultiMember simply extends the functionality of lme4, so model specification, model fitting, and model summaries all work just like in lme4.

This package is not (yet) on CRAN, so it's easiest to install it straight from Github using devtools::install_github("jvparidon/lmerMultiMember")

Feel free to reach out with any questions, notes, or bugs that need fixing!


JP van Paridon, PhD
Postdoctoral Researcher
Department of Psychology
University of Wisconsin-Madison

	[[alternative HTML version deleted]]


From ci@ire@dv04 m@iii@g oii gm@ii@com  Thu Oct  6 11:32:09 2022
From: ci@ire@dv04 m@iii@g oii gm@ii@com (ci@ire@dv04 m@iii@g oii gm@ii@com)
Date: Thu, 6 Oct 2022 11:32:09 +0200
Subject: [R-sig-ME] I would need some advice about repeated anova 2 ways and
 how to code it within lmer function
Message-ID: <027c01d8d966$82009f00$8601dd00$@gmail.com>

Dear all

I would appreciate some advice about how to analyze my data.

These data come from an experiment during which the hearing of 18 ears was
measured over time and according to different frequencies.

Each ear was measured at T1, T21, and T 28, and at each of its times, each
ear was measured at frequencies 4000Hz, 8000Hz, 16000Hz, 25000Hz, and
32000Hz.

I especially want to know if there is a time effect. The frequency effect
does not particularly interest me.

I was thinking of using a 2-factor anova for repeated data (with 2 factors
within: time (=Point)  and frequency), but I’m note sure.

I work with R and I thought to use the lmer function with this code:

mod.lmer ← lmer(hearing ~ Point * Hz Frequency +(1|id/Point) +(1|id/Hz
Frequency),
contrasts=list(Point=contr.sum, Frequency Hz=contr.sum),
data=mydata)

id is the ear identification factor

I’m not sure about the coding of random effects (1|id/Point)
+(1|id/Frequency Hz), but they give me the same results as the aov.ez
function of the afex package :

mod.ez ← aov_ez(id=“id”,
dv=“Threshold dB”,
data=mydata,
within = c(“Point”, “Frequency Hz”))

What do you think ?

Does this analysis seem correct to you? If not, what can you suggest me?

Thanks in advance for any help you can give me.

All the best

Claire Della Vedova


	[[alternative HTML version deleted]]


From @h|r@qotj @end|ng |rom gm@||@com  Fri Oct  7 03:02:45 2022
From: @h|r@qotj @end|ng |rom gm@||@com (Shira Mitchell)
Date: Thu, 6 Oct 2022 21:02:45 -0400
Subject: [R-sig-ME] Bradley Terry GLMM in R ?
Message-ID: <CADeg_O3L_6Cmn_Wv0tExeCLccFJS1p5fottqprdh3+spDCrXxA@mail.gmail.com>

We want to fit Bradley-Terry-style GLMM models in R. We looked into:

https://cran.r-project.org/web/packages/BradleyTerry2/vignettes/BradleyTerry.pdf
and
http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html

We have voter-specific variables x that influence which political message
(i vs j) wins for them:

logit[pr(i beats j | person with covariate x)] = lambda_i - lambda_j +
(beta_i - beta_j) x

We then model parameters as random effects:
lambda_i ~ N(0, sigma_lambda)
beta_i ~ N(0, sigma_beta)

Is there a way to do this in R ? We do this in TensorFlow in Python by
directly specifying design matrices with the 0,-1,1 or 0,-x,x entries.
However, I do not see how to do this in R using lme4, BradleyTerry2, mgcv,
etc.

Thanks so much,
Shira

	[[alternative HTML version deleted]]


From j@un20 @end|ng |rom @|b@ny@edu  Fri Oct  7 15:43:37 2022
From: j@un20 @end|ng |rom @|b@ny@edu (Sun, John)
Date: Fri, 7 Oct 2022 13:43:37 +0000
Subject: [R-sig-ME] How to find ACF, PACF,
 Sample Variance-Covariance Matrix of Random-Effects?
Message-ID: <BL0PR04MB456345BD58AAB7BE5823797DB95F9@BL0PR04MB4563.namprd04.prod.outlook.com>

Dear All,

I am writing to ask how to compute the (partial) autocorrelation of random-effects versus lag, and sample-variance covariance matrix of the random-effects given some data without specification of some model or structure on the random-effects covariance matrix. 

Best regards,
John 


From th|erry@onke||nx @end|ng |rom |nbo@be  Fri Oct  7 16:18:48 2022
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Fri, 7 Oct 2022 16:18:48 +0200
Subject: [R-sig-ME] Bradley Terry GLMM in R ?
In-Reply-To: <CADeg_O3L_6Cmn_Wv0tExeCLccFJS1p5fottqprdh3+spDCrXxA@mail.gmail.com>
References: <CADeg_O3L_6Cmn_Wv0tExeCLccFJS1p5fottqprdh3+spDCrXxA@mail.gmail.com>
Message-ID: <CAJuCY5ys+di7UNWKLSV8gw6qVC5yjWXVa4J7Bboe=cHAwoYDGw@mail.gmail.com>

Hi Shira,

I fit such models with the INLA package (https://www.r-inla.org/). The
trick is to define two random effects but force their parameter estimates
to be identical.

The code would contain something like f(home, model = "iid")) + f(away,
copy = "home"). Meaning home ~ N(0, sigma_beta_i) and home[i] = away[i]

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op vr 7 okt. 2022 om 15:00 schreef Shira Mitchell <shiraqotj at gmail.com>:

> We want to fit Bradley-Terry-style GLMM models in R. We looked into:
>
>
> https://cran.r-project.org/web/packages/BradleyTerry2/vignettes/BradleyTerry.pdf
> and
> http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html
>
> We have voter-specific variables x that influence which political message
> (i vs j) wins for them:
>
> logit[pr(i beats j | person with covariate x)] = lambda_i - lambda_j +
> (beta_i - beta_j) x
>
> We then model parameters as random effects:
> lambda_i ~ N(0, sigma_lambda)
> beta_i ~ N(0, sigma_beta)
>
> Is there a way to do this in R ? We do this in TensorFlow in Python by
> directly specifying design matrices with the 0,-1,1 or 0,-x,x entries.
> However, I do not see how to do this in R using lme4, BradleyTerry2, mgcv,
> etc.
>
> Thanks so much,
> Shira
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From iiori@@@wickeim@ier m@iii@g oii u@i-tuebi@ge@@de  Fri Oct  7 16:35:25 2022
From: iiori@@@wickeim@ier m@iii@g oii u@i-tuebi@ge@@de (iiori@@@wickeim@ier m@iii@g oii u@i-tuebi@ge@@de)
Date: Fri, 7 Oct 2022 16:35:25 +0200 (CEST)
Subject: [R-sig-ME] Bradley Terry GLMM in R ?
In-Reply-To: <DB9PR10MB5260F395C505DEEA73AD15EECB5F9@DB9PR10MB5260.EURPRD10.PROD.OUTLOOK.COM>
References: <CADeg_O3L_6Cmn_Wv0tExeCLccFJS1p5fottqprdh3+spDCrXxA@mail.gmail.com>
 <DB9PR10MB5260F395C505DEEA73AD15EECB5F9@DB9PR10MB5260.EURPRD10.PROD.OUTLOOK.COM>
Message-ID: <986a1fde-9b1-71a3-1815-9c801496887a@uni-tuebingen.de>

> We want to fit Bradley-Terry-style GLMM models in R. We looked into:
>
> https://cran.r-project.org/web/packages/BradleyTerry2/vignettes/BradleyTerry.pdf
> and
> http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html
>
> We have voter-specific variables x that influence which political message
> (i vs j) wins for them:
>
> logit[pr(i beats j | person with covariate x)] = lambda_i - lambda_j +
> (beta_i - beta_j) x
>
> We then model parameters as random effects:
> lambda_i ~ N(0, sigma_lambda)
> beta_i ~ N(0, sigma_beta)
>
> Is there a way to do this in R ? We do this in TensorFlow in Python by
> directly specifying design matrices with the 0,-1,1 or 0,-x,x entries.
> However, I do not see how to do this in R using lme4, BradleyTerry2, mgcv,
> etc.
>
> Thanks so much,
> Shira

Not directly what you asked for, but since you have voter covariates,
you could have a look at a tree-based Bradley-Terry model as implemented
in psychotree::bttree().

Best, Florian

---
Florian Wickelmaier
Department of Psychology
University of Tuebingen
http://www.mathpsy.uni-tuebingen.de/wickelmaier/


From @h|r@qotj @end|ng |rom gm@||@com  Fri Oct  7 23:35:58 2022
From: @h|r@qotj @end|ng |rom gm@||@com (Shira Mitchell)
Date: Fri, 7 Oct 2022 17:35:58 -0400
Subject: [R-sig-ME] Bradley Terry GLMM in R ?
In-Reply-To: <CAJuCY5ys+di7UNWKLSV8gw6qVC5yjWXVa4J7Bboe=cHAwoYDGw@mail.gmail.com>
References: <CADeg_O3L_6Cmn_Wv0tExeCLccFJS1p5fottqprdh3+spDCrXxA@mail.gmail.com>
 <CAJuCY5ys+di7UNWKLSV8gw6qVC5yjWXVa4J7Bboe=cHAwoYDGw@mail.gmail.com>
Message-ID: <CADeg_O0d+tuG0DnnnFw9o9VSh1dcMUbOitcT_rLbRcdbhNpzYw@mail.gmail.com>

Thanks so much, Thierry ! This is great.

This works except that I cannot subtract because:
f(home, model = "iid")) - f(away, copy = "home")

just drops the second term. Apologies that I'm not super familiar with INLA
syntax yet.



On Fri, Oct 7, 2022 at 10:19 AM Thierry Onkelinx <thierry.onkelinx at inbo.be>
wrote:

> Hi Shira,
>
> I fit such models with the INLA package (https://www.r-inla.org/). The
> trick is to define two random effects but force their parameter estimates
> to be identical.
>
> The code would contain something like f(home, model = "iid")) + f(away,
> copy = "home"). Meaning home ~ N(0, sigma_beta_i) and home[i] = away[i]
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op vr 7 okt. 2022 om 15:00 schreef Shira Mitchell <shiraqotj at gmail.com>:
>
>> We want to fit Bradley-Terry-style GLMM models in R. We looked into:
>>
>>
>> https://cran.r-project.org/web/packages/BradleyTerry2/vignettes/BradleyTerry.pdf
>> and
>> http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html
>>
>> We have voter-specific variables x that influence which political message
>> (i vs j) wins for them:
>>
>> logit[pr(i beats j | person with covariate x)] = lambda_i - lambda_j +
>> (beta_i - beta_j) x
>>
>> We then model parameters as random effects:
>> lambda_i ~ N(0, sigma_lambda)
>> beta_i ~ N(0, sigma_beta)
>>
>> Is there a way to do this in R ? We do this in TensorFlow in Python by
>> directly specifying design matrices with the 0,-1,1 or 0,-x,x entries.
>> However, I do not see how to do this in R using lme4, BradleyTerry2, mgcv,
>> etc.
>>
>> Thanks so much,
>> Shira
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From @@b|omberg1 @end|ng |rom uq@edu@@u  Sat Oct  8 00:34:24 2022
From: @@b|omberg1 @end|ng |rom uq@edu@@u (Simone Blomberg)
Date: Fri, 7 Oct 2022 22:34:24 +0000
Subject: [R-sig-ME] Bradley Terry GLMM in R ?
In-Reply-To: <986a1fde-9b1-71a3-1815-9c801496887a@uni-tuebingen.de>
References: <CADeg_O3L_6Cmn_Wv0tExeCLccFJS1p5fottqprdh3+spDCrXxA@mail.gmail.com>
 <DB9PR10MB5260F395C505DEEA73AD15EECB5F9@DB9PR10MB5260.EURPRD10.PROD.OUTLOOK.COM>
 <986a1fde-9b1-71a3-1815-9c801496887a@uni-tuebingen.de>
Message-ID: <2535F808-C0D1-4EAE-8EF8-941D98DEFDB4@uq.edu.au>

See this great document page 74-77. Agresti?s book is amazing and this document is essentially for doing everything in it in R

https://artowen.su.domains/courses/306a/SplusDiscrete.PDF

Cheers,
Simone.


Simone Blomberg, BSc (Hons), PhD, MAppStat

The University of Queensland
St. Lucia Queensland 4072
Australia

email: S.Blomberg1_at_uq.edu.au<http://s.blomberg1_at_uq.edu.au/>

UQ ALLY Supporting the diversity of sexuality and gender at UQ.

 Policies:

1.  I will NOT analyse your data for you.

2.  Your deadline is your problem.

If you can?t stand algebra, stay out of evolutionary biology. - J. M. Smith.

On 8 Oct 2022, at 12:59 am, florian.wickelmaier at uni-tuebingen.de wrote:

?
We want to fit Bradley-Terry-style GLMM models in R. We looked into:

https://cran.r-project.org/web/packages/BradleyTerry2/vignettes/BradleyTerry.pdf
and
http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html

We have voter-specific variables x that influence which political message
(i vs j) wins for them:

logit[pr(i beats j | person with covariate x)] = lambda_i - lambda_j +
(beta_i - beta_j) x

We then model parameters as random effects:
lambda_i ~ N(0, sigma_lambda)
beta_i ~ N(0, sigma_beta)

Is there a way to do this in R ? We do this in TensorFlow in Python by
directly specifying design matrices with the 0,-1,1 or 0,-x,x entries.
However, I do not see how to do this in R using lme4, BradleyTerry2, mgcv,
etc.

Thanks so much,
Shira

Not directly what you asked for, but since you have voter covariates,
you could have a look at a tree-based Bradley-Terry model as implemented
in psychotree::bttree().

Best, Florian

---
Florian Wickelmaier
Department of Psychology
University of Tuebingen
http://www.mathpsy.uni-tuebingen.de/wickelmaier/

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From @@b|omberg1 @end|ng |rom uq@edu@@u  Sat Oct  8 01:49:47 2022
From: @@b|omberg1 @end|ng |rom uq@edu@@u (Simone Blomberg)
Date: Fri, 7 Oct 2022 23:49:47 +0000
Subject: [R-sig-ME] Bradley Terry GLMM in R ?
In-Reply-To: <2535F808-C0D1-4EAE-8EF8-941D98DEFDB4@uq.edu.au>
References: <CADeg_O3L_6Cmn_Wv0tExeCLccFJS1p5fottqprdh3+spDCrXxA@mail.gmail.com>
 <DB9PR10MB5260F395C505DEEA73AD15EECB5F9@DB9PR10MB5260.EURPRD10.PROD.OUTLOOK.COM>
 <986a1fde-9b1-71a3-1815-9c801496887a@uni-tuebingen.de>
 <2535F808-C0D1-4EAE-8EF8-941D98DEFDB4@uq.edu.au>
Message-ID: <CAAD9D2E-D15D-44A9-AF6D-03C6087DA4E6@uq.edu.au>

More recent edition here

https://www.stat.purdue.edu/~zhanghao/MAS/handout/R%20Manual%20to%20Agresti%E2%80%99s%20Categorical%20Data%20Analysis.pdf


Simone Blomberg, BSc (Hons), PhD, MAppStat

The University of Queensland
St. Lucia Queensland 4072
Australia

email: S.Blomberg1_at_uq.edu.au<http://s.blomberg1_at_uq.edu.au/>

UQ ALLY Supporting the diversity of sexuality and gender at UQ.

 Policies:

1.  I will NOT analyse your data for you.

2.  Your deadline is your problem.

If you can?t stand algebra, stay out of evolutionary biology. - J. M. Smith.

On 8 Oct 2022, at 8:35 am, Simone Blomberg <s.blomberg1 at uq.edu.au> wrote:

? See this great document page 74-77. Agresti?s book is amazing and this document is essentially for doing everything in it in R

https://artowen.su.domains/courses/306a/SplusDiscrete.PDF

Cheers,
Simone.


Simone Blomberg, BSc (Hons), PhD, MAppStat

The University of Queensland
St. Lucia Queensland 4072
Australia

email: S.Blomberg1_at_uq.edu.au<http://s.blomberg1_at_uq.edu.au/>

UQ ALLY Supporting the diversity of sexuality and gender at UQ.

 Policies:

1.  I will NOT analyse your data for you.

2.  Your deadline is your problem.

If you can?t stand algebra, stay out of evolutionary biology. - J. M. Smith.

On 8 Oct 2022, at 12:59 am, florian.wickelmaier at uni-tuebingen.de wrote:

?
We want to fit Bradley-Terry-style GLMM models in R. We looked into:

https://cran.r-project.org/web/packages/BradleyTerry2/vignettes/BradleyTerry.pdf
and
http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html

We have voter-specific variables x that influence which political message
(i vs j) wins for them:

logit[pr(i beats j | person with covariate x)] = lambda_i - lambda_j +
(beta_i - beta_j) x

We then model parameters as random effects:
lambda_i ~ N(0, sigma_lambda)
beta_i ~ N(0, sigma_beta)

Is there a way to do this in R ? We do this in TensorFlow in Python by
directly specifying design matrices with the 0,-1,1 or 0,-x,x entries.
However, I do not see how to do this in R using lme4, BradleyTerry2, mgcv,
etc.

Thanks so much,
Shira

Not directly what you asked for, but since you have voter covariates,
you could have a look at a tree-based Bradley-Terry model as implemented
in psychotree::bttree().

Best, Florian

---
Florian Wickelmaier
Department of Psychology
University of Tuebingen
http://www.mathpsy.uni-tuebingen.de/wickelmaier/

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Mon Oct 10 10:24:29 2022
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Mon, 10 Oct 2022 10:24:29 +0200
Subject: [R-sig-ME] Bradley Terry GLMM in R ?
In-Reply-To: <CADeg_O0d+tuG0DnnnFw9o9VSh1dcMUbOitcT_rLbRcdbhNpzYw@mail.gmail.com>
References: <CADeg_O3L_6Cmn_Wv0tExeCLccFJS1p5fottqprdh3+spDCrXxA@mail.gmail.com>
 <CAJuCY5ys+di7UNWKLSV8gw6qVC5yjWXVa4J7Bboe=cHAwoYDGw@mail.gmail.com>
 <CADeg_O0d+tuG0DnnnFw9o9VSh1dcMUbOitcT_rLbRcdbhNpzYw@mail.gmail.com>
Message-ID: <CAJuCY5ysJUQQtYUCLF76iD5y9TmSiOD5OAfBt039RjVaD_pzwQ@mail.gmail.com>

Dear Shira,

- in a formula object means remove that object from the formula. Use a
weight of -1 instead.

f(home, model = "iid")) + f(away, w = -1, copy = "home")

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op vr 7 okt. 2022 om 23:36 schreef Shira Mitchell <shiraqotj at gmail.com>:

> Thanks so much, Thierry ! This is great.
>
> This works except that I cannot subtract because:
> f(home, model = "iid")) - f(away, copy = "home")
>
> just drops the second term. Apologies that I'm not super familiar with
> INLA syntax yet.
>
>
>
> On Fri, Oct 7, 2022 at 10:19 AM Thierry Onkelinx <thierry.onkelinx at inbo.be>
> wrote:
>
>> Hi Shira,
>>
>> I fit such models with the INLA package (https://www.r-inla.org/). The
>> trick is to define two random effects but force their parameter estimates
>> to be identical.
>>
>> The code would contain something like f(home, model = "iid")) + f(away,
>> copy = "home"). Meaning home ~ N(0, sigma_beta_i) and home[i] = away[i]
>>
>> Best regards,
>>
>> ir. Thierry Onkelinx
>> Statisticus / Statistician
>>
>> Vlaamse Overheid / Government of Flanders
>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>> AND FOREST
>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>> thierry.onkelinx at inbo.be
>> Havenlaan 88 bus 73, 1000 Brussel
>> www.inbo.be
>>
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of data.
>> ~ John Tukey
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>>
>> <https://www.inbo.be>
>>
>>
>> Op vr 7 okt. 2022 om 15:00 schreef Shira Mitchell <shiraqotj at gmail.com>:
>>
>>> We want to fit Bradley-Terry-style GLMM models in R. We looked into:
>>>
>>>
>>> https://cran.r-project.org/web/packages/BradleyTerry2/vignettes/BradleyTerry.pdf
>>> and
>>> http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html
>>>
>>> We have voter-specific variables x that influence which political message
>>> (i vs j) wins for them:
>>>
>>> logit[pr(i beats j | person with covariate x)] = lambda_i - lambda_j +
>>> (beta_i - beta_j) x
>>>
>>> We then model parameters as random effects:
>>> lambda_i ~ N(0, sigma_lambda)
>>> beta_i ~ N(0, sigma_beta)
>>>
>>> Is there a way to do this in R ? We do this in TensorFlow in Python by
>>> directly specifying design matrices with the 0,-1,1 or 0,-x,x entries.
>>> However, I do not see how to do this in R using lme4, BradleyTerry2,
>>> mgcv,
>>> etc.
>>>
>>> Thanks so much,
>>> Shira
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>

	[[alternative HTML version deleted]]


From j@un20 @end|ng |rom @|b@ny@edu  Mon Oct 10 23:50:21 2022
From: j@un20 @end|ng |rom @|b@ny@edu (Sun, John)
Date: Mon, 10 Oct 2022 21:50:21 +0000
Subject: [R-sig-ME] glmmTMB's variance-covariance matrix is of the vector of
 observed intercept per subject minus the quantity of the fixed intercept
 plus the random-intercept term?
Message-ID: <BL0PR04MB45636488E68DB73ACD1AB622B9209@BL0PR04MB4563.namprd04.prod.outlook.com>

Dear All,

I am writing to ask which random-vector glmmTMB estimates the variance-covariance matrix. Is the random-vector that glmmTMB the G-matrix Charles Roy Henderson describes in https://en.wikipedia.org/wiki/Mixed_model?

Suppose we have a model with random-intercepts and fixed-effects. Is the random-vector that glmmTMB estimates the variance-covariance of equal to the actual random-intercept of the individual minus the quantity of the random-effect plus the fixed-intercept effect?

The random-intercept for some individual equals the intercept's random-effect plus the fixed-effect of the intercept plus some random-error scalar drawn from a normal distribution. 
I refer to the equation in the second level of the two-level model. 

The level one equation equals alpha_i+beta*Xij + epsilon_i. 
The level two: alpha_i=delta+gamma_i + h_i.

"I" is subject, j is timepoint. Delta is fixed-intercept term, gamma_i is individual's deviation from the fixed-intercept. h_i is some deviation of the individual from the fixed-effect drawn from some normal distribution. 

Best regards,
John 


From bbo|ker @end|ng |rom gm@||@com  Tue Oct 11 00:09:58 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 10 Oct 2022 18:09:58 -0400
Subject: [R-sig-ME] 
 glmmTMB's variance-covariance matrix is of the vector of
 observed intercept per subject minus the quantity of the fixed intercept
 plus the random-intercept term?
In-Reply-To: <BL0PR04MB45636488E68DB73ACD1AB622B9209@BL0PR04MB4563.namprd04.prod.outlook.com>
References: <BL0PR04MB45636488E68DB73ACD1AB622B9209@BL0PR04MB4563.namprd04.prod.outlook.com>
Message-ID: <a9e69d23-4864-a5be-a62c-fe8d03f74a2d@gmail.com>

   glmmTMB doesn't explicitly use Henderson's equations, but the 
variance-covariance matrix estimated by glmmTMB (and all of the other R 
mixed-model packages I can think of) is *almost* the G-matrix as 
described there.  Not quite, though, because the covariance matrices 
given by R are the covariance matrices for the random effects b within a 
**single level of the grouping variable**; as written in the Wikipedia 
page, the full covariance matrix of u would include all the blocks. For 
example, if we had (for example) 10 groups in a single-level, 
intercept-only model, u would be a 10-vector and G would be a 10x10 
diagonal matrix with the among-group variance on the diagonal. VarCorr() 
would return a list of length 1 (since there's only a single grouping 
variable in the model) containing a 1x1 covariance matrix containing the 
among-group variance.

    For a random-slopes model, u would be length 20 and G would be a 
block-diagonal matrix with 10 2x2 blocks, each of which contained the 
intercept variance, slope variance, and intercept-slope covariance. 
VarCorr() would return a list of length 1 containing a 2x2 covariance 
matrix.

   The lme4 vignette (vignette("lmer", package = "lme4")) does a pretty 
good job of describing these structures.  The algorithm used is 
completely different from glmmTMB's, and some of the internal structures 
are different (e.g. glmmTMB doesn't explicitly set up a Lambda factor, 
or internally scale the random-effects covariance matrix relative to the 
residual variance), but it may be helpful.

On 2022-10-10 5:50 p.m., Sun, John wrote:
> Dear All,
> 
> I am writing to ask which random-vector glmmTMB estimates the variance-covariance matrix. Is the random-vector that glmmTMB the G-matrix Charles Roy Henderson describes in https://en.wikipedia.org/wiki/Mixed_model?
> 
> Suppose we have a model with random-intercepts and fixed-effects. Is the random-vector that glmmTMB estimates the variance-covariance of equal to the actual random-intercept of the individual minus the quantity of the random-effect plus the fixed-intercept effect?

    I'm not 100% sure what you mean, but the random effects are indeed 
defined as *deviations* of the group-level expectation from the 
population-level (fixed-effect) predictions. So I think I would say 
"yes" to this question (although I will emphasize that there is no 
subtraction going on anywhere -- the group-level effects are indeed the 
sum of the fixed effects and the group-level deviations).


> 
> The random-intercept for some individual equals the intercept's random-effect plus the fixed-effect of the intercept plus some random-error scalar drawn from a normal distribution.
> I refer to the equation in the second level of the two-level model.
> 
> The level one equation equals alpha_i+beta*Xij + epsilon_i.
> The level two: alpha_i=delta+gamma_i + h_i.
> 
> "I" is subject, j is timepoint. Delta is fixed-intercept term, gamma_i is individual's deviation from the fixed-intercept. h_i is some deviation of the individual from the fixed-effect drawn from some normal distribution.
> 
> Best regards,
> John
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics
 > E-mail is sent at my convenience; I don't expect replies outside of 
working hours.


From bbo|ker @end|ng |rom gm@||@com  Tue Oct 11 02:27:56 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 10 Oct 2022 20:27:56 -0400
Subject: [R-sig-ME] How to find ACF, PACF,
 Sample Variance-Covariance Matrix of Random-Effects?
In-Reply-To: <BL0PR04MB456345BD58AAB7BE5823797DB95F9@BL0PR04MB4563.namprd04.prod.outlook.com>
References: <BL0PR04MB456345BD58AAB7BE5823797DB95F9@BL0PR04MB4563.namprd04.prod.outlook.com>
Message-ID: <7a63c679-1cde-7914-39f4-a7a7342ff325@gmail.com>

   Can you give an example please?

   The covariance matrix is typically (for nlme and other packages that 
follow the same conventions) extracted with VarCorr().  As described in 
my other e-mail to the list, this is not *quite* the same as the full 
covariance matrix of the random-effects vector, but rather the 
covariance matrix for a single grouping variable/block.  VarCorr() 
returns a list.  So e.g. if

library(lme4)
fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)


VarCorr(fm1)$Subject gives a 2x2 covariance matrix

cov2cor() will convert this to a correlation matrix.

Or, for lmer fits at least, the correlation is already present as an 
attribute:

attr(VarCorr(fm1)$Subject, "correlation")

On 2022-10-07 9:43 a.m., Sun, John wrote:
> Dear All,
> 
> I am writing to ask how to compute the (partial) autocorrelation of random-effects versus lag, and sample-variance covariance matrix of the random-effects given some data without specification of some model or structure on the random-effects covariance matrix.
> 
> Best regards,
> John
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics
 > E-mail is sent at my convenience; I don't expect replies outside of 
working hours.


From ci@ire@dv04 m@iii@g oii gm@ii@com  Tue Oct 11 08:27:29 2022
From: ci@ire@dv04 m@iii@g oii gm@ii@com (ci@ire@dv04 m@iii@g oii gm@ii@com)
Date: Tue, 11 Oct 2022 08:27:29 +0200
Subject: [R-sig-ME] I would need some advice about repeated anova 2 ways and
 how to code it within lmer function
Message-ID: <00b101d8dd3a$8a21c420$9e654c60$@gmail.com>

Dear all

I would appreciate some advice about how to analyze my data.

These data come from an experiment during which the hearing of 18 ears was
measured over time and according to different frequencies.

Each ear was measured at T1, T21, and T 28, and at each of its times, each
ear was measured at frequencies 4000Hz, 8000Hz, 16000Hz, 25000Hz, and
32000Hz.

I especially want to know if there is a time effect. The frequency effect
does not particularly interest me.

I was thinking of using a 2-factor anova for repeated data (with 2 factors
within: time (=Point)  and frequency), but I’m note sure.

I work with R and I thought to use the lmer function with this code:

mod.lmer ← lmer(hearing ~ Point * Hz Frequency +(1|id/Point) +(1|id/Hz
Frequency),
contrasts=list(Point=contr.sum, Frequency Hz=contr.sum),
data=mydata)

id is the ear identification factor

I’m not sure about the coding of random effects (1|id/Point)
+(1|id/Frequency Hz), but they give me the same results as the aov.ez
function of the afex package :

mod.ez ← aov_ez(id=“id”,
dv=“Threshold dB”,
data=mydata,
within = c(“Point”, “Frequency Hz”))

What do you think ?

Does this analysis seem correct to you? If not, what can you suggest me?

Thanks in advance for any help you can give me.

All the best

Claire Della Vedova

 


	[[alternative HTML version deleted]]


From j@un20 @end|ng |rom @|b@ny@edu  Tue Oct 11 14:52:36 2022
From: j@un20 @end|ng |rom @|b@ny@edu (Sun, John)
Date: Tue, 11 Oct 2022 12:52:36 +0000
Subject: [R-sig-ME] Autocorrelated and Heteroscedastic Random-Slopes
Message-ID: <BL0PR04MB45636369350B57DDBD32282EB9239@BL0PR04MB4563.namprd04.prod.outlook.com>

Dear All,

I am writing to ask how to obtain autocorrelated and heteroscedastic random slopes. I know the single pipe | makes the random-slopes cross-correlated with the random-intercept. 
Are the random-slopes already autocorrelated and heteroscedastic by default in common uses of lme4-nlme?
I also want to try making the random-slopes have a Toeplitz, Unstructured or AR-1 structure with itself.

I am not talking about the correlation between the random-slopes and intercepts.
I am talking about the covariance between random-slopes with random-slopes with different periods-itself. 

Best regards,
Kpjm 


From j@un20 @end|ng |rom @|b@ny@edu  Tue Oct 11 22:17:30 2022
From: j@un20 @end|ng |rom @|b@ny@edu (Sun, John)
Date: Tue, 11 Oct 2022 20:17:30 +0000
Subject: [R-sig-ME] Why lme4 fails on schizo dataset, but glmmTMB succeeds?
Message-ID: <BL0PR04MB456333D98741D7E69140B63DB9239@BL0PR04MB4563.namprd04.prod.outlook.com>

Dear All,

I am writing to ask why lme4 fails to work with random-slopes+intercepts on schizo dataset, but glmmTMB works. 
    
>## data from http://www.uic.edu/~hedeker/SCHIZX1.DAT.txt
    ## inspiration from http://www.uic.edu/~hedeker/long.html
    #may have to manually download from website.

    fn <- "schizx1.dat"
    if (!file.exists(fn)) {
        download.file("http://www.uic.edu/~hedeker/SCHIZX1.DAT.txt",dest=fn)
    }
    ## almost works: get extra ^Z, need to strip it ...
    dd0 <- read.table("schizx1.dat")
    names(dd0) <- c("id","imps79","imps79b","imps79o","int","tx",
                   "week","sweek","txswk")
    apply(dd0,2,function(x) sum(x==-9,na.rm=TRUE))
    
    ## dangerous to use na.strings==c("-9","NA") in general?
    ## what if there are legitimate -9 values in *some* columns?
    na.vals <- function(x,crit) {
        x[crit] <- NA
        return(x)
    }
    library(plyr)  ## for mutate()
    library(MASS)  ## for glmmPQL
    library(lme4)
    dd <- mutate(dd0,
                    imps79=na.vals(imps79,imps79<0),
                    imps79b=na.vals(imps79b,imps79b<0),
                    imps79b=factor(imps79b,labels=c("le mild","ge moderate")),
                    tx=factor(tx,labels=c("placebo","drug")))
    
   
    with(dd,table(tx,sweek))
    with(na.omit(dd),table(tx,sweek))
    
    ## reduce this to sequences: most data are observed at only 4 time periods
    dd$rweek <- round(dd$sweek^2)
    dd2 <- na.omit(subset(dd,rweek %in% c(0,1,3,6)))
    nrow(na.omit(dd))-nrow(dd2)  ## lose only 34 cases (out of 1600)
    dd3 <- ddply(dd2,"id",summarise,
                 tx=tx[1],
                 trans=paste(imps79b,collapse=""))
    ttab <- with(dd3,table(trans,tx))
    ttab[rowSums(ttab)>10,]
    
    m0 <- glm(imps79b~tx*sweek,dd,family=binomial)
    
    m3 <- glmer(imps79b~tx*sweek+(1|id),dd,family=binomial)
    ## extremely slow: lots of 'step-halving' action
    m4 <- glmer(imps79b~tx*sweek+(1+sweek|id),dd,family=binomial,
                verbose=100)
    ## eventually fails to converge with maxgrad=180.4 (!!)

    library(glmmTMB)
    
    m5<-glmmTMB(imps79b~tx*sweek+us(1+sweek|id),dd,family=binomial(link="logit"))
    
    summary(m5)
     Family: binomial  ( logit )
    Formula:          
    imps79b ~ tx * sweek + us(1 + sweek | id)
    Data: dd
    
         AIC      BIC   logLik deviance df.resid 
      1173.7   1211.4   -579.9   1159.7     1596 
    
    Random effects:
    
    Conditional model:
     Groups Name        Variance Std.Dev. Corr  
     id     (Intercept) 2184.9   46.74          
            sweek        365.9   19.13    -0.81 
    Number of obs: 1603, groups:  id, 437
    
    Conditional model:
                 Estimate Std. Error z value
    (Intercept)   45.2732     7.6836   5.892
    txdrug         0.8314     6.9096   0.120
    sweek        -14.6736     3.1210  -4.702
    txdrug:sweek  -7.3960     2.9027  -2.548
                 Pr(>|z|)    
    (Intercept)  3.81e-09 ***
    txdrug         0.9042    
    sweek        2.58e-06 ***
    txdrug:sweek   0.0108 *  
    ---
    Signif. codes:  
    0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

https://github.com/lme4/lme4/blob/master/misc/issues/schizo.R

Best regards,
John


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Oct 12 01:31:22 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 11 Oct 2022 16:31:22 -0700
Subject: [R-sig-ME] 
 I would need some advice about repeated anova 2 ways and
 how to code it within lmer function
In-Reply-To: <00b101d8dd3a$8a21c420$9e654c60$@gmail.com>
References: <00b101d8dd3a$8a21c420$9e654c60$@gmail.com>
Message-ID: <33E294E9-8838-4FC2-A630-3C516C681599@dcn.davis.ca.us>

I am barely functional with mixed models, but your email is barely legible (you sent HTML formatted email but we received only plain text per the Posting Guide mentioned at the bottom of every message on this list) so I thought you should at least know we don't see whatever you saw when you sent it.

From what I can gather, you claim to be more interested in time effect but you only have three points in time but five in frequency. I think you are going to have a very difficult time extracting statistically meaningful model characterization with respect to time from this data set.

Mixed models generally require significantly more data to draw conclusions than lm/glm models... but they also try to extract more information from the data set. Are you sure an lm or glm model isn't better suited for this?

On October 10, 2022 11:27:29 PM PDT, claire.dv04 at gmail.com wrote:
>Dear all
>
>I would appreciate some advice about how to analyze my data.
>
>These data come from an experiment during which the hearing of 18 ears was
>measured over time and according to different frequencies.
>
>Each ear was measured at T1, T21, and T 28, and at each of its times, each
>ear was measured at frequencies 4000Hz, 8000Hz, 16000Hz, 25000Hz, and
>32000Hz.
>
>I especially want to know if there is a time effect. The frequency effect
>does not particularly interest me.
>
>I was thinking of using a 2-factor anova for repeated data (with 2 factors
>within: time (=Point)  and frequency), but I’m note sure.
>
>I work with R and I thought to use the lmer function with this code:
>
>mod.lmer ← lmer(hearing ~ Point * Hz Frequency +(1|id/Point) +(1|id/Hz
>Frequency),
>contrasts=list(Point=contr.sum, Frequency Hz=contr.sum),
>data=mydata)
>
>id is the ear identification factor
>
>I’m not sure about the coding of random effects (1|id/Point)
>+(1|id/Frequency Hz), but they give me the same results as the aov.ez
>function of the afex package :
>
>mod.ez ← aov_ez(id=“id”,
>dv=“Threshold dB”,
>data=mydata,
>within = c(“Point”, “Frequency Hz”))
>
>What do you think ?
>
>Does this analysis seem correct to you? If not, what can you suggest me?
>
>Thanks in advance for any help you can give me.
>
>All the best
>
>Claire Della Vedova
>
> 
>
>
>	[[alternative HTML version deleted]]
>
>_______________________________________________
>R-sig-mixed-models at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Sent from my phone. Please excuse my brevity.


From me @end|ng |rom ph||||p@|d@y@com  Fri Oct 14 06:23:28 2022
From: me @end|ng |rom ph||||p@|d@y@com (Phillip Alday)
Date: Thu, 13 Oct 2022 23:23:28 -0500
Subject: [R-sig-ME] 
 I would need some advice about repeated anova 2 ways and
 how to code it within lmer function
In-Reply-To: <00b101d8dd3a$8a21c420$9e654c60$@gmail.com>
References: <00b101d8dd3a$8a21c420$9e654c60$@gmail.com>
Message-ID: <ad0ec3c4-fed7-0087-b128-d3bd165e4f79@phillipalday.com>

Hi Claire,

Unfortunately some of your message got garbled in the conversion to
plain text, so it's a little bit hard to read. But if I understand
correctly, you're considering this model:

hearing ~ TimePoint * Frequency + (1|Ear/TimePoint) +(1|Ear/Frequency)

with sum contrasts. (I've changed the variable names to make the
essential parts of the design more explicit.)

There were 3 levels of TimePoint and 5 levels of Frequency and 18
different Ears measured. All Ears were measured at all TimePoints and
Frequencies. So in total you have 3 * 5 * 18 = 270 observations and 18
levels of the grouping variable. Both numbers are important for mixed
models.

Is this correct?

First: I'm glad you specified your contrasts! It's a common mistake to
forget this (see e.g. Laurel Brehm and my paper on this:
https://doi.org/10.1016/j.jml.2022.104334 or postprint
https://osf.io/9648f) and if you want comparability to traditional
repeated measures ANOVA, then sum contrasts are the way to go.

Next up: I don't think your random effects specification is correct. A
good heuristic is that the same variable should never appear in the
fixed effects and on the right-hand side of the | in the random effects.
There are exceptions to this rule, but generally speaking they're more
for advanced applications.

In your particular case, your random effects are equivalent to:

(1|Ear) + (1|Ear:TimePoint) +(1|Ear) + (1|Ear:Frequency)

So right away, we see that there's a repeated term, which I believe lme4
will drop automatically, but will otherwise be shrunk to zero. So in
other words, we have

(1|Ear) + (1|Ear:TimePoint) + (1|Ear:Frequency)

which turns out to be a special case of

(1 + TimePoint + Frequency | Ear)

when the covariance matrix is restricted to be compound symmetric.
Compound symmetry is related to the sphericity assumption in classical
rmANOVA, but it is an additional assumption and restriction. I don't
think it's necessarily misguided in your case though because the
unrestricted random effects structure (1 + TimePoint + Frequency | Ear)
has 28 parameters (1 intercept, 2 slopes for TimePoint, 4 slopes for
Frequency, 21 correlations). But you since the random effects are
ultimately estimating the covariance between groups, you would be trying
to estimate 28 parameters from 18 groups (ears)!

Since you're mostly interested the effect of TimePoint and don't care
about Frequency, I would probably also consider the mdoel

hearing ~ TimePoint * Frequency + (1 + TimePoint | Ear)

This leaves out Frequency from the random effects, which simplifies
things a lot. The random intercept for a particular grouping variable
(i.e. 1|Ear for your stuff) tends have the most influence on the
estimates of the fixed effects (there is a deep relationship to
Simpson's Paradox). The biggest changes from adding random slopes tend
to be in the standard errors (and hence t-statistics) for the fixed
effects of the same variable. So dropping Frequency from the random
effects might lead to anti-conservative standard errors for Frequency
and the Time:Frequency interaction, which can inflate your Type-I error
rate. But if you're not interested in Frequency, I think that's okay
(see also https://doi.org/10.1016/j.jml.2017.01.001). By keeping the
random intercept and random slopes for TimePoint together, we allow for
them to be correlated. In other words, this would allow for the change
over time to be correlated with the starting point for each Ear, which I
suspect is relevant for your research and could help capture things like
ceiling effects.

Typically, I would also do a bit more exploration of the model using
things like rePCA (see https://arxiv.org/abs/1506.04967) and plotting
the data. I encourage you to explore these tools as well.

Finally given that you're apparently looking at change over time, I
might consider using Helmert or sequential difference contrasts for
TimePoint so that you have comparisons of each step of improvement
instead of just comparisons to the baseline.

As Jeff noted, that isn't a huge amount of data, but I suspect it's
enough to fit a model and take a look. Statistical power is also a
function of effect size, so even if you can fit a model, I don't know if
you'll have sufficient power to detect any effect.

In contrast to Jeff, I think that the small number of observed
timepoints and frequencies is actually an advantage here because you're
treating them as discrete, categorical entities. Moreover, I know that
frequency response in mammalian hearing is a non-linear function and I
suspect changes over time is as well, so I think treating these as
categorical is much easier than trying some type of nonlinear
estimation. If you had more timepoints and frequencies, then number of
associated contrasts and thus model complexity would explode.

If you're only interested the question "does hearing change from T1 to
T28?" then I might even exclude the "T21" data to further simplify the
picture.

Hope that helps,
Phillip


On 11/10/22 1:27 am, claire.dv04 at gmail.com wrote:
> Dear all
> 
> I would appreciate some advice about how to analyze my data.
> 
> These data come from an experiment during which the hearing of 18 ears was
> measured over time and according to different frequencies.
> 
> Each ear was measured at T1, T21, and T 28, and at each of its times, each
> ear was measured at frequencies 4000Hz, 8000Hz, 16000Hz, 25000Hz, and
> 32000Hz.
> 
> I especially want to know if there is a time effect. The frequency effect
> does not particularly interest me.
> 
> I was thinking of using a 2-factor anova for repeated data (with 2 factors
> within: time (=Point)  and frequency), but I’m note sure.
> 
> I work with R and I thought to use the lmer function with this code:
> 
> mod.lmer ← lmer(hearing ~ Point * Hz Frequency +(1|id/Point) +(1|id/Hz
> Frequency),
> contrasts=list(Point=contr.sum, Frequency Hz=contr.sum),
> data=mydata)
> 
> id is the ear identification factor
> 
> I’m not sure about the coding of random effects (1|id/Point)
> +(1|id/Frequency Hz), but they give me the same results as the aov.ez
> function of the afex package :
> 
> mod.ez ← aov_ez(id=“id”,
> dv=“Threshold dB”,
> data=mydata,
> within = c(“Point”, “Frequency Hz”))
> 
> What do you think ?
> 
> Does this analysis seem correct to you? If not, what can you suggest me?
> 
> Thanks in advance for any help you can give me.
> 
> All the best
> 
> Claire Della Vedova
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From @oob|n@cho|8827 @end|ng |rom gm@||@com  Fri Oct 14 23:03:57 2022
From: @oob|n@cho|8827 @end|ng |rom gm@||@com (Soobin Choi)
Date: Fri, 14 Oct 2022 16:03:57 -0500
Subject: [R-sig-ME] Inquiry about lme4 in terms of weights
Message-ID: <CAA2HhSDAY4FkOuWurgt_R++12Ovs145x3CUA8EZ7_dXz1H51tw@mail.gmail.com>

Hello,



First of all, thank you so much for the great package. Would it be possible
to use multiple weights (e.g, student weight for level 1 and school weight
for level 2) with lme4 package? If so, could you please let me know how to
do it? I look forward to hearing from you.



All the best,

Soobin

	[[alternative HTML version deleted]]


From @h|r@qotj @end|ng |rom gm@||@com  Sun Oct 16 20:19:00 2022
From: @h|r@qotj @end|ng |rom gm@||@com (Shira Mitchell)
Date: Sun, 16 Oct 2022 14:19:00 -0400
Subject: [R-sig-ME] Bradley Terry GLMM in R ?
In-Reply-To: <CAJuCY5ysJUQQtYUCLF76iD5y9TmSiOD5OAfBt039RjVaD_pzwQ@mail.gmail.com>
References: <CADeg_O3L_6Cmn_Wv0tExeCLccFJS1p5fottqprdh3+spDCrXxA@mail.gmail.com>
 <CAJuCY5ys+di7UNWKLSV8gw6qVC5yjWXVa4J7Bboe=cHAwoYDGw@mail.gmail.com>
 <CADeg_O0d+tuG0DnnnFw9o9VSh1dcMUbOitcT_rLbRcdbhNpzYw@mail.gmail.com>
 <CAJuCY5ysJUQQtYUCLF76iD5y9TmSiOD5OAfBt039RjVaD_pzwQ@mail.gmail.com>
Message-ID: <CADeg_O0g7kTrhRUNhrDZjKt3HEzKpfOKZMW71uBaNTDoaNJt7g@mail.gmail.com>

Super helpful !  Thank you so much !

Out of curiosity, is there a way to fit this type of Bradley-Terry model in
lme4 ? lme4 formulas include random effect via syntax:
https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf
"(expr | factor). The expression expr is evaluated as a linear model
formula, producing a model matrix following the same rules used in standard
R modeling functions (e.g., `lm` or `glm`). The expression factor is
evaluated as an `R` factor. One way to think about the vertical bar
operator is as a special kind of interaction between the model matrix and
the grouping factor. This interaction ensures that the columns of the model
matrix have different effects for each level of the grouping factor."

So (expr | factor) is X_expr * alpha_factor.

So naively writing ~ (1 | m_1) + (1 | m_2) is alpha_{m_1}^{(1)} +
alpha_{m_2}^{(2)}, twice as many parameters as what we want which is
alpha_{m_1} - alpha_{m_2}.

But then see this stackexchange:
https://stats.stackexchange.com/questions/483833/opposing-effects-in-lme4-formulae-bradley-terry-model
"I could just make a design matrix, where player 1 gets the value 1, and
player 2 gets the value ?1. However, unless I'm missing a trick, this would
require having a separate column for each player, and plugging each player
column's name into the formula"

But suppose we create columns for all m = 1,...,M messages:

A_m = 1 if m = m_1
           -1 if m = m_2
            0 otherwise

I think then ~ (A_1 + ... + A_M  | m_1) is alpha_{m_1}^{(m_1)} -
alpha_{m_1}^{(m_2)}, also not what we would want.

Back to INLA. Suppose we now want to add random message-specific slopes for
variable X_i in addition to random message-specific intercepts:

P[i chooses m_1] = logit^-1 (beta_0 + (alpha_{m_1} - alpha_{m_2}) +
(beta_{m_1} - beta_{m_2})X_i)

alpha_1,...,alpha_M ~ N(0,sigma_intercept)
beta_1,...,beta_M ~ N(0,sigma_slope)

I see some resources about this, but nothing super comprehensive. Any
advice where to look for complete documentation ?
https://groups.google.com/g/r-inla-discussion-group/c/iQELaQF8M9Q/m/q7f4-W8YQksJ
(
https://becarioprecario.bitbucket.io/inla-gitbook/ch-multilevel.html#multilevel-models-for-longitudinal-data
https://rpubs.com/corey_sparks/431920
https://avianecologist.com/2016/10/05/multilevel-models/

Here is what we did:

data$w_X = -data$X
data$m_1_beta = data$m_1
data$m_2_beta = data$m_2

inla(depvar ~  f(m_1, model="iid", values = issues) +
                           f(m_2, w, copy = "m_1") +
                           f(m_1_beta, X, model="iid", values = issues) +
                           f(m_2_beta, w_X, copy = "m_1_beta"),
                         family="binomial",
                         data=data)




On Mon, Oct 10, 2022 at 4:24 AM Thierry Onkelinx <thierry.onkelinx at inbo.be>
wrote:

> Dear Shira,
>
> - in a formula object means remove that object from the formula. Use a
> weight of -1 instead.
>
> f(home, model = "iid")) + f(away, w = -1, copy = "home")
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op vr 7 okt. 2022 om 23:36 schreef Shira Mitchell <shiraqotj at gmail.com>:
>
>> Thanks so much, Thierry ! This is great.
>>
>> This works except that I cannot subtract because:
>> f(home, model = "iid")) - f(away, copy = "home")
>>
>> just drops the second term. Apologies that I'm not super familiar with
>> INLA syntax yet.
>>
>>
>>
>> On Fri, Oct 7, 2022 at 10:19 AM Thierry Onkelinx <
>> thierry.onkelinx at inbo.be> wrote:
>>
>>> Hi Shira,
>>>
>>> I fit such models with the INLA package (https://www.r-inla.org/). The
>>> trick is to define two random effects but force their parameter estimates
>>> to be identical.
>>>
>>> The code would contain something like f(home, model = "iid")) + f(away,
>>> copy = "home"). Meaning home ~ N(0, sigma_beta_i) and home[i] = away[i]
>>>
>>> Best regards,
>>>
>>> ir. Thierry Onkelinx
>>> Statisticus / Statistician
>>>
>>> Vlaamse Overheid / Government of Flanders
>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>>> AND FOREST
>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>>> thierry.onkelinx at inbo.be
>>> Havenlaan 88 bus 73, 1000 Brussel
>>> www.inbo.be
>>>
>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>> To call in the statistician after the experiment is done may be no more
>>> than asking him to perform a post-mortem examination: he may be able to say
>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>> The plural of anecdote is not data. ~ Roger Brinner
>>> The combination of some data and an aching desire for an answer does not
>>> ensure that a reasonable answer can be extracted from a given body of data.
>>> ~ John Tukey
>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>
>>> <https://www.inbo.be>
>>>
>>>
>>> Op vr 7 okt. 2022 om 15:00 schreef Shira Mitchell <shiraqotj at gmail.com>:
>>>
>>>> We want to fit Bradley-Terry-style GLMM models in R. We looked into:
>>>>
>>>>
>>>> https://cran.r-project.org/web/packages/BradleyTerry2/vignettes/BradleyTerry.pdf
>>>> and
>>>> http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html
>>>>
>>>> We have voter-specific variables x that influence which political
>>>> message
>>>> (i vs j) wins for them:
>>>>
>>>> logit[pr(i beats j | person with covariate x)] = lambda_i - lambda_j +
>>>> (beta_i - beta_j) x
>>>>
>>>> We then model parameters as random effects:
>>>> lambda_i ~ N(0, sigma_lambda)
>>>> beta_i ~ N(0, sigma_beta)
>>>>
>>>> Is there a way to do this in R ? We do this in TensorFlow in Python by
>>>> directly specifying design matrices with the 0,-1,1 or 0,-x,x entries.
>>>> However, I do not see how to do this in R using lme4, BradleyTerry2,
>>>> mgcv,
>>>> etc.
>>>>
>>>> Thanks so much,
>>>> Shira
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>

	[[alternative HTML version deleted]]


From @h|r@qotj @end|ng |rom gm@||@com  Sun Oct 16 23:49:00 2022
From: @h|r@qotj @end|ng |rom gm@||@com (Shira Mitchell)
Date: Sun, 16 Oct 2022 17:49:00 -0400
Subject: [R-sig-ME] Bradley Terry GLMM in R ?
In-Reply-To: <CADeg_O0g7kTrhRUNhrDZjKt3HEzKpfOKZMW71uBaNTDoaNJt7g@mail.gmail.com>
References: <CADeg_O3L_6Cmn_Wv0tExeCLccFJS1p5fottqprdh3+spDCrXxA@mail.gmail.com>
 <CAJuCY5ys+di7UNWKLSV8gw6qVC5yjWXVa4J7Bboe=cHAwoYDGw@mail.gmail.com>
 <CADeg_O0d+tuG0DnnnFw9o9VSh1dcMUbOitcT_rLbRcdbhNpzYw@mail.gmail.com>
 <CAJuCY5ysJUQQtYUCLF76iD5y9TmSiOD5OAfBt039RjVaD_pzwQ@mail.gmail.com>
 <CADeg_O0g7kTrhRUNhrDZjKt3HEzKpfOKZMW71uBaNTDoaNJt7g@mail.gmail.com>
Message-ID: <CADeg_O0dzRyG=8u7Zjk94PbwGpoqnWoQRw+VRgpoq-BS2RimgA@mail.gmail.com>

Update: Dr Heather Turner <https://www.heatherturner.net/> (author of
BradleyTerry2) suggested the hglm package, which unlike lme4 allows you to
specify generic design matrices (no longer constrained to lme4 formulas !)
Results look really similar to INLA so far. Yay !

On Sun, Oct 16, 2022 at 2:19 PM Shira Mitchell <shiraqotj at gmail.com> wrote:

> Super helpful !  Thank you so much !
>
> Out of curiosity, is there a way to fit this type of Bradley-Terry model
> in lme4 ? lme4 formulas include random effect via syntax:
> https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf
> "(expr | factor). The expression expr is evaluated as a linear model
> formula, producing a model matrix following the same rules used in standard
> R modeling functions (e.g., `lm` or `glm`). The expression factor is
> evaluated as an `R` factor. One way to think about the vertical bar
> operator is as a special kind of interaction between the model matrix and
> the grouping factor. This interaction ensures that the columns of the model
> matrix have different effects for each level of the grouping factor."
>
> So (expr | factor) is X_expr * alpha_factor.
>
> So naively writing ~ (1 | m_1) + (1 | m_2) is alpha_{m_1}^{(1)} +
> alpha_{m_2}^{(2)}, twice as many parameters as what we want which is
> alpha_{m_1} - alpha_{m_2}.
>
> But then see this stackexchange:
>
> https://stats.stackexchange.com/questions/483833/opposing-effects-in-lme4-formulae-bradley-terry-model
> "I could just make a design matrix, where player 1 gets the value 1, and
> player 2 gets the value ?1. However, unless I'm missing a trick, this would
> require having a separate column for each player, and plugging each player
> column's name into the formula"
>
> But suppose we create columns for all m = 1,...,M messages:
>
> A_m = 1 if m = m_1
>            -1 if m = m_2
>             0 otherwise
>
> I think then ~ (A_1 + ... + A_M  | m_1) is alpha_{m_1}^{(m_1)} -
> alpha_{m_1}^{(m_2)}, also not what we would want.
>
> Back to INLA. Suppose we now want to add random message-specific slopes
> for variable X_i in addition to random message-specific intercepts:
>
> P[i chooses m_1] = logit^-1 (beta_0 + (alpha_{m_1} - alpha_{m_2}) +
> (beta_{m_1} - beta_{m_2})X_i)
>
> alpha_1,...,alpha_M ~ N(0,sigma_intercept)
> beta_1,...,beta_M ~ N(0,sigma_slope)
>
> I see some resources about this, but nothing super comprehensive. Any
> advice where to look for complete documentation ?
>
> https://groups.google.com/g/r-inla-discussion-group/c/iQELaQF8M9Q/m/q7f4-W8YQksJ
> (
> https://becarioprecario.bitbucket.io/inla-gitbook/ch-multilevel.html#multilevel-models-for-longitudinal-data
> https://rpubs.com/corey_sparks/431920
> https://avianecologist.com/2016/10/05/multilevel-models/
>
> Here is what we did:
>
> data$w_X = -data$X
> data$m_1_beta = data$m_1
> data$m_2_beta = data$m_2
>
> inla(depvar ~  f(m_1, model="iid", values = issues) +
>                            f(m_2, w, copy = "m_1") +
>                            f(m_1_beta, X, model="iid", values = issues) +
>                            f(m_2_beta, w_X, copy = "m_1_beta"),
>                          family="binomial",
>                          data=data)
>
>
>
>
> On Mon, Oct 10, 2022 at 4:24 AM Thierry Onkelinx <thierry.onkelinx at inbo.be>
> wrote:
>
>> Dear Shira,
>>
>> - in a formula object means remove that object from the formula. Use a
>> weight of -1 instead.
>>
>> f(home, model = "iid")) + f(away, w = -1, copy = "home")
>>
>> Best regards,
>>
>> ir. Thierry Onkelinx
>> Statisticus / Statistician
>>
>> Vlaamse Overheid / Government of Flanders
>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>> AND FOREST
>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>> thierry.onkelinx at inbo.be
>> Havenlaan 88 bus 73, 1000 Brussel
>> www.inbo.be
>>
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of data.
>> ~ John Tukey
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>>
>> <https://www.inbo.be>
>>
>>
>> Op vr 7 okt. 2022 om 23:36 schreef Shira Mitchell <shiraqotj at gmail.com>:
>>
>>> Thanks so much, Thierry ! This is great.
>>>
>>> This works except that I cannot subtract because:
>>> f(home, model = "iid")) - f(away, copy = "home")
>>>
>>> just drops the second term. Apologies that I'm not super familiar with
>>> INLA syntax yet.
>>>
>>>
>>>
>>> On Fri, Oct 7, 2022 at 10:19 AM Thierry Onkelinx <
>>> thierry.onkelinx at inbo.be> wrote:
>>>
>>>> Hi Shira,
>>>>
>>>> I fit such models with the INLA package (https://www.r-inla.org/). The
>>>> trick is to define two random effects but force their parameter estimates
>>>> to be identical.
>>>>
>>>> The code would contain something like f(home, model = "iid")) + f(away,
>>>> copy = "home"). Meaning home ~ N(0, sigma_beta_i) and home[i] = away[i]
>>>>
>>>> Best regards,
>>>>
>>>> ir. Thierry Onkelinx
>>>> Statisticus / Statistician
>>>>
>>>> Vlaamse Overheid / Government of Flanders
>>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>>>> AND FOREST
>>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>>>> thierry.onkelinx at inbo.be
>>>> Havenlaan 88 bus 73, 1000 Brussel
>>>> www.inbo.be
>>>>
>>>>
>>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>> To call in the statistician after the experiment is done may be no more
>>>> than asking him to perform a post-mortem examination: he may be able to say
>>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>>> The plural of anecdote is not data. ~ Roger Brinner
>>>> The combination of some data and an aching desire for an answer does
>>>> not ensure that a reasonable answer can be extracted from a given body of
>>>> data. ~ John Tukey
>>>>
>>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>
>>>> <https://www.inbo.be>
>>>>
>>>>
>>>> Op vr 7 okt. 2022 om 15:00 schreef Shira Mitchell <shiraqotj at gmail.com
>>>> >:
>>>>
>>>>> We want to fit Bradley-Terry-style GLMM models in R. We looked into:
>>>>>
>>>>>
>>>>> https://cran.r-project.org/web/packages/BradleyTerry2/vignettes/BradleyTerry.pdf
>>>>> and
>>>>> http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html
>>>>>
>>>>> We have voter-specific variables x that influence which political
>>>>> message
>>>>> (i vs j) wins for them:
>>>>>
>>>>> logit[pr(i beats j | person with covariate x)] = lambda_i - lambda_j +
>>>>> (beta_i - beta_j) x
>>>>>
>>>>> We then model parameters as random effects:
>>>>> lambda_i ~ N(0, sigma_lambda)
>>>>> beta_i ~ N(0, sigma_beta)
>>>>>
>>>>> Is there a way to do this in R ? We do this in TensorFlow in Python by
>>>>> directly specifying design matrices with the 0,-1,1 or 0,-x,x entries.
>>>>> However, I do not see how to do this in R using lme4, BradleyTerry2,
>>>>> mgcv,
>>>>> etc.
>>>>>
>>>>> Thanks so much,
>>>>> Shira
>>>>>
>>>>>         [[alternative HTML version deleted]]
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>
>>>>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Sun Oct 16 23:58:36 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sun, 16 Oct 2022 17:58:36 -0400
Subject: [R-sig-ME] Bradley Terry GLMM in R ?
In-Reply-To: <CADeg_O0dzRyG=8u7Zjk94PbwGpoqnWoQRw+VRgpoq-BS2RimgA@mail.gmail.com>
References: <CADeg_O3L_6Cmn_Wv0tExeCLccFJS1p5fottqprdh3+spDCrXxA@mail.gmail.com>
 <CAJuCY5ys+di7UNWKLSV8gw6qVC5yjWXVa4J7Bboe=cHAwoYDGw@mail.gmail.com>
 <CADeg_O0d+tuG0DnnnFw9o9VSh1dcMUbOitcT_rLbRcdbhNpzYw@mail.gmail.com>
 <CAJuCY5ysJUQQtYUCLF76iD5y9TmSiOD5OAfBt039RjVaD_pzwQ@mail.gmail.com>
 <CADeg_O0g7kTrhRUNhrDZjKt3HEzKpfOKZMW71uBaNTDoaNJt7g@mail.gmail.com>
 <CADeg_O0dzRyG=8u7Zjk94PbwGpoqnWoQRw+VRgpoq-BS2RimgA@mail.gmail.com>
Message-ID: <7764d02e-13bd-9974-7af0-4eb83a2b9ee1@gmail.com>

    Also:

  you *can* use generic design matrices in lme4, using the modular 
interface described in ?lme4::modular.  I didn't engage with this thread 
enough yet to figure exactly *which* Z and X matrices you wanted to 
construct (I think in this case it's mostly the Z matrix you're worried 
about?), but the basic approach would be


## 1. use a formula and data with *approximately* what you want (e.g. 
using the right grouping variables)
lmod <- lFormula(...)
## 2. substitute whatever you want in lmod$reTrms$Zt and lmod$reTrms$Ztlist
## 3. finish
devfun <- do.call(mkLmerDevfun, lmod)
opt <- optimizeLmer(devfun)
mkMerMod(environment(devfun), opt, lmod$reTrms, fr = lmod$fr)

   There's an example here

https://bbolker.github.io/mixedmodels-misc/notes/multimember.html

   cheers
    Ben Bolker



On 2022-10-16 5:49 p.m., Shira Mitchell wrote:
> Update: Dr Heather Turner <https://www.heatherturner.net/> (author of
> BradleyTerry2) suggested the hglm package, which unlike lme4 allows you to
> specify generic design matrices (no longer constrained to lme4 formulas !)
> Results look really similar to INLA so far. Yay !
> 
> On Sun, Oct 16, 2022 at 2:19 PM Shira Mitchell <shiraqotj at gmail.com> wrote:
> 
>> Super helpful !  Thank you so much !
>>
>> Out of curiosity, is there a way to fit this type of Bradley-Terry model
>> in lme4 ? lme4 formulas include random effect via syntax:
>> https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf
>> "(expr | factor). The expression expr is evaluated as a linear model
>> formula, producing a model matrix following the same rules used in standard
>> R modeling functions (e.g., `lm` or `glm`). The expression factor is
>> evaluated as an `R` factor. One way to think about the vertical bar
>> operator is as a special kind of interaction between the model matrix and
>> the grouping factor. This interaction ensures that the columns of the model
>> matrix have different effects for each level of the grouping factor."
>>
>> So (expr | factor) is X_expr * alpha_factor.
>>
>> So naively writing ~ (1 | m_1) + (1 | m_2) is alpha_{m_1}^{(1)} +
>> alpha_{m_2}^{(2)}, twice as many parameters as what we want which is
>> alpha_{m_1} - alpha_{m_2}.
>>
>> But then see this stackexchange:
>>
>> https://stats.stackexchange.com/questions/483833/opposing-effects-in-lme4-formulae-bradley-terry-model
>> "I could just make a design matrix, where player 1 gets the value 1, and
>> player 2 gets the value ?1. However, unless I'm missing a trick, this would
>> require having a separate column for each player, and plugging each player
>> column's name into the formula"
>>
>> But suppose we create columns for all m = 1,...,M messages:
>>
>> A_m = 1 if m = m_1
>>             -1 if m = m_2
>>              0 otherwise
>>
>> I think then ~ (A_1 + ... + A_M  | m_1) is alpha_{m_1}^{(m_1)} -
>> alpha_{m_1}^{(m_2)}, also not what we would want.
>>
>> Back to INLA. Suppose we now want to add random message-specific slopes
>> for variable X_i in addition to random message-specific intercepts:
>>
>> P[i chooses m_1] = logit^-1 (beta_0 + (alpha_{m_1} - alpha_{m_2}) +
>> (beta_{m_1} - beta_{m_2})X_i)
>>
>> alpha_1,...,alpha_M ~ N(0,sigma_intercept)
>> beta_1,...,beta_M ~ N(0,sigma_slope)
>>
>> I see some resources about this, but nothing super comprehensive. Any
>> advice where to look for complete documentation ?
>>
>> https://groups.google.com/g/r-inla-discussion-group/c/iQELaQF8M9Q/m/q7f4-W8YQksJ
>> (
>> https://becarioprecario.bitbucket.io/inla-gitbook/ch-multilevel.html#multilevel-models-for-longitudinal-data
>> https://rpubs.com/corey_sparks/431920
>> https://avianecologist.com/2016/10/05/multilevel-models/
>>
>> Here is what we did:
>>
>> data$w_X = -data$X
>> data$m_1_beta = data$m_1
>> data$m_2_beta = data$m_2
>>
>> inla(depvar ~  f(m_1, model="iid", values = issues) +
>>                             f(m_2, w, copy = "m_1") +
>>                             f(m_1_beta, X, model="iid", values = issues) +
>>                             f(m_2_beta, w_X, copy = "m_1_beta"),
>>                           family="binomial",
>>                           data=data)
>>
>>
>>
>>
>> On Mon, Oct 10, 2022 at 4:24 AM Thierry Onkelinx <thierry.onkelinx at inbo.be>
>> wrote:
>>
>>> Dear Shira,
>>>
>>> - in a formula object means remove that object from the formula. Use a
>>> weight of -1 instead.
>>>
>>> f(home, model = "iid")) + f(away, w = -1, copy = "home")
>>>
>>> Best regards,
>>>
>>> ir. Thierry Onkelinx
>>> Statisticus / Statistician
>>>
>>> Vlaamse Overheid / Government of Flanders
>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>>> AND FOREST
>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>>> thierry.onkelinx at inbo.be
>>> Havenlaan 88 bus 73, 1000 Brussel
>>> www.inbo.be
>>>
>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>> To call in the statistician after the experiment is done may be no more
>>> than asking him to perform a post-mortem examination: he may be able to say
>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>> The plural of anecdote is not data. ~ Roger Brinner
>>> The combination of some data and an aching desire for an answer does not
>>> ensure that a reasonable answer can be extracted from a given body of data.
>>> ~ John Tukey
>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>
>>> <https://www.inbo.be>
>>>
>>>
>>> Op vr 7 okt. 2022 om 23:36 schreef Shira Mitchell <shiraqotj at gmail.com>:
>>>
>>>> Thanks so much, Thierry ! This is great.
>>>>
>>>> This works except that I cannot subtract because:
>>>> f(home, model = "iid")) - f(away, copy = "home")
>>>>
>>>> just drops the second term. Apologies that I'm not super familiar with
>>>> INLA syntax yet.
>>>>
>>>>
>>>>
>>>> On Fri, Oct 7, 2022 at 10:19 AM Thierry Onkelinx <
>>>> thierry.onkelinx at inbo.be> wrote:
>>>>
>>>>> Hi Shira,
>>>>>
>>>>> I fit such models with the INLA package (https://www.r-inla.org/). The
>>>>> trick is to define two random effects but force their parameter estimates
>>>>> to be identical.
>>>>>
>>>>> The code would contain something like f(home, model = "iid")) + f(away,
>>>>> copy = "home"). Meaning home ~ N(0, sigma_beta_i) and home[i] = away[i]
>>>>>
>>>>> Best regards,
>>>>>
>>>>> ir. Thierry Onkelinx
>>>>> Statisticus / Statistician
>>>>>
>>>>> Vlaamse Overheid / Government of Flanders
>>>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>>>>> AND FOREST
>>>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>>>>> thierry.onkelinx at inbo.be
>>>>> Havenlaan 88 bus 73, 1000 Brussel
>>>>> www.inbo.be
>>>>>
>>>>>
>>>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>> To call in the statistician after the experiment is done may be no more
>>>>> than asking him to perform a post-mortem examination: he may be able to say
>>>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>>>> The plural of anecdote is not data. ~ Roger Brinner
>>>>> The combination of some data and an aching desire for an answer does
>>>>> not ensure that a reasonable answer can be extracted from a given body of
>>>>> data. ~ John Tukey
>>>>>
>>>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>>
>>>>> <https://www.inbo.be>
>>>>>
>>>>>
>>>>> Op vr 7 okt. 2022 om 15:00 schreef Shira Mitchell <shiraqotj at gmail.com
>>>>>> :
>>>>>
>>>>>> We want to fit Bradley-Terry-style GLMM models in R. We looked into:
>>>>>>
>>>>>>
>>>>>> https://cran.r-project.org/web/packages/BradleyTerry2/vignettes/BradleyTerry.pdf
>>>>>> and
>>>>>> http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html
>>>>>>
>>>>>> We have voter-specific variables x that influence which political
>>>>>> message
>>>>>> (i vs j) wins for them:
>>>>>>
>>>>>> logit[pr(i beats j | person with covariate x)] = lambda_i - lambda_j +
>>>>>> (beta_i - beta_j) x
>>>>>>
>>>>>> We then model parameters as random effects:
>>>>>> lambda_i ~ N(0, sigma_lambda)
>>>>>> beta_i ~ N(0, sigma_beta)
>>>>>>
>>>>>> Is there a way to do this in R ? We do this in TensorFlow in Python by
>>>>>> directly specifying design matrices with the 0,-1,1 or 0,-x,x entries.
>>>>>> However, I do not see how to do this in R using lme4, BradleyTerry2,
>>>>>> mgcv,
>>>>>> etc.
>>>>>>
>>>>>> Thanks so much,
>>>>>> Shira
>>>>>>
>>>>>>          [[alternative HTML version deleted]]
>>>>>>
>>>>>> _______________________________________________
>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>
>>>>>
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics
 > E-mail is sent at my convenience; I don't expect replies outside of 
working hours.


From bbo|ker @end|ng |rom gm@||@com  Mon Oct 17 00:17:28 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sun, 16 Oct 2022 18:17:28 -0400
Subject: [R-sig-ME] Bradley Terry GLMM in R ?
In-Reply-To: <CADeg_O0dzRyG=8u7Zjk94PbwGpoqnWoQRw+VRgpoq-BS2RimgA@mail.gmail.com>
References: <CADeg_O3L_6Cmn_Wv0tExeCLccFJS1p5fottqprdh3+spDCrXxA@mail.gmail.com>
 <CAJuCY5ys+di7UNWKLSV8gw6qVC5yjWXVa4J7Bboe=cHAwoYDGw@mail.gmail.com>
 <CADeg_O0d+tuG0DnnnFw9o9VSh1dcMUbOitcT_rLbRcdbhNpzYw@mail.gmail.com>
 <CAJuCY5ysJUQQtYUCLF76iD5y9TmSiOD5OAfBt039RjVaD_pzwQ@mail.gmail.com>
 <CADeg_O0g7kTrhRUNhrDZjKt3HEzKpfOKZMW71uBaNTDoaNJt7g@mail.gmail.com>
 <CADeg_O0dzRyG=8u7Zjk94PbwGpoqnWoQRw+VRgpoq-BS2RimgA@mail.gmail.com>
Message-ID: <e114f5e5-6092-e42e-5caa-418a250e9132@gmail.com>

   PS  my advice about hacking the Z matrix works best/most easily if 
(1) the *dimensions* of the Z matrix (and length of theta, the 
random-effects parameter vector) are the same in the model constructed 
by `lFormula` and the modified model. (It's possible even if not, but 
you would have to hack some other components of the output of `lFormula` 
...)

On 2022-10-16 5:49 p.m., Shira Mitchell wrote:
> Update: Dr Heather Turner <https://www.heatherturner.net/> (author of
> BradleyTerry2) suggested the hglm package, which unlike lme4 allows you to
> specify generic design matrices (no longer constrained to lme4 formulas !)
> Results look really similar to INLA so far. Yay !
> 
> On Sun, Oct 16, 2022 at 2:19 PM Shira Mitchell <shiraqotj at gmail.com> wrote:
> 
>> Super helpful !  Thank you so much !
>>
>> Out of curiosity, is there a way to fit this type of Bradley-Terry model
>> in lme4 ? lme4 formulas include random effect via syntax:
>> https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf
>> "(expr | factor). The expression expr is evaluated as a linear model
>> formula, producing a model matrix following the same rules used in standard
>> R modeling functions (e.g., `lm` or `glm`). The expression factor is
>> evaluated as an `R` factor. One way to think about the vertical bar
>> operator is as a special kind of interaction between the model matrix and
>> the grouping factor. This interaction ensures that the columns of the model
>> matrix have different effects for each level of the grouping factor."
>>
>> So (expr | factor) is X_expr * alpha_factor.
>>
>> So naively writing ~ (1 | m_1) + (1 | m_2) is alpha_{m_1}^{(1)} +
>> alpha_{m_2}^{(2)}, twice as many parameters as what we want which is
>> alpha_{m_1} - alpha_{m_2}.
>>
>> But then see this stackexchange:
>>
>> https://stats.stackexchange.com/questions/483833/opposing-effects-in-lme4-formulae-bradley-terry-model
>> "I could just make a design matrix, where player 1 gets the value 1, and
>> player 2 gets the value ?1. However, unless I'm missing a trick, this would
>> require having a separate column for each player, and plugging each player
>> column's name into the formula"
>>
>> But suppose we create columns for all m = 1,...,M messages:
>>
>> A_m = 1 if m = m_1
>>             -1 if m = m_2
>>              0 otherwise
>>
>> I think then ~ (A_1 + ... + A_M  | m_1) is alpha_{m_1}^{(m_1)} -
>> alpha_{m_1}^{(m_2)}, also not what we would want.
>>
>> Back to INLA. Suppose we now want to add random message-specific slopes
>> for variable X_i in addition to random message-specific intercepts:
>>
>> P[i chooses m_1] = logit^-1 (beta_0 + (alpha_{m_1} - alpha_{m_2}) +
>> (beta_{m_1} - beta_{m_2})X_i)
>>
>> alpha_1,...,alpha_M ~ N(0,sigma_intercept)
>> beta_1,...,beta_M ~ N(0,sigma_slope)
>>
>> I see some resources about this, but nothing super comprehensive. Any
>> advice where to look for complete documentation ?
>>
>> https://groups.google.com/g/r-inla-discussion-group/c/iQELaQF8M9Q/m/q7f4-W8YQksJ
>> (
>> https://becarioprecario.bitbucket.io/inla-gitbook/ch-multilevel.html#multilevel-models-for-longitudinal-data
>> https://rpubs.com/corey_sparks/431920
>> https://avianecologist.com/2016/10/05/multilevel-models/
>>
>> Here is what we did:
>>
>> data$w_X = -data$X
>> data$m_1_beta = data$m_1
>> data$m_2_beta = data$m_2
>>
>> inla(depvar ~  f(m_1, model="iid", values = issues) +
>>                             f(m_2, w, copy = "m_1") +
>>                             f(m_1_beta, X, model="iid", values = issues) +
>>                             f(m_2_beta, w_X, copy = "m_1_beta"),
>>                           family="binomial",
>>                           data=data)
>>
>>
>>
>>
>> On Mon, Oct 10, 2022 at 4:24 AM Thierry Onkelinx <thierry.onkelinx at inbo.be>
>> wrote:
>>
>>> Dear Shira,
>>>
>>> - in a formula object means remove that object from the formula. Use a
>>> weight of -1 instead.
>>>
>>> f(home, model = "iid")) + f(away, w = -1, copy = "home")
>>>
>>> Best regards,
>>>
>>> ir. Thierry Onkelinx
>>> Statisticus / Statistician
>>>
>>> Vlaamse Overheid / Government of Flanders
>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>>> AND FOREST
>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>>> thierry.onkelinx at inbo.be
>>> Havenlaan 88 bus 73, 1000 Brussel
>>> www.inbo.be
>>>
>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>> To call in the statistician after the experiment is done may be no more
>>> than asking him to perform a post-mortem examination: he may be able to say
>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>> The plural of anecdote is not data. ~ Roger Brinner
>>> The combination of some data and an aching desire for an answer does not
>>> ensure that a reasonable answer can be extracted from a given body of data.
>>> ~ John Tukey
>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>
>>> <https://www.inbo.be>
>>>
>>>
>>> Op vr 7 okt. 2022 om 23:36 schreef Shira Mitchell <shiraqotj at gmail.com>:
>>>
>>>> Thanks so much, Thierry ! This is great.
>>>>
>>>> This works except that I cannot subtract because:
>>>> f(home, model = "iid")) - f(away, copy = "home")
>>>>
>>>> just drops the second term. Apologies that I'm not super familiar with
>>>> INLA syntax yet.
>>>>
>>>>
>>>>
>>>> On Fri, Oct 7, 2022 at 10:19 AM Thierry Onkelinx <
>>>> thierry.onkelinx at inbo.be> wrote:
>>>>
>>>>> Hi Shira,
>>>>>
>>>>> I fit such models with the INLA package (https://www.r-inla.org/). The
>>>>> trick is to define two random effects but force their parameter estimates
>>>>> to be identical.
>>>>>
>>>>> The code would contain something like f(home, model = "iid")) + f(away,
>>>>> copy = "home"). Meaning home ~ N(0, sigma_beta_i) and home[i] = away[i]
>>>>>
>>>>> Best regards,
>>>>>
>>>>> ir. Thierry Onkelinx
>>>>> Statisticus / Statistician
>>>>>
>>>>> Vlaamse Overheid / Government of Flanders
>>>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>>>>> AND FOREST
>>>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>>>>> thierry.onkelinx at inbo.be
>>>>> Havenlaan 88 bus 73, 1000 Brussel
>>>>> www.inbo.be
>>>>>
>>>>>
>>>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>> To call in the statistician after the experiment is done may be no more
>>>>> than asking him to perform a post-mortem examination: he may be able to say
>>>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>>>> The plural of anecdote is not data. ~ Roger Brinner
>>>>> The combination of some data and an aching desire for an answer does
>>>>> not ensure that a reasonable answer can be extracted from a given body of
>>>>> data. ~ John Tukey
>>>>>
>>>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>>
>>>>> <https://www.inbo.be>
>>>>>
>>>>>
>>>>> Op vr 7 okt. 2022 om 15:00 schreef Shira Mitchell <shiraqotj at gmail.com
>>>>>> :
>>>>>
>>>>>> We want to fit Bradley-Terry-style GLMM models in R. We looked into:
>>>>>>
>>>>>>
>>>>>> https://cran.r-project.org/web/packages/BradleyTerry2/vignettes/BradleyTerry.pdf
>>>>>> and
>>>>>> http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html
>>>>>>
>>>>>> We have voter-specific variables x that influence which political
>>>>>> message
>>>>>> (i vs j) wins for them:
>>>>>>
>>>>>> logit[pr(i beats j | person with covariate x)] = lambda_i - lambda_j +
>>>>>> (beta_i - beta_j) x
>>>>>>
>>>>>> We then model parameters as random effects:
>>>>>> lambda_i ~ N(0, sigma_lambda)
>>>>>> beta_i ~ N(0, sigma_beta)
>>>>>>
>>>>>> Is there a way to do this in R ? We do this in TensorFlow in Python by
>>>>>> directly specifying design matrices with the 0,-1,1 or 0,-x,x entries.
>>>>>> However, I do not see how to do this in R using lme4, BradleyTerry2,
>>>>>> mgcv,
>>>>>> etc.
>>>>>>
>>>>>> Thanks so much,
>>>>>> Shira
>>>>>>
>>>>>>          [[alternative HTML version deleted]]
>>>>>>
>>>>>> _______________________________________________
>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>
>>>>>
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics
 > E-mail is sent at my convenience; I don't expect replies outside of 
working hours.


From j@h@d||e|d @end|ng |rom ed@@c@uk  Mon Oct 17 10:45:27 2022
From: j@h@d||e|d @end|ng |rom ed@@c@uk (Jarrod Hadfield)
Date: Mon, 17 Oct 2022 08:45:27 +0000
Subject: [R-sig-ME] Bradley Terry GLMM in R ?
In-Reply-To: <CADeg_O0dzRyG=8u7Zjk94PbwGpoqnWoQRw+VRgpoq-BS2RimgA@mail.gmail.com>
References: <CADeg_O3L_6Cmn_Wv0tExeCLccFJS1p5fottqprdh3+spDCrXxA@mail.gmail.com>
 <CAJuCY5ys+di7UNWKLSV8gw6qVC5yjWXVa4J7Bboe=cHAwoYDGw@mail.gmail.com>
 <CADeg_O0d+tuG0DnnnFw9o9VSh1dcMUbOitcT_rLbRcdbhNpzYw@mail.gmail.com>
 <CAJuCY5ysJUQQtYUCLF76iD5y9TmSiOD5OAfBt039RjVaD_pzwQ@mail.gmail.com>
 <CADeg_O0g7kTrhRUNhrDZjKt3HEzKpfOKZMW71uBaNTDoaNJt7g@mail.gmail.com>
 <CADeg_O0dzRyG=8u7Zjk94PbwGpoqnWoQRw+VRgpoq-BS2RimgA@mail.gmail.com>
Message-ID: <E27AFF46-A57B-425D-A946-AE24E6567B7C@ed.ac.uk>

Hi Shira,

Perhaps a little late to be useful, but MCMCglmm also fits random-effect Bradley-Terry models. Just specify ~mm(opponent1-opponent2) in the random effect formula. The mm stands for multimembership - the BT model is like a multimembership model where some effects have been multiplied by -1, hence the ?-' rather than ?+? in the mm model formula.

Cheers,

Jarrod


> On 16 Oct 2022, at 22:49, Shira Mitchell <shiraqotj at gmail.com> wrote:
>
> This email was sent to you by someone outside the University.
> You should only click on links or attachments if you are certain that the email is genuine and the content is safe.
>
> Update: Dr Heather Turner <https://www.heatherturner.net/> (author of
> BradleyTerry2) suggested the hglm package, which unlike lme4 allows you to
> specify generic design matrices (no longer constrained to lme4 formulas !)
> Results look really similar to INLA so far. Yay !
>
> On Sun, Oct 16, 2022 at 2:19 PM Shira Mitchell <shiraqotj at gmail.com> wrote:
>
>> Super helpful !  Thank you so much !
>>
>> Out of curiosity, is there a way to fit this type of Bradley-Terry model
>> in lme4 ? lme4 formulas include random effect via syntax:
>> https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf
>> "(expr | factor). The expression expr is evaluated as a linear model
>> formula, producing a model matrix following the same rules used in standard
>> R modeling functions (e.g., `lm` or `glm`). The expression factor is
>> evaluated as an `R` factor. One way to think about the vertical bar
>> operator is as a special kind of interaction between the model matrix and
>> the grouping factor. This interaction ensures that the columns of the model
>> matrix have different effects for each level of the grouping factor."
>>
>> So (expr | factor) is X_expr * alpha_factor.
>>
>> So naively writing ~ (1 | m_1) + (1 | m_2) is alpha_{m_1}^{(1)} +
>> alpha_{m_2}^{(2)}, twice as many parameters as what we want which is
>> alpha_{m_1} - alpha_{m_2}.
>>
>> But then see this stackexchange:
>>
>> https://stats.stackexchange.com/questions/483833/opposing-effects-in-lme4-formulae-bradley-terry-model
>> "I could just make a design matrix, where player 1 gets the value 1, and
>> player 2 gets the value ?1. However, unless I'm missing a trick, this would
>> require having a separate column for each player, and plugging each player
>> column's name into the formula"
>>
>> But suppose we create columns for all m = 1,...,M messages:
>>
>> A_m = 1 if m = m_1
>>           -1 if m = m_2
>>            0 otherwise
>>
>> I think then ~ (A_1 + ... + A_M  | m_1) is alpha_{m_1}^{(m_1)} -
>> alpha_{m_1}^{(m_2)}, also not what we would want.
>>
>> Back to INLA. Suppose we now want to add random message-specific slopes
>> for variable X_i in addition to random message-specific intercepts:
>>
>> P[i chooses m_1] = logit^-1 (beta_0 + (alpha_{m_1} - alpha_{m_2}) +
>> (beta_{m_1} - beta_{m_2})X_i)
>>
>> alpha_1,...,alpha_M ~ N(0,sigma_intercept)
>> beta_1,...,beta_M ~ N(0,sigma_slope)
>>
>> I see some resources about this, but nothing super comprehensive. Any
>> advice where to look for complete documentation ?
>>
>> https://groups.google.com/g/r-inla-discussion-group/c/iQELaQF8M9Q/m/q7f4-W8YQksJ
>> (
>> https://becarioprecario.bitbucket.io/inla-gitbook/ch-multilevel.html#multilevel-models-for-longitudinal-data
>> https://rpubs.com/corey_sparks/431920
>> https://avianecologist.com/2016/10/05/multilevel-models/
>>
>> Here is what we did:
>>
>> data$w_X = -data$X
>> data$m_1_beta = data$m_1
>> data$m_2_beta = data$m_2
>>
>> inla(depvar ~  f(m_1, model="iid", values = issues) +
>>                           f(m_2, w, copy = "m_1") +
>>                           f(m_1_beta, X, model="iid", values = issues) +
>>                           f(m_2_beta, w_X, copy = "m_1_beta"),
>>                         family="binomial",
>>                         data=data)
>>
>>
>>
>>
>> On Mon, Oct 10, 2022 at 4:24 AM Thierry Onkelinx <thierry.onkelinx at inbo.be>
>> wrote:
>>
>>> Dear Shira,
>>>
>>> - in a formula object means remove that object from the formula. Use a
>>> weight of -1 instead.
>>>
>>> f(home, model = "iid")) + f(away, w = -1, copy = "home")
>>>
>>> Best regards,
>>>
>>> ir. Thierry Onkelinx
>>> Statisticus / Statistician
>>>
>>> Vlaamse Overheid / Government of Flanders
>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>>> AND FOREST
>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>>> thierry.onkelinx at inbo.be
>>> Havenlaan 88 bus 73, 1000 Brussel
>>> www.inbo.be
>>>
>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>> To call in the statistician after the experiment is done may be no more
>>> than asking him to perform a post-mortem examination: he may be able to say
>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>> The plural of anecdote is not data. ~ Roger Brinner
>>> The combination of some data and an aching desire for an answer does not
>>> ensure that a reasonable answer can be extracted from a given body of data.
>>> ~ John Tukey
>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>
>>> <https://www.inbo.be>
>>>
>>>
>>> Op vr 7 okt. 2022 om 23:36 schreef Shira Mitchell <shiraqotj at gmail.com>:
>>>
>>>> Thanks so much, Thierry ! This is great.
>>>>
>>>> This works except that I cannot subtract because:
>>>> f(home, model = "iid")) - f(away, copy = "home")
>>>>
>>>> just drops the second term. Apologies that I'm not super familiar with
>>>> INLA syntax yet.
>>>>
>>>>
>>>>
>>>> On Fri, Oct 7, 2022 at 10:19 AM Thierry Onkelinx <
>>>> thierry.onkelinx at inbo.be> wrote:
>>>>
>>>>> Hi Shira,
>>>>>
>>>>> I fit such models with the INLA package (https://www.r-inla.org/). The
>>>>> trick is to define two random effects but force their parameter estimates
>>>>> to be identical.
>>>>>
>>>>> The code would contain something like f(home, model = "iid")) + f(away,
>>>>> copy = "home"). Meaning home ~ N(0, sigma_beta_i) and home[i] = away[i]
>>>>>
>>>>> Best regards,
>>>>>
>>>>> ir. Thierry Onkelinx
>>>>> Statisticus / Statistician
>>>>>
>>>>> Vlaamse Overheid / Government of Flanders
>>>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>>>>> AND FOREST
>>>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>>>>> thierry.onkelinx at inbo.be
>>>>> Havenlaan 88 bus 73, 1000 Brussel
>>>>> www.inbo.be
>>>>>
>>>>>
>>>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>> To call in the statistician after the experiment is done may be no more
>>>>> than asking him to perform a post-mortem examination: he may be able to say
>>>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>>>> The plural of anecdote is not data. ~ Roger Brinner
>>>>> The combination of some data and an aching desire for an answer does
>>>>> not ensure that a reasonable answer can be extracted from a given body of
>>>>> data. ~ John Tukey
>>>>>
>>>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>>
>>>>> <https://www.inbo.be>
>>>>>
>>>>>
>>>>> Op vr 7 okt. 2022 om 15:00 schreef Shira Mitchell <shiraqotj at gmail.com
>>>>>> :
>>>>>
>>>>>> We want to fit Bradley-Terry-style GLMM models in R. We looked into:
>>>>>>
>>>>>>
>>>>>> https://cran.r-project.org/web/packages/BradleyTerry2/vignettes/BradleyTerry.pdf
>>>>>> and
>>>>>> http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html
>>>>>>
>>>>>> We have voter-specific variables x that influence which political
>>>>>> message
>>>>>> (i vs j) wins for them:
>>>>>>
>>>>>> logit[pr(i beats j | person with covariate x)] = lambda_i - lambda_j +
>>>>>> (beta_i - beta_j) x
>>>>>>
>>>>>> We then model parameters as random effects:
>>>>>> lambda_i ~ N(0, sigma_lambda)
>>>>>> beta_i ~ N(0, sigma_beta)
>>>>>>
>>>>>> Is there a way to do this in R ? We do this in TensorFlow in Python by
>>>>>> directly specifying design matrices with the 0,-1,1 or 0,-x,x entries.
>>>>>> However, I do not see how to do this in R using lme4, BradleyTerry2,
>>>>>> mgcv,
>>>>>> etc.
>>>>>>
>>>>>> Thanks so much,
>>>>>> Shira
>>>>>>
>>>>>>        [[alternative HTML version deleted]]
>>>>>>
>>>>>> _______________________________________________
>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>
>>>>>
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

The University of Edinburgh is a charitable body, registered in Scotland, with registration number SC005336. Is e buidheann carthannais a th? ann an Oilthigh Dh?n ?ideann, cl?raichte an Alba, ?ireamh cl?raidh SC005336.

From @r|ve@ @end|ng |rom w|@c@edu  Mon Oct 17 16:10:33 2022
From: @r|ve@ @end|ng |rom w|@c@edu (Anthony R. Ives)
Date: Mon, 17 Oct 2022 14:10:33 +0000
Subject: [R-sig-ME] Time series with multinomial outcomes
In-Reply-To: <e114f5e5-6092-e42e-5caa-418a250e9132@gmail.com>
References: <CADeg_O3L_6Cmn_Wv0tExeCLccFJS1p5fottqprdh3+spDCrXxA@mail.gmail.com>
 <CAJuCY5ys+di7UNWKLSV8gw6qVC5yjWXVa4J7Bboe=cHAwoYDGw@mail.gmail.com>
 <CADeg_O0d+tuG0DnnnFw9o9VSh1dcMUbOitcT_rLbRcdbhNpzYw@mail.gmail.com>
 <CAJuCY5ysJUQQtYUCLF76iD5y9TmSiOD5OAfBt039RjVaD_pzwQ@mail.gmail.com>
 <CADeg_O0g7kTrhRUNhrDZjKt3HEzKpfOKZMW71uBaNTDoaNJt7g@mail.gmail.com>
 <CADeg_O0dzRyG=8u7Zjk94PbwGpoqnWoQRw+VRgpoq-BS2RimgA@mail.gmail.com>
 <e114f5e5-6092-e42e-5caa-418a250e9132@gmail.com>
Message-ID: <SJ0PR06MB83273B9AF64E1A4363C9354BB4299@SJ0PR06MB8327.namprd06.prod.outlook.com>

I?m working on a times series problem with multinominal outcomes. The specific problem is time series of pollen counts in paleoecological data where the response variable is the number of pollen grains in a core sample at a given depth identified to N species, where N > 2. I can code this as a state-space model for an N-dimensional vector autoregressive process (VAR(1)) with a multinomial measurement (updating) equation, and then fit it using penalized quasi-likelihood. This works surprisingly well, but it is a pretty old-school approach.

Does anybody know if this model can be fit with existing R packages?

Many thanks!

__________________________
Anthony R. Ives (he/him/his)
UW-Madison
459 Birge Hall
608-262-1519


From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on behalf of Ben Bolker <bbolker at gmail.com>
Date: Sunday, October 16, 2022 at 5:17 PM
To: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Bradley Terry GLMM in R ?
   PS  my advice about hacking the Z matrix works best/most easily if
(1) the *dimensions* of the Z matrix (and length of theta, the
random-effects parameter vector) are the same in the model constructed
by `lFormula` and the modified model. (It's possible even if not, but
you would have to hack some other components of the output of `lFormula`
...)

On 2022-10-16 5:49 p.m., Shira Mitchell wrote:
> Update: Dr Heather Turner <https://www.heatherturner.net/> (author of
> BradleyTerry2) suggested the hglm package, which unlike lme4 allows you to
> specify generic design matrices (no longer constrained to lme4 formulas !)
> Results look really similar to INLA so far. Yay !
>
> On Sun, Oct 16, 2022 at 2:19 PM Shira Mitchell <shiraqotj at gmail.com> wrote:
>
>> Super helpful !  Thank you so much !
>>
>> Out of curiosity, is there a way to fit this type of Bradley-Terry model
>> in lme4 ? lme4 formulas include random effect via syntax:
>> https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf
>> "(expr | factor). The expression expr is evaluated as a linear model
>> formula, producing a model matrix following the same rules used in standard
>> R modeling functions (e.g., `lm` or `glm`). The expression factor is
>> evaluated as an `R` factor. One way to think about the vertical bar
>> operator is as a special kind of interaction between the model matrix and
>> the grouping factor. This interaction ensures that the columns of the model
>> matrix have different effects for each level of the grouping factor."
>>
>> So (expr | factor) is X_expr * alpha_factor.
>>
>> So naively writing ~ (1 | m_1) + (1 | m_2) is alpha_{m_1}^{(1)} +
>> alpha_{m_2}^{(2)}, twice as many parameters as what we want which is
>> alpha_{m_1} - alpha_{m_2}.
>>
>> But then see this stackexchange:
>>
>> https://stats.stackexchange.com/questions/483833/opposing-effects-in-lme4-formulae-bradley-terry-model
>> "I could just make a design matrix, where player 1 gets the value 1, and
>> player 2 gets the value ?1. However, unless I'm missing a trick, this would
>> require having a separate column for each player, and plugging each player
>> column's name into the formula"
>>
>> But suppose we create columns for all m = 1,...,M messages:
>>
>> A_m = 1 if m = m_1
>>             -1 if m = m_2
>>              0 otherwise
>>
>> I think then ~ (A_1 + ... + A_M  | m_1) is alpha_{m_1}^{(m_1)} -
>> alpha_{m_1}^{(m_2)}, also not what we would want.
>>
>> Back to INLA. Suppose we now want to add random message-specific slopes
>> for variable X_i in addition to random message-specific intercepts:
>>
>> P[i chooses m_1] = logit^-1 (beta_0 + (alpha_{m_1} - alpha_{m_2}) +
>> (beta_{m_1} - beta_{m_2})X_i)
>>
>> alpha_1,...,alpha_M ~ N(0,sigma_intercept)
>> beta_1,...,beta_M ~ N(0,sigma_slope)
>>
>> I see some resources about this, but nothing super comprehensive. Any
>> advice where to look for complete documentation ?
>>
>> https://groups.google.com/g/r-inla-discussion-group/c/iQELaQF8M9Q/m/q7f4-W8YQksJ
>> (
>> https://becarioprecario.bitbucket.io/inla-gitbook/ch-multilevel.html#multilevel-models-for-longitudinal-data
>> https://rpubs.com/corey_sparks/431920
>> https://avianecologist.com/2016/10/05/multilevel-models/
>>
>> Here is what we did:
>>
>> data$w_X = -data$X
>> data$m_1_beta = data$m_1
>> data$m_2_beta = data$m_2
>>
>> inla(depvar ~  f(m_1, model="iid", values = issues) +
>>                             f(m_2, w, copy = "m_1") +
>>                             f(m_1_beta, X, model="iid", values = issues) +
>>                             f(m_2_beta, w_X, copy = "m_1_beta"),
>>                           family="binomial",
>>                           data=data)
>>
>>
>>
>>
>> On Mon, Oct 10, 2022 at 4:24 AM Thierry Onkelinx <thierry.onkelinx at inbo.be>
>> wrote:
>>
>>> Dear Shira,
>>>
>>> - in a formula object means remove that object from the formula. Use a
>>> weight of -1 instead.
>>>
>>> f(home, model = "iid")) + f(away, w = -1, copy = "home")
>>>
>>> Best regards,
>>>
>>> ir. Thierry Onkelinx
>>> Statisticus / Statistician
>>>
>>> Vlaamse Overheid / Government of Flanders
>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>>> AND FOREST
>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>>> thierry.onkelinx at inbo.be
>>> Havenlaan 88 bus 73, 1000 Brussel
>>> www.inbo.be<http://www.inbo.be>
>>>
>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>> To call in the statistician after the experiment is done may be no more
>>> than asking him to perform a post-mortem examination: he may be able to say
>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>> The plural of anecdote is not data. ~ Roger Brinner
>>> The combination of some data and an aching desire for an answer does not
>>> ensure that a reasonable answer can be extracted from a given body of data.
>>> ~ John Tukey
>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>
>>> <https://www.inbo.be>
>>>
>>>
>>> Op vr 7 okt. 2022 om 23:36 schreef Shira Mitchell <shiraqotj at gmail.com>:
>>>
>>>> Thanks so much, Thierry ! This is great.
>>>>
>>>> This works except that I cannot subtract because:
>>>> f(home, model = "iid")) - f(away, copy = "home")
>>>>
>>>> just drops the second term. Apologies that I'm not super familiar with
>>>> INLA syntax yet.
>>>>
>>>>
>>>>
>>>> On Fri, Oct 7, 2022 at 10:19 AM Thierry Onkelinx <
>>>> thierry.onkelinx at inbo.be> wrote:
>>>>
>>>>> Hi Shira,
>>>>>
>>>>> I fit such models with the INLA package (https://www.r-inla.org/). The
>>>>> trick is to define two random effects but force their parameter estimates
>>>>> to be identical.
>>>>>
>>>>> The code would contain something like f(home, model = "iid")) + f(away,
>>>>> copy = "home"). Meaning home ~ N(0, sigma_beta_i) and home[i] = away[i]
>>>>>
>>>>> Best regards,
>>>>>
>>>>> ir. Thierry Onkelinx
>>>>> Statisticus / Statistician
>>>>>
>>>>> Vlaamse Overheid / Government of Flanders
>>>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>>>>> AND FOREST
>>>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>>>>> thierry.onkelinx at inbo.be
>>>>> Havenlaan 88 bus 73, 1000 Brussel
>>>>> www.inbo.be<http://www.inbo.be>
>>>>>
>>>>>
>>>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>> To call in the statistician after the experiment is done may be no more
>>>>> than asking him to perform a post-mortem examination: he may be able to say
>>>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>>>> The plural of anecdote is not data. ~ Roger Brinner
>>>>> The combination of some data and an aching desire for an answer does
>>>>> not ensure that a reasonable answer can be extracted from a given body of
>>>>> data. ~ John Tukey
>>>>>
>>>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>>
>>>>> <https://www.inbo.be>
>>>>>
>>>>>
>>>>> Op vr 7 okt. 2022 om 15:00 schreef Shira Mitchell <shiraqotj at gmail.com
>>>>>> :
>>>>>
>>>>>> We want to fit Bradley-Terry-style GLMM models in R. We looked into:
>>>>>>
>>>>>>
>>>>>> https://cran.r-project.org/web/packages/BradleyTerry2/vignettes/BradleyTerry.pdf
>>>>>> and
>>>>>> http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html
>>>>>>
>>>>>> We have voter-specific variables x that influence which political
>>>>>> message
>>>>>> (i vs j) wins for them:
>>>>>>
>>>>>> logit[pr(i beats j | person with covariate x)] = lambda_i - lambda_j +
>>>>>> (beta_i - beta_j) x
>>>>>>
>>>>>> We then model parameters as random effects:
>>>>>> lambda_i ~ N(0, sigma_lambda)
>>>>>> beta_i ~ N(0, sigma_beta)
>>>>>>
>>>>>> Is there a way to do this in R ? We do this in TensorFlow in Python by
>>>>>> directly specifying design matrices with the 0,-1,1 or 0,-x,x entries.
>>>>>> However, I do not see how to do this in R using lme4, BradleyTerry2,
>>>>>> mgcv,
>>>>>> etc.
>>>>>>
>>>>>> Thanks so much,
>>>>>> Shira
>>>>>>
>>>>>>          [[alternative HTML version deleted]]
>>>>>>
>>>>>> _______________________________________________
>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>
>>>>>
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

--
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics
 > E-mail is sent at my convenience; I don't expect replies outside of
working hours.

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From @h|r@qotj @end|ng |rom gm@||@com  Mon Oct 17 21:55:52 2022
From: @h|r@qotj @end|ng |rom gm@||@com (Shira Mitchell)
Date: Mon, 17 Oct 2022 15:55:52 -0400
Subject: [R-sig-ME] Bradley Terry GLMM in R ?
In-Reply-To: <E27AFF46-A57B-425D-A946-AE24E6567B7C@ed.ac.uk>
References: <CADeg_O3L_6Cmn_Wv0tExeCLccFJS1p5fottqprdh3+spDCrXxA@mail.gmail.com>
 <CAJuCY5ys+di7UNWKLSV8gw6qVC5yjWXVa4J7Bboe=cHAwoYDGw@mail.gmail.com>
 <CADeg_O0d+tuG0DnnnFw9o9VSh1dcMUbOitcT_rLbRcdbhNpzYw@mail.gmail.com>
 <CAJuCY5ysJUQQtYUCLF76iD5y9TmSiOD5OAfBt039RjVaD_pzwQ@mail.gmail.com>
 <CADeg_O0g7kTrhRUNhrDZjKt3HEzKpfOKZMW71uBaNTDoaNJt7g@mail.gmail.com>
 <CADeg_O0dzRyG=8u7Zjk94PbwGpoqnWoQRw+VRgpoq-BS2RimgA@mail.gmail.com>
 <E27AFF46-A57B-425D-A946-AE24E6567B7C@ed.ac.uk>
Message-ID: <CADeg_O12OrDgEMNQ6ejcV30evndEjAuFN-wKLdbN0K4GusyAQw@mail.gmail.com>

Thanks so much, Jarrod ! Not too late at all. Very interesting to compare
MCMC with the approximations (INLA, hglm's extended quasi likelihood). I
don't think I have the priors lined up yet across packages. The random
effects seem more dispersed according to MCMCglmm than in INLA or hglm, but
this could be due to priors not fit algorithm. Will look into the package
prior defaults.

On Mon, Oct 17, 2022 at 4:45 AM Jarrod Hadfield <j.hadfield at ed.ac.uk> wrote:

> Hi Shira,
>
> Perhaps a little late to be useful, but MCMCglmm also fits random-effect
> Bradley-Terry models. Just specify ~mm(opponent1-opponent2) in the random
> effect formula. The mm stands for multimembership - the BT model is like a
> multimembership model where some effects have been multiplied by -1, hence
> the ?-' rather than ?+? in the mm model formula.
>
> Cheers,
>
> Jarrod
>
>
> > On 16 Oct 2022, at 22:49, Shira Mitchell <shiraqotj at gmail.com> wrote:
> >
> > This email was sent to you by someone outside the University.
> > You should only click on links or attachments if you are certain that
> the email is genuine and the content is safe.
> >
> > Update: Dr Heather Turner <https://www.heatherturner.net/> (author of
> > BradleyTerry2) suggested the hglm package, which unlike lme4 allows you
> to
> > specify generic design matrices (no longer constrained to lme4 formulas
> !)
> > Results look really similar to INLA so far. Yay !
> >
> > On Sun, Oct 16, 2022 at 2:19 PM Shira Mitchell <shiraqotj at gmail.com>
> wrote:
> >
> >> Super helpful !  Thank you so much !
> >>
> >> Out of curiosity, is there a way to fit this type of Bradley-Terry model
> >> in lme4 ? lme4 formulas include random effect via syntax:
> >> https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf
> >> "(expr | factor). The expression expr is evaluated as a linear model
> >> formula, producing a model matrix following the same rules used in
> standard
> >> R modeling functions (e.g., `lm` or `glm`). The expression factor is
> >> evaluated as an `R` factor. One way to think about the vertical bar
> >> operator is as a special kind of interaction between the model matrix
> and
> >> the grouping factor. This interaction ensures that the columns of the
> model
> >> matrix have different effects for each level of the grouping factor."
> >>
> >> So (expr | factor) is X_expr * alpha_factor.
> >>
> >> So naively writing ~ (1 | m_1) + (1 | m_2) is alpha_{m_1}^{(1)} +
> >> alpha_{m_2}^{(2)}, twice as many parameters as what we want which is
> >> alpha_{m_1} - alpha_{m_2}.
> >>
> >> But then see this stackexchange:
> >>
> >>
> https://stats.stackexchange.com/questions/483833/opposing-effects-in-lme4-formulae-bradley-terry-model
> >> "I could just make a design matrix, where player 1 gets the value 1, and
> >> player 2 gets the value ?1. However, unless I'm missing a trick, this
> would
> >> require having a separate column for each player, and plugging each
> player
> >> column's name into the formula"
> >>
> >> But suppose we create columns for all m = 1,...,M messages:
> >>
> >> A_m = 1 if m = m_1
> >>           -1 if m = m_2
> >>            0 otherwise
> >>
> >> I think then ~ (A_1 + ... + A_M  | m_1) is alpha_{m_1}^{(m_1)} -
> >> alpha_{m_1}^{(m_2)}, also not what we would want.
> >>
> >> Back to INLA. Suppose we now want to add random message-specific slopes
> >> for variable X_i in addition to random message-specific intercepts:
> >>
> >> P[i chooses m_1] = logit^-1 (beta_0 + (alpha_{m_1} - alpha_{m_2}) +
> >> (beta_{m_1} - beta_{m_2})X_i)
> >>
> >> alpha_1,...,alpha_M ~ N(0,sigma_intercept)
> >> beta_1,...,beta_M ~ N(0,sigma_slope)
> >>
> >> I see some resources about this, but nothing super comprehensive. Any
> >> advice where to look for complete documentation ?
> >>
> >>
> https://groups.google.com/g/r-inla-discussion-group/c/iQELaQF8M9Q/m/q7f4-W8YQksJ
> >> (
> >>
> https://becarioprecario.bitbucket.io/inla-gitbook/ch-multilevel.html#multilevel-models-for-longitudinal-data
> >> https://rpubs.com/corey_sparks/431920
> >> https://avianecologist.com/2016/10/05/multilevel-models/
> >>
> >> Here is what we did:
> >>
> >> data$w_X = -data$X
> >> data$m_1_beta = data$m_1
> >> data$m_2_beta = data$m_2
> >>
> >> inla(depvar ~  f(m_1, model="iid", values = issues) +
> >>                           f(m_2, w, copy = "m_1") +
> >>                           f(m_1_beta, X, model="iid", values = issues) +
> >>                           f(m_2_beta, w_X, copy = "m_1_beta"),
> >>                         family="binomial",
> >>                         data=data)
> >>
> >>
> >>
> >>
> >> On Mon, Oct 10, 2022 at 4:24 AM Thierry Onkelinx <
> thierry.onkelinx at inbo.be>
> >> wrote:
> >>
> >>> Dear Shira,
> >>>
> >>> - in a formula object means remove that object from the formula. Use a
> >>> weight of -1 instead.
> >>>
> >>> f(home, model = "iid")) + f(away, w = -1, copy = "home")
> >>>
> >>> Best regards,
> >>>
> >>> ir. Thierry Onkelinx
> >>> Statisticus / Statistician
> >>>
> >>> Vlaamse Overheid / Government of Flanders
> >>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
> >>> AND FOREST
> >>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> >>> thierry.onkelinx at inbo.be
> >>> Havenlaan 88 bus 73, 1000 Brussel
> >>> www.inbo.be
> >>>
> >>>
> >>>
> ///////////////////////////////////////////////////////////////////////////////////////////
> >>> To call in the statistician after the experiment is done may be no more
> >>> than asking him to perform a post-mortem examination: he may be able
> to say
> >>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> >>> The plural of anecdote is not data. ~ Roger Brinner
> >>> The combination of some data and an aching desire for an answer does
> not
> >>> ensure that a reasonable answer can be extracted from a given body of
> data.
> >>> ~ John Tukey
> >>>
> >>>
> ///////////////////////////////////////////////////////////////////////////////////////////
> >>>
> >>> <https://www.inbo.be>
> >>>
> >>>
> >>> Op vr 7 okt. 2022 om 23:36 schreef Shira Mitchell <shiraqotj at gmail.com
> >:
> >>>
> >>>> Thanks so much, Thierry ! This is great.
> >>>>
> >>>> This works except that I cannot subtract because:
> >>>> f(home, model = "iid")) - f(away, copy = "home")
> >>>>
> >>>> just drops the second term. Apologies that I'm not super familiar with
> >>>> INLA syntax yet.
> >>>>
> >>>>
> >>>>
> >>>> On Fri, Oct 7, 2022 at 10:19 AM Thierry Onkelinx <
> >>>> thierry.onkelinx at inbo.be> wrote:
> >>>>
> >>>>> Hi Shira,
> >>>>>
> >>>>> I fit such models with the INLA package (https://www.r-inla.org/).
> The
> >>>>> trick is to define two random effects but force their parameter
> estimates
> >>>>> to be identical.
> >>>>>
> >>>>> The code would contain something like f(home, model = "iid")) +
> f(away,
> >>>>> copy = "home"). Meaning home ~ N(0, sigma_beta_i) and home[i] =
> away[i]
> >>>>>
> >>>>> Best regards,
> >>>>>
> >>>>> ir. Thierry Onkelinx
> >>>>> Statisticus / Statistician
> >>>>>
> >>>>> Vlaamse Overheid / Government of Flanders
> >>>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR
> NATURE
> >>>>> AND FOREST
> >>>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> >>>>> thierry.onkelinx at inbo.be
> >>>>> Havenlaan 88 bus 73, 1000 Brussel
> >>>>> www.inbo.be
> >>>>>
> >>>>>
> >>>>>
> ///////////////////////////////////////////////////////////////////////////////////////////
> >>>>> To call in the statistician after the experiment is done may be no
> more
> >>>>> than asking him to perform a post-mortem examination: he may be able
> to say
> >>>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> >>>>> The plural of anecdote is not data. ~ Roger Brinner
> >>>>> The combination of some data and an aching desire for an answer does
> >>>>> not ensure that a reasonable answer can be extracted from a given
> body of
> >>>>> data. ~ John Tukey
> >>>>>
> >>>>>
> ///////////////////////////////////////////////////////////////////////////////////////////
> >>>>>
> >>>>> <https://www.inbo.be>
> >>>>>
> >>>>>
> >>>>> Op vr 7 okt. 2022 om 15:00 schreef Shira Mitchell <
> shiraqotj at gmail.com
> >>>>>> :
> >>>>>
> >>>>>> We want to fit Bradley-Terry-style GLMM models in R. We looked into:
> >>>>>>
> >>>>>>
> >>>>>>
> https://cran.r-project.org/web/packages/BradleyTerry2/vignettes/BradleyTerry.pdf
> >>>>>> and
> >>>>>> http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html
> >>>>>>
> >>>>>> We have voter-specific variables x that influence which political
> >>>>>> message
> >>>>>> (i vs j) wins for them:
> >>>>>>
> >>>>>> logit[pr(i beats j | person with covariate x)] = lambda_i -
> lambda_j +
> >>>>>> (beta_i - beta_j) x
> >>>>>>
> >>>>>> We then model parameters as random effects:
> >>>>>> lambda_i ~ N(0, sigma_lambda)
> >>>>>> beta_i ~ N(0, sigma_beta)
> >>>>>>
> >>>>>> Is there a way to do this in R ? We do this in TensorFlow in Python
> by
> >>>>>> directly specifying design matrices with the 0,-1,1 or 0,-x,x
> entries.
> >>>>>> However, I do not see how to do this in R using lme4, BradleyTerry2,
> >>>>>> mgcv,
> >>>>>> etc.
> >>>>>>
> >>>>>> Thanks so much,
> >>>>>> Shira
> >>>>>>
> >>>>>>        [[alternative HTML version deleted]]
> >>>>>>
> >>>>>> _______________________________________________
> >>>>>> R-sig-mixed-models at r-project.org mailing list
> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>>>>>
> >>>>>
> >
> >        [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> The University of Edinburgh is a charitable body, registered in Scotland,
> with registration number SC005336. Is e buidheann carthannais a th? ann an
> Oilthigh Dh?n ?ideann, cl?raichte an Alba, ?ireamh cl?raidh SC005336.
>

	[[alternative HTML version deleted]]


From @h|r@qotj @end|ng |rom gm@||@com  Mon Oct 17 22:02:10 2022
From: @h|r@qotj @end|ng |rom gm@||@com (Shira Mitchell)
Date: Mon, 17 Oct 2022 16:02:10 -0400
Subject: [R-sig-ME] Bradley Terry GLMM in R ?
In-Reply-To: <CADeg_O12OrDgEMNQ6ejcV30evndEjAuFN-wKLdbN0K4GusyAQw@mail.gmail.com>
References: <CADeg_O3L_6Cmn_Wv0tExeCLccFJS1p5fottqprdh3+spDCrXxA@mail.gmail.com>
 <CAJuCY5ys+di7UNWKLSV8gw6qVC5yjWXVa4J7Bboe=cHAwoYDGw@mail.gmail.com>
 <CADeg_O0d+tuG0DnnnFw9o9VSh1dcMUbOitcT_rLbRcdbhNpzYw@mail.gmail.com>
 <CAJuCY5ysJUQQtYUCLF76iD5y9TmSiOD5OAfBt039RjVaD_pzwQ@mail.gmail.com>
 <CADeg_O0g7kTrhRUNhrDZjKt3HEzKpfOKZMW71uBaNTDoaNJt7g@mail.gmail.com>
 <CADeg_O0dzRyG=8u7Zjk94PbwGpoqnWoQRw+VRgpoq-BS2RimgA@mail.gmail.com>
 <E27AFF46-A57B-425D-A946-AE24E6567B7C@ed.ac.uk>
 <CADeg_O12OrDgEMNQ6ejcV30evndEjAuFN-wKLdbN0K4GusyAQw@mail.gmail.com>
Message-ID: <CADeg_O2p+pWAqyE-9pKaJ34icyGT1OVEwTZkUo8EZuf_68nmKA@mail.gmail.com>

Questions for Ben Bolker about the excellent GLMM FAQ:

Where does the hglm package fit into this very helpful table ?
https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#what-methods-are-available-to-fit-estimate-glmms

I wonder also about differences in model formula specifications, since some
packages (e.g. lme4) don't seem to accommodate Bradley-Terry, whereas some
packages (e.g. INLA, hglm, MCMCglmm) can accommodate.
https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#model-specification




On Mon, Oct 17, 2022 at 3:55 PM Shira Mitchell <shiraqotj at gmail.com> wrote:

> Thanks so much, Jarrod ! Not too late at all. Very interesting to compare
> MCMC with the approximations (INLA, hglm's extended quasi likelihood). I
> don't think I have the priors lined up yet across packages. The random
> effects seem more dispersed according to MCMCglmm than in INLA or hglm, but
> this could be due to priors not fit algorithm. Will look into the package
> prior defaults.
>
> On Mon, Oct 17, 2022 at 4:45 AM Jarrod Hadfield <j.hadfield at ed.ac.uk>
> wrote:
>
>> Hi Shira,
>>
>> Perhaps a little late to be useful, but MCMCglmm also fits random-effect
>> Bradley-Terry models. Just specify ~mm(opponent1-opponent2) in the random
>> effect formula. The mm stands for multimembership - the BT model is like a
>> multimembership model where some effects have been multiplied by -1, hence
>> the ?-' rather than ?+? in the mm model formula.
>>
>> Cheers,
>>
>> Jarrod
>>
>>
>> > On 16 Oct 2022, at 22:49, Shira Mitchell <shiraqotj at gmail.com> wrote:
>> >
>> > This email was sent to you by someone outside the University.
>> > You should only click on links or attachments if you are certain that
>> the email is genuine and the content is safe.
>> >
>> > Update: Dr Heather Turner <https://www.heatherturner.net/> (author of
>> > BradleyTerry2) suggested the hglm package, which unlike lme4 allows you
>> to
>> > specify generic design matrices (no longer constrained to lme4 formulas
>> !)
>> > Results look really similar to INLA so far. Yay !
>> >
>> > On Sun, Oct 16, 2022 at 2:19 PM Shira Mitchell <shiraqotj at gmail.com>
>> wrote:
>> >
>> >> Super helpful !  Thank you so much !
>> >>
>> >> Out of curiosity, is there a way to fit this type of Bradley-Terry
>> model
>> >> in lme4 ? lme4 formulas include random effect via syntax:
>> >> https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf
>> >> "(expr | factor). The expression expr is evaluated as a linear model
>> >> formula, producing a model matrix following the same rules used in
>> standard
>> >> R modeling functions (e.g., `lm` or `glm`). The expression factor is
>> >> evaluated as an `R` factor. One way to think about the vertical bar
>> >> operator is as a special kind of interaction between the model matrix
>> and
>> >> the grouping factor. This interaction ensures that the columns of the
>> model
>> >> matrix have different effects for each level of the grouping factor."
>> >>
>> >> So (expr | factor) is X_expr * alpha_factor.
>> >>
>> >> So naively writing ~ (1 | m_1) + (1 | m_2) is alpha_{m_1}^{(1)} +
>> >> alpha_{m_2}^{(2)}, twice as many parameters as what we want which is
>> >> alpha_{m_1} - alpha_{m_2}.
>> >>
>> >> But then see this stackexchange:
>> >>
>> >>
>> https://stats.stackexchange.com/questions/483833/opposing-effects-in-lme4-formulae-bradley-terry-model
>> >> "I could just make a design matrix, where player 1 gets the value 1,
>> and
>> >> player 2 gets the value ?1. However, unless I'm missing a trick, this
>> would
>> >> require having a separate column for each player, and plugging each
>> player
>> >> column's name into the formula"
>> >>
>> >> But suppose we create columns for all m = 1,...,M messages:
>> >>
>> >> A_m = 1 if m = m_1
>> >>           -1 if m = m_2
>> >>            0 otherwise
>> >>
>> >> I think then ~ (A_1 + ... + A_M  | m_1) is alpha_{m_1}^{(m_1)} -
>> >> alpha_{m_1}^{(m_2)}, also not what we would want.
>> >>
>> >> Back to INLA. Suppose we now want to add random message-specific slopes
>> >> for variable X_i in addition to random message-specific intercepts:
>> >>
>> >> P[i chooses m_1] = logit^-1 (beta_0 + (alpha_{m_1} - alpha_{m_2}) +
>> >> (beta_{m_1} - beta_{m_2})X_i)
>> >>
>> >> alpha_1,...,alpha_M ~ N(0,sigma_intercept)
>> >> beta_1,...,beta_M ~ N(0,sigma_slope)
>> >>
>> >> I see some resources about this, but nothing super comprehensive. Any
>> >> advice where to look for complete documentation ?
>> >>
>> >>
>> https://groups.google.com/g/r-inla-discussion-group/c/iQELaQF8M9Q/m/q7f4-W8YQksJ
>> >> (
>> >>
>> https://becarioprecario.bitbucket.io/inla-gitbook/ch-multilevel.html#multilevel-models-for-longitudinal-data
>> >> https://rpubs.com/corey_sparks/431920
>> >> https://avianecologist.com/2016/10/05/multilevel-models/
>> >>
>> >> Here is what we did:
>> >>
>> >> data$w_X = -data$X
>> >> data$m_1_beta = data$m_1
>> >> data$m_2_beta = data$m_2
>> >>
>> >> inla(depvar ~  f(m_1, model="iid", values = issues) +
>> >>                           f(m_2, w, copy = "m_1") +
>> >>                           f(m_1_beta, X, model="iid", values = issues)
>> +
>> >>                           f(m_2_beta, w_X, copy = "m_1_beta"),
>> >>                         family="binomial",
>> >>                         data=data)
>> >>
>> >>
>> >>
>> >>
>> >> On Mon, Oct 10, 2022 at 4:24 AM Thierry Onkelinx <
>> thierry.onkelinx at inbo.be>
>> >> wrote:
>> >>
>> >>> Dear Shira,
>> >>>
>> >>> - in a formula object means remove that object from the formula. Use a
>> >>> weight of -1 instead.
>> >>>
>> >>> f(home, model = "iid")) + f(away, w = -1, copy = "home")
>> >>>
>> >>> Best regards,
>> >>>
>> >>> ir. Thierry Onkelinx
>> >>> Statisticus / Statistician
>> >>>
>> >>> Vlaamse Overheid / Government of Flanders
>> >>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>> >>> AND FOREST
>> >>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>> >>> thierry.onkelinx at inbo.be
>> >>> Havenlaan 88 bus 73, 1000 Brussel
>> >>> www.inbo.be
>> >>>
>> >>>
>> >>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>> >>> To call in the statistician after the experiment is done may be no
>> more
>> >>> than asking him to perform a post-mortem examination: he may be able
>> to say
>> >>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> >>> The plural of anecdote is not data. ~ Roger Brinner
>> >>> The combination of some data and an aching desire for an answer does
>> not
>> >>> ensure that a reasonable answer can be extracted from a given body of
>> data.
>> >>> ~ John Tukey
>> >>>
>> >>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>> >>>
>> >>> <https://www.inbo.be>
>> >>>
>> >>>
>> >>> Op vr 7 okt. 2022 om 23:36 schreef Shira Mitchell <
>> shiraqotj at gmail.com>:
>> >>>
>> >>>> Thanks so much, Thierry ! This is great.
>> >>>>
>> >>>> This works except that I cannot subtract because:
>> >>>> f(home, model = "iid")) - f(away, copy = "home")
>> >>>>
>> >>>> just drops the second term. Apologies that I'm not super familiar
>> with
>> >>>> INLA syntax yet.
>> >>>>
>> >>>>
>> >>>>
>> >>>> On Fri, Oct 7, 2022 at 10:19 AM Thierry Onkelinx <
>> >>>> thierry.onkelinx at inbo.be> wrote:
>> >>>>
>> >>>>> Hi Shira,
>> >>>>>
>> >>>>> I fit such models with the INLA package (https://www.r-inla.org/).
>> The
>> >>>>> trick is to define two random effects but force their parameter
>> estimates
>> >>>>> to be identical.
>> >>>>>
>> >>>>> The code would contain something like f(home, model = "iid")) +
>> f(away,
>> >>>>> copy = "home"). Meaning home ~ N(0, sigma_beta_i) and home[i] =
>> away[i]
>> >>>>>
>> >>>>> Best regards,
>> >>>>>
>> >>>>> ir. Thierry Onkelinx
>> >>>>> Statisticus / Statistician
>> >>>>>
>> >>>>> Vlaamse Overheid / Government of Flanders
>> >>>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR
>> NATURE
>> >>>>> AND FOREST
>> >>>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality
>> Assurance
>> >>>>> thierry.onkelinx at inbo.be
>> >>>>> Havenlaan 88 bus 73, 1000 Brussel
>> >>>>> www.inbo.be
>> >>>>>
>> >>>>>
>> >>>>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>> >>>>> To call in the statistician after the experiment is done may be no
>> more
>> >>>>> than asking him to perform a post-mortem examination: he may be
>> able to say
>> >>>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> >>>>> The plural of anecdote is not data. ~ Roger Brinner
>> >>>>> The combination of some data and an aching desire for an answer does
>> >>>>> not ensure that a reasonable answer can be extracted from a given
>> body of
>> >>>>> data. ~ John Tukey
>> >>>>>
>> >>>>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>> >>>>>
>> >>>>> <https://www.inbo.be>
>> >>>>>
>> >>>>>
>> >>>>> Op vr 7 okt. 2022 om 15:00 schreef Shira Mitchell <
>> shiraqotj at gmail.com
>> >>>>>> :
>> >>>>>
>> >>>>>> We want to fit Bradley-Terry-style GLMM models in R. We looked
>> into:
>> >>>>>>
>> >>>>>>
>> >>>>>>
>> https://cran.r-project.org/web/packages/BradleyTerry2/vignettes/BradleyTerry.pdf
>> >>>>>> and
>> >>>>>> http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html
>> >>>>>>
>> >>>>>> We have voter-specific variables x that influence which political
>> >>>>>> message
>> >>>>>> (i vs j) wins for them:
>> >>>>>>
>> >>>>>> logit[pr(i beats j | person with covariate x)] = lambda_i -
>> lambda_j +
>> >>>>>> (beta_i - beta_j) x
>> >>>>>>
>> >>>>>> We then model parameters as random effects:
>> >>>>>> lambda_i ~ N(0, sigma_lambda)
>> >>>>>> beta_i ~ N(0, sigma_beta)
>> >>>>>>
>> >>>>>> Is there a way to do this in R ? We do this in TensorFlow in
>> Python by
>> >>>>>> directly specifying design matrices with the 0,-1,1 or 0,-x,x
>> entries.
>> >>>>>> However, I do not see how to do this in R using lme4,
>> BradleyTerry2,
>> >>>>>> mgcv,
>> >>>>>> etc.
>> >>>>>>
>> >>>>>> Thanks so much,
>> >>>>>> Shira
>> >>>>>>
>> >>>>>>        [[alternative HTML version deleted]]
>> >>>>>>
>> >>>>>> _______________________________________________
>> >>>>>> R-sig-mixed-models at r-project.org mailing list
>> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >>>>>>
>> >>>>>
>> >
>> >        [[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>> The University of Edinburgh is a charitable body, registered in Scotland,
>> with registration number SC005336. Is e buidheann carthannais a th? ann an
>> Oilthigh Dh?n ?ideann, cl?raichte an Alba, ?ireamh cl?raidh SC005336.
>>
>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Wed Oct 19 21:43:02 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 19 Oct 2022 15:43:02 -0400
Subject: [R-sig-ME] Bradley Terry GLMM in R ?
In-Reply-To: <CADeg_O2p+pWAqyE-9pKaJ34icyGT1OVEwTZkUo8EZuf_68nmKA@mail.gmail.com>
References: <CADeg_O3L_6Cmn_Wv0tExeCLccFJS1p5fottqprdh3+spDCrXxA@mail.gmail.com>
 <CAJuCY5ys+di7UNWKLSV8gw6qVC5yjWXVa4J7Bboe=cHAwoYDGw@mail.gmail.com>
 <CADeg_O0d+tuG0DnnnFw9o9VSh1dcMUbOitcT_rLbRcdbhNpzYw@mail.gmail.com>
 <CAJuCY5ysJUQQtYUCLF76iD5y9TmSiOD5OAfBt039RjVaD_pzwQ@mail.gmail.com>
 <CADeg_O0g7kTrhRUNhrDZjKt3HEzKpfOKZMW71uBaNTDoaNJt7g@mail.gmail.com>
 <CADeg_O0dzRyG=8u7Zjk94PbwGpoqnWoQRw+VRgpoq-BS2RimgA@mail.gmail.com>
 <E27AFF46-A57B-425D-A946-AE24E6567B7C@ed.ac.uk>
 <CADeg_O12OrDgEMNQ6ejcV30evndEjAuFN-wKLdbN0K4GusyAQw@mail.gmail.com>
 <CADeg_O2p+pWAqyE-9pKaJ34icyGT1OVEwTZkUo8EZuf_68nmKA@mail.gmail.com>
Message-ID: <da7029dd-1277-dea6-c359-ecab30c0ca7e@gmail.com>



On 2022-10-17 4:02 p.m., Shira Mitchell wrote:
> Questions for Ben Bolker about the excellent GLMM FAQ:
> 
> Where does the hglm package fit into this very helpful table ?
> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#what-methods-are-available-to-fit-estimate-glmms

   hglm uses a different *definition of likelihood*, so it's essentially 
fitting a different model altogether (although one that will generally 
get similar answers to other approaches).  Unfortunately, figuring out 
the exact meaning and properties of h-likelihood will take you down a 
rabbit hole ... (see refs below)
> 
> I wonder also about differences in model formula specifications, since some
> packages (e.g. lme4) don't seem to accommodate Bradley-Terry, whereas some
> packages (e.g. INLA, hglm, MCMCglmm) can accommodate.
> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#model-specification

    This is less conceptually difficult.  There's a big difference 
between the models implemented within a package *could* fit, and what 
the interfaces will allow.  For example, the machinery in lme4 *can* fit 
any model with exponential-family conditional distribution and 
multivariate Gaussian random effects (on the log scale). However, the 
interface doesn't allow a lot of flexibility to specify the X and Z 
matrices, which is what you would need in order to do Bradley-Terry 
models.  There are extension packages such as 
https://github.com/jvparidon/lmerMultiMember (which allow 
multi-membership models and hence might be extended to do Bradley-Terry 
models), gamm4 (on CRAN, imports machinery from mgcv to set up Z 
matrices corresponding to penalized regression terms in the model), etc.


Meng, Xiao-Li. ?Decoding the H-Likelihood.? Statistical Science 24, no. 
3 (August 1, 2009). https://doi.org/10.1214/09-STS277C.

Meng, Xiao?Li. ?What?s the H in H?likelihood: A Holy Grail or an 
Achilles? Heel?? In Bayesian Statistics 9, edited by Jos? M. Bernardo, 
M. J. Bayarri, James O. Berger, A. P. Dawid, David Heckerman, Adrian F. 
M. Smith, and Mike West, 0. Oxford University Press, 2011. 
https://doi.org/10.1093/acprof:oso/9780199694587.003.0016.

Jin, Shaobo, and Youngjo Lee. ?A Review of H-Likelihood and Hierarchical 
Generalized Linear Model.? WIREs Computational Statistics 13, no. 5 
(2021): e1527. https://doi.org/10.1002/wics.1527.


> 
> 
> 
> 
> On Mon, Oct 17, 2022 at 3:55 PM Shira Mitchell <shiraqotj at gmail.com> wrote:
> 
>> Thanks so much, Jarrod ! Not too late at all. Very interesting to compare
>> MCMC with the approximations (INLA, hglm's extended quasi likelihood). I
>> don't think I have the priors lined up yet across packages. The random
>> effects seem more dispersed according to MCMCglmm than in INLA or hglm, but
>> this could be due to priors not fit algorithm. Will look into the package
>> prior defaults.
>>
>> On Mon, Oct 17, 2022 at 4:45 AM Jarrod Hadfield <j.hadfield at ed.ac.uk>
>> wrote:
>>
>>> Hi Shira,
>>>
>>> Perhaps a little late to be useful, but MCMCglmm also fits random-effect
>>> Bradley-Terry models. Just specify ~mm(opponent1-opponent2) in the random
>>> effect formula. The mm stands for multimembership - the BT model is like a
>>> multimembership model where some effects have been multiplied by -1, hence
>>> the ?-' rather than ?+? in the mm model formula.
>>>
>>> Cheers,
>>>
>>> Jarrod
>>>
>>>
>>>> On 16 Oct 2022, at 22:49, Shira Mitchell <shiraqotj at gmail.com> wrote:
>>>>
>>>> This email was sent to you by someone outside the University.
>>>> You should only click on links or attachments if you are certain that
>>> the email is genuine and the content is safe.
>>>>
>>>> Update: Dr Heather Turner <https://www.heatherturner.net/> (author of
>>>> BradleyTerry2) suggested the hglm package, which unlike lme4 allows you
>>> to
>>>> specify generic design matrices (no longer constrained to lme4 formulas
>>> !)
>>>> Results look really similar to INLA so far. Yay !
>>>>
>>>> On Sun, Oct 16, 2022 at 2:19 PM Shira Mitchell <shiraqotj at gmail.com>
>>> wrote:
>>>>
>>>>> Super helpful !  Thank you so much !
>>>>>
>>>>> Out of curiosity, is there a way to fit this type of Bradley-Terry
>>> model
>>>>> in lme4 ? lme4 formulas include random effect via syntax:
>>>>> https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf
>>>>> "(expr | factor). The expression expr is evaluated as a linear model
>>>>> formula, producing a model matrix following the same rules used in
>>> standard
>>>>> R modeling functions (e.g., `lm` or `glm`). The expression factor is
>>>>> evaluated as an `R` factor. One way to think about the vertical bar
>>>>> operator is as a special kind of interaction between the model matrix
>>> and
>>>>> the grouping factor. This interaction ensures that the columns of the
>>> model
>>>>> matrix have different effects for each level of the grouping factor."
>>>>>
>>>>> So (expr | factor) is X_expr * alpha_factor.
>>>>>
>>>>> So naively writing ~ (1 | m_1) + (1 | m_2) is alpha_{m_1}^{(1)} +
>>>>> alpha_{m_2}^{(2)}, twice as many parameters as what we want which is
>>>>> alpha_{m_1} - alpha_{m_2}.
>>>>>
>>>>> But then see this stackexchange:
>>>>>
>>>>>
>>> https://stats.stackexchange.com/questions/483833/opposing-effects-in-lme4-formulae-bradley-terry-model
>>>>> "I could just make a design matrix, where player 1 gets the value 1,
>>> and
>>>>> player 2 gets the value ?1. However, unless I'm missing a trick, this
>>> would
>>>>> require having a separate column for each player, and plugging each
>>> player
>>>>> column's name into the formula"
>>>>>
>>>>> But suppose we create columns for all m = 1,...,M messages:
>>>>>
>>>>> A_m = 1 if m = m_1
>>>>>            -1 if m = m_2
>>>>>             0 otherwise
>>>>>
>>>>> I think then ~ (A_1 + ... + A_M  | m_1) is alpha_{m_1}^{(m_1)} -
>>>>> alpha_{m_1}^{(m_2)}, also not what we would want.
>>>>>
>>>>> Back to INLA. Suppose we now want to add random message-specific slopes
>>>>> for variable X_i in addition to random message-specific intercepts:
>>>>>
>>>>> P[i chooses m_1] = logit^-1 (beta_0 + (alpha_{m_1} - alpha_{m_2}) +
>>>>> (beta_{m_1} - beta_{m_2})X_i)
>>>>>
>>>>> alpha_1,...,alpha_M ~ N(0,sigma_intercept)
>>>>> beta_1,...,beta_M ~ N(0,sigma_slope)
>>>>>
>>>>> I see some resources about this, but nothing super comprehensive. Any
>>>>> advice where to look for complete documentation ?
>>>>>
>>>>>
>>> https://groups.google.com/g/r-inla-discussion-group/c/iQELaQF8M9Q/m/q7f4-W8YQksJ
>>>>> (
>>>>>
>>> https://becarioprecario.bitbucket.io/inla-gitbook/ch-multilevel.html#multilevel-models-for-longitudinal-data
>>>>> https://rpubs.com/corey_sparks/431920
>>>>> https://avianecologist.com/2016/10/05/multilevel-models/
>>>>>
>>>>> Here is what we did:
>>>>>
>>>>> data$w_X = -data$X
>>>>> data$m_1_beta = data$m_1
>>>>> data$m_2_beta = data$m_2
>>>>>
>>>>> inla(depvar ~  f(m_1, model="iid", values = issues) +
>>>>>                            f(m_2, w, copy = "m_1") +
>>>>>                            f(m_1_beta, X, model="iid", values = issues)
>>> +
>>>>>                            f(m_2_beta, w_X, copy = "m_1_beta"),
>>>>>                          family="binomial",
>>>>>                          data=data)
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> On Mon, Oct 10, 2022 at 4:24 AM Thierry Onkelinx <
>>> thierry.onkelinx at inbo.be>
>>>>> wrote:
>>>>>
>>>>>> Dear Shira,
>>>>>>
>>>>>> - in a formula object means remove that object from the formula. Use a
>>>>>> weight of -1 instead.
>>>>>>
>>>>>> f(home, model = "iid")) + f(away, w = -1, copy = "home")
>>>>>>
>>>>>> Best regards,
>>>>>>
>>>>>> ir. Thierry Onkelinx
>>>>>> Statisticus / Statistician
>>>>>>
>>>>>> Vlaamse Overheid / Government of Flanders
>>>>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>>>>>> AND FOREST
>>>>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>>>>>> thierry.onkelinx at inbo.be
>>>>>> Havenlaan 88 bus 73, 1000 Brussel
>>>>>> www.inbo.be
>>>>>>
>>>>>>
>>>>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>>> To call in the statistician after the experiment is done may be no
>>> more
>>>>>> than asking him to perform a post-mortem examination: he may be able
>>> to say
>>>>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>>>>> The plural of anecdote is not data. ~ Roger Brinner
>>>>>> The combination of some data and an aching desire for an answer does
>>> not
>>>>>> ensure that a reasonable answer can be extracted from a given body of
>>> data.
>>>>>> ~ John Tukey
>>>>>>
>>>>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>>>
>>>>>> <https://www.inbo.be>
>>>>>>
>>>>>>
>>>>>> Op vr 7 okt. 2022 om 23:36 schreef Shira Mitchell <
>>> shiraqotj at gmail.com>:
>>>>>>
>>>>>>> Thanks so much, Thierry ! This is great.
>>>>>>>
>>>>>>> This works except that I cannot subtract because:
>>>>>>> f(home, model = "iid")) - f(away, copy = "home")
>>>>>>>
>>>>>>> just drops the second term. Apologies that I'm not super familiar
>>> with
>>>>>>> INLA syntax yet.
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> On Fri, Oct 7, 2022 at 10:19 AM Thierry Onkelinx <
>>>>>>> thierry.onkelinx at inbo.be> wrote:
>>>>>>>
>>>>>>>> Hi Shira,
>>>>>>>>
>>>>>>>> I fit such models with the INLA package (https://www.r-inla.org/).
>>> The
>>>>>>>> trick is to define two random effects but force their parameter
>>> estimates
>>>>>>>> to be identical.
>>>>>>>>
>>>>>>>> The code would contain something like f(home, model = "iid")) +
>>> f(away,
>>>>>>>> copy = "home"). Meaning home ~ N(0, sigma_beta_i) and home[i] =
>>> away[i]
>>>>>>>>
>>>>>>>> Best regards,
>>>>>>>>
>>>>>>>> ir. Thierry Onkelinx
>>>>>>>> Statisticus / Statistician
>>>>>>>>
>>>>>>>> Vlaamse Overheid / Government of Flanders
>>>>>>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR
>>> NATURE
>>>>>>>> AND FOREST
>>>>>>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality
>>> Assurance
>>>>>>>> thierry.onkelinx at inbo.be
>>>>>>>> Havenlaan 88 bus 73, 1000 Brussel
>>>>>>>> www.inbo.be
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>>>>> To call in the statistician after the experiment is done may be no
>>> more
>>>>>>>> than asking him to perform a post-mortem examination: he may be
>>> able to say
>>>>>>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>>>>>>> The plural of anecdote is not data. ~ Roger Brinner
>>>>>>>> The combination of some data and an aching desire for an answer does
>>>>>>>> not ensure that a reasonable answer can be extracted from a given
>>> body of
>>>>>>>> data. ~ John Tukey
>>>>>>>>
>>>>>>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>>>>>
>>>>>>>> <https://www.inbo.be>
>>>>>>>>
>>>>>>>>
>>>>>>>> Op vr 7 okt. 2022 om 15:00 schreef Shira Mitchell <
>>> shiraqotj at gmail.com
>>>>>>>>> :
>>>>>>>>
>>>>>>>>> We want to fit Bradley-Terry-style GLMM models in R. We looked
>>> into:
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>> https://cran.r-project.org/web/packages/BradleyTerry2/vignettes/BradleyTerry.pdf
>>>>>>>>> and
>>>>>>>>> http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html
>>>>>>>>>
>>>>>>>>> We have voter-specific variables x that influence which political
>>>>>>>>> message
>>>>>>>>> (i vs j) wins for them:
>>>>>>>>>
>>>>>>>>> logit[pr(i beats j | person with covariate x)] = lambda_i -
>>> lambda_j +
>>>>>>>>> (beta_i - beta_j) x
>>>>>>>>>
>>>>>>>>> We then model parameters as random effects:
>>>>>>>>> lambda_i ~ N(0, sigma_lambda)
>>>>>>>>> beta_i ~ N(0, sigma_beta)
>>>>>>>>>
>>>>>>>>> Is there a way to do this in R ? We do this in TensorFlow in
>>> Python by
>>>>>>>>> directly specifying design matrices with the 0,-1,1 or 0,-x,x
>>> entries.
>>>>>>>>> However, I do not see how to do this in R using lme4,
>>> BradleyTerry2,
>>>>>>>>> mgcv,
>>>>>>>>> etc.
>>>>>>>>>
>>>>>>>>> Thanks so much,
>>>>>>>>> Shira
>>>>>>>>>
>>>>>>>>>         [[alternative HTML version deleted]]
>>>>>>>>>
>>>>>>>>> _______________________________________________
>>>>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>>>>
>>>>>>>>
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>> The University of Edinburgh is a charitable body, registered in Scotland,
>>> with registration number SC005336. Is e buidheann carthannais a th? ann an
>>> Oilthigh Dh?n ?ideann, cl?raichte an Alba, ?ireamh cl?raidh SC005336.
>>>
>>
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics
> E-mail is sent at my convenience; I don't expect replies outside of 
working hours.


From v@np@r|don @end|ng |rom w|@c@edu  Thu Oct 20 08:42:36 2022
From: v@np@r|don @end|ng |rom w|@c@edu (Jeroen van Paridon)
Date: Thu, 20 Oct 2022 06:42:36 +0000
Subject: [R-sig-ME] Bradley Terry GLMM in R ?
In-Reply-To: <da7029dd-1277-dea6-c359-ecab30c0ca7e@gmail.com>
References: <CADeg_O3L_6Cmn_Wv0tExeCLccFJS1p5fottqprdh3+spDCrXxA@mail.gmail.com>
 <CAJuCY5ys+di7UNWKLSV8gw6qVC5yjWXVa4J7Bboe=cHAwoYDGw@mail.gmail.com>
 <CADeg_O0d+tuG0DnnnFw9o9VSh1dcMUbOitcT_rLbRcdbhNpzYw@mail.gmail.com>
 <CAJuCY5ysJUQQtYUCLF76iD5y9TmSiOD5OAfBt039RjVaD_pzwQ@mail.gmail.com>
 <CADeg_O0g7kTrhRUNhrDZjKt3HEzKpfOKZMW71uBaNTDoaNJt7g@mail.gmail.com>
 <CADeg_O0dzRyG=8u7Zjk94PbwGpoqnWoQRw+VRgpoq-BS2RimgA@mail.gmail.com>
 <E27AFF46-A57B-425D-A946-AE24E6567B7C@ed.ac.uk>
 <CADeg_O12OrDgEMNQ6ejcV30evndEjAuFN-wKLdbN0K4GusyAQw@mail.gmail.com>
 <CADeg_O2p+pWAqyE-9pKaJ34icyGT1OVEwTZkUo8EZuf_68nmKA@mail.gmail.com>
 <da7029dd-1277-dea6-c359-ecab30c0ca7e@gmail.com>
Message-ID: <DM6PR06MB41716CEDD0B1124D582E5142B82A9@DM6PR06MB4171.namprd06.prod.outlook.com>

Hi,

Just to expand on Ben's last email: In principle, lmerMultiMember allows you to pass arbitrary indicator/weight matrices for the random effects to lme4 for model fitting as long as they have the correct shape. The package contains helper functions for generating more conventional multiple membership matrices since that was my own use-case, but if you create your own matrix with opposed (1 and -1) weights I see no reason why it shouldn't work.

Membership matrices need to be sparse matrices of class Matrix::dgCMatrix and shape n_groups x n_obs. You can probably just take whatever indicator matrix you already have, transpose it, and then cast it to the sparse format.

If you're going this route and run into any issues, feel free to reach out to me, directly.


Cheers,

JP van Paridon (he/him)
?Research Associate, Lupyan Lab
University of Wisconsin-Madison
https://github.com/jvparidon
________________________________
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on behalf of Ben Bolker <bbolker at gmail.com>
Sent: Wednesday, October 19, 2022 2:43 PM
To: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Bradley Terry GLMM in R ?



On 2022-10-17 4:02 p.m., Shira Mitchell wrote:
> Questions for Ben Bolker about the excellent GLMM FAQ:
>
> Where does the hglm package fit into this very helpful table ?
> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#what-methods-are-available-to-fit-estimate-glmms

   hglm uses a different *definition of likelihood*, so it's essentially
fitting a different model altogether (although one that will generally
get similar answers to other approaches).  Unfortunately, figuring out
the exact meaning and properties of h-likelihood will take you down a
rabbit hole ... (see refs below)
>
> I wonder also about differences in model formula specifications, since some
> packages (e.g. lme4) don't seem to accommodate Bradley-Terry, whereas some
> packages (e.g. INLA, hglm, MCMCglmm) can accommodate.
> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#model-specification

    This is less conceptually difficult.  There's a big difference
between the models implemented within a package *could* fit, and what
the interfaces will allow.  For example, the machinery in lme4 *can* fit
any model with exponential-family conditional distribution and
multivariate Gaussian random effects (on the log scale). However, the
interface doesn't allow a lot of flexibility to specify the X and Z
matrices, which is what you would need in order to do Bradley-Terry
models.  There are extension packages such as
https://github.com/jvparidon/lmerMultiMember (which allow
multi-membership models and hence might be extended to do Bradley-Terry
models), gamm4 (on CRAN, imports machinery from mgcv to set up Z
matrices corresponding to penalized regression terms in the model), etc.


Meng, Xiao-Li. ?Decoding the H-Likelihood.? Statistical Science 24, no.
3 (August 1, 2009). https://doi.org/10.1214/09-STS277C.

Meng, Xiao?Li. ?What?s the H in H?likelihood: A Holy Grail or an
Achilles? Heel?? In Bayesian Statistics 9, edited by Jos? M. Bernardo,
M. J. Bayarri, James O. Berger, A. P. Dawid, David Heckerman, Adrian F.
M. Smith, and Mike West, 0. Oxford University Press, 2011.
https://doi.org/10.1093/acprof:oso/9780199694587.003.0016.

Jin, Shaobo, and Youngjo Lee. ?A Review of H-Likelihood and Hierarchical
Generalized Linear Model.? WIREs Computational Statistics 13, no. 5
(2021): e1527. https://doi.org/10.1002/wics.1527.


>
>
>
>
> On Mon, Oct 17, 2022 at 3:55 PM Shira Mitchell <shiraqotj at gmail.com> wrote:
>
>> Thanks so much, Jarrod ! Not too late at all. Very interesting to compare
>> MCMC with the approximations (INLA, hglm's extended quasi likelihood). I
>> don't think I have the priors lined up yet across packages. The random
>> effects seem more dispersed according to MCMCglmm than in INLA or hglm, but
>> this could be due to priors not fit algorithm. Will look into the package
>> prior defaults.
>>
>> On Mon, Oct 17, 2022 at 4:45 AM Jarrod Hadfield <j.hadfield at ed.ac.uk>
>> wrote:
>>
>>> Hi Shira,
>>>
>>> Perhaps a little late to be useful, but MCMCglmm also fits random-effect
>>> Bradley-Terry models. Just specify ~mm(opponent1-opponent2) in the random
>>> effect formula. The mm stands for multimembership - the BT model is like a
>>> multimembership model where some effects have been multiplied by -1, hence
>>> the ?-' rather than ?+? in the mm model formula.
>>>
>>> Cheers,
>>>
>>> Jarrod
>>>
>>>
>>>> On 16 Oct 2022, at 22:49, Shira Mitchell <shiraqotj at gmail.com> wrote:
>>>>
>>>> This email was sent to you by someone outside the University.
>>>> You should only click on links or attachments if you are certain that
>>> the email is genuine and the content is safe.
>>>>
>>>> Update: Dr Heather Turner <https://www.heatherturner.net/> (author of
>>>> BradleyTerry2) suggested the hglm package, which unlike lme4 allows you
>>> to
>>>> specify generic design matrices (no longer constrained to lme4 formulas
>>> !)
>>>> Results look really similar to INLA so far. Yay !
>>>>
>>>> On Sun, Oct 16, 2022 at 2:19 PM Shira Mitchell <shiraqotj at gmail.com>
>>> wrote:
>>>>
>>>>> Super helpful !  Thank you so much !
>>>>>
>>>>> Out of curiosity, is there a way to fit this type of Bradley-Terry
>>> model
>>>>> in lme4 ? lme4 formulas include random effect via syntax:
>>>>> https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf
>>>>> "(expr | factor). The expression expr is evaluated as a linear model
>>>>> formula, producing a model matrix following the same rules used in
>>> standard
>>>>> R modeling functions (e.g., `lm` or `glm`). The expression factor is
>>>>> evaluated as an `R` factor. One way to think about the vertical bar
>>>>> operator is as a special kind of interaction between the model matrix
>>> and
>>>>> the grouping factor. This interaction ensures that the columns of the
>>> model
>>>>> matrix have different effects for each level of the grouping factor."
>>>>>
>>>>> So (expr | factor) is X_expr * alpha_factor.
>>>>>
>>>>> So naively writing ~ (1 | m_1) + (1 | m_2) is alpha_{m_1}^{(1)} +
>>>>> alpha_{m_2}^{(2)}, twice as many parameters as what we want which is
>>>>> alpha_{m_1} - alpha_{m_2}.
>>>>>
>>>>> But then see this stackexchange:
>>>>>
>>>>>
>>> https://stats.stackexchange.com/questions/483833/opposing-effects-in-lme4-formulae-bradley-terry-model
>>>>> "I could just make a design matrix, where player 1 gets the value 1,
>>> and
>>>>> player 2 gets the value ?1. However, unless I'm missing a trick, this
>>> would
>>>>> require having a separate column for each player, and plugging each
>>> player
>>>>> column's name into the formula"
>>>>>
>>>>> But suppose we create columns for all m = 1,...,M messages:
>>>>>
>>>>> A_m = 1 if m = m_1
>>>>>            -1 if m = m_2
>>>>>             0 otherwise
>>>>>
>>>>> I think then ~ (A_1 + ... + A_M  | m_1) is alpha_{m_1}^{(m_1)} -
>>>>> alpha_{m_1}^{(m_2)}, also not what we would want.
>>>>>
>>>>> Back to INLA. Suppose we now want to add random message-specific slopes
>>>>> for variable X_i in addition to random message-specific intercepts:
>>>>>
>>>>> P[i chooses m_1] = logit^-1 (beta_0 + (alpha_{m_1} - alpha_{m_2}) +
>>>>> (beta_{m_1} - beta_{m_2})X_i)
>>>>>
>>>>> alpha_1,...,alpha_M ~ N(0,sigma_intercept)
>>>>> beta_1,...,beta_M ~ N(0,sigma_slope)
>>>>>
>>>>> I see some resources about this, but nothing super comprehensive. Any
>>>>> advice where to look for complete documentation ?
>>>>>
>>>>>
>>> https://groups.google.com/g/r-inla-discussion-group/c/iQELaQF8M9Q/m/q7f4-W8YQksJ
>>>>> (
>>>>>
>>> https://becarioprecario.bitbucket.io/inla-gitbook/ch-multilevel.html#multilevel-models-for-longitudinal-data
>>>>> https://rpubs.com/corey_sparks/431920
>>>>> https://avianecologist.com/2016/10/05/multilevel-models/
>>>>>
>>>>> Here is what we did:
>>>>>
>>>>> data$w_X = -data$X
>>>>> data$m_1_beta = data$m_1
>>>>> data$m_2_beta = data$m_2
>>>>>
>>>>> inla(depvar ~  f(m_1, model="iid", values = issues) +
>>>>>                            f(m_2, w, copy = "m_1") +
>>>>>                            f(m_1_beta, X, model="iid", values = issues)
>>> +
>>>>>                            f(m_2_beta, w_X, copy = "m_1_beta"),
>>>>>                          family="binomial",
>>>>>                          data=data)
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> On Mon, Oct 10, 2022 at 4:24 AM Thierry Onkelinx <
>>> thierry.onkelinx at inbo.be>
>>>>> wrote:
>>>>>
>>>>>> Dear Shira,
>>>>>>
>>>>>> - in a formula object means remove that object from the formula. Use a
>>>>>> weight of -1 instead.
>>>>>>
>>>>>> f(home, model = "iid")) + f(away, w = -1, copy = "home")
>>>>>>
>>>>>> Best regards,
>>>>>>
>>>>>> ir. Thierry Onkelinx
>>>>>> Statisticus / Statistician
>>>>>>
>>>>>> Vlaamse Overheid / Government of Flanders
>>>>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>>>>>> AND FOREST
>>>>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>>>>>> thierry.onkelinx at inbo.be
>>>>>> Havenlaan 88 bus 73, 1000 Brussel
>>>>>> www.inbo.be<http://www.inbo.be>
>>>>>>
>>>>>>
>>>>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>>> To call in the statistician after the experiment is done may be no
>>> more
>>>>>> than asking him to perform a post-mortem examination: he may be able
>>> to say
>>>>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>>>>> The plural of anecdote is not data. ~ Roger Brinner
>>>>>> The combination of some data and an aching desire for an answer does
>>> not
>>>>>> ensure that a reasonable answer can be extracted from a given body of
>>> data.
>>>>>> ~ John Tukey
>>>>>>
>>>>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>>>
>>>>>> <https://www.inbo.be>
>>>>>>
>>>>>>
>>>>>> Op vr 7 okt. 2022 om 23:36 schreef Shira Mitchell <
>>> shiraqotj at gmail.com>:
>>>>>>
>>>>>>> Thanks so much, Thierry ! This is great.
>>>>>>>
>>>>>>> This works except that I cannot subtract because:
>>>>>>> f(home, model = "iid")) - f(away, copy = "home")
>>>>>>>
>>>>>>> just drops the second term. Apologies that I'm not super familiar
>>> with
>>>>>>> INLA syntax yet.
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> On Fri, Oct 7, 2022 at 10:19 AM Thierry Onkelinx <
>>>>>>> thierry.onkelinx at inbo.be> wrote:
>>>>>>>
>>>>>>>> Hi Shira,
>>>>>>>>
>>>>>>>> I fit such models with the INLA package (https://www.r-inla.org/).
>>> The
>>>>>>>> trick is to define two random effects but force their parameter
>>> estimates
>>>>>>>> to be identical.
>>>>>>>>
>>>>>>>> The code would contain something like f(home, model = "iid")) +
>>> f(away,
>>>>>>>> copy = "home"). Meaning home ~ N(0, sigma_beta_i) and home[i] =
>>> away[i]
>>>>>>>>
>>>>>>>> Best regards,
>>>>>>>>
>>>>>>>> ir. Thierry Onkelinx
>>>>>>>> Statisticus / Statistician
>>>>>>>>
>>>>>>>> Vlaamse Overheid / Government of Flanders
>>>>>>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR
>>> NATURE
>>>>>>>> AND FOREST
>>>>>>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality
>>> Assurance
>>>>>>>> thierry.onkelinx at inbo.be
>>>>>>>> Havenlaan 88 bus 73, 1000 Brussel
>>>>>>>> www.inbo.be<http://www.inbo.be>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>>>>> To call in the statistician after the experiment is done may be no
>>> more
>>>>>>>> than asking him to perform a post-mortem examination: he may be
>>> able to say
>>>>>>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>>>>>>> The plural of anecdote is not data. ~ Roger Brinner
>>>>>>>> The combination of some data and an aching desire for an answer does
>>>>>>>> not ensure that a reasonable answer can be extracted from a given
>>> body of
>>>>>>>> data. ~ John Tukey
>>>>>>>>
>>>>>>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>>>>>
>>>>>>>> <https://www.inbo.be>
>>>>>>>>
>>>>>>>>
>>>>>>>> Op vr 7 okt. 2022 om 15:00 schreef Shira Mitchell <
>>> shiraqotj at gmail.com
>>>>>>>>> :
>>>>>>>>
>>>>>>>>> We want to fit Bradley-Terry-style GLMM models in R. We looked
>>> into:
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>> https://cran.r-project.org/web/packages/BradleyTerry2/vignettes/BradleyTerry.pdf
>>>>>>>>> and
>>>>>>>>> http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html
>>>>>>>>>
>>>>>>>>> We have voter-specific variables x that influence which political
>>>>>>>>> message
>>>>>>>>> (i vs j) wins for them:
>>>>>>>>>
>>>>>>>>> logit[pr(i beats j | person with covariate x)] = lambda_i -
>>> lambda_j +
>>>>>>>>> (beta_i - beta_j) x
>>>>>>>>>
>>>>>>>>> We then model parameters as random effects:
>>>>>>>>> lambda_i ~ N(0, sigma_lambda)
>>>>>>>>> beta_i ~ N(0, sigma_beta)
>>>>>>>>>
>>>>>>>>> Is there a way to do this in R ? We do this in TensorFlow in
>>> Python by
>>>>>>>>> directly specifying design matrices with the 0,-1,1 or 0,-x,x
>>> entries.
>>>>>>>>> However, I do not see how to do this in R using lme4,
>>> BradleyTerry2,
>>>>>>>>> mgcv,
>>>>>>>>> etc.
>>>>>>>>>
>>>>>>>>> Thanks so much,
>>>>>>>>> Shira
>>>>>>>>>
>>>>>>>>>         [[alternative HTML version deleted]]
>>>>>>>>>
>>>>>>>>> _______________________________________________
>>>>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>>>>
>>>>>>>>
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>> The University of Edinburgh is a charitable body, registered in Scotland,
>>> with registration number SC005336. Is e buidheann carthannais a th? ann an
>>> Oilthigh Dh?n ?ideann, cl?raichte an Alba, ?ireamh cl?raidh SC005336.
>>>
>>
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

--
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics
> E-mail is sent at my convenience; I don't expect replies outside of
working hours.

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From he|n@v@n@||ever|oo @end|ng |rom v|@etern@@n|  Wed Oct 26 13:41:22 2022
From: he|n@v@n@||ever|oo @end|ng |rom v|@etern@@n| (Hein van Lieverloo)
Date: Wed, 26 Oct 2022 13:41:22 +0200
Subject: [R-sig-ME] Mixed-level regression vs. Box-Jenkins time series
 analysis
Message-ID: <000001d8e92f$df1af450$9d50dcf0$@viaeterna.nl>

Dear group,

 

Can anyone help me find information to compare applicability of:

*	Mixed-level regression analysis with glmmTMB, with repeated measures to deal with autocorrelation. The predictor of the effect of a change could be: 0 (before) and 1 (after).
*	Box-Jenkins time series analysis (designed to deal with autocorrelation) with multiple predictors, a.o. effect of change (comparing before and after).

 

Background of this question: Some biological cases I?m working on in the drinking water industry in The Netherlands:

*	for many years (in my spare time, which is limited): glmmTMB: Mixed-level zero-inflated Poisson-regression: invertebrate counts in drinking water distribution mains (level 1: distribution systems of 34 treatment plants, L2: 176 sampling hydrants, L3: 8 quarterly samples (time series), a multitude of variables. Seasonality is limited. I think Box-Jenkins is not the solution here.
*	with a colleague of mine, we try out the Box-Jenkins approach (I?m not familiar with it) with his program Time Series Analyst (self-developed in MatLAb). I think this can be done with glmmTMB as well, but what is best?  Simple methods (Mann-Whitney, multiple regression) are affected by single predictor issue and effects of zero counts respectively. And in any case, we want to know what method to use to take autocorrelation into account.

*	Surface water system: time series of predictors (bird species counts and temperature (seasonal) and a change in infrastructure) on response (bacterial counts).  Q: does the infrastructural change affect bacterial counts?
*	Drinking water distribution system (urban area): time series of nominal predictors (change in water quality entering whole system, one part of the system cleaned, one part not cleaned) on respons (bacterial counts). Q: what is the effect of the water quality change and what is the effect of cleaning?

 

FYI: all these issues are not a public health problem, mainly scientific research.

The Netherlands water companies boast having the best drinking water in the world, without a disinfectant residual (I have not found arguments to disagree yet)..

 

Kind regards,

 

Hein van Lieverloo

Viaeterna

 

 

J.H.M. van Lieverloo

E-mail: hein.van.lieverloo at viaeterna.nl <mailto:hein.van.lieverloo at viaeterna.nl>  

Mobile +31 6 24269886

www.viaeterna.nl <http://www.viaeterna.nl> 

 

 


	[[alternative HTML version deleted]]


From v@np@r|don @end|ng |rom w|@c@edu  Fri Oct 28 08:17:27 2022
From: v@np@r|don @end|ng |rom w|@c@edu (Jeroen van Paridon)
Date: Fri, 28 Oct 2022 06:17:27 +0000
Subject: [R-sig-ME] Bradley Terry GLMM in R ?
In-Reply-To: <DM6PR06MB41716CEDD0B1124D582E5142B82A9@DM6PR06MB4171.namprd06.prod.outlook.com>
References: <CADeg_O3L_6Cmn_Wv0tExeCLccFJS1p5fottqprdh3+spDCrXxA@mail.gmail.com>
 <CAJuCY5ys+di7UNWKLSV8gw6qVC5yjWXVa4J7Bboe=cHAwoYDGw@mail.gmail.com>
 <CADeg_O0d+tuG0DnnnFw9o9VSh1dcMUbOitcT_rLbRcdbhNpzYw@mail.gmail.com>
 <CAJuCY5ysJUQQtYUCLF76iD5y9TmSiOD5OAfBt039RjVaD_pzwQ@mail.gmail.com>
 <CADeg_O0g7kTrhRUNhrDZjKt3HEzKpfOKZMW71uBaNTDoaNJt7g@mail.gmail.com>
 <CADeg_O0dzRyG=8u7Zjk94PbwGpoqnWoQRw+VRgpoq-BS2RimgA@mail.gmail.com>
 <E27AFF46-A57B-425D-A946-AE24E6567B7C@ed.ac.uk>
 <CADeg_O12OrDgEMNQ6ejcV30evndEjAuFN-wKLdbN0K4GusyAQw@mail.gmail.com>
 <CADeg_O2p+pWAqyE-9pKaJ34icyGT1OVEwTZkUo8EZuf_68nmKA@mail.gmail.com>
 <da7029dd-1277-dea6-c359-ecab30c0ca7e@gmail.com>
 <DM6PR06MB41716CEDD0B1124D582E5142B82A9@DM6PR06MB4171.namprd06.prod.outlook.com>
Message-ID: <DM6PR06MB41718CEC15BAA18FA0F5F923B8329@DM6PR06MB4171.namprd06.prod.outlook.com>

In case it's helpful to anyone following this email thread: I wrote a vignette explaining how to fit a Bradley-Terry model in lme4 using lmerMultiMember. You can find it at https://jvparidon.github.io/lmerMultiMember/articles/bradleyterry_models.html


Cheers,

JP van Paridon (he/him)
?Research Associate, Lupyan Lab
University of Wisconsin-Madison
https://github.com/jvparidon
________________________________
From: Jeroen van Paridon <vanparidon at wisc.edu>
Sent: Thursday, October 20, 2022 1:42 AM
To: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Bradley Terry GLMM in R ?

Hi,

Just to expand on Ben's last email: In principle, lmerMultiMember allows you to pass arbitrary indicator/weight matrices for the random effects to lme4 for model fitting as long as they have the correct shape. The package contains helper functions for generating more conventional multiple membership matrices since that was my own use-case, but if you create your own matrix with opposed (1 and -1) weights I see no reason why it shouldn't work.

Membership matrices need to be sparse matrices of class Matrix::dgCMatrix and shape n_groups x n_obs. You can probably just take whatever indicator matrix you already have, transpose it, and then cast it to the sparse format.

If you're going this route and run into any issues, feel free to reach out to me, directly.


Cheers,

JP van Paridon (he/him)
?Research Associate, Lupyan Lab
University of Wisconsin-Madison
https://github.com/jvparidon
________________________________
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on behalf of Ben Bolker <bbolker at gmail.com>
Sent: Wednesday, October 19, 2022 2:43 PM
To: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Bradley Terry GLMM in R ?



On 2022-10-17 4:02 p.m., Shira Mitchell wrote:
> Questions for Ben Bolker about the excellent GLMM FAQ:
>
> Where does the hglm package fit into this very helpful table ?
> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#what-methods-are-available-to-fit-estimate-glmms

   hglm uses a different *definition of likelihood*, so it's essentially
fitting a different model altogether (although one that will generally
get similar answers to other approaches).  Unfortunately, figuring out
the exact meaning and properties of h-likelihood will take you down a
rabbit hole ... (see refs below)
>
> I wonder also about differences in model formula specifications, since some
> packages (e.g. lme4) don't seem to accommodate Bradley-Terry, whereas some
> packages (e.g. INLA, hglm, MCMCglmm) can accommodate.
> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#model-specification

    This is less conceptually difficult.  There's a big difference
between the models implemented within a package *could* fit, and what
the interfaces will allow.  For example, the machinery in lme4 *can* fit
any model with exponential-family conditional distribution and
multivariate Gaussian random effects (on the log scale). However, the
interface doesn't allow a lot of flexibility to specify the X and Z
matrices, which is what you would need in order to do Bradley-Terry
models.  There are extension packages such as
https://github.com/jvparidon/lmerMultiMember (which allow
multi-membership models and hence might be extended to do Bradley-Terry
models), gamm4 (on CRAN, imports machinery from mgcv to set up Z
matrices corresponding to penalized regression terms in the model), etc.


Meng, Xiao-Li. ?Decoding the H-Likelihood.? Statistical Science 24, no.
3 (August 1, 2009). https://doi.org/10.1214/09-STS277C.

Meng, Xiao?Li. ?What?s the H in H?likelihood: A Holy Grail or an
Achilles? Heel?? In Bayesian Statistics 9, edited by Jos? M. Bernardo,
M. J. Bayarri, James O. Berger, A. P. Dawid, David Heckerman, Adrian F.
M. Smith, and Mike West, 0. Oxford University Press, 2011.
https://doi.org/10.1093/acprof:oso/9780199694587.003.0016.

Jin, Shaobo, and Youngjo Lee. ?A Review of H-Likelihood and Hierarchical
Generalized Linear Model.? WIREs Computational Statistics 13, no. 5
(2021): e1527. https://doi.org/10.1002/wics.1527.


>
>
>
>
> On Mon, Oct 17, 2022 at 3:55 PM Shira Mitchell <shiraqotj at gmail.com> wrote:
>
>> Thanks so much, Jarrod ! Not too late at all. Very interesting to compare
>> MCMC with the approximations (INLA, hglm's extended quasi likelihood). I
>> don't think I have the priors lined up yet across packages. The random
>> effects seem more dispersed according to MCMCglmm than in INLA or hglm, but
>> this could be due to priors not fit algorithm. Will look into the package
>> prior defaults.
>>
>> On Mon, Oct 17, 2022 at 4:45 AM Jarrod Hadfield <j.hadfield at ed.ac.uk>
>> wrote:
>>
>>> Hi Shira,
>>>
>>> Perhaps a little late to be useful, but MCMCglmm also fits random-effect
>>> Bradley-Terry models. Just specify ~mm(opponent1-opponent2) in the random
>>> effect formula. The mm stands for multimembership - the BT model is like a
>>> multimembership model where some effects have been multiplied by -1, hence
>>> the ?-' rather than ?+? in the mm model formula.
>>>
>>> Cheers,
>>>
>>> Jarrod
>>>
>>>
>>>> On 16 Oct 2022, at 22:49, Shira Mitchell <shiraqotj at gmail.com> wrote:
>>>>
>>>> This email was sent to you by someone outside the University.
>>>> You should only click on links or attachments if you are certain that
>>> the email is genuine and the content is safe.
>>>>
>>>> Update: Dr Heather Turner <https://www.heatherturner.net/> (author of
>>>> BradleyTerry2) suggested the hglm package, which unlike lme4 allows you
>>> to
>>>> specify generic design matrices (no longer constrained to lme4 formulas
>>> !)
>>>> Results look really similar to INLA so far. Yay !
>>>>
>>>> On Sun, Oct 16, 2022 at 2:19 PM Shira Mitchell <shiraqotj at gmail.com>
>>> wrote:
>>>>
>>>>> Super helpful !  Thank you so much !
>>>>>
>>>>> Out of curiosity, is there a way to fit this type of Bradley-Terry
>>> model
>>>>> in lme4 ? lme4 formulas include random effect via syntax:
>>>>> https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf
>>>>> "(expr | factor). The expression expr is evaluated as a linear model
>>>>> formula, producing a model matrix following the same rules used in
>>> standard
>>>>> R modeling functions (e.g., `lm` or `glm`). The expression factor is
>>>>> evaluated as an `R` factor. One way to think about the vertical bar
>>>>> operator is as a special kind of interaction between the model matrix
>>> and
>>>>> the grouping factor. This interaction ensures that the columns of the
>>> model
>>>>> matrix have different effects for each level of the grouping factor."
>>>>>
>>>>> So (expr | factor) is X_expr * alpha_factor.
>>>>>
>>>>> So naively writing ~ (1 | m_1) + (1 | m_2) is alpha_{m_1}^{(1)} +
>>>>> alpha_{m_2}^{(2)}, twice as many parameters as what we want which is
>>>>> alpha_{m_1} - alpha_{m_2}.
>>>>>
>>>>> But then see this stackexchange:
>>>>>
>>>>>
>>> https://stats.stackexchange.com/questions/483833/opposing-effects-in-lme4-formulae-bradley-terry-model
>>>>> "I could just make a design matrix, where player 1 gets the value 1,
>>> and
>>>>> player 2 gets the value ?1. However, unless I'm missing a trick, this
>>> would
>>>>> require having a separate column for each player, and plugging each
>>> player
>>>>> column's name into the formula"
>>>>>
>>>>> But suppose we create columns for all m = 1,...,M messages:
>>>>>
>>>>> A_m = 1 if m = m_1
>>>>>            -1 if m = m_2
>>>>>             0 otherwise
>>>>>
>>>>> I think then ~ (A_1 + ... + A_M  | m_1) is alpha_{m_1}^{(m_1)} -
>>>>> alpha_{m_1}^{(m_2)}, also not what we would want.
>>>>>
>>>>> Back to INLA. Suppose we now want to add random message-specific slopes
>>>>> for variable X_i in addition to random message-specific intercepts:
>>>>>
>>>>> P[i chooses m_1] = logit^-1 (beta_0 + (alpha_{m_1} - alpha_{m_2}) +
>>>>> (beta_{m_1} - beta_{m_2})X_i)
>>>>>
>>>>> alpha_1,...,alpha_M ~ N(0,sigma_intercept)
>>>>> beta_1,...,beta_M ~ N(0,sigma_slope)
>>>>>
>>>>> I see some resources about this, but nothing super comprehensive. Any
>>>>> advice where to look for complete documentation ?
>>>>>
>>>>>
>>> https://groups.google.com/g/r-inla-discussion-group/c/iQELaQF8M9Q/m/q7f4-W8YQksJ
>>>>> (
>>>>>
>>> https://becarioprecario.bitbucket.io/inla-gitbook/ch-multilevel.html#multilevel-models-for-longitudinal-data
>>>>> https://rpubs.com/corey_sparks/431920
>>>>> https://avianecologist.com/2016/10/05/multilevel-models/
>>>>>
>>>>> Here is what we did:
>>>>>
>>>>> data$w_X = -data$X
>>>>> data$m_1_beta = data$m_1
>>>>> data$m_2_beta = data$m_2
>>>>>
>>>>> inla(depvar ~  f(m_1, model="iid", values = issues) +
>>>>>                            f(m_2, w, copy = "m_1") +
>>>>>                            f(m_1_beta, X, model="iid", values = issues)
>>> +
>>>>>                            f(m_2_beta, w_X, copy = "m_1_beta"),
>>>>>                          family="binomial",
>>>>>                          data=data)
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> On Mon, Oct 10, 2022 at 4:24 AM Thierry Onkelinx <
>>> thierry.onkelinx at inbo.be>
>>>>> wrote:
>>>>>
>>>>>> Dear Shira,
>>>>>>
>>>>>> - in a formula object means remove that object from the formula. Use a
>>>>>> weight of -1 instead.
>>>>>>
>>>>>> f(home, model = "iid")) + f(away, w = -1, copy = "home")
>>>>>>
>>>>>> Best regards,
>>>>>>
>>>>>> ir. Thierry Onkelinx
>>>>>> Statisticus / Statistician
>>>>>>
>>>>>> Vlaamse Overheid / Government of Flanders
>>>>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>>>>>> AND FOREST
>>>>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>>>>>> thierry.onkelinx at inbo.be
>>>>>> Havenlaan 88 bus 73, 1000 Brussel
>>>>>> www.inbo.be<http://www.inbo.be>
>>>>>>
>>>>>>
>>>>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>>> To call in the statistician after the experiment is done may be no
>>> more
>>>>>> than asking him to perform a post-mortem examination: he may be able
>>> to say
>>>>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>>>>> The plural of anecdote is not data. ~ Roger Brinner
>>>>>> The combination of some data and an aching desire for an answer does
>>> not
>>>>>> ensure that a reasonable answer can be extracted from a given body of
>>> data.
>>>>>> ~ John Tukey
>>>>>>
>>>>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>>>
>>>>>> <https://www.inbo.be>
>>>>>>
>>>>>>
>>>>>> Op vr 7 okt. 2022 om 23:36 schreef Shira Mitchell <
>>> shiraqotj at gmail.com>:
>>>>>>
>>>>>>> Thanks so much, Thierry ! This is great.
>>>>>>>
>>>>>>> This works except that I cannot subtract because:
>>>>>>> f(home, model = "iid")) - f(away, copy = "home")
>>>>>>>
>>>>>>> just drops the second term. Apologies that I'm not super familiar
>>> with
>>>>>>> INLA syntax yet.
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> On Fri, Oct 7, 2022 at 10:19 AM Thierry Onkelinx <
>>>>>>> thierry.onkelinx at inbo.be> wrote:
>>>>>>>
>>>>>>>> Hi Shira,
>>>>>>>>
>>>>>>>> I fit such models with the INLA package (https://www.r-inla.org/).
>>> The
>>>>>>>> trick is to define two random effects but force their parameter
>>> estimates
>>>>>>>> to be identical.
>>>>>>>>
>>>>>>>> The code would contain something like f(home, model = "iid")) +
>>> f(away,
>>>>>>>> copy = "home"). Meaning home ~ N(0, sigma_beta_i) and home[i] =
>>> away[i]
>>>>>>>>
>>>>>>>> Best regards,
>>>>>>>>
>>>>>>>> ir. Thierry Onkelinx
>>>>>>>> Statisticus / Statistician
>>>>>>>>
>>>>>>>> Vlaamse Overheid / Government of Flanders
>>>>>>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR
>>> NATURE
>>>>>>>> AND FOREST
>>>>>>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality
>>> Assurance
>>>>>>>> thierry.onkelinx at inbo.be
>>>>>>>> Havenlaan 88 bus 73, 1000 Brussel
>>>>>>>> www.inbo.be<http://www.inbo.be>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>>>>> To call in the statistician after the experiment is done may be no
>>> more
>>>>>>>> than asking him to perform a post-mortem examination: he may be
>>> able to say
>>>>>>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>>>>>>> The plural of anecdote is not data. ~ Roger Brinner
>>>>>>>> The combination of some data and an aching desire for an answer does
>>>>>>>> not ensure that a reasonable answer can be extracted from a given
>>> body of
>>>>>>>> data. ~ John Tukey
>>>>>>>>
>>>>>>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>>>>>
>>>>>>>> <https://www.inbo.be>
>>>>>>>>
>>>>>>>>
>>>>>>>> Op vr 7 okt. 2022 om 15:00 schreef Shira Mitchell <
>>> shiraqotj at gmail.com
>>>>>>>>> :
>>>>>>>>
>>>>>>>>> We want to fit Bradley-Terry-style GLMM models in R. We looked
>>> into:
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>> https://cran.r-project.org/web/packages/BradleyTerry2/vignettes/BradleyTerry.pdf
>>>>>>>>> and
>>>>>>>>> http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html
>>>>>>>>>
>>>>>>>>> We have voter-specific variables x that influence which political
>>>>>>>>> message
>>>>>>>>> (i vs j) wins for them:
>>>>>>>>>
>>>>>>>>> logit[pr(i beats j | person with covariate x)] = lambda_i -
>>> lambda_j +
>>>>>>>>> (beta_i - beta_j) x
>>>>>>>>>
>>>>>>>>> We then model parameters as random effects:
>>>>>>>>> lambda_i ~ N(0, sigma_lambda)
>>>>>>>>> beta_i ~ N(0, sigma_beta)
>>>>>>>>>
>>>>>>>>> Is there a way to do this in R ? We do this in TensorFlow in
>>> Python by
>>>>>>>>> directly specifying design matrices with the 0,-1,1 or 0,-x,x
>>> entries.
>>>>>>>>> However, I do not see how to do this in R using lme4,
>>> BradleyTerry2,
>>>>>>>>> mgcv,
>>>>>>>>> etc.
>>>>>>>>>
>>>>>>>>> Thanks so much,
>>>>>>>>> Shira
>>>>>>>>>
>>>>>>>>>         [[alternative HTML version deleted]]
>>>>>>>>>
>>>>>>>>> _______________________________________________
>>>>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>>>>
>>>>>>>>
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>> The University of Edinburgh is a charitable body, registered in Scotland,
>>> with registration number SC005336. Is e buidheann carthannais a th? ann an
>>> Oilthigh Dh?n ?ideann, cl?raichte an Alba, ?ireamh cl?raidh SC005336.
>>>
>>
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

--
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics
> E-mail is sent at my convenience; I don't expect replies outside of
working hours.

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From m@@rten@broekm@n @end|ng |rom ru@n|  Mon Oct 31 15:34:57 2022
From: m@@rten@broekm@n @end|ng |rom ru@n| (Broekman, M.J.E. (Maarten))
Date: Mon, 31 Oct 2022 14:34:57 +0000
Subject: [R-sig-ME] Questions about weights in lmer in lme4 package
Message-ID: <AM0PR10MB3364EC72D5B4D877152B34E38C379@AM0PR10MB3364.EURPRD10.PROD.OUTLOOK.COM>

Hello,
I have a few questions about the weight argument in the lmer function of the lme4 package. It would be great if someone help me with these questions.
I am trying to fit a linear mixed effect model using the lme4 package in which I want to study the effect of several variables on home range size. When I fit models without using weights, there are no problems. However, in my dataset I have population averages of home range size with differences in population size (number of individuals included in the average home range size). To account for differences in population size, I want to use the number of individuals in each population as weights in the model. As a next step, I want to calculate confidence intervals of the marginal R2, using the bootMer function. However, the confidence intervals look odd when using the weighted model, i.e. the R2 squared from the model is not within the confidence interval. I do not have this problem when using the unweighted model.
After reading this post: https://stats.stackexchange.com/questions/491625/meaning-of-the-weight-argument-in-glmer-and-lmer, I also tried scaling the weights by the mean weight, i.e., weight = #individuals/mean(#individuals). Now, the confidence intervals of the marginal R2 do include the marginal R2 from the model. The residual variation of the model is also much lower for the model with the scaled weights.
I have the following questions:

  *   Why does parametric bootstrapping leads to reasonable confidence intervals (confidence interval does include marginal R2 reported for the model) when using scaled weights, and not when using unscaled weights?
  *   Why does scaling the weights affect the residual variation of the model?
  *   Should the method with the scaled weights be the preferred method? And if yes, why?

I spend a lot of time trying to find answers to these questions on the internet. I found a lot of information, but not the answers I needed, so it would be great if someone can help me with these questions.

Thank you in advance!

Best regards,
Maarten Broekman


	[[alternative HTML version deleted]]


From br|@ng|||phd @end|ng |rom gm@||@com  Mon Oct 31 18:51:24 2022
From: br|@ng|||phd @end|ng |rom gm@||@com (Brian Gill)
Date: Mon, 31 Oct 2022 10:51:24 -0700
Subject: [R-sig-ME] Mixed model specification (control for location and
 repeated sampling of same location through time)
Message-ID: <CAHbaY_fe_NfJiJ+MPbedDXirikx2HESZpSV6oxT1id0zp06uKw@mail.gmail.com>

I have three locations (Sites) where I repeatedly measured a number of
environmental variables (X1, X2, X3) and a response (Y; normally
distributed) over time. That is, I have data on each environmental variable
and the response at many time points for each of 3 sites. For each
timepoints all three sites were sampled.

I want to model the response (Y) as a function of the environmental
variables (X1, X2, X3) while controlling for effects of Sites and Time. I
expect responses from the same site to be similar because they come from
the same location and responses measured at closer timepoints to be more
similar than those separated by more time.

Can people please advise on an appropriate model specification.

I've come up with the following so far:

Y ~ Site + X1 + X2 + X3 + (1 | Date)

Y ~ X1 + X2 + X3 + (1 | Site) + (1 | Date)

My hangups are that I think these models treat Date categorically
(controlling for variation from a particular date, but not how close or far
dates are from each other). Also, a model allowing both random intercepts
and slopes might be better as responses could vary significantly in
magnitude and direction among sites.

Any advice would be appreciated. Thanks!

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Thu Nov  3 14:45:01 2022
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Thu, 3 Nov 2022 14:45:01 +0100
Subject: [R-sig-ME] Mixed model specification (control for location and
 repeated sampling of same location through time)
In-Reply-To: <CAHbaY_fe_NfJiJ+MPbedDXirikx2HESZpSV6oxT1id0zp06uKw@mail.gmail.com>
References: <CAHbaY_fe_NfJiJ+MPbedDXirikx2HESZpSV6oxT1id0zp06uKw@mail.gmail.com>
Message-ID: <CAJuCY5yQwDUCo9gL4WH7gyzBnjk_VhrYUGibMeAOUbQZ=rQVsQ@mail.gmail.com>

Dear Brian,

You have only 3 sites. That is too few to use as a random effect.

Look into glmmTMB and INLA. They provide correlated random effects. Which
is relevant for your Date variable.

The glmmTMB formula might look like this: Y ~ Site + X1 + X2 + X3 +
ar1(Date | Site)
The INLA formula: Y ~ Site + X1 + X2 + X3 + f(Date, model = "rw1",
replicate = as.integer(Site))

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op ma 31 okt. 2022 om 18:55 schreef Brian Gill <briangillphd at gmail.com>:

> I have three locations (Sites) where I repeatedly measured a number of
> environmental variables (X1, X2, X3) and a response (Y; normally
> distributed) over time. That is, I have data on each environmental variable
> and the response at many time points for each of 3 sites. For each
> timepoints all three sites were sampled.
>
> I want to model the response (Y) as a function of the environmental
> variables (X1, X2, X3) while controlling for effects of Sites and Time. I
> expect responses from the same site to be similar because they come from
> the same location and responses measured at closer timepoints to be more
> similar than those separated by more time.
>
> Can people please advise on an appropriate model specification.
>
> I've come up with the following so far:
>
> Y ~ Site + X1 + X2 + X3 + (1 | Date)
>
> Y ~ X1 + X2 + X3 + (1 | Site) + (1 | Date)
>
> My hangups are that I think these models treat Date categorically
> (controlling for variation from a particular date, but not how close or far
> dates are from each other). Also, a model allowing both random intercepts
> and slopes might be better as responses could vary significantly in
> magnitude and direction among sites.
>
> Any advice would be appreciated. Thanks!
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From norm@n@d@ure||e @end|ng |rom @grop@r|@tech@|r  Tue Nov  8 16:06:10 2022
From: norm@n@d@ure||e @end|ng |rom @grop@r|@tech@|r (Norman DAURELLE)
Date: Tue, 8 Nov 2022 16:06:10 +0100 (CET)
Subject: [R-sig-ME] Mixed model specification (control for location and
 repeated sampling of same location through time)
In-Reply-To: <CAJuCY5yQwDUCo9gL4WH7gyzBnjk_VhrYUGibMeAOUbQZ=rQVsQ@mail.gmail.com>
References: <CAHbaY_fe_NfJiJ+MPbedDXirikx2HESZpSV6oxT1id0zp06uKw@mail.gmail.com>
 <CAJuCY5yQwDUCo9gL4WH7gyzBnjk_VhrYUGibMeAOUbQZ=rQVsQ@mail.gmail.com>
Message-ID: <486269052.15979237.1667919970641.JavaMail.zimbra@agroparistech.fr>


Dear list members, Brian, Thierry, 

I am not an expert, but I don't see why the number of sites would be a barrier to introducing it as a random effect. 

Would you care to explain the reasoning behind that statement ? 

To me, the Y ~ X1 + X2 + X3 + (1 | Site) part seems appropriate (I don't know about how to use the different dates, though). 

Sorry if this is not helpful, Brian. 

Cheers, 

Norman 




De: "Thierry Onkelinx via R-sig-mixed-models" <r-sig-mixed-models at r-project.org> 
?: "Brian Gill" <briangillphd at gmail.com> 
Cc: r-sig-mixed-models at r-project.org 
Envoy?: Jeudi 3 Novembre 2022 14:45:01 
Objet: Re: [R-sig-ME] Mixed model specification (control for location and repeated sampling of same location through time) 

Dear Brian, 

You have only 3 sites. That is too few to use as a random effect. 

Look into glmmTMB and INLA. They provide correlated random effects. Which 
is relevant for your Date variable. 

The glmmTMB formula might look like this: Y ~ Site + X1 + X2 + X3 + 
ar1(Date | Site) 
The INLA formula: Y ~ Site + X1 + X2 + X3 + f(Date, model = "rw1", 
replicate = as.integer(Site)) 

Best regards, 

ir. Thierry Onkelinx 
Statisticus / Statistician 

Vlaamse Overheid / Government of Flanders 
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND 
FOREST 
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance 
thierry.onkelinx at inbo.be 
Havenlaan 88 bus 73, 1000 Brussel 
www.inbo.be 

/////////////////////////////////////////////////////////////////////////////////////////// 
To call in the statistician after the experiment is done may be no more 
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher 
The plural of anecdote is not data. ~ Roger Brinner 
The combination of some data and an aching desire for an answer does not 
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey 
/////////////////////////////////////////////////////////////////////////////////////////// 

<https://www.inbo.be> 


Op ma 31 okt. 2022 om 18:55 schreef Brian Gill <briangillphd at gmail.com>: 

> I have three locations (Sites) where I repeatedly measured a number of 
> environmental variables (X1, X2, X3) and a response (Y; normally 
> distributed) over time. That is, I have data on each environmental variable 
> and the response at many time points for each of 3 sites. For each 
> timepoints all three sites were sampled. 
> 
> I want to model the response (Y) as a function of the environmental 
> variables (X1, X2, X3) while controlling for effects of Sites and Time. I
> expect responses from the same site to be similar because they come from
> the same location and responses measured at closer timepoints to be more
> similar than those separated by more time. 
> 
> Can people please advise on an appropriate model specification. 
> 
> I've come up with the following so far: 
> 
> Y ~ Site + X1 + X2 + X3 + (1 | Date) 
> 
> Y ~ X1 + X2 + X3 + (1 | Site) + (1 | Date) 
> 
> My hangups are that I think these models treat Date categorically 
> (controlling for variation from a particular date, but not how close or far 
> dates are from each other). Also, a model allowing both random intercepts
> and slopes might be better as responses could vary significantly in 
> magnitude and direction among sites. 
> 
> Any advice would be appreciated. Thanks! 
> 
> [[alternative HTML version deleted]] 
> 
> _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models 
> 

[[alternative HTML version deleted]] 

_______________________________________________ 
R-sig-mixed-models at r-project.org mailing list 
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models 

	[[alternative HTML version deleted]]


From p@u|@john@on @end|ng |rom g|@@gow@@c@uk  Tue Nov  8 16:23:43 2022
From: p@u|@john@on @end|ng |rom g|@@gow@@c@uk (Paul Johnson)
Date: Tue, 8 Nov 2022 15:23:43 +0000
Subject: [R-sig-ME] Mixed model specification (control for location and
 repeated sampling of same location through time)
Message-ID: <A1D543FE-00DD-46EA-87C0-80F4541EA106@glasgow.ac.uk>

Hi Norman,

The minimum number of blocks/groups required to support a random effect is discussed in Ben Bolker's GLMM FAQ wiki:

https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#should-i-treat-factor-xxx-as-fixed-or-random

"One point of particular relevance to ?modern? mixed model estimation (rather than ?classical? method-of-moments estimation) is that, for practical purposes, there must be a reasonable number of random-effects levels (e.g. blocks) ? more than 5 or 6 at a minimum."

Best wishes,
Paul

Paul Johnson
Senior Lecturer
School of Biodiversity, One Health and Veterinary Medicine
University of Glasgow
Room 362, Wolfson Link Building
Glasgow G12 8QQ
+44 (0)7814 668 613
paul.johnson at glasgow.ac.uk
https://www.gla.ac.uk/schools/bohvm/staff/pauljohnson/
https://orcid.org/0000-0001-6663-7520

?On 08/11/2022, 15:15, "R-sig-mixed-models on behalf of Norman DAURELLE via R-sig-mixed-models" <r-sig-mixed-models-bounces at r-project.org on behalf of r-sig-mixed-models at r-project.org> wrote:


    Dear list members, Brian, Thierry, 

    I am not an expert, but I don't see why the number of sites would be a barrier to introducing it as a random effect. 

    Would you care to explain the reasoning behind that statement ? 

    To me, the Y ~ X1 + X2 + X3 + (1 | Site) part seems appropriate (I don't know about how to use the different dates, though). 

    Sorry if this is not helpful, Brian. 

    Cheers, 

    Norman 




    De: "Thierry Onkelinx via R-sig-mixed-models" <r-sig-mixed-models at r-project.org> 
    ?: "Brian Gill" <briangillphd at gmail.com> 
    Cc: r-sig-mixed-models at r-project.org 
    Envoy?: Jeudi 3 Novembre 2022 14:45:01 
    Objet: Re: [R-sig-ME] Mixed model specification (control for location and repeated sampling of same location through time) 

    Dear Brian, 

    You have only 3 sites. That is too few to use as a random effect. 

    Look into glmmTMB and INLA. They provide correlated random effects. Which 
    is relevant for your Date variable. 

    The glmmTMB formula might look like this: Y ~ Site + X1 + X2 + X3 + 
    ar1(Date | Site) 
    The INLA formula: Y ~ Site + X1 + X2 + X3 + f(Date, model = "rw1", 
    replicate = as.integer(Site)) 

    Best regards, 

    ir. Thierry Onkelinx 
    Statisticus / Statistician 

    Vlaamse Overheid / Government of Flanders 
    INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND 
    FOREST 
    Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance 
    thierry.onkelinx at inbo.be 
    Havenlaan 88 bus 73, 1000 Brussel 
    www.inbo.be 

    /////////////////////////////////////////////////////////////////////////////////////////// 
    To call in the statistician after the experiment is done may be no more 
    than asking him to perform a post-mortem examination: he may be able to say
    what the experiment died of. ~ Sir Ronald Aylmer Fisher 
    The plural of anecdote is not data. ~ Roger Brinner 
    The combination of some data and an aching desire for an answer does not 
    ensure that a reasonable answer can be extracted from a given body of data.
    ~ John Tukey 
    /////////////////////////////////////////////////////////////////////////////////////////// 

    <https://www.inbo.be> 


    Op ma 31 okt. 2022 om 18:55 schreef Brian Gill <briangillphd at gmail.com>: 

    > I have three locations (Sites) where I repeatedly measured a number of 
    > environmental variables (X1, X2, X3) and a response (Y; normally 
    > distributed) over time. That is, I have data on each environmental variable 
    > and the response at many time points for each of 3 sites. For each 
    > timepoints all three sites were sampled. 
    > 
    > I want to model the response (Y) as a function of the environmental 
    > variables (X1, X2, X3) while controlling for effects of Sites and Time. I
    > expect responses from the same site to be similar because they come from
    > the same location and responses measured at closer timepoints to be more
    > similar than those separated by more time. 
    > 
    > Can people please advise on an appropriate model specification. 
    > 
    > I've come up with the following so far: 
    > 
    > Y ~ Site + X1 + X2 + X3 + (1 | Date) 
    > 
    > Y ~ X1 + X2 + X3 + (1 | Site) + (1 | Date) 
    > 
    > My hangups are that I think these models treat Date categorically 
    > (controlling for variation from a particular date, but not how close or far 
    > dates are from each other). Also, a model allowing both random intercepts
    > and slopes might be better as responses could vary significantly in 
    > magnitude and direction among sites. 
    > 
    > Any advice would be appreciated. Thanks! 
    > 
    > [[alternative HTML version deleted]] 
    > 
    > _______________________________________________ 
    > R-sig-mixed-models at r-project.org mailing list 
    > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models 
    > 

    [[alternative HTML version deleted]] 

    _______________________________________________ 
    R-sig-mixed-models at r-project.org mailing list 
    https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models 

    	[[alternative HTML version deleted]]



From norm@n@d@ure||e @end|ng |rom @grop@r|@tech@|r  Tue Nov  8 16:19:23 2022
From: norm@n@d@ure||e @end|ng |rom @grop@r|@tech@|r (Norman DAURELLE)
Date: Tue, 8 Nov 2022 16:19:23 +0100 (CET)
Subject: [R-sig-ME] Mixed model specification (control for location and
 repeated sampling of same location through time)
In-Reply-To: <A1D543FE-00DD-46EA-87C0-80F4541EA106@glasgow.ac.uk>
References: <A1D543FE-00DD-46EA-87C0-80F4541EA106@glasgow.ac.uk>
Message-ID: <259384236.16004170.1667920763730.JavaMail.zimbra@agroparistech.fr>


Dear Paul, 

thanks ! 

Norman 


De: "Paul Johnson" <paul.johnson at glasgow.ac.uk> 
?: "Norman DAURELLE" <norman.daurelle at agroparistech.fr>, "Thierry Onkelinx" <thierry.onkelinx at inbo.be> 
Cc: "r-sig-mixed-models" <r-sig-mixed-models at r-project.org>, "Brian Gill" <briangillphd at gmail.com> 
Envoy?: Mardi 8 Novembre 2022 16:23:43 
Objet: Re: [R-sig-ME] Mixed model specification (control for location and repeated sampling of same location through time) 

Hi Norman, 

The minimum number of blocks/groups required to support a random effect is discussed in Ben Bolker's GLMM FAQ wiki: 

https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#should-i-treat-factor-xxx-as-fixed-or-random 

"One point of particular relevance to ?modern? mixed model estimation (rather than ?classical? method-of-moments estimation) is that, for practical purposes, there must be a reasonable number of random-effects levels (e.g. blocks) ? more than 5 or 6 at a minimum." 

Best wishes, 
Paul 

Paul Johnson 
Senior Lecturer 
School of Biodiversity, One Health and Veterinary Medicine 
University of Glasgow 
Room 362, Wolfson Link Building 
Glasgow G12 8QQ 
+44 (0)7814 668 613 
paul.johnson at glasgow.ac.uk 
https://www.gla.ac.uk/schools/bohvm/staff/pauljohnson/ 
https://orcid.org/0000-0001-6663-7520 

On 08/11/2022, 15:15, "R-sig-mixed-models on behalf of Norman DAURELLE via R-sig-mixed-models" <r-sig-mixed-models-bounces at r-project.org on behalf of r-sig-mixed-models at r-project.org> wrote: 


Dear list members, Brian, Thierry, 

I am not an expert, but I don't see why the number of sites would be a barrier to introducing it as a random effect. 

Would you care to explain the reasoning behind that statement ? 

To me, the Y ~ X1 + X2 + X3 + (1 | Site) part seems appropriate (I don't know about how to use the different dates, though). 

Sorry if this is not helpful, Brian. 

Cheers, 

Norman 




De: "Thierry Onkelinx via R-sig-mixed-models" <r-sig-mixed-models at r-project.org> 
?: "Brian Gill" <briangillphd at gmail.com> 
Cc: r-sig-mixed-models at r-project.org 
Envoy?: Jeudi 3 Novembre 2022 14:45:01 
Objet: Re: [R-sig-ME] Mixed model specification (control for location and repeated sampling of same location through time) 

Dear Brian, 

You have only 3 sites. That is too few to use as a random effect. 

Look into glmmTMB and INLA. They provide correlated random effects. Which 
is relevant for your Date variable. 

The glmmTMB formula might look like this: Y ~ Site + X1 + X2 + X3 + 
ar1(Date | Site) 
The INLA formula: Y ~ Site + X1 + X2 + X3 + f(Date, model = "rw1", 
replicate = as.integer(Site)) 

Best regards, 

ir. Thierry Onkelinx 
Statisticus / Statistician 

Vlaamse Overheid / Government of Flanders 
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND 
FOREST 
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance 
thierry.onkelinx at inbo.be 
Havenlaan 88 bus 73, 1000 Brussel 
www.inbo.be 

/////////////////////////////////////////////////////////////////////////////////////////// 
To call in the statistician after the experiment is done may be no more 
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher 
The plural of anecdote is not data. ~ Roger Brinner 
The combination of some data and an aching desire for an answer does not 
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey 
/////////////////////////////////////////////////////////////////////////////////////////// 

<https://www.inbo.be> 


Op ma 31 okt. 2022 om 18:55 schreef Brian Gill <briangillphd at gmail.com>: 

> I have three locations (Sites) where I repeatedly measured a number of 
> environmental variables (X1, X2, X3) and a response (Y; normally 
> distributed) over time. That is, I have data on each environmental variable 
> and the response at many time points for each of 3 sites. For each 
> timepoints all three sites were sampled. 
> 
> I want to model the response (Y) as a function of the environmental 
> variables (X1, X2, X3) while controlling for effects of Sites and Time. I
> expect responses from the same site to be similar because they come from
> the same location and responses measured at closer timepoints to be more
> similar than those separated by more time. 
> 
> Can people please advise on an appropriate model specification. 
> 
> I've come up with the following so far: 
> 
> Y ~ Site + X1 + X2 + X3 + (1 | Date) 
> 
> Y ~ X1 + X2 + X3 + (1 | Site) + (1 | Date) 
> 
> My hangups are that I think these models treat Date categorically 
> (controlling for variation from a particular date, but not how close or far 
> dates are from each other). Also, a model allowing both random intercepts
> and slopes might be better as responses could vary significantly in 
> magnitude and direction among sites. 
> 
> Any advice would be appreciated. Thanks! 
> 
> [[alternative HTML version deleted]] 
> 
> _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models 
> 

[[alternative HTML version deleted]] 

_______________________________________________ 
R-sig-mixed-models at r-project.org mailing list 
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models 

[[alternative HTML version deleted]] 

	[[alternative HTML version deleted]]


From me @end|ng |rom ph||||p@|d@y@com  Tue Nov  8 16:27:26 2022
From: me @end|ng |rom ph||||p@|d@y@com (Phillip Alday)
Date: Tue, 8 Nov 2022 09:27:26 -0600
Subject: [R-sig-ME] Mixed model specification (control for location and
 repeated sampling of same location through time)
In-Reply-To: <486269052.15979237.1667919970641.JavaMail.zimbra@agroparistech.fr>
References: <CAHbaY_fe_NfJiJ+MPbedDXirikx2HESZpSV6oxT1id0zp06uKw@mail.gmail.com>
 <CAJuCY5yQwDUCo9gL4WH7gyzBnjk_VhrYUGibMeAOUbQZ=rQVsQ@mail.gmail.com>
 <486269052.15979237.1667919970641.JavaMail.zimbra@agroparistech.fr>
Message-ID: <c0b82d17-f9ae-9148-986e-e72cb502f221@phillipalday.com>

Dear Norman,

Random effects are fundamentally estimates of variance. Computing the 
variance from 3 items will lead to a very noisy estimate -- noisy to the 
point of "generally not useful".

Thierry has a nice write-up here:

https://www.muscardinus.be/2018/09/number-random-effect-levels/

This is also discussed on the GLMM FAQ:

https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#should-i-treat-factor-xxx-as-fixed-or-random

On top of the variance bit, including Site as a fixed effect doesn't led 
to a horrible model in terms of number of parameters.

There is a common point of confusion that random effects are nuisance 
parameters and thus nuisance parameters, should as replicates, should be 
in the random effects. This isn't quite right.

Best,
Phillip

On 11/8/22 09:06, Norman DAURELLE via R-sig-mixed-models wrote:
> Dear list members, Brian, Thierry,
>
> I am not an expert, but I don't see why the number of sites would be a barrier to introducing it as a random effect.
>
> Would you care to explain the reasoning behind that statement ?
>
> To me, the Y ~ X1 + X2 + X3 + (1 | Site) part seems appropriate (I don't know about how to use the different dates, though).
>
> Sorry if this is not helpful, Brian.
>
> Cheers,
>
> Norman
>
>
>
>
> De: "Thierry Onkelinx via R-sig-mixed-models" <r-sig-mixed-models at r-project.org>
> ?: "Brian Gill" <briangillphd at gmail.com>
> Cc: r-sig-mixed-models at r-project.org
> Envoy?: Jeudi 3 Novembre 2022 14:45:01
> Objet: Re: [R-sig-ME] Mixed model specification (control for location and repeated sampling of same location through time)
>
> Dear Brian,
>
> You have only 3 sites. That is too few to use as a random effect.
>
> Look into glmmTMB and INLA. They provide correlated random effects. Which
> is relevant for your Date variable.
>
> The glmmTMB formula might look like this: Y ~ Site + X1 + X2 + X3 +
> ar1(Date | Site)
> The INLA formula: Y ~ Site + X1 + X2 + X3 + f(Date, model = "rw1",
> replicate = as.integer(Site))
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op ma 31 okt. 2022 om 18:55 schreef Brian Gill <briangillphd at gmail.com>:
>
>> I have three locations (Sites) where I repeatedly measured a number of
>> environmental variables (X1, X2, X3) and a response (Y; normally
>> distributed) over time. That is, I have data on each environmental variable
>> and the response at many time points for each of 3 sites. For each
>> timepoints all three sites were sampled.
>>
>> I want to model the response (Y) as a function of the environmental
>> variables (X1, X2, X3) while controlling for effects of Sites and Time. I
>> expect responses from the same site to be similar because they come from
>> the same location and responses measured at closer timepoints to be more
>> similar than those separated by more time.
>>
>> Can people please advise on an appropriate model specification.
>>
>> I've come up with the following so far:
>>
>> Y ~ Site + X1 + X2 + X3 + (1 | Date)
>>
>> Y ~ X1 + X2 + X3 + (1 | Site) + (1 | Date)
>>
>> My hangups are that I think these models treat Date categorically
>> (controlling for variation from a particular date, but not how close or far
>> dates are from each other). Also, a model allowing both random intercepts
>> and slopes might be better as responses could vary significantly in
>> magnitude and direction among sites.
>>
>> Any advice would be appreciated. Thanks!
>>
>> [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
> [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> 	[[alternative HTML version deleted]]
>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From norm@n@d@ure||e @end|ng |rom @grop@r|@tech@|r  Tue Nov  8 16:35:56 2022
From: norm@n@d@ure||e @end|ng |rom @grop@r|@tech@|r (Norman DAURELLE)
Date: Tue, 8 Nov 2022 16:35:56 +0100 (CET)
Subject: [R-sig-ME] Mixed model specification (control for location and
 repeated sampling of same location through time)
In-Reply-To: <c0b82d17-f9ae-9148-986e-e72cb502f221@phillipalday.com>
References: <CAHbaY_fe_NfJiJ+MPbedDXirikx2HESZpSV6oxT1id0zp06uKw@mail.gmail.com>
 <CAJuCY5yQwDUCo9gL4WH7gyzBnjk_VhrYUGibMeAOUbQZ=rQVsQ@mail.gmail.com>
 <486269052.15979237.1667919970641.JavaMail.zimbra@agroparistech.fr>
 <c0b82d17-f9ae-9148-986e-e72cb502f221@phillipalday.com>
Message-ID: <998052098.16050683.1667921756526.JavaMail.zimbra@agroparistech.fr>


Dear Phillip, 

thanks. 

In my mind, a random effect was used when you know that the -typically- factor influences the outcome variable, but this effect is not the focus of your study. 

I've come to statistics through agronomy so let me explain myself with an example : years, locations or cultivars in a relationship between the severity of a disease and a crop's yield. You know yield varies between locations, years and cultivars but that's "differences to control for" which you would do with random effects. Admittedly, you usually have more than 3 levels for each factor. 

Apparently the rule for using a variable as a fixed or a random effect is not as "inherent to the variable" as I thought. 

Thanks again ! 

Norman 


De: "Phillip Alday" <me at phillipalday.com> 
?: "Norman DAURELLE" <norman.daurelle at agroparistech.fr>, "Thierry Onkelinx" <thierry.onkelinx at inbo.be> 
Cc: "r-sig-mixed-models" <r-sig-mixed-models at r-project.org>, "Brian Gill" <briangillphd at gmail.com> 
Envoy?: Mardi 8 Novembre 2022 16:27:26 
Objet: Re: [R-sig-ME] Mixed model specification (control for location and repeated sampling of same location through time) 

Dear Norman, 

Random effects are fundamentally estimates of variance. Computing the 
variance from 3 items will lead to a very noisy estimate -- noisy to the 
point of "generally not useful". 

Thierry has a nice write-up here: 

https://www.muscardinus.be/2018/09/number-random-effect-levels/ 

This is also discussed on the GLMM FAQ: 

https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#should-i-treat-factor-xxx-as-fixed-or-random 

On top of the variance bit, including Site as a fixed effect doesn't led 
to a horrible model in terms of number of parameters. 

There is a common point of confusion that random effects are nuisance 
parameters and thus nuisance parameters, should as replicates, should be 
in the random effects. This isn't quite right. 

Best, 
Phillip 

On 11/8/22 09:06, Norman DAURELLE via R-sig-mixed-models wrote: 
> Dear list members, Brian, Thierry, 
> 
> I am not an expert, but I don't see why the number of sites would be a barrier to introducing it as a random effect. 
> 
> Would you care to explain the reasoning behind that statement ? 
> 
> To me, the Y ~ X1 + X2 + X3 + (1 | Site) part seems appropriate (I don't know about how to use the different dates, though). 
> 
> Sorry if this is not helpful, Brian. 
> 
> Cheers, 
> 
> Norman 
> 
> 
> 
> 
> De: "Thierry Onkelinx via R-sig-mixed-models" <r-sig-mixed-models at r-project.org> 
> ?: "Brian Gill" <briangillphd at gmail.com> 
> Cc: r-sig-mixed-models at r-project.org 
> Envoy?: Jeudi 3 Novembre 2022 14:45:01 
> Objet: Re: [R-sig-ME] Mixed model specification (control for location and repeated sampling of same location through time) 
> 
> Dear Brian, 
> 
> You have only 3 sites. That is too few to use as a random effect. 
> 
> Look into glmmTMB and INLA. They provide correlated random effects. Which 
> is relevant for your Date variable. 
> 
> The glmmTMB formula might look like this: Y ~ Site + X1 + X2 + X3 + 
> ar1(Date | Site) 
> The INLA formula: Y ~ Site + X1 + X2 + X3 + f(Date, model = "rw1", 
> replicate = as.integer(Site)) 
> 
> Best regards, 
> 
> ir. Thierry Onkelinx 
> Statisticus / Statistician 
> 
> Vlaamse Overheid / Government of Flanders 
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND 
> FOREST 
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance 
> thierry.onkelinx at inbo.be 
> Havenlaan 88 bus 73, 1000 Brussel 
> www.inbo.be 
> 
> /////////////////////////////////////////////////////////////////////////////////////////// 
> To call in the statistician after the experiment is done may be no more 
> than asking him to perform a post-mortem examination: he may be able to say 
> what the experiment died of. ~ Sir Ronald Aylmer Fisher 
> The plural of anecdote is not data. ~ Roger Brinner 
> The combination of some data and an aching desire for an answer does not 
> ensure that a reasonable answer can be extracted from a given body of data. 
> ~ John Tukey 
> /////////////////////////////////////////////////////////////////////////////////////////// 
> 
> <https://www.inbo.be> 
> 
> 
> Op ma 31 okt. 2022 om 18:55 schreef Brian Gill <briangillphd at gmail.com>: 
> 
>> I have three locations (Sites) where I repeatedly measured a number of 
>> environmental variables (X1, X2, X3) and a response (Y; normally 
>> distributed) over time. That is, I have data on each environmental variable 
>> and the response at many time points for each of 3 sites. For each 
>> timepoints all three sites were sampled. 
>> 
>> I want to model the response (Y) as a function of the environmental 
>> variables (X1, X2, X3) while controlling for effects of Sites and Time. I 
>> expect responses from the same site to be similar because they come from 
>> the same location and responses measured at closer timepoints to be more 
>> similar than those separated by more time. 
>> 
>> Can people please advise on an appropriate model specification. 
>> 
>> I've come up with the following so far: 
>> 
>> Y ~ Site + X1 + X2 + X3 + (1 | Date) 
>> 
>> Y ~ X1 + X2 + X3 + (1 | Site) + (1 | Date) 
>> 
>> My hangups are that I think these models treat Date categorically 
>> (controlling for variation from a particular date, but not how close or far 
>> dates are from each other). Also, a model allowing both random intercepts 
>> and slopes might be better as responses could vary significantly in 
>> magnitude and direction among sites. 
>> 
>> Any advice would be appreciated. Thanks! 
>> 
>> [[alternative HTML version deleted]] 
>> 
>> _______________________________________________ 
>> R-sig-mixed-models at r-project.org mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models 
>> 
> [[alternative HTML version deleted]] 
> 
> _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models 
> 
> [[alternative HTML version deleted]] 
> 
> 
> _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models 

	[[alternative HTML version deleted]]


From norm@n@d@ure||e @end|ng |rom @grop@r|@tech@|r  Tue Nov  8 17:30:12 2022
From: norm@n@d@ure||e @end|ng |rom @grop@r|@tech@|r (Norman DAURELLE)
Date: Tue, 8 Nov 2022 17:30:12 +0100 (CET)
Subject: [R-sig-ME] random effect variance greater than output variable
 variance
Message-ID: <1438439530.16137176.1667925012732.JavaMail.zimbra@agroparistech.fr>


Dear list members, 

I used a mixed-effect linear model to estimate the effect of a disease on the yield of a crop, 
and used a formula that was as follows : 

Y ~ X + R1 + R2 + (1|year) + (1|location) + (1|cultivar) 

where for each observation : 

Y is the yield of the crop , 
X the average disease severity in the field, 
R1 and R2 the rainfall values in the 1st and 2nd part of the growing season respectively, 
and year, location and cultivar, the year location and cultivar of the observation. 

I have 5 years, 16 locations and a lot of cultivars, with an unbalanced experiment design. 

The variance given in the summary for the factor Location is greater than the variance of the yield variable taken by itself, and this surprises me. 

I wanted to show the relative importance of each factor over yield through a Venn diagram presenting the variances of each factor as part of the overall yield variance, with each factor's variance overlapping with the others', but the fact that the variance associated with a factor is greater than the variance of the output variable makes me doubt my understanding of the variances shown in a summary for a mixed-effect model. 

Would someone have a simple explanation of what exactly these variances represent ? 

I thought that for a factor with N levels, you had V= ( ? (xi-?)? ) / N, with i = 1,..., N, and xi the output variable's mean in the i-th level of the factor, and ? the overall output variable's mean. 

Is this not how the variance for a random effect is computed ? 

Thanks for any answer ! 

Cheers, 

Norman 






	[[alternative HTML version deleted]]


From mozdur| @end|ng |rom u@|bert@@c@  Tue Nov  8 20:19:27 2022
From: mozdur| @end|ng |rom u@|bert@@c@ (Zohre Mozduri)
Date: Tue, 8 Nov 2022 11:19:27 -0800
Subject: [R-sig-ME] lme4, question
Message-ID: <CAGoZ=joS6DeJLyoeRX8_7sYoT5bQD-aYu1KspBqP0-SLOBvUww@mail.gmail.com>

 Hi
I read manual of lme4 package and I have some question
I have first hand raw data about meat quality in pig (phenotypes or traits,
fixed effect and random effects). before moving to the correlation analysis
between traits, I need to the basic quality check. I want know my fixed and
random effects have significant effect on my traits or not, If they have I
will keep them or not I will remove them.
my question is how can and with which code in manual of lme4 I can do thah?

how can I use lme4 for test of significancy of fixed and random effect on
my traits? which page of manual writes these codes?
after that with which codes I can adjust my phenotype based on significance
of fixed and random effect?


Best Regards

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Wed Nov  9 09:34:07 2022
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Wed, 9 Nov 2022 09:34:07 +0100
Subject: [R-sig-ME] random effect variance greater than output variable
 variance
In-Reply-To: <1438439530.16137176.1667925012732.JavaMail.zimbra@agroparistech.fr>
References: <1438439530.16137176.1667925012732.JavaMail.zimbra@agroparistech.fr>
Message-ID: <CAJuCY5xjCwisaGAqmmxp5+YgR2NkS8t2dQZ5YDkY4JUnP5jPHA@mail.gmail.com>

Dear Norman,

Can you show us the full code of the lme4 call and the output of
summary(model). How did you calculate the variances for Y and the random
effect?

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op di 8 nov. 2022 om 17:37 schreef Norman DAURELLE via R-sig-mixed-models <
r-sig-mixed-models at r-project.org>:

>
> Dear list members,
>
> I used a mixed-effect linear model to estimate the effect of a disease on
> the yield of a crop,
> and used a formula that was as follows :
>
> Y ~ X + R1 + R2 + (1|year) + (1|location) + (1|cultivar)
>
> where for each observation :
>
> Y is the yield of the crop ,
> X the average disease severity in the field,
> R1 and R2 the rainfall values in the 1st and 2nd part of the growing
> season respectively,
> and year, location and cultivar, the year location and cultivar of the
> observation.
>
> I have 5 years, 16 locations and a lot of cultivars, with an unbalanced
> experiment design.
>
> The variance given in the summary for the factor Location is greater than
> the variance of the yield variable taken by itself, and this surprises me.
>
> I wanted to show the relative importance of each factor over yield through
> a Venn diagram presenting the variances of each factor as part of the
> overall yield variance, with each factor's variance overlapping with the
> others', but the fact that the variance associated with a factor is greater
> than the variance of the output variable makes me doubt my understanding of
> the variances shown in a summary for a mixed-effect model.
>
> Would someone have a simple explanation of what exactly these variances
> represent ?
>
> I thought that for a factor with N levels, you had V= ( ? (xi-?)? ) / N,
> with i = 1,..., N, and xi the output variable's mean in the i-th level of
> the factor, and ? the overall output variable's mean.
>
> Is this not how the variance for a random effect is computed ?
>
> Thanks for any answer !
>
> Cheers,
>
> Norman
>
>
>
>
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From benpe|zer @end|ng |rom gm@||@com  Wed Nov  9 10:29:11 2022
From: benpe|zer @end|ng |rom gm@||@com (ben pelzer)
Date: Wed, 9 Nov 2022 10:29:11 +0100
Subject: [R-sig-ME] random effect variance greater than output variable
 variance
Message-ID: <CAFgPNS957H=M6Rc+_mk_zqzWeR1mVwJgZm-kC+WW0JnwRsLw_Q@mail.gmail.com>

 Dear Norman,

The random intercepts in your model are related to (as always) value zero
of your predictors, i.e. X=0, R1=0 and R2=0. These 0-values may be far out
of the actual or possible range of values. This means that the intercept
variance is about variance between locations, say, with zero disease and
zero rainfall in both seasons. If you do not want this, rescale X, R1 and
R2 so that value zero is "in range".

Further, and that is also a reason that I respond, I was wondering if it is
a good idea to estimate random effects if there are so few units, like only
five "fixed" years e.g. A.f.a.i.k. one should have at least 20 units or so,
but maybe you or someone else could correct me.

Regards, Ben.

	[[alternative HTML version deleted]]


From norm@n@d@ure||e @end|ng |rom @grop@r|@tech@|r  Wed Nov  9 21:33:12 2022
From: norm@n@d@ure||e @end|ng |rom @grop@r|@tech@|r (Norman DAURELLE)
Date: Wed, 9 Nov 2022 21:33:12 +0100 (CET)
Subject: [R-sig-ME] random effect variance greater than output variable
 variance
In-Reply-To: <CAJuCY5xjCwisaGAqmmxp5+YgR2NkS8t2dQZ5YDkY4JUnP5jPHA@mail.gmail.com>
References: <1438439530.16137176.1667925012732.JavaMail.zimbra@agroparistech.fr>
 <CAJuCY5xjCwisaGAqmmxp5+YgR2NkS8t2dQZ5YDkY4JUnP5jPHA@mail.gmail.com>
Message-ID: <324824269.17652769.1668025992800.JavaMail.zimbra@agroparistech.fr>


Dear Thierry, 

i used these lines : 

MELM.1 <- lmer(Yield..kg.Ha. ~ Rep.severity.means + Long.term.Apr.Jun + Long.term.total 
+ (1|Location) + (1|Year) + (1|Variety), 
data = yield.disease.rainfall.df) 

summary(MELM.1) 

and compared the outputs of the summary 

summary(MELM.1) 
Linear mixed model fit by REML ['lmerMod'] 
Formula: Yield..kg.Ha. ~ Rep.severity.means + Long.term.Apr.Jun + Long.term.total + 
(1 | Location) + (1 | Year) + (1 | Variety) 
Data: yield.disease.rainfall.df 

REML criterion at convergence: 19679.6 

Scaled residuals: 
Min 1Q Median 3Q Max 
-4.1926 -0.5998 -0.0246 0.5572 5.0190 

Random effects: 
Groups Name Variance Std.Dev. 
Variety (Intercept) 106888 326.9 
Location (Intercept) 512674 716.0 
Year (Intercept) 15724 125.4 
Residual 109754 331.3 
Number of obs: 1352, groups: Variety, 22; Location, 16; Year, 4 

Fixed effects: 
Estimate Std. Error t value 
(Intercept) 160.9075 236.6696 0.680 
Rep.severity.means -3.7333 0.6512 -5.733 
Long.term.Apr.Jun -10.1864 0.8009 -12.719 
Long.term.total 9.8103 0.4631 21.182 

Correlation of Fixed Effects: 
(Intr) Rp.sv. L..A.J 
Rp.svrty.mn -0.038 
Lng.trm.A.J -0.061 -0.061 
Lng.trm.ttl -0.314 0.016 -0.699 

to var() of my output variable : 

> var(yield.disease.rainfall.df$Yield..kg.Ha.) 
[1] 435938 

and it bothers me that this variance is inferior to the one of the location factor reported for random effects in the summary, because it prevents me from using the method I wanted to use to show the results. I wanted to show how much each factor (year, location, and variety/cultivar) influences yield outside of disease severity and rainfalls. 

Do I not understand what these variance values mean for the random effects in the summary ? 
Can it not be compared to the var() of my variable of interest ? 

Thanks ! 

Norman 




De: "Thierry Onkelinx" <thierry.onkelinx at inbo.be> 
?: "Norman DAURELLE" <norman.daurelle at agroparistech.fr> 
Cc: "r-sig-mixed-models" <r-sig-mixed-models at r-project.org> 
Envoy?: Mercredi 9 Novembre 2022 09:34:07 
Objet: Re: [R-sig-ME] random effect variance greater than output variable variance 

Dear Norman, 

Can you show us the full code of the lme4 call and the output of summary(model). How did you calculate the variances for Y and the random effect? 

Best regards, 

ir. Thierry Onkelinx 
Statisticus / Statistician 

Vlaamse Overheid / Government of Flanders 
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST 
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance 
[ mailto:thierry.onkelinx at inbo.be | thierry.onkelinx at inbo.be ] 
Havenlaan 88 bus 73, 1000 Brussel 
[ http://www.inbo.be/ | www.inbo.be ] 

/////////////////////////////////////////////////////////////////////////////////////////// 
To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher 
The plural of anecdote is not data. ~ Roger Brinner 
The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey 
/////////////////////////////////////////////////////////////////////////////////////////// 

[ https://www.inbo.be/ ] 


Op di 8 nov. 2022 om 17:37 schreef Norman DAURELLE via R-sig-mixed-models < [ mailto:r-sig-mixed-models at r-project.org | r-sig-mixed-models at r-project.org ] >: 



Dear list members, 

I used a mixed-effect linear model to estimate the effect of a disease on the yield of a crop, 
and used a formula that was as follows : 

Y ~ X + R1 + R2 + (1|year) + (1|location) + (1|cultivar) 

where for each observation : 

Y is the yield of the crop , 
X the average disease severity in the field, 
R1 and R2 the rainfall values in the 1st and 2nd part of the growing season respectively, 
and year, location and cultivar, the year location and cultivar of the observation. 

I have 5 years, 16 locations and a lot of cultivars, with an unbalanced experiment design. 

The variance given in the summary for the factor Location is greater than the variance of the yield variable taken by itself, and this surprises me. 

I wanted to show the relative importance of each factor over yield through a Venn diagram presenting the variances of each factor as part of the overall yield variance, with each factor's variance overlapping with the others', but the fact that the variance associated with a factor is greater than the variance of the output variable makes me doubt my understanding of the variances shown in a summary for a mixed-effect model. 

Would someone have a simple explanation of what exactly these variances represent ? 

I thought that for a factor with N levels, you had V= ( ? (xi-?)? ) / N, with i = 1,..., N, and xi the output variable's mean in the i-th level of the factor, and ? the overall output variable's mean.

Is this not how the variance for a random effect is computed ? 

Thanks for any answer ! 

Cheers, 

Norman 






[[alternative HTML version deleted]] 

_______________________________________________ 
[ mailto:R-sig-mixed-models at r-project.org | R-sig-mixed-models at r-project.org ] mailing list 
[ https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models | https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models ] 





	[[alternative HTML version deleted]]


From norm@n@d@ure||e @end|ng |rom @grop@r|@tech@|r  Wed Nov  9 22:05:41 2022
From: norm@n@d@ure||e @end|ng |rom @grop@r|@tech@|r (Norman DAURELLE)
Date: Wed, 9 Nov 2022 22:05:41 +0100 (CET)
Subject: [R-sig-ME] random effect variance greater than output variable
 variance
In-Reply-To: <CAFgPNS957H=M6Rc+_mk_zqzWeR1mVwJgZm-kC+WW0JnwRsLw_Q@mail.gmail.com>
References: <CAFgPNS957H=M6Rc+_mk_zqzWeR1mVwJgZm-kC+WW0JnwRsLw_Q@mail.gmail.com>
Message-ID: <1211849586.17667994.1668027941669.JavaMail.zimbra@agroparistech.fr>


Dear Ben, 

is there something important about the variance being attached to the intercept ? 

If a factor influences the output variable, the deviation it accounts for is the same when it is applied to the intercept or not (meaning wether X, R1 and R2 = 0) isn't it ? 

Concerning the number of years, I mainly set them as random effects because I thought you used random effects when you know that you do not observe all levels. 

additionnally, Paul Johnson wrote this in a recent previous answer : 

[ The minimum number of blocks/groups required to support a random effect is discussed in Ben Bolker's GLMM FAQ wiki: 

https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#should-i-treat-factor-xxx-as-fixed-or-random 

"One point of particular relevance to ?modern? mixed model estimation (rather than ?classical? method-of-moments estimation) is that, for practical purposes, there must be a reasonable number of random-effects levels (e.g. blocks) ? more than 5 or 6 at a minimum." ] 

So, if you are of the opinion that practicality prevails over philosophy about when a variable should be used as a random or fixed effect, apparently 5 is ok. 

It may be worth noting that I don't wish to predict the outcome, simply to describe the effct of my fixed effects on the output. 

And I think that using year as a fixed or a random effect changes the values for the fixed effects (disease and rainfall), so I would actually rather have a reasoning that makes theoretical good sense than something that is pragmatic for use behind the treatment of the year factor. 

Regards, 

Norman 




De: "ben pelzer" <benpelzer at gmail.com> 
?: "r-sig-mixed-models" <r-sig-mixed-models at r-project.org> 
Envoy?: Mercredi 9 Novembre 2022 10:29:11 
Objet: Re: [R-sig-ME] random effect variance greater than output variable variance 

Dear Norman, 

The random intercepts in your model are related to (as always) value zero 
of your predictors, i.e. X=0, R1=0 and R2=0. These 0-values may be far out 
of the actual or possible range of values. This means that the intercept 
variance is about variance between locations, say, with zero disease and 
zero rainfall in both seasons. If you do not want this, rescale X, R1 and 
R2 so that value zero is "in range". 

Further, and that is also a reason that I respond, I was wondering if it is 
a good idea to estimate random effects if there are so few units, like only 
five "fixed" years e.g. A.f.a.i.k. one should have at least 20 units or so, 
but maybe you or someone else could correct me. 

Regards, Ben. 

[[alternative HTML version deleted]] 

_______________________________________________ 
R-sig-mixed-models at r-project.org mailing list 
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models 

	[[alternative HTML version deleted]]


From benpe|zer @end|ng |rom gm@||@com  Thu Nov 10 09:31:26 2022
From: benpe|zer @end|ng |rom gm@||@com (ben pelzer)
Date: Thu, 10 Nov 2022 09:31:26 +0100
Subject: [R-sig-ME] random effect variance greater than output variable
 variance
In-Reply-To: <1211849586.17667994.1668027941669.JavaMail.zimbra@agroparistech.fr>
References: <CAFgPNS957H=M6Rc+_mk_zqzWeR1mVwJgZm-kC+WW0JnwRsLw_Q@mail.gmail.com>
 <1211849586.17667994.1668027941669.JavaMail.zimbra@agroparistech.fr>
Message-ID: <CAFgPNS_YWpFkQAWwLBdRvgRypymNVtdUi_WG9mE23ynu2vn06g@mail.gmail.com>

Dear Norman,

Sorry, I mixed up things. In case of a random slope of an X variable it can
happen that the variance across "schools", say, is very high because it
refers to the variance between "schools" for X=0. But you do NOT have
random slopes of disease and rainfall, so my remark was not appropriate.

With the summary results, I now see what your problem is, and I have no
good answer. One thing however is this. The three random intercept
variances of location etc. refer to any given combination of X, R1 and R2.
I believe these should be interpreted as conditional variances, conditional
on the values of X, R1 and R2, that is. This is different from the total
variance of your dependent Y, which you get by using var(Y). Suppose you
run a null-model, with only the random effects and no fixed-effect
predictors X, R1 and R2, then you could compare the sum of the four
variances (incl. the residual variance ) with the one obtained by var(Y).

In general, these two variances can differ. In a "schools" example with
different nr. of pupils in a number of schools, the null-model variance
better suits the two-stage sampling design: at random sample schools,
within each school sample pupils.

Thanks for pointing me to the blog of Ben Bolker, this is great and useful
stuff.

Cheers, Ben.

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Thu Nov 10 10:19:02 2022
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Thu, 10 Nov 2022 10:19:02 +0100
Subject: [R-sig-ME] random effect variance greater than output variable
 variance
In-Reply-To: <324824269.17652769.1668025992800.JavaMail.zimbra@agroparistech.fr>
References: <1438439530.16137176.1667925012732.JavaMail.zimbra@agroparistech.fr>
 <CAJuCY5xjCwisaGAqmmxp5+YgR2NkS8t2dQZ5YDkY4JUnP5jPHA@mail.gmail.com>
 <324824269.17652769.1668025992800.JavaMail.zimbra@agroparistech.fr>
Message-ID: <CAJuCY5wBRk4zGBKATcO0kj3j=rOYoaBsBfg6napbgUACxZ+SJw@mail.gmail.com>

Dear Norman,

I think this might be due to the unbalance in your design. You need to
inspect the BLUP of the random effects. Look for the extremes in location
and variety. I would expect some combinations with an extreme positive
(negative) location effect compensated by an extreme negative (positive)
variety effect.

Furthermore look into the fixed effects. Long.term apr-jun is highly
correlated with long.term total. Their effects cancel each other to a
certain extent. I recommend to replace long.term total with its difference
with long.term apr-jun.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op wo 9 nov. 2022 om 21:40 schreef Norman DAURELLE <
norman.daurelle at agroparistech.fr>:

>
> Dear Thierry,
>
> i used these lines :
>
> MELM.1 <- lmer(Yield..kg.Ha. ~ Rep.severity.means + Long.term.Apr.Jun +
> Long.term.total
>                + (1|Location) + (1|Year) + (1|Variety),
>                data = yield.disease.rainfall.df)
>
> summary(MELM.1)
>
> and compared the outputs of the summary
>
>  summary(MELM.1)
> Linear mixed model fit by REML ['lmerMod']
> Formula: Yield..kg.Ha. ~ Rep.severity.means + Long.term.Apr.Jun +
> Long.term.total +
>     (1 | Location) + (1 | Year) + (1 | Variety)
>    Data: yield.disease.rainfall.df
>
> REML criterion at convergence: 19679.6
>
> Scaled residuals:
>     Min      1Q  Median      3Q     Max
> -4.1926 -0.5998 -0.0246  0.5572  5.0190
>
> Random effects:
>  Groups   Name        Variance Std.Dev.
>  Variety  (Intercept) 106888   326.9
>  Location (Intercept) 512674   716.0
>  Year     (Intercept)  15724   125.4
>  Residual             109754   331.3
> Number of obs: 1352, groups:  Variety, 22; Location, 16; Year, 4
>
> Fixed effects:
>                    Estimate Std. Error t value
> (Intercept)        160.9075   236.6696   0.680
> Rep.severity.means  -3.7333     0.6512  -5.733
> Long.term.Apr.Jun  -10.1864     0.8009 -12.719
> Long.term.total      9.8103     0.4631  21.182
>
> Correlation of Fixed Effects:
>             (Intr) Rp.sv. L..A.J
> Rp.svrty.mn -0.038
> Lng.trm.A.J -0.061 -0.061
> Lng.trm.ttl -0.314  0.016 -0.699
>
> to var() of my output variable :
>
> > var(yield.disease.rainfall.df$Yield..kg.Ha.)
> [1] 435938
>
> and it bothers me that this variance is inferior to the one of the
> location factor reported for random effects in the summary, because it
> prevents me from using the method I wanted to use to show the results. I
> wanted to show how much each factor (year, location, and variety/cultivar)
> influences yield outside of disease severity and rainfalls.
>
> Do I not understand what these variance values mean for the random effects
> in the summary ?
> Can it not be compared to the var() of my variable of interest ?
>
> Thanks !
>
> Norman
>
>
>
> ------------------------------
> *De: *"Thierry Onkelinx" <thierry.onkelinx at inbo.be>
> *?: *"Norman DAURELLE" <norman.daurelle at agroparistech.fr>
> *Cc: *"r-sig-mixed-models" <r-sig-mixed-models at r-project.org>
> *Envoy?: *Mercredi 9 Novembre 2022 09:34:07
> *Objet: *Re: [R-sig-ME] random effect variance greater than output
> variable variance
>
> Dear Norman,
>
> Can you show us the full code of the lme4 call and the output of
> summary(model). How did you calculate the variances for Y and the random
> effect?
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op di 8 nov. 2022 om 17:37 schreef Norman DAURELLE via R-sig-mixed-models <
> r-sig-mixed-models at r-project.org>:
>
>>
>> Dear list members,
>>
>> I used a mixed-effect linear model to estimate the effect of a disease on
>> the yield of a crop,
>> and used a formula that was as follows :
>>
>> Y ~ X + R1 + R2 + (1|year) + (1|location) + (1|cultivar)
>>
>> where for each observation :
>>
>> Y is the yield of the crop ,
>> X the average disease severity in the field,
>> R1 and R2 the rainfall values in the 1st and 2nd part of the growing
>> season respectively,
>> and year, location and cultivar, the year location and cultivar of the
>> observation.
>>
>> I have 5 years, 16 locations and a lot of cultivars, with an unbalanced
>> experiment design.
>>
>> The variance given in the summary for the factor Location is greater than
>> the variance of the yield variable taken by itself, and this surprises me.
>>
>> I wanted to show the relative importance of each factor over yield
>> through a Venn diagram presenting the variances of each factor as part of
>> the overall yield variance, with each factor's variance overlapping with
>> the others', but the fact that the variance associated with a factor is
>> greater than the variance of the output variable makes me doubt my
>> understanding of the variances shown in a summary for a mixed-effect model.
>>
>> Would someone have a simple explanation of what exactly these variances
>> represent ?
>>
>> I thought that for a factor with N levels, you had V= ( ? (xi-?)? ) / N,
>> with i = 1,..., N, and xi the output variable's mean in the i-th level of
>> the factor, and ? the overall output variable's mean.
>>
>> Is this not how the variance for a random effect is computed ?
>>
>> Thanks for any answer !
>>
>> Cheers,
>>
>> Norman
>>
>>
>>
>>
>>
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>

	[[alternative HTML version deleted]]


From b@t|c @end|ng |rom |eed@@@c@uk  Thu Nov 10 12:32:36 2022
From: b@t|c @end|ng |rom |eed@@@c@uk (Tara Cox [RPG])
Date: Thu, 10 Nov 2022 11:32:36 +0000
Subject: [R-sig-ME] Plotting MCMCglmm model predict line
Message-ID: <AS8PR03MB684060A2533DE0F333E0612DF7389@AS8PR03MB6840.eurprd03.prod.outlook.com>

Dear list,

I was hoping to get some help with a query regarding the predict.MCMCglmm() function.

I have run a univariate MCMCglmm with behaviour score as the response variable (Poisson distribution) and five fixed effects (age, age squared, sex, test number and room colour). I would like to visualize a significant quadratic age effect by plotting the model predict line and corresponding 95% credible intervals over the raw values.

I have tried to do this using predict.MCMCglmm and ggplot2, but this produces a 'jaggered' output (plot attached). Therefore I was hoping someone could help shed some light on what I might be doing wrong.

Below are my model specification and the code for plotting the model predict line:

model<- MCMCglmm(BehaviourScore~
                     Age_years_integer+
                     Age_years2_integer+
????????? Sex +
                     TestNumber +
??????????RoomColour,
                 random=~BirdID+ObserverID,
                 nitt=1260000,
????????burnin=60000,
????????thin=300,
                 verbose=FALSE,
                 pr=TRUE,
                 family="poisson",
                 data=Behaviour_data)

mpred <- predict(model, interval="confidence", marginal=~ BirdID + ObserverID)
dataf <- data.frame(Behaviour_data, mpred)

ggplot(dataf , aes(x=Age_years_integer, y=BehaviourScore))+
 geom_point(colour="grey6", alpha=0.30) +
  xlab("Age (years)") + ylab("Behaviour score (integer)") + theme_classic() +
  geom_line(data=dataf, aes(x=Age_years_integer, y=fit)) +
  geom_ribbon(data=dataf, aes(x=as.numeric(Age_years_integer), y=fit,ymin=lwr, ymax=upr),fill="red",alpha=0.3)


Apologies if this is a straightforward question, I have tried googling the issue but have had no luck.

Many thanks,
Tara


Tara Cox

Pronouns: she, her

PhD researcher

Dugdale group, School of Biology

Faculty of Biological Sciences

University of Leeds



-------------- next part --------------
A non-text attachment was scrubbed...
Name: AgeEffect.png
Type: image/png
Size: 7050 bytes
Desc: AgeEffect.png
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20221110/7d782979/attachment.png>

From w@||dm@w@@@10 @end|ng |rom gm@||@com  Thu Nov 10 17:19:42 2022
From: w@||dm@w@@@10 @end|ng |rom gm@||@com (Walid Mawass)
Date: Thu, 10 Nov 2022 09:19:42 -0700
Subject: [R-sig-ME] Plotting MCMCglmm model predict line
In-Reply-To: <AS8PR03MB684060A2533DE0F333E0612DF7389@AS8PR03MB6840.eurprd03.prod.outlook.com>
References: <AS8PR03MB684060A2533DE0F333E0612DF7389@AS8PR03MB6840.eurprd03.prod.outlook.com>
Message-ID: <CAJtCY7V_s1daMd8vB6s4uf4NkHJFALRNnXCh57bNunZx8zd9Nw@mail.gmail.com>

Hi Tara,

I see two possible things that might be giving you this plot. First, when
you are using geom_line, to draw a regression line, the argument
stat="smooth" is required. If it is not included, I think the function just
connects the predicted points with lines.
Second, you mentioned you want to plot the quadratic effect, to do that
with geom_line, you should add the formula. For quadratic: formula = y ~
x + I(x^2).

Another suggestion would be to limit your yaxis using ylim(min, max) since
it is flattening the data visually.

Hope this helps.
-- 
Walid Mawass
Ph.D. in Evolutionary Biology - UQTR
Postdoctoral Research Associate
Masel Lab - University of Arizona


On Thu, Nov 10, 2022 at 4:32 AM Tara Cox [RPG] <bstlc at leeds.ac.uk> wrote:

> Dear list,
>
> I was hoping to get some help with a query regarding the
> predict.MCMCglmm() function.
>
> I have run a univariate MCMCglmm with behaviour score as the response
> variable (Poisson distribution) and five fixed effects (age, age squared,
> sex, test number and room colour). I would like to visualize a significant
> quadratic age effect by plotting the model predict line and corresponding
> 95% credible intervals over the raw values.
>
> I have tried to do this using predict.MCMCglmm and ggplot2, but this
> produces a 'jaggered' output (plot attached). Therefore I was hoping
> someone could help shed some light on what I might be doing wrong.
>
> Below are my model specification and the code for plotting the model
> predict line:
>
> model<- MCMCglmm(BehaviourScore~
>                      Age_years_integer+
>                      Age_years2_integer+
>  Sex +
>                      TestNumber +
> RoomColour,
>                  random=~BirdID+ObserverID,
>                  nitt=1260000,
> burnin=60000,
> thin=300,
>                  verbose=FALSE,
>                  pr=TRUE,
>                  family="poisson",
>                  data=Behaviour_data)
>
> mpred <- predict(model, interval="confidence", marginal=~ BirdID +
> ObserverID)
> dataf <- data.frame(Behaviour_data, mpred)
>
> ggplot(dataf , aes(x=Age_years_integer, y=BehaviourScore))+
>  geom_point(colour="grey6", alpha=0.30) +
>   xlab("Age (years)") + ylab("Behaviour score (integer)") +
> theme_classic() +
>   geom_line(data=dataf, aes(x=Age_years_integer, y=fit)) +
>   geom_ribbon(data=dataf, aes(x=as.numeric(Age_years_integer),
> y=fit,ymin=lwr, ymax=upr),fill="red",alpha=0.3)
>
>
> Apologies if this is a straightforward question, I have tried googling the
> issue but have had no luck.
>
> Many thanks,
> Tara
>
>
> Tara Cox
>
> Pronouns: she, her
>
> PhD researcher
>
> Dugdale group, School of Biology
>
> Faculty of Biological Sciences
>
> University of Leeds
>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From jh@|t|g@ @end|ng |rom gm@||@com  Sat Nov 12 07:53:57 2022
From: jh@|t|g@ @end|ng |rom gm@||@com (J.D. Haltigan)
Date: Sat, 12 Nov 2022 01:53:57 -0500
Subject: [R-sig-ME] random effect variance greater than output variable
 variance
In-Reply-To: <CAFgPNS_YWpFkQAWwLBdRvgRypymNVtdUi_WG9mE23ynu2vn06g@mail.gmail.com>
References: <CAFgPNS957H=M6Rc+_mk_zqzWeR1mVwJgZm-kC+WW0JnwRsLw_Q@mail.gmail.com>
 <1211849586.17667994.1668027941669.JavaMail.zimbra@agroparistech.fr>
 <CAFgPNS_YWpFkQAWwLBdRvgRypymNVtdUi_WG9mE23ynu2vn06g@mail.gmail.com>
Message-ID: <CAH_7VOmc8uc6WGOeO8Soc12o9RBhwwUQ=6_YYf0g7_9bH=hpLg@mail.gmail.com>

This is the between/within variance issue in multilevel modeling. Take a
look at:

Hamaker EL, Muth?n B. The fixed versus random effects debate and how it
relates to centering in multilevel modeling. *Psychological Methods* 2020;
*25*:365.


Bell A, Fairbrother M, Jones K. Fixed and random effects models: making an
informed choice. *Quality & Quantity* 2019;*53*:1051?74.

On Thu, Nov 10, 2022 at 3:31 AM ben pelzer <benpelzer at gmail.com> wrote:

> Dear Norman,
>
> Sorry, I mixed up things. In case of a random slope of an X variable it can
> happen that the variance across "schools", say, is very high because it
> refers to the variance between "schools" for X=0. But you do NOT have
> random slopes of disease and rainfall, so my remark was not appropriate.
>
> With the summary results, I now see what your problem is, and I have no
> good answer. One thing however is this. The three random intercept
> variances of location etc. refer to any given combination of X, R1 and R2.
> I believe these should be interpreted as conditional variances, conditional
> on the values of X, R1 and R2, that is. This is different from the total
> variance of your dependent Y, which you get by using var(Y). Suppose you
> run a null-model, with only the random effects and no fixed-effect
> predictors X, R1 and R2, then you could compare the sum of the four
> variances (incl. the residual variance ) with the one obtained by var(Y).
>
> In general, these two variances can differ. In a "schools" example with
> different nr. of pupils in a number of schools, the null-model variance
> better suits the two-stage sampling design: at random sample schools,
> within each school sample pupils.
>
> Thanks for pointing me to the blog of Ben Bolker, this is great and useful
> stuff.
>
> Cheers, Ben.
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From t|m@co|e @end|ng |rom uc|@@c@uk  Mon Nov 14 11:47:44 2022
From: t|m@co|e @end|ng |rom uc|@@c@uk (Cole, Tim)
Date: Mon, 14 Nov 2022 10:47:44 +0000
Subject: [R-sig-ME] nlme update query
Message-ID: <AM6PR01MB5158C368E4B02388C5CAB91AC7059@AM6PR01MB5158.eurprd01.prod.exchangelabs.com>

I fit mixed effects models in nlme using my sitar package, and I am puzzled that some models which fitted fine at the start of November no longer converge. Nothing has changed at my end, which makes me wonder if nlme has altered its behaviour somehow ? I did update it at one point.

Have there been any recent substantive changes to nlme?

Thanks,
Tim

Population, Policy and Practice Programme
UCL Great Ormond Street Institute of Child Health
30 Guilford Street, London WC1N 1EH

	[[alternative HTML version deleted]]


From joh@nne@@r@nke @end|ng |rom jrwb@de  Mon Nov 14 15:44:45 2022
From: joh@nne@@r@nke @end|ng |rom jrwb@de (Johannes Ranke)
Date: Mon, 14 Nov 2022 15:44:45 +0100
Subject: [R-sig-ME] nlme update query
In-Reply-To: <AM6PR01MB5158C368E4B02388C5CAB91AC7059@AM6PR01MB5158.eurprd01.prod.exchangelabs.com>
References: <AM6PR01MB5158C368E4B02388C5CAB91AC7059@AM6PR01MB5158.eurprd01.prod.exchangelabs.com>
Message-ID: <2755584.PVRX5i4E1Y@ryz>

Hi Tim,

There is a Changelog on CRAN:

  https://cran.r-project.org/web/packages/nlme/ChangeLog 

Cheers,

Johannes

Am Montag, 14. November 2022, 11:47:44 CET schrieb Cole, Tim:
> I fit mixed effects models in nlme using my sitar package, and I am puzzled
> that some models which fitted fine at the start of November no longer
> converge. Nothing has changed at my end, which makes me wonder if nlme has
> altered its behaviour somehow ? I did update it at one point.
> 
> Have there been any recent substantive changes to nlme?
> 
> Thanks,
> Tim
> 
> Population, Policy and Practice Programme
> UCL Great Ormond Street Institute of Child Health
> 30 Guilford Street, London WC1N 1EH
> 
> 	[[alternative HTML version deleted]]


	[[alternative HTML version deleted]]


From norm@n@d@ure||e @end|ng |rom @grop@r|@tech@|r  Thu Nov 17 10:01:09 2022
From: norm@n@d@ure||e @end|ng |rom @grop@r|@tech@|r (Norman DAURELLE)
Date: Thu, 17 Nov 2022 10:01:09 +0100 (CET)
Subject: [R-sig-ME] random effect variance greater than output variable
 variance
In-Reply-To: <CAJuCY5wBRk4zGBKATcO0kj3j=rOYoaBsBfg6napbgUACxZ+SJw@mail.gmail.com>
References: <1438439530.16137176.1667925012732.JavaMail.zimbra@agroparistech.fr>
 <CAJuCY5xjCwisaGAqmmxp5+YgR2NkS8t2dQZ5YDkY4JUnP5jPHA@mail.gmail.com>
 <324824269.17652769.1668025992800.JavaMail.zimbra@agroparistech.fr>
 <CAJuCY5wBRk4zGBKATcO0kj3j=rOYoaBsBfg6napbgUACxZ+SJw@mail.gmail.com>
Message-ID: <1155559115.24085437.1668675669297.JavaMail.zimbra@agroparistech.fr>


Dear Thierry, 

thank you for your answer. You say "inspect the BLUP of the random effects". Does that mean using ranef() ? If not, then could you please explain what you mean ? I don't really understand it. 

Yes, you're probably right about the fixed effect of rainfall, thanks. 

regards, 

Norman 


De: "Thierry Onkelinx" <thierry.onkelinx at inbo.be> 
?: "Norman DAURELLE" <norman.daurelle at agroparistech.fr> 
Cc: "r-sig-mixed-models" <r-sig-mixed-models at r-project.org> 
Envoy?: Jeudi 10 Novembre 2022 10:19:02 
Objet: Re: [R-sig-ME] random effect variance greater than output variable variance 

Dear Norman, 

I think this might be due to the unbalance in your design. You need to inspect the BLUP of the random effects. Look for the extremes in location and variety. I would expect some combinations with an extreme positive (negative) location effect compensated by an extreme negative (positive) variety effect. 

Furthermore look into the fixed effects. Long.term apr-jun is highly correlated with long.term total. Their effects cancel each other to a certain extent. I recommend to replace long.term total with its difference with long.term apr-jun. 

Best regards, 

ir. Thierry Onkelinx 
Statisticus / Statistician 

Vlaamse Overheid / Government of Flanders 
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST 
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance 
[ mailto:thierry.onkelinx at inbo.be | thierry.onkelinx at inbo.be ] 
Havenlaan 88 bus 73, 1000 Brussel 
[ http://www.inbo.be/ | www.inbo.be ] 

/////////////////////////////////////////////////////////////////////////////////////////// 
To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher 
The plural of anecdote is not data. ~ Roger Brinner 
The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey 
/////////////////////////////////////////////////////////////////////////////////////////// 

[ https://www.inbo.be/ ] 


Op wo 9 nov. 2022 om 21:40 schreef Norman DAURELLE < [ mailto:norman.daurelle at agroparistech.fr | norman.daurelle at agroparistech.fr ] >: 




Dear Thierry, 

i used these lines : 

MELM.1 <- lmer(Yield..kg.Ha. ~ Rep.severity.means + Long.term.Apr.Jun + Long.term.total 
+ (1|Location) + (1|Year) + (1|Variety), 
data = yield.disease.rainfall.df) 

summary(MELM.1) 

and compared the outputs of the summary 

summary(MELM.1) 
Linear mixed model fit by REML ['lmerMod'] 
Formula: Yield..kg.Ha. ~ Rep.severity.means + Long.term.Apr.Jun + Long.term.total + 
(1 | Location) + (1 | Year) + (1 | Variety) 
Data: yield.disease.rainfall.df 

REML criterion at convergence: 19679.6 

Scaled residuals: 
Min 1Q Median 3Q Max 
-4.1926 -0.5998 -0.0246 0.5572 5.0190 

Random effects: 
Groups Name Variance Std.Dev. 
Variety (Intercept) 106888 326.9 
Location (Intercept) 512674 716.0 
Year (Intercept) 15724 125.4 
Residual 109754 331.3 
Number of obs: 1352, groups: Variety, 22; Location, 16; Year, 4 

Fixed effects: 
Estimate Std. Error t value 
(Intercept) 160.9075 236.6696 0.680 
Rep.severity.means -3.7333 0.6512 -5.733 
Long.term.Apr.Jun -10.1864 0.8009 -12.719 
Long.term.total 9.8103 0.4631 21.182 

Correlation of Fixed Effects: 
(Intr) Rp.sv. L..A.J 
[ http://rp.svrty.mn/ | Rp.svrty.mn ] -0.038 
Lng.trm.A.J -0.061 -0.061 
Lng.trm.ttl -0.314 0.016 -0.699 

to var() of my output variable : 

> var(yield.disease.rainfall.df$Yield..kg.Ha.) 
[1] 435938 

and it bothers me that this variance is inferior to the one of the location factor reported for random effects in the summary, because it prevents me from using the method I wanted to use to show the results. I wanted to show how much each factor (year, location, and variety/cultivar) influences yield outside of disease severity and rainfalls. 

Do I not understand what these variance values mean for the random effects in the summary ? 
Can it not be compared to the var() of my variable of interest ? 

Thanks ! 

Norman 




De: "Thierry Onkelinx" < [ mailto:thierry.onkelinx at inbo.be | thierry.onkelinx at inbo.be ] > 
?: "Norman DAURELLE" < [ mailto:norman.daurelle at agroparistech.fr | norman.daurelle at agroparistech.fr ] > 
Cc: "r-sig-mixed-models" < [ mailto:r-sig-mixed-models at r-project.org | r-sig-mixed-models at r-project.org ] > 
Envoy?: Mercredi 9 Novembre 2022 09:34:07 
Objet: Re: [R-sig-ME] random effect variance greater than output variable variance 

Dear Norman, 

Can you show us the full code of the lme4 call and the output of summary(model). How did you calculate the variances for Y and the random effect? 

Best regards, 

ir. Thierry Onkelinx 
Statisticus / Statistician 

Vlaamse Overheid / Government of Flanders 
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST 
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance 
[ mailto:thierry.onkelinx at inbo.be | thierry.onkelinx at inbo.be ] 
Havenlaan 88 bus 73, 1000 Brussel 
[ http://www.inbo.be/ | www.inbo.be ] 

/////////////////////////////////////////////////////////////////////////////////////////// 
To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher 
The plural of anecdote is not data. ~ Roger Brinner 
The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey 
/////////////////////////////////////////////////////////////////////////////////////////// 

[ https://www.inbo.be/ ] 


Op di 8 nov. 2022 om 17:37 schreef Norman DAURELLE via R-sig-mixed-models < [ mailto:r-sig-mixed-models at r-project.org | r-sig-mixed-models at r-project.org ] >: 

BQ_BEGIN

Dear list members, 

I used a mixed-effect linear model to estimate the effect of a disease on the yield of a crop, 
and used a formula that was as follows : 

Y ~ X + R1 + R2 + (1|year) + (1|location) + (1|cultivar) 

where for each observation : 

Y is the yield of the crop , 
X the average disease severity in the field, 
R1 and R2 the rainfall values in the 1st and 2nd part of the growing season respectively, 
and year, location and cultivar, the year location and cultivar of the observation. 

I have 5 years, 16 locations and a lot of cultivars, with an unbalanced experiment design. 

The variance given in the summary for the factor Location is greater than the variance of the yield variable taken by itself, and this surprises me. 

I wanted to show the relative importance of each factor over yield through a Venn diagram presenting the variances of each factor as part of the overall yield variance, with each factor's variance overlapping with the others', but the fact that the variance associated with a factor is greater than the variance of the output variable makes me doubt my understanding of the variances shown in a summary for a mixed-effect model. 

Would someone have a simple explanation of what exactly these variances represent ? 

I thought that for a factor with N levels, you had V= ( ? (xi-?)? ) / N, with i = 1,..., N, and xi the output variable's mean in the i-th level of the factor, and ? the overall output variable's mean.

Is this not how the variance for a random effect is computed ? 

Thanks for any answer ! 

Cheers, 

Norman 






[[alternative HTML version deleted]] 

_______________________________________________ 
[ mailto:R-sig-mixed-models at r-project.org | R-sig-mixed-models at r-project.org ] mailing list 
[ https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models | https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models ] 




BQ_END



	[[alternative HTML version deleted]]


From o||ve|r@r|@ue@b @end|ng |rom gm@||@com  Thu Nov 17 21:49:06 2022
From: o||ve|r@r|@ue@b @end|ng |rom gm@||@com (Rafael Lima Oliveira)
Date: Thu, 17 Nov 2022 17:49:06 -0300
Subject: [R-sig-ME] Report results for GLMM
Message-ID: <CAKmKTvH9O0t6ErZKJ5ssRKCmcASFds_W4Fr8v38dFBggwytJQQ@mail.gmail.com>

Dear list members,

I'm using gamma-based GLMMs to estimate the effect of a set of continuous
environmental variables (salinity, temperature, pH, dissolved oxygen,
turbidity and depth) on rarefied species richness and fish density along
four coastal environments.

I?m using the ?glmer? function from the ?lme4? package and  the link
function used was the logarithmic. For each locality I ran a full model and
after that, I applied a model selection using the dredge function from the
MuMIn package. After performing this procedure, I  selected five models
ranked by the smallest AIC, and its related delta (?AIC) and weight.

I'm looking at other papers in my field to find examples of what  I
should report
in my results section.

In my results I report two tables, one for each response variable with the
information from the best model (smallest AIC) from the dredge output and Anova
output (Analysis of Deviance table - Type II Wald chisquare test). Besides
that, I report effect plots only for each statistically significant
predictor variable. Should I report Pseudo-R-squared for the best model?
Instead of reporting  Anova table should I report a table with the five
models ranked by smallest AIC from the dredge output?


Thanks for any answer!
Rafael
-- 
*Rafael Lima Oliveira*
Doutorando em Biologia Animal
Universidade Federal do Esp?rito Santo - UFES
Laborat?rio de Ecologia de peixes marinhos - CEUNES/UFES
*Contato:* (75) 98873-1548 / (27) 99526-3612
*E-mail alternativo:* rafael.l.oliveira at edu.ufes.br
*Curr?culo Lattes*:  http://lattes.cnpq.br/5215941704013482

>)))?>               >)))?>               >)))?>                   >)))?>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Fri Nov 18 01:52:49 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Thu, 17 Nov 2022 19:52:49 -0500
Subject: [R-sig-ME] random effect variance greater than output variable
 variance
In-Reply-To: <1155559115.24085437.1668675669297.JavaMail.zimbra@agroparistech.fr>
References: <1438439530.16137176.1667925012732.JavaMail.zimbra@agroparistech.fr>
 <CAJuCY5xjCwisaGAqmmxp5+YgR2NkS8t2dQZ5YDkY4JUnP5jPHA@mail.gmail.com>
 <324824269.17652769.1668025992800.JavaMail.zimbra@agroparistech.fr>
 <CAJuCY5wBRk4zGBKATcO0kj3j=rOYoaBsBfg6napbgUACxZ+SJw@mail.gmail.com>
 <1155559115.24085437.1668675669297.JavaMail.zimbra@agroparistech.fr>
Message-ID: <22c4a1b1-4763-732f-47ed-a26752f5aebb@gmail.com>

   Yes, the BLUPs are the output of ranef().

On 2022-11-17 4:01 a.m., Norman DAURELLE via R-sig-mixed-models wrote:
> 
> Dear Thierry,
> 
> thank you for your answer. You say "inspect the BLUP of the random effects". Does that mean using ranef() ? If not, then could you please explain what you mean ? I don't really understand it.
> 
> Yes, you're probably right about the fixed effect of rainfall, thanks.
> 
> regards,
> 
> Norman
> 
> 
> De: "Thierry Onkelinx" <thierry.onkelinx at inbo.be>
> ?: "Norman DAURELLE" <norman.daurelle at agroparistech.fr>
> Cc: "r-sig-mixed-models" <r-sig-mixed-models at r-project.org>
> Envoy?: Jeudi 10 Novembre 2022 10:19:02
> Objet: Re: [R-sig-ME] random effect variance greater than output variable variance
> 
> Dear Norman,
> 
> I think this might be due to the unbalance in your design. You need to inspect the BLUP of the random effects. Look for the extremes in location and variety. I would expect some combinations with an extreme positive (negative) location effect compensated by an extreme negative (positive) variety effect.
> 
> Furthermore look into the fixed effects. Long.term apr-jun is highly correlated with long.term total. Their effects cancel each other to a certain extent. I recommend to replace long.term total with its difference with long.term apr-jun.
> 
> Best regards,
> 
> ir. Thierry Onkelinx
> Statisticus / Statistician
> 
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> [ mailto:thierry.onkelinx at inbo.be | thierry.onkelinx at inbo.be ]
> Havenlaan 88 bus 73, 1000 Brussel
> [ http://www.inbo.be/ | www.inbo.be ]
> 
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
> ///////////////////////////////////////////////////////////////////////////////////////////
> 
> [ https://www.inbo.be/ ]
> 
> 
> Op wo 9 nov. 2022 om 21:40 schreef Norman DAURELLE < [ mailto:norman.daurelle at agroparistech.fr | norman.daurelle at agroparistech.fr ] >:
> 
> 
> 
> 
> Dear Thierry,
> 
> i used these lines :
> 
> MELM.1 <- lmer(Yield..kg.Ha. ~ Rep.severity.means + Long.term.Apr.Jun + Long.term.total
> + (1|Location) + (1|Year) + (1|Variety),
> data = yield.disease.rainfall.df)
> 
> summary(MELM.1)
> 
> and compared the outputs of the summary
> 
> summary(MELM.1)
> Linear mixed model fit by REML ['lmerMod']
> Formula: Yield..kg.Ha. ~ Rep.severity.means + Long.term.Apr.Jun + Long.term.total +
> (1 | Location) + (1 | Year) + (1 | Variety)
> Data: yield.disease.rainfall.df
> 
> REML criterion at convergence: 19679.6
> 
> Scaled residuals:
> Min 1Q Median 3Q Max
> -4.1926 -0.5998 -0.0246 0.5572 5.0190
> 
> Random effects:
> Groups Name Variance Std.Dev.
> Variety (Intercept) 106888 326.9
> Location (Intercept) 512674 716.0
> Year (Intercept) 15724 125.4
> Residual 109754 331.3
> Number of obs: 1352, groups: Variety, 22; Location, 16; Year, 4
> 
> Fixed effects:
> Estimate Std. Error t value
> (Intercept) 160.9075 236.6696 0.680
> Rep.severity.means -3.7333 0.6512 -5.733
> Long.term.Apr.Jun -10.1864 0.8009 -12.719
> Long.term.total 9.8103 0.4631 21.182
> 
> Correlation of Fixed Effects:
> (Intr) Rp.sv. L..A.J
> [ http://rp.svrty.mn/ | Rp.svrty.mn ] -0.038
> Lng.trm.A.J -0.061 -0.061
> Lng.trm.ttl -0.314 0.016 -0.699
> 
> to var() of my output variable :
> 
>> var(yield.disease.rainfall.df$Yield..kg.Ha.)
> [1] 435938
> 
> and it bothers me that this variance is inferior to the one of the location factor reported for random effects in the summary, because it prevents me from using the method I wanted to use to show the results. I wanted to show how much each factor (year, location, and variety/cultivar) influences yield outside of disease severity and rainfalls.
> 
> Do I not understand what these variance values mean for the random effects in the summary ?
> Can it not be compared to the var() of my variable of interest ?
> 
> Thanks !
> 
> Norman
> 
> 
> 
> 
> De: "Thierry Onkelinx" < [ mailto:thierry.onkelinx at inbo.be | thierry.onkelinx at inbo.be ] >
> ?: "Norman DAURELLE" < [ mailto:norman.daurelle at agroparistech.fr | norman.daurelle at agroparistech.fr ] >
> Cc: "r-sig-mixed-models" < [ mailto:r-sig-mixed-models at r-project.org | r-sig-mixed-models at r-project.org ] >
> Envoy?: Mercredi 9 Novembre 2022 09:34:07
> Objet: Re: [R-sig-ME] random effect variance greater than output variable variance
> 
> Dear Norman,
> 
> Can you show us the full code of the lme4 call and the output of summary(model). How did you calculate the variances for Y and the random effect?
> 
> Best regards,
> 
> ir. Thierry Onkelinx
> Statisticus / Statistician
> 
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> [ mailto:thierry.onkelinx at inbo.be | thierry.onkelinx at inbo.be ]
> Havenlaan 88 bus 73, 1000 Brussel
> [ http://www.inbo.be/ | www.inbo.be ]
> 
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
> ///////////////////////////////////////////////////////////////////////////////////////////
> 
> [ https://www.inbo.be/ ]
> 
> 
> Op di 8 nov. 2022 om 17:37 schreef Norman DAURELLE via R-sig-mixed-models < [ mailto:r-sig-mixed-models at r-project.org | r-sig-mixed-models at r-project.org ] >:
> 
> BQ_BEGIN
> 
> Dear list members,
> 
> I used a mixed-effect linear model to estimate the effect of a disease on the yield of a crop,
> and used a formula that was as follows :
> 
> Y ~ X + R1 + R2 + (1|year) + (1|location) + (1|cultivar)
> 
> where for each observation :
> 
> Y is the yield of the crop ,
> X the average disease severity in the field,
> R1 and R2 the rainfall values in the 1st and 2nd part of the growing season respectively,
> and year, location and cultivar, the year location and cultivar of the observation.
> 
> I have 5 years, 16 locations and a lot of cultivars, with an unbalanced experiment design.
> 
> The variance given in the summary for the factor Location is greater than the variance of the yield variable taken by itself, and this surprises me.
> 
> I wanted to show the relative importance of each factor over yield through a Venn diagram presenting the variances of each factor as part of the overall yield variance, with each factor's variance overlapping with the others', but the fact that the variance associated with a factor is greater than the variance of the output variable makes me doubt my understanding of the variances shown in a summary for a mixed-effect model.
> 
> Would someone have a simple explanation of what exactly these variances represent ?
> 
> I thought that for a factor with N levels, you had V= ( ? (xi-?)? ) / N, with i = 1,..., N, and xi the output variable's mean in the i-th level of the factor, and ? the overall output variable's mean.
> 
> Is this not how the variance for a random effect is computed ?
> 
> Thanks for any answer !
> 
> Cheers,
> 
> Norman
> 
> 
> 
> 
> 
> 
> [[alternative HTML version deleted]]
> 
> _______________________________________________
> [ mailto:R-sig-mixed-models at r-project.org | R-sig-mixed-models at r-project.org ] mailing list
> [ https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models | https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models ]
> 
> 
> 
> 
> BQ_END
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics
 > E-mail is sent at my convenience; I don't expect replies outside of 
working hours.


From benpe|zer @end|ng |rom gm@||@com  Sun Nov 27 10:02:17 2022
From: benpe|zer @end|ng |rom gm@||@com (ben pelzer)
Date: Sun, 27 Nov 2022 10:02:17 +0100
Subject: [R-sig-ME] difference combined (interaction) and separate models
Message-ID: <CAFgPNS8+_mbD8osBgtj+wLr-8fQPqXf2VfjbJupiBzEA7BTA0w@mail.gmail.com>

Dear list,

With the British Household Panel Data from 1996-2004  I'm using a model to
estimate probabilities of transitions from voting non-labour in a given
year to voting labour in the next year (entry), and vice versa from labour
to non-labour (exit). Only cases with complete data during this period are
used, for demonstrative purposes.



There are two groups of cases, (A) those who did NOT vote labour in the
previous year, and (B) those who DID vote labour in the previous year. Both
groups have nearly 11.000 cases with 8% and 94% voting labour in the
current year, respectively.



For both groups separately, I ran a logistic model with glmer, the
dependent being whether or not a person would vote labour in the current
year. There are two predictors: a dummy for race, nonwhite vs. white, and
"feeling of economic deprivation", values 0 through 4, higher means "more
deprivation". A random intercept was used over person id "pid". The
commands used are:



mA <- glmer(labourvote ~ nonwhite+deprivation + (1|pid),

            family=binomial, groupA, nAGQ=20)



mB <- glmer(labourvote ~ nonwhite+deprivation + (1|pid),

            family=binomial, groupB, nAGQ=20)



The first model estimates "entry into labour"; the second model estimates
"stay in labour" meaning that for "exit from labour" the signs of the
fixed-effect estimates should be reversed.


AGQ was used because there is a clear difference between the estimates by
the default Laplace estimation and the AGQ method: for group A
"deprivation" was insignificant (p=0.55) with Laplace, whereas significant
(p=0.03) with for AGQ. I also use mixed_model from package GLMMadaptive,
and the results are close to those of glmer, both nAGQ=20.



Next, I estimated a single model for the combined data of both groups
"AplusB" and hoped to find the same results as for the single groups above,
by using interaction of the predictors with previous year vote. This was
done to show the advantage of such an interaction model, enabling to test
the difference between the predictor effects in both groups. However, I
eliminated the intercept from fixed and random parts, to show that one
obtains the same results as for the single groups. I used mixed_models of
GLMMadaptive, because it enables AGQ for this model, which glmer does not.
I created two dummy variables "previouslab" and "previousnonlab",
indicating if the previous year vote was lab(our) or nonlab(our). With no
random person effect and using "glm" the results of the interaction model
were indeed identical to those of the two groups apart. Also, if I use
lmer, with a random person effect, as if it was a linear model, the "apart"
results are almost equal to the "combined" results. However, with a random
person effect added in logistic, the results are not identical. The
following command was use for the interaction model:



fm <- mixed_model(labourvote ~ -1 + previouslab + previousnonlab +

                         deprivation:previouslab +

                         deprivation:previousnonlab +

                         nonwhite:previouslab +

                         nonwhite:previousnonlab,

                  random = ~ -1 + previouslab + previousnonlab| pid,
nAGQ=20,

                  data = AplusB, family = binomial)



Below are the mixed_model results for the two groups apart and for the
combined data. The results are not "shockingly" different but also not what
you would call "close". My question is: why? I tried nAGQ=40 but the
estimates are highly similar to those of nAGQ=20, and thus different from
the "apart" estimates.



*previousnonlab group apart:*
              StdDev
(Intercept) 3.110278

Fixed effects:
            Estimate Std.Err  z-value   p-value
(Intercept)  -3.8789  0.1414 -27.4310   < 1e-04
nonwhite      2.3097  0.7747   2.9816 0.0028678
deprivation   0.1531  0.0673   2.2754 0.0228835

*previouslab group apart:*
              StdDev
(Intercept) 2.898665

Fixed effects:
            Estimate Std.Err z-value  p-value
(Intercept)   4.0641  0.1657 24.5267  < 1e-04
nonwhite      1.0066  0.6002  1.6772 0.093511
deprivation  -0.0057  0.0676 -0.0851 0.932175



*Both groups simultaneously*

Random effects covariance matrix:

                StdDev    Corr

previouslab     2.8452

previousnonlab  2.7811 -0.7098



Fixed effects:

                           Estimate Std.Err  z-value   p-value

previousnonlab              -4.3185  0.1573 -27.4614   < 1e-04

previousnonlab:nonwhite      1.7022  0.6803   2.5021 0.0123447

previousnonlab:deprivation   0.1761  0.0631   2.7906 0.0052605



previouslab                  4.7951  0.2046  23.4310   < 1e-04

previouslab:nonwhite         0.7012  0.5795   1.2100 0.2262629

previouslab:deprivation     -0.0267  0.0656  -0.4070 0.6840386


Thanks for any explanation of where these differences may come from!

	[[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Tue Nov 29 17:43:41 2022
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Tue, 29 Nov 2022 10:43:41 -0600
Subject: [R-sig-ME] Detecting Group Size Differences in Clusters
Message-ID: <CACgv6yUK_tgyg34GRnEge61iwafZ-sEyPgxFyjfdf7WdY+aZeA@mail.gmail.com>

Hello All,

Imagine we have two groups of students in a school: group A and group B.

The distribution of these two groups of students in each class (CLASS)
nested in each course (COURSE) is shown below (*the actual data is larger*).

**QUESTION: I was wondering if there is a mixed method technique to answer
the following questions:

1- Overall, is a significantly larger group A vs. B students found in the
data?

2- For each course, is a significantly larger group A vs. B students found
in data?

Warm Regards,
Simon

dat <- read.table(text="
COURSE          CLASS  GROUP_A   GROUP_B
algebra         1         25        8
algebra         2         35        9
number_theory   3         18        7
number_theory   4         14        11
math_games      5         12        5
math_games      6         19        4
",header=TRUE)

	[[alternative HTML version deleted]]


From c@miiie@mo@t@ici@i m@iii@g oii u@ibe@ch  Tue Nov 29 21:41:24 2022
From: c@miiie@mo@t@ici@i m@iii@g oii u@ibe@ch (c@miiie@mo@t@ici@i m@iii@g oii u@ibe@ch)
Date: Tue, 29 Nov 2022 20:41:24 +0000
Subject: [R-sig-ME] Too high condition R-square value - beta family
Message-ID: <e204320a1f5c46d2bc6e7fa5d1549c8a@unibe.ch>

Dear list members,

I am using glmmTMB to fit a beta family (with log link) to some proportion data (varying from 0-1, which I rescaled from 0.01 to 0.99).  I have two continuous rescaled predictors (including a time variable) and a binary treatment predictor. My only goal is to assess if there is any treatment effect (i.e. not to make predictions, so maybe overfitting is less of an issue here). As random effect I have my individuals ID (~160 individuals, and around 28 observations per individuals). The model fits reasonably well, but the main issue is that I get a very high conditional R-square: 0.986 (from: performance::r2(fit)) (marginal: 0.034) with the warning: "mu of 0.6 is too close to zero, estimate of random effect variances may be unreliable".

I tried many thing, including checking if the model is singular (performance::check_singularity())) and it appeared not to be, removing the fixed effects does not change anything either, shuffling the individualsID lead too conditional R-squared around 0.25, removing hens with random intercept mode in the extreme did not change anything either (though model fits generally better). Visualising the data, reveals the individuals to be indeed quite consistent, but likely not up to the level that we could explain 98.7% of the variance, so I am quite confident the model is not reliable. Its the first time I am using beta regression and I feel that I am missing an important point here, any insight would be greatly appreciated!

Best,
Camille


	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Tue Nov 29 22:00:39 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Tue, 29 Nov 2022 16:00:39 -0500
Subject: [R-sig-ME] Too high condition R-square value - beta family
In-Reply-To: <e204320a1f5c46d2bc6e7fa5d1549c8a@unibe.ch>
References: <e204320a1f5c46d2bc6e7fa5d1549c8a@unibe.ch>
Message-ID: <4656165a-0d5d-a273-c41d-190088bad9ce@gmail.com>

 ?? Thanks.? Can you please post the results of summary() applied to 
your fitted model?? That could give us some more clues ...

On 2022-11-29 3:41 PM, camille.montalcini at unibe.ch wrote:
> Dear list members,
>
> I am using glmmTMB to fit a beta family (with log link) to some proportion data (varying from 0-1, which I rescaled from 0.01 to 0.99).  I have two continuous rescaled predictors (including a time variable) and a binary treatment predictor. My only goal is to assess if there is any treatment effect (i.e. not to make predictions, so maybe overfitting is less of an issue here). As random effect I have my individuals ID (~160 individuals, and around 28 observations per individuals). The model fits reasonably well, but the main issue is that I get a very high conditional R-square: 0.986 (from: performance::r2(fit)) (marginal: 0.034) with the warning: "mu of 0.6 is too close to zero, estimate of random effect variances may be unreliable".
>
> I tried many thing, including checking if the model is singular (performance::check_singularity())) and it appeared not to be, removing the fixed effects does not change anything either, shuffling the individualsID lead too conditional R-squared around 0.25, removing hens with random intercept mode in the extreme did not change anything either (though model fits generally better). Visualising the data, reveals the individuals to be indeed quite consistent, but likely not up to the level that we could explain 98.7% of the variance, so I am quite confident the model is not reliable. Its the first time I am using beta regression and I feel that I am missing an important point here, any insight would be greatly appreciated!
>
> Best,
> Camille
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From d@|uedecke @end|ng |rom uke@de  Tue Nov 29 22:50:28 2022
From: d@|uedecke @end|ng |rom uke@de (=?utf-8?Q?Daniel_L=C3=BCdecke?=)
Date: Tue, 29 Nov 2022 22:50:28 +0100
Subject: [R-sig-ME] [EXT] Re: Too high condition R-square value - beta
 family
In-Reply-To: <4656165a-0d5d-a273-c41d-190088bad9ce@gmail.com>
References: <e204320a1f5c46d2bc6e7fa5d1549c8a@unibe.ch>
 <4656165a-0d5d-a273-c41d-190088bad9ce@gmail.com>
Message-ID: <000301d9043c$98795e60$c96c1b20$@uke.de>

It can be that the calculation of the random effects variances is not accurate. The code in the *insight* package (which is used by performance::r2()) has "mu * (1 - mu) / (1 + phi)" to calculate the distributional variance; glmmTMB::beta_family()$variance, however, returns "mu * (1 - mu)". The docs in ?glmmTMB::beta_family, again, say: "Beta distribution: parameterization of Ferrari and Cribari-Neto (2004) and the betareg package (Cribari-Neto and Zeileis 2010); V=?(1??)/(?+1)" (which is what is used in *insight*).

I'm not sure that this is the issue, but it might be. Would be good to know which of the two formulas is the correct / more accurate one.

-----Urspr?ngliche Nachricht-----
Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im Auftrag von Ben Bolker
Gesendet: Dienstag, 29. November 2022 22:01
An: r-sig-mixed-models at r-project.org
Betreff: [EXT] Re: [R-sig-ME] Too high condition R-square value - beta family

    Thanks.  Can you please post the results of summary() applied to 
your fitted model?  That could give us some more clues ...

On 2022-11-29 3:41 PM, camille.montalcini at unibe.ch wrote:
> Dear list members,
>
> I am using glmmTMB to fit a beta family (with log link) to some proportion data (varying from 0-1, which I rescaled from 0.01 to 0.99).  I have two continuous rescaled predictors (including a time variable) and a binary treatment predictor. My only goal is to assess if there is any treatment effect (i.e. not to make predictions, so maybe overfitting is less of an issue here). As random effect I have my individuals ID (~160 individuals, and around 28 observations per individuals). The model fits reasonably well, but the main issue is that I get a very high conditional R-square: 0.986 (from: performance::r2(fit)) (marginal: 0.034) with the warning: "mu of 0.6 is too close to zero, estimate of random effect variances may be unreliable".
>
> I tried many thing, including checking if the model is singular (performance::check_singularity())) and it appeared not to be, removing the fixed effects does not change anything either, shuffling the individualsID lead too conditional R-squared around 0.25, removing hens with random intercept mode in the extreme did not change anything either (though model fits generally better). Visualising the data, reveals the individuals to be indeed quite consistent, but likely not up to the level that we could explain 98.7% of the variance, so I am quite confident the model is not reliable. Its the first time I am using beta regression and I feel that I am missing an important point here, any insight would be greatly appreciated!
>
> Best,
> Camille
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Joachim Pr?l?, Prof. Dr. Blanche Schwappach-Pignataro, Marya Verdel
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING

From bbo|ker @end|ng |rom gm@||@com  Tue Nov 29 23:13:43 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Tue, 29 Nov 2022 17:13:43 -0500
Subject: [R-sig-ME] [EXT] Re: Too high condition R-square value - beta
 family
In-Reply-To: <000301d9043c$98795e60$c96c1b20$@uke.de>
References: <e204320a1f5c46d2bc6e7fa5d1549c8a@unibe.ch>
 <4656165a-0d5d-a273-c41d-190088bad9ce@gmail.com>
 <000301d9043c$98795e60$c96c1b20$@uke.de>
Message-ID: <728f28c8-f6cc-7cb7-0621-2d99a5970189@gmail.com>

 ? This gets tricky (and possibly farther into the weeds than the OP is 
interested in).

 ? tl;dr provided everyone is using the right components of the model 
output in the right places, these two different definitions don't 
necessarily represent a problem.

 ?? The $variance component of 'family' objects in R (as produced by 
functions such as gaussian(), Gamma(), etc.) gives only the component of 
the variance that depends on the mean: for example, 
gaussian()$variance() returns a vector of all 1s.? (The reason for this 
goes back to the classical definitions of generalized linear models, 
where the dispersion parameter [the scaling factor of the variance that 
is *independent* of the mean] is a nuisance parameter that can be 
ignored for many purposes.)? If you want the conditional variance of a 
prediction, you typically need to multiply the $variance() output by a 
dispersion value (you can get this by running sigma() on the model, 
although for glmmTMB families you need to check `?sigma.glmmTMB`: in the 
case of the Beta family I think you need 
$variance(predicted_mu)/(1+sigma(fitted_model)).


More discussion:

* https://github.com/glmmTMB/glmmTMB/issues/294

* https://github.com/glmmTMB/glmmTMB/issues/169#issuecomment-676086686 
(you're asking the same question here!)


On 2022-11-29 4:50 PM, Daniel L?decke wrote:
> It can be that the calculation of the random effects variances is not accurate. The code in the *insight* package (which is used by performance::r2()) has "mu * (1 - mu) / (1 + phi)" to calculate the distributional variance; glmmTMB::beta_family()$variance, however, returns "mu * (1 - mu)". The docs in ?glmmTMB::beta_family, again, say: "Beta distribution: parameterization of Ferrari and Cribari-Neto (2004) and the betareg package (Cribari-Neto and Zeileis 2010); V=?(1??)/(?+1)" (which is what is used in *insight*).
>
> I'm not sure that this is the issue, but it might be. Would be good to know which of the two formulas is the correct / more accurate one.
>
> -----Urspr?ngliche Nachricht-----
> Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im Auftrag von Ben Bolker
> Gesendet: Dienstag, 29. November 2022 22:01
> An: r-sig-mixed-models at r-project.org
> Betreff: [EXT] Re: [R-sig-ME] Too high condition R-square value - beta family
>
>      Thanks.  Can you please post the results of summary() applied to
> your fitted model?  That could give us some more clues ...
>
> On 2022-11-29 3:41 PM, camille.montalcini at unibe.ch wrote:
>> Dear list members,
>>
>> I am using glmmTMB to fit a beta family (with log link) to some proportion data (varying from 0-1, which I rescaled from 0.01 to 0.99).  I have two continuous rescaled predictors (including a time variable) and a binary treatment predictor. My only goal is to assess if there is any treatment effect (i.e. not to make predictions, so maybe overfitting is less of an issue here). As random effect I have my individuals ID (~160 individuals, and around 28 observations per individuals). The model fits reasonably well, but the main issue is that I get a very high conditional R-square: 0.986 (from: performance::r2(fit)) (marginal: 0.034) with the warning: "mu of 0.6 is too close to zero, estimate of random effect variances may be unreliable".
>>
>> I tried many thing, including checking if the model is singular (performance::check_singularity())) and it appeared not to be, removing the fixed effects does not change anything either, shuffling the individualsID lead too conditional R-squared around 0.25, removing hens with random intercept mode in the extreme did not change anything either (though model fits generally better). Visualising the data, reveals the individuals to be indeed quite consistent, but likely not up to the level that we could explain 98.7% of the variance, so I am quite confident the model is not reliable. Its the first time I am using beta regression and I feel that I am missing an important point here, any insight would be greatly appreciated!
>>
>> Best,
>> Camille
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> --
>
> _____________________________________________________________________
>
> Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
> Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Joachim Pr?l?, Prof. Dr. Blanche Schwappach-Pignataro, Marya Verdel
> _____________________________________________________________________
>
> SAVE PAPER - THINK BEFORE PRINTING


From d@|uedecke @end|ng |rom uke@de  Tue Nov 29 23:36:07 2022
From: d@|uedecke @end|ng |rom uke@de (=?UTF-8?Q?Daniel_L=C3=BCdecke?=)
Date: Tue, 29 Nov 2022 23:36:07 +0100
Subject: [R-sig-ME] 
 [EXT] Re: AW: Re: Too high condition R-square value - beta family
In-Reply-To: <728f28c8-f6cc-7cb7-0621-2d99a5970189@gmail.com>
References: <e204320a1f5c46d2bc6e7fa5d1549c8a@unibe.ch>
 <4656165a-0d5d-a273-c41d-190088bad9ce@gmail.com>
 <000301d9043c$98795e60$c96c1b20$@uke.de>
 <728f28c8-f6cc-7cb7-0621-2d99a5970189@gmail.com>
Message-ID: <000301d90442$f96f3db0$ec4db910$@uke.de>

> you're asking the same question here!)

? My guess is I wanted to dig a bit deeper into the topic to gain a better understanding of that issue, but finding the time to do so is crucial, and then it's forgotten.

-----Urspr?ngliche Nachricht-----
Von: Ben Bolker <bbolker at gmail.com> 
Gesendet: Dienstag, 29. November 2022 23:14
An: Daniel L?decke <d.luedecke at uke.de>; r-sig-mixed-models at r-project.org
Betreff: [EXT] Re: AW: Re: [R-sig-ME] Too high condition R-square value - beta family

   This gets tricky (and possibly farther into the weeds than the OP is 
interested in).

   tl;dr provided everyone is using the right components of the model 
output in the right places, these two different definitions don't 
necessarily represent a problem.

    The $variance component of 'family' objects in R (as produced by 
functions such as gaussian(), Gamma(), etc.) gives only the component of 
the variance that depends on the mean: for example, 
gaussian()$variance() returns a vector of all 1s.  (The reason for this 
goes back to the classical definitions of generalized linear models, 
where the dispersion parameter [the scaling factor of the variance that 
is *independent* of the mean] is a nuisance parameter that can be 
ignored for many purposes.)  If you want the conditional variance of a 
prediction, you typically need to multiply the $variance() output by a 
dispersion value (you can get this by running sigma() on the model, 
although for glmmTMB families you need to check `?sigma.glmmTMB`: in the 
case of the Beta family I think you need 
$variance(predicted_mu)/(1+sigma(fitted_model)).


More discussion:

* https://github.com/glmmTMB/glmmTMB/issues/294

* https://github.com/glmmTMB/glmmTMB/issues/169#issuecomment-676086686 
(you're asking the same question here!)


On 2022-11-29 4:50 PM, Daniel L?decke wrote:
> It can be that the calculation of the random effects variances is not accurate. The code in the *insight* package (which is used by performance::r2()) has "mu * (1 - mu) / (1 + phi)" to calculate the distributional variance; glmmTMB::beta_family()$variance, however, returns "mu * (1 - mu)". The docs in ?glmmTMB::beta_family, again, say: "Beta distribution: parameterization of Ferrari and Cribari-Neto (2004) and the betareg package (Cribari-Neto and Zeileis 2010); V=?(1??)/(?+1)" (which is what is used in *insight*).
>
> I'm not sure that this is the issue, but it might be. Would be good to know which of the two formulas is the correct / more accurate one.
>
> -----Urspr?ngliche Nachricht-----
> Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im Auftrag von Ben Bolker
> Gesendet: Dienstag, 29. November 2022 22:01
> An: r-sig-mixed-models at r-project.org
> Betreff: [EXT] Re: [R-sig-ME] Too high condition R-square value - beta family
>
>      Thanks.  Can you please post the results of summary() applied to
> your fitted model?  That could give us some more clues ...
>
> On 2022-11-29 3:41 PM, camille.montalcini at unibe.ch wrote:
>> Dear list members,
>>
>> I am using glmmTMB to fit a beta family (with log link) to some proportion data (varying from 0-1, which I rescaled from 0.01 to 0.99).  I have two continuous rescaled predictors (including a time variable) and a binary treatment predictor. My only goal is to assess if there is any treatment effect (i.e. not to make predictions, so maybe overfitting is less of an issue here). As random effect I have my individuals ID (~160 individuals, and around 28 observations per individuals). The model fits reasonably well, but the main issue is that I get a very high conditional R-square: 0.986 (from: performance::r2(fit)) (marginal: 0.034) with the warning: "mu of 0.6 is too close to zero, estimate of random effect variances may be unreliable".
>>
>> I tried many thing, including checking if the model is singular (performance::check_singularity())) and it appeared not to be, removing the fixed effects does not change anything either, shuffling the individualsID lead too conditional R-squared around 0.25, removing hens with random intercept mode in the extreme did not change anything either (though model fits generally better). Visualising the data, reveals the individuals to be indeed quite consistent, but likely not up to the level that we could explain 98.7% of the variance, so I am quite confident the model is not reliable. Its the first time I am using beta regression and I feel that I am missing an important point here, any insight would be greatly appreciated!
>>
>> Best,
>> Camille
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> --
>
> _____________________________________________________________________
>
> Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
> Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Joachim Pr?l?, Prof. Dr. Blanche Schwappach-Pignataro, Marya Verdel
> _____________________________________________________________________
>
> SAVE PAPER - THINK BEFORE PRINTING

--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Joachim Pr?l?, Prof. Dr. Blanche Schwappach-Pignataro, Marya Verdel
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING

From bbo|ker @end|ng |rom gm@||@com  Tue Nov 29 23:39:00 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Tue, 29 Nov 2022 17:39:00 -0500
Subject: [R-sig-ME] 
 [EXT] Re: AW: Re: Too high condition R-square value - beta family
In-Reply-To: <000301d90442$f96f3db0$ec4db910$@uke.de>
References: <e204320a1f5c46d2bc6e7fa5d1549c8a@unibe.ch>
 <4656165a-0d5d-a273-c41d-190088bad9ce@gmail.com>
 <000301d9043c$98795e60$c96c1b20$@uke.de>
 <728f28c8-f6cc-7cb7-0621-2d99a5970189@gmail.com>
 <000301d90442$f96f3db0$ec4db910$@uke.de>
Message-ID: <3399b828-ff87-587a-61db-a1551267d89c@gmail.com>

 ? Now that R manuals have nicer-rendering LaTeX components maybe I'll 
bother to write a more complete description under the "Details" section 
in ?family.glmmTMB ...

On 2022-11-29 5:36 PM, Daniel L?decke wrote:
>> you're asking the same question here!)
> ? My guess is I wanted to dig a bit deeper into the topic to gain a better understanding of that issue, but finding the time to do so is crucial, and then it's forgotten.
>
> -----Urspr?ngliche Nachricht-----
> Von: Ben Bolker <bbolker at gmail.com>
> Gesendet: Dienstag, 29. November 2022 23:14
> An: Daniel L?decke <d.luedecke at uke.de>; r-sig-mixed-models at r-project.org
> Betreff: [EXT] Re: AW: Re: [R-sig-ME] Too high condition R-square value - beta family
>
>     This gets tricky (and possibly farther into the weeds than the OP is
> interested in).
>
>     tl;dr provided everyone is using the right components of the model
> output in the right places, these two different definitions don't
> necessarily represent a problem.
>
>      The $variance component of 'family' objects in R (as produced by
> functions such as gaussian(), Gamma(), etc.) gives only the component of
> the variance that depends on the mean: for example,
> gaussian()$variance() returns a vector of all 1s.  (The reason for this
> goes back to the classical definitions of generalized linear models,
> where the dispersion parameter [the scaling factor of the variance that
> is *independent* of the mean] is a nuisance parameter that can be
> ignored for many purposes.)  If you want the conditional variance of a
> prediction, you typically need to multiply the $variance() output by a
> dispersion value (you can get this by running sigma() on the model,
> although for glmmTMB families you need to check `?sigma.glmmTMB`: in the
> case of the Beta family I think you need
> $variance(predicted_mu)/(1+sigma(fitted_model)).
>
>
> More discussion:
>
> * https://github.com/glmmTMB/glmmTMB/issues/294
>
> * https://github.com/glmmTMB/glmmTMB/issues/169#issuecomment-676086686
> (you're asking the same question here!)
>
>
> On 2022-11-29 4:50 PM, Daniel L?decke wrote:
>> It can be that the calculation of the random effects variances is not accurate. The code in the *insight* package (which is used by performance::r2()) has "mu * (1 - mu) / (1 + phi)" to calculate the distributional variance; glmmTMB::beta_family()$variance, however, returns "mu * (1 - mu)". The docs in ?glmmTMB::beta_family, again, say: "Beta distribution: parameterization of Ferrari and Cribari-Neto (2004) and the betareg package (Cribari-Neto and Zeileis 2010); V=?(1??)/(?+1)" (which is what is used in *insight*).
>>
>> I'm not sure that this is the issue, but it might be. Would be good to know which of the two formulas is the correct / more accurate one.
>>
>> -----Urspr?ngliche Nachricht-----
>> Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im Auftrag von Ben Bolker
>> Gesendet: Dienstag, 29. November 2022 22:01
>> An: r-sig-mixed-models at r-project.org
>> Betreff: [EXT] Re: [R-sig-ME] Too high condition R-square value - beta family
>>
>>       Thanks.  Can you please post the results of summary() applied to
>> your fitted model?  That could give us some more clues ...
>>
>> On 2022-11-29 3:41 PM, camille.montalcini at unibe.ch wrote:
>>> Dear list members,
>>>
>>> I am using glmmTMB to fit a beta family (with log link) to some proportion data (varying from 0-1, which I rescaled from 0.01 to 0.99).  I have two continuous rescaled predictors (including a time variable) and a binary treatment predictor. My only goal is to assess if there is any treatment effect (i.e. not to make predictions, so maybe overfitting is less of an issue here). As random effect I have my individuals ID (~160 individuals, and around 28 observations per individuals). The model fits reasonably well, but the main issue is that I get a very high conditional R-square: 0.986 (from: performance::r2(fit)) (marginal: 0.034) with the warning: "mu of 0.6 is too close to zero, estimate of random effect variances may be unreliable".
>>>
>>> I tried many thing, including checking if the model is singular (performance::check_singularity())) and it appeared not to be, removing the fixed effects does not change anything either, shuffling the individualsID lead too conditional R-squared around 0.25, removing hens with random intercept mode in the extreme did not change anything either (though model fits generally better). Visualising the data, reveals the individuals to be indeed quite consistent, but likely not up to the level that we could explain 98.7% of the variance, so I am quite confident the model is not reliable. Its the first time I am using beta regression and I feel that I am missing an important point here, any insight would be greatly appreciated!
>>>
>>> Best,
>>> Camille
>>>
>>>
>>> 	[[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>> --
>>
>> _____________________________________________________________________
>>
>> Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
>> Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Joachim Pr?l?, Prof. Dr. Blanche Schwappach-Pignataro, Marya Verdel
>> _____________________________________________________________________
>>
>> SAVE PAPER - THINK BEFORE PRINTING
> --
>
> _____________________________________________________________________
>
> Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
> Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Joachim Pr?l?, Prof. Dr. Blanche Schwappach-Pignataro, Marya Verdel
> _____________________________________________________________________
>
> SAVE PAPER - THINK BEFORE PRINTING


From don-|me4 @end|ng |rom |@|@@c@3-|nc@com  Wed Nov 30 02:15:24 2022
From: don-|me4 @end|ng |rom |@|@@c@3-|nc@com (Don Cohen)
Date: Wed, 30 Nov 2022 01:15:24 +0000
Subject: [R-sig-ME] differences between lmer and glmmTMB
Message-ID: <25478.44716.662956.649250@losangelesyouthorchestra.org>


I wonder whether I'm missing something fundamental.
Should these two calls produce very similar results?

> mbase <- lmer(data = strict, LN_GCS ~ hour + group_size + age + dom_rank + Pregnancy + (1|ID) + (1|groupid) + (1|AssayNum))
> TMBmbase <- glmmTMB(data = strict, LN_GCS ~ hour + group_size + age + dom_rank + Pregnancy + (1|ID) + (1|groupid) + (1|AssayNum))

And similarly these two?

mfull <- lmer(data = strict, LN_GCS ~ poly(hour,2) + poly(group_size,2) + poly(age,2) + poly(dom_rank,2) + Pregnancy + takeover + (1|ID) + (1|groupid) + (1|AssayNum))

TMBmfull <- glmmTMB(data = strict, LN_GCS ~ poly(hour,2) + poly(group_size,2) + poly(age,2) + poly(dom_rank,2) + Pregnancy + takeover + (1|ID) + (1|groupid) + (1|AssayNum))

The results do look very similar with one glaring exception:

summary(mfull) says
REML criterion at convergence: 2088.6
summary(mbase) says
REML criterion at convergence: 2141.6

AICctab shows a difference in AIC of 42.7 (df 16 and 11)

whereas

summary(TMBmfull)
     AIC      BIC   logLik deviance df.resid
  2133.0   2206.7  -1050.5   2101.0      727

summary(TMBmbase)
     AIC      BIC   logLik deviance df.resid
  2135.4   2186.1  -1056.7   2113.4      732

for a difference in AIC of 2.4 !
(though AICctab shows a difference of 2.0 ?)

Is there some explanation?  Should I believe one and not the other?


From don-|me4 @end|ng |rom |@|@@c@3-|nc@com  Wed Nov 30 18:26:35 2022
From: don-|me4 @end|ng |rom |@|@@c@3-|nc@com (Don Cohen)
Date: Wed, 30 Nov 2022 17:26:35 +0000
Subject: [R-sig-ME] differences between lmer and glmmTMB - follow up
In-Reply-To: <25478.44716.662956.649250@losangelesyouthorchestra.org>
References: <25478.44716.662956.649250@losangelesyouthorchestra.org>
Message-ID: <25479.37451.722338.170278@losangelesyouthorchestra.org>


[Summary: two models that differ only in one added fixed effect
 the deviance of the one with extra fixed effect coming out higher]

I think this is a related problem:

> mfull <- lmer(data = strict, LN_GCS ~ poly(hour,2) + poly(group_size,2) + poly(age,2) + poly(dom_rank,2) + Pregnancy + takeover + (1|ID) + (1|groupid) + (1|AssayNum))

> summary(mfull)
Linear mixed model fit by REML. t-tests use Satterthwaite's method [
lmerModLmerTest]
Formula: LN_GCS ~ poly(hour, 2) + poly(group_size, 2) + poly(age, 2) +
    poly(dom_rank, 2) + Pregnancy + takeover + (1 | ID) + (1 |
    groupid) + (1 | AssayNum)
   Data: strict
REML criterion at convergence: 2088.6
...
> m <- lmer(data = strict, LN_GCS ~ poly(hour,2) + poly(group_size,2) + poly(age,2) + poly(dom_rank,2) + Pregnancy + (1|ID) + (1|groupid) + (1|AssayNum))

> summary(m)
Linear mixed model fit by REML. t-tests use Satterthwaite's method [
lmerModLmerTest]
Formula: LN_GCS ~ poly(hour, 2) + poly(group_size, 2) + poly(age, 2) +
    poly(dom_rank, 2) + Pregnancy + (1 | ID) + (1 | groupid) +
    (1 | AssayNum)
   Data: strict
REML criterion at convergence: 2086.4
...

The REML criterion is supposeed to be -2 x loglik (deviance), right?
How can it be lower for m than for mfull when mfull contains all the same 
effects (plus one more) ?  

> AICctab(mfull, m, weights = T)
      dAICc df weight
m      0.0  15 0.89
mfull  4.2  16 0.11

If I use glmmTMB I get the expected results - same loglik, AIC
different by 2


From bbo|ker @end|ng |rom gm@||@com  Wed Nov 30 20:24:18 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 30 Nov 2022 14:24:18 -0500
Subject: [R-sig-ME] differences between lmer and glmmTMB - follow up
In-Reply-To: <25479.37451.722338.170278@losangelesyouthorchestra.org>
References: <25478.44716.662956.649250@losangelesyouthorchestra.org>
 <25479.37451.722338.170278@losangelesyouthorchestra.org>
Message-ID: <3cefa9cc-e27e-b1c1-9340-9ffc0b3f341c@gmail.com>

A few things going on here:

1.? It is simply wrong/meaningless to compare REML criteria across 
models with different fixed effects.? lme4 tries to protect you from 
this, to some extent (e.g. the anova.merMod() method refits REML models 
via ML before comparing), but it can't entirely protect you.? So the 
issue in this e-mail is moot.? (In other words, *don't use AIC tables to 
compare models with different fixed effects fitted by REML*.)

2. lme4::lmer() uses REML by default, glmmTMB uses ML by default.? At 
least for the reproducible example below, if we call glmmTMB with 
REML=TRUE we get answers that are the same up to very small 
(floating-point imprecision) differences:

library(lme4)
library(glmmTMB)
m1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
m2 <- glmmTMB(Reaction ~ Days + (Days|Subject), sleepstudy, REML = TRUE)

## results are equivalent

all.equal(logLik(m1), logLik(m2))
coef(summary(m1))
coef(summary(m2))$cond

## compare differences in REML criterion

m3 <- update(m1, . ~ . - (Days|Subject) + (1|Subject))
m4 <- update(m2, . ~ . - (Days|Subject) + (1|Subject))
logLik(m1) - logLik(m3)
logLik(m2) - logLik(m4)

On 2022-11-30 12:26 PM, Don Cohen wrote:
> [Summary: two models that differ only in one added fixed effect
>   the deviance of the one with extra fixed effect coming out higher]
>
> I think this is a related problem:
>
>> mfull <- lmer(data = strict, LN_GCS ~ poly(hour,2) + poly(group_size,2) + poly(age,2) + poly(dom_rank,2) + Pregnancy + takeover + (1|ID) + (1|groupid) + (1|AssayNum))
>> summary(mfull)
> Linear mixed model fit by REML. t-tests use Satterthwaite's method [
> lmerModLmerTest]
> Formula: LN_GCS ~ poly(hour, 2) + poly(group_size, 2) + poly(age, 2) +
>      poly(dom_rank, 2) + Pregnancy + takeover + (1 | ID) + (1 |
>      groupid) + (1 | AssayNum)
>     Data: strict
> REML criterion at convergence: 2088.6
> ...
>> m <- lmer(data = strict, LN_GCS ~ poly(hour,2) + poly(group_size,2) + poly(age,2) + poly(dom_rank,2) + Pregnancy + (1|ID) + (1|groupid) + (1|AssayNum))
>> summary(m)
> Linear mixed model fit by REML. t-tests use Satterthwaite's method [
> lmerModLmerTest]
> Formula: LN_GCS ~ poly(hour, 2) + poly(group_size, 2) + poly(age, 2) +
>      poly(dom_rank, 2) + Pregnancy + (1 | ID) + (1 | groupid) +
>      (1 | AssayNum)
>     Data: strict
> REML criterion at convergence: 2086.4
> ...
>
> The REML criterion is supposeed to be -2 x loglik (deviance), right?
> How can it be lower for m than for mfull when mfull contains all the same
> effects (plus one more) ?
>
>> AICctab(mfull, m, weights = T)
>        dAICc df weight
> m      0.0  15 0.89
> mfull  4.2  16 0.11
>
> If I use glmmTMB I get the expected results - same loglik, AIC
> different by 2
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From |ndr@@ench@uh@n @end|ng |rom gm@||@com  Fri Dec  2 13:43:49 2022
From: |ndr@@ench@uh@n @end|ng |rom gm@||@com (Indrasen Chauhan)
Date: Fri, 2 Dec 2022 18:13:49 +0530
Subject: [R-sig-ME] Fitting cenexponential family in MCMCglmm
Message-ID: <CACkfs7wxL20huvoGhtLfGeHRBkggVtu8QfrFOEmwT2x=sax=fw@mail.gmail.com>

Dear sirs,

I am estimating the heritability of survival of Malpura sheep from birth to
weaning (90 days), three to six months, six to twelve months and birth to
twelve months for a twenty year data set. The data has a right censored
exponential data as a response variable.

The data have following common structure:

$ itag    : int  2890 2891 2892 2893 2894 2899 2901 2902 2903 2905 ...

 $ stag    : int  2602 2258 2248 2602 2602 2599 2667 2639 2667 2599 ...

 $ dtag    : int  2422 2720 2199 2483 2659 2638 1807 2488 1823 2082 ...

 $ season  : chr  "Winter" "Winter" "Winter" "Winter" ...

 $ sex     : chr  "Female" "Female" "Male" "Male" ...

 $ tyb     : chr  "Single" "Single" "Single" "Single" ...

 $ ewt     : num  26 33.4 31.4 34 29.8 29 28 28.4 34 28.5 ...

 $ bwt     : num  2.6 3.2 3 3.8 2.8 3.4 2.9 2.4 3.2 3 ...

 $ dtd    : chr  "2670" "1966" "444" "1442" ...

 $ cens    : int  0 0 0 0 0 0 0 0 0 0 ...



?itag?, ?stag? and ?dtag? are the identities of individuals, sire and dam,
respectively. ?Season?, ?sex?, ?tyb? (type of birth), ?ewt? (ewe weight at
the time of lambing), and ?bwt? (birthweight) are the covariates for the
model. ?dtd? (response variable) is the days to survival for the specific
duration of the study, ranging from 1 to 90 days for birth to weaning data.
?cens? is the censoring code. For birth to weaning data, the lambs alive at
the end of weaning (90 days) are recorded as ?right censored?, and time of
failure of such lambs is taken as 90 days. Moreover, the lambs which are
removed from the flock before weaning due to reasons other than death
(e.g., culling) are also recorded as ?right censored? (in Cox regression).

In mailing lists on MCMCglmm, I have found the following information:

m1<-MCMCglmm (cbind (ymin, ymax) ~1, family="cenexponential", data=dat)

(https://stat.ethz.ch/pipermail/r-sig-mixed-models/2010q4/004933.html)

I am taking example of birth to weaning data.

I have doubts on the following aspects:

(1)    Will the ?ymax? be equal to 90 days (days to weaning) or equal to
the maximum number of days an animal survives even after weaning? What
about the information about animals which are still in the flock at the
time of data collection? The data on such animals are right censored in Cox
regression. What to do with data on such animals while estimating
heritability by using MCMCglmm? Will ?dtd? contain data (days to survival)
about every animal including the animals which are still in the flock at
the time of data collection or for only the animals which died before the
end of weaning?

(2)    What about the data about the lambs which are removed from the flock
before weaning due to reasons other than death (e.g., culling)?

(3)    How to set up data (?dtd?) for left censored and left truncated
information?

(4)    What to do about censoring code. I could not find any information
about this anywhere for MCMCglmm.


Anticipating response at the earliest.

With kind regards.

-- 
Dr. Indrasen Chauhan
Scientist, ICAR-IVRI,Mukteshwar
Nainital,Uttarakhand-263138 (INDIA)
Mobile:9461883473,8910856302

	[[alternative HTML version deleted]]


From RAYMOND@POMPONIO @end|ng |rom CUANSCHUTZ@EDU  Fri Dec  2 17:36:46 2022
From: RAYMOND@POMPONIO @end|ng |rom CUANSCHUTZ@EDU (Pomponio, Raymond)
Date: Fri, 2 Dec 2022 16:36:46 +0000
Subject: [R-sig-ME] Modeling error covariance in linear mixed models with
 non-default correlation structures
Message-ID: <CY4PR05MB347947629594E0B752D7CE40F3179@CY4PR05MB3479.namprd05.prod.outlook.com>

Hello,

I'm trying to fit a linear mixed model that accounts for correlated errors at the subject level. This has been described as modeling the "R" matrix in a model where the overall variance of Y can be expressed as Var(Y) = ZGZ' + R.

The nlme package offers the ability to model covariance structures through its corClasses feature. I'm capable of fitting a model with first order auto-regressive covariance structure as in the following example:

  > library(nlme)
  > dat <- data.frame(
     id      = c(rep(1, 18), rep(2, 14), rep(3, 12)),
     rec_num = c(1:18, 1:14, 1:12),
     day     = c(rep(1:9, each=2), rep(1:7, each=2), rep(1:6, each=2)),
     am_pm   = c("am", "pm"),
     y       = c(rnorm(18, mean=2), rnorm(14, mean=1), rnorm(12, mean=0))
     )
  > lme(
     y ~ 1,
     data        = dat,
     random      = ~ 1 | id,
     correlation = corAR1(form = ~ rec_num |id),
     control     = lmeControl(opt="optim")
     )


Each row of the above data.frame corresponds to one recorded observation, with multiple observations per id. Measurements were taken twice daily (one in the AM and one in the PM) over multiple days. The auto-regressive correlation structure makes sense since observations that are closer in time would be more-highly correlated with one another.

However, I want to use a non-default correlation structure to address the specific design of this dataset. I am hoping to implement a "direct product" structure that accounts for doubly-repeated measures, since we have reason to believe that measurements taken in the morning would be correlated with one another, as would measurements taken on nearby days.

Here I have to refer to SAS's "REPEATED" statement since one of the default choices for covariance structures is the direct product called UN at AR(1), the Kronecker product of an unstructured matrix and an auto-regressive matrix.

I am aware that non-default correlation structures can be defined by users in R. In the documentation for nlme, the authors mention:

  "Users may define their own corStruct classes by specifying a constructor function and, at a minimum, methods for the functions corMatrix and coef. For examples of these functions, see the methods for classes corSymm and corAR1."

Unfortunately the examples of code for corSymm and corAR1 are beyond my understanding of object-oriented programming in R. I am wondering if anyone has successfully defined their own correlation structure. More generally, I wonder if anyone has encountered the same issue in linear mixed models where there may be multiple sources of repeated measures, and modeling correlation via AR1 alone is not addressing the specific design of the dataset.

I expect that my user-defined correlation structure would be called as in the following sample code:

  > lme(
     y ~ 1,
     data        = dat,
     random      = ~ 1 | id,
     correlation = corUNxAR1(form = ~ am_pm + day |id),
     control     = lmeControl(opt="optim")
     )

Lastly, for context this is for a graduate school course on longitudinal data analysis. Until this point, we have relied heavily on SAS since we have not found a way to implement Kronecker product covariance structures in R. We are hoping to find a solution here and demonstrate that such complex cases can be handled in R.

Some relevant details below:

* R version 4.2.1
* nlme version 3.1-157

	[[alternative HTML version deleted]]


From j@h@d||e|d @end|ng |rom ed@@c@uk  Sat Dec  3 15:43:21 2022
From: j@h@d||e|d @end|ng |rom ed@@c@uk (Jarrod Hadfield)
Date: Sat, 3 Dec 2022 14:43:21 +0000
Subject: [R-sig-ME] Fitting cenexponential family in MCMCglmm
In-Reply-To: <CACkfs7wxL20huvoGhtLfGeHRBkggVtu8QfrFOEmwT2x=sax=fw@mail.gmail.com>
References: <CACkfs7wxL20huvoGhtLfGeHRBkggVtu8QfrFOEmwT2x=sax=fw@mail.gmail.com>
Message-ID: <AS8PR05MB8659D969D608755597794457AC169@AS8PR05MB8659.eurprd05.prod.outlook.com>

Hi,

If the lamb is known to have died ymin and ymax should be equal and be the day on which the lamb died. If the lamb was alive at the end it should have 90 in ymin and Inf in ymax. If the lamb was culled it should have the day of culling as ymin and Inf as ymax (assuming the day on which it was culled gives no information on how long it would have lived had it not been culled).

Cheers,

Jarrod


From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on behalf of Indrasen Chauhan <indrasenchauhan at gmail.com>
Date: Friday, 2 December 2022 at 12:44
To: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: [R-sig-ME] Fitting cenexponential family in MCMCglmm
This email was sent to you by someone outside the University.
You should only click on links or attachments if you are certain that the email is genuine and the content is safe.

Dear sirs,

I am estimating the heritability of survival of Malpura sheep from birth to
weaning (90 days), three to six months, six to twelve months and birth to
twelve months for a twenty year data set. The data has a right censored
exponential data as a response variable.

The data have following common structure:

$ itag    : int  2890 2891 2892 2893 2894 2899 2901 2902 2903 2905 ...

 $ stag    : int  2602 2258 2248 2602 2602 2599 2667 2639 2667 2599 ...

 $ dtag    : int  2422 2720 2199 2483 2659 2638 1807 2488 1823 2082 ...

 $ season  : chr  "Winter" "Winter" "Winter" "Winter" ...

 $ sex     : chr  "Female" "Female" "Male" "Male" ...

 $ tyb     : chr  "Single" "Single" "Single" "Single" ...

 $ ewt     : num  26 33.4 31.4 34 29.8 29 28 28.4 34 28.5 ...

 $ bwt     : num  2.6 3.2 3 3.8 2.8 3.4 2.9 2.4 3.2 3 ...

 $ dtd    : chr  "2670" "1966" "444" "1442" ...

 $ cens    : int  0 0 0 0 0 0 0 0 0 0 ...



?itag?, ?stag? and ?dtag? are the identities of individuals, sire and dam,
respectively. ?Season?, ?sex?, ?tyb? (type of birth), ?ewt? (ewe weight at
the time of lambing), and ?bwt? (birthweight) are the covariates for the
model. ?dtd? (response variable) is the days to survival for the specific
duration of the study, ranging from 1 to 90 days for birth to weaning data.
?cens? is the censoring code. For birth to weaning data, the lambs alive at
the end of weaning (90 days) are recorded as ?right censored?, and time of
failure of such lambs is taken as 90 days. Moreover, the lambs which are
removed from the flock before weaning due to reasons other than death
(e.g., culling) are also recorded as ?right censored? (in Cox regression).

In mailing lists on MCMCglmm, I have found the following information:

m1<-MCMCglmm (cbind (ymin, ymax) ~1, family="cenexponential", data=dat)

(https://stat.ethz.ch/pipermail/r-sig-mixed-models/2010q4/004933.html)

I am taking example of birth to weaning data.

I have doubts on the following aspects:

(1)    Will the ?ymax? be equal to 90 days (days to weaning) or equal to
the maximum number of days an animal survives even after weaning? What
about the information about animals which are still in the flock at the
time of data collection? The data on such animals are right censored in Cox
regression. What to do with data on such animals while estimating
heritability by using MCMCglmm? Will ?dtd? contain data (days to survival)
about every animal including the animals which are still in the flock at
the time of data collection or for only the animals which died before the
end of weaning?

(2)    What about the data about the lambs which are removed from the flock
before weaning due to reasons other than death (e.g., culling)?

(3)    How to set up data (?dtd?) for left censored and left truncated
information?

(4)    What to do about censoring code. I could not find any information
about this anywhere for MCMCglmm.


Anticipating response at the earliest.

With kind regards.

--
Dr. Indrasen Chauhan
Scientist, ICAR-IVRI,Mukteshwar
Nainital,Uttarakhand-263138 (INDIA)
Mobile:9461883473,8910856302

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
The University of Edinburgh is a charitable body, registered in Scotland, with registration number SC005336. Is e buidheann carthannais a th? ann an Oilthigh Dh?n ?ideann, cl?raichte an Alba, ?ireamh cl?raidh SC005336.

	[[alternative HTML version deleted]]


From jun@y@n @end|ng |rom uconn@edu  Mon Dec  5 11:50:46 2022
From: jun@y@n @end|ng |rom uconn@edu (Yan, Jun)
Date: Mon, 5 Dec 2022 10:50:46 +0000
Subject: [R-sig-ME] interpretation of random effects variance from glmer()
 with the Gamma family
Message-ID: <DM6PR05MB4282622D7037BE30A4AA9205F0189@DM6PR05MB4282.namprd05.prod.outlook.com>

Hi All,

The summary of an lme4::glmer() fit with the Gamma family has a section on random effects. For example:

Random effects:
 Groups   Name        Variance Std.Dev.
 Year     (Intercept) 0.002536 0.05036
 Residual             0.011198 0.10582
Number of obs: 91, groups:  Year, 12

The function call that I used was

fit2 <- glmer(y ~ (1 | Year), data = finals, family = Gamma(link = "log"))

I understand that the normal random effect of "Year" has variance 0.00256. My question is, doe the variance for the "Residual", 0.011198, mean the variance of the Gamma family or the dispersion parameter (which is the reciprocal of the shape of the Gamma distribution, the same as what is reported from a glm fit)?

Any tips are appreciated.
Jun Yan, Professor
Department of Statistics, University of Connecticut
215 Glenbrook Rd. Unit 4120  Storrs, CT 06269
Voice: 860-486-3416  Fax: 860-486-4113
Web: http://statistics.uconn.edu/person/jun-yan/
http://scholar.google.com/citations?user=4jVhnnEAAAAJ&hl=en


	[[alternative HTML version deleted]]


From benpe|zer @end|ng |rom gm@||@com  Wed Dec  7 12:30:46 2022
From: benpe|zer @end|ng |rom gm@||@com (ben pelzer)
Date: Wed, 7 Dec 2022 12:30:46 +0100
Subject: [R-sig-ME] GEE different working matrices for males and females
Message-ID: <CAFgPNS8KhHA7VxAJGYW3QoAcz-=axCY6L8oRkDdVr+_S4k9aaQ@mail.gmail.com>

Dear list,



I'm estimating logistic models with geepack in R, using geeglm. Is there a
way to specify separate AR1 working matrices for two groups, males and
females? I found an example on how this can be done for "exchangeable"
matrices which differ for males and females. But I cannot figure out how to
do this for different AR1 matrices.



In geepack one can specify a "userdefined" working matrix. The genZcor( )
function can be used to generate the design-matrix for unstructured
correlations, which can then serve as a basis to obtain the design matrix Z
of e.g. an exchangeable structure. This design-matrix can then also be
extended to obtain different exchangeable correlations for males and
females.  The geepack manual also gives an example of how a Toeplitz
design-matrix can be generated, which can then also (although I did not try
that yet) be extended into a separate Toeplitz for males and females. But
there is no example of how to obtain an AR1 design matrix for the
correlations.



Thanks for any help, Ben.

	[[alternative HTML version deleted]]


From @h|r@qotj @end|ng |rom gm@||@com  Fri Dec  9 15:53:05 2022
From: @h|r@qotj @end|ng |rom gm@||@com (Shira Mitchell)
Date: Fri, 9 Dec 2022 15:53:05 +0100
Subject: [R-sig-ME] Bradley Terry GLMM in R ?
In-Reply-To: <CADeg_O2p+pWAqyE-9pKaJ34icyGT1OVEwTZkUo8EZuf_68nmKA@mail.gmail.com>
References: <CADeg_O3L_6Cmn_Wv0tExeCLccFJS1p5fottqprdh3+spDCrXxA@mail.gmail.com>
 <CAJuCY5ys+di7UNWKLSV8gw6qVC5yjWXVa4J7Bboe=cHAwoYDGw@mail.gmail.com>
 <CADeg_O0d+tuG0DnnnFw9o9VSh1dcMUbOitcT_rLbRcdbhNpzYw@mail.gmail.com>
 <CAJuCY5ysJUQQtYUCLF76iD5y9TmSiOD5OAfBt039RjVaD_pzwQ@mail.gmail.com>
 <CADeg_O0g7kTrhRUNhrDZjKt3HEzKpfOKZMW71uBaNTDoaNJt7g@mail.gmail.com>
 <CADeg_O0dzRyG=8u7Zjk94PbwGpoqnWoQRw+VRgpoq-BS2RimgA@mail.gmail.com>
 <E27AFF46-A57B-425D-A946-AE24E6567B7C@ed.ac.uk>
 <CADeg_O12OrDgEMNQ6ejcV30evndEjAuFN-wKLdbN0K4GusyAQw@mail.gmail.com>
 <CADeg_O2p+pWAqyE-9pKaJ34icyGT1OVEwTZkUo8EZuf_68nmKA@mail.gmail.com>
Message-ID: <CADeg_O2OYsn1ndkBcVKkwqR2KZZ6uX0AA+E5Boe8T4u_oss7eA@mail.gmail.com>

Hi Dr van Paridon,

Thank you so much !

We are returning to this after our busy election season. We are using your
awesome lmerMultiMember package and have questions.

We have voter-specific variables x that influence which political message
(i vs j) wins for them:

logit[pr(i beats j | person with covariate x)] = lambda_i - lambda_j +
(beta_i - beta_j) x

We then model parameters as random effects:
lambda_i ~ N(0, sigma_lambda)
beta_i ~ N(0, sigma_beta)

m <- lmerMultiMember::glmer(depvar ~ 1 + (1 | indicators) + (x |
indicators),
                                                    family = binomial,
                                                    memberships =
list(indicators = W),
                                                    data = dat_train)

This runs beautifully. :)

Now suppose we want the strength of message i among people with covariates
x (e.g. a specific age). In reality we have a few covariates, both
continuous (x | indicators) and categorical groups (1 | indicators:group).

pr(i beats a hypothetical average message | person with covariate x) =
logit^-1 ( lambda_i + beta_i x)

We have a data set that crosses all population x values with all messages,
dat_population_all_messages.

Also, if we want to predict specific match-ups from the training data
dat_train, how do we do that ?

Thanks again !!

Shira

---------

https://stat.ethz.ch/pipermail/r-sig-mixed-models/2022q4/030224.html

In case it's helpful to anyone following this email thread: I wrote a
vignette explaining how to fit a Bradley-Terry model in lme4 using
lmerMultiMember. You can find it at
https://jvparidon.github.io/lmerMultiMember/articles/bradleyterry_models.html


Cheers,

JP van Paridon (he/him)
Research Associate, Lupyan Lab
University of Wisconsin-Madisonhttps://github.com/jvparidon
________________________________
From: Jeroen van Paridon <vanparidon using wisc.edu
<https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>
Sent: Thursday, October 20, 2022 1:42 AM
To: r-sig-mixed-models using r-project.org
<https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
<r-sig-mixed-models using r-project.org
<https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>
Subject: Re: [R-sig-ME] Bradley Terry GLMM in R ?

Hi,

Just to expand on Ben's last email: In principle, lmerMultiMember
allows you to pass arbitrary indicator/weight matrices for the random
effects to lme4 for model fitting as long as they have the correct
shape. The package contains helper functions for generating more
conventional multiple membership matrices since that was my own
use-case, but if you create your own matrix with opposed (1 and -1)
weights I see no reason why it shouldn't work.

Membership matrices need to be sparse matrices of class
Matrix::dgCMatrix and shape n_groups x n_obs. You can probably just
take whatever indicator matrix you already have, transpose it, and
then cast it to the sparse format.

If you're going this route and run into any issues, feel free to reach
out to me, directly.


Cheers,

JP van Paridon (he/him)
Research Associate, Lupyan Lab
University of Wisconsin-Madisonhttps://github.com/jvparidon
________________________________


On Mon, Oct 17, 2022 at 10:02 PM Shira Mitchell <shiraqotj at gmail.com> wrote:

> Questions for Ben Bolker about the excellent GLMM FAQ:
>
> Where does the hglm package fit into this very helpful table ?
>
> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#what-methods-are-available-to-fit-estimate-glmms
>
> I wonder also about differences in model formula specifications, since
> some packages (e.g. lme4) don't seem to accommodate Bradley-Terry, whereas
> some packages (e.g. INLA, hglm, MCMCglmm) can accommodate.
> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#model-specification
>
>
>
>
> On Mon, Oct 17, 2022 at 3:55 PM Shira Mitchell <shiraqotj at gmail.com>
> wrote:
>
>> Thanks so much, Jarrod ! Not too late at all. Very interesting to compare
>> MCMC with the approximations (INLA, hglm's extended quasi likelihood). I
>> don't think I have the priors lined up yet across packages. The random
>> effects seem more dispersed according to MCMCglmm than in INLA or hglm, but
>> this could be due to priors not fit algorithm. Will look into the package
>> prior defaults.
>>
>> On Mon, Oct 17, 2022 at 4:45 AM Jarrod Hadfield <j.hadfield at ed.ac.uk>
>> wrote:
>>
>>> Hi Shira,
>>>
>>> Perhaps a little late to be useful, but MCMCglmm also fits random-effect
>>> Bradley-Terry models. Just specify ~mm(opponent1-opponent2) in the random
>>> effect formula. The mm stands for multimembership - the BT model is like a
>>> multimembership model where some effects have been multiplied by -1, hence
>>> the ?-' rather than ?+? in the mm model formula.
>>>
>>> Cheers,
>>>
>>> Jarrod
>>>
>>>
>>> > On 16 Oct 2022, at 22:49, Shira Mitchell <shiraqotj at gmail.com> wrote:
>>> >
>>> > This email was sent to you by someone outside the University.
>>> > You should only click on links or attachments if you are certain that
>>> the email is genuine and the content is safe.
>>> >
>>> > Update: Dr Heather Turner <https://www.heatherturner.net/> (author of
>>> > BradleyTerry2) suggested the hglm package, which unlike lme4 allows
>>> you to
>>> > specify generic design matrices (no longer constrained to lme4
>>> formulas !)
>>> > Results look really similar to INLA so far. Yay !
>>> >
>>> > On Sun, Oct 16, 2022 at 2:19 PM Shira Mitchell <shiraqotj at gmail.com>
>>> wrote:
>>> >
>>> >> Super helpful !  Thank you so much !
>>> >>
>>> >> Out of curiosity, is there a way to fit this type of Bradley-Terry
>>> model
>>> >> in lme4 ? lme4 formulas include random effect via syntax:
>>> >> https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf
>>> >> "(expr | factor). The expression expr is evaluated as a linear model
>>> >> formula, producing a model matrix following the same rules used in
>>> standard
>>> >> R modeling functions (e.g., `lm` or `glm`). The expression factor is
>>> >> evaluated as an `R` factor. One way to think about the vertical bar
>>> >> operator is as a special kind of interaction between the model matrix
>>> and
>>> >> the grouping factor. This interaction ensures that the columns of the
>>> model
>>> >> matrix have different effects for each level of the grouping factor."
>>> >>
>>> >> So (expr | factor) is X_expr * alpha_factor.
>>> >>
>>> >> So naively writing ~ (1 | m_1) + (1 | m_2) is alpha_{m_1}^{(1)} +
>>> >> alpha_{m_2}^{(2)}, twice as many parameters as what we want which is
>>> >> alpha_{m_1} - alpha_{m_2}.
>>> >>
>>> >> But then see this stackexchange:
>>> >>
>>> >>
>>> https://stats.stackexchange.com/questions/483833/opposing-effects-in-lme4-formulae-bradley-terry-model
>>> >> "I could just make a design matrix, where player 1 gets the value 1,
>>> and
>>> >> player 2 gets the value ?1. However, unless I'm missing a trick, this
>>> would
>>> >> require having a separate column for each player, and plugging each
>>> player
>>> >> column's name into the formula"
>>> >>
>>> >> But suppose we create columns for all m = 1,...,M messages:
>>> >>
>>> >> A_m = 1 if m = m_1
>>> >>           -1 if m = m_2
>>> >>            0 otherwise
>>> >>
>>> >> I think then ~ (A_1 + ... + A_M  | m_1) is alpha_{m_1}^{(m_1)} -
>>> >> alpha_{m_1}^{(m_2)}, also not what we would want.
>>> >>
>>> >> Back to INLA. Suppose we now want to add random message-specific
>>> slopes
>>> >> for variable X_i in addition to random message-specific intercepts:
>>> >>
>>> >> P[i chooses m_1] = logit^-1 (beta_0 + (alpha_{m_1} - alpha_{m_2}) +
>>> >> (beta_{m_1} - beta_{m_2})X_i)
>>> >>
>>> >> alpha_1,...,alpha_M ~ N(0,sigma_intercept)
>>> >> beta_1,...,beta_M ~ N(0,sigma_slope)
>>> >>
>>> >> I see some resources about this, but nothing super comprehensive. Any
>>> >> advice where to look for complete documentation ?
>>> >>
>>> >>
>>> https://groups.google.com/g/r-inla-discussion-group/c/iQELaQF8M9Q/m/q7f4-W8YQksJ
>>> >> (
>>> >>
>>> https://becarioprecario.bitbucket.io/inla-gitbook/ch-multilevel.html#multilevel-models-for-longitudinal-data
>>> >> https://rpubs.com/corey_sparks/431920
>>> >> https://avianecologist.com/2016/10/05/multilevel-models/
>>> >>
>>> >> Here is what we did:
>>> >>
>>> >> data$w_X = -data$X
>>> >> data$m_1_beta = data$m_1
>>> >> data$m_2_beta = data$m_2
>>> >>
>>> >> inla(depvar ~  f(m_1, model="iid", values = issues) +
>>> >>                           f(m_2, w, copy = "m_1") +
>>> >>                           f(m_1_beta, X, model="iid", values =
>>> issues) +
>>> >>                           f(m_2_beta, w_X, copy = "m_1_beta"),
>>> >>                         family="binomial",
>>> >>                         data=data)
>>> >>
>>> >>
>>> >>
>>> >>
>>> >> On Mon, Oct 10, 2022 at 4:24 AM Thierry Onkelinx <
>>> thierry.onkelinx at inbo.be>
>>> >> wrote:
>>> >>
>>> >>> Dear Shira,
>>> >>>
>>> >>> - in a formula object means remove that object from the formula. Use
>>> a
>>> >>> weight of -1 instead.
>>> >>>
>>> >>> f(home, model = "iid")) + f(away, w = -1, copy = "home")
>>> >>>
>>> >>> Best regards,
>>> >>>
>>> >>> ir. Thierry Onkelinx
>>> >>> Statisticus / Statistician
>>> >>>
>>> >>> Vlaamse Overheid / Government of Flanders
>>> >>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR
>>> NATURE
>>> >>> AND FOREST
>>> >>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>>> >>> thierry.onkelinx at inbo.be
>>> >>> Havenlaan 88 bus 73, 1000 Brussel
>>> >>> www.inbo.be
>>> >>>
>>> >>>
>>> >>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>> >>> To call in the statistician after the experiment is done may be no
>>> more
>>> >>> than asking him to perform a post-mortem examination: he may be able
>>> to say
>>> >>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>> >>> The plural of anecdote is not data. ~ Roger Brinner
>>> >>> The combination of some data and an aching desire for an answer does
>>> not
>>> >>> ensure that a reasonable answer can be extracted from a given body
>>> of data.
>>> >>> ~ John Tukey
>>> >>>
>>> >>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>> >>>
>>> >>> <https://www.inbo.be>
>>> >>>
>>> >>>
>>> >>> Op vr 7 okt. 2022 om 23:36 schreef Shira Mitchell <
>>> shiraqotj at gmail.com>:
>>> >>>
>>> >>>> Thanks so much, Thierry ! This is great.
>>> >>>>
>>> >>>> This works except that I cannot subtract because:
>>> >>>> f(home, model = "iid")) - f(away, copy = "home")
>>> >>>>
>>> >>>> just drops the second term. Apologies that I'm not super familiar
>>> with
>>> >>>> INLA syntax yet.
>>> >>>>
>>> >>>>
>>> >>>>
>>> >>>> On Fri, Oct 7, 2022 at 10:19 AM Thierry Onkelinx <
>>> >>>> thierry.onkelinx at inbo.be> wrote:
>>> >>>>
>>> >>>>> Hi Shira,
>>> >>>>>
>>> >>>>> I fit such models with the INLA package (https://www.r-inla.org/).
>>> The
>>> >>>>> trick is to define two random effects but force their parameter
>>> estimates
>>> >>>>> to be identical.
>>> >>>>>
>>> >>>>> The code would contain something like f(home, model = "iid")) +
>>> f(away,
>>> >>>>> copy = "home"). Meaning home ~ N(0, sigma_beta_i) and home[i] =
>>> away[i]
>>> >>>>>
>>> >>>>> Best regards,
>>> >>>>>
>>> >>>>> ir. Thierry Onkelinx
>>> >>>>> Statisticus / Statistician
>>> >>>>>
>>> >>>>> Vlaamse Overheid / Government of Flanders
>>> >>>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR
>>> NATURE
>>> >>>>> AND FOREST
>>> >>>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality
>>> Assurance
>>> >>>>> thierry.onkelinx at inbo.be
>>> >>>>> Havenlaan 88 bus 73, 1000 Brussel
>>> >>>>> www.inbo.be
>>> >>>>>
>>> >>>>>
>>> >>>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>> >>>>> To call in the statistician after the experiment is done may be no
>>> more
>>> >>>>> than asking him to perform a post-mortem examination: he may be
>>> able to say
>>> >>>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>> >>>>> The plural of anecdote is not data. ~ Roger Brinner
>>> >>>>> The combination of some data and an aching desire for an answer
>>> does
>>> >>>>> not ensure that a reasonable answer can be extracted from a given
>>> body of
>>> >>>>> data. ~ John Tukey
>>> >>>>>
>>> >>>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>> >>>>>
>>> >>>>> <https://www.inbo.be>
>>> >>>>>
>>> >>>>>
>>> >>>>> Op vr 7 okt. 2022 om 15:00 schreef Shira Mitchell <
>>> shiraqotj at gmail.com
>>> >>>>>> :
>>> >>>>>
>>> >>>>>> We want to fit Bradley-Terry-style GLMM models in R. We looked
>>> into:
>>> >>>>>>
>>> >>>>>>
>>> >>>>>>
>>> https://cran.r-project.org/web/packages/BradleyTerry2/vignettes/BradleyTerry.pdf
>>> >>>>>> and
>>> >>>>>> http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html
>>> >>>>>>
>>> >>>>>> We have voter-specific variables x that influence which political
>>> >>>>>> message
>>> >>>>>> (i vs j) wins for them:
>>> >>>>>>
>>> >>>>>> logit[pr(i beats j | person with covariate x)] = lambda_i -
>>> lambda_j +
>>> >>>>>> (beta_i - beta_j) x
>>> >>>>>>
>>> >>>>>> We then model parameters as random effects:
>>> >>>>>> lambda_i ~ N(0, sigma_lambda)
>>> >>>>>> beta_i ~ N(0, sigma_beta)
>>> >>>>>>
>>> >>>>>> Is there a way to do this in R ? We do this in TensorFlow in
>>> Python by
>>> >>>>>> directly specifying design matrices with the 0,-1,1 or 0,-x,x
>>> entries.
>>> >>>>>> However, I do not see how to do this in R using lme4,
>>> BradleyTerry2,
>>> >>>>>> mgcv,
>>> >>>>>> etc.
>>> >>>>>>
>>> >>>>>> Thanks so much,
>>> >>>>>> Shira
>>> >>>>>>
>>> >>>>>>        [[alternative HTML version deleted]]
>>> >>>>>>
>>> >>>>>> _______________________________________________
>>> >>>>>> R-sig-mixed-models at r-project.org mailing list
>>> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>> >>>>>>
>>> >>>>>
>>> >
>>> >        [[alternative HTML version deleted]]
>>> >
>>> > _______________________________________________
>>> > R-sig-mixed-models at r-project.org mailing list
>>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>> The University of Edinburgh is a charitable body, registered in
>>> Scotland, with registration number SC005336. Is e buidheann carthannais a
>>> th? ann an Oilthigh Dh?n ?ideann, cl?raichte an Alba, ?ireamh cl?raidh
>>> SC005336.
>>>
>>

	[[alternative HTML version deleted]]


From v@np@r|don @end|ng |rom w|@c@edu  Fri Dec  9 21:22:01 2022
From: v@np@r|don @end|ng |rom w|@c@edu (Jeroen van Paridon)
Date: Fri, 9 Dec 2022 20:22:01 +0000
Subject: [R-sig-ME] Bradley Terry GLMM in R ?
In-Reply-To: <CADeg_O2OYsn1ndkBcVKkwqR2KZZ6uX0AA+E5Boe8T4u_oss7eA@mail.gmail.com>
References: <CADeg_O3L_6Cmn_Wv0tExeCLccFJS1p5fottqprdh3+spDCrXxA@mail.gmail.com>
 <CAJuCY5ys+di7UNWKLSV8gw6qVC5yjWXVa4J7Bboe=cHAwoYDGw@mail.gmail.com>
 <CADeg_O0d+tuG0DnnnFw9o9VSh1dcMUbOitcT_rLbRcdbhNpzYw@mail.gmail.com>
 <CAJuCY5ysJUQQtYUCLF76iD5y9TmSiOD5OAfBt039RjVaD_pzwQ@mail.gmail.com>
 <CADeg_O0g7kTrhRUNhrDZjKt3HEzKpfOKZMW71uBaNTDoaNJt7g@mail.gmail.com>
 <CADeg_O0dzRyG=8u7Zjk94PbwGpoqnWoQRw+VRgpoq-BS2RimgA@mail.gmail.com>
 <E27AFF46-A57B-425D-A946-AE24E6567B7C@ed.ac.uk>
 <CADeg_O12OrDgEMNQ6ejcV30evndEjAuFN-wKLdbN0K4GusyAQw@mail.gmail.com>
 <CADeg_O2p+pWAqyE-9pKaJ34icyGT1OVEwTZkUo8EZuf_68nmKA@mail.gmail.com>
 <CADeg_O2OYsn1ndkBcVKkwqR2KZZ6uX0AA+E5Boe8T4u_oss7eA@mail.gmail.com>
Message-ID: <DM6PR06MB41713C92ED24D8D9FB349875B81C9@DM6PR06MB4171.namprd06.prod.outlook.com>

Hi Shira,

I'm glad you're finding it useful!

If I understand correctly, the model you've run already includes continuous covariates (e.g. (x | indicators)), so I think your question is mostly about the categorical groups?

Your intuition to specify them as random effects using the interaction notation (e.g. (1 | indicators:group)) would be correct for lme4, but unfortunately the lmerMultiMember way of specifying these interactions is a little bit messier (there's some backend complexities that prevent me from using the same syntax as lme4). Instead, you'll have to pre-generate the indicator matrices for the interaction groupings using the interaction_weights() function. There's a worked example of how to do this in the lmerMultiMember vignettes (here: https://jvparidon.github.io/lmerMultiMember/articles/lmermultimember_intro.html#using-nestedinteraction-multiple-membership-to-find-the-player-with-the-strongest-year-of-the-2010s) but the basic steps are as follows:

  1.  Generate a Bradley-Terry matrix Wbt for your messages using bradleyterry_from_sparse() or whatever other method you're using.
  2.  Create a sparse matrix Wg for the grouping factor you want to nest by, using Matrix::fac2sparse(group).
  3.  Generate the interaction matrix Wbtxg using interaction_weights(Wbt, Wg) and then use that as your indicator matrix.

I tend to name the interaction matrix dummies the same way I would specify interactions, with an X replacing the colon, so indicators:group would become indicatorsXgroup. This makes it a little easier to keep track of what the different dummies in the model formula mean.

If you have multiple categorical factors that you want to use as random effects groupings (e.g. (1 | indicatorsXgender) + (1 | indicatorsXemployment_status)) you can create separate interaction matrices for those indicator:grouping interactions.
In most cases, it would make sense to also include the indicators for the messages only, so your formula might end up looking something like (1 + age | indicators) + (1 | indicatorsXgender) + (1 | indicatorsXemployment_status), for example.

I hope this explanation makes sense, but do let me know if it doesn't! (I don't talk about Bradley-Terry models very often, so my terminology and notation may be a bit off.)
Two things to be aware of as you're doing this:

  1.  I haven't gotten around to optimizing the interaction_weights() function, so it's a pretty slow implementation. Generating a matrix for a large dataset could take a few minutes! (I'm hopeful I'll get around to fixing that over the holidays...)
  2.  When you make interaction matrices and add covariates, the number of random effects levels tends to explode, and it's very easy for the number of random effects levels that would need to be estimated to exceed the number of observations in your data. That tends to make the model unidentifiable, meaning that lmerMultiMember can't fit it. I've programmed the package so that it throws an error if this happens, just to be safe.

As for the prediction question: The standard lme4 predict() method should work, so for getting predictions on the training data you can just call:
m <- lmerMultiMember::glmer(...)
predict(m)

Which will return a vector of predictions, in the order of your original data. You can then use that for poststratification, etc.

Unfortunately there is not yet a good method to generate predictions for synthetic cases (i.e. cases with a combination of predictor levels not observed in your training data). Statistically I think this should be possible, but unfortunately the technical implementation would be quite a bit of work, so I haven't gotten around to it.


I hope this helps. Do let me know if any part of it was unclear!


Cheers,

JP
________________________________
From: Shira Mitchell <shiraqotj at gmail.com>
Sent: Friday, December 9, 2022 8:53 AM
To: JP van Paridon <jvparidon at gmail.com>; Jeroen van Paridon <vanparidon at wisc.edu>
Cc: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Bradley Terry GLMM in R ?

Hi Dr van Paridon,

Thank you so much !

We are returning to this after our busy election season. We are using your awesome lmerMultiMember package and have questions.

We have voter-specific variables x that influence which political message (i vs j) wins for them:

logit[pr(i beats j | person with covariate x)] = lambda_i - lambda_j + (beta_i - beta_j) x

We then model parameters as random effects:
lambda_i ~ N(0, sigma_lambda)
beta_i ~ N(0, sigma_beta)

m <- lmerMultiMember::glmer(depvar ~ 1 + (1 | indicators) + (x | indicators),
                                                    family = binomial,
                                                    memberships = list(indicators = W),
                                                    data = dat_train)

This runs beautifully. :)

Now suppose we want the strength of message i among people with covariates x (e.g. a specific age). In reality we have a few covariates, both continuous (x | indicators) and categorical groups (1 | indicators:group).

pr(i beats a hypothetical average message | person with covariate x) = logit^-1 ( lambda_i + beta_i x)

We have a data set that crosses all population x values with all messages, dat_population_all_messages.

Also, if we want to predict specific match-ups from the training data dat_train, how do we do that ?

Thanks again !!

Shira

---------

https://stat.ethz.ch/pipermail/r-sig-mixed-models/2022q4/030224.html

In case it's helpful to anyone following this email thread: I wrote a vignette explaining how to fit a Bradley-Terry model in lme4 using lmerMultiMember. You can find it at https://jvparidon.github.io/lmerMultiMember/articles/bradleyterry_models.html


Cheers,

JP van Paridon (he/him)
Research Associate, Lupyan Lab
University of Wisconsin-Madison
https://github.com/jvparidon
________________________________
From: Jeroen van Paridon <vanparidon using wisc.edu<https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>
Sent: Thursday, October 20, 2022 1:42 AM
To: r-sig-mixed-models using r-project.org<https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models> <r-sig-mixed-models using r-project.org<https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>
Subject: Re: [R-sig-ME] Bradley Terry GLMM in R ?

Hi,

Just to expand on Ben's last email: In principle, lmerMultiMember allows you to pass arbitrary indicator/weight matrices for the random effects to lme4 for model fitting as long as they have the correct shape. The package contains helper functions for generating more conventional multiple membership matrices since that was my own use-case, but if you create your own matrix with opposed (1 and -1) weights I see no reason why it shouldn't work.

Membership matrices need to be sparse matrices of class Matrix::dgCMatrix and shape n_groups x n_obs. You can probably just take whatever indicator matrix you already have, transpose it, and then cast it to the sparse format.

If you're going this route and run into any issues, feel free to reach out to me, directly.


Cheers,

JP van Paridon (he/him)
Research Associate, Lupyan Lab
University of Wisconsin-Madison
https://github.com/jvparidon
________________________________

On Mon, Oct 17, 2022 at 10:02 PM Shira Mitchell <shiraqotj at gmail.com<mailto:shiraqotj at gmail.com>> wrote:
Questions for Ben Bolker about the excellent GLMM FAQ:

Where does the hglm package fit into this very helpful table ?
https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#what-methods-are-available-to-fit-estimate-glmms

I wonder also about differences in model formula specifications, since some packages (e.g. lme4) don't seem to accommodate Bradley-Terry, whereas some packages (e.g. INLA, hglm, MCMCglmm) can accommodate.
https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#model-specification




On Mon, Oct 17, 2022 at 3:55 PM Shira Mitchell <shiraqotj at gmail.com<mailto:shiraqotj at gmail.com>> wrote:
Thanks so much, Jarrod ! Not too late at all. Very interesting to compare MCMC with the approximations (INLA, hglm's extended quasi likelihood). I don't think I have the priors lined up yet across packages. The random effects seem more dispersed according to MCMCglmm than in INLA or hglm, but this could be due to priors not fit algorithm. Will look into the package prior defaults.

On Mon, Oct 17, 2022 at 4:45 AM Jarrod Hadfield <j.hadfield at ed.ac.uk<mailto:j.hadfield at ed.ac.uk>> wrote:
Hi Shira,

Perhaps a little late to be useful, but MCMCglmm also fits random-effect Bradley-Terry models. Just specify ~mm(opponent1-opponent2) in the random effect formula. The mm stands for multimembership - the BT model is like a multimembership model where some effects have been multiplied by -1, hence the ?-' rather than ?+? in the mm model formula.

Cheers,

Jarrod


> On 16 Oct 2022, at 22:49, Shira Mitchell <shiraqotj at gmail.com<mailto:shiraqotj at gmail.com>> wrote:
>
> This email was sent to you by someone outside the University.
> You should only click on links or attachments if you are certain that the email is genuine and the content is safe.
>
> Update: Dr Heather Turner <https://www.heatherturner.net/> (author of
> BradleyTerry2) suggested the hglm package, which unlike lme4 allows you to
> specify generic design matrices (no longer constrained to lme4 formulas !)
> Results look really similar to INLA so far. Yay !
>
> On Sun, Oct 16, 2022 at 2:19 PM Shira Mitchell <shiraqotj at gmail.com<mailto:shiraqotj at gmail.com>> wrote:
>
>> Super helpful !  Thank you so much !
>>
>> Out of curiosity, is there a way to fit this type of Bradley-Terry model
>> in lme4 ? lme4 formulas include random effect via syntax:
>> https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf
>> "(expr | factor). The expression expr is evaluated as a linear model
>> formula, producing a model matrix following the same rules used in standard
>> R modeling functions (e.g., `lm` or `glm`). The expression factor is
>> evaluated as an `R` factor. One way to think about the vertical bar
>> operator is as a special kind of interaction between the model matrix and
>> the grouping factor. This interaction ensures that the columns of the model
>> matrix have different effects for each level of the grouping factor."
>>
>> So (expr | factor) is X_expr * alpha_factor.
>>
>> So naively writing ~ (1 | m_1) + (1 | m_2) is alpha_{m_1}^{(1)} +
>> alpha_{m_2}^{(2)}, twice as many parameters as what we want which is
>> alpha_{m_1} - alpha_{m_2}.
>>
>> But then see this stackexchange:
>>
>> https://stats.stackexchange.com/questions/483833/opposing-effects-in-lme4-formulae-bradley-terry-model
>> "I could just make a design matrix, where player 1 gets the value 1, and
>> player 2 gets the value ?1. However, unless I'm missing a trick, this would
>> require having a separate column for each player, and plugging each player
>> column's name into the formula"
>>
>> But suppose we create columns for all m = 1,...,M messages:
>>
>> A_m = 1 if m = m_1
>>           -1 if m = m_2
>>            0 otherwise
>>
>> I think then ~ (A_1 + ... + A_M  | m_1) is alpha_{m_1}^{(m_1)} -
>> alpha_{m_1}^{(m_2)}, also not what we would want.
>>
>> Back to INLA. Suppose we now want to add random message-specific slopes
>> for variable X_i in addition to random message-specific intercepts:
>>
>> P[i chooses m_1] = logit^-1 (beta_0 + (alpha_{m_1} - alpha_{m_2}) +
>> (beta_{m_1} - beta_{m_2})X_i)
>>
>> alpha_1,...,alpha_M ~ N(0,sigma_intercept)
>> beta_1,...,beta_M ~ N(0,sigma_slope)
>>
>> I see some resources about this, but nothing super comprehensive. Any
>> advice where to look for complete documentation ?
>>
>> https://groups.google.com/g/r-inla-discussion-group/c/iQELaQF8M9Q/m/q7f4-W8YQksJ
>> (
>> https://becarioprecario.bitbucket.io/inla-gitbook/ch-multilevel.html#multilevel-models-for-longitudinal-data
>> https://rpubs.com/corey_sparks/431920
>> https://avianecologist.com/2016/10/05/multilevel-models/
>>
>> Here is what we did:
>>
>> data$w_X = -data$X
>> data$m_1_beta = data$m_1
>> data$m_2_beta = data$m_2
>>
>> inla(depvar ~  f(m_1, model="iid", values = issues) +
>>                           f(m_2, w, copy = "m_1") +
>>                           f(m_1_beta, X, model="iid", values = issues) +
>>                           f(m_2_beta, w_X, copy = "m_1_beta"),
>>                         family="binomial",
>>                         data=data)
>>
>>
>>
>>
>> On Mon, Oct 10, 2022 at 4:24 AM Thierry Onkelinx <thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>>
>> wrote:
>>
>>> Dear Shira,
>>>
>>> - in a formula object means remove that object from the formula. Use a
>>> weight of -1 instead.
>>>
>>> f(home, model = "iid")) + f(away, w = -1, copy = "home")
>>>
>>> Best regards,
>>>
>>> ir. Thierry Onkelinx
>>> Statisticus / Statistician
>>>
>>> Vlaamse Overheid / Government of Flanders
>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>>> AND FOREST
>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>>> thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>
>>> Havenlaan 88 bus 73, 1000 Brussel
>>> www.inbo.be<http://www.inbo.be>
>>>
>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>> To call in the statistician after the experiment is done may be no more
>>> than asking him to perform a post-mortem examination: he may be able to say
>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>> The plural of anecdote is not data. ~ Roger Brinner
>>> The combination of some data and an aching desire for an answer does not
>>> ensure that a reasonable answer can be extracted from a given body of data.
>>> ~ John Tukey
>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>
>>> <https://www.inbo.be>
>>>
>>>
>>> Op vr 7 okt. 2022 om 23:36 schreef Shira Mitchell <shiraqotj at gmail.com<mailto:shiraqotj at gmail.com>>:
>>>
>>>> Thanks so much, Thierry ! This is great.
>>>>
>>>> This works except that I cannot subtract because:
>>>> f(home, model = "iid")) - f(away, copy = "home")
>>>>
>>>> just drops the second term. Apologies that I'm not super familiar with
>>>> INLA syntax yet.
>>>>
>>>>
>>>>
>>>> On Fri, Oct 7, 2022 at 10:19 AM Thierry Onkelinx <
>>>> thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>> wrote:
>>>>
>>>>> Hi Shira,
>>>>>
>>>>> I fit such models with the INLA package (https://www.r-inla.org/). The
>>>>> trick is to define two random effects but force their parameter estimates
>>>>> to be identical.
>>>>>
>>>>> The code would contain something like f(home, model = "iid")) + f(away,
>>>>> copy = "home"). Meaning home ~ N(0, sigma_beta_i) and home[i] = away[i]
>>>>>
>>>>> Best regards,
>>>>>
>>>>> ir. Thierry Onkelinx
>>>>> Statisticus / Statistician
>>>>>
>>>>> Vlaamse Overheid / Government of Flanders
>>>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>>>>> AND FOREST
>>>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>>>>> thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>
>>>>> Havenlaan 88 bus 73, 1000 Brussel
>>>>> www.inbo.be<http://www.inbo.be>
>>>>>
>>>>>
>>>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>> To call in the statistician after the experiment is done may be no more
>>>>> than asking him to perform a post-mortem examination: he may be able to say
>>>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>>>> The plural of anecdote is not data. ~ Roger Brinner
>>>>> The combination of some data and an aching desire for an answer does
>>>>> not ensure that a reasonable answer can be extracted from a given body of
>>>>> data. ~ John Tukey
>>>>>
>>>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>>
>>>>> <https://www.inbo.be>
>>>>>
>>>>>
>>>>> Op vr 7 okt. 2022 om 15:00 schreef Shira Mitchell <shiraqotj at gmail.com<mailto:shiraqotj at gmail.com>
>>>>>> :
>>>>>
>>>>>> We want to fit Bradley-Terry-style GLMM models in R. We looked into:
>>>>>>
>>>>>>
>>>>>> https://cran.r-project.org/web/packages/BradleyTerry2/vignettes/BradleyTerry.pdf
>>>>>> and
>>>>>> http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html
>>>>>>
>>>>>> We have voter-specific variables x that influence which political
>>>>>> message
>>>>>> (i vs j) wins for them:
>>>>>>
>>>>>> logit[pr(i beats j | person with covariate x)] = lambda_i - lambda_j +
>>>>>> (beta_i - beta_j) x
>>>>>>
>>>>>> We then model parameters as random effects:
>>>>>> lambda_i ~ N(0, sigma_lambda)
>>>>>> beta_i ~ N(0, sigma_beta)
>>>>>>
>>>>>> Is there a way to do this in R ? We do this in TensorFlow in Python by
>>>>>> directly specifying design matrices with the 0,-1,1 or 0,-x,x entries.
>>>>>> However, I do not see how to do this in R using lme4, BradleyTerry2,
>>>>>> mgcv,
>>>>>> etc.
>>>>>>
>>>>>> Thanks so much,
>>>>>> Shira
>>>>>>
>>>>>>        [[alternative HTML version deleted]]
>>>>>>
>>>>>> _______________________________________________
>>>>>> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>
>>>>>
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

The University of Edinburgh is a charitable body, registered in Scotland, with registration number SC005336. Is e buidheann carthannais a th? ann an Oilthigh Dh?n ?ideann, cl?raichte an Alba, ?ireamh cl?raidh SC005336.

	[[alternative HTML version deleted]]


From @h|r@qotj @end|ng |rom gm@||@com  Sun Dec 11 15:45:31 2022
From: @h|r@qotj @end|ng |rom gm@||@com (Shira Mitchell)
Date: Sun, 11 Dec 2022 15:45:31 +0100
Subject: [R-sig-ME] Bradley Terry GLMM in R ?
In-Reply-To: <DM6PR06MB41713C92ED24D8D9FB349875B81C9@DM6PR06MB4171.namprd06.prod.outlook.com>
References: <CADeg_O3L_6Cmn_Wv0tExeCLccFJS1p5fottqprdh3+spDCrXxA@mail.gmail.com>
 <CAJuCY5ys+di7UNWKLSV8gw6qVC5yjWXVa4J7Bboe=cHAwoYDGw@mail.gmail.com>
 <CADeg_O0d+tuG0DnnnFw9o9VSh1dcMUbOitcT_rLbRcdbhNpzYw@mail.gmail.com>
 <CAJuCY5ysJUQQtYUCLF76iD5y9TmSiOD5OAfBt039RjVaD_pzwQ@mail.gmail.com>
 <CADeg_O0g7kTrhRUNhrDZjKt3HEzKpfOKZMW71uBaNTDoaNJt7g@mail.gmail.com>
 <CADeg_O0dzRyG=8u7Zjk94PbwGpoqnWoQRw+VRgpoq-BS2RimgA@mail.gmail.com>
 <E27AFF46-A57B-425D-A946-AE24E6567B7C@ed.ac.uk>
 <CADeg_O12OrDgEMNQ6ejcV30evndEjAuFN-wKLdbN0K4GusyAQw@mail.gmail.com>
 <CADeg_O2p+pWAqyE-9pKaJ34icyGT1OVEwTZkUo8EZuf_68nmKA@mail.gmail.com>
 <CADeg_O2OYsn1ndkBcVKkwqR2KZZ6uX0AA+E5Boe8T4u_oss7eA@mail.gmail.com>
 <DM6PR06MB41713C92ED24D8D9FB349875B81C9@DM6PR06MB4171.namprd06.prod.outlook.com>
Message-ID: <CADeg_O0YzZfSksPjwGrM0suEY8y5FrD8O1CU8m7FMU0s3EP7oQ@mail.gmail.com>

Hi JP !

This is all super helpful. Follow-up questions:

1. What is the difference between
anova(m_extra, m) as used in
http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#testing-significance-of-random-effects

and
lmtest::lrtest(m_extra, m) as used in
https://github.com/jvparidon/lmerMultiMember/blob/main/vignettes/lmermultimember_intro.Rmd
?

2. In summary(m) output, why is "Group memberships per observation for
multiple membership REs:" seem to always be all zeros ?
https://jvparidon.github.io/lmerMultiMember/articles/bradleyterry_models.html#sanity-check-on-random-intercepts-team-strength

3. We need to generate predictions for synthetic cases:
pr(message i beats a hypothetical average message | person with covariate
x) = logit^-1 ( lambda_i + beta_i x)
If I understand correctly, this isn't yet implemented ? So I wrote a clunky
ad hoc version for my model that pulls lambda_i and beta_i from ranef(m)
and multiplies by covariate x matrices and takes inverse logit to get the
above.

Thanks again,
Shira

On Fri, Dec 9, 2022 at 9:22 PM Jeroen van Paridon <vanparidon at wisc.edu>
wrote:

> Hi Shira,
>
> I'm glad you're finding it useful!
>
> If I understand correctly, the model you've run already includes
> continuous covariates (e.g. (x | indicators)), so I think your question
> is mostly about the categorical groups?
>
> Your intuition to specify them as random effects using the interaction
> notation (e.g. (1 | indicators:group)) would be correct for lme4, but
> unfortunately the lmerMultiMember way of specifying these interactions is a
> little bit messier (there's some backend complexities that prevent me from
> using the same syntax as lme4). Instead, you'll have to pre-generate the
> indicator matrices for the interaction groupings using the
> interaction_weights() function. There's a worked example of how to do
> this in the lmerMultiMember vignettes (here:
> https://jvparidon.github.io/lmerMultiMember/articles/lmermultimember_intro.html#using-nestedinteraction-multiple-membership-to-find-the-player-with-the-strongest-year-of-the-2010s)
> but the basic steps are as follows:
>
>    1. Generate a Bradley-Terry matrix Wbt for your messages using
>    bradleyterry_from_sparse() or whatever other method you're using.
>    2. Create a sparse matrix Wg for the grouping factor you want to nest
>    by, using Matrix::fac2sparse(group).
>    3. Generate the interaction matrix Wbtxg using interaction_weights(Wbt,
>    Wg) and then use that as your indicator matrix.
>
> I tend to name the interaction matrix dummies the same way I would specify
> interactions, with an X replacing the colon, so indicators:group would
> become indicatorsXgroup. This makes it a little easier to keep track of
> what the different dummies in the model formula mean.
>
> If you have multiple categorical factors that you want to use as random
> effects groupings (e.g. (1 | indicatorsXgender) + (1 |
> indicatorsXemployment_status)) you can create separate interaction matrices
> for those indicator:grouping interactions.
> In most cases, it would make sense to *also* include the indicators for
> the messages *only*, so your formula might end up looking something like
> (1 + age | indicators) + (1 | indicatorsXgender) + (1 |
> indicatorsXemployment_status), for example.
>
> I hope this explanation makes sense, but do let me know if it doesn't! (I
> don't talk about Bradley-Terry models very often, so my terminology and
> notation may be a bit off.)
> Two things to be aware of as you're doing this:
>
>    1. I haven't gotten around to optimizing the interaction_weights()
>    function, so it's a pretty slow implementation. Generating a matrix for a
>    large dataset could take a few minutes! (I'm hopeful I'll get around to
>    fixing that over the holidays...)
>    2. When you make interaction matrices and add covariates, the number
>    of random effects levels tends to explode, and it's very easy for the
>    number of random effects levels that would need to be estimated to exceed
>    the number of observations in your data. That tends to make the model
>    unidentifiable, meaning that lmerMultiMember can't fit it. I've programmed
>    the package so that it throws an error if this happens, just to be safe.
>
>
> As for the prediction question: The standard lme4 predict() method should
> work, so for getting predictions on the training data you can just call:
> m <- lmerMultiMember::glmer(...)
> predict(m)
>
> Which will return a vector of predictions, in the order of your original
> data. You can then use that for poststratification, etc.
>
> Unfortunately there is not yet a good method to generate predictions for
> synthetic cases (i.e. cases with a combination of predictor levels not
> observed in your training data). Statistically I think this should be
> possible, but unfortunately the technical implementation would be quite a
> bit of work, so I haven't gotten around to it.
>
>
> I hope this helps. Do let me know if any part of it was unclear!
>
>
> Cheers,
>
> JP
> ------------------------------
> *From:* Shira Mitchell <shiraqotj at gmail.com>
> *Sent:* Friday, December 9, 2022 8:53 AM
> *To:* JP van Paridon <jvparidon at gmail.com>; Jeroen van Paridon <
> vanparidon at wisc.edu>
> *Cc:* r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
> *Subject:* Re: [R-sig-ME] Bradley Terry GLMM in R ?
>
> Hi Dr van Paridon,
>
> Thank you so much !
>
> We are returning to this after our busy election season. We are using your
> awesome lmerMultiMember package and have questions.
>
> We have voter-specific variables x that influence which political message
> (i vs j) wins for them:
>
> logit[pr(i beats j | person with covariate x)] = lambda_i - lambda_j +
> (beta_i - beta_j) x
>
> We then model parameters as random effects:
> lambda_i ~ N(0, sigma_lambda)
> beta_i ~ N(0, sigma_beta)
>
> m <- lmerMultiMember::glmer(depvar ~ 1 + (1 | indicators) + (x |
> indicators),
>                                                     family = binomial,
>                                                     memberships =
> list(indicators = W),
>                                                     data = dat_train)
>
> This runs beautifully. :)
>
> Now suppose we want the strength of message i among people with covariates
> x (e.g. a specific age). In reality we have a few covariates, both
> continuous (x | indicators) and categorical groups (1 | indicators:group).
>
> pr(i beats a hypothetical average message | person with covariate x) =
> logit^-1 ( lambda_i + beta_i x)
>
> We have a data set that crosses all population x values with all messages,
> dat_population_all_messages.
>
> Also, if we want to predict specific match-ups from the training data
> dat_train, how do we do that ?
>
> Thanks again !!
>
> Shira
>
> ---------
>
> https://stat.ethz.ch/pipermail/r-sig-mixed-models/2022q4/030224.html
>
> In case it's helpful to anyone following this email thread: I wrote a vignette explaining how to fit a Bradley-Terry model in lme4 using lmerMultiMember. You can find it at https://jvparidon.github.io/lmerMultiMember/articles/bradleyterry_models.html
>
>
> Cheers,
>
> JP van Paridon (he/him)
> Research Associate, Lupyan Lab
> University of Wisconsin-Madisonhttps://github.com/jvparidon
> ________________________________
> From: Jeroen van Paridon <vanparidon using wisc.edu <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>
> Sent: Thursday, October 20, 2022 1:42 AM
> To: r-sig-mixed-models using r-project.org <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models> <r-sig-mixed-models using r-project.org <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>
> Subject: Re: [R-sig-ME] Bradley Terry GLMM in R ?
>
> Hi,
>
> Just to expand on Ben's last email: In principle, lmerMultiMember allows you to pass arbitrary indicator/weight matrices for the random effects to lme4 for model fitting as long as they have the correct shape. The package contains helper functions for generating more conventional multiple membership matrices since that was my own use-case, but if you create your own matrix with opposed (1 and -1) weights I see no reason why it shouldn't work.
>
> Membership matrices need to be sparse matrices of class Matrix::dgCMatrix and shape n_groups x n_obs. You can probably just take whatever indicator matrix you already have, transpose it, and then cast it to the sparse format.
>
> If you're going this route and run into any issues, feel free to reach out to me, directly.
>
>
> Cheers,
>
> JP van Paridon (he/him)
> Research Associate, Lupyan Lab
> University of Wisconsin-Madisonhttps://github.com/jvparidon
> ________________________________
>
>
> On Mon, Oct 17, 2022 at 10:02 PM Shira Mitchell <shiraqotj at gmail.com>
> wrote:
>
> Questions for Ben Bolker about the excellent GLMM FAQ:
>
> Where does the hglm package fit into this very helpful table ?
>
> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#what-methods-are-available-to-fit-estimate-glmms
>
> I wonder also about differences in model formula specifications, since
> some packages (e.g. lme4) don't seem to accommodate Bradley-Terry, whereas
> some packages (e.g. INLA, hglm, MCMCglmm) can accommodate.
> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#model-specification
>
>
>
>
> On Mon, Oct 17, 2022 at 3:55 PM Shira Mitchell <shiraqotj at gmail.com>
> wrote:
>
> Thanks so much, Jarrod ! Not too late at all. Very interesting to compare
> MCMC with the approximations (INLA, hglm's extended quasi likelihood). I
> don't think I have the priors lined up yet across packages. The random
> effects seem more dispersed according to MCMCglmm than in INLA or hglm, but
> this could be due to priors not fit algorithm. Will look into the package
> prior defaults.
>
> On Mon, Oct 17, 2022 at 4:45 AM Jarrod Hadfield <j.hadfield at ed.ac.uk>
> wrote:
>
> Hi Shira,
>
> Perhaps a little late to be useful, but MCMCglmm also fits random-effect
> Bradley-Terry models. Just specify ~mm(opponent1-opponent2) in the random
> effect formula. The mm stands for multimembership - the BT model is like a
> multimembership model where some effects have been multiplied by -1, hence
> the ?-' rather than ?+? in the mm model formula.
>
> Cheers,
>
> Jarrod
>
>
> > On 16 Oct 2022, at 22:49, Shira Mitchell <shiraqotj at gmail.com> wrote:
> >
> > This email was sent to you by someone outside the University.
> > You should only click on links or attachments if you are certain that
> the email is genuine and the content is safe.
> >
> > Update: Dr Heather Turner <https://www.heatherturner.net/> (author of
> > BradleyTerry2) suggested the hglm package, which unlike lme4 allows you
> to
> > specify generic design matrices (no longer constrained to lme4 formulas
> !)
> > Results look really similar to INLA so far. Yay !
> >
> > On Sun, Oct 16, 2022 at 2:19 PM Shira Mitchell <shiraqotj at gmail.com>
> wrote:
> >
> >> Super helpful !  Thank you so much !
> >>
> >> Out of curiosity, is there a way to fit this type of Bradley-Terry model
> >> in lme4 ? lme4 formulas include random effect via syntax:
> >> https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf
> >> "(expr | factor). The expression expr is evaluated as a linear model
> >> formula, producing a model matrix following the same rules used in
> standard
> >> R modeling functions (e.g., `lm` or `glm`). The expression factor is
> >> evaluated as an `R` factor. One way to think about the vertical bar
> >> operator is as a special kind of interaction between the model matrix
> and
> >> the grouping factor. This interaction ensures that the columns of the
> model
> >> matrix have different effects for each level of the grouping factor."
> >>
> >> So (expr | factor) is X_expr * alpha_factor.
> >>
> >> So naively writing ~ (1 | m_1) + (1 | m_2) is alpha_{m_1}^{(1)} +
> >> alpha_{m_2}^{(2)}, twice as many parameters as what we want which is
> >> alpha_{m_1} - alpha_{m_2}.
> >>
> >> But then see this stackexchange:
> >>
> >>
> https://stats.stackexchange.com/questions/483833/opposing-effects-in-lme4-formulae-bradley-terry-model
> >> "I could just make a design matrix, where player 1 gets the value 1, and
> >> player 2 gets the value ?1. However, unless I'm missing a trick, this
> would
> >> require having a separate column for each player, and plugging each
> player
> >> column's name into the formula"
> >>
> >> But suppose we create columns for all m = 1,...,M messages:
> >>
> >> A_m = 1 if m = m_1
> >>           -1 if m = m_2
> >>            0 otherwise
> >>
> >> I think then ~ (A_1 + ... + A_M  | m_1) is alpha_{m_1}^{(m_1)} -
> >> alpha_{m_1}^{(m_2)}, also not what we would want.
> >>
> >> Back to INLA. Suppose we now want to add random message-specific slopes
> >> for variable X_i in addition to random message-specific intercepts:
> >>
> >> P[i chooses m_1] = logit^-1 (beta_0 + (alpha_{m_1} - alpha_{m_2}) +
> >> (beta_{m_1} - beta_{m_2})X_i)
> >>
> >> alpha_1,...,alpha_M ~ N(0,sigma_intercept)
> >> beta_1,...,beta_M ~ N(0,sigma_slope)
> >>
> >> I see some resources about this, but nothing super comprehensive. Any
> >> advice where to look for complete documentation ?
> >>
> >>
> https://groups.google.com/g/r-inla-discussion-group/c/iQELaQF8M9Q/m/q7f4-W8YQksJ
> >> (
> >>
> https://becarioprecario.bitbucket.io/inla-gitbook/ch-multilevel.html#multilevel-models-for-longitudinal-data
> >> https://rpubs.com/corey_sparks/431920
> >> https://avianecologist.com/2016/10/05/multilevel-models/
> >>
> >> Here is what we did:
> >>
> >> data$w_X = -data$X
> >> data$m_1_beta = data$m_1
> >> data$m_2_beta = data$m_2
> >>
> >> inla(depvar ~  f(m_1, model="iid", values = issues) +
> >>                           f(m_2, w, copy = "m_1") +
> >>                           f(m_1_beta, X, model="iid", values = issues) +
> >>                           f(m_2_beta, w_X, copy = "m_1_beta"),
> >>                         family="binomial",
> >>                         data=data)
> >>
> >>
> >>
> >>
> >> On Mon, Oct 10, 2022 at 4:24 AM Thierry Onkelinx <
> thierry.onkelinx at inbo.be>
> >> wrote:
> >>
> >>> Dear Shira,
> >>>
> >>> - in a formula object means remove that object from the formula. Use a
> >>> weight of -1 instead.
> >>>
> >>> f(home, model = "iid")) + f(away, w = -1, copy = "home")
> >>>
> >>> Best regards,
> >>>
> >>> ir. Thierry Onkelinx
> >>> Statisticus / Statistician
> >>>
> >>> Vlaamse Overheid / Government of Flanders
> >>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
> >>> AND FOREST
> >>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> >>> thierry.onkelinx at inbo.be
> >>> Havenlaan 88 bus 73, 1000 Brussel
> >>> www.inbo.be
> >>>
> >>>
> >>>
> ///////////////////////////////////////////////////////////////////////////////////////////
> >>> To call in the statistician after the experiment is done may be no more
> >>> than asking him to perform a post-mortem examination: he may be able
> to say
> >>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> >>> The plural of anecdote is not data. ~ Roger Brinner
> >>> The combination of some data and an aching desire for an answer does
> not
> >>> ensure that a reasonable answer can be extracted from a given body of
> data.
> >>> ~ John Tukey
> >>>
> >>>
> ///////////////////////////////////////////////////////////////////////////////////////////
> >>>
> >>> <https://www.inbo.be>
> >>>
> >>>
> >>> Op vr 7 okt. 2022 om 23:36 schreef Shira Mitchell <shiraqotj at gmail.com
> >:
> >>>
> >>>> Thanks so much, Thierry ! This is great.
> >>>>
> >>>> This works except that I cannot subtract because:
> >>>> f(home, model = "iid")) - f(away, copy = "home")
> >>>>
> >>>> just drops the second term. Apologies that I'm not super familiar with
> >>>> INLA syntax yet.
> >>>>
> >>>>
> >>>>
> >>>> On Fri, Oct 7, 2022 at 10:19 AM Thierry Onkelinx <
> >>>> thierry.onkelinx at inbo.be> wrote:
> >>>>
> >>>>> Hi Shira,
> >>>>>
> >>>>> I fit such models with the INLA package (https://www.r-inla.org/).
> The
> >>>>> trick is to define two random effects but force their parameter
> estimates
> >>>>> to be identical.
> >>>>>
> >>>>> The code would contain something like f(home, model = "iid")) +
> f(away,
> >>>>> copy = "home"). Meaning home ~ N(0, sigma_beta_i) and home[i] =
> away[i]
> >>>>>
> >>>>> Best regards,
> >>>>>
> >>>>> ir. Thierry Onkelinx
> >>>>> Statisticus / Statistician
> >>>>>
> >>>>> Vlaamse Overheid / Government of Flanders
> >>>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR
> NATURE
> >>>>> AND FOREST
> >>>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> >>>>> thierry.onkelinx at inbo.be
> >>>>> Havenlaan 88 bus 73, 1000 Brussel
> >>>>> www.inbo.be
> >>>>>
> >>>>>
> >>>>>
> ///////////////////////////////////////////////////////////////////////////////////////////
> >>>>> To call in the statistician after the experiment is done may be no
> more
> >>>>> than asking him to perform a post-mortem examination: he may be able
> to say
> >>>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> >>>>> The plural of anecdote is not data. ~ Roger Brinner
> >>>>> The combination of some data and an aching desire for an answer does
> >>>>> not ensure that a reasonable answer can be extracted from a given
> body of
> >>>>> data. ~ John Tukey
> >>>>>
> >>>>>
> ///////////////////////////////////////////////////////////////////////////////////////////
> >>>>>
> >>>>> <https://www.inbo.be>
> >>>>>
> >>>>>
> >>>>> Op vr 7 okt. 2022 om 15:00 schreef Shira Mitchell <
> shiraqotj at gmail.com
> >>>>>> :
> >>>>>
> >>>>>> We want to fit Bradley-Terry-style GLMM models in R. We looked into:
> >>>>>>
> >>>>>>
> >>>>>>
> https://cran.r-project.org/web/packages/BradleyTerry2/vignettes/BradleyTerry.pdf
> >>>>>> and
> >>>>>> http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html
> >>>>>>
> >>>>>> We have voter-specific variables x that influence which political
> >>>>>> message
> >>>>>> (i vs j) wins for them:
> >>>>>>
> >>>>>> logit[pr(i beats j | person with covariate x)] = lambda_i -
> lambda_j +
> >>>>>> (beta_i - beta_j) x
> >>>>>>
> >>>>>> We then model parameters as random effects:
> >>>>>> lambda_i ~ N(0, sigma_lambda)
> >>>>>> beta_i ~ N(0, sigma_beta)
> >>>>>>
> >>>>>> Is there a way to do this in R ? We do this in TensorFlow in Python
> by
> >>>>>> directly specifying design matrices with the 0,-1,1 or 0,-x,x
> entries.
> >>>>>> However, I do not see how to do this in R using lme4, BradleyTerry2,
> >>>>>> mgcv,
> >>>>>> etc.
> >>>>>>
> >>>>>> Thanks so much,
> >>>>>> Shira
> >>>>>>
> >>>>>>        [[alternative HTML version deleted]]
> >>>>>>
> >>>>>> _______________________________________________
> >>>>>> R-sig-mixed-models at r-project.org mailing list
> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>>>>>
> >>>>>
> >
> >        [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> The University of Edinburgh is a charitable body, registered in Scotland,
> with registration number SC005336. Is e buidheann carthannais a th? ann an
> Oilthigh Dh?n ?ideann, cl?raichte an Alba, ?ireamh cl?raidh SC005336.
>
>

	[[alternative HTML version deleted]]


From v@np@r|don @end|ng |rom w|@c@edu  Mon Dec 12 02:55:38 2022
From: v@np@r|don @end|ng |rom w|@c@edu (Jeroen van Paridon)
Date: Mon, 12 Dec 2022 01:55:38 +0000
Subject: [R-sig-ME] Bradley Terry GLMM in R ?
In-Reply-To: <CADeg_O0YzZfSksPjwGrM0suEY8y5FrD8O1CU8m7FMU0s3EP7oQ@mail.gmail.com>
References: <CADeg_O3L_6Cmn_Wv0tExeCLccFJS1p5fottqprdh3+spDCrXxA@mail.gmail.com>
 <CAJuCY5ys+di7UNWKLSV8gw6qVC5yjWXVa4J7Bboe=cHAwoYDGw@mail.gmail.com>
 <CADeg_O0d+tuG0DnnnFw9o9VSh1dcMUbOitcT_rLbRcdbhNpzYw@mail.gmail.com>
 <CAJuCY5ysJUQQtYUCLF76iD5y9TmSiOD5OAfBt039RjVaD_pzwQ@mail.gmail.com>
 <CADeg_O0g7kTrhRUNhrDZjKt3HEzKpfOKZMW71uBaNTDoaNJt7g@mail.gmail.com>
 <CADeg_O0dzRyG=8u7Zjk94PbwGpoqnWoQRw+VRgpoq-BS2RimgA@mail.gmail.com>
 <E27AFF46-A57B-425D-A946-AE24E6567B7C@ed.ac.uk>
 <CADeg_O12OrDgEMNQ6ejcV30evndEjAuFN-wKLdbN0K4GusyAQw@mail.gmail.com>
 <CADeg_O2p+pWAqyE-9pKaJ34icyGT1OVEwTZkUo8EZuf_68nmKA@mail.gmail.com>
 <CADeg_O2OYsn1ndkBcVKkwqR2KZZ6uX0AA+E5Boe8T4u_oss7eA@mail.gmail.com>
 <DM6PR06MB41713C92ED24D8D9FB349875B81C9@DM6PR06MB4171.namprd06.prod.outlook.com>
 <CADeg_O0YzZfSksPjwGrM0suEY8y5FrD8O1CU8m7FMU0s3EP7oQ@mail.gmail.com>
Message-ID: <DM6PR06MB4171A2C7E2EC0CEF8CE0852AB8E29@DM6PR06MB4171.namprd06.prod.outlook.com>

Hi Shira,

I'll just answer your questions in order:

  1.
For the generalized linear mixed model presented in the lmerMultiMember vignette, both anova() and lmtest::lrtest() perform a Chi-square test, so there's no meaningful difference other than that anova() also returns AIC and BIC statistics. I wasn't intending to discuss AIC and BIC in the vignette, so I just used lrtest() for its simpler results table :-)
I may reconsider that decision however, because there are other cases where anova() and lrtest() will return different results or even different tests, and anova() usually has the more sensible default behavior. Two examples I am aware of:

A) For simple linear models using lm(), it seems that lrtest() performs a Chi-square test, while anova() performs an F-test. (See https://stats.stackexchange.com/questions/535709/anova-vs-likelihood-ratio-test-different-result)
I believe anova() does not perform the F-test for mixed-effects models because estimating degrees of freedom for these models is not straightforward.

B) For linear mixed effects models that return lmerMod (so lmer() rather than glmer()), the default behavior of anova() is to refit REML-fit models with ML before comparing them, whereas lrtest() will just compare the REML-fit models without warning. (As I understand it, comparing REML-fit models is considered acceptable if the models only differ in their random effects, but wrong if the models differ in their fixed effects.)

  2.  The summary output for the group memberships is something I implemented as a quick diagnostic feature for multiple membership models. If you know all your observations are supposed to have e.g. 2 memberships, or 4 memberships on average but never more than 8, this information is easy to verify in the model summary. Strictly speaking I guess these are properties of the indicator matrix rather than the model, but I wanted to put them in a place where people are likely to see it if there is something wrong with their indicator matrix!

I didn't consider Bradley-Terry models when I first implemented the summary, but it makes sense that the summary stats all return 0: The indicators for each observation are supposed to sum to 0 for B-T models, so for these models non-zero min/mean/max values would indicate there is a problem with your indicator matrix.
On my to-do list I have adding a "mean count of non-zero values per observation" stat to the summary, which should make diagnosing problems with B-T indicator matrices easier.

  3.  That is exactly how I would go about predicting unobserved combinations of predictors, for now. One small tip for automatically looking up random effects when you're using matrices for nested grouping factors: interaction_weights() combines grouping levels using a period as separator. So if you've used interaction_weights(color, shape) the some of the levels in the indicator matrix would be red.square, green.circle, etc.

I hope this helps!
I really appreciate the feedback about usability/vignettes/features/etc.


Cheers,

JP


________________________________
From: Shira Mitchell <shiraqotj at gmail.com>
Sent: Sunday, December 11, 2022 8:45 AM
To: Jeroen van Paridon <vanparidon at wisc.edu>
Cc: JP van Paridon <jvparidon at gmail.com>; r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Bradley Terry GLMM in R ?

Hi JP !

This is all super helpful. Follow-up questions:

1. What is the difference between
anova(m_extra, m) as used in http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#testing-significance-of-random-effects
and
lmtest::lrtest(m_extra, m) as used in https://github.com/jvparidon/lmerMultiMember/blob/main/vignettes/lmermultimember_intro.Rmd
?

2. In summary(m) output, why is "Group memberships per observation for multiple membership REs:" seem to always be all zeros ?
https://jvparidon.github.io/lmerMultiMember/articles/bradleyterry_models.html#sanity-check-on-random-intercepts-team-strength

3. We need to generate predictions for synthetic cases:
pr(message i beats a hypothetical average message | person with covariate x) = logit^-1 ( lambda_i + beta_i x)
If I understand correctly, this isn't yet implemented ? So I wrote a clunky ad hoc version for my model that pulls lambda_i and beta_i from ranef(m) and multiplies by covariate x matrices and takes inverse logit to get the above.

Thanks again,
Shira

On Fri, Dec 9, 2022 at 9:22 PM Jeroen van Paridon <vanparidon at wisc.edu<mailto:vanparidon at wisc.edu>> wrote:
Hi Shira,

I'm glad you're finding it useful!

If I understand correctly, the model you've run already includes continuous covariates (e.g. (x | indicators)), so I think your question is mostly about the categorical groups?

Your intuition to specify them as random effects using the interaction notation (e.g. (1 | indicators:group)) would be correct for lme4, but unfortunately the lmerMultiMember way of specifying these interactions is a little bit messier (there's some backend complexities that prevent me from using the same syntax as lme4). Instead, you'll have to pre-generate the indicator matrices for the interaction groupings using the interaction_weights() function. There's a worked example of how to do this in the lmerMultiMember vignettes (here: https://jvparidon.github.io/lmerMultiMember/articles/lmermultimember_intro.html#using-nestedinteraction-multiple-membership-to-find-the-player-with-the-strongest-year-of-the-2010s) but the basic steps are as follows:

  1.  Generate a Bradley-Terry matrix Wbt for your messages using bradleyterry_from_sparse() or whatever other method you're using.
  2.  Create a sparse matrix Wg for the grouping factor you want to nest by, using Matrix::fac2sparse(group).
  3.  Generate the interaction matrix Wbtxg using interaction_weights(Wbt, Wg) and then use that as your indicator matrix.

I tend to name the interaction matrix dummies the same way I would specify interactions, with an X replacing the colon, so indicators:group would become indicatorsXgroup. This makes it a little easier to keep track of what the different dummies in the model formula mean.

If you have multiple categorical factors that you want to use as random effects groupings (e.g. (1 | indicatorsXgender) + (1 | indicatorsXemployment_status)) you can create separate interaction matrices for those indicator:grouping interactions.
In most cases, it would make sense to also include the indicators for the messages only, so your formula might end up looking something like (1 + age | indicators) + (1 | indicatorsXgender) + (1 | indicatorsXemployment_status), for example.

I hope this explanation makes sense, but do let me know if it doesn't! (I don't talk about Bradley-Terry models very often, so my terminology and notation may be a bit off.)
Two things to be aware of as you're doing this:

  1.  I haven't gotten around to optimizing the interaction_weights() function, so it's a pretty slow implementation. Generating a matrix for a large dataset could take a few minutes! (I'm hopeful I'll get around to fixing that over the holidays...)
  2.  When you make interaction matrices and add covariates, the number of random effects levels tends to explode, and it's very easy for the number of random effects levels that would need to be estimated to exceed the number of observations in your data. That tends to make the model unidentifiable, meaning that lmerMultiMember can't fit it. I've programmed the package so that it throws an error if this happens, just to be safe.

As for the prediction question: The standard lme4 predict() method should work, so for getting predictions on the training data you can just call:
m <- lmerMultiMember::glmer(...)
predict(m)

Which will return a vector of predictions, in the order of your original data. You can then use that for poststratification, etc.

Unfortunately there is not yet a good method to generate predictions for synthetic cases (i.e. cases with a combination of predictor levels not observed in your training data). Statistically I think this should be possible, but unfortunately the technical implementation would be quite a bit of work, so I haven't gotten around to it.


I hope this helps. Do let me know if any part of it was unclear!


Cheers,

JP
________________________________
From: Shira Mitchell <shiraqotj at gmail.com<mailto:shiraqotj at gmail.com>>
Sent: Friday, December 9, 2022 8:53 AM
To: JP van Paridon <jvparidon at gmail.com<mailto:jvparidon at gmail.com>>; Jeroen van Paridon <vanparidon at wisc.edu<mailto:vanparidon at wisc.edu>>
Cc: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org> <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
Subject: Re: [R-sig-ME] Bradley Terry GLMM in R ?

Hi Dr van Paridon,

Thank you so much !

We are returning to this after our busy election season. We are using your awesome lmerMultiMember package and have questions.

We have voter-specific variables x that influence which political message (i vs j) wins for them:

logit[pr(i beats j | person with covariate x)] = lambda_i - lambda_j + (beta_i - beta_j) x

We then model parameters as random effects:
lambda_i ~ N(0, sigma_lambda)
beta_i ~ N(0, sigma_beta)

m <- lmerMultiMember::glmer(depvar ~ 1 + (1 | indicators) + (x | indicators),
                                                    family = binomial,
                                                    memberships = list(indicators = W),
                                                    data = dat_train)

This runs beautifully. :)

Now suppose we want the strength of message i among people with covariates x (e.g. a specific age). In reality we have a few covariates, both continuous (x | indicators) and categorical groups (1 | indicators:group).

pr(i beats a hypothetical average message | person with covariate x) = logit^-1 ( lambda_i + beta_i x)

We have a data set that crosses all population x values with all messages, dat_population_all_messages.

Also, if we want to predict specific match-ups from the training data dat_train, how do we do that ?

Thanks again !!

Shira

---------

https://stat.ethz.ch/pipermail/r-sig-mixed-models/2022q4/030224.html

In case it's helpful to anyone following this email thread: I wrote a vignette explaining how to fit a Bradley-Terry model in lme4 using lmerMultiMember. You can find it at https://jvparidon.github.io/lmerMultiMember/articles/bradleyterry_models.html


Cheers,

JP van Paridon (he/him)
Research Associate, Lupyan Lab
University of Wisconsin-Madison
https://github.com/jvparidon
________________________________
From: Jeroen van Paridon <vanparidon using wisc.edu<https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>
Sent: Thursday, October 20, 2022 1:42 AM
To: r-sig-mixed-models using r-project.org<https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models> <r-sig-mixed-models using r-project.org<https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>
Subject: Re: [R-sig-ME] Bradley Terry GLMM in R ?

Hi,

Just to expand on Ben's last email: In principle, lmerMultiMember allows you to pass arbitrary indicator/weight matrices for the random effects to lme4 for model fitting as long as they have the correct shape. The package contains helper functions for generating more conventional multiple membership matrices since that was my own use-case, but if you create your own matrix with opposed (1 and -1) weights I see no reason why it shouldn't work.

Membership matrices need to be sparse matrices of class Matrix::dgCMatrix and shape n_groups x n_obs. You can probably just take whatever indicator matrix you already have, transpose it, and then cast it to the sparse format.

If you're going this route and run into any issues, feel free to reach out to me, directly.


Cheers,

JP van Paridon (he/him)
Research Associate, Lupyan Lab
University of Wisconsin-Madison
https://github.com/jvparidon
________________________________

On Mon, Oct 17, 2022 at 10:02 PM Shira Mitchell <shiraqotj at gmail.com<mailto:shiraqotj at gmail.com>> wrote:
Questions for Ben Bolker about the excellent GLMM FAQ:

Where does the hglm package fit into this very helpful table ?
https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#what-methods-are-available-to-fit-estimate-glmms

I wonder also about differences in model formula specifications, since some packages (e.g. lme4) don't seem to accommodate Bradley-Terry, whereas some packages (e.g. INLA, hglm, MCMCglmm) can accommodate.
https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#model-specification




On Mon, Oct 17, 2022 at 3:55 PM Shira Mitchell <shiraqotj at gmail.com<mailto:shiraqotj at gmail.com>> wrote:
Thanks so much, Jarrod ! Not too late at all. Very interesting to compare MCMC with the approximations (INLA, hglm's extended quasi likelihood). I don't think I have the priors lined up yet across packages. The random effects seem more dispersed according to MCMCglmm than in INLA or hglm, but this could be due to priors not fit algorithm. Will look into the package prior defaults.

On Mon, Oct 17, 2022 at 4:45 AM Jarrod Hadfield <j.hadfield at ed.ac.uk<mailto:j.hadfield at ed.ac.uk>> wrote:
Hi Shira,

Perhaps a little late to be useful, but MCMCglmm also fits random-effect Bradley-Terry models. Just specify ~mm(opponent1-opponent2) in the random effect formula. The mm stands for multimembership - the BT model is like a multimembership model where some effects have been multiplied by -1, hence the ?-' rather than ?+? in the mm model formula.

Cheers,

Jarrod


> On 16 Oct 2022, at 22:49, Shira Mitchell <shiraqotj at gmail.com<mailto:shiraqotj at gmail.com>> wrote:
>
> This email was sent to you by someone outside the University.
> You should only click on links or attachments if you are certain that the email is genuine and the content is safe.
>
> Update: Dr Heather Turner <https://www.heatherturner.net/> (author of
> BradleyTerry2) suggested the hglm package, which unlike lme4 allows you to
> specify generic design matrices (no longer constrained to lme4 formulas !)
> Results look really similar to INLA so far. Yay !
>
> On Sun, Oct 16, 2022 at 2:19 PM Shira Mitchell <shiraqotj at gmail.com<mailto:shiraqotj at gmail.com>> wrote:
>
>> Super helpful !  Thank you so much !
>>
>> Out of curiosity, is there a way to fit this type of Bradley-Terry model
>> in lme4 ? lme4 formulas include random effect via syntax:
>> https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf
>> "(expr | factor). The expression expr is evaluated as a linear model
>> formula, producing a model matrix following the same rules used in standard
>> R modeling functions (e.g., `lm` or `glm`). The expression factor is
>> evaluated as an `R` factor. One way to think about the vertical bar
>> operator is as a special kind of interaction between the model matrix and
>> the grouping factor. This interaction ensures that the columns of the model
>> matrix have different effects for each level of the grouping factor."
>>
>> So (expr | factor) is X_expr * alpha_factor.
>>
>> So naively writing ~ (1 | m_1) + (1 | m_2) is alpha_{m_1}^{(1)} +
>> alpha_{m_2}^{(2)}, twice as many parameters as what we want which is
>> alpha_{m_1} - alpha_{m_2}.
>>
>> But then see this stackexchange:
>>
>> https://stats.stackexchange.com/questions/483833/opposing-effects-in-lme4-formulae-bradley-terry-model
>> "I could just make a design matrix, where player 1 gets the value 1, and
>> player 2 gets the value ?1. However, unless I'm missing a trick, this would
>> require having a separate column for each player, and plugging each player
>> column's name into the formula"
>>
>> But suppose we create columns for all m = 1,...,M messages:
>>
>> A_m = 1 if m = m_1
>>           -1 if m = m_2
>>            0 otherwise
>>
>> I think then ~ (A_1 + ... + A_M  | m_1) is alpha_{m_1}^{(m_1)} -
>> alpha_{m_1}^{(m_2)}, also not what we would want.
>>
>> Back to INLA. Suppose we now want to add random message-specific slopes
>> for variable X_i in addition to random message-specific intercepts:
>>
>> P[i chooses m_1] = logit^-1 (beta_0 + (alpha_{m_1} - alpha_{m_2}) +
>> (beta_{m_1} - beta_{m_2})X_i)
>>
>> alpha_1,...,alpha_M ~ N(0,sigma_intercept)
>> beta_1,...,beta_M ~ N(0,sigma_slope)
>>
>> I see some resources about this, but nothing super comprehensive. Any
>> advice where to look for complete documentation ?
>>
>> https://groups.google.com/g/r-inla-discussion-group/c/iQELaQF8M9Q/m/q7f4-W8YQksJ
>> (
>> https://becarioprecario.bitbucket.io/inla-gitbook/ch-multilevel.html#multilevel-models-for-longitudinal-data
>> https://rpubs.com/corey_sparks/431920
>> https://avianecologist.com/2016/10/05/multilevel-models/
>>
>> Here is what we did:
>>
>> data$w_X = -data$X
>> data$m_1_beta = data$m_1
>> data$m_2_beta = data$m_2
>>
>> inla(depvar ~  f(m_1, model="iid", values = issues) +
>>                           f(m_2, w, copy = "m_1") +
>>                           f(m_1_beta, X, model="iid", values = issues) +
>>                           f(m_2_beta, w_X, copy = "m_1_beta"),
>>                         family="binomial",
>>                         data=data)
>>
>>
>>
>>
>> On Mon, Oct 10, 2022 at 4:24 AM Thierry Onkelinx <thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>>
>> wrote:
>>
>>> Dear Shira,
>>>
>>> - in a formula object means remove that object from the formula. Use a
>>> weight of -1 instead.
>>>
>>> f(home, model = "iid")) + f(away, w = -1, copy = "home")
>>>
>>> Best regards,
>>>
>>> ir. Thierry Onkelinx
>>> Statisticus / Statistician
>>>
>>> Vlaamse Overheid / Government of Flanders
>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>>> AND FOREST
>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>>> thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>
>>> Havenlaan 88 bus 73, 1000 Brussel
>>> www.inbo.be<http://www.inbo.be>
>>>
>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>> To call in the statistician after the experiment is done may be no more
>>> than asking him to perform a post-mortem examination: he may be able to say
>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>> The plural of anecdote is not data. ~ Roger Brinner
>>> The combination of some data and an aching desire for an answer does not
>>> ensure that a reasonable answer can be extracted from a given body of data.
>>> ~ John Tukey
>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>
>>> <https://www.inbo.be>
>>>
>>>
>>> Op vr 7 okt. 2022 om 23:36 schreef Shira Mitchell <shiraqotj at gmail.com<mailto:shiraqotj at gmail.com>>:
>>>
>>>> Thanks so much, Thierry ! This is great.
>>>>
>>>> This works except that I cannot subtract because:
>>>> f(home, model = "iid")) - f(away, copy = "home")
>>>>
>>>> just drops the second term. Apologies that I'm not super familiar with
>>>> INLA syntax yet.
>>>>
>>>>
>>>>
>>>> On Fri, Oct 7, 2022 at 10:19 AM Thierry Onkelinx <
>>>> thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>> wrote:
>>>>
>>>>> Hi Shira,
>>>>>
>>>>> I fit such models with the INLA package (https://www.r-inla.org/). The
>>>>> trick is to define two random effects but force their parameter estimates
>>>>> to be identical.
>>>>>
>>>>> The code would contain something like f(home, model = "iid")) + f(away,
>>>>> copy = "home"). Meaning home ~ N(0, sigma_beta_i) and home[i] = away[i]
>>>>>
>>>>> Best regards,
>>>>>
>>>>> ir. Thierry Onkelinx
>>>>> Statisticus / Statistician
>>>>>
>>>>> Vlaamse Overheid / Government of Flanders
>>>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>>>>> AND FOREST
>>>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>>>>> thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>
>>>>> Havenlaan 88 bus 73, 1000 Brussel
>>>>> www.inbo.be<http://www.inbo.be>
>>>>>
>>>>>
>>>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>> To call in the statistician after the experiment is done may be no more
>>>>> than asking him to perform a post-mortem examination: he may be able to say
>>>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>>>> The plural of anecdote is not data. ~ Roger Brinner
>>>>> The combination of some data and an aching desire for an answer does
>>>>> not ensure that a reasonable answer can be extracted from a given body of
>>>>> data. ~ John Tukey
>>>>>
>>>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>>
>>>>> <https://www.inbo.be>
>>>>>
>>>>>
>>>>> Op vr 7 okt. 2022 om 15:00 schreef Shira Mitchell <shiraqotj at gmail.com<mailto:shiraqotj at gmail.com>
>>>>>> :
>>>>>
>>>>>> We want to fit Bradley-Terry-style GLMM models in R. We looked into:
>>>>>>
>>>>>>
>>>>>> https://cran.r-project.org/web/packages/BradleyTerry2/vignettes/BradleyTerry.pdf
>>>>>> and
>>>>>> http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html
>>>>>>
>>>>>> We have voter-specific variables x that influence which political
>>>>>> message
>>>>>> (i vs j) wins for them:
>>>>>>
>>>>>> logit[pr(i beats j | person with covariate x)] = lambda_i - lambda_j +
>>>>>> (beta_i - beta_j) x
>>>>>>
>>>>>> We then model parameters as random effects:
>>>>>> lambda_i ~ N(0, sigma_lambda)
>>>>>> beta_i ~ N(0, sigma_beta)
>>>>>>
>>>>>> Is there a way to do this in R ? We do this in TensorFlow in Python by
>>>>>> directly specifying design matrices with the 0,-1,1 or 0,-x,x entries.
>>>>>> However, I do not see how to do this in R using lme4, BradleyTerry2,
>>>>>> mgcv,
>>>>>> etc.
>>>>>>
>>>>>> Thanks so much,
>>>>>> Shira
>>>>>>
>>>>>>        [[alternative HTML version deleted]]
>>>>>>
>>>>>> _______________________________________________
>>>>>> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>
>>>>>
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

The University of Edinburgh is a charitable body, registered in Scotland, with registration number SC005336. Is e buidheann carthannais a th? ann an Oilthigh Dh?n ?ideann, cl?raichte an Alba, ?ireamh cl?raidh SC005336.

	[[alternative HTML version deleted]]


From mo|||eebrook@ @end|ng |rom gm@||@com  Mon Dec 12 11:50:23 2022
From: mo|||eebrook@ @end|ng |rom gm@||@com (Mollie Brooks)
Date: Mon, 12 Dec 2022 11:50:23 +0100
Subject: [R-sig-ME] cross validation of glmmLasso
Message-ID: <EE1BC835-5FD3-4F10-AD24-D426C2C4B7D6@gmail.com>

I?m interested in doing cross validation on GLMMs fit with LASSO. I found two functions for doing this: lmmen::cv.glmmLasso and cv.glmmLasso::cv.glmmLasso. With a small amount of digging, it looks like lmmen has been used more since it was on CRAN in the past and it shows up in a thesis and a working paper on Google scholar.

Both packages only work with a single random effect and I need 2 RE for my data set. I managed to fix that problem in the cv.glmmLasso package (https://github.com/thepira/cv.glmmLasso/pull/18 <https://github.com/thepira/cv.glmmLasso/pull/18>). Making lmmen work with multiple random effects is a little harder.

Another problem with lmmen is that the example from the helpfile isn?t working for me. So I?m not sure I should put time into making it work with multiple random effects.
> tmp=cv.glmmLasso(initialize_example(seed=1))
Error in rep(0, d.size) : invalid 'times' argument
In addition: Warning message:
In cv.glmmLasso(initialize_example(seed = 1)) : NAs introduced by coercion 

I?m wondering if anyone else has already been down this rabbit hole and can offer advice. Is there another package (or random code lying around) for doing cross validation on GLMMs with LASSO that is more thoroughly tested and currently in use?

Cheers,
Mollie



	[[alternative HTML version deleted]]


From mo|||eebrook@ @end|ng |rom gm@||@com  Mon Dec 12 13:09:40 2022
From: mo|||eebrook@ @end|ng |rom gm@||@com (Mollie Brooks)
Date: Mon, 12 Dec 2022 13:09:40 +0100
Subject: [R-sig-ME] cross validation of glmmLasso
In-Reply-To: <EE1BC835-5FD3-4F10-AD24-D426C2C4B7D6@gmail.com>
References: <EE1BC835-5FD3-4F10-AD24-D426C2C4B7D6@gmail.com>
Message-ID: <CFF615E4-9D1E-4E4A-B045-67761F7CB30B@gmail.com>

Following up with a reproducible example?
Both of the models below can be fit in glmmLasso, but only the one with a single RE (lm1) can be cross-validated with lmmen.

library(glmmLasso)
data("soccer")
library(lmmen)

soccer[,c(4,5,9:16)]<-scale(soccer[,c(4,5,9:16)],center=TRUE,scale=TRUE)
soccer<-data.frame(soccer)

lm1 <- glmmLasso(points ~ transfer.spendings + ave.unfair.score 
                 + ball.possession + tackles 
                 + ave.attend + sold.out, rnd = list(team=~1), 
                 lambda=50, data = soccer)

summary(lm1)

lm2 <- glmmLasso(points ~ transfer.spendings + ave.unfair.score 
                 + ball.possession + tackles 
                 + ave.attend + sold.out, rnd = list(team=~1, pos=~1), 
                 lambda=50, data = soccer)

summary(lm2)


cv.lm1 <- cv.glmmLasso(form.fixed= points ~ transfer.spendings + ave.unfair.score 
                    + ball.possession + tackles 
                    + ave.attend + sold.out, 
                    form.rnd = list(team=~1), 
                    lambda=seq(5,250, by=5), dat = soccer)

cv.lm2 <- cv.glmmLasso(form.fixed= points ~ transfer.spendings + ave.unfair.score 
                    + ball.possession + tackles 
                    + ave.attend + sold.out, 
                    form.rnd = list(team=~1, pos=~1), 
                    lambda=seq(5,250, by=5), dat = soccer)

This is the error (below). I was getting a similar error with cv.glmmLasso:: cv.glmmLasso until I made the change described in the pull request from my earlier email. For that package, it was to be due to assuming that q_start was a scalar (as in the case of a single RE).

> cv.lm2=cv.glmmLasso(form.fixed= points ~ transfer.spendings + ave.unfair.score 
+                     + ball.possession + tackles 
+                     + ave.attend + sold.out, 
+                     form.rnd = list(team=~1, pos=~1), 
+                     lambda=seq(5,250, by=5), dat = soccer)
Error in diag(diag(q_start), sum(s)) : 
  'nrow' or 'ncol' cannot be specified when 'x' is a matrix
In addition: There were 50 or more warnings (use warnings() to see the first 50)
> traceback()
8: stop("'nrow' or 'ncol' cannot be specified when 'x' is a matrix")
7: diag(diag(q_start), sum(s))
6: est.glmmLasso.RE(fix = fix, rnd = rnd, data = data, lambda = lambda, 
       family = family, final.re = final.re, switch.NR = switch.NR, 
       control = control)
5: est.glmmLasso(fix, rnd, data = data, lambda = lambda, family = family, 
       switch.NR = switch.NR, final.re = final.re, control = control)
4: glmmLasso::glmmLasso(fix = as.formula(form.fixed), rnd = form.rnd, 
       data = dat, lambda = lambda[opt], switch.NR = FALSE, final.re = TRUE, 
       control = list(start = Delta.start[opt, ], q_start = Q.start.base))
3: withCallingHandlers(expr, warning = function(w) if (inherits(w, 
       classes)) tryInvokeRestart("muffleWarning"))
2: suppressWarnings({
       final <- glmmLasso::glmmLasso(fix = as.formula(form.fixed), 
           rnd = form.rnd, data = dat, lambda = lambda[opt], switch.NR = FALSE, 
           final.re = TRUE, control = list(start = Delta.start[opt, 
               ], q_start = Q.start.base))
       final
   })
1: cv.glmmLasso(form.fixed = points ~ transfer.spendings + ave.unfair.score + 
       ball.possession + tackles + ave.attend + sold.out, form.rnd = list(team = ~1, 
       pos = ~1), lambda = seq(5, 250, by = 5), dat = soccer)


Cheers,
Mollie

> On 12 Dec 2022, at 11.50, Mollie Brooks <mollieebrooks at gmail.com> wrote:
> 
> I?m interested in doing cross validation on GLMMs fit with LASSO. I found two functions for doing this: lmmen::cv.glmmLasso and cv.glmmLasso::cv.glmmLasso. With a small amount of digging, it looks like lmmen has been used more since it was on CRAN in the past and it shows up in a thesis and a working paper on Google scholar.
> 
> Both packages only work with a single random effect and I need 2 RE for my data set. I managed to fix that problem in the cv.glmmLasso package (https://github.com/thepira/cv.glmmLasso/pull/18 <https://github.com/thepira/cv.glmmLasso/pull/18>). Making lmmen work with multiple random effects is a little harder.
> 
> Another problem with lmmen is that the example from the helpfile isn?t working for me. So I?m not sure I should put time into making it work with multiple random effects.
> > tmp=cv.glmmLasso(initialize_example(seed=1))
> Error in rep(0, d.size) : invalid 'times' argument
> In addition: Warning message:
> In cv.glmmLasso(initialize_example(seed = 1)) : NAs introduced by coercion 
> 
> I?m wondering if anyone else has already been down this rabbit hole and can offer advice. Is there another package (or random code lying around) for doing cross validation on GLMMs with LASSO that is more thoroughly tested and currently in use?
> 
> Cheers,
> Mollie
> 
> 


	[[alternative HTML version deleted]]


From mo|||eebrook@ @end|ng |rom gm@||@com  Mon Dec 12 13:31:14 2022
From: mo|||eebrook@ @end|ng |rom gm@||@com (Mollie Brooks)
Date: Mon, 12 Dec 2022 13:31:14 +0100
Subject: [R-sig-ME] cross validation of glmmLasso
In-Reply-To: <CFF615E4-9D1E-4E4A-B045-67761F7CB30B@gmail.com>
References: <EE1BC835-5FD3-4F10-AD24-D426C2C4B7D6@gmail.com>
 <CFF615E4-9D1E-4E4A-B045-67761F7CB30B@gmail.com>
Message-ID: <D0460C0D-5D56-4F97-A797-9AE2B907D98E@gmail.com>

I found that Ben Bolker is already working on this problem https://github.com/yonicd/lmmen/compare/master...bbolker:lmmen:master <https://github.com/yonicd/lmmen/compare/master...bbolker:lmmen:master>

I?ll follow up there. 

The lesson I?m learning with glmmLasso is to always check forks on GitHub to see if someone already started working on the problem. Thanks Ben!

Cheers,
Mollie


> On 12 Dec 2022, at 13.09, Mollie Brooks <mollieebrooks at gmail.com> wrote:
> 
> Following up with a reproducible example?
> Both of the models below can be fit in glmmLasso, but only the one with a single RE (lm1) can be cross-validated with lmmen.
> 
> library(glmmLasso)
> data("soccer")
> library(lmmen)
> 
> soccer[,c(4,5,9:16)]<-scale(soccer[,c(4,5,9:16)],center=TRUE,scale=TRUE)
> soccer<-data.frame(soccer)
> 
> lm1 <- glmmLasso(points ~ transfer.spendings + ave.unfair.score 
>                  + ball.possession + tackles 
>                  + ave.attend + sold.out, rnd = list(team=~1), 
>                  lambda=50, data = soccer)
> 
> summary(lm1)
> 
> lm2 <- glmmLasso(points ~ transfer.spendings + ave.unfair.score 
>                  + ball.possession + tackles 
>                  + ave.attend + sold.out, rnd = list(team=~1, pos=~1), 
>                  lambda=50, data = soccer)
> 
> summary(lm2)
> 
> 
> cv.lm1 <- cv.glmmLasso(form.fixed= points ~ transfer.spendings + ave.unfair.score 
>                     + ball.possession + tackles 
>                     + ave.attend + sold.out, 
>                     form.rnd = list(team=~1), 
>                     lambda=seq(5,250, by=5), dat = soccer)
> 
> cv.lm2 <- cv.glmmLasso(form.fixed= points ~ transfer.spendings + ave.unfair.score 
>                     + ball.possession + tackles 
>                     + ave.attend + sold.out, 
>                     form.rnd = list(team=~1, pos=~1), 
>                     lambda=seq(5,250, by=5), dat = soccer)
> 
> This is the error (below). I was getting a similar error with cv.glmmLasso:: cv.glmmLasso until I made the change described in the pull request from my earlier email. For that package, it was to be due to assuming that q_start was a scalar (as in the case of a single RE).
> 
> > cv.lm2=cv.glmmLasso(form.fixed= points ~ transfer.spendings + ave.unfair.score 
> +                     + ball.possession + tackles 
> +                     + ave.attend + sold.out, 
> +                     form.rnd = list(team=~1, pos=~1), 
> +                     lambda=seq(5,250, by=5), dat = soccer)
> Error in diag(diag(q_start), sum(s)) : 
>   'nrow' or 'ncol' cannot be specified when 'x' is a matrix
> In addition: There were 50 or more warnings (use warnings() to see the first 50)
> > traceback()
> 8: stop("'nrow' or 'ncol' cannot be specified when 'x' is a matrix")
> 7: diag(diag(q_start), sum(s))
> 6: est.glmmLasso.RE(fix = fix, rnd = rnd, data = data, lambda = lambda, 
>        family = family, final.re = final.re, switch.NR = switch.NR, 
>        control = control)
> 5: est.glmmLasso(fix, rnd, data = data, lambda = lambda, family = family, 
>        switch.NR = switch.NR, final.re = final.re, control = control)
> 4: glmmLasso::glmmLasso(fix = as.formula(form.fixed), rnd = form.rnd, 
>        data = dat, lambda = lambda[opt], switch.NR = FALSE, final.re = TRUE, 
>        control = list(start = Delta.start[opt, ], q_start = Q.start.base))
> 3: withCallingHandlers(expr, warning = function(w) if (inherits(w, 
>        classes)) tryInvokeRestart("muffleWarning"))
> 2: suppressWarnings({
>        final <- glmmLasso::glmmLasso(fix = as.formula(form.fixed), 
>            rnd = form.rnd, data = dat, lambda = lambda[opt], switch.NR = FALSE, 
>            final.re = TRUE, control = list(start = Delta.start[opt, 
>                ], q_start = Q.start.base))
>        final
>    })
> 1: cv.glmmLasso(form.fixed = points ~ transfer.spendings + ave.unfair.score + 
>        ball.possession + tackles + ave.attend + sold.out, form.rnd = list(team = ~1, 
>        pos = ~1), lambda = seq(5, 250, by = 5), dat = soccer)
> 
> 
> Cheers,
> Mollie
> 
>> On 12 Dec 2022, at 11.50, Mollie Brooks <mollieebrooks at gmail.com <mailto:mollieebrooks at gmail.com>> wrote:
>> 
>> I?m interested in doing cross validation on GLMMs fit with LASSO. I found two functions for doing this: lmmen::cv.glmmLasso and cv.glmmLasso::cv.glmmLasso. With a small amount of digging, it looks like lmmen has been used more since it was on CRAN in the past and it shows up in a thesis and a working paper on Google scholar.
>> 
>> Both packages only work with a single random effect and I need 2 RE for my data set. I managed to fix that problem in the cv.glmmLasso package (https://github.com/thepira/cv.glmmLasso/pull/18 <https://github.com/thepira/cv.glmmLasso/pull/18>). Making lmmen work with multiple random effects is a little harder.
>> 
>> Another problem with lmmen is that the example from the helpfile isn?t working for me. So I?m not sure I should put time into making it work with multiple random effects.
>> > tmp=cv.glmmLasso(initialize_example(seed=1))
>> Error in rep(0, d.size) : invalid 'times' argument
>> In addition: Warning message:
>> In cv.glmmLasso(initialize_example(seed = 1)) : NAs introduced by coercion 
>> 
>> I?m wondering if anyone else has already been down this rabbit hole and can offer advice. Is there another package (or random code lying around) for doing cross validation on GLMMs with LASSO that is more thoroughly tested and currently in use?
>> 
>> Cheers,
>> Mollie
>> 
>> 
> 


	[[alternative HTML version deleted]]


From mo|||eebrook@ @end|ng |rom gm@||@com  Tue Dec 13 14:33:10 2022
From: mo|||eebrook@ @end|ng |rom gm@||@com (Mollie Brooks)
Date: Tue, 13 Dec 2022 14:33:10 +0100
Subject: [R-sig-ME] cross validation of glmmLasso
In-Reply-To: <D0460C0D-5D56-4F97-A797-9AE2B907D98E@gmail.com>
References: <EE1BC835-5FD3-4F10-AD24-D426C2C4B7D6@gmail.com>
 <CFF615E4-9D1E-4E4A-B045-67761F7CB30B@gmail.com>
 <D0460C0D-5D56-4F97-A797-9AE2B907D98E@gmail.com>
Message-ID: <4DDB2CC3-42A8-4844-97B2-02B3C6428659@gmail.com>

To follow up with what I found (and then I promise to stop spamming you all)?

There are now versions of both lmmen::cv.glmmLasso and cv.glmmLasso::cv.glmmLasso that work with multiple random effects, but they aren?t merged into the original GitHub repositories yet. 

It seems that lmmen::cv.glmmLasso finds the best penalty parameter lambda using BIC, whereas cv.glmmLasso::cv.glmmLasso will do K-fold cross validation to find lambda. Both are described in Appendix A of Groll and Tutz 2014.

cheers,
Mollie
 

> On 12 Dec 2022, at 13.31, Mollie Brooks <mollieebrooks at gmail.com> wrote:
> 
> I found that Ben Bolker is already working on this problem https://github.com/yonicd/lmmen/compare/master...bbolker:lmmen:master <https://github.com/yonicd/lmmen/compare/master...bbolker:lmmen:master>
> 
> I?ll follow up there. 
> 
> The lesson I?m learning with glmmLasso is to always check forks on GitHub to see if someone already started working on the problem. Thanks Ben!
> 
> Cheers,
> Mollie
> 
> 
>> On 12 Dec 2022, at 13.09, Mollie Brooks <mollieebrooks at gmail.com <mailto:mollieebrooks at gmail.com>> wrote:
>> 
>> Following up with a reproducible example?
>> Both of the models below can be fit in glmmLasso, but only the one with a single RE (lm1) can be cross-validated with lmmen.
>> 
>> library(glmmLasso)
>> data("soccer")
>> library(lmmen)
>> 
>> soccer[,c(4,5,9:16)]<-scale(soccer[,c(4,5,9:16)],center=TRUE,scale=TRUE)
>> soccer<-data.frame(soccer)
>> 
>> lm1 <- glmmLasso(points ~ transfer.spendings + ave.unfair.score 
>>                  + ball.possession + tackles 
>>                  + ave.attend + sold.out, rnd = list(team=~1), 
>>                  lambda=50, data = soccer)
>> 
>> summary(lm1)
>> 
>> lm2 <- glmmLasso(points ~ transfer.spendings + ave.unfair.score 
>>                  + ball.possession + tackles 
>>                  + ave.attend + sold.out, rnd = list(team=~1, pos=~1), 
>>                  lambda=50, data = soccer)
>> 
>> summary(lm2)
>> 
>> 
>> cv.lm1 <- cv.glmmLasso(form.fixed= points ~ transfer.spendings + ave.unfair.score 
>>                     + ball.possession + tackles 
>>                     + ave.attend + sold.out, 
>>                     form.rnd = list(team=~1), 
>>                     lambda=seq(5,250, by=5), dat = soccer)
>> 
>> cv.lm2 <- cv.glmmLasso(form.fixed= points ~ transfer.spendings + ave.unfair.score 
>>                     + ball.possession + tackles 
>>                     + ave.attend + sold.out, 
>>                     form.rnd = list(team=~1, pos=~1), 
>>                     lambda=seq(5,250, by=5), dat = soccer)
>> 
>> This is the error (below). I was getting a similar error with cv.glmmLasso:: cv.glmmLasso until I made the change described in the pull request from my earlier email. For that package, it was to be due to assuming that q_start was a scalar (as in the case of a single RE).
>> 
>> > cv.lm2=cv.glmmLasso(form.fixed= points ~ transfer.spendings + ave.unfair.score 
>> +                     + ball.possession + tackles 
>> +                     + ave.attend + sold.out, 
>> +                     form.rnd = list(team=~1, pos=~1), 
>> +                     lambda=seq(5,250, by=5), dat = soccer)
>> Error in diag(diag(q_start), sum(s)) : 
>>   'nrow' or 'ncol' cannot be specified when 'x' is a matrix
>> In addition: There were 50 or more warnings (use warnings() to see the first 50)
>> > traceback()
>> 8: stop("'nrow' or 'ncol' cannot be specified when 'x' is a matrix")
>> 7: diag(diag(q_start), sum(s))
>> 6: est.glmmLasso.RE(fix = fix, rnd = rnd, data = data, lambda = lambda, 
>>        family = family, final.re = final.re, switch.NR = switch.NR, 
>>        control = control)
>> 5: est.glmmLasso(fix, rnd, data = data, lambda = lambda, family = family, 
>>        switch.NR = switch.NR, final.re = final.re, control = control)
>> 4: glmmLasso::glmmLasso(fix = as.formula(form.fixed), rnd = form.rnd, 
>>        data = dat, lambda = lambda[opt], switch.NR = FALSE, final.re = TRUE, 
>>        control = list(start = Delta.start[opt, ], q_start = Q.start.base))
>> 3: withCallingHandlers(expr, warning = function(w) if (inherits(w, 
>>        classes)) tryInvokeRestart("muffleWarning"))
>> 2: suppressWarnings({
>>        final <- glmmLasso::glmmLasso(fix = as.formula(form.fixed), 
>>            rnd = form.rnd, data = dat, lambda = lambda[opt], switch.NR = FALSE, 
>>            final.re = TRUE, control = list(start = Delta.start[opt, 
>>                ], q_start = Q.start.base))
>>        final
>>    })
>> 1: cv.glmmLasso(form.fixed = points ~ transfer.spendings + ave.unfair.score + 
>>        ball.possession + tackles + ave.attend + sold.out, form.rnd = list(team = ~1, 
>>        pos = ~1), lambda = seq(5, 250, by = 5), dat = soccer)
>> 
>> 
>> Cheers,
>> Mollie
>> 
>>> On 12 Dec 2022, at 11.50, Mollie Brooks <mollieebrooks at gmail.com <mailto:mollieebrooks at gmail.com>> wrote:
>>> 
>>> I?m interested in doing cross validation on GLMMs fit with LASSO. I found two functions for doing this: lmmen::cv.glmmLasso and cv.glmmLasso::cv.glmmLasso. With a small amount of digging, it looks like lmmen has been used more since it was on CRAN in the past and it shows up in a thesis and a working paper on Google scholar.
>>> 
>>> Both packages only work with a single random effect and I need 2 RE for my data set. I managed to fix that problem in the cv.glmmLasso package (https://github.com/thepira/cv.glmmLasso/pull/18 <https://github.com/thepira/cv.glmmLasso/pull/18>). Making lmmen work with multiple random effects is a little harder.
>>> 
>>> Another problem with lmmen is that the example from the helpfile isn?t working for me. So I?m not sure I should put time into making it work with multiple random effects.
>>> > tmp=cv.glmmLasso(initialize_example(seed=1))
>>> Error in rep(0, d.size) : invalid 'times' argument
>>> In addition: Warning message:
>>> In cv.glmmLasso(initialize_example(seed = 1)) : NAs introduced by coercion 
>>> 
>>> I?m wondering if anyone else has already been down this rabbit hole and can offer advice. Is there another package (or random code lying around) for doing cross validation on GLMMs with LASSO that is more thoroughly tested and currently in use?
>>> 
>>> Cheers,
>>> Mollie
>>> 
>>> 
>> 
> 


	[[alternative HTML version deleted]]


From dmb@te@ @end|ng |rom gm@||@com  Tue Dec 13 18:39:58 2022
From: dmb@te@ @end|ng |rom gm@||@com (Douglas Bates)
Date: Tue, 13 Dec 2022 11:39:58 -0600
Subject: [R-sig-ME] Suggestions on how to correct a misapprehension?
Message-ID: <CAO7JsnSyaJ8F6yU_FP39EDog41Na081cbDKczMjBUvyTznNVnA@mail.gmail.com>

So my family is having to live through a "Someone is wrong on the
internet", https://xkcd.com/386/, moment. In the past couple of days I have
twice encountered the same mistaken characterization of how the parameter
estimates in lmer and in the MixedModels.jl package are evaluated.

As we documented in our 2015 paper http://dx.doi.org/10.18637/jss.v067.i01
in lme4 the REML estimates or the ML estimates for the parameters of a
linear mixed-effects model are evaluated by constrained optimization of a
profiled log-likelihood or profiled log-restricted-likelihood.  The
parameters directly being optimized are the elements of relative covariance
factors.  The profiling involves solving a penalized least squares
problem.  This PLS representation, and the use of sparse matrices, is what
allows for fitting models with random effects associated with crossed or
partially crossed grouping factors, such as "subject" and "item".  To many
users this capability is one of the big selling points for lme4.

In our paper we explain in great detail why this approach is, in our
opinion, superior to earlier approaches.  And if someone doesn't believe
us, both lme4 and MixedModels.jl are Open Source projects so anyone who
wants to do so can just go read the code to find out what actually is done.

So it came as a surprise when reading the Wikipedia entry on mixed models,
https://en.wikipedia.org/wiki/Mixed_model, to learn that lme4 and
MixedModels.jl use an EM algorithm that Mary Lindstrom and I described (
https://doi.org/10.1080%2F01621459.1988.10478693) in 1988.  It is possible
that in the early days of lme4 there was such an implementation, but not in
the last 15 years, and there definitely has never been such an
implementation in MixedModels.jl.  I noticed that the Python package
statsmodels is described in the Wikipedia article and in their
documentation, https://www.statsmodels.org/stable/mixed_linear.html, as
using that EM algorithm. I didn't verify this in the code because reading
code based on numpy and scipy causes me to start ranting and raving to the
extent that family members need to take away my laptop and put me in a
quiet room with the window shades drawn until I promise to behave myself.

Anyway the Python statsmodels documentation claims that lme4 uses this
method, which it doesn't.

I have never gone through the process of proposing an edit in a Wikipedia
article.  As I understand it I would need to create a login etc.  Would
anyone who does have such a login be willing to propose an edit and save me
the steps?

	[[alternative HTML version deleted]]


From he|n@v@n@||ever|oo @end|ng |rom v|@etern@@n|  Tue Dec 13 18:49:30 2022
From: he|n@v@n@||ever|oo @end|ng |rom v|@etern@@n| (Hein van Lieverloo)
Date: Tue, 13 Dec 2022 18:49:30 +0100
Subject: [R-sig-ME] Suggestions on how to correct a misapprehension?
In-Reply-To: <CAO7JsnSyaJ8F6yU_FP39EDog41Na081cbDKczMjBUvyTznNVnA@mail.gmail.com>
References: <CAO7JsnSyaJ8F6yU_FP39EDog41Na081cbDKczMjBUvyTznNVnA@mail.gmail.com>
Message-ID: <003e01d90f1b$40d90b80$c28b2280$@viaeterna.nl>

Hi Douglas,

First: ?on the form of your 
Second: thanks for the details and support for your quest! 
Third: anyone can get a login on Wikipedia, I have made some changes myself (not on the page you refer to ?). This also explains why it is 'less than perfect' and should never be used as a single source.

Best,

Hein

-----Original Message-----
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of Douglas Bates
Sent: Tuesday, 13 December 2022 18:40
To: R Mixed Models <r-sig-mixed-models at r-project.org>
Subject: [R-sig-ME] Suggestions on how to correct a misapprehension?

So my family is having to live through a "Someone is wrong on the internet", https://xkcd.com/386/, moment. In the past couple of days I have twice encountered the same mistaken characterization of how the parameter estimates in lmer and in the MixedModels.jl package are evaluated.

As we documented in our 2015 paper http://dx.doi.org/10.18637/jss.v067.i01
in lme4 the REML estimates or the ML estimates for the parameters of a linear mixed-effects model are evaluated by constrained optimization of a profiled log-likelihood or profiled log-restricted-likelihood.  The parameters directly being optimized are the elements of relative covariance factors.  The profiling involves solving a penalized least squares problem.  This PLS representation, and the use of sparse matrices, is what allows for fitting models with random effects associated with crossed or partially crossed grouping factors, such as "subject" and "item".  To many users this capability is one of the big selling points for lme4.

In our paper we explain in great detail why this approach is, in our opinion, superior to earlier approaches.  And if someone doesn't believe us, both lme4 and MixedModels.jl are Open Source projects so anyone who wants to do so can just go read the code to find out what actually is done.

So it came as a surprise when reading the Wikipedia entry on mixed models, https://en.wikipedia.org/wiki/Mixed_model, to learn that lme4 and MixedModels.jl use an EM algorithm that Mary Lindstrom and I described (
https://doi.org/10.1080%2F01621459.1988.10478693) in 1988.  It is possible that in the early days of lme4 there was such an implementation, but not in the last 15 years, and there definitely has never been such an implementation in MixedModels.jl.  I noticed that the Python package statsmodels is described in the Wikipedia article and in their documentation, https://www.statsmodels.org/stable/mixed_linear.html, as using that EM algorithm. I didn't verify this in the code because reading code based on numpy and scipy causes me to start ranting and raving to the extent that family members need to take away my laptop and put me in a quiet room with the window shades drawn until I promise to behave myself.

Anyway the Python statsmodels documentation claims that lme4 uses this method, which it doesn't.

I have never gone through the process of proposing an edit in a Wikipedia article.  As I understand it I would need to create a login etc.  Would anyone who does have such a login be willing to propose an edit and save me the steps?

	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From me @end|ng |rom ph||||p@|d@y@com  Tue Dec 13 18:55:27 2022
From: me @end|ng |rom ph||||p@|d@y@com (Phillip Alday)
Date: Tue, 13 Dec 2022 11:55:27 -0600
Subject: [R-sig-ME] Suggestions on how to correct a misapprehension?
In-Reply-To: <CAO7JsnSyaJ8F6yU_FP39EDog41Na081cbDKczMjBUvyTznNVnA@mail.gmail.com>
References: <CAO7JsnSyaJ8F6yU_FP39EDog41Na081cbDKczMjBUvyTznNVnA@mail.gmail.com>
Message-ID: <14ad0400-5422-595d-94ba-1255c95f1530@phillipalday.com>

I believe I have a login somewhere for Wikipedia and would be happy to 
make an edit when I get a few minutes. This has bothered me for a while 
too, but I get busy and forget to do something about it ...

Skimming the Python code, I'm not sure that it's using EM, but there's a 
lot of indirection and it's a codebase I'm not familiar with. I'm also 
not convinced that SAS is using EM (and I recently read the PROC MIXED 
documentation).

Currently Wikipedia reads so:

> One method used to fit such mixed models is that of the 
> expectation?maximization algorithm 
> <https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm> 
> where the variance components are treated as unobserved nuisance 
> parameters <https://en.wikipedia.org/wiki/Nuisance_parameter> in the 
> joint likelihood.^[11] 
> <https://en.wikipedia.org/wiki/Mixed_model#cite_note-11> Currently, 
> this is the method implemented in major statistical software such as R 
> <https://en.wikipedia.org/wiki/R_(programming_language)> (lme4 
> package), Python 
> <https://en.wikipedia.org/wiki/Python_(programming_language)> 
> (statsmodels <https://en.wikipedia.org/wiki/Statsmodels> package), 
> Julia <https://en.wikipedia.org/wiki/Julia_(programming_language)> 
> (MixedModels.jl package), and SAS 
> <https://en.wikipedia.org/wiki/SAS_(software)> (proc mixed). The 
> solution to the mixed model equations is a maximum likelihood estimate 
> <https://en.wikipedia.org/wiki/Maximum_likelihood_estimate> when the 
> distribution of the errors is normal.

My proposed edit would be something along the lines of:

> There are several methods to fit mixed models, including 
> expectation-maximization, generalized least squares (used by R's 
> nlme), penalized least squares (used by R's lme4 and MixedModels.jl) 
> and direct optimization of the likelihood (used by e.g. R's glmmTMB). 
> Notably, while the canonical form proposed by Henderson is useful for 
> theory, many popular software packages use a different formulation for 
> numerical computation in order to take advantage of sparse matrix 
> methods (e.g. lme4 and MixedModels.jl).

Less technical "listeners" on this list should feel free to speak up now 
if this is unclear because Wikipedia should be for a fairly broad audience!

Phillip

On 12/13/22 11:39, Douglas Bates wrote:
> So my family is having to live through a "Someone is wrong on the
> internet",https://xkcd.com/386/, moment. In the past couple of days I have
> twice encountered the same mistaken characterization of how the parameter
> estimates in lmer and in the MixedModels.jl package are evaluated.
>
> As we documented in our 2015 paperhttp://dx.doi.org/10.18637/jss.v067.i01
> in lme4 the REML estimates or the ML estimates for the parameters of a
> linear mixed-effects model are evaluated by constrained optimization of a
> profiled log-likelihood or profiled log-restricted-likelihood.  The
> parameters directly being optimized are the elements of relative covariance
> factors.  The profiling involves solving a penalized least squares
> problem.  This PLS representation, and the use of sparse matrices, is what
> allows for fitting models with random effects associated with crossed or
> partially crossed grouping factors, such as "subject" and "item".  To many
> users this capability is one of the big selling points for lme4.
>
> In our paper we explain in great detail why this approach is, in our
> opinion, superior to earlier approaches.  And if someone doesn't believe
> us, both lme4 and MixedModels.jl are Open Source projects so anyone who
> wants to do so can just go read the code to find out what actually is done.
>
> So it came as a surprise when reading the Wikipedia entry on mixed models,
> https://en.wikipedia.org/wiki/Mixed_model, to learn that lme4 and
> MixedModels.jl use an EM algorithm that Mary Lindstrom and I described (
> https://doi.org/10.1080%2F01621459.1988.10478693) in 1988.  It is possible
> that in the early days of lme4 there was such an implementation, but not in
> the last 15 years, and there definitely has never been such an
> implementation in MixedModels.jl.  I noticed that the Python package
> statsmodels is described in the Wikipedia article and in their
> documentation,https://www.statsmodels.org/stable/mixed_linear.html, as
> using that EM algorithm. I didn't verify this in the code because reading
> code based on numpy and scipy causes me to start ranting and raving to the
> extent that family members need to take away my laptop and put me in a
> quiet room with the window shades drawn until I promise to behave myself.
>
> Anyway the Python statsmodels documentation claims that lme4 uses this
> method, which it doesn't.
>
> I have never gone through the process of proposing an edit in a Wikipedia
> article.  As I understand it I would need to create a login etc.  Would
> anyone who does have such a login be willing to propose an edit and save me
> the steps?
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org  mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue Dec 13 19:20:43 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 13 Dec 2022 10:20:43 -0800
Subject: [R-sig-ME] Suggestions on how to correct a misapprehension?
In-Reply-To: <14ad0400-5422-595d-94ba-1255c95f1530@phillipalday.com>
References: <CAO7JsnSyaJ8F6yU_FP39EDog41Na081cbDKczMjBUvyTznNVnA@mail.gmail.com>
 <14ad0400-5422-595d-94ba-1255c95f1530@phillipalday.com>
Message-ID: <79BB914A-D247-4F3C-8B1B-C2AC1C54F99E@dcn.davis.ca.us>

Definitely a layperson in this field, but my reading of your proposed edit (matrix representation) doesn't feel equivalent to the objections raised by Doug, which appear to me to be deeper into what is being optimized than just the numerical efficiency of the calculations.

On December 13, 2022 9:55:27 AM PST, Phillip Alday <me at phillipalday.com> wrote:
>I believe I have a login somewhere for Wikipedia and would be happy to 
>make an edit when I get a few minutes. This has bothered me for a while 
>too, but I get busy and forget to do something about it ...
>
>Skimming the Python code, I'm not sure that it's using EM, but there's a 
>lot of indirection and it's a codebase I'm not familiar with. I'm also 
>not convinced that SAS is using EM (and I recently read the PROC MIXED 
>documentation).
>
>Currently Wikipedia reads so:
>
>> One method used to fit such mixed models is that of the 
>> expectation?maximization algorithm 
>> <https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm> 
>> where the variance components are treated as unobserved nuisance 
>> parameters <https://en.wikipedia.org/wiki/Nuisance_parameter> in the 
>> joint likelihood.^[11] 
>> <https://en.wikipedia.org/wiki/Mixed_model#cite_note-11> Currently, 
>> this is the method implemented in major statistical software such as R 
>> <https://en.wikipedia.org/wiki/R_(programming_language)> (lme4 
>> package), Python 
>> <https://en.wikipedia.org/wiki/Python_(programming_language)> 
>> (statsmodels <https://en.wikipedia.org/wiki/Statsmodels> package), 
>> Julia <https://en.wikipedia.org/wiki/Julia_(programming_language)> 
>> (MixedModels.jl package), and SAS 
>> <https://en.wikipedia.org/wiki/SAS_(software)> (proc mixed). The 
>> solution to the mixed model equations is a maximum likelihood estimate 
>> <https://en.wikipedia.org/wiki/Maximum_likelihood_estimate> when the 
>> distribution of the errors is normal.
>
>My proposed edit would be something along the lines of:
>
>> There are several methods to fit mixed models, including 
>> expectation-maximization, generalized least squares (used by R's 
>> nlme), penalized least squares (used by R's lme4 and MixedModels.jl) 
>> and direct optimization of the likelihood (used by e.g. R's glmmTMB). 
>> Notably, while the canonical form proposed by Henderson is useful for 
>> theory, many popular software packages use a different formulation for 
>> numerical computation in order to take advantage of sparse matrix 
>> methods (e.g. lme4 and MixedModels.jl).
>
>Less technical "listeners" on this list should feel free to speak up now 
>if this is unclear because Wikipedia should be for a fairly broad audience!
>
>Phillip
>
>On 12/13/22 11:39, Douglas Bates wrote:
>> So my family is having to live through a "Someone is wrong on the
>> internet",https://xkcd.com/386/, moment. In the past couple of days I have
>> twice encountered the same mistaken characterization of how the parameter
>> estimates in lmer and in the MixedModels.jl package are evaluated.
>>
>> As we documented in our 2015 paperhttp://dx.doi.org/10.18637/jss.v067.i01
>> in lme4 the REML estimates or the ML estimates for the parameters of a
>> linear mixed-effects model are evaluated by constrained optimization of a
>> profiled log-likelihood or profiled log-restricted-likelihood.  The
>> parameters directly being optimized are the elements of relative covariance
>> factors.  The profiling involves solving a penalized least squares
>> problem.  This PLS representation, and the use of sparse matrices, is what
>> allows for fitting models with random effects associated with crossed or
>> partially crossed grouping factors, such as "subject" and "item".  To many
>> users this capability is one of the big selling points for lme4.
>>
>> In our paper we explain in great detail why this approach is, in our
>> opinion, superior to earlier approaches.  And if someone doesn't believe
>> us, both lme4 and MixedModels.jl are Open Source projects so anyone who
>> wants to do so can just go read the code to find out what actually is done.
>>
>> So it came as a surprise when reading the Wikipedia entry on mixed models,
>> https://en.wikipedia.org/wiki/Mixed_model, to learn that lme4 and
>> MixedModels.jl use an EM algorithm that Mary Lindstrom and I described (
>> https://doi.org/10.1080%2F01621459.1988.10478693) in 1988.  It is possible
>> that in the early days of lme4 there was such an implementation, but not in
>> the last 15 years, and there definitely has never been such an
>> implementation in MixedModels.jl.  I noticed that the Python package
>> statsmodels is described in the Wikipedia article and in their
>> documentation,https://www.statsmodels.org/stable/mixed_linear.html, as
>> using that EM algorithm. I didn't verify this in the code because reading
>> code based on numpy and scipy causes me to start ranting and raving to the
>> extent that family members need to take away my laptop and put me in a
>> quiet room with the window shades drawn until I promise to behave myself.
>>
>> Anyway the Python statsmodels documentation claims that lme4 uses this
>> method, which it doesn't.
>>
>> I have never gone through the process of proposing an edit in a Wikipedia
>> article.  As I understand it I would need to create a login etc.  Would
>> anyone who does have such a login be willing to propose an edit and save me
>> the steps?
>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org  mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>	[[alternative HTML version deleted]]
>
>_______________________________________________
>R-sig-mixed-models at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Sent from my phone. Please excuse my brevity.


From jhm@|ndon@|d @end|ng |rom gm@||@com  Tue Dec 13 21:04:03 2022
From: jhm@|ndon@|d @end|ng |rom gm@||@com (John H Maindonald)
Date: Wed, 14 Dec 2022 09:04:03 +1300
Subject: [R-sig-ME] Suggestions on how to correct a misapprehension?
In-Reply-To: <003e01d90f1b$40d90b80$c28b2280$@viaeterna.nl>
References: <CAO7JsnSyaJ8F6yU_FP39EDog41Na081cbDKczMjBUvyTznNVnA@mail.gmail.com>
 <003e01d90f1b$40d90b80$c28b2280$@viaeterna.nl>
Message-ID: <DCA21393-202E-41C7-98E2-195C37E0E5B7@gmail.com>

There is of course no perfect source.  The beauty of Wikipedia is that what appears there
is open, unlike most formally published content, to correction of the source document. 
There is a large amount of published content, some of it implemented in R packages,
that is never exposed to critique in any formal published form.

I have been surprised at the quality of Wikipedia, by and large, as a first place to go for 
information on technical issues.

Douglas,  can you comment further on what it is about code based on code based on 
numpy and scipy that sets you going?

John Maindonld.

> On 14/12/2022, at 06:49, Hein van Lieverloo <hein.van.lieverloo at viaeterna.nl> wrote:
> 
> Hi Douglas,
> 
> First: ?on the form of your 
> Second: thanks for the details and support for your quest! 
> Third: anyone can get a login on Wikipedia, I have made some changes myself (not on the page you refer to ?). This also explains why it is 'less than perfect' and should never be used as a single source.
> 
> Best,
> 
> Hein
> 
> -----Original Message-----
> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of Douglas Bates
> Sent: Tuesday, 13 December 2022 18:40
> To: R Mixed Models <r-sig-mixed-models at r-project.org>
> Subject: [R-sig-ME] Suggestions on how to correct a misapprehension?
> 
> So my family is having to live through a "Someone is wrong on the internet", https://xkcd.com/386/, moment. In the past couple of days I have twice encountered the same mistaken characterization of how the parameter estimates in lmer and in the MixedModels.jl package are evaluated.
> 
> As we documented in our 2015 paper http://dx.doi.org/10.18637/jss.v067.i01
> in lme4 the REML estimates or the ML estimates for the parameters of a linear mixed-effects model are evaluated by constrained optimization of a profiled log-likelihood or profiled log-restricted-likelihood.  The parameters directly being optimized are the elements of relative covariance factors.  The profiling involves solving a penalized least squares problem.  This PLS representation, and the use of sparse matrices, is what allows for fitting models with random effects associated with crossed or partially crossed grouping factors, such as "subject" and "item".  To many users this capability is one of the big selling points for lme4.
> 
> In our paper we explain in great detail why this approach is, in our opinion, superior to earlier approaches.  And if someone doesn't believe us, both lme4 and MixedModels.jl are Open Source projects so anyone who wants to do so can just go read the code to find out what actually is done.
> 
> So it came as a surprise when reading the Wikipedia entry on mixed models, https://en.wikipedia.org/wiki/Mixed_model, to learn that lme4 and MixedModels.jl use an EM algorithm that Mary Lindstrom and I described (
> https://doi.org/10.1080%2F01621459.1988.10478693) in 1988.  It is possible that in the early days of lme4 there was such an implementation, but not in the last 15 years, and there definitely has never been such an implementation in MixedModels.jl.  I noticed that the Python package statsmodels is described in the Wikipedia article and in their documentation, https://www.statsmodels.org/stable/mixed_linear.html, as using that EM algorithm. I didn't verify this in the code because reading code based on numpy and scipy causes me to start ranting and raving to the extent that family members need to take away my laptop and put me in a quiet room with the window shades drawn until I promise to behave myself.
> 
> Anyway the Python statsmodels documentation claims that lme4 uses this method, which it doesn't.
> 
> I have never gone through the process of proposing an edit in a Wikipedia article.  As I understand it I would need to create a login etc.  Would anyone who does have such a login be willing to propose an edit and save me the steps?
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From @pro @end|ng |rom un|me|b@edu@@u  Tue Dec 13 22:37:42 2022
From: @pro @end|ng |rom un|me|b@edu@@u (Andrew Robinson)
Date: Tue, 13 Dec 2022 21:37:42 +0000
Subject: [R-sig-ME] Suggestions on how to correct a misapprehension?
In-Reply-To: <DCA21393-202E-41C7-98E2-195C37E0E5B7@gmail.com>
References: <CAO7JsnSyaJ8F6yU_FP39EDog41Na081cbDKczMjBUvyTznNVnA@mail.gmail.com>
 <003e01d90f1b$40d90b80$c28b2280$@viaeterna.nl>
 <DCA21393-202E-41C7-98E2-195C37E0E5B7@gmail.com>
Message-ID: <abc053d0-476a-44f8-895e-bb45679f1b08@Spark>

.... although not to the extent that "...family members need to take away my laptop and put me in a quiet room with the window shades drawn until I promise to behave myself..."

:-)

--
Andrew Robinson
Chief Executive Officer, CEBRA and Professor of Biosecurity,
School/s of BioSciences and Mathematics & Statistics
University of Melbourne, VIC 3010 Australia
Tel: (+61) 0403 138 955
Email: apro at unimelb.edu.au
Website: https://researchers.ms.unimelb.edu.au/~apro at unimelb/

I acknowledge the Traditional Owners of the land I inhabit, and pay my respects to their Elders.
On 14 Dec 2022 at 7:06 AM +1100, John H Maindonald <jhmaindonald at gmail.com>, wrote:
There is of course no perfect source. The beauty of Wikipedia is that what appears there
is open, unlike most formally published content, to correction of the source document.
There is a large amount of published content, some of it implemented in R packages,
that is never exposed to critique in any formal published form.

I have been surprised at the quality of Wikipedia, by and large, as a first place to go for
information on technical issues.

Douglas, can you comment further on what it is about code based on code based on
numpy and scipy that sets you going?

John Maindonld.

On 14/12/2022, at 06:49, Hein van Lieverloo <hein.van.lieverloo at viaeterna.nl> wrote:

Hi Douglas,

First: ?on the form of your
Second: thanks for the details and support for your quest!
Third: anyone can get a login on Wikipedia, I have made some changes myself (not on the page you refer to ?). This also explains why it is 'less than perfect' and should never be used as a single source.

Best,

Hein

-----Original Message-----
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of Douglas Bates
Sent: Tuesday, 13 December 2022 18:40
To: R Mixed Models <r-sig-mixed-models at r-project.org>
Subject: [R-sig-ME] Suggestions on how to correct a misapprehension?

So my family is having to live through a "Someone is wrong on the internet", https://xkcd.com/386, moment. In the past couple of days I have twice encountered the same mistaken characterization of how the parameter estimates in lmer and in the MixedModels.jl package are evaluated.

As we documented in our 2015 paper http://dx.doi.org/10.18637/jss.v067.i01
in lme4 the REML estimates or the ML estimates for the parameters of a linear mixed-effects model are evaluated by constrained optimization of a profiled log-likelihood or profiled log-restricted-likelihood. The parameters directly being optimized are the elements of relative covariance factors. The profiling involves solving a penalized least squares problem. This PLS representation, and the use of sparse matrices, is what allows for fitting models with random effects associated with crossed or partially crossed grouping factors, such as "subject" and "item". To many users this capability is one of the big selling points for lme4.

In our paper we explain in great detail why this approach is, in our opinion, superior to earlier approaches. And if someone doesn't believe us, both lme4 and MixedModels.jl are Open Source projects so anyone who wants to do so can just go read the code to find out what actually is done.

So it came as a surprise when reading the Wikipedia entry on mixed models, https://en.wikipedia.org/wiki/Mixed_model, to learn that lme4 and MixedModels.jl use an EM algorithm that Mary Lindstrom and I described (
https://doi.org/10.1080%2F01621459.1988.10478693) in 1988. It is possible that in the early days of lme4 there was such an implementation, but not in the last 15 years, and there definitely has never been such an implementation in MixedModels.jl. I noticed that the Python package statsmodels is described in the Wikipedia article and in their documentation, https://www.statsmodels.org/stable/mixed_linear.html, as using that EM algorithm. I didn't verify this in the code because reading code based on numpy and scipy causes me to start ranting and raving to the extent that family members need to take away my laptop and put me in a quiet room with the window shades drawn until I promise to behave myself.

Anyway the Python statsmodels documentation claims that lme4 uses this method, which it doesn't.

I have never gone through the process of proposing an edit in a Wikipedia article. As I understand it I would need to create a login etc. Would anyone who does have such a login be willing to propose an edit and save me the steps?

[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


	[[alternative HTML version deleted]]


From ru@@e||-|enth @end|ng |rom u|ow@@edu  Wed Dec 14 01:17:20 2022
From: ru@@e||-|enth @end|ng |rom u|ow@@edu (Lenth, Russell V)
Date: Wed, 14 Dec 2022 00:17:20 +0000
Subject: [R-sig-ME] Suggestions on how to correct a misapprehension?
Message-ID: <DM6PR04MB44744185FCBEF96033AD3A4BF1E09@DM6PR04MB4474.namprd04.prod.outlook.com>

Doug,

With all respect, you are the one who knows exactly how you want to change the Wikipedia article, and I think you should be the one to repair it. You can't expect someone else to do it the way you want.

Russ
--
Russell V. Lenth  -  Professor Emeritus
Department of Statistics and Actuarial Science
The University of Iowa  -  Iowa City, IA 52242  USA
Dept office (319)335-0712  -  FAX (319)335-3017
russell-lenth at uiowa.edu  -  http://www.stat.uiowa.edu/~rlenth/

	[[alternative HTML version deleted]]


From |upp @end|ng |rom uch|c@go@edu  Wed Dec 14 02:25:33 2022
From: |upp @end|ng |rom uch|c@go@edu (Stuart Luppescu)
Date: Wed, 14 Dec 2022 10:25:33 +0900
Subject: [R-sig-ME] Suggestions on how to correct a misapprehension?
In-Reply-To: <CAO7JsnSyaJ8F6yU_FP39EDog41Na081cbDKczMjBUvyTznNVnA@mail.gmail.com>
References: <CAO7JsnSyaJ8F6yU_FP39EDog41Na081cbDKczMjBUvyTznNVnA@mail.gmail.com>
Message-ID: <7abf39f9-c39f-bab9-6e82-74bdf3e86123@uchicago.edu>

On 12/14/22 02:39, Douglas Bates wrote:
> I have never gone through the process of proposing an edit in a Wikipedia
> article.  As I understand it I would need to create a login etc.  Would
> anyone who does have such a login be willing to propose an edit and save me
> the steps?

I was in this situation recently. I found AN ERROR in Wikipedia and 
resolved to fix it, but the Wikipedia admins wouldn't let me. I had been 
logged into my VPN on DigitalOcean, and they disable all attempts to 
edit articles from DigitalOcean. I had to log out of the VPN, and erase 
my Wikipedia cookies before I could correct the entry. In any case, I 
think I have a Wikipedia login, but I couldn't remember it so I just did 
the edit as "guest". If you want to be able to talk about your edit with 
other people post hoc, I think you might need a login. Otherwise, just 
do it without a login.

-- 
Stuart Luppescu
Chief Psychometrician (ret.)
UChicago Consortium on School Research


From bbo|ker @end|ng |rom gm@||@com  Wed Dec 14 02:38:16 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Tue, 13 Dec 2022 20:38:16 -0500
Subject: [R-sig-ME] Suggestions on how to correct a misapprehension?
In-Reply-To: <7abf39f9-c39f-bab9-6e82-74bdf3e86123@uchicago.edu>
References: <CAO7JsnSyaJ8F6yU_FP39EDog41Na081cbDKczMjBUvyTznNVnA@mail.gmail.com>
 <7abf39f9-c39f-bab9-6e82-74bdf3e86123@uchicago.edu>
Message-ID: <a3a26dcf-8f55-205c-cbaf-e701dbba5045@gmail.com>

   FWIW I can see that JP van Paridon has already gotten us most of the 
way there.

FWIW I don't think SAS uses EM either, at least as far as I can tell:

https://documentation.sas.com/doc/en/statcdc/14.2/statug/statug_mixed_details58.htm

PROC MIXED uses a ridge-stabilized Newton-Raphson algorithm to optimize 
either a full (ML) or residual (REML) likelihood function. The 
Newton-Raphson algorithm is preferred to the EM algorithm (Lindstrom and 
Bates 1988). PROC MIXED profiles the likelihood with respect to the 
fixed effects and also with respect to the residual variance whenever it 
appears reasonable to do so. The residual profiling can be avoided by 
using the NOPROFILE option of the PROC MIXED statement. PROC MIXED uses 
the MIVQUE0 method (Rao 1972; Giesbrecht 1989) to compute initial values.

On 2022-12-13 8:25 p.m., Stuart Luppescu wrote:
> On 12/14/22 02:39, Douglas Bates wrote:
>> I have never gone through the process of proposing an edit in a Wikipedia
>> article.? As I understand it I would need to create a login etc.? Would
>> anyone who does have such a login be willing to propose an edit and 
>> save me
>> the steps?
> 
> I was in this situation recently. I found AN ERROR in Wikipedia and 
> resolved to fix it, but the Wikipedia admins wouldn't let me. I had been 
> logged into my VPN on DigitalOcean, and they disable all attempts to 
> edit articles from DigitalOcean. I had to log out of the VPN, and erase 
> my Wikipedia cookies before I could correct the entry. In any case, I 
> think I have a Wikipedia login, but I couldn't remember it so I just did 
> the edit as "guest". If you want to be able to talk about your edit with 
> other people post hoc, I think you might need a login. Otherwise, just 
> do it without a login.
>


From v@np@r|don @end|ng |rom w|@c@edu  Wed Dec 14 03:11:46 2022
From: v@np@r|don @end|ng |rom w|@c@edu (Jeroen van Paridon)
Date: Wed, 14 Dec 2022 02:11:46 +0000
Subject: [R-sig-ME] Suggestions on how to correct a misapprehension?
In-Reply-To: <a3a26dcf-8f55-205c-cbaf-e701dbba5045@gmail.com>
References: <CAO7JsnSyaJ8F6yU_FP39EDog41Na081cbDKczMjBUvyTznNVnA@mail.gmail.com>
 <7abf39f9-c39f-bab9-6e82-74bdf3e86123@uchicago.edu>
 <a3a26dcf-8f55-205c-cbaf-e701dbba5045@gmail.com>
Message-ID: <DM6PR06MB4171800AA0E67C67BFAD84DEB8E09@DM6PR06MB4171.namprd06.prod.outlook.com>

Hi Ben,

It seems like Python's statsmodels might not be using EM either, which would mean that exactly none of the four stats packages listed in the original Wikipedia article actually use EM.

My edits were a mix of Phillip Alday's thoughts on the matter and mine; if I can work out exactly what statsmodels is doing I'll make further changes. (And I'm open to suggestions about anything else that might be good to add!


Cheers,

JP
________________________________
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on behalf of Ben Bolker <bbolker at gmail.com>
Sent: Tuesday, December 13, 2022 7:38:16 PM
To: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Suggestions on how to correct a misapprehension?

   FWIW I can see that JP van Paridon has already gotten us most of the
way there.

FWIW I don't think SAS uses EM either, at least as far as I can tell:

https://documentation.sas.com/doc/en/statcdc/14.2/statug/statug_mixed_details58.htm

PROC MIXED uses a ridge-stabilized Newton-Raphson algorithm to optimize
either a full (ML) or residual (REML) likelihood function. The
Newton-Raphson algorithm is preferred to the EM algorithm (Lindstrom and
Bates 1988). PROC MIXED profiles the likelihood with respect to the
fixed effects and also with respect to the residual variance whenever it
appears reasonable to do so. The residual profiling can be avoided by
using the NOPROFILE option of the PROC MIXED statement. PROC MIXED uses
the MIVQUE0 method (Rao 1972; Giesbrecht 1989) to compute initial values.

On 2022-12-13 8:25 p.m., Stuart Luppescu wrote:
> On 12/14/22 02:39, Douglas Bates wrote:
>> I have never gone through the process of proposing an edit in a Wikipedia
>> article.  As I understand it I would need to create a login etc.  Would
>> anyone who does have such a login be willing to propose an edit and
>> save me
>> the steps?
>
> I was in this situation recently. I found AN ERROR in Wikipedia and
> resolved to fix it, but the Wikipedia admins wouldn't let me. I had been
> logged into my VPN on DigitalOcean, and they disable all attempts to
> edit articles from DigitalOcean. I had to log out of the VPN, and erase
> my Wikipedia cookies before I could correct the entry. In any case, I
> think I have a Wikipedia login, but I couldn't remember it so I just did
> the edit as "guest". If you want to be able to talk about your edit with
> other people post hoc, I think you might need a login. Otherwise, just
> do it without a login.
>

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From |@w|@wt @end|ng |rom gm@||@com  Wed Dec 14 04:13:58 2022
From: |@w|@wt @end|ng |rom gm@||@com (Timothy MacKenzie)
Date: Tue, 13 Dec 2022 21:13:58 -0600
Subject: [R-sig-ME] Multivariate mixed models with different outcome
 distributions
Message-ID: <CADreqix_OtcvXPbZXb7-oF_g_opw8QiG1xoPG-QOtq7sTWf_Qw@mail.gmail.com>

Hello Colleagues,

I have a multivariate data structure (below) where the dependent
variables (DV) seem to have different distributions.

For instance, *ac* is measured in proportions and perhaps
beta-distributed, but *fl* and *le* may be normally distributed.

Would it make methodological sense to fit such DVs in a multivariate
mixed model given that they are theoretically related but practically
measured on different scales?

Any resources to provide mixed model strategies in such a situation?

Many thanks for your help,
Tim M

Score ~ DV + (1 | subj_id) ## Would this make sense?

# Data structure:
subj_id  DV     Score
1            ac      .5
1            fl        23.1
1            le       1.4
2            ac      .7
2            fl        19.6
2            le       2.1


From bbo|ker @end|ng |rom gm@||@com  Wed Dec 14 04:27:57 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Tue, 13 Dec 2022 22:27:57 -0500
Subject: [R-sig-ME] Multivariate mixed models with different outcome
 distributions
In-Reply-To: <CADreqix_OtcvXPbZXb7-oF_g_opw8QiG1xoPG-QOtq7sTWf_Qw@mail.gmail.com>
References: <CADreqix_OtcvXPbZXb7-oF_g_opw8QiG1xoPG-QOtq7sTWf_Qw@mail.gmail.com>
Message-ID: <CABghstT1Duf_YNJhouem6xkEJpoXxVo6qUy91S3_P29d2k8NkQ@mail.gmail.com>

MCMCglmm can handle this case

On Tue, Dec 13, 2022, 10:14 PM Timothy MacKenzie <fswfswt at gmail.com> wrote:

> Hello Colleagues,
>
> I have a multivariate data structure (below) where the dependent
> variables (DV) seem to have different distributions.
>
> For instance, *ac* is measured in proportions and perhaps
> beta-distributed, but *fl* and *le* may be normally distributed.
>
> Would it make methodological sense to fit such DVs in a multivariate
> mixed model given that they are theoretically related but practically
> measured on different scales?
>
> Any resources to provide mixed model strategies in such a situation?
>
> Many thanks for your help,
> Tim M
>
> Score ~ DV + (1 | subj_id) ## Would this make sense?
>
> # Data structure:
> subj_id  DV     Score
> 1            ac      .5
> 1            fl        23.1
> 1            le       1.4
> 2            ac      .7
> 2            fl        19.6
> 2            le       2.1
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From |upp @end|ng |rom uch|c@go@edu  Wed Dec 14 07:01:10 2022
From: |upp @end|ng |rom uch|c@go@edu (Stuart Luppescu)
Date: Wed, 14 Dec 2022 15:01:10 +0900
Subject: [R-sig-ME] Suggestions on how to correct a misapprehension?
In-Reply-To: <DM6PR06MB4171800AA0E67C67BFAD84DEB8E09@DM6PR06MB4171.namprd06.prod.outlook.com>
References: <CAO7JsnSyaJ8F6yU_FP39EDog41Na081cbDKczMjBUvyTznNVnA@mail.gmail.com>
 <7abf39f9-c39f-bab9-6e82-74bdf3e86123@uchicago.edu>
 <a3a26dcf-8f55-205c-cbaf-e701dbba5045@gmail.com>
 <DM6PR06MB4171800AA0E67C67BFAD84DEB8E09@DM6PR06MB4171.namprd06.prod.outlook.com>
Message-ID: <624a393f-29ae-dd6a-1750-9f71cf9f699d@uchicago.edu>

On 12/14/22 11:11, Jeroen van Paridon via R-sig-mixed-models wrote:
> none of the four stats packages listed in the original Wikipedia article actually use EM.

The original author of the article might have been confused by the fact 
that Bryk and Raudenbush's HLM programs use EM.

-- 
Stuart Luppescu
Chief Psychometrician (ret.)
UChicago Consortium on School Research


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Wed Dec 14 10:17:11 2022
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Wed, 14 Dec 2022 10:17:11 +0100
Subject: [R-sig-ME] Suggestions on how to correct a misapprehension?
In-Reply-To: <DM6PR06MB4171800AA0E67C67BFAD84DEB8E09@DM6PR06MB4171.namprd06.prod.outlook.com>
References: <CAO7JsnSyaJ8F6yU_FP39EDog41Na081cbDKczMjBUvyTznNVnA@mail.gmail.com>
 <7abf39f9-c39f-bab9-6e82-74bdf3e86123@uchicago.edu>
 <a3a26dcf-8f55-205c-cbaf-e701dbba5045@gmail.com>
 <DM6PR06MB4171800AA0E67C67BFAD84DEB8E09@DM6PR06MB4171.namprd06.prod.outlook.com>
Message-ID: <25497.38039.480506.122042@stat.math.ethz.ch>

>>>>> Jeroen van Paridon via R-sig-mixed-models 
>>>>>     on Wed, 14 Dec 2022 02:11:46 +0000 writes:

    > Hi Ben, It seems like Python's statsmodels might not be
    > using EM either, which would mean that exactly none of the
    > four stats packages listed in the original Wikipedia
    > article actually use EM.

That's not correct, either:

nlme  clearly uses *BOTH*  EM (initial steps) *and*
Newton-Raphson (==> non-sense in the SAS docu you cited).

Note the reference on lme's help page,

  Lindstrom, M.J. and Bates, D.M. (1988) "Newton-Raphson and EM
  Algorithms for Linear Mixed-Effects Models for Repeated-Measures
  Data", Journal of the American Statistical Association, 83, 1014--1022. 

which even has both algorithms in the title.


    > My edits were a mix of Phillip Alday's thoughts on the
    > matter and mine; if I can work out exactly what
    > statsmodels is doing I'll make further changes. (And I'm
    > open to suggestions about anything else that might be good
    > to add!

I've also amended Wikipedia pages (in my name) since 2006, as I
now looked up...

The current entry about lme4 / Mixed Models is still "grossly" misleading,
in my view.  Notably as it basically relates everything to
Henderson's MM equations -- which were phantastic when stated
63 years ago.

The main idea of Doug Bates' approach (in lme4, MixedModels) --
which Doug explained nicely -- namely that you can profile out much of the
parameter space (beta, sigma) via PLS while only needing to optimize
(the PLS resulting profiled likelihood) over the var-cov
parameters (theta) is still not all mentioned. 

Martin


    > Cheers,

    > JP ________________________________ From:
    > R-sig-mixed-models
    > <r-sig-mixed-models-bounces at r-project.org> on behalf of
    > Ben Bolker <bbolker at gmail.com> Sent: Tuesday, December 13,
    > 2022 7:38:16 PM To: r-sig-mixed-models at r-project.org
    > <r-sig-mixed-models at r-project.org> Subject: Re: [R-sig-ME]
    > Suggestions on how to correct a misapprehension?

    >    FWIW I can see that JP van Paridon has already gotten
    > us most of the way there.

    > FWIW I don't think SAS uses EM either, at least as far as
    > I can tell:

    > https://documentation.sas.com/doc/en/statcdc/14.2/statug/statug_mixed_details58.htm

    > PROC MIXED uses a ridge-stabilized Newton-Raphson
    > algorithm to optimize either a full (ML) or residual
    > (REML) likelihood function. The Newton-Raphson algorithm
    > is preferred to the EM algorithm (Lindstrom and Bates
    > 1988). PROC MIXED profiles the likelihood with respect to
    > the fixed effects and also with respect to the residual
    > variance whenever it appears reasonable to do so. The
    > residual profiling can be avoided by using the NOPROFILE
    > option of the PROC MIXED statement. PROC MIXED uses the
    > MIVQUE0 method (Rao 1972; Giesbrecht 1989) to compute
    > initial values.

    > On 2022-12-13 8:25 p.m., Stuart Luppescu wrote:
    >> On 12/14/22 02:39, Douglas Bates wrote:
    >>> I have never gone through the process of proposing an
    >>> edit in a Wikipedia article.  As I understand it I would
    >>> need to create a login etc.  Would anyone who does have
    >>> such a login be willing to propose an edit and save me
    >>> the steps?
    >> 
    >> I was in this situation recently. I found AN ERROR in
    >> Wikipedia and resolved to fix it, but the Wikipedia
    >> admins wouldn't let me. I had been logged into my VPN on
    >> DigitalOcean, and they disable all attempts to edit
    >> articles from DigitalOcean. I had to log out of the VPN,
    >> and erase my Wikipedia cookies before I could correct the
    >> entry. In any case, I think I have a Wikipedia login, but
    >> I couldn't remember it so I just did the edit as
    >> "guest". If you want to be able to talk about your edit
    >> with other people post hoc, I think you might need a
    >> login. Otherwise, just do it without a login.
    >> 

    > _______________________________________________
    > R-sig-mixed-models at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

    > 	[[alternative HTML version deleted]]

    > _______________________________________________
    > R-sig-mixed-models at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From pb@||ey @end|ng |rom @|r@org  Wed Dec 14 16:30:13 2022
From: pb@||ey @end|ng |rom @|r@org (Bailey, Paul)
Date: Wed, 14 Dec 2022 15:30:13 +0000
Subject: [R-sig-ME] Suggestions on how to correct a
In-Reply-To: <mailman.19965.881.1671009436.1609.r-sig-mixed-models@r-project.org>
References: <mailman.19965.881.1671009436.1609.r-sig-mixed-models@r-project.org>
Message-ID: <DS7PR05MB72399EE0B3B8AFD28DA3CA32BEE09@DS7PR05MB7239.namprd05.prod.outlook.com>

Hi Martin,

I don't know how to reply to the digest and get the thread right, so I apologize if I get it wrong.

I think the 2015 paper you link to describes linear models but you say "lme4" below. I think you mean only lme4::lmer.

The vignette labeled "Computational Methods" in the CRAN package (v 1.1-31) in section 4.2 "The PIRLS algorithm for [u-tilde] and [beta-tilde]" says that this is what is done for glmer as well as lmer. But I think that can't be, because I think that would then mean that when I requested the Laplace approximation (nAGQ=1) I would be getting the PLS results (nAGQ=0 results) for beta. But please correct me if I'm wrong.


Best,
Paul Bailey


From |@w|@wt @end|ng |rom gm@||@com  Wed Dec 14 18:09:19 2022
From: |@w|@wt @end|ng |rom gm@||@com (Timothy MacKenzie)
Date: Wed, 14 Dec 2022 11:09:19 -0600
Subject: [R-sig-ME] Multivariate mixed models with different outcome
 distributions
In-Reply-To: <CABghstT1Duf_YNJhouem6xkEJpoXxVo6qUy91S3_P29d2k8NkQ@mail.gmail.com>
References: <CADreqix_OtcvXPbZXb7-oF_g_opw8QiG1xoPG-QOtq7sTWf_Qw@mail.gmail.com>
 <CABghstT1Duf_YNJhouem6xkEJpoXxVo6qUy91S3_P29d2k8NkQ@mail.gmail.com>
Message-ID: <CADreqizDcYyGeTUYwFn-yn3me9+6kAtcZ5E+eEtS8ULvHSoT8Q@mail.gmail.com>

Dear Ben,

Thank you for the hint. Regarding MCMCglmm, I couldn't find "beta" in
the family of allowable distributions in the package. Did you have a
specific set of distribution families in mind to handle normal and
beta responses simultaneously?

Also, I noticed the brms package apparently can handle different
response distributions, is there a reason, in your expert opinion, to
opt for MCMCglmm?

Many thanks,
Tim M

On Tue, Dec 13, 2022 at 9:28 PM Ben Bolker <bbolker at gmail.com> wrote:
>
> MCMCglmm can handle this case
>
> On Tue, Dec 13, 2022, 10:14 PM Timothy MacKenzie <fswfswt at gmail.com> wrote:
>>
>> Hello Colleagues,
>>
>> I have a multivariate data structure (below) where the dependent
>> variables (DV) seem to have different distributions.
>>
>> For instance, *ac* is measured in proportions and perhaps
>> beta-distributed, but *fl* and *le* may be normally distributed.
>>
>> Would it make methodological sense to fit such DVs in a multivariate
>> mixed model given that they are theoretically related but practically
>> measured on different scales?
>>
>> Any resources to provide mixed model strategies in such a situation?
>>
>> Many thanks for your help,
>> Tim M
>>
>> Score ~ DV + (1 | subj_id) ## Would this make sense?
>>
>> # Data structure:
>> subj_id  DV     Score
>> 1            ac      .5
>> 1            fl        23.1
>> 1            le       1.4
>> 2            ac      .7
>> 2            fl        19.6
>> 2            le       2.1
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From dmb@te@ @end|ng |rom gm@||@com  Wed Dec 14 18:45:52 2022
From: dmb@te@ @end|ng |rom gm@||@com (Douglas Bates)
Date: Wed, 14 Dec 2022 11:45:52 -0600
Subject: [R-sig-ME] Suggestions on how to correct a misapprehension?
In-Reply-To: <25497.38039.480506.122042@stat.math.ethz.ch>
References: <CAO7JsnSyaJ8F6yU_FP39EDog41Na081cbDKczMjBUvyTznNVnA@mail.gmail.com>
 <7abf39f9-c39f-bab9-6e82-74bdf3e86123@uchicago.edu>
 <a3a26dcf-8f55-205c-cbaf-e701dbba5045@gmail.com>
 <DM6PR06MB4171800AA0E67C67BFAD84DEB8E09@DM6PR06MB4171.namprd06.prod.outlook.com>
 <25497.38039.480506.122042@stat.math.ethz.ch>
Message-ID: <CAO7JsnR8+JBhwo7dT-DQ=5gR4Jforz9NT7swP+=0-xqCV+=0zA@mail.gmail.com>

Thank you to all who commented and especially to JP van Paridon for editing
the Wikipedia article.

I think the new version is a great improvement in accuracy.

There is a subtle, but important, difference between lme4/MixedModels and
other implementations of mixed-effects models in terms of the scale on
which the parameters being directly optimized are defined.  I don't know
how to phrase this in an easily understood way.  I'll try to explain it and
perhaps someone can suggest a more coherent description.

I mentioned that lme4/MixedModels profile the log-likelihood with respect
to elements of a  "relative covariance factor".  The easiest way to
understand what that phrase means is to consider a case with scalar random
effects. Let ??? be the variance of the random effects.  The factor of this
variance is the standard deviation, ??. (In the case of vector-valued
random effects we use the Cholesky factor of the positive semi-definite
variance-covariance matrix.)  The relative covariance factor is ?? / ?
where ? is the standard deviation of the per-observation or "residual"
noise.

The "profiling" of the objective means that, given a value of ?? / ?, we
can determine the optimal value of the log-likelihood over all possible
values of ?, the fixed-effects coefficients, and ??, the residual variance,
with a direct (i.e. non-iterative) calculation of solving a penalized least
squares problem.

In contrast, most other implementations optimize with respect to the
elements of the precision matrix, G??, of the random effects because that
is the way that Henderson's mixed-model equations were originally written.

The choice of whether to optimize on the "standard deviation scale" or the
"precision scale" determines the nature of the boundary cases.  It can be
shown that on the standard deviation scale it is possible, and not
uncommon, to converge to a singular covariance matrix (?? / ? ?0 for scalar
random effects) but not to an infinite standard deviation.  On the
precision scale the situation is reversed - a precision of 0 is not
possible but infinite precision is possible.  As a result the optimization
on the standard deviation scale is more stable than on the precision
scale.  It is much easier to set a lower bound of 0 on a parameter like ??
/ ? than to try to converge on a precision parameter that, technically,
will diverge to infinity.

This is a subtle and technical distinction and thus probably not suitable
for a Wikipedia explanation.

I was asked why I'm not a fan of numpy and scipy.  I find them unattractive
from the software-engineering point of view.  As far as I know, and I am
not a Python expert, Python offers only a methods-within-class-definitions
style of object-oriented programming.  R and Julia allow for defining
methods for generic functions with single (S3 methods in R) or multiple
(Julia and S4 methods in R) dispatch from a function call to an
implementation method.  For fitting statistical models or for numerical
linear algebra the "generics + classes = methods" style is a much better
fit, as John Chambers was wise enough to see when he created the S3 system
for S.

Consider the Matrix package in R or the LinearAlgebra package in Julia.
They each define many many different types of matrices and methods for
operations on combinations of these types.  For example, there are 141
different methods for `mul!` (matrix multiplication overwriting one of its
arguments with the product) in Julia's LinearAlgebra package, and that is
without counting the methods for `lmul!`, `rmul!`, `*` (like R's `%*%`),
etc.
That is, there are several hundred specializations of the general concept
of "multiplication of linear algebra objects"  and this is how you make
linear algebra fast - by specializing the generic concept on the types of
all of the operands.

Now look at
https://docs.scipy.org/doc//scipy/reference/linalg.html#module-scipy.linalg
to see how it is done in SciPy.  There is no dispatch - just a collection
of awkwardly-named, stand-alone functions that cover only a handful of the
possible cases.  We might as well be back writing Fortran subroutines - at
least Lapack did some forms of dispatch by coding the types of arguments in
the subroutine name.

It just seems to me that numpy and scipy are pretty weak tea for scientific
programming in what is apparently the world's most popular programming
language.



On Wed, Dec 14, 2022 at 3:17 AM Martin Maechler <maechler at stat.math.ethz.ch>
wrote:

> >>>>> Jeroen van Paridon via R-sig-mixed-models
> >>>>>     on Wed, 14 Dec 2022 02:11:46 +0000 writes:
>
>     > Hi Ben, It seems like Python's statsmodels might not be
>     > using EM either, which would mean that exactly none of the
>     > four stats packages listed in the original Wikipedia
>     > article actually use EM.
>
> That's not correct, either:
>
> nlme  clearly uses *BOTH*  EM (initial steps) *and*
> Newton-Raphson (==> non-sense in the SAS docu you cited).
>
> Note the reference on lme's help page,
>
>   Lindstrom, M.J. and Bates, D.M. (1988) "Newton-Raphson and EM
>   Algorithms for Linear Mixed-Effects Models for Repeated-Measures
>   Data", Journal of the American Statistical Association, 83, 1014--1022.
>
> which even has both algorithms in the title.
>
>
>     > My edits were a mix of Phillip Alday's thoughts on the
>     > matter and mine; if I can work out exactly what
>     > statsmodels is doing I'll make further changes. (And I'm
>     > open to suggestions about anything else that might be good
>     > to add!
>
> I've also amended Wikipedia pages (in my name) since 2006, as I
> now looked up...
>
> The current entry about lme4 / Mixed Models is still "grossly" misleading,
> in my view.  Notably as it basically relates everything to
> Henderson's MM equations -- which were phantastic when stated
> 63 years ago.
>
> The main idea of Doug Bates' approach (in lme4, MixedModels) --
> which Doug explained nicely -- namely that you can profile out much of the
> parameter space (beta, sigma) via PLS while only needing to optimize
> (the PLS resulting profiled likelihood) over the var-cov
> parameters (theta) is still not all mentioned.
>
> Martin
>
>
>     > Cheers,
>
>     > JP ________________________________ From:
>     > R-sig-mixed-models
>     > <r-sig-mixed-models-bounces at r-project.org> on behalf of
>     > Ben Bolker <bbolker at gmail.com> Sent: Tuesday, December 13,
>     > 2022 7:38:16 PM To: r-sig-mixed-models at r-project.org
>     > <r-sig-mixed-models at r-project.org> Subject: Re: [R-sig-ME]
>     > Suggestions on how to correct a misapprehension?
>
>     >    FWIW I can see that JP van Paridon has already gotten
>     > us most of the way there.
>
>     > FWIW I don't think SAS uses EM either, at least as far as
>     > I can tell:
>
>     >
> https://documentation.sas.com/doc/en/statcdc/14.2/statug/statug_mixed_details58.htm
>
>     > PROC MIXED uses a ridge-stabilized Newton-Raphson
>     > algorithm to optimize either a full (ML) or residual
>     > (REML) likelihood function. The Newton-Raphson algorithm
>     > is preferred to the EM algorithm (Lindstrom and Bates
>     > 1988). PROC MIXED profiles the likelihood with respect to
>     > the fixed effects and also with respect to the residual
>     > variance whenever it appears reasonable to do so. The
>     > residual profiling can be avoided by using the NOPROFILE
>     > option of the PROC MIXED statement. PROC MIXED uses the
>     > MIVQUE0 method (Rao 1972; Giesbrecht 1989) to compute
>     > initial values.
>
>     > On 2022-12-13 8:25 p.m., Stuart Luppescu wrote:
>     >> On 12/14/22 02:39, Douglas Bates wrote:
>     >>> I have never gone through the process of proposing an
>     >>> edit in a Wikipedia article.  As I understand it I would
>     >>> need to create a login etc.  Would anyone who does have
>     >>> such a login be willing to propose an edit and save me
>     >>> the steps?
>     >>
>     >> I was in this situation recently. I found AN ERROR in
>     >> Wikipedia and resolved to fix it, but the Wikipedia
>     >> admins wouldn't let me. I had been logged into my VPN on
>     >> DigitalOcean, and they disable all attempts to edit
>     >> articles from DigitalOcean. I had to log out of the VPN,
>     >> and erase my Wikipedia cookies before I could correct the
>     >> entry. In any case, I think I have a Wikipedia login, but
>     >> I couldn't remember it so I just did the edit as
>     >> "guest". If you want to be able to talk about your edit
>     >> with other people post hoc, I think you might need a
>     >> login. Otherwise, just do it without a login.
>     >>
>
>     > _______________________________________________
>     > R-sig-mixed-models at r-project.org mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>     >   [[alternative HTML version deleted]]
>
>     > _______________________________________________
>     > R-sig-mixed-models at r-project.org mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Wed Dec 14 19:12:08 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 14 Dec 2022 13:12:08 -0500
Subject: [R-sig-ME] Multivariate mixed models with different outcome
 distributions
In-Reply-To: <CADreqizDcYyGeTUYwFn-yn3me9+6kAtcZ5E+eEtS8ULvHSoT8Q@mail.gmail.com>
References: <CADreqix_OtcvXPbZXb7-oF_g_opw8QiG1xoPG-QOtq7sTWf_Qw@mail.gmail.com>
 <CABghstT1Duf_YNJhouem6xkEJpoXxVo6qUy91S3_P29d2k8NkQ@mail.gmail.com>
 <CADreqizDcYyGeTUYwFn-yn3me9+6kAtcZ5E+eEtS8ULvHSoT8Q@mail.gmail.com>
Message-ID: <a31e3dac-2312-fa56-fc70-447569f83346@gmail.com>

   I didn't realize that brms does multi-type models, but apparently it 
does:

https://cran.r-project.org/web/packages/brms/vignettes/brms_multivariate.html

   ... so yes, I would go for brms in this case.

   cheers
    Ben


On 2022-12-14 12:09 p.m., Timothy MacKenzie wrote:
> Dear Ben,
> 
> Thank you for the hint. Regarding MCMCglmm, I couldn't find "beta" in
> the family of allowable distributions in the package. Did you have a
> specific set of distribution families in mind to handle normal and
> beta responses simultaneously?
> 
> Also, I noticed the brms package apparently can handle different
> response distributions, is there a reason, in your expert opinion, to
> opt for MCMCglmm?
> 
> Many thanks,
> Tim M
> 
> On Tue, Dec 13, 2022 at 9:28 PM Ben Bolker <bbolker at gmail.com> wrote:
>>
>> MCMCglmm can handle this case
>>
>> On Tue, Dec 13, 2022, 10:14 PM Timothy MacKenzie <fswfswt at gmail.com> wrote:
>>>
>>> Hello Colleagues,
>>>
>>> I have a multivariate data structure (below) where the dependent
>>> variables (DV) seem to have different distributions.
>>>
>>> For instance, *ac* is measured in proportions and perhaps
>>> beta-distributed, but *fl* and *le* may be normally distributed.
>>>
>>> Would it make methodological sense to fit such DVs in a multivariate
>>> mixed model given that they are theoretically related but practically
>>> measured on different scales?
>>>
>>> Any resources to provide mixed model strategies in such a situation?
>>>
>>> Many thanks for your help,
>>> Tim M
>>>
>>> Score ~ DV + (1 | subj_id) ## Would this make sense?
>>>
>>> # Data structure:
>>> subj_id  DV     Score
>>> 1            ac      .5
>>> 1            fl        23.1
>>> 1            le       1.4
>>> 2            ac      .7
>>> 2            fl        19.6
>>> 2            le       2.1
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics
 > E-mail is sent at my convenience; I don't expect replies outside of 
working hours.


From h|gh@t@t @end|ng |rom h|gh@t@t@com  Thu Dec 15 12:04:16 2022
From: h|gh@t@t @end|ng |rom h|gh@t@t@com (Highland Statistics Ltd)
Date: Thu, 15 Dec 2022 11:04:16 +0000
Subject: [R-sig-ME] Multivariate mixed models with different outcome,
 distributions
In-Reply-To: <mailman.19970.9.1671102001.52397.r-sig-mixed-models@r-project.org>
References: <mailman.19970.9.1671102001.52397.r-sig-mixed-models@r-project.org>
Message-ID: <d31fa305-c78a-3433-5011-f7b32f6ae528@highstat.com>



> Today's Topics:
>
>     1. Re: Multivariate mixed models with different outcome
>        distributions (Ben Bolker)
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Wed, 14 Dec 2022 13:12:08 -0500
> From: Ben Bolker <bbolker at gmail.com>
> To: Timothy MacKenzie <fswfswt at gmail.com>
> Cc: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
> Subject: Re: [R-sig-ME] Multivariate mixed models with different
> 	outcome distributions
> Message-ID: <a31e3dac-2312-fa56-fc70-447569f83346 at gmail.com>
> Content-Type: text/plain; charset="utf-8"; Format="flowed"
>
>     I didn't realize that brms does multi-type models, but apparently it
> does:
>
> https://cran.r-project.org/web/packages/brms/vignettes/brms_multivariate.html
>
>     ... so yes, I would go for brms in this case.
>
>     cheers
>      Ben
>

Hello,

The R-INLA package can do this as well...and it can also do the beta 
distribution.

Kind regards,

Alain





> On 2022-12-14 12:09 p.m., Timothy MacKenzie wrote:
>> Dear Ben,
>>
>> Thank you for the hint. Regarding MCMCglmm, I couldn't find "beta" in
>> the family of allowable distributions in the package. Did you have a
>> specific set of distribution families in mind to handle normal and
>> beta responses simultaneously?
>>
>> Also, I noticed the brms package apparently can handle different
>> response distributions, is there a reason, in your expert opinion, to
>> opt for MCMCglmm?
>>
>> Many thanks,
>> Tim M
>>
>> On Tue, Dec 13, 2022 at 9:28 PM Ben Bolker <bbolker at gmail.com> wrote:
>>> MCMCglmm can handle this case
>>>
>>> On Tue, Dec 13, 2022, 10:14 PM Timothy MacKenzie <fswfswt at gmail.com> wrote:
>>>> Hello Colleagues,
>>>>
>>>> I have a multivariate data structure (below) where the dependent
>>>> variables (DV) seem to have different distributions.
>>>>
>>>> For instance, *ac* is measured in proportions and perhaps
>>>> beta-distributed, but *fl* and *le* may be normally distributed.
>>>>
>>>> Would it make methodological sense to fit such DVs in a multivariate
>>>> mixed model given that they are theoretically related but practically
>>>> measured on different scales?
>>>>
>>>> Any resources to provide mixed model strategies in such a situation?
>>>>
>>>> Many thanks for your help,
>>>> Tim M
>>>>
>>>> Score ~ DV + (1 | subj_id) ## Would this make sense?
>>>>
>>>> # Data structure:
>>>> subj_id  DV     Score
>>>> 1            ac      .5
>>>> 1            fl        23.1
>>>> 1            le       1.4
>>>> 2            ac      .7
>>>> 2            fl        19.6
>>>> 2            le       2.1
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Dr. Alain F. Zuur
Highland Statistics Ltd.
9 St Clair Wynd
AB41 6DZ Newburgh, UK
Email: highstat at highstat.com
URL:   www.highstat.com


From jorgemmtte|xe|r@ @end|ng |rom gm@||@com  Thu Dec 15 16:17:51 2022
From: jorgemmtte|xe|r@ @end|ng |rom gm@||@com (Jorge Teixeira)
Date: Thu, 15 Dec 2022 15:17:51 +0000
Subject: [R-sig-ME] Modelling football matches
Message-ID: <CAOYO_yB63WA-=XLsEiO2K+ty4aBX4oope_uucmUvaa_ZNecTSQ@mail.gmail.com>

Hi.

1) Assuming that most are somewhat familiar with football, and that it is
world cup time, what do you think of this model to compare differences in
distance covered between stages (group stage vs final stage)?

lmer(distance ~ stage + (1|player) + (1|game/game_part), data=my_data)

2) In theory, which random slopes do you think should be added, if any?

Thank you.

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Thu Dec 15 18:12:16 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Thu, 15 Dec 2022 12:12:16 -0500
Subject: [R-sig-ME] Modelling football matches
In-Reply-To: <CAOYO_yB63WA-=XLsEiO2K+ty4aBX4oope_uucmUvaa_ZNecTSQ@mail.gmail.com>
References: <CAOYO_yB63WA-=XLsEiO2K+ty4aBX4oope_uucmUvaa_ZNecTSQ@mail.gmail.com>
Message-ID: <ca745d94-ac01-3422-cd66-0d85058d8936@gmail.com>


    For a positive-valued variable like distance you might want to 
consider a log-linear model (lmer(log(distance) ~ ...) or a Gamma GLMM 
(glmer(distance  ~ ..., family = Gamma(link="log"))

   I believe the full model here would use random slopes ('slopes' in 
the broad sense since stage is a categorical variable) of stage 
(stage|player) - (stage|game) won't work because each game is only one 
stage.

   I'm not sure about the definition of 'game_part', but you might want 
to add a *fixed* effect of game_part as well as the 'game_part within 
game' nested random effect.

   There's probably a huge amount of covariate information you could add 
(e.g. player's position, player's age), probably other stuff too (random 
effect of team?)

On 2022-12-15 10:17 a.m., Jorge Teixeira wrote:
> Hi.
> 
> 1) Assuming that most are somewhat familiar with football, and that it is
> world cup time, what do you think of this model to compare differences in
> distance covered between stages (group stage vs final stage)?
> 
> lmer(distance ~ stage + (1|player) + (1|game/game_part), data=my_data)
> 
> 2) In theory, which random slopes do you think should be added, if any?
> 
> Thank you.
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics
 > E-mail is sent at my convenience; I don't expect replies outside of 
working hours.


From bbo|ker @end|ng |rom gm@||@com  Thu Dec 15 18:13:40 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Thu, 15 Dec 2022 12:13:40 -0500
Subject: [R-sig-ME] Modelling football matches
In-Reply-To: <CAOYO_yB63WA-=XLsEiO2K+ty4aBX4oope_uucmUvaa_ZNecTSQ@mail.gmail.com>
References: <CAOYO_yB63WA-=XLsEiO2K+ty4aBX4oope_uucmUvaa_ZNecTSQ@mail.gmail.com>
Message-ID: <56d79961-4bf8-cc7c-7c6a-26cca06ff6d7@gmail.com>


    For a positive-valued variable like distance you might want to 
consider a log-linear model (lmer(log(distance) ~ ...) or a Gamma GLMM 
(glmer(distance  ~ ..., family = Gamma(link="log"))

   I believe the full model here would use random slopes ('slopes' in 
the broad sense since stage is a categorical variable) of stage 
(stage|player) - (stage|game) won't work because each game is only one 
stage.

   I'm not sure about the definition of 'game_part', but you might want 
to add a *fixed* effect of game_part as well as the 'game_part within 
game' nested random effect.

   There's probably a huge amount of covariate information you could add 
(e.g. player's position, player's age), probably other stuff too (random 
effect of team?)

On 2022-12-15 10:17 a.m., Jorge Teixeira wrote:
> Hi.
> 
> 1) Assuming that most are somewhat familiar with football, and that it is
> world cup time, what do you think of this model to compare differences in
> distance covered between stages (group stage vs final stage)?
> 
> lmer(distance ~ stage + (1|player) + (1|game/game_part), data=my_data)
> 
> 2) In theory, which random slopes do you think should be added, if any?
> 
> Thank you.
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics
 > E-mail is sent at my convenience; I don't expect replies outside of 
working hours.


From |@w|@wt @end|ng |rom gm@||@com  Thu Dec 15 18:20:42 2022
From: |@w|@wt @end|ng |rom gm@||@com (Timothy MacKenzie)
Date: Thu, 15 Dec 2022 11:20:42 -0600
Subject: [R-sig-ME] Multivariate mixed models with different outcome
 distributions
In-Reply-To: <a31e3dac-2312-fa56-fc70-447569f83346@gmail.com>
References: <CADreqix_OtcvXPbZXb7-oF_g_opw8QiG1xoPG-QOtq7sTWf_Qw@mail.gmail.com>
 <CABghstT1Duf_YNJhouem6xkEJpoXxVo6qUy91S3_P29d2k8NkQ@mail.gmail.com>
 <CADreqizDcYyGeTUYwFn-yn3me9+6kAtcZ5E+eEtS8ULvHSoT8Q@mail.gmail.com>
 <a31e3dac-2312-fa56-fc70-447569f83346@gmail.com>
Message-ID: <CADreqizwMdaxV-8BUJ7LwGxEReNCaNjQ9NZ_yDEZTW62g9LTRw@mail.gmail.com>

Dear Ben,

Thank you for your confirmation. There are two things that I want to
better understand.

First, brms::brm() etc. require wide-format data. For my data (below
see long-format data from a single student), wide-formatting it will
create 256 columns for each subject (attached)! Is using brm() etc.
really practical here?

Second, nlme::lme() allows modeling the residuals. If I model the
residuals from my responses (CAL_type) on their current scale (some
proportions, some normal ones) with lme() and find a relatively good
fitting model, would that be a second best solution?

Thanks,
Tim M

LONG-FORMAT="
Class Person Task_order Task_type Time Score  CAL_type  Mot_ex Mot_inr
Mot_ide Mot_int Mot_amot   Eng_leng_txt Eng_time_on_tsk
1     1      S-C        simple    1    5      com_mult  4      2
3       3       1          300          20
1     1      S-C        simple    1   .3      com_dc/t  4      2
3       3       1          300          20
1     1      S-C        simple    1    2      com_cn/t  4      2
3       3       1          300          20
1     1      S-C        simple    1    3      com_cn/c  4      2
3       3       1          300          20
1     1      S-C        simple    1   .4      ac        4      2
3       3       1          300          20
1     1      S-C        simple    1    1      lex_vo    4      2
3       3       1          300          20
1     1      S-C        simple    1    5      lex_fr    4      2
3       3       1          300          20

1     1      S-C        complex   2    2      com_mult  3      4
2       1       2          200          25
1     1      S-C        complex   2   .3      com_dc/t  3      4
2       1       2          200          25
1     1      S-C        complex   2    4      com_cn/t  3      4
2       1       2          200          25
1     1      S-C        complex   2    3      com_cn/c  3      4
2       1       2          200          25
1     1      S-C        complex   2   .4      ac        3      4
2       1       2          200          25
1     1      S-C        complex   2    4      lex_vo    3      4
2       1       2          200          25
1     1      S-C        complex   2    5      lex_fr    3      4
2       1       2          200          25

1     1      S-C        simple    3    4      com_mult  5      2
3       4       3          100          10
1     1      S-C        simple    3   .2      com_dc/t  5      2
3       4       3          100          10
1     1      S-C        simple    3    3      com_cn/t  5      2
3       4       3          100          10
1     1      S-C        simple    3    3      com_cn/c  5      2
3       4       3          100          10
1     1      S-C        simple    3   .6      ac        5      2
3       4       3          100          10
1     1      S-C        simple    3    6      lex_vo    5      2
3       4       3          100          10
1     1      S-C        simple    3    6      lex_fr    5      2
3       4       3          100          10

1     1      S-C        complex   4    1      com_mult  1      3
2       5       4          400          35
1     1      S-C        complex   4   .1      com_dc/t  1      3
2       5       4          400          35
1     1      S-C        complex   4    1      com_cn/t  1      3
2       5       4          400          35
1     1      S-C        complex   4    3      com_cn/c  1      3
2       5       4          400          35
1     1      S-C        complex   4   .3      ac        1      3
2       5       4          400          35
1     1      S-C        complex   4    5      lex_vo    1      3
2       5       4          400          35
1     1      S-C        complex   4    5      lex_fr    1      3
2       5       4          400          35
"

On Wed, Dec 14, 2022 at 12:12 PM Ben Bolker <bbolker at gmail.com> wrote:
>
>    I didn't realize that brms does multi-type models, but apparently it
> does:
>
> https://cran.r-project.org/web/packages/brms/vignettes/brms_multivariate.html
>
>    ... so yes, I would go for brms in this case.
>
>    cheers
>     Ben
>
>
> On 2022-12-14 12:09 p.m., Timothy MacKenzie wrote:
> > Dear Ben,
> >
> > Thank you for the hint. Regarding MCMCglmm, I couldn't find "beta" in
> > the family of allowable distributions in the package. Did you have a
> > specific set of distribution families in mind to handle normal and
> > beta responses simultaneously?
> >
> > Also, I noticed the brms package apparently can handle different
> > response distributions, is there a reason, in your expert opinion, to
> > opt for MCMCglmm?
> >
> > Many thanks,
> > Tim M
> >
> > On Tue, Dec 13, 2022 at 9:28 PM Ben Bolker <bbolker at gmail.com> wrote:
> >>
> >> MCMCglmm can handle this case
> >>
> >> On Tue, Dec 13, 2022, 10:14 PM Timothy MacKenzie <fswfswt at gmail.com> wrote:
> >>>
> >>> Hello Colleagues,
> >>>
> >>> I have a multivariate data structure (below) where the dependent
> >>> variables (DV) seem to have different distributions.
> >>>
> >>> For instance, *ac* is measured in proportions and perhaps
> >>> beta-distributed, but *fl* and *le* may be normally distributed.
> >>>
> >>> Would it make methodological sense to fit such DVs in a multivariate
> >>> mixed model given that they are theoretically related but practically
> >>> measured on different scales?
> >>>
> >>> Any resources to provide mixed model strategies in such a situation?
> >>>
> >>> Many thanks for your help,
> >>> Tim M
> >>>
> >>> Score ~ DV + (1 | subj_id) ## Would this make sense?
> >>>
> >>> # Data structure:
> >>> subj_id  DV     Score
> >>> 1            ac      .5
> >>> 1            fl        23.1
> >>> 1            le       1.4
> >>> 2            ac      .7
> >>> 2            fl        19.6
> >>> 2            le       2.1
> >>>
> >>> _______________________________________________
> >>> R-sig-mixed-models at r-project.org mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> --
> Dr. Benjamin Bolker
> Professor, Mathematics & Statistics and Biology, McMaster University
> Director, School of Computational Science and Engineering
> (Acting) Graduate chair, Mathematics & Statistics
>  > E-mail is sent at my convenience; I don't expect replies outside of
> working hours.

From jorgemmtte|xe|r@ @end|ng |rom gm@||@com  Thu Dec 15 21:02:04 2022
From: jorgemmtte|xe|r@ @end|ng |rom gm@||@com (Jorge Teixeira)
Date: Thu, 15 Dec 2022 20:02:04 +0000
Subject: [R-sig-ME] Modelling football matches
In-Reply-To: <CAOYO_yB63WA-=XLsEiO2K+ty4aBX4oope_uucmUvaa_ZNecTSQ@mail.gmail.com>
References: <CAOYO_yB63WA-=XLsEiO2K+ty4aBX4oope_uucmUvaa_ZNecTSQ@mail.gmail.com>
Message-ID: <CAOYO_yDZFvG9X8Nh6NvqWfStcwuDEfjNZroxxT-GCOZjmQ38_A@mail.gmail.com>

Thank you, Ben.

Yes, indeed there are many more things that could be added - I was trying
to discuss a more fundamental structure.

Game_part was related to the fact that each game has part 1 and part 2.

1) I agree it makes sense to have game_part as fixed effect too, with this
result

lmer(distance ~ stage + *game_part* + (1|player) + (1|game/game_part),
data=my_data)

2) As for the random slopes, my question was that I believe the variation
by game and game_part might be different across players. Can random slopes
account for that?

3) For outcomes such as relative average heart rate, that are bounded by
100%, do you recommend a specific family of models?

Thanks once again.

Date: Thu, 15 Dec 2022 12:12:16 -0500
From: Ben Bolker <bbolker at gmail.com>
To: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Modelling football matches
Message-ID: <ca745d94-ac01-3422-cd66-0d85058d8936 at gmail.com>
Content-Type: text/plain; charset="utf-8"; Format="flowed"


    For a positive-valued variable like distance you might want to
consider a log-linear model (lmer(log(distance) ~ ...) or a Gamma GLMM
(glmer(distance  ~ ..., family = Gamma(link="log"))

   I believe the full model here would use random slopes ('slopes' in
the broad sense since stage is a categorical variable) of stage
(stage|player) - (stage|game) won't work because each game is only one
stage.

   I'm not sure about the definition of 'game_part', but you might want
to add a *fixed* effect of game_part as well as the 'game_part within
game' nested random effect.

   There's probably a huge amount of covariate information you could add
(e.g. player's position, player's age), probably other stuff too (random
effect of team?)

Jorge Teixeira <jorgemmtteixeira at gmail.com> escreveu no dia quinta,
15/12/2022 ?(s) 15:17:

> Hi.
>
> 1) Assuming that most are somewhat familiar with football, and that it is
> world cup time, what do you think of this model to compare differences in
> distance covered between stages (group stage vs final stage)?
>
> lmer(distance ~ stage + (1|player) + (1|game/game_part), data=my_data)
>
> 2) In theory, which random slopes do you think should be added, if any?
>
> Thank you.
>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Sat Dec 17 01:25:14 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Fri, 16 Dec 2022 19:25:14 -0500
Subject: [R-sig-ME] Multivariate mixed models with different outcome
 distributions
In-Reply-To: <CADreqizwMdaxV-8BUJ7LwGxEReNCaNjQ9NZ_yDEZTW62g9LTRw@mail.gmail.com>
References: <CADreqix_OtcvXPbZXb7-oF_g_opw8QiG1xoPG-QOtq7sTWf_Qw@mail.gmail.com>
 <CABghstT1Duf_YNJhouem6xkEJpoXxVo6qUy91S3_P29d2k8NkQ@mail.gmail.com>
 <CADreqizDcYyGeTUYwFn-yn3me9+6kAtcZ5E+eEtS8ULvHSoT8Q@mail.gmail.com>
 <a31e3dac-2312-fa56-fc70-447569f83346@gmail.com>
 <CADreqizwMdaxV-8BUJ7LwGxEReNCaNjQ9NZ_yDEZTW62g9LTRw@mail.gmail.com>
Message-ID: <5ed02a51-69fd-3d24-cb9e-d8416ec5da05@gmail.com>

   Is your multivariate response 256-dimensional (i.e. you have 256 
responses from each individual which you want to treat as correlated)? 
That's *very* unlikely to work if you just stick it into a standard 
multivariate framework, as you'll have to estimate a 256?256 covariance 
matrix (almost 33,000 parameters). A factor-analytic or reduced-rank 
model could work (e.g. glmmTMB now allows this -- although it's not well 
tested and definitely not with examples this large!), but doesn't do 
multi-type models).

   I'm not quite sure what you intend to do with nlme::lme() here.

  It may well be that just doing something like an arcsine-sqrt 
transform (or logit, if you prefer and have no exact 0/1 values) for 
your proportion data and then modeling everything as a multi-response 
Gaussian would be good enough ...



On 2022-12-15 12:20 p.m., Timothy MacKenzie wrote:
> Dear Ben,
> 
> Thank you for your confirmation. There are two things that I want to
> better understand.
> 
> First, brms::brm() etc. require wide-format data. For my data (below
> see long-format data from a single student), wide-formatting it will
> create 256 columns for each subject (attached)! Is using brm() etc.
> really practical here?
> 
> Second, nlme::lme() allows modeling the residuals. If I model the
> residuals from my responses (CAL_type) on their current scale (some
> proportions, some normal ones) with lme() and find a relatively good
> fitting model, would that be a second best solution?
> 
> Thanks,
> Tim M
> 
> LONG-FORMAT="
> Class Person Task_order Task_type Time Score  CAL_type  Mot_ex Mot_inr
> Mot_ide Mot_int Mot_amot   Eng_leng_txt Eng_time_on_tsk
> 1     1      S-C        simple    1    5      com_mult  4      2
> 3       3       1          300          20
> 1     1      S-C        simple    1   .3      com_dc/t  4      2
> 3       3       1          300          20
> 1     1      S-C        simple    1    2      com_cn/t  4      2
> 3       3       1          300          20
> 1     1      S-C        simple    1    3      com_cn/c  4      2
> 3       3       1          300          20
> 1     1      S-C        simple    1   .4      ac        4      2
> 3       3       1          300          20
> 1     1      S-C        simple    1    1      lex_vo    4      2
> 3       3       1          300          20
> 1     1      S-C        simple    1    5      lex_fr    4      2
> 3       3       1          300          20
> 
> 1     1      S-C        complex   2    2      com_mult  3      4
> 2       1       2          200          25
> 1     1      S-C        complex   2   .3      com_dc/t  3      4
> 2       1       2          200          25
> 1     1      S-C        complex   2    4      com_cn/t  3      4
> 2       1       2          200          25
> 1     1      S-C        complex   2    3      com_cn/c  3      4
> 2       1       2          200          25
> 1     1      S-C        complex   2   .4      ac        3      4
> 2       1       2          200          25
> 1     1      S-C        complex   2    4      lex_vo    3      4
> 2       1       2          200          25
> 1     1      S-C        complex   2    5      lex_fr    3      4
> 2       1       2          200          25
> 
> 1     1      S-C        simple    3    4      com_mult  5      2
> 3       4       3          100          10
> 1     1      S-C        simple    3   .2      com_dc/t  5      2
> 3       4       3          100          10
> 1     1      S-C        simple    3    3      com_cn/t  5      2
> 3       4       3          100          10
> 1     1      S-C        simple    3    3      com_cn/c  5      2
> 3       4       3          100          10
> 1     1      S-C        simple    3   .6      ac        5      2
> 3       4       3          100          10
> 1     1      S-C        simple    3    6      lex_vo    5      2
> 3       4       3          100          10
> 1     1      S-C        simple    3    6      lex_fr    5      2
> 3       4       3          100          10
> 
> 1     1      S-C        complex   4    1      com_mult  1      3
> 2       5       4          400          35
> 1     1      S-C        complex   4   .1      com_dc/t  1      3
> 2       5       4          400          35
> 1     1      S-C        complex   4    1      com_cn/t  1      3
> 2       5       4          400          35
> 1     1      S-C        complex   4    3      com_cn/c  1      3
> 2       5       4          400          35
> 1     1      S-C        complex   4   .3      ac        1      3
> 2       5       4          400          35
> 1     1      S-C        complex   4    5      lex_vo    1      3
> 2       5       4          400          35
> 1     1      S-C        complex   4    5      lex_fr    1      3
> 2       5       4          400          35
> "
> 
> On Wed, Dec 14, 2022 at 12:12 PM Ben Bolker <bbolker at gmail.com> wrote:
>>
>>     I didn't realize that brms does multi-type models, but apparently it
>> does:
>>
>> https://cran.r-project.org/web/packages/brms/vignettes/brms_multivariate.html
>>
>>     ... so yes, I would go for brms in this case.
>>
>>     cheers
>>      Ben
>>
>>
>> On 2022-12-14 12:09 p.m., Timothy MacKenzie wrote:
>>> Dear Ben,
>>>
>>> Thank you for the hint. Regarding MCMCglmm, I couldn't find "beta" in
>>> the family of allowable distributions in the package. Did you have a
>>> specific set of distribution families in mind to handle normal and
>>> beta responses simultaneously?
>>>
>>> Also, I noticed the brms package apparently can handle different
>>> response distributions, is there a reason, in your expert opinion, to
>>> opt for MCMCglmm?
>>>
>>> Many thanks,
>>> Tim M
>>>
>>> On Tue, Dec 13, 2022 at 9:28 PM Ben Bolker <bbolker at gmail.com> wrote:
>>>>
>>>> MCMCglmm can handle this case
>>>>
>>>> On Tue, Dec 13, 2022, 10:14 PM Timothy MacKenzie <fswfswt at gmail.com> wrote:
>>>>>
>>>>> Hello Colleagues,
>>>>>
>>>>> I have a multivariate data structure (below) where the dependent
>>>>> variables (DV) seem to have different distributions.
>>>>>
>>>>> For instance, *ac* is measured in proportions and perhaps
>>>>> beta-distributed, but *fl* and *le* may be normally distributed.
>>>>>
>>>>> Would it make methodological sense to fit such DVs in a multivariate
>>>>> mixed model given that they are theoretically related but practically
>>>>> measured on different scales?
>>>>>
>>>>> Any resources to provide mixed model strategies in such a situation?
>>>>>
>>>>> Many thanks for your help,
>>>>> Tim M
>>>>>
>>>>> Score ~ DV + (1 | subj_id) ## Would this make sense?
>>>>>
>>>>> # Data structure:
>>>>> subj_id  DV     Score
>>>>> 1            ac      .5
>>>>> 1            fl        23.1
>>>>> 1            le       1.4
>>>>> 2            ac      .7
>>>>> 2            fl        19.6
>>>>> 2            le       2.1
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>> --
>> Dr. Benjamin Bolker
>> Professor, Mathematics & Statistics and Biology, McMaster University
>> Director, School of Computational Science and Engineering
>> (Acting) Graduate chair, Mathematics & Statistics
>>   > E-mail is sent at my convenience; I don't expect replies outside of
>> working hours.

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics
 > E-mail is sent at my convenience; I don't expect replies outside of 
working hours.


From bbo|ker @end|ng |rom gm@||@com  Sat Dec 17 01:26:13 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Fri, 16 Dec 2022 19:26:13 -0500
Subject: [R-sig-ME] Modelling football matches
In-Reply-To: <CAOYO_yDZFvG9X8Nh6NvqWfStcwuDEfjNZroxxT-GCOZjmQ38_A@mail.gmail.com>
References: <CAOYO_yB63WA-=XLsEiO2K+ty4aBX4oope_uucmUvaa_ZNecTSQ@mail.gmail.com>
 <CAOYO_yDZFvG9X8Nh6NvqWfStcwuDEfjNZroxxT-GCOZjmQ38_A@mail.gmail.com>
Message-ID: <bbaa9343-cffa-f1f7-536d-ea48266328a4@gmail.com>



On 2022-12-15 3:02 p.m., Jorge Teixeira wrote:
> Thank you, Ben.
> 
> Yes, indeed there are many more things that could be added - I was 
> trying to discuss a more fundamental structure.
> 
> Game_part was related to the fact that each game has part 1 and part 2.
> 
> 1) I agree it makes sense to have game_part as fixed effect too, with 
> this result
> 
> lmer(distance ~ stage + *game_part* + (1|player) + (1|game/game_part), 
> data=my_data)
> 
> 2) As for the random slopes, my question was that I believe the 
> variation by game and game_part might be different across players. Can 
> random slopes account for that?

   That's a little challenging with 'typical' mixed model machinery. 
Models where both the mean (location) and variance (scale) vary 
according to covariates or groups are called 'location-scale' models. 
There is a category in the mixed models task view 
<https://cran.r-project.org/web/views/MixedModels.html> that covers 
this, but I'm not sure whether the scale is allowed to vary as a *random 
effect* -- it certainly isn't in glmmTMB.
> 
> 3) For outcomes such as relative average heart rate, that are bounded by 
> 100%, do you recommend a specific family of models?

   Provided it doesn't go to exactly 0 or 100%, beta is the natural choice.

> 
> Thanks once again.
> 
> Date: Thu, 15 Dec 2022 12:12:16 -0500
> From: Ben Bolker <bbolker at gmail.com <mailto:bbolker at gmail.com>>
> To: r-sig-mixed-models at r-project.org 
> <mailto:r-sig-mixed-models at r-project.org>
> Subject: Re: [R-sig-ME] Modelling football matches
> Message-ID: <ca745d94-ac01-3422-cd66-0d85058d8936 at gmail.com 
> <mailto:ca745d94-ac01-3422-cd66-0d85058d8936 at gmail.com>>
> Content-Type: text/plain; charset="utf-8"; Format="flowed"
> 
> 
>  ? ? For a positive-valued variable like distance you might want to
> consider a log-linear model (lmer(log(distance) ~ ...) or a Gamma GLMM
> (glmer(distance? ~ ..., family = Gamma(link="log"))
> 
>  ? ?I believe the full model here would use random slopes ('slopes' in
> the broad sense since stage is a categorical variable) of stage
> (stage|player) - (stage|game) won't work because each game is only one
> stage.
> 
>  ? ?I'm not sure about the definition of 'game_part', but you might want
> to add a *fixed* effect of game_part as well as the 'game_part within
> game' nested random effect.
> 
>  ? ?There's probably a huge amount of covariate information you could add
> (e.g. player's position, player's age), probably other stuff too (random
> effect of team?)
> 
> Jorge Teixeira <jorgemmtteixeira at gmail.com 
> <mailto:jorgemmtteixeira at gmail.com>> escreveu no dia quinta, 15/12/2022 
> ?(s) 15:17:
> 
>     Hi.
> 
>     1) Assuming that most are somewhat familiar with football, and that
>     it is world cup time, what do you think of this model to compare
>     differences in distance covered between stages (group stage vs final
>     stage)?
> 
>     lmer(distance ~ stage + (1|player) + (1|game/game_part), data=my_data)
> 
>     2) In theory, which random slopes do you think should be added, if any?
> 
>     Thank you.
> 

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics
 > E-mail is sent at my convenience; I don't expect replies outside of 
working hours.


From |@w|@wt @end|ng |rom gm@||@com  Sat Dec 17 04:15:42 2022
From: |@w|@wt @end|ng |rom gm@||@com (Timothy MacKenzie)
Date: Fri, 16 Dec 2022 21:15:42 -0600
Subject: [R-sig-ME] Multivariate mixed models with different outcome
 distributions
In-Reply-To: <5ed02a51-69fd-3d24-cb9e-d8416ec5da05@gmail.com>
References: <CADreqix_OtcvXPbZXb7-oF_g_opw8QiG1xoPG-QOtq7sTWf_Qw@mail.gmail.com>
 <CABghstT1Duf_YNJhouem6xkEJpoXxVo6qUy91S3_P29d2k8NkQ@mail.gmail.com>
 <CADreqizDcYyGeTUYwFn-yn3me9+6kAtcZ5E+eEtS8ULvHSoT8Q@mail.gmail.com>
 <a31e3dac-2312-fa56-fc70-447569f83346@gmail.com>
 <CADreqizwMdaxV-8BUJ7LwGxEReNCaNjQ9NZ_yDEZTW62g9LTRw@mail.gmail.com>
 <5ed02a51-69fd-3d24-cb9e-d8416ec5da05@gmail.com>
Message-ID: <CADreqiwDiYgVodTEwFdgQTOJ+7ysPiRzeVcFUBaBJ_fEwGa5BA@mail.gmail.com>

Thank you Ben. These are great suggestions. Do you have an example of
use for `rr()` in glmmTMB?

Response to some of your comments:

Is your multivariate response 256-dimensional?

>>>> No, I have 7 response variables (CAL_type). Once wide-formatted, I will overall (responses and predictors) will have 63 columns (256 was the result of a bug in my previous code for wide-formatting).

I'm not quite sure what you intend to do with nlme::lme() here.

>>>> I thought when working with response variables that don't conform to normality (as in proportions), I may end up with non-random patterns in my residuals. I thought using the variance functions available in lme(weight = ...) might be an option to overcome this problem without needing to transform the responses that don't conform to morality.

Thank you again,
Tim M

On Fri, Dec 16, 2022 at 6:25 PM Ben Bolker <bbolker at gmail.com> wrote:
>
>    Is your multivariate response 256-dimensional (i.e. you have 256
> responses from each individual which you want to treat as correlated)?
> That's *very* unlikely to work if you just stick it into a standard
> multivariate framework, as you'll have to estimate a 256?256 covariance
> matrix (almost 33,000 parameters). A factor-analytic or reduced-rank
> model could work (e.g. glmmTMB now allows this -- although it's not well
> tested and definitely not with examples this large!), but doesn't do
> multi-type models).
>
>    I'm not quite sure what you intend to do with nlme::lme() here.
>
>   It may well be that just doing something like an arcsine-sqrt
> transform (or logit, if you prefer and have no exact 0/1 values) for
> your proportion data and then modeling everything as a multi-response
> Gaussian would be good enough ...
>
>
>
> On 2022-12-15 12:20 p.m., Timothy MacKenzie wrote:
> > Dear Ben,
> >
> > Thank you for your confirmation. There are two things that I want to
> > better understand.
> >
> > First, brms::brm() etc. require wide-format data. For my data (below
> > see long-format data from a single student), wide-formatting it will
> > create 256 columns for each subject (attached)! Is using brm() etc.
> > really practical here?
> >
> > Second, nlme::lme() allows modeling the residuals. If I model the
> > residuals from my responses (CAL_type) on their current scale (some
> > proportions, some normal ones) with lme() and find a relatively good
> > fitting model, would that be a second best solution?
> >
> > Thanks,
> > Tim M
> >
> > LONG-FORMAT="
> > Class Person Task_order Task_type Time Score  CAL_type  Mot_ex Mot_inr
> > Mot_ide Mot_int Mot_amot   Eng_leng_txt Eng_time_on_tsk
> > 1     1      S-C        simple    1    5      com_mult  4      2
> > 3       3       1          300          20
> > 1     1      S-C        simple    1   .3      com_dc/t  4      2
> > 3       3       1          300          20
> > 1     1      S-C        simple    1    2      com_cn/t  4      2
> > 3       3       1          300          20
> > 1     1      S-C        simple    1    3      com_cn/c  4      2
> > 3       3       1          300          20
> > 1     1      S-C        simple    1   .4      ac        4      2
> > 3       3       1          300          20
> > 1     1      S-C        simple    1    1      lex_vo    4      2
> > 3       3       1          300          20
> > 1     1      S-C        simple    1    5      lex_fr    4      2
> > 3       3       1          300          20
> >
> > 1     1      S-C        complex   2    2      com_mult  3      4
> > 2       1       2          200          25
> > 1     1      S-C        complex   2   .3      com_dc/t  3      4
> > 2       1       2          200          25
> > 1     1      S-C        complex   2    4      com_cn/t  3      4
> > 2       1       2          200          25
> > 1     1      S-C        complex   2    3      com_cn/c  3      4
> > 2       1       2          200          25
> > 1     1      S-C        complex   2   .4      ac        3      4
> > 2       1       2          200          25
> > 1     1      S-C        complex   2    4      lex_vo    3      4
> > 2       1       2          200          25
> > 1     1      S-C        complex   2    5      lex_fr    3      4
> > 2       1       2          200          25
> >
> > 1     1      S-C        simple    3    4      com_mult  5      2
> > 3       4       3          100          10
> > 1     1      S-C        simple    3   .2      com_dc/t  5      2
> > 3       4       3          100          10
> > 1     1      S-C        simple    3    3      com_cn/t  5      2
> > 3       4       3          100          10
> > 1     1      S-C        simple    3    3      com_cn/c  5      2
> > 3       4       3          100          10
> > 1     1      S-C        simple    3   .6      ac        5      2
> > 3       4       3          100          10
> > 1     1      S-C        simple    3    6      lex_vo    5      2
> > 3       4       3          100          10
> > 1     1      S-C        simple    3    6      lex_fr    5      2
> > 3       4       3          100          10
> >
> > 1     1      S-C        complex   4    1      com_mult  1      3
> > 2       5       4          400          35
> > 1     1      S-C        complex   4   .1      com_dc/t  1      3
> > 2       5       4          400          35
> > 1     1      S-C        complex   4    1      com_cn/t  1      3
> > 2       5       4          400          35
> > 1     1      S-C        complex   4    3      com_cn/c  1      3
> > 2       5       4          400          35
> > 1     1      S-C        complex   4   .3      ac        1      3
> > 2       5       4          400          35
> > 1     1      S-C        complex   4    5      lex_vo    1      3
> > 2       5       4          400          35
> > 1     1      S-C        complex   4    5      lex_fr    1      3
> > 2       5       4          400          35
> > "
> >
> > On Wed, Dec 14, 2022 at 12:12 PM Ben Bolker <bbolker at gmail.com> wrote:
> >>
> >>     I didn't realize that brms does multi-type models, but apparently it
> >> does:
> >>
> >> https://cran.r-project.org/web/packages/brms/vignettes/brms_multivariate.html
> >>
> >>     ... so yes, I would go for brms in this case.
> >>
> >>     cheers
> >>      Ben
> >>
> >>
> >> On 2022-12-14 12:09 p.m., Timothy MacKenzie wrote:
> >>> Dear Ben,
> >>>
> >>> Thank you for the hint. Regarding MCMCglmm, I couldn't find "beta" in
> >>> the family of allowable distributions in the package. Did you have a
> >>> specific set of distribution families in mind to handle normal and
> >>> beta responses simultaneously?
> >>>
> >>> Also, I noticed the brms package apparently can handle different
> >>> response distributions, is there a reason, in your expert opinion, to
> >>> opt for MCMCglmm?
> >>>
> >>> Many thanks,
> >>> Tim M
> >>>
> >>> On Tue, Dec 13, 2022 at 9:28 PM Ben Bolker <bbolker at gmail.com> wrote:
> >>>>
> >>>> MCMCglmm can handle this case
> >>>>
> >>>> On Tue, Dec 13, 2022, 10:14 PM Timothy MacKenzie <fswfswt at gmail.com> wrote:
> >>>>>
> >>>>> Hello Colleagues,
> >>>>>
> >>>>> I have a multivariate data structure (below) where the dependent
> >>>>> variables (DV) seem to have different distributions.
> >>>>>
> >>>>> For instance, *ac* is measured in proportions and perhaps
> >>>>> beta-distributed, but *fl* and *le* may be normally distributed.
> >>>>>
> >>>>> Would it make methodological sense to fit such DVs in a multivariate
> >>>>> mixed model given that they are theoretically related but practically
> >>>>> measured on different scales?
> >>>>>
> >>>>> Any resources to provide mixed model strategies in such a situation?
> >>>>>
> >>>>> Many thanks for your help,
> >>>>> Tim M
> >>>>>
> >>>>> Score ~ DV + (1 | subj_id) ## Would this make sense?
> >>>>>
> >>>>> # Data structure:
> >>>>> subj_id  DV     Score
> >>>>> 1            ac      .5
> >>>>> 1            fl        23.1
> >>>>> 1            le       1.4
> >>>>> 2            ac      .7
> >>>>> 2            fl        19.6
> >>>>> 2            le       2.1
> >>>>>
> >>>>> _______________________________________________
> >>>>> R-sig-mixed-models at r-project.org mailing list
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>
> >> --
> >> Dr. Benjamin Bolker
> >> Professor, Mathematics & Statistics and Biology, McMaster University
> >> Director, School of Computational Science and Engineering
> >> (Acting) Graduate chair, Mathematics & Statistics
> >>   > E-mail is sent at my convenience; I don't expect replies outside of
> >> working hours.
>
> --
> Dr. Benjamin Bolker
> Professor, Mathematics & Statistics and Biology, McMaster University
> Director, School of Computational Science and Engineering
> (Acting) Graduate chair, Mathematics & Statistics
>  > E-mail is sent at my convenience; I don't expect replies outside of
> working hours.


From dom|n|k-z|@j@ @end|ng |rom web@de  Mon Dec 19 11:50:39 2022
From: dom|n|k-z|@j@ @end|ng |rom web@de (Ziaja, Dominik)
Date: Mon, 19 Dec 2022 11:50:39 +0100
Subject: [R-sig-ME] Calculating proportion variance explained by random
 effects in zi-component
Message-ID: <8b57a741-ae00-f60c-11ae-269ab515933a@web.de>

Dear GLMM-modelers,

I would like to report the proportion of variance each random effect
explains in addition to the fixed effects. For this, I use the
"get_variance" function from the insight package for the conditional
components. For the data I want to model I needed to implement a
zero-inflation model (not a hurdle model) However, I can't really find a
downstream wrapper/implementation to use the get_variance function onto
the zero-inflation component (as is the case for e.g. emmeans, Anova).

Using the VarCorr function/the summary output I'm able to get the
variances of the individual random effects. However, I don't exactly
know how to calculate the other sums of variances reported with the
insight package (var.fixed, var.residual, var.distribution,
var.dispersion).

I was thinking of 3 different ways to achieve this goal and was
wondering whether someone might have an idea/a hint/a direction.

1. Is there actually an implemented possibility to apply get_variance()
to the zi-component which I simply overlooked?

2. Is there a way to get the information which measurements were used
for the zero-inflation-component of the zero-inflation model so I could
then calculate a binomial model on exactly these measurements. My hope
would be to then apply get_variance() onto this model.

3. How would I go about to calculate the missing variances manually by
myself (var.fixed, var.residual, var.distribution, var.dispersion)?

I also made a stack-overflow post about this some time ago, I hope it is
worded correctly and clearly enough.
Link:
https://stackoverflow.com/questions/74689961/calculate-proportion-of-random-effect-variance-from-zero-inflation-component-of

I'm happy for any answers, hints or directions.

sincerely,
Dominik


From @r|ve@ @end|ng |rom w|@c@edu  Mon Dec 19 15:01:16 2022
From: @r|ve@ @end|ng |rom w|@c@edu (Anthony R. Ives)
Date: Mon, 19 Dec 2022 14:01:16 +0000
Subject: [R-sig-ME] Calculating proportion variance explained by random
 effects in zi-component
In-Reply-To: <8b57a741-ae00-f60c-11ae-269ab515933a@web.de>
References: <8b57a741-ae00-f60c-11ae-269ab515933a@web.de>
Message-ID: <SJ0PR06MB83274257DCD06B3226B31DCDB4E59@SJ0PR06MB8327.namprd06.prod.outlook.com>

Dominik,

Are you sure you want to report the proportion of variance each random effect explains? I know this is very common, but I think it is more informative to report a partial R2 for each the random effects. A partial R2 would give heuristically the explanatory power of the model lost when a random effect is removed. In your particular case, this will be much easier to compute. Partial R2s based on likelihoods only require the logLik of the full and reduced models. You could just use the code from the package rr2 (which might need a few changes for glmmTMB()).

Cheers, Tony

From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on behalf of Ziaja, Dominik <dominik-ziaja at web.de>
Date: Monday, December 19, 2022 at 7:39 AM
To: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: [R-sig-ME] Calculating proportion variance explained by random effects in zi-component
Dear GLMM-modelers,

I would like to report the proportion of variance each random effect
explains in addition to the fixed effects. For this, I use the
"get_variance" function from the insight package for the conditional
components. For the data I want to model I needed to implement a
zero-inflation model (not a hurdle model) However, I can't really find a
downstream wrapper/implementation to use the get_variance function onto
the zero-inflation component (as is the case for e.g. emmeans, Anova).

Using the VarCorr function/the summary output I'm able to get the
variances of the individual random effects. However, I don't exactly
know how to calculate the other sums of variances reported with the
insight package (var.fixed, var.residual, var.distribution,
var.dispersion).

I was thinking of 3 different ways to achieve this goal and was
wondering whether someone might have an idea/a hint/a direction.

1. Is there actually an implemented possibility to apply get_variance()
to the zi-component which I simply overlooked?

2. Is there a way to get the information which measurements were used
for the zero-inflation-component of the zero-inflation model so I could
then calculate a binomial model on exactly these measurements. My hope
would be to then apply get_variance() onto this model.

3. How would I go about to calculate the missing variances manually by
myself (var.fixed, var.residual, var.distribution, var.dispersion)?

I also made a stack-overflow post about this some time ago, I hope it is
worded correctly and clearly enough.
Link:
https://stackoverflow.com/questions/74689961/calculate-proportion-of-random-effect-variance-from-zero-inflation-component-of

I'm happy for any answers, hints or directions.

sincerely,
Dominik

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From benpe|zer @end|ng |rom gm@||@com  Wed Dec 28 15:13:26 2022
From: benpe|zer @end|ng |rom gm@||@com (ben pelzer)
Date: Wed, 28 Dec 2022 15:13:26 +0100
Subject: [R-sig-ME] subject-specific interpetation two random effects
Message-ID: <CAFgPNS-cZLVJ4yM1R2XwCYUNeiMpcx2xmgUiECqknyQcdTAZbA@mail.gmail.com>

Dear list,

Sorry for cross-posting this question; I got no response on Cross
Validated. Hope to get one here.


I have two groups of pupils, males and females in a number of schools. So,
two-level data, pupils nested in schools. For each gender group, a logistic
model was estimated with a school-level predictor "type" of school. Next, a
"complete" model was estimated for both gender groups simultaneously. Here
is the syntax I used:


library(GLMMadaptive)


ma  <- mixed_model(y ~ 1+type, random=~ 1|school, family=binomial,
data=maledata)



mb  <- mixed_model(y ~ 1+type, random=~ 1|school, family=binomial,
data=femaledata)



mab <- mixed_model(y ~ 0+male+female+type:male+type:female,

                   random=~ 0+male+female|school, family=binomial,
data=completedata)


The final model mab uses interaction terms and is specified without
intercepts, so that its estimates can directly be compared with those of
the separate group models ma and mb.


I expected that the final "complete" model mab would give the same
estimates for the effects of "type" as the separate models do. More in
particular, that the regression coefficients of type:male and type:female
in model mab would be equal to the coefficients of "type" in ma and mb,
respectively.


However, this is not (exactly) the case: the coefficients are relatively
close, but clearly not equal. I first thought that these dissimilarities
might be due to the number of quadrature points used by mixed_model. Hence,
I chose larger values for nAGQ but this did not help: the dissimilarities
persist, and all estimates hardly change. Also, no convergence problems
exist, everything seems all right.


Now my guess is that these differences are caused by the fact that the
regression coefficients are subject-specific. That is, for model ma, the
effect of "type" expresses the influence of schooltype for two schools with
the same random school effect across male pupils. In contrast, the effect
of "type" in model mab expresses the effect of schooltype for two schools
with the same random school effect across male pupils and also across
female pupils. So in mab, there are two random school effects, one for
males and the other for females, which BOTH have to be equal. It toke me
quite a while to realise this, and still I'm not completely sure. I would
really appreciate it if someone could confirm my suspicion. Thanks!!!


Ben.

	[[alternative HTML version deleted]]


From zp@|mp@o @end|ng |rom gm@||@com  Wed Dec 28 23:51:43 2022
From: zp@|mp@o @end|ng |rom gm@||@com (Zach Simpson)
Date: Wed, 28 Dec 2022 16:51:43 -0600
Subject: [R-sig-ME] 
 interpretation of random effects variance from glmer()
 with the Gamma family
Message-ID: <CAJByKzo=sA8hEWyZ5RZGw9K_VOx_FXV1A8P37W7mwt1rsmwrHw@mail.gmail.com>

Hi Jun,

In the summary of an lme4::merMod object, the standard deviation given
for the 'Residual' is the same as lme4::sigma(), which for
non-Gaussian models is "the square root of the residual deviance per
degree of freedom". 'Variance' is the square of that value.

In this R-sig-ME thread you shared with me, Ben Bolker points out how
things relate for the Gamma family:
https://stat.ethz.ch/pipermail/r-sig-mixed-models/2017q4/026168.html

"In the classic GLM sense `phi=sigma^2=1/shape` is the dispersion
parameter, because we define `variance= V(mu) * phi`,  `V=mu^2`
(`(shape*scale)^2/shape = shape*scale^2 - variance`)."

where shape and scale are the parameters for the Gamma distribution
(in shape-scale form).
(I believe there was a tiny typo there, Ben originally had `variance =
V(mu) / phi` )

Example:

## sim data for mixed-effects case
set.seed(321)
fake_data_mix <- tibble(
  rand_int = rnorm(50, 0, sd = 1)
) %>%
  mutate(
    clust_id = row_number(),
    log_mu = map(rand_int, ~rnorm(n = 30, mean = 5 + .x, sd = 0.2))
  ) %>%
  unnest(cols = log_mu) %>%
  mutate(y = rgamma(n = n(), shape = exp(log_mu), scale = 1))

fit_2_glmer <- glmer(y ~ (1|clust_id), family = Gamma(link = 'log'),
                     data = fake_data_mix)
summary(fit_2_glmer)
# Residual variance: 0.06871; std dev: 0.2621
sigma(fit_2_glmer) ^ 2 # 0.06871
sigma(fit_2_glmer) # 0.2621

Unlike for glm, the dispersion parameter (phi) isn't reported anywhere
by glmer from what I can see...
For the mixed model context, Simon Wood in his 2017 GAM book (Ch. 3;
p. 150 in the 2nd Ed.) gives an REML estimate of phi for GLMMs that
involves the effective degrees of freedom (alas this is a bit over my
head).

Cheers,
Zach Simpson

On 12/5/22 10:50, Jun Yan wrote:
>
> Hi All,
>
> The summary of an lme4::glmer() fit with the Gamma family has a section on random effects. For example:
>
> Random effects:
>  Groups   Name        Variance Std.Dev.
>  Year     (Intercept) 0.002536 0.05036
>  Residual             0.011198 0.10582
> Number of obs: 91, groups:  Year, 12
>
> The function call that I used was
>
> fit2 <- glmer(y ~ (1 | Year), data = finals, family = Gamma(link = "log"))
>
> I understand that the normal random effect of "Year" has variance 0.00256. My question is, doe the variance for the "Residual", 0.011198, mean the variance of the Gamma family or the dispersion parameter (which is the reciprocal of the shape of the Gamma distribution, the same as what is reported from a glm fit)?
>
> Any tips are appreciated.


From d@r|zopou|o@ @end|ng |rom er@@mu@mc@n|  Thu Dec 29 07:37:48 2022
From: d@r|zopou|o@ @end|ng |rom er@@mu@mc@n| (Dimitris Rizopoulos)
Date: Thu, 29 Dec 2022 06:37:48 +0000
Subject: [R-sig-ME] subject-specific interpetation two random effects
In-Reply-To: <CAFgPNS-cZLVJ4yM1R2XwCYUNeiMpcx2xmgUiECqknyQcdTAZbA@mail.gmail.com>
References: <CAFgPNS-cZLVJ4yM1R2XwCYUNeiMpcx2xmgUiECqknyQcdTAZbA@mail.gmail.com>
Message-ID: <PAXPR04MB9277D37C26217AE420FD65D8E8F39@PAXPR04MB9277.eurprd04.prod.outlook.com>

The last model says that the random effects for males and females are correlated-you would need to assume independent random effects.

Best,
Dimitris


??
Dimitris Rizopoulos
Professor of Biostatistics
Erasmus University Medical Center
The Netherlands
________________________________
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on behalf of ben pelzer <benpelzer at gmail.com>
Sent: Wednesday, December 28, 2022 3:13:26 PM
To: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
Subject: [R-sig-ME] subject-specific interpetation two random effects



Waarschuwing: Deze e-mail is afkomstig van buiten de organisatie. Klik niet op links en open geen bijlagen, tenzij u de afzender herkent en weet dat de inhoud veilig is.
Caution: This email originated from outside of the organization. Do not click links or open attachments unless you recognize the sender and know the content is safe.



Dear list,

Sorry for cross-posting this question; I got no response on Cross
Validated. Hope to get one here.


I have two groups of pupils, males and females in a number of schools. So,
two-level data, pupils nested in schools. For each gender group, a logistic
model was estimated with a school-level predictor "type" of school. Next, a
"complete" model was estimated for both gender groups simultaneously. Here
is the syntax I used:


library(GLMMadaptive)


ma  <- mixed_model(y ~ 1+type, random=~ 1|school, family=binomial,
data=maledata)



mb  <- mixed_model(y ~ 1+type, random=~ 1|school, family=binomial,
data=femaledata)



mab <- mixed_model(y ~ 0+male+female+type:male+type:female,

                   random=~ 0+male+female|school, family=binomial,
data=completedata)


The final model mab uses interaction terms and is specified without
intercepts, so that its estimates can directly be compared with those of
the separate group models ma and mb.


I expected that the final "complete" model mab would give the same
estimates for the effects of "type" as the separate models do. More in
particular, that the regression coefficients of type:male and type:female
in model mab would be equal to the coefficients of "type" in ma and mb,
respectively.


However, this is not (exactly) the case: the coefficients are relatively
close, but clearly not equal. I first thought that these dissimilarities
might be due to the number of quadrature points used by mixed_model. Hence,
I chose larger values for nAGQ but this did not help: the dissimilarities
persist, and all estimates hardly change. Also, no convergence problems
exist, everything seems all right.


Now my guess is that these differences are caused by the fact that the
regression coefficients are subject-specific. That is, for model ma, the
effect of "type" expresses the influence of schooltype for two schools with
the same random school effect across male pupils. In contrast, the effect
of "type" in model mab expresses the effect of schooltype for two schools
with the same random school effect across male pupils and also across
female pupils. So in mab, there are two random school effects, one for
males and the other for females, which BOTH have to be equal. It toke me
quite a while to realise this, and still I'm not completely sure. I would
really appreciate it if someone could confirm my suspicion. Thanks!!!


Ben.

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-mixed-models&data=05%7C01%7Cd.rizopoulos%40erasmusmc.nl%7C09a5f446bc1443dd190608dae8ddbf2a%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C0%7C638078336334964127%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=FAr0RrkoPQSniGE4JOjKQzQfjkohGTucLO0VC3R93oE%3D&reserved=0

	[[alternative HTML version deleted]]


From benpe|zer @end|ng |rom gm@||@com  Fri Dec 30 15:38:25 2022
From: benpe|zer @end|ng |rom gm@||@com (ben pelzer)
Date: Fri, 30 Dec 2022 15:38:25 +0100
Subject: [R-sig-ME] subject-specific interpetation two random effects
In-Reply-To: <PAXPR04MB9277D37C26217AE420FD65D8E8F39@PAXPR04MB9277.eurprd04.prod.outlook.com>
References: <CAFgPNS-cZLVJ4yM1R2XwCYUNeiMpcx2xmgUiECqknyQcdTAZbA@mail.gmail.com>
 <PAXPR04MB9277D37C26217AE420FD65D8E8F39@PAXPR04MB9277.eurprd04.prod.outlook.com>
Message-ID: <CAFgPNS8Dt4V8ECBbj5m8xqCvTh+NqQ34jNvOBL+Z+qUycNRgDA@mail.gmail.com>

Dear Dimitris,

Thanks for your response, that was really helpful and the estimates are now
as good as equal!

I thought the difference was caused by the subject-specific interpretation,
which for the interaction model "mab" is different than for the two
separate models, if I'm right ...

After reading your comment, I also tried linear models (using lme) for
males/females separately and for the two groups combined using interaction
(for linear models the subject-specific interpretation would not make a
difference). Again it appeared that allowing for correlation of the
intercepts for males and females leads to slightly different estimates of
the fixed effects, whereas no correlation leads to exactly the same fixed
effects as in the separate analyses.

Thanks for your help, and, of course, a happy and healthy 2023,

Ben.


On Thu, 29 Dec 2022 at 07:37, Dimitris Rizopoulos <d.rizopoulos at erasmusmc.nl>
wrote:

> The last model says that the random effects for males and females are
> correlated-you would need to assume independent random effects.
>
> Best,
> Dimitris
>
>
> ??
> Dimitris Rizopoulos
> Professor of Biostatistics
> Erasmus University Medical Center
> The Netherlands
> ------------------------------
> *From:* R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on
> behalf of ben pelzer <benpelzer at gmail.com>
> *Sent:* Wednesday, December 28, 2022 3:13:26 PM
> *To:* r-sig-mixed-models <r-sig-mixed-models at r-project.org>
> *Subject:* [R-sig-ME] subject-specific interpetation two random effects
>
>
>
> Waarschuwing: Deze e-mail is afkomstig van buiten de organisatie. Klik
> niet op links en open geen bijlagen, tenzij u de afzender herkent en weet
> dat de inhoud veilig is.
> Caution: This email originated from outside of the organization. Do not
> click links or open attachments unless you recognize the sender and know
> the content is safe.
>
>
>
> Dear list,
>
> Sorry for cross-posting this question; I got no response on Cross
> Validated. Hope to get one here.
>
>
> I have two groups of pupils, males and females in a number of schools. So,
> two-level data, pupils nested in schools. For each gender group, a logistic
> model was estimated with a school-level predictor "type" of school. Next, a
> "complete" model was estimated for both gender groups simultaneously. Here
> is the syntax I used:
>
>
> library(GLMMadaptive)
>
>
> ma  <- mixed_model(y ~ 1+type, random=~ 1|school, family=binomial,
> data=maledata)
>
>
>
> mb  <- mixed_model(y ~ 1+type, random=~ 1|school, family=binomial,
> data=femaledata)
>
>
>
> mab <- mixed_model(y ~ 0+male+female+type:male+type:female,
>
>                    random=~ 0+male+female|school, family=binomial,
> data=completedata)
>
>
> The final model mab uses interaction terms and is specified without
> intercepts, so that its estimates can directly be compared with those of
> the separate group models ma and mb.
>
>
> I expected that the final "complete" model mab would give the same
> estimates for the effects of "type" as the separate models do. More in
> particular, that the regression coefficients of type:male and type:female
> in model mab would be equal to the coefficients of "type" in ma and mb,
> respectively.
>
>
> However, this is not (exactly) the case: the coefficients are relatively
> close, but clearly not equal. I first thought that these dissimilarities
> might be due to the number of quadrature points used by mixed_model. Hence,
> I chose larger values for nAGQ but this did not help: the dissimilarities
> persist, and all estimates hardly change. Also, no convergence problems
> exist, everything seems all right.
>
>
> Now my guess is that these differences are caused by the fact that the
> regression coefficients are subject-specific. That is, for model ma, the
> effect of "type" expresses the influence of schooltype for two schools with
> the same random school effect across male pupils. In contrast, the effect
> of "type" in model mab expresses the effect of schooltype for two schools
> with the same random school effect across male pupils and also across
> female pupils. So in mab, there are two random school effects, one for
> males and the other for females, which BOTH have to be equal. It toke me
> quite a while to realise this, and still I'm not completely sure. I would
> really appreciate it if someone could confirm my suspicion. Thanks!!!
>
>
> Ben.
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
>
> https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-mixed-models&data=05%7C01%7Cd.rizopoulos%40erasmusmc.nl%7C09a5f446bc1443dd190608dae8ddbf2a%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C0%7C638078336334964127%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=FAr0RrkoPQSniGE4JOjKQzQfjkohGTucLO0VC3R93oE%3D&reserved=0
>

	[[alternative HTML version deleted]]


From |@w|@wt @end|ng |rom gm@||@com  Fri Dec 30 21:30:41 2022
From: |@w|@wt @end|ng |rom gm@||@com (Timothy MacKenzie)
Date: Fri, 30 Dec 2022 14:30:41 -0600
Subject: [R-sig-ME] separate ar1 structures for multiple outcomes in a
 single model
Message-ID: <CADreqix0-e6SDH8Ly9DqORFbmgOAo228cOECfnHnV3Bfbw0g6A@mail.gmail.com>

Dear Colleagues,

I was wondering if there is a way to fit "ar1" random structures
separately for multiple DVs in a single model?

I made the following two attempts using glmmTMB without success:

1- glmmTMB(y ~ time*DVs + ar1(0 + interaction(time,DVs) | id),
             data = dat)

2- glmmTMB(y ~ time*DVs + diag(0 + DVs | id) +
              ar1(0 + interaction(time,DVs) | id),  data = dat)

where "time" and "DVs" are factors with 4 and 5 levels, respectively.

Thanks,
Tim M


