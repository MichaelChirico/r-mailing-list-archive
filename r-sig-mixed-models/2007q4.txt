From dieter.menne at menne-biomed.de  Mon Oct  1 11:28:19 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 1 Oct 2007 09:28:19 +0000 (UTC)
Subject: [R-sig-ME] Gastric emptying data: nlmer vs. nlme
Message-ID: <loom.20071001T092046-562@post.gmane.org>

Dear Group,

I have tried to redo published nlme-fits of gastric emptying data recorded by
MRI with nlmer. nlme needs time, but give reasonable results, informing me about
the known correlation between two nonlinear parameters, while nlmer gave up
after two iterations without telling me why.

I hope I got the translation to nlmer syntax correctly. What's wrong?

Dieter Menne


----------------------------------------------------
# Gastric Emptying data from
# Goetze et. al (University Hospital of Z?rich)
# http://ajpgi.physiology.org/cgi/content/abstract/00498.2005v1
# Demo data set from http://www.menne-biomed.de/gastempt
# R-Version: 2.5.1, i386, mingw32
# lme4_0.99875-8.zip

useNlme = TRUE

# Get data
ge = read.table("http://www.menne-biomed.de/gastempt/gastempt.csv",
  sep=",",header=TRUE)
#ge = read.table("gastempt.csv",sep=",",header=TRUE)
ge$subj = as.factor(ge$subj)
ge$subjtreat = as.factor(paste(ge$subj,ge$treat,sep="."))
ge$t = as.double(ge$t)


EmptInit= function(mCall,LHS,data){ # dummy, not explicitely used
  stop("Should not be called")
}

# Standard LinExp model for gastric emptying

SSEmptLinExp=selfStart(~v0*(1+kappa*t/tempt)*exp(-t/tempt),
  initial=EmptInit, parameters= c("v0","kappa","tempt"))

# nlme final is 643,1.4,74
start = list(fixed=c(v0=600,kappa=1.0,tempt=70))

if (useNlme) {
  library(nlme)
  contr = nlmeControl(pnlsTol=0.3,pnlsMaxIter=200,msMaxIter=200)
  ge0.nlme=nlme(v~SSEmptLinExp(t,v0,kappa,tempt),
    fixed =list(v0+kappa+tempt~1),
    groups = ~subjtreat,
    control=contr, start=start,  data=ge, verbose=F
    )
  summary(ge0.nlme)
  # Note correlation between kappa and tempt
  ge0A.nlme = update(ge0.nlme,random=v0+kappa~1)
  summary(ge0A.nlme)
  anova(ge0A.nlme,ge0.nlme)
}

if (!useNlme) {
  library(lme4) # lme4_0.99875-8.zip
  # This stops after two iterations without telling me details
  ge0.nlmer = nlmer(
    v~SSEmptLinExp(t,v0,kappa,tempt)~(v0+kappa+tempt|subjtreat),
    data=ge,  start=start,verbose=T)
  show(ge0.nlmer)
  # This is close to the result of ge0A.nlme)
  ge0A.nlmer = nlmer(
    v~SSEmptLinExp(t,v0,kappa,tempt)~(v0+kappa|subjtreat),
    data=ge,  start=start,verbose=T)
  show(ge0.nlmer)
}



From kyler at mail.smu.edu  Mon Oct  1 20:57:24 2007
From: kyler at mail.smu.edu (Roberts, Kyle)
Date: Mon, 1 Oct 2007 13:57:24 -0500
Subject: [R-sig-ME] Repeated Measures Problem
Message-ID: <F8AF6BF6CD1CC040AF35B0C2D1680BBB03A584DF@s31xe7.systems.smu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20071001/ae451db1/attachment.pl>

From gangchen at mail.nih.gov  Tue Oct  2 20:27:54 2007
From: gangchen at mail.nih.gov (Gang Chen)
Date: Tue, 2 Oct 2007 14:27:54 -0400
Subject: [R-sig-ME] Development version of lme4
In-Reply-To: <40e66e0b0709051211y22c7e896g94ef1c38200a50cb@mail.gmail.com>
References: <3002B649-86D2-42DA-981E-9639CAAF8E37@mail.nih.gov>
	<40e66e0b0709051211y22c7e896g94ef1c38200a50cb@mail.gmail.com>
Message-ID: <0AA34D9D-827B-4CA2-B7B4-1DD18577D271@mail.nih.gov>

Dr. Bates,

I have successfully compiled the development version of lme4 package  
on my computer, but have encountered some problem while trying out  
the approach you suggested a while ago. The line I'm having trouble  
with is

> fm at ZtXy <- fm at Zt %*% Xy

It gives me the following error message:

Error in checkSlotAssignment(object, name, value) :
         assignment of an object of class "dgeMatrix" is not valid  
for slot "ZtXy" in an object of class "lmer"; is(value, "matrix") is  
not TRUE

The previous matrix stored in

> fm at ZtXy

is

 > fm at ZtXy
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
[1,]   12    6    6    4    4    3    2    2    2     2     1     1
[2,]   12    6    6    4    4    3    2    2    2     2     1     1
[3,]   12    6    6    4    4    3    2    2    2     2     1     1
[4,]   12    6    6    4    4    3    2    2    2     2     1     1
[5,]   12    6    6    4    4    3    2    2    2     2     1     1
[6,]   12    6    6    4    4    3    2    2    2     2     1     1
[7,]   12    6    6    4    4    3    2    2    2     2     1     1
[8,]   12    6    6    4    4    3    2    2    2     2     1     1
             [,13]
[1,]  0.001770365
[2,]  0.017938064
[3,]  0.003173793
[4,]  0.008991505
[5,] -0.007523851
[6,]  0.040761106
[7,]  0.013619571
[8,]  0.010122583

while the new matrix at the right-hand side has the same size:

> fm at Zt %*% Xy

8 x 13 Matrix of class "dgeMatrix"
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
[1,]   12    6    6    4    4    3    2    2    2     2     1     1
[2,]   12    6    6    4    4    3    2    2    2     2     1     1
[3,]   12    6    6    4    4    3    2    2    2     2     1     1
[4,]   12    6    6    4    4    3    2    2    2     2     1     1
[5,]   12    6    6    4    4    3    2    2    2     2     1     1
[6,]   12    6    6    4    4    3    2    2    2     2     1     1
[7,]   12    6    6    4    4    3    2    2    2     2     1     1
[8,]   12    6    6    4    4    3    2    2    2     2     1     1
              [,13]
[1,]  0.0122570704
[2,]  0.0201602195
[3,]  0.0064441410
[4,]  0.0001785527
[5,] -0.0021326855
[6,]  0.0596808494
[7,]  0.0128992660
[8,]  0.0108945209

Did I do something inappropriate?

 > sessionInfo()
R version 2.5.1 (2007-06-27)
i386-apple-darwin8.9.1

locale:
C

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"   
"methods"
[7] "base"

other attached packages:
         lme4       Matrix      lattice
"0.999375-0" "0.999375-2"    "0.15-11"

Thanks,
Gang


> If you look at the lmer function in the development version of the  
> lme4 package (currently at
> https://svn.r-project.org/R-packages/branches/gappy-lmer, soon to be
> at http://r-forge.r-project.org/projects/lme4 for some value of
> "soon") you will see that it follows the equations in my useR
> presentation fairly closely.  The Xy array is n by (p + 1) with X in
> the first p columns and y in the p + 1st column.  The object of class
> "lmer" has slots named y, Zt (Z-transpose), ZtXy (Zt %*% Xy), and
> XytXy (crossprod(Xy)). After fitting the model to the first simulated
> response, producing the object 'fm',  the only operations needed to
> update the model are
>
>  fm at y <- newy
>  Xy <- cbind(fm at X, fm at y)
>  fm at ZtXy <- fm at Zt %*% Xy
>  fm at XytXy <- crossprod(Xy)
>  lme4:::mer_finalize(fm, verbose)
>
> where 'verbose' is a logical scalar indicating if you want verbose
> output during the optimization phase.  Once you get things working on
> a small example you would probably want to turn that off.
>
> Please note that this code applies to the development version of the
> lme4 package.



From njs at pobox.com  Wed Oct  3 08:40:40 2007
From: njs at pobox.com (Nathaniel Smith)
Date: Tue, 2 Oct 2007 23:40:40 -0700
Subject: [R-sig-ME] [PATCH] lmList does not work for generalized lm's
Message-ID: <20071003064040.GA22506@frances.vorpus.org>

The lmList function from current (0.99875-8, and svn head) lme4 gives
an error if one attempts to use it for a glm, e.g.:

> lmList(TIME ~ LOGPROB3 | SUBJ, ff, inverse.gaussian(link="identity"))
Error in FUN(X[[1L]], ...) : 
  unused argument(s) (family = list(family = "inverse.gaussian", link = "identity", linkfun = function (mu) 

The problem is in line 53 of lmList.R -- a simple patch against
current svn that fixes this problem is attached.

-- Nathaniel

-- 
Electrons find their paths in subtle ways.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: lmList-glm-fix.patch
Type: text/x-diff
Size: 534 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20071002/5f637270/attachment.bin>

From njs at pobox.com  Wed Oct  3 08:50:34 2007
From: njs at pobox.com (Nathaniel Smith)
Date: Tue, 2 Oct 2007 23:50:34 -0700
Subject: [R-sig-ME] [PATCH] lmList does not work for generalized lm's
In-Reply-To: <20071003064040.GA22506@frances.vorpus.org>
References: <20071003064040.GA22506@frances.vorpus.org>
Message-ID: <20071003065034.GB22506@frances.vorpus.org>

On Tue, Oct 02, 2007 at 11:40:40PM -0700, Nathaniel Smith wrote:
> The problem is in line 53 of lmList.R -- a simple patch against
> current svn that fixes this problem is attached.

Oops.  Please disregard previous patch -- it wasn't the best solution,
and besides, there is another bug that also has to be fixed for
lmList to work for glms.  (I guess I am just the first person to ever
try using this code?)

A corrected (and tested!) patch is attached.

-- Nathaniel

-- 
- Don't let your informants burn anything.
- Don't grow old.
- Be good grad students.
  -- advice of Murray B. Emeneau on the occasion of his 100th birthday
-------------- next part --------------
A non-text attachment was scrubbed...
Name: lmList-glm-fix-2.patch
Type: text/x-diff
Size: 796 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20071002/35d0be0b/attachment.bin>

From bates at stat.wisc.edu  Thu Oct  4 21:52:28 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 4 Oct 2007 14:52:28 -0500
Subject: [R-sig-ME] Gastric emptying data: nlmer vs. nlme
In-Reply-To: <loom.20071001T092046-562@post.gmane.org>
References: <loom.20071001T092046-562@post.gmane.org>
Message-ID: <40e66e0b0710041252w73c73e4aj340e031d2db416a4@mail.gmail.com>

Dieter,

Thanks for the very thorough description of the problem and for
including a reproducible example.  I enclose a modified version of
your script and the output using the development version of the lme4
package (https://svn.R-project.org/R-packages/branches/gappy-lmer/)

You will see that the development version does better on the first
model but still ends up giving a "false convergence" message.  I then
fit ge0A.nlmer and added another model ge0B.nlmer that removes the
random effect for kappa.  The (conservative) p-value for a test of H0:
ge0B.nlmer versus Ha: ge0A.nlmer is

> pchisq(979.76761 - 973.73274, df = 2)
[1] 0.9510734

so incorporating random effects for kappa is, at best, marginally significant.

The reason that the first model is so difficult to fit is because
there are 6 variance-covariance parameters and only 8 levels of
subj:treat.  You are trying to estimate too many variance-covariance
parameters from too few groups.  The likelihood surface will be very
flat and the parameter estimates will be ill-defined.

The reason that nlmer from the 0.99875-8 release of lme4 gave up has
to do with the calculation of the conditional modes of the random
effects to evaluate the Laplace approximation to the deviance.  In
that version I retained the values of the conditional modes of the
random effects (these are the values the maximum the conditional
density of the random effects given the data and the current values of
the model parameters - the Laplace approximation is evaluated at these
values and the adaptive Gauss-Hermite approximation is centered around
these values) between evaluations of the deviance.  That's a good idea
because the vector of the conditional modes for a different set of
parameters is going to be similar to the current values so you have
good starting estimates.  However, it is a bad idea in that the
evaluation of the approximation to the deviance will not only depend
on the values of the parameters and the data but also on where the
last value was taken.  This means that the function value being
optimized is not reproducible and that causes a lot of problems in a
derivative-free optimization.

To avoid this I now start each evaluation of the conditional modes at
the same point (all random effects start at zero) so the evaluation is
reproducible.

On 10/1/07, Dieter Menne <dieter.menne at menne-biomed.de> wrote:
> Dear Group,
>
> I have tried to redo published nlme-fits of gastric emptying data recorded by
> MRI with nlmer. nlme needs time, but give reasonable results, informing me about
> the known correlation between two nonlinear parameters, while nlmer gave up
> after two iterations without telling me why.
>
> I hope I got the translation to nlmer syntax correctly. What's wrong?
>
> Dieter Menne
>
>
> ----------------------------------------------------
> # Gastric Emptying data from
> # Goetze et. al (University Hospital of Z?rich)
> # http://ajpgi.physiology.org/cgi/content/abstract/00498.2005v1
> # Demo data set from http://www.menne-biomed.de/gastempt
> # R-Version: 2.5.1, i386, mingw32
> # lme4_0.99875-8.zip
>
> useNlme = TRUE
>
> # Get data
> ge = read.table("http://www.menne-biomed.de/gastempt/gastempt.csv",
>   sep=",",header=TRUE)
> #ge = read.table("gastempt.csv",sep=",",header=TRUE)
> ge$subj = as.factor(ge$subj)
> ge$subjtreat = as.factor(paste(ge$subj,ge$treat,sep="."))
> ge$t = as.double(ge$t)
>
>
> EmptInit= function(mCall,LHS,data){ # dummy, not explicitely used
>   stop("Should not be called")
> }
>
> # Standard LinExp model for gastric emptying
>
> SSEmptLinExp=selfStart(~v0*(1+kappa*t/tempt)*exp(-t/tempt),
>   initial=EmptInit, parameters= c("v0","kappa","tempt"))
>
> # nlme final is 643,1.4,74
> start = list(fixed=c(v0=600,kappa=1.0,tempt=70))
>
> if (useNlme) {
>   library(nlme)
>   contr = nlmeControl(pnlsTol=0.3,pnlsMaxIter=200,msMaxIter=200)
>   ge0.nlme=nlme(v~SSEmptLinExp(t,v0,kappa,tempt),
>     fixed =list(v0+kappa+tempt~1),
>     groups = ~subjtreat,
>     control=contr, start=start,  data=ge, verbose=F
>     )
>   summary(ge0.nlme)
>   # Note correlation between kappa and tempt
>   ge0A.nlme = update(ge0.nlme,random=v0+kappa~1)
>   summary(ge0A.nlme)
>   anova(ge0A.nlme,ge0.nlme)
> }
>
> if (!useNlme) {
>   library(lme4) # lme4_0.99875-8.zip
>   # This stops after two iterations without telling me details
>   ge0.nlmer = nlmer(
>     v~SSEmptLinExp(t,v0,kappa,tempt)~(v0+kappa+tempt|subjtreat),
>     data=ge,  start=start,verbose=T)
>   show(ge0.nlmer)
>   # This is close to the result of ge0A.nlme)
>   ge0A.nlmer = nlmer(
>     v~SSEmptLinExp(t,v0,kappa,tempt)~(v0+kappa|subjtreat),
>     data=ge,  start=start,verbose=T)
>   show(ge0.nlmer)
> }
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From otter at otter-rsch.com  Fri Oct  5 07:46:50 2007
From: otter at otter-rsch.com (dave fournier)
Date: Thu, 04 Oct 2007 22:46:50 -0700
Subject: [R-sig-ME]  lmer vs lmer2
References: <40e66e0b0709060917y1b3c5395n24045fc1a5f1d172@mail.gmail.com>
Message-ID: <4705CFCA.9030100@otter-rsch.com>


Hi,

I checked this example out with ADMB-RE using a modification of
our glmmADMB program  and have found the following:

1)

Parameter estimates with ADMB-RE are stable and
I get almost the same ones with or without the group 177 observations.

2) I get almost exactly the same LL estimate as SAS.

3) My estimates  for the fixed effects are similar to those in
    lmer2 except for the Intercept

Here are the estimates for lmer2 without group 177
    Estimate Std. Error t value
(Intercept) -1.948119   0.095877  -20.32
Height       1.640650   0.032800   50.02
Age          0.019379   0.001310   14.79
InitHeight   0.143977   0.111043    1.30
InitAge     -0.014618   0.007501   -1.95

these are the ADMB-RE estimates without group 177
  LL = 2294.85
   real_b           -2.0369e+000 1.0393e-001
   real_b           1.6460e+000 3.4587e-002
   real_b           1.9275e-002 1.3685e-003
   real_b           2.4857e-001 1.1984e-001
   real_b           -2.1290e-002 8.1749e-003

these are the estimates with group 177

   real_b           -2.0353e+000 1.0380e-001
   real_b           1.6438e+000 3.4430e-002
   real_b           1.9337e-002 1.3595e-003
   real_b           2.5070e-001 1.1966e-001
   real_b          -2.1486e-002 8.1618e-003

Here are the lmer2 estimates with group 177 included
(Intercept) -2.048023   0.101413  -20.19
Height       1.643644   0.031106   52.84
Age          0.019092   0.001391   13.73
InitHeight   0.262909   0.118516    2.22
InitAge     -0.021540   0.008111   -2.66

I think it is highly unlikely that the lmer2 estimate of
-1.948119 is the "correct" one and changes so much with
the addition of these few observations, while just by chance
ADMB-RE is wrong but happens to get the same estimate
for Intercept with and without group 177.
So it appears that lmer2 is not trustworthy.

Does anyone understand why the SAS point estimates appear to be 
completely different?

     Cheers,

       Dave



David A. Fournier
P.O. Box 2040,
Sidney, B.C. V8l 3S3
Canada
Phone/FAX 250-655-3364
http://otter-
-- 
David A. Fournier
P.O. Box 2040,
Sidney, B.C. V8l 3S3
Canada
Phone/FAX 250-655-3364
http://otter-rsch.com



From bates at stat.wisc.edu  Fri Oct  5 00:41:11 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 4 Oct 2007 17:41:11 -0500
Subject: [R-sig-ME] lmer vs lmer2
In-Reply-To: <4705CFCA.9030100@otter-rsch.com>
References: <40e66e0b0709060917y1b3c5395n24045fc1a5f1d172@mail.gmail.com>
	<4705CFCA.9030100@otter-rsch.com>
Message-ID: <40e66e0b0710041541n486a8cbu687d1142b7062e94@mail.gmail.com>

On 10/5/07, dave fournier <otter at otter-rsch.com> wrote:
>
> Hi,
>
> I checked this example out with ADMB-RE using a modification of
> our glmmADMB program  and have found the following:
>
> 1)
>
> Parameter estimates with ADMB-RE are stable and
> I get almost the same ones with or without the group 177 observations.
>
> 2) I get almost exactly the same LL estimate as SAS.
>
> 3) My estimates  for the fixed effects are similar to those in
>     lmer2 except for the Intercept
>
> Here are the estimates for lmer2 without group 177
>     Estimate Std. Error t value
> (Intercept) -1.948119   0.095877  -20.32
> Height       1.640650   0.032800   50.02
> Age          0.019379   0.001310   14.79
> InitHeight   0.143977   0.111043    1.30
> InitAge     -0.014618   0.007501   -1.95
>
> these are the ADMB-RE estimates without group 177
>   LL = 2294.85
>    real_b           -2.0369e+000 1.0393e-001
>    real_b           1.6460e+000 3.4587e-002
>    real_b           1.9275e-002 1.3685e-003
>    real_b           2.4857e-001 1.1984e-001
>    real_b           -2.1290e-002 8.1749e-003
>
> these are the estimates with group 177
>
>    real_b           -2.0353e+000 1.0380e-001
>    real_b           1.6438e+000 3.4430e-002
>    real_b           1.9337e-002 1.3595e-003
>    real_b           2.5070e-001 1.1966e-001
>    real_b          -2.1486e-002 8.1618e-003
>
> Here are the lmer2 estimates with group 177 included
> (Intercept) -2.048023   0.101413  -20.19
> Height       1.643644   0.031106   52.84
> Age          0.019092   0.001391   13.73
> InitHeight   0.262909   0.118516    2.22
> InitAge     -0.021540   0.008111   -2.66
>
> I think it is highly unlikely that the lmer2 estimate of
> -1.948119 is the "correct" one and changes so much with
> the addition of these few observations, while just by chance
> ADMB-RE is wrong but happens to get the same estimate
> for Intercept with and without group 177.
> So it appears that lmer2 is not trustworthy.
>
> Does anyone understand why the SAS point estimates appear to be
> completely different?

Because the SAS program is fitting a different model?

If you look at the sample SAS programs on the web site for the book
you will see that the authors are fitting models with fixed effects
for the logarithm of the height and the logarithm of the base height.

I have sort of lost track of the discussion of this example but I can
reproduce the results from Garrett Fitzmaurice's SAS analysis of these
data except for the variance-covariance of the random effects in the
model with correlated random effects for the intercept, the age and
the logarithm of the height.  With the development version of the lme4
package I get a (near) singular variance-covariance matrix in that
model fit while SAS PROC MIXED doesn't indicate a problem with the
fit.  The only indication of a problem from SAS is the large standard
errors on the estimates of the variance-covariance parameters.

I enclose the R script and output using the development version of the
lme4 package.  I have copied the variable names, etc. from the SAS
programs on Garrett's web site.  I fit two versions of each model, one
with all the subjects' data (fm1, fm2 and fm3) and one eliminating the
data for subject 197 (fm1a, fm2a and fm3a).  (Dave: according to the
information on Garrett's web site it is subject 197, not 177, who
appears to be an outlier.)

The clue that model fm3a has a singular variance covariance matrix is
the estimated correlation of -1.000.  Also, the verbose output shows
the converged value of the second parameter is very close to zero.
The first three parameters represent the variances of linear
combinations of the random effects.  The interpretation is that a
linear combination of the random effects for the intercept and for age
has zero variance.

The big change in the development version of the lme4 package relative
to earlier versions is a rewriting of the mixed model equations so
that a singular variance-covariance matrix for the random effects is
approached smoothly, even though it is on the boundary.

I have permission from the book's authors to create an R package with
the data sets from the book.  The package will be called AppLong and
will include sample analyses reproducing the SAS analyses as best I
can.
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: fev1_Rout.txt
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20071004/fd58543a/attachment.txt>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: fev1_R.txt
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20071004/fd58543a/attachment-0001.txt>

From dieter.menne at menne-biomed.de  Fri Oct  5 08:28:11 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 5 Oct 2007 06:28:11 +0000 (UTC)
Subject: [R-sig-ME] Gastric emptying data: nlmer vs. nlme
References: <loom.20071001T092046-562@post.gmane.org>
	<40e66e0b0710041252w73c73e4aj340e031d2db416a4@mail.gmail.com>
Message-ID: <loom.20071005T062137-485@post.gmane.org>

Douglas,

thanks a lot for your explanations.

> The reason that the first model is so difficult to fit is because
> there are 6 variance-covariance parameters and only 8 levels of
> subj:treat.  You are trying to estimate too many variance-covariance
> parameters from too few groups.  The likelihood surface will be very
> flat and the parameter estimates will be ill-defined.

I realized that the demo example was badly stripped down, but as you 
noted it was the idea to compare nlmer and nlme.
 
>>>This means that the function value being optimized is not 
>> reproducible and that causes a lot of problems in a
>> derivative-free optimization.

Here I am confused. I first had explicitly added the gradient, until I realized
that for this simple function selfStart() had produced it automatically. Or did
I misunderstand "derivative-free"?

Dieter

----
EmptInit= function(mCall,LHS,data){ # dummy, not explicitly used
   stop("Should not be called")
}
# Standard LinExp model for gastric emptying
SSEmptLinExp=selfStart(~v0*(1+kappa*t/tempt)*exp(-t/tempt),
   initial=EmptInit, parameters= c("v0","kappa","tempt"))
SSEmptLinExp
----

function (t, v0, kappa, tempt) 
{
    .expr1 <- kappa * t
    .expr3 <- 1 + .expr1/tempt
    .expr4 <- v0 * .expr3
    .expr7 <- exp(-t/tempt)
    .expr13 <- tempt^2
    .value <- .expr4 * .expr7
    .grad <- array(0, c(length(.value), 3L), list(NULL, c("v0", 
        "kappa", "tempt")))
    .grad[, "v0"] <- .expr3 * .expr7
    .grad[, "kappa"] <- v0 * (t/tempt) * .expr7
    .grad[, "tempt"] <- .expr4 * (.expr7 * (t/.expr13)) - v0 * 
        (.expr1/.expr13) * .expr7
    attr(.value, "gradient") <- .grad
    .value
}
<environment: 0x0383825c>
attr(,"initial")
function(mCall,LHS,data){ # dummy, not explicitly used
  stop("Should not be called")
}
attr(,"pnames")
[1] "v0"    "kappa" "tempt"
attr(,"class")
[1] "selfStart"
>



From lamprianou at yahoo.com  Fri Oct  5 12:22:19 2007
From: lamprianou at yahoo.com (Iasonas Lamprianou)
Date: Fri, 5 Oct 2007 03:22:19 -0700 (PDT)
Subject: [R-sig-ME] R-sig-mixed-models
Message-ID: <394176.37212.qm@web54110.mail.re2.yahoo.com>

Dear friends, 
I am trying the following commands, because I want to explore my data before doing an analysis with TEACHER as random effect. 

> data <-groupedData( DependentVariable ~ Predictor |SCHOOL/TEACHER,data=data)
> samp <- sample(levels(data$TEACHER),29)
> level2.subgroup <- subset(data, TEACHER %in% samp)
> level2<-lmList(DependentVariable  ~ Predictor  | TEACHER,data=level2.subgroup)  
and i get this error        Error in na.fail.default(data) : missing values in object

what is this error? What do I do wrong?
 
Dr. Iasonas Lamprianou
Department of Education
The University of Manchester
Oxford Road, Manchester M13 9PL, UK
Tel. 0044 161 275 3485
iasonas.lamprianou at manchester.ac.uk


----- Original Message ----
From: "r-sig-mixed-models-request at r-project.org" <r-sig-mixed-models-request at r-project.org>
To: r-sig-mixed-models at r-project.org
Sent: Friday, 5 October, 2007 1:41:25 AM
Subject: R-sig-mixed-models Digest, Vol 10, Issue 4


Send R-sig-mixed-models mailing list submissions to
    r-sig-mixed-models at r-project.org

To subscribe or unsubscribe via the World Wide Web, visit
    https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
or, via email, send a message with subject or body 'help' to
    r-sig-mixed-models-request at r-project.org

You can reach the person managing the list at
    r-sig-mixed-models-owner at r-project.org

When replying, please edit your Subject line so it is more specific
than "Re: Contents of R-sig-mixed-models digest..."


Today's Topics:

   1. Re: Gastric emptying data: nlmer vs. nlme (Douglas Bates)
   2.  lmer vs lmer2 (dave fournier)
   3. Re: lmer vs lmer2 (Douglas Bates)


----------------------------------------------------------------------

Message: 1
Date: Thu, 4 Oct 2007 14:52:28 -0500
From: "Douglas Bates" <bates at stat.wisc.edu>
Subject: Re: [R-sig-ME] Gastric emptying data: nlmer vs. nlme
To: "Dieter Menne" <dieter.menne at menne-biomed.de>
Cc: r-sig-mixed-models at r-project.org
Message-ID:
    <40e66e0b0710041252w73c73e4aj340e031d2db416a4 at mail.gmail.com>
Content-Type: text/plain; charset=ISO-8859-1

Dieter,

Thanks for the very thorough description of the problem and for
including a reproducible example.  I enclose a modified version of
your script and the output using the development version of the lme4
package (https://svn.R-project.org/R-packages/branches/gappy-lmer/)

You will see that the development version does better on the first
model but still ends up giving a "false convergence" message.  I then
fit ge0A.nlmer and added another model ge0B.nlmer that removes the
random effect for kappa.  The (conservative) p-value for a test of H0:
ge0B.nlmer versus Ha: ge0A.nlmer is

> pchisq(979.76761 - 973.73274, df = 2)
[1] 0.9510734

so incorporating random effects for kappa is, at best, marginally significant.

The reason that the first model is so difficult to fit is because
there are 6 variance-covariance parameters and only 8 levels of
subj:treat.  You are trying to estimate too many variance-covariance
parameters from too few groups.  The likelihood surface will be very
flat and the parameter estimates will be ill-defined.

The reason that nlmer from the 0.99875-8 release of lme4 gave up has
to do with the calculation of the conditional modes of the random
effects to evaluate the Laplace approximation to the deviance.  In
that version I retained the values of the conditional modes of the
random effects (these are the values the maximum the conditional
density of the random effects given the data and the current values of
the model parameters - the Laplace approximation is evaluated at these
values and the adaptive Gauss-Hermite approximation is centered around
these values) between evaluations of the deviance.  That's a good idea
because the vector of the conditional modes for a different set of
parameters is going to be similar to the current values so you have
good starting estimates.  However, it is a bad idea in that the
evaluation of the approximation to the deviance will not only depend
on the values of the parameters and the data but also on where the
last value was taken.  This means that the function value being
optimized is not reproducible and that causes a lot of problems in a
derivative-free optimization.

To avoid this I now start each evaluation of the conditional modes at
the same point (all random effects start at zero) so the evaluation is
reproducible.

On 10/1/07, Dieter Menne <dieter.menne at menne-biomed.de> wrote:
> Dear Group,
>
> I have tried to redo published nlme-fits of gastric emptying data recorded by
> MRI with nlmer. nlme needs time, but give reasonable results, informing me about
> the known correlation between two nonlinear parameters, while nlmer gave up
> after two iterations without telling me why.
>
> I hope I got the translation to nlmer syntax correctly. What's wrong?
>
> Dieter Menne
>
>
> ----------------------------------------------------
> # Gastric Emptying data from
> # Goetze et. al (University Hospital of Z?rich)
> # http://ajpgi.physiology.org/cgi/content/abstract/00498.2005v1
> # Demo data set from http://www.menne-biomed.de/gastempt
> # R-Version: 2.5.1, i386, mingw32
> # lme4_0.99875-8.zip
>
> useNlme = TRUE
>
> # Get data
> ge = read.table("http://www.menne-biomed.de/gastempt/gastempt.csv";,
>   sep=",",header=TRUE)
> #ge = read.table("gastempt.csv",sep=",",header=TRUE)
> ge$subj = as.factor(ge$subj)
> ge$subjtreat = as.factor(paste(ge$subj,ge$treat,sep="."))
> ge$t = as.double(ge$t)
>
>
> EmptInit= function(mCall,LHS,data){ # dummy, not explicitely used
>   stop("Should not be called")
> }
>
> # Standard LinExp model for gastric emptying
>
> SSEmptLinExp=selfStart(~v0*(1+kappa*t/tempt)*exp(-t/tempt),
>   initial=EmptInit, parameters= c("v0","kappa","tempt"))
>
> # nlme final is 643,1.4,74
> start = list(fixed=c(v0=600,kappa=1.0,tempt=70))
>
> if (useNlme) {
>   library(nlme)
>   contr = nlmeControl(pnlsTol=0.3,pnlsMaxIter=200,msMaxIter=200)
>   ge0.nlme=nlme(v~SSEmptLinExp(t,v0,kappa,tempt),
>     fixed =list(v0+kappa+tempt~1),
>     groups = ~subjtreat,
>     control=contr, start=start,  data=ge, verbose=F
>     )
>   summary(ge0.nlme)
>   # Note correlation between kappa and tempt
>   ge0A.nlme = update(ge0.nlme,random=v0+kappa~1)
>   summary(ge0A.nlme)
>   anova(ge0A.nlme,ge0.nlme)
> }
>
> if (!useNlme) {
>   library(lme4) # lme4_0.99875-8.zip
>   # This stops after two iterations without telling me details
>   ge0.nlmer = nlmer(
>     v~SSEmptLinExp(t,v0,kappa,tempt)~(v0+kappa+tempt|subjtreat),
>     data=ge,  start=start,verbose=T)
>   show(ge0.nlmer)
>   # This is close to the result of ge0A.nlme)
>   ge0A.nlmer = nlmer(
>     v~SSEmptLinExp(t,v0,kappa,tempt)~(v0+kappa|subjtreat),
>     data=ge,  start=start,verbose=T)
>   show(ge0.nlmer)
> }
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



------------------------------

Message: 2
Date: Thu, 04 Oct 2007 22:46:50 -0700
From: dave fournier <otter at otter-rsch.com>
Subject: [R-sig-ME]  lmer vs lmer2
To: r-sig-mixed-models at r-project.org
Message-ID: <4705CFCA.9030100 at otter-rsch.com>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed


Hi,

I checked this example out with ADMB-RE using a modification of
our glmmADMB program  and have found the following:

1)

Parameter estimates with ADMB-RE are stable and
I get almost the same ones with or without the group 177 observations.

2) I get almost exactly the same LL estimate as SAS.

3) My estimates  for the fixed effects are similar to those in
    lmer2 except for the Intercept

Here are the estimates for lmer2 without group 177
    Estimate Std. Error t value
(Intercept) -1.948119   0.095877  -20.32
Height       1.640650   0.032800   50.02
Age          0.019379   0.001310   14.79
InitHeight   0.143977   0.111043    1.30
InitAge     -0.014618   0.007501   -1.95

these are the ADMB-RE estimates without group 177
  LL = 2294.85
   real_b           -2.0369e+000 1.0393e-001
   real_b           1.6460e+000 3.4587e-002
   real_b           1.9275e-002 1.3685e-003
   real_b           2.4857e-001 1.1984e-001
   real_b           -2.1290e-002 8.1749e-003

these are the estimates with group 177

   real_b           -2.0353e+000 1.0380e-001
   real_b           1.6438e+000 3.4430e-002
   real_b           1.9337e-002 1.3595e-003
   real_b           2.5070e-001 1.1966e-001
   real_b          -2.1486e-002 8.1618e-003

Here are the lmer2 estimates with group 177 included
(Intercept) -2.048023   0.101413  -20.19
Height       1.643644   0.031106   52.84
Age          0.019092   0.001391   13.73
InitHeight   0.262909   0.118516    2.22
InitAge     -0.021540   0.008111   -2.66

I think it is highly unlikely that the lmer2 estimate of
-1.948119 is the "correct" one and changes so much with
the addition of these few observations, while just by chance
ADMB-RE is wrong but happens to get the same estimate
for Intercept with and without group 177.
So it appears that lmer2 is not trustworthy.

Does anyone understand why the SAS point estimates appear to be 
completely different?

     Cheers,

       Dave



David A. Fournier
P.O. Box 2040,
Sidney, B.C. V8l 3S3
Canada
Phone/FAX 250-655-3364
http://otter-
-- 
David A. Fournier
P.O. Box 2040,
Sidney, B.C. V8l 3S3
Canada
Phone/FAX 250-655-3364
http://otter-rsch.com



------------------------------

Message: 3
Date: Thu, 4 Oct 2007 17:41:11 -0500
From: "Douglas Bates" <bates at stat.wisc.edu>
Subject: Re: [R-sig-ME] lmer vs lmer2
To: davef at otter-rsch.com, "Garrett Fitzmaurice"
    <fitzmaur at hsph.harvard.edu>,    "Nan Laird" <LAIRD at hsph.harvard.edu>
Cc: r-sig-mixed-models at r-project.org
Message-ID:
    <40e66e0b0710041541n486a8cbu687d1142b7062e94 at mail.gmail.com>
Content-Type: text/plain; charset="iso-8859-1"

On 10/5/07, dave fournier <otter at otter-rsch.com> wrote:
>
> Hi,
>
> I checked this example out with ADMB-RE using a modification of
> our glmmADMB program  and have found the following:
>
> 1)
>
> Parameter estimates with ADMB-RE are stable and
> I get almost the same ones with or without the group 177 observations.
>
> 2) I get almost exactly the same LL estimate as SAS.
>
> 3) My estimates  for the fixed effects are similar to those in
>     lmer2 except for the Intercept
>
> Here are the estimates for lmer2 without group 177
>     Estimate Std. Error t value
> (Intercept) -1.948119   0.095877  -20.32
> Height       1.640650   0.032800   50.02
> Age          0.019379   0.001310   14.79
> InitHeight   0.143977   0.111043    1.30
> InitAge     -0.014618   0.007501   -1.95
>
> these are the ADMB-RE estimates without group 177
>   LL = 2294.85
>    real_b           -2.0369e+000 1.0393e-001
>    real_b           1.6460e+000 3.4587e-002
>    real_b           1.9275e-002 1.3685e-003
>    real_b           2.4857e-001 1.1984e-001
>    real_b           -2.1290e-002 8.1749e-003
>
> these are the estimates with group 177
>
>    real_b           -2.0353e+000 1.0380e-001
>    real_b           1.6438e+000 3.4430e-002
>    real_b           1.9337e-002 1.3595e-003
>    real_b           2.5070e-001 1.1966e-001
>    real_b          -2.1486e-002 8.1618e-003
>
> Here are the lmer2 estimates with group 177 included
> (Intercept) -2.048023   0.101413  -20.19
> Height       1.643644   0.031106   52.84
> Age          0.019092   0.001391   13.73
> InitHeight   0.262909   0.118516    2.22
> InitAge     -0.021540   0.008111   -2.66
>
> I think it is highly unlikely that the lmer2 estimate of
> -1.948119 is the "correct" one and changes so much with
> the addition of these few observations, while just by chance
> ADMB-RE is wrong but happens to get the same estimate
> for Intercept with and without group 177.
> So it appears that lmer2 is not trustworthy.
>
> Does anyone understand why the SAS point estimates appear to be
> completely different?

Because the SAS program is fitting a different model?

If you look at the sample SAS programs on the web site for the book
you will see that the authors are fitting models with fixed effects
for the logarithm of the height and the logarithm of the base height.

I have sort of lost track of the discussion of this example but I can
reproduce the results from Garrett Fitzmaurice's SAS analysis of these
data except for the variance-covariance of the random effects in the
model with correlated random effects for the intercept, the age and
the logarithm of the height.  With the development version of the lme4
package I get a (near) singular variance-covariance matrix in that
model fit while SAS PROC MIXED doesn't indicate a problem with the
fit.  The only indication of a problem from SAS is the large standard
errors on the estimates of the variance-covariance parameters.

I enclose the R script and output using the development version of the
lme4 package.  I have copied the variable names, etc. from the SAS
programs on Garrett's web site.  I fit two versions of each model, one
with all the subjects' data (fm1, fm2 and fm3) and one eliminating the
data for subject 197 (fm1a, fm2a and fm3a).  (Dave: according to the
information on Garrett's web site it is subject 197, not 177, who
appears to be an outlier.)

The clue that model fm3a has a singular variance covariance matrix is
the estimated correlation of -1.000.  Also, the verbose output shows
the converged value of the second parameter is very close to zero.
The first three parameters represent the variances of linear
combinations of the random effects.  The interpretation is that a
linear combination of the random effects for the intercept and for age
has zero variance.

The big change in the development version of the lme4 package relative
to earlier versions is a rewriting of the mixed model equations so
that a singular variance-covariance matrix for the random effects is
approached smoothly, even though it is on the boundary.

I have permission from the book's authors to create an R package with
the data sets from the book.  The package will be called AppLong and
will include sample analyses reproducing the SAS analyses as best I
can.
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: fev1_Rout.txt
Url: https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20071004/fd58543a/attachment.txt 
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: fev1_R.txt
Url: https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20071004/fd58543a/attachment-0001.txt 

------------------------------

_______________________________________________
R-sig-mixed-models mailing list
R-sig-mixed-models at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


End of R-sig-mixed-models Digest, Vol 10, Issue 4
*************************************************


      ___________________________________________________________ 
Want ideas for reducing your carbon footprint? Visit Yahoo! For Good  http://uk.promotions.yahoo.com/forgood/environment.html



From otter at otter-rsch.com  Fri Oct  5 15:48:38 2007
From: otter at otter-rsch.com (dave fournier)
Date: Fri, 05 Oct 2007 06:48:38 -0700
Subject: [R-sig-ME]  lmer vs lmer2
References: <4705CFCA.9030100@otter-rsch.com>
Message-ID: <470640B6.8030603@otter-rsch.com>

Thanks for that Doug, and I apologize for my bad eyesight.
I really can't see the screen in my old age!

It was unfortunate that when I removed the wrong
observations from the data the LL turned out to be
almost identical to the one from the SAS analysis.

Doing it properly, when I remove  the observations for group 197 from
the analysis I obtain the estimates

   real_b           -1.9486e+00 9.5787e-02
   real_b            1.6408e+00 3.3554e-02
   real_b            1.9368e-02 1.3501e-03
   real_b            1.4427e-01 1.1077e-01
   real_b           -1.4614e-02 7.4902e-03

which are identical  to lmer2
for all practical purposes.

  (Intercept) -1.948119   0.095877  -20.32
 > Height       1.640650   0.032800   50.02
 > Age          0.019379   0.001310   14.79
 > InitHeight   0.143977   0.111043    1.30
 > InitAge     -0.014618   0.007501   -1.95

However what I was  interested in was the application
of slightly robust methods in NLMM (Once you go robust
they are nonlinear even if the originalmodel is linear.)
So I fit the entire data set using a
conservative robust likelihood,
a 95% 05% mixture of two normal with the 05% one
having 3 times the std dev. of the 95% one The estimates I obtained
are

  real_b           -1.9730e+000 9.7074e-002
  real_b           1.6160e+000 2.7502e-002
  real_b           1.9959e-002 1.2192e-003
  real_b           2.1801e-001 1.1086e-001
  real_b           -1.9375e-002 7.5518e-003

compared to the non robust fit to all the data of

  real_b           -2.0353e+000 1.0380e-001
  real_b           1.6438e+000 3.4430e-002
  real_b           1.9337e-002 1.3595e-003
  real_b           2.5070e-001 1.1966e-001
  real_b          -2.1486e-002 8.1618e-003

which is not bad when one does not have to physically remove the
"bad" data. So what I really wanted to argue is that one should 
routinely use conservative robust methods when fitting RE models and in 
passing point out that ADMB-Re privdes a good platform for doing this.

    Cheers,

     Dave



-- 
David A. Fournier
P.O. Box 2040,
Sidney, B.C. V8l 3S3
Canada
Phone/FAX 250-655-3364
http://otter-rsch.com



From bates at stat.wisc.edu  Fri Oct  5 15:29:25 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 5 Oct 2007 08:29:25 -0500
Subject: [R-sig-ME] Gastric emptying data: nlmer vs. nlme
In-Reply-To: <loom.20071005T062137-485@post.gmane.org>
References: <loom.20071001T092046-562@post.gmane.org>
	<40e66e0b0710041252w73c73e4aj340e031d2db416a4@mail.gmail.com>
	<loom.20071005T062137-485@post.gmane.org>
Message-ID: <40e66e0b0710050629o6f1d1577rc6a990c04584e586@mail.gmail.com>

On 10/5/07, Dieter Menne <dieter.menne at menne-biomed.de> wrote:
> Douglas,
>
> thanks a lot for your explanations.
>
> > The reason that the first model is so difficult to fit is because
> > there are 6 variance-covariance parameters and only 8 levels of
> > subj:treat.  You are trying to estimate too many variance-covariance
> > parameters from too few groups.  The likelihood surface will be very
> > flat and the parameter estimates will be ill-defined.
>
> I realized that the demo example was badly stripped down, but as you
> noted it was the idea to compare nlmer and nlme.
>
> >>>This means that the function value being optimized is not
> >> reproducible and that causes a lot of problems in a
> >> derivative-free optimization.
>
> Here I am confused. I first had explicitly added the gradient, until I realized
> that for this simple function selfStart() had produced it automatically. Or did
> I misunderstand "derivative-free"?

I may have been too terse in my explanation and left you confused
about the derivative I was referring to.  We want the maximum
likelihood estimates for the model but we can't evaluate the
log-likelihood analytically.  In nlmer it is the Laplace approximation
to the log-likelihood that is optimized (nlme uses a different
optimization method that Mary Lindstrom and I suggested and which is
closely related to the penalized quasi-likelihood, or PQL, method for
generalized linear mixed models).    This is a function of the
fixed-effects parameters and the parameters that define the
variance-covariance of the random effects.

Given values of these parameters we determine the conditional modes of
the random effects by optimizing a penalized nonlinear least squares
objective with respect to the random effects. For this "inner
optimization" we use the gradient of the nonlinear model with respect
to the parameters - the "gradient" attribute of the model function
evaluation.

Once we have the conditional modes we evaluate the Laplace
approximation and return that to the function that is doing the "outer
optimization".  We don't have a gradient for the log-likelihood so the
log-likelihood itself is optimized with a derivative-free method
subject to constraints on some of the parameters (the variances or
relative variances must be non-negative).

What I found was that I needed to restart the penalized nonlinear
least squares optimization with respect to the random effects (i.e.
the "inner optimization") from the same starting estimates for the
random effects - the zero vector - every time.  Otherwise the value of
the Laplace approximation to the log-likelihood is somewhat stochastic
and that throws off the "outer optimization".  It's a general problem
with optimizers - if you do not get a reproducible evaluation of the
objective function - the log-likelihood in this case - the optimizer
gets very confused.

While on the topic of the outer optimization and the inner
optimization - Spencer Graves raised the issue several months ago of
why go to all this trouble of the inner optimization.  That is, why
find the conditional modes of the random effects?  The answer is that
the inner optimization is easy and fast.  It can have a large number
of parameters (3 * n where n is the number of levels of the grouping
factor) but the penalized least squares objective is separable (each
group of 3 random effects determine only the predictions for
observations at one subj:treat level) and the penalty part of the
objective "regularizes" the optimization.

I hope this helps.

> ----
> EmptInit= function(mCall,LHS,data){ # dummy, not explicitly used
>    stop("Should not be called")
> }
> # Standard LinExp model for gastric emptying
> SSEmptLinExp=selfStart(~v0*(1+kappa*t/tempt)*exp(-t/tempt),
>    initial=EmptInit, parameters= c("v0","kappa","tempt"))
> SSEmptLinExp
> ----
>
> function (t, v0, kappa, tempt)
> {
>     .expr1 <- kappa * t
>     .expr3 <- 1 + .expr1/tempt
>     .expr4 <- v0 * .expr3
>     .expr7 <- exp(-t/tempt)
>     .expr13 <- tempt^2
>     .value <- .expr4 * .expr7
>     .grad <- array(0, c(length(.value), 3L), list(NULL, c("v0",
>         "kappa", "tempt")))
>     .grad[, "v0"] <- .expr3 * .expr7
>     .grad[, "kappa"] <- v0 * (t/tempt) * .expr7
>     .grad[, "tempt"] <- .expr4 * (.expr7 * (t/.expr13)) - v0 *
>         (.expr1/.expr13) * .expr7
>     attr(.value, "gradient") <- .grad
>     .value
> }
> <environment: 0x0383825c>
> attr(,"initial")
> function(mCall,LHS,data){ # dummy, not explicitly used
>   stop("Should not be called")
> }
> attr(,"pnames")
> [1] "v0"    "kappa" "tempt"
> attr(,"class")
> [1] "selfStart"
> >
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From upsattar at yahoo.com  Sat Oct  6 20:03:34 2007
From: upsattar at yahoo.com (Abdus Sattar)
Date: Sat, 6 Oct 2007 11:03:34 -0700 (PDT)
Subject: [R-sig-ME] Extracting estfun & df from lmer2 model
Message-ID: <327747.20610.qm@web58109.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20071006/6a223cb2/attachment.pl>

From otter at otter-rsch.com  Sun Oct  7 07:43:14 2007
From: otter at otter-rsch.com (dave fournier)
Date: Sat, 06 Oct 2007 22:43:14 -0700
Subject: [R-sig-ME]   lmer vs lmer2 vs AD Model Builder RE
Message-ID: <470871F2.7080707@otter-rsch.com>


As a further thought on robust estimation for
mixed models I fit  the full data set with
a model based on a 95% mixture of a normal distribution
and a 5% Cauchy distribution  I obtained the
parameter estimates:

   real_b          -1.9405e+000 9.5807e-002
   real_b           1.6089e+000 3.2403e-002
   real_b           2.0168e-002 1.3129e-003
   real_b           1.8537e-001 1.1414e-001
   real_b          -1.7127e-002 7.7816e-003

compared to the estimates from R with the
outlier removed.

(Intercept) -1.948119   0.095877  -20.32
   Height       1.640650   0.032800   50.02
   Age          0.019379   0.001310   14.79
   InitHeight   0.143977   0.111043    1.30
   InitAge     -0.014618   0.007501   -1.95

They are almost identical for statistical purposes.
Also by restricting the parameterization of the full covariance matrix 
for the random effects slightly we are able to estimate it
which speaks to the stability of  the AD Model builder Random Effects
software.




David A. Fournier
P.O. Box 2040,
Sidney, B.C. V8l 3S3
Canada
Phone/FAX 250-655-3364
http://otter-rsch.com



From upsattar at yahoo.com  Mon Oct  8 17:36:50 2007
From: upsattar at yahoo.com (Abdus Sattar)
Date: Mon, 8 Oct 2007 08:36:50 -0700 (PDT)
Subject: [R-sig-ME] "estfun" & "df"
Message-ID: <543273.67908.qm@web58112.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20071008/3c9d9109/attachment.pl>

From pts007 at hotmail.com  Fri Oct 12 04:12:35 2007
From: pts007 at hotmail.com (ts p)
Date: Fri, 12 Oct 2007 02:12:35 +0000
Subject: [R-sig-ME] how to extract the estimated scale in the results of lmer
Message-ID: <BAY138-F2839FB6BFD12F4DDA1C7E08FA00@phx.gbl>

Hello,

I got the estimated scale by lmer as follows:
Estimated scale (compare to  1 )  0.892683
My question is how to extract the value of the estimated scale from the 
results of lmer.
And I know in some software the scale is constrained to 1. how lmer can do 
it?

Thanks a lot!

Tian



From bernd.weiss at uni-koeln.de  Fri Oct 12 04:35:58 2007
From: bernd.weiss at uni-koeln.de (Bernd Weiss)
Date: Fri, 12 Oct 2007 04:35:58 +0200
Subject: [R-sig-ME] how to extract the estimated scale in the results of
 lmer
In-Reply-To: <BAY138-F2839FB6BFD12F4DDA1C7E08FA00@phx.gbl>
References: <BAY138-F2839FB6BFD12F4DDA1C7E08FA00@phx.gbl>
Message-ID: <470EDD8E.50204@uni-koeln.de>

ts p schrieb:
> Hello,
> 
> I got the estimated scale by lmer as follows:
> Estimated scale (compare to  1 )  0.892683
> My question is how to extract the value of the estimated scale from the 
> results of lmer.
> And I know in some software the scale is constrained to 1. how lmer can do 
> it?
> 
> Thanks a lot!
> 

Dear Tian,

I guess

library(mlmRev)
(fm1 <- lmer(use ~ urban+age+livch+(1|district), Contraception, binomial))
slot(summary(fm1),"sigma")

is what you are looking for.

HTH,

Bernd



From njs at pobox.com  Sat Oct 13 11:21:48 2007
From: njs at pobox.com (Nathaniel Smith)
Date: Sat, 13 Oct 2007 02:21:48 -0700
Subject: [R-sig-ME] Simple bug in lmer: reliably crashes R
Message-ID: <20071013092148.GA6536@frances.vorpus.org>

There seems to be a minor input-validation bug in lme4 0.99875-8,
where fitting a model with (0 | ...) crashes R.

To reproduce:

> library(lme4)
> frame <- data.frame(A=1:100, B=1:100, C=rep(1:10, 10)
> lmer(A ~ B + (0 | C), frame)
Process R floating point exception at Sat Oct 13 01:26:25 2007

Obviously the workaround is "don't do that then", but it would
probably be good to fix at some point.

-- Nathaniel

-- 
Damn the Solar System.  Bad light; planets too distant; pestered with
comets; feeble contrivance; could make a better one myself.
  -- Lord Jeffrey



From J.Hadfield at ed.ac.uk  Sat Oct 13 20:45:05 2007
From: J.Hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Sat, 13 Oct 2007 19:45:05 +0100
Subject: [R-sig-ME] multivariate mixed models, again.
Message-ID: <60560B88-E190-4EDE-961F-F849C93E7FE6@ed.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20071013/6768e054/attachment.pl>

From bates at stat.wisc.edu  Sun Oct 14 17:20:17 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 14 Oct 2007 10:20:17 -0500
Subject: [R-sig-ME] Simple bug in lmer: reliably crashes R
In-Reply-To: <20071013092148.GA6536@frances.vorpus.org>
References: <20071013092148.GA6536@frances.vorpus.org>
Message-ID: <40e66e0b0710140820j43c72ebxbb733a9eba4ee690@mail.gmail.com>

I have just uploaded lme4_0.99875-9 to CRAN's incoming directory.
This version generates an error when trying to fit a model with terms
that reduce to (0|grp).

Thanks for the report.

On 10/13/07, Nathaniel Smith <njs at pobox.com> wrote:
> There seems to be a minor input-validation bug in lme4 0.99875-8,
> where fitting a model with (0 | ...) crashes R.
>
> To reproduce:
>
> > library(lme4)
> > frame <- data.frame(A=1:100, B=1:100, C=rep(1:10, 10)
> > lmer(A ~ B + (0 | C), frame)
> Process R floating point exception at Sat Oct 13 01:26:25 2007
>
> Obviously the workaround is "don't do that then", but it would
> probably be good to fix at some point.



From njs at pobox.com  Sun Oct 14 21:46:16 2007
From: njs at pobox.com (Nathaniel Smith)
Date: Sun, 14 Oct 2007 12:46:16 -0700
Subject: [R-sig-ME] Simple bug in lmer: reliably crashes R
In-Reply-To: <40e66e0b0710140820j43c72ebxbb733a9eba4ee690@mail.gmail.com>
References: <20071013092148.GA6536@frances.vorpus.org>
	<40e66e0b0710140820j43c72ebxbb733a9eba4ee690@mail.gmail.com>
Message-ID: <20071014194616.GA13587@frances.vorpus.org>

On Sun, Oct 14, 2007 at 10:20:17AM -0500, Douglas Bates wrote:
> I have just uploaded lme4_0.99875-9 to CRAN's incoming directory.
> This version generates an error when trying to fit a model with terms
> that reduce to (0|grp).
> 
> Thanks for the report.

No problem, and thanks for the quick response.

It looks like -9 still has the problem with lmList being broken for
glm's that I reported last week:
  https://stat.ethz.ch/pipermail/r-sig-mixed-models/2007q4/000374.html

-- Nathaniel



From bates at stat.wisc.edu  Wed Oct 17 22:04:47 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 17 Oct 2007 15:04:47 -0500
Subject: [R-sig-ME] [R] coef se in lme
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80EB5E70B@dc1ex01.air.org>
References: <68E7981938EAF54F987AD3848A0A6416E58342@ka-mail01.dfu.local>
	<2323A6D37908A847A7C32F1E3662C80EB5E70B@dc1ex01.air.org>
Message-ID: <40e66e0b0710171304s231b55cej8253eefbd7cb6e87@mail.gmail.com>

On 10/15/07, Doran, Harold <HDoran at air.org> wrote:
> ?vcov

The vcov method returns the estimated variance-covariance matrix of
the fixed-effects only.  I think Irene's question is about the
combination of the fixed-effects parameters and the BLUPs of the
random effects that is returned by the coef method applied to an lmer
object.  (You may recall that you were the person who requested such a
method in lme4 like the coef method in nlme :-)

On the face of it this quantity should be easy to define and evaluate
but in fact it is not easy to do so because these are combinations of
model parameters (the fixed effects) and unobserved random variables
(the random effects).  It gets a bit tricky trying to decide what the
variance of this combination would be.  I think there is a sensible
definition, or at least a computationally reasonable definition, but
there are still a few slippery points in the argument.

Lately I have taken to referring to the "estimates" of the random
effects, what are sometimes called the BLUPs or Best Linear Unbiased
Predictors, as the "conditional modes" of the random effects.  That
is, they are the values that maximize the density of the random
effects given the observed data and the values of the model
parameters.  For a linear mixed model the conditional distribution of
the random effects is multivariate normal so the conditional modes are
also the conditional means.  Also, we can evaluate the conditional
variance-covariance matrix of the random effects up to a scale factor.

The next part is where things get a bit hazy for me but I think it
makes sense to consider the joint distribution of the estimator of the
fixed-effects parameters and the random effects conditional on the
data and, possibly, on the variance components.  Conditional on the
relative variance-covariance of the random effects (i.e. the matrix
that occurs as the penalty term in the penalized least squares
representation of the model) the joint distribution of the
fixed-effects estimators and the random effects is multivariate normal
with mean and variance-covariance matrix determined from the
mixed-model equations.

This big (p+q by p+q, where p is the dimension of the fixed effects
and q is the dimension of the random effects) variance-covariance
matrix could be evaluated and, from that, the variance of any linear
combination of components.  However, I have my doubts about whether it
is the most sensible answer to evaluate.  Conditioning on the relative
variance-covariance matrix of the random effects is cheating, in a
way.  It would be like saying we have a known variance, $\sigma^2$
when, in fact, we are using an estimate.  The fact that we don't know
$\sigma^2$ is what gives rise to the t distributions and F
distributions in linear models and we are all trained to pay careful
attention to the number of degrees of freedom in that estimate and how
it affects our ideas of the precision of the estimates of other model
parameters.  For mixed models, though, many practioners are quite
comfortable conditioning on the value of some of the variance
components but not others.  It could turn out that conditioning on the
relative variance-covariance of the random effects is not a big deal
but I don't know.  I haven't examined it in detail and I don't know of
others who have.

Another approach entirely is to use Markov chain Monte Carlo to
examine the joint distribution of the parameters (in the Bayesian
sense) and the random effects.  If you save the fixed effects and the
random effects from the MCMC chain then you can evaluate the linear
combination of interest throughout the chain and get an empirical
distribution of the quantities returned by coef.

This is probably an unsatisfactory answer for Irene who may have
wanted something quick and simple.  Unfortunately, I don't think there
is a quick, simple answer here.

I suggest we move this discussion to the R-SIG-Mixed-Models list which
I am cc:ing on this reply.

> -----Original Message-----
> From: r-help-bounces at r-project.org on behalf of Irene Mantzouni
> Sent: Mon 10/15/2007 3:20 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] coef se in lme
>
> Hi all!
>
> How is it possible to estimate  standard errors for coef obtained from lme?
> Is there sth like se.coef() for lmer or what is the anaytical solution?
>
> Thank you!
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



From arild.husby at ed.ac.uk  Thu Oct 18 10:53:19 2007
From: arild.husby at ed.ac.uk (Arild Husby)
Date: Thu, 18 Oct 2007 09:53:19 +0100
Subject: [R-sig-ME] change in variance components depending on scaling of
	fixed effects
Message-ID: <47171EFF.5020502@ed.ac.uk>

Dear all,

I am trying to understand the output from a binomial lmer object and why 
the scaling of a fixed effect changes the variance components.

In the model p2rec is cbind(number recruits2,number recruits 1), Pop is 
populations (five level factor) and ja is year (continous covariate 
running from 1955-2004). I've used the Laplace optimization method, due 
to earlier reports of unstability of PQL when running binomial models.

First example: (ja continous covariate, range: 1955-2004)

> totmod2 <- lmer(p2rec~Pop*ja + (1|VROUW)+(1|ja), data=dltab2, 
family=binomial, method="Laplace", na.action=na.omit)
> summary(totmod2)
Generalized linear mixed model fit using Laplace
Formula: p2rec ~ Pop * ja + (1 | VROUW) + (1 | ja)
 Data: dltab2
Family: binomial(logit link)
 AIC   BIC logLik deviance
12456 12519  -6216    12432
Random effects:
Groups Name        Variance Std.Dev.
VROUW  (Intercept) 2.19300  1.48088
ja     (Intercept) 0.09675  0.31105
number of obs: 1323, groups: VROUW, 1088; ja, 48

Estimated scale (compare to  1 )  22.97855


I then scale  ja so that:  dltab2$ja<-scale(dltab2$ja, scale=FALSE)

> totmod2 <- lmer(p2rec~Pop*ja + (1|VROUW)+(1|ja), data=dltab2, 
family=binomial, method="Laplace", na.action=na.omit)
> summary(totmod2)
Generalized linear mixed model fit using Laplace
Formula: p2rec ~ Pop * ja + (1 | VROUW) + (1 | ja)
 Data: dltab2
Family: binomial(logit link)
 AIC  BIC logLik deviance
983.8 1046 -479.9    959.8
Random effects:
Groups Name        Variance Std.Dev.
VROUW  (Intercept) 0.54162  0.73595
ja     (Intercept) 0.29192  0.54029
number of obs: 1323, groups: VROUW, 1088; ja, 48

Estimated scale (compare to  1 )  0.7061424


Different scaling:  dltab2$ja<-scale(dltab2$ja, center=1000, scale=FALSE)

> totmod2 <- lmer(p2rec~Pop*ja + (1|VROUW)+(1|ja), data=dltab2, 
family=binomial, method="Laplace", na.action=na.omit)
> summary(totmod2)
Generalized linear mixed model fit using Laplace
Formula: p2rec ~ Pop * ja + (1 | VROUW) + (1 | ja)
 Data: dltab2
Family: binomial(logit link)
AIC  BIC logLik deviance
7136 7198  -3556     7112
Random effects:
Groups Name        Variance Std.Dev.
VROUW  (Intercept) 2.19300  1.48088
ja     (Intercept) 0.09675  0.31105
number of obs: 1323, groups: VROUW, 1088; ja, 48

Estimated scale (compare to  1 )  3.083302


Estimates of fixed effects changes as one would expect (so have not 
printed them here), but I do not understand why there is such a massive 
difference in the variance components.

Note that the first and last example has the same estimates of variance 
components, but that the estimated scale is massively different.

All help is highly appreciated.


(See also a posting by Steven Orzack on a similar problem: 
http://tolstoy.newcastle.edu.au/R/e2/help/07/07/22076.html)


All help is highly appreciated.

Thanks very much,

Arild Husby

> sessionInfo()
R version 2.5.0 (2007-04-23)
i386-pc-mingw32

locale:
LC_COLLATE=English_United Kingdom.1252;LC_CTYPE=English_United 
Kingdom.1252;LC_MONETARY=English_United 
Kingdom.1252;LC_NUMERIC=C;LC_TIME=English_United Kingdom.1252

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  
"methods"   "base"   
other attached packages:
     lme4      Matrix     lattice
"0.99875-2" "0.99875-2"   "0.15-11"



-- 
Arild Husby
Institute of Evolutionary Biology
Room 413, Ashworth Labs,
King's Buildings,
University of Edinburgh
EH9 3JT, UK

E-mail: arild.husby at ed.ac.uk
web: http://homepages.ed.ac.uk/loeske/arild.html
Tel: +44 (0)131 650 5990
Mob: +44 (0)798 275 0668



From otter at otter-rsch.com  Thu Oct 18 09:57:51 2007
From: otter at otter-rsch.com (dave fournier)
Date: Thu, 18 Oct 2007 00:57:51 -0700
Subject: [R-sig-ME]  [R] coef se in lme
References: <2323A6D37908A847A7C32F1E3662C80EB5E70B@dc1ex01.air.org>
Message-ID: <471711FF.4070007@otter-rsch.com>


Here is one approach to this problem.
In the AD Model Builder Random Effects package we provide estimated
standard deviations for any function of the fixed and random effects, 
(here I include the parameters which detemine the covarince matrices if 
present) and the random effects. This is for general nonlinear random 
effects models, but the calculations can be used for linear models as 
well. We calculates these estimates as follows. Let L(x,u)
be the log-likelihood function for the parameters x and u given the 
observed data,
where u is the vector of random effects and x is the vector of the other 
parameters. Let F(x) be the log-likelihood for x after the u have been 
integrated out. This integration might be exact or more commonly via the 
Laplace approximation or something else.
For any x let uhat(x) be the value of u which maximizes L(x,u),
and let xhat be the value of x which maximizes
F(x). The the estimate for the covaraince matrix for the x is then
S_xx = inv(F_xx) and the estimated full covariance matrix Sigma for the 
x and u is given by


S_xx                 S_xx * uhat_x
(S_xx * uhat_x)' uhat' * S_xx * uhat_x + inv(L_uu)

where ' denotes transpose _x denotes first derivative wrt x (note that 
uhat is a function of x so that uhat_x makes sense)and _xx _uu denote 
the second derivatives wrt x and u. we then use Sigma and the delta 
method to estimate the standard deviation of any (differentiable) 
function of x and u.
~
~
-- 
David A. Fournier
P.O. Box 2040,
Sidney, B.C. V8l 3S3
Canada
Phone/FAX 250-655-3364
http://otter-rsch.com



From HDoran at air.org  Thu Oct 18 15:19:49 2007
From: HDoran at air.org (Doran, Harold)
Date: Thu, 18 Oct 2007 09:19:49 -0400
Subject: [R-sig-ME] [R] coef se in lme
In-Reply-To: <40e66e0b0710171304s231b55cej8253eefbd7cb6e87@mail.gmail.com>
Message-ID: <2323A6D37908A847A7C32F1E3662C80E012DDFC7@dc1ex01.air.org>

I'm particularly interested in this, so I'll bite. Let me start with
something very basic and build up later as I think more about this.

Assume we have a model such as

Y_ij = \mu + a_i + e_ij \quad a_i \sim N(0, s^2), e_ij \sim N(0, r^2)

In this basic model, we also assume the variance components are
orthogonal, denoted e_ij \bot a_i

Now, from ranef() I get the estimate of a for all i (what Doug calls the
conditional modes) and, also using the postVar argument in this
extractor function I get variances of these conditional modes. 

Now, vcov() would return the variance for the fixed parameter \mu.

Given that we can find the variances for each term in the model, and
that they are assumed orthogonal, why is it that we can't simply take
the variance of a linear combination? 

For example, if I want to find:

\hat{y}_ij = \mu + a_i

Then the variance of this would be simply

Var(\hat{y}_ij) = var(\mu) + var(a_i)

And in this case there is no covariance given e_ij \bot a_i, right? 

Even if this scenario is too simplistic, I assume I can use a taylor
series expansion to derive something that gives me an estimate of the
variance of \hat{y}_ij in closed form that is pretty close, no?

OK, I'm ready for my lashing.

Harold


> -----Original Message-----
> From: dmbates at gmail.com [mailto:dmbates at gmail.com] On Behalf 
> Of Douglas Bates
> Sent: Wednesday, October 17, 2007 4:05 PM
> To: Doran, Harold
> Cc: Irene Mantzouni; r-help at stat.math.ethz.ch; R-SIG-Mixed-Models
> Subject: Re: [R] coef se in lme
> 
> On 10/15/07, Doran, Harold <HDoran at air.org> wrote:
> > ?vcov
> 
> The vcov method returns the estimated variance-covariance 
> matrix of the fixed-effects only.  I think Irene's question 
> is about the combination of the fixed-effects parameters and 
> the BLUPs of the random effects that is returned by the coef 
> method applied to an lmer object.  (You may recall that you 
> were the person who requested such a method in lme4 like the 
> coef method in nlme :-)
> 
> On the face of it this quantity should be easy to define and 
> evaluate but in fact it is not easy to do so because these 
> are combinations of model parameters (the fixed effects) and 
> unobserved random variables (the random effects).  It gets a 
> bit tricky trying to decide what the variance of this 
> combination would be.  I think there is a sensible 
> definition, or at least a computationally reasonable 
> definition, but there are still a few slippery points in the argument.
> 
> Lately I have taken to referring to the "estimates" of the 
> random effects, what are sometimes called the BLUPs or Best 
> Linear Unbiased Predictors, as the "conditional modes" of the 
> random effects.  That is, they are the values that maximize 
> the density of the random effects given the observed data and 
> the values of the model parameters.  For a linear mixed model 
> the conditional distribution of the random effects is 
> multivariate normal so the conditional modes are also the 
> conditional means.  Also, we can evaluate the conditional 
> variance-covariance matrix of the random effects up to a scale factor.
> 
> The next part is where things get a bit hazy for me but I 
> think it makes sense to consider the joint distribution of 
> the estimator of the fixed-effects parameters and the random 
> effects conditional on the data and, possibly, on the 
> variance components.  Conditional on the relative 
> variance-covariance of the random effects (i.e. the matrix 
> that occurs as the penalty term in the penalized least 
> squares representation of the model) the joint distribution 
> of the fixed-effects estimators and the random effects is 
> multivariate normal with mean and variance-covariance matrix 
> determined from the mixed-model equations.
> 
> This big (p+q by p+q, where p is the dimension of the fixed 
> effects and q is the dimension of the random effects) 
> variance-covariance matrix could be evaluated and, from that, 
> the variance of any linear combination of components.  
> However, I have my doubts about whether it is the most 
> sensible answer to evaluate.  Conditioning on the relative 
> variance-covariance matrix of the random effects is cheating, 
> in a way.  It would be like saying we have a known variance, 
> $\sigma^2$ when, in fact, we are using an estimate.  The fact 
> that we don't know $\sigma^2$ is what gives rise to the t 
> distributions and F distributions in linear models and we are 
> all trained to pay careful attention to the number of degrees 
> of freedom in that estimate and how it affects our ideas of 
> the precision of the estimates of other model parameters.  
> For mixed models, though, many practioners are quite 
> comfortable conditioning on the value of some of the variance 
> components but not others.  It could turn out that 
> conditioning on the relative variance-covariance of the 
> random effects is not a big deal but I don't know.  I haven't 
> examined it in detail and I don't know of others who have.
> 
> Another approach entirely is to use Markov chain Monte Carlo 
> to examine the joint distribution of the parameters (in the Bayesian
> sense) and the random effects.  If you save the fixed effects 
> and the random effects from the MCMC chain then you can 
> evaluate the linear combination of interest throughout the 
> chain and get an empirical distribution of the quantities 
> returned by coef.
> 
> This is probably an unsatisfactory answer for Irene who may 
> have wanted something quick and simple.  Unfortunately, I 
> don't think there is a quick, simple answer here.
> 
> I suggest we move this discussion to the R-SIG-Mixed-Models 
> list which I am cc:ing on this reply.
> 
> > -----Original Message-----
> > From: r-help-bounces at r-project.org on behalf of Irene Mantzouni
> > Sent: Mon 10/15/2007 3:20 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] coef se in lme
> >
> > Hi all!
> >
> > How is it possible to estimate  standard errors for coef 
> obtained from lme?
> > Is there sth like se.coef() for lmer or what is the 
> anaytical solution?
> >
> > Thank you!
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 



From bates at stat.wisc.edu  Thu Oct 18 17:49:47 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 18 Oct 2007 10:49:47 -0500
Subject: [R-sig-ME] [R] coef se in lme
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E012DDFC7@dc1ex01.air.org>
References: <40e66e0b0710171304s231b55cej8253eefbd7cb6e87@mail.gmail.com>
	<2323A6D37908A847A7C32F1E3662C80E012DDFC7@dc1ex01.air.org>
Message-ID: <40e66e0b0710180849q4382b7efmb66fbe3638bd1758@mail.gmail.com>

On 10/18/07, Doran, Harold <HDoran at air.org> wrote:
> I'm particularly interested in this, so I'll bite. Let me start with
> something very basic and build up later as I think more about this.

> Assume we have a model such as

> Y_ij = \mu + a_i + e_ij \quad a_i \sim N(0, s^2), e_ij \sim N(0, r^2)

> In this basic model, we also assume the variance components are
> orthogonal, denoted e_ij \bot a_i

> Now, from ranef() I get the estimate of a for all i (what Doug calls the
> conditional modes) and, also using the postVar argument in this
> extractor function I get variances of these conditional modes.

> Now, vcov() would return the variance for the fixed parameter \mu.

> Given that we can find the variances for each term in the model, and
> that they are assumed orthogonal, why is it that we can't simply take
> the variance of a linear combination?

Because the conditional distribution of the random effects (given the
data) depends on the fixed effects.  If you change \mu you will get
different conditonal modes for the a_i and you must take this into
account when considering the variation in the combination you show
below.

> For example, if I want to find:

> \hat{y}_ij = \mu + a_i
> Then the variance of this would be simply

> Var(\hat{y}_ij) = var(\mu) + var(a_i)

That would apply if \hat{\mu} and a_i were independent, but they aren't.

For this example (and thank you for suggesting a simple example for
discussion), the calculation I suggested would be conditional on the
ratio s/r or, equivalently, the ratio s^2/r^2.  I refer to this as the
"relative variance" of the random effects or, in more general cases,
the relative variance-covariance of the random effects.

If we condition on both variance components, s^2 and r^2, the joint
distribution of the random effects and the estimators of the fixed
effects is multivariate normal.  The mean of this distribution is the
solution to a penalized linear least squares problem.  Exactly the
same calculations as we would use for a linear least squares problem
will give us the joint distribution of the estimators of the fixed
effects and the random effects marginalized over r^2 but conditional
on the ratio s^2/r^2.

The "marginalized over r^2" of that last sentence is what leads us to
t distributions instead of z distributions and, after suitable
normalization, F distributions instead of chi-squared distributions.
What I haven't quite got a handle on yet is whether it is as important
to consider the effects of uncertainty in our estimate of s^2/r^2 as
it is to consider the effects of uncertainty in our estimate of s^2.
For fixed effects models we know that in some circumstances it is very
important to consider the uncertainty in our estimate of r^2.  This is
what Gosset did in formulating the T distribution and what Fisher
generalized into the analysis of variance.

Can we reasonably assume that variability in our estimate of r^2 is
important but variability in our estimate of s^2/r^2 is not?  If so
then we can apply the usual results from least squares to the
penalized least squares problem (conditional on the ratio r^2/s^2) and
we're done.  We need the model matrices for the fixed and the random
effects and the penalty matrix and we do need to be careful of how the
computation is done, especially in cases such as you consider where
the number of random effects can get into the millions, but the theory
is straightforward.  However, I don't think it is that cut and dried
in all cases.

It is likely that, in your world, conditioning on an estimate of
s^2/r^2 would not be a stretch.  In fact, you could condition on an
estimate of r^2 while you were at it because you have a huge number of
degrees of freedom in your estimate of the residual variance.  The
differences between a T distribution with 10 million degrees of
freedom and a standard normal distribution are not worth worrying
about.

So I think that in your world it would be reasonable to use the
penalized least squares representation and calculate the results from
that.  You mentioned above the "posterior variances" (which I would
call conditional variances if I were to do it over again) of the
random effects.  These come from the penalized least squares
representation but conditional on the fixed effects.  The get the
joint distribution of the random effects and the estimators of the
fixed effects it is only necessary to extend that calculation a bit.
>From the joint distribution we can get the variance of the linear
combination you want to consider.

For smaller data sets it may be worthwhile going the Markov chain
Monte Carlo route because that takes into account the variation in all
the parameter estimates and in the random effects.  However, Steven
Novick recently sent me mail with a reproducible example about
difficulties with the Markov chain Monte Carlo samples drawn from some
models.  I'll send a slightly expanded version of that report to the
list for comment later today.


> And in this case there is no covariance given e_ij \bot a_i, right?
>
> Even if this scenario is too simplistic, I assume I can use a taylor
> series expansion to derive something that gives me an estimate of the
> variance of \hat{y}_ij in closed form that is pretty close, no?
>
> OK, I'm ready for my lashing.
>
> Harold
>
>
> > -----Original Message-----
> > From: dmbates at gmail.com [mailto:dmbates at gmail.com] On Behalf
> > Of Douglas Bates
> > Sent: Wednesday, October 17, 2007 4:05 PM
> > To: Doran, Harold
> > Cc: Irene Mantzouni; r-help at stat.math.ethz.ch; R-SIG-Mixed-Models
> > Subject: Re: [R] coef se in lme
> >
> > On 10/15/07, Doran, Harold <HDoran at air.org> wrote:
> > > ?vcov
> >
> > The vcov method returns the estimated variance-covariance
> > matrix of the fixed-effects only.  I think Irene's question
> > is about the combination of the fixed-effects parameters and
> > the BLUPs of the random effects that is returned by the coef
> > method applied to an lmer object.  (You may recall that you
> > were the person who requested such a method in lme4 like the
> > coef method in nlme :-)
> >
> > On the face of it this quantity should be easy to define and
> > evaluate but in fact it is not easy to do so because these
> > are combinations of model parameters (the fixed effects) and
> > unobserved random variables (the random effects).  It gets a
> > bit tricky trying to decide what the variance of this
> > combination would be.  I think there is a sensible
> > definition, or at least a computationally reasonable
> > definition, but there are still a few slippery points in the argument.
> >
> > Lately I have taken to referring to the "estimates" of the
> > random effects, what are sometimes called the BLUPs or Best
> > Linear Unbiased Predictors, as the "conditional modes" of the
> > random effects.  That is, they are the values that maximize
> > the density of the random effects given the observed data and
> > the values of the model parameters.  For a linear mixed model
> > the conditional distribution of the random effects is
> > multivariate normal so the conditional modes are also the
> > conditional means.  Also, we can evaluate the conditional
> > variance-covariance matrix of the random effects up to a scale factor.
> >
> > The next part is where things get a bit hazy for me but I
> > think it makes sense to consider the joint distribution of
> > the estimator of the fixed-effects parameters and the random
> > effects conditional on the data and, possibly, on the
> > variance components.  Conditional on the relative
> > variance-covariance of the random effects (i.e. the matrix
> > that occurs as the penalty term in the penalized least
> > squares representation of the model) the joint distribution
> > of the fixed-effects estimators and the random effects is
> > multivariate normal with mean and variance-covariance matrix
> > determined from the mixed-model equations.
> >
> > This big (p+q by p+q, where p is the dimension of the fixed
> > effects and q is the dimension of the random effects)
> > variance-covariance matrix could be evaluated and, from that,
> > the variance of any linear combination of components.
> > However, I have my doubts about whether it is the most
> > sensible answer to evaluate.  Conditioning on the relative
> > variance-covariance matrix of the random effects is cheating,
> > in a way.  It would be like saying we have a known variance,
> > $\sigma^2$ when, in fact, we are using an estimate.  The fact
> > that we don't know $\sigma^2$ is what gives rise to the t
> > distributions and F distributions in linear models and we are
> > all trained to pay careful attention to the number of degrees
> > of freedom in that estimate and how it affects our ideas of
> > the precision of the estimates of other model parameters.
> > For mixed models, though, many practioners are quite
> > comfortable conditioning on the value of some of the variance
> > components but not others.  It could turn out that
> > conditioning on the relative variance-covariance of the
> > random effects is not a big deal but I don't know.  I haven't
> > examined it in detail and I don't know of others who have.
> >
> > Another approach entirely is to use Markov chain Monte Carlo
> > to examine the joint distribution of the parameters (in the Bayesian
> > sense) and the random effects.  If you save the fixed effects
> > and the random effects from the MCMC chain then you can
> > evaluate the linear combination of interest throughout the
> > chain and get an empirical distribution of the quantities
> > returned by coef.
> >
> > This is probably an unsatisfactory answer for Irene who may
> > have wanted something quick and simple.  Unfortunately, I
> > don't think there is a quick, simple answer here.
> >
> > I suggest we move this discussion to the R-SIG-Mixed-Models
> > list which I am cc:ing on this reply.
> >
> > > -----Original Message-----
> > > From: r-help-bounces at r-project.org on behalf of Irene Mantzouni
> > > Sent: Mon 10/15/2007 3:20 PM
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] coef se in lme
> > >
> > > Hi all!
> > >
> > > How is it possible to estimate  standard errors for coef
> > obtained from lme?
> > > Is there sth like se.coef() for lmer or what is the
> > anaytical solution?
> > >
> > > Thank you!
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
>



From bates at stat.wisc.edu  Thu Oct 18 22:17:20 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 18 Oct 2007 15:17:20 -0500
Subject: [R-sig-ME] mcmcsamp and the prior on the variance components
Message-ID: <40e66e0b0710181317k504e0495m57aa96298541d323@mail.gmail.com>

In a previous message I mentioned that Steven Novick of
GlaxoSmithKline had written to me regarding peculiar behavior in some
chains generated by mcmcsamp in the lme4 package.  I enclose a script
that he provided to reproduce the problem.  I modified it slightly to
only try the BRugs model if the package exists.  I never use Windows
so I don't have access to BRugs.

If you run the script and check the plots you will see that the
estimate of the logarithm of the batch standard deviation takes
excursions when the number of batches in the sample is small.  These
excursions correspond to estimates that are very small, effectively
zero.  The prior being used is an improper, non-informative prior that
corresponds to a locally uniform prior on the logarithm of the
variance (or, equivalently, the logarithm of the standard deviation).
The end result is that the posterior distribution of the logarithms of
the standard deviations is proportional to the likelihoods.  It will
always be true that the likelihood, as a function of the logarithm of
the batch variance, reaches a plateau at large negative values,
corresponding to batch variances very close to zero.  When you have a
small number of batches it is possible for the chain to jump onto that
plateau and then it takes a long time for the chain to get off the
plateau.

I illustrate the plateau with a R script and the resulting plot.  This
script requires the development version of the lme4 package.

Steve's colleague John Peterson directed us to a paper by James Hobert
and George Casella in JASA, 1996 on "The Effect of Improper Priors on
Gibbs Sampling in Hierarchical Linear Models" that could be of
interest regarding this issue.

The big question is what to do about the plateau effect.  It results
in an improper posterior density so I can appreciate the argument that
the improper prior is not suitable.  I view improper priors as
reasonable if they produce proper posterior densities but that won't be
the case here.
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: mcmc_ex_R.txt
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20071018/3eea0d29/attachment.txt>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: mcmc_ex1_Rout.txt
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20071018/3eea0d29/attachment-0001.txt>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: profiled.pdf
Type: application/pdf
Size: 11638 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20071018/3eea0d29/attachment.pdf>

From otter at otter-rsch.com  Fri Oct 19 07:32:30 2007
From: otter at otter-rsch.com (dave fournier)
Date: Thu, 18 Oct 2007 22:32:30 -0700
Subject: [R-sig-ME]  [R] coef se in lme
References: <2323A6D37908A847A7C32F1E3662C80E012DDFC7@dc1ex01.air.org>
Message-ID: <4718416E.6010002@otter-rsch.com>

The AD Model Builder method I posted earlier takes into account the
uncertainty in the mean and both std deviations in Harold's simple model.

        Y_ij - mu + u_i +eps_ij

To illustrate this
I built a little model and simulated a data set with 1<=i<=10, 1<=j<=5 
observations.  Below are the parameter estiamtes tigether with their
estimated std devs.  The true values were mu=3.0 sigma_u=2.0 and sigma=3.0



>     1   mu           2.4091e+00 7.4307e-01
>     2   log_sigma_u  6.3908e-01 3.4913e-01
>     3   log_sigma    1.1354e+00 1.1180e-01
>     4   u           -3.1695e-01 6.4561e-01
>     5   u            2.0441e-01 6.4496e-01
>     6   u            5.1996e-01 6.4748e-01
>     7   u            4.3754e-01 6.4661e-01
>     8   u            4.8381e-01 6.4708e-01
>     9   u            1.2214e+00 6.6075e-01
>    10   u           -1.7631e+00 6.7793e-01
>    11   u            3.3987e-01 6.4578e-01
>    12   u           -1.7663e-01 6.4485e-01
>    13   u           -9.5030e-01 6.5439e-01

then I fixed log_sigma_u and log_sigma at their
estimated values and obtained.
> # fixed sd's
>  index   name   value      std dev
>      1   mu  2.4093e+00 7.4307e-01
>      2   u  -3.1736e-01 6.4407e-01
>      3   u   2.0456e-01 6.4407e-01
>      4   u   5.2045e-01 6.4407e-01
>      5   u   4.3794e-01 6.4407e-01
>      6   u   4.8426e-01 6.4407e-01
>      7   u   1.2226e+00 6.4407e-01
>      8   u  -1.7650e+00 6.4407e-01
>      9   u   3.4016e-01 6.4407e-01
>     10   u  -1.7689e-01 6.4407e-01
>     11   u  -9.5138e-01 6.4407e-01
and finally I almost fixed mu
(can't fix it completley because then there would be
no "fixed" effects and the model thinks there is
nothing to do.) and obtained

> # all fixed
>  index   name   value      std dev
>      1   mu  2.4093e+00 2.2361e-03
>      2   u  -3.1736e-01 5.9145e-01
>      3   u   2.0456e-01 5.9145e-01
>      4   u   5.2045e-01 5.9145e-01
>      5   u   4.3794e-01 5.9145e-01
>      6   u   4.8426e-01 5.9145e-01
>      7   u   1.2226e+00 5.9145e-01
>      8   u  -1.7650e+00 5.9145e-01
>      9   u   3.4016e-01 5.9145e-01
>     10   u  -1.7689e-01 5.9145e-01
>     11   u  -9.5138e-01 5.9145e-01
So for example u(1) the first random effect has estimated std devs

  6.4561e-01, 6.4407e-01, and 5.9145e-01


under the three models. It appears that most of the "extra" uncertainty 
in u(1) comes from the uncertainty in mu.

-- 
David A. Fournier
P.O. Box 2040,
Sidney, B.C. V8l 3S3
Canada
Phone/FAX 250-655-3364
http://otter-rsch.com



From bates at stat.wisc.edu  Fri Oct 19 00:03:05 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 18 Oct 2007 17:03:05 -0500
Subject: [R-sig-ME] [R] coef se in lme
In-Reply-To: <471711FF.4070007@otter-rsch.com>
References: <2323A6D37908A847A7C32F1E3662C80EB5E70B@dc1ex01.air.org>
	<471711FF.4070007@otter-rsch.com>
Message-ID: <40e66e0b0710181503x6c3fec49s509d683c953b61e8@mail.gmail.com>

On 10/18/07, dave fournier <otter at otter-rsch.com> wrote:

> Here is one approach to this problem.

> In the AD Model Builder Random Effects package we provide estimated
> standard deviations for any function of the fixed and random effects,
> (here I include the parameters which detemine the covarince matrices if
> present) and the random effects. This is for general nonlinear random
> effects models, but the calculations can be used for linear models as
> well. We calculate these estimates as follows. Let L(x,u)
> be the log-likelihood function for the parameters x and u given the
> observed data,
> where u is the vector of random effects and x is the vector of the other
> parameters.

I know it may sound pedantic but I don't know what a log-likelihood
L(x,u) would be because you are treating parameters and the random
effects as if they are the same type of object and they're not.  If
you want to use a Bayesian approach you can kind of level the playing
field and say that everything is a parameter except for the observed
data values.  However, Bayesians also need to come up with a prior and
that isn't trivial in this case, as I tried to indicate in my message
about the mcmcsamp chain taking excursions.

I find I can easily confuse myself in the theory of the maximum
likelihood or REML estimates if I am not careful about the terminology
and the roles of the different coefficients in the linear predictor.
I think I would call that function L(x,u) the conditional density of
the random effects given the data.  The parameters determine the joint
density for the random effects and responses so plugging the observed
values of the responses into this expression yields the conditional
density of the random effects.

> Let F(x) be the log-likelihood for x after the u have been
> integrated out. This integration might be exact or more commonly via the
> Laplace approximation or something else.
> For any x let uhat(x) be the value of u which maximizes L(x,u),

I think that is what I would call the conditional modes of the random
effects.  These depend on the observed responses and the model
parameters.

> and let xhat be the value of x which maximizes F(x).

> The estimate for the covariance matrix for the x is then
> S_xx = inv(F_xx) and the estimated full covariance matrix Sigma for the
> x and u is given by

> S_xx                 S_xx * uhat_x
> (S_xx * uhat_x)' uhat' * S_xx * uhat_x + inv(L_uu)

> where ' denotes transpose _x denotes first derivative wrt x (note that
> uhat is a function of x so that uhat_x makes sense) and _xx _uu denote
> the second derivatives wrt x and u. we then use Sigma and the delta
> method to estimate the standard deviation of any (differentiable)
> function of x and u.

I'm getting a little bit lost here.  In the example you sent based on
Harold's discussion, the dimension of x is 3 and the dimension of u is
10 so Sigma is a 13 by 13 matrix, right?  S_xx is 3 by 3 and L_uu is
10 by 10.  To form the product S_xx*uhat_x I think that uhat_x needs
to be 3 by 10.  Is that right?  (I'm used to writing the Jacobian of a
vector-valued function of a vector the other way around.)

It looks like you are missing a _x in the first term in "uhat' * S_xx * uhat_x".

To evaluate L_uu you need a value of x.  I assume you will use the
parameter estimates. Correct?

Will the parameterization of x affect the result?  If I write the
model in terms of the logarithms of the variances instead of the
variances I will definitely get a different Sigma but will the result
for a linear combination of mu and some of the u's stay invariant?  If
it isn't invariant, how do I choose the parameterization of the
variance components?

Can you give a bit more detail on how you justify mixing derivatives
of the marginal log-likelihood (F) with derivatives of the conditional
density (L).  Do you know that these are on the same scale?  I'm
willing to believe that they are - it is just that I can't see right
off why they should be.



From otter at otter-rsch.com  Fri Oct 19 12:10:27 2007
From: otter at otter-rsch.com (dave fournier)
Date: Fri, 19 Oct 2007 03:10:27 -0700
Subject: [R-sig-ME]  [R] coef se in lme
References: <471711FF.4070007@otter-rsch.com>
Message-ID: <47188293.9070502@otter-rsch.com>

ouglas Bates wrote:
 > On 10/18/07, dave fournier <otter at otter-rsch.com> wrote:
 >
 >
 >> Here is one approach to this problem.
 >>
 >
 >
 >> In the AD Model Builder Random Effects package we provide estimated
 >> standard deviations for any function of the fixed and random effects,
 >> (here I include the parameters which detemine the covarince matrices if
 >> present) and the random effects. This is for general nonlinear random
 >> effects models, but the calculations can be used for linear models as
 >> well. We calculate these estimates as follows. Let L(x,u)
 >> be the log-likelihood function for the parameters x and u given the
 >> observed data,
 >> where u is the vector of random effects and x is the vector of the other
 >> parameters.
 >>
 >
 > I know it may sound pedantic but I don't know what a log-likelihood
 > L(x,u) would be because you are treating parameters and the random
 > effects as if they are the same type of object and they're not.  If
 > you want to use a Bayesian approach you can kind of level the playing
 > field and say that everything is a parameter except for the observed
 > data values.  However, Bayesians also need to come up with a prior and
 > that isn't trivial in this case, as I tried to indicate in my message
 > about the mcmcsamp chain taking excursions.
 >
 > I find I can easily confuse myself in the theory of the maximum
 > likelihood or REML estimates if I am not careful about the terminology
 > and the roles of the different coefficients in the linear predictor.
 > I think I would call that function L(x,u) the conditional density of
 > the random effects given the data.  The parameters determine the joint
 > density for the random effects and responses so plugging the observed
 > values of the responses into this expression yields the conditional
 > density of the random effects
 >
OK we will call them that.

 >> Let F(x) be the log-likelihood for x after the u have been
 >> integrated out. This integration might be exact or more commonly via the
 >> Laplace approximation or something else.
 >> For any x let uhat(x) be the value of u which maximizes L(x,u),
 >>
 >
 > I think that is what I would call the conditional modes of the random
 > effects.  These depend on the observed responses and the model
 > parameters.
 >

   That's right.
 >
 >> and let xhat be the value of x which maximizes F(x).
 >>
 >
 >
 >> The estimate for the covariance matrix for the x is then
 >> S_xx = inv(F_xx) and the estimated full covariance matrix Sigma for the
 >> x and u is given by
 >>
 >
 >
 >> S_xx                 S_xx * uhat_x
 >> (S_xx * uhat_x)' uhat' * S_xx * uhat_x + inv(L_uu)
 >>
 >
 >
 >> where ' denotes transpose _x denotes first derivative wrt x (note that
 >> uhat is a function of x so that uhat_x makes sense) and _xx _uu denote
 >> the second derivatives wrt x and u. we then use Sigma and the delta
 >> method to estimate the standard deviation of any (differentiable)
 >> function of x and u.
 >>
 >
 > I'm getting a little bit lost here.  In the example you sent based on
 > Harold's discussion, the dimension of x is 3 and the dimension of u is
 > 10 so Sigma is a 13 by 13 matrix,
that is right.

 > right?  S_xx is 3 by 3 and L_uu is
 > 10 by 10.  To form the product S_xx*uhat_x I think that uhat_x needs
 > to be 3 by 10.  Is that right?  (I'm used to writing the Jacobian of a
 > vector-valued function of a vector the other way around.)
 >
Yes it is either 3x10 or 10x3.   Since S_xx is 10 by 10 if one wirtes 
S_xx * uhat_x  uhat_x must be 10 by 3
i.e 10 rows and 3 columns.

 > It looks like you are missing a _x in the first term in "uhat' * S_xx 
* uhat_x".
 >
 >
That is right it should be  uhat_x' * S_xx * uhat_x
 > To evaluate L_uu you need a value of x.  I assume you will use the
 > parameter estimates. Correct?
 >
 >
Yes the derivatives are evaluated at  xhat,uhat.

 > Will the parameterization of x affect the result?
No.    supposte that   we have a reparamseterization  of the x's given by

   x=g(y)

  then wrt y the function is   F(g(y)) so the Hessian wrt y is

                    g_y' * F_xx * g_y

This is due to the fact that F_x=0.  Taking the inverse gives

                inv( g_y) * inv(F_xx) * inv(g_y')

so that when you do the delta method the g_y matrices cancel out.
 > If I write the
 > model in terms of the logarithms of the variances instead of the
 > variances I will definitely get a different Sigma
Yes but if you then use to delta method to construct the covariance 
matrix for the depdent variables
exp(log_sigmas) you will recover the original. This is just the chain rule.

 > but will the result
 > for a linear combination of mu and some of the u's stay invariant?  If
 > it isn't invariant, how do I choose the parameterization of the
 > variance components?
 >
Same as above. the whole thing is independent of how one parameterizes 
the x's.
 > Can you give a bit more detail on how you justify mixing derivatives
 > of the marginal log-likelihood (F) with derivatives of the conditional
 > density (L).  Do you know that these are on the same scale?  I'm
 > willing to believe that they are - it is just that I can't see right
 > off why they should be.
 >
 >
Think of it this way.  You can use the marginal likelihood Hessian F_xx 
to construct the estimated covariance matrix
for the independent variables x. Then via the delta  method you 
calculate the covariance matrix for the dependent variables
uhat(x). So far this is just pure frequentist stuff. Now if you know 
what that uhat actually are inv(L_uu) is an estimate for the
variance of the u's.  So a simple estimate of the overall variance of 
the u's is to add the two together. So
uhat_x' * S * uhat_x is the extra variance in the u's introduced by the 
uncertainty in the true value of the uhat
due to uncertainty in the true value of the x's.


-- 
David A. Fournier
P.O. Box 2040, Sidney, B.C. V8l 3S3
Canada
Phone/FAX 250-655-3364
http://otter-rsch.com


-- 
David A. Fournier
P.O. Box 2040,
Sidney, B.C. V8l 3S3
Canada
Phone/FAX 250-655-3364
http://otter-rsch.com



From David.Duffy at qimr.edu.au  Fri Oct 19 04:06:37 2007
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Fri, 19 Oct 2007 12:06:37 +1000 (EST)
Subject: [R-sig-ME] [R] coef se in lme
In-Reply-To: <40e66e0b0710181503x6c3fec49s509d683c953b61e8@mail.gmail.com>
References: <2323A6D37908A847A7C32F1E3662C80EB5E70B@dc1ex01.air.org><471711FF.4070007@otter-rsch.com>
	<40e66e0b0710181503x6c3fec49s509d683c953b61e8@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0710191058240.4460@orpheus.qimr.edu.au>

On Thu, 18 Oct 2007, Douglas Bates wrote:

> On 10/18/07, dave fournier <otter at otter-rsch.com> wrote:
>
>> In the AD Model Builder Random Effects package we provide estimated
>> standard deviations for any function of the fixed and random effects,
>> (here I include the parameters which detemine the covarince matrices if
>> present) and the random effects. This is for general nonlinear random
>> effects models, but the calculations can be used for linear models as
>> well. We calculate these estimates as follows. Let L(x,u)
>> be the log-likelihood function for the parameters x and u given the
>> observed data,
>> where u is the vector of random effects and x is the vector of the other
>> parameters.
>
> I know it may sound pedantic but I don't know what a log-likelihood
> L(x,u) would be because you are treating parameters and the random
> effects as if they are the same type of object and they're not.  If
>
>> Let F(x) be the log-likelihood for x after the u have been
>> integrated out. This integration might be exact or more commonly via the
>> Laplace approximation or something else.
>> For any x let uhat(x) be the value of u which maximizes L(x,u),
>
> I think that is what I would call the conditional modes of the random
> effects.  These depend on the observed responses and the model
> parameters.
>
>> and let xhat be the value of x which maximizes F(x).
>
>> The estimate for the covariance matrix for the x is then
>> S_xx = inv(F_xx) and the estimated full covariance matrix Sigma for the
>> x and u is given by
>
>> S_xx                 S_xx * uhat_x
>> (S_xx * uhat_x)' uhat' * S_xx * uhat_x + inv(L_uu)
>
>> where ' denotes transpose _x denotes first derivative wrt x (note that
>> uhat is a function of x so that uhat_x makes sense) and _xx _uu denote
>> the second derivatives wrt x and u. we then use Sigma and the delta
>> method to estimate the standard deviation of any (differentiable)
>> function of x and u.
>
[Snip]
>
> Can you give a bit more detail on how you justify mixing derivatives
> of the marginal log-likelihood (F) with derivatives of the conditional
> density (L).  Do you know that these are on the same scale?  I'm
> willing to believe that they are - it is just that I can't see right
> off why they should be.
>

I find all of this is a bit above my head, but I do have a paper by Matt 
Wand _Fisher information for generalised linear mixed models_
Journal of Multivariate Analysis, (2007), 98, 1412-1416.

This looks at the canonical-link GLMM where the random effects are
Gaussian for a simple one-level/random intercepts model, and
ends rather abruptly ;) with

"Remark 2. Approximate standard errors for the maximum likelihood
estimates beta-hat and sigma-squared-hat can be obtained from the
diagonal entries of I(b-hat,s-hat2)^-1. However, as pointed out in Remark 1,
implementation is often hindered by intractable multivariate integrals.
Additionally, dependence among the entries of y induced by u means that
central limit theorems of the type: I (bhat,shat2)^-1 {(bhat,shat2)-(b,s2)
2)} converges in distribution to a N (0, I) random vector, have not been
established in general and, hence, interpretation of standard errors
is cloudy. Nevertheless, there are many special cases, such as
m-dependence when the data are from a longitudinal study, for which
central limit theorems can be established."

You'll have to read the paper which gives the derivation and formulae.


David Duffy.
-- 
| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v



From maechler at stat.math.ethz.ch  Fri Oct 19 09:23:10 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 19 Oct 2007 09:23:10 +0200
Subject: [R-sig-ME] change in variance components depending on scaling
	of fixed effects
In-Reply-To: <47171EFF.5020502@ed.ac.uk>
References: <47171EFF.5020502@ed.ac.uk>
Message-ID: <18200.23390.616219.292409@stat.math.ethz.ch>

>>>>> "AH" == Arild Husby <arild.husby at ed.ac.uk>
>>>>>     on Thu, 18 Oct 2007 09:53:19 +0100 writes:

    AH> Dear all,
    AH> I am trying to understand the output from a binomial lmer object and why 
    AH> the scaling of a fixed effect changes the variance components.

[....................... ]


    AH> All help is highly appreciated.

    AH> Thanks very much,
    AH> Arild Husby

    >> sessionInfo()
    AH> R version 2.5.0 (2007-04-23)
    	  	  -----
  [.......]

    AH> attached base packages:
    AH> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  
    AH>     "methods"   "base"   
    AH> other attached packages:
    AH> lme4      Matrix     lattice
    AH> "0.99875-2" "0.99875-2"   "0.15-11"
    	 ---------

Can you  upgrade to R 2.5.1 
    (at least! --- always do upgrade from x.y.0 to x.y.1  !!! )
or better 2.6.0 and then
    upgrade.packages() ?
This will upgrade your lme4 to version  0.99875-8 (important!) 
and Matrix to 0.999375-3 {also possibly quite important}

Also, (and even alternatively to upgrading) 
if you provided a  *reproducible*  example
[e.g. you could dput(.) part of your data frame, or better
 save() it and put the file up for downloading; or use
 set.seed() and create random data which exhibits the same "feature"]
"your audience" could at least see if the current versions of
lme4 give the same results or not..

Regards,
Martin Maechler, ETH Zurich



    AH> -- 
    AH> Arild Husby
    AH> Institute of Evolutionary Biology
    AH> Room 413, Ashworth Labs,
    AH> King's Buildings,
    AH> University of Edinburgh
    AH> EH9 3JT, UK

    AH> E-mail: arild.husby at ed.ac.uk
    AH> web: http://homepages.ed.ac.uk/loeske/arild.html
    AH> Tel: +44 (0)131 650 5990
    AH> Mob: +44 (0)798 275 0668

    AH> _______________________________________________
    AH> R-sig-mixed-models at r-project.org mailing list
    AH> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From njs at pobox.com  Fri Oct 19 10:44:15 2007
From: njs at pobox.com (Nathaniel Smith)
Date: Fri, 19 Oct 2007 01:44:15 -0700
Subject: [R-sig-ME] lmer() producing wildly inaccurate values for some GLM
	models
Message-ID: <20071019084415.GA18600@frances.vorpus.org>

Hello,

The data I am trying to model are positive and right-skewed (they're
measurements of fixation times recorded from an eye-tracker), so I've
been trying to use lmer's GLM fitting capability to model them.  In
particular, I've been investigating whether it makes most sense to
use something like a gamma or inverse gaussian distribution with an
identity link (since theory and examination of the data both suggest
that the mean of the response is in fact linear in my predictors of
interest).  I was hoping to use log likelihood to work out which
model made the most sense (e.g., using the approach of Vuong 1989),
but... something seemed funny about the numbers I was getting for the
likelihood, and also the random effects, so I did some testing.

lmer's log-likelihood calculations for these models appear to be
completely wrong.  In particular, the log likelihood that it
calculates seems to be many orders of magnitude smaller than it could
possibly be, and orders of magnitude smaller when fitting an inverse
gaussian distribution than when fitting a gamma distribution, even if
the data in fact are closer to a gamma distribution.

To demonstrate the problem, I generated some toy data where there the
true random effects are null.  This isn't necessary to cause the
problem (i.e., lmer isn't just breaking because the model is
over-specified); I did things this way because it makes the models
fit by lmer and glm more directly comparable.

This code takes 10,000 samples from the true model for a Gamma GLM
with E(y) = 200 + 3*x, identity link, and shape parameter 5, then
attempts to fit it in various ways:

---------------------------->8----------------------------

set.seed(0)
samples <- 10000
true.shape <- 5
x <- rgamma(samples, 2, scale=2)
i <- 200
b <- 3
true.scale <- (i + b*x)/true.shape
y <- rgamma(samples, true.shape, scale=true.scale)
subj <- factor(c(rep("a", samples/2), rep("b", samples/2)))
gaussian.lm <- lm(y ~ x)
gaussian.lmer <- lmer(y ~ x + (x | subj))
gamma.glm <- glm(y ~ x, family=Gamma(link="identity"))
gamma.lmer <- lmer(y ~ x + (x | subj), family=Gamma(link="identity"))
ig.glm <- glm(y ~ x, family=inverse.gaussian(link="identity"))
ig.lmer <- lmer(y ~ x + (x | subj),
                family=inverse.gaussian(link="identity"))
nums.glm <- function(model) c(coef(model), logLik=logLik(model))
nums.lmer <- function(model) c(fixef(model), logLik=logLik(model))
rbind(gaussian.lm=nums.glm(gaussian.lm), gaussian.lmer=nums.lmer(gaussian.lmer),
      gamma.glm=nums.glm(gamma.glm), gamma.lmer=nums.lmer(gamma.lmer),
      ig.glm=nums.glm(ig.glm), ig.lmer=nums.lmer(ig.lmer))

----------------------------8<----------------------------

Output:

---------------------------->8----------------------------

              (Intercept)        x        logLik
gaussian.lm      203.0480 2.686375 -59761.426766
gaussian.lmer    203.0279 2.693471 -59759.572897
gamma.glm        202.8143 2.745581 -59054.739626
gamma.lmer       202.8143 2.745464  -1025.060862
ig.glm           202.7024 2.774986 -59309.711506
ig.lmer          202.7024 2.774986     -5.788107

----------------------------8<----------------------------

So, as you can see:
  -- Every fitting function does well at recovering the correct
     regression coefficients (and furthermore there is excellent
     agreement between (g)lm and lmer -- lmer correctly identifies
     the lack of random effects in all cases).
  -- The (g)lm log-likelihoods are all plausible (also I checked the
     one for gamma by hand, and it is correct), and they correctly
     identify that this is a gamma rather than inverse-gaussian
     distribution
  -- The lmer log-likelihood for the gaussian model seems accurate
  -- The lmer log-likelihoods for the other two models are completely
     bonkers (I *wish* I had a model that achieved a log likelihood of
     -5.8 over 10,000 samples...)

The lmer's reported AIC, BIC, and deviance appear to be similarly
underestimated.


Additionally, the random effects calculated by lmer for these models
seem very fishy, but I'm not as confident in my analysis here,
because I'm not entirely sure my toy data is correctly sampling from
the true distribution that lmer assumes.  Anyway, here's the code,
which is attempting to create a model like that above, but this time
with 10 subjects, each of whose intercept and slope terms are sampled
independently from normal distributions with different variances --
400 and 1, respectively:

---------------------------->8----------------------------

set.seed(0)
subjects <- 10
samples <- 1000
true.shape <- 5
i <- 200
i.var <- 400
b <- 3
b.var <- 1
x <- NULL
subj <- NULL
true.scale <- NULL
for (s in 1:subjects) {
  subj <- c(subj, rep(s, samples))
  x.subj <- rgamma(samples, 2, scale=2)
  x <- c(x, x.subj)
  i.subj <- rnorm(1, i, sqrt(i.var))
  b.subj <- rnorm(1, b, sqrt(b.var))
  true.scale <- c(true.scale, (i.subj + b*x.subj)/true.shape)
}
subj <- factor(subj)
y <- rgamma(subjects * samples, true.shape, scale=true.scale)
gaussian.lm <- lm(y ~ x)
gaussian.lmer <- lmer(y ~ x + (x | subj))
gamma.glm <- glm(y ~ x, family=Gamma(link="identity"))
gamma.lmer <- lmer(y ~ x + (x | subj), family=Gamma(link="identity"))
ig.glm <- glm(y ~ x, family=inverse.gaussian(link="identity"))
ig.lmer <- lmer(y ~ x + (x | subj),
family=inverse.gaussian(link="identity"))
nums.glm <- function(model) c(coef(model), logLik=logLik(model))
nums.lmer <- function(model) c(fixef(model), logLik=logLik(model))
rbind(gaussian.lm=nums.glm(gaussian.lm), gaussian.lmer=nums.lmer(gaussian.lmer),
      gamma.glm=nums.glm(gamma.glm), gamma.lmer=nums.lmer(gamma.lmer),
      ig.glm=nums.glm(ig.glm), ig.lmer=nums.lmer(ig.lmer))
VarCorr(gaussian.lmer)$subj
VarCorr(gamma.lmer)$subj
VarCorr(ig.lmer)$subj

---------------------------->8----------------------------

Output:

---------------------------->8----------------------------

              (Intercept)        x        logLik
gaussian.lm      192.1295 2.636897 -59432.332233
gaussian.lmer    192.0867 2.648113 -59257.574632
gamma.glm        192.3558 2.579673 -58668.208775
gamma.lmer       192.1256 2.578616  -1038.636243
ig.glm           192.4572 2.553089 -58910.126512
ig.lmer          192.4572 2.553093     -6.340702
> VarCorr(gaussian.lmer)$subj
2 x 2 Matrix of class "dpoMatrix"
            (Intercept)          x
(Intercept)   252.21320 11.4298584
x              11.42986  0.5179811
> VarCorr(gamma.lmer)$subj
2 x 2 Matrix of class "dpoMatrix"
            (Intercept)          x
(Intercept)   41.109713 1.63892630
x              1.638926 0.06795769
> VarCorr(ig.lmer)$subj
2 x 2 Matrix of class "dpoMatrix"
              (Intercept)             x
(Intercept)  1.538172e-06 -8.630771e-19
x           -8.630771e-19  5.124452e-13

---------------------------->8----------------------------

The first part just duplicates what I found above, to show that it
still arises with models that are not overspecified.

The second part is the variance-covariance matrices -- in no case are
the estimates anywhere near accurate, and for the inverse gaussian
model it somehow has decided that there are effectively no random
effects at all.  I see the same behavior on my real data -- that is,
random effects for the inverse gaussian model coming out to
effectively zero.  I can't say if the estimates for the gamma model
are also incorrect, because I don't know what the true values are
:-).  But this is not giving me confidence that the values I'm getting
are anywhere near true, either :-(.

This is all with R 2.6.0 on x86-64 linux, with lme4 0.99875-8.

Help!
-- Nathaniel

-- 
So let us espouse a less contested notion of truth and falsehood, even
if it is philosophically debatable (if we listen to philosophers, we
must debate everything, and there would be no end to the discussion).
  -- Serendipities, Umberto Eco



From lamprianou at yahoo.com  Fri Oct 19 12:03:59 2007
From: lamprianou at yahoo.com (Iasonas Lamprianou)
Date: Fri, 19 Oct 2007 03:03:59 -0700 (PDT)
Subject: [R-sig-ME] R-sig-mixed-models Digest, Vol 10, Issue 16
Message-ID: <914677.43219.qm@web54111.mail.re2.yahoo.com>

Dear friends,
I had sent the following email:
Dear friends,
I am using lmer to run a binomial multilevel model. However, it takes approximately 8 hours to run the model. I was forced to download MLWin and I tried it. It takes only 3 to 4 minutes to converge. Am I doing something wrong with lmer? This is the model I try to fit. 

m1 <- lmer(score ~ 1+item+(1|final_id), data=malt, family=binomial,method="PQL", 
+ control=list(gradient = FALSE, msVerbose=TRUE))

I also attach the dataset in case somebody want to replicate my model.


However, I received no responses, does anyone have an answer to my question?

jason
 
Dr. Iasonas Lamprianou
Department of Education
The University of Manchester
Oxford Road, Manchester M13 9PL, UK
Tel. 0044 161 275 3485
iasonas.lamprianou at manchester.ac.uk


----- Original Message ----
From: "r-sig-mixed-models-request at r-project.org" <r-sig-mixed-models-request at r-project.org>
To: r-sig-mixed-models at r-project.org
Sent: Friday, 19 October, 2007 11:44:28 AM
Subject: R-sig-mixed-models Digest, Vol 10, Issue 16

Send R-sig-mixed-models mailing list submissions to
    r-sig-mixed-models at r-project.org

To subscribe or unsubscribe via the World Wide Web, visit
    https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
or, via email, send a message with subject or body 'help' to
    r-sig-mixed-models-request at r-project.org

You can reach the person managing the list at
    r-sig-mixed-models-owner at r-project.org

When replying, please edit your Subject line so it is more specific
than "Re: Contents of R-sig-mixed-models digest..."


Today's Topics:

  1.  [R] coef se in lme (dave fournier)
  2. Re: [R] coef se in lme (Douglas Bates)
  3.  [R] coef se in lme (dave fournier)
  4. Re: [R] coef se in lme (David Duffy)
  5. Re: change in variance components depending on scaling    of
      fixed effects (Martin Maechler)
  6. lmer() producing wildly inaccurate values for some GLM    models
      (Nathaniel Smith)


----------------------------------------------------------------------

Message: 1
Date: Thu, 18 Oct 2007 22:32:30 -0700
From: dave fournier <otter at otter-rsch.com>
Subject: [R-sig-ME]  [R] coef se in lme
To: r-sig-mixed-models at r-project.org
Message-ID: <4718416E.6010002 at otter-rsch.com>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

The AD Model Builder method I posted earlier takes into account the
uncertainty in the mean and both std deviations in Harold's simple model.

        Y_ij - mu + u_i +eps_ij

To illustrate this
I built a little model and simulated a data set with 1<=i<=10, 1<=j<=5 
observations.  Below are the parameter estiamtes tigether with their
estimated std devs.  The true values were mu=3.0 sigma_u=2.0 and sigma=3.0



>    1  mu          2.4091e+00 7.4307e-01
>    2  log_sigma_u  6.3908e-01 3.4913e-01
>    3  log_sigma    1.1354e+00 1.1180e-01
>    4  u          -3.1695e-01 6.4561e-01
>    5  u            2.0441e-01 6.4496e-01
>    6  u            5.1996e-01 6.4748e-01
>    7  u            4.3754e-01 6.4661e-01
>    8  u            4.8381e-01 6.4708e-01
>    9  u            1.2214e+00 6.6075e-01
>    10  u          -1.7631e+00 6.7793e-01
>    11  u            3.3987e-01 6.4578e-01
>    12  u          -1.7663e-01 6.4485e-01
>    13  u          -9.5030e-01 6.5439e-01

then I fixed log_sigma_u and log_sigma at their
estimated values and obtained.
> # fixed sd's
>  index  name  value      std dev
>      1  mu  2.4093e+00 7.4307e-01
>      2  u  -3.1736e-01 6.4407e-01
>      3  u  2.0456e-01 6.4407e-01
>      4  u  5.2045e-01 6.4407e-01
>      5  u  4.3794e-01 6.4407e-01
>      6  u  4.8426e-01 6.4407e-01
>      7  u  1.2226e+00 6.4407e-01
>      8  u  -1.7650e+00 6.4407e-01
>      9  u  3.4016e-01 6.4407e-01
>    10  u  -1.7689e-01 6.4407e-01
>    11  u  -9.5138e-01 6.4407e-01
and finally I almost fixed mu
(can't fix it completley because then there would be
no "fixed" effects and the model thinks there is
nothing to do.) and obtained

> # all fixed
>  index  name  value      std dev
>      1  mu  2.4093e+00 2.2361e-03
>      2  u  -3.1736e-01 5.9145e-01
>      3  u  2.0456e-01 5.9145e-01
>      4  u  5.2045e-01 5.9145e-01
>      5  u  4.3794e-01 5.9145e-01
>      6  u  4.8426e-01 5.9145e-01
>      7  u  1.2226e+00 5.9145e-01
>      8  u  -1.7650e+00 5.9145e-01
>      9  u  3.4016e-01 5.9145e-01
>    10  u  -1.7689e-01 5.9145e-01
>    11  u  -9.5138e-01 5.9145e-01
So for example u(1) the first random effect has estimated std devs

  6.4561e-01, 6.4407e-01, and 5.9145e-01


under the three models. It appears that most of the "extra" uncertainty 
in u(1) comes from the uncertainty in mu.

-- 
David A. Fournier
P.O. Box 2040,
Sidney, B.C. V8l 3S3
Canada
Phone/FAX 250-655-3364
http://otter-rsch.com



------------------------------

Message: 2
Date: Thu, 18 Oct 2007 17:03:05 -0500
From: "Douglas Bates" <bates at stat.wisc.edu>
Subject: Re: [R-sig-ME] [R] coef se in lme
To: davef at otter-rsch.com
Cc: r-sig-mixed-models at r-project.org
Message-ID:
    <40e66e0b0710181503x6c3fec49s509d683c953b61e8 at mail.gmail.com>
Content-Type: text/plain; charset=ISO-8859-1

On 10/18/07, dave fournier <otter at otter-rsch.com> wrote:

> Here is one approach to this problem.

> In the AD Model Builder Random Effects package we provide estimated
> standard deviations for any function of the fixed and random effects,
> (here I include the parameters which detemine the covarince matrices if
> present) and the random effects. This is for general nonlinear random
> effects models, but the calculations can be used for linear models as
> well. We calculate these estimates as follows. Let L(x,u)
> be the log-likelihood function for the parameters x and u given the
> observed data,
> where u is the vector of random effects and x is the vector of the other
> parameters.

I know it may sound pedantic but I don't know what a log-likelihood
L(x,u) would be because you are treating parameters and the random
effects as if they are the same type of object and they're not.  If
you want to use a Bayesian approach you can kind of level the playing
field and say that everything is a parameter except for the observed
data values.  However, Bayesians also need to come up with a prior and
that isn't trivial in this case, as I tried to indicate in my message
about the mcmcsamp chain taking excursions.

I find I can easily confuse myself in the theory of the maximum
likelihood or REML estimates if I am not careful about the terminology
and the roles of the different coefficients in the linear predictor.
I think I would call that function L(x,u) the conditional density of
the random effects given the data.  The parameters determine the joint
density for the random effects and responses so plugging the observed
values of the responses into this expression yields the conditional
density of the random effects.

> Let F(x) be the log-likelihood for x after the u have been
> integrated out. This integration might be exact or more commonly via the
> Laplace approximation or something else.
> For any x let uhat(x) be the value of u which maximizes L(x,u),

I think that is what I would call the conditional modes of the random
effects.  These depend on the observed responses and the model
parameters.

> and let xhat be the value of x which maximizes F(x).

> The estimate for the covariance matrix for the x is then
> S_xx = inv(F_xx) and the estimated full covariance matrix Sigma for the
> x and u is given by

> S_xx                S_xx * uhat_x
> (S_xx * uhat_x)' uhat' * S_xx * uhat_x + inv(L_uu)

> where ' denotes transpose _x denotes first derivative wrt x (note that
> uhat is a function of x so that uhat_x makes sense) and _xx _uu denote
> the second derivatives wrt x and u. we then use Sigma and the delta
> method to estimate the standard deviation of any (differentiable)
> function of x and u.

I'm getting a little bit lost here.  In the example you sent based on
Harold's discussion, the dimension of x is 3 and the dimension of u is
10 so Sigma is a 13 by 13 matrix, right?  S_xx is 3 by 3 and L_uu is
10 by 10.  To form the product S_xx*uhat_x I think that uhat_x needs
to be 3 by 10.  Is that right?  (I'm used to writing the Jacobian of a
vector-valued function of a vector the other way around.)

It looks like you are missing a _x in the first term in "uhat' * S_xx * uhat_x".

To evaluate L_uu you need a value of x.  I assume you will use the
parameter estimates. Correct?

Will the parameterization of x affect the result?  If I write the
model in terms of the logarithms of the variances instead of the
variances I will definitely get a different Sigma but will the result
for a linear combination of mu and some of the u's stay invariant?  If
it isn't invariant, how do I choose the parameterization of the
variance components?

Can you give a bit more detail on how you justify mixing derivatives
of the marginal log-likelihood (F) with derivatives of the conditional
density (L).  Do you know that these are on the same scale?  I'm
willing to believe that they are - it is just that I can't see right
off why they should be.



------------------------------

Message: 3
Date: Fri, 19 Oct 2007 03:10:27 -0700
From: dave fournier <otter at otter-rsch.com>
Subject: [R-sig-ME]  [R] coef se in lme
To: r-sig-mixed-models at r-project.org
Message-ID: <47188293.9070502 at otter-rsch.com>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

ouglas Bates wrote:
> On 10/18/07, dave fournier <otter at otter-rsch.com> wrote:
>
>
>> Here is one approach to this problem.
>>
>
>
>> In the AD Model Builder Random Effects package we provide estimated
>> standard deviations for any function of the fixed and random effects,
>> (here I include the parameters which detemine the covarince matrices if
>> present) and the random effects. This is for general nonlinear random
>> effects models, but the calculations can be used for linear models as
>> well. We calculate these estimates as follows. Let L(x,u)
>> be the log-likelihood function for the parameters x and u given the
>> observed data,
>> where u is the vector of random effects and x is the vector of the other
>> parameters.
>>
>
> I know it may sound pedantic but I don't know what a log-likelihood
> L(x,u) would be because you are treating parameters and the random
> effects as if they are the same type of object and they're not.  If
> you want to use a Bayesian approach you can kind of level the playing
> field and say that everything is a parameter except for the observed
> data values.  However, Bayesians also need to come up with a prior and
> that isn't trivial in this case, as I tried to indicate in my message
> about the mcmcsamp chain taking excursions.
>
> I find I can easily confuse myself in the theory of the maximum
> likelihood or REML estimates if I am not careful about the terminology
> and the roles of the different coefficients in the linear predictor.
> I think I would call that function L(x,u) the conditional density of
> the random effects given the data.  The parameters determine the joint
> density for the random effects and responses so plugging the observed
> values of the responses into this expression yields the conditional
> density of the random effects
>
OK we will call them that.

>> Let F(x) be the log-likelihood for x after the u have been
>> integrated out. This integration might be exact or more commonly via the
>> Laplace approximation or something else.
>> For any x let uhat(x) be the value of u which maximizes L(x,u),
>>
>
> I think that is what I would call the conditional modes of the random
> effects.  These depend on the observed responses and the model
> parameters.
>

  That's right.
>
>> and let xhat be the value of x which maximizes F(x).
>>
>
>
>> The estimate for the covariance matrix for the x is then
>> S_xx = inv(F_xx) and the estimated full covariance matrix Sigma for the
>> x and u is given by
>>
>
>
>> S_xx                S_xx * uhat_x
>> (S_xx * uhat_x)' uhat' * S_xx * uhat_x + inv(L_uu)
>>
>
>
>> where ' denotes transpose _x denotes first derivative wrt x (note that
>> uhat is a function of x so that uhat_x makes sense) and _xx _uu denote
>> the second derivatives wrt x and u. we then use Sigma and the delta
>> method to estimate the standard deviation of any (differentiable)
>> function of x and u.
>>
>
> I'm getting a little bit lost here.  In the example you sent based on
> Harold's discussion, the dimension of x is 3 and the dimension of u is
> 10 so Sigma is a 13 by 13 matrix,
that is right.

> right?  S_xx is 3 by 3 and L_uu is
> 10 by 10.  To form the product S_xx*uhat_x I think that uhat_x needs
> to be 3 by 10.  Is that right?  (I'm used to writing the Jacobian of a
> vector-valued function of a vector the other way around.)
>
Yes it is either 3x10 or 10x3.  Since S_xx is 10 by 10 if one wirtes 
S_xx * uhat_x  uhat_x must be 10 by 3
i.e 10 rows and 3 columns.

> It looks like you are missing a _x in the first term in "uhat' * S_xx 
* uhat_x".
>
>
That is right it should be  uhat_x' * S_xx * uhat_x
> To evaluate L_uu you need a value of x.  I assume you will use the
> parameter estimates. Correct?
>
>
Yes the derivatives are evaluated at  xhat,uhat.

> Will the parameterization of x affect the result?
No.    supposte that  we have a reparamseterization  of the x's given by

  x=g(y)

  then wrt y the function is  F(g(y)) so the Hessian wrt y is

                    g_y' * F_xx * g_y

This is due to the fact that F_x=0.  Taking the inverse gives

                inv( g_y) * inv(F_xx) * inv(g_y')

so that when you do the delta method the g_y matrices cancel out.
> If I write the
> model in terms of the logarithms of the variances instead of the
> variances I will definitely get a different Sigma
Yes but if you then use to delta method to construct the covariance 
matrix for the depdent variables
exp(log_sigmas) you will recover the original. This is just the chain rule.

> but will the result
> for a linear combination of mu and some of the u's stay invariant?   If
> it isn't invariant, how do I choose the parameterization of the
> variance components?
>
Same as above. the whole thing is independent of how one parameterizes 
the x's.
> Can you give a bit more detail on how you justify mixing derivatives
> of the marginal log-likelihood (F) with derivatives of the conditional
> density (L).  Do you know that these are on the same scale?  I'm
> willing to believe that they are - it is just that I can't see right
> off why they should be.
>
>
Think of it this way.  You can use the marginal likelihood Hessian F_xx 
to construct the estimated covariance matrix
for the independent variables x. Then via the delta  method you 
calculate the covariance matrix for the dependent variables
uhat(x). So far this is just pure frequentist stuff. Now if you know 
what that uhat actually are inv(L_uu) is an estimate for the
variance of the u's.  So a simple estimate of the overall variance of 
the u's is to add the two together. So
uhat_x' * S * uhat_x is the extra variance in the u's introduced by the 
uncertainty in the true value of the uhat
due to uncertainty in the true value of the x's.


-- 
David A. Fournier
P.O. Box 2040, Sidney, B.C. V8l 3S3
Canada
Phone/FAX 250-655-3364
http://otter-rsch.com


-- 
David A. Fournier
P.O. Box 2040,
Sidney, B.C. V8l 3S3
Canada
Phone/FAX 250-655-3364
http://otter-rsch.com



------------------------------

Message: 4
Date: Fri, 19 Oct 2007 12:06:37 +1000 (EST)
From: David Duffy <David.Duffy at qimr.edu.au>
Subject: Re: [R-sig-ME] [R] coef se in lme
To: Douglas Bates <bates at stat.wisc.edu>
Cc: r-sig-mixed-models at r-project.org, davef at otter-rsch.com
Message-ID: <Pine.LNX.4.64.0710191058240.4460 at orpheus.qimr.edu.au>
Content-Type: TEXT/PLAIN; charset=US-ASCII; format=flowed

On Thu, 18 Oct 2007, Douglas Bates wrote:

> On 10/18/07, dave fournier <otter at otter-rsch.com> wrote:
>
>> In the AD Model Builder Random Effects package we provide estimated
>> standard deviations for any function of the fixed and random effects,
>> (here I include the parameters which detemine the covarince matrices if
>> present) and the random effects. This is for general nonlinear random
>> effects models, but the calculations can be used for linear models as
>> well. We calculate these estimates as follows. Let L(x,u)
>> be the log-likelihood function for the parameters x and u given the
>> observed data,
>> where u is the vector of random effects and x is the vector of the other
>> parameters.
>
> I know it may sound pedantic but I don't know what a log-likelihood
> L(x,u) would be because you are treating parameters and the random
> effects as if they are the same type of object and they're not.  If
>
>> Let F(x) be the log-likelihood for x after the u have been
>> integrated out. This integration might be exact or more commonly via the
>> Laplace approximation or something else.
>> For any x let uhat(x) be the value of u which maximizes L(x,u),
>
> I think that is what I would call the conditional modes of the random
> effects.  These depend on the observed responses and the model
> parameters.
>
>> and let xhat be the value of x which maximizes F(x).
>
>> The estimate for the covariance matrix for the x is then
>> S_xx = inv(F_xx) and the estimated full covariance matrix Sigma for the
>> x and u is given by
>
>> S_xx                S_xx * uhat_x
>> (S_xx * uhat_x)' uhat' * S_xx * uhat_x + inv(L_uu)
>
>> where ' denotes transpose _x denotes first derivative wrt x (note that
>> uhat is a function of x so that uhat_x makes sense) and _xx _uu denote
>> the second derivatives wrt x and u. we then use Sigma and the delta
>> method to estimate the standard deviation of any (differentiable)
>> function of x and u.
>
[Snip]
>
> Can you give a bit more detail on how you justify mixing derivatives
> of the marginal log-likelihood (F) with derivatives of the conditional
> density (L).  Do you know that these are on the same scale?  I'm
> willing to believe that they are - it is just that I can't see right
> off why they should be.
>

I find all of this is a bit above my head, but I do have a paper by Matt 
Wand _Fisher information for generalised linear mixed models_
Journal of Multivariate Analysis, (2007), 98, 1412-1416.

This looks at the canonical-link GLMM where the random effects are
Gaussian for a simple one-level/random intercepts model, and
ends rather abruptly ;) with

"Remark 2. Approximate standard errors for the maximum likelihood
estimates beta-hat and sigma-squared-hat can be obtained from the
diagonal entries of I(b-hat,s-hat2)^-1. However, as pointed out in Remark 1,
implementation is often hindered by intractable multivariate integrals.
Additionally, dependence among the entries of y induced by u means that
central limit theorems of the type: I (bhat,shat2)^-1 {(bhat,shat2)-(b,s2)
2)} converges in distribution to a N (0, I) random vector, have not been
established in general and, hence, interpretation of standard errors
is cloudy. Nevertheless, there are many special cases, such as
m-dependence when the data are from a longitudinal study, for which
central limit theorems can be established."

You'll have to read the paper which gives the derivation and formulae.


David Duffy.
-- 
| David Duffy (MBBS PhD)                                        ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /    *
| Epidemiology Unit, Queensland Institute of Medical Research  \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v



------------------------------

Message: 5
Date: Fri, 19 Oct 2007 09:23:10 +0200
From: Martin Maechler <maechler at stat.math.ethz.ch>
Subject: Re: [R-sig-ME] change in variance components depending on
    scaling    of fixed effects
To: Arild Husby <arild.husby at ed.ac.uk>
Cc: R-SIG-Mixed-Models <r-sig-mixed-models at r-project.org>
Message-ID: <18200.23390.616219.292409 at stat.math.ethz.ch>
Content-Type: text/plain; charset=us-ascii

>>>>> "AH" == Arild Husby <arild.husby at ed.ac.uk>
>>>>>    on Thu, 18 Oct 2007 09:53:19 +0100 writes:

    AH> Dear all,
    AH> I am trying to understand the output from a binomial lmer object and why 
    AH> the scaling of a fixed effect changes the variance components.

[....................... ]


    AH> All help is highly appreciated.

    AH> Thanks very much,
    AH> Arild Husby

    >> sessionInfo()
    AH> R version 2.5.0 (2007-04-23)
                -----
  [.......]

    AH> attached base packages:
    AH> [1] "stats"    "graphics"  "grDevices" "utils"    "datasets"   
    AH>    "methods"  "base"  
    AH> other attached packages:
    AH> lme4      Matrix    lattice
    AH> "0.99875-2" "0.99875-2"  "0.15-11"
        ---------

Can you  upgrade to R 2.5.1 
    (at least! --- always do upgrade from x.y.0 to x.y.1  !!! )
or better 2.6.0 and then
    upgrade.packages() ?
This will upgrade your lme4 to version  0.99875-8 (important!) 
and Matrix to 0.999375-3 {also possibly quite important}

Also, (and even alternatively to upgrading) 
if you provided a  *reproducible*  example
[e.g. you could dput(.) part of your data frame, or better
save() it and put the file up for downloading; or use
set.seed() and create random data which exhibits the same "feature"]
"your audience" could at least see if the current versions of
lme4 give the same results or not..

Regards,
Martin Maechler, ETH Zurich



    AH> -- 
    AH> Arild Husby
    AH> Institute of Evolutionary Biology
    AH> Room 413, Ashworth Labs,
    AH> King's Buildings,
    AH> University of Edinburgh
    AH> EH9 3JT, UK

    AH> E-mail: arild.husby at ed.ac.uk
    AH> web: http://homepages.ed.ac.uk/loeske/arild.html
    AH> Tel: +44 (0)131 650 5990
    AH> Mob: +44 (0)798 275 0668

    AH> _______________________________________________
    AH> R-sig-mixed-models at r-project.org mailing list
    AH> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



------------------------------

Message: 6
Date: Fri, 19 Oct 2007 01:44:15 -0700
From: Nathaniel Smith <njs at pobox.com>
Subject: [R-sig-ME] lmer() producing wildly inaccurate values for some
    GLM    models
To: R-SIG-Mixed-Models <r-sig-mixed-models at r-project.org>
Message-ID: <20071019084415.GA18600 at frances.vorpus.org>
Content-Type: text/plain; charset=us-ascii

Hello,

The data I am trying to model are positive and right-skewed (they're
measurements of fixation times recorded from an eye-tracker), so I've
been trying to use lmer's GLM fitting capability to model them.  In
particular, I've been investigating whether it makes most sense to
use something like a gamma or inverse gaussian distribution with an
identity link (since theory and examination of the data both suggest
that the mean of the response is in fact linear in my predictors of
interest).  I was hoping to use log likelihood to work out which
model made the most sense (e.g., using the approach of Vuong 1989),
but... something seemed funny about the numbers I was getting for the
likelihood, and also the random effects, so I did some testing.

lmer's log-likelihood calculations for these models appear to be
completely wrong.  In particular, the log likelihood that it
calculates seems to be many orders of magnitude smaller than it could
possibly be, and orders of magnitude smaller when fitting an inverse
gaussian distribution than when fitting a gamma distribution, even if
the data in fact are closer to a gamma distribution.

To demonstrate the problem, I generated some toy data where there the
true random effects are null.  This isn't necessary to cause the
problem (i.e., lmer isn't just breaking because the model is
over-specified); I did things this way because it makes the models
fit by lmer and glm more directly comparable.

This code takes 10,000 samples from the true model for a Gamma GLM
with E(y) = 200 + 3*x, identity link, and shape parameter 5, then
attempts to fit it in various ways:

---------------------------->8----------------------------

set.seed(0)
samples <- 10000
true.shape <- 5
x <- rgamma(samples, 2, scale=2)
i <- 200
b <- 3
true.scale <- (i + b*x)/true.shape
y <- rgamma(samples, true.shape, scale=true.scale)
subj <- factor(c(rep("a", samples/2), rep("b", samples/2)))
gaussian.lm <- lm(y ~ x)
gaussian.lmer <- lmer(y ~ x + (x | subj))
gamma.glm <- glm(y ~ x, family=Gamma(link="identity"))
gamma.lmer <- lmer(y ~ x + (x | subj), family=Gamma(link="identity"))
ig.glm <- glm(y ~ x, family=inverse.gaussian(link="identity"))
ig.lmer <- lmer(y ~ x + (x | subj),
                family=inverse.gaussian(link="identity"))
nums.glm <- function(model) c(coef(model), logLik=logLik(model))
nums.lmer <- function(model) c(fixef(model), logLik=logLik(model))
rbind(gaussian.lm=nums.glm(gaussian.lm), gaussian.lmer=nums.lmer(gaussian.lmer),
      gamma.glm=nums.glm(gamma.glm), gamma.lmer=nums.lmer(gamma.lmer),
      ig.glm=nums.glm(ig.glm), ig.lmer=nums.lmer(ig.lmer))

----------------------------8<----------------------------

Output:

---------------------------->8----------------------------

              (Intercept)        x        logLik
gaussian.lm      203.0480 2.686375 -59761.426766
gaussian.lmer    203.0279 2.693471 -59759.572897
gamma.glm        202.8143 2.745581 -59054.739626
gamma.lmer      202.8143 2.745464  -1025.060862
ig.glm          202.7024 2.774986 -59309.711506
ig.lmer          202.7024 2.774986    -5.788107

----------------------------8<----------------------------

So, as you can see:
  -- Every fitting function does well at recovering the correct
    regression coefficients (and furthermore there is excellent
    agreement between (g)lm and lmer -- lmer correctly identifies
    the lack of random effects in all cases).
  -- The (g)lm log-likelihoods are all plausible (also I checked the
    one for gamma by hand, and it is correct), and they correctly
    identify that this is a gamma rather than inverse-gaussian
    distribution
  -- The lmer log-likelihood for the gaussian model seems accurate
  -- The lmer log-likelihoods for the other two models are completely
    bonkers (I *wish* I had a model that achieved a log likelihood of
    -5.8 over 10,000 samples...)

The lmer's reported AIC, BIC, and deviance appear to be similarly
underestimated.


Additionally, the random effects calculated by lmer for these models
seem very fishy, but I'm not as confident in my analysis here,
because I'm not entirely sure my toy data is correctly sampling from
the true distribution that lmer assumes.  Anyway, here's the code,
which is attempting to create a model like that above, but this time
with 10 subjects, each of whose intercept and slope terms are sampled
independently from normal distributions with different variances --
400 and 1, respectively:

---------------------------->8----------------------------

set.seed(0)
subjects <- 10
samples <- 1000
true.shape <- 5
i <- 200
i.var <- 400
b <- 3
b.var <- 1
x <- NULL
subj <- NULL
true.scale <- NULL
for (s in 1:subjects) {
  subj <- c(subj, rep(s, samples))
  x.subj <- rgamma(samples, 2, scale=2)
  x <- c(x, x.subj)
  i.subj <- rnorm(1, i, sqrt(i.var))
  b.subj <- rnorm(1, b, sqrt(b.var))
  true.scale <- c(true.scale, (i.subj + b*x.subj)/true.shape)
}
subj <- factor(subj)
y <- rgamma(subjects * samples, true.shape, scale=true.scale)
gaussian.lm <- lm(y ~ x)
gaussian.lmer <- lmer(y ~ x + (x | subj))
gamma.glm <- glm(y ~ x, family=Gamma(link="identity"))
gamma.lmer <- lmer(y ~ x + (x | subj), family=Gamma(link="identity"))
ig.glm <- glm(y ~ x, family=inverse.gaussian(link="identity"))
ig.lmer <- lmer(y ~ x + (x | subj),
family=inverse.gaussian(link="identity"))
nums.glm <- function(model) c(coef(model), logLik=logLik(model))
nums.lmer <- function(model) c(fixef(model), logLik=logLik(model))
rbind(gaussian.lm=nums.glm(gaussian.lm), gaussian.lmer=nums.lmer(gaussian.lmer),
      gamma.glm=nums.glm(gamma.glm), gamma.lmer=nums.lmer(gamma.lmer),
      ig.glm=nums.glm(ig.glm), ig.lmer=nums.lmer(ig.lmer))
VarCorr(gaussian.lmer)$subj
VarCorr(gamma.lmer)$subj
VarCorr(ig.lmer)$subj

---------------------------->8----------------------------

Output:

---------------------------->8----------------------------

              (Intercept)        x        logLik
gaussian.lm      192.1295 2.636897 -59432.332233
gaussian.lmer    192.0867 2.648113 -59257.574632
gamma.glm        192.3558 2.579673 -58668.208775
gamma.lmer      192.1256 2.578616  -1038.636243
ig.glm          192.4572 2.553089 -58910.126512
ig.lmer          192.4572 2.553093    -6.340702
> VarCorr(gaussian.lmer)$subj
2 x 2 Matrix of class "dpoMatrix"
            (Intercept)          x
(Intercept)  252.21320 11.4298584
x              11.42986  0.5179811
> VarCorr(gamma.lmer)$subj
2 x 2 Matrix of class "dpoMatrix"
            (Intercept)          x
(Intercept)  41.109713 1.63892630
x              1.638926 0.06795769
> VarCorr(ig.lmer)$subj
2 x 2 Matrix of class "dpoMatrix"
              (Intercept)            x
(Intercept)  1.538172e-06 -8.630771e-19
x          -8.630771e-19  5.124452e-13

---------------------------->8----------------------------

The first part just duplicates what I found above, to show that it
still arises with models that are not overspecified.

The second part is the variance-covariance matrices -- in no case are
the estimates anywhere near accurate, and for the inverse gaussian
model it somehow has decided that there are effectively no random
effects at all.  I see the same behavior on my real data -- that is,
random effects for the inverse gaussian model coming out to
effectively zero.  I can't say if the estimates for the gamma model
are also incorrect, because I don't know what the true values are
:-).  But this is not giving me confidence that the values I'm getting
are anywhere near true, either :-(.

This is all with R 2.6.0 on x86-64 linux, with lme4 0.99875-8.

Help!
-- Nathaniel

-- 
So let us espouse a less contested notion of truth and falsehood, even
if it is philosophically debatable (if we listen to philosophers, we
must debate everything, and there would be no end to the discussion).
  -- Serendipities, Umberto Eco



------------------------------

_______________________________________________
R-sig-mixed-models mailing list
R-sig-mixed-models at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


End of R-sig-mixed-models Digest, Vol 10, Issue 16
**************************************************


      ___________________________________________________________ 
Want ideas for reducing your carbon footprint? Visit Yahoo! For Good  http://uk.promotions.yahoo.com/forgood/environment.html



From njs at pobox.com  Sat Oct 20 01:54:12 2007
From: njs at pobox.com (Nathaniel Smith)
Date: Fri, 19 Oct 2007 16:54:12 -0700
Subject: [R-sig-ME] lmer() producing wildly inaccurate values for some
	GLM models
In-Reply-To: <20071019084415.GA18600@frances.vorpus.org>
References: <20071019084415.GA18600@frances.vorpus.org>
Message-ID: <20071019235412.GA25477@frances.vorpus.org>

A few more notes on this, in response to a followup off-list:

On Fri, Oct 19, 2007 at 01:44:15AM -0700, Nathaniel Smith wrote:
> for (s in 1:subjects) {
>   subj <- c(subj, rep(s, samples))
>   x.subj <- rgamma(samples, 2, scale=2)
>   x <- c(x, x.subj)
>   i.subj <- rnorm(1, i, sqrt(i.var))
>   b.subj <- rnorm(1, b, sqrt(b.var))
>   true.scale <- c(true.scale, (i.subj + b*x.subj)/true.shape)

The 'b' in the last line line above should, of course, instead be
b.subj.  (This is from the second test case.)

With that fix, the output changes a bit, but not much:

---------------------------->8----------------------------
              (Intercept)        x        logLik
gaussian.lm      192.1596 2.474115 -59410.907309
gaussian.lmer    192.1226 2.483921 -59226.456758
gamma.glm        192.3719 2.420455 -58651.253027
gamma.lmer       192.4162 2.398446  -1039.161105
ig.glm           192.4670 2.395605 -58895.259401
ig.lmer          192.4670 2.395606     -6.385455
> VarCorr(gaussian.lmer)$subj
2 x 2 Matrix of class "dpoMatrix"
            (Intercept)         x
(Intercept)   252.30861 12.853766
x              12.85377  0.698928
> VarCorr(gamma.lmer)$subj
2 x 2 Matrix of class "dpoMatrix"
            (Intercept)         x
(Intercept)   39.109410 2.1247739
x              2.124774 0.1177906
> VarCorr(ig.lmer)$subj
2 x 2 Matrix of class "dpoMatrix"
              (Intercept)             x
(Intercept)  1.529936e-06 -6.192435e-20
x           -6.192435e-20  5.148065e-13
---------------------------->8----------------------------

The logLik is still low, the estimated effects are still off, etc.  

It was also pointed out that of course I'm fitting a model that allows
the intercept and slope to covary, which is not true in my original
data.  One would hope that lmer would discover this and set the
effect correlations to 0, but in any case, if I fit the more
restricted model, then I get a better gaussian fit but
still-anomalous results from the others:

gaussian.restricted <- lmer(y ~ x + (1 | subj) + (0 + x | subj))
gamma.restricted <- lmer(y ~ x + (1 | subj) + (0 + x | subj),
                         family=Gamma(link="identity"))
ig.restricted <- lmer(y ~ x + (1 | subj) + (0 + x | subj),
                      family=inverse.gaussian(link="identity"))

Gaussian model:
Random effects:
 Groups   Name        Variance  Std.Dev.
 subj     (Intercept)  299.7609 17.3136 
 subj     x              1.0649  1.0320 
 Residual             8136.4169 90.2021 
number of obs: 10000, groups: subj, 10; subj, 10

Gamma model:
Random effects:
 Groups   Name        Variance   Std.Dev.  
 subj     (Intercept) 5.8623e+01 7.6565e+00
 subj     x           9.9887e-11 9.9943e-06
 Residual             1.9977e-01 4.4696e-01
number of obs: 10000, groups: subj, 10; subj, 10

Inverse gaussian model:
Random effects:
 Groups   Name        Variance   Std.Dev.  
 subj     (Intercept) 1.5280e-06 1.2361e-03
 subj     x           5.1481e-13 7.1750e-07
 Residual             1.0296e-03 3.2088e-02
number of obs: 10000, groups: subj, 10; subj, 10

-- Nathaniel

-- 
Eternity is very long, especially towards the end.
  -- Woody Allen



From kjbeath at kagi.com  Sat Oct 20 04:03:32 2007
From: kjbeath at kagi.com (Ken Beath)
Date: Sat, 20 Oct 2007 12:03:32 +1000
Subject: [R-sig-ME] change in variance components depending on scaling
	of fixed effects
In-Reply-To: <47171EFF.5020502@ed.ac.uk>
References: <47171EFF.5020502@ed.ac.uk>
Message-ID: <C63D1A0D-9C84-430E-9707-F95C65F47884@kagi.com>

On 18/10/2007, at 6:53 PM, Arild Husby wrote:

> Dear all,
>
> I am trying to understand the output from a binomial lmer object  
> and why
> the scaling of a fixed effect changes the variance components.
>
> In the model p2rec is cbind(number recruits2,number recruits 1),  
> Pop is
> populations (five level factor) and ja is year (continous covariate
> running from 1955-2004). I've used the Laplace optimization method,  
> due
> to earlier reports of unstability of PQL when running binomial models.
>

This could be due to numerical problems, which are fixed by the  
centring. This sort of problem depends a lot on starting values.   
Another possibility is that the Laplace approximation is breaking  
down, which leads to similar numerical problems.  You haven't  
described the data, other than it is binomial but if the counts per  
observation is small, then there will be problems, especially  
considering there are 1323 in only 1088  groups. Unfortunately the  
AGQ is not implemented, as this is a better approximation. Trying  
with Stata would be worthwhile, either gllamm or xtlogit should work.

Ken



> First example: (ja continous covariate, range: 1955-2004)
>
>> totmod2 <- lmer(p2rec~Pop*ja + (1|VROUW)+(1|ja), data=dltab2,
> family=binomial, method="Laplace", na.action=na.omit)
>> summary(totmod2)
> Generalized linear mixed model fit using Laplace
> Formula: p2rec ~ Pop * ja + (1 | VROUW) + (1 | ja)
>  Data: dltab2
> Family: binomial(logit link)
>  AIC   BIC logLik deviance
> 12456 12519  -6216    12432
> Random effects:
> Groups Name        Variance Std.Dev.
> VROUW  (Intercept) 2.19300  1.48088
> ja     (Intercept) 0.09675  0.31105
> number of obs: 1323, groups: VROUW, 1088; ja, 48
>
> Estimated scale (compare to  1 )  22.97855
>
>
> I then scale  ja so that:  dltab2$ja<-scale(dltab2$ja, scale=FALSE)
>
>> totmod2 <- lmer(p2rec~Pop*ja + (1|VROUW)+(1|ja), data=dltab2,
> family=binomial, method="Laplace", na.action=na.omit)
>> summary(totmod2)
> Generalized linear mixed model fit using Laplace
> Formula: p2rec ~ Pop * ja + (1 | VROUW) + (1 | ja)
>  Data: dltab2
> Family: binomial(logit link)
>  AIC  BIC logLik deviance
> 983.8 1046 -479.9    959.8
> Random effects:
> Groups Name        Variance Std.Dev.
> VROUW  (Intercept) 0.54162  0.73595
> ja     (Intercept) 0.29192  0.54029
> number of obs: 1323, groups: VROUW, 1088; ja, 48
>
> Estimated scale (compare to  1 )  0.7061424
>
>
> Different scaling:  dltab2$ja<-scale(dltab2$ja, center=1000,  
> scale=FALSE)
>
>> totmod2 <- lmer(p2rec~Pop*ja + (1|VROUW)+(1|ja), data=dltab2,
> family=binomial, method="Laplace", na.action=na.omit)
>> summary(totmod2)
> Generalized linear mixed model fit using Laplace
> Formula: p2rec ~ Pop * ja + (1 | VROUW) + (1 | ja)
>  Data: dltab2
> Family: binomial(logit link)
> AIC  BIC logLik deviance
> 7136 7198  -3556     7112
> Random effects:
> Groups Name        Variance Std.Dev.
> VROUW  (Intercept) 2.19300  1.48088
> ja     (Intercept) 0.09675  0.31105
> number of obs: 1323, groups: VROUW, 1088; ja, 48
>
> Estimated scale (compare to  1 )  3.083302
>
>
> Estimates of fixed effects changes as one would expect (so have not
> printed them here), but I do not understand why there is such a  
> massive
> difference in the variance components.
>
> Note that the first and last example has the same estimates of  
> variance
> components, but that the estimated scale is massively different.
>
> All help is highly appreciated.
>
>
> (See also a posting by Steven Orzack on a similar problem:
> http://tolstoy.newcastle.edu.au/R/e2/help/07/07/22076.html)
>
>
> All help is highly appreciated.
>
> Thanks very much,
>
> Arild Husby
>
>> sessionInfo()
> R version 2.5.0 (2007-04-23)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=English_United Kingdom.1252;LC_CTYPE=English_United
> Kingdom.1252;LC_MONETARY=English_United
> Kingdom.1252;LC_NUMERIC=C;LC_TIME=English_United Kingdom.1252
>
> attached base packages:
> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"
> "methods"   "base"
> other attached packages:
>      lme4      Matrix     lattice
> "0.99875-2" "0.99875-2"   "0.15-11"
>
>
>
> -- 
> Arild Husby
> Institute of Evolutionary Biology
> Room 413, Ashworth Labs,
> King's Buildings,
> University of Edinburgh
> EH9 3JT, UK
>
> E-mail: arild.husby at ed.ac.uk
> web: http://homepages.ed.ac.uk/loeske/arild.html
> Tel: +44 (0)131 650 5990
> Mob: +44 (0)798 275 0668
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From gsrwork at yahoo.com  Mon Oct 29 22:10:39 2007
From: gsrwork at yahoo.com (Abdus Sattar)
Date: Mon, 29 Oct 2007 14:10:39 -0700 (PDT)
Subject: [R-sig-ME] gradient and Hessian from lmer2
Message-ID: <215180.98028.qm@web58415.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20071029/4f914e90/attachment.pl>

From kari.ruohonen at utu.fi  Tue Oct 30 10:48:19 2007
From: kari.ruohonen at utu.fi (Kari Ruohonen)
Date: Tue, 30 Oct 2007 11:48:19 +0200
Subject: [R-sig-ME] Error message from nlmer
Message-ID: <1193737699.8804.4.camel@wittgenstein>

I apologise that I re-post this question but I just learnt about this
specific mixed model list, and since I got no replies from r-help I
thought there may be people able to help on this list that are not
reading the main r-help.

Once more my sincere apologies if someone feels this is misuse of the
list. I would appreciate very much any hints to solve the problem
described below since I am totally puzzled.

-------- Forwarded Message --------
From: Kari Ruohonen <kari.ruohonen at utu.fi>
To: r-help at r-project.org
Subject: Error message from nlmer
Date: Thu, 25 Oct 2007 15:31:46 +0300

Hi,

I have R 2.6.0 with updated lme4 and Matrix packages, and I am trying to
fit a nonlinear multilevel model. I get the following error message:

Error in nlmer(f ~ grModel(x, w, Tmin, Tmax, Topt, kopt, m) ~ kopt |
flat,  : 
  gradient attribute of evaluated model must be a numeric matrix

and I wonder what this may indicate.

The nonlinear model I try to fit is as follows:

> grModel
function(x,w,Tmin,Tmax,Topt,kopt,m)((x-Tmin)*(x-Tmax))/((x-Tmin)*(x-Tmax)-(x-Topt)^2)*kopt*w^m

and my call of nlmer is as follows:

nlmr1<-nlmer(feed~grModel(temp,MW,Tmin,Tmax,Topt,kopt,m)~kopt|
flat,data=egi,start=c(Tmin=0.2,Tmax=21.1,Topt=14.6,kopt=2.5,m=.51))

'flat' is a factor with 11 levels.

My start values are from nls that fits the model with no errors. The
call I have used is as follows:

nls1<-nls(feed~grModel(temp,MW,Tmin,Tmax,Topt,kopt,m),data=egi,start=c(Tmin=1,Tmax=20,Topt=13,kopt=3,m=.7))

The examples given on the help page of nlmer run with no errors.

I would appreciate any hints and help to track the problem indicated by
the error message.

Thanks,

Kari



From bates at stat.wisc.edu  Tue Oct 30 13:31:29 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 30 Oct 2007 07:31:29 -0500
Subject: [R-sig-ME] Error message from nlmer
In-Reply-To: <1193737699.8804.4.camel@wittgenstein>
References: <1193737699.8804.4.camel@wittgenstein>
Message-ID: <40e66e0b0710300531s6a00140bt14fdec1c593dd319@mail.gmail.com>

On 10/30/07, Kari Ruohonen <kari.ruohonen at utu.fi> wrote:
> I apologise that I re-post this question but I just learnt about this
> specific mixed model list, and since I got no replies from r-help I
> thought there may be people able to help on this list that are not
> reading the main r-help.
>
> Once more my sincere apologies if someone feels this is misuse of the
> list. I would appreciate very much any hints to solve the problem
> described below since I am totally puzzled.

No apology needed.  This type of message is entirely appropriate for this list.

I didn't notice your message on R-help or I would have responded there.

The answer to your question is that nlmer currently requires an
analytic gradient of the nonlinear model function.  You could use the
deriv function to create a version of your grModel that returns the
gradient.

> grModel <- deriv(expression(((x-Tmin)*(x-Tmax))/((x-Tmin)*(x-Tmax)-(x-Topt)^2)*kopt*w^m),
+        c("Tmin","Tmax","Topt","kopt","m"), function(x, Tmin, Tmax,
Topt, kopt, m){})
> grModel <- function(x,w,Tmin,Tmax,Topt,kopt,m)((x-Tmin)*(x-Tmax))/((x-Tmin)*(x-Tmax)-(x-Topt)^2)*kopt*w^m
> str(body(grModel))
 language ((x - Tmin) * (x - Tmax))/((x - Tmin) * (x - Tmax) - (x -
Topt)^2) *      kopt * w^m
> grModg <- deriv(body(grModel), namevec = c("Tmin","Tmax","Topt","kopt","m"), func = grModel)
> grModg
function (x, w, Tmin, Tmax, Topt, kopt, m)
{
    .expr1 <- x - Tmin
    .expr2 <- x - Tmax
    .expr3 <- .expr1 * .expr2
    .expr4 <- x - Topt
    .expr6 <- .expr3 - .expr4^2
    .expr7 <- .expr3/.expr6
    .expr8 <- .expr7 * kopt
    .expr9 <- w^m
    .expr13 <- .expr6^2
    .value <- .expr8 * .expr9
    .grad <- array(0, c(length(.value), 5L), list(NULL, c("Tmin",
        "Tmax", "Topt", "kopt", "m")))
    .grad[, "Tmin"] <- -((.expr2/.expr6 - .expr3 * .expr2/.expr13) *
        kopt * .expr9)
    .grad[, "Tmax"] <- -((.expr1/.expr6 - .expr3 * .expr1/.expr13) *
        kopt * .expr9)
    .grad[, "Topt"] <- -(.expr3 * (2 * .expr4)/.expr13 * kopt *
        .expr9)
    .grad[, "kopt"] <- .expr7 * .expr9
    .grad[, "m"] <- .expr8 * (.expr9 * log(w))
    attr(.value, "gradient") <- .grad
    .value
}

Now use grModg instead of grModel.
> -------- Forwarded Message --------
> From: Kari Ruohonen <kari.ruohonen at utu.fi>
> To: r-help at r-project.org
> Subject: Error message from nlmer
> Date: Thu, 25 Oct 2007 15:31:46 +0300
>
> Hi,
>
> I have R 2.6.0 with updated lme4 and Matrix packages, and I am trying to
> fit a nonlinear multilevel model. I get the following error message:
>
> Error in nlmer(f ~ grModel(x, w, Tmin, Tmax, Topt, kopt, m) ~ kopt |
> flat,  :
>   gradient attribute of evaluated model must be a numeric matrix
>
> and I wonder what this may indicate.
>
> The nonlinear model I try to fit is as follows:
>
> > grModel
> function(x,w,Tmin,Tmax,Topt,kopt,m)((x-Tmin)*(x-Tmax))/((x-Tmin)*(x-Tmax)-(x-Topt)^2)*kopt*w^m
>
> and my call of nlmer is as follows:
>
> nlmr1<-nlmer(feed~grModel(temp,MW,Tmin,Tmax,Topt,kopt,m)~kopt|
> flat,data=egi,start=c(Tmin=0.2,Tmax=21.1,Topt=14.6,kopt=2.5,m=.51))
>
> 'flat' is a factor with 11 levels.
>
> My start values are from nls that fits the model with no errors. The
> call I have used is as follows:
>
> nls1<-nls(feed~grModel(temp,MW,Tmin,Tmax,Topt,kopt,m),data=egi,start=c(Tmin=1,Tmax=20,Topt=13,kopt=3,m=.7))
>
> The examples given on the help page of nlmer run with no errors.
>
> I would appreciate any hints and help to track the problem indicated by
> the error message.
>
> Thanks,
>
> Kari
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From sciandra at dssm.unipa.it  Wed Oct 31 16:28:36 2007
From: sciandra at dssm.unipa.it (Mariangela Sciandra)
Date: Wed, 31 Oct 2007 16:28:36 +0100 (CET)
Subject: [R-sig-ME] Correlation in glmmPQL
Message-ID: <2014.147.163.25.192.1193844516.squirrel@dssm.unipa.it>

Hi all,

I have a question.
I'm analyzing a dataset using the glmmPQL function including only one
fixed effect (treatment), a random intercept and a within group residual
autocorrelation structure of order 1 (with coefficient Phi).

mod<-glmmPQL(y~1+treatn,random=~1|idnew,family=binomial,data=Toenail,correlation=corAR1())

If I try to write step by step the linear predictor like below...

X.mod<-model.matrix(~1+treatn,data=Toenail)
beta.mod<-as.matrix(fixef(mod))
eta1.mod<-X.mod%*%beta.mod
BLUP.mod<-as.matrix(ranef(mod))
eta2.mod<-Z%*%BLUP.mod
eta.mod<-eta1.mod+eta.mod2

...eta.mod will be qual to fitted(mod,level=1).

i don't understand this result. Why in the linear pedictor eta.mod it
doesn't need to add  Phi*elag, where elag is tha lag 1 within-group
residual?

How does Phi get incorporated into the linear predictor?

Thanks in advance,

Mariangela


*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*
Mariangela Sciandra
Assegnista di ricerca in "Statistica"
Dipartimento Scienze Statistiche e Matematiche "Silvio Vianelli"
Viale delle Scienze, Edificio 13
90128, Palermo
Tel:0039-0916626242



From r.j.forsyth at newcastle.ac.uk  Thu Nov  1 13:42:44 2007
From: r.j.forsyth at newcastle.ac.uk (Rob Forsyth)
Date: Thu, 1 Nov 2007 12:42:44 +0000
Subject: [R-sig-ME] nlme and NONMEM
Message-ID: <DF300952-503B-48E7-9E49-839A5B6C4040@newcastle.ac.uk>

I'd appreciate hearing from anyone (off list if you think it more  
appropriate) who can share their comparative experiences of non- 
linear mixed effects modelling with both nlme and NONMEM. The latter  
appears the traditional tool of choice particularly in pharmacology.  
Having built up some familiarity with nlme I am now collaborating (on  
a non-pharmacological project) with someone strongly encouraging me  
to move to NONMEM, although that clearly represents another  
considerable learning curve. The main argument in favour is the  
relative difficulty I have had in getting convergence with nlme  
models in my relatively sparse datasets particularly when (as in my  
case) I am interested in the random effects covariance matrix and  
wish to avoid having to coerce it using pdDiag().

I note the following comment from Douglas Bates on the R-help archive

> The nonlinear optimization codes used by S-PLUS and R are different.
> There are advantages to the code used in R relative to the code used
> in S-PLUS but there are also disadvantages. One of the disadvantages
> is that the code in R will try very large steps during its initial
> exploration phase then it gets trapped in remote regions of the
> parameter space. For nlme this means that the estimate of the
> variance-covariance matrix of the random effects becomes singular.
>
> Recent versions of the nlme library for R have a subdirectory called
> scripts that contains R scripts for the examples from each of the
> chapters in our book. If you check them you will see that not all of
> the nonlinear examples work in the R version of nlme. We plan to
> modify the choice of starting estimates and the internal algorithms to
> improve this but it is a long and laborious process. I ask for your
> patience.
>

Can Doug or anyone comment on whether the development work on  
lme4:::nlmer has included any steps in this direction or not?

Thanks

Rob Forsyth



From bates at stat.wisc.edu  Thu Nov  1 22:42:29 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 1 Nov 2007 16:42:29 -0500
Subject: [R-sig-ME] nlme and NONMEM
In-Reply-To: <DF300952-503B-48E7-9E49-839A5B6C4040@newcastle.ac.uk>
References: <DF300952-503B-48E7-9E49-839A5B6C4040@newcastle.ac.uk>
Message-ID: <40e66e0b0711011442p6c2b8695m3c96a2571bb30146@mail.gmail.com>

On 11/1/07, Rob Forsyth <r.j.forsyth at newcastle.ac.uk> wrote:
> I'd appreciate hearing from anyone (off list if you think it more
> appropriate) who can share their comparative experiences of non-
> linear mixed effects modelling with both nlme and NONMEM. The latter
> appears the traditional tool of choice particularly in pharmacology.
> Having built up some familiarity with nlme I am now collaborating (on
> a non-pharmacological project) with someone strongly encouraging me
> to move to NONMEM, although that clearly represents another
> considerable learning curve. The main argument in favour is the
> relative difficulty I have had in getting convergence with nlme
> models in my relatively sparse datasets particularly when (as in my
> case) I am interested in the random effects covariance matrix and
> wish to avoid having to coerce it using pdDiag().

> I note the following comment from Douglas Bates on the R-help archive

> > The nonlinear optimization codes used by S-PLUS and R are different.
> > There are advantages to the code used in R relative to the code used
> > in S-PLUS but there are also disadvantages. One of the disadvantages
> > is that the code in R will try very large steps during its initial
> > exploration phase then it gets trapped in remote regions of the
> > parameter space. For nlme this means that the estimate of the
> > variance-covariance matrix of the random effects becomes singular.

> > Recent versions of the nlme library for R have a subdirectory called
> > scripts that contains R scripts for the examples from each of the
> > chapters in our book. If you check them you will see that not all of
> > the nonlinear examples work in the R version of nlme. We plan to
> > modify the choice of starting estimates and the internal algorithms to
> > improve this but it is a long and laborious process. I ask for your
> > patience.

> Can Doug or anyone comment on whether the development work on
> lme4:::nlmer has included any steps in this direction or not?

Yes.

The algorithm in nlme alternates between solving a linear
mixed-effects problem to update estimates of the variance components
and solving a penalized nonlinear least squares problem to update
estimates of the fixed-effects parameters and our approximation to the
conditional distribution of the random effects.  This type of
algorithm that alternates between two conditional optimizations is
appealing because each of the sub-problems is much simpler than the
general problem.  However it may have poor convergence properties.  In
particular it may end up bouncing back and forth between two different
conditional optima.

Also, at the time we wrote nlme we tried to remove the constraints on
the variance components by transforming them away (In simple
situations we iterate on the logarithm of the relative variances of
the random effects.)  This works well except when the estimate of the
variance component is zero.  Trying to reach zero when iterating on
the logarithm scale can lead to very flat likelihood surfaces.

In the nlmer function I use the same parameterization of the
variance-covariance of the random effects as in lmer and use the
Laplace approximation to the log-likelihood.  Both of these changes
should provide more reliable convergence, although the nlmer code has
not been vetted to nearly the same extent as has the nlme code.  In
other words, I am confident that the algorithm is superior but the
implementation may still need some work.

Regarding NONMEM, I think the work Jose Pinheiro and I did on nlme and
my current work on lme4 is based on a different philosophy than is the
basis of NONMEM.  As I have mentioned on this and other forums (fora?)
I want to be confident that the results from the code that I write
actually do represent an optimum of the objective function (such as
the likelihood or log-likelihood).  Nonlinear mixed-effects models for
sparse data frequently end up being over-parameterized. In such cases
I view it as a feature and not a bug that nlme or nlmer will indicate
failure to converge.  They may also fail to converge when there is a
well-defined optimum.  That behavior is not a feature.

As I understand it from people who have used NONMEM (I once had access
to a copy of NONMEM but was never successful in getting it to run and
haven't tried since then) it will produce estimates just about every
time it is run.  Considering how ill-defined the parameter estimates
in some nonlinear mixed-effects model fits can be, I don't view this
as a feature.

Many people feel that statistical techniques and statistical software
are some sort of magic that can extract information from data, even
when the information is not there.  As I understand it from
conversations many years ago with Lewis Sheiner, his motivation in
developing NONMEM (with Stu Beal) was to be able to use routine
clinical data (such as the Quinidine data in the nlme package) to
estimate population pharmacokinetic parameters.

Routine clinical data like these are very sparse. In the Quinidine
example the majority of subjects have 1, 2 or 3 concentration
measurements

> table(table(subset(Quinidine, !is.na(Quinidine$conc))$Subject))

 1  2  3  4  5  6  7 10 11
46 33 31  9  3  8  2  1  3

and frequently these measurements are at widely spaced time points
relative to the dosing schedule.  Such cases contribute almost no
information to the parameter estimates, yet I have had pharmacologists
suggest to me that it would be wonderful to use study designs in which
each patient has only one concentration measurement and somehow the
magic of nonlinear mixed effects will conjure estimates from such
data.

The real world doesn't work like that.  If you have only one
observation per person it should make sense that no amount of
statistical magic will be able to separate the per-observation noise
from the per-person variability.

So when I am told that NONMEM converged to parameter estimates on a
problem where nlme or nlmer failed to converge I think (and sometimes
say) "You mean NONMEM *declared* convergence to a set of estimates".
Declaring convergence and converging can be different.



From n.l.pace at utah.edu  Fri Nov  2 00:34:12 2007
From: n.l.pace at utah.edu (Nathan Leon Pace, MD, MStat)
Date: Thu, 01 Nov 2007 17:34:12 -0600
Subject: [R-sig-ME] nlme and NONMEM
In-Reply-To: <40e66e0b0711011442p6c2b8695m3c96a2571bb30146@mail.gmail.com>
Message-ID: <C34FBE94.8584%n.l.pace@utah.edu>

Hi All,

This thread reminds me of an experience using nlme about 10 years ago. I was
remodeling a previously analyzed (and published) pharmacokinetic data set on
the drug remifentanil; NONMEM had been used to estimate a 3 compartment (6
parameter) model. The data included multiple plasma concentration values for
each subject.

Using nlme, no convergence was possible for a three compartment model
despite various choices of the control language and covariance structure. A
two compartment model converged.

Doug provided very useful tips to me at the time. For example, a visual
inspection of the raw data (time course of remifentanil concentration decay)
revealed only one inflection point in the decay curves for most subjects,
whereas two inflection points would be consistent with a three compartment
model. The data was not sufficient to fit a three compartment model.

I have never used NONMEM. Speaking to associates using NONMEM in the 90s,
they assured me that NONMEM could always be tweaked to converge. This was
considered a virtue.

This is an example NONMEM allowing overparametrized models.

Nathan
-- 
Nathan Leon Pace, MD, MStat
University of Utah
n.l.pace at utah.edu
W: 801.581.6393
F: 801.581.4367
M: 801.205.1019



> From: Douglas Bates <bates at stat.wisc.edu>
> Date: Thu, 1 Nov 2007 16:42:29 -0500
> To: Rob Forsyth <r.j.forsyth at newcastle.ac.uk>
> Cc: <r-sig-mixed-models at r-project.org>
> Subject: Re: [R-sig-ME] nlme and NONMEM
> 
> On 11/1/07, Rob Forsyth <r.j.forsyth at newcastle.ac.uk> wrote:
>> I'd appreciate hearing from anyone (off list if you think it more
>> appropriate) who can share their comparative experiences of non-
>> linear mixed effects modelling with both nlme and NONMEM. The latter
>> appears the traditional tool of choice particularly in pharmacology.
>> Having built up some familiarity with nlme I am now collaborating (on
>> a non-pharmacological project) with someone strongly encouraging me
>> to move to NONMEM, although that clearly represents another
>> considerable learning curve. The main argument in favour is the
>> relative difficulty I have had in getting convergence with nlme
>> models in my relatively sparse datasets particularly when (as in my
>> case) I am interested in the random effects covariance matrix and
>> wish to avoid having to coerce it using pdDiag().
> 
>> I note the following comment from Douglas Bates on the R-help archive
> 
>>> The nonlinear optimization codes used by S-PLUS and R are different.
>>> There are advantages to the code used in R relative to the code used
>>> in S-PLUS but there are also disadvantages. One of the disadvantages
>>> is that the code in R will try very large steps during its initial
>>> exploration phase then it gets trapped in remote regions of the
>>> parameter space. For nlme this means that the estimate of the
>>> variance-covariance matrix of the random effects becomes singular.
> 
>>> Recent versions of the nlme library for R have a subdirectory called
>>> scripts that contains R scripts for the examples from each of the
>>> chapters in our book. If you check them you will see that not all of
>>> the nonlinear examples work in the R version of nlme. We plan to
>>> modify the choice of starting estimates and the internal algorithms to
>>> improve this but it is a long and laborious process. I ask for your
>>> patience.
> 
>> Can Doug or anyone comment on whether the development work on
>> lme4:::nlmer has included any steps in this direction or not?
> 
> Yes.
> 
> The algorithm in nlme alternates between solving a linear
> mixed-effects problem to update estimates of the variance components
> and solving a penalized nonlinear least squares problem to update
> estimates of the fixed-effects parameters and our approximation to the
> conditional distribution of the random effects.  This type of
> algorithm that alternates between two conditional optimizations is
> appealing because each of the sub-problems is much simpler than the
> general problem.  However it may have poor convergence properties.  In
> particular it may end up bouncing back and forth between two different
> conditional optima.
> 
> Also, at the time we wrote nlme we tried to remove the constraints on
> the variance components by transforming them away (In simple
> situations we iterate on the logarithm of the relative variances of
> the random effects.)  This works well except when the estimate of the
> variance component is zero.  Trying to reach zero when iterating on
> the logarithm scale can lead to very flat likelihood surfaces.
> 
> In the nlmer function I use the same parameterization of the
> variance-covariance of the random effects as in lmer and use the
> Laplace approximation to the log-likelihood.  Both of these changes
> should provide more reliable convergence, although the nlmer code has
> not been vetted to nearly the same extent as has the nlme code.  In
> other words, I am confident that the algorithm is superior but the
> implementation may still need some work.
> 
> Regarding NONMEM, I think the work Jose Pinheiro and I did on nlme and
> my current work on lme4 is based on a different philosophy than is the
> basis of NONMEM.  As I have mentioned on this and other forums (fora?)
> I want to be confident that the results from the code that I write
> actually do represent an optimum of the objective function (such as
> the likelihood or log-likelihood).  Nonlinear mixed-effects models for
> sparse data frequently end up being over-parameterized. In such cases
> I view it as a feature and not a bug that nlme or nlmer will indicate
> failure to converge.  They may also fail to converge when there is a
> well-defined optimum.  That behavior is not a feature.
> 
> As I understand it from people who have used NONMEM (I once had access
> to a copy of NONMEM but was never successful in getting it to run and
> haven't tried since then) it will produce estimates just about every
> time it is run.  Considering how ill-defined the parameter estimates
> in some nonlinear mixed-effects model fits can be, I don't view this
> as a feature.
> 
> Many people feel that statistical techniques and statistical software
> are some sort of magic that can extract information from data, even
> when the information is not there.  As I understand it from
> conversations many years ago with Lewis Sheiner, his motivation in
> developing NONMEM (with Stu Beal) was to be able to use routine
> clinical data (such as the Quinidine data in the nlme package) to
> estimate population pharmacokinetic parameters.
> 
> Routine clinical data like these are very sparse. In the Quinidine
> example the majority of subjects have 1, 2 or 3 concentration
> measurements
> 
>> table(table(subset(Quinidine, !is.na(Quinidine$conc))$Subject))
> 
>  1  2  3  4  5  6  7 10 11
> 46 33 31  9  3  8  2  1  3
> 
> and frequently these measurements are at widely spaced time points
> relative to the dosing schedule.  Such cases contribute almost no
> information to the parameter estimates, yet I have had pharmacologists
> suggest to me that it would be wonderful to use study designs in which
> each patient has only one concentration measurement and somehow the
> magic of nonlinear mixed effects will conjure estimates from such
> data.
> 
> The real world doesn't work like that.  If you have only one
> observation per person it should make sense that no amount of
> statistical magic will be able to separate the per-observation noise
> from the per-person variability.
> 
> So when I am told that NONMEM converged to parameter estimates on a
> problem where nlme or nlmer failed to converge I think (and sometimes
> say) "You mean NONMEM *declared* convergence to a set of estimates".
> Declaring convergence and converging can be different.
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From ima at difres.dk  Fri Nov  2 10:04:17 2007
From: ima at difres.dk (Irene Mantzouni)
Date: Fri, 2 Nov 2007 10:04:17 +0100
Subject: [R-sig-ME] [R] coef se in lme
References: <68E7981938EAF54F987AD3848A0A6416E58342@ka-mail01.dfu.local><2323A6D37908A847A7C32F1E3662C80EB5E70B@dc1ex01.air.org>
	<40e66e0b0710171304s231b55cej8253eefbd7cb6e87@mail.gmail.com>
Message-ID: <68E7981938EAF54F987AD3848A0A6416E58370@ka-mail01.dfu.local>

Thank you very much for the reply (and hopefully I am replying back in the proper way). 
Do you think the delta method would be an acceptable way to estimate approximate confidence intervals for the resulting group specific coefficients (combining fixed effects and BLUPS)? 
Regarding the MCMC related approach, how is it possible to save the fixed and the
random effects from the MCMC chain? Can this be implemented through nlme library or is there some more clear cut way (I wish I had a strong statistical background and abilities but... :)) to evaluate the empirical distribution of a parameter that is linear combination of these quantities? 

All the best,
Irene
 

________________________________

???: dmbates at gmail.com ?? ?????? Douglas Bates
????????: ??? 17/10/2007 10:04 ??
????: Doran, Harold
????.: Irene Mantzouni; r-help at stat.math.ethz.ch; R-SIG-Mixed-Models
????: Re: [R] coef se in lme



On 10/15/07, Doran, Harold <HDoran at air.org> wrote:
> ?vcov

The vcov method returns the estimated variance-covariance matrix of
the fixed-effects only.  I think Irene's question is about the
combination of the fixed-effects parameters and the BLUPs of the
random effects that is returned by the coef method applied to an lmer
object.  (You may recall that you were the person who requested such a
method in lme4 like the coef method in nlme :-)

On the face of it this quantity should be easy to define and evaluate
but in fact it is not easy to do so because these are combinations of
model parameters (the fixed effects) and unobserved random variables
(the random effects).  It gets a bit tricky trying to decide what the
variance of this combination would be.  I think there is a sensible
definition, or at least a computationally reasonable definition, but
there are still a few slippery points in the argument.

Lately I have taken to referring to the "estimates" of the random
effects, what are sometimes called the BLUPs or Best Linear Unbiased
Predictors, as the "conditional modes" of the random effects.  That
is, they are the values that maximize the density of the random
effects given the observed data and the values of the model
parameters.  For a linear mixed model the conditional distribution of
the random effects is multivariate normal so the conditional modes are
also the conditional means.  Also, we can evaluate the conditional
variance-covariance matrix of the random effects up to a scale factor.

The next part is where things get a bit hazy for me but I think it
makes sense to consider the joint distribution of the estimator of the
fixed-effects parameters and the random effects conditional on the
data and, possibly, on the variance components.  Conditional on the
relative variance-covariance of the random effects (i.e. the matrix
that occurs as the penalty term in the penalized least squares
representation of the model) the joint distribution of the
fixed-effects estimators and the random effects is multivariate normal
with mean and variance-covariance matrix determined from the
mixed-model equations.

This big (p+q by p+q, where p is the dimension of the fixed effects
and q is the dimension of the random effects) variance-covariance
matrix could be evaluated and, from that, the variance of any linear
combination of components.  However, I have my doubts about whether it
is the most sensible answer to evaluate.  Conditioning on the relative
variance-covariance matrix of the random effects is cheating, in a
way.  It would be like saying we have a known variance, $\sigma^2$
when, in fact, we are using an estimate.  The fact that we don't know
$\sigma^2$ is what gives rise to the t distributions and F
distributions in linear models and we are all trained to pay careful
attention to the number of degrees of freedom in that estimate and how
it affects our ideas of the precision of the estimates of other model
parameters.  For mixed models, though, many practioners are quite
comfortable conditioning on the value of some of the variance
components but not others.  It could turn out that conditioning on the
relative variance-covariance of the random effects is not a big deal
but I don't know.  I haven't examined it in detail and I don't know of
others who have.

Another approach entirely is to use Markov chain Monte Carlo to
examine the joint distribution of the parameters (in the Bayesian
sense) and the random effects.  If you save the fixed effects and the
random effects from the MCMC chain then you can evaluate the linear
combination of interest throughout the chain and get an empirical
distribution of the quantities returned by coef.

This is probably an unsatisfactory answer for Irene who may have
wanted something quick and simple.  Unfortunately, I don't think there
is a quick, simple answer here.

I suggest we move this discussion to the R-SIG-Mixed-Models list which
I am cc:ing on this reply.

> -----Original Message-----
> From: r-help-bounces at r-project.org on behalf of Irene Mantzouni
> Sent: Mon 10/15/2007 3:20 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] coef se in lme
>
> Hi all!
>
> How is it possible to estimate  standard errors for coef obtained from lme?
> Is there sth like se.coef() for lmer or what is the anaytical solution?
>
> Thank you!
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



From yong.li at unimelb.edu.au  Tue Nov  6 14:30:04 2007
From: yong.li at unimelb.edu.au (Yong Li)
Date: Wed, 07 Nov 2007 00:30:04 +1100
Subject: [R-sig-ME] Optimising parameters of a process model using R.
Message-ID: <86DBA0678E017341B449A62F258E295615485B@IS-EX-BEV3.unimelb.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20071107/273970f7/attachment.pl>

From yong.li at unimelb.edu.au  Tue Nov  6 14:05:36 2007
From: yong.li at unimelb.edu.au (Yong Li)
Date: Wed, 07 Nov 2007 00:05:36 +1100
Subject: [R-sig-ME] Optimising parameters of a process model using R
Message-ID: <86DBA0678E017341B449A62F258E295615485A@IS-EX-BEV3.unimelb.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20071107/0bde1e21/attachment.pl>

From ima at difres.dk  Wed Nov  7 15:55:22 2007
From: ima at difres.dk (Irene Mantzouni)
Date: Wed, 7 Nov 2007 15:55:22 +0100
Subject: [R-sig-ME] mixed model testing
Message-ID: <68E7981938EAF54F987AD3848A0A6416E5837C@ka-mail01.dfu.local>

Is there a formal way to prove the need of a mixed model, apart from e.g. comparing the intervals estimated by lmList fit? 
For example, should I compare (with AIC ML?) a model with seperately (unpooled) estimated fixed slopes (i.e.using an index for each group) with a model that treats this parameter as a random effect (both models treat the remaining parameters as random)?
 
Thank you!



From john.maindonald at anu.edu.au  Wed Nov  7 22:46:47 2007
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Thu, 8 Nov 2007 08:46:47 +1100
Subject: [R-sig-ME] mixed model testing
In-Reply-To: <68E7981938EAF54F987AD3848A0A6416E5837C@ka-mail01.dfu.local>
References: <68E7981938EAF54F987AD3848A0A6416E5837C@ka-mail01.dfu.local>
Message-ID: <77651871-6E0D-4496-BADE-EC8D770402C5@anu.edu.au>

Whether or not you need a mixed model, e.g. random versus
fixed slopes, depends on how you intend to use results.

Suppose you have lines of depression vs lawn roller weight
calculated for a number of lawns. If the data will always be
used to make predictions for one of those same lawns, a
fixed slopes model is fine.

If you want to use the data to make a prediction for another
lawn from the same "population" (the population from which
this lawn is a random sample, right?), you need to model
the slope as a random effect.

Now for a more subtle point:

In the prediction for another lawn situation, it is possible that
the slope random effect can be zero, and analysts do very
commonly make this sort of assumption, maybe without
realizing that this is what they are doing.  You can test whether
the slope random effect is zero but, especially if you have data
from a few lawns only, failure to reject the null (zero random
effect) is not a secure basis for inferences that assume that
the slope is indeed zero. The "test for zero random effect, then
infer" is open to Box's pithy objection that
"... to make preliminary tests on variances is rather like putting to
sea in a rowing boat to find out whether conditions are sufficiently
calm for an ocean liner to leave port".


John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.


On 8 Nov 2007, at 1:55 AM, Irene Mantzouni wrote:

> Is there a formal way to prove the need of a mixed model, apart from  
> e.g. comparing the intervals estimated by lmList fit?
> For example, should I compare (with AIC ML?) a model with seperately  
> (unpooled) estimated fixed slopes (i.e.using an index for each  
> group) with a model that treats this parameter as a random effect  
> (both models treat the remaining parameters as random)?
>
> Thank you!
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From Fredrik.X.Nilsson at skane.se  Thu Nov  8 10:00:27 2007
From: Fredrik.X.Nilsson at skane.se (Nilsson Fredrik X)
Date: Thu, 8 Nov 2007 10:00:27 +0100
Subject: [R-sig-ME] mixed model testing
In-Reply-To: <77651871-6E0D-4496-BADE-EC8D770402C5@anu.edu.au>
Message-ID: <87A0C64299B27148B40BE0DB83EDE2DBEFE25C@RSMAIL002.REG.SKANE.SE>

Dear John,

Forgive me for putting my nose out, I hope that I'm not rude, but I am a bit bewildered by your mail (and by statistical modelling).

I agree that if your model is:

Lawndepression~lawn.roller.weight +(1|lawn.id),

When, in fact, it *should be* (because you simulated the data or you're God):

Lawndepression~lawn.roller.weight +(lawn.roller.weight|lawn.id),

then you might erroneously fail to reject the null hypothesis that the random effect for slope is zero. But in real life one does not know the true model, and there are an infinite number of (functional forms for) random effects that such tests may fail upon. What should one do, why stop at the linear term? Why not saturate the model with random effects? (And still, you don't know whether you have the right model). 

In your example one would perhaps like to think that VERY light lawn movers did not cause any depression at all, and that there is a maximum depression that a lawn mover could cause, so there should at least be an f(lawn.roller.weight), or we do as Venables suggest in http://www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf : we centre our lawn mover weights and keep in the middle of this interval where things look linear and fit a linear, Gaussian model.

Then I don't quite get your point from Box' quote since that concerned heterogeneous variances, right? Here the situation is quite the opposite to rejecting the null, using a test for heterogeneous variances that is more sensitive than the ANOVA for departures from homogeneity is akin to sending out a rowing boat on a rough sea where the ocean liner (ANOVA) would safely fare. We failed to reject the null hypothesis of zero slope.

So what is your suggestion of a sensible modelling strategy? (Without being biased by what you see in your dataset; I used to like inspecting the data fitting lmList if possible, then fitting a rather complex model, and then removing insignificant terms, then checking assumptions. After having read Harrell's book (Regression modelling strategies) I'm a bit uncertain what to do when people ask me to analyze their data, since they don't like to think too much about it. Harrell's suggestion that one could check the literature for a sensible model seems pernicious to me since these old results are based on the very same modelling strategy that he rejects. Should one use the Bayesian framework with flat priors?)


Best regards,

Fredrik Nilsson

-----Ursprungligt meddelande-----
Fr?n: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] F?r John Maindonald
Skickat: den 7 november 2007 22:47
Till: Irene Mantzouni
Kopia: r-sig-mixed-models at r-project.org; r-help at stat.math.ethz.ch
?mne: Re: [R-sig-ME] mixed model testing

Whether or not you need a mixed model, e.g. random versus
fixed slopes, depends on how you intend to use results.

Suppose you have lines of depression vs lawn roller weight
calculated for a number of lawns. If the data will always be
used to make predictions for one of those same lawns, a
fixed slopes model is fine.

If you want to use the data to make a prediction for another
lawn from the same "population" (the population from which
this lawn is a random sample, right?), you need to model
the slope as a random effect.

Now for a more subtle point:

In the prediction for another lawn situation, it is possible that
the slope random effect can be zero, and analysts do very
commonly make this sort of assumption, maybe without
realizing that this is what they are doing.  You can test whether
the slope random effect is zero but, especially if you have data
from a few lawns only, failure to reject the null (zero random
effect) is not a secure basis for inferences that assume that
the slope is indeed zero. The "test for zero random effect, then
infer" is open to Box's pithy objection that
"... to make preliminary tests on variances is rather like putting to
sea in a rowing boat to find out whether conditions are sufficiently
calm for an ocean liner to leave port".


John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.


On 8 Nov 2007, at 1:55 AM, Irene Mantzouni wrote:

> Is there a formal way to prove the need of a mixed model, apart from  
> e.g. comparing the intervals estimated by lmList fit?
> For example, should I compare (with AIC ML?) a model with seperately  
> (unpooled) estimated fixed slopes (i.e.using an index for each  
> group) with a model that treats this parameter as a random effect  
> (both models treat the remaining parameters as random)?
>
> Thank you!
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From John.Maindonald at anu.edu.au  Thu Nov  8 12:16:34 2007
From: John.Maindonald at anu.edu.au (John Maindonald)
Date: Thu, 8 Nov 2007 22:16:34 +1100
Subject: [R-sig-ME] mixed model testing
In-Reply-To: <87A0C64299B27148B40BE0DB83EDE2DBEFE25C@RSMAIL002.REG.SKANE.SE>
References: <87A0C64299B27148B40BE0DB83EDE2DBEFE25C@RSMAIL002.REG.SKANE.SE>
Message-ID: <A68194D0-C07F-4BC0-848E-4798DF1BDD20@anu.edu.au>

For simplicity, I limited attention to a rather small class
of models.  I assumed that the only fixed effect that the
data would support is a linear term, and I do not mind
adding an intercept.  That is realistic, I believe, for data
of this type.

One should not limit oneself to a single random effect
if indeed the data are sampled in a manner (e.g., lawns
within soil types) that makes it natural to expect some
further random effect.  In my example as stated, the data
have no such structure. (NB, the random sample comment,
meaning simple random sample).

I agree that I have used Box's example outside of the
context in which he used it.  A better example might be
use of a reconnaissance that has only a 20% chance
of detecting such pirates as may be present on the high
seas, before deciding whether a valuable cargo that will
venture into those seas should have an escort.

Removing insignificant terms can help understanding
and interpretation.  But if one wants to make anything
of the coefficients, it is necessary to check that the
remaining coefficients have not changed substantially.
If there are more than a few terms to consider, and data
are not from a designed experiment, and attempt to
interpret coefficients is likely to be hazardous. You
mentioned the Harrell book. Rosenbaum's "Observational
Studies" (2edn, Springer, 2002) merits careful attention.

Why do you think Harrell's suggestion pernicious?  Models
that are in the literature can be a good starting point, and
the accompanying discussion an aid to understanding
the science. They may turn out to be more or less right
(as far as one can tell), or to require modification, or the
data may demolish them.  But at least one has a starting
point, rather than an almost unlimited choice of models.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.


On 8 Nov 2007, at 8:00 PM, Nilsson Fredrik X wrote:

> Dear John,
>
> Forgive me for putting my nose out, I hope that I'm not rude, but I  
> am a bit bewildered by your mail (and by statistical modelling).
>
> I agree that if your model is:
>
> Lawndepression~lawn.roller.weight +(1|lawn.id),
>
> When, in fact, it *should be* (because you simulated the data or  
> you're God):
>
> Lawndepression~lawn.roller.weight +(lawn.roller.weight|lawn.id),
>
> then you might erroneously fail to reject the null hypothesis that  
> the random effect for slope is zero. But in real life one does not  
> know the true model, and there are an infinite number of (functional  
> forms for) random effects that such tests may fail upon. What should  
> one do, why stop at the linear term? Why not saturate the model with  
> random effects? (And still, you don't know whether you have the  
> right model).
>
> In your example one would perhaps like to think that VERY light lawn  
> movers did not cause any depression at all, and that there is a  
> maximum depression that a lawn mover could cause, so there should at  
> least be an f(lawn.roller.weight), or we do as Venables suggest in http://www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf 
>  : we centre our lawn mover weights and keep in the middle of this  
> interval where things look linear and fit a linear, Gaussian model.
>
> Then I don't quite get your point from Box' quote since that  
> concerned heterogeneous variances, right? Here the situation is  
> quite the opposite to rejecting the null, using a test for  
> heterogeneous variances that is more sensitive than the ANOVA for  
> departures from homogeneity is akin to sending out a rowing boat on  
> a rough sea where the ocean liner (ANOVA) would safely fare. We  
> failed to reject the null hypothesis of zero slope.
>
> So what is your suggestion of a sensible modelling strategy?  
> (Without being biased by what you see in your dataset; I used to  
> like inspecting the data fitting lmList if possible, then fitting a  
> rather complex model, and then removing insignificant terms, then  
> checking assumptions. After having read Harrell's book (Regression  
> modelling strategies) I'm a bit uncertain what to do when people ask  
> me to analyze their data, since they don't like to think too much  
> about it. Harrell's suggestion that one could check the literature  
> for a sensible model seems pernicious to me since these old results  
> are based on the very same modelling strategy that he rejects.  
> Should one use the Bayesian framework with flat priors?)
>
>
> Best regards,
>
> Fredrik Nilsson
>
> -----Ursprungligt meddelande-----
> Fr?n: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org 
> ] F?r John Maindonald
> Skickat: den 7 november 2007 22:47
> Till: Irene Mantzouni
> Kopia: r-sig-mixed-models at r-project.org; r-help at stat.math.ethz.ch
> ?mne: Re: [R-sig-ME] mixed model testing
>
> Whether or not you need a mixed model, e.g. random versus
> fixed slopes, depends on how you intend to use results.
>
> Suppose you have lines of depression vs lawn roller weight
> calculated for a number of lawns. If the data will always be
> used to make predictions for one of those same lawns, a
> fixed slopes model is fine.
>
> If you want to use the data to make a prediction for another
> lawn from the same "population" (the population from which
> this lawn is a random sample, right?), you need to model
> the slope as a random effect.
>
> Now for a more subtle point:
>
> In the prediction for another lawn situation, it is possible that
> the slope random effect can be zero, and analysts do very
> commonly make this sort of assumption, maybe without
> realizing that this is what they are doing.  You can test whether
> the slope random effect is zero but, especially if you have data
> from a few lawns only, failure to reject the null (zero random
> effect) is not a secure basis for inferences that assume that
> the slope is indeed zero. The "test for zero random effect, then
> infer" is open to Box's pithy objection that
> "... to make preliminary tests on variances is rather like putting to
> sea in a rowing boat to find out whether conditions are sufficiently
> calm for an ocean liner to leave port".
>
>
> John Maindonald             email: john.maindonald at anu.edu.au
> phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
> Centre for Mathematics & Its Applications, Room 1194,
> John Dedman Mathematical Sciences Building (Building 27)
> Australian National University, Canberra ACT 0200.
>
>
> On 8 Nov 2007, at 1:55 AM, Irene Mantzouni wrote:
>
>> Is there a formal way to prove the need of a mixed model, apart from
>> e.g. comparing the intervals estimated by lmList fit?
>> For example, should I compare (with AIC ML?) a model with seperately
>> (unpooled) estimated fixed slopes (i.e.using an index for each
>> group) with a model that treats this parameter as a random effect
>> (both models treat the remaining parameters as random)?
>>
>> Thank you!
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From Fredrik.X.Nilsson at skane.se  Thu Nov  8 12:54:11 2007
From: Fredrik.X.Nilsson at skane.se (Nilsson Fredrik X)
Date: Thu, 8 Nov 2007 12:54:11 +0100
Subject: [R-sig-ME] mixed model testing
In-Reply-To: <A68194D0-C07F-4BC0-848E-4798DF1BDD20@anu.edu.au>
Message-ID: <87A0C64299B27148B40BE0DB83EDE2DBEFE283@RSMAIL002.REG.SKANE.SE>

Dear John,

Thank you for your reply. Well then I agree, a random effect should be added if one suspects that it should be there. Neat example with pirates!

Pernicious is a too strong word, but dangerous. Since, as I said, many of the results found in the literature are based on screening for covariates of interest, including only those that have a p-value less than 0.30 (say) in a forward selection model. Using simple linear, or at best, (fractional) polynomial bases to represent covariates, and almost always ignoring interactions (difficult to present). I suppose that it would be OK to be inspired by them, but with a good amount of distrust (if Harrell is right, which seems to be the case). This distrust could perhaps be lessened when the literature concerns randomised trials although covariates may find their way into this area to (and still no interactions...).

Best regards,

Fredrik


-----Ursprungligt meddelande-----
Fr?n: John Maindonald [mailto:John.Maindonald at anu.edu.au] 
Skickat: den 8 november 2007 12:17
Till: Nilsson Fredrik X
Kopia: r-sig-mixed-models at r-project.org
?mne: Re: SV: [R-sig-ME] mixed model testing

For simplicity, I limited attention to a rather small class
of models.  I assumed that the only fixed effect that the
data would support is a linear term, and I do not mind
adding an intercept.  That is realistic, I believe, for data
of this type.

One should not limit oneself to a single random effect
if indeed the data are sampled in a manner (e.g., lawns
within soil types) that makes it natural to expect some
further random effect.  In my example as stated, the data
have no such structure. (NB, the random sample comment,
meaning simple random sample).

I agree that I have used Box's example outside of the
context in which he used it.  A better example might be
use of a reconnaissance that has only a 20% chance
of detecting such pirates as may be present on the high
seas, before deciding whether a valuable cargo that will
venture into those seas should have an escort.

Removing insignificant terms can help understanding
and interpretation.  But if one wants to make anything
of the coefficients, it is necessary to check that the
remaining coefficients have not changed substantially.
If there are more than a few terms to consider, and data
are not from a designed experiment, and attempt to
interpret coefficients is likely to be hazardous. You
mentioned the Harrell book. Rosenbaum's "Observational
Studies" (2edn, Springer, 2002) merits careful attention.

Why do you think Harrell's suggestion pernicious?  Models
that are in the literature can be a good starting point, and
the accompanying discussion an aid to understanding
the science. They may turn out to be more or less right
(as far as one can tell), or to require modification, or the
data may demolish them.  But at least one has a starting
point, rather than an almost unlimited choice of models.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.


On 8 Nov 2007, at 8:00 PM, Nilsson Fredrik X wrote:

> Dear John,
>
> Forgive me for putting my nose out, I hope that I'm not rude, but I  
> am a bit bewildered by your mail (and by statistical modelling).
>
> I agree that if your model is:
>
> Lawndepression~lawn.roller.weight +(1|lawn.id),
>
> When, in fact, it *should be* (because you simulated the data or  
> you're God):
>
> Lawndepression~lawn.roller.weight +(lawn.roller.weight|lawn.id),
>
> then you might erroneously fail to reject the null hypothesis that  
> the random effect for slope is zero. But in real life one does not  
> know the true model, and there are an infinite number of (functional  
> forms for) random effects that such tests may fail upon. What should  
> one do, why stop at the linear term? Why not saturate the model with  
> random effects? (And still, you don't know whether you have the  
> right model).
>
> In your example one would perhaps like to think that VERY light lawn  
> movers did not cause any depression at all, and that there is a  
> maximum depression that a lawn mover could cause, so there should at  
> least be an f(lawn.roller.weight), or we do as Venables suggest in http://www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf 
>  : we centre our lawn mover weights and keep in the middle of this  
> interval where things look linear and fit a linear, Gaussian model.
>
> Then I don't quite get your point from Box' quote since that  
> concerned heterogeneous variances, right? Here the situation is  
> quite the opposite to rejecting the null, using a test for  
> heterogeneous variances that is more sensitive than the ANOVA for  
> departures from homogeneity is akin to sending out a rowing boat on  
> a rough sea where the ocean liner (ANOVA) would safely fare. We  
> failed to reject the null hypothesis of zero slope.
>
> So what is your suggestion of a sensible modelling strategy?  
> (Without being biased by what you see in your dataset; I used to  
> like inspecting the data fitting lmList if possible, then fitting a  
> rather complex model, and then removing insignificant terms, then  
> checking assumptions. After having read Harrell's book (Regression  
> modelling strategies) I'm a bit uncertain what to do when people ask  
> me to analyze their data, since they don't like to think too much  
> about it. Harrell's suggestion that one could check the literature  
> for a sensible model seems pernicious to me since these old results  
> are based on the very same modelling strategy that he rejects.  
> Should one use the Bayesian framework with flat priors?)
>
>
> Best regards,
>
> Fredrik Nilsson
>
> -----Ursprungligt meddelande-----
> Fr?n: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org 
> ] F?r John Maindonald
> Skickat: den 7 november 2007 22:47
> Till: Irene Mantzouni
> Kopia: r-sig-mixed-models at r-project.org; r-help at stat.math.ethz.ch
> ?mne: Re: [R-sig-ME] mixed model testing
>
> Whether or not you need a mixed model, e.g. random versus
> fixed slopes, depends on how you intend to use results.
>
> Suppose you have lines of depression vs lawn roller weight
> calculated for a number of lawns. If the data will always be
> used to make predictions for one of those same lawns, a
> fixed slopes model is fine.
>
> If you want to use the data to make a prediction for another
> lawn from the same "population" (the population from which
> this lawn is a random sample, right?), you need to model
> the slope as a random effect.
>
> Now for a more subtle point:
>
> In the prediction for another lawn situation, it is possible that
> the slope random effect can be zero, and analysts do very
> commonly make this sort of assumption, maybe without
> realizing that this is what they are doing.  You can test whether
> the slope random effect is zero but, especially if you have data
> from a few lawns only, failure to reject the null (zero random
> effect) is not a secure basis for inferences that assume that
> the slope is indeed zero. The "test for zero random effect, then
> infer" is open to Box's pithy objection that
> "... to make preliminary tests on variances is rather like putting to
> sea in a rowing boat to find out whether conditions are sufficiently
> calm for an ocean liner to leave port".
>
>
> John Maindonald             email: john.maindonald at anu.edu.au
> phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
> Centre for Mathematics & Its Applications, Room 1194,
> John Dedman Mathematical Sciences Building (Building 27)
> Australian National University, Canberra ACT 0200.
>
>
> On 8 Nov 2007, at 1:55 AM, Irene Mantzouni wrote:
>
>> Is there a formal way to prove the need of a mixed model, apart from
>> e.g. comparing the intervals estimated by lmList fit?
>> For example, should I compare (with AIC ML?) a model with seperately
>> (unpooled) estimated fixed slopes (i.e.using an index for each
>> group) with a model that treats this parameter as a random effect
>> (both models treat the remaining parameters as random)?
>>
>> Thank you!
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From Fabian.Scheipl at stat.uni-muenchen.de  Thu Nov  8 13:48:58 2007
From: Fabian.Scheipl at stat.uni-muenchen.de (Fabian Scheipl)
Date: Thu, 8 Nov 2007 13:48:58 +0100
Subject: [R-sig-ME] mixed model testing
Message-ID: <4836bc6a0711080448j599c23b0i2e7b82be7dc8f5a4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20071108/0c96d384/attachment.pl>

From bates at stat.wisc.edu  Thu Nov  8 18:40:42 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 8 Nov 2007 11:40:42 -0600
Subject: [R-sig-ME] Nested Mixed Models in lme4
In-Reply-To: <47330D93.40800@imada.sdu.dk>
References: <47330D93.40800@imada.sdu.dk>
Message-ID: <40e66e0b0711080940j643fe7eapc50696d721d5932a@mail.gmail.com>

On 11/8/07, Marco Chiarandini <marco at imada.sdu.dk> wrote:
> Dear Prof. Bates,

> I am trying to use the function lmer from lme4 to
> analyse the following nested factorial design.

> I have three treatment factors (neighborhood,
> initial, k);
> I have three group factors crossing (size, dens,
> inst).

Did you mean to write (size, dens, type) there?

Also, by "factor" do you mean that you regard all of these variables
as categorical?  If so, you should check the form of the size variable
in the data frame.  It is being stored as a numeric variable, not as a
factor.  If you want to interpret this  variable as a categorical
factor you should convert it to a factor or, as seems likely in this
case, an ordered factor.  (See ?factor and ?ordered)

> I have one random factor (Subject or, in my case,
> inst) nested within the groups generated by the
> crossing of the group factors.

Nesting is automatically handled appropriately in lmer as long as the
levels of the inst factor are distinct. That is, if each distinct
level of the inst factor has a distinct label, which also appears
likely in this case because I see the first label contains G and 200
and I assume that these parts of the name correspond to the level of
the type and size variables.  If so, then you simply need to use inst
as the grouping factor for the random effects.

The only time that nesting must be explicitly stated is when the
levels of the variable(s) at the inner level(s) are incomplete.  I
call this "implicit nesting".  Suppose I choose 20 different plants
from an experimental plot and extract several seeds from each plant
then perform multiple analyses on each seed, I could label the plants
"A", "B", "C", ...., "T" and the seeds "a", "b","c", ... for each
plant.  This is implicit nesting in that I only have, say, 12 seed
labels but there may be one or two hundred seeds.  To specify a
particular seed I must not only specify its seed label but also the
plant from which it came.  If I just specify "seed" in a model formula
I will get an inappropriate model fit.  I need to somehow specify seed
within plant.

In my view this is not a characteristic of the experiment - it's just
a dumb way of labeling the seeds.  If you use labels like "Aa", "Ab",
..., as I suspect you have done for your "inst" factor, then the
problem goes away.

> I would like to write a formula for lmer of the kind:
>
> treat1 + treat2 + treat3 + group1 + group2 +
> group3 + Subject(group1*group2*group3)
>
> eventually plus some interaction terms.
>
> I am trying the following:
>
> mod <-
> lmer(err~k+initial+neighborhood+size+dens+type+(1|inst),data=Case3)

I think that specifcation corresponds to the model that you describe
above, although I am not quite sure what the distinction between a
treatment factor and a group factor is.

>  > summary(mod)
> Linear mixed-effects model fit by REML
> Formula: err ~ k + initial + neighborhood + size +
> dens + type + (1 |      inst)
>     Data: Case3
>     AIC   BIC logLik MLdeviance REMLdeviance
>   16472 16542  -8224      16433        16448
> Random effects:
>   Groups   Name        Variance Std.Dev.
>   inst     (Intercept)  2.61    1.62
>   Residual             49.23    7.02
> number of obs: 2430, groups: inst, 90

It appears that the random effect for the inst factor may be
unnecessary.  You may want to check what the log-likelihood for a
model with only fixed-effects is.

>
>  > anova(mod)
> Analysis of Variance Table
>               Df Sum Sq Mean Sq
> k             2   5473    2736
> initial       2  72168   36084
> neighborhood  2  18622    9311
> size          1     52      52
> dens          2      2       1
> type          1  31378   31378

> But results are very different from what I get in SAS

I can't really comment on that without knowing how you specified the
model in SAS and what analysis of variance results from SAS you are
comparing.  This analysis of variance table, like most such tables in
R, is the decomposition of the variation in the response according to
the terms in the order they were given in the formula.  As Bill
Venables describes in his famous (and, regretably, unpublished) paper
"Exegeses on linear models" (just search for the title in a search
engine) this is the only decomposition that makes sense but that does
not deter many people, including the authors of SAS, from creating
other decompositions that may on the surface appear to make sense but
do not withstand careful scrutiny.

It appears that you may have a completely balanced experiment here
(I'm guessing that it is a computer experiment) in which case the
decomposition is invariant to reordering of the terms in the model.
In general, if type, size and dens are blocking factors or
environmental factors (that is, they represent a known source of
variability and you wish to control for these factors in examining
your experimental factors) then they should be entered first in the
formula.

> and I cannot figure out why the anova method
> does not return the test for significance.

Ah, that's a long story.  One can calculate F-ratios for fixed-effects
terms in a linear mixed model but they don't have an F distribution
except in certain balanced cases.  Determining a p-value for a
fixed-effects term in a mixed model fit to unbalanced data is not
trivial.  At one time I did list p-values in such a table but they
were approximations and rather coarse approximations that erred in the
wrong direction.  Some users, quite reasonably, objected that these
could be dangerously misleading so my current solution is not to
return a p-value at all.

> If I try this formula, the computation never stops:
>
> mod <- lmer(err~initial+neighborhood+k+(1|inst) +
> (inst|type)+(inst|size) + (inst|dens),data=Case3)

You really, really don't want to try that.  The expression on the left
hand side of a random-effects term is treated as a linear model
formula from which a model matrix is evaluated.  The model matrix for
inst has 90 columns so each level of type is being modeled as having
90, possibly correlated, random effects associated with it.  The same
for size and dens.  90 correlated random effects requires estimation
of 90 variance parameters and 4005 covariance parameters.  The general
rule is that a factor on the left hand side of the '|' should have a
very small number of levels whereas a factor on the right hand side
should have a large number of levels.

>
> Which should be the right formula?
> In case, this is the structure of my data:
>
>  > str(Case3)
> 'data.frame':   2430 obs. of  8 variables:
>   $ err         : num  12.60 11.03  9.99  2.48
> 2.48 ...
>   $ initial     : Factor w/ 3 levels
> "greedy_cov","lightest_add",..: 2 2 2 1 1 1 3 3 3
> 2 ...
>   $ neighborhood: Factor w/ 3 levels
> "k-add","k-cov",..: 1 1 1 1 1 1 1 1 1 2 ...
>   $ k           : Factor w/ 3 levels "1","3","5":
> 1 2 3 1 2 3 1 2 3 1 ...
>   $ type        : Factor w/ 2 levels "G","U": 1 1
> 1 1 1 1 1 1 1 1 ...
>   $ size        : num  200 200 200 200 200 200 200
> 200 200 200 ...
>   $ dens        : Factor w/ 3 levels "L","M","H":
> 1 1 1 1 1 1 1 1 1 1 ...
>   $ inst        : Factor w/ 90 levels
> "G-200-0.1-1",..: 1 1 1 1 1 1 1 1 1 1 ...
>
>
>
> Thank you in advance for any suggestion you may
> provide.
>
> Best regards,
>
> Marco
>
>
>
>
> --
> Marco Chiarandini
> http://www.imada.sdu.dk/~marco
> Department of Mathematics             Email:
> marco at imada.sdu.dk
> and Computer Science,                 Phone: +45 6550 4031
> University of Southern Denmark        Fax: +45
> 6593 2691
>
>
>



From njs at pobox.com  Fri Nov  9 06:06:47 2007
From: njs at pobox.com (Nathaniel Smith)
Date: Fri, 9 Nov 2007 05:06:47 +0000
Subject: [R-sig-ME] lme4 bugs: ping
Message-ID: <20071109050647.GB5326@frances.vorpus.org>

I've reported two bugs to this list recently that received no
response:
  lmList does not work for glms (patch included):
     https://stat.ethz.ch/pipermail/r-sig-mixed-models/2007q4/000374.html
  lmer doesn't either:
     https://stat.ethz.ch/pipermail/r-sig-mixed-models/2007q4/000402.html
     https://stat.ethz.ch/pipermail/r-sig-mixed-models/2007q4/000404.html

The first problem is easy enough to fix or work around, just annoying
(but also trivial to fix upstream, since a patch is included!).  The
latter is much more problematic -- lmer is silently giving wildly
invalid results without any indication that they are, in fact,
invalid, and I have no idea how to work around this problem, which
I am encountering while trying to analyze real data that I really
intend to publish once I know how to analyze it.

Doug, I know you're busy, but any chance you could take a look at the
above, esp. the latter?

-- Nathaniel

-- 
"The problem...is that sets have a very limited range of
activities -- they can't carry pianos, for example, nor drink
beer."



From h.buettger at bioconsult-sh.de  Fri Nov  9 09:45:12 2007
From: h.buettger at bioconsult-sh.de (=?ISO-8859-15?Q?=22H=2E_B=FCttger=22?=)
Date: Fri, 09 Nov 2007 09:45:12 +0100
Subject: [R-sig-ME] lmer warning message
Message-ID: <47341E18.2040406@bioconsult-sh.de>

Hi,

I am getting these two warning message for different data when I'am 
using a lmer (R version 2.5.1):

Fehler in if (any(sd < 0)) return("'sd' slot has negative entries") : 
Fehlender Wert, wo TRUE/FALSE n?tig ist

and

CHOLMOD warning: matrix not positive definite
Fehler in objective(.par, ...) : Cholmod error `matrix not positive 
definite' at file:../Supernodal/t_cholmod_super_numeric.c, line 614


Whats does it mean?


Thanks for your help!

Heike



From ima at difres.dk  Fri Nov  9 16:59:27 2007
From: ima at difres.dk (Irene Mantzouni)
Date: Fri, 9 Nov 2007 16:59:27 +0100
Subject: [R-sig-ME] segmented regression mixed model?
Message-ID: <68E7981938EAF54F987AD3848A0A6416E5837E@ka-mail01.dfu.local>

Hi all!
 
Is it possible to use a segmented regression model as the functional form of a linear (or maybe non-linear?) mixed model?
 
Cheers,
Irene



From marco at imada.sdu.dk  Fri Nov  9 17:41:34 2007
From: marco at imada.sdu.dk (Marco Chiarandini)
Date: Fri, 09 Nov 2007 17:41:34 +0100
Subject: [R-sig-ME] Nested Mixed Models in lme4
In-Reply-To: <40e66e0b0711080940j643fe7eapc50696d721d5932a@mail.gmail.com>
References: <47330D93.40800@imada.sdu.dk>
	<40e66e0b0711080940j643fe7eapc50696d721d5932a@mail.gmail.com>
Message-ID: <47348DBE.1060407@imada.sdu.dk>

Dear Prof. Bates,


>> I am trying to use the function lmer from lme4 to
>> analyse the following nested factorial design.
> 
>> I have three treatment factors (neighborhood,
>> initial, k);
>> I have three group factors crossing (size, dens,
>> inst).
> 
> Did you mean to write (size, dens, type) there?
> 
> Also, by "factor" do you mean that you regard all of these variables
> as categorical?  If so, you should check the form of the size variable
> in the data frame.  It is being stored as a numeric variable, not as a
> factor.  If you want to interpret this  variable as a categorical
> factor you should convert it to a factor or, as seems likely in this
> case, an ordered factor.  (See ?factor and ?ordered)


yes, thank you a lot! All your corrections are 
appropriate! inst should have been type and all 
variables should have been categorical. My mistake.
Also: as you correctly pointed out, the data are 
from a computer experiment and perfectly balanced, 
and by group factors I meant blocking factors.

Your very clear explanation solved my concerns 
about the nesting! Thanks!

I've also redone the comparison with SAS and now 
results correspond.
The reason was mainly that I needed a quite 
different formula:

lmer(err~initial*neighborhood + initial*k + 
initial*type + initial*size + initial*dens + 
neighborhood*k + neighborhood*type + 
neighborhood*size + neighborhood*dens + k*type + 
k*size + k*dens + type*size + type*dens + 
size*dens + initial*neighborhood*k + 
(1|inst),data=Case3)

True also that we were using lsmeans in SAS that 
you discourage.

To me it would remain only to understand how I 
could obtain the results in a cell means format 
like those in SAS. But this seems to be a problem 
also in lm and hence I must probably study better 
how things work to find the way. Trying something 
of the kind:

fmm1 <- 
lmer(err~-1+ordered(size)+dens+type+(k+initial+neighborhood)^3+(1|inst),data=Case3)

does not seem to help much.

I left all the analysis I did, code + results, 
(SAS and R) at:

http://www.imada.sdu.dk/~marco/Mixed/


Thank you a lot very much for the help!

Best regards,

Marco



-- 
Marco Chiarandini 
http://www.imada.sdu.dk/~marco
Department of Mathematics	      Email: 
marco at imada.sdu.dk
and Computer Science,		      Phone: +45 6550 4031
University of Southern Denmark        Fax: +45 
6593 2691



From bates at stat.wisc.edu  Fri Nov  9 22:02:52 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 9 Nov 2007 15:02:52 -0600
Subject: [R-sig-ME] lme4 is now on R-forge
Message-ID: <40e66e0b0711091302h7ccb532bx94c7312526a774b6@mail.gmail.com>

Users of the current version of the lme4 package have reported several
problems and, for some time, I have been unresponsive about such reports
or I have made reference to the development version of the package.  Let
me emphasize that I am grateful for the reports and, indeed, have fixed
several of these problems in the development version of the package.
However, I have held off releasing the development version because of
one small problem - it doesn't fit generalized linear mixed models correctly.

I have had to go back and reformulate the model from scratch so that I
can understand it and design the code.  As anyone who has developed
and maintained a large project can attest, the only way to build
trustworthy code (and to maintain your sanity) is to modularize the
code.  It goes without saying that before you can decide how to
modularize the code you must be able to decompose the steps in the
computation.  The development version is designed to handle linear
mixed models, generalized linear mixed models, nonlinear mixed models
and generalized nonlinear mixed models with nested or crossed or
partially crossed random factors.  It has taken me a long time to
decide how all those pieces fit together.  Only in the last couple of
weeks have I have managed to convince myself that I know how it all
fits together.  The task of convincing others remains, and is
decidedly non-trivial, but I feel that I can decompose the
computational steps now.

It will take a while to move from the equations in my lab notebook to
released code and, during that process, I will probably need to
reformulate the slots in the S4 classes.  My method of getting to the
final design of the data structures and algorithms is to keep doing it
wrong 'til I do it right.

So that others have easy access to the development version of the
package I have moved the repository for the development version of
the package to http://R-forge.R-project.org/packages/lme4

Martin and I had planned to do this move in a way that would preserve
the history of the changes from the current repository
but that is not easy to do because of the way that the Matrix and lme4
packages were merged then un-merged.  Thus I have made a clean break
and installed the development version (the one known as gappy-lmer) on
R-forge.  You can access it at the URL given above or as
http://lme4.r-forge.r-project.org/, at the expense of one additional click.

Starting tomorrow you should also be able to install the development
version of the package with

install.packages("lme4", repos = "http://r-forge.r-project.org")

Please be aware that the class representations can change so when
using the development version you should not count on being able to re-use
a fitted model after installing a new version.  You should retain the original
data so you can refit the model if necessary.



From Gustaf.Granath at ebc.uu.se  Wed Nov 14 12:58:28 2007
From: Gustaf.Granath at ebc.uu.se (Gustaf Granath)
Date: Wed, 14 Nov 2007 12:58:28 +0100
Subject: [R-sig-ME] Resid() and estimable() functions with lmer
Message-ID: <473AE2E4.9030205@ebc.uu.se>

Hi all,

Two questions:

1. Is there a way to evaluate models from lmer() with a poisson  
distribution? I get the following error message:

library(lme4)
lmer(tot.fruit~infl.treat+def.treat+(1|initial.size),family=poisson)->model
plot(fitted(model),resid(model))
Error: 'resid' is not implemented yet

Are there any other options?

2. Why doesn't the function estimable() in gmodels work with lmer()  
using a poisson distribution? I know that my coefficients are right  
because it works fine if I run the model without poisson distribution.

#CODE (with latest R version and package updates)#
#Without Poisson distribution#

library(lme4)
library(gmodels)
lmer(tot.fruit~infl.treat+def.treat+(1|initial.size))->model1
summary(model1)

Linear mixed-effects model fit by REML
Formula: tot.fruit ~ infl.treat + def.treat + (1 | initial.size)
   AIC  BIC logLik MLdeviance REMLdeviance
  2449 2471  -1218       2447         2437
Random effects:
  Groups       Name        Variance Std.Dev.
  initial.size (Intercept) 71.585   8.4608
  Residual                 49.856   7.0608
number of obs: 323, groups: initial.size, 292

Fixed effects:
             Estimate Std. Error t value
(Intercept)  12.8846     1.3028   9.890
infl.treat1  -0.4738     1.1819  -0.401
def.treat2   -3.5522     1.6022  -2.217
def.treat3   -2.1757     1.6461  -1.322
def.treat4   -2.1613     1.7003  -1.271

Correlation of Fixed Effects:
             (Intr) infl.1 df.tr2 df.tr3
infl.treat1 -0.413
def.treat2  -0.616  0.013
def.treat3  -0.641  0.002  0.493
def.treat4  -0.638  0.028  0.469  0.524

#Coefficients#

mean.infl.treat0=c(1,0,1/4,1/4,1/4)
mean.infl.treat1=c(1,1,1/4,1/4,1/4)
mean.def.treat1=c(1,1/2,0,0,0)
mean.def.treat2=c(1,1/2,1,0,0)
mean.def.treat3=c(1,1/2,0,1,0)
mean.def.treat4=c(1,1/2,0,0,1)
means=rbind(mean.infl.treat0,mean.infl.treat1,mean.def.treat1,mean.def.treat2,mean.def.treat3,mean.def.treat4)
estimable(model1,means,sim.lmer=TRUE,n.sim=1000)

                       Estimate Std. Error p value
(1 0 0.25 0.25 0.25) 10.897825  0.8356260       0
(1 1 0.25 0.25 0.25) 10.421633  0.9364839       0
(1 0.5 0 0 0)        12.577408  1.2055979       0
(1 0.5 1 0 0)         9.097063  1.1769760       0
(1 0.5 0 1 0)        10.523485  1.2238635       0
(1 0.5 0 0 1)        10.440959  1.2031459       0

#With Poisson distribution#

lmer(tot.fruit~infl.treat+def.treat+(1|initial.size),family=poisson)->model2
summary(model2)

Generalized linear mixed model fit using Laplace
Formula: tot.fruit ~ infl.treat + def.treat + (1 | initial.size)
  Family: poisson(log link)
   AIC  BIC logLik deviance
  1073 1096 -530.5     1061
Random effects:
  Groups       Name        Variance Std.Dev.
  initial.size (Intercept) 0.84201  0.91761
number of obs: 323, groups: initial.size, 292

Estimated scale (compare to  1 )  1.130529

Fixed effects:
             Estimate Std. Error z value Pr(>|z|)
(Intercept)  2.08865    0.10478  19.934  < 2e-16 ***
infl.treat1  0.10615    0.09412   1.128  0.25941
def.treat2  -0.37354    0.11398  -3.277  0.00105 **
def.treat3  -0.18566    0.12679  -1.464  0.14310
def.treat4  -0.10624    0.13451  -0.790  0.42962
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Correlation of Fixed Effects:
             (Intr) infl.1 df.tr2 df.tr3
infl.treat1 -0.418
def.treat2  -0.554  0.016
def.treat3  -0.599 -0.001  0.467
def.treat4  -0.623  0.055  0.460  0.548

estimable(model2,lsmeans,sim.lmer=TRUE,n.sim=1000)
Error in FUN(newX[, i], ...) :
   `param' has no names and does not match number of coefficients of  
model. Unable to construct coefficient vector

#End of CODE#

Any ideas as to why this is? It does say in the help of ?estimable  
that it should work for lmer objects.

Thanks in advance for any help,

Gustaf (PhD student)


Dept of Plant Ecology
Evolutionary Biology Centre (EBC)
Uppsala University



From bates at stat.wisc.edu  Wed Nov 14 14:17:45 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 14 Nov 2007 07:17:45 -0600
Subject: [R-sig-ME] Resid() and estimable() functions with lmer
In-Reply-To: <473AE2E4.9030205@ebc.uu.se>
References: <473AE2E4.9030205@ebc.uu.se>
Message-ID: <40e66e0b0711140517l69a470cx53929c094002144d@mail.gmail.com>

On Nov 14, 2007 5:58 AM, Gustaf Granath <Gustaf.Granath at ebc.uu.se> wrote:
> Hi all,

> Two questions:

> 1. Is there a way to evaluate models from lmer() with a poisson
> distribution? I get the following error message:

> library(lme4)
> lmer(tot.fruit~infl.treat+def.treat+(1|initial.size),family=poisson)->model
> plot(fitted(model),resid(model))
> Error: 'resid' is not implemented yet

The model can be fit, it is the resid function that is not yet
implemented.  The reason it is not yet implemented is because there
are many different kinds of residuals for generalized linear mixed
models.  It requires considerably more design and coding than does
obtaining the residuals for a linear mixed model, and even those are
difficult to define for the general model (Do you incorporate the
random effects or not?  If you do, what do you use in the case of
multiple grouping factors?)

Right now I am thinking about other issues in the form of the model so
I can incorporate both generalized linear mixed models and nonlinear
mixed models.  Residuals for generalized linear mixed models are
further down the priority list.

> Are there any other options?
>
> 2. Why doesn't the function estimable() in gmodels work with lmer()
> using a poisson distribution? I know that my coefficients are right
> because it works fine if I run the model without poisson distribution.

I don't know what the estimable function expects the structure of an
lmer or glmer model to be but it would not surprise me if it did not
coincide with the current version.    The structure is continuously
evolving, not because I want to make things difficult for others but
because my understanding of the model and the computational methods
evolves.  On something as complicated as a generalized nonlinear mixed
model with crossed grouping factors it is not surprising that one
doesn't get the design down on the first try.  At least this one
doesn't.  I hope the structure of the classes and the algorithms will
stabilize soon (say, by the end of the year).

Even then it will be difficult to implement something like estimable
without doing finite difference calculations for generalized linear
mixed models.  A matrix that determines the marginal variance
covariance for the fixed-effects estimates (well, not quite but I
don't want to get into the technicalities) is calculated for linear
mixed models but not for generalized linear mixed models.

> #CODE (with latest R version and package updates)#
> #Without Poisson distribution#
>
> library(lme4)
> library(gmodels)
> lmer(tot.fruit~infl.treat+def.treat+(1|initial.size))->model1
> summary(model1)
>
> Linear mixed-effects model fit by REML
> Formula: tot.fruit ~ infl.treat + def.treat + (1 | initial.size)
>    AIC  BIC logLik MLdeviance REMLdeviance
>   2449 2471  -1218       2447         2437
> Random effects:
>   Groups       Name        Variance Std.Dev.
>   initial.size (Intercept) 71.585   8.4608
>   Residual                 49.856   7.0608
> number of obs: 323, groups: initial.size, 292
>
> Fixed effects:
>              Estimate Std. Error t value
> (Intercept)  12.8846     1.3028   9.890
> infl.treat1  -0.4738     1.1819  -0.401
> def.treat2   -3.5522     1.6022  -2.217
> def.treat3   -2.1757     1.6461  -1.322
> def.treat4   -2.1613     1.7003  -1.271
>
> Correlation of Fixed Effects:
>              (Intr) infl.1 df.tr2 df.tr3
> infl.treat1 -0.413
> def.treat2  -0.616  0.013
> def.treat3  -0.641  0.002  0.493
> def.treat4  -0.638  0.028  0.469  0.524
>
> #Coefficients#
>
> mean.infl.treat0=c(1,0,1/4,1/4,1/4)
> mean.infl.treat1=c(1,1,1/4,1/4,1/4)
> mean.def.treat1=c(1,1/2,0,0,0)
> mean.def.treat2=c(1,1/2,1,0,0)
> mean.def.treat3=c(1,1/2,0,1,0)
> mean.def.treat4=c(1,1/2,0,0,1)
> means=rbind(mean.infl.treat0,mean.infl.treat1,mean.def.treat1,mean.def.treat2,mean.def.treat3,mean.def.treat4)
> estimable(model1,means,sim.lmer=TRUE,n.sim=1000)
>
>                        Estimate Std. Error p value
> (1 0 0.25 0.25 0.25) 10.897825  0.8356260       0
> (1 1 0.25 0.25 0.25) 10.421633  0.9364839       0
> (1 0.5 0 0 0)        12.577408  1.2055979       0
> (1 0.5 1 0 0)         9.097063  1.1769760       0
> (1 0.5 0 1 0)        10.523485  1.2238635       0
> (1 0.5 0 0 1)        10.440959  1.2031459       0
>
> #With Poisson distribution#
>
> lmer(tot.fruit~infl.treat+def.treat+(1|initial.size),family=poisson)->model2
> summary(model2)
>
> Generalized linear mixed model fit using Laplace
> Formula: tot.fruit ~ infl.treat + def.treat + (1 | initial.size)
>   Family: poisson(log link)
>    AIC  BIC logLik deviance
>   1073 1096 -530.5     1061
> Random effects:
>   Groups       Name        Variance Std.Dev.
>   initial.size (Intercept) 0.84201  0.91761
> number of obs: 323, groups: initial.size, 292
>
> Estimated scale (compare to  1 )  1.130529
>
> Fixed effects:
>              Estimate Std. Error z value Pr(>|z|)
> (Intercept)  2.08865    0.10478  19.934  < 2e-16 ***
> infl.treat1  0.10615    0.09412   1.128  0.25941
> def.treat2  -0.37354    0.11398  -3.277  0.00105 **
> def.treat3  -0.18566    0.12679  -1.464  0.14310
> def.treat4  -0.10624    0.13451  -0.790  0.42962
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> Correlation of Fixed Effects:
>              (Intr) infl.1 df.tr2 df.tr3
> infl.treat1 -0.418
> def.treat2  -0.554  0.016
> def.treat3  -0.599 -0.001  0.467
> def.treat4  -0.623  0.055  0.460  0.548
>
> estimable(model2,lsmeans,sim.lmer=TRUE,n.sim=1000)
> Error in FUN(newX[, i], ...) :
>    `param' has no names and does not match number of coefficients of
> model. Unable to construct coefficient vector
>
> #End of CODE#
>
> Any ideas as to why this is? It does say in the help of ?estimable
> that it should work for lmer objects.
>
> Thanks in advance for any help,
>
> Gustaf (PhD student)
>
>
> Dept of Plant Ecology
> Evolutionary Biology Centre (EBC)
> Uppsala University
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From ima at difres.dk  Wed Nov 14 20:23:52 2007
From: ima at difres.dk (Irene Mantzouni)
Date: Wed, 14 Nov 2007 20:23:52 +0100
Subject: [R-sig-ME] non-linear GLMMs
Message-ID: <68E7981938EAF54F987AD3848A0A6416E5838B@ka-mail01.dfu.local>

Is it (or will it become...) possible to fit a mixed non-linear SSmicmen model assuming log-normal error?
I have tried to use a log transformed micmen model but it does not seem so powerful. 
Any other ideas about possible approaches are more than welcome!
 
Irene



From saly at ucdavis.edu  Thu Nov 15 08:02:40 2007
From: saly at ucdavis.edu (Sharif S. Aly)
Date: Wed, 14 Nov 2007 23:02:40 -0800
Subject: [R-sig-ME] mixed model with crossed effects
Message-ID: <473BEF10.3070105@ucdavis.edu>

Dear list,
If you please help me with the R command for a 4 level random effects 
model I would be grateful.  I chose to put the details of my study at 
the end of the email as some may prefer not to read it.

My outcome is continuous (normally distributed). All levels are factor 
levels. The lowest level (veterinarian) is crossed with level 2 (day of 
sampling) which is crossed with level 3 (sampled pen or location on 
farm) which is nested in level 4 (dairy).

I understand I have to specify the grouping structure of the data before 
the model command, is that correct?  My model so far is:

XCoutcome<-GroupedData: outcome~ Dairy | Pen.number | cons

lme(AvgPCR~ 1, random = ~ 1 | Dairy /Pen.number / 
pdBlocked(list(pdIdent(~ENV),pdIdent(~Veterinarian))), data=XCoutcome)

Best regards,
Sharif

Description of study design:
My data is comprised of bacteria counts in samples collected by 2 
veterinarians over 3 days from different pens (stalls that house cows) 
nested in different dairies. Vet is crossed with day which is crossed 
with pen (because vet 1 is the same vet 1 who sampled on all days (as is 
vet 2), and day 1 is the same day 1 of sampling in all pens (as is days 
2 and 3)); pens however are nested in dairies, meaning that pen 1 in 
dairy 1 is different that pen 1 in dairy 2, 3 and 4.
My objective was to estimate the similarity in bacterial counts in 
samples collected by 2 vets, in the same pen, in the same dairy (that 
specific intraclass correlation coefficient) and for reasons I can 
explain, I chose to have only a fixed effect intercept (basically we are 
not interested in the effect of any particular vet, pen or dairy or day, 
i.e. we do not wish to estimate the effect of Nov 16 per say)



Sharif Aly,
Graduate group in Epidemiology,
University of California, Davis



From otter at otter-rsch.com  Fri Nov 16 03:51:35 2007
From: otter at otter-rsch.com (dave fournier)
Date: Thu, 15 Nov 2007 18:51:35 -0800
Subject: [R-sig-ME]  non-linear GLMMs
Message-ID: <473D05B7.4080703@otter-rsch.com>

If you can't get satisfaction in R you can do this easily
with AD Model Builder's Random Effects module.


    Cheers,

     Dave


-- 
David A. Fournier
P.O. Box 2040,
Sidney, B.C. V8l 3S3
Canada
Phone/FAX 250-655-3364
http://otter-rsch.com



From danw at sussex.ac.uk  Thu Nov 15 19:20:48 2007
From: danw at sussex.ac.uk (Dan Wright)
Date: Thu, 15 Nov 2007 18:20:48 +0000
Subject: [R-sig-ME] Making own corStruct
In-Reply-To: <40e66e0b0711091302h7ccb532bx94c7312526a774b6@mail.gmail.com>
References: <40e66e0b0711091302h7ccb532bx94c7312526a774b6@mail.gmail.com>
Message-ID: <18FE4881CE18CE34DBB7E293@psycho934.lifesci.susx.ac.uk>


Hi Everyone,

I was wanting to write my own cor structure for the nlme package to put 
into a gls. Does anybody have a good place to look for how to do this? I 
think I remember it being in the handouts from a course with Jose Pinheiro, 
but I lent them to doctoral student who graduated and it does seem to be in 
that section of their book.

Thanks in advance


Dan


Daniel B. Wright
Psychology Department
University of Sussex
BN1 9QH, UK
danw at sussex.ac.uk
http://www.sussex.ac.uk/Users/danw/



From lamprianou at yahoo.com  Thu Nov 22 07:14:39 2007
From: lamprianou at yahoo.com (Iasonas Lamprianou)
Date: Wed, 21 Nov 2007 22:14:39 -0800 (PST)
Subject: [R-sig-ME] two dependent variables
Message-ID: <625010.82400.qm@web54104.mail.re2.yahoo.com>

Dear friends, I have two binomial dependent variables. What sort of analysis can I use? Which package?

Jason
 
Dr. Iasonas Lamprianou
Department of Education
The University of Manchester
Oxford Road, Manchester M13 9PL, UK
Tel. 0044 161 275 3485
iasonas.lamprianou at manchester.ac.uk


----- Original Message ----
From: "r-sig-mixed-models-request at r-project.org" <r-sig-mixed-models-request at r-project.org>
To: r-sig-mixed-models at r-project.org
Sent: Thursday, 15 November, 2007 1:00:01 PM
Subject: R-sig-mixed-models Digest, Vol 11, Issue 8

Send R-sig-mixed-models mailing list submissions to
    r-sig-mixed-models at r-project.org

To subscribe or unsubscribe via the World Wide Web, visit
    https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
or, via email, send a message with subject or body 'help' to
    r-sig-mixed-models-request at r-project.org

You can reach the person managing the list at
    r-sig-mixed-models-owner at r-project.org

When replying, please edit your Subject line so it is more specific
than "Re: Contents of R-sig-mixed-models digest..."


Today's Topics:

  1. Resid() and estimable() functions with lmer (Gustaf Granath)
  2. Re: Resid() and estimable() functions with lmer (Douglas Bates)
  3. non-linear GLMMs (Irene Mantzouni)
  4. mixed model with crossed effects (Sharif S. Aly)


----------------------------------------------------------------------

Message: 1
Date: Wed, 14 Nov 2007 12:58:28 +0100
From: Gustaf Granath <Gustaf.Granath at ebc.uu.se>
Subject: [R-sig-ME] Resid() and estimable() functions with lmer
To: r-sig-mixed-models at r-project.org
Message-ID: <473AE2E4.9030205 at ebc.uu.se>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Hi all,

Two questions:

1. Is there a way to evaluate models from lmer() with a poisson  
distribution? I get the following error message:

library(lme4)
lmer(tot.fruit~infl.treat+def.treat+(1|initial.size),family=poisson)->model
plot(fitted(model),resid(model))
Error: 'resid' is not implemented yet

Are there any other options?

2. Why doesn't the function estimable() in gmodels work with lmer()  
using a poisson distribution? I know that my coefficients are right  
because it works fine if I run the model without poisson distribution.

#CODE (with latest R version and package updates)#
#Without Poisson distribution#

library(lme4)
library(gmodels)
lmer(tot.fruit~infl.treat+def.treat+(1|initial.size))->model1
summary(model1)

Linear mixed-effects model fit by REML
Formula: tot.fruit ~ infl.treat + def.treat + (1 | initial.size)
  AIC  BIC logLik MLdeviance REMLdeviance
  2449 2471  -1218      2447        2437
Random effects:
  Groups      Name        Variance Std.Dev.
  initial.size (Intercept) 71.585  8.4608
  Residual                49.856  7.0608
number of obs: 323, groups: initial.size, 292

Fixed effects:
            Estimate Std. Error t value
(Intercept)  12.8846    1.3028  9.890
infl.treat1  -0.4738    1.1819  -0.401
def.treat2  -3.5522    1.6022  -2.217
def.treat3  -2.1757    1.6461  -1.322
def.treat4  -2.1613    1.7003  -1.271

Correlation of Fixed Effects:
            (Intr) infl.1 df.tr2 df.tr3
infl.treat1 -0.413
def.treat2  -0.616  0.013
def.treat3  -0.641  0.002  0.493
def.treat4  -0.638  0.028  0.469  0.524

#Coefficients#

mean.infl.treat0=c(1,0,1/4,1/4,1/4)
mean.infl.treat1=c(1,1,1/4,1/4,1/4)
mean.def.treat1=c(1,1/2,0,0,0)
mean.def.treat2=c(1,1/2,1,0,0)
mean.def.treat3=c(1,1/2,0,1,0)
mean.def.treat4=c(1,1/2,0,0,1)
means=rbind(mean.infl.treat0,mean.infl.treat1,mean.def.treat1,mean.def.treat2,mean.def.treat3,mean.def.treat4)
estimable(model1,means,sim.lmer=TRUE,n.sim=1000)

                      Estimate Std. Error p value
(1 0 0.25 0.25 0.25) 10.897825  0.8356260      0
(1 1 0.25 0.25 0.25) 10.421633  0.9364839      0
(1 0.5 0 0 0)        12.577408  1.2055979      0
(1 0.5 1 0 0)        9.097063  1.1769760      0
(1 0.5 0 1 0)        10.523485  1.2238635      0
(1 0.5 0 0 1)        10.440959  1.2031459      0

#With Poisson distribution#

lmer(tot.fruit~infl.treat+def.treat+(1|initial.size),family=poisson)->model2
summary(model2)

Generalized linear mixed model fit using Laplace
Formula: tot.fruit ~ infl.treat + def.treat + (1 | initial.size)
  Family: poisson(log link)
  AIC  BIC logLik deviance
  1073 1096 -530.5    1061
Random effects:
  Groups      Name        Variance Std.Dev.
  initial.size (Intercept) 0.84201  0.91761
number of obs: 323, groups: initial.size, 292

Estimated scale (compare to  1 )  1.130529

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)  2.08865    0.10478  19.934  < 2e-16 ***
infl.treat1  0.10615    0.09412  1.128  0.25941
def.treat2  -0.37354    0.11398  -3.277  0.00105 **
def.treat3  -0.18566    0.12679  -1.464  0.14310
def.treat4  -0.10624    0.13451  -0.790  0.42962
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Correlation of Fixed Effects:
            (Intr) infl.1 df.tr2 df.tr3
infl.treat1 -0.418
def.treat2  -0.554  0.016
def.treat3  -0.599 -0.001  0.467
def.treat4  -0.623  0.055  0.460  0.548

estimable(model2,lsmeans,sim.lmer=TRUE,n.sim=1000)
Error in FUN(newX[, i], ...) :
  `param' has no names and does not match number of coefficients of  
model. Unable to construct coefficient vector

#End of CODE#

Any ideas as to why this is? It does say in the help of ?estimable  
that it should work for lmer objects.

Thanks in advance for any help,

Gustaf (PhD student)


Dept of Plant Ecology
Evolutionary Biology Centre (EBC)
Uppsala University



------------------------------

Message: 2
Date: Wed, 14 Nov 2007 07:17:45 -0600
From: "Douglas Bates" <bates at stat.wisc.edu>
Subject: Re: [R-sig-ME] Resid() and estimable() functions with lmer
To: Gustaf.Granath at ebc.uu.se
Cc: r-sig-mixed-models at r-project.org
Message-ID:
    <40e66e0b0711140517l69a470cx53929c094002144d at mail.gmail.com>
Content-Type: text/plain; charset=ISO-8859-1

On Nov 14, 2007 5:58 AM, Gustaf Granath <Gustaf.Granath at ebc.uu.se> wrote:
> Hi all,

> Two questions:

> 1. Is there a way to evaluate models from lmer() with a poisson
> distribution? I get the following error message:

> library(lme4)
> lmer(tot.fruit~infl.treat+def.treat+(1|initial.size),family=poisson)->model
> plot(fitted(model),resid(model))
> Error: 'resid' is not implemented yet

The model can be fit, it is the resid function that is not yet
implemented.  The reason it is not yet implemented is because there
are many different kinds of residuals for generalized linear mixed
models.  It requires considerably more design and coding than does
obtaining the residuals for a linear mixed model, and even those are
difficult to define for the general model (Do you incorporate the
random effects or not?  If you do, what do you use in the case of
multiple grouping factors?)

Right now I am thinking about other issues in the form of the model so
I can incorporate both generalized linear mixed models and nonlinear
mixed models.  Residuals for generalized linear mixed models are
further down the priority list.

> Are there any other options?
>
> 2. Why doesn't the function estimable() in gmodels work with lmer()
> using a poisson distribution? I know that my coefficients are right
> because it works fine if I run the model without poisson distribution.

I don't know what the estimable function expects the structure of an
lmer or glmer model to be but it would not surprise me if it did not
coincide with the current version.    The structure is continuously
evolving, not because I want to make things difficult for others but
because my understanding of the model and the computational methods
evolves.  On something as complicated as a generalized nonlinear mixed
model with crossed grouping factors it is not surprising that one
doesn't get the design down on the first try.  At least this one
doesn't.  I hope the structure of the classes and the algorithms will
stabilize soon (say, by the end of the year).

Even then it will be difficult to implement something like estimable
without doing finite difference calculations for generalized linear
mixed models.  A matrix that determines the marginal variance
covariance for the fixed-effects estimates (well, not quite but I
don't want to get into the technicalities) is calculated for linear
mixed models but not for generalized linear mixed models.

> #CODE (with latest R version and package updates)#
> #Without Poisson distribution#
>
> library(lme4)
> library(gmodels)
> lmer(tot.fruit~infl.treat+def.treat+(1|initial.size))->model1
> summary(model1)
>
> Linear mixed-effects model fit by REML
> Formula: tot.fruit ~ infl.treat + def.treat + (1 | initial.size)
>    AIC  BIC logLik MLdeviance REMLdeviance
>  2449 2471  -1218      2447        2437
> Random effects:
>  Groups      Name        Variance Std.Dev.
>  initial.size (Intercept) 71.585  8.4608
>  Residual                49.856  7.0608
> number of obs: 323, groups: initial.size, 292
>
> Fixed effects:
>              Estimate Std. Error t value
> (Intercept)  12.8846    1.3028  9.890
> infl.treat1  -0.4738    1.1819  -0.401
> def.treat2  -3.5522    1.6022  -2.217
> def.treat3  -2.1757    1.6461  -1.322
> def.treat4  -2.1613    1.7003  -1.271
>
> Correlation of Fixed Effects:
>              (Intr) infl.1 df.tr2 df.tr3
> infl.treat1 -0.413
> def.treat2  -0.616  0.013
> def.treat3  -0.641  0.002  0.493
> def.treat4  -0.638  0.028  0.469  0.524
>
> #Coefficients#
>
> mean.infl.treat0=c(1,0,1/4,1/4,1/4)
> mean.infl.treat1=c(1,1,1/4,1/4,1/4)
> mean.def.treat1=c(1,1/2,0,0,0)
> mean.def.treat2=c(1,1/2,1,0,0)
> mean.def.treat3=c(1,1/2,0,1,0)
> mean.def.treat4=c(1,1/2,0,0,1)
> means=rbind(mean.infl.treat0,mean.infl.treat1,mean.def.treat1,mean.def.treat2,mean.def.treat3,mean.def.treat4)
> estimable(model1,means,sim.lmer=TRUE,n.sim=1000)
>
>                        Estimate Std. Error p value
> (1 0 0.25 0.25 0.25) 10.897825  0.8356260      0
> (1 1 0.25 0.25 0.25) 10.421633  0.9364839      0
> (1 0.5 0 0 0)        12.577408  1.2055979      0
> (1 0.5 1 0 0)        9.097063  1.1769760      0
> (1 0.5 0 1 0)        10.523485  1.2238635      0
> (1 0.5 0 0 1)        10.440959  1.2031459      0
>
> #With Poisson distribution#
>
> lmer(tot.fruit~infl.treat+def.treat+(1|initial.size),family=poisson)->model2
> summary(model2)
>
> Generalized linear mixed model fit using Laplace
> Formula: tot.fruit ~ infl.treat + def.treat + (1 | initial.size)
>  Family: poisson(log link)
>    AIC  BIC logLik deviance
>  1073 1096 -530.5    1061
> Random effects:
>  Groups      Name        Variance Std.Dev.
>  initial.size (Intercept) 0.84201  0.91761
> number of obs: 323, groups: initial.size, 292
>
> Estimated scale (compare to  1 )  1.130529
>
> Fixed effects:
>              Estimate Std. Error z value Pr(>|z|)
> (Intercept)  2.08865    0.10478  19.934  < 2e-16 ***
> infl.treat1  0.10615    0.09412  1.128  0.25941
> def.treat2  -0.37354    0.11398  -3.277  0.00105 **
> def.treat3  -0.18566    0.12679  -1.464  0.14310
> def.treat4  -0.10624    0.13451  -0.790  0.42962
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> Correlation of Fixed Effects:
>              (Intr) infl.1 df.tr2 df.tr3
> infl.treat1 -0.418
> def.treat2  -0.554  0.016
> def.treat3  -0.599 -0.001  0.467
> def.treat4  -0.623  0.055  0.460  0.548
>
> estimable(model2,lsmeans,sim.lmer=TRUE,n.sim=1000)
> Error in FUN(newX[, i], ...) :
>    `param' has no names and does not match number of coefficients of
> model. Unable to construct coefficient vector
>
> #End of CODE#
>
> Any ideas as to why this is? It does say in the help of ?estimable
> that it should work for lmer objects.
>
> Thanks in advance for any help,
>
> Gustaf (PhD student)
>
>
> Dept of Plant Ecology
> Evolutionary Biology Centre (EBC)
> Uppsala University
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



------------------------------

Message: 3
Date: Wed, 14 Nov 2007 20:23:52 +0100
From: "Irene Mantzouni" <ima at difres.dk>
Subject: [R-sig-ME] non-linear GLMMs
To: <R-SIG-Mixed-Models at r-project.org>
Message-ID:
    <68E7981938EAF54F987AD3848A0A6416E5838B at ka-mail01.dfu.local>
Content-Type: text/plain;    charset="ISO-8859-7"

Is it (or will it become...) possible to fit a mixed non-linear SSmicmen model assuming log-normal error?
I have tried to use a log transformed micmen model but it does not seem so powerful. 
Any other ideas about possible approaches are more than welcome!

Irene



------------------------------

Message: 4
Date: Wed, 14 Nov 2007 23:02:40 -0800
From: "Sharif S. Aly" <saly at ucdavis.edu>
Subject: [R-sig-ME] mixed model with crossed effects
To: r-sig-mixed-models at r-project.org
Message-ID: <473BEF10.3070105 at ucdavis.edu>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Dear list,
If you please help me with the R command for a 4 level random effects 
model I would be grateful.  I chose to put the details of my study at 
the end of the email as some may prefer not to read it.

My outcome is continuous (normally distributed). All levels are factor 
levels. The lowest level (veterinarian) is crossed with level 2 (day of 
sampling) which is crossed with level 3 (sampled pen or location on 
farm) which is nested in level 4 (dairy).

I understand I have to specify the grouping structure of the data before 
the model command, is that correct?  My model so far is:

XCoutcome<-GroupedData: outcome~ Dairy | Pen.number | cons

lme(AvgPCR~ 1, random = ~ 1 | Dairy /Pen.number / 
pdBlocked(list(pdIdent(~ENV),pdIdent(~Veterinarian))), data=XCoutcome)

Best regards,
Sharif

Description of study design:
My data is comprised of bacteria counts in samples collected by 2 
veterinarians over 3 days from different pens (stalls that house cows) 
nested in different dairies. Vet is crossed with day which is crossed 
with pen (because vet 1 is the same vet 1 who sampled on all days (as is 
vet 2), and day 1 is the same day 1 of sampling in all pens (as is days 
2 and 3)); pens however are nested in dairies, meaning that pen 1 in 
dairy 1 is different that pen 1 in dairy 2, 3 and 4.
My objective was to estimate the similarity in bacterial counts in 
samples collected by 2 vets, in the same pen, in the same dairy (that 
specific intraclass correlation coefficient) and for reasons I can 
explain, I chose to have only a fixed effect intercept (basically we are 
not interested in the effect of any particular vet, pen or dairy or day, 
i.e. we do not wish to estimate the effect of Nov 16 per say)



Sharif Aly,
Graduate group in Epidemiology,
University of California, Davis



------------------------------

_______________________________________________
R-sig-mixed-models mailing list
R-sig-mixed-models at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


End of R-sig-mixed-models Digest, Vol 11, Issue 8
*************************************************


      ___________________________________________________________
Yahoo! Answers - Got a question? Someone out there knows the answer. Try it
now.
http://uk.answers.yahoo.com/



From joseph.powell at bbsrc.ac.uk  Thu Nov 22 16:26:40 2007
From: joseph.powell at bbsrc.ac.uk (joseph powell (RI))
Date: Thu, 22 Nov 2007 15:26:40 -0000
Subject: [R-sig-ME] Weighting variance on the residual in lme
Message-ID: <1F16910BB8546C4DA5526FABB0C98D097F3875@ebre2ksrv1.ebrc.bbsrc.ac.uk>

Dear all,

I wanted to check if I've fitted a model as I think I have, and if not
ask for suggestions for corrections. I've fitted the following model
within a function;

lme.out <- summary(lme(trait ~ genotype, random=~1|sire,
weights=~1/no.offspring, na.action="na.omit")) 

I wanted to include a weighting of 1/no.offspring for the variance of
the residual. Is this what I have fitted? If not suggestions would be
much appreciated.

Many thanks,

Joseph



From ima at difres.dk  Thu Nov 22 18:30:29 2007
From: ima at difres.dk (Irene Mantzouni)
Date: Thu, 22 Nov 2007 18:30:29 +0100
Subject: [R-sig-ME] hierarchical unbalanced design
Message-ID: <68E7981938EAF54F987AD3848A0A6416013BB0AF@ka-mail01.dfu.local>

Hi all!
 
I have a mixed model with timeseries of y and x for populations:
lme(y~x,random=1|Population)
I am wondering if I can make it hierarchical by adding a random effect for year:
lme(y~x,random=1|Population/Year)
However the design is unbalanced: the time-series have different lengths for each population although the time-periods are overlapping in general. 
 
Would it be meaningful to include the Year effect this way? 
 
Thank you!
 
Irene



From lamprianou at yahoo.com  Mon Nov 26 18:50:22 2007
From: lamprianou at yahoo.com (Iasonas Lamprianou)
Date: Mon, 26 Nov 2007 09:50:22 -0800 (PST)
Subject: [R-sig-ME] corelated errors
Message-ID: <8658.52888.qm@web54104.mail.re2.yahoo.com>

Dear friends, may we use lmer to estimate models where residuals can be correlated (no conditional independence
          assumption)

thanks

 
Dr. Iasonas Lamprianou
Department of Education
The University of Manchester
Oxford Road, Manchester M13 9PL, UK
Tel. 0044 161 275 3485
iasonas.lamprianou at manchester.ac.uk


----- Original Message ----
From: "r-sig-mixed-models-request at r-project.org" <r-sig-mixed-models-request at r-project.org>
To: r-sig-mixed-models at r-project.org
Sent: Saturday, 10 November, 2007 1:00:01 PM
Subject: R-sig-mixed-models Digest, Vol 11, Issue 7

Send R-sig-mixed-models mailing list submissions to
    r-sig-mixed-models at r-project.org

To subscribe or unsubscribe via the World Wide Web, visit
    https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
or, via email, send a message with subject or body 'help' to
    r-sig-mixed-models-request at r-project.org

You can reach the person managing the list at
    r-sig-mixed-models-owner at r-project.org

When replying, please edit your Subject line so it is more specific
than "Re: Contents of R-sig-mixed-models digest..."


Today's Topics:

  1. segmented regression mixed model? (Irene Mantzouni)
  2. Re: Nested Mixed Models in lme4 (Marco Chiarandini)
  3. lme4 is now on R-forge (Douglas Bates)


----------------------------------------------------------------------

Message: 1
Date: Fri, 9 Nov 2007 16:59:27 +0100
From: "Irene Mantzouni" <ima at difres.dk>
Subject: [R-sig-ME] segmented regression mixed model?
To: <r-sig-mixed-models at r-project.org>
Message-ID:
    <68E7981938EAF54F987AD3848A0A6416E5837E at ka-mail01.dfu.local>
Content-Type: text/plain;    charset="ISO-8859-7"

Hi all!

Is it possible to use a segmented regression model as the functional form of a linear (or maybe non-linear?) mixed model?

Cheers,
Irene



------------------------------

Message: 2
Date: Fri, 09 Nov 2007 17:41:34 +0100
From: Marco Chiarandini <marco at imada.sdu.dk>
Subject: Re: [R-sig-ME] Nested Mixed Models in lme4
To: Douglas Bates <bates at stat.wisc.edu>
Cc: r-sig-mixed-models at r-project.org
Message-ID: <47348DBE.1060407 at imada.sdu.dk>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Dear Prof. Bates,


>> I am trying to use the function lmer from lme4 to
>> analyse the following nested factorial design.
> 
>> I have three treatment factors (neighborhood,
>> initial, k);
>> I have three group factors crossing (size, dens,
>> inst).
> 
> Did you mean to write (size, dens, type) there?
> 
> Also, by "factor" do you mean that you regard all of these variables
> as categorical?  If so, you should check the form of the size variable
> in the data frame.  It is being stored as a numeric variable, not as a
> factor.  If you want to interpret this  variable as a categorical
> factor you should convert it to a factor or, as seems likely in this
> case, an ordered factor.  (See ?factor and ?ordered)


yes, thank you a lot! All your corrections are 
appropriate! inst should have been type and all 
variables should have been categorical. My mistake.
Also: as you correctly pointed out, the data are 
from a computer experiment and perfectly balanced, 
and by group factors I meant blocking factors.

Your very clear explanation solved my concerns 
about the nesting! Thanks!

I've also redone the comparison with SAS and now 
results correspond.
The reason was mainly that I needed a quite 
different formula:

lmer(err~initial*neighborhood + initial*k + 
initial*type + initial*size + initial*dens + 
neighborhood*k + neighborhood*type + 
neighborhood*size + neighborhood*dens + k*type + 
k*size + k*dens + type*size + type*dens + 
size*dens + initial*neighborhood*k + 
(1|inst),data=Case3)

True also that we were using lsmeans in SAS that 
you discourage.

To me it would remain only to understand how I 
could obtain the results in a cell means format 
like those in SAS. But this seems to be a problem 
also in lm and hence I must probably study better 
how things work to find the way. Trying something 
of the kind:

fmm1 <- 
lmer(err~-1+ordered(size)+dens+type+(k+initial+neighborhood)^3+(1|inst),data=Case3)

does not seem to help much.

I left all the analysis I did, code + results, 
(SAS and R) at:

http://www.imada.sdu.dk/~marco/Mixed/


Thank you a lot very much for the help!

Best regards,

Marco



-- 
Marco Chiarandini 
http://www.imada.sdu.dk/~marco
Department of Mathematics          Email: 
marco at imada.sdu.dk
and Computer Science,              Phone: +45 6550 4031
University of Southern Denmark        Fax: +45 
6593 2691



------------------------------

Message: 3
Date: Fri, 9 Nov 2007 15:02:52 -0600
From: "Douglas Bates" <bates at stat.wisc.edu>
Subject: [R-sig-ME] lme4 is now on R-forge
To: R-SIG-Mixed-Models at r-project.org
Message-ID:
    <40e66e0b0711091302h7ccb532bx94c7312526a774b6 at mail.gmail.com>
Content-Type: text/plain; charset=ISO-8859-1

Users of the current version of the lme4 package have reported several
problems and, for some time, I have been unresponsive about such reports
or I have made reference to the development version of the package.   Let
me emphasize that I am grateful for the reports and, indeed, have fixed
several of these problems in the development version of the package.
However, I have held off releasing the development version because of
one small problem - it doesn't fit generalized linear mixed models correctly.

I have had to go back and reformulate the model from scratch so that I
can understand it and design the code.  As anyone who has developed
and maintained a large project can attest, the only way to build
trustworthy code (and to maintain your sanity) is to modularize the
code.  It goes without saying that before you can decide how to
modularize the code you must be able to decompose the steps in the
computation.  The development version is designed to handle linear
mixed models, generalized linear mixed models, nonlinear mixed models
and generalized nonlinear mixed models with nested or crossed or
partially crossed random factors.  It has taken me a long time to
decide how all those pieces fit together.  Only in the last couple of
weeks have I have managed to convince myself that I know how it all
fits together.  The task of convincing others remains, and is
decidedly non-trivial, but I feel that I can decompose the
computational steps now.

It will take a while to move from the equations in my lab notebook to
released code and, during that process, I will probably need to
reformulate the slots in the S4 classes.  My method of getting to the
final design of the data structures and algorithms is to keep doing it
wrong 'til I do it right.

So that others have easy access to the development version of the
package I have moved the repository for the development version of
the package to http://R-forge.R-project.org/packages/lme4

Martin and I had planned to do this move in a way that would preserve
the history of the changes from the current repository
but that is not easy to do because of the way that the Matrix and lme4
packages were merged then un-merged.  Thus I have made a clean break
and installed the development version (the one known as gappy-lmer) on
R-forge.  You can access it at the URL given above or as
http://lme4.r-forge.r-project.org/, at the expense of one additional click.

Starting tomorrow you should also be able to install the development
version of the package with

install.packages("lme4", repos = "http://r-forge.r-project.org")

Please be aware that the class representations can change so when
using the development version you should not count on being able to re-use
a fitted model after installing a new version.  You should retain the original
data so you can refit the model if necessary.



------------------------------

_______________________________________________
R-sig-mixed-models mailing list
R-sig-mixed-models at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


End of R-sig-mixed-models Digest, Vol 11, Issue 7
*************************************************


      ___________________________________________________________ 
Want ideas for reducing your carbon footprint? Visit Yahoo! For Good  http://uk.promotions.yahoo.com/forgood/environment.html



From A.Robinson at ms.unimelb.edu.au  Mon Nov 26 19:11:30 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Tue, 27 Nov 2007 05:11:30 +1100
Subject: [R-sig-ME] corelated errors
In-Reply-To: <8658.52888.qm@web54104.mail.re2.yahoo.com>
References: <8658.52888.qm@web54104.mail.re2.yahoo.com>
Message-ID: <20071126181130.GY51962@ms.unimelb.edu.au>

Hi Iasonas,

it depends on the correlation structure.  If you can produce the
structure using crossed or nested random effects, then, yes.  If you
are referring to the kinds of correlations that are produced by the
correlation argument in lme(), then, no, not yet.

Cheers

Andrew

On Mon, Nov 26, 2007 at 09:50:22AM -0800, Iasonas Lamprianou wrote:
> Dear friends, may we use lmer to estimate models where residuals can be correlated (no conditional independence
>           assumption)
> 
> thanks
> 
>  
> Dr. Iasonas Lamprianou
> Department of Education
> The University of Manchester
> Oxford Road, Manchester M13 9PL, UK
> Tel. 0044 161 275 3485
> iasonas.lamprianou at manchester.ac.uk
> 
> 
> ----- Original Message ----
> From: "r-sig-mixed-models-request at r-project.org" <r-sig-mixed-models-request at r-project.org>
> To: r-sig-mixed-models at r-project.org
> Sent: Saturday, 10 November, 2007 1:00:01 PM
> Subject: R-sig-mixed-models Digest, Vol 11, Issue 7
> 
> Send R-sig-mixed-models mailing list submissions to
>     r-sig-mixed-models at r-project.org
> 
> To subscribe or unsubscribe via the World Wide Web, visit
>     https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> or, via email, send a message with subject or body 'help' to
>     r-sig-mixed-models-request at r-project.org
> 
> You can reach the person managing the list at
>     r-sig-mixed-models-owner at r-project.org
> 
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of R-sig-mixed-models digest..."
> 
> 
> Today's Topics:
> 
>   1. segmented regression mixed model? (Irene Mantzouni)
>   2. Re: Nested Mixed Models in lme4 (Marco Chiarandini)
>   3. lme4 is now on R-forge (Douglas Bates)
> 
> 
> ----------------------------------------------------------------------
> 
> Message: 1
> Date: Fri, 9 Nov 2007 16:59:27 +0100
> From: "Irene Mantzouni" <ima at difres.dk>
> Subject: [R-sig-ME] segmented regression mixed model?
> To: <r-sig-mixed-models at r-project.org>
> Message-ID:
>     <68E7981938EAF54F987AD3848A0A6416E5837E at ka-mail01.dfu.local>
> Content-Type: text/plain;    charset="ISO-8859-7"
> 
> Hi all!
> 
> Is it possible to use a segmented regression model as the functional form of a linear (or maybe non-linear?) mixed model?
> 
> Cheers,
> Irene
> 
> 
> 
> ------------------------------
> 
> Message: 2
> Date: Fri, 09 Nov 2007 17:41:34 +0100
> From: Marco Chiarandini <marco at imada.sdu.dk>
> Subject: Re: [R-sig-ME] Nested Mixed Models in lme4
> To: Douglas Bates <bates at stat.wisc.edu>
> Cc: r-sig-mixed-models at r-project.org
> Message-ID: <47348DBE.1060407 at imada.sdu.dk>
> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
> 
> Dear Prof. Bates,
> 
> 
> >> I am trying to use the function lmer from lme4 to
> >> analyse the following nested factorial design.
> > 
> >> I have three treatment factors (neighborhood,
> >> initial, k);
> >> I have three group factors crossing (size, dens,
> >> inst).
> > 
> > Did you mean to write (size, dens, type) there?
> > 
> > Also, by "factor" do you mean that you regard all of these variables
> > as categorical?  If so, you should check the form of the size variable
> > in the data frame.  It is being stored as a numeric variable, not as a
> > factor.  If you want to interpret this  variable as a categorical
> > factor you should convert it to a factor or, as seems likely in this
> > case, an ordered factor.  (See ?factor and ?ordered)
> 
> 
> yes, thank you a lot! All your corrections are 
> appropriate! inst should have been type and all 
> variables should have been categorical. My mistake.
> Also: as you correctly pointed out, the data are 
> from a computer experiment and perfectly balanced, 
> and by group factors I meant blocking factors.
> 
> Your very clear explanation solved my concerns 
> about the nesting! Thanks!
> 
> I've also redone the comparison with SAS and now 
> results correspond.
> The reason was mainly that I needed a quite 
> different formula:
> 
> lmer(err~initial*neighborhood + initial*k + 
> initial*type + initial*size + initial*dens + 
> neighborhood*k + neighborhood*type + 
> neighborhood*size + neighborhood*dens + k*type + 
> k*size + k*dens + type*size + type*dens + 
> size*dens + initial*neighborhood*k + 
> (1|inst),data=Case3)
> 
> True also that we were using lsmeans in SAS that 
> you discourage.
> 
> To me it would remain only to understand how I 
> could obtain the results in a cell means format 
> like those in SAS. But this seems to be a problem 
> also in lm and hence I must probably study better 
> how things work to find the way. Trying something 
> of the kind:
> 
> fmm1 <- 
> lmer(err~-1+ordered(size)+dens+type+(k+initial+neighborhood)^3+(1|inst),data=Case3)
> 
> does not seem to help much.
> 
> I left all the analysis I did, code + results, 
> (SAS and R) at:
> 
> http://www.imada.sdu.dk/~marco/Mixed/
> 
> 
> Thank you a lot very much for the help!
> 
> Best regards,
> 
> Marco
> 
> 
> 
> -- 
> Marco Chiarandini 
> http://www.imada.sdu.dk/~marco
> Department of Mathematics          Email: 
> marco at imada.sdu.dk
> and Computer Science,              Phone: +45 6550 4031
> University of Southern Denmark        Fax: +45 
> 6593 2691
> 
> 
> 
> ------------------------------
> 
> Message: 3
> Date: Fri, 9 Nov 2007 15:02:52 -0600
> From: "Douglas Bates" <bates at stat.wisc.edu>
> Subject: [R-sig-ME] lme4 is now on R-forge
> To: R-SIG-Mixed-Models at r-project.org
> Message-ID:
>     <40e66e0b0711091302h7ccb532bx94c7312526a774b6 at mail.gmail.com>
> Content-Type: text/plain; charset=ISO-8859-1
> 
> Users of the current version of the lme4 package have reported several
> problems and, for some time, I have been unresponsive about such reports
> or I have made reference to the development version of the package.   Let
> me emphasize that I am grateful for the reports and, indeed, have fixed
> several of these problems in the development version of the package.
> However, I have held off releasing the development version because of
> one small problem - it doesn't fit generalized linear mixed models correctly.
> 
> I have had to go back and reformulate the model from scratch so that I
> can understand it and design the code.  As anyone who has developed
> and maintained a large project can attest, the only way to build
> trustworthy code (and to maintain your sanity) is to modularize the
> code.  It goes without saying that before you can decide how to
> modularize the code you must be able to decompose the steps in the
> computation.  The development version is designed to handle linear
> mixed models, generalized linear mixed models, nonlinear mixed models
> and generalized nonlinear mixed models with nested or crossed or
> partially crossed random factors.  It has taken me a long time to
> decide how all those pieces fit together.  Only in the last couple of
> weeks have I have managed to convince myself that I know how it all
> fits together.  The task of convincing others remains, and is
> decidedly non-trivial, but I feel that I can decompose the
> computational steps now.
> 
> It will take a while to move from the equations in my lab notebook to
> released code and, during that process, I will probably need to
> reformulate the slots in the S4 classes.  My method of getting to the
> final design of the data structures and algorithms is to keep doing it
> wrong 'til I do it right.
> 
> So that others have easy access to the development version of the
> package I have moved the repository for the development version of
> the package to http://R-forge.R-project.org/packages/lme4
> 
> Martin and I had planned to do this move in a way that would preserve
> the history of the changes from the current repository
> but that is not easy to do because of the way that the Matrix and lme4
> packages were merged then un-merged.  Thus I have made a clean break
> and installed the development version (the one known as gappy-lmer) on
> R-forge.  You can access it at the URL given above or as
> http://lme4.r-forge.r-project.org/, at the expense of one additional click.
> 
> Starting tomorrow you should also be able to install the development
> version of the package with
> 
> install.packages("lme4", repos = "http://r-forge.r-project.org")
> 
> Please be aware that the class representations can change so when
> using the development version you should not count on being able to re-use
> a fitted model after installing a new version.  You should retain the original
> data so you can refit the model if necessary.
> 
> 
> 
> ------------------------------
> 
> _______________________________________________
> R-sig-mixed-models mailing list
> R-sig-mixed-models at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> End of R-sig-mixed-models Digest, Vol 11, Issue 7
> *************************************************
> 
> 
>       ___________________________________________________________ 
> Want ideas for reducing your carbon footprint? Visit Yahoo! For Good  http://uk.promotions.yahoo.com/forgood/environment.html
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/



From divaker at gmail.com  Tue Nov 27 03:25:39 2007
From: divaker at gmail.com (Divaker)
Date: Tue, 27 Nov 2007 07:55:39 +0530
Subject: [R-sig-ME] lme - nested - One fixed effect and another within that
	as random effect
Message-ID: <000601c8309c$d13c98e0$0201a8c0@homec63425920b>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20071127/12197b1a/attachment.pl>

From HDoran at air.org  Tue Nov 27 15:43:01 2007
From: HDoran at air.org (Doran, Harold)
Date: Tue, 27 Nov 2007 09:43:01 -0500
Subject: [R-sig-ME] lme - nested - One fixed effect and another within
	thatas random effect
In-Reply-To: <000601c8309c$d13c98e0$0201a8c0@homec63425920b>
Message-ID: <2323A6D37908A847A7C32F1E3662C80E012DE89B@dc1ex01.air.org>

What do you mean by not working? Your lme and lmer code seem to be
equivalent. 

> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org 
> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Divaker
> Sent: Monday, November 26, 2007 9:26 PM
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] lme - nested - One fixed effect and 
> another within thatas random effect
> 
> Dear R mixed model Users and Dr. Bates,
> 
> I am trying to work out a problem given in Nested design - 
> Design of Experiments by  Montgomery -  p.561 using lme It is 
> a mixed model with Supplier as fixed effect and batches 
> within the supplier as random effects.
> 
> When I tried my hands on lmer instead of lme, I get what is 
> required  as below.
>  
> > proclme=lmer(Purity~Supplier+(1|Supplier:Batch),process)
> > print(summary(proclme))
> Linear mixed-effects model fit by REML
> Formula: Purity ~ Supplier + (1 | Supplier:Batch) 
>    Data: process 
>    AIC   BIC logLik MLdeviance REMLdeviance
>  150.8 157.2 -71.42      146.9        142.8
> Random effects:
>  Groups         Name        Variance Std.Dev.
>  Supplier:Batch (Intercept) 1.7162   1.3100   Here we have 
> the variance of random effect exactly as in the book
>  Residual                   2.6368   1.6238  
> number of obs: 36, groups: Supplier:Batch, 12
>  
> Fixed effects:
>             Estimate Std. Error t value
> (Intercept)  -0.4167     0.8055 -0.5173
> SupplierT2    0.7500     1.1391  0.6584
> SupplierT3    1.5833     1.1391  1.3900
>  
> Correlation of Fixed Effects:
>            (Intr) SpplT2
> SupplierT2 -0.707       
> SupplierT3 -0.707  0.500
> 
> But using lme, it is not working. Is there any way out.lme is 
> not accepting the format random=~1|Supplier : Batch I am more 
> comfortable using lme since the supporting docs are exaustive 
> and we have access to many support functions for lme
> 
> This code is not appropriate and also not working
> library(nlme)
> proclme=lme(Purity~Supplier,random = ~1|Supplier/Batch,process)
> summary(proclme)
> VarCorr(proclme)
> 
> 
> Divaker
> 
> Dr. C. Divaker Durairaj, ME, Ph.D
> Professor, Farm Machinery
> Agricultural Machinery Research Centre
> Tamil Nadu Agricultural University
> Coimbatore 641003, India
> Ph: 91-422-6611204
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 



From bates at stat.wisc.edu  Tue Nov 27 16:00:59 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 27 Nov 2007 09:00:59 -0600
Subject: [R-sig-ME] lme - nested - One fixed effect and another within
	thatas random effect
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E012DE89B@dc1ex01.air.org>
References: <000601c8309c$d13c98e0$0201a8c0@homec63425920b>
	<2323A6D37908A847A7C32F1E3662C80E012DE89B@dc1ex01.air.org>
Message-ID: <40e66e0b0711270700l7b0f1719kaacc5b47b212a41a@mail.gmail.com>

On Nov 27, 2007 8:43 AM, Doran, Harold <HDoran at air.org> wrote:
> What do you mean by not working? Your lme and lmer code seem to be
> equivalent.

Not really.  Supplier:Batch is the batch grouping factor without the
implicit nesting.  That is, it has a different level for each
different batch (which seems to me to be the only sensible way to
define such a factor but, in the old days, people seemed to think it
was important to specify nesting implicitly).  The expression
Supplier/Batch implies two random effects, one for Supplier and one
for Supplier:Batch.

The simple way out is to define

process$realBatch <- with(process, Supplier:Batch)

and fit the lme model with random = ~ 1|realBatch

> > -----Original Message-----
> > From: r-sig-mixed-models-bounces at r-project.org
> > [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Divaker
> > Sent: Monday, November 26, 2007 9:26 PM
> > To: r-sig-mixed-models at r-project.org
> > Subject: [R-sig-ME] lme - nested - One fixed effect and
> > another within thatas random effect
> >
> > Dear R mixed model Users and Dr. Bates,
> >
> > I am trying to work out a problem given in Nested design -
> > Design of Experiments by  Montgomery -  p.561 using lme It is
> > a mixed model with Supplier as fixed effect and batches
> > within the supplier as random effects.
> >
> > When I tried my hands on lmer instead of lme, I get what is
> > required  as below.
> >
> > > proclme=lmer(Purity~Supplier+(1|Supplier:Batch),process)
> > > print(summary(proclme))
> > Linear mixed-effects model fit by REML
> > Formula: Purity ~ Supplier + (1 | Supplier:Batch)
> >    Data: process
> >    AIC   BIC logLik MLdeviance REMLdeviance
> >  150.8 157.2 -71.42      146.9        142.8
> > Random effects:
> >  Groups         Name        Variance Std.Dev.
> >  Supplier:Batch (Intercept) 1.7162   1.3100   Here we have
> > the variance of random effect exactly as in the book
> >  Residual                   2.6368   1.6238
> > number of obs: 36, groups: Supplier:Batch, 12
> >
> > Fixed effects:
> >             Estimate Std. Error t value
> > (Intercept)  -0.4167     0.8055 -0.5173
> > SupplierT2    0.7500     1.1391  0.6584
> > SupplierT3    1.5833     1.1391  1.3900
> >
> > Correlation of Fixed Effects:
> >            (Intr) SpplT2
> > SupplierT2 -0.707
> > SupplierT3 -0.707  0.500
> >
> > But using lme, it is not working. Is there any way out.lme is
> > not accepting the format random=~1|Supplier : Batch I am more
> > comfortable using lme since the supporting docs are exaustive
> > and we have access to many support functions for lme
> >
> > This code is not appropriate and also not working
> > library(nlme)
> > proclme=lme(Purity~Supplier,random = ~1|Supplier/Batch,process)
> > summary(proclme)
> > VarCorr(proclme)
> >
> >
> > Divaker
> >
> > Dr. C. Divaker Durairaj, ME, Ph.D
> > Professor, Farm Machinery
> > Agricultural Machinery Research Centre
> > Tamil Nadu Agricultural University
> > Coimbatore 641003, India
> > Ph: 91-422-6611204
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From robert.h.creecy at census.gov  Fri Nov 30 15:20:36 2007
From: robert.h.creecy at census.gov (robert.h.creecy at census.gov)
Date: Fri, 30 Nov 2007 09:20:36 -0500
Subject: [R-sig-ME] Simple one level model with known variances
Message-ID: <OFD5935A4B.3D5126EA-ON852573A2.005C0CA0-852573A3.004ECA15@census.gov>


All,

I have a very simple set of county data, with one dependent variable (y)
and three covariates (x),
one observation for each county. I have a lot of these type of problems to
do.

y is actually a summary statistic that comes from a survey within the
county,
and a variable (psi2) of the estimated sampling variance of y for each
county is also provided.

I am trying to figure out how to specify the known variances psi2_i within
the model

y_i = X_i beta + u_i + e_i    ,   b_i ~ N (0, psi2_i)  ,     e_i ~ N(0,
sigma^2)

u_i is a random effect for county i that comes from the uncertainty in the
survey estimate for that county.

I have looked at Chap. 5 of Pinheiro and Bates trying out how to use the
variance functions
as weights in lme or gls, but without success.  Also no luck with
RSiteSearch.

Any suggestions?

I have included below a sample problem for which I think I have the correct
maximum likelihood
estimates for beta and sigma^2 and random effects u., (in res$beta,
res$sig2e and res$u below)
 which I got with a quick hack, but which I would like to reproduce with
some standard R method.

Thanks

Rob

xy <-
structure(c(0.02810951054828, -0.0059751447363, -0.00259486765798,
-0.02408532375275, 0.01233681858059, 0.00335238957434, -0.00437531552244,
0.0085573052387, -0.00295186914992, -5.848326315e-05, 0.05808473441535,
-0.00624215347984, -0.02844949344284, 0.01463634952832, -0.01537450106168,
0.00789132330785, -0.01758284279198, 0.02185240478034, 0.02861913422167,
0.00269505879316, 0.00645232139019, -0.01078894167733, -0.0549443656218,
0.05394083115548, 0.00336404780951, 0.01039785126456, 0.00651310752339,
0.00545016685343, 0.01795693134289, 0.00254833115767, 0.00629453997195,
0.0057251039691, 0.00669739748211, 0.01247983496454, 0.0004798937386,
0.00358346930317, 0.01047475275143, 0.00226774833578, 0.04163824745781,
0.0300726114684, 0.00537941752895, 0.0037613012053, 0.00372696087132,
0.01272218132145, 0.00444575648527, 0.01355607900327, 0.00495450061539,
0.00665815715587, 0.70018484288354, 0.77119868616205, 0.69679575751332,
0.83832303711917, 0.71465870943677, 0.81584990584669, 0.70391345385899,
0.83198374691626, 0.64693819827464, 0.77172704235109, 0.55166943102329,
0.74741880136058, 0.6975613116561, 0.67302720669703, 0.67255775259468,
0.59988447097333, 0.78113776756213, 0.68642481825379, 0.59974397269042,
0.68629725672171, 0.67463529808584, 0.7008583976577, 0.25786272376619,
0.59450936196294, 0.00490892500726, 0.01068001582643, 0.02329564566335,
0.02006917927665, 0.0006186210563, 0.00012864166829, 0.01773028685484,
0.00278397080297, 0.00204658742582, 0.01360133771674, 0.01381488736597,
0.01851231408294, 0.00535066427928, 0.02991886041973, 0.01497999546635,
-0.00826793443986, 0.00515847523844, 0.01793658872856, 0.00964263205073,
0.00625921353626, 0.00390790240252, 0.00237179752867, 0.25453351836804,
0.00663340112391, 0.00010209967539, 8.4110404969007e-06, 4.8349423626399
e-06,
0.00010760136729, 0.00021301765714, 2.203499903e-05, 9.413510752e-05,
4.250192525e-05, 0.00027227682566, 2.106425928e-05, 0.00047273827565,
1.843809247e-05, 1.210348838e-05, 0.00058437528606, 3.159376081803e-06,
5.8875782795912e-06, 0.00016728261028, 0.00010030468982, 0.00049383802885,
0.00025073676102, 3.900598614e-05, 6.284136268e-05, 0.00018436588238,
1.36434877e-05), .Dim = c(24L, 5L), .Dimnames = list(c("1", "2",
"3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14",
"15", "16", "17", "18", "19", "20", "21", "22", "23", "24"),
    c("y", "x1", "x2", "x3", "psi2")))

 xy
               y                         x1                  x2
x3                     psi2
1   2.810951e-02 0.0033640478 0.7001848  0.0049089250 1.020997e-04
2  -5.975145e-03 0.0103978513 0.7711987  0.0106800158 8.411040e-06
3  -2.594868e-03 0.0065131075 0.6967958  0.0232956457 4.834942e-06
4  -2.408532e-02 0.0054501669 0.8383230  0.0200691793 1.076014e-04
5   1.233682e-02 0.0179569313 0.7146587  0.0006186211 2.130177e-04
6   3.352390e-03 0.0025483312 0.8158499  0.0001286417 2.203500e-05
7  -4.375316e-03 0.0062945400 0.7039135  0.0177302869 9.413511e-05
8   8.557305e-03 0.0057251040 0.8319837  0.0027839708 4.250193e-05
9  -2.951869e-03 0.0066973975 0.6469382  0.0020465874 2.722768e-04
10 -5.848326e-05 0.0124798350 0.7717270  0.0136013377 2.106426e-05
11  5.808473e-02 0.0004798937 0.5516694  0.0138148874 4.727383e-04
12 -6.242153e-03 0.0035834693 0.7474188  0.0185123141 1.843809e-05
13 -2.844949e-02 0.0104747528 0.6975613  0.0053506643 1.210349e-05
14  1.463635e-02 0.0022677483 0.6730272  0.0299188604 5.843753e-04
15 -1.537450e-02 0.0416382475 0.6725578  0.0149799955 3.159376e-06
16  7.891323e-03 0.0300726115 0.5998845 -0.0082679344 5.887578e-06
17 -1.758284e-02 0.0053794175 0.7811378  0.0051584752 1.672826e-04
18  2.185240e-02 0.0037613012 0.6864248  0.0179365887 1.003047e-04
19  2.861913e-02 0.0037269609 0.5997440  0.0096426321 4.938380e-04
20  2.695059e-03 0.0127221813 0.6862973  0.0062592135 2.507368e-04
21  6.452321e-03 0.0044457565 0.6746353  0.0039079024 3.900599e-05
22 -1.078894e-02 0.0135560790 0.7008584  0.0023717975 6.284136e-05
23 -5.494437e-02 0.0049545006 0.2578627  0.2545335184 1.843659e-04
24  5.394083e-02 0.0066581572 0.5945094  0.0066334011 1.364349e-05

> res$beta
(Intercept)          x1          x2          x3
  0.1369432  -0.9514077  -0.1672438  -0.5581261
> res$u
            1             2             3             4             5
 5.290481e-03  8.914887e-05 -1.039673e-04 -1.707174e-03  6.828549e-03
            6             7             8             9            10
 6.073888e-04 -2.725684e-03  3.517122e-03 -1.481810e-02  1.257265e-03
           11            12            13            14            15
 1.581453e-02 -4.299002e-04 -2.350822e-03  7.036544e-03  1.467205e-04
           16            17            18            19            20
-1.563975e-04 -7.831411e-03  4.896880e-03  6.726648e-04 -2.295879e-03
           21            22            23            24
-2.078902e-03 -4.358961e-03 -1.027172e-03  1.943738e-03
attr(,"format")
[1] "best"
> res$sig2e
[1] 0.0001721026



From dieter.menne at menne-biomed.de  Fri Nov 30 15:35:41 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 30 Nov 2007 14:35:41 +0000 (UTC)
Subject: [R-sig-ME] lme4 is now on R-forge
References: <40e66e0b0711091302h7ccb532bx94c7312526a774b6@mail.gmail.com>
Message-ID: <loom.20071130T143455-270@post.gmane.org>

Douglas Bates <bates at ...> writes:

> Starting tomorrow you should also be able to install the development
> version of the package with
> 
> install.packages("lme4", repos = "http://r-forge.r-project.org")
> 

When I try this, I get:

> install.packages("lme4", repos = "http://r-forge.r-project.org")
Warning: unable to access index for repository
http://r-forge.r-project.org/bin/windows/contrib/2.6
Warning message:
package ?lme4? is not available 


Dieter



From r.j.forsyth at newcastle.ac.uk  Fri Nov 30 18:16:31 2007
From: r.j.forsyth at newcastle.ac.uk (Rob Forsyth)
Date: Fri, 30 Nov 2007 17:16:31 +0000
Subject: [R-sig-ME] Difficulties fitting steeply rising sigmoid Emax
	functions
Message-ID: <EAC0F820-CD1C-48B8-A8E9-60F0FA82C8A8@newcastle.ac.uk>

I would very much appreciate help with the following problem. I have a  
simulated dataset comprising a sigmoid Emax function  
(E0+Emax*Dose^Hill)/(D50^Hill+Dose^Hill)) with added error terms. I am  
wanting to create nlsList() models to read back estimates of the  
original E0, Emax, D50 and Hill but get singular gradient and other  
errors.

I realise poor starting estimates will be a large part of the problem.  
It's quite a demanding dataset with some very high values of Hill  
(creating steeply rising curves which make locating D50 tricky) and  
significant "noise" from the error terms. I have written a selfStart  
sigmoid Emax function deriving initial estimates from a linear  
regression of log(theta/1-theta)~log(Dose) (where theta = (Emax-E0)/ 
Emax, which has slope of Hill and intercept of -Hill*logD50) however  
this does not perform well enough in this dataset and I'd appreciate  
any suggestions for more robust algorithms. I am aware that taking the  
log of the X axis converts this to the four-parameter logistic  
function but the SSfpl() function does not perform much better.

As an alternative since this is simulated data I do of course have  
access to the "real" values for E0, Emax, D50 and Hill (they are  
indeed present in the groupedData object I am trying to fit) but I am  
uncertain of the syntax required to utilise them - also in due course  
I want to create an nlme() model of the data: will this approach break  
if I do this?

Thanks

Rob Forsyth



From maechler at stat.math.ethz.ch  Fri Nov 30 18:26:16 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 30 Nov 2007 18:26:16 +0100
Subject: [R-sig-ME] R-forge problem (windows binary packages)! {Re: ... lme4
	...}
In-Reply-To: <loom.20071130T143455-270@post.gmane.org>
References: <40e66e0b0711091302h7ccb532bx94c7312526a774b6@mail.gmail.com>
	<loom.20071130T143455-270@post.gmane.org>
Message-ID: <18256.18360.167252.338974@stat.math.ethz.ch>

>>>>> "DM" == Dieter Menne <dieter.menne at menne-biomed.de>
>>>>>     on Fri, 30 Nov 2007 14:35:41 +0000 (UTC) writes:

    DM> Douglas Bates <bates at ...> writes:
    >> Starting tomorrow you should also be able to install the development
    >> version of the package with
    >> 
    >> install.packages("lme4", repos = "http://r-forge.r-project.org")
    >> 

    DM> When I try this, I get:

    >> install.packages("lme4", repos = "http://r-forge.r-project.org")
    DM> Warning: unable to access index for repository
    DM> http://r-forge.r-project.org/bin/windows/contrib/2.6
    DM> Warning message:
    DM> package ?lme4? is not available 

I see a severe problem with R-forge at the moment :
The 2.6   bin/windows/ directory looks completely empty.

Martin



From bates at stat.wisc.edu  Fri Nov 30 19:45:26 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 30 Nov 2007 12:45:26 -0600
Subject: [R-sig-ME] R-forge problem (windows binary packages)! {Re: ...
	lme4 ...}
In-Reply-To: <18256.18360.167252.338974@stat.math.ethz.ch>
References: <40e66e0b0711091302h7ccb532bx94c7312526a774b6@mail.gmail.com>
	<loom.20071130T143455-270@post.gmane.org>
	<18256.18360.167252.338974@stat.math.ethz.ch>
Message-ID: <40e66e0b0711301045n4420d083m1f4304b595ae2798@mail.gmail.com>

On Nov 30, 2007 11:26 AM, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> >>>>> "DM" == Dieter Menne <dieter.menne at menne-biomed.de>
> >>>>>     on Fri, 30 Nov 2007 14:35:41 +0000 (UTC) writes:
>
>     DM> Douglas Bates <bates at ...> writes:
>     >> Starting tomorrow you should also be able to install the development
>     >> version of the package with
>     >>
>     >> install.packages("lme4", repos = "http://r-forge.r-project.org")
>     >>
>
>     DM> When I try this, I get:
>
>     >> install.packages("lme4", repos = "http://r-forge.r-project.org")
>     DM> Warning: unable to access index for repository
>     DM> http://r-forge.r-project.org/bin/windows/contrib/2.6
>     DM> Warning message:
>     DM> package 'lme4' is not available
>
> I see a severe problem with R-forge at the moment :
> The 2.6   bin/windows/ directory looks completely empty.
>
> Martin

Also, the link labeled win32-latest.zip on the SCM page for the lme4
package on R-forge does not contain a Win32 binary package.  Instead
it is the build log.

The source package, labelled src-latest.tar.gz, does appear to be intact.

I will be updating the sources for the package this afternoon.  I
finally got generalized linear mixed models working in the development
version.  If the problem for Windows binary packages on R-forge is
resolved (it does look as if they are being built - I think the
problem is that the wrong file is being returned from the build
process) soon I will just ask Windows users to wait for a day or two.
If it is still a problem early next week I will create a Windows
binary package using win-builder.r-project.org and make it available
via http.



From stefan.theussl at wu-wien.ac.at  Fri Nov 30 21:07:11 2007
From: stefan.theussl at wu-wien.ac.at (=?ISO-8859-1?Q?Stefan_Theu=DFl?=)
Date: Fri, 30 Nov 2007 21:07:11 +0100
Subject: [R-sig-ME] R-forge problem (windows binary packages)! {Re: ...
 lme4 ...}
In-Reply-To: <40e66e0b0711301045n4420d083m1f4304b595ae2798@mail.gmail.com>
References: <40e66e0b0711091302h7ccb532bx94c7312526a774b6@mail.gmail.com>	
	<loom.20071130T143455-270@post.gmane.org>	
	<18256.18360.167252.338974@stat.math.ethz.ch>
	<40e66e0b0711301045n4420d083m1f4304b595ae2798@mail.gmail.com>
Message-ID: <47506D6F.90608@wu-wien.ac.at>

Hello Doug,

We had troubles with disk space on R-Forge. Among others this caused the 
website to malfunction. It is possible that the windows binary build 
process had been affected too.
However, everything should be OK by now except that the win binaries are 
not available yet.
I try to trigger the windows build process from home, but this implies a 
remote desktop connection which is rather slow.
Nevertheless, if I have no success with rdesktop tonight, tomorrow at 6 
am CET, the packages will be built automatically.

Best,
Stefan

Douglas Bates wrote:
> On Nov 30, 2007 11:26 AM, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
>   
>>>>>>> "DM" == Dieter Menne <dieter.menne at menne-biomed.de>
>>>>>>>     on Fri, 30 Nov 2007 14:35:41 +0000 (UTC) writes:
>>>>>>>               
>>     DM> Douglas Bates <bates at ...> writes:
>>     >> Starting tomorrow you should also be able to install the development
>>     >> version of the package with
>>     >>
>>     >> install.packages("lme4", repos = "http://r-forge.r-project.org")
>>     >>
>>
>>     DM> When I try this, I get:
>>
>>     >> install.packages("lme4", repos = "http://r-forge.r-project.org")
>>     DM> Warning: unable to access index for repository
>>     DM> http://r-forge.r-project.org/bin/windows/contrib/2.6
>>     DM> Warning message:
>>     DM> package 'lme4' is not available
>>
>> I see a severe problem with R-forge at the moment :
>> The 2.6   bin/windows/ directory looks completely empty.
>>
>> Martin
>>     
>
> Also, the link labeled win32-latest.zip on the SCM page for the lme4
> package on R-forge does not contain a Win32 binary package.  Instead
> it is the build log.
>
> The source package, labelled src-latest.tar.gz, does appear to be intact.
>
> I will be updating the sources for the package this afternoon.  I
> finally got generalized linear mixed models working in the development
> version.  If the problem for Windows binary packages on R-forge is
> resolved (it does look as if they are being built - I think the
> problem is that the wrong file is being returned from the build
> process) soon I will just ask Windows users to wait for a day or two.
> If it is still a problem early next week I will create a Windows
> binary package using win-builder.r-project.org and make it available
> via http.
>
>



From bates at stat.wisc.edu  Sat Dec  1 18:59:23 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 1 Dec 2007 11:59:23 -0600
Subject: [R-sig-ME] [R] lmer and method call
In-Reply-To: <loom.20071201T160227-587@post.gmane.org>
References: <37787.70.71.25.135.1196388560.squirrel@www.cfenet.ubc.ca>
	<40e66e0b0712010726j599dc405ifac521481b651ee1@mail.gmail.com>
	<loom.20071201T160227-587@post.gmane.org>
Message-ID: <40e66e0b0712010959j18ddf542x6798ab07c793e50b@mail.gmail.com>

On Dec 1, 2007 10:08 AM, Dieter Menne <dieter.menne at menne-biomed.de> wrote:
> Douglas Bates <bates <at> stat.wisc.edu> writes:
>
> (lmer)
>
> > The default is PQL, to refine the
> > starting estimates, followed by optimization of the Laplace
> > approximation.  In some cases it is an advantage to suppress the PQL
> > iterations which can be done with one of the settings for the control
> > argument.
>
> I had found out the hard way that it is often better to let PQL
> play the game rather loosely.  Yet I never dared to tell someone, for fear
> the approximation could end up in the wrong slot,

> Any rules (beside trying variants) if I can trust such a result?

I'm not sure I understand the sense of your first statement.  Do you
mean that you have found that you should use PQL or you should not use
PQL?

I would advise using the Laplace approximation for the final
estimates.  At one time I thought it would be much slower than the PQL
iterations but it doesn't seem to be that bad.

I also thought that PQL would refine the starting estimates in the
sense that it would take comparatively crude starting values and get
you much closer to the optimum before you switched to Laplace.
However, because PQL is an algorithm that iterates on both the fixed
effects and the random effects with fixed weights, then updates the
weights, then goes back to the fixed effects and random effects, etc.
there is a possibility that the early weights can force poor values of
the fixed effects and later iterations do not recover.

I tend to prefer the Laplace approximation directly without any PQL
iterations.  That is

 method = "Laplace", control = list(usePQL = FALSE)

I would be interested in learning what experiences you or others have
had with the different approaches.

I am cc:ing this to the R-SIG-mixed-models list and suggest we switch
to that list only for further discussion.



From dieter.menne at menne-biomed.de  Mon Dec  3 14:54:31 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 3 Dec 2007 13:54:31 +0000 (UTC)
Subject: [R-sig-ME] glmer != lmer(,..family=binomioal)
Message-ID: <loom.20071203T135043-886@post.gmane.org>

Dear lmers,

from reading the docs and the first lines of lmer() code, I got the
understanding that the two formulations below should be equivalent. The first
works as expected, the second gives an envir() error. What's wrong?

Dieter


x=data.frame(resp=rbinom(100,1,0.8),
           treat=sample(letters[7:8],100,TRUE),
           pat=as.factor(sample(letters[1:4],100,TRUE))
           )
glmer(resp~treat+(1|pat),family=binomial,data=x)
lmer(resp~treat+(1|pat),family=binomial,data=x)



From bates at stat.wisc.edu  Mon Dec  3 15:00:58 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 3 Dec 2007 08:00:58 -0600
Subject: [R-sig-ME] glmer != lmer(,..family=binomioal)
In-Reply-To: <loom.20071203T135043-886@post.gmane.org>
References: <loom.20071203T135043-886@post.gmane.org>
Message-ID: <40e66e0b0712030600r1856786fy2a38e858d66b242d@mail.gmail.com>

Can you tell us the version of the lme4 package that you are using, please?

On Dec 3, 2007 7:54 AM, Dieter Menne <dieter.menne at menne-biomed.de> wrote:
> Dear lmers,
>
> from reading the docs and the first lines of lmer() code, I got the
> understanding that the two formulations below should be equivalent. The first
> works as expected, the second gives an envir() error. What's wrong?
>
> Dieter
>
>
> x=data.frame(resp=rbinom(100,1,0.8),
>            treat=sample(letters[7:8],100,TRUE),
>            pat=as.factor(sample(letters[1:4],100,TRUE))
>            )
> glmer(resp~treat+(1|pat),family=binomial,data=x)
> lmer(resp~treat+(1|pat),family=binomial,data=x)
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From dieter.menne at menne-biomed.de  Mon Dec  3 15:10:13 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 3 Dec 2007 14:10:13 +0000 (UTC)
Subject: [R-sig-ME] glmer != lmer(,..family=binomioal)
References: <loom.20071203T135043-886@post.gmane.org>
	<40e66e0b0712030600r1856786fy2a38e858d66b242d@mail.gmail.com>
Message-ID: <loom.20071203T140645-63@post.gmane.org>

Douglas Bates <bates at ...> writes:

> 
> Can you tell us the version of the lme4 package that you are using, please?

Bad, Dieter. Time to read the posting guide. 

This is my home-build version for Windows from r-forge. I admit that I forgot to
explicitly check for a fresh version of Matrix.

Dieter


Package: lme4
Version: 0.999375-0
Date: 2007-11-10
Title: Linear mixed-effects models using S4 classes
Author: Douglas Bates <bates at stat.wisc.edu>
Maintainer: Douglas Bates <bates at stat.wisc.edu>
Description: Fit linear and generalized linear mixed-effects models.
Depends: methods, R(>= 2.6.0), Matrix(>= 0.999375-1), lattice
LinkingTo: Matrix, stats
Imports: graphics, stats
Suggests: coda, mlmRev, MEMSS
LazyLoad: yes
LazyData: yes
License: GPL (>=2)
Packaged: Sun Dec 2 09:18:26 2007; theussl
Built: R 2.6.1; i386-pc-mingw32; 2007-12-03 11:36:46; windows



From bates at stat.wisc.edu  Mon Dec  3 15:20:36 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 3 Dec 2007 08:20:36 -0600
Subject: [R-sig-ME] glmer != lmer(,..family=binomioal)
In-Reply-To: <loom.20071203T140645-63@post.gmane.org>
References: <loom.20071203T135043-886@post.gmane.org>
	<40e66e0b0712030600r1856786fy2a38e858d66b242d@mail.gmail.com>
	<loom.20071203T140645-63@post.gmane.org>
Message-ID: <40e66e0b0712030620l2be8ff3dmf6715a92feac47ff@mail.gmail.com>

Hmm.  Those calls should be equivalent in that version so I will look
into why they aren't.

At present I am involved in hand-to-hand combat with that code trying
to determine why the calculation of the Laplace approximation for the
glmer methods is not correct.  I thought I had it working but on
further checking I discovered that I don't.  Don't use that version
for glmer for a few days.


On Dec 3, 2007 8:10 AM, Dieter Menne <dieter.menne at menne-biomed.de> wrote:
> Douglas Bates <bates at ...> writes:
>
> >
> > Can you tell us the version of the lme4 package that you are using, please?
>
> Bad, Dieter. Time to read the posting guide.
>
> This is my home-build version for Windows from r-forge. I admit that I forgot to
> explicitly check for a fresh version of Matrix.
>
> Dieter
>
>
> Package: lme4
> Version: 0.999375-0
> Date: 2007-11-10
> Title: Linear mixed-effects models using S4 classes
> Author: Douglas Bates <bates at stat.wisc.edu>
> Maintainer: Douglas Bates <bates at stat.wisc.edu>
> Description: Fit linear and generalized linear mixed-effects models.
> Depends: methods, R(>= 2.6.0), Matrix(>= 0.999375-1), lattice
> LinkingTo: Matrix, stats
> Imports: graphics, stats
> Suggests: coda, mlmRev, MEMSS
> LazyLoad: yes
> LazyData: yes
> License: GPL (>=2)
> Packaged: Sun Dec 2 09:18:26 2007; theussl
> Built: R 2.6.1; i386-pc-mingw32; 2007-12-03 11:36:46; windows
>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From dieter.menne at menne-biomed.de  Tue Dec  4 08:55:18 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Tue, 4 Dec 2007 07:55:18 +0000 (UTC)
Subject: [R-sig-ME] lmer (binomial) and mcmcsamp
Message-ID: <loom.20071204T074500-257@post.gmane.org>

Douglas,

I am using the latest CRAN version; details see below.

I tried to run mcmcsamp on a problem of the form 

The model is

 lmer(RespB~Treat+Gender+(1|Pat),family=binomial) 


Results look ok, in reasonable agreement with method="PQL", and in good
agreement with G?ran's glmmML.

Running mcmcsamp() gave strange results. See 

http://www.menne-biomed.de/uni/mcmc20000.pdf
http://www.menne-biomed.de/uni/mcmc2000.pdf

I cannot get rid of the occasional stuck flat segments in between, even if I run
several times. Is this

-- Expected behavior, do your homework
-- Somewhat fishy; don't trust the results
-- Ill posed problem
-- Well known issue with current lmer
-- ...

Dieter

#------------------------------
Generalized linear mixed model fit using Laplace 
Formula: form 
   Data: x 
 Family: binomial(logit link)
 AIC BIC logLik deviance
 340 356   -166      332
Random effects:
 Groups Name        Variance Std.Dev.
 Pat    (Intercept) 2.15     1.47    
number of obs: 426, groups: Pat, 20

Estimated scale (compare to  1 )  0.859 

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)   -1.809      0.497   -3.64  0.00027 ***
TreatFOS      -0.267      0.290   -0.92  0.35775    
Genderm       -0.775      0.778   -0.99  0.31975    
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 

Correlation of Fixed Effects:
         (Intr) TrtFOS
TreatFOS -0.274       
Genderm  -0.593  0.009


Package: lme4
Version: 0.99875-9
Date: 2007-10-14
Built: R 2.6.0; i386-pc-mingw32; 2007-10-15 12:22:16; windows



From bates at stat.wisc.edu  Tue Dec  4 17:56:39 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 4 Dec 2007 10:56:39 -0600
Subject: [R-sig-ME] lmer (binomial) and mcmcsamp
In-Reply-To: <loom.20071204T074500-257@post.gmane.org>
References: <loom.20071204T074500-257@post.gmane.org>
Message-ID: <40e66e0b0712040856j580d0567ud8daafd163fbd92c@mail.gmail.com>

On Dec 4, 2007 1:55 AM, Dieter Menne <dieter.menne at menne-biomed.de> wrote:
> Douglas,
>
> I am using the latest CRAN version; details see below.
>
> I tried to run mcmcsamp on a problem of the form
>
> The model is
>
>  lmer(RespB~Treat+Gender+(1|Pat),family=binomial)
>
>
> Results look ok, in reasonable agreement with method="PQL", and in good
> agreement with G?ran's glmmML.
>
> Running mcmcsamp() gave strange results. See
>
> http://www.menne-biomed.de/uni/mcmc20000.pdf
> http://www.menne-biomed.de/uni/mcmc2000.pdf
>
> I cannot get rid of the occasional stuck flat segments in between, even if I run
> several times. Is this
>
> -- Expected behavior, do your homework
> -- Somewhat fishy; don't trust the results
> -- Ill posed problem
> -- Well known issue with current lmer
> -- ...
>
> Dieter

This is a known issue with the current lmer and you should not trust
the results.

The problem is that I use a Metropolis-Hastings update for the fixed
effects and the random effects together, conditional on the variance
components.  In awkward cases the approximate distribution from which
the sample is drawn is not a good approximation to the conditional
distribution of these parameters and the proposed step keeps getting
rejected.

I believe the solution is to separate the fixed effects from the
random effects but that is difficult to do in the formulation for the
0.99875 series.  This was one of the reasons for changing the internal
representation in the 0.999375 series (the development version) but I
am still encountering difficulties in some of the code design for that
series.  I thought I had the penalized iteratively reweighted least
squares algorithm for determining the conditional modes working but
then I tried it on a difficult example and found that it isn't working
as well as I thought it was.

I am working on it.  It is just taking considerably longer than I
thought it would.

> #------------------------------
> Generalized linear mixed model fit using Laplace
> Formula: form
>    Data: x
>  Family: binomial(logit link)
>  AIC BIC logLik deviance
>  340 356   -166      332
> Random effects:
>  Groups Name        Variance Std.Dev.
>  Pat    (Intercept) 2.15     1.47
> number of obs: 426, groups: Pat, 20
>
> Estimated scale (compare to  1 )  0.859
>
> Fixed effects:
>             Estimate Std. Error z value Pr(>|z|)
> (Intercept)   -1.809      0.497   -3.64  0.00027 ***
> TreatFOS      -0.267      0.290   -0.92  0.35775
> Genderm       -0.775      0.778   -0.99  0.31975
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> Correlation of Fixed Effects:
>          (Intr) TrtFOS
> TreatFOS -0.274
> Genderm  -0.593  0.009
>
>
> Package: lme4
> Version: 0.99875-9
> Date: 2007-10-14
> Built: R 2.6.0; i386-pc-mingw32; 2007-10-15 12:22:16; windows
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From dieter.menne at menne-biomed.de  Tue Dec  4 18:06:36 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Tue, 4 Dec 2007 17:06:36 +0000 (UTC)
Subject: [R-sig-ME] lmer (binomial) and mcmcsamp
References: <loom.20071204T074500-257@post.gmane.org>
	<40e66e0b0712040856j580d0567ud8daafd163fbd92c@mail.gmail.com>
Message-ID: <loom.20071204T170316-66@post.gmane.org>

Douglas Bates <bates at ...> writes:

> 
> This is a known issue with the current lmer and you should not trust
> the results.

Thanks, Douglas. May I suggest to use

prettyversion = round(-log(1-Version),1)

as an easier-to-read alternative to version number like 0.999375. After all 1.00
does not have the thrill of tex-pi.

Dieter



From r.j.forsyth at newcastle.ac.uk  Thu Dec  6 12:55:32 2007
From: r.j.forsyth at newcastle.ac.uk (Rob Forsyth)
Date: Thu, 6 Dec 2007 11:55:32 +0000
Subject: [R-sig-ME] Extracting significance for fixed effect
Message-ID: <7BC08466-A342-4A2D-A8C9-5E76EB21D054@newcastle.ac.uk>

A hopefully simple query. I have an nlme model based on the SSlogis()  
function in which I am testing for a fixed effect of GROUP on xmid. I  
can see the p value of xmid.GROUP in the summary() but what is the  
syntax to extract this as a variable for further use?

thanks

Rob



From dieter.menne at menne-biomed.de  Thu Dec  6 15:20:54 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 6 Dec 2007 14:20:54 +0000 (UTC)
Subject: [R-sig-ME] Extracting significance for fixed effect
References: <7BC08466-A342-4A2D-A8C9-5E76EB21D054@newcastle.ac.uk>
Message-ID: <loom.20071206T141457-13@post.gmane.org>

Rob Forsyth <r.j.forsyth at ...> writes:

> 
> A hopefully simple query. I have an nlme model based on the SSlogis()  
> function in which I am testing for a fixed effect of GROUP on xmid. I  
> can see the p value of xmid.GROUP in the summary() but what is the  
> syntax to extract this as a variable for further use?
> 

library(nlme)
fm1Soy.lis <- nlsList(weight ~ SSlogis(Time, Asym, xmid, scal),
                       data = Soybean)
fm1Soy.nlme <- nlme(fm1Soy.lis)
# Dear Nikolaus, I wish future versions to have a nice 
# extractor function for the following.
fm1Soy.tab = summary(fm1Soy.nlme)$tTable

... and I leave it as an easy exercise to extract the wanted from fm1Soy.tab.

Note that this is a developer list for a future version of lme(x); for nlme you
usually should contact r-help. And next time, please supply an example such as
the above to make life easier, which will increase the chances to get a response.


Dieter



From notwombe at yahoo.com  Thu Dec  6 21:33:31 2007
From: notwombe at yahoo.com (kennedy otwombe)
Date: Thu, 6 Dec 2007 12:33:31 -0800 (PST)
Subject: [R-sig-ME] glmmPQL inquiry
Message-ID: <376844.51701.qm@web54009.mail.re2.yahoo.com>

Dear R users,

I have longitudinal data that is all binary and i have run it in SAS using the following code without a problem:

proc glimmix data=navs;
class id;
model y(event='1')=x1 x2 x3/solution distribution=binary;
random intercept/subject=id type=cs;
run;

I have also written a code in R for the same analysis but i am getting the following error message (iteration 1
Error in switch(mode(x), "NULL" = structure(NULL, class = "formula"),  : 
  invalid formula). 

My code in R reads as follows:

>fit<-glmmPQL(y~x1+x2+x3, random=~1|id, family=binomial, data=navs)
>summary(fit)

I am not sure where the problem lies but i realise that glmmPQL does not seem to cater for binary distributions and i aint sure how to model the probability of the event Y=1 which is my interest in the data i am assuming. My data looks as follows:

  t id y x0 x1 x2 x3
  1  1 0  0  0  1  1
  2  1 0  0  1  0  1
  3  1 0  0  1  0  1
  1  2 0  0  0  1  0
  2  2 0  1  1  1  0
  3  2 0  1  1  1  0

I will appreciate any ideas from this network.

 
Kennedy.N.Otwombe
School of Statistics & Actuarial Science 
University of the Witwatersrand 
Private Bag 3 
Wits 2050 
Johannesburg 
South Africa


      ____________________________________________________________________________________
Looking for last minute shopping deals?



From kjbeath at kagi.com  Thu Dec  6 22:23:58 2007
From: kjbeath at kagi.com (Ken Beath)
Date: Fri, 7 Dec 2007 08:23:58 +1100
Subject: [R-sig-ME] glmmPQL inquiry
In-Reply-To: <376844.51701.qm@web54009.mail.re2.yahoo.com>
References: <376844.51701.qm@web54009.mail.re2.yahoo.com>
Message-ID: <4000BCEE-AACD-4D76-B9B4-CCA3C64CACAE@kagi.com>

On 07/12/2007, at 7:33 AM, kennedy otwombe wrote:

> Dear R users,
>
> I have longitudinal data that is all binary and i have run it in SAS  
> using the following code without a problem:
>
> proc glimmix data=navs;
> class id;
> model y(event='1')=x1 x2 x3/solution distribution=binary;
> random intercept/subject=id type=cs;
> run;
>
> I have also written a code in R for the same analysis but i am  
> getting the following error message (iteration 1
> Error in switch(mode(x), "NULL" = structure(NULL, class =  
> "formula"),  :
>  invalid formula).
>
> My code in R reads as follows:
>
>> fit<-glmmPQL(y~x1+x2+x3, random=~1|id, family=binomial, data=navs)
>> summary(fit)
>
> I am not sure where the problem lies but i realise that glmmPQL does  
> not seem to cater for binary distributions and i aint sure how to  
> model the probability of the event Y=1 which is my interest in the  
> data i am assuming. My data looks as follows:
>
>  t id y x0 x1 x2 x3
>  1  1 0  0  0  1  1
>  2  1 0  0  1  0  1
>  3  1 0  0  1  0  1
>  1  2 0  0  0  1  0
>  2  2 0  1  1  1  0
>  3  2 0  1  1  1  0
>

Your code is OK and doesn't give the "invalid formula" message with  
the current R (is this the one you are using?).

The example data wont work because all the y values are zero.

Ken



From sundar.dorai-raj at pdf.com  Thu Dec  6 21:57:55 2007
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 06 Dec 2007 12:57:55 -0800
Subject: [R-sig-ME] glmmPQL inquiry
In-Reply-To: <376844.51701.qm@web54009.mail.re2.yahoo.com>
References: <376844.51701.qm@web54009.mail.re2.yahoo.com>
Message-ID: <47586253.4020407@pdf.com>



kennedy otwombe said the following on 12/6/2007 12:33 PM:
> Dear R users,
> 
> I have longitudinal data that is all binary and i have run it in SAS using the following code without a problem:
> 
> proc glimmix data=navs;
> class id;
> model y(event='1')=x1 x2 x3/solution distribution=binary;
> random intercept/subject=id type=cs;
> run;
> 
> I have also written a code in R for the same analysis but i am getting the following error message (iteration 1
> Error in switch(mode(x), "NULL" = structure(NULL, class = "formula"),  : 
>   invalid formula). 
> 
> My code in R reads as follows:
> 
>> fit<-glmmPQL(y~x1+x2+x3, random=~1|id, family=binomial, data=navs)
>> summary(fit)
> 
> I am not sure where the problem lies but i realise that glmmPQL does not seem to cater for binary distributions and i aint sure how to model the probability of the event Y=1 which is my interest in the data i am assuming. My data looks as follows:
> 
>   t id y x0 x1 x2 x3
>   1  1 0  0  0  1  1
>   2  1 0  0  1  0  1
>   3  1 0  0  1  0  1
>   1  2 0  0  0  1  0
>   2  2 0  1  1  1  0
>   3  2 0  1  1  1  0
> 
> I will appreciate any ideas from this network.
> 
>  
> Kennedy.N.Otwombe
> School of Statistics & Actuarial Science 
> University of the Witwatersrand 
> Private Bag 3 
> Wits 2050 
> Johannesburg 
> South Africa
> 

Hi, Kennedy,

You have not given us enough information:

1. Result from sessionInfo (includes MASS version assuming glmmPQL is 
from MASS, nlme version, and R version)
2. Realistic data - your example data has y == 0 always. You will never 
produce a realistic model for these observations. And if I fake it and 
assign some ones to "y" the code you provide works.

library(MASS)
navs <- read.table(con <- textConnection("t id y x0 x1 x2 x3
   1  1 0  0  0  1  1
   2  1 0  0  1  0  1
   3  1 0  0  1  0  1
   1  2 0  0  0  1  0
   2  2 0  1  1  1  0
   3  2 1  1  1  1  0"), header = TRUE) ## last row has a 1
close(con)
fit <- glmmPQL(y~x1+x2+x3, random=~1|id, family=binomial, data=navs)

 > summary(fit)
Linear mixed-effects model fit by maximum likelihood
  Data: navs
   AIC BIC logLik
    NA  NA     NA

Random effects:
  Formula: ~1 | id
          (Intercept)  Residual
StdDev: 8.009602e-05 0.5773503

Variance function:
  Structure: fixed weights
  Formula: ~invwt
Fixed effects: y ~ x1 + x2 + x3
                 Value Std.Error DF       t-value p-value
(Intercept) -61.13214   6171227  2 -9.905993e-06       1
x1           30.56607   2631420  2  1.161581e-05       1
x2           30.56607   4160641  2  7.346481e-06       1
x3            0.00000   3721390  0  0.000000e+00     NaN
  Correlation:
    (Intr) x1     x2
x1 -0.853
x2 -0.944  0.632
x3 -0.905  0.707  0.894

Standardized Within-Group Residuals:
           Min            Q1           Med            Q3           Max
-1.732051e+00 -4.715395e-16 -2.827740e-16  4.694641e-17  1.732051e+00

Number of Observations: 6
Number of Groups: 2
Warning message:
In pt(q, df, lower.tail, log.p) : NaNs produced
 > sessionInfo()
R version 2.6.1 (2007-11-26)
i386-pc-mingw32

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United 
States.1252;LC_MONETARY=English_United 
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] nlme_3.1-86 MASS_7.2-38

loaded via a namespace (and not attached):
[1] grid_2.6.1     lattice_0.17-2
 >



From notwombe at yahoo.com  Fri Dec  7 09:58:28 2007
From: notwombe at yahoo.com (kennedy otwombe)
Date: Fri, 7 Dec 2007 00:58:28 -0800 (PST)
Subject: [R-sig-ME] glmmPQL inquiry
Message-ID: <431397.3419.qm@web54010.mail.re2.yahoo.com>

Hi Sundar/Ken,
I would like to clarify my inquiry. The data structure i gave was just a sample of my data. I actually have 50 subjects each with three binary observations (hence 150 observations). So the error i initially gave emanates from the analysis involving all the 50 subjects. It just turns out that for the sample i gave, the first two subjects have only 0 as their entries but this varies as you look through the data.
I am using the latest version of R i.e 2.6.1 and i downloaded the nlme version currently available on the CRAN website. I am aslo assuming my random part is the intercept.
Hope this clarifies my inquiry.

Kennedy.N.Otwombe
School of Statistics & Actuarial Science 
University of the Witwatersrand 
Private Bag 3 
Wits 2050 
Johannesburg 
South Africa



----- Original Message ----
From: Sundar Dorai-Raj <sundar.dorai-raj at pdf.com>
To: kennedy otwombe <notwombe at yahoo.com>
Cc: R-sig-mixed-models at r-project.org
Sent: Thursday, December 6, 2007 10:57:55 PM
Subject: Re: [R-sig-ME] glmmPQL inquiry



kennedy otwombe said the following on 12/6/2007 12:33 PM:
> Dear R users,
> 
> I have longitudinal data that is all binary and i have run it in SAS using the following code without a problem:
> 
> proc glimmix data=navs;
> class id;
> model y(event='1')=x1 x2 x3/solution distribution=binary;
> random intercept/subject=id type=cs;
> run;
> 
> I have also written a code in R for the same analysis but i am getting the following error message (iteration 1
> Error in switch(mode(x), "NULL" = structure(NULL, class = "formula"),   : 
>  invalid formula). 
> 
> My code in R reads as follows:
> 
>> fit<-glmmPQL(y~x1+x2+x3, random=~1|id, family=binomial, data=navs)
>> summary(fit)
> 
> I am not sure where the problem lies but i realise that glmmPQL does not seem to cater for binary distributions and i aint sure how to model the probability of the event Y=1 which is my interest in the data i am assuming. My data looks as follows:
> 
>  t id y x0 x1 x2 x3
>  1  1 0  0  0  1  1
>  2  1 0  0  1  0  1
>  3  1 0  0  1  0  1
>  1  2 0  0  0  1  0
>  2  2 0  1  1  1  0
>  3  2 0  1  1  1  0
> 
> I will appreciate any ideas from this network.
> 
>  
> Kennedy.N.Otwombe
> School of Statistics & Actuarial Science 
> University of the Witwatersrand 
> Private Bag 3 
> Wits 2050 
> Johannesburg 
> South Africa
> 

Hi, Kennedy,

You have not given us enough information:

1. Result from sessionInfo (includes MASS version assuming glmmPQL is 
from MASS, nlme version, and R version)
2. Realistic data - your example data has y == 0 always. You will never 
produce a realistic model for these observations. And if I fake it and 
assign some ones to "y" the code you provide works.

library(MASS)
navs <- read.table(con <- textConnection("t id y x0 x1 x2 x3
  1  1 0  0  0  1  1
  2  1 0  0  1  0  1
  3  1 0  0  1  0  1
  1  2 0  0  0  1  0
  2  2 0  1  1  1  0
  3  2 1  1  1  1  0"), header = TRUE) ## last row has a 1
close(con)
fit <- glmmPQL(y~x1+x2+x3, random=~1|id, family=binomial, data=navs)

> summary(fit)
Linear mixed-effects model fit by maximum likelihood
  Data: navs
  AIC BIC logLik
    NA  NA    NA

Random effects:
  Formula: ~1 | id
          (Intercept)  Residual
StdDev: 8.009602e-05 0.5773503

Variance function:
  Structure: fixed weights
  Formula: ~invwt
Fixed effects: y ~ x1 + x2 + x3
                Value Std.Error DF      t-value p-value
(Intercept) -61.13214  6171227  2 -9.905993e-06      1
x1          30.56607  2631420  2  1.161581e-05      1
x2          30.56607  4160641  2  7.346481e-06      1
x3            0.00000  3721390  0  0.000000e+00    NaN
  Correlation:
    (Intr) x1    x2
x1 -0.853
x2 -0.944  0.632
x3 -0.905  0.707  0.894

Standardized Within-Group Residuals:
          Min            Q1          Med            Q3          Max
-1.732051e+00 -4.715395e-16 -2.827740e-16  4.694641e-17  1.732051e+00

Number of Observations: 6
Number of Groups: 2
Warning message:
In pt(q, df, lower.tail, log.p) : NaNs produced
> sessionInfo()
R version 2.6.1 (2007-11-26)
i386-pc-mingw32

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United 
States.1252;LC_MONETARY=English_United 
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] stats    graphics  grDevices utils    datasets  methods  base

other attached packages:
[1] nlme_3.1-86 MASS_7.2-38

loaded via a namespace (and not attached):
[1] grid_2.6.1    lattice_0.17-2
>


      ____________________________________________________________________________________
Looking for last minute shopping deals?



From kjbeath at kagi.com  Fri Dec  7 10:30:15 2007
From: kjbeath at kagi.com (Ken Beath)
Date: Fri, 7 Dec 2007 20:30:15 +1100
Subject: [R-sig-ME] glmmPQL inquiry
In-Reply-To: <431397.3419.qm@web54010.mail.re2.yahoo.com>
References: <431397.3419.qm@web54010.mail.re2.yahoo.com>
Message-ID: <725E443D-F015-493D-A4E9-3F61E5B6DC16@kagi.com>

On 07/12/2007, at 7:58 PM, kennedy otwombe wrote:

> Hi Sundar/Ken,
> I would like to clarify my inquiry. The data structure i gave was  
> just a sample of my data. I actually have 50 subjects each with  
> three binary observations (hence 150 observations). So the error i  
> initially gave emanates from the analysis involving all the 50  
> subjects. It just turns out that for the sample i gave, the first  
> two subjects have only 0 as their entries but this varies as you  
> look through the data.
> I am using the latest version of R i.e 2.6.1 and i downloaded the  
> nlme version currently available on the CRAN website. I am aslo  
> assuming my random part is the intercept.
> Hope this clarifies my inquiry.
>

If I modify your code to include better values for y, using code

library(MASS)
navs<-data.frame(matrix(c(1,1,1,0,0,1,1,
2,1,0,0,1,0,1,
3,1,0,0,1,0,1,
1,2,1,0,0,1,0,
2,2,1,1,1,1,0,
3,2,0,1,1,1,0),nrow=6,byrow=T))

names(navs) <- c("t","id","y","x0","x1","x2","x3")

fit<-glmmPQL(y~x1+x2+x3, random=~1|id, family=binomial, data=navs)
summary(fit)

the results are fine, except for some problems due to not having  
enough subjects, as in the following output.

Ken

 > library(MASS)
 > navs<-data.frame(matrix(c(1,1,1,0,0,1,1,
+ 2,1,0,0,1,0,1,
+ 3,1,0,0,1,0,1,
+ 1,2,1,0,0,1,0,
+ 2,2,1,1,1,1,0,
+ 3,2,0,1,1,1,0),nrow=6,byrow=T))
 >
 > names(navs) <- c("t","id","y","x0","x1","x2","x3")
 >
 > fit<-glmmPQL(y~x1+x2+x3, random=~1|id, family=binomial, data=navs)
iteration 1
iteration 2
iteration 3
iteration 4
iteration 5
iteration 6
iteration 7
iteration 8
iteration 9
iteration 10
 > summary(fit)
Linear mixed-effects model fit by maximum likelihood
  Data: navs
   AIC BIC logLik
    NA  NA     NA

Random effects:
  Formula: ~1 | id
          (Intercept)  Residual
StdDev: 8.009601e-05 0.5773503

Variance function:
  Structure: fixed weights
  Formula: ~invwt
Fixed effects: y ~ x1 + x2 + x3
                  Value Std.Error DF       t-value p-value
(Intercept)   0.000353   6171979  2  5.700000e-11       1
x1          -30.566351   2631792  2 -1.161427e-05       1
x2           30.565997   4161051  2  7.345739e-06       1
x3           -0.000071   3721849  0 -1.900000e-11     NaN
  Correlation:
    (Intr) x1     x2
x1 -0.853
x2 -0.944  0.632
x3 -0.905  0.707  0.894

Standardized Within-Group Residuals:
           Min            Q1           Med            Q3           Max
-1.732051e+00 -5.451298e-17  8.078053e-16  1.506878e-15  1.732051e+00

Number of Observations: 6
Number of Groups: 2
Warning message:
In pt(q, df, lower.tail, log.p) : NaNs produced



From kyler at mail.smu.edu  Fri Dec  7 18:18:55 2007
From: kyler at mail.smu.edu (Roberts, Kyle)
Date: Fri, 7 Dec 2007 11:18:55 -0600
Subject: [R-sig-ME] QQ Norm
In-Reply-To: <725E443D-F015-493D-A4E9-3F61E5B6DC16@kagi.com>
References: <431397.3419.qm@web54010.mail.re2.yahoo.com>
	<725E443D-F015-493D-A4E9-3F61E5B6DC16@kagi.com>
Message-ID: <F8AF6BF6CD1CC040AF35B0C2D1680BBB041F0CCF@s31xe7.systems.smu.edu>

Just a simple yes or no.  Is there any difference between:

qqnorm(resid(lmer.out))
and
qqmath(~resid(lmer.out))

Thanks,
Kyle

********************************************************
Dr. J. Kyle Roberts
Department of Literacy, Language and Learning
School of Education and Human Development
Southern Methodist University
P.O. Box 750381
Dallas, TX  75275
214-768-4494
http://www.hlm-online.com/



From bates at stat.wisc.edu  Fri Dec  7 19:15:37 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 7 Dec 2007 12:15:37 -0600
Subject: [R-sig-ME] QQ Norm
In-Reply-To: <F8AF6BF6CD1CC040AF35B0C2D1680BBB041F0CCF@s31xe7.systems.smu.edu>
References: <431397.3419.qm@web54010.mail.re2.yahoo.com>
	<725E443D-F015-493D-A4E9-3F61E5B6DC16@kagi.com>
	<F8AF6BF6CD1CC040AF35B0C2D1680BBB041F0CCF@s31xe7.systems.smu.edu>
Message-ID: <40e66e0b0712071015wf84e875uc5cac668332e92c8@mail.gmail.com>

On Dec 7, 2007 11:18 AM, Roberts, Kyle <kyler at mail.smu.edu> wrote:
> Just a simple yes or no.  Is there any difference between:
>
> qqnorm(resid(lmer.out))
> and
> qqmath(~resid(lmer.out))

That's a bad question for which to say that you want a simple yes/no
because the answer is obciously yes and now you are going to need to
ask what the differences are.



From bates at stat.wisc.edu  Fri Dec  7 23:04:21 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 7 Dec 2007 16:04:21 -0600
Subject: [R-sig-ME] QQ Norm
In-Reply-To: <40e66e0b0712071015wf84e875uc5cac668332e92c8@mail.gmail.com>
References: <431397.3419.qm@web54010.mail.re2.yahoo.com>
	<725E443D-F015-493D-A4E9-3F61E5B6DC16@kagi.com>
	<F8AF6BF6CD1CC040AF35B0C2D1680BBB041F0CCF@s31xe7.systems.smu.edu>
	<40e66e0b0712071015wf84e875uc5cac668332e92c8@mail.gmail.com>
Message-ID: <40e66e0b0712071404o457376fcx7644c694a60518a3@mail.gmail.com>

On Dec 7, 2007 12:15 PM, Douglas Bates <bates at stat.wisc.edu> wrote:
> On Dec 7, 2007 11:18 AM, Roberts, Kyle <kyler at mail.smu.edu> wrote:
> > Just a simple yes or no.  Is there any difference between:
> >
> > qqnorm(resid(lmer.out))
> > and
> > qqmath(~resid(lmer.out))
>
> That's a bad question for which to say that you want a simple yes/no
> because the answer is obciously yes and now you are going to need to
> ask what the differences are.

I was being facetious with that answer.  There are obvious differences
in that qqnorm is a standard graphics function and qqmath is
lattice/grid.  Other than that there isn't really a difference.

The only class of objects for which the lme4 package defines a special
qqmath method is the ranef classes, such as ranef.mer and ranef.lmer.
For those classes the method creates a form of "caterpillar plot" if
you ask for the posterior variances (which, if I had it to do over
again, I would call "conditional variances") to be returned (i.e.
postVar = TRUE).

By the way, it is now possible to omit the ~ in the first argument for
qqmath so you could write

qqmath(resid(lmer.out))

Whether it is advisable to do so is something I still haven't decided
for myself.



From kjbeath at kagi.com  Sat Dec  8 07:56:48 2007
From: kjbeath at kagi.com (Ken Beath)
Date: Sat, 8 Dec 2007 17:56:48 +1100
Subject: [R-sig-ME] glmmPQL inquiry
In-Reply-To: <79485.95320.qm@web54011.mail.re2.yahoo.com>
References: <79485.95320.qm@web54011.mail.re2.yahoo.com>
Message-ID: <ACE4813F-5DBB-4A3B-9F79-BAFFF0DBBAA9@kagi.com>

I don't know, possibly it is an error in glmmPQL, but I don't think  
compound symmetry is needed, I would expect it is the default.

You could try lmer, it may be better. My preference for binomials  
where there is a high correlation within clusters/subjects is not to  
use Laplace or PQL but adaptive Gauss-Hermite quadrature, which last  
time I looked didn't seem to be available in lmer, contrary to what  
the help says. This unfortunately means Stata or SAS.

Ken


On 07/12/2007, at 8:49 PM, kennedy otwombe wrote:

> Hi Ken,
> With my dataset that has 50 subjects each with three observations, i  
> get the error i gave if i use the compound symmetry covariance  
> structure. Would you have an idea why this is happening?
>
> Kennedy
>
> ----- Original Message ----
> From: Ken Beath <kjbeath at kagi.com>
> To: kennedy otwombe <notwombe at yahoo.com>
> Cc: R-SIG-Mixed-Models <R-sig-mixed-models at r-project.org>
> Sent: Friday, December 7, 2007 11:30:15 AM
> Subject: Re: [R-sig-ME] glmmPQL inquiry
>
> On 07/12/2007, at 7:58 PM, kennedy otwombe wrote:
>
>> Hi Sundar/Ken,
>> I would like to clarify my inquiry. The data structure i gave was
>> just a sample of my data. I actually have 50 subjects each with
>> three binary observations (hence 150 observations). So the error i
>> initially gave emanates from the analysis involving all the 50
>> subjects. It just turns out that for the sample i gave, the first
>> two subjects have only 0 as their entries but this varies as you
>> look through the data.
>> I am using the latest version of R i.e 2.6.1 and i downloaded the
>> nlme version currently available on the CRAN website. I am aslo
>> assuming my random part is the intercept.
>> Hope this clarifies my inquiry.
>>
>
> If I modify your code to include better values for y, using code
>
> library(MASS)
> navs<-data.frame(matrix(c(1,1,1,0,0,1,1,
> 2,1,0,0,1,0,1,
> 3,1,0,0,1,0,1,
> 1,2,1,0,0,1,0,
> 2,2,1,1,1,1,0,
> 3,2,0,1,1,1,0),nrow=6,byrow=T))
>
> names(navs) <- c("t","id","y","x0","x1","x2","x3")
>
> fit<-glmmPQL(y~x1+x2+x3, random=~1|id, family=binomial, data=navs)
> summary(fit)
>
> the results are fine, except for some problems due to not having
> enough subjects, as in the following output.
>
> Ken
>
>> library(MASS)
>> navs<-data.frame(matrix(c(1,1,1,0,0,1,1,
> + 2,1,0,0,1,0,1,
> + 3,1,0,0,1,0,1,
> + 1,2,1,0,0,1,0,
> + 2,2,1,1,1,1,0,
> + 3,2,0,1,1,1,0),nrow=6,byrow=T))
>>
>> names(navs) <- c("t","id","y","x0","x1","x2","x3")
>>
>> fit<-glmmPQL(y~x1+x2+x3, random=~1|id, family=binomial, data=navs)
> iteration 1
> iteration 2
> iteration 3
> iteration 4
> iteration 5
> iteration 6
> iteration 7
> iteration 8
> iteration 9
> iteration 10
>> summary(fit)
> Linear mixed-effects model fit by maximum likelihood
>  Data: navs
>  AIC BIC logLik
>    NA  NA    NA
>
> Random effects:
>  Formula: ~1 | id
>          (Intercept)  Residual
> StdDev: 8.009601e-05 0.5773503
>
> Variance function:
>  Structure: fixed weights
>  Formula: ~invwt
> Fixed effects: y ~ x1 + x2 + x3
>                  Value Std.Error DF      t-value p-value
> (Intercept)  0.000353  6171979  2  5.700000e-11      1
> x1          -30.566351  2631792  2 -1.161427e-05      1
> x2          30.565997  4161051  2  7.345739e-06      1
> x3          -0.000071  3721849  0 -1.900000e-11    NaN
>  Correlation:
>    (Intr) x1    x2
> x1 -0.853
> x2 -0.944  0.632
> x3 -0.905  0.707  0.894
>
> Standardized Within-Group Residuals:
>          Min            Q1          Med            Q3          Max
> -1.732051e+00 -5.451298e-17  8.078053e-16  1.506878e-15  1.732051e+00
>
> Number of Observations: 6
> Number of Groups: 2
> Warning message:
> In pt(q, df, lower.tail, log.p) : NaNs produced
>
>
>       
> ____________________________________________________________________________________
> Looking for last minute shopping deals?
> Find them fast with Yahoo! Search.  http://tools.search.yahoo.com/newsearch/category.php?category=shopping
>



From dieter.menne at menne-biomed.de  Sat Dec  8 16:47:10 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sat, 8 Dec 2007 15:47:10 +0000 (UTC)
Subject: [R-sig-ME] lmer (binomial) and mcmcsamp.. and now?
References: <loom.20071204T074500-257@post.gmane.org>
	<40e66e0b0712040856j580d0567ud8daafd163fbd92c@mail.gmail.com>
Message-ID: <loom.20071208T153535-935@post.gmane.org>

Dieter Menne wrote

> > I tried to run mcmcsamp on a problem of the form
> >
> > The model is
> >
> >  lmer(RespB~Treat+Gender+(1|Pat),family=binomial)
> >
> >
> > Results look ok, in reasonable agreement with method="PQL", and in good
> > agreement with G?ran's glmmML.
> >
> > Running mcmcsamp() gave strange results. See
> >
> > http://www.menne-biomed.de/uni/mcmc20000.pdf
> > http://www.menne-biomed.de/uni/mcmc2000.pdf

Douglas Bates <bates at ...> writes:

> This is a known issue with the current lmer and you should not trust
> the results.
> 
... removed....

I am in a reviewer-generated deep hole. He (or she) asked for contrasts computed
via glmm, which works well by combining glmmPQL and estimable (from gmodels).
However, (s)he mentioned that glmmPQL could not be valid (Agresti?), because
errors could be underestimated.

Is there currently any way to compute arbitrary contrasts with lmer/glmer other
than the default treatment contrasts in summary()? mcmcsamp cannot be used (see
above), parameter _contrast_ of lrm1 seems to be dead (note that it does not
complain about the following, even if you replace sloap by grump, and I have not
found an example using it):

  lrm1 = lmer(RespB~Treat*Gender(1|Pat),
       family=binomial,data=x,contrast="sloap")

and neither gmodels' estimable nor multcomp do accept glmer results.

Dieter



From francoisetsandrine.mercier at wanadoo.fr  Sun Dec  9 22:22:15 2007
From: francoisetsandrine.mercier at wanadoo.fr (Sandrine-et-Francois)
Date: Sun, 9 Dec 2007 22:22:15 +0100
Subject: [R-sig-ME] Lmer output for negative binomial data
Message-ID: <066701c83aa9$953df9d0$0d01a8c0@Amazone>

Dear R-list,
May I ask for help in interpretating the output of 'lmer' (from the lme4 
package) when dealing with negative binomial data ?

I'm using the functions glm.nb (from the MASS package) and lmer (from the 
lme4) to fit respectively fixed-effects and mixed-effects generalized linear 
models to data, generated from a negative binomial distribution : count ~ 
Neg.Bin (mu, theta). Here is the code:
==============================================================================
#Generate the data frame
set.seed(2153)
mydf<-data.frame(subjs=seq(1:nsubjids),
counts=rnbinom(nsubjids*ntimes, size=0.5, mu=1.8))

#Model
require(MASS); require(lme4)
summary(glm.nb(counts~1, data=mydf))
summary(lmer(counts~1+(1|subjs), 
family=negative.binomial(theta=fixed.nb0$theta), data=mydf))
==============================================================================
The glm.nb output gives : mu=exp(0.5306) and theta=0.513.
I use the theta estimate from glm.nb as input into lmer, and I obtain, 
mu=exp(0.5306).

The output from lmer gives the following for the Random effects:
Random effects:
 Groups   Name        Variance   Std.Dev.
 subjs    (Intercept) 3.5577e-10 1.8862e-05
 Residual             7.1155e-01 8.4353e-01
number of obs: 30, groups: subjs, 10

I interprete the "subjs" component as an individual error term "e" (so, that 
mu=exp(0.5306)*exp(e)) with e~N(0, 3.5577e-10) ? Is this correct ?
What about the 'Residual' term ?

Thanks for your help,
Best regards,
Fran?ois



From saly at ucdavis.edu  Mon Dec 10 03:08:06 2007
From: saly at ucdavis.edu (Sharif S. Aly)
Date: Sun, 09 Dec 2007 18:08:06 -0800
Subject: [R-sig-ME] error regarding levels in grouping factor
Message-ID: <475C9F86.2000300@ucdavis.edu>

Dear R-mixed models list,
After running a mixed model with level 4 and 3 crossed with level 2,
and level 2 nested within level 1:

lmer(y~ 1 + (1|level1/level2)+(1|level3)+(1|level4),data)

I get the following error:

ERROR:
number of levels in grouping factor(s) ?level2

Would someone kindly give me some insight as to what may have caused
this error or how to remedy this problem?

Best regards,
Sharif


From bolker at ufl.edu  Mon Dec 10 13:18:40 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Mon, 10 Dec 2007 07:18:40 -0500
Subject: [R-sig-ME] Lmer output for negative binomial data
In-Reply-To: <066701c83aa9$953df9d0$0d01a8c0@Amazone>
References: <066701c83aa9$953df9d0$0d01a8c0@Amazone>
Message-ID: <475D2EA0.2000303@ufl.edu>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

  I believe it's basically telling you that there's no
additional variation to assign to the within-subject level,
so it's setting the within-subject sd very very small (2 x 10^{-5}).
In a way this is not surprising, since the theta you've estimated
from glm.nb has already accounted for any additional variation
in the data above the sampling (Poisson) variation.

  The statistical model is (I think!)

y ~ neg binom(mu, theta)
mu_{ij} ~ exp(0.5306 + eps_i + eps_ij)

  where eps_i (subject effect) ~ Normal(0,3.6e-10)
        eps_ij (error term) ~ Normal(0,0.715)

  although actually I'm not sure whether the eps_ij
applies to mu or to y ...

  Ben Bolker


Sandrine-et-Francois wrote:
> Dear R-list,
> May I ask for help in interpretating the output of 'lmer' (from the lme4 
> package) when dealing with negative binomial data ?
> 
> I'm using the functions glm.nb (from the MASS package) and lmer (from the 
> lme4) to fit respectively fixed-effects and mixed-effects generalized linear 
> models to data, generated from a negative binomial distribution : count ~ 
> Neg.Bin (mu, theta). Here is the code:
> ==============================================================================
> #Generate the data frame
> set.seed(2153)
> mydf<-data.frame(subjs=seq(1:nsubjids),
> counts=rnbinom(nsubjids*ntimes, size=0.5, mu=1.8))
> 
> #Model
> require(MASS); require(lme4)
> summary(glm.nb(counts~1, data=mydf))
> summary(lmer(counts~1+(1|subjs), 
> family=negative.binomial(theta=fixed.nb0$theta), data=mydf))
> ==============================================================================
> The glm.nb output gives : mu=exp(0.5306) and theta=0.513.
> I use the theta estimate from glm.nb as input into lmer, and I obtain, 
> mu=exp(0.5306).
> 
> The output from lmer gives the following for the Random effects:
> Random effects:
>  Groups   Name        Variance   Std.Dev.
>  subjs    (Intercept) 3.5577e-10 1.8862e-05
>  Residual             7.1155e-01 8.4353e-01
> number of obs: 30, groups: subjs, 10
> 
> I interprete the "subjs" component as an individual error term "e" (so, that 
> mu=exp(0.5306)*exp(e)) with e~N(0, 3.5577e-10) ? Is this correct ?
> What about the 'Residual' term ?
> 
> Thanks for your help,
> Best regards,
> Fran?ois
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.6 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFHXS6gc5UpGjwzenMRAtXdAJ9Zvb1YYH9Ohwrx25h9i+dn16eIRgCbBRSe
pS0ZZovk1rrks/UVCK2jLC4=
=TSEd
-----END PGP SIGNATURE-----



From datkins at fuller.edu  Wed Dec 12 00:26:17 2007
From: datkins at fuller.edu (David Atkins)
Date: Tue, 11 Dec 2007 15:26:17 -0800
Subject: [R-sig-ME] nlmer: gradient for random change point model
Message-ID: <475F1C99.8070603@fuller.edu>


Hi all--

I am attempting to implement a nonlinear mixed-effects model in nlmer() 
that I have gotten to work (well, at least run) in nlme().  However, 
nlmer() requires the gradient function, and I've gotten a bit stuck there.

Here's a bit of background, and some data are attached with script below:

In psychotherapy research we often see two phases of improvement: an 
early rapid phase, and a later slower phase.  Thus, a piecewise linear 
model can be a reasonably good fit, except that the breakpoint is 
different for different folk.  The following article describes how it is 
possible to set up a nonlinear mixed-effects model like this with a 
random effect for the breakpoint:

Cudeck, R., & Klebe, K. J. (2002). Multiphase mixed-effects models for 
repeated measures data. Psychological Methods, 7, 41-63.

A biostatistician colleague of mine has used SAS to fit such a model to 
some weekly psychotherapy data of a mutual colleague.  However, NLMIXED 
didn't particularly like our data, and he had to search over a wide grid 
of initial starting values to ever get it to converge.

Given this and Doug's recent efforts with the lme4 library, I thought it 
might be worthwhile to try fitting the model using nlmer().  I've got it 
running in nlme(), though it often bonks and gives error msgs.  At the 
moment, I haven't been successful in getting it to run in nlmer().  This 
  appears to be due to the fact that nlmer() wants a gradient explicitly 
provided (whereas nlme() doesn't), and deriv() doesn't recognize pmax() 
that we use in the nonlinear formula.

The data attached are a small subset of the actual data, which are quite 
a bit larger (241 individuals) and more "messy" (i.e., some mostly 
linear, some rather chaotic growth patterns), which no doubt lead to 
some of the difficulty in model fitting.  Finally, this is my first 
foray into nonlinear mixed-effects models, though I regularly use linear 
and generalized linear mixed-effects models.  Any general pointers or 
obvious "user errors" appreciated.

Thanks in advance.

cheers, Dave
-- 
Dave Atkins, PhD
Associate Professor in Clinical Psychology
Fuller Graduate School of Psychology
Email: datkins at fuller.edu
Phone: 626.584.5554

 > sessionInfo()
R version 2.6.1 (2007-11-26)
i386-pc-mingw32

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United 
States.1252;LC_MONETARY=English_United 
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices datasets  utils     methods   base

other attached packages:
[1] nlme_3.1-86       Matrix_0.999375-3 lattice_0.17-2

loaded via a namespace (and not attached):
[1] gdata_2.3.1     grid_2.6.1      gtools_2.4.0    lme4_0.999375-0 
tools_2.6.1


load("randomChangePoint.Rdata")

### get a picture of the data
library(lattice)
xyplot(dv ~ weeks | id, data = small.df, type = c("g","p","smooth"),
       pch = 16)

### fit simple linear model with "forced" break at 6 weeks
test.lm <- lm(dv ~ weeks + pmax(0, (weeks - 6)),
                        data = small.df)
summary(test.lm)

### fit nonlinear model
test.nls <- nls(dv ~ b0 + b1*weeks + b2*pmax(0, (weeks - brk)),
                           data = small.df,
                           start = c(b0 = 28, b1 = -4.1, b2 = 4.1, brk = 6))
summary(test.nls)

### use nlme() to fit nonlinear mixed-effects model, though keep it to
### diagonal random-effects for now
library(nlme)

test.nlme <- nlme(dv ~ b0 + b1*weeks + b2*pmax(0, (weeks - brk)),
                       data = small.df,
                       fixed = b0 + b1 + b2 + brk ~ 1,
                       random = pdDiag(b0 + b1 + b2 + brk ~ 1),
                       groups = ~ id,
                       control = list(msVerbose = TRUE, niterEM = 200,
                                     opt = "nlminb", tol = 1e-5),
                       start = coef(test.nls))
summary(test.nlme)

### swap packages
detach("package:nlme")

library(lme4)

test.nlmer <- nlmer(dv ~ b0 + b1*weeks + b2*pmax(0, (weeks - brk))
                         ~  (b0 | id),
                         data = small.df,
                         verbose = TRUE,
                         start = coef(test.nls))
### gives error about gradient, though deriv() doesn't like pmax()


From datkins at fuller.edu  Wed Dec 12 00:34:18 2007
From: datkins at fuller.edu (David Atkins)
Date: Tue, 11 Dec 2007 15:34:18 -0800
Subject: [R-sig-ME] [with data] nlmer: gradient for random change point model
Message-ID: <475F1E7A.2090805@fuller.edu>


looks like the attachment didn't come through, so data can be found:

http://www.fuller.edu/sop/faculty/datkins/cp_content/randomChangePoint.Rdata


-------- Original Message --------
Subject: nlmer: gradient for random change point model
Date: Tue, 11 Dec 2007 15:26:17 -0800
From: David Atkins <datkins at fuller.edu>
To: r-sig-mixed-models at r-project.org


Hi all--

I am attempting to implement a nonlinear mixed-effects model in nlmer()
that I have gotten to work (well, at least run) in nlme().  However,
nlmer() requires the gradient function, and I've gotten a bit stuck there.

Here's a bit of background, and some data are attached with script below:

In psychotherapy research we often see two phases of improvement: an
early rapid phase, and a later slower phase.  Thus, a piecewise linear
model can be a reasonably good fit, except that the breakpoint is
different for different folk.  The following article describes how it is
possible to set up a nonlinear mixed-effects model like this with a
random effect for the breakpoint:

Cudeck, R., & Klebe, K. J. (2002). Multiphase mixed-effects models for
repeated measures data. Psychological Methods, 7, 41-63.

A biostatistician colleague of mine has used SAS to fit such a model to
some weekly psychotherapy data of a mutual colleague.  However, NLMIXED
didn't particularly like our data, and he had to search over a wide grid
of initial starting values to ever get it to converge.

Given this and Doug's recent efforts with the lme4 library, I thought it
might be worthwhile to try fitting the model using nlmer().  I've got it
running in nlme(), though it often bonks and gives error msgs.  At the
moment, I haven't been successful in getting it to run in nlmer().  This
   appears to be due to the fact that nlmer() wants a gradient explicitly
provided (whereas nlme() doesn't), and deriv() doesn't recognize pmax()
that we use in the nonlinear formula.

The data attached are a small subset of the actual data, which are quite
a bit larger (241 individuals) and more "messy" (i.e., some mostly
linear, some rather chaotic growth patterns), which no doubt lead to
some of the difficulty in model fitting.  Finally, this is my first
foray into nonlinear mixed-effects models, though I regularly use linear
and generalized linear mixed-effects models.  Any general pointers or
obvious "user errors" appreciated.

Thanks in advance.

cheers, Dave
-- 
Dave Atkins, PhD
Associate Professor in Clinical Psychology
Fuller Graduate School of Psychology
Email: datkins at fuller.edu
Phone: 626.584.5554

  > sessionInfo()
R version 2.6.1 (2007-11-26)
i386-pc-mingw32

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
States.1252;LC_MONETARY=English_United
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices datasets  utils     methods   base

other attached packages:
[1] nlme_3.1-86       Matrix_0.999375-3 lattice_0.17-2

loaded via a namespace (and not attached):
[1] gdata_2.3.1     grid_2.6.1      gtools_2.4.0    lme4_0.999375-0
tools_2.6.1


load("randomChangePoint.Rdata")

### get a picture of the data
library(lattice)
xyplot(dv ~ weeks | id, data = small.df, type = c("g","p","smooth"),
        pch = 16)

### fit simple linear model with "forced" break at 6 weeks
test.lm <- lm(dv ~ weeks + pmax(0, (weeks - 6)),
                         data = small.df)
summary(test.lm)

### fit nonlinear model
test.nls <- nls(dv ~ b0 + b1*weeks + b2*pmax(0, (weeks - brk)),
                            data = small.df,
                            start = c(b0 = 28, b1 = -4.1, b2 = 4.1, brk 
= 6))
summary(test.nls)

### use nlme() to fit nonlinear mixed-effects model, though keep it to
### diagonal random-effects for now
library(nlme)

test.nlme <- nlme(dv ~ b0 + b1*weeks + b2*pmax(0, (weeks - brk)),
                        data = small.df,
                        fixed = b0 + b1 + b2 + brk ~ 1,
                        random = pdDiag(b0 + b1 + b2 + brk ~ 1),
                        groups = ~ id,
                        control = list(msVerbose = TRUE, niterEM = 200,
                                      opt = "nlminb", tol = 1e-5),
                        start = coef(test.nls))
summary(test.nlme)

### swap packages
detach("package:nlme")

library(lme4)

test.nlmer <- nlmer(dv ~ b0 + b1*weeks + b2*pmax(0, (weeks - brk))
                          ~  (b0 | id),
                          data = small.df,
                          verbose = TRUE,
                          start = coef(test.nls))
### gives error about gradient, though deriv() doesn't like pmax()



-- 
Dave Atkins, PhD
Associate Professor in Clinical Psychology
Fuller Graduate School of Psychology
Email: datkins at fuller.edu
Phone: 626.584.5554



From kjbeath at kagi.com  Wed Dec 12 11:54:48 2007
From: kjbeath at kagi.com (Ken Beath)
Date: Wed, 12 Dec 2007 21:54:48 +1100
Subject: [R-sig-ME] nlmer: gradient for random change point model
In-Reply-To: <475F1C99.8070603@fuller.edu>
References: <475F1C99.8070603@fuller.edu>
Message-ID: <5DA5C21C-47A0-4366-B749-A125634283A3@kagi.com>

On 12/12/2007, at 10:26 AM, David Atkins wrote:

>
> Hi all--
>
> I am attempting to implement a nonlinear mixed-effects model in  
> nlmer() that I have gotten to work (well, at least run) in nlme().   
> However, nlmer() requires the gradient function, and I've gotten a  
> bit stuck there.
>
> Here's a bit of background, and some data are attached with script  
> below:
>
> In psychotherapy research we often see two phases of improvement: an  
> early rapid phase, and a later slower phase.  Thus, a piecewise  
> linear model can be a reasonably good fit, except that the  
> breakpoint is different for different folk.  The following article  
> describes how it is possible to set up a nonlinear mixed-effects  
> model like this with a random effect for the breakpoint:
>
> Cudeck, R., & Klebe, K. J. (2002). Multiphase mixed-effects models  
> for repeated measures data. Psychological Methods, 7, 41-63.


I expect that all of these (I remember reading it for nlme) require  
that the function have continuous derivatives (probably twice) in the  
parameters, which it wont be with the type of breakpoint you are  
using. So one of their other methods might work. Or see http://www.biostat.wustl.edu/archives/html/s-news/2000-04/msg00209.html 
  for methods for nls that will probably work with nlme as well.

>
>
> A biostatistician colleague of mine has used SAS to fit such a model  
> to some weekly psychotherapy data of a mutual colleague.  However,  
> NLMIXED didn't particularly like our data, and he had to search over  
> a wide grid of initial starting values to ever get it to converge.
>


One worry is that it hasn't converged to the correct values when this  
happens.

Ken



From kjbeath at kagi.com  Thu Dec 13 05:32:09 2007
From: kjbeath at kagi.com (Ken Beath)
Date: Thu, 13 Dec 2007 15:32:09 +1100
Subject: [R-sig-ME] nlmer: gradient for random change point model
In-Reply-To: <47600115.90703@fuller.edu>
References: <475F1C99.8070603@fuller.edu>
	<5DA5C21C-47A0-4366-B749-A125634283A3@kagi.com>
	<47600115.90703@fuller.edu>
Message-ID: <E2A63749-2734-405D-A6DE-4554DC6CC81B@kagi.com>

On 13/12/2007, at 2:41 AM, David Atkins wrote:

>
> Ken--
>
> Many thanks for the reply and the pointer to Mary Lindstrom's post.  
> However, I can't seem to get her function to work with nls().  I  
> assume that the underscores in her function should be replaced with  
> assignments (ie, <-)?
>
> My slightly modified version of her function is below, and data  
> attached.  Any thoughts?
>
> cheers, Dave
>
> # http://www.biostat.wustl.edu/archives/html/s-news/2000-04/msg00209.html
> #
> # It is also possible to fit a linear break point model where the  
> break
> # point is a true parameter in the model The model function below will
> # work in nls.  It includes a very short quadratic piece where the  
> break
> # point is to keep the derivative w.r.t. the break point  
> continuous.  -
> # Mary Lindstrom
>
> hockey <- function(x, alpha1, beta1, beta2, brk, eps = range(x)/100) {

should be eps = diff(range(x)/100)

>
>       ## alpha1 is the intercept of the left line segment
>       ## beta1 is the slope of the left line segment
>       ## beta2 is the slope of the right line segment
>       ## brk is location of the break point
>       ## 2*eps is the length of the connecting quadratic piece
>
>       ## reference: Bacon & Watts "Estimating the Transition Between
>       ## Two Intersecting Straight Lines", Biometrika, 1971
>
>        x1 <- brk - eps
>        x2 <- brk + eps
>        b <- (x2*beta1 - x1*beta2)/(x2 - x1)
>        cc <- (beta2 - b)/(2*x2)
>        a <- alpha1 + beta1*x1 - b*x1 - cc*x1^2
>        alpha2 <- - beta2*x2 + (a + b*x2 + cc*x2^2)
>
>        lebrk <- (x <= brk - eps)
>        gebrk <- (x >= brk + eps)
>        eqbrk <- (x > brk - eps & x < brk + eps)
>
>        result <- rep(0,length(x))
>        result[lebrk] <- alpha1 + beta1*x[lebrk]
>        result[eqbrk] <- a + b*x[eqbrk] + cc*x[eqbrk]^2
>        result[gebrk] <- alpha2 + beta2*x[gebrk]
>        result
> }
>
> test.nls <- nls(dv ~ hockey(weeks, alpha1, beta1, beta2, brk),
>                          data = small.df)
>
> Dave Atkins, PhD
> Associate Professor in Clinical Psychology
> Fuller Graduate School of Psychology
> Email: datkins at fuller.edu
> Phone: 626.584.5554
>

The zero derivatives are due to the interval around the breakpoint not  
containing any data, I suggest placing the initial changepoint in the  
centre of the data and then increasing eps until it works.

I expect things will get even worse with a mixed model so it would be  
worthwhile plotting observed versus predicted for each subject to make  
sure the breakpoints are in sensible places. I think this may be a  
difficult model to fit.

Ken



From kjbeath at kagi.com  Thu Dec 13 11:29:06 2007
From: kjbeath at kagi.com (Ken Beath)
Date: Thu, 13 Dec 2007 21:29:06 +1100
Subject: [R-sig-ME] nlmer: gradient for random change point model
In-Reply-To: <E2A63749-2734-405D-A6DE-4554DC6CC81B@kagi.com>
References: <475F1C99.8070603@fuller.edu>
	<5DA5C21C-47A0-4366-B749-A125634283A3@kagi.com>
	<47600115.90703@fuller.edu>
	<E2A63749-2734-405D-A6DE-4554DC6CC81B@kagi.com>
Message-ID: <627E6FF1-8471-4BE9-AD88-7353D173D051@kagi.com>

On 13/12/2007, at 3:32 PM, Ken Beath wrote:

>
> The zero derivatives are due to the interval around the breakpoint not
> containing any data, I suggest placing the initial changepoint in the
> centre of the data and then increasing eps until it works.
>
>

This is not correct, reason is that starting values for both slopes  
must be different, otherwise changing the breakpoint location doesn't  
change anything. It also helps to get the starting values close to the  
true values.
Following works fine

test.nls <- nls(dv ~ hockey(weeks, alpha1, beta1, beta2, brk),
						start= list(alpha1=30,beta1=-6,beta2=0,brk=5),
						trace=TRUE,
                        data = small.df)

Ken



From vmuggeo at dssm.unipa.it  Thu Dec 13 13:35:34 2007
From: vmuggeo at dssm.unipa.it (vito muggeo)
Date: Thu, 13 Dec 2007 13:35:34 +0100
Subject: [R-sig-ME] nlmer: gradient for random change point model
In-Reply-To: <E2A63749-2734-405D-A6DE-4554DC6CC81B@kagi.com>
References: <475F1C99.8070603@fuller.edu>	<5DA5C21C-47A0-4366-B749-A125634283A3@kagi.com>	<47600115.90703@fuller.edu>
	<E2A63749-2734-405D-A6DE-4554DC6CC81B@kagi.com>
Message-ID: <47612716.6010908@dssm.unipa.it>

A completely (simpler and naive) approach could be to use an algorithm 
which works reasonably well in non-mixed GLM (see the package segmented).

The algorithm works via linear approximations of the kink function, 
therefore extension to mixed models is, at least in principle, 
straightforward. I discuss such an approach in a paper published in the 
Proceedings of the 19th IWSM, and I have (very) raw R code implementing 
the idea.

Let me know off-list if you are interested in the paper and/or code,

best,
vito


estimate the cluster-specific parameters (includ

Ken Beath ha scritto:
> On 13/12/2007, at 2:41 AM, David Atkins wrote:
> 
>> Ken--
>>
>> Many thanks for the reply and the pointer to Mary Lindstrom's post.  
>> However, I can't seem to get her function to work with nls().  I  
>> assume that the underscores in her function should be replaced with  
>> assignments (ie, <-)?
>>
>> My slightly modified version of her function is below, and data  
>> attached.  Any thoughts?
>>
>> cheers, Dave
>>
>> # http://www.biostat.wustl.edu/archives/html/s-news/2000-04/msg00209.html
>> #
>> # It is also possible to fit a linear break point model where the  
>> break
>> # point is a true parameter in the model The model function below will
>> # work in nls.  It includes a very short quadratic piece where the  
>> break
>> # point is to keep the derivative w.r.t. the break point  
>> continuous.  -
>> # Mary Lindstrom
>>
>> hockey <- function(x, alpha1, beta1, beta2, brk, eps = range(x)/100) {
> 
> should be eps = diff(range(x)/100)
> 
>>       ## alpha1 is the intercept of the left line segment
>>       ## beta1 is the slope of the left line segment
>>       ## beta2 is the slope of the right line segment
>>       ## brk is location of the break point
>>       ## 2*eps is the length of the connecting quadratic piece
>>
>>       ## reference: Bacon & Watts "Estimating the Transition Between
>>       ## Two Intersecting Straight Lines", Biometrika, 1971
>>
>>        x1 <- brk - eps
>>        x2 <- brk + eps
>>        b <- (x2*beta1 - x1*beta2)/(x2 - x1)
>>        cc <- (beta2 - b)/(2*x2)
>>        a <- alpha1 + beta1*x1 - b*x1 - cc*x1^2
>>        alpha2 <- - beta2*x2 + (a + b*x2 + cc*x2^2)
>>
>>        lebrk <- (x <= brk - eps)
>>        gebrk <- (x >= brk + eps)
>>        eqbrk <- (x > brk - eps & x < brk + eps)
>>
>>        result <- rep(0,length(x))
>>        result[lebrk] <- alpha1 + beta1*x[lebrk]
>>        result[eqbrk] <- a + b*x[eqbrk] + cc*x[eqbrk]^2
>>        result[gebrk] <- alpha2 + beta2*x[gebrk]
>>        result
>> }
>>
>> test.nls <- nls(dv ~ hockey(weeks, alpha1, beta1, beta2, brk),
>>                          data = small.df)
>>
>> Dave Atkins, PhD
>> Associate Professor in Clinical Psychology
>> Fuller Graduate School of Psychology
>> Email: datkins at fuller.edu
>> Phone: 626.584.5554
>>
> 
> The zero derivatives are due to the interval around the breakpoint not  
> containing any data, I suggest placing the initial changepoint in the  
> centre of the data and then increasing eps until it works.
> 
> I expect things will get even worse with a mixed model so it would be  
> worthwhile plotting observed versus predicted for each subject to make  
> sure the breakpoints are in sensible places. I think this may be a  
> difficult model to fit.
> 
> Ken
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 

-- 
====================================
Vito M.R. Muggeo
Dip.to Sc Statist e Matem `Vianelli'
Universit? di Palermo
viale delle Scienze, edificio 13
90128 Palermo - ITALY
tel: 091 6626240
fax: 091 485726/485612



From baron at psych.upenn.edu  Thu Dec 13 18:45:37 2007
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Thu, 13 Dec 2007 12:45:37 -0500
Subject: [R-sig-ME] p values for factor effect in lmer
Message-ID: <20071213174537.GA789@psych.upenn.edu>

Suppose I have something like

l1 <- lmer(y ~ x + f + (1 | s))

where f is a factor with levels 1, 2, 3.

l1 itself lists the individual values, f1, f2, f3, not the overall
effect of f.

anova(l1) gives the overall effect of f, but no p values.

pvals.fnc(l1) from the languageR package is like l1 itself.  But I
want to know whether the factor has a significant effect.

I could do

l2 <- lmer(y ~ x + (1 | s))
anova(l1,l2)

but, in my experience so far, this method gives answers that don't
look right and don't agree with other methods.  I have not been
trusting it to provide p values.  pvals.fnc(), by contrast, usually
gives very sensible values that agree with other approaches.

Can anyone suggest another approach?  ("Wait until the next version of
..." is a legitimate answer, if it is true.)

Or maybe I should trust anova(l1,l2) after all.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From Aurelie.Lemmens at econ.kuleuven.be  Tue Dec 18 15:42:54 2007
From: Aurelie.Lemmens at econ.kuleuven.be (Lemmens, Aurelie)
Date: Tue, 18 Dec 2007 15:42:54 +0100
Subject: [R-sig-ME] specifying the structure of the residual
	variance-covariance matrix
Message-ID: <F9E424F22569F3469181E1BA1EE24CED0172C401@ECONSRVEX5.econ.kuleuven.ac.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20071218/48f6c98b/attachment.pl>

From bates at stat.wisc.edu  Thu Dec 20 00:10:51 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 19 Dec 2007 17:10:51 -0600
Subject: [R-sig-ME] Development version of the lme4 package on the R-forge
	web site
Message-ID: <40e66e0b0712191510m195f1da9o46dffb1d244257bd@mail.gmail.com>

I have been struggling for a long time with a part of the code for
generalized linear mixed models in the new formulation for lmer
objects.  This has taken much longer than I ever anticipated it would
but I think I have finally gotten past this log jam.  I believe the
development version now on R-forge is producing the proper estimates
of the parameters.

The current version is incomplete. To get the estimates of variance
components you must use the optional argument verbose = TRUE and be
able to decipher the values of the parameters being optimized. Also,
the standard errors of the fixed effects are not currently being
calculated - they are arbitrarily set to 1.

All of these "infelicities" are straightforward to fix and I will do
so after I finish grading exams.  Right now I would appreciate it if a
few brave testers could try out some model fits using  this version
and tell me if the answers are consistent with those from earlier
versions of lme4 or from other software to which you may have access.

Here is a sample run

> example(cbpp)

cbpp> ## response as a matrix
cbpp> (m1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),
cbpp+             cbpp, binomial, verbose = TRUE))
  0:     101.94202: 0.845154 -1.26902 -1.17076 -1.30141 -1.78228
  1:     100.53027: 0.673606 -1.31464 -1.12352 -1.26457 -1.75616
  2:     100.30572: 0.574054 -1.37591 -1.02938 -1.17669 -1.68180
  3:     100.19154: 0.686359 -1.39110 -0.949652 -1.09270 -1.58420
  4:     100.11118: 0.651201 -1.40779 -0.962929 -1.10154 -1.58146
  5:     100.09935: 0.637395 -1.40425 -0.990891 -1.12134 -1.56147
  6:     100.09784: 0.647778 -1.39980 -0.988828 -1.12366 -1.56676
  7:     100.09623: 0.640796 -1.39918 -0.987904 -1.12854 -1.57632
  8:     100.09598: 0.643683 -1.39963 -0.990595 -1.12789 -1.57733
  9:     100.09598: 0.640981 -1.39885 -0.992325 -1.12640 -1.57935
 10:     100.09588: 0.642623 -1.39874 -0.992076 -1.12758 -1.57972
 11:     100.09587: 0.642176 -1.39887 -0.992048 -1.12803 -1.57981
 12:     100.09586: 0.642561 -1.39857 -0.992080 -1.12844 -1.57994
 13:     100.09586: 0.642118 -1.39821 -0.992232 -1.12857 -1.58018
 14:     100.09586: 0.642238 -1.39849 -0.992317 -1.12864 -1.58022
 15:     100.09586: 0.642248 -1.39853 -0.992324 -1.12866 -1.58032
 16:     100.09586: 0.642264 -1.39853 -0.992327 -1.12866 -1.58032
 17:     100.09586: 0.642260 -1.39854 -0.992335 -1.12868 -1.58031
Generalized linear mixed model fit using Laplace
Formula: cbind(incidence, size - incidence) ~ period + (1 | herd)
   Data: cbpp
 Family: binomial(logit link)
   AIC   BIC logLik deviance
 110.1 120.2 -50.05    100.1
Random effects:
 Groups   Name        Variance  Std.Dev.
 herd     (Intercept) 0.0079327 0.089066
 Residual             0.0192308 0.138675
Number of obs: 56, groups: herd, 15

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)  -1.3985     1.0000 -1.3985    0.162
period2      -0.9923     1.0000 -0.9923    0.321
period3      -1.1287     1.0000 -1.1287    0.259
period4      -1.5804     1.0000 -1.5804    0.114

Correlation of Fixed Effects:
        (Intr) perid2 perid3
period2 0.000
period3 0.000  0.000
period4 0.000  0.000  0.000

Don't pay attention to the reported estimates of the variance
components.  The important numbers are in the verbose output.  The
first number is the Laplace approximation to the deviance (negative
twice the log-likelihoood) and the second is the relative standard
deviation of the random effects.  For a binomial model there is no
separate scale factor in the variance of the responses so the relative
standard deviation is the standard deviation (in other words, it is
"relative to unity").  The next four numbers are, as you can see, the
fixed effects.

If you do have a chance to run a test I would appreciate learning of
the results.  The source package on R-forge should be updated tonight.
 I'm not sure when the Windows binary version is updated.  If you
install the development version of the package please try to reproduce
the example shown above before you run your own tests.

This version will fit GLMMs for the binomial family with logit or
probit links and for the Poisson family.  Other families will be added
as needed.  General capabilities for using a user-defined family will
take longer.



