From d@|uedecke @end|ng |rom uke@de  Mon Apr  4 12:26:00 2022
From: d@|uedecke @end|ng |rom uke@de (=?utf-8?Q?Daniel_L=C3=BCdecke?=)
Date: Mon, 4 Apr 2022 12:26:00 +0200
Subject: [R-sig-ME] 
 Representation model problem with standardized variables
In-Reply-To: <668477118.139932.1648640020214@mail.yahoo.com>
References: <668477118.139932.1648640020214.ref@mail.yahoo.com>
 <668477118.139932.1648640020214@mail.yahoo.com>
Message-ID: <006701d8480e$618c8250$24a586f0$@uke.de>

Dear Alexandre,
this one might solve your issue. I use the plot() method from ggeffects, but you can of course use ggplot2 as well, to build your own plot from scratch. The basic idea is to map the values of the range from the unstandardized variable with those from the standardized one, and then use "scale_x_...()" from ggplot to change the labels. At the bottom, you find the two plots when you use the unstandardized data in your model, and you can see, that the axis labels (range) represents the original scale.

Best
Daniel

library(datawizard)
library(lme4)
library(ggeffects)
library(ggplot2)

myds <- read.csv("https://raw.githubusercontent.com/Leprechault/trash/main/ds.desenvol.csv")
d.scale <- standardize(myds, select = c("temp", "storage"))
m_6 <- glmer.nb(development ~ poly(temp,2) + poly(storage,2) + (1 | storage ), data = d.scale)


# for temp
mydf <- ggpredict(m_6, terms = "temp [all]")

# retrieve center and scale from standardization
center_temp <- attributes(d.scale)$center["temp"]
scale_temp <- attributes(d.scale)$scale["temp"]

# scaled range, calculate back to range of unstandardized
scaled_range <- c(-1, 0, 1, 2)
new_range <- round(scaled_range * scale_temp + center_temp)

# scaled range
plot(mydf, add.data = TRUE)

# original range
plot(mydf, add.data = TRUE) + 
  scale_x_continuous(
    breaks = scaled_range, 
    labels = new_range
  )


# for storage
mydf <- ggpredict(m_6, terms = "storage [all]")

# retrieve center and scale from standardization
center_storage <- attributes(d.scale)$center["storage"]
scale_storage <- attributes(d.scale)$scale["storage"]

# scaled range, calculate back to range of unstandardized
scaled_range <- c(-1, 0, 1)
new_range <- round(scaled_range * scale_storage + center_storage)

# scaled range
plot(mydf, add.data = TRUE)

# original range
plot(mydf, add.data = TRUE) + 
  scale_x_continuous(
    breaks = scaled_range, 
    labels = new_range
  )


# compare to plots w/o standardization
m_7 <- glmer.nb(development ~ poly(temp,2) + poly(storage,2) + (1 | storage ), data = myds)
ggpredict(m_7, terms = "temp [all]") |> plot(add.data = TRUE)
ggpredict(m_7, terms = "storage [all]") |> plot(add.data = TRUE)

-----Urspr?ngliche Nachricht-----
Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im Auftrag von Alexandre Santos via R-sig-mixed-models
Gesendet: Mittwoch, 30. M?rz 2022 13:34
An: r-sig-mixed-models at r-project.org
Betreff: [R-sig-ME] Representation model problem with standardized variables

Hi Everyone!!

I standardized my input variables (ds.scale) before glmm adjustments but in the final plot, I have a problem with the real-world scale of my variables and the predicted values by model (m_6). I?d like the original scale of my temp and storage variables represented in my better model (m_6). What is the correct approach for this? Do not standardise my input variables, despite I lot of warmings? Some data transformation at the end? I make:

#Packages
library(lme4)
library(ggplot2)
library(ggeffects)
library(tidyverse)
library(bbmle) 
library(broom)

#Open my dataset
myds<-read.csv("https://raw.githubusercontent.com/Leprechault/trash/main/ds.desenvol.csv")
str(myds)
# 'data.frame': 400 obs. of  4 variables:
#  $ temp       : num  0 0 0 0 0 0 0 0 0 0 ...
#  $ storage    : int  5 5 5 5 5 5 5 5 5 5 ...
#  $ rep        : chr  "r1" "r2" "r3" "r4" ...
#  $ development: int  0 23 22 27 24 25 24 22 0 22 ...

# Storage (days) is temporally correlated with temperature then mixed model
ds.scale<- myds %>%
  mutate(across(c(temp, storage), ~ drop(scale(.))))

# Models creation Poisson/Negative binomial
m_1 <- glmer(development ~ temp + storage +
               (1 | storage ), data = ds.scale, 
                 family = "poisson")
m_2 <- glmer(development ~ poly(temp,2) + storage +
               (1 | storage ), data = ds.scale, 
                 family = "poisson")  
m_3 <- glmer(development ~ poly(temp,2) + poly(storage,2) +
               (1 | storage ), data = ds.scale, 
                 family = "poisson")  
m_4 <- glmer.nb(development ~ temp + storage +
               (1 | storage ), data = ds.scale)
m_5 <- glmer.nb(development ~ poly(temp,2) + storage +
               (1 | storage ), data = ds.scale)  
m_6 <- glmer.nb(development ~ poly(temp,2) + poly(storage,2) +
               (1 | storage ), data = ds.scale)   
modList <- tibble::lst(m_1,m_2,m_3,m_4,m_5,m_6)
bbmle::AICtab(modList)

#     dAIC df
# m_6  0.0 7 
# m_3  1.0 6 
# m_5  3.3 6 
# m_2  5.0 5 
# m_4 17.9 5 
# m_1 21.0 4

# Plot the results for my better model (m_6)
mydf <- ggpredict(m_6, terms = c("temp [all]", "storage[all]"))

# For temp
ggplot(mydf, aes(x, predicted)) +
  geom_point(data=myds, aes(temp, development), alpha = 0.5) + 
  geom_line() +
  labs(x = "temp", y = "development")

# For storage
ggplot(mydf, aes(x, predicted)) +
  geom_point(data=myds, aes(storage, development), alpha = 0.5) + 
  geom_line() +
  labs(x = "storage", y = "development")
# -------------------------------------------------------------------------------------------  


Please, any help with it?
--
Alexandre dos Santos
Geotechnologies and Spatial Statistics applied to Forest Entomology

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Joachim Pr?l?, Prof. Dr. Blanche Schwappach-Pignataro, Marya Verdel
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING

From @|ex@ndre@@nto@br @end|ng |rom y@hoo@com@br  Mon Apr  4 13:57:35 2022
From: @|ex@ndre@@nto@br @end|ng |rom y@hoo@com@br (Alexandre Santos)
Date: Mon, 4 Apr 2022 11:57:35 +0000 (UTC)
Subject: [R-sig-ME] 
 Representation model problem with standardized variables
In-Reply-To: <006701d8480e$618c8250$24a586f0$@uke.de>
References: <668477118.139932.1648640020214.ref@mail.yahoo.com>
 <668477118.139932.1648640020214@mail.yahoo.com>
 <006701d8480e$618c8250$24a586f0$@uke.de>
Message-ID: <762822913.1740514.1649073455557@mail.yahoo.com>

Dear Daniel,

Thank you so much and my problem was solved.

The hard to me is to retrieve the center and scale from standardization and after?calculating back, but now with your answer looks easy!

Best wishes,

Alexandre


?--Alexandre dos Santos
Geotechnologies and Spatial Statistics applied to Forest Entomology
Instituto Federal de Mato Grosso (IFMT) - Campus Caceres
Caixa Postal 244 (PO Box)
Avenida dos Ramires, s/n - Vila RealCaceres - MT - CEP 78201-380 (ZIP code)
Phone: (+55) 65 99686-6970 / (+55) 65 3221-2674
Lattes CV:?http://lattes.cnpq.br/1360403201088680
OrcID:?orcid.org/0000-0001-8232-6722
ResearchGate:?http://www.researchgate.net/profile/Alexandre_Santos10
Publons:?https://publons.com/researcher/3085587/alexandre-dos-santos/--






Em segunda-feira, 4 de abril de 2022 06:26:04 AMT, Daniel L?decke <d.luedecke at uke.de> escreveu: 





Dear Alexandre,
this one might solve your issue. I use the plot() method from ggeffects, but you can of course use ggplot2 as well, to build your own plot from scratch. The basic idea is to map the values of the range from the unstandardized variable with those from the standardized one, and then use "scale_x_...()" from ggplot to change the labels. At the bottom, you find the two plots when you use the unstandardized data in your model, and you can see, that the axis labels (range) represents the original scale.

Best
Daniel

library(datawizard)
library(lme4)
library(ggeffects)
library(ggplot2)

myds <- read.csv("https://raw.githubusercontent.com/Leprechault/trash/main/ds.desenvol.csv")
d.scale <- standardize(myds, select = c("temp", "storage"))
m_6 <- glmer.nb(development ~ poly(temp,2) + poly(storage,2) + (1 | storage ), data = d.scale)


# for temp
mydf <- ggpredict(m_6, terms = "temp [all]")

# retrieve center and scale from standardization
center_temp <- attributes(d.scale)$center["temp"]
scale_temp <- attributes(d.scale)$scale["temp"]

# scaled range, calculate back to range of unstandardized
scaled_range <- c(-1, 0, 1, 2)
new_range <- round(scaled_range * scale_temp + center_temp)

# scaled range
plot(mydf, add.data = TRUE)

# original range
plot(mydf, add.data = TRUE) + 
? scale_x_continuous(
? ? breaks = scaled_range, 
? ? labels = new_range
? )


# for storage
mydf <- ggpredict(m_6, terms = "storage [all]")

# retrieve center and scale from standardization
center_storage <- attributes(d.scale)$center["storage"]
scale_storage <- attributes(d.scale)$scale["storage"]

# scaled range, calculate back to range of unstandardized
scaled_range <- c(-1, 0, 1)
new_range <- round(scaled_range * scale_storage + center_storage)

# scaled range
plot(mydf, add.data = TRUE)

# original range
plot(mydf, add.data = TRUE) + 
? scale_x_continuous(
? ? breaks = scaled_range, 
? ? labels = new_range
? )


# compare to plots w/o standardization
m_7 <- glmer.nb(development ~ poly(temp,2) + poly(storage,2) + (1 | storage ), data = myds)
ggpredict(m_7, terms = "temp [all]") |> plot(add.data = TRUE)
ggpredict(m_7, terms = "storage [all]") |> plot(add.data = TRUE)

-----Urspr?ngliche Nachricht-----
Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im Auftrag von Alexandre Santos via R-sig-mixed-models
Gesendet: Mittwoch, 30. M?rz 2022 13:34
An: r-sig-mixed-models at r-project.org
Betreff: [R-sig-ME] Representation model problem with standardized variables

Hi Everyone!!

I standardized my input variables (ds.scale) before glmm adjustments but in the final plot, I have a problem with the real-world scale of my variables and the predicted values by model (m_6). I?d like the original scale of my temp and storage variables represented in my better model (m_6). What is the correct approach for this? Do not standardise my input variables, despite I lot of warmings? Some data transformation at the end? I make:

#Packages
library(lme4)
library(ggplot2)
library(ggeffects)
library(tidyverse)
library(bbmle) 
library(broom)

#Open my dataset
myds<-read.csv("https://raw.githubusercontent.com/Leprechault/trash/main/ds.desenvol.csv")
str(myds)
# 'data.frame': 400 obs. of? 4 variables:
#? $ temp? ? ? : num? 0 0 0 0 0 0 0 0 0 0 ...
#? $ storage? ? : int? 5 5 5 5 5 5 5 5 5 5 ...
#? $ rep? ? ? ? : chr? "r1" "r2" "r3" "r4" ...
#? $ development: int? 0 23 22 27 24 25 24 22 0 22 ...

# Storage (days) is temporally correlated with temperature then mixed model
ds.scale<- myds %>%
? mutate(across(c(temp, storage), ~ drop(scale(.))))

# Models creation Poisson/Negative binomial
m_1 <- glmer(development ~ temp + storage +
? ? ? ? ? ? ? (1 | storage ), data = ds.scale, 
? ? ? ? ? ? ? ? family = "poisson")
m_2 <- glmer(development ~ poly(temp,2) + storage +
? ? ? ? ? ? ? (1 | storage ), data = ds.scale, 
? ? ? ? ? ? ? ? family = "poisson")? 
m_3 <- glmer(development ~ poly(temp,2) + poly(storage,2) +
? ? ? ? ? ? ? (1 | storage ), data = ds.scale, 
? ? ? ? ? ? ? ? family = "poisson")? 
m_4 <- glmer.nb(development ~ temp + storage +
? ? ? ? ? ? ? (1 | storage ), data = ds.scale)
m_5 <- glmer.nb(development ~ poly(temp,2) + storage +
? ? ? ? ? ? ? (1 | storage ), data = ds.scale)? 
m_6 <- glmer.nb(development ~ poly(temp,2) + poly(storage,2) +
? ? ? ? ? ? ? (1 | storage ), data = ds.scale)? 
modList <- tibble::lst(m_1,m_2,m_3,m_4,m_5,m_6)
bbmle::AICtab(modList)

#? ? dAIC df
# m_6? 0.0 7 
# m_3? 1.0 6 
# m_5? 3.3 6 
# m_2? 5.0 5 
# m_4 17.9 5 
# m_1 21.0 4

# Plot the results for my better model (m_6)
mydf <- ggpredict(m_6, terms = c("temp [all]", "storage[all]"))

# For temp
ggplot(mydf, aes(x, predicted)) +
? geom_point(data=myds, aes(temp, development), alpha = 0.5) + 
? geom_line() +
? labs(x = "temp", y = "development")

# For storage
ggplot(mydf, aes(x, predicted)) +
? geom_point(data=myds, aes(storage, development), alpha = 0.5) + 
? geom_line() +
? labs(x = "storage", y = "development")
# -------------------------------------------------------------------------------------------? 


Please, any help with it?
--
Alexandre dos Santos
Geotechnologies and Spatial Statistics applied to Forest Entomology


_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Joachim Pr?l?, Prof. Dr. Blanche Schwappach-Pignataro, Marya Verdel
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING


From th|erry@onke||nx @end|ng |rom |nbo@be  Tue Apr  5 10:05:43 2022
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Tue, 5 Apr 2022 10:05:43 +0200
Subject: [R-sig-ME] 
 Representation model problem with standardized variables
In-Reply-To: <762822913.1740514.1649073455557@mail.yahoo.com>
References: <668477118.139932.1648640020214.ref@mail.yahoo.com>
 <668477118.139932.1648640020214@mail.yahoo.com>
 <006701d8480e$618c8250$24a586f0$@uke.de>
 <762822913.1740514.1649073455557@mail.yahoo.com>
Message-ID: <CAJuCY5ybCeGdX0aVxh5QY95m26i=y3HSF+Za7bg-Cjc3+oytoA@mail.gmail.com>

Dear Alexandre,

I prefer to manually scale the variables. Instead of centering to the mean,
I prefer to center to some relevant value within or close to the data. And
instead of dividing by the standard error, I divide by a sensible power of
10. The resulting variable and its coefficient are much easier to interpret.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op ma 4 apr. 2022 om 13:57 schreef Alexandre Santos via R-sig-mixed-models <
r-sig-mixed-models at r-project.org>:

> Dear Daniel,
>
> Thank you so much and my problem was solved.
>
> The hard to me is to retrieve the center and scale from standardization
> and after calculating back, but now with your answer looks easy!
>
> Best wishes,
>
> Alexandre
>
>
>  --Alexandre dos Santos
> Geotechnologies and Spatial Statistics applied to Forest Entomology
> Instituto Federal de Mato Grosso (IFMT) - Campus Caceres
> Caixa Postal 244 (PO Box)
> Avenida dos Ramires, s/n - Vila RealCaceres - MT - CEP 78201-380 (ZIP code)
> Phone: (+55) 65 99686-6970 / (+55) 65 3221-2674
> Lattes CV: http://lattes.cnpq.br/1360403201088680
> OrcID: orcid.org/0000-0001-8232-6722
> ResearchGate: http://www.researchgate.net/profile/Alexandre_Santos10
> Publons: https://publons.com/researcher/3085587/alexandre-dos-santos/--
>
>
>
>
>
>
> Em segunda-feira, 4 de abril de 2022 06:26:04 AMT, Daniel L?decke <
> d.luedecke at uke.de> escreveu:
>
>
>
>
>
> Dear Alexandre,
> this one might solve your issue. I use the plot() method from ggeffects,
> but you can of course use ggplot2 as well, to build your own plot from
> scratch. The basic idea is to map the values of the range from the
> unstandardized variable with those from the standardized one, and then use
> "scale_x_...()" from ggplot to change the labels. At the bottom, you find
> the two plots when you use the unstandardized data in your model, and you
> can see, that the axis labels (range) represents the original scale.
>
> Best
> Daniel
>
> library(datawizard)
> library(lme4)
> library(ggeffects)
> library(ggplot2)
>
> myds <- read.csv("
> https://raw.githubusercontent.com/Leprechault/trash/main/ds.desenvol.csv")
> d.scale <- standardize(myds, select = c("temp", "storage"))
> m_6 <- glmer.nb(development ~ poly(temp,2) + poly(storage,2) + (1 |
> storage ), data = d.scale)
>
>
> # for temp
> mydf <- ggpredict(m_6, terms = "temp [all]")
>
> # retrieve center and scale from standardization
> center_temp <- attributes(d.scale)$center["temp"]
> scale_temp <- attributes(d.scale)$scale["temp"]
>
> # scaled range, calculate back to range of unstandardized
> scaled_range <- c(-1, 0, 1, 2)
> new_range <- round(scaled_range * scale_temp + center_temp)
>
> # scaled range
> plot(mydf, add.data = TRUE)
>
> # original range
> plot(mydf, add.data = TRUE) +
>   scale_x_continuous(
>     breaks = scaled_range,
>     labels = new_range
>   )
>
>
> # for storage
> mydf <- ggpredict(m_6, terms = "storage [all]")
>
> # retrieve center and scale from standardization
> center_storage <- attributes(d.scale)$center["storage"]
> scale_storage <- attributes(d.scale)$scale["storage"]
>
> # scaled range, calculate back to range of unstandardized
> scaled_range <- c(-1, 0, 1)
> new_range <- round(scaled_range * scale_storage + center_storage)
>
> # scaled range
> plot(mydf, add.data = TRUE)
>
> # original range
> plot(mydf, add.data = TRUE) +
>   scale_x_continuous(
>     breaks = scaled_range,
>     labels = new_range
>   )
>
>
> # compare to plots w/o standardization
> m_7 <- glmer.nb(development ~ poly(temp,2) + poly(storage,2) + (1 |
> storage ), data = myds)
> ggpredict(m_7, terms = "temp [all]") |> plot(add.data = TRUE)
> ggpredict(m_7, terms = "storage [all]") |> plot(add.data = TRUE)
>
> -----Urspr?ngliche Nachricht-----
> Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im
> Auftrag von Alexandre Santos via R-sig-mixed-models
> Gesendet: Mittwoch, 30. M?rz 2022 13:34
> An: r-sig-mixed-models at r-project.org
> Betreff: [R-sig-ME] Representation model problem with standardized
> variables
>
> Hi Everyone!!
>
> I standardized my input variables (ds.scale) before glmm adjustments but
> in the final plot, I have a problem with the real-world scale of my
> variables and the predicted values by model (m_6). I?d like the original
> scale of my temp and storage variables represented in my better model
> (m_6). What is the correct approach for this? Do not standardise my input
> variables, despite I lot of warmings? Some data transformation at the end?
> I make:
>
> #Packages
> library(lme4)
> library(ggplot2)
> library(ggeffects)
> library(tidyverse)
> library(bbmle)
> library(broom)
>
> #Open my dataset
> myds<-read.csv("
> https://raw.githubusercontent.com/Leprechault/trash/main/ds.desenvol.csv")
> str(myds)
> # 'data.frame': 400 obs. of  4 variables:
> #  $ temp      : num  0 0 0 0 0 0 0 0 0 0 ...
> #  $ storage    : int  5 5 5 5 5 5 5 5 5 5 ...
> #  $ rep        : chr  "r1" "r2" "r3" "r4" ...
> #  $ development: int  0 23 22 27 24 25 24 22 0 22 ...
>
> # Storage (days) is temporally correlated with temperature then mixed model
> ds.scale<- myds %>%
>   mutate(across(c(temp, storage), ~ drop(scale(.))))
>
> # Models creation Poisson/Negative binomial
> m_1 <- glmer(development ~ temp + storage +
>               (1 | storage ), data = ds.scale,
>                 family = "poisson")
> m_2 <- glmer(development ~ poly(temp,2) + storage +
>               (1 | storage ), data = ds.scale,
>                 family = "poisson")
> m_3 <- glmer(development ~ poly(temp,2) + poly(storage,2) +
>               (1 | storage ), data = ds.scale,
>                 family = "poisson")
> m_4 <- glmer.nb(development ~ temp + storage +
>               (1 | storage ), data = ds.scale)
> m_5 <- glmer.nb(development ~ poly(temp,2) + storage +
>               (1 | storage ), data = ds.scale)
> m_6 <- glmer.nb(development ~ poly(temp,2) + poly(storage,2) +
>               (1 | storage ), data = ds.scale)
> modList <- tibble::lst(m_1,m_2,m_3,m_4,m_5,m_6)
> bbmle::AICtab(modList)
>
> #    dAIC df
> # m_6  0.0 7
> # m_3  1.0 6
> # m_5  3.3 6
> # m_2  5.0 5
> # m_4 17.9 5
> # m_1 21.0 4
>
> # Plot the results for my better model (m_6)
> mydf <- ggpredict(m_6, terms = c("temp [all]", "storage[all]"))
>
> # For temp
> ggplot(mydf, aes(x, predicted)) +
>   geom_point(data=myds, aes(temp, development), alpha = 0.5) +
>   geom_line() +
>   labs(x = "temp", y = "development")
>
> # For storage
> ggplot(mydf, aes(x, predicted)) +
>   geom_point(data=myds, aes(storage, development), alpha = 0.5) +
>   geom_line() +
>   labs(x = "storage", y = "development")
> #
> -------------------------------------------------------------------------------------------
>
>
>
> Please, any help with it?
> --
> Alexandre dos Santos
> Geotechnologies and Spatial Statistics applied to Forest Entomology
>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> --
>
> _____________________________________________________________________
>
> Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen
> Rechts; Gerichtsstand: Hamburg | www.uke.de
> Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Joachim
> Pr?l?, Prof. Dr. Blanche Schwappach-Pignataro, Marya Verdel
> _____________________________________________________________________
>
> SAVE PAPER - THINK BEFORE PRINTING
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From b@t|c @end|ng |rom |eed@@@c@uk  Mon Apr  4 12:19:07 2022
From: b@t|c @end|ng |rom |eed@@@c@uk (Tara Cox [RPG])
Date: Mon, 4 Apr 2022 10:19:07 +0000
Subject: [R-sig-ME] Bivariate MCMCglmm: fixed effect error
Message-ID: <AS8PR03MB684022B27A05F0FD2E113E93F7E29@AS8PR03MB6840.eurprd03.prod.outlook.com>

Dear list,

I am stuck attempting to run a bivariate MCMCglmm that uses personality score and age at first breeding (years) as response variables (both modelled Poisson). I have multiple repeats per individual for personality score, but a single value for age at first breeding. For fixed effects, I have included drivers known to impact personality score (age when tested, age2, colour of room that individual was tested in, and the test number) as well as age at first breeding (natal territory quality and island-wide food availability).

To account for the fact that individuals born more recently in the dataset lived short lives (as per Bouwhuis et al., 2010), I have also included cohort as a fixed effect for age at first breeding.

I've been using the covu method with a parameter expanded prior and model specification outlined below, but I am having issues when trying to run the model with cohort as a fixed effect (error message states: '## Error in MCMCglmm(fitness.stack ~ variable - 1 + at.level(variable, : Mixed model equations singular: use a (stronger) prior'). I have tried setting cohort as both numeric and factor when including it as a fixed effect, but this doesn't make a difference. I know the issue is not with any other part of the model, as it runs successfully when cohort is removed as a fixed effect.


prior.age <- list(G = list(G1 = list(V = diag(1), nu = 1),                            # rand effect for Cohort (fitted for AGE_BREEDING)
                                         G2 = list(V = diag(1), nu = 1)),                           # rand effect for Observer (fitted for Personality)
                           R = list(R1 = list(V = diag(2), nu = 0.002, covu = TRUE),  # 2-way var-cov matrix of ID for Personality, residual for AGE_BREEDING
                                         R2 = list(V = diag(1), nu = 0.002)))                       # residual for Personality


m1  <-   MCMCglmm(fitness.stack ~ variable - 1 +
                            at.level(variable, "Personality"):Personality_Age +
                            at.level(variable, "Personality"):Personality_Age2 +
                            at.level(variable, "Personality"):Personality_RoomColour +
                            at.level(variable, "Personality"):TestNumber +
                            at.level(variable, "AGE_BREEDING"):logNatalTerritoryQuality +
                            at.level(variable, "AGE_BREEDING"):logIslandWideFoodAvailability +
                            at.level(variable, "AGE_BREEDING"):Cohort,
                          random = ~ us(at.level(variable,"Personality")):Observer + us(at.level(variable,"AGE_BREEDING")):Cohort + us(at.level(variable, "Personality")):BirdID,
                          rcov = ~us(at.level(variable, "AGE_BREEDING")):BirdID + idh(at.level(variable, "Personality")):units,
                          family = NULL, # specified already in the data as poisson for both
                          prior = prior.age,
                          nitt=4600000,
                          burnin=60000,
                          thin=500,
                          verbose = FALSE,
                          pr=FALSE,
                          data = stack)


In case the issue is related to sample size or data structure, I have also attached an Rmd containing this info.

If anyone could provide any insight into how I can get my model to run successfully, it would be much appreciated!

Best wishes,
Tara


Tara Cox

Pronouns: she, her, hers

PhD researcher

Dugdale group, School of Biology

Faculty of Biological Sciences

University of Leeds



From bbo|ker @end|ng |rom gm@||@com  Wed Apr  6 00:58:31 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Tue, 5 Apr 2022 18:58:31 -0400
Subject: [R-sig-ME] Bivariate MCMCglmm: fixed effect error
In-Reply-To: <AS8PR03MB684022B27A05F0FD2E113E93F7E29@AS8PR03MB6840.eurprd03.prod.outlook.com>
References: <AS8PR03MB684022B27A05F0FD2E113E93F7E29@AS8PR03MB6840.eurprd03.prod.outlook.com>
Message-ID: <d4f22be3-6518-0007-ebaa-c3c3ca419fb6@gmail.com>

   A couple of quick thoughts.

   1. These doesn't look parameter-expanded priors to me (don't you have 
to specify alpha.V != 0 in order to get parameter-expanded priors?)


  prior: optional list of prior specifications having 3 possible
           elements: ?R? (R-structure) ?G? (G-structure) and ?B? (fixed
           effects). ?B? is a list containing the expected value (?mu?)
           and a (co)variance matrix (?V?) representing the strength of
           belief: the defaults are ?B$mu?=0 and ?B$V?=I*1e+10, where
           where I is an identity matrix of appropriate dimension.  The
           priors for the variance structures (?R? and ?G?) are lists
           with the expected (co)variances (?V?) and degree of belief
           parameter (?nu?) for the inverse-Wishart, and also the mean
           vector (?alpha.mu?) and covariance matrix (?alpha.V?) for the
           redundant working parameters. The defaults are ?nu?=0, ?V?=1,
           ?alpha.mu?=0, and ?alpha.V?=0. When ?alpha.V? is non-zero,
           parameter expanded algorithms are used.

2. MCMCglmm is explicitly telling you to use stronger priors, which I 
would interpret in this context as "larger values of nu" (nu = 0.002 
seems *very* small ...)

   I don't use MCMCglmm every day, so I may have missed something/be 
misunderstanding.


On 4/4/22 6:19 AM, Tara Cox [RPG] wrote:
> Dear list,
> 
> I am stuck attempting to run a bivariate MCMCglmm that uses personality score and age at first breeding (years) as response variables (both modelled Poisson). I have multiple repeats per individual for personality score, but a single value for age at first breeding. For fixed effects, I have included drivers known to impact personality score (age when tested, age2, colour of room that individual was tested in, and the test number) as well as age at first breeding (natal territory quality and island-wide food availability).
> 
> To account for the fact that individuals born more recently in the dataset lived short lives (as per Bouwhuis et al., 2010), I have also included cohort as a fixed effect for age at first breeding.
> 
> I've been using the covu method with a parameter expanded prior and model specification outlined below, but I am having issues when trying to run the model with cohort as a fixed effect (error message states: '## Error in MCMCglmm(fitness.stack ~ variable - 1 + at.level(variable, : Mixed model equations singular: use a (stronger) prior'). I have tried setting cohort as both numeric and factor when including it as a fixed effect, but this doesn't make a difference. I know the issue is not with any other part of the model, as it runs successfully when cohort is removed as a fixed effect.
> 
> 
> prior.age <- list(G = list(G1 = list(V = diag(1), nu = 1),                            # rand effect for Cohort (fitted for AGE_BREEDING)
>                                           G2 = list(V = diag(1), nu = 1)),                           # rand effect for Observer (fitted for Personality)
>                             R = list(R1 = list(V = diag(2), nu = 0.002, covu = TRUE),  # 2-way var-cov matrix of ID for Personality, residual for AGE_BREEDING
>                                           R2 = list(V = diag(1), nu = 0.002)))                       # residual for Personality
> 
> 
> m1  <-   MCMCglmm(fitness.stack ~ variable - 1 +
>                              at.level(variable, "Personality"):Personality_Age +
>                              at.level(variable, "Personality"):Personality_Age2 +
>                              at.level(variable, "Personality"):Personality_RoomColour +
>                              at.level(variable, "Personality"):TestNumber +
>                              at.level(variable, "AGE_BREEDING"):logNatalTerritoryQuality +
>                              at.level(variable, "AGE_BREEDING"):logIslandWideFoodAvailability +
>                              at.level(variable, "AGE_BREEDING"):Cohort,
>                            random = ~ us(at.level(variable,"Personality")):Observer + us(at.level(variable,"AGE_BREEDING")):Cohort + us(at.level(variable, "Personality")):BirdID,
>                            rcov = ~us(at.level(variable, "AGE_BREEDING")):BirdID + idh(at.level(variable, "Personality")):units,
>                            family = NULL, # specified already in the data as poisson for both
>                            prior = prior.age,
>                            nitt=4600000,
>                            burnin=60000,
>                            thin=500,
>                            verbose = FALSE,
>                            pr=FALSE,
>                            data = stack)
> 
> 
> In case the issue is related to sample size or data structure, I have also attached an Rmd containing this info.
> 
> If anyone could provide any insight into how I can get my model to run successfully, it would be much appreciated!
> 
> Best wishes,
> Tara
> 
> 
> Tara Cox
> 
> Pronouns: she, her, hers
> 
> PhD researcher
> 
> Dugdale group, School of Biology
> 
> Faculty of Biological Sciences
> 
> University of Leeds
> 
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics


From me @end|ng |rom ph||||p@|d@y@com  Wed Apr  6 04:19:00 2022
From: me @end|ng |rom ph||||p@|d@y@com (Phillip Alday)
Date: Tue, 5 Apr 2022 21:19:00 -0500
Subject: [R-sig-ME] strip plot design with pseudoreplication
In-Reply-To: <CAEyHP-0Z7jG9cneHn1=obBihGmVNVx7wCar=tTa_8Nh8=ZgzMg@mail.gmail.com>
References: <CAEyHP-0Z7jG9cneHn1=obBihGmVNVx7wCar=tTa_8Nh8=ZgzMg@mail.gmail.com>
Message-ID: <7e284ded-6bb2-99cb-4405-e0c34412533c@phillipalday.com>

I am not commenting on whether the model makes sense, but you can use
the same nesting syntax in lme4 that you used in lme:

m2_lmer<-lmer(REND.~AMBIENTE*TRATAMIENTO+
         (1|BLOQUE/AMBIENTE/parcela),
        data=data1)


On 22/3/22 8:06 am, Javier Moreira wrote:
> hi, members of the group.
> 
> I'm trying to find the way to express a model within R.
> 
> My intention is to use a set of data from a wheat trial, harvested with GPS
> mapping. I'm comparing the standard model that use the average of the plot
> (experimental unit) vs. one that use every pseudo-replication (observation
> unit-harvest point).
> 
> The design is a strip plot or split block, with 2 factors and 3 error
> terms. For the first step, I use the Agricolae package.
> 
> model0_fijo = with(data21,strip.plot(BLOCK = BLOQUE,
>                    COL = AMBIENTE,
>                    ROW = TRATAMIENTO,
>                    Y = RTO))
> *data21, n=72
> and this produces an anova with 3 error terms (Ea,Eb,Ec) and the correct
> distribution of degrees of freedom (Ambiente, Ea/ Tratamiento,Eb/
> interacci?n, Ec).
> 
> When I try to use the pseudo-replication, i first try to use lme(), and to
> take into account the correct grouping, it produces 2 different Error
> terms, and i get (Ambiente, Ea, and tratamamiento and interaction, Eb).
> This makes the design as it was a split plot (2 error terms).
> 
> modelo2_MM<-lme(REND.~1+TRATAMIENTO*AMBIENTE,
>                 random=~1|BLOQUE/AMBIENTE/TRATAMIENTO,
>                 data=data1)
> *data1, n=3095
> 
> According to a tutorial
> <https://vsni.co.uk/case-studies/dealing-with-pseudo-replication-in-linear-mixed-models>for
> the use of ASreml package i was able to use lmer() to account for the
> block:plot interaction (1|BLOQUE:parcela) and get the same result as the
> tutorial. But, this leaves the model with only 1 error term applied to
> every factor and interaction. So, it takes into account the
> pseudo-replication but not the correct error assign for the anova.
> 
> m2_lmer<-lmer(REND.~AMBIENTE*TRATAMIENTO+
>         (1|BLOQUE)+(1|BLOQUE:AMBIENTE)+(1|BLOQUE:parcela),
>        data=data1)
> 
> In the initial steps, to find a way with lmer() to get a 3 error model that
> accounts for pseudo-replication would be great.
> Following that, i also have to account for spatial correlation, and that
> isn't an option within lmer() so, i have to get the same model translate to
> lme().
> 
> Thanks a lot for your help. This data analysis is for a on farm trial from
> my Msc thesis.
> 
> best regards,
> 
> Ing. Agr. Javier Moreira
>


From j@v|ermore|r@ @end|ng |rom gm@||@com  Wed Apr  6 16:12:58 2022
From: j@v|ermore|r@ @end|ng |rom gm@||@com (Javier Moreira)
Date: Wed, 6 Apr 2022 11:12:58 -0300
Subject: [R-sig-ME] strip plot design with pseudoreplication
In-Reply-To: <7e284ded-6bb2-99cb-4405-e0c34412533c@phillipalday.com>
References: <CAEyHP-0Z7jG9cneHn1=obBihGmVNVx7wCar=tTa_8Nh8=ZgzMg@mail.gmail.com>
 <7e284ded-6bb2-99cb-4405-e0c34412533c@phillipalday.com>
Message-ID: <CAEyHP-1eXWK30zUcNan+qLW3AAew7njP8HbheQbUAip=CO7r0w@mail.gmail.com>

Hi, thanks for your help.
I was reading my question, and find
I have tried that way, but still I'm getting 2 errors. I should edit the
question because later i found the correct way on lmer

m2_lmer<-lmer(REND.~AMBIENTE*TRATAMIENTO+

(1|BLOQUE)+(1|BLOQUE:AMBIENTE)+(1|BLOQUE:TRATAMIENTO)+(1|BLOQUE:AMBIENTE:TRATAMIENTO),
       data=data1)

I see your answer is the right response for my question. The error was on
the question.
Actually what i need is the translation of this model to lme. I have
another random term:  (1|BLOQUE:AMBIENTE:TRATAMIENTO) that makes the model
to account for every error term needed.

again, thanks for your help.

regards

El mar, 5 abr 2022 a las 23:19, Phillip Alday (<me at phillipalday.com>)
escribi?:

> I am not commenting on whether the model makes sense, but you can use
> the same nesting syntax in lme4 that you used in lme:
>
> m2_lmer<-lmer(REND.~AMBIENTE*TRATAMIENTO+
>          (1|BLOQUE/AMBIENTE/parcela),
>         data=data1)
>
>
> On 22/3/22 8:06 am, Javier Moreira wrote:
> > hi, members of the group.
> >
> > I'm trying to find the way to express a model within R.
> >
> > My intention is to use a set of data from a wheat trial, harvested with
> GPS
> > mapping. I'm comparing the standard model that use the average of the
> plot
> > (experimental unit) vs. one that use every pseudo-replication
> (observation
> > unit-harvest point).
> >
> > The design is a strip plot or split block, with 2 factors and 3 error
> > terms. For the first step, I use the Agricolae package.
> >
> > model0_fijo = with(data21,strip.plot(BLOCK = BLOQUE,
> >                    COL = AMBIENTE,
> >                    ROW = TRATAMIENTO,
> >                    Y = RTO))
> > *data21, n=72
> > and this produces an anova with 3 error terms (Ea,Eb,Ec) and the correct
> > distribution of degrees of freedom (Ambiente, Ea/ Tratamiento,Eb/
> > interacci?n, Ec).
> >
> > When I try to use the pseudo-replication, i first try to use lme(), and
> to
> > take into account the correct grouping, it produces 2 different Error
> > terms, and i get (Ambiente, Ea, and tratamamiento and interaction, Eb).
> > This makes the design as it was a split plot (2 error terms).
> >
> > modelo2_MM<-lme(REND.~1+TRATAMIENTO*AMBIENTE,
> >                 random=~1|BLOQUE/AMBIENTE/TRATAMIENTO,
> >                 data=data1)
> > *data1, n=3095
> >
> > According to a tutorial
> > <
> https://vsni.co.uk/case-studies/dealing-with-pseudo-replication-in-linear-mixed-models
> >for
> > the use of ASreml package i was able to use lmer() to account for the
> > block:plot interaction (1|BLOQUE:parcela) and get the same result as the
> > tutorial. But, this leaves the model with only 1 error term applied to
> > every factor and interaction. So, it takes into account the
> > pseudo-replication but not the correct error assign for the anova.
> >
> > m2_lmer<-lmer(REND.~AMBIENTE*TRATAMIENTO+
> >         (1|BLOQUE)+(1|BLOQUE:AMBIENTE)+(1|BLOQUE:parcela),
> >        data=data1)
> >
> > In the initial steps, to find a way with lmer() to get a 3 error model
> that
> > accounts for pseudo-replication would be great.
> > Following that, i also have to account for spatial correlation, and that
> > isn't an option within lmer() so, i have to get the same model translate
> to
> > lme().
> >
> > Thanks a lot for your help. This data analysis is for a on farm trial
> from
> > my Msc thesis.
> >
> > best regards,
> >
> > Ing. Agr. Javier Moreira
> >
>


-- 
Javier Moreira de Souza
Ingeniero Agr?nomo
099 406 006

	[[alternative HTML version deleted]]


From motyoc@k@ @end|ng |rom y@hoo@com  Thu Apr  7 19:18:35 2022
From: motyoc@k@ @end|ng |rom y@hoo@com (Andras Farkas)
Date: Thu, 7 Apr 2022 17:18:35 +0000 (UTC)
Subject: [R-sig-ME] lower and upper bounds for optimizer="nloptwrap"
References: <2025244783.1606999.1649351915387.ref@mail.yahoo.com>
Message-ID: <2025244783.1606999.1649351915387@mail.yahoo.com>

Hello,

wonder if I could get some help on how to specify lower and upper bounds?for optimizer="nloptwrap". To date my approach looks like:


lmercontrollist<-lmerControl(optimizer = "nloptwrap",
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?restart_edge = TRUE,
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?boundary.tol = 1e-5,
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?calc.derivs = TRUE,
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?use.last.params = FALSE,
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?sparseX = FALSE,
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?standardize.X = FALSE,
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?## input checking options
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.nobs.vs.rankZ = "ignore",
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.nobs.vs.nlev = "stop",
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.nlev.gtreq.5 = "ignore",
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.nlev.gtr.1 = "stop",
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.nobs.vs.nRE= "stop",
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.rankX = c("message+drop.cols", "silent.drop.cols", "warn+drop.cols",
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?"stop.deficient", "ignore"),
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.scaleX = c("warning","stop","silent.rescale",
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? "message+rescale","warn+rescale","ignore"),
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.formula.LHS = "stop",
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?## convergence checking options
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.conv.grad? ? ?= .makeCC("warning", tol = 2e-3, relTol = NULL),
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.conv.singular = .makeCC(action = "message", tol = formals(isSingular)$tol),
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.conv.hess? ? ?= .makeCC(action = "warning", tol = 1e-6),
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?## optimizer args
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?optCtrl = list(maxeval=100,lower = rep.int(0, 21),upper = rep.int(5, 21)),
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?mod.type = "lmer")

let us ignore model appropriateness here and just focus on the constraints:?

m1 <- lmer(Reaction ~ Days + Subject+ (1|Days),data=sleepstudy,verbose=1,control =lmercontrollist)
summary(m1)


?as you will see the fixed effect values are negative. I also tried lb and ub instead of lower and upper based on?nloptr function, but also did not help. Besides, just to be sure I understand correctly, the length of the lower/upper bounds must be the same as the length of fixed and random effect, correct? here I set to 21 based on output but am not exactly sure if the residual has to be included in the count or not...

much appreciate your help,

Andras?


From me @end|ng |rom ph||||p@|d@y@com  Thu Apr  7 19:35:28 2022
From: me @end|ng |rom ph||||p@|d@y@com (Phillip Alday)
Date: Thu, 7 Apr 2022 12:35:28 -0500
Subject: [R-sig-ME] lower and upper bounds for optimizer="nloptwrap"
In-Reply-To: <2025244783.1606999.1649351915387@mail.yahoo.com>
References: <2025244783.1606999.1649351915387.ref@mail.yahoo.com>
 <2025244783.1606999.1649351915387@mail.yahoo.com>
Message-ID: <6514b9ef-33f8-356c-f633-4ad95654a077@phillipalday.com>

lme4 uses the profiled log likelihood -- for linear mixed models, the
maximum-likelihood estimates of the fixed effects can be computed
directly for a given value of the random effects. Because of this, the
optimization bounds are for the random effects (or more precisely, the
lower Cholesky factor of the scaled random effects) and only serve to
restrict them to possible values (positive variances and correlations in
[-1, +1]).

In other words, you can't constrain the fixed effects in lme4.

Is there an inferential reason why you want to constrain the fixed effects?

On 7/4/22 12:18 pm, Andras Farkas via R-sig-mixed-models wrote:
> Hello,
> 
> wonder if I could get some help on how to specify lower and upper bounds?for optimizer="nloptwrap". To date my approach looks like:
> 
> 
> lmercontrollist<-lmerControl(optimizer = "nloptwrap",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?restart_edge = TRUE,
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?boundary.tol = 1e-5,
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?calc.derivs = TRUE,
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?use.last.params = FALSE,
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?sparseX = FALSE,
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?standardize.X = FALSE,
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?## input checking options
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.nobs.vs.rankZ = "ignore",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.nobs.vs.nlev = "stop",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.nlev.gtreq.5 = "ignore",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.nlev.gtr.1 = "stop",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.nobs.vs.nRE= "stop",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.rankX = c("message+drop.cols", "silent.drop.cols", "warn+drop.cols",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?"stop.deficient", "ignore"),
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.scaleX = c("warning","stop","silent.rescale",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? "message+rescale","warn+rescale","ignore"),
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.formula.LHS = "stop",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?## convergence checking options
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.conv.grad? ? ?= .makeCC("warning", tol = 2e-3, relTol = NULL),
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.conv.singular = .makeCC(action = "message", tol = formals(isSingular)$tol),
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.conv.hess? ? ?= .makeCC(action = "warning", tol = 1e-6),
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?## optimizer args
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?optCtrl = list(maxeval=100,lower = rep.int(0, 21),upper = rep.int(5, 21)),
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?mod.type = "lmer")
> 
> let us ignore model appropriateness here and just focus on the constraints:?
> 
> m1 <- lmer(Reaction ~ Days + Subject+ (1|Days),data=sleepstudy,verbose=1,control =lmercontrollist)
> summary(m1)
> 
> 
> ?as you will see the fixed effect values are negative. I also tried lb and ub instead of lower and upper based on?nloptr function, but also did not help. Besides, just to be sure I understand correctly, the length of the lower/upper bounds must be the same as the length of fixed and random effect, correct? here I set to 21 based on output but am not exactly sure if the residual has to be included in the count or not...
> 
> much appreciate your help,
> 
> Andras?
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From motyoc@k@ @end|ng |rom y@hoo@com  Thu Apr  7 20:05:53 2022
From: motyoc@k@ @end|ng |rom y@hoo@com (Andras Farkas)
Date: Thu, 7 Apr 2022 18:05:53 +0000 (UTC)
Subject: [R-sig-ME] lower and upper bounds for optimizer="nloptwrap"
In-Reply-To: <6514b9ef-33f8-356c-f633-4ad95654a077@phillipalday.com>
References: <2025244783.1606999.1649351915387.ref@mail.yahoo.com>
 <2025244783.1606999.1649351915387@mail.yahoo.com>
 <6514b9ef-33f8-356c-f633-4ad95654a077@phillipalday.com>
Message-ID: <527670223.1622144.1649354753648@mail.yahoo.com>

Philip,

thanks. my actual model is based on some biological process/es where a negative value (or at times a value outside of a particular range) would not make sense, if estimated... I have done the model run with brms in the past which made this easi(er), but I also wanted to see if I can take advantage of automated model selection procedures available in R (have "way too many potential predictors"), so switched to frequentist approach to try to make it work with glmulti as I am far from being an R coding expert and there are enough available resources on the web that I have found helpful to set up the glmulti run with lmer... But I guess I may be stuck with bayesian it seems if I must box them in?


thanks for the help

Andras?






On Thursday, April 7, 2022, 01:35:41 PM EDT, Phillip Alday <me at phillipalday.com> wrote: 





lme4 uses the profiled log likelihood -- for linear mixed models, the
maximum-likelihood estimates of the fixed effects can be computed
directly for a given value of the random effects. Because of this, the
optimization bounds are for the random effects (or more precisely, the
lower Cholesky factor of the scaled random effects) and only serve to
restrict them to possible values (positive variances and correlations in
[-1, +1]).

In other words, you can't constrain the fixed effects in lme4.

Is there an inferential reason why you want to constrain the fixed effects?

On 7/4/22 12:18 pm, Andras Farkas via R-sig-mixed-models wrote:
> Hello,
> 
> wonder if I could get some help on how to specify lower and upper bounds?for optimizer="nloptwrap". To date my approach looks like:
> 
> 
> lmercontrollist<-lmerControl(optimizer = "nloptwrap",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?restart_edge = TRUE,
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?boundary.tol = 1e-5,
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?calc.derivs = TRUE,
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?use.last.params = FALSE,
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?sparseX = FALSE,
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?standardize.X = FALSE,
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?## input checking options
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.nobs.vs.rankZ = "ignore",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.nobs.vs.nlev = "stop",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.nlev.gtreq.5 = "ignore",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.nlev.gtr.1 = "stop",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.nobs.vs.nRE= "stop",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.rankX = c("message+drop.cols", "silent.drop.cols", "warn+drop.cols",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?"stop.deficient", "ignore"),
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.scaleX = c("warning","stop","silent.rescale",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? "message+rescale","warn+rescale","ignore"),
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.formula.LHS = "stop",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?## convergence checking options
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.conv.grad? ? ?= .makeCC("warning", tol = 2e-3, relTol = NULL),
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.conv.singular = .makeCC(action = "message", tol = formals(isSingular)$tol),
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.conv.hess? ? ?= .makeCC(action = "warning", tol = 1e-6),
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?## optimizer args
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?optCtrl = list(maxeval=100,lower = rep.int(0, 21),upper = rep.int(5, 21)),
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?mod.type = "lmer")
> 
> let us ignore model appropriateness here and just focus on the constraints:?
> 
> m1 <- lmer(Reaction ~ Days + Subject+ (1|Days),data=sleepstudy,verbose=1,control =lmercontrollist)
> summary(m1)
> 
> 
> ?as you will see the fixed effect values are negative. I also tried lb and ub instead of lower and upper based on?nloptr function, but also did not help. Besides, just to be sure I understand correctly, the length of the lower/upper bounds must be the same as the length of fixed and random effect, correct? here I set to 21 based on output but am not exactly sure if the residual has to be included in the count or not...
> 
> much appreciate your help,
> 
> Andras?

> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

> 


From motyoc@k@ @end|ng |rom y@hoo@com  Thu Apr  7 20:33:57 2022
From: motyoc@k@ @end|ng |rom y@hoo@com (Andras Farkas)
Date: Thu, 7 Apr 2022 18:33:57 +0000 (UTC)
Subject: [R-sig-ME] lower and upper bounds for optimizer="nloptwrap"
In-Reply-To: <527670223.1622144.1649354753648@mail.yahoo.com>
References: <2025244783.1606999.1649351915387.ref@mail.yahoo.com>
 <2025244783.1606999.1649351915387@mail.yahoo.com>
 <6514b9ef-33f8-356c-f633-4ad95654a077@phillipalday.com>
 <527670223.1622144.1649354753648@mail.yahoo.com>
Message-ID: <1804903896.1630758.1649356437205@mail.yahoo.com>

funny, had to write this email to somehow find this here. Ben Bolker did some work on this it seems in past, guess have to take it with the warnings... May be helpful for others...

https://rstudio-pubs-static.s3.amazonaws.com/108675_02daa6544e584a928b296c3bca9a65d5.html



Andras?






On Thursday, April 7, 2022, 02:05:53 PM EDT, Andras Farkas <motyocska at yahoo.com> wrote: 





Philip,

thanks. my actual model is based on some biological process/es where a negative value (or at times a value outside of a particular range) would not make sense, if estimated... I have done the model run with brms in the past which made this easi(er), but I also wanted to see if I can take advantage of automated model selection procedures available in R (have "way too many potential predictors"), so switched to frequentist approach to try to make it work with glmulti as I am far from being an R coding expert and there are enough available resources on the web that I have found helpful to set up the glmulti run with lmer... But I guess I may be stuck with bayesian it seems if I must box them in?


thanks for the help

Andras?






On Thursday, April 7, 2022, 01:35:41 PM EDT, Phillip Alday <me at phillipalday.com> wrote: 





lme4 uses the profiled log likelihood -- for linear mixed models, the
maximum-likelihood estimates of the fixed effects can be computed
directly for a given value of the random effects. Because of this, the
optimization bounds are for the random effects (or more precisely, the
lower Cholesky factor of the scaled random effects) and only serve to
restrict them to possible values (positive variances and correlations in
[-1, +1]).

In other words, you can't constrain the fixed effects in lme4.

Is there an inferential reason why you want to constrain the fixed effects?

On 7/4/22 12:18 pm, Andras Farkas via R-sig-mixed-models wrote:
> Hello,
> 
> wonder if I could get some help on how to specify lower and upper bounds?for optimizer="nloptwrap". To date my approach looks like:
> 
> 
> lmercontrollist<-lmerControl(optimizer = "nloptwrap",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?restart_edge = TRUE,
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?boundary.tol = 1e-5,
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?calc.derivs = TRUE,
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?use.last.params = FALSE,
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?sparseX = FALSE,
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?standardize.X = FALSE,
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?## input checking options
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.nobs.vs.rankZ = "ignore",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.nobs.vs.nlev = "stop",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.nlev.gtreq.5 = "ignore",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.nlev.gtr.1 = "stop",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.nobs.vs.nRE= "stop",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.rankX = c("message+drop.cols", "silent.drop.cols", "warn+drop.cols",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?"stop.deficient", "ignore"),
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.scaleX = c("warning","stop","silent.rescale",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? "message+rescale","warn+rescale","ignore"),
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.formula.LHS = "stop",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?## convergence checking options
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.conv.grad? ? ?= .makeCC("warning", tol = 2e-3, relTol = NULL),
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.conv.singular = .makeCC(action = "message", tol = formals(isSingular)$tol),
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.conv.hess? ? ?= .makeCC(action = "warning", tol = 1e-6),
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?## optimizer args
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?optCtrl = list(maxeval=100,lower = rep.int(0, 21),upper = rep.int(5, 21)),
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?mod.type = "lmer")
> 
> let us ignore model appropriateness here and just focus on the constraints:?
> 
> m1 <- lmer(Reaction ~ Days + Subject+ (1|Days),data=sleepstudy,verbose=1,control =lmercontrollist)
> summary(m1)
> 
> 
> ?as you will see the fixed effect values are negative. I also tried lb and ub instead of lower and upper based on?nloptr function, but also did not help. Besides, just to be sure I understand correctly, the length of the lower/upper bounds must be the same as the length of fixed and random effect, correct? here I set to 21 based on output but am not exactly sure if the residual has to be included in the count or not...
> 
> much appreciate your help,
> 
> Andras?

> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

> 


From bbo|ker @end|ng |rom gm@||@com  Fri Apr  8 03:24:45 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Thu, 7 Apr 2022 21:24:45 -0400
Subject: [R-sig-ME] lower and upper bounds for optimizer="nloptwrap"
In-Reply-To: <527670223.1622144.1649354753648@mail.yahoo.com>
References: <2025244783.1606999.1649351915387.ref@mail.yahoo.com>
 <2025244783.1606999.1649351915387@mail.yahoo.com>
 <6514b9ef-33f8-356c-f633-4ad95654a077@phillipalday.com>
 <527670223.1622144.1649354753648@mail.yahoo.com>
Message-ID: <3cad21d7-6599-6ccf-f6ad-ea41e28435f1@gmail.com>

    glmmTMB should let you pass lower/upper bounds to the nlminb 
optimizer, although it's a little bit obscure (you need

  glmmTMB(optCtrl = list(lower = ..., upper = ...)

and you need to know the mapping/order in which the various model 
components get embedded so you can line up the lower/upper controls 
appropriately.

library(glmmTMB)
data("sleepstudy", package = "lme4")
fm1 <- glmmTMB(Reaction ~ Days + (Days|Subject), sleepstudy)
fm1$obj$par
  beta  beta betad theta theta theta
     1     1     0     0     0     0

this tells you that the fixed effects parameters come first in the 
parameter vector ...

   I don't think glmulti is going to work with glmmTMB though.

On 4/7/22 2:05 PM, Andras Farkas via R-sig-mixed-models wrote:
> Philip,
> 
> thanks. my actual model is based on some biological process/es where a negative value (or at times a value outside of a particular range) would not make sense, if estimated... I have done the model run with brms in the past which made this easi(er), but I also wanted to see if I can take advantage of automated model selection procedures available in R (have "way too many potential predictors"), so switched to frequentist approach to try to make it work with glmulti as I am far from being an R coding expert and there are enough available resources on the web that I have found helpful to set up the glmulti run with lmer... But I guess I may be stuck with bayesian it seems if I must box them in?
> 
> 
> thanks for the help
> 
> Andras
> 
> 
> 
> 
> 
> 
> On Thursday, April 7, 2022, 01:35:41 PM EDT, Phillip Alday <me at phillipalday.com> wrote:
> 
> 
> 
> 
> 
> lme4 uses the profiled log likelihood -- for linear mixed models, the
> maximum-likelihood estimates of the fixed effects can be computed
> directly for a given value of the random effects. Because of this, the
> optimization bounds are for the random effects (or more precisely, the
> lower Cholesky factor of the scaled random effects) and only serve to
> restrict them to possible values (positive variances and correlations in
> [-1, +1]).
> 
> In other words, you can't constrain the fixed effects in lme4.
> 
> Is there an inferential reason why you want to constrain the fixed effects?
> 
> On 7/4/22 12:18 pm, Andras Farkas via R-sig-mixed-models wrote:
>> Hello,
>>
>> wonder if I could get some help on how to specify lower and upper bounds?for optimizer="nloptwrap". To date my approach looks like:
>>
>>
>> lmercontrollist<-lmerControl(optimizer = "nloptwrap",
>>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?restart_edge = TRUE,
>>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?boundary.tol = 1e-5,
>>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?calc.derivs = TRUE,
>>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?use.last.params = FALSE,
>>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?sparseX = FALSE,
>>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?standardize.X = FALSE,
>>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?## input checking options
>>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.nobs.vs.rankZ = "ignore",
>>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.nobs.vs.nlev = "stop",
>>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.nlev.gtreq.5 = "ignore",
>>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.nlev.gtr.1 = "stop",
>>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.nobs.vs.nRE= "stop",
>>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.rankX = c("message+drop.cols", "silent.drop.cols", "warn+drop.cols",
>>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?"stop.deficient", "ignore"),
>>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.scaleX = c("warning","stop","silent.rescale",
>>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? "message+rescale","warn+rescale","ignore"),
>>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.formula.LHS = "stop",
>>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?## convergence checking options
>>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.conv.grad? ? ?= .makeCC("warning", tol = 2e-3, relTol = NULL),
>>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.conv.singular = .makeCC(action = "message", tol = formals(isSingular)$tol),
>>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?check.conv.hess? ? ?= .makeCC(action = "warning", tol = 1e-6),
>>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?## optimizer args
>>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?optCtrl = list(maxeval=100,lower = rep.int(0, 21),upper = rep.int(5, 21)),
>>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?mod.type = "lmer")
>>
>> let us ignore model appropriateness here and just focus on the constraints:
>>
>> m1 <- lmer(Reaction ~ Days + Subject+ (1|Days),data=sleepstudy,verbose=1,control =lmercontrollist)
>> summary(m1)
>>
>>
>>  ?as you will see the fixed effect values are negative. I also tried lb and ub instead of lower and upper based on?nloptr function, but also did not help. Besides, just to be sure I understand correctly, the length of the lower/upper bounds must be the same as the length of fixed and random effect, correct? here I set to 21 based on output but am not exactly sure if the residual has to be included in the count or not...
>>
>> much appreciate your help,
>>
>> Andras
> 
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
>>
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics


From @jb201 @end|ng |rom c@@e@edu  Fri Apr  8 14:40:50 2022
From: @jb201 @end|ng |rom c@@e@edu (Amanda Barabas)
Date: Fri, 8 Apr 2022 08:40:50 -0400
Subject: [R-sig-ME] nested random effects in lmer
Message-ID: <CA+nfb=0DjwP8E3SMWxreM6K1OouhDNX2eVPN7S93P5L7o=c9Dg@mail.gmail.com>

Hello R members,

I have a situation involving nested data for which I couldn?t find a clear
answer and am hoping someone can help.



I do work using laboratory mice. A common study design involves repeated
measures of single animals receiving the same treatment over time. For
these designs, it?s recommended to use a nested design since each animal
typically only experiences one treatment level. It looks like I could code
this by adding these terms to a lmer model:



(1|Mouse) + (1|Treatment:Mouse)



However, we also like to account for sex in the models. A single mouse can
only have one sex, so I would similarly code this nested effect with:



(1|Mouse) + (1|Sex:Mouse)



If I have a single model where Mouse is nested in both Treatment and Sex,
do I need an additional term to reflect this? Would this code be correct?:



(1|Mouse) + (1|Treatment:Mouse) + (1|Sex:Mouse)



For comparison, I?ve previously used JMP for nested models. In JMP, Mouse
nested in both Treatment and Sex would look like this: Mouse(Treatment,
Sex). I?m not sure if the above line of code would be the R equivalent or
if I need to add a term that includes both Treatment and Sex in the same
set of parentheses. My searches weren?t very helpful for this situation.
I?d greatly appreciate any resources anyone may have about this type of
design.

Thanks,
-- 
Amanda Barabas, B.S.
PhD Candidate, Department of Animal Science
Purdue University

	[[alternative HTML version deleted]]


From motyoc@k@ @end|ng |rom y@hoo@com  Sat Apr  9 21:11:03 2022
From: motyoc@k@ @end|ng |rom y@hoo@com (Andras Farkas)
Date: Sat, 9 Apr 2022 19:11:03 +0000 (UTC)
Subject: [R-sig-ME] lmer formula
References: <1253113674.137086.1649531463118.ref@mail.yahoo.com>
Message-ID: <1253113674.137086.1649531463118@mail.yahoo.com>

Hello,

could you please provide input on the following formula (please ignore the widely known example of the sleepstudy data and its analysis for a moment and assume we have this formula here) :

 m1 <- lmer(Reaction ~ ((A1*Days)/(B2*C1)) + (1 | Subject), sleepstudy)

this simple example - similar to my actual real life model of greeter complexity - what I would try to do is to estimate fixed effects for "Days" and "C1", but not for "A1"
and "B2". In this hypothetical example A1, B2, AND C1 all have a column with values in the data sleepstudy, but instead of estimating a coefficient for A1 and B2 we would just use the actual values of them as they appear in the spreadsheet in the calculations.

hope I am not way off with my question, but could you please advise as to how the model formula would need to be written in this case?

much appreciate the help,

thanks,


Andras?


From bbo|ker @end|ng |rom gm@||@com  Sun Apr 10 01:07:26 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sat, 9 Apr 2022 19:07:26 -0400
Subject: [R-sig-ME] lmer formula
In-Reply-To: <1253113674.137086.1649531463118@mail.yahoo.com>
References: <1253113674.137086.1649531463118.ref@mail.yahoo.com>
 <1253113674.137086.1649531463118@mail.yahoo.com>
Message-ID: <c2272f41-e82d-83ee-552d-6dddfd2ff4dd@gmail.com>


   This is not possible in lme4, which requires a *linear* model (some 
incorporation of constants is possible via offsets, but only additive 
constants).

   Using nlme() from the nlme package, something like

nlme(Reaction ~ A1*mu1/(B2*mu2) + mu3,
      fixed = list(mu1 ~ Days,
                   mu2 ~ C1),
      random = ~ 1 | mu3
)

might work.  But keep in mind that when you fit nonlinear mixed effects 
models you often have to be much clearer by what you mean by "estimating 
fixed effects for" something -- this will typically involve a sub-model 
that is a linear model for a parameter that enters the top-level 
nonlinear equation.



On 4/9/22 3:11 PM, Andras Farkas via R-sig-mixed-models wrote:
> Hello,
> 
> could you please provide input on the following formula (please ignore the widely known example of the sleepstudy data and its analysis for a moment and assume we have this formula here) :
> 
>   m1 <- lmer(Reaction ~ ((A1*Days)/(B2*C1)) + (1 | Subject), sleepstudy)
> 
> this simple example - similar to my actual real life model of greeter complexity - what I would try to do is to estimate fixed effects for "Days" and "C1", but not for "A1"
> and "B2". In this hypothetical example A1, B2, AND C1 all have a column with values in the data sleepstudy, but instead of estimating a coefficient for A1 and B2 we would just use the actual values of them as they appear in the spreadsheet in the calculations.
> 
> hope I am not way off with my question, but could you please advise as to how the model formula would need to be written in this case?
> 
> much appreciate the help,
> 
> thanks,
> 
> 
> Andras
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From me @end|ng |rom ph||||p@|d@y@com  Mon Apr 11 05:18:18 2022
From: me @end|ng |rom ph||||p@|d@y@com (Phillip Alday)
Date: Sun, 10 Apr 2022 22:18:18 -0500
Subject: [R-sig-ME] nested random effects in lmer
In-Reply-To: <CA+nfb=0DjwP8E3SMWxreM6K1OouhDNX2eVPN7S93P5L7o=c9Dg@mail.gmail.com>
References: <CA+nfb=0DjwP8E3SMWxreM6K1OouhDNX2eVPN7S93P5L7o=c9Dg@mail.gmail.com>
Message-ID: <0b392dc0-d87b-2fbe-62b8-a2e9bd92a6e2@phillipalday.com>

I'm not familiar with JMP, but I suspect that both treatment and sex
should be fixed effects. If each mouse receives only a single treatment,
then you would have:

1 + treatment * sex + (1|mouse)

if there is treatment by sex interaction

or

1 + treatment + sex + (1|mouse)

if there is no treatment by sex interaction.

If each mouse received multiple treatments, then you may want to
consider (1 + treatment|mouse) instead of (1|mouse) in order to allow
for the treatment effect size to vary between mice.


The GLMM FAQ has a small section on this:

https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#should-i-treat-factor-xxx-as-fixed-or-random

Hope that helps a bit,
Phillip

On 8/4/22 7:40 am, Amanda Barabas wrote:
> Hello R members,
> 
> I have a situation involving nested data for which I couldn?t find a clear
> answer and am hoping someone can help.
> 
> 
> 
> I do work using laboratory mice. A common study design involves repeated
> measures of single animals receiving the same treatment over time. For
> these designs, it?s recommended to use a nested design since each animal
> typically only experiences one treatment level. It looks like I could code
> this by adding these terms to a lmer model:
> 
> 
> 
> (1|Mouse) + (1|Treatment:Mouse)
> 
> 
> 
> However, we also like to account for sex in the models. A single mouse can
> only have one sex, so I would similarly code this nested effect with:
> 
> 
> 
> (1|Mouse) + (1|Sex:Mouse)
> 
> 
> 
> If I have a single model where Mouse is nested in both Treatment and Sex,
> do I need an additional term to reflect this? Would this code be correct?:
> 
> 
> 
> (1|Mouse) + (1|Treatment:Mouse) + (1|Sex:Mouse)
> 
> 
> 
> For comparison, I?ve previously used JMP for nested models. In JMP, Mouse
> nested in both Treatment and Sex would look like this: Mouse(Treatment,
> Sex). I?m not sure if the above line of code would be the R equivalent or
> if I need to add a term that includes both Treatment and Sex in the same
> set of parentheses. My searches weren?t very helpful for this situation.
> I?d greatly appreciate any resources anyone may have about this type of
> design.
> 
> Thanks,
>


From 120081541 @end|ng |rom @u|e@edu@cn  Thu Apr 14 10:56:20 2022
From: 120081541 @end|ng |rom @u|e@edu@cn (=?utf-8?B?5rGq54eV5pWP?=)
Date: Thu, 14 Apr 2022 09:56:20 +0100
Subject: [R-sig-ME] how to get the relative importance of each variable in
 linear mixed model
Message-ID: <tencent_7843E6F00CECB5736C249AF3@qq.com>

dear sir,

&nbsp;I used lme4 in R to fit the mixed model.&nbsp; i would like to get&nbsp;the relative importance of each variable.&nbsp;i know anova can&nbsp;&nbsp;do the task&nbsp;&nbsp;in linear model, but it can not work in&nbsp;linear mixed model.
so, my question is&nbsp;how to get the relative importance of each variable of both fixed effects and random effects in linear mixed model?
look forwar to your reply.


best wishes
yanmin wang











???

??????/?????????

?????????962?




&nbsp;
	[[alternative HTML version deleted]]


From me @end|ng |rom ph||||p@|d@y@com  Fri Apr 15 04:47:46 2022
From: me @end|ng |rom ph||||p@|d@y@com (Phillip Alday)
Date: Thu, 14 Apr 2022 21:47:46 -0500
Subject: [R-sig-ME] 
 how to get the relative importance of each variable in
 linear mixed model
In-Reply-To: <tencent_7843E6F00CECB5736C249AF3@qq.com>
References: <tencent_7843E6F00CECB5736C249AF3@qq.com>
Message-ID: <9fed4a8f-2f13-92e8-d9e4-7afe18015cbd@phillipalday.com>

See this section of the GLMM FAQ for a discussion of effect sizes and
variable importance:

https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#model-summaries-goodness-of-fit-decomposition-of-variance-etc.


Phillip

On 14/4/22 3:56 am, ??? wrote:
> dear sir,
> 
> &nbsp;I used lme4 in R to fit the mixed model.&nbsp; i would like to get&nbsp;the relative importance of each variable.&nbsp;i know anova can&nbsp;&nbsp;do the task&nbsp;&nbsp;in linear model, but it can not work in&nbsp;linear mixed model.
> so, my question is&nbsp;how to get the relative importance of each variable of both fixed effects and random effects in linear mixed model?
> look forwar to your reply.
> 
> 
> best wishes
> yanmin wang
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> ???
> 
> ??????/?????????
> 
> ?????????962?
> 
> 
> 
> 
> &nbsp;
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From w|||pe@t1 @end|ng |rom gm@||@com  Mon Apr 18 17:09:49 2022
From: w|||pe@t1 @end|ng |rom gm@||@com (will peat)
Date: Mon, 18 Apr 2022 16:09:49 +0100
Subject: [R-sig-ME] Question about glmmTMB
Message-ID: <CAC1piu22SZg6AdWz=rtRoHtW5WWq36gJSnXgEv_R+oHmirDDcg@mail.gmail.com>

Hi there,

I was on github looking for help with an issue and I saw this email listed
as a place to ask questions!

I am trying to apply a poisson GLMM using glmmTMB and have used the
following code to do so:

> MP1 <- glmmTMB(Abundance ~ Year + Location + Depth + All_predator +
+                  (1 | Site/Site.Transect),
+                data = plandat,
+                family = "poisson")

But have come across the following error:

Error in .Call("getParameterOrder", data, parameters, new.env(), PACKAGE =
DLL) :
  Incorrect number of arguments (3), expecting 4 for 'getParameterOrder'

This has led me to some threads which I struggle to understand. Is there
anyway to get around this error?

Apologies if this is not the right email address or if this is not what the
email is intended for!

Best wishes
Will

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Mon Apr 18 17:19:14 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 18 Apr 2022 11:19:14 -0400
Subject: [R-sig-ME] Question about glmmTMB
In-Reply-To: <CAC1piu22SZg6AdWz=rtRoHtW5WWq36gJSnXgEv_R+oHmirDDcg@mail.gmail.com>
References: <CAC1piu22SZg6AdWz=rtRoHtW5WWq36gJSnXgEv_R+oHmirDDcg@mail.gmail.com>
Message-ID: <CABghstR=-sYUEW-PjMFyjdUZGksp4oKMY-yXZGdmbtARRaqrNA@mail.gmail.com>

   This is a fine place to ask, but in general cross-posting is discouraged ...

https://stackoverflow.com/questions/71913002/issue-with-glmm-in-glmmtmb

For the record,

This is a binary-incompatibility problem as documented
[here](https://github.com/glmmTMB/glmmTMB/issues/791). You should
probably have seen a "Package version inconsistency detected" message
telling you to re-install glmmTMB from source. Depending on the
release sequence of TMB and glmmTMB on CRAN, updating from CRAN as
usual might work; otherwise you need to re-install from source (see
?glmmTMB::reinstalling).

  For guidance on where to ask glmmTMB questions see
https://github.com/glmmTMB/glmmTMB#where-to-ask-questions=

  cheers
   Ben Bolker

On Mon, Apr 18, 2022 at 11:15 AM will peat <willpeat1 at gmail.com> wrote:
>
> Hi there,
>
> I was on github looking for help with an issue and I saw this email listed
> as a place to ask questions!
>
> I am trying to apply a poisson GLMM using glmmTMB and have used the
> following code to do so:
>
> > MP1 <- glmmTMB(Abundance ~ Year + Location + Depth + All_predator +
> +                  (1 | Site/Site.Transect),
> +                data = plandat,
> +                family = "poisson")
>
> But have come across the following error:
>
> Error in .Call("getParameterOrder", data, parameters, new.env(), PACKAGE =
> DLL) :
>   Incorrect number of arguments (3), expecting 4 for 'getParameterOrder'
>
> This has led me to some threads which I struggle to understand. Is there
> anyway to get around this error?
>
> Apologies if this is not the right email address or if this is not what the
> email is intended for!
>
> Best wishes
> Will
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From O|uw@@eun@E@@n @end|ng |rom ||verpoo|@@c@uk  Tue Apr 19 15:54:57 2022
From: O|uw@@eun@E@@n @end|ng |rom ||verpoo|@@c@uk (Esan, Oluwaseun)
Date: Tue, 19 Apr 2022 13:54:57 +0000
Subject: [R-sig-ME] Error in logLik.reStruct(object,
 conLin) : NA/NaN/Inf in foreign function call (arg 3)
 when Exponential Correlation added to model
Message-ID: <6bfb31653b2944e898c83bf751c557e2@liverpool.ac.uk>

Dear List Members,

I am relatively new to R and Mixed Effects Models using LME.

I am unable to post sample data:


I am trying to get the effect of pregnancy on lung function  and age on women with a certain genetic condition. We know lung function declines with age in people with the condition but we do not know the effect pregnancy has on this relationship.

I have repeat measurements of lung function measured up to 4 times in a year for up to 14 years for each woman (A total of 331,722 observations for 11668 women).

I am having problems when trying to add an exponential serial correlation to a validated model.

library(nlme)

covariate 1= age
M1<-lme(lung function~age +covariate1+covariate2+covariate3+ timesincepregnancy ,data=df5,random=~1+age|id, na.action=na.exclude, method ="ML", control=lmeControl(opt='optim'), corr=corExp(form=~age|id, nugget=T))

The following error occurs:

Error in logLik.reStruct(object, conLin) :
  NA/NaN/Inf in foreign function call (arg 3)

When I run it on random sample of 40 women it runs without an error. Is there any further processing I need to do on the data? I have already removed duplicates.

Previous studies have shown exponential correlation structure is the best fit for the data but I cannot seem to get this to work on the dataset I have.

All the best,
Seun


	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Tue Apr 19 16:51:23 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Tue, 19 Apr 2022 10:51:23 -0400
Subject: [R-sig-ME] Error in logLik.reStruct(object,
 conLin) : NA/NaN/Inf in foreign function call (arg 3)
 when Exponential Correlation added to model
In-Reply-To: <6bfb31653b2944e898c83bf751c557e2@liverpool.ac.uk>
References: <6bfb31653b2944e898c83bf751c557e2@liverpool.ac.uk>
Message-ID: <9e2115da-c764-007d-0a96-4ddd5855af8b@gmail.com>


   These are hard problems to debug.

    The error message means *some* numerical issue  has occurred, but 
without a reproducible example it's really hard to diagnose. None of 
these examples ends up being diagnosed, but some have advice about 
debugging/troubleshooting:

https://stackoverflow.com/questions/16488736/r-error-in-lme-function-na-nan-inf-in-foreign-function-call-arg-3

https://stackoverflow.com/questions/34900270/lme-na-nan-inf-in-foreign-function-call-arg-3


Slightly different: 
https://stat.ethz.ch/pipermail/r-help/2005-November/082151.html

https://stats.stackexchange.com/questions/129781/why-will-higher-order-of-polynomials-using-poly-function-solve-in-lme-model-an

   You could try glmmTMB:


glmmTMB(lung_function~age +covariate1+covariate2+covariate3+
        timesincepregnancy + (1+age|id) + ar1(0+age|id),
        data=df5, na.action=na.exclude)

  [glmmTMB will include an iid residual error term and hence a nugget 
term by default, unless you add `+0` to the overall model formula]

On 4/19/22 9:54 AM, Esan, Oluwaseun wrote:
> Dear List Members,
> 
> I am relatively new to R and Mixed Effects Models using LME.
> 
> I am unable to post sample data:
> 
> 
> I am trying to get the effect of pregnancy on lung function  and age on women with a certain genetic condition. We know lung function declines with age in people with the condition but we do not know the effect pregnancy has on this relationship.
> 
> I have repeat measurements of lung function measured up to 4 times in a year for up to 14 years for each woman (A total of 331,722 observations for 11668 women).
> 
> I am having problems when trying to add an exponential serial correlation to a validated model.
> 
> library(nlme)
> 
> covariate 1= age
> M1<-lme(lung function~age +covariate1+covariate2+covariate3+ timesincepregnancy ,data=df5,random=~1+age|id, na.action=na.exclude, method ="ML", control=lmeControl(opt='optim'), corr=corExp(form=~age|id, nugget=T))
> 
> The following error occurs:
> 
> Error in logLik.reStruct(object, conLin) :
>    NA/NaN/Inf in foreign function call (arg 3)
> 
> When I run it on random sample of 40 women it runs without an error. Is there any further processing I need to do on the data? I have already removed duplicates.
> 
> Previous studies have shown exponential correlation structure is the best fit for the data but I cannot seem to get this to work on the dataset I have.
> 
> All the best,
> Seun
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics


From bbo|ker @end|ng |rom gm@||@com  Tue Apr 19 17:43:19 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Tue, 19 Apr 2022 11:43:19 -0400
Subject: [R-sig-ME] Error in logLik.reStruct(object,
 conLin) : NA/NaN/Inf in foreign function call (arg 3)
 when Exponential Correlation added to model
In-Reply-To: <6bfb31653b2944e898c83bf751c557e2@liverpool.ac.uk>
References: <6bfb31653b2944e898c83bf751c557e2@liverpool.ac.uk>
Message-ID: <1c116634-7d2a-9575-55a1-def36b39e790@gmail.com>

   PS can you perhaps find a subset of women for which it *breaks*? e.g. 
by bisection:

   * run the model on the first half of the data (the first 5834 women)
   * run the model on the second half of the data

   see if either half gives the error.  If so, subdivide the 'bad' half 
of the data and continue ...

On 4/19/22 9:54 AM, Esan, Oluwaseun wrote:
> Dear List Members,
> 
> I am relatively new to R and Mixed Effects Models using LME.
> 
> I am unable to post sample data:
> 
> 
> I am trying to get the effect of pregnancy on lung function  and age on women with a certain genetic condition. We know lung function declines with age in people with the condition but we do not know the effect pregnancy has on this relationship.
>  
> I have repeat measurements of lung function measured up to 4 times in a year for up to 14 years for each woman (A total of 331,722 observations for 11668 women).
> 
> I am having problems when trying to add an exponential serial correlation to a validated model.
> 
> library(nlme)
> 
> covariate 1= age
> M1<-lme(lung function~age +covariate1+covariate2+covariate3+ timesincepregnancy ,data=df5,random=~1+age|id, na.action=na.exclude, method ="ML", control=lmeControl(opt='optim'), corr=corExp(form=~age|id, nugget=T))
> 
> The following error occurs:
> 
> Error in logLik.reStruct(object, conLin) :
>    NA/NaN/Inf in foreign function call (arg 3)
> 
> When I run it on random sample of 40 women it runs without an error. Is there any further processing I need to do on the data? I have already removed duplicates.
> 
> Previous studies have shown exponential correlation structure is the best fit for the data but I cannot seem to get this to work on the dataset I have.
> 
> All the best,
> Seun
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics


From m|r@nd@@||x @end|ng |rom gm@||@com  Thu Apr 28 23:32:12 2022
From: m|r@nd@@||x @end|ng |rom gm@||@com (Miranda Fix)
Date: Thu, 28 Apr 2022 14:32:12 -0700
Subject: [R-sig-ME] Shape parameter for Gamma glmer?
Message-ID: <CA+qDBRvWT0F6pf1e420xz3JqQsmdFf71XXhc0PxiHyXCn1oJLQ@mail.gmail.com>

Hi,

My understanding is that one can obtain the Gamma shape parameter from a
glmer object, let's say "mod", by 1/sigma(mod)^2. In other words, 1 over
the dispersion. However, for a glm the dispersion is only a crude estimate
and there is a gamma.shape function in the MASS package to provide a better
estimate of the shape parameter. So my question is twofold:

1) Is sigma(mod)^2 a good estimate of the dispersion in the glmer case?
(i.e., is 1/sigma(mod)^2 a good estimate of the shape?)

2) If not, is there an equivalent of MASS::gamma.shape for a glmer object
instead of a glm object?

Thanks so much for your help!

	[[alternative HTML version deleted]]


From |uc@@|e|@ten @end|ng |rom ru@n|  Thu Apr 28 08:39:07 2022
From: |uc@@|e|@ten @end|ng |rom ru@n| (Leisten, L. (Luca))
Date: Thu, 28 Apr 2022 06:39:07 +0000
Subject: [R-sig-ME] Help with General Additive Mixed Model
Message-ID: <1651127947300.84198@ru.nl>

Dear all,


I have a little bit of an off topic question regarding general additive mixed models (GAMM).


I am using the bam (or gam) function of the mgcv package in R and have some model specification and interpretation issues. As I have no experience with GAM models I was wondering if there is anyone around that has experience with them and could help me answer some questions?


Specifically my questions concern (1) the model specification of my model, as I get different results compared to a linear mixed model that includes poly terms, (2) the differences between the gam and bam function, as both give me different estimates, (3) and the difference between running a full model vs running two subsets as again, those give me different conclusions.


I would very much appreciate any help and would love a meeting if anyone has some time available to help! I'm not sure how the email list works, but in case you'd like to contact me directly my email is Luca.leisten at ru.nl.


Thanks for your help!


Best,

Luca Marie Leisten


	[[alternative HTML version deleted]]


From me @end|ng |rom ph||||p@|d@y@com  Fri Apr 29 19:20:42 2022
From: me @end|ng |rom ph||||p@|d@y@com (Phillip Alday)
Date: Fri, 29 Apr 2022 18:20:42 +0100
Subject: [R-sig-ME] Shape parameter for Gamma glmer?
In-Reply-To: <7b9e9a2f-feb2-ef1b-a445-3ef9d5cc79be@phillipalday.com>
References: <CA+qDBRvWT0F6pf1e420xz3JqQsmdFf71XXhc0PxiHyXCn1oJLQ@mail.gmail.com>
 <7b9e9a2f-feb2-ef1b-a445-3ef9d5cc79be@phillipalday.com>
Message-ID: <e458c2bc-64e9-c730-f384-0d13250e5eef@phillipalday.com>

Once again in plain text for the list
> On 4/28/22 16:32, Miranda Fix wrote:
> Hi,
>
> My understanding is that one can obtain the Gamma shape parameter from a
> glmer object, let's say "mod", by 1/sigma(mod)^2. In other words, 1 over
> the dispersion. However, for a glm the dispersion is only a crude estimate
> and there is a gamma.shape function in the MASS package to provide a better
> estimate of the shape parameter. So my question is twofold:
>
> 1) Is sigma(mod)^2 a good estimate of the dispersion in the glmer case?
> (i.e., is 1/sigma(mod)^2 a good estimate of the shape?)
> 

I suspect it's about the same quality as for glm() because ultimately
the dispersion is estimated in a fairly similar manner.

> 2) If not, is there an equivalent of MASS::gamma.shape for a glmer object
> instead of a glm object?
> 
Not to my knowledge, though looking at the source for
MASS::gamma.shape.glm I suspect that it would be trivial to modify it to
work with glmer. You would "just" need to change the bits that extract
the relevant components from the model (fitted values, deviance, etc),
and the rest should work. The only tricky bit is the residual degrees of
freedom, though maybe somebody here can provide an idea about that. (I'm
pretty weak on gamma regression, even in the non mixed case, to be honest.)

Phillip
> 
> 
>
> Thanks so much for your help!
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From doggene @end|ng |rom e@rth||nk@net  Fri Apr 29 19:32:49 2022
From: doggene @end|ng |rom e@rth||nk@net (Liz Hare)
Date: Fri, 29 Apr 2022 13:32:49 -0400
Subject: [R-sig-ME] Help with General Additive Mixed Model
In-Reply-To: <1651127947300.84198@ru.nl>
References: <1651127947300.84198@ru.nl>
Message-ID: <2C74D73B-4F40-4B5F-A77A-9421F38E4177@earthlink.net>



I found this book to be really helpful with GAMs and questions like your #3 about submodels.

Wood, S. N. Generalized Additive Models: an Introduction with R, CRC Press, New York (2017)

Liz

.


Liz Hare, PhD
Dog Genetics LLC
doggene at earthlink.net
http://www.doggenetics.com

> On Apr 28, 2022, at 2:39 AM, Leisten, L. (Luca) <luca.leisten at ru.nl> wrote:
> 
> Dear all,
> 
> 
> I have a little bit of an off topic question regarding general additive mixed models (GAMM).
> 
> 
> I am using the bam (or gam) function of the mgcv package in R and have some model specification and interpretation issues. As I have no experience with GAM models I was wondering if there is anyone around that has experience with them and could help me answer some questions?
> 
> 
> Specifically my questions concern (1) the model specification of my model, as I get different results compared to a linear mixed model that includes poly terms, (2) the differences between the gam and bam function, as both give me different estimates, (3) and the difference between running a full model vs running two subsets as again, those give me different conclusions.
> 
> 
> I would very much appreciate any help and would love a meeting if anyone has some time available to help! I'm not sure how the email list works, but in case you'd like to contact me directly my email is Luca.leisten at ru.nl.
> 
> 
> Thanks for your help!
> 
> 
> Best,
> 
> Luca Marie Leisten
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From v|ctor|@@w||||t@ @end|ng |rom gm@||@com  Fri Apr 29 20:57:49 2022
From: v|ctor|@@w||||t@ @end|ng |rom gm@||@com (Victoria Pattison-Willits)
Date: Fri, 29 Apr 2022 14:57:49 -0400
Subject: [R-sig-ME] Modelling non-negative non-zero continuous data
In-Reply-To: <CAPKijhpHbCMfJcr8r053OBotJYbAKZ8O_Yj8GgX7oPARks9OHg@mail.gmail.com>
References: <CAPKijhpHbCMfJcr8r053OBotJYbAKZ8O_Yj8GgX7oPARks9OHg@mail.gmail.com>
Message-ID: <CAPzcKfvk4npgKwuPr8az=14B1B0Z6T=gfuYgozr5DpShh88uXg@mail.gmail.com>

Dear all,

I am hoping you can help me. I am trying to model chick tarsi (leg length)
data. Briefly, I have mean measurements of tarsus length from 457 nests.
The data were collected across 31 sites (10 nests in each site) over a
six-year period so I have an *a priori *nested random effects structure:
(1|SITE_ID/BOX_NUMBER) + (1|YEAR). (Although  I have had to remove the
nested nest_box term due to convergence issues - there is a lot of variance
between nests within sites.)

The problem that I am running into is that the data is bound between the
values 12.73 and 20.12 mm. Both the data itself and the residuals from a
lmer model are left-skewed because the data is non-negative and non-zero.

The initial suite of models I have tried follows the below: I am running
models using both glmmTMB and lme (lmer).

(Also I have run the same models using the same length data for a bunch of
other response variables with no issues including various breeding outcomes
and chick measurements). Fixed covariates are scaled and centred: (sc.)
e.g.
```{r}
TL_BUILT_FULL_TMB2<-
glmmTMB(me_TARSUS~sc.BUILT_PERCENT+sc.GARDEN_PERCENT+sc.RINGING_AGE+sc.AprHatchDate+sc.BROOD_ATRINGING+(1|SITE_ID)+(1|YEAR),
data=DF_CHICK_TARSUS, family = gaussian)

summary(TL_BUILT_FULL_TMB2)
```
I am a little stumped as to what to do - I have run the same model using
reflected and log (and/or square root) transformed data - which does seem
to resolve the residual issues. However, I know that this is not the best
resolution and is rarely done, and transforming data even for the more
commonly found right-skewed data is increasingly discouraged. However, I am
not finding (and this may be me not using the correct terms in my search!)
any other options to overcome the issue of non-negative non-zero data -
plenty of advice for ecological data that is right-skewed or left-skewed
and zero-inflated!

If anyone can help me I would really appreciate it. Thank you all so much
as always in advance for your time and knowledge sharing. I am gradually
building up my competence in R and mixed modelling and this forum has been
really helpful on this steep learning curve! I am hoping I am just missing
something obvious! Please let me know if you need any other information
from me. Thank you!

Very best wishes.

Vicki Willits


>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Mon May  2 09:43:39 2022
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Mon, 2 May 2022 09:43:39 +0200
Subject: [R-sig-ME] Modelling non-negative non-zero continuous data
In-Reply-To: <CAPzcKfvk4npgKwuPr8az=14B1B0Z6T=gfuYgozr5DpShh88uXg@mail.gmail.com>
References: <CAPKijhpHbCMfJcr8r053OBotJYbAKZ8O_Yj8GgX7oPARks9OHg@mail.gmail.com>
 <CAPzcKfvk4npgKwuPr8az=14B1B0Z6T=gfuYgozr5DpShh88uXg@mail.gmail.com>
Message-ID: <CAJuCY5wmjfc1wwS2U_G4JORDPat=8WOTK0Y2WCNZDHaAHJnUXg@mail.gmail.com>

Dear Vicki,

When you have only one measurement per nest box, then you can't have
"nest box" as a random effect as it would confound with the residuals.
I recommend adding "year" as a fixed effect factor. I wrote a blog post on
the required number of levels for a random effect:
https://www.muscardinus.be/2018/09/number-random-effect-levels/
I presume you did an exploratory data analysis and handle covariates with
strong correlation. Do all covariates have a linear relationship with the
response?

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op vr 29 apr. 2022 om 20:58 schreef Victoria Pattison-Willits <
victoriaswillits at gmail.com>:

> Dear all,
>
> I am hoping you can help me. I am trying to model chick tarsi (leg length)
> data. Briefly, I have mean measurements of tarsus length from 457 nests.
> The data were collected across 31 sites (10 nests in each site) over a
> six-year period so I have an *a priori *nested random effects structure:
> (1|SITE_ID/BOX_NUMBER) + (1|YEAR). (Although  I have had to remove the
> nested nest_box term due to convergence issues - there is a lot of variance
> between nests within sites.)
>
> The problem that I am running into is that the data is bound between the
> values 12.73 and 20.12 mm. Both the data itself and the residuals from a
> lmer model are left-skewed because the data is non-negative and non-zero.
>
> The initial suite of models I have tried follows the below: I am running
> models using both glmmTMB and lme (lmer).
>
> (Also I have run the same models using the same length data for a bunch of
> other response variables with no issues including various breeding outcomes
> and chick measurements). Fixed covariates are scaled and centred: (sc.)
> e.g.
> ```{r}
> TL_BUILT_FULL_TMB2<-
>
> glmmTMB(me_TARSUS~sc.BUILT_PERCENT+sc.GARDEN_PERCENT+sc.RINGING_AGE+sc.AprHatchDate+sc.BROOD_ATRINGING+(1|SITE_ID)+(1|YEAR),
> data=DF_CHICK_TARSUS, family = gaussian)
>
> summary(TL_BUILT_FULL_TMB2)
> ```
> I am a little stumped as to what to do - I have run the same model using
> reflected and log (and/or square root) transformed data - which does seem
> to resolve the residual issues. However, I know that this is not the best
> resolution and is rarely done, and transforming data even for the more
> commonly found right-skewed data is increasingly discouraged. However, I am
> not finding (and this may be me not using the correct terms in my search!)
> any other options to overcome the issue of non-negative non-zero data -
> plenty of advice for ecological data that is right-skewed or left-skewed
> and zero-inflated!
>
> If anyone can help me I would really appreciate it. Thank you all so much
> as always in advance for your time and knowledge sharing. I am gradually
> building up my competence in R and mixed modelling and this forum has been
> really helpful on this steep learning curve! I am hoping I am just missing
> something obvious! Please let me know if you need any other information
> from me. Thank you!
>
> Very best wishes.
>
> Vicki Willits
>
>
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From v|ctor|@@w||||t@ @end|ng |rom gm@||@com  Mon May  2 12:52:01 2022
From: v|ctor|@@w||||t@ @end|ng |rom gm@||@com (Victoria Pattison-Willits)
Date: Mon, 2 May 2022 06:52:01 -0400
Subject: [R-sig-ME] Modelling non-negative non-zero continuous data
In-Reply-To: <CAJuCY5wmjfc1wwS2U_G4JORDPat=8WOTK0Y2WCNZDHaAHJnUXg@mail.gmail.com>
References: <CAPKijhpHbCMfJcr8r053OBotJYbAKZ8O_Yj8GgX7oPARks9OHg@mail.gmail.com>
 <CAPzcKfvk4npgKwuPr8az=14B1B0Z6T=gfuYgozr5DpShh88uXg@mail.gmail.com>
 <CAJuCY5wmjfc1wwS2U_G4JORDPat=8WOTK0Y2WCNZDHaAHJnUXg@mail.gmail.com>
Message-ID: <CAPzcKfsB6JA0dJjz8GzwVYP-egKqC-suicBOHcfVcoVR_i+Veg@mail.gmail.com>

Hi Thierry

Thank you so much for the advice and link to your blog post. I have
investigated all the different random effects structures including
modelling year as either a fixed or random variable. depending on the
response variable I  am dealing with, model fit and validation is most
often better modelled when it is a random effect ? but will certainly look
into modelling it as a fixed effect again. And to confirm Yes all
exploratory analyses were done and all covariates are fine! And all have a
linear relationship.

Thanks so much again your advice is most welcome and appreciated!

Best wishes

Vicki
On Mon, May 2, 2022 at 3:43 AM, Thierry Onkelinx <thierry.onkelinx at inbo.be>
wrote:

> Dear Vicki,
>
> When you have only one measurement per nest box, then you can't have
> "nest box" as a random effect as it would confound with the residuals.
> I recommend adding "year" as a fixed effect factor. I wrote a blog post on
> the required number of levels for a random effect:
> https://www.muscardinus.be/2018/09/number-random-effect-levels/
> I presume you did an exploratory data analysis and handle covariates with
> strong correlation. Do all covariates have a linear relationship with the
> response?
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88
> <https://www.google.com/maps/search/Havenlaan+88?entry=gmail&source=g>
> bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op vr 29 apr. 2022 om 20:58 schreef Victoria Pattison-Willits <
> victoriaswillits at gmail.com>:
>
>> Dear all,
>>
>> I am hoping you can help me. I am trying to model chick tarsi (leg length)
>> data. Briefly, I have mean measurements of tarsus length from 457 nests.
>> The data were collected across 31 sites (10 nests in each site) over a
>> six-year period so I have an *a priori *nested random effects structure:
>
>
>> (1|SITE_ID/BOX_NUMBER) + (1|YEAR). (Although  I have had to remove the
>> nested nest_box term due to convergence issues - there is a lot of
>> variance
>> between nests within sites.)
>>
>> The problem that I am running into is that the data is bound between the
>> values 12.73 and 20.12 mm. Both the data itself and the residuals from a
>> lmer model are left-skewed because the data is non-negative and non-zero.
>>
>> The initial suite of models I have tried follows the below: I am running
>> models using both glmmTMB and lme (lmer).
>>
>> (Also I have run the same models using the same length data for a bunch of
>> other response variables with no issues including various breeding
>> outcomes
>> and chick measurements). Fixed covariates are scaled and centred: (sc.)
>> e.g.
>> ```{r}
>> TL_BUILT_FULL_TMB2<-
>>
>> glmmTMB(me_TARSUS~sc.BUILT_PERCENT+sc.GARDEN_PERCENT+sc.RINGING_AGE+sc.AprHatchDate+sc.BROOD_ATRINGING+(1|SITE_ID)+(1|YEAR),
>> data=DF_CHICK_TARSUS, family = gaussian)
>>
>> summary(TL_BUILT_FULL_TMB2)
>> ```
>> I am a little stumped as to what to do - I have run the same model using
>> reflected and log (and/or square root) transformed data - which does seem
>> to resolve the residual issues. However, I know that this is not the best
>> resolution and is rarely done, and transforming data even for the more
>> commonly found right-skewed data is increasingly discouraged. However, I
>> am
>> not finding (and this may be me not using the correct terms in my search!)
>> any other options to overcome the issue of non-negative non-zero data -
>> plenty of advice for ecological data that is right-skewed or left-skewed
>> and zero-inflated!
>>
>> If anyone can help me I would really appreciate it. Thank you all so much
>> as always in advance for your time and knowledge sharing. I am gradually
>> building up my competence in R and mixed modelling and this forum has been
>> really helpful on this steep learning curve! I am hoping I am just missing
>> something obvious! Please let me know if you need any other information
>> from me. Thank you!
>>
>> Very best wishes.
>>
>> Vicki Willits
>>
>>
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >
>>
>>         [[alternative HTML version deleted]]
>
>
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From tr|chter @end|ng |rom un|-bremen@de  Thu May  5 14:01:30 2022
From: tr|chter @end|ng |rom un|-bremen@de (Tim Richter-Heitmann | Universitaet Bremen)
Date: Thu, 05 May 2022 14:01:30 +0200
Subject: [R-sig-ME] how to code a hopefully simple mixed model with
 spatially autocorrelated variables
Message-ID: <20220505140130.Horde.mq0_I-m0FFBiuzAUhpONXXc@webmail.uni-bremen.de>


Dear group,

i have the following data:

outcome: Abundance data (likely negative binomial or Poisson),  
potentially autocorrelated

environmental predictors: 8 continuous variables, potentially  
autocorrelated, potentially collinear.

A Spatial structure of the measurement: the depth of the measurement  
into a sediment ("Core Depth"). Continuous, in cm. It has only one  
dimension, like a gradient.

Location: a random effect. A factor with 7 levels with about 17 - 25  
observations each. The location of the measurement on the seafloor.  
These Locations are so far apart, that i cant imagine that they are  
autocorrelated. Thus, id like to use it as factor.

I would like to model something like:

gls(Abundance ~ Predictors + 1|Location, correlation = corGaus(~Depth))

Here are some questions:
1. Is this ok? Since depth is linear and progressive, can it be  
treated like a temporal structure? Or could Depth be also modelled as  
a fixed effect? It is clear from the data that abundance varies by  
Depth.
2. What would be the best way to select the best explaining variables?
3. How to get the negative binomial distribution of the outcome into  
the model?

I am very curious about your advise.

Best, Tim


-- 
Dr. Tim Richter-Heitmann

University of Bremen
Microbial Ecophysiology Group (AG Friedrich)
FB02 - Biologie/Chemie
Leobener Stra?e (NW2 A2130)
D-28359 Bremen
Tel.: 0049(0)421 218-63062
Fax: 0049(0)421 218-63069


From th|erry@onke||nx @end|ng |rom |nbo@be  Thu May  5 15:12:12 2022
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Thu, 5 May 2022 15:12:12 +0200
Subject: [R-sig-ME] how to code a hopefully simple mixed model with
 spatially autocorrelated variables
In-Reply-To: <20220505140130.Horde.mq0_I-m0FFBiuzAUhpONXXc@webmail.uni-bremen.de>
References: <20220505140130.Horde.mq0_I-m0FFBiuzAUhpONXXc@webmail.uni-bremen.de>
Message-ID: <CAJuCY5zx9gkhr2sihxQ0+zSRiCPf28=7Shrs_PvP-6OOCSW9RA@mail.gmail.com>

Dear Tim,

1. Time is a 1D correlation structure. Depth can be thought of as a 1D
correlation too. So you can use similar structures.
2. Model building is as much an art as a science. The full model should
make sense. Don't include variables for which you can't explain their
relevance. Avoid confounding variables. Sometimes you can reduce the
confounding by creating new variables based on the available variables. You
might want to contact a local statistician.
3. Start with Poisson. Switch to negative binomial in case of
overdispersion. Have a look at the packages glmmtmb and INLA. They provide
more distributions than nlme and allow correlated random effects (which you
want to model the depth).

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op do 5 mei 2022 om 14:06 schreef Tim Richter-Heitmann | Universitaet
Bremen <trichter at uni-bremen.de>:

>
> Dear group,
>
> i have the following data:
>
> outcome: Abundance data (likely negative binomial or Poisson),
> potentially autocorrelated
>
> environmental predictors: 8 continuous variables, potentially
> autocorrelated, potentially collinear.
>
> A Spatial structure of the measurement: the depth of the measurement
> into a sediment ("Core Depth"). Continuous, in cm. It has only one
> dimension, like a gradient.
>
> Location: a random effect. A factor with 7 levels with about 17 - 25
> observations each. The location of the measurement on the seafloor.
> These Locations are so far apart, that i cant imagine that they are
> autocorrelated. Thus, id like to use it as factor.
>
> I would like to model something like:
>
> gls(Abundance ~ Predictors + 1|Location, correlation = corGaus(~Depth))
>
> Here are some questions:
> 1. Is this ok? Since depth is linear and progressive, can it be
> treated like a temporal structure? Or could Depth be also modelled as
> a fixed effect? It is clear from the data that abundance varies by
> Depth.
> 2. What would be the best way to select the best explaining variables?
> 3. How to get the negative binomial distribution of the outcome into
> the model?
>
> I am very curious about your advise.
>
> Best, Tim
>
>
> --
> Dr. Tim Richter-Heitmann
>
> University of Bremen
> Microbial Ecophysiology Group (AG Friedrich)
> FB02 - Biologie/Chemie
> Leobener Stra?e (NW2 A2130)
> D-28359 Bremen
> Tel.: 0049(0)421 218-63062
> Fax: 0049(0)421 218-63069
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From o||verhooker @end|ng |rom pr@t@t|@t|c@@com  Fri May  6 13:15:49 2022
From: o||verhooker @end|ng |rom pr@t@t|@t|c@@com (Oliver Hooker)
Date: Fri, 6 May 2022 12:15:49 +0100
Subject: [R-sig-ME] =?utf-8?q?ONLINE_COURSE_=E2=80=93_Introduction_To_Mix?=
	=?utf-8?q?ed_Models_Using_R_And_Rstudio_=28IMMR06=29?=
Message-ID: <CAEsSYzwSf9z0AfQJWKB_-=MWDe25MRJJzQgPGcK+fDue8bPmjQ@mail.gmail.com>

ONLINE COURSE ? Introduction To Mixed Models Using R And Rstudio (IMMR06)
This course will be delivered live

https://www.prstatistics.com/course/introduction-to-mixed-models-using-r-and-rstudio-immr06/

Please feel free to share!


ABOUT THIS COURSE

In this two day course, we provide a comprehensive practical and
theoretical introduction to multilevel models, also known as hierarchical
or mixed effects models. We will focus primarily on multilevel linear
models, but also cover multilevel generalized linear models. Likewise, we
will also describe Bayesian approaches to multilevel modelling. On Day 1,
we will begin by focusing on random effects multilevel models. These models
make it clear how multilevel models are in fact models of models. In
addition, random effects models serve as a solid basis for understanding
mixed effects, i.e. fixed and random effects, models. In this coverage of
random effects, we will also cover the important concepts of statistical
shrinkage in the estimation of effects, as well as intraclass correlation.
We then proceed to cover linear mixed effects models, particularly focusing
on varying intercept and/or varying slopes regresssion models. On Day 2, we
cover further aspects of linear mixed effects models, including multilevel
models for nested and crossed data data, and group level predictor
variables. On Day 2, we also cover Bayesian approaches to multilevel levels
using the brms R package.

Please email oliverhooker at prstatistics.com.

-- 

Oliver Hooker PhD.
PR statistics

	[[alternative HTML version deleted]]


From tr|chter @end|ng |rom un|-bremen@de  Mon May  9 15:41:37 2022
From: tr|chter @end|ng |rom un|-bremen@de (Tim Richter-Heitmann)
Date: Mon, 9 May 2022 15:41:37 +0200
Subject: [R-sig-ME] 
 how to code a mixed model with spatially autocorrelated
 variables in glmmTMB
In-Reply-To: <CAJuCY5zx9gkhr2sihxQ0+zSRiCPf28=7Shrs_PvP-6OOCSW9RA@mail.gmail.com>
References: <20220505140130.Horde.mq0_I-m0FFBiuzAUhpONXXc@webmail.uni-bremen.de>
 <CAJuCY5zx9gkhr2sihxQ0+zSRiCPf28=7Shrs_PvP-6OOCSW9RA@mail.gmail.com>
Message-ID: <1707a49b-712f-b0bf-dd6e-80bc259dff74@uni-bremen.de>

Dear group,

dear Thierry,


thank you very much for your valuable input.

Before i am going to ask my question, here is a reminder of my dataset:


i have the following data:

/outcome: Abundance data (likely negative binomial or Poisson), //
//potentially autocorrelated//
////
//environmental predictors: 8 continuous variables, potentially //
//autocorrelated, potentially collinear.//
////
//A Spatial structure of the measurement: the depth of the measurement //
//into a sediment ("Core Depth"). Continuous, in cm. It has only one //
//dimension, like a gradient.//
////
//Location: a random effect. A factor with 7 levels with about 17 - 25 //
//observations each. The location of the measurement on the seafloor. //
//These Locations are so far apart, that i cant imagine that they are //
//autocorrelated. Thus, id like to use it as factor./

In nlme, I have coded the model like:

mod.0 <- lme(outcome~ environmental predictors,
 ??????????????????????????????? random = ~ 1|Location,
 ??????????????????????????????? correlation = corLin(form = ~ Depth),
 ??????????????????????????????? method="ML",
 ??????????????????????????????? data=total)

(Depth as another predictor variable never improved the models, so i 
stick to just formulate them as a correlation structure; corLin yielded 
better AICs than CorAR1; i also checked collinearity and there is none).

However, the residual plots cleary show some heteroscedasticity towards 
higher values, so i would like to switch to another family to model the 
abundance (as they are counts).

In glmmTMB, i could code

mod.1 <- glmmTMB(Abundance ~ Some continous predictors+ (1|Core), 
data=total, family=nbinom1)

and

mod.2 <- glmmTMB(Abundance ~ ar1(as.factor(Depth) + 0 | Core), 
data=total, family=nbinom1)

(Note: mod.1 did not converge)

Here are? my questions:

1. How can i combine both right hand sites in glmmTMB to have a model 
that encompasses all parameters, just like mod.0 in mlme?

(it is that just concatening them yields either an error or ignores one 
half of the model)

2. glmmTMB requires coordinates for spatial structures. I can code Depth 
with an "all-zero"-Coordinate to have binary coordinates. Is this better 
than using ar1?

Thanks for your advice and input!

Cheers, Tim





Am 05.05.2022 um 15:12 schrieb Thierry Onkelinx:
> Dear Tim,
>
> 1. Time is a 1D correlation structure. Depth can be thought of as a 1D 
> correlation too. So you can use similar structures.
> 2. Model building is as much an?art as a science. The full model 
> should make sense. Don't include variables for which you can't explain 
> their relevance. Avoid confounding variables. Sometimes you can reduce 
> the confounding by creating new variables based on the available 
> variables. You might want to contact a local statistician.
> 3. Start with Poisson. Switch to negative binomial in case of 
> overdispersion. Have a look at the packages glmmtmb and INLA. They 
> provide more distributions than nlme and allow correlated random 
> effects (which you want to model the depth).
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE 
> AND FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be <http://www.inbo.be>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no 
> more than asking him to perform a post-mortem examination: he may be 
> able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does 
> not ensure that a reasonable answer can be extracted from a given body 
> of data. ~ John Tukey
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op do 5 mei 2022 om 14:06 schreef Tim Richter-Heitmann | Universitaet 
> Bremen <trichter at uni-bremen.de>:
>
>
>     Dear group,
>
>     i have the following data:
>
>     outcome: Abundance data (likely negative binomial or Poisson),
>     potentially autocorrelated
>
>     environmental predictors: 8 continuous variables, potentially
>     autocorrelated, potentially collinear.
>
>     A Spatial structure of the measurement: the depth of the measurement
>     into a sediment ("Core Depth"). Continuous, in cm. It has only one
>     dimension, like a gradient.
>
>     Location: a random effect. A factor with 7 levels with about 17 - 25
>     observations each. The location of the measurement on the seafloor.
>     These Locations are so far apart, that i cant imagine that they are
>     autocorrelated. Thus, id like to use it as factor.
>
>     I would like to model something like:
>
>     gls(Abundance ~ Predictors + 1|Location, correlation =
>     corGaus(~Depth))
>
>     Here are some questions:
>     1. Is this ok? Since depth is linear and progressive, can it be
>     treated like a temporal structure? Or could Depth be also modelled as
>     a fixed effect? It is clear from the data that abundance varies by
>     Depth.
>     2. What would be the best way to select the best explaining variables?
>     3. How to get the negative binomial distribution of the outcome into
>     the model?
>
>     I am very curious about your advise.
>
>     Best, Tim
>
>
>     -- 
>     Dr. Tim Richter-Heitmann
>
>     University of Bremen
>     Microbial Ecophysiology Group (AG Friedrich)
>     FB02 - Biologie/Chemie
>     Leobener Stra?e (NW2 A2130)
>     D-28359 Bremen
>     Tel.: 0049(0)421 218-63062
>     Fax: 0049(0)421 218-63069
>
>     _______________________________________________
>     R-sig-mixed-models at r-project.org mailing list
>     https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
-- 
Dr. Tim Richter-Heitmann

University of Bremen
Microbial Ecophysiology Group (AG Friedrich)
FB02 - Biologie/Chemie
Leobener Stra?e (NW2 A2130)
D-28359 Bremen
Tel.: 0049(0)421 218-63062
Fax: 0049(0)421 218-63069

	[[alternative HTML version deleted]]


From tr|chter @end|ng |rom un|-bremen@de  Mon May  9 15:43:25 2022
From: tr|chter @end|ng |rom un|-bremen@de (Tim Richter-Heitmann)
Date: Mon, 9 May 2022 15:43:25 +0200
Subject: [R-sig-ME] 
 how to code a mixed model with spatially autocorrelated
 variables in glmmTMB - erratum
In-Reply-To: <1707a49b-712f-b0bf-dd6e-80bc259dff74@uni-bremen.de>
References: <20220505140130.Horde.mq0_I-m0FFBiuzAUhpONXXc@webmail.uni-bremen.de>
 <CAJuCY5zx9gkhr2sihxQ0+zSRiCPf28=7Shrs_PvP-6OOCSW9RA@mail.gmail.com>
 <1707a49b-712f-b0bf-dd6e-80bc259dff74@uni-bremen.de>
Message-ID: <8ca58df7-4ef5-ecb2-0fac-500b73a3d48a@uni-bremen.de>

In mod.1 and mod.2, "Core" should read "Location".

Am 09.05.2022 um 15:41 schrieb Tim Richter-Heitmann:
> Dear group,
>
> dear Thierry,
>
>
> thank you very much for your valuable input.
>
> Before i am going to ask my question, here is a reminder of my dataset:
>
>
> i have the following data:
>
> /outcome: Abundance data (likely negative binomial or Poisson), //
> //potentially autocorrelated//
> ////
> //environmental predictors: 8 continuous variables, potentially //
> //autocorrelated, potentially collinear.//
> ////
> //A Spatial structure of the measurement: the depth of the measurement //
> //into a sediment ("Core Depth"). Continuous, in cm. It has only one //
> //dimension, like a gradient.//
> ////
> //Location: a random effect. A factor with 7 levels with about 17 - 25 //
> //observations each. The location of the measurement on the seafloor. //
> //These Locations are so far apart, that i cant imagine that they are //
> //autocorrelated. Thus, id like to use it as factor./
>
> In nlme, I have coded the model like:
>
> mod.0 <- lme(outcome~ environmental predictors,
>   ??????????????????????????????? random = ~ 1|Location,
>   ??????????????????????????????? correlation = corLin(form = ~ Depth),
>   ??????????????????????????????? method="ML",
>   ??????????????????????????????? data=total)
>
> (Depth as another predictor variable never improved the models, so i
> stick to just formulate them as a correlation structure; corLin yielded
> better AICs than CorAR1; i also checked collinearity and there is none).
>
> However, the residual plots cleary show some heteroscedasticity towards
> higher values, so i would like to switch to another family to model the
> abundance (as they are counts).
>
> In glmmTMB, i could code
>
> mod.1 <- glmmTMB(Abundance ~ Some continous predictors+ (1|Core),
> data=total, family=nbinom1)
>
> and
>
> mod.2 <- glmmTMB(Abundance ~ ar1(as.factor(Depth) + 0 | Core),
> data=total, family=nbinom1)
>
> (Note: mod.1 did not converge)
>
> Here are? my questions:
>
> 1. How can i combine both right hand sites in glmmTMB to have a model
> that encompasses all parameters, just like mod.0 in mlme?
>
> (it is that just concatening them yields either an error or ignores one
> half of the model)
>
> 2. glmmTMB requires coordinates for spatial structures. I can code Depth
> with an "all-zero"-Coordinate to have binary coordinates. Is this better
> than using ar1?
>
> Thanks for your advice and input!
>
> Cheers, Tim
>
>
>
>
>
> Am 05.05.2022 um 15:12 schrieb Thierry Onkelinx:
>> Dear Tim,
>>
>> 1. Time is a 1D correlation structure. Depth can be thought of as a 1D
>> correlation too. So you can use similar structures.
>> 2. Model building is as much an?art as a science. The full model
>> should make sense. Don't include variables for which you can't explain
>> their relevance. Avoid confounding variables. Sometimes you can reduce
>> the confounding by creating new variables based on the available
>> variables. You might want to contact a local statistician.
>> 3. Start with Poisson. Switch to negative binomial in case of
>> overdispersion. Have a look at the packages glmmtmb and INLA. They
>> provide more distributions than nlme and allow correlated random
>> effects (which you want to model the depth).
>>
>> Best regards,
>>
>> ir. Thierry Onkelinx
>> Statisticus / Statistician
>>
>> Vlaamse Overheid / Government of Flanders
>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>> AND FOREST
>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>> thierry.onkelinx at inbo.be
>> Havenlaan 88 bus 73, 1000 Brussel
>> www.inbo.be <http://www.inbo.be>
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>> To call in the statistician after the experiment is done may be no
>> more than asking him to perform a post-mortem examination: he may be
>> able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does
>> not ensure that a reasonable answer can be extracted from a given body
>> of data. ~ John Tukey
>> ///////////////////////////////////////////////////////////////////////////////////////////
>>
>> <https://www.inbo.be>
>>
>>
>> Op do 5 mei 2022 om 14:06 schreef Tim Richter-Heitmann | Universitaet
>> Bremen <trichter at uni-bremen.de>:
>>
>>
>>      Dear group,
>>
>>      i have the following data:
>>
>>      outcome: Abundance data (likely negative binomial or Poisson),
>>      potentially autocorrelated
>>
>>      environmental predictors: 8 continuous variables, potentially
>>      autocorrelated, potentially collinear.
>>
>>      A Spatial structure of the measurement: the depth of the measurement
>>      into a sediment ("Core Depth"). Continuous, in cm. It has only one
>>      dimension, like a gradient.
>>
>>      Location: a random effect. A factor with 7 levels with about 17 - 25
>>      observations each. The location of the measurement on the seafloor.
>>      These Locations are so far apart, that i cant imagine that they are
>>      autocorrelated. Thus, id like to use it as factor.
>>
>>      I would like to model something like:
>>
>>      gls(Abundance ~ Predictors + 1|Location, correlation =
>>      corGaus(~Depth))
>>
>>      Here are some questions:
>>      1. Is this ok? Since depth is linear and progressive, can it be
>>      treated like a temporal structure? Or could Depth be also modelled as
>>      a fixed effect? It is clear from the data that abundance varies by
>>      Depth.
>>      2. What would be the best way to select the best explaining variables?
>>      3. How to get the negative binomial distribution of the outcome into
>>      the model?
>>
>>      I am very curious about your advise.
>>
>>      Best, Tim
>>
>>
>>      --
>>      Dr. Tim Richter-Heitmann
>>
>>      University of Bremen
>>      Microbial Ecophysiology Group (AG Friedrich)
>>      FB02 - Biologie/Chemie
>>      Leobener Stra?e (NW2 A2130)
>>      D-28359 Bremen
>>      Tel.: 0049(0)421 218-63062
>>      Fax: 0049(0)421 218-63069
>>
>>      _______________________________________________
>>      R-sig-mixed-models at r-project.org mailing list
>>      https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
-- 
Dr. Tim Richter-Heitmann

University of Bremen
Microbial Ecophysiology Group (AG Friedrich)
FB02 - Biologie/Chemie
Leobener Stra?e (NW2 A2130)
D-28359 Bremen
Tel.: 0049(0)421 218-63062
Fax: 0049(0)421 218-63069


From th|erry@onke||nx @end|ng |rom |nbo@be  Mon May  9 18:52:48 2022
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Mon, 9 May 2022 18:52:48 +0200
Subject: [R-sig-ME] 
 how to code a mixed model with spatially autocorrelated
 variables in glmmTMB
In-Reply-To: <1707a49b-712f-b0bf-dd6e-80bc259dff74@uni-bremen.de>
References: <20220505140130.Horde.mq0_I-m0FFBiuzAUhpONXXc@webmail.uni-bremen.de>
 <CAJuCY5zx9gkhr2sihxQ0+zSRiCPf28=7Shrs_PvP-6OOCSW9RA@mail.gmail.com>
 <1707a49b-712f-b0bf-dd6e-80bc259dff74@uni-bremen.de>
Message-ID: <CAJuCY5xafYWa4o1_zYLf-zhNa545qJPvAc_4jVU=V+HJwJFCnQ@mail.gmail.com>

Dear Tim,

Note that you need the normalised residuals because those take the
correlation structure and variance structure into account.

I use INLA for more complex models. So I can't help you that much with the
details of glmmTMB. Without the data it is hard to tell why a model doesn't
converge.

Setting a dummy dimension to a fixed value is a trick that I use to handle
unidirectional spatial structures with packages that require two dimensions.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op ma 9 mei 2022 om 15:42 schreef Tim Richter-Heitmann <
trichter at uni-bremen.de>:

> Dear group,
>
> dear Thierry,
>
>
> thank you very much for your valuable input.
>
> Before i am going to ask my question, here is a reminder of my dataset:
>
>
> i have the following data:
>
> /outcome: Abundance data (likely negative binomial or Poisson), //
> //potentially autocorrelated//
> ////
> //environmental predictors: 8 continuous variables, potentially //
> //autocorrelated, potentially collinear.//
> ////
> //A Spatial structure of the measurement: the depth of the measurement //
> //into a sediment ("Core Depth"). Continuous, in cm. It has only one //
> //dimension, like a gradient.//
> ////
> //Location: a random effect. A factor with 7 levels with about 17 - 25 //
> //observations each. The location of the measurement on the seafloor. //
> //These Locations are so far apart, that i cant imagine that they are //
> //autocorrelated. Thus, id like to use it as factor./
>
> In nlme, I have coded the model like:
>
> mod.0 <- lme(outcome~ environmental predictors,
>                                  random = ~ 1|Location,
>                                  correlation = corLin(form = ~ Depth),
>                                  method="ML",
>                                  data=total)
>
> (Depth as another predictor variable never improved the models, so i
> stick to just formulate them as a correlation structure; corLin yielded
> better AICs than CorAR1; i also checked collinearity and there is none).
>
> However, the residual plots cleary show some heteroscedasticity towards
> higher values, so i would like to switch to another family to model the
> abundance (as they are counts).
>
> In glmmTMB, i could code
>
> mod.1 <- glmmTMB(Abundance ~ Some continous predictors+ (1|Core),
> data=total, family=nbinom1)
>
> and
>
> mod.2 <- glmmTMB(Abundance ~ ar1(as.factor(Depth) + 0 | Core),
> data=total, family=nbinom1)
>
> (Note: mod.1 did not converge)
>
> Here are  my questions:
>
> 1. How can i combine both right hand sites in glmmTMB to have a model
> that encompasses all parameters, just like mod.0 in mlme?
>
> (it is that just concatening them yields either an error or ignores one
> half of the model)
>
> 2. glmmTMB requires coordinates for spatial structures. I can code Depth
> with an "all-zero"-Coordinate to have binary coordinates. Is this better
> than using ar1?
>
> Thanks for your advice and input!
>
> Cheers, Tim
>
>
>
>
>
> Am 05.05.2022 um 15:12 schrieb Thierry Onkelinx:
> > Dear Tim,
> >
> > 1. Time is a 1D correlation structure. Depth can be thought of as a 1D
> > correlation too. So you can use similar structures.
> > 2. Model building is as much an art as a science. The full model
> > should make sense. Don't include variables for which you can't explain
> > their relevance. Avoid confounding variables. Sometimes you can reduce
> > the confounding by creating new variables based on the available
> > variables. You might want to contact a local statistician.
> > 3. Start with Poisson. Switch to negative binomial in case of
> > overdispersion. Have a look at the packages glmmtmb and INLA. They
> > provide more distributions than nlme and allow correlated random
> > effects (which you want to model the depth).
> >
> > Best regards,
> >
> > ir. Thierry Onkelinx
> > Statisticus / Statistician
> >
> > Vlaamse Overheid / Government of Flanders
> > INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
> > AND FOREST
> > Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> > thierry.onkelinx at inbo.be
> > Havenlaan 88 bus 73, 1000 Brussel
> > www.inbo.be <http://www.inbo.be>
> >
> >
> ///////////////////////////////////////////////////////////////////////////////////////////
> > To call in the statistician after the experiment is done may be no
> > more than asking him to perform a post-mortem examination: he may be
> > able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> > The plural of anecdote is not data. ~ Roger Brinner
> > The combination of some data and an aching desire for an answer does
> > not ensure that a reasonable answer can be extracted from a given body
> > of data. ~ John Tukey
> >
> ///////////////////////////////////////////////////////////////////////////////////////////
> >
> > <https://www.inbo.be>
> >
> >
> > Op do 5 mei 2022 om 14:06 schreef Tim Richter-Heitmann | Universitaet
> > Bremen <trichter at uni-bremen.de>:
> >
> >
> >     Dear group,
> >
> >     i have the following data:
> >
> >     outcome: Abundance data (likely negative binomial or Poisson),
> >     potentially autocorrelated
> >
> >     environmental predictors: 8 continuous variables, potentially
> >     autocorrelated, potentially collinear.
> >
> >     A Spatial structure of the measurement: the depth of the measurement
> >     into a sediment ("Core Depth"). Continuous, in cm. It has only one
> >     dimension, like a gradient.
> >
> >     Location: a random effect. A factor with 7 levels with about 17 - 25
> >     observations each. The location of the measurement on the seafloor.
> >     These Locations are so far apart, that i cant imagine that they are
> >     autocorrelated. Thus, id like to use it as factor.
> >
> >     I would like to model something like:
> >
> >     gls(Abundance ~ Predictors + 1|Location, correlation =
> >     corGaus(~Depth))
> >
> >     Here are some questions:
> >     1. Is this ok? Since depth is linear and progressive, can it be
> >     treated like a temporal structure? Or could Depth be also modelled as
> >     a fixed effect? It is clear from the data that abundance varies by
> >     Depth.
> >     2. What would be the best way to select the best explaining
> variables?
> >     3. How to get the negative binomial distribution of the outcome into
> >     the model?
> >
> >     I am very curious about your advise.
> >
> >     Best, Tim
> >
> >
> >     --
> >     Dr. Tim Richter-Heitmann
> >
> >     University of Bremen
> >     Microbial Ecophysiology Group (AG Friedrich)
> >     FB02 - Biologie/Chemie
> >     Leobener Stra?e (NW2 A2130)
> >     D-28359 Bremen
> >     Tel.: 0049(0)421 218-63062
> >     Fax: 0049(0)421 218-63069
> >
> >     _______________________________________________
> >     R-sig-mixed-models at r-project.org mailing list
> >     https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
> --
> Dr. Tim Richter-Heitmann
>
> University of Bremen
> Microbial Ecophysiology Group (AG Friedrich)
> FB02 - Biologie/Chemie
> Leobener Stra?e (NW2 A2130)
> D-28359 Bremen
> Tel.: 0049(0)421 218-63062
> Fax: 0049(0)421 218-63069
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From emm@nue|@cur|@ @end|ng |rom p@r|@de@c@rte@@|r  Thu May 12 09:29:34 2022
From: emm@nue|@cur|@ @end|ng |rom p@r|@de@c@rte@@|r (Emmanuel Curis)
Date: Thu, 12 May 2022 09:29:34 +0200
Subject: [R-sig-ME] Point estimate outside of its confidence interval with
 lmer
Message-ID: <Yny3XvrTuYBKUPb8@info124.pharmacie.univ-paris5.fr>

Hello,

I have encountered an unexpected result for some datasets when using
confint after fitting a model with lmer: the confidence intervals for
the standard deviations in the model did not included the point
estimate, given by summary for instance.

I think the problem is a mix of small sample size and REML vs ML, but
I would be happy to have confirmation that my interpretation is
correct, since I'm not very familiar with profiling and REML vs
ML... and wonder if something more problematic is occuring.

My interpretation is that lmer fits using REML by default, hence the
variances are estimated "unbiased" (the equivalent of dividing by n -
k for the residual variance of a linear model, but I'm not sure
exactly what is n and k for random effects variance estimation).  But
when confint is called, using profile, the ML profile is used, hence
variance confidence intervals are estimated "biased" (the equivalent
of dividing by n).  However, when n is small, dividing by n - k and n
may give very different results, hence in some cases the profiled
confidence interval for ML estimate does not include the REML
estimate, for the standard deviations.  I guess in practice the
difference between REML and ML is more complex than just using n or
n-k, but would it be idea?

Support for this is that
 1) when using bootstrap, there seems to be no such discrepancy
 2) when fitting using REML = FALSE, the profiled IC is the same and
    does include the (different) point estimate

Does it sound correct?

Additionnal questions, if this interpretation is correct:
 - would it make sense to make confidence intervals based on REML
   profiles, and not ML profiles? if so, how?

 - wouldn't a warning be a good idea when point estimates are outside
   CI, with the explaination if it is indeed REML vs ML?

 - if this is indeed a "small sample size" problem, I guess in such
   cases any asymptotic result is difficult to trust, right?  Does it
   mean profiled interval cannot be trusted also, neither nested
   models tests, and that only bootstrap may be used?
   
 - in such cases, is there any argument to prefer REML over ML or
   vice-versa?

Thanks in advance for your help,

-- 
                                Emmanuel CURIS
                                emmanuel.curis at parisdescartes.fr

Page WWW: http://emmanuel.curis.online.fr/index.html


From wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n|  Thu May 12 13:01:30 2022
From: wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n| (Viechtbauer, Wolfgang (NP))
Date: Thu, 12 May 2022 11:01:30 +0000
Subject: [R-sig-ME] 
 Point estimate outside of its confidence interval with lmer
In-Reply-To: <Yny3XvrTuYBKUPb8@info124.pharmacie.univ-paris5.fr>
References: <Yny3XvrTuYBKUPb8@info124.pharmacie.univ-paris5.fr>
Message-ID: <fb8d6bb938da49f5a337dc3d83d92086@UM-MAIL3214.unimaas.nl>

Dear Emmanuel,

Please see below for my responses.

Best,
Wolfgang

>-----Original Message-----
>From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org] On
>Behalf Of Emmanuel Curis
>Sent: Thursday, 12 May, 2022 9:30
>To: r-sig-mixed-models at r-project.org
>Subject: [R-sig-ME] Point estimate outside of its confidence interval with lmer
>
>Hello,
>
>I have encountered an unexpected result for some datasets when using confint
>after fitting a model with lmer: the confidence intervals for the standard
>deviations in the model did not included the point estimate, given by summary
>for instance.
>
>I think the problem is a mix of small sample size and REML vs ML, but I would
>be happy to have confirmation that my interpretation is correct, since I'm
>not very familiar with profiling and REML vs ML... and wonder if something
>more problematic is occuring.
>
>My interpretation is that lmer fits using REML by default,

Correct.

>hence the variances are estimated "unbiased" (the equivalent of dividing by n
>- k for the residual variance of a linear model, but I'm not sure exactly
>what is n and k for random effects variance estimation).

Only for some very special cases can we show that the variance components are
estimated unbiasedly. However, yes, REML estimation tends to provide estimates
of variance components that are approximately unbiased in many cases.

Also, for more complex models, changing a ML estimate into a REML one isn't
simply a multiplication of the ML estimate by some factor that depends on
things like the sample size (and in more complex models, number of clusters)
and the number of parameters.

>But when confint is called, using profile, the ML profile is used, hence
>variance confidence intervals are estimated "biased" (the equivalent
>of dividing by n).  However, when n is small, dividing by n - k and n
>may give very different results, hence in some cases the profiled
>confidence interval for ML estimate does not include the REML
>estimate, for the standard deviations.  I guess in practice the
>difference between REML and ML is more complex than just using n or
>n-k, but would it be idea?

Correct - see above. So I wouldn't just heuristically apply some 'correction'
that only comes from simple regression models.

Actually, when constructing a profile likelihood CI for a variance component
(which is what confint() does by default for lmer model objects), one can also
profile the restricted log likelihood function, which would guarantee that the
estimate falls into the CI. I actually thought that this is what confint()
does for lmer objects (fitted with REML), but I checked and you are correct -
even when the model was fitted with REML, all profile likelihood CIs
(including those for variance components) are based on the regular likelihood.

Without a reproducible example, I cannot tell you whether this is really the
reason for the estimate falling outside of the CI, but it certainly could be.

>Support for this is that
> 1) when using bootstrap, there seems to be no such discrepancy
> 2) when fitting using REML = FALSE, the profiled IC is the same and
>    does include the (different) point estimate
>
>Does it sound correct?

Not sure how 1) helps to diagnose this, but 2) certainly provides support for
the hypothesis that the discrepancy arises from different likelihoods being
used for estimation and profiling.

>Additionnal questions, if this interpretation is correct:
> - would it make sense to make confidence intervals based on REML
>   profiles, and not ML profiles? if so, how?

As noted above, this can be done for variance components, but I don't see a
way of doing this with confint() for lmer model objects.

> - wouldn't a warning be a good idea when point estimates are outside
>   CI, with the explaination if it is indeed REML vs ML?

That's up to the developers of lme4 to decide.

> - if this is indeed a "small sample size" problem, I guess in such
>   cases any asymptotic result is difficult to trust, right?  Does it
>   mean profiled interval cannot be trusted also, neither nested
>   models tests, and that only bootstrap may be used?

Profiling the likelihood relies on the asymptotic behavior of the likelihood
ratio test. So yes, profiling likelihood CIs may not have a nominal coverage
rate in small samples (for some definition of 'small', which is context
dependent).

> - in such cases, is there any argument to prefer REML over ML or
>   vice-versa?

One can make arguments in favor of both ML and REML. REML tends to provide
approximately unbiased estimates, but with larger mean-squared error, while ML
tends to provide negative biased estimates, but with smaller MSE.

>Thanks in advance for your help,
>
>--
>                                Emmanuel CURIS
>                                emmanuel.curis at parisdescartes.fr
>
>Page WWW: http://emmanuel.curis.online.fr/index.html


From bbo|ker @end|ng |rom gm@||@com  Thu May 12 15:54:56 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Thu, 12 May 2022 09:54:56 -0400
Subject: [R-sig-ME] 
 Point estimate outside of its confidence interval with lmer
In-Reply-To: <fb8d6bb938da49f5a337dc3d83d92086@UM-MAIL3214.unimaas.nl>
References: <Yny3XvrTuYBKUPb8@info124.pharmacie.univ-paris5.fr>
 <fb8d6bb938da49f5a337dc3d83d92086@UM-MAIL3214.unimaas.nl>
Message-ID: <6f8cb2d0-f5cb-8198-c991-e39414928692@gmail.com>

   For what it's worth, there is a (relatively ancient) Github issue 
discussing this:

https://github.com/lme4/lme4/issues/325

   Further evidence that we ought to add a warning, or at least a 
message, saying that the profiles are based on the ML fit.

   Can anyone point me to some theory that supports/describes the 
process of constructing CIs based on the profile of the REML statistic ... ?

   cheers
    Ben Bolker


On 2022-05-12 7:01 a.m., Viechtbauer, Wolfgang (NP) wrote:
> Dear Emmanuel,
> 
> Please see below for my responses.
> 
> Best,
> Wolfgang
> 
>> -----Original Message-----
>> From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org] On
>> Behalf Of Emmanuel Curis
>> Sent: Thursday, 12 May, 2022 9:30
>> To: r-sig-mixed-models at r-project.org
>> Subject: [R-sig-ME] Point estimate outside of its confidence interval with lmer
>>
>> Hello,
>>
>> I have encountered an unexpected result for some datasets when using confint
>> after fitting a model with lmer: the confidence intervals for the standard
>> deviations in the model did not included the point estimate, given by summary
>> for instance.
>>
>> I think the problem is a mix of small sample size and REML vs ML, but I would
>> be happy to have confirmation that my interpretation is correct, since I'm
>> not very familiar with profiling and REML vs ML... and wonder if something
>> more problematic is occuring.
>>
>> My interpretation is that lmer fits using REML by default,
> 
> Correct.
> 
>> hence the variances are estimated "unbiased" (the equivalent of dividing by n
>> - k for the residual variance of a linear model, but I'm not sure exactly
>> what is n and k for random effects variance estimation).
> 
> Only for some very special cases can we show that the variance components are
> estimated unbiasedly. However, yes, REML estimation tends to provide estimates
> of variance components that are approximately unbiased in many cases.
> 
> Also, for more complex models, changing a ML estimate into a REML one isn't
> simply a multiplication of the ML estimate by some factor that depends on
> things like the sample size (and in more complex models, number of clusters)
> and the number of parameters.
> 
>> But when confint is called, using profile, the ML profile is used, hence
>> variance confidence intervals are estimated "biased" (the equivalent
>> of dividing by n).  However, when n is small, dividing by n - k and n
>> may give very different results, hence in some cases the profiled
>> confidence interval for ML estimate does not include the REML
>> estimate, for the standard deviations.  I guess in practice the
>> difference between REML and ML is more complex than just using n or
>> n-k, but would it be idea?
> 
> Correct - see above. So I wouldn't just heuristically apply some 'correction'
> that only comes from simple regression models.
> 
> Actually, when constructing a profile likelihood CI for a variance component
> (which is what confint() does by default for lmer model objects), one can also
> profile the restricted log likelihood function, which would guarantee that the
> estimate falls into the CI. I actually thought that this is what confint()
> does for lmer objects (fitted with REML), but I checked and you are correct -
> even when the model was fitted with REML, all profile likelihood CIs
> (including those for variance components) are based on the regular likelihood.
> 
> Without a reproducible example, I cannot tell you whether this is really the
> reason for the estimate falling outside of the CI, but it certainly could be.
> 
>> Support for this is that
>> 1) when using bootstrap, there seems to be no such discrepancy
>> 2) when fitting using REML = FALSE, the profiled IC is the same and
>>     does include the (different) point estimate
>>
>> Does it sound correct?
> 
> Not sure how 1) helps to diagnose this, but 2) certainly provides support for
> the hypothesis that the discrepancy arises from different likelihoods being
> used for estimation and profiling.
> 
>> Additionnal questions, if this interpretation is correct:
>> - would it make sense to make confidence intervals based on REML
>>    profiles, and not ML profiles? if so, how?
> 
> As noted above, this can be done for variance components, but I don't see a
> way of doing this with confint() for lmer model objects.
> 
>> - wouldn't a warning be a good idea when point estimates are outside
>>    CI, with the explaination if it is indeed REML vs ML?
> 
> That's up to the developers of lme4 to decide.
> 
>> - if this is indeed a "small sample size" problem, I guess in such
>>    cases any asymptotic result is difficult to trust, right?  Does it
>>    mean profiled interval cannot be trusted also, neither nested
>>    models tests, and that only bootstrap may be used?
> 
> Profiling the likelihood relies on the asymptotic behavior of the likelihood
> ratio test. So yes, profiling likelihood CIs may not have a nominal coverage
> rate in small samples (for some definition of 'small', which is context
> dependent).
> 
>> - in such cases, is there any argument to prefer REML over ML or
>>    vice-versa?
> 
> One can make arguments in favor of both ML and REML. REML tends to provide
> approximately unbiased estimates, but with larger mean-squared error, while ML
> tends to provide negative biased estimates, but with smaller MSE.
> 
>> Thanks in advance for your help,
>>
>> --
>>                                 Emmanuel CURIS
>>                                 emmanuel.curis at parisdescartes.fr
>>
>> Page WWW: http://emmanuel.curis.online.fr/index.html
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics


From wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n|  Thu May 12 18:01:04 2022
From: wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n| (Viechtbauer, Wolfgang (NP))
Date: Thu, 12 May 2022 16:01:04 +0000
Subject: [R-sig-ME] 
 Point estimate outside of its confidence interval with lmer
In-Reply-To: <6f8cb2d0-f5cb-8198-c991-e39414928692@gmail.com>
References: <Yny3XvrTuYBKUPb8@info124.pharmacie.univ-paris5.fr>
 <fb8d6bb938da49f5a337dc3d83d92086@UM-MAIL3214.unimaas.nl>
 <6f8cb2d0-f5cb-8198-c991-e39414928692@gmail.com>
Message-ID: <399bbc4c32d64dbe9e23b5578856e293@UM-MAIL3214.unimaas.nl>

Hi Ben,

There is nothing unusual about profiling the restricted likelihood and in fact, LRTs and profile likelihood CIs for variance components tend to perform slightly better. See, for example:

Morrell, C. H. (1998). Likelihood ratio testing of variance components in the linear mixed-effects model using restricted maximum likelihood. Biometrics, 54(4), 1560-1568. 

This is also discussed in various books on mixed-effects models. For example, Verbeke & Molenberghs' "Linear Mixed Models for Longitudinal Data" (see section 6.3.2: "Further, in contrast to the LR test for fixed effects (see Section 6.2.5), valid LR tests are also obtained under REML estimation. [...]"). So, if one can conduct a LRT for variance components under REML estimation, one can also profile the restricted likelihood and use this to construct CIs.

Of course, the usual caveats apply when the parameter tested falls on the boundary of the parameter space.

Best,
Wolfgang

>-----Original Message-----
>From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org] On
>Behalf Of Ben Bolker
>Sent: Thursday, 12 May, 2022 15:55
>To: r-sig-mixed-models at r-project.org
>Subject: Re: [R-sig-ME] Point estimate outside of its confidence interval with
>lmer
>
>   For what it's worth, there is a (relatively ancient) Github issue
>discussing this:
>
>https://github.com/lme4/lme4/issues/325
>
>   Further evidence that we ought to add a warning, or at least a
>message, saying that the profiles are based on the ML fit.
>
>   Can anyone point me to some theory that supports/describes the
>process of constructing CIs based on the profile of the REML statistic ... ?
>
>   cheers
>    Ben Bolker
>
>
>On 2022-05-12 7:01 a.m., Viechtbauer, Wolfgang (NP) wrote:
>> Dear Emmanuel,
>>
>> Please see below for my responses.
>>
>> Best,
>> Wolfgang
>>
>>> -----Original Message-----
>>> From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org] On
>>> Behalf Of Emmanuel Curis
>>> Sent: Thursday, 12 May, 2022 9:30
>>> To: r-sig-mixed-models at r-project.org
>>> Subject: [R-sig-ME] Point estimate outside of its confidence interval with
>lmer
>>>
>>> Hello,
>>>
>>> I have encountered an unexpected result for some datasets when using confint
>>> after fitting a model with lmer: the confidence intervals for the standard
>>> deviations in the model did not included the point estimate, given by summary
>>> for instance.
>>>
>>> I think the problem is a mix of small sample size and REML vs ML, but I would
>>> be happy to have confirmation that my interpretation is correct, since I'm
>>> not very familiar with profiling and REML vs ML... and wonder if something
>>> more problematic is occuring.
>>>
>>> My interpretation is that lmer fits using REML by default,
>>
>> Correct.
>>
>>> hence the variances are estimated "unbiased" (the equivalent of dividing by n
>>> - k for the residual variance of a linear model, but I'm not sure exactly
>>> what is n and k for random effects variance estimation).
>>
>> Only for some very special cases can we show that the variance components are
>> estimated unbiasedly. However, yes, REML estimation tends to provide estimates
>> of variance components that are approximately unbiased in many cases.
>>
>> Also, for more complex models, changing a ML estimate into a REML one isn't
>> simply a multiplication of the ML estimate by some factor that depends on
>> things like the sample size (and in more complex models, number of clusters)
>> and the number of parameters.
>>
>>> But when confint is called, using profile, the ML profile is used, hence
>>> variance confidence intervals are estimated "biased" (the equivalent
>>> of dividing by n).  However, when n is small, dividing by n - k and n
>>> may give very different results, hence in some cases the profiled
>>> confidence interval for ML estimate does not include the REML
>>> estimate, for the standard deviations.  I guess in practice the
>>> difference between REML and ML is more complex than just using n or
>>> n-k, but would it be idea?
>>
>> Correct - see above. So I wouldn't just heuristically apply some 'correction'
>> that only comes from simple regression models.
>>
>> Actually, when constructing a profile likelihood CI for a variance component
>> (which is what confint() does by default for lmer model objects), one can also
>> profile the restricted log likelihood function, which would guarantee that the
>> estimate falls into the CI. I actually thought that this is what confint()
>> does for lmer objects (fitted with REML), but I checked and you are correct -
>> even when the model was fitted with REML, all profile likelihood CIs
>> (including those for variance components) are based on the regular likelihood.
>>
>> Without a reproducible example, I cannot tell you whether this is really the
>> reason for the estimate falling outside of the CI, but it certainly could be.
>>
>>> Support for this is that
>>> 1) when using bootstrap, there seems to be no such discrepancy
>>> 2) when fitting using REML = FALSE, the profiled IC is the same and
>>>     does include the (different) point estimate
>>>
>>> Does it sound correct?
>>
>> Not sure how 1) helps to diagnose this, but 2) certainly provides support for
>> the hypothesis that the discrepancy arises from different likelihoods being
>> used for estimation and profiling.
>>
>>> Additionnal questions, if this interpretation is correct:
>>> - would it make sense to make confidence intervals based on REML
>>>    profiles, and not ML profiles? if so, how?
>>
>> As noted above, this can be done for variance components, but I don't see a
>> way of doing this with confint() for lmer model objects.
>>
>>> - wouldn't a warning be a good idea when point estimates are outside
>>>    CI, with the explaination if it is indeed REML vs ML?
>>
>> That's up to the developers of lme4 to decide.
>>
>>> - if this is indeed a "small sample size" problem, I guess in such
>>>    cases any asymptotic result is difficult to trust, right?  Does it
>>>    mean profiled interval cannot be trusted also, neither nested
>>>    models tests, and that only bootstrap may be used?
>>
>> Profiling the likelihood relies on the asymptotic behavior of the likelihood
>> ratio test. So yes, profiling likelihood CIs may not have a nominal coverage
>> rate in small samples (for some definition of 'small', which is context
>> dependent).
>>
>>> - in such cases, is there any argument to prefer REML over ML or
>>>    vice-versa?
>>
>> One can make arguments in favor of both ML and REML. REML tends to provide
>> approximately unbiased estimates, but with larger mean-squared error, while ML
>> tends to provide negative biased estimates, but with smaller MSE.
>>
>>> Thanks in advance for your help,
>>>
>>> --
>>>                                 Emmanuel CURIS
>>>                                 emmanuel.curis at parisdescartes.fr
>>>
>>> Page WWW: http://emmanuel.curis.online.fr/index.html
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>--
>Dr. Benjamin Bolker
>Professor, Mathematics & Statistics and Biology, McMaster University
>Director, School of Computational Science and Engineering
>(Acting) Graduate chair, Mathematics & Statistics
>
>_______________________________________________
>R-sig-mixed-models at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From piuo244 m@iii@g oii @ucki@@du@i@@c@@z  Thu May 12 23:29:26 2022
From: piuo244 m@iii@g oii @ucki@@du@i@@c@@z (piuo244 m@iii@g oii @ucki@@du@i@@c@@z)
Date: Thu, 12 May 2022 21:29:26 +0000 (UTC)
Subject: [R-sig-ME] Force the random effects variances to be the same
References: <640986506.2274287.1652390966599.ref@mail.yahoo.com>
Message-ID: <640986506.2274287.1652390966599@mail.yahoo.com>

Hi,
I would like to use?lme4::glmer to?fit a model of the form
y ~ x + (0+m|group) + (0+f|group)

where a group looks like?
m f1 00 11/2 1/21/2 1/2
1/2 1/2

I was wondering if it is possible to force the variances of the random effects to be the same?
Many thanks,Zoe?
	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Fri May 13 09:36:56 2022
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Fri, 13 May 2022 09:36:56 +0200
Subject: [R-sig-ME] Force the random effects variances to be the same
In-Reply-To: <640986506.2274287.1652390966599@mail.yahoo.com>
References: <640986506.2274287.1652390966599.ref@mail.yahoo.com>
 <640986506.2274287.1652390966599@mail.yahoo.com>
Message-ID: <CAJuCY5ycOCz+nZiqPwbX8iuFU2N71wtn_MvZegpsVGG7Y6S3WA@mail.gmail.com>

Why would you want to do that?

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op do 12 mei 2022 om 23:29 schreef pluo244 at aucklanduni.ac.nz <
pluo244 at aucklanduni.ac.nz>:

> Hi,
> I would like to use lme4::glmer to fit a model of the form
> y ~ x + (0+m|group) + (0+f|group)
>
> where a group looks like
> m f1 00 11/2 1/21/2 1/2
> 1/2 1/2
>
> I was wondering if it is possible to force the variances of the random
> effects to be the same?
> Many thanks,Zoe
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From piuo244 m@iii@g oii @ucki@@du@i@@c@@z  Fri May 13 11:17:04 2022
From: piuo244 m@iii@g oii @ucki@@du@i@@c@@z (piuo244 m@iii@g oii @ucki@@du@i@@c@@z)
Date: Fri, 13 May 2022 09:17:04 +0000 (UTC)
Subject: [R-sig-ME] Force the random effects variances to be the same
In-Reply-To: <CAJuCY5ycOCz+nZiqPwbX8iuFU2N71wtn_MvZegpsVGG7Y6S3WA@mail.gmail.com>
References: <640986506.2274287.1652390966599.ref@mail.yahoo.com>
 <640986506.2274287.1652390966599@mail.yahoo.com>
 <CAJuCY5ycOCz+nZiqPwbX8iuFU2N71wtn_MvZegpsVGG7Y6S3WA@mail.gmail.com>
Message-ID: <1533628497.2880028.1652433424025@mail.yahoo.com>

 Hi?Thierry,
Because I would like to fit a heritability model, where the genetic variance should be the same for mother and father. I was doing this with lme4qtl which allows correlated random effects. In my case, the correlation matrix is a block diagonal matrix, and a block looks like
1? ? ?0? ?1/2 1/2 1/20? ? ?1? ?1/2 1/2 1/21/2 1/2 1? ? 1/2 1/21/2 1/2 1/2 1? ? 1/2
1/2 1/2 1/2 1/2 1

But the performance of the Laplace approximation is quite poor for binary data with small cluster sizes. I think letting nAGQ=3 might help, but lme4qtl does not allow me to do so.
Cheers,Zoe    On Friday, May 13, 2022, 07:37:07 PM GMT+12, Thierry Onkelinx <thierry.onkelinx at inbo.be> wrote:  
 
 Why would you want to do that?
ir. Thierry Onkelinx
Statisticus / Statistician
Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance 
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////



Op do 12 mei 2022 om 23:29 schreef pluo244 at aucklanduni.ac.nz <pluo244 at aucklanduni.ac.nz>:

Hi,
I would like to use?lme4::glmer to?fit a model of the form
y ~ x + (0+m|group) + (0+f|group)

where a group looks like?
m f1 00 11/2 1/21/2 1/2
1/2 1/2

I was wondering if it is possible to force the variances of the random effects to be the same?
Many thanks,Zoe?
? ? ? ? [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

  
	[[alternative HTML version deleted]]


From emm@nue|@cur|@ @end|ng |rom p@r|@de@c@rte@@|r  Fri May 13 17:18:53 2022
From: emm@nue|@cur|@ @end|ng |rom p@r|@de@c@rte@@|r (Emmanuel Curis)
Date: Fri, 13 May 2022 17:18:53 +0200
Subject: [R-sig-ME] 
 Point estimate outside of its confidence interval with lmer
In-Reply-To: <399bbc4c32d64dbe9e23b5578856e293@UM-MAIL3214.unimaas.nl>
References: <Yny3XvrTuYBKUPb8@info124.pharmacie.univ-paris5.fr>
 <fb8d6bb938da49f5a337dc3d83d92086@UM-MAIL3214.unimaas.nl>
 <6f8cb2d0-f5cb-8198-c991-e39414928692@gmail.com>
 <399bbc4c32d64dbe9e23b5578856e293@UM-MAIL3214.unimaas.nl>
Message-ID: <Yn523RHlAuilmh6p@info124.pharmacie.univ-paris5.fr>

Thanks Ben & Wolfgang for your explainations.

I'll try to prepare an anonymised and selected dataset to check that
it is the only reason of this discrepancy.

Wolfgang, by bootstrap as a check, I meant that since bootstrapped
dataset are refited using REML, the sampled sigma are from REML hence
IC based on them should include the observed one ? and this is the
case, in favor of a REML/ML discrepancy.  But that could also be that
the khi-square approximation is just false, indeed.  IC are not much
larger by bootsrap, however, instead shifted when comparing boostrap
IC for REML and ML fits.

Best regards,

Le Thu, May 12, 2022 at 04:01:04PM +0000, Viechtbauer, Wolfgang (NP) a ?crit :
> Hi Ben,
> 
> There is nothing unusual about profiling the restricted likelihood and in fact, LRTs and profile likelihood CIs for variance components tend to perform slightly better. See, for example:
> 
> Morrell, C. H. (1998). Likelihood ratio testing of variance components in the linear mixed-effects model using restricted maximum likelihood. Biometrics, 54(4), 1560-1568. 
> 
> This is also discussed in various books on mixed-effects models. For example, Verbeke & Molenberghs' "Linear Mixed Models for Longitudinal Data" (see section 6.3.2: "Further, in contrast to the LR test for fixed effects (see Section 6.2.5), valid LR tests are also obtained under REML estimation. [...]"). So, if one can conduct a LRT for variance components under REML estimation, one can also profile the restricted likelihood and use this to construct CIs.
> 
> Of course, the usual caveats apply when the parameter tested falls on the boundary of the parameter space.
> 
> Best,
> Wolfgang
> 
> >-----Original Message-----
> >From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org] On
> >Behalf Of Ben Bolker
> >Sent: Thursday, 12 May, 2022 15:55
> >To: r-sig-mixed-models at r-project.org
> >Subject: Re: [R-sig-ME] Point estimate outside of its confidence interval with
> >lmer
> >
> >   For what it's worth, there is a (relatively ancient) Github issue
> >discussing this:
> >
> >https://github.com/lme4/lme4/issues/325
> >
> >   Further evidence that we ought to add a warning, or at least a
> >message, saying that the profiles are based on the ML fit.
> >
> >   Can anyone point me to some theory that supports/describes the
> >process of constructing CIs based on the profile of the REML statistic ... ?
> >
> >   cheers
> >    Ben Bolker
> >
> >
> >On 2022-05-12 7:01 a.m., Viechtbauer, Wolfgang (NP) wrote:
> >> Dear Emmanuel,
> >>
> >> Please see below for my responses.
> >>
> >> Best,
> >> Wolfgang
> >>
> >>> -----Original Message-----
> >>> From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org] On
> >>> Behalf Of Emmanuel Curis
> >>> Sent: Thursday, 12 May, 2022 9:30
> >>> To: r-sig-mixed-models at r-project.org
> >>> Subject: [R-sig-ME] Point estimate outside of its confidence interval with
> >lmer
> >>>
> >>> Hello,
> >>>
> >>> I have encountered an unexpected result for some datasets when using confint
> >>> after fitting a model with lmer: the confidence intervals for the standard
> >>> deviations in the model did not included the point estimate, given by summary
> >>> for instance.
> >>>
> >>> I think the problem is a mix of small sample size and REML vs ML, but I would
> >>> be happy to have confirmation that my interpretation is correct, since I'm
> >>> not very familiar with profiling and REML vs ML... and wonder if something
> >>> more problematic is occuring.
> >>>
> >>> My interpretation is that lmer fits using REML by default,
> >>
> >> Correct.
> >>
> >>> hence the variances are estimated "unbiased" (the equivalent of dividing by n
> >>> - k for the residual variance of a linear model, but I'm not sure exactly
> >>> what is n and k for random effects variance estimation).
> >>
> >> Only for some very special cases can we show that the variance components are
> >> estimated unbiasedly. However, yes, REML estimation tends to provide estimates
> >> of variance components that are approximately unbiased in many cases.
> >>
> >> Also, for more complex models, changing a ML estimate into a REML one isn't
> >> simply a multiplication of the ML estimate by some factor that depends on
> >> things like the sample size (and in more complex models, number of clusters)
> >> and the number of parameters.
> >>
> >>> But when confint is called, using profile, the ML profile is used, hence
> >>> variance confidence intervals are estimated "biased" (the equivalent
> >>> of dividing by n).  However, when n is small, dividing by n - k and n
> >>> may give very different results, hence in some cases the profiled
> >>> confidence interval for ML estimate does not include the REML
> >>> estimate, for the standard deviations.  I guess in practice the
> >>> difference between REML and ML is more complex than just using n or
> >>> n-k, but would it be idea?
> >>
> >> Correct - see above. So I wouldn't just heuristically apply some 'correction'
> >> that only comes from simple regression models.
> >>
> >> Actually, when constructing a profile likelihood CI for a variance component
> >> (which is what confint() does by default for lmer model objects), one can also
> >> profile the restricted log likelihood function, which would guarantee that the
> >> estimate falls into the CI. I actually thought that this is what confint()
> >> does for lmer objects (fitted with REML), but I checked and you are correct -
> >> even when the model was fitted with REML, all profile likelihood CIs
> >> (including those for variance components) are based on the regular likelihood.
> >>
> >> Without a reproducible example, I cannot tell you whether this is really the
> >> reason for the estimate falling outside of the CI, but it certainly could be.
> >>
> >>> Support for this is that
> >>> 1) when using bootstrap, there seems to be no such discrepancy
> >>> 2) when fitting using REML = FALSE, the profiled IC is the same and
> >>>     does include the (different) point estimate
> >>>
> >>> Does it sound correct?
> >>
> >> Not sure how 1) helps to diagnose this, but 2) certainly provides support for
> >> the hypothesis that the discrepancy arises from different likelihoods being
> >> used for estimation and profiling.
> >>
> >>> Additionnal questions, if this interpretation is correct:
> >>> - would it make sense to make confidence intervals based on REML
> >>>    profiles, and not ML profiles? if so, how?
> >>
> >> As noted above, this can be done for variance components, but I don't see a
> >> way of doing this with confint() for lmer model objects.
> >>
> >>> - wouldn't a warning be a good idea when point estimates are outside
> >>>    CI, with the explaination if it is indeed REML vs ML?
> >>
> >> That's up to the developers of lme4 to decide.
> >>
> >>> - if this is indeed a "small sample size" problem, I guess in such
> >>>    cases any asymptotic result is difficult to trust, right?  Does it
> >>>    mean profiled interval cannot be trusted also, neither nested
> >>>    models tests, and that only bootstrap may be used?
> >>
> >> Profiling the likelihood relies on the asymptotic behavior of the likelihood
> >> ratio test. So yes, profiling likelihood CIs may not have a nominal coverage
> >> rate in small samples (for some definition of 'small', which is context
> >> dependent).
> >>
> >>> - in such cases, is there any argument to prefer REML over ML or
> >>>    vice-versa?
> >>
> >> One can make arguments in favor of both ML and REML. REML tends to provide
> >> approximately unbiased estimates, but with larger mean-squared error, while ML
> >> tends to provide negative biased estimates, but with smaller MSE.
> >>
> >>> Thanks in advance for your help,
> >>>
> >>> --
> >>>                                 Emmanuel CURIS
> >>>                                 emmanuel.curis at parisdescartes.fr
> >>>
> >>> Page WWW: http://emmanuel.curis.online.fr/index.html
> >>
> >> _______________________________________________
> >> R-sig-mixed-models at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
> >--
> >Dr. Benjamin Bolker
> >Professor, Mathematics & Statistics and Biology, McMaster University
> >Director, School of Computational Science and Engineering
> >(Acting) Graduate chair, Mathematics & Statistics
> >
> >_______________________________________________
> >R-sig-mixed-models at r-project.org mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
                                Emmanuel CURIS
                                emmanuel.curis at parisdescartes.fr

Page WWW: http://emmanuel.curis.online.fr/index.html


From bbo|ker @end|ng |rom gm@||@com  Fri May 13 20:00:20 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Fri, 13 May 2022 14:00:20 -0400
Subject: [R-sig-ME] Force the random effects variances to be the same
In-Reply-To: <CAJuCY5ycOCz+nZiqPwbX8iuFU2N71wtn_MvZegpsVGG7Y6S3WA@mail.gmail.com>
References: <640986506.2274287.1652390966599.ref@mail.yahoo.com>
 <640986506.2274287.1652390966599@mail.yahoo.com>
 <CAJuCY5ycOCz+nZiqPwbX8iuFU2N71wtn_MvZegpsVGG7Y6S3WA@mail.gmail.com>
Message-ID: <d2e57ac6-815e-d8c2-8c82-824ffa0103a1@gmail.com>

  Doesn't lme4qtl allow nAGQ > 1 ?

https://github.com/variani/lme4qtl/blob/0c173ea8d8386b205f62ad642698519a861650b4/R/relmatGlmer.R#L22-L24

   I can't quite see what your groups look like since HTML structure 
gets thrown away by the mailing list ...  is each of the random effect 
scalar (i.e. variance among females, variance among males?)

   Something *approximately* like

glmod <- glFormula(..., data = ..., family = binomial)
devfun <- do.call(mkGlmerDevfun, glmod)
opt <- optimizeGlmer(devfun)
devfun <- updateGlmerDevfun(devfun, glmod$reTrms)

   (These first four steps are just repeating what's in ?lme4:modular))

At this point it gets slightly trickier.

Write a wrapper function that repeats the first element of the given 
parameter vector, then passes it to devfun (in general the random 
effects parameters are at the beginning of the parameter vector); the 
parameter vector for this will be one shorter than the parameter for the 
full model.

devfun2 <- function(p) {
   devfun(c(p[1], p))
}

nfixed <- ## figure out the number of fixed-effect parameters
opt <- nloptwrap(par = c(opt$par[-1], rep(0, nfixed)), fn = devfun2,
    lower = c(0, rep(-Inf, nfixed))

  This should give you the answer, it might take a little more fussing 
to put it back into a merMod object (something like):

opt$par <- c(opt$par[1], opt$par)  ## augment parameter vector
mkMerMod(environment(devfun), opt, glmod$reTrms, fr = glmod$fr)

   I'm not sure all of that is right, but it might be.

  cheers
    Ben Bolker


On 2022-05-13 3:36 a.m., Thierry Onkelinx via R-sig-mixed-models wrote:
> Why would you want to do that?
> 
> ir. Thierry Onkelinx
> Statisticus / Statistician
> 
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
> 
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
> ///////////////////////////////////////////////////////////////////////////////////////////
> 
> <https://www.inbo.be>
> 
> 
> Op do 12 mei 2022 om 23:29 schreef pluo244 at aucklanduni.ac.nz <
> pluo244 at aucklanduni.ac.nz>:
> 
>> Hi,
>> I would like to use lme4::glmer to fit a model of the form
>> y ~ x + (0+m|group) + (0+f|group)
>>
>> where a group looks like
>> m f1 00 11/2 1/21/2 1/2
>> 1/2 1/2
>>
>> I was wondering if it is possible to force the variances of the random
>> effects to be the same?
>> Many thanks,Zoe
>>          [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics


From wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n|  Sun May 15 19:42:38 2022
From: wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n| (Viechtbauer, Wolfgang (NP))
Date: Sun, 15 May 2022 17:42:38 +0000
Subject: [R-sig-ME] 
 Point estimate outside of its confidence interval with lmer
In-Reply-To: <Yn523RHlAuilmh6p@info124.pharmacie.univ-paris5.fr>
References: <Yny3XvrTuYBKUPb8@info124.pharmacie.univ-paris5.fr>
 <fb8d6bb938da49f5a337dc3d83d92086@UM-MAIL3214.unimaas.nl>
 <6f8cb2d0-f5cb-8198-c991-e39414928692@gmail.com>
 <399bbc4c32d64dbe9e23b5578856e293@UM-MAIL3214.unimaas.nl>
 <Yn523RHlAuilmh6p@info124.pharmacie.univ-paris5.fr>
Message-ID: <31de5c82c5ca44daa07d4ed247e46dc5@UM-MAIL3214.unimaas.nl>

After simulating a bunch of datasets, I was able to find one where this issue arises.

##############################

dat <- structure(list(id = c(1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L,
4L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 8L, 9L, 9L, 9L, 9L, 10L,
10L, 10L), yij = c(1.3342, -1.3854, -2.3733, -0.2884, -0.567, -0.6045, 0.494,
0.7738, 0.7991, -0.2539, -0.3956, 1.8508, 2.0849, 0.8782, 1.1726, 0.0818,
0.4099, 0.2279, 0.3244, -0.4167, -0.998, -0.9063, 0.9197, 0.806, -0.1283,
4.1764, -0.0946, -0.2065, 0.0359, 1.2685, -0.0226, 0.6875, 0.1928), obs =
c(1L, 2L, 3L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 1L, 1L, 2L, 3L, 4L, 5L, 1L,
2L, 3L, 4L, 5L, 1L, 2L, 1L, 1L, 2L, 3L, 4L, 1L, 2L, 3L )), row.names = c(NA,
-33L), class = "data.frame")

library(lme4)
library(nlme)
library(metafor)

res1 <- lmer(yij ~ 1 + (1 | id), data=dat)
res2 <- lme(yij ~ 1, random = ~ 1 | id, data=dat)
res3 <- rma.mv(yij ~ 1, V=0, random = ~ 1 | id/obs, data=dat)

summary(res1)
summary(res2)
summary(res3)

# so we get the same results from all fits

# profile likelihood CI for sigma based on ML estimation
confint(res1, parm=".sigma")

# the CI for sigma does not include the REML estimate of sigma
sigma(res1)

# get the profile likelihood CI for sigma based on REML estimation
confint(res3, sigma2=2)

# examine the REML profile for sigma^2
ci <- confint(res3, sigma2=2)
profile(res3, sigma2=2, cline=TRUE, steps=100, xlim=c(0.4,2.2))
abline(v=ci$random[1,2:3], lty="dotted")

# note: for this small dataset, the ML fit is really rather different
summary(update(res1, REML=FALSE))

# double-check the profile likelihood CI based on ML estimation
res3 <- update(res3, method="ML")
res3
confint(res3, sigma2=2)

# the lower bound is rather different compared to confint(res1, parm=".sigma")

# examine the ML profile for sigma^2
ci <- confint(res3, sigma2=2)
profile(res3, sigma2=2, cline=TRUE, steps=100, xlim=c(0.4,2.2))
abline(v=ci$random[1,2:3], lty="dotted")

##############################

So, as far as I can tell, there are really two issues here:

1) As noted already, confint.merMod() uses the ML likelihood to profile all parameters including the variance components. This can in principle lead to the CI not including a particular REML estimate of a parameter. However, the switch to ML makes sense in so far as confint.merMod() also profiles fixed effects for which one must use ML estimation. It would be *really* confusing if the CIs for the fixed effects are based on the regular while the CIs for variance components are based on restricted likelihood. Might be nice though to emphasize under help(confint.merMod) that this is happening.

2) However, in the example above, the profiling itself goes wrong, leading to an incorrect lower bound of the profile likelihood CI for sigma. After playing around with some of the settings to profile.merMod(), I got the same CI as metafor with:

confint(res1, parm=".sigma", delta.cutoff=1/64)

So, @Emmanuel, you might want to check if the issue you are encountering is not one of type 1) but rather of type 2).

Best,
Wolfgang

>-----Original Message-----
>From: Emmanuel Curis [mailto:emmanuel.curis at parisdescartes.fr]
>Sent: Friday, 13 May, 2022 17:19
>To: Viechtbauer, Wolfgang (NP)
>Cc: Ben Bolker; r-sig-mixed-models at r-project.org
>Subject: Re: [R-sig-ME] Point estimate outside of its confidence interval with
>lmer
>
>Thanks Ben & Wolfgang for your explainations.
>
>I'll try to prepare an anonymised and selected dataset to check that
>it is the only reason of this discrepancy.
>
>Wolfgang, by bootstrap as a check, I meant that since bootstrapped
>dataset are refited using REML, the sampled sigma are from REML hence
>IC based on them should include the observed one ? and this is the
>case, in favor of a REML/ML discrepancy.  But that could also be that
>the khi-square approximation is just false, indeed.  IC are not much
>larger by bootsrap, however, instead shifted when comparing boostrap
>IC for REML and ML fits.
>
>Best regards,
>
>Le Thu, May 12, 2022 at 04:01:04PM +0000, Viechtbauer, Wolfgang (NP) a ?crit :
>> Hi Ben,
>>
>> There is nothing unusual about profiling the restricted likelihood and in fact,
>LRTs and profile likelihood CIs for variance components tend to perform slightly
>better. See, for example:
>>
>> Morrell, C. H. (1998). Likelihood ratio testing of variance components in the
>linear mixed-effects model using restricted maximum likelihood. Biometrics,
>54(4), 1560-1568.
>>
>> This is also discussed in various books on mixed-effects models. For example,
>Verbeke & Molenberghs' "Linear Mixed Models for Longitudinal Data" (see section
>6.3.2: "Further, in contrast to the LR test for fixed effects (see Section
>6.2.5), valid LR tests are also obtained under REML estimation. [...]"). So, if
>one can conduct a LRT for variance components under REML estimation, one can also
>profile the restricted likelihood and use this to construct CIs.
>>
>> Of course, the usual caveats apply when the parameter tested falls on the
>boundary of the parameter space.
>>
>> Best,
>> Wolfgang
>>
>> >-----Original Message-----
>> >From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org] On
>> >Behalf Of Ben Bolker
>> >Sent: Thursday, 12 May, 2022 15:55
>> >To: r-sig-mixed-models at r-project.org
>> >Subject: Re: [R-sig-ME] Point estimate outside of its confidence interval with
>> >lmer
>> >
>> >   For what it's worth, there is a (relatively ancient) Github issue
>> >discussing this:
>> >
>> >https://github.com/lme4/lme4/issues/325
>> >
>> >   Further evidence that we ought to add a warning, or at least a
>> >message, saying that the profiles are based on the ML fit.
>> >
>> >   Can anyone point me to some theory that supports/describes the
>> >process of constructing CIs based on the profile of the REML statistic ... ?
>> >
>> >   cheers
>> >    Ben Bolker
>> >
>> >
>> >On 2022-05-12 7:01 a.m., Viechtbauer, Wolfgang (NP) wrote:
>> >> Dear Emmanuel,
>> >>
>> >> Please see below for my responses.
>> >>
>> >> Best,
>> >> Wolfgang
>> >>
>> >>> -----Original Message-----
>> >>> From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org]
>On
>> >>> Behalf Of Emmanuel Curis
>> >>> Sent: Thursday, 12 May, 2022 9:30
>> >>> To: r-sig-mixed-models at r-project.org
>> >>> Subject: [R-sig-ME] Point estimate outside of its confidence interval with
>> >lmer
>> >>>
>> >>> Hello,
>> >>>
>> >>> I have encountered an unexpected result for some datasets when using
>confint
>> >>> after fitting a model with lmer: the confidence intervals for the standard
>> >>> deviations in the model did not included the point estimate, given by
>summary
>> >>> for instance.
>> >>>
>> >>> I think the problem is a mix of small sample size and REML vs ML, but I
>would
>> >>> be happy to have confirmation that my interpretation is correct, since I'm
>> >>> not very familiar with profiling and REML vs ML... and wonder if something
>> >>> more problematic is occuring.
>> >>>
>> >>> My interpretation is that lmer fits using REML by default,
>> >>
>> >> Correct.
>> >>
>> >>> hence the variances are estimated "unbiased" (the equivalent of dividing by
>n
>> >>> - k for the residual variance of a linear model, but I'm not sure exactly
>> >>> what is n and k for random effects variance estimation).
>> >>
>> >> Only for some very special cases can we show that the variance components
>are
>> >> estimated unbiasedly. However, yes, REML estimation tends to provide
>estimates
>> >> of variance components that are approximately unbiased in many cases.
>> >>
>> >> Also, for more complex models, changing a ML estimate into a REML one isn't
>> >> simply a multiplication of the ML estimate by some factor that depends on
>> >> things like the sample size (and in more complex models, number of clusters)
>> >> and the number of parameters.
>> >>
>> >>> But when confint is called, using profile, the ML profile is used, hence
>> >>> variance confidence intervals are estimated "biased" (the equivalent
>> >>> of dividing by n).  However, when n is small, dividing by n - k and n
>> >>> may give very different results, hence in some cases the profiled
>> >>> confidence interval for ML estimate does not include the REML
>> >>> estimate, for the standard deviations.  I guess in practice the
>> >>> difference between REML and ML is more complex than just using n or
>> >>> n-k, but would it be idea?
>> >>
>> >> Correct - see above. So I wouldn't just heuristically apply some
>'correction'
>> >> that only comes from simple regression models.
>> >>
>> >> Actually, when constructing a profile likelihood CI for a variance component
>> >> (which is what confint() does by default for lmer model objects), one can
>also
>> >> profile the restricted log likelihood function, which would guarantee that
>the
>> >> estimate falls into the CI. I actually thought that this is what confint()
>> >> does for lmer objects (fitted with REML), but I checked and you are correct
>-
>> >> even when the model was fitted with REML, all profile likelihood CIs
>> >> (including those for variance components) are based on the regular
>likelihood.
>> >>
>> >> Without a reproducible example, I cannot tell you whether this is really the
>> >> reason for the estimate falling outside of the CI, but it certainly could
>be.
>> >>
>> >>> Support for this is that
>> >>> 1) when using bootstrap, there seems to be no such discrepancy
>> >>> 2) when fitting using REML = FALSE, the profiled IC is the same and
>> >>>     does include the (different) point estimate
>> >>>
>> >>> Does it sound correct?
>> >>
>> >> Not sure how 1) helps to diagnose this, but 2) certainly provides support
>for
>> >> the hypothesis that the discrepancy arises from different likelihoods being
>> >> used for estimation and profiling.
>> >>
>> >>> Additionnal questions, if this interpretation is correct:
>> >>> - would it make sense to make confidence intervals based on REML
>> >>>    profiles, and not ML profiles? if so, how?
>> >>
>> >> As noted above, this can be done for variance components, but I don't see a
>> >> way of doing this with confint() for lmer model objects.
>> >>
>> >>> - wouldn't a warning be a good idea when point estimates are outside
>> >>>    CI, with the explaination if it is indeed REML vs ML?
>> >>
>> >> That's up to the developers of lme4 to decide.
>> >>
>> >>> - if this is indeed a "small sample size" problem, I guess in such
>> >>>    cases any asymptotic result is difficult to trust, right?  Does it
>> >>>    mean profiled interval cannot be trusted also, neither nested
>> >>>    models tests, and that only bootstrap may be used?
>> >>
>> >> Profiling the likelihood relies on the asymptotic behavior of the likelihood
>> >> ratio test. So yes, profiling likelihood CIs may not have a nominal coverage
>> >> rate in small samples (for some definition of 'small', which is context
>> >> dependent).
>> >>
>> >>> - in such cases, is there any argument to prefer REML over ML or
>> >>>    vice-versa?
>> >>
>> >> One can make arguments in favor of both ML and REML. REML tends to provide
>> >> approximately unbiased estimates, but with larger mean-squared error, while
>ML
>> >> tends to provide negative biased estimates, but with smaller MSE.
>> >>
>> >>> Thanks in advance for your help,
>> >>>
>> >>> --
>> >>>                                 Emmanuel CURIS
>> >>>                                 emmanuel.curis at parisdescartes.fr
>> >>>
>> >>> Page WWW: http://emmanuel.curis.online.fr/index.html

From @@n@gy @end|ng |rom @zh@r@edu@eg  Wed May 25 01:38:35 2022
From: @@n@gy @end|ng |rom @zh@r@edu@eg (Abdullah Nagy)
Date: Tue, 24 May 2022 23:38:35 +0000
Subject: [R-sig-ME] Years as random effect
Message-ID: <AM0PR06MB608255F933F872CC27EF3EBBB2D79@AM0PR06MB6082.eurprd06.prod.outlook.com>

Dear All,

I'm trying to fit a GLMM to presence absence data using binomial distribution in glmmTMB R package.

My model formula: Midden(0/1) ~ scale(Tree Height) + scale(I(Tree Height^2)) + scale(NDVI) + scale(Distance_to_quarry)+(1|ID) + (1|YEAR)
The ID is 39 level and data were collected in 2009 and 2021.

My questions are:

  1.  Is it correct to include year as crossed random effect to avoid pseudoreplication ?
  2.  When I use year as a crossed random effect, I get Unusually large Z-statistics (theta_1|YEAR.1=-5.456213), how to solve this issue ?

Thank you,
Abdullah




Abdullah Nagy
Assistant lecturer of Ecology
Al-Azhar University in Cairo
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ????? ??????: ?? ???? ????????? ??????? ?? ??? ??????? ??? ????? ????? ?? ??????? ??? ?????? ????? ??? ???? ??????? ????. ?????? ??? ?? ??? ??? ???????? ??????? ??? ??????? ???????? ?????? ?????? ????? ???? ??????? ?????? ????? ??????? ???? ?? ???? ??? ???? ????????? ?? ????. ??? ???? ???? ???? ??? ?? ????? ??? ??????? ????? ????? ?? ????? ???? ??? ????????? ??????? ???? ???? ???? ??? ?????? ???? ?? ??????? ?? ?????????. ????? ?????? ?? ???? ?????? ?????? ????? ???????? ?? ??? ??????? ??? ??? ?????? ??? ???? ???????? ??? ????? ?????? ??? ?? ????? ????? ?????? ??????? ?? ????? ????? ?? ?? ????? ?? ????? ???? ?????? ??? ?????? ?????????? Disclaimer: This message and any attachments are intended exclusively for the named addressee(s) and may contain confidential information. If you are not the intended/named addressee/recipient you are hereby notified that any disclosure, copying, distribution or use of the information (including attachments) contained herein is strictly prohibited under current legislation. Likewise, no action may be taken based on the information provided in this message unless a prior written consent is obtained from the person in charge of such content. If you received this transmission in error, please contact the sender immediately and permanently delete the email, any attachments, and all copies thereof from any drives or storage media and destroy any printouts of the email or attachments. It is worth noting that any opinions expressed in this communication are not necessarily those of Al-Azhar University and they are solely the writer's. We advise that, in keeping with best business practice, the recipient must ensure they are actually virus free, as Al-Azhar University bears no liability for any damage resulting from any viruses communicated via this email.

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Wed May 25 02:11:17 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Tue, 24 May 2022 20:11:17 -0400
Subject: [R-sig-ME] Years as random effect
In-Reply-To: <AM0PR06MB608255F933F872CC27EF3EBBB2D79@AM0PR06MB6082.eurprd06.prod.outlook.com>
References: <AM0PR06MB608255F933F872CC27EF3EBBB2D79@AM0PR06MB6082.eurprd06.prod.outlook.com>
Message-ID: <190db79f-82e1-a408-69c6-ddc379e1482c@gmail.com>



On 2022-05-24 7:38 p.m., Abdullah Nagy wrote:
> Dear All,
> 
> I'm trying to fit a GLMM to presence absence data using binomial distribution in glmmTMB R package.
> 
> My model formula: Midden(0/1) ~ scale(Tree Height) + scale(I(Tree Height^2)) + scale(NDVI) + scale(Distance_to_quarry)+(1|ID) + (1|YEAR)
> The ID is 39 level and data were collected in 2009 and 2021.
> 
> My questions are:
> 
>    1.  Is it correct to include year as crossed random effect to avoid pseudoreplication ?
>    2.  When I use year as a crossed random effect, I get Unusually large Z-statistics (theta_1|YEAR.1=-5.456213), how to solve this issue ?
> 
> Thank you,
> Abdullah


     While year may *conceptually* be a random effect, it's unlikely to 
work very well in practice (unless you do fancy things, random effects 
are hard to fit when there are fewer than 5-6 levels). I would recommend 
including YEAR as a categorical (factor) fixed effect.

    For the other question we might need more detail (is this a warning 
message, or the results of diagnose(), or  ... ?) but is most likely an 
indication that you don't have enough information to estimate a positive 
among-year variance (solution as above: treat YEAR as fixed instead).

   Depending on the size of your data set (total observations, and 
especially the total number of the less-common outcome (0 or 1)), it 
might be worth considering fitting an interaction between YEAR and any 
predictors that vary within years (Tree height, distance to quarry -- 
not sure about NDVI, that would depend on your observational design)

   cheers
    Ben Bolker
> 
> 
> 
> 
> Abdullah Nagy
> Assistant lecturer of Ecology
> Al-Azhar University in Cairo
> ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ????? ??????: ?? ???? ????????? ??????? ?? ??? ??????? ??? ????? ????? ?? ??????? ??? ?????? ????? ??? ???? ??????? ????. ?????? ??? ?? ??? ??? ???????? ??????? ??? ??????? ???????? ?????? ?????? ????? ???? ??????? ?????? ????? ??????? ???? ?? ???? ??? ???? ????????? ?? ????. ??? ???? ???? ???? ??? ?? ????? ??? ??????? ????? ????? ?? ????? ???? ??? ????????? ??????? ???? ???? ???? ??? ?????? ???? ?? ??????? ?? ?????????. ????? ?????? ?? ???? ?????? ?????? ????? ???????? ?? ??? ??????? ??? ??? ?????? ??? ???? ???????? ??? ????? ?????? ??? ?? ????? ????? ?????? ??????? ?? ????? ????? ?? ?? ????? ?? ????? ???? ?????? ??? ?????? ?????????? Disclaimer: This message and any attachments are intended exclusively for the n
>   amed addressee(s) and may contain confidential information. If you are not the intended/named addressee/recipient you are hereby notified that any disclosure, copying, distribution or use of the information (including attachments) contained herein is strictly prohibited under current legislation. Likewise, no action may be taken based on the information provided in this message unless a prior written consent is obtained from the person in charge of such content. If you received this transmission in error, please contact the sender immediately and permanently delete the email, any attachments, and all copies thereof from any drives or storage media and destroy any printouts of the email or attachments. It is worth noting that any opinions expressed in this communication are not necessarily those of Al-Azhar University and they are solely the writer's. We advise that, in keeping with best business practice, the recipient must ensure they are actually virus free, as Al-Azhar University
>    bears no liability for any damage resulting from any viruses communicated via this email.
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics

From hedyeh@h @end|ng |rom u@c@edu  Fri May 27 20:17:59 2022
From: hedyeh@h @end|ng |rom u@c@edu (Hedyeh Ahmadi)
Date: Fri, 27 May 2022 18:17:59 +0000
Subject: [R-sig-ME] Questions regarding bs() interaction in lmer()
Message-ID: <BYAPR07MB50944D6AC4278D8598AE7D2ED1D89@BYAPR07MB5094.namprd07.prod.outlook.com>

Hello All,
I am running a lmer() model as follows. The following model gives all the interactions for before and after knots shown below. I have 3 questions:

  1.  I would like to suppress the highlighted interactions that is before and after the knot. I have tried the update() command with -bs(reshist, knots = 7, degree = 1)2:bs(age, knots = 126, degree = 1)1, which did not work.
  2.  How would I define the knots for the interaction term and is what I have a correct specification?
  3.  Is there a package that would take this model and output interpretable parameter estimates for splines?

Thank you in advance for your time

smri_left <-
  lmer(smri ~ 1 +
         bs(reshist, knots=7, degree = 1)+
         bs(age, knots=126, degree = 1)+
         bs(reshist, knots=7.787, degree = 1)*bs(age, knots=126, degree = 1)+
         ethnicity.1_bl + high.bl + neighb_avg_p_bl
         (1|site/subject)  ,
         REML = FALSE,
         control = lmerControl(optimizer = "Nelder_Mead"),
         data=WG)

Fixed effects:
                                                                                                                            Estimate         Std. Error     df                      t value             Pr(>|t|)
(Intercept)                                                                                                         2.6679353    0.0114634  652.5132635    232.736        < 0.0000000000000002 ***
bs(reshist, knots = 7, degree = 1)1                                                                0.0090600    0.0100475 1059.5955954   0.902              0.36741
bs(reshist, knots = 7, degree = 1)2                                                                0.0082266    0.0132667  559.1847547   0.620              0.53545
bs(age, knots = 126, degree = 1)1                                                                -0.0137868    0.0081986 6143.2470351  -1.682              0.09270 .
bs(age, knots = 126, degree = 1)2                                                                -0.0834209    0.0094686 5534.8186306  -8.810          < 0.0000000000000002 ***
ethnicity.1_blHispanic                                                                                     0.0154627    0.0030354 3492.3232613   5.094           0.00000036893 ***
ethnicity.1_blOther                                                                                          0.0158286    0.0033031 8304.5129245   4.792             0.00000167908 ***
high.blBachelor                                                                                                 0.0022701    0.0044227 8691.0032954   0.513              0.60777
high.blHS Diploma/GED                                                                                  -0.0009402    0.0045056 8757.0427133  -0.209              0.83472
neighb_avg_p_bl                                                                                              0.0001371    0.0009128 8628.5294654   0.150              0.88060
bs(reshist, knots = 7, degree = 1)1:bs(age, knots = 126, degree = 1)1   -0.0091887    0.0095058 6161.8339520  -0.967              0.33376
bs(reshist, knots = 7, degree = 1)2:bs(age, knots = 126, degree = 1)1   -0.0148081    0.0128074 6412.6567741  -1.156              0.24764
bs(reshist, knots = 7, degree = 1)1:bs(age, knots = 126, degree = 1)2    0.0080632    0.0109929 5538.9731189   0.733              0.46329
bs(reshist, knots = 7, degree = 1)2:bs(age, knots = 126, degree = 1)2   -0.0592144    0.0150237 5595.8147976  -3.941        0.00008199861 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Best,

Hedyeh Ahmadi, Ph.D.
Statistician
Keck School of Medicine
Department of Preventive Medicine
University of Southern California

LinkedIn
www.linkedin.com/in/hedyeh-ahmadi<http://www.linkedin.com/in/hedyeh-ahmadi>
<http://www.linkedin.com/in/hedyeh-ahmadi><http://www.linkedin.com/in/hedyeh-ahmadi>





	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Sat May 28 02:06:38 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Fri, 27 May 2022 20:06:38 -0400
Subject: [R-sig-ME] Questions regarding bs() interaction in lmer()
In-Reply-To: <BYAPR07MB50944D6AC4278D8598AE7D2ED1D89@BYAPR07MB5094.namprd07.prod.outlook.com>
References: <BYAPR07MB50944D6AC4278D8598AE7D2ED1D89@BYAPR07MB5094.namprd07.prod.outlook.com>
Message-ID: <5dc9ae6d-36d3-9782-87f3-382b348f89fc@gmail.com>



On 2022-05-27 2:17 p.m., Hedyeh Ahmadi wrote:
> Hello All,
> I am running a lmer() model as follows. The following model gives all the interactions for before and after knots shown below. I have 3 questions:
> 
>    1.  I would like to suppress the highlighted interactions that is before and after the knot. I have tried the update() command with -bs(reshist, knots = 7, degree = 1)2:bs(age, knots = 126, degree = 1)1, which did not work.

   It's not clear to me that there's an easy way to suppress particular 
components of such an interaction.

>    2.  How would I define the knots for the interaction term and is what I have a correct specification?

   The knots would be constructed from the interaction.

>    3.  Is there a package that would take this model and output interpretable parameter estimates for splines?

   I don't really know of *any* spline framework that gives readily 
interpretable parameter estimates.  In my experience people usually look 
at a variety of different kinds of prediction plots.

    If you're going to do a lot of stuff with splines I would recommend 
the mgcv package, which allows a broad range of flexible spline 
parameterizations, along with the 'gratia' package for post-processing 
and the paper by Pedersen et al on hierarchical GAMs:

Pedersen, Eric J., David L. Miller, Gavin L. Simpson, and Noam Ross. 
?Hierarchical Generalized Additive Models in Ecology: An Introduction 
with Mgcv.? PeerJ 7 (May 27, 2019): e6876. 
https://doi.org/10.7717/peerj.6876.

   You might get more useful advice from the list if you give a little 
bit more context / reasons *why* you want to do these particular things ...

   cheers
    Ben Bolker
> 
> Thank you in advance for your time
> 
> smri_left <-
>    lmer(smri ~ 1 +
>           bs(reshist, knots=7, degree = 1)+
>           bs(age, knots=126, degree = 1)+
>           bs(reshist, knots=7.787, degree = 1)*bs(age, knots=126, degree = 1)+
>           ethnicity.1_bl + high.bl + neighb_avg_p_bl
>           (1|site/subject)  ,
>           REML = FALSE,
>           control = lmerControl(optimizer = "Nelder_Mead"),
>           data=WG)
> 
> Fixed effects:
>                                                                                                                              Estimate         Std. Error     df                      t value             Pr(>|t|)
> (Intercept)                                                                                                         2.6679353    0.0114634  652.5132635    232.736        < 0.0000000000000002 ***
> bs(reshist, knots = 7, degree = 1)1                                                                0.0090600    0.0100475 1059.5955954   0.902              0.36741
> bs(reshist, knots = 7, degree = 1)2                                                                0.0082266    0.0132667  559.1847547   0.620              0.53545
> bs(age, knots = 126, degree = 1)1                                                                -0.0137868    0.0081986 6143.2470351  -1.682              0.09270 .
> bs(age, knots = 126, degree = 1)2                                                                -0.0834209    0.0094686 5534.8186306  -8.810          < 0.0000000000000002 ***
> ethnicity.1_blHispanic                                                                                     0.0154627    0.0030354 3492.3232613   5.094           0.00000036893 ***
> ethnicity.1_blOther                                                                                          0.0158286    0.0033031 8304.5129245   4.792             0.00000167908 ***
> high.blBachelor                                                                                                 0.0022701    0.0044227 8691.0032954   0.513              0.60777
> high.blHS Diploma/GED                                                                                  -0.0009402    0.0045056 8757.0427133  -0.209              0.83472
> neighb_avg_p_bl                                                                                              0.0001371    0.0009128 8628.5294654   0.150              0.88060
> bs(reshist, knots = 7, degree = 1)1:bs(age, knots = 126, degree = 1)1   -0.0091887    0.0095058 6161.8339520  -0.967              0.33376
> bs(reshist, knots = 7, degree = 1)2:bs(age, knots = 126, degree = 1)1   -0.0148081    0.0128074 6412.6567741  -1.156              0.24764
> bs(reshist, knots = 7, degree = 1)1:bs(age, knots = 126, degree = 1)2    0.0080632    0.0109929 5538.9731189   0.733              0.46329
> bs(reshist, knots = 7, degree = 1)2:bs(age, knots = 126, degree = 1)2   -0.0592144    0.0150237 5595.8147976  -3.941        0.00008199861 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> Best,
> 
> Hedyeh Ahmadi, Ph.D.
> Statistician
> Keck School of Medicine
> Department of Preventive Medicine
> University of Southern California
> 
> LinkedIn
> www.linkedin.com/in/hedyeh-ahmadi<http://www.linkedin.com/in/hedyeh-ahmadi>
> <http://www.linkedin.com/in/hedyeh-ahmadi><http://www.linkedin.com/in/hedyeh-ahmadi>
> 
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics


From bbo|ker @end|ng |rom gm@||@com  Tue May 31 23:49:52 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Tue, 31 May 2022 17:49:52 -0400
Subject: [R-sig-ME] lme4 package
In-Reply-To: <CAJt0zy_f=_v331+fNr+q_D2KPGUOtx_f2fYXwbqsNt-mi3V-6A@mail.gmail.com>
References: <CAJt0zy_f=_v331+fNr+q_D2KPGUOtx_f2fYXwbqsNt-mi3V-6A@mail.gmail.com>
Message-ID: <6c28a499-48ec-704d-d70f-c91e928bbf42@gmail.com>

   This is not a particularly mixed-model-specific question.  The 
general approach would be something like:

   var_combs <- list(model1 = c("X1", "X7"),
                model2 = c("X2", "X3", "X12"),
                model3 = c("X4", "X8", "X9", "X11"),
         ## etc.)

(I don't know the logic behind your choice of combinations of predictors 
so can't automate it any further)

   Then:

  results <- list()
  for (m in names(var_combs)) {
      form <- reformulate(c(var_combs[[m]], "(1|group)"), response = "Y")
      fit <- lmer(form, data = ..., ....)
      results[[m]] <- fit
  }

   This assumes that the random-effects component is only variation in 
the intercept across `group`.

There are a huge variety of alternatives -- you could use lapply() or 
purrr::map() instead of a for loop, do something to parallelize the fits 
across cores, etc. etc..

On 2022-05-31 4:38 p.m., mina jahan wrote:
> Hi,
> I want to run linear mixed-effects models based on the different subsets of
> predictor variables (the outcome variable in each model is similar) using
> package lme4. For example, a dataset contains a Y as a longitudinal outcome
> variable and X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, and X12 as
> predictor variables. I want to create a loop to run these models:
> Y~X1+X7
> Y~X2+X3+X12
> Y~X4+X8+X9+X11
> Y~X5+X6+X10
> 
> Please guide me.
> 
> Best regards,
> Mina
> 
> 	[[alternative HTML version deleted]]
> 

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics


From bbo|ker @end|ng |rom gm@||@com  Tue May 31 23:53:34 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Tue, 31 May 2022 17:53:34 -0400
Subject: [R-sig-ME] Fwd: Band - Structure variance and covariance
In-Reply-To: <fd3c66be-6a2d-559f-6ab5-866197c5cbde@gmail.com>
References: <fd3c66be-6a2d-559f-6ab5-866197c5cbde@gmail.com>
Message-ID: <42570516-1ff0-10f0-bfb4-184ee198c55e@gmail.com>

   This went to r-sig-mixed-models-owner at r-project.org by accident, 
resending ...


-------- Forwarded Message --------
Subject: Re: Band - Structure variance and covariance
Date: Tue, 31 May 2022 10:38:24 -0400
From: Ben Bolker <bbolker at gmail.com>
To: Breno Gabriel <omatematico.breno at gmail.com>, 
r-sig-mixed-models-owner at r-project.org

   There is no lme package [sic], I'm going to assume that you're using 
the lme() *function* from the nlme package.  In principle you can define 
your own classes of covariance matrix, but in practice this is tricky / 
not very well documented. (If you want to dig in this I would take a 
look at the internal code of the correlation structures defined in the 
'ape' package ...

   Others may have more useful advice ...

On 2022-05-31 10:14 a.m., Breno Gabriel wrote:
> Hi.
> How could I introduce a matrix of *Band* variances and covariances into
> error matrix R?
> I have a mix of mixed effects where repeated measurements are not
> equidistant, but I did not find any computational routine or how to
> introduce this matrix through *R software*.
> I am using the *lme library*.
> 
> The matrix she has this structure (for example 3x3):
> 
> \sigma_1         \sigma_{12}       0
> \sigma_{12}    \sigma_2          \sigma_{23}
> 0                     \sigma_{23}     \sigma_3
> 
> 	[[alternative HTML version deleted]]
> 

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics


From bbo|ker @end|ng |rom gm@||@com  Wed Jun  1 00:31:34 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Tue, 31 May 2022 18:31:34 -0400
Subject: [R-sig-ME] lme4 package
In-Reply-To: <CAJt0zy8+a1r9PB_PMoDcBtgykFU09dfda=K8R7uXiw4R=ssCKg@mail.gmail.com>
References: <CAJt0zy_f=_v331+fNr+q_D2KPGUOtx_f2fYXwbqsNt-mi3V-6A@mail.gmail.com>
 <6c28a499-48ec-704d-d70f-c91e928bbf42@gmail.com>
 <CAJt0zy8+a1r9PB_PMoDcBtgykFU09dfda=K8R7uXiw4R=ssCKg@mail.gmail.com>
Message-ID: <c2429ab6-a7a8-29c7-792c-0df923cc3bbe@gmail.com>

  [please keep r-sig-mixed-models at r-project.org in the Cc: list; I 
really don't have the energy to engage in off-list consultations ...]

   If you have 240 subsets of predictor variables, they must be defined 
*somewhere*, either as a list that you've got already or in some 
programmatic way.  Again, this isn't a mixed-model-specific question ...

   How do you decide which subsets of the variables you're going to try? 
  (There aren't any obvious patterns in the examples you've given here.)

   Ben Bolker

On 2022-05-31 6:06 p.m., mina jahan wrote:
> Dear Ben,
> Thank you for your response. This code is very useful.
> My data file contains 1000 predictor variables and I should run lmer 
> model with different subsets of predictor variables (e.g. a model with 5 
> predictor variables, a model with 7 predictor variables, ...). I use 
> your code to define subsets of the predictor variables.
> "var_combs <- list(model1 = c("X1", "X7"),model2 = c("X2", "X3", 
> "X12"),model3 = c("X4", "X8", "X9", "X11"))"
> But this code will be too long for 240 subsets of predictor variables. 
> Is there a remedial to solve this problem?
> 
> 
> Best regards,
> Mina
> 
> 
> 
> 
> 
> On Wed, 1 Jun 2022 at 02:20, Ben Bolker <bbolker at gmail.com 
> <mailto:bbolker at gmail.com>> wrote:
> 
>      ? ?This is not a particularly mixed-model-specific question.? The
>     general approach would be something like:
> 
>      ? ?var_combs <- list(model1 = c("X1", "X7"),
>      ? ? ? ? ? ? ? ? model2 = c("X2", "X3", "X12"),
>      ? ? ? ? ? ? ? ? model3 = c("X4", "X8", "X9", "X11"),
>      ? ? ? ? ?## etc.)
> 
>     (I don't know the logic behind your choice of combinations of
>     predictors
>     so can't automate it any further)
> 
>      ? ?Then:
> 
>      ? results <- list()
>      ? for (m in names(var_combs)) {
>      ? ? ? form <- reformulate(c(var_combs[[m]], "(1|group)"), response
>     = "Y")
>      ? ? ? fit <- lmer(form, data = ..., ....)
>      ? ? ? results[[m]] <- fit
>      ? }
> 
>      ? ?This assumes that the random-effects component is only variation in
>     the intercept across `group`.
> 
>     There are a huge variety of alternatives -- you could use lapply() or
>     purrr::map() instead of a for loop, do something to parallelize the
>     fits
>     across cores, etc. etc..
> 
>     On 2022-05-31 4:38 p.m., mina jahan wrote:
>      > Hi,
>      > I want to run linear mixed-effects models based on the different
>     subsets of
>      > predictor variables (the outcome variable in each model is
>     similar) using
>      > package lme4. For example, a dataset contains a Y as a
>     longitudinal outcome
>      > variable and X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, and X12 as
>      > predictor variables. I want to create a loop to run these models:
>      > Y~X1+X7
>      > Y~X2+X3+X12
>      > Y~X4+X8+X9+X11
>      > Y~X5+X6+X10
>      >
>      > Please guide me.
>      >
>      > Best regards,
>      > Mina
>      >
>      >? ? ? ?[[alternative HTML version deleted]]
>      >
> 
>     -- 
>     Dr. Benjamin Bolker
>     Professor, Mathematics & Statistics and Biology, McMaster University
>     Director, School of Computational Science and Engineering
>     (Acting) Graduate chair, Mathematics & Statistics
> 

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics


From m|n@j@h@ng|r|984 @end|ng |rom gm@||@com  Wed Jun  1 00:17:15 2022
From: m|n@j@h@ng|r|984 @end|ng |rom gm@||@com (mina jahan)
Date: Wed, 1 Jun 2022 02:47:15 +0430
Subject: [R-sig-ME] lme4 package
In-Reply-To: <6c28a499-48ec-704d-d70f-c91e928bbf42@gmail.com>
References: <CAJt0zy_f=_v331+fNr+q_D2KPGUOtx_f2fYXwbqsNt-mi3V-6A@mail.gmail.com>
 <6c28a499-48ec-704d-d70f-c91e928bbf42@gmail.com>
Message-ID: <CAJt0zy_Ei7iJW8vw2h=UMAfZ2OGZzueXP3oAhZ2juEovTt=JNQ@mail.gmail.com>

Dear Ben,
Thank you for your response. This code is very useful.
My data file contains 1000 predictor variables and I should run lmer model
with different subsets of predictor variables (e.g. a model with 5
predictor variables, a model with 7 predictor variables, ...). I use your
code to define subsets of the predictor variables.
"var_combs <- list(model1 = c("X1", "X7"),model2 = c("X2", "X3",
"X12"),model3 = c("X4", "X8", "X9", "X11"))"
But this code will be too long for 240 subsets of predictor variables. Is
there a remedial to solve this problem?


Best regards,
Mina

On Wed, Jun 1, 2022, 2:20 AM Ben Bolker <bbolker at gmail.com> wrote:

>    This is not a particularly mixed-model-specific question.  The
> general approach would be something like:
>
>    var_combs <- list(model1 = c("X1", "X7"),
>                 model2 = c("X2", "X3", "X12"),
>                 model3 = c("X4", "X8", "X9", "X11"),
>          ## etc.)
>
> (I don't know the logic behind your choice of combinations of predictors
> so can't automate it any further)
>
>    Then:
>
>   results <- list()
>   for (m in names(var_combs)) {
>       form <- reformulate(c(var_combs[[m]], "(1|group)"), response = "Y")
>       fit <- lmer(form, data = ..., ....)
>       results[[m]] <- fit
>   }
>
>    This assumes that the random-effects component is only variation in
> the intercept across `group`.
>
> There are a huge variety of alternatives -- you could use lapply() or
> purrr::map() instead of a for loop, do something to parallelize the fits
> across cores, etc. etc..
>
> On 2022-05-31 4:38 p.m., mina jahan wrote:
> > Hi,
> > I want to run linear mixed-effects models based on the different subsets
> of
> > predictor variables (the outcome variable in each model is similar) using
> > package lme4. For example, a dataset contains a Y as a longitudinal
> outcome
> > variable and X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, and X12 as
> > predictor variables. I want to create a loop to run these models:
> > Y~X1+X7
> > Y~X2+X3+X12
> > Y~X4+X8+X9+X11
> > Y~X5+X6+X10
> >
> > Please guide me.
> >
> > Best regards,
> > Mina
> >
> >       [[alternative HTML version deleted]]
> >
>
> --
> Dr. Benjamin Bolker
> Professor, Mathematics & Statistics and Biology, McMaster University
> Director, School of Computational Science and Engineering
> (Acting) Graduate chair, Mathematics & Statistics
>

	[[alternative HTML version deleted]]


From y@nm|n@w@ng @end|ng |rom p|ymouth@@c@uk  Mon Jun  6 08:56:27 2022
From: y@nm|n@w@ng @end|ng |rom p|ymouth@@c@uk (Yanmin Wang)
Date: Mon, 6 Jun 2022 06:56:27 +0000
Subject: [R-sig-ME] whether the data can include participants compelete the
 questionnaire on only one accasion
Message-ID: <PR2PR03MB51487D9171FF53FC652CD3D4C5A29@PR2PR03MB5148.eurprd03.prod.outlook.com>

it is known that linear mixed model is suitable for repeated measure. my longitudinal data is unbalanced, and many participants compelete the questionnaire on only one accasion. can i include these participants?
any help is appeciated!
________________________________
[http://www.plymouth.ac.uk/images/email_footer.gif]<http://www.plymouth.ac.uk/worldclass>

This email and any files with it are confidential and intended solely for the use of the recipient to whom it is addressed. If you are not the intended recipient then copying, distribution or other use of the information contained is strictly prohibited and you should not rely on it. If you have received this email in error please let the sender know immediately and delete it from your system(s). Internet emails are not necessarily secure. While we take every care, University of Plymouth accepts no responsibility for viruses and it is your responsibility to scan emails and their attachments. University of Plymouth does not accept responsibility for any changes made after it was sent. Nothing in this email or its attachments constitutes an order for goods or services unless accompanied by an official order form.

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Tue Jun  7 01:30:46 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 6 Jun 2022 19:30:46 -0400
Subject: [R-sig-ME] 
 whether the data can include participants compelete the
 questionnaire on only one accasion
In-Reply-To: <PR2PR03MB51487D9171FF53FC652CD3D4C5A29@PR2PR03MB5148.eurprd03.prod.outlook.com>
References: <PR2PR03MB51487D9171FF53FC652CD3D4C5A29@PR2PR03MB5148.eurprd03.prod.outlook.com>
Message-ID: <4a63b995-5aff-c9d6-fb46-31df1d5e2d20@gmail.com>

    This is theoretically possible.
   For some degree of sparsity (i.e. a small enough fraction of 
participants with >1 observation) it will become computationally 
impractical/not worth the trouble.
    I would go ahead and try it out on your data.  Ideally you would try 
it out on some synthetic data, for example something like:

library(lme4)
set.seed(101)
subject <- rep(1:100, times = rep(1:2, each = 50))
x <- rnorm(length(subject))
dd <- data.frame(x, subject)
dd$y <- simulate(~ x + (1|subject),
                  newdata = dd,
                  newparams = list(beta = c(1,2), theta = 1, sigma = 1),
                  family = gaussian)[[1]]
lmer(y ~ x + (1|subject), family = gaussian, data = dd)


   (try something that matches your experimental/observational design 
reasonably well)

On 2022-06-06 2:56 a.m., Yanmin Wang wrote:
> it is known that linear mixed model is suitable for repeated measure. my longitudinal data is unbalanced, and many participants compelete the questionnaire on only one accasion. can i include these participants?
> any help is appeciated!
> ________________________________
> [http://www.plymouth.ac.uk/images/email_footer.gif]<http://www.plymouth.ac.uk/worldclass>
> 
> This email and any files with it are confidential and intended solely for the use of the recipient to whom it is addressed. If you are not the intended recipient then copying, distribution or other use of the information contained is strictly prohibited and you should not rely on it. If you have received this email in error please let the sender know immediately and delete it from your system(s). Internet emails are not necessarily secure. While we take every care, University of Plymouth accepts no responsibility for viruses and it is your responsibility to scan emails and their attachments. University of Plymouth does not accept responsibility for any changes made after it was sent. Nothing in this email or its attachments constitutes an order for goods or services unless accompanied by an official order form.
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics


From K@ppe|D @end|ng |rom c@rd|||@@c@uk  Mon Jun 20 11:54:53 2022
From: K@ppe|D @end|ng |rom c@rd|||@@c@uk (Djenifer Kappel)
Date: Mon, 20 Jun 2022 09:54:53 +0000
Subject: [R-sig-ME] glmmTMB dispformula models
Message-ID: <LO6P265MB5918F2A8BF0A9B0B641F8C05FCB09@LO6P265MB5918.GBRP265.PROD.OUTLOOK.COM>

Hi all,

I am using glmmTMB to model the effects of genetic factors on longitudinal data of blood tests.
We are interested in the effect of those factors on the mean blood test outcome (using a gamma distribution), but also if they affect the variance of the outcome (dispersion).
This is what I am testing at the moment:
  model <- glmmTMB(outcome ~ gene_sets + covar1+ covar2+covar3+ (1|Individual),
                    dispformula = ~gene_sets,
                    data = mydata,
                    family = Gamma(link=log),
                    na.action = na.exclude,
                    control = glmmTMBControl(profile = TRUE))

After reading the glmmTMB vignette it is not clear to me what variance is being modelled when we specify a dispformula.
Is this the residual variance of the model on the full dataset or the intra-individual variability (on the repeated measures).
I supposed it is the latter given that we cannot specify a random effect on dispformula, but I would appreciate if anyone could help me understand this.

Many thanks!
Djenifer

--

Djenifer Kappel, Ph.D
Research Associate

MRC Centre for Neuropsychiatric Genetics and Genomics
Room 2.01 Desk 36
Cardiff University School of Medicine
Hadyn Ellis Building
Maindy Road
Cardiff
CF24 4HQ

Email: KappelD at cardiff.ac.uk<mailto:KappelD at cardiff.ac.uk>

Please note, replies are not expected outside of your working hours.
The University welcomes correspondence in Welsh or English. Corresponding in Welsh will not lead to any delay.
Djenifer Kappel, Ph.D
Cydymaith Ymchwil

Canolfan y Cyngor Ymchwil Feddygol ar gyfer Geneteg Niwroseiciatrig a Genomeg
Ystafell 2.01 - 36
Ysgol Meddygaeth Prifysgol Caerdydd
Adeliad Hadyn Ellis
Heol Maindy
Caerdydd
CF24 4HQ

Ebost: KappelD at cardiff.ac.uk<mailto:KappelD at cardiff.ac.uk>

Noder, ni ddisgwylir atebion y tu allan i'ch oriau gwaith arferol.
Mae'r Brifysgol yn croesawu gohebiaeth yn Gymraeg neu yn Saesneg.
Ni fydd gohebu yn Gymraeg yn creu unrhyw oedi.



	[[alternative HTML version deleted]]


From jhm@|ndon@|d @end|ng |rom gm@||@com  Mon Jun 20 22:58:04 2022
From: jhm@|ndon@|d @end|ng |rom gm@||@com (John H Maindonald)
Date: Tue, 21 Jun 2022 08:58:04 +1200
Subject: [R-sig-ME] glmmTMB dispformula models
In-Reply-To: <LO6P265MB5918F2A8BF0A9B0B641F8C05FCB09@LO6P265MB5918.GBRP265.PROD.OUTLOOK.COM>
References: <LO6P265MB5918F2A8BF0A9B0B641F8C05FCB09@LO6P265MB5918.GBRP265.PROD.OUTLOOK.COM>
Message-ID: <7EB748AE-7452-4610-B18A-29418F1F6687@statsresearch.co.nz>

You are, as I understand the matter, modifying the variance as specified by
the Gamma family.  See 
  vignette('timeMortality', package='qra?)
where it is the betabinomial variance that is modified. 
John Maindonald.

> On 20/06/2022, at 21:54, Djenifer Kappel <KappelD at cardiff.ac.uk> wrote:
> 
> Hi all,
> 
> I am using glmmTMB to model the effects of genetic factors on longitudinal data of blood tests.
> We are interested in the effect of those factors on the mean blood test outcome (using a gamma distribution), but also if they affect the variance of the outcome (dispersion).
> This is what I am testing at the moment:
>  model <- glmmTMB(outcome ~ gene_sets + covar1+ covar2+covar3+ (1|Individual),
>                    dispformula = ~gene_sets,
>                    data = mydata,
>                    family = Gamma(link=log),
>                    na.action = na.exclude,
>                    control = glmmTMBControl(profile = TRUE))
> 
> After reading the glmmTMB vignette it is not clear to me what variance is being modelled when we specify a dispformula.
> Is this the residual variance of the model on the full dataset or the intra-individual variability (on the repeated measures).
> I supposed it is the latter given that we cannot specify a random effect on dispformula, but I would appreciate if anyone could help me understand this.
> 
> Many thanks!
> Djenifer
> 
> --
> 
> Djenifer Kappel, Ph.D
> Research Associate
> 
> MRC Centre for Neuropsychiatric Genetics and Genomics
> Room 2.01 Desk 36
> Cardiff University School of Medicine
> Hadyn Ellis Building
> Maindy Road
> Cardiff
> CF24 4HQ
> 
> Email: KappelD at cardiff.ac.uk<mailto:KappelD at cardiff.ac.uk>
> 
> Please note, replies are not expected outside of your working hours.
> The University welcomes correspondence in Welsh or English. Corresponding in Welsh will not lead to any delay.
> Djenifer Kappel, Ph.D
> Cydymaith Ymchwil
> 
> Canolfan y Cyngor Ymchwil Feddygol ar gyfer Geneteg Niwroseiciatrig a Genomeg
> Ystafell 2.01 - 36
> Ysgol Meddygaeth Prifysgol Caerdydd
> Adeliad Hadyn Ellis
> Heol Maindy
> Caerdydd
> CF24 4HQ
> 
> Ebost: KappelD at cardiff.ac.uk<mailto:KappelD at cardiff.ac.uk>
> 
> Noder, ni ddisgwylir atebion y tu allan i'ch oriau gwaith arferol.
> Mae'r Brifysgol yn croesawu gohebiaeth yn Gymraeg neu yn Saesneg.
> Ni fydd gohebu yn Gymraeg yn creu unrhyw oedi.
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From rengg||@ @end|ng |rom ethz@ch  Wed Jun 22 12:38:41 2022
From: rengg||@ @end|ng |rom ethz@ch (Aaron Renggli)
Date: Wed, 22 Jun 2022 12:38:41 +0200
Subject: [R-sig-ME] glmmTMB: profile confidence interval scale
Message-ID: <CANJ0tRgihcrxFUtavq_BjLO0zuwdXgHqFmJvLpknQU3xP266Gw@mail.gmail.com>

I have an issue using profiled confidence intervals in a random slope
model.

The helpfile for confint.glmmTMB regarding the different methods to compute
confidence intervals says:
" [...] while "profile" and "uniroot" report them on the underlying
("theta") scale: for each random effect, the first set of parameter values
are standard deviations on the log scale, while remaining parameters
represent correlations on the scaled Cholesky scale (see the"
It just cuts off there.
I don't understand what is meant by the scaled Cholesky scale for the
correlation parameters. How would I transform the respective confidence
interval back to the original scale?

Thank you for your help

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Wed Jun 22 16:11:49 2022
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 22 Jun 2022 10:11:49 -0400
Subject: [R-sig-ME] glmmTMB: profile confidence interval scale
In-Reply-To: <CANJ0tRgihcrxFUtavq_BjLO0zuwdXgHqFmJvLpknQU3xP266Gw@mail.gmail.com>
References: <CANJ0tRgihcrxFUtavq_BjLO0zuwdXgHqFmJvLpknQU3xP266Gw@mail.gmail.com>
Message-ID: <3e2ee201-7930-65f3-d159-e22736fcca59@gmail.com>

   This is a documentation bug, will fix.

   If you go to the *current* (online) version of the 'covstruct' 
vignette (which is what was meant to be referred to, and which corrects 
an error found in the current version):  ?=?/sqrt(1+?^2), where ? is the 
Cholesky-scale parameter and rho is the correlation.




https://glmmtmb.github.io/glmmTMB/articles/covstruct.html

On 2022-06-22 6:38 a.m., Aaron Renggli wrote:
> I have an issue using profiled confidence intervals in a random slope
> model.
> 
> The helpfile for confint.glmmTMB regarding the different methods to compute
> confidence intervals says:
> " [...] while "profile" and "uniroot" report them on the underlying
> ("theta") scale: for each random effect, the first set of parameter values
> are standard deviations on the log scale, while remaining parameters
> represent correlations on the scaled Cholesky scale (see the"
> It just cuts off there.
> I don't understand what is meant by the scaled Cholesky scale for the
> correlation parameters. How would I transform the respective confidence
> interval back to the original scale?
> 
> Thank you for your help
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
(Acting) Graduate chair, Mathematics & Statistics


From emm@@me||or @end|ng |rom br|@to|@@c@uk  Thu Jun 23 17:21:23 2022
From: emm@@me||or @end|ng |rom br|@to|@@c@uk (Emma Mellor)
Date: Thu, 23 Jun 2022 15:21:23 +0000
Subject: [R-sig-ME] question about convergence warning and some odd-looking
 odds ratios in a glmer model
Message-ID: <DB9PR06MB77709385850D4167C42136B0A4B59@DB9PR06MB7770.eurprd06.prod.outlook.com>

Hi all,

I'm running glmer models in lme4 on: R version 4.1.2,
Platform: x86_64-w64-mingw32/x64 (64-bit),
Running under: Windows 10 x64 (build 19044)

My outcome (presence/absence of abnormal behaviour in pet birds) is binary, so I used family = binomial.  Random effects are 'Owner_ID' - in reality, very few birds share a household, and the vast majority are single birds. I'm getting warning messages about convergence when 'Species_ID' is included the model as a predictor (I do not get error messages with other predictors - it's just this one). I've tabulated the data and I don't see any obvious reason for the error, such as missing/very few cases per levels within variables, so I'm at a bit of a loss as what to do about it. Copy of warning message next, and I'll provide a copy of my code below:

Warning messages:
1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.0562293 (tol = 0.002, component 1)
2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?

Species does have a significant effect on my outcome (which is expected), so I've spilt the dataset by species to do pairwise comparisons to try and work out what's what. When I do so, I don't get the warnings but I do get, in some cases, enormous odds ratios (and one set of normal-looking ones too).

Perhaps I'm over-reacting? But given the previous warnings I got with the initial (all four species) model, it gave me reason to pause and want to check.

Can anyone advise about what I should do about the warnings, and whether I can trust those odds ratios, please?

Thanks for any help you're able to give!

Best wishes,

Emma

Code to demonstrate here:

library(lme4)
library(dplyr)

data<-structure(list(Owner_ID = c(62L, 222L, 147L, 187L, 407L, 208L, 205L, 348L, 29L, 244L,
                                146L, 414L, 278L, 528L, 433L, 1039L, 930L, 902L, 1466L, 1977L,
                                704L, 2020L, 1423L, 782L, 308L, 291L, 61L, 1512L, 1164L, 1164L,
                                28L, 1549L, 1882L, 1181L, 569L, 135L, 609L, 1059L, 663L, 1465L,
                                1207L, 713L, 1420L, 1318L, 1357L, 1727L, 1415L, 1572L, 948L,
                                948L, 948L, 1897L, 1649L, 1162L, 974L, 1153L, 1802L, 1412L, 872L,
                                1708L, 1360L, 1616L, 1960L, 1960L, 1965L, 1794L, 636L, 339L,
                                1783L, 1783L, 1594L, 1558L, 1695L, 1822L, 1700L, 1667L, 869L,
                                534L, 1667L, 524L, 1684L, 782L, 782L, 664L, 1700L, 67L, 1416L,
                                1418L, 1352L, 1321L, 1569L, 598L, 821L, 604L, 2013L, 1707L, 785L,
                                1534L, 1299L, 565L, 1284L, 1244L, 1385L, 853L, 1014L, 1542L,
                                1771L, 1876L, 1063L, 1212L, 1000L, 1784L, 1439L, 547L, 745L,
                                717L, 702L, 1829L, 1503L, 862L, 667L, 1135L, 1541L, 1507L, 1507L,
                                680L, 1597L, 741L, 801L, 856L, 1295L, 1295L, 955L, 1850L, 1769L,
                                1393L, 1393L, 1291L, 1111L, 1405L, 728L, 1993L, 901L, 566L, 1020L,
                                1293L, 1320L, 1087L, 1176L, 1898L, 1940L, 1177L, 1296L, 1462L,
                                863L, 1918L, 1854L, 1508L, 747L, 1647L, 1671L, 1646L, 1949L,
                                1094L, 1184L, 1455L, 899L, 1627L, 804L, 1490L, 1768L, 555L, 735L,
                                2022L, 1317L, 1620L, 1605L, 919L, 597L, 1849L, 1336L, 1898L,
                                29L, 1392L, 1438L, 1198L, 1943L, 1139L, 1716L, 1986L, 962L, 1372L,
                                1673L, 1640L, 1640L, 1805L, 1163L, 1172L, 1165L, 1915L, 1915L,
                                1576L, 1526L, 1436L, 1998L, 1590L, 906L, 770L, 1398L, 656L, 1207L,
                                1120L, 805L, 805L, 1437L, 1682L, 800L, 1543L, 1238L, 2008L, 1069L,
                                1243L, 1326L, 1170L, 802L, 873L, 943L, 1330L, 1586L, 1833L, 1717L,
                                1654L, 1748L, 881L, 1006L, 1637L, 1701L, 1823L, 1613L, 1599L,
                                875L, 1813L, 1725L, 1005L, 1322L, 1839L, 352L, 1674L, 1358L,
                                1688L, 1724L, 1367L, 1827L, 1286L, 1150L, 1780L, 842L, 1118L,
                                884L, 1440L, 173L, 1744L, 578L, 1635L, 1568L, 861L, 1091L, 650L,
                                1354L, 1368L, 815L, 721L, 1787L, 1787L, 1678L, 1864L, 1786L,
                                1698L, 1984L, 1612L, 1159L, 1611L, 822L, 1042L, 1042L, 858L,
                                858L, 1731L, 1655L, 993L, 160L, 478L, 445L, 239L, 531L, 303L,
                                18L, 171L, 531L, 492L, 159L, 74L, 456L, 488L, 305L, 423L, 477L,
                                467L, 9L, 464L, 102L, 232L, 131L, 118L, 143L, 34L, 196L, 30L,
                                153L, 412L, 371L, 104L, 16L, 63L, 248L, 212L, 482L, 480L, 266L,
                                259L, 283L, 283L, 148L, 156L, 523L, 368L, 513L, 241L, 246L, 441L,
                                229L, 320L, 245L, 287L, 468L, 468L, 383L, 367L, 54L, 380L, 25L,
                                37L, 73L, 496L, 546L, 382L, 369L, 236L, 113L, 365L, 499L, 227L,
                                465L, 425L, 1819L, 657L, 635L, 1910L, 924L, 924L, 1485L, 1289L,
                                627L, 1076L, 627L, 1219L, 874L, 1289L, 1292L, 1754L, 972L, 1084L,
                                1240L, 777L, 609L, 626L, 1500L, 788L, 1470L, 1136L, 582L, 1891L,
                                645L, 1557L, 339L, 1904L, 2029L, 1107L, 665L, 631L, 631L, 1214L,
                                1214L, 738L, 973L, 888L, 1919L, 1772L, 1772L, 351L, 314L, 342L,
                                1077L, 134L, 273L, 495L, 166L, 514L, 247L, 203L, 125L, 2026L,
                                154L, 1570L, 1947L, 1055L, 1231L, 1318L, 1437L, 626L, 206L, 164L,
                                708L, 1711L, 922L, 1900L, 93L, 385L, 84L, 1173L, 294L, 470L,
                                23L, 1113L, 870L, 870L, 1029L, 2009L, 963L, 1746L, 988L, 1108L,
                                1749L, 1749L, 1749L, 1749L, 1828L, 1103L, 1844L, 970L, 769L,
                                1188L, 1712L, 1188L, 1776L, 1901L, 860L, 1777L, 1325L, 1608L,
                                1828L, 1608L, 1542L, 328L, 1942L, 1843L, 967L, 1990L, 1852L,
                                1056L, 1847L, 204L, 1338L, 846L, 729L, 1393L, 864L, 1396L, 1393L,
                                1777L),
               Outcome = c(1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L,
                               0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
                               0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
                               0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L,
                               1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L,
                               0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L,
                               0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 1L,
                               0L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L,
                               0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L,
                               1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L,
                               1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
                               1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L,
                               0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L,
                               0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L,
                               1L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
                               0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 1L,
                               0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L,
                               0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L,
                               1L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 0L,
                               0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L,
                               0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 1L,
                               0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L,
                               0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
                               1L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
                               1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L,
                               1L, 1L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 1L,
                               1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L,
                               0L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 1L, 1L,
                               1L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L,
                               0L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L,
                               0L, 0L, 1L, 1L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 1L),
               Species_ID = c("C", "C", "C", "C", "C", "C", "D", "C", "C", "C", "C",
                              "C", "C", "C", "C", "D", "C", "A", "C", "C", "C", "C", "C", "C",
                              "C", "C", "D", "C", "C", "C", "C", "C", "C", "D", "D", "C", "A",
                              "A", "C", "C", "D", "C", "C", "D", "C", "C", "C", "C", "A", "A",
                              "C", "C", "C", "C", "C", "C", "A", "C", "C", "C", "C", "C", "A",
                              "D", "A", "C", "C", "A", "C", "C", "C", "C", "C", "C", "C", "C",
                              "C", "C", "C", "C", "D", "C", "C", "A", "A", "C", "C", "C", "C",
                              "C", "C", "C", "A", "A", "C", "C", "C", "C", "C", "C", "C", "D",
                              "C", "C", "A", "C", "C", "C", "C", "C", "D", "C", "C", "C", "C",
                              "D", "A", "C", "C", "C", "C", "C", "A", "C", "C", "C", "C", "C",
                              "C", "C", "C", "C", "D", "C", "C", "A", "C", "A", "D", "C", "A",
                              "C", "C", "D", "C", "C", "C", "D", "C", "C", "C", "C", "C", "C",
                              "C", "C", "C", "C", "C", "C", "C", "C", "C", "C", "C", "C", "C",
                              "C", "C", "C", "C", "C", "A", "C", "D", "C", "C", "C", "C", "C",
                              "D", "C", "C", "C", "A", "C", "C", "C", "C", "D", "C", "C", "C",
                              "A", "C", "A", "C", "C", "D", "A", "C", "A", "C", "A", "C", "A",
                              "A", "D", "D", "D", "A", "D", "A", "A", "A", "A", "D", "D", "A",
                              "C", "D", "D", "D", "C", "C", "C", "C", "C", "C", "A", "C", "C",
                              "C", "C", "C", "C", "C", "C", "A", "C", "C", "C", "C", "C", "A",
                              "C", "C", "C", "A", "C", "C", "C", "C", "D", "A", "C", "C", "C",
                              "C", "C", "C", "C", "C", "C", "C", "C", "C", "C", "C", "D", "C",
                              "C", "C", "C", "C", "A", "A", "A", "D", "A", "C", "C", "A", "A",
                              "A", "A", "A", "C", "A", "A", "C", "C", "C", "C", "C", "D", "D",
                              "D", "C", "C", "D", "C", "C", "D", "D", "D", "D", "D", "C", "D",
                              "D", "C", "C", "C", "C", "D", "C", "C", "D", "A", "D", "D", "C",
                              "C", "D", "A", "C", "A", "D", "D", "C", "C", "C", "D", "C", "C",
                              "D", "C", "C", "D", "C", "C", "C", "C", "A", "A", "C", "C", "C",
                              "C", "C", "D", "D", "C", "C", "C", "C", "C", "A", "D", "C", "D",
                              "A", "A", "C", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B",
                              "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B",
                              "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B",
                              "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B",
                              "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B",
                              "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B",
                              "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B",
                              "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B",
                              "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B",
                              "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B")))
data<-as.data.frame(data)
attach(data)

m1<-glmer(Outcome~ Species_ID+(1 |Owner_ID),
          data=data, family= (binomial(link="logit")),na.action = na.omit,
          glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 1000000)))
#get convergence error (tol >0.002)

#Ben Bolker advised to first do this as a check because of the warning:
m1_fit_all<-allFit(m1)

ss<-summary(m1_fit_all)
ss$which.OK
#all seem OK

m1b<-glmer(Outcome~ 1+(1 |Owner_ID),
           data=data, family= (binomial(link="logit")),na.action = na.omit,
           glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 1000000)))

anova(m1,m1b)
#so, an effect of species (as expected - in line with our previous research)
summary(m1)
#z values look a bit suspect (to me)

#splitting dataset by species (A v B first)
# to suss what's going on. Will give you an example of what look like mad odds ratios
a_b_data<-data %>% filter(Species_ID%in% c("A", "B"))
detach(data)
attach(a_b_data)

#and then re-run
m1c<-glmer(Outcome~ Species_ID+(1 |Owner_ID),
           data=a_b_data, family= (binomial(link="logit")),na.action = na.omit,
           glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 1000000)))
#no error this time

m1e<-glmer(Outcome~ 1+(1 |Owner_ID),
           data=a_b_data, family= (binomial(link="logit")),na.action = na.omit,
           glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 1000000)))

anova(m1c,m1e)
#species has an effect
summary(m1c)
#z value looks less crazy, but want to take a look at the odds ratios
se <- sqrt(diag(vcov(m1c)))
(tab <- cbind(Est = fixef(m1c), LL = fixef(m1c) - 1.96 * se, UL = fixef(m1c) + 1.96 *
                se))

#this for odds ratios. Upper limit looks especially mad (24,231,520!!)
exp(tab)

#I'll give an example of a normal-looking one too

#splitting dataset by another species pair (C v D)

c_d_data<-data %>% filter(Species_ID%in% c("C", "D"))
detach(a_b_data)
attach(c_d_data)

#and then re-run
m1r<-glmer(Outcome~ Species_ID+(1 |Owner_ID),
           data=c_d_data, family= (binomial(link="logit")),na.action = na.omit,
           glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 1000000)))
#no error
m1t<-glmer(Outcome~ 1+(1 |Owner_ID),
           data=c_d_data, family= (binomial(link="logit")),na.action = na.omit,
           glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 1000000)))

anova(m1r,m1t)
#effect of species here too
#get the odds ratios
se <- sqrt(diag(vcov(m1r)))
(tab <- cbind(Est = fixef(m1r), LL = fixef(m1r) - 1.96 * se, UL = fixef(m1r) + 1.96 *
                se))

#this for (less alarming) odds ratios.
exp(tab)

Dr Emma Mellor
Research Associate

University of Bristol
Bristol Veterinary School
Langford House
Langford
BS40 5DU

My working days are generally Monday, Tuesday and Friday. My work schedule may not be the same as yours - please do not feel obliged to respond outside of your own working hours


	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Fri Jun 24 15:52:38 2022
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Fri, 24 Jun 2022 15:52:38 +0200
Subject: [R-sig-ME] 
 question about convergence warning and some odd-looking
 odds ratios in a glmer model
In-Reply-To: <DB9PR06MB77709385850D4167C42136B0A4B59@DB9PR06MB7770.eurprd06.prod.outlook.com>
References: <DB9PR06MB77709385850D4167C42136B0A4B59@DB9PR06MB7770.eurprd06.prod.outlook.com>
Message-ID: <CAJuCY5zF5cUbGZ-6Tks3FreSVG_S5YAww1-+BGOWWNyXxLMCyQ@mail.gmail.com>

Dear Emma,

Running m1 without specifying the optimiser made the model run. However it
returns extreme estimates for the fixed effects and the random effect
variance. The problem is in the fact that most owners have only one bird.
This makes the random effect nearly behave as an observation level random
effect.

See if you can cluster the owners into some sensible groups or use some
covariates describing the home situation of the bird.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op do 23 jun. 2022 om 19:10 schreef Emma Mellor <emma.mellor at bristol.ac.uk>:

> Hi all,
>
> I'm running glmer models in lme4 on: R version 4.1.2,
> Platform: x86_64-w64-mingw32/x64 (64-bit),
> Running under: Windows 10 x64 (build 19044)
>
> My outcome (presence/absence of abnormal behaviour in pet birds) is
> binary, so I used family = binomial.  Random effects are 'Owner_ID' - in
> reality, very few birds share a household, and the vast majority are single
> birds. I'm getting warning messages about convergence when 'Species_ID' is
> included the model as a predictor (I do not get error messages with other
> predictors - it's just this one). I've tabulated the data and I don't see
> any obvious reason for the error, such as missing/very few cases per levels
> within variables, so I'm at a bit of a loss as what to do about it. Copy of
> warning message next, and I'll provide a copy of my code below:
>
> Warning messages:
> 1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 0.0562293 (tol = 0.002,
> component 1)
> 2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model is nearly unidentifiable: very large eigenvalue
>  - Rescale variables?
>
> Species does have a significant effect on my outcome (which is expected),
> so I've spilt the dataset by species to do pairwise comparisons to try and
> work out what's what. When I do so, I don't get the warnings but I do get,
> in some cases, enormous odds ratios (and one set of normal-looking ones
> too).
>
> Perhaps I'm over-reacting? But given the previous warnings I got with the
> initial (all four species) model, it gave me reason to pause and want to
> check.
>
> Can anyone advise about what I should do about the warnings, and whether I
> can trust those odds ratios, please?
>
> Thanks for any help you're able to give!
>
> Best wishes,
>
> Emma
>
> Code to demonstrate here:
>
> library(lme4)
> library(dplyr)
>
> data<-structure(list(Owner_ID = c(62L, 222L, 147L, 187L, 407L, 208L, 205L,
> 348L, 29L, 244L,
>                                 146L, 414L, 278L, 528L, 433L, 1039L, 930L,
> 902L, 1466L, 1977L,
>                                 704L, 2020L, 1423L, 782L, 308L, 291L, 61L,
> 1512L, 1164L, 1164L,
>                                 28L, 1549L, 1882L, 1181L, 569L, 135L,
> 609L, 1059L, 663L, 1465L,
>                                 1207L, 713L, 1420L, 1318L, 1357L, 1727L,
> 1415L, 1572L, 948L,
>                                 948L, 948L, 1897L, 1649L, 1162L, 974L,
> 1153L, 1802L, 1412L, 872L,
>                                 1708L, 1360L, 1616L, 1960L, 1960L, 1965L,
> 1794L, 636L, 339L,
>                                 1783L, 1783L, 1594L, 1558L, 1695L, 1822L,
> 1700L, 1667L, 869L,
>                                 534L, 1667L, 524L, 1684L, 782L, 782L,
> 664L, 1700L, 67L, 1416L,
>                                 1418L, 1352L, 1321L, 1569L, 598L, 821L,
> 604L, 2013L, 1707L, 785L,
>                                 1534L, 1299L, 565L, 1284L, 1244L, 1385L,
> 853L, 1014L, 1542L,
>                                 1771L, 1876L, 1063L, 1212L, 1000L, 1784L,
> 1439L, 547L, 745L,
>                                 717L, 702L, 1829L, 1503L, 862L, 667L,
> 1135L, 1541L, 1507L, 1507L,
>                                 680L, 1597L, 741L, 801L, 856L, 1295L,
> 1295L, 955L, 1850L, 1769L,
>                                 1393L, 1393L, 1291L, 1111L, 1405L, 728L,
> 1993L, 901L, 566L, 1020L,
>                                 1293L, 1320L, 1087L, 1176L, 1898L, 1940L,
> 1177L, 1296L, 1462L,
>                                 863L, 1918L, 1854L, 1508L, 747L, 1647L,
> 1671L, 1646L, 1949L,
>                                 1094L, 1184L, 1455L, 899L, 1627L, 804L,
> 1490L, 1768L, 555L, 735L,
>                                 2022L, 1317L, 1620L, 1605L, 919L, 597L,
> 1849L, 1336L, 1898L,
>                                 29L, 1392L, 1438L, 1198L, 1943L, 1139L,
> 1716L, 1986L, 962L, 1372L,
>                                 1673L, 1640L, 1640L, 1805L, 1163L, 1172L,
> 1165L, 1915L, 1915L,
>                                 1576L, 1526L, 1436L, 1998L, 1590L, 906L,
> 770L, 1398L, 656L, 1207L,
>                                 1120L, 805L, 805L, 1437L, 1682L, 800L,
> 1543L, 1238L, 2008L, 1069L,
>                                 1243L, 1326L, 1170L, 802L, 873L, 943L,
> 1330L, 1586L, 1833L, 1717L,
>                                 1654L, 1748L, 881L, 1006L, 1637L, 1701L,
> 1823L, 1613L, 1599L,
>                                 875L, 1813L, 1725L, 1005L, 1322L, 1839L,
> 352L, 1674L, 1358L,
>                                 1688L, 1724L, 1367L, 1827L, 1286L, 1150L,
> 1780L, 842L, 1118L,
>                                 884L, 1440L, 173L, 1744L, 578L, 1635L,
> 1568L, 861L, 1091L, 650L,
>                                 1354L, 1368L, 815L, 721L, 1787L, 1787L,
> 1678L, 1864L, 1786L,
>                                 1698L, 1984L, 1612L, 1159L, 1611L, 822L,
> 1042L, 1042L, 858L,
>                                 858L, 1731L, 1655L, 993L, 160L, 478L,
> 445L, 239L, 531L, 303L,
>                                 18L, 171L, 531L, 492L, 159L, 74L, 456L,
> 488L, 305L, 423L, 477L,
>                                 467L, 9L, 464L, 102L, 232L, 131L, 118L,
> 143L, 34L, 196L, 30L,
>                                 153L, 412L, 371L, 104L, 16L, 63L, 248L,
> 212L, 482L, 480L, 266L,
>                                 259L, 283L, 283L, 148L, 156L, 523L, 368L,
> 513L, 241L, 246L, 441L,
>                                 229L, 320L, 245L, 287L, 468L, 468L, 383L,
> 367L, 54L, 380L, 25L,
>                                 37L, 73L, 496L, 546L, 382L, 369L, 236L,
> 113L, 365L, 499L, 227L,
>                                 465L, 425L, 1819L, 657L, 635L, 1910L,
> 924L, 924L, 1485L, 1289L,
>                                 627L, 1076L, 627L, 1219L, 874L, 1289L,
> 1292L, 1754L, 972L, 1084L,
>                                 1240L, 777L, 609L, 626L, 1500L, 788L,
> 1470L, 1136L, 582L, 1891L,
>                                 645L, 1557L, 339L, 1904L, 2029L, 1107L,
> 665L, 631L, 631L, 1214L,
>                                 1214L, 738L, 973L, 888L, 1919L, 1772L,
> 1772L, 351L, 314L, 342L,
>                                 1077L, 134L, 273L, 495L, 166L, 514L, 247L,
> 203L, 125L, 2026L,
>                                 154L, 1570L, 1947L, 1055L, 1231L, 1318L,
> 1437L, 626L, 206L, 164L,
>                                 708L, 1711L, 922L, 1900L, 93L, 385L, 84L,
> 1173L, 294L, 470L,
>                                 23L, 1113L, 870L, 870L, 1029L, 2009L,
> 963L, 1746L, 988L, 1108L,
>                                 1749L, 1749L, 1749L, 1749L, 1828L, 1103L,
> 1844L, 970L, 769L,
>                                 1188L, 1712L, 1188L, 1776L, 1901L, 860L,
> 1777L, 1325L, 1608L,
>                                 1828L, 1608L, 1542L, 328L, 1942L, 1843L,
> 967L, 1990L, 1852L,
>                                 1056L, 1847L, 204L, 1338L, 846L, 729L,
> 1393L, 864L, 1396L, 1393L,
>                                 1777L),
>                Outcome = c(1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L,
> 0L, 0L,
>                                0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L,
>                                0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L,
>                                0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L,
> 0L, 1L, 0L, 0L, 0L,
>                                1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 1L, 0L, 0L, 0L,
>                                0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 1L, 0L,
>                                0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L,
> 0L, 0L, 0L, 0L, 1L,
>                                0L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 1L,
>                                0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L,
> 0L, 0L, 1L, 0L, 1L,
>                                1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L,
> 0L, 0L, 0L, 1L, 0L,
>                                1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L,
>                                1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 0L,
> 0L, 0L, 0L, 0L, 1L,
>                                0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 1L, 0L,
>                                0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 1L,
> 0L, 1L, 0L, 0L, 1L,
>                                1L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L,
>                                0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L,
> 1L, 0L, 0L, 1L, 1L,
>                                0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 1L, 0L, 1L, 0L,
>                                0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L,
> 0L, 0L, 0L, 0L, 1L,
>                                1L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L,
> 0L, 1L, 0L, 1L, 0L,
>                                0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L,
> 0L, 0L, 0L, 1L, 0L,
>                                0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 0L,
> 1L, 1L, 0L, 0L, 1L,
>                                0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 1L, 1L,
> 1L, 0L, 0L, 0L, 0L,
>                                0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L,
>                                1L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L,
>                                1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L,
> 0L, 1L, 0L, 1L, 0L,
>                                1L, 1L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 1L, 1L,
> 0L, 0L, 1L, 0L, 1L,
>                                1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 0L,
> 0L, 0L, 0L, 0L, 0L,
>                                0L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L,
> 1L, 1L, 0L, 1L, 1L,
>                                1L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
> 0L, 0L, 1L, 0L, 0L,
>                                0L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 1L,
> 0L, 0L, 0L, 0L, 1L,
>                                0L, 0L, 1L, 1L, 0L, 1L, 1L, 0L, 0L, 0L, 0L,
> 1L),
>                Species_ID = c("C", "C", "C", "C", "C", "C", "D", "C", "C",
> "C", "C",
>                               "C", "C", "C", "C", "D", "C", "A", "C", "C",
> "C", "C", "C", "C",
>                               "C", "C", "D", "C", "C", "C", "C", "C", "C",
> "D", "D", "C", "A",
>                               "A", "C", "C", "D", "C", "C", "D", "C", "C",
> "C", "C", "A", "A",
>                               "C", "C", "C", "C", "C", "C", "A", "C", "C",
> "C", "C", "C", "A",
>                               "D", "A", "C", "C", "A", "C", "C", "C", "C",
> "C", "C", "C", "C",
>                               "C", "C", "C", "C", "D", "C", "C", "A", "A",
> "C", "C", "C", "C",
>                               "C", "C", "C", "A", "A", "C", "C", "C", "C",
> "C", "C", "C", "D",
>                               "C", "C", "A", "C", "C", "C", "C", "C", "D",
> "C", "C", "C", "C",
>                               "D", "A", "C", "C", "C", "C", "C", "A", "C",
> "C", "C", "C", "C",
>                               "C", "C", "C", "C", "D", "C", "C", "A", "C",
> "A", "D", "C", "A",
>                               "C", "C", "D", "C", "C", "C", "D", "C", "C",
> "C", "C", "C", "C",
>                               "C", "C", "C", "C", "C", "C", "C", "C", "C",
> "C", "C", "C", "C",
>                               "C", "C", "C", "C", "C", "A", "C", "D", "C",
> "C", "C", "C", "C",
>                               "D", "C", "C", "C", "A", "C", "C", "C", "C",
> "D", "C", "C", "C",
>                               "A", "C", "A", "C", "C", "D", "A", "C", "A",
> "C", "A", "C", "A",
>                               "A", "D", "D", "D", "A", "D", "A", "A", "A",
> "A", "D", "D", "A",
>                               "C", "D", "D", "D", "C", "C", "C", "C", "C",
> "C", "A", "C", "C",
>                               "C", "C", "C", "C", "C", "C", "A", "C", "C",
> "C", "C", "C", "A",
>                               "C", "C", "C", "A", "C", "C", "C", "C", "D",
> "A", "C", "C", "C",
>                               "C", "C", "C", "C", "C", "C", "C", "C", "C",
> "C", "C", "D", "C",
>                               "C", "C", "C", "C", "A", "A", "A", "D", "A",
> "C", "C", "A", "A",
>                               "A", "A", "A", "C", "A", "A", "C", "C", "C",
> "C", "C", "D", "D",
>                               "D", "C", "C", "D", "C", "C", "D", "D", "D",
> "D", "D", "C", "D",
>                               "D", "C", "C", "C", "C", "D", "C", "C", "D",
> "A", "D", "D", "C",
>                               "C", "D", "A", "C", "A", "D", "D", "C", "C",
> "C", "D", "C", "C",
>                               "D", "C", "C", "D", "C", "C", "C", "C", "A",
> "A", "C", "C", "C",
>                               "C", "C", "D", "D", "C", "C", "C", "C", "C",
> "A", "D", "C", "D",
>                               "A", "A", "C", "B", "B", "B", "B", "B", "B",
> "B", "B", "B", "B",
>                               "B", "B", "B", "B", "B", "B", "B", "B", "B",
> "B", "B", "B", "B",
>                               "B", "B", "B", "B", "B", "B", "B", "B", "B",
> "B", "B", "B", "B",
>                               "B", "B", "B", "B", "B", "B", "B", "B", "B",
> "B", "B", "B", "B",
>                               "B", "B", "B", "B", "B", "B", "B", "B", "B",
> "B", "B", "B", "B",
>                               "B", "B", "B", "B", "B", "B", "B", "B", "B",
> "B", "B", "B", "B",
>                               "B", "B", "B", "B", "B", "B", "B", "B", "B",
> "B", "B", "B", "B",
>                               "B", "B", "B", "B", "B", "B", "B", "B", "B",
> "B", "B", "B", "B",
>                               "B", "B", "B", "B", "B", "B", "B", "B", "B",
> "B", "B", "B", "B",
>                               "B", "B", "B", "B", "B", "B", "B", "B", "B",
> "B", "B")))
> data<-as.data.frame(data)
> attach(data)
>
> m1<-glmer(Outcome~ Species_ID+(1 |Owner_ID),
>           data=data, family= (binomial(link="logit")),na.action = na.omit,
>           glmerControl(optimizer="bobyqa", optCtrl = list(maxfun =
> 1000000)))
> #get convergence error (tol >0.002)
>
> #Ben Bolker advised to first do this as a check because of the warning:
> m1_fit_all<-allFit(m1)
>
> ss<-summary(m1_fit_all)
> ss$which.OK
> #all seem OK
>
> m1b<-glmer(Outcome~ 1+(1 |Owner_ID),
>            data=data, family= (binomial(link="logit")),na.action = na.omit,
>            glmerControl(optimizer="bobyqa", optCtrl = list(maxfun =
> 1000000)))
>
> anova(m1,m1b)
> #so, an effect of species (as expected - in line with our previous
> research)
> summary(m1)
> #z values look a bit suspect (to me)
>
> #splitting dataset by species (A v B first)
> # to suss what's going on. Will give you an example of what look like mad
> odds ratios
> a_b_data<-data %>% filter(Species_ID%in% c("A", "B"))
> detach(data)
> attach(a_b_data)
>
> #and then re-run
> m1c<-glmer(Outcome~ Species_ID+(1 |Owner_ID),
>            data=a_b_data, family= (binomial(link="logit")),na.action =
> na.omit,
>            glmerControl(optimizer="bobyqa", optCtrl = list(maxfun =
> 1000000)))
> #no error this time
>
> m1e<-glmer(Outcome~ 1+(1 |Owner_ID),
>            data=a_b_data, family= (binomial(link="logit")),na.action =
> na.omit,
>            glmerControl(optimizer="bobyqa", optCtrl = list(maxfun =
> 1000000)))
>
> anova(m1c,m1e)
> #species has an effect
> summary(m1c)
> #z value looks less crazy, but want to take a look at the odds ratios
> se <- sqrt(diag(vcov(m1c)))
> (tab <- cbind(Est = fixef(m1c), LL = fixef(m1c) - 1.96 * se, UL =
> fixef(m1c) + 1.96 *
>                 se))
>
> #this for odds ratios. Upper limit looks especially mad (24,231,520!!)
> exp(tab)
>
> #I'll give an example of a normal-looking one too
>
> #splitting dataset by another species pair (C v D)
>
> c_d_data<-data %>% filter(Species_ID%in% c("C", "D"))
> detach(a_b_data)
> attach(c_d_data)
>
> #and then re-run
> m1r<-glmer(Outcome~ Species_ID+(1 |Owner_ID),
>            data=c_d_data, family= (binomial(link="logit")),na.action =
> na.omit,
>            glmerControl(optimizer="bobyqa", optCtrl = list(maxfun =
> 1000000)))
> #no error
> m1t<-glmer(Outcome~ 1+(1 |Owner_ID),
>            data=c_d_data, family= (binomial(link="logit")),na.action =
> na.omit,
>            glmerControl(optimizer="bobyqa", optCtrl = list(maxfun =
> 1000000)))
>
> anova(m1r,m1t)
> #effect of species here too
> #get the odds ratios
> se <- sqrt(diag(vcov(m1r)))
> (tab <- cbind(Est = fixef(m1r), LL = fixef(m1r) - 1.96 * se, UL =
> fixef(m1r) + 1.96 *
>                 se))
>
> #this for (less alarming) odds ratios.
> exp(tab)
>
> Dr Emma Mellor
> Research Associate
>
> University of Bristol
> Bristol Veterinary School
> Langford House
> Langford
> BS40 5DU
>
> My working days are generally Monday, Tuesday and Friday. My work schedule
> may not be the same as yours - please do not feel obliged to respond
> outside of your own working hours
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From jov23 @end|ng |rom b@th@@c@uk  Mon Jun 27 04:05:12 2022
From: jov23 @end|ng |rom b@th@@c@uk (Jose Valdebenito Chavez)
Date: Mon, 27 Jun 2022 02:05:12 +0000
Subject: [R-sig-ME] poor convergence due to problematic data? multinomial
 MCMCglmm model
Message-ID: <CWXP265MB342956B0996D662ED46FA63A8BB99@CWXP265MB3429.GBRP265.PROD.OUTLOOK.COM>

Hi,

I am having problems with MCMCglmm models trying to describe drug resistance in birds.

Response variable: present/absence of drug resistance. That is cbind(all.present, all.absent) which will be e.g.  Anas crecca = 4, 1.
Explanatory variable: mating system, categorical with two levels (monogamy, polygyny)
Number of species (and number of rows): 31
Actual number of individuals birds examined: 711


The Model:

prior=list(R = list(V = diag(1), nu = 0.02),
               G = list(G1 = list(V = diag(1), nu = 2, alpha.mu = c(0), alpha.V = diag(1))))

mp3 <- MCMCglmm(cbind(all.present, all.absent) ~ msys,
                random=~species,
                ginverse=list(species=inv.phylo$Ainv),
                prior=prior,
                family = "multinomial2",
                data = data,
                nitt=5002000,burnin=2000,thin=5000)

The Outcome:

Iterations = 2001:4997001
Thinning interval  = 5000
Sample size  = 1000
DIC: 172.8442

G-structure:  ~species
              post.mean     l-95% CI     u-95% CI   eff.samp
species       2.065.      2.773e-06       8.973       1000

R-structure:  ~units
              post.mean.  l-95% CI     u-95% CI   eff.samp
units         4.189.        0.004191    12.83          1000

Location effects: cbind(all.present, all.absent) ~ msys
                      post.mean l-95% CI u-95% CI       eff.samp  pMCMC
(Intercept)            4.166      2.174        6.947        939.42       0.002 **
msyspolygyny   434.100   63.507    702.711           26.66    <0.001 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1


By the outcome and trace plots the model seems fine except for msyspolygyny, and I haven?t been able to correct the issue so far.
Increasing nitt to 5002000 improved the autocorrelation a bit but still far from good.

diag(autocorr(mp3$Sol)[2, , ])
 (Intercept)   msyspolygyny
0.01511989    0.94800971

Convergence for polygyny is, not surprisingly, bad.

gelman.diag(mcmc.list(mp3$Sol, mp3b$Sol))
Potential scale reduction factors:

                        Point est. Upper C.I.
(Intercept)           0.999       1.00
msyspolygyny     1.357       2.11

Multivariate psrf
1.23

Then I checked and I realised that the problem is likely to originate from the data itself that has just 3 species with a polygynous mating system, out of 31.

Is there a way to improve the autocorrelation/convergence? Run it for even longer? Maybe by specifying a prior able to handle it?

Thanks,
Jose



-------------------------

Jos? O. Valdebenito


	[[alternative HTML version deleted]]


From w@||dm@w@@@10 @end|ng |rom gm@||@com  Mon Jun 27 21:43:16 2022
From: w@||dm@w@@@10 @end|ng |rom gm@||@com (Walid Mawass)
Date: Mon, 27 Jun 2022 15:43:16 -0400
Subject: [R-sig-ME] 
 poor convergence due to problematic data? multinomial MCMCglmm model
In-Reply-To: <CWXP265MB342956B0996D662ED46FA63A8BB99@CWXP265MB3429.GBRP265.PROD.OUTLOOK.COM>
References: <CWXP265MB342956B0996D662ED46FA63A8BB99@CWXP265MB3429.GBRP265.PROD.OUTLOOK.COM>
Message-ID: <CAJtCY7WJ73CTC7SrHqCQ1dWMYrb0YovVPjMqie1HhNMtm5V+KQ@mail.gmail.com>

One thing you can do is increase your burn-in time to at least 5*10^5.
Right now you have it at 2000 for a 5000000 iterations which is quite
short. And of course like you mentioned the issue with polygyny is the lack
of representation in the datatset

Walid Mawass

On Mon, Jun 27, 2022, 2:55 PM Jose Valdebenito Chavez via
R-sig-mixed-models <r-sig-mixed-models at r-project.org> wrote:

> Hi,
>
> I am having problems with MCMCglmm models trying to describe drug
> resistance in birds.
>
> Response variable: present/absence of drug resistance. That is
> cbind(all.present, all.absent) which will be e.g.  Anas crecca = 4, 1.
> Explanatory variable: mating system, categorical with two levels
> (monogamy, polygyny)
> Number of species (and number of rows): 31
> Actual number of individuals birds examined: 711
>
>
> The Model:
>
> prior=list(R = list(V = diag(1), nu = 0.02),
>                G = list(G1 = list(V = diag(1), nu = 2, alpha.mu = c(0),
> alpha.V = diag(1))))
>
> mp3 <- MCMCglmm(cbind(all.present, all.absent) ~ msys,
>                 random=~species,
>                 ginverse=list(species=inv.phylo$Ainv),
>                 prior=prior,
>                 family = "multinomial2",
>                 data = data,
>                 nitt=5002000,burnin=2000,thin=5000)
>
> The Outcome:
>
> Iterations = 2001:4997001
> Thinning interval  = 5000
> Sample size  = 1000
> DIC: 172.8442
>
> G-structure:  ~species
>               post.mean     l-95% CI     u-95% CI   eff.samp
> species       2.065.      2.773e-06       8.973       1000
>
> R-structure:  ~units
>               post.mean.  l-95% CI     u-95% CI   eff.samp
> units         4.189.        0.004191    12.83          1000
>
> Location effects: cbind(all.present, all.absent) ~ msys
>                       post.mean l-95% CI u-95% CI       eff.samp  pMCMC
> (Intercept)            4.166      2.174        6.947        939.42
>  0.002 **
> msyspolygyny   434.100   63.507    702.711           26.66    <0.001 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
>
> By the outcome and trace plots the model seems fine except for
> msyspolygyny, and I haven?t been able to correct the issue so far.
> Increasing nitt to 5002000 improved the autocorrelation a bit but still
> far from good.
>
> diag(autocorr(mp3$Sol)[2, , ])
>  (Intercept)   msyspolygyny
> 0.01511989    0.94800971
>
> Convergence for polygyny is, not surprisingly, bad.
>
> gelman.diag(mcmc.list(mp3$Sol, mp3b$Sol))
> Potential scale reduction factors:
>
>                         Point est. Upper C.I.
> (Intercept)           0.999       1.00
> msyspolygyny     1.357       2.11
>
> Multivariate psrf
> 1.23
>
> Then I checked and I realised that the problem is likely to originate from
> the data itself that has just 3 species with a polygynous mating system,
> out of 31.
>
> Is there a way to improve the autocorrelation/convergence? Run it for even
> longer? Maybe by specifying a prior able to handle it?
>
> Thanks,
> Jose
>
>
>
> -------------------------
>
> Jos? O. Valdebenito
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From emm@@me||or @end|ng |rom br|@to|@@c@uk  Tue Jun 28 14:24:40 2022
From: emm@@me||or @end|ng |rom br|@to|@@c@uk (Emma Mellor)
Date: Tue, 28 Jun 2022 12:24:40 +0000
Subject: [R-sig-ME] 
 question about convergence warning and some odd-looking
 odds ratios in a glmer model
In-Reply-To: <CAJuCY5zF5cUbGZ-6Tks3FreSVG_S5YAww1-+BGOWWNyXxLMCyQ@mail.gmail.com>
References: <DB9PR06MB77709385850D4167C42136B0A4B59@DB9PR06MB7770.eurprd06.prod.outlook.com>
 <CAJuCY5zF5cUbGZ-6Tks3FreSVG_S5YAww1-+BGOWWNyXxLMCyQ@mail.gmail.com>
Message-ID: <DB9PR06MB7770433F35CB0A86888E00D3A4B89@DB9PR06MB7770.eurprd06.prod.outlook.com>

Hi Thierry,

Thanks for your email, and apologies for my slow reply ? I wanted to have a play about with my data a bit more before replying.

> Running m1 without specifying the optimiser made the model run.

OK, thank you. Do you know why that worked? I?d like to try and understand why that might be contributing to the problem, please.

In subsequent models with extra predictors (describing the home environment and demographics), I?ve had instances in which what I *think* is the default optimiser (Nelder_Mead) failed, yet the bobyqa ran OK. Does it sound like it?s therefore better to use bobyqa regardless?

> See if you can cluster the owners into some sensible groups or use some covariates describing the home situation of the bird.

And thanks for this too. We don?t have additional data on the owners to cluster them, I?m afraid. But we do have variables describing the home environment and demographics ? I?m taking a look at these now/

Thanks again for your advice! Really appreciate it.

Best wishes,

Emma

From: Thierry Onkelinx <thierry.onkelinx at inbo.be>
Sent: 24 June 2022 14:53
To: Emma Mellor <emma.mellor at bristol.ac.uk>
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] question about convergence warning and some odd-looking odds ratios in a glmer model

Dear Emma,

Running m1 without specifying the optimiser made the model run. However it returns extreme estimates for the fixed effects and the random effect variance. The problem is in the fact that most owners have only one bird. This makes the random effect nearly behave as an observation level random effect.

See if you can cluster the owners into some sensible groups or use some covariates describing the home situation of the bird.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be<http://www.inbo.be>
///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

[https://inbo-website-prd-532750756126.s3-eu-west-1.amazonaws.com/inbologoleeuw_nl.png]<https://www.inbo.be/>


Op do 23 jun. 2022 om 19:10 schreef Emma Mellor <emma.mellor at bristol.ac.uk<mailto:emma.mellor at bristol.ac.uk>>:
Hi all,

I'm running glmer models in lme4 on: R version 4.1.2,
Platform: x86_64-w64-mingw32/x64 (64-bit),
Running under: Windows 10 x64 (build 19044)

My outcome (presence/absence of abnormal behaviour in pet birds) is binary, so I used family = binomial.  Random effects are 'Owner_ID' - in reality, very few birds share a household, and the vast majority are single birds. I'm getting warning messages about convergence when 'Species_ID' is included the model as a predictor (I do not get error messages with other predictors - it's just this one). I've tabulated the data and I don't see any obvious reason for the error, such as missing/very few cases per levels within variables, so I'm at a bit of a loss as what to do about it. Copy of warning message next, and I'll provide a copy of my code below:

Warning messages:
1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.0562293 (tol = 0.002, component 1)
2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?

Species does have a significant effect on my outcome (which is expected), so I've spilt the dataset by species to do pairwise comparisons to try and work out what's what. When I do so, I don't get the warnings but I do get, in some cases, enormous odds ratios (and one set of normal-looking ones too).

Perhaps I'm over-reacting? But given the previous warnings I got with the initial (all four species) model, it gave me reason to pause and want to check.

Can anyone advise about what I should do about the warnings, and whether I can trust those odds ratios, please?

Thanks for any help you're able to give!

Best wishes,

Emma

Code to demonstrate here:

library(lme4)
library(dplyr)

data<-structure(list(Owner_ID = c(62L, 222L, 147L, 187L, 407L, 208L, 205L, 348L, 29L, 244L,
                                146L, 414L, 278L, 528L, 433L, 1039L, 930L, 902L, 1466L, 1977L,
                                704L, 2020L, 1423L, 782L, 308L, 291L, 61L, 1512L, 1164L, 1164L,
                                28L, 1549L, 1882L, 1181L, 569L, 135L, 609L, 1059L, 663L, 1465L,
                                1207L, 713L, 1420L, 1318L, 1357L, 1727L, 1415L, 1572L, 948L,
                                948L, 948L, 1897L, 1649L, 1162L, 974L, 1153L, 1802L, 1412L, 872L,
                                1708L, 1360L, 1616L, 1960L, 1960L, 1965L, 1794L, 636L, 339L,
                                1783L, 1783L, 1594L, 1558L, 1695L, 1822L, 1700L, 1667L, 869L,
                                534L, 1667L, 524L, 1684L, 782L, 782L, 664L, 1700L, 67L, 1416L,
                                1418L, 1352L, 1321L, 1569L, 598L, 821L, 604L, 2013L, 1707L, 785L,
                                1534L, 1299L, 565L, 1284L, 1244L, 1385L, 853L, 1014L, 1542L,
                                1771L, 1876L, 1063L, 1212L, 1000L, 1784L, 1439L, 547L, 745L,
                                717L, 702L, 1829L, 1503L, 862L, 667L, 1135L, 1541L, 1507L, 1507L,
                                680L, 1597L, 741L, 801L, 856L, 1295L, 1295L, 955L, 1850L, 1769L,
                                1393L, 1393L, 1291L, 1111L, 1405L, 728L, 1993L, 901L, 566L, 1020L,
                                1293L, 1320L, 1087L, 1176L, 1898L, 1940L, 1177L, 1296L, 1462L,
                                863L, 1918L, 1854L, 1508L, 747L, 1647L, 1671L, 1646L, 1949L,
                                1094L, 1184L, 1455L, 899L, 1627L, 804L, 1490L, 1768L, 555L, 735L,
                                2022L, 1317L, 1620L, 1605L, 919L, 597L, 1849L, 1336L, 1898L,
                                29L, 1392L, 1438L, 1198L, 1943L, 1139L, 1716L, 1986L, 962L, 1372L,
                                1673L, 1640L, 1640L, 1805L, 1163L, 1172L, 1165L, 1915L, 1915L,
                                1576L, 1526L, 1436L, 1998L, 1590L, 906L, 770L, 1398L, 656L, 1207L,
                                1120L, 805L, 805L, 1437L, 1682L, 800L, 1543L, 1238L, 2008L, 1069L,
                                1243L, 1326L, 1170L, 802L, 873L, 943L, 1330L, 1586L, 1833L, 1717L,
                                1654L, 1748L, 881L, 1006L, 1637L, 1701L, 1823L, 1613L, 1599L,
                                875L, 1813L, 1725L, 1005L, 1322L, 1839L, 352L, 1674L, 1358L,
                                1688L, 1724L, 1367L, 1827L, 1286L, 1150L, 1780L, 842L, 1118L,
                                884L, 1440L, 173L, 1744L, 578L, 1635L, 1568L, 861L, 1091L, 650L,
                                1354L, 1368L, 815L, 721L, 1787L, 1787L, 1678L, 1864L, 1786L,
                                1698L, 1984L, 1612L, 1159L, 1611L, 822L, 1042L, 1042L, 858L,
                                858L, 1731L, 1655L, 993L, 160L, 478L, 445L, 239L, 531L, 303L,
                                18L, 171L, 531L, 492L, 159L, 74L, 456L, 488L, 305L, 423L, 477L,
                                467L, 9L, 464L, 102L, 232L, 131L, 118L, 143L, 34L, 196L, 30L,
                                153L, 412L, 371L, 104L, 16L, 63L, 248L, 212L, 482L, 480L, 266L,
                                259L, 283L, 283L, 148L, 156L, 523L, 368L, 513L, 241L, 246L, 441L,
                                229L, 320L, 245L, 287L, 468L, 468L, 383L, 367L, 54L, 380L, 25L,
                                37L, 73L, 496L, 546L, 382L, 369L, 236L, 113L, 365L, 499L, 227L,
                                465L, 425L, 1819L, 657L, 635L, 1910L, 924L, 924L, 1485L, 1289L,
                                627L, 1076L, 627L, 1219L, 874L, 1289L, 1292L, 1754L, 972L, 1084L,
                                1240L, 777L, 609L, 626L, 1500L, 788L, 1470L, 1136L, 582L, 1891L,
                                645L, 1557L, 339L, 1904L, 2029L, 1107L, 665L, 631L, 631L, 1214L,
                                1214L, 738L, 973L, 888L, 1919L, 1772L, 1772L, 351L, 314L, 342L,
                                1077L, 134L, 273L, 495L, 166L, 514L, 247L, 203L, 125L, 2026L,
                                154L, 1570L, 1947L, 1055L, 1231L, 1318L, 1437L, 626L, 206L, 164L,
                                708L, 1711L, 922L, 1900L, 93L, 385L, 84L, 1173L, 294L, 470L,
                                23L, 1113L, 870L, 870L, 1029L, 2009L, 963L, 1746L, 988L, 1108L,
                                1749L, 1749L, 1749L, 1749L, 1828L, 1103L, 1844L, 970L, 769L,
                                1188L, 1712L, 1188L, 1776L, 1901L, 860L, 1777L, 1325L, 1608L,
                                1828L, 1608L, 1542L, 328L, 1942L, 1843L, 967L, 1990L, 1852L,
                                1056L, 1847L, 204L, 1338L, 846L, 729L, 1393L, 864L, 1396L, 1393L,
                                1777L),
               Outcome = c(1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L,
                               0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
                               0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
                               0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L,
                               1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L,
                               0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L,
                               0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 1L,
                               0L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L,
                               0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L,
                               1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L,
                               1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
                               1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L,
                               0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L,
                               0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L,
                               1L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
                               0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 1L,
                               0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L,
                               0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L,
                               1L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 0L,
                               0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L,
                               0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 1L,
                               0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L,
                               0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
                               1L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
                               1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L,
                               1L, 1L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 1L,
                               1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L,
                               0L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 1L, 1L,
                               1L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L,
                               0L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L,
                               0L, 0L, 1L, 1L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 1L),
               Species_ID = c("C", "C", "C", "C", "C", "C", "D", "C", "C", "C", "C",
                              "C", "C", "C", "C", "D", "C", "A", "C", "C", "C", "C", "C", "C",
                              "C", "C", "D", "C", "C", "C", "C", "C", "C", "D", "D", "C", "A",
                              "A", "C", "C", "D", "C", "C", "D", "C", "C", "C", "C", "A", "A",
                              "C", "C", "C", "C", "C", "C", "A", "C", "C", "C", "C", "C", "A",
                              "D", "A", "C", "C", "A", "C", "C", "C", "C", "C", "C", "C", "C",
                              "C", "C", "C", "C", "D", "C", "C", "A", "A", "C", "C", "C", "C",
                              "C", "C", "C", "A", "A", "C", "C", "C", "C", "C", "C", "C", "D",
                              "C", "C", "A", "C", "C", "C", "C", "C", "D", "C", "C", "C", "C",
                              "D", "A", "C", "C", "C", "C", "C", "A", "C", "C", "C", "C", "C",
                              "C", "C", "C", "C", "D", "C", "C", "A", "C", "A", "D", "C", "A",
                              "C", "C", "D", "C", "C", "C", "D", "C", "C", "C", "C", "C", "C",
                              "C", "C", "C", "C", "C", "C", "C", "C", "C", "C", "C", "C", "C",
                              "C", "C", "C", "C", "C", "A", "C", "D", "C", "C", "C", "C", "C",
                              "D", "C", "C", "C", "A", "C", "C", "C", "C", "D", "C", "C", "C",
                              "A", "C", "A", "C", "C", "D", "A", "C", "A", "C", "A", "C", "A",
                              "A", "D", "D", "D", "A", "D", "A", "A", "A", "A", "D", "D", "A",
                              "C", "D", "D", "D", "C", "C", "C", "C", "C", "C", "A", "C", "C",
                              "C", "C", "C", "C", "C", "C", "A", "C", "C", "C", "C", "C", "A",
                              "C", "C", "C", "A", "C", "C", "C", "C", "D", "A", "C", "C", "C",
                              "C", "C", "C", "C", "C", "C", "C", "C", "C", "C", "C", "D", "C",
                              "C", "C", "C", "C", "A", "A", "A", "D", "A", "C", "C", "A", "A",
                              "A", "A", "A", "C", "A", "A", "C", "C", "C", "C", "C", "D", "D",
                              "D", "C", "C", "D", "C", "C", "D", "D", "D", "D", "D", "C", "D",
                              "D", "C", "C", "C", "C", "D", "C", "C", "D", "A", "D", "D", "C",
                              "C", "D", "A", "C", "A", "D", "D", "C", "C", "C", "D", "C", "C",
                              "D", "C", "C", "D", "C", "C", "C", "C", "A", "A", "C", "C", "C",
                              "C", "C", "D", "D", "C", "C", "C", "C", "C", "A", "D", "C", "D",
                              "A", "A", "C", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B",
                              "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B",
                              "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B",
                              "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B",
                              "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B",
                              "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B",
                              "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B",
                              "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B",
                              "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B",
                              "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B")))
data<-as.data.frame(data)
attach(data)

m1<-glmer(Outcome~ Species_ID+(1 |Owner_ID),
          data=data, family= (binomial(link="logit")),na.action = na.omit,
          glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 1000000)))
#get convergence error (tol >0.002)

#Ben Bolker advised to first do this as a check because of the warning:
m1_fit_all<-allFit(m1)

ss<-summary(m1_fit_all)
ss$which.OK
#all seem OK

m1b<-glmer(Outcome~ 1+(1 |Owner_ID),
           data=data, family= (binomial(link="logit")),na.action = na.omit,
           glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 1000000)))

anova(m1,m1b)
#so, an effect of species (as expected - in line with our previous research)
summary(m1)
#z values look a bit suspect (to me)

#splitting dataset by species (A v B first)
# to suss what's going on. Will give you an example of what look like mad odds ratios
a_b_data<-data %>% filter(Species_ID%in% c("A", "B"))
detach(data)
attach(a_b_data)

#and then re-run
m1c<-glmer(Outcome~ Species_ID+(1 |Owner_ID),
           data=a_b_data, family= (binomial(link="logit")),na.action = na.omit,
           glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 1000000)))
#no error this time

m1e<-glmer(Outcome~ 1+(1 |Owner_ID),
           data=a_b_data, family= (binomial(link="logit")),na.action = na.omit,
           glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 1000000)))

anova(m1c,m1e)
#species has an effect
summary(m1c)
#z value looks less crazy, but want to take a look at the odds ratios
se <- sqrt(diag(vcov(m1c)))
(tab <- cbind(Est = fixef(m1c), LL = fixef(m1c) - 1.96 * se, UL = fixef(m1c) + 1.96 *
                se))

#this for odds ratios. Upper limit looks especially mad (24,231,520!!)
exp(tab)

#I'll give an example of a normal-looking one too

#splitting dataset by another species pair (C v D)

c_d_data<-data %>% filter(Species_ID%in% c("C", "D"))
detach(a_b_data)
attach(c_d_data)

#and then re-run
m1r<-glmer(Outcome~ Species_ID+(1 |Owner_ID),
           data=c_d_data, family= (binomial(link="logit")),na.action = na.omit,
           glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 1000000)))
#no error
m1t<-glmer(Outcome~ 1+(1 |Owner_ID),
           data=c_d_data, family= (binomial(link="logit")),na.action = na.omit,
           glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 1000000)))

anova(m1r,m1t)
#effect of species here too
#get the odds ratios
se <- sqrt(diag(vcov(m1r)))
(tab <- cbind(Est = fixef(m1r), LL = fixef(m1r) - 1.96 * se, UL = fixef(m1r) + 1.96 *
                se))

#this for (less alarming) odds ratios.
exp(tab)

Dr Emma Mellor
Research Associate

University of Bristol
Bristol Veterinary School
Langford House
Langford
BS40 5DU

My working days are generally Monday, Tuesday and Friday. My work schedule may not be the same as yours - please do not feel obliged to respond outside of your own working hours


        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


