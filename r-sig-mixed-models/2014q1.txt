From russell-lenth at uiowa.edu  Wed Jan  1 15:21:01 2014
From: russell-lenth at uiowa.edu (Lenth, Russell V)
Date: Wed, 1 Jan 2014 14:21:01 +0000
Subject: [R-sig-ME] Sample size calculation
In-Reply-To: <mailman.3.1388574001.4437.r-sig-mixed-models@r-project.org>
References: <mailman.3.1388574001.4437.r-sig-mixed-models@r-project.org>
Message-ID: <326EF40C-4800-4DBC-9FF8-9A7367CC97D0@uiowa.edu>

Maybe I can help. But what's the situation? Do you already have pilot data analyzed with one of those packages? If so, you can use that analysis to estimate the variance component nets needed for the sample-size calculation. If not, your first step really is to do such a pilot study. 

The next step is to figure out the power function for the test(s) of interest. That depends on the intended model for the data and the expected mean squares (at least under the usual uniformly wonderful normality conditions). You also need to establish what size difference or other effect is of interest to detect with a stated power and significance level. 

There is software that can help with the power calculations, but let's get more details first on those other steps.

Russ Lenth

Sent from my iPad

> Hi list
> Can someone illustrate me how is  possible  to calculate  sample size for linear mixed model (not repeated measure) build with  lme4 or nlme package
> Thank in advance
> happy Holidays 
> Bonitta Gianluca 


From bbolker at gmail.com  Wed Jan  1 17:15:56 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 01 Jan 2014 11:15:56 -0500
Subject: [R-sig-ME] Sample size calculation
In-Reply-To: <326EF40C-4800-4DBC-9FF8-9A7367CC97D0@uiowa.edu>
References: <mailman.3.1388574001.4437.r-sig-mixed-models@r-project.org>
	<326EF40C-4800-4DBC-9FF8-9A7367CC97D0@uiowa.edu>
Message-ID: <52C43F3C.3060201@gmail.com>

On 14-01-01 09:21 AM, Lenth, Russell V wrote:
> Maybe I can help. But what's the situation? Do you already have pilot
> data analyzed with one of those packages? If so, you can use that
> analysis to estimate the variance component nets needed for the
> sample-size calculation. If not, your first step really is to do such
> a pilot study.
> 
> The next step is to figure out the power function for the test(s) of
> interest. That depends on the intended model for the data and the
> expected mean squares (at least under the usual uniformly wonderful
> normality conditions). You also need to establish what size
> difference or other effect is of interest to detect with a stated
> power and significance level.
> 
> There is software that can help with the power calculations, but
> let's get more details first on those other steps.
> 
> Russ Lenth
> 

  Seems like very good advice to me (although I'm not sure what
"variance component nets" means -- is that a typo?)

  If you don't have pilot data but you know enough about your system to
have some idea what realistic variance components and effect sizes
should be (and if you don't, you probably don't know enough to design a
sensible experiment in any case!)

  There are several packages dedicated to power analysis for mixed
models (pamm, longpower, nlmeU, odprism), mostly (all?) by simulation.
The simulation capabilities in the development version of lme4 make it
easier to roll your own power analysis.
> 
>> Hi list Can someone illustrate me how is  possible  to calculate
>> sample size for linear mixed model (not repeated measure) build
>> with  lme4 or nlme package Thank in advance happy Holidays Bonitta
>> Gianluca
> 
> _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From russell-lenth at uiowa.edu  Wed Jan  1 18:05:37 2014
From: russell-lenth at uiowa.edu (Lenth, Russell V)
Date: Wed, 1 Jan 2014 17:05:37 +0000
Subject: [R-sig-ME] Sample size calculation
In-Reply-To: <52C43F3C.3060201@gmail.com>
References: <mailman.3.1388574001.4437.r-sig-mixed-models@r-project.org>
	<326EF40C-4800-4DBC-9FF8-9A7367CC97D0@uiowa.edu>,
	<52C43F3C.3060201@gmail.com>
Message-ID: <683277FD-EE35-4878-A889-90567EC819D8@uiowa.edu>

Yes, that was a typo. Just delete "nets" and you get what I meant. Sorry about that.

Russ

Sent from my iPad

> On Jan 1, 2014, at 10:16 AM, "Ben Bolker" <bbolker at gmail.com> wrote:
> 
>> On 14-01-01 09:21 AM, Lenth, Russell V wrote:
>> Maybe I can help. But what's the situation? Do you already have pilot
>> data analyzed with one of those packages? If so, you can use that
>> analysis to estimate the variance component nets needed for the
>> sample-size calculation. If not, your first step really is to do such
>> a pilot study.
>> 
>> The next step is to figure out the power function for the test(s) of
>> interest. That depends on the intended model for the data and the
>> expected mean squares (at least under the usual uniformly wonderful
>> normality conditions). You also need to establish what size
>> difference or other effect is of interest to detect with a stated
>> power and significance level.
>> 
>> There is software that can help with the power calculations, but
>> let's get more details first on those other steps.
>> 
>> Russ Lenth
> 
>  Seems like very good advice to me (although I'm not sure what
> "variance component nets" means -- is that a typo?)
> 
>  If you don't have pilot data but you know enough about your system to
> have some idea what realistic variance components and effect sizes
> should be (and if you don't, you probably don't know enough to design a
> sensible experiment in any case!)
> 
>  There are several packages dedicated to power analysis for mixed
> models (pamm, longpower, nlmeU, odprism), mostly (all?) by simulation.
> The simulation capabilities in the development version of lme4 make it
> easier to roll your own power analysis.
>> 
>>> Hi list Can someone illustrate me how is  possible  to calculate
>>> sample size for linear mixed model (not repeated measure) build
>>> with  lme4 or nlme package Thank in advance happy Holidays Bonitta
>>> Gianluca
>> 
>> _______________________________________________ 
>> R-sig-mixed-models at r-project.org mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 


From sheryn.olson at maine.edu  Thu Jan  2 02:31:09 2014
From: sheryn.olson at maine.edu (Sheryn Olson)
Date: Wed, 1 Jan 2014 20:31:09 -0500
Subject: [R-sig-ME] Fit negative binomial glmm credible intervals from the
	predict function?
Message-ID: <CAATUFec60Y0ESJKdVx2+aD7RfqnJYriqi2bbEyxsc_+frSD+cw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140101/0bd869ca/attachment.pl>

From bbonit at tin.it  Thu Jan  2 13:42:14 2014
From: bbonit at tin.it (Bonitta gianluca)
Date: Thu, 2 Jan 2014 12:42:14 +0000 (UTC)
Subject: [R-sig-ME] Sample size calculation
References: <mailman.3.1388574001.4437.r-sig-mixed-models@r-project.org>
	<326EF40C-4800-4DBC-9FF8-9A7367CC97D0@uiowa.edu>,
	<52C43F3C.3060201@gmail.com>
	<683277FD-EE35-4878-A889-90567EC819D8@uiowa.edu>
Message-ID: <loom.20140102T133356-952@post.gmane.org>

Thank You for soon reply fisrt of all

i have the dataset and r script maby i can send and you can help to 
prototipe the script in r ...for example



for professor Ben :

i had seen long power pack but i think this need repeated measure for the 
use i have not repeated measure ...do You think is possible to suite long 
power pack for my case?



I have also try simulative way for post hoc power calculation.... but this 
yeld pap power analysis paradox... when p-value is "big" power is low , 
when p-value is "small" ower is hight  so i prefer to determinate a priori 
power ....

thank you so much


From r.turner at auckland.ac.nz  Thu Jan  2 23:32:58 2014
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Fri, 03 Jan 2014 11:32:58 +1300
Subject: [R-sig-ME] Nested fixed effects --- redux.
Message-ID: <52C5E91A.4050106@auckland.ac.nz>


This question is (a) lengthy, and (b) a bit vague.  Just delete this 
posting if you don't feel like confronting these defects.

Last March there was some discussion on this list in respect of
nested fixed effects in linear models.  I expressed the opinion that
nested fixed effects don't really make much sense, but others disagreed,
and pointed out that one might be interested in testing hypotheses
about such effects.

Recently I have undertaken a consulting project in which this issue has
arisen.  I managed to "test for" a nested fixed effect, using anova()
applied to the output of lm().  I also conducted that test using glht()
from the "multcomp" package, applied the output of lm() with the nested
fixed effect omitted from the model.

The outputs of the two tests were "close, but no cigar".  After puzzling
about this for a while, I guess that the discrepancy might be due to the
fact that my data were (somewhat) unbalanced.  I constructed a balanced
data set, applied the two tests, and bingo!  Exact agreement.

I would appreciate it if some learned and knowledgeable person or 
persons could offer some insight into what is going on here.  Presumably 
slightly different hypotheses are being tested by the two tests in the 
case of unbalanced data.  Has anyone a solid understanding (which they 
could express in simple terms that a Bear of Very Little Brain could 
understand) of just what hypotheses are being tested?

The "real" data set is rather large, unwieldy, and complicated.  To 
provide a reproducible example I offer a small and simple data set which 
is taken from Devore's textbook on intro stats for engineers.
This data set consists (I *think*; I don't have the book to hand) of 
percentages of polyunsaturated fat in various brands of margarine.  Two 
of the brands (Mazola and Fleischman) are corn-based; the others are 
soy-based.  We thus have the fixed effect "brand" nested within "base". 
  One might be interested in testing whether there is a difference 
between the two bases.  Devore sets the exercise of estimating the 
difference (with confidence interval) between (mu_1 + mu_2 + mu_3 + 
mu_4)/4 and (mu_5 + mu_6)/2, where mu_i is the mean of the response 
variable corresponding to the i-th brand.
I conducted a test for the equality of these two quantities using both
anova() and glht().

The data are unbalanced in that 4 of the brands have 4 observations each 
and 2 of them have 5 observations.  The results of the tests are not 
*quite* identical.  When I threw away an observation from each of the 
5-observation brands so that the data became balanced, the results of 
the two tests *were* identical.

I have attached the data set in "dput" form in the file "oleo.txt" and a 
script, that effects my analysis of these data in the file, 
"scrDemo.txt". The data are also available in the "Devore7" package as 
"ex10.26".

I *suspect* that what is going is that the glht() test is doing 
*something like* using "Type III" sums of squares whereas the anova() 
test is using sequential ("Type I") sums of squares.  But I am not sure 
about this and don't know how to work out what is really happening.  I 
did try to apply the Anova() function from the "car" package with "Type" 
set equal to "III", but it threw an error to the effect that
"there are aliased coefficients in the model".  (Not surprising.)

I would be grateful for any insight that anyone can offer.

	cheers,

	Rolf Turner
-------------- next part --------------
structure(list(Imperial = c(14.1, 13.6, 14.4, 14.3, NA), Parkay = c(12.8, 
12.5, 13.4, 13, 12.3), Blue.Bonnet = c(13.5, 13.4, 14.1, 14.3, 
NA), Chiffon = c(13.2, 12.7, 12.6, 13.9, NA), Mazola = c(16.8, 
17.2, 16.4, 17.3, 18), Fleischmann = c(18.1, 17.2, 18.7, 18.4, 
NA)), .Names = c("Imperial", "Parkay", "Blue.Bonnet", "Chiffon", 
"Mazola", "Fleischmann"), class = "data.frame", row.names = c("1", 
"2", "3", "4", "5"))
-------------- next part --------------
#
# Script scrDemo.
#
require(multcomp)

# Prepare the data.
Oleo  <- dget("oleo.txt")
brand <- factor(rep(names(Oleo),each=5),levels=names(Oleo))
base  <- factor(c(rep("soy",20),rep("corn",10)),levels=c("soy","corn"))
Z     <- data.frame(papfua=as.vector(as.matrix(Oleo)),brand=brand,base=base)
Zb    <- Z[-5*(1:6),]
Zu    <- Z[-5*c(1,3,4,6),]

# Balanced data (Zb).
cat("Balanced data:\n")
fitb1 <- lm(papfua ~ brand,data=Zb)
fitb2 <- lm(papfua ~ base + brand%in%base,data=Zb)
cat("Checking that the two fits are identical.\n")
print(anova(fitb1,fitb2))
cat("Coefficients of the fit *without* the nested fixed effect:\n")
print(coef(fitb1))
ctrst <- matrix(c(0,0.25,0.25,0.25,-0.5,-0.5),1)
rsltb <- glht(fitb1,ctrst)
cat("Test of the nested fixed effect using glht():\n")
print(summary(rsltb))
cat("Test of the nested fixed effect using anova():\n")
print(anova(fitb2))
cat("The p-values look identical and the F statistic is\n")
cat("the square of the \"t value\".  But check more carefully:\n")
A <- as.vector(summary(rsltb)$test$pvalues)
B <- anova(fitb2)[1,5]
cat("Difference in the p-values:\n")
print(A-B)
A <- unname(summary(rsltb)$test$tstat^2)
B <- anova(fitb2)[1,4]
cat("Difference in the statistics:\n")
print(A-B)

# Unbalanced data (Zu).
cat("Slightly unbalanced data:\n")
fitu1 <- lm(papfua ~ brand,data=Zu)
fitu2 <- lm(papfua ~ base + brand%in%base,data=Zu)
cat("Checking that the two fits are identical.\n")
print(anova(fitu1,fitu2))
rsltu <- glht(fitu1,ctrst)
cat("Test of the nested fixed effect using glht():\n")
print(summary(rsltu))
cat("Test of the nested fixed effect using anova():\n")
print(anova(fitu2))
cat("The p-values are not quite identical and the F statistic is\n")
cat("not quite the square of the \"t value\". Checking more carefully:\n")
A <- as.vector(summary(rsltu)$test$pvalues)
B <- anova(fitu2)[1,5]
cat("Difference in the p-values:\n")
print(A-B)
# Hard to tell if they really are different.  But ...
A <- unname(summary(rsltu)$test$tstat^2)
B <- anova(fitu2)[1,4]
cat("Difference in the statistics:\n")
print(A-B)

From bbolker at gmail.com  Fri Jan  3 05:04:15 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 02 Jan 2014 23:04:15 -0500
Subject: [R-sig-ME] Sample size calculation
In-Reply-To: <loom.20140102T133356-952@post.gmane.org>
References: <mailman.3.1388574001.4437.r-sig-mixed-models@r-project.org>	<326EF40C-4800-4DBC-9FF8-9A7367CC97D0@uiowa.edu>,
	<52C43F3C.3060201@gmail.com>	<683277FD-EE35-4878-A889-90567EC819D8@uiowa.edu>
	<loom.20140102T133356-952@post.gmane.org>
Message-ID: <52C636BF.4070507@gmail.com>

On 14-01-02 07:42 AM, Bonitta gianluca wrote:
> Thank You for soon reply fisrt of all
> 
> i have the dataset and r script maby i can send and you can help to 
> prototipe the script in r ...for example
> 
> 
> 
> for professor Ben :
> 
> i had seen long power pack but i think this need repeated measure for the 
> use i have not repeated measure ...do You think is possible to suite long 
> power pack for my case?

  I didn't necessarily mean that you should use longpower, I was just
giving a list of packages geared toward mixed model power analysis.
> 
> I have also try simulative way for post hoc power calculation.... but this 
> yeld pap power analysis paradox... when p-value is "big" power is low , 
> when p-value is "small" ower is hight  so i prefer to determinate a priori 
> power ....

  I didn't mean that you should do a post hoc power analysis (I
completely agree with you that post hoc power analysis is a bad idea!)
> 
> thank you so much
> 

  I just pushed an example of what I mean by simulation-based power
analysis to Rpubs: http://rpubs.com/bbolker/11703

 cheers
    Ben Bolker


From tim.cole at ucl.ac.uk  Mon Jan  6 10:04:39 2014
From: tim.cole at ucl.ac.uk (Cole, Tim)
Date: Mon, 6 Jan 2014 09:04:39 +0000
Subject: [R-sig-ME] c++ exception with logistic glmer
Message-ID: <CEF02225.2C406%tim.cole@ucl.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140106/697bec1d/attachment.pl>

From bbolker at gmail.com  Mon Jan  6 14:43:21 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 06 Jan 2014 08:43:21 -0500
Subject: [R-sig-ME] c++ exception with logistic glmer
In-Reply-To: <CEF02225.2C406%tim.cole@ucl.ac.uk>
References: <CEF02225.2C406%tim.cole@ucl.ac.uk>
Message-ID: <52CAB2F9.50306@gmail.com>

On 14-01-06 04:04 AM, Cole, Tim wrote:
> The dataset consists of longitudinal measures of bone score during
> puberty, where 1000 indicates maturity. The aim is to estimate median
> age at maturity in four groups by sex and ethnicity.
> 
> This glm code works fine, but ignores the longitudinal element. lm2
> <- glm(I(RUSBoneScore == 1000) ~ log(DecAge) + Sex * Ethnicity,
> family=binomial, data=na.omit(bh[, 3:6]))
> 
> This code, which adds a random subject effect, fails with the
> unhelpful error message below. lm3 <- glmer(I(RUSBoneScore == 1000) ~
> log(DecAge) + Sex * Ethnicity + (1 | BHID), family=binomial,
> data=na.omit(bh[, 2:6]))
> 
> Error in pwrssUpdate(pp, resp, tolPwrss, GHrule(0L), compDev,
> verbose) : c++ exception (unknown reason)
> 
> I've tried various alternatives but they all fail in the same way.
> Thoughts please.

    Hard to say without a reproducible example. My main comment is that
this is *not* a problem I have seen anyone report before.  sessionInfo()
please?  Can you try with verbose=100? Can you make a reproducible
example that is a reasonable size and doesn't have confidentiality problems?

  Ben Bolker

> 
> Thanks, Tim Cole -- Tim.Cole at ucl.ac.uk<mailto:Tim.Cole at ich.ucl.ac.uk>
> Phone +44(0)20 7905 2666 Fax +44(0)20 7905 2381 Centre for Paediatric
> Epidemiology and Biostatistics UCL Institute of Child Health, London
> WC1N 1EH, UK
>


From bbolker at gmail.com  Tue Jan  7 03:08:06 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 7 Jan 2014 02:08:06 +0000 (UTC)
Subject: [R-sig-ME]
	=?utf-8?q?Fit_negative_binomial_glmm_credible_interval?=
	=?utf-8?q?s_from_the=09predict_function=3F?=
References: <CAATUFec60Y0ESJKdVx2+aD7RfqnJYriqi2bbEyxsc_+frSD+cw@mail.gmail.com>
Message-ID: <loom.20140107T030759-892@post.gmane.org>

Sheryn Olson <sheryn.olson at ...> writes:

> 
> Hello Ben and all,
> 
> Thank you Ben, very much for your previous reply.  Yes, stand/plot makes
> sense.  After talking with a few statisticians, I got convergence!   The
> repeated measure issue is solvable with a dummy variable for each of the 3
> years (like a fiscal or school year) - so yr1 = 2010smr+2011wtr, and so
> on.  Pellet plots were sampled 6 times, 3 each season, vegetation only
> once.  Among year variability can be high.
> 
> I'm now attempting to make predictive plots from models.  I used UCLA's
> method posted at their IRDA site that has a negative binomial example:
> 
>             http://www.ats.ucla.edu/stat/r/dae/nbreg.htm
>            newdata2 <- cbind(newdata2, predict(m1, newdata2,type =
> "link", se.fit=TRUE))
>            newdata2 <- within(newdata2, {
>            DaysAbsent <- exp(fit)
>                  LL <- exp(fit - 1.96 * se.fit)
>                  UL <- exp(fit + 1.96 * se.fit)
>              })
> but I have an uneasy feeling about fitting standard errors from a
> gaussian distribution.

  In case it helps, what you're assuming is Normal is the *sampling
distribution of the parameters*, not the data themselves ...

  The bottom line is that what you're doing seems reasonable, with
the caveats already expressed in ?predict.glmmadmb ...

> In the past I've used MCMCglmm to estimate credible intervals around the
> beta coefficients of a categorical variable, but now,
> I have a continuous variable, conifer sapling count.

  Well, technically it's a discrete (count) variable, not continuous ...

  Section 2 of the glmmADMB vignette [vignette("glmmADMB")] has
a bit of information about running post-fit MCMC.   There should
be an example of how to generate confidence intervals (basically,
you would generate predictions for each sampled set of coefficients
and then compute the quantiles or credible intervals of the
predictions), but there isn't (yet ...)

> So, lots of questions!
> 
> How does one make an accurate predictive graph from a nbinom model?

  Good question.  Not easy, you may need to take some shortcuts
and hope for the best.

> Does it make sense to set up a dataframe with the means of the
> covariates when the covariates'
> distributions were skewed count data?

  It depends what values you want to predict for.  The data frame
typically includes "typical" values of the parameters; you're welcome
to compute predictions for the medians instead if you prefer, or
for a range of values.

> How do I use MCMCglmm correctly?

  ??  Do you mean the 'mcmc' argument of glmmadmb?

> Is "se.fit" valid for the nbinom log link, or....?

  [Update: I just wrote a bunch of stuff about how se.fit doesn't
work for glmmadmb, when it indeed does! However, I had already
written all the stuff below, which describes more generally how
you would do it by hand, so I'm just going to send it anyway]

  The basic recipe for constructing confidence intervals, as laid out
in http://glmm.wikidot.com/faq in the "Predictions and/or confidence
(or prediction) intervals on predictions" section, is

 (1) construct the model matrix (X)
  * If you want predictions on the original model (not this case),
the model.matrix() accessor may work to extract it from the fit for you
  * If not, or if you want to construct predictions for new data,
you need to call model.matrix() with the fixed-effect formula (you
may or may not be able to extract the fixed-effect part of the
formula only from the model, but it's usually fairly easy just
to respecify it

  (2) extract the variance-covariance matrix of the fixed effect
predictors, V (usually just vcov(model))

  (3) extract the fixed-effect coefficients (usually just fixef(model)
or coef(model))

 then the predictions (on the linear predictor scale, at the population
level [i.e. ignoring all random effects]) are

 X %*% beta

(plus an offset if there is one)

and the standard errors of prediction, *CONDITIONAL ON THE
RANDOM EFFECTS* [i.e. ignoring all uncertainty due to uncertainty
of the random effects] are

  sqrt(diag(X %*% V %*% t(X)))

 Then to get confidence intervals you should compute

  inverse_link(fit +/- 1.96*se.fit) exactly as you have below

This assumes further that the sampling distribution of the
coefficients is Normal (in which case the linear computation
we did above will also lead to a Normal distribution, on 
the linear predictor scale).  This might not be true for
small or badly behaved data sets (of course it is never
exactly true except asymptotically ...), but there's not
much you can do about it without working much harder.

> 
> my Code:
> ########## CONIFER SAPLINGS by pellets/ha/month (phm)  #####################
> #### mod1.all <- glmmadmb(pellets ~ season * (t.con.splgs +
> t.dec.trees + pctMidCov + t.BAtrees + cc) +
> 
> ####                     offset(ln.days)+(1|stand/plot)+ (1|hareyr),
> data=hv, family="nbinom")
> predCS <- data.frame(
>                      t.con.splgs = rep(seq(from = min(hv$t.con.splgs),
> to = max(hv$t.con.splgs),length.out = 100), 2),
> 
>                          t.dec.trees=rep(mean(hv$t.dec.trees),200),
>                          pctMidCov=rep(mean(hv$pctMidCov),200),
> 
> t.BAtrees=rep(mean(hv$t.BAtrees),200),cc=rep(mean(hv$cc),200),
> 
>                          ln.days=rep(log(30.25),200),
>                          season = factor(rep(1:2, each = 100), levels
> = 1:2, labels =levels(hv$season)))
> 
> predCS <- cbind(predCS, predict(mod1.all, predCS, type = "link", se.fit=TRUE))
> 
> predCS <- within(predCS, {
>   pellets <- exp(fit)
>   LL <- exp(fit - 1.96 * se.fit)   # these seem wrong !!
>   UL <- exp(fit + 1.96 * se.fit)   # ??
> })

  Do you mean you're suspicious of the code, or that the numbers
it produces seem wrong?

> 
> head(predCS)           #  check to see fits
> 
>   t.con.splgs t.dec.trees pctMidCov t.BAtrees       cc  ln.days season
>       fit    se.fit       UL       LL   pellets
> 1   0.0000000    3.493585  40.80654  4.117168 82.84937 3.409496    smr
> -1.051665 0.5187013 6437.304 842.6536 0.3493557
> 
> 2   0.7142493    3.493585  40.80654  4.117168 82.84937 3.409496    smr
> -1.046329 0.5187194 6471.975 847.1320 0.3512248
> 3   1.4284985    3.493585  40.80654  4.117168 82.84937 3.409496    smr
> -1.040993 0.5187633 6507.163 851.5911 0.3531040
> 
> 4   2.1427478    3.493585  40.80654  4.117168 82.84937 3.409496    smr
> -1.035657 0.5188331 6542.872 856.0304 0.3549932
> 5   2.8569971    3.493585  40.80654  4.117168 82.84937 3.409496    smr
> -1.030321 0.5189286 6579.111 860.4492 0.3568926
> 
> 6   3.5712464    3.493585  40.80654  4.117168 82.84937 3.409496    smr
> -1.024984 0.5190500 6615.885 864.8472 0.3588020
>   con.splgs.ha      phm
> 1      0.00000 2329.038
> 2      5.10152 2341.499
> 3     20.40608 2354.027
> 
> 4     45.91368 2366.622
> 5     81.62432 2379.284
> 6    127.53801 2392.014
> 
> ############# pellets/ha/month (phm) scale up
> #############  and scale up saplings to per ha from 0.1ha plot level
> predCS <- within(predCS, {
> 
>   pellets <- exp(fit)
>   phm <- pellets/1.5*10000
>    LL <- (exp(fit - 1.96 * se.fit))/1.5*10000
>    UL <- (exp(fit + 1.96 * se.fit))/1.5*10000
>    con.splgs.ha <- 10*(t.con.splgs^2)          # backtransform square
> root and scale up
> 
> })
> CS <- ggplot(predCS, aes(con.splgs.ha, phm)) +
> 
>   theme_bw() +   #eliminates background, gridlines, but not border
>   theme(
>     plot.background = element_blank()
>    ,panel.grid.major = element_blank()
> 
>    ,panel.grid.minor = element_blank()
> #   ,panel.border = element_blank()
>    ,panel.background = element_blank()
>   ) +
> 
>   geom_ribbon(aes(ymin = LL, ymax = UL, fill = season),alpha = .25) +
>   #geom_line(aes(colour = season),size = 1) +
> 
>   geom_smooth(aes(colour=season, linetype=season),size = 1, se = F) +
>   scale_colour_manual(values=c("wtr"= 4, "smr" = 3)) +
>   scale_x_continuous(limits = c(0, 25000)) +
>   scale_y_continuous(limits = c(0, 12000)) +
> 
>   scale_fill_brewer(palette="Accent") +
> 
>   theme(legend.position = 'none') +         #get rid of the legend
>   labs(x = "Conifer Sapling Count per ha", y = "Predicted Pellets/ha/month")
> 
> print(CS)
> 
> Thanks for any ideas.
> Sheryn
>


From tim.cole at ucl.ac.uk  Tue Jan  7 15:12:13 2014
From: tim.cole at ucl.ac.uk (Cole, Tim)
Date: Tue, 7 Jan 2014 14:12:13 +0000
Subject: [R-sig-ME] R-sig-mixed-models Digest, Vol 85, Issue 6
In-Reply-To: <mailman.5.1389092402.27733.r-sig-mixed-models@r-project.org>
References: <mailman.5.1389092402.27733.r-sig-mixed-models@r-project.org>
Message-ID: <CEF1B227.2C4CA%tim.cole@ucl.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140107/cbf76fda/attachment.pl>

From jersa at centrum.cz  Tue Jan  7 11:24:55 2014
From: jersa at centrum.cz (jersa at centrum.cz)
Date: Tue, 07 Jan 2014 11:24:55 +0100
Subject: [R-sig-ME] help
Message-ID: <20140107112455.AF49D5A7@centrum.cz>

Dear glmer experts,

I would be very happy if someone could help with following problem.	

I have following data: I planted seed bags in the vicinity of 10 mother plants into three directions and 4 distances (10,31,56, 100cm). The germination success (0/1) was asseed by extracting one seed bag per microsite for next three years. I am interested into the effect of distance on germination success and possible differencies in germination between years. The data have lots of 0 values.

I used originaly following syntax 
glmer(germination~distance+(1|plant/direction/year),family=binomial,data=seed) 
but I was told that distance, year and direction can interact and not always in the way as specified by this type of nesting.

Would this be correct the one to test interaction between year and distance?
glmer(germination~distance+(1|plant/direction)+(1|distance:year),family=binomial,data=seed) 

Thank you very much for any hints. Jana


From cubranic at stat.ubc.ca  Mon Jan  6 08:37:24 2014
From: cubranic at stat.ubc.ca (Davor Cubranic)
Date: Sun, 5 Jan 2014 23:37:24 -0800
Subject: [R-sig-ME] [Rcpp-devel] Question on lme4 book
In-Reply-To: <CAC2h7us-KmJ3PozwmGWygyeJfwk+dOxen3d+dXBW6EK6faDTnw@mail.gmail.com>
References: <CAO7JsnQugtx-ZdKWcsg5A2wLJOWSQq+Ssxvd6HimYD48f4Zcyg@mail.gmail.com>
	<CAC2h7us-KmJ3PozwmGWygyeJfwk+dOxen3d+dXBW6EK6faDTnw@mail.gmail.com>
Message-ID: <699E4E0F-D329-48F1-9909-5BFB2CDA3E95@stat.ubc.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140105/066d7c3d/attachment.pl>

From David.Duffy at qimr.edu.au  Wed Jan  8 01:04:02 2014
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Wed, 8 Jan 2014 10:04:02 +1000
Subject: [R-sig-ME] R-sig-mixed-models Digest, Vol 85, Issue 6
In-Reply-To: <CEF1B227.2C4CA%tim.cole@ucl.ac.uk>
References: <mailman.5.1389092402.27733.r-sig-mixed-models@r-project.org>
	<CEF1B227.2C4CA%tim.cole@ucl.ac.uk>
Message-ID: <alpine.LMD.2.00.1401080947001.30070@orpheus.qimr.edu.au>

On Wed, 8 Jan 2014, Cole, Tim wrote:

> The offending code is
> mod <- glmer(mat ~ log(DecAge) + Sex * Ethnicity + (1 | BHID), family=binomial, nAGQ=0, data=df)
>
> The error message is simple to generate with a 
> trivial dataset, restricted to subjects with 2 to 4 records and both 0 
> and 1 values for mat.
>
> mat is a binary mature bone measure, which for each subject consists of 
> zero or more 0s followed by zero or more 1s, the 0/1 transition 
> providing useful information about the age of maturity.

In your example, it looks like quasi-separation for mat v. DecAge in males 
might be to blame (when I plot them), and:

library(glmmML)
glmmML(mat ~ log(DecAge) + Sex * Ethnicity, cluster=BHID, data=df,
        control = list(maxit = 2000))
[glmmml] fail = 1
Max. No. of iterations reached without convergence

Cheers, David Duffy.

| David Duffy (MBBS PhD)
| email: David.Duffy at qimrberghofer.edu.au  ph: INT+61+7+3362-0217 fax: -0101
| Genetic Epidemiology, QIMR Berghofer Institute of Medical Research
| 300 Herston Rd, Brisbane, Queensland 4006, Australia  GPG 4D0B994A


From bbolker at gmail.com  Wed Jan  8 04:11:23 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 8 Jan 2014 03:11:23 +0000 (UTC)
Subject: [R-sig-ME] help
References: <20140107112455.AF49D5A7@centrum.cz>
Message-ID: <loom.20140108T034928-810@post.gmane.org>

 <jersa at ...> writes:

> 
> Dear glmer experts,
> 
> I would be very happy if someone could help with following problem.	
 
> I have following data: I planted seed bags in the vicinity of 10
> mother plants into three directions and 4 distances (10,31,56,
> 100cm). The germination success (0/1) was asseed by extracting one
> seed bag per microsite for next three years. I am interested into
> the effect of distance on germination success and possible
> differencies in germination between years. The data have lots of 0
> values.

> I used originaly following syntax 
> glmer(germination~distance+(1|plant/direction/year),
>    family=binomial,data=seed)

  Logically, direction and distance seem more like fixed effects to me
(see http://glmm.wikidot.com/faq#fixed_vs_random for more discussion),
but this leads to some serious overparameterization problems, so
you may actually be better off treating them as grouping factors
as you are here.

  Since you have a randomized-block design (all levels of fixed
effects are replicated within every block), you could *in principle*
fit a full model:

glmer(germination~distance*direction*year+
    (distance*direction*year|plant), ...)

that accounts for the variation in all effects among plants, but
it certainly won't be practical -- there are 36 combinations
of year/direction/distance, and the random effect here would try
to estimate all of the correlations among them, so you'd have
36 fixed-effect parameters and (36*37/2) random effect parameters --
somewhat crazy.  You have a total of 360 observations, but if you
have "lots of zeros" then the effective sample size is more
appropriately considered as the number of successful germinations
(see Harrell _Regression Modeling Strategies_).  If we suppose you
have 10% germination overall, you shouldn't be trying to fit more
than three or four (approx. N/10) parameters to this data set, so you're
going to be having some difficulty ...

  Even

  germination~(distance+direction+year)^2+ (1|plant)

which fits all the two-way interactions between distance/direction/year
is way too complex ...

  The logical problem with your

 plant/direction/year 

specification is that it assumes that the
effects of direction can only vary within plants, not consistently
across plants (maybe reasonable if your directions differ for each
plant and are not e.g. North/South/West), and worse that the
effect of year can only vary within plant and direction and not
overall.  It's tempting to use (1|plant/direction)+(1|year) , but
then you'll be in trouble because it's hard to estimate a variance
from three points (you'll probably end up concluding, wrongly, that
there's zero variance across years).  Logically you could add year
as a fixed effect, but that then costs another two parameters, which
you can hardly afford to spend ...

  To get back to your original question about interactions -- unless
they're very large, I think you're going to have a hard time detecting
them in any case with this size data set.  What I might do is use something
close to your original model, or perhaps

  ~distance+year+(1|plant/direction)

or

   ~distance+year+(distance|plant)

(since distance is your variable of primary interest, you really
should be trying to allow for among-plant variation in it -- see
Schielzeth and Forstmeier 2009) -- this is not going to be practical
unless you treat distance as a continuous variable though.

  Bottom line: I would try to do something fairly simple and sensible,
*LOOK AT YOUR DATA* to try to see what the main patterns are, and
hope that large interactions etc. will emerge in the model diagnostic
plots if they're there.

  good luck
   Ben Bolker


From emmanuel.curis at parisdescartes.fr  Wed Jan  8 06:46:20 2014
From: emmanuel.curis at parisdescartes.fr (Emmanuel Curis)
Date: Wed, 8 Jan 2014 06:46:20 +0100
Subject: [R-sig-ME] R-sig-mixed-models Digest, Vol 85, Issue 6
In-Reply-To: <CEF1B227.2C4CA%tim.cole@ucl.ac.uk>
References: <mailman.5.1389092402.27733.r-sig-mixed-models@r-project.org>
	<CEF1B227.2C4CA%tim.cole@ucl.ac.uk>
Message-ID: <20140108054620.GB31187@info124.pharmacie.univ-paris5.fr>

Hi,

I'm not sure if this may be related or not to the issue, but in the
subset you sent you seem to have duplicate rows, differing only by
their row name (see for instance 3090 & 3697, 3156 & 3763). I guess
this is because some columns were removed, but could it be related to
the difficulties to fit, and the oddities of error occuring or not
according to the subset ?

Best regards,

On Tue, Jan 07, 2014 at 02:12:13PM +0000, Cole, Tim wrote:
? Thanks Ben for your speedy response, and for confirming that my
problem is new.

? The dataset has 5048 records and 607 subjects. I've experimented with subsets of the data and it is rather complicated - I'll send you the data and code separately. The error message is simple to generate with a trivial dataset, restricted to subjects with 2 to 4 records and both 0 and 1 values for mat. But there is also a problem with subjects that have 10 records, though not 9 or 11 ? very odd.
? 
? mat is a binary mature bone measure, which for each subject consists of zero or more 0s followed by zero or more 1s, the 0/1 transition providing useful information about the age of maturity.
? 
?      BHID  Sex Ethnicity    DecAge mat
? 2     102 Girl     White  9.273973   0
? 4251  102 Girl     White 15.520000   1
? 55    169 Girl     White  9.805381   0
? 1876  169 Girl     White 12.876781   0
? 3090  169 Girl     White 14.846575   1
? 3697  169 Girl     White 14.846575   1
? 676   184 Girl     White 10.425736   0
? 6139  184 Girl     White 19.452430   1
? 1896  192  Boy     White 12.493616   0
? 6145  192  Boy     White 19.430527   1
? 1307  356  Boy     Black 11.246575   0
? 1914  356  Boy     Black 12.307274   0
? 6163  356  Boy     Black 19.255305   1
? 3156  386  Boy     Black 14.890000   0
? 3763  386  Boy     Black 14.890000   0
? 4977  386  Boy     Black 17.130000   0
? 6191  386  Boy     Black 19.364819   1
? 156   421 Girl     Black  9.235616   0
? 6226  421 Girl     Black 19.526352   1
? 264   531 Girl     Black  9.799984   0
? 1478  531 Girl     Black 11.796624   0
? 2692  531 Girl     Black 13.763384   0
? 5120  531 Girl     Black 16.752000   1
? 1103  778 Girl     White 10.732375   0
? 1710  778 Girl     White 11.747646   0
? 6566  778 Girl     White 19.433265   1
? 1791  869 Girl     Black 11.968995   0
? 3005  869 Girl     Black 13.997055   0
? 4826  869 Girl     Black 16.073000   0
? 6040  869 Girl     Black 18.124000   1
? 4834  880  Boy     White 15.504000   0
? 5441  880  Boy     White 17.076000   1
? 6048  880  Boy     White 17.823000   1
? 6655  880  Boy     White 18.562628   1
? 
? Best wishes,
? Tim
? 
? On 14-01-06 04:04 AM, Cole, Tim wrote:
? The dataset consists of longitudinal measures of bone score during
? puberty, where 1000 indicates maturity. The aim is to estimate median
? age at maturity in four groups by sex and ethnicity.
? This glm code works fine, but ignores the longitudinal element. lm2
? <- glm(I(RUSBoneScore == 1000) ~ log(DecAge) + Sex * Ethnicity,
? family=binomial, data=na.omit(bh[, 3:6]))
? This code, which adds a random subject effect, fails with the
? unhelpful error message below. lm3 <- glmer(I(RUSBoneScore == 1000) ~
? log(DecAge) + Sex * Ethnicity + (1 | BHID), family=binomial,
? data=na.omit(bh[, 2:6]))
? Error in pwrssUpdate(pp, resp, tolPwrss, GHrule(0L), compDev,
? verbose) : c++ exception (unknown reason)
? I've tried various alternatives but they all fail in the same way.
? Thoughts please.
? 
?     Hard to say without a reproducible example. My main comment is that
? this is *not* a problem I have seen anyone report before.  sessionInfo()
? please?  Can you try with verbose=100? Can you make a reproducible
? example that is a reasonable size and doesn't have confidentiality problems?
? 
?   Ben Bolker
? 
? 	[[alternative HTML version deleted]]
? 

? _______________________________________________
? R-sig-mixed-models at r-project.org mailing list
? https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
                                Emmanuel CURIS
                                emmanuel.curis at parisdescartes.fr

Page WWW: http://emmanuel.curis.online.fr/index.html


From tim.cole at ucl.ac.uk  Wed Jan  8 10:54:11 2014
From: tim.cole at ucl.ac.uk (Cole, Tim)
Date: Wed, 8 Jan 2014 09:54:11 +0000
Subject: [R-sig-ME] R-sig-mixed-models Digest, Vol 85, Issue 6
In-Reply-To: <alpine.LMD.2.00.1401080947001.30070@orpheus.qimr.edu.au>
References: <mailman.5.1389092402.27733.r-sig-mixed-models@r-project.org>
	<CEF1B227.2C4CA%tim.cole@ucl.ac.uk>
	<alpine.LMD.2.00.1401080947001.30070@orpheus.qimr.edu.au>
Message-ID: <CEF2D022.1DA4F%tim.cole@ucl.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140108/6d86fe6c/attachment.pl>

From tim.cole at ucl.ac.uk  Wed Jan  8 11:13:00 2014
From: tim.cole at ucl.ac.uk (Cole, Tim)
Date: Wed, 8 Jan 2014 10:13:00 +0000
Subject: [R-sig-ME] R-sig-mixed-models Digest, Vol 85, Issue 6
In-Reply-To: <20140108054620.GB31187@info124.pharmacie.univ-paris5.fr>
References: <mailman.5.1389092402.27733.r-sig-mixed-models@r-project.org>
	<CEF1B227.2C4CA%tim.cole@ucl.ac.uk>
	<20140108054620.GB31187@info124.pharmacie.univ-paris5.fr>
Message-ID: <CEF2D0C8.1DA54%tim.cole@ucl.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140108/65f1cd92/attachment.pl>

From bbolker at gmail.com  Wed Jan  8 16:07:28 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 08 Jan 2014 10:07:28 -0500
Subject: [R-sig-ME] R-sig-mixed-models Digest, Vol 85, Issue 6
In-Reply-To: <CEF2D0C8.1DA54%tim.cole@ucl.ac.uk>
References: <mailman.5.1389092402.27733.r-sig-mixed-models@r-project.org>
	<CEF1B227.2C4CA%tim.cole@ucl.ac.uk>
	<20140108054620.GB31187@info124.pharmacie.univ-paris5.fr>
	<CEF2D0C8.1DA54%tim.cole@ucl.ac.uk>
Message-ID: <52CD69B0.4060805@gmail.com>

On 14-01-08 05:13 AM, Cole, Tim wrote:
> Well spotted! This, it turns out, explains the fitting problems. I feel
> a bit foolish not noticing it myself. 
> 
> I'm not sure if [gn]lmer should flag or ban duplicate measurements, as
> in principle they could be legitimate (though not in my case).
> 
> Many thanks,
> Tim Cole

   In any case it still shouldn't crash with an obscure error message!
   I can imagine this issue *might* have been fixed by some of our
'clamping' fixes, i.e.

https://github.com/lme4/lme4/commit/c51a142be1781520f8bd1d7bb6aee27bf4e96568

  If you have a chance it would be great if you could test with a
development version of lme4 and see if the problem goes away ...

  Ben Bolker

> ---
> Tim.Cole at ucl.ac.uk <mailto:Tim.Cole at ucl.ac.uk> Phone +44(0)20 7905 2666
> Fax +44(0)20 7905 2381
> Centre for Paediatric Epidemiology and Biostatistics
> UCL Institute of Child Health, London WC1N 1EH, UK
> 
> From: Emmanuel Curis <emmanuel.curis at parisdescartes.fr
> <mailto:emmanuel.curis at parisdescartes.fr>>
> Date: Wednesday, 8 January 2014 05:46
> To: Tim Cole <tim.cole at ucl.ac.uk <mailto:tim.cole at ucl.ac.uk>>
> Cc: Ben Bolker <bbolker at gmail.com <mailto:bbolker at gmail.com>>,
> "r-sig-mixed-models at r-project.org
> <mailto:r-sig-mixed-models at r-project.org>"
> <r-sig-mixed-models at r-project.org <mailto:r-sig-mixed-models at r-project.org>>
> Subject: Re: [R-sig-ME] R-sig-mixed-models Digest, Vol 85, Issue 6
> 
> Hi,
> 
> I'm not sure if this may be related or not to the issue, but in the
> subset you sent you seem to have duplicate rows, differing only by
> their row name (see for instance 3090 & 3697, 3156 & 3763). I guess
> this is because some columns were removed, but could it be related to
> the difficulties to fit, and the oddities of error occuring or not
> according to the subset ?
> 
> Best regards,
> 
> On Tue, Jan 07, 2014 at 02:12:13PM +0000, Cole, Tim wrote:
> ? The dataset has 5048 records and 607 subjects. I've experimented with
> subsets of the data and it is rather complicated - I'll send you the
> data and code separately. The error message is simple to generate with a
> trivial dataset, restricted to subjects with 2 to 4 records and both 0
> and 1 values for mat. But there is also a problem with subjects that
> have 10 records, though not 9 or 11 ? very odd.
> ?
> ? mat is a binary mature bone measure, which for each subject consists
> of zero or more 0s followed by zero or more 1s, the 0/1 transition
> providing useful information about the age of maturity.
> 
> ? On 14-01-06 04:04 AM, Cole, Tim wrote:
> ? The dataset consists of longitudinal measures of bone score during
> ? puberty, where 1000 indicates maturity. The aim is to estimate median
> ? age at maturity in four groups by sex and ethnicity.
> ? This glm code works fine, but ignores the longitudinal element. lm2
> ? <- glm(I(RUSBoneScore == 1000) ~ log(DecAge) + Sex * Ethnicity,
> ? family=binomial, data=na.omit(bh[, 3:6]))
> ? This code, which adds a random subject effect, fails with the
> ? unhelpful error message below. lm3 <- glmer(I(RUSBoneScore == 1000) ~
> ? log(DecAge) + Sex * Ethnicity + (1 | BHID), family=binomial,
> ? data=na.omit(bh[, 2:6]))
> ? Error in pwrssUpdate(pp, resp, tolPwrss, GHrule(0L), compDev,
> ? verbose) : c++ exception (unknown reason)
> ? I've tried various alternatives but they all fail in the same way.
> ? Thoughts please.
> ?
> ?     Hard to say without a reproducible example. My main comment is that
> ? this is *not* a problem I have seen anyone report before.  sessionInfo()
> ? please?  Can you try with verbose=100? Can you make a reproducible
> ? example that is a reasonable size and doesn't have confidentiality
> problems?
> ?
> ?   Ben Bolker
> ?
> ? _______________________________________________
> ? R-sig-mixed-models at r-project.org
> <mailto:R-sig-mixed-models at r-project.org> mailing list
> ? https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> -- 
>                                 Emmanuel CURIS
>                                 emmanuel.curis at parisdescartes.fr
> <mailto:emmanuel.curis at parisdescartes.fr>
> 
> Page WWW: http://emmanuel.curis.online.fr/index.html
> 
>


From bbolker at gmail.com  Thu Jan  9 01:08:59 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 08 Jan 2014 19:08:59 -0500
Subject: [R-sig-ME] help
In-Reply-To: <20140108173447.4CCC7AC0@centrum.cz>
References: <20140107112455.AF49D5A7@centrum.cz>
	<loom.20140108T034928-810@post.gmane.org>
	<20140108173447.4CCC7AC0@centrum.cz>
Message-ID: <52CDE89B.1040904@gmail.com>

   [With apologies, I am re-forwarding this to r-sig-mixed-models: I
prefer to keep discussions there so they can be publicly viewed/archived.]

On 14-01-08 11:34 AM, jersa at centrum.cz wrote:
> Dear Ben, thank you so much for your help. It is rather hard for me
> to imagine all the statistical consequencies when building this
> model. Basically I do not see any clear pattern in the data, it seems
> that mother vicinity has no effect on germination and germination is
> quite random. I only see that the second year yielded most
> germinations, which is also confirmed in the tests. 

 OK

> The set up of
> directions was in all plants same (N, SE and SW).

   So there would be at least some possibility of consistent effects of
direction across plants (as well as random variations in those effects
across plants)

> 
> I tried to fit both variants you suggested (see bellow). And the
> results are nearly identical. May I aks one more question about
> plotting the probability to germinate with distance? Which function
> would you recommend for this?


  It's probably best to construct a new data frame that has the desired
distance vector in it, e.g.

   newdat <- data.frame(distance=seq(100))

then use predict() to get the probability

  newdat$germ_prob <- predict(model,newdata=newdat,type="response")

> 
> Thank you very much and have a nice day Jana
> 
>> prot1<-glmer(germination~distance+year+(1|plant/direction),family=binomial,data=prot)
>>
>> 
> summary(prot1)
> Random effects: Groups     Name        Variance Std.Dev. 
> direction:plant (Intercept) 0.0544   0.2332 plant
> (Intercept) 1.4151   1.1896 Number of obs: 252, groups:
> direction:plant, 21; plant, 7

I thought you said you had 10 plants, and I would have inferred 360
observations from your experimental design -- actually only 7 / 252?
> 
> Fixed effects: Estimate Std. Error z value Pr(>|z|) 

(Intercept) -1.840153   0.617722  -2.979  0.00289 **
vzd         -0.004394   0.005174  -0.849  0.39572
year2009     0.995211   0.434851   2.289  0.02210 *
year2011     0.588760   0.444732   1.324  0.18555

  Treating distance (I assume that's vzd?) as continuous is a good idea,
although you should take a look at the data to see that there isn't a
strong nonlinear trend.

> 
>> prot2<-glmer(germination~distance+year+(distance|plant),family=binomial,data=prot)
>>
>> 
summary(prot2)
> 
> Random effects: Groups Name        Variance  Std.Dev. Corr 
                   kytka (Intercept) 7.580e-01 0.870637
                         vzd         4.647e-05 0.006817 1.00
> Number of obs: 252, groups: kytka, 7

  Here 'kytka' is plant?   The distance effect and intercept are
perfectly correlated here, suggesting overfitting -- and the distance
variance is extremely small (ditto) -- yo
> 
> Fixed effects: Estimate Std. Error z value Pr(>|z|) 
(Intercept) -1.707002   0.527848  -3.234  0.00122 **
vzd         -0.007350   0.005982  -1.229  0.21916
year2009     0.993268   0.435158   2.283  0.02246 *
year2011     0.587586   0.444938   1.321  0.18663

  Notice the fixed-effect estimates here are nearly identical to the
previous case, because the estimates for the variances of the terms you
added to the model are nearly zero.


From bates at stat.wisc.edu  Fri Jan 10 17:01:34 2014
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 10 Jan 2014 10:01:34 -0600
Subject: [R-sig-ME] question regarding output using lme4
In-Reply-To: <60EC7FAB16017B46BB72E5D2C6CF3EF801DCC54AB1@GU-MBX06.ad.gumail.local>
References: <60EC7FAB16017B46BB72E5D2C6CF3EF801DCC54AB1@GU-MBX06.ad.gumail.local>
Message-ID: <CAO7JsnQEws0UjK94jfHS3oiBCaMXQy=qmcahTY_6g==QGnyJfQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140110/21bea0f8/attachment.pl>

From elin.renborg at bioenv.gu.se  Sat Jan 11 15:23:35 2014
From: elin.renborg at bioenv.gu.se (Elin Renborg)
Date: Sat, 11 Jan 2014 14:23:35 +0000
Subject: [R-sig-ME] question regarding output using lme4
Message-ID: <60EC7FAB16017B46BB72E5D2C6CF3EF801DCC55C1C@GU-MBX06.ad.gumail.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140111/f682ff32/attachment.pl>

From bbolker at gmail.com  Sat Jan 11 15:54:18 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 11 Jan 2014 09:54:18 -0500
Subject: [R-sig-ME] question regarding output using lme4
In-Reply-To: <60EC7FAB16017B46BB72E5D2C6CF3EF801DCC55C1C@GU-MBX06.ad.gumail.local>
References: <60EC7FAB16017B46BB72E5D2C6CF3EF801DCC55C1C@GU-MBX06.ad.gumail.local>
Message-ID: <52D15B1A.1020702@gmail.com>

On 14-01-11 09:23 AM, Elin Renborg wrote:
> I wonder if someone could help me? I get the output below when I use
> the lmer function from the lme4 package, and I don?t seem to find an
> answer to what dam.1 and sire.1 stand for.
> 
> Best regards,
> 
> Elin Renborg
> 
> 

  Your problem is that you have inadvertently specified the 'sire' and
'dam' random effects twice; since (1 | sire/salt) (for example) expands
to (1|sire) + (1|sire:salt) (where sire:salt indicates an *interaction*
between sire and salt),  (1 | sire ) + (1 |sire/salt) expands to (1 |
sire) + (1| sire) + (1|sire:salt).

  It would be nice if we could (1) automatically detect and
collapse/drop redundant terms; (2) do a better job warning on redundant
terms, but so far we can't.

  The solution is simply to use (1|sire/salt) + (1|dam/salt), leaving
out the (1|sire) and (1|dam) terms.


>> lmer.tot <- lmer(metsucc ~ matrix + salt + (1 | sire) + (1 | dam) +
>> (1 | sire/salt) + (1 | dam/salt), weights=baseS, data = gf) 
>> summary(lmer.tot)
> Linear mixed model fit by REML ['lmerMod'] Formula: metsucc ~ matrix
> + salt + (1 | sire) + (1 | dam) + (1 | sire/salt) +      (1 |
> dam/salt) Data: gf
> 
> REML criterion at convergence: 240.7323
> 


If you look at the variance estimates here, and just focus on the orders
of magnitude, you'll see that both 'dam' terms (dam and dam.1) are tiny,
and one of the sire terms is much smaller than the

 Groups    Name        Variance  Std.Dev.
 salt.dam  (Intercept) 1.073e-03 3.275e-02
 salt.sire (Intercept) 5.051e-05 7.107e-03
 dam       (Intercept) 2.313e-19 4.810e-10
 sire      (Intercept) 1.943e-10 1.394e-05
 dam.1     (Intercept) 2.249e-18 1.500e-09
 sire.1    (Intercept) 7.323e-05 8.558e-03
 Residual              6.299e-03 7.936e-02


> Fixed effects: Estimate Std. Error t value (Intercept)  0.15144
> 0.01336   11.34 matrixF2     0.07083    0.01530    4.63 matrixF3
> -0.03134    0.01525   -2.06 matrixF4     0.04096    0.01529    2.68 
> matrixF5     0.03625    0.01527    2.37 matrixF6     0.03222
> 0.01522    2.12 salt27.5     0.63279    0.01219   51.90 salt30.5
> 0.61029    0.01220   50.01 salt33.5     0.02474    0.01223    2.02 
> salt36.5    -0.17741    0.01228  -14.45
> 
> The experimental design is as follows (with one family missing): 6
> matrices of 9 crosses (3 males x 3 females) yielding 53 families (one
> missing). All families were exposed to 5 different salinities (salt)
> yielding 265 observations. Male/sire and female/dam effects are
> treated as random and matrix and salt as fixed.
> 
>> str(gf)
> 'data.frame': 265 obs. of  16 variables: $ pop    : Factor w/ 1 level
> "Gullmarsfjorden": 1 1 1 1 1 1 1 1 1 1 ... $ date   : Factor w/ 3
> levels "17/07/2011","18/07/2011",..: 1 1 1 1 1 1 1 1 1 1 ... $ matrix
> : Factor w/ 6 levels "F1","F2","F3",..: 1 1 1 1 1 1 1 1 1 1 ... $
> animal : Factor w/ 53 levels "GF01","GF02",..: 1 1 1 1 1 2 2 2 2 2
> ... $ sire   : Factor w/ 18 levels "GS01","GS02",..: 1 1 1 1 1 1 1 1
> 1 1 ... $ dam    : Factor w/ 18 levels "GD01","GD02",..: 1 1 1 1 1 2
> 2 2 2 2 ... $ sal    : num  24.5 27.5 30.5 33.5 36.5 24.5 27.5 30.5
> 33.5 36.5 ... $ cleav  : int  28 45 24 22 0 42 47 33 27 0 ... $ fert
> : int  14 4 3 16 22 14 4 13 14 5 ... $ tot    : int  67 71 57 54 38
> 88 72 78 66 9 ... $ met    : int  14 31 19 6 0 29 38 24 0 0 ... $
> larv   : int  0 6 5 1 0 1 6 14 0 0 ... $ base   : int  42 49 27 38 22
> 56 51 46 41 5 ... $ metsucc: num  0.333 0.633 0.704 0.158 0 ... $
> salt   : Factor w/ 5 levels "24.5","27.5",..: 1 2 3 4 5 1 2 3 4 5
> ... $ baseS  : num  6.48 7 5.2 6.16 4.69 ?
> 
> __________________________________
> 
> Elin Renborg, PhD Student Dept of Biological & Environmental Sciences
> - Tj?rn? University of Gothenburg SE-452 96 Str?mstad, Sweden
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> [[alternative HTML version deleted]]
> 
> 
> 
> _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From robert.espesser at lpl-aix.fr  Sun Jan 12 16:10:30 2014
From: robert.espesser at lpl-aix.fr (espesser)
Date: Sun, 12 Jan 2014 16:10:30 +0100
Subject: [R-sig-ME] bug in confint with boot method and several random terms
	?
Message-ID: <52D2B066.70902@lpl-aix.fr>


Dear all,

Here is a reproducible example of a possible bug concerning 
confint.merMod with method="boot",
when more than a single random term is specified.
The problem does not concern the bootstrapping itself, but the external 
function  bootFun.
Unhappily, I'm not skill enough to fix it.
I did not found previous mention of this , but apologizes if I'm missing 
something.

Thank you for your help

R. espesser



library(lme4)

data(Penicillin)

fm2 <- lmer(diameter ~ 1 + (1|plate) + (1|sample), Penicillin)
summary(fm2)
Linear mixed model fit by REML ['lmerMod']
Formula: diameter ~ 1 + (1 | plate) + (1 | sample)
    Data: Penicillin

REML criterion at convergence: 330.8606

Random effects:
  Groups   Name        Variance Std.Dev.
  plate    (Intercept) 0.7169   0.8467
  sample   (Intercept) 3.7309   1.9316
  Residual             0.3024   0.5499
Number of obs: 144, groups: plate, 24; sample, 6

Fixed effects:
             Estimate Std. Error t value
(Intercept)  22.9722     0.8086   28.41

  confint(fm2, method="boot",nsim=100)
Computing bootstrap confidence intervals ...
Error in setNames(Cv_to_Sv(th, s = sigma(x)), c(tnames(x, old = FALSE,  :
   'names' attribute [3] must be the same length as the vector [2]
In addition: Warning message:
In m0[lower.tri(m0, diag = TRUE)] <- x :
   number of items to replace is not a multiple of replacement length




# It is OK with only one random intercept:

  fm2_single <- lmer(diameter ~ 1 + (1|plate) , Penicillin)

  confint(fm2_single, method="boot",nsim=100)
Computing bootstrap confidence intervals ...
                          2.5 %     97.5 %
sd_(Intercept)|plate  0.000000  0.7437895
sigma                 1.751577  2.1949818
(Intercept)          22.606299 23.3667589


I
##########################
  sessionInfo()
R version 3.0.2 (2013-09-25)
Platform: x86_64-w64-mingw32/x64 (64-bit)

locale:
[1] LC_COLLATE=French_France.1252  LC_CTYPE=French_France.1252    
LC_MONETARY=French_France.1252
[4] LC_NUMERIC=C                   LC_TIME=French_France.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] boot_1.3-9      lme4_1.0-5      Matrix_1.1-0    lattice_0.20-23

loaded via a namespace (and not attached):
  [1] colorspace_1.2-4   dichromat_2.0-0    digest_0.6.4       
ggplot2_0.9.3.1    grid_3.0.2
  [6] gtable_0.1.2       labeling_0.2       MASS_7.3-29        
minqa_1.2.1        munsell_0.4.2
[11] nlme_3.1-111       plyr_1.8           proto_0.3-10       
RColorBrewer_1.0-5 reshape2_1.2.2
[16] scales_0.2.3       splines_3.0.2      stringr_0.6.2      tools_3.0.2
 >


-- 
Robert Espesser
CNRS UMR  7309 - Universit? Aix-Marseille
5 Avenue Pasteur
13100 AIX-EN-PROVENCE

Tel: +33 (0)413 55 36 26


From bbolker at gmail.com  Sun Jan 12 16:39:06 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 12 Jan 2014 10:39:06 -0500
Subject: [R-sig-ME] bug in confint with boot method and several random
 terms ?
In-Reply-To: <52D2B066.70902@lpl-aix.fr>
References: <52D2B066.70902@lpl-aix.fr>
Message-ID: <52D2B71A.1040606@gmail.com>

On 14-01-12 10:10 AM, espesser wrote:
> 
> Dear all,
> 
> Here is a reproducible example of a possible bug concerning
> confint.merMod with method="boot",
> when more than a single random term is specified.
> The problem does not concern the bootstrapping itself, but the external
> function  bootFun.
> Unhappily, I'm not skill enough to fix it.
> I did not found previous mention of this , but apologizes if I'm missing
> something.
> 
> Thank you for your help
> 
> R. espesser
> 

  This was fixed recently (11 days ago) in the development version.  I
will work on putting the latest version (just bumped to 1.1-4) on
http://lme4.r-forge.r-project.org/repos .  In the meantime, if you have
development tools installed (which has gotten to be relatively easy,
even on Windows ...) you can always install the latest version via
library("devtools"); install_github("lme4","lme4")

  We are hoping for another CRAN release fairly soon, but holding your
breath might be dangerous.

  Ben Bolker

> 
> 
> library(lme4)
> 
> data(Penicillin)
> 
> fm2 <- lmer(diameter ~ 1 + (1|plate) + (1|sample), Penicillin)
> summary(fm2)
> Linear mixed model fit by REML ['lmerMod']
> Formula: diameter ~ 1 + (1 | plate) + (1 | sample)
>    Data: Penicillin
> 
> REML criterion at convergence: 330.8606
> 
> Random effects:
>  Groups   Name        Variance Std.Dev.
>  plate    (Intercept) 0.7169   0.8467
>  sample   (Intercept) 3.7309   1.9316
>  Residual             0.3024   0.5499
> Number of obs: 144, groups: plate, 24; sample, 6
> 
> Fixed effects:
>             Estimate Std. Error t value
> (Intercept)  22.9722     0.8086   28.41
> 
>  confint(fm2, method="boot",nsim=100)
> Computing bootstrap confidence intervals ...
> Error in setNames(Cv_to_Sv(th, s = sigma(x)), c(tnames(x, old = FALSE,  :
>   'names' attribute [3] must be the same length as the vector [2]
> In addition: Warning message:
> In m0[lower.tri(m0, diag = TRUE)] <- x :
>   number of items to replace is not a multiple of replacement length
> 
> 
> 
> 
> # It is OK with only one random intercept:
> 
>  fm2_single <- lmer(diameter ~ 1 + (1|plate) , Penicillin)
> 
>  confint(fm2_single, method="boot",nsim=100)
> Computing bootstrap confidence intervals ...
>                          2.5 %     97.5 %
> sd_(Intercept)|plate  0.000000  0.7437895
> sigma                 1.751577  2.1949818
> (Intercept)          22.606299 23.3667589
> 
> 
> I
> ##########################
>  sessionInfo()
> R version 3.0.2 (2013-09-25)
> Platform: x86_64-w64-mingw32/x64 (64-bit)
> 
> locale:
> [1] LC_COLLATE=French_France.1252  LC_CTYPE=French_France.1252   
> LC_MONETARY=French_France.1252
> [4] LC_NUMERIC=C                   LC_TIME=French_France.1252
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> 
> other attached packages:
> [1] boot_1.3-9      lme4_1.0-5      Matrix_1.1-0    lattice_0.20-23
> 
> loaded via a namespace (and not attached):
>  [1] colorspace_1.2-4   dichromat_2.0-0    digest_0.6.4      
> ggplot2_0.9.3.1    grid_3.0.2
>  [6] gtable_0.1.2       labeling_0.2       MASS_7.3-29       
> minqa_1.2.1        munsell_0.4.2
> [11] nlme_3.1-111       plyr_1.8           proto_0.3-10      
> RColorBrewer_1.0-5 reshape2_1.2.2
> [16] scales_0.2.3       splines_3.0.2      stringr_0.6.2      tools_3.0.2
>>
> 
>


From chatfield at alumni.rice.edu  Sun Jan 12 23:30:33 2014
From: chatfield at alumni.rice.edu (Robert Chatfield)
Date: Sun, 12 Jan 2014 14:30:33 -0800
Subject: [R-sig-ME] Formula and Start-values for simplest "non-linear" mixed
	model - nlmer (nlme??, lmer ?)
Message-ID: <701A375D-59B0-467D-B904-AEEE05601F65@alumni.rice.edu>

I'm trying to solve simple mixed-effects problem that seems to require nlmer() from lme4
or nlme() from nlme ...  
(Apologies if this is a duplicate post:  I've checked as best I can...RC)

The problem is simply one with a random x-intercept
when we have have a set of dependent-concentrations that all depend on a single
tracer-concentration,
... but each dependent-concentration (chemical species.type) has a random slope dependence on the tracer-concentration
  (for all measurement instances)
... and each measurement instance has its own random baseline-concentration (at which the tracer-concentrations
also become  0 ... with some small error) 

So the problem is to find the random baseline-concentration for instances (id)  "j"
and the random slopes for each dependent-concentration (species.type)   "i" 
i.e. ... using plotmath expression notation ...
expression(y[ ij ] == a[ i ] *(  x[j] + {x^o}[ j ]  )

Since the a[ i ] must apply to both the x and to x^o , I guess that one must use 
nlmer (or nlme:  I am still confused by the syntax of nlme() )

first.nlmer = nlmer( y ~ a*(x+ x.nought) ~ a|species.type + x.nought | id,
  start=start.vals.1,  data=good.est.merge.long.dat  )

In this case, the y are repeated many times in a "long" list as specified for lmer or nlmer, one time each
for each species.type, so we formally have a long vector as a regression
function of a long vector (n of species.types   times   number of instances


(NOTE: It does not seem to work to look for the usual  y intercepts by running the problem "backwards"
as a multiple regression of a single tracer-variable versus many species.type concentrations
(y[ ij ] == a[ i ] *( + {x^o}[ j ]  
x[j]  = alpha(i) * y[ij] + x^o[j]  
since there is an "exact" but trivial solution x^0[j] = x[j]  ... 

Two questions ... 
======================================================================================
Mostly: (1)
I could not convince nlmer that the the starting values were vectors ... perhaps this is
elementary R
start.vals.1 = c(a=aij, x.nought=x.nought.lin)    (SEE VALUES BELOW)
#   start.vals.1 = cbind(a=aij, x.nought=x.nought.lin)  # tried this -- does not work

where this calculation for aij
len.lon.dat = dim(good.est.merge.long.dat)[1]   #  repeats once for each species.type
len.lon.dat = dim(good.est.merge.long.dat)[1]   #  repeats once for each species.type 

aij = vector(mode="numeric",length= len.lon.dat)
#  The name aij is given since
for ( i in 1:len.lon.dat ) {
spec.type.nm = as.character(species.type[i])
aij[i] = (ranef(try.spec.2.lmer,drop=T)[["species.type"]])[[spec.type.nm]]
}

I am wondering if the names are not being passed along correctly ...
> names(start.vals.1)[1]
[1] "a1"
> start.vals.1[1]
       a1 
0.002266363 
> start.vals.1[1802]
    a1802 
-0.02694557 
> start.vals.1[1803]
x.nought1 
-0.3141149 
> start.vals.1[3604]
x.nought1802 
-0.1204595 
Could this naming be confusing to nlmer?  The start values must be named.

Now I have made a similar lmer() formula work ... unfortunately it gives only y-intercepts
and there are somewhat unsatisfactory approximations to make this give me x-intercepts
======================================================================================
ALSO (2) 
Is this the correct last portion of the three-part formula (there are no examples)
~ a|species.type + x.nought | id


*** (I am happy to use nlme() if someone could help me with the syntax, or tell me how
to formulate this as a linear model with imposed conditions.

Computer details:  Two Macintoshes running OS X 10.5.8 
"R version 2.15.3 (2013-03-01)"
R 2.15.3 GUI   and most recent nlmer from lme4 0.999999-0

THANKS TO ALL ..

Robert Chatfield
Earth Science Division
NASA Ames Research Center  650-604-5490
======================================================================================
POSTSCRIPT ... to convince you that the basic approach works with lmer for linear formulas
(PS: the somewhat unsatisfactory approximation that depends on normalizing all the 
y and doing a bit of geometery
ttry.spec.2.lmer = lmer( y ~ x  + (x- 1 | species.type) + ( 1 | id) + 1 , data=good.est.merge.long.dat )

This solves just fine 
here
id  is the sample or "instance" id , goes  for j = 1:106
species.type is a factor construct that describes each species
  (in detail species.type describes each species for each burning-type ... for example, Flaming or Smoldering or ..
   but this detail is not important ... just treat this as if we had 68 species.type categories  .... i= 1::68

This gives some y intercepts (not really wanted) from which I obtain some
very approximate x intercepts  ... this is not fully described ...
x_approx = x_first_guess + 
       ( fixef(try.spec.2.lmer)[["(Intercept)"]]   + ranef(try.spec.2.lmer,drop=T)$id ) / ( fixef(try.spec.2.lmer)[["x"]]  )
======================================================================================


From bbolker at gmail.com  Mon Jan 13 00:59:34 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 12 Jan 2014 18:59:34 -0500
Subject: [R-sig-ME] Error in lme4 rank of X = 28 < ncol(X) = 29
In-Reply-To: <3C1397953420394CA1E185D115FC2E290C50A0@uqexmdb6.soe.uq.edu.au>
References: <3C1397953420394CA1E185D115FC2E290C460E@uqexmdb6.soe.uq.edu.au>
	<52C8F29F.5050404@lists.r-forge.r-project.org>
	<3C1397953420394CA1E185D115FC2E290C50A0@uqexmdb6.soe.uq.edu.au>
Message-ID: <52D32C66.407@gmail.com>

On 14-01-12 06:47 PM, Michael Williamson wrote:
> Good Morning,
> 
> I spoke you recently concerning a problem with a rank deficient error
> when running a glmer model (see below)
> 
> My boss wants me to try and run a model working out rates using a
> glmmadbd model and I'm getting the same rank deficient problem -
> SABmod<-glmmadmb(SABRate~Socialbeh+Compcat+DistancePodCat+

DistanceSingerCat+Pods15Cat+SingersCat+Windspeed.km.h.+BoatPhs,random=~1|FocalID,family="nbinom1",
> zeroInflation=TRUE, data=Behav)
> 
> Error in glmmadmb(SABRate ~ Socialbeh + Compcat + DistancePodCat +
> DistanceSingerCat +  : rank of X = 28 < ncol(X) = 29
> 
> An updated version of lme4 seemed to fix the previous model and it
> now takes into account rank deficiencies and I was wondering if there
> was a version on glmmadmb package that may do this also. I've tried
> to reinstall the newest version but doesn't appear to work.

  No, there isn't -- glmmADMB hasn't implemented this feature (yet).
You're welcome to look at the chkRank.drop.cols() function in
https://github.com/lme4/lme4/blob/master/R/modular.R to see how lme4
does it ... one way or the other, though, you're going to have to drop a
column.  You can use qr() or svd() to figure out which columns are
multicollinear ...

  I'm taking the liberty of cc'ing this back to r-sig-mixed-models.

 cheers
   Ben Bolker



> 
> Thanks for your time
> 
> Mike Williamson
> 
> -----Original Message----- From: lme4 maintainer
> [mailto:bbolker at gmail.com] Sent: Sunday, 5 January 2014 3:50 PM To:
> Michael Williamson;
> "lme4-authors at lists.r-forge.r-project.org"@newmailhub.uq.edu.au 
> Subject: Re: Error in lme4 rank of X = 28 < ncol(X) = 29
> 
> On 14-01-02 05:32 PM, Michael Williamson wrote:
>> Good Morning,
>> 
>> My name is Mike Williamson and I am a research assistant for the 
>> University of Queensland. I'm in the process of running some
>> analyses with R on a dataset I have been working on in
>> collaboration with a PhD student. Just before Christmas she sent me
>> some code for me to use on my dataset. I started upon it yesterday
>> but was consistently coming up with the error below
>> 
>>> SABMod<- 
>>> glmer(SABinom~Socialbeh+Compcat+DistancePodCat+DistanceSingerCat+Pods
>>>
>>> 
15+
> SingersCat+Windspeed.km.h.+BoatPhs+(1|FocalID),data=BehavTr,family="bino
>
> 
SingersCat+mial")
>> 
>>> 
>> Error in lme4::glFormula(formula = SABinom ~ Socialbeh + Compcat +
>>  DistancePodCat +  :
>> 
>> rank of X = 29 < ncol(X) = 31
>> 
>> Sadly she is completely out of contact on field work for the next 
>> 2-3 weeks and I'm pretty stuck on this so apologies for the email
>> but I was wondering if might be able to help.
>> 
>> I'm working on the whether the relationship of boat phase (before 
>> approach attempt after) affects the rates of binomial data of
>> surface active behaviours of whales (SABinom).  The predictor
>> variables are something she has been working on and told me to
>> include in my model.
>> 
>> From the bit of research I've done it looks like the error says my
>>  data is rank deficient. Now I think this is most likely because of
>> my low samples for my approach and attempt but I can't find
>> anywhere online or in papers (such as Zuul 2009) to confirm this or
>> find an option for low sample sizes. I've attached the sample sizes
>> for presence absence of surface active behaviours for boat phase
>> below. As you can see approach and attempt only have 5 and 15
>> between. This is due to the approach and attempt phases being
>> shorter periods than before and after. Is low sample size the
>> problem here? And if so do you have any suggestions for how I could
>> go about this, or any books to read. I've been trawling the
>> libraries online and don't seem to be able find anything.
> 
> Sorry to take so long to get back to you.  Your data are indeed
> rank-deficient; the error is telling you that you are trying to
> estimate 31 fixed-effect parameters (columns of the X matrix), but
> that there are only 29 linearly independent combinations of predictor
> variables in your data set.  Small sample size is not an inherent
> problem, but it is generally more likely (with an unbalanced data
> set) that you will end up with a rank-deficient problem.
> 
> I was hoping I had written more somewhere else in the past about how
> to use model.matrix() and svd() to diagnose multicollinearity
> problems --
> https://stat.ethz.ch/pipermail/r-sig-mixed-models/2012q4/019499.html
> isn't very complete (I should add it to the FAQ).
> 
> The fact that this table is badly unbalanced doesn't automatically
> mean you have a problem, but in combination with some of your other
> variables it is more likely to make the problem unidentifiable.
> 
>> 0    1
>> 
>> After    7445 1015 Approach  351    5 Attempt   991   15 Before
>> 2079  344
> 
> If you can install a recent development version of lme4 (e.g.
> 
> install.packages("lme4",repos="http://lme4.r-forge.r-project.org/repos"),
>
> 
that may help -- the current development version automatically tries to
adjust the fixed-effect model matrix to get rid of rank deficiency.
> 
> Ben Bolker
>


From chatfield at alumni.rice.edu  Mon Jan 13 18:32:44 2014
From: chatfield at alumni.rice.edu (Robert Chatfield)
Date: Mon, 13 Jan 2014 09:32:44 -0800
Subject: [R-sig-ME]
 =?windows-1252?q?Formula_and_Start-values_=85__nlmer_?=
 =?windows-1252?q?=28actual_error_messages=29?=
In-Reply-To: <mailman.0.1389610802.32367.r-sig-mixed-models@r-project.org>
References: <mailman.0.1389610802.32367.r-sig-mixed-models@r-project.org>
Message-ID: <ED374FC2-D99D-4D7A-80EE-E0D1F7E3EC87@alumni.rice.edu>

Hmm:  I should have included the actual error message I received from nlmer
at about the line "Could this naming be confusing to nlmer?  The start values must be named."

> first.nlmer = nlmer( y ~ a*(x+ x.nought) ~ (a|species.type) + (x.nought|id),
+      start=start.vals.1,  data=good.est.merge.long.dat  )
Error in nlmer(y ~ a * (x + x.nought) ~ (a | species.type) + (x.nought |  : 
  not all parameter names are used in the nonlinear model expression

where named list is given by ...
> start.vals.1 = c(a=aij, x.nought=C.nought.lin)
> start.vals.1[1]
         a1 
0.002266363 
> start.vals.1[3604]
x.nought1802 
  -0.1204595 

Using single values with a single name also does not work, nor does a construct
with cbind:

> first.nlmer = nlmer( y ~ a*(x+ x.nought) ~ (a|species.type) + (x.nought|id),
+      start=c(a=0.00364901,x.nought=0),  data=good.est.merge.long.dat  )
Error in nlmer(y ~ a * (x + x.nought) ~ (a | species.type) + (x.nought |  : 
  gradient attribute of evaluated model must be a numeric matrix

Bob Chatfield

From sumayia.bashir at stir.ac.uk  Sat Jan 11 23:56:53 2014
From: sumayia.bashir at stir.ac.uk (Sumayia Bashir)
Date: Sat, 11 Jan 2014 22:56:53 +0000
Subject: [R-sig-ME] Need help with MCMCglmm
Message-ID: <82632B9B-BE4F-4451-A284-52EFD1791B4B@stir.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140111/a2feabb7/attachment.pl>

From holtermann at hwwi.org  Tue Jan 14 15:31:46 2014
From: holtermann at hwwi.org (Linus Holtermann)
Date: Tue, 14 Jan 2014 15:31:46 +0100
Subject: [R-sig-ME] MCMCglmm: Prior for Sir function
Message-ID: <AD0050057515F54084E7D5B93478C8481F71A0195C@winxbede39.exchange.xchg>

Hello,

i am working with the wonderful MCMCglmm package in R. My models are hierarchical multilevel-models (two levels).
I incorporate the sir-function in my model and it works fine. The estimation results are reliable but a warning appears "Prior for sir not defined". How can i manage to specify a Prior for the sir function?


My second question is regarding the p-values for random effects in MCMC Setting. When estimating multilevel-models with ML Wald-Tests are often used to test if a random effect is significant. In a MCMC framework can I use p-values that are calculate in the same way as those for the fixed effects?


Mit freundlichen Gr??en
 
 
Linus Holtermann
Hamburgisches WeltWirtschaftsInstitut gemeinn?tzige GmbH (HWWI)
Heimhuder Stra?e 71
20148 Hamburg
Tel +49-(0)40-340576-336
Fax+49-(0)40-340576-776
Internet: www.hwwi.org
Email: holtermann at hwwi.org
 
AmtsgerichtHamburg HRB 94303
Gesch?ftsf?hrer: Prof. Dr. Thomas Straubhaar, Gunnar Geyer
Umsatzsteuer-ID: DE 241849425

From agnes.schneider at gmx.de  Tue Jan 14 17:24:00 2014
From: agnes.schneider at gmx.de (Agnes Schneider)
Date: Tue, 14 Jan 2014 17:24:00 +0100
Subject: [R-sig-ME] Generalized Linear Models
Message-ID: <8BDA5FB7-B354-4D55-AF69-1C3E229F1C66@gmx.de>

Hi,

I'm carrying out an analysis on future time expressions in English on the basis of a corpora of spoken language. I am using a linear mixed effects model (glmer) because when I coded the data I realized that there is considerable variability within each conversation concerning the choice of future markers. So I have a model consisting of a categorical dependent variable (Future time marker WILL or BE GOING TO), a number of fixed effects (syntactic, semantic and extralinguistic variables), an a random effect which is File. Not all of my independent variables show a significant effect on the choice of future marker. My questions now are:

1. Is the procedure at arriving at a minimal adequate model the same as for logistic regression models (glm)?
2. How do I find out whether there is reason to assume overdispersion?
3. How do I find out whether my models (both the initial and the final model) have predictive power?
4. How do I determine whether interspeaker variability (File) is stronger than the fixed effects?

I am grateful for any comment on my questions!!
Thanks
Agnes


From bbolker at gmail.com  Tue Jan 14 19:55:23 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 14 Jan 2014 13:55:23 -0500
Subject: [R-sig-ME] Generalized Linear Models
In-Reply-To: <8BDA5FB7-B354-4D55-AF69-1C3E229F1C66@gmx.de>
References: <8BDA5FB7-B354-4D55-AF69-1C3E229F1C66@gmx.de>
Message-ID: <52D5881B.6010904@gmail.com>

On 14-01-14 11:24 AM, Agnes Schneider wrote:
> Hi,
> 
> I'm carrying out an analysis on future time expressions in English on
> the basis of a corpora of spoken language. I am using a linear mixed
> effects model (glmer) because when I coded the data I realized that
> there is considerable variability within each conversation concerning
> the choice of future markers. So I have a model consisting of a
> categorical dependent variable (Future time marker WILL or BE GOING
> TO), a number of fixed effects (syntactic, semantic and
> extralinguistic variables), an a random effect which is File. Not all
> of my independent variables show a significant effect on the choice
> of future marker. My questions now are:
> 
> 1. Is the procedure at arriving at a minimal adequate model the same
> as for logistic regression models (glm)? 

  More or less, although there is some debate as to whether one should
try to discard non-significant random-effects terms or not: see Barr et
al 2013 (ref below), and whether one should consider fixed or random
effects first (I am personally uneasy with the concept of "minimal
adequate models" for confirmatory testing in the first place).

2. How do I find out whether
> there is reason to assume overdispersion? 

  See http://glmm.wikidot.com/faq#overdispersion_est and following (note
that overdispersion is unidentifiable in the case of binary data with
unique predictors, and already taken into account in models with
estimated scale parameters [Gaussian, gamma, etc.])

3. How do I find out
> whether my models (both the initial and the final model) have
> predictive power? 

  Don't know exactly what you mean here.  You could look at
http://glmm.wikidot.com/faq#rsquared

4. How do I determine whether interspeaker
> variability (File) is stronger than the fixed effects?

  As a first cut, comparing the magnitude of the standard deviation
estimate to the size of the fixed effects should do (assuming that the
fixed effect predictors are appropriately scaled).  Beyond that, it
would depend exactly what you mean.

> 
> I am grateful for any comment on my questions!! Thanks Agnes
> 
> _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 


@article{barr_random_2013,
	title = {Random effects structure for confirmatory hypothesis testing:
Keep it maximal},
	volume = {68},
	issn = {{0749596X}},
	shorttitle = {Random effects structure for confirmatory hypothesis
testing},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X12001180},
	doi = {10.1016/j.jml.2012.11.001},
	number = {3},
	urldate = {2013-06-04},
	journal = {Journal of Memory and Language},
	author = {Barr, Dale J. and Levy, Roger and Scheepers, Christoph and
Tily, Harry J.},
	month = apr,
	year = {2013},
	pages = {255--278},
}


From jersa at centrum.cz  Wed Jan 15 10:43:47 2014
From: jersa at centrum.cz (jersa at centrum.cz)
Date: Wed, 15 Jan 2014 10:43:47 +0100
Subject: [R-sig-ME] =?utf-8?q?post-hoc_comparison_on_interaction_term_in_l?=
	=?utf-8?q?me_using_contrasts?=
In-Reply-To: <20140107112455.AF49D5A7@centrum.cz>
References: <20140107112455.AF49D5A7@centrum.cz>
Message-ID: <20140115104347.A41989F3@centrum.cz>

Dear R experts,

I have a significant interaction term in my lme model and I was searching for a way, how to perform post hoc test.
I was told that direct Tukey test of interaction using glht within lme gives unreliable results and should be avoided.
I have searched for solutions and found out only a recomendation to build up a contrast matrix using function contrast.

I have 7 plant species and 4 treatments which significantly interact and I used folowing syntax to build the matrix

> cm<-contrast(MY MODEL, a=list(species=c("A","B","C","D","E","F","G"), treat=c("A","B","C")),
b=list( species=c("A","B","C","D","E","F","G"), treat=c("D","D","D")))

> cmtrx <- cm$X
> ttgl<-glht( MY MODEL,lin=cmtrx)
> confint(ttgl)

the outcome is 21 rows numbered from 1 to 21 (corresponds to 7 species * 3 treatments), and it is not clear to me, how the combinations are ordered?
i.e. 1 == 0  is for species A : treat A against species A : treat D
2 == 0   is for species A : treat B against species A : treat D

the outcome is estimated values plus confidence intervals such as
               Estimate       lwr            upr       
1 == 0  -0.2935212 -0.4847410 -0.1023014
2 == 0  -0.4448065 -0.6360263 -0.2535867
..........

I suppose this needs to be further digested by some function from multcomp package to estimate significance of these tests.
I tried to extend ttgl to ttgl<-glht( MY MODEL,lin=cmtrx(tension = "Tukey")) but that does not work.
summary(ttgl)$test$pvalues also has not yield what I need.
Can you please advise me how to go on?

Or is there easier way how to deal with interactions in lme?

Thank you very much for any help.
Best from Jana


From russell-lenth at uiowa.edu  Wed Jan 15 15:30:35 2014
From: russell-lenth at uiowa.edu (Lenth, Russell V)
Date: Wed, 15 Jan 2014 14:30:35 +0000
Subject: [R-sig-ME] post-hoc comparison on interaction term in lme,
 using contrasts
References: <mailman.3.1389783604.4619.r-sig-mixed-models@r-project.org>
Message-ID: <51F0C7C54B032A42A23B74A088E7141C1ED5CEEE@itsnt443.iowa.uiowa.edu>

The lsmeans package might be helpful here.

You can visualize the predictions via an interaction-plot:

    library(lsmeans)
    lsmip(MY_MODEL, treat ~ species)

It looks like you want the treatment comparisons A-D, B-D, C-D for each
species -- is that right? If so, it can be done using

    lsmeans(MY_MODEL, trt.vs.ctrl ~ treat | species, ref = 4)

Russ

Russell V. Lenth  -  Professor Emeritus
Department of Statistics and Actuarial Science   
The University of Iowa  -  Iowa City, IA 52242  USA   
Voice (319)335-0712 (Dept. office)  -  FAX (319)335-3017

On 1/15/2014 5:02 AM, r-sig-mixed-models-request at r-project.org wrote:
> Dear R experts,
>
> I have a significant interaction term in my lme model and I was searching for a way, how to perform post hoc test.
> I was told that direct Tukey test of interaction using glht within lme gives unreliable results and should be avoided.
> I have searched for solutions and found out only a recomendation to build up a contrast matrix using function contrast.
>
> I have 7 plant species and 4 treatments which significantly interact and I used folowing syntax to build the matrix
>
>> > cm<-contrast(MY MODEL, a=list(species=c("A","B","C","D","E","F","G"), treat=c("A","B","C")),
> b=list( species=c("A","B","C","D","E","F","G"), treat=c("D","D","D")))
>
>> > cmtrx <- cm$X
>> > ttgl<-glht( MY MODEL,lin=cmtrx)
>> > confint(ttgl)
> the outcome is 21 rows numbered from 1 to 21 (corresponds to 7 species * 3 treatments), and it is not clear to me, how the combinations are ordered?
> i.e. 1 == 0  is for species A : treat A against species A : treat D
> 2 == 0   is for species A : treat B against species A : treat D
>
> the outcome is estimated values plus confidence intervals such as
>                Estimate       lwr            upr       
> 1 == 0  -0.2935212 -0.4847410 -0.1023014
> 2 == 0  -0.4448065 -0.6360263 -0.2535867
> ..........
>
> I suppose this needs to be further digested by some function from multcomp package to estimate significance of these tests.
> I tried to extend ttgl to ttgl<-glht( MY MODEL,lin=cmtrx(tension = "Tukey")) but that does not work.
> summary(ttgl)$test$pvalues also has not yield what I need.
> Can you please advise me how to go on?
>
> Or is there easier way how to deal with interactions in lme?
>
> Thank you very much for any help.
> Best from Jana



From deter088 at umn.edu  Wed Jan 15 21:27:01 2014
From: deter088 at umn.edu (Charles Determan Jr)
Date: Wed, 15 Jan 2014 14:27:01 -0600
Subject: [R-sig-ME] Replicating SAS results in R with unstructured
	covariance matrix
In-Reply-To: <CAF-NzEpZjfV06_D5QnJT7oG3Yi=JB6sDxPf8rTiQO0CAyMRjdg@mail.gmail.com>
References: <CAF-NzEpZjfV06_D5QnJT7oG3Yi=JB6sDxPf8rTiQO0CAyMRjdg@mail.gmail.com>
Message-ID: <CAOLJphnwHeskHY7NUsBgA4AAwNBALR=Vgt4+xK7AnG7bmAAH0A@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140115/0c099c0a/attachment.pl>

From agnes.schneider at gmx.de  Wed Jan 15 22:34:16 2014
From: agnes.schneider at gmx.de (Agnes Schneider)
Date: Wed, 15 Jan 2014 22:34:16 +0100
Subject: [R-sig-ME] Generalized Linear Models
In-Reply-To: <52D5881B.6010904@gmail.com>
References: <8BDA5FB7-B354-4D55-AF69-1C3E229F1C66@gmx.de>
	<52D5881B.6010904@gmail.com>
Message-ID: <B7730505-46B9-4C9A-849C-D485DE8CD730@gmx.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140115/4a4309f7/attachment.pl>

From wolfgang.viechtbauer at maastrichtuniversity.nl  Thu Jan 16 00:14:30 2014
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (STAT))
Date: Thu, 16 Jan 2014 00:14:30 +0100
Subject: [R-sig-ME] Replicating SAS results in R with unstructured
 covariance matrix
Message-ID: <077E31A57DA26E46AB0D493C9966AC730D9D035316@UM-MAIL4112.unimaas.nl>

It's been a while since I used SAS for mixed-effects model, but I think the model:

PROC MIXED data=Comb method=reml;
  CLASS FAC1 TIME SUBJECT FAC2;
  MODEL RESPONSE = BIAS FAC2 FAC1 TIME TIME*FAC1 / ddfm=Residual solution;
  REPEATED TIME / subject=SUBJECT type=un;
RUN;

can be fit with gls() using:

gls(RESPONSE ~ BIAS + factor(FAC2) + factor(TIME)* factor(FAC1), correlation = corSymm(form = ~ TIME | SUBJECT), weights = varIdent(form = ~ 1 | TIME), data=Comb)

TIME needs to be integer valued. SUBJECT should be a factor. ddfm=Residual implies that the fixed effects are tested with t-tests using df = n-rank(X) (where X is the model matrix), while gls() uses the normal distribution to compute those p-values. But this is a minor issue, except in those pesky borderline cases, but then again neither that t-distribution nor that normal distribution is exactly right and one should withhold judgement anyway.

It would be nice if you could provide feedback whether the syntax above did the trick.

Best,
Wolfgang

--   
Wolfgang Viechtbauer, Ph.D., Statistician   
Department of Psychiatry and Psychology   
School for Mental Health and Neuroscience   
Faculty of Health, Medicine, and Life Sciences   
Maastricht University, P.O. Box 616 (VIJV1)   
6200 MD Maastricht, The Netherlands   
+31 (43) 388-4170 | http://www.wvbauer.com   


> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-
> bounces at r-project.org] On Behalf Of Charles Determan Jr
> Sent: Wednesday, January 15, 2014 21:27
> To: Tony K.-T.
> Cc: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Replicating SAS results in R with unstructured
> covariance matrix
> 
> Hi Tony,
> 
> Unfortunately I never did solve this problem.  I experimented endlessly
> with all sorts of combinations of correlation structures available with
> lme.  Unless someone reads this will a wonderful solution to replicate the
> unstructure structure analysis from SAS I can't help.  The compound
> symmetry correlation structure was the only that really matched up well.
> I'm sorry I couldn't be more help.  I ended up moving on to using other
> multivariate analyses like PLSDA and Random Forest because of the
> seemingly
> incompatible analyses.  I wanted to make sure anyone could replicate my
> analysis independent of the statistical program.  I poke at it
> occasionally
> but still nothing for the unstructured analysis.  If I ever do I will be
> sure to post it.
> 
> Wish you the best in you analysis,
> Charles
> 
> 
> On Wed, Jan 15, 2014 at 1:44 PM, Tony K.-T. <tkamth at gmail.com> wrote:
> 
> > Dear Charles,
> >
> > I have seen your post regarding replicating the results of SAS (Proc
> > Mixed) using unstructured covariance matrix using R nlme package (gls).
> >  Have you been able to solve this problem?
> >
> > I am also in the process of doing so on a repeated measures dataset to
> no
> > success (not converging identically, estimates are off), if you can
> spare
> > some thoughts on this it'll be really great.
> >
> > The model that I am trying to replicate from SAS to R.
> >
> > PROC MIXED data=Comb method=reml;
> >   CLASS FAC1 TIME SUBJECT FAC2;
> >   MODEL RESPONSE = BIAS FAC2 FAC1 TIME TIME*FAC1 / ddfm=Residual
> solution;
> >   REPEATED  TIME/ subject=SUBJECT type=un;
> > RUN;
> >
> > Cheers,
> >
> > --
> > Tony Kam-Thong, Dr.-Ing.
> > tel.:+41 78 604 5172
> > e-mail:tkamth at gmail.com
> >
> 
> 
> 
> --
> Charles Determan
> Integrated Biosciences PhD Candidate
> University of Minnesota
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From jersa at centrum.cz  Thu Jan 16 09:24:29 2014
From: jersa at centrum.cz (jersa at centrum.cz)
Date: Thu, 16 Jan 2014 09:24:29 +0100
Subject: [R-sig-ME]
	=?utf-8?q?post-hoc_comparison_on_interaction_term_in_l?=
	=?utf-8?q?me=2C_using_contrasts?=
In-Reply-To: <51F0C7C54B032A42A23B74A088E7141C1ED5CEEE@itsnt443.iowa.uiowa.edu>
References: <mailman.3.1389783604.4619.r-sig-mixed-models@r-project.org>
	<51F0C7C54B032A42A23B74A088E7141C1ED5CEEE@itsnt443.iowa.uiowa.edu>
Message-ID: <20140116092429.3E7D99A5@centrum.cz>

Dear Russ, that was very helpful! Works perfect. Thank you very much. Best from Jana

______________________________________________________________
> Od: "Lenth, Russell V" <russell-lenth at uiowa.edu>
> Komu: "r-sig-mixed-models at r-project.org" <r-sig-mixed-models at r-project.org>
> Datum: 15.01.2014 15:30
> P?edm?t: Re: [R-sig-ME] post-hoc comparison on interaction term in lme, using contrasts
>
>The lsmeans package might be helpful here.
>
>You can visualize the predictions via an interaction-plot:
>
>    library(lsmeans)
>    lsmip(MY_MODEL, treat ~ species)
>
>It looks like you want the treatment comparisons A-D, B-D, C-D for each
>species -- is that right? If so, it can be done using
>
>    lsmeans(MY_MODEL, trt.vs.ctrl ~ treat | species, ref = 4)
>
>Russ
>
>Russell V. Lenth  -  Professor Emeritus
>Department of Statistics and Actuarial Science   
>The University of Iowa  -  Iowa City, IA 52242  USA   
>Voice (319)335-0712 (Dept. office)  -  FAX (319)335-3017
>
>On 1/15/2014 5:02 AM, r-sig-mixed-models-request at r-project.org wrote:
>> Dear R experts,
>>
>> I have a significant interaction term in my lme model and I was searching for a way, how to perform post hoc test.
>> I was told that direct Tukey test of interaction using glht within lme gives unreliable results and should be avoided.
>> I have searched for solutions and found out only a recomendation to build up a contrast matrix using function contrast.
>>
>> I have 7 plant species and 4 treatments which significantly interact and I used folowing syntax to build the matrix
>>
>>> > cm<-contrast(MY MODEL, a=list(species=c("A","B","C","D","E","F","G"), treat=c("A","B","C")),
>> b=list( species=c("A","B","C","D","E","F","G"), treat=c("D","D","D")))
>>
>>> > cmtrx <- cm$X
>>> > ttgl<-glht( MY MODEL,lin=cmtrx)
>>> > confint(ttgl)
>> the outcome is 21 rows numbered from 1 to 21 (corresponds to 7 species * 3 treatments), and it is not clear to me, how the combinations are ordered?
>> i.e. 1 == 0  is for species A : treat A against species A : treat D
>> 2 == 0   is for species A : treat B against species A : treat D
>>
>> the outcome is estimated values plus confidence intervals such as
>>                Estimate       lwr            upr       
>> 1 == 0  -0.2935212 -0.4847410 -0.1023014
>> 2 == 0  -0.4448065 -0.6360263 -0.2535867
>> ..........
>>
>> I suppose this needs to be further digested by some function from multcomp package to estimate significance of these tests.
>> I tried to extend ttgl to ttgl<-glht( MY MODEL,lin=cmtrx(tension = "Tukey")) but that does not work.
>> summary(ttgl)$test$pvalues also has not yield what I need.
>> Can you please advise me how to go on?
>>
>> Or is there easier way how to deal with interactions in lme?
>>
>> Thank you very much for any help.
>> Best from Jana
>
>


From holtermann at hwwi.org  Thu Jan 16 13:44:08 2014
From: holtermann at hwwi.org (Linus Holtermann)
Date: Thu, 16 Jan 2014 13:44:08 +0100
Subject: [R-sig-ME] RESET Test and Heteroscedasticity Test for Hierarchical
 Multilevel Model
Message-ID: <AD0050057515F54084E7D5B93478C8481F71A0196A@winxbede39.exchange.xchg>

Hello together,

Are there implemented tests in R to check heteroscedasticity and linearity for multilevel regressions? I am using a two Level Regression Framework (lmer).
Is it  necessary to do those tests if i use a MCMC Approach (MCMCglmm)? How do i perform those test when using MCMC to estimate multilevel regressions (gaussian distribution)?
Thanks in advance

Best regards,

 
Linus Holtermann
Hamburgisches WeltWirtschaftsInstitut gemeinn?tzige GmbH (HWWI)
Heimhuder Stra?e 71
20148 Hamburg
Tel +49-(0)40-340576-336
Fax+49-(0)40-340576-776
Internet: www.hwwi.org
Email: holtermann at hwwi.org
 
AmtsgerichtHamburg HRB 94303
Gesch?ftsf?hrer: Prof. Dr. Thomas Straubhaar, Gunnar Geyer
Umsatzsteuer-ID: DE 241849425

From holtermann at hwwi.org  Thu Jan 16 14:11:42 2014
From: holtermann at hwwi.org (Linus Holtermann)
Date: Thu, 16 Jan 2014 14:11:42 +0100
Subject: [R-sig-ME] Small data set and validity of Multilevel Regression
	results
Message-ID: <AD0050057515F54084E7D5B93478C8481F71A0196B@winxbede39.exchange.xchg>

Hello,

I want to estimate a hierarchical multilevel regression (two levels). My data set is fairly small: 25 groups and 255 individual observations distributed unequally among groups (group member size varies between 4 and 55). Several books point out that the small sample size could lead to biased estimates of the random effects. There was no clear conclusion which reasons are responsible for the bias. 
My question is as follows: which of the statements are true?
-  Adding (more) regressors to Level 1 (individuals) will reduce the validity of the estimation results
- Adding (more) cross-level intaraction effects will reduce the validity of the estimation results
- Adding (more) random slopes will reduce the validity of the estimation results

Could a MCMC approach help in my example? 

Thanks in advance, 



 
Linus Holtermann
Hamburgisches WeltWirtschaftsInstitut gemeinn?tzige GmbH (HWWI)
Heimhuder Stra?e 71
20148 Hamburg
Tel +49-(0)40-340576-336
Fax+49-(0)40-340576-776
Internet: www.hwwi.org
Email: holtermann at hwwi.org
 
AmtsgerichtHamburg HRB 94303
Gesch?ftsf?hrer: Prof. Dr. Thomas Straubhaar, Gunnar Geyer
Umsatzsteuer-ID: DE 241849425

From bbolker at gmail.com  Thu Jan 16 18:14:08 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 16 Jan 2014 12:14:08 -0500
Subject: [R-sig-ME] lmer Problem
In-Reply-To: <52D7FF97.40907@utoronto.ca>
References: <52D7FF97.40907@utoronto.ca>
Message-ID: <52D81360.6000908@gmail.com>

  I'm taking the liberty of cc'ing this back to r-sig-mixed-models,
**without** the data, because I think the answer will be of general
interest.  You're not doing anything wrong, and I don't think there's
anything pathological about your data -- there's just not quite enough
of it, or it's a little too noisy.  This collapse of the correlation to
+/- 1 is analogous to an estimate of zero variance for a random effect,
just more confusing.  Some of your choices:

  * leave the model as is but treat with caution
  * force slope-intercept correlation to zero by using (1|pid) +
(0+Actual|pid) -- **HOWEVER** as pointed out previously on this list
this is really just a reparameterization of the model, and not
necessarily the best idea
  * drop the slope:group random effect (i.e. just use (1|pid))
  * use blme::blmer to impose a weak prior on the variance-covariance matrix
  * get more data (ha!)


Here's what I did to explore the issue:

####
library(Hmisc)
library(lme4)

load("thorpe_example.RData")

## devel version gives warnings about max|grad| >0.001, but
## this is a false positive: the fit is singular, and we shouldn't
## be checking the gradient under those conditions, or at least
## should be checking it more carefully with respect to the boundary
## (the gradient _parallel_ to the boundary should be zero ...)

summary(dth.fit1 <- lmer(Estimate~Actual+(Actual|pid),data=dth))

pp <- profile(dth.fit1)
library(lattice)
xyplot(pp)

library(ggplot2)
theme_set(theme_bw())
## ggplot complains about 'labelled' objects
dth2 <- transform(subset(dth,select=c(Actual,Estimate,pid)),
                  pid=factor(pid),
                  Actual=as.numeric(Actual),
                  Estimate=as.numeric(Estimate))
ggplot(dth2,aes(Actual,Estimate,colour=pid))+geom_point()+
    geom_smooth(se=FALSE,method="lm")+theme(legend.position="none")

## switching optimizers doesn't help (because we got the right
## answer in the first place)
dth.fit2 <- update(dth.fit1,control=lmerControl(optimizer="Nelder_Mead"))

## centering the predictor doesn't help
dth.fit3 <- update(dth.fit1,
                   data=transform(dth,Actual=Actual-mean(Actual)))


th <- getME(dth.fit1,"theta")
## should th[3] really be exactly zero?
lme4:::checkConv(dth.fit1 at optinfo$derivs,getME(dth.fit1,"theta"),
                 lbound=getME(dth.fit1,"lower"))
## right now check.conv.singular="ignore", but we shouldn't
## be testing gradients and Hessians when we have a singular fit
## (this would be classified as a singular fit since the default
## tolerance for singularity is 1e-4)

## compute and display slice
library(bbmle)
dd <- update(dth.fit1,devFunOnly=TRUE)
ss <- slice2D(getME(dth.fit1,"theta"),dd)
splom(ss)

## plot random effects
dotplot(ranef(dth.fit1,condVar=TRUE),
              scales = list(x = list(relation = 'free')))[["pid"]]

## fit with blme

library(blme)
summary(dth.fit4 <- blmer(Estimate~Actual+(Actual|pid),data=dth))
VarCorr(dth.fit4)


On 14-01-16 10:49 AM, Kevin E. Thorpe wrote:
> Dear Ben.
> 
> My apologies for sending this directly to you off-list, but given that I
> am sending you data I felt it was safest.
> 
> My problem is that in the attached analysis I am getting a correlation
> between the intercept and slope random effects of exactly 1.00.  I
> thought that this might just be rounding in the output, but I think not,
> given the attached caterpillar plot.
> 
> Either I am doing something wrong, or I have yet another pathological
> data set.
> 
> I have included sessionInfo in my output (the Rout file).
> 
> If you could take a look and offer any suggestions, I would be most
> appreciative.
> 
> Thank you,
> 
> Kevin
>


From wolfgang.viechtbauer at maastrichtuniversity.nl  Thu Jan 16 18:58:00 2014
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (STAT))
Date: Thu, 16 Jan 2014 18:58:00 +0100
Subject: [R-sig-ME] Replicating SAS results in R with unstructured
 covariance matrix
In-Reply-To: <CAF-NzEp8pn2Vtmt7AXZtB4Z5B-=KRTHUcdppP_e4Ki1bhjmn0Q@mail.gmail.com>
References: <077E31A57DA26E46AB0D493C9966AC730D9D035316@UM-MAIL4112.unimaas.nl>
	<CAF-NzEp8pn2Vtmt7AXZtB4Z5B-=KRTHUcdppP_e4Ki1bhjmn0Q@mail.gmail.com>
Message-ID: <077E31A57DA26E46AB0D493C9966AC730D9D1B2BF6@UM-MAIL4112.unimaas.nl>

For corSymm, 'form' needs to be:

"a one sided formula of the form ~ t, or ~ t | g, specifying a time covariate t and, optionally, a grouping factor g. A covariate for this correlation structure must be integer valued. When a grouping factor is present in form, the correlation structure is assumed to apply only to observations within the same grouping level; observations with different grouping levels are assumed to be uncorrelated. Defaults to ~ 1, which corresponds to using the order of the observations in the data as a covariate, and no groups."

So, yes, with corSymm(form = ~ 1 | SUBJECT), gls() will assume that the first measurement for a particular subject corresponds to time = 1. If that's not the case (and that can of course happen if rows with missing data have been deleted), then things get misalligned. Then you need the time covariate, so that things can be matched up properly.

Best,
Wolfgang

> -----Original Message-----
> From: Tony K.-T. [mailto:tkamth at gmail.com]
> Sent: Thursday, January 16, 2014 18:13
> To: Viechtbauer Wolfgang (STAT)
> Cc: r-sig-mixed-models at r-project.org; Charles Determan Jr
> Subject: Re: [R-sig-ME] Replicating SAS results in R with unstructured
> covariance matrix
> 
> Dear Wolfgang,
> 
> Thanks for your response.
> 
> This was an agonizing ordeal to say the least, but the inclusion of a
> "consecutive" integer time value seemed to have made the difference in my
> case, the results are replicating. ?The only reasoning that I can come up
> with now is due to missing repeated measurement for some of the subjects,
> had we left it to just ...?correlation = corSymm(form = ~ 1 | SUBJECT)...,
> the package risks mixing up the order in the covariance matrix in
> different subjects, the specification of weights=?varIdent(form = ~ 1 |
> TIME) is not enough. ??(e.g.?assign TIME 5 measurements to TIME 1 if TIME
> 5 is the very first recording of the Subject j). ? Correct me if you think
> I am wrong because I would assume that the package would handle this.
> 
> I will make some test on this later on.
> 
> Thank you for your help,
> 
> Tony
> 
> On Thu, Jan 16, 2014 at 12:14 AM, Viechtbauer Wolfgang (STAT)
> <wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
> It's been a while since I used SAS for mixed-effects model, but I think
> the model:
> 
> PROC MIXED data=Comb method=reml;
> ? CLASS FAC1 TIME SUBJECT FAC2;
> ? MODEL RESPONSE = BIAS FAC2 FAC1 TIME TIME*FAC1 / ddfm=Residual solution;
> ? REPEATED TIME / subject=SUBJECT type=un;
> RUN;
> can be fit with gls() using:
> 
> gls(RESPONSE ~ BIAS + factor(FAC2) + factor(TIME)* factor(FAC1),
> correlation = corSymm(form = ~ TIME | SUBJECT), weights = varIdent(form =
> ~ 1 | TIME), data=Comb)
> 
> TIME needs to be integer valued. SUBJECT should be a factor. ddfm=Residual
> implies that the fixed effects are tested with t-tests using df = n-
> rank(X) (where X is the model matrix), while gls() uses the normal
> distribution to compute those p-values. But this is a minor issue, except
> in those pesky borderline cases, but then again neither that t-
> distribution nor that normal distribution is exactly right and one should
> withhold judgement anyway.
> 
> It would be nice if you could provide feedback whether the syntax above
> did the trick.
> 
> Best,
> Wolfgang
> 
> --
> Wolfgang Viechtbauer, Ph.D., Statistician
> Department of Psychiatry and Psychology
> School for Mental Health and Neuroscience
> Faculty of Health, Medicine, and Life Sciences
> Maastricht University, P.O. Box 616 (VIJV1)
> 6200 MD Maastricht, The Netherlands
> +31 (43) 388-4170?| http://www.wvbauer.com
> 
> 
> > -----Original Message-----
> > From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-
> models-
> > bounces at r-project.org] On Behalf Of Charles Determan Jr
> > Sent: Wednesday, January 15, 2014 21:27
> > To: Tony K.-T.
> > Cc: r-sig-mixed-models at r-project.org
> > Subject: Re: [R-sig-ME] Replicating SAS results in R with unstructured
> > covariance matrix
> >
> > Hi Tony,
> >
> > Unfortunately I never did solve this problem. ?I experimented endlessly
> > with all sorts of combinations of correlation structures available with
> > lme. ?Unless someone reads this will a wonderful solution to replicate
> the
> > unstructure structure analysis from SAS I can't help. ?The compound
> > symmetry correlation structure was the only that really matched up well.
> > I'm sorry I couldn't be more help. ?I ended up moving on to using other
> > multivariate analyses like PLSDA and Random Forest because of the
> > seemingly
> > incompatible analyses. ?I wanted to make sure anyone could replicate my
> > analysis independent of the statistical program. ?I poke at it
> > occasionally
> > but still nothing for the unstructured analysis. ?If I ever do I will be
> > sure to post it.
> >
> > Wish you the best in you analysis,
> > Charles


From segerfan83 at gmail.com  Thu Jan 16 23:45:03 2014
From: segerfan83 at gmail.com (AvianResearchDivision)
Date: Thu, 16 Jan 2014 17:45:03 -0500
Subject: [R-sig-ME] Always allow for correlation of random effects?
In-Reply-To: <077E31A57DA26E46AB0D493C9966AC730D99B0D876@UM-MAIL4112.unimaas.nl>
References: <CAHe08Sj39CQM=stWc0TtmKXDk4vA880D_T-MvqNfOvxp4xHwJQ@mail.gmail.com>
	<loom.20131203T220519-969@post.gmane.org>
	<BAY172-W39A186EB3F3B377969F974CBD50@phx.gbl>
	<CAG+WrEyFx06yZrp_AAbL2LmFiqjPRXA=Ef6BrWU3nVyLmY6D6g@mail.gmail.com>
	<loom.20131204T015620-146@post.gmane.org>
	<CAHe08SjQFduBLzqNJ6kCTfg+-buKuNku529r5vN=AsZBKBLS8w@mail.gmail.com>
	<077E31A57DA26E46AB0D493C9966AC730D99B0D876@UM-MAIL4112.unimaas.nl>
Message-ID: <CAHe08Sip2=8TaQB2qUfCXMaR6x7CX7UX5bX_qSXU7S896zrPpA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140116/811b08cc/attachment.pl>

From lplough at umces.edu  Thu Jan 16 23:50:18 2014
From: lplough at umces.edu (Louis)
Date: Thu, 16 Jan 2014 17:50:18 -0500
Subject: [R-sig-ME] using lme4 to model regression with non-independent
	(nested?) data
Message-ID: <52D8622A.5040904@umces.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140116/4eeb4fc0/attachment.pl>

From a.reynaldi at student.unsw.edu.au  Fri Jan 17 00:30:10 2014
From: a.reynaldi at student.unsw.edu.au (Arnold Reynaldi)
Date: Thu, 16 Jan 2014 23:30:10 +0000
Subject: [R-sig-ME] Help with groupedData function
Message-ID: <62FF4C4E32840642930D994866A2910593FCBF@INFPWXM002.ad.unsw.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140116/5e601d15/attachment.pl>

From David.Duffy at qimr.edu.au  Fri Jan 17 04:10:57 2014
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Fri, 17 Jan 2014 13:10:57 +1000
Subject: [R-sig-ME] using lme4 to model regression with
 non-independent(nested?) data
In-Reply-To: <52D8622A.5040904@umces.edu>
References: <52D8622A.5040904@umces.edu>
Message-ID: <alpine.LMD.2.00.1401171258590.1284@orpheus.qimr.edu.au>

On Fri, 17 Jan 2014, Louis wrote:

> Dear lme4 list,
> I am having trouble modelling regression with seemingly non-independent,
> nested observations and I think a mixed model approach (using *lme4* for
> example) is required. I am examining the effect of genetic relatedness
> of males on the proportion of offspring sired in cases of multiple
> paternity (multiple males contributing to a brood of eggs from one
> female). I have estimates of the genetic relatedness of each male and
> the female it mated with, as well as the proportion of offspring that
> male sired in the female's brood of eggs. There are 3-5 males
> contributing to a brood, and 9 separate broods sampled for paternity.
>
> *My questions are: *1)Should observations be nested within a female,
> 2)and/or should female be treated as a random effect (9 broods from 9
> females) or both? 3) should relatedness remain a fixed effect? Again, I
> am concerned that I have not adequately dealt with the non-independence
> of male proportion values.
> ID Female   Relat.   Prop.
> 1    A      .12      .3
> 2    A      .03      .02
> 4    A      .23      .68

Heavens! I don't think lme4 is the right tool, though I think you could 
get an approximately right model (negative covariance between males within 
each clutch). The female only comes into the multinomial bit, setting 
total clutch size, as I see it, so I don't see the point of a RE there. 
Personally, I would permute males within clutches, with a correlation 
coefficient as the measure of association.

Female Male N_Sired relatedness 
A      A1   3       0.12 
A      A2   1       0.03 
A      A3   6       0.68 
B      B1   2       0.05
...


| David Duffy (MBBS PhD)
| email: David.Duffy at qimrberghofer.edu.au  ph: INT+61+7+3362-0217 fax: -0101
| Genetic Epidemiology, QIMR Berghofer Institute of Medical Research
| 300 Herston Rd, Brisbane, Queensland 4006, Australia  GPG 4D0B994A


From queirozrafaelmv at yahoo.com.br  Fri Jan 17 18:05:16 2014
From: queirozrafaelmv at yahoo.com.br (Rafael Maia)
Date: Fri, 17 Jan 2014 12:05:16 -0500
Subject: [R-sig-ME] Multiresponse MCMCglmm model specification and questions
Message-ID: <7613F758-290D-451A-87CF-F508218BDC91@yahoo.com.br>

Dear list members,

I am attempting to run a multiresponse model to investigate factors influencing the evolution of a trait measured in both males and females of multiple species. Therefore I chose a multiresponse approach using MCMCglmm, but my model has a series of little caveats (don?t they all?) that I?d like to make sure I?m addressing correctly. Here is a brief description of the model I?m trying to fit:

Ymale, Yfemale: the same response variable, measured in males and females

X1: a variable (factor, 2 levels) that I expect to affect both sexes differently
X2: a variable (factor, 2 levels) that I expect to affect both sexes equally (or estimate the joint effect across sexes)
X3: a continuous variable measured only for males (which I therefore expect to only have an effect in male measurements)

so I am currently specifying the model as below:

mod <- cbind(Ymale, Yfemale) ~ trait*X1 + X2 + at.level(trait, 1):X3

ainv <- inverseA(phylo)$Ainv

Prior <- list(R=list(V=diag(2), nu=0.002), G=list(G1=list(V=diag(2), nu=0.002)))

result <- MCMCglmm(mod, 
    random = ~us(trait):spp, rcov = ~idh(trait):units, 
    data=dat, prior = Prior, family = rep(?gaussian?, 2),
    nitt=2100000, burnin=100000, thin=1000)

with that, I have a couple questions (besides: does this model specification look reasonable?):

1. Prior: I wanna make sure I?m specifying an inverse-gamma with scale=shape=0.001 for the variance components. Based on footnote 1 of the Course Notes (p.102) I think I am based on the changes, but I?ve seen several times in mailing lists responses and other papers nu=1.002 being used for a matrix of same dimensions. My results are robust to either specification but I?d like to make sure I?m doing what I think I?m doing, given that there have been changes on this regard.

2. I am calculating the correlation between ymale and female as:
    result$VCV[,2]/sqrt(result$VCV[,1]*result$VCV[,4])
as described in the tutorial vignette. My values, however, are extremely high. The two traits are indeed strongly correlated among species, but I?m getting credible intervals of 0.96 - 0.99 (for comparison, a raw non-phylogenetic correlation of the traits is 0.75). Am I forgetting to include any variance components to calculate the intersexual correlation? Is this increase in the correlation expected given the inclusion of the fixed and random effects in the model?

3. I have a hypothesis that the correlation between male and female traits should be stronger for one level of X1 than for the other. Is it possible to specify random terms in order to calculate (co)variances of the traits conditional on X1?
    
I deeply appreciate any help you can provide. Cheers!


Abra?os,
Rafael Maia
---
http://www.rafaelmaia.net/
PhD Candidate, Integrated Bioscience
University of Akron
"A little learning is a dangerous thing; drink deep, or taste not the Pierian spring." (A. Pope)


From Tom.Wenseleers at bio.kuleuven.be  Mon Jan 20 00:53:57 2014
From: Tom.Wenseleers at bio.kuleuven.be (Tom Wenseleers)
Date: Sun, 19 Jan 2014 23:53:57 +0000
Subject: [R-sig-ME] Testing significance of deviation of sex ratio (prop of
 females) from a priori predicted proportion
Message-ID: <37EFC97028F3E44082ACC5CBEC00563011360B86@ICTS-S-MBX13.luna.kuleuven.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140119/29880047/attachment.pl>

From t.h.ergon at ibv.uio.no  Wed Jan 15 11:18:35 2014
From: t.h.ergon at ibv.uio.no (=?ISO-8859-1?Q?Torbj=F8rn_Ergon?=)
Date: Wed, 15 Jan 2014 11:18:35 +0100
Subject: [R-sig-ME] glmmadmb mcmc - scale of variance components
Message-ID: <52D6607B.7080703@ibv.uio.no>

Dear list,

Could anyone tell me what is the scale of the variance components 
(tempL.x parameters) in the mcmc output from glmmadmb (version 0.7.7)?

 From previous posts it seems that they are supposed to be log of the 
standard deviation in the random effects distribution, but I don't get 
any negative values (I expect variance <1) so I guess that has changed. 
Are they variances or standard deviations? I presume the 
variances/standard deviations are on the scale of the linear predictor.

Cheers,

Torbj?rn Ergon


From tkamth at gmail.com  Thu Jan 16 18:12:46 2014
From: tkamth at gmail.com (Tony K.-T.)
Date: Thu, 16 Jan 2014 18:12:46 +0100
Subject: [R-sig-ME] Replicating SAS results in R with unstructured
 covariance matrix
In-Reply-To: <077E31A57DA26E46AB0D493C9966AC730D9D035316@UM-MAIL4112.unimaas.nl>
References: <077E31A57DA26E46AB0D493C9966AC730D9D035316@UM-MAIL4112.unimaas.nl>
Message-ID: <CAF-NzEp8pn2Vtmt7AXZtB4Z5B-=KRTHUcdppP_e4Ki1bhjmn0Q@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140116/f2f2cc48/attachment.pl>

From Thierry.ONKELINX at inbo.be  Mon Jan 20 09:23:36 2014
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Mon, 20 Jan 2014 08:23:36 +0000
Subject: [R-sig-ME] Testing significance of deviation of sex ratio (prop
 of females) from a priori predicted proportion
In-Reply-To: <37EFC97028F3E44082ACC5CBEC00563011360B86@ICTS-S-MBX13.luna.kuleuven.be>
References: <37EFC97028F3E44082ACC5CBEC00563011360B86@ICTS-S-MBX13.luna.kuleuven.be>
Message-ID: <AA818EAD2576BC488B4F623941DA7427EAFF5864@inbomail.inbo.be>

Dear Tom,

I would add the null hypothesis as an offset. Then the intercept would be the deviation from that null hypothesis.

Fit <- glmer(cbind(females, males) ~ offset(qlogis(0.75)) + (1|colony), family = binomial, data = data)
library(multcomp)
confint(glht(Fit))

Addingspacesmakescodemuchmorereadable.

Best regards,

Thierry

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Tom Wenseleers
Verzonden: maandag 20 januari 2014 0:54
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Dear all,
I have counts of males and females produced by different colonies of a species of social insects.
I fitted the model
Fit=glmer(cbind(females,names)~1+(1|colony),family=binomial,data=data)

However, how can I test within such a framework if the overall average sex ratio deviates from an a priori predicted value (e.g. half females, or ? females)?
I presume this would have to be done based on the fitted intercept. But how does one do this? Also, what would be the best way to get 95% conf lims on the estimate? Using likelihood profiling, or parametric bootstrap? Does anybody happen to have any example calculation?

Cheers,
Tom


_______________________________________________________________________________________

Prof. Tom Wenseleers
*      Lab. of Socioecology and Social Evolution
           Dept. of Biology
           Zoological Institute
           K.U.Leuven
           Naamsestraat 59, box 2466
           B-3000 Leuven
           Belgium
* +32 (0)16 32 39 64 / +32 (0)472 40 45 96
* tom.wenseleers at bio.kuleuven.be
http://bio.kuleuven.be/ento/wenseleers/twenseleers.htm



        [[alternative HTML version deleted]]

* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From Tom.Wenseleers at bio.kuleuven.be  Mon Jan 20 10:02:09 2014
From: Tom.Wenseleers at bio.kuleuven.be (Tom Wenseleers)
Date: Mon, 20 Jan 2014 09:02:09 +0000
Subject: [R-sig-ME] Testing significance of deviation of sex ratio (prop
 of females) from a priori predicted proportion
In-Reply-To: <AA818EAD2576BC488B4F623941DA7427EAFF5864@inbomail.inbo.be>
References: <37EFC97028F3E44082ACC5CBEC00563011360B86@ICTS-S-MBX13.luna.kuleuven.be>
	<AA818EAD2576BC488B4F623941DA7427EAFF5864@inbomail.inbo.be>
Message-ID: <37EFC97028F3E44082ACC5CBEC00563011360CC4@ICTS-S-MBX13.luna.kuleuven.be>

Hi Thierry,
Many thanks for that - that's exactly what I needed!

Cheers,
Tom

-----Original Message-----
From: ONKELINX, Thierry [mailto:Thierry.ONKELINX at inbo.be] 
Sent: 20 January 2014 09:24
To: Tom Wenseleers; r-sig-mixed-models at r-project.org
Subject: RE: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Dear Tom,

I would add the null hypothesis as an offset. Then the intercept would be the deviation from that null hypothesis.

Fit <- glmer(cbind(females, males) ~ offset(qlogis(0.75)) + (1|colony), family = binomial, data = data)
library(multcomp)
confint(glht(Fit))

Addingspacesmakescodemuchmorereadable.

Best regards,

Thierry

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Tom Wenseleers
Verzonden: maandag 20 januari 2014 0:54
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Dear all,
I have counts of males and females produced by different colonies of a species of social insects.
I fitted the model
Fit=glmer(cbind(females,names)~1+(1|colony),family=binomial,data=data)

However, how can I test within such a framework if the overall average sex ratio deviates from an a priori predicted value (e.g. half females, or ? females)?
I presume this would have to be done based on the fitted intercept. But how does one do this? Also, what would be the best way to get 95% conf lims on the estimate? Using likelihood profiling, or parametric bootstrap? Does anybody happen to have any example calculation?

Cheers,
Tom


_______________________________________________________________________________________

Prof. Tom Wenseleers
*      Lab. of Socioecology and Social Evolution
           Dept. of Biology
           Zoological Institute
           K.U.Leuven
           Naamsestraat 59, box 2466
           B-3000 Leuven
           Belgium
* +32 (0)16 32 39 64 / +32 (0)472 40 45 96
* tom.wenseleers at bio.kuleuven.be
http://bio.kuleuven.be/ento/wenseleers/twenseleers.htm



        [[alternative HTML version deleted]]

* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * * Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From Thierry.ONKELINX at inbo.be  Mon Jan 20 12:36:09 2014
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Mon, 20 Jan 2014 11:36:09 +0000
Subject: [R-sig-ME] Testing significance of deviation of sex ratio (prop
 of females) from a priori predicted proportion
In-Reply-To: <37EFC97028F3E44082ACC5CBEC00563011360D10@ICTS-S-MBX13.luna.kuleuven.be>
References: <37EFC97028F3E44082ACC5CBEC00563011360B86@ICTS-S-MBX13.luna.kuleuven.be>
	<AA818EAD2576BC488B4F623941DA7427EAFF5864@inbomail.inbo.be>
	<37EFC97028F3E44082ACC5CBEC00563011360D10@ICTS-S-MBX13.luna.kuleuven.be>
Message-ID: <AA818EAD2576BC488B4F623941DA7427EAFF5A1A@inbomail.inbo.be>

Hi Tom,

Try

data$baseline <- qlogis(0.75)
fit <- glmer(cbind(females, males) ~ offset(baseline) + (1|colony), family = binomial(link=logit), data = data)

or send me a reproducible example.

Please keep the mailing list in cc.

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey


-----Oorspronkelijk bericht-----
Van: Tom Wenseleers [mailto:Tom.Wenseleers at bio.kuleuven.be]
Verzonden: maandag 20 januari 2014 10:37
Aan: ONKELINX, Thierry
Onderwerp: RE: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Hi Thierry,
One problem though I bump into:

library(devtools)
install_github("lme4",user="lme4")
library(lme4)

If I then try
fit=glmer(cbind(femalesmales) ~ offset(qlogis(0.75)) + (1|colony), family = binomial(link=logit), data = data) I get the error Error in model.frame.default(data = data, drop.unused.levels = TRUE,  :
  variable lengths differ (found for 'offset(qlogis(0.75))')

Any thoughts what I am doing wrong? What version of lme4 should I be using?

Cheers,
Tom

-----Original Message-----
From: ONKELINX, Thierry [mailto:Thierry.ONKELINX at inbo.be]
Sent: 20 January 2014 09:24
To: Tom Wenseleers; r-sig-mixed-models at r-project.org
Subject: RE: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Dear Tom,

I would add the null hypothesis as an offset. Then the intercept would be the deviation from that null hypothesis.

Fit <- glmer(cbind(females, males) ~ offset(qlogis(0.75)) + (1|colony), family = binomial, data = data)
library(multcomp)
confint(glht(Fit))

Addingspacesmakescodemuchmorereadable.

Best regards,

Thierry

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Tom Wenseleers
Verzonden: maandag 20 januari 2014 0:54
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Dear all,
I have counts of males and females produced by different colonies of a species of social insects.
I fitted the model
Fit=glmer(cbind(females,names)~1+(1|colony),family=binomial,data=data)

However, how can I test within such a framework if the overall average sex ratio deviates from an a priori predicted value (e.g. half females, or ? females)?
I presume this would have to be done based on the fitted intercept. But how does one do this? Also, what would be the best way to get 95% conf lims on the estimate? Using likelihood profiling, or parametric bootstrap? Does anybody happen to have any example calculation?

Cheers,
Tom


_______________________________________________________________________________________

Prof. Tom Wenseleers
*      Lab. of Socioecology and Social Evolution
           Dept. of Biology
           Zoological Institute
           K.U.Leuven
           Naamsestraat 59, box 2466
           B-3000 Leuven
           Belgium
* +32 (0)16 32 39 64 / +32 (0)472 40 45 96
* tom.wenseleers at bio.kuleuven.be
http://bio.kuleuven.be/ento/wenseleers/twenseleers.htm



        [[alternative HTML version deleted]]

* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * * Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From Tom.Wenseleers at bio.kuleuven.be  Mon Jan 20 14:31:16 2014
From: Tom.Wenseleers at bio.kuleuven.be (Tom Wenseleers)
Date: Mon, 20 Jan 2014 13:31:16 +0000
Subject: [R-sig-ME] Testing significance of deviation of sex ratio (prop
 of females) from a priori predicted proportion
In-Reply-To: <AA818EAD2576BC488B4F623941DA7427EAFF5A1A@inbomail.inbo.be>
References: <37EFC97028F3E44082ACC5CBEC00563011360B86@ICTS-S-MBX13.luna.kuleuven.be>
	<AA818EAD2576BC488B4F623941DA7427EAFF5864@inbomail.inbo.be>
	<37EFC97028F3E44082ACC5CBEC00563011360D10@ICTS-S-MBX13.luna.kuleuven.be>
	<AA818EAD2576BC488B4F623941DA7427EAFF5A1A@inbomail.inbo.be>
Message-ID: <37EFC97028F3E44082ACC5CBEC00563011360E5C@ICTS-S-MBX13.luna.kuleuven.be>

Hi Thierry,
I now tried

library(devtools)
install_github("lme4",user="lme4")  # latest development version
library(lme4)
library(multcomp)
library(gtools)
library(car)

data=read.csv("http://www.kuleuven.be/bio/ento/temp/test_data_sexratio.csv",header=T,stringsAsFactors=T)
data$baseline=logit(0.75)
fit0=glmer(cbind(female,male) ~ offset(baseline)+(1|colony), family = binomial(link=logit), data = data)
summary(fit0)

# test significance of deviation from 3:4 (0.75) using LRT:
Anova(fit0,type=3,test="Chisq") # significance of deviation from 0.75 tested using LRT

# same model but without offset to calculate confidence limits on non-offsetted data
fit1=glmer(cbind(female,male) ~ (1|colony), family = binomial(link=logit), data = data)
 
# confidence limits calculated using glht
cl=confint(glht(fit1))
inv.logit(cl$confint[1,]) # est prop of females, with 95% confidence limits

# confidence limits calculated using likelihood profiling
cl=confint(fit1) # confidence limits calculated using likelihood profiling
inv.logit(c(attr(fit1,"beta"),as.matrix(cl)[2,])) # mean and 95% confidence limits

Is this correct you reckon? And is the significance calculated using the LRT correct? And what method would be preferred to calculate the confidence limits? Using glht? (Would that use Wald type confidence limits) Or using likelihood profiling (am I correct that this would correspond to the LRT result)?

Cheers & thanks again for your help!
Tom

-----Original Message-----
From: ONKELINX, Thierry [mailto:Thierry.ONKELINX at inbo.be] 
Sent: 20 January 2014 12:36
To: Tom Wenseleers
Cc: r-sig-mixed-models at r-project.org
Subject: RE: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Hi Tom,

Try

data$baseline <- qlogis(0.75)
fit <- glmer(cbind(females, males) ~ offset(baseline) + (1|colony), family = binomial(link=logit), data = data)

or send me a reproducible example.

Please keep the mailing list in cc.

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey


-----Oorspronkelijk bericht-----
Van: Tom Wenseleers [mailto:Tom.Wenseleers at bio.kuleuven.be]
Verzonden: maandag 20 januari 2014 10:37
Aan: ONKELINX, Thierry
Onderwerp: RE: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Hi Thierry,
One problem though I bump into:

library(devtools)
install_github("lme4",user="lme4")
library(lme4)

If I then try
fit=glmer(cbind(femalesmales) ~ offset(qlogis(0.75)) + (1|colony), family = binomial(link=logit), data = data) I get the error Error in model.frame.default(data = data, drop.unused.levels = TRUE,  :
  variable lengths differ (found for 'offset(qlogis(0.75))')

Any thoughts what I am doing wrong? What version of lme4 should I be using?

Cheers,
Tom

-----Original Message-----
From: ONKELINX, Thierry [mailto:Thierry.ONKELINX at inbo.be]
Sent: 20 January 2014 09:24
To: Tom Wenseleers; r-sig-mixed-models at r-project.org
Subject: RE: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Dear Tom,

I would add the null hypothesis as an offset. Then the intercept would be the deviation from that null hypothesis.

Fit <- glmer(cbind(females, males) ~ offset(qlogis(0.75)) + (1|colony), family = binomial, data = data)
library(multcomp)
confint(glht(Fit))

Addingspacesmakescodemuchmorereadable.

Best regards,

Thierry

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Tom Wenseleers
Verzonden: maandag 20 januari 2014 0:54
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Dear all,
I have counts of males and females produced by different colonies of a species of social insects.
I fitted the model
Fit=glmer(cbind(females,names)~1+(1|colony),family=binomial,data=data)

However, how can I test within such a framework if the overall average sex ratio deviates from an a priori predicted value (e.g. half females, or ? females)?
I presume this would have to be done based on the fitted intercept. But how does one do this? Also, what would be the best way to get 95% conf lims on the estimate? Using likelihood profiling, or parametric bootstrap? Does anybody happen to have any example calculation?

Cheers,
Tom


_______________________________________________________________________________________

Prof. Tom Wenseleers
*      Lab. of Socioecology and Social Evolution
           Dept. of Biology
           Zoological Institute
           K.U.Leuven
           Naamsestraat 59, box 2466
           B-3000 Leuven
           Belgium
* +32 (0)16 32 39 64 / +32 (0)472 40 45 96
* tom.wenseleers at bio.kuleuven.be
http://bio.kuleuven.be/ento/wenseleers/twenseleers.htm



        [[alternative HTML version deleted]]

* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * * Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * * Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From tobias.heed at uni-hamburg.de  Mon Jan 20 15:23:53 2014
From: tobias.heed at uni-hamburg.de (Tobias Heed)
Date: Mon, 20 Jan 2014 15:23:53 +0100
Subject: [R-sig-ME] non-orthogonal contrasts
Message-ID: <AC638A30-0B3B-4B57-96F6-6895F4B35207@uni-hamburg.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140120/86757bc1/attachment.pl>

From Thierry.ONKELINX at inbo.be  Mon Jan 20 15:40:43 2014
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Mon, 20 Jan 2014 14:40:43 +0000
Subject: [R-sig-ME] Testing significance of deviation of sex ratio (prop
 of females) from a priori predicted proportion
In-Reply-To: <37EFC97028F3E44082ACC5CBEC00563011360E5C@ICTS-S-MBX13.luna.kuleuven.be>
References: <37EFC97028F3E44082ACC5CBEC00563011360B86@ICTS-S-MBX13.luna.kuleuven.be>
	<AA818EAD2576BC488B4F623941DA7427EAFF5864@inbomail.inbo.be>
	<37EFC97028F3E44082ACC5CBEC00563011360D10@ICTS-S-MBX13.luna.kuleuven.be>
	<AA818EAD2576BC488B4F623941DA7427EAFF5A1A@inbomail.inbo.be>
	<37EFC97028F3E44082ACC5CBEC00563011360E5C@ICTS-S-MBX13.luna.kuleuven.be>
Message-ID: <AA818EAD2576BC488B4F623941DA7427EAFF5B0E@inbomail.inbo.be>

Dear Tom,

I presume you have multiple measurements per colony? If not, it does not makes sense to use colony as a random intercept. And hence to use mixed models...

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey


-----Oorspronkelijk bericht-----
Van: Tom Wenseleers [mailto:Tom.Wenseleers at bio.kuleuven.be]
Verzonden: maandag 20 januari 2014 14:31
Aan: ONKELINX, Thierry; r-sig-mixed-models at r-project.org
Onderwerp: RE: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Hi Thierry,
I now tried

library(devtools)
install_github("lme4",user="lme4")  # latest development version
library(lme4)
library(multcomp)
library(gtools)
library(car)

data=read.csv("http://www.kuleuven.be/bio/ento/temp/test_data_sexratio.csv",header=T,stringsAsFactors=T)
data$baseline=logit(0.75)
fit0=glmer(cbind(female,male) ~ offset(baseline)+(1|colony), family = binomial(link=logit), data = data)
summary(fit0)

# test significance of deviation from 3:4 (0.75) using LRT:
Anova(fit0,type=3,test="Chisq") # significance of deviation from 0.75 tested using LRT

# same model but without offset to calculate confidence limits on non-offsetted data
fit1=glmer(cbind(female,male) ~ (1|colony), family = binomial(link=logit), data = data)

# confidence limits calculated using glht
cl=confint(glht(fit1))
inv.logit(cl$confint[1,]) # est prop of females, with 95% confidence limits

# confidence limits calculated using likelihood profiling
cl=confint(fit1) # confidence limits calculated using likelihood profiling
inv.logit(c(attr(fit1,"beta"),as.matrix(cl)[2,])) # mean and 95% confidence limits

Is this correct you reckon? And is the significance calculated using the LRT correct? And what method would be preferred to calculate the confidence limits? Using glht? (Would that use Wald type confidence limits) Or using likelihood profiling (am I correct that this would correspond to the LRT result)?

Cheers & thanks again for your help!
Tom

-----Original Message-----
From: ONKELINX, Thierry [mailto:Thierry.ONKELINX at inbo.be]
Sent: 20 January 2014 12:36
To: Tom Wenseleers
Cc: r-sig-mixed-models at r-project.org
Subject: RE: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Hi Tom,

Try

data$baseline <- qlogis(0.75)
fit <- glmer(cbind(females, males) ~ offset(baseline) + (1|colony), family = binomial(link=logit), data = data)

or send me a reproducible example.

Please keep the mailing list in cc.

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey


-----Oorspronkelijk bericht-----
Van: Tom Wenseleers [mailto:Tom.Wenseleers at bio.kuleuven.be]
Verzonden: maandag 20 januari 2014 10:37
Aan: ONKELINX, Thierry
Onderwerp: RE: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Hi Thierry,
One problem though I bump into:

library(devtools)
install_github("lme4",user="lme4")
library(lme4)

If I then try
fit=glmer(cbind(femalesmales) ~ offset(qlogis(0.75)) + (1|colony), family = binomial(link=logit), data = data) I get the error Error in model.frame.default(data = data, drop.unused.levels = TRUE,  :
  variable lengths differ (found for 'offset(qlogis(0.75))')

Any thoughts what I am doing wrong? What version of lme4 should I be using?

Cheers,
Tom

-----Original Message-----
From: ONKELINX, Thierry [mailto:Thierry.ONKELINX at inbo.be]
Sent: 20 January 2014 09:24
To: Tom Wenseleers; r-sig-mixed-models at r-project.org
Subject: RE: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Dear Tom,

I would add the null hypothesis as an offset. Then the intercept would be the deviation from that null hypothesis.

Fit <- glmer(cbind(females, males) ~ offset(qlogis(0.75)) + (1|colony), family = binomial, data = data)
library(multcomp)
confint(glht(Fit))

Addingspacesmakescodemuchmorereadable.

Best regards,

Thierry

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Tom Wenseleers
Verzonden: maandag 20 januari 2014 0:54
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Dear all,
I have counts of males and females produced by different colonies of a species of social insects.
I fitted the model
Fit=glmer(cbind(females,names)~1+(1|colony),family=binomial,data=data)

However, how can I test within such a framework if the overall average sex ratio deviates from an a priori predicted value (e.g. half females, or ? females)?
I presume this would have to be done based on the fitted intercept. But how does one do this? Also, what would be the best way to get 95% conf lims on the estimate? Using likelihood profiling, or parametric bootstrap? Does anybody happen to have any example calculation?

Cheers,
Tom


_______________________________________________________________________________________

Prof. Tom Wenseleers
*      Lab. of Socioecology and Social Evolution
           Dept. of Biology
           Zoological Institute
           K.U.Leuven
           Naamsestraat 59, box 2466
           B-3000 Leuven
           Belgium
* +32 (0)16 32 39 64 / +32 (0)472 40 45 96
* tom.wenseleers at bio.kuleuven.be
http://bio.kuleuven.be/ento/wenseleers/twenseleers.htm



        [[alternative HTML version deleted]]

* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * * Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * * Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From Tom.Wenseleers at bio.kuleuven.be  Mon Jan 20 17:21:37 2014
From: Tom.Wenseleers at bio.kuleuven.be (Tom Wenseleers)
Date: Mon, 20 Jan 2014 16:21:37 +0000
Subject: [R-sig-ME] Testing significance of deviation of sex ratio (prop
 of females) from a priori predicted proportion
In-Reply-To: <AA818EAD2576BC488B4F623941DA7427EAFF5B0E@inbomail.inbo.be>
References: <37EFC97028F3E44082ACC5CBEC00563011360B86@ICTS-S-MBX13.luna.kuleuven.be>
	<AA818EAD2576BC488B4F623941DA7427EAFF5864@inbomail.inbo.be>
	<37EFC97028F3E44082ACC5CBEC00563011360D10@ICTS-S-MBX13.luna.kuleuven.be>
	<AA818EAD2576BC488B4F623941DA7427EAFF5A1A@inbomail.inbo.be>
	<37EFC97028F3E44082ACC5CBEC00563011360E5C@ICTS-S-MBX13.luna.kuleuven.be>
	<AA818EAD2576BC488B4F623941DA7427EAFF5B0E@inbomail.inbo.be>
Message-ID: <37EFC97028F3E44082ACC5CBEC005630113610B2@ICTS-S-MBX13.luna.kuleuven.be>

Hi Thierry,
Ha, are you sure - can't I use it to model possible overdispersion in this case?

Cheers,
Tom

-----Original Message-----
From: ONKELINX, Thierry [mailto:Thierry.ONKELINX at inbo.be] 
Sent: 20 January 2014 15:41
To: Tom Wenseleers; r-sig-mixed-models at r-project.org
Subject: RE: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Dear Tom,

I presume you have multiple measurements per colony? If not, it does not makes sense to use colony as a random intercept. And hence to use mixed models...

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey


-----Oorspronkelijk bericht-----
Van: Tom Wenseleers [mailto:Tom.Wenseleers at bio.kuleuven.be]
Verzonden: maandag 20 januari 2014 14:31
Aan: ONKELINX, Thierry; r-sig-mixed-models at r-project.org
Onderwerp: RE: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Hi Thierry,
I now tried

library(devtools)
install_github("lme4",user="lme4")  # latest development version
library(lme4)
library(multcomp)
library(gtools)
library(car)

data=read.csv("http://www.kuleuven.be/bio/ento/temp/test_data_sexratio.csv",header=T,stringsAsFactors=T)
data$baseline=logit(0.75)
fit0=glmer(cbind(female,male) ~ offset(baseline)+(1|colony), family = binomial(link=logit), data = data)
summary(fit0)

# test significance of deviation from 3:4 (0.75) using LRT:
Anova(fit0,type=3,test="Chisq") # significance of deviation from 0.75 tested using LRT

# same model but without offset to calculate confidence limits on non-offsetted data
fit1=glmer(cbind(female,male) ~ (1|colony), family = binomial(link=logit), data = data)

# confidence limits calculated using glht
cl=confint(glht(fit1))
inv.logit(cl$confint[1,]) # est prop of females, with 95% confidence limits

# confidence limits calculated using likelihood profiling
cl=confint(fit1) # confidence limits calculated using likelihood profiling
inv.logit(c(attr(fit1,"beta"),as.matrix(cl)[2,])) # mean and 95% confidence limits

Is this correct you reckon? And is the significance calculated using the LRT correct? And what method would be preferred to calculate the confidence limits? Using glht? (Would that use Wald type confidence limits) Or using likelihood profiling (am I correct that this would correspond to the LRT result)?

Cheers & thanks again for your help!
Tom

-----Original Message-----
From: ONKELINX, Thierry [mailto:Thierry.ONKELINX at inbo.be]
Sent: 20 January 2014 12:36
To: Tom Wenseleers
Cc: r-sig-mixed-models at r-project.org
Subject: RE: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Hi Tom,

Try

data$baseline <- qlogis(0.75)
fit <- glmer(cbind(females, males) ~ offset(baseline) + (1|colony), family = binomial(link=logit), data = data)

or send me a reproducible example.

Please keep the mailing list in cc.

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey


-----Oorspronkelijk bericht-----
Van: Tom Wenseleers [mailto:Tom.Wenseleers at bio.kuleuven.be]
Verzonden: maandag 20 januari 2014 10:37
Aan: ONKELINX, Thierry
Onderwerp: RE: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Hi Thierry,
One problem though I bump into:

library(devtools)
install_github("lme4",user="lme4")
library(lme4)

If I then try
fit=glmer(cbind(femalesmales) ~ offset(qlogis(0.75)) + (1|colony), family = binomial(link=logit), data = data) I get the error Error in model.frame.default(data = data, drop.unused.levels = TRUE,  :
  variable lengths differ (found for 'offset(qlogis(0.75))')

Any thoughts what I am doing wrong? What version of lme4 should I be using?

Cheers,
Tom

-----Original Message-----
From: ONKELINX, Thierry [mailto:Thierry.ONKELINX at inbo.be]
Sent: 20 January 2014 09:24
To: Tom Wenseleers; r-sig-mixed-models at r-project.org
Subject: RE: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Dear Tom,

I would add the null hypothesis as an offset. Then the intercept would be the deviation from that null hypothesis.

Fit <- glmer(cbind(females, males) ~ offset(qlogis(0.75)) + (1|colony), family = binomial, data = data)
library(multcomp)
confint(glht(Fit))

Addingspacesmakescodemuchmorereadable.

Best regards,

Thierry

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Tom Wenseleers
Verzonden: maandag 20 januari 2014 0:54
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Dear all,
I have counts of males and females produced by different colonies of a species of social insects.
I fitted the model
Fit=glmer(cbind(females,names)~1+(1|colony),family=binomial,data=data)

However, how can I test within such a framework if the overall average sex ratio deviates from an a priori predicted value (e.g. half females, or ? females)?
I presume this would have to be done based on the fitted intercept. But how does one do this? Also, what would be the best way to get 95% conf lims on the estimate? Using likelihood profiling, or parametric bootstrap? Does anybody happen to have any example calculation?

Cheers,
Tom


_______________________________________________________________________________________

Prof. Tom Wenseleers
*      Lab. of Socioecology and Social Evolution
           Dept. of Biology
           Zoological Institute
           K.U.Leuven
           Naamsestraat 59, box 2466
           B-3000 Leuven
           Belgium
* +32 (0)16 32 39 64 / +32 (0)472 40 45 96
* tom.wenseleers at bio.kuleuven.be
http://bio.kuleuven.be/ento/wenseleers/twenseleers.htm



        [[alternative HTML version deleted]]

* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * * Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * * Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * * Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From jake987722 at hotmail.com  Tue Jan 21 03:52:44 2014
From: jake987722 at hotmail.com (Jake Westfall)
Date: Mon, 20 Jan 2014 19:52:44 -0700
Subject: [R-sig-ME] non-orthogonal contrasts
In-Reply-To: <AC638A30-0B3B-4B57-96F6-6895F4B35207@uni-hamburg.de>
References: <AC638A30-0B3B-4B57-96F6-6895F4B35207@uni-hamburg.de>
Message-ID: <BAY172-W10BA8E107CAB49D4C3159FCBA40@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140120/8f432464/attachment.pl>

From bbolker at gmail.com  Tue Jan 21 05:22:40 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 20 Jan 2014 23:22:40 -0500
Subject: [R-sig-ME] Formula and Start-values for simplest "non-linear"
 mixed model - nlmer (nlme??, lmer ?)
In-Reply-To: <82E31B97-AFDE-4A42-A12F-DECB28DFB05E@ALUMNI.RICE.EDU>
References: <82E31B97-AFDE-4A42-A12F-DECB28DFB05E@ALUMNI.RICE.EDU>
Message-ID: <52DDF610.4030300@gmail.com>

   Here's a start (I hope) -- new examples for nlmer ...  for the naming
issue, I would consider brute force:  R has a feature that often annoys
me that the names of vectors get carried along ...


> x <- c(a=1,b=2)
> y <- c(a=x[1])
> y
a.a
  1

instead try y<-c(a=unname(x[1]))


> y
a
1

## nonlinear mixed models --- 3-part formulas ---
## 1. basic nonlinear fit. Use stats::SSlogis for its
## implementation of the 3-parameter logistic curve.
## "SS" stands for "self-starting logistic", but the
## "self-starting" part is not currently used by nlmer ... 'start' is
## necessary
startvec <- c(Asym = 200, xmid = 725, scal = 350)
(nm1 <- nlmer(circumference ~ SSlogis(age, Asym, xmid, scal) ~ Asym|Tree,
             Orange, start = startvec))
## 2. re-run with "quick and dirty" PIRLS step
(nm1a <- update(nm1, nAGQ = 0L))
## 3. Fit the same model with a user-built function:
## a. Define formula
nform <- ~Asym/(1+exp((xmid-input)/scal))
## b. Use deriv() to construct function:
nfun <- deriv(nform,namevec=c("Asym","xmid","scal"),
              function.arg=c("input","Asym","xmid","scal"))
nm1b <- update(nm1,circumference ~ nfun(age, Asym, xmid, scal)  ~ Asym |
Tree)
## 4. User-built function without using derivs():
##    derivatives could be computed more efficiently
##    by pre-computing components, but these are essentially
##    the gradients as one would derive them by hand
nfun2 <- function(input, Asym, xmid, scal) {
    value <- Asym/(1+exp((xmid-input)/scal))
    grad <- cbind(Asym=1/(1+exp((xmid-input)/scal)),
              xmid=-Asym/(1+exp((xmid-input)/scal))^2*1/scal*
                    exp((xmid-input)/scal),
              scal=-Asym/(1+exp((xmid-input)/scal))^2*
                     -(xmid-input)/scal^2*exp((xmid-input)/scal))
    attr(value,"gradient") <- grad
    value
}
stopifnot(all.equal(attr(nfun(2,1,3,4),"gradient"),
                    attr(nfun(2,1,3,4),"gradient")))
nm1c <- update(nm1,circumference ~ nfun2(age, Asym, xmid, scal)  ~ Asym
| Tree)

On 14-01-16 01:01 PM, Robert Chatfield wrote:
> Hi, Ben.  There's been no response yet to the question about nlmer.   
> Could you start me off with  an answer to the question of how to send
> start values   start.vals=    as a named vector?  
> 
> It's fine to take the answer on the list.
> 
> 
> (NOTE: It does not seem to work to look for the usual  y intercepts by running the problem "backwards"
> as a multiple regression of a single tracer-variable versus many species.type concentrations
> (y[ ij ] == a[ i ] *( + {x^o}[ j ]  
> x[j]  = alpha(i) * y[ij] + x^o[j]  
> since there is an "exact" but trivial solution x^0[j] = x[j]  ... 
> 
> Two questions ... 
> ======================================================================================
> Mostly: (1)
> I could not convince nlmer that the the starting values were vectors ... perhaps this is
> elementary R
> start.vals.1 = c(a=aij, x.nought=x.nought.lin)    (SEE VALUES BELOW)
> #   start.vals.1 = cbind(a=aij, x.nought=x.nought.lin)  # tried this -- does not work
> 
> where this calculation for aij
> len.lon.dat = dim(good.est.merge.long.dat)[1]   #  repeats once for each species.type
> len.lon.dat = dim(good.est.merge.long.dat)[1]   #  repeats once for each species.type 
> 
> aij = vector(mode="numeric",length= len.lon.dat)
> #  The name aij is given since
> for ( i in 1:len.lon.dat ) {
> spec.type.nm = as.character(species.type[i])
> aij[i] = (ranef(try.spec.2.lmer,drop=T)[["species.type"]])[[spec.type.nm]]
> }
> 
> 
>  I've been using c( , ) but I'm wondering
> if nlmer expects the results that occur for a vector of start.vals
> 
> I am wondering if the names are not being passed along correctly ...
>> names(start.vals.1)[1]
> [1] "a1"
>> start.vals.1[1]
>       a1 
> 0.002266363 
>> start.vals.1[1802]
>    a1802 
> -0.02694557 
>> start.vals.1[1803]
> x.nought1 
> -0.3141149 
>> start.vals.1[3604]
> x.nought1802 
> -0.1204595 
> Could this naming be confusing to nlmer?  The start values must be named.
> 
> == from a follow-up post ====  "not all parameter names"  meaning?
> 
>  first.nlmer = nlmer( y ~ a*(x+ x.nought) ~ (a|species.type) + (x.nought|id),
> 
> +      start=start.vals.1,  data=good.est.merge.long.dat  )
> Error in nlmer(y ~ a * (x + x.nought) ~ (a | species.type) + (x.nought |  : 
>   not all parameter names are used in the nonlinear model expression
> 
> where named list is given by ...
>>
>  start.vals.1 = c(a=aij, x.nought=C.nought.lin)
> 
>>
>  start.vals.1[1]
> 
>          a1 
> 0.002266363 
>>
>  start.vals.1[3604]
> 
> x.nought1802 
>   -0.1204595 
> 
> Using single values with a single name also does not work, nor does a construct
> with cbind:
> 
>>
>  first.nlmer = nlmer( y ~ a*(x+ x.nought) ~ (a|species.type) + (x.nought|id),
> 
> +      start=c(a=0.00364901,x.nought=0),  data=good.est.merge.long.dat  )
> Error in nlmer(y ~ a * (x + x.nought) ~ (a | species.type) + (x.nought |  : 
>   gradient attribute of evaluated model must be a numeric matrix
> 
> Bob Chatfield
> 
>


From Thierry.ONKELINX at inbo.be  Tue Jan 21 09:25:14 2014
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 21 Jan 2014 08:25:14 +0000
Subject: [R-sig-ME] Testing significance of deviation of sex ratio (prop
 of females) from a priori predicted proportion
In-Reply-To: <37EFC97028F3E44082ACC5CBEC005630113610B2@ICTS-S-MBX13.luna.kuleuven.be>
References: <37EFC97028F3E44082ACC5CBEC00563011360B86@ICTS-S-MBX13.luna.kuleuven.be>
	<AA818EAD2576BC488B4F623941DA7427EAFF5864@inbomail.inbo.be>
	<37EFC97028F3E44082ACC5CBEC00563011360D10@ICTS-S-MBX13.luna.kuleuven.be>
	<AA818EAD2576BC488B4F623941DA7427EAFF5A1A@inbomail.inbo.be>
	<37EFC97028F3E44082ACC5CBEC00563011360E5C@ICTS-S-MBX13.luna.kuleuven.be>
	<AA818EAD2576BC488B4F623941DA7427EAFF5B0E@inbomail.inbo.be>
	<37EFC97028F3E44082ACC5CBEC005630113610B2@ICTS-S-MBX13.luna.kuleuven.be>
Message-ID: <AA818EAD2576BC488B4F623941DA7427EAFF5CA9@inbomail.inbo.be>

Since you have no other random effects, your model boils down to a glm(cbind(female,male) ~ offset(baseline), family = quasibinomial).

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey


-----Oorspronkelijk bericht-----
Van: Tom Wenseleers [mailto:Tom.Wenseleers at bio.kuleuven.be]
Verzonden: maandag 20 januari 2014 17:22
Aan: ONKELINX, Thierry; r-sig-mixed-models at r-project.org
Onderwerp: RE: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Hi Thierry,
Ha, are you sure - can't I use it to model possible overdispersion in this case?

Cheers,
Tom

-----Original Message-----
From: ONKELINX, Thierry [mailto:Thierry.ONKELINX at inbo.be]
Sent: 20 January 2014 15:41
To: Tom Wenseleers; r-sig-mixed-models at r-project.org
Subject: RE: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Dear Tom,

I presume you have multiple measurements per colony? If not, it does not makes sense to use colony as a random intercept. And hence to use mixed models...

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey


-----Oorspronkelijk bericht-----
Van: Tom Wenseleers [mailto:Tom.Wenseleers at bio.kuleuven.be]
Verzonden: maandag 20 januari 2014 14:31
Aan: ONKELINX, Thierry; r-sig-mixed-models at r-project.org
Onderwerp: RE: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Hi Thierry,
I now tried

library(devtools)
install_github("lme4",user="lme4")  # latest development version
library(lme4)
library(multcomp)
library(gtools)
library(car)

data=read.csv("http://www.kuleuven.be/bio/ento/temp/test_data_sexratio.csv",header=T,stringsAsFactors=T)
data$baseline=logit(0.75)
fit0=glmer(cbind(female,male) ~ offset(baseline)+(1|colony), family = binomial(link=logit), data = data)
summary(fit0)

# test significance of deviation from 3:4 (0.75) using LRT:
Anova(fit0,type=3,test="Chisq") # significance of deviation from 0.75 tested using LRT

# same model but without offset to calculate confidence limits on non-offsetted data
fit1=glmer(cbind(female,male) ~ (1|colony), family = binomial(link=logit), data = data)

# confidence limits calculated using glht
cl=confint(glht(fit1))
inv.logit(cl$confint[1,]) # est prop of females, with 95% confidence limits

# confidence limits calculated using likelihood profiling
cl=confint(fit1) # confidence limits calculated using likelihood profiling
inv.logit(c(attr(fit1,"beta"),as.matrix(cl)[2,])) # mean and 95% confidence limits

Is this correct you reckon? And is the significance calculated using the LRT correct? And what method would be preferred to calculate the confidence limits? Using glht? (Would that use Wald type confidence limits) Or using likelihood profiling (am I correct that this would correspond to the LRT result)?

Cheers & thanks again for your help!
Tom

-----Original Message-----
From: ONKELINX, Thierry [mailto:Thierry.ONKELINX at inbo.be]
Sent: 20 January 2014 12:36
To: Tom Wenseleers
Cc: r-sig-mixed-models at r-project.org
Subject: RE: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Hi Tom,

Try

data$baseline <- qlogis(0.75)
fit <- glmer(cbind(females, males) ~ offset(baseline) + (1|colony), family = binomial(link=logit), data = data)

or send me a reproducible example.

Please keep the mailing list in cc.

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey


-----Oorspronkelijk bericht-----
Van: Tom Wenseleers [mailto:Tom.Wenseleers at bio.kuleuven.be]
Verzonden: maandag 20 januari 2014 10:37
Aan: ONKELINX, Thierry
Onderwerp: RE: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Hi Thierry,
One problem though I bump into:

library(devtools)
install_github("lme4",user="lme4")
library(lme4)

If I then try
fit=glmer(cbind(femalesmales) ~ offset(qlogis(0.75)) + (1|colony), family = binomial(link=logit), data = data) I get the error Error in model.frame.default(data = data, drop.unused.levels = TRUE,  :
  variable lengths differ (found for 'offset(qlogis(0.75))')

Any thoughts what I am doing wrong? What version of lme4 should I be using?

Cheers,
Tom

-----Original Message-----
From: ONKELINX, Thierry [mailto:Thierry.ONKELINX at inbo.be]
Sent: 20 January 2014 09:24
To: Tom Wenseleers; r-sig-mixed-models at r-project.org
Subject: RE: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Dear Tom,

I would add the null hypothesis as an offset. Then the intercept would be the deviation from that null hypothesis.

Fit <- glmer(cbind(females, males) ~ offset(qlogis(0.75)) + (1|colony), family = binomial, data = data)
library(multcomp)
confint(glht(Fit))

Addingspacesmakescodemuchmorereadable.

Best regards,

Thierry

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Tom Wenseleers
Verzonden: maandag 20 januari 2014 0:54
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] Testing significance of deviation of sex ratio (prop of females) from a priori predicted proportion

Dear all,
I have counts of males and females produced by different colonies of a species of social insects.
I fitted the model
Fit=glmer(cbind(females,names)~1+(1|colony),family=binomial,data=data)

However, how can I test within such a framework if the overall average sex ratio deviates from an a priori predicted value (e.g. half females, or ? females)?
I presume this would have to be done based on the fitted intercept. But how does one do this? Also, what would be the best way to get 95% conf lims on the estimate? Using likelihood profiling, or parametric bootstrap? Does anybody happen to have any example calculation?

Cheers,
Tom


_______________________________________________________________________________________

Prof. Tom Wenseleers
*      Lab. of Socioecology and Social Evolution
           Dept. of Biology
           Zoological Institute
           K.U.Leuven
           Naamsestraat 59, box 2466
           B-3000 Leuven
           Belgium
* +32 (0)16 32 39 64 / +32 (0)472 40 45 96
* tom.wenseleers at bio.kuleuven.be
http://bio.kuleuven.be/ento/wenseleers/twenseleers.htm



        [[alternative HTML version deleted]]

* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * * Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * * Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * * Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From Tania.Cerni at unitn.it  Tue Jan 21 12:21:15 2014
From: Tania.Cerni at unitn.it (Cerni, Tania)
Date: Tue, 21 Jan 2014 12:21:15 +0100
Subject: [R-sig-ME] lmerconvenience function
Message-ID: <06085402EB5F15428D04F536A3BE2C8D10A5D75279@MBX.unitn.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140121/0e2e1fd9/attachment.pl>

From tobias.heed at uni-hamburg.de  Tue Jan 21 15:04:22 2014
From: tobias.heed at uni-hamburg.de (Tobias Heed)
Date: Tue, 21 Jan 2014 15:04:22 +0100
Subject: [R-sig-ME] non-orthogonal contrasts
In-Reply-To: <BAY172-W10BA8E107CAB49D4C3159FCBA40@phx.gbl>
References: <AC638A30-0B3B-4B57-96F6-6895F4B35207@uni-hamburg.de>
	<BAY172-W10BA8E107CAB49D4C3159FCBA40@phx.gbl>
Message-ID: <543DDFD5-C69F-4413-875F-2F081873008D@uni-hamburg.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140121/ce64b5c2/attachment.pl>

From bbolker at gmail.com  Tue Jan 21 15:19:08 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 21 Jan 2014 09:19:08 -0500
Subject: [R-sig-ME] =?windows-1252?q?R_package_=91lme4=92?=
In-Reply-To: <CAMk4_3K6EABR8yodqU8grG5-wkrHRdcto7yQHLQ6t7Mo+RMWdg@mail.gmail.com>
References: <CAMk4_3K6EABR8yodqU8grG5-wkrHRdcto7yQHLQ6t7Mo+RMWdg@mail.gmail.com>
Message-ID: <52DE81DC.3030701@gmail.com>

On 14-01-21 05:36 AM, Lorenzo Raggi wrote:
> Dear package curator,
> 
> first of all I apologize if I write to you directly.

 I'm taking the liberty of cc'ing this to the r-sig-mixed-models mailing
list; it's often better to send to the list where a wider range of
people can answer, and where others can read the answers as well.

> I did find some problem in using the lme4 package.
> In particular, using the scrip (TBRT2011Script.txt) on example data
> TBRTQuality.csv from a webinar available at:
> 
> http://www.extension.org/pages/61006/estimating-heritability-and-blups-for-traits-using-tomato-phenotypic-data#.Ut5B2LTSJpi

Just want to point out the note on that page:

IMPORTANT NOTE: The code provided has been tested for 2.x of R. There
are issues with the code for version 3.0.
> 
> when I use the lmer function I get the following error:
> 
> Error in checkNlevels(reTrms$flist, n = n, control) :
>   grouping factors must have > 1 sampled level
> 
> When I tried to find some hints in in how solve the problem in the
> manual for Package ?lme4? at page 33, third line, it looks that there is
> a printing problem regarding the "option(...)" command description that
> is interrupted. This was a bit frustrating.

  I see that line is too long, but (1) you can see it on your own
computer, better formatted, by typing ?lmerControl in R (when the lme4
package is loaded); (2) the more relevant point is on the previous page:
you want  control=lmerControl(check.nlev.gtr.1="ignore") (and presumably
control=lmerControl(check.nlev.gtr.1="ignore",check.nlev.gtr.5="ignore"), for
completeness.

> 
> Sorry again for bothering you and
> 
> looking forward to hear fro you
> 
> best regards
> 
> Lorenzo
> 
> 
> 
> -- 
> Lorenzo Raggi, Ph.D.
> University of Perugia
> Dept. of Applied Biology
> 
> Borgo XX Giugno, 74
> 06121 Perugia, Italy
> tel.  +39 0755856212
> fax  +39 0755856224


From bbolker at gmail.com  Wed Jan 22 19:30:42 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 22 Jan 2014 13:30:42 -0500
Subject: [R-sig-ME] problem with lme4
In-Reply-To: <01016C07-60C3-42DB-9E05-8E0DE94FCBB5@uzh.ch>
References: <01016C07-60C3-42DB-9E05-8E0DE94FCBB5@uzh.ch>
Message-ID: <52E00E52.4080600@gmail.com>

  [cc'ing to r-sig-mixed-models]

  You haven't given a reproducible example, but I believe that simply
replacing   model.s2 at X with getME(model.s2,"X") in the second line
should work.  I think you should also be able to get the same results
(predictions with and without random values) via the 'REform' argument
to predict() (possibly switching to 'ReForm' more recently): see
`?predict.merMod

On 14-01-22 12:59 PM, Linda Heeb wrote:
> Dear Mr Bolker
> 
> Since the new lme4 package this script doesn?t work any more?.
> 
> model.s2<-glmer(y~licking.duration+CN+(1|nest)+(1|session),data=bold.t, family=poisson)
> bold.t$F0 <- inv.logit(model.s2 at X %*% fixef(model.s2))*100 
> bold.t$F1 <- fitted(model.s2)*100 
> p <- qplot(licking.duration, interactions*100, facets=.~CN, data=bold.t)
> p + theme_bw() + geom_line(aes(y=F0)) + geom_line(aes(y=F1, col=mumID)) + opts(legend.position = "none") 
> 
> 
> Fehler in plogis(x) : 
>   kein Slot des Namens "X" f?r dieses Objekt der Klasse ?glmerMod?
> 
> 
> I hope you can tell my why and show me how I can improve it.
> 
> Cheers, Linda Heeb
> 
>


From jake987722 at hotmail.com  Wed Jan 22 22:50:06 2014
From: jake987722 at hotmail.com (Jake Westfall)
Date: Wed, 22 Jan 2014 14:50:06 -0700
Subject: [R-sig-ME] non-orthogonal contrasts
In-Reply-To: <543DDFD5-C69F-4413-875F-2F081873008D@uni-hamburg.de>
References: <AC638A30-0B3B-4B57-96F6-6895F4B35207@uni-hamburg.de>
	<BAY172-W10BA8E107CAB49D4C3159FCBA40@phx.gbl>,
	<543DDFD5-C69F-4413-875F-2F081873008D@uni-hamburg.de>
Message-ID: <BAY172-W3065CB688DD98B48BF86FDCBA70@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140122/372b9d4e/attachment.pl>

From chatfield at alumni.rice.edu  Thu Jan 23 04:46:56 2014
From: chatfield at alumni.rice.edu (Robert Chatfield)
Date: Wed, 22 Jan 2014 19:46:56 -0800
Subject: [R-sig-ME]
 =?windows-1252?q?Formula_and_Start-values__Any_help_at?=
 =?windows-1252?q?_all=3F=85__Self-contained_example?=
In-Reply-To: <mailman.0.1389610802.32367.r-sig-mixed-models@r-project.org>
References: <mailman.0.1389610802.32367.r-sig-mixed-models@r-project.org>
Message-ID: <E5374544-662F-46A9-8721-8916C8592DA2@alumni.rice.edu>

Dear list,
I still have no idea how to specify or give start values for vectors in a formula for nlmer()
The technique works fine for an approximate method I use in lmer()
(That technique depends heavily on some scaling of variables.)

Here is a self-contained example using some mock data whose origin is described.

I always get one of these two errors when trying to specify the nlmer() model
Error in nlmer(ylong ~ xinter(a, xlong, x0.est) ~ (a | species) + (x0.est |  : 
  not all parameter names are used in the nonlinear model expression
or
Error: s > 0 is not TRUE

Can  I get any help on formula specification or start values?
I have coded in a function with derivatives using deriv() .


> # Example of emissions estimation
> #
> # Define a "true" background concentration ... 100 instnaces
> x0 = rnorm(100, 20, 5)
> # Define a perturbation of the key tracer -- log normal
> xd = rlnorm( 100, log(8), 0.3)
> # Define the measurable, x
> x = x0 + xd
> # Define some "emission factors" linearly relating several dependent variables 
> # as they are related to the key tracer (above background)
> # 10 different dependent variables or "species"
> a = 0.1*rexp(10,rate=0.6)
> # Define the pure response variable (yun ... "y unscaled")
> yun = t( outer( a,xd ) )
> # Add some noise in both the response, both w.r.t. the dependent species 
> #    and also the measurement instance
> for (i in 1:10 ) yun[,i] = yun[,i]*(1+rnorm(100,0.05,0.1))
> for (j in 1:100 ) yun[j,] = yun[j,]*(1+rnorm(10,0.05,0.1))
> #
> y = yun    #  dimension a scaled response and fill it with scaled values (still all positive)
> ysc = vector(mode="numeric", length=10)
> asc = vector(mode="numeric", length=10)
> for ( i in 1:10 ) { 
+    ytemp =  scale(yun[,i], center=F) # get ytemp to define scaling factor
+    y[,i] =  ytemp   # scale
+    ysc[i] = attributes(ytemp)$`scaled:scale`
+    asc[i] = 1/ysc[i]    # useful version of a to relate scaled quantities
+ }
> #
> # Make a "longi" list of the tracer-species, repeating once for every response
> # Note:  normally I use reshape() to do this.  See: ?reshape
> xlong = rep(x,times=10) 
> ylong = as.vector(y) # collapse the matrix into a similar "long" vector
> # Make a factor variable describing each measurement instance and each dependent variable 
> id = as.factor(rep(paste("sample",as.character(1:100),sep=""),times=10))
> species = as.factor(rep(paste("species",as.character(1:10),sep=""),times=100))
> #
> # Create the data-frame for nlmer
> trial.dat = data.frame(xlong=xlong,ylong=ylong,id=id,species=species)
> #
> # Define a formula ... trying to force a vector definition 
> ##  xinterform <- ~a*(xlong+ x0.est)  # previous try
> #
> xinterform <- ~( a * xlong+ x0.est)
> # Explicitly declaring these as vectors gives an error
> # xinterform <- ~( a=vector(mode="numeric",length=1000) * xlong=vector(mode="numeric",length=1000) + x0.est=vector(mode="numeric",length=1000) )
> # Doesn't work:
> # Error in deriv.formula(xinterform, namevec = c("a", "xlong", "x0.est"),  : 
> #  invalid expression in 'FindSubexprs'
> 
> xinter <-  deriv(xinterform, namevec=c("a","xlong","x0.est"),
+                     function.arg=c("a","xlong","x0.est"))
> #
> start.vec = c(a=rep(0.16,times=1000),x0.est=rep(20,times=1000))
> #
> test.nlmer = nlmer( ylong ~ xinter(a,xlong,x0.est)  ~ (a|species) + (x0.est |id),
+      start=start.vec,  data=trial.dat  )
Error in nlmer(ylong ~ xinter(a, xlong, x0.est) ~ (a | species) + (x0.est |  : 
  not all parameter names are used in the nonlinear model expression
> #
> start.vec = unname(start.vec)
> test.nlmer = nlmer( ylong ~ xinter(a,xlong,x0.est)  ~ (a|species) + (x0.est |id),
+      start=start.vec,  data=trial.dat  )
Error: s > 0 is not TRUE

Notes -- how R defines start.vec...


> start.vec = unname(start.vec)
> start.vec[c(1,1000, 1001,2000)]
[1]  0.16  0.16 20.00 20.00

> start.vec = c(a=rep(0.16,times=1000),x0.est=rep(20,times=1000))
> start.vec[c(1,1000, 1001,2000)]
        a1      a1000    x0.est1 x0.est1000 
      0.16       0.16      20.00      20.00 


From teplitsky at mnhn.fr  Thu Jan 23 11:07:05 2014
From: teplitsky at mnhn.fr (Celine Teplitsky)
Date: Thu, 23 Jan 2014 11:07:05 +0100
Subject: [R-sig-ME] distribution for ratios?
Message-ID: <52E0E9C9.8060900@mnhn.fr>

Dear all,

we are trying to run a meta-analysis on factors affecting heritability 
in MCMCglmm. As it is a ratio of 2 variances, the values are bounded 
between one and zero, but can not be interpreted as Success/NbTrials, so 
we can not use a binomial distribution.

So the trouble we're having is knowing which distribution to use? Should 
we just use a Gaussian?

Many thanks in advance for your help

Best regards

Celine

-- 

Celine Teplitsky
D?partement Ecologie et Gestion de la Biodiversit? UMR 7204
Unit? Conservation des Esp?ces, Restauration et Suivi des Populations
Case Postale 51
55 rue Buffon 75005 Paris

Webpage : http://www2.mnhn.fr/cersp/spip.php?rubrique96
Fax : (33-1)-4079-3835
Phone: (33-1)-4079-3443


From highstat at highstat.com  Thu Jan 23 11:18:04 2014
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Thu, 23 Jan 2014 10:18:04 +0000
Subject: [R-sig-ME] distribution for ratios? (Celine Teplitsky)
In-Reply-To: <mailman.1229.1390471647.4563.r-sig-mixed-models@r-project.org>
References: <mailman.1229.1390471647.4563.r-sig-mixed-models@r-project.org>
Message-ID: <52E0EC5C.2070800@highstat.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140123/9ec64722/attachment.pl>

From stevedrd at yahoo.com  Thu Jan 23 12:19:50 2014
From: stevedrd at yahoo.com (Steve Denham)
Date: Thu, 23 Jan 2014 03:19:50 -0800 (PST)
Subject: [R-sig-ME] distribution for ratios?
In-Reply-To: <52E0E9C9.8060900@mnhn.fr>
References: <52E0E9C9.8060900@mnhn.fr>
Message-ID: <1390475990.65213.YahooMailNeo@web140604.mail.bf1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140123/5e425e62/attachment.pl>

From i.m.s.white at ed.ac.uk  Thu Jan 23 12:50:26 2014
From: i.m.s.white at ed.ac.uk (i white)
Date: Thu, 23 Jan 2014 11:50:26 +0000
Subject: [R-sig-ME] distribution for ratios?
In-Reply-To: <52E0E9C9.8060900@mnhn.fr>
References: <52E0E9C9.8060900@mnhn.fr>
Message-ID: <52E10202.5080405@ed.ac.uk>

Treat as Gaussian after logistic or arcsine-square root transformation?

On 01/23/2014 10:07 AM, Celine Teplitsky wrote:
> Dear all,
>
> we are trying to run a meta-analysis on factors affecting heritability
> in MCMCglmm. As it is a ratio of 2 variances, the values are bounded
> between one and zero, but can not be interpreted as Success/NbTrials, so
> we can not use a binomial distribution.
>
> So the trouble we're having is knowing which distribution to use? Should
> we just use a Gaussian?
>
> Many thanks in advance for your help
>
> Best regards
>
> Celine
>

-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From bbolker at gmail.com  Thu Jan 23 19:44:24 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 23 Jan 2014 13:44:24 -0500
Subject: [R-sig-ME] Fwd: Re: problem with lme4
In-Reply-To: <E4FF294B-DA5E-4FAE-A51A-B172EE037958@uzh.ch>
References: <E4FF294B-DA5E-4FAE-A51A-B172EE037958@uzh.ch>
Message-ID: <52E16308.6000602@gmail.com>


 [cc'ing back to r-sig-mixed-models; I strongly prefer to keep
conversations on-list]

  It's a little hard to say without the original data, but I'm pretty
sure your problem is with ggplot rather than lme4.  In particular, you
should probably use geom_line(aes(y=F1, col=factor(ind) )), or
geom_line(aes(y=F1,col=ind, group=ind)); ggplot interprets continuous
and categorical variables differently.


-------- Original Message --------
Subject: 	Re: problem with lme4
Date: 	Thu, 23 Jan 2014 14:25:47 +0100
From: 	Linda Heeb <linda.heeb at uzh.ch>
To: 	Ben Bolker <bbolker at gmail.com>



Thanks for your help, but I still have a problem.
Do you know how I can make a smooth line out of my ?alone" data, like
the blue line that i got from the method=glm? So that the zigzag
disappear (you can see it on the attached picture)

thanks

model.s2<-glmer(number~session+treatment+amount.pup+drink+alone+lick+(1|ind),data=bold.t,
family=poisson)
bold.t$F0 <- inv.logit(getME(model.s2,"X") %*% fixef(model.s2))
bold.t$F1 <- fitted(model.s2)
p <- qplot(alone,geom="smooth", number,data=bold.t, method="glm")
p + theme_bw() + geom_line(aes(y=F0)) + geom_line(aes(y=F1, col=ind )) +
theme(legend.position = "none")




Am 22.01.2014 um 19:30 schrieb Ben Bolker <bbolker at gmail.com
<mailto:bbolker at gmail.com>>:

>  [cc'ing to r-sig-mixed-models]
>
>  You haven't given a reproducible example, but I believe that simply
> replacing   model.s2 at X with getME(model.s2,"X") in the second line
> should work.  I think you should also be able to get the same results
> (predictions with and without random values) via the 'REform' argument
> to predict() (possibly switching to 'ReForm' more recently): see
> `?predict.merMod
>
> On 14-01-22 12:59 PM, Linda Heeb wrote:
>> Dear Mr Bolker
>>
>> Since the new lme4 package this script doesn?t work any more?.
>>
>> model.s2<-glmer(y~licking.duration+CN+(1|nest)+(1|session),data=bold.t,
>> family=poisson)
>> bold.t$F0 <- inv.logit(model.s2 at X %*% fixef(model.s2))*100
>> bold.t$F1 <- fitted(model.s2)*100
>> p <- qplot(licking.duration, interactions*100, facets=.~CN, data=bold.t)
>> p + theme_bw() + geom_line(aes(y=F0)) + geom_line(aes(y=F1,
>> col=mumID)) + opts(legend.position = "none")
>>
>>
>> Fehler in plogis(x) :
>>  kein Slot des Namens "X" f?r dieses Objekt der Klasse ?glmerMod?
>>
>>
>> I hope you can tell my why and show me how I can improve it.
>>
>> Cheers, Linda Heeb
>>
>>
>


From bergh.thijsvanden at gmail.com  Thu Jan 23 21:43:04 2014
From: bergh.thijsvanden at gmail.com (Thijs vanden Bergh)
Date: Thu, 23 Jan 2014 21:43:04 +0100
Subject: [R-sig-ME] mixed effect model question
Message-ID: <CAJkOg=w1hjhS_9bWn+rHp79dac5dnrzCZhgco1ZNB3g7aTXvgQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140123/1c3bdb32/attachment.pl>

From David.Duffy at qimr.edu.au  Fri Jan 24 05:01:40 2014
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Fri, 24 Jan 2014 14:01:40 +1000
Subject: [R-sig-ME] distribution for ratios?
In-Reply-To: <52E0E9C9.8060900@mnhn.fr>
References: <52E0E9C9.8060900@mnhn.fr>
Message-ID: <alpine.LMD.2.00.1401241355330.7150@orpheus.qimr.edu.au>

On Thu, 23 Jan 2014, Celine Teplitsky wrote:

> we are trying to run a meta-analysis on factors affecting heritability in 
> MCMCglmm.

I'm surprised no-one has suggested a bivariate analysis of V_g and 
V_e (gamma, or even gaussian).

"... the so-called co-efficient of heritability, which I regard as one of 
those unfortunate short-cuts, which have often emerged in biometry for 
lack of a more thorough analysis of the data."


| David Duffy (MBBS PhD)
| email: David.Duffy at qimrberghofer.edu.au  ph: INT+61+7+3362-0217 fax: -0101
| Genetic Epidemiology, QIMR Berghofer Institute of Medical Research
| 300 Herston Rd, Brisbane, Queensland 4006, Australia  GPG 4D0B994A


From iaingallagher at btopenworld.com  Fri Jan 24 11:48:13 2014
From: iaingallagher at btopenworld.com (Iain Gallagher)
Date: Fri, 24 Jan 2014 10:48:13 +0000 (GMT)
Subject: [R-sig-ME] degrees of freedom in mixed model
Message-ID: <1390560493.2093.YahooMailNeo@web87905.mail.ir2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140124/d8982a27/attachment.pl>

From S.Ellison at LGCGroup.com  Fri Jan 24 14:40:07 2014
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Fri, 24 Jan 2014 13:40:07 +0000
Subject: [R-sig-ME] degrees of freedom in mixed model
In-Reply-To: <1390560493.2093.YahooMailNeo@web87905.mail.ir2.yahoo.com>
References: <1390560493.2093.YahooMailNeo@web87905.mail.ir2.yahoo.com>
Message-ID: <A4E5A0B016B8CB41A485FC629B633CED592F6BD298@GOLD.corp.lgc-group.com>


> library(lme4)
> model1 <- lmer(value~group + (1|animal), data=bip)
> summary(model1)
> 
.......
> so I'd then have:
> 
> qf(0.95,3,5) or qf(0.95,3,4)
> 
> for my critical F value?
> 
> Any advice (incuding whether the appraoch is right) would be useful.

It's the wrong approach.

You are using lmer, which uses maximum likelihood estimation, not classical sums of squares. The degrees of freedom don't mean the same thing, and the distribution of REML estimates of variance isn't necessarily chi-squared. So F is interpretable in the same way as it would be in classical anova.

If you want p-values from an lmer model, you could get hold of the lmerTest package. Other recommended approaches include variants on MCMC. There is a great deal of controversy on this point, though; try Googling "p-values from lmer" with particular attention to anything by Douglas Bates (the package author). You _should_ find enough to make you worry that the method used by lmerTest (which as I understand it implements a method used by SAS) comes with quite strong theoretical objections. I am quite sure the lmerTest authors know that perfectly well and offer lmerTest as a package for those who want to find out or for those whose management insist on a SAS-compatible answer. But if I read correctly, that doesn't make it the right thing to do

[Caveat - I'm a chemist. I could be wrong about this]

> 
> Best
> 
> iain
> 
> 	[[alternative HTML version deleted]]



*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From jbaldwin at fs.fed.us  Fri Jan 24 16:40:32 2014
From: jbaldwin at fs.fed.us (Baldwin, Jim -FS)
Date: Fri, 24 Jan 2014 15:40:32 +0000
Subject: [R-sig-ME] degrees of freedom in mixed model
In-Reply-To: <A4E5A0B016B8CB41A485FC629B633CED592F6BD298@GOLD.corp.lgc-group.com>
References: <1390560493.2093.YahooMailNeo@web87905.mail.ir2.yahoo.com>
	<A4E5A0B016B8CB41A485FC629B633CED592F6BD298@GOLD.corp.lgc-group.com>
Message-ID: <DDC5EC9B78340042B0D5A0C3789D456919A7DB2D@001FSN2MPN1-061.001f.mgd2.msft.net>

S Ellison:  Despite you being a chemist, I think you're at least mostly correct.  But from the construction of my statement, it's obvious that I am a statistician and I'm allowed, by law, to be wrong 5% of the time.  And if I claim to be a Frequentist, I don't even have to identify which of my particular statements are incorrect.

Jim

Jim Baldwin
Station Statistician
Pacific Southwest Research Station
USDA Forest Service

-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of S Ellison
Sent: Friday, January 24, 2014 5:40 AM
To: Iain Gallagher; r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] degrees of freedom in mixed model


> library(lme4)
> model1 <- lmer(value~group + (1|animal), data=bip)
> summary(model1)
>
.......
> so I'd then have:
>
> qf(0.95,3,5) or qf(0.95,3,4)
>
> for my critical F value?
>
> Any advice (incuding whether the appraoch is right) would be useful.

It's the wrong approach.

You are using lmer, which uses maximum likelihood estimation, not classical sums of squares. The degrees of freedom don't mean the same thing, and the distribution of REML estimates of variance isn't necessarily chi-squared. So F is interpretable in the same way as it would be in classical anova.

If you want p-values from an lmer model, you could get hold of the lmerTest package. Other recommended approaches include variants on MCMC. There is a great deal of controversy on this point, though; try Googling "p-values from lmer" with particular attention to anything by Douglas Bates (the package author). You _should_ find enough to make you worry that the method used by lmerTest (which as I understand it implements a method used by SAS) comes with quite strong theoretical objections. I am quite sure the lmerTest authors know that perfectly well and offer lmerTest as a package for those who want to find out or for those whose management insist on a SAS-compatible answer. But if I read correctly, that doesn't make it the right thing to do

[Caveat - I'm a chemist. I could be wrong about this]

>
> Best
>
> iain
>
>       [[alternative HTML version deleted]]



*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:17}}


From jake987722 at hotmail.com  Fri Jan 24 19:07:01 2014
From: jake987722 at hotmail.com (Jake Westfall)
Date: Fri, 24 Jan 2014 11:07:01 -0700
Subject: [R-sig-ME] degrees of freedom in mixed model
In-Reply-To: <DDC5EC9B78340042B0D5A0C3789D456919A7DB2D@001FSN2MPN1-061.001f.mgd2.msft.net>
References: <1390560493.2093.YahooMailNeo@web87905.mail.ir2.yahoo.com>,
	<A4E5A0B016B8CB41A485FC629B633CED592F6BD298@GOLD.corp.lgc-group.com>,
	<DDC5EC9B78340042B0D5A0C3789D456919A7DB2D@001FSN2MPN1-061.001f.mgd2.msft.net>
Message-ID: <BAY172-W31BAD8FDA24790F82CB7FDCBA10@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140124/0b41b328/attachment.pl>

From bbolker at gmail.com  Fri Jan 24 19:19:40 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 24 Jan 2014 13:19:40 -0500
Subject: [R-sig-ME] degrees of freedom in mixed model
In-Reply-To: <BAY172-W31BAD8FDA24790F82CB7FDCBA10@phx.gbl>
References: <1390560493.2093.YahooMailNeo@web87905.mail.ir2.yahoo.com>,
	<A4E5A0B016B8CB41A485FC629B633CED592F6BD298@GOLD.corp.lgc-group.com>,
	<DDC5EC9B78340042B0D5A0C3789D456919A7DB2D@001FSN2MPN1-061.001f.mgd2.msft.net>
	<BAY172-W31BAD8FDA24790F82CB7FDCBA10@phx.gbl>
Message-ID: <52E2AEBC.9050703@gmail.com>

On 14-01-24 01:07 PM, Jake Westfall wrote:
> Meh... My feeling is that the amount of controversy on this point is
> rather more limited than S Ellison lets on. Of course Bates is
> (famously, at this point) deeply skeptical about the approximate
> degrees of freedom approaches, but I get the impression that few of
> the rest of us who have spent some time thinking about the matter
> have any sort of strong feelings about it. The Satterthwaite method
> (implemented in lmerTest) is widely used, well understood, and
> basically seems to work quite well for controlling error rates in
> most cases, based on simulations. I think if you are wanting to
> scrutinize your model and the tests of the coefficients therein,
> there are far bigger fish to fry than worrying about the issue of
> approximate DFs vs. bootstrapping vs. MCMC vs. ... Jake

    I more or less agree.  The issue with F distributions, degrees of
freedom, etc etc., is mostly a problem with complex designs that don't
fit into the classical method-of-moments/ANOVA paradigm (R-side effects
[which lme4 doesn't do yet], crossed and partially crossed random
effects, etc.).  In simple cases (as in the example here), the results
of (restricted) ML analyses should more or less line up with the
classical results.  In addition to lmerTest, as pointed out by S?ren
Hojsgaard in the original thread on r-help, the Kenward-Roger
approximation is available in the PBKRtest package ...

  If you asked me about 'denominator df' calculations for GLMMs I would
be considerably more pessimistic ...

Schaalje, G., J. McBride, and G. Fellingham. 2002. ?Adequacy of
Approximations to Distributions of Test Statistics in Complex Mixed
Linear Models.? Journal of Agricultural, Biological & Environmental
Statistics 7 (14): 512?24.
http://www.ingentaconnect.com/content/asa/jabes/2002/00000007/00000004/art00004.


>> From: jbaldwin at fs.fed.us To: S.Ellison at LGCGroup.com;
>> iaingallagher at btopenworld.com; r-sig-mixed-models at r-project.org 
>> Date: Fri, 24 Jan 2014 15:40:32 +0000 Subject: Re: [R-sig-ME]
>> degrees of freedom in mixed model
>> 
>> S Ellison:  Despite you being a chemist, I think you're at least
>> mostly correct.  But from the construction of my statement, it's
>> obvious that I am a statistician and I'm allowed, by law, to be
>> wrong 5% of the time.  And if I claim to be a Frequentist, I don't
>> even have to identify which of my particular statements are
>> incorrect.
>> 
>> Jim
>> 
>> Jim Baldwin Station Statistician Pacific Southwest Research
>> Station USDA Forest Service
>> 
>> -----Original Message----- From:
>> r-sig-mixed-models-bounces at r-project.org
>> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of S
>> Ellison Sent: Friday, January 24, 2014 5:40 AM To: Iain Gallagher;
>> r-sig-mixed-models at r-project.org Subject: Re: [R-sig-ME] degrees of
>> freedom in mixed model
>> 
>> 
>>> library(lme4) model1 <- lmer(value~group + (1|animal), data=bip) 
>>> summary(model1)
>>> 
>> .......
>>> so I'd then have:
>>> 
>>> qf(0.95,3,5) or qf(0.95,3,4)
>>> 
>>> for my critical F value?
>>> 
>>> Any advice (incuding whether the appraoch is right) would be
>>> useful.
>> 
>> It's the wrong approach.
>> 
>> You are using lmer, which uses maximum likelihood estimation, not
>> classical sums of squares. The degrees of freedom don't mean the
>> same thing, and the distribution of REML estimates of variance
>> isn't necessarily chi-squared. So F is interpretable in the same
>> way as it would be in classical anova.
>> 
>> If you want p-values from an lmer model, you could get hold of the
>> lmerTest package. Other recommended approaches include variants on
>> MCMC. There is a great deal of controversy on this point, though;
>> try Googling "p-values from lmer" with particular attention to
>> anything by Douglas Bates (the package author). You _should_ find
>> enough to make you worry that the method used by lmerTest (which as
>> I understand it implements a method used by SAS) comes with quite
>> strong theoretical objections. I am quite sure the lmerTest authors
>> know that perfectly well and offer lmerTest as a package for those
>> who want to find out or for those whose management insist on a
>> SAS-compatible answer. But if I read correctly, that doesn't make
>> it the right thing to do
>> 
>> [Caveat - I'm a chemist. I could be wrong about this]
>> 
>>> 
>>> Best
>>> 
>>> iain
>>> 
>>> [[alternative HTML version deleted]]
>> 
>> 
>> 
>> *******************************************************************
>>
>> 
This email and any attachments are confidential. Any u...{{dropped:9}}
> 
> _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From bbolker at gmail.com  Fri Jan 24 19:28:50 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 24 Jan 2014 13:28:50 -0500
Subject: [R-sig-ME] glmmadmb mcmc - scale of variance components
In-Reply-To: <52D6607B.7080703@ibv.uio.no>
References: <52D6607B.7080703@ibv.uio.no>
Message-ID: <52E2B0E2.8010307@gmail.com>

On 14-01-15 05:18 AM, Torbj?rn Ergon wrote:
> Dear list,
> 
> Could anyone tell me what is the scale of the variance components
> (tempL.x parameters) in the mcmc output from glmmadmb (version 0.7.7)?
> 
> From previous posts it seems that they are supposed to be log of the
> standard deviation in the random effects distribution, but I don't get
> any negative values (I expect variance <1) so I guess that has changed.
> Are they variances or standard deviations? I presume the
> variances/standard deviations are on the scale of the linear predictor.

 The vignette (vignette("glmmADMB",package="glmmADMB")) says (p. 15)
that tmpL contains the RE variances on the standard-deviation scale.
(And yes, this is on the scale of the linear predictor.)

   (I'm not claiming this information is easy to find!)

  cheers
    Ben Bolker


From iaingallagher at btopenworld.com  Fri Jan 24 23:03:28 2014
From: iaingallagher at btopenworld.com (Iain Gallagher)
Date: Fri, 24 Jan 2014 22:03:28 +0000 (GMT)
Subject: [R-sig-ME] degrees of freedom in mixed model
In-Reply-To: <52E2AEBC.9050703@gmail.com>
References: <1390560493.2093.YahooMailNeo@web87905.mail.ir2.yahoo.com>,
	<A4E5A0B016B8CB41A485FC629B633CED592F6BD298@GOLD.corp.lgc-group.com>,
	<DDC5EC9B78340042B0D5A0C3789D456919A7DB2D@001FSN2MPN1-061.001f.mgd2.msft.net>
	<BAY172-W31BAD8FDA24790F82CB7FDCBA10@phx.gbl>
	<52E2AEBC.9050703@gmail.com>
Message-ID: <1390601008.90102.YahooMailNeo@web87904.mail.ir2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140124/da54c21f/attachment.pl>

From Ariel.Muldoon at oregonstate.edu  Fri Jan 24 23:44:20 2014
From: Ariel.Muldoon at oregonstate.edu (Muldoon, Ariel)
Date: Fri, 24 Jan 2014 22:44:20 +0000
Subject: [R-sig-ME] degrees of freedom in mixed model
In-Reply-To: <52E2AEBC.9050703@gmail.com>
References: <1390560493.2093.YahooMailNeo@web87905.mail.ir2.yahoo.com>,
	<A4E5A0B016B8CB41A485FC629B633CED592F6BD298@GOLD.corp.lgc-group.com>,
	<DDC5EC9B78340042B0D5A0C3789D456919A7DB2D@001FSN2MPN1-061.001f.mgd2.msft.net>
	<BAY172-W31BAD8FDA24790F82CB7FDCBA10@phx.gbl>
	<52E2AEBC.9050703@gmail.com>
Message-ID: <6057235ECEA65D4DBC2696B69BA378A6088A37BE@EX3.oregonstate.edu>

A small addition to the discussion:  I was recently reading Stroup's "Generalized Linear Mixed Models", which discusses this degrees of freedom issue for LMM's a bit.  For models outside the more "classical" paradigm, it seems that the Kenward-Roger correction controls the type I error rate but the Satterthwaite correction does not (although I did not go on to read the original papers on the subject). 

Ariel

-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Ben Bolker
Sent: Friday, January 24, 2014 10:20 AM
To: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] degrees of freedom in mixed model

On 14-01-24 01:07 PM, Jake Westfall wrote:
> Meh... My feeling is that the amount of controversy on this point is 
> rather more limited than S Ellison lets on. Of course Bates is 
> (famously, at this point) deeply skeptical about the approximate 
> degrees of freedom approaches, but I get the impression that few of 
> the rest of us who have spent some time thinking about the matter have 
> any sort of strong feelings about it. The Satterthwaite method 
> (implemented in lmerTest) is widely used, well understood, and 
> basically seems to work quite well for controlling error rates in most 
> cases, based on simulations. I think if you are wanting to scrutinize 
> your model and the tests of the coefficients therein, there are far 
> bigger fish to fry than worrying about the issue of approximate DFs 
> vs. bootstrapping vs. MCMC vs. ... Jake

    I more or less agree.  The issue with F distributions, degrees of freedom, etc etc., is mostly a problem with complex designs that don't fit into the classical method-of-moments/ANOVA paradigm (R-side effects [which lme4 doesn't do yet], crossed and partially crossed random effects, etc.).  In simple cases (as in the example here), the results of (restricted) ML analyses should more or less line up with the classical results.  In addition to lmerTest, as pointed out by S?ren Hojsgaard in the original thread on r-help, the Kenward-Roger approximation is available in the PBKRtest package ...

  If you asked me about 'denominator df' calculations for GLMMs I would be considerably more pessimistic ...

Schaalje, G., J. McBride, and G. Fellingham. 2002. "Adequacy of Approximations to Distributions of Test Statistics in Complex Mixed Linear Models." Journal of Agricultural, Biological & Environmental Statistics 7 (14): 512-24.
http://www.ingentaconnect.com/content/asa/jabes/2002/00000007/00000004/art00004.


>> From: jbaldwin at fs.fed.us To: S.Ellison at LGCGroup.com; 
>> iaingallagher at btopenworld.com; r-sig-mixed-models at r-project.org
>> Date: Fri, 24 Jan 2014 15:40:32 +0000 Subject: Re: [R-sig-ME] degrees 
>> of freedom in mixed model
>> 
>> S Ellison:  Despite you being a chemist, I think you're at least 
>> mostly correct.  But from the construction of my statement, it's 
>> obvious that I am a statistician and I'm allowed, by law, to be wrong 
>> 5% of the time.  And if I claim to be a Frequentist, I don't even 
>> have to identify which of my particular statements are incorrect.
>> 
>> Jim
>> 
>> Jim Baldwin Station Statistician Pacific Southwest Research Station 
>> USDA Forest Service
>> 
>> -----Original Message----- From:
>> r-sig-mixed-models-bounces at r-project.org
>> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of S 
>> Ellison Sent: Friday, January 24, 2014 5:40 AM To: Iain Gallagher; 
>> r-sig-mixed-models at r-project.org Subject: Re: [R-sig-ME] degrees of 
>> freedom in mixed model
>> 
>> 
>>> library(lme4) model1 <- lmer(value~group + (1|animal), data=bip)
>>> summary(model1)
>>> 
>> .......
>>> so I'd then have:
>>> 
>>> qf(0.95,3,5) or qf(0.95,3,4)
>>> 
>>> for my critical F value?
>>> 
>>> Any advice (incuding whether the appraoch is right) would be useful.
>> 
>> It's the wrong approach.
>> 
>> You are using lmer, which uses maximum likelihood estimation, not 
>> classical sums of squares. The degrees of freedom don't mean the same 
>> thing, and the distribution of REML estimates of variance isn't 
>> necessarily chi-squared. So F is interpretable in the same way as it 
>> would be in classical anova.
>> 
>> If you want p-values from an lmer model, you could get hold of the 
>> lmerTest package. Other recommended approaches include variants on 
>> MCMC. There is a great deal of controversy on this point, though; try 
>> Googling "p-values from lmer" with particular attention to anything 
>> by Douglas Bates (the package author). You _should_ find enough to 
>> make you worry that the method used by lmerTest (which as I 
>> understand it implements a method used by SAS) comes with quite 
>> strong theoretical objections. I am quite sure the lmerTest authors 
>> know that perfectly well and offer lmerTest as a package for those 
>> who want to find out or for those whose management insist on a 
>> SAS-compatible answer. But if I read correctly, that doesn't make it 
>> the right thing to do
>> 
>> [Caveat - I'm a chemist. I could be wrong about this]
>> 
>>> 
>>> Best
>>> 
>>> iain
>>> 
>>> [[alternative HTML version deleted]]
>> 
>> 
>> 
>> *******************************************************************
>>
>> 
This email and any attachments are confidential. Any u.....{{dropped:11}}


From jake987722 at hotmail.com  Fri Jan 24 23:56:12 2014
From: jake987722 at hotmail.com (Jake Westfall)
Date: Fri, 24 Jan 2014 15:56:12 -0700
Subject: [R-sig-ME] degrees of freedom in mixed model
In-Reply-To: <6057235ECEA65D4DBC2696B69BA378A6088A37BE@EX3.oregonstate.edu>
References: <1390560493.2093.YahooMailNeo@web87905.mail.ir2.yahoo.com>, ,
	<A4E5A0B016B8CB41A485FC629B633CED592F6BD298@GOLD.corp.lgc-group.com>,
	,
	<DDC5EC9B78340042B0D5A0C3789D456919A7DB2D@001FSN2MPN1-061.001f.mgd2.msft.net>,
	<BAY172-W31BAD8FDA24790F82CB7FDCBA10@phx.gbl>,
	<52E2AEBC.9050703@gmail.com>,
	<6057235ECEA65D4DBC2696B69BA378A6088A37BE@EX3.oregonstate.edu>
Message-ID: <BAY172-W37744CC3C3274CF9CD77CCCBA10@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140124/a94ccfd3/attachment.pl>

From 538280 at gmail.com  Sat Jan 25 00:04:55 2014
From: 538280 at gmail.com (Greg Snow)
Date: Fri, 24 Jan 2014 16:04:55 -0700
Subject: [R-sig-ME] degrees of freedom in mixed model
In-Reply-To: <52E2AEBC.9050703@gmail.com>
References: <1390560493.2093.YahooMailNeo@web87905.mail.ir2.yahoo.com>
	<A4E5A0B016B8CB41A485FC629B633CED592F6BD298@GOLD.corp.lgc-group.com>
	<DDC5EC9B78340042B0D5A0C3789D456919A7DB2D@001FSN2MPN1-061.001f.mgd2.msft.net>
	<BAY172-W31BAD8FDA24790F82CB7FDCBA10@phx.gbl>
	<52E2AEBC.9050703@gmail.com>
Message-ID: <CAFEqCdz5gB3b6YcQV1X1SW_0PzF7pQxpPta2X3G5KwYm9NA9mw@mail.gmail.com>

This post:  https://stat.ethz.ch/pipermail/r-sig-mixed-models/2009q1/001819.html
from a few years ago simulates data (based on a real dataset) and
computes a p-value, then looks at the distribution of the p-value.  In
some cases the p-value was distributed uniformly (when the null was
true) and so the approximation was reasonble, in another case the
p-values were very non-uniform (when the null was true), so for that
case the approximation failed.  It also shows that instead of worrying
about the exact distribution, you can just use the distribution
simulated from the null.

Similar simulations could be done to compare the above suggestions for
the structure of the data and questions of interest.

On Fri, Jan 24, 2014 at 11:19 AM, Ben Bolker <bbolker at gmail.com> wrote:
> On 14-01-24 01:07 PM, Jake Westfall wrote:
>> Meh... My feeling is that the amount of controversy on this point is
>> rather more limited than S Ellison lets on. Of course Bates is
>> (famously, at this point) deeply skeptical about the approximate
>> degrees of freedom approaches, but I get the impression that few of
>> the rest of us who have spent some time thinking about the matter
>> have any sort of strong feelings about it. The Satterthwaite method
>> (implemented in lmerTest) is widely used, well understood, and
>> basically seems to work quite well for controlling error rates in
>> most cases, based on simulations. I think if you are wanting to
>> scrutinize your model and the tests of the coefficients therein,
>> there are far bigger fish to fry than worrying about the issue of
>> approximate DFs vs. bootstrapping vs. MCMC vs. ... Jake
>
>     I more or less agree.  The issue with F distributions, degrees of
> freedom, etc etc., is mostly a problem with complex designs that don't
> fit into the classical method-of-moments/ANOVA paradigm (R-side effects
> [which lme4 doesn't do yet], crossed and partially crossed random
> effects, etc.).  In simple cases (as in the example here), the results
> of (restricted) ML analyses should more or less line up with the
> classical results.  In addition to lmerTest, as pointed out by S?ren
> Hojsgaard in the original thread on r-help, the Kenward-Roger
> approximation is available in the PBKRtest package ...
>
>   If you asked me about 'denominator df' calculations for GLMMs I would
> be considerably more pessimistic ...
>
> Schaalje, G., J. McBride, and G. Fellingham. 2002. ?Adequacy of
> Approximations to Distributions of Test Statistics in Complex Mixed
> Linear Models.? Journal of Agricultural, Biological & Environmental
> Statistics 7 (14): 512?24.
> http://www.ingentaconnect.com/content/asa/jabes/2002/00000007/00000004/art00004.
>
>
>>> From: jbaldwin at fs.fed.us To: S.Ellison at LGCGroup.com;
>>> iaingallagher at btopenworld.com; r-sig-mixed-models at r-project.org
>>> Date: Fri, 24 Jan 2014 15:40:32 +0000 Subject: Re: [R-sig-ME]
>>> degrees of freedom in mixed model
>>>
>>> S Ellison:  Despite you being a chemist, I think you're at least
>>> mostly correct.  But from the construction of my statement, it's
>>> obvious that I am a statistician and I'm allowed, by law, to be
>>> wrong 5% of the time.  And if I claim to be a Frequentist, I don't
>>> even have to identify which of my particular statements are
>>> incorrect.
>>>
>>> Jim
>>>
>>> Jim Baldwin Station Statistician Pacific Southwest Research
>>> Station USDA Forest Service
>>>
>>> -----Original Message----- From:
>>> r-sig-mixed-models-bounces at r-project.org
>>> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of S
>>> Ellison Sent: Friday, January 24, 2014 5:40 AM To: Iain Gallagher;
>>> r-sig-mixed-models at r-project.org Subject: Re: [R-sig-ME] degrees of
>>> freedom in mixed model
>>>
>>>
>>>> library(lme4) model1 <- lmer(value~group + (1|animal), data=bip)
>>>> summary(model1)
>>>>
>>> .......
>>>> so I'd then have:
>>>>
>>>> qf(0.95,3,5) or qf(0.95,3,4)
>>>>
>>>> for my critical F value?
>>>>
>>>> Any advice (incuding whether the appraoch is right) would be
>>>> useful.
>>>
>>> It's the wrong approach.
>>>
>>> You are using lmer, which uses maximum likelihood estimation, not
>>> classical sums of squares. The degrees of freedom don't mean the
>>> same thing, and the distribution of REML estimates of variance
>>> isn't necessarily chi-squared. So F is interpretable in the same
>>> way as it would be in classical anova.
>>>
>>> If you want p-values from an lmer model, you could get hold of the
>>> lmerTest package. Other recommended approaches include variants on
>>> MCMC. There is a great deal of controversy on this point, though;
>>> try Googling "p-values from lmer" with particular attention to
>>> anything by Douglas Bates (the package author). You _should_ find
>>> enough to make you worry that the method used by lmerTest (which as
>>> I understand it implements a method used by SAS) comes with quite
>>> strong theoretical objections. I am quite sure the lmerTest authors
>>> know that perfectly well and offer lmerTest as a package for those
>>> who want to find out or for those whose management insist on a
>>> SAS-compatible answer. But if I read correctly, that doesn't make
>>> it the right thing to do
>>>
>>> [Caveat - I'm a chemist. I could be wrong about this]
>>>
>>>>
>>>> Best
>>>>
>>>> iain
>>>>
>>>> [[alternative HTML version deleted]]
>>>
>>>
>>>
>>> *******************************************************************
>>>
>>>
> This email and any attachments are confidential. Any u...{{dropped:9}}
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From bbolker at gmail.com  Sat Jan 25 20:45:05 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 25 Jan 2014 19:45:05 +0000 (UTC)
Subject: [R-sig-ME]
	=?utf-8?q?Formula_and_Start-values__Any_help_at_all=3F?=
	=?utf-8?q?=E2=80=A6_Self-contained_example?=
References: <mailman.0.1389610802.32367.r-sig-mixed-models@r-project.org>
	<E5374544-662F-46A9-8721-8916C8592DA2@alumni.rice.edu>
Message-ID: <loom.20140125T204029-820@post.gmane.org>

Robert Chatfield <chatfield at ...> writes:

>  Dear list, I still have no idea how to specify or give start values
> for vectors in a formula for nlmer() The technique works fine for an
> approximate method I use in lmer() (That technique depends heavily
> on some scaling of variables.)  Here is a self-contained example
> using some mock data whose origin is described.

  I don't know if I got sensible answers or not, but I seem to have
succeeded in fitting a model.  The key changes were: (1) starting
parameters are *single* values (these are starting values for the
mean/central values of the parameters, not of the random effects);
(2) leaving "xlong" out of namevec in the deriv() call, because it's
a predictor variable, not a parameter (you don't want its gradient)
                                             
Slight modifications:

## Example of emissions estimation

set.seed(101)
## Define a "true" background concentration ... 100 instnaces
x0 = rnorm(100, 20, 5)
## Define a perturbation of the key tracer -- log normal
xd = rlnorm( 100, log(8), 0.3)
## Define the measurable, x
x = x0 + xd
## Define some "emission factors" linearly relating several dependent variables 
## as they are related to the key tracer (above background)
## 10 different dependent variables or "species"
a = 0.1*rexp(10,rate=0.6)
## Define the pure response variable (yun ... "y unscaled")
yun = t( outer( a,xd ) )
## Add some noise in both the response, both w.r.t. the dependent species 
##    and also the measurement instance
## for (i in 1:10 ) yun[,i] = yun[,i]*(1+rnorm(100,0.05,0.1))
## for (j in 1:100 ) yun[j,] = yun[j,]*(1+rnorm(10,0.05,0.1))
yun <- sweep(yun,1,rnorm(nrow(yun),0.05,0.1),"*")
yun <- sweep(yun,2,rnorm(ncol(yun),0.05,0.1),"*")
##
y = yun    
##  dimension a scaled response and fill it with scaled values
##    (still all positive)
ysc = vector(mode="numeric", length=10)
asc = vector(mode="numeric", length=10)
for ( i in 1:10 ) { 
    ytemp =  scale(yun[,i], center=FALSE) # get ytemp to define scaling factor
    y[,i] =  ytemp   # scale
    ysc[i] = attributes(ytemp)$`scaled:scale`
    asc[i] = 1/ysc[i]    # useful version of a to relate scaled quantities
}
##
## Make a "longi" list of the tracer-species, repeating once for every response
## Note:  normally I use reshape() to do this.  See: ?reshape
xlong = rep(x,times=10) 
ylong = as.vector(y) # collapse the matrix into a similar "long" vector
## Make a factor variable describing each measurement 
## instance and each dependent variable 
id = as.factor(rep(paste("sample",as.character(1:100),sep=""),times=10))
species = as.factor(rep(paste("species",as.character(1:10),sep=""),times=100))
##
## Create the data-frame for nlmer
trial.dat = data.frame(xlong=xlong,ylong=ylong,id=id,species=species)
##
xinterform <- ~( a * xlong+ x0.est)
xinter <-  deriv(xinterform, namevec=c("a","x0.est"),
                 function.arg=c("a","xlong","x0.est"))
##
library(lme4)
start.vec <- c(a=0.16,x0.est=20)
##
test.nlmer = nlmer( ylong ~ 
    xinter(a,xlong,x0.est)  ~ (a|species) + (x0.est |id),
    start=start.vec,  data=trial.dat  )


From bbolker at gmail.com  Sun Jan 26 17:44:49 2014
From: bbolker at gmail.com (lme4 maintainer)
Date: Sun, 26 Jan 2014 11:44:49 -0500
Subject: [R-sig-ME] [Lme4-authors] Applying 'predict' to glmer model
	object?
In-Reply-To: <CAFyVWTQ4DW+=yBJNVFA_JrmCuFL5bUj4BK1qU=n5wNOcCQUVZA@mail.gmail.com>
References: <CAFyVWTQ4DW+=yBJNVFA_JrmCuFL5bUj4BK1qU=n5wNOcCQUVZA@mail.gmail.com>
Message-ID: <52E53B81.7000207@lists.r-forge.r-project.org>

On 14-01-26 08:10 AM, Jay Ulfelder wrote:
> Hi, folks:
> 
> Thanks so much for putting together a great package.
> 


 [I'm cc'ing back to r-sig-mixed-models because I think that's a more
appropriate place ...]

> I'm trying to estimate a logistic regression model with random
> intercepts for countries from country-year TSCS data and then use that
> model to generate out-of-sample predictions. I'm having no trouble with
> the first step but can't seem to do the second. Here's what I'm seeing in R:
> 

  A reproducible example <http://tinyurl.com/reproducible-000> would be
nice -- so would the results of sessionInfo() -- but we'll see what we
can do.

>> glmer.f <- formula(cou.a.d.1 ~ colbrit + colfrnc + colespn + postcw +
> ageln + xxxcimrln + 
> +   cou.tries5d + world.ln + region.ln + slowgrowth + polcat2 + polcat3
> + polcat7 + durableln + 
> +   elcelethc + nld.any.1 + civconc + ( 1 | country) )
> 
>> glmer.mod <- glmer(glmer.f, family = binomial, data = train, na.action
> = na.exclude)
> 
>> test$glmer.p <- predict(glmer.mod, newdata = test, type = "response",
> na.action = na.exclude)
> Error in UseMethod("predict") : 
>   no applicable method for 'predict' applied to an object of class "mer"
> 
> Can you please tell me what I'm missing?

  It's hard to know exactly.  It seems as though you might be using an
older version of lme4 (for which the predict() method isn't implemented,
because older (pre-1.0) versions of lme4 produce objects of class 'mer';
newer versions produce 'merMod' objects.

  Ben Bolker


From chatfield at alumni.rice.edu  Mon Jan 27 00:09:34 2014
From: chatfield at alumni.rice.edu (Robert Chatfield)
Date: Sun, 26 Jan 2014 15:09:34 -0800
Subject: [R-sig-ME] =?windows-1252?q?Formula_and_Start-values__Any_help_at?=
 =?windows-1252?q?_all=3F=85_Yes!?=
In-Reply-To: <loom.20140125T204029-820@post.gmane.org>
References: <mailman.0.1389610802.32367.r-sig-mixed-models@r-project.org>
	<E5374544-662F-46A9-8721-8916C8592DA2@alumni.rice.edu>
	<loom.20140125T204029-820@post.gmane.org>
Message-ID: <5E3D226D-932B-4405-8D38-C70BC72774D9@alumni.rice.edu>

Partial conclusions:

Ben's two observations have been a tremendous help to me.  Use of nlmer
following (1) and (2) allow me to get a solution in the case of my original problem,
although some questions of interpretation still remain.

(1) Use single values as starting values, i.e., the mean/central values of the parameters

(2) including sensitivities to parameters only (not to additional independent variables)
in the deriv() function.

In summary for this problem, ... 
=> Getting (apparently) good  x0.est values  from ylong ~ ( a*(xlong  - x0.est) )
is the major accomplishment of nlmer()...  one can work out the other questions.
=> Such success is likely to be data-set dependent since the problem can easily
be ill conditioned (I surmise). 

==============================================================
Now, 
If anyone has been following the discussion of the idealized examples, there are
some flaws in what's in the correspondence that remain 
and that confuse my discussion and his for those who might follow.
Ask me for 

(a) Importantly, the non-linear formula is best expressed 
xinterform <- ~( a*(xlong  - x0.est) )
(note sign of x0.est is changed and the parentheses
are all-important. a*xlong - x0.est is linear.)

(b) Analysis of the idealized problem suggests that it is 
poorly conditioned, with the effects of x0.est and a being 
too much correlated.  
=> I surmise that the success of my use of nlmer with
the original data derives from the probability distributions 
involved.  More work would be needed to quantify this
understanding.


(c) Details -- I used "outer" incorrectly to express 
for (i in 1:10 ) yun[,i] = yun[,i]*(1+rnorm(100,0.15,0.1))
for (j in 1:100 ) yun[j,] = yun[j,]*(1+rnorm(10,0.15,0.1))
while Ben left out the important (1+rnorm( )    ) in his version using sweep.
(d) Details: I still have difficulty in getting meaningfully non-0  random effects for the
slopes "a" ... Again, this could be my bad coding or it could reflect the
distributions of the y variables as they are normalized. 

Thanks much from this newbie, ... fixed effects models have
solved that "there ought to be a way" quandary for me.

Bob Chatfield

On Jan 25, 2014, at 11:45 AM, Ben Bolker <bbolker at gmail.com> wrote:

> Robert Chatfield <chatfield at ...> writes:
> 
>> Dear list, I still have no idea how to specify or give start values
>> for vectors in a formula for nlmer() The technique works fine for an
>> approximate method I use in lmer() (That technique depends heavily
>> on some scaling of variables.)  Here is a self-contained example
>> using some mock data whose origin is described.
> 
>  I don't know if I got sensible answers or not, but I seem to have
> succeeded in fitting a model.  The key changes were: (1) starting
> parameters are *single* values (these are starting values for the
> mean/central values of the parameters, not of the random effects);
> (2) leaving "xlong" out of namevec in the deriv() call, because it's
> a predictor variable, not a parameter (you don't want its gradient)
> 


From spa268 at nyu.edu  Mon Jan 27 08:51:13 2014
From: spa268 at nyu.edu (Stephen Politzer-Ahles)
Date: Mon, 27 Jan 2014 11:51:13 +0400
Subject: [R-sig-ME] Maximal random-effects lmer not converging
Message-ID: <CAJT2k_-6-DYt-KDF6g4nEYd=BF0FyLQ8kVssOSBxgbeQrrOhVg@mail.gmail.com>

Hello,

I am trying to model a somewhat complicated dataset (which includes a
2x2x4 interaction) with maximal random effects, based on the
suggestions from Barr et al. (2013). The maximal model is of course
not converging, and there are several things I don't understand about
how to proceed.

1. I've seen several suggestions that, when a model fails to converge,
you should look at the non-convergent model and then kick out
whichever random slope accounted for the least variance. But since my
model includes a four-level factor, I get different variances for each
level of the factor (and the problem is compounded by the interaction
terms, see the snippet below; there are also other random effects for
control variables, which I have not shown):

Random effects:
 Groups    Name                       Variance  Std.Dev.  Corr
Subject   Factor1a:Factor2a  1.553e-08 1.246e-04
           Factor1b:Factor2a  2.000e-08 1.414e-04 0.69
           Factor1a:Factor2b   7.322e-09 8.557e-05 0.69 0.99
           Factor1b:Factor2b   2.624e-08 1.620e-04 0.55 0.70 0.71
           Factor1a:Factor2c   5.017e-08 2.240e-04 0.41 0.65 0.65 0.89
           Factor1b:Factor2c   2.220e-08 1.490e-04 0.25 0.48 0.55 0.78
0.90
           Factor1a:Factor2d 3.972e-08 1.993e-04 0.50 0.67 0.72 0.93
0.94 0.95
           Factor1b:Factor2d 1.642e-08 1.282e-04 0.36 0.79 0.78 0.83
0.81 0.71 0.81

So how do I evaluate the amount of variance accounted for by a
particular factor (or interaction), in order to determine which ones
to remove from the model?

2. I am trying to model the random effects structure without
correlations, since I'm having a hard time getting convergence. Barr
et al. (2013) suggest that if you're not using correlations, then the
factors should be coded with deviation coding rather than treatment
coding. However, deviation coding does not make theoretical sense for
the variables I'm looking at; my design has a 4-level factor, and one
of those is a 'baseline' level against which I want to compare the
other three (my dependent measure is reaction times, and I want to see
which conditions are faster than baseline). So in this case should I
estimate the model with deviation coding, and then use post-hoc tests
(with some package like glht) later on to compare conditions somehow?
Or just go ahead using treatment coding instead of deviation coding?

Thank you,
Steve


Stephen Politzer-Ahles
New York University, Abu Dhabi
Neuroscience of Language Lab
http://www.nyu.edu/projects/politzer-ahles/


From highstat at highstat.com  Mon Jan 27 13:38:29 2014
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Mon, 27 Jan 2014 12:38:29 +0000
Subject: [R-sig-ME] Statistics courses
Message-ID: <52E65345.20506@highstat.com>


There are a few remaining places on the following three statistics 
courses in Coimbra, Lisbon and Elche (Alicante).


Course: Data exploration, linear regression, GLM & GAM in R. With 
introduction to R.
Where:  University of Coimbra, Coimbra, Portugal
When:   3-7 February, 2014


Course: Introduction to Linear Mixed Effects Models, GLMM and MCMC with R
Where:  University of Lisbon, Lisbon, Portugal
When:   10-14 February, 2014


Course: Beginner's Guide to GAM  and GAMM with R
Where:  Elche, Alicante, Spain
When:   10-14 March, 2014


For full details, flyers, prices, etc. see:
http://www.highstat.com/statscourse.htm

Kind regards,

Alain

-- 
Dr. Alain F. Zuur

First author of:
1. Beginner's Guide to GAMM with R (2014).
2. Beginner's Guide to GLM and GLMM with R (2013).
3. Begginner's Guide to GAM with R (2012).
4. Zero Inflated Models and GLMM with R (2012).
5. A Beginner's Guide to R (2009).
6. Mixed effects models and extensions in ecology with R (2009).
7. Analysing Ecological Data (2007).

Highland Statistics Ltd.
9 St Clair Wynd
UK - AB41 6DZ Newburgh
Tel:   0044 1358 788177
Email: highstat at highstat.com
URL:   www.highstat.com


From S.Ellison at LGCGroup.com  Tue Jan 28 13:23:41 2014
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Tue, 28 Jan 2014 12:23:41 +0000
Subject: [R-sig-ME] degrees of freedom in mixed model
In-Reply-To: <A4E5A0B016B8CB41A485FC629B633CED592F6BD298@GOLD.corp.lgc-group.com>
References: <1390560493.2093.YahooMailNeo@web87905.mail.ir2.yahoo.com>
	<A4E5A0B016B8CB41A485FC629B633CED592F6BD298@GOLD.corp.lgc-group.com>
Message-ID: <A4E5A0B016B8CB41A485FC629B633CED592F6BDBE8@GOLD.corp.lgc-group.com>

Just for the record, I just noticed that there was a rather crucial 'not' missing from my initial post, just before 'interpretable' in 
> So F is interpretable in the same way as it would be in classical anova.

But others have covered it more than adequately, and it's good to be reminded that using K-R is indeed not often a serious concern in LMMs despite the sometimes heated debate.

Me, I just wish I could find a good, simple answer for fixed effects confidence intervals with crossed random effects in small experiments ...

Steve E




*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From teplitsky at mnhn.fr  Tue Jan 28 15:29:26 2014
From: teplitsky at mnhn.fr (Celine Teplitsky)
Date: Tue, 28 Jan 2014 15:29:26 +0100
Subject: [R-sig-ME] distribution for ratios?
In-Reply-To: <alpine.LMD.2.00.1401241355330.7150@orpheus.qimr.edu.au>
References: <52E0E9C9.8060900@mnhn.fr>
	<alpine.LMD.2.00.1401241355330.7150@orpheus.qimr.edu.au>
Message-ID: <52E7BEC6.9010402@mnhn.fr>

Dear all,

thank you for your suggestions.

The bivariate analysis is unfortunately not an option: in 'older' models 
using parent offspring regression, only heritability is estimated, there 
are no estimates of Va or Ve.

Thanks again

Celine


> On Thu, 23 Jan 2014, Celine Teplitsky wrote:
>
>> we are trying to run a meta-analysis on factors affecting 
>> heritability in MCMCglmm.
>
> I'm surprised no-one has suggested a bivariate analysis of V_g and V_e 
> (gamma, or even gaussian).
>
> "... the so-called co-efficient of heritability, which I regard as one 
> of those unfortunate short-cuts, which have often emerged in biometry 
> for lack of a more thorough analysis of the data."
>
>
> | David Duffy (MBBS PhD)
> | email: David.Duffy at qimrberghofer.edu.au  ph: INT+61+7+3362-0217 fax: 
> -0101
> | Genetic Epidemiology, QIMR Berghofer Institute of Medical Research
> | 300 Herston Rd, Brisbane, Queensland 4006, Australia  GPG 4D0B994A
>
>


-- 

Celine Teplitsky
D?partement Ecologie et Gestion de la Biodiversit? UMR 7204
Unit? Conservation des Esp?ces, Restauration et Suivi des Populations
Case Postale 51
55 rue Buffon 75005 Paris

Webpage : http://www2.mnhn.fr/cersp/spip.php?rubrique96
Fax : (33-1)-4079-3835
Phone: (33-1)-4079-3443


From slu at ccsr.uchicago.edu  Wed Jan 29 02:06:37 2014
From: slu at ccsr.uchicago.edu (Stuart Luppescu)
Date: Tue, 28 Jan 2014 19:06:37 -0600
Subject: [R-sig-ME] Don't understand this variance components output
Message-ID: <1390957597.24232.14.camel@localhost>

Hello, I'm analyzing ratings of teacher performance on 9 components.
Each teacher is rated between 1 and 6 times. The average number of
ratings per teacher is about 3.5. The structure of the data is (teachers
nested within observations) crossed by components. The lmer call is:
 
lme6 <- lmer(rating ~ (1|tid.f) +  (1|comp.f) + (1|tid.f/obsorder.f),
data=ratings, REML=FALSE)

The outcome is rating; tid.f is the teacher ID; comp.f is the component
ID; obsorder.f is the observation. Here is the summary output:

Random effects:
 Groups           Name        Variance Std.Dev.
 obsorder.f.tid.f (Intercept) 0.06574  0.2564  
 tid.f            (Intercept) 0.16309  0.4038  
 tid.f.1          (Intercept) 0.03498  0.1870  
 comp.f           (Intercept) 0.01589  0.1261  
 Residual                     0.19696  0.4438  
Number of obs: 166405, groups: obsorder.f:tid.f, 18496; tid.f, 5486;
comp.f, 9

Fixed effects:
            Estimate Std. Error t value
(Intercept)  2.82680    0.04252   66.49

What I don't get is the line in the list of random effects labeled
tid.f.1. What is this? Am I not specifying the model correctly?

Here is my sessionInfo:
R version 3.0.2 (2013-09-25)
Platform: x86_64-redhat-linux-gnu (64-bit)

locale:
 [1] LC_CTYPE=en_US       LC_NUMERIC=C         LC_TIME=en_US       
 [4] LC_COLLATE=en_US     LC_MONETARY=en_US    LC_MESSAGES=en_US   
 [7] LC_PAPER=en_US       LC_NAME=C            LC_ADDRESS=C        
[10] LC_TELEPHONE=C       LC_MEASUREMENT=en_US LC_IDENTIFICATION=C 

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods
base     

other attached packages:
 [1] plyr_1.8        reshape2_1.2.2  MCMCglmm_2.17   corpcor_1.6.6  
 [5] ape_3.0-11      coda_0.16-1     tensorA_0.36    MASS_7.3-29    
 [9] foreign_0.8-59  lme4_1.0-5      Matrix_1.1-1.1  lattice_0.20-24

loaded via a namespace (and not attached):
[1] compiler_3.0.2 grid_3.0.2     minqa_1.2.2    nlme_3.1-113
splines_3.0.2 
[6] stringr_0.6.2  tools_3.0.2   

Thanks very much for the help.

-- 
Stuart Luppescu <slu at ccsr.uchicago.edu>


From bbolker at gmail.com  Wed Jan 29 05:23:51 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 28 Jan 2014 23:23:51 -0500
Subject: [R-sig-ME] Don't understand this variance components output
In-Reply-To: <1390957597.24232.14.camel@localhost>
References: <1390957597.24232.14.camel@localhost>
Message-ID: <52E88257.9030005@gmail.com>

On 14-01-28 08:06 PM, Stuart Luppescu wrote:
> Hello, I'm analyzing ratings of teacher performance on 9 components.
> Each teacher is rated between 1 and 6 times. The average number of
> ratings per teacher is about 3.5. The structure of the data is (teachers
> nested within observations) crossed by components. The lmer call is:
>  
> lme6 <- lmer(rating ~ (1|tid.f) +  (1|comp.f) + (1|tid.f/obsorder.f),
> data=ratings, REML=FALSE)
> 
> The outcome is rating; tid.f is the teacher ID; comp.f is the component
> ID; obsorder.f is the observation. Here is the summary output:
> 
> Random effects:
>  Groups           Name        Variance Std.Dev.
>  obsorder.f.tid.f (Intercept) 0.06574  0.2564  
>  tid.f            (Intercept) 0.16309  0.4038  
>  tid.f.1          (Intercept) 0.03498  0.1870  
>  comp.f           (Intercept) 0.01589  0.1261  
>  Residual                     0.19696  0.4438  
> Number of obs: 166405, groups: obsorder.f:tid.f, 18496; tid.f, 5486;
> comp.f, 9
> 
> Fixed effects:
>             Estimate Std. Error t value
> (Intercept)  2.82680    0.04252   66.49
> 
> What I don't get is the line in the list of random effects labeled
> tid.f.1. What is this? Am I not specifying the model correctly?


tl;dr   just use (1|tid.f/obsorder.f), drop (1|tid.f) -- it's redundant.

  I don't know if it's a FAQ or not, but I feel like I've answered this
one before (maybe off-list though) ... the problem is that you've got
both (1|tid.f) (variation in intercepts among 'tid.f' levels) and
(1|tid.f/obsorder.f) (variation among obsorder.f nested within tid.f).
In particular, tid.f/obsorder.f expands to tid.f + tid.f:obsorder.f
(i.e., tid.f and obsorder.f nested within tid.f), so you have two tid.f
terms.  Unlike regular (e.g. lm/glm) formulae, these redundant terms
don't automatically get merged ...  I've tried to write code that would
reliably detect this sort of overparameterization, but so far I haven't
managed.

  Ben Bolker


> 
> Here is my sessionInfo:
> R version 3.0.2 (2013-09-25)
> Platform: x86_64-redhat-linux-gnu (64-bit)
> 
> locale:
>  [1] LC_CTYPE=en_US       LC_NUMERIC=C         LC_TIME=en_US       
>  [4] LC_COLLATE=en_US     LC_MONETARY=en_US    LC_MESSAGES=en_US   
>  [7] LC_PAPER=en_US       LC_NAME=C            LC_ADDRESS=C        
> [10] LC_TELEPHONE=C       LC_MEASUREMENT=en_US LC_IDENTIFICATION=C 
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods
> base     
> 
> other attached packages:
>  [1] plyr_1.8        reshape2_1.2.2  MCMCglmm_2.17   corpcor_1.6.6  
>  [5] ape_3.0-11      coda_0.16-1     tensorA_0.36    MASS_7.3-29    
>  [9] foreign_0.8-59  lme4_1.0-5      Matrix_1.1-1.1  lattice_0.20-24
> 
> loaded via a namespace (and not attached):
> [1] compiler_3.0.2 grid_3.0.2     minqa_1.2.2    nlme_3.1-113
> splines_3.0.2 
> [6] stringr_0.6.2  tools_3.0.2   
> 
> Thanks very much for the help.
>


From fbromano at sabanciuniv.edu  Wed Jan 29 12:22:59 2014
From: fbromano at sabanciuniv.edu (Francesco)
Date: Wed, 29 Jan 2014 13:22:59 +0200
Subject: [R-sig-ME] Reporting main effects in glmer
Message-ID: <52E8E493.9070000@sabanciuniv.edu>

Dear all,


fairly simple question this time. I need to report a main effect and or 
pairwise comparisons for a model that has a binary DV 'success' and a 
categorical predictor X with three levels (A, B, and C). My model looks 
like this:

Correct ~ 1 + A + (1 | Part) + (1 | Item)

The original call was:

 >object<-glmer(Correct~1+A+(1|Part)+(1|Item), family=binomial, data=data)


with the following summary output:

Generalized linear mixed model fit by maximum likelihood ['glmerMod']
  Family: binomial ( logit )
Formula: ......
    Data: .....

       AIC       BIC    logLik  deviance
  218.9053  242.0455 -104.4526  208.9053

Random effects:
  Groups Name        Variance Std.Dev.
  Part   (Intercept) 9.0129   3.0021
  Item   (Intercept) 0.7356   0.8577
Number of obs: 756, groups: Part, 45; Item, 18

Fixed effects:
               Estimate Std. Error z value Pr(>|z|)
(Intercept)     -95.14 4122464.75       0        1
A             89.88 4122464.75       0        1
B             90.75 4122464.75       0        1

Correlation of Fixed Effects:
      (Intr)   A
A -1.000
B -1.000  1.000

where Part and Item are random effects for participant and items. 
Basically the three groups are very similar in their response, where one 
group (the reference level group in this analysis) has no 0s and all 1s. 
Releveling generates a Pr(>|z|) of .54 for the difference between group 
B and C so I believe the model is ok.

In this scenario I would not expect A to have a main effect on the model 
but when I compare it to the same model minus the A predictor, R yields 
the following:


 > anova(object,objectminusA)

Data: data
Models:
objectminusA: Correct ~ 1 + (1 | Part) + (1 | Item)
object: Correct ~ 1 + A + (1 | Part) + (1 | Item)
              Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(>Chisq)
objectminusA  3 221.64 235.53 -107.82 215.64
object  5 218.91 242.04 -104.45   208.91 6.7367      2    0.03445 *

My questions are as follows:
1. Should this be interpreted as there being a main effect but no 
significant difference exists between the three levels of the predictor?
2. How do I report the result in my paper?

Many thanks in advance for any help.



-- 
Frank Romano

Sabanci University
website: http://sabanciuniv.academia.edu/FrancescoRomano


From kovla123 at hotmail.com  Wed Jan 29 15:34:08 2014
From: kovla123 at hotmail.com (Maxim Kovalenko)
Date: Wed, 29 Jan 2014 15:34:08 +0100
Subject: [R-sig-ME] MCMCglmm model for heteroskedasticity at level one
Message-ID: <DUB119-W12E255DA17CF9D44C6BE2480AC0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140129/37adb586/attachment.pl>

From montpied at nancy.inra.fr  Tue Jan 28 14:35:20 2014
From: montpied at nancy.inra.fr (Pierre Montpied)
Date: Tue, 28 Jan 2014 14:35:20 +0100
Subject: [R-sig-ME] Model with interaction alone
Message-ID: <52E7B218.5060105@nancy.inra.fr>

Hi list,

I could not find any answer to my problem thus I post it on this list.

I have a design where  sites (Site) are nested in different regions 
(Reg) and a pair of plots in each site
is assigned two levels of a treatment (Trait).

I fit this model with lme (or lmer)  with  two crossed fixed factors 
(Reg *Trait) and a random factor Site

All works fine when I fit the complete model "Response~Reg + Treat + 
Reg:Treat"t or equivalently "Response~Reg*Trait"

But when I try the interaction alone by "Response~Reg:Trait" I get the 
error message:
"Error in MEEM(object, conLin, control$niterEM) :   Singularity in 
backsolve at level 0, block 1".

When I replace "Reg:Trait" by "interaction(Reg,Trait)" or by 
"factor(Reg:Trait)" all works fine
whereas these should be equivalent to "Reg:Trait"

Any explanation ?

Below is a script simulating this

library(nlme)

Nrep<-10 # replicates = site number per region
Data<-data.frame(expand.grid(c("T1","T2"),rep(c("R1","R2"),each=Nrep)),
         Site=paste("S",rep(1:(Nrep*2),each=2),sep="")
         )
names(Data)<-c("Trait","Reg","Site")
Data$Trait<-factor(Data$Trait)
Data$Reg<-factor(Data$Reg)
Data$Site<-factor(Data$Site)
Data$RegTrait<-factor(paste(Data$Reg,Data$Trait,sep="."))
str(Data)
table(Data$Reg,Data$Trait,Data$Site)
table(Data$Site,Data$Trait,Data$Reg)


Mu<-20    # Intercept
Alpha2<-1    # Fixed effect Trait
Beta2<-2    # Fixed effect Region
Gamma22<-2    # Interaction Trait:Region
Sigi<-1.5    # Random effect Site
Sige<-1    # Residual

# Simulation:
Data$Rep<- Mu +
         Alpha2*(Data$Trait=="T2") +
         Beta2*(Data$Reg=="R2") +
         Gamma22*(Data$Reg=="R2")*(Data$Trait=="T2") +
         rep(rnorm(Nrep*2,0,Sigi),each=2) +
         rnorm(Nrep*4,0,Sige)


Modele.lme<-lme(Rep~Trait*Reg,
     random=~1|Site,
     data=Data)# OK

Modele.lme<-lme(Rep~Trait + Reg + Trait:Reg,
     random=~1|Site,
     data=Data)# OK

# Interaction alone:

Modele.lme<-lme(Rep~Trait:Reg,
     random=~1|Site,
     data=Data)# Error:

# Error in MEEM(object, conLin, control$niterEM) :
#  Singularity in backsolve at level 0, block 1


Modele.lme<-lme(Rep~interaction(Trait,Reg),
     random=~1|Site,
     data=Data)# OK

Modele.lme<-lme(Rep~factor(Trait:Reg),
     random=~1|Site,
     data=Data)# OK

Thank  in advance

-- 
=========================================================================
Pierre MONTPIED
INRA Universit? de Lorraine                     Tel : (33) 3 83 39 40 74
UMR EEF Ecophysiologie Ecologie Foresti?res     FAX : (33) 3 83 39 40 69
F-54280 CHAMPENOUX
http://www.nancy.inra.fr                        montpied at nancy.inra.fr


From bates at stat.wisc.edu  Wed Jan 29 19:26:58 2014
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 29 Jan 2014 12:26:58 -0600
Subject: [R-sig-ME] Reporting main effects in glmer
In-Reply-To: <52E8E493.9070000@sabanciuniv.edu>
References: <52E8E493.9070000@sabanciuniv.edu>
Message-ID: <CAO7JsnRyCUe34uF34HJmChj2gmrZDZDYfjgYC6hhzo-DKpp+0w@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140129/95e2ac55/attachment.pl>

From fbromano at sabanciuniv.edu  Wed Jan 29 19:43:41 2014
From: fbromano at sabanciuniv.edu (Francesco)
Date: Wed, 29 Jan 2014 20:43:41 +0200
Subject: [R-sig-ME] Reporting main effects in glmer
In-Reply-To: <CAO7JsnRyCUe34uF34HJmChj2gmrZDZDYfjgYC6hhzo-DKpp+0w@mail.gmail.com>
References: <52E8E493.9070000@sabanciuniv.edu>
	<CAO7JsnRyCUe34uF34HJmChj2gmrZDZDYfjgYC6hhzo-DKpp+0w@mail.gmail.com>
Message-ID: <52E94BDD.7030807@sabanciuniv.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140129/b5718afe/attachment.pl>

From sebastian.daza at gmail.com  Thu Jan 30 15:41:36 2014
From: sebastian.daza at gmail.com (=?ISO-8859-1?Q?Sebasti=E1n_Daza?=)
Date: Thu, 30 Jan 2014 08:41:36 -0600
Subject: [R-sig-ME] glmmADMB symbol error
Message-ID: <CAJk5_vHvMw+X1XfCcELouFVUSSmkzATCRqJV65S1HBcho6XRTg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140130/ea3c0264/attachment.pl>

From holtermann at hwwi.org  Fri Jan 31 12:24:56 2014
From: holtermann at hwwi.org (Linus Holtermann)
Date: Fri, 31 Jan 2014 12:24:56 +0100
Subject: [R-sig-ME] Specify prior for sir-function and starting values in
 the MCMCglmm packages?
Message-ID: <AD0050057515F54084E7D5B93478C8481F7BA0260B@winxbede39.exchange.xchg>

Dear list members,

unfortunately Jarrod Hadfield is on field work until the end of June and cant help me with my problems.  Maybe someone on the mailing list knows how to specify starting values and a Prior for the sir-function in MCMCglmm. Short code examples would be great.

Best regards, 

 
Linus Holtermann
Hamburgisches WeltWirtschaftsInstitut gemeinn?tzige GmbH (HWWI)
Heimhuder Stra?e 71
20148 Hamburg
Tel +49-(0)40-340576-336
Fax+49-(0)40-340576-776
Internet: www.hwwi.org
Email: holtermann at hwwi.org
 
AmtsgerichtHamburg HRB 94303
Gesch?ftsf?hrer: Prof. Dr. Thomas Straubhaar, Gunnar Geyer
Umsatzsteuer-ID: DE 241849425

From bbolker at gmail.com  Fri Jan 31 15:01:39 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 31 Jan 2014 09:01:39 -0500
Subject: [R-sig-ME] convert mer to merMod?
In-Reply-To: <182794B2C0CC3346A421E675832EC20D21EDA1FD@MBX07.ad.oak.ox.ac.uk>
References: <182794B2C0CC3346A421E675832EC20D21EDA1FD@MBX07.ad.oak.ox.ac.uk>
Message-ID: <52EBACC3.3070803@gmail.com>

On 14-01-31 07:12 AM, Matthew Husband wrote:
> Dear Ben Bolker-
> 
> Sorry to trouble you with this ? it looks like the new lme4 package has
> moved on to a new class for lmer().  However, I have some older analyses
> saved that are class mer.  Is there a way to convert them so that I can
> use them with the the standard functions (I.e. summary(), etc.).  It
> will be a great pain to try to rerun the analyses?
> 
> Best,
> -Matt
> 
  [cc'ing back to r-sig-mixed-models]

  In principle this could be done (and it would be handy to have), but I
think in practice the easiest thing to do is to install lme4.0, the
backward-compatibility version of lme4; this is available from the
repository at http://lme4.r-forge.r-project.org/repos, i.e.

  install.packages("lme4.0",repos="http://lme4.r-forge.r-project.org/repos")

(if you don't have the dependencies installed already you have to do
something like repos=c("...",getOption("repos")), but lme4.0 has a
fairly short dependency list and you should probably have them already
if you've already been using lme4).

  This also has the advantage of signalling that you're using an older
version of lme4, causing less confusion if someone else gets slightly
different results with a newer version.

  You will also need to have a look at the convert_old_lme4() function
in the lme4.0 package.

  Ben Bolker


From bbolker at gmail.com  Fri Jan 31 22:21:53 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 31 Jan 2014 21:21:53 +0000 (UTC)
Subject: [R-sig-ME] glmmADMB symbol error
References: <CAJk5_vHvMw+X1XfCcELouFVUSSmkzATCRqJV65S1HBcho6XRTg@mail.gmail.com>
Message-ID: <loom.20140131T221147-722@post.gmane.org>

Sebasti?n Daza <sebastian.daza at ...> writes:

> 
> I ran an example using glmmADMB, and I got this error:
> 

library(glmmADMB)
data(bacteria,package="MASS")
bacteria$present <- as.numeric(bacteria$y)-1
  (bfit <-  glmmadmb(present ~ trt + I(week > 2), random = ~ 1 | ID,
    family = "binomial", data = bacteria))

> 
> Error in paste0(symbol1, paste0(paste0(var, collapse = symbol2))) :
>   argument "symbol1" is missing, with no default
> 
> Any ideas?
> 

 Hmm...  I don't get this problem.  I also don't see "symbol1"
anywhere in the R code for glmmADMB or R2admb, which is making
me think that a function in some other package is masking something ...

  My sessionInfo has

R Under development (unstable) (2014-01-02 r64625)
Platform: i686-pc-linux-gnu (32-bit)

[locale information snipped]

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] glmmADMB_0.7.7 R2admb_0.7.10  MASS_7.3-29   

loaded via a namespace (and not attached):
[1] compiler_3.1.0  grid_3.1.0      lattice_0.20-24 Matrix_1.1-1   
[5] nlme_3.1-113    tools_3.1.0    

other attached packages:
[1] glmmADMB_0.7.7 R2admb_0.7.10  MASS_7.3-29   

loaded via a namespace (and not attached):
[1] grid_3.1.0      lattice_0.20-24 Matrix_1.1-1    nlme_3.1-113   
[5] tools_3.1.0

   I am using a development version of R, but I doubt that's
the problem ... could you try from a clean session (i.e. without
any unnecessary packages) and see what happens?

Hmm: after loading multcomp, foreign, and data.table I get sessionInfo()
more similar to yours, but things still work:
not quite the same versions of all the packages, but still ...

other attached packages:
[1] data.table_1.8.10 foreign_0.8-57    multcomp_1.3-1    TH.data_1.0-2    
[5] survival_2.37-4   mvtnorm_0.9-9996  glmmADMB_0.7.7    R2admb_0.7.10    
[9] MASS_7.3-29      

loaded via a namespace (and not attached):
[1] compiler_3.1.0  grid_3.1.0      lattice_0.20-24 Matrix_1.1-1   
[5] nlme_3.1-113    sandwich_2.3-0  tools_3.1.0     zoo_1.7-10

  Can you try traceback() and see where the error is actually happening?

  Ben Bolker


> R version 3.0.2 (2013-09-25)
> Platform: x86_64-apple-darwin10.8.0 (64-bit)
> 
> locale:
> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
> 
> attached base packages:
> [1] splines   stats     graphics  grDevices utils     datasets  methods
> base
> 
> other attached packages:
> [1] multcomp_1.3-1    TH.data_1.0-3     survival_2.37-7   mvtnorm_0.9-9997
>  data.table_1.8.10 glmmADMB_0.7.7    R2admb_0.7.10
> [8] MASS_7.3-29       foreign_0.8-59
> 
> loaded via a namespace (and not attached):
> [1] grid_3.0.2      lattice_0.20-24 Matrix_1.1-1.1  nlme_3.1-113
>  sandwich_2.3-0  tools_3.0.2     zoo_1.7-10
>


From bbolker at gmail.com  Fri Jan 31 22:46:45 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 31 Jan 2014 21:46:45 +0000 (UTC)
Subject: [R-sig-ME] Model with interaction alone
References: <52E7B218.5060105@nancy.inra.fr>
Message-ID: <loom.20140131T222234-622@post.gmane.org>

Pierre Montpied <montpied at ...> writes:

> 
> Hi list,
> 
> I could not find any answer to my problem thus I post it on this list.
> 
> I have a design where  sites (Site) are nested in different regions 
> (Reg) and a pair of plots in each site
> is assigned two levels of a treatment (Trait).
> 
> I fit this model with lme (or lmer)  with  two crossed fixed factors 
> (Reg *Trait) and a random factor Site
> 
> All works fine when I fit the complete model "Response~Reg + Treat + 
> Reg:Treat"t or equivalently "Response~Reg*Trait"
> 
> But when I try the interaction alone by "Response~Reg:Trait" I get the 
> error message:
> "Error in MEEM(object, conLin, control$niterEM) :   Singularity in 
> backsolve at level 0, block 1".
> 
> When I replace "Reg:Trait" by "interaction(Reg,Trait)" or by 
> "factor(Reg:Trait)" all works fine
> whereas these should be equivalent to "Reg:Trait"
> 
> Any explanation ?
> 
> Below is a script simulating this


Thanks for the reproducible example.  I have to admit I don't fully
understand what's going on here, and in particular the difference
between "Reg:Trait" and "interaction(Reg,Trait)" and "factor(Reg:Trait)",
but the bottom line is that in the first case, R has decided that
it should include an intercept column in the model (fixed-effect
model matrix) as well as the 4 levels of the interaction, so the
fixed-effect model is overparameterized, which leads to the error.
In the second two cases R drops the intercept, so the model matrix
constructed by model.matrix() has only 4 rather than 5 columns
(see the end of this post for example).
The equivalent error from lme4::lmer is *slightly* more transparent
(and I'll try to improve it further).  If you specify Reg:Trait-1 or
Reg:Trait+0 (explicitly suppressing the intercept), everything works
again.

> 
library(nlme)
 
Nrep <- 10 # replicates = site number per region
Data <- expand.grid(Trait=c("T1","T2"),
                    Reg=rep(c("R1","R2"),each=Nrep),
                    Site=paste0("S",rep(1:(Nrep*2),each=2)))
Data <- transform(Data,RegTrait=factor(paste(Data$Reg,Data$Trait,sep=".")))
## with(Data,table(Reg,Trait,Site))
with(Data,table(Site,Trait,Reg))
 
Mu<-20    # Intercept
Alpha2<-1    # Fixed effect Trait
Beta2<-2    # Fixed effect Region
Gamma22<-2    # Interaction Trait:Region
Sigi<-1.5    # Random effect Site
Sige<-1    # Residual
 
## Simulation:
set.seed(101)
Data$Rep<- Mu +
         Alpha2*(Data$Trait=="T2") +
          Beta2*(Data$Reg=="R2") +
          Gamma22*(Data$Reg=="R2")*(Data$Trait=="T2") +
          rep(rnorm(Nrep*2,0,Sigi),each=2) +
          rnorm(Nrep*4,0,Sige)

library(nlme)

## the following two models should be _exactly_ identical
Modele.lme.1 <-lme(Rep~Trait*Reg,
     random=~1|Site,
     data=Data)# OK
 
Modele.lme.2 <-lme(Rep~Trait + Reg + Trait:Reg,
      random=~1|Site,
      data=Data)# OK
 
## Interaction alone:
 
Modele.lme.3 <- lme(Rep~Trait:Reg,
      random=~1|Site,
      data=Data)# Error:
## Error in MEEM(object, conLin, control$niterEM) :
##  Singularity in backsolve at level 0, block 1

Modele.lme.4 <- lme(Rep~interaction(Trait,Reg),
      random=~1|Site,
      data=Data)# OK
names(fixef(Modele.lme.4))
 
Modele.lme.5 <- lme(Rep~factor(Trait:Reg),
      random=~1|Site,
      data=Data)# OK
names(fixef(Modele.lme.5))

Modele.lme.6 <- lme(Rep~Trait:Reg-1,
      random=~1|Site,
      data=Data)# OK
names(fixef(Modele.lme.6))

library(lme4)
Modele.lmer.1 <- lmer(Rep~Trait*Reg+(1|Site),
                      data=Data)
Modele.lmer.2 <- lmer(Rep~Trait:Reg+(1|Site),
                      data=Data)  ## error: rank of X 
Modele.lmer.3 <- lmer(Rep~Trait:Reg-1+(1|Site),
                      data=Data)
fixef(Modele.lmer.3)

## check model matrices
X1 <- model.matrix(~Trait*Reg,Data)
X2 <- model.matrix(~Trait:Reg,Data)
X3 <- model.matrix(~Trait:Reg-1,Data)


From bbolker at gmail.com  Fri Jan 31 23:01:05 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 31 Jan 2014 22:01:05 +0000 (UTC)
Subject: [R-sig-ME] Reporting main effects in glmer
References: <52E8E493.9070000@sabanciuniv.edu>
	<CAO7JsnRyCUe34uF34HJmChj2gmrZDZDYfjgYC6hhzo-DKpp+0w@mail.gmail.com>
	<52E94BDD.7030807@sabanciuniv.edu>
Message-ID: <loom.20140131T224739-622@post.gmane.org>

Francesco <fbromano at ...> writes:

>

  I think that your basic problem here is that you are
seeing _complete separation_ in your groups; you mention that
"...   one group (the reference level group in this analysis) has
no 0s and all 1s."  This gives rise to the _Hauck-Donner_ effect,
discussed by Venables and Ripley (MASS: http://tinyurl.com/hauckdonner)
and easily Googlable, where the Wald estimates of the standard
errors and p-values (which are what's reported by summary()) are
bogus.  

Using group C as your reference level is particularly problematic: by
reporting a baseline log-odds of -95 for the intercept, glmer is
essentially getting as close to -Inf as it can before giving up.  All
other comparisons, which are made with reference to the baseline
levels, will be similarly enormous.  (For reference, any estimate with
an absolute value of >10 corresponds to proportional differences in
odds of more than 22,000, and probably indicates complete separation,
or some other issue.)  You have log-odds differences of 90 between C
and A and 91 between C and B, which are bound to be significant with
any sensible data set.

Some of your choices:

(1) use confint() to get likelihood profile confidence intervals,
not subject to the Hauck-Donner effect (but this won't give you
p-values;
(2) use bglmer from the blme package to impose weak priors on
the fixed-effect parameters;
(3) simply report the overall effect.

You should probably do some reading on complete separation
and Hauck-Donner ...

 cheers
    Ben Bolker

[snip]

> 'Correct' is a binary DV and X is a between-subjects, categorical IV 
> factor with three levels (A, B, and C).
> 
> The original call was:
> 
>  >object<-glmer(Correct~1+X+(1|Part)+(1|Item), family=binomial, data=data)
>  >summary(object)
> 
> Generalized linear mixed model fit by maximum likelihood ['glmerMod']
>   Family: binomial ( logit )
> Formula: Correct ~ 1 + X + (1 | Part) + (1 | Item)
>     Data: .....
> 
>        AIC       BIC    logLik  deviance
>   218.9053  242.0455 -104.4526  208.9053
> 
> Random effects:
>   Groups Name        Variance Std.Dev.
>   Part   (Intercept) 9.0129 3.0021 
>   Item   (Intercept) 0.7356   0.8577
> Number of obs: 756, groups: Part, 45; Item, 18
> 
> Fixed effects:
>                Estimate Std. Error z value Pr(>|z|)
> (Intercept)     -95.14 4122464.75       0        1
> A             89.88 4122464.75       0        1
> B             90.75 4122464.75       0        1
> 
> Correlation of Fixed Effects:
>       (Intr)   A
> A -1.000
> B -1.000  1.000
> 
 [snip] ...

> >     where Part and Item are random effects for participant and items.
> >     Basically the three groups are very similar in their response,
> >     where one group (the reference level group in this analysis) has
> >     no 0s and all 1s. Releveling generates a Pr(>|z|) of .54 for the
> >     difference between group B and C so I believe the model is ok.
> >
> >
> >
> >     In this scenario I would not expect A to have a main effect on the
> >     model but when I compare it to the same model minus the A
> >     predictor, R yields the following:
> >
> >
> >     > anova(object,objectminusA)
> >
> >     Data: data
> >     Models:
> >     objectminusA: Correct ~ 1 + (1 | Part) + (1 | Item)
> >     object: Correct ~ 1 + A + (1 | Part) + (1 | Item)
> >                  Df    AIC    BIC  logLik deviance  Chisq Chi Df
> >     Pr(>Chisq)
> >     objectminusA  3 221.64 235.53 -107.82 215.64
> >     object  5 218.91 242.04 -104.45   208.91 6.7367      2  0.03445 *
> >
> >     My questions are as follows:
> >     1. Should this be interpreted as there being a main effect but no
> >     significant difference exists between the three levels of the
> >     predictor?
> >     2. How do I report the result in my paper?


From bbolker at gmail.com  Fri Jan 31 23:12:30 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 31 Jan 2014 17:12:30 -0500
Subject: [R-sig-ME] Don't understand this variance components output
In-Reply-To: <1391007752.27215.7.camel@musuko.uchicago.edu>
References: <1390957597.24232.14.camel@localhost>	 <52E88257.9030005@gmail.com>
	<1391007752.27215.7.camel@musuko.uchicago.edu>
Message-ID: <52EC1FCE.2080809@gmail.com>

On 14-01-29 10:02 AM, Stuart Luppescu wrote:
> On Tue, 2014-01-28 at 23:23 -0500, Ben Bolker wrote:
>> just use (1|tid.f/obsorder.f), drop (1|tid.f) -- it's redundant.
>>
>>   I don't know if it's a FAQ or not, but I feel like I've answered
>> this
>> one before (maybe off-list though) ... the problem is that you've got
>> both (1|tid.f) (variation in intercepts among 'tid.f' levels) and
>> (1|tid.f/obsorder.f) (variation among obsorder.f nested within tid.f).
>> In particular, tid.f/obsorder.f expands to tid.f + tid.f:obsorder.f
>> (i.e., tid.f and obsorder.f nested within tid.f),
> 
> 
> 
> Ben, thanks very much for the help. If this is tl then dr.
> 
> I'm still confused by how to specify interactions, especially when I
> have both nested and crossed factors. I read Doug Bates's book but this
> is not clear. 
> 
> I have teachers crossed by components, and observations nested within
> teachers. Do I specify the nesting/crossing structure in the model
> statement or in the factor identifiers? That is, for observations nested
> within teachers, do I need observation identifiers that look like
> "Teacher1Observation1", "Teacher1Observation2", etc.? What I have now is
> just 1, 2, 3, 4, ... for teacher and 1, 2, 3, 4, ... for observation. I
> believe I tried the former but when I included an
> observation-by-component interaction, I got a message saying the number
> of levels has to be less than the number of cases.
> 
> Thanks again.

  [cc'ing back to r-sig-mixed-models]

>From http://glmm.wikidot.com/faq:

Whether you explicitly specify a random effect as nested or not depends
(in part) on the way the levels of the random effects are coded. If the
'lower-level' random effect is coded with unique levels, then the two
syntaxes (1|a/b) (or (1|a+a:b)) and (1|a)+(1|b) are equivalent. If the
lower-level random effect has the same labels within each larger group
(e.g. blocks 1, 2, 3, 4 within sites A, B, and C) then the explicit
nesting (1|a/b) is required. It seems to be considered best practice to
code the nested level uniquely (e.g. A1, A2, ?, B1, B2, ?) so that
confusion between nested and crossed effects is less likely.

   ... but note this is NOT the same as (1|a) + (1|a/b) , which is what
you were trying to do in your model.  lme4 processes each random effect
term independently, so it can't "see" that the variation in intercept
across 'a' groupings is repeated twice, once from the (1|a) term and
once from the (1|a/b) term (which is exactly equivalent to (1|a+a:b)).

 If you try to do (1|a*b), which corresponds to (1|a+b+a:b), it sounds
like you're specifying an overparameterized model with your design -- if
elements of 'b' aren't repeated across levels of 'a' then you don't have
enough information to fit the model.

This is also discussed at
https://stat.ethz.ch/pipermail/r-sig-mixed-models/2007q4/000422.html

  hope that helps.

  Ben Bolker


From bbolker at gmail.com  Fri Jan 31 23:20:29 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 31 Jan 2014 22:20:29 +0000 (UTC)
Subject: [R-sig-ME] Maximal random-effects lmer not converging
References: <CAJT2k_-6-DYt-KDF6g4nEYd=BF0FyLQ8kVssOSBxgbeQrrOhVg@mail.gmail.com>
Message-ID: <loom.20140131T231314-435@post.gmane.org>

Stephen Politzer-Ahles <spa268 at ...> writes:

> 
> Hello,
> 
> I am trying to model a somewhat complicated dataset (which includes a
> 2x2x4 interaction) with maximal random effects, based on the
> suggestions from Barr et al. (2013). The maximal model is of course
> not converging, and there are several things I don't understand about
> how to proceed.
> 
> 1. I've seen several suggestions that, when a model fails to converge,
> you should look at the non-convergent model and then kick out
> whichever random slope accounted for the least variance. But since my
> model includes a four-level factor, I get different variances for each
> level of the factor (and the problem is compounded by the interaction
> terms, see the snippet below; there are also other random effects for
> control variables, which I have not shown):

  When you say "not converging", what do you mean exactly?  Are you getting
warnings, and if so what are they (precisely)?  Or are you stating the
fact that you're getting estimates of random-effects variances that are
effectively zero, or estimates of correlations that are +/- 1?

> Random effects:
>  Groups    Name                       Variance  Std.Dev.  Corr
> Subject   Factor1a:Factor2a  1.553e-08 1.246e-04
>            Factor1b:Factor2a  2.000e-08 1.414e-04 0.69
>            Factor1a:Factor2b   7.322e-09 8.557e-05 0.69 0.99
>            Factor1b:Factor2b   2.624e-08 1.620e-04 0.55 0.70 0.71
>            Factor1a:Factor2c   5.017e-08 2.240e-04 0.41 0.65 0.65 0.89
>            Factor1b:Factor2c   2.220e-08 1.490e-04 0.25 0.48 0.55 0.78
> 0.90
>            Factor1a:Factor2d 3.972e-08 1.993e-04 0.50 0.67 0.72 0.93
> 0.94 0.95
>            Factor1b:Factor2d 1.642e-08 1.282e-04 0.36 0.79 0.78 0.83
> 0.81 0.71 0.81
> 
> So how do I evaluate the amount of variance accounted for by a
> particular factor (or interaction), in order to determine which ones
> to remove from the model?

  Well, this is *one* component of the variance structure -- there's no
way to drop one part of it.  (You can't say "I want to fit an interaction
among A, B, and C, but I want to drop the B:C term" -- or at least it's
difficult and unlikely to be sensible).  You could try (Factor1+Factor2|Subject)
instead of (Factor1:Factor2|subject) -- that would reduce this block from 
an 8x8 variance-covariance matrix (dimension=nlevels(1)*nlevels(2)) to a 5x5 
(nlevels(1)+nlevels(2)-1) variance-covariance matrix, or from 8*9/2=36
parameters
to 5*6/2=15 ...

> 
> 2. I am trying to model the random effects structure without
> correlations, since I'm having a hard time getting convergence. Barr
> et al. (2013) suggest that if you're not using correlations, then the
> factors should be coded with deviation coding rather than treatment
> coding. However, deviation coding does not make theoretical sense for
> the variables I'm looking at; my design has a 4-level factor, and one
> of those is a 'baseline' level against which I want to compare the
> other three (my dependent measure is reaction times, and I want to see
> which conditions are faster than baseline). So in this case should I
> estimate the model with deviation coding, and then use post-hoc tests
> (with some package like glht) later on to compare conditions somehow?
> Or just go ahead using treatment coding instead of deviation coding?

   Can't help you with this one without spending a lot more time thinking
about it.  Sorry.  The fundamental problem is that when you force correlations
to zero, the predictions about what's going on at any particular combination
of factor levels then depends on the coding -- it is no longer invariant
to the coding chosen ...

> 
> Thank you,
> Steve
> 
> Stephen Politzer-Ahles
> New York University, Abu Dhabi
> Neuroscience of Language Lab
> http://www.nyu.edu/projects/politzer-ahles/
> 
>


From bbolker at gmail.com  Fri Jan 31 23:29:22 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 31 Jan 2014 22:29:22 +0000 (UTC)
Subject: [R-sig-ME] mixed effect model question
References: <CAJkOg=w1hjhS_9bWn+rHp79dac5dnrzCZhgco1ZNB3g7aTXvgQ@mail.gmail.com>
Message-ID: <loom.20140131T232110-229@post.gmane.org>

Thijs vanden Bergh <bergh.thijsvanden at ...> writes:

> 
> Dear list,
> 
> I have a number of questions concerning some mixed effects model that i am
> trying to fit on my data.
> 
> Basically the design of my experiment is as follows: we have three sites,
> low, mid and high (L, M and H; factor is called *Site*).
> At each of these three sites we have measured evapotranspiration  rates (
> *ET*; ET was measured using lysimeters, individuals were coded as *id*) for
> a number of days during summer (in total there are 15 days worth of data at
> L and M and 30 at H; factor is called *Date*).
> ET was measured for three vegetation types at all sites 
> (let say vegetation
> types are called A, B and C at all sites and the factor 
> is called *Class*).
> We simulated grazing in these three vegetation types by clipping (applied
> to half of the lysimeters of all vegetation types; factor is called
> *Treatment*).
> 
> The main aim of the excercise is to come to understand effects of
> elevation, treatment and vegetation type. Of particular interest also is
> the interaction between site and treatment as it would idicate weather the
> treatment effect depends on elevation.
> 
> One main problem is that elevation is not replicated. In the ideal case,
> site would have been a third random effect with sites at low mid and high
> elevation being replicated across different elevational gradients. One
> solution proposed to me is to take site as a random effect and then compare
> models with and without it. This will tell me that site does contribute
> significantly to the varibility in ET but it does not say anything about
> the direction of the effect. I was therefore tempted to keep site as a
> fixed effect, and then in the discussion state that effects of site are
> potentially confounded in other non altitude related phenomena (to then
> argue why this is likely or unlikely).

  I agree that you should keep site as a fixed effect.  Using site
as a random effect will work poorly in any case, for technical reasons
(it's hard to get an accurate and unbiased estimate of the among-site
variance when there are a small number of sites measured).  Since this
is a fundamental limitation of the experimental design (site effects
are confounded with elevation), it's just something you have to live
with.

> I thus started my analyses using Site, Treatment and Class as fixed
> effects. Because ET rates vary a great deal between days due to differences
> in weather conditions and due to the progressing of the season  i took id
> and date (factor) as a random effect.
> 
> something like:
> lme.v5.000    <-  lme((ET) ~ (site + class + treatment)^2,
>                     data = all.con.2009.sym,
> 
> random=list(ww=pdBlocked(list(pdIdent(~Date-1),pdIdent(~id-1)))),
>                     method="REML",
>                     na.action = na.exclude,
>                     weights=varstruc5)
> 
> the first question off course is, would this be a correct/good/defencible
> approach?
> What about including date as a random effect? is that indeed correct or
> would that not be necessary? It is highly significant.

  I think it's reasonable to treat date as random, although some
would argue against it on philosophical grounds.  If there is 'deterministic'
variation over the season (i.e. variation that could be modeled as
a linear or quadratic or spline curve), then you might want to include
date as *both* a fixed effect and a random effect; this won't change
the overall fit of the model, but will decompose among-date variation
into a smooth trend and variation around that trend.

> One further question i have is the following. There are some vegetation
> types that do not occur at all sites and / or that do not all receive the
> treatment. It seems that a mixed model does not converge on a design with
> missing levels and i was wondering if there is a workaround. I know that i
> can do pairwise comparisons using glht from the multcomp library on a model
> with one single explanatory variable (interaction(site, class, treatment)).
> Is there a way to probe the effect of class and treatment despite missing
> levels?

Recent versions of lme4 have methods, contributed by Rune Haubo, that
will automatically drop redundant columns of the fixed effect model
matrix (there is a 'check.rankX' option within lmerControl whose default
option is "message+drop.cols", i.e. drop redundant columns and generate
a message).  Alternatively you could construct your own fixed effect
model matrix, drop redundant columns yourself, cbind() your random
effects, and use those derived variables as the fixed effects in an
lme() call ...

  Ben Bolker


From chatfield at alumni.rice.edu  Sat Feb  1 01:51:01 2014
From: chatfield at alumni.rice.edu (Robert Chatfield)
Date: Fri, 31 Jan 2014 16:51:01 -0800
Subject: [R-sig-ME] Simple: don't understand lmer formula and slopes
In-Reply-To: <mailman.1863.1391206366.4563.r-sig-mixed-models@r-project.org>
References: <mailman.1863.1391206366.4563.r-sig-mixed-models@r-project.org>
Message-ID: <D36BA9EB-7BAF-4B7E-ADDE-39A31BFD86FF@alumni.rice.edu>

Here's a very simple mis-understanding of formula specification in lme4 - lmer
(Thanks to Ben and this group for help with a more fundamental question),

How does one specify slopes in lmer?
(This illustrates the essentials of a more complex problem.  I know that
I could simply using  a rescaling  of the y variables with estimated 
standard deviations) 

(Again) I have a single control tracer variable, and a whole set
of dependent variables with varying slopes.  The way I am specifying
the problem, I get no random effect for a, the slope, as it should
depend on species.

The lmer formal

 test.1.lmer = lmer( ylong ~ xlong + (xlong - 1 | species) + 1 , data=trial.dat)  

I provide an example below suggesting that I have built the 
data frame correctly, I think.  

# How this was run ? 
# Macintosh 10.8.5
#  R 2.15.2 GUI 1.53 Leopard build 64-bit (6335)
# lme4  version 0.999999-0


Bob Chatfield
Robert.B.Chatfield at nasa.gov
Earth Science Division, NASA (Ames), Mountain View, CA 94035 USA


 #  Test an lmer with a variable slope ... specification of random effect
> # 
> set.seed(101) # for reprducibility
> ## Define a perturbation of the key tracer -- log normal
> x = 10 +  rlnorm( 100, log(15), 0.3)
> #
> ## Define some "emission factors" linearly relating several dependent variables 
> ## as they are related to the key tracer (above background)
> ## 10 different dependent variables or "species"
> a = 0.1*rexp(10,rate=0.6)
> ## Define the pure response variable (y ... "y unscaled")
> y = array(dim=c(100,10))
> for ( i in 1:100 ) {
+   for ( j in 1:10) y[i,j] = a[j] * xd[i]
+ }
> ## Add some noise in both the response, both w.r.t. the dependent species 
> ##    and also the measurement instance
> for (i in 1:10 ) y[,i] = y[,i]*(1+rnorm(100,0.15,0.1))
> for (j in 1:100 ) y[j,] = y[j,]*(1+rnorm(10,0.15,0.1))
> #
> matplot(x,y,xlim=c(0,45),ylim=c(-1.5,max(as.vector(y))),cex=0.5)
> # points(x,y[,9],cex=2)
> points(x0,rep(0.025,100),cex=0.8)
> #
> ## Make a "long" list of the tracer-species, repeating once for every response
> ## Note:  normally I use reshape() to do this.  See: ?reshape
> xlong = rep(x,times=10) 
> ylong = as.vector(y) # collapse the matrix into a similar "long" vector
> ## Make a factor variable describing each measurement 
> ## instance and each dependent variable 
> species = as.factor(rep(paste("species",as.character(1:10),sep=""),times=100))
> #
> xlong = rep(x,times=10) 
> ylong = as.vector(y) # collapse the matrix into a similar "long" vector
> ## Make a factor variable describing each measurement 
> ## instance and each dependent variable 
> species = as.factor(rep(paste("species",as.character(1:10),sep=""),times=100))
> #
> trial.dat = data.frame(xlong=xlong,ylong=ylong,species=species)
> #  See example of structure below
> #
> test.1.lmer = lmer( ylong ~ xlong + (xlong - 1 | species) + 1 , data=trial.dat)  
> summary(test.1.lmer)
Linear mixed model fit by REML 
Formula: ylong ~ xlong + (xlong - 1 | species) + 1 
   Data: trial.dat 
  AIC  BIC logLik deviance REMLdev
 3896 3916  -1944     3877    3888
Random effects:
 Groups   Name  Variance Std.Dev.
 species  xlong 0.000    0.0000  
 Residual       2.833    1.6832  
Number of obs: 1000, groups: species, 10

Fixed effects:
            Estimate Std. Error t value
(Intercept) -0.96101    0.32930  -2.918
xlong        0.09945    0.01279   7.775

Correlation of Fixed Effects:
      (Intr)
xlong -0.987
> #
> cat( "standard deviation of original idealized data and of random effects")
standard deviation of original idealized data and of random effects
> (sd(a))
[1] 0.0821322
> sd( (ranef(test.1.lmer)$species)[["xlong"]])
[1] 0
> #
> # Add estimated slope to graph
> abline(fixef(test.1.lmer),col="darkred")
> 

#  Example of the dataset

> trial.dat[1:3,]
     xlong     ylong  species
1 23.60230 0.7164348 species1
2 27.70397 0.9228682 species2
3 22.25050 0.5650994 species3
      xlong     ylong   species
9  29.75010 0.8829359  species9
10 24.02824 0.6977277 species10

> trial.dat[11:13,]
      xlong     ylong  species
11 27.56634 0.6923857 species1
12 21.81768 0.5428702 species2
13 33.02031 0.9520825 species3
> trial.dat[101:103,]
       xlong     ylong  species
101 23.60230 0.3621467 species1
102 27.70397 0.4328762 species2
103 22.25050 0.3848661 species3
> trial.dat[111:113,]
       xlong     ylong  species
111 27.56634 0.5934288 species1
112 21.81768 0.3077124 species2
113 33.02031 0.8165563 species3

From wolfgang.viechtbauer at maastrichtuniversity.nl  Sat Feb  1 10:39:27 2014
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (STAT))
Date: Sat, 1 Feb 2014 10:39:27 +0100
Subject: [R-sig-ME] Simple: don't understand lmer formula and slopes
In-Reply-To: <D36BA9EB-7BAF-4B7E-ADDE-39A31BFD86FF@alumni.rice.edu>
References: <mailman.1863.1391206366.4563.r-sig-mixed-models@r-project.org>,
	<D36BA9EB-7BAF-4B7E-ADDE-39A31BFD86FF@alumni.rice.edu>
Message-ID: <077E31A57DA26E46AB0D493C9966AC730D9CB791BD@UM-MAIL4112.unimaas.nl>

You are fitting a model that allows the slope of the relationship between ylong and xlong to differ among species. Let's plot the data in a way that more clearly illustrates what is going on:

library(nlme)
dat.group <- groupedData(ylong ~ xlong | species, data=trial.dat)
plot(dat.group, pch=19, cex=.5)

So, it looks like that slope is pretty much the same for the different species. Not a big surprise that the slope variance is estimated to be zero.

Best,
Wolfgang

--
Wolfgang Viechtbauer, Ph.D., Statistician
Department of Psychiatry and Psychology
School for Mental Health and Neuroscience
Faculty of Health, Medicine, and Life Sciences
Maastricht University, P.O. Box 616 (VIJV1)
6200 MD Maastricht, The Netherlands
+31 (43) 388-4170 | http://www.wvbauer.com
________________________________________
From: r-sig-mixed-models-bounces at r-project.org [r-sig-mixed-models-bounces at r-project.org] On Behalf Of Robert Chatfield [chatfield at alumni.rice.edu]
Sent: Saturday, February 01, 2014 1:51 AM
To: r-sig-mixed-models at r-project.org
Cc: Robert Chatfield
Subject: [R-sig-ME] Simple: don't understand lmer formula and slopes

Here's a very simple mis-understanding of formula specification in lme4 - lmer
(Thanks to Ben and this group for help with a more fundamental question),

How does one specify slopes in lmer?
(This illustrates the essentials of a more complex problem.  I know that
I could simply using  a rescaling  of the y variables with estimated
standard deviations)

(Again) I have a single control tracer variable, and a whole set
of dependent variables with varying slopes.  The way I am specifying
the problem, I get no random effect for a, the slope, as it should
depend on species.

The lmer formal

 test.1.lmer = lmer( ylong ~ xlong + (xlong - 1 | species) + 1 , data=trial.dat)

I provide an example below suggesting that I have built the
data frame correctly, I think.

# How this was run ?
# Macintosh 10.8.5
#  R 2.15.2 GUI 1.53 Leopard build 64-bit (6335)
# lme4  version 0.999999-0


Bob Chatfield
Robert.B.Chatfield at nasa.gov
Earth Science Division, NASA (Ames), Mountain View, CA 94035 USA


 #  Test an lmer with a variable slope ... specification of random effect
> #
> set.seed(101) # for reprducibility
> ## Define a perturbation of the key tracer -- log normal
> x = 10 +  rlnorm( 100, log(15), 0.3)
> #
> ## Define some "emission factors" linearly relating several dependent variables
> ## as they are related to the key tracer (above background)
> ## 10 different dependent variables or "species"
> a = 0.1*rexp(10,rate=0.6)
> ## Define the pure response variable (y ... "y unscaled")
> y = array(dim=c(100,10))
> for ( i in 1:100 ) {
+   for ( j in 1:10) y[i,j] = a[j] * xd[i]
+ }
> ## Add some noise in both the response, both w.r.t. the dependent species
> ##    and also the measurement instance
> for (i in 1:10 ) y[,i] = y[,i]*(1+rnorm(100,0.15,0.1))
> for (j in 1:100 ) y[j,] = y[j,]*(1+rnorm(10,0.15,0.1))
> #
> matplot(x,y,xlim=c(0,45),ylim=c(-1.5,max(as.vector(y))),cex=0.5)
> # points(x,y[,9],cex=2)
> points(x0,rep(0.025,100),cex=0.8)
> #
> ## Make a "long" list of the tracer-species, repeating once for every response
> ## Note:  normally I use reshape() to do this.  See: ?reshape
> xlong = rep(x,times=10)
> ylong = as.vector(y) # collapse the matrix into a similar "long" vector
> ## Make a factor variable describing each measurement
> ## instance and each dependent variable
> species = as.factor(rep(paste("species",as.character(1:10),sep=""),times=100))
> #
> xlong = rep(x,times=10)
> ylong = as.vector(y) # collapse the matrix into a similar "long" vector
> ## Make a factor variable describing each measurement
> ## instance and each dependent variable
> species = as.factor(rep(paste("species",as.character(1:10),sep=""),times=100))
> #
> trial.dat = data.frame(xlong=xlong,ylong=ylong,species=species)
> #  See example of structure below
> #
> test.1.lmer = lmer( ylong ~ xlong + (xlong - 1 | species) + 1 , data=trial.dat)
> summary(test.1.lmer)
Linear mixed model fit by REML
Formula: ylong ~ xlong + (xlong - 1 | species) + 1
   Data: trial.dat
  AIC  BIC logLik deviance REMLdev
 3896 3916  -1944     3877    3888
Random effects:
 Groups   Name  Variance Std.Dev.
 species  xlong 0.000    0.0000
 Residual       2.833    1.6832
Number of obs: 1000, groups: species, 10

Fixed effects:
            Estimate Std. Error t value
(Intercept) -0.96101    0.32930  -2.918
xlong        0.09945    0.01279   7.775

Correlation of Fixed Effects:
      (Intr)
xlong -0.987
> #
> cat( "standard deviation of original idealized data and of random effects")
standard deviation of original idealized data and of random effects
> (sd(a))
[1] 0.0821322
> sd( (ranef(test.1.lmer)$species)[["xlong"]])
[1] 0
> #
> # Add estimated slope to graph
> abline(fixef(test.1.lmer),col="darkred")
>

#  Example of the dataset

> trial.dat[1:3,]
     xlong     ylong  species
1 23.60230 0.7164348 species1
2 27.70397 0.9228682 species2
3 22.25050 0.5650994 species3
      xlong     ylong   species
9  29.75010 0.8829359  species9
10 24.02824 0.6977277 species10

> trial.dat[11:13,]
      xlong     ylong  species
11 27.56634 0.6923857 species1
12 21.81768 0.5428702 species2
13 33.02031 0.9520825 species3
> trial.dat[101:103,]
       xlong     ylong  species
101 23.60230 0.3621467 species1
102 27.70397 0.4328762 species2
103 22.25050 0.3848661 species3
> trial.dat[111:113,]
       xlong     ylong  species
111 27.56634 0.5934288 species1
112 21.81768 0.3077124 species2
113 33.02031 0.8165563 species3
_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From henrik.singmann at psychologie.uni-freiburg.de  Sun Feb  2 00:37:26 2014
From: henrik.singmann at psychologie.uni-freiburg.de (Henrik Singmann)
Date: Sun, 2 Feb 2014 00:37:26 +0100
Subject: [R-sig-ME] random slopes versus nested random intercepts
Message-ID: <001f01cf1fa6$909379d0$b1ba6d70$@psychologie.uni-freiburg.de>

Dear all,

I have some problems wrapping my head around how to specify the random
effects structure of my data.

The data are response times (rt) from participants (variable: code) who have
worked on two different tasks in two sessions (i.e., session is the variable
coding the task). All participants have data for both sessions and I know
that session influences the rts:

> with(dl, table(session, code))
       code
session AD0210 AF0402 AL2301 AM0107 
      0    270    270    270    270 
      1    250    250    250    250 
[...]

> aggregate(rt ~ session, dl, mean)
  session       rt
1       0 2747.435
2       1 2240.403

Hence I want to model (1) the effect of session, (2) the variance introduced
by the participants, and (3) potential differences in the effect of session
across participants.

For (1) I can add a fixed effect of session and for (2) a random intercept
for participants (i.e., code).
For (3) I see two different possibilities:
 - (a) by introducing random session slopes for the participant random
intercept or 
 - (b) a (crossed) random intercept for each participant:session interaction
(i.e., basically a nested random effect).

Although possibility (a) (random slopes) seems to more closely capture the
intention (allow the session effect to vary across participants), (b) should
essentially do the same (allowing a custom intercept for each
participant:session level). However, when comparing the models, version (a)
fits better:

dl <- read.table("http://pastebin.com/raw.php?i=DDx5devK", header = TRUE)
require(lme4)
options(contrasts=c('contr.sum', 'contr.poly'))
dl$session <- factor(dl$session)

m_a <- lmer(rt ~ session + (session|code), dl)
m_b <- lmer(rt ~ session + (1|code) + (1|code:session), dl)

anova(m_a, m_b, refit = FALSE)
## Data: dl
## Models:
## m_b: rt ~ session + (1 | code) + (1 | code:session)
## m_a: rt ~ session + (session | code)
##     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(>Chisq)  
## m_b  5 224156 224193 -112073   224146                           
## m_a  6 224154 224199 -112071   224142 3.6329      1    0.05665 .
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Can someone explain why (a) provides a better fit? Am I wrong in my
assumption that both models should in principal capture the same variation?

Thanks,
Henrik


Some extra information follows, but the main question is above:

The reason why I am trying both (a) and (b) is that my ultimate goal is to
compare two further fixed effects, value and encoding. These additional
effects should also be allowed to vary across participants, sessions, and
their interaction (i.e., the effect of e.g., encoding can be different for
each participant, in each session, and the session:encoding effect should
also be allowed to be different for each participants). Achieving this seems
to me nicer via b than a:

a <- lmer(rt ~ session + value + encoding + session:encoding + session:value
+ (session + encoding + value + session:encoding + session:value|code), dl,
control = lmerControl(optCtrl = list(maxfun = 500000)))

b <- lmer(rt ~ session + value + encoding + session:encoding + session:value
+ (value + encoding|code) + (value + encoding|code:session), dl, control =
lmerControl(optCtrl = list(maxfun = 500000)))

## anova(a, b, refit = FALSE)
## Data: dl
## Models:
## b: rt ~ session + value + encoding + session:encoding + session:value + 
## b:     (value + encoding | code) + (value + encoding | code:session)
## a: rt ~ session + value + encoding + session:encoding + session:value + 
## a:     (session + encoding + value + session:encoding + session:value | 
## a:         code)
##   Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(>Chisq)  
## b 29 223217 223434 -111580   223159                           
## a 45 223222 223559 -111566   223132 27.054     16    0.04089 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Note that this last analysis throws some warnings of nonconvergence.

> sessionInfo()
R version 3.0.2 (2013-09-25)
Platform: x86_64-w64-mingw32/x64 (64-bit)

locale:
[1] LC_COLLATE=German_Germany.1252  LC_CTYPE=German_Germany.1252
LC_MONETARY=German_Germany.1252
[4] LC_NUMERIC=C                    LC_TIME=German_Germany.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] lme4_1.1-4   Matrix_1.1-2

loaded via a namespace (and not attached):
[1] grid_3.0.2      lattice_0.20-24 MASS_7.3-29     minqa_1.2.1
nlme_3.1-113    Rcpp_0.10.6     splines_3.0.2  
[8] tools_3.0.2


From jake987722 at hotmail.com  Sun Feb  2 03:14:26 2014
From: jake987722 at hotmail.com (Jake Westfall)
Date: Sat, 1 Feb 2014 19:14:26 -0700
Subject: [R-sig-ME] random slopes versus nested random intercepts
In-Reply-To: <001f01cf1fa6$909379d0$b1ba6d70$@psychologie.uni-freiburg.de>
References: <001f01cf1fa6$909379d0$b1ba6d70$@psychologie.uni-freiburg.de>
Message-ID: <BAY172-W474B737434CC53F4BE75DFCBA80@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140201/5538fc0a/attachment.pl>

From spa268 at nyu.edu  Sun Feb  2 14:13:22 2014
From: spa268 at nyu.edu (Stephen Politzer-Ahles)
Date: Sun, 2 Feb 2014 17:13:22 +0400
Subject: [R-sig-ME] Maximal random-effects lmer not converging
Message-ID: <CAJT2k_82Dkan+SJ_mbDxgRRipQ+F4whPXGFQ1ZKD0z1fc0LaDg@mail.gmail.com>

Hi Ben, thanks for your message. Here's a sample of the error messages I get:
Warning message:
In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,  :
  failure to converge in 10000 evaluations

(the specific number of evaluations varies; I've tried it with as mugh
as 1,000,000 and still get failure to converge.)

Regarding your second point: yes, is basically my question, I want to
know how to identify the variance handled by a bigger factor when all
I see in the summary is the variances handled b y the dummy-coded
coefficients. For example, say I have a model with one continuous
predictor and one 4-level factor predictor (sample attached), and I
put in random slopes for both of those. If the model doesn't converge,
I need to know whether to remove the random slope for the continuous
predictor or the random slope for the factor. But in the lmer summary,
I will get one variance estimate for the continuous predictor, and
three for the various components of that four-level factor. How do I
know, then, which predictor to not include random slopes for?

Best,
Steve






Stephen Politzer-Ahles
New York University, Abu Dhabi
Neuroscience of Language Lab
http://www.nyu.edu/projects/politzer-ahles/

>
>
> Message: 1
> Date: Fri, 31 Jan 2014 22:20:29 +0000 (UTC)
> From: Ben Bolker <bbolker at gmail.com>
> To: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Maximal random-effects lmer not converging
> Message-ID: <loom.20140131T231314-435 at post.gmane.org>
> Content-Type: text/plain; charset=us-ascii
>
> Stephen Politzer-Ahles <spa268 at ...> writes:
>
> >
> > Hello,
> >
> > I am trying to model a somewhat complicated dataset (which includes a
> > 2x2x4 interaction) with maximal random effects, based on the
> > suggestions from Barr et al. (2013). The maximal model is of course
> > not converging, and there are several things I don't understand about
> > how to proceed.
> >
> > 1. I've seen several suggestions that, when a model fails to converge,
> > you should look at the non-convergent model and then kick out
> > whichever random slope accounted for the least variance. But since my
> > model includes a four-level factor, I get different variances for each
> > level of the factor (and the problem is compounded by the interaction
> > terms, see the snippet below; there are also other random effects for
> > control variables, which I have not shown):
>
>   When you say "not converging", what do you mean exactly?  Are you getting
> warnings, and if so what are they (precisely)?  Or are you stating the
> fact that you're getting estimates of random-effects variances that are
> effectively zero, or estimates of correlations that are +/- 1?
>
> > Random effects:
> >  Groups    Name                       Variance  Std.Dev.  Corr
> > Subject   Factor1a:Factor2a  1.553e-08 1.246e-04
> >            Factor1b:Factor2a  2.000e-08 1.414e-04 0.69
> >            Factor1a:Factor2b   7.322e-09 8.557e-05 0.69 0.99
> >            Factor1b:Factor2b   2.624e-08 1.620e-04 0.55 0.70 0.71
> >            Factor1a:Factor2c   5.017e-08 2.240e-04 0.41 0.65 0.65 0.89
> >            Factor1b:Factor2c   2.220e-08 1.490e-04 0.25 0.48 0.55 0.78
> > 0.90
> >            Factor1a:Factor2d 3.972e-08 1.993e-04 0.50 0.67 0.72 0.93
> > 0.94 0.95
> >            Factor1b:Factor2d 1.642e-08 1.282e-04 0.36 0.79 0.78 0.83
> > 0.81 0.71 0.81
> >
> > So how do I evaluate the amount of variance accounted for by a
> > particular factor (or interaction), in order to determine which ones
> > to remove from the model?
>
>   Well, this is *one* component of the variance structure -- there's no
> way to drop one part of it.  (You can't say "I want to fit an interaction
> among A, B, and C, but I want to drop the B:C term" -- or at least it's
> difficult and unlikely to be sensible).  You could try (Factor1+Factor2|Subject)
> instead of (Factor1:Factor2|subject) -- that would reduce this block from
> an 8x8 variance-covariance matrix (dimension=nlevels(1)*nlevels(2)) to a 5x5
> (nlevels(1)+nlevels(2)-1) variance-covariance matrix, or from 8*9/2=36
> parameters
> to 5*6/2=15 ...
>
> >
> > 2. I am trying to model the random effects structure without
> > correlations, since I'm having a hard time getting convergence. Barr
> > et al. (2013) suggest that if you're not using correlations, then the
> > factors should be coded with deviation coding rather than treatment
> > coding. However, deviation coding does not make theoretical sense for
> > the variables I'm looking at; my design has a 4-level factor, and one
> > of those is a 'baseline' level against which I want to compare the
> > other three (my dependent measure is reaction times, and I want to see
> > which conditions are faster than baseline). So in this case should I
> > estimate the model with deviation coding, and then use post-hoc tests
> > (with some package like glht) later on to compare conditions somehow?
> > Or just go ahead using treatment coding instead of deviation coding?
>
>    Can't help you with this one without spending a lot more time thinking
> about it.  Sorry.  The fundamental problem is that when you force correlations
> to zero, the predictions about what's going on at any particular combination
> of factor levels then depends on the coding -- it is no longer invariant
> to the coding chosen ...
>
> >
> > Thank you,
> > Steve
> >
> > Stephen Politzer-Ahles
> > New York University, Abu Dhabi
> > Neuroscience of Language Lab
> > http://www.nyu.edu/projects/politzer-ahles/
> >
> >
>

From bbolker at gmail.com  Sun Feb  2 17:34:57 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 02 Feb 2014 11:34:57 -0500
Subject: [R-sig-ME] interpretation of predict plot (glmer)
In-Reply-To: <CAKrv1Ax7zNVeswtp0purYWc0Shug_3tch10+JD2pLu9iHeZG=A@mail.gmail.com>
References: <CAKrv1Ax7zNVeswtp0purYWc0Shug_3tch10+JD2pLu9iHeZG=A@mail.gmail.com>
Message-ID: <52EE73B1.6000007@gmail.com>

  As you can see, these kinds of e-mails to me personally often slip
through the cracks.  It's quite a bit better to send them to
r-sig-mixed-models at r-project.org, where someone else might answer them,
and where the answers

On 14-01-16 09:42 AM, Barbara horta e costa wrote:
> Dear Ben Bolker,
> I'd like to ask you one question about a result I am obtaining, if it's
> possible for you. Thanks so much in advance. 
> 
> I have run the following functions glmer and predict:
> 
> mF2<- glmer(CapturasTotaisInOutMenosRej_g ~ CovosTotalInOut_amostrados +
>  Season3 + (1 | VesselCode),family=Gamma(link=log),dataCovosOctopus)
> 
> newdata2 <- with(dataCovosOctopus, data.frame
> (CovosTotalInOut_amostrados=seq (10, 1000, length.out= 1000),
> Season3=rep (c("Sp", "SAW"), 500),VesselCode=rep(c("SB-1281-L",
> "SB-1287-L", "SB-1308-L", "SB-1324-L"),250)))
> 
> p4 <- predict(mF2,newdata2,type="response",REform=~(1|VesselCode))
> 
> Am I doing right? Is this newdata2 ok?

  Well, I don't know; it seems reasonable, except that (1) you might
want to have values of your continuous predictor repeated for all the
factor combinations and (2) 1000 values of your continuous predictor
might be overkill.  I like to use expand.grid for this, e.g.

newdat2 <- with(dataCovosOctopus,
  expand.grid(CovosTotalInOut_amostrados=seq(10, 1000, length.out=101),
              Season3=levels(Season3),
              VesselCode=levels(VesselCode)))

> 
> And I run the following plot. I'd like to correctly interpret the plot,
> that has 3 response curves.
> Inline images 2
> 
> 
> How can I interpret this plot?

  Don't quite know what to say about this.  It looks like there are a
bunch of overlapping values along the lower curve,  so perhaps you have
two 'outlier' random effects?  Don't know why one can't easily see the
response.  I often use ggplot to add distinctions among random effect
levels and fixed effects.

> I run the same predict but selecting the option REform=NA, and the
> subsequent plot only has 2 responses so I'm thinking one curve one is
> due to the random effect. But how can I explain the different responses
> and to which predictors should I relate with?

  Again hard to say.

> 
> Thanks a lot for your help,
> Best,
> Barbara
> 
> 
> -- 
> B?rbara H. Costa
> Marine Biologist Researcher
> Sciaena - Marine Sciences and Cooperation
> www.sciaena.org <http://www.sciaena.org/>
> 
> CCMAR | http://www.ccmar.ualg.pt <http://www.ccmar.ualg.pt/biomares/>
> ISPA | http://www.ispa.pt/ui/uie/index.asp
> BIOMARES | http://www.ccmar.ualg.pt/biomares/
> MSI-UCSB | http://www.msi.ucsb.edu/
>


From Jurij.Diaci at bf.uni-lj.si  Sun Feb  2 18:33:10 2014
From: Jurij.Diaci at bf.uni-lj.si (Diaci, Jurij)
Date: Sun, 2 Feb 2014 17:33:10 +0000
Subject: [R-sig-ME] Combining two sets of subplots obtained with different
	protocols
Message-ID: <2704FB2E9FC47F418E968B11C9F06D1506A1B66B@MAIL-BF>

Dear all,
we study oak natural regeneration in black pine plantations. Since oak seedlings are rare, we made two sets of smaller subplots (1,5 x 1,5 m) within each larger stand plot (20 x 20 m). One set was with subplots cantered at oak seedling (Oak subplot), while the second one was located independently of tree seedlings (Herb subplot; sensu Maguire & Forman, 1983). We measured several ecological variables (soil moisture, light climate?) and estimated coverage of ground vegetation per species.
We wonder, if are allowed to analyse this two data sets together in one mixed negative binomial model, with oak seedling density as dependent variable, ecological factors as fixed factors, large plots as random factor and take into account affiliation to each set of subplots as a fixed factor.
Maguire, D.A., Forman, R.T.T., 1983. Herb cover effects on tree seedling patterns in a mature Hemlock-Hardwood forest. Ecology 64, 1367-1380.

Many thanks in advance for your help.

Best regards,
Jurij

From jbaldwin at fs.fed.us  Sun Feb  2 19:02:11 2014
From: jbaldwin at fs.fed.us (Baldwin, Jim -FS)
Date: Sun, 2 Feb 2014 18:02:11 +0000
Subject: [R-sig-ME] Combining two sets of subplots obtained with
 different	protocols
In-Reply-To: <2704FB2E9FC47F418E968B11C9F06D1506A1B66B@MAIL-BF>
References: <2704FB2E9FC47F418E968B11C9F06D1506A1B66B@MAIL-BF>
Message-ID: <DDC5EC9B78340042B0D5A0C3789D456919A83F32@001FSN2MPN1-062.001f.mgd2.msft.net>

I might not be understanding the selection process for the oak subplots.  If those are centered on an oak seedling and you use oak seedling density as the dependent variable, would you not have to account for those oak subplots having at least one oak seedling?

Jim

Jim Baldwin
Station Statistician
Pacific Southwest Research Station
USDA Forest Service

-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Diaci, Jurij
Sent: Sunday, February 02, 2014 9:33 AM
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Combining two sets of subplots obtained with different protocols

Dear all,
we study oak natural regeneration in black pine plantations. Since oak seedlings are rare, we made two sets of smaller subplots (1,5 x 1,5 m) within each larger stand plot (20 x 20 m). One set was with subplots cantered at oak seedling (Oak subplot), while the second one was located independently of tree seedlings (Herb subplot; sensu Maguire & Forman, 1983). We measured several ecological variables (soil moisture, light climate...) and estimated coverage of ground vegetation per species.
We wonder, if are allowed to analyse this two data sets together in one mixed negative binomial model, with oak seedling density as dependent variable, ecological factors as fixed factors, large plots as random factor and take into account affiliation to each set of subplots as a fixed factor.
Maguire, D.A., Forman, R.T.T., 1983. Herb cover effects on tree seedling patterns in a mature Hemlock-Hardwood forest. Ecology 64, 1367-1380.

Many thanks in advance for your help.

Best regards,
Jurij
_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models




This electronic message contains information generated by the USDA solely for the intended recipients. Any unauthorized interception of this message or the use or disclosure of the information it contains may violate the law and subject the violator to civil or criminal penalties. If you believe you have received this message in error, please notify the sender and delete the email immediately.


From Jurij.Diaci at bf.uni-lj.si  Sun Feb  2 19:59:43 2014
From: Jurij.Diaci at bf.uni-lj.si (Diaci, Jurij)
Date: Sun, 2 Feb 2014 18:59:43 +0000
Subject: [R-sig-ME] Combining two sets of subplots obtained with
 different protocols
Message-ID: <2704FB2E9FC47F418E968B11C9F06D1506A1B6A2@MAIL-BF>

Jim, thank you for the replay.
 
Herb plots were systematically sampled on the Central transect running through the middle of the large/stand plot. Every 2 m left and right from the Central transect, we installed the first and the second transect/line for installation of oak subplots. First oak seedling crossing the sampling transect defined the centre of the oak subplot, next subplot was at least 2 m from the first subplot etc. This was continued until minimum of 12 oak subplots were sampled per large plot.
We used Herb plots for estimation of oak seedling density, while we wonder if we could use both sets of subplots for a regression model.
 
Jurij

________________________________________
Od: Baldwin, Jim -FS [jbaldwin at fs.fed.us]
Poslano: 2. februar 2014 19:02
Za: Diaci, Jurij; r-sig-mixed-models at r-project.org
Zadeva: RE: [R-sig-ME] Combining two sets of subplots obtained with different   protocols

I might not be understanding the selection process for the oak subplots.  If those are centered on an oak seedling and you use oak seedling density as the dependent variable, would you not have to account for those oak subplots having at least one oak seedling?

Jim

Jim Baldwin
Station Statistician
Pacific Southwest Research Station
USDA Forest Service

-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Diaci, Jurij
Sent: Sunday, February 02, 2014 9:33 AM
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Combining two sets of subplots obtained with different protocols

Dear all,
we study oak natural regeneration in black pine plantations. Since oak seedlings are rare, we made two sets of smaller subplots (1,5 x 1,5 m) within each larger stand plot (20 x 20 m). One set was with subplots cantered at oak seedling (Oak subplot), while the second one was located independently of tree seedlings (Herb subplot; sensu Maguire & Forman, 1983). We measured several ecological variables (soil moisture, light climate...) and estimated coverage of ground vegetation per species.
We wonder, if are allowed to analyse this two data sets together in one mixed negative binomial model, with oak seedling density as dependent variable, ecological factors as fixed factors, large plots as random factor and take into account affiliation to each set of subplots as a fixed factor.
Maguire, D.A., Forman, R.T.T., 1983. Herb cover effects on tree seedling patterns in a mature Hemlock-Hardwood forest. Ecology 64, 1367-1380.

Many thanks in advance for your help.

Best regards,
Jurij
_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models




This electronic message contains information generated by the USDA solely for the intended recipients. Any unauthorized interception of this message or the use or disclosure of the information it contains may violate the law and subject the violator to civil or criminal penalties. If you believe you have received this message in error, please notify the sender and delete the email immediately.

From barbarahcosta at gmail.com  Mon Feb  3 11:17:30 2014
From: barbarahcosta at gmail.com (Barbara horta e costa)
Date: Mon, 3 Feb 2014 10:17:30 +0000
Subject: [R-sig-ME] interpretation of predict plot (glmer)
In-Reply-To: <52EE73B1.6000007@gmail.com>
References: <CAKrv1Ax7zNVeswtp0purYWc0Shug_3tch10+JD2pLu9iHeZG=A@mail.gmail.com>
	<52EE73B1.6000007@gmail.com>
Message-ID: <CAKrv1AzvsXYrOvTqRr9HxEF1ZJyX0RZFiuFdLzeOgJJRqbGO5A@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140203/5718e0ae/attachment.pl>

From koen.hufkens at gmail.com  Tue Feb  4 00:21:21 2014
From: koen.hufkens at gmail.com (Koen Hufkens)
Date: Mon, 3 Feb 2014 18:21:21 -0500
Subject: [R-sig-ME] Recreating tree allometry model comparisons (Banin et
	al. 2012)
Message-ID: <CAHDtqq=8ws=UxC1uNGwuWOBTBSB_S-f6pZXSK_UcsfPZiYweWw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140203/e46966f9/attachment.pl>

From a.hayward at sheffield.ac.uk  Tue Feb  4 12:25:08 2014
From: a.hayward at sheffield.ac.uk (Adam Hayward)
Date: Tue, 4 Feb 2014 11:25:08 +0000
Subject: [R-sig-ME] prunePed in MCMCglmm
Message-ID: <CALQiR0_V4S3A2kurGJ1de6QBgL2U8TEHyChjAmnEc+HEkF4PQQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140204/75b74f75/attachment.pl>

From bruce at gilstraps.org  Tue Feb  4 20:42:05 2014
From: bruce at gilstraps.org (Bruce Gilstrap)
Date: Tue, 4 Feb 2014 13:42:05 -0600
Subject: [R-sig-ME] model specification for 2x2x2 factorial between-subjects
 design with repeated measures
Message-ID: <CAN_bDiCiyZWZZ1=+qbCZ-Z547t-DBn8PRhRsZzSeDfYR24cWUg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140204/e41e424b/attachment.pl>

From koen.hufkens at gmail.com  Tue Feb  4 20:57:13 2014
From: koen.hufkens at gmail.com (Koen Hufkens)
Date: Tue, 4 Feb 2014 14:57:13 -0500
Subject: [R-sig-ME] Recreating tree allometry model comparisons (Banin et
	al. 2012)
Message-ID: <CAHDtqqmLaNTws_LrPktsiaKocw9-8hO8uA3AZXMZJAuPPxjADw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140204/b9d0c421/attachment.pl>

From David.Duffy at qimr.edu.au  Wed Feb  5 00:56:52 2014
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Wed, 5 Feb 2014 09:56:52 +1000
Subject: [R-sig-ME] prunePed in MCMCglmm
In-Reply-To: <CALQiR0_V4S3A2kurGJ1de6QBgL2U8TEHyChjAmnEc+HEkF4PQQ@mail.gmail.com>
References: <CALQiR0_V4S3A2kurGJ1de6QBgL2U8TEHyChjAmnEc+HEkF4PQQ@mail.gmail.com>
Message-ID: <alpine.LMD.2.00.1402050931590.2880@orpheus.qimr.edu.au>

On Tue, 4 Feb 2014, Adam Hayward wrote:

> Hi all,
>
> I'm having problems using the prunePed function in the MCMCglmm package and
> wondered if anyone else had come across a similar issue.
>
> The result is that I end up with a "prunedpedigree" file which contains
> 5,161 records- one for each phenotyped individual, giving their dam and
> sire. However, it does not provide additional records for the parentage of
> their parents or other informative relatives.

ISTM prunePed() doesn't work if the individual IDs are factors (numeric 
and character are OK).  With keep=FALSE, prunePed() reduces to this loop:

     nind <- length(ind.keep) + 1
     while (length(ind.keep) != nind) {
         nind <- length(ind.keep)
         ind.keep <- union(na.omit(c(unlist(pedigree[, 2:3][match(ind.keep,
             pedigree[, 1]), ]))), ind.keep)
     }
     pedigree <- pedigree[sort(match(ind.keep, pedigree[, 1])),]

which fails for factor IDs in the union of the parental IDs with the 
existing set of individual IDs. This was a bit confusing in that the
BTped example uses factor IDs.

Cheers, David Duffy.

| David Duffy (MBBS PhD)
| email: David.Duffy at qimrberghofer.edu.au  ph: INT+61+7+3362-0217 fax: -0101
| Genetic Epidemiology, QIMR Berghofer Institute of Medical Research
| 300 Herston Rd, Brisbane, Queensland 4006, Australia  GPG 4D0B994A


From spa268 at nyu.edu  Wed Feb  5 12:18:27 2014
From: spa268 at nyu.edu (Stephen Politzer-Ahles)
Date: Wed, 5 Feb 2014 15:18:27 +0400
Subject: [R-sig-ME] model specification for 2x2x2 factorial
 between-subjects design with repeated measures
Message-ID: <CAJT2k_9cseg=y+Wja3=WRtxphjOfV5rC4o7qjUTigZvAusPUDA@mail.gmail.com>

Hi Bruce,

I don't have answers for all of your questions, but I just wanted to
add that I think you should treat Time as another fixed factor. What
you are mainly interested, it sounds like, is how the DV changes from
time 1 to time 2, and whether that change is different between group
c3.0 and group c3.1. So that's a question about the interaction
between time and c3. Ignoring the covariates, the way I would specify
this model (in lmer{lme4} syntax, because that's what I'm more
familiar with):

m <- lmer( DV ~ c3 / time + (1|Subject), data )

(I put 'time' as nested below c3 because I am assuming you're not
interested in the main effect of c3, only the interaction. Also
nesting it like this makes the output of summary(m) easy to read. But
if you want all effects, you can use * instead of / .)

>From there, you could add the covariates without adding all the
interactions (although it's possible that you might care about those
interactions as well, for example if you have reason to think that
people in c2.1 respond better to the treatment c3.1 than people in
c2.0 do):

m1 <- lmer( DV ~ c1 + c2 + (c3/time) + (1|Subject), data )

You can then compare this to a maximally similar model that does not
have the interaction:

m2 <- lmer( DV ~ c1 + c2 + (c3+time) + (1|Subject), data )

And of course you should consider using a maximal random effects
structure (in the models I put above, (1|Subject) means there are only
random intercepts per subject).

The interactions are starting to sound complicated; you might be able
to simplify your work if for each subject you subtract the dv at time
1 from the dv at time 2, so the new DV is just a measure of how much
they improved from time 1 to time 2, and then you don't have to worry
about always testing the interaction with time.


Stephen Politzer-Ahles
New York University, Abu Dhabi
Neuroscience of Language Lab
http://www.nyu.edu/projects/politzer-ahles/



> Message: 2
> Date: Tue, 4 Feb 2014 13:42:05 -0600
> From: Bruce Gilstrap <bruce at gilstraps.org>
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] model specification for 2x2x2 factorial
>         between-subjects design with repeated measures
> Message-ID:
>         <CAN_bDiCiyZWZZ1=+qbCZ-Z547t-DBn8PRhRsZzSeDfYR24cWUg at mail.gmail.com>
> Content-Type: text/plain
>
> My colleagues and I are interested in analyzing a dataset generated by a
> 2x2x2 factorial design where each of the 3 factors are between-subjects
> factors, each consisting of 2 levels, which results in 8 experimental
> conditions. Therefore, each subject was randomly assigned to 1 of these
> conditions. Additionally, the DV was measured twice, so our design includes
> a repeated measure. Basically, then, the data look like this (in so-called
> *long* form):
>
> subj  c1  c2  c3  time   dv
>    1   0   0   0     1  1.1
>    2   0   0   1     1  3.2
>    3   0   1   0     1  4.4
>    4   0   1   1     1  6.2
>    5   1   0   0     1  3.6
>    6   1   0   1     1  2.2
>    7   1   1   0     1  5.1
>    8   1   1   1     1  5.5
>    1   0   0   0     2  3.7
>    2   0   0   1     2  6.1
>    3   0   1   0     2  4.7
>    4   0   1   1     2  2.0
>    5   1   0   0     2  4.6
>    6   1   0   1     2  4.3
>    7   1   1   0     2  5.2
>    8   1   1   1     2  5.0
>
> So far, this is a relatively straightforward design. However, it is
> complicated by the fact that the third condition is essentially a
> treatment. That is, it was unknown to the subjects when the DV was measured
> at time 1. After the first measurement of the DV, they were presented with
> a treatment, and then we measured the DV at time 2. Therefore, the design
> includes a pre-/post-test element as well. Our research question is focused
> on the effect of the treatment (c3), and we are not directly interested in
> the effects of c1 and c2. We included these conditions because prior
> research suggests that they may affect our DV. Therefore, we are planning
> to treat these conditions as covariates.
>
> Our plan is to analyze the data using multilevel modeling (in R) due to the
> non-independence of observations created by the repeated measure. Using the
> lme function from the nlme package, here are the models we plan to compare:
>
> m0 <- lme(fixed  = DV ~ 1,
>           random = ~ 1 | subj/time,
>           method = "ML",
>           data = d.long)
>
> m1 <- lme(fixed  = DV ~ 1 + c1 + c2,
>           random = ~ 1 | subj/time,
>           method = "ML",
>           data = d.long)
>
> m2 <- lme(fixed  = DV ~ 1 + c1 + c2 + c3,
>           random = ~ 1 | subj/time,
>           method = "ML",
>           data = d.long)
>
> where:
>
>    - m0 is the null (baseline) model
>    - m1 is what we are calling the covariate model (intended to address the
>    question, "Do the covariates have the effects suggested by prior research?")
>    - m2 is the hypothesized model (intended to address our research
>    question, "Does the treatment have the effect we hypothesized?")
>
> Of course, there is other work to be done after specifying the models, but
> we are most interested at this point in making sure we have specified them
> appropriately.
>
> We would appreciate receiving advice about our plans for analyzing these
> data, and are particularly interested in comments about the following
> issues:
>
>    1. Is our intended approach to treat c1 and c2 as covariates defensible?
>    2. If so, will our R-specific model specifications allow us to answer
>    our research question?
>    3. In any case, are there any better approaches?
>
> Thanks,
>
> Bruce
>
>
> Here is some R code that will generate a basic dataset like the one shown
> above:
>   subj <- rep(c(1,2,3,4,5,6,7,8), times = 2)
>   c1   <- rep(c(0,0,0,0,1,1,1,1), times = 2)
>   c2   <- rep(c(0,0,1,1), times = 4)
>   c3   <- rep(c(0,1), times = 8)
>   time <- c(rep(1, times = 8), rep(2, times = 8))
>   set.seed(14)
>   dv   <- runif(16, 1.0, 7.0)
>   data <- as.data.frame(cbind(subj, c1, c2, c3, time, dv))
>   data[ ,1:5 ] <- lapply(data[ ,1:5 ], factor)
>   data
>   str(data)
>
>         [[alternative HTML version deleted]]
>
>
>
> ------------------------------
>
> Message: 3
> Date: Tue, 4 Feb 2014 14:57:13 -0500
> From: Koen Hufkens <koen.hufkens at gmail.com>
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] Recreating tree allometry model comparisons (Banin
>         et      al. 2012)
> Message-ID:
>         <CAHDtqqmLaNTws_LrPktsiaKocw9-8hO8uA3AZXMZJAuPPxjADw at mail.gmail.com>
> Content-Type: text/plain
>
> Hi list,
>
> I have the feeling this came through the list and my message bounced, if it
> did not excuses for double posting.
>
> Using mixed models has been a while for me so I'm struggling with
> recreating an approach as described in this publication (Banin et al. 2012:
> http://onlinelibrary.wiley.com/doi/10.1111/j.1466-8238.2012.00778.x/full).
>
> A description of the methodology is described here: "To determine whether
> curve parameters were significantly different amongst all pairs of
> continents, the dataset was split into all possible paired combinations and
> a full model (where continent was specified as a fixed factor to provide
> separate estimates for two continents) compared to a reduced model (where
> data for the two continents were pooled) (Motulsky & Christopoulos, 2004)...
>
> ... Full and reduced models were compared using AICs and likelihood ratio
> tests, which provide a p-value and an opportunity to assess whether models
> provide a significant improvement (Pinheiro & Bates, 2000) . "
>
> The model or curve as mentioned above is defined as
>
> H ~ a - b * exp(-c * DBH) where H and DBH are the measured tree height and
> tree diameter at breast height, respectively, see equation 1, in the above
> paper.
>
> DBHmodel <- function(x,a,b,c){a-b*exp(-c*x)}
>
> Instead of continents I would like to pairwise compare site locations,
> hence the setup will not change as I will substitute 'continent' for 'site'.
>
> I define the 'full' model as such:
>
> dataGrouped <- groupedData(height~dbh | site, data=data)
> model1 <-
> nlsList(height~DBHmod(dbh,a,b,c),data=dataGrouped,start=list(a=40,b=40,c=0.02))
> model1.nlme <- nlme(model1)
>
> This gives me two site based models but I'm at a loss on how to 1) define
> the reduced 'population' model such such that it plays nicely with nlme and
> treats the data as 'one' 2) perform the likelihood ratio tests on this due
> to the lack of the former.
>
> Any help is appreciated while I wander through Pinheiro and Bates to look
> for a solution as well. I feel that this should be trivial but I'm still
> lost in the nlme model formulation.
>
> Kind regards,
> K
>
> --
> Dr. Koen Hufkens
>
> Harvard University
> Department of Organismic & Evolutionary Biology
> Richardson Lab
> ~
> Ghent University
> Faculty of Bioscience Engineering
> Isotope Bioscience Laboratory
> <http://www.isofys.ugent.be>
>
>         [[alternative HTML version deleted]]
>
>
>
> ------------------------------
>
> Message: 4
> Date: Wed, 5 Feb 2014 09:56:52 +1000
> From: David Duffy <David.Duffy at qimr.edu.au>
> To: Adam Hayward <a.hayward at sheffield.ac.uk>
> Cc: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] prunePed in MCMCglmm
> Message-ID: <alpine.LMD.2.00.1402050931590.2880 at orpheus.qimr.edu.au>
> Content-Type: text/plain; charset="US-ASCII"; format=flowed
>
> On Tue, 4 Feb 2014, Adam Hayward wrote:
>
> > Hi all,
> >
> > I'm having problems using the prunePed function in the MCMCglmm package and
> > wondered if anyone else had come across a similar issue.
> >
> > The result is that I end up with a "prunedpedigree" file which contains
> > 5,161 records- one for each phenotyped individual, giving their dam and
> > sire. However, it does not provide additional records for the parentage of
> > their parents or other informative relatives.
>
> ISTM prunePed() doesn't work if the individual IDs are factors (numeric
> and character are OK).  With keep=FALSE, prunePed() reduces to this loop:
>
>      nind <- length(ind.keep) + 1
>      while (length(ind.keep) != nind) {
>          nind <- length(ind.keep)
>          ind.keep <- union(na.omit(c(unlist(pedigree[, 2:3][match(ind.keep,
>              pedigree[, 1]), ]))), ind.keep)
>      }
>      pedigree <- pedigree[sort(match(ind.keep, pedigree[, 1])),]
>
> which fails for factor IDs in the union of the parental IDs with the
> existing set of individual IDs. This was a bit confusing in that the
> BTped example uses factor IDs.
>
> Cheers, David Duffy.
>
> | David Duffy (MBBS PhD)
> | email: David.Duffy at qimrberghofer.edu.au  ph: INT+61+7+3362-0217 fax: -0101
> | Genetic Epidemiology, QIMR Berghofer Institute of Medical Research
> | 300 Herston Rd, Brisbane, Queensland 4006, Australia  GPG 4D0B994A
>
>
>
> ------------------------------
>
> _______________________________________________
> R-sig-mixed-models mailing list
> R-sig-mixed-models at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
> End of R-sig-mixed-models Digest, Vol 86, Issue 5
> *************************************************


From tim.cole at ucl.ac.uk  Wed Feb  5 12:27:31 2014
From: tim.cole at ucl.ac.uk (Cole, Tim)
Date: Wed, 5 Feb 2014 11:27:31 +0000
Subject: [R-sig-ME] model specification for 2x2x2 factorial
 between-subjects design with repeated measures
Message-ID: <CF17D09B.1E892%tim.cole@ucl.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140205/ef347d9a/attachment.pl>

From jake987722 at hotmail.com  Wed Feb  5 17:42:39 2014
From: jake987722 at hotmail.com (Jake Westfall)
Date: Wed, 5 Feb 2014 09:42:39 -0700
Subject: [R-sig-ME] model specification for 2x2x2 factorial
 between-subjects design with repeated measures
In-Reply-To: <CF17D09B.1E892%tim.cole@ucl.ac.uk>
References: <CF17D09B.1E892%tim.cole@ucl.ac.uk>
Message-ID: <BAY172-W17D60EB19EFDFDDED71B54CB950@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140205/bc7c2c95/attachment.pl>

From A.Wilson at exeter.ac.uk  Wed Feb  5 00:34:36 2014
From: A.Wilson at exeter.ac.uk (Wilson, Alastair)
Date: Tue, 4 Feb 2014 23:34:36 +0000
Subject: [R-sig-ME] pedigreemm number of levels per grouping factor
Message-ID: <DBBDA54CB159CC41A0A01219B109A4D001BBD248@VMEXCHANGEMBS5A.isad.isadroot.ex.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140204/6eada5b8/attachment.pl>

From bbolker at gmail.com  Wed Feb  5 19:35:21 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 05 Feb 2014 13:35:21 -0500
Subject: [R-sig-ME] pedigreemm number of levels per grouping factor
In-Reply-To: <DBBDA54CB159CC41A0A01219B109A4D001BBD248@VMEXCHANGEMBS5A.isad.isadroot.ex.ac.uk>
References: <DBBDA54CB159CC41A0A01219B109A4D001BBD248@VMEXCHANGEMBS5A.isad.isadroot.ex.ac.uk>
Message-ID: <52F28469.9010203@gmail.com>

On 14-02-04 06:34 PM, Wilson, Alastair wrote:
> I???m trying to fit a pedigree-based mixed model (ie animal model) to
> estimate a genetic variance and a couple of fixed effects in a
> simulated data set. I???ve fitted the model using AsReml-R and
> MCMCglmm. I wanted to try using pedigreemm but get a error
> 
> Error in checkNlevels(reTrms$flist, n = n, control) : number of
> levels of each grouping factor must be < number of observations
> 
> I *think* this is telling me I don???t have multiple observations per
> factor level (here ID) which is true. However, I shouldn???t need
> repeat measures to estimate the random effect variance here (i.e.
> VA), so am assuming I have not got the code (below) right. Any
> pointers v gratefully received.
> 
> Alastair

  I'm not sure, but I guess that this is one of the cases where
individual-level random effects *do* make sense in a LMM context.  I
don't know whether pedigreemm allows you to pass arguments through to
lmer, but if it does you can use
control=lmerControl(check.nobs.vs.nlev="ignore") to suppress this check.
 You might also be able to do this globally via

options(lmerControl=list(check.nobs.vs.nlev="ignore"))

  (The maintainer of the package might want to consider suppressing this
check by default, if this is a common issue ...)

  Ben Bolker



> 
> ############### library(pedigreemm) setwd("c:/prussia/")
> 
> voles<-read.table("voles.txt", header=T) 
> voles$ID<-as.factor(voles$ID) voles$sex<-as.factor(voles$sex)
> 
> volesPED<-read.table("volesPED.txt",header=T) 
> volesPED2<-pedigree(volesPED$SIRE,volesPED$DAM,volesPED$ID)
> 
> model3<-pedigreemm(size~forage+sex+(1|ID),
> pedigree=list(ID=volesPED2), data=voles, REML=TRUE)
> 

  It would be nice if the example were reproducible ...


From j.hadfield at ed.ac.uk  Thu Feb  6 16:19:57 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Thu, 06 Feb 2014 15:19:57 +0000
Subject: [R-sig-ME] Specify prior for sir-function and starting values
 in the MCMCglmm packages?
In-Reply-To: <AD0050057515F54084E7D5B93478C8481F7BA0260B@winxbede39.exchange.xchg>
References: <AD0050057515F54084E7D5B93478C8481F7BA0260B@winxbede39.exchange.xchg>
Message-ID: <20140206151957.208841061ad30so4@www.staffmail.ed.ac.uk>

Hi Linus,

Sorry for the delay. The warning

"priors for sir parameters not implemented"

is correct, and so the priors are essentially flat priors, equivalent  
to a Gaussian with mean zero and very large variance. The default  
prior in MCMCglmm for the fixed is mean zero and a variance of 10^8  
and so I would expect them to give very similar answers (unless there  
is confounding). For some models/data (for example categorical data  
and complete or near-complete separation) then these priors can behave  
poorly, but given sir models can only be fitted to Gaussian data in  
MCMCglmm, I guess this is not the case.

In the next update (which I realise is taking a long time) I will  
implement prior distributions and starting values.

Cheers,

Jarrod






Quoting Linus Holtermann <holtermann at hwwi.org> on Fri, 31 Jan 2014  
12:24:56 +0100:

> Dear list members,
>
> unfortunately Jarrod Hadfield is on field work until the end of June  
> and cant help me with my problems.  Maybe someone on the mailing  
> list knows how to specify starting values and a Prior for the  
> sir-function in MCMCglmm. Short code examples would be great.
>
> Best regards,
>
>
> Linus Holtermann
> Hamburgisches WeltWirtschaftsInstitut gemeinn?tzige GmbH (HWWI)
> Heimhuder Stra?e 71
> 20148 Hamburg
> Tel +49-(0)40-340576-336
> Fax+49-(0)40-340576-776
> Internet: www.hwwi.org
> Email: holtermann at hwwi.org
>
> AmtsgerichtHamburg HRB 94303
> Gesch?ftsf?hrer: Prof. Dr. Thomas Straubhaar, Gunnar Geyer
> Umsatzsteuer-ID: DE 241849425
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From j.hadfield at ed.ac.uk  Thu Feb  6 16:32:31 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Thu, 06 Feb 2014 15:32:31 +0000
Subject: [R-sig-ME] MCMCglmm model for heteroskedasticity at level one
In-Reply-To: <DUB119-W12E255DA17CF9D44C6BE2480AC0@phx.gbl>
References: <DUB119-W12E255DA17CF9D44C6BE2480AC0@phx.gbl>
Message-ID: <20140206153231.210813rrjkg5n19c@www.staffmail.ed.ac.uk>

Hi Maxim,

I'm not very clear about the model you want to fit. If we take your  
stackexchange model, do you wish to fit a random intercept-slope model  
(with the covariate x1) at the country level and the residual level?

If so, then random=~us(1 + x1):country fits the country  
intercept/slopes (with covariance between them). The between country  
variance is quadratic in x1:

V_i + 2*x1*V_is + (x1^2)*V_s

where V_i is the variance in intercepts, V_s is the variance in slopes  
and V_is is covariance between intercepts and slopes.

This change in variance with respect to x1 is where the information  
comes from for estimating level-1 heterogeneity. If you fit (assuming  
x1 is always positive):

idh(sqrt(x1)):units

in the *random* effect part of the model, and leave the residual part  
as the default units, then the level-1 variance is linear in x1:

Vu+sqrt(x1)^2*Vs = Vu+x1*Vs

where Vu is the units variance, and Vs is the variance associated with  
the random `slopes' in the random effect part of the model.  Clearly  
you could transform x1 differently.

Not sure if this is useful?

Jarrod











Quoting Maxim Kovalenko <kovla123 at hotmail.com> on Wed, 29 Jan 2014  
15:34:08 +0100:

>
>
> Dear all,
>
> I kindly ask the help of this community with the following issue. My  
> research question is about individual-level variance of employment  
> stability in several European countries. The number of countries is  
> small (N=13), therefore MCMC is a more proper estimation method than  
> ML. Variance on the country level doesn't interest me as much, but I  
> do have to take it into account of course.
> When I run my model in R/MLwiN via R2MLwiN package, its formula is  
> specified as approximately follows:
> y ~ (0|cons+careertype+gender) + (1|cons+careertype) + (2|cons)
> To decode: there fixed part contains the intercept, career type  
> (more or less job mobility) and gender. Random effects on the  
> individual level entail the constant and career type, whereas random  
> effects on level to include only the constant. Therefore it is a  
> random intercept model with heteroskedasticity on level one. For  
> more details on this model please see  
> http://stats.stackexchange.com/questions/83148/specifying-a-multilevel-model-in-mcmcglmm-r-that-is-heteroskedastic-at-level, I have included a screenshot there to give a better  
> idea.
> Is it possible at all to estimate a similar model in MCMCglmm? I  
> cannot figure out how to specify variance at level one to be  
> dependent on one of the predictors.
> Thank you so much in advance for any tips!
> Kind regards,Maxim
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From j.hadfield at ed.ac.uk  Thu Feb  6 16:50:07 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Thu, 06 Feb 2014 15:50:07 +0000
Subject: [R-sig-ME] Multiresponse MCMCglmm model specification and
 questions
In-Reply-To: <7613F758-290D-451A-87CF-F508218BDC91@yahoo.com.br>
References: <7613F758-290D-451A-87CF-F508218BDC91@yahoo.com.br>
Message-ID: <20140206155007.16662plaxz55khmo@www.staffmail.ed.ac.uk>

Hi Rafael,

1/ The reason that nu=1.002 is sometimes used for a 2x2 covariance  
matrix is because the marginal prior for one of the variances is  
equivalent to an inverse-Wishart with nu=0.002. You would also have to  
rescale V to get an inverse-gamma distribution for the marginal  
variance, as given in the CourseNotes footnote.

2/ This is the correlation in the phyloegentic effects, after  
accounting for the fixed effects. It may also make sense to have the  
residual structure as us(trait):units to model any residual  
correlation between the traits. The chain can get `stuck' at extreme  
correlations (-1 or 1) as it can for variances close to zero. The best  
way to see if this is a problem is to plot the MCMC time-series for  
the correlation (not the covariance) and see if it looks horrible.  
Parameter expansion can alleviate this problem, but the resulting  
priors on the covariance matrix are not inverse-Wishart but scaled  
Multivariate-F.

3/ Tricky  - and demanding of the data.

us(at.level(X1,1):trait):spp+us(at.level(X1,2):trait):spp

Fits two phylogentic covariance matrices, one for each of the two  
levels of X1. However, it assumes that there are no phylogenetic  
correlations in the response
between species that have different levels of X1.

us(X1:trait):spp

fits the phylogenetic covariances between the different levels of X1 too.

However, unless you have a very large phylogeny I would consider  
sticking to the simpler model. In addition, MCMCglmm cannot fit block  
diagonal structures for the residual (co)variances, and I think you  
would want to do this to ensure that the phylogenetic covariances  
weren't actually due to a mis-specified residual structure.  A  
saturated residual variance structure would be 2 2x2 covariance  
matrices, one for each level of X1. A single 4X4 covariance matrix  
(us(trait:X1):units) can be fitted, but the 2 2x2 off-diagonal blocks  
(the residual covariances between observations made on different  
levels of X1) cannot be estimated (no observation is associated with  
*both* levels of X1). As discussed elsewhere (I think a post by Celine  
Teplitsky) this may cause problems.

Cheers,

Jarrod





Quoting Rafael Maia <queirozrafaelmv at yahoo.com.br> on Fri, 17 Jan 2014  
12:05:16 -0500:

> Dear list members,
>
> I am attempting to run a multiresponse model to investigate factors  
> influencing the evolution of a trait measured in both males and  
> females of multiple species. Therefore I chose a multiresponse  
> approach using MCMCglmm, but my model has a series of little caveats  
> (don?t they all?) that I?d like to make sure I?m addressing  
> correctly. Here is a brief description of the model I?m trying to fit:
>
> Ymale, Yfemale: the same response variable, measured in males and females
>
> X1: a variable (factor, 2 levels) that I expect to affect both sexes  
> differently
> X2: a variable (factor, 2 levels) that I expect to affect both sexes  
> equally (or estimate the joint effect across sexes)
> X3: a continuous variable measured only for males (which I therefore  
> expect to only have an effect in male measurements)
>
> so I am currently specifying the model as below:
>
> mod <- cbind(Ymale, Yfemale) ~ trait*X1 + X2 + at.level(trait, 1):X3
>
> ainv <- inverseA(phylo)$Ainv
>
> Prior <- list(R=list(V=diag(2), nu=0.002), G=list(G1=list(V=diag(2),  
> nu=0.002)))
>
> result <- MCMCglmm(mod,
>     random = ~us(trait):spp, rcov = ~idh(trait):units,
>     data=dat, prior = Prior, family = rep(?gaussian?, 2),
>     nitt=2100000, burnin=100000, thin=1000)
>
> with that, I have a couple questions (besides: does this model  
> specification look reasonable?):
>
> 1. Prior: I wanna make sure I?m specifying an inverse-gamma with  
> scale=shape=0.001 for the variance components. Based on footnote 1  
> of the Course Notes (p.102) I think I am based on the changes, but  
> I?ve seen several times in mailing lists responses and other papers  
> nu=1.002 being used for a matrix of same dimensions. My results are  
> robust to either specification but I?d like to make sure I?m doing  
> what I think I?m doing, given that there have been changes on this  
> regard.
>
> 2. I am calculating the correlation between ymale and female as:
>     result$VCV[,2]/sqrt(result$VCV[,1]*result$VCV[,4])
> as described in the tutorial vignette. My values, however, are  
> extremely high. The two traits are indeed strongly correlated among  
> species, but I?m getting credible intervals of 0.96 - 0.99 (for  
> comparison, a raw non-phylogenetic correlation of the traits is  
> 0.75). Am I forgetting to include any variance components to  
> calculate the intersexual correlation? Is this increase in the  
> correlation expected given the inclusion of the fixed and random  
> effects in the model?
>
> 3. I have a hypothesis that the correlation between male and female  
> traits should be stronger for one level of X1 than for the other. Is  
> it possible to specify random terms in order to calculate  
> (co)variances of the traits conditional on X1?
>
> I deeply appreciate any help you can provide. Cheers!
>
>
> Abra?os,
> Rafael Maia
> ---
> http://www.rafaelmaia.net/
> PhD Candidate, Integrated Bioscience
> University of Akron
> "A little learning is a dangerous thing; drink deep, or taste not  
> the Pierian spring." (A. Pope)
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From j.hadfield at ed.ac.uk  Thu Feb  6 16:53:03 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Thu, 06 Feb 2014 15:53:03 +0000
Subject: [R-sig-ME] Need help with MCMCglmm
In-Reply-To: <82632B9B-BE4F-4451-A284-52EFD1791B4B@stir.ac.uk>
References: <82632B9B-BE4F-4451-A284-52EFD1791B4B@stir.ac.uk>
Message-ID: <20140206155303.12064rbxl2wwkjk0@www.staffmail.ed.ac.uk>

Hi Sumayia,

I'm not sure why you are getting this error. Has the problem been  
resolved? If not and you can send me the data I will take a look at it.

Cheers,

Jarrod


Quoting Sumayia Bashir <sumayia.bashir at stir.ac.uk> on Sat, 11 Jan 2014  
22:56:53 +0000:

> Hi,
>
> I am trying to run a MCMCglmm model using multinomial family but  
> getting an error message.
>
> My prior is:
>
> prior1<-list(
> G=list((G1=list(V=0.01, nu = 0.1)),
>     (G2=list(V=diag(2)* 0.01, nu = 0.1))),
> R=list(V= diag(2)*0.01,nu=0.1))
> prior1
>
> my model is following:
>
> m1<-MCMCglmm(cbind(M18, A18) ~ sex + treatment + group,
> random = ~ batch + idh(group):mitotype,
> rcov = ~ idh(group):units,
> family = "multinomial2",
> prior  = prior1,
> verbose = FALSE,
> nitt=150000,thin=100,burnin=50000,
> data = SBR)
>
> when I try to run the above model I get the following error message
>
> Error in if (nJ < 1) { : missing value where TRUE/FALSE needed
>
> Can any one please help me to understand what is wrong with my model setting?
>
> Many Thanks
>
> Sumayia
>
> --
> The University of Stirling has been ranked in the top 12 of UK  
> universities for graduate employment*.
> 94% of our 2012 graduates were in work and/or further study within  
> six months of graduation.
> *The Telegraph
> The University of Stirling is a charity registered in Scotland,  
> number SC 011159.
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From j.hadfield at ed.ac.uk  Thu Feb  6 16:56:59 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Thu, 06 Feb 2014 15:56:59 +0000
Subject: [R-sig-ME] predictions from MCMCglmm multinomial model
In-Reply-To: <F7787B82-3289-4F03-B3E7-8BBD9AFD0BCE@gmail.com>
References: <F7787B82-3289-4F03-B3E7-8BBD9AFD0BCE@gmail.com>
Message-ID: <20140206155659.6118464qxrvjn9yc@www.staffmail.ed.ac.uk>

Hi Achaz,

Perhaps this is now sorted. If you have marginal=NULL in the call to  
predict then you will still get a prediction for each herder. However,  
since you have no fixed effects in the model except trait (such that  
all herders in a region have the same fixed effect structure) then you  
can pull out the prediction for a single herder in each region, and  
this will be equivalent to a prediction for each region.

Cheers,

Jarrod

Quoting Achaz Hardenberg <achaz.hardenberg at gmail.com> on Thu, 12 Dec  
2013 10:27:58 +0100:

> Dear all,
> I have fitted the following model:
>
> sheep-predation.mcmc<-MCMCglmm(cbind(Puma,Fox,Dog,Surv.sheep)~trait-1,random=~Region,rcov=us(trait):units, family=  
> "multinomial4",data=carnivorosAllsheep,verbose=FALSE)
> where Puma,Fox and Dog are the counts of sheep predated by these  
> predators, as denounced by herders (one row per herder); Surv.sheep  
> are the number of sheep remaining after the predations. herders are  
> grouped in 4 different Regions which I specified as a random effect.
>
> I would like to get the predicted probability of predation with  
> credible intervals for each Region and for each species of predator.  
> I tried:
> predict(sheep-predation.mcmc,marginal=~Region,interval="confidence?), but it  
> gives me the prediction for each herder in each Region, rather than  
> the global estimate for each region/species.
>
> Any help is greatly appreciated!
> cheers,
>
> Achaz von Hardenberg
> Gran Paradiso National Park
>
>
>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From anniehoen at gmail.com  Thu Feb  6 23:17:38 2014
From: anniehoen at gmail.com (Anne Hoen)
Date: Thu, 6 Feb 2014 17:17:38 -0500
Subject: [R-sig-ME] help with pdDiag and nlme
Message-ID: <F94F456D-6294-40BA-B656-CA94290E3603@gmail.com>

Hi,
I am trying to fit a non-linear mixed effects model using a 3-parameter logistic function. 
My data set can be made using this:

plate<-c(rep("A", 6), rep("B", 6), rep("C", 6), rep("D", 6))
preparation<-rep(c(rep("reference", 3), rep("test", 3)), 4)
concentration<-rep(c(1, 2, 3), 8)
OD<-runif(24, 0, 1)
data<-data.frame(plate, preparation, concentration, OD)
data<-groupedData(OD~concentration | plate, data=data)

I want to fit a model like this:

nlme(OD~SSlogis(concentration, Asym, xmid, scal), 
     fixed=preparation~1, 
     random=pdDiag(plate~1),
     data=data)

But I get the error: Error in nlme.formula(OD ~ SSlogis(concentration, Asym, xmid, scal), fixed = preparation ~  : 
  subscript out of bounds

I've determined that (I think) the error comes from the random effect specification, because when I run: pdDiag(data$plate~1), I get: Uninitialized positive definite matrix structure of class pdDiag, which is not what I expected. I am not sure what is wrong. If anyone can help me, I would appreciate it so much!!

Thanks
Annie

From A.Wilson at exeter.ac.uk  Thu Feb  6 16:50:45 2014
From: A.Wilson at exeter.ac.uk (Wilson, Alastair)
Date: Thu, 6 Feb 2014 15:50:45 +0000
Subject: [R-sig-ME] pedigreemm number of levels per grouping factor
In-Reply-To: <52F28469.9010203@gmail.com>
References: <DBBDA54CB159CC41A0A01219B109A4D001BBD248@VMEXCHANGEMBS5A.isad.isadroot.ex.ac.uk>
	<52F28469.9010203@gmail.com>
Message-ID: <DBBDA54CB159CC41A0A01219B109A4D001BBF06C@VMEXCHANGEMBS5A.isad.isadroot.ex.ac.uk>


Thanks for the suggestion Ben. Sadly it didn't provide a solution. The error message indicates the variance component I am after is unidentifiable, which would not be the case if I have correctly got the random ID effect linked to the pedigree.

Error in .sortCsparse(.Call(dtCMatrix_sparse_solve, a, b)) : 
  Dimensions of system to be solved are inconsistent
In addition: Warning message:
In checkZrank(reTrms$Zt, n = n, control, nonSmall = 1e+06) :
  number of observations <= rank(Z); variance-covariance matrix will be unidentifiable

I'll maybe contact the package maintainer directly unless anyone else has thoughts/experience with pedigreemm.

Alastair


-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Ben Bolker
Sent: 05 February 2014 18:35
To: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] pedigreemm number of levels per grouping factor

On 14-02-04 06:34 PM, Wilson, Alastair wrote:
> I???m trying to fit a pedigree-based mixed model (ie animal model) to 
> estimate a genetic variance and a couple of fixed effects in a 
> simulated data set. I???ve fitted the model using AsReml-R and 
> MCMCglmm. I wanted to try using pedigreemm but get a error
> 
> Error in checkNlevels(reTrms$flist, n = n, control) : number of levels 
> of each grouping factor must be < number of observations
> 
> I *think* this is telling me I don???t have multiple observations per 
> factor level (here ID) which is true. However, I shouldn???t need 
> repeat measures to estimate the random effect variance here (i.e.
> VA), so am assuming I have not got the code (below) right. Any 
> pointers v gratefully received.
> 
> Alastair

  I'm not sure, but I guess that this is one of the cases where individual-level random effects *do* make sense in a LMM context.  I don't know whether pedigreemm allows you to pass arguments through to lmer, but if it does you can use
control=lmerControl(check.nobs.vs.nlev="ignore") to suppress this check.
 You might also be able to do this globally via

options(lmerControl=list(check.nobs.vs.nlev="ignore"))

  (The maintainer of the package might want to consider suppressing this check by default, if this is a common issue ...)

  Ben Bolker



> 
> ############### library(pedigreemm) setwd("c:/prussia/")
> 
> voles<-read.table("voles.txt", header=T)
> voles$ID<-as.factor(voles$ID) voles$sex<-as.factor(voles$sex)
> 
> volesPED<-read.table("volesPED.txt",header=T)
> volesPED2<-pedigree(volesPED$SIRE,volesPED$DAM,volesPED$ID)
> 
> model3<-pedigreemm(size~forage+sex+(1|ID),
> pedigree=list(ID=volesPED2), data=voles, REML=TRUE)
> 

  It would be nice if the example were reproducible ...

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From yulya258 at yahoo.com  Fri Feb  7 04:09:02 2014
From: yulya258 at yahoo.com (Yla Savh)
Date: Thu, 6 Feb 2014 19:09:02 -0800 (PST)
Subject: [R-sig-ME] Differences in autocorrelation between GLS and LME?
Message-ID: <1391742542.45078.YahooMailNeo@web162703.mail.bf1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140206/511418d6/attachment.pl>

From Tom.Wenseleers at bio.kuleuven.be  Fri Feb  7 13:24:30 2014
From: Tom.Wenseleers at bio.kuleuven.be (Tom Wenseleers)
Date: Fri, 7 Feb 2014 12:24:30 +0000
Subject: [R-sig-ME] Checking overdispersion in binomial mixed model
Message-ID: <37EFC97028F3E44082ACC5CBEC0056301136C008@ICTS-S-MBX13.luna.kuleuven.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140207/bc48e8e9/attachment.pl>

From alku at dtu.dk  Fri Feb  7 13:54:06 2014
From: alku at dtu.dk (Alexandra Kuznetsova)
Date: Fri, 7 Feb 2014 12:54:06 +0000
Subject: [R-sig-ME] bug/problem with update for merMod
Message-ID: <0566E17B6DEC62459078112371B7508E12FD31@ait-pex02mbx05.win.dtu.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140207/fe191ed9/attachment.pl>

From S.Ellison at lgcgroup.com  Fri Feb  7 14:12:58 2014
From: S.Ellison at lgcgroup.com (S Ellison)
Date: Fri, 7 Feb 2014 13:12:58 +0000
Subject: [R-sig-ME] Checking overdispersion in binomial mixed model
In-Reply-To: <37EFC97028F3E44082ACC5CBEC0056301136C008@ICTS-S-MBX13.luna.kuleuven.be>
References: <37EFC97028F3E44082ACC5CBEC0056301136C008@ICTS-S-MBX13.luna.kuleuven.be>
Message-ID: <A4E5A0B016B8CB41A485FC629B633CED592F87E46C@GOLD.corp.lgc-group.com>



> -----Original Message-----
> return(  sqrt( sum(c(modelglmer at resid, modelglmer at u) ^2) / n ) ) }
...

> However, if I try this in the new version I get the error
> disp_glmer(fit3) #looks fine (0,998)
> Error in disp_glmer(fit3) :
>   no slot of name "resid" for this object of class "glmerMod"

Try residuals(modelglmer) instead of modelglmer at resid ?

S Ellison



*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From bbolker at gmail.com  Fri Feb  7 16:10:18 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 7 Feb 2014 15:10:18 +0000 (UTC)
Subject: [R-sig-ME] bug/problem with update for merMod
References: <0566E17B6DEC62459078112371B7508E12FD31@ait-pex02mbx05.win.dtu.dk>
Message-ID: <loom.20140207T155900-25@post.gmane.org>

Alexandra Kuznetsova <alku at ...> writes:

> 
> Dear all,
> 
> I have an issue with update.merMod function. I would like
> to call it within a function and to send some extra
> parameters to it.
> The code below fails with an error:
> 
updateModel <- function(model, reml, l)
{
 
   nfit <- update(object=model, formula.=as.formula(paste(".~."))
                  , REML=reml ,contrasts=l, evaluate=FALSE)
   env <- environment(formula(model))
   nfit <- eval(nfit, envir = env)
   #nfit <- eval.parent(nfit)
   #nfit <- eval(nfit, parent.frame())
 }
 
> m <- lmer(Reaction ~ Days +(1|Subject), data=sleepstudy)
> updateModel(m, FALSE, "contr.SAS")
> 
> Error in model.matrix.default(fixedform, fr, contrasts) : object 'l' not found
> 
> eval.parent(nfit) fails as well.
> 
> If I use evaluate=TRUE, then there is no problem, 
> but I need to use evaluate=FALSE
> 
> > sessionInfo()
> R version 3.0.2 (2013-09-25)
> Platform: x86_64-w64-mingw32/x64 (64-bit)
> 
[snip]

> other attached packages:
> [1] lme4_1.1-2   Matrix_1.1-0
> 
 
[snip]

> Is there some way to overcome this problem/bug?
> 
> Thank you in advance for any suggestion!
> 
> With best regards,
> Alexandra
> 

  It's not at all surprising that update fails when you do fancy
things with environments: it's a big headache trying to keep everything in
sync.  If you get a chance could you post this at 
https://github.com/lme4/lme4/issues ?

  Ben Bolker


From bbolker at gmail.com  Fri Feb  7 16:21:57 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 7 Feb 2014 15:21:57 +0000 (UTC)
Subject: [R-sig-ME] bug/problem with update for merMod
References: <0566E17B6DEC62459078112371B7508E12FD31@ait-pex02mbx05.win.dtu.dk>
Message-ID: <loom.20140207T162041-816@post.gmane.org>

Alexandra Kuznetsova <alku at ...> writes:

> 
> Dear all,
> 
> I have an issue with update.merMod function.  
>I would like to call it within a function and to send some extra
> parameters to it.
> The code below fails with an error:
> 
> updateModel <- function(model, reml, l)
> {
> 
>   nfit <- update(object=model, formula.=as.formula(paste(".~."))
>                  , REML=reml ,contrasts=l, evaluate=FALSE)
>   env <- environment(formula(model))
>   nfit <- eval(nfit, envir = env)
>   #nfit <- eval.parent(nfit)
>   #nfit <- eval(nfit, parent.frame())
> }
> 
> m <- lmer(Reaction ~ Days +(1|Subject), data=sleepstudy)
> updateModel(m, FALSE, "contr.SAS")
> 
> Error in model.matrix.default(fixedform, fr, contrasts) : object 'l' not found
> 
> eval.parent(nfit) fails as well.
> 

  This works (for me) if I copy the new arguments into the environment
of the formula:

updateModel <- function(model, reml, l)
{

  nfit <- update(object=model, formula.=as.formula(paste(".~."))
                 , REML=reml ,contrasts=l, evaluate=FALSE)
  env <- environment(formula(model))
  assign("l",l,envir=env)
  assign("reml",reml,envir=env)
  eval(nfit, envir = env)
}

library(lme4)
m <- lmer(Reaction ~ Days +(1|Subject), data=sleepstudy)
updateModel(m, FALSE, "contr.SAS")


From A.Varah at pgr.reading.ac.uk  Fri Feb  7 17:06:12 2014
From: A.Varah at pgr.reading.ac.uk (Alexa Varah)
Date: Fri, 7 Feb 2014 16:06:12 +0000
Subject: [R-sig-ME] MuMin package - use of AICc or QAICc?
Message-ID: <b9bbe586374144208a7a061647591e15@DBXPR01MB080.eurprd01.prod.exchangelabs.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140207/146dc94f/attachment.pl>

From Tom.Wenseleers at bio.kuleuven.be  Fri Feb  7 17:20:02 2014
From: Tom.Wenseleers at bio.kuleuven.be (Tom Wenseleers)
Date: Fri, 7 Feb 2014 16:20:02 +0000
Subject: [R-sig-ME] Checking overdispersion in binomial mixed model
In-Reply-To: <A4E5A0B016B8CB41A485FC629B633CED592F87E46C@GOLD.corp.lgc-group.com>
References: <37EFC97028F3E44082ACC5CBEC0056301136C008@ICTS-S-MBX13.luna.kuleuven.be>
	<A4E5A0B016B8CB41A485FC629B633CED592F87E46C@GOLD.corp.lgc-group.com>
Message-ID: <37EFC97028F3E44082ACC5CBEC0056301136C396@ICTS-S-MBX13.luna.kuleuven.be>

Ha, many thanks!
Just saw there is also the code on the http://glmm.wikidot.com/faq I'll probably used that one then!

Cheers & thanks!
Tom

-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of S Ellison
Sent: 07 February 2014 11:13
To: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Checking overdispersion in binomial mixed model



> -----Original Message-----
> return(  sqrt( sum(c(modelglmer at resid, modelglmer at u) ^2) / n ) ) }
...

> However, if I try this in the new version I get the error
> disp_glmer(fit3) #looks fine (0,998)
> Error in disp_glmer(fit3) :
>   no slot of name "resid" for this object of class "glmerMod"

Try residuals(modelglmer) instead of modelglmer at resid ?

S Ellison



*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:6}}


From juan.santos at ti.bund.de  Fri Feb  7 18:32:55 2014
From: juan.santos at ti.bund.de (Juan Santos)
Date: Fri, 07 Feb 2014 18:32:55 +0100
Subject: [R-sig-ME] =?utf-8?q?random_slope_model_overinflate_CI=C2=B4s?=
Message-ID: <52F518C7.60308@ti.bund.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140207/d7aa53f1/attachment.pl>

From rlevy at ucsd.edu  Fri Feb  7 20:55:43 2014
From: rlevy at ucsd.edu (Levy, Roger)
Date: Fri, 7 Feb 2014 19:55:43 +0000
Subject: [R-sig-ME] installing lme4.0 on OS X 10.9
Message-ID: <E666E3F3-5114-452E-8E31-28B19D45005B@ucsd.edu>

Hi all,

My Mac laptop died and I am now reinstalling all my packages from scratch.  Previously I was using lme4_0.999999-2 and I figure that with a fresh install, I might as well install lme4.0 instead.  But I am having trouble compiling it:

> install.packages("lme4.0", repos="http://r-forge.r-project.org",type="source")
trying URL 'http://r-forge.r-project.org/src/contrib/lme4.0_0.999999-4.tar.gz'
Content type 'application/x-gzip' length 772384 bytes (754 Kb)
opened URL
==================================================
downloaded 754 Kb

* installing *source* package ?lme4.0? ...
** libs
llvm-gcc-4.2 -arch x86_64 -std=gnu99 -I/Library/Frameworks/R.framework/Resources/include -DNDEBUG  -I/usr/local/include -I"/Library/Frameworks/R.framework/Versions/3.0/Resources/library/Matrix/include" -I"/Library/Frameworks/R.framework/Versions/3.0/Resources/library/stats/include"   -fPIC  -mtune=core2 -g -O2  -c init.c -o init.o
/bin/sh: llvm-gcc-4.2: command not found


This happens even after I install Xcode and also llvm-gcc42 on MacPorts; 

Does anyone have any suggestions?  FWIW:

> sessionInfo()
R version 3.0.2 (2013-09-25)
Platform: x86_64-apple-darwin10.8.0 (64-bit)

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

loaded via a namespace (and not attached):
[1] tools_3.0.2


I?ve also tried installing from the command line; this bottoms out in a different error:

$ R CMD build lme4.0
?
<snip>
?
ld: warning: directory not found for option ?-L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3/x86_64?
ld: warning: directory not found for option ?-L/usr/local/lib/x86_64?
ld: warning: directory not found for option ?-L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3?
ld: library not found for -lgfortran
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make: *** [lme4.so] Error 1


More generally, I?m having trouble figuring out what I should install in order to be able to compile lme4.0 from source.

Best & many thanks in advance for any guidance,

Roger Levy


From tobias.heed.uhh at gmail.com  Fri Feb  7 21:06:53 2014
From: tobias.heed.uhh at gmail.com (Tobias Heed)
Date: Fri, 7 Feb 2014 21:06:53 +0100
Subject: [R-sig-ME] installing lme4.0 on OS X 10.9
In-Reply-To: <E666E3F3-5114-452E-8E31-28B19D45005B@ucsd.edu>
References: <E666E3F3-5114-452E-8E31-28B19D45005B@ucsd.edu>
Message-ID: <etPan.52f53cdd.6b8b4567.2e5a@Macintosh.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140207/09dfd0f9/attachment.pl>

From Tom.Wenseleers at bio.kuleuven.be  Fri Feb  7 21:34:04 2014
From: Tom.Wenseleers at bio.kuleuven.be (Tom Wenseleers)
Date: Fri, 7 Feb 2014 20:34:04 +0000
Subject: [R-sig-ME] Slight differences in fitted coefficients in lme4_1.0-6
 compared to lme4_0.999999-2
Message-ID: <37EFC97028F3E44082ACC5CBEC0056301136C4CA@ICTS-S-MBX13.luna.kuleuven.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140207/3371cced/attachment.pl>

From bbolker at gmail.com  Fri Feb  7 21:51:44 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 07 Feb 2014 15:51:44 -0500
Subject: [R-sig-ME] Slight differences in fitted coefficients in
 lme4_1.0-6 compared to lme4_0.999999-2
In-Reply-To: <37EFC97028F3E44082ACC5CBEC0056301136C4CA@ICTS-S-MBX13.luna.kuleuven.be>
References: <37EFC97028F3E44082ACC5CBEC0056301136C4CA@ICTS-S-MBX13.luna.kuleuven.be>
Message-ID: <52F54760.7010608@gmail.com>

On 14-02-07 03:34 PM, Tom Wenseleers wrote:
> Dear all, I noticed that I get very slight differences in my current
> lme4 1.0-6 models compared to the old ones I obtained earlier using
> lme4_0.999999-2. I was just wondering whether it would somehow still
> be possible to reproduce the output of the old lme4_0.999999-2, by
> setting appropriate options of the optimizer to use etc? Or is this
> not possible? I also tried installing the old lme4 version using
> install_url in package devel, but if I try this I get a complaint
> that the old version doesn't work with R.0.2. Any easy way to go back
> to the old version (I need this to be able to fully reproduce
> published results)?
> 

  I think you should be able to install lme4.0 from
http://lme4.r-forge.r-project.org/repos/  to reproduce previous outputs.
 You *might* be able to reproduce previous results by setting
control=lmerControl(optimizer="optimx",optCtrl=list(method="nlminb")),
but I don't think we could guarantee that -- too much of the internal
machinery has changed too radically.

  Ben Bolker



> Cheers, Tom
> 
> 
> 
> [[alternative HTML version deleted]]
> 
> _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From jake987722 at hotmail.com  Fri Feb  7 23:09:53 2014
From: jake987722 at hotmail.com (Jake Westfall)
Date: Fri, 7 Feb 2014 15:09:53 -0700
Subject: [R-sig-ME] Slight differences in fitted coefficients in
 lme4_1.0-6 compared to lme4_0.999999-2
In-Reply-To: <52F54760.7010608@gmail.com>
References: <37EFC97028F3E44082ACC5CBEC0056301136C4CA@ICTS-S-MBX13.luna.kuleuven.be>,
	<52F54760.7010608@gmail.com>
Message-ID: <BAY172-W49969076C724A3AC48B335CB970@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140207/18e908e3/attachment.pl>

From Tom.Wenseleers at bio.kuleuven.be  Fri Feb  7 23:31:49 2014
From: Tom.Wenseleers at bio.kuleuven.be (Tom Wenseleers)
Date: Fri, 7 Feb 2014 22:31:49 +0000
Subject: [R-sig-ME] Slight differences in fitted coefficients in
 lme4_1.0-6 compared to lme4_0.999999-2
In-Reply-To: <BAY172-W49969076C724A3AC48B335CB970@phx.gbl>
References: <37EFC97028F3E44082ACC5CBEC0056301136C4CA@ICTS-S-MBX13.luna.kuleuven.be>,
	<52F54760.7010608@gmail.com>
	<BAY172-W49969076C724A3AC48B335CB970@phx.gbl>
Message-ID: <37EFC97028F3E44082ACC5CBEC0056301136C595@ICTS-S-MBX13.luna.kuleuven.be>

One thing I noticed is that the intercepts of my models seem quite different from what I got before. Is the new version not using dummy coding by default or something? Or where could that come from? @Jake: how do I specify they bobyqa optimizer actually?

Cheers,
Tom

-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Jake Westfall
Sent: 07 February 2014 20:10
To: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Slight differences in fitted coefficients in lme4_1.0-6 compared to lme4_0.999999-2

Not sure if this thread is the time/place for me to bring this up, but here goes... I *routinely* find that the new Nelder-Mead optimizer in lme4 >= 1.0 provides worse solutions than the old bobyqa optimizer -- "worse" in the sense that, comparing the same model fitted to the same dataset using NM vs. bobyqa, the coefficients are noticeably different and deviance for the former model is noticeably higher. When I switch to bobyqa I pretty much reproduce the results of my models fitted under lme4 < 1.0... and bobyqa is faster too! At this point, I've gotten to where I just always instruct lme4 to use bobyqa and don't even check anymore to see what Nelder-Mead comes up with. One very important thing to mention here is that the overwhelming majority of models that I fit involve crossed random effects. So maybe the new Nelder-Mead optimizer fairly consistently outperforms bobyqa for nested random effects models, and this is the motivation for making it the new lme4 default, but i!
 n my experience, for the kind of models that I fit, bobyqa pretty much always does better.

Jake

> Date: Fri, 7 Feb 2014 15:51:44 -0500
> From: bbolker at gmail.com
> To: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Slight differences in fitted coefficients in 
> lme4_1.0-6 compared to lme4_0.999999-2
> 
> On 14-02-07 03:34 PM, Tom Wenseleers wrote:
> > Dear all, I noticed that I get very slight differences in my current
> > lme4 1.0-6 models compared to the old ones I obtained earlier using 
> > lme4_0.999999-2. I was just wondering whether it would somehow still 
> > be possible to reproduce the output of the old lme4_0.999999-2, by 
> > setting appropriate options of the optimizer to use etc? Or is this 
> > not possible? I also tried installing the old lme4 version using 
> > install_url in package devel, but if I try this I get a complaint 
> > that the old version doesn't work with R.0.2. Any easy way to go 
> > back to the old version (I need this to be able to fully reproduce 
> > published results)?
> > 
> 
>   I think you should be able to install lme4.0 from 
> http://lme4.r-forge.r-project.org/repos/  to reproduce previous outputs.
>  You *might* be able to reproduce previous results by setting 
> control=lmerControl(optimizer="optimx",optCtrl=list(method="nlminb")),
> but I don't think we could guarantee that -- too much of the internal 
> machinery has changed too radically.
> 
>   Ben Bolker
> 
> 
> 
> > Cheers, Tom
> > 
> > 
> > 
> > [[alternative HTML version deleted]]
> > 
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
 		 	   		  
	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From steve.walker at utoronto.ca  Sat Feb  8 00:22:55 2014
From: steve.walker at utoronto.ca (Steve Walker)
Date: Fri, 07 Feb 2014 18:22:55 -0500
Subject: [R-sig-ME] Slight differences in fitted coefficients in
 lme4_1.0-6 compared to lme4_0.999999-2
In-Reply-To: <37EFC97028F3E44082ACC5CBEC0056301136C595@ICTS-S-MBX13.luna.kuleuven.be>
References: <37EFC97028F3E44082ACC5CBEC0056301136C4CA@ICTS-S-MBX13.luna.kuleuven.be>,
	<52F54760.7010608@gmail.com>	<BAY172-W49969076C724A3AC48B335CB970@phx.gbl>
	<37EFC97028F3E44082ACC5CBEC0056301136C595@ICTS-S-MBX13.luna.kuleuven.be>
Message-ID: <52F56ACF.8040604@utoronto.ca>

The development version of lme4 on github now has the default optimizer 
switched back to bobyqa.  To explicitly set the optimizer, use something 
like:

fm1 <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy,
             control = lmerControl(optimizer = "bobyqa"))


Cheers,
Steve


On 2/7/2014, 5:31 PM, Tom Wenseleers wrote:
> One thing I noticed is that the intercepts of my models seem quite different from what I got before. Is the new version not using dummy coding by default or something? Or where could that come from? @Jake: how do I specify they bobyqa optimizer actually?
>
> Cheers,
> Tom
>
> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Jake Westfall
> Sent: 07 February 2014 20:10
> To: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Slight differences in fitted coefficients in lme4_1.0-6 compared to lme4_0.999999-2
>
> Not sure if this thread is the time/place for me to bring this up, but here goes... I *routinely* find that the new Nelder-Mead optimizer in lme4 >= 1.0 provides worse solutions than the old bobyqa optimizer -- "worse" in the sense that, comparing the same model fitted to the same dataset using NM vs. bobyqa, the coefficients are noticeably different and deviance for the former model is noticeably higher. When I switch to bobyqa I pretty much reproduce the results of my models fitted under lme4 < 1.0... and bobyqa is faster too! At this point, I've gotten to where I just always instruct lme4 to use bobyqa and don't even check anymore to see what Nelder-Mead comes up with. One very important thing to mention here is that the overwhelming majority of models that I fit involve crossed random effects. So maybe the new Nelder-Mead optimizer fairly consistently outperforms bobyqa for nested random effects models, and this is the motivation for making it the new lme4 default, but!
  i!
>   n my experience, for the kind of models that I fit, bobyqa pretty much always does better.
>
> Jake
>
>> Date: Fri, 7 Feb 2014 15:51:44 -0500
>> From: bbolker at gmail.com
>> To: r-sig-mixed-models at r-project.org
>> Subject: Re: [R-sig-ME] Slight differences in fitted coefficients in
>> lme4_1.0-6 compared to lme4_0.999999-2
>>
>> On 14-02-07 03:34 PM, Tom Wenseleers wrote:
>>> Dear all, I noticed that I get very slight differences in my current
>>> lme4 1.0-6 models compared to the old ones I obtained earlier using
>>> lme4_0.999999-2. I was just wondering whether it would somehow still
>>> be possible to reproduce the output of the old lme4_0.999999-2, by
>>> setting appropriate options of the optimizer to use etc? Or is this
>>> not possible? I also tried installing the old lme4 version using
>>> install_url in package devel, but if I try this I get a complaint
>>> that the old version doesn't work with R.0.2. Any easy way to go
>>> back to the old version (I need this to be able to fully reproduce
>>> published results)?
>>>
>>
>>    I think you should be able to install lme4.0 from
>> http://lme4.r-forge.r-project.org/repos/  to reproduce previous outputs.
>>   You *might* be able to reproduce previous results by setting
>> control=lmerControl(optimizer="optimx",optCtrl=list(method="nlminb")),
>> but I don't think we could guarantee that -- too much of the internal
>> machinery has changed too radically.
>>
>>    Ben Bolker
>>
>>
>>
>>> Cheers, Tom
>>>
>>>
>>>
>>> [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>   		 	   		
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>


From Tom.Wenseleers at bio.kuleuven.be  Sat Feb  8 00:24:07 2014
From: Tom.Wenseleers at bio.kuleuven.be (Tom Wenseleers)
Date: Fri, 7 Feb 2014 23:24:07 +0000
Subject: [R-sig-ME] Slight differences in fitted coefficients in
 lme4_1.0-6 compared to lme4_0.999999-2
In-Reply-To: <BAY172-W49969076C724A3AC48B335CB970@phx.gbl>
References: <37EFC97028F3E44082ACC5CBEC0056301136C4CA@ICTS-S-MBX13.luna.kuleuven.be>,
	<52F54760.7010608@gmail.com>
	<BAY172-W49969076C724A3AC48B335CB970@phx.gbl>
Message-ID: <37EFC97028F3E44082ACC5CBEC0056301136C605@ICTS-S-MBX13.luna.kuleuven.be>

Hi Jake,
So is setting control=lmerControl(optimizer="bobyqa") enough then?

Cheers,Tom

-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Jake Westfall
Sent: 07 February 2014 20:10
To: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Slight differences in fitted coefficients in lme4_1.0-6 compared to lme4_0.999999-2

Not sure if this thread is the time/place for me to bring this up, but here goes... I *routinely* find that the new Nelder-Mead optimizer in lme4 >= 1.0 provides worse solutions than the old bobyqa optimizer -- "worse" in the sense that, comparing the same model fitted to the same dataset using NM vs. bobyqa, the coefficients are noticeably different and deviance for the former model is noticeably higher. When I switch to bobyqa I pretty much reproduce the results of my models fitted under lme4 < 1.0... and bobyqa is faster too! At this point, I've gotten to where I just always instruct lme4 to use bobyqa and don't even check anymore to see what Nelder-Mead comes up with. One very important thing to mention here is that the overwhelming majority of models that I fit involve crossed random effects. So maybe the new Nelder-Mead optimizer fairly consistently outperforms bobyqa for nested random effects models, and this is the motivation for making it the new lme4 default, but i!
 n my experience, for the kind of models that I fit, bobyqa pretty much always does better.

Jake

> Date: Fri, 7 Feb 2014 15:51:44 -0500
> From: bbolker at gmail.com
> To: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Slight differences in fitted coefficients in 
> lme4_1.0-6 compared to lme4_0.999999-2
> 
> On 14-02-07 03:34 PM, Tom Wenseleers wrote:
> > Dear all, I noticed that I get very slight differences in my current
> > lme4 1.0-6 models compared to the old ones I obtained earlier using 
> > lme4_0.999999-2. I was just wondering whether it would somehow still 
> > be possible to reproduce the output of the old lme4_0.999999-2, by 
> > setting appropriate options of the optimizer to use etc? Or is this 
> > not possible? I also tried installing the old lme4 version using 
> > install_url in package devel, but if I try this I get a complaint 
> > that the old version doesn't work with R.0.2. Any easy way to go 
> > back to the old version (I need this to be able to fully reproduce 
> > published results)?
> > 
> 
>   I think you should be able to install lme4.0 from 
> http://lme4.r-forge.r-project.org/repos/  to reproduce previous outputs.
>  You *might* be able to reproduce previous results by setting 
> control=lmerControl(optimizer="optimx",optCtrl=list(method="nlminb")),
> but I don't think we could guarantee that -- too much of the internal 
> machinery has changed too radically.
> 
>   Ben Bolker
> 
> 
> 
> > Cheers, Tom
> > 
> > 
> > 
> > [[alternative HTML version deleted]]
> > 
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
 		 	   		  
	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From ukoether at uke.de  Sat Feb  8 00:26:17 2014
From: ukoether at uke.de (=?ISO-8859-1?Q?Ulf_K=F6ther?=)
Date: Sat, 8 Feb 2014 00:26:17 +0100
Subject: [R-sig-ME] Slight differences in fitted coefficients in
 lme4_1.0-6 compared to lme4_0.999999-2
In-Reply-To: <BAY172-W49969076C724A3AC48B335CB970@phx.gbl>
References: <37EFC97028F3E44082ACC5CBEC0056301136C4CA@ICTS-S-MBX13.luna.kuleuven.be>,
	<52F54760.7010608@gmail.com>
	<BAY172-W49969076C724A3AC48B335CB970@phx.gbl>
Message-ID: <52F56B99.1070707@uke.de>

@ Jake: I am not in any sense a statistician nor programmer and hence no
reference, but I can just support this observation. I often have noisy
data which in 90% of the cases cannot be fit using NM (non-convergence
although with many iterations) but which is consistently dealt with by
using bobyqa (with an equal amount of iterations)...
And I do not have the impression that its results seem to be inaccurate,
but that NM gets stuck in many situations.

Ulf

Am 07.02.2014 23:09, schrieb Jake Westfall:
> Not sure if this thread is the time/place for me to bring this up, but here goes... I *routinely* find that the new Nelder-Mead optimizer in lme4 >= 1.0 provides worse solutions than the old bobyqa optimizer -- "worse" in the sense that, comparing the same model fitted to the same dataset using NM vs. bobyqa, the coefficients are noticeably different and deviance for the former model is noticeably higher. When I switch to bobyqa I pretty much reproduce the results of my models fitted under lme4 < 1.0... and bobyqa is faster too! At this point, I've gotten to where I just always instruct lme4 to use bobyqa and don't even check anymore to see what Nelder-Mead comes up with. One very important thing to mention here is that the overwhelming majority of models that I fit involve crossed random effects. So maybe the new Nelder-Mead optimizer fairly consistently outperforms bobyqa for nested random effects models, and this is the motivation for making it the new lme4 default, but i!
>  n my experience, for the kind of models that I fit, bobyqa pretty much always does better.
>
> Jake
>
>> Date: Fri, 7 Feb 2014 15:51:44 -0500
>> From: bbolker at gmail.com
>> To: r-sig-mixed-models at r-project.org
>> Subject: Re: [R-sig-ME] Slight differences in fitted coefficients in lme4_1.0-6 compared to lme4_0.999999-2
>>
>> On 14-02-07 03:34 PM, Tom Wenseleers wrote:
>>> Dear all, I noticed that I get very slight differences in my current
>>> lme4 1.0-6 models compared to the old ones I obtained earlier using
>>> lme4_0.999999-2. I was just wondering whether it would somehow still
>>> be possible to reproduce the output of the old lme4_0.999999-2, by
>>> setting appropriate options of the optimizer to use etc? Or is this
>>> not possible? I also tried installing the old lme4 version using
>>> install_url in package devel, but if I try this I get a complaint
>>> that the old version doesn't work with R.0.2. Any easy way to go back
>>> to the old version (I need this to be able to fully reproduce
>>> published results)?
>>>
>>   I think you should be able to install lme4.0 from
>> http://lme4.r-forge.r-project.org/repos/  to reproduce previous outputs.
>>  You *might* be able to reproduce previous results by setting
>> control=lmerControl(optimizer="optimx",optCtrl=list(method="nlminb")),
>> but I don't think we could guarantee that -- too much of the internal
>> machinery has changed too radically.
>>
>>   Ben Bolker
>>
>>
>>
>>> Cheers, Tom
>>>
>>>
>>>
>>> [[alternative HTML version deleted]]
>>>
>>> _______________________________________________ 
>>> R-sig-mixed-models at r-project.org mailing list 
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>  		 	   		  
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> .
>

-- 
________________________________________

Dipl.-Psych. Ulf K?ther

PEPP-Team & AG Klinische Neuropsychologie
Klinik f?r Psychiatrie und Psychotherapie
Universit?tsklinikum Hamburg-Eppendorf
Martinistr. 52
20246 Hamburg

PEPP-Team:
Tel.: +49 (0) 40 7410 53243
pepp at uke.de

Pers?nlich:
Tel.: +49 (0) 40 7410 55851
Mobil: (9) 55851
ukoether at uke.de

AG Klinische Neuropsychologie:
Fax:  +49 (0) 40 7410 57566
www.uke.de/neuropsych
________________________________________


--

Besuchen Sie uns auf: www.uke.de
_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg
Vorstandsmitglieder: Prof. Dr. Christian Gerloff (Vertreter des Vorsitzenden), Prof. Dr. Dr. Uwe Koch-Gromus, Joachim Pr?l?, Rainer Schoppik
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING


From bbolker at gmail.com  Sat Feb  8 02:57:18 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 07 Feb 2014 20:57:18 -0500
Subject: [R-sig-ME] Slight differences in fitted coefficients in
 lme4_1.0-6 compared to lme4_0.999999-2
In-Reply-To: <52F56B99.1070707@uke.de>
References: <37EFC97028F3E44082ACC5CBEC0056301136C4CA@ICTS-S-MBX13.luna.kuleuven.be>,
	<52F54760.7010608@gmail.com>	<BAY172-W49969076C724A3AC48B335CB970@phx.gbl>
	<52F56B99.1070707@uke.de>
Message-ID: <52F58EFE.60709@gmail.com>

  This is all very helpful, and reinforces our decision (mentioned by
Steve Walker) to switch to a bobyqa default in an imminent release. We
had hoped to do more systematic testing, but anecdotal evidence from
many users is better than anecdotal evidence just from the problems the
authors have stumbled across.  If there are users out there who have
encountered the *opposite* scenario (Nelder-Mead works better than
bobyqa) we'd love to hear it, but we know this is harder to detect
(because N-M is the default, it is more likely that people will notice
problems with N-M and switch to bobyqa than the opposite).

Tom: default contrasts haven't changed, so I don't know what's up with
your intercept terms.  Maybe options(contrasts=...) was set formerly?
You could check attributes(old_lme4_model at frame) (in lme4.0) or
attributes(model.frame(new_lme4_model)) (in lme4) to compare ...

If you are an experienced user it would be great if you could try the
most recent development version (via
devtools::install_github("lme4","lme4") and report unusual or
interesting results to the list ... we're particularly interested in (1)
bobyqa glitches and (2) obvious false positive warnings from the new
convergence testing code -- especially examples of singular fits that
report large gradients (more generally, any last-minute comments or
pleas for bug fixes/minor features should be reported to us soon). We
don't currently consider any of the issues at
https://github.com/lme4/lme4/issues?state=open release-critical, except
https://github.com/lme4/lme4/issues/120, which should be closed as soon
as we can convince ourselves there aren't too many false positives.


On 14-02-07 06:26 PM, Ulf K?ther wrote:
> @ Jake: I am not in any sense a statistician nor programmer and hence
> no reference, but I can just support this observation. I often have
> noisy data which in 90% of the cases cannot be fit using NM
> (non-convergence although with many iterations) but which is
> consistently dealt with by using bobyqa (with an equal amount of
> iterations)... And I do not have the impression that its results seem
> to be inaccurate, but that NM gets stuck in many situations.
> 
> Ulf
> 
> Am 07.02.2014 23:09, schrieb Jake Westfall:
>> Not sure if this thread is the time/place for me to bring this up,
>> but here goes... I *routinely* find that the new Nelder-Mead
>> optimizer in lme4 >= 1.0 provides worse solutions than the old
>> bobyqa optimizer -- "worse" in the sense that, comparing the same
>> model fitted to the same dataset using NM vs. bobyqa, the
>> coefficients are noticeably different and deviance for the former
>> model is noticeably higher. When I switch to bobyqa I pretty much
>> reproduce the results of my models fitted under lme4 < 1.0... and
>> bobyqa is faster too! At this point, I've gotten to where I just
>> always instruct lme4 to use bobyqa and don't even check anymore to
>> see what Nelder-Mead comes up with. One very important thing to
>> mention here is that the overwhelming majority of models that I fit
>> involve crossed random effects. So maybe the new Nelder-Mead
>> optimizer fairly consistently outperforms bobyqa for nested random
>> effects models, and this is the motivation for making it the new
>> lme4 default, but i! n my experience, for the kind of models that I
>> fit, bobyqa pretty much always does better.
>> 
>> Jake
>> 
>>> Date: Fri, 7 Feb 2014 15:51:44 -0500 From: bbolker at gmail.com To:
>>> r-sig-mixed-models at r-project.org Subject: Re: [R-sig-ME] Slight
>>> differences in fitted coefficients in lme4_1.0-6 compared to
>>> lme4_0.999999-2
>>> 
>>> On 14-02-07 03:34 PM, Tom Wenseleers wrote:
>>>> Dear all, I noticed that I get very slight differences in my
>>>> current lme4 1.0-6 models compared to the old ones I obtained
>>>> earlier using lme4_0.999999-2. I was just wondering whether it
>>>> would somehow still be possible to reproduce the output of the
>>>> old lme4_0.999999-2, by setting appropriate options of the
>>>> optimizer to use etc? Or is this not possible? I also tried
>>>> installing the old lme4 version using install_url in package
>>>> devel, but if I try this I get a complaint that the old version
>>>> doesn't work with R.0.2. Any easy way to go back to the old
>>>> version (I need this to be able to fully reproduce published
>>>> results)?
>>>> 
>>> I think you should be able to install lme4.0 from 
>>> http://lme4.r-forge.r-project.org/repos/  to reproduce previous
>>> outputs. You *might* be able to reproduce previous results by
>>> setting 
>>> control=lmerControl(optimizer="optimx",optCtrl=list(method="nlminb")),
>>>
>>> 
but I don't think we could guarantee that -- too much of the internal
>>> machinery has changed too radically.
>>> 
>>> Ben Bolker
>>> 
>>> 
>>> 
>>>> Cheers, Tom
>>>> 
>>>> 
>>>> 
>>>> [[alternative HTML version deleted]]
>>>> 
>>>> _______________________________________________ 
>>>> R-sig-mixed-models at r-project.org mailing list 
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>> 
>>> _______________________________________________ 
>>> R-sig-mixed-models at r-project.org mailing list 
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>  [[alternative HTML version deleted]]
>> 
>> _______________________________________________ 
>> R-sig-mixed-models at r-project.org mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models .
>> 
>


From David.Duffy at qimr.edu.au  Sat Feb  8 10:06:23 2014
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Sat, 8 Feb 2014 19:06:23 +1000
Subject: [R-sig-ME] pedigreemm number of levels per grouping factor
In-Reply-To: <DBBDA54CB159CC41A0A01219B109A4D001BBF2C1@VMEXCHANGEMBS5A.isad.isadroot.ex.ac.uk>
References: <DBBDA54CB159CC41A0A01219B109A4D001BBD248@VMEXCHANGEMBS5A.isad.isadroot.ex.ac.uk><52F28469.9010203@gmail.com>
	<DBBDA54CB159CC41A0A01219B109A4D001BBF06C@VMEXCHANGEMBS5A.isad.isadroot.ex.ac.uk>
	<alpine.LMD.2.00.1402071324230.9819@orpheus.qimr.edu.au>
	<DBBDA54CB159CC41A0A01219B109A4D001BBF2C1@VMEXCHANGEMBS5A.isad.isadroot.ex.ac.uk>
Message-ID: <alpine.LMD.2.00.1402081824430.19877@orpheus.qimr.edu.au>

On Fri, 7 Feb 2014, Wilson, Alastair wrote:

> Thanks - that's v much appreciated. Data file and pedigree structures 
> attached.

OK, runs fine with older versions of pedigreemm and lmer

Linear mixed model fit by REML
Formula: size ~ sex + forage + (1 | ID)
    Data: pheno
   AIC  BIC logLik deviance REMLdev
  1160 1180 -574.8     1143    1150
Random effects:
  Groups   Name        Variance Std.Dev.
  ID       (Intercept) 0.35597  0.59663
  Residual             0.73231  0.85575
Number of obs: 400, groups: ID, 400

Fixed effects:
             Estimate Std. Error t value
(Intercept)  5.29569    0.24255  21.833
sex          0.14436    0.09604   1.503
forage       0.88907    0.30644   2.901

Correlation of Fixed Effects:
        (Intr) sex
sex    -0.594
forage -0.717  0.000

The newest version of pedigreemm is failing in pedigreemm::relfactor(), 
which is supposed to produce the Cholesky factor of the relationship 
matrix, where the error arises in:

solve(t(as(ped,"sparseMatrix")),
       as(factor(labs, levels = ped at label),"sparseMatrix"))

specifically in your exampe, NRM is 500*500 but labs is 400*1 (100 
unphenotyped founders)

Cheers, David Duffy.

| David Duffy (MBBS PhD)
| email: David.Duffy at qimrberghofer.edu.au  ph: INT+61+7+3362-0217 fax: -0101
| Genetic Epidemiology, QIMR Berghofer Institute of Medical Research
| 300 Herston Rd, Brisbane, Queensland 4006, Australia  GPG 4D0B994A


From rlevy at ucsd.edu  Sun Feb  9 23:41:46 2014
From: rlevy at ucsd.edu (Levy, Roger)
Date: Sun, 9 Feb 2014 22:41:46 +0000
Subject: [R-sig-ME] installing lme4.0 on OS X 10.9
In-Reply-To: <etPan.52f53cdd.6b8b4567.2e5a@Macintosh.local>
References: <E666E3F3-5114-452E-8E31-28B19D45005B@ucsd.edu>
	<etPan.52f53cdd.6b8b4567.2e5a@Macintosh.local>
Message-ID: <FCC7C722-0BB8-4137-AF6E-D9D7E9D836A8@ucsd.edu>

Thanks so much, Tobias.  Unfortunately, setting up a Makevars file like you suggested simply leaves me with the same errors in R as I get in the command line:

> install.packages("lme4.0", repos="http://r-forge.r-project.org",type="source")
?
<snip>
?
clang -dynamiclib -Wl,-headerpad_max_install_names  -undefined dynamic_lookup -single_module -multiply_defined suppress -L/usr/local/lib -L/usr/local/lib -o lme4.so init.o lmer.o local_stubs.o -L/Library/Frameworks/R.framework/Resources/lib -lRlapack -L/Library/Frameworks/R.framework/Resources/lib -lRblas -L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3/x86_64 -L/usr/local/lib/x86_64 -L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3 -lgfortran  -F/Library/Frameworks/R.framework/.. -framework R -Wl,-framework -Wl,CoreFoundation
?
ld: warning: directory not found for option '-L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3/x86_64'
ld: warning: directory not found for option '-L/usr/local/lib/x86_64'
ld: warning: directory not found for option '-L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3'
ld: library not found for -lgfortran
clang: error: linker command failed with exit code 1 (use -v to see invocation)


Any further thoughts or suggestions on this, anyone?

Best

Roger

On Feb 7, 2014, at 12:06 PM, Tobias Heed <tobias.heed.uhh at gmail.com> wrote:

> Dear Roger,
> 
> if I remember correctly, this solved the problem for me:
> on the top level of your user account, create a folder .R (if it?s not there yet);
> in there, create a new text file named Makevars;
> into this file, paste the following:
> 
> CC=clang
> CXX=clang++ -arch x86_64 -ftemplate-depth-256 -stdlib=libstdc++
> CXXFLAGS= -O3 -pedantic
> 
> hope it helps
> Best,
> Tobias
> 
> 
> From: Levy, Roger rlevy at ucsd.edu
> Reply: Levy, Roger rlevy at ucsd.edu
> Date: February 7,2014 at 20:59:11
> To: r-sig-mixed-models at r-project.org r-sig-mixed-models at r-project.org
> Subject:  [R-sig-ME] installing lme4.0 on OS X 10.9 
>> Hi all, 
>> 
>> My Mac laptop died and I am now reinstalling all my packages from scratch. Previously I was using lme4_0.999999-2 and I figure that with a fresh install, I might as well install lme4.0 instead. But I am having trouble compiling it: 
>> 
>> > install.packages("lme4.0", repos="http://r-forge.r-project.org",type="source") 
>> trying URL 'http://r-forge.r-project.org/src/contrib/lme4.0_0.999999-4.tar.gz' 
>> Content type 'application/x-gzip' length 772384 bytes (754 Kb) 
>> opened URL 
>> ================================================== 
>> downloaded 754 Kb 
>> 
>> * installing *source* package ?lme4.0? ... 
>> ** libs 
>> llvm-gcc-4.2 -arch x86_64 -std=gnu99 -I/Library/Frameworks/R.framework/Resources/include -DNDEBUG -I/usr/local/include -I"/Library/Frameworks/R.framework/Versions/3.0/Resources/library/Matrix/include" -I"/Library/Frameworks/R.framework/Versions/3.0/Resources/library/stats/include" -fPIC -mtune=core2 -g -O2 -c init.c -o init.o 
>> /bin/sh: llvm-gcc-4.2: command not found 
>> 
>> 
>> This happens even after I install Xcode and also llvm-gcc42 on MacPorts; 
>> 
>> Does anyone have any suggestions? FWIW: 
>> 
>> > sessionInfo() 
>> R version 3.0.2 (2013-09-25) 
>> Platform: x86_64-apple-darwin10.8.0 (64-bit) 
>> 
>> locale: 
>> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 
>> 
>> attached base packages: 
>> [1] stats graphics grDevices utils datasets methods base 
>> 
>> loaded via a namespace (and not attached): 
>> [1] tools_3.0.2 
>> 
>> 
>> I?ve also tried installing from the command line; this bottoms out in a different error: 
>> 
>> $ R CMD build lme4.0 
>> ? 
>> <snip> 
>> ? 
>> ld: warning: directory not found for option ?-L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3/x86_64? 
>> ld: warning: directory not found for option ?-L/usr/local/lib/x86_64? 
>> ld: warning: directory not found for option ?-L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3? 
>> ld: library not found for -lgfortran 
>> clang: error: linker command failed with exit code 1 (use -v to see invocation) 
>> make: *** [lme4.so] Error 1 
>> 
>> 
>> More generally, I?m having trouble figuring out what I should install in order to be able to compile lme4.0 from source. 
>> 
>> Best & many thanks in advance for any guidance, 
>> 
>> Roger Levy 
>> 
>> _______________________________________________ 
>> R-sig-mixed-models at r-project.org mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From Hugo.Mildenberger at web.de  Mon Feb 10 00:31:48 2014
From: Hugo.Mildenberger at web.de (Hugo.Mildenberger at web.de)
Date: Mon, 10 Feb 2014 00:31:48 +0100
Subject: [R-sig-ME] installing lme4.0 on OS X 10.9
In-Reply-To: <FCC7C722-0BB8-4137-AF6E-D9D7E9D836A8@ucsd.edu>
References: <E666E3F3-5114-452E-8E31-28B19D45005B@ucsd.edu>
	<etPan.52f53cdd.6b8b4567.2e5a@Macintosh.local>
	<FCC7C722-0BB8-4137-AF6E-D9D7E9D836A8@ucsd.edu>
Message-ID: <20140210003148.58ad495406adfc0b4bc4c506@zotac.lan>

Roger,

the file lme4/src/Makevars contains the line

     PKG_LIBS = `$(R_HOME)/bin/Rscript -e "Rcpp:::LdFlags()"`

so reinstalling Rcpp might help? libgfortran.so comes with gcc if fortran support 
was configured. I believe Rcpp or some other packages had been compiled by a 
rather old version of gcc.  You may run the Rscript command manually to
see if the Rcpp libraries are part of the problem.

Best

Hugo


On Sun, 9 Feb 2014 22:41:46 +0000
"Levy, Roger" <rlevy at ucsd.edu> wrote:

> Thanks so much, Tobias.  Unfortunately, setting up a Makevars file like you suggested simply leaves me with the same errors in R as I get in the command line:
> 
> > install.packages("lme4.0", repos="http://r-forge.r-project.org",type="source")
> ?
> <snip>
> ?
> clang -dynamiclib -Wl,-headerpad_max_install_names  -undefined dynamic_lookup -single_module -multiply_defined suppress -L/usr/local/lib -L/usr/local/lib -o lme4.so init.o lmer.o local_stubs.o -L/Library/Frameworks/R.framework/Resources/lib -lRlapack -L/Library/Frameworks/R.framework/Resources/lib -lRblas -L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3/x86_64 -L/usr/local/lib/x86_64 -L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3 -lgfortran  -F/Library/Frameworks/R.framework/.. -framework R -Wl,-framework -Wl,CoreFoundation
> ?
> ld: warning: directory not found for option '-L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3/x86_64'
> ld: warning: directory not found for option '-L/usr/local/lib/x86_64'
> ld: warning: directory not found for option '-L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3'
> ld: library not found for -lgfortran
> clang: error: linker command failed with exit code 1 (use -v to see invocation)
> 
> 
> Any further thoughts or suggestions on this, anyone?
> 
> Best
> 
> Roger
> 
> On Feb 7, 2014, at 12:06 PM, Tobias Heed <tobias.heed.uhh at gmail.com> wrote:
> 
> > Dear Roger,
> > 
> > if I remember correctly, this solved the problem for me:
> > on the top level of your user account, create a folder .R (if it?s not there yet);
> > in there, create a new text file named Makevars;
> > into this file, paste the following:
> > 
> > CC=clang
> > CXX=clang++ -arch x86_64 -ftemplate-depth-256 -stdlib=libstdc++
> > CXXFLAGS= -O3 -pedantic
> > 
> > hope it helps
> > Best,
> > Tobias
> > 
> > 
> > From: Levy, Roger rlevy at ucsd.edu
> > Reply: Levy, Roger rlevy at ucsd.edu
> > Date: February 7,2014 at 20:59:11
> > To: r-sig-mixed-models at r-project.org r-sig-mixed-models at r-project.org
> > Subject:  [R-sig-ME] installing lme4.0 on OS X 10.9 
> >> Hi all, 
> >> 
> >> My Mac laptop died and I am now reinstalling all my packages from scratch. Previously I was using lme4_0.999999-2 and I figure that with a fresh install, I might as well install lme4.0 instead. But I am having trouble compiling it: 
> >> 
> >> > install.packages("lme4.0", repos="http://r-forge.r-project.org",type="source") 
> >> trying URL 'http://r-forge.r-project.org/src/contrib/lme4.0_0.999999-4.tar.gz' 
> >> Content type 'application/x-gzip' length 772384 bytes (754 Kb) 
> >> opened URL 
> >> ================================================== 
> >> downloaded 754 Kb 
> >> 
> >> * installing *source* package ?lme4.0? ... 
> >> ** libs 
> >> llvm-gcc-4.2 -arch x86_64 -std=gnu99 -I/Library/Frameworks/R.framework/Resources/include -DNDEBUG -I/usr/local/include -I"/Library/Frameworks/R.framework/Versions/3.0/Resources/library/Matrix/include" -I"/Library/Frameworks/R.framework/Versions/3.0/Resources/library/stats/include" -fPIC -mtune=core2 -g -O2 -c init.c -o init.o 
> >> /bin/sh: llvm-gcc-4.2: command not found 
> >> 
> >> 
> >> This happens even after I install Xcode and also llvm-gcc42 on MacPorts; 
> >> 
> >> Does anyone have any suggestions? FWIW: 
> >> 
> >> > sessionInfo() 
> >> R version 3.0.2 (2013-09-25) 
> >> Platform: x86_64-apple-darwin10.8.0 (64-bit) 
> >> 
> >> locale: 
> >> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 
> >> 
> >> attached base packages: 
> >> [1] stats graphics grDevices utils datasets methods base 
> >> 
> >> loaded via a namespace (and not attached): 
> >> [1] tools_3.0.2 
> >> 
> >> 
> >> I?ve also tried installing from the command line; this bottoms out in a different error: 
> >> 
> >> $ R CMD build lme4.0 
> >> ? 
> >> <snip> 
> >> ? 
> >> ld: warning: directory not found for option ?-L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3/x86_64? 
> >> ld: warning: directory not found for option ?-L/usr/local/lib/x86_64? 
> >> ld: warning: directory not found for option ?-L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3? 
> >> ld: library not found for -lgfortran 
> >> clang: error: linker command failed with exit code 1 (use -v to see invocation) 
> >> make: *** [lme4.so] Error 1 
> >> 
> >> 
> >> More generally, I?m having trouble figuring out what I should install in order to be able to compile lme4.0 from source. 
> >> 
> >> Best & many thanks in advance for any guidance, 
> >> 
> >> Roger Levy 
> >> 
> >> _______________________________________________ 
> >> R-sig-mixed-models at r-project.org mailing list 
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 


From greg.guerin at adelaide.edu.au  Mon Feb 10 01:02:17 2014
From: greg.guerin at adelaide.edu.au (Greg Guerin)
Date: Mon, 10 Feb 2014 00:02:17 +0000
Subject: [R-sig-ME] random slope model overinflate CI?s
Message-ID: <F6E2AA0CA3EC104BAD0A6D32A2863372CBBA9E7B@MAILMB04.ad.adelaide.edu.au>

Hello Juan,

I have trouble following the second half of the message, so perhaps I am missing your point, but responding to this part:

"Digging into the experimental data I realized the convenience of including  *fish_length* as random slope. After this change, the diagnostic criteria (AIC, BIC)  improved a lot, compared to the first modeling approach but the winner model only  gave the intercept as significant fixed effect:

mod45<-glmer(prop~1+(fish_length|haul),family=binomial,data=X.red,nAGQ=1,weights=ntotal)"

The reason this model gave only intercept as a fixed effect is that only intercept was specified as a fixed effect. To include as both  random and fixed, you would need to specify it twice, along the lines of a ~ b + (1+b|c)

Greg

--
Dr Greg Guerin
Postdoctoral Fellow
School of Earth and Environmental Sciences, Faculty of Science
The University of Adelaide
Level 12, Schulz Building, North Terrace Campus
greg.guerin at adelaide.edu.au
http://www.adelaide.edu.au/directory/greg.guerin

CRICOS Provider Number 00123M
-----------------------------------------------------------
IMPORTANT: This message may contain confidential or legally privileged
information. If you think it was sent to
you by mistake, please delete all copies and advise the sender. For the purposes
of the SPAM Act 2003, this
email is authorised by The University of Adelaide.


------------------------------

Date: Fri, 07 Feb 2014 18:32:55 +0100
From: Juan Santos <juan.santos at ti.bund.de>
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] random slope model overinflate CI?s
Message-ID: <52F518C7.60308 at ti.bund.de>
Content-Type: text/plain

     Hello,

I ??m dealing with a  fishing dataset obtained from a research cruise two
trawl gears were fishing in paralel (twin trawling) during each of the
fishing realizations (hauls) carried out. The response variable is the
proportion of catches in the test gear prop=test/(test+reference).
therefore:

  # prop=0.5 can be interpreted as  both  gears having the  same fishing
efficiency on a given fish species
  # prop<0.5 means therefore less catch efficiency in the test gear.

The dataset contains a total of 14 hauls balanced in two different test
*gear_setup* (setup1 (7 hauls) setup2 (7hauls)). Caught fishes were
counted and measured (*fish_length*)

The aim of the modeling approach was:

-To evaluate if fish length size or gear setup significantly affects the
response *prpp* .

Proposed model and first approach:

A mixed modeling approach using glmer(). Haul as  random intercept. The
best model of this random intercept approach was:

mod5<-glmer(prop~gear_setup*fish_length+(1|haul),family=binomial,data=X.red,nAGQ=1,weights=ntotal)

              Df    AIC    BIC  logLik deviance   Chisq Chi Df Pr(>Chisq)
mod5  4 2067.4  2083.6 -1029.7   2059.4  7.3644      0  < 2.2e-16 ***

Random effects:

  Groups Name        Variance Std.Dev.
  haul   (Intercept)      0.2131   0.4617
Number of obs: 428, groups: haul, 14

Fixed effects:
                          Estimate Std. Error z value Pr(>|z|)
(Intercept)    -3.55598    0.29711 -11.969  < 2e-16 ***
setup2           1.60421    0.42128   3.808  0.00014 ***
fish_length    0.08660    0.01005   8.613  < 2e-16 ***
setup2:l        -0.06390    0.01409  -4.534  5.8e-06 ***


Second  approach:

Digging into the experimental data I realized the convenience of
including  *fish_length* as random slope. After this change, the
diagnostic criteria (AIC, BIC)  improved a lot, compared to the first
modeling approach but the winner model only  gave the intercept as
significant fixed effect:

mod45<-glmer(prop~1+(fish_length|haul),family=binomial,data=X.red,nAGQ=1,weights=ntotal)

       AIC       BIC              logLik         deviance
1876.6   1892.9283 -934.3459 1868.6918

Random effects:
  Groups Name        Variance Std.Dev. Corr
  haul   (Intercept)   6.97522    2.6411
fish_length             0.01171   0.1082   -0.98
Number of obs: 428, groups: haul, 14

Fixed effects:
             Estimate Std. Error z value Pr(>|z|)
(Intercept)  -1.4575     0.1292  -11.28   <2e-16 ***

As I understand this model output, Including the  *fish_length* as
random slope mask any fixed effect included in the model, since  the
functional form of *prop* conditioned to *fish_length*  varied a lot
between hauls. The problem arise when estimating the  confidence
intervals from this model;


   prop.mu       F_lci      F_uci      R_lci            R_uci
            0.18    0.15      0.23         0.001        0.97


where *prop.mu* is the expected *prop* estimated in mod45; *F_* denotes
the CI estimated using the fixed effect uncertainity, and *R_*
represents the confidence intervals adding the uncertainity produced by
the betweenn haul variation (sigma_2= 6.97522) . The *R_ * band is
unexpectely high ranging  from 0.001 to 0.97, crossing by far the
reference value of p=0.5 (equal catches).

I interpret this results as follows:

- Altough during the experiment there was a  significant catch reduction
in test gear (0.18[0.15-0.23 ]), this catch reduction is not significant
if we raise the inference into  a "population of hauls" perpective
(0.18[ 0.001-0.97]).

So the questions are:

1.- Is my interpretation correct?
2.- How can this be possible, when plotting the experimental *prop*
against *fish_length* -plot(prop~fish_length ) -  for each haul
separately , it shows *prop* to  be less than 0.5 for  ~95% of
*fish_length* presented in the catches?

Thank you in advance.



        [[alternative HTML version deleted]]



------------------------------


From bbolker at gmail.com  Mon Feb 10 03:36:02 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 10 Feb 2014 02:36:02 +0000 (UTC)
Subject: [R-sig-ME] pedigreemm number of levels per grouping factor
References: <DBBDA54CB159CC41A0A01219B109A4D001BBD248@VMEXCHANGEMBS5A.isad.isadroot.ex.ac.uk><52F28469.9010203@gmail.com>
	<DBBDA54CB159CC41A0A01219B109A4D001BBF06C@VMEXCHANGEMBS5A.isad.isadroot.ex.ac.uk>
	<alpine.LMD.2.00.1402071324230.9819@orpheus.qimr.edu.au>
	<DBBDA54CB159CC41A0A01219B109A4D001BBF2C1@VMEXCHANGEMBS5A.isad.isadroot.ex.ac.uk>
	<alpine.LMD.2.00.1402081824430.19877@orpheus.qimr.edu.au>
Message-ID: <loom.20140210T033440-585@post.gmane.org>

David Duffy <David.Duffy at ...> writes:

> 
> On Fri, 7 Feb 2014, Wilson, Alastair wrote:
> 
> > Thanks - that's v much appreciated. Data file and pedigree structures 
> > attached.
> 
> OK, runs fine with older versions of pedigreemm and lmer



 [snip]

> 
> The newest version of pedigreemm is failing in pedigreemm::relfactor(), 
> which is supposed to produce the Cholesky factor of the relationship 
> matrix, where the error arises in:
> 
> solve(t(as(ped,"sparseMatrix")),
>        as(factor(labs, levels = ped <at> label),"sparseMatrix"))
> 
> specifically in your exampe, NRM is 500*500 but labs is 400*1 (100 
> unphenotyped founders)
> 
> Cheers, David Duffy.


  I'm not following this particularly carefully, but if this turns out
to be an issue on the lme4 end (or something that could be alleviated
on the lme4 end, even if it's not an lme4 bug per se), please let the
maintainers know/consider posting an issue at https://github.com/lme4/lme4
...

   Ben Bolker


From David.Duffy at qimr.edu.au  Mon Feb 10 08:06:45 2014
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Mon, 10 Feb 2014 17:06:45 +1000
Subject: [R-sig-ME] pedigreemm number of levels per grouping factor
In-Reply-To: <loom.20140210T033440-585@post.gmane.org>
References: <DBBDA54CB159CC41A0A01219B109A4D001BBD248@VMEXCHANGEMBS5A.isad.isadroot.ex.ac.uk><52F28469.9010203@gmail.com><DBBDA54CB159CC41A0A01219B109A4D001BBF06C@VMEXCHANGEMBS5A.isad.isadroot.ex.ac.uk><alpine.LMD.2.00.1402071324230.9819@orpheus.qimr.edu.au><DBBDA54CB159CC41A0A01219B109A4D001BBF2C1@VMEXCHANGEMBS5A.isad.isadroot.ex.ac.uk><alpine.LMD.2.00.1402081824430.19877@orpheus.qimr.edu.au>
	<loom.20140210T033440-585@post.gmane.org>
Message-ID: <alpine.LMD.2.00.1402101619280.24226@orpheus.qimr.edu.au>

On Mon, 10 Feb 2014, Ben Bolker wrote:

>  I'm not following this particularly carefully, but if this turns out
> to be an issue on the lme4 end (or something that could be alleviated
> on the lme4 end, even if it's not an lme4 bug per se), please let the
> maintainers know/consider posting an issue at https://github.com/lme4/lme4

It is a pure pedigreemm problem.  I was making a smaller example for the 
maintainers, which has been sent.

x <- data.frame(id=c(1,2,3,4), sire=c(NA,NA,1,1),
                 dam=c(NA,NA,2,2), y=c(NA,NA,3,4))
library(pedigreemm)
p <- pedigree(x$sire, x$dam, x$id)
pedigreemm(y ~ (1|id), pedigree=list(id=p), data=x,
            control=lmerControl(check.nobs.vs.nlev="ignore"))
Error in .sortCsparse(.Call(dtCMatrix_sparse_solve, a, b)) :
   Dimensions of system to be solved are inconsistent

5: .sortCsparse(.Call(dtCMatrix_sparse_solve, a, b))
3: solve(t(as(ped, "sparseMatrix")), as(factor(labs, levels = ped at label),
        "sparseMatrix"))
2: relfactor(pedigree[[i]], rownames(Zt)[rowsi])

  t(as(ped, "sparseMatrix"))
4 x 4 sparse Matrix of class "dtCMatrix" (unitriangular)
      1 2    3    4
[1,] 1 . -0.5 -0.5
[2,] . 1 -0.5 -0.5
[3,] . .  1.0  .
[4,] . .  .    1.0

  as(factor(labs, levels = ped at label), "sparseMatrix")
2 x 2 sparse Matrix of class "dgCMatrix"

3 1 .
4 . 1

Cheers, David Duffy

(who doesn't have time at the moment to work out a fix ;))

| David Duffy (MBBS PhD)
| email: David.Duffy at qimrberghofer.edu.au  ph: INT+61+7+3362-0217 fax: -0101
| Genetic Epidemiology, QIMR Berghofer Institute of Medical Research
| 300 Herston Rd, Brisbane, Queensland 4006, Australia  GPG 4D0B994A


From tobias.heed.uhh at gmail.com  Mon Feb 10 09:36:57 2014
From: tobias.heed.uhh at gmail.com (Tobias Heed)
Date: Mon, 10 Feb 2014 09:36:57 +0100
Subject: [R-sig-ME] installing lme4.0 on OS X 10.9
In-Reply-To: <FCC7C722-0BB8-4137-AF6E-D9D7E9D836A8@ucsd.edu>
References: <E666E3F3-5114-452E-8E31-28B19D45005B@ucsd.edu>
	<etPan.52f53cdd.6b8b4567.2e5a@Macintosh.local>
	<FCC7C722-0BB8-4137-AF6E-D9D7E9D836A8@ucsd.edu>
Message-ID: <etPan.52f88faa.3730bb65.2e5a@Macintosh.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140210/7d472df6/attachment.pl>

From tim.cole at ucl.ac.uk  Mon Feb 10 12:09:49 2014
From: tim.cole at ucl.ac.uk (Cole, Tim)
Date: Mon, 10 Feb 2014 11:09:49 +0000
Subject: [R-sig-ME] installing lme4.0 on OS X 10.9
Message-ID: <CF1E6334.2D45A%tim.cole@ucl.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140210/d805d30e/attachment.pl>

From juan.santos at ti.bund.de  Mon Feb 10 16:25:32 2014
From: juan.santos at ti.bund.de (Juan Santos)
Date: Mon, 10 Feb 2014 16:25:32 +0100
Subject: [R-sig-ME] random slope model overinflate CIs (Greg Guerin)
In-Reply-To: <mailman.2510.1392016014.4563.r-sig-mixed-models@r-project.org>
References: <mailman.2510.1392016014.4563.r-sig-mixed-models@r-project.org>
Message-ID: <52F8EF6C.3060108@ti.bund.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140210/85d2be02/attachment.pl>

From simon.urbanek at r-project.org  Mon Feb 10 19:08:29 2014
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Mon, 10 Feb 2014 13:08:29 -0500
Subject: [R-sig-ME] [R-SIG-Mac]  installing lme4.0 on OS X 10.9
In-Reply-To: <CF1E6334.2D45A%tim.cole@ucl.ac.uk>
References: <CF1E6334.2D45A%tim.cole@ucl.ac.uk>
Message-ID: <D93104CD-46EA-4528-92B1-371CC205C79F@r-project.org>

Roger does not seem to have a working Fortran compiler which is required for lme4. He may want to install one, e.g. from
http://cran.r-project.org/bin/macosx/tools/

BTW: please don't point people to obsolete SO posts - the proper cure to most Mavericks problems is to simply install more recent build of R.

Cheers,
Simon


On Feb 10, 2014, at 6:09 AM, Cole, Tim <tim.cole at ucl.ac.uk> wrote:

> I'm forwarding this to R-sig-Mac.
> 
> Tim Cole
> --
> Tim.Cole at ucl.ac.uk<mailto:Tim.Cole at ich.ucl.ac.uk> Phone +44(0)20 7905 2666 Fax +44(0)20 7905 2381
> Centre for Paediatric Epidemiology and Biostatistics
> UCL Institute of Child Health, London WC1N 1EH, UK
> 
> From: Tobias Heed <tobias.heed.uhh at gmail.com<mailto:tobias.heed.uhh at gmail.com>>
> To: Levy, Roger <rlevy at ucsd.edu<mailto:rlevy at ucsd.edu>>
> Cc: "=?utf-8?Q?r-sig-mixed-models=40r-project.org?="
> <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
> Subject: Re: [R-sig-ME] installing lme4.0 on OS X 10.9
> Message-ID: <etPan.52f88faa.3730bb65.2e5a at Macintosh.local<mailto:etPan.52f88faa.3730bb65.2e5a at Macintosh.local>>
> Content-Type: text/plain
> 
> Roger,
> 
> sorry to hear this didn?t work. Unfortunately, I do not recall everything I tried back then. I do recall that the problem was that gcc doesn?t come with Mavericks. A few things still,
> 1, after reinstalling your Mac, did you update XCode? This is probably necessary.
> 2, after making the Makevars file, did you restart R?
> 
> Assuming that most probably you did both, I found these links in my notes, maybe they can point you in the right direction.
> http://stackoverflow.com/questions/19503995/error-when-with-xcode-5-0-and-rcpp-command-line-tools-are-installed
> http://stackoverflow.com/questions/19533220/cannot-install-r-package-from-source-in-mac-osx-maverick
> 
> hope this helps.
> Best,
> Tobias
> 
> 
> --
> --------------------------------------------------------------------------------------------------------------
> Tobias Heed, PhD
> Biological Psychology and Neuropsychology
> Department of Psychology & Human Movement Science  |  University of Hamburg
> Von-Melle-Park 11, Room 208  |  D-20146 Hamburg, Germany
> Phone: (49) 40 - 42838 5831  |  Fax:   (49) 40 - 42838 6591
> tobias.heed at uni-hamburg.de<mailto:tobias.heed at uni-hamburg.de>  |  Website  |  Google Scholar
> --------------------------------------------------------------------------------------------------------------
> From: Levy, Roger rlevy at ucsd.edu<mailto:rlevy at ucsd.edu>
> Reply: Levy, Roger rlevy at ucsd.edu<mailto:rlevy at ucsd.edu>
> Date: February 9,2014 at 23:41:48
> To: Tobias Heed tobias.heed.uhh at gmail.com<mailto:tobias.heed.uhh at gmail.com>
> Cc: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org> r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>
> Subject:  Re: [R-sig-ME] installing lme4.0 on OS X 10.9
> Thanks so much, Tobias. Unfortunately, setting up a Makevars file like you suggested simply leaves me with the same errors in R as I get in the command line:
> 
> install.packages("lme4.0", repos="http://r-forge.r-project.org",type="source")
> ?
> <snip>
> ?
> clang -dynamiclib -Wl,-headerpad_max_install_names -undefined dynamic_lookup -single_module -multiply_defined suppress -L/usr/local/lib -L/usr/local/lib -o lme4.so init.o lmer.o local_stubs.o -L/Library/Frameworks/R.framework/Resources/lib -lRlapack -L/Library/Frameworks/R.framework/Resources/lib -lRblas -L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3/x86_64 -L/usr/local/lib/x86_64 -L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3 -lgfortran -F/Library/Frameworks/R.framework/.. -framework R -Wl,-framework -Wl,CoreFoundation
> ?
> ld: warning: directory not found for option '-L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3/x86_64'
> ld: warning: directory not found for option '-L/usr/local/lib/x86_64'
> ld: warning: directory not found for option '-L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3'
> ld: library not found for -lgfortran
> clang: error: linker command failed with exit code 1 (use -v to see invocation)
> 
> 
> Any further thoughts or suggestions on this, anyone?
> 
> Best
> 
> Roger
> 
> On Feb 7, 2014, at 12:06 PM, Tobias Heed <tobias.heed.uhh at gmail.com<mailto:tobias.heed.uhh at gmail.com>> wrote:
> 
> Dear Roger,
> 
> if I remember correctly, this solved the problem for me:
> on the top level of your user account, create a folder .R (if it?s not there yet);
> in there, create a new text file named Makevars;
> into this file, paste the following:
> 
> CC=clang
> CXX=clang++ -arch x86_64 -ftemplate-depth-256 -stdlib=libstdc++
> CXXFLAGS= -O3 -pedantic
> 
> hope it helps
> Best,
> Tobias
> 
> 
> From: Levy, Roger rlevy at ucsd.edu<mailto:rlevy at ucsd.edu>
> Reply: Levy, Roger rlevy at ucsd.edu<mailto:rlevy at ucsd.edu>
> Date: February 7,2014 at 20:59:11
> To: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org> r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>
> Subject: [R-sig-ME] installing lme4.0 on OS X 10.9
> Hi all,
> 
> My Mac laptop died and I am now reinstalling all my packages from scratch. Previously I was using lme4_0.999999-2 and I figure that with a fresh install, I might as well install lme4.0 instead. But I am having trouble compiling it:
> 
>> install.packages("lme4.0", repos="http://r-forge.r-project.org",type="source")
> trying URL 'http://r-forge.r-project.org/src/contrib/lme4.0_0.999999-4.tar.gz'
> Content type 'application/x-gzip' length 772384 bytes (754 Kb)
> opened URL
> ==================================================
> downloaded 754 Kb
> 
> * installing *source* package ?lme4.0? ...
> ** libs
> llvm-gcc-4.2 -arch x86_64 -std=gnu99 -I/Library/Frameworks/R.framework/Resources/include -DNDEBUG -I/usr/local/include -I"/Library/Frameworks/R.framework/Versions/3.0/Resources/library/Matrix/include" -I"/Library/Frameworks/R.framework/Versions/3.0/Resources/library/stats/include" -fPIC -mtune=core2 -g -O2 -c init.c -o init.o
> /bin/sh: llvm-gcc-4.2: command not found
> 
> 
> This happens even after I install Xcode and also llvm-gcc42 on MacPorts;
> 
> Does anyone have any suggestions? FWIW:
> 
>> sessionInfo()
> R version 3.0.2 (2013-09-25)
> Platform: x86_64-apple-darwin10.8.0 (64-bit)
> 
> locale:
> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
> 
> attached base packages:
> [1] stats graphics grDevices utils datasets methods base
> 
> loaded via a namespace (and not attached):
> [1] tools_3.0.2
> 
> 
> I?ve also tried installing from the command line; this bottoms out in a different error:
> 
> $ R CMD build lme4.0
> ?
> <snip>
> ?
> ld: warning: directory not found for option ?-L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3/x86_64?
> ld: warning: directory not found for option ?-L/usr/local/lib/x86_64?
> ld: warning: directory not found for option ?-L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3?
> ld: library not found for -lgfortran
> clang: error: linker command failed with exit code 1 (use -v to see invocation)
> make: *** [lme4.so] Error 1
> 
> 
> More generally, I?m having trouble figuring out what I should install in order to be able to compile lme4.0 from source.
> 
> Best & many thanks in advance for any guidance,
> 
> Roger Levy
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Mac mailing list
> R-SIG-Mac at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-mac


From rlevy at ucsd.edu  Mon Feb 10 19:26:01 2014
From: rlevy at ucsd.edu (Levy, Roger)
Date: Mon, 10 Feb 2014 18:26:01 +0000
Subject: [R-sig-ME] [R-SIG-Mac]  installing lme4.0 on OS X 10.9
In-Reply-To: <D93104CD-46EA-4528-92B1-371CC205C79F@r-project.org>
References: <CF1E6334.2D45A%tim.cole@ucl.ac.uk>
	<D93104CD-46EA-4528-92B1-371CC205C79F@r-project.org>
Message-ID: <BD22E2BF-0A29-47E1-B345-C50BE603C807@ucsd.edu>

Thank you!  Installing gfortran-4.2.3.pkg fixed things.  FWIW, it was indeed the most recent build of R that I had installed.

So that I (and perhaps other list members) understand the issues more completely: one thing that was very confounding to me is that installing gcc49 via MacPorts *does* provide a Fortran compiler, which goes in /opt/local/bin/gfortran.  However, I couldn?t figure out how to get R to use this compiler.  Perhaps this requires properly setting variables in ~/.R/Makevars, but I couldn?t figure out what the proper settings would be.  Do you have any guidance on this?

Best & thanks once more.

Roger



On Feb 10, 2014, at 11:08 AM, Simon Urbanek <simon.urbanek at r-project.org> wrote:

> Roger does not seem to have a working Fortran compiler which is required for lme4. He may want to install one, e.g. from
> http://cran.r-project.org/bin/macosx/tools/
> 
> BTW: please don't point people to obsolete SO posts - the proper cure to most Mavericks problems is to simply install more recent build of R.
> 
> Cheers,
> Simon
> 
> 
> On Feb 10, 2014, at 6:09 AM, Cole, Tim <tim.cole at ucl.ac.uk> wrote:
> 
>> I'm forwarding this to R-sig-Mac.
>> 
>> Tim Cole
>> --
>> Tim.Cole at ucl.ac.uk<mailto:Tim.Cole at ich.ucl.ac.uk> Phone +44(0)20 7905 2666 Fax +44(0)20 7905 2381
>> Centre for Paediatric Epidemiology and Biostatistics
>> UCL Institute of Child Health, London WC1N 1EH, UK
>> 
>> From: Tobias Heed <tobias.heed.uhh at gmail.com<mailto:tobias.heed.uhh at gmail.com>>
>> To: Levy, Roger <rlevy at ucsd.edu<mailto:rlevy at ucsd.edu>>
>> Cc: "=?utf-8?Q?r-sig-mixed-models=40r-project.org?="
>> <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
>> Subject: Re: [R-sig-ME] installing lme4.0 on OS X 10.9
>> Message-ID: <etPan.52f88faa.3730bb65.2e5a at Macintosh.local<mailto:etPan.52f88faa.3730bb65.2e5a at Macintosh.local>>
>> Content-Type: text/plain
>> 
>> Roger,
>> 
>> sorry to hear this didn?t work. Unfortunately, I do not recall everything I tried back then. I do recall that the problem was that gcc doesn?t come with Mavericks. A few things still,
>> 1, after reinstalling your Mac, did you update XCode? This is probably necessary.
>> 2, after making the Makevars file, did you restart R?
>> 
>> Assuming that most probably you did both, I found these links in my notes, maybe they can point you in the right direction.
>> http://stackoverflow.com/questions/19503995/error-when-with-xcode-5-0-and-rcpp-command-line-tools-are-installed
>> http://stackoverflow.com/questions/19533220/cannot-install-r-package-from-source-in-mac-osx-maverick
>> 
>> hope this helps.
>> Best,
>> Tobias
>> 
>> 
>> --
>> --------------------------------------------------------------------------------------------------------------
>> Tobias Heed, PhD
>> Biological Psychology and Neuropsychology
>> Department of Psychology & Human Movement Science  |  University of Hamburg
>> Von-Melle-Park 11, Room 208  |  D-20146 Hamburg, Germany
>> Phone: (49) 40 - 42838 5831  |  Fax:   (49) 40 - 42838 6591
>> tobias.heed at uni-hamburg.de<mailto:tobias.heed at uni-hamburg.de>  |  Website  |  Google Scholar
>> --------------------------------------------------------------------------------------------------------------
>> From: Levy, Roger rlevy at ucsd.edu<mailto:rlevy at ucsd.edu>
>> Reply: Levy, Roger rlevy at ucsd.edu<mailto:rlevy at ucsd.edu>
>> Date: February 9,2014 at 23:41:48
>> To: Tobias Heed tobias.heed.uhh at gmail.com<mailto:tobias.heed.uhh at gmail.com>
>> Cc: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org> r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>
>> Subject:  Re: [R-sig-ME] installing lme4.0 on OS X 10.9
>> Thanks so much, Tobias. Unfortunately, setting up a Makevars file like you suggested simply leaves me with the same errors in R as I get in the command line:
>> 
>> install.packages("lme4.0", repos="http://r-forge.r-project.org",type="source")
>> ?
>> <snip>
>> ?
>> clang -dynamiclib -Wl,-headerpad_max_install_names -undefined dynamic_lookup -single_module -multiply_defined suppress -L/usr/local/lib -L/usr/local/lib -o lme4.so init.o lmer.o local_stubs.o -L/Library/Frameworks/R.framework/Resources/lib -lRlapack -L/Library/Frameworks/R.framework/Resources/lib -lRblas -L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3/x86_64 -L/usr/local/lib/x86_64 -L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3 -lgfortran -F/Library/Frameworks/R.framework/.. -framework R -Wl,-framework -Wl,CoreFoundation
>> ?
>> ld: warning: directory not found for option '-L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3/x86_64'
>> ld: warning: directory not found for option '-L/usr/local/lib/x86_64'
>> ld: warning: directory not found for option '-L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3'
>> ld: library not found for -lgfortran
>> clang: error: linker command failed with exit code 1 (use -v to see invocation)
>> 
>> 
>> Any further thoughts or suggestions on this, anyone?
>> 
>> Best
>> 
>> Roger
>> 
>> On Feb 7, 2014, at 12:06 PM, Tobias Heed <tobias.heed.uhh at gmail.com<mailto:tobias.heed.uhh at gmail.com>> wrote:
>> 
>> Dear Roger,
>> 
>> if I remember correctly, this solved the problem for me:
>> on the top level of your user account, create a folder .R (if it?s not there yet);
>> in there, create a new text file named Makevars;
>> into this file, paste the following:
>> 
>> CC=clang
>> CXX=clang++ -arch x86_64 -ftemplate-depth-256 -stdlib=libstdc++
>> CXXFLAGS= -O3 -pedantic
>> 
>> hope it helps
>> Best,
>> Tobias
>> 
>> 
>> From: Levy, Roger rlevy at ucsd.edu<mailto:rlevy at ucsd.edu>
>> Reply: Levy, Roger rlevy at ucsd.edu<mailto:rlevy at ucsd.edu>
>> Date: February 7,2014 at 20:59:11
>> To: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org> r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>
>> Subject: [R-sig-ME] installing lme4.0 on OS X 10.9
>> Hi all,
>> 
>> My Mac laptop died and I am now reinstalling all my packages from scratch. Previously I was using lme4_0.999999-2 and I figure that with a fresh install, I might as well install lme4.0 instead. But I am having trouble compiling it:
>> 
>>> install.packages("lme4.0", repos="http://r-forge.r-project.org",type="source")
>> trying URL 'http://r-forge.r-project.org/src/contrib/lme4.0_0.999999-4.tar.gz'
>> Content type 'application/x-gzip' length 772384 bytes (754 Kb)
>> opened URL
>> ==================================================
>> downloaded 754 Kb
>> 
>> * installing *source* package ?lme4.0? ...
>> ** libs
>> llvm-gcc-4.2 -arch x86_64 -std=gnu99 -I/Library/Frameworks/R.framework/Resources/include -DNDEBUG -I/usr/local/include -I"/Library/Frameworks/R.framework/Versions/3.0/Resources/library/Matrix/include" -I"/Library/Frameworks/R.framework/Versions/3.0/Resources/library/stats/include" -fPIC -mtune=core2 -g -O2 -c init.c -o init.o
>> /bin/sh: llvm-gcc-4.2: command not found
>> 
>> 
>> This happens even after I install Xcode and also llvm-gcc42 on MacPorts;
>> 
>> Does anyone have any suggestions? FWIW:
>> 
>>> sessionInfo()
>> R version 3.0.2 (2013-09-25)
>> Platform: x86_64-apple-darwin10.8.0 (64-bit)
>> 
>> locale:
>> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>> 
>> attached base packages:
>> [1] stats graphics grDevices utils datasets methods base
>> 
>> loaded via a namespace (and not attached):
>> [1] tools_3.0.2
>> 
>> 
>> I?ve also tried installing from the command line; this bottoms out in a different error:
>> 
>> $ R CMD build lme4.0
>> ?
>> <snip>
>> ?
>> ld: warning: directory not found for option ?-L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3/x86_64?
>> ld: warning: directory not found for option ?-L/usr/local/lib/x86_64?
>> ld: warning: directory not found for option ?-L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3?
>> ld: library not found for -lgfortran
>> clang: error: linker command failed with exit code 1 (use -v to see invocation)
>> make: *** [lme4.so] Error 1
>> 
>> 
>> More generally, I?m having trouble figuring out what I should install in order to be able to compile lme4.0 from source.
>> 
>> Best & many thanks in advance for any guidance,
>> 
>> Roger Levy
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> _______________________________________________
>> R-SIG-Mac mailing list
>> R-SIG-Mac at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mac
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From simon.urbanek at r-project.org  Mon Feb 10 20:12:49 2014
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Mon, 10 Feb 2014 14:12:49 -0500
Subject: [R-sig-ME] [R-SIG-Mac]  installing lme4.0 on OS X 10.9
In-Reply-To: <BD22E2BF-0A29-47E1-B345-C50BE603C807@ucsd.edu>
References: <CF1E6334.2D45A%tim.cole@ucl.ac.uk>
	<D93104CD-46EA-4528-92B1-371CC205C79F@r-project.org>
	<BD22E2BF-0A29-47E1-B345-C50BE603C807@ucsd.edu>
Message-ID: <73994D3C-D4BE-454C-AB6D-FD2ACA644243@r-project.org>


On Feb 10, 2014, at 1:26 PM, Levy, Roger <rlevy at ucsd.edu> wrote:

> Thank you!  Installing gfortran-4.2.3.pkg fixed things.  FWIW, it was indeed the most recent build of R that I had installed.
> 
> So that I (and perhaps other list members) understand the issues more completely: one thing that was very confounding to me is that installing gcc49 via MacPorts *does* provide a Fortran compiler, which goes in /opt/local/bin/gfortran.  However, I couldn?t figure out how to get R to use this compiler.  Perhaps this requires properly setting variables in ~/.R/Makevars, but I couldn?t figure out what the proper settings would be.  Do you have any guidance on this?
> 

The CRAN build is a native build of R and only supports native compilers. If you use MacPorts you're entirely on your own - you may want to use MacPorts R in that case or compile everything from sources accordingly.

Cheers,
Simon


> Best & thanks once more.
> 
> Roger
> 
> 
> 
> On Feb 10, 2014, at 11:08 AM, Simon Urbanek <simon.urbanek at r-project.org> wrote:
> 
>> Roger does not seem to have a working Fortran compiler which is required for lme4. He may want to install one, e.g. from
>> http://cran.r-project.org/bin/macosx/tools/
>> 
>> BTW: please don't point people to obsolete SO posts - the proper cure to most Mavericks problems is to simply install more recent build of R.
>> 
>> Cheers,
>> Simon
>> 
>> 
>> On Feb 10, 2014, at 6:09 AM, Cole, Tim <tim.cole at ucl.ac.uk> wrote:
>> 
>>> I'm forwarding this to R-sig-Mac.
>>> 
>>> Tim Cole
>>> --
>>> Tim.Cole at ucl.ac.uk<mailto:Tim.Cole at ich.ucl.ac.uk> Phone +44(0)20 7905 2666 Fax +44(0)20 7905 2381
>>> Centre for Paediatric Epidemiology and Biostatistics
>>> UCL Institute of Child Health, London WC1N 1EH, UK
>>> 
>>> From: Tobias Heed <tobias.heed.uhh at gmail.com<mailto:tobias.heed.uhh at gmail.com>>
>>> To: Levy, Roger <rlevy at ucsd.edu<mailto:rlevy at ucsd.edu>>
>>> Cc: "=?utf-8?Q?r-sig-mixed-models=40r-project.org?="
>>> <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
>>> Subject: Re: [R-sig-ME] installing lme4.0 on OS X 10.9
>>> Message-ID: <etPan.52f88faa.3730bb65.2e5a at Macintosh.local<mailto:etPan.52f88faa.3730bb65.2e5a at Macintosh.local>>
>>> Content-Type: text/plain
>>> 
>>> Roger,
>>> 
>>> sorry to hear this didn?t work. Unfortunately, I do not recall everything I tried back then. I do recall that the problem was that gcc doesn?t come with Mavericks. A few things still,
>>> 1, after reinstalling your Mac, did you update XCode? This is probably necessary.
>>> 2, after making the Makevars file, did you restart R?
>>> 
>>> Assuming that most probably you did both, I found these links in my notes, maybe they can point you in the right direction.
>>> http://stackoverflow.com/questions/19503995/error-when-with-xcode-5-0-and-rcpp-command-line-tools-are-installed
>>> http://stackoverflow.com/questions/19533220/cannot-install-r-package-from-source-in-mac-osx-maverick
>>> 
>>> hope this helps.
>>> Best,
>>> Tobias
>>> 
>>> 
>>> --
>>> --------------------------------------------------------------------------------------------------------------
>>> Tobias Heed, PhD
>>> Biological Psychology and Neuropsychology
>>> Department of Psychology & Human Movement Science  |  University of Hamburg
>>> Von-Melle-Park 11, Room 208  |  D-20146 Hamburg, Germany
>>> Phone: (49) 40 - 42838 5831  |  Fax:   (49) 40 - 42838 6591
>>> tobias.heed at uni-hamburg.de<mailto:tobias.heed at uni-hamburg.de>  |  Website  |  Google Scholar
>>> --------------------------------------------------------------------------------------------------------------
>>> From: Levy, Roger rlevy at ucsd.edu<mailto:rlevy at ucsd.edu>
>>> Reply: Levy, Roger rlevy at ucsd.edu<mailto:rlevy at ucsd.edu>
>>> Date: February 9,2014 at 23:41:48
>>> To: Tobias Heed tobias.heed.uhh at gmail.com<mailto:tobias.heed.uhh at gmail.com>
>>> Cc: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org> r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>
>>> Subject:  Re: [R-sig-ME] installing lme4.0 on OS X 10.9
>>> Thanks so much, Tobias. Unfortunately, setting up a Makevars file like you suggested simply leaves me with the same errors in R as I get in the command line:
>>> 
>>> install.packages("lme4.0", repos="http://r-forge.r-project.org",type="source")
>>> ?
>>> <snip>
>>> ?
>>> clang -dynamiclib -Wl,-headerpad_max_install_names -undefined dynamic_lookup -single_module -multiply_defined suppress -L/usr/local/lib -L/usr/local/lib -o lme4.so init.o lmer.o local_stubs.o -L/Library/Frameworks/R.framework/Resources/lib -lRlapack -L/Library/Frameworks/R.framework/Resources/lib -lRblas -L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3/x86_64 -L/usr/local/lib/x86_64 -L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3 -lgfortran -F/Library/Frameworks/R.framework/.. -framework R -Wl,-framework -Wl,CoreFoundation
>>> ?
>>> ld: warning: directory not found for option '-L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3/x86_64'
>>> ld: warning: directory not found for option '-L/usr/local/lib/x86_64'
>>> ld: warning: directory not found for option '-L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3'
>>> ld: library not found for -lgfortran
>>> clang: error: linker command failed with exit code 1 (use -v to see invocation)
>>> 
>>> 
>>> Any further thoughts or suggestions on this, anyone?
>>> 
>>> Best
>>> 
>>> Roger
>>> 
>>> On Feb 7, 2014, at 12:06 PM, Tobias Heed <tobias.heed.uhh at gmail.com<mailto:tobias.heed.uhh at gmail.com>> wrote:
>>> 
>>> Dear Roger,
>>> 
>>> if I remember correctly, this solved the problem for me:
>>> on the top level of your user account, create a folder .R (if it?s not there yet);
>>> in there, create a new text file named Makevars;
>>> into this file, paste the following:
>>> 
>>> CC=clang
>>> CXX=clang++ -arch x86_64 -ftemplate-depth-256 -stdlib=libstdc++
>>> CXXFLAGS= -O3 -pedantic
>>> 
>>> hope it helps
>>> Best,
>>> Tobias
>>> 
>>> 
>>> From: Levy, Roger rlevy at ucsd.edu<mailto:rlevy at ucsd.edu>
>>> Reply: Levy, Roger rlevy at ucsd.edu<mailto:rlevy at ucsd.edu>
>>> Date: February 7,2014 at 20:59:11
>>> To: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org> r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>
>>> Subject: [R-sig-ME] installing lme4.0 on OS X 10.9
>>> Hi all,
>>> 
>>> My Mac laptop died and I am now reinstalling all my packages from scratch. Previously I was using lme4_0.999999-2 and I figure that with a fresh install, I might as well install lme4.0 instead. But I am having trouble compiling it:
>>> 
>>>> install.packages("lme4.0", repos="http://r-forge.r-project.org",type="source")
>>> trying URL 'http://r-forge.r-project.org/src/contrib/lme4.0_0.999999-4.tar.gz'
>>> Content type 'application/x-gzip' length 772384 bytes (754 Kb)
>>> opened URL
>>> ==================================================
>>> downloaded 754 Kb
>>> 
>>> * installing *source* package ?lme4.0? ...
>>> ** libs
>>> llvm-gcc-4.2 -arch x86_64 -std=gnu99 -I/Library/Frameworks/R.framework/Resources/include -DNDEBUG -I/usr/local/include -I"/Library/Frameworks/R.framework/Versions/3.0/Resources/library/Matrix/include" -I"/Library/Frameworks/R.framework/Versions/3.0/Resources/library/stats/include" -fPIC -mtune=core2 -g -O2 -c init.c -o init.o
>>> /bin/sh: llvm-gcc-4.2: command not found
>>> 
>>> 
>>> This happens even after I install Xcode and also llvm-gcc42 on MacPorts;
>>> 
>>> Does anyone have any suggestions? FWIW:
>>> 
>>>> sessionInfo()
>>> R version 3.0.2 (2013-09-25)
>>> Platform: x86_64-apple-darwin10.8.0 (64-bit)
>>> 
>>> locale:
>>> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>>> 
>>> attached base packages:
>>> [1] stats graphics grDevices utils datasets methods base
>>> 
>>> loaded via a namespace (and not attached):
>>> [1] tools_3.0.2
>>> 
>>> 
>>> I?ve also tried installing from the command line; this bottoms out in a different error:
>>> 
>>> $ R CMD build lme4.0
>>> ?
>>> <snip>
>>> ?
>>> ld: warning: directory not found for option ?-L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3/x86_64?
>>> ld: warning: directory not found for option ?-L/usr/local/lib/x86_64?
>>> ld: warning: directory not found for option ?-L/usr/local/lib/gcc/i686-apple-darwin8/4.2.3?
>>> ld: library not found for -lgfortran
>>> clang: error: linker command failed with exit code 1 (use -v to see invocation)
>>> make: *** [lme4.so] Error 1
>>> 
>>> 
>>> More generally, I?m having trouble figuring out what I should install in order to be able to compile lme4.0 from source.
>>> 
>>> Best & many thanks in advance for any guidance,
>>> 
>>> Roger Levy
>>> 
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>> 
>>> 	[[alternative HTML version deleted]]
>>> 
>>> _______________________________________________
>>> R-SIG-Mac mailing list
>>> R-SIG-Mac at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mac
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 


From thomaschroder at gmail.com  Tue Feb 11 02:48:06 2014
From: thomaschroder at gmail.com (Thomas Schroder)
Date: Mon, 10 Feb 2014 23:48:06 -0200
Subject: [R-sig-ME] Converging mixed models with large number of random
	coefficients
Message-ID: <CANjXykuTY2ox92tEbHDuv6dvyjngxMLn8x_y+8YoHBw=QAQeKA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140210/c620b280/attachment.pl>

From Tania.Cerni at unitn.it  Tue Feb 11 12:07:50 2014
From: Tania.Cerni at unitn.it (Cerni, Tania)
Date: Tue, 11 Feb 2014 12:07:50 +0100
Subject: [R-sig-ME] random structure in lmer
Message-ID: <06085402EB5F15428D04F536A3BE2C8D10A60404CC@MBX.unitn.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140211/88a9a07d/attachment.pl>

From bbolker at gmail.com  Tue Feb 11 14:53:04 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 11 Feb 2014 08:53:04 -0500
Subject: [R-sig-ME] random structure in lmer
In-Reply-To: <06085402EB5F15428D04F536A3BE2C8D10A60404CC@MBX.unitn.it>
References: <06085402EB5F15428D04F536A3BE2C8D10A60404CC@MBX.unitn.it>
Message-ID: <52FA2B40.5030604@gmail.com>

On 14-02-11 06:07 AM, Cerni, Tania wrote:
> Dear all, I'm  a PhD student in Cognitive science. I'm working on an
> analysis with mixed models with R (lme4 and lmerTest)  but I'm  still
> in trouble with random factors. I have read some Baayen's works and I
> tried to follow his methods, but every time, after comparing models
> with different slopes, the output of lmer is the summary of the model
> preceded by:
> 
> "Asymptotic covariance matrix A is not positive!"
> 
> I want to ask you what kind of problems can I have with this message
> and, most of all, I'm writing to you to ask if you can advise me a
> shared procedure for random structure.

  For what it's worth, this message comes from lmerTest, not
lme4, and I suspect it actually has to do with your fixed-effect
structure, or the interaction between your fixed-effect structure
and your random-effect structure, not your random-effect structure
per se.  I suspect there is some near-perfect multicollinearity
among some of your factors, but I'm not sure.  Have you thought
carefully about your experimental design to make sure that all
of the parameters you want to estimate are really identifiable ... ?

   Ben Bolker

> 
> I have this kind of models: with 3way interaction or 2way
> interaction.
> 
> lex0 = lmer(rt~Trial +first_letter+letters+w.pw*group*manipulation+
> (1|Sj)+ (1|Word),lex)
> 
> the main thing is to see if there is an interaction between groups
> and manipulation.  The other fixed factors are predictors
> 
> 
> 
> Random effects:
> 
> Groups   Name        Variance Std.Dev.
> 
> Word     (Intercept) 0.006321 0.0795
> Sj       (Intercept) 0.022424 0.1497
> Residual             0.032240 0.1796
> 
> Number of obs: 7169, groups: Word, 240; Sj, 32
> 
> 
> 
  [snip]


> How can I adjust my random structure? Thanks, Tania


From bbolker at gmail.com  Tue Feb 11 16:07:46 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 11 Feb 2014 10:07:46 -0500
Subject: [R-sig-ME] Fwd: R:  random structure in lmer
In-Reply-To: <06085402EB5F15428D04F536A3BE2C8D10A6040565@MBX.unitn.it>
References: <06085402EB5F15428D04F536A3BE2C8D10A6040565@MBX.unitn.it>
Message-ID: <52FA3CC2.1090101@gmail.com>


  cc'ing back to r-sig-mixed-models ...

  The example you showed in your original post did not have random
slopes, which might explain my confusion ...

  Next question: when you fit the random-slopes model, do you have a
singular fit (i.e. do you have variances estimated to be zero, or
correlations estimated to be +/- 1.0, or are some of the elements of
getME(fitted_model,"theta") equal to zero)?

  I think this would explain a non-positive-definite Hessian matrix
(i.e., matrix of second derivatives of the fitted parameters).

  If that is the case, there is a lot of discussion on this list about
what to do in this situation, although no entirely satisfactory answers.
(There's not much very specific at http://glmm.wikidot.com/faq ; I
should add a section.)

  Ben Bolker


-------- Original Message --------
Subject: R: [R-sig-ME] random structure in lmer
Date: Tue, 11 Feb 2014 16:01:22 +0100
From: Cerni, Tania <Tania.Cerni at unitn.it>
To: Ben Bolker <bbolker at gmail.com>

Thanks for the answer,
Well, I receive this message every time I introduce a random slope in
the model. Not with the simple structure  (1|Sj)+ (1|Word).
In the experiment I'm interested in this interaction:
w.pw*group*manipulation or group*manipulation. The other variables are
inside only to see if they have an effect that can potentiate or delete
the interaction (for example "letters" is the length of the word,
"Trial" is the trail number to see an effect of fatigue) and I do not
think about multicollinearity.
Do you think I have to investigate only the interaction without
controlling words parameters?

Tania

-----Messaggio originale-----
Da: Ben Bolker [mailto:bbolker at gmail.com]
Inviato: marted? 11 febbraio 2014 14:53
A: Cerni, Tania; r-sig-mixed-models at r-project.org
Oggetto: Re: [R-sig-ME] random structure in lmer

On 14-02-11 06:07 AM, Cerni, Tania wrote:
> Dear all, I'm  a PhD student in Cognitive science. I'm working on an 
> analysis with mixed models with R (lme4 and lmerTest)  but I'm  still 
> in trouble with random factors. I have read some Baayen's works and I 
> tried to follow his methods, but every time, after comparing models 
> with different slopes, the output of lmer is the summary of the model 
> preceded by:
> 
> "Asymptotic covariance matrix A is not positive!"
> 
> I want to ask you what kind of problems can I have with this message 
> and, most of all, I'm writing to you to ask if you can advise me a 
> shared procedure for random structure.

  For what it's worth, this message comes from lmerTest, not lme4, and I
suspect it actually has to do with your fixed-effect structure, or the
interaction between your fixed-effect structure and your random-effect
structure, not your random-effect structure per se.  I suspect there is
some near-perfect multicollinearity among some of your factors, but I'm
not sure.  Have you thought carefully about your experimental design to
make sure that all of the parameters you want to estimate are really
identifiable ... ?

   Ben Bolker

> 
> I have this kind of models: with 3way interaction or 2way interaction.
> 
> lex0 = lmer(rt~Trial +first_letter+letters+w.pw*group*manipulation+
> (1|Sj)+ (1|Word),lex)
> 
> the main thing is to see if there is an interaction between groups and 
> manipulation.  The other fixed factors are predictors
> 
> 
> 
> Random effects:
> 
> Groups   Name        Variance Std.Dev.
> 
> Word     (Intercept) 0.006321 0.0795
> Sj       (Intercept) 0.022424 0.1497
> Residual             0.032240 0.1796
> 
> Number of obs: 7169, groups: Word, 240; Sj, 32
> 
> 
> 
  [snip]


> How can I adjust my random structure? Thanks, Tania


From A.Wilson at exeter.ac.uk  Tue Feb 11 13:09:16 2014
From: A.Wilson at exeter.ac.uk (Wilson, Alastair)
Date: Tue, 11 Feb 2014 12:09:16 +0000
Subject: [R-sig-ME] pedigreemm number of levels per grouping factor
In-Reply-To: <alpine.LMD.2.00.1402081824430.19877@orpheus.qimr.edu.au>
References: <DBBDA54CB159CC41A0A01219B109A4D001BBD248@VMEXCHANGEMBS5A.isad.isadroot.ex.ac.uk><52F28469.9010203@gmail.com>
	<DBBDA54CB159CC41A0A01219B109A4D001BBF06C@VMEXCHANGEMBS5A.isad.isadroot.ex.ac.uk>
	<alpine.LMD.2.00.1402071324230.9819@orpheus.qimr.edu.au>
	<DBBDA54CB159CC41A0A01219B109A4D001BBF2C1@VMEXCHANGEMBS5A.isad.isadroot.ex.ac.uk>,
	<alpine.LMD.2.00.1402081824430.19877@orpheus.qimr.edu.au>
Message-ID: <DBBDA54CB159CC41A0A01219B109A4D001BC0C23@VMEXCHANGEMBS5A.isad.isadroot.ex.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140211/276e7048/attachment.pl>

From asafw.at.wharton at gmail.com  Tue Feb 11 23:25:31 2014
From: asafw.at.wharton at gmail.com (Asaf Weinstein)
Date: Tue, 11 Feb 2014 17:25:31 -0500
Subject: [R-sig-ME] lme4: constrain sigma to 0
In-Reply-To: <528E637D.2070302@gmail.com>
References: <528E4605.5060607@uni-mannheim.de>
	<528E637D.2070302@gmail.com>
Message-ID: <CAGG0PdBu8yHYpHafCjmr0Jx_yB-LHqzVX13FbGnRUS-OOBypNQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140211/bff4e2e3/attachment.pl>

From r.turner at auckland.ac.nz  Wed Feb 12 00:18:18 2014
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Wed, 12 Feb 2014 12:18:18 +1300
Subject: [R-sig-ME] lme4: constrain sigma to 0
In-Reply-To: <CAGG0PdBu8yHYpHafCjmr0Jx_yB-LHqzVX13FbGnRUS-OOBypNQ@mail.gmail.com>
References: <528E4605.5060607@uni-mannheim.de> <528E637D.2070302@gmail.com>
	<CAGG0PdBu8yHYpHafCjmr0Jx_yB-LHqzVX13FbGnRUS-OOBypNQ@mail.gmail.com>
Message-ID: <52FAAFBA.1030304@auckland.ac.nz>


Several millennia ago I put in a feature-request that the facility for 
constraining sigma to 0 be added.  So far, as far as I can tell, it 
hasn't been.

Apparently it is (very) difficult to add such a constraint due to the 
way that lmer approaches the maximization of the likelihood.  (This, 
rather than any intrinsic mathematical block to such a constraint.)

I wanted to be able to fit a fairly simple repeated measures model with 
the covariance over time being an "arbitrary" n-x-n positive definite 
matrix (where "n" is the number of time points).  This can't be done
using lmer() unless one can constrain the "overall" variance to be zero,
otherwise the diagonal of the covariance matrix is un-identifiable.

Since one cannot constrain sigma to be 0, one can't fit this model with 
lmer().  Bummer.

cheers,

Rolf Turner

On 12/02/14 11:25, Asaf Weinstein wrote:
> Dear lme4 community (and Ben),
>
> I am interested in obtaining estimates for the usual linear model but with
> a known/ fixed value of residual variance.
> When searching for an answer in the forum I came across this question by
> Maya (machunsky at uni-mannheim.de), and it looks like there still isn't a
> built-in method to obtain MLEs under constraints. Is that correct?
>
> Thank you so much,
> Asaf
>
>
> On 21 November 2013 14:48, Ben Bolker <bbolker at gmail.com> wrote:
>
>> On 13-11-21 12:42 PM, Maya Machunsky wrote:
>>>   Dear Dr. Bolker,
>>>
>>> I am using lmer to estimate fixed effects for exerimental data (in
>>> Psychology). At the moment I am trying to run a model in which the
>>> residual variance is fixed to 0 and I am wondering whether it is
>>> possible to run such a model with lmer. I found messages from 2011 (a
>>> discussion between you and Rolf Turner) according to which it was not
>>> possible to constrain sigma to 0. Now I am asking myself whether it is
>>> now with a new version of the lme4 package possible.
>>>
>>> Thanks a lot in advance and
>>> best regards,
>>> Maya Machunsky
>>
>>    No, still not possible (or at least not easy), because the
>> residual variance is profiled out, so it doesn't enter directly
>> into the calculations -- it's not a parameter you can set.
>>
>>    You can sort of achieve this using blmer to set a 'point' prior
>> on sigma (i.e. fix it at a specific value) -- but you can't set the
>> residual variance to *exactly* zero.
>>
>> library(lme4)
>> ## fit model
>> fm0 <- lmer(Reaction ~ Days + (1 | Subject), sleepstudy)
>> library(blme)
>> fm1 <- blmer(Reaction ~ Days + (1 | Subject), sleepstudy,
>>         cov.prior=NULL,
>>         resid.prior=point(0.0001))
>>
>> sumfun <- function(fm) {
>>      c(sd_Subject=unname(sqrt(unlist(VarCorr(fm)))),
>>        sd_Residual=sigma(fm),
>>        REMLdev=unname(deviance(fm)))
>>
>> }
>> cfun <- function(sigma=1) {
>>      fm <- blmer(Reaction ~ Days + (1 | Subject), sleepstudy,
>>                  cov.prior=NULL,
>>                  resid.prior=point(sigma))
>>      sumfun(fm)
>> }
>> svec <- c(10^seq(-5,2,by=0.25),20:60)
>> sigmaprof <- rbind(sumfun(fm0),t(sapply(svec,cfun)))
>>
>> sigmaprof <- as.data.frame(sigmaprof[order(sigmaprof[,"sd_Residual"]),])
>> par(las=1,bty="l")
>>
>> ## REML deviance vs. residual value -- LOG scale
>> plot(REMLdev~sd_Residual,data=sigmaprof,type="b",log="xy")
>> abline(v=sigma(fm0),col=2)
>> ## leave out last point to improve scaling
>> plot(sd_Subject~sd_Residual,data=sigmaprof[-30,],type="b",log="x")
>> abline(v=sigma(fm0),col=2)
>>
>> plot(REMLdev~sd_Residual,data=sigmaprof,
>>      subset=REMLdev-min(REMLdev)<30,type="b",log="xy")
>>
>> Compare with the likelihood profile:
>>
>> pp <- profile(fm1)
>> xyplot(pp)
>>
>>
>> There *might* also be a way to do this by digging into the
>> profiling machinery a bit more.
>>
>>    As always, corrections and improvements are welcome.
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From vjd4 at nyu.edu  Wed Feb 12 00:48:22 2014
From: vjd4 at nyu.edu (Vincent Dorie)
Date: Tue, 11 Feb 2014 18:48:22 -0500
Subject: [R-sig-ME] lme4: constrain sigma to 0
In-Reply-To: <52FAAFBA.1030304@auckland.ac.nz>
References: <528E4605.5060607@uni-mannheim.de> <528E637D.2070302@gmail.com>
	<CAGG0PdBu8yHYpHafCjmr0Jx_yB-LHqzVX13FbGnRUS-OOBypNQ@mail.gmail.com>
	<52FAAFBA.1030304@auckland.ac.nz>
Message-ID: <02325EDF-88B5-4F9F-A7C0-8079A04D672B@nyu.edu>

Any chance someone could write out the specifics of such a model? If I can wrap my head around it and its not too hard, I could try to throw it into blme.

On Feb 11, 2014, at 6:18 PM, Rolf Turner wrote:

> 
> Several millennia ago I put in a feature-request that the facility for constraining sigma to 0 be added.  So far, as far as I can tell, it hasn't been.
> 
> Apparently it is (very) difficult to add such a constraint due to the way that lmer approaches the maximization of the likelihood.  (This, rather than any intrinsic mathematical block to such a constraint.)
> 
> I wanted to be able to fit a fairly simple repeated measures model with the covariance over time being an "arbitrary" n-x-n positive definite matrix (where "n" is the number of time points).  This can't be done
> using lmer() unless one can constrain the "overall" variance to be zero,
> otherwise the diagonal of the covariance matrix is un-identifiable.
> 
> Since one cannot constrain sigma to be 0, one can't fit this model with lmer().  Bummer.
> 
> cheers,
> 
> Rolf Turner
> 
> On 12/02/14 11:25, Asaf Weinstein wrote:
>> Dear lme4 community (and Ben),
>> 
>> I am interested in obtaining estimates for the usual linear model but with
>> a known/ fixed value of residual variance.
>> When searching for an answer in the forum I came across this question by
>> Maya (machunsky at uni-mannheim.de), and it looks like there still isn't a
>> built-in method to obtain MLEs under constraints. Is that correct?
>> 
>> Thank you so much,
>> Asaf
>> 
>> 
>> On 21 November 2013 14:48, Ben Bolker <bbolker at gmail.com> wrote:
>> 
>>> On 13-11-21 12:42 PM, Maya Machunsky wrote:
>>>>  Dear Dr. Bolker,
>>>> 
>>>> I am using lmer to estimate fixed effects for exerimental data (in
>>>> Psychology). At the moment I am trying to run a model in which the
>>>> residual variance is fixed to 0 and I am wondering whether it is
>>>> possible to run such a model with lmer. I found messages from 2011 (a
>>>> discussion between you and Rolf Turner) according to which it was not
>>>> possible to constrain sigma to 0. Now I am asking myself whether it is
>>>> now with a new version of the lme4 package possible.
>>>> 
>>>> Thanks a lot in advance and
>>>> best regards,
>>>> Maya Machunsky
>>> 
>>>   No, still not possible (or at least not easy), because the
>>> residual variance is profiled out, so it doesn't enter directly
>>> into the calculations -- it's not a parameter you can set.
>>> 
>>>   You can sort of achieve this using blmer to set a 'point' prior
>>> on sigma (i.e. fix it at a specific value) -- but you can't set the
>>> residual variance to *exactly* zero.
>>> 
>>> library(lme4)
>>> ## fit model
>>> fm0 <- lmer(Reaction ~ Days + (1 | Subject), sleepstudy)
>>> library(blme)
>>> fm1 <- blmer(Reaction ~ Days + (1 | Subject), sleepstudy,
>>>        cov.prior=NULL,
>>>        resid.prior=point(0.0001))
>>> 
>>> sumfun <- function(fm) {
>>>     c(sd_Subject=unname(sqrt(unlist(VarCorr(fm)))),
>>>       sd_Residual=sigma(fm),
>>>       REMLdev=unname(deviance(fm)))
>>> 
>>> }
>>> cfun <- function(sigma=1) {
>>>     fm <- blmer(Reaction ~ Days + (1 | Subject), sleepstudy,
>>>                 cov.prior=NULL,
>>>                 resid.prior=point(sigma))
>>>     sumfun(fm)
>>> }
>>> svec <- c(10^seq(-5,2,by=0.25),20:60)
>>> sigmaprof <- rbind(sumfun(fm0),t(sapply(svec,cfun)))
>>> 
>>> sigmaprof <- as.data.frame(sigmaprof[order(sigmaprof[,"sd_Residual"]),])
>>> par(las=1,bty="l")
>>> 
>>> ## REML deviance vs. residual value -- LOG scale
>>> plot(REMLdev~sd_Residual,data=sigmaprof,type="b",log="xy")
>>> abline(v=sigma(fm0),col=2)
>>> ## leave out last point to improve scaling
>>> plot(sd_Subject~sd_Residual,data=sigmaprof[-30,],type="b",log="x")
>>> abline(v=sigma(fm0),col=2)
>>> 
>>> plot(REMLdev~sd_Residual,data=sigmaprof,
>>>     subset=REMLdev-min(REMLdev)<30,type="b",log="xy")
>>> 
>>> Compare with the likelihood profile:
>>> 
>>> pp <- profile(fm1)
>>> xyplot(pp)
>>> 
>>> 
>>> There *might* also be a way to do this by digging into the
>>> profiling machinery a bit more.
>>> 
>>>   As always, corrections and improvements are welcome.
>>> 
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> 
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From jake987722 at hotmail.com  Wed Feb 12 01:28:36 2014
From: jake987722 at hotmail.com (Jake Westfall)
Date: Tue, 11 Feb 2014 17:28:36 -0700
Subject: [R-sig-ME] lme4: constrain sigma to 0
In-Reply-To: <02325EDF-88B5-4F9F-A7C0-8079A04D672B@nyu.edu>
References: <528E4605.5060607@uni-mannheim.de> <528E637D.2070302@gmail.com>,
	<CAGG0PdBu8yHYpHafCjmr0Jx_yB-LHqzVX13FbGnRUS-OOBypNQ@mail.gmail.com>,
	<52FAAFBA.1030304@auckland.ac.nz>,
	<02325EDF-88B5-4F9F-A7C0-8079A04D672B@nyu.edu>
Message-ID: <BAY172-W1522082EAFE8A8CBA211C2CB920@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140211/168e5bfe/attachment.pl>

From r.turner at auckland.ac.nz  Tue Feb 11 23:49:46 2014
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Wed, 12 Feb 2014 11:49:46 +1300
Subject: [R-sig-ME] lme4: constrain sigma to 0
In-Reply-To: <02325EDF-88B5-4F9F-A7C0-8079A04D672B@nyu.edu>
References: <528E4605.5060607@uni-mannheim.de> <528E637D.2070302@gmail.com>
	<CAGG0PdBu8yHYpHafCjmr0Jx_yB-LHqzVX13FbGnRUS-OOBypNQ@mail.gmail.com>
	<52FAAFBA.1030304@auckland.ac.nz>
	<02325EDF-88B5-4F9F-A7C0-8079A04D672B@nyu.edu>
Message-ID: <52FAA90A.9000400@auckland.ac.nz>


I can write out the specifics without much difficulty.  I want to be 
careful about being absolutely clear in what I say, but, so it will take 
me a little while.  I will get back to you fairly soon.

Thanks.

cheers,

Rolf Turner

On 12/02/14 12:48, Vincent Dorie wrote:
> Any chance someone could write out the specifics of such a model? If I can wrap my head around it and its not too hard, I could try to throw it into blme.
>
> On Feb 11, 2014, at 6:18 PM, Rolf Turner wrote:
>
>>
>> Several millennia ago I put in a feature-request that the facility for constraining sigma to 0 be added.  So far, as far as I can tell, it hasn't been.
>>
>> Apparently it is (very) difficult to add such a constraint due to the way that lmer approaches the maximization of the likelihood.  (This, rather than any intrinsic mathematical block to such a constraint.)
>>
>> I wanted to be able to fit a fairly simple repeated measures model with the covariance over time being an "arbitrary" n-x-n positive definite matrix (where "n" is the number of time points).  This can't be done
>> using lmer() unless one can constrain the "overall" variance to be zero,
>> otherwise the diagonal of the covariance matrix is un-identifiable.
>>
>> Since one cannot constrain sigma to be 0, one can't fit this model with lmer().  Bummer.
>>
>> cheers,
>>
>> Rolf Turner
>>
>> On 12/02/14 11:25, Asaf Weinstein wrote:
>>> Dear lme4 community (and Ben),
>>>
>>> I am interested in obtaining estimates for the usual linear model but with
>>> a known/ fixed value of residual variance.
>>> When searching for an answer in the forum I came across this question by
>>> Maya (machunsky at uni-mannheim.de), and it looks like there still isn't a
>>> built-in method to obtain MLEs under constraints. Is that correct?
>>>
>>> Thank you so much,
>>> Asaf
>>>
>>>
>>> On 21 November 2013 14:48, Ben Bolker <bbolker at gmail.com> wrote:
>>>
>>>> On 13-11-21 12:42 PM, Maya Machunsky wrote:
>>>>>   Dear Dr. Bolker,
>>>>>
>>>>> I am using lmer to estimate fixed effects for exerimental data (in
>>>>> Psychology). At the moment I am trying to run a model in which the
>>>>> residual variance is fixed to 0 and I am wondering whether it is
>>>>> possible to run such a model with lmer. I found messages from 2011 (a
>>>>> discussion between you and Rolf Turner) according to which it was not
>>>>> possible to constrain sigma to 0. Now I am asking myself whether it is
>>>>> now with a new version of the lme4 package possible.
>>>>>
>>>>> Thanks a lot in advance and
>>>>> best regards,
>>>>> Maya Machunsky
>>>>
>>>>    No, still not possible (or at least not easy), because the
>>>> residual variance is profiled out, so it doesn't enter directly
>>>> into the calculations -- it's not a parameter you can set.
>>>>
>>>>    You can sort of achieve this using blmer to set a 'point' prior
>>>> on sigma (i.e. fix it at a specific value) -- but you can't set the
>>>> residual variance to *exactly* zero.
>>>>
>>>> library(lme4)
>>>> ## fit model
>>>> fm0 <- lmer(Reaction ~ Days + (1 | Subject), sleepstudy)
>>>> library(blme)
>>>> fm1 <- blmer(Reaction ~ Days + (1 | Subject), sleepstudy,
>>>>         cov.prior=NULL,
>>>>         resid.prior=point(0.0001))
>>>>
>>>> sumfun <- function(fm) {
>>>>      c(sd_Subject=unname(sqrt(unlist(VarCorr(fm)))),
>>>>        sd_Residual=sigma(fm),
>>>>        REMLdev=unname(deviance(fm)))
>>>>
>>>> }
>>>> cfun <- function(sigma=1) {
>>>>      fm <- blmer(Reaction ~ Days + (1 | Subject), sleepstudy,
>>>>                  cov.prior=NULL,
>>>>                  resid.prior=point(sigma))
>>>>      sumfun(fm)
>>>> }
>>>> svec <- c(10^seq(-5,2,by=0.25),20:60)
>>>> sigmaprof <- rbind(sumfun(fm0),t(sapply(svec,cfun)))
>>>>
>>>> sigmaprof <- as.data.frame(sigmaprof[order(sigmaprof[,"sd_Residual"]),])
>>>> par(las=1,bty="l")
>>>>
>>>> ## REML deviance vs. residual value -- LOG scale
>>>> plot(REMLdev~sd_Residual,data=sigmaprof,type="b",log="xy")
>>>> abline(v=sigma(fm0),col=2)
>>>> ## leave out last point to improve scaling
>>>> plot(sd_Subject~sd_Residual,data=sigmaprof[-30,],type="b",log="x")
>>>> abline(v=sigma(fm0),col=2)
>>>>
>>>> plot(REMLdev~sd_Residual,data=sigmaprof,
>>>>      subset=REMLdev-min(REMLdev)<30,type="b",log="xy")
>>>>
>>>> Compare with the likelihood profile:
>>>>
>>>> pp <- profile(fm1)
>>>> xyplot(pp)
>>>>
>>>>
>>>> There *might* also be a way to do this by digging into the
>>>> profiling machinery a bit more.
>>>>
>>>>    As always, corrections and improvements are welcome.
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>
>>> 	[[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From hans.ekbrand at gmail.com  Wed Feb 12 01:58:51 2014
From: hans.ekbrand at gmail.com (Hans Ekbrand)
Date: Wed, 12 Feb 2014 01:58:51 +0100
Subject: [R-sig-ME] Converging mixed models with large number of random
 coefficients
In-Reply-To: <CANjXykuTY2ox92tEbHDuv6dvyjngxMLn8x_y+8YoHBw=QAQeKA@mail.gmail.com>
References: <CANjXykuTY2ox92tEbHDuv6dvyjngxMLn8x_y+8YoHBw=QAQeKA@mail.gmail.com>
Message-ID: <20140212005851.GA3490@hans>

On Mon, Feb 10, 2014 at 11:48:06PM -0200, Thomas Schroder wrote:
>  I realize that one approach to this problem would be to reduce the number
> of random coefficients that account for individual tree variation (Li et
> al., 2012). But since individual tree heights and diameters are only
> obtainable from each individual tree, I guess that all coefficients (which
> are associated to these variables) should be random (Gelman, 2007). I
> wonder if there is any problem with my interpretation of the mixed models
> theory?

Perhaps I have misunderstood your mail (I am not familiar with nlme),
but from your description above I think you have misunderstood what
random effects do.

That the tree is random is not an argument for making a variable like
diameter random with the argument that it is a property of the tree.

If you want a general effect (a single estime) of how diameter affects
the outcome, you should have diameter as a fixed effect in your model.

If you want an estimate - per tree - of how diameter affects the
outcome for that particular tree you will need variation of diameter
within that particular tree.

I think you want the former - after all, science tends to be about the
universal stuff, not the particulars - and in that case you should
make diameter a fixed effect, and only keep the tree-id variable as
random.

Diameter was only one example, the same goes for the other variables
as well.



kind regards,

Hans Ekbrand


From slu at ccsr.uchicago.edu  Wed Feb 12 04:37:16 2014
From: slu at ccsr.uchicago.edu (Stuart Luppescu)
Date: Tue, 11 Feb 2014 21:37:16 -0600
Subject: [R-sig-ME] lme4: constrain sigma to 0
In-Reply-To: <52FAA90A.9000400@auckland.ac.nz>
References: <528E4605.5060607@uni-mannheim.de> <528E637D.2070302@gmail.com>
	<CAGG0PdBu8yHYpHafCjmr0Jx_yB-LHqzVX13FbGnRUS-OOBypNQ@mail.gmail.com>
	<52FAAFBA.1030304@auckland.ac.nz>
	<02325EDF-88B5-4F9F-A7C0-8079A04D672B@nyu.edu>
	<52FAA90A.9000400@auckland.ac.nz>
Message-ID: <1392176236.20925.5.camel@localhost>

On Wed, 2014-02-12 at 11:49 +1300, Rolf Turner wrote:
> I can write out the specifics without much difficulty.  I want to be 
> careful about being absolutely clear in what I say, but, so it will
> take 
> me a little while.  I will get back to you fairly soon.

Can this be done flexibly so one could set the residual variance to any
value? I would like to fix the residual variance to 1.0 to adjust for
variation in measurement error in the observations as explained in
Raudenbush and Bryk 2002 p.354ff.

-- 
Stuart Luppescu <slu at ccsr.uchicago.edu>


From Steve.Candy at aad.gov.au  Wed Feb 12 06:00:16 2014
From: Steve.Candy at aad.gov.au (Steve Candy)
Date: Wed, 12 Feb 2014 16:00:16 +1100
Subject: [R-sig-ME] lme4: constrain sigma to 0 [SEC=Unclassified]
Message-ID: <0D892F0FB767BC498AF747F72935E9534AB24D0C@KGS-EXMBS3.AAD.GOV.AU>

One approach is to be able to constrain sigma to 1 where this fits into a strategy of estimatiing all higher-level variance components as "gammas" which are the ratios of these variance components to sigma^2. Sigma can also be estimated or fixed to a constant.

This makes sense when lmer or lme is used within a PQL algorithm for fitting a GLMM with binomial response.

I am not sure setting sigma to zero is useful since there will always be some level of units-level variance but it would be good to have the option of constraining it to some positive nonzero value.
___________________________________________________________________________

    Australian Antarctic Division - Commonwealth of Australia
IMPORTANT: This transmission is intended for the addressee only. If you are not the
intended recipient, you are notified that use or dissemination of this communication is
strictly prohibited by Commonwealth law. If you have received this transmission in error,
please notify the sender immediately by e-mail or by telephoning +61 3 6232 3209 and
DELETE the message.
        Visit our web site at http://www.antarctica.gov.au/
___________________________________________________________________________


From David.Duffy at qimr.edu.au  Wed Feb 12 09:37:16 2014
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Wed, 12 Feb 2014 18:37:16 +1000
Subject: [R-sig-ME] lme4: constrain sigma to 0 [SEC=Unclassified]
In-Reply-To: <0D892F0FB767BC498AF747F72935E9534AB24D0C@KGS-EXMBS3.AAD.GOV.AU>
References: <0D892F0FB767BC498AF747F72935E9534AB24D0C@KGS-EXMBS3.AAD.GOV.AU>
Message-ID: <alpine.LMD.2.00.1402121829280.26480@orpheus.qimr.edu.au>

For those who would like to fit these models, you could muck around with 
the OpenMx package, which allows you to fix or constrain any parameters 
you like ;)  To fix the residuals to zero, change free to FALSE in the
"residual variances" bit.  Parameters with the same "labels" are 
automatically constrained equal.

#
# Based on an example in the OpenMX documentation
#
require(lme4)
data(sleepstudy)
sleep2 <- reshape(sleepstudy, direction="wide", idvar="Subject", 
timevar="Days")
rownames(sleep2) <- sleep2$Subject
sleep2 <- sleep2[, -1]
names(sleep2) <- gsub("\\.","",names(sleep2)) # Mx no like "."

require(OpenMx)
growthCurveModel <- mxModel("Sleepstudy as Linear Growth Curve Model",
     type="RAM",
     mxData(
         observed=sleep2,
         type="raw"
     ),
     manifestVars=names(sleep2),
     latentVars=c("intercept","slope"),
     # residual variances
     mxPath(
         from=names(sleep2),
         arrows=2,
         free=TRUE,
         values = rep(1, ncol(sleep2)),
         labels=rep("residual", ncol(sleep2))
     ),
     # latent variances and covariance
     mxPath(
         from=c("intercept","slope"),
         arrows=2,
                 connect="unique.pairs",
         free=TRUE,
         values=c(1, 1, 1),
         labels=c("vari", "cov", "vars")
     ),
     # intercept loadings
     mxPath(
         from="intercept",
         to=names(sleep2),
         arrows=1,
         free=FALSE,
         values = rep(1, ncol(sleep2))
     ),
     # slope loadings
     mxPath(
         from="slope",
         to=names(sleep2),
         arrows=1,
         free=FALSE,
         values=seq(0,9)
                              ),
     # manifest means
     mxPath(from="one",
         to=names(sleep2),
         arrows=1,
         free=FALSE,
         values = rep(0, ncol(sleep2))
     ),
     # latent means
     mxPath(from="one",
         to=c("intercept", "slope"),
         arrows=1,
         free=TRUE,
         values=c(1, 1),
         labels=c("meani", "means")
     )
) # close model

growthCurveFit <- mxRun(growthCurveModel)
print(summary(growthCurveFit))
print(growthCurveFit at output$estimate)



| David Duffy (MBBS PhD)
| email: David.Duffy at qimrberghofer.edu.au  ph: INT+61+7+3362-0217 fax: -0101
| Genetic Epidemiology, QIMR Berghofer Institute of Medical Research
| 300 Herston Rd, Brisbane, Queensland 4006, Australia  GPG 4D0B994A


From vjd4 at nyu.edu  Wed Feb 12 14:34:59 2014
From: vjd4 at nyu.edu (Vincent Dorie)
Date: Wed, 12 Feb 2014 08:34:59 -0500
Subject: [R-sig-ME] lme4: constrain sigma to 0
In-Reply-To: <alpine.LMD.2.00.1402121829280.26480@orpheus.qimr.edu.au>
References: <0D892F0FB767BC498AF747F72935E9534AB24D0C@KGS-EXMBS3.AAD.GOV.AU>
	<alpine.LMD.2.00.1402121829280.26480@orpheus.qimr.edu.au>
Message-ID: <CA325975-187F-4CAD-83E9-0C09921FAE39@nyu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140212/ba05558f/attachment.pl>

From wolfgang.viechtbauer at maastrichtuniversity.nl  Wed Feb 12 17:54:07 2014
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (STAT))
Date: Wed, 12 Feb 2014 17:54:07 +0100
Subject: [R-sig-ME] lme4: constrain sigma to 0
In-Reply-To: <CA325975-187F-4CAD-83E9-0C09921FAE39@nyu.edu>
References: <0D892F0FB767BC498AF747F72935E9534AB24D0C@KGS-EXMBS3.AAD.GOV.AU>
	<alpine.LMD.2.00.1402121829280.26480@orpheus.qimr.edu.au>
	<CA325975-187F-4CAD-83E9-0C09921FAE39@nyu.edu>
Message-ID: <077E31A57DA26E46AB0D493C9966AC730D9D72340F@UM-MAIL4112.unimaas.nl>

If you are really masochistic, you could trick the rma.mv() function from the 'metafor' package to fit such a model (in meta-analyses, you fix the variances of the residuals to known values -- and here, we could set those values to 0).

As an example, let me follow up on what Rolf Turner wanted to do:

> I wanted to be able to fit a fairly simple repeated measures model with
> the covariance over time being an "arbitrary" n-x-n positive definite
> matrix (where "n" is the number of time points).  This can't be done
> using lmer() unless one can constrain the "overall" variance to be zero,
> otherwise the diagonal of the covariance matrix is un-identifiable.

##################################################################

library(metafor)
library(nlme)
library(lme4)

data(Orthodont)

V0 <- rep(0, nrow(Orthodont))

res1 <- rma.mv(distance ~ age, V=V0, random = ~ age | Subject, struct="UN", data=Orthodont, control=list(optimizer="nlminb", REMLf=FALSE))
summary(res1)

res2 <- lme(distance ~ age, random = ~ factor(age) - 1 | Subject, data=Orthodont)
summary(res2)

res3 <- gls(distance ~ age, correlation = corSymm(form = ~ 1 | Subject), weights = varIdent(form = ~ 1 | age), data=Orthodont)
summary(res3)

res4 <- lmer(distance ~ age + (factor(age) - 1 | Subject), data=Orthodont)
summary(res4)

# Note that lme() and lmer() also fit this model, but here the residual variance is being estimated. 
# As mentioned earlier by Rolf Turner, that model is actually over-parameterized. Marginally, you end 
# up with the same fit though:

logLik(res1)
logLik(res2)
logLik(res3)
logLik(res4)

# So, lme() and lmer() are splitting up the variance along the diagonal into variance due to the random 
# effects and residual variance. Added up, this yields the same values as with rma.mv() and gls():

round(res1$tau2, 4)
round(as.numeric(VarCorr(res2)[1:4,1]) + res2$sigma^2, 4)
round(unname(c(res3$sigma^2, (res3$sigma * coef(res3$modelStruct, unconstrained=FALSE)[7:9])^2)), 4)
round(unname(diag(VarCorr(res4)[[1]]) + sigma(res4)^2), 4)

# But the split is arbitrary and depends on how the optimization is done:

res2$sigma
sigma(res4)

##################################################################

A few more notes:

1) I had to switch to 'nlminb' for rma.mv() since the default ('optim' with 'Nelder-Mead') did not converge and gave strange results when it did (by increasing the number of iterations).

2) With REMLf=FALSE, a particular constant term is left out from the REML likelihood that is also not included in the likelihood computation for the other functions.

3) rma.mv() is slow as it was never intended for this. But it does work. In other cases, it may not.

Best,
Wolfgang

--   
Wolfgang Viechtbauer, Ph.D., Statistician   
Department of Psychiatry and Psychology   
School for Mental Health and Neuroscience   
Faculty of Health, Medicine, and Life Sciences   
Maastricht University, P.O. Box 616 (VIJV1)   
6200 MD Maastricht, The Netherlands   
+31 (43) 388-4170 | http://www.wvbauer.com   


> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-
> models-bounces at r-project.org] On Behalf Of Vincent Dorie
> Sent: Wednesday, February 12, 2014 14:35
> To: r-sig-mixed-models
> Subject: Re: [R-sig-ME] lme4: constrain sigma to 0
> 
> It was also in an email from Ben a few messages up the chain that blme
> works for this, just not for sigma = 0. e.g.
> 
>   blmer(formula, data, cov.prior = NULL, resid.prior = point(1.0))
> 
> 
> On Feb 12, 2014, at 3:37 AM, David Duffy wrote:
> 
> > For those who would like to fit these models, you could muck around
> with the OpenMx package, which allows you to fix or constrain any
> parameters you like ;)  To fix the residuals to zero, change free to
> FALSE in the
> > "residual variances" bit.  Parameters with the same "labels" are
> automatically constrained equal.
> >
> > #
> > # Based on an example in the OpenMX documentation
> > #
> > require(lme4)
> > data(sleepstudy)
> > sleep2 <- reshape(sleepstudy, direction="wide", idvar="Subject",
> timevar="Days")
> > rownames(sleep2) <- sleep2$Subject
> > sleep2 <- sleep2[, -1]
> > names(sleep2) <- gsub("\\.","",names(sleep2)) # Mx no like "."
> >
> > require(OpenMx)
> > growthCurveModel <- mxModel("Sleepstudy as Linear Growth Curve Model",
> >    type="RAM",
> >    mxData(
> >        observed=sleep2,
> >        type="raw"
> >    ),
> >    manifestVars=names(sleep2),
> >    latentVars=c("intercept","slope"),
> >    # residual variances
> >    mxPath(
> >        from=names(sleep2),
> >        arrows=2,
> >        free=TRUE,
> >        values = rep(1, ncol(sleep2)),
> >        labels=rep("residual", ncol(sleep2))
> >    ),
> >    # latent variances and covariance
> >    mxPath(
> >        from=c("intercept","slope"),
> >        arrows=2,
> >                connect="unique.pairs",
> >        free=TRUE,
> >        values=c(1, 1, 1),
> >        labels=c("vari", "cov", "vars")
> >    ),
> >    # intercept loadings
> >    mxPath(
> >        from="intercept",
> >        to=names(sleep2),
> >        arrows=1,
> >        free=FALSE,
> >        values = rep(1, ncol(sleep2))
> >    ),
> >    # slope loadings
> >    mxPath(
> >        from="slope",
> >        to=names(sleep2),
> >        arrows=1,
> >        free=FALSE,
> >        values=seq(0,9)
> >                             ),
> >    # manifest means
> >    mxPath(from="one",
> >        to=names(sleep2),
> >        arrows=1,
> >        free=FALSE,
> >        values = rep(0, ncol(sleep2))
> >    ),
> >    # latent means
> >    mxPath(from="one",
> >        to=c("intercept", "slope"),
> >        arrows=1,
> >        free=TRUE,
> >        values=c(1, 1),
> >        labels=c("meani", "means")
> >    )
> > ) # close model
> >
> > growthCurveFit <- mxRun(growthCurveModel)
> > print(summary(growthCurveFit))
> > print(growthCurveFit at output$estimate)
> >
> >
> >
> > | David Duffy (MBBS PhD)
> > | email: David.Duffy at qimrberghofer.edu.au  ph: INT+61+7+3362-0217 fax:
> -0101
> > | Genetic Epidemiology, QIMR Berghofer Institute of Medical Research
> > | 300 Herston Rd, Brisbane, Queensland 4006, Australia  GPG 4D0B994A
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From corentin.gonthier at univ-savoie.fr  Wed Feb 12 13:25:37 2014
From: corentin.gonthier at univ-savoie.fr (Corentin Gonthier)
Date: Wed, 12 Feb 2014 13:25:37 +0100
Subject: [R-sig-ME] Multivariate multilevel analysis with nlme
Message-ID: <52FB6841.8040403@univ-savoie.fr>

Dear R and statistics experts,

I am currently trying to analyze a dataset which I think requires a 
multivariate multilevel analysis. I am unsure both of the appropriate 
model and of how to fit it with R. Any insight about any aspect of the 
analysis (be it the multivariate part, the multilevel part, the R 
implementation...) will be extremely appreciated - even a cursory glance 
to tell me if something looks wrong would be a big help.

I come from a psychology background and I am new to multilevel modeling; 
despite reading a lot of material about this type of analyses my 
understanding of the math is simply too superficial to be certain about 
what I am doing. I would really need your help to check whether my 
analysis is correct: if you're reading this, you are definitely more 
skillful than I am!

***

Our study is about whether the architectural design of a clinic will 
influence symptoms of a pathology for permanent residents in this clinic.
We have collected data on 13 symptoms for 8 patients per clinic in 21 
clinics.

Our hypothesis is that overall, architecture has an effect on 
symptomatology. There is a clinic-level IV (architecture) and we also 
control for a patient-level IV (medication). We expect a cross-level 
interaction.

The 13 symptom DVs are correlated +.20 on average, which I think means a 
multivariate analysis is appropriate. It also seems necessary to 
mitigate the risk of type I errors.

I use nlme because as far as I understand, lmer doesn't have the option 
to correlate residuals as necessary for a multivariate analysis.

***

To run the multivariate analysis with nlme I have standardized my DVs, 
stacked the 13 DVs in a single column, and added a dummy variable to 
flag which row corresponds to which symptom. This dummy variable is 
categorical rather than continuous so as to prevent R from running a 
growth-curve-like analysis.

The data looks like this:

Clinic    Patient    Symptom    Score        Medication Architecture
1            1                EP1            -0.12               1     
               3.2
1            1                EP2            0.11 1                     3.2
1            1                EP3            0.13 1                     3.2
1            2                EP1            0.56 4                     3.2
1            2                EP2            0.67 4                     3.2
1            2                EP3            0.23 4                     3.2
2            3                EP1            -0.22 3                     5.1
2            3                EP2            0.25 3                     5.1
2            3                EP3            0.14 3                     5.1
2            4                EP1            0.78 6                     5.1
2            4                EP2            0.89 6                     5.1
2            4                EP3            -0.11 6                     5.1


***

- To run the analysis as multivariate, I use both "Symptom" and the 
"Symptom:Architecture" interaction as IVs and I remove the intercept in 
both the fixed and stochastic parts of the model. I do not include the 
main effect of "Architecture" as an IV.

- The effect of patient-level variables should be the same within all 
clinics, so there is no random effect for "Medication".

- I do not want to constrain equality between the effect of 
"Architecture" on the different Symptoms.

- Due to the multivariate nature of the analysis, I also expect the 
residuals to be correlated, with different correlations between the 13 
different Symptoms; therefore I specify the covariance structure of 
residuals as "corSymm" (non-zero but unstructured and not invariant 
across Symptoms, if I get this correctly). This gives :
     correlation = corSymm(form = ~1|Patient/Clinic)

- I also expect heteroscedasticity between the different symptoms (there 
should be more variance on certain Symptoms), so I add the following 
option :
     weights = varIdent(form = ~1|Symptom).

***

The model I come up with in nlme is therefore the following :

model1 = lme(fixed = Score ~ Symptom + Medication:Symptom + 
Architecture:symptom + Medication:Architecture:Symptom - 1,
+ random = ~ Symptom - 1 | Patient/clinic,
+ correlation = corSymm(form= ~ 1|Patient/Clinic),
+ weights=varIdent(form= ~ 1|Symptom)
+ method = "ML")

In order to test the effect of the architectural variable, I then 
compare this model to the following constrained model, dropping all the 
terms related to architecture:

model2 = lme(fixed = Score ~ Symptom + Medication:Symptom - 1,
+ random = ~ Symptom - 1 | Patient/clinic,
+ correlation = corSymm(form= ~ 1|Patient/Clinic),
+ weights=varIdent(form= ~ 1|Symptom)
+ method = "ML")


I compare the two models with the command
     anova(model1, model2)
and look at the likelihood ratio test. I have read that this is not the 
best way to test the significance of fixed effects, but I don't see 
another way to run the analysis as multivariate.


Overall, does this look correct to you?

Thank you so much for your invaluable insight!

Best wishes,

--Corentin Gonthier

-- 
Ph.D. Student in cognitive neuroscience
Department of Psychology and NeuroCognition
University of Grenoble
France


From jackiewood7 at gmail.com  Thu Feb 13 00:04:22 2014
From: jackiewood7 at gmail.com (Jackie Wood)
Date: Wed, 12 Feb 2014 18:04:22 -0500
Subject: [R-sig-ME] Modelling heterogeneous variances for an interaction
	term in MCMCglmm?
Message-ID: <CAOxxGRnK+m-mJ4zYQpPXEojPPzbd8dHiXNVvYqXuMdCi8eRx8g@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140212/5c3a63d1/attachment.pl>

From juwb08 at hampshire.edu  Thu Feb 13 00:10:35 2014
From: juwb08 at hampshire.edu (juwb08 at hampshire.edu)
Date: Wed, 12 Feb 2014 18:10:35 -0500
Subject: [R-sig-ME] new version of lme4 - Log-Likelihood of model returned
	as "-Inf"
In-Reply-To: <20140212180246.Horde.IBHmX-rBk2lS_-2W09gzmWA@webmail.hampshire.edu>
References: <20140212180246.Horde.IBHmX-rBk2lS_-2W09gzmWA@webmail.hampshire.edu>
Message-ID: <20140212181035.Horde.DeXeC-rBk2lS_-9rZPNzybA@webmail.hampshire.edu>


Hi All,

I was working with Poisson glmms in lme4 last year in 2013 using  
Windows XP, lme4 version 0.999375-16 (2008-06-23) and R version 2.15.1  
(2012-06-22), evaluating a set of candidate models with AIC to select  
the best ones.

This worked well until I updated lme4 and R last month.

I now updated to R version 3.0.2 (2013-09-25) -- "Frisbee Sailing" and  
lme4 version 1.0-5 (2013-10-24), still Windows XP, and am running the  
same models again on the same data, and lme4 now returns "-Inf" for  
the log likelihood, so it can't calculate AIC.

Using the previous versions, I got stable values for Log-Likelihoods  
and calculated AICs with them.

Attached is a sample of the dataset and the code I am using.

How can I get a Log-Likelihood estimate with the current versions of  
lme4 and R?

Thanks in advance.
Justin Baldwin

Hampshire College, MA, USA
-------------- next part --------------
setwd("C:/Documents and Settings/Justin/Desktop/Research/REU2")

library(lme4)

shits<-read.csv("PretEx-EXAMPLE.csv", header=T)
shits2<-subset(shits, GRT_min!="NA")
shits2$BatID<-as.factor(shits2$BatID)
shits2$PlantID_factor<-as.factor(shits2$PlantID_factor)

summary(glmer(GRT_min~Treatment_Factor+Mass_Consumed+FedByHand+(1|BatID/PlantID_factor), data=shits2, family="poisson", REML=F))


From juwb08 at hampshire.edu  Thu Feb 13 00:14:11 2014
From: juwb08 at hampshire.edu (juwb08 at hampshire.edu)
Date: Wed, 12 Feb 2014 18:14:11 -0500
Subject: [R-sig-ME] new version of lme4 - Log-Likelihood of model
 returned as "-Inf"
In-Reply-To: <20140212181035.Horde.DeXeC-rBk2lS_-9rZPNzybA@webmail.hampshire.edu>
References: <20140212180246.Horde.IBHmX-rBk2lS_-2W09gzmWA@webmail.hampshire.edu>
	<20140212181035.Horde.DeXeC-rBk2lS_-9rZPNzybA@webmail.hampshire.edu>
Message-ID: <20140212181411.Horde.0Xcna-rBk2lS-ABDwBYD3FA@webmail.hampshire.edu>


... the dataset failed to attach, so here it is.
Sorry!

Quoting juwb08 at hampshire.edu:

> Hi All,
>
> I was working with Poisson glmms in lme4 last year in 2013 using  
> Windows XP, lme4 version 0.999375-16 (2008-06-23) and R version  
> 2.15.1 (2012-06-22), evaluating a set of candidate models with AIC  
> to select the best ones.
>
> This worked well until I updated lme4 and R last month.
>
> I now updated to R version 3.0.2 (2013-09-25) -- "Frisbee Sailing"  
> and lme4 version 1.0-5 (2013-10-24), still Windows XP, and am  
> running the same models again on the same data, and lme4 now returns  
> "-Inf" for the log likelihood, so it can't calculate AIC.
>
> Using the previous versions, I got stable values for Log-Likelihoods  
> and calculated AICs with them.
>
> Attached is a sample of the dataset and the code I am using.
>
> How can I get a Log-Likelihood estimate with the current versions of  
> lme4 and R?
>
> Thanks in advance.
> Justin Baldwin
>
> Hampshire College, MA, USA



From r.turner at auckland.ac.nz  Thu Feb 13 01:03:05 2014
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Thu, 13 Feb 2014 13:03:05 +1300
Subject: [R-sig-ME] lme4: constrain sigma to 0
In-Reply-To: <02325EDF-88B5-4F9F-A7C0-8079A04D672B@nyu.edu>
References: <528E4605.5060607@uni-mannheim.de> <528E637D.2070302@gmail.com>
	<CAGG0PdBu8yHYpHafCjmr0Jx_yB-LHqzVX13FbGnRUS-OOBypNQ@mail.gmail.com>
	<52FAAFBA.1030304@auckland.ac.nz>
	<02325EDF-88B5-4F9F-A7C0-8079A04D672B@nyu.edu>
Message-ID: <52FC0BB9.3050407@auckland.ac.nz>


Took me a while, but I have managed to write up some specifics of the 
problem that I had in mind.  The gremlins seem to have changed things 
since the last time I played around with this stuff, and what I am now
getting differs from what I recollect.  However there is still a bit of
a problem.

I communicated with both Doug Bates and Ben Bolker on this issue, but 
cannot now for the life of me find any record of the emails that went 
back and forth.  And I keep ***everything***.  (Always the way; 
everything but what you want is available.)

Anyhow --- I have attached what I hope is a clear write-up in pdf format 
and a script to demonstrate what goes on using simulated data.

I hope these get through ...

cheers,

Rolf Turner


On 12/02/14 12:48, Vincent Dorie wrote:

> Any chance someone could write out the specifics of such a model? If I
can wrap my head around it and its not too hard, I could try to throw it
into blme.
>
> On Feb 11, 2014, at 6:18 PM, Rolf Turner wrote:
>
>> Several millennia ago I put in a feature-request that the facility for
>> constraining sigma to 0 be added. So far, as far as I can tell, it
>> hasn't been.
>>
>> Apparently it is (very) difficult to add such a constraint due to
>> the way that lmer approaches the maximization of the likelihood.
>> (This, rather than any intrinsic mathematical block to such a
>> constraint.)
>>
>> I wanted to be able to fit a fairly simple repeated measures model
>> with the covariance over time being an "arbitrary" n-x-n positive
>> definite matrix (where "n" is the number of time points).  This
>> can't be done using lmer() unless one can constrain the "overall"
>> variance to be zero, otherwise the diagonal of the covariance
>> matrix is un-identifiable.
>>
>> Since one cannot constrain sigma to be 0, one can't fit this model
>> with lmer().  Bummer.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: specifics.pdf
Type: application/pdf
Size: 33767 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140213/a166695c/attachment-0001.pdf>
-------------- next part --------------
#
# Script demoScript
#

library(MASS)
library(lme4)

# Generate simulated data.  No attempt is made to make these data
# realistic.

N <- 2000
MU <- c(0,-1,2)
Sigma <- 2*matrix(c(1.00,0.50,0.25,
                    0.50,1.00,0.50,
                    0.25,0.50,1.00),ncol=3)
set.seed(42)
Y <- mvrnorm(N,MU,Sigma)
y <- as.vector(t(Y))
dat.sim <- data.frame(y=y,time=factor(rep(1:3,N)),
                          student=factor(rep(1:N,each=3)))

# Fit with lmer():
fit <- lmer(y ~ time + (time|student),data=dat.sim,REML=TRUE)
V   <- VarCorr(fit)
CM  <- V[[1]]
attributes(CM) <- attributes(CM)["dim"]
sig.hat <- attr(V,"sc")

# Fixed effect estimates:
m <- matrix(c(1,-1,-1,0,1,0,0,0,1),3,3)
print(m%*%apply(Y,2,mean)) # Aggrees with "fit".

# Standard errors:
print(sqrt(diag(CM))) # Agrees with the "Std.dev" column under
                      # "Random effects" from "fit".
# The following agree with each other and with the standard
# errors of the fixed effects from "fit".
print(sqrt(diag(m%*%var(Y)%*%t(m)))/sqrt(N))
print(sqrt(diag(CM + sig.hat^2 * m%*%t(m)))/sqrt(N))

From bbolker at gmail.com  Thu Feb 13 01:26:54 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 12 Feb 2014 19:26:54 -0500
Subject: [R-sig-ME] new version of lme4 - Log-Likelihood of model
 returned as "-Inf"
In-Reply-To: <20140212181411.Horde.0Xcna-rBk2lS-ABDwBYD3FA@webmail.hampshire.edu>
References: <20140212180246.Horde.IBHmX-rBk2lS_-2W09gzmWA@webmail.hampshire.edu>	<20140212181035.Horde.DeXeC-rBk2lS_-9rZPNzybA@webmail.hampshire.edu>
	<20140212181411.Horde.0Xcna-rBk2lS-ABDwBYD3FA@webmail.hampshire.edu>
Message-ID: <52FC114E.4050203@gmail.com>

  Actually, it got stripped by the mailing list software.  Can you post
it somewhere?  (You can also e-mail it to me.)  (Maybe it got censored
because your data frame is called "shits", which I assume is pronounced
"ess-hits" unless these are data on scat samples :-) ?)

 Perhaps you have the Poisson analogue of complete separation, i.e. a
combination of factor levels within which all the responses are zero?
If so, the bglmer function from the blme package might be helpful ...

On 14-02-12 06:14 PM, juwb08 at hampshire.edu wrote:
> 
> ... the dataset failed to attach, so here it is.
> Sorry!
> 
> Quoting juwb08 at hampshire.edu:
> 
>> Hi All,
>>
>> I was working with Poisson glmms in lme4 last year in 2013 using
>> Windows XP, lme4 version 0.999375-16 (2008-06-23) and R version 2.15.1
>> (2012-06-22), evaluating a set of candidate models with AIC to select
>> the best ones.
>>
>> This worked well until I updated lme4 and R last month.
>>
>> I now updated to R version 3.0.2 (2013-09-25) -- "Frisbee Sailing" and
>> lme4 version 1.0-5 (2013-10-24), still Windows XP, and am running the
>> same models again on the same data, and lme4 now returns "-Inf" for
>> the log likelihood, so it can't calculate AIC.
>>
>> Using the previous versions, I got stable values for Log-Likelihoods
>> and calculated AICs with them.
>>
>> Attached is a sample of the dataset and the code I am using.
>>
>> How can I get a Log-Likelihood estimate with the current versions of
>> lme4 and R?
>>
>> Thanks in advance.
>> Justin Baldwin
>>
>> Hampshire College, MA, USA
> 
> 
> 
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From bbolker at gmail.com  Thu Feb 13 01:36:03 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 12 Feb 2014 19:36:03 -0500
Subject: [R-sig-ME] new version of lme4 - Log-Likelihood of model
 returned as "-Inf"
In-Reply-To: <20140212181411.Horde.0Xcna-rBk2lS-ABDwBYD3FA@webmail.hampshire.edu>
References: <20140212180246.Horde.IBHmX-rBk2lS_-2W09gzmWA@webmail.hampshire.edu>	<20140212181035.Horde.DeXeC-rBk2lS_-9rZPNzybA@webmail.hampshire.edu>
	<20140212181411.Horde.0Xcna-rBk2lS-ABDwBYD3FA@webmail.hampshire.edu>
Message-ID: <52FC1373.7060407@gmail.com>

  On the other hand, this example doesn't replicate the problem (it uses
lme4 1.1-4, which has a "simulate from scratch" functionality ...)

library(lme4)
d <- expand.grid(fac1=LETTERS[1:2],fac2=letters[1:2],grp=factor(1:10),
                 obs=1:10)
d$y <- simulate(~fac1*fac2+(1|grp),family=poisson,
         newdata=d,newparams=list(beta=c(2,1,-1,-1),theta=c(1)))[[1]]
d <- transform(d,y=ifelse(fac1=="A" & fac2=="b",0,y))
g1 <- glmer(y~fac1*fac2+(1|grp),family=poisson,data=d)
logLik(g1)

this gives convergence warnings and large values of the fixed effect
parameters (+/- 20), but a finite log-likelihood and parameters (the
MLEs *should* be infinite, but the algorithm can't get there ...)


## simulated value of y
y <- c(12, 29, 0, 6, 21, 92, 0, 10, 4, 21, 0, 3, 7, 23, 0, 2, 3, 8,
0, 0, 12, 37, 0, 9, 1, 9, 0, 0, 2, 9, 0, 0, 43, 103, 0, 15, 26,
53, 0, 4, 7, 29, 0, 6, 21, 80, 0, 10, 4, 12, 0, 0, 5, 22, 0,
5, 3, 3, 0, 1, 14, 34, 0, 4, 3, 7, 0, 1, 4, 4, 0, 0, 34, 92,
0, 17, 30, 51, 0, 7, 12, 30, 0, 8, 22, 73, 0, 10, 4, 18, 0, 2,
10, 25, 0, 2, 2, 8, 0, 2, 13, 23, 0, 5, 4, 9, 0, 1, 5, 8, 0,
1, 45, 123, 0, 14, 27, 52, 0, 11, 3, 27, 0, 0, 31, 70, 0, 10,
12, 17, 0, 1, 10, 18, 0, 1, 8, 9, 0, 1, 13, 35, 0, 6, 3, 7, 0,
0, 2, 8, 0, 2, 38, 108, 0, 9, 29, 54, 0, 6, 13, 24, 0, 5, 22,
68, 0, 13, 4, 17, 0, 1, 12, 22, 0, 3, 3, 7, 0, 3, 17, 22, 0,
6, 4, 9, 0, 0, 4, 8, 0, 0, 32, 103, 0, 23, 26, 75, 0, 9, 14,
26, 0, 2, 37, 72, 0, 8, 4, 19, 0, 0, 8, 11, 0, 4, 1, 4, 0, 1,
8, 39, 0, 4, 4, 11, 0, 0, 2, 6, 0, 1, 38, 112, 0, 16, 23, 70,
0, 9, 10, 27, 0, 6, 28, 65, 0, 7, 8, 14, 0, 2, 11, 21, 0, 1,
1, 7, 0, 0, 11, 30, 0, 5, 3, 11, 0, 1, 3, 7, 0, 2, 34, 113, 0,
17, 23, 63, 0, 6, 16, 35, 0, 3, 20, 78, 0, 8, 6, 16, 0, 2, 7,
25, 0, 2, 2, 5, 0, 0, 13, 37, 0, 5, 2, 11, 0, 1, 8, 6, 0, 2,
36, 107, 0, 14, 22, 74, 0, 6, 14, 24, 0, 5, 40, 73, 0, 7, 11,
21, 0, 7, 9, 22, 0, 3, 5, 4, 0, 1, 10, 35, 0, 5, 2, 7, 0, 3,
3, 12, 0, 0, 41, 118, 0, 17, 21, 59, 0, 5, 9, 31, 0, 6, 26, 63,
0, 6, 5, 25, 0, 1, 6, 30, 0, 3, 4, 9, 0, 3, 12, 32, 0, 8, 5,
11, 0, 0, 1, 14, 0, 0, 40, 89, 0, 13, 16, 58, 0, 11)
On 14-02-12 06:14 PM, juwb08 at hampshire.edu wrote:
> 
> ... the dataset failed to attach, so here it is.
> Sorry!
> 
> Quoting juwb08 at hampshire.edu:
> 
>> Hi All,
>>
>> I was working with Poisson glmms in lme4 last year in 2013 using
>> Windows XP, lme4 version 0.999375-16 (2008-06-23) and R version 2.15.1
>> (2012-06-22), evaluating a set of candidate models with AIC to select
>> the best ones.
>>
>> This worked well until I updated lme4 and R last month.
>>
>> I now updated to R version 3.0.2 (2013-09-25) -- "Frisbee Sailing" and
>> lme4 version 1.0-5 (2013-10-24), still Windows XP, and am running the
>> same models again on the same data, and lme4 now returns "-Inf" for
>> the log likelihood, so it can't calculate AIC.
>>
>> Using the previous versions, I got stable values for Log-Likelihoods
>> and calculated AICs with them.
>>
>> Attached is a sample of the dataset and the code I am using.
>>
>> How can I get a Log-Likelihood estimate with the current versions of
>> lme4 and R?
>>
>> Thanks in advance.
>> Justin Baldwin
>>
>> Hampshire College, MA, USA
> 
> 
> 
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From j.hadfield at ed.ac.uk  Thu Feb 13 11:15:32 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Thu, 13 Feb 2014 10:15:32 +0000
Subject: [R-sig-ME] Modelling heterogeneous variances for an interaction
 term in MCMCglmm?
In-Reply-To: <CAOxxGRnK+m-mJ4zYQpPXEojPPzbd8dHiXNVvYqXuMdCi8eRx8g@mail.gmail.com>
References: <CAOxxGRnK+m-mJ4zYQpPXEojPPzbd8dHiXNVvYqXuMdCi8eRx8g@mail.gmail.com>
Message-ID: <20140213101532.11034r2dom8x34n4@www.staffmail.ed.ac.uk>

Hi Jackie,

Unless you have a very large amount of data I would be careful about  
fitting 3 12x12 covariance matrices. However, that in itself should  
not generate the error.

Does

prior=list(R=list(V=diag(1),n=0.002),G=list(G1=list(V=diag(12),n=0.002),G2=list(V=diag(12),n=0.002)))

model=MCMCglmm(grad.lin~pop.bin*group,mev=mev,data=data,prior=prior,random=~idh(pop.bin*group):study+idh(pop.bin*group):pop,rcov=~units,nitt=300000,burnin=100000,thin=50)

run? If so can you let me know. If not can you make sure that both  
pop.bin and group are coded as factors.

Cheers,

Jarrod



Quoting Jackie Wood <jackiewood7 at gmail.com> on Wed, 12 Feb 2014  
18:04:22 -0500:

> Hello all,
>
> I am using MCMCglmm and attempting to create a model which allows
> heterogeneous variances for different levels of a moderator variable where
> the moderator is an interaction term. I attempted the run the following
> model:
>
> prior=list(R=list(V=diag(12),n=0.002),G=list(G1=list(V=diag(12),n=0.002),G2=list(V=diag(12),n=0.002)))
>
> model=MCMCglmm(grad.lin~pop.bin*group,mev=mev,data=data,prior=prior,random=~idh(pop.bin*group):study
> +idh(pop.bin*group):pop,rcov=~idh(pop.bin*group):units,nitt=300000,burnin=100000,thin=50)
>
> where "pop.bin" corresponds to bins of population size and has 4 levels,
> and "group" has 3 levels (Fish, Birds, Plants).
>
> When I run the above model I receive the error:
> Error in priorformat(if (NOpriorG) { :V is the wrong dimension for some
> prior$G/prior$R elements4
>
> I tried running a simpler model with just "pop.bin" instead of the
> interaction term and "V=diag(4)" in the prior, and it seems to run just
> fine.
>
> I'm unsure whether the problem is simply too many levels and I perhaps
> don't have enough data or statistical power to run this model or whether I
> made a simple (or huge) error in the model or prior specification. If
> anyone could shed some light on this it would be a huge help.
>
> Cheers,
> Jackie
>
>
> --
> Jacquelyn L.A. Wood, MSc.
> PhD Candidate
> Biology Dept.
> Concordia University
> 7141 Sherbrooke St. West
> Montreal, QC
> H4B 1R6
> Phone: (514) 293-7255
> Fax: (514) 848-2881
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From wolfgang.viechtbauer at maastrichtuniversity.nl  Thu Feb 13 13:22:17 2014
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (STAT))
Date: Thu, 13 Feb 2014 13:22:17 +0100
Subject: [R-sig-ME] lme4: constrain sigma to 0
In-Reply-To: <52FC0BB9.3050407@auckland.ac.nz>
References: <528E4605.5060607@uni-mannheim.de> <528E637D.2070302@gmail.com>
	<CAGG0PdBu8yHYpHafCjmr0Jx_yB-LHqzVX13FbGnRUS-OOBypNQ@mail.gmail.com>
	<52FAAFBA.1030304@auckland.ac.nz>
	<02325EDF-88B5-4F9F-A7C0-8079A04D672B@nyu.edu>,
	<52FC0BB9.3050407@auckland.ac.nz>
Message-ID: <077E31A57DA26E46AB0D493C9966AC730D9CB791C4@UM-MAIL4112.unimaas.nl>

Is there a particular reason why you would want to fit this with lmer() as opposed to gls() from nlme? In your demo script:

res <- gls(y ~ time, correlation = corSymm(form = ~ 1 | student), weights = varIdent(form = ~ 1 | time), data=dat.sim)

will fit that model.

Best,
Wolfgang
________________________________________
From: r-sig-mixed-models-bounces at r-project.org [r-sig-mixed-models-bounces at r-project.org] On Behalf Of Rolf Turner [r.turner at auckland.ac.nz]
Sent: Thursday, February 13, 2014 1:03 AM
To: Vincent Dorie
Cc: r-sig-mixed-models
Subject: Re: [R-sig-ME] lme4: constrain sigma to 0

Took me a while, but I have managed to write up some specifics of the
problem that I had in mind.  The gremlins seem to have changed things
since the last time I played around with this stuff, and what I am now
getting differs from what I recollect.  However there is still a bit of
a problem.

I communicated with both Doug Bates and Ben Bolker on this issue, but
cannot now for the life of me find any record of the emails that went
back and forth.  And I keep ***everything***.  (Always the way;
everything but what you want is available.)

Anyhow --- I have attached what I hope is a clear write-up in pdf format
and a script to demonstrate what goes on using simulated data.

I hope these get through ...

cheers,

Rolf Turner

On 12/02/14 12:48, Vincent Dorie wrote:

> Any chance someone could write out the specifics of such a model? If I
can wrap my head around it and its not too hard, I could try to throw it
into blme.
>
> On Feb 11, 2014, at 6:18 PM, Rolf Turner wrote:
>
>> Several millennia ago I put in a feature-request that the facility for
>> constraining sigma to 0 be added. So far, as far as I can tell, it
>> hasn't been.
>>
>> Apparently it is (very) difficult to add such a constraint due to
>> the way that lmer approaches the maximization of the likelihood.
>> (This, rather than any intrinsic mathematical block to such a
>> constraint.)
>>
>> I wanted to be able to fit a fairly simple repeated measures model
>> with the covariance over time being an "arbitrary" n-x-n positive
>> definite matrix (where "n" is the number of time points).  This
>> can't be done using lmer() unless one can constrain the "overall"
>> variance to be zero, otherwise the diagonal of the covariance
>> matrix is un-identifiable.
>>
>> Since one cannot constrain sigma to be 0, one can't fit this model
>> with lmer().  Bummer.

From Emma.Stone at bristol.ac.uk  Thu Feb 13 14:14:48 2014
From: Emma.Stone at bristol.ac.uk (Emma Stone)
Date: Thu, 13 Feb 2014 15:14:48 +0200
Subject: [R-sig-ME] Unable to install glmmadmb
Message-ID: <CANGK3coe8kW1977+28U9wCGFeYnsRQqNR5e-R3viEa_f3H1MGw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140213/8aba7f17/attachment.pl>

From jackiewood7 at gmail.com  Thu Feb 13 18:44:24 2014
From: jackiewood7 at gmail.com (Jackie Wood)
Date: Thu, 13 Feb 2014 12:44:24 -0500
Subject: [R-sig-ME] Modelling heterogeneous variances for an interaction
 term in MCMCglmm?
In-Reply-To: <20140213101532.11034r2dom8x34n4@www.staffmail.ed.ac.uk>
References: <CAOxxGRnK+m-mJ4zYQpPXEojPPzbd8dHiXNVvYqXuMdCi8eRx8g@mail.gmail.com>
	<20140213101532.11034r2dom8x34n4@www.staffmail.ed.ac.uk>
Message-ID: <CAOxxGRk+r60KQPJtH=L6MhiMcBJTUfkVcDKr6U2EUh5j9i1S3Q@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140213/020e3258/attachment.pl>

From bbolker at gmail.com  Thu Feb 13 19:14:35 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 13 Feb 2014 13:14:35 -0500
Subject: [R-sig-ME] Unable to install glmmadmb
In-Reply-To: <CANGK3coe8kW1977+28U9wCGFeYnsRQqNR5e-R3viEa_f3H1MGw@mail.gmail.com>
References: <CANGK3coe8kW1977+28U9wCGFeYnsRQqNR5e-R3viEa_f3H1MGw@mail.gmail.com>
Message-ID: <52FD0B8B.2000602@gmail.com>

  This sounds like a connection problem.  Have you tried downloading the
file directly from
http://glmmadmb.r-forge.r-project.org/repos/bin/windows/contrib/3.0/glmmADMB_0.7.7.zip
(e.g. in a web browser), checking to see that the resulting file is the
right size (6.2 Mb), and installing it from the local zip file?

  Ben Bolker

On 14-02-13 08:14 AM, Emma Stone wrote:
> Dear All,
> 
> 
> I am having issues trying to install the glmmADMB package into R. I am
> getting the following errors
> 
> Installing package into 'C:/Users/emzstone/Documents/R/win-library/3.0'
> 
> (as 'lib' is unspecified)
> 
> trying URL '
> http://glmmadmb.r-forge.r-project.org/repos/bin/windows/contrib/3.0/glmmADMB_0.7.7.zip
> '
> 
> Content type 'application/zip' length 6476249 bytes (6.2 Mb)
> 
> opened URL
> 
> downloaded 2.6 Mb
> 
> 
> Error in read.dcf(file.path(pkgname, "DESCRIPTION"), c("Package", "Type"))
> :
> 
>   cannot open the connection
> 
> In addition: Warning messages:
> 
> 1: In download.file(url, destfile, method, mode = "wb", ...) :
> 
>   downloaded length 2722816 != reported length 6476249
> 
> 2: In unzip(zipname, exdir = dest) : error 1 in extracting from zip file
> 
> 3: In read.dcf(file.path(pkgname, "DESCRIPTION"), c("Package", "Type")) :
> 
>   cannot open compressed file 'glmmADMB/DESCRIPTION', probable reason 'No
> such file or directory'
> 
> Also the sessionInfo() is:
> 
> R version 3.0.1 (2013-05-16)
> 
> Platform: x86_64-w64-mingw32/x64 (64-bit)
> 
> 
> locale:
> 
> [1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United
> Kingdom.1252    LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C
> 
> 
> [5] LC_TIME=English_United Kingdom.1252
> 
> 
> attached base packages:
> 
> [1] stats     graphics  grDevices utils     datasets  methods   base
> 
> 
> other attached packages:
> 
> [1] lme4_0.999999-2 Matrix_1.0-12   lattice_0.20-15
> 
> 
> loaded via a namespace (and not attached):
> 
> [1] grid_3.0.1   nlme_3.1-109 stats4_3.0.1 tools_3.0.1
> 
>>
> I have tried to download and install the latest version of R also from the
> R website and that does not work either.
> 
> Any ideas?
> 
> EMma
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From j.hadfield at ed.ac.uk  Thu Feb 13 19:28:19 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Thu, 13 Feb 2014 18:28:19 +0000
Subject: [R-sig-ME] Modelling heterogeneous variances for an interaction
 term in MCMCglmm?
In-Reply-To: <CAOxxGRk+r60KQPJtH=L6MhiMcBJTUfkVcDKr6U2EUh5j9i1S3Q@mail.gmail.com>
References: <CAOxxGRnK+m-mJ4zYQpPXEojPPzbd8dHiXNVvYqXuMdCi8eRx8g@mail.gmail.com>
	<20140213101532.11034r2dom8x34n4@www.staffmail.ed.ac.uk>
	<CAOxxGRk+r60KQPJtH=L6MhiMcBJTUfkVcDKr6U2EUh5j9i1S3Q@mail.gmail.com>
Message-ID: <20140213182819.56013repu2i1lps0@www.staffmail.ed.ac.uk>

Hi Jackie,

The reason that it does not run is that each observation needs to be  
associated with a single effect. For ease, imagine two factors fac1  
and fac2 have 2-levels each (A and B, and C and D respectively).  
MCMCglmm removes the intercept from formula inside variance functions  
(e.g. idh) so the R-structure model is

fac1*fac2-1

which for the four types of observation gives:

    fac1A fac1B fac2D fac1B:fac2D
AC     1     0     0           0
AD     1     0     1           0
BC     0     1     0           0
BD     0     1     1           1

This means that the (residual) variance for AD is actually  
V[1,1]+V[3,3]+V[1,3]+V[3,1] where V is the estimated covariance  
matrix. This makes interpretation difficult, and actually MCMCglmm  
does not allow you to fit this type of R-structure (although it does  
allow the G-structures to be of this form).


Perhaps a better way of doing it is to fit idh(fac1:fac2) because this gives

    fac1A:fac2C fac1A:fac2D fac1B:fac2C fac1B:fac2D
AC           1           0           0           0
AD           0           1           0           0
BC           0           0           1           0
BD           0           0           0           1

which means that there is a one-to-one mapping between the diagonal  
elements of V and the variance within each factor combination.  
MCMCglmm will allow you to fit this type of R-structure. The next  
version of MCMCglmm (which will probably be released next week) will  
issue more sensible warnings when invalid R-structures are specified.

Note however, that if you fit idh(fac1:fac2) for all sources of  
variance (residual and random effects) and you have fac1:fac2-1 in the  
fixed formula then the analysis is exactly equivalent to fitting  
separate models to the data from each factor combination.

Applying a function to a posterior distribution results in a valid new  
posterior distribution. It therefore makes more sense to evaluate the  
functions for the folded normal for each MCMC iteration to give a  
posterior distribution for the expected absolute value of selection.  
This is advantageous because mean(f(x)) is not always equal to  
f(mean(x)) when f is non-linear and it also allows you to use things  
like HPDinterval in order to determine the uncertainty in your  
inferences. Note that it is hard to know what influence a prior placed  
directly on x will do for inferences about x, let alone on f(x).

Cheers,

Jarrod






Quoting Jackie Wood <jackiewood7 at gmail.com> on Thu, 13 Feb 2014  
12:44:24 -0500:

> Hi Jarrod,
>
> Thanks for your response! I made the change you suggested and the model
> stopped after 3000 iterations and asked for a stronger prior, I used n=1,
> and it seemed to run ok after that.
>
> I was wondering if I might impose on you to ask a follow-up question? My
> response variable "grad.lin" are linear selection gradients and therefore
> have directionality, but we are also interested in the magnitude. So we are
> essentially investigating the potential effect of population size on
> selection. I have read a number of previous articles and syntheses
> regarding selection and understand that this requires the use of a folded
> normal distribution. In fact, one of those articles was one that you
> collaborated on with Michael Morrissey. I contacted Dr. Morrissey, and he
> was kind enough to provide an example of functions that could be used to
> obtain the corresponding values for the magnitude of selection once the
> posterior mode and variance were obtained from an MCMCglmm model using the
> selection gradients "straight up" as it were. The functions were as follows:
>
> mu.fnorm<-function(mu,sigma){dnorm(mu,0,sigma)*2*sigma^2+mu*(2*pnorm(mu,0,sigma)-1)}
> var.fnorm<-function(mu,sigma){mu^2+sigma^2-(sigma*sqrt(2/pi)*exp((-1*mu^2)/(2*sigma^2))+mu*(1-2*pnorm(-1*mu/sigma,0,1)))^2}
>
> I assumed on my own that the value for the posterior mode of selection that
> is plugged into the functions above is given for each level of "pop.bin" in
> the object:
>
> posterior.mode(model$Sol)
>
> and that the posterior variance of selection to use in the functions comes
> from the units variance (after the random effects "study" and "pop"  are
> accounted for) given in:
>
> posterior.mode(model$VCV)
>
> In any case, to make a long story short, the reason why I was attempting to
> use the prior with "R=list(V=diag(12),n=0.002)" is to obtain estimates of
> units variance for the different levels of the interaction term for use in
> the folded normal functions to obtain values for the magnitude of
> selection. So the change to the model you provided works, but only one
> overall estimate of units variance is given.
>
> Anyway, perhaps this is not a problem if I am totally incorrect in the
> assumption that units variance is what I'm looking for.
>
> I checked the data and all three of the groups (fish, plants, and birds)
> are represented in each of the pop.size bins though it's admittedly not an
> even spread. For example, there is more data for birds than for fish or
> plants in each of the bins.
>
> Thanks again,
> Jackie
>
>
> On Thu, Feb 13, 2014 at 5:15 AM, Jarrod Hadfield <j.hadfield at ed.ac.uk>wrote:
>
>> Hi Jackie,
>>
>> Unless you have a very large amount of data I would be careful about
>> fitting 3 12x12 covariance matrices. However, that in itself should not
>> generate the error.
>>
>> Does
>>
>> prior=list(R=list(V=diag(1),n=0.002),G=list(G1=list(V=diag(
>> 12),n=0.002),G2=list(V=diag(12),n=0.002)))
>>
>> model=MCMCglmm(grad.lin~pop.bin*group,mev=mev,data=data,
>> prior=prior,random=~idh(pop.bin*group):study+idh(pop.bin*
>> group):pop,rcov=~units,nitt=300000,burnin=100000,thin=50)
>>
>> run? If so can you let me know. If not can you make sure that both pop.bin
>> and group are coded as factors.
>>
>> Cheers,
>>
>> Jarrod
>>
>>
>>
>>
>> Quoting Jackie Wood <jackiewood7 at gmail.com> on Wed, 12 Feb 2014 18:04:22
>> -0500:
>>
>>  Hello all,
>>>
>>> I am using MCMCglmm and attempting to create a model which allows
>>> heterogeneous variances for different levels of a moderator variable where
>>> the moderator is an interaction term. I attempted the run the following
>>> model:
>>>
>>> prior=list(R=list(V=diag(12),n=0.002),G=list(G1=list(V=
>>> diag(12),n=0.002),G2=list(V=diag(12),n=0.002)))
>>>
>>> model=MCMCglmm(grad.lin~pop.bin*group,mev=mev,data=data,
>>> prior=prior,random=~idh(pop.bin*group):study
>>> +idh(pop.bin*group):pop,rcov=~idh(pop.bin*group):units,nitt=
>>> 300000,burnin=100000,thin=50)
>>>
>>> where "pop.bin" corresponds to bins of population size and has 4 levels,
>>> and "group" has 3 levels (Fish, Birds, Plants).
>>>
>>> When I run the above model I receive the error:
>>> Error in priorformat(if (NOpriorG) { :V is the wrong dimension for some
>>> prior$G/prior$R elements4
>>>
>>> I tried running a simpler model with just "pop.bin" instead of the
>>> interaction term and "V=diag(4)" in the prior, and it seems to run just
>>> fine.
>>>
>>> I'm unsure whether the problem is simply too many levels and I perhaps
>>> don't have enough data or statistical power to run this model or whether I
>>> made a simple (or huge) error in the model or prior specification. If
>>> anyone could shed some light on this it would be a huge help.
>>>
>>> Cheers,
>>> Jackie
>>>
>>>
>>> --
>>> Jacquelyn L.A. Wood, MSc.
>>> PhD Candidate
>>> Biology Dept.
>>> Concordia University
>>> 7141 Sherbrooke St. West
>>> Montreal, QC
>>> H4B 1R6
>>> Phone: (514) 293-7255
>>> Fax: (514) 848-2881
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>>
>>>
>>
>>
>> --
>> The University of Edinburgh is a charitable body, registered in
>> Scotland, with registration number SC005336.
>>
>>
>>
>
>
> --
> Jacquelyn L.A. Wood, MSc.
> PhD Candidate
> Biology Dept.
> Concordia University
> 7141 Sherbrooke St. West
> Montreal, QC
> H4B 1R6
> Phone: (514) 293-7255
> Fax: (514) 848-2881
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From vjd4 at nyu.edu  Thu Feb 13 21:05:37 2014
From: vjd4 at nyu.edu (Vincent Dorie)
Date: Thu, 13 Feb 2014 15:05:37 -0500
Subject: [R-sig-ME] lme4: constrain sigma to 0
In-Reply-To: <52FC0BB9.3050407@auckland.ac.nz>
References: <528E4605.5060607@uni-mannheim.de> <528E637D.2070302@gmail.com>
	<CAGG0PdBu8yHYpHafCjmr0Jx_yB-LHqzVX13FbGnRUS-OOBypNQ@mail.gmail.com>
	<52FAAFBA.1030304@auckland.ac.nz>
	<02325EDF-88B5-4F9F-A7C0-8079A04D672B@nyu.edu>
	<52FC0BB9.3050407@auckland.ac.nz>
Message-ID: <4D55FE28-9BFA-4489-AE3C-458067047BFF@nyu.edu>

Thanks for the writeup, it helped a lot.

I guess I can see why this hasn't been added to lmer, since it doesn't really exploit the sparse matrix decomposition machinery lmer uses. From a nuts and bolts perspective, it would probably make more sense to pass off resid.var = 0 calls to nlme or anything that does generalized least squares.

Here's a followup question for anyone that would like to fit a zero residual variance model, but is it reasonable to say that this is equivalent to wanting to apply the covariance structure from the random effects to the observations instead? One hurdle that I can see is that the model only makes sense if you only have a single grouping factor that induces a random effect for every observation. Enforcing that could cause some frustration when trying to specify the model.

An alternative that might be to include a separate argument to lmer that lets you specify residual error structure. In this case, that would highlight that this is an lm, not an lmm. However, there doesn't seem to be anything in principle that prevents you from doing a proper mixed effects model in this fashion, so long as that error structure is sparse. You could pretty easily kludge it by wrapping an lmer optim call and updating the response, fixed, and random effect design matrices with every iteration.

Vince

On Feb 12, 2014, at 7:03 PM, Rolf Turner wrote:

> 
> Took me a while, but I have managed to write up some specifics of the problem that I had in mind.  The gremlins seem to have changed things since the last time I played around with this stuff, and what I am now
> getting differs from what I recollect.  However there is still a bit of
> a problem.
> 
> I communicated with both Doug Bates and Ben Bolker on this issue, but cannot now for the life of me find any record of the emails that went back and forth.  And I keep ***everything***.  (Always the way; everything but what you want is available.)
> 
> Anyhow --- I have attached what I hope is a clear write-up in pdf format and a script to demonstrate what goes on using simulated data.
> 
> I hope these get through ...
> 
> cheers,
> 
> Rolf Turner
> 
> 
> On 12/02/14 12:48, Vincent Dorie wrote:
> 
>> Any chance someone could write out the specifics of such a model? If I
> can wrap my head around it and its not too hard, I could try to throw it
> into blme.
>> 
>> On Feb 11, 2014, at 6:18 PM, Rolf Turner wrote:
>> 
>>> Several millennia ago I put in a feature-request that the facility for
>>> constraining sigma to 0 be added. So far, as far as I can tell, it
>>> hasn't been.
>>> 
>>> Apparently it is (very) difficult to add such a constraint due to
>>> the way that lmer approaches the maximization of the likelihood.
>>> (This, rather than any intrinsic mathematical block to such a
>>> constraint.)
>>> 
>>> I wanted to be able to fit a fairly simple repeated measures model
>>> with the covariance over time being an "arbitrary" n-x-n positive
>>> definite matrix (where "n" is the number of time points).  This
>>> can't be done using lmer() unless one can constrain the "overall"
>>> variance to be zero, otherwise the diagonal of the covariance
>>> matrix is un-identifiable.
>>> 
>>> Since one cannot constrain sigma to be 0, one can't fit this model
>>> with lmer().  Bummer.
> <specifics.pdf><demoScript.txt>


From vjd4 at nyu.edu  Thu Feb 13 21:22:27 2014
From: vjd4 at nyu.edu (Vincent Dorie)
Date: Thu, 13 Feb 2014 15:22:27 -0500
Subject: [R-sig-ME] lme4: constrain sigma to 0
In-Reply-To: <52FC0BB9.3050407@auckland.ac.nz>
References: <528E4605.5060607@uni-mannheim.de> <528E637D.2070302@gmail.com>
	<CAGG0PdBu8yHYpHafCjmr0Jx_yB-LHqzVX13FbGnRUS-OOBypNQ@mail.gmail.com>
	<52FAAFBA.1030304@auckland.ac.nz>
	<02325EDF-88B5-4F9F-A7C0-8079A04D672B@nyu.edu>
	<52FC0BB9.3050407@auckland.ac.nz>
Message-ID: <8EAE058D-BBAD-4141-B0F1-D28504930011@nyu.edu>

Actually strike what I just said. Identifiability would get in the way more often than not and make it a terrible idea to implement in the general case.

On Feb 12, 2014, at 7:03 PM, Rolf Turner wrote:

> 
> Took me a while, but I have managed to write up some specifics of the problem that I had in mind.  The gremlins seem to have changed things since the last time I played around with this stuff, and what I am now
> getting differs from what I recollect.  However there is still a bit of
> a problem.
> 
> I communicated with both Doug Bates and Ben Bolker on this issue, but cannot now for the life of me find any record of the emails that went back and forth.  And I keep ***everything***.  (Always the way; everything but what you want is available.)
> 
> Anyhow --- I have attached what I hope is a clear write-up in pdf format and a script to demonstrate what goes on using simulated data.
> 
> I hope these get through ...
> 
> cheers,
> 
> Rolf Turner
> 
> 
> On 12/02/14 12:48, Vincent Dorie wrote:
> 
>> Any chance someone could write out the specifics of such a model? If I
> can wrap my head around it and its not too hard, I could try to throw it
> into blme.
>> 
>> On Feb 11, 2014, at 6:18 PM, Rolf Turner wrote:
>> 
>>> Several millennia ago I put in a feature-request that the facility for
>>> constraining sigma to 0 be added. So far, as far as I can tell, it
>>> hasn't been.
>>> 
>>> Apparently it is (very) difficult to add such a constraint due to
>>> the way that lmer approaches the maximization of the likelihood.
>>> (This, rather than any intrinsic mathematical block to such a
>>> constraint.)
>>> 
>>> I wanted to be able to fit a fairly simple repeated measures model
>>> with the covariance over time being an "arbitrary" n-x-n positive
>>> definite matrix (where "n" is the number of time points).  This
>>> can't be done using lmer() unless one can constrain the "overall"
>>> variance to be zero, otherwise the diagonal of the covariance
>>> matrix is un-identifiable.
>>> 
>>> Since one cannot constrain sigma to be 0, one can't fit this model
>>> with lmer().  Bummer.
> <specifics.pdf><demoScript.txt>


From r.turner at auckland.ac.nz  Thu Feb 13 20:04:31 2014
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Fri, 14 Feb 2014 08:04:31 +1300
Subject: [R-sig-ME] lme4: constrain sigma to 0
In-Reply-To: <077E31A57DA26E46AB0D493C9966AC730D9CB791C4@UM-MAIL4112.unimaas.nl>
References: <528E4605.5060607@uni-mannheim.de> <528E637D.2070302@gmail.com>
	<CAGG0PdBu8yHYpHafCjmr0Jx_yB-LHqzVX13FbGnRUS-OOBypNQ@mail.gmail.com>
	<52FAAFBA.1030304@auckland.ac.nz>
	<02325EDF-88B5-4F9F-A7C0-8079A04D672B@nyu.edu>,
	<52FC0BB9.3050407@auckland.ac.nz>
	<077E31A57DA26E46AB0D493C9966AC730D9CB791C4@UM-MAIL4112.unimaas.nl>
Message-ID: <52FD173F.7070800@auckland.ac.nz>

On 14/02/14 01:22, Viechtbauer Wolfgang (STAT) wrote:
> Is there a particular reason why you would want to fit this with
> lmer() as opposed to gls() from nlme? In your demo script:
>
> res <- gls(y ~ time, correlation = corSymm(form = ~ 1 | student),
> weights = varIdent(form = ~ 1 | time), data=dat.sim)
>
> will fit that model.

Well yes.  As I said in my write-up, the objective is really to be able 
to cross-check lmer() results with an alternative analysis that is 
available in a simple case, to verify (one hopes) that one has the 
lmer() syntax correct.

When I was actually faced with this problem, the data set and model that 
I wished to fit were actually much more complicated than the toy model 
that I discussed in my write-up.  The toy model was just a tentative 
step toward seeing if I could handle setting up the model for lmer() and 
understand/interpret the results correctly.  When the identifiability 
issue arose, I'm afraid that I basically gave up on lmer().

cheers,

Rolf Turner


> ________________________________________
> From: r-sig-mixed-models-bounces at r-project.org [r-sig-mixed-models-bounces at r-project.org] On Behalf Of Rolf Turner [r.turner at auckland.ac.nz]
> Sent: Thursday, February 13, 2014 1:03 AM
> To: Vincent Dorie
> Cc: r-sig-mixed-models
> Subject: Re: [R-sig-ME] lme4: constrain sigma to 0
>
> Took me a while, but I have managed to write up some specifics of the
> problem that I had in mind.  The gremlins seem to have changed things
> since the last time I played around with this stuff, and what I am now
> getting differs from what I recollect.  However there is still a bit of
> a problem.
>
> I communicated with both Doug Bates and Ben Bolker on this issue, but
> cannot now for the life of me find any record of the emails that went
> back and forth.  And I keep ***everything***.  (Always the way;
> everything but what you want is available.)
>
> Anyhow --- I have attached what I hope is a clear write-up in pdf format
> and a script to demonstrate what goes on using simulated data.
>
> I hope these get through ...
>
> cheers,
>
> Rolf Turner
>
> On 12/02/14 12:48, Vincent Dorie wrote:
>
>> Any chance someone could write out the specifics of such a model? If I
> can wrap my head around it and its not too hard, I could try to throw it
> into blme.
>>
>> On Feb 11, 2014, at 6:18 PM, Rolf Turner wrote:
>>
>>> Several millennia ago I put in a feature-request that the facility for
>>> constraining sigma to 0 be added. So far, as far as I can tell, it
>>> hasn't been.
>>>
>>> Apparently it is (very) difficult to add such a constraint due to
>>> the way that lmer approaches the maximization of the likelihood.
>>> (This, rather than any intrinsic mathematical block to such a
>>> constraint.)
>>>
>>> I wanted to be able to fit a fairly simple repeated measures model
>>> with the covariance over time being an "arbitrary" n-x-n positive
>>> definite matrix (where "n" is the number of time points).  This
>>> can't be done using lmer() unless one can constrain the "overall"
>>> variance to be zero, otherwise the diagonal of the covariance
>>> matrix is un-identifiable.
>>>
>>> Since one cannot constrain sigma to be 0, one can't fit this model
>>> with lmer().  Bummer.


From r.turner at auckland.ac.nz  Thu Feb 13 20:11:19 2014
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Fri, 14 Feb 2014 08:11:19 +1300
Subject: [R-sig-ME] lme4: constrain sigma to 0
In-Reply-To: <8EAE058D-BBAD-4141-B0F1-D28504930011@nyu.edu>
References: <528E4605.5060607@uni-mannheim.de> <528E637D.2070302@gmail.com>
	<CAGG0PdBu8yHYpHafCjmr0Jx_yB-LHqzVX13FbGnRUS-OOBypNQ@mail.gmail.com>
	<52FAAFBA.1030304@auckland.ac.nz>
	<02325EDF-88B5-4F9F-A7C0-8079A04D672B@nyu.edu>
	<52FC0BB9.3050407@auckland.ac.nz>
	<8EAE058D-BBAD-4141-B0F1-D28504930011@nyu.edu>
Message-ID: <52FD18D7.3000504@auckland.ac.nz>

On 14/02/14 09:22, Vincent Dorie wrote:

> Actually strike what I just said. Identifiability would get in the
> way more often than not and make it a terrible idea to implement in
> the general case.

I don't follow you here.  The problem, it seems to me, is that lmer() 
insists on fitting an ***unidentifiable*** model, rather than the 
perfectly legitimate identifiable model that is actually desired.

It seems to me that the desired model is the basic case of a repeated 
measures model, whence (it seems to me) it is reasonable to expect to be 
able to fit it in software for mixed modelling.

I know it is impolitic to say this on this mailing list, but ``SAS can 
do it''.  (And without breaking a sweat.)

cheers,

Rolf Turner

>
> On Feb 12, 2014, at 7:03 PM, Rolf Turner wrote:
>
>>
>> Took me a while, but I have managed to write up some specifics of the problem that I had in mind.  The gremlins seem to have changed things since the last time I played around with this stuff, and what I am now
>> getting differs from what I recollect.  However there is still a bit of
>> a problem.
>>
>> I communicated with both Doug Bates and Ben Bolker on this issue, but cannot now for the life of me find any record of the emails that went back and forth.  And I keep ***everything***.  (Always the way; everything but what you want is available.)
>>
>> Anyhow --- I have attached what I hope is a clear write-up in pdf format and a script to demonstrate what goes on using simulated data.
>>
>> I hope these get through ...
>>
>> cheers,
>>
>> Rolf Turner
>>
>>
>> On 12/02/14 12:48, Vincent Dorie wrote:
>>
>>> Any chance someone could write out the specifics of such a model? If I
>> can wrap my head around it and its not too hard, I could try to throw it
>> into blme.
>>>
>>> On Feb 11, 2014, at 6:18 PM, Rolf Turner wrote:
>>>
>>>> Several millennia ago I put in a feature-request that the facility for
>>>> constraining sigma to 0 be added. So far, as far as I can tell, it
>>>> hasn't been.
>>>>
>>>> Apparently it is (very) difficult to add such a constraint due to
>>>> the way that lmer approaches the maximization of the likelihood.
>>>> (This, rather than any intrinsic mathematical block to such a
>>>> constraint.)
>>>>
>>>> I wanted to be able to fit a fairly simple repeated measures model
>>>> with the covariance over time being an "arbitrary" n-x-n positive
>>>> definite matrix (where "n" is the number of time points).  This
>>>> can't be done using lmer() unless one can constrain the "overall"
>>>> variance to be zero, otherwise the diagonal of the covariance
>>>> matrix is un-identifiable.
>>>>
>>>> Since one cannot constrain sigma to be 0, one can't fit this model
>>>> with lmer().  Bummer.
>> <specifics.pdf><demoScript.txt>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From vjd4 at nyu.edu  Thu Feb 13 23:05:35 2014
From: vjd4 at nyu.edu (Vincent Dorie)
Date: Thu, 13 Feb 2014 17:05:35 -0500
Subject: [R-sig-ME] lme4: constrain sigma to 0
In-Reply-To: <52FD18D7.3000504@auckland.ac.nz>
References: <528E4605.5060607@uni-mannheim.de> <528E637D.2070302@gmail.com>
	<CAGG0PdBu8yHYpHafCjmr0Jx_yB-LHqzVX13FbGnRUS-OOBypNQ@mail.gmail.com>
	<52FAAFBA.1030304@auckland.ac.nz>
	<02325EDF-88B5-4F9F-A7C0-8079A04D672B@nyu.edu>
	<52FC0BB9.3050407@auckland.ac.nz>
	<8EAE058D-BBAD-4141-B0F1-D28504930011@nyu.edu>
	<52FD18D7.3000504@auckland.ac.nz>
Message-ID: <C1BA87A6-DF02-4F6F-88D3-417AA4BF7D99@nyu.edu>

I meant identifiability only in the case of a full mixed model using a residual covariance structure, with variance at the observation and group level. I haven't exactly meditated on it, but you would have a marginal covariance of Sigma_y + Z Sigma_b Z' which could be difficult to identify depending on how things were blocked off.

From what I can tell, the case you're proposing sets Sigma_y to 0, Z to identity, and uses the block repetition of the covariance that goes with the random effects for the observations themselves. I can't speak for the lmer developers, but if I wanted to add that class of models I would do so by adding an extra argument to lmer for the residual variance structure.

Vince

On Feb 13, 2014, at 2:11 PM, Rolf Turner wrote:

> On 14/02/14 09:22, Vincent Dorie wrote:
> 
>> Actually strike what I just said. Identifiability would get in the
>> way more often than not and make it a terrible idea to implement in
>> the general case.
> 
> I don't follow you here.  The problem, it seems to me, is that lmer() insists on fitting an ***unidentifiable*** model, rather than the perfectly legitimate identifiable model that is actually desired.
> 
> It seems to me that the desired model is the basic case of a repeated measures model, whence (it seems to me) it is reasonable to expect to be able to fit it in software for mixed modelling.
> 
> I know it is impolitic to say this on this mailing list, but ``SAS can do it''.  (And without breaking a sweat.)
> 
> cheers,
> 
> Rolf Turner
> 
>> 
>> On Feb 12, 2014, at 7:03 PM, Rolf Turner wrote:
>> 
>>> 
>>> Took me a while, but I have managed to write up some specifics of the problem that I had in mind.  The gremlins seem to have changed things since the last time I played around with this stuff, and what I am now
>>> getting differs from what I recollect.  However there is still a bit of
>>> a problem.
>>> 
>>> I communicated with both Doug Bates and Ben Bolker on this issue, but cannot now for the life of me find any record of the emails that went back and forth.  And I keep ***everything***.  (Always the way; everything but what you want is available.)
>>> 
>>> Anyhow --- I have attached what I hope is a clear write-up in pdf format and a script to demonstrate what goes on using simulated data.
>>> 
>>> I hope these get through ...
>>> 
>>> cheers,
>>> 
>>> Rolf Turner
>>> 
>>> 
>>> On 12/02/14 12:48, Vincent Dorie wrote:
>>> 
>>>> Any chance someone could write out the specifics of such a model? If I
>>> can wrap my head around it and its not too hard, I could try to throw it
>>> into blme.
>>>> 
>>>> On Feb 11, 2014, at 6:18 PM, Rolf Turner wrote:
>>>> 
>>>>> Several millennia ago I put in a feature-request that the facility for
>>>>> constraining sigma to 0 be added. So far, as far as I can tell, it
>>>>> hasn't been.
>>>>> 
>>>>> Apparently it is (very) difficult to add such a constraint due to
>>>>> the way that lmer approaches the maximization of the likelihood.
>>>>> (This, rather than any intrinsic mathematical block to such a
>>>>> constraint.)
>>>>> 
>>>>> I wanted to be able to fit a fairly simple repeated measures model
>>>>> with the covariance over time being an "arbitrary" n-x-n positive
>>>>> definite matrix (where "n" is the number of time points).  This
>>>>> can't be done using lmer() unless one can constrain the "overall"
>>>>> variance to be zero, otherwise the diagonal of the covariance
>>>>> matrix is un-identifiable.
>>>>> 
>>>>> Since one cannot constrain sigma to be 0, one can't fit this model
>>>>> with lmer().  Bummer.
>>> <specifics.pdf><demoScript.txt>
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> 
> 


From jackiewood7 at gmail.com  Fri Feb 14 00:41:34 2014
From: jackiewood7 at gmail.com (Jackie Wood)
Date: Thu, 13 Feb 2014 18:41:34 -0500
Subject: [R-sig-ME] Modelling heterogeneous variances for an interaction
 term in MCMCglmm?
In-Reply-To: <20140213182819.56013repu2i1lps0@www.staffmail.ed.ac.uk>
References: <CAOxxGRnK+m-mJ4zYQpPXEojPPzbd8dHiXNVvYqXuMdCi8eRx8g@mail.gmail.com>
	<20140213101532.11034r2dom8x34n4@www.staffmail.ed.ac.uk>
	<CAOxxGRk+r60KQPJtH=L6MhiMcBJTUfkVcDKr6U2EUh5j9i1S3Q@mail.gmail.com>
	<20140213182819.56013repu2i1lps0@www.staffmail.ed.ac.uk>
Message-ID: <CAOxxGRkHjcujpgHnwivaBpmeH9-ZJW4C5-1CkSA68AG+nRHZdg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140213/6a92a2ad/attachment.pl>

From bbolker at gmail.com  Fri Feb 14 04:28:10 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 13 Feb 2014 22:28:10 -0500
Subject: [R-sig-ME] new version of lme4 - Log-Likelihood of model
 returned as "-Inf"
In-Reply-To: <20140213214610.Horde.BeWfNvrBk2lS-YNyjomX3AA@webmail.hampshire.edu>
References: <20140212180246.Horde.IBHmX-rBk2lS_-2W09gzmWA@webmail.hampshire.edu>
	<20140212181035.Horde.DeXeC-rBk2lS_-9rZPNzybA@webmail.hampshire.edu>
	<20140212181411.Horde.0Xcna-rBk2lS-ABDwBYD3FA@webmail.hampshire.edu>
	<52FD5982.1050700@gmail.com>
	<20140213214610.Horde.BeWfNvrBk2lS-YNyjomX3AA@webmail.hampshire.edu>
Message-ID: <52FD8D4A.6030103@gmail.com>

  It looks like the proximal problem here is that you have non-integer
responses in the Poisson distribution.  I haven't dug in to figure out
why this should cause a problem; having non-integer responses in a
nominally Poisson response breaks the nominal statistical model, but
normal GLMs can manage anyway, and there's *probably* no reason that the
internal lme4 code couldn't be made a bit more flexible.
   Where do your non-integer responses come from?

   I would also comment offhand that trying to fit a random effect of
(1|BatID/PlantID) seems rather optimistic; you have a total of 26
observations (after removing NAs), and 121 Bat:Plant combinations, so
very few of the combinations are actually measured, and are measured at
most twice ...  the BatID:PlantID interaction is going to translate
_almost_ to an observation-level random effect.  When I fit without the
single non-integer response, I get a zero variance estimate for the
Bat:Plant interaction ...

  Ben Bolker


On 14-02-13 09:46 PM, juwb08 at hampshire.edu wrote:
> Hi Ben Bolker,
> I tried running the code with lme4 version 1.1-4, and receive a model
> that I can print, but it still lacks the LogLikelihood. When I call
> summary on that model I get a new error message.
> 
> "
> Error in diag(vcov(object, use.hessian = use.hessian)) :
>   error in evaluating the argument 'x' in selecting a method for
> function 'diag': Error in if (nonSymm > stopTol)
> stop(sprintf("calculated variance-covariance matrix is non-symmetric
> (tol=%f)",  :
>   missing value where TRUE/FALSE needed
> "
> 
> I had not received any error message with lme4 version 1.0-5 when
> calling summary() on that model. Still lost about the -Inf as the
> LogLikelihood.
> 
> cheers
> Justin Baldwin
> 
> 
> Quoting Ben Bolker <bbolker at gmail.com>:
> 
>> Still haven't seen the data set ...
>>
>>  cheers
>>    Ben Bolker
>>
>> On 14-02-12 06:14 PM, juwb08 at hampshire.edu wrote:
>>>
>>> ... the dataset failed to attach, so here it is.
>>> Sorry!
>>>
>>> Quoting juwb08 at hampshire.edu:
>>>
>>>> Hi All,
>>>>
>>>> I was working with Poisson glmms in lme4 last year in 2013 using
>>>> Windows XP, lme4 version 0.999375-16 (2008-06-23) and R version 2.15.1
>>>> (2012-06-22), evaluating a set of candidate models with AIC to select
>>>> the best ones.
>>>>
>>>> This worked well until I updated lme4 and R last month.
>>>>
>>>> I now updated to R version 3.0.2 (2013-09-25) -- "Frisbee Sailing" and
>>>> lme4 version 1.0-5 (2013-10-24), still Windows XP, and am running the
>>>> same models again on the same data, and lme4 now returns "-Inf" for
>>>> the log likelihood, so it can't calculate AIC.
>>>>
>>>> Using the previous versions, I got stable values for Log-Likelihoods
>>>> and calculated AICs with them.
>>>>
>>>> Attached is a sample of the dataset and the code I am using.
>>>>
>>>> How can I get a Log-Likelihood estimate with the current versions of
>>>> lme4 and R?
>>>>
>>>> Thanks in advance.
>>>> Justin Baldwin
>>>>
>>>> Hampshire College, MA, USA
>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
> 
> 
>


From j.hadfield at ed.ac.uk  Fri Feb 14 07:53:04 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Fri, 14 Feb 2014 06:53:04 +0000
Subject: [R-sig-ME] Modelling heterogeneous variances for an interaction
 term in MCMCglmm?
In-Reply-To: <CAOxxGRkHjcujpgHnwivaBpmeH9-ZJW4C5-1CkSA68AG+nRHZdg@mail.gmail.com>
References: <CAOxxGRnK+m-mJ4zYQpPXEojPPzbd8dHiXNVvYqXuMdCi8eRx8g@mail.gmail.com>
	<20140213101532.11034r2dom8x34n4@www.staffmail.ed.ac.uk>
	<CAOxxGRk+r60KQPJtH=L6MhiMcBJTUfkVcDKr6U2EUh5j9i1S3Q@mail.gmail.com>
	<20140213182819.56013repu2i1lps0@www.staffmail.ed.ac.uk>
	<CAOxxGRkHjcujpgHnwivaBpmeH9-ZJW4C5-1CkSA68AG+nRHZdg@mail.gmail.com>
Message-ID: <20140214065304.1273259w67fud6ro@www.staffmail.ed.ac.uk>

Hi Jackie,

The function is taking the standard deviation not the variance. You  
also need to decide what sources of variance to include. Lets imagine  
you want the expected absolute value of a random draw from the first  
of 12 combinations, and you want to include the variance due to the  
two random terms and the residual. Then,

mu.fnorm(m1$Sol[,1], sqrt(rowSums(m1$VCV[,seq(1,12*3,12)])))

or something close to it should work.

Jarrod



Quoting Jackie Wood <jackiewood7 at gmail.com> on Thu, 13 Feb 2014  
18:41:34 -0500:

> Hi Jarrod,
>
> Thanks so much for the information regarding R-structure and model
> specification. Your explanation for that cleared things up substantially.
>
> In regards to the folded normal, I completely understand what you're saying
> though I'm unsure how that would be coded in MCMCglmm. My experience with
> MCMCglmm thus far has been estimating heritability and Qst so I might think
> it would be something along the lines of:
>
> posterior.fnorm<-mu.fnorm(model$Sol,model$VCV),
> var.fnorm(model$Sol,model$VCV)
> posterior.mode(posterior.fnorm)
> HPDinterval(posterior.fnorm,0.95)
>
> The first line of code doesn't work (and is probably pretty messed up), but
> I'm just wondering if it is in the right direction. I think evaluating the
> functions for the folded normal for each MCMC iteration as you suggested
> involves applying "mu.fnorm" and "var.fnorm" to the posterior distribution
> of solutions and the posterior distribution of variance matrices given by
> an MCMCglmm model but how the two functions would be combined to give
> "posterior.fnorm" in this case is not clear to me.
>
> Thanks,
> Jackie
>
>
>
>
> On Thu, Feb 13, 2014 at 1:28 PM, Jarrod Hadfield <j.hadfield at ed.ac.uk>wrote:
>
>> Hi Jackie,
>>
>> The reason that it does not run is that each observation needs to be
>> associated with a single effect. For ease, imagine two factors fac1 and
>> fac2 have 2-levels each (A and B, and C and D respectively). MCMCglmm
>> removes the intercept from formula inside variance functions (e.g. idh) so
>> the R-structure model is
>>
>> fac1*fac2-1
>>
>> which for the four types of observation gives:
>>
>>    fac1A fac1B fac2D fac1B:fac2D
>> AC     1     0     0           0
>> AD     1     0     1           0
>> BC     0     1     0           0
>> BD     0     1     1           1
>>
>> This means that the (residual) variance for AD is actually
>> V[1,1]+V[3,3]+V[1,3]+V[3,1] where V is the estimated covariance matrix.
>> This makes interpretation difficult, and actually MCMCglmm does not allow
>> you to fit this type of R-structure (although it does allow the
>> G-structures to be of this form).
>>
>>
>> Perhaps a better way of doing it is to fit idh(fac1:fac2) because this
>> gives
>>
>>    fac1A:fac2C fac1A:fac2D fac1B:fac2C fac1B:fac2D
>> AC           1           0           0           0
>> AD           0           1           0           0
>> BC           0           0           1           0
>> BD           0           0           0           1
>>
>> which means that there is a one-to-one mapping between the diagonal
>> elements of V and the variance within each factor combination. MCMCglmm
>> will allow you to fit this type of R-structure. The next version of
>> MCMCglmm (which will probably be released next week) will issue more
>> sensible warnings when invalid R-structures are specified.
>>
>> Note however, that if you fit idh(fac1:fac2) for all sources of variance
>> (residual and random effects) and you have fac1:fac2-1 in the fixed formula
>> then the analysis is exactly equivalent to fitting separate models to the
>> data from each factor combination.
>>
>> Applying a function to a posterior distribution results in a valid new
>> posterior distribution. It therefore makes more sense to evaluate the
>> functions for the folded normal for each MCMC iteration to give a posterior
>> distribution for the expected absolute value of selection. This is
>> advantageous because mean(f(x)) is not always equal to f(mean(x)) when f is
>> non-linear and it also allows you to use things like HPDinterval in order
>> to determine the uncertainty in your inferences. Note that it is hard to
>> know what influence a prior placed directly on x will do for inferences
>> about x, let alone on f(x).
>>
>> Cheers,
>>
>> Jarrod
>>
>>
>>
>>
>>
>>
>>
>> Quoting Jackie Wood <jackiewood7 at gmail.com> on Thu, 13 Feb 2014 12:44:24
>> -0500:
>>
>>  Hi Jarrod,
>>>
>>> Thanks for your response! I made the change you suggested and the model
>>> stopped after 3000 iterations and asked for a stronger prior, I used n=1,
>>> and it seemed to run ok after that.
>>>
>>> I was wondering if I might impose on you to ask a follow-up question? My
>>> response variable "grad.lin" are linear selection gradients and therefore
>>> have directionality, but we are also interested in the magnitude. So we
>>> are
>>> essentially investigating the potential effect of population size on
>>> selection. I have read a number of previous articles and syntheses
>>> regarding selection and understand that this requires the use of a folded
>>> normal distribution. In fact, one of those articles was one that you
>>> collaborated on with Michael Morrissey. I contacted Dr. Morrissey, and he
>>> was kind enough to provide an example of functions that could be used to
>>> obtain the corresponding values for the magnitude of selection once the
>>> posterior mode and variance were obtained from an MCMCglmm model using the
>>> selection gradients "straight up" as it were. The functions were as
>>> follows:
>>>
>>> mu.fnorm<-function(mu,sigma){dnorm(mu,0,sigma)*2*sigma^2+
>>> mu*(2*pnorm(mu,0,sigma)-1)}
>>> var.fnorm<-function(mu,sigma){mu^2+sigma^2-(sigma*sqrt(2/pi)
>>> *exp((-1*mu^2)/(2*sigma^2))+mu*(1-2*pnorm(-1*mu/sigma,0,1)))^2}
>>>
>>> I assumed on my own that the value for the posterior mode of selection
>>> that
>>> is plugged into the functions above is given for each level of "pop.bin"
>>> in
>>> the object:
>>>
>>> posterior.mode(model$Sol)
>>>
>>> and that the posterior variance of selection to use in the functions comes
>>> from the units variance (after the random effects "study" and "pop"  are
>>> accounted for) given in:
>>>
>>> posterior.mode(model$VCV)
>>>
>>> In any case, to make a long story short, the reason why I was attempting
>>> to
>>> use the prior with "R=list(V=diag(12),n=0.002)" is to obtain estimates of
>>> units variance for the different levels of the interaction term for use in
>>> the folded normal functions to obtain values for the magnitude of
>>> selection. So the change to the model you provided works, but only one
>>> overall estimate of units variance is given.
>>>
>>> Anyway, perhaps this is not a problem if I am totally incorrect in the
>>> assumption that units variance is what I'm looking for.
>>>
>>> I checked the data and all three of the groups (fish, plants, and birds)
>>> are represented in each of the pop.size bins though it's admittedly not an
>>> even spread. For example, there is more data for birds than for fish or
>>> plants in each of the bins.
>>>
>>> Thanks again,
>>> Jackie
>>>
>>>
>>> On Thu, Feb 13, 2014 at 5:15 AM, Jarrod Hadfield <j.hadfield at ed.ac.uk
>>> >wrote:
>>>
>>>  Hi Jackie,
>>>>
>>>> Unless you have a very large amount of data I would be careful about
>>>> fitting 3 12x12 covariance matrices. However, that in itself should not
>>>> generate the error.
>>>>
>>>> Does
>>>>
>>>> prior=list(R=list(V=diag(1),n=0.002),G=list(G1=list(V=diag(
>>>> 12),n=0.002),G2=list(V=diag(12),n=0.002)))
>>>>
>>>> model=MCMCglmm(grad.lin~pop.bin*group,mev=mev,data=data,
>>>> prior=prior,random=~idh(pop.bin*group):study+idh(pop.bin*
>>>> group):pop,rcov=~units,nitt=300000,burnin=100000,thin=50)
>>>>
>>>> run? If so can you let me know. If not can you make sure that both
>>>> pop.bin
>>>> and group are coded as factors.
>>>>
>>>> Cheers,
>>>>
>>>> Jarrod
>>>>
>>>>
>>>>
>>>>
>>>> Quoting Jackie Wood <jackiewood7 at gmail.com> on Wed, 12 Feb 2014 18:04:22
>>>> -0500:
>>>>
>>>>  Hello all,
>>>>
>>>>>
>>>>> I am using MCMCglmm and attempting to create a model which allows
>>>>> heterogeneous variances for different levels of a moderator variable
>>>>> where
>>>>> the moderator is an interaction term. I attempted the run the following
>>>>> model:
>>>>>
>>>>> prior=list(R=list(V=diag(12),n=0.002),G=list(G1=list(V=
>>>>> diag(12),n=0.002),G2=list(V=diag(12),n=0.002)))
>>>>>
>>>>> model=MCMCglmm(grad.lin~pop.bin*group,mev=mev,data=data,
>>>>> prior=prior,random=~idh(pop.bin*group):study
>>>>> +idh(pop.bin*group):pop,rcov=~idh(pop.bin*group):units,nitt=
>>>>> 300000,burnin=100000,thin=50)
>>>>>
>>>>> where "pop.bin" corresponds to bins of population size and has 4 levels,
>>>>> and "group" has 3 levels (Fish, Birds, Plants).
>>>>>
>>>>> When I run the above model I receive the error:
>>>>> Error in priorformat(if (NOpriorG) { :V is the wrong dimension for some
>>>>> prior$G/prior$R elements4
>>>>>
>>>>> I tried running a simpler model with just "pop.bin" instead of the
>>>>> interaction term and "V=diag(4)" in the prior, and it seems to run just
>>>>> fine.
>>>>>
>>>>> I'm unsure whether the problem is simply too many levels and I perhaps
>>>>> don't have enough data or statistical power to run this model or
>>>>> whether I
>>>>> made a simple (or huge) error in the model or prior specification. If
>>>>> anyone could shed some light on this it would be a huge help.
>>>>>
>>>>> Cheers,
>>>>> Jackie
>>>>>
>>>>>
>>>>> --
>>>>> Jacquelyn L.A. Wood, MSc.
>>>>> PhD Candidate
>>>>> Biology Dept.
>>>>> Concordia University
>>>>> 7141 Sherbrooke St. West
>>>>> Montreal, QC
>>>>> H4B 1R6
>>>>> Phone: (514) 293-7255
>>>>> Fax: (514) 848-2881
>>>>>
>>>>>         [[alternative HTML version deleted]]
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>
>>>>>
>>>>>
>>>>>
>>>>
>>>> --
>>>> The University of Edinburgh is a charitable body, registered in
>>>> Scotland, with registration number SC005336.
>>>>
>>>>
>>>>
>>>>
>>>
>>> --
>>> Jacquelyn L.A. Wood, MSc.
>>> PhD Candidate
>>> Biology Dept.
>>> Concordia University
>>> 7141 Sherbrooke St. West
>>> Montreal, QC
>>> H4B 1R6
>>> Phone: (514) 293-7255
>>> Fax: (514) 848-2881
>>>
>>>
>>
>>
>> --
>> The University of Edinburgh is a charitable body, registered in
>> Scotland, with registration number SC005336.
>>
>>
>>
>
>
> --
> Jacquelyn L.A. Wood, MSc.
> PhD Candidate
> Biology Dept.
> Concordia University
> 7141 Sherbrooke St. West
> Montreal, QC
> H4B 1R6
> Phone: (514) 293-7255
> Fax: (514) 848-2881
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From roby.joehanes at nih.gov  Fri Feb 14 18:22:48 2014
From: roby.joehanes at nih.gov (Joehanes, Roby (NIH/NHLBI) [F])
Date: Fri, 14 Feb 2014 17:22:48 +0000
Subject: [R-sig-ME] Pedigreemm on lme4-1.0
Message-ID: <CF23BB16.8455%roby.joehanes@nih.gov>

Hi all:

Just wondering if there is any people working on pedigreemm so that it can run on the new lme4 version 1.0? In current version of lme4 1.0, pedigreemm is broken. May I know the latest status on pedigreemm?

Thanks,
Roby


From asafw.at.wharton at gmail.com  Thu Feb 13 20:15:25 2014
From: asafw.at.wharton at gmail.com (Asaf Weinstein)
Date: Thu, 13 Feb 2014 14:15:25 -0500
Subject: [R-sig-ME] lme4: constrain sigma to 0
In-Reply-To: <077E31A57DA26E46AB0D493C9966AC730D9CB791C4@UM-MAIL4112.unimaas.nl>
References: <528E4605.5060607@uni-mannheim.de> <528E637D.2070302@gmail.com>
	<CAGG0PdBu8yHYpHafCjmr0Jx_yB-LHqzVX13FbGnRUS-OOBypNQ@mail.gmail.com>
	<52FAAFBA.1030304@auckland.ac.nz>
	<02325EDF-88B5-4F9F-A7C0-8079A04D672B@nyu.edu>
	<52FC0BB9.3050407@auckland.ac.nz>
	<077E31A57DA26E46AB0D493C9966AC730D9CB791C4@UM-MAIL4112.unimaas.nl>
Message-ID: <CAGG0PdCu8YwDOwgXY-t2Qp31_9NrBySyrV1E_4r0S_tsYdwVbg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140213/20483c41/attachment.pl>

From szymonmarian.drobniak at uzh.ch  Thu Feb 13 14:45:52 2014
From: szymonmarian.drobniak at uzh.ch (Szymek Drobniak)
Date: Thu, 13 Feb 2014 14:45:52 +0100
Subject: [R-sig-ME] MCMCglmm and massing records in relation to ainv
Message-ID: <CANXb-o5_g0zm8jpOGkFndWGNmu2DSzmTt_-Z_5J7DXJ=kaae2g@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140213/abcd1fcb/attachment.pl>

From bbolker at gmail.com  Fri Feb 14 23:29:45 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 14 Feb 2014 17:29:45 -0500
Subject: [R-sig-ME] Pedigreemm on lme4-1.0
In-Reply-To: <CF23BB16.8455%roby.joehanes@nih.gov>
References: <CF23BB16.8455%roby.joehanes@nih.gov>
Message-ID: <52FE98D9.2010405@gmail.com>

On 14-02-14 12:22 PM, Joehanes, Roby (NIH/NHLBI) [F] wrote:
> Hi all:
> 
> Just wondering if there is any people working on pedigreemm so that
> it can run on the new lme4 version 1.0? In current version of lme4
> 1.0, pedigreemm is broken. May I know the latest status on
> pedigreemm?
> 
> Thanks, Roby
> 
> _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 

  I'm a little surprised, because the current CRAN page suggests it's
OK: http://cran.r-project.org/web/packages/pedigreemm/index.html -- this
page wouldn't exist, I think, if the package didn't at least pass its
checks with the new version of lme4 (and it says that it requires lme4
>= 1.0).
   Perhaps you could provide a little more information for those of us
who don't use pedigreemm on a regular basis -- does it fail to install?
 Installs but doesn't pass checks/examples fail?  Examples run but give
incorrect answers?  Some particular cases don't work?
   Posting here seems perfectly reasonable, but I would also suggest
contacting the maintainer (listed at the URL above) as well.  If that
maintainer is unwilling or unable to fix whatever's broken, then
presumably someone else will have to step forward to act as maintainer ...

  I'm willing to help fix what's broken, in particular on the
lme4-interface end, but would need to get *specific* examples of what's
not working ...

  Ben Bolker


From bbolker at gmail.com  Fri Feb 14 23:32:40 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 14 Feb 2014 17:32:40 -0500
Subject: [R-sig-ME] issue with glmer() with the newest updated lme4
	version
In-Reply-To: <CAAdccW0FiBUtNQ5JgCDV7H858iH=_cjdbRJC2+UTSsNSn2B9Pg@mail.gmail.com>
References: <CAAdccW0FiBUtNQ5JgCDV7H858iH=_cjdbRJC2+UTSsNSn2B9Pg@mail.gmail.com>
Message-ID: <52FE9988.8040407@gmail.com>

  The best place for general or even possibly-general lme4 questions is
r-sig-mixed-models at r-project.org : I'm forwarding this answer there now
...  You can go to
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models to subscribe.

On 14-02-14 01:42 PM, Colleen Arnison wrote:
> Hi Ben,
> 
> I have fit a glmer () model to binomial data with 12 fixed (various
> landscape variables)  and  2 random effects (elk ID and Area).
> 
> I need to apply (ie predict) this model to new data (a subset of the
> original data I used to make it) and having some issues. 
> 
> I found on a few help forums to download the newest version of lme4 from
> your GigHub website (https://github.com/lme4/lme4/) and that made the
> predict () method for merMod objects work (GREAT!!!), however, when I
> then try to run the full script to create the original glmer model an
> error occurs (Warning message: In mer_finalize(ans) : false convergence
> (8)) which never occurred when I use the original CRAN version of lme4. 

  This seems quite strange -- that error message comes from OLD
(pre-1.0) versions of lme4.  Somehow you're loading different versions
of lme4.  Have you started from a clean R session?  What are the results
of sessionInfo()?  What are the results of

inst.pkg <- installed.packages()
grep("lme",rownames(inst.pkg),value=TRUE)

?

> 
> Is there any other way around this?? Another way to predict a glmer
> model with new data, or make the function glmer work in the updated lme4
> on GigHub??
> 
> Thanks so much for your help,
> Colleen
> 
> PS sorry I am emailing you directly, I am new to R and discussion groups
> and did not know where I could post this question.
> 
> 
> model<- glmer(USE~Cover+Elevation+slope+
>              prisk_s+BrnAge+FireSmt+NDVI+BurnCat+SummerUse+
>              Type*DistLinear+DWater+
>              (1|Area) + (1|ANIMAL_ID), data=data, family=binomial)
> 
> testData <- data[sample(nrow(data))[1:10000], ] 
> 
> predict(model, newdata=testData)


From j.hadfield at ed.ac.uk  Sat Feb 15 00:01:05 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Fri, 14 Feb 2014 23:01:05 +0000
Subject: [R-sig-ME] MCMCglmm and massing records in relation to ainv
In-Reply-To: <CANXb-o5_g0zm8jpOGkFndWGNmu2DSzmTt_-Z_5J7DXJ=kaae2g@mail.gmail.com>
References: <CANXb-o5_g0zm8jpOGkFndWGNmu2DSzmTt_-Z_5J7DXJ=kaae2g@mail.gmail.com>
Message-ID: <20140214230105.8207415x1o9d0m04@www.staffmail.ed.ac.uk>

Hi Szymek,

I'm aware of this problem.  The current work around is to create a  
record in the data.frame for the internal node and set the response to  
NA and the covariates (including fixed_eff) to anything you like.  The  
next MCMCghlmm update (probably next week) will fix this - or I can  
send you a version now if you like?

Cheers,

Jarrod




Quoting Szymek Drobniak <szymonmarian.drobniak at uzh.ch> on Thu, 13 Feb  
2014 14:45:52 +0100:

> Hello,
>
> I know that MCMCglmm - when using pedigree/phylogeny data - generates dummy
> entries for all phantom nodes in the tree that were introduced in the
> process of creating the pedigree. I'm currently working on a series of
> phylo-corrected models and have a problem: although my data does provide
> data for all tips of the tree I use - and both the data and tree have the
> same sets of species (anyway, otherwise it would generate another error) -
> but still I'm getting the warning  message that 1 additional record was
> generated. This prevents me from fitting residual structures of the kind
> rcov=~idh(fixed_eff):units since then it claims that the R-structure does
> not define unique residual for each data point.
>
> Does anyone has ideas how could I add this missing record to my data (i.e.
> what this stands for?)
>
> Cheers
> szymek
>
> --
>
> *Dr Szymon Drobniak*
> Anthropological Institute and Museum
> Office Y42-K-66
> University of Z?rich - Irchel
> Winterthurerstrasse 190
> CH-8057 Z?rich, Switzerland
>
> 	[[alternative HTML version deleted]]
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From roby.joehanes at nih.gov  Sat Feb 15 00:07:33 2014
From: roby.joehanes at nih.gov (Joehanes, Roby (NIH/NHLBI) [F])
Date: Fri, 14 Feb 2014 23:07:33 +0000
Subject: [R-sig-ME] Pedigreemm on lme4-1.0
In-Reply-To: <52FE98D9.2010405@gmail.com>
References: <CF23BB16.8455%roby.joehanes@nih.gov> <52FE98D9.2010405@gmail.com>
Message-ID: <CF240879.84DD%roby.joehanes@nih.gov>

Hi Ben:

The package could install and the examples in the manual could run.
However, all the examples given in the pedigreemm technical paper do not:
milk <- within(milk, sdMilk <- milk / sd(milk))
fm1 <- pedigreemm(sdMilk ~ lact + log(dim) + (1|id) + (1|herd), data =
milk, pedigree = list(id = pedCowsR))

The error message is:
Error in .sortCsparse(.Call(dtCMatrix_sparse_solve, a, b)) :
  Dimensions of system to be solved are inconsistent

The same goes to these examples (same error message):
milk <- within(milk, idPE <- id)
fm2 <- pedigreemm(sdMilk ~ lact + (1|id) + log(dim) + (1|idPE) + (1|herd),
data = milk, pedigree = list(id = pedCows))

fm3 <- pedigreemm(NCM ~ birth + calvingYear + (1|sire) + (1|herd),  data =
mastitis, pedigree = list(sire = pedSires), family = "poisson")

fm4 <- pedigreemm(mastitis ~ birth + calvingYear + (1|sire) + (1|herd),
data = mastitis, pedigree = list(sire = pedSires),


cf: http://www.journalofanimalscience.org/content/88/2/497.full.pdf+html

Updating/recompiling Rcpp, RcppEigen, and lme4 do not help. I see no test
codes at all in pedigreemm-0.3-1 package. As such, pedigreemm at the
present state could only be used for toy examples. Or this could be a
problem exclusively on my side.

Plus, I remember that I submitted a patch to allow specification of one
observation per ID. It seems that the patch did not transfer over. I
always get this error message:
Error in checkNlevels(reTrms$flist, n = n, control) :
  number of levels of each grouping factor must be < number of observations
http://r-forge.r-project.org/tracker/index.php?func=detail&aid=1928&group_i
d=60&atid=300


I can try to fix this problem, but I will need guidance from you and
others since I have been absent from the lme4 development for 1.5 years
now. The patch above is now obsolete with lme4-v1.0.

Sincerely,
Roby





On 2/14/14 5:29 PM, "Ben Bolker" <bbolker at gmail.com> wrote:

>On 14-02-14 12:22 PM, Joehanes, Roby (NIH/NHLBI) [F] wrote:
>> Hi all:
>> 
>> Just wondering if there is any people working on pedigreemm so that
>> it can run on the new lme4 version 1.0? In current version of lme4
>> 1.0, pedigreemm is broken. May I know the latest status on
>> pedigreemm?
>> 
>> Thanks, Roby
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> 
>
>  I'm a little surprised, because the current CRAN page suggests it's
>OK: http://cran.r-project.org/web/packages/pedigreemm/index.html -- this
>page wouldn't exist, I think, if the package didn't at least pass its
>checks with the new version of lme4 (and it says that it requires lme4
>>= 1.0).
>   Perhaps you could provide a little more information for those of us
>who don't use pedigreemm on a regular basis -- does it fail to install?
> Installs but doesn't pass checks/examples fail?  Examples run but give
>incorrect answers?  Some particular cases don't work?
>   Posting here seems perfectly reasonable, but I would also suggest
>contacting the maintainer (listed at the URL above) as well.  If that
>maintainer is unwilling or unable to fix whatever's broken, then
>presumably someone else will have to step forward to act as maintainer ...
>
>  I'm willing to help fix what's broken, in particular on the
>lme4-interface end, but would need to get *specific* examples of what's
>not working ...
>
>  Ben Bolker


From bbolker at gmail.com  Sat Feb 15 03:17:15 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 14 Feb 2014 21:17:15 -0500
Subject: [R-sig-ME] Pedigreemm on lme4-1.0
In-Reply-To: <CF240879.84DD%roby.joehanes@nih.gov>
References: <CF23BB16.8455%roby.joehanes@nih.gov> <52FE98D9.2010405@gmail.com>
	<CF240879.84DD%roby.joehanes@nih.gov>
Message-ID: <52FECE2B.9020109@gmail.com>

On 14-02-14 06:07 PM, Joehanes, Roby (NIH/NHLBI) [F] wrote:
> Hi Ben:
> 
> The package could install and the examples in the manual could run.
> However, all the examples given in the pedigreemm technical paper do not:
> milk <- within(milk, sdMilk <- milk / sd(milk))
> fm1 <- pedigreemm(sdMilk ~ lact + log(dim) + (1|id) + (1|herd), data =
> milk, pedigree = list(id = pedCowsR))
> 
> The error message is:
> Error in .sortCsparse(.Call(dtCMatrix_sparse_solve, a, b)) :
>   Dimensions of system to be solved are inconsistent
> 
> The same goes to these examples (same error message):
> milk <- within(milk, idPE <- id)
> fm2 <- pedigreemm(sdMilk ~ lact + (1|id) + log(dim) + (1|idPE) + (1|herd),
> data = milk, pedigree = list(id = pedCows))
> 
> fm3 <- pedigreemm(NCM ~ birth + calvingYear + (1|sire) + (1|herd),  data =
> mastitis, pedigree = list(sire = pedSires), family = "poisson")
> 
> fm4 <- pedigreemm(mastitis ~ birth + calvingYear + (1|sire) + (1|herd),
> data = mastitis, pedigree = list(sire = pedSires),
> 
> 
> cf: http://www.journalofanimalscience.org/content/88/2/497.full.pdf+html
> 
> Updating/recompiling Rcpp, RcppEigen, and lme4 do not help. I see no test
> codes at all in pedigreemm-0.3-1 package. As such, pedigreemm at the
> present state could only be used for toy examples. Or this could be a
> problem exclusively on my side.

  I don't think so, I can replicate your first example.
> 
> Plus, I remember that I submitted a patch to allow specification of one
> observation per ID. It seems that the patch did not transfer over. I
> always get this error message:
> Error in checkNlevels(reTrms$flist, n = n, control) :
>   number of levels of each grouping factor must be < number of observations
> http://r-forge.r-project.org/tracker/index.php?func=detail&aid=1928&group_i
> d=60&atid=300

  Your patch was appreciated, but superseded by fairly major
architectural changes we made. There is a fairly straightforward way now
to bypass this test:

control=lmerControl(check.nobs.vs.nlev="ignore")

This really should be done within the call in pedigreemm, but it can be
enabled globally as follows:

options(lmerControl=list(check.nobs.vs.nlev="ignore"))

> I can try to fix this problem, but I will need guidance from you and
> others since I have been absent from the lme4 development for 1.5 years
> now. The patch above is now obsolete with lme4-v1.0.
> 
> Sincerely,
> Roby
> 
> 
> 
> 
> 
> On 2/14/14 5:29 PM, "Ben Bolker" <bbolker at gmail.com> wrote:
> 
>> On 14-02-14 12:22 PM, Joehanes, Roby (NIH/NHLBI) [F] wrote:
>>> Hi all:
>>>
>>> Just wondering if there is any people working on pedigreemm so that
>>> it can run on the new lme4 version 1.0? In current version of lme4
>>> 1.0, pedigreemm is broken. May I know the latest status on
>>> pedigreemm?
>>>
>>> Thanks, Roby
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>
>>  I'm a little surprised, because the current CRAN page suggests it's
>> OK: http://cran.r-project.org/web/packages/pedigreemm/index.html -- this
>> page wouldn't exist, I think, if the package didn't at least pass its
>> checks with the new version of lme4 (and it says that it requires lme4
>>> = 1.0).
>>   Perhaps you could provide a little more information for those of us
>> who don't use pedigreemm on a regular basis -- does it fail to install?
>> Installs but doesn't pass checks/examples fail?  Examples run but give
>> incorrect answers?  Some particular cases don't work?
>>   Posting here seems perfectly reasonable, but I would also suggest
>> contacting the maintainer (listed at the URL above) as well.  If that
>> maintainer is unwilling or unable to fix whatever's broken, then
>> presumably someone else will have to step forward to act as maintainer ...
>>
>>  I'm willing to help fix what's broken, in particular on the
>> lme4-interface end, but would need to get *specific* examples of what's
>> not working ...
>>
>>  Ben Bolker
>


From thlytras at gmail.com  Mon Feb 17 11:02:19 2014
From: thlytras at gmail.com (Theodore Lytras)
Date: Mon, 17 Feb 2014 12:02:19 +0200
Subject: [R-sig-ME] Huber-White standard errors for glmer Poisson fits
Message-ID: <5857029.xL9nhObg6x@equinox2>

Dear list,

I am a PhD student in epidemiology, trying to analyze a multi-site cohort 
study with R. I have a series of mixed-effects Poisson model objects fitted 
with glmer(), and I am trying to find a way to calculate Huber-White robust 
standard errors for these.

The only program I found that calculates Huber-White SEs for mixed-model fits 
is GLLAMM in Stata, so I would like to reproduce this functionality in R.

I took as a starting point the package "sandwich" by Achim Zeileis, which 
unfortunately provides a sandwich covariance matrix only for glm objects. So I 
tried to modify his code (functions estfun.glm(), bread.glm() and sandwich()) 
and the function robust.se() found in [1] to make it work with glmer objects, 
simplifying things as much as possible (focusing only on Poisson fits). 

Thus I came up with the code in [2]. Function robustSEglmm() takes a glmer fit 
with family="Poisson" and the associated clustering variable and tries to 
compute Huber-White standard errors.

What I found was the following: if estfunGlmm() uses the "working" residuals 
(provided by residuals.merMod()), as is the case in the original estfun.glm() 
and explained in the "sandwich" package documentation, the resulting SEs are 
far too large, 6-10 times as those produced by GLLAMM.

However, after reading a few things about the various types of residuals in 
[3], I tried replacing the working residuals in estfunGlmm() with "score" 
residuals, and to my surprise I obtained SEs very close to those returned by 
GLLAMM (?1%).

So my question is: is this approach statistically correct and valid?

Since I can't say I really understand the maths involved (I'm a medical doctor 
by profession), I turned to Achim Zeileis for help. Cutting and pasting from 
his answer:

[quote]

Well, one needs to work out the "right" residuals: The estfun() needs to 
be the derivative of the log-likelihood with respect to the parameters. 
And the bread() essentially the second derivative.

Given that the derivative of the log-likelihood is also called the score 
function, it may well be that the "score residuals" that you mention above 
are what you are looking for. But I'm not familiar enough with the model 
classes and their implementation in R to know for sure.

[/quote]

So, could someone more familiar with the internals of lme4 shed more light on 
the issue? If this approach to calculate Huber-White standard error is indeed 
correct, I guess it would be useful to many people beside myself (I've seen 
this discussed before on this list) and would extend the capabilities of R.

Thank you all in advance,

Theodore Lytras


[1] http://diffuseprior.wordpress.com/2013/01/12/the-cluster-bootstrap/
[2] https://gist.github.com/anonymous/9046934
[3] 
http://books.google.com/books?id=tOeqO6Hs-6gC&lpg=PA53&vq=residuals&hl=el&pg=PA55#v=onepage&q&f=false


From bbolker at gmail.com  Mon Feb 17 18:08:01 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 17 Feb 2014 12:08:01 -0500
Subject: [R-sig-ME] Huber-White standard errors for glmer Poisson fits
In-Reply-To: <5857029.xL9nhObg6x@equinox2>
References: <5857029.xL9nhObg6x@equinox2>
Message-ID: <530241F1.5080706@gmail.com>

  This looks reasonable to me (at least following Hardin and Hilbe's
definition of the score residual).  For the Poisson distribution, it
turns out that the score residuals and the raw residuals are the same
(because (1/d(eta)/d(mu))) = Var(mu) in this case).

  Here's a function (not thoroughly tested) that computes score
residuals more generally, at the price of using some information about
the internal structure of the object -- it's a fairly little translation
of Hardin and Hilbe's definition.

scoreResids <- function(object,...) {
    ff <- object at resp$family
    eta.mu <- 1/ff$mu.eta(predict(object))
    y <- object at resp$y
    mu <- object at resp$mu
    (y-mu)/ff$var(mu)/eta.mu
}

 Otherwise, though, I haven't had the time to work through to see why
working residuals are used in estfun.glm.  I'm guessing that the issue
is actually a difference in the lme4:::weights.merMod function, which
doesn't actually have a "working" option (but the option is silently
ignored !!) -- I think that if one implemented 'working' rather than
'prior' weights, that this would probably work in parallel with estfun.glm.

  I agree that your functions will be very useful.

 Ben Bolker


On 14-02-17 05:02 AM, Theodore Lytras wrote:
> Dear list,
> 
> I am a PhD student in epidemiology, trying to analyze a multi-site cohort 
> study with R. I have a series of mixed-effects Poisson model objects fitted 
> with glmer(), and I am trying to find a way to calculate Huber-White robust 
> standard errors for these.
> 
> The only program I found that calculates Huber-White SEs for mixed-model fits 
> is GLLAMM in Stata, so I would like to reproduce this functionality in R.
> 
> I took as a starting point the package "sandwich" by Achim Zeileis, which 
> unfortunately provides a sandwich covariance matrix only for glm objects. So I 
> tried to modify his code (functions estfun.glm(), bread.glm() and sandwich()) 
> and the function robust.se() found in [1] to make it work with glmer objects, 
> simplifying things as much as possible (focusing only on Poisson fits). 
> 
> Thus I came up with the code in [2]. Function robustSEglmm() takes a glmer fit 
> with family="Poisson" and the associated clustering variable and tries to 
> compute Huber-White standard errors.
> 
> What I found was the following: if estfunGlmm() uses the "working" residuals 
> (provided by residuals.merMod()), as is the case in the original estfun.glm() 
> and explained in the "sandwich" package documentation, the resulting SEs are 
> far too large, 6-10 times as those produced by GLLAMM.
> 
> However, after reading a few things about the various types of residuals in 
> [3], I tried replacing the working residuals in estfunGlmm() with "score" 
> residuals, and to my surprise I obtained SEs very close to those returned by 
> GLLAMM (?1%).
> 
> So my question is: is this approach statistically correct and valid?
> 
> Since I can't say I really understand the maths involved (I'm a medical doctor 
> by profession), I turned to Achim Zeileis for help. Cutting and pasting from 
> his answer:
> 
> [quote]
> 
> Well, one needs to work out the "right" residuals: The estfun() needs to 
> be the derivative of the log-likelihood with respect to the parameters. 
> And the bread() essentially the second derivative.
> 
> Given that the derivative of the log-likelihood is also called the score 
> function, it may well be that the "score residuals" that you mention above 
> are what you are looking for. But I'm not familiar enough with the model 
> classes and their implementation in R to know for sure.
> 
> [/quote]
> 
> So, could someone more familiar with the internals of lme4 shed more light on 
> the issue? If this approach to calculate Huber-White standard error is indeed 
> correct, I guess it would be useful to many people beside myself (I've seen 
> this discussed before on this list) and would extend the capabilities of R.
> 
> Thank you all in advance,
> 
> Theodore Lytras
> 
> 
> [1] http://diffuseprior.wordpress.com/2013/01/12/the-cluster-bootstrap/
> [2] https://gist.github.com/anonymous/9046934
> [3] 
> http://books.google.com/books?id=tOeqO6Hs-6gC&lpg=PA53&vq=residuals&hl=el&pg=PA55#v=onepage&q&f=false
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From steve.walker at utoronto.ca  Tue Feb 18 00:58:47 2014
From: steve.walker at utoronto.ca (Steve Walker)
Date: Mon, 17 Feb 2014 18:58:47 -0500
Subject: [R-sig-ME] Huber-White standard errors for glmer Poisson fits
In-Reply-To: <530241F1.5080706@gmail.com>
References: <5857029.xL9nhObg6x@equinox2> <530241F1.5080706@gmail.com>
Message-ID: <5302A237.9060005@utoronto.ca>

If it helps, I have just pushed a new lme4:::weights.merMod to the 
development version of lme4, that does allow a "working" option:

https://github.com/lme4/lme4/commit/9953d93d1837

Cheers,
Steve

On 2/17/2014, 12:08 PM, Ben Bolker wrote:
>    This looks reasonable to me (at least following Hardin and Hilbe's
> definition of the score residual).  For the Poisson distribution, it
> turns out that the score residuals and the raw residuals are the same
> (because (1/d(eta)/d(mu))) = Var(mu) in this case).
>
>    Here's a function (not thoroughly tested) that computes score
> residuals more generally, at the price of using some information about
> the internal structure of the object -- it's a fairly little translation
> of Hardin and Hilbe's definition.
>
> scoreResids <- function(object,...) {
>      ff <- object at resp$family
>      eta.mu <- 1/ff$mu.eta(predict(object))
>      y <- object at resp$y
>      mu <- object at resp$mu
>      (y-mu)/ff$var(mu)/eta.mu
> }
>
>   Otherwise, though, I haven't had the time to work through to see why
> working residuals are used in estfun.glm.  I'm guessing that the issue
> is actually a difference in the lme4:::weights.merMod function, which
> doesn't actually have a "working" option (but the option is silently
> ignored !!) -- I think that if one implemented 'working' rather than
> 'prior' weights, that this would probably work in parallel with estfun.glm.
>
>    I agree that your functions will be very useful.
>
>   Ben Bolker
>
>
> On 14-02-17 05:02 AM, Theodore Lytras wrote:
>> Dear list,
>>
>> I am a PhD student in epidemiology, trying to analyze a multi-site cohort
>> study with R. I have a series of mixed-effects Poisson model objects fitted
>> with glmer(), and I am trying to find a way to calculate Huber-White robust
>> standard errors for these.
>>
>> The only program I found that calculates Huber-White SEs for mixed-model fits
>> is GLLAMM in Stata, so I would like to reproduce this functionality in R.
>>
>> I took as a starting point the package "sandwich" by Achim Zeileis, which
>> unfortunately provides a sandwich covariance matrix only for glm objects. So I
>> tried to modify his code (functions estfun.glm(), bread.glm() and sandwich())
>> and the function robust.se() found in [1] to make it work with glmer objects,
>> simplifying things as much as possible (focusing only on Poisson fits).
>>
>> Thus I came up with the code in [2]. Function robustSEglmm() takes a glmer fit
>> with family="Poisson" and the associated clustering variable and tries to
>> compute Huber-White standard errors.
>>
>> What I found was the following: if estfunGlmm() uses the "working" residuals
>> (provided by residuals.merMod()), as is the case in the original estfun.glm()
>> and explained in the "sandwich" package documentation, the resulting SEs are
>> far too large, 6-10 times as those produced by GLLAMM.
>>
>> However, after reading a few things about the various types of residuals in
>> [3], I tried replacing the working residuals in estfunGlmm() with "score"
>> residuals, and to my surprise I obtained SEs very close to those returned by
>> GLLAMM (?1%).
>>
>> So my question is: is this approach statistically correct and valid?
>>
>> Since I can't say I really understand the maths involved (I'm a medical doctor
>> by profession), I turned to Achim Zeileis for help. Cutting and pasting from
>> his answer:
>>
>> [quote]
>>
>> Well, one needs to work out the "right" residuals: The estfun() needs to
>> be the derivative of the log-likelihood with respect to the parameters.
>> And the bread() essentially the second derivative.
>>
>> Given that the derivative of the log-likelihood is also called the score
>> function, it may well be that the "score residuals" that you mention above
>> are what you are looking for. But I'm not familiar enough with the model
>> classes and their implementation in R to know for sure.
>>
>> [/quote]
>>
>> So, could someone more familiar with the internals of lme4 shed more light on
>> the issue? If this approach to calculate Huber-White standard error is indeed
>> correct, I guess it would be useful to many people beside myself (I've seen
>> this discussed before on this list) and would extend the capabilities of R.
>>
>> Thank you all in advance,
>>
>> Theodore Lytras
>>
>>
>> [1] http://diffuseprior.wordpress.com/2013/01/12/the-cluster-bootstrap/
>> [2] https://gist.github.com/anonymous/9046934
>> [3]
>> http://books.google.com/books?id=tOeqO6Hs-6gC&lpg=PA53&vq=residuals&hl=el&pg=PA55#v=onepage&q&f=false
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From thlytras at gmail.com  Tue Feb 18 01:48:35 2014
From: thlytras at gmail.com (Theodore Lytras)
Date: Tue, 18 Feb 2014 02:48:35 +0200
Subject: [R-sig-ME] Huber-White standard errors for glmer Poisson fits
In-Reply-To: <5302A237.9060005@utoronto.ca>
References: <5857029.xL9nhObg6x@equinox2> <530241F1.5080706@gmail.com>
	<5302A237.9060005@utoronto.ca>
Message-ID: <2009499.aLnXHCDgbF@equinox2>

I just tried your new weights.merMod(). 
Specifying "working" weights and "working" residuals gets me the exact same 
standard errors as with normal weights (all equal to 1) and "score" residuals, 
i.e. very close to what GLLAMM produces.
So I guess that was indeed the missing link - having a "working" option for 
weights.merMod(). Now one can easily extend estfun.glm() to work with merMod 
objects.

Thanks to both of you!

Theodore

???? ??? 17 ??? 2014 18:58:47 Steve Walker ??????:
> If it helps, I have just pushed a new lme4:::weights.merMod to the
> development version of lme4, that does allow a "working" option:
> 
> https://github.com/lme4/lme4/commit/9953d93d1837
> 
> Cheers,
> Steve
> 
> On 2/17/2014, 12:08 PM, Ben Bolker wrote:
> >    This looks reasonable to me (at least following Hardin and Hilbe's
> > 
> > definition of the score residual).  For the Poisson distribution, it
> > turns out that the score residuals and the raw residuals are the same
> > (because (1/d(eta)/d(mu))) = Var(mu) in this case).
> > 
> >    Here's a function (not thoroughly tested) that computes score
> > 
> > residuals more generally, at the price of using some information about
> > the internal structure of the object -- it's a fairly little translation
> > of Hardin and Hilbe's definition.
> > 
> > scoreResids <- function(object,...) {
> > 
> >      ff <- object at resp$family
> >      eta.mu <- 1/ff$mu.eta(predict(object))
> >      y <- object at resp$y
> >      mu <- object at resp$mu
> >      (y-mu)/ff$var(mu)/eta.mu
> > 
> > }
> > 
> >   Otherwise, though, I haven't had the time to work through to see why
> > 
> > working residuals are used in estfun.glm.  I'm guessing that the issue
> > is actually a difference in the lme4:::weights.merMod function, which
> > doesn't actually have a "working" option (but the option is silently
> > ignored !!) -- I think that if one implemented 'working' rather than
> > 'prior' weights, that this would probably work in parallel with
> > estfun.glm.
> > 
> >    I agree that your functions will be very useful.
> >   
> >   Ben Bolker
> > 
> > On 14-02-17 05:02 AM, Theodore Lytras wrote:
> >> Dear list,
> >> 
> >> I am a PhD student in epidemiology, trying to analyze a multi-site cohort
> >> study with R. I have a series of mixed-effects Poisson model objects
> >> fitted
> >> with glmer(), and I am trying to find a way to calculate Huber-White
> >> robust
> >> standard errors for these.
> >> 
> >> The only program I found that calculates Huber-White SEs for mixed-model
> >> fits is GLLAMM in Stata, so I would like to reproduce this functionality
> >> in R.
> >> 
> >> I took as a starting point the package "sandwich" by Achim Zeileis, which
> >> unfortunately provides a sandwich covariance matrix only for glm objects.
> >> So I tried to modify his code (functions estfun.glm(), bread.glm() and
> >> sandwich()) and the function robust.se() found in [1] to make it work
> >> with glmer objects, simplifying things as much as possible (focusing
> >> only on Poisson fits).
> >> 
> >> Thus I came up with the code in [2]. Function robustSEglmm() takes a
> >> glmer fit with family="Poisson" and the associated clustering variable
> >> and tries to compute Huber-White standard errors.
> >> 
> >> What I found was the following: if estfunGlmm() uses the "working"
> >> residuals (provided by residuals.merMod()), as is the case in the
> >> original estfun.glm() and explained in the "sandwich" package
> >> documentation, the resulting SEs are far too large, 6-10 times as those
> >> produced by GLLAMM.
> >> 
> >> However, after reading a few things about the various types of residuals
> >> in
> >> [3], I tried replacing the working residuals in estfunGlmm() with "score"
> >> residuals, and to my surprise I obtained SEs very close to those returned
> >> by GLLAMM (?1%).
> >> 
> >> So my question is: is this approach statistically correct and valid?
> >> 
> >> Since I can't say I really understand the maths involved (I'm a medical
> >> doctor by profession), I turned to Achim Zeileis for help. Cutting and
> >> pasting from his answer:
> >> 
> >> [quote]
> >> 
> >> Well, one needs to work out the "right" residuals: The estfun() needs to
> >> be the derivative of the log-likelihood with respect to the parameters.
> >> And the bread() essentially the second derivative.
> >> 
> >> Given that the derivative of the log-likelihood is also called the score
> >> function, it may well be that the "score residuals" that you mention
> >> above
> >> are what you are looking for. But I'm not familiar enough with the model
> >> classes and their implementation in R to know for sure.
> >> 
> >> [/quote]
> >> 
> >> So, could someone more familiar with the internals of lme4 shed more
> >> light on the issue? If this approach to calculate Huber-White standard
> >> error is indeed correct, I guess it would be useful to many people
> >> beside myself (I've seen this discussed before on this list) and would
> >> extend the capabilities of R.
> >> 
> >> Thank you all in advance,
> >> 
> >> Theodore Lytras
> >> 
> >> 
> >> [1] http://diffuseprior.wordpress.com/2013/01/12/the-cluster-bootstrap/
> >> [2] https://gist.github.com/anonymous/9046934
> >> [3]
> >> http://books.google.com/books?id=tOeqO6Hs-6gC&lpg=PA53&vq=residuals&hl=el
> >> &pg=PA55#v=onepage&q&f=false


From jackiewood7 at gmail.com  Tue Feb 18 16:33:51 2014
From: jackiewood7 at gmail.com (Jackie Wood)
Date: Tue, 18 Feb 2014 10:33:51 -0500
Subject: [R-sig-ME] Modelling heterogeneous variances for an interaction
 term in MCMCglmm?
In-Reply-To: <20140214065304.1273259w67fud6ro@www.staffmail.ed.ac.uk>
References: <CAOxxGRnK+m-mJ4zYQpPXEojPPzbd8dHiXNVvYqXuMdCi8eRx8g@mail.gmail.com>
	<20140213101532.11034r2dom8x34n4@www.staffmail.ed.ac.uk>
	<CAOxxGRk+r60KQPJtH=L6MhiMcBJTUfkVcDKr6U2EUh5j9i1S3Q@mail.gmail.com>
	<20140213182819.56013repu2i1lps0@www.staffmail.ed.ac.uk>
	<CAOxxGRkHjcujpgHnwivaBpmeH9-ZJW4C5-1CkSA68AG+nRHZdg@mail.gmail.com>
	<20140214065304.1273259w67fud6ro@www.staffmail.ed.ac.uk>
Message-ID: <CAOxxGR=AA1g0HEqP-Bsx9eVDMsT5gMfhrrtnPesOsWxKMjVvOw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140218/0095df71/attachment.pl>

From j.hadfield at ed.ac.uk  Tue Feb 18 18:20:11 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Tue, 18 Feb 2014 17:20:11 +0000
Subject: [R-sig-ME] Modelling heterogeneous variances for an interaction
 term in MCMCglmm?
In-Reply-To: <CAOxxGR=AA1g0HEqP-Bsx9eVDMsT5gMfhrrtnPesOsWxKMjVvOw@mail.gmail.com>
References: <CAOxxGRnK+m-mJ4zYQpPXEojPPzbd8dHiXNVvYqXuMdCi8eRx8g@mail.gmail.com>
	<20140213101532.11034r2dom8x34n4@www.staffmail.ed.ac.uk>
	<CAOxxGRk+r60KQPJtH=L6MhiMcBJTUfkVcDKr6U2EUh5j9i1S3Q@mail.gmail.com>
	<20140213182819.56013repu2i1lps0@www.staffmail.ed.ac.uk>
	<CAOxxGRkHjcujpgHnwivaBpmeH9-ZJW4C5-1CkSA68AG+nRHZdg@mail.gmail.com>
	<20140214065304.1273259w67fud6ro@www.staffmail.ed.ac.uk>
	<CAOxxGR=AA1g0HEqP-Bsx9eVDMsT5gMfhrrtnPesOsWxKMjVvOw@mail.gmail.com>
Message-ID: <20140218172011.1920452uxp2wokmc@www.staffmail.ed.ac.uk>

Hi Jackie,

1/ you don't need to recode pop.bin in the csv file.  The intercept in  
the model is the mu term for pop.bin #1, the intercept + the second  
coefficient in the model is the mu term for pop.bin #2. Alternatively,  
you could fit ~pop.bin-1 rather than ~pop.bin in the fixed part of the  
model. In this case the four fixed effects are the four mu terms  
without having to add things together.

2/ You don't need the var.fnorm function if you just want the mean magnitude

3/ You can condition on any random term you like. For example, if you  
wanted to know what the magnitude within a study was, but you wanted  
it averaged over populations you would discard the study variance.  
However, if you wanted to know  what the magnitude for a particular  
study was then you should add the study random effect (not the  
variance but the effect, so use pr=TRUE in the call o MCMCglmm) to the  
mu term.  If you think that the residual variation is not biological  
variation (e.g. measurement error) then you should leave it out of the  
calculations. This is what Michael and I did.

Cheers,

Jarrod





Quoting Jackie Wood <jackiewood7 at gmail.com> on Tue, 18 Feb 2014  
10:33:51 -0500:

> Hi Jarrod,
>
>  I discussed the code you provided last week with a colleague, and we just
> want to be sure that we are interpreting and executing it properly. As an
> example, say we specify a simple model with only pop.bin (which has four
> levels) allowing for heterogeneous variances at each level:
>
> prior1=list(R=list(V=diag(4),n=0.002),G=list(G1=list(V=diag(4),n=0.002),G2=list(V=diag(4),n=0.002)))
> model1=MCMCglmm(grad.lin.value~pop.bin,mev=mev,data=data,prior=prior1,random=~idh(pop.bin):study.IDR+idh(pop.bin):pop.IDR,rcov=~idh(pop.bin):units,nitt=300000,burnin=100000,thin=50,family="gaussian")
>
> In the object model$Sol there are four columns and we are interested in the
> first (the intercept, which corresponds to population bin #1) hence we use
> "model1$Sol[,1]" as you said.
>
> If we include all the sources of variance (study, pop, and units), we
> specify a vector of the column numbers that contain the random effects
> variance estimates for the intercept:
>
> m1 = c(1, 5, 10)
> model1$VCV[,m1]
>
> We then estimate the posterior mode and HPD credible intervals on the
> folded normal distribution with the following:
>
> mu.fnorm<-function(mu,sigma){dnorm(mu,0,sigma)*2*sigma^2+mu*(2*pnorm(mu,0,sigma)-1)}
>
> newMode = mu.fnorm(model1$Sol[,1], sqrt(rowSums(model1$VCV[,m1])))
> HPDinterval(newMode)
> posterior.mode(newMode)
>
>  If we are correct, the resulting output should be the posterior mode and
> HPD credible intervals for the magnitude of selection for population bin #1?
>
>  If we wanted to then calculate the magnitude of selection for population
> bin #2, we would recode the bins in the CSV file such that population bin
> #2 becomes the intercept and then follow the same method as for population
> bin #1 to obtain the magnitude of selection for population bin #2 (there
> could be an easier way to do this of course, but we just want to make sure
> this is getting at what we're interested in).
>
>
> There is an additional function provided by Mr. Morrissey, "var.fnorm",
> which gives the variance of selection under the folded normal but we are
> assuming that this is not necessary to compute the magnitude of selection
> and its confidence intervals (we only need the mu.fnorm function for that)
> and it is simply useful if one is also interested in what the variance of
> selection is in each of the pop size bins.
>
>
> Finally, we were hoping if you could clarify one more issue for us
> regarding which source of variance to include. Our model is structured to
> produce three variance estimates: variance attributable to study,
> population, and the residual variance. Study and population essentially
> represent "nuisance parameters" to us that were included in the model to
> account for issues of non-independence in our dataset. If we were
> interested in calculating the magnitude of selection after accounting for
> these sources of variation, should we include only the residual variance
> estimate as "sigma" in the mu.fnorm function? Or should we still include
> the estimates of study-level and population-level variance in our final
> calculations?
>
>
> Jackie
>
>
> On Fri, Feb 14, 2014 at 1:53 AM, Jarrod Hadfield <j.hadfield at ed.ac.uk>wrote:
>
>> Hi Jackie,
>>
>> The function is taking the standard deviation not the variance. You also
>> need to decide what sources of variance to include. Lets imagine you want
>> the expected absolute value of a random draw from the first of 12
>> combinations, and you want to include the variance due to the two random
>> terms and the residual. Then,
>>
>> mu.fnorm(m1$Sol[,1], sqrt(rowSums(m1$VCV[,seq(1,12*3,12)])))
>>
>> or something close to it should work.
>>
>> Jarrod
>>
>>
>>
>>
>> Quoting Jackie Wood <jackiewood7 at gmail.com> on Thu, 13 Feb 2014 18:41:34
>> -0500:
>>
>>  Hi Jarrod,
>>>
>>> Thanks so much for the information regarding R-structure and model
>>> specification. Your explanation for that cleared things up substantially.
>>>
>>> In regards to the folded normal, I completely understand what you're
>>> saying
>>> though I'm unsure how that would be coded in MCMCglmm. My experience with
>>> MCMCglmm thus far has been estimating heritability and Qst so I might
>>> think
>>> it would be something along the lines of:
>>>
>>> posterior.fnorm<-mu.fnorm(model$Sol,model$VCV),
>>> var.fnorm(model$Sol,model$VCV)
>>> posterior.mode(posterior.fnorm)
>>> HPDinterval(posterior.fnorm,0.95)
>>>
>>> The first line of code doesn't work (and is probably pretty messed up),
>>> but
>>> I'm just wondering if it is in the right direction. I think evaluating the
>>> functions for the folded normal for each MCMC iteration as you suggested
>>> involves applying "mu.fnorm" and "var.fnorm" to the posterior distribution
>>> of solutions and the posterior distribution of variance matrices given by
>>> an MCMCglmm model but how the two functions would be combined to give
>>> "posterior.fnorm" in this case is not clear to me.
>>>
>>> Thanks,
>>> Jackie
>>>
>>>
>>>
>>>
>>> On Thu, Feb 13, 2014 at 1:28 PM, Jarrod Hadfield <j.hadfield at ed.ac.uk
>>> >wrote:
>>>
>>>  Hi Jackie,
>>>>
>>>> The reason that it does not run is that each observation needs to be
>>>> associated with a single effect. For ease, imagine two factors fac1 and
>>>> fac2 have 2-levels each (A and B, and C and D respectively). MCMCglmm
>>>> removes the intercept from formula inside variance functions (e.g. idh)
>>>> so
>>>> the R-structure model is
>>>>
>>>> fac1*fac2-1
>>>>
>>>> which for the four types of observation gives:
>>>>
>>>>    fac1A fac1B fac2D fac1B:fac2D
>>>> AC     1     0     0           0
>>>> AD     1     0     1           0
>>>> BC     0     1     0           0
>>>> BD     0     1     1           1
>>>>
>>>> This means that the (residual) variance for AD is actually
>>>> V[1,1]+V[3,3]+V[1,3]+V[3,1] where V is the estimated covariance matrix.
>>>> This makes interpretation difficult, and actually MCMCglmm does not allow
>>>> you to fit this type of R-structure (although it does allow the
>>>> G-structures to be of this form).
>>>>
>>>>
>>>> Perhaps a better way of doing it is to fit idh(fac1:fac2) because this
>>>> gives
>>>>
>>>>    fac1A:fac2C fac1A:fac2D fac1B:fac2C fac1B:fac2D
>>>> AC           1           0           0           0
>>>> AD           0           1           0           0
>>>> BC           0           0           1           0
>>>> BD           0           0           0           1
>>>>
>>>> which means that there is a one-to-one mapping between the diagonal
>>>> elements of V and the variance within each factor combination. MCMCglmm
>>>> will allow you to fit this type of R-structure. The next version of
>>>> MCMCglmm (which will probably be released next week) will issue more
>>>> sensible warnings when invalid R-structures are specified.
>>>>
>>>> Note however, that if you fit idh(fac1:fac2) for all sources of variance
>>>> (residual and random effects) and you have fac1:fac2-1 in the fixed
>>>> formula
>>>> then the analysis is exactly equivalent to fitting separate models to the
>>>> data from each factor combination.
>>>>
>>>> Applying a function to a posterior distribution results in a valid new
>>>> posterior distribution. It therefore makes more sense to evaluate the
>>>> functions for the folded normal for each MCMC iteration to give a
>>>> posterior
>>>> distribution for the expected absolute value of selection. This is
>>>> advantageous because mean(f(x)) is not always equal to f(mean(x)) when f
>>>> is
>>>> non-linear and it also allows you to use things like HPDinterval in order
>>>> to determine the uncertainty in your inferences. Note that it is hard to
>>>> know what influence a prior placed directly on x will do for inferences
>>>> about x, let alone on f(x).
>>>>
>>>> Cheers,
>>>>
>>>> Jarrod
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> Quoting Jackie Wood <jackiewood7 at gmail.com> on Thu, 13 Feb 2014 12:44:24
>>>> -0500:
>>>>
>>>>  Hi Jarrod,
>>>>
>>>>>
>>>>> Thanks for your response! I made the change you suggested and the model
>>>>> stopped after 3000 iterations and asked for a stronger prior, I used
>>>>> n=1,
>>>>> and it seemed to run ok after that.
>>>>>
>>>>> I was wondering if I might impose on you to ask a follow-up question? My
>>>>> response variable "grad.lin" are linear selection gradients and
>>>>> therefore
>>>>> have directionality, but we are also interested in the magnitude. So we
>>>>> are
>>>>> essentially investigating the potential effect of population size on
>>>>> selection. I have read a number of previous articles and syntheses
>>>>> regarding selection and understand that this requires the use of a
>>>>> folded
>>>>> normal distribution. In fact, one of those articles was one that you
>>>>> collaborated on with Michael Morrissey. I contacted Dr. Morrissey, and
>>>>> he
>>>>> was kind enough to provide an example of functions that could be used to
>>>>> obtain the corresponding values for the magnitude of selection once the
>>>>> posterior mode and variance were obtained from an MCMCglmm model using
>>>>> the
>>>>> selection gradients "straight up" as it were. The functions were as
>>>>> follows:
>>>>>
>>>>> mu.fnorm<-function(mu,sigma){dnorm(mu,0,sigma)*2*sigma^2+
>>>>> mu*(2*pnorm(mu,0,sigma)-1)}
>>>>> var.fnorm<-function(mu,sigma){mu^2+sigma^2-(sigma*sqrt(2/pi)
>>>>> *exp((-1*mu^2)/(2*sigma^2))+mu*(1-2*pnorm(-1*mu/sigma,0,1)))^2}
>>>>>
>>>>> I assumed on my own that the value for the posterior mode of selection
>>>>> that
>>>>> is plugged into the functions above is given for each level of "pop.bin"
>>>>> in
>>>>> the object:
>>>>>
>>>>> posterior.mode(model$Sol)
>>>>>
>>>>> and that the posterior variance of selection to use in the functions
>>>>> comes
>>>>> from the units variance (after the random effects "study" and "pop"  are
>>>>> accounted for) given in:
>>>>>
>>>>> posterior.mode(model$VCV)
>>>>>
>>>>> In any case, to make a long story short, the reason why I was attempting
>>>>> to
>>>>> use the prior with "R=list(V=diag(12),n=0.002)" is to obtain estimates
>>>>> of
>>>>> units variance for the different levels of the interaction term for use
>>>>> in
>>>>> the folded normal functions to obtain values for the magnitude of
>>>>> selection. So the change to the model you provided works, but only one
>>>>> overall estimate of units variance is given.
>>>>>
>>>>> Anyway, perhaps this is not a problem if I am totally incorrect in the
>>>>> assumption that units variance is what I'm looking for.
>>>>>
>>>>> I checked the data and all three of the groups (fish, plants, and birds)
>>>>> are represented in each of the pop.size bins though it's admittedly not
>>>>> an
>>>>> even spread. For example, there is more data for birds than for fish or
>>>>> plants in each of the bins.
>>>>>
>>>>> Thanks again,
>>>>> Jackie
>>>>>
>>>>>
>>>>> On Thu, Feb 13, 2014 at 5:15 AM, Jarrod Hadfield <j.hadfield at ed.ac.uk
>>>>> >wrote:
>>>>>
>>>>>  Hi Jackie,
>>>>>
>>>>>>
>>>>>> Unless you have a very large amount of data I would be careful about
>>>>>> fitting 3 12x12 covariance matrices. However, that in itself should not
>>>>>> generate the error.
>>>>>>
>>>>>> Does
>>>>>>
>>>>>> prior=list(R=list(V=diag(1),n=0.002),G=list(G1=list(V=diag(
>>>>>> 12),n=0.002),G2=list(V=diag(12),n=0.002)))
>>>>>>
>>>>>> model=MCMCglmm(grad.lin~pop.bin*group,mev=mev,data=data,
>>>>>> prior=prior,random=~idh(pop.bin*group):study+idh(pop.bin*
>>>>>> group):pop,rcov=~units,nitt=300000,burnin=100000,thin=50)
>>>>>>
>>>>>> run? If so can you let me know. If not can you make sure that both
>>>>>> pop.bin
>>>>>> and group are coded as factors.
>>>>>>
>>>>>> Cheers,
>>>>>>
>>>>>> Jarrod
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> Quoting Jackie Wood <jackiewood7 at gmail.com> on Wed, 12 Feb 2014
>>>>>> 18:04:22
>>>>>> -0500:
>>>>>>
>>>>>>  Hello all,
>>>>>>
>>>>>>
>>>>>>> I am using MCMCglmm and attempting to create a model which allows
>>>>>>> heterogeneous variances for different levels of a moderator variable
>>>>>>> where
>>>>>>> the moderator is an interaction term. I attempted the run the
>>>>>>> following
>>>>>>> model:
>>>>>>>
>>>>>>> prior=list(R=list(V=diag(12),n=0.002),G=list(G1=list(V=
>>>>>>> diag(12),n=0.002),G2=list(V=diag(12),n=0.002)))
>>>>>>>
>>>>>>> model=MCMCglmm(grad.lin~pop.bin*group,mev=mev,data=data,
>>>>>>> prior=prior,random=~idh(pop.bin*group):study
>>>>>>> +idh(pop.bin*group):pop,rcov=~idh(pop.bin*group):units,nitt=
>>>>>>> 300000,burnin=100000,thin=50)
>>>>>>>
>>>>>>> where "pop.bin" corresponds to bins of population size and has 4
>>>>>>> levels,
>>>>>>> and "group" has 3 levels (Fish, Birds, Plants).
>>>>>>>
>>>>>>> When I run the above model I receive the error:
>>>>>>> Error in priorformat(if (NOpriorG) { :V is the wrong dimension for
>>>>>>> some
>>>>>>> prior$G/prior$R elements4
>>>>>>>
>>>>>>> I tried running a simpler model with just "pop.bin" instead of the
>>>>>>> interaction term and "V=diag(4)" in the prior, and it seems to run
>>>>>>> just
>>>>>>> fine.
>>>>>>>
>>>>>>> I'm unsure whether the problem is simply too many levels and I perhaps
>>>>>>> don't have enough data or statistical power to run this model or
>>>>>>> whether I
>>>>>>> made a simple (or huge) error in the model or prior specification. If
>>>>>>> anyone could shed some light on this it would be a huge help.
>>>>>>>
>>>>>>> Cheers,
>>>>>>> Jackie
>>>>>>>
>>>>>>>
>>>>>>> --
>>>>>>> Jacquelyn L.A. Wood, MSc.
>>>>>>> PhD Candidate
>>>>>>> Biology Dept.
>>>>>>> Concordia University
>>>>>>> 7141 Sherbrooke St. West
>>>>>>> Montreal, QC
>>>>>>> H4B 1R6
>>>>>>> Phone: (514) 293-7255
>>>>>>> Fax: (514) 848-2881
>>>>>>>
>>>>>>>         [[alternative HTML version deleted]]
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>> --
>>>>>> The University of Edinburgh is a charitable body, registered in
>>>>>> Scotland, with registration number SC005336.
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>> --
>>>>> Jacquelyn L.A. Wood, MSc.
>>>>> PhD Candidate
>>>>> Biology Dept.
>>>>> Concordia University
>>>>> 7141 Sherbrooke St. West
>>>>> Montreal, QC
>>>>> H4B 1R6
>>>>> Phone: (514) 293-7255
>>>>> Fax: (514) 848-2881
>>>>>
>>>>>
>>>>>
>>>>
>>>> --
>>>> The University of Edinburgh is a charitable body, registered in
>>>> Scotland, with registration number SC005336.
>>>>
>>>>
>>>>
>>>>
>>>
>>> --
>>> Jacquelyn L.A. Wood, MSc.
>>> PhD Candidate
>>> Biology Dept.
>>> Concordia University
>>> 7141 Sherbrooke St. West
>>> Montreal, QC
>>> H4B 1R6
>>> Phone: (514) 293-7255
>>> Fax: (514) 848-2881
>>>
>>>
>>
>>
>> --
>> The University of Edinburgh is a charitable body, registered in
>> Scotland, with registration number SC005336.
>>
>>
>>
>
>
> --
> Jacquelyn L.A. Wood, MSc.
> PhD Candidate
> Biology Dept.
> Concordia University
> 7141 Sherbrooke St. West
> Montreal, QC
> H4B 1R6
> Phone: (514) 293-7255
> Fax: (514) 848-2881
>


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From bergh.thijsvanden at gmail.com  Tue Feb 18 18:22:53 2014
From: bergh.thijsvanden at gmail.com (Thijs vanden Bergh)
Date: Tue, 18 Feb 2014 18:22:53 +0100
Subject: [R-sig-ME] mixed effect model question
In-Reply-To: <loom.20140131T232110-229@post.gmane.org>
References: <CAJkOg=w1hjhS_9bWn+rHp79dac5dnrzCZhgco1ZNB3g7aTXvgQ@mail.gmail.com>
	<loom.20140131T232110-229@post.gmane.org>
Message-ID: <CAJkOg=wySX51jpAGbZJsuTF5BT0t4MUcXUnUNHnYF1okRp7DQQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140218/ed034669/attachment.pl>

From kovla123 at hotmail.com  Tue Feb 18 18:58:35 2014
From: kovla123 at hotmail.com (Maxim Kovalenko)
Date: Tue, 18 Feb 2014 18:58:35 +0100
Subject: [R-sig-ME] MCMCglmm model for heteroskedasticity at level one
In-Reply-To: <DUB119-W366CF993D5583F97F2E37880980@phx.gbl>
References: <DUB119-W12E255DA17CF9D44C6BE2480AC0@phx.gbl>,
	<20140206153231.210813rrjkg5n19c@www.staffmail.ed.ac.uk>,
	<DUB119-W366CF993D5583F97F2E37880980@phx.gbl>
Message-ID: <DUB119-W235067DE53111B927400C480980@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140218/8d744498/attachment.pl>

From j.hadfield at ed.ac.uk  Tue Feb 18 19:22:55 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Tue, 18 Feb 2014 18:22:55 +0000
Subject: [R-sig-ME] MCMCglmm model for heteroskedasticity at level one
In-Reply-To: <DUB119-W235067DE53111B927400C480980@phx.gbl>
References: <DUB119-W12E255DA17CF9D44C6BE2480AC0@phx.gbl>,
	<20140206153231.210813rrjkg5n19c@www.staffmail.ed.ac.uk>,
	<DUB119-W366CF993D5583F97F2E37880980@phx.gbl>
	<DUB119-W235067DE53111B927400C480980@phx.gbl>
Message-ID: <20140218182255.3706411hw0ia1rsw@www.staffmail.ed.ac.uk>

Hi Maxim,

variance functions (e.g. us()) only take categorical terms in rcov.  
The closest you can get to the model given in:

https://dl.dropboxusercontent.com/u/18527706/mcmc-modexample.png

where I take ij indexes the ith observation for the jth individual (?) is:

MCMCglmm(y~x1+x2, random=~individual +idh(x2):units,rcov=~units,  
data=data, prior=prior)

the three fixed effects are 3.978, 2.222 & -0.049. The first variance  
is Omega_u[1], the second variance is Omega_e[2,2] and the third  
variance is Omega_e[1,1]. Omega_e[1,2] is set to zero by assumption,  
and it is not possible to relax this assumption in MCMCglmm.

Cheers,

Jarrod





Quoting Maxim Kovalenko <kovla123 at hotmail.com> on Tue, 18 Feb 2014  
18:58:35 +0100:

>
>
>
>
>
> Hello Jarrod,
> Thank you for your response. Your answer does help somewhat, but I  
> think my main difficulty is translating between the algebraic  
> representation of the model in MLwiN and the formula syntax of  
> MCMCglmm. The user manual for the latter does not elaborate much on  
> the 'rcov' argument, unfortunately; and I have trouble understanding  
> its relation to the specification of random effects on the  
> individual level in the 'random' argument.
> I have created a more simple model in MLwiN to provide a better  
> learning example:  
> https://dl.dropboxusercontent.com/u/18527706/mcmc-modexample.png
>  To clarify, x0 is the vector of 1's (the constant), x1 is a dummy  
> variable pertaining to the fixed part only, and x2 is a continuous  
> variable. My research question is whether individual-level variance  
> is dependent on x2. To keep things simple I only include the random  
> intercept on the country level.
> Now, on SO  
> (http://stats.stackexchange.com/questions/83148/specifying-a-multilevel-model-in-mcmcglmm-r-that-is-heteroskedastic-at-level) it has been suggested that the call for this model could be as  
> follows:
> MCMCglmm(y~x1+x2, random=~us(1):country + us(1+x2):individual,
>                   rcov=~us(x2):units, data=data,
>                   prior=prior)
> Would that be correct? What would be the difference for model with  
> and without the 'rcov' option?
> Unfortunately, the model does not converge in MCMCglmm, so I cannot  
> compare based on the estimates. In MLwiN I had to use hierarchical  
> clustering for the model to converge, is that functionality also  
> present in MCMCglmm?
>
> Thank you!
> Kind regards,Maxim
>
>
>
>> Date: Thu, 6 Feb 2014 15:32:31 +0000
>> From: j.hadfield at ed.ac.uk
>> To: kovla123 at hotmail.com
>> CC: r-sig-mixed-models at r-project.org
>> Subject: Re: [R-sig-ME] MCMCglmm model for heteroskedasticity at level one
>>
>> Hi Maxim,
>>
>> I'm not very clear about the model you want to fit. If we take your
>> stackexchange model, do you wish to fit a random intercept-slope model
>> (with the covariate x1) at the country level and the residual level?
>>
>> If so, then random=~us(1 + x1):country fits the country
>> intercept/slopes (with covariance between them). The between country
>> variance is quadratic in x1:
>>
>> V_i + 2*x1*V_is + (x1^2)*V_s
>>
>> where V_i is the variance in intercepts, V_s is the variance in slopes
>> and V_is is covariance between intercepts and slopes.
>>
>> This change in variance with respect to x1 is where the information
>> comes from for estimating level-1 heterogeneity. If you fit (assuming
>> x1 is always positive):
>>
>> idh(sqrt(x1)):units
>>
>> in the *random* effect part of the model, and leave the residual part
>> as the default units, then the level-1 variance is linear in x1:
>>
>> Vu+sqrt(x1)^2*Vs = Vu+x1*Vs
>>
>> where Vu is the units variance, and Vs is the variance associated with
>> the random `slopes' in the random effect part of the model.  Clearly
>> you could transform x1 differently.
>>
>> Not sure if this is useful?
>>
>> Jarrod
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>> Quoting Maxim Kovalenko <kovla123 at hotmail.com> on Wed, 29 Jan 2014
>> 15:34:08 +0100:
>>
>> >
>> >
>> > Dear all,
>> >
>> > I kindly ask the help of this community with the following issue. My
>> > research question is about individual-level variance of employment
>> > stability in several European countries. The number of countries is
>> > small (N=13), therefore MCMC is a more proper estimation method than
>> > ML. Variance on the country level doesn't interest me as much, but I
>> > do have to take it into account of course.
>> > When I run my model in R/MLwiN via R2MLwiN package, its formula is
>> > specified as approximately follows:
>> > y ~ (0|cons+careertype+gender) + (1|cons+careertype) + (2|cons)
>> > To decode: there fixed part contains the intercept, career type
>> > (more or less job mobility) and gender. Random effects on the
>> > individual level entail the constant and career type, whereas random
>> > effects on level to include only the constant. Therefore it is a
>> > random intercept model with heteroskedasticity on level one. For
>> > more details on this model please see
>> >  
>> http://stats.stackexchange.com/questions/83148/specifying-a-multilevel-model-in-mcmcglmm-r-that-is-heteroskedastic-at-level, I have included a screenshot there to give a  
>> better
>> > idea.
>> > Is it possible at all to estimate a similar model in MCMCglmm? I
>> > cannot figure out how to specify variance at level one to be
>> > dependent on one of the predictors.
>> > Thank you so much in advance for any tips!
>> > Kind regards,Maxim
>> > 	[[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >
>> >
>>
>>
>>
>> --
>> The University of Edinburgh is a charitable body, registered in
>> Scotland, with registration number SC005336.
>>
>>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From bates at stat.wisc.edu  Tue Feb 18 20:39:21 2014
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 18 Feb 2014 13:39:21 -0600
Subject: [R-sig-ME] R2 of a linear mixed model in the "nlme" package
In-Reply-To: <CAGzZEyD7mOXg+g5VPOmXk8A0x9kPSp8fjpnw6fiO94pbd+GBtA@mail.gmail.com>
References: <CAGzZEyD7mOXg+g5VPOmXk8A0x9kPSp8fjpnw6fiO94pbd+GBtA@mail.gmail.com>
Message-ID: <CAO7JsnRPJS-xM5W2P89kHD0yscfd3mOrUFHdpgn_KJALfrAYwA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140218/1cf3b29a/attachment.pl>

From njbisaac at gmail.com  Tue Feb 18 21:02:52 2014
From: njbisaac at gmail.com (Nick Isaac)
Date: Tue, 18 Feb 2014 20:02:52 +0000
Subject: [R-sig-ME] Fwd: Variance Inflation Factors
In-Reply-To: <CAMab-Eyexk1bdFGPPkz9E0Li6n0DeqS81U6PnobKZ5J_3QjjiA@mail.gmail.com>
References: <CAMab-Eyexk1bdFGPPkz9E0Li6n0DeqS81U6PnobKZ5J_3QjjiA@mail.gmail.com>
Message-ID: <CAMab-Ex9FzD2WFeuXSeQSH+vV-JZOPW_+cb1gtZqOyr7gmVynQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140218/a42adbd0/attachment.pl>

From njbisaac at gmail.com  Tue Feb 18 21:14:05 2014
From: njbisaac at gmail.com (Nick Isaac)
Date: Tue, 18 Feb 2014 20:14:05 +0000
Subject: [R-sig-ME] R2 of a linear mixed model in the "nlme" package
In-Reply-To: <CAO7JsnRPJS-xM5W2P89kHD0yscfd3mOrUFHdpgn_KJALfrAYwA@mail.gmail.com>
References: <CAGzZEyD7mOXg+g5VPOmXk8A0x9kPSp8fjpnw6fiO94pbd+GBtA@mail.gmail.com>
	<CAO7JsnRPJS-xM5W2P89kHD0yscfd3mOrUFHdpgn_KJALfrAYwA@mail.gmail.com>
Message-ID: <CAMab-ExZDM8WJ2KMU2sNRgoep9b_9v67tpQ-R=fgTK6dr2dz8Q@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140218/fbc9f912/attachment.pl>

From djcher at gmail.com  Tue Feb 18 20:36:27 2014
From: djcher at gmail.com (Daniel Cher)
Date: Tue, 18 Feb 2014 11:36:27 -0800
Subject: [R-sig-ME] lmer estimates account for random effect?
Message-ID: <CAGd27ZoVZHYcbgY8wWeDCkTUdsyCS5xr3YCBkU8qUi2giD_dPQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140218/a8fb3f7b/attachment.pl>

From njbisaac at gmail.com  Tue Feb 18 09:11:48 2014
From: njbisaac at gmail.com (Nick Isaac)
Date: Tue, 18 Feb 2014 08:11:48 +0000
Subject: [R-sig-ME] Variance Inflation Factors
Message-ID: <CAMab-Eyexk1bdFGPPkz9E0Li6n0DeqS81U6PnobKZ5J_3QjjiA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140218/273c460a/attachment.pl>

From bbolker at gmail.com  Tue Feb 18 22:06:12 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 18 Feb 2014 16:06:12 -0500
Subject: [R-sig-ME] Pedigreemm on lme4-1.0
In-Reply-To: <CF291CF0.85B4%roby.joehanes@nih.gov>
References: <CF23BB16.8455%roby.joehanes@nih.gov> <52FE98D9.2010405@gmail.com>
	<CF240879.84DD%roby.joehanes@nih.gov>
	<alpine.LMD.2.00.1402151808360.21127@orpheus.qimr.edu.au>
	<CF291CF0.85B4%roby.joehanes@nih.gov>
Message-ID: <CABghstQnR8uaF1RmP8mMk3cZATZrv0xsvTdon_YYc+p=Md3e1w@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140218/5335d200/attachment.pl>

From David.Duffy at qimr.edu.au  Wed Feb 19 05:06:18 2014
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Wed, 19 Feb 2014 14:06:18 +1000
Subject: [R-sig-ME] Pedigreemm on lme4-1.0
In-Reply-To: <CABghstQnR8uaF1RmP8mMk3cZATZrv0xsvTdon_YYc+p=Md3e1w@mail.gmail.com>
References: <CF23BB16.8455%roby.joehanes@nih.gov>
	<52FE98D9.2010405@gmail.com><CF240879.84DD%roby.joehanes@nih.gov><alpine.LMD.2.00.1402151808360.21127@orpheus.qimr.edu.au><CF291CF0.85B4%roby.joehanes@nih.gov>
	<CABghstQnR8uaF1RmP8mMk3cZATZrv0xsvTdon_YYc+p=Md3e1w@mail.gmail.com>
Message-ID: <alpine.LMD.2.00.1402191340420.17804@orpheus.qimr.edu.au>

On Wed, 19 Feb 2014, Ben Bolker wrote:

> The performance issue you're referring to is I think related to 
> https://github.com/lme4/lme4/issues/150 ... we're hoping to look into 
> this soon; it is definitely important, and we're afraid it's pretty 
> deep.

Using that InstEval example gives me:

Program                  Time      Notes
Wombat (AI-REML)         14.2 s

lmer4.0                  11.5 s    lme4.0_0.999999-4, Matrix_1.1-0
lmer4-nm                 25.9 s    lme4_1.0-6, Matrix_1.1-0

lmer4-bobqa              14.8 s    lme4_1.0-6, Matrix_1.1-0

lmer4-optimx-bobqa       25.2 s    lme4_1.0-6, Matrix_1.1-0
lmer4-optimx-bobqa-kkt=F 15.3 s    lme4_1.0-6, Matrix_1.1-0

lmer4-nlminb             19.8 s    lme4_1.0-6, Matrix_1.1-0
lmer4-nlminb-kkt=F       10.0 s    lme4_1.0-6, Matrix_1.1-0

lmer4-bfgs               23.4 s    lme4_1.0-6, Matrix_1.1-0

That is, the test for the Kuhn, Karush, Tucker optimality condition
seems quite expensive for this problem, and the differences between
optimizers might completely explain the 4.0 v 4 differences.

Cheers, David Duffy.

| David Duffy (MBBS PhD)
| email: David.Duffy at qimrberghofer.edu.au  ph: INT+61+7+3362-0217 fax: -0101
| Genetic Epidemiology, QIMR Berghofer Institute of Medical Research
| 300 Herston Rd, Brisbane, Queensland 4006, Australia  GPG 4D0B994A


From Friso.muijsers at uni-oldenburg.de  Wed Feb 19 09:06:21 2014
From: Friso.muijsers at uni-oldenburg.de (Friso Muijsers)
Date: Wed, 19 Feb 2014 09:06:21 +0100
Subject: [R-sig-ME] R2 of a linear mixed model in the "nlme" package
In-Reply-To: <CAMab-ExZDM8WJ2KMU2sNRgoep9b_9v67tpQ-R=fgTK6dr2dz8Q@mail.gmail.com>
References: <CAGzZEyD7mOXg+g5VPOmXk8A0x9kPSp8fjpnw6fiO94pbd+GBtA@mail.gmail.com>	<CAO7JsnRPJS-xM5W2P89kHD0yscfd3mOrUFHdpgn_KJALfrAYwA@mail.gmail.com>
	<CAMab-ExZDM8WJ2KMU2sNRgoep9b_9v67tpQ-R=fgTK6dr2dz8Q@mail.gmail.com>
Message-ID: <530465FD.1030704@uni-oldenburg.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140219/d9bfd4c9/attachment.pl>

From bbolker at gmail.com  Wed Feb 19 10:22:04 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 19 Feb 2014 04:22:04 -0500
Subject: [R-sig-ME] Pedigreemm on lme4-1.0
In-Reply-To: <alpine.LMD.2.00.1402191340420.17804@orpheus.qimr.edu.au>
References: <CF23BB16.8455%roby.joehanes@nih.gov> <52FE98D9.2010405@gmail.com>
	<CF240879.84DD%roby.joehanes@nih.gov>
	<alpine.LMD.2.00.1402151808360.21127@orpheus.qimr.edu.au>
	<CF291CF0.85B4%roby.joehanes@nih.gov>
	<CABghstQnR8uaF1RmP8mMk3cZATZrv0xsvTdon_YYc+p=Md3e1w@mail.gmail.com>
	<alpine.LMD.2.00.1402191340420.17804@orpheus.qimr.edu.au>
Message-ID: <CABghstRX1squrh-iusefXjGH2jByiXNj+PHaLtsQLqNL8QqjAg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140219/26727b5e/attachment.pl>

From David.Duffy at qimr.edu.au  Wed Feb 19 12:37:56 2014
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Wed, 19 Feb 2014 21:37:56 +1000
Subject: [R-sig-ME] Pedigreemm on lme4-1.0
In-Reply-To: <CABghstRX1squrh-iusefXjGH2jByiXNj+PHaLtsQLqNL8QqjAg@mail.gmail.com>
References: <CF23BB16.8455%roby.joehanes@nih.gov><52FE98D9.2010405@gmail.com><CF240879.84DD%roby.joehanes@nih.gov><alpine.LMD.2.00.1402151808360.21127@orpheus.qimr.edu.au><CF291CF0.85B4%roby.joehanes@nih.gov><CABghstQnR8uaF1RmP8mMk3cZATZrv0xsvTdon_YYc+p=Md3e1w@mail.gmail.com><alpine.LMD.2.00.1402191340420.17804@orpheus.qimr.edu.au>
	<CABghstRX1squrh-iusefXjGH2jByiXNj+PHaLtsQLqNL8QqjAg@mail.gmail.com>
Message-ID: <alpine.LMD.2.00.1402192125320.2256@orpheus.qimr.edu.au>

On Wed, 19 Feb 2014, Ben Bolker wrote:

> Great news if it holds up generally  -- thanks for the investigation.

Not for the milk example from pedigreemm that Joehanes was running ;(

On my machine:

Wombat (AI-REML)                  0.6 s
lmer4-nm 	                193.3 s	lme4_1.0-6, Matrix_1.1-0
lmer4-optimx-nlminb 	        190.3 s	lme4_1.0-6, Matrix_1.1-0
lmer4-optimx-nlminb-kkt=F	186.2 s	lme4_1.0-6, Matrix_1.1-0
lmer4-bobqa 	                196.1 s	lme4_1.0-6, Matrix_1.1-0
lmer4-optimx-bobqa-kkt=F 	196.2 s	lme4_1.0-6, Matrix_1.1-0

I'll have a look at the iteration history...

Cheers, David Duffy.


From rdiaz02 at gmail.com  Wed Feb 19 13:40:32 2014
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Wed, 19 Feb 2014 13:40:32 +0100
Subject: [R-sig-ME] Model is nearly unidentifiable with glmer
Message-ID: <87d2ijp96n.fsf@gmail.com>


Dear All,

Summary:
========

I am running some models with glmer that are giving, among others, the
warning message

In checkConv(attr(opt, "derivs"), opt$par, checkCtrl = control$checkConv,  :
  Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?


but I cannot understand why the model is nearly unidentifable, based both
on the details of the experimental design, and by comparison with other
approaches (that run without complains and provide similar results).
Moreover, I have no idea what I am supposed to rescale.




Details:
========
 
The call to glmer
-----------------

gt.0 <- glmer(nS ~ Tree + Model + sh + S.Type + S.Time + S.Size + Method +
                            (1|dataSetID) + (1|obs), 
                            family = poisson,
                            data = NP.all,
                            control = glmerControl(
                                check.conv.singular="warning",
                                optCtrl = list(maxfun = 10000))
 				)

The model is for Poisson data, allowing for overdispersion (with the obs
variable) and with a "dataSetID" random effect. (The glmerControl has been
added to try to understand what might be happening).



The design
----------

The data come from a simulation experiment. For each combination of

Tree * Model * sh * S.Type * S.Time * S.Size there are 20 replicate
simulations (each identified by a dataSetID), so a full factorial
design. Each variable is a factor variable (6 levels for Tree, 4 for Model,
2 for sh, 3 for S.Type, 2 for S.Time, 3 for S.Size)

Each replicate simulation, identified by a "dataSetID", is subject to four
Methods (a factor). That is why dataSetID is a random effect.

There are no missing values.

Thus, there are 17280 dataSetID groups (6*4*2*3*2*3*20), each with four
observations, resulting in a total of 69120 observations. glmer does
report these values just fine.

Therefore, based on that design, I think I should be able to fit not just
the model above, but a model with all possible interactions. In fact, that
model (a saturated model in the log-linear parlance, IIUC) is what I want
to start from.


The warnings
------------

glmer gives two warnings:
1: In checkConv(attr(opt, "derivs"), opt$par, checkCtrl = control$checkConv,  :
  Model failed to converge with max|grad| = 392.339 (tol = 0.001)
2: In checkConv(attr(opt, "derivs"), opt$par, checkCtrl = control$checkConv,  :
  Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?


The failure to converge I don't like, but OK. However, the "nearly
unidentifiable" I just don't understand. And what variables am I supposed
to rescale?


Removing the random effect for obs or for dataSetID does not eliminate the
warnings (and, in fact, that should not be the problem, since both obs and
dataSetID get non-zero estimates).

 
Other methods
-------------

I have fitted the above model with:

- MCMCglmm (using the priors list(R = list(V = 1, nu = 0.002), G = list(G1
  = list(V = 1, nu = 0.002))))


- bglmer (with all as per default)


- glmer2stan (here using a more complex model with a bunch of interactions)

- glmmadmb (with family "nbinom")

and even if glmmadmb does complain about lack of convergence, there are no
further problems.

Moreover, with MCMCglmm I've fitted the model with interactions. Again, no
apparent problems (beyond slow mixing in some cases).


In addition, I have also fitted several other models that include all
possible interactions, just to make sure again I do not have something
silly in the model matrix. In some cases, the response is different (as
when fitting a lm/lmer). Beyond the lack of convergence of lmer fit, no
other problems.


## a GLM with interactions but, of course, without the dataSetID random
   effect.

glm(nS ~ Tree * Model * sh *
                             S.Type * S.Time * S.Size * Method,
                             data = NP.all,
                             family = poisson
                             )

## a linear model with interactions 			     
			     
lm(Dissim ~ Tree * Model * sh *
                             S.Type * S.Time * S.Size * Method,
                             data = NP.all
                             )
 
## a lmers

lmer(Dissim ~ Tree * Model * sh *
                             S.Type * S.Time * S.Size * Method +
                             (1|dataSetID),
                             data = NP.all,
                             control = lmerControl(
                                 check.conv.singular="warning",
                                 optCtrl = list(maxfun = 10000))
                             )
## This complaints of lack of convergence, but no identifiability
                             problems. 
			     

			     
(Lack of) Differences between fits
-----------------------------------

If anyone wants I can provide, of course, the output from the
fits. Anyway, the qualitative summary is that the the estimates for the
fixed are very similar between glmer, admb, bglmer, and MCMCglmm and
the estimates for the random effects are very similar for MCMCglmm,
bglmer, and glmer.





So, what am I missing?


Best,



R.



-- 
Ramon Diaz-Uriarte
Department of Biochemistry, Lab B-25
Facultad de Medicina 
Universidad Aut?noma de Madrid 
Arzobispo Morcillo, 4
28029 Madrid
Spain

Phone: +34-91-497-2412

Email: rdiaz02 at gmail.com
       ramon.diaz at iib.uam.es

http://ligarto.org/rdiaz


From tina.wey at gmail.com  Wed Feb 19 17:21:38 2014
From: tina.wey at gmail.com (Tina Wey)
Date: Wed, 19 Feb 2014 17:21:38 +0100
Subject: [R-sig-ME] How do you do?
Message-ID: <20140219162140.JRAR3766.eastrmfepo202.cox.net@eastrmimpo306>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140219/43a3b861/attachment.pl>

From juliet.hannah at gmail.com  Thu Feb 20 17:22:17 2014
From: juliet.hannah at gmail.com (Juliet Hannah)
Date: Thu, 20 Feb 2014 11:22:17 -0500
Subject: [R-sig-ME] order of models when using anova
Message-ID: <CALzuZRSoJNXqzSOYQC+GFHTxxSZGj2z_qMawKYL3nQ-W7=U+KQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140220/c46791a6/attachment.pl>

From steve.taylor at aut.ac.nz  Thu Feb 20 21:53:23 2014
From: steve.taylor at aut.ac.nz (Steve Taylor)
Date: Thu, 20 Feb 2014 20:53:23 +0000
Subject: [R-sig-ME] order of models when using anova
In-Reply-To: <CALzuZRSoJNXqzSOYQC+GFHTxxSZGj2z_qMawKYL3nQ-W7=U+KQ@mail.gmail.com>
References: <CALzuZRSoJNXqzSOYQC+GFHTxxSZGj2z_qMawKYL3nQ-W7=U+KQ@mail.gmail.com>
Message-ID: <CCE952776B6679469977532BD863C39C7E7D607B@Lewis.autuni.aut.ac.nz>

Both ways work.  I think anova() is smart enough to figure out which model is the smaller one.

library(lme4)
example(glmer)
# Compare these outputs...
anova(gm1,gm2)
anova(gm2,gm1)

cheers,
     Steve
____________________________________________

Steve Taylor
   Biostatistician
   Faculty of Health and Environmental Sciences
   AUT University
   http://www.aut.ac.nz/profiles/steve-taylor
____________________________________________

-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Juliet Hannah
Sent: Friday, 21 February 2014 5:22a
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] order of models when using anova

All,

When using the anova command to compare nested models I thought the order was

anova(reduced model, full model)

Where the 'full model' has more terms.


However, looking at the slides on:

http://lme4.r-forge.r-project.org/slides/2009-07-21-Seewiesen/6Interactions-4a4.pdf


pg 3 (comparing the model fits), it looks like I had this backward.

Did I have it backward, or is lme4 different, or is the order not important?

Thanks,

Juliet

	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From r.turner at auckland.ac.nz  Thu Feb 20 22:39:24 2014
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Fri, 21 Feb 2014 10:39:24 +1300
Subject: [R-sig-ME] order of models when using anova
In-Reply-To: <CCE952776B6679469977532BD863C39C7E7D607B@Lewis.autuni.aut.ac.nz>
References: <CALzuZRSoJNXqzSOYQC+GFHTxxSZGj2z_qMawKYL3nQ-W7=U+KQ@mail.gmail.com>
	<CCE952776B6679469977532BD863C39C7E7D607B@Lewis.autuni.aut.ac.nz>
Message-ID: <5306760C.2030401@auckland.ac.nz>


In general anova() is *not* really quite smart enough. Consider the case 
of ordinary garden-variety linear models:

x <- seq(0,1,length=20)
set.seed(42)
y <- rnorm(20)
f1 <- lm(y ~ x)
f2 <- lm(y ~ x + I(x^2))
anova(f2,f1)

This gives a negative degrees of freedom and a negative sum of squares. 
  The F statistic and p-value are correct however, so the "not quite 
smart enough" aspect is really just cosmetic.  Still, those negative 
values are disconcerting.

It does appear that the anova() function used on "mer" objects is 
smarter than anova.lm().  It is hard/impossible to figure out just 
*what* is being used when one calls anova(gm2,gm1) because of these 
bloody awful obscurantist S4 classes and methods which seem to be 
involved here.

Bottom line:  In general the appropriate syntax for anova() would appear 
to be anova(reduced model, full model) as the OP stated.  But for "mer" 
objects it doesn't matter.

cheers,

Rolf

On 21/02/14 09:53, Steve Taylor wrote:
> Both ways work.  I think anova() is smart enough to figure out which model is the smaller one.
>
> library(lme4)
> example(glmer)
> # Compare these outputs...
> anova(gm1,gm2)
> anova(gm2,gm1)
>
> cheers,
>       Steve
> ____________________________________________
>
> Steve Taylor
>     Biostatistician
>     Faculty of Health and Environmental Sciences
>     AUT University
>     http://www.aut.ac.nz/profiles/steve-taylor
> ____________________________________________
>
> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Juliet Hannah
> Sent: Friday, 21 February 2014 5:22a
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] order of models when using anova
>
> All,
>
> When using the anova command to compare nested models I thought the order was
>
> anova(reduced model, full model)
>
> Where the 'full model' has more terms.
>
>
> However, looking at the slides on:
>
> http://lme4.r-forge.r-project.org/slides/2009-07-21-Seewiesen/6Interactions-4a4.pdf
>
>
> pg 3 (comparing the model fits), it looks like I had this backward.
>
> Did I have it backward, or is lme4 different, or is the order not important?
>
> Thanks,
>
> Juliet
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From juliet.hannah at gmail.com  Fri Feb 21 01:17:24 2014
From: juliet.hannah at gmail.com (Juliet Hannah)
Date: Thu, 20 Feb 2014 19:17:24 -0500
Subject: [R-sig-ME] order of models when using anova
In-Reply-To: <5306760C.2030401@auckland.ac.nz>
References: <CALzuZRSoJNXqzSOYQC+GFHTxxSZGj2z_qMawKYL3nQ-W7=U+KQ@mail.gmail.com>
	<CCE952776B6679469977532BD863C39C7E7D607B@Lewis.autuni.aut.ac.nz>
	<5306760C.2030401@auckland.ac.nz>
Message-ID: <CALzuZRR5pGv162sSFHfT4mMu371cGyF79B5BnhLrmYk37UqPWg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140220/5335976f/attachment.pl>

From H.Quene at uu.nl  Fri Feb 21 01:32:58 2014
From: H.Quene at uu.nl (=?UTF-8?B?SHVnbyBRdWVuw6k=?=)
Date: Fri, 21 Feb 2014 01:32:58 +0100
Subject: [R-sig-ME] lme4 problem solved by updating Rcpp
Message-ID: <53069EBA.9050801@uu.nl>

Dear list members,

Since a few days I've had difficulties in using lme4_1.0-6 (CRAN binary 
version) on a MacBook8,1.
My call to lmer() resulted in an error message:

 > (fm1 <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy))
Error in initializePtr() :
   function 'dataptr' not provided by package 'Rcpp'

A repeated call of lmer() caused R to crash entirely.

As it turned out, this problem disappeared after I updated package Rcpp 
from version Rcpp_0.10.6 to Rcpp_0.11.0 (CRAN binaries). When lme4 was 
installed it did warn about an obsolete version of the Matrix package, 
but it did not complain about the Rcpp package. Hence the solution to 
update Rcpp might be useful to have in the list archive, for others 
experiencing similar problems.

With kind regards, Hugo Quen?

-- 
Dr Hugo Quen? | assoc prof | Dept Languages Literature and Communication
| Utrecht inst Linguistics OTS | Utrecht University | Trans 10 | 3512JK
Utrecht | The Netherlands | +31 30 253 6070 | www.uu.nl/hum/staff/HQuene
| uu.academia.edu/HugoQuene |


From r.turner at auckland.ac.nz  Fri Feb 21 03:13:11 2014
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Fri, 21 Feb 2014 15:13:11 +1300
Subject: [R-sig-ME] order of models when using anova
In-Reply-To: <CALzuZRR5pGv162sSFHfT4mMu371cGyF79B5BnhLrmYk37UqPWg@mail.gmail.com>
References: <CALzuZRSoJNXqzSOYQC+GFHTxxSZGj2z_qMawKYL3nQ-W7=U+KQ@mail.gmail.com>
	<CCE952776B6679469977532BD863C39C7E7D607B@Lewis.autuni.aut.ac.nz>
	<5306760C.2030401@auckland.ac.nz>
	<CALzuZRR5pGv162sSFHfT4mMu371cGyF79B5BnhLrmYk37UqPWg@mail.gmail.com>
Message-ID: <5306B637.4090507@auckland.ac.nz>

On 21/02/14 13:17, Juliet Hannah wrote:
> Thanks Rolf! Very helpful. Maybe my question was not as stupid as I had
> feared!

Not stupid at all, IMHO.

cheers,

Rolf


From mtoncic at ffri.hr  Fri Feb 21 11:45:52 2014
From: mtoncic at ffri.hr (marKo)
Date: Fri, 21 Feb 2014 11:45:52 +0100
Subject: [R-sig-ME] Help with analyzing multivariate outcomes
Message-ID: <53072E60.9080808@ffri.hr>

I have some problems analyzing multivariate dependent variables using a 
mixed-effect approach.
My dataset consists of two-dimensional outcome (like a position on a 
grid) measured approx 300 times for 100 subjects.
For one dimension the model would be something like that:

m1<-lmer(y1~time+(time|id),...)

but I would like to analyze the y2 as well (not in an independent 
analysis). I do not wont to decompose the data (PCA or similar) and than 
conduct the analysis on on the first component (i guess that it would 
not be appropriate). I somehow want to model the trajectories (basic 
idea). I know that the problem is not so well explained and fuzzy (it is 
like this in my head to)

Any ideas, suggestions, readings would be greatly appreciated.

Thanks a lot,

Marko


From mtoncic at ffri.hr  Fri Feb 21 12:27:33 2014
From: mtoncic at ffri.hr (marKo)
Date: Fri, 21 Feb 2014 12:27:33 +0100
Subject: [R-sig-ME] Help with analyzing multivariate outcomes
In-Reply-To: <DUB126-W618EC7BDCF55B4D11D15D8D6850@phx.gbl>
References: <53072E60.9080808@ffri.hr>
	<DUB126-W618EC7BDCF55B4D11D15D8D6850@phx.gbl>
Message-ID: <53073825.6040809@ffri.hr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140221/8a73accb/attachment.pl>

From njbisaac at gmail.com  Fri Feb 21 13:41:52 2014
From: njbisaac at gmail.com (Nick Isaac)
Date: Fri, 21 Feb 2014 12:41:52 +0000
Subject: [R-sig-ME] Variance Inflation Factors
Message-ID: <CAMab-ExMSeuXeqaExW7jyQwSpVSMSTL09yVADeXJTixXDt6XFQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140221/71a2d765/attachment.pl>

From jfox at mcmaster.ca  Fri Feb 21 16:16:44 2014
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 21 Feb 2014 10:16:44 -0500
Subject: [R-sig-ME] Variance Inflation Factors
In-Reply-To: <CAMab-ExMSeuXeqaExW7jyQwSpVSMSTL09yVADeXJTixXDt6XFQ@mail.gmail.com>
References: <CAMab-ExMSeuXeqaExW7jyQwSpVSMSTL09yVADeXJTixXDt6XFQ@mail.gmail.com>
Message-ID: <000401cf2f17$ef0306e0$cd0914a0$@mcmaster.ca>

Dear Nick,

For a linear model, it's also possible to calculate VIFs (and generalized
VIFs) from the covariance matrix of the coefficients, and this approach
probably generalizes more readily. This is also the approach taken in the
vif() function in the car package, for which there's just an "lm" method. 

E.g., for a model fit by lmer() or glmer() in the lme4 package, the
following should work:

coef.merMod <- function(object, ...){
    fixef(object)
}
vcov.merMod <- function (object, ...){
    as.matrix(lme4:::vcov.merMod(object))
}

vif.merMod <- function(mod, ...){
    car:::vif.lm(mod, ...)
}

The first two functions are necessary because car:::vif.lm() expects coef()
to return the fixed-effect vector and vcov() to return an ordinary matrix.

I hope this helps,
 John

> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-
> models-bounces at r-project.org] On Behalf Of Nick Isaac
> Sent: February-21-14 7:42 AM
> To: Help Mixed Models
> Subject: [R-sig-ME] Variance Inflation Factors
> 
> I sent this a few days ago but it looks like the text may have been
hidden.
> Apologies if you saw it and ignored!
> 
> I have a glmer model with potential collinearity problems, so I'd like to
> estimate the Variance Inflation Factor for each of my fixed effects.
> Technically this doesn't seem challenging, but there are choices to make
and
> it's not clear to me which one is best.
> 
> I can calculating VIF for each fixed effect as 1/(1-R^2_i), where R^2_i is
the
> R^2 value from a model with the the ith fixed effect as the dependent
> variable and all others as independent variables.
> 
> Since the original model is mixed effects, I feel it would be appropriate
to
> include the random effects in each of the VIF models. Fortunately it's now
> easy to get R^2 values from mer objects using the formulae in Nakegawa &
> Schielzeth (2012), as implemented by the r.squaredGLMM() function in the
> MuMIn package.
> 
> The question is whether it would be more appropriate to use the marginal
or
> conditional R^2 value (the latter incorporates variance attributable to
random
> effects). Any opinions?
> 
> Best wishes, Nick
> 
> --
> http://drnickisaac.weebly.com/
> http://www.ceh.ac.uk/StaffWebPages/DrNickIsaac.html
> *BES Macroecology Special Interest Group
>
<http://www.britishecologicalsociety.org/getting-involved/special-interest-
> groups/macroecology/>*
> @drnickisaac
> 
> *
> <http://www.britishecologicalsociety.org/getting_involved/special_interest
> _groups/Macroecology.php>*
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


---
This email is free from viruses and malware because avast! Antivirus protection is active.


From asafw.at.wharton at gmail.com  Wed Feb 19 20:30:04 2014
From: asafw.at.wharton at gmail.com (Asaf Weinstein)
Date: Wed, 19 Feb 2014 14:30:04 -0500
Subject: [R-sig-ME] Order in which variance components are reported
Message-ID: <CAGG0PdBLLtxnPCSYH9c_cU0WnacdmLJ=Gh5B8RHLY7R6C_Jf3Q@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140219/d6bc6c06/attachment.pl>

From bates at stat.wisc.edu  Fri Feb 21 23:43:52 2014
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 21 Feb 2014 16:43:52 -0600
Subject: [R-sig-ME] glmer function in lme4 package
In-Reply-To: <9ED811556BF51A4C8E7FD77C5133339B454833C6@ICESMBX2.ices.on.ca>
References: <9ED811556BF51A4C8E7FD77C5133339B454833C6@ICESMBX2.ices.on.ca>
Message-ID: <CAO7JsnRWsw7G50yrhKdJMK_oBHuqPm7h7NZ3OviSrn_Z7AWDCw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140221/c8e5edd1/attachment.pl>

From zia207 at gmail.com  Sat Feb 22 09:30:58 2014
From: zia207 at gmail.com (Zia Ahmed)
Date: Sat, 22 Feb 2014 14:30:58 +0600
Subject: [R-sig-ME] Help: Split-Split model nested in a nested structure
Message-ID: <CAKqUiGkV368NETTo0yAv4yV_BR-1Dtj0nm88oXnyp5g44J_nvA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140222/b3b7115a/attachment.pl>

From zia207 at gmail.com  Sat Feb 22 12:34:40 2014
From: zia207 at gmail.com (Zia Ahmed)
Date: Sat, 22 Feb 2014 17:34:40 +0600
Subject: [R-sig-ME] Split-Split model in nested structure
Message-ID: <CAKqUiGkeM4YTCPoxTTsozUfeEYX6zAarDTcCbY9+AX733ZSaOA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140222/463d3169/attachment.pl>

From i.m.s.white at ed.ac.uk  Sat Feb 22 18:09:06 2014
From: i.m.s.white at ed.ac.uk (ian m s white)
Date: Sat, 22 Feb 2014 17:09:06 +0000
Subject: [R-sig-ME] Help: Split-Split model nested in a nested structure
In-Reply-To: <CAKqUiGkV368NETTo0yAv4yV_BR-1Dtj0nm88oXnyp5g44J_nvA@mail.gmail.com>
References: <CAKqUiGkV368NETTo0yAv4yV_BR-1Dtj0nm88oXnyp5g44J_nvA@mail.gmail.com>
Message-ID: <FD060FD0-3640-4C54-AE49-7A3D079D300A@ed.ac.uk>

For one district, the one way to obtain the split-split anova is

village <- gl(2,48)
field <- gl(16,6)
sow <- gl(2,24,96)
foo <- data.frame(village,field,sow)
foo$nit <- gl(2,3,96)
foo$geno <- gl(3,1,96)
foo$Y <- rnorm(96)
fit <- aov(Y ~ village*sow*nit*gen + Error(field/nit), data = foo)
summary(fit)

Note that fields are numbered 1 to 16 over the two villages.
 
On 22 Feb 2014, at 08:30, Zia Ahmed <zia207 at gmail.com> wrote:

> Dear list members.
> We are trying see effect of N on yield of three wheat genotypes under late
> and early planting conditions in saline and non-saline environments. Our
> experimental structure as follows:
> 
> Districts (2: DIST01 and DIST02) - Not randomly selected
> 
> ENV (Saline and Non-Saline) - One  saline and non-saline environment (eg.
> village) were selected in each district (not random).
> 
> *SOWING Time  (Late and Early):* 8 farmers' fields were selected randomly
> in each environment or village.  These are spatially distributed in a
> village. Among them, 4 were grouped as early and 4 were grouped as a late
> sowing group.
> 
> *N_TREAT  (N0 and N100)* - Then, each farmer field was splitted  into
> two  main
> plots where two N-treatment were assigned randomly.
> 
> *GENOTYPE ( G1,  G2, G3*) - Then, N-main plots were further splited into
> three subplots here 3 wheat varieties were  assigned  randomly.
> 
> This experiment was repeated for two years. Farmers' fields in Year 1 and
> Year 2 were not same. We are not interested to see the District effect on
> wheat yield here. Rather, we like to see the effect of saline environment,
> sowing time, N  and genotype and their interaction  on wheat yield for year
> 1 and year 2 separately.
> We are using the following ANOVA model. We do not know whether we are
> missing something here.  Help will be highly arreciated.
> Regards
> Zia Ahmed, CIMMYT
> 
> model<-aov(YIELD~ENV*SOWING*N_RATE*GEN+Error(FARMERS/N_RATE/GEN),
> data=mydata)
> 
> summary(model)
> 
> 
> 
> # District: two
> 
> DIST<-as.factor(rep(c("DIST01","DIST02"),each=96))
> 
> # ENV: Saline and Non-saline environment
> 
> ENV<-as.factor(rep(rep(c("Saline","Non-saline"),each=48),2))
> 
> # Farmers 16
> 
> FARMERS<-as.factor(rep(c("F1","F2","F3","F4","F5","F6","F7","F8",
> 
>                         "F9","F10","F11",
> "F12","F13","F14","F15","F16"),each=12))
> 
> # Showing Date: two
> 
> SOWING<-as.factor(rep(rep(c("Early","Late"),each=6),16))
> 
> # Nitrogen treatments: N0 and N100
> 
> N_RATE<-as.factor(rep(rep(c("N0","N100"),each=3),32))
> 
> # Genotype
> 
> GEN<-as.factor(rep(rep(c("V1","V2","V3"),each=1),64))
> 
> # Response: Wheat Yield
> 
> set.seed(1234)
> 
> YIELD <- rnorm(n=192, mean=3.0, sd=0.5)
> 
> # Create Data Frame
> 
> mydata<-data.frame(DIST,ENV, FARMERS,SOWING,N_RATE,GEN,YIELD)
> 
> mydata
> 
>> 
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From bbolker at gmail.com  Sat Feb 22 18:45:09 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 22 Feb 2014 17:45:09 +0000
Subject: [R-sig-ME] Order in which variance components are reported
In-Reply-To: <CAGG0PdBLLtxnPCSYH9c_cU0WnacdmLJ=Gh5B8RHLY7R6C_Jf3Q@mail.gmail.com>
References: <CAGG0PdBLLtxnPCSYH9c_cU0WnacdmLJ=Gh5B8RHLY7R6C_Jf3Q@mail.gmail.com>
Message-ID: <CABghstQ7R01dagqpZo-Y4ahbtg0g-v3iqT93AnE1mJ87R-msoA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140222/756fecb9/attachment.pl>

From bbolker at gmail.com  Sat Feb 22 18:57:28 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 22 Feb 2014 17:57:28 +0000
Subject: [R-sig-ME] lme4 problem solved by updating Rcpp
In-Reply-To: <53069EBA.9050801@uu.nl>
References: <53069EBA.9050801@uu.nl>
Message-ID: <CABghstQsVZyWobLh1r1BFpKtueWJsTOO269YZnWv7e5Zm0PsYw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140222/8148497b/attachment.pl>

From zia207 at gmail.com  Sat Feb 22 07:18:27 2014
From: zia207 at gmail.com (Zia Ahmed)
Date: Sat, 22 Feb 2014 12:18:27 +0600
Subject: [R-sig-ME] Help: Split-Split model nested in a nested structure
Message-ID: <CAKqUiGntDsuLks_rpZST480ciLfWaFZz_rhmmubNshif9MPwYQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140222/0ba85d75/attachment.pl>

From peter.austin at ices.on.ca  Sat Feb 22 16:51:55 2014
From: peter.austin at ices.on.ca (Austin, Peter)
Date: Sat, 22 Feb 2014 15:51:55 +0000
Subject: [R-sig-ME] glmer function in lme4 - false convergence warning
	message
Message-ID: <9ED811556BF51A4C8E7FD77C5133339B45483532@ICESMBX2.ices.on.ca>

I am writing with a question about a warning message that I received when I used the glmer function to fit a complementary log-log binary regression model that incorporated two different sets of random effects. When running the model, I received the following warning message:

Warning message: In mer_finalize(ans) : false convergence (8).

However, output was produced, including regression coefficients, their associated standard errors, z values, and p-values. From the output, everything looked normal. How should I interpret the warning message, and what actions, if any, should I take?

Thanks very much,

Peter
___________________________________________________________________________ This email may contain confidential and/or privileged information for the sole use of the intended recipient. Any review or distribution by others is strictly prohibited. If you have received this email in error, please contact the sender and delete all copies. Opinions, conclusions or other information expressed or contained in this email are not given or endorsed by the sender unless otherwise affirmed independently by the sender.


From bates at stat.wisc.edu  Sat Feb 22 22:01:05 2014
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 22 Feb 2014 15:01:05 -0600
Subject: [R-sig-ME] Order in which variance components are reported
In-Reply-To: <CABghstQ7R01dagqpZo-Y4ahbtg0g-v3iqT93AnE1mJ87R-msoA@mail.gmail.com>
References: <CAGG0PdBLLtxnPCSYH9c_cU0WnacdmLJ=Gh5B8RHLY7R6C_Jf3Q@mail.gmail.com>
	<CABghstQ7R01dagqpZo-Y4ahbtg0g-v3iqT93AnE1mJ87R-msoA@mail.gmail.com>
Message-ID: <CAO7JsnQ_V3yDtSXadtugF9TQzaqDSatSrfORVr3oiExAtHk4zg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140222/4e1178e4/attachment.pl>

From highstat at highstat.com  Sun Feb 23 11:44:02 2014
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Sun, 23 Feb 2014 10:44:02 +0000
Subject: [R-sig-ME] glmer function in lme4 - false convergence
 warning	message, (Austin, Peter)
In-Reply-To: <mailman.3569.1393102886.4563.r-sig-mixed-models@r-project.org>
References: <mailman.3569.1393102886.4563.r-sig-mixed-models@r-project.org>
Message-ID: <5309D0F2.9030800@highstat.com>



------------------------------

Message: 6
Date: Sat, 22 Feb 2014 15:51:55 +0000
From: "Austin, Peter" <peter.austin at ices.on.ca>
To: "r-sig-mixed-models at r-project.org"
	<r-sig-mixed-models at r-project.org>
Cc: "Austin, Peter" <peter.austin at ices.on.ca>
Subject: [R-sig-ME] glmer function in lme4 - false convergence warning
	message
Message-ID:
	<9ED811556BF51A4C8E7FD77C5133339B45483532 at ICESMBX2.ices.on.ca>
Content-Type: text/plain; charset="iso-8859-1"

I am writing with a question about a warning message that I received when I used the glmer function to fit a complementary log-log binary regression model that incorporated two different sets of random effects. When running the model, I received the following warning message:

Warning message: In mer_finalize(ans) : false convergence (8).

However, output was produced, including regression coefficients, their associated standard errors, z values, and p-values. From the output, everything looked normal. How should I interpret the warning message, and what actions, if any, should I take?

Thanks very much,

Peter
----



Standardize your continous covariates...most of the times that helps.

Alain Zuur





-- 
Dr. Alain F. Zuur

First author of:
1. Beginner's Guide to GAMM with R (2014).
2. Beginner's Guide to GLM and GLMM with R (2013).
3. Begginner's Guide to GAM with R (2012).
4. Zero Inflated Models and GLMM with R (2012).
5. A Beginner's Guide to R (2009).
6. Mixed effects models and extensions in ecology with R (2009).
7. Analysing Ecological Data (2007).

Highland Statistics Ltd.
9 St Clair Wynd
UK - AB41 6DZ Newburgh
Tel:   0044 1358 788177
Email: highstat at highstat.com
URL:   www.highstat.com


From bbolker at gmail.com  Sun Feb 23 21:05:05 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 23 Feb 2014 15:05:05 -0500
Subject: [R-sig-ME] GLMM R package
In-Reply-To: <1851_1392985935_s1LCWERS001472_CAPrOx-A-AzKSuFkwL619T9WBrKv=in75HrxOUvK78qBYv67fzw@mail.gmail.com>
References: <1851_1392985935_s1LCWERS001472_CAPrOx-A-AzKSuFkwL619T9WBrKv=in75HrxOUvK78qBYv67fzw@mail.gmail.com>
Message-ID: <530A5471.9050005@mcmaster.ca>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 14-02-21 07:31 AM, Alexis Cerezo wrote:
> Dear Dr. Bolker,
> 
> My name is Alexis Cerezo, I?m a quantitative ecologist working in 
> Argentina and Guatemala.  I write to you because, in your paper in
> TREE from 2009 on GLMM?s (Generalized linear mixed models: a
> practical guide for ecology and evolution), you mention an R
> package called GLMM (table 1), which has the capability of running
> GLMM?s with temporal or spatial autocorrelation structure.  I?ve
> looked for it but was unable to find it on the web, perhaps you
> could point me in the right direction, or send the package to me.

  [I'm Cc'ing this to the r-sig-mixed-models mailing list: I don't
have any very helpful answers, but maybe someone else will ...]

  That's really weird -- I haven't looked at that table in a long
time, and I really don't know what I meant -- there are occasionally
packages that pop up but are then not maintained on CRAN, but I don't
know what I would have meant at the time.

> I understand glmmPQL runs glmm?s with spatial or temporal 
> autocorrelation, but you mention in your paper that under PQL 
> estimation, parameter estimates (for fixed effects?) are biased if
> the variance of random effects is large, so I want to compare the
> results.

   What is fairly well established is that *random-effects variance
estimates* are downwardly biased under PQL.  It's not clear in general
what effect this has on fixed-effect parameter estimates.

  I don't know of a solution for spatial GLMMs that is entirely
satisfactory (i.e., both statistically reliable and easy to use). I
could (and perhaps should) write a lot more about this; to my
knowledge there's not really a better *published* reference than the
Dormann et al 2009 paper referenced below.  Some possible clues to
follow up, and for others to comment on:

  Bayesian methods or approximate Bayesian methods -- INLA,, geoRglm

  Marginal likelihood approximations (I know Doug Bates doesn't like
this term but I don't know a better rubric): use gls() or lme() with
spatial structure and 'weights' argument to mimic the mean variance
relationship;  AD Model Builder (see the 'spatial examples' demo page)

  I ran across some discussion of copula-based approaches somewhere
recently.

  Does anyone else have useful ideas?

> I also came across an R script that you developed which calculates
> AIC (QAICc, actually) values from models derived with glmmPQL, but
> I can?t seem to find it again (I was looking for something else,
> and stumbled into it, but unfortunately didn?t save it), would you
> please send it to me?

  I'm not sure I ever managed to create that script -- I may have
talked about it, but the destruction of the likelihood and AIC slots
in glmmPQL is pretty thorough, so it's hard to get the necessary
information back.

  Again, others on the list are encouraged to chime in ...
> 
> Thanks so much, I hope I don?t take up too much of your time.
> 
> Best regards,
> 
> Alexis
> 
> -- Alexis Cerezo
> 
> Departamento de M?todos Cuantitativos y Sistemas de Informaci?n 
> Facultad de Agronom?a-Universidad de Buenos Aires



@article{dormann_methods_2007,
	title = {Methods to account for spatial autocorrelation in the
analysis of species distributional data: a review},
	volume = {30},
	url = {http://dx.doi.org/10.1111/j.2007.0906-7590.05171.x},
	doi = {10.1111/j.2007.0906-7590.05171.x},
	abstract = {Species distributional or trait data based on range map
(extent-of-occurrence) or atlas survey data often display spatial
autocorrelation, i.e. locations close to each other exhibit more
similar values than those further apart. If this pattern remains
present in the residuals of a statistical model based on such data,
one of the key assumptions of standard statistical analyses, that
residuals are independent and identically distributed (i.i.d), is
violated. The violation of the assumption of i.i.d. residuals may bias
parameter estimates and can increase type I error rates (falsely
rejecting the null hypothesis of no effect). While this is
increasingly recognised by researchers analysing species distribution
data, there is, to our knowledge, no comprehensive overview of the
many available spatial statistical methods to take spatial
autocorrelation into account in tests of statistical significance.
Here, we describe six different statistical approaches to infer
correlates of species' distributions, for both presence/absence
(binary response) and species abundance data (poisson or normally
distributed response), while accounting for spatial autocorrelation in
model residuals: autocovariate regression; spatial eigenvector
mapping; generalised least squares; (conditional and simultaneous)
autoregressive models and generalised estimating equations. A
comprehensive comparison of the relative merits of these methods is
beyond the scope of this paper. To demonstrate each method's
implementation, however, we undertook preliminary tests based on
simulated data. These preliminary tests verified that most of the
spatial modeling techniques we examined showed good type I error
control and precise parameter estimates, at least when confronted with
simplistic simulated data containing spatial autocorrelation in the
errors. However, we found that for presence/absence data the results
and conclusions were very variable between the different methods. This
is likely due to the low information content of binary maps. Also, in
contrast with previous studies, we found that autocovariate methods
consistently underestimated the effects of environmental controls of
species distributions. Given their widespread use, in particular for
the modelling of species presence/absence data (e.g. climate envelope
models), we argue that this warrants further study and caution in
their use. To aid other ecologists in making use of the methods
described, code to implement them in freely available software is
provided in an electronic appendix.},
	number = {5},
	journal = {Ecography},
	author = {Dormann, Carsten F. and {Miguel B. Ara?jo} and Roger Bivand
and Janine Bolliger and Gudrun Carl and Richard G. Davies and
Alexandre Hirzel and Walter Jetz and W. Daniel Kissling and Ingolf
K?hn and Ralf Ohlem?ller and Pedro R. Peres-Neto and Bj?rn Reineking
and Boris Schr?der and Frank M. Schurr and Robert Wilson},
	year = {2007},
	pages = {609--628}
}

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.11 (GNU/Linux)
Comment: Using GnuPG with Thunderbird - http://www.enigmail.net/

iQEcBAEBAgAGBQJTClRwAAoJEOCV5YRblxUHL90H/A8AHG0J3IZMIRNME5CfvC+5
vakqE3Ebk3nmK9lDxAApIVjHz3Hrz1+deZMleTh3NJ2lFs9kI7ng53By1qn4MHG3
h5C+w+R3y7qD32nlKeoa0iYBrHvCKQk7UmmzAqT1j0gxexSGSLRpfpyIstc5maeW
yXr1mAXSRfxVAsDN+7SMHw981GdhaZTQ1/1xNiV19r/sQjBu0NBV7TFUNkZ7gIwq
DZL5yiUlTiagu8SiScrql9/ywIRH38I29m1F663Bldl5WIQSuX0WRmBCDPJc0bN4
246lK+EXRWAb/d9qsNHn1pDX06UdmIpYFO2nhow7fUiEDP1UppnrI6tkIl0NaS8=
=5bow
-----END PGP SIGNATURE-----


From rdiaz02 at gmail.com  Mon Feb 24 16:46:06 2014
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Mon, 24 Feb 2014 16:46:06 +0100
Subject: [R-sig-ME] Model is nearly unidentifiable with glmer
In-Reply-To: <87d2ijp96n.fsf@gmail.com>
References: <87d2ijp96n.fsf@gmail.com>
Message-ID: <87a9dgzf7l.fsf@gmail.com>


I think I figured out what was happening. This is the summary, in case it
helps anyone else in the future.


The problem seems to be caused by no variation at the "dataSetID" (the
random effect) level in some experimental level combinations. 


If I fit the model to some subsets of the among-dataSetID combinations
(i.e., exactly the same design, except with fewer values for some factors),
I can get a "singular fit" warning and 0 for the variance of the random
effect (and ranef which are all zero). Sure enough, at least in some cases,
changing just a single value of the response gets rid of the warning (and
leads to non-zero variance of the random effect).


Now, when I use larger models that are a superset of the above singular fit
models, I get the "nearly unidentifiable" problem, even when the output
from the fit does not show 0 for the variance of the random effects (and
the warning will disappear if we change just a few values of the response).


As originally thought, the warning is not caused by some inherent problem
in the model matrix ---i.e., in the design---.  But I could not identify
the problem because I was making wrong assumptions when seeing that the
random effects in the large models did not show 0 variance.



Best,


R.


 

On Wed, 19-02-2014, at 13:40, rdiaz02 at gmail.com wrote:
> Dear All,
>
> Summary:
> ========
>
> I am running some models with glmer that are giving, among others, the
> warning message
>
> In checkConv(attr(opt, "derivs"), opt$par, checkCtrl = control$checkConv,  :
>   Model is nearly unidentifiable: very large eigenvalue
>  - Rescale variables?
>
>
> but I cannot understand why the model is nearly unidentifable, based both
> on the details of the experimental design, and by comparison with other
> approaches (that run without complains and provide similar results).
> Moreover, I have no idea what I am supposed to rescale.
>
>
>
>
> Details:
> ========
>  
> The call to glmer
> -----------------
>
> gt.0 <- glmer(nS ~ Tree + Model + sh + S.Type + S.Time + S.Size + Method +
>                             (1|dataSetID) + (1|obs), 
>                             family = poisson,
>                             data = NP.all,
>                             control = glmerControl(
>                                 check.conv.singular="warning",
>                                 optCtrl = list(maxfun = 10000))
>  				)
>
> The model is for Poisson data, allowing for overdispersion (with the obs
> variable) and with a "dataSetID" random effect. (The glmerControl has been
> added to try to understand what might be happening).
>
>
>
> The design
> ----------
>
> The data come from a simulation experiment. For each combination of
>
> Tree * Model * sh * S.Type * S.Time * S.Size there are 20 replicate
> simulations (each identified by a dataSetID), so a full factorial
> design. Each variable is a factor variable (6 levels for Tree, 4 for Model,
> 2 for sh, 3 for S.Type, 2 for S.Time, 3 for S.Size)
>
> Each replicate simulation, identified by a "dataSetID", is subject to four
> Methods (a factor). That is why dataSetID is a random effect.
>
> There are no missing values.
>
> Thus, there are 17280 dataSetID groups (6*4*2*3*2*3*20), each with four
> observations, resulting in a total of 69120 observations. glmer does
> report these values just fine.
>
> Therefore, based on that design, I think I should be able to fit not just
> the model above, but a model with all possible interactions. In fact, that
> model (a saturated model in the log-linear parlance, IIUC) is what I want
> to start from.
>
>
> The warnings
> ------------
>
> glmer gives two warnings:
> 1: In checkConv(attr(opt, "derivs"), opt$par, checkCtrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 392.339 (tol = 0.001)
> 2: In checkConv(attr(opt, "derivs"), opt$par, checkCtrl = control$checkConv,  :
>   Model is nearly unidentifiable: very large eigenvalue
>  - Rescale variables?
>
>
> The failure to converge I don't like, but OK. However, the "nearly
> unidentifiable" I just don't understand. And what variables am I supposed
> to rescale?
>
>
> Removing the random effect for obs or for dataSetID does not eliminate the
> warnings (and, in fact, that should not be the problem, since both obs and
> dataSetID get non-zero estimates).
>
>  
> Other methods
> -------------
>
> I have fitted the above model with:
>
> - MCMCglmm (using the priors list(R = list(V = 1, nu = 0.002), G = list(G1
>   = list(V = 1, nu = 0.002))))
>
>
> - bglmer (with all as per default)
>
>
> - glmer2stan (here using a more complex model with a bunch of interactions)
>
> - glmmadmb (with family "nbinom")
>
> and even if glmmadmb does complain about lack of convergence, there are no
> further problems.
>
> Moreover, with MCMCglmm I've fitted the model with interactions. Again, no
> apparent problems (beyond slow mixing in some cases).
>
>
> In addition, I have also fitted several other models that include all
> possible interactions, just to make sure again I do not have something
> silly in the model matrix. In some cases, the response is different (as
> when fitting a lm/lmer). Beyond the lack of convergence of lmer fit, no
> other problems.
>
>
> ## a GLM with interactions but, of course, without the dataSetID random
>    effect.
>
> glm(nS ~ Tree * Model * sh *
>                              S.Type * S.Time * S.Size * Method,
>                              data = NP.all,
>                              family = poisson
>                              )
>
> ## a linear model with interactions 			     
> 			     
> lm(Dissim ~ Tree * Model * sh *
>                              S.Type * S.Time * S.Size * Method,
>                              data = NP.all
>                              )
>  
> ## a lmers
>
> lmer(Dissim ~ Tree * Model * sh *
>                              S.Type * S.Time * S.Size * Method +
>                              (1|dataSetID),
>                              data = NP.all,
>                              control = lmerControl(
>                                  check.conv.singular="warning",
>                                  optCtrl = list(maxfun = 10000))
>                              )
> ## This complaints of lack of convergence, but no identifiability
>                              problems. 
> 			     
>
> 			     
> (Lack of) Differences between fits
> -----------------------------------
>
> If anyone wants I can provide, of course, the output from the
> fits. Anyway, the qualitative summary is that the the estimates for the
> fixed are very similar between glmer, admb, bglmer, and MCMCglmm and
> the estimates for the random effects are very similar for MCMCglmm,
> bglmer, and glmer.
>
>
>
>
>
> So, what am I missing?
>
>
> Best,
>
>
>
> R.

-- 
Ramon Diaz-Uriarte
Department of Biochemistry, Lab B-25
Facultad de Medicina 
Universidad Aut?noma de Madrid 
Arzobispo Morcillo, 4
28029 Madrid
Spain

Phone: +34-91-497-2412

Email: rdiaz02 at gmail.com
       ramon.diaz at iib.uam.es

http://ligarto.org/rdiaz


From zaslavsk at hcp.med.harvard.edu  Sun Feb 23 00:02:15 2014
From: zaslavsk at hcp.med.harvard.edu (Zaslavsky, Alan M.)
Date: Sat, 22 Feb 2014 18:02:15 -0500
Subject: [R-sig-ME] Specifying fixed residual variances for multilevel models
Message-ID: <A1E2A16402841149A85612B5EC3D6D76028544865F0C@ITCCRMAIL01.MED.HARVARD.EDU>

We have a bunch of analyses in which we would like to specify residual variances that are calculated outside the multilevel modeling function.  We were proceeding on the assumption that using varFixed() for the weights argument to lme() would do this, but after doing a few simulations it became evident that this was only fixing the residual variances up to a proportionality constant and in fact rescaling the weights had no effect on estimates of random effects variances.  

I've spent a few hours scanning the documentation, list comments and FAQs without tracking down a solution.  This seems to be more a feature than a bug, although comments I see on this list and Github, among other places suggest that the handling of residual variances or weights is the subject of current development efforts (lme4a?).  Not surprising --- in fact the terms 'weights' has at least as many meanings as 'fixed effects', with potentially different analytic implications (especially for mixed models).  I would urge talking explicitly about residual variance models, which have a clear interpretation.

Any thoughts?  

                Alan Zaslavsky
                zaslavsk at hcp.med.harvard.edu

NLME EXAMPLE
> group=rep(1:40,10)                        ## 40 groups of 10 observations
> V=401:800/400                              ## residual variances
> y=rnorm(40)[group]+rnorm(400)*sqrt(V)
> lme(fixed=y~1,random=~1|group)   ## unweighted
Linear mixed-effects model fit by REML
  Data: NULL 
  Log-restricted-likelihood: -696.8343
  Fixed: y ~ 1 
(Intercept) 
  0.3656883 
Random effects:
 Formula: ~1 | group
        (Intercept) Residual
StdDev:   0.9231727 1.257827
Number of Observations: 400
Number of Groups: 40 
> lme(fixed=y~1,random=~1|group,weight=varFixed(~V))
Linear mixed-effects model fit by REML
  Data: NULL 
  Log-restricted-likelihood: -692.0942
  Fixed: y ~ 1 
(Intercept) 
  0.3513349 
Random effects:
 Formula: ~1 | group
        (Intercept) Residual
StdDev:   0.9261319 1.021891
Variance function:
 Structure: fixed weights
 Formula: ~V 
Number of Observations: 400
Number of Groups: 40 
##  the one above specified the residual variances that were used in generating the
##  the data and there estimated scale factor close to 1, as it should
> V2=3*V
> lme(fixed=y~1,random=~1|group,weight=varFixed(~V2))
Linear mixed-effects model fit by REML
  Data: NULL 
  Log-restricted-likelihood: -692.0942
  Fixed: y ~ 1 
(Intercept) 
  0.3513349 
Random effects:
 Formula: ~1 | group
        (Intercept)  Residual
StdDev:   0.9261319 0.5899889
Variance function:
 Structure: fixed weights
 Formula: ~V2 
Number of Observations: 400
Number of Groups: 40 
###  here we increased the specified residual variance by a factor of 3, but it was ignored except that the residual scale factor went down by a factor of sqrt(3) as shown below.  All very good if you only know *relative* residual variances but not helpful if you know them *absolutely*
> (1.021891/.5899889)^2
[1] 3.000001

LMER (same data)
> lmer(formula=y~1+(1|group),weights=V)
Linear mixed model fit by REML ['lmerMod']
Formula: y ~ 1 + (1 | group) 
REML criterion at convergence: 1570.624 
Random effects:
 Groups   Name        Std.Dev.
 group    (Intercept) 0.7569  
 Residual             1.2824  
Number of obs: 400, groups: group, 40
Fixed Effects:
(Intercept)  
     0.3788  
> lmer(formula=y~1+(1|group),weights=V2)
Linear mixed model fit by REML ['lmerMod']
Formula: y ~ 1 + (1 | group) 
REML criterion at convergence: 2010.069 
Random effects:
 Groups   Name        Std.Dev.
 group    (Intercept) 0.437   
 Residual             1.282   
Number of obs: 400, groups: group, 40
Fixed Effects:
(Intercept)  
     0.3788  
############  even stranger -- rescaling residual variance leaves residual var estimate alone, but changes level-2 variance by factor of 3
> (.7569/.437)^2
[1] 2.999951

From maechler at stat.math.ethz.ch  Mon Feb 24 19:11:10 2014
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 24 Feb 2014 19:11:10 +0100
Subject: [R-sig-ME] Model is nearly unidentifiable with glmer
In-Reply-To: <87a9dgzf7l.fsf@gmail.com>
References: <87d2ijp96n.fsf@gmail.com>
	<87a9dgzf7l.fsf@gmail.com>
Message-ID: <21259.35646.496086.844973@stat.math.ethz.ch>

>>>>> Ramon Diaz-Uriarte <rdiaz02 at gmail.com>
>>>>>     on Mon, 24 Feb 2014 16:46:06 +0100 writes:

    > I think I figured out what was happening. This is the summary, in case it
    > helps anyone else in the future.

Thank you, Ramon,
for this summary even though nobody seemed to have helped you.

I think we lme4 developers should be quite happy if you could
provide us with a reproducible example -- off-public if you
want.

If your real data is somewhat confidential,
I think you could use

new.y <- simulate(<fitted glmer>)

and add new.y to your data.frame and subtract the original y
(and then check that 'new.y' also shows the problem).
We'd be happy to receive the 
      save(<data-frame>, file = "RDU-glmer-ex.rda")
data together with an R script using glmer() and producing the
warnings you've been seeing.

Thank you in advance.
Martin Maechler, ETH Zurich

    > The problem seems to be caused by no variation at the "dataSetID" (the
    > random effect) level in some experimental level combinations. 


    > If I fit the model to some subsets of the among-dataSetID combinations
    > (i.e., exactly the same design, except with fewer values for some factors),
    > I can get a "singular fit" warning and 0 for the variance of the random
    > effect (and ranef which are all zero). Sure enough, at least in some cases,
    > changing just a single value of the response gets rid of the warning (and
    > leads to non-zero variance of the random effect).


    > Now, when I use larger models that are a superset of the above singular fit
    > models, I get the "nearly unidentifiable" problem, even when the output
    > from the fit does not show 0 for the variance of the random effects (and
    > the warning will disappear if we change just a few values of the response).


    > As originally thought, the warning is not caused by some inherent problem
    > in the model matrix ---i.e., in the design---.  But I could not identify
    > the problem because I was making wrong assumptions when seeing that the
    > random effects in the large models did not show 0 variance.



    > Best,


    > R.


 

    > On Wed, 19-02-2014, at 13:40, rdiaz02 at gmail.com wrote:
    >> Dear All,
    >> 
    >> Summary:
    >> ========
    >> 
    >> I am running some models with glmer that are giving, among others, the
    >> warning message
    >> 
    >> In checkConv(attr(opt, "derivs"), opt$par, checkCtrl = control$checkConv,  :
    >> Model is nearly unidentifiable: very large eigenvalue
    >> - Rescale variables?
    >> 
    >> 
    >> but I cannot understand why the model is nearly unidentifable, based both
    >> on the details of the experimental design, and by comparison with other
    >> approaches (that run without complains and provide similar results).
    >> Moreover, I have no idea what I am supposed to rescale.
    >> 
    >> 
    >> 
    >> 
    >> Details:
    >> ========
    >> 
    >> The call to glmer
    >> -----------------
    >> 
    >> gt.0 <- glmer(nS ~ Tree + Model + sh + S.Type + S.Time + S.Size + Method +
    >> (1|dataSetID) + (1|obs), 
    >> family = poisson,
    >> data = NP.all,
    >> control = glmerControl(
    >> check.conv.singular="warning",
    >> optCtrl = list(maxfun = 10000))
    >> )
    >> 
    >> The model is for Poisson data, allowing for overdispersion (with the obs
    >> variable) and with a "dataSetID" random effect. (The glmerControl has been
    >> added to try to understand what might be happening).
    >> 
    >> 
    >> 
    >> The design
    >> ----------
    >> 
    >> The data come from a simulation experiment. For each combination of
    >> 
    >> Tree * Model * sh * S.Type * S.Time * S.Size there are 20 replicate
    >> simulations (each identified by a dataSetID), so a full factorial
    >> design. Each variable is a factor variable (6 levels for Tree, 4 for Model,
    >> 2 for sh, 3 for S.Type, 2 for S.Time, 3 for S.Size)
    >> 
    >> Each replicate simulation, identified by a "dataSetID", is subject to four
    >> Methods (a factor). That is why dataSetID is a random effect.
    >> 
    >> There are no missing values.
    >> 
    >> Thus, there are 17280 dataSetID groups (6*4*2*3*2*3*20), each with four
    >> observations, resulting in a total of 69120 observations. glmer does
    >> report these values just fine.
    >> 
    >> Therefore, based on that design, I think I should be able to fit not just
    >> the model above, but a model with all possible interactions. In fact, that
    >> model (a saturated model in the log-linear parlance, IIUC) is what I want
    >> to start from.
    >> 
    >> 
    >> The warnings
    >> ------------
    >> 
    >> glmer gives two warnings:
    >> 1: In checkConv(attr(opt, "derivs"), opt$par, checkCtrl = control$checkConv,  :
    >> Model failed to converge with max|grad| = 392.339 (tol = 0.001)
    >> 2: In checkConv(attr(opt, "derivs"), opt$par, checkCtrl = control$checkConv,  :
    >> Model is nearly unidentifiable: very large eigenvalue
    >> - Rescale variables?
    >> 
    >> 
    >> The failure to converge I don't like, but OK. However, the "nearly
    >> unidentifiable" I just don't understand. And what variables am I supposed
    >> to rescale?
    >> 
    >> 
    >> Removing the random effect for obs or for dataSetID does not eliminate the
    >> warnings (and, in fact, that should not be the problem, since both obs and
    >> dataSetID get non-zero estimates).
    >> 
    >> 
    >> Other methods
    >> -------------
    >> 
    >> I have fitted the above model with:
    >> 
    >> - MCMCglmm (using the priors list(R = list(V = 1, nu = 0.002), G = list(G1
    >> = list(V = 1, nu = 0.002))))
    >> 
    >> 
    >> - bglmer (with all as per default)
    >> 
    >> 
    >> - glmer2stan (here using a more complex model with a bunch of interactions)
    >> 
    >> - glmmadmb (with family "nbinom")
    >> 
    >> and even if glmmadmb does complain about lack of convergence, there are no
    >> further problems.
    >> 
    >> Moreover, with MCMCglmm I've fitted the model with interactions. Again, no
    >> apparent problems (beyond slow mixing in some cases).
    >> 
    >> 
    >> In addition, I have also fitted several other models that include all
    >> possible interactions, just to make sure again I do not have something
    >> silly in the model matrix. In some cases, the response is different (as
    >> when fitting a lm/lmer). Beyond the lack of convergence of lmer fit, no
    >> other problems.
    >> 
    >> 
    >> ## a GLM with interactions but, of course, without the dataSetID random
    >> effect.
    >> 
    >> glm(nS ~ Tree * Model * sh *
    >> S.Type * S.Time * S.Size * Method,
    >> data = NP.all,
    >> family = poisson
    >> )
    >> 
    >> ## a linear model with interactions 			     
    >> 
    >> lm(Dissim ~ Tree * Model * sh *
    >> S.Type * S.Time * S.Size * Method,
    >> data = NP.all
    >> )
    >> 
    >> ## a lmers
    >> 
    >> lmer(Dissim ~ Tree * Model * sh *
    >> S.Type * S.Time * S.Size * Method +
    >> (1|dataSetID),
    >> data = NP.all,
    >> control = lmerControl(
    >> check.conv.singular="warning",
    >> optCtrl = list(maxfun = 10000))
    >> )
    >> ## This complaints of lack of convergence, but no identifiability
    >> problems. 
    >> 
    >> 
    >> 
    >> (Lack of) Differences between fits
    >> -----------------------------------
    >> 
    >> If anyone wants I can provide, of course, the output from the
    >> fits. Anyway, the qualitative summary is that the the estimates for the
    >> fixed are very similar between glmer, admb, bglmer, and MCMCglmm and
    >> the estimates for the random effects are very similar for MCMCglmm,
    >> bglmer, and glmer.
    >> 
    >> 
    >> 
    >> 
    >> 
    >> So, what am I missing?
    >> 
    >> 
    >> Best,
    >> 
    >> 
    >> 
    >> R.

    > -- 
    > Ramon Diaz-Uriarte
    > Department of Biochemistry, Lab B-25
    > Facultad de Medicina 
    > Universidad Aut?noma de Madrid 
    > Arzobispo Morcillo, 4
    > 28029 Madrid
    > Spain

    > Phone: +34-91-497-2412

    > Email: rdiaz02 at gmail.com
    > ramon.diaz at iib.uam.es

    > http://ligarto.org/rdiaz

    > _______________________________________________
    > R-sig-mixed-models at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From rdiaz02 at gmail.com  Mon Feb 24 20:07:21 2014
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Mon, 24 Feb 2014 20:07:21 +0100
Subject: [R-sig-ME] Model is nearly unidentifiable with glmer
In-Reply-To: <21259.35646.496086.844973@stat.math.ethz.ch>
References: <87d2ijp96n.fsf@gmail.com> <87a9dgzf7l.fsf@gmail.com>
	<21259.35646.496086.844973@stat.math.ethz.ch>
Message-ID: <8738j8z5w6.fsf@gmail.com>


Dear Martin,



On Mon, 24-02-2014, at 19:11, maechler at stat.math.ethz.ch wrote:
>>>>>> Ramon Diaz-Uriarte <rdiaz02 at gmail.com>
>>>>>>     on Mon, 24 Feb 2014 16:46:06 +0100 writes:
>
>     > I think I figured out what was happening. This is the summary, in case it
>     > helps anyone else in the future.
>
> Thank you, Ramon,
> for this summary even though nobody seemed to have helped you.
>
> I think we lme4 developers should be quite happy if you could
> provide us with a reproducible example -- off-public if you
> want.

Sure, I can provide the data and scripts to show the problem (as well as my
explorations of the issue). The RData is about 2.5 MB; should I send it to
the list as an attachment?


Best,


R.


>
> If your real data is somewhat confidential,
> I think you could use
>
> new.y <- simulate(<fitted glmer>)
>
> and add new.y to your data.frame and subtract the original y
> (and then check that 'new.y' also shows the problem).
> We'd be happy to receive the 
>       save(<data-frame>, file = "RDU-glmer-ex.rda")
> data together with an R script using glmer() and producing the
> warnings you've been seeing.
>
> Thank you in advance.
> Martin Maechler, ETH Zurich
>
>     > The problem seems to be caused by no variation at the "dataSetID" (the
>     > random effect) level in some experimental level combinations. 
>
>
>     > If I fit the model to some subsets of the among-dataSetID combinations
>     > (i.e., exactly the same design, except with fewer values for some factors),
>     > I can get a "singular fit" warning and 0 for the variance of the random
>     > effect (and ranef which are all zero). Sure enough, at least in some cases,
>     > changing just a single value of the response gets rid of the warning (and
>     > leads to non-zero variance of the random effect).
>
>
>     > Now, when I use larger models that are a superset of the above singular fit
>     > models, I get the "nearly unidentifiable" problem, even when the output
>     > from the fit does not show 0 for the variance of the random effects (and
>     > the warning will disappear if we change just a few values of the response).
>
>
>     > As originally thought, the warning is not caused by some inherent problem
>     > in the model matrix ---i.e., in the design---.  But I could not identify
>     > the problem because I was making wrong assumptions when seeing that the
>     > random effects in the large models did not show 0 variance.
>
>
>
>     > Best,
>
>
>     > R.
>
>
>  
>
>     > On Wed, 19-02-2014, at 13:40, rdiaz02 at gmail.com wrote:
>     >> Dear All,
>     >> 
>     >> Summary:
>     >> ========
>     >> 
>     >> I am running some models with glmer that are giving, among others, the
>     >> warning message
>     >> 
>     >> In checkConv(attr(opt, "derivs"), opt$par, checkCtrl = control$checkConv,  :
>     >> Model is nearly unidentifiable: very large eigenvalue
>     >> - Rescale variables?
>     >> 
>     >> 
>     >> but I cannot understand why the model is nearly unidentifable, based both
>     >> on the details of the experimental design, and by comparison with other
>     >> approaches (that run without complains and provide similar results).
>     >> Moreover, I have no idea what I am supposed to rescale.
>     >> 
>     >> 
>     >> 
>     >> 
>     >> Details:
>     >> ========
>     >> 
>     >> The call to glmer
>     >> -----------------
>     >> 
>     >> gt.0 <- glmer(nS ~ Tree + Model + sh + S.Type + S.Time + S.Size + Method +
>     >> (1|dataSetID) + (1|obs), 
>     >> family = poisson,
>     >> data = NP.all,
>     >> control = glmerControl(
>     >> check.conv.singular="warning",
>     >> optCtrl = list(maxfun = 10000))
>     >> )
>     >> 
>     >> The model is for Poisson data, allowing for overdispersion (with the obs
>     >> variable) and with a "dataSetID" random effect. (The glmerControl has been
>     >> added to try to understand what might be happening).
>     >> 
>     >> 
>     >> 
>     >> The design
>     >> ----------
>     >> 
>     >> The data come from a simulation experiment. For each combination of
>     >> 
>     >> Tree * Model * sh * S.Type * S.Time * S.Size there are 20 replicate
>     >> simulations (each identified by a dataSetID), so a full factorial
>     >> design. Each variable is a factor variable (6 levels for Tree, 4 for Model,
>     >> 2 for sh, 3 for S.Type, 2 for S.Time, 3 for S.Size)
>     >> 
>     >> Each replicate simulation, identified by a "dataSetID", is subject to four
>     >> Methods (a factor). That is why dataSetID is a random effect.
>     >> 
>     >> There are no missing values.
>     >> 
>     >> Thus, there are 17280 dataSetID groups (6*4*2*3*2*3*20), each with four
>     >> observations, resulting in a total of 69120 observations. glmer does
>     >> report these values just fine.
>     >> 
>     >> Therefore, based on that design, I think I should be able to fit not just
>     >> the model above, but a model with all possible interactions. In fact, that
>     >> model (a saturated model in the log-linear parlance, IIUC) is what I want
>     >> to start from.
>     >> 
>     >> 
>     >> The warnings
>     >> ------------
>     >> 
>     >> glmer gives two warnings:
>     >> 1: In checkConv(attr(opt, "derivs"), opt$par, checkCtrl = control$checkConv,  :
>     >> Model failed to converge with max|grad| = 392.339 (tol = 0.001)
>     >> 2: In checkConv(attr(opt, "derivs"), opt$par, checkCtrl = control$checkConv,  :
>     >> Model is nearly unidentifiable: very large eigenvalue
>     >> - Rescale variables?
>     >> 
>     >> 
>     >> The failure to converge I don't like, but OK. However, the "nearly
>     >> unidentifiable" I just don't understand. And what variables am I supposed
>     >> to rescale?
>     >> 
>     >> 
>     >> Removing the random effect for obs or for dataSetID does not eliminate the
>     >> warnings (and, in fact, that should not be the problem, since both obs and
>     >> dataSetID get non-zero estimates).
>     >> 
>     >> 
>     >> Other methods
>     >> -------------
>     >> 
>     >> I have fitted the above model with:
>     >> 
>     >> - MCMCglmm (using the priors list(R = list(V = 1, nu = 0.002), G = list(G1
>     >> = list(V = 1, nu = 0.002))))
>     >> 
>     >> 
>     >> - bglmer (with all as per default)
>     >> 
>     >> 
>     >> - glmer2stan (here using a more complex model with a bunch of interactions)
>     >> 
>     >> - glmmadmb (with family "nbinom")
>     >> 
>     >> and even if glmmadmb does complain about lack of convergence, there are no
>     >> further problems.
>     >> 
>     >> Moreover, with MCMCglmm I've fitted the model with interactions. Again, no
>     >> apparent problems (beyond slow mixing in some cases).
>     >> 
>     >> 
>     >> In addition, I have also fitted several other models that include all
>     >> possible interactions, just to make sure again I do not have something
>     >> silly in the model matrix. In some cases, the response is different (as
>     >> when fitting a lm/lmer). Beyond the lack of convergence of lmer fit, no
>     >> other problems.
>     >> 
>     >> 
>     >> ## a GLM with interactions but, of course, without the dataSetID random
>     >> effect.
>     >> 
>     >> glm(nS ~ Tree * Model * sh *
>     >> S.Type * S.Time * S.Size * Method,
>     >> data = NP.all,
>     >> family = poisson
>     >> )
>     >> 
>     >> ## a linear model with interactions 			     
>     >> 
>     >> lm(Dissim ~ Tree * Model * sh *
>     >> S.Type * S.Time * S.Size * Method,
>     >> data = NP.all
>     >> )
>     >> 
>     >> ## a lmers
>     >> 
>     >> lmer(Dissim ~ Tree * Model * sh *
>     >> S.Type * S.Time * S.Size * Method +
>     >> (1|dataSetID),
>     >> data = NP.all,
>     >> control = lmerControl(
>     >> check.conv.singular="warning",
>     >> optCtrl = list(maxfun = 10000))
>     >> )
>     >> ## This complaints of lack of convergence, but no identifiability
>     >> problems. 
>     >> 
>     >> 
>     >> 
>     >> (Lack of) Differences between fits
>     >> -----------------------------------
>     >> 
>     >> If anyone wants I can provide, of course, the output from the
>     >> fits. Anyway, the qualitative summary is that the the estimates for the
>     >> fixed are very similar between glmer, admb, bglmer, and MCMCglmm and
>     >> the estimates for the random effects are very similar for MCMCglmm,
>     >> bglmer, and glmer.
>     >> 
>     >> 
>     >> 
>     >> 
>     >> 
>     >> So, what am I missing?
>     >> 
>     >> 
>     >> Best,
>     >> 
>     >> 
>     >> 
>     >> R.
>
>     > -- 
>     > Ramon Diaz-Uriarte
>     > Department of Biochemistry, Lab B-25
>     > Facultad de Medicina 
>     > Universidad Aut?noma de Madrid 
>     > Arzobispo Morcillo, 4
>     > 28029 Madrid
>     > Spain
>
>     > Phone: +34-91-497-2412
>
>     > Email: rdiaz02 at gmail.com
>     > ramon.diaz at iib.uam.es
>
>     > http://ligarto.org/rdiaz
>
>     > _______________________________________________
>     > R-sig-mixed-models at r-project.org mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Ramon Diaz-Uriarte
Department of Biochemistry, Lab B-25
Facultad de Medicina 
Universidad Aut?noma de Madrid 
Arzobispo Morcillo, 4
28029 Madrid
Spain

Phone: +34-91-497-2412

Email: rdiaz02 at gmail.com
       ramon.diaz at iib.uam.es

http://ligarto.org/rdiaz


From vjd4 at nyu.edu  Mon Feb 24 20:26:45 2014
From: vjd4 at nyu.edu (Vincent Dorie)
Date: Mon, 24 Feb 2014 14:26:45 -0500
Subject: [R-sig-ME] Specifying fixed residual variances for multilevel
	models
In-Reply-To: <A1E2A16402841149A85612B5EC3D6D76028544865F0C@ITCCRMAIL01.MED.HARVARD.EDU>
References: <A1E2A16402841149A85612B5EC3D6D76028544865F0C@ITCCRMAIL01.MED.HARVARD.EDU>
Message-ID: <2E5306D0-E242-4994-BD42-C2CCD6DF4A43@nyu.edu>

I'm not familiar with how lme works, but if the lmer formulation is sufficient then you can fit models of that sort using blmer in the blme package. Simply fix the common residual variance to one and supply the weights, at which point they'll behave as desired.

For example,

library(blme)
blmer(formula = y ~ 1 + (1 | group), weights = V,
      resid.prior = point(1.0), cov.prior = NULL)

Vince

On Feb 22, 2014, at 6:02 PM, Zaslavsky, Alan M. wrote:

> We have a bunch of analyses in which we would like to specify residual variances that are calculated outside the multilevel modeling function.  We were proceeding on the assumption that using varFixed() for the weights argument to lme() would do this, but after doing a few simulations it became evident that this was only fixing the residual variances up to a proportionality constant and in fact rescaling the weights had no effect on estimates of random effects variances.  
> 
> I've spent a few hours scanning the documentation, list comments and FAQs without tracking down a solution.  This seems to be more a feature than a bug, although comments I see on this list and Github, among other places suggest that the handling of residual variances or weights is the subject of current development efforts (lme4a?).  Not surprising --- in fact the terms 'weights' has at least as many meanings as 'fixed effects', with potentially different analytic implications (especially for mixed models).  I would urge talking explicitly about residual variance models, which have a clear interpretation.
> 
> Any thoughts?  
> 
>                Alan Zaslavsky
>                zaslavsk at hcp.med.harvard.edu
> 
> NLME EXAMPLE
>> group=rep(1:40,10)                        ## 40 groups of 10 observations
>> V=401:800/400                              ## residual variances
>> y=rnorm(40)[group]+rnorm(400)*sqrt(V)
>> lme(fixed=y~1,random=~1|group)   ## unweighted
> Linear mixed-effects model fit by REML
>  Data: NULL 
>  Log-restricted-likelihood: -696.8343
>  Fixed: y ~ 1 
> (Intercept) 
>  0.3656883 
> Random effects:
> Formula: ~1 | group
>        (Intercept) Residual
> StdDev:   0.9231727 1.257827
> Number of Observations: 400
> Number of Groups: 40 
>> lme(fixed=y~1,random=~1|group,weight=varFixed(~V))
> Linear mixed-effects model fit by REML
>  Data: NULL 
>  Log-restricted-likelihood: -692.0942
>  Fixed: y ~ 1 
> (Intercept) 
>  0.3513349 
> Random effects:
> Formula: ~1 | group
>        (Intercept) Residual
> StdDev:   0.9261319 1.021891
> Variance function:
> Structure: fixed weights
> Formula: ~V 
> Number of Observations: 400
> Number of Groups: 40 
> ##  the one above specified the residual variances that were used in generating the
> ##  the data and there estimated scale factor close to 1, as it should
>> V2=3*V
>> lme(fixed=y~1,random=~1|group,weight=varFixed(~V2))
> Linear mixed-effects model fit by REML
>  Data: NULL 
>  Log-restricted-likelihood: -692.0942
>  Fixed: y ~ 1 
> (Intercept) 
>  0.3513349 
> Random effects:
> Formula: ~1 | group
>        (Intercept)  Residual
> StdDev:   0.9261319 0.5899889
> Variance function:
> Structure: fixed weights
> Formula: ~V2 
> Number of Observations: 400
> Number of Groups: 40 
> ###  here we increased the specified residual variance by a factor of 3, but it was ignored except that the residual scale factor went down by a factor of sqrt(3) as shown below.  All very good if you only know *relative* residual variances but not helpful if you know them *absolutely*
>> (1.021891/.5899889)^2
> [1] 3.000001
> 
> LMER (same data)
>> lmer(formula=y~1+(1|group),weights=V)
> Linear mixed model fit by REML ['lmerMod']
> Formula: y ~ 1 + (1 | group) 
> REML criterion at convergence: 1570.624 
> Random effects:
> Groups   Name        Std.Dev.
> group    (Intercept) 0.7569  
> Residual             1.2824  
> Number of obs: 400, groups: group, 40
> Fixed Effects:
> (Intercept)  
>     0.3788  
>> lmer(formula=y~1+(1|group),weights=V2)
> Linear mixed model fit by REML ['lmerMod']
> Formula: y ~ 1 + (1 | group) 
> REML criterion at convergence: 2010.069 
> Random effects:
> Groups   Name        Std.Dev.
> group    (Intercept) 0.437   
> Residual             1.282   
> Number of obs: 400, groups: group, 40
> Fixed Effects:
> (Intercept)  
>     0.3788  
> ############  even stranger -- rescaling residual variance leaves residual var estimate alone, but changes level-2 variance by factor of 3
>> (.7569/.437)^2
> [1] 2.999951
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From bbolker at gmail.com  Tue Feb 25 02:04:45 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 24 Feb 2014 20:04:45 -0500
Subject: [R-sig-ME] spatial GLMMs
Message-ID: <530BEC2D.1000802@gmail.com>

  Just came across this, of possible interest to folks here.  Haven't
taken a look at it yet ...

Testing environmental and genetic effects in the presence of spatial
autocorrelation: Rousset and Ferdy, Ecography
Article first published online: 18 FEB 2014
DOI: 10.1111/ecog.00566

  Ben Bolker


From kw.stat at gmail.com  Tue Feb 25 02:19:27 2014
From: kw.stat at gmail.com (Kevin Wright)
Date: Mon, 24 Feb 2014 19:19:27 -0600
Subject: [R-sig-ME] spatial GLMMs
In-Reply-To: <530BEC2D.1000802@gmail.com>
References: <530BEC2D.1000802@gmail.com>
Message-ID: <CAKFxdiQ7C_B5GFtBhxpxFZLab5UJpkPUotvbXOP9airBNDvxSg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140224/55875ace/attachment.pl>

From rdiaz02 at gmail.com  Tue Feb 25 11:48:11 2014
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Tue, 25 Feb 2014 11:48:11 +0100
Subject: [R-sig-ME] Model is nearly unidentifiable with glmer
In-Reply-To: <21259.35646.496086.844973@stat.math.ethz.ch>
References: <87d2ijp96n.fsf@gmail.com> <87a9dgzf7l.fsf@gmail.com>
	<21259.35646.496086.844973@stat.math.ethz.ch>
Message-ID: <87vbw3xyc4.fsf@gmail.com>

Dear Martin,


On Mon, 24-02-2014, at 19:11, maechler at stat.math.ethz.ch wrote:
>>>>>> Ramon Diaz-Uriarte <rdiaz02 at gmail.com>
>>>>>>     on Mon, 24 Feb 2014 16:46:06 +0100 writes:
>
>     > I think I figured out what was happening. This is the summary, in case it
>     > helps anyone else in the future.
>
> Thank you, Ramon,
> for this summary even though nobody seemed to have helped you.
>
> I think we lme4 developers should be quite happy if you could
> provide us with a reproducible example -- off-public if you
> want.
>
> If your real data is somewhat confidential,
> I think you could use
>
> new.y <- simulate(<fitted glmer>)
>
> and add new.y to your data.frame and subtract the original y
> (and then check that 'new.y' also shows the problem).
> We'd be happy to receive the 
>       save(<data-frame>, file = "RDU-glmer-ex.rda")
> data together with an R script using glmer() and producing the
> warnings you've been seeing.


I've left at

http://ligarto.org/rdiaz/nearly-unidentifiable.tar.gz


a small (230 KB) file with:

- data
- R code to show the problem
- two runs of that code on different machines



Best,


R.



>
> Thank you in advance.
> Martin Maechler, ETH Zurich
>
>     > The problem seems to be caused by no variation at the "dataSetID" (the
>     > random effect) level in some experimental level combinations. 
>
>
>     > If I fit the model to some subsets of the among-dataSetID combinations
>     > (i.e., exactly the same design, except with fewer values for some factors),
>     > I can get a "singular fit" warning and 0 for the variance of the random
>     > effect (and ranef which are all zero). Sure enough, at least in some cases,
>     > changing just a single value of the response gets rid of the warning (and
>     > leads to non-zero variance of the random effect).
>
>
>     > Now, when I use larger models that are a superset of the above singular fit
>     > models, I get the "nearly unidentifiable" problem, even when the output
>     > from the fit does not show 0 for the variance of the random effects (and
>     > the warning will disappear if we change just a few values of the response).
>
>
>     > As originally thought, the warning is not caused by some inherent problem
>     > in the model matrix ---i.e., in the design---.  But I could not identify
>     > the problem because I was making wrong assumptions when seeing that the
>     > random effects in the large models did not show 0 variance.
>
>
>
>     > Best,
>
>
>     > R.
>
>
>  
>
>     > On Wed, 19-02-2014, at 13:40, rdiaz02 at gmail.com wrote:
>     >> Dear All,
>     >> 
>     >> Summary:
>     >> ========
>     >> 
>     >> I am running some models with glmer that are giving, among others, the
>     >> warning message
>     >> 
>     >> In checkConv(attr(opt, "derivs"), opt$par, checkCtrl = control$checkConv,  :
>     >> Model is nearly unidentifiable: very large eigenvalue
>     >> - Rescale variables?
>     >> 
>     >> 
>     >> but I cannot understand why the model is nearly unidentifable, based both
>     >> on the details of the experimental design, and by comparison with other
>     >> approaches (that run without complains and provide similar results).
>     >> Moreover, I have no idea what I am supposed to rescale.
>     >> 
>     >> 
>     >> 
>     >> 
>     >> Details:
>     >> ========
>     >> 
>     >> The call to glmer
>     >> -----------------
>     >> 
>     >> gt.0 <- glmer(nS ~ Tree + Model + sh + S.Type + S.Time + S.Size + Method +
>     >> (1|dataSetID) + (1|obs), 
>     >> family = poisson,
>     >> data = NP.all,
>     >> control = glmerControl(
>     >> check.conv.singular="warning",
>     >> optCtrl = list(maxfun = 10000))
>     >> )
>     >> 
>     >> The model is for Poisson data, allowing for overdispersion (with the obs
>     >> variable) and with a "dataSetID" random effect. (The glmerControl has been
>     >> added to try to understand what might be happening).
>     >> 
>     >> 
>     >> 
>     >> The design
>     >> ----------
>     >> 
>     >> The data come from a simulation experiment. For each combination of
>     >> 
>     >> Tree * Model * sh * S.Type * S.Time * S.Size there are 20 replicate
>     >> simulations (each identified by a dataSetID), so a full factorial
>     >> design. Each variable is a factor variable (6 levels for Tree, 4 for Model,
>     >> 2 for sh, 3 for S.Type, 2 for S.Time, 3 for S.Size)
>     >> 
>     >> Each replicate simulation, identified by a "dataSetID", is subject to four
>     >> Methods (a factor). That is why dataSetID is a random effect.
>     >> 
>     >> There are no missing values.
>     >> 
>     >> Thus, there are 17280 dataSetID groups (6*4*2*3*2*3*20), each with four
>     >> observations, resulting in a total of 69120 observations. glmer does
>     >> report these values just fine.
>     >> 
>     >> Therefore, based on that design, I think I should be able to fit not just
>     >> the model above, but a model with all possible interactions. In fact, that
>     >> model (a saturated model in the log-linear parlance, IIUC) is what I want
>     >> to start from.
>     >> 
>     >> 
>     >> The warnings
>     >> ------------
>     >> 
>     >> glmer gives two warnings:
>     >> 1: In checkConv(attr(opt, "derivs"), opt$par, checkCtrl = control$checkConv,  :
>     >> Model failed to converge with max|grad| = 392.339 (tol = 0.001)
>     >> 2: In checkConv(attr(opt, "derivs"), opt$par, checkCtrl = control$checkConv,  :
>     >> Model is nearly unidentifiable: very large eigenvalue
>     >> - Rescale variables?
>     >> 
>     >> 
>     >> The failure to converge I don't like, but OK. However, the "nearly
>     >> unidentifiable" I just don't understand. And what variables am I supposed
>     >> to rescale?
>     >> 
>     >> 
>     >> Removing the random effect for obs or for dataSetID does not eliminate the
>     >> warnings (and, in fact, that should not be the problem, since both obs and
>     >> dataSetID get non-zero estimates).
>     >> 
>     >> 
>     >> Other methods
>     >> -------------
>     >> 
>     >> I have fitted the above model with:
>     >> 
>     >> - MCMCglmm (using the priors list(R = list(V = 1, nu = 0.002), G = list(G1
>     >> = list(V = 1, nu = 0.002))))
>     >> 
>     >> 
>     >> - bglmer (with all as per default)
>     >> 
>     >> 
>     >> - glmer2stan (here using a more complex model with a bunch of interactions)
>     >> 
>     >> - glmmadmb (with family "nbinom")
>     >> 
>     >> and even if glmmadmb does complain about lack of convergence, there are no
>     >> further problems.
>     >> 
>     >> Moreover, with MCMCglmm I've fitted the model with interactions. Again, no
>     >> apparent problems (beyond slow mixing in some cases).
>     >> 
>     >> 
>     >> In addition, I have also fitted several other models that include all
>     >> possible interactions, just to make sure again I do not have something
>     >> silly in the model matrix. In some cases, the response is different (as
>     >> when fitting a lm/lmer). Beyond the lack of convergence of lmer fit, no
>     >> other problems.
>     >> 
>     >> 
>     >> ## a GLM with interactions but, of course, without the dataSetID random
>     >> effect.
>     >> 
>     >> glm(nS ~ Tree * Model * sh *
>     >> S.Type * S.Time * S.Size * Method,
>     >> data = NP.all,
>     >> family = poisson
>     >> )
>     >> 
>     >> ## a linear model with interactions 			     
>     >> 
>     >> lm(Dissim ~ Tree * Model * sh *
>     >> S.Type * S.Time * S.Size * Method,
>     >> data = NP.all
>     >> )
>     >> 
>     >> ## a lmers
>     >> 
>     >> lmer(Dissim ~ Tree * Model * sh *
>     >> S.Type * S.Time * S.Size * Method +
>     >> (1|dataSetID),
>     >> data = NP.all,
>     >> control = lmerControl(
>     >> check.conv.singular="warning",
>     >> optCtrl = list(maxfun = 10000))
>     >> )
>     >> ## This complaints of lack of convergence, but no identifiability
>     >> problems. 
>     >> 
>     >> 
>     >> 
>     >> (Lack of) Differences between fits
>     >> -----------------------------------
>     >> 
>     >> If anyone wants I can provide, of course, the output from the
>     >> fits. Anyway, the qualitative summary is that the the estimates for the
>     >> fixed are very similar between glmer, admb, bglmer, and MCMCglmm and
>     >> the estimates for the random effects are very similar for MCMCglmm,
>     >> bglmer, and glmer.
>     >> 
>     >> 
>     >> 
>     >> 
>     >> 
>     >> So, what am I missing?
>     >> 
>     >> 
>     >> Best,
>     >> 
>     >> 
>     >> 
>     >> R.
>
>     > -- 
>     > Ramon Diaz-Uriarte
>     > Department of Biochemistry, Lab B-25
>     > Facultad de Medicina 
>     > Universidad Aut?noma de Madrid 
>     > Arzobispo Morcillo, 4
>     > 28029 Madrid
>     > Spain
>
>     > Phone: +34-91-497-2412
>
>     > Email: rdiaz02 at gmail.com
>     > ramon.diaz at iib.uam.es
>
>     > http://ligarto.org/rdiaz
>
>     > _______________________________________________
>     > R-sig-mixed-models at r-project.org mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Ramon Diaz-Uriarte
Department of Biochemistry, Lab B-25
Facultad de Medicina 
Universidad Aut?noma de Madrid 
Arzobispo Morcillo, 4
28029 Madrid
Spain

Phone: +34-91-497-2412

Email: rdiaz02 at gmail.com
       ramon.diaz at iib.uam.es

http://ligarto.org/rdiaz


From mtoncic at ffri.hr  Tue Feb 25 13:13:41 2014
From: mtoncic at ffri.hr (marKo)
Date: Tue, 25 Feb 2014 13:13:41 +0100
Subject: [R-sig-ME] Help with analyzing multivariate outcomes
In-Reply-To: <55488E7A-FAA5-40B8-9345-2421FC5F75AB@me.com>
References: <53072E60.9080808@ffri.hr>
	<DUB126-W618EC7BDCF55B4D11D15D8D6850@phx.gbl>
	<53073825.6040809@ffri.hr>
	<55488E7A-FAA5-40B8-9345-2421FC5F75AB@me.com>
Message-ID: <530C88F5.9020901@ffri.hr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140225/544a5266/attachment.pl>

From mike.lwrnc at gmail.com  Tue Feb 25 14:38:22 2014
From: mike.lwrnc at gmail.com (Mike Lawrence)
Date: Tue, 25 Feb 2014 09:38:22 -0400
Subject: [R-sig-ME] Characterizing correlation between binomial intercepts
Message-ID: <CAB+QPJBdbRiH28N95M68HXLDzFK3pKFMMCNiUUAOsbREnJgtRw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140225/29550177/attachment.pl>

From peter.austin at ices.on.ca  Mon Feb 24 22:58:08 2014
From: peter.austin at ices.on.ca (Austin, Peter)
Date: Mon, 24 Feb 2014 21:58:08 +0000
Subject: [R-sig-ME] Problems installing lme4 1.0-6 (won't compile due to
 Rcpp not compiling)
Message-ID: <9ED811556BF51A4C8E7FD77C5133339B45486993@ICESMBX2.ices.on.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140224/6cde5368/attachment.pl>

From jake987722 at hotmail.com  Tue Feb 25 19:11:04 2014
From: jake987722 at hotmail.com (Jake Westfall)
Date: Tue, 25 Feb 2014 11:11:04 -0700
Subject: [R-sig-ME] Characterizing correlation between binomial
 intercepts
In-Reply-To: <CAB+QPJBdbRiH28N95M68HXLDzFK3pKFMMCNiUUAOsbREnJgtRw@mail.gmail.com>
References: <CAB+QPJBdbRiH28N95M68HXLDzFK3pKFMMCNiUUAOsbREnJgtRw@mail.gmail.com>
Message-ID: <BAY172-W5F971FDB65D0D5BF202A9CB810@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140225/4da6e6ff/attachment.pl>

From mike.lwrnc at gmail.com  Tue Feb 25 20:15:41 2014
From: mike.lwrnc at gmail.com (Mike Lawrence)
Date: Tue, 25 Feb 2014 15:15:41 -0400
Subject: [R-sig-ME] Characterizing correlation between binomial
	intercepts
In-Reply-To: <BAY172-W5F971FDB65D0D5BF202A9CB810@phx.gbl>
References: <CAB+QPJBdbRiH28N95M68HXLDzFK3pKFMMCNiUUAOsbREnJgtRw@mail.gmail.com>
	<BAY172-W5F971FDB65D0D5BF202A9CB810@phx.gbl>
Message-ID: <CAB+QPJBTBj6jZ9Ep5=PWfX=Pc2XWWX6KqD-nqYN6Ag_y7hPv-g@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140225/03237932/attachment.pl>

From jake987722 at hotmail.com  Tue Feb 25 22:33:47 2014
From: jake987722 at hotmail.com (Jake Westfall)
Date: Tue, 25 Feb 2014 14:33:47 -0700
Subject: [R-sig-ME] Characterizing correlation between binomial
 intercepts
In-Reply-To: <CAB+QPJBTBj6jZ9Ep5=PWfX=Pc2XWWX6KqD-nqYN6Ag_y7hPv-g@mail.gmail.com>
References: <CAB+QPJBdbRiH28N95M68HXLDzFK3pKFMMCNiUUAOsbREnJgtRw@mail.gmail.com>,
	<BAY172-W5F971FDB65D0D5BF202A9CB810@phx.gbl>,
	<CAB+QPJBTBj6jZ9Ep5=PWfX=Pc2XWWX6KqD-nqYN6Ag_y7hPv-g@mail.gmail.com>
Message-ID: <BAY172-W2428C9FA0112B46B90E66CCB810@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140225/2d8c8ed8/attachment.pl>

From wolfgang.viechtbauer at maastrichtuniversity.nl  Tue Feb 25 22:42:18 2014
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (STAT))
Date: Tue, 25 Feb 2014 22:42:18 +0100
Subject: [R-sig-ME] Specifying fixed residual variances for multilevel
 models
In-Reply-To: <A1E2A16402841149A85612B5EC3D6D76028544865F0C@ITCCRMAIL01.MED.HARVARD.EDU>
References: <A1E2A16402841149A85612B5EC3D6D76028544865F0C@ITCCRMAIL01.MED.HARVARD.EDU>
Message-ID: <077E31A57DA26E46AB0D493C9966AC730DA0009B9A@UM-MAIL4112.unimaas.nl>

In meta-analytic models, we commonly fix the residual variances to (at least approximately) known values. Following your example, you could fit the desired models with the 'metafor' package with:

library(metafor)

rma.mv(y, V, random = ~ 1 | group)
rma.mv(y, V2, random = ~ 1 | group)

This will really fix those residual variances to V and V2 (not just up to that proportionality constant).

At the moment, rma.mv() isn't optimized for speed, so it takes a bit to get the model to converge. The next version of metafor (to be released in the near future) will be faster.

Best,
Wolfgang

--   
Wolfgang Viechtbauer, Ph.D., Statistician   
Department of Psychiatry and Psychology   
School for Mental Health and Neuroscience   
Faculty of Health, Medicine, and Life Sciences   
Maastricht University, P.O. Box 616 (VIJV1)   
6200 MD Maastricht, The Netherlands   
+31 (43) 388-4170 | http://www.wvbauer.com   


> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-
> models-bounces at r-project.org] On Behalf Of Zaslavsky, Alan M.
> Sent: Sunday, February 23, 2014 00:02
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] Specifying fixed residual variances for multilevel
> models
> 
> We have a bunch of analyses in which we would like to specify residual
> variances that are calculated outside the multilevel modeling function.
> We were proceeding on the assumption that using varFixed() for the
> weights argument to lme() would do this, but after doing a few
> simulations it became evident that this was only fixing the residual
> variances up to a proportionality constant and in fact rescaling the
> weights had no effect on estimates of random effects variances.
> 
> I've spent a few hours scanning the documentation, list comments and FAQs
> without tracking down a solution.  This seems to be more a feature than a
> bug, although comments I see on this list and Github, among other places
> suggest that the handling of residual variances or weights is the subject
> of current development efforts (lme4a?).  Not surprising --- in fact the
> terms 'weights' has at least as many meanings as 'fixed effects', with
> potentially different analytic implications (especially for mixed
> models).  I would urge talking explicitly about residual variance models,
> which have a clear interpretation.
> 
> Any thoughts?
> 
>                 Alan Zaslavsky
>                 zaslavsk at hcp.med.harvard.edu
> 
> NLME EXAMPLE
> > group=rep(1:40,10)                        ## 40 groups of 10
> observations
> > V=401:800/400                              ## residual variances
> > y=rnorm(40)[group]+rnorm(400)*sqrt(V)
> > lme(fixed=y~1,random=~1|group)   ## unweighted
> Linear mixed-effects model fit by REML
>   Data: NULL
>   Log-restricted-likelihood: -696.8343
>   Fixed: y ~ 1
> (Intercept)
>   0.3656883
> Random effects:
>  Formula: ~1 | group
>         (Intercept) Residual
> StdDev:   0.9231727 1.257827
> Number of Observations: 400
> Number of Groups: 40
> > lme(fixed=y~1,random=~1|group,weight=varFixed(~V))
> Linear mixed-effects model fit by REML
>   Data: NULL
>   Log-restricted-likelihood: -692.0942
>   Fixed: y ~ 1
> (Intercept)
>   0.3513349
> Random effects:
>  Formula: ~1 | group
>         (Intercept) Residual
> StdDev:   0.9261319 1.021891
> Variance function:
>  Structure: fixed weights
>  Formula: ~V
> Number of Observations: 400
> Number of Groups: 40
> ##  the one above specified the residual variances that were used in
> generating the
> ##  the data and there estimated scale factor close to 1, as it should
> > V2=3*V
> > lme(fixed=y~1,random=~1|group,weight=varFixed(~V2))
> Linear mixed-effects model fit by REML
>   Data: NULL
>   Log-restricted-likelihood: -692.0942
>   Fixed: y ~ 1
> (Intercept)
>   0.3513349
> Random effects:
>  Formula: ~1 | group
>         (Intercept)  Residual
> StdDev:   0.9261319 0.5899889
> Variance function:
>  Structure: fixed weights
>  Formula: ~V2
> Number of Observations: 400
> Number of Groups: 40
> ###  here we increased the specified residual variance by a factor of 3,
> but it was ignored except that the residual scale factor went down by a
> factor of sqrt(3) as shown below.  All very good if you only know
> *relative* residual variances but not helpful if you know them
> *absolutely*
> > (1.021891/.5899889)^2
> [1] 3.000001
> 
> LMER (same data)
> > lmer(formula=y~1+(1|group),weights=V)
> Linear mixed model fit by REML ['lmerMod']
> Formula: y ~ 1 + (1 | group)
> REML criterion at convergence: 1570.624
> Random effects:
>  Groups   Name        Std.Dev.
>  group    (Intercept) 0.7569
>  Residual             1.2824
> Number of obs: 400, groups: group, 40
> Fixed Effects:
> (Intercept)
>      0.3788
> > lmer(formula=y~1+(1|group),weights=V2)
> Linear mixed model fit by REML ['lmerMod']
> Formula: y ~ 1 + (1 | group)
> REML criterion at convergence: 2010.069
> Random effects:
>  Groups   Name        Std.Dev.
>  group    (Intercept) 0.437
>  Residual             1.282
> Number of obs: 400, groups: group, 40
> Fixed Effects:
> (Intercept)
>      0.3788
> ############  even stranger -- rescaling residual variance leaves
> residual var estimate alone, but changes level-2 variance by factor of 3
> > (.7569/.437)^2
> [1] 2.999951
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From jwiley.psych at gmail.com  Tue Feb 25 23:04:30 2014
From: jwiley.psych at gmail.com (Joshua Wiley)
Date: Tue, 25 Feb 2014 14:04:30 -0800
Subject: [R-sig-ME] Characterizing correlation between binomial
	intercepts
In-Reply-To: <BAY172-W2428C9FA0112B46B90E66CCB810@phx.gbl>
References: <CAB+QPJBdbRiH28N95M68HXLDzFK3pKFMMCNiUUAOsbREnJgtRw@mail.gmail.com>
	<BAY172-W5F971FDB65D0D5BF202A9CB810@phx.gbl>
	<CAB+QPJBTBj6jZ9Ep5=PWfX=Pc2XWWX6KqD-nqYN6Ag_y7hPv-g@mail.gmail.com>
	<BAY172-W2428C9FA0112B46B90E66CCB810@phx.gbl>
Message-ID: <CANz9Z_+YxRYTxj8hwijTzVpccWNxkqN0UoP7pHD0nU0OOEZgHQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140225/b365be9d/attachment.pl>

From David.Duffy at qimr.edu.au  Wed Feb 26 01:26:23 2014
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Wed, 26 Feb 2014 10:26:23 +1000
Subject: [R-sig-ME] Model is nearly unidentifiable with glmer
In-Reply-To: <87vbw3xyc4.fsf@gmail.com>
References: <87d2ijp96n.fsf@gmail.com>
	<87a9dgzf7l.fsf@gmail.com><21259.35646.496086.844973@stat.math.ethz.ch>
	<87vbw3xyc4.fsf@gmail.com>
Message-ID: <alpine.LMD.2.00.1402260956350.10131@orpheus.qimr.edu.au>

On Tue, 25 Feb 2014, Ramon Diaz-Uriarte wrote:

> http://ligarto.org/rdiaz/nearly-unidentifiable.tar.gz
>
> a small (230 KB) file with:
>
> - data
> - R code to show the problem
> - two runs of that code on different machines

I don't seem to get those warnings when I run your "f02" ("very large 
eigenvalue") problem here - get the same parameter estimates (x86_64, 
lme4_1.0-6, Matrix_1.1-0).  The estimated RE variance is very small.

Your cut-down "singularity" example gives a variance for dataSetID with a 
starting value for theta, eg glmer(nS ~ Method + (1|dataSetID), 
family=poisson, data = d2, start=0.01) - again v. small.


| David Duffy (MBBS PhD)
| email: David.Duffy at qimrberghofer.edu.au  ph: INT+61+7+3362-0217 fax: -0101
| Genetic Epidemiology, QIMR Berghofer Institute of Medical Research
| 300 Herston Rd, Brisbane, Queensland 4006, Australia  GPG 4D0B994A


From mike.lwrnc at gmail.com  Wed Feb 26 01:42:48 2014
From: mike.lwrnc at gmail.com (Mike Lawrence)
Date: Tue, 25 Feb 2014 20:42:48 -0400
Subject: [R-sig-ME] Characterizing correlation between binomial
	intercepts
In-Reply-To: <CANz9Z_+YxRYTxj8hwijTzVpccWNxkqN0UoP7pHD0nU0OOEZgHQ@mail.gmail.com>
References: <CAB+QPJBdbRiH28N95M68HXLDzFK3pKFMMCNiUUAOsbREnJgtRw@mail.gmail.com>
	<BAY172-W5F971FDB65D0D5BF202A9CB810@phx.gbl>
	<CAB+QPJBTBj6jZ9Ep5=PWfX=Pc2XWWX6KqD-nqYN6Ag_y7hPv-g@mail.gmail.com>
	<BAY172-W2428C9FA0112B46B90E66CCB810@phx.gbl>
	<CANz9Z_+YxRYTxj8hwijTzVpccWNxkqN0UoP7pHD0nU0OOEZgHQ@mail.gmail.com>
Message-ID: <CAB+QPJAEdN+C=7A8BVxn=cCexqzFJgJjr+rmxyU1R2pjHKDphw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140225/9cdd0154/attachment.pl>

From jake987722 at hotmail.com  Wed Feb 26 02:11:14 2014
From: jake987722 at hotmail.com (Jake Westfall)
Date: Tue, 25 Feb 2014 18:11:14 -0700
Subject: [R-sig-ME] Characterizing correlation between binomial
 intercepts
In-Reply-To: <CAB+QPJAEdN+C=7A8BVxn=cCexqzFJgJjr+rmxyU1R2pjHKDphw@mail.gmail.com>
References: <CAB+QPJBdbRiH28N95M68HXLDzFK3pKFMMCNiUUAOsbREnJgtRw@mail.gmail.com>,
	<BAY172-W5F971FDB65D0D5BF202A9CB810@phx.gbl>
	<CAB+QPJBTBj6jZ9Ep5=PWfX=Pc2XWWX6KqD-nqYN6Ag_y7hPv-g@mail.gmail.com>,
	<BAY172-W2428C9FA0112B46B90E66CCB810@phx.gbl>
	<CANz9Z_+YxRYTxj8hwijTzVpccWNxkqN0UoP7pHD0nU0OOEZgHQ@mail.gmail.com>,
	<CAB+QPJAEdN+C=7A8BVxn=cCexqzFJgJjr+rmxyU1R2pjHKDphw@mail.gmail.com>
Message-ID: <BAY172-W232CEF58934D409CB43AB1CB800@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140225/b386f363/attachment.pl>

From bbolker at gmail.com  Wed Feb 26 03:19:54 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 25 Feb 2014 21:19:54 -0500
Subject: [R-sig-ME] error using predict() with lmer
In-Reply-To: <3C1397953420394CA1E185D115FC2E290D5073@uqexmdb6.soe.uq.edu.au>
References: <3C1397953420394CA1E185D115FC2E290D5073@uqexmdb6.soe.uq.edu.au>
Message-ID: <530D4F4A.3010102@gmail.com>

On 14-02-25 08:46 PM, Michael Williamson wrote:
> Good Morning,
> 
>  
> 
> I?ve been trying to get some predictive values put f my model using a
> code my colleague gave me. We have similar sets of data but ti doesn?t
> seem to be working for me and I?m getting the same error.

  I'm taking the liberty of sending this to the r-sig-mixed-models list,
which is probably a better forum.
> 
> My model codes is quite simple and is LongDiveTrModAtt <-
> lmer(Log10LagTime ~ Depth+AttBoatPhs+(1|FocalID),
> data=LongDiveTr,na.action=na.omit).
> 
> I then run the following code using a dataset I?ve already created
> (Graph_LDAtt) which has a column for AttBoatPhs (with 5 types of
> boatphase)  and Depth (with an average depth for each boat phase).
> 
>  
> 
> PredictLogDive <- predict(LongDiveTrModAtt, Graph_LDAtt, level=0) 
> 
> 
> But I get this error
> 
>  
> 
> Error in factor(FocalID) : object 'FocalID' not found
> 
> In addition: Warning message:
> 
> In predict.merMod(LongDiveTrModAtt, Graph_LDAtt, level = 0) :
> 
>   unused arguments ignored
> 
>  
> 
> I thought by putting level=0 that should take account of the random
> effect of the FocalID. It?s working for my colleague and neither of su
> are sure why it is not working here.

  You seem to be mixing the syntax for predict.lme, from the nlme
package, with the syntax for predict.merMod, from the lme4 package.  lme
uses level=[integer], because lme is largely restricted to fitting
models where there are discretely definable levels (i.e., nested random
effects).  predict.merMod instead uses a REform argument (switched to
re.form in the development version) which can be specified as NA or ~0
to predict at the population level.  (This is also why you get a warning
about unused arguments being ignored ...)

  It's hard for me to see how this could be working correctly for your
colleague ... unless they're using lme instead of lmer, in which case
their model would have to be quite different: lme(Log10LagTime ~
Depth+AttBoatPhs, random= ~1|FocalID, data=LongDiveTr,na.action=na.omit)


  Ben Bolker


From alexejpksiren at gmail.com  Wed Feb 26 04:33:15 2014
From: alexejpksiren at gmail.com (Alexej Siren)
Date: Tue, 25 Feb 2014 22:33:15 -0500
Subject: [R-sig-ME] Error in sprintf
Message-ID: <002801cf32a3$7c1fa360$745eea20$@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140225/c15800e2/attachment.pl>

From rdiaz02 at gmail.com  Wed Feb 26 09:58:02 2014
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Wed, 26 Feb 2014 09:58:02 +0100
Subject: [R-sig-ME] Model is nearly unidentifiable with glmer
In-Reply-To: <alpine.LMD.2.00.1402260956350.10131@orpheus.qimr.edu.au>
References: <87d2ijp96n.fsf@gmail.com> <87a9dgzf7l.fsf@gmail.com>
	<21259.35646.496086.844973@stat.math.ethz.ch>
	<87vbw3xyc4.fsf@gmail.com>
	<alpine.LMD.2.00.1402260956350.10131@orpheus.qimr.edu.au>
Message-ID: <87a9degsit.fsf@gmail.com>

Dear David,

On Wed, 26-02-2014, at 01:26, David.Duffy at qimr.edu.au wrote:
> On Tue, 25 Feb 2014, Ramon Diaz-Uriarte wrote:
>
>> http://ligarto.org/rdiaz/nearly-unidentifiable.tar.gz
>>
>> a small (230 KB) file with:
>>
>> - data
>> - R code to show the problem
>> - two runs of that code on different machines
>
> I don't seem to get those warnings when I run your "f02" ("very large 
> eigenvalue") problem here - get the same parameter estimates (x86_64, 
> lme4_1.0-6, Matrix_1.1-0).  The estimated RE variance is very small.
>

I think the main difference can be the version of lme4: I am using 1.1-4
(and Matrix v. 1.1-2) and I think between v.1.0 and 1.1 of lme4 there is an
increase in the reporting of warnings (I think I've got this impression
from messages in the list and from the comments at the github repository
---e.g., https://github.com/lme4/lme4/issues/120).



> Your cut-down "singularity" example gives a variance for dataSetID with a 
> starting value for theta, eg glmer(nS ~ Method + (1|dataSetID), 
> family=poisson, data = d2, start=0.01) - again v. small.

If I do that (again, using lme4 v.1.1-4 and of course with the
check.conv.singular set to "warning") I still get the "singular fit"
warning (though not if I use bglmer, from package blme).


Best,


R.

>
>
> | David Duffy (MBBS PhD)
> | email: David.Duffy at qimrberghofer.edu.au  ph: INT+61+7+3362-0217 fax: -0101
> | Genetic Epidemiology, QIMR Berghofer Institute of Medical Research
> | 300 Herston Rd, Brisbane, Queensland 4006, Australia  GPG 4D0B994A

-- 
Ramon Diaz-Uriarte
Department of Biochemistry, Lab B-25
Facultad de Medicina 
Universidad Aut?noma de Madrid 
Arzobispo Morcillo, 4
28029 Madrid
Spain

Phone: +34-91-497-2412

Email: rdiaz02 at gmail.com
       ramon.diaz at iib.uam.es

http://ligarto.org/rdiaz


From bbolker at gmail.com  Wed Feb 26 21:33:18 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 26 Feb 2014 15:33:18 -0500
Subject: [R-sig-ME] Error in sprintf
In-Reply-To: <002801cf32a3$7c1fa360$745eea20$@gmail.com>
References: <002801cf32a3$7c1fa360$745eea20$@gmail.com>
Message-ID: <530E4F8E.3050105@gmail.com>

On 14-02-25 10:33 PM, Alexej Siren wrote:
> Hello,
> 
>  
> 
> I'm reanalyzing binomial GLMM's using the same script and data from last
> year.  I'm using the lme4 and AICcmodavg packages and able to produce an AIC
> table but not able to get a summary of the top model - or any model.
> Instead, I get the error listed below.  
> 
>  
> 
> Error in sprintf(gettext(fmt, domain = domain), ...) : 
> 
>   invalid type of argument[1]: 'symbol'
> 
>  
> 
> However, if I use the MuMIn package I can get a table and a summary of the
> model-averaged coefficients.
> 



> 
> Since I analyzed this data I've installed the newest version of R and have
> updated all packages.  I've checked the data and script which have not been
> modified since last summer when it worked.  Further, I have searched through
> various forums to learn more about this error but I've found scant
> information that relates to my issue.  Are there changes in the newer
> version of R or in a package that would elicit this error?  Perhaps the
> structure of my data needs to be changed?  Any suggestions would be
> welcomed.  
> 
>  
> 
  There have been a lot of changes to R, but with this amount of
information it's almost impossible to say anything -- is there any
chance you could provide (1) a reproducible example (see
http://tinyurl.com/reproducible-000 for hints) (2) the results of
sessionInfo() (to tell us what versions of packages you're using) (3)
the results of "traceback()" after you get the error?  I can't find any
instance of "sprintf(gettext" in the lme4 package code, but that only
suggests (not guarantees) that the proximal problem is in one of the
other packages you're using.

  Ben Bolker


From b.meyer at psychologie.uzh.ch  Wed Feb 26 18:24:39 2014
From: b.meyer at psychologie.uzh.ch (Bertolt Meyer)
Date: Wed, 26 Feb 2014 18:24:39 +0100
Subject: [R-sig-ME] confint.merMod(): method = "boot" throws error
Message-ID: <9AA0C722-BEF4-40AB-AA14-691186411B26@psychologie.uzh.ch>

Dear all,

I am trying to obtain confidence intervals on the parameters of a merMod with bootstrapping. However, I always get the following error:

> confint.merMod(mmodel.7, method = "boot", nsim = 100)

Computing bootstrap confidence intervals ...
Error in if (const(t, min(1e-08, mean(t, na.rm = TRUE)/1e+06))) { : 
 Missing value where TRUE/FALSE is required

However, using confint.merMod() with method = "Wald" on the same model works just fine:

> confint.merMod(mmodel.7, method = "Wald")
                      2.5 %       97.5 %
(Intercept)     4.1936561326  4.768229069
sex2            0.0569452437  0.393527752
age            -0.0122385301  0.004655781
tenure          0.0238700983  0.048076967
network_size   -0.0022468908  0.008291748
dage           -0.0085075176  0.006415557
dtenure         0.0005230516  0.019461232
dsex1          -0.3336595531 -0.022736774
time_l         -8.2010266502 -6.205144078
time_q          0.9684946890  2.249428767
time_c         -2.7975435909 -1.524616092
dtenure:time_l -0.4159371669 -0.109693667
dtenure:time_q  0.0291175752  0.249497811
dtenure:time_c -0.1558022397  0.055678434

I tried this with different models, it's always the same. Does anybody have an idea why it fails with method = "boot"?

Thank you and best greetings,
Bertolt

--
Dr. Bertolt Meyer, Dipl.-Psych.
Senior Research and Teaching Associate
Social and Business Psychology
Institute of Psychology, University of Zurich 
Binzmuehlestr. 14/13
CH-8050 Zurich
Switzerland

b.meyer at psychologie.uzh.ch


From bbolker at gmail.com  Thu Feb 27 02:06:55 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 27 Feb 2014 01:06:55 +0000 (UTC)
Subject: [R-sig-ME] confint.merMod(): method = "boot" throws error
References: <9AA0C722-BEF4-40AB-AA14-691186411B26@psychologie.uzh.ch>
Message-ID: <loom.20140227T020045-422@post.gmane.org>

Bertolt Meyer <b.meyer at ...> writes:

> 
> Dear all,
> 
> I am trying to obtain confidence intervals on the parameters 
> of a merMod with bootstrapping. However, I
> always get the following error:
> 
> > confint.merMod(mmodel.7, method = "boot", nsim = 100)
> 
> Computing bootstrap confidence intervals ...
> Error in if (const(t, min(1e-08, mean(t, na.rm = TRUE)/1e+06))) { : 
>  Missing value where TRUE/FALSE is required
> 
> However, using confint.merMod() with method = "Wald" 
> on the same model works just fine:
> 

  [snip]
 
> I tried this with different models, it's always the same. Does anybody have
> an idea why it fails with method = "boot"?
> 

  This sounds like https://github.com/lme4/lme4/issues/158 
Fixed a couple of months ago, but not yet in a released version.
If this is actually the case, you can work around it by removing
NA values from your data set ahead of time.

  Ben Bolker


From bbolker at gmail.com  Thu Feb 27 04:24:19 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 27 Feb 2014 03:24:19 +0000 (UTC)
Subject: [R-sig-ME] Error in sprintf
References: <002801cf32a3$7c1fa360$745eea20$@gmail.com>
	<530E4F8E.3050105@gmail.com>
Message-ID: <loom.20140227T042112-25@post.gmane.org>

Ben Bolker <bbolker at ...> writes:

> 
> On 14-02-25 10:33 PM, Alexej Siren wrote:
> > Hello,
> > 
> >  
> > 
> > I'm reanalyzing binomial GLMM's using the same script and data from last
> > year.  I'm using the lme4 and AICcmodavg packages 
> and able to produce an AIC
> > table but not able to get a summary of the top model - or any model.
> > Instead, I get the error listed below.  
> > 
> >  
> > 
> > Error in sprintf(gettext(fmt, domain = domain), ...) : 
> > 
> >   invalid type of argument[1]: 'symbol'
> > 
> > 

PS this may have been answered earlier on this list --

http://r.789695.n4.nabble.com/Error-running-MuMIn-dredge-function-
using-glmer-models-td4679991.html
[WARNING, broken URL to make gmane happy]

(I found this by googling "error in sprintf gettext symbol" -- this
message is hits #2 and #3)

The link suggests you can work around the problem by (1) installing
MuMIn version 1.9.14 from R-forge (2) omitting na.action=na.omit.

Your problem *might* be something different, but this looks like a
good place to start.

  Ben Bolker


From bbolker at gmail.com  Thu Feb 27 04:25:31 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 27 Feb 2014 03:25:31 +0000 (UTC)
Subject: [R-sig-ME] Problems installing lme4 1.0-6 (won't compile due to
	Rcpp not compiling)
References: <9ED811556BF51A4C8E7FD77C5133339B45486993@ICESMBX2.ices.on.ca>
Message-ID: <loom.20140227T042452-505@post.gmane.org>

Austin, Peter <peter.austin at ...> writes:

> 
> I am having trouble installing the most recent version 
> of lme4 (version 1.0-6) on a Unix system. The
> problems appears to be that the package Rcpp won't compile.
>  Our Unix administrator has tried this
> different versions of R and different versions of Rcpp.
> 
> Do you have any suggestions as to how to proceed?
> 
> Thanks,
> 
> Peter
> 
> 

  (1) We would need more information than this to help out (what
Unix system, what compiler, detailed error messages).  (2) There is
an Rcpp mailing list, probably best to ask there ...

  Ben Bolker


From anderstl at gmail.com  Thu Feb 27 18:12:07 2014
From: anderstl at gmail.com (Thomas Lee Anderson)
Date: Thu, 27 Feb 2014 11:12:07 -0600
Subject: [R-sig-ME] prediction from glmer.nb
Message-ID: <CALFR8h9BjVN0O4M7BvxHmjJTZ9DBPg_FWwTh0-PFBaX3Zwdzow@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140227/bf0561f1/attachment.pl>

From asafw.at.wharton at gmail.com  Wed Feb 26 20:26:17 2014
From: asafw.at.wharton at gmail.com (Asaf Weinstein)
Date: Wed, 26 Feb 2014 14:26:17 -0500
Subject: [R-sig-ME] Order in which variance components are reported
In-Reply-To: <CAO7JsnQ_V3yDtSXadtugF9TQzaqDSatSrfORVr3oiExAtHk4zg@mail.gmail.com>
References: <CAGG0PdBLLtxnPCSYH9c_cU0WnacdmLJ=Gh5B8RHLY7R6C_Jf3Q@mail.gmail.com>
	<CABghstQ7R01dagqpZo-Y4ahbtg0g-v3iqT93AnE1mJ87R-msoA@mail.gmail.com>
	<CAO7JsnQ_V3yDtSXadtugF9TQzaqDSatSrfORVr3oiExAtHk4zg@mail.gmail.com>
Message-ID: <CAGG0PdAyOKaCUOiJGq4CJVBzazpbi5fYPgj8k=i0ugN1EK+X6g@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140226/9717870e/attachment.pl>

From asiren at wildcats.unh.edu  Wed Feb 26 04:30:05 2014
From: asiren at wildcats.unh.edu (Alexej Siren)
Date: Wed, 26 Feb 2014 03:30:05 +0000
Subject: [R-sig-ME] Error in sprintf
Message-ID: <4425451e20664e3cbc99da4552b41244@CO1PR04MB379.namprd04.prod.outlook.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140226/114817fc/attachment.pl>

From dhocking at umass.edu  Tue Feb 25 23:31:34 2014
From: dhocking at umass.edu (Daniel Hocking)
Date: Tue, 25 Feb 2014 17:31:34 -0500
Subject: [R-sig-ME] nlmer self starting function
Message-ID: <BC9774DE-A084-4897-A7A9-742420F0DC2C@umass.edu>

I am trying to use the following nlmer function to estimate random break points in a polynomial smooth break point regression (sense Hout et al. 2011: http://onlinelibrary.wiley.com.silk.library.umass.edu/doi/10.1002/sim.4127/abstract)

nl1 <- nlmer(Reaction ~ SSPoly(Days,n0,n1,n2,n3) ~ (n0|Subject)+(n1+n2|Subject)+(n3|Subject), data=sleepstudy, start = c(n0=22, n1=0, n2=-1, n3=-4))

I get the error: Error in eval(expr, envir, enclos) : could not find function ?SSPoly"

I assume this is due to changes in lme4 versions. I am not experienced with the nlmer function and writing custom functions and start functions. Does anyone have recommendations on the easiest way to implement this model (preferable without going to WinBUGS/JAGS)?

Thanks,
Dan
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Daniel Hocking
Department of Environmental Conservation
Northeast Climate Science Center
University of Massachusetts

http://www.danieljhocking.wordpress.com
dhocking at umass.edu
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


From alexejpksiren at gmail.com  Fri Feb 28 03:41:26 2014
From: alexejpksiren at gmail.com (Alexej Siren)
Date: Thu, 27 Feb 2014 21:41:26 -0500
Subject: [R-sig-ME] Error in sprintf
In-Reply-To: <loom.20140227T042112-25@post.gmane.org>
References: <002801cf32a3$7c1fa360$745eea20$@gmail.com>	<530E4F8E.3050105@gmail.com>
	<loom.20140227T042112-25@post.gmane.org>
Message-ID: <000e01cf342e$948d1e50$bda75af0$@gmail.com>

Hello Ben,

Thanks for the reference.  I checked that thread and others before I sent
the message to the list.  However, MuMIn works fine; I was not able to get a
summary of parameter estimates when not model averaging.

I found a workaround to the problem by removing the subset command and
creating separate csv files.  I haven't taken the time to investigate why
the subset syntax isn't working but I will do that soon as I often subset
data.  I've included the formula below in case you're interested.   

fm1 <- glmer(ua ~ X.regen*X.hardwood + (1|pinky), data_std,
family=binomial(logit), subset=(season=="lof"))

Thanks for your response and I'll post the solution when I find the problem.

Alexej


-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org
[mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Ben Bolker
Sent: Wednesday, February 26, 2014 10:24 PM
To: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Error in sprintf

Ben Bolker <bbolker at ...> writes:

> 
> On 14-02-25 10:33 PM, Alexej Siren wrote:
> > Hello,
> > 
> >  
> > 
> > I'm reanalyzing binomial GLMM's using the same script and data from 
> > last year.  I'm using the lme4 and AICcmodavg packages
> and able to produce an AIC
> > table but not able to get a summary of the top model - or any model.
> > Instead, I get the error listed below.  
> > 
> >  
> > 
> > Error in sprintf(gettext(fmt, domain = domain), ...) : 
> > 
> >   invalid type of argument[1]: 'symbol'
> > 
> > 

PS this may have been answered earlier on this list --

http://r.789695.n4.nabble.com/Error-running-MuMIn-dredge-function-
using-glmer-models-td4679991.html
[WARNING, broken URL to make gmane happy]

(I found this by googling "error in sprintf gettext symbol" -- this message
is hits #2 and #3)

The link suggests you can work around the problem by (1) installing MuMIn
version 1.9.14 from R-forge (2) omitting na.action=na.omit.

Your problem *might* be something different, but this looks like a good
place to start.

  Ben Bolker

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From emilia.ellsiepen at gmail.com  Fri Feb 28 17:04:21 2014
From: emilia.ellsiepen at gmail.com (Emilia Ellsiepen)
Date: Fri, 28 Feb 2014 17:04:21 +0100
Subject: [R-sig-ME] likelihood-ratio tests in conflict with coefficiants in
 maximal random effect model
Message-ID: <CAOU=wbkzKV2e2+k2q3B6rx+ANFzj7MDKtG4xnQ1vmUAfKF17uA@mail.gmail.com>

Dear list members,

in analyzing a data set using lmers with maximal random effect
structure and subsequent likelihood-ratio tests (LRTs) following Barr
et al. 2013, I ran into the following problem: In some of the LRTs, it
turned out that the simpler model (only main effects) has a higher
likelihood than the more complicated model (including interaction),
resulting in Chi=0. If I simplify the models by taking out the
interactions in the two random effect terms, the LRT for the
interaction has a highly significant result.

Now, I figured this might either be a strong case for Barr et al., or
something is going wrong, as the data do suggest an interaction, and
the t-value for the interaction coefficiant in the model summary of
the maximal random effect model is 1.69 - which I would have expected
to be a (close to) marginal effect. Also, a lot of researchers who do
use maximal random effect models rather interpret the t-values
directly instead of running LRTs, which then might lead to different
interpretations of the same data.

My questions are:
1) Is the LRT reported below reliable, indicating there is no
interaction, or does the non-expected difference in logLik suggest
that something went wrong?
2) Is it possible that the control parameter that increases the number
of evaluations has something to do with it?


Here are some details:
The design is 2x2 within subjects and items, the factors are sum-coded.
I used REML=F to straightforwardly perform the LRTs afterwards and
increased the number of evaluations for the model to converge

m1 = lmer(score ~ Order*Voice +
(Order*Voice|subject)+(Order*Voice|sentence) ,REML=F,
control=lmerControl(optCtrl=list(maxfun=50000)),rawdata)
mi = lmer(score ~ Order+Voice +
(Order*Voice|subject)+(Order*Voice|sentence) ,REML=F,
control=lmerControl(optCtrl=list(maxfun=50000)),rawdata)

anova(m1,mi)

Models:
mi: score ~ Order + Voice + (Order * Voice | subject) + (Order *
mi:     Voice | sentence)
m1: score ~ Order * Voice + (Order * Voice | subject) + (Order *
m1:     Voice | sentence)
   Df    AIC    BIC  logLik deviance Chisq Chi Df Pr(>Chisq)
mi 24 1633.0 1742.9 -792.48   1585.0
m1 25 1664.1 1778.5 -807.03   1614.1     0      1          1


summary(m1)

Linear mixed model fit by maximum likelihood ['lmerMod']
Formula: score ~ Order * Voice + (Order * Voice | subject) + (Order *
    Voice | sentence)

      AIC       BIC    logLik  deviance
1664.0644 1778.5457 -807.0322 1614.0644

Random effects:
 Groups   Name          Variance  Std.Dev. Corr
 subject  (Intercept)   0.0387850 0.19694
          Order1        0.0005166 0.02273  0.65
          Voice1        0.0326954 0.18082  0.71 0.55
          Order1:Voice1 0.0266763 0.16333  0.34 0.76 0.57
 sentence (Intercept)   0.0683897 0.26151
          Order1        0.0779307 0.27916  1.00
          Voice1        0.0096621 0.09830  0.61 0.60
          Order1:Voice1 0.1013066 0.31829  0.55 0.55 0.34
 Residual               0.4272769 0.65366
Number of obs: 720, groups: subject, 36; sentence, 20

Fixed effects:
              Estimate Std. Error t value
(Intercept)    0.26184    0.07135   3.670
Order1        -0.12645    0.06711  -1.884
Voice1        -0.10761    0.04455  -2.415
Order1:Voice1  0.13562    0.08000   1.695

Correlation of Fixed Effects:
            (Intr) Order1 Voice1
Order1      0.776
Voice1      0.469  0.294
Order1:Voc1 0.458  0.468  0.282

----------------------------------------------------
Emilia Ellsiepen
Institut f?r Linguistik
Goethe-Universit?t Frankfurt


From bates at stat.wisc.edu  Fri Feb 28 17:27:02 2014
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 28 Feb 2014 10:27:02 -0600
Subject: [R-sig-ME] likelihood-ratio tests in conflict with coefficiants
 in maximal random effect model
In-Reply-To: <CAOU=wbkzKV2e2+k2q3B6rx+ANFzj7MDKtG4xnQ1vmUAfKF17uA@mail.gmail.com>
References: <CAOU=wbkzKV2e2+k2q3B6rx+ANFzj7MDKtG4xnQ1vmUAfKF17uA@mail.gmail.com>
Message-ID: <CAO7JsnTHhcExPyKMLBWeHRQbSDgr+HBx0PQ0bnAhBspiFk-YDA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140228/5f447e43/attachment.pl>

From bbolker at gmail.com  Fri Feb 28 18:29:32 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 28 Feb 2014 12:29:32 -0500
Subject: [R-sig-ME] likelihood-ratio tests in conflict with coefficiants
 in maximal random effect model
In-Reply-To: <CAO7JsnTHhcExPyKMLBWeHRQbSDgr+HBx0PQ0bnAhBspiFk-YDA@mail.gmail.com>
References: <CAOU=wbkzKV2e2+k2q3B6rx+ANFzj7MDKtG4xnQ1vmUAfKF17uA@mail.gmail.com>
	<CAO7JsnTHhcExPyKMLBWeHRQbSDgr+HBx0PQ0bnAhBspiFk-YDA@mail.gmail.com>
Message-ID: <5310C77C.1090404@gmail.com>

On 14-02-28 11:27 AM, Douglas Bates wrote:
> On Fri, Feb 28, 2014 at 10:04 AM, Emilia Ellsiepen <
> emilia.ellsiepen at gmail.com> wrote:
> 
>> Dear list members,
>>
>> in analyzing a data set using lmers with maximal random effect
>> structure and subsequent likelihood-ratio tests (LRTs) following Barr
>> et al. 2013, I ran into the following problem: In some of the LRTs, it
>> turned out that the simpler model (only main effects) has a higher
>> likelihood than the more complicated model (including interaction),
>> resulting in Chi=0. If I simplify the models by taking out the
>> interactions in the two random effect terms, the LRT for the
>> interaction has a highly significant result.>

[snip]

> It shows that the more complex model has not converged to the optimum
> parameter values.  This can be because the optimizer being used is a bad
> choice (in recent versions of the lme4 package the default was a
> Nelder-Mead optimizer that can declare convergence to values that are not
> the optimal values) or it can be because the model is too complex.  We say
> that such models are over-parameterized.
> 
> This is why the Barr et al. advice is dangerous.  In model selection there
> are two basic strategies: forward and backward.  Forward selection starts
> with a simple model and adds terms until they are no longer justified.
>  Backward selection starts with the most complex model and drops terms.  It
> is well known that backward selection is problematic when you can't fit the
> most complex model.  Yet Barr et al. say unequivocally that you must use
> backward selection.  The result will be that many researchers, especially
> in linguistics, will encounter these problems of complex models providing
> worse fits than simpler models.
> 
> I wish that Barr et al. would have provided software that is guaranteed to
> fit even the most complex model to a global optimum when they stated their
> "one size fits all" strategy.  they didn't and those with experience in
> numerical optimization can tell you why.  It is not possible to guarantee
> convergence to an optimum in over-parameterized models.
> 

  [snip]

  I would be very interested to know whether the more thorough
convergence tests that we have implemented in the development version of
lme4 would correctly report that there are convergence problems with the
model containing the maximal RE structure ...

  Ben Bolker


From bbolker at gmail.com  Fri Feb 28 18:59:40 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 28 Feb 2014 12:59:40 -0500
Subject: [R-sig-ME] Creation of a dummy variable with Lme4
In-Reply-To: <ec0c111e9369ef1d18443ba20e1ac106@courriel-fec.usherbrooke.ca>
References: <ec0c111e9369ef1d18443ba20e1ac106@courriel-fec.usherbrooke.ca>
Message-ID: <5310CE8C.1050803@gmail.com>

On 14-02-28 12:07 PM, Camille Le Gall-Payne wrote:
>  
> 
> Hi Mr. Bolker, 
> 

[cc'ing to r-sig-mixed-models]


> I am having a little problem with the package lme4. I am trying to test
> the random structure of a GLMM model. With the previous version of the
> package I could create a dummy variable and perform a LRT test of the
> random structure. However, with the new version of lme4 the glmer
> function prevents us to use a random variable with only one level. 
> 
> I was wondering if there is a way to create a dummy variable with glmer
> ? or If you know of another way to test the random structure of a GLMM
> model ? 
> 
> Thank you, 
> 
> Have a nice day 
> 
> Camille 


  I hadn't seen this particular trick before, but I can see what you're
doing -- hopefully the dummy grouping variable you're creating here ends
up with an estimated variance of zero (otherwise I would worry about
whether your results are sensible or not).  You can use

 control=glmerControl(check.nlev.gtr.1="ignore")

in your glmer() call to override the error you would otherwise get.

  It's also worth considering that in lme4 > 1.0, the likelihood values
that are computed are commensurate with those returned by glm().  I
don't know offhand whether anova(m1,m0) (where m1 is a glmer fit/merMod
object and m0 is a glm fit/object) works, but there's no reason in
principle why it shouldn't.

   In principle you could also do this by extracting the deviance
function (dd <- update(m1,devFunOnly=TRUE)) and optimizing over the
fixed-effects parameters while holding the random-effects parameters at
0 (i.e. computing the profiled likelihood for zero random effects), but
this is really just a highly inefficient way to fit the same model that
you would get by leaving out the REs and using glm().

  Ben Bolker


From rlevy at ucsd.edu  Fri Feb 28 21:27:02 2014
From: rlevy at ucsd.edu (Levy, Roger)
Date: Fri, 28 Feb 2014 20:27:02 +0000
Subject: [R-sig-ME] likelihood-ratio tests in conflict with coefficiants
 in maximal random effect model
In-Reply-To: <CAO7JsnTHhcExPyKMLBWeHRQbSDgr+HBx0PQ0bnAhBspiFk-YDA@mail.gmail.com>
References: <CAOU=wbkzKV2e2+k2q3B6rx+ANFzj7MDKtG4xnQ1vmUAfKF17uA@mail.gmail.com>
	<CAO7JsnTHhcExPyKMLBWeHRQbSDgr+HBx0PQ0bnAhBspiFk-YDA@mail.gmail.com>
Message-ID: <B536A3C0-1895-4FDB-B92B-FA3E11C0DCA2@ucsd.edu>

As one of the co-authors of the Barr et al. 2013 paper, I would like to clarify what I believe are some misunderstandings of the central message of our paper?[see below]

On Feb 28, 2014, at 9:27 AM, Douglas Bates <bates at stat.wisc.edu> wrote:

> On Fri, Feb 28, 2014 at 10:04 AM, Emilia Ellsiepen <
> emilia.ellsiepen at gmail.com> wrote:
> 
>> Dear list members,
>> 
>> in analyzing a data set using lmers with maximal random effect
>> structure and subsequent likelihood-ratio tests (LRTs) following Barr
>> et al. 2013, I ran into the following problem: In some of the LRTs, it
>> turned out that the simpler model (only main effects) has a higher
>> likelihood than the more complicated model (including interaction),
>> resulting in Chi=0. If I simplify the models by taking out the
>> interactions in the two random effect terms, the LRT for the
>> interaction has a highly significant result.
>> 
>> Now, I figured this might either be a strong case for Barr et al., or
>> something is going wrong, as the data do suggest an interaction, and
>> the t-value for the interaction coefficiant in the model summary of
>> the maximal random effect model is 1.69 - which I would have expected
>> to be a (close to) marginal effect. Also, a lot of researchers who do
>> use maximal random effect models rather interpret the t-values
>> directly instead of running LRTs, which then might lead to different
>> interpretations of the same data.
>> 
> 
> It shows that the more complex model has not converged to the optimum
> parameter values.  This can be because the optimizer being used is a bad
> choice (in recent versions of the lme4 package the default was a
> Nelder-Mead optimizer that can declare convergence to values that are not
> the optimal values) or it can be because the model is too complex.  We say
> that such models are over-parameterized.

First off, it is not clear that Emilia?s specific problem is being caused by over-parameterization.  Emilia, could you perhaps give more information about the nature of the dataset that you?re analyzing?  Is it a 2x2 within-subjects, within-sentence balanced design without a great deal of missing data?  In my experience with the last few pre-1.0 versions, lme4 is generally very good at converging to an optimum for these kinds of datasets with the number of observations and groups your fitted model reports.  Have you tried fitting the model with the nlminb optimizer, either by including 

optimizer="optimx",optCtrl=list(method="nlminb?)

in the list of arguments to lmerControl, or by using the last pre-1.0 version of lme4 (available as lme4.0 on R-Forge)?  Do you still get similar problems with the nlminb optimizer?  (You should definitely not get the result that the simpler model has a higher log-likelihood.)

> This is why the Barr et al. advice is dangerous.  In model selection there
> are two basic strategies: forward and backward.  Forward selection starts
> with a simple model and adds terms until they are no longer justified.
> Backward selection starts with the most complex model and drops terms.  It
> is well known that backward selection is problematic when you can't fit the
> most complex model.  Yet Barr et al. say unequivocally that you must use
> backward selection.  

We don?t say that you must use backward selection.  Rather, we argue against model selection altogether for determining random-effects specification for confirmatory hypothesis testing.  We advocate ?design-driven? random-effects specification for confirmatory hypothesis testing (see pages 262-263).  That is, if you are fitting a model with the goal of testing the effect of some predictor X and X varies within levels of a random effects grouping factor (say, in Emilia?s case, within subjects and sentences), then a within-grouping-factor random slope for X plays a critical role as, effectively, a potential confound that should be kept in the model as a control.  If X varies across levels, the within-grouping-factor random intercept plays that role.

One might object by saying that what we advocate is even worse, because keeping all the random effects in will make it even less likely that the models converge to a global optimum.  But we make a distinction between fixed-effects predictors of critical theoretical interest for one?s confirmatory hypothesis test and other fixed-effects predictors in the model.  It is only for the former that we are arguing against model selection; for the latter, we think that more flexibility is probably reasonable.  Typically, for any particular hypothesis test, we are not talking about a huge number of parameters that must be included in order to satisfy design-driven requirements.  (We discuss this issue on page 275.)

> The result will be that many researchers, especially
> in linguistics, will encounter these problems of complex models providing
> worse fits than simpler models.

Once again, I don?t think that there is enough evidence in the original post to conclude that this necessarily was a problem of overparameterization.

> I wish that Barr et al. would have provided software that is guaranteed to
> fit even the most complex model to a global optimum when they stated their
> "one size fits all" strategy.  they didn't and those with experience in
> numerical optimization can tell you why.  It is not possible to guarantee
> convergence to an optimum in over-parameterized models.
> 
> This is why papers like Barr et al. that, based on a single simulation
> study, state absolute rules about the complex and subtle process of model
> selection, are dangerous.

Let me quote from the discussion section of our paper (page 276):

"Research is needed to evaluate the various possible strategies that one could follow when one cannot fit a maximal model. Both our theoretical analysis and simulations suggest a general rule of thumb: for whatever fixed effects are of critical interest, the corresponding random effects should be present in that analysis."

I am sorry if this statement, which I think is a reasonable summary of our overall position, seems like an absolute rule or a one-size-fits-all strategy; it was certainly not my intent that it come across as one, nor do I think it was the intent of my co-authors.  We recognize that model selection can be a complex and subtle process.  The goal of our paper was to provide readers with better guidance as to how design-driven considerations ? which are extremely important for confirmatory hypothesis testing! ? should be taken into account within this process.

Best

Roger


> My questions are:
>> 1) Is the LRT reported below reliable, indicating there is no
>> interaction, or does the non-expected difference in logLik suggest
>> that something went wrong?
>> 2) Is it possible that the control parameter that increases the number
>> of evaluations has something to do with it?
>> 
>> 
>> Here are some details:
>> The design is 2x2 within subjects and items, the factors are sum-coded.
>> I used REML=F to straightforwardly perform the LRTs afterwards and
>> increased the number of evaluations for the model to converge
>> 
>> m1 = lmer(score ~ Order*Voice +
>> (Order*Voice|subject)+(Order*Voice|sentence) ,REML=F,
>> control=lmerControl(optCtrl=list(maxfun=50000)),rawdata)
>> mi = lmer(score ~ Order+Voice +
>> (Order*Voice|subject)+(Order*Voice|sentence) ,REML=F,
>> control=lmerControl(optCtrl=list(maxfun=50000)),rawdata)
>> 
>> anova(m1,mi)
>> 
>> Models:
>> mi: score ~ Order + Voice + (Order * Voice | subject) + (Order *
>> mi:     Voice | sentence)
>> m1: score ~ Order * Voice + (Order * Voice | subject) + (Order *
>> m1:     Voice | sentence)
>> Df    AIC    BIC  logLik deviance Chisq Chi Df Pr(>Chisq)
>> mi 24 1633.0 1742.9 -792.48   1585.0
>> m1 25 1664.1 1778.5 -807.03   1614.1     0      1          1
>> 
>> 
>> summary(m1)
>> 
>> Linear mixed model fit by maximum likelihood ['lmerMod']
>> Formula: score ~ Order * Voice + (Order * Voice | subject) + (Order *
>> Voice | sentence)
>> 
>>  AIC       BIC    logLik  deviance
>> 1664.0644 1778.5457 -807.0322 1614.0644
>> 
>> Random effects:
>> Groups   Name          Variance  Std.Dev. Corr
>> subject  (Intercept)   0.0387850 0.19694
>>      Order1        0.0005166 0.02273  0.65
>>      Voice1        0.0326954 0.18082  0.71 0.55
>>      Order1:Voice1 0.0266763 0.16333  0.34 0.76 0.57
>> sentence (Intercept)   0.0683897 0.26151
>>      Order1        0.0779307 0.27916  1.00
>>      Voice1        0.0096621 0.09830  0.61 0.60
>>      Order1:Voice1 0.1013066 0.31829  0.55 0.55 0.34
>> Residual               0.4272769 0.65366
>> Number of obs: 720, groups: subject, 36; sentence, 20
>> 
>> Fixed effects:
>>          Estimate Std. Error t value
>> (Intercept)    0.26184    0.07135   3.670
>> Order1        -0.12645    0.06711  -1.884
>> Voice1        -0.10761    0.04455  -2.415
>> Order1:Voice1  0.13562    0.08000   1.695
>> 
>> Correlation of Fixed Effects:
>>        (Intr) Order1 Voice1
>> Order1      0.776
>> Voice1      0.469  0.294
>> Order1:Voc1 0.458  0.468  0.282
>> 
>> ----------------------------------------------------
>> Emilia Ellsiepen
>> Institut f?r Linguistik
>> Goethe-Universit?t Frankfurt
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> 
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From bates at stat.wisc.edu  Fri Feb 28 22:18:18 2014
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 28 Feb 2014 15:18:18 -0600
Subject: [R-sig-ME] likelihood-ratio tests in conflict with coefficiants
 in maximal random effect model
In-Reply-To: <B536A3C0-1895-4FDB-B92B-FA3E11C0DCA2@ucsd.edu>
References: <CAOU=wbkzKV2e2+k2q3B6rx+ANFzj7MDKtG4xnQ1vmUAfKF17uA@mail.gmail.com>
	<CAO7JsnTHhcExPyKMLBWeHRQbSDgr+HBx0PQ0bnAhBspiFk-YDA@mail.gmail.com>
	<B536A3C0-1895-4FDB-B92B-FA3E11C0DCA2@ucsd.edu>
Message-ID: <CAO7JsnQAjaoivXbBW0PZgjxHpofTiK59RuobvXJwMaty3VHc7g@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140228/7b24324a/attachment.pl>

From bbolker at gmail.com  Fri Feb 28 23:30:31 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 28 Feb 2014 17:30:31 -0500
Subject: [R-sig-ME] glmmADMB (mixed binomal random effects)
In-Reply-To: <29234_1393626011_s1SMKAxA017087_20140228221936.9066518699A@r-forge.r-project.org>
References: <29234_1393626011_s1SMKAxA017087_20140228221936.9066518699A@r-forge.r-project.org>
Message-ID: <53110E07.4050006@mcmaster.ca>

On 14-02-28 05:19 PM, Julian Chen wrote:
> Hello:
> 
> I am new to this and now running a simple model of mixed effects
> negative binomial regression by using glmmADMB my data is very
> simple as follows: Counts Year Test1 Test2
> 
> 
> Here counts is the number of students annually admitted (could be
> zero) Year 1 to 6 (totally 6 numbers represnts six years) Test 1 is
> the score of test one range from (0 to 100) Test 2 is the score of
> test two range from (0 to 100)
> 
> Here I asssume Variable Year is random effects. The following is
> the model I used: rnb1<-glmmadmb(ACC ~ F1 + F2 + (1|YR), data=dat,
> family="nbinom", link="log");
> 
> But I always got the wrong message as below:
> 
> rnb1<-glmmadmb(Counts ~ Test1 + Test2 + (1|Year), data=dat,
> family="nbinom", link="log"); Error in Droplevels(eval(parse(text =
> x), data)) : all grouping variables in random effects must be
> factors
> 
> I have no idea what I should do now? Please give me some comments.
> Many thanks!
> 
  [cc'ing to r-sig-mixed-models]

  make sure that your grouping variable (Year) is a factor.

  Year is coded as numeric.  Your choices:

1. convert to a factor on the fly:
    Counts ~ Test1 + Test2 + (1|factor(Year)
2. convert to a factor in the data frame:
   dat <- transform(dat,Year=factor(Year))
3. make a new factor variable in the data frame:
   dat <- transform(dat,fYear=factor(Year))
   Counts ~ Test1 + Test2 + (1|fYear)

#1 is quickest but might be slightly less robust sometimes
#2 is robust but you might want to keep Year as numeric for other purposes
#3 keeps year as numeric and adds an additional factor version of the
variable


From bbolker at gmail.com  Sat Mar  1 00:25:05 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 28 Feb 2014 23:25:05 +0000 (UTC)
Subject: [R-sig-ME] nlmer self starting function
References: <BC9774DE-A084-4897-A7A9-742420F0DC2C@umass.edu>
Message-ID: <loom.20140301T001602-66@post.gmane.org>

Daniel Hocking <dhocking at ...> writes:

 
> I am trying to use the following nlmer function to estimate random
> break points in a polynomial smooth break point regression (sense
> Hout et al. 2011:

> nl1 <- nlmer(Reaction ~ SSPoly(Days,n0,n1,n2,n3) ~
> (n0|Subject)+(n1+n2|Subject)+(n3|Subject), data=sleepstudy, start =
> c(n0=22, n1=0, n2=-1, n3=-4)) I get the error: Error in eval(expr,
> envir, enclos) : could not find function ?SSPoly"

The SSBW and SSPoly functions are implemented by Hout et al., they're
not (and have never been) in lme4.  I looked at the paper and couldn't
find any details about what these functions actually look like or
where they can be obtained: 

http://onlinelibrary.wiley.com/doi/10.1002/sim.4127/full#app1

says, in full,

"The Bacon?Watts mixed-effects model and the polynomial mixed-effects
model can be estimated in R using nlmer with user-defined
self-starting non-linear models. We define self-starting functions
SSBW and SSPoly, and use calls such as [EQUATION: nlmer(y ~ SSBW(...),
...)]  and [EQUATION: nlmer(y~SSPoly(...), ...)]  where t denotes time
and id is the subject identifier in data.

Self-starting SSBW and SSPoly include specifications of the
gradient. For the Bacon?Watts model, the gradient in SSBW is in closed
form. For the polynomial model, we used numerical procedures in SSPoly
to approximate the gradient. As a result, the polynomial model takes
more time to be estimated than the Bacon-Watts model."

I couldn't see any further supplementary materials anywhere in the web
page for the article. Googling "'van den Hout' SSPoly' didn't help
either.  I guess you'll have to contact the authors.

> I assume this is due to changes in lme4 versions. I am not
> experienced with the nlmer function and writing custom functions and
> start functions. Does anyone have recommendations on the easiest way
> to implement this model (preferable without going to WinBUGS/JAGS)?

 good luck,
  Ben Bolker


From rlevy at ucsd.edu  Sat Mar  1 00:30:22 2014
From: rlevy at ucsd.edu (Levy, Roger)
Date: Fri, 28 Feb 2014 23:30:22 +0000
Subject: [R-sig-ME] likelihood-ratio tests in conflict with coefficiants
 in maximal random effect model
In-Reply-To: <CAO7JsnQAjaoivXbBW0PZgjxHpofTiK59RuobvXJwMaty3VHc7g@mail.gmail.com>
References: <CAOU=wbkzKV2e2+k2q3B6rx+ANFzj7MDKtG4xnQ1vmUAfKF17uA@mail.gmail.com>
	<CAO7JsnTHhcExPyKMLBWeHRQbSDgr+HBx0PQ0bnAhBspiFk-YDA@mail.gmail.com>
	<B536A3C0-1895-4FDB-B92B-FA3E11C0DCA2@ucsd.edu>
	<CAO7JsnQAjaoivXbBW0PZgjxHpofTiK59RuobvXJwMaty3VHc7g@mail.gmail.com>
Message-ID: <869AA1B9-50F2-4D08-9DC0-750BFFC2F783@ucsd.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140228/5879cff9/attachment.pl>

From rdiaz02 at gmail.com  Sat Mar  1 12:44:04 2014
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Sat, 01 Mar 2014 12:44:04 +0100
Subject: [R-sig-ME] MCMCglmm: predictions for posterior predictive checks
Message-ID: <878usu9m9n.fsf@gmail.com>

Dear List,

I want to perform a simple posterior predictive check on some Poisson
models I've fitted using MCMCglmm. I immediately though about using
predict.MCMCglmm as

predict(mymodel, type = "response", marginal = 0, interval = "prediction")

However, predict returns the expectation (in fact one of its very last
lines have pred <- matrix(colMeans(post.pred), dim(post.pred)[2], 1)).


I can hack predict.MCMCglmm and directly return the "post.pred" object
which, IIUC, would give me the "y^{rep}" (in Gelman et al., 1996,
notation.).  But having to do this makes me wonder if I am understanding
this correctly.

Is directly using the "post.pred" object the right way of getting the
y^{rep} with MCMCglmm?


Best,


R.




P.S. I am using "marginal = 0" as I want what, e.g., Green et al., 2009
("Use of posterior predictive assessments to evaluate model fit in
multilevel logistic regression", Vet. Res, 40) call "full": "The predictive
distribution was conditional on all fixed effect and random effect
parameters estimated in the final model and a replicate observation
y_{ijk}^{full} generated from the conditional distribution (...)".

-- 
Ramon Diaz-Uriarte
Department of Biochemistry, Lab B-25
Facultad de Medicina 
Universidad Aut?noma de Madrid 
Arzobispo Morcillo, 4
28029 Madrid
Spain

Phone: +34-91-497-2412

Email: rdiaz02 at gmail.com
       ramon.diaz at iib.uam.es

http://ligarto.org/rdiaz


From j.hadfield at ed.ac.uk  Sat Mar  1 19:32:36 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Sat, 01 Mar 2014 18:32:36 +0000
Subject: [R-sig-ME] MCMCglmm: predictions for posterior predictive checks
In-Reply-To: <878usu9m9n.fsf@gmail.com>
References: <878usu9m9n.fsf@gmail.com>
Message-ID: <20140301183236.12226j9ic3on8vc4@www.staffmail.ed.ac.uk>

Hi,

post.pred is equivalent to y^{rep} but you have to be careful about  
whether you want to marginlaise the random effects or not. If you  
don't marginalise the random effects then post.pred is usually the  
joint posterior predictive distribution of y (where I take y to be a  
vector, and joint refers to the fact that it is the joint distribution  
of y^{rep}_{1},y^{rep}_{2} ...y^{rep}_{n}). However, post.pred always  
marginalises the residuals (or it wouldn't be a predictive  
distribution!).  If, for example, the model was bivariate with  
observations x and y, and R-structure of the form us(trait):units then  
post.pred does not give the joint posterior predictive distribution of  
x and y: it will give the joint posterior predictive distribution of x  
marginal to the residuals of y, and vice versa.

If the random effects are marginalised then post.pred is returning the  
marginal posterior predictive distribution for each observation. This  
might not always be what you want.  Imagine you make two observations  
on the same individuals at each of two time points, and you have the  
random effect structure us(time):individual.  This fits different  
between-individual variances for observations at different times, and  
also fits a between-individual across-time covariance. If you  
marginalise the individual effects (for example if you want to get a  
posterior predictive distribution for an observation made on a random  
individual) then post.pred will not preserve the covariation structure  
of the four observations. For example, have Y_ijk as the kth  
observation at time j for individual i. We have Y_i11, Y_i12 Y_i21,  
Y_i22.  Imagine us(time):individual gives us a covariance matrix V  
with 1 and 2 along the diagonal, and sqrt(1/2) on the off-diagonals  
(ie. a correlation of 0.5). If you marginalise the individual effects  
then the they are assumed to be drawn from a distribution with  
diagonal variance structure Diag{1,1,2,2}, whereas perhaps you would  
prefer them to be drawn from kronecker(V, diag(2)).

This is a bit hard to explain! If it doesn't make sense let me know,  
and I'll try again.

Cheers,

Jarrod














Cheers,

Jarrod


Quoting Ramon Diaz-Uriarte <rdiaz02 at gmail.com> on Sat, 01 Mar 2014  
12:44:04 +0100:

> Dear List,
>
> I want to perform a simple posterior predictive check on some Poisson
> models I've fitted using MCMCglmm. I immediately though about using
> predict.MCMCglmm as
>
> predict(mymodel, type = "response", marginal = 0, interval = "prediction")
>
> However, predict returns the expectation (in fact one of its very last
> lines have pred <- matrix(colMeans(post.pred), dim(post.pred)[2], 1)).
>
>
> I can hack predict.MCMCglmm and directly return the "post.pred" object
> which, IIUC, would give me the "y^{rep}" (in Gelman et al., 1996,
> notation.).  But having to do this makes me wonder if I am understanding
> this correctly.
>
> Is directly using the "post.pred" object the right way of getting the
> y^{rep} with MCMCglmm?
>
>
> Best,
>
>
> R.
>
>
>
>
> P.S. I am using "marginal = 0" as I want what, e.g., Green et al., 2009
> ("Use of posterior predictive assessments to evaluate model fit in
> multilevel logistic regression", Vet. Res, 40) call "full": "The predictive
> distribution was conditional on all fixed effect and random effect
> parameters estimated in the final model and a replicate observation
> y_{ijk}^{full} generated from the conditional distribution (...)".
>
> --
> Ramon Diaz-Uriarte
> Department of Biochemistry, Lab B-25
> Facultad de Medicina
> Universidad Aut?noma de Madrid
> Arzobispo Morcillo, 4
> 28029 Madrid
> Spain
>
> Phone: +34-91-497-2412
>
> Email: rdiaz02 at gmail.com
>        ramon.diaz at iib.uam.es
>
> http://ligarto.org/rdiaz
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From j.hadfield at ed.ac.uk  Sat Mar  1 20:19:49 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Sat, 01 Mar 2014 19:19:49 +0000
Subject: [R-sig-ME] MCMCglmm: predictions for posterior predictive checks
In-Reply-To: <20140301183236.12226j9ic3on8vc4@www.staffmail.ed.ac.uk>
References: <878usu9m9n.fsf@gmail.com>
	<20140301183236.12226j9ic3on8vc4@www.staffmail.ed.ac.uk>
Message-ID: <20140301191949.17354bp6h0d002xk@www.staffmail.ed.ac.uk>

Hi,

Sorry, kronecker(V, diag(2)) should have been kronecker(V, matrix(1,2,2)).

Cheers,

Jarrod



Quoting Jarrod Hadfield <j.hadfield at ed.ac.uk> on Sat, 01 Mar 2014  
18:32:36 +0000:

> Hi,
>
> post.pred is equivalent to y^{rep} but you have to be careful about  
> whether you want to marginlaise the random effects or not. If you  
> don't marginalise the random effects then post.pred is usually the  
> joint posterior predictive distribution of y (where I take y to be a  
> vector, and joint refers to the fact that it is the joint  
> distribution of y^{rep}_{1},y^{rep}_{2} ...y^{rep}_{n}). However,  
> post.pred always marginalises the residuals (or it wouldn't be a  
> predictive distribution!).  If, for example, the model was bivariate  
> with observations x and y, and R-structure of the form  
> us(trait):units then post.pred does not give the joint posterior  
> predictive distribution of x and y: it will give the joint posterior  
> predictive distribution of x marginal to the residuals of y, and  
> vice versa.
>
> If the random effects are marginalised then post.pred is returning  
> the marginal posterior predictive distribution for each observation.  
> This might not always be what you want.  Imagine you make two  
> observations on the same individuals at each of two time points, and  
> you have the random effect structure us(time):individual.  This fits  
> different between-individual variances for observations at different  
> times, and also fits a between-individual across-time covariance. If  
> you marginalise the individual effects (for example if you want to  
> get a posterior predictive distribution for an observation made on a  
> random individual) then post.pred will not preserve the covariation  
> structure of the four observations. For example, have Y_ijk as the  
> kth observation at time j for individual i. We have Y_i11, Y_i12  
> Y_i21, Y_i22.  Imagine us(time):individual gives us a covariance  
> matrix V with 1 and 2 along the diagonal, and sqrt(1/2) on the  
> off-diagonals (ie. a correlation of 0.5). If you marginalise the  
> individual effects then the they are assumed to be drawn from a  
> distribution with diagonal variance structure Diag{1,1,2,2}, whereas  
> perhaps you would prefer them to be drawn from kronecker(V, diag(2)).
>
> This is a bit hard to explain! If it doesn't make sense let me know,  
> and I'll try again.
>
> Cheers,
>
> Jarrod
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> Cheers,
>
> Jarrod
>
>
> Quoting Ramon Diaz-Uriarte <rdiaz02 at gmail.com> on Sat, 01 Mar 2014  
> 12:44:04 +0100:
>
>> Dear List,
>>
>> I want to perform a simple posterior predictive check on some Poisson
>> models I've fitted using MCMCglmm. I immediately though about using
>> predict.MCMCglmm as
>>
>> predict(mymodel, type = "response", marginal = 0, interval = "prediction")
>>
>> However, predict returns the expectation (in fact one of its very last
>> lines have pred <- matrix(colMeans(post.pred), dim(post.pred)[2], 1)).
>>
>>
>> I can hack predict.MCMCglmm and directly return the "post.pred" object
>> which, IIUC, would give me the "y^{rep}" (in Gelman et al., 1996,
>> notation.).  But having to do this makes me wonder if I am understanding
>> this correctly.
>>
>> Is directly using the "post.pred" object the right way of getting the
>> y^{rep} with MCMCglmm?
>>
>>
>> Best,
>>
>>
>> R.
>>
>>
>>
>>
>> P.S. I am using "marginal = 0" as I want what, e.g., Green et al., 2009
>> ("Use of posterior predictive assessments to evaluate model fit in
>> multilevel logistic regression", Vet. Res, 40) call "full": "The predictive
>> distribution was conditional on all fixed effect and random effect
>> parameters estimated in the final model and a replicate observation
>> y_{ijk}^{full} generated from the conditional distribution (...)".
>>
>> --
>> Ramon Diaz-Uriarte
>> Department of Biochemistry, Lab B-25
>> Facultad de Medicina
>> Universidad Aut?noma de Madrid
>> Arzobispo Morcillo, 4
>> 28029 Madrid
>> Spain
>>
>> Phone: +34-91-497-2412
>>
>> Email: rdiaz02 at gmail.com
>>       ramon.diaz at iib.uam.es
>>
>> http://ligarto.org/rdiaz
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>
>
> -- 
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From bbolker at gmail.com  Sat Mar  1 22:11:52 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 1 Mar 2014 21:11:52 +0000 (UTC)
Subject: [R-sig-ME] prediction from glmer.nb
References: <CALFR8h9BjVN0O4M7BvxHmjJTZ9DBPg_FWwTh0-PFBaX3Zwdzow@mail.gmail.com>
Message-ID: <loom.20140301T033910-111@post.gmane.org>

Thomas Lee Anderson <anderstl at ...> writes:

> 
> Greetings,
> 
> I have a general and somewhat basic question about how glmer.nb works in
> terms of what scale the parameter estimates are on, and predicting from
> glmer.nb. My understanding from reading through some of the help files in
> the glmm FAQ wiki page is that in order to predict from the glmer models,
> predicted values need to be back transformed to place them on the same
> scale as the response variable (accomplished by using type="response" in
> the predict command).  Otherwise, the predicted values would be on the log
> scale in my case (using glmer.nb).  Am I interpreting 
> this correctly and/or
> correct on what scale the predicted values would be?  I am getting rather
> nonsensical values (at least in my estimation) from the predict command
> when I use the type="response". The raw data for the response variable
> ranges from 0-10, and I am predicting over a range 
> values that we actually
> observed for the covariates but the predicted values sometimes 
> exceed >
> 10^27 larger than the actual data.  Any thoughts as 
> to what I'm doing wrong
> or my ignorance of what is actually going on would be appreciated.

  Maybe there's a bug in glmer.nb?  It's somewhat experimental (I thought
the documentation said that: it doesn't, but the brevity of the help page
might be a hint to that effect ...)
> 
> I understand the etiquette is to give a reproducible example but I wasn't
> sure if it was necessary for the question, and I honestly wasn't sure the
> best way to accomplish that and was having 
> difficulty replicating the issue
> with simulated data. 

  This actually strongly suggests that we're going to need something
more like your real data in order to figure out the problem.

> Below is a sample of the data, and the output of the
> model summary if that is helpful. Let me know if other information is
> needed.
> 
>      Site Year      AMAN      AMOP  Structure
> 1   0.002 2012 3.7500000 0.0000000      13.0
> 2   0.002 2013 1.9666667 0.0000000      13.0
> 3  0.002A 2012 0.3333333 2.7777778       0.0
> 4  0.002A 2013 0.0000000 0.0000000       0.0
> 5   0.003 2012 0.0000000 0.2500000     100.0
> 6   0.003 2013 0.0000000 0.1904762     100.0
> 7      10 2012 0.0000000 0.0000000      95.0
> 8      10 2013 0.2333333 0.0000000      95.0
> 9     103 2012 0.0000000 4.6666667      32.5
> 
> 10    103 2013 0.0000000 1.9166667      32.5
> 
> > summary(aman.d5)Generalized linear mixed model fit by maximum likelihood
['glmerMod']
>  Family: Negative Binomial(0.6346) ( log )
> Formula: AMAN ~ AMOP * Structure + (1 | Year)
>    Data: ..2
> 
>       AIC       BIC    logLik  deviance
>  917.1346  940.7914 -452.5673  905.1346
> 
> Random effects:
>  Groups   Name        Variance  Std.Dev.
>  Year     (Intercept) 2.953e-10 1.718e-05
>  Residual             9.836e-01 9.918e-01
> Number of obs: 381, groups: Year, 2

  Notice here that the random effects variance is practically
zero, which suggests that you will do equally well (i.e. get
almost identical results) with MASS::glm.nb -- a reasonable
workaround.
  In general this will almost always happen with a grouping
variable with only 2 levels.  See http://glmm.wikidot.com/faq for
more discussion.  If all your data sets look like this, you
should really save yourself some trouble and just fit Year as
a fixed effect (i.e., use MASS::glm.nb)

  It's also quite weird that you have non-integer response
values (AMAN).  Except possibly in some very particular situations, that
doesn't make sense.  Are these densities rather than counts?
(If so you should consider using the raw counts as the response
variable, with log(sampling area) as an offset term.)

> 
> Fixed effects:
>                 Estimate Std. Error t value Pr(>|z|)
> (Intercept)    -0.124857   0.130868  -0.954 0.340047
> AMOP           -0.558918   0.208893  -2.676 0.007459 **
> Structure      -0.004156   0.003175  -1.309 0.190474
> AMOP:Structure  0.013938   0.004152   3.357 0.000788 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

  Besides this, nothing looks particularly odd


From rdiaz02 at gmail.com  Sun Mar  2 00:11:14 2014
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Sun, 02 Mar 2014 00:11:14 +0100
Subject: [R-sig-ME] MCMCglmm: predictions for posterior predictive checks
In-Reply-To: <20140301183236.12226j9ic3on8vc4@www.staffmail.ed.ac.uk>
References: <878usu9m9n.fsf@gmail.com>
	<20140301183236.12226j9ic3on8vc4@www.staffmail.ed.ac.uk>
Message-ID: <878ust8qgd.fsf@gmail.com>


Hi Jarrod,

Thanks for the detailed answer.  But let me make sure I follow (apologies
in advance if the questions seem dumb). It is basically double-checking.



On Sat, 01-03-2014, at 19:32, j.hadfield at ed.ac.uk wrote:
> Hi,
>
> post.pred is equivalent to y^{rep} but you have to be careful about  
> whether you want to marginlaise the random effects or not. If you  
> don't marginalise the random effects then post.pred is usually the  
> joint posterior predictive distribution of y (where I take y to be a  
> vector, and joint refers to the fact that it is the joint distribution  
> of y^{rep}_{1},y^{rep}_{2} ...y^{rep}_{n}). However, post.pred always  

and "n" is the number of observations (NOT the number of iterations). In
terms of the code for predict.MCMCglmm, n is nrow(pred) or
nrow(post.pred). Is this correct?



> marginalises the residuals (or it wouldn't be a predictive  
> distribution!).

Before we get to the bivariate, let me make sure I follow in two simpler
cases. First, in a univariate Gaussian, post.pred marginalises over the
"e". This seems intuitively obvious, as otherwise y^{rep}_{i} would be
identical to y^{rep}_{i} (for i constant) over all iterations.


Now, in a univariate Poisson (my case, actually), you also use a residual
"e" (p. 35 of the course notes or p. 2 of the paper). So post.pred also
marginalises over "e" here.  This fits my understanding of what the code
does (since the possible latent variables are never used, and marginal =
0 does work even if the model was fitted with "pl = FALSE").

However, the posterior predictions DO make use of the variance of "e" in
the Poisson case too. The variance of "e" (the units) needs to be used if
the posterior predictions incorporate the possible overdispersion. I
think I can see this in the code, e.g. when postvar is created as

postvar <- t(apply(object$VCV, 1, function(x) {
    x[pos.error[object$error.term]]
}))




> If, for example, the model was bivariate with observations x and y, and
> R-structure of the form us(trait):units then post.pred does not give the
> joint posterior predictive distribution of x and y: it will give the
> joint posterior predictive distribution of x marginal to the residuals of
> y, and vice versa.
>

Aha, thanks. Would it even be possible to think about doing it otherwise
using us(trait):units?  (Not that I am asking for it; I am just trying to
understand if it could thought of).


> If the random effects are marginalised then post.pred is returning the  
> marginal posterior predictive distribution for each observation. This  
> might not always be what you want.  Imagine you make two observations  
> on the same individuals at each of two time points, and you have the  
> random effect structure us(time):individual.  This fits different  
> between-individual variances for observations at different times, and  
> also fits a between-individual across-time covariance. If you  
> marginalise the individual effects (for example if you want to get a  
> posterior predictive distribution for an observation made on a random  
> individual) then post.pred will not preserve the covariation structure  
> of the four observations. For example, have Y_ijk as the kth  
> observation at time j for individual i. We have Y_i11, Y_i12 Y_i21,  
> Y_i22.  Imagine us(time):individual gives us a covariance matrix V  
> with 1 and 2 along the diagonal, and sqrt(1/2) on the off-diagonals  
> (ie. a correlation of 0.5). If you marginalise the individual effects  
> then the they are assumed to be drawn from a distribution with  
> diagonal variance structure Diag{1,1,2,2}, whereas perhaps you would  
> prefer them to be drawn from kronecker(V, diag(2)).
>
> This is a bit hard to explain! If it doesn't make sense let me know,  
> and I'll try again.
>

I had to pay a lot of attention, and I am not sure I got it fully right
(even after your second email, with the kronecker correction), but I think
I see the possible problem.




If I may, let me now go back to the posterior predictive problem.  For
simplicity suppose a single response and a single random effect (e.g.,
plot), so some of the complicating issues above do not apply.

To me it made most sense to obtain the discrepancy without marginalising
random effects (as in Green et al., 2009). But we might also obtain the
discrepancy after marginalising. I've checked in Gelman and Hill's (Data
analysis using regression ...) and Gelman et al. (Bayesian data analysis)
and it seems to me (emphasize the "seems to me") that they do the
equivalent of marginalising over the random effects (note that the
random/fixed effects terminology is not liked by Gelman, etc); that is what
I think I read in p. 524 and ff (sect. 24. 2) of Gelman and Hill and p.159
(sect. 6.5) and 148 and ff. in Gelman et al. (2013 edition). Any
comments/suggestions?



Thanks again for your explanations.


Best,


R.





> Cheers,
>
> Jarrod
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> Cheers,
>
> Jarrod
>
>
> Quoting Ramon Diaz-Uriarte <rdiaz02 at gmail.com> on Sat, 01 Mar 2014  
> 12:44:04 +0100:
>
>> Dear List,
>>
>> I want to perform a simple posterior predictive check on some Poisson
>> models I've fitted using MCMCglmm. I immediately though about using
>> predict.MCMCglmm as
>>
>> predict(mymodel, type = "response", marginal = 0, interval = "prediction")
>>
>> However, predict returns the expectation (in fact one of its very last
>> lines have pred <- matrix(colMeans(post.pred), dim(post.pred)[2], 1)).
>>
>>
>> I can hack predict.MCMCglmm and directly return the "post.pred" object
>> which, IIUC, would give me the "y^{rep}" (in Gelman et al., 1996,
>> notation.).  But having to do this makes me wonder if I am understanding
>> this correctly.
>>
>> Is directly using the "post.pred" object the right way of getting the
>> y^{rep} with MCMCglmm?
>>
>>
>> Best,
>>
>>
>> R.
>>
>>
>>
>>
>> P.S. I am using "marginal = 0" as I want what, e.g., Green et al., 2009
>> ("Use of posterior predictive assessments to evaluate model fit in
>> multilevel logistic regression", Vet. Res, 40) call "full": "The predictive
>> distribution was conditional on all fixed effect and random effect
>> parameters estimated in the final model and a replicate observation
>> y_{ijk}^{full} generated from the conditional distribution (...)".
>>
>> --
>> Ramon Diaz-Uriarte
>> Department of Biochemistry, Lab B-25
>> Facultad de Medicina
>> Universidad Aut?noma de Madrid
>> Arzobispo Morcillo, 4
>> 28029 Madrid
>> Spain
>>
>> Phone: +34-91-497-2412
>>
>> Email: rdiaz02 at gmail.com
>>        ramon.diaz at iib.uam.es
>>
>> http://ligarto.org/rdiaz
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>

-- 
Ramon Diaz-Uriarte
Department of Biochemistry, Lab B-25
Facultad de Medicina 
Universidad Aut?noma de Madrid 
Arzobispo Morcillo, 4
28029 Madrid
Spain

Phone: +34-91-497-2412

Email: rdiaz02 at gmail.com
       ramon.diaz at iib.uam.es

http://ligarto.org/rdiaz


From emilia.ellsiepen at gmail.com  Mon Mar  3 10:47:15 2014
From: emilia.ellsiepen at gmail.com (Emilia Ellsiepen)
Date: Mon, 3 Mar 2014 10:47:15 +0100
Subject: [R-sig-ME] likelihood-ratio tests in conflict with coefficiants
 in maximal random effect model
In-Reply-To: <5310C77C.1090404@gmail.com>
References: <CAOU=wbkzKV2e2+k2q3B6rx+ANFzj7MDKtG4xnQ1vmUAfKF17uA@mail.gmail.com>
	<CAO7JsnTHhcExPyKMLBWeHRQbSDgr+HBx0PQ0bnAhBspiFk-YDA@mail.gmail.com>
	<5310C77C.1090404@gmail.com>
Message-ID: <CAOU=wb=3pb_evg8mR99=azq2xmqcV=JgOCGzEAs9_b8AyzucLg@mail.gmail.com>

2014-02-28 18:29 GMT+01:00 Ben Bolker <bbolker at gmail.com>:
> On 14-02-28 11:27 AM, Douglas Bates wrote:
>> On Fri, Feb 28, 2014 at 10:04 AM, Emilia Ellsiepen <
>> emilia.ellsiepen at gmail.com> wrote:
>>
>>> Dear list members,
>>>
>>> in analyzing a data set using lmers with maximal random effect
>>> structure and subsequent likelihood-ratio tests (LRTs) following Barr
>>> et al. 2013, I ran into the following problem: In some of the LRTs, it
>>> turned out that the simpler model (only main effects) has a higher
>>> likelihood than the more complicated model (including interaction),
>>> resulting in Chi=0. If I simplify the models by taking out the
>>> interactions in the two random effect terms, the LRT for the
>>> interaction has a highly significant result.>
>
> [snip]
>
>> It shows that the more complex model has not converged to the optimum
>> parameter values.  This can be because the optimizer being used is a bad
>> choice (in recent versions of the lme4 package the default was a
>> Nelder-Mead optimizer that can declare convergence to values that are not
>> the optimal values) or it can be because the model is too complex.  We say
>> that such models are over-parameterized.
>>
>> This is why the Barr et al. advice is dangerous.  In model selection there
>> are two basic strategies: forward and backward.  Forward selection starts
>> with a simple model and adds terms until they are no longer justified.
>>  Backward selection starts with the most complex model and drops terms.  It
>> is well known that backward selection is problematic when you can't fit the
>> most complex model.  Yet Barr et al. say unequivocally that you must use
>> backward selection.  The result will be that many researchers, especially
>> in linguistics, will encounter these problems of complex models providing
>> worse fits than simpler models.
>>
>> I wish that Barr et al. would have provided software that is guaranteed to
>> fit even the most complex model to a global optimum when they stated their
>> "one size fits all" strategy.  they didn't and those with experience in
>> numerical optimization can tell you why.  It is not possible to guarantee
>> convergence to an optimum in over-parameterized models.
>>
>
>   [snip]
>
>   I would be very interested to know whether the more thorough
> convergence tests that we have implemented in the development version of
> lme4 would correctly report that there are convergence problems with the
> model containing the maximal RE structure ...
>
>   Ben Bolker
>

Yes, it does! After installing the development version, I get the
following error message:

Warning messages:
1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 12.213 (tol = 0.001)
2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge: degenerate  Hessian with 3 negative eigenvalues

Thank you,
Emilia

-----------------------------------------
Emilia Ellsiepen
Institut f?r Linguistik
Goethe-Universit?t Frankfurt


From emilia.ellsiepen at gmail.com  Mon Mar  3 11:09:58 2014
From: emilia.ellsiepen at gmail.com (Emilia Ellsiepen)
Date: Mon, 3 Mar 2014 11:09:58 +0100
Subject: [R-sig-ME] likelihood-ratio tests in conflict with coefficiants
 in maximal random effect model
In-Reply-To: <CAO7JsnQAjaoivXbBW0PZgjxHpofTiK59RuobvXJwMaty3VHc7g@mail.gmail.com>
References: <CAOU=wbkzKV2e2+k2q3B6rx+ANFzj7MDKtG4xnQ1vmUAfKF17uA@mail.gmail.com>
	<CAO7JsnTHhcExPyKMLBWeHRQbSDgr+HBx0PQ0bnAhBspiFk-YDA@mail.gmail.com>
	<B536A3C0-1895-4FDB-B92B-FA3E11C0DCA2@ucsd.edu>
	<CAO7JsnQAjaoivXbBW0PZgjxHpofTiK59RuobvXJwMaty3VHc7g@mail.gmail.com>
Message-ID: <CAOU=wbmBEWBq6LV6uOz-C6RQ1cRVnHC9wcJQ_iSCMu8oi5LMFQ@mail.gmail.com>

Thank you for your feedback and this interesting discussion.


>> First off, it is not clear that Emilia's specific problem is being caused
>> by over-parameterization.  Emilia, could you perhaps give more information
>> about the nature of the dataset that you're analyzing?  Is it a 2x2
>> within-subjects, within-sentence balanced design without a great deal of
>> missing data?  In my experience with the last few pre-1.0 versions, lme4 is
>> generally very good at converging to an optimum for these kinds of datasets
>> with the number of observations and groups your fitted model reports.  Have
>> you tried fitting the model with the nlminb optimizer, either by including
>>
>> optimizer="optimx",optCtrl=list(method="nlminb")
>>
>> in the list of arguments to lmerControl, or by using the last pre-1.0
>> version of lme4 (available as lme4.0 on R-Forge)?  Do you still get similar
>> problems with the nlminb optimizer?  (You should definitely not get the
>> result that the simpler model has a higher log-likelihood.)

The design was a balanced 2x2 with-in subjects and with-in sentences
design without any missing data from a magnitude estimation
experiment.
When I use the nlminb optimizer (by installing the lme4.0 version), I
do get the interaction using the likelihood-ratio test, but I also get
the following warning message:

Warning message:
In mer_finalize(ans) : singular convergence (7)

I suppose that I should not be interpreting this model either and thus
will try to simplify the random effect structure.
Would you otherwise recomment to use whatever optimizer works best for
the data set in question? Shouldn't the optimizer be reported as well
then?

Emilia
-----------------------------------------
Emilia Ellsiepen
Institut f?r Linguistik
Goethe-Universit?t Frankfurt


From dhocking at wildcats.unh.edu  Sat Mar  1 02:10:46 2014
From: dhocking at wildcats.unh.edu (Daniel J Hocking)
Date: Sat, 1 Mar 2014 01:10:46 +0000
Subject: [R-sig-ME] namer self starting function
Message-ID: <58977E35-20EC-482D-819D-1F25B9B8E369@wildcats.unh.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140301/606b1423/attachment.pl>

From rlevy at ucsd.edu  Mon Mar  3 18:22:04 2014
From: rlevy at ucsd.edu (Levy, Roger)
Date: Mon, 3 Mar 2014 17:22:04 +0000
Subject: [R-sig-ME] likelihood-ratio tests in conflict with coefficiants
 in maximal random effect model
In-Reply-To: <CAOU=wbmcZBjD9OtoD105J3MK7VtWqJuovZWJbHYb=BBbTXP3Xg@mail.gmail.com>
References: <CAOU=wbkzKV2e2+k2q3B6rx+ANFzj7MDKtG4xnQ1vmUAfKF17uA@mail.gmail.com>
	<CAO7JsnTHhcExPyKMLBWeHRQbSDgr+HBx0PQ0bnAhBspiFk-YDA@mail.gmail.com>
	<B536A3C0-1895-4FDB-B92B-FA3E11C0DCA2@ucsd.edu>
	<CAO7JsnQAjaoivXbBW0PZgjxHpofTiK59RuobvXJwMaty3VHc7g@mail.gmail.com>
	<CAOU=wbmBEWBq6LV6uOz-C6RQ1cRVnHC9wcJQ_iSCMu8oi5LMFQ@mail.gmail.com>
	<A280C545-EF56-41E4-BAFD-82EA0B5C61A1@ucsd.edu>
	<CAOU=wbmcZBjD9OtoD105J3MK7VtWqJuovZWJbHYb=BBbTXP3Xg@mail.gmail.com>
Message-ID: <F368889F-E281-4A5D-A258-CD9D9A5ACDBF@ucsd.edu>

(cc-ing the list, which I forgot to do on the last response?)


On Mar 3, 2014, at 8:41 AM, Emilia Ellsiepen <emilia.ellsiepen at gmail.com> wrote:

> 
> 
> 2014-03-03 16:23 GMT+01:00 Levy, Roger <rlevy at ucsd.edu>:
>> 
>>> On Mar 3, 2014, at 3:09, "Emilia Ellsiepen" <emilia.ellsiepen at gmail.com> wrote:
>>> 
>>> Thank you for your feedback and this interesting discussion.
>>> 
>>> 
>>>>> First off, it is not clear that Emilia's specific problem is being caused
>>>>> by over-parameterization.  Emilia, could you perhaps give more information
>>>>> about the nature of the dataset that you're analyzing?  Is it a 2x2
>>>>> within-subjects, within-sentence balanced design without a great deal of
>>>>> missing data?  In my experience with the last few pre-1.0 versions, lme4 is
>>>>> generally very good at converging to an optimum for these kinds of datasets
>>>>> with the number of observations and groups your fitted model reports.  Have
>>>>> you tried fitting the model with the nlminb optimizer, either by including
>>>>> 
>>>>> optimizer="optimx",optCtrl=list(method="nlminb")
>>>>> 
>>>>> in the list of arguments to lmerControl, or by using the last pre-1.0
>>>>> version of lme4 (available as lme4.0 on R-Forge)?  Do you still get similar
>>>>> problems with the nlminb optimizer?  (You should definitely not get the
>>>>> result that the simpler model has a higher log-likelihood.)
>>> 
>>> The design was a balanced 2x2 with-in subjects and with-in sentences
>>> design without any missing data from a magnitude estimation
>>> experiment.
>>> When I use the nlminb optimizer (by installing the lme4.0 version), I
>>> do get the interaction using the likelihood-ratio test, but I also get
>>> the following warning message:
>>> 
>>> Warning message:
>>> In mer_finalize(ans) : singular convergence (7)
>> 
>> Thanks for this follow-up, Emilia! Question: do you get this warning when you fit the more complex model (with the fixed-effects interaction), or the null-hypothesis model (without the fixed-effects interaction)?

> That was for the more complex model with interaction. The one without
> interaction converged without problems.

Thanks Emilia.  Could you please give us a bit more information ? show us the fitted models (both the null and alternative-hypothesis models), and also the results of the call to anova()?

Best

Roger

From maechler at stat.math.ethz.ch  Tue Mar  4 10:42:32 2014
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 4 Mar 2014 10:42:32 +0100
Subject: [R-sig-ME] likelihood-ratio tests in conflict with coefficiants
 in maximal random effect model
In-Reply-To: <CAOU=wb=3pb_evg8mR99=azq2xmqcV=JgOCGzEAs9_b8AyzucLg@mail.gmail.com>
References: <CAOU=wbkzKV2e2+k2q3B6rx+ANFzj7MDKtG4xnQ1vmUAfKF17uA@mail.gmail.com>
	<CAO7JsnTHhcExPyKMLBWeHRQbSDgr+HBx0PQ0bnAhBspiFk-YDA@mail.gmail.com>
	<5310C77C.1090404@gmail.com>
	<CAOU=wb=3pb_evg8mR99=azq2xmqcV=JgOCGzEAs9_b8AyzucLg@mail.gmail.com>
Message-ID: <21269.40968.204628.304384@stat.math.ethz.ch>

>>>>> Emilia Ellsiepen <emilia.ellsiepen at gmail.com>
>>>>>     on Mon, 3 Mar 2014 10:47:15 +0100 writes:

    > 2014-02-28 18:29 GMT+01:00 Ben Bolker <bbolker at gmail.com>:
    >> On 14-02-28 11:27 AM, Douglas Bates wrote:
    >>> On Fri, Feb 28, 2014 at 10:04 AM, Emilia Ellsiepen <
    >>> emilia.ellsiepen at gmail.com> wrote:
    >>> 
    >>>> Dear list members,
    >>>> 
    >>>> in analyzing a data set using lmers with maximal random effect
    >>>> structure and subsequent likelihood-ratio tests (LRTs) following Barr
    >>>> et al. 2013, I ran into the following problem: In some of the LRTs, it
    >>>> turned out that the simpler model (only main effects) has a higher
    >>>> likelihood than the more complicated model (including interaction),
    >>>> resulting in Chi=0. If I simplify the models by taking out the
    >>>> interactions in the two random effect terms, the LRT for the
    >>>> interaction has a highly significant result.>
    >> 
    >> [snip]
    >> 
    >>> It shows that the more complex model has not converged to the optimum
    >>> parameter values.  This can be because the optimizer being used is a bad
    >>> choice (in recent versions of the lme4 package the default was a
    >>> Nelder-Mead optimizer that can declare convergence to values that are not
    >>> the optimal values) or it can be because the model is too complex.  We say
    >>> that such models are over-parameterized.
    >>> 
    >>> This is why the Barr et al. advice is dangerous.  In model selection there
    >>> are two basic strategies: forward and backward.  Forward selection starts
    >>> with a simple model and adds terms until they are no longer justified.
    >>> Backward selection starts with the most complex model and drops terms.  It
    >>> is well known that backward selection is problematic when you can't fit the
    >>> most complex model.  Yet Barr et al. say unequivocally that you must use
    >>> backward selection.  The result will be that many researchers, especially
    >>> in linguistics, will encounter these problems of complex models providing
    >>> worse fits than simpler models.
    >>> 
    >>> I wish that Barr et al. would have provided software that is guaranteed to
    >>> fit even the most complex model to a global optimum when they stated their
    >>> "one size fits all" strategy.  they didn't and those with experience in
    >>> numerical optimization can tell you why.  It is not possible to guarantee
    >>> convergence to an optimum in over-parameterized models.
    >>> 
    >> 
    >> [snip]
    >> 
    >> I would be very interested to know whether the more thorough
    >> convergence tests that we have implemented in the development version of
    >> lme4 would correctly report that there are convergence problems with the
    >> model containing the maximal RE structure ...
    >> 
    >> Ben Bolker
    >> 

    > Yes, it does! After installing the development version, I get the
    > following error message:

    > Warning messages:
    > 1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
    > Model failed to converge with max|grad| = 12.213 (tol = 0.001)
    > 2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
    > Model failed to converge: degenerate  Hessian with 3 negative eigenvalues

Aah, great!  
That's very good news.  Thank you, Emilia, for reporting here.
We (lme4 developers) might consider changing the warning into an
error in such an extreme case of non-convergence (12.2 compared
to 0.001). OTOH, good scientists would care about warnings.

So --- just for the readership who'll read this in the future:
Doug Bates was right on spot, and I just like to underline his
comments about the danger of doing science based on data
analysis which "blindly" follows strict 'cooking recipes' rules.
Such rules are something that should be discouraged (if taken strictly) 
rather than taught much more often.

Yes, it would be nice and satisfying if one could always do
backward selection in such cases, but life's reality is not
always nice.

Martin

--
Martin <Maechler at stat.math.ethz.ch>  http://stat.ethz.ch/people/maechler
Seminar f?r Statistik, ETH Z?rich  HG G 16      R?mistrasse 101
CH-8092 Zurich, SWITZERLAND
phone: +41-44-632-3408       fax: ...-1228      <><


From john.hodsoll at kcl.ac.uk  Tue Mar  4 12:25:17 2014
From: john.hodsoll at kcl.ac.uk (Hodsoll, John)
Date: Tue, 4 Mar 2014 11:25:17 +0000
Subject: [R-sig-ME] MCMCglmm variance estimates Poisson distribution
Message-ID: <fccb672635fe43ca80fee6325436cdd5@AM3PR03MB563.eurprd03.prod.outlook.com>

Dear list

I'm trying to get some unadjusted estimates and 95% CI for a set of correlated count data (due to repeated measures on the same cluster) . To do this I was trying to run an over-dispersed poisson model using a glmer and MCMCglmm. 

I want to use MCMCglmm as that's the package I wish to use for my main analysis. However, it seems to over-estimate the variance meaning that the mean value I get from the intercept only model y = XB + Var/2 (ch2 jarrod hadfield's course notes) is slightly greater than the actual mean. For example, if I fit the model 

priortr <- list(R=list(V=1, nu=0.001))

mcmc.c11.cf2 <- MCMCglmm(totflct ~ 1, family="poisson", prior = priortr, data=uc11,
                         nitt = 100000, burnin = 10000, thin = 90)
summary(mcmc.c11.cf2)

I'm ignoring the random effect and assuming the additive over-dispersion term will capture all the extra variance. For a count rate of 4.69 in the data I get 4.79 and for a count of 5.2 I get 5.52. On the other hand, if I use glmer including a per observation random effect I get the correct means

re.uc12.cf <- glmer(totflct ~ (1|obs), family=poisson, data=uc12)
summary(re.uc12.cf)? 

Is there something I missing here?

Regards
John Hodsoll


From j.hadfield at ed.ac.uk  Tue Mar  4 13:07:05 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Tue, 04 Mar 2014 12:07:05 +0000
Subject: [R-sig-ME] MCMCglmm variance estimates Poisson distribution
In-Reply-To: <fccb672635fe43ca80fee6325436cdd5@AM3PR03MB563.eurprd03.prod.outlook.com>
References: <fccb672635fe43ca80fee6325436cdd5@AM3PR03MB563.eurprd03.prod.outlook.com>
Message-ID: <20140304120705.21091rcjyfy7gs74@www.staffmail.ed.ac.uk>

Dear John,

How are you calculating the posterior expectation:

1/
posterior.mode(exp(mcmc.c11.cf2$Sol+mcmc.c11.cf2$VCV/2))
2/
mean(exp(mcmc.c11.cf2$Sol+mcmc.c11.cf2$VCV/2))
3/
exp(posterior.mode(mcmc.c11.cf2$Sol)+posterior.mode(mcmc.c11.cf2$VCV/2))
4/
exp(mean(mcmc.c11.cf2$Sol)+mean(mcmc.c11.cf2$VCV/2))

If it is not by method 1/ try that and see if there is less of a discrepancy.

Cheers,

Jarrod

Quoting "Hodsoll, John" <john.hodsoll at kcl.ac.uk> on Tue, 4 Mar 2014  
11:25:17 +0000:

> Dear list
>
> I'm trying to get some unadjusted estimates and 95% CI for a set of  
> correlated count data (due to repeated measures on the same cluster)  
> . To do this I was trying to run an over-dispersed poisson model  
> using a glmer and MCMCglmm.
>
> I want to use MCMCglmm as that's the package I wish to use for my  
> main analysis. However, it seems to over-estimate the variance  
> meaning that the mean value I get from the intercept only model y =  
> XB + Var/2 (ch2 jarrod hadfield's course notes) is slightly greater  
> than the actual mean. For example, if I fit the model
>
> priortr <- list(R=list(V=1, nu=0.001))
>
> mcmc.c11.cf2 <- MCMCglmm(totflct ~ 1, family="poisson", prior =  
> priortr, data=uc11,
>                          nitt = 100000, burnin = 10000, thin = 90)
> summary(mcmc.c11.cf2)
>
> I'm ignoring the random effect and assuming the additive  
> over-dispersion term will capture all the extra variance. For a  
> count rate of 4.69 in the data I get 4.79 and for a count of 5.2 I  
> get 5.52. On the other hand, if I use glmer including a per  
> observation random effect I get the correct means
>
> re.uc12.cf <- glmer(totflct ~ (1|obs), family=poisson, data=uc12)
> summary(re.uc12.cf)?
>
> Is there something I missing here?
>
> Regards
> John Hodsoll
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From john.hodsoll at kcl.ac.uk  Tue Mar  4 17:40:11 2014
From: john.hodsoll at kcl.ac.uk (Hodsoll, John)
Date: Tue, 4 Mar 2014 16:40:11 +0000
Subject: [R-sig-ME] MCMCglmm variance estimates Poisson distribution
In-Reply-To: <20140304120705.21091rcjyfy7gs74@www.staffmail.ed.ac.uk>
References: <fccb672635fe43ca80fee6325436cdd5@AM3PR03MB563.eurprd03.prod.outlook.com>
	<20140304120705.21091rcjyfy7gs74@www.staffmail.ed.ac.uk>
Message-ID: <01c73e1e649c4efea40846227960cde2@AM3PR03MB563.eurprd03.prod.outlook.com>

Dear Jarrod 

Thanks for your reply. I was using 4, but all give a similar answer

1. 4.796014

2. 4.792395

3. 4.798754

4  4.790677

As I said I'm not sure I'm missing something obvious?

Cheers
John


-----Original Message-----
From: Jarrod Hadfield [mailto:j.hadfield at ed.ac.uk] 
Sent: 04 March 2014 12:07
To: Hodsoll, John
Cc: 'r-sig-mixed-models at r-project.org'
Subject: Re: [R-sig-ME] MCMCglmm variance estimates Poisson distribution

Dear John,

How are you calculating the posterior expectation:

1/
posterior.mode(exp(mcmc.c11.cf2$Sol+mcmc.c11.cf2$VCV/2))
2/
mean(exp(mcmc.c11.cf2$Sol+mcmc.c11.cf2$VCV/2))
3/
exp(posterior.mode(mcmc.c11.cf2$Sol)+posterior.mode(mcmc.c11.cf2$VCV/2))
4/
exp(mean(mcmc.c11.cf2$Sol)+mean(mcmc.c11.cf2$VCV/2))

If it is not by method 1/ try that and see if there is less of a discrepancy.

Cheers,

Jarrod

Quoting "Hodsoll, John" <john.hodsoll at kcl.ac.uk> on Tue, 4 Mar 2014
11:25:17 +0000:

> Dear list
>
> I'm trying to get some unadjusted estimates and 95% CI for a set of 
> correlated count data (due to repeated measures on the same cluster) . 
> To do this I was trying to run an over-dispersed poisson model using a 
> glmer and MCMCglmm.
>
> I want to use MCMCglmm as that's the package I wish to use for my main 
> analysis. However, it seems to over-estimate the variance meaning that 
> the mean value I get from the intercept only model y = XB + Var/2 (ch2 
> jarrod hadfield's course notes) is slightly greater than the actual 
> mean. For example, if I fit the model
>
> priortr <- list(R=list(V=1, nu=0.001))
>
> mcmc.c11.cf2 <- MCMCglmm(totflct ~ 1, family="poisson", prior = 
> priortr, data=uc11,
>                          nitt = 100000, burnin = 10000, thin = 90)
> summary(mcmc.c11.cf2)
>
> I'm ignoring the random effect and assuming the additive 
> over-dispersion term will capture all the extra variance. For a count 
> rate of 4.69 in the data I get 4.79 and for a count of 5.2 I get 5.52. 
> On the other hand, if I use glmer including a per observation random 
> effect I get the correct means
>
> re.uc12.cf <- glmer(totflct ~ (1|obs), family=poisson, data=uc12)
> summary(re.uc12.cf)
>
> Is there something I missing here?
>
> Regards
> John Hodsoll
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



--
The University of Edinburgh is a charitable body, registered in Scotland, with registration number SC005336.


From j.hadfield at ed.ac.uk  Tue Mar  4 18:13:00 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Tue, 04 Mar 2014 17:13:00 +0000
Subject: [R-sig-ME] MCMCglmm variance estimates Poisson distribution
In-Reply-To: <01c73e1e649c4efea40846227960cde2@AM3PR03MB563.eurprd03.prod.outlook.com>
References: <fccb672635fe43ca80fee6325436cdd5@AM3PR03MB563.eurprd03.prod.outlook.com>
	<20140304120705.21091rcjyfy7gs74@www.staffmail.ed.ac.uk>
	<01c73e1e649c4efea40846227960cde2@AM3PR03MB563.eurprd03.prod.outlook.com>
Message-ID: <20140304171300.1566766p30l4llwk@www.staffmail.ed.ac.uk>

Hi John,

Perhaps the output from:

summary(mcmc.c11.cf2)

and

summary(re.uc12.cf)

will shed some light?  Also is there a reason that the data frames  
differ in each (uc11 and uc12)?

Failing something `obvious' then it must be the prior. How many  
observations is this based on?

Cheers,

Jarrod


Quoting "Hodsoll, John" <john.hodsoll at kcl.ac.uk> on Tue, 4 Mar 2014  
16:40:11 +0000:

> Dear Jarrod
>
> Thanks for your reply. I was using 4, but all give a similar answer
>
> 1. 4.796014
>
> 2. 4.792395
>
> 3. 4.798754
>
> 4  4.790677
>
> As I said I'm not sure I'm missing something obvious?
>
> Cheers
> John
>
>
> -----Original Message-----
> From: Jarrod Hadfield [mailto:j.hadfield at ed.ac.uk]
> Sent: 04 March 2014 12:07
> To: Hodsoll, John
> Cc: 'r-sig-mixed-models at r-project.org'
> Subject: Re: [R-sig-ME] MCMCglmm variance estimates Poisson distribution
>
> Dear John,
>
> How are you calculating the posterior expectation:
>
> 1/
> posterior.mode(exp(mcmc.c11.cf2$Sol+mcmc.c11.cf2$VCV/2))
> 2/
> mean(exp(mcmc.c11.cf2$Sol+mcmc.c11.cf2$VCV/2))
> 3/
> exp(posterior.mode(mcmc.c11.cf2$Sol)+posterior.mode(mcmc.c11.cf2$VCV/2))
> 4/
> exp(mean(mcmc.c11.cf2$Sol)+mean(mcmc.c11.cf2$VCV/2))
>
> If it is not by method 1/ try that and see if there is less of a discrepancy.
>
> Cheers,
>
> Jarrod
>
> Quoting "Hodsoll, John" <john.hodsoll at kcl.ac.uk> on Tue, 4 Mar 2014
> 11:25:17 +0000:
>
>> Dear list
>>
>> I'm trying to get some unadjusted estimates and 95% CI for a set of
>> correlated count data (due to repeated measures on the same cluster) .
>> To do this I was trying to run an over-dispersed poisson model using a
>> glmer and MCMCglmm.
>>
>> I want to use MCMCglmm as that's the package I wish to use for my main
>> analysis. However, it seems to over-estimate the variance meaning that
>> the mean value I get from the intercept only model y = XB + Var/2 (ch2
>> jarrod hadfield's course notes) is slightly greater than the actual
>> mean. For example, if I fit the model
>>
>> priortr <- list(R=list(V=1, nu=0.001))
>>
>> mcmc.c11.cf2 <- MCMCglmm(totflct ~ 1, family="poisson", prior =
>> priortr, data=uc11,
>>                          nitt = 100000, burnin = 10000, thin = 90)
>> summary(mcmc.c11.cf2)
>>
>> I'm ignoring the random effect and assuming the additive
>> over-dispersion term will capture all the extra variance. For a count
>> rate of 4.69 in the data I get 4.79 and for a count of 5.2 I get 5.52.
>> On the other hand, if I use glmer including a per observation random
>> effect I get the correct means
>>
>> re.uc12.cf <- glmer(totflct ~ (1|obs), family=poisson, data=uc12)
>> summary(re.uc12.cf)
>>
>> Is there something I missing here?
>>
>> Regards
>> John Hodsoll
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>
>
>
> --
> The University of Edinburgh is a charitable body, registered in  
> Scotland, with registration number SC005336.
>
>
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From vjd4 at nyu.edu  Tue Mar  4 18:20:41 2014
From: vjd4 at nyu.edu (Vincent Dorie)
Date: Tue, 4 Mar 2014 12:20:41 -0500
Subject: [R-sig-ME] Complete separation example
Message-ID: <95F3854F-F2A0-4AD6-B0BD-AE3B3CCEEBFF@nyu.edu>

Hi list, can anyone point me to a data set that exhibits complete or quasi-complete separation when fit as a glmm? It's for a vignette, so something public would be ideal.

With thanks,
Vince

From j.hadfield at ed.ac.uk  Tue Mar  4 18:28:41 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Tue, 04 Mar 2014 17:28:41 +0000
Subject: [R-sig-ME] Complete separation example
In-Reply-To: <95F3854F-F2A0-4AD6-B0BD-AE3B3CCEEBFF@nyu.edu>
References: <95F3854F-F2A0-4AD6-B0BD-AE3B3CCEEBFF@nyu.edu>
Message-ID: <20140304172841.21042migbdcbutq8@www.staffmail.ed.ac.uk>

Hi Vince,

Sex effect on horn type in Soay sheep is completely separated (the  
trait is partly sex-limited):

data(SShorns)
m1<-glm(I(horn=="polled")~sex, data=SShorns, family=binomial)

Cheers,

Jarrod


Quoting Vincent Dorie <vjd4 at nyu.edu> on Tue, 4 Mar 2014 12:20:41 -0500:

> Hi list, can anyone point me to a data set that exhibits complete or  
> quasi-complete separation when fit as a glmm? It's for a vignette,  
> so something public would be ideal.
>
> With thanks,
> Vince
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From rlevy at ucsd.edu  Tue Mar  4 18:39:01 2014
From: rlevy at ucsd.edu (Levy, Roger)
Date: Tue, 4 Mar 2014 17:39:01 +0000
Subject: [R-sig-ME] likelihood-ratio tests in conflict with coefficiants
 in maximal random effect model
In-Reply-To: <CAOU=wbn6e37tL0visOmkg+VBW0E483uB4xg6ve6XGWVU7FpqrQ@mail.gmail.com>
References: <CAOU=wbkzKV2e2+k2q3B6rx+ANFzj7MDKtG4xnQ1vmUAfKF17uA@mail.gmail.com>
	<CAO7JsnTHhcExPyKMLBWeHRQbSDgr+HBx0PQ0bnAhBspiFk-YDA@mail.gmail.com>
	<B536A3C0-1895-4FDB-B92B-FA3E11C0DCA2@ucsd.edu>
	<CAO7JsnQAjaoivXbBW0PZgjxHpofTiK59RuobvXJwMaty3VHc7g@mail.gmail.com>
	<CAOU=wbmBEWBq6LV6uOz-C6RQ1cRVnHC9wcJQ_iSCMu8oi5LMFQ@mail.gmail.com>
	<A280C545-EF56-41E4-BAFD-82EA0B5C61A1@ucsd.edu>
	<CAOU=wbmcZBjD9OtoD105J3MK7VtWqJuovZWJbHYb=BBbTXP3Xg@mail.gmail.com>
	<F368889F-E281-4A5D-A258-CD9D9A5ACDBF@ucsd.edu>
	<CAOU=wbn6e37tL0visOmkg+VBW0E483uB4xg6ve6XGWVU7FpqrQ@mail.gmail.com>
Message-ID: <4F4F6DEF-3651-497A-8447-3474FC0496F2@ucsd.edu>




On Mar 4, 2014, at 3:10 AM, Emilia Ellsiepen <emilia.ellsiepen at gmail.com> wrote:

> Thank you for your support, Roger - actually the problem vanished when
> I used sum-coding (after installing the packages and reloading the
> data, I accidentally skipped this step yesterday). So, in fact with
> the old lme4.0 version, the model did converge. I pasted in all model
> summaries and LRTs for dummy and sum coding below.
> Would you recommend to always use the old version then?

Thanks so much, Emilia, for following up.  This is very interesting.  Something doesn?t quite add up, though, in the model summaries and LRTs that you pasted below, because the log-likelihoods in the model summaries are not the same as the log-likelihoods reported in the results of anova().  I notice that you?re not using REML=F as an argument in your call to lmer().  Could you perhaps rerun all four model-fitting functions and the two anova() calls with REML=F, and show us the complete results?

Best & thanks again,

Roger


> *** with dummy coding:
> 
>> m1 = lmer(score ~ Order*Voice + (Order*Voice|subject)+(Order*Voice|sentence) ,rawdata)
> Warning message:
> In mer_finalize(ans) : singular convergence (7)
>> summary(m1)
> Linear mixed model fit by REML
> Formula: score ~ Order * Voice + (Order * Voice | subject) + (Order *
>    Voice | sentence)
>   Data: rawdata
>  AIC  BIC logLik deviance REMLdev
> 1626 1740 -787.9     1562    1576
> Random effects:
> Groups   Name                         Variance Std.Dev. Corr
> subject  (Intercept)                  0.097201 0.31177
>          OrderDat second              0.055057 0.23464  -0.690
>          VoicePassive                 0.161333 0.40166  -0.952  0.878
>          OrderDat second:VoicePassive 0.182879 0.42764   0.732 -0.986 -0.900
> sentence (Intercept)                  0.015671 0.12518
>          OrderDat second              0.068464 0.26166  -0.663
>          VoicePassive                 0.025271 0.15897  -0.237 -0.031
>          OrderDat second:VoicePassive 0.053708 0.23175   0.031 -0.589 -0.425
> Residual                              0.465541 0.68231
> Number of obs: 720, groups: subject, 36; sentence, 20
> 
> Fixed effects:
>                             Estimate Std. Error t value
> (Intercept)                   0.16341    0.07791   2.097
> OrderDat second              -0.01835    0.10062  -0.182
> VoicePassive                 -0.05604    0.10449  -0.536
> OrderDat second:VoicePassive  0.54250    0.13458   4.031
> 
> Correlation of Fixed Effects:
>            (Intr) OrdrDs VcPssv
> OrderDtscnd -0.647
> VoicePassiv -0.754  0.459
> OrdrDscn:VP  0.509 -0.717 -0.729
> 
>> mi = lmer(score ~ Order+Voice + (Order*Voice|subject)+(Order*Voice|sentence) ,rawdata)
>> summary(mi)
> Linear mixed model fit by REML
> Formula: score ~ Order + Voice + (Order * Voice | subject) + (Order *
>    Voice | sentence)
>   Data: rawdata
>  AIC  BIC logLik deviance REMLdev
> 1634 1744 -793.2     1574    1586
> Random effects:
> Groups   Name                         Variance Std.Dev. Corr
> subject  (Intercept)                  0.115077 0.33923
>          OrderDat second              0.094406 0.30726  -0.738
>          VoicePassive                 0.211833 0.46025  -0.951  0.893
>          OrderDat second:VoicePassive 0.330228 0.57465   0.761 -0.999 -0.902
> sentence (Intercept)                  0.019567 0.13988
>          OrderDat second              0.111327 0.33366  -0.814
>          VoicePassive                 0.048545 0.22033  -0.451  0.420
>          OrderDat second:VoicePassive 0.184456 0.42948   0.478 -0.771 -0.793
> Residual                              0.463180 0.68057
> Number of obs: 720, groups: subject, 36; sentence, 20
> 
> Fixed effects:
>                Estimate Std. Error t value
> (Intercept)     0.009676   0.066665   0.145
> OrderDat second 0.276029   0.069910   3.948
> VoicePassive    0.237470   0.071133   3.338
> 
> Correlation of Fixed Effects:
>            (Intr) OrdrDs
> OrderDtscnd -0.472
> VoicePassiv -0.631 -0.160
> 
>> anova(m1,mi)
> Data: rawdata
> Models:
> mi: score ~ Order + Voice + (Order * Voice | subject) + (Order *
> mi:     Voice | sentence)
> m1: score ~ Order * Voice + (Order * Voice | subject) + (Order *
> m1:     Voice | sentence)
>   Df    AIC    BIC  logLik  Chisq Chi Df Pr(>Chisq)
> mi 24 1622.5 1732.4 -787.24
> m1 25 1611.7 1726.2 -780.86 12.772      1  0.0003518 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> *** with sum-coding
> 
>> contrasts(rawdata$Order) = contr.sum(2)
>> contrasts(rawdata$Voice) = contr.sum(2)
>> m1 = lmer(score ~ Order*Voice + (Order*Voice|subject)+(Order*Voice|sentence) ,rawdata)
>> summary(m1)
> Linear mixed model fit by REML
> Formula: score ~ Order * Voice + (Order * Voice | subject) + (Order *
>    Voice | sentence)
>   Data: rawdata
>  AIC  BIC logLik deviance REMLdev
> 1631 1746 -790.7     1562    1581
> Random effects:
> Groups   Name          Variance   Std.Dev.  Corr
> subject  (Intercept)   1.9949e-02 0.1412424
>          Order1        1.4195e-05 0.0037676 -1.000
>          Voice1        1.3413e-02 0.1158136  0.833 -0.833
>          Order1:Voice1 1.1566e-02 0.1075434  0.286 -0.286  0.615
> sentence (Intercept)   2.9850e-03 0.0546352
>          Order1        1.1405e-02 0.1067934  0.257
>          Voice1        5.8322e-03 0.0763687  0.297 -0.438
>          Order1:Voice1 3.4893e-03 0.0590699 -0.884  0.182 -0.328
> Residual               4.6543e-01 0.6822234
> Number of obs: 720, groups: subject, 36; sentence, 20
> 
> Fixed effects:
>              Estimate Std. Error t value
> (Intercept)    0.26184    0.03674   7.127
> Order1        -0.12645    0.03489  -3.625
> Voice1        -0.10761    0.03620  -2.972
> Order1:Voice1  0.13562    0.03380   4.013
> 
> Correlation of Fixed Effects:
>            (Intr) Order1 Voice1
> Order1       0.047
> Voice1       0.331 -0.150
> Order1:Voc1 -0.018  0.046  0.113
>> mi = lmer(score ~ Order+Voice + (Order*Voice|subject)+(Order*Voice|sentence) ,rawdata)
>> summary(mi)
> Linear mixed model fit by REML
> Formula: score ~ Order + Voice + (Order * Voice | subject) + (Order *
>    Voice | sentence)
>   Data: rawdata
>  AIC  BIC logLik deviance REMLdev
> 1637 1747 -794.6     1575    1589
> Random effects:
> Groups   Name          Variance   Std.Dev.  Corr
> subject  (Intercept)   2.0497e-02 0.1431662
>          Order1        4.1101e-06 0.0020273 -1.000
>          Voice1        1.3959e-02 0.1181500  0.808 -0.808
>          Order1:Voice1 2.0579e-02 0.1434552  0.290 -0.290  0.540
> sentence (Intercept)   2.6765e-03 0.0517352
>          Order1        1.1696e-02 0.1081500  0.320
>          Voice1        5.0552e-03 0.0710997  0.275 -0.531
>          Order1:Voice1 1.1587e-02 0.1076410 -0.805  0.200 -0.293
> Residual               4.6331e-01 0.6806653
> Number of obs: 720, groups: subject, 36; sentence, 20
> 
> Fixed effects:
>            Estimate Std. Error t value
> (Intercept)  0.26626    0.03667   7.261
> Order1      -0.13508    0.03495  -3.865
> Voice1      -0.11835    0.03568  -3.317
> 
> Correlation of Fixed Effects:
>       (Intr) Order1
> Order1  0.066
> Voice1  0.332 -0.175
>> anova(m1,mi)
> Data: rawdata
> Models:
> mi: score ~ Order + Voice + (Order * Voice | subject) + (Order *
> mi:     Voice | sentence)
> m1: score ~ Order * Voice + (Order * Voice | subject) + (Order *
> m1:     Voice | sentence)
>   Df    AIC    BIC  logLik Chisq Chi Df Pr(>Chisq)
> mi 24 1622.6 1732.5 -787.29
> m1 25 1611.8 1726.3 -780.90 12.78      1  0.0003504 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> 
> 2014-03-03 18:22 GMT+01:00 Levy, Roger <rlevy at ucsd.edu>:
>> (cc-ing the list, which I forgot to do on the last response...)
>> 
>> 
>> On Mar 3, 2014, at 8:41 AM, Emilia Ellsiepen <emilia.ellsiepen at gmail.com> wrote:
>> 
>>> 
>>> 
>>> 2014-03-03 16:23 GMT+01:00 Levy, Roger <rlevy at ucsd.edu>:
>>>> 
>>>>> On Mar 3, 2014, at 3:09, "Emilia Ellsiepen" <emilia.ellsiepen at gmail.com> wrote:
>>>>> 
>>>>> Thank you for your feedback and this interesting discussion.
>>>>> 
>>>>> 
>>>>>>> First off, it is not clear that Emilia's specific problem is being caused
>>>>>>> by over-parameterization.  Emilia, could you perhaps give more information
>>>>>>> about the nature of the dataset that you're analyzing?  Is it a 2x2
>>>>>>> within-subjects, within-sentence balanced design without a great deal of
>>>>>>> missing data?  In my experience with the last few pre-1.0 versions, lme4 is
>>>>>>> generally very good at converging to an optimum for these kinds of datasets
>>>>>>> with the number of observations and groups your fitted model reports.  Have
>>>>>>> you tried fitting the model with the nlminb optimizer, either by including
>>>>>>> 
>>>>>>> optimizer="optimx",optCtrl=list(method="nlminb")
>>>>>>> 
>>>>>>> in the list of arguments to lmerControl, or by using the last pre-1.0
>>>>>>> version of lme4 (available as lme4.0 on R-Forge)?  Do you still get similar
>>>>>>> problems with the nlminb optimizer?  (You should definitely not get the
>>>>>>> result that the simpler model has a higher log-likelihood.)
>>>>> 
>>>>> The design was a balanced 2x2 with-in subjects and with-in sentences
>>>>> design without any missing data from a magnitude estimation
>>>>> experiment.
>>>>> When I use the nlminb optimizer (by installing the lme4.0 version), I
>>>>> do get the interaction using the likelihood-ratio test, but I also get
>>>>> the following warning message:
>>>>> 
>>>>> Warning message:
>>>>> In mer_finalize(ans) : singular convergence (7)
>>>> 
>>>> Thanks for this follow-up, Emilia! Question: do you get this warning when you fit the more complex model (with the fixed-effects interaction), or the null-hypothesis model (without the fixed-effects interaction)?
>> 
>>> That was for the more complex model with interaction. The one without
>>> interaction converged without problems.
>> 
>> Thanks Emilia.  Could you please give us a bit more information -- show us the fitted models (both the null and alternative-hypothesis models), and also the results of the call to anova()?
>> 
>> Best
>> 
>> Roger


From bbolker at gmail.com  Tue Mar  4 19:42:52 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 04 Mar 2014 13:42:52 -0500
Subject: [R-sig-ME] Complete separation example
In-Reply-To: <95F3854F-F2A0-4AD6-B0BD-AE3B3CCEEBFF@nyu.edu>
References: <95F3854F-F2A0-4AD6-B0BD-AE3B3CCEEBFF@nyu.edu>
Message-ID: <53161EAC.1080501@gmail.com>

On 14-03-04 12:20 PM, Vincent Dorie wrote:
> Hi list, can anyone point me to a data set that exhibits complete or
> quasi-complete separation when fit as a glmm? It's for a vignette, so
> something public would be ideal.
> 
> With thanks, Vince _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 

  I think the Culcita example referenced in
http://rpubs.com/bbolker/glmmchapter shows complete separation if one
observation (observation #20) is dropped.  But Jarrod's example may be
cleaner.

  Ben


From vjd4 at nyu.edu  Tue Mar  4 18:59:13 2014
From: vjd4 at nyu.edu (Vincent Dorie)
Date: Tue, 4 Mar 2014 12:59:13 -0500
Subject: [R-sig-ME] Complete separation example
In-Reply-To: <20140304172841.21042migbdcbutq8@www.staffmail.ed.ac.uk>
References: <95F3854F-F2A0-4AD6-B0BD-AE3B3CCEEBFF@nyu.edu>
	<20140304172841.21042migbdcbutq8@www.staffmail.ed.ac.uk>
Message-ID: <B244C650-47AA-4E59-B532-E808A05A9E00@nyu.edu>

Thanks Jarrod, but I'm specifically looking for something where a mixed model is appropriate. My googling has shown me that it happens often enough, but I haven't found much with available data.

On Mar 4, 2014, at 12:28 PM, Jarrod Hadfield wrote:

> Hi Vince,
> 
> Sex effect on horn type in Soay sheep is completely separated (the trait is partly sex-limited):
> 
> data(SShorns)
> m1<-glm(I(horn=="polled")~sex, data=SShorns, family=binomial)
> 
> Cheers,
> 
> Jarrod
> 
> 
> Quoting Vincent Dorie <vjd4 at nyu.edu> on Tue, 4 Mar 2014 12:20:41 -0500:
> 
>> Hi list, can anyone point me to a data set that exhibits complete or quasi-complete separation when fit as a glmm? It's for a vignette, so something public would be ideal.
>> 
>> With thanks,
>> Vince
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> 
>> 
> 
> 
> 
> -- 
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
> 
> 


From emilia.ellsiepen at gmail.com  Tue Mar  4 11:10:27 2014
From: emilia.ellsiepen at gmail.com (Emilia Ellsiepen)
Date: Tue, 4 Mar 2014 11:10:27 +0100
Subject: [R-sig-ME] likelihood-ratio tests in conflict with coefficiants
 in maximal random effect model
In-Reply-To: <F368889F-E281-4A5D-A258-CD9D9A5ACDBF@ucsd.edu>
References: <CAOU=wbkzKV2e2+k2q3B6rx+ANFzj7MDKtG4xnQ1vmUAfKF17uA@mail.gmail.com>
	<CAO7JsnTHhcExPyKMLBWeHRQbSDgr+HBx0PQ0bnAhBspiFk-YDA@mail.gmail.com>
	<B536A3C0-1895-4FDB-B92B-FA3E11C0DCA2@ucsd.edu>
	<CAO7JsnQAjaoivXbBW0PZgjxHpofTiK59RuobvXJwMaty3VHc7g@mail.gmail.com>
	<CAOU=wbmBEWBq6LV6uOz-C6RQ1cRVnHC9wcJQ_iSCMu8oi5LMFQ@mail.gmail.com>
	<A280C545-EF56-41E4-BAFD-82EA0B5C61A1@ucsd.edu>
	<CAOU=wbmcZBjD9OtoD105J3MK7VtWqJuovZWJbHYb=BBbTXP3Xg@mail.gmail.com>
	<F368889F-E281-4A5D-A258-CD9D9A5ACDBF@ucsd.edu>
Message-ID: <CAOU=wbn6e37tL0visOmkg+VBW0E483uB4xg6ve6XGWVU7FpqrQ@mail.gmail.com>

Thank you for your support, Roger - actually the problem vanished when
I used sum-coding (after installing the packages and reloading the
data, I accidentally skipped this step yesterday). So, in fact with
the old lme4.0 version, the model did converge. I pasted in all model
summaries and LRTs for dummy and sum coding below.
Would you recommend to always use the old version then?



*** with dummy coding:

> m1 = lmer(score ~ Order*Voice + (Order*Voice|subject)+(Order*Voice|sentence) ,rawdata)
Warning message:
In mer_finalize(ans) : singular convergence (7)
> summary(m1)
Linear mixed model fit by REML
Formula: score ~ Order * Voice + (Order * Voice | subject) + (Order *
    Voice | sentence)
   Data: rawdata
  AIC  BIC logLik deviance REMLdev
 1626 1740 -787.9     1562    1576
Random effects:
 Groups   Name                         Variance Std.Dev. Corr
 subject  (Intercept)                  0.097201 0.31177
          OrderDat second              0.055057 0.23464  -0.690
          VoicePassive                 0.161333 0.40166  -0.952  0.878
          OrderDat second:VoicePassive 0.182879 0.42764   0.732 -0.986 -0.900
 sentence (Intercept)                  0.015671 0.12518
          OrderDat second              0.068464 0.26166  -0.663
          VoicePassive                 0.025271 0.15897  -0.237 -0.031
          OrderDat second:VoicePassive 0.053708 0.23175   0.031 -0.589 -0.425
 Residual                              0.465541 0.68231
Number of obs: 720, groups: subject, 36; sentence, 20

Fixed effects:
                             Estimate Std. Error t value
(Intercept)                   0.16341    0.07791   2.097
OrderDat second              -0.01835    0.10062  -0.182
VoicePassive                 -0.05604    0.10449  -0.536
OrderDat second:VoicePassive  0.54250    0.13458   4.031

Correlation of Fixed Effects:
            (Intr) OrdrDs VcPssv
OrderDtscnd -0.647
VoicePassiv -0.754  0.459
OrdrDscn:VP  0.509 -0.717 -0.729

> mi = lmer(score ~ Order+Voice + (Order*Voice|subject)+(Order*Voice|sentence) ,rawdata)
> summary(mi)
Linear mixed model fit by REML
Formula: score ~ Order + Voice + (Order * Voice | subject) + (Order *
    Voice | sentence)
   Data: rawdata
  AIC  BIC logLik deviance REMLdev
 1634 1744 -793.2     1574    1586
Random effects:
 Groups   Name                         Variance Std.Dev. Corr
 subject  (Intercept)                  0.115077 0.33923
          OrderDat second              0.094406 0.30726  -0.738
          VoicePassive                 0.211833 0.46025  -0.951  0.893
          OrderDat second:VoicePassive 0.330228 0.57465   0.761 -0.999 -0.902
 sentence (Intercept)                  0.019567 0.13988
          OrderDat second              0.111327 0.33366  -0.814
          VoicePassive                 0.048545 0.22033  -0.451  0.420
          OrderDat second:VoicePassive 0.184456 0.42948   0.478 -0.771 -0.793
 Residual                              0.463180 0.68057
Number of obs: 720, groups: subject, 36; sentence, 20

Fixed effects:
                Estimate Std. Error t value
(Intercept)     0.009676   0.066665   0.145
OrderDat second 0.276029   0.069910   3.948
VoicePassive    0.237470   0.071133   3.338

Correlation of Fixed Effects:
            (Intr) OrdrDs
OrderDtscnd -0.472
VoicePassiv -0.631 -0.160

> anova(m1,mi)
Data: rawdata
Models:
mi: score ~ Order + Voice + (Order * Voice | subject) + (Order *
mi:     Voice | sentence)
m1: score ~ Order * Voice + (Order * Voice | subject) + (Order *
m1:     Voice | sentence)
   Df    AIC    BIC  logLik  Chisq Chi Df Pr(>Chisq)
mi 24 1622.5 1732.4 -787.24
m1 25 1611.7 1726.2 -780.86 12.772      1  0.0003518 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

*** with sum-coding

> contrasts(rawdata$Order) = contr.sum(2)
> contrasts(rawdata$Voice) = contr.sum(2)
> m1 = lmer(score ~ Order*Voice + (Order*Voice|subject)+(Order*Voice|sentence) ,rawdata)
> summary(m1)
Linear mixed model fit by REML
Formula: score ~ Order * Voice + (Order * Voice | subject) + (Order *
    Voice | sentence)
   Data: rawdata
  AIC  BIC logLik deviance REMLdev
 1631 1746 -790.7     1562    1581
Random effects:
 Groups   Name          Variance   Std.Dev.  Corr
 subject  (Intercept)   1.9949e-02 0.1412424
          Order1        1.4195e-05 0.0037676 -1.000
          Voice1        1.3413e-02 0.1158136  0.833 -0.833
          Order1:Voice1 1.1566e-02 0.1075434  0.286 -0.286  0.615
 sentence (Intercept)   2.9850e-03 0.0546352
          Order1        1.1405e-02 0.1067934  0.257
          Voice1        5.8322e-03 0.0763687  0.297 -0.438
          Order1:Voice1 3.4893e-03 0.0590699 -0.884  0.182 -0.328
 Residual               4.6543e-01 0.6822234
Number of obs: 720, groups: subject, 36; sentence, 20

Fixed effects:
              Estimate Std. Error t value
(Intercept)    0.26184    0.03674   7.127
Order1        -0.12645    0.03489  -3.625
Voice1        -0.10761    0.03620  -2.972
Order1:Voice1  0.13562    0.03380   4.013

Correlation of Fixed Effects:
            (Intr) Order1 Voice1
Order1       0.047
Voice1       0.331 -0.150
Order1:Voc1 -0.018  0.046  0.113
> mi = lmer(score ~ Order+Voice + (Order*Voice|subject)+(Order*Voice|sentence) ,rawdata)
> summary(mi)
Linear mixed model fit by REML
Formula: score ~ Order + Voice + (Order * Voice | subject) + (Order *
    Voice | sentence)
   Data: rawdata
  AIC  BIC logLik deviance REMLdev
 1637 1747 -794.6     1575    1589
Random effects:
 Groups   Name          Variance   Std.Dev.  Corr
 subject  (Intercept)   2.0497e-02 0.1431662
          Order1        4.1101e-06 0.0020273 -1.000
          Voice1        1.3959e-02 0.1181500  0.808 -0.808
          Order1:Voice1 2.0579e-02 0.1434552  0.290 -0.290  0.540
 sentence (Intercept)   2.6765e-03 0.0517352
          Order1        1.1696e-02 0.1081500  0.320
          Voice1        5.0552e-03 0.0710997  0.275 -0.531
          Order1:Voice1 1.1587e-02 0.1076410 -0.805  0.200 -0.293
 Residual               4.6331e-01 0.6806653
Number of obs: 720, groups: subject, 36; sentence, 20

Fixed effects:
            Estimate Std. Error t value
(Intercept)  0.26626    0.03667   7.261
Order1      -0.13508    0.03495  -3.865
Voice1      -0.11835    0.03568  -3.317

Correlation of Fixed Effects:
       (Intr) Order1
Order1  0.066
Voice1  0.332 -0.175
> anova(m1,mi)
Data: rawdata
Models:
mi: score ~ Order + Voice + (Order * Voice | subject) + (Order *
mi:     Voice | sentence)
m1: score ~ Order * Voice + (Order * Voice | subject) + (Order *
m1:     Voice | sentence)
   Df    AIC    BIC  logLik Chisq Chi Df Pr(>Chisq)
mi 24 1622.6 1732.5 -787.29
m1 25 1611.8 1726.3 -780.90 12.78      1  0.0003504 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1


2014-03-03 18:22 GMT+01:00 Levy, Roger <rlevy at ucsd.edu>:
> (cc-ing the list, which I forgot to do on the last response...)
>
>
> On Mar 3, 2014, at 8:41 AM, Emilia Ellsiepen <emilia.ellsiepen at gmail.com> wrote:
>
>>
>>
>> 2014-03-03 16:23 GMT+01:00 Levy, Roger <rlevy at ucsd.edu>:
>>>
>>>> On Mar 3, 2014, at 3:09, "Emilia Ellsiepen" <emilia.ellsiepen at gmail.com> wrote:
>>>>
>>>> Thank you for your feedback and this interesting discussion.
>>>>
>>>>
>>>>>> First off, it is not clear that Emilia's specific problem is being caused
>>>>>> by over-parameterization.  Emilia, could you perhaps give more information
>>>>>> about the nature of the dataset that you're analyzing?  Is it a 2x2
>>>>>> within-subjects, within-sentence balanced design without a great deal of
>>>>>> missing data?  In my experience with the last few pre-1.0 versions, lme4 is
>>>>>> generally very good at converging to an optimum for these kinds of datasets
>>>>>> with the number of observations and groups your fitted model reports.  Have
>>>>>> you tried fitting the model with the nlminb optimizer, either by including
>>>>>>
>>>>>> optimizer="optimx",optCtrl=list(method="nlminb")
>>>>>>
>>>>>> in the list of arguments to lmerControl, or by using the last pre-1.0
>>>>>> version of lme4 (available as lme4.0 on R-Forge)?  Do you still get similar
>>>>>> problems with the nlminb optimizer?  (You should definitely not get the
>>>>>> result that the simpler model has a higher log-likelihood.)
>>>>
>>>> The design was a balanced 2x2 with-in subjects and with-in sentences
>>>> design without any missing data from a magnitude estimation
>>>> experiment.
>>>> When I use the nlminb optimizer (by installing the lme4.0 version), I
>>>> do get the interaction using the likelihood-ratio test, but I also get
>>>> the following warning message:
>>>>
>>>> Warning message:
>>>> In mer_finalize(ans) : singular convergence (7)
>>>
>>> Thanks for this follow-up, Emilia! Question: do you get this warning when you fit the more complex model (with the fixed-effects interaction), or the null-hypothesis model (without the fixed-effects interaction)?
>
>> That was for the more complex model with interaction. The one without
>> interaction converged without problems.
>
> Thanks Emilia.  Could you please give us a bit more information -- show us the fitted models (both the null and alternative-hypothesis models), and also the results of the call to anova()?
>
> Best
>
> Roger


From john.hodsoll at kcl.ac.uk  Wed Mar  5 08:18:39 2014
From: john.hodsoll at kcl.ac.uk (Hodsoll, John)
Date: Wed, 5 Mar 2014 07:18:39 +0000
Subject: [R-sig-ME] MCMCglmm variance estimates Poisson distribution
In-Reply-To: <20140304171300.1566766p30l4llwk@www.staffmail.ed.ac.uk>
References: <fccb672635fe43ca80fee6325436cdd5@AM3PR03MB563.eurprd03.prod.outlook.com>
	<20140304120705.21091rcjyfy7gs74@www.staffmail.ed.ac.uk>
	<01c73e1e649c4efea40846227960cde2@AM3PR03MB563.eurprd03.prod.outlook.com>,
	<20140304171300.1566766p30l4llwk@www.staffmail.ed.ac.uk>
Message-ID: <2f4fcdab251347fcafab58f6d59dd9c7@AM3PR03MB563.eurprd03.prod.outlook.com>

Hi Jarrod 
 >  is there a reason that the data frames differ in each (uc11 and uc12)?
Yes. Two different baseline conditions, cut and paste error. 

With the same data frame

summary(mcmc.c11.cf2)
 Iterations = 10001:99911
 Thinning interval  = 90
 Sample size  = 1000 

 DIC: 7489.396 

 R-structure:  ~units

      post.mean l-95% CI u-95% CI eff.samp
units    0.7111   0.6276   0.7912     1000

 Location effects: totflct ~ 1 

            post.mean l-95% CI u-95% CI eff.samp  pMCMC    
(Intercept)     1.211    1.160    1.263     1000 <0.001 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 

> posterior.mode(exp(mcmc.c11.cf$Sol+mcmc.c11.cf$VCV/2))
(Intercept) 
     4.7921 

> exp(mean(mcmc.c11.cf2$Sol)+mean(mcmc.c11.cf2$VCV/2))
[1] 4.793908

And for glmer... 
Generalized linear mixed model fit by the Laplace approximation 
Formula: totflct ~ (1 | obs) 
   Data: uc11 
  AIC  BIC logLik deviance
 6205 6216  -3100     6201
Random effects:
 Groups Name        Variance Std.Dev.
 obs    (Intercept) 0.083672 0.28926 
Number of obs: 1607, groups: obs, 168

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  1.50622    0.02535   59.43   <2e-16 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 
__

for which exp(1.50622 + 0.083672/2) = 4.70232

I take your point re prior. Number of observations is 1607 so I thought this should be sufficient to limit the influence of the prior?

Cheers
John
______________________________________
From: Jarrod Hadfield <j.hadfield at ed.ac.uk>
Sent: 04 March 2014 17:13
To: Hodsoll, John
Cc: 'r-sig-mixed-models at r-project.org'
Subject: RE: [R-sig-ME] MCMCglmm variance estimates Poisson distribution

Hi John,

Perhaps the output from:

summary(mcmc.c11.cf2)

and

summary(re.uc12.cf)

will shed some light?  Also is there a reason that the data frames
differ in each (uc11 and uc12)?

Failing something `obvious' then it must be the prior. How many
observations is this based on?

Cheers,

Jarrod


Quoting "Hodsoll, John" <john.hodsoll at kcl.ac.uk> on Tue, 4 Mar 2014
16:40:11 +0000:

> Dear Jarrod
>
> Thanks for your reply. I was using 4, but all give a similar answer
>
> 1. 4.796014
>
> 2. 4.792395
>
> 3. 4.798754
>
> 4  4.790677
>
> As I said I'm not sure I'm missing something obvious?
>
> Cheers
> John
>
>
> -----Original Message-----
> From: Jarrod Hadfield [mailto:j.hadfield at ed.ac.uk]
> Sent: 04 March 2014 12:07
> To: Hodsoll, John
> Cc: 'r-sig-mixed-models at r-project.org'
> Subject: Re: [R-sig-ME] MCMCglmm variance estimates Poisson distribution
>
> Dear John,
>
> How are you calculating the posterior expectation:
>
> 1/
> posterior.mode(exp(mcmc.c11.cf2$Sol+mcmc.c11.cf2$VCV/2))
> 2/
> mean(exp(mcmc.c11.cf2$Sol+mcmc.c11.cf2$VCV/2))
> 3/
> exp(posterior.mode(mcmc.c11.cf2$Sol)+posterior.mode(mcmc.c11.cf2$VCV/2))
> 4/
> exp(mean(mcmc.c11.cf2$Sol)+mean(mcmc.c11.cf2$VCV/2))
>
> If it is not by method 1/ try that and see if there is less of a discrepancy.
>
> Cheers,
>
> Jarrod
>
> Quoting "Hodsoll, John" <john.hodsoll at kcl.ac.uk> on Tue, 4 Mar 2014
> 11:25:17 +0000:
>
>> Dear list
>>
>> I'm trying to get some unadjusted estimates and 95% CI for a set of
>> correlated count data (due to repeated measures on the same cluster) .
>> To do this I was trying to run an over-dispersed poisson model using a
>> glmer and MCMCglmm.
>>
>> I want to use MCMCglmm as that's the package I wish to use for my main
>> analysis. However, it seems to over-estimate the variance meaning that
>> the mean value I get from the intercept only model y = XB + Var/2 (ch2
>> jarrod hadfield's course notes) is slightly greater than the actual
>> mean. For example, if I fit the model
>>
>> priortr <- list(R=list(V=1, nu=0.001))
>>
>> mcmc.c11.cf2 <- MCMCglmm(totflct ~ 1, family="poisson", prior =
>> priortr, data=uc11,
>>                          nitt = 100000, burnin = 10000, thin = 90)
>> summary(mcmc.c11.cf2)
>>
>> I'm ignoring the random effect and assuming the additive
>> over-dispersion term will capture all the extra variance. For a count
>> rate of 4.69 in the data I get 4.79 and for a count of 5.2 I get 5.52.
>> On the other hand, if I use glmer including a per observation random
>> effect I get the correct means
>>
>> re.uc12.cf <- glmer(totflct ~ (1|obs), family=poisson, data=uc12)
>> summary(re.uc12.cf)
>>
>> Is there something I missing here?
>>
>> Regards
>> John Hodsoll
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>
>
>
> --
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
>
>
>
>



--
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.



From paul.tanger at colostate.edu  Wed Mar  5 08:24:13 2014
From: paul.tanger at colostate.edu (Paul Tanger)
Date: Wed, 5 Mar 2014 00:24:13 -0700
Subject: [R-sig-ME] plot observed vs fitted with model with NAs in data
Message-ID: <CAPEW02bk+Fn6uo9=3GVnPvosaAm60W6ugvr8cPyqyvaEUjOK6A@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140305/5b363e37/attachment.pl>

From j.hadfield at ed.ac.uk  Wed Mar  5 09:17:56 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Wed, 05 Mar 2014 08:17:56 +0000
Subject: [R-sig-ME] MCMCglmm variance estimates Poisson distribution
In-Reply-To: <2f4fcdab251347fcafab58f6d59dd9c7@AM3PR03MB563.eurprd03.prod.outlook.com>
References: <fccb672635fe43ca80fee6325436cdd5@AM3PR03MB563.eurprd03.prod.outlook.com>
	<20140304120705.21091rcjyfy7gs74@www.staffmail.ed.ac.uk>
	<01c73e1e649c4efea40846227960cde2@AM3PR03MB563.eurprd03.prod.outlook.com>,
	<20140304171300.1566766p30l4llwk@www.staffmail.ed.ac.uk>
	<2f4fcdab251347fcafab58f6d59dd9c7@AM3PR03MB563.eurprd03.prod.outlook.com>
Message-ID: <20140305081756.490468kgezamfg0s@www.staffmail.ed.ac.uk>

Hi John,

The two models are different. The obs variable in the lmer model does  
not have a unique level for each observation:

obs: 1607, groups: obs, 168

Cheers,

Jarrod


Quoting "Hodsoll, John" <john.hodsoll at kcl.ac.uk> on Wed, 5 Mar 2014  
07:18:39 +0000:

> Hi Jarrod
>  >  is there a reason that the data frames differ in each (uc11 and uc12)?
> Yes. Two different baseline conditions, cut and paste error.
>
> With the same data frame
>
> summary(mcmc.c11.cf2)
>  Iterations = 10001:99911
>  Thinning interval  = 90
>  Sample size  = 1000
>
>  DIC: 7489.396
>
>  R-structure:  ~units
>
>       post.mean l-95% CI u-95% CI eff.samp
> units    0.7111   0.6276   0.7912     1000
>
>  Location effects: totflct ~ 1
>
>             post.mean l-95% CI u-95% CI eff.samp  pMCMC
> (Intercept)     1.211    1.160    1.263     1000 <0.001 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
>> posterior.mode(exp(mcmc.c11.cf$Sol+mcmc.c11.cf$VCV/2))
> (Intercept)
>      4.7921
>
>> exp(mean(mcmc.c11.cf2$Sol)+mean(mcmc.c11.cf2$VCV/2))
> [1] 4.793908
>
> And for glmer...
> Generalized linear mixed model fit by the Laplace approximation
> Formula: totflct ~ (1 | obs)
>    Data: uc11
>   AIC  BIC logLik deviance
>  6205 6216  -3100     6201
> Random effects:
>  Groups Name        Variance Std.Dev.
>  obs    (Intercept) 0.083672 0.28926
> Number of obs: 1607, groups: obs, 168
>
> Fixed effects:
>             Estimate Std. Error z value Pr(>|z|)
> (Intercept)  1.50622    0.02535   59.43   <2e-16 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> __
>
> for which exp(1.50622 + 0.083672/2) = 4.70232
>
> I take your point re prior. Number of observations is 1607 so I  
> thought this should be sufficient to limit the influence of the prior?
>
> Cheers
> John
> ______________________________________
> From: Jarrod Hadfield <j.hadfield at ed.ac.uk>
> Sent: 04 March 2014 17:13
> To: Hodsoll, John
> Cc: 'r-sig-mixed-models at r-project.org'
> Subject: RE: [R-sig-ME] MCMCglmm variance estimates Poisson distribution
>
> Hi John,
>
> Perhaps the output from:
>
> summary(mcmc.c11.cf2)
>
> and
>
> summary(re.uc12.cf)
>
> will shed some light?  Also is there a reason that the data frames
> differ in each (uc11 and uc12)?
>
> Failing something `obvious' then it must be the prior. How many
> observations is this based on?
>
> Cheers,
>
> Jarrod
>
>
> Quoting "Hodsoll, John" <john.hodsoll at kcl.ac.uk> on Tue, 4 Mar 2014
> 16:40:11 +0000:
>
>> Dear Jarrod
>>
>> Thanks for your reply. I was using 4, but all give a similar answer
>>
>> 1. 4.796014
>>
>> 2. 4.792395
>>
>> 3. 4.798754
>>
>> 4  4.790677
>>
>> As I said I'm not sure I'm missing something obvious?
>>
>> Cheers
>> John
>>
>>
>> -----Original Message-----
>> From: Jarrod Hadfield [mailto:j.hadfield at ed.ac.uk]
>> Sent: 04 March 2014 12:07
>> To: Hodsoll, John
>> Cc: 'r-sig-mixed-models at r-project.org'
>> Subject: Re: [R-sig-ME] MCMCglmm variance estimates Poisson distribution
>>
>> Dear John,
>>
>> How are you calculating the posterior expectation:
>>
>> 1/
>> posterior.mode(exp(mcmc.c11.cf2$Sol+mcmc.c11.cf2$VCV/2))
>> 2/
>> mean(exp(mcmc.c11.cf2$Sol+mcmc.c11.cf2$VCV/2))
>> 3/
>> exp(posterior.mode(mcmc.c11.cf2$Sol)+posterior.mode(mcmc.c11.cf2$VCV/2))
>> 4/
>> exp(mean(mcmc.c11.cf2$Sol)+mean(mcmc.c11.cf2$VCV/2))
>>
>> If it is not by method 1/ try that and see if there is less of a  
>> discrepancy.
>>
>> Cheers,
>>
>> Jarrod
>>
>> Quoting "Hodsoll, John" <john.hodsoll at kcl.ac.uk> on Tue, 4 Mar 2014
>> 11:25:17 +0000:
>>
>>> Dear list
>>>
>>> I'm trying to get some unadjusted estimates and 95% CI for a set of
>>> correlated count data (due to repeated measures on the same cluster) .
>>> To do this I was trying to run an over-dispersed poisson model using a
>>> glmer and MCMCglmm.
>>>
>>> I want to use MCMCglmm as that's the package I wish to use for my main
>>> analysis. However, it seems to over-estimate the variance meaning that
>>> the mean value I get from the intercept only model y = XB + Var/2 (ch2
>>> jarrod hadfield's course notes) is slightly greater than the actual
>>> mean. For example, if I fit the model
>>>
>>> priortr <- list(R=list(V=1, nu=0.001))
>>>
>>> mcmc.c11.cf2 <- MCMCglmm(totflct ~ 1, family="poisson", prior =
>>> priortr, data=uc11,
>>>                          nitt = 100000, burnin = 10000, thin = 90)
>>> summary(mcmc.c11.cf2)
>>>
>>> I'm ignoring the random effect and assuming the additive
>>> over-dispersion term will capture all the extra variance. For a count
>>> rate of 4.69 in the data I get 4.79 and for a count of 5.2 I get 5.52.
>>> On the other hand, if I use glmer including a per observation random
>>> effect I get the correct means
>>>
>>> re.uc12.cf <- glmer(totflct ~ (1|obs), family=poisson, data=uc12)
>>> summary(re.uc12.cf)
>>>
>>> Is there something I missing here?
>>>
>>> Regards
>>> John Hodsoll
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>>
>>
>>
>>
>> --
>> The University of Edinburgh is a charitable body, registered in
>> Scotland, with registration number SC005336.
>>
>>
>>
>>
>
>
>
> --
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
>
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From Thierry.ONKELINX at inbo.be  Wed Mar  5 09:24:44 2014
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Wed, 5 Mar 2014 08:24:44 +0000
Subject: [R-sig-ME] plot observed vs fitted with model with NAs in data
In-Reply-To: <CAPEW02bk+Fn6uo9=3GVnPvosaAm60W6ugvr8cPyqyvaEUjOK6A@mail.gmail.com>
References: <CAPEW02bk+Fn6uo9=3GVnPvosaAm60W6ugvr8cPyqyvaEUjOK6A@mail.gmail.com>
Message-ID: <AA818EAD2576BC488B4F623941DA7427F3A11D25@inbomail.inbo.be>

You probably need to fit your model with the na.action = na.exclude argument. A more detailed answer requires more information. Have a look at http://www.r-project.org/posting-guide.html

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Paul Tanger
Verzonden: woensdag 5 maart 2014 8:24
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] plot observed vs fitted with model with NAs in data

Hi,
I have a model that I fit from data with some NAs in it.. everything is going well until I try to plot observed vs fitted.. and I can figure out how to fix this.. any ideas?  I can see that there are 1049 observed and
1050 fitted.. but can I resolve this without fitting new models after discarding NAs?  I would really rather not do that.

>plot(model, resp~ fitted(.))
Error in `[<-.data.frame`(`*tmp*`, , ".y", value = c(130.983333333333,  :
  replacement has 1050 rows, data has 1049

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From john.hodsoll at kcl.ac.uk  Wed Mar  5 11:13:57 2014
From: john.hodsoll at kcl.ac.uk (Hodsoll, John)
Date: Wed, 5 Mar 2014 10:13:57 +0000
Subject: [R-sig-ME] MCMCglmm variance estimates Poisson distribution
In-Reply-To: <20140305081756.490468kgezamfg0s@www.staffmail.ed.ac.uk>
References: <fccb672635fe43ca80fee6325436cdd5@AM3PR03MB563.eurprd03.prod.outlook.com>
	<20140304120705.21091rcjyfy7gs74@www.staffmail.ed.ac.uk>
	<01c73e1e649c4efea40846227960cde2@AM3PR03MB563.eurprd03.prod.outlook.com>,
	<20140304171300.1566766p30l4llwk@www.staffmail.ed.ac.uk>
	<2f4fcdab251347fcafab58f6d59dd9c7@AM3PR03MB563.eurprd03.prod.outlook.com>
	<20140305081756.490468kgezamfg0s@www.staffmail.ed.ac.uk>
Message-ID: <b6fe5f741b6a4828b2c5f973a610acaa@AM3PR03MB563.eurprd03.prod.outlook.com>

Hi Jarrod

Ah ok, thanks so if I run 

re.uc11.cf <- glmer(totflct ~ (1|iobs), family=poisson, data=uc11)
summary(re.uc11.cf)

with a term for each observation I get

Generalized linear mixed model fit by the Laplace approximation 
Formula: totflct ~ (1 | iobs) 
   Data: uc11 
  AIC  BIC logLik deviance
 3960 3971  -1978     3956
Random effects:
 Groups Name        Variance Std.Dev.
 iobs   (Intercept) 0.69221  0.83199 
Number of obs: 1607, groups: iobs, 1607

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)   1.2133     0.0249   48.74   <2e-16 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

which corresponds to the estimates from the MCMC model. 

My problem is now that the estimate it doesn't match the data (mean 4.69), but at least the models are in agreement.

Thanks
John

-----Original Message-----
From: Jarrod Hadfield [mailto:j.hadfield at ed.ac.uk] 
Sent: 05 March 2014 08:18
To: Hodsoll, John
Cc: r-sig-mixed-models at r-project.org
Subject: RE: [R-sig-ME] MCMCglmm variance estimates Poisson distribution

Hi John,

The two models are different. The obs variable in the lmer model does not have a unique level for each observation:

obs: 1607, groups: obs, 168

Cheers,

Jarrod


Quoting "Hodsoll, John" <john.hodsoll at kcl.ac.uk> on Wed, 5 Mar 2014
07:18:39 +0000:

> Hi Jarrod
>  >  is there a reason that the data frames differ in each (uc11 and uc12)?
> Yes. Two different baseline conditions, cut and paste error.
>
> With the same data frame
>
> summary(mcmc.c11.cf2)
>  Iterations = 10001:99911
>  Thinning interval  = 90
>  Sample size  = 1000
>
>  DIC: 7489.396
>
>  R-structure:  ~units
>
>       post.mean l-95% CI u-95% CI eff.samp
> units    0.7111   0.6276   0.7912     1000
>
>  Location effects: totflct ~ 1
>
>             post.mean l-95% CI u-95% CI eff.samp  pMCMC
> (Intercept)     1.211    1.160    1.263     1000 <0.001 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
>> posterior.mode(exp(mcmc.c11.cf$Sol+mcmc.c11.cf$VCV/2))
> (Intercept)
>      4.7921
>
>> exp(mean(mcmc.c11.cf2$Sol)+mean(mcmc.c11.cf2$VCV/2))
> [1] 4.793908
>
> And for glmer...
> Generalized linear mixed model fit by the Laplace approximation
> Formula: totflct ~ (1 | obs)
>    Data: uc11
>   AIC  BIC logLik deviance
>  6205 6216  -3100     6201
> Random effects:
>  Groups Name        Variance Std.Dev.
>  obs    (Intercept) 0.083672 0.28926
> Number of obs: 1607, groups: obs, 168
>
> Fixed effects:
>             Estimate Std. Error z value Pr(>|z|)
> (Intercept)  1.50622    0.02535   59.43   <2e-16 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 __
>
> for which exp(1.50622 + 0.083672/2) = 4.70232
>
> I take your point re prior. Number of observations is 1607 so I 
> thought this should be sufficient to limit the influence of the prior?
>
> Cheers
> John
> ______________________________________
> From: Jarrod Hadfield <j.hadfield at ed.ac.uk>
> Sent: 04 March 2014 17:13
> To: Hodsoll, John
> Cc: 'r-sig-mixed-models at r-project.org'
> Subject: RE: [R-sig-ME] MCMCglmm variance estimates Poisson 
> distribution
>
> Hi John,
>
> Perhaps the output from:
>
> summary(mcmc.c11.cf2)
>
> and
>
> summary(re.uc12.cf)
>
> will shed some light?  Also is there a reason that the data frames 
> differ in each (uc11 and uc12)?
>
> Failing something `obvious' then it must be the prior. How many 
> observations is this based on?
>
> Cheers,
>
> Jarrod
>
>
> Quoting "Hodsoll, John" <john.hodsoll at kcl.ac.uk> on Tue, 4 Mar 2014
> 16:40:11 +0000:
>
>> Dear Jarrod
>>
>> Thanks for your reply. I was using 4, but all give a similar answer
>>
>> 1. 4.796014
>>
>> 2. 4.792395
>>
>> 3. 4.798754
>>
>> 4  4.790677
>>
>> As I said I'm not sure I'm missing something obvious?
>>
>> Cheers
>> John
>>
>>
>> -----Original Message-----
>> From: Jarrod Hadfield [mailto:j.hadfield at ed.ac.uk]
>> Sent: 04 March 2014 12:07
>> To: Hodsoll, John
>> Cc: 'r-sig-mixed-models at r-project.org'
>> Subject: Re: [R-sig-ME] MCMCglmm variance estimates Poisson 
>> distribution
>>
>> Dear John,
>>
>> How are you calculating the posterior expectation:
>>
>> 1/
>> posterior.mode(exp(mcmc.c11.cf2$Sol+mcmc.c11.cf2$VCV/2))
>> 2/
>> mean(exp(mcmc.c11.cf2$Sol+mcmc.c11.cf2$VCV/2))
>> 3/
>> exp(posterior.mode(mcmc.c11.cf2$Sol)+posterior.mode(mcmc.c11.cf2$VCV/
>> 2))
>> 4/
>> exp(mean(mcmc.c11.cf2$Sol)+mean(mcmc.c11.cf2$VCV/2))
>>
>> If it is not by method 1/ try that and see if there is less of a 
>> discrepancy.
>>
>> Cheers,
>>
>> Jarrod
>>
>> Quoting "Hodsoll, John" <john.hodsoll at kcl.ac.uk> on Tue, 4 Mar 2014
>> 11:25:17 +0000:
>>
>>> Dear list
>>>
>>> I'm trying to get some unadjusted estimates and 95% CI for a set of 
>>> correlated count data (due to repeated measures on the same cluster) .
>>> To do this I was trying to run an over-dispersed poisson model using 
>>> a glmer and MCMCglmm.
>>>
>>> I want to use MCMCglmm as that's the package I wish to use for my 
>>> main analysis. However, it seems to over-estimate the variance 
>>> meaning that the mean value I get from the intercept only model y = 
>>> XB + Var/2 (ch2 jarrod hadfield's course notes) is slightly greater 
>>> than the actual mean. For example, if I fit the model
>>>
>>> priortr <- list(R=list(V=1, nu=0.001))
>>>
>>> mcmc.c11.cf2 <- MCMCglmm(totflct ~ 1, family="poisson", prior = 
>>> priortr, data=uc11,
>>>                          nitt = 100000, burnin = 10000, thin = 90)
>>> summary(mcmc.c11.cf2)
>>>
>>> I'm ignoring the random effect and assuming the additive 
>>> over-dispersion term will capture all the extra variance. For a 
>>> count rate of 4.69 in the data I get 4.79 and for a count of 5.2 I get 5.52.
>>> On the other hand, if I use glmer including a per observation random 
>>> effect I get the correct means
>>>
>>> re.uc12.cf <- glmer(totflct ~ (1|obs), family=poisson, data=uc12)
>>> summary(re.uc12.cf)
>>>
>>> Is there something I missing here?
>>>
>>> Regards
>>> John Hodsoll
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list 
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>>
>>
>>
>>
>> --
>> The University of Edinburgh is a charitable body, registered in 
>> Scotland, with registration number SC005336.
>>
>>
>>
>>
>
>
>
> --
> The University of Edinburgh is a charitable body, registered in 
> Scotland, with registration number SC005336.
>
>
>



--
The University of Edinburgh is a charitable body, registered in Scotland, with registration number SC005336.



From paul.tanger at colostate.edu  Wed Mar  5 16:40:35 2014
From: paul.tanger at colostate.edu (Paul Tanger)
Date: Wed, 5 Mar 2014 08:40:35 -0700
Subject: [R-sig-ME] plot observed vs fitted with model with NAs in data
In-Reply-To: <AA818EAD2576BC488B4F623941DA7427F3A11D25@inbomail.inbo.be>
References: <CAPEW02bk+Fn6uo9=3GVnPvosaAm60W6ugvr8cPyqyvaEUjOK6A@mail.gmail.com>
	<AA818EAD2576BC488B4F623941DA7427F3A11D25@inbomail.inbo.be>
Message-ID: <CAPEW02ZF8EtSVyikKAy20FLYS-QHDRZP6FnavP2A1h3JtaNkiA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140305/89954337/attachment.pl>

From bbolker at gmail.com  Wed Mar  5 16:50:24 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 05 Mar 2014 10:50:24 -0500
Subject: [R-sig-ME] plot observed vs fitted with model with NAs in data
In-Reply-To: <CAPEW02ZF8EtSVyikKAy20FLYS-QHDRZP6FnavP2A1h3JtaNkiA@mail.gmail.com>
References: <CAPEW02bk+Fn6uo9=3GVnPvosaAm60W6ugvr8cPyqyvaEUjOK6A@mail.gmail.com>	<AA818EAD2576BC488B4F623941DA7427F3A11D25@inbomail.inbo.be>
	<CAPEW02ZF8EtSVyikKAy20FLYS-QHDRZP6FnavP2A1h3JtaNkiA@mail.gmail.com>
Message-ID: <531747C0.9060606@gmail.com>


  I don't see 'na.action' in your code anywhere.

  This works for me:

mymodel = lmer(hp ~ (1| disp) + (1|cyl), data=mydata,
    na.action=na.exclude)
plot(mymodel, hp ~ fitted(.))

  **However**, the real answer is this item from
https://github.com/lme4/lme4/blob/master/inst/NEWS.Rd, under BUG FIXES
for version 1.1-3:

\item \code{fitted} now respects \code{na.action} settings (Github
      issue #149)

We are hoping to release a new version to CRAN very soon, but in the
meantime version 1.1-4 is available at
http://lme4.r-forge.r-project.org/repos , or from github ...

  Ben Bolker


On 14-03-05 10:40 AM, Paul Tanger wrote:
> Thanks for that tip but it didn't seem to help.  I get the same error.  Is
> there a way to tell it not to predict a value for an NA?  Here is a
> reproducible example:
> 
> data(mtcars)
> mydata = mtcars[2:8]
> # insert some NAs
> mydata$hp = apply(mydata[2], 2, function(x) {x[sample(c(1:length(x)),
> floor(length(x)/10)) ] = NA; x} )
> # run model
> mymodel = lmer(hp ~ (1| disp) + (1|cyl), data=mydata)
> # try to plot obs vs fitted
> plot(mymodel, hp ~ fitted(.))
> 
> Note that whatever na.action I attempt, the error remains.
> Maybe there is a simple solution that I'm missing?
> 
> Thanks!
> 
> 
> On Wed, Mar 5, 2014 at 1:24 AM, ONKELINX, Thierry
> <Thierry.ONKELINX at inbo.be>wrote:
> 
>> You probably need to fit your model with the na.action = na.exclude
>> argument. A more detailed answer requires more information. Have a look at
>> http://www.r-project.org/posting-guide.html
>>
>> ir. Thierry Onkelinx
>> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
>> Forest
>> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
>> Kliniekstraat 25
>> 1070 Anderlecht
>> Belgium
>> + 32 2 525 02 51
>> + 32 54 43 61 85
>> Thierry.Onkelinx at inbo.be
>> www.inbo.be
>>
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to say
>> what the experiment died of.
>> ~ Sir Ronald Aylmer Fisher
>>
>> The plural of anecdote is not data.
>> ~ Roger Brinner
>>
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of data.
>> ~ John Tukey
>>
>> -----Oorspronkelijk bericht-----
>> Van: r-sig-mixed-models-bounces at r-project.org [mailto:
>> r-sig-mixed-models-bounces at r-project.org] Namens Paul Tanger
>> Verzonden: woensdag 5 maart 2014 8:24
>> Aan: r-sig-mixed-models at r-project.org
>> Onderwerp: [R-sig-ME] plot observed vs fitted with model with NAs in data
>>
>> Hi,
>> I have a model that I fit from data with some NAs in it.. everything is
>> going well until I try to plot observed vs fitted.. and I can figure out
>> how to fix this.. any ideas?  I can see that there are 1049 observed and
>> 1050 fitted.. but can I resolve this without fitting new models after
>> discarding NAs?  I would really rather not do that.
>>
>>> plot(model, resp~ fitted(.))
>> Error in `[<-.data.frame`(`*tmp*`, , ".y", value = c(130.983333333333,  :
>>   replacement has 1050 rows, data has 1049
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> * * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
>> Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver
>> weer en binden het INBO onder geen enkel beding, zolang dit bericht niet
>> bevestigd is door een geldig ondertekend document.
>> The views expressed in this message and any annex are purely those of the
>> writer and may not be regarded as stating an official position of INBO, as
>> long as the message is not confirmed by a duly signed document.
>>
> 
> 
>


From rlevy at ucsd.edu  Wed Mar  5 17:25:58 2014
From: rlevy at ucsd.edu (Levy, Roger)
Date: Wed, 5 Mar 2014 16:25:58 +0000
Subject: [R-sig-ME] likelihood-ratio tests in conflict with coefficiants
 in maximal random effect model
In-Reply-To: <CAOU=wbm32QG-FR5QZYFRMAU+Zym=uh79fVNeE+WGQ3x5=XKFFQ@mail.gmail.com>
References: <CAOU=wbkzKV2e2+k2q3B6rx+ANFzj7MDKtG4xnQ1vmUAfKF17uA@mail.gmail.com>
	<CAO7JsnTHhcExPyKMLBWeHRQbSDgr+HBx0PQ0bnAhBspiFk-YDA@mail.gmail.com>
	<B536A3C0-1895-4FDB-B92B-FA3E11C0DCA2@ucsd.edu>
	<CAO7JsnQAjaoivXbBW0PZgjxHpofTiK59RuobvXJwMaty3VHc7g@mail.gmail.com>
	<CAOU=wbmBEWBq6LV6uOz-C6RQ1cRVnHC9wcJQ_iSCMu8oi5LMFQ@mail.gmail.com>
	<A280C545-EF56-41E4-BAFD-82EA0B5C61A1@ucsd.edu>
	<CAOU=wbmcZBjD9OtoD105J3MK7VtWqJuovZWJbHYb=BBbTXP3Xg@mail.gmail.com>
	<F368889F-E281-4A5D-A258-CD9D9A5ACDBF@ucsd.edu>
	<CAOU=wbn6e37tL0visOmkg+VBW0E483uB4xg6ve6XGWVU7FpqrQ@mail.gmail.com>
	<4F4F6DEF-3651-497A-8447-3474FC0496F2@ucsd.edu>
	<CAOU=wbm32QG-FR5QZYFRMAU+Zym=uh79fVNeE+WGQ3x5=XKFFQ@mail.gmail.com>
Message-ID: <0C585488-74FE-40F2-BBCF-05FB6CEEE40D@ucsd.edu>


On Mar 5, 2014, at 3:15 AM, Emilia Ellsiepen <emilia.ellsiepen at gmail.com> wrote:

> 2014-03-04 18:39 GMT+01:00 Levy, Roger <rlevy at ucsd.edu>:
>> 
>> 
>> 
>> On Mar 4, 2014, at 3:10 AM, Emilia Ellsiepen <emilia.ellsiepen at gmail.com> wrote:
>> 
>>> Thank you for your support, Roger - actually the problem vanished when
>>> I used sum-coding (after installing the packages and reloading the
>>> data, I accidentally skipped this step yesterday). So, in fact with
>>> the old lme4.0 version, the model did converge. I pasted in all model
>>> summaries and LRTs for dummy and sum coding below.
>>> Would you recommend to always use the old version then?
>> 
>> Thanks so much, Emilia, for following up.  This is very interesting.  Something doesn't quite add up, though, in the model summaries and LRTs that you pasted below, because the log-likelihoods in the model summaries are not the same as the log-likelihoods reported in the results of anova().  I notice that you're not using REML=F as an argument in your call to lmer().  Could you perhaps rerun all four model-fitting functions and the two anova() calls with REML=F, and show us the complete results?
>> 
>> Best & thanks again,
>> 
>> Roger
>> 
> Rerunning with REML=F led to a singular convergence in the
> null-hypothesis model with dummy coding. The sum coded variant still
> converges without warnings. While the two LRTs give in general the
> same result for both coding schemes, as I would have expected, there
> are minimal differences in the logLik, AIC and BIC values.

Thanks so much once again, Emilia, for continuing to follow up.  These results clear everything up.  As you note, there are now only small differences between the log-likelihoods of equivalent models with the two different contrast codings.  (You could probably get the models to agree even more by tightening the tolerance criterion for convergence.)  Note also that the likelihood-ratio test under either parameterization gives you a result highly consistent with the t-statistic results for the maximal model, addressing one of the concerns that you originally voiced.  I don?t see any evidence that the maximal model is overparameterized (though I would be happy to hear any arguments to the contrary from other list users!).  So I would trust the LRT results you?re seeing now ? that the interaction you care about is highly significant.

It would be worthwhile to check whether you get the same results running the same code using the newest version of lme4 with the nlminb optimizer ? and I?m sure a number of list members would be interested in knowing!

Best and I?m glad that we?ve gotten to the bottom of things!

Roger



> Here are the results in detail:
> **** dummy coding:
> 
>> m1 = lmer(score ~ Order*Voice + (Order*Voice|subject)+(Order*Voice|sentence), REML=F ,rawdata)
>> summary(m1)
> Linear mixed model fit by maximum likelihood
> Formula: score ~ Order * Voice + (Order * Voice | subject) + (Order *
> Voice | sentence)
>   Data: rawdata
>  AIC  BIC logLik deviance REMLdev
> 1612 1726 -780.8     1562    1576
> Random effects:
> Groups   Name                         Variance Std.Dev. Corr
> subject  (Intercept)                  0.092788 0.30461
>          OrderDat second              0.051950 0.22793  -0.689
>          VoicePassive                 0.152335 0.39030  -0.954  0.875
>          OrderDat second:VoicePassive 0.171303 0.41389   0.736 -0.991 -0.903
> sentence (Intercept)                  0.013700 0.11705
>          OrderDat second              0.060511 0.24599  -0.652
>          VoicePassive                 0.021227 0.14570  -0.215 -0.067
>          OrderDat second:VoicePassive 0.045796 0.21400   0.016 -0.587 -0.404
> Residual                              0.466165 0.68276
> Number of obs: 720, groups: subject, 36; sentence, 20
> 
> Fixed effects:
>                             Estimate Std. Error t value
> (Intercept)                   0.16341    0.07650   2.136
> OrderDat second              -0.01835    0.09823  -0.187
> VoicePassive                 -0.05604    0.10234  -0.548
> OrderDat second:VoicePassive  0.54250    0.13194   4.112
> 
> Correlation of Fixed Effects:
>            (Intr) OrdrDs VcPssv
> OrderDtscnd -0.646
> VoicePassiv -0.757  0.461
> OrdrDscn:VP  0.514 -0.719 -0.730
> 
>> mi = lmer(score ~ Order+Voice + (Order*Voice|subject)+(Order*Voice|sentence) ,REML=F,rawdata)
> Warning message:
> In mer_finalize(ans) : singular convergence (7)
>> summary(mi)
> Linear mixed model fit by maximum likelihood
> Formula: score ~ Order + Voice + (Order * Voice | subject) + (Order *
>    Voice | sentence)
>   Data: rawdata
>  AIC  BIC logLik deviance REMLdev
> 1622 1732 -787.2     1574    1586
> Random effects:
> Groups   Name                         Variance Std.Dev. Corr
> subject  (Intercept)                  0.112099 0.33481
>          OrderDat second              0.095834 0.30957  -0.743
>          VoicePassive                 0.206993 0.45497  -0.953  0.895
>          OrderDat second:VoicePassive 0.333823 0.57777   0.765 -0.999 -0.906
> sentence (Intercept)                  0.017953 0.13399
>          OrderDat second              0.105527 0.32485  -0.832
>          VoicePassive                 0.046129 0.21478  -0.473  0.454
>          OrderDat second:VoicePassive 0.183020 0.42781   0.526 -0.786 -0.819
> Residual                              0.463528 0.68083
> Number of obs: 720, groups: subject, 36; sentence, 20
> 
> Fixed effects:
>                Estimate Std. Error t value
> (Intercept)     0.008662   0.065187   0.133
> OrderDat second 0.275830   0.068133   4.048
> VoicePassive    0.237267   0.069355   3.421
> 
> Correlation of Fixed Effects:
>            (Intr) OrdrDs
> OrderDtscnd -0.468
> VoicePassiv -0.632 -0.161
>> anova(m1,mi)
> Data: rawdata
> Models:
> mi: score ~ Order + Voice + (Order * Voice | subject) + (Order *
> mi:     Voice | sentence)
> m1: score ~ Order * Voice + (Order * Voice | subject) + (Order *
> m1:     Voice | sentence)
>   Df    AIC    BIC  logLik  Chisq Chi Df Pr(>Chisq)
> mi 24 1622.4 1732.3 -787.21
> m1 25 1611.6 1726.1 -780.82 12.788      1  0.0003489 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> 
> **** sum coding:
> 
>> contrasts(rawdata$Order) = contr.sum(2)
>> contrasts(rawdata$Voice) = contr.sum(2)
>> m1 = lmer(score ~ Order*Voice + (Order*Voice|subject)+(Order*Voice|sentence), REML=F ,rawdata)
>> summary(m1)
> Linear mixed model fit by maximum likelihood
> Formula: score ~ Order * Voice + (Order * Voice | subject) + (Order *
>    Voice | sentence)
>   Data: rawdata
>  AIC  BIC logLik deviance REMLdev
> 1612 1726 -780.9     1562    1581
> Random effects:
> Groups   Name          Variance   Std.Dev.  Corr
> subject  (Intercept)   1.8907e-02 0.1375028
>          Order1        1.2567e-05 0.0035449 -1.000
>          Voice1        1.2530e-02 0.1119373  0.866 -0.866
>          Order1:Voice1 1.0808e-02 0.1039636  0.292 -0.292  0.630
> sentence (Intercept)   2.7251e-03 0.0522022
>          Order1        1.0143e-02 0.1007109  0.250
>          Voice1        5.1207e-03 0.0715590  0.333 -0.465
>          Order1:Voice1 2.9324e-03 0.0541514 -0.877  0.186 -0.353
> Residual               4.6608e-01 0.6826980
> Number of obs: 720, groups: subject, 36; sentence, 20
> 
> Fixed effects:
>              Estimate Std. Error t value
> (Intercept)    0.26184    0.03618   7.238
> Order1        -0.12645    0.03398  -3.721
> Voice1        -0.10761    0.03538  -3.042
> Order1:Voice1  0.13562    0.03308   4.100
> 
> Correlation of Fixed Effects:
>            (Intr) Order1 Voice1
> Order1       0.042
> Voice1       0.338 -0.147
> Order1:Voc1 -0.007  0.043  0.116
>> mi = lmer(score ~ Order+Voice + (Order*Voice|subject)+(Order*Voice|sentence) ,REML=F,rawdata)
>> summary(mi)
> Linear mixed model fit by maximum likelihood
> Formula: score ~ Order + Voice + (Order * Voice | subject) + (Order *
>    Voice | sentence)
>   Data: rawdata
>  AIC  BIC logLik deviance REMLdev
> 1623 1732 -787.3     1575    1589
> Random effects:
> Groups   Name          Variance   Std.Dev.  Corr
> subject  (Intercept)   1.9424e-02 0.1393698
>          Order1        2.6892e-06 0.0016399 -1.000
>          Voice1        1.3041e-02 0.1141955  0.838 -0.838
>          Order1:Voice1 2.0839e-02 0.1443579  0.290 -0.290  0.541
> sentence (Intercept)   2.4074e-03 0.0490650
>          Order1        1.0471e-02 0.1023269  0.323
>          Voice1        4.3300e-03 0.0658028  0.280 -0.571
>          Order1:Voice1 1.1412e-02 0.1068262 -0.784  0.204 -0.300
> Residual               4.6362e-01 0.6808933
> Number of obs: 720, groups: subject, 36; sentence, 20
> 
> Fixed effects:
>            Estimate Std. Error t value
> (Intercept)  0.26513    0.03609   7.346
> Order1      -0.13474    0.03407  -3.955
> Voice1      -0.11837    0.03480  -3.401
> 
> Correlation of Fixed Effects:
>       (Intr) Order1
> Order1  0.063
> Voice1  0.334 -0.173
> 
>> anova(m1,mi)
> Data: rawdata
> Models:
> mi: score ~ Order + Voice + (Order * Voice | subject) + (Order *
> mi:     Voice | sentence)
> m1: score ~ Order * Voice + (Order * Voice | subject) + (Order *
> m1:     Voice | sentence)
>   Df    AIC    BIC  logLik  Chisq Chi Df Pr(>Chisq)
> mi 24 1622.5 1732.4 -787.26
> m1 25 1611.7 1726.2 -780.86 12.796      1  0.0003473 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1


From paul.tanger at colostate.edu  Wed Mar  5 23:12:01 2014
From: paul.tanger at colostate.edu (Paul Tanger)
Date: Wed, 5 Mar 2014 15:12:01 -0700
Subject: [R-sig-ME] plot observed vs fitted with model with NAs in data
In-Reply-To: <531747C0.9060606@gmail.com>
References: <CAPEW02bk+Fn6uo9=3GVnPvosaAm60W6ugvr8cPyqyvaEUjOK6A@mail.gmail.com>
	<AA818EAD2576BC488B4F623941DA7427F3A11D25@inbomail.inbo.be>
	<CAPEW02ZF8EtSVyikKAy20FLYS-QHDRZP6FnavP2A1h3JtaNkiA@mail.gmail.com>
	<531747C0.9060606@gmail.com>
Message-ID: <CAPEW02Yrm47P4DotpZor8XegEBrV5pkWUV3eCBtWTYax53OdQA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140305/fcd92319/attachment.pl>

From bbolker at gmail.com  Thu Mar  6 04:29:55 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 05 Mar 2014 22:29:55 -0500
Subject: [R-sig-ME] thoughts about Stroup 2014
Message-ID: <5317EBB3.8090700@mcmaster.ca>

Kevin Wright <kw.stat at ...> writes:

> 
> I'll add another that just appeared:
> 
> Rethinking the Analysis of Non-Normal Data in Plant and Soil
> Science, Walt Stroup. Published in Agron. J. 106:1-17 (2014) 
> doi:10.2134/agronj2013.0342
> 
> One key takeaway--the linear predictor in linear mixed models may
> not be appropriate as the linear predictor in a GLMM.
> 
> Kevin Wright

  A couple of quick thoughts about this paper:

* I thought it was very well written, and enjoyed reading it.
* It's reassuring that the view from the SAS side of the fence isn't
_too_ horribly different from the way I look at things.
* To my eye, the takeaway about linear predictors is really just
saying that one often needs to incorporate an observation-level
random effect to account for overdispersion in GLMMs (score one for
MCMCglmm here ...)
* GLIMMIX can do correlated G-side random effects -- something that
is contemplated for the 'flexLambda' development branch of lme4, but
it will be a while before we get there.
* On the use of Kenward-Roger for GLMMs: "Although the Kenward?Roger
adjustment was derived for the LMM with
normally distributed data and is an ad hoc procedure for GLMMs
with non-normal data, informal simulation studies consistently
have suggested that the adjustment is accurate."
* I thought it was interesting that Beta regression is included within
the GLMM functionality in SAS -- since Beta distributions aren't in
the exponential family I think of them as (slightly) outside the scope
of GLMMs.
* I would have liked to see Table 2 (summary of analyses) done in
a graphical format.


From holtermann at hwwi.org  Thu Mar  6 11:53:50 2014
From: holtermann at hwwi.org (Linus Holtermann)
Date: Thu, 6 Mar 2014 11:53:50 +0100
Subject: [R-sig-ME] Splines in MCMCglmm
Message-ID: <AD0050057515F54084E7D5B93478C8481F7BA0265F@winxbede39.exchange.xchg>

Hello,

For various reasons i need the MCMCglmm-package for my model specification, but it seems to be useful to incorporate splines in my model. I want to employ the same procedure as in the gamm4-package. I try to incorporate the splines via Jarrod?s function, which based on the mgvc-package. I attached my data and codes. 
For the thin-plate-splines the fixed parts are similiar between MCMCglmm and gamm4, but the random parts differ. It is worth noting, that the predictions are the same.
When using P-Splines instead of thin-plate-splines both fixed parts and random parts differ. I guess, i made a mistake somewhere.
I also add two verions where more than one covariate has spline-functions.

Are my codes ok or is there a mistake? Why does the results for P-Spline differ by a substantial amount?

R-Code:


library(gamm4)
library(MCMCglmm)
library(lme4)
library(nlme)

### read data: y = respones variable; x_1...x_8 covariates (x_8=dummy); country = groups
data <- read.csv2("....data_test.csv")	
attach(data)

### grand mean centering of covariates (x_8 not centered [dummy])
grand_mean <- function(gm){
gm - mean(gm)
}

data$x_1_c <- grand_mean(x_1)
data$x_2_c <- grand_mean(x_2)
data$x_3_c <- grand_mean(x_3)
data$x_4_c <- grand_mean(x_4)
data$x_5_c <- grand_mean(x_5)
data$x_6_c <- grand_mean(x_6)
data$x_7_c <- grand_mean(x_7)

### Model without Splines in MCMCglmm and gamm4 (Varying Intercept)

prior_mc1 <- list(R = list(V = 1, nu=0.002), G = list(G1 = list(V = diag(1), nu = 0.002)))
mc <- MCMCglmm(y~x_1_c+x_2_c+x_3_c+x_4_c+x_5_c+x_6_c+x_7_c+x_8+x_8:x_7_c,
random = ~country,data =data,family="gaussian" , verbose = F,prior = prior_mc1,
 burnin=10000,nitt=110000,thin=10,saveX=T,saveZ=T,saveXL=T,pr=T,pl=T)
summary(mc)

# same model in gamm4

gamm <- gamm4(y~x_1_c+x_2_c+x_3_c+x_4_c+x_5_c+x_6_c+x_7_c+x_8+x_8:x_7_c,
data=data,random=~(1|country))
summary(gamm$mer)

### identical output!!! ###

### use standard splines in the mgvc-package(thin plate splines)
### use Jarrod?s function

library(mgcv)

spl2<-function(formula, data, p=TRUE, dataX=data){

  aug<-nrow(data)-nrow(dataX)

  if(aug!=0){
    if(aug<0){
      stop("sorry nrow(dataX) must be less than or equal to nrow(data)")
    }else{
      augX<-matrix(0, aug, ncol(dataX))
      colnames(augX)<-colnames(dataX)
      dataX<-rbind(dataX, augX)
    }
  }
  smooth.spec.object<-interpret.gam(formula)$smooth.spec[[1]]
  sm<-smoothCon(smooth.spec.object, data=data, knots=NULL,absorb.cons=TRUE, dataX=dataX)[[1]]

  Sed<-eigen(sm$S[[1]])
  Su<-Sed$vectors
  Sd<-Sed$values
  nonzeros <- which(Sd > sqrt(.Machine$double.eps))
  
  if(p){
    Zn<-sm$X%*%Su[,nonzeros, drop=FALSE]%*%diag(1/sqrt(Sd[nonzeros]))
  }else{
    Zn<-sm$X[,-nonzeros, drop=FALSE]
  }
  return(Zn[1:(nrow(data)-aug),,drop=FALSE])
}

# A function for setting up fixed (p=FALSE) and random (p=TRUE) effect design matrices
# for a reparmetrised smooth.
# data is used to set up the basis, but the design matrices are based on dataX 

### set thin plate spline for ONE COVARIATE x_5_c ###

data$Xo<-spl2(~s(x_5_c,k=10), data=data, p=F)
data$Zn<-spl2(~s(x_5_c,k=10), data=data)

# MCMCglmm
prior_mc1 <- list(R = list(V = 1, nu=0.002), G = list(G1 = list(V = diag(1), nu = 0.002),G2 = list(V = diag(1), nu = 0.002)))
mc_spl <- MCMCglmm(y~x_1_c+x_2_c+x_3_c+x_4_c+Xo+x_6_c+x_7_c+x_8+x_8:x_7_c,
random = ~country+idv(Zn),data =data,family="gaussian" , verbose = F,prior = prior_mc1,
 burnin=10000,nitt=110000,thin=10,saveX=T,saveZ=T,saveXL=T,pr=T,pl=T)
summary(mc_spl)

# gamm4
gamm_spl <- gamm4(y~x_1_c+x_2_c+x_3_c+x_4_c+s(x_5_c,k=10)+x_6_c+x_7_c+x_8+x_8:x_7_c,
data=data,random=~(1|country))
summary(gamm_spl$mer)

plot(predict(gamm_spl$mer), predict(mc_spl, marginal=NULL))

### results for fixed part a similiar, but not for random parts
### Prediction are the same
### ???


### use P-Splines for covariate x_5_c

data$Xo_p<-spl2(~s(x_5_c,bs="ps",k=10), data=data, p=F)
data$Zn_p<-spl2(~s(x_5_c,bs="ps",k=10), data=data)

# MCMCglmm
prior_mc1 <- list(R = list(V = 1, nu=0.002), G = list(G1 = list(V = diag(1), nu = 0.002),G2 = list(V = diag(1), nu = 0.002)))
mc_spl_p <- MCMCglmm(y~x_1_c+x_2_c+x_3_c+x_4_c+Xo_p+x_6_c+x_7_c+x_8+x_8:x_7_c,
random = ~country+idv(Zn_p),data =data,family="gaussian" , verbose = F,prior = prior_mc1,
 burnin=10000,nitt=110000,thin=10,saveX=T,saveZ=T,saveXL=T,pr=T,pl=T)
summary(mc_spl_p)

# gamm4
gamm_spl_p <- gamm4(y~x_1_c+x_2_c+x_3_c+x_4_c+s(x_5_c,bs="ps",k=10)+x_6_c+x_7_c+x_8+x_8:x_7_c,
data=data,random=~(1|country))
summary(gamm_spl_p$mer)

plot(predict(gamm_spl_p$mer), predict(mc_spl_p, marginal=NULL))

### results for fixed parts and random parts differ
### Prediction are nearly the same
### ???

#-----------------------------------------------------------------------

### set thin plate splines for covariates x_1_c ... x_7_c

data$Xo_1<-spl2(~s(x_1_c,k=10), data=data, p=F)
data$Zn_1<-spl2(~s(x_1_c,k=10), data=data)
data$Xo_2<-spl2(~s(x_2_c,k=10), data=data, p=F)
data$Zn_2<-spl2(~s(x_2_c,k=10), data=data)
data$Xo_3<-spl2(~s(x_3_c,k=10), data=data, p=F)
data$Zn_3<-spl2(~s(x_3_c,k=10), data=data)
data$Xo_4<-spl2(~s(x_4_c,k=10), data=data, p=F)
data$Zn_4<-spl2(~s(x_4_c,k=10), data=data)
data$Xo_5<-spl2(~s(x_5_c,k=10), data=data, p=F)
data$Zn_5<-spl2(~s(x_5_c,k=10), data=data)
data$Xo_6<-spl2(~s(x_6_c,k=10), data=data, p=F)
data$Zn_6<-spl2(~s(x_6_c,k=10), data=data)
data$Xo_7<-spl2(~s(x_7_c,k=10), data=data, p=F)
data$Zn_7<-spl2(~s(x_7_c,k=10), data=data)

# Version a (that will take some time)

prior_mc_spl_a <- list(R = list(V = 1, nu=0.002), G = list(G1 = list(V = diag(1), nu = 0.002),G2 = list(V = diag(1), nu = 0.002),
G3 = list(V = diag(1), nu = 0.002),G4 = list(V = diag(1), nu = 0.002),G5 = list(V = diag(1), nu = 0.002),
G6 = list(V = diag(1), nu = 0.002),G7 = list(V = diag(1), nu = 0.002),G8 = list(V = diag(1), nu = 0.002)))
mc_spl_a <- MCMCglmm(y~Xo_1+Xo_2+Xo_3+Xo_4+Xo_5+Xo_6+Xo_7+x_8+x_8:x_7_c,
random = ~country+idv(Zn_1)+idv(Zn_2)+idv(Zn_3)+idv(Zn_4)+idv(Zn_5)+idv(Zn_6)+idv(Zn_7),
data =data,family="gaussian" , verbose = F,prior = prior_mc_spl_a,
 burnin=10000,nitt=110000,thin=10,saveX=T,saveZ=T,saveXL=T,pr=T,pl=T)
summary(mc_spl_a)

gamm_spl_a <- gamm4(y~s(x_1_c,k=10)+s(x_2_c,k=10)+s(x_3_c,k=10)+s(x_4_c,k=10)+s(x_5_c,k=10)+s(x_6_c,k=10)+s(x_7_c,k=10)+x_8+x_8:x_7_c,
data=data,random=~(1|country))
summary(gamm_spl_a$mer)

plot(predict(gamm_spl_a$mer), predict(mc_spl_a, marginal=NULL))
 
### fixed parts are similiar, but random parts differ
### Predictions are nearly the same
### ???

# Version b: cbind Zns to one matrix

Zn_b <- cbind(data$Zn_1,data$Zn_2,data$Zn_3,data$Zn_4,data$Zn_5,data$Zn_6,data$Zn_7)

prior_mc_spl_b <- list(R = list(V = 1, nu=0.002), G = list(G1 = list(V = diag(1), nu = 0.002),G2 = list(V = diag(1), nu = 0.002)))
mc_spl_b <- MCMCglmm(y~Xo_1+Xo_2+Xo_3+Xo_4+Xo_5+Xo_6+Xo_7+x_8+x_8:x_7_c,
random = ~country+idv(Zn_b),data =data,family="gaussian" , verbose = F,prior = prior_mc_spl_b,
burnin=10000,nitt=110000,thin=10,saveX=T,saveZ=T,saveXL=T,pr=T,pl=T)
summary(mc_spl_b)

plot(predict(mc_spl_b, marginal=NULL), predict(mc_spl_a, marginal=NULL))

### Prediction are the same between Version a and Version b
### So, are both methods ok???


best regards,


Linus Holtermann
Hamburgisches WeltWirtschaftsInstitut gemeinn?tzige GmbH (HWWI)
Heimhuder Stra?e 71
20148 Hamburg
Tel +49-(0)40-340576-336
Fax+49-(0)40-340576-776
Internet: www.hwwi.org
Email: holtermann at hwwi.org
 
AmtsgerichtHamburg HRB 94303
Gesch?ftsf?hrer: Prof. Dr. Thomas Straubhaar, Gunnar Geyer
Umsatzsteuer-ID: DE 241849425
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: test_splines_MCMCglmm.R
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140306/8f97e5b3/attachment.pl>

From senegaf at yahoo.fr  Thu Mar  6 16:23:00 2014
From: senegaf at yahoo.fr (Senegaf)
Date: Thu, 6 Mar 2014 15:23:00 +0000 (GMT)
Subject: [R-sig-ME] formula random terms in lmer function
Message-ID: <1394119380.86638.YahooMailNeo@web172504.mail.ir2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140306/a9ad373d/attachment.pl>

From bbolker at gmail.com  Thu Mar  6 19:01:14 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 06 Mar 2014 13:01:14 -0500
Subject: [R-sig-ME] formula random terms in lmer function
In-Reply-To: <1394119380.86638.YahooMailNeo@web172504.mail.ir2.yahoo.com>
References: <1394119380.86638.YahooMailNeo@web172504.mail.ir2.yahoo.com>
Message-ID: <5318B7EA.5050400@gmail.com>

On 14-03-06 10:23 AM, Senegaf wrote:
> Dear R-sig-mixed-models Group,
> 
> I am new in R and I would confirm my understanding of lmer function in lme4 via the example below: 
> 
> lmer(Y~(1|X)+(1|X:Y)+(1|Y:Z))
> 
> X is random
> The interaction between X and Y is random
> Z nested into Y is random
> 
> If so, should I unsderstand that interaction and nesting are similarly specified.

   See http://glmm.wikidot.com/faq#modelspec

  There are no fixed effects in this model (although there is an
implicit intercept term).

  X, X:Y, and Y:Z are *grouping variables*.  X, Y, and Z will be treated
as categorical variables (factors); if this doesn't make sense (e.g. if
one of them is a numeric covariate with unique values for every
observation), then the model doesn't make sense. All three random effect
terms in the model (anything with a | in it is a random effect term) are
*scalar, intercept* random effects -- that is, the baseline/mean values
varies randomly among groups defined by the levels of X, X:Y, and Y:Z.

  : denotes an interaction.

  You could say that Y is *nested* in X, and you could specify the model
equivalently as

  (1|X/Y) + (1|Y:Z)

  I really don't know what you're trying to do without more context, but
it *might* be that you want (1|X/Y/Z) (Z nested within Y within X), or
equivalently  (1|X) + (1|X:Y) + (1|X:Y:Z).

  Hope that helps.

  Ben Bolker



> 
> Grateful if someone could advise. Thank you!
> 
> Gaston


From j.hadfield at ed.ac.uk  Thu Mar  6 22:02:15 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Thu, 06 Mar 2014 21:02:15 +0000
Subject: [R-sig-ME] Splines in MCMCglmm
In-Reply-To: <AD0050057515F54084E7D5B93478C8481F7BA0265F@winxbede39.exchange.xchg>
References: <AD0050057515F54084E7D5B93478C8481F7BA0265F@winxbede39.exchange.xchg>
Message-ID: <20140306210215.11353gw8ntua9e68@www.staffmail.ed.ac.uk>

Hi Linus,

I think the splines estimated in MCMCglmm are agreeing with gamm4.  
After all, the predictions are almost identical. There are differences  
in the estimates due to priors and/or Monte Carlo error but they're  
pretty subtle in the context of the uncertainty around the estimates.  
The splines are being heavily penalised in both, perhaps a bit more in  
MCMCglmm where the IW prior puts higher mass on variances of 0.  
Parameter expanded priors may give more similar results.

The agreement between the predictions from models spl_a and spl_b is  
not a general phenomenon: in the first model you are penalsing all  
splines independently, in the latter you are penalising them all to  
the same degree. The reason that they both agree is that there is  
little evidence that the relationships differ from linearity for any  
of the covariates.

For people wishing to use spl2 to fit splines in MCMCglmm, please be  
aware I have not tested this function very carefully.

Cheers,

Jarrod







Quoting Linus Holtermann <holtermann at hwwi.org> on Thu, 6 Mar 2014  
11:53:50 +0100:

> Hello,
>
> For various reasons i need the MCMCglmm-package for my model  
> specification, but it seems to be useful to incorporate splines in  
> my model. I want to employ the same procedure as in the  
> gamm4-package. I try to incorporate the splines via Jarrod?s  
> function, which based on the mgvc-package. I attached my data and  
> codes.
> For the thin-plate-splines the fixed parts are similiar between  
> MCMCglmm and gamm4, but the random parts differ. It is worth noting,  
> that the predictions are the same.
> When using P-Splines instead of thin-plate-splines both fixed parts  
> and random parts differ. I guess, i made a mistake somewhere.
> I also add two verions where more than one covariate has spline-functions.
>
> Are my codes ok or is there a mistake? Why does the results for  
> P-Spline differ by a substantial amount?
>
> R-Code:
>
>
> library(gamm4)
> library(MCMCglmm)
> library(lme4)
> library(nlme)
>
> ### read data: y = respones variable; x_1...x_8 covariates  
> (x_8=dummy); country = groups
> data <- read.csv2("....data_test.csv")
> attach(data)
>
> ### grand mean centering of covariates (x_8 not centered [dummy])
> grand_mean <- function(gm){
> gm - mean(gm)
> }
>
> data$x_1_c <- grand_mean(x_1)
> data$x_2_c <- grand_mean(x_2)
> data$x_3_c <- grand_mean(x_3)
> data$x_4_c <- grand_mean(x_4)
> data$x_5_c <- grand_mean(x_5)
> data$x_6_c <- grand_mean(x_6)
> data$x_7_c <- grand_mean(x_7)
>
> ### Model without Splines in MCMCglmm and gamm4 (Varying Intercept)
>
> prior_mc1 <- list(R = list(V = 1, nu=0.002), G = list(G1 = list(V =  
> diag(1), nu = 0.002)))
> mc <- MCMCglmm(y~x_1_c+x_2_c+x_3_c+x_4_c+x_5_c+x_6_c+x_7_c+x_8+x_8:x_7_c,
> random = ~country,data =data,family="gaussian" , verbose = F,prior =  
> prior_mc1,
>  burnin=10000,nitt=110000,thin=10,saveX=T,saveZ=T,saveXL=T,pr=T,pl=T)
> summary(mc)
>
> # same model in gamm4
>
> gamm <- gamm4(y~x_1_c+x_2_c+x_3_c+x_4_c+x_5_c+x_6_c+x_7_c+x_8+x_8:x_7_c,
> data=data,random=~(1|country))
> summary(gamm$mer)
>
> ### identical output!!! ###
>
> ### use standard splines in the mgvc-package(thin plate splines)
> ### use Jarrod?s function
>
> library(mgcv)
>
> spl2<-function(formula, data, p=TRUE, dataX=data){
>
>   aug<-nrow(data)-nrow(dataX)
>
>   if(aug!=0){
>     if(aug<0){
>       stop("sorry nrow(dataX) must be less than or equal to nrow(data)")
>     }else{
>       augX<-matrix(0, aug, ncol(dataX))
>       colnames(augX)<-colnames(dataX)
>       dataX<-rbind(dataX, augX)
>     }
>   }
>   smooth.spec.object<-interpret.gam(formula)$smooth.spec[[1]]
>   sm<-smoothCon(smooth.spec.object, data=data,  
> knots=NULL,absorb.cons=TRUE, dataX=dataX)[[1]]
>
>   Sed<-eigen(sm$S[[1]])
>   Su<-Sed$vectors
>   Sd<-Sed$values
>   nonzeros <- which(Sd > sqrt(.Machine$double.eps))
>
>   if(p){
>     Zn<-sm$X%*%Su[,nonzeros, drop=FALSE]%*%diag(1/sqrt(Sd[nonzeros]))
>   }else{
>     Zn<-sm$X[,-nonzeros, drop=FALSE]
>   }
>   return(Zn[1:(nrow(data)-aug),,drop=FALSE])
> }
>
> # A function for setting up fixed (p=FALSE) and random (p=TRUE)  
> effect design matrices
> # for a reparmetrised smooth.
> # data is used to set up the basis, but the design matrices are  
> based on dataX
>
> ### set thin plate spline for ONE COVARIATE x_5_c ###
>
> data$Xo<-spl2(~s(x_5_c,k=10), data=data, p=F)
> data$Zn<-spl2(~s(x_5_c,k=10), data=data)
>
> # MCMCglmm
> prior_mc1 <- list(R = list(V = 1, nu=0.002), G = list(G1 = list(V =  
> diag(1), nu = 0.002),G2 = list(V = diag(1), nu = 0.002)))
> mc_spl <- MCMCglmm(y~x_1_c+x_2_c+x_3_c+x_4_c+Xo+x_6_c+x_7_c+x_8+x_8:x_7_c,
> random = ~country+idv(Zn),data =data,family="gaussian" , verbose =  
> F,prior = prior_mc1,
>  burnin=10000,nitt=110000,thin=10,saveX=T,saveZ=T,saveXL=T,pr=T,pl=T)
> summary(mc_spl)
>
> # gamm4
> gamm_spl <-  
> gamm4(y~x_1_c+x_2_c+x_3_c+x_4_c+s(x_5_c,k=10)+x_6_c+x_7_c+x_8+x_8:x_7_c,
> data=data,random=~(1|country))
> summary(gamm_spl$mer)
>
> plot(predict(gamm_spl$mer), predict(mc_spl, marginal=NULL))
>
> ### results for fixed part a similiar, but not for random parts
> ### Prediction are the same
> ### ???
>
>
> ### use P-Splines for covariate x_5_c
>
> data$Xo_p<-spl2(~s(x_5_c,bs="ps",k=10), data=data, p=F)
> data$Zn_p<-spl2(~s(x_5_c,bs="ps",k=10), data=data)
>
> # MCMCglmm
> prior_mc1 <- list(R = list(V = 1, nu=0.002), G = list(G1 = list(V =  
> diag(1), nu = 0.002),G2 = list(V = diag(1), nu = 0.002)))
> mc_spl_p <-  
> MCMCglmm(y~x_1_c+x_2_c+x_3_c+x_4_c+Xo_p+x_6_c+x_7_c+x_8+x_8:x_7_c,
> random = ~country+idv(Zn_p),data =data,family="gaussian" , verbose =  
> F,prior = prior_mc1,
>  burnin=10000,nitt=110000,thin=10,saveX=T,saveZ=T,saveXL=T,pr=T,pl=T)
> summary(mc_spl_p)
>
> # gamm4
> gamm_spl_p <-  
> gamm4(y~x_1_c+x_2_c+x_3_c+x_4_c+s(x_5_c,bs="ps",k=10)+x_6_c+x_7_c+x_8+x_8:x_7_c,
> data=data,random=~(1|country))
> summary(gamm_spl_p$mer)
>
> plot(predict(gamm_spl_p$mer), predict(mc_spl_p, marginal=NULL))
>
> ### results for fixed parts and random parts differ
> ### Prediction are nearly the same
> ### ???
>
> #-----------------------------------------------------------------------
>
> ### set thin plate splines for covariates x_1_c ... x_7_c
>
> data$Xo_1<-spl2(~s(x_1_c,k=10), data=data, p=F)
> data$Zn_1<-spl2(~s(x_1_c,k=10), data=data)
> data$Xo_2<-spl2(~s(x_2_c,k=10), data=data, p=F)
> data$Zn_2<-spl2(~s(x_2_c,k=10), data=data)
> data$Xo_3<-spl2(~s(x_3_c,k=10), data=data, p=F)
> data$Zn_3<-spl2(~s(x_3_c,k=10), data=data)
> data$Xo_4<-spl2(~s(x_4_c,k=10), data=data, p=F)
> data$Zn_4<-spl2(~s(x_4_c,k=10), data=data)
> data$Xo_5<-spl2(~s(x_5_c,k=10), data=data, p=F)
> data$Zn_5<-spl2(~s(x_5_c,k=10), data=data)
> data$Xo_6<-spl2(~s(x_6_c,k=10), data=data, p=F)
> data$Zn_6<-spl2(~s(x_6_c,k=10), data=data)
> data$Xo_7<-spl2(~s(x_7_c,k=10), data=data, p=F)
> data$Zn_7<-spl2(~s(x_7_c,k=10), data=data)
>
> # Version a (that will take some time)
>
> prior_mc_spl_a <- list(R = list(V = 1, nu=0.002), G = list(G1 =  
> list(V = diag(1), nu = 0.002),G2 = list(V = diag(1), nu = 0.002),
> G3 = list(V = diag(1), nu = 0.002),G4 = list(V = diag(1), nu =  
> 0.002),G5 = list(V = diag(1), nu = 0.002),
> G6 = list(V = diag(1), nu = 0.002),G7 = list(V = diag(1), nu =  
> 0.002),G8 = list(V = diag(1), nu = 0.002)))
> mc_spl_a <- MCMCglmm(y~Xo_1+Xo_2+Xo_3+Xo_4+Xo_5+Xo_6+Xo_7+x_8+x_8:x_7_c,
> random =  
> ~country+idv(Zn_1)+idv(Zn_2)+idv(Zn_3)+idv(Zn_4)+idv(Zn_5)+idv(Zn_6)+idv(Zn_7),
> data =data,family="gaussian" , verbose = F,prior = prior_mc_spl_a,
>  burnin=10000,nitt=110000,thin=10,saveX=T,saveZ=T,saveXL=T,pr=T,pl=T)
> summary(mc_spl_a)
>
> gamm_spl_a <-  
> gamm4(y~s(x_1_c,k=10)+s(x_2_c,k=10)+s(x_3_c,k=10)+s(x_4_c,k=10)+s(x_5_c,k=10)+s(x_6_c,k=10)+s(x_7_c,k=10)+x_8+x_8:x_7_c,
> data=data,random=~(1|country))
> summary(gamm_spl_a$mer)
>
> plot(predict(gamm_spl_a$mer), predict(mc_spl_a, marginal=NULL))
>
> ### fixed parts are similiar, but random parts differ
> ### Predictions are nearly the same
> ### ???
>
> # Version b: cbind Zns to one matrix
>
> Zn_b <-  
> cbind(data$Zn_1,data$Zn_2,data$Zn_3,data$Zn_4,data$Zn_5,data$Zn_6,data$Zn_7)
>
> prior_mc_spl_b <- list(R = list(V = 1, nu=0.002), G = list(G1 =  
> list(V = diag(1), nu = 0.002),G2 = list(V = diag(1), nu = 0.002)))
> mc_spl_b <- MCMCglmm(y~Xo_1+Xo_2+Xo_3+Xo_4+Xo_5+Xo_6+Xo_7+x_8+x_8:x_7_c,
> random = ~country+idv(Zn_b),data =data,family="gaussian" , verbose =  
> F,prior = prior_mc_spl_b,
> burnin=10000,nitt=110000,thin=10,saveX=T,saveZ=T,saveXL=T,pr=T,pl=T)
> summary(mc_spl_b)
>
> plot(predict(mc_spl_b, marginal=NULL), predict(mc_spl_a, marginal=NULL))
>
> ### Prediction are the same between Version a and Version b
> ### So, are both methods ok???
>
>
> best regards,
>
>
> Linus Holtermann
> Hamburgisches WeltWirtschaftsInstitut gemeinn?tzige GmbH (HWWI)
> Heimhuder Stra?e 71
> 20148 Hamburg
> Tel +49-(0)40-340576-336
> Fax+49-(0)40-340576-776
> Internet: www.hwwi.org
> Email: holtermann at hwwi.org
>
> AmtsgerichtHamburg HRB 94303
> Gesch?ftsf?hrer: Prof. Dr. Thomas Straubhaar, Gunnar Geyer
> Umsatzsteuer-ID: DE 241849425



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From vasishth.shravan at gmail.com  Fri Mar  7 14:21:28 2014
From: vasishth.shravan at gmail.com (Shravan Vasishth)
Date: Fri, 7 Mar 2014 07:21:28 -0600
Subject: [R-sig-ME] likelihood-ratio tests in conflict with coefficiants
 in maximal random effect model
Message-ID: <CA+1m9_7VRbiMm653KGXsAvy7pQFULUoc9e9m+bn2NyZON3zD-g@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140307/50f3d8a2/attachment.pl>

From rlevy at ucsd.edu  Fri Mar  7 18:13:46 2014
From: rlevy at ucsd.edu (Levy, Roger)
Date: Fri, 7 Mar 2014 17:13:46 +0000
Subject: [R-sig-ME] likelihood-ratio tests in conflict with coefficiants
 in maximal random effect model
In-Reply-To: <CA+1m9_7VRbiMm653KGXsAvy7pQFULUoc9e9m+bn2NyZON3zD-g@mail.gmail.com>
References: <CA+1m9_7VRbiMm653KGXsAvy7pQFULUoc9e9m+bn2NyZON3zD-g@mail.gmail.com>
Message-ID: <37D14517-1703-4CBD-960C-8F827929E6D4@ucsd.edu>


On Mar 7, 2014, at 6:21 AM, Shravan Vasishth <vasishth.shravan at gmail.com> wrote:

> Hi Roger and Emilia, and others,
> 
> I just wanted to say that in Emilia's data, she has 36 subjects and 20
> items. Roger, would you agree that it is very difficult with this amount of
> data to accurately estimate the full variance-covariance matrices for
> subjects and for items random effects, especially the correlation
> parameters? The numbers that lmer returns, for such sizes of data, are
> pretty wild estimates, and often have no bearing to the true underlying
> correlations. I think that in this situation we might be asking too much
> from lmer, without giving it enough data. If, on the other hand, we have a
> lot of data by subjects and items, it becomes possible to estimate these
> parameters.
> 
> I believe this may have been, at least partly, the intent of Douglas Bates'
> original message about overparameterization.

That?s a good question.  I imagine there is a fair bit of uncertainty regarding the correlation parameters, though I would guess that it?s not huge for this-sized dataset. The point estimates that lme4(.0) give us don?t quantify this uncertainty, but of course we could use Bayesian methods to get a better sense of them.

More generally, this point that you raise, Shravan, is precisely the reason that I tend to favor likelihood-ratio tests over the t-statistic for the purposes of confirmatory hypothesis tests like Emilia?s.  As Baayen, Davidson and Bates (2008, page 396) crucially point out, the t-statistic is computed conditional on a point estimate of the random-effects covariance matrix, and fails to take into account uncertainty in the estimate of this matrix.  The likelihood ratio does not have this problem.  (It has other problems ? namely that the log likelihood ratio is not truly chi-squared distributed ? but with 20 items and 36 subjects in a balanced design I would expect that the chi-squared approximation is fairly close.  And at any rate, the same problem exists with the t statistic.)

So my take is that how much we should worry about these issues depends in part on our modeling goals.  For a confirmatory hypothesis test like Emilia?s on her dataset, I wouldn?t worry much about overparameterization for the models she was showing us.  If she wanted to aggressively interpret the parameter estimates resulting from a particular model fit, on the other hand, I would be much more cautious.

Best

Roger

From emilia.ellsiepen at gmail.com  Wed Mar  5 11:15:35 2014
From: emilia.ellsiepen at gmail.com (Emilia Ellsiepen)
Date: Wed, 5 Mar 2014 11:15:35 +0100
Subject: [R-sig-ME] likelihood-ratio tests in conflict with coefficiants
 in maximal random effect model
In-Reply-To: <4F4F6DEF-3651-497A-8447-3474FC0496F2@ucsd.edu>
References: <CAOU=wbkzKV2e2+k2q3B6rx+ANFzj7MDKtG4xnQ1vmUAfKF17uA@mail.gmail.com>
	<CAO7JsnTHhcExPyKMLBWeHRQbSDgr+HBx0PQ0bnAhBspiFk-YDA@mail.gmail.com>
	<B536A3C0-1895-4FDB-B92B-FA3E11C0DCA2@ucsd.edu>
	<CAO7JsnQAjaoivXbBW0PZgjxHpofTiK59RuobvXJwMaty3VHc7g@mail.gmail.com>
	<CAOU=wbmBEWBq6LV6uOz-C6RQ1cRVnHC9wcJQ_iSCMu8oi5LMFQ@mail.gmail.com>
	<A280C545-EF56-41E4-BAFD-82EA0B5C61A1@ucsd.edu>
	<CAOU=wbmcZBjD9OtoD105J3MK7VtWqJuovZWJbHYb=BBbTXP3Xg@mail.gmail.com>
	<F368889F-E281-4A5D-A258-CD9D9A5ACDBF@ucsd.edu>
	<CAOU=wbn6e37tL0visOmkg+VBW0E483uB4xg6ve6XGWVU7FpqrQ@mail.gmail.com>
	<4F4F6DEF-3651-497A-8447-3474FC0496F2@ucsd.edu>
Message-ID: <CAOU=wbm32QG-FR5QZYFRMAU+Zym=uh79fVNeE+WGQ3x5=XKFFQ@mail.gmail.com>

2014-03-04 18:39 GMT+01:00 Levy, Roger <rlevy at ucsd.edu>:
>
>
>
> On Mar 4, 2014, at 3:10 AM, Emilia Ellsiepen <emilia.ellsiepen at gmail.com> wrote:
>
>> Thank you for your support, Roger - actually the problem vanished when
>> I used sum-coding (after installing the packages and reloading the
>> data, I accidentally skipped this step yesterday). So, in fact with
>> the old lme4.0 version, the model did converge. I pasted in all model
>> summaries and LRTs for dummy and sum coding below.
>> Would you recommend to always use the old version then?
>
> Thanks so much, Emilia, for following up.  This is very interesting.  Something doesn't quite add up, though, in the model summaries and LRTs that you pasted below, because the log-likelihoods in the model summaries are not the same as the log-likelihoods reported in the results of anova().  I notice that you're not using REML=F as an argument in your call to lmer().  Could you perhaps rerun all four model-fitting functions and the two anova() calls with REML=F, and show us the complete results?
>
> Best & thanks again,
>
> Roger
>
Rerunning with REML=F led to a singular convergence in the
null-hypothesis model with dummy coding. The sum coded variant still
converges without warnings. While the two LRTs give in general the
same result for both coding schemes, as I would have expected, there
are minimal differences in the logLik, AIC and BIC values.

Here are the results in detail:
**** dummy coding:

> m1 = lmer(score ~ Order*Voice + (Order*Voice|subject)+(Order*Voice|sentence), REML=F ,rawdata)
> summary(m1)
Linear mixed model fit by maximum likelihood
Formula: score ~ Order * Voice + (Order * Voice | subject) + (Order *
Voice | sentence)
   Data: rawdata
  AIC  BIC logLik deviance REMLdev
 1612 1726 -780.8     1562    1576
Random effects:
 Groups   Name                         Variance Std.Dev. Corr
 subject  (Intercept)                  0.092788 0.30461
          OrderDat second              0.051950 0.22793  -0.689
          VoicePassive                 0.152335 0.39030  -0.954  0.875
          OrderDat second:VoicePassive 0.171303 0.41389   0.736 -0.991 -0.903
 sentence (Intercept)                  0.013700 0.11705
          OrderDat second              0.060511 0.24599  -0.652
          VoicePassive                 0.021227 0.14570  -0.215 -0.067
          OrderDat second:VoicePassive 0.045796 0.21400   0.016 -0.587 -0.404
 Residual                              0.466165 0.68276
Number of obs: 720, groups: subject, 36; sentence, 20

Fixed effects:
                             Estimate Std. Error t value
(Intercept)                   0.16341    0.07650   2.136
OrderDat second              -0.01835    0.09823  -0.187
VoicePassive                 -0.05604    0.10234  -0.548
OrderDat second:VoicePassive  0.54250    0.13194   4.112

Correlation of Fixed Effects:
            (Intr) OrdrDs VcPssv
OrderDtscnd -0.646
VoicePassiv -0.757  0.461
OrdrDscn:VP  0.514 -0.719 -0.730

> mi = lmer(score ~ Order+Voice + (Order*Voice|subject)+(Order*Voice|sentence) ,REML=F,rawdata)
Warning message:
In mer_finalize(ans) : singular convergence (7)
> summary(mi)
Linear mixed model fit by maximum likelihood
Formula: score ~ Order + Voice + (Order * Voice | subject) + (Order *
    Voice | sentence)
   Data: rawdata
  AIC  BIC logLik deviance REMLdev
 1622 1732 -787.2     1574    1586
Random effects:
 Groups   Name                         Variance Std.Dev. Corr
 subject  (Intercept)                  0.112099 0.33481
          OrderDat second              0.095834 0.30957  -0.743
          VoicePassive                 0.206993 0.45497  -0.953  0.895
          OrderDat second:VoicePassive 0.333823 0.57777   0.765 -0.999 -0.906
 sentence (Intercept)                  0.017953 0.13399
          OrderDat second              0.105527 0.32485  -0.832
          VoicePassive                 0.046129 0.21478  -0.473  0.454
          OrderDat second:VoicePassive 0.183020 0.42781   0.526 -0.786 -0.819
 Residual                              0.463528 0.68083
Number of obs: 720, groups: subject, 36; sentence, 20

Fixed effects:
                Estimate Std. Error t value
(Intercept)     0.008662   0.065187   0.133
OrderDat second 0.275830   0.068133   4.048
VoicePassive    0.237267   0.069355   3.421

Correlation of Fixed Effects:
            (Intr) OrdrDs
OrderDtscnd -0.468
VoicePassiv -0.632 -0.161
> anova(m1,mi)
Data: rawdata
Models:
mi: score ~ Order + Voice + (Order * Voice | subject) + (Order *
mi:     Voice | sentence)
m1: score ~ Order * Voice + (Order * Voice | subject) + (Order *
m1:     Voice | sentence)
   Df    AIC    BIC  logLik  Chisq Chi Df Pr(>Chisq)
mi 24 1622.4 1732.3 -787.21
m1 25 1611.6 1726.1 -780.82 12.788      1  0.0003489 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1


**** sum coding:

> contrasts(rawdata$Order) = contr.sum(2)
> contrasts(rawdata$Voice) = contr.sum(2)
> m1 = lmer(score ~ Order*Voice + (Order*Voice|subject)+(Order*Voice|sentence), REML=F ,rawdata)
> summary(m1)
Linear mixed model fit by maximum likelihood
Formula: score ~ Order * Voice + (Order * Voice | subject) + (Order *
    Voice | sentence)
   Data: rawdata
  AIC  BIC logLik deviance REMLdev
 1612 1726 -780.9     1562    1581
Random effects:
 Groups   Name          Variance   Std.Dev.  Corr
 subject  (Intercept)   1.8907e-02 0.1375028
          Order1        1.2567e-05 0.0035449 -1.000
          Voice1        1.2530e-02 0.1119373  0.866 -0.866
          Order1:Voice1 1.0808e-02 0.1039636  0.292 -0.292  0.630
 sentence (Intercept)   2.7251e-03 0.0522022
          Order1        1.0143e-02 0.1007109  0.250
          Voice1        5.1207e-03 0.0715590  0.333 -0.465
          Order1:Voice1 2.9324e-03 0.0541514 -0.877  0.186 -0.353
 Residual               4.6608e-01 0.6826980
Number of obs: 720, groups: subject, 36; sentence, 20

Fixed effects:
              Estimate Std. Error t value
(Intercept)    0.26184    0.03618   7.238
Order1        -0.12645    0.03398  -3.721
Voice1        -0.10761    0.03538  -3.042
Order1:Voice1  0.13562    0.03308   4.100

Correlation of Fixed Effects:
            (Intr) Order1 Voice1
Order1       0.042
Voice1       0.338 -0.147
Order1:Voc1 -0.007  0.043  0.116
> mi = lmer(score ~ Order+Voice + (Order*Voice|subject)+(Order*Voice|sentence) ,REML=F,rawdata)
> summary(mi)
Linear mixed model fit by maximum likelihood
Formula: score ~ Order + Voice + (Order * Voice | subject) + (Order *
    Voice | sentence)
   Data: rawdata
  AIC  BIC logLik deviance REMLdev
 1623 1732 -787.3     1575    1589
Random effects:
 Groups   Name          Variance   Std.Dev.  Corr
 subject  (Intercept)   1.9424e-02 0.1393698
          Order1        2.6892e-06 0.0016399 -1.000
          Voice1        1.3041e-02 0.1141955  0.838 -0.838
          Order1:Voice1 2.0839e-02 0.1443579  0.290 -0.290  0.541
 sentence (Intercept)   2.4074e-03 0.0490650
          Order1        1.0471e-02 0.1023269  0.323
          Voice1        4.3300e-03 0.0658028  0.280 -0.571
          Order1:Voice1 1.1412e-02 0.1068262 -0.784  0.204 -0.300
 Residual               4.6362e-01 0.6808933
Number of obs: 720, groups: subject, 36; sentence, 20

Fixed effects:
            Estimate Std. Error t value
(Intercept)  0.26513    0.03609   7.346
Order1      -0.13474    0.03407  -3.955
Voice1      -0.11837    0.03480  -3.401

Correlation of Fixed Effects:
       (Intr) Order1
Order1  0.063
Voice1  0.334 -0.173

> anova(m1,mi)
Data: rawdata
Models:
mi: score ~ Order + Voice + (Order * Voice | subject) + (Order *
mi:     Voice | sentence)
m1: score ~ Order * Voice + (Order * Voice | subject) + (Order *
m1:     Voice | sentence)
   Df    AIC    BIC  logLik  Chisq Chi Df Pr(>Chisq)
mi 24 1622.5 1732.4 -787.26
m1 25 1611.7 1726.2 -780.86 12.796      1  0.0003473 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1


From afabian at bioinf.uni-leipzig.de  Thu Mar  6 13:50:21 2014
From: afabian at bioinf.uni-leipzig.de (Fabian Amman)
Date: Thu, 06 Mar 2014 13:50:21 +0100
Subject: [R-sig-ME] negative binomial distribution mixture model
Message-ID: <1394110221.21831.37.camel@gin>

Dear mixed-models group

I'd like to ask for your advice in the following matter:

I have two data vectors of observed count data: 'A' and 'B', whereas
count A[n] and B[n] refer to the same observation point.
'A' is assumed to follow a negative binomial distribution.
'B' is assumed to be the result from two underlying processes. For one
again an independent negative binomial distribution and additionally a
kind of shadowing effect from 'A', where a certain fraction 'f' of
counts from 'A' are also observed in 'B'.

Accordingly, one can simulate such data by:

        set.seed(13579)
        f <- runif(1)
        A <- rnbinom(100, mu = 100, size = 1)
        B <- floor(f*A) + rnbinom(100, mu = 20, size = 1)
        
Now to my question: Since 'B' is a mixed model of two negative binomial
distributed variables, how can I estimate the value of factor 'f'
explaining the underlying data best?

The final result should be a vector 'B2', correcting 'B' for its shadow
portion: 'B2 = B - (A/f)'; Since I know 'A' and 'B', to gain this I need
to estimate 'f' from the data and the underlying assumption of a
negative binomial distribution.

As I guess you could figure from my question, I m not really a statistic
nor an R expert, therefore any help is highly appreciated.

Thank you very much in advance.
Regards
Fabian


From bbolker at gmail.com  Fri Mar  7 19:42:51 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 07 Mar 2014 13:42:51 -0500
Subject: [R-sig-ME] negative binomial distribution mixture model
In-Reply-To: <1394110221.21831.37.camel@gin>
References: <1394110221.21831.37.camel@gin>
Message-ID: <531A132B.9010403@gmail.com>

On 14-03-06 07:50 AM, Fabian Amman wrote:
> Dear mixed-models group
> 
> I'd like to ask for your advice in the following matter:
> 
> I have two data vectors of observed count data: 'A' and 'B', whereas
> count A[n] and B[n] refer to the same observation point.
> 'A' is assumed to follow a negative binomial distribution.
> 'B' is assumed to be the result from two underlying processes. For one
> again an independent negative binomial distribution and additionally a
> kind of shadowing effect from 'A', where a certain fraction 'f' of
> counts from 'A' are also observed in 'B'.
> 
> Accordingly, one can simulate such data by:
> 
>         set.seed(13579)
>         f <- runif(1)
>         A <- rnbinom(100, mu = 100, size = 1)
>         B <- floor(f*A) + rnbinom(100, mu = 20, size = 1)
>         
> Now to my question: Since 'B' is a mixed model of two negative binomial
> distributed variables, how can I estimate the value of factor 'f'
> explaining the underlying data best?
> 
> The final result should be a vector 'B2', correcting 'B' for its shadow
> portion: 'B2 = B - (A/f)'; Since I know 'A' and 'B', to gain this I need
> to estimate 'f' from the data and the underlying assumption of a
> negative binomial distribution.
> 
> As I guess you could figure from my question, I m not really a statistic
> nor an R expert, therefore any help is highly appreciated.
> 
> Thank you very much in advance.
> Regards
> Fabian
> 

  This isn't really appropriate for this list: it's a "mixture" model,
of sorts, rather than a "mixed" model.  ("Mixed" models, at least in the
context of this list, refer to models where some of the predictors are
assumed to be random variables drawn from a multivariate normal
distribution.)  If your model doesn't have observed categorical
variables that are associated with unobserved Gaussian random variables,
then you're not really in the right place.

  I'll make these comments, though.

  (1) If I had to do this I would probably set it up in BUGS
(WinBUGS/JAGS).  However, there may be a more elegant/closed-form way to
do it.

  (2) your simulation model will make more sense as a *statistical*
model if you say

   B <- rbinom(100, size=A, prob=f) + rnbinom(100, mu=20, size=1)

instead of using floor().  Then B is at least a well-defined random
variable (the sum of a binomial sample of a NB variable and a NB
variable).  I don't remember my generating-function tricks very well,
but it's even possible that a binomial sample of a NB variable is still
NB ...

  I would suggest CrossValidated (http://stats.stackexchange.com)

 good luck,
    Ben Bolker


From vasishth.shravan at gmail.com  Fri Mar  7 20:18:41 2014
From: vasishth.shravan at gmail.com (Shravan Vasishth)
Date: Fri, 7 Mar 2014 13:18:41 -0600
Subject: [R-sig-ME] likelihood-ratio tests in conflict with coefficiants
 in maximal random effect model
Message-ID: <CA+1m9_6xJDJskDeQ3Axd9Jkgr3Z3VycXYLJN+wpG2R-wkV1u+Q@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140307/79e6cc36/attachment.pl>

From V.J.Wickens at pgr.reading.ac.uk  Fri Mar  7 20:20:19 2014
From: V.J.Wickens at pgr.reading.ac.uk (Victoria Wickens)
Date: Fri, 7 Mar 2014 19:20:19 +0000
Subject: [R-sig-ME] Curved residuals vs fitted plot
Message-ID: <828e0de028ff456282ad24640f6ee915@AM3PR01MB161.eurprd01.prod.exchangelabs.com>

Dear mixed-models group,

Thank you for taking time to read this plea for help.

I wondered if you could give me any advice regarding the curved residuals vs fitted plot with my model:

model3<-glmer(y~Treatment*Distance.m+(1|Field/Field.section/Transect/fffPlot/Bait.ID),
binomial(link=logit),data=dframe1)

My aim is to explain aphid removal from bait trays in response to treatment (presence of absence of a flower strip) and distance (five distances, or plots, moving away from the margins; 1, 5, 10, 15, and 20m). (More experimental details after message in case helpful).

I am using R version 3.0.2 and the latest version of lme4. The resid vs fitted figure is not great, is this because I need to look at a logit fitted scale? (See here to view the figure: <http://imgur.com/CYe5CxC> http://imgur.com/CYe5CxC)

Any help at all would be more than amazing,

Victoria

Experimental setup
There are two fields overall, each contains one treatment (Mit) and one control (Non). Looking at one treatment, in one field section, there are three transects parallel to each other and perpendicular to the margin treatment (T1, T2 and T3). There are five plots along each transect at specific distances from the treatment margins (1, 5, 10, 15, 20m). At each plot, three replica bait trays were placed (a, b and c). Each tray contained 20 aphids glued to foam. In total, I had two farm fields at the largest scale and between them there were 180 bait trays overall (90 per field).


Script attached, also listed below


library(lme4)

Field<-c("L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,
"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B")

Treatment<-c("Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,
"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,
"Mit" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,
"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit")

Field.section<-c("D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"B" ,"B" ,
"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A")

Transect<-c("T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,
"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,
"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,
"T6" ,"T6" ,"T6" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3")

Distance.m<-c(1 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10
,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20)

Aphid.rm.success<-c(3 ,7 ,19 ,19 ,2 ,0 ,0 ,0 ,0 ,0 ,0 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,0 ,0 ,8 ,16 ,2 ,0 ,2 ,1 ,1 ,3 ,0 ,0 ,9 ,10 ,15 ,18 ,1 ,0 ,14 ,10 ,4 ,1 ,3 ,12 ,2 ,4 ,0 ,20 ,17 ,19 ,11 ,19 ,5 ,0 ,0 ,3 ,0 ,0 ,0 ,3 ,11 ,0 ,19 ,13 ,13 ,10 ,9 ,5 ,19 ,1 ,2 ,0 ,12 ,2 ,0 ,1 ,7 ,15 ,19 ,6 ,19 ,18 ,17 ,1 ,0 ,0 ,1 ,1 ,9 ,18 ,11 ,19 ,1 ,3 ,0 ,0 ,1 ,1 ,0 ,1 ,1 ,3 ,0 ,0 ,0 ,4 ,1 ,0 ,0 ,2 ,1 ,6 ,0 ,0 ,0 ,1 ,0 ,0 ,1 ,1 ,0 ,0 ,2 ,1 ,11 ,1 ,0 ,3 ,3 ,0 ,1 ,3 ,0 ,0 ,1 ,0 ,0 ,8 ,1 ,3 ,2 ,1 ,3 ,0 ,
1 ,2 ,0 ,1 ,1 ,0 ,0 ,0 ,8 ,0 ,0 ,0 ,7 ,1 ,0 ,1 ,1 ,0 ,1 ,0 ,1 ,0 ,1 ,5 ,0 ,20 ,0 ,17 ,0 ,10 ,1 ,0 ,5 ,0 ,0 ,0 ,0 ,0)

Aphid.remain.fail<-c(17 ,13 ,1 ,1 ,18 ,20 ,20 ,20 ,20 ,20 ,20 ,19 ,19 ,19 ,17 ,19 ,19 ,19 ,20 ,20 ,12 ,4 ,18 ,20 ,18 ,19 ,19 ,17 ,20 ,20 ,10 ,9 ,5 ,2 ,19 ,20 ,6 ,10 ,16 ,19 ,17 ,8 ,18 ,16 ,20 ,0 ,3 ,1 ,9 ,2 ,15 ,20 ,20 ,17 ,20 ,20 ,20 ,17 ,9 ,20 ,1 ,7 ,7 ,10 ,11 ,15 ,1 ,19 ,18 ,20 ,8 ,18 ,20 ,19 ,13 ,5 ,1 ,14 ,0 ,1 ,3 ,19 ,20 ,20 ,19 ,18 ,11 ,2 ,9 ,0 ,19 ,18 ,20 ,20 ,19 ,19 ,20 ,19 ,19 ,16 ,20 ,20 ,20 ,16 ,19 ,20 ,20 ,18 ,19 ,14 ,20 ,20 ,20 ,19 ,20 ,20 ,19 ,19 ,20 ,20 ,18 ,19 ,
9 ,19 ,19 ,18 ,17 ,20 ,19 ,17 ,20 ,20 ,19 ,20 ,17 ,12 ,19 ,17 ,18 ,19 ,17 ,20 ,19 ,18 ,20 ,18 ,20 ,20 ,19 ,20 ,12 ,20 ,20 ,20 ,13 ,19 ,20 ,19 ,19 ,20 ,20 ,20 ,19 ,20 ,19 ,15 ,20 ,0 ,20 ,3 ,20 ,10 ,19 ,20 ,15 ,20 ,20 ,20 ,20 ,20)

fffPlot<-c("D T1 1" ,"D T1 1" ,"D T1 1" ,"D T1 1" ,"D T1 5" ,"D T1 5" ,"D T1 5" ,"D T1 10" ,"D T1 10" ,"D T1 10" ,"D T1 15" ,"D T1 15" ,"D T1 15" ,"D T1 20" ,"D T1 20" ,"D T1 20" ,"D T2 1" ,"D T2 1" ,"D T2 1" ,"D T2 5" ,"D T2 5" ,"D T2 5" ,"D T2 10" ,"D T2 10" ,"D T2 10" ,"D T2 15" ,"D T2 15" ,"D T2 15" ,"D T2 20" ,"D T2 20" ,"D T2 20" ,"D T3 1" ,"D T3 1" ,"D T3 1" ,"D T3 5" ,"D T3 5" ,"D T3 5" ,"D T3 10" ,"D T3 10" ,"D T3 15" ,"D T3 15" ,"D T3 15" ,"D T3 20" ,"D T3 20" ,"D T3 20" ,
"C T1 1" ,"C T1 1" ,"C T1 1" ,"C T1 5" ,"C T1 5" ,"C T1 5" ,"C T1 10" ,"C T1 10" ,"C T1 10" ,"C T1 15" ,"C T1 15" ,"C T1 15" ,"C T1 20" ,"C T1 20" ,"C T1 20" ,"C T2 1" ,"C T2 1" ,"C T2 1" ,"C T2 5" ,"C T2 5" ,"C T2 5" ,"C T2 10" ,"C T2 10" ,"C T2 10" ,"C T2 15" ,"C T2 15" ,"C T2 15" ,"C T2 20" ,"C T2 20" ,"C T2 20" ,"C T3 1" ,"C T3 1" ,"C T3 1" ,"C T3 5" ,"C T3 5" ,"C T3 5" ,"C T3 10" ,"C T3 10" ,"C T3 10" ,"C T3 15" ,"C T3 15" ,"C T3 15" ,"C T3 20" ,"C T3 20" ,"C T3 20" ,"B T1 1" ,
"B T1 1" ,"B T1 1" ,"B T1 5" ,"B T1 5" ,"B T1 5" ,"B T1 10" ,"B T1 10" ,"B T1 10" ,"B T1 15" ,"B T1 15" ,"B T1 15" ,"B T1 20" ,"B T1 20" ,"B T1 20" ,"B T2 1" ,"B T2 1" ,"B T2 1" ,"B T2 5" ,"B T2 5" ,"B T2 5" ,"B T2 10" ,"B T2 10" ,"B T2 10" ,"B T2 15" ,"B T2 15" ,"B T2 15" ,"B T2 20" ,"B T2 20" ,"B T2 20" ,"B T3 1" ,"B T3 1" ,"B T3 1" ,"B T3 5" ,"B T3 5" ,"B T3 5" ,"B T3 10" ,"B T3 10" ,"B T3 10" ,"B T3 15" ,"B T3 15" ,"B T3 15" ,"B T3 20" ,"B T3 20" ,"B T3 20" ,"A T1 1" ,"A T1 1" ,"A T1 1" ,
"A T1 5" ,"A T1 5" ,"A T1 5" ,"A T1 10" ,"A T1 10" ,"A T1 10" ,"A T1 15" ,"A T1 15" ,"A T1 15" ,"A T1 20" ,"A T1 20" ,"A T1 20" ,"A T2 1" ,"A T2 1" ,"A T2 1" ,"A T2 5" ,"A T2 5" ,"A T2 5" ,"A T2 10" ,"A T2 10" ,"A T2 10" ,"A T2 15" ,"A T2 15" ,"A T2 15" ,"A T2 20" ,"A T2 20" ,"A T2 20" ,"A T3 1" ,"A T3 1" ,"A T3 1" ,"A T3 5" ,"A T3 5" ,"A T3 5" ,"A T3 10" ,"A T3 10" ,"A T3 10" ,"A T3 15" ,"A T3 15" ,"A T3 15" ,"A T3 20" ,"A T3 20" ,"A T3 20")

Bait.ID<-c("ID1" ,"ID2" ,"ID3" ,"ID3" ,"ID4" ,"ID5" ,"ID6" ,"ID7" ,"ID8" ,"ID9" ,"ID10" ,"ID11" ,"ID12" ,"ID13" ,"ID14" ,"ID15" ,"ID16" ,"ID17" ,"ID18" ,"ID19" ,"ID20" ,"ID21" ,"ID22" ,"ID23" ,"ID24" ,"ID25" ,"ID26" ,"ID27" ,"ID28" ,"ID29" ,"ID30" ,"ID31" ,"ID32" ,"ID33" ,"ID34" ,"ID35" ,"ID36" ,"ID37" ,"ID38" ,"ID39" ,"ID40" ,"ID41" ,"ID42" ,"ID43" ,"ID44" ,"ID45" ,"ID46" ,"ID47" ,"ID48" ,"ID49" ,"ID50" ,"ID51" ,"ID52" ,"ID53" ,"ID54" ,"ID55" ,"ID56" ,"ID57" ,"ID58" ,"ID59" ,"ID60" ,"ID61" ,"ID62" ,
"ID63" ,"ID64" ,"ID65" ,"ID66" ,"ID67" ,"ID68" ,"ID69" ,"ID70" ,"ID71" ,"ID72" ,"ID73" ,"ID74" ,"ID75" ,"ID76" ,"ID77" ,"ID78" ,"ID79" ,"ID80" ,"ID81" ,"ID82" ,"ID83" ,"ID84" ,"ID85" ,"ID86" ,"ID87" ,"ID88" ,"ID89" ,"ID90" ,"ID91" ,"ID92" ,"ID93" ,"ID94" ,"ID95" ,"ID96" ,"ID97" ,"ID98" ,"ID99" ,"ID100" ,"ID101" ,"ID102" ,"ID103" ,"ID104" ,"ID105" ,"ID106" ,"ID107" ,"ID108" ,"ID109" ,"ID110" ,"ID111" ,"ID112" ,"ID113" ,"ID114" ,"ID115" ,"ID116" ,"ID117" ,"ID118" ,"ID119" ,"ID120" ,"ID121" ,"ID122" ,
"ID123" ,"ID124" ,"ID125" ,"ID126" ,"ID127" ,"ID128" ,"ID129" ,"ID130" ,"ID131" ,"ID132" ,"ID133" ,"ID134" ,"ID135" ,"ID136" ,"ID137" ,"ID138" ,"ID139" ,"ID140" ,"ID141" ,"ID142" ,"ID143" ,"ID144" ,"ID145" ,"ID146" ,"ID147" ,"ID148" ,"ID149" ,"ID150" ,"ID151" ,"ID152" ,"ID153" ,"ID154" ,"ID155" ,"ID156" ,"ID157" ,"ID158" ,"ID159" ,"ID160" ,"ID161" ,"ID162" ,"ID163" ,"ID164" ,"ID165" ,"ID166" ,"ID167" ,"ID168" ,"ID169" ,"ID170" ,"ID171" ,"ID172" ,"ID173" ,"ID174" ,"ID175" ,"ID176" ,"ID177" ,"ID178" ,
"ID179")


dframe1<-data.frame(Field,Treatment,Field.section,Transect,Distance.m,Aphid.rm.success,Aphid.remain.fail,fffPlot,Bait.ID)

y<-cbind(Aphid.rm.success,Aphid.remain.fail)

model3<-glmer(y~Treatment*Distance.m+(1|Field/Field.section/Transect/fffPlot/Bait.ID), # Bait.ID to remove overdispersion
binomial(link=logit),data=dframe1) # note I kept fffPlot smallest scale in (better residuals)
summary(model3)

plot(model3) # issue? Or just messy ecological data?

______________
Victoria Wickens
PhD student
Room GU08
Centre for Agri-Environmental Research (CAER)
School of Agriculture, Policy & Development
University of Reading
Reading RG6 7BE
____________

Email: v.j.wickens at pgr.reading.ac.uk<mailto:v.j.wickens at pgr.reading.ac.uk>
http://www.reading.ac.uk/caer/staff_students.html
https://www.facebook.com/UniRdgAPD

From paul.johnson at glasgow.ac.uk  Sat Mar  8 02:26:38 2014
From: paul.johnson at glasgow.ac.uk (Paul Johnson)
Date: Sat, 8 Mar 2014 01:26:38 +0000
Subject: [R-sig-ME] Curved residuals vs fitted plot
In-Reply-To: <828e0de028ff456282ad24640f6ee915@AM3PR01MB161.eurprd01.prod.exchangelabs.com>
References: <828e0de028ff456282ad24640f6ee915@AM3PR01MB161.eurprd01.prod.exchangelabs.com>
Message-ID: <DAB55EFEBDAD1F4888EDB2E8F5180BC96C0B0880F6@CMS03.campus.gla.ac.uk>

Hi Victoria,

Positive correlation between the residuals and fitted values is expected whenever you have a row-level random effect to mop up overdispersion. It happens because overdispersion implies that the low responses are lower than predicted and the high responses are higher than predicted, even allowing for binomial sampling error. The gap between a low predicted proportion and a still lower observed proportion is bridged partly by a negative residual and partly by a negative overdispersion random effect (which can be thought of as an additional 'free' residual to the constrained residual due to binomial sampling error). The same happens at the highest predicted proportions, except that both residual and random effect will be positive, hence the positive correlation between the predicted (fitted) values, which contain the overdispersion random effects, and the residuals.

This problem is dealt with, for binomial GLMMs, in chapter 7 of Alain Zuur's Beginner's guide to GLM and GLMM with R. I don't have this book handy and I can't remember his solution. One solution is to shift the overdispersion random effect from the fitted values to the residuals, which makes sense given that they are really just add-on residuals - see the example code below.

This thread discusses the same problem for lognormal-Poisson GLMMs:
https://stat.ethz.ch/pipermail/r-sig-mixed-models/2013q3/020819.html

Best wishes,
Paul

# simulate binomial data with a positive trend and overdispersion
obs <- factor(1:100)
x <- seq(-1, 0.98, 0.02)
set.seed(12345678)
y <- rbinom(100, 20, plogis(x + rnorm(100, 0, 1)))
par(mfrow=c(3, 1))
plot(y/20 ~ x)
# fit logitnormal-binomial GLMM
fit <- glmer(cbind(y, 20-y) ~ I(1:100) + (1|obs), family="binomial")
summary(fit)
# standard residual vs fitted plot
plot(fitted(fit), resid(fit))
abline(h=0)
# shift overdispersion random effect from fitted values to residuals
Fitted <- plogis(qlogis(fitted(fit)) - ranef(fit)$obs[[1]])
Resid <- (y/20 - Fitted) / sqrt(Fitted * (1 - Fitted)/20)
plot(Fitted, Resid)
abline(h=0)
# NB THIS GETS RID OF THE TREND BUT ISN'T RIGHT!
# This is because I've standardised the variance using the 
# variance function for the binomial, p(1-p)/n, but the distribution
# underlying the residuals is now a logitnormal-binomial
# so should incorporate the overdispersion variance. I don't 
# know how to do this.


________________________________________
From: r-sig-mixed-models-bounces at r-project.org [r-sig-mixed-models-bounces at r-project.org] On Behalf Of Victoria Wickens [V.J.Wickens at pgr.reading.ac.uk]
Sent: 07 March 2014 19:20
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Curved residuals vs fitted plot

Dear mixed-models group,

Thank you for taking time to read this plea for help.

I wondered if you could give me any advice regarding the curved residuals vs fitted plot with my model:

model3<-glmer(y~Treatment*Distance.m+(1|Field/Field.section/Transect/fffPlot/Bait.ID),
binomial(link=logit),data=dframe1)

My aim is to explain aphid removal from bait trays in response to treatment (presence of absence of a flower strip) and distance (five distances, or plots, moving away from the margins; 1, 5, 10, 15, and 20m). (More experimental details after message in case helpful).

I am using R version 3.0.2 and the latest version of lme4. The resid vs fitted figure is not great, is this because I need to look at a logit fitted scale? (See here to view the figure: <http://imgur.com/CYe5CxC> http://imgur.com/CYe5CxC)

Any help at all would be more than amazing,

Victoria

Experimental setup
There are two fields overall, each contains one treatment (Mit) and one control (Non). Looking at one treatment, in one field section, there are three transects parallel to each other and perpendicular to the margin treatment (T1, T2 and T3). There are five plots along each transect at specific distances from the treatment margins (1, 5, 10, 15, 20m). At each plot, three replica bait trays were placed (a, b and c). Each tray contained 20 aphids glued to foam. In total, I had two farm fields at the largest scale and between them there were 180 bait trays overall (90 per field).


Script attached, also listed below


library(lme4)

Field<-c("L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,
"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B")

Treatment<-c("Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,
"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,
"Mit" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,
"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit")

Field.section<-c("D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"B" ,"B" ,
"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A")

Transect<-c("T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,
"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,
"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,
"T6" ,"T6" ,"T6" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3")

Distance.m<-c(1 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10
,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20)

Aphid.rm.success<-c(3 ,7 ,19 ,19 ,2 ,0 ,0 ,0 ,0 ,0 ,0 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,0 ,0 ,8 ,16 ,2 ,0 ,2 ,1 ,1 ,3 ,0 ,0 ,9 ,10 ,15 ,18 ,1 ,0 ,14 ,10 ,4 ,1 ,3 ,12 ,2 ,4 ,0 ,20 ,17 ,19 ,11 ,19 ,5 ,0 ,0 ,3 ,0 ,0 ,0 ,3 ,11 ,0 ,19 ,13 ,13 ,10 ,9 ,5 ,19 ,1 ,2 ,0 ,12 ,2 ,0 ,1 ,7 ,15 ,19 ,6 ,19 ,18 ,17 ,1 ,0 ,0 ,1 ,1 ,9 ,18 ,11 ,19 ,1 ,3 ,0 ,0 ,1 ,1 ,0 ,1 ,1 ,3 ,0 ,0 ,0 ,4 ,1 ,0 ,0 ,2 ,1 ,6 ,0 ,0 ,0 ,1 ,0 ,0 ,1 ,1 ,0 ,0 ,2 ,1 ,11 ,1 ,0 ,3 ,3 ,0 ,1 ,3 ,0 ,0 ,1 ,0 ,0 ,8 ,1 ,3 ,2 ,1 ,3 ,0 ,
1 ,2 ,0 ,1 ,1 ,0 ,0 ,0 ,8 ,0 ,0 ,0 ,7 ,1 ,0 ,1 ,1 ,0 ,1 ,0 ,1 ,0 ,1 ,5 ,0 ,20 ,0 ,17 ,0 ,10 ,1 ,0 ,5 ,0 ,0 ,0 ,0 ,0)

Aphid.remain.fail<-c(17 ,13 ,1 ,1 ,18 ,20 ,20 ,20 ,20 ,20 ,20 ,19 ,19 ,19 ,17 ,19 ,19 ,19 ,20 ,20 ,12 ,4 ,18 ,20 ,18 ,19 ,19 ,17 ,20 ,20 ,10 ,9 ,5 ,2 ,19 ,20 ,6 ,10 ,16 ,19 ,17 ,8 ,18 ,16 ,20 ,0 ,3 ,1 ,9 ,2 ,15 ,20 ,20 ,17 ,20 ,20 ,20 ,17 ,9 ,20 ,1 ,7 ,7 ,10 ,11 ,15 ,1 ,19 ,18 ,20 ,8 ,18 ,20 ,19 ,13 ,5 ,1 ,14 ,0 ,1 ,3 ,19 ,20 ,20 ,19 ,18 ,11 ,2 ,9 ,0 ,19 ,18 ,20 ,20 ,19 ,19 ,20 ,19 ,19 ,16 ,20 ,20 ,20 ,16 ,19 ,20 ,20 ,18 ,19 ,14 ,20 ,20 ,20 ,19 ,20 ,20 ,19 ,19 ,20 ,20 ,18 ,19 ,
9 ,19 ,19 ,18 ,17 ,20 ,19 ,17 ,20 ,20 ,19 ,20 ,17 ,12 ,19 ,17 ,18 ,19 ,17 ,20 ,19 ,18 ,20 ,18 ,20 ,20 ,19 ,20 ,12 ,20 ,20 ,20 ,13 ,19 ,20 ,19 ,19 ,20 ,20 ,20 ,19 ,20 ,19 ,15 ,20 ,0 ,20 ,3 ,20 ,10 ,19 ,20 ,15 ,20 ,20 ,20 ,20 ,20)

fffPlot<-c("D T1 1" ,"D T1 1" ,"D T1 1" ,"D T1 1" ,"D T1 5" ,"D T1 5" ,"D T1 5" ,"D T1 10" ,"D T1 10" ,"D T1 10" ,"D T1 15" ,"D T1 15" ,"D T1 15" ,"D T1 20" ,"D T1 20" ,"D T1 20" ,"D T2 1" ,"D T2 1" ,"D T2 1" ,"D T2 5" ,"D T2 5" ,"D T2 5" ,"D T2 10" ,"D T2 10" ,"D T2 10" ,"D T2 15" ,"D T2 15" ,"D T2 15" ,"D T2 20" ,"D T2 20" ,"D T2 20" ,"D T3 1" ,"D T3 1" ,"D T3 1" ,"D T3 5" ,"D T3 5" ,"D T3 5" ,"D T3 10" ,"D T3 10" ,"D T3 15" ,"D T3 15" ,"D T3 15" ,"D T3 20" ,"D T3 20" ,"D T3 20" ,
"C T1 1" ,"C T1 1" ,"C T1 1" ,"C T1 5" ,"C T1 5" ,"C T1 5" ,"C T1 10" ,"C T1 10" ,"C T1 10" ,"C T1 15" ,"C T1 15" ,"C T1 15" ,"C T1 20" ,"C T1 20" ,"C T1 20" ,"C T2 1" ,"C T2 1" ,"C T2 1" ,"C T2 5" ,"C T2 5" ,"C T2 5" ,"C T2 10" ,"C T2 10" ,"C T2 10" ,"C T2 15" ,"C T2 15" ,"C T2 15" ,"C T2 20" ,"C T2 20" ,"C T2 20" ,"C T3 1" ,"C T3 1" ,"C T3 1" ,"C T3 5" ,"C T3 5" ,"C T3 5" ,"C T3 10" ,"C T3 10" ,"C T3 10" ,"C T3 15" ,"C T3 15" ,"C T3 15" ,"C T3 20" ,"C T3 20" ,"C T3 20" ,"B T1 1" ,
"B T1 1" ,"B T1 1" ,"B T1 5" ,"B T1 5" ,"B T1 5" ,"B T1 10" ,"B T1 10" ,"B T1 10" ,"B T1 15" ,"B T1 15" ,"B T1 15" ,"B T1 20" ,"B T1 20" ,"B T1 20" ,"B T2 1" ,"B T2 1" ,"B T2 1" ,"B T2 5" ,"B T2 5" ,"B T2 5" ,"B T2 10" ,"B T2 10" ,"B T2 10" ,"B T2 15" ,"B T2 15" ,"B T2 15" ,"B T2 20" ,"B T2 20" ,"B T2 20" ,"B T3 1" ,"B T3 1" ,"B T3 1" ,"B T3 5" ,"B T3 5" ,"B T3 5" ,"B T3 10" ,"B T3 10" ,"B T3 10" ,"B T3 15" ,"B T3 15" ,"B T3 15" ,"B T3 20" ,"B T3 20" ,"B T3 20" ,"A T1 1" ,"A T1 1" ,"A T1 1" ,
"A T1 5" ,"A T1 5" ,"A T1 5" ,"A T1 10" ,"A T1 10" ,"A T1 10" ,"A T1 15" ,"A T1 15" ,"A T1 15" ,"A T1 20" ,"A T1 20" ,"A T1 20" ,"A T2 1" ,"A T2 1" ,"A T2 1" ,"A T2 5" ,"A T2 5" ,"A T2 5" ,"A T2 10" ,"A T2 10" ,"A T2 10" ,"A T2 15" ,"A T2 15" ,"A T2 15" ,"A T2 20" ,"A T2 20" ,"A T2 20" ,"A T3 1" ,"A T3 1" ,"A T3 1" ,"A T3 5" ,"A T3 5" ,"A T3 5" ,"A T3 10" ,"A T3 10" ,"A T3 10" ,"A T3 15" ,"A T3 15" ,"A T3 15" ,"A T3 20" ,"A T3 20" ,"A T3 20")

Bait.ID<-c("ID1" ,"ID2" ,"ID3" ,"ID3" ,"ID4" ,"ID5" ,"ID6" ,"ID7" ,"ID8" ,"ID9" ,"ID10" ,"ID11" ,"ID12" ,"ID13" ,"ID14" ,"ID15" ,"ID16" ,"ID17" ,"ID18" ,"ID19" ,"ID20" ,"ID21" ,"ID22" ,"ID23" ,"ID24" ,"ID25" ,"ID26" ,"ID27" ,"ID28" ,"ID29" ,"ID30" ,"ID31" ,"ID32" ,"ID33" ,"ID34" ,"ID35" ,"ID36" ,"ID37" ,"ID38" ,"ID39" ,"ID40" ,"ID41" ,"ID42" ,"ID43" ,"ID44" ,"ID45" ,"ID46" ,"ID47" ,"ID48" ,"ID49" ,"ID50" ,"ID51" ,"ID52" ,"ID53" ,"ID54" ,"ID55" ,"ID56" ,"ID57" ,"ID58" ,"ID59" ,"ID60" ,"ID61" ,"ID62" ,
"ID63" ,"ID64" ,"ID65" ,"ID66" ,"ID67" ,"ID68" ,"ID69" ,"ID70" ,"ID71" ,"ID72" ,"ID73" ,"ID74" ,"ID75" ,"ID76" ,"ID77" ,"ID78" ,"ID79" ,"ID80" ,"ID81" ,"ID82" ,"ID83" ,"ID84" ,"ID85" ,"ID86" ,"ID87" ,"ID88" ,"ID89" ,"ID90" ,"ID91" ,"ID92" ,"ID93" ,"ID94" ,"ID95" ,"ID96" ,"ID97" ,"ID98" ,"ID99" ,"ID100" ,"ID101" ,"ID102" ,"ID103" ,"ID104" ,"ID105" ,"ID106" ,"ID107" ,"ID108" ,"ID109" ,"ID110" ,"ID111" ,"ID112" ,"ID113" ,"ID114" ,"ID115" ,"ID116" ,"ID117" ,"ID118" ,"ID119" ,"ID120" ,"ID121" ,"ID122" ,
"ID123" ,"ID124" ,"ID125" ,"ID126" ,"ID127" ,"ID128" ,"ID129" ,"ID130" ,"ID131" ,"ID132" ,"ID133" ,"ID134" ,"ID135" ,"ID136" ,"ID137" ,"ID138" ,"ID139" ,"ID140" ,"ID141" ,"ID142" ,"ID143" ,"ID144" ,"ID145" ,"ID146" ,"ID147" ,"ID148" ,"ID149" ,"ID150" ,"ID151" ,"ID152" ,"ID153" ,"ID154" ,"ID155" ,"ID156" ,"ID157" ,"ID158" ,"ID159" ,"ID160" ,"ID161" ,"ID162" ,"ID163" ,"ID164" ,"ID165" ,"ID166" ,"ID167" ,"ID168" ,"ID169" ,"ID170" ,"ID171" ,"ID172" ,"ID173" ,"ID174" ,"ID175" ,"ID176" ,"ID177" ,"ID178" ,
"ID179")


dframe1<-data.frame(Field,Treatment,Field.section,Transect,Distance.m,Aphid.rm.success,Aphid.remain.fail,fffPlot,Bait.ID)

y<-cbind(Aphid.rm.success,Aphid.remain.fail)

model3<-glmer(y~Treatment*Distance.m+(1|Field/Field.section/Transect/fffPlot/Bait.ID), # Bait.ID to remove overdispersion
binomial(link=logit),data=dframe1) # note I kept fffPlot smallest scale in (better residuals)
summary(model3)

plot(model3) # issue? Or just messy ecological data?

______________
Victoria Wickens
PhD student
Room GU08
Centre for Agri-Environmental Research (CAER)
School of Agriculture, Policy & Development
University of Reading
Reading RG6 7BE
____________

Email: v.j.wickens at pgr.reading.ac.uk<mailto:v.j.wickens at pgr.reading.ac.uk>
http://www.reading.ac.uk/caer/staff_students.html
https://www.facebook.com/UniRdgAPD

From bbolker at gmail.com  Sat Mar  8 23:24:52 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 08 Mar 2014 17:24:52 -0500
Subject: [R-sig-ME] gee, geese and glmer
In-Reply-To: <007801cf3a86$6d27b060$47771120$@bu.edu>
References: <007801cf3a86$6d27b060$47771120$@bu.edu>
Message-ID: <531B98B4.4050403@gmail.com>

On 14-03-07 11:25 PM, Ming-Huei Chen wrote:
> Hi Ben,
> 
>  
> 
> In an analysis we found that glmer in new lme4 gave result different
> from old lme4, gee and geese, where old lme4 seems to be closer to gee
> and geese.. Please see highlighted sex effect below. Case by sex (2x2)
> table is also given. Can you please let us know how would you look into
> the results? Thanks!
> 

   [cc'ing to r-sig-mixed-models: **please** try r-sig-mixed-models
first, not personal e-mail to me ...]

  I can't say exactly what's going here; without having a reproducible
example
<http://tinyurl.com/reproducible-000> it's hard to say precisely.  Thoughts:

 * gee and geese are giving _exactly_ the same parameter estimates, to
8 significant digits, so I would guess they are wrapping identical
underlying methods.

 * As far as diagnosing the issue with lme4 1.0-6:
   * does changing the optimization method, i.e.
 glmerControl(optimizer="optimx",optCtrl=list(method="nlminb"))
   [must do library("optimx") first] or
 glmerControl(optimizer="bobyqa")

  change the result?

 * I would be curious whether the soon-to-be-released version 1.1-4
(which can be installed from github or lme4.r-forge.r-project.org/repos)
gives either (1) convergence warnings or (2) different/better answers

 * You can try specifying the starting values for lme4 to diagnose
misconvergence; for example, start lme4 from the estimates given
by old lme4/lme4.0 and see if it gives a similar answer.

 * You can use the 'slice' and 'splom.slice' functions from
bbmle to visualize the likelihood surfaces

  good luck,
   Ben Bolker

> Ming-Huei
> 

> ###GEE
> 
>> summary(gee(case~sex+PC1+PC2+PC3+PC4,id=famid,family=binomial,data=data))$coef
>                Estimate Naive S.E.     Naive z Robust S.E.   Robust z
> (Intercept) -1.88047373 0.13532162 -13.8963286  0.15960440 -11.782092
> sex         -0.23436854 0.08611269  -2.7216494  0.09050577  -2.589543
> PC1         -0.05478639 0.06195318  -0.8843192  0.06822178  -0.803063
> PC2         -0.09934572 0.06494563  -1.5296753  0.06520811  -1.523518
> PC3         -0.07020391 0.06626875  -1.0593818  0.06962147  -1.008366
> PC4         -0.13413097 0.06746716  -1.9880927  0.06979901  -1.921674
> 

> ###GEESE
> 
>> summary(geese(case~sex+PC1+PC2+PC3+PC4,id=famid,family=binomial,data=data))$mean
> 
>                estimate     san.se        wald           p
> 
> (Intercept) -1.88047373 0.15960440 138.8176912 0.000000000
> sex         -0.23436854 0.09050577   6.7057312 0.009610351
> PC1         -0.05478639 0.06822178   0.6449102 0.421938319
> PC2         -0.09934572 0.06520811   2.3211071 0.127629159
> PC3         -0.07020391 0.06962147   1.0168016 0.313278888
> PC4         -0.13413097 0.06979901   3.6928324 0.054646745
> 
> ### lme4_0.999999-2
> 
>> summary(glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data=data))
>             Estimate Std. Error z value Pr(>|z|)
> (Intercept) -3.01599    0.28305 -10.655   <2e-16 ***
> sex         -0.41056    0.16285  -2.521   0.0117 *
> PC1         -0.17116    0.12903  -1.326   0.1847
> PC2         -0.15510    0.13382  -1.159   0.2465
> PC3         -0.19044    0.13580  -1.402   0.1608
> PC4          0.02532    0.13732   0.184   0.8537
> 
> ###lme4_1.0-6
> 
>> summary(glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data=data))
> 
>             Estimate Std. Error z value Pr(>|z|)
> 
> (Intercept) -10.2784     0.8631 -11.909   <2e-16 ***
> sex           0.3497     0.1975   1.770   0.0767 .
> PC1          -0.3555     0.1623  -2.190   0.0285 *
> PC2          -0.1087     0.1653  -0.657   0.5109
> PC3          -0.2242     0.1652  -1.357   0.1748
> PC4           0.1103     0.1671   0.660   0.5091
> 
> Case by sex
> 
>         1    2
>   0   2554 3021
>   1    310  290
>


From V.J.Wickens at pgr.reading.ac.uk  Mon Mar 10 15:29:01 2014
From: V.J.Wickens at pgr.reading.ac.uk (Victoria Wickens)
Date: Mon, 10 Mar 2014 14:29:01 +0000
Subject: [R-sig-ME] Curved residuals vs fitted plot
In-Reply-To: <DAB55EFEBDAD1F4888EDB2E8F5180BC96C0B0880F6@CMS03.campus.gla.ac.uk>
References: <828e0de028ff456282ad24640f6ee915@AM3PR01MB161.eurprd01.prod.exchangelabs.com>,
	<DAB55EFEBDAD1F4888EDB2E8F5180BC96C0B0880F6@CMS03.campus.gla.ac.uk>
Message-ID: <88612d7954b8480089e4ed643bb9a509@AM3PR01MB161.eurprd01.prod.exchangelabs.com>

Hi Paul,

Thank you for reading my email and for providing clear explanation of why I have the curvature. I found your response and the link very helpful. Sadly I am having trouble getting the R book which is not available at my library, but I do have a copy of the Mixed Effects Models and Extensions in Ecology with R by Zuur.

I now understand the fitted vs resid plots produce curved scatter trends due to the observational random effect and have implemented the code you provided to shift the effect from the fitted values to the residuals to create a new plot (see here http://imgur.com/edit).

My plot is not nearly as well behaved as your example though, with a steep tail of residuals with high fitted values. Does this mean my model is a bad fit? I'm not sure of the next best move to take.

I have tried the following (all with the observational random effect, Bait.ID, to remove overdispersion but with similar resid vs fitted plots as seen on the web-link):

1. glmer
2. glmer + polynomial regression (logDistance factor) 
3. glmer + quadratic term (Distance^2)
4. gamm (limited success)

Is there another way to remove overdispersion or account for it? Or am I going about this wrong?

Thank you once again for taking time to help me. I really appreciate it,

Victoria
______________
Victoria Wickens
PhD student
Room GU08
Centre for Agri-Environmental Research (CAER)
School of Agriculture, Policy & Development
University of Reading
Reading RG6 7BE
____________

Email: v.j.wickens at pgr.reading.ac.uk
http://www.reading.ac.uk/caer/staff_students.html
https://www.facebook.com/UniRdgAPD

________________________________________
From: Paul Johnson <paul.johnson at glasgow.ac.uk>
Sent: 08 March 2014 01:26
To: Victoria Wickens; r-sig-mixed-models at r-project.org
Subject: RE: Curved residuals vs fitted plot

Hi Victoria,

Positive correlation between the residuals and fitted values is expected whenever you have a row-level random effect to mop up overdispersion. It happens because overdispersion implies that the low responses are lower than predicted and the high responses are higher than predicted, even allowing for binomial sampling error. The gap between a low predicted proportion and a still lower observed proportion is bridged partly by a negative residual and partly by a negative overdispersion random effect (which can be thought of as an additional 'free' residual to the constrained residual due to binomial sampling error). The same happens at the highest predicted proportions, except that both residual and random effect will be positive, hence the positive correlation between the predicted (fitted) values, which contain the overdispersion random effects, and the residuals.

This problem is dealt with, for binomial GLMMs, in chapter 7 of Alain Zuur's Beginner's guide to GLM and GLMM with R. I don't have this book handy and I can't remember his solution. One solution is to shift the overdispersion random effect from the fitted values to the residuals, which makes sense given that they are really just add-on residuals - see the example code below.

This thread discusses the same problem for lognormal-Poisson GLMMs:
https://stat.ethz.ch/pipermail/r-sig-mixed-models/2013q3/020819.html

Best wishes,
Paul

# simulate binomial data with a positive trend and overdispersion
obs <- factor(1:100)
x <- seq(-1, 0.98, 0.02)
set.seed(12345678)
y <- rbinom(100, 20, plogis(x + rnorm(100, 0, 1)))
par(mfrow=c(3, 1))
plot(y/20 ~ x)
# fit logitnormal-binomial GLMM
fit <- glmer(cbind(y, 20-y) ~ I(1:100) + (1|obs), family="binomial")
summary(fit)
# standard residual vs fitted plot
plot(fitted(fit), resid(fit))
abline(h=0)
# shift overdispersion random effect from fitted values to residuals
Fitted <- plogis(qlogis(fitted(fit)) - ranef(fit)$obs[[1]])
Resid <- (y/20 - Fitted) / sqrt(Fitted * (1 - Fitted)/20)
plot(Fitted, Resid)
abline(h=0)
# NB THIS GETS RID OF THE TREND BUT ISN'T RIGHT!
# This is because I've standardised the variance using the
# variance function for the binomial, p(1-p)/n, but the distribution
# underlying the residuals is now a logitnormal-binomial
# so should incorporate the overdispersion variance. I don't
# know how to do this.


________________________________________
From: r-sig-mixed-models-bounces at r-project.org [r-sig-mixed-models-bounces at r-project.org] On Behalf Of Victoria Wickens [V.J.Wickens at pgr.reading.ac.uk]
Sent: 07 March 2014 19:20
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Curved residuals vs fitted plot

Dear mixed-models group,

Thank you for taking time to read this plea for help.

I wondered if you could give me any advice regarding the curved residuals vs fitted plot with my model:

model3<-glmer(y~Treatment*Distance.m+(1|Field/Field.section/Transect/fffPlot/Bait.ID),
binomial(link=logit),data=dframe1)

My aim is to explain aphid removal from bait trays in response to treatment (presence of absence of a flower strip) and distance (five distances, or plots, moving away from the margins; 1, 5, 10, 15, and 20m). (More experimental details after message in case helpful).

I am using R version 3.0.2 and the latest version of lme4. The resid vs fitted figure is not great, is this because I need to look at a logit fitted scale? (See here to view the figure: <http://imgur.com/CYe5CxC> http://imgur.com/CYe5CxC)

Any help at all would be more than amazing,

Victoria

Experimental setup
There are two fields overall, each contains one treatment (Mit) and one control (Non). Looking at one treatment, in one field section, there are three transects parallel to each other and perpendicular to the margin treatment (T1, T2 and T3). There are five plots along each transect at specific distances from the treatment margins (1, 5, 10, 15, 20m). At each plot, three replica bait trays were placed (a, b and c). Each tray contained 20 aphids glued to foam. In total, I had two farm fields at the largest scale and between them there were 180 bait trays overall (90 per field).


Script attached, also listed below


library(lme4)

Field<-c("L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,
"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B")

Treatment<-c("Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,
"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,
"Mit" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,
"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit")

Field.section<-c("D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"B" ,"B" ,
"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A")

Transect<-c("T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,
"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,
"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,
"T6" ,"T6" ,"T6" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3")

Distance.m<-c(1 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10
,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20)

Aphid.rm.success<-c(3 ,7 ,19 ,19 ,2 ,0 ,0 ,0 ,0 ,0 ,0 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,0 ,0 ,8 ,16 ,2 ,0 ,2 ,1 ,1 ,3 ,0 ,0 ,9 ,10 ,15 ,18 ,1 ,0 ,14 ,10 ,4 ,1 ,3 ,12 ,2 ,4 ,0 ,20 ,17 ,19 ,11 ,19 ,5 ,0 ,0 ,3 ,0 ,0 ,0 ,3 ,11 ,0 ,19 ,13 ,13 ,10 ,9 ,5 ,19 ,1 ,2 ,0 ,12 ,2 ,0 ,1 ,7 ,15 ,19 ,6 ,19 ,18 ,17 ,1 ,0 ,0 ,1 ,1 ,9 ,18 ,11 ,19 ,1 ,3 ,0 ,0 ,1 ,1 ,0 ,1 ,1 ,3 ,0 ,0 ,0 ,4 ,1 ,0 ,0 ,2 ,1 ,6 ,0 ,0 ,0 ,1 ,0 ,0 ,1 ,1 ,0 ,0 ,2 ,1 ,11 ,1 ,0 ,3 ,3 ,0 ,1 ,3 ,0 ,0 ,1 ,0 ,0 ,8 ,1 ,3 ,2 ,1 ,3 ,0 ,
1 ,2 ,0 ,1 ,1 ,0 ,0 ,0 ,8 ,0 ,0 ,0 ,7 ,1 ,0 ,1 ,1 ,0 ,1 ,0 ,1 ,0 ,1 ,5 ,0 ,20 ,0 ,17 ,0 ,10 ,1 ,0 ,5 ,0 ,0 ,0 ,0 ,0)

Aphid.remain.fail<-c(17 ,13 ,1 ,1 ,18 ,20 ,20 ,20 ,20 ,20 ,20 ,19 ,19 ,19 ,17 ,19 ,19 ,19 ,20 ,20 ,12 ,4 ,18 ,20 ,18 ,19 ,19 ,17 ,20 ,20 ,10 ,9 ,5 ,2 ,19 ,20 ,6 ,10 ,16 ,19 ,17 ,8 ,18 ,16 ,20 ,0 ,3 ,1 ,9 ,2 ,15 ,20 ,20 ,17 ,20 ,20 ,20 ,17 ,9 ,20 ,1 ,7 ,7 ,10 ,11 ,15 ,1 ,19 ,18 ,20 ,8 ,18 ,20 ,19 ,13 ,5 ,1 ,14 ,0 ,1 ,3 ,19 ,20 ,20 ,19 ,18 ,11 ,2 ,9 ,0 ,19 ,18 ,20 ,20 ,19 ,19 ,20 ,19 ,19 ,16 ,20 ,20 ,20 ,16 ,19 ,20 ,20 ,18 ,19 ,14 ,20 ,20 ,20 ,19 ,20 ,20 ,19 ,19 ,20 ,20 ,18 ,19 ,
9 ,19 ,19 ,18 ,17 ,20 ,19 ,17 ,20 ,20 ,19 ,20 ,17 ,12 ,19 ,17 ,18 ,19 ,17 ,20 ,19 ,18 ,20 ,18 ,20 ,20 ,19 ,20 ,12 ,20 ,20 ,20 ,13 ,19 ,20 ,19 ,19 ,20 ,20 ,20 ,19 ,20 ,19 ,15 ,20 ,0 ,20 ,3 ,20 ,10 ,19 ,20 ,15 ,20 ,20 ,20 ,20 ,20)

fffPlot<-c("D T1 1" ,"D T1 1" ,"D T1 1" ,"D T1 1" ,"D T1 5" ,"D T1 5" ,"D T1 5" ,"D T1 10" ,"D T1 10" ,"D T1 10" ,"D T1 15" ,"D T1 15" ,"D T1 15" ,"D T1 20" ,"D T1 20" ,"D T1 20" ,"D T2 1" ,"D T2 1" ,"D T2 1" ,"D T2 5" ,"D T2 5" ,"D T2 5" ,"D T2 10" ,"D T2 10" ,"D T2 10" ,"D T2 15" ,"D T2 15" ,"D T2 15" ,"D T2 20" ,"D T2 20" ,"D T2 20" ,"D T3 1" ,"D T3 1" ,"D T3 1" ,"D T3 5" ,"D T3 5" ,"D T3 5" ,"D T3 10" ,"D T3 10" ,"D T3 15" ,"D T3 15" ,"D T3 15" ,"D T3 20" ,"D T3 20" ,"D T3 20" ,
"C T1 1" ,"C T1 1" ,"C T1 1" ,"C T1 5" ,"C T1 5" ,"C T1 5" ,"C T1 10" ,"C T1 10" ,"C T1 10" ,"C T1 15" ,"C T1 15" ,"C T1 15" ,"C T1 20" ,"C T1 20" ,"C T1 20" ,"C T2 1" ,"C T2 1" ,"C T2 1" ,"C T2 5" ,"C T2 5" ,"C T2 5" ,"C T2 10" ,"C T2 10" ,"C T2 10" ,"C T2 15" ,"C T2 15" ,"C T2 15" ,"C T2 20" ,"C T2 20" ,"C T2 20" ,"C T3 1" ,"C T3 1" ,"C T3 1" ,"C T3 5" ,"C T3 5" ,"C T3 5" ,"C T3 10" ,"C T3 10" ,"C T3 10" ,"C T3 15" ,"C T3 15" ,"C T3 15" ,"C T3 20" ,"C T3 20" ,"C T3 20" ,"B T1 1" ,
"B T1 1" ,"B T1 1" ,"B T1 5" ,"B T1 5" ,"B T1 5" ,"B T1 10" ,"B T1 10" ,"B T1 10" ,"B T1 15" ,"B T1 15" ,"B T1 15" ,"B T1 20" ,"B T1 20" ,"B T1 20" ,"B T2 1" ,"B T2 1" ,"B T2 1" ,"B T2 5" ,"B T2 5" ,"B T2 5" ,"B T2 10" ,"B T2 10" ,"B T2 10" ,"B T2 15" ,"B T2 15" ,"B T2 15" ,"B T2 20" ,"B T2 20" ,"B T2 20" ,"B T3 1" ,"B T3 1" ,"B T3 1" ,"B T3 5" ,"B T3 5" ,"B T3 5" ,"B T3 10" ,"B T3 10" ,"B T3 10" ,"B T3 15" ,"B T3 15" ,"B T3 15" ,"B T3 20" ,"B T3 20" ,"B T3 20" ,"A T1 1" ,"A T1 1" ,"A T1 1" ,
"A T1 5" ,"A T1 5" ,"A T1 5" ,"A T1 10" ,"A T1 10" ,"A T1 10" ,"A T1 15" ,"A T1 15" ,"A T1 15" ,"A T1 20" ,"A T1 20" ,"A T1 20" ,"A T2 1" ,"A T2 1" ,"A T2 1" ,"A T2 5" ,"A T2 5" ,"A T2 5" ,"A T2 10" ,"A T2 10" ,"A T2 10" ,"A T2 15" ,"A T2 15" ,"A T2 15" ,"A T2 20" ,"A T2 20" ,"A T2 20" ,"A T3 1" ,"A T3 1" ,"A T3 1" ,"A T3 5" ,"A T3 5" ,"A T3 5" ,"A T3 10" ,"A T3 10" ,"A T3 10" ,"A T3 15" ,"A T3 15" ,"A T3 15" ,"A T3 20" ,"A T3 20" ,"A T3 20")

Bait.ID<-c("ID1" ,"ID2" ,"ID3" ,"ID3" ,"ID4" ,"ID5" ,"ID6" ,"ID7" ,"ID8" ,"ID9" ,"ID10" ,"ID11" ,"ID12" ,"ID13" ,"ID14" ,"ID15" ,"ID16" ,"ID17" ,"ID18" ,"ID19" ,"ID20" ,"ID21" ,"ID22" ,"ID23" ,"ID24" ,"ID25" ,"ID26" ,"ID27" ,"ID28" ,"ID29" ,"ID30" ,"ID31" ,"ID32" ,"ID33" ,"ID34" ,"ID35" ,"ID36" ,"ID37" ,"ID38" ,"ID39" ,"ID40" ,"ID41" ,"ID42" ,"ID43" ,"ID44" ,"ID45" ,"ID46" ,"ID47" ,"ID48" ,"ID49" ,"ID50" ,"ID51" ,"ID52" ,"ID53" ,"ID54" ,"ID55" ,"ID56" ,"ID57" ,"ID58" ,"ID59" ,"ID60" ,"ID61" ,"ID62" ,
"ID63" ,"ID64" ,"ID65" ,"ID66" ,"ID67" ,"ID68" ,"ID69" ,"ID70" ,"ID71" ,"ID72" ,"ID73" ,"ID74" ,"ID75" ,"ID76" ,"ID77" ,"ID78" ,"ID79" ,"ID80" ,"ID81" ,"ID82" ,"ID83" ,"ID84" ,"ID85" ,"ID86" ,"ID87" ,"ID88" ,"ID89" ,"ID90" ,"ID91" ,"ID92" ,"ID93" ,"ID94" ,"ID95" ,"ID96" ,"ID97" ,"ID98" ,"ID99" ,"ID100" ,"ID101" ,"ID102" ,"ID103" ,"ID104" ,"ID105" ,"ID106" ,"ID107" ,"ID108" ,"ID109" ,"ID110" ,"ID111" ,"ID112" ,"ID113" ,"ID114" ,"ID115" ,"ID116" ,"ID117" ,"ID118" ,"ID119" ,"ID120" ,"ID121" ,"ID122" ,
"ID123" ,"ID124" ,"ID125" ,"ID126" ,"ID127" ,"ID128" ,"ID129" ,"ID130" ,"ID131" ,"ID132" ,"ID133" ,"ID134" ,"ID135" ,"ID136" ,"ID137" ,"ID138" ,"ID139" ,"ID140" ,"ID141" ,"ID142" ,"ID143" ,"ID144" ,"ID145" ,"ID146" ,"ID147" ,"ID148" ,"ID149" ,"ID150" ,"ID151" ,"ID152" ,"ID153" ,"ID154" ,"ID155" ,"ID156" ,"ID157" ,"ID158" ,"ID159" ,"ID160" ,"ID161" ,"ID162" ,"ID163" ,"ID164" ,"ID165" ,"ID166" ,"ID167" ,"ID168" ,"ID169" ,"ID170" ,"ID171" ,"ID172" ,"ID173" ,"ID174" ,"ID175" ,"ID176" ,"ID177" ,"ID178" ,
"ID179")


dframe1<-data.frame(Field,Treatment,Field.section,Transect,Distance.m,Aphid.rm.success,Aphid.remain.fail,fffPlot,Bait.ID)

y<-cbind(Aphid.rm.success,Aphid.remain.fail)

model3<-glmer(y~Treatment*Distance.m+(1|Field/Field.section/Transect/fffPlot/Bait.ID), # Bait.ID to remove overdispersion
binomial(link=logit),data=dframe1) # note I kept fffPlot smallest scale in (better residuals)
summary(model3)

plot(model3) # issue? Or just messy ecological data?

______________
Victoria Wickens
PhD student
Room GU08
Centre for Agri-Environmental Research (CAER)
School of Agriculture, Policy & Development
University of Reading
Reading RG6 7BE
____________

Email: v.j.wickens at pgr.reading.ac.uk<mailto:v.j.wickens at pgr.reading.ac.uk>
http://www.reading.ac.uk/caer/staff_students.html
https://www.facebook.com/UniRdgAPD

From msaussac at chu-clermontferrand.fr  Mon Mar 10 14:46:33 2014
From: msaussac at chu-clermontferrand.fr (Saussac Mathilde)
Date: Mon, 10 Mar 2014 14:46:33 +0100
Subject: [R-sig-ME] problem with lme4
Message-ID: <0523A5CE5D1EED4D9A9846E530EE382D040C7A4EEC7C@E2K7-CCR.chu-clermontferrand.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140310/7aacd3d0/attachment.pl>

From highstat at highstat.com  Mon Mar 10 21:04:48 2014
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Mon, 10 Mar 2014 21:04:48 +0100
Subject: [R-sig-ME] Course: Beginner's Guide to MCMC,
	mixed models and GLMM with R
Message-ID: <531E1AE0.1000507@highstat.com>

We would like to announce the following stats course:

Course: Introduction to MCMC, Linear mixed effects models and GLMM with R
Where: CIMA Research Foundation, Savona, Italy
URL flyer:   http://www.highstat.com/Courses/Flyer2014_05CIMA.pdf
Course website: http://www.highstat.com/statscourse.htm


Kind regards,

Alain


-- 
Dr. Alain F. Zuur

First author of:
1. Beginner's Guide to GAMM with R (2014).
2. Beginner's Guide to GLM and GLMM with R (2013).
3. Begginner's Guide to GAM with R (2012).
4. Zero Inflated Models and GLMM with R (2012).
5. A Beginner's Guide to R (2009).
6. Mixed effects models and extensions in ecology with R (2009).
7. Analysing Ecological Data (2007).

Highland Statistics Ltd.
9 St Clair Wynd
UK - AB41 6DZ Newburgh
Tel:   0044 1358 788177
Email: highstat at highstat.com
URL:   www.highstat.com


From mhchen at bu.edu  Mon Mar 10 19:36:03 2014
From: mhchen at bu.edu (Ming-Huei Chen)
Date: Mon, 10 Mar 2014 14:36:03 -0400
Subject: [R-sig-ME] gee, geese and glmer
In-Reply-To: <531B98B4.4050403@gmail.com>
References: <007801cf3a86$6d27b060$47771120$@bu.edu>
	<531B98B4.4050403@gmail.com>
Message-ID: <005301cf3c8f$9a9a7f40$cfcf7dc0$@bu.edu>

Thanks, Ben!

Changing optimization method does not change results for lme4 1.0-6. I will
install 1.1-4 version and get back to you.

Best,

Ming-Huei

-----Original Message-----
From: Ben Bolker [mailto:bbolker at gmail.com] 
Sent: Saturday, March 8, 2014 5:25 PM
To: r-sig-mixed-models at r-project.org
Cc: Ming-Huei Chen; 'Yang, Qiong'
Subject: Re: gee, geese and glmer

On 14-03-07 11:25 PM, Ming-Huei Chen wrote:
> Hi Ben,
> 
>  
> 
> In an analysis we found that glmer in new lme4 gave result different 
> from old lme4, gee and geese, where old lme4 seems to be closer to gee 
> and geese.. Please see highlighted sex effect below. Case by sex (2x2) 
> table is also given. Can you please let us know how would you look 
> into the results? Thanks!
> 

   [cc'ing to r-sig-mixed-models: **please** try r-sig-mixed-models first,
not personal e-mail to me ...]

  I can't say exactly what's going here; without having a reproducible
example <http://tinyurl.com/reproducible-000> it's hard to say precisely.
Thoughts:

 * gee and geese are giving _exactly_ the same parameter estimates, to
8 significant digits, so I would guess they are wrapping identical
underlying methods.

 * As far as diagnosing the issue with lme4 1.0-6:
   * does changing the optimization method, i.e.
 glmerControl(optimizer="optimx",optCtrl=list(method="nlminb"))
   [must do library("optimx") first] or
 glmerControl(optimizer="bobyqa")

  change the result?

 * I would be curious whether the soon-to-be-released version 1.1-4 (which
can be installed from github or lme4.r-forge.r-project.org/repos) gives
either (1) convergence warnings or (2) different/better answers

 * You can try specifying the starting values for lme4 to diagnose
misconvergence; for example, start lme4 from the estimates given by old
lme4/lme4.0 and see if it gives a similar answer.

 * You can use the 'slice' and 'splom.slice' functions from bbmle to
visualize the likelihood surfaces

  good luck,
   Ben Bolker

> Ming-Huei
> 

> ###GEE
> 
>> summary(gee(case~sex+PC1+PC2+PC3+PC4,id=famid,family=binomial,data=da
>> ta))$coef
>                Estimate Naive S.E.     Naive z Robust S.E.   Robust z
> (Intercept) -1.88047373 0.13532162 -13.8963286  0.15960440 -11.782092
> sex         -0.23436854 0.08611269  -2.7216494  0.09050577  -2.589543
> PC1         -0.05478639 0.06195318  -0.8843192  0.06822178  -0.803063
> PC2         -0.09934572 0.06494563  -1.5296753  0.06520811  -1.523518
> PC3         -0.07020391 0.06626875  -1.0593818  0.06962147  -1.008366
> PC4         -0.13413097 0.06746716  -1.9880927  0.06979901  -1.921674
> 

> ###GEESE
> 
>> summary(geese(case~sex+PC1+PC2+PC3+PC4,id=famid,family=binomial,data=
>> data))$mean
> 
>                estimate     san.se        wald           p
> 
> (Intercept) -1.88047373 0.15960440 138.8176912 0.000000000
> sex         -0.23436854 0.09050577   6.7057312 0.009610351
> PC1         -0.05478639 0.06822178   0.6449102 0.421938319
> PC2         -0.09934572 0.06520811   2.3211071 0.127629159
> PC3         -0.07020391 0.06962147   1.0168016 0.313278888
> PC4         -0.13413097 0.06979901   3.6928324 0.054646745
> 
> ### lme4_0.999999-2
> 
>> summary(glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data
>> =data))
>             Estimate Std. Error z value Pr(>|z|)
> (Intercept) -3.01599    0.28305 -10.655   <2e-16 ***
> sex         -0.41056    0.16285  -2.521   0.0117 *
> PC1         -0.17116    0.12903  -1.326   0.1847
> PC2         -0.15510    0.13382  -1.159   0.2465
> PC3         -0.19044    0.13580  -1.402   0.1608
> PC4          0.02532    0.13732   0.184   0.8537
> 
> ###lme4_1.0-6
> 
>> summary(glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data
>> =data))
> 
>             Estimate Std. Error z value Pr(>|z|)
> 
> (Intercept) -10.2784     0.8631 -11.909   <2e-16 ***
> sex           0.3497     0.1975   1.770   0.0767 .
> PC1          -0.3555     0.1623  -2.190   0.0285 *
> PC2          -0.1087     0.1653  -0.657   0.5109
> PC3          -0.2242     0.1652  -1.357   0.1748
> PC4           0.1103     0.1671   0.660   0.5091
> 
> Case by sex
> 
>         1    2
>   0   2554 3021
>   1    310  290
>


From qyang at bu.edu  Mon Mar 10 19:19:56 2014
From: qyang at bu.edu (Yang, Qiong)
Date: Mon, 10 Mar 2014 18:19:56 +0000
Subject: [R-sig-ME] gee, geese and glmer
In-Reply-To: <531B98B4.4050403@gmail.com>
References: <007801cf3a86$6d27b060$47771120$@bu.edu>
	<531B98B4.4050403@gmail.com>
Message-ID: <6984CA5898097D459F8EC753F025B0D839696447@IST-EX10MBX-3.ad.bu.edu>

Hi Ben,

We wonder if you can add an option in lmer() of current lme4 version to call the algorithm used in lme4_0.999999-2?

For our package (analyze rare genetic variant) to be put on CRAN, we need to use current version of lme4. However, at this point, there are still issues that cannot be resolved with newer versions of lme4. It is very difficult for us to keep waiting and testing the new release, and hope all the issues resolved and no new issues coming up. lme4_0.999999-2 has been used by us for a long time with little problem. Your help on this is highly appreciated.
Best,
Qiong

-----Original Message-----
From: Ben Bolker [mailto:bbolker at gmail.com] 
Sent: Saturday, March 08, 2014 5:25 PM
To: r-sig-mixed-models at r-project.org
Cc: Chen, Ming-Huei; Yang, Qiong
Subject: Re: gee, geese and glmer

On 14-03-07 11:25 PM, Ming-Huei Chen wrote:
> Hi Ben,
> 
>  
> 
> In an analysis we found that glmer in new lme4 gave result different 
> from old lme4, gee and geese, where old lme4 seems to be closer to gee 
> and geese.. Please see highlighted sex effect below. Case by sex (2x2) 
> table is also given. Can you please let us know how would you look 
> into the results? Thanks!
> 

   [cc'ing to r-sig-mixed-models: **please** try r-sig-mixed-models first, not personal e-mail to me ...]

  I can't say exactly what's going here; without having a reproducible example <http://tinyurl.com/reproducible-000> it's hard to say precisely.  Thoughts:

 * gee and geese are giving _exactly_ the same parameter estimates, to
8 significant digits, so I would guess they are wrapping identical underlying methods.

 * As far as diagnosing the issue with lme4 1.0-6:
   * does changing the optimization method, i.e.
 glmerControl(optimizer="optimx",optCtrl=list(method="nlminb"))
   [must do library("optimx") first] or
 glmerControl(optimizer="bobyqa")

  change the result?

 * I would be curious whether the soon-to-be-released version 1.1-4 (which can be installed from github or lme4.r-forge.r-project.org/repos) gives either (1) convergence warnings or (2) different/better answers

 * You can try specifying the starting values for lme4 to diagnose misconvergence; for example, start lme4 from the estimates given by old lme4/lme4.0 and see if it gives a similar answer.

 * You can use the 'slice' and 'splom.slice' functions from bbmle to visualize the likelihood surfaces

  good luck,
   Ben Bolker

> Ming-Huei
> 

> ###GEE
> 
>> summary(gee(case~sex+PC1+PC2+PC3+PC4,id=famid,family=binomial,data=da
>> ta))$coef
>                Estimate Naive S.E.     Naive z Robust S.E.   Robust z
> (Intercept) -1.88047373 0.13532162 -13.8963286  0.15960440 -11.782092
> sex         -0.23436854 0.08611269  -2.7216494  0.09050577  -2.589543
> PC1         -0.05478639 0.06195318  -0.8843192  0.06822178  -0.803063
> PC2         -0.09934572 0.06494563  -1.5296753  0.06520811  -1.523518
> PC3         -0.07020391 0.06626875  -1.0593818  0.06962147  -1.008366
> PC4         -0.13413097 0.06746716  -1.9880927  0.06979901  -1.921674
> 

> ###GEESE
> 
>> summary(geese(case~sex+PC1+PC2+PC3+PC4,id=famid,family=binomial,data=
>> data))$mean
> 
>                estimate     san.se        wald           p
> 
> (Intercept) -1.88047373 0.15960440 138.8176912 0.000000000
> sex         -0.23436854 0.09050577   6.7057312 0.009610351
> PC1         -0.05478639 0.06822178   0.6449102 0.421938319
> PC2         -0.09934572 0.06520811   2.3211071 0.127629159
> PC3         -0.07020391 0.06962147   1.0168016 0.313278888
> PC4         -0.13413097 0.06979901   3.6928324 0.054646745
> 
> ### lme4_0.999999-2
> 
>> summary(glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data
>> =data))
>             Estimate Std. Error z value Pr(>|z|)
> (Intercept) -3.01599    0.28305 -10.655   <2e-16 ***
> sex         -0.41056    0.16285  -2.521   0.0117 *
> PC1         -0.17116    0.12903  -1.326   0.1847
> PC2         -0.15510    0.13382  -1.159   0.2465
> PC3         -0.19044    0.13580  -1.402   0.1608
> PC4          0.02532    0.13732   0.184   0.8537
> 
> ###lme4_1.0-6
> 
>> summary(glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data
>> =data))
> 
>             Estimate Std. Error z value Pr(>|z|)
> 
> (Intercept) -10.2784     0.8631 -11.909   <2e-16 ***
> sex           0.3497     0.1975   1.770   0.0767 .
> PC1          -0.3555     0.1623  -2.190   0.0285 *
> PC2          -0.1087     0.1653  -0.657   0.5109
> PC3          -0.2242     0.1652  -1.357   0.1748
> PC4           0.1103     0.1671   0.660   0.5091
> 
> Case by sex
> 
>         1    2
>   0   2554 3021
>   1    310  290
> 


From maechler at stat.math.ethz.ch  Tue Mar 11 10:45:19 2014
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 11 Mar 2014 10:45:19 +0100
Subject: [R-sig-ME] gee, geese and glmer
In-Reply-To: <6984CA5898097D459F8EC753F025B0D839696447@IST-EX10MBX-3.ad.bu.edu>
References: <007801cf3a86$6d27b060$47771120$@bu.edu>
	<531B98B4.4050403@gmail.com>
	<6984CA5898097D459F8EC753F025B0D839696447@IST-EX10MBX-3.ad.bu.edu>
Message-ID: <21278.56111.63015.103003@stat.math.ethz.ch>

>>>>> Yang, Qiong <qyang at bu.edu>
>>>>>     on Mon, 10 Mar 2014 18:19:56 +0000 writes:

    > Hi Ben, We wonder if you can add an option in lmer() of
    > current lme4 version to call the algorithm used in
    > lme4_0.999999-2?

unfortunately not.

For this reason, we had planned for many months, starting in
August 2012 and announced on this mailing list at least a year
ago that we would provide the 'lme4.0' package  (back-compatible
as well as possible in light of computer OS updates incl system
libraries, and R updates, ...) in order to 
provide useRs the possibility of reproducible research and data
analysis for their analyses done with "old-CRAN" lme4 versions.

Unfortunately (for us), the CRAN maintainers decided that
providing lme4.0 in addition to lme4 was a bad idea, and
explicit forbid such actions in the (then new) CRAN policy
document.  I'm still not at all happy with that decision.

    > For our package (analyze rare genetic variant) to be put
    > on CRAN, we need to use current version of lme4. However,
    > at this point, there are still issues that cannot be
    > resolved with newer versions of lme4. It is very difficult
    > resolved with newer versions of lme4. It is very difficult
    > for us to keep waiting and testing the new release, and
    > hope all the issues resolved and no new issues coming
    > up. lme4_0.999999-2 has been used by us for a long time
    > with little problem. 

Good to hear.  Such cases were exactly the reason why we (lme4
authors) made such considerable effort to provide  lme4.0 for 
reproducible research and data analysis.

at the bottom of
  https://github.com/lme4/lme4/blob/master/README.md
we mention the state and give installation instruction of
lme4.0, but as you say, this does not solve the problem for
other package maintainers: If they want a CRAN package, they
(currently? I'm optimistic beyond reason :-)
cannot have a 'Depends: lme4.0'  (or "Imports:..." or similar).

    > Your help on this is highly
    > appreciated.  Best, Qiong

You're welcome; currentl there's not more we can do.
Martin

--
Martin Maechler,
ETH Zurich, Switzerland



    > -----Original Message----- From: Ben Bolker
    > [mailto:bbolker at gmail.com] Sent: Saturday, March 08, 2014
    > 5:25 PM To: r-sig-mixed-models at r-project.org Cc: Chen,
    > Ming-Huei; Yang, Qiong Subject: Re: gee, geese and glmer

    > On 14-03-07 11:25 PM, Ming-Huei Chen wrote:
    >> Hi Ben,
    >> 
    >> 
    >> 
    >> In an analysis we found that glmer in new lme4 gave
    >> result different from old lme4, gee and geese, where old
    >> lme4 seems to be closer to gee and geese.. Please see
    >> highlighted sex effect below. Case by sex (2x2) table is
    >> also given. Can you please let us know how would you look
    >> into the results? Thanks!
    >> 

    >    [cc'ing to r-sig-mixed-models: **please** try
    > r-sig-mixed-models first, not personal e-mail to me ...]

    >   I can't say exactly what's going here; without having a
    > reproducible example <http://tinyurl.com/reproducible-000>
    > it's hard to say precisely.  Thoughts:

    >  * gee and geese are giving _exactly_ the same parameter
    > estimates, to 8 significant digits, so I would guess they
    > are wrapping identical underlying methods.

    >  * As far as diagnosing the issue with lme4 1.0-6: * does
    > changing the optimization method, i.e.
    > glmerControl(optimizer="optimx",optCtrl=list(method="nlminb"))
    > [must do library("optimx") first] or
    > glmerControl(optimizer="bobyqa")

    >   change the result?

    >  * I would be curious whether the soon-to-be-released
    > version 1.1-4 (which can be installed from github or
    > lme4.r-forge.r-project.org/repos) gives either (1)
    > convergence warnings or (2) different/better answers

    >  * You can try specifying the starting values for lme4 to
    > diagnose misconvergence; for example, start lme4 from the
    > estimates given by old lme4/lme4.0 and see if it gives a
    > similar answer.

    >  * You can use the 'slice' and 'splom.slice' functions
    > from bbmle to visualize the likelihood surfaces

    >   good luck, Ben Bolker

    >> Ming-Huei
    >> 

    >> ###GEE
    >> 
    >>> summary(gee(case~sex+PC1+PC2+PC3+PC4,id=famid,family=binomial,data=da
    >>> ta))$coef
    >> Estimate Naive S.E.  Naive z Robust S.E.  Robust z
    >> (Intercept) -1.88047373 0.13532162 -13.8963286 0.15960440
    >> -11.782092 sex -0.23436854 0.08611269 -2.7216494
    >> 0.09050577 -2.589543 PC1 -0.05478639 0.06195318
    >> -0.8843192 0.06822178 -0.803063 PC2 -0.09934572
    >> 0.06494563 -1.5296753 0.06520811 -1.523518 PC3
    >> -0.07020391 0.06626875 -1.0593818 0.06962147 -1.008366
    >> PC4 -0.13413097 0.06746716 -1.9880927 0.06979901
    >> -1.921674
    >> 

    >> ###GEESE
    >> 
    >>> summary(geese(case~sex+PC1+PC2+PC3+PC4,id=famid,family=binomial,data=
    >>> data))$mean
    >> 
    >> estimate san.se wald p
    >> 
    >> (Intercept) -1.88047373 0.15960440 138.8176912
    >> 0.000000000 sex -0.23436854 0.09050577 6.7057312
    >> 0.009610351 PC1 -0.05478639 0.06822178 0.6449102
    >> 0.421938319 PC2 -0.09934572 0.06520811 2.3211071
    >> 0.127629159 PC3 -0.07020391 0.06962147 1.0168016
    >> 0.313278888 PC4 -0.13413097 0.06979901 3.6928324
    >> 0.054646745
    >> 
    >> ### lme4_0.999999-2
    >> 
    >>> summary(glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data
    >>> =data))
    >> Estimate Std. Error z value Pr(>|z|) (Intercept) -3.01599
    >> 0.28305 -10.655 <2e-16 *** sex -0.41056 0.16285 -2.521
    >> 0.0117 * PC1 -0.17116 0.12903 -1.326 0.1847 PC2 -0.15510
    >> 0.13382 -1.159 0.2465 PC3 -0.19044 0.13580 -1.402 0.1608
    >> PC4 0.02532 0.13732 0.184 0.8537
    >> 
    >> ###lme4_1.0-6
    >> 
    >>> summary(glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data
    >>> =data))
    >> 
    >> Estimate Std. Error z value Pr(>|z|)
    >> 
    >> (Intercept) -10.2784 0.8631 -11.909 <2e-16 *** sex 0.3497
    >> 0.1975 1.770 0.0767 .  PC1 -0.3555 0.1623 -2.190 0.0285 *
    >> PC2 -0.1087 0.1653 -0.657 0.5109 PC3 -0.2242 0.1652
    >> -1.357 0.1748 PC4 0.1103 0.1671 0.660 0.5091
    >> 
    >> Case by sex
    >> 
    >> 1 2 0 2554 3021 1 310 290
    >> 

    > _______________________________________________
    > R-sig-mixed-models at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From wolfgang.viechtbauer at maastrichtuniversity.nl  Tue Mar 11 11:35:55 2014
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (STAT))
Date: Tue, 11 Mar 2014 11:35:55 +0100
Subject: [R-sig-ME] Release Dates lme4 Version
Message-ID: <077E31A57DA26E46AB0D493C9966AC730DA0345FB5@UM-MAIL4112.unimaas.nl>

Hello All,

This is actually more of a question directly for the lme4 maintainers, but I think others may be interested as well, so I am sending this to the mailing list. I noticed that there are already versions 1.1-0, 1.1-3, and 1.1-4 of lme4 on GitHub (with some fixes/changes that I am quite keen on), while the CRAN version is currently at 1.0-6. I am just wondering whether there are any definite (or semi-definite) plans on when to release a new CRAN version (yes, I have installed the dev version from GitHub; this is more about benefitting from those fixes/changes when using a package that depends on lme4 and that is installed by users in the usual way via CRAN).

(and as usual: thanks to the lme4 maintainers for all of their hard work!)

Best,
Wolfgang

--   
Wolfgang Viechtbauer, Ph.D., Statistician   
Department of Psychiatry and Psychology   
School for Mental Health and Neuroscience   
Faculty of Health, Medicine, and Life Sciences   
Maastricht University, P.O. Box 616 (VIJV1)   
6200 MD Maastricht, The Netherlands   
+31 (43) 388-4170 | http://www.wvbauer.com   


From rdiaz02 at gmail.com  Tue Mar 11 11:48:57 2014
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Tue, 11 Mar 2014 11:48:57 +0100
Subject: [R-sig-ME] Modeling precision and recall with GLMMs
Message-ID: <87ob1d57ty.fsf@gmail.com>

Dear All,

I am examining the performance of a couple of classification-like methods
under different scenarios. Two of the metrics I am using are precision and
recall (TP/(TP + FP) and TP/(TP + FN), where TP, FP, and FN are "true
positives", "false positives", and "false negatives" in a simple two-way
confusion matrix). Some of the combinations of methods have been used on
exactly the same data sets. So it is easy to set up a binomial model (or
multinomial2 if using MCMCglmm) such as


cbind(TP, FP) ~ fixed effects + (1|dataset) 



However, the left hand side sounds questionable, specially with precision:
the expression TP/(TP + FP) has, in the denominator, a (TP + FP) [the
number of results returned, or retrieved instances, etc] that, itself, can
be highly method-dependent (i.e., affected by the fixed effects). So rather
than a true proportion, this seems more like a ratio, where each of TP and
FP have their own variance, a covariance, etc, and thus the error
distribution is a mess (not the tidy thing of a binomial).


I've looked around in the literature and have not found much (maybe the
problem are my searching skills :-). Most people use rankings of methods,
not directly modeling precision or recall in the left-hand side of a
(generalized) linear model. A couple of papers use a linear model on the
log-transformed response (which I think is even worse than the above
binomial model, specially with lots of 0s or 1s). Some other people use a
single measure, such as the F-measure or Matthews correlation coefficient,
and I am using something similar too, but I specifically wanted to also
model precision and recall.


An option would be a multi-response model with MCMCglmm, but I am not sure
if this is appropriate either (dependence of the sum of FP and TP on the
fixed effects).


Best,


R.

-- 
Ramon Diaz-Uriarte
Department of Biochemistry, Lab B-25
Facultad de Medicina 
Universidad Aut?noma de Madrid 
Arzobispo Morcillo, 4
28029 Madrid
Spain

Phone: +34-91-497-2412

Email: rdiaz02 at gmail.com
       ramon.diaz at iib.uam.es

http://ligarto.org/rdiaz


From bbolker at gmail.com  Tue Mar 11 14:13:38 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 11 Mar 2014 09:13:38 -0400
Subject: [R-sig-ME] Release Dates lme4 Version
In-Reply-To: <077E31A57DA26E46AB0D493C9966AC730DA0345FB5@UM-MAIL4112.unimaas.nl>
References: <077E31A57DA26E46AB0D493C9966AC730DA0345FB5@UM-MAIL4112.unimaas.nl>
Message-ID: <531F0C02.1030403@gmail.com>

On 14-03-11 06:35 AM, Viechtbauer Wolfgang (STAT) wrote:
> Hello All,
> 
> This is actually more of a question directly for the lme4
> maintainers, but I think others may be interested as well, so I am
> sending this to the mailing list. I noticed that there are already
> versions 1.1-0, 1.1-3, and 1.1-4 of lme4 on GitHub (with some
> fixes/changes that I am quite keen on), while the CRAN version is
> currently at 1.0-6. I am just wondering whether there are any
> definite (or semi-definite) plans on when to release a new CRAN
> version (yes, I have installed the dev version from GitHub; this is
> more about benefitting from those fixes/changes when using a package
> that depends on lme4 and that is installed by users in the usual way
> via CRAN).
> 
> (and as usual: thanks to the lme4 maintainers for all of their hard
> work!)
> 
> Best, Wolfgang

  Very definite: we're in the process of submitting 1.1-4 to CRAN right
now (we had to hurry up a bit because there have been changes in R-devel
and upstream packages that we needed to accommodate).  There are a small
number of downstream packages that our new release breaks, and we're
working on contacting those maintainers/helping them fix the problems.

   The thing most users will notice in installing the new version is a
wider range of convergence warnings.  Overall we think this is a good
idea: users who didn't know that their fits were dodgy will find out.
On the other hand, we also know there will be some false positives, and
we look forward (!!) to reassuring users ... (this will be a big
motivation to get improved diagnostic/troubleshooting documentation set
up, within the package or on glmm.wikidot.com/faq or elsewhere ...)

  cheers
    Ben Bolker


From bbolker at gmail.com  Tue Mar 11 14:26:27 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 11 Mar 2014 09:26:27 -0400
Subject: [R-sig-ME] gee, geese and glmer
In-Reply-To: <21278.56111.63015.103003@stat.math.ethz.ch>
References: <007801cf3a86$6d27b060$47771120$@bu.edu>	<531B98B4.4050403@gmail.com>	<6984CA5898097D459F8EC753F025B0D839696447@IST-EX10MBX-3.ad.bu.edu>
	<21278.56111.63015.103003@stat.math.ethz.ch>
Message-ID: <531F0F03.5090907@gmail.com>

   I would also point out that we are indeed very interested, in the
medium term (the short term is very very busy!), in making sure that
your issues are resolved.  We would like lme4 to dominate lme4.0 (i.e.,
to work better in all circumstances). So far it's been a bit difficult
since we have been debugging remotely -- short of the suggestions I gave
below, and without access to a reproducible example, it's very hard
indeed for me to say much more.

  sincerely
    Ben Bolker

On 14-03-11 05:45 AM, Martin Maechler wrote:
>>>>>> Yang, Qiong <qyang at bu.edu>
>>>>>>     on Mon, 10 Mar 2014 18:19:56 +0000 writes:
> 
>     > Hi Ben, We wonder if you can add an option in lmer() of
>     > current lme4 version to call the algorithm used in
>     > lme4_0.999999-2?
> 
> unfortunately not.
> 
> For this reason, we had planned for many months, starting in
> August 2012 and announced on this mailing list at least a year
> ago that we would provide the 'lme4.0' package  (back-compatible
> as well as possible in light of computer OS updates incl system
> libraries, and R updates, ...) in order to 
> provide useRs the possibility of reproducible research and data
> analysis for their analyses done with "old-CRAN" lme4 versions.
> 
> Unfortunately (for us), the CRAN maintainers decided that
> providing lme4.0 in addition to lme4 was a bad idea, and
> explicit forbid such actions in the (then new) CRAN policy
> document.  I'm still not at all happy with that decision.
> 
>     > For our package (analyze rare genetic variant) to be put
>     > on CRAN, we need to use current version of lme4. However,
>     > at this point, there are still issues that cannot be
>     > resolved with newer versions of lme4. It is very difficult
>     > resolved with newer versions of lme4. It is very difficult
>     > for us to keep waiting and testing the new release, and
>     > hope all the issues resolved and no new issues coming
>     > up. lme4_0.999999-2 has been used by us for a long time
>     > with little problem. 
> 
> Good to hear.  Such cases were exactly the reason why we (lme4
> authors) made such considerable effort to provide  lme4.0 for 
> reproducible research and data analysis.
> 
> at the bottom of
>   https://github.com/lme4/lme4/blob/master/README.md
> we mention the state and give installation instruction of
> lme4.0, but as you say, this does not solve the problem for
> other package maintainers: If they want a CRAN package, they
> (currently? I'm optimistic beyond reason :-)
> cannot have a 'Depends: lme4.0'  (or "Imports:..." or similar).
> 
>     > Your help on this is highly
>     > appreciated.  Best, Qiong
> 
> You're welcome; currentl there's not more we can do.
> Martin
> 
> --
> Martin Maechler,
> ETH Zurich, Switzerland
> 
> 
> 
>     > -----Original Message----- From: Ben Bolker
>     > [mailto:bbolker at gmail.com] Sent: Saturday, March 08, 2014
>     > 5:25 PM To: r-sig-mixed-models at r-project.org Cc: Chen,
>     > Ming-Huei; Yang, Qiong Subject: Re: gee, geese and glmer
> 
>     > On 14-03-07 11:25 PM, Ming-Huei Chen wrote:
>     >> Hi Ben,
>     >> 
>     >> 
>     >> 
>     >> In an analysis we found that glmer in new lme4 gave
>     >> result different from old lme4, gee and geese, where old
>     >> lme4 seems to be closer to gee and geese.. Please see
>     >> highlighted sex effect below. Case by sex (2x2) table is
>     >> also given. Can you please let us know how would you look
>     >> into the results? Thanks!
>     >> 
> 
>     >    [cc'ing to r-sig-mixed-models: **please** try
>     > r-sig-mixed-models first, not personal e-mail to me ...]
> 
>     >   I can't say exactly what's going here; without having a
>     > reproducible example <http://tinyurl.com/reproducible-000>
>     > it's hard to say precisely.  Thoughts:
> 
>     >  * gee and geese are giving _exactly_ the same parameter
>     > estimates, to 8 significant digits, so I would guess they
>     > are wrapping identical underlying methods.
> 
>     >  * As far as diagnosing the issue with lme4 1.0-6: * does
>     > changing the optimization method, i.e.
>     > glmerControl(optimizer="optimx",optCtrl=list(method="nlminb"))
>     > [must do library("optimx") first] or
>     > glmerControl(optimizer="bobyqa")
> 
>     >   change the result?
> 
>     >  * I would be curious whether the soon-to-be-released
>     > version 1.1-4 (which can be installed from github or
>     > lme4.r-forge.r-project.org/repos) gives either (1)
>     > convergence warnings or (2) different/better answers
> 
>     >  * You can try specifying the starting values for lme4 to
>     > diagnose misconvergence; for example, start lme4 from the
>     > estimates given by old lme4/lme4.0 and see if it gives a
>     > similar answer.
> 
>     >  * You can use the 'slice' and 'splom.slice' functions
>     > from bbmle to visualize the likelihood surfaces
> 
>     >   good luck, Ben Bolker
> 
>     >> Ming-Huei
>     >> 
> 
>     >> ###GEE
>     >> 
>     >>> summary(gee(case~sex+PC1+PC2+PC3+PC4,id=famid,family=binomial,data=da
>     >>> ta))$coef
>     >> Estimate Naive S.E.  Naive z Robust S.E.  Robust z
>     >> (Intercept) -1.88047373 0.13532162 -13.8963286 0.15960440
>     >> -11.782092 sex -0.23436854 0.08611269 -2.7216494
>     >> 0.09050577 -2.589543 PC1 -0.05478639 0.06195318
>     >> -0.8843192 0.06822178 -0.803063 PC2 -0.09934572
>     >> 0.06494563 -1.5296753 0.06520811 -1.523518 PC3
>     >> -0.07020391 0.06626875 -1.0593818 0.06962147 -1.008366
>     >> PC4 -0.13413097 0.06746716 -1.9880927 0.06979901
>     >> -1.921674
>     >> 
> 
>     >> ###GEESE
>     >> 
>     >>> summary(geese(case~sex+PC1+PC2+PC3+PC4,id=famid,family=binomial,data=
>     >>> data))$mean
>     >> 
>     >> estimate san.se wald p
>     >> 
>     >> (Intercept) -1.88047373 0.15960440 138.8176912
>     >> 0.000000000 sex -0.23436854 0.09050577 6.7057312
>     >> 0.009610351 PC1 -0.05478639 0.06822178 0.6449102
>     >> 0.421938319 PC2 -0.09934572 0.06520811 2.3211071
>     >> 0.127629159 PC3 -0.07020391 0.06962147 1.0168016
>     >> 0.313278888 PC4 -0.13413097 0.06979901 3.6928324
>     >> 0.054646745
>     >> 
>     >> ### lme4_0.999999-2
>     >> 
>     >>> summary(glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data
>     >>> =data))
>     >> Estimate Std. Error z value Pr(>|z|) (Intercept) -3.01599
>     >> 0.28305 -10.655 <2e-16 *** sex -0.41056 0.16285 -2.521
>     >> 0.0117 * PC1 -0.17116 0.12903 -1.326 0.1847 PC2 -0.15510
>     >> 0.13382 -1.159 0.2465 PC3 -0.19044 0.13580 -1.402 0.1608
>     >> PC4 0.02532 0.13732 0.184 0.8537
>     >> 
>     >> ###lme4_1.0-6
>     >> 
>     >>> summary(glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data
>     >>> =data))
>     >> 
>     >> Estimate Std. Error z value Pr(>|z|)
>     >> 
>     >> (Intercept) -10.2784 0.8631 -11.909 <2e-16 *** sex 0.3497
>     >> 0.1975 1.770 0.0767 .  PC1 -0.3555 0.1623 -2.190 0.0285 *
>     >> PC2 -0.1087 0.1653 -0.657 0.5109 PC3 -0.2242 0.1652
>     >> -1.357 0.1748 PC4 0.1103 0.1671 0.660 0.5091
>     >> 
>     >> Case by sex
>     >> 
>     >> 1 2 0 2554 3021 1 310 290
>     >> 
> 
>     > _______________________________________________
>     > R-sig-mixed-models at r-project.org mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From wolfgang.viechtbauer at maastrichtuniversity.nl  Tue Mar 11 16:08:55 2014
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (STAT))
Date: Tue, 11 Mar 2014 16:08:55 +0100
Subject: [R-sig-ME] Release Dates lme4 Version
In-Reply-To: <531F0C02.1030403@gmail.com>
References: <077E31A57DA26E46AB0D493C9966AC730DA0345FB5@UM-MAIL4112.unimaas.nl>
	<531F0C02.1030403@gmail.com>
Message-ID: <077E31A57DA26E46AB0D493C9966AC730DA0346131@UM-MAIL4112.unimaas.nl>

Great! Looking forward to the new version on CRAN.

Best,
Wolfgang

> -----Original Message-----
> From: Ben Bolker [mailto:bbolker at gmail.com]
> Sent: Tuesday, March 11, 2014 14:14
> To: Viechtbauer Wolfgang (STAT); r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Release Dates lme4 Version
> 
> On 14-03-11 06:35 AM, Viechtbauer Wolfgang (STAT) wrote:
> > Hello All,
> >
> > This is actually more of a question directly for the lme4
> > maintainers, but I think others may be interested as well, so I am
> > sending this to the mailing list. I noticed that there are already
> > versions 1.1-0, 1.1-3, and 1.1-4 of lme4 on GitHub (with some
> > fixes/changes that I am quite keen on), while the CRAN version is
> > currently at 1.0-6. I am just wondering whether there are any
> > definite (or semi-definite) plans on when to release a new CRAN
> > version (yes, I have installed the dev version from GitHub; this is
> > more about benefitting from those fixes/changes when using a package
> > that depends on lme4 and that is installed by users in the usual way
> > via CRAN).
> >
> > (and as usual: thanks to the lme4 maintainers for all of their hard
> > work!)
> >
> > Best, Wolfgang
> 
>   Very definite: we're in the process of submitting 1.1-4 to CRAN right
> now (we had to hurry up a bit because there have been changes in R-devel
> and upstream packages that we needed to accommodate).  There are a small
> number of downstream packages that our new release breaks, and we're
> working on contacting those maintainers/helping them fix the problems.
> 
>    The thing most users will notice in installing the new version is a
> wider range of convergence warnings.  Overall we think this is a good
> idea: users who didn't know that their fits were dodgy will find out.
> On the other hand, we also know there will be some false positives, and
> we look forward (!!) to reassuring users ... (this will be a big
> motivation to get improved diagnostic/troubleshooting documentation set
> up, within the package or on glmm.wikidot.com/faq or elsewhere ...)
> 
>   cheers
>     Ben Bolker


From mhnunes at fc.ul.pt  Tue Mar 11 16:26:17 2014
From: mhnunes at fc.ul.pt (Maria Helena Mourino Silva Nunes)
Date: Tue, 11 Mar 2014 15:26:17 +0000
Subject: [R-sig-ME] Mixed Effects with Cox model (multiple events)
Message-ID: <CE9565EEC2A21641A7FF431FBCB78892BE87B5820D@FC-MBXCLUSTER.fc.ul.pt>

Dear all,

I want to describe the time interval between mammography screenings as a function of several clinical and demographic variables. For each woman, I have several events and each event corresponds to the time interval between two consecutive mammography screenings. Also, women's data sets come from ten different primary health centres. I have data from 1995 up to 2013.

I was thinking on developing a Cox regression model with multiple events (as, for instance, Andersen and Gill model). Having in mind there are differences between the primary health centres, I was wondering if I could use a mixed model where women were nested on the primary health centres. I've already tried to use the "coxme" package but it didn't work. Is it possible to use a mixed model in this context? What package should I use?

Thanks, in advanced, for your attention.

Best regards,
Helena Mouri?o Nunes.


From bbolker at gmail.com  Tue Mar 11 16:39:15 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 11 Mar 2014 11:39:15 -0400
Subject: [R-sig-ME] Mixed Effects with Cox model (multiple events)
In-Reply-To: <CE9565EEC2A21641A7FF431FBCB78892BE87B5820D@FC-MBXCLUSTER.fc.ul.pt>
References: <CE9565EEC2A21641A7FF431FBCB78892BE87B5820D@FC-MBXCLUSTER.fc.ul.pt>
Message-ID: <531F2E23.6060207@gmail.com>

On 14-03-11 11:26 AM, Maria Helena Mourino Silva Nunes wrote:
> Dear all,
> 
> I want to describe the time interval between mammography screenings
> as a function of several clinical and demographic variables. For each
> woman, I have several events and each event corresponds to the time
> interval between two consecutive mammography screenings. Also,
> women's data sets come from ten different primary health centres. I
> have data from 1995 up to 2013.
> 
> I was thinking on developing a Cox regression model with multiple
> events (as, for instance, Andersen and Gill model). Having in mind
> there are differences between the primary health centres, I was
> wondering if I could use a mixed model where women were nested on the
> primary health centres. I've already tried to use the "coxme" package
> but it didn't work. Is it possible to use a mixed model in this
> context? What package should I use?
> 
> Thanks, in advanced, for your attention.
> 
> Best regards, Helena Mouri?o Nunes.

  For CPH you will probably have to use a specialized package.  The
survival package has a frailty() function that "allows one to add a
simple random effects term to a Cox or survreg model" (and choose among
gamma, gaussian, or t-distributed random effects).  I don't know how
this will fit in with your multi-event model, but that's where I would
start ...

  Ben Bolker


From V.J.Wickens at pgr.reading.ac.uk  Tue Mar 11 16:57:27 2014
From: V.J.Wickens at pgr.reading.ac.uk (Victoria Wickens)
Date: Tue, 11 Mar 2014 15:57:27 +0000
Subject: [R-sig-ME] Curved residuals vs fitted plot
In-Reply-To: <88612d7954b8480089e4ed643bb9a509@AM3PR01MB161.eurprd01.prod.exchangelabs.com>
References: <828e0de028ff456282ad24640f6ee915@AM3PR01MB161.eurprd01.prod.exchangelabs.com>,
	<DAB55EFEBDAD1F4888EDB2E8F5180BC96C0B0880F6@CMS03.campus.gla.ac.uk>,
	<88612d7954b8480089e4ed643bb9a509@AM3PR01MB161.eurprd01.prod.exchangelabs.com>
Message-ID: <dc094083f5b84835b47efedbcafbfcef@AM3PR01MB161.eurprd01.prod.exchangelabs.com>

Apologies, the link to the plot did not work. Here is the functioning link to illustrate the bizzare plot  http://imgur.com/rx2Oc3s   

Would you suggest this plot illustrates a bad model fit?

Would you recommend I abandon the two-vector response and try for binary? If I use 0, 1s it can't be overdispersed, but has less power. However, if the first model assumptions cannot be upheld because of the residuals, binary might be my only option?

Thank once again for taking the time to read this,

Victoria
______________
Victoria Wickens
PhD student
Room GU08
Centre for Agri-Environmental Research (CAER)
School of Agriculture, Policy & Development
University of Reading
Reading RG6 7BE
____________

Email: v.j.wickens at pgr.reading.ac.uk
http://www.reading.ac.uk/caer/staff_students.html
https://www.facebook.com/UniRdgAPD

________________________________________
From: r-sig-mixed-models-bounces at r-project.org <r-sig-mixed-models-bounces at r-project.org> on behalf of Victoria Wickens <V.J.Wickens at pgr.reading.ac.uk>
Sent: 10 March 2014 14:29
To: Paul Johnson; r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Curved residuals vs fitted plot

Hi Paul,

Thank you for reading my email and for providing clear explanation of why I have the curvature. I found your response and the link very helpful. Sadly I am having trouble getting the R book which is not available at my library, but I do have a copy of the Mixed Effects Models and Extensions in Ecology with R by Zuur.

I now understand the fitted vs resid plots produce curved scatter trends due to the observational random effect and have implemented the code you provided to shift the effect from the fitted values to the residuals to create a new plot (see here http://imgur.com/edit).

My plot is not nearly as well behaved as your example though, with a steep tail of residuals with high fitted values. Does this mean my model is a bad fit? I'm not sure of the next best move to take.

I have tried the following (all with the observational random effect, Bait.ID, to remove overdispersion but with similar resid vs fitted plots as seen on the web-link):

1. glmer
2. glmer + polynomial regression (logDistance factor)
3. glmer + quadratic term (Distance^2)
4. gamm (limited success)

Is there another way to remove overdispersion or account for it? Or am I going about this wrong?

Thank you once again for taking time to help me. I really appreciate it,

Victoria
______________
Victoria Wickens
PhD student
Room GU08
Centre for Agri-Environmental Research (CAER)
School of Agriculture, Policy & Development
University of Reading
Reading RG6 7BE
____________

Email: v.j.wickens at pgr.reading.ac.uk
http://www.reading.ac.uk/caer/staff_students.html
https://www.facebook.com/UniRdgAPD

________________________________________
From: Paul Johnson <paul.johnson at glasgow.ac.uk>
Sent: 08 March 2014 01:26
To: Victoria Wickens; r-sig-mixed-models at r-project.org
Subject: RE: Curved residuals vs fitted plot

Hi Victoria,

Positive correlation between the residuals and fitted values is expected whenever you have a row-level random effect to mop up overdispersion. It happens because overdispersion implies that the low responses are lower than predicted and the high responses are higher than predicted, even allowing for binomial sampling error. The gap between a low predicted proportion and a still lower observed proportion is bridged partly by a negative residual and partly by a negative overdispersion random effect (which can be thought of as an additional 'free' residual to the constrained residual due to binomial sampling error). The same happens at the highest predicted proportions, except that both residual and random effect will be positive, hence the positive correlation between the predicted (fitted) values, which contain the overdispersion random effects, and the residuals.

This problem is dealt with, for binomial GLMMs, in chapter 7 of Alain Zuur's Beginner's guide to GLM and GLMM with R. I don't have this book handy and I can't remember his solution. One solution is to shift the overdispersion random effect from the fitted values to the residuals, which makes sense given that they are really just add-on residuals - see the example code below.

This thread discusses the same problem for lognormal-Poisson GLMMs:
https://stat.ethz.ch/pipermail/r-sig-mixed-models/2013q3/020819.html

Best wishes,
Paul

# simulate binomial data with a positive trend and overdispersion
obs <- factor(1:100)
x <- seq(-1, 0.98, 0.02)
set.seed(12345678)
y <- rbinom(100, 20, plogis(x + rnorm(100, 0, 1)))
par(mfrow=c(3, 1))
plot(y/20 ~ x)
# fit logitnormal-binomial GLMM
fit <- glmer(cbind(y, 20-y) ~ I(1:100) + (1|obs), family="binomial")
summary(fit)
# standard residual vs fitted plot
plot(fitted(fit), resid(fit))
abline(h=0)
# shift overdispersion random effect from fitted values to residuals
Fitted <- plogis(qlogis(fitted(fit)) - ranef(fit)$obs[[1]])
Resid <- (y/20 - Fitted) / sqrt(Fitted * (1 - Fitted)/20)
plot(Fitted, Resid)
abline(h=0)
# NB THIS GETS RID OF THE TREND BUT ISN'T RIGHT!
# This is because I've standardised the variance using the
# variance function for the binomial, p(1-p)/n, but the distribution
# underlying the residuals is now a logitnormal-binomial
# so should incorporate the overdispersion variance. I don't
# know how to do this.


________________________________________
From: r-sig-mixed-models-bounces at r-project.org [r-sig-mixed-models-bounces at r-project.org] On Behalf Of Victoria Wickens [V.J.Wickens at pgr.reading.ac.uk]
Sent: 07 March 2014 19:20
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Curved residuals vs fitted plot

Dear mixed-models group,

Thank you for taking time to read this plea for help.

I wondered if you could give me any advice regarding the curved residuals vs fitted plot with my model:

model3<-glmer(y~Treatment*Distance.m+(1|Field/Field.section/Transect/fffPlot/Bait.ID),
binomial(link=logit),data=dframe1)

My aim is to explain aphid removal from bait trays in response to treatment (presence of absence of a flower strip) and distance (five distances, or plots, moving away from the margins; 1, 5, 10, 15, and 20m). (More experimental details after message in case helpful).

I am using R version 3.0.2 and the latest version of lme4. The resid vs fitted figure is not great, is this because I need to look at a logit fitted scale? (See here to view the figure: <http://imgur.com/CYe5CxC> http://imgur.com/CYe5CxC)

Any help at all would be more than amazing,

Victoria

Experimental setup
There are two fields overall, each contains one treatment (Mit) and one control (Non). Looking at one treatment, in one field section, there are three transects parallel to each other and perpendicular to the margin treatment (T1, T2 and T3). There are five plots along each transect at specific distances from the treatment margins (1, 5, 10, 15, 20m). At each plot, three replica bait trays were placed (a, b and c). Each tray contained 20 aphids glued to foam. In total, I had two farm fields at the largest scale and between them there were 180 bait trays overall (90 per field).


Script attached, also listed below


library(lme4)

Field<-c("L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,
"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B")

Treatment<-c("Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,
"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,
"Mit" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,
"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit")

Field.section<-c("D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"B" ,"B" ,
"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A")

Transect<-c("T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,
"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,
"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,
"T6" ,"T6" ,"T6" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3")

Distance.m<-c(1 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10
,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20)

Aphid.rm.success<-c(3 ,7 ,19 ,19 ,2 ,0 ,0 ,0 ,0 ,0 ,0 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,0 ,0 ,8 ,16 ,2 ,0 ,2 ,1 ,1 ,3 ,0 ,0 ,9 ,10 ,15 ,18 ,1 ,0 ,14 ,10 ,4 ,1 ,3 ,12 ,2 ,4 ,0 ,20 ,17 ,19 ,11 ,19 ,5 ,0 ,0 ,3 ,0 ,0 ,0 ,3 ,11 ,0 ,19 ,13 ,13 ,10 ,9 ,5 ,19 ,1 ,2 ,0 ,12 ,2 ,0 ,1 ,7 ,15 ,19 ,6 ,19 ,18 ,17 ,1 ,0 ,0 ,1 ,1 ,9 ,18 ,11 ,19 ,1 ,3 ,0 ,0 ,1 ,1 ,0 ,1 ,1 ,3 ,0 ,0 ,0 ,4 ,1 ,0 ,0 ,2 ,1 ,6 ,0 ,0 ,0 ,1 ,0 ,0 ,1 ,1 ,0 ,0 ,2 ,1 ,11 ,1 ,0 ,3 ,3 ,0 ,1 ,3 ,0 ,0 ,1 ,0 ,0 ,8 ,1 ,3 ,2 ,1 ,3 ,0 ,
1 ,2 ,0 ,1 ,1 ,0 ,0 ,0 ,8 ,0 ,0 ,0 ,7 ,1 ,0 ,1 ,1 ,0 ,1 ,0 ,1 ,0 ,1 ,5 ,0 ,20 ,0 ,17 ,0 ,10 ,1 ,0 ,5 ,0 ,0 ,0 ,0 ,0)

Aphid.remain.fail<-c(17 ,13 ,1 ,1 ,18 ,20 ,20 ,20 ,20 ,20 ,20 ,19 ,19 ,19 ,17 ,19 ,19 ,19 ,20 ,20 ,12 ,4 ,18 ,20 ,18 ,19 ,19 ,17 ,20 ,20 ,10 ,9 ,5 ,2 ,19 ,20 ,6 ,10 ,16 ,19 ,17 ,8 ,18 ,16 ,20 ,0 ,3 ,1 ,9 ,2 ,15 ,20 ,20 ,17 ,20 ,20 ,20 ,17 ,9 ,20 ,1 ,7 ,7 ,10 ,11 ,15 ,1 ,19 ,18 ,20 ,8 ,18 ,20 ,19 ,13 ,5 ,1 ,14 ,0 ,1 ,3 ,19 ,20 ,20 ,19 ,18 ,11 ,2 ,9 ,0 ,19 ,18 ,20 ,20 ,19 ,19 ,20 ,19 ,19 ,16 ,20 ,20 ,20 ,16 ,19 ,20 ,20 ,18 ,19 ,14 ,20 ,20 ,20 ,19 ,20 ,20 ,19 ,19 ,20 ,20 ,18 ,19 ,
9 ,19 ,19 ,18 ,17 ,20 ,19 ,17 ,20 ,20 ,19 ,20 ,17 ,12 ,19 ,17 ,18 ,19 ,17 ,20 ,19 ,18 ,20 ,18 ,20 ,20 ,19 ,20 ,12 ,20 ,20 ,20 ,13 ,19 ,20 ,19 ,19 ,20 ,20 ,20 ,19 ,20 ,19 ,15 ,20 ,0 ,20 ,3 ,20 ,10 ,19 ,20 ,15 ,20 ,20 ,20 ,20 ,20)

fffPlot<-c("D T1 1" ,"D T1 1" ,"D T1 1" ,"D T1 1" ,"D T1 5" ,"D T1 5" ,"D T1 5" ,"D T1 10" ,"D T1 10" ,"D T1 10" ,"D T1 15" ,"D T1 15" ,"D T1 15" ,"D T1 20" ,"D T1 20" ,"D T1 20" ,"D T2 1" ,"D T2 1" ,"D T2 1" ,"D T2 5" ,"D T2 5" ,"D T2 5" ,"D T2 10" ,"D T2 10" ,"D T2 10" ,"D T2 15" ,"D T2 15" ,"D T2 15" ,"D T2 20" ,"D T2 20" ,"D T2 20" ,"D T3 1" ,"D T3 1" ,"D T3 1" ,"D T3 5" ,"D T3 5" ,"D T3 5" ,"D T3 10" ,"D T3 10" ,"D T3 15" ,"D T3 15" ,"D T3 15" ,"D T3 20" ,"D T3 20" ,"D T3 20" ,
"C T1 1" ,"C T1 1" ,"C T1 1" ,"C T1 5" ,"C T1 5" ,"C T1 5" ,"C T1 10" ,"C T1 10" ,"C T1 10" ,"C T1 15" ,"C T1 15" ,"C T1 15" ,"C T1 20" ,"C T1 20" ,"C T1 20" ,"C T2 1" ,"C T2 1" ,"C T2 1" ,"C T2 5" ,"C T2 5" ,"C T2 5" ,"C T2 10" ,"C T2 10" ,"C T2 10" ,"C T2 15" ,"C T2 15" ,"C T2 15" ,"C T2 20" ,"C T2 20" ,"C T2 20" ,"C T3 1" ,"C T3 1" ,"C T3 1" ,"C T3 5" ,"C T3 5" ,"C T3 5" ,"C T3 10" ,"C T3 10" ,"C T3 10" ,"C T3 15" ,"C T3 15" ,"C T3 15" ,"C T3 20" ,"C T3 20" ,"C T3 20" ,"B T1 1" ,
"B T1 1" ,"B T1 1" ,"B T1 5" ,"B T1 5" ,"B T1 5" ,"B T1 10" ,"B T1 10" ,"B T1 10" ,"B T1 15" ,"B T1 15" ,"B T1 15" ,"B T1 20" ,"B T1 20" ,"B T1 20" ,"B T2 1" ,"B T2 1" ,"B T2 1" ,"B T2 5" ,"B T2 5" ,"B T2 5" ,"B T2 10" ,"B T2 10" ,"B T2 10" ,"B T2 15" ,"B T2 15" ,"B T2 15" ,"B T2 20" ,"B T2 20" ,"B T2 20" ,"B T3 1" ,"B T3 1" ,"B T3 1" ,"B T3 5" ,"B T3 5" ,"B T3 5" ,"B T3 10" ,"B T3 10" ,"B T3 10" ,"B T3 15" ,"B T3 15" ,"B T3 15" ,"B T3 20" ,"B T3 20" ,"B T3 20" ,"A T1 1" ,"A T1 1" ,"A T1 1" ,
"A T1 5" ,"A T1 5" ,"A T1 5" ,"A T1 10" ,"A T1 10" ,"A T1 10" ,"A T1 15" ,"A T1 15" ,"A T1 15" ,"A T1 20" ,"A T1 20" ,"A T1 20" ,"A T2 1" ,"A T2 1" ,"A T2 1" ,"A T2 5" ,"A T2 5" ,"A T2 5" ,"A T2 10" ,"A T2 10" ,"A T2 10" ,"A T2 15" ,"A T2 15" ,"A T2 15" ,"A T2 20" ,"A T2 20" ,"A T2 20" ,"A T3 1" ,"A T3 1" ,"A T3 1" ,"A T3 5" ,"A T3 5" ,"A T3 5" ,"A T3 10" ,"A T3 10" ,"A T3 10" ,"A T3 15" ,"A T3 15" ,"A T3 15" ,"A T3 20" ,"A T3 20" ,"A T3 20")

Bait.ID<-c("ID1" ,"ID2" ,"ID3" ,"ID3" ,"ID4" ,"ID5" ,"ID6" ,"ID7" ,"ID8" ,"ID9" ,"ID10" ,"ID11" ,"ID12" ,"ID13" ,"ID14" ,"ID15" ,"ID16" ,"ID17" ,"ID18" ,"ID19" ,"ID20" ,"ID21" ,"ID22" ,"ID23" ,"ID24" ,"ID25" ,"ID26" ,"ID27" ,"ID28" ,"ID29" ,"ID30" ,"ID31" ,"ID32" ,"ID33" ,"ID34" ,"ID35" ,"ID36" ,"ID37" ,"ID38" ,"ID39" ,"ID40" ,"ID41" ,"ID42" ,"ID43" ,"ID44" ,"ID45" ,"ID46" ,"ID47" ,"ID48" ,"ID49" ,"ID50" ,"ID51" ,"ID52" ,"ID53" ,"ID54" ,"ID55" ,"ID56" ,"ID57" ,"ID58" ,"ID59" ,"ID60" ,"ID61" ,"ID62" ,
"ID63" ,"ID64" ,"ID65" ,"ID66" ,"ID67" ,"ID68" ,"ID69" ,"ID70" ,"ID71" ,"ID72" ,"ID73" ,"ID74" ,"ID75" ,"ID76" ,"ID77" ,"ID78" ,"ID79" ,"ID80" ,"ID81" ,"ID82" ,"ID83" ,"ID84" ,"ID85" ,"ID86" ,"ID87" ,"ID88" ,"ID89" ,"ID90" ,"ID91" ,"ID92" ,"ID93" ,"ID94" ,"ID95" ,"ID96" ,"ID97" ,"ID98" ,"ID99" ,"ID100" ,"ID101" ,"ID102" ,"ID103" ,"ID104" ,"ID105" ,"ID106" ,"ID107" ,"ID108" ,"ID109" ,"ID110" ,"ID111" ,"ID112" ,"ID113" ,"ID114" ,"ID115" ,"ID116" ,"ID117" ,"ID118" ,"ID119" ,"ID120" ,"ID121" ,"ID122" ,
"ID123" ,"ID124" ,"ID125" ,"ID126" ,"ID127" ,"ID128" ,"ID129" ,"ID130" ,"ID131" ,"ID132" ,"ID133" ,"ID134" ,"ID135" ,"ID136" ,"ID137" ,"ID138" ,"ID139" ,"ID140" ,"ID141" ,"ID142" ,"ID143" ,"ID144" ,"ID145" ,"ID146" ,"ID147" ,"ID148" ,"ID149" ,"ID150" ,"ID151" ,"ID152" ,"ID153" ,"ID154" ,"ID155" ,"ID156" ,"ID157" ,"ID158" ,"ID159" ,"ID160" ,"ID161" ,"ID162" ,"ID163" ,"ID164" ,"ID165" ,"ID166" ,"ID167" ,"ID168" ,"ID169" ,"ID170" ,"ID171" ,"ID172" ,"ID173" ,"ID174" ,"ID175" ,"ID176" ,"ID177" ,"ID178" ,
"ID179")


dframe1<-data.frame(Field,Treatment,Field.section,Transect,Distance.m,Aphid.rm.success,Aphid.remain.fail,fffPlot,Bait.ID)

y<-cbind(Aphid.rm.success,Aphid.remain.fail)

model3<-glmer(y~Treatment*Distance.m+(1|Field/Field.section/Transect/fffPlot/Bait.ID), # Bait.ID to remove overdispersion
binomial(link=logit),data=dframe1) # note I kept fffPlot smallest scale in (better residuals)
summary(model3)

plot(model3) # issue? Or just messy ecological data?

______________
Victoria Wickens
PhD student
Room GU08
Centre for Agri-Environmental Research (CAER)
School of Agriculture, Policy & Development
University of Reading
Reading RG6 7BE
____________

Email: v.j.wickens at pgr.reading.ac.uk<mailto:v.j.wickens at pgr.reading.ac.uk>
http://www.reading.ac.uk/caer/staff_students.html
https://www.facebook.com/UniRdgAPD
_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From bbolker at gmail.com  Tue Mar 11 17:02:44 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 11 Mar 2014 12:02:44 -0400
Subject: [R-sig-ME] problem with lme4
In-Reply-To: <0523A5CE5D1EED4D9A9846E530EE382D040C7A4EED31@E2K7-CCR.chu-clermontferrand.fr>
References: <0523A5CE5D1EED4D9A9846E530EE382D040C7A4EED31@E2K7-CCR.chu-clermontferrand.fr>
Message-ID: <531F33A4.3020204@gmail.com>

On 14-03-11 11:43 AM, Saussac Mathilde wrote:
> Dear all,
> 
>  
> 
> I am a French student in BioStatistic and I have a question about the
> package lme4.
> 
>  
> 
> I am using the glmer function but it doesn?t work ? I have these errors :

  Can you say what version you are using?

  From your results below it looks very much like you have issues with
complete separation (i.e., some categories in your data have
all-success or all-failure outcomes).  In general a binomial model
with extreme parameters (absolute value > 10) suggests complete or
near-complete separation [on the probability scale a change of 10
log-odds units corresponds to a change from p=0.5 to p=0.99995, or
from 0.006 to 0.993 ...]  More recent versions of lme4 are somewhat
better at dealing with the complete-separation issue; these are
available from Github or from http://lme4.r-forge.r-project.org/repos,
but we are also hoping that a new version will arrive on CRAN within
the next few days (week?).

  More generally, adjusting tolerances is (as you suggest) a little
bit dangerous, as it may get you silly results.

  The most principled thing to do with complete-separation results is
to use some form of penalization or regularization or prior (closely
related concepts for ways of squashing extreme values in toward zero);
at present you can use the blme package or MCMCglmm to impose such
priors.

  Can you give a reproducible example
<http://tinyurl.com/reproducible-000>, or perhaps the results of
with(d,table(AP2,Motif)) ?  I would expect that some categories
of AP2 have all successes or none ...

> 
> */>model <- glmer(Motif ~ as.factor(AP2) + (1|ID), data=d,
> family=binomial(link="logit"))/*
> 
> /Erreur dans pwrssUpdate(pp, resp, tolPwrss, GQmat, compDev, fac,
> verbose) : /
> 
> /  Downdated VtV is not positive definite /
> 
> Or sometimes this one :
> 
> /Erreur : pwrssUpdate did not converge in 30 iterations/
> 
> I found on the web, that we can change the tolerance (Tolpwrss) and
> indeed it works.
> 
> But, for different values, my results are completely different (see the
> examples for tolPwrss = 1e-5 and 1e-2) :
> 
> */> model <- glmer(Motif ~ as.factor(AP2) + (1|ID), data=d,
> family=binomial(link="logit"),control= glmerControl(tolPwrss=1e-5,
> optimizer="bobyqa"))/*
> 
> /Generalized linear mixed model fit by maximum likelihood ['glmerMod']/
> 
> /Family: binomial ( logit )/
> 
> /Formula: Motif ~ as.factor(AP2) + (1 | ID) /
> 
> /   Data: d /
> 
> / /
> 
> /     AIC      BIC   logLik deviance /
> 
> / 78.9974  93.2092 -35.4987  70.9974 /
> 
> / /
> 
> /Random effects:/
> 
> /Groups Name        Variance Std.Dev./
> 
> /ID     (Intercept) 578.5    24.05   /
> 
> /Number of obs: 258, groups: ID, 218/
> 
> / /
> 
> /Fixed effects:/
> 
> /                 Estimate Std. Error z value Pr(>|z|)  /
> 
> /(Intercept)        10.205      6.149   1.660    0.097 ./
> 
> /as.factor(AP2)1    15.057  46157.700   0.000    1.000  /
> 
> /as.factor(AP2)2     9.356     10.219   0.916    0.360  /
> 
> /---/
> 
> /Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? //0.1 ? ? 1 /
> 
> / /
> 
> /Correlation of Fixed Effects:/
> 
> /            (Intr) a.(AP2)1/
> 
> /as.fc(AP2)1  0.000         /
> 
> /as.fc(AP2)2 -0.303  0.000  /
> 
> / /
> 
> */> model <- glmer(Motif ~ as.factor(AP2) + (1|ID), data=d,
> family=binomial(link="logit"),control= glmerControl(tolPwrss=1e-2,
> optimizer="bobyqa"))/*
> 
> /Generalized linear mixed model fit by maximum likelihood ['glmerMod']/
> 
> /Family: binomial ( logit )/
> 
> /Formula: Motif ~ as.factor(AP2) + (1 | ID) /
> 
> /   Data: d /
> 
> / /
> 
> /     AIC      BIC   logLik deviance /
> 
> /112.9273 127.1392 -52.4637 104.9273 /
> 
> / /
> 
> /Random effects:/
> 
> /Groups Name        Variance Std.Dev./
> 
> /ID     (Intercept) 3.794    1.948   /
> 
> /Number of obs: 258, groups: ID, 218/
> 
> / /
> 
> /Fixed effects:/
> 
> /                Estimate Std. Error z value Pr(>|z|)    /
> 
> /(Intercept)       2.8039     0.4435   6.322 2.58e-10 ***/
> 
> /as.factor(AP2)1  10.5258   117.9609   0.089   0.9289    /
> 
> /as.factor(AP2)2   2.1162     1.0125   2.090   0.0366 *  /
> 
> /---/
> 
> /Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 /
> 
> / /
> 
> /Correlation of Fixed Effects:/
> 
> /            (Intr) a.(AP2)1/
> 
> /as.fc(AP2)1 -0.004         /
> 
> /as.fc(AP2)2 -0.415  0.002  /
> 
> / /
> 
> So I want to know, until which threshold of tolPwrss are results reliable ??
> 
> Is it correct to change this value for all my model ? (Sometimes it
> doesn?t work until I put tolPwrss=0.01 or 0.1 ..)
> 
>  
> 
> I hope you will understand what I explained and you will be able to help
> me ..
>  
> If there is few things I haven't been clear, don't hesitate to ask me
> again.
>  
> Thank you for your attention to these matters.
> 
>  
> 
> Best regards,
> 
>  
> 
> Mathilde
> 
>  
>


From msaussac at chu-clermontferrand.fr  Tue Mar 11 16:43:50 2014
From: msaussac at chu-clermontferrand.fr (Saussac Mathilde)
Date: Tue, 11 Mar 2014 16:43:50 +0100
Subject: [R-sig-ME] problem with lme4
Message-ID: <0523A5CE5D1EED4D9A9846E530EE382D040C7A4EED31@E2K7-CCR.chu-clermontferrand.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140311/dc30e8d3/attachment.pl>

From steve.walker at utoronto.ca  Tue Mar 11 20:01:11 2014
From: steve.walker at utoronto.ca (Steve Walker)
Date: Tue, 11 Mar 2014 15:01:11 -0400
Subject: [R-sig-ME] problem with lme4
In-Reply-To: <0523A5CE5D1EED4D9A9846E530EE382D040C7A4EED31@E2K7-CCR.chu-clermontferrand.fr>
References: <0523A5CE5D1EED4D9A9846E530EE382D040C7A4EED31@E2K7-CCR.chu-clermontferrand.fr>
Message-ID: <531F5D77.3090702@utoronto.ca>

Mathilde,

My first thought is complete/quasi-complete separation, but I can't be 
sure without looking at the data.  If I'm right, as the tolerance is 
turned down, glmer tries to reach larger and larger values of the 
as.factor(AP2)1 coefficient.  Once you get too large, numerical 
instabilities take over and you get error messages.

For example, when tolPwrss = 1e-5, the linear predictor for observations 
in the as.factor(AP2)1 category is about 25, which is an extremely large 
number on the logit scale (corresponding to a probability that is pretty 
darn close to one).

If I'm right, you might want to try Vince Dorie's blme package, which 
can put prior distributions on fixed effect coefficients to keep them 
from blowing up.

Cheers,
Steve

On 3/11/2014, 11:43 AM, Saussac Mathilde wrote:
> Dear all,
>
> I am a French student in BioStatistic and I have a question about the package lme4.
>
> I am using the glmer function but it doesn't work ... I have these errors :
>> model <- glmer(Motif ~ as.factor(AP2) + (1|ID), data=d, family=binomial(link="logit"))
> Erreur dans pwrssUpdate(pp, resp, tolPwrss, GQmat, compDev, fac, verbose) :
>    Downdated VtV is not positive definite
>
> Or sometimes this one :
> Erreur : pwrssUpdate did not converge in 30 iterations
>
> I found on the web, that we can change the tolerance (Tolpwrss) and indeed it works.
> But, for different values, my results are completely different (see the examples for tolPwrss = 1e-5 and 1e-2) :
>> model <- glmer(Motif ~ as.factor(AP2) + (1|ID), data=d, family=binomial(link="logit"),control= glmerControl(tolPwrss=1e-5, optimizer="bobyqa"))
> Generalized linear mixed model fit by maximum likelihood ['glmerMod']
> Family: binomial ( logit )
> Formula: Motif ~ as.factor(AP2) + (1 | ID)
>     Data: d
>
>       AIC      BIC   logLik deviance
>   78.9974  93.2092 -35.4987  70.9974
>
> Random effects:
> Groups Name        Variance Std.Dev.
> ID     (Intercept) 578.5    24.05
> Number of obs: 258, groups: ID, 218
>
> Fixed effects:
>                   Estimate Std. Error z value Pr(>|z|)
> (Intercept)        10.205      6.149   1.660    0.097 .
> as.factor(AP2)1    15.057  46157.700   0.000    1.000
> as.factor(AP2)2     9.356     10.219   0.916    0.360
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> Correlation of Fixed Effects:
>              (Intr) a.(AP2)1
> as.fc(AP2)1  0.000
> as.fc(AP2)2 -0.303  0.000
>
>> model <- glmer(Motif ~ as.factor(AP2) + (1|ID), data=d, family=binomial(link="logit"),control= glmerControl(tolPwrss=1e-2, optimizer="bobyqa"))
> Generalized linear mixed model fit by maximum likelihood ['glmerMod']
> Family: binomial ( logit )
> Formula: Motif ~ as.factor(AP2) + (1 | ID)
>     Data: d
>
>       AIC      BIC   logLik deviance
> 112.9273 127.1392 -52.4637 104.9273
>
> Random effects:
> Groups Name        Variance Std.Dev.
> ID     (Intercept) 3.794    1.948
> Number of obs: 258, groups: ID, 218
>
> Fixed effects:
>                  Estimate Std. Error z value Pr(>|z|)
> (Intercept)       2.8039     0.4435   6.322 2.58e-10 ***
> as.factor(AP2)1  10.5258   117.9609   0.089   0.9289
> as.factor(AP2)2   2.1162     1.0125   2.090   0.0366 *
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> Correlation of Fixed Effects:
>              (Intr) a.(AP2)1
> as.fc(AP2)1 -0.004
> as.fc(AP2)2 -0.415  0.002
>
> So I want to know, until which threshold of tolPwrss are results reliable ??
> Is it correct to change this value for all my model ? (Sometimes it doesn't work until I put tolPwrss=0.01 or 0.1 ..)
>
> I hope you will understand what I explained and you will be able to help me ..
>
> If there is few things I haven't been clear, don't hesitate to ask me again.
>
> Thank you for your attention to these matters.
>
> Best regards,
>
> Mathilde
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>


From alexandre.courtiol at gmail.com  Tue Mar 11 20:51:07 2014
From: alexandre.courtiol at gmail.com (Alexandre Courtiol)
Date: Tue, 11 Mar 2014 20:51:07 +0100
Subject: [R-sig-ME] Bug in lme4: attr(mod@frame,
	"na.action") is not re-initialized to NULL
Message-ID: <CAERMt4fEjtxsnLr-sE0ovu_DOyKEisWqtERFcVdiTE6bcvaOjg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140311/15ff1d36/attachment.pl>

From steve.walker at utoronto.ca  Tue Mar 11 21:16:28 2014
From: steve.walker at utoronto.ca (Steve Walker)
Date: Tue, 11 Mar 2014 16:16:28 -0400
Subject: [R-sig-ME] Bug in lme4: attr(mod@frame,
 "na.action") is not re-initialized to NULL
In-Reply-To: <CAERMt4fEjtxsnLr-sE0ovu_DOyKEisWqtERFcVdiTE6bcvaOjg@mail.gmail.com>
References: <CAERMt4fEjtxsnLr-sE0ovu_DOyKEisWqtERFcVdiTE6bcvaOjg@mail.gmail.com>
Message-ID: <531F6F1C.7020305@utoronto.ca>

This seems to work:

refit(mod, simulate(mod, na.action = na.exclude))

Cheers,
Steve

On 3/11/2014, 3:51 PM, Alexandre Courtiol wrote:
> I think I found a bug... and it may affect several versions of lmer (but I
> did not try the one before 1.0).
> The bug is that the attribute na.action of the model frame is not
> re-initialized to NULL (both in lmer and glmer).
>
> example:
>
> data <- data.frame(y=rnorm(100), K=factor(rep(1:10,10)))
> data[3, "y"] <- NA
> library(lme4)
> mod <- lmer(y~1+(1|K), data=data)
> dim(mod at frame) # 99 2 The model frame has excluded the line as it should
> attr(mod at frame, "na.action") # 3 The value 3 is still considered missing
> while it should not
>
> This feature is slightly annoying because it creates the function refit to
> crash and others using refit (I came to discover that from a crash in
> PBmodcomp from pbkrtest)...
>
> example:
>
> refit(mod, simulate(mod)) # CRASH
> attr(mod at frame,"na.action") <- NULL # fast and dirty patch...
> refit(mod, simulate(mod)) # works
>
> Thanks to the lmer team for your great work!
> PS: If you don't see that as a bug, please send an informative warning.
>
>
>
>> sessionInfo()
> R version 3.0.2 (2013-09-25)
> Platform: x86_64-pc-linux-gnu (64-bit)
>
> attached base packages:
> [1] parallel  stats     graphics  grDevices utils     datasets  methods
> [8] base
>
> other attached packages:
> [1] pbkrtest_0.3-8 MASS_7.3-29    lme4_1.1-4     Rcpp_0.11.0
> Matrix_1.1-2
>
> loaded via a namespace (and not attached):
> [1] grid_3.0.2          lattice_0.20-27     minqa_1.2.3
> [4] nlme_3.1-113        RcppEigen_0.3.2.0.3 splines_3.0.2
> [7] tools_3.0.2
>


From alexandre.courtiol at gmail.com  Tue Mar 11 21:26:03 2014
From: alexandre.courtiol at gmail.com (Alexandre Courtiol)
Date: Tue, 11 Mar 2014 21:26:03 +0100
Subject: [R-sig-ME] Bug in lme4: attr(mod@frame,
 "na.action") is not re-initialized to NULL
In-Reply-To: <531F6F1C.7020305@utoronto.ca>
References: <CAERMt4fEjtxsnLr-sE0ovu_DOyKEisWqtERFcVdiTE6bcvaOjg@mail.gmail.com>
	<531F6F1C.7020305@utoronto.ca>
Message-ID: <CAERMt4dbwiFbP7hEX52s75wKz6ZpHwaZE6UdhrRf6PO7W034sA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140311/9be94301/attachment.pl>

From bbolker at gmail.com  Tue Mar 11 21:34:01 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 11 Mar 2014 16:34:01 -0400
Subject: [R-sig-ME] Bug in lme4: attr(mod@frame,
 "na.action") is not re-initialized to NULL
In-Reply-To: <CAERMt4dbwiFbP7hEX52s75wKz6ZpHwaZE6UdhrRf6PO7W034sA@mail.gmail.com>
References: <CAERMt4fEjtxsnLr-sE0ovu_DOyKEisWqtERFcVdiTE6bcvaOjg@mail.gmail.com>	<531F6F1C.7020305@utoronto.ca>
	<CAERMt4dbwiFbP7hEX52s75wKz6ZpHwaZE6UdhrRf6PO7W034sA@mail.gmail.com>
Message-ID: <531F7339.4060705@gmail.com>

  I think I agree that this is a bug; in our defense, there are *lots*
of possible permutations of when and how NAs can show up in data.
What surprises me slightly is that this doesn't show up in any of the
permutations tested at

https://github.com/lme4/lme4/blob/master/inst/tests/napredict2.R

...

On 14-03-11 04:26 PM, Alexandre Courtiol wrote:
> Yes, but it does not solve the problem for all function that use a direct
> call to refit, e.g. PBmodcomp...
> So it would be beter that the attributes of the model frame really
> correspond to the model frame, no?
> alex
> 
> 
> 2014-03-11 21:16 GMT+01:00 Steve Walker <steve.walker at utoronto.ca>:
> 
>> This seems to work:
>>
>> refit(mod, simulate(mod, na.action = na.exclude))
>>
>> Cheers,
>> Steve
>>
>>
>> On 3/11/2014, 3:51 PM, Alexandre Courtiol wrote:
>>
>>> I think I found a bug... and it may affect several versions of lmer (but I
>>> did not try the one before 1.0).
>>> The bug is that the attribute na.action of the model frame is not
>>> re-initialized to NULL (both in lmer and glmer).
>>>
>>> example:
>>>
>>> data <- data.frame(y=rnorm(100), K=factor(rep(1:10,10)))
>>> data[3, "y"] <- NA
>>> library(lme4)
>>> mod <- lmer(y~1+(1|K), data=data)
>>> dim(mod at frame) # 99 2 The model frame has excluded the line as it should
>>> attr(mod at frame, "na.action") # 3 The value 3 is still considered missing
>>> while it should not
>>>
>>> This feature is slightly annoying because it creates the function refit to
>>> crash and others using refit (I came to discover that from a crash in
>>> PBmodcomp from pbkrtest)...
>>>
>>> example:
>>>
>>> refit(mod, simulate(mod)) # CRASH
>>> attr(mod at frame,"na.action") <- NULL # fast and dirty patch...
>>> refit(mod, simulate(mod)) # works
>>>
>>> Thanks to the lmer team for your great work!
>>> PS: If you don't see that as a bug, please send an informative warning.
>>>
>>>
>>>
>>>  sessionInfo()
>>>>
>>> R version 3.0.2 (2013-09-25)
>>> Platform: x86_64-pc-linux-gnu (64-bit)
>>>
>>> attached base packages:
>>> [1] parallel  stats     graphics  grDevices utils     datasets  methods
>>> [8] base
>>>
>>> other attached packages:
>>> [1] pbkrtest_0.3-8 MASS_7.3-29    lme4_1.1-4     Rcpp_0.11.0
>>> Matrix_1.1-2
>>>
>>> loaded via a namespace (and not attached):
>>> [1] grid_3.0.2          lattice_0.20-27     minqa_1.2.3
>>> [4] nlme_3.1-113        RcppEigen_0.3.2.0.3 splines_3.0.2
>>> [7] tools_3.0.2
>>>
>>>
>>
> 
>


From bbolker at gmail.com  Tue Mar 11 21:44:21 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 11 Mar 2014 16:44:21 -0400
Subject: [R-sig-ME] Bug in lme4: attr(mod@frame,
 "na.action") is not re-initialized to NULL
In-Reply-To: <CAERMt4dbwiFbP7hEX52s75wKz6ZpHwaZE6UdhrRf6PO7W034sA@mail.gmail.com>
References: <CAERMt4fEjtxsnLr-sE0ovu_DOyKEisWqtERFcVdiTE6bcvaOjg@mail.gmail.com>	<531F6F1C.7020305@utoronto.ca>
	<CAERMt4dbwiFbP7hEX52s75wKz6ZpHwaZE6UdhrRf6PO7W034sA@mail.gmail.com>
Message-ID: <531F75A5.2010305@gmail.com>

PS I've added an issue on github

https://github.com/lme4/lme4/issues/189

Unfortunately, I think it may require quite a lot of hard thought to
find the right way to fix this.  Among other things, we currently rely
on this attribute to decide which `na.action` to use for prediction or
simulation based on the original data set ...

  Further discussion on Github.


On 14-03-11 04:26 PM, Alexandre Courtiol wrote:
> Yes, but it does not solve the problem for all function that use a direct
> call to refit, e.g. PBmodcomp...
> So it would be beter that the attributes of the model frame really
> correspond to the model frame, no?
> alex
> 
> 
> 2014-03-11 21:16 GMT+01:00 Steve Walker <steve.walker at utoronto.ca>:
> 
>> This seems to work:
>>
>> refit(mod, simulate(mod, na.action = na.exclude))
>>
>> Cheers,
>> Steve
>>
>>
>> On 3/11/2014, 3:51 PM, Alexandre Courtiol wrote:
>>
>>> I think I found a bug... and it may affect several versions of lmer (but I
>>> did not try the one before 1.0).
>>> The bug is that the attribute na.action of the model frame is not
>>> re-initialized to NULL (both in lmer and glmer).
>>>
>>> example:
>>>
>>> data <- data.frame(y=rnorm(100), K=factor(rep(1:10,10)))
>>> data[3, "y"] <- NA
>>> library(lme4)
>>> mod <- lmer(y~1+(1|K), data=data)
>>> dim(mod at frame) # 99 2 The model frame has excluded the line as it should
>>> attr(mod at frame, "na.action") # 3 The value 3 is still considered missing
>>> while it should not
>>>
>>> This feature is slightly annoying because it creates the function refit to
>>> crash and others using refit (I came to discover that from a crash in
>>> PBmodcomp from pbkrtest)...
>>>
>>> example:
>>>
>>> refit(mod, simulate(mod)) # CRASH
>>> attr(mod at frame,"na.action") <- NULL # fast and dirty patch...
>>> refit(mod, simulate(mod)) # works
>>>
>>> Thanks to the lmer team for your great work!
>>> PS: If you don't see that as a bug, please send an informative warning.
>>>
>>>
>>>
>>>  sessionInfo()
>>>>
>>> R version 3.0.2 (2013-09-25)
>>> Platform: x86_64-pc-linux-gnu (64-bit)
>>>
>>> attached base packages:
>>> [1] parallel  stats     graphics  grDevices utils     datasets  methods
>>> [8] base
>>>
>>> other attached packages:
>>> [1] pbkrtest_0.3-8 MASS_7.3-29    lme4_1.1-4     Rcpp_0.11.0
>>> Matrix_1.1-2
>>>
>>> loaded via a namespace (and not attached):
>>> [1] grid_3.0.2          lattice_0.20-27     minqa_1.2.3
>>> [4] nlme_3.1-113        RcppEigen_0.3.2.0.3 splines_3.0.2
>>> [7] tools_3.0.2
>>>
>>>
>>
> 
>


From mateusfreua at gmail.com  Wed Mar 12 00:50:31 2014
From: mateusfreua at gmail.com (Mateus Freua)
Date: Tue, 11 Mar 2014 20:50:31 -0300
Subject: [R-sig-ME] simultaneous nonlinear mixed effect equations
Message-ID: <CAKxyq5Mn0eEnNM2j=toEbfTPQt6-T-HwjLgDB=PXUPBai-F=-Q@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140311/83530d31/attachment.pl>

From jake987722 at hotmail.com  Wed Mar 12 04:54:00 2014
From: jake987722 at hotmail.com (Jake Westfall)
Date: Tue, 11 Mar 2014 21:54:00 -0600
Subject: [R-sig-ME] Modeling precision and recall with GLMMs
In-Reply-To: <87ob1d57ty.fsf@gmail.com>
References: <87ob1d57ty.fsf@gmail.com>
Message-ID: <BAY172-W469E1A75B654CD55248982CB760@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140311/d6bdc897/attachment.pl>

From jake987722 at hotmail.com  Wed Mar 12 05:04:26 2014
From: jake987722 at hotmail.com (Jake Westfall)
Date: Tue, 11 Mar 2014 22:04:26 -0600
Subject: [R-sig-ME] Modeling precision and recall with GLMMs
In-Reply-To: <BAY172-W469E1A75B654CD55248982CB760@phx.gbl>
References: <87ob1d57ty.fsf@gmail.com>,
	<BAY172-W469E1A75B654CD55248982CB760@phx.gbl>
Message-ID: <BAY172-W6B2B67230ACC725F6BE90CB760@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140311/275b87eb/attachment.pl>

From rdiaz02 at gmail.com  Wed Mar 12 11:56:57 2014
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Wed, 12 Mar 2014 11:56:57 +0100
Subject: [R-sig-ME] Modeling precision and recall with GLMMs
In-Reply-To: <BAY172-W469E1A75B654CD55248982CB760@phx.gbl>
References: <87ob1d57ty.fsf@gmail.com>
	<BAY172-W469E1A75B654CD55248982CB760@phx.gbl>
Message-ID: <87d2hr4rd2.fsf@gmail.com>


Hi Jake,


On Wed, 12-03-2014, at 04:54, jake987722 at hotmail.com wrote:

> Hi Ramon,

> I'm not sure that I fully understand the details of what you
> want to accomplish. But I do want to ask: you jump right into your email
> assuming that of course you want to model precision and recall, but what
> about modelling the data directly (i.e., individual classification
> decisions) rather than summaries of the data? Then you could work
> backward (forward?) from the model results to compute what the implied
> precision and recall would be

Sorry I did not provide enough details. I am comparing some methods for
reconstructing networks, and the True positives and False positives, for
instance, refer to the number of correctly inferred edges and to the
number of edges that a procedure recovers that are not in the original
network, respectively.

So the network reconstruction methods model the data directly, and what I
want to model is how good or bad are what they return as a function of
several other variables (related to several dimensions of the toughness of
the problem, etc)


> If you decided that modelling the data directly would work for your
> purposes, then one way of doing this would be to regress classification
> decisions ("P" or "N") on actual classifications ("P" or "N").

I am not sure that would work. For each data set, each method returns a
bunch of "P"s and "N"s. But what I want to do is model not the relationship
between truth and prediction, but rather how good or bad each method is (at
trying to reconstruct the truth).

> If this is done in a probit model, it is equivalent to the equal-variance
> signal detection model studied at length in psychology, with the
> intercept being the "criterion" in signal detection language (denoted c),
> and the slope being "sensitivity" (denoted d' or d-prime). It should
> definitely be possible to compute precision and recall from c and
> d'.

I am not familiar with this approach in psychology. As I say above, I am
not sure this addresses the problem I want to address but do you have some
pointer to the literature where I can read more about the approach?


Best,


R.

> This might be simpler with a logit rather than probit link
> function.
>
> Let me know if I have misunderstood what you are trying to
> accomplish

> Jake

>> From: rdiaz02 at gmail.com.> To:
>> r-sig-mixed-models at r-project.org.> Date: Tue, 11 Mar 2014 11:48:57
>> +0100.> CC: ramon.diaz at iib.uam.es.> Subject: [R-sig-ME] Modeling
>> precision and recall with GLMMs.> .

>>  Dear All,. .

>>  I am examining the performance of a couple of classification-like
>>  methods. under different scenarios. Two of the metrics I am using are
>>  precision and. recall (TP/(TP + FP) and TP/(TP + FN), where TP, FP, and
>>  FN are "true. positives", "false positives", and "false negatives" in a
>>  simple two-way. confusion matrix). Some of the combinations of methods
>>  have been used on. exactly the same data sets. So it is easy to set up a
>>  binomial model (or. multinomial2 if using MCMCglmm) such as.

>> cbind(TP, FP) ~ fixed effects + (1|dataset)

>> However, the left hand side sounds questionable, specially with
>> precision:. the expression TP/(TP + FP) has, in the denominator, a (TP +
>> FP) [the. number of results returned, or retrieved instances, etc] that,
>> itself, can. be highly method-dependent (i.e., affected by the fixed
>> effects). So rather. than a true proportion, this seems more like a
>> ratio, where each of TP and. FP have their own variance, a covariance,
>> etc, and thus the error. distribution is a mess (not the tidy thing of a
>> binomial).


>> I've looked around in the literature and have not found much (maybe
>> the. problem are my searching skills :-). Most people use rankings of
>> methods,. not directly modeling precision or recall in the left-hand
>> side of a. (generalized) linear model. A couple of papers use a linear
>> model on the. log-transformed response (which I think is even worse than
>> the above. binomial model, specially with lots of 0s or 1s). Some other
>> people use a. single measure, such as the F-measure or Matthews
>> correlation coefficient,. and I am using something similar too, but I
>> specifically wanted to also. model precision and recall.. . .

>> An option would be a multi-response model with MCMCglmm, but I am not
>> sure if this is appropriate either (dependence of the sum of FP and TP
>> on the. fixed effects).. . .


>> Best,

 
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Ramon Diaz-Uriarte
Department of Biochemistry, Lab B-25
Facultad de Medicina 
Universidad Aut?noma de Madrid 
Arzobispo Morcillo, 4
28029 Madrid
Spain

Phone: +34-91-497-2412

Email: rdiaz02 at gmail.com
       ramon.diaz at iib.uam.es

http://ligarto.org/rdiaz


From rdiaz02 at gmail.com  Wed Mar 12 11:58:50 2014
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Wed, 12 Mar 2014 11:58:50 +0100
Subject: [R-sig-ME] Modeling precision and recall with GLMMs
In-Reply-To: <BAY172-W6B2B67230ACC725F6BE90CB760@phx.gbl>
References: <87ob1d57ty.fsf@gmail.com>
	<BAY172-W469E1A75B654CD55248982CB760@phx.gbl>
	<BAY172-W6B2B67230ACC725F6BE90CB760@phx.gbl>
Message-ID: <87bnxb4r9x.fsf@gmail.com>



On Wed, 12-03-2014, at 05:04, jake987722 at hotmail.com wrote:

> A little detail I forgot to mention in my last email is that in the
> probit/SDT model, the intercept = criterion if the actual classifications
> predictor is contrast-coded (-1 vs. +1), not if dummy coded. The idea is
> basically that it estimates response bias...



Thanks for the clarification. 

R.



> Jake..

> From: jake987722 at hotmail.com.To: r-sig-mixed-models at r-project.org.Date:
> Tue, 11 Mar 2014 21:54:00 -0600.Subject: Re: [R-sig-ME] Modeling
> precision and recall with GLMMs..Hi Ramon,. .I'm not sure that I fully
> understand the details of what you want to accomplish. But I do want to
> ask: you jump right into your email assuming that of course you want to
> model precision and recall, but what about modelling the data directly
> (i.e., individual classification decisions) rather than summaries of the
> data? Then you could work backward (forward?) from the model results to
> compute what the implied precision and recall would be.. .If you decided
> that modelling the data directly would work for your purposes, then one
> way of doing this would be to regress classification decisions ("P" or
> "N") on actual classifications ("P" or "N"). If this is done in a probit
> model, it is equivalent to the equal-variance signal detection model
> studied at length in psychology, with the intercept being the "criterion"
> in signal detection language (denoted c), and the slope being
> "sensitivity" (denoted d' or d-prime). It should definitely be possible
> to compute precision and recall from c and d'. This might be simpler with
> a logit rather than probit link function.. .Let me know if I have
> misunderstood what you are trying to accomplish.. .Jake. .> From:
> rdiaz02 at gmail.com.> To: r-sig-mixed-models at r-project.org.> Date: Tue, 11
> Mar 2014 11:48:57 +0100.> CC: ramon.diaz at iib.uam.es.> Subject: [R-sig-ME]
> Modeling precision and recall with GLMMs.> .> Dear All,.> .> I am
> examining the performance of a couple of classification-like methods.>
> under different scenarios. Two of the metrics I am using are precision
> and.> recall (TP/(TP + FP) and TP/(TP + FN), where TP, FP, and FN are
> "true.> positives", "false positives", and "false negatives" in a simple
> two-way.> confusion matrix). Some of the combinations of methods have
> been used on.> exactly the same data sets. So it is easy to set up a
> binomial model (or.> multinomial2 if using MCMCglmm) such as.> .> .>
> cbind(TP, FP) ~ fixed effects + (1|dataset) .> .> .> .> However, the left
> hand side sounds questionable, specially with precision:.> the expression
> TP/(TP + FP) has, in the denominator, a (TP + FP) [the.> number of
> results returned, or retrieved instances, etc] that, itself, can.> be
> highly method-dependent (i.e., affected by the fixed effects). So
> rather.> than a true proportion, this seems more like a ratio, where each
> of TP and.> FP have their own variance, a covariance, etc, and thus the
> error.> distribution is a mess (not the tidy thing of a binomial)..> .>
> .> I've looked around in the literature and have not found much (maybe
> the.> problem are my searching skills :-). Most people use rankings of
> methods,.> not directly modeling precision or recall in the left-hand
> side of a.> (generalized) linear model. A couple of papers use a linear
> model on the.> log-transformed response (which I think is even worse than
> the above.> binomial model, specially with lots of 0s or 1s). Some other
> people use a.> single measure, such as the F-measure or Matthews
> correlation coefficient,.> and I am using something similar too, but I
> specifically wanted to also.> model precision and recall..> .> .> An
> option would be a multi-response model with MCMCglmm, but I am not sure.>
> if this is appropriate either (dependence of the sum of FP and TP on
> the.> fixed effects)..> .> .> Best,.> .> .> R..> .> -- .> Ramon
> Diaz-Uriarte.> Department of Biochemistry, Lab B-25.> Facultad de
> Medicina .> Universidad Aut.noma de Madrid .> Arzobispo Morcillo, 4.>
> 28029 Madrid.> Spain.> .> Phone: +34-91-497-2412.> .> Email:
> rdiaz02 at gmail.com.> ramon.diaz at iib.uam.es.> .> http://ligarto.org/rdiaz.>
> .> _______________________________________________.>
> R-sig-mixed-models at r-project.org mailing list.>
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models. .. .  ..
> ..[[alternative HTML version
> deleted]]. .._______________________________________________.R-sig-mixed-models at r-project.org
> mailing list.https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> .. .  ..  ..[[alternative HTML version
> deleted]].._______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Ramon Diaz-Uriarte
Department of Biochemistry, Lab B-25
Facultad de Medicina 
Universidad Aut?noma de Madrid 
Arzobispo Morcillo, 4
28029 Madrid
Spain

Phone: +34-91-497-2412

Email: rdiaz02 at gmail.com
       ramon.diaz at iib.uam.es

http://ligarto.org/rdiaz


From paul.johnson at glasgow.ac.uk  Wed Mar 12 12:46:16 2014
From: paul.johnson at glasgow.ac.uk (Paul Johnson)
Date: Wed, 12 Mar 2014 11:46:16 +0000
Subject: [R-sig-ME] Curved residuals vs fitted plot
In-Reply-To: <dc094083f5b84835b47efedbcafbfcef@AM3PR01MB161.eurprd01.prod.exchangelabs.com>
References: <828e0de028ff456282ad24640f6ee915@AM3PR01MB161.eurprd01.prod.exchangelabs.com>,
	<DAB55EFEBDAD1F4888EDB2E8F5180BC96C0B0880F6@CMS03.campus.gla.ac.uk>,
	<88612d7954b8480089e4ed643bb9a509@AM3PR01MB161.eurprd01.prod.exchangelabs.com>
	<dc094083f5b84835b47efedbcafbfcef@AM3PR01MB161.eurprd01.prod.exchangelabs.com>
Message-ID: <9777E8C4-9215-413C-B8FC-166A4B460A22@glasgow.ac.uk>

Hi Victoria,

I get quite a different residual plot from your model - code pasted below - which looks much better. There still appears to be a weak residual trend but these plots can be deceptive especially if there's a mass of points in one part of the graph. The apparent trend might be an artefact of skew at either end of the probability scale. I believe there's no requirement for these residuals to be normal, or even symmetrical, and they're more likely to be skewed close to predicted probabilities of 0 or 1.  E.g. you have several rows with zero responses, which inevitably produce a lump of negative residuals. (This residual plot should also be viewed with caution for the reason I mentioned earlier about standardising the residuals using the binomial variance function rather than the logitnormal-binomial variance function (does it exist, anyone?).)

Probably a better way to assess the model is to plot on the same graph the data and the model predictions, with prediction intervals. E.g. plot prediction intervals on boxplot(Aphid.rm.prop ~ Treatment*Distance.m, data=dframe1).

> > Would you recommend I abandon the two-vector response and try for binary? If I use 0, 1s it can't be overdispersed, but has less power.

If you expand the data from binomial to binary, and still fit the same random effects, you are fitting the same model, with the same power, except that the observation-level overdispersion random effect becomes a more conventional "group" random effect. The only difference will be to make the model fit more slowly. E.g. "+ (1|Site)" implies the the same model in both of these data sets:

Collapsed:
1 2 Site1
2 1 Site2

Expanded:
0 Site1
1 Site1
0 Site1
1 Site2
1 Site2
0 Site2

Best wishes,
Paul



sapply(list(Field,Treatment,Field.section,Transect,Distance.m,Aphid.rm.success,Aphid.remain.fail,fffPlot,Bait.ID),function(x)length(unique(x)))
### Bait.ID has a duplicated level so has n-1 levels - remake with n levels:
Bait.ID<-paste("ID",1:length(Bait.ID),sep="")
dframe1<-data.frame(Field,Treatment,Field.section,Transect,Distance.m,Aphid.rm.success,Aphid.remain.fail,fffPlot,Bait.ID)
rm(Field,Treatment,Field.section,Transect,Distance.m,Aphid.rm.success,Aphid.remain.fail,fffPlot,Bait.ID) # removes ambiguity about source of objects
head(dframe1)
dframe1$n <- dframe1$Aphid.rm.success + dframe1$Aphid.remain.fail
dframe1$Aphid.rm.prop <- dframe1$Aphid.rm.success / dframe1$n

plot(Aphid.rm.prop ~ jitter(Distance.m,factor=0.2), data=dframe1,col=as.numeric(Treatment),pch=as.numeric(Treatment))
boxplot(Aphid.rm.prop ~ Treatment, data=dframe1)
boxplot(Aphid.rm.prop ~ Treatment*Distance.m, data=dframe1)

### I changed Field from random to fixed, because although it's conceptually a random effect, two levels are too few to get a reasonable variance estimate
model3<-glmer(cbind(Aphid.rm.success,Aphid.remain.fail) ~ Treatment*Distance.m+Field+(1|Field.section/Transect/fffPlot/Bait.ID), # Bait.ID to remove overdispersion
binomial(link=logit),data=dframe1) # note I kept fffPlot smallest scale in (better residuals)
summary(model3)
plot(model3) # issue? Or just messy ecological data?

### look at random effect residuals
dotplot(ranef(model3,condVar=TRUE))$Bait.ID
dotplot(ranef(model3,condVar=TRUE))$fffPlot
dotplot(ranef(model3,condVar=TRUE))$Transect  # close to zero variance at this level
dotplot(ranef(model3,condVar=TRUE))$Field.section  # close to zero variance at this level
### these look sufficiently close to normal, but should do a QQ plot, at least on the factors that vary
qqmath(ranef(model3,condVar=TRUE))$Bait.ID
qqmath(ranef(model3,condVar=TRUE))$fffPlot

# shift overdispersion random effect from fitted values to residuals
Fitted <- plogis(qlogis(fitted(model3)) - ranef(model3)$Bait.ID[[1]])
Resid <- (dframe1$Aphid.rm.prop - Fitted) / sqrt(Fitted * (1 - Fitted)/dframe1$n)
# plot residuals differentiating between treatments and distances
plot(Fitted, Resid, col=dframe1$Treatment, pch=21, bg=grey(pnorm(scale(dframe1$Distance.m))))
abline(h=0)



On 11 Mar 2014, at 15:57, Victoria Wickens <V.J.Wickens at pgr.reading.ac.uk> wrote:

> Apologies, the link to the plot did not work. Here is the functioning link to illustrate the bizzare plot  http://imgur.com/rx2Oc3s
>
> Would you suggest this plot illustrates a bad model fit?
>
> Would you recommend I abandon the two-vector response and try for binary? If I use 0, 1s it can't be overdispersed, but has less power. However, if the first model assumptions cannot be upheld because of the residuals, binary might be my only option?
>
> Thank once again for taking the time to read this,
>
> Victoria
> ______________
> Victoria Wickens
> PhD student
> Room GU08
> Centre for Agri-Environmental Research (CAER)
> School of Agriculture, Policy & Development
> University of Reading
> Reading RG6 7BE
> ____________
>
> Email: v.j.wickens at pgr.reading.ac.uk
> http://www.reading.ac.uk/caer/staff_students.html
> https://www.facebook.com/UniRdgAPD
>
> ________________________________________
> From: r-sig-mixed-models-bounces at r-project.org <r-sig-mixed-models-bounces at r-project.org> on behalf of Victoria Wickens <V.J.Wickens at pgr.reading.ac.uk>
> Sent: 10 March 2014 14:29
> To: Paul Johnson; r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Curved residuals vs fitted plot
>
> Hi Paul,
>
> Thank you for reading my email and for providing clear explanation of why I have the curvature. I found your response and the link very helpful. Sadly I am having trouble getting the R book which is not available at my library, but I do have a copy of the Mixed Effects Models and Extensions in Ecology with R by Zuur.
>
> I now understand the fitted vs resid plots produce curved scatter trends due to the observational random effect and have implemented the code you provided to shift the effect from the fitted values to the residuals to create a new plot (see here http://imgur.com/edit).
>
> My plot is not nearly as well behaved as your example though, with a steep tail of residuals with high fitted values. Does this mean my model is a bad fit? I'm not sure of the next best move to take.
>
> I have tried the following (all with the observational random effect, Bait.ID, to remove overdispersion but with similar resid vs fitted plots as seen on the web-link):
>
> 1. glmer
> 2. glmer + polynomial regression (logDistance factor)
> 3. glmer + quadratic term (Distance^2)
> 4. gamm (limited success)
>
> Is there another way to remove overdispersion or account for it? Or am I going about this wrong?
>
> Thank you once again for taking time to help me. I really appreciate it,
>
> Victoria
> ______________
> Victoria Wickens
> PhD student
> Room GU08
> Centre for Agri-Environmental Research (CAER)
> School of Agriculture, Policy & Development
> University of Reading
> Reading RG6 7BE
> ____________
>
> Email: v.j.wickens at pgr.reading.ac.uk
> http://www.reading.ac.uk/caer/staff_students.html
> https://www.facebook.com/UniRdgAPD
>
> ________________________________________
> From: Paul Johnson <paul.johnson at glasgow.ac.uk>
> Sent: 08 March 2014 01:26
> To: Victoria Wickens; r-sig-mixed-models at r-project.org
> Subject: RE: Curved residuals vs fitted plot
>
> Hi Victoria,
>
> Positive correlation between the residuals and fitted values is expected whenever you have a row-level random effect to mop up overdispersion. It happens because overdispersion implies that the low responses are lower than predicted and the high responses are higher than predicted, even allowing for binomial sampling error. The gap between a low predicted proportion and a still lower observed proportion is bridged partly by a negative residual and partly by a negative overdispersion random effect (which can be thought of as an additional 'free' residual to the constrained residual due to binomial sampling error). The same happens at the highest predicted proportions, except that both residual and random effect will be positive, hence the positive correlation between the predicted (fitted) values, which contain the overdispersion random effects, and the residuals.
>
> This problem is dealt with, for binomial GLMMs, in chapter 7 of Alain Zuur's Beginner's guide to GLM and GLMM with R. I don't have this book handy and I can't remember his solution. One solution is to shift the overdispersion random effect from the fitted values to the residuals, which makes sense given that they are really just add-on residuals - see the example code below.
>
> This thread discusses the same problem for lognormal-Poisson GLMMs:
> https://stat.ethz.ch/pipermail/r-sig-mixed-models/2013q3/020819.html
>
> Best wishes,
> Paul
>
> # simulate binomial data with a positive trend and overdispersion
> obs <- factor(1:100)
> x <- seq(-1, 0.98, 0.02)
> set.seed(12345678)
> y <- rbinom(100, 20, plogis(x + rnorm(100, 0, 1)))
> par(mfrow=c(3, 1))
> plot(y/20 ~ x)
> # fit logitnormal-binomial GLMM
> fit <- glmer(cbind(y, 20-y) ~ I(1:100) + (1|obs), family="binomial")
> summary(fit)
> # standard residual vs fitted plot
> plot(fitted(fit), resid(fit))
> abline(h=0)
> # shift overdispersion random effect from fitted values to residuals
> Fitted <- plogis(qlogis(fitted(fit)) - ranef(fit)$obs[[1]])
> Resid <- (y/20 - Fitted) / sqrt(Fitted * (1 - Fitted)/20)
> plot(Fitted, Resid)
> abline(h=0)
> # NB THIS GETS RID OF THE TREND BUT ISN'T RIGHT!
> # This is because I've standardised the variance using the
> # variance function for the binomial, p(1-p)/n, but the distribution
> # underlying the residuals is now a logitnormal-binomial
> # so should incorporate the overdispersion variance. I don't
> # know how to do this.
>
>
> ________________________________________
> From: r-sig-mixed-models-bounces at r-project.org [r-sig-mixed-models-bounces at r-project.org] On Behalf Of Victoria Wickens [V.J.Wickens at pgr.reading.ac.uk]
> Sent: 07 March 2014 19:20
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] Curved residuals vs fitted plot
>
> Dear mixed-models group,
>
> Thank you for taking time to read this plea for help.
>
> I wondered if you could give me any advice regarding the curved residuals vs fitted plot with my model:
>
> model3<-glmer(y~Treatment*Distance.m+(1|Field/Field.section/Transect/fffPlot/Bait.ID),
> binomial(link=logit),data=dframe1)
>
> My aim is to explain aphid removal from bait trays in response to treatment (presence of absence of a flower strip) and distance (five distances, or plots, moving away from the margins; 1, 5, 10, 15, and 20m). (More experimental details after message in case helpful).
>
> I am using R version 3.0.2 and the latest version of lme4. The resid vs fitted figure is not great, is this because I need to look at a logit fitted scale? (See here to view the figure: <http://imgur.com/CYe5CxC> http://imgur.com/CYe5CxC)
>
> Any help at all would be more than amazing,
>
> Victoria
>
> Experimental setup
> There are two fields overall, each contains one treatment (Mit) and one control (Non). Looking at one treatment, in one field section, there are three transects parallel to each other and perpendicular to the margin treatment (T1, T2 and T3). There are five plots along each transect at specific distances from the treatment margins (1, 5, 10, 15, 20m). At each plot, three replica bait trays were placed (a, b and c). Each tray contained 20 aphids glued to foam. In total, I had two farm fields at the largest scale and between them there were 180 bait trays overall (90 per field).
>
>
> Script attached, also listed below
>
>
> library(lme4)
>
> Field<-c("L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,
> "B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B")
>
> Treatment<-c("Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,
> "Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,
> "Mit" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,
> "Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit")
>
> Field.section<-c("D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"B" ,"B" ,
> "B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A")
>
> Transect<-c("T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,
> "T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,
> "T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,
> "T6" ,"T6" ,"T6" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3")
>
> Distance.m<-c(1 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10
> ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20)
>
> Aphid.rm.success<-c(3 ,7 ,19 ,19 ,2 ,0 ,0 ,0 ,0 ,0 ,0 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,0 ,0 ,8 ,16 ,2 ,0 ,2 ,1 ,1 ,3 ,0 ,0 ,9 ,10 ,15 ,18 ,1 ,0 ,14 ,10 ,4 ,1 ,3 ,12 ,2 ,4 ,0 ,20 ,17 ,19 ,11 ,19 ,5 ,0 ,0 ,3 ,0 ,0 ,0 ,3 ,11 ,0 ,19 ,13 ,13 ,10 ,9 ,5 ,19 ,1 ,2 ,0 ,12 ,2 ,0 ,1 ,7 ,15 ,19 ,6 ,19 ,18 ,17 ,1 ,0 ,0 ,1 ,1 ,9 ,18 ,11 ,19 ,1 ,3 ,0 ,0 ,1 ,1 ,0 ,1 ,1 ,3 ,0 ,0 ,0 ,4 ,1 ,0 ,0 ,2 ,1 ,6 ,0 ,0 ,0 ,1 ,0 ,0 ,1 ,1 ,0 ,0 ,2 ,1 ,11 ,1 ,0 ,3 ,3 ,0 ,1 ,3 ,0 ,0 ,1 ,0 ,0 ,8 ,1 ,3 ,2 ,1 ,3 ,0 ,
> 1 ,2 ,0 ,1 ,1 ,0 ,0 ,0 ,8 ,0 ,0 ,0 ,7 ,1 ,0 ,1 ,1 ,0 ,1 ,0 ,1 ,0 ,1 ,5 ,0 ,20 ,0 ,17 ,0 ,10 ,1 ,0 ,5 ,0 ,0 ,0 ,0 ,0)
>
> Aphid.remain.fail<-c(17 ,13 ,1 ,1 ,18 ,20 ,20 ,20 ,20 ,20 ,20 ,19 ,19 ,19 ,17 ,19 ,19 ,19 ,20 ,20 ,12 ,4 ,18 ,20 ,18 ,19 ,19 ,17 ,20 ,20 ,10 ,9 ,5 ,2 ,19 ,20 ,6 ,10 ,16 ,19 ,17 ,8 ,18 ,16 ,20 ,0 ,3 ,1 ,9 ,2 ,15 ,20 ,20 ,17 ,20 ,20 ,20 ,17 ,9 ,20 ,1 ,7 ,7 ,10 ,11 ,15 ,1 ,19 ,18 ,20 ,8 ,18 ,20 ,19 ,13 ,5 ,1 ,14 ,0 ,1 ,3 ,19 ,20 ,20 ,19 ,18 ,11 ,2 ,9 ,0 ,19 ,18 ,20 ,20 ,19 ,19 ,20 ,19 ,19 ,16 ,20 ,20 ,20 ,16 ,19 ,20 ,20 ,18 ,19 ,14 ,20 ,20 ,20 ,19 ,20 ,20 ,19 ,19 ,20 ,20 ,18 ,19 ,
> 9 ,19 ,19 ,18 ,17 ,20 ,19 ,17 ,20 ,20 ,19 ,20 ,17 ,12 ,19 ,17 ,18 ,19 ,17 ,20 ,19 ,18 ,20 ,18 ,20 ,20 ,19 ,20 ,12 ,20 ,20 ,20 ,13 ,19 ,20 ,19 ,19 ,20 ,20 ,20 ,19 ,20 ,19 ,15 ,20 ,0 ,20 ,3 ,20 ,10 ,19 ,20 ,15 ,20 ,20 ,20 ,20 ,20)
>
> fffPlot<-c("D T1 1" ,"D T1 1" ,"D T1 1" ,"D T1 1" ,"D T1 5" ,"D T1 5" ,"D T1 5" ,"D T1 10" ,"D T1 10" ,"D T1 10" ,"D T1 15" ,"D T1 15" ,"D T1 15" ,"D T1 20" ,"D T1 20" ,"D T1 20" ,"D T2 1" ,"D T2 1" ,"D T2 1" ,"D T2 5" ,"D T2 5" ,"D T2 5" ,"D T2 10" ,"D T2 10" ,"D T2 10" ,"D T2 15" ,"D T2 15" ,"D T2 15" ,"D T2 20" ,"D T2 20" ,"D T2 20" ,"D T3 1" ,"D T3 1" ,"D T3 1" ,"D T3 5" ,"D T3 5" ,"D T3 5" ,"D T3 10" ,"D T3 10" ,"D T3 15" ,"D T3 15" ,"D T3 15" ,"D T3 20" ,"D T3 20" ,"D T3 20" ,
> "C T1 1" ,"C T1 1" ,"C T1 1" ,"C T1 5" ,"C T1 5" ,"C T1 5" ,"C T1 10" ,"C T1 10" ,"C T1 10" ,"C T1 15" ,"C T1 15" ,"C T1 15" ,"C T1 20" ,"C T1 20" ,"C T1 20" ,"C T2 1" ,"C T2 1" ,"C T2 1" ,"C T2 5" ,"C T2 5" ,"C T2 5" ,"C T2 10" ,"C T2 10" ,"C T2 10" ,"C T2 15" ,"C T2 15" ,"C T2 15" ,"C T2 20" ,"C T2 20" ,"C T2 20" ,"C T3 1" ,"C T3 1" ,"C T3 1" ,"C T3 5" ,"C T3 5" ,"C T3 5" ,"C T3 10" ,"C T3 10" ,"C T3 10" ,"C T3 15" ,"C T3 15" ,"C T3 15" ,"C T3 20" ,"C T3 20" ,"C T3 20" ,"B T1 1" ,
> "B T1 1" ,"B T1 1" ,"B T1 5" ,"B T1 5" ,"B T1 5" ,"B T1 10" ,"B T1 10" ,"B T1 10" ,"B T1 15" ,"B T1 15" ,"B T1 15" ,"B T1 20" ,"B T1 20" ,"B T1 20" ,"B T2 1" ,"B T2 1" ,"B T2 1" ,"B T2 5" ,"B T2 5" ,"B T2 5" ,"B T2 10" ,"B T2 10" ,"B T2 10" ,"B T2 15" ,"B T2 15" ,"B T2 15" ,"B T2 20" ,"B T2 20" ,"B T2 20" ,"B T3 1" ,"B T3 1" ,"B T3 1" ,"B T3 5" ,"B T3 5" ,"B T3 5" ,"B T3 10" ,"B T3 10" ,"B T3 10" ,"B T3 15" ,"B T3 15" ,"B T3 15" ,"B T3 20" ,"B T3 20" ,"B T3 20" ,"A T1 1" ,"A T1 1" ,"A T1 1" ,
> "A T1 5" ,"A T1 5" ,"A T1 5" ,"A T1 10" ,"A T1 10" ,"A T1 10" ,"A T1 15" ,"A T1 15" ,"A T1 15" ,"A T1 20" ,"A T1 20" ,"A T1 20" ,"A T2 1" ,"A T2 1" ,"A T2 1" ,"A T2 5" ,"A T2 5" ,"A T2 5" ,"A T2 10" ,"A T2 10" ,"A T2 10" ,"A T2 15" ,"A T2 15" ,"A T2 15" ,"A T2 20" ,"A T2 20" ,"A T2 20" ,"A T3 1" ,"A T3 1" ,"A T3 1" ,"A T3 5" ,"A T3 5" ,"A T3 5" ,"A T3 10" ,"A T3 10" ,"A T3 10" ,"A T3 15" ,"A T3 15" ,"A T3 15" ,"A T3 20" ,"A T3 20" ,"A T3 20")
>
> Bait.ID<-c("ID1" ,"ID2" ,"ID3" ,"ID3" ,"ID4" ,"ID5" ,"ID6" ,"ID7" ,"ID8" ,"ID9" ,"ID10" ,"ID11" ,"ID12" ,"ID13" ,"ID14" ,"ID15" ,"ID16" ,"ID17" ,"ID18" ,"ID19" ,"ID20" ,"ID21" ,"ID22" ,"ID23" ,"ID24" ,"ID25" ,"ID26" ,"ID27" ,"ID28" ,"ID29" ,"ID30" ,"ID31" ,"ID32" ,"ID33" ,"ID34" ,"ID35" ,"ID36" ,"ID37" ,"ID38" ,"ID39" ,"ID40" ,"ID41" ,"ID42" ,"ID43" ,"ID44" ,"ID45" ,"ID46" ,"ID47" ,"ID48" ,"ID49" ,"ID50" ,"ID51" ,"ID52" ,"ID53" ,"ID54" ,"ID55" ,"ID56" ,"ID57" ,"ID58" ,"ID59" ,"ID60" ,"ID61" ,"ID62" ,
> "ID63" ,"ID64" ,"ID65" ,"ID66" ,"ID67" ,"ID68" ,"ID69" ,"ID70" ,"ID71" ,"ID72" ,"ID73" ,"ID74" ,"ID75" ,"ID76" ,"ID77" ,"ID78" ,"ID79" ,"ID80" ,"ID81" ,"ID82" ,"ID83" ,"ID84" ,"ID85" ,"ID86" ,"ID87" ,"ID88" ,"ID89" ,"ID90" ,"ID91" ,"ID92" ,"ID93" ,"ID94" ,"ID95" ,"ID96" ,"ID97" ,"ID98" ,"ID99" ,"ID100" ,"ID101" ,"ID102" ,"ID103" ,"ID104" ,"ID105" ,"ID106" ,"ID107" ,"ID108" ,"ID109" ,"ID110" ,"ID111" ,"ID112" ,"ID113" ,"ID114" ,"ID115" ,"ID116" ,"ID117" ,"ID118" ,"ID119" ,"ID120" ,"ID121" ,"ID122" ,
> "ID123" ,"ID124" ,"ID125" ,"ID126" ,"ID127" ,"ID128" ,"ID129" ,"ID130" ,"ID131" ,"ID132" ,"ID133" ,"ID134" ,"ID135" ,"ID136" ,"ID137" ,"ID138" ,"ID139" ,"ID140" ,"ID141" ,"ID142" ,"ID143" ,"ID144" ,"ID145" ,"ID146" ,"ID147" ,"ID148" ,"ID149" ,"ID150" ,"ID151" ,"ID152" ,"ID153" ,"ID154" ,"ID155" ,"ID156" ,"ID157" ,"ID158" ,"ID159" ,"ID160" ,"ID161" ,"ID162" ,"ID163" ,"ID164" ,"ID165" ,"ID166" ,"ID167" ,"ID168" ,"ID169" ,"ID170" ,"ID171" ,"ID172" ,"ID173" ,"ID174" ,"ID175" ,"ID176" ,"ID177" ,"ID178" ,
> "ID179")
>
>
> dframe1<-data.frame(Field,Treatment,Field.section,Transect,Distance.m,Aphid.rm.success,Aphid.remain.fail,fffPlot,Bait.ID)
>
> y<-cbind(Aphid.rm.success,Aphid.remain.fail)
>
> model3<-glmer(y~Treatment*Distance.m+(1|Field/Field.section/Transect/fffPlot/Bait.ID), # Bait.ID to remove overdispersion
> binomial(link=logit),data=dframe1) # note I kept fffPlot smallest scale in (better residuals)
> summary(model3)
>
> plot(model3) # issue? Or just messy ecological data?
>
> ______________
> Victoria Wickens
> PhD student
> Room GU08
> Centre for Agri-Environmental Research (CAER)
> School of Agriculture, Policy & Development
> University of Reading
> Reading RG6 7BE
> ____________
>
> Email: v.j.wickens at pgr.reading.ac.uk<mailto:v.j.wickens at pgr.reading.ac.uk>
> http://www.reading.ac.uk/caer/staff_students.html
> https://www.facebook.com/UniRdgAPD
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From Daniel.Wright at act.org  Wed Mar 12 15:26:28 2014
From: Daniel.Wright at act.org (Daniel Wright)
Date: Wed, 12 Mar 2014 14:26:28 +0000
Subject: [R-sig-ME] Modeling precision and recall with GLMMs
In-Reply-To: <87d2hr4rd2.fsf@gmail.com>
References: <87ob1d57ty.fsf@gmail.com>
	<BAY172-W469E1A75B654CD55248982CB760@phx.gbl>
	<87d2hr4rd2.fsf@gmail.com>
Message-ID: <3a6ddeda8b7442308e2a4062f4d57d80@BLUPR04MB610.namprd04.prod.outlook.com>

The "how good or bad" each method is, is what will come out of the method Jake is suggesting.

Using multilevel models for these is common in the memory recognition literature in psychology for the last decade or so, but is also relevant in lots of other areas like medical diagnostics. If the variable IS_ij is whether person i saw stimulus j (0 not seen, 1 seen), and SAY_ij is whether the person says she saw the stimuli, then a multilevel probit or logit regression, with careful coding of the variables, can mimic the standard SDT models. The critical variable for saying if people are accurate is the coefficient in front of SAY. If you have different conditions, COND_j, then interactions between COND_j (or COND_ij if varied within subject) and SAY_ij examine if accuracy varies among these. An important plus of the multilevel models is the coefficients can vary by person and/or stimuli. 


> Hi Ramon,

> I'm not sure that I fully understand the details of what you want to 
> accomplish. But I do want to ask: you jump right into your email 
> assuming that of course you want to model precision and recall, but 
> what about modelling the data directly (i.e., individual 
> classification
> decisions) rather than summaries of the data? Then you could work 
> backward (forward?) from the model results to compute what the implied 
> precision and recall would be

Sorry I did not provide enough details. I am comparing some methods for reconstructing networks, and the True positives and False positives, for instance, refer to the number of correctly inferred edges and to the number of edges that a procedure recovers that are not in the original network, respectively.

So the network reconstruction methods model the data directly, and what I want to model is how good or bad are what they return as a function of several other variables (related to several dimensions of the toughness of the problem, etc)


> If you decided that modelling the data directly would work for your 
> purposes, then one way of doing this would be to regress 
> classification decisions ("P" or "N") on actual classifications ("P" or "N").

I am not sure that would work. For each data set, each method returns a bunch of "P"s and "N"s. But what I want to do is model not the relationship between truth and prediction, but rather how good or bad each method is (at trying to reconstruct the truth).

> If this is done in a probit model, it is equivalent to the 
> equal-variance signal detection model studied at length in psychology, 
> with the intercept being the "criterion" in signal detection language 
> (denoted c), and the slope being "sensitivity" (denoted d' or 
> d-prime). It should definitely be possible to compute precision and 
> recall from c and d'.

I am not familiar with this approach in psychology. As I say above, I am not sure this addresses the problem I want to address but do you have some pointer to the literature where I can read more about the approach?


Best,


R.

> This might be simpler with a logit rather than probit link function.
>
> Let me know if I have misunderstood what you are trying to accomplish

> Jake

>> From: rdiaz02 at gmail.com.> To:
>> r-sig-mixed-models at r-project.org.> Date: Tue, 11 Mar 2014 11:48:57
>> +0100.> CC: ramon.diaz at iib.uam.es.> Subject: [R-sig-ME] Modeling
>> precision and recall with GLMMs.> .

>>  Dear All,. .

>>  I am examining the performance of a couple of classification-like  
>> methods. under different scenarios. Two of the metrics I am using are  
>> precision and. recall (TP/(TP + FP) and TP/(TP + FN), where TP, FP, 
>> and  FN are "true. positives", "false positives", and "false 
>> negatives" in a  simple two-way. confusion matrix). Some of the 
>> combinations of methods  have been used on. exactly the same data 
>> sets. So it is easy to set up a  binomial model (or. multinomial2 if using MCMCglmm) such as.

>> cbind(TP, FP) ~ fixed effects + (1|dataset)

>> However, the left hand side sounds questionable, specially with 
>> precision:. the expression TP/(TP + FP) has, in the denominator, a 
>> (TP +
>> FP) [the. number of results returned, or retrieved instances, etc] 
>> that, itself, can. be highly method-dependent (i.e., affected by the 
>> fixed effects). So rather. than a true proportion, this seems more 
>> like a ratio, where each of TP and. FP have their own variance, a 
>> covariance, etc, and thus the error. distribution is a mess (not the 
>> tidy thing of a binomial).


>> I've looked around in the literature and have not found much (maybe 
>> the. problem are my searching skills :-). Most people use rankings of 
>> methods,. not directly modeling precision or recall in the left-hand 
>> side of a. (generalized) linear model. A couple of papers use a 
>> linear model on the. log-transformed response (which I think is even 
>> worse than the above. binomial model, specially with lots of 0s or 
>> 1s). Some other people use a. single measure, such as the F-measure 
>> or Matthews correlation coefficient,. and I am using something 
>> similar too, but I specifically wanted to also. model precision and recall.. . .

>> An option would be a multi-response model with MCMCglmm, but I am not 
>> sure if this is appropriate either (dependence of the sum of FP and 
>> TP on the. fixed effects).. . .


>> Best,

 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

--
Ramon Diaz-Uriarte
Department of Biochemistry, Lab B-25
Facultad de Medicina
Universidad Aut?noma de Madrid
Arzobispo Morcillo, 4
28029 Madrid
Spain

Phone: +34-91-497-2412

Email: rdiaz02 at gmail.com
       ramon.diaz at iib.uam.es

http://ligarto.org/rdiaz

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From jake987722 at hotmail.com  Wed Mar 12 18:58:28 2014
From: jake987722 at hotmail.com (Jake Westfall)
Date: Wed, 12 Mar 2014 11:58:28 -0600
Subject: [R-sig-ME] Modeling precision and recall with GLMMs
In-Reply-To: <3a6ddeda8b7442308e2a4062f4d57d80@BLUPR04MB610.namprd04.prod.outlook.com>
References: <87ob1d57ty.fsf@gmail.com>,
	<BAY172-W469E1A75B654CD55248982CB760@phx.gbl>
	<87d2hr4rd2.fsf@gmail.com>,
	<3a6ddeda8b7442308e2a4062f4d57d80@BLUPR04MB610.namprd04.prod.outlook.com>
Message-ID: <BAY172-W20D42F1BD2F1B154BC35D5CB760@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140312/dd80052f/attachment.pl>

From msaussac at chu-clermontferrand.fr  Wed Mar 12 09:39:21 2014
From: msaussac at chu-clermontferrand.fr (Saussac Mathilde)
Date: Wed, 12 Mar 2014 09:39:21 +0100
Subject: [R-sig-ME] problem with lme4
In-Reply-To: <531F33A4.3020204@gmail.com>
References: <0523A5CE5D1EED4D9A9846E530EE382D040C7A4EED31@E2K7-CCR.chu-clermontferrand.fr>
	<531F33A4.3020204@gmail.com>
Message-ID: <0523A5CE5D1EED4D9A9846E530EE382D040C7A4EED48@E2K7-CCR.chu-clermontferrand.fr>

Thank you very much for your answers ! 

I was using the version 2.15.3 of R and 1.0-4 of lme4. 

You are right, there a complete separation in the case I told you: 
> with(d,table(AP2,Motif))
      Motif
AP2     0       1
  0         14    84
  1         0       44
  2         3      113 

I downloaded the latest versions of R and lme4 and I don't have mistakes in this case anymore ! 

But for others variables, like this case, I still have the mistake  : 
> modelM31R <- glmer(Motif~as.factor(recours) + (1|ID), data=d31, family=binomial(link="logit"))
Error: pwrssUpdate did not converge in 30 iterations
> with(d31,table(Motif,recours))
        recours
Motif     1        2     3     4
    0          7        8     1     1
    1          185   41   11   4
Or I have some mistakes when I want to use the function interaction in the model. 

As you suggest, I am going try to use the blme package or MCMCglmm.

Thank you very much for your help and your advices,  
Mathilde

-----Message d'origine-----
De?: Ben Bolker [mailto:bbolker at gmail.com] 
Envoy??: mardi 11 mars 2014 17:03
??: Saussac Mathilde; r-sig-mixed-models at r-project.org
Cc?: Pereira Bruno
Objet?: Re: problem with lme4

On 14-03-11 11:43 AM, Saussac Mathilde wrote:
> Dear all,
> 
>  
> 
> I am a French student in BioStatistic and I have a question about the 
> package lme4.
> 
>  
> 
> I am using the glmer function but it doesn't work . I have these errors :

  Can you say what version you are using?

  From your results below it looks very much like you have issues with complete separation (i.e., some categories in your data have all-success or all-failure outcomes).  In general a binomial model with extreme parameters (absolute value > 10) suggests complete or near-complete separation [on the probability scale a change of 10 log-odds units corresponds to a change from p=0.5 to p=0.99995, or from 0.006 to 0.993 ...]  More recent versions of lme4 are somewhat better at dealing with the complete-separation issue; these are available from Github or from http://lme4.r-forge.r-project.org/repos,
but we are also hoping that a new version will arrive on CRAN within the next few days (week?).

  More generally, adjusting tolerances is (as you suggest) a little bit dangerous, as it may get you silly results.

  The most principled thing to do with complete-separation results is to use some form of penalization or regularization or prior (closely related concepts for ways of squashing extreme values in toward zero); at present you can use the blme package or MCMCglmm to impose such priors.

  Can you give a reproducible example
<http://tinyurl.com/reproducible-000>, or perhaps the results of
with(d,table(AP2,Motif)) ?  I would expect that some categories of AP2 have all successes or none ...

> 
> */>model <- glmer(Motif ~ as.factor(AP2) + (1|ID), data=d,
> family=binomial(link="logit"))/*
> 
> /Erreur dans pwrssUpdate(pp, resp, tolPwrss, GQmat, compDev, fac,
> verbose) : /
> 
> /  Downdated VtV is not positive definite /
> 
> Or sometimes this one :
> 
> /Erreur : pwrssUpdate did not converge in 30 iterations/
> 
> I found on the web, that we can change the tolerance (Tolpwrss) and 
> indeed it works.
> 
> But, for different values, my results are completely different (see 
> the examples for tolPwrss = 1e-5 and 1e-2) :
> 
> */> model <- glmer(Motif ~ as.factor(AP2) + (1|ID), data=d, 
> family=binomial(link="logit"),control= glmerControl(tolPwrss=1e-5,
> optimizer="bobyqa"))/*
> 
> /Generalized linear mixed model fit by maximum likelihood 
> ['glmerMod']/
> 
> /Family: binomial ( logit )/
> 
> /Formula: Motif ~ as.factor(AP2) + (1 | ID) /
> 
> /   Data: d /
> 
> / /
> 
> /     AIC      BIC   logLik deviance /
> 
> / 78.9974  93.2092 -35.4987  70.9974 /
> 
> / /
> 
> /Random effects:/
> 
> /Groups Name        Variance Std.Dev./
> 
> /ID     (Intercept) 578.5    24.05   /
> 
> /Number of obs: 258, groups: ID, 218/
> 
> / /
> 
> /Fixed effects:/
> 
> /                 Estimate Std. Error z value Pr(>|z|)  /
> 
> /(Intercept)        10.205      6.149   1.660    0.097 ./
> 
> /as.factor(AP2)1    15.057  46157.700   0.000    1.000  /
> 
> /as.factor(AP2)2     9.356     10.219   0.916    0.360  /
> 
> /---/
> 
> /Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' //0.1 ' ' 1 /
> 
> / /
> 
> /Correlation of Fixed Effects:/
> 
> /            (Intr) a.(AP2)1/
> 
> /as.fc(AP2)1  0.000         /
> 
> /as.fc(AP2)2 -0.303  0.000  /
> 
> / /
> 
> */> model <- glmer(Motif ~ as.factor(AP2) + (1|ID), data=d, 
> family=binomial(link="logit"),control= glmerControl(tolPwrss=1e-2,
> optimizer="bobyqa"))/*
> 
> /Generalized linear mixed model fit by maximum likelihood 
> ['glmerMod']/
> 
> /Family: binomial ( logit )/
> 
> /Formula: Motif ~ as.factor(AP2) + (1 | ID) /
> 
> /   Data: d /
> 
> / /
> 
> /     AIC      BIC   logLik deviance /
> 
> /112.9273 127.1392 -52.4637 104.9273 /
> 
> / /
> 
> /Random effects:/
> 
> /Groups Name        Variance Std.Dev./
> 
> /ID     (Intercept) 3.794    1.948   /
> 
> /Number of obs: 258, groups: ID, 218/
> 
> / /
> 
> /Fixed effects:/
> 
> /                Estimate Std. Error z value Pr(>|z|)    /
> 
> /(Intercept)       2.8039     0.4435   6.322 2.58e-10 ***/
> 
> /as.factor(AP2)1  10.5258   117.9609   0.089   0.9289    /
> 
> /as.factor(AP2)2   2.1162     1.0125   2.090   0.0366 *  /
> 
> /---/
> 
> /Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 /
> 
> / /
> 
> /Correlation of Fixed Effects:/
> 
> /            (Intr) a.(AP2)1/
> 
> /as.fc(AP2)1 -0.004         /
> 
> /as.fc(AP2)2 -0.415  0.002  /
> 
> / /
> 
> So I want to know, until which threshold of tolPwrss are results reliable ??
> 
> Is it correct to change this value for all my model ? (Sometimes it 
> doesn't work until I put tolPwrss=0.01 or 0.1 ..)
> 
>  
> 
> I hope you will understand what I explained and you will be able to 
> help me ..
>  
> If there is few things I haven't been clear, don't hesitate to ask me 
> again.
>  
> Thank you for your attention to these matters.
> 
>  
> 
> Best regards,
> 
>  
> 
> Mathilde
> 
>  
> 


From jbauder at oriannesociety.org  Wed Mar 12 20:32:55 2014
From: jbauder at oriannesociety.org (Javan Bauder)
Date: Wed, 12 Mar 2014 15:32:55 -0400
Subject: [R-sig-ME] formatting lmer with nested fixed and random effects
Message-ID: <003301cf3e29$de2ce860$9a86b920$@org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140312/c9d6d0c4/attachment.pl>

From V.J.Wickens at pgr.reading.ac.uk  Wed Mar 12 23:28:20 2014
From: V.J.Wickens at pgr.reading.ac.uk (Victoria Wickens)
Date: Wed, 12 Mar 2014 22:28:20 +0000
Subject: [R-sig-ME] Curved residuals vs fitted plot
In-Reply-To: <9777E8C4-9215-413C-B8FC-166A4B460A22@glasgow.ac.uk>
References: <828e0de028ff456282ad24640f6ee915@AM3PR01MB161.eurprd01.prod.exchangelabs.com>,
	<DAB55EFEBDAD1F4888EDB2E8F5180BC96C0B0880F6@CMS03.campus.gla.ac.uk>,
	<88612d7954b8480089e4ed643bb9a509@AM3PR01MB161.eurprd01.prod.exchangelabs.com>
	<dc094083f5b84835b47efedbcafbfcef@AM3PR01MB161.eurprd01.prod.exchangelabs.com>,
	<9777E8C4-9215-413C-B8FC-166A4B460A22@glasgow.ac.uk>
Message-ID: <24108904e9684e4b92e33c9e67ca90ed@AM3PR01MB161.eurprd01.prod.exchangelabs.com>

Hi Paul,

I can't say thank you enough, honestly. I followed the code you provided and found your advice so incredibly helpful. Here's hoping other people with a similar situation will read this and be helped too.

Thank you so much,

Victoria
______________
Victoria Wickens
PhD student
Room GU08
Centre for Agri-Environmental Research (CAER)
School of Agriculture, Policy & Development
University of Reading
Reading RG6 7BE
____________

Email: v.j.wickens at pgr.reading.ac.uk
http://www.reading.ac.uk/caer/staff_students.html
https://www.facebook.com/UniRdgAPD

________________________________________
From: Paul Johnson <paul.johnson at glasgow.ac.uk>
Sent: 12 March 2014 11:46
To: Victoria Wickens
Cc: r-sig-mixed-models at r-project.org
Subject: Re: Curved residuals vs fitted plot

Hi Victoria,

I get quite a different residual plot from your model - code pasted below - which looks much better. There still appears to be a weak residual trend but these plots can be deceptive especially if there's a mass of points in one part of the graph. The apparent trend might be an artefact of skew at either end of the probability scale. I believe there's no requirement for these residuals to be normal, or even symmetrical, and they're more likely to be skewed close to predicted probabilities of 0 or 1.  E.g. you have several rows with zero responses, which inevitably produce a lump of negative residuals. (This residual plot should also be viewed with caution for the reason I mentioned earlier about standardising the residuals using the binomial variance function rather than the logitnormal-binomial variance function (does it exist, anyone?).)

Probably a better way to assess the model is to plot on the same graph the data and the model predictions, with prediction intervals. E.g. plot prediction intervals on boxplot(Aphid.rm.prop ~ Treatment*Distance.m, data=dframe1).

> > Would you recommend I abandon the two-vector response and try for binary? If I use 0, 1s it can't be overdispersed, but has less power.

If you expand the data from binomial to binary, and still fit the same random effects, you are fitting the same model, with the same power, except that the observation-level overdispersion random effect becomes a more conventional "group" random effect. The only difference will be to make the model fit more slowly. E.g. "+ (1|Site)" implies the the same model in both of these data sets:

Collapsed:
1 2 Site1
2 1 Site2

Expanded:
0 Site1
1 Site1
0 Site1
1 Site2
1 Site2
0 Site2

Best wishes,
Paul



sapply(list(Field,Treatment,Field.section,Transect,Distance.m,Aphid.rm.success,Aphid.remain.fail,fffPlot,Bait.ID),function(x)length(unique(x)))
### Bait.ID has a duplicated level so has n-1 levels - remake with n levels:
Bait.ID<-paste("ID",1:length(Bait.ID),sep="")
dframe1<-data.frame(Field,Treatment,Field.section,Transect,Distance.m,Aphid.rm.success,Aphid.remain.fail,fffPlot,Bait.ID)
rm(Field,Treatment,Field.section,Transect,Distance.m,Aphid.rm.success,Aphid.remain.fail,fffPlot,Bait.ID) # removes ambiguity about source of objects
head(dframe1)
dframe1$n <- dframe1$Aphid.rm.success + dframe1$Aphid.remain.fail
dframe1$Aphid.rm.prop <- dframe1$Aphid.rm.success / dframe1$n

plot(Aphid.rm.prop ~ jitter(Distance.m,factor=0.2), data=dframe1,col=as.numeric(Treatment),pch=as.numeric(Treatment))
boxplot(Aphid.rm.prop ~ Treatment, data=dframe1)
boxplot(Aphid.rm.prop ~ Treatment*Distance.m, data=dframe1)

### I changed Field from random to fixed, because although it's conceptually a random effect, two levels are too few to get a reasonable variance estimate
model3<-glmer(cbind(Aphid.rm.success,Aphid.remain.fail) ~ Treatment*Distance.m+Field+(1|Field.section/Transect/fffPlot/Bait.ID), # Bait.ID to remove overdispersion
binomial(link=logit),data=dframe1) # note I kept fffPlot smallest scale in (better residuals)
summary(model3)
plot(model3) # issue? Or just messy ecological data?

### look at random effect residuals
dotplot(ranef(model3,condVar=TRUE))$Bait.ID
dotplot(ranef(model3,condVar=TRUE))$fffPlot
dotplot(ranef(model3,condVar=TRUE))$Transect  # close to zero variance at this level
dotplot(ranef(model3,condVar=TRUE))$Field.section  # close to zero variance at this level
### these look sufficiently close to normal, but should do a QQ plot, at least on the factors that vary
qqmath(ranef(model3,condVar=TRUE))$Bait.ID
qqmath(ranef(model3,condVar=TRUE))$fffPlot

# shift overdispersion random effect from fitted values to residuals
Fitted <- plogis(qlogis(fitted(model3)) - ranef(model3)$Bait.ID[[1]])
Resid <- (dframe1$Aphid.rm.prop - Fitted) / sqrt(Fitted * (1 - Fitted)/dframe1$n)
# plot residuals differentiating between treatments and distances
plot(Fitted, Resid, col=dframe1$Treatment, pch=21, bg=grey(pnorm(scale(dframe1$Distance.m))))
abline(h=0)



On 11 Mar 2014, at 15:57, Victoria Wickens <V.J.Wickens at pgr.reading.ac.uk> wrote:

> Apologies, the link to the plot did not work. Here is the functioning link to illustrate the bizzare plot  http://imgur.com/rx2Oc3s
>
> Would you suggest this plot illustrates a bad model fit?
>
> Would you recommend I abandon the two-vector response and try for binary? If I use 0, 1s it can't be overdispersed, but has less power. However, if the first model assumptions cannot be upheld because of the residuals, binary might be my only option?
>
> Thank once again for taking the time to read this,
>
> Victoria
> ______________
> Victoria Wickens
> PhD student
> Room GU08
> Centre for Agri-Environmental Research (CAER)
> School of Agriculture, Policy & Development
> University of Reading
> Reading RG6 7BE
> ____________
>
> Email: v.j.wickens at pgr.reading.ac.uk
> http://www.reading.ac.uk/caer/staff_students.html
> https://www.facebook.com/UniRdgAPD
>
> ________________________________________
> From: r-sig-mixed-models-bounces at r-project.org <r-sig-mixed-models-bounces at r-project.org> on behalf of Victoria Wickens <V.J.Wickens at pgr.reading.ac.uk>
> Sent: 10 March 2014 14:29
> To: Paul Johnson; r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Curved residuals vs fitted plot
>
> Hi Paul,
>
> Thank you for reading my email and for providing clear explanation of why I have the curvature. I found your response and the link very helpful. Sadly I am having trouble getting the R book which is not available at my library, but I do have a copy of the Mixed Effects Models and Extensions in Ecology with R by Zuur.
>
> I now understand the fitted vs resid plots produce curved scatter trends due to the observational random effect and have implemented the code you provided to shift the effect from the fitted values to the residuals to create a new plot (see here http://imgur.com/edit).
>
> My plot is not nearly as well behaved as your example though, with a steep tail of residuals with high fitted values. Does this mean my model is a bad fit? I'm not sure of the next best move to take.
>
> I have tried the following (all with the observational random effect, Bait.ID, to remove overdispersion but with similar resid vs fitted plots as seen on the web-link):
>
> 1. glmer
> 2. glmer + polynomial regression (logDistance factor)
> 3. glmer + quadratic term (Distance^2)
> 4. gamm (limited success)
>
> Is there another way to remove overdispersion or account for it? Or am I going about this wrong?
>
> Thank you once again for taking time to help me. I really appreciate it,
>
> Victoria
> ______________
> Victoria Wickens
> PhD student
> Room GU08
> Centre for Agri-Environmental Research (CAER)
> School of Agriculture, Policy & Development
> University of Reading
> Reading RG6 7BE
> ____________
>
> Email: v.j.wickens at pgr.reading.ac.uk
> http://www.reading.ac.uk/caer/staff_students.html
> https://www.facebook.com/UniRdgAPD
>
> ________________________________________
> From: Paul Johnson <paul.johnson at glasgow.ac.uk>
> Sent: 08 March 2014 01:26
> To: Victoria Wickens; r-sig-mixed-models at r-project.org
> Subject: RE: Curved residuals vs fitted plot
>
> Hi Victoria,
>
> Positive correlation between the residuals and fitted values is expected whenever you have a row-level random effect to mop up overdispersion. It happens because overdispersion implies that the low responses are lower than predicted and the high responses are higher than predicted, even allowing for binomial sampling error. The gap between a low predicted proportion and a still lower observed proportion is bridged partly by a negative residual and partly by a negative overdispersion random effect (which can be thought of as an additional 'free' residual to the constrained residual due to binomial sampling error). The same happens at the highest predicted proportions, except that both residual and random effect will be positive, hence the positive correlation between the predicted (fitted) values, which contain the overdispersion random effects, and the residuals.
>
> This problem is dealt with, for binomial GLMMs, in chapter 7 of Alain Zuur's Beginner's guide to GLM and GLMM with R. I don't have this book handy and I can't remember his solution. One solution is to shift the overdispersion random effect from the fitted values to the residuals, which makes sense given that they are really just add-on residuals - see the example code below.
>
> This thread discusses the same problem for lognormal-Poisson GLMMs:
> https://stat.ethz.ch/pipermail/r-sig-mixed-models/2013q3/020819.html
>
> Best wishes,
> Paul
>
> # simulate binomial data with a positive trend and overdispersion
> obs <- factor(1:100)
> x <- seq(-1, 0.98, 0.02)
> set.seed(12345678)
> y <- rbinom(100, 20, plogis(x + rnorm(100, 0, 1)))
> par(mfrow=c(3, 1))
> plot(y/20 ~ x)
> # fit logitnormal-binomial GLMM
> fit <- glmer(cbind(y, 20-y) ~ I(1:100) + (1|obs), family="binomial")
> summary(fit)
> # standard residual vs fitted plot
> plot(fitted(fit), resid(fit))
> abline(h=0)
> # shift overdispersion random effect from fitted values to residuals
> Fitted <- plogis(qlogis(fitted(fit)) - ranef(fit)$obs[[1]])
> Resid <- (y/20 - Fitted) / sqrt(Fitted * (1 - Fitted)/20)
> plot(Fitted, Resid)
> abline(h=0)
> # NB THIS GETS RID OF THE TREND BUT ISN'T RIGHT!
> # This is because I've standardised the variance using the
> # variance function for the binomial, p(1-p)/n, but the distribution
> # underlying the residuals is now a logitnormal-binomial
> # so should incorporate the overdispersion variance. I don't
> # know how to do this.
>
>
> ________________________________________
> From: r-sig-mixed-models-bounces at r-project.org [r-sig-mixed-models-bounces at r-project.org] On Behalf Of Victoria Wickens [V.J.Wickens at pgr.reading.ac.uk]
> Sent: 07 March 2014 19:20
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] Curved residuals vs fitted plot
>
> Dear mixed-models group,
>
> Thank you for taking time to read this plea for help.
>
> I wondered if you could give me any advice regarding the curved residuals vs fitted plot with my model:
>
> model3<-glmer(y~Treatment*Distance.m+(1|Field/Field.section/Transect/fffPlot/Bait.ID),
> binomial(link=logit),data=dframe1)
>
> My aim is to explain aphid removal from bait trays in response to treatment (presence of absence of a flower strip) and distance (five distances, or plots, moving away from the margins; 1, 5, 10, 15, and 20m). (More experimental details after message in case helpful).
>
> I am using R version 3.0.2 and the latest version of lme4. The resid vs fitted figure is not great, is this because I need to look at a logit fitted scale? (See here to view the figure: <http://imgur.com/CYe5CxC> http://imgur.com/CYe5CxC)
>
> Any help at all would be more than amazing,
>
> Victoria
>
> Experimental setup
> There are two fields overall, each contains one treatment (Mit) and one control (Non). Looking at one treatment, in one field section, there are three transects parallel to each other and perpendicular to the margin treatment (T1, T2 and T3). There are five plots along each transect at specific distances from the treatment margins (1, 5, 10, 15, 20m). At each plot, three replica bait trays were placed (a, b and c). Each tray contained 20 aphids glued to foam. In total, I had two farm fields at the largest scale and between them there were 180 bait trays overall (90 per field).
>
>
> Script attached, also listed below
>
>
> library(lme4)
>
> Field<-c("L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,"L" ,
> "B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B")
>
> Treatment<-c("Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,
> "Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,
> "Mit" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,
> "Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Non" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit" ,"Mit")
>
> Field.section<-c("D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"D" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"C" ,"B" ,"B" ,
> "B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"B" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A" ,"A")
>
> Transect<-c("T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T10" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T11" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,
> "T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T12" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T7" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T8" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,
> "T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T9" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T4" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T5" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,"T6" ,
> "T6" ,"T6" ,"T6" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T1" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T2" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3" ,"T3")
>
> Distance.m<-c(1 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10
> ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20 ,1 ,1 ,1 ,5 ,5 ,5 ,10 ,10 ,10 ,15 ,15 ,15 ,20 ,20 ,20)
>
> Aphid.rm.success<-c(3 ,7 ,19 ,19 ,2 ,0 ,0 ,0 ,0 ,0 ,0 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,0 ,0 ,8 ,16 ,2 ,0 ,2 ,1 ,1 ,3 ,0 ,0 ,9 ,10 ,15 ,18 ,1 ,0 ,14 ,10 ,4 ,1 ,3 ,12 ,2 ,4 ,0 ,20 ,17 ,19 ,11 ,19 ,5 ,0 ,0 ,3 ,0 ,0 ,0 ,3 ,11 ,0 ,19 ,13 ,13 ,10 ,9 ,5 ,19 ,1 ,2 ,0 ,12 ,2 ,0 ,1 ,7 ,15 ,19 ,6 ,19 ,18 ,17 ,1 ,0 ,0 ,1 ,1 ,9 ,18 ,11 ,19 ,1 ,3 ,0 ,0 ,1 ,1 ,0 ,1 ,1 ,3 ,0 ,0 ,0 ,4 ,1 ,0 ,0 ,2 ,1 ,6 ,0 ,0 ,0 ,1 ,0 ,0 ,1 ,1 ,0 ,0 ,2 ,1 ,11 ,1 ,0 ,3 ,3 ,0 ,1 ,3 ,0 ,0 ,1 ,0 ,0 ,8 ,1 ,3 ,2 ,1 ,3 ,0 ,
> 1 ,2 ,0 ,1 ,1 ,0 ,0 ,0 ,8 ,0 ,0 ,0 ,7 ,1 ,0 ,1 ,1 ,0 ,1 ,0 ,1 ,0 ,1 ,5 ,0 ,20 ,0 ,17 ,0 ,10 ,1 ,0 ,5 ,0 ,0 ,0 ,0 ,0)
>
> Aphid.remain.fail<-c(17 ,13 ,1 ,1 ,18 ,20 ,20 ,20 ,20 ,20 ,20 ,19 ,19 ,19 ,17 ,19 ,19 ,19 ,20 ,20 ,12 ,4 ,18 ,20 ,18 ,19 ,19 ,17 ,20 ,20 ,10 ,9 ,5 ,2 ,19 ,20 ,6 ,10 ,16 ,19 ,17 ,8 ,18 ,16 ,20 ,0 ,3 ,1 ,9 ,2 ,15 ,20 ,20 ,17 ,20 ,20 ,20 ,17 ,9 ,20 ,1 ,7 ,7 ,10 ,11 ,15 ,1 ,19 ,18 ,20 ,8 ,18 ,20 ,19 ,13 ,5 ,1 ,14 ,0 ,1 ,3 ,19 ,20 ,20 ,19 ,18 ,11 ,2 ,9 ,0 ,19 ,18 ,20 ,20 ,19 ,19 ,20 ,19 ,19 ,16 ,20 ,20 ,20 ,16 ,19 ,20 ,20 ,18 ,19 ,14 ,20 ,20 ,20 ,19 ,20 ,20 ,19 ,19 ,20 ,20 ,18 ,19 ,
> 9 ,19 ,19 ,18 ,17 ,20 ,19 ,17 ,20 ,20 ,19 ,20 ,17 ,12 ,19 ,17 ,18 ,19 ,17 ,20 ,19 ,18 ,20 ,18 ,20 ,20 ,19 ,20 ,12 ,20 ,20 ,20 ,13 ,19 ,20 ,19 ,19 ,20 ,20 ,20 ,19 ,20 ,19 ,15 ,20 ,0 ,20 ,3 ,20 ,10 ,19 ,20 ,15 ,20 ,20 ,20 ,20 ,20)
>
> fffPlot<-c("D T1 1" ,"D T1 1" ,"D T1 1" ,"D T1 1" ,"D T1 5" ,"D T1 5" ,"D T1 5" ,"D T1 10" ,"D T1 10" ,"D T1 10" ,"D T1 15" ,"D T1 15" ,"D T1 15" ,"D T1 20" ,"D T1 20" ,"D T1 20" ,"D T2 1" ,"D T2 1" ,"D T2 1" ,"D T2 5" ,"D T2 5" ,"D T2 5" ,"D T2 10" ,"D T2 10" ,"D T2 10" ,"D T2 15" ,"D T2 15" ,"D T2 15" ,"D T2 20" ,"D T2 20" ,"D T2 20" ,"D T3 1" ,"D T3 1" ,"D T3 1" ,"D T3 5" ,"D T3 5" ,"D T3 5" ,"D T3 10" ,"D T3 10" ,"D T3 15" ,"D T3 15" ,"D T3 15" ,"D T3 20" ,"D T3 20" ,"D T3 20" ,
> "C T1 1" ,"C T1 1" ,"C T1 1" ,"C T1 5" ,"C T1 5" ,"C T1 5" ,"C T1 10" ,"C T1 10" ,"C T1 10" ,"C T1 15" ,"C T1 15" ,"C T1 15" ,"C T1 20" ,"C T1 20" ,"C T1 20" ,"C T2 1" ,"C T2 1" ,"C T2 1" ,"C T2 5" ,"C T2 5" ,"C T2 5" ,"C T2 10" ,"C T2 10" ,"C T2 10" ,"C T2 15" ,"C T2 15" ,"C T2 15" ,"C T2 20" ,"C T2 20" ,"C T2 20" ,"C T3 1" ,"C T3 1" ,"C T3 1" ,"C T3 5" ,"C T3 5" ,"C T3 5" ,"C T3 10" ,"C T3 10" ,"C T3 10" ,"C T3 15" ,"C T3 15" ,"C T3 15" ,"C T3 20" ,"C T3 20" ,"C T3 20" ,"B T1 1" ,
> "B T1 1" ,"B T1 1" ,"B T1 5" ,"B T1 5" ,"B T1 5" ,"B T1 10" ,"B T1 10" ,"B T1 10" ,"B T1 15" ,"B T1 15" ,"B T1 15" ,"B T1 20" ,"B T1 20" ,"B T1 20" ,"B T2 1" ,"B T2 1" ,"B T2 1" ,"B T2 5" ,"B T2 5" ,"B T2 5" ,"B T2 10" ,"B T2 10" ,"B T2 10" ,"B T2 15" ,"B T2 15" ,"B T2 15" ,"B T2 20" ,"B T2 20" ,"B T2 20" ,"B T3 1" ,"B T3 1" ,"B T3 1" ,"B T3 5" ,"B T3 5" ,"B T3 5" ,"B T3 10" ,"B T3 10" ,"B T3 10" ,"B T3 15" ,"B T3 15" ,"B T3 15" ,"B T3 20" ,"B T3 20" ,"B T3 20" ,"A T1 1" ,"A T1 1" ,"A T1 1" ,
> "A T1 5" ,"A T1 5" ,"A T1 5" ,"A T1 10" ,"A T1 10" ,"A T1 10" ,"A T1 15" ,"A T1 15" ,"A T1 15" ,"A T1 20" ,"A T1 20" ,"A T1 20" ,"A T2 1" ,"A T2 1" ,"A T2 1" ,"A T2 5" ,"A T2 5" ,"A T2 5" ,"A T2 10" ,"A T2 10" ,"A T2 10" ,"A T2 15" ,"A T2 15" ,"A T2 15" ,"A T2 20" ,"A T2 20" ,"A T2 20" ,"A T3 1" ,"A T3 1" ,"A T3 1" ,"A T3 5" ,"A T3 5" ,"A T3 5" ,"A T3 10" ,"A T3 10" ,"A T3 10" ,"A T3 15" ,"A T3 15" ,"A T3 15" ,"A T3 20" ,"A T3 20" ,"A T3 20")
>
> Bait.ID<-c("ID1" ,"ID2" ,"ID3" ,"ID3" ,"ID4" ,"ID5" ,"ID6" ,"ID7" ,"ID8" ,"ID9" ,"ID10" ,"ID11" ,"ID12" ,"ID13" ,"ID14" ,"ID15" ,"ID16" ,"ID17" ,"ID18" ,"ID19" ,"ID20" ,"ID21" ,"ID22" ,"ID23" ,"ID24" ,"ID25" ,"ID26" ,"ID27" ,"ID28" ,"ID29" ,"ID30" ,"ID31" ,"ID32" ,"ID33" ,"ID34" ,"ID35" ,"ID36" ,"ID37" ,"ID38" ,"ID39" ,"ID40" ,"ID41" ,"ID42" ,"ID43" ,"ID44" ,"ID45" ,"ID46" ,"ID47" ,"ID48" ,"ID49" ,"ID50" ,"ID51" ,"ID52" ,"ID53" ,"ID54" ,"ID55" ,"ID56" ,"ID57" ,"ID58" ,"ID59" ,"ID60" ,"ID61" ,"ID62" ,
> "ID63" ,"ID64" ,"ID65" ,"ID66" ,"ID67" ,"ID68" ,"ID69" ,"ID70" ,"ID71" ,"ID72" ,"ID73" ,"ID74" ,"ID75" ,"ID76" ,"ID77" ,"ID78" ,"ID79" ,"ID80" ,"ID81" ,"ID82" ,"ID83" ,"ID84" ,"ID85" ,"ID86" ,"ID87" ,"ID88" ,"ID89" ,"ID90" ,"ID91" ,"ID92" ,"ID93" ,"ID94" ,"ID95" ,"ID96" ,"ID97" ,"ID98" ,"ID99" ,"ID100" ,"ID101" ,"ID102" ,"ID103" ,"ID104" ,"ID105" ,"ID106" ,"ID107" ,"ID108" ,"ID109" ,"ID110" ,"ID111" ,"ID112" ,"ID113" ,"ID114" ,"ID115" ,"ID116" ,"ID117" ,"ID118" ,"ID119" ,"ID120" ,"ID121" ,"ID122" ,
> "ID123" ,"ID124" ,"ID125" ,"ID126" ,"ID127" ,"ID128" ,"ID129" ,"ID130" ,"ID131" ,"ID132" ,"ID133" ,"ID134" ,"ID135" ,"ID136" ,"ID137" ,"ID138" ,"ID139" ,"ID140" ,"ID141" ,"ID142" ,"ID143" ,"ID144" ,"ID145" ,"ID146" ,"ID147" ,"ID148" ,"ID149" ,"ID150" ,"ID151" ,"ID152" ,"ID153" ,"ID154" ,"ID155" ,"ID156" ,"ID157" ,"ID158" ,"ID159" ,"ID160" ,"ID161" ,"ID162" ,"ID163" ,"ID164" ,"ID165" ,"ID166" ,"ID167" ,"ID168" ,"ID169" ,"ID170" ,"ID171" ,"ID172" ,"ID173" ,"ID174" ,"ID175" ,"ID176" ,"ID177" ,"ID178" ,
> "ID179")
>
>
> dframe1<-data.frame(Field,Treatment,Field.section,Transect,Distance.m,Aphid.rm.success,Aphid.remain.fail,fffPlot,Bait.ID)
>
> y<-cbind(Aphid.rm.success,Aphid.remain.fail)
>
> model3<-glmer(y~Treatment*Distance.m+(1|Field/Field.section/Transect/fffPlot/Bait.ID), # Bait.ID to remove overdispersion
> binomial(link=logit),data=dframe1) # note I kept fffPlot smallest scale in (better residuals)
> summary(model3)
>
> plot(model3) # issue? Or just messy ecological data?
>
> ______________
> Victoria Wickens
> PhD student
> Room GU08
> Centre for Agri-Environmental Research (CAER)
> School of Agriculture, Policy & Development
> University of Reading
> Reading RG6 7BE
> ____________
>
> Email: v.j.wickens at pgr.reading.ac.uk<mailto:v.j.wickens at pgr.reading.ac.uk>
> http://www.reading.ac.uk/caer/staff_students.html
> https://www.facebook.com/UniRdgAPD
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From rdiaz02 at gmail.com  Thu Mar 13 01:47:31 2014
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Thu, 13 Mar 2014 01:47:31 +0100
Subject: [R-sig-ME] Modeling precision and recall with GLMMs
In-Reply-To: <BAY172-W20D42F1BD2F1B154BC35D5CB760@phx.gbl>
References: <87ob1d57ty.fsf@gmail.com>
	<3a6ddeda8b7442308e2a4062f4d57d80@BLUPR04MB610.namprd04.prod.outlook.com>
	<BAY172-W20D42F1BD2F1B154BC35D5CB760@phx.gbl>
Message-ID: <871ty653h8.fsf@gmail.com>


Hi Jake and Daniel,

I was extremely obtuse in my answer to your reply: what you and Daniel
suggest certainly makes a lot of sense.


On Thu, 01-01-1970, at 01:00, Jake Westfall <jake987722 at hotmail.com> wrote:
> Hi Ramon,
>
>> I am not sure that would work. For each data set, each method returns a
>> bunch of "P"s and "N"s. But what I want to do is model not the
>> relationship between truth and prediction, but rather how good or bad
>> each method is (at trying to reconstruct the truth).
>
> I'm not sure I see the problem. Surely the question of "how good or bad"
> each method is is answered by examining which method leads to the
> strongest correspondence between truth and prediction. That is the idea
> behind what I suggest.
>

Yes, I see it now.

> As for the fact that each method returns many data points, again I do not
> see the problem. You are using a multilevel model after all, right? So it
> seems to me that within that framework, you have classification decisions
> (the data) nested in algorithms, which are crossed with datasets. You
> could in principle use a crossed random effects model, but I think it
> would make more sense to treat algorithms as fixed.
>

I agree.

> Here's an example of what this might look like. The outcome variable is
> "decision" (numeric: 0 or 1), the predictors are "truth" (numeric: -1 or
> 1) and "algorithm" (factor denoting the algorithm). The model could look
> like: glmer(decision ~ 0 + algorithm/truth + (1|dataset))

> This syntax in the fixed effects estimates separate intercepts and slopes
> for each algorithm. The intercepts get at response bias while the slopes
> get at accuracy. As noted previously, these two estimates can be
> transformed to precision and recall.
>
> You could also reverse decision and truth so that we have:
> glmer(truth ~ 0 + algorithm/decision + (1|dataset))

> This might make more sense given the random effects for datasets, which
> in this second case allow for different datasets having different base
> rates of the two classes.

This seems reasonable, in particular since the base rates of the two
classes can be very different between datasets.  But datasets affect also
the "quality" of the signal, the d' in SDT.


> In the former case the random intercepts allowed for different datasets
> to lead to different rates of response bias, which is not crazy but isn't
> as intuitive to me as the second interpretation.

I also do not find having different response biases by datasets intuitive.

However, placing truth as the dependent does not seem intuitive to me. In
the first model we have P(1|Signal) or P(1|Noise) but reverting that is
awkward to me; then, I am not sure what a "residual" would mean, and I am
not sure if the coefficients retain the same meaning (intercept capturing
response bias by algorithm, or easy mapping to recall and precision, or
other features explained in, say, "Signal detection theory and
generalized linear models", by DeCarlo ---I've googled a little bit since
Daniel's and your last email :-).


Regardless, this is certainly a really nice way to approach the problem I
originally posted. Moreover, I could easily add edge-specific covariates
that could be related to how hard it is to correctly inferring those (i.e.,
for how small d' is); this would be really neat.


I am intrigued because the literature I am familiar with that compares the
performance of these types of algorithms (or classification algorithms in
general) often uses ranking based on metrics such as recall, precision,
area under the ROC curve, etc, without directly attempting to model the
original responses. So I am not sure if I am not missing something
obvious. 


A more general concern I have (which might explain the previous paragraph)
is that I am not sure if SDT (or what I've been able to speed read about
SDT in the last couple of hours :-) is a good model for the problem. In
particular, even if for each data set and algorithm we have hits, misses,
false alarms, etc, the yes or no decisions are not individual
decisions on each single edge of the network, but rather based on, e.g.,
minimizing some error function over all edges for a given dataset.


Thanks again for your detailed explanation.


Best,


R.



> Let me know if this makes some sense.
>
> Jake
>
>
>> From: Daniel.Wright at act.org
>> To: rdiaz02 at gmail.com; jake987722 at hotmail.com
>> CC: r-sig-mixed-models at r-project.org
>> Subject: RE: [R-sig-ME] Modeling precision and recall with GLMMs
>> Date: Wed, 12 Mar 2014 14:26:28 +0000
>> 
>> The "how good or bad" each method is, is what will come out of the method Jake is suggesting.
>> 
>> Using multilevel models for these is common in the memory recognition literature in psychology for the last decade or so, but is also relevant in lots of other areas like medical diagnostics. If the variable IS_ij is whether person i saw stimulus j (0 not seen, 1 seen), and SAY_ij is whether the person says she saw the stimuli, then a multilevel probit or logit regression, with careful coding of the variables, can mimic the standard SDT models. The critical variable for saying if people are accurate is the coefficient in front of SAY. If you have different conditions, COND_j, then interactions between COND_j (or COND_ij if varied within subject) and SAY_ij examine if accuracy varies among these. An important plus of the multilevel models is the coefficients can vary by person and/or stimuli. 
>> 
>> 
>> > Hi Ramon,
>> 
>> > I'm not sure that I fully understand the details of what you want to 
>> > accomplish. But I do want to ask: you jump right into your email 
>> > assuming that of course you want to model precision and recall, but 
>> > what about modelling the data directly (i.e., individual 
>> > classification
>> > decisions) rather than summaries of the data? Then you could work 
>> > backward (forward?) from the model results to compute what the implied 
>> > precision and recall would be
>> 
>> Sorry I did not provide enough details. I am comparing some methods for reconstructing networks, and the True positives and False positives, for instance, refer to the number of correctly inferred edges and to the number of edges that a procedure recovers that are not in the original network, respectively.
>> 
>> So the network reconstruction methods model the data directly, and what I want to model is how good or bad are what they return as a function of several other variables (related to several dimensions of the toughness of the problem, etc)
>> 
>> 
>> > If you decided that modelling the data directly would work for your 
>> > purposes, then one way of doing this would be to regress 
>> > classification decisions ("P" or "N") on actual classifications ("P" or "N").
>> 
>> I am not sure that would work. For each data set, each method returns a bunch of "P"s and "N"s. But what I want to do is model not the relationship between truth and prediction, but rather how good or bad each method is (at trying to reconstruct the truth).
>> 
>> > If this is done in a probit model, it is equivalent to the 
>> > equal-variance signal detection model studied at length in psychology, 
>> > with the intercept being the "criterion" in signal detection language 
>> > (denoted c), and the slope being "sensitivity" (denoted d' or 
>> > d-prime). It should definitely be possible to compute precision and 
>> > recall from c and d'.
>> 
>> I am not familiar with this approach in psychology. As I say above, I am not sure this addresses the problem I want to address but do you have some pointer to the literature where I can read more about the approach?
>> 
>> 
>> Best,
>> 
>> 
>> R.
>> 
>> > This might be simpler with a logit rather than probit link function.
>> >
>> > Let me know if I have misunderstood what you are trying to accomplish
>> 
>> > Jake
>> 
>> >> From: rdiaz02 at gmail.com.> To:
>> >> r-sig-mixed-models at r-project.org.> Date: Tue, 11 Mar 2014 11:48:57
>> >> +0100.> CC: ramon.diaz at iib.uam.es.> Subject: [R-sig-ME] Modeling
>> >> precision and recall with GLMMs.> .
>> 
>> >>  Dear All,. .
>> 
>> >>  I am examining the performance of a couple of classification-like  
>> >> methods. under different scenarios. Two of the metrics I am using are  
>> >> precision and. recall (TP/(TP + FP) and TP/(TP + FN), where TP, FP, 
>> >> and  FN are "true. positives", "false positives", and "false 
>> >> negatives" in a  simple two-way. confusion matrix). Some of the 
>> >> combinations of methods  have been used on. exactly the same data 
>> >> sets. So it is easy to set up a  binomial model (or. multinomial2 if using MCMCglmm) such as.
>> 
>> >> cbind(TP, FP) ~ fixed effects + (1|dataset)
>> 
>> >> However, the left hand side sounds questionable, specially with 
>> >> precision:. the expression TP/(TP + FP) has, in the denominator, a 
>> >> (TP +
>> >> FP) [the. number of results returned, or retrieved instances, etc] 
>> >> that, itself, can. be highly method-dependent (i.e., affected by the 
>> >> fixed effects). So rather. than a true proportion, this seems more 
>> >> like a ratio, where each of TP and. FP have their own variance, a 
>> >> covariance, etc, and thus the error. distribution is a mess (not the 
>> >> tidy thing of a binomial).
>> 
>> 
>> >> I've looked around in the literature and have not found much (maybe 
>> >> the. problem are my searching skills :-). Most people use rankings of 
>> >> methods,. not directly modeling precision or recall in the left-hand 
>> >> side of a. (generalized) linear model. A couple of papers use a 
>> >> linear model on the. log-transformed response (which I think is even 
>> >> worse than the above. binomial model, specially with lots of 0s or 
>> >> 1s). Some other people use a. single measure, such as the F-measure 
>> >> or Matthews correlation coefficient,. and I am using something 
>> >> similar too, but I specifically wanted to also. model precision and recall.. . .
>> 
>> >> An option would be a multi-response model with MCMCglmm, but I am not 
>> >> sure if this is appropriate either (dependence of the sum of FP and 
>> >> TP on the. fixed effects).. . .
>> 
>> 
>> >> Best,
>> 
>>  
>> > R-sig-mixed-models at r-project.org mailing list 
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> 
>> --
>> Ramon Diaz-Uriarte
>> Department of Biochemistry, Lab B-25
>> Facultad de Medicina
>> Universidad Aut.noma de Madrid
>> Arzobispo Morcillo, 4
>> 28029 Madrid
>> Spain
>> 
>> Phone: +34-91-497-2412
>> 
>> Email: rdiaz02 at gmail.com
>>        ramon.diaz at iib.uam.es
>> 
>> http://ligarto.org/rdiaz
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>  		 	   		  
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Ramon Diaz-Uriarte
Department of Biochemistry, Lab B-25
Facultad de Medicina
Universidad Aut?noma de Madrid 
Arzobispo Morcillo, 4
28029 Madrid
Spain

Phone: +34-91-497-2412

Email: rdiaz02 at gmail.com
       ramon.diaz at iib.uam.es

http://ligarto.org/rdiaz


From r.turner at auckland.ac.nz  Thu Mar 13 02:55:38 2014
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Thu, 13 Mar 2014 14:55:38 +1300
Subject: [R-sig-ME] Bernoulli glmm question.
Message-ID: <5321101A.7040108@auckland.ac.nz>


I am trying to help a graduate student in linguistics analyse her data. 
  Very much a case of the blind leading the blind, but I gotta try!

Summary of the structure of the data:

A number of (Mandarin speaking) students are assessed on their 
pronunciation of a suite of "test items" --- English language words.
(E.g. umbrella, helicopter, knife.)  They are assessed phoneme by 
phoneme in each word.  The response, at least in the context of my 
question, is whether they got the pronunciation right (y = 1) or wrong 
(y = 0).

The phonemes are classified into 7 types:

* Initial consonant
* Medial consonant
* Final consonant
* Initial consonant cluster
* Medial consonant cluster
* Final consonant cluster
* vowel

The students are classified by sex ("gender" to those wimps who are too 
embarrassed to say the word "sex").

I thought to fit a Bernoulli model with "type" (of phoneme) and sex (of 
the student) as predictors, with "student" being a random effect.

The syntax that I tried was:

fit <- lmer(y ~ sex + type + (1 | student), family = binomial, data = X)

where "X" is a data frame containing the relevant variables.

Main effects only, no interactions, so as to keep things simple --- at 
least initially.

First impressions from the fit:  Girls do significantly better than 
boys, and vowels are significantly easier than final consonant clusters 
(which form the baseline) and initial and medial clusters are 
significantly harder for the kids to pronounce than are the final 
clusters.  Single consonants (initial, medial, and final) do not differ 
significantly from the baseline in their difficulty level.

The bit about vowels being easier conforms to the graduate student's 
expectations and is kind of obvious from a rough inspection of the data.

There are 50 "test items" (words).  In the data set that I am initially 
looking at there are 54 students.  There are a total of 10314 observations.

(I am just looking at the oldest group of students to start with.  There 
are 6 other groups and eventually I will put all 7 groups together and 
investigate an age (or "level") effect as well.)

Would anyone be kind enough to comment on my efforts so far? Please try 
not to be too rude! :-)  Am I on the right track? Am I overlooking any 
glaring traps for young players? Have I got the syntax of my call to 
lmer() correct?

One thing that I am nervous about:

If I fit the "trivial model"

fit0 <- lmer(y ~ 1 + (1 | student), family = binomial, data = X)

the resulting coefficients are just the estimates (BLUPs?) of the 
"random intercepts, is it not so?  If I calculate the variance of these 
coefficients:

	var(coef(fit0)$student[,1])

I get 0.0226.  I thought that this value would be "pretty similar to" 
(though not exactly the same as) the estimated random effect variance. 
But the latter is 0.0502 --- which seems to me to be quite different.

A 95% confidence interval for sigma^2 on the basis of my "var(coef ...)" 
calculation, assuming that (n-1)*s^2/sigma^2 ~ chi-squared_{n-1},
is [0.0160, 0.0345] (to 4 decimal places) so the estimated random effect 
variance from fit0 is "significantly different" from my naive estimate.

My thinking must be out to lunch here.  Can someone put me back on the 
rails.  (My humblest apologies for the mixed metaphors. :-) )

Thanks for any words of wisdom.

cheers,

Rolf Turner


From marko.bachl at uni-hohenheim.de  Thu Mar 13 08:34:37 2014
From: marko.bachl at uni-hohenheim.de (Marko Bachl)
Date: Thu, 13 Mar 2014 08:34:37 +0100
Subject: [R-sig-ME] Bernoulli glmm question.
In-Reply-To: <5321101A.7040108@auckland.ac.nz>
References: <5321101A.7040108@auckland.ac.nz>
Message-ID: <CAE5vbrv3+zAznaxFB-Vka_W38-jBtJFieU56-zeA_AKwXvUSwg@mail.gmail.com>

Dear Rolf,
just a short technical reply to your question at the end - if a part
of your question is how to technically extract the random effect
variances from a lmer() fit.

> One thing that I am nervous about:
>
> If I fit the "trivial model"
>
> fit0 <- lmer(y ~ 1 + (1 | student), family = binomial, data = X)
>
> the resulting coefficients are just the estimates (BLUPs?) of the "random
> intercepts, is it not so?  If I calculate the variance of these
> coefficients:
>
>         var(coef(fit0)$student[,1])

The syntax to get the estimates of the random intercept variances from
a lmer() fit is

sapply(VarCorr(fit0), diag)

which should give you exactly the estimates from summary(fit0). There
has been some discussion on the list on why var(coef()) does not work,
but I cannot recall the answer or find the relevant thread.

Overall, your analytical approach seems to make sense to me - but I am
by no means an expert. I was wondering whether a second random effect
of "word" should (has to be?) included, if all students are tested
with the same words. Such a model would account for the assumption
that there are other characteristics of the words beyond the list of
your "phonemes" which have an effect on correct pronunciation. The
model would be

fit1  <- lmer(y ~ 1 + (1 | student) + (1 | word), family = binomial, data = X)

where "word" and "student" are crossed random effects, which would
perfectly balanced if every student was tested with every word. But
again: I am not exactly familiar with this kind of research, so I am
just guessing...

Best regards
Marko

-- 
www.komm.uni-hohenheim.de/bachl


From tibor at linguistics.rub.de  Thu Mar 13 08:40:07 2014
From: tibor at linguistics.rub.de (Tibor Kiss)
Date: Thu, 13 Mar 2014 08:40:07 +0100
Subject: [R-sig-ME] Bernoulli glmm question.
In-Reply-To: <5321101A.7040108@auckland.ac.nz>
References: <5321101A.7040108@auckland.ac.nz>
Message-ID: <EF7A2F64-E852-4C2A-A0E1-732713BD9B36@linguistics.rub.de>

Hi,

I cannot say much about your question concerning the variance, but I would probably include "word" as a random factor as well. It's not easy to understand from your email, but I assume that each word is 'cut' into phonemes, so that your 10314 observations are actually 10314 slices of the 50 words fed to the 54 students. So "y" will be 0 or 1 for a phoneme. It might be the case that the whole word influences the pronunciation, and the words have been chosen at random, I assume, so I would include them as a random factor. 

Also, I would use glmer directly, but that might be cosmetics.

With kind regards

Tibor

----------------------------------------------
Prof. Dr. Tibor Kiss
Sprachwissenschaftliches Institut
Ruhr-Universit?t Bochum
www.linguistics.rub.de/~kiss


Am 13.03.2014 um 02:55 schrieb Rolf Turner <r.turner at auckland.ac.nz>:

> 
> I am trying to help a graduate student in linguistics analyse her data.  Very much a case of the blind leading the blind, but I gotta try!
> 
> Summary of the structure of the data:
> 
> A number of (Mandarin speaking) students are assessed on their pronunciation of a suite of "test items" --- English language words.
> (E.g. umbrella, helicopter, knife.)  They are assessed phoneme by phoneme in each word.  The response, at least in the context of my question, is whether they got the pronunciation right (y = 1) or wrong (y = 0).
> 
> The phonemes are classified into 7 types:
> 
> * Initial consonant
> * Medial consonant
> * Final consonant
> * Initial consonant cluster
> * Medial consonant cluster
> * Final consonant cluster
> * vowel
> 
> The students are classified by sex ("gender" to those wimps who are too embarrassed to say the word "sex").
> 
> I thought to fit a Bernoulli model with "type" (of phoneme) and sex (of the student) as predictors, with "student" being a random effect.
> 
> The syntax that I tried was:
> 
> fit <- lmer(y ~ sex + type + (1 | student), family = binomial, data = X)
> 
> where "X" is a data frame containing the relevant variables.
> 
> Main effects only, no interactions, so as to keep things simple --- at least initially.
> 
> First impressions from the fit:  Girls do significantly better than boys, and vowels are significantly easier than final consonant clusters (which form the baseline) and initial and medial clusters are significantly harder for the kids to pronounce than are the final clusters.  Single consonants (initial, medial, and final) do not differ significantly from the baseline in their difficulty level.
> 
> The bit about vowels being easier conforms to the graduate student's expectations and is kind of obvious from a rough inspection of the data.
> 
> There are 50 "test items" (words).  In the data set that I am initially looking at there are 54 students.  There are a total of 10314 observations.
> 
> (I am just looking at the oldest group of students to start with.  There are 6 other groups and eventually I will put all 7 groups together and investigate an age (or "level") effect as well.)
> 
> Would anyone be kind enough to comment on my efforts so far? Please try not to be too rude! :-)  Am I on the right track? Am I overlooking any glaring traps for young players? Have I got the syntax of my call to lmer() correct?
> 
> One thing that I am nervous about:
> 
> If I fit the "trivial model"
> 
> fit0 <- lmer(y ~ 1 + (1 | student), family = binomial, data = X)
> 
> the resulting coefficients are just the estimates (BLUPs?) of the "random intercepts, is it not so?  If I calculate the variance of these coefficients:
> 
> 	var(coef(fit0)$student[,1])
> 
> I get 0.0226.  I thought that this value would be "pretty similar to" (though not exactly the same as) the estimated random effect variance. But the latter is 0.0502 --- which seems to me to be quite different.
> 
> A 95% confidence interval for sigma^2 on the basis of my "var(coef ...)" calculation, assuming that (n-1)*s^2/sigma^2 ~ chi-squared_{n-1},
> is [0.0160, 0.0345] (to 4 decimal places) so the estimated random effect variance from fit0 is "significantly different" from my naive estimate.
> 
> My thinking must be out to lunch here.  Can someone put me back on the rails.  (My humblest apologies for the mixed metaphors. :-) )
> 
> Thanks for any words of wisdom.
> 
> cheers,
> 
> Rolf Turner
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From robert.espesser at lpl-aix.fr  Thu Mar 13 14:16:36 2014
From: robert.espesser at lpl-aix.fr (espesser)
Date: Thu, 13 Mar 2014 14:16:36 +0100
Subject: [R-sig-ME] Bernoulli glmm question.
In-Reply-To: <EF7A2F64-E852-4C2A-A0E1-732713BD9B36@linguistics.rub.de>
References: <5321101A.7040108@auckland.ac.nz>
	<EF7A2F64-E852-4C2A-A0E1-732713BD9B36@linguistics.rub.de>
Message-ID: <5321AFB4.3010104@lpl-aix.fr>

  Hi,

for the case given by Rolf Turner, I would add word as a random factor. 
"word" appears as a grouping factor for the observation  "type"

  I am " more nervous ( :)  )" about  the relevance of  more complex 
random effect designs,
accounting for correlations between type and student , and/or  type and 
(word and student)
(To tell the truth, I have similar  questions about  a model I'm 
currently  working on),  as:

fit2<- glmer(y ~ sex + type + (type | student) +(1|word), family = binomial, data = X)
where there are 7  (possibly correlated), random effects for each student

or
fit3<- glmer(y ~ sex + type + (type | student) +(type|word), family = binomial, data = X)
where there are 7  (possibly correlated), random effects for each student,
and 7  (possibly correlated), random effects for for each word

D. Bates suggested , at least as a starting point:

fit3a<- glmer(y ~ sex + type + (1|student ) +(1+type:student) +(1|word)+ (1+ type:word), family = binomial, data = X)

which is a simpler model to estimate (without any correlations), still accounting for the interactions between type and (student, word)


Which would be the "right " model ?
Is the data-driven (forward) selection of random terms still accepted  (or not, according to D. Barr ) ?
Recent discussions on this list showed at least some controverse about it.

I would be glad to be "corrected" , enlighted or advised on these points.
Thanks, in advanced, for your attention

Best regards

Robert Espesser
CNRS UMR  7309 - Universit? Aix-Marseille
5 Avenue Pasteur
13100 AIX-EN-PROVENCE

Tel: +33 (0)413 55 36 26




Le 13/03/2014 08:40, Tibor Kiss a ?crit :
> Hi,
>
> I cannot say much about your question concerning the variance, but I would probably include "word" as a random factor as well. It's not easy to understand from your email, but I assume that each word is 'cut' into phonemes, so that your 10314 observations are actually 10314 slices of the 50 words fed to the 54 students. So "y" will be 0 or 1 for a phoneme. It might be the case that the whole word influences the pronunciation, and the words have been chosen at random, I assume, so I would include them as a random factor.
>
> Also, I would use glmer directly, but that might be cosmetics.
>
> With kind regards
>
> Tibor
>
> ----------------------------------------------
> Prof. Dr. Tibor Kiss
> Sprachwissenschaftliches Institut
> Ruhr-Universit?t Bochum
> www.linguistics.rub.de/~kiss
>
>
> Am 13.03.2014 um 02:55 schrieb Rolf Turner<r.turner at auckland.ac.nz>:
>
>> I am trying to help a graduate student in linguistics analyse her data.  Very much a case of the blind leading the blind, but I gotta try!
>>
>> Summary of the structure of the data:
>>
>> A number of (Mandarin speaking) students are assessed on their pronunciation of a suite of "test items" --- English language words.
>> (E.g. umbrella, helicopter, knife.)  They are assessed phoneme by phoneme in each word.  The response, at least in the context of my question, is whether they got the pronunciation right (y = 1) or wrong (y = 0).
>>
>> The phonemes are classified into 7 types:
>>
>> * Initial consonant
>> * Medial consonant
>> * Final consonant
>> * Initial consonant cluster
>> * Medial consonant cluster
>> * Final consonant cluster
>> * vowel
>>
>> The students are classified by sex ("gender" to those wimps who are too embarrassed to say the word "sex").
>>
>> I thought to fit a Bernoulli model with "type" (of phoneme) and sex (of the student) as predictors, with "student" being a random effect.
>>
>> The syntax that I tried was:
>>
>> fit<- lmer(y ~ sex + type + (1 | student), family = binomial, data = X)
>>
>> where "X" is a data frame containing the relevant variables.
>>
>> Main effects only, no interactions, so as to keep things simple --- at least initially.
>>
>> First impressions from the fit:  Girls do significantly better than boys, and vowels are significantly easier than final consonant clusters (which form the baseline) and initial and medial clusters are significantly harder for the kids to pronounce than are the final clusters.  Single consonants (initial, medial, and final) do not differ significantly from the baseline in their difficulty level.
>>
>> The bit about vowels being easier conforms to the graduate student's expectations and is kind of obvious from a rough inspection of the data.
>>
>> There are 50 "test items" (words).  In the data set that I am initially looking at there are 54 students.  There are a total of 10314 observations.
>>
>> (I am just looking at the oldest group of students to start with.  There are 6 other groups and eventually I will put all 7 groups together and investigate an age (or "level") effect as well.)
>>
>> Would anyone be kind enough to comment on my efforts so far? Please try not to be too rude! :-)  Am I on the right track? Am I overlooking any glaring traps for young players? Have I got the syntax of my call to lmer() correct?
>>
>> One thing that I am nervous about:
>>
>> If I fit the "trivial model"
>>
>> fit0<- lmer(y ~ 1 + (1 | student), family = binomial, data = X)
>>
>> the resulting coefficients are just the estimates (BLUPs?) of the "random intercepts, is it not so?  If I calculate the variance of these coefficients:
>>
>> 	var(coef(fit0)$student[,1])
>>
>> I get 0.0226.  I thought that this value would be "pretty similar to" (though not exactly the same as) the estimated random effect variance. But the latter is 0.0502 --- which seems to me to be quite different.
>>
>> A 95% confidence interval for sigma^2 on the basis of my "var(coef ...)" calculation, assuming that (n-1)*s^2/sigma^2 ~ chi-squared_{n-1},
>> is [0.0160, 0.0345] (to 4 decimal places) so the estimated random effect variance from fit0 is "significantly different" from my naive estimate.
>>
>> My thinking must be out to lunch here.  Can someone put me back on the rails.  (My humblest apologies for the mixed metaphors. :-) )
>>
>> Thanks for any words of wisdom.
>>
>> cheers,
>>
>> Rolf Turner
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From elgringacho at gmail.com  Thu Mar 13 16:56:38 2014
From: elgringacho at gmail.com (Dylan Craven)
Date: Thu, 13 Mar 2014 16:56:38 +0100
Subject: [R-sig-ME] meta-regression using nlme with study-specific
	weights
Message-ID: <CAJiJPrN0YQvFa-LUcKV70_1Q2MBTE4KHHfHcHx2dLGbYFcmF5Q@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140313/0007578f/attachment.pl>

From qyang at bu.edu  Thu Mar 13 19:01:02 2014
From: qyang at bu.edu (Yang, Qiong)
Date: Thu, 13 Mar 2014 18:01:02 +0000
Subject: [R-sig-ME] gee, geese and glmer
In-Reply-To: <531F0F03.5090907@gmail.com>
References: <007801cf3a86$6d27b060$47771120$@bu.edu>
	<531B98B4.4050403@gmail.com>
	<6984CA5898097D459F8EC753F025B0D839696447@IST-EX10MBX-3.ad.bu.edu>
	<21278.56111.63015.103003@stat.math.ethz.ch>
	<531F0F03.5090907@gmail.com>
Message-ID: <6984CA5898097D459F8EC753F025B0D8396982CD@IST-EX10MBX-3.ad.bu.edu>

Hi Ben and Martin,

Thanks you for your reply and taking time to explain the situation.
Maybe I wasn't clear in my previous message or don't have a deeper understanding of the CRAN policy: Instead of separate packages for "old-CRAN" lme4 (lme4.0), and the current lme4, which is not allowed by CRAN maintainer, is it possible to have an option in current lme4 to call the algorithm in old lme4?
I thought the new lme4 implements a different algorithm and why not make the old algorithm available in current lme4 lmer() as an option. It is like we can request pearson or spearman correlations in cor(). Sorry if I simplified the problem too much. 

To Ben, Sorry that we cannot share data because of confidentiality policy. But you are able to debug our problem through PC screen sharing where you can gain control and run programs on our Linux 
session without possessing the data. If that sounds like a good idea, let us know when you might be available to do so.

Thanks,
Qiong
-----Original Message-----
From: Ben Bolker [mailto:bbolker at gmail.com] 
Sent: Tuesday, March 11, 2014 9:26 AM
To: Martin Maechler; Yang, Qiong
Cc: r-sig-mixed-models at r-project.org; Chen, Ming-Huei
Subject: Re: [R-sig-ME] gee, geese and glmer

   I would also point out that we are indeed very interested, in the medium term (the short term is very very busy!), in making sure that your issues are resolved.  We would like lme4 to dominate lme4.0 (i.e., to work better in all circumstances). So far it's been a bit difficult since we have been debugging remotely -- short of the suggestions I gave below, and without access to a reproducible example, it's very hard indeed for me to say much more.

  sincerely
    Ben Bolker

On 14-03-11 05:45 AM, Martin Maechler wrote:
>>>>>> Yang, Qiong <qyang at bu.edu>
>>>>>>     on Mon, 10 Mar 2014 18:19:56 +0000 writes:
> 
>     > Hi Ben, We wonder if you can add an option in lmer() of
>     > current lme4 version to call the algorithm used in
>     > lme4_0.999999-2?
> 
> unfortunately not.
> 
> For this reason, we had planned for many months, starting in August 
> 2012 and announced on this mailing list at least a year ago that we 
> would provide the 'lme4.0' package  (back-compatible as well as 
> possible in light of computer OS updates incl system libraries, and R 
> updates, ...) in order to provide useRs the possibility of 
> reproducible research and data analysis for their analyses done with 
> "old-CRAN" lme4 versions.
> 
> Unfortunately (for us), the CRAN maintainers decided that providing 
> lme4.0 in addition to lme4 was a bad idea, and explicit forbid such 
> actions in the (then new) CRAN policy document.  I'm still not at all 
> happy with that decision.
> 
>     > For our package (analyze rare genetic variant) to be put
>     > on CRAN, we need to use current version of lme4. However,
>     > at this point, there are still issues that cannot be
>     > resolved with newer versions of lme4. It is very difficult
>     > resolved with newer versions of lme4. It is very difficult
>     > for us to keep waiting and testing the new release, and
>     > hope all the issues resolved and no new issues coming
>     > up. lme4_0.999999-2 has been used by us for a long time
>     > with little problem. 
> 
> Good to hear.  Such cases were exactly the reason why we (lme4
> authors) made such considerable effort to provide  lme4.0 for 
> reproducible research and data analysis.
> 
> at the bottom of
>   https://github.com/lme4/lme4/blob/master/README.md
> we mention the state and give installation instruction of lme4.0, but 
> as you say, this does not solve the problem for other package 
> maintainers: If they want a CRAN package, they (currently? I'm 
> optimistic beyond reason :-) cannot have a 'Depends: lme4.0'  (or 
> "Imports:..." or similar).
> 
>     > Your help on this is highly
>     > appreciated.  Best, Qiong
> 
> You're welcome; currentl there's not more we can do.
> Martin
> 
> --
> Martin Maechler,
> ETH Zurich, Switzerland
> 
> 
> 
>     > -----Original Message----- From: Ben Bolker
>     > [mailto:bbolker at gmail.com] Sent: Saturday, March 08, 2014
>     > 5:25 PM To: r-sig-mixed-models at r-project.org Cc: Chen,
>     > Ming-Huei; Yang, Qiong Subject: Re: gee, geese and glmer
> 
>     > On 14-03-07 11:25 PM, Ming-Huei Chen wrote:
>     >> Hi Ben,
>     >> 
>     >> 
>     >> 
>     >> In an analysis we found that glmer in new lme4 gave
>     >> result different from old lme4, gee and geese, where old
>     >> lme4 seems to be closer to gee and geese.. Please see
>     >> highlighted sex effect below. Case by sex (2x2) table is
>     >> also given. Can you please let us know how would you look
>     >> into the results? Thanks!
>     >>
> 
>     >    [cc'ing to r-sig-mixed-models: **please** try
>     > r-sig-mixed-models first, not personal e-mail to me ...]
> 
>     >   I can't say exactly what's going here; without having a
>     > reproducible example <http://tinyurl.com/reproducible-000>
>     > it's hard to say precisely.  Thoughts:
> 
>     >  * gee and geese are giving _exactly_ the same parameter
>     > estimates, to 8 significant digits, so I would guess they
>     > are wrapping identical underlying methods.
> 
>     >  * As far as diagnosing the issue with lme4 1.0-6: * does
>     > changing the optimization method, i.e.
>     > glmerControl(optimizer="optimx",optCtrl=list(method="nlminb"))
>     > [must do library("optimx") first] or
>     > glmerControl(optimizer="bobyqa")
> 
>     >   change the result?
> 
>     >  * I would be curious whether the soon-to-be-released
>     > version 1.1-4 (which can be installed from github or
>     > lme4.r-forge.r-project.org/repos) gives either (1)
>     > convergence warnings or (2) different/better answers
> 
>     >  * You can try specifying the starting values for lme4 to
>     > diagnose misconvergence; for example, start lme4 from the
>     > estimates given by old lme4/lme4.0 and see if it gives a
>     > similar answer.
> 
>     >  * You can use the 'slice' and 'splom.slice' functions
>     > from bbmle to visualize the likelihood surfaces
> 
>     >   good luck, Ben Bolker
> 
>     >> Ming-Huei
>     >>
> 
>     >> ###GEE
>     >> 
>     >>> summary(gee(case~sex+PC1+PC2+PC3+PC4,id=famid,family=binomial,data=da
>     >>> ta))$coef
>     >> Estimate Naive S.E.  Naive z Robust S.E.  Robust z
>     >> (Intercept) -1.88047373 0.13532162 -13.8963286 0.15960440
>     >> -11.782092 sex -0.23436854 0.08611269 -2.7216494
>     >> 0.09050577 -2.589543 PC1 -0.05478639 0.06195318
>     >> -0.8843192 0.06822178 -0.803063 PC2 -0.09934572
>     >> 0.06494563 -1.5296753 0.06520811 -1.523518 PC3
>     >> -0.07020391 0.06626875 -1.0593818 0.06962147 -1.008366
>     >> PC4 -0.13413097 0.06746716 -1.9880927 0.06979901
>     >> -1.921674
>     >>
> 
>     >> ###GEESE
>     >> 
>     >>> summary(geese(case~sex+PC1+PC2+PC3+PC4,id=famid,family=binomial,data=
>     >>> data))$mean
>     >> 
>     >> estimate san.se wald p
>     >> 
>     >> (Intercept) -1.88047373 0.15960440 138.8176912
>     >> 0.000000000 sex -0.23436854 0.09050577 6.7057312
>     >> 0.009610351 PC1 -0.05478639 0.06822178 0.6449102
>     >> 0.421938319 PC2 -0.09934572 0.06520811 2.3211071
>     >> 0.127629159 PC3 -0.07020391 0.06962147 1.0168016
>     >> 0.313278888 PC4 -0.13413097 0.06979901 3.6928324
>     >> 0.054646745
>     >> 
>     >> ### lme4_0.999999-2
>     >> 
>     >>> summary(glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data
>     >>> =data))
>     >> Estimate Std. Error z value Pr(>|z|) (Intercept) -3.01599
>     >> 0.28305 -10.655 <2e-16 *** sex -0.41056 0.16285 -2.521
>     >> 0.0117 * PC1 -0.17116 0.12903 -1.326 0.1847 PC2 -0.15510
>     >> 0.13382 -1.159 0.2465 PC3 -0.19044 0.13580 -1.402 0.1608
>     >> PC4 0.02532 0.13732 0.184 0.8537
>     >> 
>     >> ###lme4_1.0-6
>     >> 
>     >>> summary(glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data
>     >>> =data))
>     >> 
>     >> Estimate Std. Error z value Pr(>|z|)
>     >> 
>     >> (Intercept) -10.2784 0.8631 -11.909 <2e-16 *** sex 0.3497
>     >> 0.1975 1.770 0.0767 .  PC1 -0.3555 0.1623 -2.190 0.0285 *
>     >> PC2 -0.1087 0.1653 -0.657 0.5109 PC3 -0.2242 0.1652
>     >> -1.357 0.1748 PC4 0.1103 0.1671 0.660 0.5091
>     >> 
>     >> Case by sex
>     >> 
>     >> 1 2 0 2554 3021 1 310 290
>     >>
> 
>     > _______________________________________________
>     > R-sig-mixed-models at r-project.org mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 


From bbolker at gmail.com  Thu Mar 13 21:09:22 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 13 Mar 2014 16:09:22 -0400
Subject: [R-sig-ME] gee, geese and glmer
In-Reply-To: <6984CA5898097D459F8EC753F025B0D8396982CD@IST-EX10MBX-3.ad.bu.edu>
References: <007801cf3a86$6d27b060$47771120$@bu.edu>	<531B98B4.4050403@gmail.com>	<6984CA5898097D459F8EC753F025B0D839696447@IST-EX10MBX-3.ad.bu.edu>
	<21278.56111.63015.103003@stat.math.ethz.ch>
	<531F0F03.5090907@gmail.com>
	<6984CA5898097D459F8EC753F025B0D8396982CD@IST-EX10MBX-3.ad.bu.edu>
Message-ID: <53221072.9080203@gmail.com>

On 14-03-13 02:01 PM, Yang, Qiong wrote:
> Hi Ben and Martin,
> 
> Thanks you for your reply and taking time to explain the situation. 
> Maybe I wasn't clear in my previous message or don't have a deeper
> understanding of the CRAN policy: Instead of separate packages for
> "old-CRAN" lme4 (lme4.0), and the current lme4, which is not allowed
> by CRAN maintainer, is it possible to have an option in current lme4
> to call the algorithm in old lme4? I thought the new lme4 implements
> a different algorithm and why not make the old algorithm available in
> current lme4 lmer() as an option. It is like we can request pearson
> or spearman correlations in cor(). Sorry if I simplified the problem
> too much.

   It's not so much CRAN policy as the architecture of the software.  A
great deal of the internal structure has changed between versions <1.0
and > 1.0, so putting both versions into one package would basically
mean having copies of all of the old and all of the new code -- and
keeping it all safely distinguished and separate would probably be
substantially more work (we have about as much as we can to do keep up
with a single package ...)  As we said before, we were surprised by the
rejection of lme4.0 by CRAN -- our previous planning had assumed that we
would be able to put lme4.0 on CRAN.  Somewhat to our surprise, there
have been relatively few people contacting us with troubles similar to
yours -- for the most part it seems that is (thankfully) very rare that
lme4 gives significantly worse answers than lme4.0.

  So far it's not clear that lme4's answers are actually worse than
lme4.0's (I admit that all other things being equal, closer to GEE is
more likely to be correct -- on the other hand, we know that GEE gives
marginal estimates of fixed parameters while GLMM gives conditional
estimates, so we shouldn't expect them to be the same).  Can I confirm:

* you've tried this with the most recent development version of lme4
(1.1-4) and you do *not* get any convergence warnings?

* have you compared the deviances based on the old (lme4.0 / lme4 < 1.0)
and new packages?  Here is the outline of how you would do this:

library(lme4)   ## load new package
m1 <- glmer(...) ## lme4 fit
library(lme4.0)  ## load old package
m0 <- lme4.0::glmer(...) ## lme4.0 fit
## extract full parameter set for lme4.0 and lme4 fits
m0parms <- c(getME(m0,"theta"),fixef(m0))
m1parms <- c(lme4::getME(m1,"theta"),fixef(m1))
nparms <- length(m0parms)
ntheta <- length(getME(m0,"theta"))

par(las=1,bty="l")
plot(m0parms,m1parms,col=rep(2:1,c(ntheta,nparms-ntheta)))
abline(a=0,b=1)

## set up deviance function
dd <- lme4::glmer(...,devFunOnly=TRUE)
## compare
dd(m0parms)
dd(m1parms)

  If the new deviance is lower than the old deviance, that's a strong
indication that the new version is actually getting a _better_ fit than
the old one.

  Another diagnostic tool is:

library(bbmle)
ss1 <- slice2D(fun = dd, params = m1parms, verbose = FALSE)
splom(ss1)

  This will show you cross-sections of the deviance surface ...


> To Ben, Sorry that we cannot share data because of confidentiality
> policy. But you are able to debug our problem through PC screen
> sharing where you can gain control and run programs on our Linux 
> session without possessing the data. If that sounds like a good idea,
> let us know when you might be available to do so.

  Can you generate a simulated example that replicates your problem?
simulate() is a useful tool for generating data that are similar, but
not identical to, your original data (you can also anonymize factor
labels etc.).

  PC screen sharing seems difficult -- I have easy access to MacOS and
Linux systems, but not Windows.

> 
> Thanks, Qiong -----Original Message----- From: Ben Bolker
> [mailto:bbolker at gmail.com] Sent: Tuesday, March 11, 2014 9:26 AM To:
> Martin Maechler; Yang, Qiong Cc: r-sig-mixed-models at r-project.org;
> Chen, Ming-Huei Subject: Re: [R-sig-ME] gee, geese and glmer
> 
> I would also point out that we are indeed very interested, in the
> medium term (the short term is very very busy!), in making sure that
> your issues are resolved.  We would like lme4 to dominate lme4.0
> (i.e., to work better in all circumstances). So far it's been a bit
> difficult since we have been debugging remotely -- short of the
> suggestions I gave below, and without access to a reproducible
> example, it's very hard indeed for me to say much more.
> 
> sincerely Ben Bolker
> 
> On 14-03-11 05:45 AM, Martin Maechler wrote:
>>>>>>> Yang, Qiong <qyang at bu.edu> on Mon, 10 Mar 2014 18:19:56
>>>>>>> +0000 writes:
>> 
>>> Hi Ben, We wonder if you can add an option in lmer() of current
>>> lme4 version to call the algorithm used in lme4_0.999999-2?
>> 
>> unfortunately not.
>> 
>> For this reason, we had planned for many months, starting in August
>>  2012 and announced on this mailing list at least a year ago that
>> we would provide the 'lme4.0' package  (back-compatible as well as
>>  possible in light of computer OS updates incl system libraries,
>> and R updates, ...) in order to provide useRs the possibility of 
>> reproducible research and data analysis for their analyses done
>> with "old-CRAN" lme4 versions.
>> 
>> Unfortunately (for us), the CRAN maintainers decided that providing
>>  lme4.0 in addition to lme4 was a bad idea, and explicit forbid
>> such actions in the (then new) CRAN policy document.  I'm still not
>> at all happy with that decision.
>> 
>>> For our package (analyze rare genetic variant) to be put on CRAN,
>>> we need to use current version of lme4. However, at this point,
>>> there are still issues that cannot be resolved with newer
>>> versions of lme4. It is very difficult resolved with newer
>>> versions of lme4. It is very difficult for us to keep waiting and
>>> testing the new release, and hope all the issues resolved and no
>>> new issues coming up. lme4_0.999999-2 has been used by us for a
>>> long time with little problem.
>> 
>> Good to hear.  Such cases were exactly the reason why we (lme4 
>> authors) made such considerable effort to provide  lme4.0 for 
>> reproducible research and data analysis.
>> 
>> at the bottom of 
>> https://github.com/lme4/lme4/blob/master/README.md we mention the
>> state and give installation instruction of lme4.0, but as you say,
>> this does not solve the problem for other package maintainers: If
>> they want a CRAN package, they (currently? I'm optimistic beyond
>> reason :-) cannot have a 'Depends: lme4.0'  (or "Imports:..." or
>> similar).
>> 
>>> Your help on this is highly appreciated.  Best, Qiong
>> 
>> You're welcome; currentl there's not more we can do. Martin
>> 
>> -- Martin Maechler, ETH Zurich, Switzerland
>> 
>> 
>> 
>>> -----Original Message----- From: Ben Bolker 
>>> [mailto:bbolker at gmail.com] Sent: Saturday, March 08, 2014 5:25 PM
>>> To: r-sig-mixed-models at r-project.org Cc: Chen, Ming-Huei; Yang,
>>> Qiong Subject: Re: gee, geese and glmer
>> 
>>> On 14-03-07 11:25 PM, Ming-Huei Chen wrote:
>>>> Hi Ben,
>>>> 
>>>> 
>>>> 
>>>> In an analysis we found that glmer in new lme4 gave result
>>>> different from old lme4, gee and geese, where old lme4 seems to
>>>> be closer to gee and geese.. Please see highlighted sex effect
>>>> below. Case by sex (2x2) table is also given. Can you please
>>>> let us know how would you look into the results? Thanks!
>>>> 
>> 
>>> [cc'ing to r-sig-mixed-models: **please** try r-sig-mixed-models
>>> first, not personal e-mail to me ...]
>> 
>>> I can't say exactly what's going here; without having a 
>>> reproducible example <http://tinyurl.com/reproducible-000> it's
>>> hard to say precisely.  Thoughts:
>> 
>>> * gee and geese are giving _exactly_ the same parameter 
>>> estimates, to 8 significant digits, so I would guess they are
>>> wrapping identical underlying methods.
>> 
>>> * As far as diagnosing the issue with lme4 1.0-6: * does changing
>>> the optimization method, i.e. 
>>> glmerControl(optimizer="optimx",optCtrl=list(method="nlminb")) 
>>> [must do library("optimx") first] or 
>>> glmerControl(optimizer="bobyqa")
>> 
>>> change the result?
>> 
>>> * I would be curious whether the soon-to-be-released version
>>> 1.1-4 (which can be installed from github or 
>>> lme4.r-forge.r-project.org/repos) gives either (1) convergence
>>> warnings or (2) different/better answers
>> 
>>> * You can try specifying the starting values for lme4 to diagnose
>>> misconvergence; for example, start lme4 from the estimates given
>>> by old lme4/lme4.0 and see if it gives a similar answer.
>> 
>>> * You can use the 'slice' and 'splom.slice' functions from bbmle
>>> to visualize the likelihood surfaces
>> 
>>> good luck, Ben Bolker
>> 
>>>> Ming-Huei
>>>> 
>> 
>>>> ###GEE
>>>> 
>>>>> summary(gee(case~sex+PC1+PC2+PC3+PC4,id=famid,family=binomial,data=da
>>
>>>>> 
>>> ta))$coef
>>>> Estimate Naive S.E.  Naive z Robust S.E.  Robust z (Intercept)
>>>> -1.88047373 0.13532162 -13.8963286 0.15960440 -11.782092 sex
>>>> -0.23436854 0.08611269 -2.7216494 0.09050577 -2.589543 PC1
>>>> -0.05478639 0.06195318 -0.8843192 0.06822178 -0.803063 PC2
>>>> -0.09934572 0.06494563 -1.5296753 0.06520811 -1.523518 PC3 
>>>> -0.07020391 0.06626875 -1.0593818 0.06962147 -1.008366 PC4
>>>> -0.13413097 0.06746716 -1.9880927 0.06979901 -1.921674
>>>> 
>> 
>>>> ###GEESE
>>>> 
>>>>> summary(geese(case~sex+PC1+PC2+PC3+PC4,id=famid,family=binomial,data=
>>
>>>>> 
>>> data))$mean
>>>> 
>>>> estimate san.se wald p
>>>> 
>>>> (Intercept) -1.88047373 0.15960440 138.8176912 0.000000000 sex
>>>> -0.23436854 0.09050577 6.7057312 0.009610351 PC1 -0.05478639
>>>> 0.06822178 0.6449102 0.421938319 PC2 -0.09934572 0.06520811
>>>> 2.3211071 0.127629159 PC3 -0.07020391 0.06962147 1.0168016 
>>>> 0.313278888 PC4 -0.13413097 0.06979901 3.6928324 0.054646745
>>>> 
>>>> ### lme4_0.999999-2
>>>> 
>>>>> summary(glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data
>>
>>>>> 
>>> =data))
>>>> Estimate Std. Error z value Pr(>|z|) (Intercept) -3.01599 
>>>> 0.28305 -10.655 <2e-16 *** sex -0.41056 0.16285 -2.521 0.0117 *
>>>> PC1 -0.17116 0.12903 -1.326 0.1847 PC2 -0.15510 0.13382 -1.159
>>>> 0.2465 PC3 -0.19044 0.13580 -1.402 0.1608 PC4 0.02532 0.13732
>>>> 0.184 0.8537
>>>> 
>>>> ###lme4_1.0-6
>>>> 
>>>>> summary(glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data
>>
>>>>> 
>>> =data))
>>>> 
>>>> Estimate Std. Error z value Pr(>|z|)
>>>> 
>>>> (Intercept) -10.2784 0.8631 -11.909 <2e-16 *** sex 0.3497 
>>>> 0.1975 1.770 0.0767 .  PC1 -0.3555 0.1623 -2.190 0.0285 * PC2
>>>> -0.1087 0.1653 -0.657 0.5109 PC3 -0.2242 0.1652 -1.357 0.1748
>>>> PC4 0.1103 0.1671 0.660 0.5091
>>>> 
>>>> Case by sex
>>>> 
>>>> 1 2 0 2554 3021 1 310 290
>>>> 
>> 
>>> _______________________________________________ 
>>> R-sig-mixed-models at r-project.org mailing list 
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> 
>


From r.turner at auckland.ac.nz  Thu Mar 13 22:23:35 2014
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Fri, 14 Mar 2014 10:23:35 +1300
Subject: [R-sig-ME] Bernoulli glmm question.
In-Reply-To: <5321AFB4.3010104@lpl-aix.fr>
References: <5321101A.7040108@auckland.ac.nz>
	<EF7A2F64-E852-4C2A-A0E1-732713BD9B36@linguistics.rub.de>
	<5321AFB4.3010104@lpl-aix.fr>
Message-ID: <532221D7.7030202@auckland.ac.nz>


Thanks to Marko Bachl, Tibor Kiss, Robert Espesser, and off-list Kevin 
Thorpe and Robert Kushler who all provided useful feedback and advice.

The overall import of the replies was that I should include ***word*** 
as a random effect, which I shall do.  In retrospect it's kind of 
obvious that this effect should be included.  Duhhhh!

Robert Espesser also suggested that I might wish to include some 
interaction terms in the model. He's almost surely correct, but I'm a 
bit reluctant to do so as that would take me (and definitely) the 
graduate student well out of our comfort zones.  I'll play around with 
this idea and try the various models that Prof. Espesser suggested, and 
see how much difference they make to the inferences to be drawn about 
the fixed effects.

Thanks again.

cheers,

Rolf Turner


From bbolker at gmail.com  Fri Mar 14 04:28:46 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 13 Mar 2014 23:28:46 -0400
Subject: [R-sig-ME] gee, geese and glmer
In-Reply-To: <000301cf3f1f$f07fb0f0$d17f12d0$@bu.edu>
References: <007801cf3a86$6d27b060$47771120$@bu.edu>	<531B98B4.4050403@gmail.com>	<6984CA5898097D459F8EC753F025B0D839696447@IST-EX10MBX-3.ad.bu.edu>
	<21278.56111.63015.103003@stat.math.ethz.ch>
	<531F0F03.5090907@gmail.com>
	<6984CA5898097D459F8EC753F025B0D8396982CD@IST-EX10MBX-3.ad.bu.edu>
	<53221072.9080203@gmail.com>
	<000301cf3f1f$f07fb0f0$d17f12d0$@bu.edu>
Message-ID: <5322776E.90206@gmail.com>

  You should be able to install lme4.0 via:

install.packages("lme4.0",
    repos="http://lme4.r-forge.r-project.org/repos")

It should give identical results to lme4_0.999999-2.

  Also, if you can pull out the coefficients (both the theta and the
fixed effect parameters) and save them somewhere, you don't need to run
old and new lme4 in the same session.  You just need to be able to pull
both sets of parameters into an R session where you have lme4 >= 1.0
loaded, use devFunOnly=TRUE to extract the deviance function, and apply
it to both the old and the new sets of parameters.

  Ben Bolker


On 14-03-13 08:54 PM, Ming-Huei Chen wrote:
> Thanks Ben and Martin!
> 
> I tried lme4 1.1-4 and the result was identical to lme4 1.0-6 without
> warning (by @optinfo$warnings). As you mentioned about convergence warning,
> I redid the analysis with lme4_0.999999-2 and saw a convergence warning. I
> also tried MCMCglmm and here is the result, sex effect is insignificant but
> the estimate is negative as in GEE and lme4_0.999999-2. But as you
> mentioned, it's different methodology.. 
> 
> test <-
> MCMCglmm(case~sex+PC1+PC2+PC3+PC4,random=~famid,family="categorical",data=da
> ta)
>             post.mean  l-95% CI  u-95% CI eff.samp  pMCMC
> (Intercept) -2.469391 -2.821757 -2.078372    4.290 <0.001 ***
> sex         -0.055439 -0.330237  0.111188    5.426  0.626
> PC1         -0.197212 -0.332290 -0.018821    6.833  0.074 .
> PC2         -0.158433 -0.364921 -0.003554    5.142  0.002 **
> PC3         -0.130129 -0.320582  0.065973    4.094  0.320
> PC4          0.002677 -0.306265  0.233621    2.060  0.812
> 
> As for comparing deviance, because different versions of lme4 need to be
> installed in different versions of R and there is actually no lme4.0, so I
> don't know how to use two versions of lme4 in the same R session... Since
> you planned to get lme4.0 on CRAN, so you should have the package... in that
> case, can you please send it to me? Or? 
> 
> Our data contain pedigree information which cannot be shared (even pedigree
> structure), so simulate may not help. 
> 
> Best,
> 
> Ming-Huei
> 
> -----Original Message-----
> From: Ben Bolker [mailto:bbolker at gmail.com] 
> Sent: Thursday, March 13, 2014 4:09 PM
> To: Yang, Qiong; Martin Maechler
> Cc: r-sig-mixed-models at r-project.org; Chen, Ming-Huei
> Subject: Re: [R-sig-ME] gee, geese and glmer
> 
> On 14-03-13 02:01 PM, Yang, Qiong wrote:
>> Hi Ben and Martin,
>>
>> Thanks you for your reply and taking time to explain the situation. 
>> Maybe I wasn't clear in my previous message or don't have a deeper 
>> understanding of the CRAN policy: Instead of separate packages for 
>> "old-CRAN" lme4 (lme4.0), and the current lme4, which is not allowed 
>> by CRAN maintainer, is it possible to have an option in current lme4 
>> to call the algorithm in old lme4? I thought the new lme4 implements a 
>> different algorithm and why not make the old algorithm available in 
>> current lme4 lmer() as an option. It is like we can request pearson or 
>> spearman correlations in cor(). Sorry if I simplified the problem too 
>> much.
> 
>    It's not so much CRAN policy as the architecture of the software.  A
> great deal of the internal structure has changed between versions <1.0 and >
> 1.0, so putting both versions into one package would basically mean having
> copies of all of the old and all of the new code -- and keeping it all
> safely distinguished and separate would probably be substantially more work
> (we have about as much as we can to do keep up with a single package ...)
> As we said before, we were surprised by the rejection of lme4.0 by CRAN --
> our previous planning had assumed that we would be able to put lme4.0 on
> CRAN.  Somewhat to our surprise, there have been relatively few people
> contacting us with troubles similar to yours -- for the most part it seems
> that is (thankfully) very rare that
> lme4 gives significantly worse answers than lme4.0.
> 
>   So far it's not clear that lme4's answers are actually worse than lme4.0's
> (I admit that all other things being equal, closer to GEE is more likely to
> be correct -- on the other hand, we know that GEE gives marginal estimates
> of fixed parameters while GLMM gives conditional estimates, so we shouldn't
> expect them to be the same).  Can I confirm:
> 
> * you've tried this with the most recent development version of lme4
> (1.1-4) and you do *not* get any convergence warnings?
> 
> * have you compared the deviances based on the old (lme4.0 / lme4 < 1.0) and
> new packages?  Here is the outline of how you would do this:
> 
> library(lme4)   ## load new package
> m1 <- glmer(...) ## lme4 fit
> library(lme4.0)  ## load old package
> m0 <- lme4.0::glmer(...) ## lme4.0 fit
> ## extract full parameter set for lme4.0 and lme4 fits m0parms <-
> c(getME(m0,"theta"),fixef(m0)) m1parms <-
> c(lme4::getME(m1,"theta"),fixef(m1))
> nparms <- length(m0parms)
> ntheta <- length(getME(m0,"theta"))
> 
> par(las=1,bty="l")
> plot(m0parms,m1parms,col=rep(2:1,c(ntheta,nparms-ntheta)))
> abline(a=0,b=1)
> 
> ## set up deviance function
> dd <- lme4::glmer(...,devFunOnly=TRUE)
> ## compare
> dd(m0parms)
> dd(m1parms)
> 
>   If the new deviance is lower than the old deviance, that's a strong
> indication that the new version is actually getting a _better_ fit than the
> old one.
> 
>   Another diagnostic tool is:
> 
> library(bbmle)
> ss1 <- slice2D(fun = dd, params = m1parms, verbose = FALSE)
> splom(ss1)
> 
>   This will show you cross-sections of the deviance surface ...
> 
> 
>> To Ben, Sorry that we cannot share data because of confidentiality 
>> policy. But you are able to debug our problem through PC screen 
>> sharing where you can gain control and run programs on our Linux 
>> session without possessing the data. If that sounds like a good idea, 
>> let us know when you might be available to do so.
> 
>   Can you generate a simulated example that replicates your problem?
> simulate() is a useful tool for generating data that are similar, but not
> identical to, your original data (you can also anonymize factor labels
> etc.).
> 
>   PC screen sharing seems difficult -- I have easy access to MacOS and Linux
> systems, but not Windows.
> 
>>
>> Thanks, Qiong -----Original Message----- From: Ben Bolker 
>> [mailto:bbolker at gmail.com] Sent: Tuesday, March 11, 2014 9:26 AM To:
>> Martin Maechler; Yang, Qiong Cc: r-sig-mixed-models at r-project.org; 
>> Chen, Ming-Huei Subject: Re: [R-sig-ME] gee, geese and glmer
>>
>> I would also point out that we are indeed very interested, in the 
>> medium term (the short term is very very busy!), in making sure that 
>> your issues are resolved.  We would like lme4 to dominate lme4.0 
>> (i.e., to work better in all circumstances). So far it's been a bit 
>> difficult since we have been debugging remotely -- short of the 
>> suggestions I gave below, and without access to a reproducible 
>> example, it's very hard indeed for me to say much more.
>>
>> sincerely Ben Bolker
>>
>> On 14-03-11 05:45 AM, Martin Maechler wrote:
>>>>>>>> Yang, Qiong <qyang at bu.edu> on Mon, 10 Mar 2014 18:19:56
>>>>>>>> +0000 writes:
>>>
>>>> Hi Ben, We wonder if you can add an option in lmer() of current
>>>> lme4 version to call the algorithm used in lme4_0.999999-2?
>>>
>>> unfortunately not.
>>>
>>> For this reason, we had planned for many months, starting in August
>>>  2012 and announced on this mailing list at least a year ago that we 
>>> would provide the 'lme4.0' package  (back-compatible as well as  
>>> possible in light of computer OS updates incl system libraries, and R 
>>> updates, ...) in order to provide useRs the possibility of 
>>> reproducible research and data analysis for their analyses done with 
>>> "old-CRAN" lme4 versions.
>>>
>>> Unfortunately (for us), the CRAN maintainers decided that providing
>>>  lme4.0 in addition to lme4 was a bad idea, and explicit forbid such 
>>> actions in the (then new) CRAN policy document.  I'm still not at all 
>>> happy with that decision.
>>>
>>>> For our package (analyze rare genetic variant) to be put on CRAN, we 
>>>> need to use current version of lme4. However, at this point, there 
>>>> are still issues that cannot be resolved with newer versions of 
>>>> lme4. It is very difficult resolved with newer versions of lme4. It 
>>>> is very difficult for us to keep waiting and testing the new 
>>>> release, and hope all the issues resolved and no new issues coming 
>>>> up. lme4_0.999999-2 has been used by us for a long time with little 
>>>> problem.
>>>
>>> Good to hear.  Such cases were exactly the reason why we (lme4
>>> authors) made such considerable effort to provide  lme4.0 for 
>>> reproducible research and data analysis.
>>>
>>> at the bottom of
>>> https://github.com/lme4/lme4/blob/master/README.md we mention the 
>>> state and give installation instruction of lme4.0, but as you say, 
>>> this does not solve the problem for other package maintainers: If 
>>> they want a CRAN package, they (currently? I'm optimistic beyond 
>>> reason :-) cannot have a 'Depends: lme4.0'  (or "Imports:..." or 
>>> similar).
>>>
>>>> Your help on this is highly appreciated.  Best, Qiong
>>>
>>> You're welcome; currentl there's not more we can do. Martin
>>>
>>> -- Martin Maechler, ETH Zurich, Switzerland
>>>
>>>
>>>
>>>> -----Original Message----- From: Ben Bolker 
>>>> [mailto:bbolker at gmail.com] Sent: Saturday, March 08, 2014 5:25 PM
>>>> To: r-sig-mixed-models at r-project.org Cc: Chen, Ming-Huei; Yang, 
>>>> Qiong Subject: Re: gee, geese and glmer
>>>
>>>> On 14-03-07 11:25 PM, Ming-Huei Chen wrote:
>>>>> Hi Ben,
>>>>>
>>>>>
>>>>>
>>>>> In an analysis we found that glmer in new lme4 gave result 
>>>>> different from old lme4, gee and geese, where old lme4 seems to be 
>>>>> closer to gee and geese.. Please see highlighted sex effect below. 
>>>>> Case by sex (2x2) table is also given. Can you please let us know 
>>>>> how would you look into the results? Thanks!
>>>>>
>>>
>>>> [cc'ing to r-sig-mixed-models: **please** try r-sig-mixed-models 
>>>> first, not personal e-mail to me ...]
>>>
>>>> I can't say exactly what's going here; without having a reproducible 
>>>> example <http://tinyurl.com/reproducible-000> it's hard to say 
>>>> precisely.  Thoughts:
>>>
>>>> * gee and geese are giving _exactly_ the same parameter estimates, 
>>>> to 8 significant digits, so I would guess they are wrapping 
>>>> identical underlying methods.
>>>
>>>> * As far as diagnosing the issue with lme4 1.0-6: * does changing 
>>>> the optimization method, i.e.
>>>> glmerControl(optimizer="optimx",optCtrl=list(method="nlminb"))
>>>> [must do library("optimx") first] or
>>>> glmerControl(optimizer="bobyqa")
>>>
>>>> change the result?
>>>
>>>> * I would be curious whether the soon-to-be-released version
>>>> 1.1-4 (which can be installed from github or
>>>> lme4.r-forge.r-project.org/repos) gives either (1) convergence 
>>>> warnings or (2) different/better answers
>>>
>>>> * You can try specifying the starting values for lme4 to diagnose 
>>>> misconvergence; for example, start lme4 from the estimates given by 
>>>> old lme4/lme4.0 and see if it gives a similar answer.
>>>
>>>> * You can use the 'slice' and 'splom.slice' functions from bbmle to 
>>>> visualize the likelihood surfaces
>>>
>>>> good luck, Ben Bolker
>>>
>>>>> Ming-Huei
>>>>>
>>>
>>>>> ###GEE
>>>>>
>>>>>> summary(gee(case~sex+PC1+PC2+PC3+PC4,id=famid,family=binomial,data
>>>>>> =da
>>>
>>>>>>
>>>> ta))$coef
>>>>> Estimate Naive S.E.  Naive z Robust S.E.  Robust z (Intercept)
>>>>> -1.88047373 0.13532162 -13.8963286 0.15960440 -11.782092 sex
>>>>> -0.23436854 0.08611269 -2.7216494 0.09050577 -2.589543 PC1
>>>>> -0.05478639 0.06195318 -0.8843192 0.06822178 -0.803063 PC2
>>>>> -0.09934572 0.06494563 -1.5296753 0.06520811 -1.523518 PC3
>>>>> -0.07020391 0.06626875 -1.0593818 0.06962147 -1.008366 PC4
>>>>> -0.13413097 0.06746716 -1.9880927 0.06979901 -1.921674
>>>>>
>>>
>>>>> ###GEESE
>>>>>
>>>>>> summary(geese(case~sex+PC1+PC2+PC3+PC4,id=famid,family=binomial,da
>>>>>> ta=
>>>
>>>>>>
>>>> data))$mean
>>>>>
>>>>> estimate san.se wald p
>>>>>
>>>>> (Intercept) -1.88047373 0.15960440 138.8176912 0.000000000 sex
>>>>> -0.23436854 0.09050577 6.7057312 0.009610351 PC1 -0.05478639
>>>>> 0.06822178 0.6449102 0.421938319 PC2 -0.09934572 0.06520811
>>>>> 2.3211071 0.127629159 PC3 -0.07020391 0.06962147 1.0168016
>>>>> 0.313278888 PC4 -0.13413097 0.06979901 3.6928324 0.054646745
>>>>>
>>>>> ### lme4_0.999999-2
>>>>>
>>>>>> summary(glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,d
>>>>>> ata
>>>
>>>>>>
>>>> =data))
>>>>> Estimate Std. Error z value Pr(>|z|) (Intercept) -3.01599
>>>>> 0.28305 -10.655 <2e-16 *** sex -0.41056 0.16285 -2.521 0.0117 *
>>>>> PC1 -0.17116 0.12903 -1.326 0.1847 PC2 -0.15510 0.13382 -1.159
>>>>> 0.2465 PC3 -0.19044 0.13580 -1.402 0.1608 PC4 0.02532 0.13732
>>>>> 0.184 0.8537
>>>>>
>>>>> ###lme4_1.0-6
>>>>>
>>>>>> summary(glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,d
>>>>>> ata
>>>
>>>>>>
>>>> =data))
>>>>>
>>>>> Estimate Std. Error z value Pr(>|z|)
>>>>>
>>>>> (Intercept) -10.2784 0.8631 -11.909 <2e-16 *** sex 0.3497
>>>>> 0.1975 1.770 0.0767 .  PC1 -0.3555 0.1623 -2.190 0.0285 * PC2
>>>>> -0.1087 0.1653 -0.657 0.5109 PC3 -0.2242 0.1652 -1.357 0.1748
>>>>> PC4 0.1103 0.1671 0.660 0.5091
>>>>>
>>>>> Case by sex
>>>>>
>>>>> 1 2 0 2554 3021 1 310 290
>>>>>
>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list 
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>
>


From bbolker at gmail.com  Fri Mar 14 18:33:20 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 14 Mar 2014 13:33:20 -0400
Subject: [R-sig-ME] FW: GLMM Question
In-Reply-To: <25396_1394810808_s2EFQhmV013190_6D9F635A0599704DA9AFAFBDB89439EE0C3F45F8@VETMXHT.ads.vet.k-state.edu>
References: <25396_1394810808_s2EFQhmV013190_6D9F635A0599704DA9AFAFBDB89439EE0C3F45F8@VETMXHT.ads.vet.k-state.edu>
Message-ID: <53233D60.5030909@mcmaster.ca>

  These are all good questions.  I'm taking the liberty of forwarding
it to r-sig-mixed-models, as that's a public venue where multiple
people can ask questions, answers are publicly viewable, and answers
are archived.

  Short answers: drop1() (my preference) or the anova method from the
'car' package are good for testing whole factors (you may also want to
see the ?pvalues help page in recent versions of the lme4 package).

  For translation to probabilities, you may want
predict(...,type="response").  You may also find the effects package
useful.

 http://glmm.wikidot.com/faq can also be helpful; if you have
particular suggestions for material to go there, you can add it
yourself or suggest it on the r-sig-mixed-models list and someone
(maybe me) might add it ...

  good luck
    Ben Bolker


On 14-03-14 11:18 AM, Brad White wrote:
> 
> 
> Dr. Bolker,
> 
> Hi, I imagine you receive quite a few questions regarding modeling 
> applications, but I have searched online to no avail.  I found
> your paper in Trends and Ecology and Evolution from 2010 very
> useful, but it didn?t answer all my questions.
> 
> 
> 
> I?m a researcher in cattle disease and we have worked in the area
> of evaluating disease risk among groups (count data) as well as
> monitoring behavioral changes (again, count data).  In the past, we
> have done events/trials modeling with GLMM to account for random
> effects of repeated measures on individual animal as well as
> housing (calves housed within pens and sometimes pens within
> operations). For a little background (probably more than you want
> or need), I attached one of our articles describing behavior and
> the figures represent the type of information I?d like to glean
> from these models.
> 
> 
> 
> I have previously used SAS GLIMMIX to model our outcomes, but
> would really like to model these with R as I think this would be
> helpful for our graduate students (and me) in the future.  Our
> projects are set up to test specific hypotheses, thus generating
> estimates and inferences are important to me. I?ve tried several
> methods in R, but can?t seem to figure out the best way to evaluate
> model output (ideally I?d like to have a test for the entire
> factor, not just the tests on individual parameters of the factor).
> Also, I typically transform the estimates from these models to
> probabilities, but am having trouble from this part also.  I would
> much appreciate some direction on places to find further resources
> on this, it appears from my online searching that this is not the
> common way R output has been used.
> 
> 
> 
> As a specific example, below are models created in glmer and
> glmmpql where success is the event happening (count of seconds) and
> failure is the total number of seconds ? success.  Var1 is our
> treatment of interest (3 levels) and study day is our time factor
> of interest (again three levels).  When running these models, I get
> the estimate for Var1Hi, Var1Lo, but not for Var1CO as it is in the
> intercept. I would like to have a test for the overall effect of
> Var1 to test our hypothesis that Var1 impacts outcome: if so, then
> I would test individual treatments. Same is true for study day.
> Then I?d also like to create a table with the estimates (and SE)
> transformed to probabilities for the multiple variables.
> 
> 
> 
> Any help you can provide is much appreciated as I?ve struggled with
> this a while.
> 
> 
> 
> Thanks,
> 
> 
> 
> Brad
> 
> 
> 
> Glmer model:
> 
> 
> 
> model <- glmer(cbind(success, failure) ~ var1 + studyday + (1 |
> pen) + (1 | calfid),
> 
> family = binomial(link = "logit"), data = mydata)
> 
> summary(model)
> 
> 
> 
> Generalized linear mixed model fit by maximum likelihood
> ['glmerMod']
> 
> Family: binomial ( logit )
> 
> Formula: cbind(success, failure) ~ var1 + studyday + (1 | pen) +
> (1 |      calfid)
> 
> Data: mydata
> 
> 
> 
> AIC       BIC    logLik  deviance
> 
> 394776.0  394794.6 -197381.0  394762.0
> 
> 
> 
> Random effects:
> 
> Groups Name        Variance  Std.Dev.
> 
> calfid (Intercept) 4.352e+00 2.086e+00
> 
> pen    (Intercept) 3.237e-10 1.799e-05
> 
> Number of obs: 105, groups: calfid, 35; pen, 3
> 
> 
> 
> Fixed effects:
> 
> Estimate Std. Error z value Pr(>|z|)
> 
> (Intercept) -1.315422   0.604552   -2.18   0.0296 *
> 
> var1Hi       1.395121   0.853311    1.63   0.1021
> 
> var1Lo       1.256089   0.872415    1.44   0.1499
> 
> studyday1   -0.618371   0.001991 -310.62   <2e-16 ***
> 
> studyday2   -0.484639   0.001981 -244.64   <2e-16 ***
> 
> ---
> 
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> 
> 
> Correlation of Fixed Effects:
> 
> (Intr) var1Hi var1Lo stdyd1
> 
> var1Hi    -0.708
> 
> var1Lo    -0.693  0.491
> 
> studyday1 -0.002  0.000  0.000
> 
> studyday2 -0.002  0.000  0.000  0.589
> 
> 
> 
> glmmPQL model
> 
> model <- glmmPQL(cbind(success, failure) ~ var1 + studyday, 
> random=list(~1|pen, ~1|calfid),
> 
> family = binomial(link = "logit"), data = mydata)
> 
> 
> 
> summary of glmmPQL model
> 
> Linear mixed-effects model fit by maximum likelihood
> 
> Data: mydata
> 
> AIC BIC logLik
> 
> NA  NA     NA
> 
> 
> 
> Random effects:
> 
> Formula: ~1 | pen
> 
> (Intercept)
> 
> StdDev:   0.2823529
> 
> 
> 
> Formula: ~1 | calfid %in% pen
> 
> (Intercept) Residual
> 
> StdDev: 0.0008658502 103.4659
> 
> 
> 
> Variance function:
> 
> Structure: fixed weights
> 
> Formula: ~invwt
> 
> Fixed effects: cbind(success, failure) ~ var1 + studyday
> 
> Value Std.Error DF    t-value p-value
> 
> (Intercept) -0.1944745 0.2510724 68 -0.7745754  0.4413
> 
> var1Hi       0.2526331 0.1879374 30  1.3442412  0.1889
> 
> var1Lo       0.1101132 0.1942646 30  0.5668208  0.5751
> 
> studyday1   -0.5603492 0.1997436 68 -2.8053418  0.0065
> 
> studyday2   -0.4560453 0.1985154 68 -2.2972798  0.0247
> 
> Correlation:
> 
> (Intr) var1Hi var1Lo stdyd1
> 
> var1Hi    -0.381
> 
> var1Lo    -0.373  0.497
> 
> studyday1 -0.460 -0.006  0.001
> 
> studyday2 -0.463 -0.007  0.003  0.584
> 
> 
> 
> Standardized Within-Group Residuals:
> 
> Min           Q1          Med           Q3          Max
> 
> -2.553151595 -0.664950798  0.007380211  0.706653488  2.626906196
> 
> 
> 
> Number of Observations: 105
> 
> Number of Groups:
> 
> pen calfid %in% pen
> 
> 3              35
> 
> 
> 
> 
> 
> 
> 
> Brad White, DVM, MS
> 
> Beef Production Medicine
> 
> Q211 Mosier Hall
> 
> Manhattan, KS 66506
> 
> Phone: 785-532-4243
> 
> 
>


From bbolker at gmail.com  Fri Mar 14 19:11:57 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 14 Mar 2014 18:11:57 +0000 (UTC)
Subject: [R-sig-ME] Release Dates lme4 Version
References: <077E31A57DA26E46AB0D493C9966AC730DA0345FB5@UM-MAIL4112.unimaas.nl>
	<531F0C02.1030403@gmail.com>
	<077E31A57DA26E46AB0D493C9966AC730DA0346131@UM-MAIL4112.unimaas.nl>
Message-ID: <loom.20140314T191100-348@post.gmane.org>

Viechtbauer Wolfgang (STAT <wolfgang.viechtbauer at ...> writes:

> 
> Great! Looking forward to the new version on CRAN.
> 
> Best,
> Wolfgang

  New version (1.1-5) is on CRAN now; binaries gradually being built/
propagating to mirrors.  See 
https://github.com/lme4/lme4/blob/master/inst/NEWS.Rd
for changes in this release ...

  cheers
    Ben Bolker


From r.turner at auckland.ac.nz  Sat Mar 15 00:16:31 2014
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Sat, 15 Mar 2014 12:16:31 +1300
Subject: [R-sig-ME] Release Dates lme4 Version
In-Reply-To: <loom.20140314T191100-348@post.gmane.org>
References: <077E31A57DA26E46AB0D493C9966AC730DA0345FB5@UM-MAIL4112.unimaas.nl>
	<531F0C02.1030403@gmail.com>
	<077E31A57DA26E46AB0D493C9966AC730DA0346131@UM-MAIL4112.unimaas.nl>
	<loom.20140314T191100-348@post.gmane.org>
Message-ID: <53238DCF.8070204@auckland.ac.nz>

On 15/03/14 07:11, Ben Bolker wrote:
> Viechtbauer Wolfgang (STAT <wolfgang.viechtbauer at ...> writes:
>
>>
>> Great! Looking forward to the new version on CRAN.
>>
>> Best,
>> Wolfgang
>
>    New version (1.1-5) is on CRAN now; binaries gradually being built/
> propagating to mirrors.  See
> https://github.com/lme4/lme4/blob/master/inst/NEWS.Rd
> for changes in this release ...

Just tried to install version 1.1-5 (tar ball obtained from Austrian 
mirror).  It complains that  "?RcppEigen? 0.3.1.2.1 is being loaded, but 
0.3.1.2.3 is required".

But the latest version of RcppEigen that I can find on CRAN is good ol' 
0.3.1.2.1.

So do I have to go somewhere else to get 0.3.1.2.3?  That's a bit 
bewildering for us bunnies.

cheers,

Rolf Turner


From vjd4 at nyu.edu  Sat Mar 15 00:27:34 2014
From: vjd4 at nyu.edu (Vincent Dorie)
Date: Fri, 14 Mar 2014 19:27:34 -0400
Subject: [R-sig-ME] Release Dates lme4 Version
In-Reply-To: <53238DCF.8070204@auckland.ac.nz>
References: <077E31A57DA26E46AB0D493C9966AC730DA0345FB5@UM-MAIL4112.unimaas.nl>
	<531F0C02.1030403@gmail.com>
	<077E31A57DA26E46AB0D493C9966AC730DA0346131@UM-MAIL4112.unimaas.nl>
	<loom.20140314T191100-348@post.gmane.org>
	<53238DCF.8070204@auckland.ac.nz>
Message-ID: <23FF5589-96CC-4B2F-8A99-D50637B02867@nyu.edu>

As far as I can tell, there is a bit of understating when it comes to version numbers for R itself. RcppEigen 0.3.1.2.1 depends on Rcp 0.11.0, which in turn depends on R 3.0.0. So even though just about every package in the lme4 chain claims to work on 2.15 or so, the backwards compatibility has been broken.

On Mar 14, 2014, at 7:16 PM, Rolf Turner wrote:

> On 15/03/14 07:11, Ben Bolker wrote:
>> Viechtbauer Wolfgang (STAT <wolfgang.viechtbauer at ...> writes:
>> 
>>> 
>>> Great! Looking forward to the new version on CRAN.
>>> 
>>> Best,
>>> Wolfgang
>> 
>>   New version (1.1-5) is on CRAN now; binaries gradually being built/
>> propagating to mirrors.  See
>> https://github.com/lme4/lme4/blob/master/inst/NEWS.Rd
>> for changes in this release ...
> 
> Just tried to install version 1.1-5 (tar ball obtained from Austrian mirror).  It complains that  "?RcppEigen? 0.3.1.2.1 is being loaded, but 0.3.1.2.3 is required".
> 
> But the latest version of RcppEigen that I can find on CRAN is good ol' 0.3.1.2.1.
> 
> So do I have to go somewhere else to get 0.3.1.2.3?  That's a bit bewildering for us bunnies.
> 
> cheers,
> 
> Rolf Turner
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From bbolker at gmail.com  Sun Mar 16 20:11:27 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 16 Mar 2014 15:11:27 -0400
Subject: [R-sig-ME] Release Dates lme4 Version
In-Reply-To: <23FF5589-96CC-4B2F-8A99-D50637B02867@nyu.edu>
References: <077E31A57DA26E46AB0D493C9966AC730DA0345FB5@UM-MAIL4112.unimaas.nl>	<531F0C02.1030403@gmail.com>	<077E31A57DA26E46AB0D493C9966AC730DA0346131@UM-MAIL4112.unimaas.nl>	<loom.20140314T191100-348@post.gmane.org>	<53238DCF.8070204@auckland.ac.nz>
	<23FF5589-96CC-4B2F-8A99-D50637B02867@nyu.edu>
Message-ID: <5325F75F.8000800@gmail.com>


  Hmmm. As Brian Ripley pointed out to me offline,

> Not so.  RcppEigen (> 0.3.1.2.3) appears to mean >= 0.3.2.0, which
depends on Rcpp (>= 0.10.5) and R (>= 2.15.1) (also the dependence of
Rcpp 0.10.5).

  I just double-checked this myself and (as usual) he seems to be right.

  Vince, maybe you were looking at RcppEigen 0.3.2.1.1 instead?

  Rolf, where were you looking on CRAN for RcppEigen?

 * http://cran.r-project.org/web/packages/RcppEigen/index.html shows
version 0.3.2.1.1
 * The Windows binary available for 2.15 is 0.3.2.0.2:
http://cran.r-project.org/bin/windows/contrib/2.15/RcppEigen_0.3.2.0.2.zip

  cheers
    Ben Bolker


On 14-03-14 07:27 PM, Vincent Dorie wrote:
> As far as I can tell, there is a bit of understating when it comes to
> version numbers for R itself. RcppEigen 0.3.1.2.1 depends on Rcp
> 0.11.0, which in turn depends on R 3.0.0. So even though just about
> every package in the lme4 chain claims to work on 2.15 or so, the
> backwards compatibility has been broken.
> 
> On Mar 14, 2014, at 7:16 PM, Rolf Turner wrote:
> 
>> On 15/03/14 07:11, Ben Bolker wrote:
>>> Viechtbauer Wolfgang (STAT <wolfgang.viechtbauer at ...> writes:
>>> 
>>>> 
>>>> Great! Looking forward to the new version on CRAN.
>>>> 
>>>> Best, Wolfgang
>>> 
>>> New version (1.1-5) is on CRAN now; binaries gradually being
>>> built/ propagating to mirrors.  See 
>>> https://github.com/lme4/lme4/blob/master/inst/NEWS.Rd for changes
>>> in this release ...
>> 
>> Just tried to install version 1.1-5 (tar ball obtained from
>> Austrian mirror).  It complains that  "?RcppEigen? 0.3.1.2.1 is
>> being loaded, but 0.3.1.2.3 is required".
>> 
>> But the latest version of RcppEigen that I can find on CRAN is good
>> ol' 0.3.1.2.1.
>> 
>> So do I have to go somewhere else to get 0.3.1.2.3?  That's a bit
>> bewildering for us bunnies.
>> 
>> cheers,
>> 
>> Rolf Turner
>> 
>> _______________________________________________ 
>> R-sig-mixed-models at r-project.org mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From vjd4 at nyu.edu  Sun Mar 16 21:03:25 2014
From: vjd4 at nyu.edu (Vincent Dorie)
Date: Sun, 16 Mar 2014 16:03:25 -0400
Subject: [R-sig-ME] Release Dates lme4 Version
In-Reply-To: <5325F75F.8000800@gmail.com>
References: <077E31A57DA26E46AB0D493C9966AC730DA0345FB5@UM-MAIL4112.unimaas.nl>	<531F0C02.1030403@gmail.com>	<077E31A57DA26E46AB0D493C9966AC730DA0346131@UM-MAIL4112.unimaas.nl>	<loom.20140314T191100-348@post.gmane.org>	<53238DCF.8070204@auckland.ac.nz>
	<23FF5589-96CC-4B2F-8A99-D50637B02867@nyu.edu>
	<5325F75F.8000800@gmail.com>
Message-ID: <2339A4C7-DF1C-4F09-9F82-CD2AAF9C1924@nyu.edu>

I wrote that in a rush so I'm likely mistaken, but when I tried to test backwards compatibility using 2.15.3 the packages weren't available. Specifically the lme4 version for macs on CRAN was  < 1.0. It looks as if OS X users don't have the same depth of backwards compatibility out of the box, although I suppose that could depend on the mirror.

Anyhow, moral may or may not be: just upgrade everything.

On Mar 16, 2014, at 3:11 PM, Ben Bolker wrote:

> 
>  Hmmm. As Brian Ripley pointed out to me offline,
> 
>> Not so.  RcppEigen (> 0.3.1.2.3) appears to mean >= 0.3.2.0, which
> depends on Rcpp (>= 0.10.5) and R (>= 2.15.1) (also the dependence of
> Rcpp 0.10.5).
> 
>  I just double-checked this myself and (as usual) he seems to be right.
> 
>  Vince, maybe you were looking at RcppEigen 0.3.2.1.1 instead?
> 
>  Rolf, where were you looking on CRAN for RcppEigen?
> 
> * http://cran.r-project.org/web/packages/RcppEigen/index.html shows
> version 0.3.2.1.1
> * The Windows binary available for 2.15 is 0.3.2.0.2:
> http://cran.r-project.org/bin/windows/contrib/2.15/RcppEigen_0.3.2.0.2.zip
> 
>  cheers
>    Ben Bolker
> 
> 
> On 14-03-14 07:27 PM, Vincent Dorie wrote:
>> As far as I can tell, there is a bit of understating when it comes to
>> version numbers for R itself. RcppEigen 0.3.1.2.1 depends on Rcp
>> 0.11.0, which in turn depends on R 3.0.0. So even though just about
>> every package in the lme4 chain claims to work on 2.15 or so, the
>> backwards compatibility has been broken.
>> 
>> On Mar 14, 2014, at 7:16 PM, Rolf Turner wrote:
>> 
>>> On 15/03/14 07:11, Ben Bolker wrote:
>>>> Viechtbauer Wolfgang (STAT <wolfgang.viechtbauer at ...> writes:
>>>> 
>>>>> 
>>>>> Great! Looking forward to the new version on CRAN.
>>>>> 
>>>>> Best, Wolfgang
>>>> 
>>>> New version (1.1-5) is on CRAN now; binaries gradually being
>>>> built/ propagating to mirrors.  See 
>>>> https://github.com/lme4/lme4/blob/master/inst/NEWS.Rd for changes
>>>> in this release ...
>>> 
>>> Just tried to install version 1.1-5 (tar ball obtained from
>>> Austrian mirror).  It complains that  "?RcppEigen? 0.3.1.2.1 is
>>> being loaded, but 0.3.1.2.3 is required".
>>> 
>>> But the latest version of RcppEigen that I can find on CRAN is good
>>> ol' 0.3.1.2.1.
>>> 
>>> So do I have to go somewhere else to get 0.3.1.2.3?  That's a bit
>>> bewildering for us bunnies.
>>> 
>>> cheers,
>>> 
>>> Rolf Turner
>>> 
>>> _______________________________________________ 
>>> R-sig-mixed-models at r-project.org mailing list 
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> 
>> _______________________________________________ 
>> R-sig-mixed-models at r-project.org mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> 
> 


From bbolker at gmail.com  Sun Mar 16 21:20:52 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 16 Mar 2014 16:20:52 -0400
Subject: [R-sig-ME] Release Dates lme4 Version
In-Reply-To: <2339A4C7-DF1C-4F09-9F82-CD2AAF9C1924@nyu.edu>
References: <077E31A57DA26E46AB0D493C9966AC730DA0345FB5@UM-MAIL4112.unimaas.nl>	<531F0C02.1030403@gmail.com>	<077E31A57DA26E46AB0D493C9966AC730DA0346131@UM-MAIL4112.unimaas.nl>	<loom.20140314T191100-348@post.gmane.org>	<53238DCF.8070204@auckland.ac.nz>
	<23FF5589-96CC-4B2F-8A99-D50637B02867@nyu.edu>
	<5325F75F.8000800@gmail.com>
	<2339A4C7-DF1C-4F09-9F82-CD2AAF9C1924@nyu.edu>
Message-ID: <532607A4.60209@gmail.com>

On 14-03-16 04:03 PM, Vincent Dorie wrote:
> I wrote that in a rush so I'm likely mistaken, but when I tried to test backwards 
> compatibility using 2.15.3 the packages weren't available. Specifically the lme4 
> version for macs on CRAN was  < 1.0. It looks as if OS X users don't
have the
> same depth of backwards compatibility out of the box, although
> I suppose that could depend on the mirror.

  Well, http://cran.r-project.org/bin/macosx/contrib should be the
reference site, and that appears only to have binaries available for
3.0, 3.1, r-devel, and r-release.  I'm not even sure where packages
would be pulled from for R 2.15.3 ...

> 
> Anyhow, moral may or may not be: just upgrade everything.
> 
> On Mar 16, 2014, at 3:11 PM, Ben Bolker wrote:
> 
>>
>>  Hmmm. As Brian Ripley pointed out to me offline,
>>
>>> Not so.  RcppEigen (> 0.3.1.2.3) appears to mean >= 0.3.2.0, which
>> depends on Rcpp (>= 0.10.5) and R (>= 2.15.1) (also the dependence of
>> Rcpp 0.10.5).
>>
>>  I just double-checked this myself and (as usual) he seems to be right.
>>
>>  Vince, maybe you were looking at RcppEigen 0.3.2.1.1 instead?
>>
>>  Rolf, where were you looking on CRAN for RcppEigen?
>>
>> * http://cran.r-project.org/web/packages/RcppEigen/index.html shows
>> version 0.3.2.1.1
>> * The Windows binary available for 2.15 is 0.3.2.0.2:
>> http://cran.r-project.org/bin/windows/contrib/2.15/RcppEigen_0.3.2.0.2.zip
>>
>>  cheers
>>    Ben Bolker
>>
>>
>> On 14-03-14 07:27 PM, Vincent Dorie wrote:
>>> As far as I can tell, there is a bit of understating when it comes to
>>> version numbers for R itself. RcppEigen 0.3.1.2.1 depends on Rcp
>>> 0.11.0, which in turn depends on R 3.0.0. So even though just about
>>> every package in the lme4 chain claims to work on 2.15 or so, the
>>> backwards compatibility has been broken.
>>>
>>> On Mar 14, 2014, at 7:16 PM, Rolf Turner wrote:
>>>
>>>> On 15/03/14 07:11, Ben Bolker wrote:
>>>>> Viechtbauer Wolfgang (STAT <wolfgang.viechtbauer at ...> writes:
>>>>>
>>>>>>
>>>>>> Great! Looking forward to the new version on CRAN.
>>>>>>
>>>>>> Best, Wolfgang
>>>>>
>>>>> New version (1.1-5) is on CRAN now; binaries gradually being
>>>>> built/ propagating to mirrors.  See 
>>>>> https://github.com/lme4/lme4/blob/master/inst/NEWS.Rd for changes
>>>>> in this release ...
>>>>
>>>> Just tried to install version 1.1-5 (tar ball obtained from
>>>> Austrian mirror).  It complains that  "?RcppEigen? 0.3.1.2.1 is
>>>> being loaded, but 0.3.1.2.3 is required".
>>>>
>>>> But the latest version of RcppEigen that I can find on CRAN is good
>>>> ol' 0.3.1.2.1.
>>>>
>>>> So do I have to go somewhere else to get 0.3.1.2.3?  That's a bit
>>>> bewildering for us bunnies.
>>>>
>>>> cheers,
>>>>
>>>> Rolf Turner
>>>>
>>>> _______________________________________________ 
>>>> R-sig-mixed-models at r-project.org mailing list 
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>> _______________________________________________ 
>>> R-sig-mixed-models at r-project.org mailing list 
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>
> 
r


From r.turner at auckland.ac.nz  Sun Mar 16 21:47:19 2014
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Mon, 17 Mar 2014 09:47:19 +1300
Subject: [R-sig-ME] Release Dates lme4 Version
In-Reply-To: <5325F75F.8000800@gmail.com>
References: <077E31A57DA26E46AB0D493C9966AC730DA0345FB5@UM-MAIL4112.unimaas.nl>	<531F0C02.1030403@gmail.com>	<077E31A57DA26E46AB0D493C9966AC730DA0346131@UM-MAIL4112.unimaas.nl>	<loom.20140314T191100-348@post.gmane.org>	<53238DCF.8070204@auckland.ac.nz>
	<23FF5589-96CC-4B2F-8A99-D50637B02867@nyu.edu>
	<5325F75F.8000800@gmail.com>
Message-ID: <53260DD7.6080806@auckland.ac.nz>

On 17/03/14 08:11, Ben Bolker wrote:

<SNIP>
>
>    Rolf, where were you looking on CRAN for RcppEigen?
>

Sorry.  Lysdexia.

cheers,

Rolf


From karolyn at zymoresearch.com  Mon Mar 17 17:02:05 2014
From: karolyn at zymoresearch.com (Karolyn Giang)
Date: Mon, 17 Mar 2014 16:02:05 +0000 (UTC)
Subject: [R-sig-ME] Easily perform your own Genome-wide Epigenetic analyses
Message-ID: <144d0c74a9b.327f.f4f20f@localhost.localdomain>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140317/6edbc0c0/attachment.pl>

From bbolker at gmail.com  Tue Mar 18 03:28:45 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 17 Mar 2014 22:28:45 -0400
Subject: [R-sig-ME] Fwd: RE:  gee, geese and glmer
In-Reply-To: <6984CA5898097D459F8EC753F025B0D8396992C2@IST-EX10MBX-3.ad.bu.edu>
References: <6984CA5898097D459F8EC753F025B0D8396992C2@IST-EX10MBX-3.ad.bu.edu>
Message-ID: <5327AF5D.70704@gmail.com>


[forwarding a conversation about lme4/lme4.0 incompatibilities.  This
example looks pretty interesting, as it seems hard to prove that lme4
*isn't* giving the right answer/an answer that is numerically superior
to lme4.0, yet the lme4.0 answer is biologically preferable/more similar
to other estimation approaches.  I don't know yet if we will eventually
find out that (1) the data are weird in a way that explains the
difference; (2) lme4 is actually misconverging, preliminary evidence to
the contrary; (3) ???  Enlightening comments are welcome.]


-------- Original Message --------
Subject: RE: [R-sig-ME] gee, geese and glmer
Date: Tue, 18 Mar 2014 00:18:01 +0000
From: Yang, Qiong <qyang at bu.edu>
To: Ben Bolker <bbolker at gmail.com>, "Chen, Ming-Huei" <mhchen at bu.edu>
CC: 'lme4-authors at lists.r-forge.r-project.org'
<lme4-authors at r-forge.wu-wien.ac.at>

Hi Ben,
Thanks for be mindful about our data confidentiality- what we
communicated in this email sequence so far can be copied to the general
mailing list.

Just to clarify further: All the cases are singletons(i.e. families of 1
member) while controls are from extended families (i.e. families of
multiple members).

The predictor variables do not identify the cases from controls.
We fitted reduce model where the only predictor is sex which does not
identify cases from controls however still observed reversed sign on
beta coefficients between lme4 and other software:
First, let's look at crude case by sex table
        1    2
  0   2554 3021
  1    310  290
Ignoring family structure: Odds Ratio=0.79; beta=-0.23

Since all cases are singletons, I don't think including family-specific
random effects should change the direction of sex effects. Therefore I
tend to think the effect direction given by lme4 is not correct. Below
are the results with only sex as predictor.
> library(lme4)
> m1 <- glmer(case~sex+(1|famid),family=binomial,data=data)
> m1 at optinfo$warnings
list()
> coef(summary(m1))
               Estimate Std. Error    z value      Pr(>|z|)
(Intercept) -10.2359223  0.4644516 -22.038725 1.225388e-107
sex           0.3531757  0.1837722   1.921813  5.462931e-02
> library(lme4.0)
> m0 <- lme4.0::glmer(case~sex+(1|famid),family=binomial,data=data)
Warning message:
In mer_finalize(ans) : false convergence (8)
> coef(summary(m0))
              Estimate Std. Error    z value     Pr(>|z|)
(Intercept) -3.2807992  0.2868701 -11.436533 2.746424e-30
sex         -0.2592505  0.1629382  -1.591097 1.115877e-01

> library(geepack)
> summary(geese(case~sex,id=famid,family=binomial,data=data))$mean
              estimate     san.se      wald           p
(Intercept) -1.8742252 0.16010676 137.03271 0.000000000
sex         -0.2346185 0.09068416   6.69363 0.009675797
> library(gee)
> summary(gee(case~sex,id=famid,family=binomial,data=data))$coef
              Estimate Naive S.E.    Naive z Robust S.E.   Robust z
(Intercept) -1.8742252 0.13510959 -13.871889  0.16010676 -11.706097
sex         -0.2346185 0.08601766  -2.727562  0.09068416  -2.587205

Thanks a lot, Qiong

-----Original Message-----
From: Ben Bolker [mailto:bbolker at gmail.com]
Sent: Monday, March 17, 2014 3:51 PM
To: Chen, Ming-Huei; Yang, Qiong
Cc: 'lme4-authors at lists.r-forge.r-project.org'
Subject: Re: [R-sig-ME] gee, geese and glmer


  [I'm not going to cc: to r-sig-mixed-models, as you may consider parts
of it to be sensitive, but I would like to ask permission to do so in
the future, as I think it would be useful to have these conversations in
public -- please let me know]


On 14-03-17 03:39 PM, Ming-Huei Chen wrote:
> Thanks, Ben!
> 
> Our data was combined by two cohorts, one of them contains only cases 
> and is an unrelated sample (249 male, 182 female). Do you think this 
> may lead to what we observed here?

  Possibly, although I would think the cohorts would have to be
identifiable somehow from the predictor variables.

> 
> * I don't know why I got an error...
> Error: could not find function "slicetrans"

  slicetrans is a function from the bbmle package, but it turns out to
be un-exported (this is still rather experimental functionality), so you
would need to use bbmle:::slicetrans

> 
> * Do you mean using nAGQ? If so, I used nAGQ=5 an dgot a warning.
>> x <-
> summary(lme4::glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial
> ,data=
> test.dat,nAGQ=5))
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, checkCtrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 0.00275074 (tol = 0.001) 
> Fixed effects:
>              Estimate Std. Error z value Pr(>|z|)
> (Intercept) -2.675364   0.282841  -9.459   <2e-16 ***
> sex         -0.053593   0.146444  -0.366   0.7144
> PC1         -0.205982   0.114760  -1.795   0.0727 .
> PC2         -0.161422   0.119219  -1.354   0.1757
> PC3         -0.191577   0.121107  -1.582   0.1137
> PC4          0.002723   0.122446   0.022   0.9823

  This warning *might* be spurious (the lme4 maintainers have been
having a private conversation about the fact that one may get
false-positive warnings about gradients when the fit is singular):

> 
> * Do you mean use fixef.prior? fixef.prior: a BLME prior of family 
> normal, t, or NULL. So which one is weaker prior?

   t is weaker than Normal (i.e., for the same mean (typically 0) and
variance it has fatter tails).  However, there are two dimensions of
"weakness" -- to make a prior weaker you can make the scale (i.e.
variance) larger, and/or make the shape parameter more diffuse (i.e.,
make the tails fatter, in the sequence Gaussian (= t_\infty -> t_{large
n} -> t_{smaller n}).  (You may end up discovering that the results
depend on the strength of the prior ... which will open up interesting
cans of worms in terms of what the data are actually telling you ...)
> 
> Best,
> 
> Ming-Huei
> 
> -----Original Message-----
> From: Ben Bolker [mailto:bbolker at gmail.com]
> Sent: Friday, March 14, 2014 5:33 PM
> To: Ming-Huei Chen; 'Yang, Qiong'
> Cc: 'lme4-authors at lists.r-forge.r-project.org'
> Subject: Re: [R-sig-ME] gee, geese and glmer
> 
>   OK, this definitely looks well-behaved.  I am leaning still more 
> heavily toward the idea that new-lme4 is giving an answer that is 
> technically more correct (the splom plot shows the focal parameters 
> and the location of the minimum computed 'slice' value as closed and 
> open circles (I forget which is which); if they don't coincide, that 
> suggests a misconvergence.  There are a few cases where there is a 
> barely visible difference (i.e. you can see the open circle peeking 
> out from around the edge of the closed circle), but they're generally 
> very close.  The deviance surface looks sensibly smooth/ellipsoidal, and none of the parameters appear to be on boundary.
> (There's a little problem with the slice2D code that limits the ranges 
> sampled to be either all-positive or all-negative, I think.)
> 
>    If so, that makes the difference between GEE, Laplace 
> approximation, and Bayesian GLMM as much of a statistical as a computational problem.
> A few more things I would try to follow up:
> 
>   * try s2 <- slicetrans(m0, m1, dd); plot(s2) to examine the 
> cross-section of the deviance function between the new and old parameters
>   * try using Gauss-Hermite quadrature to see if that makes a difference
>   * try blme::bglmer to impose a weak prior on the fixed effects (see 
> if that makes the results closer to MCMCglmm)
> 
> 
> 
> On 14-03-14 02:15 PM, Ming-Huei Chen wrote:
>> Here it is.... 
>>
>> ###new
>>> library(lme4)
>>> m1 <-
>> glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data=test.da
>> t
>> )
>>> m1 at optinfo$warnings
>> list()
>>> library(optimx)
>> Loading required package: numDeriv
>>> m1.1 <-
>> glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data=test.da
>> t
>> ,contr
>> ol=glmerControl(optimizer="optimx",optCtrl=list(method="nlminb")))
>>> m1.1 at optinfo$warnings
>> list()
>>
>> ###old
>>> library(lme4.0)
>>> m0 <-
>> glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data=test.da
>> t
>> )
>> Warning message:
>> In mer_finalize(ans) : false convergence (8)
>>> library(optimx)
>> Loading required package: numDeriv
>>> m0.0 <-
>> glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data=test.da
>> t
>> ,contr
>> ol=glmerControl(optimizer="optimx",optCtrl=list(method="nlminb")))
>> Error in do.call(lmerControl, control) :
>>   could not find function "glmerControl"
>>
>> Best,
>>
>> Ming-Huei
>> -----Original Message-----
>> From: Ben Bolker [mailto:bbolker at gmail.com]
>> Sent: Friday, March 14, 2014 11:08 AM
>> To: Ming-Huei Chen; 'Yang, Qiong'
>> Cc: lme4-authors at lists.r-forge.r-project.org
>> Subject: Re: [R-sig-ME] gee, geese and glmer
>>
>>   The fact that the new deviance is lower suggests that the new 
>> version of
>> lme4 is actually finding a _better_ fit to the data; so does the 
>> existence of a convergence warning from old lme4 (it's interesting 
>> that using
>> control=glmerControl(optimizer="optimx",optCtrl=list(method="nlminb")
>> ) doesn't give you a similar warning ...) . There is still some minor 
>> possibility that there's a glitch in the deviance calculation -- a 
>> good cross-check would be to evaluate the deviance of both sets of 
>> parameters for old lme4 as well, but that's a bit harder to do 
>> because
>> old-lme4 doesn't have the same level of flexibility).
>>
>>   The parameter comparisons that new-lme4 is giving fairly extreme 
>> estimates for one of the fixed parameters (old=-3, new=-10; looking 
>> at the parameter tables below, it looks like this is the intercept) 
>> and a very large estimate of the random-effects standard deviation. 
>> (I could have figured this out from the parameter tables too, but I'm 
>> lazy and graphical presentation makes it much more obvious.)
>>
>>   This is looking like a case of (quasi-)complete separation, which 
>> would explain why MCMCglmm is giving a much less extreme result (it 
>> imposes weak priors on all of the parameters by default).  It would 
>> be interesting to try blme::bglmer on this case as well.
>>
>> Can you send the (graphical) results of
>>
>> dd <- lme4::glmer(...,devFunOnly=TRUE)
>> library(bbmle)
>> ss1 <- slice2D(fun = dd, params = m1parms, verbose = FALSE)
>> splom(ss1)
>>
>> ??
>>
>> On 14-03-14 09:39 AM, Ming-Huei Chen wrote:
>>> Thanks, Ben!
>>>
>>> The new deviance is lower than the old one. The plot of estimates is 
>>> also attached.
>>>> dd(m0parms)
>>> [1] 3301.035
>>>> dd(m1parms)
>>> [1] 2991.881
>>>
>>> Best,
>>>
>>> Ming-Huei
>>> -----Original Message-----
>>> From: Ben Bolker [mailto:bbolker at gmail.com]
>>> Sent: Thursday, March 13, 2014 11:29 PM
>>> To: Ming-Huei Chen; 'Yang, Qiong'; 'Martin Maechler'
>>> Cc: r-sig-mixed-models at r-project.org
>>> Subject: Re: [R-sig-ME] gee, geese and glmer
>>>
>>>   You should be able to install lme4.0 via:
>>>
>>> install.packages("lme4.0",
>>>     repos="http://lme4.r-forge.r-project.org/repos")
>>>
>>> It should give identical results to lme4_0.999999-2.
>>>
>>>   Also, if you can pull out the coefficients (both the theta and the 
>>> fixed effect parameters) and save them somewhere, you don't need to 
>>> run old and new lme4 in the same session.  You just need to be able 
>>> to pull both sets of parameters into an R session where you have 
>>> lme4
>>>> =
>>> 1.0 loaded, use devFunOnly=TRUE to extract the deviance function, 
>>> and apply it to both the old and the new sets of parameters.
>>>
>>>   Ben Bolker
>>>
>>>
>>> On 14-03-13 08:54 PM, Ming-Huei Chen wrote:
>>>> Thanks Ben and Martin!
>>>>
>>>> I tried lme4 1.1-4 and the result was identical to lme4 1.0-6 
>>>> without warning (by @optinfo$warnings). As you mentioned about 
>>>> convergence warning, I redid the analysis with lme4_0.999999-2 and 
>>>> saw a convergence warning. I also tried MCMCglmm and here is the 
>>>> result, sex effect is insignificant but the estimate is negative as 
>>>> in GEE and lme4_0.999999-2. But as you mentioned, it's different
> methodology..
>>>>
>>>> test <-
>>>> MCMCglmm(case~sex+PC1+PC2+PC3+PC4,random=~famid,family="categorical"
>>>> ,
>>>> d
>>>> ata=da
>>>> ta)
>>>>             post.mean  l-95% CI  u-95% CI eff.samp  pMCMC
>>>> (Intercept) -2.469391 -2.821757 -2.078372    4.290 <0.001 ***
>>>> sex         -0.055439 -0.330237  0.111188    5.426  0.626
>>>> PC1         -0.197212 -0.332290 -0.018821    6.833  0.074 .
>>>> PC2         -0.158433 -0.364921 -0.003554    5.142  0.002 **
>>>> PC3         -0.130129 -0.320582  0.065973    4.094  0.320
>>>> PC4          0.002677 -0.306265  0.233621    2.060  0.812
>>>>
>>>> As for comparing deviance, because different versions of lme4 need 
>>>> to be installed in different versions of R and there is actually no 
>>>> lme4.0, so I don't know how to use two versions of lme4 in the same 
>>>> R session... Since you planned to get lme4.0 on CRAN, so you should 
>>>> have the package... in that case, can you please send it to me? Or?
>>>>
>>>> Our data contain pedigree information which cannot be shared (even 
>>>> pedigree structure), so simulate may not help.
>>>>
>>>> Best,
>>>>
>>>> Ming-Huei
>>>>
>>>> -----Original Message-----
>>>> From: Ben Bolker [mailto:bbolker at gmail.com]
>>>> Sent: Thursday, March 13, 2014 4:09 PM
>>>> To: Yang, Qiong; Martin Maechler
>>>> Cc: r-sig-mixed-models at r-project.org; Chen, Ming-Huei
>>>> Subject: Re: [R-sig-ME] gee, geese and glmer
>>>>
>>>> On 14-03-13 02:01 PM, Yang, Qiong wrote:
>>>>> Hi Ben and Martin,
>>>>>
>>>>> Thanks you for your reply and taking time to explain the situation. 
>>>>> Maybe I wasn't clear in my previous message or don't have a deeper 
>>>>> understanding of the CRAN policy: Instead of separate packages for 
>>>>> "old-CRAN" lme4 (lme4.0), and the current lme4, which is not 
>>>>> allowed by CRAN maintainer, is it possible to have an option in 
>>>>> current lme4 to call the algorithm in old lme4? I thought the new
>>>>> lme4 implements a different algorithm and why not make the old 
>>>>> algorithm available in current lme4 lmer() as an option. It is 
>>>>> like we can request pearson or spearman correlations in cor(). 
>>>>> Sorry if I simplified the problem too much.
>>>>
>>>>    It's not so much CRAN policy as the architecture of the software.  
>>>> A great deal of the internal structure has changed between versions
>>>> <1.0 and > 1.0, so putting both versions into one package would 
>>>> basically mean having copies of all of the old and all of the new 
>>>> code
>>>> -- and keeping it all safely distinguished and separate would 
>>>> probably be substantially more work (we have about as much as we 
>>>> can to do keep up with a single package ...) As we said before, we 
>>>> were surprised by the rejection of lme4.0 by CRAN -- our previous 
>>>> planning had assumed that we would be able to put lme4.0 on CRAN.
>>>> Somewhat to our surprise, there have been relatively few people 
>>>> contacting us with troubles similar to yours -- for the most part 
>>>> it seems that is
>>>> (thankfully) very rare that
>>>> lme4 gives significantly worse answers than lme4.0.
>>>>
>>>>   So far it's not clear that lme4's answers are actually worse than 
>>>> lme4.0's (I admit that all other things being equal, closer to GEE 
>>>> is more likely to be correct -- on the other hand, we know that GEE 
>>>> gives marginal estimates of fixed parameters while GLMM gives 
>>>> conditional estimates, so we shouldn't expect them to be the same).
>>>> Can
>> I confirm:
>>>>
>>>> * you've tried this with the most recent development version of 
>>>> lme4
>>>> (1.1-4) and you do *not* get any convergence warnings?
>>>>
>>>> * have you compared the deviances based on the old (lme4.0 / lme4 <
>>>> 1.0) and new packages?  Here is the outline of how you would do this:
>>>>
>>>> library(lme4)   ## load new package
>>>> m1 <- glmer(...) ## lme4 fit
>>>> library(lme4.0)  ## load old package
>>>> m0 <- lme4.0::glmer(...) ## lme4.0 fit ## extract full parameter 
>>>> set for lme4.0 and lme4 fits m0parms <-
>>>> c(getME(m0,"theta"),fixef(m0)) m1parms <-
>>>> c(lme4::getME(m1,"theta"),fixef(m1))
>>>> nparms <- length(m0parms)
>>>> ntheta <- length(getME(m0,"theta"))
>>>>
>>>> par(las=1,bty="l")
>>>> plot(m0parms,m1parms,col=rep(2:1,c(ntheta,nparms-ntheta)))
>>>> abline(a=0,b=1)
>>>>
>>>> ## set up deviance function
>>>> dd <- lme4::glmer(...,devFunOnly=TRUE) ## compare
>>>> dd(m0parms)
>>>> dd(m1parms)
>>>>
>>>>   If the new deviance is lower than the old deviance, that's a 
>>>> strong indication that the new version is actually getting a 
>>>> _better_ fit than the old one.
>>>>
>>>>   Another diagnostic tool is:
>>>>
>>>> library(bbmle)
>>>> ss1 <- slice2D(fun = dd, params = m1parms, verbose = FALSE)
>>>> splom(ss1)
>>>>
>>>>   This will show you cross-sections of the deviance surface ...
>>>>
>>>>
>>>>> To Ben, Sorry that we cannot share data because of confidentiality 
>>>>> policy. But you are able to debug our problem through PC screen 
>>>>> sharing where you can gain control and run programs on our Linux 
>>>>> session without possessing the data. If that sounds like a good 
>>>>> idea, let us know when you might be available to do so.
>>>>
>>>>   Can you generate a simulated example that replicates your problem?
>>>> simulate() is a useful tool for generating data that are similar, 
>>>> but not identical to, your original data (you can also anonymize 
>>>> factor labels etc.).
>>>>
>>>>   PC screen sharing seems difficult -- I have easy access to MacOS 
>>>> and Linux systems, but not Windows.
>>>>
>>>>>
>>>>> Thanks, Qiong -----Original Message----- From: Ben Bolker 
>>>>> [mailto:bbolker at gmail.com] Sent: Tuesday, March 11, 2014 9:26 AM To:
>>>>> Martin Maechler; Yang, Qiong Cc: r-sig-mixed-models at r-project.org; 
>>>>> Chen, Ming-Huei Subject: Re: [R-sig-ME] gee, geese and glmer
>>>>>
>>>>> I would also point out that we are indeed very interested, in the 
>>>>> medium term (the short term is very very busy!), in making sure 
>>>>> that your issues are resolved.  We would like lme4 to dominate
>>>>> lme4.0 (i.e., to work better in all circumstances). So far it's 
>>>>> been a bit difficult since we have been debugging remotely -- 
>>>>> short of the suggestions I gave below, and without access to a 
>>>>> reproducible example, it's very hard indeed for me to say much more.
>>>>>
>>>>> sincerely Ben Bolker
>>>>>
>>>>> On 14-03-11 05:45 AM, Martin Maechler wrote:
>>>>>>>>>>> Yang, Qiong <qyang at bu.edu> on Mon, 10 Mar 2014 18:19:56
>>>>>>>>>>> +0000 writes:
>>>>>>
>>>>>>> Hi Ben, We wonder if you can add an option in lmer() of current
>>>>>>> lme4 version to call the algorithm used in lme4_0.999999-2?
>>>>>>
>>>>>> unfortunately not.
>>>>>>
>>>>>> For this reason, we had planned for many months, starting in 
>>>>>> August
>>>>>>  2012 and announced on this mailing list at least a year ago that 
>>>>>> we would provide the 'lme4.0' package  (back-compatible as well 
>>>>>> as possible in light of computer OS updates incl system 
>>>>>> libraries, and R updates, ...) in order to provide useRs the 
>>>>>> possibility of reproducible research and data analysis for their 
>>>>>> analyses done with "old-CRAN" lme4 versions.
>>>>>>
>>>>>> Unfortunately (for us), the CRAN maintainers decided that 
>>>>>> providing
>>>>>>  lme4.0 in addition to lme4 was a bad idea, and explicit forbid 
>>>>>> such actions in the (then new) CRAN policy document.  I'm still 
>>>>>> not at all happy with that decision.
>>>>>>
>>>>>>> For our package (analyze rare genetic variant) to be put on 
>>>>>>> CRAN, we need to use current version of lme4. However, at this 
>>>>>>> point, there are still issues that cannot be resolved with newer 
>>>>>>> versions of lme4. It is very difficult resolved with newer 
>>>>>>> versions
> of lme4.
>>>>>>> It is very difficult for us to keep waiting and testing the new 
>>>>>>> release, and hope all the issues resolved and no new issues 
>>>>>>> coming up. lme4_0.999999-2 has been used by us for a long time 
>>>>>>> with little problem.
>>>>>>
>>>>>> Good to hear.  Such cases were exactly the reason why we (lme4
>>>>>> authors) made such considerable effort to provide  lme4.0 for 
>>>>>> reproducible research and data analysis.
>>>>>>
>>>>>> at the bottom of
>>>>>> https://github.com/lme4/lme4/blob/master/README.md we mention the 
>>>>>> state and give installation instruction of lme4.0, but as you 
>>>>>> say, this does not solve the problem for other package 
>>>>>> maintainers: If they want a CRAN package, they (currently? I'm 
>>>>>> optimistic beyond reason :-) cannot have a 'Depends: lme4.0'  (or 
>>>>>> "Imports:..." or similar).
>>>>>>
>>>>>>> Your help on this is highly appreciated.  Best, Qiong
>>>>>>
>>>>>> You're welcome; currentl there's not more we can do. Martin
>>>>>>
>>>>>> -- Martin Maechler, ETH Zurich, Switzerland
>>>>>>
>>>>>>
>>>>>>
>>>>>>> -----Original Message----- From: Ben Bolker 
>>>>>>> [mailto:bbolker at gmail.com] Sent: Saturday, March 08, 2014 5:25 
>>>>>>> PM
>>>>>>> To: r-sig-mixed-models at r-project.org Cc: Chen, Ming-Huei; Yang, 
>>>>>>> Qiong Subject: Re: gee, geese and glmer
>>>>>>
>>>>>>> On 14-03-07 11:25 PM, Ming-Huei Chen wrote:
>>>>>>>> Hi Ben,
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> In an analysis we found that glmer in new lme4 gave result 
>>>>>>>> different from old lme4, gee and geese, where old lme4 seems to 
>>>>>>>> be closer to gee and geese.. Please see highlighted sex effect
> below.
>>>>>>>> Case by sex (2x2) table is also given. Can you please let us 
>>>>>>>> know how would you look into the results? Thanks!
>>>>>>>>
>>>>>>
>>>>>>> [cc'ing to r-sig-mixed-models: **please** try r-sig-mixed-models 
>>>>>>> first, not personal e-mail to me ...]
>>>>>>
>>>>>>> I can't say exactly what's going here; without having a 
>>>>>>> reproducible example <http://tinyurl.com/reproducible-000> it's 
>>>>>>> hard to say precisely.  Thoughts:
>>>>>>
>>>>>>> * gee and geese are giving _exactly_ the same parameter 
>>>>>>> estimates, to 8 significant digits, so I would guess they are 
>>>>>>> wrapping identical underlying methods.
>>>>>>
>>>>>>> * As far as diagnosing the issue with lme4 1.0-6: * does 
>>>>>>> changing the optimization method, i.e.
>>>>>>> glmerControl(optimizer="optimx",optCtrl=list(method="nlminb"))
>>>>>>> [must do library("optimx") first] or
>>>>>>> glmerControl(optimizer="bobyqa")
>>>>>>
>>>>>>> change the result?
>>>>>>
>>>>>>> * I would be curious whether the soon-to-be-released version
>>>>>>> 1.1-4 (which can be installed from github or
>>>>>>> lme4.r-forge.r-project.org/repos) gives either (1) convergence 
>>>>>>> warnings or (2) different/better answers
>>>>>>
>>>>>>> * You can try specifying the starting values for lme4 to 
>>>>>>> diagnose misconvergence; for example, start lme4 from the 
>>>>>>> estimates given by old lme4/lme4.0 and see if it gives a similar answer.
>>>>>>
>>>>>>> * You can use the 'slice' and 'splom.slice' functions from bbmle 
>>>>>>> to visualize the likelihood surfaces
>>>>>>
>>>>>>> good luck, Ben Bolker
>>>>>>
>>>>>>>> Ming-Huei
>>>>>>>>
>>>>>>
>>>>>>>> ###GEE
>>>>>>>>
>>>>>>>>> summary(gee(case~sex+PC1+PC2+PC3+PC4,id=famid,family=binomial,
>>>>>>>>> d
>>>>>>>>> a
>>>>>>>>> t
>>>>>>>>> a
>>>>>>>>> =da
>>>>>>
>>>>>>>>>
>>>>>>> ta))$coef
>>>>>>>> Estimate Naive S.E.  Naive z Robust S.E.  Robust z (Intercept)
>>>>>>>> -1.88047373 0.13532162 -13.8963286 0.15960440 -11.782092 sex
>>>>>>>> -0.23436854 0.08611269 -2.7216494 0.09050577 -2.589543 PC1
>>>>>>>> -0.05478639 0.06195318 -0.8843192 0.06822178 -0.803063 PC2
>>>>>>>> -0.09934572 0.06494563 -1.5296753 0.06520811 -1.523518 PC3
>>>>>>>> -0.07020391 0.06626875 -1.0593818 0.06962147 -1.008366 PC4
>>>>>>>> -0.13413097 0.06746716 -1.9880927 0.06979901 -1.921674
>>>>>>>>
>>>>>>
>>>>>>>> ###GEESE
>>>>>>>>
>>>>>>>>> summary(geese(case~sex+PC1+PC2+PC3+PC4,id=famid,family=binomia
>>>>>>>>> l
>>>>>>>>> ,
>>>>>>>>> d
>>>>>>>>> a
>>>>>>>>> ta=
>>>>>>
>>>>>>>>>
>>>>>>> data))$mean
>>>>>>>>
>>>>>>>> estimate san.se wald p
>>>>>>>>
>>>>>>>> (Intercept) -1.88047373 0.15960440 138.8176912 0.000000000 sex
>>>>>>>> -0.23436854 0.09050577 6.7057312 0.009610351 PC1 -0.05478639
>>>>>>>> 0.06822178 0.6449102 0.421938319 PC2 -0.09934572 0.06520811
>>>>>>>> 2.3211071 0.127629159 PC3 -0.07020391 0.06962147 1.0168016
>>>>>>>> 0.313278888 PC4 -0.13413097 0.06979901 3.6928324 0.054646745
>>>>>>>>
>>>>>>>> ### lme4_0.999999-2
>>>>>>>>
>>>>>>>>> summary(glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomi
>>>>>>>>> a
>>>>>>>>> l
>>>>>>>>> ,
>>>>>>>>> d
>>>>>>>>> ata
>>>>>>
>>>>>>>>>
>>>>>>> =data))
>>>>>>>> Estimate Std. Error z value Pr(>|z|) (Intercept) -3.01599
>>>>>>>> 0.28305 -10.655 <2e-16 *** sex -0.41056 0.16285 -2.521 0.0117 *
>>>>>>>> PC1 -0.17116 0.12903 -1.326 0.1847 PC2 -0.15510 0.13382 -1.159
>>>>>>>> 0.2465 PC3 -0.19044 0.13580 -1.402 0.1608 PC4 0.02532 0.13732
>>>>>>>> 0.184 0.8537
>>>>>>>>
>>>>>>>> ###lme4_1.0-6
>>>>>>>>
>>>>>>>>> summary(glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomi
>>>>>>>>> a
>>>>>>>>> l
>>>>>>>>> ,
>>>>>>>>> d
>>>>>>>>> ata
>>>>>>
>>>>>>>>>
>>>>>>> =data))
>>>>>>>>
>>>>>>>> Estimate Std. Error z value Pr(>|z|)
>>>>>>>>
>>>>>>>> (Intercept) -10.2784 0.8631 -11.909 <2e-16 *** sex 0.3497
>>>>>>>> 0.1975 1.770 0.0767 .  PC1 -0.3555 0.1623 -2.190 0.0285 * PC2
>>>>>>>> -0.1087 0.1653 -0.657 0.5109 PC3 -0.2242 0.1652 -1.357 0.1748
>>>>>>>> PC4 0.1103 0.1671 0.660 0.5091
>>>>>>>>
>>>>>>>> Case by sex
>>>>>>>>
>>>>>>>> 1 2 0 2554 3021 1 310 290
>>>>>>>>
>>>>>>
>>>>>>> _______________________________________________
>>>>>>> R-sig-mixed-models at r-project.org mailing list 
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>
>>>>>
>>>>
>


From David.Duffy at qimr.edu.au  Tue Mar 18 05:10:34 2014
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Tue, 18 Mar 2014 14:10:34 +1000
Subject: [R-sig-ME] Fwd: RE:  gee, geese and glmer
In-Reply-To: <5327AF5D.70704@gmail.com>
References: <6984CA5898097D459F8EC753F025B0D8396992C2@IST-EX10MBX-3.ad.bu.edu>
	<5327AF5D.70704@gmail.com>
Message-ID: <alpine.LMD.2.00.1403181350590.26263@orpheus.qimr.edu.au>

On Tue, 18 Mar 2014, Ben Bolker wrote:

>
> [forwarding a conversation about lme4/lme4.0 incompatibilities.  This
> example looks pretty interesting, as it seems hard to prove that lme4
> *isn't* giving the right answer/an answer that is numerically superior
> to lme4.0, yet the lme4.0 answer is biologically preferable/more similar
> to other estimation approaches.  I don't know yet if we will eventually
> find out that (1) the data are weird in a way that explains the
> difference; (2) lme4 is actually misconverging, preliminary evidence to
> the contrary; (3) ???  Enlightening comments are welcome.]

>> Just to clarify further: All the cases are singletons(i.e. families of 
>> 1 member) while controls are from extended families (i.e. families of
>> multiple members).

lme4 has got stuck. The setup as described is pretty pathological: a) sex 
is not usually correlated within families, and more significantly b) there 
are no families containing both cases and controls.  So, the variance for 
famid should be zero, and we should get the same answer as a binomial 
regression.

| David Duffy (MBBS PhD)
| email: David.Duffy at qimrberghofer.edu.au  ph: INT+61+7+3362-0217 fax: -0101
| Genetic Epidemiology, QIMR Berghofer Institute of Medical Research
| 300 Herston Rd, Brisbane, Queensland 4006, Australia  GPG 4D0B994A


From r.turner at auckland.ac.nz  Tue Mar 18 06:59:37 2014
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 18 Mar 2014 18:59:37 +1300
Subject: [R-sig-ME] Convergence failure with glmer().
Message-ID: <5327E0C9.8040903@auckland.ac.nz>



This is kind of a follow-up to a question that I asked a few days ago 
with the subject line "Bernoulli glmm question".

[The responses to my question more-or-less confirmed that I was 
basically on the right track with my model and [g]lmer() syntax.
The suggestion (in retrospect "obvious") was made that I should include 
a "word" random effect in my model.  It was also suggested that I should 
include some interaction terms.  However no result "came back" after 
over an hour of waiting, so I killed the job and gave up on this.]

However --- yesterday, after a few hiccups, I installed the latest 
version of lme4 (version 1.1-5) from CRAN.

Now the model that I successfully fitted with my rather elderly version 
(cannot remember the version number; I *think* I could dig it up from 
backups if it is of any interest) of lme4 results in a warning:

> Model failed to converge with max|grad| = 0.0922338 (tol = 0.001)

I tried switching the optimizer to "Nelder-Mead" and got an apparently 
"even worse" warning:

> Model failed to converge with max|grad| = 0.150396 (tol = 0.001)

For the record, my call was:

fit <- glmer(y ~ sex+type+(1|student)+1|word),family=binomial,data=X,
              control=glmerControl(optimizer="Nelder_Mead"))

This is just a warning, not an error, but it does seem to indicate that 
the fit obtained is not (quite?) correct.

Is there anything that I can *do* about this?  I've tried reading the 
help files, but failed to discern anything useful in this regard.  I 
would appreciate suggestions.

If anyone is sufficiently interested, I can make the data set available 
to them.  It's a wee bit on the large side, but; 10314 records.

Thanks for any words of wisdom.

cheers,

Rolf Turner


From j.hadfield at ed.ac.uk  Tue Mar 18 09:36:21 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Tue, 18 Mar 2014 08:36:21 +0000
Subject: [R-sig-ME] MCMCglmm update (2.18)
Message-ID: <20140318083621.82913v92tay77o8w@www.staffmail.ed.ac.uk>

Hi,

I have updated MCMCglmm to version 2.18 and the source is on CRAN now.  
I would advise users to upgrade: there are a lot of minor changes and  
bug fixes but the main changes are:

BUGS:

DIC with Gaussian/non-Gaussian mixtures

DIC (and the deviance) was incorrectly calculated in multivariate  
models with a mixture of Gaussian and non-Gaussian responses when  
residual correlations existed between the two types of response. I'm  
sorry that this bug has persisted so long - I don't use DIC because I  
think it is not focused at the right level for most scientific  
inference, particularly with non-Gaussian data as here. I vacillate  
between getting rid of it from MCMCglmm (because when I see it  
reported, its nearly always inappropriately) and retaining it (because  
sometimes it might be useful). If people have strong opinions let me  
know.

Gelman's Prior

The function gelman.prior generates a prior correlation matrix for the  
fixed effects had the predictors been transformed according to  
procedures outlined in Gelman et al's "A weakly informative default  
prior distribution for logistic and other regression models" and an  
i.i.d prior placed on the resulting coefficients. The documentation  
for the argument "scale" in gelman.prior says it is `the prior  
variance for regression parameters' (i.e. the prior covariance matrix  
is the prior correlation multiplied by this number). In version 2.17  
this should have read `the prior standard deviation' rather than  
`prior variance'. In version 2.18 it reads the `prior standard  
deviation' and this is now correct.


UPDATES:

Block Diagonal R-structures

Block Diagonal R-structures are now allowed. For example imagine a  
bivariate model where pairs of observations are made on individuals of  
different sex.  There may be a need to fit different 2x2 residual  
covariance matrices for the two sexes.   rcov=~us(trait:sex):units  
would fit a 4x4 covariance matrix, but the between-sex residual  
covariances would be estimated despite not being identifiable (no  
individual can be both sexes).  Now, models of the form  
rcov=~us(trait:at(sex, "M")):units+us(trait:at(sex, "F")):units can be  
fitted that allow the non-identified covariances to be effectively set  
to zero.

Probit Models

For categorical (ordered or not) data, MCMCglmm uses a redundant  
parameteristaion whereby the non-identified `residuals' are retained  
and their variance (usually) fixed in the prior.  For probit models it  
turns out that the redundant parameteristaion is not necessary and the  
MCMC algorithm will mix without it (as opposed to other models). In  
order to avoid confusion there is a new family called "threshold" (an  
old name for a probit model from genetics). It is identical to  
"ordinal" except the residual variance now refers to the variance  of  
the link function (set to one in a standard probit analysis) rather  
than the variance of the non-identified `residuals'.  Assuming R is  
fixed to one in the prior for both the "threshold" and "ordinal"  
models  then if the ordinal model location effects are divided by  
sqrt(2) and the variance components by 2 then they should be  
equivalent to those in the "threshold model". I have provided an  
example at the end of the email.

People may wonder what the point is given they are identical. There  
are three advantages.

1) Mixing is generally better because everything, including the latent  
variables, can be Gibbs sampled.
2) DIC can be focused on the linear predictor (i.e. Xb+Zu) rather than  
the latent variable (i.e. Xb+Zu+e) as with Gaussian responses (but  
unlike other non-Gaussian responses).
3) Residual correlations are less constrained in multivariate models.  
Imagine two binary response variables analysed in an ordinal model  
with rcov=~corg(trait):units, and have r as the estimated residual  
correlation. The estimated residual latent-scale correlation is   
r/sqrt(1+1)*sqrt(1+1) = r/2 and so the residual latent-scale  
correlation is constrained to lie between -0.5 and 0.5. Having  
family="threshold" the residual latent-scale correlation is r. Please  
note that if |r| is large then numerical problems can occur (it is  
equivalent to the extreme category problems sometimes seen in  
univariate models). This should be evident because the posterior for r  
will get trapped close to 1 or -1.

Cheers,

Jarrod

# Ordinal versus Threshold
set.seed(1000)
x<-rnorm(300)
id.e<-rnorm(150, 0, 1)
id<-gl(150,2)
y<-rbinom(300, 1, pnorm(0.5-x+id.e[id]))

# 150 individuals measured twice with link-scale repeatability of 0.5.
# intercept is 0.5, and the regression coefficient associated with x is -1

data<-data.frame(y=y, x=x, id=id)

prior<-list(R=list(V=1, fix=1), G=list(G1=list(V=1, nu=0)))

m1<-MCMCglmm(y~x, random=~id, family="threshold", data=data, prior=prior)
m2<-MCMCglmm(y~x, random=~id, family="ordinal", data=data, slice=TRUE,  
prior=prior)

plot(mcmc.list(m2$VCV[,1]/2, m1$VCV[,1]))
plot(mcmc.list(m2$Sol/sqrt(2), m1$Sol))

# note that the change of scale means the prior influence is different  
between the two models and so really V in the prior for G and B (the  
fixed effects) should be divided by two to make them exact.

# Alternatively:


prior.2<-list(R=list(V=2, fix=1), G=list(G1=list(V=1, nu=0)))

m3<-MCMCglmm(y~x, random=~id, family="threshold", data=data, prior=prior.2)


plot(mcmc.list(m2$VCV[,1], m3$VCV[,1]))
plot(mcmc.list(m2$Sol, m3$Sol))


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From bbolker at gmail.com  Tue Mar 18 13:51:00 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 18 Mar 2014 08:51:00 -0400
Subject: [R-sig-ME] Convergence failure with glmer().
In-Reply-To: <5327E0C9.8040903@auckland.ac.nz>
References: <5327E0C9.8040903@auckland.ac.nz>
Message-ID: <53284134.2050700@gmail.com>

On 14-03-18 01:59 AM, Rolf Turner wrote:
> 
> 
> This is kind of a follow-up to a question that I asked a few days ago
> with the subject line "Bernoulli glmm question".
> 
> [The responses to my question more-or-less confirmed that I was
> basically on the right track with my model and [g]lmer() syntax.
> The suggestion (in retrospect "obvious") was made that I should include
> a "word" random effect in my model.  It was also suggested that I should
> include some interaction terms.  However no result "came back" after
> over an hour of waiting, so I killed the job and gave up on this.]
> 
> However --- yesterday, after a few hiccups, I installed the latest
> version of lme4 (version 1.1-5) from CRAN.
> 
> Now the model that I successfully fitted with my rather elderly version
> (cannot remember the version number; I *think* I could dig it up from
> backups if it is of any interest) of lme4 results in a warning:
> 
>> Model failed to converge with max|grad| = 0.0922338 (tol = 0.001)
> 
> I tried switching the optimizer to "Nelder-Mead" and got an apparently
> "even worse" warning:
> 
>> Model failed to converge with max|grad| = 0.150396 (tol = 0.001)
> 
> For the record, my call was:
> 
> fit <- glmer(y ~ sex+type+(1|student)+1|word),family=binomial,data=X,
>              control=glmerControl(optimizer="Nelder_Mead"))
> 
> This is just a warning, not an error, but it does seem to indicate that
> the fit obtained is not (quite?) correct.
> 
> Is there anything that I can *do* about this?  I've tried reading the
> help files, but failed to discern anything useful in this regard.  I
> would appreciate suggestions.
> 
> If anyone is sufficiently interested, I can make the data set available
> to them.  It's a wee bit on the large side, but; 10314 records.
> 
> Thanks for any words of wisdom.

  My guess is that this is a false positive.  Some trouble-shooting
thoughts:

 * are the parameters you're getting similar to those you got before?
 * if you install lme4.0 (from http://lme4.r-forge.r-project.org/repos),
do you get similar parameters to those you got before?
 * if you extract the parameters from lme4.0, make a deviance function
from lme4 using devFunOnly, and compare, do you get similar or better
(lower) deviances for the new lme4-fit parameters?
 * is your fit singular, i.e. are any of the 'theta' parameters
corresponding to diagonal elements (I think in your case you only have
scalar random effects, so this would correspond to any of them) equal or
close to zero?  Do the components of the gradient that have large
absolute value (which(abs(fit at optinfo#derivs$grad)>0.002)) correspond to
the singular theta parameters?

  Ben Bolker


From bbolker at gmail.com  Tue Mar 18 15:06:15 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 18 Mar 2014 10:06:15 -0400
Subject: [R-sig-ME] gee, geese and glmer
In-Reply-To: <00d301cf4254$bd5fc510$381f4f30$@bu.edu>
References: <007801cf3a86$6d27b060$47771120$@bu.edu>	<531B98B4.4050403@gmail.com>	<6984CA5898097D459F8EC753F025B0D839696447@IST-EX10MBX-3.ad.bu.edu>
	<21278.56111.63015.103003@stat.math.ethz.ch>
	<531F0F03.5090907@gmail.com>
	<6984CA5898097D459F8EC753F025B0D8396982CD@IST-EX10MBX-3.ad.bu.edu>
	<53221072.9080203@gmail.com>
	<000301cf3f1f$f07fb0f0$d17f12d0$@bu.edu> <5322776E.90206@gmail.com>
	<005f01cf3f8a$d73cda80$85b68f80$@bu.edu>
	<53231B51.8070300@gmail.com>
	<011401cf3fb1$71a56620$54f03260$@bu.edu>
	<532375A4.5060703@gmail.com>
	<013201cf4218$90a933d0$b1fb9b70$@bu.edu>
	<5327522A.8090708@gmail.com>
	<6984CA5898097D459F8EC753F025B0D8396992C2@IST-EX10MBX-3.ad.bu.edu>
	<00d301cf4254$bd5fc510$381f4f30$@bu.edu>
Message-ID: <532852D7.5040405@gmail.com>


  This simplified version should help, I think.
  I'm still puzzled why lme4 1.1-5 is getting stuck with many different
optimizers, and why the slice plot looked sensible.  I would like to be
able to visualize the likelihood surface better.
  Have you gotten bbmle:::slicetrans to work yet?

On 14-03-17 10:49 PM, Ming-Huei Chen wrote:
> Qiong,
> 
> Controls are from extended families but NOT all the cases are singletons.
> Some cases are from extended families. Sorry I didn't make it clear. 
> 
> Best,
> 
> Ming-Huei
> 
> -----Original Message-----
> From: Yang, Qiong [mailto:qyang at bu.edu] 
> Sent: Monday, March 17, 2014 8:18 PM
> To: Ben Bolker; Chen, Ming-Huei
> Cc: 'lme4-authors at lists.r-forge.r-project.org'
> Subject: RE: [R-sig-ME] gee, geese and glmer
> 
> Hi Ben,
> Thanks for be mindful about our data confidentiality- what we communicated
> in this email sequence so far can be copied to the general mailing list.
> 
> Just to clarify further: All the cases are singletons(i.e. families of 1
> member) while controls are from extended families (i.e. families of multiple
> members). 
> 
> The predictor variables do not identify the cases from controls.
> We fitted reduce model where the only predictor is sex which does not
> identify cases from controls however still observed reversed sign on beta
> coefficients between lme4 and other software:
> First, let's look at crude case by sex table
>         1    2
>   0   2554 3021
>   1    310  290
> Ignoring family structure: Odds Ratio=0.79; beta=-0.23
> 
> Since all cases are singletons, I don't think including family-specific
> random effects should change the direction of sex effects. Therefore I tend
> to think the effect direction given by lme4 is not correct. Below are the
> results with only sex as predictor. 
>> library(lme4)
>> m1 <- glmer(case~sex+(1|famid),family=binomial,data=data)
>> m1 at optinfo$warnings
> list()
>> coef(summary(m1))
>                Estimate Std. Error    z value      Pr(>|z|)
> (Intercept) -10.2359223  0.4644516 -22.038725 1.225388e-107
> sex           0.3531757  0.1837722   1.921813  5.462931e-02
>> library(lme4.0)
>> m0 <- lme4.0::glmer(case~sex+(1|famid),family=binomial,data=data)
> Warning message:
> In mer_finalize(ans) : false convergence (8)
>> coef(summary(m0))
>               Estimate Std. Error    z value     Pr(>|z|)
> (Intercept) -3.2807992  0.2868701 -11.436533 2.746424e-30
> sex         -0.2592505  0.1629382  -1.591097 1.115877e-01
> 
>> library(geepack)
>> summary(geese(case~sex,id=famid,family=binomial,data=data))$mean
>               estimate     san.se      wald           p
> (Intercept) -1.8742252 0.16010676 137.03271 0.000000000
> sex         -0.2346185 0.09068416   6.69363 0.009675797
>> library(gee)
>> summary(gee(case~sex,id=famid,family=binomial,data=data))$coef
>               Estimate Naive S.E.    Naive z Robust S.E.   Robust z
> (Intercept) -1.8742252 0.13510959 -13.871889  0.16010676 -11.706097
> sex         -0.2346185 0.08601766  -2.727562  0.09068416  -2.587205
> 
> Thanks a lot, Qiong
> 
> -----Original Message-----
> From: Ben Bolker [mailto:bbolker at gmail.com] 
> Sent: Monday, March 17, 2014 3:51 PM
> To: Chen, Ming-Huei; Yang, Qiong
> Cc: 'lme4-authors at lists.r-forge.r-project.org'
> Subject: Re: [R-sig-ME] gee, geese and glmer
> 
> 
>   [I'm not going to cc: to r-sig-mixed-models, as you may consider parts of
> it to be sensitive, but I would like to ask permission to do so in the
> future, as I think it would be useful to have these conversations in public
> -- please let me know]
> 
> 
> On 14-03-17 03:39 PM, Ming-Huei Chen wrote:
>> Thanks, Ben!
>>
>> Our data was combined by two cohorts, one of them contains only cases 
>> and is an unrelated sample (249 male, 182 female). Do you think this 
>> may lead to what we observed here?
> 
>   Possibly, although I would think the cohorts would have to be identifiable
> somehow from the predictor variables.
> 
>>
>> * I don't know why I got an error...
>> Error: could not find function "slicetrans"
> 
>   slicetrans is a function from the bbmle package, but it turns out to be
> un-exported (this is still rather experimental functionality), so you would
> need to use bbmle:::slicetrans
> 
>>
>> * Do you mean using nAGQ? If so, I used nAGQ=5 an dgot a warning.
>>> x <-
>> summary(lme4::glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial
>> ,data=
>> test.dat,nAGQ=5))
>> Warning message:
>> In checkConv(attr(opt, "derivs"), opt$par, checkCtrl = control$checkConv,
> :
>>   Model failed to converge with max|grad| = 0.00275074 (tol = 0.001) 
>> Fixed effects:
>>              Estimate Std. Error z value Pr(>|z|)
>> (Intercept) -2.675364   0.282841  -9.459   <2e-16 ***
>> sex         -0.053593   0.146444  -0.366   0.7144
>> PC1         -0.205982   0.114760  -1.795   0.0727 .
>> PC2         -0.161422   0.119219  -1.354   0.1757
>> PC3         -0.191577   0.121107  -1.582   0.1137
>> PC4          0.002723   0.122446   0.022   0.9823
> 
>   This warning *might* be spurious (the lme4 maintainers have been having a
> private conversation about the fact that one may get false-positive warnings
> about gradients when the fit is singular):
> 
>>
>> * Do you mean use fixef.prior? fixef.prior: a BLME prior of family 
>> normal, t, or NULL. So which one is weaker prior?
> 
>    t is weaker than Normal (i.e., for the same mean (typically 0) and
> variance it has fatter tails).  However, there are two dimensions of
> "weakness" -- to make a prior weaker you can make the scale (i.e.
> variance) larger, and/or make the shape parameter more diffuse (i.e., make
> the tails fatter, in the sequence Gaussian (= t_\infty -> t_{large n} ->
> t_{smaller n}).  (You may end up discovering that the results depend on the
> strength of the prior ... which will open up interesting cans of worms in
> terms of what the data are actually telling you ...)
>>
>> Best,
>>
>> Ming-Huei
>>
>> -----Original Message-----
>> From: Ben Bolker [mailto:bbolker at gmail.com]
>> Sent: Friday, March 14, 2014 5:33 PM
>> To: Ming-Huei Chen; 'Yang, Qiong'
>> Cc: 'lme4-authors at lists.r-forge.r-project.org'
>> Subject: Re: [R-sig-ME] gee, geese and glmer
>>
>>   OK, this definitely looks well-behaved.  I am leaning still more 
>> heavily toward the idea that new-lme4 is giving an answer that is 
>> technically more correct (the splom plot shows the focal parameters 
>> and the location of the minimum computed 'slice' value as closed and 
>> open circles (I forget which is which); if they don't coincide, that 
>> suggests a misconvergence.  There are a few cases where there is a 
>> barely visible difference (i.e. you can see the open circle peeking 
>> out from around the edge of the closed circle), but they're generally 
>> very close.  The deviance surface looks sensibly smooth/ellipsoidal, and
> none of the parameters appear to be on boundary.
>> (There's a little problem with the slice2D code that limits the ranges 
>> sampled to be either all-positive or all-negative, I think.)
>>
>>    If so, that makes the difference between GEE, Laplace 
>> approximation, and Bayesian GLMM as much of a statistical as a
> computational problem.
>> A few more things I would try to follow up:
>>
>>   * try s2 <- slicetrans(m0, m1, dd); plot(s2) to examine the 
>> cross-section of the deviance function between the new and old parameters
>>   * try using Gauss-Hermite quadrature to see if that makes a difference
>>   * try blme::bglmer to impose a weak prior on the fixed effects (see 
>> if that makes the results closer to MCMCglmm)
>>
>>
>>
>> On 14-03-14 02:15 PM, Ming-Huei Chen wrote:
>>> Here it is.... 
>>>
>>> ###new
>>>> library(lme4)
>>>> m1 <-
>>> glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data=test.da
>>> t
>>> )
>>>> m1 at optinfo$warnings
>>> list()
>>>> library(optimx)
>>> Loading required package: numDeriv
>>>> m1.1 <-
>>> glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data=test.da
>>> t
>>> ,contr
>>> ol=glmerControl(optimizer="optimx",optCtrl=list(method="nlminb")))
>>>> m1.1 at optinfo$warnings
>>> list()
>>>
>>> ###old
>>>> library(lme4.0)
>>>> m0 <-
>>> glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data=test.da
>>> t
>>> )
>>> Warning message:
>>> In mer_finalize(ans) : false convergence (8)
>>>> library(optimx)
>>> Loading required package: numDeriv
>>>> m0.0 <-
>>> glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data=test.da
>>> t
>>> ,contr
>>> ol=glmerControl(optimizer="optimx",optCtrl=list(method="nlminb")))
>>> Error in do.call(lmerControl, control) :
>>>   could not find function "glmerControl"
>>>
>>> Best,
>>>
>>> Ming-Huei
>>> -----Original Message-----
>>> From: Ben Bolker [mailto:bbolker at gmail.com]
>>> Sent: Friday, March 14, 2014 11:08 AM
>>> To: Ming-Huei Chen; 'Yang, Qiong'
>>> Cc: lme4-authors at lists.r-forge.r-project.org
>>> Subject: Re: [R-sig-ME] gee, geese and glmer
>>>
>>>   The fact that the new deviance is lower suggests that the new 
>>> version of
>>> lme4 is actually finding a _better_ fit to the data; so does the 
>>> existence of a convergence warning from old lme4 (it's interesting 
>>> that using
>>> control=glmerControl(optimizer="optimx",optCtrl=list(method="nlminb")
>>> ) doesn't give you a similar warning ...) . There is still some minor 
>>> possibility that there's a glitch in the deviance calculation -- a 
>>> good cross-check would be to evaluate the deviance of both sets of 
>>> parameters for old lme4 as well, but that's a bit harder to do 
>>> because
>>> old-lme4 doesn't have the same level of flexibility).
>>>
>>>   The parameter comparisons that new-lme4 is giving fairly extreme 
>>> estimates for one of the fixed parameters (old=-3, new=-10; looking 
>>> at the parameter tables below, it looks like this is the intercept) 
>>> and a very large estimate of the random-effects standard deviation. 
>>> (I could have figured this out from the parameter tables too, but I'm 
>>> lazy and graphical presentation makes it much more obvious.)
>>>
>>>   This is looking like a case of (quasi-)complete separation, which 
>>> would explain why MCMCglmm is giving a much less extreme result (it 
>>> imposes weak priors on all of the parameters by default).  It would 
>>> be interesting to try blme::bglmer on this case as well.
>>>
>>> Can you send the (graphical) results of
>>>
>>> dd <- lme4::glmer(...,devFunOnly=TRUE)
>>> library(bbmle)
>>> ss1 <- slice2D(fun = dd, params = m1parms, verbose = FALSE)
>>> splom(ss1)
>>>
>>> ??
>>>
>>> On 14-03-14 09:39 AM, Ming-Huei Chen wrote:
>>>> Thanks, Ben!
>>>>
>>>> The new deviance is lower than the old one. The plot of estimates is 
>>>> also attached.
>>>>> dd(m0parms)
>>>> [1] 3301.035
>>>>> dd(m1parms)
>>>> [1] 2991.881
>>>>
>>>> Best,
>>>>
>>>> Ming-Huei
>>>> -----Original Message-----
>>>> From: Ben Bolker [mailto:bbolker at gmail.com]
>>>> Sent: Thursday, March 13, 2014 11:29 PM
>>>> To: Ming-Huei Chen; 'Yang, Qiong'; 'Martin Maechler'
>>>> Cc: r-sig-mixed-models at r-project.org
>>>> Subject: Re: [R-sig-ME] gee, geese and glmer
>>>>
>>>>   You should be able to install lme4.0 via:
>>>>
>>>> install.packages("lme4.0",
>>>>     repos="http://lme4.r-forge.r-project.org/repos")
>>>>
>>>> It should give identical results to lme4_0.999999-2.
>>>>
>>>>   Also, if you can pull out the coefficients (both the theta and the 
>>>> fixed effect parameters) and save them somewhere, you don't need to 
>>>> run old and new lme4 in the same session.  You just need to be able 
>>>> to pull both sets of parameters into an R session where you have 
>>>> lme4
>>>>> =
>>>> 1.0 loaded, use devFunOnly=TRUE to extract the deviance function, 
>>>> and apply it to both the old and the new sets of parameters.
>>>>
>>>>   Ben Bolker
>>>>
>>>>
>>>> On 14-03-13 08:54 PM, Ming-Huei Chen wrote:
>>>>> Thanks Ben and Martin!
>>>>>
>>>>> I tried lme4 1.1-4 and the result was identical to lme4 1.0-6 
>>>>> without warning (by @optinfo$warnings). As you mentioned about 
>>>>> convergence warning, I redid the analysis with lme4_0.999999-2 and 
>>>>> saw a convergence warning. I also tried MCMCglmm and here is the 
>>>>> result, sex effect is insignificant but the estimate is negative as 
>>>>> in GEE and lme4_0.999999-2. But as you mentioned, it's different
>> methodology..
>>>>>
>>>>> test <-
>>>>> MCMCglmm(case~sex+PC1+PC2+PC3+PC4,random=~famid,family="categorical"
>>>>> ,
>>>>> d
>>>>> ata=da
>>>>> ta)
>>>>>             post.mean  l-95% CI  u-95% CI eff.samp  pMCMC
>>>>> (Intercept) -2.469391 -2.821757 -2.078372    4.290 <0.001 ***
>>>>> sex         -0.055439 -0.330237  0.111188    5.426  0.626
>>>>> PC1         -0.197212 -0.332290 -0.018821    6.833  0.074 .
>>>>> PC2         -0.158433 -0.364921 -0.003554    5.142  0.002 **
>>>>> PC3         -0.130129 -0.320582  0.065973    4.094  0.320
>>>>> PC4          0.002677 -0.306265  0.233621    2.060  0.812
>>>>>
>>>>> As for comparing deviance, because different versions of lme4 need 
>>>>> to be installed in different versions of R and there is actually no 
>>>>> lme4.0, so I don't know how to use two versions of lme4 in the same 
>>>>> R session... Since you planned to get lme4.0 on CRAN, so you should 
>>>>> have the package... in that case, can you please send it to me? Or?
>>>>>
>>>>> Our data contain pedigree information which cannot be shared (even 
>>>>> pedigree structure), so simulate may not help.
>>>>>
>>>>> Best,
>>>>>
>>>>> Ming-Huei
>>>>>
>>>>> -----Original Message-----
>>>>> From: Ben Bolker [mailto:bbolker at gmail.com]
>>>>> Sent: Thursday, March 13, 2014 4:09 PM
>>>>> To: Yang, Qiong; Martin Maechler
>>>>> Cc: r-sig-mixed-models at r-project.org; Chen, Ming-Huei
>>>>> Subject: Re: [R-sig-ME] gee, geese and glmer
>>>>>
>>>>> On 14-03-13 02:01 PM, Yang, Qiong wrote:
>>>>>> Hi Ben and Martin,
>>>>>>
>>>>>> Thanks you for your reply and taking time to explain the situation. 
>>>>>> Maybe I wasn't clear in my previous message or don't have a deeper 
>>>>>> understanding of the CRAN policy: Instead of separate packages for 
>>>>>> "old-CRAN" lme4 (lme4.0), and the current lme4, which is not 
>>>>>> allowed by CRAN maintainer, is it possible to have an option in 
>>>>>> current lme4 to call the algorithm in old lme4? I thought the new
>>>>>> lme4 implements a different algorithm and why not make the old 
>>>>>> algorithm available in current lme4 lmer() as an option. It is 
>>>>>> like we can request pearson or spearman correlations in cor(). 
>>>>>> Sorry if I simplified the problem too much.
>>>>>
>>>>>    It's not so much CRAN policy as the architecture of the software.  
>>>>> A great deal of the internal structure has changed between versions
>>>>> <1.0 and > 1.0, so putting both versions into one package would 
>>>>> basically mean having copies of all of the old and all of the new 
>>>>> code
>>>>> -- and keeping it all safely distinguished and separate would 
>>>>> probably be substantially more work (we have about as much as we 
>>>>> can to do keep up with a single package ...) As we said before, we 
>>>>> were surprised by the rejection of lme4.0 by CRAN -- our previous 
>>>>> planning had assumed that we would be able to put lme4.0 on CRAN.
>>>>> Somewhat to our surprise, there have been relatively few people 
>>>>> contacting us with troubles similar to yours -- for the most part 
>>>>> it seems that is
>>>>> (thankfully) very rare that
>>>>> lme4 gives significantly worse answers than lme4.0.
>>>>>
>>>>>   So far it's not clear that lme4's answers are actually worse than 
>>>>> lme4.0's (I admit that all other things being equal, closer to GEE 
>>>>> is more likely to be correct -- on the other hand, we know that GEE 
>>>>> gives marginal estimates of fixed parameters while GLMM gives 
>>>>> conditional estimates, so we shouldn't expect them to be the same).
>>>>> Can
>>> I confirm:
>>>>>
>>>>> * you've tried this with the most recent development version of 
>>>>> lme4
>>>>> (1.1-4) and you do *not* get any convergence warnings?
>>>>>
>>>>> * have you compared the deviances based on the old (lme4.0 / lme4 <
>>>>> 1.0) and new packages?  Here is the outline of how you would do this:
>>>>>
>>>>> library(lme4)   ## load new package
>>>>> m1 <- glmer(...) ## lme4 fit
>>>>> library(lme4.0)  ## load old package
>>>>> m0 <- lme4.0::glmer(...) ## lme4.0 fit ## extract full parameter 
>>>>> set for lme4.0 and lme4 fits m0parms <-
>>>>> c(getME(m0,"theta"),fixef(m0)) m1parms <-
>>>>> c(lme4::getME(m1,"theta"),fixef(m1))
>>>>> nparms <- length(m0parms)
>>>>> ntheta <- length(getME(m0,"theta"))
>>>>>
>>>>> par(las=1,bty="l")
>>>>> plot(m0parms,m1parms,col=rep(2:1,c(ntheta,nparms-ntheta)))
>>>>> abline(a=0,b=1)
>>>>>
>>>>> ## set up deviance function
>>>>> dd <- lme4::glmer(...,devFunOnly=TRUE) ## compare
>>>>> dd(m0parms)
>>>>> dd(m1parms)
>>>>>
>>>>>   If the new deviance is lower than the old deviance, that's a 
>>>>> strong indication that the new version is actually getting a 
>>>>> _better_ fit than the old one.
>>>>>
>>>>>   Another diagnostic tool is:
>>>>>
>>>>> library(bbmle)
>>>>> ss1 <- slice2D(fun = dd, params = m1parms, verbose = FALSE)
>>>>> splom(ss1)
>>>>>
>>>>>   This will show you cross-sections of the deviance surface ...
>>>>>
>>>>>
>>>>>> To Ben, Sorry that we cannot share data because of confidentiality 
>>>>>> policy. But you are able to debug our problem through PC screen 
>>>>>> sharing where you can gain control and run programs on our Linux 
>>>>>> session without possessing the data. If that sounds like a good 
>>>>>> idea, let us know when you might be available to do so.
>>>>>
>>>>>   Can you generate a simulated example that replicates your problem?
>>>>> simulate() is a useful tool for generating data that are similar, 
>>>>> but not identical to, your original data (you can also anonymize 
>>>>> factor labels etc.).
>>>>>
>>>>>   PC screen sharing seems difficult -- I have easy access to MacOS 
>>>>> and Linux systems, but not Windows.
>>>>>
>>>>>>
>>>>>> Thanks, Qiong -----Original Message----- From: Ben Bolker 
>>>>>> [mailto:bbolker at gmail.com] Sent: Tuesday, March 11, 2014 9:26 AM To:
>>>>>> Martin Maechler; Yang, Qiong Cc: r-sig-mixed-models at r-project.org; 
>>>>>> Chen, Ming-Huei Subject: Re: [R-sig-ME] gee, geese and glmer
>>>>>>
>>>>>> I would also point out that we are indeed very interested, in the 
>>>>>> medium term (the short term is very very busy!), in making sure 
>>>>>> that your issues are resolved.  We would like lme4 to dominate
>>>>>> lme4.0 (i.e., to work better in all circumstances). So far it's 
>>>>>> been a bit difficult since we have been debugging remotely -- 
>>>>>> short of the suggestions I gave below, and without access to a 
>>>>>> reproducible example, it's very hard indeed for me to say much more.
>>>>>>
>>>>>> sincerely Ben Bolker
>>>>>>
>>>>>> On 14-03-11 05:45 AM, Martin Maechler wrote:
>>>>>>>>>>>> Yang, Qiong <qyang at bu.edu> on Mon, 10 Mar 2014 18:19:56
>>>>>>>>>>>> +0000 writes:
>>>>>>>
>>>>>>>> Hi Ben, We wonder if you can add an option in lmer() of current
>>>>>>>> lme4 version to call the algorithm used in lme4_0.999999-2?
>>>>>>>
>>>>>>> unfortunately not.
>>>>>>>
>>>>>>> For this reason, we had planned for many months, starting in 
>>>>>>> August
>>>>>>>  2012 and announced on this mailing list at least a year ago that 
>>>>>>> we would provide the 'lme4.0' package  (back-compatible as well 
>>>>>>> as possible in light of computer OS updates incl system 
>>>>>>> libraries, and R updates, ...) in order to provide useRs the 
>>>>>>> possibility of reproducible research and data analysis for their 
>>>>>>> analyses done with "old-CRAN" lme4 versions.
>>>>>>>
>>>>>>> Unfortunately (for us), the CRAN maintainers decided that 
>>>>>>> providing
>>>>>>>  lme4.0 in addition to lme4 was a bad idea, and explicit forbid 
>>>>>>> such actions in the (then new) CRAN policy document.  I'm still 
>>>>>>> not at all happy with that decision.
>>>>>>>
>>>>>>>> For our package (analyze rare genetic variant) to be put on 
>>>>>>>> CRAN, we need to use current version of lme4. However, at this 
>>>>>>>> point, there are still issues that cannot be resolved with newer 
>>>>>>>> versions of lme4. It is very difficult resolved with newer 
>>>>>>>> versions
>> of lme4.
>>>>>>>> It is very difficult for us to keep waiting and testing the new 
>>>>>>>> release, and hope all the issues resolved and no new issues 
>>>>>>>> coming up. lme4_0.999999-2 has been used by us for a long time 
>>>>>>>> with little problem.
>>>>>>>
>>>>>>> Good to hear.  Such cases were exactly the reason why we (lme4
>>>>>>> authors) made such considerable effort to provide  lme4.0 for 
>>>>>>> reproducible research and data analysis.
>>>>>>>
>>>>>>> at the bottom of
>>>>>>> https://github.com/lme4/lme4/blob/master/README.md we mention the 
>>>>>>> state and give installation instruction of lme4.0, but as you 
>>>>>>> say, this does not solve the problem for other package 
>>>>>>> maintainers: If they want a CRAN package, they (currently? I'm 
>>>>>>> optimistic beyond reason :-) cannot have a 'Depends: lme4.0'  (or 
>>>>>>> "Imports:..." or similar).
>>>>>>>
>>>>>>>> Your help on this is highly appreciated.  Best, Qiong
>>>>>>>
>>>>>>> You're welcome; currentl there's not more we can do. Martin
>>>>>>>
>>>>>>> -- Martin Maechler, ETH Zurich, Switzerland
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>> -----Original Message----- From: Ben Bolker 
>>>>>>>> [mailto:bbolker at gmail.com] Sent: Saturday, March 08, 2014 5:25 
>>>>>>>> PM
>>>>>>>> To: r-sig-mixed-models at r-project.org Cc: Chen, Ming-Huei; Yang, 
>>>>>>>> Qiong Subject: Re: gee, geese and glmer
>>>>>>>
>>>>>>>> On 14-03-07 11:25 PM, Ming-Huei Chen wrote:
>>>>>>>>> Hi Ben,
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> In an analysis we found that glmer in new lme4 gave result 
>>>>>>>>> different from old lme4, gee and geese, where old lme4 seems to 
>>>>>>>>> be closer to gee and geese.. Please see highlighted sex effect
>> below.
>>>>>>>>> Case by sex (2x2) table is also given. Can you please let us 
>>>>>>>>> know how would you look into the results? Thanks!
>>>>>>>>>
>>>>>>>
>>>>>>>> [cc'ing to r-sig-mixed-models: **please** try r-sig-mixed-models 
>>>>>>>> first, not personal e-mail to me ...]
>>>>>>>
>>>>>>>> I can't say exactly what's going here; without having a 
>>>>>>>> reproducible example <http://tinyurl.com/reproducible-000> it's 
>>>>>>>> hard to say precisely.  Thoughts:
>>>>>>>
>>>>>>>> * gee and geese are giving _exactly_ the same parameter 
>>>>>>>> estimates, to 8 significant digits, so I would guess they are 
>>>>>>>> wrapping identical underlying methods.
>>>>>>>
>>>>>>>> * As far as diagnosing the issue with lme4 1.0-6: * does 
>>>>>>>> changing the optimization method, i.e.
>>>>>>>> glmerControl(optimizer="optimx",optCtrl=list(method="nlminb"))
>>>>>>>> [must do library("optimx") first] or
>>>>>>>> glmerControl(optimizer="bobyqa")
>>>>>>>
>>>>>>>> change the result?
>>>>>>>
>>>>>>>> * I would be curious whether the soon-to-be-released version
>>>>>>>> 1.1-4 (which can be installed from github or
>>>>>>>> lme4.r-forge.r-project.org/repos) gives either (1) convergence 
>>>>>>>> warnings or (2) different/better answers
>>>>>>>
>>>>>>>> * You can try specifying the starting values for lme4 to 
>>>>>>>> diagnose misconvergence; for example, start lme4 from the 
>>>>>>>> estimates given by old lme4/lme4.0 and see if it gives a similar
> answer.
>>>>>>>
>>>>>>>> * You can use the 'slice' and 'splom.slice' functions from bbmle 
>>>>>>>> to visualize the likelihood surfaces
>>>>>>>
>>>>>>>> good luck, Ben Bolker
>>>>>>>
>>>>>>>>> Ming-Huei
>>>>>>>>>
>>>>>>>
>>>>>>>>> ###GEE
>>>>>>>>>
>>>>>>>>>> summary(gee(case~sex+PC1+PC2+PC3+PC4,id=famid,family=binomial,
>>>>>>>>>> d
>>>>>>>>>> a
>>>>>>>>>> t
>>>>>>>>>> a
>>>>>>>>>> =da
>>>>>>>
>>>>>>>>>>
>>>>>>>> ta))$coef
>>>>>>>>> Estimate Naive S.E.  Naive z Robust S.E.  Robust z (Intercept)
>>>>>>>>> -1.88047373 0.13532162 -13.8963286 0.15960440 -11.782092 sex
>>>>>>>>> -0.23436854 0.08611269 -2.7216494 0.09050577 -2.589543 PC1
>>>>>>>>> -0.05478639 0.06195318 -0.8843192 0.06822178 -0.803063 PC2
>>>>>>>>> -0.09934572 0.06494563 -1.5296753 0.06520811 -1.523518 PC3
>>>>>>>>> -0.07020391 0.06626875 -1.0593818 0.06962147 -1.008366 PC4
>>>>>>>>> -0.13413097 0.06746716 -1.9880927 0.06979901 -1.921674
>>>>>>>>>
>>>>>>>
>>>>>>>>> ###GEESE
>>>>>>>>>
>>>>>>>>>> summary(geese(case~sex+PC1+PC2+PC3+PC4,id=famid,family=binomia
>>>>>>>>>> l
>>>>>>>>>> ,
>>>>>>>>>> d
>>>>>>>>>> a
>>>>>>>>>> ta=
>>>>>>>
>>>>>>>>>>
>>>>>>>> data))$mean
>>>>>>>>>
>>>>>>>>> estimate san.se wald p
>>>>>>>>>
>>>>>>>>> (Intercept) -1.88047373 0.15960440 138.8176912 0.000000000 sex
>>>>>>>>> -0.23436854 0.09050577 6.7057312 0.009610351 PC1 -0.05478639
>>>>>>>>> 0.06822178 0.6449102 0.421938319 PC2 -0.09934572 0.06520811
>>>>>>>>> 2.3211071 0.127629159 PC3 -0.07020391 0.06962147 1.0168016
>>>>>>>>> 0.313278888 PC4 -0.13413097 0.06979901 3.6928324 0.054646745
>>>>>>>>>
>>>>>>>>> ### lme4_0.999999-2
>>>>>>>>>
>>>>>>>>>> summary(glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomi
>>>>>>>>>> a
>>>>>>>>>> l
>>>>>>>>>> ,
>>>>>>>>>> d
>>>>>>>>>> ata
>>>>>>>
>>>>>>>>>>
>>>>>>>> =data))
>>>>>>>>> Estimate Std. Error z value Pr(>|z|) (Intercept) -3.01599
>>>>>>>>> 0.28305 -10.655 <2e-16 *** sex -0.41056 0.16285 -2.521 0.0117 *
>>>>>>>>> PC1 -0.17116 0.12903 -1.326 0.1847 PC2 -0.15510 0.13382 -1.159
>>>>>>>>> 0.2465 PC3 -0.19044 0.13580 -1.402 0.1608 PC4 0.02532 0.13732
>>>>>>>>> 0.184 0.8537
>>>>>>>>>
>>>>>>>>> ###lme4_1.0-6
>>>>>>>>>
>>>>>>>>>> summary(glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomi
>>>>>>>>>> a
>>>>>>>>>> l
>>>>>>>>>> ,
>>>>>>>>>> d
>>>>>>>>>> ata
>>>>>>>
>>>>>>>>>>
>>>>>>>> =data))
>>>>>>>>>
>>>>>>>>> Estimate Std. Error z value Pr(>|z|)
>>>>>>>>>
>>>>>>>>> (Intercept) -10.2784 0.8631 -11.909 <2e-16 *** sex 0.3497
>>>>>>>>> 0.1975 1.770 0.0767 .  PC1 -0.3555 0.1623 -2.190 0.0285 * PC2
>>>>>>>>> -0.1087 0.1653 -0.657 0.5109 PC3 -0.2242 0.1652 -1.357 0.1748
>>>>>>>>> PC4 0.1103 0.1671 0.660 0.5091
>>>>>>>>>
>>>>>>>>> Case by sex
>>>>>>>>>
>>>>>>>>> 1 2 0 2554 3021 1 310 290
>>>>>>>>>
>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> R-sig-mixed-models at r-project.org mailing list 
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>>
>>>>>>
>>>>>
>>
> 
>


From bbolker at gmail.com  Tue Mar 18 15:30:24 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 18 Mar 2014 10:30:24 -0400
Subject: [R-sig-ME] gee, geese and glmer
In-Reply-To: <00c901cf4254$62359ac0$26a0d040$@bu.edu>
References: <007801cf3a86$6d27b060$47771120$@bu.edu>	<531B98B4.4050403@gmail.com>	<6984CA5898097D459F8EC753F025B0D839696447@IST-EX10MBX-3.ad.bu.edu>
	<21278.56111.63015.103003@stat.math.ethz.ch>
	<531F0F03.5090907@gmail.com>
	<6984CA5898097D459F8EC753F025B0D8396982CD@IST-EX10MBX-3.ad.bu.edu>
	<53221072.9080203@gmail.com>
	<000301cf3f1f$f07fb0f0$d17f12d0$@bu.edu> <5322776E.90206@gmail.com>
	<005f01cf3f8a$d73cda80$85b68f80$@bu.edu>
	<53231B51.8070300@gmail.com>
	<011401cf3fb1$71a56620$54f03260$@bu.edu>
	<532375A4.5060703@gmail.com>
	<013201cf4218$90a933d0$b1fb9b70$@bu.edu>
	<5327522A.8090708@gmail.com>
	<00c901cf4254$62359ac0$26a0d040$@bu.edu>
Message-ID: <53285880.2030502@gmail.com>

On 14-03-17 10:47 PM, Ming-Huei Chen wrote:
> Thanks Ben!
> 
> S2 plot is attached. 

   Despite David Duffy's comments (which make sense) I still don't see
any internal evidence that lme4 is getting stuck at a local minimum --
the S2 plot is consistent with the new lme4 fit being (much) better.
*If* all the data were singletons, then the among-family variation would
be an observation-level random effect, which would be (as previously
commented frequently on this list) unidentifiable, but if they're not
all singletons ...

  (You've said previously you have pedigree data in this analysis -- I'm
a little unclear as to how that would work, since 'vanilla' lme4 doesn't
support pedigree structures ...)

  It probably makes sense to move forward with the sex+(1|famid) model
for the purpose of diagnosis (simpler, and appears to show the same issue).

  I thought I had a reproducible example of this phenomenon, but it
turned out it was just an odd simulation case.
> 
> I got an error using blme...

 packageVersion("blme") ?

This works for me:

library(lme4)
library(blme)

bglmer(cbind(incidence,size-incidence) ~ period + (1|herd),
    family=binomial, data=cbpp, fixef.prior=t)

> 
>>
> blme::bglmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data=test.da
> t,fixef.prior=t)
> Error in get("checkConv", lme4Env)(attr(opt, "derivs"), opt$par, ctrl =
> control$checkConv,  :
>   unused argument (ctrl = control$checkConv)
> 
> Best,
> 
> Ming-Huei
> 
> -----Original Message-----
> From: Ben Bolker [mailto:bbolker at gmail.com] 
> Sent: Monday, March 17, 2014 3:51 PM
> To: Ming-Huei Chen; 'Yang, Qiong'
> Cc: 'lme4-authors at lists.r-forge.r-project.org'
> Subject: Re: [R-sig-ME] gee, geese and glmer
> 
> 
>   [I'm not going to cc: to r-sig-mixed-models, as you may consider parts of
> it to be sensitive, but I would like to ask permission to do so in the
> future, as I think it would be useful to have these conversations in public
> -- please let me know]
> 
> 
> On 14-03-17 03:39 PM, Ming-Huei Chen wrote:
>> Thanks, Ben!
>>
>> Our data was combined by two cohorts, one of them contains only cases 
>> and is an unrelated sample (249 male, 182 female). Do you think this 
>> may lead to what we observed here?
> 
>   Possibly, although I would think the cohorts would have to be identifiable
> somehow from the predictor variables.
> 
>>
>> * I don't know why I got an error...
>> Error: could not find function "slicetrans"
> 
>   slicetrans is a function from the bbmle package, but it turns out to be
> un-exported (this is still rather experimental functionality), so you would
> need to use bbmle:::slicetrans
> 
>>
>> * Do you mean using nAGQ? If so, I used nAGQ=5 an dgot a warning.
>>> x <-
>> summary(lme4::glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial
>> ,data=
>> test.dat,nAGQ=5))
>> Warning message:
>> In checkConv(attr(opt, "derivs"), opt$par, checkCtrl = control$checkConv,
> :
>>   Model failed to converge with max|grad| = 0.00275074 (tol = 0.001) 
>> Fixed effects:
>>              Estimate Std. Error z value Pr(>|z|)
>> (Intercept) -2.675364   0.282841  -9.459   <2e-16 ***
>> sex         -0.053593   0.146444  -0.366   0.7144
>> PC1         -0.205982   0.114760  -1.795   0.0727 .
>> PC2         -0.161422   0.119219  -1.354   0.1757
>> PC3         -0.191577   0.121107  -1.582   0.1137
>> PC4          0.002723   0.122446   0.022   0.9823
> 
>   This warning *might* be spurious (the lme4 maintainers have been having a
> private conversation about the fact that one may get false-positive warnings
> about gradients when the fit is singular):
> 
>>
>> * Do you mean use fixef.prior? fixef.prior: a BLME prior of family 
>> normal, t, or NULL. So which one is weaker prior?
> 
>    t is weaker than Normal (i.e., for the same mean (typically 0) and
> variance it has fatter tails).  However, there are two dimensions of
> "weakness" -- to make a prior weaker you can make the scale (i.e.
> variance) larger, and/or make the shape parameter more diffuse (i.e., make
> the tails fatter, in the sequence Gaussian (= t_\infty -> t_{large n} ->
> t_{smaller n}).  (You may end up discovering that the results depend on the
> strength of the prior ... which will open up interesting cans of worms in
> terms of what the data are actually telling you ...)
>>
>> Best,
>>
>> Ming-Huei
>>
>> -----Original Message-----
>> From: Ben Bolker [mailto:bbolker at gmail.com]
>> Sent: Friday, March 14, 2014 5:33 PM
>> To: Ming-Huei Chen; 'Yang, Qiong'
>> Cc: 'lme4-authors at lists.r-forge.r-project.org'
>> Subject: Re: [R-sig-ME] gee, geese and glmer
>>
>>   OK, this definitely looks well-behaved.  I am leaning still more 
>> heavily toward the idea that new-lme4 is giving an answer that is 
>> technically more correct (the splom plot shows the focal parameters 
>> and the location of the minimum computed 'slice' value as closed and 
>> open circles (I forget which is which); if they don't coincide, that 
>> suggests a misconvergence.  There are a few cases where there is a 
>> barely visible difference (i.e. you can see the open circle peeking 
>> out from around the edge of the closed circle), but they're generally 
>> very close.  The deviance surface looks sensibly smooth/ellipsoidal, and
> none of the parameters appear to be on boundary.
>> (There's a little problem with the slice2D code that limits the ranges 
>> sampled to be either all-positive or all-negative, I think.)
>>
>>    If so, that makes the difference between GEE, Laplace 
>> approximation, and Bayesian GLMM as much of a statistical as a
> computational problem.
>> A few more things I would try to follow up:
>>
>>   * try s2 <- slicetrans(m0, m1, dd); plot(s2) to examine the 
>> cross-section of the deviance function between the new and old parameters
>>   * try using Gauss-Hermite quadrature to see if that makes a difference
>>   * try blme::bglmer to impose a weak prior on the fixed effects (see 
>> if that makes the results closer to MCMCglmm)
>>
>>
>>
>> On 14-03-14 02:15 PM, Ming-Huei Chen wrote:
>>> Here it is.... 
>>>
>>> ###new
>>>> library(lme4)
>>>> m1 <-
>>> glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data=test.da
>>> t
>>> )
>>>> m1 at optinfo$warnings
>>> list()
>>>> library(optimx)
>>> Loading required package: numDeriv
>>>> m1.1 <-
>>> glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data=test.da
>>> t
>>> ,contr
>>> ol=glmerControl(optimizer="optimx",optCtrl=list(method="nlminb")))
>>>> m1.1 at optinfo$warnings
>>> list()
>>>
>>> ###old
>>>> library(lme4.0)
>>>> m0 <-
>>> glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data=test.da
>>> t
>>> )
>>> Warning message:
>>> In mer_finalize(ans) : false convergence (8)
>>>> library(optimx)
>>> Loading required package: numDeriv
>>>> m0.0 <-
>>> glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data=test.da
>>> t
>>> ,contr
>>> ol=glmerControl(optimizer="optimx",optCtrl=list(method="nlminb")))
>>> Error in do.call(lmerControl, control) :
>>>   could not find function "glmerControl"
>>>
>>> Best,
>>>
>>> Ming-Huei
>>> -----Original Message-----
>>> From: Ben Bolker [mailto:bbolker at gmail.com]
>>> Sent: Friday, March 14, 2014 11:08 AM
>>> To: Ming-Huei Chen; 'Yang, Qiong'
>>> Cc: lme4-authors at lists.r-forge.r-project.org
>>> Subject: Re: [R-sig-ME] gee, geese and glmer
>>>
>>>   The fact that the new deviance is lower suggests that the new 
>>> version of
>>> lme4 is actually finding a _better_ fit to the data; so does the 
>>> existence of a convergence warning from old lme4 (it's interesting 
>>> that using
>>> control=glmerControl(optimizer="optimx",optCtrl=list(method="nlminb")
>>> ) doesn't give you a similar warning ...) . There is still some minor 
>>> possibility that there's a glitch in the deviance calculation -- a 
>>> good cross-check would be to evaluate the deviance of both sets of 
>>> parameters for old lme4 as well, but that's a bit harder to do 
>>> because
>>> old-lme4 doesn't have the same level of flexibility).
>>>
>>>   The parameter comparisons that new-lme4 is giving fairly extreme 
>>> estimates for one of the fixed parameters (old=-3, new=-10; looking 
>>> at the parameter tables below, it looks like this is the intercept) 
>>> and a very large estimate of the random-effects standard deviation. 
>>> (I could have figured this out from the parameter tables too, but I'm 
>>> lazy and graphical presentation makes it much more obvious.)
>>>
>>>   This is looking like a case of (quasi-)complete separation, which 
>>> would explain why MCMCglmm is giving a much less extreme result (it 
>>> imposes weak priors on all of the parameters by default).  It would 
>>> be interesting to try blme::bglmer on this case as well.
>>>
>>> Can you send the (graphical) results of
>>>
>>> dd <- lme4::glmer(...,devFunOnly=TRUE)
>>> library(bbmle)
>>> ss1 <- slice2D(fun = dd, params = m1parms, verbose = FALSE)
>>> splom(ss1)
>>>
>>> ??
>>>
>>> On 14-03-14 09:39 AM, Ming-Huei Chen wrote:
>>>> Thanks, Ben!
>>>>
>>>> The new deviance is lower than the old one. The plot of estimates is 
>>>> also attached.
>>>>> dd(m0parms)
>>>> [1] 3301.035
>>>>> dd(m1parms)
>>>> [1] 2991.881
>>>>
>>>> Best,
>>>>
>>>> Ming-Huei
>>>> -----Original Message-----
>>>> From: Ben Bolker [mailto:bbolker at gmail.com]
>>>> Sent: Thursday, March 13, 2014 11:29 PM
>>>> To: Ming-Huei Chen; 'Yang, Qiong'; 'Martin Maechler'
>>>> Cc: r-sig-mixed-models at r-project.org
>>>> Subject: Re: [R-sig-ME] gee, geese and glmer
>>>>
>>>>   You should be able to install lme4.0 via:
>>>>
>>>> install.packages("lme4.0",
>>>>     repos="http://lme4.r-forge.r-project.org/repos")
>>>>
>>>> It should give identical results to lme4_0.999999-2.
>>>>
>>>>   Also, if you can pull out the coefficients (both the theta and the 
>>>> fixed effect parameters) and save them somewhere, you don't need to 
>>>> run old and new lme4 in the same session.  You just need to be able 
>>>> to pull both sets of parameters into an R session where you have 
>>>> lme4
>>>>> =
>>>> 1.0 loaded, use devFunOnly=TRUE to extract the deviance function, 
>>>> and apply it to both the old and the new sets of parameters.
>>>>
>>>>   Ben Bolker
>>>>
>>>>
>>>> On 14-03-13 08:54 PM, Ming-Huei Chen wrote:
>>>>> Thanks Ben and Martin!
>>>>>
>>>>> I tried lme4 1.1-4 and the result was identical to lme4 1.0-6 
>>>>> without warning (by @optinfo$warnings). As you mentioned about 
>>>>> convergence warning, I redid the analysis with lme4_0.999999-2 and 
>>>>> saw a convergence warning. I also tried MCMCglmm and here is the 
>>>>> result, sex effect is insignificant but the estimate is negative as 
>>>>> in GEE and lme4_0.999999-2. But as you mentioned, it's different
>> methodology..
>>>>>
>>>>> test <-
>>>>> MCMCglmm(case~sex+PC1+PC2+PC3+PC4,random=~famid,family="categorical"
>>>>> ,
>>>>> d
>>>>> ata=da
>>>>> ta)
>>>>>             post.mean  l-95% CI  u-95% CI eff.samp  pMCMC
>>>>> (Intercept) -2.469391 -2.821757 -2.078372    4.290 <0.001 ***
>>>>> sex         -0.055439 -0.330237  0.111188    5.426  0.626
>>>>> PC1         -0.197212 -0.332290 -0.018821    6.833  0.074 .
>>>>> PC2         -0.158433 -0.364921 -0.003554    5.142  0.002 **
>>>>> PC3         -0.130129 -0.320582  0.065973    4.094  0.320
>>>>> PC4          0.002677 -0.306265  0.233621    2.060  0.812
>>>>>
>>>>> As for comparing deviance, because different versions of lme4 need 
>>>>> to be installed in different versions of R and there is actually no 
>>>>> lme4.0, so I don't know how to use two versions of lme4 in the same 
>>>>> R session... Since you planned to get lme4.0 on CRAN, so you should 
>>>>> have the package... in that case, can you please send it to me? Or?
>>>>>
>>>>> Our data contain pedigree information which cannot be shared (even 
>>>>> pedigree structure), so simulate may not help.
>>>>>
>>>>> Best,
>>>>>
>>>>> Ming-Huei
>>>>>
>>>>> -----Original Message-----
>>>>> From: Ben Bolker [mailto:bbolker at gmail.com]
>>>>> Sent: Thursday, March 13, 2014 4:09 PM
>>>>> To: Yang, Qiong; Martin Maechler
>>>>> Cc: r-sig-mixed-models at r-project.org; Chen, Ming-Huei
>>>>> Subject: Re: [R-sig-ME] gee, geese and glmer
>>>>>
>>>>> On 14-03-13 02:01 PM, Yang, Qiong wrote:
>>>>>> Hi Ben and Martin,
>>>>>>
>>>>>> Thanks you for your reply and taking time to explain the situation. 
>>>>>> Maybe I wasn't clear in my previous message or don't have a deeper 
>>>>>> understanding of the CRAN policy: Instead of separate packages for 
>>>>>> "old-CRAN" lme4 (lme4.0), and the current lme4, which is not 
>>>>>> allowed by CRAN maintainer, is it possible to have an option in 
>>>>>> current lme4 to call the algorithm in old lme4? I thought the new
>>>>>> lme4 implements a different algorithm and why not make the old 
>>>>>> algorithm available in current lme4 lmer() as an option. It is 
>>>>>> like we can request pearson or spearman correlations in cor(). 
>>>>>> Sorry if I simplified the problem too much.
>>>>>
>>>>>    It's not so much CRAN policy as the architecture of the software.  
>>>>> A great deal of the internal structure has changed between versions
>>>>> <1.0 and > 1.0, so putting both versions into one package would 
>>>>> basically mean having copies of all of the old and all of the new 
>>>>> code
>>>>> -- and keeping it all safely distinguished and separate would 
>>>>> probably be substantially more work (we have about as much as we 
>>>>> can to do keep up with a single package ...) As we said before, we 
>>>>> were surprised by the rejection of lme4.0 by CRAN -- our previous 
>>>>> planning had assumed that we would be able to put lme4.0 on CRAN.
>>>>> Somewhat to our surprise, there have been relatively few people 
>>>>> contacting us with troubles similar to yours -- for the most part 
>>>>> it seems that is
>>>>> (thankfully) very rare that
>>>>> lme4 gives significantly worse answers than lme4.0.
>>>>>
>>>>>   So far it's not clear that lme4's answers are actually worse than 
>>>>> lme4.0's (I admit that all other things being equal, closer to GEE 
>>>>> is more likely to be correct -- on the other hand, we know that GEE 
>>>>> gives marginal estimates of fixed parameters while GLMM gives 
>>>>> conditional estimates, so we shouldn't expect them to be the same).
>>>>> Can
>>> I confirm:
>>>>>
>>>>> * you've tried this with the most recent development version of 
>>>>> lme4
>>>>> (1.1-4) and you do *not* get any convergence warnings?
>>>>>
>>>>> * have you compared the deviances based on the old (lme4.0 / lme4 <
>>>>> 1.0) and new packages?  Here is the outline of how you would do this:
>>>>>
>>>>> library(lme4)   ## load new package
>>>>> m1 <- glmer(...) ## lme4 fit
>>>>> library(lme4.0)  ## load old package
>>>>> m0 <- lme4.0::glmer(...) ## lme4.0 fit ## extract full parameter 
>>>>> set for lme4.0 and lme4 fits m0parms <-
>>>>> c(getME(m0,"theta"),fixef(m0)) m1parms <-
>>>>> c(lme4::getME(m1,"theta"),fixef(m1))
>>>>> nparms <- length(m0parms)
>>>>> ntheta <- length(getME(m0,"theta"))
>>>>>
>>>>> par(las=1,bty="l")
>>>>> plot(m0parms,m1parms,col=rep(2:1,c(ntheta,nparms-ntheta)))
>>>>> abline(a=0,b=1)
>>>>>
>>>>> ## set up deviance function
>>>>> dd <- lme4::glmer(...,devFunOnly=TRUE) ## compare
>>>>> dd(m0parms)
>>>>> dd(m1parms)
>>>>>
>>>>>   If the new deviance is lower than the old deviance, that's a 
>>>>> strong indication that the new version is actually getting a 
>>>>> _better_ fit than the old one.
>>>>>
>>>>>   Another diagnostic tool is:
>>>>>
>>>>> library(bbmle)
>>>>> ss1 <- slice2D(fun = dd, params = m1parms, verbose = FALSE)
>>>>> splom(ss1)
>>>>>
>>>>>   This will show you cross-sections of the deviance surface ...
>>>>>
>>>>>
>>>>>> To Ben, Sorry that we cannot share data because of confidentiality 
>>>>>> policy. But you are able to debug our problem through PC screen 
>>>>>> sharing where you can gain control and run programs on our Linux 
>>>>>> session without possessing the data. If that sounds like a good 
>>>>>> idea, let us know when you might be available to do so.
>>>>>
>>>>>   Can you generate a simulated example that replicates your problem?
>>>>> simulate() is a useful tool for generating data that are similar, 
>>>>> but not identical to, your original data (you can also anonymize 
>>>>> factor labels etc.).
>>>>>
>>>>>   PC screen sharing seems difficult -- I have easy access to MacOS 
>>>>> and Linux systems, but not Windows.
>>>>>
>>>>>>
>>>>>> Thanks, Qiong -----Original Message----- From: Ben Bolker 
>>>>>> [mailto:bbolker at gmail.com] Sent: Tuesday, March 11, 2014 9:26 AM To:
>>>>>> Martin Maechler; Yang, Qiong Cc: r-sig-mixed-models at r-project.org; 
>>>>>> Chen, Ming-Huei Subject: Re: [R-sig-ME] gee, geese and glmer
>>>>>>
>>>>>> I would also point out that we are indeed very interested, in the 
>>>>>> medium term (the short term is very very busy!), in making sure 
>>>>>> that your issues are resolved.  We would like lme4 to dominate
>>>>>> lme4.0 (i.e., to work better in all circumstances). So far it's 
>>>>>> been a bit difficult since we have been debugging remotely -- 
>>>>>> short of the suggestions I gave below, and without access to a 
>>>>>> reproducible example, it's very hard indeed for me to say much more.
>>>>>>
>>>>>> sincerely Ben Bolker
>>>>>>
>>>>>> On 14-03-11 05:45 AM, Martin Maechler wrote:
>>>>>>>>>>>> Yang, Qiong <qyang at bu.edu> on Mon, 10 Mar 2014 18:19:56
>>>>>>>>>>>> +0000 writes:
>>>>>>>
>>>>>>>> Hi Ben, We wonder if you can add an option in lmer() of current
>>>>>>>> lme4 version to call the algorithm used in lme4_0.999999-2?
>>>>>>>
>>>>>>> unfortunately not.
>>>>>>>
>>>>>>> For this reason, we had planned for many months, starting in 
>>>>>>> August
>>>>>>>  2012 and announced on this mailing list at least a year ago that 
>>>>>>> we would provide the 'lme4.0' package  (back-compatible as well 
>>>>>>> as possible in light of computer OS updates incl system 
>>>>>>> libraries, and R updates, ...) in order to provide useRs the 
>>>>>>> possibility of reproducible research and data analysis for their 
>>>>>>> analyses done with "old-CRAN" lme4 versions.
>>>>>>>
>>>>>>> Unfortunately (for us), the CRAN maintainers decided that 
>>>>>>> providing
>>>>>>>  lme4.0 in addition to lme4 was a bad idea, and explicit forbid 
>>>>>>> such actions in the (then new) CRAN policy document.  I'm still 
>>>>>>> not at all happy with that decision.
>>>>>>>
>>>>>>>> For our package (analyze rare genetic variant) to be put on 
>>>>>>>> CRAN, we need to use current version of lme4. However, at this 
>>>>>>>> point, there are still issues that cannot be resolved with newer 
>>>>>>>> versions of lme4. It is very difficult resolved with newer 
>>>>>>>> versions
>> of lme4.
>>>>>>>> It is very difficult for us to keep waiting and testing the new 
>>>>>>>> release, and hope all the issues resolved and no new issues 
>>>>>>>> coming up. lme4_0.999999-2 has been used by us for a long time 
>>>>>>>> with little problem.
>>>>>>>
>>>>>>> Good to hear.  Such cases were exactly the reason why we (lme4
>>>>>>> authors) made such considerable effort to provide  lme4.0 for 
>>>>>>> reproducible research and data analysis.
>>>>>>>
>>>>>>> at the bottom of
>>>>>>> https://github.com/lme4/lme4/blob/master/README.md we mention the 
>>>>>>> state and give installation instruction of lme4.0, but as you 
>>>>>>> say, this does not solve the problem for other package 
>>>>>>> maintainers: If they want a CRAN package, they (currently? I'm 
>>>>>>> optimistic beyond reason :-) cannot have a 'Depends: lme4.0'  (or 
>>>>>>> "Imports:..." or similar).
>>>>>>>
>>>>>>>> Your help on this is highly appreciated.  Best, Qiong
>>>>>>>
>>>>>>> You're welcome; currentl there's not more we can do. Martin
>>>>>>>
>>>>>>> -- Martin Maechler, ETH Zurich, Switzerland
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>> -----Original Message----- From: Ben Bolker 
>>>>>>>> [mailto:bbolker at gmail.com] Sent: Saturday, March 08, 2014 5:25 
>>>>>>>> PM
>>>>>>>> To: r-sig-mixed-models at r-project.org Cc: Chen, Ming-Huei; Yang, 
>>>>>>>> Qiong Subject: Re: gee, geese and glmer
>>>>>>>
>>>>>>>> On 14-03-07 11:25 PM, Ming-Huei Chen wrote:
>>>>>>>>> Hi Ben,
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> In an analysis we found that glmer in new lme4 gave result 
>>>>>>>>> different from old lme4, gee and geese, where old lme4 seems to 
>>>>>>>>> be closer to gee and geese.. Please see highlighted sex effect
>> below.
>>>>>>>>> Case by sex (2x2) table is also given. Can you please let us 
>>>>>>>>> know how would you look into the results? Thanks!
>>>>>>>>>
>>>>>>>
>>>>>>>> [cc'ing to r-sig-mixed-models: **please** try r-sig-mixed-models 
>>>>>>>> first, not personal e-mail to me ...]
>>>>>>>
>>>>>>>> I can't say exactly what's going here; without having a 
>>>>>>>> reproducible example <http://tinyurl.com/reproducible-000> it's 
>>>>>>>> hard to say precisely.  Thoughts:
>>>>>>>
>>>>>>>> * gee and geese are giving _exactly_ the same parameter 
>>>>>>>> estimates, to 8 significant digits, so I would guess they are 
>>>>>>>> wrapping identical underlying methods.
>>>>>>>
>>>>>>>> * As far as diagnosing the issue with lme4 1.0-6: * does 
>>>>>>>> changing the optimization method, i.e.
>>>>>>>> glmerControl(optimizer="optimx",optCtrl=list(method="nlminb"))
>>>>>>>> [must do library("optimx") first] or
>>>>>>>> glmerControl(optimizer="bobyqa")
>>>>>>>
>>>>>>>> change the result?
>>>>>>>
>>>>>>>> * I would be curious whether the soon-to-be-released version
>>>>>>>> 1.1-4 (which can be installed from github or
>>>>>>>> lme4.r-forge.r-project.org/repos) gives either (1) convergence 
>>>>>>>> warnings or (2) different/better answers
>>>>>>>
>>>>>>>> * You can try specifying the starting values for lme4 to 
>>>>>>>> diagnose misconvergence; for example, start lme4 from the 
>>>>>>>> estimates given by old lme4/lme4.0 and see if it gives a similar
> answer.
>>>>>>>
>>>>>>>> * You can use the 'slice' and 'splom.slice' functions from bbmle 
>>>>>>>> to visualize the likelihood surfaces
>>>>>>>
>>>>>>>> good luck, Ben Bolker
>>>>>>>
>>>>>>>>> Ming-Huei
>>>>>>>>>
>>>>>>>
>>>>>>>>> ###GEE
>>>>>>>>>
>>>>>>>>>> summary(gee(case~sex+PC1+PC2+PC3+PC4,id=famid,family=binomial,
>>>>>>>>>> d
>>>>>>>>>> a
>>>>>>>>>> t
>>>>>>>>>> a
>>>>>>>>>> =da
>>>>>>>
>>>>>>>>>>
>>>>>>>> ta))$coef
>>>>>>>>> Estimate Naive S.E.  Naive z Robust S.E.  Robust z (Intercept)
>>>>>>>>> -1.88047373 0.13532162 -13.8963286 0.15960440 -11.782092 sex
>>>>>>>>> -0.23436854 0.08611269 -2.7216494 0.09050577 -2.589543 PC1
>>>>>>>>> -0.05478639 0.06195318 -0.8843192 0.06822178 -0.803063 PC2
>>>>>>>>> -0.09934572 0.06494563 -1.5296753 0.06520811 -1.523518 PC3
>>>>>>>>> -0.07020391 0.06626875 -1.0593818 0.06962147 -1.008366 PC4
>>>>>>>>> -0.13413097 0.06746716 -1.9880927 0.06979901 -1.921674
>>>>>>>>>
>>>>>>>
>>>>>>>>> ###GEESE
>>>>>>>>>
>>>>>>>>>> summary(geese(case~sex+PC1+PC2+PC3+PC4,id=famid,family=binomia
>>>>>>>>>> l
>>>>>>>>>> ,
>>>>>>>>>> d
>>>>>>>>>> a
>>>>>>>>>> ta=
>>>>>>>
>>>>>>>>>>
>>>>>>>> data))$mean
>>>>>>>>>
>>>>>>>>> estimate san.se wald p
>>>>>>>>>
>>>>>>>>> (Intercept) -1.88047373 0.15960440 138.8176912 0.000000000 sex
>>>>>>>>> -0.23436854 0.09050577 6.7057312 0.009610351 PC1 -0.05478639
>>>>>>>>> 0.06822178 0.6449102 0.421938319 PC2 -0.09934572 0.06520811
>>>>>>>>> 2.3211071 0.127629159 PC3 -0.07020391 0.06962147 1.0168016
>>>>>>>>> 0.313278888 PC4 -0.13413097 0.06979901 3.6928324 0.054646745
>>>>>>>>>
>>>>>>>>> ### lme4_0.999999-2
>>>>>>>>>
>>>>>>>>>> summary(glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomi
>>>>>>>>>> a
>>>>>>>>>> l
>>>>>>>>>> ,
>>>>>>>>>> d
>>>>>>>>>> ata
>>>>>>>
>>>>>>>>>>
>>>>>>>> =data))
>>>>>>>>> Estimate Std. Error z value Pr(>|z|) (Intercept) -3.01599
>>>>>>>>> 0.28305 -10.655 <2e-16 *** sex -0.41056 0.16285 -2.521 0.0117 *
>>>>>>>>> PC1 -0.17116 0.12903 -1.326 0.1847 PC2 -0.15510 0.13382 -1.159
>>>>>>>>> 0.2465 PC3 -0.19044 0.13580 -1.402 0.1608 PC4 0.02532 0.13732
>>>>>>>>> 0.184 0.8537
>>>>>>>>>
>>>>>>>>> ###lme4_1.0-6
>>>>>>>>>
>>>>>>>>>> summary(glmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomi
>>>>>>>>>> a
>>>>>>>>>> l
>>>>>>>>>> ,
>>>>>>>>>> d
>>>>>>>>>> ata
>>>>>>>
>>>>>>>>>>
>>>>>>>> =data))
>>>>>>>>>
>>>>>>>>> Estimate Std. Error z value Pr(>|z|)
>>>>>>>>>
>>>>>>>>> (Intercept) -10.2784 0.8631 -11.909 <2e-16 *** sex 0.3497
>>>>>>>>> 0.1975 1.770 0.0767 .  PC1 -0.3555 0.1623 -2.190 0.0285 * PC2
>>>>>>>>> -0.1087 0.1653 -0.657 0.5109 PC3 -0.2242 0.1652 -1.357 0.1748
>>>>>>>>> PC4 0.1103 0.1671 0.660 0.5091
>>>>>>>>>
>>>>>>>>> Case by sex
>>>>>>>>>
>>>>>>>>> 1 2 0 2554 3021 1 310 290
>>>>>>>>>
>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> R-sig-mixed-models at r-project.org mailing list 
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>>
>>>>>>
>>>>>
>>


From vjd4 at nyu.edu  Tue Mar 18 16:47:57 2014
From: vjd4 at nyu.edu (Vincent Dorie)
Date: Tue, 18 Mar 2014 11:47:57 -0400
Subject: [R-sig-ME] gee, geese and glmer
In-Reply-To: <53285880.2030502@gmail.com>
References: <007801cf3a86$6d27b060$47771120$@bu.edu>	<531B98B4.4050403@gmail.com>	<6984CA5898097D459F8EC753F025B0D839696447@IST-EX10MBX-3.ad.bu.edu>
	<21278.56111.63015.103003@stat.math.ethz.ch>
	<531F0F03.5090907@gmail.com>
	<6984CA5898097D459F8EC753F025B0D8396982CD@IST-EX10MBX-3.ad.bu.edu>
	<53221072.9080203@gmail.com>
	<000301cf3f1f$f07fb0f0$d17f12d0$@bu.edu> <5322776E.90206@gmail.com>
	<005f01cf3f8a$d73cda80$85b68f80$@bu.edu>
	<53231B51.8070300@gmail.com>
	<011401cf3fb1$71a56620$54f03260$@bu.edu>
	<532375A4.5060703@gmail.com>
	<013201cf4218$90a933d0$b1fb9b70$@bu.edu>
	<5327522A.8090708@gmail.com>
	<00c901cf4254$62359ac0$26a0d040$@bu.edu>
	<53285880.2030502@gmail.com>
Message-ID: <3B0125C6-1F8E-493C-9A16-F67CE4FF63B6@nyu.edu>

>> 
>> blme::bglmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data=test.da
>> t,fixef.prior=t)
>> Error in get("checkConv", lme4Env)(attr(opt, "derivs"), opt$par, ctrl =
>> control$checkConv,  :
>>  unused argument (ctrl = control$checkConv)

I believe this can only happen if the installed version of lme4 is out-of-date. If there was a point where checkConv was added but didn't yet have the ctrl formal, you would see that error.

Vince

From vjd4 at nyu.edu  Tue Mar 18 21:23:16 2014
From: vjd4 at nyu.edu (Vincent Dorie)
Date: Tue, 18 Mar 2014 16:23:16 -0400
Subject: [R-sig-ME] gee, geese and glmer
In-Reply-To: <00db01cf42db$0f691650$2e3b42f0$@bu.edu>
References: <007801cf3a86$6d27b060$47771120$@bu.edu>	<531B98B4.4050403@gmail.com>	<6984CA5898097D459F8EC753F025B0D839696447@IST-EX10MBX-3.ad.bu.edu>
	<21278.56111.63015.103003@stat.math.ethz.ch>
	<531F0F03.5090907@gmail.com>
	<6984CA5898097D459F8EC753F025B0D8396982CD@IST-EX10MBX-3.ad.bu.edu>
	<53221072.9080203@gmail.com>
	<000301cf3f1f$f07fb0f0$d17f12d0$@bu.edu> <5322776E.90206@gmail.com>
	<005f01cf3f8a$d73cda80$85b68f80$@bu.edu>
	<53231B51.8070300@gmail.com>
	<011401cf3fb1$71a56620$54f03260$@bu.edu>
	<532375A4.5060703@gmail.com>
	<013201cf4218$90a933d0$b1fb9b70$@bu.edu>
	<5327522A.8090708@gmail.com>
	<00c901cf4254$62359ac0$26a0d040$@bu.edu>
	<53285880.2030502@gmail.com>
	<3B0125C6-1F8E-493C-9A16-F67CE4FF63B6@nyu.edu>
	<00db01cf42db$0f691650$2e3b42f0$@bu.edu>
Message-ID: <DA2E7521-C4EE-4B9F-882F-F244C9D82032@nyu.edu>

1.1-5 is the latest version on CRAN. As far as I know, blme works with the versions of lme4 >= 1.0 that made it to CRAN, so 1.0-4, 1.0-5, 1.0-6 and 1.1-5.

Vince


On Mar 18, 2014, at 2:51 PM, Ming-Huei Chen wrote:

> Is lme4_1.1-4 out-of-date? This is the version I used and encountered the
> error. 
> 
> Best,
> 
> Ming-Huei
> 
> -----Original Message-----
> From: Vincent Dorie [mailto:vjd4 at nyu.edu] 
> Sent: Tuesday, March 18, 2014 11:48 AM
> To: Ben Bolker
> Cc: Yang, Qiong; Ming-Huei Chen; r-sig-mixed-models;
> lme4-authors at lists.r-forge.r-project.org
> Subject: Re: [R-sig-ME] gee, geese and glmer
> 
>>> 
>>> blme::bglmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data=
>>> test.da
>>> t,fixef.prior=t)
>>> Error in get("checkConv", lme4Env)(attr(opt, "derivs"), opt$par, ctrl 
>>> = control$checkConv,  :
>>> unused argument (ctrl = control$checkConv)
> 
> I believe this can only happen if the installed version of lme4 is
> out-of-date. If there was a point where checkConv was added but didn't yet
> have the ctrl formal, you would see that error.
> 
> Vince
> 


From bbolker at gmail.com  Tue Mar 18 21:30:37 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 18 Mar 2014 16:30:37 -0400
Subject: [R-sig-ME] gee, geese and glmer
In-Reply-To: <00db01cf42db$0f691650$2e3b42f0$@bu.edu>
References: <007801cf3a86$6d27b060$47771120$@bu.edu>	<531B98B4.4050403@gmail.com>	<6984CA5898097D459F8EC753F025B0D839696447@IST-EX10MBX-3.ad.bu.edu>
	<21278.56111.63015.103003@stat.math.ethz.ch>
	<531F0F03.5090907@gmail.com>
	<6984CA5898097D459F8EC753F025B0D8396982CD@IST-EX10MBX-3.ad.bu.edu>
	<53221072.9080203@gmail.com>
	<000301cf3f1f$f07fb0f0$d17f12d0$@bu.edu> <5322776E.90206@gmail.com>
	<005f01cf3f8a$d73cda80$85b68f80$@bu.edu>
	<53231B51.8070300@gmail.com>
	<011401cf3fb1$71a56620$54f03260$@bu.edu>
	<532375A4.5060703@gmail.com>
	<013201cf4218$90a933d0$b1fb9b70$@bu.edu>
	<5327522A.8090708@gmail.com>
	<00c901cf4254$62359ac0$26a0d040$@bu.edu>
	<53285880.2030502@gmail.com>
	<3B0125C6-1F8E-493C-9A16-F67CE4FF63B6@nyu.edu>
	<00db01cf42db$0f691650$2e3b42f0$@bu.edu>
Message-ID: <5328ACED.3000907@gmail.com>

On 14-03-18 02:51 PM, Ming-Huei Chen wrote:
> Is lme4_1.1-4 out-of-date? This is the version I used and encountered the
> error. 
> 
> Best,
> 
> Ming-Huei

  Yes, 1.1-5 was released on CRAN recently (and blme did some updating
to match).

  Ben Bolker

> 
> -----Original Message-----
> From: Vincent Dorie [mailto:vjd4 at nyu.edu] 
> Sent: Tuesday, March 18, 2014 11:48 AM
> To: Ben Bolker
> Cc: Yang, Qiong; Ming-Huei Chen; r-sig-mixed-models;
> lme4-authors at lists.r-forge.r-project.org
> Subject: Re: [R-sig-ME] gee, geese and glmer
> 
>>>
>>> blme::bglmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data=
>>> test.da
>>> t,fixef.prior=t)
>>> Error in get("checkConv", lme4Env)(attr(opt, "derivs"), opt$par, ctrl 
>>> = control$checkConv,  :
>>>  unused argument (ctrl = control$checkConv)
> 
> I believe this can only happen if the installed version of lme4 is
> out-of-date. If there was a point where checkConv was added but didn't yet
> have the ctrl formal, you would see that error.
> 
> Vince
>


From vjd4 at nyu.edu  Wed Mar 19 03:56:12 2014
From: vjd4 at nyu.edu (Vincent Dorie)
Date: Tue, 18 Mar 2014 22:56:12 -0400
Subject: [R-sig-ME] gee, geese and glmer
In-Reply-To: <002d01cf431b$cc8fae20$65af0a60$@bu.edu>
References: <007801cf3a86$6d27b060$47771120$@bu.edu>	<531B98B4.4050403@gmail.com>	<6984CA5898097D459F8EC753F025B0D839696447@IST-EX10MBX-3.ad.bu.edu>
	<21278.56111.63015.103003@stat.math.ethz.ch>
	<531F0F03.5090907@gmail.com>
	<6984CA5898097D459F8EC753F025B0D8396982CD@IST-EX10MBX-3.ad.bu.edu>
	<53221072.9080203@gmail.com>
	<000301cf3f1f$f07fb0f0$d17f12d0$@bu.edu> <5322776E.90206@gmail.com>
	<005f01cf3f8a$d73cda80$85b68f80$@bu.edu>
	<53231B51.8070300@gmail.com>
	<011401cf3fb1$71a56620$54f03260$@bu.edu>
	<532375A4.5060703@gmail.com>
	<013201cf4218$90a933d0$b1fb9b70$@bu.edu>
	<5327522A.8090708@gmail.com>
	<00c901cf4254$62359ac0$26a0d040$@bu.edu>
	<53285880.2030502@gmail.com>
	<3B0125C6-1F8E-493C-9A16-F67CE4FF63B6@nyu.edu>
	<00db01cf42db$0f691650$2e3b42f0$@bu.edu>
	<5328ACED.3000907@gmail.com>
	<002d01cf431b$cc8fae20$65af0a60$@bu.edu>
Message-ID: <E2E4C207-A6D8-4377-B6C2-AD78943BBABC@nyu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140318/bec333bb/attachment.pl>

From slesquin at gmail.com  Wed Mar 19 08:01:28 2014
From: slesquin at gmail.com (sandrine le squin)
Date: Wed, 19 Mar 2014 11:01:28 +0400
Subject: [R-sig-ME] nested random factor
Message-ID: <CAKzWUB=qKW1XnP658TuNcF1-8bGqZTbMi+URpM5QM-eWb1=gwA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140319/40827d52/attachment.pl>

From seth at swbigelow.net  Wed Mar 19 15:46:37 2014
From: seth at swbigelow.net (Seth Bigelow)
Date: Wed, 19 Mar 2014 10:46:37 -0400
Subject: [R-sig-ME] nested random factor
In-Reply-To: <CAKzWUB=qKW1XnP658TuNcF1-8bGqZTbMi+URpM5QM-eWb1=gwA@mail.gmail.com>
References: <CAKzWUB=qKW1XnP658TuNcF1-8bGqZTbMi+URpM5QM-eWb1=gwA@mail.gmail.com>
Message-ID: <001501cf4382$09111e40$1b335ac0$@net>

Well -- I don't know about the 'correct' model, but you might consider two
ideas. First, the variance among samples might differ according to location.
If so, you would get a better model by nesting sample within location, e.g.,
using the statement (1|location/sample), rather than (1|location) +
(1|sample). You can test whether this model is better with likelihood ratio
tests, i.e., anova(UnNestedmodel,NestedModel).

Second, you probably want to account for serial autocorrelation in samples
taken from the same location, repeatedly over time -- i.e. in the 'week'
variable. I've done this in SAS but never mastered the syntax in nlme/lme4.

Good luck
-Seth 



-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org
[mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of sandrine le
squin
Sent: Wednesday, March 19, 2014 3:01 AM
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] nested random factor

Dear all,

I'm using lmer from lme4 package but I am not sure of what I'm doing.

The dataset has the following variables :
- Treatment (fixed, 3 levels)
- Location (fixed, 4 levels)
- Sample (random, 5 levels). 5 samples are taken in each location (randomly)
- Subsample (random, 3 levels). 3 subsamples are taken in each sample. One
treatment is assigned (randomly) to one of the subsample.

The study was conducted every Wednesday during 15 weeks in 2013, from week
32 to week 47 (no observations for week 33).

For the first location, samples are taken weeks 32, 36, 39, 42 and 49.
For the second location, samples are taken the week 46.
For the third location, samples are taken weeks 35, 38, 41 and 44.
For the 4th location, samples are taken week 34, 37, 40, 43, 47.

For each location* i*, week *j(i)*, sample* k(ij)* and treatment *l*, the
"richness in sugar" of each subsample has been recorded.
The aim of the study is to see if there is an effect of the factor
"treatment" on the response "richness".

I am new in mixed model so I did some research but I am a little bit
confused with the sampling design.

I consider "location", "week" and "sample" as random factor, with sample
nested in week nested in location.
To avoid implicitly nested representations, "sample" has been recoded
(factor with 75 levels (5 samples*15 weeks)).

I write in R:
lmer(richness ~ treatment + (1|location) + (1|week) + (1|sample), data =
mydata)

Is my model correct ?

Thank you for your help

	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From bbolker at gmail.com  Wed Mar 19 15:57:50 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 19 Mar 2014 10:57:50 -0400
Subject: [R-sig-ME] nested random factor
In-Reply-To: <001501cf4382$09111e40$1b335ac0$@net>
References: <CAKzWUB=qKW1XnP658TuNcF1-8bGqZTbMi+URpM5QM-eWb1=gwA@mail.gmail.com>
	<001501cf4382$09111e40$1b335ac0$@net>
Message-ID: <5329B06E.5090702@gmail.com>

On 14-03-19 10:46 AM, Seth Bigelow wrote:
> Well -- I don't know about the 'correct' model, but you might consider two
> ideas. First, the variance among samples might differ according to location.
> If so, you would get a better model by nesting sample within location, e.g.,
> using the statement (1|location/sample), rather than (1|location) +
> (1|sample). You can test whether this model is better with likelihood ratio
> tests, i.e., anova(UnNestedmodel,NestedModel).

  (1|location/sample) is equivalent to (1|location)+(1|location:sample).
 If the samples are uniquely labeled (e.g. A1, A2, A3, B1, B2, B3, ...
rather than 1, 2, 3, 1, 2, 3,) then these are *also* equivalent to
(1|location)+(1|sample).  If they are not uniquely labeled then
(1|location)+(1|sample) represents crossed random effects, which you
probably *don't* want unless there is something that makes the first
sample in every location the same.  (lme can't do crossed random effects
in this way.)

> 
> Second, you probably want to account for serial autocorrelation in samples
> taken from the same location, repeatedly over time -- i.e. in the 'week'
> variable. I've done this in SAS but never mastered the syntax in nlme/lme4.

  lme4 can't do this, you would need lme with e.g.
correlation=corAR1(formula=~week), although the uneven sampling of weeks
could be problematic.

> 
> Good luck
> -Seth 
> 
> 
> 
> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org
> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of sandrine le
> squin
> Sent: Wednesday, March 19, 2014 3:01 AM
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] nested random factor
> 
> Dear all,
> 
> I'm using lmer from lme4 package but I am not sure of what I'm doing.
> 
> The dataset has the following variables :
> - Treatment (fixed, 3 levels)
> - Location (fixed, 4 levels)
> - Sample (random, 5 levels). 5 samples are taken in each location (randomly)
> - Subsample (random, 3 levels). 3 subsamples are taken in each sample. One
> treatment is assigned (randomly) to one of the subsample.
> 
> The study was conducted every Wednesday during 15 weeks in 2013, from week
> 32 to week 47 (no observations for week 33).
> 
> For the first location, samples are taken weeks 32, 36, 39, 42 and 49.
> For the second location, samples are taken the week 46.
> For the third location, samples are taken weeks 35, 38, 41 and 44.
> For the 4th location, samples are taken week 34, 37, 40, 43, 47.
> 
> For each location* i*, week *j(i)*, sample* k(ij)* and treatment *l*, the
> "richness in sugar" of each subsample has been recorded.
> The aim of the study is to see if there is an effect of the factor
> "treatment" on the response "richness".
> 
> I am new in mixed model so I did some research but I am a little bit
> confused with the sampling design.
> 
> I consider "location", "week" and "sample" as random factor, with sample
> nested in week nested in location.
> To avoid implicitly nested representations, "sample" has been recoded
> (factor with 75 levels (5 samples*15 weeks)).
> 
> I write in R:
> lmer(richness ~ treatment + (1|location) + (1|week) + (1|sample), data =
> mydata)

  You said above the location and treatment are fixed: did you mean
location was random?

  I would use

 richness ~ treatment + (1|location/week/sample)

since within any given week samples are only taken in one location.

4 levels for location is a little low for reliable estimation of random
effects, so you might consider

 Since the treatments are replicated within locations, weeks,
treatments, etc., you might need to consider
(treatment|location/week/sample), although that might be overfitting the
model ...


> 
> Is my model correct ?
> 
> Thank you for your help
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From bates at stat.wisc.edu  Thu Mar 20 16:08:51 2014
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 20 Mar 2014 10:08:51 -0500
Subject: [R-sig-ME] comparison estimation maximum likelihood and
	restricted maximum likelihood
In-Reply-To: <1395325422.59326.YahooMailNeo@web160704.mail.bf1.yahoo.com>
References: <1395325422.59326.YahooMailNeo@web160704.mail.bf1.yahoo.com>
Message-ID: <CAO7JsnT4X=W2_SxujO6M_tsEk=S9Qz2YXp85AueewUb4mcxh1g@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140320/c2bcb9df/attachment.pl>

From shamil at gmail.com  Thu Mar 20 17:13:09 2014
From: shamil at gmail.com (Shamil Sadigov)
Date: Thu, 20 Mar 2014 18:13:09 +0200
Subject: [R-sig-ME] MCMCglmm predictions
Message-ID: <CABENjUaWwew+HiRM3yB-KGbxaLYgs7Xrx2f=v4z0VYiBe_AZnw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140320/40687ad7/attachment.pl>

From j.hadfield at ed.ac.uk  Thu Mar 20 17:24:25 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Thu, 20 Mar 2014 16:24:25 +0000
Subject: [R-sig-ME] MCMCglmm predictions
In-Reply-To: <CABENjUaWwew+HiRM3yB-KGbxaLYgs7Xrx2f=v4z0VYiBe_AZnw@mail.gmail.com>
References: <CABENjUaWwew+HiRM3yB-KGbxaLYgs7Xrx2f=v4z0VYiBe_AZnw@mail.gmail.com>
Message-ID: <20140320162425.11932jw93i7tap0k@www.staffmail.ed.ac.uk>

Hi Shamil,

This is fixed in MCMCglmm_2.18 which is now on CRAN. The Mac OS X  
binary has not been built yet (perhaps tonight) but you can install  
from source. For other OS its ready. Let me know if there is still a  
problem.

Cheers,

Jarrod

Quoting Shamil Sadigov <shamil at gmail.com> on Thu, 20 Mar 2014 18:13:09 +0200:

> Hello All,
>
> I am using the MCMCglmm package to fit a multivariate ordered probit model
> for 5 ordinal variables. Parameter estimation was successful, so was the
> prediction on the linear scale. However, when I tried prediction on the
> response scale I got the following error:
>
>> eq5d.mcmc2bfix.p.pred <- predict(eq5d.mcmc2bfix.p, type="response")
> Error in M[, which(rm.v), ] <- 0 : incorrect number of subscripts
>
> When I looked at the predict.MCMCglmm() function from the package, I
> noticed that the M matrix was specified as a 3-dimentional array - hence
> the incorrect number of substripts error.
>
>  M <- model.matrix(~v.terms[nz] - 1)
>                   M[, which(rm.v), ] <- 0
>
> I am using the version:
>
>> packageVersion("MCMCglmm")
> [1] '2.17'
>
>
> Note:
>> eq5d.mcmc2bfix.p.pred <- predict(eq5d.mcmc2bfix.p, type="terms")
> works with no error.
>
> Regards,
> Shamil.
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From kburls at unr.edu  Fri Mar 21 00:05:48 2014
From: kburls at unr.edu (Kevin Burls)
Date: Thu, 20 Mar 2014 23:05:48 +0000
Subject: [R-sig-ME] repeated measures random effects structure
Message-ID: <E16ACB3C-3F38-43B8-A76B-81ADCECFA0E5@unr.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140320/cc6a1c76/attachment.pl>

From b.pelzer at maw.ru.nl  Fri Mar 21 12:13:36 2014
From: b.pelzer at maw.ru.nl (Ben Pelzer)
Date: Fri, 21 Mar 2014 12:13:36 +0100
Subject: [R-sig-ME] lmer option nAGQ=0
In-Reply-To: <20140320162425.11932jw93i7tap0k@www.staffmail.ed.ac.uk>
References: <CABENjUaWwew+HiRM3yB-KGbxaLYgs7Xrx2f=v4z0VYiBe_AZnw@mail.gmail.com>
	<20140320162425.11932jw93i7tap0k@www.staffmail.ed.ac.uk>
Message-ID: <532C1EE0.9010803@maw.ru.nl>

Dear list,

In R version 3.0.3, I recently udated the lme4 package and for a small 
dataset of N=108 pigsties, I ran logistic regression with a random 
intercept across the pigsties. The dependent variable is assumed to be 
binomially distributed, and represents the number of pigs in the sty 
that have a roundworm infection. There is a dichotomous predictor, 
denoting two different types of sty. Using the option nAGQ=0 produces 
PQL estimates. These estimates are, however, quite different from those 
obtained using sas (glimmix), spss (genlinmixed) and glmmPQL in R: these 
three routines produce very similar estimates of the two fixed and the 
one random effect. Now I'm wondering what the reason for the differences 
compared with lmer nAGQ=0 may be. The option to run PQL with lmer may be 
attractive if one has many (complex) models and large datasets, hence my 
questioning. Thanks for any help!!

Ben.


From cvr7 at cdc.gov  Fri Mar 21 12:15:13 2014
From: cvr7 at cdc.gov (Rose, Charles E. (CDC/OID/NCHHSTP))
Date: Fri, 21 Mar 2014 11:15:13 +0000
Subject: [R-sig-ME] (no subject)
Message-ID: <4633829E615E2E4D8CF24ABFBC86B75B079522@EMBX-CLFT1.cdc.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140321/480a10d7/attachment.pl>

From cvr7 at cdc.gov  Fri Mar 21 12:17:45 2014
From: cvr7 at cdc.gov (Rose, Charles E. (CDC/OID/NCHHSTP))
Date: Fri, 21 Mar 2014 11:17:45 +0000
Subject: [R-sig-ME] Recall:
Message-ID: <4633829E615E2E4D8CF24ABFBC86B75B07A538@EMBX-CLFT1.cdc.gov>

Rose, Charles E. (CDC/OID/NCHHSTP) would like to recall the message, "".

From holtermann at hwwi.org  Fri Mar 21 12:24:38 2014
From: holtermann at hwwi.org (Linus Holtermann)
Date: Fri, 21 Mar 2014 12:24:38 +0100
Subject: [R-sig-ME] Multilevel with spatial effects
Message-ID: <AD0050057515F54084E7D5B93478C8481F7BA02674@winxbede39.exchange.xchg>

Hello,

i found the following working-paper: Corrado/Fingleton "Multilevel Modelling with Spatial Effects" (http://ideas.repec.org/p/str/wpaper/1105.html).
I am interested in fitting a SAR-Multilevel Model as on page 10-11. The inclusion of the group-mean of Y  as exogen variable will result in a endogenity problem. Is it possible to fit these models with existing R-packages? I attach some data for Y, group-mean Y, X and "group". How can i fit the equation (23) on page 10?
Moreover i am interested in fitting a model that is able to identify interactions effects (page 13-14). Is that possible in R?

Another question regards spatial autocorrelation for Level-1 Units. For example, i have districts (Level-1) and regions (Level-2) and i anticipate spatial autocorrelation on Level-1. Can I use a classical spatial weigth matrix (e.g. 8 nearest neigbours (districts) weighted by inverse distance) where neigbours can be districts in the same or nearby regions and incorporate these spatial lag in the multilevel equation? 
In other words, can i replace the Level-1 regression in multilevel model with a classical spatial regression as used in the spdep-packages? Or do i face serious problems like cross-level-endogenity? Would a MCMC approach help?

I know that These are some advanced questions, but i hope someone can help.
Thanks,

best regards,

 
Linus Holtermann
Hamburgisches WeltWirtschaftsInstitut gemeinn?tzige GmbH (HWWI)
Heimhuder Stra?e 71
20148 Hamburg
Tel +49-(0)40-340576-336
Fax+49-(0)40-340576-776
Internet: www.hwwi.org
Email: holtermann at hwwi.org
 
AmtsgerichtHamburg HRB 94303
Gesch?ftsf?hrer: Prof. Dr. Thomas Straubhaar, Gunnar Geyer
Umsatzsteuer-ID: DE 241849425

From mcasals at aspb.cat  Fri Mar 21 13:49:29 2014
From: mcasals at aspb.cat (=?ISO-8859-1?Q?Mart=ED_Casals?=)
Date: Fri, 21 Mar 2014 13:49:29 +0100
Subject: [R-sig-ME] Multilevel with spatial effects
In-Reply-To: <AD0050057515F54084E7D5B93478C8481F7BA02674@winxbede39.exchange.xchg>
References: <AD0050057515F54084E7D5B93478C8481F7BA02674@winxbede39.exchange.xchg>
Message-ID: <CADtmEn6fRTVjQB6aSwcbV78XE1SHrMOf5qVZxn1tZkEtTUQ04w@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140321/ce61fe82/attachment.pl>

From shamil at gmail.com  Fri Mar 21 13:56:25 2014
From: shamil at gmail.com (Shamil Sadigov)
Date: Fri, 21 Mar 2014 14:56:25 +0200
Subject: [R-sig-ME] Predicted values in MCMCglmm family="threshold"
Message-ID: <CABENjUb_QOpahuWeuVceZOO0dWfjmyTS3zdjTwew5vt7BXO9pA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140321/5969663b/attachment.pl>

From bbolker at gmail.com  Fri Mar 21 14:12:46 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 21 Mar 2014 09:12:46 -0400
Subject: [R-sig-ME] lmer option nAGQ=0
In-Reply-To: <532C1EE0.9010803@maw.ru.nl>
References: <CABENjUaWwew+HiRM3yB-KGbxaLYgs7Xrx2f=v4z0VYiBe_AZnw@mail.gmail.com>	<20140320162425.11932jw93i7tap0k@www.staffmail.ed.ac.uk>
	<532C1EE0.9010803@maw.ru.nl>
Message-ID: <532C3ACE.2060306@gmail.com>

On 14-03-21 07:13 AM, Ben Pelzer wrote:
> Dear list,
> 
> In R version 3.0.3, I recently udated the lme4 package and for a small
> dataset of N=108 pigsties, I ran logistic regression with a random
> intercept across the pigsties. The dependent variable is assumed to be
> binomially distributed, and represents the number of pigs in the sty
> that have a roundworm infection. There is a dichotomous predictor,
> denoting two different types of sty. Using the option nAGQ=0 produces
> PQL estimates. These estimates are, however, quite different from those
> obtained using sas (glimmix), spss (genlinmixed) and glmmPQL in R: these
> three routines produce very similar estimates of the two fixed and the
> one random effect. Now I'm wondering what the reason for the differences
> compared with lmer nAGQ=0 may be. The option to run PQL with lmer may be
> attractive if one has many (complex) models and large datasets, hence my
> questioning. Thanks for any help!!
> 
> Ben.


  It's not true that AGQ=0 produces PQL estimates (so it's not at all
surprising that the results don't match the results of PQL estimates).
Rather, it produces 'conditional estimates' -- it estimates the
conditional modes, but doesn't apply a Laplace approximation (or any
other).  Don't have time to say more now, sorry.

  Ben Bolker


From bbolker at gmail.com  Fri Mar 21 14:14:36 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 21 Mar 2014 09:14:36 -0400
Subject: [R-sig-ME] Fwd: ADMB course Montpellier ISEC
In-Reply-To: <10CB298A-A0E5-46B5-86F8-FCFE0610DC4F@ufl.edu>
References: <10CB298A-A0E5-46B5-86F8-FCFE0610DC4F@ufl.edu>
Message-ID: <532C3B3C.1010103@gmail.com>


  Forwarded from Mollie Brooks -- of possible interest here.

-------- Original Message --------
Subject: 	ADMB course Montpellier ISEC
Date: 	Fri, 21 Mar 2014 12:52:00 +0100
From: 	Mollie Brooks <mbrooks at ufl.edu>

Dear Mixed Modellers,

One method for fitting models with random effects and latent variables
is by writing the model in the open-source program ADMB and running it
using the R package R2admb (
http://cran.r-project.org/web/packages/R2admb/vignettes/R2admb.pdf ).

This can be helpful when fitting nonlinear models that are more
complicated than GLMMs (
http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12044/full ). Also,
the R package glmmADMB was written using ADMB (
http://glmmadmb.r-forge.r-project.org ).

If you are interested in learning this program, I would like to bring an
upcoming workshop to your attention. It will be held at the
International Statistical Ecology Conference in Montpellier, France at
the end of June http://isec2014.sciencesconf.org/resource/page/id/15

Sorry to spam you with something that is only tangentially related to R,
but I think it's useful.

Thank you,
Mollie

------------------------
Mollie Brooks, PhD
Postdoctoral Researcher, Population Ecology Research Group
http://www.popecol.org
Institute of Evolutionary Biology & Environmental Studies, University of
Z?rich


From j.hadfield at ed.ac.uk  Fri Mar 21 15:14:50 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Fri, 21 Mar 2014 14:14:50 +0000
Subject: [R-sig-ME] Predicted values in MCMCglmm family="threshold"
In-Reply-To: <CABENjUb_QOpahuWeuVceZOO0dWfjmyTS3zdjTwew5vt7BXO9pA@mail.gmail.com>
References: <CABENjUb_QOpahuWeuVceZOO0dWfjmyTS3zdjTwew5vt7BXO9pA@mail.gmail.com>
Message-ID: <20140321141450.504315i4ekhimudc@www.staffmail.ed.ac.uk>

Hi,

have

cp<-c(-Inf, 0, cp.est, Inf)

where cp.est are the estimated cutpoints (if there are any - with 2  
categories there are none).

Have linear predictor nu = xb or nu=xb+zu. If the former (and there  
are random effects) then have v the sum of the variance components  
associated with that term, and if the latter have v as the units  
variance associated with that term.

Have obs<-1:k where k is the number of categories (2+the number of  
estimated cutpoints) and the probability of falling into a category  
conditional on nu and v is:

pnorm(cp[obs+1], nu , sqrt(v)) ? pnorm(cp[obs], nu, sqrt(v))

for family=threshold, and

pnorm(cp[obs+1], nu , sqrt(v+1)) ? pnorm(cp[obs], nu, sqrt(v+1))

for ordinal.

For example,

cp.est<-1
cp<-c(-Inf, 0, cp.est, Inf)
k<-2+length(cp.est)
obs<-1:k
nu<--1
v<-2
pnorm(cp[obs+1], nu , sqrt(v))-pnorm(cp[obs], nu, sqrt(v))

Jarrod


Quoting Shamil Sadigov <shamil at gmail.com> on Fri, 21 Mar 2014 14:56:25 +0200:

> Hi Jarrod,
>
> I am using the new family="threshold" in MCMCglmm version 2.18 with a
> 5-variate ordered response. I would like to obtain the predicted responses
> for on the original ordinal scale, but I am not sure how to do so for
> either "ordinal" or the "threshold" family.
>
>  1. For family="threshold" the posterior predicted probabilities are :
>
>                 post.pred[, keep] <- pnorm(post.pred[, keep], 0,
> sqrt(postvar[, keep]))
>
> How can I classify these probabilities into the original ordinal scale?
>
>
> 2. I can see that for family="ordinal", cut points (CP) are used in
> predict.MCMCglmm():
>
>   for (i in 2:(dim(CP)[2] - 1)) {
>                   q <- q + (pnorm(CP[, i + 1] - post.pred[, keep],  0,
> sqrt(postvar[, keep] + 1)) - pnorm(CP[, i] - post.pred[, keep], 0,
> sqrt(postvar[, keep] + 1))) * (i - 1)
>                                         }
> Are the thresholds and the posterior predictive values (using type =
> "terms") on the linear (latent variable) scale?
>
> What would be the interpretation of the predicted values obtained from
> using type= "response" with family = "ordinal"? (All 5 ordinal responses
> are coded 1-3, and the predicted values from predict.MCMCglmm are real
> numbers between 0-6.)
>
> Regards,
> Shamil.
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From j.hadfield at ed.ac.uk  Fri Mar 21 15:24:12 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Fri, 21 Mar 2014 14:24:12 +0000
Subject: [R-sig-ME] Predicted values in MCMCglmm family="threshold"
In-Reply-To: <20140321141450.504315i4ekhimudc@www.staffmail.ed.ac.uk>
References: <CABENjUb_QOpahuWeuVceZOO0dWfjmyTS3zdjTwew5vt7BXO9pA@mail.gmail.com>
	<20140321141450.504315i4ekhimudc@www.staffmail.ed.ac.uk>
Message-ID: <20140321142412.47024rvgurxlf12c@www.staffmail.ed.ac.uk>

Hi,

? should be - (and microsoft should be *)

Jarrod


Quoting Jarrod Hadfield <j.hadfield at ed.ac.uk> on Fri, 21 Mar 2014  
14:14:50 +0000:

> Hi,
>
> have
>
> cp<-c(-Inf, 0, cp.est, Inf)
>
> where cp.est are the estimated cutpoints (if there are any - with 2  
> categories there are none).
>
> Have linear predictor nu = xb or nu=xb+zu. If the former (and there  
> are random effects) then have v the sum of the variance components  
> associated with that term, and if the latter have v as the units  
> variance associated with that term.
>
> Have obs<-1:k where k is the number of categories (2+the number of  
> estimated cutpoints) and the probability of falling into a category  
> conditional on nu and v is:
>
> pnorm(cp[obs+1], nu , sqrt(v)) ? pnorm(cp[obs], nu, sqrt(v))
>
> for family=threshold, and
>
> pnorm(cp[obs+1], nu , sqrt(v+1)) ? pnorm(cp[obs], nu, sqrt(v+1))
>
> for ordinal.
>
> For example,
>
> cp.est<-1
> cp<-c(-Inf, 0, cp.est, Inf)
> k<-2+length(cp.est)
> obs<-1:k
> nu<--1
> v<-2
> pnorm(cp[obs+1], nu , sqrt(v))-pnorm(cp[obs], nu, sqrt(v))
>
> Jarrod
>
>
> Quoting Shamil Sadigov <shamil at gmail.com> on Fri, 21 Mar 2014 14:56:25 +0200:
>
>> Hi Jarrod,
>>
>> I am using the new family="threshold" in MCMCglmm version 2.18 with a
>> 5-variate ordered response. I would like to obtain the predicted responses
>> for on the original ordinal scale, but I am not sure how to do so for
>> either "ordinal" or the "threshold" family.
>>
>> 1. For family="threshold" the posterior predicted probabilities are :
>>
>>                post.pred[, keep] <- pnorm(post.pred[, keep], 0,
>> sqrt(postvar[, keep]))
>>
>> How can I classify these probabilities into the original ordinal scale?
>>
>>
>> 2. I can see that for family="ordinal", cut points (CP) are used in
>> predict.MCMCglmm():
>>
>>  for (i in 2:(dim(CP)[2] - 1)) {
>>                  q <- q + (pnorm(CP[, i + 1] - post.pred[, keep],  0,
>> sqrt(postvar[, keep] + 1)) - pnorm(CP[, i] - post.pred[, keep], 0,
>> sqrt(postvar[, keep] + 1))) * (i - 1)
>>                                        }
>> Are the thresholds and the posterior predictive values (using type =
>> "terms") on the linear (latent variable) scale?
>>
>> What would be the interpretation of the predicted values obtained from
>> using type= "response" with family = "ordinal"? (All 5 ordinal responses
>> are coded 1-3, and the predicted values from predict.MCMCglmm are real
>> numbers between 0-6.)
>>
>> Regards,
>> Shamil.
>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>
>
>
> -- 
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From b.pelzer at maw.ru.nl  Fri Mar 21 16:14:55 2014
From: b.pelzer at maw.ru.nl (Ben Pelzer)
Date: Fri, 21 Mar 2014 16:14:55 +0100
Subject: [R-sig-ME] lmer option nAGQ=0
In-Reply-To: <532C3ACE.2060306@gmail.com>
References: <CABENjUaWwew+HiRM3yB-KGbxaLYgs7Xrx2f=v4z0VYiBe_AZnw@mail.gmail.com>	<20140320162425.11932jw93i7tap0k@www.staffmail.ed.ac.uk>
	<532C1EE0.9010803@maw.ru.nl> <532C3ACE.2060306@gmail.com>
Message-ID: <532C576F.6000501@maw.ru.nl>

Hi Ben,

The pdf file of lme4 speaks of "the penalized iteratively reweighted 
least squares step" and I was so naive as to think thought this would be 
the same as PQL. Of course, if this would have been so, it would have 
been explicitly mentioned there.

Though I don't know what "conditional modes" are (and when these could 
be useful) it's good to know that it's NOT the same as pql! Sorry for 
the confusion, and thanks for solving this,

Ben.


On 21-3-2014 14:12, Ben Bolker wrote:
> On 14-03-21 07:13 AM, Ben Pelzer wrote:
>> Dear list,
>>
>> In R version 3.0.3, I recently udated the lme4 package and for a small
>> dataset of N=108 pigsties, I ran logistic regression with a random
>> intercept across the pigsties. The dependent variable is assumed to be
>> binomially distributed, and represents the number of pigs in the sty
>> that have a roundworm infection. There is a dichotomous predictor,
>> denoting two different types of sty. Using the option nAGQ=0 produces
>> PQL estimates. These estimates are, however, quite different from those
>> obtained using sas (glimmix), spss (genlinmixed) and glmmPQL in R: these
>> three routines produce very similar estimates of the two fixed and the
>> one random effect. Now I'm wondering what the reason for the differences
>> compared with lmer nAGQ=0 may be. The option to run PQL with lmer may be
>> attractive if one has many (complex) models and large datasets, hence my
>> questioning. Thanks for any help!!
>>
>> Ben.
>
>    It's not true that AGQ=0 produces PQL estimates (so it's not at all
> surprising that the results don't match the results of PQL estimates).
> Rather, it produces 'conditional estimates' -- it estimates the
> conditional modes, but doesn't apply a Laplace approximation (or any
> other).  Don't have time to say more now, sorry.
>
>    Ben Bolker
>
>
>
>


From bbolker at gmail.com  Mon Mar 24 02:18:53 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 23 Mar 2014 21:18:53 -0400
Subject: [R-sig-ME] Question of Brasil
In-Reply-To: <CAAJcTECwE9A9+GFgW_NGtTFyDqKB=Ed0LUTmWrDsu_hPoxeJRg@mail.gmail.com>
References: <CAAJcTECwE9A9+GFgW_NGtTFyDqKB=Ed0LUTmWrDsu_hPoxeJRg@mail.gmail.com>
Message-ID: <532F87FD.7020809@gmail.com>

On 14-03-23 08:34 PM, Alejandra Tapia wrote:
> Dear Ben,
> 
> I have two question:
> 
> 1) In lme4 package  when we use the function glmer with Laplace
> Aproximation (nAGQ=1) or Adaptative Gaussian Quadrature (nAGQ=5), the
> random effects by which method are calculated?
> 
> 2) For to glmer or command resid() using the same method that 1) for
> calculate or random effects?
> 
> Thank's
> 
> Greetings
> -- 
> Alejandra
> IME-USP, Brasil

  I'm taking the liberty of cc'ing this to the r-sig-mixed-models list,
because I really can't answer private lme4 questions, no matter how much
I would like to -- I would get no time to do anything else.

  I'm afraid, in addition, that I don't understand your questions very
well.  (1) Standard references on GLMMs such as Bolker et al 2008
explain what Laplace approximation and Gaussian quadrature are: if you
need more specific information, you'll have to rephrase your question
more precisely. (2) I really don't understand this at all. Can you
rephrase or give an example?

  sincerely
    Ben Bolker
>


From ned.dochtermann at gmail.com  Mon Mar 24 21:46:15 2014
From: ned.dochtermann at gmail.com (Ned Dochtermann)
Date: Mon, 24 Mar 2014 15:46:15 -0500
Subject: [R-sig-ME] Multiple random and cross-classified factors
 specification and caveats in a generalized linear model (with MCMCglmm)
Message-ID: <53309997.6050208@gmail.com>

Dear list,
Since I tend to be verbose, the question eventually being asked below is 
whether the model specification seems appropriate and, being unfamiliar 
to this sort of categorical cross-classified model, whether there are 
glaring problems I'm missing.

I'm trying to analyze a dataset of paired contest outputs with winners 
and losers. I'm wanting to use a mixed model as opposed to an 
alternative approach because I'm particularly interested in whether 
there are different among-individual variances for a particular grouping 
variable (handedness). Individuals show up as both focal individual and 
opponent across multiple contests so there are a few ways in which 
pseudoreplication enters the dataset.

The data basically looks like:
Focal.ID        Opp.ID                  Contest.ID Handedness        Won
Bob                Jack                      1    L                    
         W
Jack                Bob                      1   R                    
         L
Bob                Sam                      2 L                            W
Sam               Bob                       2 L                            L
Matt              Sam                      3 R                           W
Sam               Matt                     3 L                            L
...
John              Steve                    880                   R    
                        W
Steve             John                     880 L                        
     L


We have 880 contests with 588 unique focal id's, with 58% of individuals 
competing in at least one contest. Unfortunately some individuals had to 
be excluded so both individuals from a contest aren't always included. 
The fixed portion of the analysis includes only an intercept (based on a 
published analysis someone else did and because of the specific 
question). The random part of the model is then:

random=~idh(Handedness):Focal.ID+Opp.ID+Contest.ID

I'm then looking at the posterior distribution difference in latent 
scale variances for lefties and righties. I don't actually care about 
the variances of the other effects, they're just included because they 
seemingly should be. The thought was that this sort of model structure 
would at least partially encompass the data structure.
Does this structure seem reasonable?
I know detecting variance differences between groups is going to be 
tough, but this is the data we could obtain.

Thanks,
Ned

-- 
Ned A. Dochtermann
Assistant Professor / Department of Biological Sciences
*NORTH DAKOTA **STATE UNIVERSITY*
p: 701.231.7353 / f: 701.231.7149 / www.ndsu.edu

https://sites.google.com/site/neddochtermann/
ned.dochtermann at ndsu.edu


From M.Fairbrother at bristol.ac.uk  Tue Mar 25 03:08:39 2014
From: M.Fairbrother at bristol.ac.uk (Malcolm Fairbrother)
Date: Tue, 25 Mar 2014 02:08:39 +0000
Subject: [R-sig-ME] R-sig-mixed-models Digest, Vol 87, Issue 34
In-Reply-To: <mailman.5.1395486002.17262.r-sig-mixed-models@r-project.org>
References: <mailman.5.1395486002.17262.r-sig-mixed-models@r-project.org>
Message-ID: <CAAH-yP-xLC2ap2q3uHdZ_2iUmE_7m3gyN-YBRT5ToGzQVuTa3w@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140325/bcadde91/attachment.pl>

From t.o.lentz at uu.nl  Tue Mar 25 08:54:11 2014
From: t.o.lentz at uu.nl (Tom Lentz)
Date: Tue, 25 Mar 2014 08:54:11 +0100
Subject: [R-sig-ME] Random effects of logistic regression: bias towards the
	mean?
Message-ID: <53313623.6000100@uu.nl>

Dear all,

The following question might be due to my poor understanding of logistic 
regression, in which case I would be very grateful for an explanation or 
a pointer to reading material.

With my current understanding I think that logistic regression as 
typically done with lmer and family="binomial" (actually calls glmer, as 
calling lmer is now deprecated) behaves in an unexpected way, because it 
does not make random effects be near zero but moves them towards chance, 
i.e. towards positive values if the probability of a hit is below 0.5 
and towards negative values if the probability of a hit is above 0.5. At 
first I thought this was shrinkage, but it does not happen if data is 
aggregated and a normal linear mixed model fitted to percentages, but I 
think that is ugly and should lead to worse or equal results, not better 
ones, because the percentages cannot be normally distributed, especially 
if they are far from chance.

I have discovered this issue with the analysis of eye-tracking data, in 
which the chance of looking at the target was around 0.25, but the fixed 
effects in my model were lower than the mean and the random effects for 
participant and item were not around zero (hence, participants tend to 
be better than the fixed effect/average and items generally tend to be 
recognised better than on the fixed effects predicted/average). The 
result is that the fixed effects estimates are not at the average 
values, but lower.

As my data set might have had a poorly understood conspiracy in it, I 
simulated data. Every simulated data set had 40 participants and 40 
items (easy if you make it up!), but no effect of fixed effects; there 
was a condition (A, B, C or D) but the outcome was not influenced by 
this condition. The dependent variable was drawn with rbinom(1600, 1, 
probability), where probability was varied: 0.1, 0.15, 0.2 up till 0.9.

For each probability I ran 2000 analyses with this formula:
lmer(outcome ~ cond + (1|i) + (1|p), data=dataset, family = "binomial")
  and looked at the random effects for item and participants. Indeed, 
the lower the hit rate (the probability of the dependent variable 
outcome being TRUE or 1), the higher the average random effect, with a 
zero average for the random effects only at a 0.5 probability (or 0 
logit). A plot can be found at 
<http://www.hum.uu.nl/medewerkers/t.o.lentz/plotRanefsR3.pdf>.

The fixed effect of cond should not be significant, as the data is made 
up without regard to it. Indeed, at an alpha of 0.05 a spurious 
significant effect was only found in 4,2 % of the simulations. So, the 
analyses are not causing errors for hypothesis testing, but the 
estimates of the random effects are off. Is there a good explanation or 
is this unexpected behaviour?

Version information: I have detected the problem a while ago, still in R 
2, but it still happens in R 3.0.3 with lme4 version 1.1-5.

Thanks in advance for your help!

Kind regards,

Tom

TO Lentz PhD
Postdoctoral Researcher,
Parsing and Metrical Structure: Where Phonology Meets Processing

Utrecht Institute of Linguistics OTS
Utrecht University
Trans 10
3512 JK Utrecht
Netherlands


From tibor at linguistics.rub.de  Tue Mar 25 09:06:29 2014
From: tibor at linguistics.rub.de (Tibor Kiss)
Date: Tue, 25 Mar 2014 09:06:29 +0100
Subject: [R-sig-ME] Random effects of logistic regression: bias towards
	the mean?
In-Reply-To: <53313623.6000100@uu.nl>
References: <53313623.6000100@uu.nl>
Message-ID: <A27911B3-0AB3-4E57-993A-611F09C12ECC@linguistics.rub.de>

Dear Tom,

it is hard to answer your question without the actual output of your model (not of your simulation), i.e. print(summary(model), corr = F). It is also not clear to me what you are actually measuring, i.e. what the "hit" should be. Perhaps you can elaborate.


With kind regards

Tibor
 

Am 25.03.2014 um 08:54 schrieb Tom Lentz:

> Dear all,
> 
> The following question might be due to my poor understanding of logistic regression, in which case I would be very grateful for an explanation or a pointer to reading material.
> 
> With my current understanding I think that logistic regression as typically done with lmer and family="binomial" (actually calls glmer, as calling lmer is now deprecated) behaves in an unexpected way, because it does not make random effects be near zero but moves them towards chance, i.e. towards positive values if the probability of a hit is below 0.5 and towards negative values if the probability of a hit is above 0.5. At first I thought this was shrinkage, but it does not happen if data is aggregated and a normal linear mixed model fitted to percentages, but I think that is ugly and should lead to worse or equal results, not better ones, because the percentages cannot be normally distributed, especially if they are far from chance.
> 
> I have discovered this issue with the analysis of eye-tracking data, in which the chance of looking at the target was around 0.25, but the fixed effects in my model were lower than the mean and the random effects for participant and item were not around zero (hence, participants tend to be better than the fixed effect/average and items generally tend to be recognised better than on the fixed effects predicted/average). The result is that the fixed effects estimates are not at the average values, but lower.
> 
> As my data set might have had a poorly understood conspiracy in it, I simulated data. Every simulated data set had 40 participants and 40 items (easy if you make it up!), but no effect of fixed effects; there was a condition (A, B, C or D) but the outcome was not influenced by this condition. The dependent variable was drawn with rbinom(1600, 1, probability), where probability was varied: 0.1, 0.15, 0.2 up till 0.9.
> 
> For each probability I ran 2000 analyses with this formula:
> lmer(outcome ~ cond + (1|i) + (1|p), data=dataset, family = "binomial")
> and looked at the random effects for item and participants. Indeed, the lower the hit rate (the probability of the dependent variable outcome being TRUE or 1), the higher the average random effect, with a zero average for the random effects only at a 0.5 probability (or 0 logit). A plot can be found at <http://www.hum.uu.nl/medewerkers/t.o.lentz/plotRanefsR3.pdf>.
> 
> The fixed effect of cond should not be significant, as the data is made up without regard to it. Indeed, at an alpha of 0.05 a spurious significant effect was only found in 4,2 % of the simulations. So, the analyses are not causing errors for hypothesis testing, but the estimates of the random effects are off. Is there a good explanation or is this unexpected behaviour?
> 
> Version information: I have detected the problem a while ago, still in R 2, but it still happens in R 3.0.3 with lme4 version 1.1-5.
> 
> Thanks in advance for your help!
> 
> Kind regards,
> 
> Tom
> 
> TO Lentz PhD
> Postdoctoral Researcher,
> Parsing and Metrical Structure: Where Phonology Meets Processing
> 
> Utrecht Institute of Linguistics OTS
> Utrecht University
> Trans 10
> 3512 JK Utrecht
> Netherlands
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From j.hadfield at ed.ac.uk  Tue Mar 25 09:56:20 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Tue, 25 Mar 2014 08:56:20 +0000
Subject: [R-sig-ME] R-sig-mixed-models Digest, Vol 87, Issue 34
In-Reply-To: <CAAH-yP-xLC2ap2q3uHdZ_2iUmE_7m3gyN-YBRT5ToGzQVuTa3w@mail.gmail.com>
References: <mailman.5.1395486002.17262.r-sig-mixed-models@r-project.org>
	<CAAH-yP-xLC2ap2q3uHdZ_2iUmE_7m3gyN-YBRT5ToGzQVuTa3w@mail.gmail.com>
Message-ID: <20140325085620.10033te0vc50ka68@www.staffmail.ed.ac.uk>

Hi Malcolm,

I've commented in-line ...

Quoting Malcolm Fairbrother <M.Fairbrother at bristol.ac.uk> on Tue, 25  
Mar 2014 02:08:39 +0000:

> Dear Jarrod,
>
> Following on from your response to Shamil, and your comments about the new
> features in MCMCglmm version 2.18, can I please ask you to confirm a couple
> details about the extraction of predicted probabilities from fitted
> threshold models?
>
> As I did once before when asking a related question (
> http://permalink.gmane.org/gmane.comp.lang.r.lme4.devel/9424), I'll refer
> to the "wine" dataset in the ordinal package. My questions appear below.
>
> library(ordinal)
> library(MCMCglmm)
>
> data(wine)
> str(wine)
> summary(clmod <- clmm2(rating ~ temp + contact, random=judge, data=wine,
> Hess=TRUE, nAGQ=10, link="probit"))
>
> # to get predicted probabilities for each possible response, for one
> combination of covariates:
> diff(pnorm(c(-Inf,clmod$Theta, Inf) - clmod$beta %*% c(0,1), 0,
> sqrt(1+clmod$stDev))) # for tempcold and contactyes
>
> # now fit the same model, using MCMCglmm, two ways:
> prior1 <- list(R = list(V = 1, fix=1), G = list(G1 = list(V = 1, nu =
> 0.002)))
> MC1 <- MCMCglmm(rating ~ temp + contact, random=~judge, data=wine,
> family="ordinal", prior=prior1, nitt=130000, thin=100, verbose=F)
> MC2 <- MCMCglmm(rating ~ temp + contact, random=~judge, data=wine,
> family="threshold", prior=prior1, nitt=130000, thin=100, verbose=F)
>
> # and get predicted probabilities, using the results from MCMCglmm,
> marginalising the random effects:
> diff(pnorm(c(-Inf, 0, colMeans(MC1$CP), Inf)-colMeans(MC1$Sol) %*%
> c(1,0,1), 0, sqrt(1+sum(colMeans(MC1$VCV))))) # for tempcold and contactyes
> diff(pnorm(c(-Inf, 0, colMeans(MC2$CP), Inf)-colMeans(MC2$Sol) %*%
> c(1,0,1), 0, sqrt(1+sum(colMeans(MC2$VCV))))) # for tempcold and contactyes
>
> # Q1: They're not the same... and I'm not sure what precisely would make
> them the same. (The ones from MC1 match the ones from clmod, but those from
> MC2 don't.)

The threshold variance should not include the probit variance of 1.  
The probit variance is what is specified as the units variance in  
family="threshold":

diff(pnorm(c(-Inf, 0, colMeans(MC2$CP), Inf)-colMeans(MC2$Sol) %*%  
c(1,0,1), 0,sqrt(sum(colMeans(MC2$VCV))))) # for tempcold and contactyes

PS your code is a neater way of getting the probabilities than mine,  
but perhaps it is easier to understand as:

diff(pnorm(c(-Inf, 0, colMeans(MC2$CP), Inf), colMeans(MC2$Sol) %*%  
c(1,0,1),sqrt(sum(colMeans(MC2$VCV))))) # for tempcold and contactyes


>
> # finally, fit a similar model but with a random slope for the binary
> covariate "contact":
> prior2 <- list(R = list(V = 1, fix=1), G = list(G1 = list(V = diag(2), nu =
> 2.002))) # I believe this is an uninformative prior
> MC3 <- MCMCglmm(rating ~ temp + contact, random=~ us(1+contact):judge,
> data=wine, family="threshold", prior=prior2, nitt=130000, thin=100,
> verbose=F, pr=T)
>
> # Q2: How would one get predicted probabilities here, (a) marginalising the
> random effects, and (b) for a particular judge (say #1)?
>
> # for (b), would it be something like this (again for tempcold and
> contactyes)?
> diff(pnorm(c(-Inf, 0, colMeans(MC3$CP), Inf)-colMeans(MC3$Sol)[c(1:4,13)]
> %*% c(1,0,1,1,1), 0, sqrt(1+sum(colMeans(MC1$VCV)))))
>
> # but I'm guessing the "sqrt(1+sum(colMeans(MC1$VCV)))" bit is wrong... I'm
> not sure what you mean by "the sum of the variance components associated
> with that term"

In this model we have to decide when predicting whether we want to  
predict for contact=yes or contact=no. We could marginalise this too,  
but this would require knowing something about the distribution of  
contact in the population, and this isn't modelled in this instance.  
Lets say we want to predict for contact=yes as you have attempted. The  
prediction for judge 1 is:

diff(pnorm(c(-Inf, 0, colMeans(MC3$CP),  
Inf)-colMeans(MC3$Sol)[c(1:4,13)] %*% c(1,0,1,1,1), 0,  
sqrt(mean(MC3$VCV[,"units"]))))

because the only random terms left to marginalise are the residuals.

Marginalising the judge effects is a bit trickier in this instance  
because the judge effects for contact=yes are the sum of two terms and  
we need to know the variance of that sum. It is the sum of the  
variances for each effect plus twice the covariance between them.   
With the units variance too this is just equal to the sum of all  
(co)variances (because MCMCglmm returns matrices so gives the  
covariance twice):

diff(pnorm(c(-Inf, 0, colMeans(MC3$CP), Inf)-colMeans(MC3$Sol)[c(1:3)]  
%*% c(1,0,1), 0, sqrt(sum(colMeans(MC3$VCV)))))

In some ways it would be easier (and more instructive in this example)  
to have fitted us(contact) rather than us(1+contact). With the former  
parameterisation the first set of random effects is associated with  
contact=no (as before) but the second set of random effects is  
associated directly with contact=yes (unlike before). Marginalising  
the random effects for contact=yes is then:

diff(pnorm(c(-Inf, 0, colMeans(MC3$CP), Inf)-colMeans(MC3$Sol)[c(1:3)]  
%*% c(1,0,1), 0, sqrt(sum(colMeans(MC3$VCV[,c("units",  
"yes:yes.judge")])))))

and for contact=no is

diff(pnorm(c(-Inf, 0, colMeans(MC3$CP), Inf)-colMeans(MC3$Sol)[c(1:3)]  
%*% c(1,0,1), 0, sqrt(sum(colMeans(MC3$VCV[,c("units",  
"no:no.judge")])))))

The covariances are not required under this formulation for these predictions.

Note that these predictions are for based on the posterior mean of the  
parameters. It would be more satisfying to create predictions for each  
posterior sample of the parameters, and then average the predictions.  
This would give the posterior mean predictions rather than the  
predictions conditional on the posterior mean of the parameters.  
However, in many cases the two things will be close.

Cheers,

Jarrod

>
> As always, any assistance would be much, much appreciated.
>
> - Malcolm
>
>
>
> Date: Fri, 21 Mar 2014 14:14:50 +0000
>> From: Jarrod Hadfield <j.hadfield at ed.ac.uk>
>> To: Shamil Sadigov <shamil at gmail.com>
>> Cc: r-sig-mixed-models at r-project.org
>> Subject: Re: [R-sig-ME] Predicted values in MCMCglmm
>>         family="threshold"
>>
>> Hi,
>>
>> have
>>
>> cp<-c(-Inf, 0, cp.est, Inf)
>>
>> where cp.est are the estimated cutpoints (if there are any - with 2
>> categories there are none).
>>
>> Have linear predictor nu = xb or nu=xb+zu. If the former (and there
>> are random effects) then have v the sum of the variance components
>> associated with that term, and if the latter have v as the units
>> variance associated with that term.
>>
>> Have obs<-1:k where k is the number of categories (2+the number of
>> estimated cutpoints) and the probability of falling into a category
>> conditional on nu and v is:
>>
>> pnorm(cp[obs+1], nu , sqrt(v)) ? pnorm(cp[obs], nu, sqrt(v))
>>
>> for family=threshold, and
>>
>> pnorm(cp[obs+1], nu , sqrt(v+1)) ? pnorm(cp[obs], nu, sqrt(v+1))
>>
>> for ordinal.
>>
>> For example,
>>
>> cp.est<-1
>> cp<-c(-Inf, 0, cp.est, Inf)
>> k<-2+length(cp.est)
>> obs<-1:k
>> nu<--1
>> v<-2
>> pnorm(cp[obs+1], nu , sqrt(v))-pnorm(cp[obs], nu, sqrt(v))
>>
>> Jarrod
>>
>>
>> Quoting Shamil Sadigov <shamil at gmail.com> on Fri, 21 Mar 2014 14:56:25
>> +0200:
>>
>> > Hi Jarrod,
>> >
>> > I am using the new family="threshold" in MCMCglmm version 2.18 with a
>> > 5-variate ordered response. I would like to obtain the predicted
>> responses
>> > for on the original ordinal scale, but I am not sure how to do so for
>> > either "ordinal" or the "threshold" family.
>> >
>> >  1. For family="threshold" the posterior predicted probabilities are :
>> >
>> >                 post.pred[, keep] <- pnorm(post.pred[, keep], 0,
>> > sqrt(postvar[, keep]))
>> >
>> > How can I classify these probabilities into the original ordinal scale?
>> >
>> >
>> > 2. I can see that for family="ordinal", cut points (CP) are used in
>> > predict.MCMCglmm():
>> >
>> >   for (i in 2:(dim(CP)[2] - 1)) {
>> >                   q <- q + (pnorm(CP[, i + 1] - post.pred[, keep],  0,
>> > sqrt(postvar[, keep] + 1)) - pnorm(CP[, i] - post.pred[, keep], 0,
>> > sqrt(postvar[, keep] + 1))) * (i - 1)
>> >                                         }
>> > Are the thresholds and the posterior predictive values (using type =
>> > "terms") on the linear (latent variable) scale?
>> >
>> > What would be the interpretation of the predicted values obtained from
>> > using type= "response" with family = "ordinal"? (All 5 ordinal responses
>> > are coded 1-3, and the predicted values from predict.MCMCglmm are real
>> > numbers between 0-6.)
>> >
>> > Regards,
>> > Shamil.
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >
>> >
>>
>>
>>
>> --
>> The University of Edinburgh is a charitable body, registered in
>> Scotland, with registration number SC005336.
>>
>>
>>
>> ------------------------------
>>
>> Message: 2
>> Date: Fri, 21 Mar 2014 14:24:12 +0000
>> From: Jarrod Hadfield <j.hadfield at ed.ac.uk>
>> To: Jarrod Hadfield <j.hadfield at ed.ac.uk>
>> Cc: r-sig-mixed-models at r-project.org
>> Subject: Re: [R-sig-ME] Predicted values in MCMCglmm
>>         family="threshold"
>> Message-ID: <20140321142412.47024rvgurxlf12c at www.staffmail.ed.ac.uk>
>> Content-Type: text/plain; charset=ISO-8859-1; DelSp="Yes";
>>         format="flowed"
>>
>> Hi,
>>
>> ? should be - (and microsoft should be *)
>>
>> Jarrod
>>
>>
>> Quoting Jarrod Hadfield <j.hadfield at ed.ac.uk> on Fri, 21 Mar 2014
>> 14:14:50 +0000:
>>
>> > Hi,
>> >
>> > have
>> >
>> > cp<-c(-Inf, 0, cp.est, Inf)
>> >
>> > where cp.est are the estimated cutpoints (if there are any - with 2
>> > categories there are none).
>> >
>> > Have linear predictor nu = xb or nu=xb+zu. If the former (and there
>> > are random effects) then have v the sum of the variance components
>> > associated with that term, and if the latter have v as the units
>> > variance associated with that term.
>> >
>> > Have obs<-1:k where k is the number of categories (2+the number of
>> > estimated cutpoints) and the probability of falling into a category
>> > conditional on nu and v is:
>> >
>> > pnorm(cp[obs+1], nu , sqrt(v)) ? pnorm(cp[obs], nu, sqrt(v))
>> >
>> > for family=threshold, and
>> >
>> > pnorm(cp[obs+1], nu , sqrt(v+1)) ? pnorm(cp[obs], nu, sqrt(v+1))
>> >
>> > for ordinal.
>> >
>> > For example,
>> >
>> > cp.est<-1
>> > cp<-c(-Inf, 0, cp.est, Inf)
>> > k<-2+length(cp.est)
>> > obs<-1:k
>> > nu<--1
>> > v<-2
>> > pnorm(cp[obs+1], nu , sqrt(v))-pnorm(cp[obs], nu, sqrt(v))
>> >
>> > Jarrod
>> >
>> >
>> > Quoting Shamil Sadigov <shamil at gmail.com> on Fri, 21 Mar 2014 14:56:25
>> +0200:
>> >
>> >> Hi Jarrod,
>> >>
>> >> I am using the new family="threshold" in MCMCglmm version 2.18 with a
>> >> 5-variate ordered response. I would like to obtain the predicted
>> responses
>> >> for on the original ordinal scale, but I am not sure how to do so for
>> >> either "ordinal" or the "threshold" family.
>> >>
>> >> 1. For family="threshold" the posterior predicted probabilities are :
>> >>
>> >>                post.pred[, keep] <- pnorm(post.pred[, keep], 0,
>> >> sqrt(postvar[, keep]))
>> >>
>> >> How can I classify these probabilities into the original ordinal scale?
>> >>
>> >>
>> >> 2. I can see that for family="ordinal", cut points (CP) are used in
>> >> predict.MCMCglmm():
>> >>
>> >>  for (i in 2:(dim(CP)[2] - 1)) {
>> >>                  q <- q + (pnorm(CP[, i + 1] - post.pred[, keep],  0,
>> >> sqrt(postvar[, keep] + 1)) - pnorm(CP[, i] - post.pred[, keep], 0,
>> >> sqrt(postvar[, keep] + 1))) * (i - 1)
>> >>                                        }
>> >> Are the thresholds and the posterior predictive values (using type =
>> >> "terms") on the linear (latent variable) scale?
>> >>
>> >> What would be the interpretation of the predicted values obtained from
>> >> using type= "response" with family = "ordinal"? (All 5 ordinal responses
>> >> are coded 1-3, and the predicted values from predict.MCMCglmm are real
>> >> numbers between 0-6.)
>> >>
>> >> Regards,
>> >> Shamil.
>>
>>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From slesquin at gmail.com  Tue Mar 25 11:58:13 2014
From: slesquin at gmail.com (sandrine le squin)
Date: Tue, 25 Mar 2014 14:58:13 +0400
Subject: [R-sig-ME] nested random factor
In-Reply-To: <5329B06E.5090702@gmail.com>
References: <CAKzWUB=qKW1XnP658TuNcF1-8bGqZTbMi+URpM5QM-eWb1=gwA@mail.gmail.com>
	<001501cf4382$09111e40$1b335ac0$@net> <5329B06E.5090702@gmail.com>
Message-ID: <CAKzWUBkS1HZZtUAQoNcqvtz8mdxGh7nYZvLqebsdMjyaOqeCVg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140325/3e7c63bd/attachment.pl>

From t.o.lentz at uu.nl  Tue Mar 25 17:39:33 2014
From: t.o.lentz at uu.nl (Tom Lentz)
Date: Tue, 25 Mar 2014 17:39:33 +0100
Subject: [R-sig-ME] Random effects of logistic regression: bias towards
 the mean?
In-Reply-To: <5331A1A7.4040206@uu.nl>
References: <5331A1A7.4040206@uu.nl>
Message-ID: <5331B145.4050206@uu.nl>

Dear Tibor,

Thanks for your interest, of course I'm happy to elaborate.

First, note that the 'hit' in this case does not mean anything, I'm just
trying to simulate data. In my own psycholinguistic eye-tracking
research, it would be if a participant looks at a target region of the
screen; it could also be if a participant responds correctly, but here
it is just a binomial dependent variable with different probability of
being 1 (true, or 'hit', or 'correct', but that is just interpretation
of in fact meaningless data I generated with a randomiser).

So, I made datasets with different probabilities of hits, all based on a
dataset without a dependent variable, but with a column p (e.g.
participant) and a column i (item), a column cond (condition). This
dataset pretends to be a nicely designed experiment in which 40
p(articipants) receive 40 i(tems) that can come in four conditions
(A,B,C,D); each p receives 10 i's in condition A, 10 in B etc. and the
items are rotated through p's.

In each simulated analysis, I added a dependent variable outcome, with a
probability of a hit (i.e. the value 1). This probability varies (I have
walked through different ones in the simulations), but below you can see
one simulation in which the probability is 0.1, for the whole column
outcome (so without relation to the dependent variable)
> rbinom(1600, 1, 0.1) -> dataset$outcome

> summary(dataset)
        p              i        cond       outcome
  1      :  40   1      :  40   A:400   Min.   :0.00000
  2      :  40   2      :  40   B:400   1st Qu.:0.00000
  3      :  40   3      :  40   C:400   Median :0.00000
  4      :  40   4      :  40   D:400   Mean   :0.09375
  5      :  40   5      :  40           3rd Qu.:0.00000
  6      :  40   6      :  40           Max.   :1.00000
  (Other):1360   (Other):1360

Indeed, the mean value is close to 0.10, 9.375% of the data has outcome
= 1 and the rest outcome = 0.

The model is made as follows:
> glmer(outcome ~ cond + (1|i) + (1|p), data=dataset, family=binomial) -> m

with the output:
> m
Generalized linear mixed model fit by maximum likelihood (Laplace
Approximation) ['glmerMod']
  Family: binomial ( logit )
Formula: outcome ~ cond + (1 | i) + (1 | p)
    Data: dataset
       AIC       BIC    logLik  deviance  df.resid
1005.1915 1037.4581 -496.5958  993.1915      1594
Random effects:
  Groups Name        Std.Dev.
  i      (Intercept) 0.1881
  p      (Intercept) 0.2185
Number of obs: 1600, groups: i, 40; p, 40
Fixed Effects:
(Intercept)        condB        condC        condD
     -2.3476      -0.1291       0.1723       0.1172


Now, the random effect on the intercept for p is on average:
> colMeans(ranef(m)$p)
(Intercept)
0.003661209

.. and for i:
> colMeans(ranef(m)$i)
(Intercept)
  0.00271321


I would expect these random intercepts to be 0. Of course, one analysis
is not enough to conclude there is something wrong, but after iterating
the process I just described 2000 times for each probability of a 1 for
the dependent variable outcome, I still got an average value that was
higher the lower the probability is; the average random effects cross
zero when the hit probability crosses chance (0.5). The problem is that
(as dependent variable = fixed effect + random effect) a random effect
estimate that does not average zero makes the fixed effect estimate
different.

I hope this elaboration helps, but do not hesitate to ask more details!

Kind regards,

Tom

On 25-03-14 09:06, Tibor Kiss wrote:
> Dear Tom,
>
> it is hard to answer your question without the actual output of your model (not of your simulation), i.e. print(summary(model), corr = F). It is also not clear to me what you are actually measuring, i.e. what the "hit" should be. Perhaps you can elaborate.
>
>
> With kind regards
>
> Tibor
>
>
> Am 25.03.2014 um 08:54 schrieb Tom Lentz:
>
>> Dear all,
>>
>> The following question might be due to my poor understanding of logistic regression, in which case I would be very grateful for an explanation or a pointer to reading material.
>>
>> With my current understanding I think that logistic regression as typically done with lmer and family="binomial" (actually calls glmer, as calling lmer is now deprecated) behaves in an unexpected way, because it does not make random effects be near zero but moves them towards chance, i.e. towards positive values if the probability of a hit is below 0.5 and towards negative values if the probability of a hit is above 0.5. At first I thought this was shrinkage, but it does not happen if data is aggregated and a normal linear mixed model fitted to percentages, but I think that is ugly and should lead to worse or equal results, not better ones, because the percentages cannot be normally distributed, especially if they are far from chance.
>>
>> I have discovered this issue with the analysis of eye-tracking data, in which the chance of looking at the target was around 0.25, but the fixed effects in my model were lower than the mean and the random effects for participant and item were not around zero (hence, participants tend to be better than the fixed effect/average and items generally tend to be recognised better than on the fixed effects predicted/average). The result is that the fixed effects estimates are not at the average values, but lower.
>>
>> As my data set might have had a poorly understood conspiracy in it, I simulated data. Every simulated data set had 40 participants and 40 items (easy if you make it up!), but no effect of fixed effects; there was a condition (A, B, C or D) but the outcome was not influenced by this condition. The dependent variable was drawn with rbinom(1600, 1, probability), where probability was varied: 0.1, 0.15, 0.2 up till 0.9.
>>
>> For each probability I ran 2000 analyses with this formula:
>> lmer(outcome ~ cond + (1|i) + (1|p), data=dataset, family = "binomial")
>> and looked at the random effects for item and participants. Indeed, the lower the hit rate (the probability of the dependent variable outcome being TRUE or 1), the higher the average random effect, with a zero average for the random effects only at a 0.5 probability (or 0 logit). A plot can be found at <http://www.hum.uu.nl/medewerkers/t.o.lentz/plotRanefsR3.pdf>.
>>
>> The fixed effect of cond should not be significant, as the data is made up without regard to it. Indeed, at an alpha of 0.05 a spurious significant effect was only found in 4,2 % of the simulations. So, the analyses are not causing errors for hypothesis testing, but the estimates of the random effects are off. Is there a good explanation or is this unexpected behaviour?
>>
>> Version information: I have detected the problem a while ago, still in R 2, but it still happens in R 3.0.3 with lme4 version 1.1-5.
>>
>> Thanks in advance for your help!
>>
>> Kind regards,
>>
>> Tom
>>
>> TO Lentz PhD
>> Postdoctoral Researcher,
>> Parsing and Metrical Structure: Where Phonology Meets Processing
>>
>> Utrecht Institute of Linguistics OTS
>> Utrecht University
>> Trans 10
>> 3512 JK Utrecht
>> Netherlands
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From adianto_pangaribuan at yahoo.com  Sat Mar 22 01:47:33 2014
From: adianto_pangaribuan at yahoo.com (Adianto Pangaribuan)
Date: Fri, 21 Mar 2014 17:47:33 -0700 (PDT)
Subject: [R-sig-ME] compare maximum likelihood and restricted maximum
	likelihood
Message-ID: <1395449253.78352.YahooMailNeo@web160706.mail.bf1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140321/958972ef/attachment.pl>

From ga38tiv at mytum.de  Fri Mar 21 09:00:22 2014
From: ga38tiv at mytum.de (Lionel)
Date: Fri, 21 Mar 2014 09:00:22 +0100
Subject: [R-sig-ME] repeated measures random effects structure
In-Reply-To: <E16ACB3C-3F38-43B8-A76B-81ADCECFA0E5@unr.edu>
References: <E16ACB3C-3F38-43B8-A76B-81ADCECFA0E5@unr.edu>
Message-ID: <532BF196.8090006@tum.de>

Dear Kevin,

The first model correspond to a nested nested random intercept of 
replicates within generation, this means that the model will estimate 
the standard deviation of the intercept between each replicates within 
generation, I guess it will also estimate the overall standard deviation 
of the intercept between the generation.
The second model is a random slope (of generation effect on your 
response)+intercept of replicates, this time the model will estimate the 
standard deviation of the intercept between the replicates as well as 
the standard deviation of slopes of response vs generation between the 
replicates. This modelling structure is similar to a fixed effect linear 
model with one continuous variable, one grouping variable plus their 
interaction.

These two models are therefore rather different, I would argue that your 
first model would be more appropriate for your experimental design, you 
should make sure that the replicate ID are unique over the generation.

Sincerely yours,
Lionel

On 03/21/2014 12:05 AM, Kevin Burls wrote:
> Hello,
>
> I am analyzing the results from an artificial selection experiment and have a question regarding the random effects structure. My experimental design is fairly simple: there are two treatments, with 5 replicates for each treatment, measuring a response from each replicate each generation for 20 generations. I am interested not only in the effects of treatment, but in a main effect of time, well as the interaction (looking for a difference between treatments over time). I have looked over quite a few of the examples regarding the structure for such an experiment and have found two alternative models that seem appropriate, but I am unclear what the difference is. These models are:
>
> lmer1<- Response~treatment*generation + (1 | generation/replicate); a more typical nested effects structure
>
> lmer2<- Response~treatment*generation + (generation | replicate) ; following the sleepstudy design described in section 2 of the 2012 lme4 implementation guide by Dr. Bates.
>
> I am sure part of my confusion is not really understanding what it means to put something in front of the grouping factor, versus a nested design. It seems as though the second one could be more appropriate for a repeated measures framework, but I am at a loss to explain why.
>
> Thanks for your help,
>
> Kevin Burls
>
> Kevin Burls
> Ph.D. candidate
> EECB Program
> University of Nevada, Reno
> kburls at unr.edu<mailto:kburls at unr.edu>
> http://wolfweb.unr.edu/~kburls
> www.nevadabugs.org<http://www.nevadabugs.org>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From tomd792 at gmail.com  Wed Mar 26 19:19:27 2014
From: tomd792 at gmail.com (Tom Davis)
Date: Wed, 26 Mar 2014 19:19:27 +0100
Subject: [R-sig-ME] Wrong convergence warnings with glmer in lme4 version
	1.1-6??
Message-ID: <CA+doL7zex9Fy6RG3cCshNXzUndOc39MefQRBE=t4sf70d-HHaw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140326/fb09af5d/attachment.pl>

From nimrod.rubinstein at gmail.com  Wed Mar 26 17:16:20 2014
From: nimrod.rubinstein at gmail.com (nimrod.rubinstein)
Date: Wed, 26 Mar 2014 12:16:20 -0400
Subject: [R-sig-ME] A mixed effects model for variances of binomial
	probability as a response
Message-ID: <CAEDz9ehmsKL=59RnAtTzEjWxeuqyShf+CQxLcY36s25UMjCk6A@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140326/e5666b1d/attachment.pl>

From nimrod.rubinstein at gmail.com  Wed Mar 26 21:20:11 2014
From: nimrod.rubinstein at gmail.com (nimrod.rubinstein)
Date: Wed, 26 Mar 2014 16:20:11 -0400
Subject: [R-sig-ME] A mixed effects model for variances of binomial
 probability as a response
In-Reply-To: <9B75E7CF385CB94EAD6587DD96AC2D9701075C45C0@SFSMCEXMBX3.sanfordhealth.org>
References: <CAEDz9ehmsKL=59RnAtTzEjWxeuqyShf+CQxLcY36s25UMjCk6A@mail.gmail.com>
	<9B75E7CF385CB94EAD6587DD96AC2D9701075C45C0@SFSMCEXMBX3.sanfordhealth.org>
Message-ID: <CAEDz9ejCP2u0xjUQeeCeoO18jwShP6ER=wd-pk0Xm2JNptK7kA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140326/aa18bc77/attachment.pl>

From bbolker at gmail.com  Wed Mar 26 21:36:48 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 26 Mar 2014 16:36:48 -0400
Subject: [R-sig-ME] Wrong convergence warnings with glmer in lme4
 version 1.1-6??
In-Reply-To: <CA+doL7zex9Fy6RG3cCshNXzUndOc39MefQRBE=t4sf70d-HHaw@mail.gmail.com>
References: <CA+doL7zex9Fy6RG3cCshNXzUndOc39MefQRBE=t4sf70d-HHaw@mail.gmail.com>
Message-ID: <53333A60.6090007@gmail.com>



   Quick answer: these are probably false positives, driven by some
combination of the following:

 * we are using derivative-free methods (bobyqa, Nelder-Mead) rather
than derivative (or approximate-derivative-) based methods (nlminb,
L-BFGS-B) to optimize over the parameters.  Therefore, while using
approximate derivatives to assess convergence is a good idea, we don't
have any formal guarantee of how small the derivatives are supposed to
be (the stopping conditions for the derivative-free methods are not
based explicitly on derivatives, although they should of course be
related to the local derivatives); nor do we know how large a gradient
we should be worried about.
 * it's possible that the finite-difference approximations to the
gradients are themselves inaccurate
 * one flaw of our current approach is that we don't take singular fits
into account: that is, we include even the gradient elements for which
the parameters are up against a boundary (zero variances, or perfect
correlations). (That said, I've seen at least a few cases of relatively
large max-abs-grad values where the fit was *not* singular.)

  We (lme4 maintainers) are sorry for any inconvenience or worry, and
are working to resolve these issues.  However, we'd rather try to
understand what's going on here and convince ourselves that there is
*not* something worrisome going on, rather than just increase the
tolerances for the tests and make everything quiet again ...

  When I run your example (thanks for the reproducible example!) I get

 m1 at optinfo$derivs$gradient

 [1]  0.339026660 -0.008029951  0.145529325  0.093438612  0.123937398
 [6]  0.138252908  0.113161827  0.050304411  0.172043387  0.061015458
[11]  0.122968431  0.159325086  0.117684399  0.072351372  0.134278462
[16]  0.052197579 -0.001719254  0.109679304  0.199987785  0.129893883
[21]  0.138903688  0.024782721  0.047988965  0.028119425  0.096921221

Double-check internal gradient calculation with (slightly more
expensive, and non-boundary-respecting) numDeriv::grad

library(numDeriv)
dd <- update(m1,devFunOnly=TRUE)
grad(dd,unlist(getME(m1,c("theta","beta"))))

[1]  0.33902876 -0.00802679  0.14553678  0.09346970  0.12393419 0.13824924
 [7]  0.11335159  0.05030383  0.17205489  0.06102088  0.12296410  0.15933547
[13]  0.11768797  0.07235296  0.13426145  0.05219938 -0.00172568  0.10990842
[19]  0.19998764  0.12991478  0.13889906  0.02478346  0.04799346  0.02810442
[25]  0.09692275

  Looks sensible.

  I don't have more time to devote this right now, but my next steps
would/will be:

 * restart the optimization from the fitted optimum (possibly? using
refit(), or update(m1,start=getME(m1,c("theta","beta")))) and see if
that makes the max grad smaller
 * try a range of optimizers, especially nlminb, as described here:
http://stackoverflow.com/questions/21344555/convergence-error-for-development-version-of-lme4/21370041#21370041

  Ben Bolker






On 14-03-26 02:19 PM, Tom Davis wrote:
> Dear lme4 experts,
> 
> Yesterday, I ran the code for two published papers (de Boeck et al.,2011;
> de Boeck and Partchev, 2012) on psychometric modeling with glmer in lme4
> version 1.1-6 and the vast majority of the models I ran produce convergence
> warnings (even the simple ones).
> 
> For instance, the basic Rasch model in de Boeck et al. (2011) yields:
> 
>> ## our example data set is bundled with lme4
>> data("VerbAgg")
>>
>> ## A first example: A Rasch model with fixed item effects and random
> person effects
>> m1 = glmer(r2 ~ 0 + item + (1|id), data=VerbAgg, family="binomial",
> + control=glmerControl(optCtrl=list(maxfun=100000)))
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 0.308607 (tol = 0.001)
> 
> I am a bit puzzeled because, to my knowledge, especially the models for the
> VerAgg data (included in lme4) have been checked in many other programs
> (also ltm in R) and I heard that glmer produces results that are valid and
> consistent with SAS, HLM, ltm, and so on. However, this dataset produces
> convergence warnings even though the models are comparatively simple for
> psychometric research (basic Rasch and LLTM) and the estimates all seem
> reasonable.
> 
> I also tried some datasets in the ltm package. Again, convergence warnings
> (the mobility data). The estimates are close to what ltm gives me.
> 
> Even when I simulate data from a basic Rasch model using the rmvlogis
> function in ltm with a couple of extreme item parameters (which occur in
> psychometric tests), I also get these warnings. This happens despite glmer
> seems to recover the true values very well. The "gradient" errors and the
> hessian errors tend to increase with sample size and when I use
> binomial("probit") instead of binomial("logit"). It also seems like vector
> random effects produce many warnings. optimizer="bobqya" tends to reduce
> the warnings but not consistently. To me, it seems like the warnings occur
> randomly all over the place. Sometimes glmer "converges" (no warnings) with
> one optimizer setting and the values that are very close to the true
> values. However, with another optimizer setting, one gets practically
> exactly the same estimates and virtually the same likelihood value but a
> warning. I really do not understand what is going on.
> 
> I had no warnings using version 1.0-5 and version 1.0-6 so this seems to be
> a recent problem of lme4?
> 
> Is it best to ignore all these convergence warnings for now? Should I
> switch back to an older version of lme4 to avoid this problem? Should I
> generally avoid using large datasets with lme4?
> 
> Many thanks in advance,
> Tom
> 
> 
> Papers (scripts are online):
> De Boeck, P., Bakker, M., Zwitser, R., Nivard, M., Hofman, A., Tuerlinckx,
> F., & Partchev, I. (2011). The estimation of item response models with the
> lmer function from the lme4 package in R. Journal of Statistical Software,
> 39, 1-28.  http://www.jstatsoft.org/v39/i12
> 
> De Boeck, P., & Partchev, I. (2012). IRTrees: Tree-based item response
> models of the GLMM family. Journal of Statistical Software, 48, 1-28.
> http://www.jstatsoft.org/v48/c01/
> 
> 
> Simulated data:
> 
>> library("reshape")
>> library("ltm")
>> library("lme4")
>> set.seed("12345")
>>
>> simrasch<-data.frame(rmvlogis(200, cbind(seq(-5, 5, 0.5), 1)))
>> rasch(simrasch,IRT.param=F)
> 
> Call:
> rasch(data = simrasch, IRT.param = F)
> 
> Coefficients:
>     X1      X2      X3      X4      X5      X6      X7      X8      X9
> X10     X11     X12
> 17.342   5.157   5.157   3.441   3.141   2.100   2.193   1.969   1.484
> 0.717   0.054  -0.347
>    X13     X14     X15     X16     X17     X18     X19     X20
> X21       z
> -0.874  -1.415  -2.103  -2.696  -3.065  -3.152  -4.453  -4.216  -5.175
> 1.091
> 
> Log.Lik: -1329.186
> 
>>
>> simrasch$person = rownames(simrasch)
>> simraschlong = melt(simrasch, id = "person")
>> glmer(value ~ 0 + variable + (1|person), data=simraschlong,
> family=binomial)
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: value ~ 0 + variable + (1 | person)
>    Data: simraschlong
>       AIC       BIC    logLik  deviance  df.resid
>  2702.484  2842.026 -1329.242  2658.484      4178
> Random effects:
>  Groups Name        Std.Dev.
>  person (Intercept) 1.091
> Number of obs: 4200, groups:  person, 200
> Fixed Effects:
>  variableX1   variableX2   variableX3   variableX4   variableX5
> variableX6   variableX7
>    19.04000      5.14239      5.13514      3.43274      3.13736
> 2.10085      2.19335
>  variableX8   variableX9  variableX10  variableX11  variableX12
> variableX13  variableX14
>     1.97086      1.48698      0.71828      0.05206     -0.35255
> -0.88099     -1.42318
> variableX15  variableX16  variableX17  variableX18  variableX19
> variableX20  variableX21
>    -2.11070     -2.70069     -3.06758     -3.15545     -4.44743
> -4.21083     -5.16615
> Warning messages:
> 1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 0.135068 (tol = 0.001)
> 2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge: degenerate  Hessian with 1 negative eigenvalues
>> glmer(value ~ 0 + variable + (1|person), data=simraschlong,
> family=binomial,
> + control=glmerControl(optimizer="bobyqa"))
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: value ~ 0 + variable + (1 | person)
>    Data: simraschlong
>       AIC       BIC    logLik  deviance  df.resid
>  2702.482  2842.025 -1329.241  2658.482      4178
> Random effects:
>  Groups Name        Std.Dev.
>  person (Intercept) 1.09
> Number of obs: 4200, groups:  person, 200
> Fixed Effects:
>  variableX1   variableX2   variableX3   variableX4   variableX5
> variableX6   variableX7
>    17.75458      5.14416      5.14417      3.43797      3.13991
> 2.10444      2.19676
>  variableX8   variableX9  variableX10  variableX11  variableX12
> variableX13  variableX14
>     1.97412      1.48907      0.72049      0.05389     -0.34882
> -0.87841     -1.42030
> variableX15  variableX16  variableX17  variableX18  variableX19
> variableX20  variableX21
>    -2.10751     -2.69767     -3.06471     -3.15126     -4.44429
> -4.20819     -5.16356
> 
> 
>> simrasch<-data.frame(rmvlogis(1000, cbind(seq(-5, 5, 0.5), 1)))
>> rasch(simrasch,IRT.param=F)
> 
> Call:
> rasch(data = simrasch, IRT.param = F)
> 
> Coefficients:
>     X1      X2      X3      X4      X5      X6      X7      X8      X9
> X10     X11     X12
>  5.195   4.422   3.868   3.599   3.192   2.558   2.211   1.605   0.984
> 0.559   0.003  -0.398
>    X13     X14     X15     X16     X17     X18     X19     X20
> X21       z
> -0.985  -1.590  -1.994  -2.401  -2.876  -3.411  -3.815  -4.683  -4.832
> 1.014
> 
> Log.Lik: -6874.631
> 
>>
>> simrasch$person = rownames(simrasch)
>> simraschlong = melt(simrasch, id = "person")
>> glmer(value ~ 0 + variable + (1|person), data=simraschlong,
> family=binomial)
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: value ~ 0 + variable + (1 | person)
>    Data: simraschlong
>       AIC       BIC    logLik  deviance  df.resid
> 13793.707 13968.657 -6874.853 13749.707     20978
> Random effects:
>  Groups Name        Std.Dev.
>  person (Intercept) 1.014
> Number of obs: 21000, groups:  person, 1000
> Fixed Effects:
>  variableX1   variableX2   variableX3   variableX4   variableX5
> variableX6   variableX7
>    5.182206     4.415991     3.871713     3.599747     3.191841
> 2.561370     2.219830
>  variableX8   variableX9  variableX10  variableX11  variableX12
> variableX13  variableX14
>    1.611983     0.992610     0.564159     0.003716    -0.398537
> -0.987771    -1.591732
> variableX15  variableX16  variableX17  variableX18  variableX19
> variableX20  variableX21
>   -1.998437    -2.400455    -2.870716    -3.408691    -3.806151
> -4.672642    -4.813643
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 0.907091 (tol = 0.001)
>> glmer(value ~ 0 + variable + (1|person), data=simraschlong,
> family=binomial,
> + control=glmerControl(optimizer="bobyqa"))
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: value ~ 0 + variable + (1 | person)
>    Data: simraschlong
>       AIC       BIC    logLik  deviance  df.resid
> 13793.696 13968.646 -6874.848 13749.696     20978
> Random effects:
>  Groups Name        Std.Dev.
>  person (Intercept) 1.014
> Number of obs: 21000, groups:  person, 1000
> Fixed Effects:
>  variableX1   variableX2   variableX3   variableX4   variableX5
> variableX6   variableX7
>    5.184774     4.414100     3.862784     3.594478     3.190096
> 2.559954     2.214893
>  variableX8   variableX9  variableX10  variableX11  variableX12
> variableX13  variableX14
>    1.610161     0.988363     0.562450     0.002823    -0.400700
> -0.990108    -1.595035
> variableX15  variableX16  variableX17  variableX18  variableX19
> variableX20  variableX21
>   -1.997929    -2.403493    -2.875794    -3.408200    -3.809483
> -4.674454    -4.822606
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 0.00124419 (tol = 0.001)
>>
>>
>>
> 
> 
>> set.seed("12345")
>> simrasch<-data.frame(rmvlogis(200, cbind(c(seq(-3, 3, 1),10), 1)))
>> rasch(simrasch,IRT.param=F)
> 
> Call:
> rasch(data = simrasch, IRT.param = F)
> 
> Coefficients:
>      X1       X2       X3       X4       X5       X6       X7
> X8        z
>   3.226    1.820    1.461    0.044   -0.706   -1.494   -2.771  -10.454
> 0.801
> 
> Log.Lik: -647.099
> 
>>
>> simrasch$person = rownames(simrasch)
>> simraschlong = melt(simrasch, id = "person")
>> glmer(value ~ 0 + variable + (1|person), data=simraschlong,
> family=binomial)
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: value ~ 0 + variable + (1 | person)
>    Data: simraschlong
>       AIC       BIC    logLik  deviance  df.resid
> 1312.7956 1361.1955 -647.3978 1294.7956      1591
> Random effects:
>  Groups Name        Std.Dev.
>  person (Intercept) 0.7881
> Number of obs: 1600, groups:  person, 200
> Fixed Effects:
> variableX1  variableX2  variableX3  variableX4  variableX5  variableX6
> variableX7  variableX8
>    3.21214     1.82080     1.46473     0.04478    -0.71031    -1.49811
> -2.76154   -27.10659
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge: degenerate  Hessian with 1 negative eigenvalues
>> glmer(value ~ 0 + variable + (1|person), data=simraschlong,
> family=binomial,
> + control=glmerControl(optimizer="bobyqa"))
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: value ~ 0 + variable + (1 | person)
>    Data: simraschlong
>       AIC       BIC    logLik  deviance  df.resid
> 1312.7956 1361.1955 -647.3978 1294.7956      1591
> Random effects:
>  Groups Name        Std.Dev.
>  person (Intercept) 0.7881
> Number of obs: 1600, groups:  person, 200
> Fixed Effects:
> variableX1  variableX2  variableX3  variableX4  variableX5  variableX6
> variableX7  variableX8
>    3.21213     1.82080     1.46473     0.04478    -0.71032    -1.49811
> -2.76154   -19.56371
> 
> 
>> set.seed("12345")
>> simrasch<-data.frame(rmvlogis(10000, cbind(c(seq(-3, 3, 1),10), 1)))
>> rasch(simrasch,IRT.param=F)
> 
> Call:
> rasch(data = simrasch, IRT.param = F)
> 
> Coefficients:
>     X1      X2      X3      X4      X5      X6      X7      X8       z
>  3.018   2.026   1.011   0.012  -1.013  -1.941  -2.977  -9.696   0.986
> 
> Log.Lik: -31914.71
> 
>>
>> simrasch$person = rownames(simrasch)
>> simraschlong = melt(simrasch, id = "person")
>> glmer(value ~ 0 + variable + (1|person), data=simraschlong,
> family=binomial)
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: value ~ 0 + variable + (1 | person)
>    Data: simraschlong
>       AIC       BIC    logLik  deviance  df.resid
>  63888.47  63972.08 -31935.24  63870.47     79991
> Random effects:
>  Groups Name        Std.Dev.
>  person (Intercept) 0.9739
> Number of obs: 80000, groups:  person, 10000
> Fixed Effects:
> variableX1  variableX2  variableX3  variableX4  variableX5  variableX6
> variableX7  variableX8
>    3.01884     2.03562     1.02945     0.02299    -1.01066    -1.93548
> -2.95234    -9.44503
> Warning messages:
> 1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 25.6865 (tol = 0.001)
> 2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge: degenerate  Hessian with 1 negative eigenvalues
>> glmer(value ~ 0 + variable + (1|person), data=simraschlong,
> family=binomial,
> + control=glmerControl(optimizer="bobyqa"))
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: value ~ 0 + variable + (1 | person)
>    Data: simraschlong
>       AIC       BIC    logLik  deviance  df.resid
>  63887.88  63971.49 -31934.94  63869.88     79991
> Random effects:
>  Groups Name        Std.Dev.
>  person (Intercept) 0.9737
> Number of obs: 80000, groups:  person, 10000
> Fixed Effects:
> variableX1  variableX2  variableX3  variableX4  variableX5  variableX6
> variableX7  variableX8
>    3.00598     2.02857     1.01952     0.01191    -1.02146    -1.94484
> -2.96534    -9.65211
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 0.0128742 (tol = 0.001)
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From rodjunper at yahoo.com.br  Thu Mar 27 03:25:47 2014
From: rodjunper at yahoo.com.br (Rodrigo Junqueira Pereira)
Date: Wed, 26 Mar 2014 22:25:47 -0400
Subject: [R-sig-ME] Help with a mixed model analysis
Message-ID: <AB30255D-4142-4FC7-85EA-369E4F072A21@yahoo.com.br>

Dear all.

I?m new on lme4 package and I would like to run a mixed model analysis on repeated measures longitudinal data. I have the following data.frame with the columns:

weight: weight of the animal;
sex: sex of the calf (2 classes);
age: age of the animal;
treatment: level of a protein supplement (levels: 0, 0.5, 1, 1.5, 2);
day: days to calving of the animal;

I would like to study the effect of the levels of the protein supplement (treatment) on the weight trend (quadratic regression) according to days to calving. However, is important to consider the additional fixed effects of the sex of the calf and the age of the animal (linear regression: intercept + slope) on the weight.

I have unequal number of replicates by treatment and unequal number of records by animal.

Could anyone help me with the model syntax on lme4 package?

Is it possible to get p-values for treatment comparisons?

??
Rodrigo.

From bbolker at gmail.com  Thu Mar 27 03:45:18 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 27 Mar 2014 02:45:18 +0000 (UTC)
Subject: [R-sig-ME]
	=?utf-8?q?compare_maximum_likelihood_and_restricted_ma?=
	=?utf-8?q?ximum=09likelihood?=
References: <1395449253.78352.YahooMailNeo@web160706.mail.bf1.yahoo.com>
Message-ID: <loom.20140327T033920-442@post.gmane.org>

Adianto Pangaribuan <adianto_pangaribuan at ...> writes:

> 
> I want to compare maximum likelihood and restricted maximum 
> likelihood for linear mixed effects model.
> Can you give me suggest how to compare the two estimation?
> If I use AIC, BIC and value of log-likelihood, the
> best estimation is MLE. But, I think i am wrong with it.

  This doesn't sound on the face of it like a sensible question.
ML and REML are two different approaches to estimation -- they're not
directly comparable.  Depending on your background I don't know what
analogy will make sense, but "which fits better, least-squares regression
or logistic regression?" might be a similar question.  They're 
different criteria.

   REML generally gives a less biased estimate of the variance components.
   ML gives estimates that are comparable across models with different
fixed-effect components.

   It's conceivable that a more extensive description of what you're
trying to do would convince us that this is indeed a sensible question
(and provide you with more useful feedback) -- although I doubt it.

  sincerely
    Ben Bolker


From bbolker at gmail.com  Thu Mar 27 04:01:26 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 26 Mar 2014 23:01:26 -0400
Subject: [R-sig-ME] Help with a mixed model analysis
In-Reply-To: <AB30255D-4142-4FC7-85EA-369E4F072A21@yahoo.com.br>
References: <AB30255D-4142-4FC7-85EA-369E4F072A21@yahoo.com.br>
Message-ID: <53339486.2050704@gmail.com>

On 14-03-26 10:25 PM, Rodrigo Junqueira Pereira wrote:
> Dear all.
> 
> I?m new on lme4 package and I would like to run a mixed model
> analysis on repeated measures longitudinal data. I have the following
> data.frame with the columns:
> 
> weight: weight of the animal; sex: sex of the calf (2 classes); age:
> age of the animal; treatment: level of a protein supplement (levels:
> 0, 0.5, 1, 1.5, 2); day: days to calving of the animal;
> 
> I would like to study the effect of the levels of the protein
> supplement (treatment) on the weight trend (quadratic regression)
> according to days to calving. However, is important to consider the
> additional fixed effects of the sex of the calf and the age of the
> animal (linear regression: intercept + slope) on the weight.
> 
> I have unequal number of replicates by treatment and unequal number
> of records by animal.
> 
> Could anyone help me with the model syntax on lme4 package?

  The simplest model might be

weight ~ age*sex+poly(days,2)*treatment + (1|animal)

  However, there are a lot of things you might need to think about.

* I'm not sure what you mean by "the effect of the levels of the ..
treatment ... on the weight trend according to days to calving".
Maybe this is poly(days,2)*treatment, as above (the pattern with respect
to days is quadratic, but the slope and quadratic trend could change
linearly with the treatment).  But maybe you meant that the pattern is
quadratic with respect to the level of treatment?
* If you think the pattern of change with respect to treatment is
complex you might make it a categorical variable (factor), possibly an
ordered factor or a factor with successive-difference contrasts
(MASS::contr.sdif).
* you might also consider generalized additive mixed models (mgcv package)
* in principle any predictor that varies within animal could/should be
considered as interacting with the random effect of animal: in this case
I think you would want (poly(days,2)|animal) instead of (1|animal) (see
Schielzeth and Forstmeier 2009).

> 
> Is it possible to get p-values for treatment comparisons?

  See the ?pvalues help page in the lme4 package.

> 
> ?? Rodrigo. _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From rdiaz02 at gmail.com  Thu Mar 27 09:39:09 2014
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Thu, 27 Mar 2014 09:39:09 +0100
Subject: [R-sig-ME] A mixed effects model for variances of
	binomialprobability as a responsee
In-Reply-To: <CAEDz9ehmsKL=59RnAtTzEjWxeuqyShf+CQxLcY36s25UMjCk6A@mail.gmail.com>
References: <CAEDz9ehmsKL=59RnAtTzEjWxeuqyShf+CQxLcY36s25UMjCk6A@mail.gmail.com>
Message-ID: <87ppl8c9xu.fsf@gmail.com>

Dear Nimrod,

Just my 2 cents (even less than 2):

On Thu, 01-01-1970, at 01:00, nimrod.rubinstein <nimrod.rubinstein at gmail.com> wrote:
> Hi all,
>
> I have the following data:
>
> 50 subjects that belong to 2 groups (25 in each group). For each subject I
> have replicated measurements (in the range of 15-30 replicates per subject)
> of the number of trails as well as the number successes out of these
> trails. Each single trial in each single replicate for each subject is
> assumed to be a Bernoulli experiment, therefore the number of successes in
> each replicate is assumed to be a binomially distributed random variable.
> To complicate things even more, for each subject from each group I have not
> 1 but 3 series of these replicated measurements, from 3 different
> environments (the 3 environments are the same for all subjects).
>
> A few example lines of how my data looks like:
> subject group environment     number.trails  number.successes
>
>     S1    G1          E1 10100,13209,17621  8500,11622,14684
>
>     S1    G1          E2 13322,12225,10024  10351,10222,7951
>
>     S1    G1          E3  9812,13500,13214  7245,10622,10684
>
>     .
>
>     .
>
>     .
>
>     S50    G2          E1 60547,75345,68412 12014,13214,15244
>
>     S50    G2          E2 50897,62541,61247 10141,13251,30645
>
>     S50    G2          E3 59214,84264,74156 17245,16425,11123
>
>
> Where the comma separated number of successes correspond to the comma
> separated number of trials in each replicate.
>
> I want to test whether the variances of the success probabilities of
> subjects from group G1 are significantly smaller than the variances of the
> success probabilities of subjects from group G2.
>

But, as you say in point 1. below (and in another reply), if this is really
binomial, then you already know that if probabilities in G1 are different
from probabilities in G2 your variances will differ.

So I am not sure I even understand this hypothesis/question.


> In other words, for each subject in each replicate a success probability
> can be estimated from the observed data. Then the variance of those success
> probabilities over all replicates from a specific environment can be
> estimated. I would like to know whether these variances are smaller in
> subjects from G1 vs. subjects from G2.
>
> The issues are:
>
>
>    1. The variance of a success probability depends on the success
>    probability itself (natural to binomially distributed data). I need to
>    account for this effect.
>    2. The number of trails (although at least in the hundreds for all
>    subjects) varies several orders of magnitude among subjects. That by itself
>    affects the accuracy of estimating the success probability, but in
>    addition, I cannot rule out an inherent dependency between the number of
>    observed trials and the success probability (there is no control over the
>    number of trials in the experiment - it is observed). In other words,
>    having a lower number of trails by itself may affect the success
>    probability.
>

That last issue seems very problematic to me. If I understand correctly, we
could think of a perverse situation where, say, you can never observe low
success probabilities if those are related to number of trials around 0.


Can you do a simple plot of probability of success (or its logit) vs number
of trials, to rule out perverse scenarios? Is probability of success always
larger than 0? And number of trials always sufficiently different from 0?


And you say "varies by subject", so the number of trials is always the same
for a subject? And the potential relationship prob <-> number of trials is
due to among-subject variation? So you could approach the problem via a
random effect for subject? If you compare G1 and G2 for number of trials:
are there differences?





>
> I though a simple way to test my hypothesis is to compute the success
> probability in each replicate as number.successes/number.trails, and then
> the sample mean and sample variance of these success probabilities over all
> replicates, as well as the mean number of trials over all replicates (added
> to my data as sample.mean, sample.var and mean.trials fields). Then I can
> use the lmer function in the lme4 package this way:
>
> null.model = lmer(sample.var ~ sample.mean + mean.trials + (1|environment),
> my.data, REML = F)
> alt.model = lmer(sample.var ~ sample.mean + mean.trials + group +
> (1|environment), my.data, REML = F)
>
> So fixed effects are: sample.mean, mean.trials and group, and random
> effects is environment since the three environments in my data are simply a
> random sample from many other possible environments.


I do not see how this would answer your questions, given the above
concerns/issues and other possible approaches (just compare prob. between
G1 and G2, accounting for environment).


Best,


R.


>
> To test whether group has a significant effect I'll do:
>
> anova(null.model, alt.model).
>
> My concerns are:
>
>
>    1. Should I add subject as a random effect? My intuition is not to since
>    subjects are unique to groups, which is the effect I'm after. However, I do
>    want to account for the fact that sample variances of the same subject from
>    the three different environments are correlated.
>    2. The response - sample variances, cannot be assumed to follow a normal
>    distribution (they only take positive values to begin with and as far as I
>    know (n-1)*sample.variance/population.variance ~ chi.square with df = n-1)
>    and therefore the assumptions of a linear mixed effects model do not
>    strictly hold. Is there a generalized linear model that would be
>    appropriate for this?
>
> Thanks a lot,
> nimrod
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Ramon Diaz-Uriarte
Department of Biochemistry, Lab B-25
Facultad de Medicina
Universidad Aut?noma de Madrid 
Arzobispo Morcillo, 4
28029 Madrid
Spain

Phone: +34-91-497-2412

Email: rdiaz02 at gmail.com
       ramon.diaz at iib.uam.es

http://ligarto.org/rdiaz


From t.o.lentz at uu.nl  Thu Mar 27 15:33:56 2014
From: t.o.lentz at uu.nl (Tom Lentz)
Date: Thu, 27 Mar 2014 15:33:56 +0100
Subject: [R-sig-ME] Random effects of logistic regression: bias towards
 the mean?
In-Reply-To: <074505DA-4E81-457F-B519-9C80F037943C@linguistics.rub.de>
References: <53313623.6000100@uu.nl>
	<A27911B3-0AB3-4E57-993A-611F09C12ECC@linguistics.rub.de>
	<5331A1A7.4040206@uu.nl>
	<1A2801E8-34C0-4573-AADD-7D94E14092E9@linguistics.rub.de>
	<5331BF53.5000300@uu.nl>
	<074505DA-4E81-457F-B519-9C80F037943C@linguistics.rub.de>
Message-ID: <533436D4.20405@uu.nl>

Dear Tibor,

Thanks for your answer again.

Here are replies to your remarks:
 > w.r.t. the fixed values the model is obviously degenerate, since none of
 > them is significant. I would suggest that you simulate without fixed
 > effects. (Your intercept includes non-significant condition A.)
So, I did this (overnight) and the result is the same; the random 
effects are only on average zero when the probability of the dependent 
variable is 0.5 (i.e., the logit is 0). With lower probability, the 
random effects are on average positive, with higher probability, they 
are negative.

 > I must also admit that I still do not understand the purpose of your
 > simulation. I an actual experiment, you wanted to determine the
 > influence of items and subjects (or something else, I e.g. test for the
 > influence of individual words). With the regard to logic regression (and
 > intercept random effects), the subjects or items would show that the
 > logistic curve is shifted on the x-axis.
I'm sorry that my question is not clear. What I was trying to test was 
whether the random effects are zero on average, as I think (1) they 
should be and (2) that they might not be. This is of importance (to me) 
because (in other models I made of real data) the estimates for the 
fixed effects were structurally too low; the fitted values were still 
right because the added random effect for item and participant was more 
often positive than negative. So, if on a certain level of factors the 
fixed effect estimate is e.g. 20%, the actual data is at 23%, but most 
participants are slightly better than 20% (average random effect of 
participant is above 0), and the same happens for items. I would rather 
see the fixed effect to be similar to the mean and I was not sure if my 
expectations/hopes are invalid or if the models are not estimated 
correctly (and if so, how that could be solved).

 > Also, in linguistics, you would
 > normally try to identify which subset of the random effects play an
 > actual role (by determining their individual standard deviation).
Yes, this is an extra check on model validity, but not the main goal of 
my analysis. I hoped all items (in my real experiments, words to be 
recognised) would be comparable and hypothesis testing of the condition 
fixed effects should show whether fixed effects actually exist, taking 
the random effects into account. I could look at which words are 
recognised better than average, but if I take a positive random effect 
to indicate this, most of the words are recognised better than average 
(when I assume the fixed effects represent mean values)

I can illustrate my problem by showing one of the models, with a hit 
probability of 0.2 when generating, which resulted in 20.19% of the 
observations to be hits.

The model is the following:
print(summary(m), corr=F)
Generalized linear mixed model fit by maximum likelihood (Laplace 
Approximation) ['glmerMod']
  Family: binomial ( logit )
Formula: outcome ~ 1 + (1 | i) + (1 | p)
    Data: dataset

      AIC      BIC   logLik deviance df.resid
   1615.5   1631.7   -804.8   1609.5     1597

Scaled residuals:
     Min      1Q  Median      3Q     Max
-0.5106 -0.5038 -0.5011 -0.4971  2.0172

Random effects:
  Groups Name        Variance Std.Dev.
  i      (Intercept) 0.000000 0.00000
  p      (Intercept) 0.005559 0.07456
Number of obs: 1600, groups: i, 40; p, 40

Fixed effects:
             Estimate Std. Error z value Pr(>|z|)
(Intercept) -1.37628    0.06434  -21.39   <2e-16 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1


The random effect for item is now zero, as hoped, but for participant it 
is not:
colMeans(ranef(m)$p)
  (Intercept)
5.537502e-05

The intercept fixed effect in the model is:
inv.logit(-1.37628)
[1] 0.2016071
which is close to 0.2019, but when the random effect is added:
inv.logit(-1.37628 + colMeans(ranef(m)$p))
(Intercept)
    0.201616
the estimate is closer to the mean probability.

The same would also hold in logit space:
logit(0.2019)
-1.374461
while the fixed effect is -1.37628 (too low), but with the positive 
random effect for p it is closer: -1.376225

Although this effect is small, it is consistent (this is why I simulated 
making such models 2000 times, to make sure it is not a fluke of one 
particular model).

Thanks again for your help and your time, I look forward to your response!

Kind regards,

Tom


On 26-03-14 07:45, Tibor Kiss wrote:
> Hi Tom,
>

>
>
>  From this background, I do not understand what you wanted to gain by
> your simulation.
>
> Perhaps my answer is stll of use.
>
> With kind regards
>
> Tibor
>
> qrcode.png <http://www.linguistics.rub.de/~kiss> 		
> *Prof. Dr. Tibor Kiss <mailto:tibor at linguistics.rub.de >*,
> Sprachwissenschaftliches Institut
> Ruhr-Universit?t Bochum<http://www.linguistics.rub.de>D-44780 Bochum
> Office: +49-234-322-5114 <tel:+49-234-322-5114>
>
>
>
> Am 25.03.2014 um 18:39 schrieb Tom Lentz <t.o.lentz at uu.nl
> <mailto:t.o.lentz at uu.nl>>:
>
>> Sure!
>>
>> Generalized linear mixed model fit by maximum likelihood (Laplace
>> Approximation) ['glmerMod']
>> Family: binomial ( logit )
>> Formula: outcome ~ cond + (1 | i) + (1 | p)
>>   Data: dataset5
>>
>>     AIC      BIC   logLik deviance df.resid
>>  1005.2   1037.5   -496.6    993.2     1594
>>
>> Scaled residuals:
>>    Min      1Q  Median      3Q     Max
>> -0.3882 -0.3306 -0.3131 -0.2919  3.7239
>>
>> Random effects:
>> Groups Name        Variance Std.Dev.
>> i      (Intercept) 0.03539  0.1881
>> p      (Intercept) 0.04775  0.2185
>> Number of obs: 1600, groups: i, 40; p, 40
>>
>> Fixed effects:
>>            Estimate Std. Error z value Pr(>|z|)
>> (Intercept)  -2.3476     0.1861 -12.617   <2e-16 ***
>> condB        -0.1291     0.2541  -0.508    0.611
>> condC         0.1723     0.2393   0.720    0.471
>> condD         0.1172     0.2417   0.485    0.628
>> ---
>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>
>> On 25-03-14 16:51, Tibor Kiss wrote:
>>> Hi,
>>>
>>> could you kindly output print(summary(m), corr = F).
>>>
>>> Best
>>>
>>> Tibor
>>>
>>>
>>> qrcode.png <http://www.linguistics.rub.de/~kiss>
>>> *Prof. Dr. Tibor Kiss <mailto:tibor at linguistics.rub.de >*,
>>> Sprachwissenschaftliches Institut
>>> Ruhr-Universit?t Bochum<http://www.linguistics.rub.de>D-44780 Bochum
>>> Office: +49-234-322-5114 <tel:+49-234-322-5114>
>>>
>>>
>>>
>>> Am 25.03.2014 um 16:32 schrieb Tom Lentz <t.o.lentz at uu.nl
>>> <mailto:t.o.lentz at uu.nl>
>>> <mailto:t.o.lentz at uu.nl>>:
>>>
>>>> > m
>>>> Generalized linear mixed model fit by maximum likelihood (Laplace
>>>> Approximation) ['glmerMod']
>>>> Family: binomial ( logit )
>>>> Formula: outcome ~ cond + (1 | i) + (1 | p)
>>>>  Data: dataset
>>>>     AIC       BIC    logLik  deviance  df.resid
>>>> 1005.1915 1037.4581 -496.5958  993.1915      1594
>>>> Random effects:
>>>> Groups Name        Std.Dev.
>>>> i      (Intercept) 0.1881
>>>> p      (Intercept) 0.2185
>>>> Number of obs: 1600, groups: i, 40; p, 40
>>>> Fixed Effects:
>>>> (Intercept)        condB        condC        condD
>>>>   -2.3476      -0.1291       0.1723       0.1172
>>>
>


From nimrod.rubinstein at gmail.com  Thu Mar 27 00:45:12 2014
From: nimrod.rubinstein at gmail.com (nimrod.rubinstein)
Date: Wed, 26 Mar 2014 19:45:12 -0400
Subject: [R-sig-ME] A mixed effects model for variances of binomial
 probability as a response
In-Reply-To: <9B75E7CF385CB94EAD6587DD96AC2D9701075C4662@SFSMCEXMBX3.sanfordhealth.org>
References: <CAEDz9ehmsKL=59RnAtTzEjWxeuqyShf+CQxLcY36s25UMjCk6A@mail.gmail.com>
	<9B75E7CF385CB94EAD6587DD96AC2D9701075C45C0@SFSMCEXMBX3.sanfordhealth.org>
	<CAEDz9ejCP2u0xjUQeeCeoO18jwShP6ER=wd-pk0Xm2JNptK7kA@mail.gmail.com>
	<9B75E7CF385CB94EAD6587DD96AC2D9701075C4662@SFSMCEXMBX3.sanfordhealth.org>
Message-ID: <CAEDz9ehq==y9yzDWwS=1h+8z9vPUw_cAGJWNixnRwfFwOQGRyA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140326/0b2a9629/attachment.pl>

From adianto_pangaribuan at yahoo.com  Thu Mar 27 02:36:13 2014
From: adianto_pangaribuan at yahoo.com (Adianto Pangaribuan)
Date: Wed, 26 Mar 2014 18:36:13 -0700 (PDT)
Subject: [R-sig-ME] comparison maximum likelihood and restricted maximum
	likelihood
Message-ID: <1395884173.55288.YahooMailNeo@web160702.mail.bf1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140326/bf16bd69/attachment.pl>

From thomas.worthington at okstate.edu  Thu Mar 27 17:23:54 2014
From: thomas.worthington at okstate.edu (Worthington, Thomas A)
Date: Thu, 27 Mar 2014 16:23:54 +0000
Subject: [R-sig-ME] Temporal correlation structure in mixed model
Message-ID: <C1A5238848713043B7C18ED38FFEF1F812B350D8@STWMB01.ad.okstate.edu>

Dear All 

I'm trying to run a mixed model on ecology data. The setup is as follows, a group of fish are placed in a flume and attempt to pass a barrier. The dependent variable is the number of approaches to pass the barrier per minute of the trial, with each trial lasting 10 minutes. There  are two configurations of the barrier.  There are 20 trials in total, 10 for each setup. The aim of the study is to see whether the number of approaches in minute 1, minute 2, minute 3 etc.  is different between the two setups. Therefore my model is of the form Approaches ~ Setup + Minute + Setup * Minute, where minute (10 levels) and setup  (2 levels) are both factors.  

I have three potential models and am slightly confused over the model specification, I believe I need to include a correlation structure to deal with temporal autocorrelation but am unclear over whether a random effect is needed instead or both are required. I have included a variance structure to deal with the issue of homogeneity of variance in the data. The three models are as follows         

test1 <- gls(Approaches ~ Setup * Minute, data=data, correlation = corAR1(form =~ Minute|Trial), weights = varIdent(form=~ 1| Minute * Setup), control=list(opt="optim"))

test2 <- lme(Approaches ~ Setup * Minute, data=data, random = ~ Minute | Trial, weights = varIdent(form=~ 1| Minute * Setup), control=list(opt="optim"))

test3 <- lme(Approaches ~ Setup * Minute, data=data, random = ~ Minute | Trial, weights = varIdent(form=~ 1| Minute * Setup), correlation = corAR1(form =~ Minute|Trial), control=list(opt="optim"))

I believe (maybe wrongly) that Test 1 and 2 would be broadly similar, but if some could suggest the most sensible model I would greatly appreciate it 

Best wishes

Tom


From bbolker at gmail.com  Thu Mar 27 17:27:45 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 27 Mar 2014 12:27:45 -0400
Subject: [R-sig-ME] comparison maximum likelihood and restricted maximum
 likelihood
In-Reply-To: <1395884173.55288.YahooMailNeo@web160702.mail.bf1.yahoo.com>
References: <1395884173.55288.YahooMailNeo@web160702.mail.bf1.yahoo.com>
Message-ID: <53345181.2020201@gmail.com>

On 14-03-26 09:36 PM, Adianto Pangaribuan wrote:
> I want to compare estimation MLE and RMLE for Linear Mixed Effects
> Models. I use criteria like AIC, BIC, and log-likelihood. I get the
> result MLE is the best estimation. But, I think i'm wrong to compare
> the estimations with criteria. Can you give the solution how I can
> compare the estimation and get the best estimations for my data?
> 
> Thank you.
> 
> [[alternative HTML version deleted]]

  Didn't you ask this already, and didn't I already answer you?

http://article.gmane.org/gmane.comp.lang.r.lme4.devel/11679

 cheers
   Ben Bolker


From bbolker at gmail.com  Thu Mar 27 17:57:50 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 27 Mar 2014 12:57:50 -0400
Subject: [R-sig-ME] Temporal correlation structure in mixed model
In-Reply-To: <C1A5238848713043B7C18ED38FFEF1F812B350D8@STWMB01.ad.okstate.edu>
References: <C1A5238848713043B7C18ED38FFEF1F812B350D8@STWMB01.ad.okstate.edu>
Message-ID: <5334588E.60605@gmail.com>

On 14-03-27 12:23 PM, Worthington, Thomas A wrote:
> Dear All 

> I'm trying to run a mixed model on ecology data. The setup is as
  follows, a group of fish are placed in a flume and attempt to pass a
  barrier. The dependent variable is the number of approaches to pass
  the barrier per minute of the trial, with each trial lasting 10
  minutes. There are two configurations of the barrier.  There are 20
  trials in total, 10 for each setup. The aim of the study is to see
  whether the number of approaches in minute 1, minute 2, minute 3
  etc.  is different between the two setups. Therefore my model is of
  the form Approaches ~ Setup + Minute + Setup * Minute, where minute
  (10 levels) and setup (2 levels) are both factors.

The dependent variable sounds more like a count than a continuous
variable, so you might consider a Poisson GLMM, _but_ it is harder
to incorporate temporal autocorrelation in that case, so if it seems
reasonable you might try to stick with the linear mixed model (i.e.
Gaussian/Normal responses).  How big is your data set (is a 'trial'
equivalent to a single fish?  10 levels=10 minutes?  Does that you
mean you have a total of 200 observations?)  What is a typical
number/range of attempts per minute?


> I have three potential models and am slightly confused over the
  model specification, I believe I need to include a correlation
  structure to deal with temporal autocorrelation but am unclear over
  whether a random effect is needed instead or both are required. I
  have included a variance structure to deal with the issue of
  homogeneity of variance in the data.

A Poisson GLMM might help here.

> The three models are as follows



> test1 <- gls(Approaches ~ Setup * Minute, data=data, correlation =
  corAR1(form =~ Minute|Trial), weights = varIdent(form=~ 1| Minute *
  Setup), control=list(opt="optim"))

This will estimate correlation, but will assume that the average
number of attempts is the same for every trial within a setup
category.


> test2 <- lme(Approaches ~ Setup * Minute, data=data, random = ~
  Minute | Trial, weights = varIdent(form=~ 1| Minute * Setup),
  control=list(opt="optim"))

  This looks overspecified (does it really work??) -- it is allowing
the differences among every pair of minutes to be characterized
for every trial.  Even ~1|Trial/Minute is overspecified, since
there is only one observation per minute per trial (thus the
among-minute-within-trial variance would be confounded with
the residual variance).

> test3 <- lme(Approaches ~ Setup * Minute, data=data, random = ~
  Minute | Trial, weights = varIdent(form=~ 1| Minute * Setup),
  correlation = corAR1(form =~ Minute|Trial),
  control=list(opt="optim"))

This looks even more overspecified.  I would suggest

lme(Approaches ~ Setup * Minute, data=data, random = ~ 1 | Trial,
   weights = varIdent(form=~ 1| Minute * Setup),
  correlation = corAR1(form =~ Minute|Trial),
  control=list(opt="optim"))

 which allows for variation among trials (1|Trial) and autocorrelation
within Trial -- the Minute:Trial interaction will show up in the
residual variance.

  This still looks ambitious if you have a total of 200 data points,
as the varIdent() specification will estimate variances for all 20
of the minute*setup combinations.
  * Would log- or square-root transforming your data take care of
the heteroscedasticity adequately?
  * Are you sure you want to treat minute as categorical, estimating
a separate response for every minute?  Or would a smooth trend
(linear, quadratic, or spline) describe the data adequately and
more parsimoniously?  (This applies to both the mean and variance
description)


> I believe (maybe wrongly) that Test 1 and 2 would be broadly
  similar, but if some could suggest the most sensible model I would
  greatly appreciate it

> Best wishes
> 
> Tom
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From bbolker at gmail.com  Thu Mar 27 18:17:48 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 27 Mar 2014 17:17:48 +0000 (UTC)
Subject: [R-sig-ME] Wrong convergence warnings with glmer in lme4
	version 1.1-6??
References: <CA+doL7zex9Fy6RG3cCshNXzUndOc39MefQRBE=t4sf70d-HHaw@mail.gmail.com>
	<53333A60.6090007@gmail.com>
Message-ID: <loom.20140327T175854-289@post.gmane.org>

Ben Bolker <bbolker at ...> writes:

PS ...

> 
>   Looks sensible.
> 
>   I don't have more time to devote this right now, but my next steps
> would/will be:
> 
>  * restart the optimization from the fitted optimum (possibly? using
> refit(), or update(m1,start=getME(m1,c("theta","beta")))) and see if
> that makes the max grad smaller

  Steve Walker tried this, and it does 'work' -- successive refits
don't change the answer much, and the final gradient gets
progressively smaller (although it takes two refits to get below the
default test tolerance).  (The code above doesn't work as written
because the fixed effect component of the starting parameter list has
to be named 'fixef' -- we should change this to allow 'beta' as well
...)

>  * try a range of optimizers, especially nlminb, as described here:
> http://stackoverflow.com/questions/21344555/
  convergence-error-for-development-version-of-lme4/21370041#21370041
[URL broken to make gmane happy]
> 

  I tried this with the full range of optimizers listed at that link.
*Only* the built-in Nelder-Mead has the problem; all other optimizers
(optimx + nlminb or L-BFGS-B; nloptr + Nelder-Mead or BOBYQA; built-in
bobyqa) get max(abs(gradient)) considerably less than the tolerance --
but the actual fitted model changes very little (the log-likelihood increases
by <0.01).

  So this does seem to be a false positive.  Still doesn't explain
why this is happening with Nelder-Mead, or under what circumstances
it's likely to happen (although big models do look prone to it).  We
should probably switch away from Nelder-Mead as the default throughout
(this was already done for lmer models in the last release, but not
for glmer), although I would love to do some more testing before jumping
out of the frying pan ...


>   Ben Bolker


From thomas.worthington at okstate.edu  Thu Mar 27 18:55:56 2014
From: thomas.worthington at okstate.edu (Worthington, Thomas A)
Date: Thu, 27 Mar 2014 17:55:56 +0000
Subject: [R-sig-ME] Temporal correlation structure in mixed model
In-Reply-To: <5334588E.60605@gmail.com>
References: <C1A5238848713043B7C18ED38FFEF1F812B350D8@STWMB01.ad.okstate.edu>
	<5334588E.60605@gmail.com>
Message-ID: <C1A5238848713043B7C18ED38FFEF1F812B35254@STWMB01.ad.okstate.edu>

Dear Ben 

Thank you for your suggestions  in answer to your questions

A trial is multiple fish but it is impossible to tell individuals apart, therefore the dependent variable is for the group. 

Yes the ten levels relates to an overall trial length of 10 minutes and yes there are a total of 200 observations

The typical number of approaches varies from 0.2 - 0.5 for setup 0 and 0.2 - 1.7 for setup 1, this value is corrected for fish that past the barrier. >From examination of the data the number of approaches is only different between the two setups for minutes 1, 2 and possibly 3.  The rational for treating minute as a categorical variable  was determine if there was an attraction to the barrier due to setup in those first minutes, which I was going to test using planned contrast once the overall model had been specified. WE had toyed with a simpler approach of multiple t-tests the correcting for multiple comparisons

In terms of the model you suggested maybe it would be preferable to set the variance structure to weights = varIdent(form=~ 1| Minute) as the difference in variance is mostly between minutes? I avoided transforming the data as it was suggested that it might be preferable to model the variance structure in Zuur et al 2009 Mixed Effects Models and Extensions in Ecology with R. 
  
test4b<-lme(Approaches ~ Setup * Minute, data=data, random = ~ 1 | Trial, weights = varIdent(form=~ 1| Minute), correlation = corAR1(form =~ Minute|Trial), control=list(opt="optim"))

I have compared the 5 models with AIC and BIC and 4b seems to fit the best (Test 4a being the one you put forth as a suggestion)

       df       AIC
test1  25 -40.44546
test2  27 -32.25960
test3  28 -38.20182
test4a 26 -42.20184
test4b 16 -20.23438

and 

       df      BIC
test1  25 41.50741
test2  27 56.24950
test3  28 53.58539
test4a 26 43.02914
test4b 16 32.21545

Best wishes

Tom 

-----Original Message-----
From: Ben Bolker [mailto:bbolker at gmail.com] 
Sent: Thursday, March 27, 2014 12:58 PM
To: Worthington, Thomas A; r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Temporal correlation structure in mixed model

On 14-03-27 12:23 PM, Worthington, Thomas A wrote:
> Dear All

> I'm trying to run a mixed model on ecology data. The setup is as
  follows, a group of fish are placed in a flume and attempt to pass a
  barrier. The dependent variable is the number of approaches to pass
  the barrier per minute of the trial, with each trial lasting 10
  minutes. There are two configurations of the barrier.  There are 20
  trials in total, 10 for each setup. The aim of the study is to see
  whether the number of approaches in minute 1, minute 2, minute 3
  etc.  is different between the two setups. Therefore my model is of
  the form Approaches ~ Setup + Minute + Setup * Minute, where minute
  (10 levels) and setup (2 levels) are both factors.

The dependent variable sounds more like a count than a continuous variable, so you might consider a Poisson GLMM, _but_ it is harder to incorporate temporal autocorrelation in that case, so if it seems reasonable you might try to stick with the linear mixed model (i.e.
Gaussian/Normal responses).  How big is your data set (is a 'trial'
equivalent to a single fish?  10 levels=10 minutes?  Does that you mean you have a total of 200 observations?)  What is a typical number/range of attempts per minute?


> I have three potential models and am slightly confused over the
  model specification, I believe I need to include a correlation
  structure to deal with temporal autocorrelation but am unclear over
  whether a random effect is needed instead or both are required. I
  have included a variance structure to deal with the issue of
  homogeneity of variance in the data.

A Poisson GLMM might help here.

> The three models are as follows



> test1 <- gls(Approaches ~ Setup * Minute, data=data, correlation =
  corAR1(form =~ Minute|Trial), weights = varIdent(form=~ 1| Minute *
  Setup), control=list(opt="optim"))

This will estimate correlation, but will assume that the average number of attempts is the same for every trial within a setup category.


> test2 <- lme(Approaches ~ Setup * Minute, data=data, random = ~
  Minute | Trial, weights = varIdent(form=~ 1| Minute * Setup),
  control=list(opt="optim"))

  This looks overspecified (does it really work??) -- it is allowing the differences among every pair of minutes to be characterized for every trial.  Even ~1|Trial/Minute is overspecified, since there is only one observation per minute per trial (thus the among-minute-within-trial variance would be confounded with the residual variance).

> test3 <- lme(Approaches ~ Setup * Minute, data=data, random = ~
  Minute | Trial, weights = varIdent(form=~ 1| Minute * Setup),
  correlation = corAR1(form =~ Minute|Trial),
  control=list(opt="optim"))

This looks even more overspecified.  I would suggest

lme(Approaches ~ Setup * Minute, data=data, random = ~ 1 | Trial,
   weights = varIdent(form=~ 1| Minute * Setup),
  correlation = corAR1(form =~ Minute|Trial),
  control=list(opt="optim"))

 which allows for variation among trials (1|Trial) and autocorrelation within Trial -- the Minute:Trial interaction will show up in the residual variance.

  This still looks ambitious if you have a total of 200 data points, as the varIdent() specification will estimate variances for all 20 of the minute*setup combinations.
  * Would log- or square-root transforming your data take care of the heteroscedasticity adequately?
  * Are you sure you want to treat minute as categorical, estimating a separate response for every minute?  Or would a smooth trend (linear, quadratic, or spline) describe the data adequately and more parsimoniously?  (This applies to both the mean and variance
description)


> I believe (maybe wrongly) that Test 1 and 2 would be broadly
  similar, but if some could suggest the most sensible model I would
  greatly appreciate it

> Best wishes
> 
> Tom
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 


From thomas.worthington at okstate.edu  Fri Mar 28 00:55:06 2014
From: thomas.worthington at okstate.edu (Worthington, Thomas A)
Date: Thu, 27 Mar 2014 23:55:06 +0000
Subject: [R-sig-ME] Temporal correlation structure in mixed model
In-Reply-To: <C1A5238848713043B7C18ED38FFEF1F812B35254@STWMB01.ad.okstate.edu>
References: <C1A5238848713043B7C18ED38FFEF1F812B350D8@STWMB01.ad.okstate.edu>
	<5334588E.60605@gmail.com>
	<C1A5238848713043B7C18ED38FFEF1F812B35254@STWMB01.ad.okstate.edu>
Message-ID: <C1A5238848713043B7C18ED38FFEF1F812B35748@STWMB01.ad.okstate.edu>

Ok so it appears I was specifying the factors incorrectly when I use

data$Setup<-factor(data$Setup)
data$Minute<-factor(data$Minute)
data$Trial<-factor(data$Trial)

and rerun the model I get the following error message 

Error in structure(res, levels = lv, names = nm, class = "factor") : 
  'names' attribute [180] must be the same length as the vector [0]

Which appears to be related to the correlation structure, as when I remove this the model runs. Although the model output called with 

summary(test4b)

now contains comparisons between Minute 1 and Minutes 2, 3, 4 etc rather than the overall statistics for Setup, Minute and the interaction 

Any suggestions of a way forward would be appreciated? Or do I need a model that contains a least one continuous variable?

Best wishes

Tom   



-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Worthington, Thomas A
Sent: Thursday, March 27, 2014 1:56 PM
To: Ben Bolker; r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Temporal correlation structure in mixed model

Dear Ben 

Thank you for your suggestions  in answer to your questions

A trial is multiple fish but it is impossible to tell individuals apart, therefore the dependent variable is for the group. 

Yes the ten levels relates to an overall trial length of 10 minutes and yes there are a total of 200 observations

The typical number of approaches varies from 0.2 - 0.5 for setup 0 and 0.2 - 1.7 for setup 1, this value is corrected for fish that past the barrier. >From examination of the data the number of approaches is only different between the two setups for minutes 1, 2 and possibly 3.  The rational for treating minute as a categorical variable  was determine if there was an attraction to the barrier due to setup in those first minutes, which I was going to test using planned contrast once the overall model had been specified. WE had toyed with a simpler approach of multiple t-tests the correcting for multiple comparisons

In terms of the model you suggested maybe it would be preferable to set the variance structure to weights = varIdent(form=~ 1| Minute) as the difference in variance is mostly between minutes? I avoided transforming the data as it was suggested that it might be preferable to model the variance structure in Zuur et al 2009 Mixed Effects Models and Extensions in Ecology with R. 
  
test4b<-lme(Approaches ~ Setup * Minute, data=data, random = ~ 1 | Trial, weights = varIdent(form=~ 1| Minute), correlation = corAR1(form =~ Minute|Trial), control=list(opt="optim"))

I have compared the 5 models with AIC and BIC and 4b seems to fit the best (Test 4a being the one you put forth as a suggestion)

       df       AIC
test1  25 -40.44546
test2  27 -32.25960
test3  28 -38.20182
test4a 26 -42.20184
test4b 16 -20.23438

and 

       df      BIC
test1  25 41.50741
test2  27 56.24950
test3  28 53.58539
test4a 26 43.02914
test4b 16 32.21545

Best wishes

Tom 

-----Original Message-----
From: Ben Bolker [mailto:bbolker at gmail.com]
Sent: Thursday, March 27, 2014 12:58 PM
To: Worthington, Thomas A; r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Temporal correlation structure in mixed model

On 14-03-27 12:23 PM, Worthington, Thomas A wrote:
> Dear All

> I'm trying to run a mixed model on ecology data. The setup is as
  follows, a group of fish are placed in a flume and attempt to pass a
  barrier. The dependent variable is the number of approaches to pass
  the barrier per minute of the trial, with each trial lasting 10
  minutes. There are two configurations of the barrier.  There are 20
  trials in total, 10 for each setup. The aim of the study is to see
  whether the number of approaches in minute 1, minute 2, minute 3
  etc.  is different between the two setups. Therefore my model is of
  the form Approaches ~ Setup + Minute + Setup * Minute, where minute
  (10 levels) and setup (2 levels) are both factors.

The dependent variable sounds more like a count than a continuous variable, so you might consider a Poisson GLMM, _but_ it is harder to incorporate temporal autocorrelation in that case, so if it seems reasonable you might try to stick with the linear mixed model (i.e.
Gaussian/Normal responses).  How big is your data set (is a 'trial'
equivalent to a single fish?  10 levels=10 minutes?  Does that you mean you have a total of 200 observations?)  What is a typical number/range of attempts per minute?


> I have three potential models and am slightly confused over the
  model specification, I believe I need to include a correlation
  structure to deal with temporal autocorrelation but am unclear over
  whether a random effect is needed instead or both are required. I
  have included a variance structure to deal with the issue of
  homogeneity of variance in the data.

A Poisson GLMM might help here.

> The three models are as follows



> test1 <- gls(Approaches ~ Setup * Minute, data=data, correlation =
  corAR1(form =~ Minute|Trial), weights = varIdent(form=~ 1| Minute *
  Setup), control=list(opt="optim"))

This will estimate correlation, but will assume that the average number of attempts is the same for every trial within a setup category.


> test2 <- lme(Approaches ~ Setup * Minute, data=data, random = ~
  Minute | Trial, weights = varIdent(form=~ 1| Minute * Setup),
  control=list(opt="optim"))

  This looks overspecified (does it really work??) -- it is allowing the differences among every pair of minutes to be characterized for every trial.  Even ~1|Trial/Minute is overspecified, since there is only one observation per minute per trial (thus the among-minute-within-trial variance would be confounded with the residual variance).

> test3 <- lme(Approaches ~ Setup * Minute, data=data, random = ~
  Minute | Trial, weights = varIdent(form=~ 1| Minute * Setup),
  correlation = corAR1(form =~ Minute|Trial),
  control=list(opt="optim"))

This looks even more overspecified.  I would suggest

lme(Approaches ~ Setup * Minute, data=data, random = ~ 1 | Trial,
   weights = varIdent(form=~ 1| Minute * Setup),
  correlation = corAR1(form =~ Minute|Trial),
  control=list(opt="optim"))

 which allows for variation among trials (1|Trial) and autocorrelation within Trial -- the Minute:Trial interaction will show up in the residual variance.

  This still looks ambitious if you have a total of 200 data points, as the varIdent() specification will estimate variances for all 20 of the minute*setup combinations.
  * Would log- or square-root transforming your data take care of the heteroscedasticity adequately?
  * Are you sure you want to treat minute as categorical, estimating a separate response for every minute?  Or would a smooth trend (linear, quadratic, or spline) describe the data adequately and more parsimoniously?  (This applies to both the mean and variance
description)


> I believe (maybe wrongly) that Test 1 and 2 would be broadly
  similar, but if some could suggest the most sensible model I would
  greatly appreciate it

> Best wishes
> 
> Tom
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From rdiaz02 at gmail.com  Fri Mar 28 03:06:52 2014
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Fri, 28 Mar 2014 03:06:52 +0100
Subject: [R-sig-ME] A mixed effects model for variances of
	binomialprobability as a responsee
In-Reply-To: <CAEDz9ehy59ehrdfrHavkBZJ=3L44hxkcE=XXRfPatp9mxZ6RDw@mail.gmail.com>
References: <CAEDz9ehmsKL=59RnAtTzEjWxeuqyShf+CQxLcY36s25UMjCk6A@mail.gmail.com>
	<87ppl8c9xu.fsf@gmail.com>
	<CAEDz9ehy59ehrdfrHavkBZJ=3L44hxkcE=XXRfPatp9mxZ6RDw@mail.gmail.com>
Message-ID: <8761mzaxfn.fsf@gmail.com>



On Thu, 01-01-1970, at 01:00, nimrod.rubinstein <nimrod.rubinstein at gmail.com> wrote:
> Hi Ramon (and others reading this),
>
> Thanks for the response.
>
> I'm attaching my real data - it has these columns:
> subject - subject name (S1, S2,...)
> group - group name (G1, G2) - a subject can only belong to either of the
> two groups
> environment - environment name (E1, E2, E3).
> the next 27 columns (named n.1-n.27) are the number of trails in each
> replicate
> the following 27 columns (named k.1-k.27) are the number of successes out
> of the number of trials in each corresponding replicate (so k.1 is the
> number of success out of n.1 trials, and so on).
>
> Note that there are NA values in corresponding n and k cells in the table.
> Also, not all subjects were sampled from all 3 environments due to
> technical problems, so the data are not fully balanced.
>
> I know this table is a bit different from my earlier description of the
> data. It is so only and it is because I wanted to make it simple.
>
> Let me try and clarify, and hopefully all of the below will answer your
> questions.
>
> What may affect the variances of the success probabilities (proportions)?
> 1. The proportion itself (natural to binomially distributed data). See
> attached Fig1 that plots sample variances of success probabilities vs.
> their sample mean.

But what are those variances? How are you computing them? (See below, though)


> 2. The number of trails in each replicate. As I said, the number of trails
> is inherent to the subject and the environment it was measured in, the
> experimenter has no control over it. As the attached Fig2 shows, subjects
> with a high average number of trials have lower sample variances of their
> success probabilities. The opposite however is not clear. The sample
> variances of subjects with a low average number of trials vary greatly. So
> obviously, the number of trails has an effect on the variance of the
> proportion but there's still a sizable fraction of variance (of the
> proportion variances) that are not explained by that.
> 3. The group - that's the effect I'm interested in. Fig1 and Fig2 show that
> the G1 dots have lower sample variances of success probabilities than G2
> dots.
> 4. The environment. These environments were chosen at random and therefore
> I think they should be REs. (In all attached figures the environments isn't
> indicated)
>
> I've also attached a figure of the success probabilities vs, the number of
> trials (Fig3).

Yes, I think the scenario I was concerned about is not here. Good.

>
> I think the figures show pretty clearly that both number of trails, success
> probability, and group affect the variance of success probability, and
> therefore the model I'm looking for should estimate all of these.
>

It is way too late here, but maybe we are using the same name (variance)
for two different things here: the variance of p, which is just a simple
function of the proportion, of the p itself, and the variance in the
collection of p's, where each p is not necessarily estimating the same
probability. And I think you care about the second problem, something like
saying that individuals in G1 and G2 have different dispersion in their
p's.  So you have a distribution for the p's of G1 and a distribution for
the p's of G2, and you want to know whether those distributions have the
same variance while also possibly controlling for other factors. I think
the book by Gelman and Hill has some examples of this kind (I do not have
it here now, though).



Best,


R.



>
> Thanks a lot,
> Nimrod
>
>
> On Thu, Mar 27, 2014 at 4:39 AM, Ramon Diaz-Uriarte <rdiaz02 at gmail.com>wrote:
>
>> Dear Nimrod,
>>
>> Just my 2 cents (even less than 2):
>>
>> On Thu, 01-01-1970, at 01:00, nimrod.rubinstein <
>> nimrod.rubinstein at gmail.com> wrote:
>> > Hi all,
>> >
>> > I have the following data:
>> >
>> > 50 subjects that belong to 2 groups (25 in each group). For each subject
>> I
>> > have replicated measurements (in the range of 15-30 replicates per
>> subject)
>> > of the number of trails as well as the number successes out of these
>> > trails. Each single trial in each single replicate for each subject is
>> > assumed to be a Bernoulli experiment, therefore the number of successes
>> in
>> > each replicate is assumed to be a binomially distributed random variable.
>> > To complicate things even more, for each subject from each group I have
>> not
>> > 1 but 3 series of these replicated measurements, from 3 different
>> > environments (the 3 environments are the same for all subjects).
>> >
>> > A few example lines of how my data looks like:
>> > subject group environment     number.trails  number.successes
>> >
>> >     S1    G1          E1 10100,13209,17621  8500,11622,14684
>> >
>> >     S1    G1          E2 13322,12225,10024  10351,10222,7951
>> >
>> >     S1    G1          E3  9812,13500,13214  7245,10622,10684
>> >
>> >     .
>> >
>> >     .
>> >
>> >     .
>> >
>> >     S50    G2          E1 60547,75345,68412 12014,13214,15244
>> >
>> >     S50    G2          E2 50897,62541,61247 10141,13251,30645
>> >
>> >     S50    G2          E3 59214,84264,74156 17245,16425,11123
>> >
>> >
>> > Where the comma separated number of successes correspond to the comma
>> > separated number of trials in each replicate.
>> >
>> > I want to test whether the variances of the success probabilities of
>> > subjects from group G1 are significantly smaller than the variances of
>> the
>> > success probabilities of subjects from group G2.
>> >
>>
>> But, as you say in point 1. below (and in another reply), if this is really
>> binomial, then you already know that if probabilities in G1 are different
>> from probabilities in G2 your variances will differ.
>>
>> So I am not sure I even understand this hypothesis/question.
>>
>>
>> > In other words, for each subject in each replicate a success probability
>> > can be estimated from the observed data. Then the variance of those
>> success
>> > probabilities over all replicates from a specific environment can be
>> > estimated. I would like to know whether these variances are smaller in
>> > subjects from G1 vs. subjects from G2.
>> >
>> > The issues are:
>> >
>> >
>> >    1. The variance of a success probability depends on the success
>> >    probability itself (natural to binomially distributed data). I need to
>> >    account for this effect.
>> >    2. The number of trails (although at least in the hundreds for all
>> >    subjects) varies several orders of magnitude among subjects. That by
>> itself
>> >    affects the accuracy of estimating the success probability, but in
>> >    addition, I cannot rule out an inherent dependency between the number
>> of
>> >    observed trials and the success probability (there is no control over
>> the
>> >    number of trials in the experiment - it is observed). In other words,
>> >    having a lower number of trails by itself may affect the success
>> >    probability.
>> >
>>
>> That last issue seems very problematic to me. If I understand correctly, we
>> could think of a perverse situation where, say, you can never observe low
>> success probabilities if those are related to number of trials around 0.
>>
>>
>> Can you do a simple plot of probability of success (or its logit) vs number
>> of trials, to rule out perverse scenarios? Is probability of success always
>> larger than 0? And number of trials always sufficiently different from 0?
>>
>>
>> And you say "varies by subject", so the number of trials is always the same
>> for a subject? And the potential relationship prob <-> number of trials is
>> due to among-subject variation? So you could approach the problem via a
>> random effect for subject? If you compare G1 and G2 for number of trials:
>> are there differences?
>>
>>
>>
>>
>>
>> >
>> > I though a simple way to test my hypothesis is to compute the success
>> > probability in each replicate as number.successes/number.trails, and then
>> > the sample mean and sample variance of these success probabilities over
>> all
>> > replicates, as well as the mean number of trials over all replicates
>> (added
>> > to my data as sample.mean, sample.var and mean.trials fields). Then I can
>> > use the lmer function in the lme4 package this way:
>> >
>> > null.model = lmer(sample.var ~ sample.mean + mean.trials +
>> (1|environment),
>> > my.data, REML = F)
>> > alt.model = lmer(sample.var ~ sample.mean + mean.trials + group +
>> > (1|environment), my.data, REML = F)
>> >
>> > So fixed effects are: sample.mean, mean.trials and group, and random
>> > effects is environment since the three environments in my data are
>> simply a
>> > random sample from many other possible environments.
>>
>>
>> I do not see how this would answer your questions, given the above
>> concerns/issues and other possible approaches (just compare prob. between
>> G1 and G2, accounting for environment).
>>
>>
>> Best,
>>
>>
>> R.
>>
>>
>> >
>> > To test whether group has a significant effect I'll do:
>> >
>> > anova(null.model, alt.model).
>> >
>> > My concerns are:
>> >
>> >
>> >    1. Should I add subject as a random effect? My intuition is not to
>> since
>> >    subjects are unique to groups, which is the effect I'm after.
>> However, I do
>> >    want to account for the fact that sample variances of the same
>> subject from
>> >    the three different environments are correlated.
>> >    2. The response - sample variances, cannot be assumed to follow a
>> normal
>> >    distribution (they only take positive values to begin with and as far
>> as I
>> >    know (n-1)*sample.variance/population.variance ~ chi.square with df =
>> n-1)
>> >    and therefore the assumptions of a linear mixed effects model do not
>> >    strictly hold. Is there a generalized linear model that would be
>> >    appropriate for this?
>> >
>> > Thanks a lot,
>> > nimrod
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>> --
>> Ramon Diaz-Uriarte
>> Department of Biochemistry, Lab B-25
>> Facultad de Medicina
>> Universidad Aut?noma de Madrid
>> Arzobispo Morcillo, 4
>> 28029 Madrid
>> Spain
>>
>> Phone: +34-91-497-2412
>>
>> Email: rdiaz02 at gmail.com
>>        ramon.diaz at iib.uam.es
>>
>> http://ligarto.org/rdiaz
>>
>>

-- 
Ramon Diaz-Uriarte
Department of Biochemistry, Lab B-25
Facultad de Medicina
Universidad Aut?noma de Madrid 
Arzobispo Morcillo, 4
28029 Madrid
Spain

Phone: +34-91-497-2412

Email: rdiaz02 at gmail.com
       ramon.diaz at iib.uam.es

http://ligarto.org/rdiaz


From albrechj at staff.uni-marburg.de  Fri Mar 28 08:44:18 2014
From: albrechj at staff.uni-marburg.de (albrechj at staff.uni-marburg.de)
Date: Fri, 28 Mar 2014 08:44:18 +0100
Subject: [R-sig-ME] set up of priors in MCMCglmm for different error families
Message-ID: <20140328084418.Horde.Uj2F2ORZtAAMX62xXFAGww1@home.staff.uni-marburg.de>

Dear list,

I want to us MCMCglmm to fit three phylogenetic mixed models. One for  
each of three response variables. Basically, the model formula is as  
follows:

response ~ trait1 * trait2 + (1|plant_species) + (1|site) + (1|year);

The data is based on observations from 15 plant species in 13 study  
sites across two years, but the dataset is not completely full  
factorial with respect to the random grouping factors. The dataset is  
relatively small (n = 121).

The error families of the three response variables are gaussian,  
multinomial2 (i.e., binomial), and ztpoisson (zero-truncated poisson).

My question is now which priors should I use for these three models. I  
have searched the course notes and on the list for suggestions, and  
ended up with flat *improper* priors for the residual variance and  
parameter expanded priors for the (co)variance in the random terms.

However, at this point I'am stuck and I don't know whether the priors  
are specified correctly.

Could anyone give advice on the correct specification of the priors  
for these models? I have attached the r script below (I hope it is not  
too confusing, because I have written this script to fit the models on  
multiple cores parallel).


Thank you.

J?rg


The code for model fitting:
################################################################################
### modelfitting
################################################################################
if(! file.exists(paste(compiled_data_victoria_dir,
    "phylogenetic_mcmcglmm_results.rda",sep="/"))) {
### set up the phylogenetic covariance matrix
# based on a tree including 15 plant species
Ainv<-inverseA(phylo1, nodes="TIPS")$Ainv

### set up the model formulas
responses <- c("plant_d", "cbind(removal, total_rem)", "richness")
### paste response and predictors into one character string
fixed1 <- paste(responses, "s.fruit_density_con * s.phen_d", sep=" ~ ")
### formula for random effects
random1 <- "~ plant_species + site + year"
### set up error families
families1 <- c("gaussian", "multinomial2", "ztpoisson")

### set up priors
priors1 <- list(list(R=list(V=1, nu=0),
                      G=list(G1=list(V=1, nu=0, alpha.mu=0, alpha.V=1e3),
                             G2=list(V=1, nu=0, alpha.mu=0, alpha.V=1e3),
                             G3=list(V=1, nu=0, alpha.mu=0, alpha.V=1e3))),
                 list(R=list(V=1, nu=0),
                      G=list(G1=list(V=1, nu=1e3, alpha.mu=0, alpha.V=1),
                             G2=list(V=1, nu=1e3, alpha.mu=0, alpha.V=1),
                             G3=list(V=1, nu=1e3, alpha.mu=0, alpha.V=1))),
                 list(R=list(V=1, nu=0),
                      G=list(G1=list(V=1, nu=0, alpha.mu=0, alpha.V=1e3),
                             G2=list(V=1, nu=0, alpha.mu=0, alpha.V=1e3),
                             G3=list(V=1, nu=0, alpha.mu=0, alpha.V=1e3))))

### make a list containing the formulas for fixed and random terms,
# error family, and prior information for for each modelfit
mod_setup <- list()
for (i in 1:length(mod_formulas1)) {
mod_setup[[i]] <- list(fixed = fixed1[i], random = random1,
                        family = families1[i], prior = priors1[[i]])
}

### function to fit the models
run1 <- function(mod_setup)  {
   MCMCglmm(fixed = as.formula(mod_setup$fixed),
            random = as.formula(mod_setup$random),
            ginverse = list(plant_species = Ainv), prior = mod_setup$prior,
            data = data_complete, nodes = "TIPS", family = mod_setup$family,
            nitt = 1.5e6, thin = 1e3, burnin = 5e5)
}

### parallel fitting of models on multiple cores
system.time(result1 <- mclapply(mod_setup, run1, mc.cores=length(mod_setup)))
### save the result
save(list=c("result1"), file=paste(compiled_data_victoria_dir,
      "phylogenetic_mcmcglmm_results.rda",sep="/"))
} else {
### load the result if allready exists
   base::load(file=paste(compiled_data_victoria_dir,
              "phylogenetic_mcmcglmm_results.rda",sep="/"))
}
################################################################################
### End of script
################################################################################


From albrechj at staff.uni-marburg.de  Fri Mar 28 13:44:19 2014
From: albrechj at staff.uni-marburg.de (albrechj at staff.uni-marburg.de)
Date: Fri, 28 Mar 2014 13:44:19 +0100
Subject: [R-sig-ME] set up of priors in MCMCglmm for phylogenetic mixed
	models
Message-ID: <20140328134419.Horde.vcCxk6G7pryXmYP5LJZ73A4@home.staff.uni-marburg.de>

Dear list,

I want to us MCMCglmm to fit phylogenetic mixed models. I would like  
to fit models for three response variables. The data is based on  
observations from 15 plant species in 13 study sites across two years,  
but the dataset is not completely full factorial with respect to the  
random grouping factors. The dataset is relatively small (n = 128).

The error families of the three response variables are gaussian,  
multinomial2 (i.e., binomial), and ztpoisson (zero-truncated poisson).

The models look as follows:
###############################################################
# model 1, Gaussian
prior1 <- list(R=list(V=1, nu=0),
                      G=list(G1=list(V=1, nu=0, alpha.mu=0, alpha.V=1e3),
                             G2=list(V=1, nu=0, alpha.mu=0, alpha.V=1e3),
                             G3=list(V=1, nu=0, alpha.mu=0, alpha.V=1e3)))

model1 <- MCMCglmm(fixed = plant_d ~ s.fruit_density_con * s.phen_d,
                                            random = ~ plant_species +  
site + year,
                                            ginverse =  
list(plant_species = Ainv), prior = prior1,
                                            data = data_complete,  
nodes = "TIPS", family = "gaussian",
                                            nitt = 1.5e6, thin = 1e3,  
burnin = 5e5)

###############################################################
# model 2, multinomial2 (binomial)
prior2 <- list(R=list(V=1, nu=0),
                      G=list(G1=list(V=1, nu=1e3, alpha.mu=0, alpha.V=1),
                             G2=list(V=1, nu=1e3, alpha.mu=0, alpha.V=1),
                             G3=list(V=1, nu=1e3, alpha.mu=0, alpha.V=1)))

model2 <- MCMCglmm(fixed = cbind(removal, total_rem) ~  
s.fruit_density_con * s.phen_d,
                                            random = ~ plant_species +  
site + year,
                                            ginverse =  
list(plant_species = Ainv), prior = prior2,
                                            data = data_complete,  
nodes = "TIPS", family = "multinomial2",
                                            nitt = 1.5e6, thin = 1e3,  
burnin = 5e5)

###############################################################
# model 3, zero-truncated poisson
prior3 <- list(R=list(V=1, nu=0),
                      G=list(G1=list(V=1, nu=0, alpha.mu=0, alpha.V=1e3),
                             G2=list(V=1, nu=0, alpha.mu=0, alpha.V=1e3),
                             G3=list(V=1, nu=0, alpha.mu=0, alpha.V=1e3)))

model3 <- MCMCglmm(fixed = richness ~ s.fruit_density_con * s.phen_d,
                                            random = ~ plant_species +  
site + year,
                                            ginverse =  
list(plant_species = Ainv), prior = prior3,
                                            data = data_complete,  
nodes = "TIPS", family = "ztpoisson",
                                            nitt = 1.5e6, thin = 1e3,  
burnin = 5e5)


My question is now which priors should I use for these three models. I  
have searched the course notes and on the list for suggestions, and  
ended up with flat *improper* priors for the residual variance and  
parameter expanded priors for the (co)variance in the random terms.

In the second model (with the binomial family) I used *parameter  
expanded priors* for the random effects (see below), as suggested by  
Jarrod Hadfield based on de Villemereuil et al. (2013) Methods in  
Ecology and Evolution 4:260-275.

However, at this point I'am stuck and I don't know whether the priors  
are specified correctly.


Could anyone give advice on the correct specification of the priors  
for these models?

I would really appreciate any suggestions.

Thanks,

J?rg


From CintiaAkemi.Oi at bio.kuleuven.be  Fri Mar 28 13:31:12 2014
From: CintiaAkemi.Oi at bio.kuleuven.be (Cintia Akemi Oi)
Date: Fri, 28 Mar 2014 12:31:12 +0000
Subject: [R-sig-ME] Message error
Message-ID: <C1988BA41CF49042A290C6CF791789EF1378D0BD@ICTS-S-MBX13.luna.kuleuven.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140328/e5b1ec88/attachment.pl>

From nimrod.rubinstein at gmail.com  Thu Mar 27 20:33:25 2014
From: nimrod.rubinstein at gmail.com (nimrod.rubinstein)
Date: Thu, 27 Mar 2014 15:33:25 -0400
Subject: [R-sig-ME] A mixed effects model for variances of
 binomialprobability as a responsee
In-Reply-To: <87ppl8c9xu.fsf@gmail.com>
References: <CAEDz9ehmsKL=59RnAtTzEjWxeuqyShf+CQxLcY36s25UMjCk6A@mail.gmail.com>
	<87ppl8c9xu.fsf@gmail.com>
Message-ID: <CAEDz9ehy59ehrdfrHavkBZJ=3L44hxkcE=XXRfPatp9mxZ6RDw@mail.gmail.com>

Hi Ramon (and others reading this),

Thanks for the response.

I'm attaching my real data - it has these columns:
subject - subject name (S1, S2,...)
group - group name (G1, G2) - a subject can only belong to either of the
two groups
environment - environment name (E1, E2, E3).
the next 27 columns (named n.1-n.27) are the number of trails in each
replicate
the following 27 columns (named k.1-k.27) are the number of successes out
of the number of trials in each corresponding replicate (so k.1 is the
number of success out of n.1 trials, and so on).

Note that there are NA values in corresponding n and k cells in the table.
Also, not all subjects were sampled from all 3 environments due to
technical problems, so the data are not fully balanced.

I know this table is a bit different from my earlier description of the
data. It is so only and it is because I wanted to make it simple.

Let me try and clarify, and hopefully all of the below will answer your
questions.

What may affect the variances of the success probabilities (proportions)?
1. The proportion itself (natural to binomially distributed data). See
attached Fig1 that plots sample variances of success probabilities vs.
their sample mean.
2. The number of trails in each replicate. As I said, the number of trails
is inherent to the subject and the environment it was measured in, the
experimenter has no control over it. As the attached Fig2 shows, subjects
with a high average number of trials have lower sample variances of their
success probabilities. The opposite however is not clear. The sample
variances of subjects with a low average number of trials vary greatly. So
obviously, the number of trails has an effect on the variance of the
proportion but there's still a sizable fraction of variance (of the
proportion variances) that are not explained by that.
3. The group - that's the effect I'm interested in. Fig1 and Fig2 show that
the G1 dots have lower sample variances of success probabilities than G2
dots.
4. The environment. These environments were chosen at random and therefore
I think they should be REs. (In all attached figures the environments isn't
indicated)

I've also attached a figure of the success probabilities vs, the number of
trials (Fig3).

I think the figures show pretty clearly that both number of trails, success
probability, and group affect the variance of success probability, and
therefore the model I'm looking for should estimate all of these.


Thanks a lot,
Nimrod


On Thu, Mar 27, 2014 at 4:39 AM, Ramon Diaz-Uriarte <rdiaz02 at gmail.com>wrote:

> Dear Nimrod,
>
> Just my 2 cents (even less than 2):
>
> On Thu, 01-01-1970, at 01:00, nimrod.rubinstein <
> nimrod.rubinstein at gmail.com> wrote:
> > Hi all,
> >
> > I have the following data:
> >
> > 50 subjects that belong to 2 groups (25 in each group). For each subject
> I
> > have replicated measurements (in the range of 15-30 replicates per
> subject)
> > of the number of trails as well as the number successes out of these
> > trails. Each single trial in each single replicate for each subject is
> > assumed to be a Bernoulli experiment, therefore the number of successes
> in
> > each replicate is assumed to be a binomially distributed random variable.
> > To complicate things even more, for each subject from each group I have
> not
> > 1 but 3 series of these replicated measurements, from 3 different
> > environments (the 3 environments are the same for all subjects).
> >
> > A few example lines of how my data looks like:
> > subject group environment     number.trails  number.successes
> >
> >     S1    G1          E1 10100,13209,17621  8500,11622,14684
> >
> >     S1    G1          E2 13322,12225,10024  10351,10222,7951
> >
> >     S1    G1          E3  9812,13500,13214  7245,10622,10684
> >
> >     .
> >
> >     .
> >
> >     .
> >
> >     S50    G2          E1 60547,75345,68412 12014,13214,15244
> >
> >     S50    G2          E2 50897,62541,61247 10141,13251,30645
> >
> >     S50    G2          E3 59214,84264,74156 17245,16425,11123
> >
> >
> > Where the comma separated number of successes correspond to the comma
> > separated number of trials in each replicate.
> >
> > I want to test whether the variances of the success probabilities of
> > subjects from group G1 are significantly smaller than the variances of
> the
> > success probabilities of subjects from group G2.
> >
>
> But, as you say in point 1. below (and in another reply), if this is really
> binomial, then you already know that if probabilities in G1 are different
> from probabilities in G2 your variances will differ.
>
> So I am not sure I even understand this hypothesis/question.
>
>
> > In other words, for each subject in each replicate a success probability
> > can be estimated from the observed data. Then the variance of those
> success
> > probabilities over all replicates from a specific environment can be
> > estimated. I would like to know whether these variances are smaller in
> > subjects from G1 vs. subjects from G2.
> >
> > The issues are:
> >
> >
> >    1. The variance of a success probability depends on the success
> >    probability itself (natural to binomially distributed data). I need to
> >    account for this effect.
> >    2. The number of trails (although at least in the hundreds for all
> >    subjects) varies several orders of magnitude among subjects. That by
> itself
> >    affects the accuracy of estimating the success probability, but in
> >    addition, I cannot rule out an inherent dependency between the number
> of
> >    observed trials and the success probability (there is no control over
> the
> >    number of trials in the experiment - it is observed). In other words,
> >    having a lower number of trails by itself may affect the success
> >    probability.
> >
>
> That last issue seems very problematic to me. If I understand correctly, we
> could think of a perverse situation where, say, you can never observe low
> success probabilities if those are related to number of trials around 0.
>
>
> Can you do a simple plot of probability of success (or its logit) vs number
> of trials, to rule out perverse scenarios? Is probability of success always
> larger than 0? And number of trials always sufficiently different from 0?
>
>
> And you say "varies by subject", so the number of trials is always the same
> for a subject? And the potential relationship prob <-> number of trials is
> due to among-subject variation? So you could approach the problem via a
> random effect for subject? If you compare G1 and G2 for number of trials:
> are there differences?
>
>
>
>
>
> >
> > I though a simple way to test my hypothesis is to compute the success
> > probability in each replicate as number.successes/number.trails, and then
> > the sample mean and sample variance of these success probabilities over
> all
> > replicates, as well as the mean number of trials over all replicates
> (added
> > to my data as sample.mean, sample.var and mean.trials fields). Then I can
> > use the lmer function in the lme4 package this way:
> >
> > null.model = lmer(sample.var ~ sample.mean + mean.trials +
> (1|environment),
> > my.data, REML = F)
> > alt.model = lmer(sample.var ~ sample.mean + mean.trials + group +
> > (1|environment), my.data, REML = F)
> >
> > So fixed effects are: sample.mean, mean.trials and group, and random
> > effects is environment since the three environments in my data are
> simply a
> > random sample from many other possible environments.
>
>
> I do not see how this would answer your questions, given the above
> concerns/issues and other possible approaches (just compare prob. between
> G1 and G2, accounting for environment).
>
>
> Best,
>
>
> R.
>
>
> >
> > To test whether group has a significant effect I'll do:
> >
> > anova(null.model, alt.model).
> >
> > My concerns are:
> >
> >
> >    1. Should I add subject as a random effect? My intuition is not to
> since
> >    subjects are unique to groups, which is the effect I'm after.
> However, I do
> >    want to account for the fact that sample variances of the same
> subject from
> >    the three different environments are correlated.
> >    2. The response - sample variances, cannot be assumed to follow a
> normal
> >    distribution (they only take positive values to begin with and as far
> as I
> >    know (n-1)*sample.variance/population.variance ~ chi.square with df =
> n-1)
> >    and therefore the assumptions of a linear mixed effects model do not
> >    strictly hold. Is there a generalized linear model that would be
> >    appropriate for this?
> >
> > Thanks a lot,
> > nimrod
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> --
> Ramon Diaz-Uriarte
> Department of Biochemistry, Lab B-25
> Facultad de Medicina
> Universidad Aut?noma de Madrid
> Arzobispo Morcillo, 4
> 28029 Madrid
> Spain
>
> Phone: +34-91-497-2412
>
> Email: rdiaz02 at gmail.com
>        ramon.diaz at iib.uam.es
>
> http://ligarto.org/rdiaz
>
>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Fig1.png
Type: image/png
Size: 6661 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140327/2f26ba93/attachment-0003.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Fig2.png
Type: image/png
Size: 7407 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140327/2f26ba93/attachment-0004.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Fig3.png
Type: image/png
Size: 16521 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140327/2f26ba93/attachment-0005.png>

From nimrod.rubinstein at gmail.com  Fri Mar 28 15:56:18 2014
From: nimrod.rubinstein at gmail.com (nimrod.rubinstein)
Date: Fri, 28 Mar 2014 10:56:18 -0400
Subject: [R-sig-ME] A mixed effects model for variances of
 binomialprobability as a responsee
In-Reply-To: <8761mzaxfn.fsf@gmail.com>
References: <CAEDz9ehmsKL=59RnAtTzEjWxeuqyShf+CQxLcY36s25UMjCk6A@mail.gmail.com>
	<87ppl8c9xu.fsf@gmail.com>
	<CAEDz9ehy59ehrdfrHavkBZJ=3L44hxkcE=XXRfPatp9mxZ6RDw@mail.gmail.com>
	<8761mzaxfn.fsf@gmail.com>
Message-ID: <CAEDz9ehCqRrnXuytEbFf+c5dGRTXmsSGPAMixPYxc+UcHPDhBA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140328/f59af234/attachment.pl>

From nimrod.rubinstein at gmail.com  Sat Mar 29 07:22:04 2014
From: nimrod.rubinstein at gmail.com (nimrod.rubinstein)
Date: Sat, 29 Mar 2014 02:22:04 -0400
Subject: [R-sig-ME] A mixed effects model for variances of
 binomialprobability as a responsee
In-Reply-To: <CAEDz9ehCqRrnXuytEbFf+c5dGRTXmsSGPAMixPYxc+UcHPDhBA@mail.gmail.com>
References: <CAEDz9ehmsKL=59RnAtTzEjWxeuqyShf+CQxLcY36s25UMjCk6A@mail.gmail.com>
	<87ppl8c9xu.fsf@gmail.com>
	<CAEDz9ehy59ehrdfrHavkBZJ=3L44hxkcE=XXRfPatp9mxZ6RDw@mail.gmail.com>
	<8761mzaxfn.fsf@gmail.com>
	<CAEDz9ehCqRrnXuytEbFf+c5dGRTXmsSGPAMixPYxc+UcHPDhBA@mail.gmail.com>
Message-ID: <CAEDz9ejY76tr5mb93nQiKGibHXUdRXgtX1kOxTuY3GtDM2xS0Q@mail.gmail.com>

I'm reattaching my data as an RData object. Here's how it looks like:

> head(df)
  subject group environment     n     k
1      S1    G1          E1  1419  1169
2      S1    G1          E2  5992   734
3      S1    G1          E3  5133   519
4      S2    G1          E1 23187 16038
5      S2    G1          E2 93255 46592
6      S2    G1          E3 35382 24486

This is probably much easire to work with now.

After defining the response as:
y=cbind(df$k,(df$n-df$k)) (following "The R Book" by Michael Crawley, Ch.
16)

I tried fitting this model:
model.null=glmer(y~log(n)+(1|subject)+(1|environment),data=df,family=binomial(link="logit"))

but I'm getting these warning messages:
Warning messages:
1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.0937283 (tol = 0.001)
2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?

Are there any parameters that can be tuned in order to achieve convergence?

Also, what should I rescale? Taking log(n) instead of n as a fixed effect
removed this warning:
1: In checkScaleX(X, ctrl = control) :
  Some predictor variables are on very different scales: consider rescaling

Any idea what else should I rescale?

Also, if my alternative model is:
alt.null=glmer(y~group+log(n)+(1|subject)+(1|environment),data=df,family=binomial(link="logit"))

Is there a way I can test whether the over-dispersion in G1 is
significantly smaller than in G2


Thanks a lot


On Fri, Mar 28, 2014 at 10:56 AM, nimrod.rubinstein <
nimrod.rubinstein at gmail.com> wrote:

> Hi Ramon and others,
>
> Thanks a lot for the advice.
>
> I think I need to rephrase my hypothesis, as follows:
> The data are binomial - for each subject, from each group, in each
> environment, I have replicates of trials and successes in these trials.
> I would like to know:
> 1. Are the data for each group (proportion or probability of success being
> the response, environment and perhaps subject begin random effects)
> significantly over-dispersed?
> 2. If 1 is true, then is the over-dispersion in group G1 significantly
> smaller than in group G2?
>
> It seems that glmer might be able to achieve that but I'm not entirely
> sure how.
>
> Advice would be greatly appreciated.
>
>
>
> On Thu, Mar 27, 2014 at 10:06 PM, Ramon Diaz-Uriarte <rdiaz02 at gmail.com>wrote:
>
>>
>>
>> On Thu, 01-01-1970, at 01:00, nimrod.rubinstein <
>> nimrod.rubinstein at gmail.com> wrote:
>> > Hi Ramon (and others reading this),
>> >
>> > Thanks for the response.
>> >
>> > I'm attaching my real data - it has these columns:
>> > subject - subject name (S1, S2,...)
>> > group - group name (G1, G2) - a subject can only belong to either of the
>> > two groups
>> > environment - environment name (E1, E2, E3).
>> > the next 27 columns (named n.1-n.27) are the number of trails in each
>> > replicate
>> > the following 27 columns (named k.1-k.27) are the number of successes
>> out
>> > of the number of trials in each corresponding replicate (so k.1 is the
>> > number of success out of n.1 trials, and so on).
>> >
>> > Note that there are NA values in corresponding n and k cells in the
>> table.
>> > Also, not all subjects were sampled from all 3 environments due to
>> > technical problems, so the data are not fully balanced.
>> >
>> > I know this table is a bit different from my earlier description of the
>> > data. It is so only and it is because I wanted to make it simple.
>> >
>> > Let me try and clarify, and hopefully all of the below will answer your
>> > questions.
>> >
>> > What may affect the variances of the success probabilities
>> (proportions)?
>> > 1. The proportion itself (natural to binomially distributed data). See
>> > attached Fig1 that plots sample variances of success probabilities vs.
>> > their sample mean.
>>
>> But what are those variances? How are you computing them? (See below,
>> though)
>>
>>
>> > 2. The number of trails in each replicate. As I said, the number of
>> trails
>> > is inherent to the subject and the environment it was measured in, the
>> > experimenter has no control over it. As the attached Fig2 shows,
>> subjects
>> > with a high average number of trials have lower sample variances of
>> their
>> > success probabilities. The opposite however is not clear. The sample
>> > variances of subjects with a low average number of trials vary greatly.
>> So
>> > obviously, the number of trails has an effect on the variance of the
>> > proportion but there's still a sizable fraction of variance (of the
>> > proportion variances) that are not explained by that.
>> > 3. The group - that's the effect I'm interested in. Fig1 and Fig2 show
>> that
>> > the G1 dots have lower sample variances of success probabilities than G2
>> > dots.
>> > 4. The environment. These environments were chosen at random and
>> therefore
>> > I think they should be REs. (In all attached figures the environments
>> isn't
>> > indicated)
>> >
>> > I've also attached a figure of the success probabilities vs, the number
>> of
>> > trials (Fig3).
>>
>> Yes, I think the scenario I was concerned about is not here. Good.
>>
>> >
>> > I think the figures show pretty clearly that both number of trails,
>> success
>> > probability, and group affect the variance of success probability, and
>> > therefore the model I'm looking for should estimate all of these.
>> >
>>
>> It is way too late here, but maybe we are using the same name (variance)
>> for two different things here: the variance of p, which is just a simple
>> function of the proportion, of the p itself, and the variance in the
>> collection of p's, where each p is not necessarily estimating the same
>> probability. And I think you care about the second problem, something like
>> saying that individuals in G1 and G2 have different dispersion in their
>> p's.  So you have a distribution for the p's of G1 and a distribution for
>> the p's of G2, and you want to know whether those distributions have the
>> same variance while also possibly controlling for other factors. I think
>> the book by Gelman and Hill has some examples of this kind (I do not have
>> it here now, though).
>>
>>
>>
>> Best,
>>
>>
>> R.
>>
>>
>>
>> >
>> > Thanks a lot,
>> > Nimrod
>> >
>> >
>> > On Thu, Mar 27, 2014 at 4:39 AM, Ramon Diaz-Uriarte <rdiaz02 at gmail.com
>> >wrote:
>> >
>> >> Dear Nimrod,
>> >>
>> >> Just my 2 cents (even less than 2):
>> >>
>> >> On Thu, 01-01-1970, at 01:00, nimrod.rubinstein <
>> >> nimrod.rubinstein at gmail.com> wrote:
>> >> > Hi all,
>> >> >
>> >> > I have the following data:
>> >> >
>> >> > 50 subjects that belong to 2 groups (25 in each group). For each
>> subject
>> >> I
>> >> > have replicated measurements (in the range of 15-30 replicates per
>> >> subject)
>> >> > of the number of trails as well as the number successes out of these
>> >> > trails. Each single trial in each single replicate for each subject
>> is
>> >> > assumed to be a Bernoulli experiment, therefore the number of
>> successes
>> >> in
>> >> > each replicate is assumed to be a binomially distributed random
>> variable.
>> >> > To complicate things even more, for each subject from each group I
>> have
>> >> not
>> >> > 1 but 3 series of these replicated measurements, from 3 different
>> >> > environments (the 3 environments are the same for all subjects).
>> >> >
>> >> > A few example lines of how my data looks like:
>> >> > subject group environment     number.trails  number.successes
>> >> >
>> >> >     S1    G1          E1 10100,13209,17621  8500,11622,14684
>> >> >
>> >> >     S1    G1          E2 13322,12225,10024  10351,10222,7951
>> >> >
>> >> >     S1    G1          E3  9812,13500,13214  7245,10622,10684
>> >> >
>> >> >     .
>> >> >
>> >> >     .
>> >> >
>> >> >     .
>> >> >
>> >> >     S50    G2          E1 60547,75345,68412 12014,13214,15244
>> >> >
>> >> >     S50    G2          E2 50897,62541,61247 10141,13251,30645
>> >> >
>> >> >     S50    G2          E3 59214,84264,74156 17245,16425,11123
>> >> >
>> >> >
>> >> > Where the comma separated number of successes correspond to the comma
>> >> > separated number of trials in each replicate.
>> >> >
>> >> > I want to test whether the variances of the success probabilities of
>> >> > subjects from group G1 are significantly smaller than the variances
>> of
>> >> the
>> >> > success probabilities of subjects from group G2.
>> >> >
>> >>
>> >> But, as you say in point 1. below (and in another reply), if this is
>> really
>> >> binomial, then you already know that if probabilities in G1 are
>> different
>> >> from probabilities in G2 your variances will differ.
>> >>
>> >> So I am not sure I even understand this hypothesis/question.
>> >>
>> >>
>> >> > In other words, for each subject in each replicate a success
>> probability
>> >> > can be estimated from the observed data. Then the variance of those
>> >> success
>> >> > probabilities over all replicates from a specific environment can be
>> >> > estimated. I would like to know whether these variances are smaller
>> in
>> >> > subjects from G1 vs. subjects from G2.
>> >> >
>> >> > The issues are:
>> >> >
>> >> >
>> >> >    1. The variance of a success probability depends on the success
>> >> >    probability itself (natural to binomially distributed data). I
>> need to
>> >> >    account for this effect.
>> >> >    2. The number of trails (although at least in the hundreds for all
>> >> >    subjects) varies several orders of magnitude among subjects. That
>> by
>> >> itself
>> >> >    affects the accuracy of estimating the success probability, but in
>> >> >    addition, I cannot rule out an inherent dependency between the
>> number
>> >> of
>> >> >    observed trials and the success probability (there is no control
>> over
>> >> the
>> >> >    number of trials in the experiment - it is observed). In other
>> words,
>> >> >    having a lower number of trails by itself may affect the success
>> >> >    probability.
>> >> >
>> >>
>> >> That last issue seems very problematic to me. If I understand
>> correctly, we
>> >> could think of a perverse situation where, say, you can never observe
>> low
>> >> success probabilities if those are related to number of trials around
>> 0.
>> >>
>> >>
>> >> Can you do a simple plot of probability of success (or its logit) vs
>> number
>> >> of trials, to rule out perverse scenarios? Is probability of success
>> always
>> >> larger than 0? And number of trials always sufficiently different from
>> 0?
>> >>
>> >>
>> >> And you say "varies by subject", so the number of trials is always the
>> same
>> >> for a subject? And the potential relationship prob <-> number of
>> trials is
>> >> due to among-subject variation? So you could approach the problem via a
>> >> random effect for subject? If you compare G1 and G2 for number of
>> trials:
>> >> are there differences?
>> >>
>> >>
>> >>
>> >>
>> >>
>> >> >
>> >> > I though a simple way to test my hypothesis is to compute the success
>> >> > probability in each replicate as number.successes/number.trails, and
>> then
>> >> > the sample mean and sample variance of these success probabilities
>> over
>> >> all
>> >> > replicates, as well as the mean number of trials over all replicates
>> >> (added
>> >> > to my data as sample.mean, sample.var and mean.trials fields). Then
>> I can
>> >> > use the lmer function in the lme4 package this way:
>> >> >
>> >> > null.model = lmer(sample.var ~ sample.mean + mean.trials +
>> >> (1|environment),
>> >> > my.data, REML = F)
>> >> > alt.model = lmer(sample.var ~ sample.mean + mean.trials + group +
>> >> > (1|environment), my.data, REML = F)
>> >> >
>> >> > So fixed effects are: sample.mean, mean.trials and group, and random
>> >> > effects is environment since the three environments in my data are
>> >> simply a
>> >> > random sample from many other possible environments.
>> >>
>> >>
>> >> I do not see how this would answer your questions, given the above
>> >> concerns/issues and other possible approaches (just compare prob.
>> between
>> >> G1 and G2, accounting for environment).
>> >>
>> >>
>> >> Best,
>> >>
>> >>
>> >> R.
>> >>
>> >>
>> >> >
>> >> > To test whether group has a significant effect I'll do:
>> >> >
>> >> > anova(null.model, alt.model).
>> >> >
>> >> > My concerns are:
>> >> >
>> >> >
>> >> >    1. Should I add subject as a random effect? My intuition is not to
>> >> since
>> >> >    subjects are unique to groups, which is the effect I'm after.
>> >> However, I do
>> >> >    want to account for the fact that sample variances of the same
>> >> subject from
>> >> >    the three different environments are correlated.
>> >> >    2. The response - sample variances, cannot be assumed to follow a
>> >> normal
>> >> >    distribution (they only take positive values to begin with and as
>> far
>> >> as I
>> >> >    know (n-1)*sample.variance/population.variance ~ chi.square with
>> df =
>> >> n-1)
>> >> >    and therefore the assumptions of a linear mixed effects model do
>> not
>> >> >    strictly hold. Is there a generalized linear model that would be
>> >> >    appropriate for this?
>> >> >
>> >> > Thanks a lot,
>> >> > nimrod
>> >> >
>> >> >       [[alternative HTML version deleted]]
>> >> >
>> >> > _______________________________________________
>> >> > R-sig-mixed-models at r-project.org mailing list
>> >> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >>
>> >> --
>> >> Ramon Diaz-Uriarte
>> >> Department of Biochemistry, Lab B-25
>> >> Facultad de Medicina
>> >> Universidad Aut?noma de Madrid
>> >> Arzobispo Morcillo, 4
>> >> 28029 Madrid
>> >> Spain
>> >>
>> >> Phone: +34-91-497-2412
>> >>
>> >> Email: rdiaz02 at gmail.com
>> >>        ramon.diaz at iib.uam.es
>> >>
>> >> http://ligarto.org/rdiaz
>> >>
>> >>
>>
>> --
>> Ramon Diaz-Uriarte
>> Department of Biochemistry, Lab B-25
>> Facultad de Medicina
>> Universidad Aut?noma de Madrid
>> Arzobispo Morcillo, 4
>> 28029 Madrid
>> Spain
>>
>> Phone: +34-91-497-2412
>>
>> Email: rdiaz02 at gmail.com
>>        ramon.diaz at iib.uam.es
>>
>> http://ligarto.org/rdiaz
>>
>>
>

From szymonmarian.drobniak at uzh.ch  Fri Mar 28 11:07:11 2014
From: szymonmarian.drobniak at uzh.ch (Szymek Drobniak)
Date: Fri, 28 Mar 2014 11:07:11 +0100
Subject: [R-sig-ME] Prediction from a multinomial model in MCMCglmm
Message-ID: <CANXb-o4bTnfbRqrVDE2WAeq7xC94bJ2e-KUXbWBeWwM2txehRw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140328/e90e4eeb/attachment.pl>

From k.neijenhuijs at gmail.com  Sat Mar 29 19:49:13 2014
From: k.neijenhuijs at gmail.com (Koen Neijenhuijs)
Date: Sat, 29 Mar 2014 19:49:13 +0100
Subject: [R-sig-ME] GPU computing with lme4
Message-ID: <AB52D034-4990-439E-A198-246021FA868B@gmail.com>

Dear list,

I know of a few R packages specifically tailored for GPU computing of models. I have not been able to find information on whether lme4 supports any kind of GPU computing, either internally or externally (through third party packages/software).

Is there anyone who has some information regarding this possibility and if it is possible, which GPU architectures might be compatible?

Kind regards,

Koen Neijenhuijs

From neilandertal at gmail.com  Sat Mar 29 20:25:23 2014
From: neilandertal at gmail.com (Neil Collier)
Date: Sat, 29 Mar 2014 20:25:23 +0100
Subject: [R-sig-ME] Message error
In-Reply-To: <C1988BA41CF49042A290C6CF791789EF1378D0BD@ICTS-S-MBX13.luna.kuleuven.be>
References: <C1988BA41CF49042A290C6CF791789EF1378D0BD@ICTS-S-MBX13.luna.kuleuven.be>
Message-ID: <CAOVghqqmooqZPpctiTVQXbuzH_V-23O8PVr2QKBOrPSTbNG=gg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140329/fd6faa39/attachment.pl>

From bbolker at gmail.com  Sat Mar 29 22:35:06 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 29 Mar 2014 17:35:06 -0400
Subject: [R-sig-ME] GPU computing with lme4
In-Reply-To: <AB52D034-4990-439E-A198-246021FA868B@gmail.com>
References: <AB52D034-4990-439E-A198-246021FA868B@gmail.com>
Message-ID: <53373C8A.8010303@gmail.com>

On 14-03-29 02:49 PM, Koen Neijenhuijs wrote:
> Dear list,
> 
> I know of a few R packages specifically tailored for GPU computing of
> models. I have not been able to find information on whether lme4
> supports any kind of GPU computing, either internally or externally
> (through third party packages/software).
> 
> Is there anyone who has some information regarding this possibility
> and if it is possible, which GPU architectures might be compatible?
> 
> Kind regards,
> 
> Koen Neijenhuijs _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

  I am not aware that lme4 supports any kind of GPU computing, which
means that it's unlikely that it does. Doug Bates might chime in.  I
would say that your best hope (although rather unlikely) is that someone
is thinking about how to get the Eigen package, which does the heavy
computational lifting in lme4 ...

  People looking for even more high-performance mixed model computing
might take a look at Doug's MixedModel package in Julia.

  cheers
    Ben Bolker
>


From paul.tanger at colostate.edu  Sun Mar 30 06:47:54 2014
From: paul.tanger at colostate.edu (Paul Tanger)
Date: Sat, 29 Mar 2014 22:47:54 -0600
Subject: [R-sig-ME] lsmeans for multiple response variables
Message-ID: <CAPEW02YGZcyCLdb3oDxMVUBuQ-Y+N5xG7tqTi_OX9Zib_xfGUw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140329/41b8dfa9/attachment.pl>

From bbolker at gmail.com  Sun Mar 30 16:59:09 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 30 Mar 2014 10:59:09 -0400
Subject: [R-sig-ME] Message error
In-Reply-To: <CAOVghqqmooqZPpctiTVQXbuzH_V-23O8PVr2QKBOrPSTbNG=gg@mail.gmail.com>
References: <C1988BA41CF49042A290C6CF791789EF1378D0BD@ICTS-S-MBX13.luna.kuleuven.be>
	<CAOVghqqmooqZPpctiTVQXbuzH_V-23O8PVr2QKBOrPSTbNG=gg@mail.gmail.com>
Message-ID: <5338313D.8030700@gmail.com>

   We probably need more detail to know anything for sure.

  These warnings were introduced in the most recent release of lme4
(1.1-5), and *may* be false positives.  Two things to check:

 (1) is your model fit singular?  If m is your fitted model, try
something like

  lbound <- getME(m,"lower")
  theta <- getME(m,"theta")
  any(lbound==0 & theta<1e-8)

 or fit your model with (g)lmerControl(check.conv.singular="warning")

 (2) if this is a glmer() fit, try glmerControl(optimizer="bobyqa") and
see if that helps.


On 14-03-29 03:25 PM, Neil Collier wrote:
> I'm not sure about the details of the error message but if R is suggesting
> to rescale the variables then you can do this using the scale() function as
> one option. Doing this usually helps the model converge.
> 
> ?scale
> 
> Neil.
> 
> 
> On Fri, Mar 28, 2014 at 1:31 PM, Cintia Akemi Oi <
> CintiaAkemi.Oi at bio.kuleuven.be> wrote:
> 
>> Hello,
>>
>> I'm having this warning messages:
>>
>> Warning messages:
>> 1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>>   Model failed to converge with max|grad| = 0.0523852 (tol = 0.001)
>> 2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>>   Model is nearly unidentifiable: very large eigenvalue
>>  - Rescale variables?
>>
>> Thanks in advance.
>>
>>
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From bbolker at gmail.com  Sun Mar 30 17:29:31 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 30 Mar 2014 11:29:31 -0400
Subject: [R-sig-ME] lsmeans for multiple response variables
In-Reply-To: <CAPEW02YGZcyCLdb3oDxMVUBuQ-Y+N5xG7tqTi_OX9Zib_xfGUw@mail.gmail.com>
References: <CAPEW02YGZcyCLdb3oDxMVUBuQ-Y+N5xG7tqTi_OX9Zib_xfGUw@mail.gmail.com>
Message-ID: <5338385B.8080305@gmail.com>

On 14-03-30 12:47 AM, Paul Tanger wrote:
> Hi,
> Not sure this is the best place to ask since I'm fitting a fixed effect
> model.. please advise if there is a better forum..
> I'm trying to get lsmeans for a set of response variables, but I'm getting
> this error and I can't figure out how to get it working.. it just spits out
> the lsmeans for the first response.
> Here is an example:

  The short answers are that (1) this is indeed not a mixed-model question;
try r-help at r-project.org or Stack Overflow (2) you may not be aware that
you're fitting two separate linear models here --  from ?lm

   If ?response? is a matrix a linear model is fitted separately by
     least-squares to each column of the matrix.

This is a relatively less-used feature, so presumably lsmeans()
doesn't expect it.  You could fit the two models separately, or
use lapply() ... or request of the lsmeans maintainer that they
change lsmeans to allow such support.

  Ben Bolker


From k.neijenhuijs at gmail.com  Mon Mar 31 12:15:33 2014
From: k.neijenhuijs at gmail.com (Koen Neijenhuijs)
Date: Mon, 31 Mar 2014 12:15:33 +0200
Subject: [R-sig-ME] R-sig-mixed-models Digest, Vol 87, Issue 48
In-Reply-To: <mailman.5.1396173602.7105.r-sig-mixed-models@r-project.org>
References: <mailman.5.1396173602.7105.r-sig-mixed-models@r-project.org>
Message-ID: <CACdAfxt-r=6jVtdEC+t8BTpWYty_NsTt0gXkHkL_B=JtEBWxvw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140331/403d42f7/attachment.pl>

From john.hodsoll at kcl.ac.uk  Mon Mar 31 15:04:03 2014
From: john.hodsoll at kcl.ac.uk (Hodsoll, John)
Date: Mon, 31 Mar 2014 13:04:03 +0000
Subject: [R-sig-ME] summary fn zero-inflated MCMC models.
Message-ID: <af732c6f3e7f402eab6d5732d8ae4c9e@AM3PR03MB563.eurprd03.prod.outlook.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140331/16389550/attachment.pl>

From j.hadfield at ed.ac.uk  Mon Mar 31 18:38:37 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Mon, 31 Mar 2014 17:38:37 +0100
Subject: [R-sig-ME] summary fn zero-inflated MCMC models.
In-Reply-To: <af732c6f3e7f402eab6d5732d8ae4c9e@AM3PR03MB563.eurprd03.prod.outlook.com>
References: <af732c6f3e7f402eab6d5732d8ae4c9e@AM3PR03MB563.eurprd03.prod.outlook.com>
Message-ID: <20140331173837.13842co7j7pjutxc@www.staffmail.ed.ac.uk>

Hi,

That's annoying. The bug happens with print.summary, and will occur  
for all zero-inflated, hurdle and multinomial models with idh residual  
structure. I will upload a fix ASAP. For now,

cf.za.1$Residual$nrt<-2

should work when using summary(cf.za.1).

Cheers,

Jarrod



Quoting "Hodsoll, John" <john.hodsoll at kcl.ac.uk> on Mon, 31 Mar 2014  
13:04:03 +0000:

> Dear all / Jarrod
>
> Have just updated to the latest version of MCMCglmm but am now  
> unable to produce model info using the summary function for zero  
> inflated models. It doesn't work on hurdle models fitted on the  
> previous version either (but these do on an older version I have on  
> my laptop).
>
> Error is
>
> Error in rep(rep(1:length(object$Residual$nrt), object$Residual$nrt),  :
>   invalid 'times' argument
>
> Unless something was wrong with my model specification previously  
> and something in the update is now showing that?
>
> Best wishses
> John Hodsoll
>
> prior and model definitions..
>
> #### Define prior  #####
> zal2.prior <-  list(B= list (mu = matrix(c(rep(0,20)),20),V = diag(20)*(20)),
>                     R = list(V = diag(2), n = 0.002, fix = 2),
>                     G = list(G1 = list(V = diag(2), n = 0.002)))
>
>
> zal2.prior$B$mu[9] <- 1
> diag(zal2.prior$B$V)[9]<-1e-9
>
> zal2.prior$B$mu[10] <- 1
> diag(zal2.prior$B$V)[10]<-1e-9
>
> #Level 2 models with complementary log log link.
>
> system.time(
>   cf.za.1 <- MCMCglmm(totflct ~ trait-1 + trait:(expcon.r*period.x +  
> log.beds + nshift + cday),
>                       data = rswAll.df, family = "zapoisson",
>                       random = ~us(trait):wardn,
>                       rcov = ~idh(trait):units,
>                       prior = zal2.prior,
>                  #     nitt = 300000, burnin = 50000, thin = 250,
>                       verbose = TRUE, pr = TRUE, pl = FALSE)
> )
>
> summary(cf.za.1)
> save(cf.za.1, file="cf.za.1.rda")
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From bbolker at gmail.com  Mon Mar 31 23:42:23 2014
From: bbolker at gmail.com (lme4 maintainer)
Date: Mon, 31 Mar 2014 17:42:23 -0400
Subject: [R-sig-ME] [Lme4-authors] Underdispersion in GLMM
In-Reply-To: <CF49F8D43312F648A7CE460833970E414C7B609C@mbx05.adf.bham.ac.uk>
References: <CF49F8D43312F648A7CE460833970E414C7B609C@mbx05.adf.bham.ac.uk>
Message-ID: <5339E13F.9010704@lists.r-forge.r-project.org>

  [I'm taking the liberty of forwarding this to r-sig-mixed-models,
where I think it is more appropriate.]

  A few thoughts about underdispersion:

 * underdispersed _count_ data (e.g. litter sizes for large mammals) are
often better captured by an ordinal model (e.g. ordinal::clmm)
 * you _might_ be able to fit underdispersed binomial data by allowing
negative correlation structure within groups: this could be tricky,
though, depending on the size and complexity of your data set.  (The
standard approach of nested random effects is essentially a compound
symmetry structure, but only allows positive/non-negative correlations.
 The nlme package enabled fitting models with a more general CS
structure, but that's not implemented in lme4, and slightly harder to do
in GLMMs in general).
 * the standard 'quasi-likelihood' approach, i.e. taking the estimated
level of underdispersion and shrinking all the confidence intervals
accordingly, might be a reasonable first/hackish approach.  The thing to
be careful about there is that I have yet to read any good treatment (or
figure out for myself) how quasi-likelihood estimates of 'residual'
variance interact with the estimates of the random effects variances ...

  good luck,
  Ben Bolker

On 14-03-31 03:51 PM, Eva Reindl wrote:
> Dear Mr Bates, Mr Maechler, Mr Bolker, Mr Walker, Mr Christensen, and
> Mr Singmann,
> 
> My name is Eva Reindl and I am a Phd student at the University of
> Birmingham working with your lme4 package to analyse my data. I am
> running GLMMs with a binomial error structure and found that my full
> model is heavily underdispersed. I am writing you because I would
> like to ask whether your package offers any strategy to deal with
> underdispersion. Any help or reference to resources would be greatly
> appreciated.
> 
> I would like to thank you very much in advance for your efforts.
> 
> Yours sincerely,
> 
> Eva Reindl
> 
> Eva Reindl MSc Doctoral Researcher at School of Psychology University
> of Birmingham Edgbaston Birmingham B15 2TT United Kingdom
> 
> phone: 0121 414 7209 E-mail: EMR328 at bham.ac.uk 
> _______________________________________________ Lme4-authors mailing
> list Lme4-authors at lists.r-forge.r-project.org 
> https://lists.r-forge.r-project.org/cgi-bin/mailman/listinfo/lme4-authors
>


From qyang at bu.edu  Mon Mar 31 16:09:47 2014
From: qyang at bu.edu (Yang, Qiong)
Date: Mon, 31 Mar 2014 14:09:47 +0000
Subject: [R-sig-ME] gee, geese and glmer
In-Reply-To: <002d01cf431b$cc8fae20$65af0a60$@bu.edu>
References: <007801cf3a86$6d27b060$47771120$@bu.edu>
	<531B98B4.4050403@gmail.com>
	<6984CA5898097D459F8EC753F025B0D839696447@IST-EX10MBX-3.ad.bu.edu>
	<21278.56111.63015.103003@stat.math.ethz.ch>
	<531F0F03.5090907@gmail.com>
	<6984CA5898097D459F8EC753F025B0D8396982CD@IST-EX10MBX-3.ad.bu.edu>
	<53221072.9080203@gmail.com> <000301cf3f1f$f07fb0f0$d17f12d0$@bu.edu>
	<5322776E.90206@gmail.com> <005f01cf3f8a$d73cda80$85b68f80$@bu.edu>
	<53231B51.8070300@gmail.com> <011401cf3fb1$71a56620$54f03260$@bu.edu>
	<532375A4.5060703@gmail.com> <013201cf4218$90a933d0$b1fb9b70$@bu.edu>
	<5327522A.8090708@gmail.com> <00c901cf4254$62359ac0$26a0d040$@bu.edu>
	<53285880.2030502@gmail.com>
	<3B0125C6-1F8E-493C-9A16-F67CE4FF63B6@nyu.edu>
	<00db01cf42db$0f691650$2e3b42f0$@bu.edu> <5328ACED.3000907@gmail.com>
	<002d01cf431b$cc8fae20$65af0a60$@bu.edu>
Message-ID: <6984CA5898097D459F8EC753F025B0D8396A6705@IST-EX10MBX-3.ad.bu.edu>

Hi Ben,
I did some exploration of the data. There is a substantial number of cases who are singleton (sorry I thought all cases are singleton in last email).
In overall sample, the crude odds ratio (OR) ignoring family structure is negative consistent with old lme, GEE. New lme4 returns a positive OR.
	All			
	male(1)	female(2)		
0	2554	3021		OR=
1	310	290		0.790872495
				
If we only include singleton cases -versus family controls, the crude OR is negative.		
	male(1)	female(2)		
0	2554	3021		OR=
1	249	182		0.617934166
				
If we only include family cases-versus family controls, the crude OR is positive.		
	male(1)	female(2)	
0	2554	3021		OR=
1	61	108		1.496801081

I wonder if singleton cases are dealt differently between new and old lme4 that causes different signs of sex in the results?

Thanks
Qiong 
----Original Message-----
From: Ming-Huei Chen [mailto:mhchen at bu.edu] 
Sent: Tuesday, March 18, 2014 10:35 PM
To: 'Ben Bolker'; 'Vincent Dorie'
Cc: Yang, Qiong; 'r-sig-mixed-models'; 'lme4-authors at lists.r-forge.r-project.org'
Subject: RE: [R-sig-ME] gee, geese and glmer

I just installed latest lme4. Here is the result... 

>
blme::bglmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data=test.da
t,fixef.prior=t)
Cov prior  : famid ~ wishart(df = 3.5, scale = Inf, posterior.scale = cov, common.scale = TRUE) Fixef prior: t(df = 5, scale = c(100, 0, 0, 0, ...), common.scale = FALSE) Prior dev  : 16.1703

Generalized linear mixed model fit by maximum likelihood (Laplace
  Approximation) [bglmerMod]
 Family: binomial ( logit )
Formula: case ~ sex + PC1 + PC2 + PC3 + PC4 + (1 | famid)
   Data: test.dat
      AIC       BIC    logLik  deviance  df.resid
 3005.889  3052.986 -1495.944  2991.889      6168
Random effects:
 Groups Name        Std.Dev.
 famid  (Intercept) 19.89
Number of obs: 6175, groups: famid, 1592 Fixed Effects:
(Intercept)          sex          PC1          PC2          PC3          PC4
   -10.2654       0.3371      -0.3533      -0.1078      -0.2225       0.1102

-----Original Message-----
From: Ben Bolker [mailto:bbolker at gmail.com]
Sent: Tuesday, March 18, 2014 4:31 PM
To: Ming-Huei Chen; 'Vincent Dorie'
Cc: 'Yang, Qiong'; 'r-sig-mixed-models'; 'lme4-authors at lists.r-forge.r-project.org'
Subject: Re: [R-sig-ME] gee, geese and glmer

On 14-03-18 02:51 PM, Ming-Huei Chen wrote:
> Is lme4_1.1-4 out-of-date? This is the version I used and encountered 
> the error.
> 
> Best,
> 
> Ming-Huei

  Yes, 1.1-5 was released on CRAN recently (and blme did some updating to match).

  Ben Bolker

> 
> -----Original Message-----
> From: Vincent Dorie [mailto:vjd4 at nyu.edu]
> Sent: Tuesday, March 18, 2014 11:48 AM
> To: Ben Bolker
> Cc: Yang, Qiong; Ming-Huei Chen; r-sig-mixed-models; 
> lme4-authors at lists.r-forge.r-project.org
> Subject: Re: [R-sig-ME] gee, geese and glmer
> 
>>>
>>> blme::bglmer(case~sex+PC1+PC2+PC3+PC4+(1|famid),family=binomial,data
>>> =
>>> test.da
>>> t,fixef.prior=t)
>>> Error in get("checkConv", lme4Env)(attr(opt, "derivs"), opt$par, 
>>> ctrl = control$checkConv,  :
>>>  unused argument (ctrl = control$checkConv)
> 
> I believe this can only happen if the installed version of lme4 is 
> out-of-date. If there was a point where checkConv was added but didn't 
> yet have the ctrl formal, you would see that error.
> 
> Vince
> 


