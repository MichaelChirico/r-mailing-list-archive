From pdalgd at gmail.com  Sun Jul  1 11:39:21 2012
From: pdalgd at gmail.com (peter dalgaard)
Date: Sun, 1 Jul 2012 11:39:21 +0200
Subject: [R-sig-ME] "mixed" MANOVAs
In-Reply-To: <8BBF49B5-97A2-4A9B-9E96-9463F98226B3@gmail.com>
References: <1340989717.33487.YahooMailNeo@web29701.mail.ird.yahoo.com>
	<8BBF49B5-97A2-4A9B-9E96-9463F98226B3@gmail.com>
Message-ID: <81113AD7-99EF-4F7C-9426-34F43B5F0053@gmail.com>


On Jun 30, 2012, at 21:22 , peter dalgaard wrote:

>> 
>> m <-  aov(cbind(yield,foo) ~  N*P*K + Error(block), npk)
> Warning message:
> In aov(cbind(yield, foo) ~ N * P * K + Error(block), npk) :
>  Error() model is singular
> 
> (The two do basically the same thing, manova() does little more than adding an extra class to the result.)
> 
> The warning is a bit obscure; it probably just means that something inside aov() didn't expect a matrix response. 

On closer examination, the warning is just a buglet: Inside aov() we should be looking at NROW(er.fit$coefficients), not length(). So the situation seems to be that the feature (multistratum MANOVA) has indeed implemented itself...

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From muhammad.mullah at mail.mcgill.ca  Sun Jul  1 02:31:22 2012
From: muhammad.mullah at mail.mcgill.ca (Muhammad Mullah)
Date: Sun, 1 Jul 2012 00:31:22 +0000
Subject: [R-sig-ME] Using glmer in R
Message-ID: <1813C6762739724EA611B27328FABABF7E5877@EXMBX2010-1.campus.MCGILL.CA>

Hi,
Can anybody helps me to figure out how ?glmer? or ?lmer? in R can be used to estimate the random effects in mixed model so that all random effects are independent and have the same variance, i.e., the variance covariance matrix will have same value for diagonal element and zero for covariance.

With Kind Regards,
Shadeque


From w.d.wadsworth at gmail.com  Sun Jul  1 21:34:24 2012
From: w.d.wadsworth at gmail.com (W Duncan Wadsworth)
Date: Sun, 1 Jul 2012 14:34:24 -0500
Subject: [R-sig-ME] lmer() syntax and model specification
Message-ID: <B60B4EF8-A861-4474-9189-EFD0FECE2529@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120701/eb0705e9/attachment.pl>

From w.d.wadsworth at gmail.com  Sun Jul  1 21:37:47 2012
From: w.d.wadsworth at gmail.com (W Duncan Wadsworth)
Date: Sun, 1 Jul 2012 14:37:47 -0500
Subject: [R-sig-ME] edit: lmer() syntax and model specification
Message-ID: <BEE3D19C-633F-4AB8-A15B-FCA25FAC56DD@gmail.com>

woops, arithmetic error!

"Four treatments, crossed with three aspects (North, South, None), giving eight distinct plots.  There are 3 to 8 trees of each species in each of the eight plots and they are outfitted with sensors which provide the (normal) response variable."

*should read*

"Four treatments, crossed with three aspects (North, South, None), giving twelve distinct plots.  There are 3 to 8 trees of each species in each of the twelve plots and they are outfitted with sensors which provide the (normal) response variable. "

From David.Duffy at qimr.edu.au  Mon Jul  2 00:49:01 2012
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Mon, 2 Jul 2012 08:49:01 +1000 (EST)
Subject: [R-sig-ME] Using glmer in R
In-Reply-To: <1813C6762739724EA611B27328FABABF7E5877@EXMBX2010-1.campus.MCGILL.CA>
References: <1813C6762739724EA611B27328FABABF7E5877@EXMBX2010-1.campus.MCGILL.CA>
Message-ID: <Pine.LNX.4.64.1207020837580.29665@orpheus.qimr.edu.au>

On Sun, 1 Jul 2012, Muhammad Mullah wrote:

> Can anybody helps me to figure out how ?glmer? or ?lmer? in R can be 
> used to estimate the random effects in mixed model so that all random 
> effects are independent and have the same variance, i.e., the variance 
> covariance matrix will have same value for diagonal element and zero for 
> covariance.

Usually the question is how to make them heterogenous ;)

Have you read the vignettes
vignette(package="lme4")
the draft book chapters at
http://lme4.r-forge.r-project.org/
lecture notes at
http://lme4.r-forge.r-project.org/slides/
wiki at
http://glmm.wikidot.com/


From cacamendes85 at gmail.com  Mon Jul  2 18:31:55 2012
From: cacamendes85 at gmail.com (Camila Mendes)
Date: Mon, 2 Jul 2012 09:31:55 -0700
Subject: [R-sig-ME] On deriving lmer formula
Message-ID: <CAPX9pWqbM-v-n6UDmTySbt08auXcu_9YMVh2exwTZ6F6izRHyw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120702/9311df64/attachment.pl>

From bbolker at gmail.com  Mon Jul  2 18:39:35 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 2 Jul 2012 16:39:35 +0000 (UTC)
Subject: [R-sig-ME] "mixed" MANOVAs
References: <1340989717.33487.YahooMailNeo@web29701.mail.ird.yahoo.com>
	<8BBF49B5-97A2-4A9B-9E96-9463F98226B3@gmail.com>
	<81113AD7-99EF-4F7C-9426-34F43B5F0053@gmail.com>
Message-ID: <loom.20120702T183807-651@post.gmane.org>

peter dalgaard <pdalgd at ...> writes:

> 
> 
> On Jun 30, 2012, at 21:22 , peter dalgaard wrote:
 
[snip]
 
> On closer examination, the warning is just a buglet: Inside aov() we
> should be looking at NROW(er.fit$coefficients), not length(). So the
> situation seems to be that the feature (multistratum MANOVA) has
> indeed implemented itself...

fortune() candidate?

You know you've written a mature software environment when it starts
implementing new features on its own ...

  Ben Bolker


From pauljohn32 at gmail.com  Tue Jul  3 01:16:44 2012
From: pauljohn32 at gmail.com (Paul Johnson)
Date: Mon, 2 Jul 2012 18:16:44 -0500
Subject: [R-sig-ME] Fitting multilevel model to a recipe and simulating
 the model.
In-Reply-To: <1340976400.16374.YahooMailNeo@web113407.mail.gq1.yahoo.com>
References: <1340976400.16374.YahooMailNeo@web113407.mail.gq1.yahoo.com>
Message-ID: <CAErODj8ueBGT39Ps=icJ1qgv0WeE8cHy_Tg7FOEbOYUkR5HxtQ@mail.gmail.com>

On Fri, Jun 29, 2012 at 8:26 AM, Justice Moses K. Aheto
<justiceaheto at yahoo.com> wrote:
> Dear All,
> I am a new R user but currently working on my dissertation which request that I fit multilevel model to a given situation that can be located in the attachment to this mail and simulate the model 10000 times.
> In addition, I will like to compute the standard error, t-value and p-value for the fitted model.
> I am available to provide further clarification on the task should you request for.
> Could someone help me in this regards.
> I do appreciate any assistance given me.
> Many thanks in advance.
>
> Kind regards.
>
> Justice Moses K. Aheto
> (Chief Executive Officer)
> Statistics & Analytics Consultancy Services Ltd.

In order to get this done, you will have to learn quite a bit of R and
mixed effects modeling.  What is your time frame for the project?  I
see you are a Chief Executive Officer, it may behoove you to hire a
programmer rather than learning this yourself. I think you will see
you have to think this through in stages, you try and fail in
translating your simple idea into a final result.

I have some notes about building simulations of that sort here:

http://pj.freefaculty.org/guides/stat/MonteCarloExperiments

I have a tutorial on that similar exercise here:

http://winstat.quant.ku.edu/svn/hpcexample/trunk/Ex80-PrevSci2007

Read through the Version-X.R files to get the idea.  Putting that on a
cluster computing system is the end goal.

I'm serious, though. You have to work through one test sample very
carefully to be sure you understand your model, and then worry about
drawing more samples and summarizing.

I'm working on a tutorial to go through those steps for a mixed model
just exactly as you describe, but have only worked so far on the parts
for generating one data set and analyzing it.  Next I'll come to the
part about doing that over and over and harvesting the results.

You need to learn lmer well enough to decide which parameter estimates
are worth tracking in a simulation, and ignoring the rest.  Especially
where we are concerned with estimating variance components and the
associated degreees of freedom, or confidence intervals for
predictions, I find we have plenty of questions and not so many
definitive answers.

pj
-- 
Paul E. Johnson
Professor, Political Science    Assoc. Director
1541 Lilac Lane, Room 504     Center for Research Methods
University of Kansas               University of Kansas
http://pj.freefaculty.org            http://quant.ku.edu


From pdalgd at gmail.com  Tue Jul  3 09:20:39 2012
From: pdalgd at gmail.com (peter dalgaard)
Date: Tue, 3 Jul 2012 09:20:39 +0200
Subject: [R-sig-ME] "mixed" MANOVAs
In-Reply-To: <loom.20120702T183807-651@post.gmane.org>
References: <1340989717.33487.YahooMailNeo@web29701.mail.ird.yahoo.com>
	<8BBF49B5-97A2-4A9B-9E96-9463F98226B3@gmail.com>
	<81113AD7-99EF-4F7C-9426-34F43B5F0053@gmail.com>
	<loom.20120702T183807-651@post.gmane.org>
Message-ID: <F6287F09-6537-44B9-93E5-CC40B9794EB5@gmail.com>


On Jul 2, 2012, at 18:39 , Ben Bolker wrote:

> peter dalgaard <pdalgd at ...> writes:
> 
>> 
>> 
>> On Jun 30, 2012, at 21:22 , peter dalgaard wrote:
> 
> [snip]
> 
>> On closer examination, the warning is just a buglet: Inside aov() we
>> should be looking at NROW(er.fit$coefficients), not length(). So the
>> situation seems to be that the feature (multistratum MANOVA) has
>> indeed implemented itself...
> 
> fortune() candidate?
> 
> You know you've written a mature software environment when it starts
> implementing new features on its own ...

Given that manova() was based on a similar S-PLUS routine, perhaps it is not all that strange. 

Contrary to my initial belief, this really has nothing to do with anova.mlm(). It is summary.manova that does the grunt work. The ingenious part is that manova() does not return an object of class "manova" in the multistratum case, but instead an object of class "aovlist" in which each component has class "manova", and summary.aovlist() automatically does the right thing. 

However, this has been the case since at least the "split from base" in 2003 (quite a few files were moved around at the time and it is painful to track changes in SVN further back). 

The note in ?manova was there at that time too. It may simply have been a mistake based on the fact that summary.manova() has no apparent knowledge of error strata (which as per the above it doesn't need to have.)

Cc to Brian, maybe he can recall the issue.


-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From w.d.wadsworth at gmail.com  Tue Jul  3 19:34:30 2012
From: w.d.wadsworth at gmail.com (W Duncan Wadsworth)
Date: Tue, 3 Jul 2012 12:34:30 -0500
Subject: [R-sig-ME] lmer() syntax and model specification
Message-ID: <4ED2FFB4-AA29-4F42-BEAE-2A323EEF91C5@gmail.com>

Hello All,

Even though I've read around I still find myself struggling with the model specification, and subsequently, the lmer() syntax for a project I'm working on. If anyone out there has comments or advice it would be greatly appreciated.

The design:

An experiment in plant ecology.  Two species, one measured from 2008-2011, the other measured from 2007-2011 (so species-by-year is not fully crossed).  Four treatments, crossed with three aspects (North, South, None), giving twelve distinct plots.  There are 3 to 8 trees of each species in each of the twelve plots and they are outfitted with sensors which provide the (normal) response variable. Some trees have died and others have been fitted with sensors to replace them.  The plot-level predictor is precipitation event so the long form of the data has a row for each tree's response to each rain event.  I would like to use year as a fixed factor where rain events are nested within years and consider the species and treatment trends over the available years.

What I think is a reasonable model:

for t = 2007, 2008, 2009, 2010, 2011 and i = tree number and j = treatment,

Response_tij = beta_0 + beta_1 * Year_t + beta_2 * Species_ij + beta_3 * Year_t * Species_ij + u_0j + u_1j * Year_t + u_0i|j + u_1i|j * rain event + epsilon_tij

What I think the lmer() syntax is for this model:

Response ~ 1 + Year : Species + (1 + Year | treatment) + (1 + rain event | treatment : tree number)

where tree number is coded "implicitly" using Prof. Bates' terminology. (See http://tolstoy.newcastle.edu.au/R/e12/help/10/11/3521.html .)  What I'm really going for is a model which fits varying slope and intercept regressions for each tree's responses to rain events, grouped by year, so that slopes and intercepts can be compared between treatments and across years.

Thanks in advance,
Duncan

P.S.  To further complicate things I have a number of site wide predictors and a tree level predictor that would ideally be considered in the model selection process.  Each of those predictors is rain event specific, i.e. it changes for each row of the long form data.  How could those predictor be brought in?  Also, where would aspect go?  More thanks.

From andyflies at gmail.com  Tue Jul  3 23:47:51 2012
From: andyflies at gmail.com (Andy Flies)
Date: Tue, 03 Jul 2012 17:47:51 -0400
Subject: [R-sig-ME] Mixed model correlation structure for unbalanced
	longitudinal data
Message-ID: <4FF36887.5050702@gmail.com>

Dear R users,

I have data from a long-term study that has opportunistically collected 
samples over the past 10 years. My data set is highly unbalanced because 
of the opportunistic sample collection.I have a single sample from 19 
individuals, 2 samples from 4 individuals, and 3 samples from 2 
individuals.I know that lmer can accommodate unbalanced data sets, but I 
am unsure if my data set is too unbalanced.

I am testing if social rank, reproductive status, and age affect my 
response variables. I also need to determine if sample collection 
parameters such as sample date and the time from anesthetizing the 
animal to the time the sample was collected affects the response variables.
Here are what I see as potential options:

1)Use a mixed model with subject as random intercept and sample date as 
random slope to account for potential temporal autocorrelation within 
the repeat samples.
Lmer( y ~ 1 + x1 + x2 + x3 + ? (1 + date | subject)

2)Use a mixed model with subject as random intercept. Initial data 
exploration does not show any obvious temporal autocorrelation.
Lmer( y ~ 1 + x1 + x2 + x3 + ? (1 | subject)

3)Use a GEE and specify an autoregressive correlation structure. I think 
this would be a good option, but from what I have found in the 
literature, my sample size is too small for this.

4)Use the mean for each individual and use a standard linear model. This 
option is not good because it does not allow me to include reproductive 
status as a predictor because reproductive status changes between samples.

5)Use only a single sample from each individual in standard linear 
model. This option is not good because my already limited sample size 
would be further reduced.

Please let me know which of the above options would be best or if you 
can suggest a better option. Any advice or literature references are 
sincerely appreciated.

Thanks,
Andy

-- 
Andy Flies - Ph.D. Candidate
Zoology Department Ecology, Evolutionary Biology, and Behavior (EEBB) 
Program
Michigan State University


From highstat at highstat.com  Wed Jul  4 12:21:54 2012
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Wed, 04 Jul 2012 11:21:54 +0100
Subject: [R-sig-ME] Mixed model correlation structure for unbalanced,
 longitudinal data
In-Reply-To: <mailman.5.1341396002.11535.r-sig-mixed-models@r-project.org>
References: <mailman.5.1341396002.11535.r-sig-mixed-models@r-project.org>
Message-ID: <4FF41942.3070205@highstat.com>




------------------------------

Message: 2
Date: Tue, 03 Jul 2012 17:47:51 -0400
From: Andy Flies <andyflies at gmail.com>
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Mixed model correlation structure for unbalanced
	longitudinal data
Message-ID: <4FF36887.5050702 at gmail.com>
Content-Type: text/plain; charset=windows-1252; format=flowed

Dear R users,

I have data from a long-term study that has opportunistically collected
samples over the past 10 years. My data set is highly unbalanced because
of the opportunistic sample collection.I have a single sample from 19
individuals, 2 samples from 4 individuals, and 3 samples from 2
individuals.I know that lmer can accommodate unbalanced data sets, but I
am unsure if my data set is too unbalanced.

I am testing if social rank, reproductive status, and age affect my
response variables. I also need to determine if sample collection
parameters such as sample date and the time from anesthetizing the
animal to the time the sample was collected affects the response variables.
Here are what I see as potential options:

1)Use a mixed model with subject as random intercept and sample date as
random slope to account for potential temporal autocorrelation within
the repeat samples.
Lmer( y ~ 1 + x1 + x2 + x3 + ? (1 + date | subject)

2)Use a mixed model with subject as random intercept. Initial data
exploration does not show any obvious temporal autocorrelation.
Lmer( y ~ 1 + x1 + x2 + x3 + ? (1 | subject)

3)Use a GEE and specify an autoregressive correlation structure. I think
this would be a good option, but from what I have found in the
literature, my sample size is too small for this.

4)Use the mean for each individual and use a standard linear model. This
option is not good because it does not allow me to include reproductive
status as a predictor because reproductive status changes between samples.

5)Use only a single sample from each individual in standard linear
model. This option is not good because my already limited sample size
would be further reduced.

Please let me know which of the above options would be best or if you
can suggest a better option. Any advice or literature references are
sincerely appreciated.

Thanks,
Andy




Andy...do I understand it well that you have 33 observations in total? If so...then
I don't want to be the boogie man....but......seriously consider simplifying all these
models. Option 4 with only 1 or 2 covariates would be my choice. Ask yourself whether it makes sense to analyze
these data at all...perhaps making only some simple graphs?


Alain



-- 

Dr. Alain F. Zuur
First author of:

1. Analysing Ecological Data (2007).
Zuur, AF, Ieno, EN and Smith, GM. Springer. 680 p.
URL: www.springer.com/0-387-45967-7


2. Mixed effects models and extensions in ecology with R. (2009).
Zuur, AF, Ieno, EN, Walker, N, Saveliev, AA, and Smith, GM. Springer.
http://www.springer.com/life+sci/ecology/book/978-0-387-87457-9


3. A Beginner's Guide to R (2009).
Zuur, AF, Ieno, EN, Meesters, EHWG. Springer
http://www.springer.com/statistics/computational/book/978-0-387-93836-3


4. Zero Inflated Models and Generalized Linear Mixed Models with R. (2012) Zuur, Saveliev, Ieno.
http://www.highstat.com/book4.htm

Other books: http://www.highstat.com/books.htm


Statistical consultancy, courses, data analysis and software
Highland Statistics Ltd.
6 Laverock road
UK - AB41 6FN Newburgh
Tel: 0044 1358 788177
Email: highstat at highstat.com
URL: www.highstat.com
URL: www.brodgar.com


From m.fenati at libero.it  Wed Jul  4 16:48:18 2012
From: m.fenati at libero.it (m.fenati at libero.it)
Date: Wed, 4 Jul 2012 16:48:18 +0200 (CEST)
Subject: [R-sig-ME] MCMCglmm: priors for ordinal regression
Message-ID: <1080250.1669651341413298860.JavaMail.defaultUser@defaultHost>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120704/b8d48961/attachment.pl>

From h.l.ward at qmul.ac.uk  Wed Jul  4 17:24:36 2012
From: h.l.ward at qmul.ac.uk (Helen Ward)
Date: Wed, 04 Jul 2012 16:24:36 +0100
Subject: [R-sig-ME] A zero inflated Poisson model in MCMCglmm
Message-ID: <4FF46034.8070203@qmul.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120704/f57389d7/attachment.pl>

From David.Duffy at qimr.edu.au  Wed Jul  4 23:17:01 2012
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Thu, 5 Jul 2012 07:17:01 +1000
Subject: [R-sig-ME] A zero inflated Poisson model in MCMCglmm
In-Reply-To: <4FF46034.8070203@qmul.ac.uk>
References: <4FF46034.8070203@qmul.ac.uk>
Message-ID: <Pine.LNX.4.64.1207050715460.21356@orpheus.qimr.edu.au>

Sewall Wright used an ordinal model for toes, IIRC ;).  It might be more 
stable.

Cheers, David Duffy,


From dieter.menne at menne-biomed.de  Thu Jul  5 07:35:01 2012
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 5 Jul 2012 05:35:01 +0000 (UTC)
Subject: [R-sig-ME] Adjusting for random recording intervals in glmer/poisson
Message-ID: <loom.20120705T071753-588@post.gmane.org>

In a clinical study, events in patients were observed during multiple visits; on
each visit, a continuous predictor variable for the poisson-distributed number
of events was also available, it is the endpoint of the study.

The following model would be suitable

glmer(nevent~predictor + (1|subj),data=d, family=poisson)

but there is a catch: the recording interval on each day varies randomly, not
related to study parameters, from 30 to 60 minutes. The statistical consultant
at the university recommended the conservative solution to truncate ALL records
to the first 30 minutes, and discard the tails, but the PhD student who did the
study was not too happy to loose all data beyond 30 minutes.

A compromise would be to normalize all data to events/45 minutes (or
median(duration)), assuming that the variance in duration is not too large.

Is there a better way to factor out the nuisance parameter duration?

Dieter


From jwiley.psych at gmail.com  Thu Jul  5 07:52:19 2012
From: jwiley.psych at gmail.com (Joshua Wiley)
Date: Wed, 4 Jul 2012 22:52:19 -0700
Subject: [R-sig-ME] Adjusting for random recording intervals in
	glmer/poisson
In-Reply-To: <loom.20120705T071753-588@post.gmane.org>
References: <loom.20120705T071753-588@post.gmane.org>
Message-ID: <CANz9Z_Lpb2Ep1qu-v3aFBb_jyF1rJNn-JmxLG3CAOHxEpxJDLg@mail.gmail.com>

Hi Dieter,

I do not think that I understand the question or problem very well.
What is the significance of the recording interval varying?  If the
issue is that with a longer recording time, there are more
opportunities for events to occur, then what about treating duration
as an exposure and including it in the offset?  Essentially you model
rate then rather than counts.

Again apologies if I grossly misunderstanding the issue.

Cheers,

Josh


On Wed, Jul 4, 2012 at 10:35 PM, Dieter Menne
<dieter.menne at menne-biomed.de> wrote:
>
> In a clinical study, events in patients were observed during multiple visits; on
> each visit, a continuous predictor variable for the poisson-distributed number
> of events was also available, it is the endpoint of the study.
>
> The following model would be suitable
>
> glmer(nevent~predictor + (1|subj),data=d, family=poisson)
>
> but there is a catch: the recording interval on each day varies randomly, not
> related to study parameters, from 30 to 60 minutes. The statistical consultant
> at the university recommended the conservative solution to truncate ALL records
> to the first 30 minutes, and discard the tails, but the PhD student who did the
> study was not too happy to loose all data beyond 30 minutes.
>
> A compromise would be to normalize all data to events/45 minutes (or
> median(duration)), assuming that the variance in duration is not too large.
>
> Is there a better way to factor out the nuisance parameter duration?
>
> Dieter
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models




--
Joshua Wiley
Ph.D. Student, Health Psychology
Programmer Analyst II, Statistical Consulting Group
University of California, Los Angeles
https://joshuawiley.com/


From dieter.menne at menne-biomed.de  Thu Jul  5 08:16:33 2012
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 5 Jul 2012 08:16:33 +0200
Subject: [R-sig-ME] Adjusting for random recording intervals in
	glmer/poisson
In-Reply-To: <CANz9Z_Lpb2Ep1qu-v3aFBb_jyF1rJNn-JmxLG3CAOHxEpxJDLg@mail.gmail.com>
References: <loom.20120705T071753-588@post.gmane.org>
	<CANz9Z_Lpb2Ep1qu-v3aFBb_jyF1rJNn-JmxLG3CAOHxEpxJDLg@mail.gmail.com>
Message-ID: <000001cd5a75$b9c5bdb0$2d513910$@menne@menne-biomed.de>

Joshua Wiley wrote:

> What is the significance of the recording interval varying?  If the
> issue is that with a longer recording time, there are more
> opportunities for events to occur, then what about treating duration as
> an exposure and including it in the offset?  Essentially you model rate
> then rather than counts.

Good to hear that you suggest it to put it into the offset; I wanted to do this, but was not sure what exactly to put into the offset term. Duration or log(duration)?

Dieter


Apologies: I forgot to attach the simulated sample data in the original message

library(lme4)
nsubj = 10
nvisit = 5
set.seed(100)
d = data.frame(
  subj = as.factor(1:nsubj),
  duration = runif(nsubj*nvisit,30,60),# in minutes
  predictor = rnorm(nsubj*nvisit,50,10))
d$nevent = with(d,rpois(nsubj*nvisit,predictor*duration/500))

# Proposed solution by university statistician: 
# use only the data from the first 30 minutes (not shown here) and do
glmer(nevent~predictor + (1|subj),data=d, family=poisson)
# Result is not correct, because truncated data not used

# Proposed by Joshua
glmer(nevent~predictor+offset(log(duration)) + (1|subj), data=d, family=poisson)


From jwiley.psych at gmail.com  Thu Jul  5 08:26:42 2012
From: jwiley.psych at gmail.com (Joshua Wiley)
Date: Wed, 4 Jul 2012 23:26:42 -0700
Subject: [R-sig-ME] Adjusting for random recording intervals in
	glmer/poisson
In-Reply-To: <4ff53153.8a53b40a.4ba6.4e3bSMTPIN_ADDED@mx.google.com>
References: <loom.20120705T071753-588@post.gmane.org>
	<CANz9Z_Lpb2Ep1qu-v3aFBb_jyF1rJNn-JmxLG3CAOHxEpxJDLg@mail.gmail.com>
	<4ff53153.8a53b40a.4ba6.4e3bSMTPIN_ADDED@mx.google.com>
Message-ID: <6B83267A-235E-4863-9076-1FB258AA905D@gmail.com>

I do not recall for glmer off hand.  Ultimately, you want it To use the same link function as your outcom.  The question is whether glmer does this for you automatically or is like glmmadmb where it is your responsibility to know and use your link on the offset term.  Also, I thought offset was an argument, not a function in glmer, but I could be wrong (I know offset is used as you show it in things like glm).  I imagine you can try it both ways, perhaps with simple simulated data where the effect would be clear.  Or perhaps it's documented, it's late here but I can look in the morning.

Cheers,

Josh

On Jul 4, 2012, at 23:16, "Dieter Menne" <dieter.menne at menne-biomed.de> wrote:

> Joshua Wiley wrote:
> 
>> What is the significance of the recording interval varying?  If the
>> issue is that with a longer recording time, there are more
>> opportunities for events to occur, then what about treating duration as
>> an exposure and including it in the offset?  Essentially you model rate
>> then rather than counts.
> 
> Good to hear that you suggest it to put it into the offset; I wanted to do this, but was not sure what exactly to put into the offset term. Duration or log(duration)?
> 
> Dieter
> 
> 
> Apologies: I forgot to attach the simulated sample data in the original message
> 
> library(lme4)
> nsubj = 10
> nvisit = 5
> set.seed(100)
> d = data.frame(
>  subj = as.factor(1:nsubj),
>  duration = runif(nsubj*nvisit,30,60),# in minutes
>  predictor = rnorm(nsubj*nvisit,50,10))
> d$nevent = with(d,rpois(nsubj*nvisit,predictor*duration/500))
> 
> # Proposed solution by university statistician: 
> # use only the data from the first 30 minutes (not shown here) and do
> glmer(nevent~predictor + (1|subj),data=d, family=poisson)
> # Result is not correct, because truncated data not used
> 
> # Proposed by Joshua
> glmer(nevent~predictor+offset(log(duration)) + (1|subj), data=d, family=poisson)
> 
> 
> 
> 
> 


From dieter.menne at menne-biomed.de  Thu Jul  5 08:39:38 2012
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 5 Jul 2012 08:39:38 +0200
Subject: [R-sig-ME] Adjusting for random recording intervals in
	glmer/poisson
In-Reply-To: <6B83267A-235E-4863-9076-1FB258AA905D@gmail.com>
References: <loom.20120705T071753-588@post.gmane.org>
	<CANz9Z_Lpb2Ep1qu-v3aFBb_jyF1rJNn-JmxLG3CAOHxEpxJDLg@mail.gmail.com>
	<4ff53153.8a53b40a.4ba6.4e3bSMTPIN_ADDED@mx.google.com>
	<6B83267A-235E-4863-9076-1FB258AA905D@gmail.com>
Message-ID: <000001cd5a78$f30949e0$d91bdda0$@menne@menne-biomed.de>

Joshua Wiley wrote:

>> Also, I thought offset was an argument, not a function in glmer, but I
could be
> wrong (I know offset is used as you shown it in things like glm). 

I first simply tried it the glm way, then only noted that it is different in

glmer, but both methods give the same result.

library(lme4)
nsubj = 10
nvisit = 5
set.seed(100)
d = data.frame(
  subj = as.factor(1:nsubj),
  duration = runif(nsubj*nvisit,30,60),# in minutes
  predictor = rnorm(nsubj*nvisit,50,10))
d$nevent = with(d,rpois(nsubj*nvisit,predictor*duration/500))

# The glm way to use offset seems to work ok
glmer(nevent~predictor+offset(log(duration)) + (1|subj), data=d, 
      family=poisson)

# The lme4 documented way
glmer(nevent~predictor+ (1|subj), data=d, 
      offset= log(duration),family=poisson)


From laurent_step at yahoo.fr  Thu Jul  5 10:01:27 2012
From: laurent_step at yahoo.fr (laurent stephane)
Date: Thu, 5 Jul 2012 09:01:27 +0100 (BST)
Subject: [R-sig-ME] singular convergence with lmer()
Message-ID: <1341475287.26894.YahooMailNeo@web29506.mail.ird.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120705/bd67cbeb/attachment.pl>

From john.mueller at louisville.edu  Thu Jul  5 16:17:57 2012
From: john.mueller at louisville.edu (Mueller,John Martin)
Date: Thu, 5 Jul 2012 14:17:57 +0000
Subject: [R-sig-ME] Is there an option to not include the correlation matrix
 in output of the glmmadmb function?
Message-ID: <109E4F6374337D4C9153497DE5F3EEC51CA37DE5@EXMBX07.ad.louisville.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120705/946c9a9c/attachment.pl>

From jenn.s.barrett at gmail.com  Thu Jul  5 17:44:29 2012
From: jenn.s.barrett at gmail.com (Jennifer Barrett)
Date: Thu, 5 Jul 2012 08:44:29 -0700
Subject: [R-sig-ME] Zero-inflated mixed effects model - clarification of
 zeros modeled and R package questions
In-Reply-To: <CAErODj_-9isC0nJ7OE08dGy8gF2P+bSmeBfSf+Sz_=AY384Z=w@mail.gmail.com>
References: <CAEbqvwo7a+hOLPcZ1BKUerSir+Xwcczk3AY5CeFbc80woLeM3A@mail.gmail.com>
	<CAErODj_-9isC0nJ7OE08dGy8gF2P+bSmeBfSf+Sz_=AY384Z=w@mail.gmail.com>
Message-ID: <CAEbqvwqkZLCoc9GTjuE9V250HON6B9j0G4BN1+f59KqQFu3b3w@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120705/d00c2d5d/attachment.pl>

From andyflies at gmail.com  Thu Jul  5 18:22:40 2012
From: andyflies at gmail.com (Andy Flies)
Date: Thu, 05 Jul 2012 12:22:40 -0400
Subject: [R-sig-ME] Mixed model correlation structure for unbalanced,
 longitudinal data
Message-ID: <4FF5BF50.2080801@gmail.com>

> Message: 2
> Date: Tue, 03 Jul 2012 17:47:51 -0400
> From: Andy Flies <andyflies at gmail.com  <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>
> To:r-sig-mixed-models at r-project.org  <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
> Subject: [R-sig-ME] Mixed model correlation structure for unbalanced
> 	longitudinal data
> Message-ID: <4FF36887.5050702 at gmail.com  <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>
> Content-Type: text/plain; charset=windows-1252; format=flowed
>
> Dear R users,
>
> I have data from a long-term study that has opportunistically collected
> samples over the past 10 years. My data set is highly unbalanced because
> of the opportunistic sample collection.I have a single sample from 19
> individuals, 2 samples from 4 individuals, and 3 samples from 2
> individuals.I know that lmer can accommodate unbalanced data sets, but I
> am unsure if my data set is too unbalanced.
>
> I am testing if social rank, reproductive status, and age affect my
> response variables. I also need to determine if sample collection
> parameters such as sample date and the time from anesthetizing the
> animal to the time the sample was collected affects the response variables.
> Here are what I see as potential options:
>
> 1)Use a mixed model with subject as random intercept and sample date as
> random slope to account for potential temporal autocorrelation within
> the repeat samples.
> Lmer( y ~ 1 + x1 + x2 + x3 + ? (1 + date | subject)
>
> 2)Use a mixed model with subject as random intercept. Initial data
> exploration does not show any obvious temporal autocorrelation.
> Lmer( y ~ 1 + x1 + x2 + x3 + ? (1 | subject)
>
> 3)Use a GEE and specify an autoregressive correlation structure. I think
> this would be a good option, but from what I have found in the
> literature, my sample size is too small for this.
>
> 4)Use the mean for each individual and use a standard linear model. This
> option is not good because it does not allow me to include reproductive
> status as a predictor because reproductive status changes between samples.
>
> 5)Use only a single sample from each individual in standard linear
> model. This option is not good because my already limited sample size
> would be further reduced.
>
> Please let me know which of the above options would be best or if you
> can suggest a better option. Any advice or literature references are
> sincerely appreciated.
>
> Thanks,
> Andy
>
>
>
>
> Andy...do I understand it well that you have 33 observations in total? If so...then
> I don't want to be the boogie man....but......seriously consider simplifying all these
> models. Option 4 with only 1 or 2 covariates would be my choice. Ask yourself whether it makes sense to analyze
> these data at all...perhaps making only some simple graphs?
>
>
> Alain
>
>
>
> -- 
>
> Dr. Alain F. Zuur
> First author of:
>
> 1. Analysing Ecological Data (2007).
> Zuur, AF, Ieno, EN and Smith, GM. Springer. 680 p.
> URL:www.springer.com/0-387-45967-7
>
>
> 2. Mixed effects models and extensions in ecology with R. (2009).
> Zuur, AF, Ieno, EN, Walker, N, Saveliev, AA, and Smith, GM. Springer.
> http://www.springer.com/life+sci/ecology/book/978-0-387-87457-9
>
>
> 3. A Beginner's Guide to R (2009).
> Zuur, AF, Ieno, EN, Meesters, EHWG. Springer
> http://www.springer.com/statistics/computational/book/978-0-387-93836-3
>
>
> 4. Zero Inflated Models and Generalized Linear Mixed Models with R. (2012) Zuur, Saveliev, Ieno.
> http://www.highstat.com/book4.htm
>
> Other books:http://www.highstat.com/books.htm
>
>
> Statistical consultancy, courses, data analysis and software
> Highland Statistics Ltd.
> 6 Laverock road
> UK - AB41 6FN Newburgh
> Tel: 0044 1358 788177
> Email:highstat at highstat.com  <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
> URL:www.highstat.com
> URL:www.brodgar.com



Hello Alain,

Thank you for responding to my question.

My full data set has 96 observations from 72 individuals. The subset of 
data I referred to in my initial question was a subset that included 
observations on adult females that have no missing values in any of the 
covariates. I am primarily interested in the following question: Does 
social rank affect my response variables? Social rank data are available 
for all observations, whereas some of the covariates are missing values, 
so the subset sample size increases if I drop covariates from the 
analysis. Some of the covariates, such as age and sample collection 
date, are correlated for the repeated measures, so potential 
collinearity issues might justify dropping one of the variables from the 
analysis anyway. Additionally, simple graphs of the response variables 
vs. the covariates do not suggest two-way relationships in most cases. 
Based on the techniques outlined in your data exploration methods paper, 
I agree that it would be better to keep only 1 or 2 covariates in the 
analysis and use the collapsed mean of repeated measures, rather than 
using a mixed model approach.

I initially planned to include the sample collection date as a covariate 
in the subset models to assess whether or not the sample collection date 
is important in any of the data subsets. However, there is no biological 
reason to expect the effect of sample collection date would be different 
in any of the data subsets (i.e. males, females, juveniles, adults). I 
think it would be more appropriate to assess the effect of sample 
collection date in the full data set and then not include it as a 
covariate in the subset analysis.

Thanks you,

Andy


From bbolker at gmail.com  Fri Jul  6 16:46:18 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 06 Jul 2012 10:46:18 -0400
Subject: [R-sig-ME] Is there an option to not include the correlation
 matrix in output of the glmmadmb function?
Message-ID: <4FF6FA3A.8080606@gmail.com>

  I'm a little confused.  Can you specify which version of glmmADMB
you're using (i.e. results of sessionInfo()) ?  Can you give a small
reproducible example?

  In the current version it doesn't seem that the correlation matrix of
the fixed-effect parameters is printed at all?

For example, on my system:

  data(bacteria,package="MASS")
     bacteria$present <- as.numeric(bacteria$y)-1
     (bfit <-  glmmadmb(present ~ trt + I(week > 2), random = ~ 1 | ID,
                          family = "binomial", data = bacteria))
summary(bfit)

Call:
glmmadmb(formula = present ~ trt + I(week > 2), data = bacteria,
    family = "binomial", random = ~1 | ID)


Coefficients:
                Estimate Std. Error z value Pr(>|z|)
(Intercept)        3.548      0.696    5.10  3.5e-07 ***
trtdrug           -1.367      0.677   -2.02  0.04355 *
trtdrug+          -0.783      0.683   -1.15  0.25197
I(week > 2)TRUE   -1.599      0.476   -3.36  0.00078 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Number of observations: total=220, ID=50
Random effect variance(s):
Group=ID
            Variance StdDev
(Intercept)    1.554  1.246

Log-likelihood: -96.1307


From justiceaheto at yahoo.com  Sat Jul  7 14:22:49 2012
From: justiceaheto at yahoo.com (Justice Moses K. Aheto)
Date: Sat, 7 Jul 2012 05:22:49 -0700 (PDT)
Subject: [R-sig-ME] Problem resolved
Message-ID: <1341663769.63063.YahooMailNeo@web113415.mail.gq1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120707/01ad9aa5/attachment.pl>

From bbolker at gmail.com  Sun Jul  8 01:44:24 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 07 Jul 2012 19:44:24 -0400
Subject: [R-sig-ME] Zero-inflated mixed effects model - clarification of
 zeros modeled and R package questions
Message-ID: <4FF8C9D8.60702@gmail.com>


  A couple of quick responses:

Hi folks,

  [snip]

Thanks for replying so quickly Alain ? it?s much appreciated. To follow-up
on your comments:

-          Re: Spatial Autocorrelation  - I have dealt with spatial
autocorrelation in the past, though with continuous log-normal data (no
random effects - hence I used a spatial autoregressive model). I have
mentioned the likelihood of spatial autocorrelation in the residuals to my
employer/supervisor; however, he has advised that we proceed with the model
without accounting for autocorrelation, expecting that a large part it may
be explained by the environmental variables (which are no doubt clustered)
once the model is fitted. I?m skeptical, as some of these species also
might seek ?safety in numbers? selecting sites based on the abundance of
conspecifics nearby, and large flocks at a given site are likely to utilize
habitat at neighboring sites as well (if suitable). We shall see!

BMB>  You can always do a post-fitting test, graphical or statistical, for
the presence of spatial autocorrelation -- if you don't see anything
(clustering of residuals in a spatial plot of residuals, significant
Moran's I, or interesting-looking spatial variogram/correlogram)
then you should be OK ...

-          Re: random effect in the binomial process of a ZIP - don?t I
have to include this, given the repeated measures?

BMB>  It depends.  In principle, there could be a random effect in
the binomial process of the ZIP.  In practice, at some point the
model becomes too computationally unwieldy/unstable, due to complexity
and possible overfitting.  Again, you can take the general strategy
of leaving out potentially difficult model complications, then see if
you can detect them in the residuals (in this case, differences in
deviation between predicted vs actual zeros in different groups)

Thanks to everyone else as well for your input. After reading your
responses, and diving into the lit a little more, you've convinced me that
MCMC is the way to go. However, I now have a few more quick (hopefully?)
questions:
- Because I'm a tad afraid of WinBugs, I decided to look at MCMCglmm as
well. I noticed that the course notes for MCMCglmm state that ?*As is often
the case the parameters of the zero-inflation model mixes poorly? Poor
mixing is often associated with distributions that may not be zero-inflated
but instead over-dispersed.*?  Am I correct in thus assuming that if the
data are indeed zero-inflated, ?poor mixing? is not a problem? Or might
this also arise through other means?

BMB>  Poor mixing can happen any time you have a complex model.
Check the trace plots.

- Is there an advantage to using MCMCglmm versus winBUGS or vice versa? It
seems either one will take some time to correctly code/specify, so I might
as well go the route that makes the most sense/is more highly recommended.

BMB> WinBUGS is more flexible, MCMCglmm is (much) faster and easier
for those problems which it can handle.  If you don't see yourself
needing to go beyond the problems that MCMCglmm can handle, I would
stick with it.

- And most importantly: As I mentioned in my original message, we had
wanted to compare competing hypotheses for what shoreline attributes
influence shorebird distributions, and to then use MMI in prediction;
however, I?ve read that DIC is not recommended for mixed effects models
(even though MuMIn accepts MCMCglmm output). According to a post by Jarrod
Hadfield, this is especially true for non-Gaussian data because the level
of focus is on the sampled observations (i.e., for ?*observations (y) on
children within schools...DIC would be focused at "can we predict how many
times *these* children miss the bus*"*)*. What are my options then for
model comparison/selection and prediction? Recall that we want to estimate
the total abundance of each shorebird species within the entire study
region (with confidence intervals). I'm really stuck here...

BMB> DIC is indeed problematic for several reasons: there's the
level-of-focus problem, and the problem that its derivation assumes
multivariate normal posterior distributions ...  You could try to count
parameters in a naive way (i.e. one parameter per variance or
covariance parameter, which is probably the right way to do it
for the "population" level of focus -- see Vaida and Blanchard 2005),
and use AIC based on the mean deviance as suggested by
Brooks, S.  2002.  Discussion of the paper by Spiegelhalter, Best,
Carlin, and van der Linde.  Journal of the Royal Statistical Society
B.  64: 616-618.

  I would also say that you could just hope that one model
stands out so that you don't have to use MMI ...

  Ben Bolker

Thanks in advance... this is a huge statistical leap for me.

Cheers,
Jenn

On Thu, Jun 21, 2012 at 8:43 PM, Paul Johnson <pauljohn32 at gmail.com> wrote:

> Dear Jennifer:
> Response below
>
> On Wed, Jun 20, 2012 at 5:32 PM, Jennifer Barrett
> <jenn.s.barrett at gmail.com> wrote:
> > Hi folks,
> >
> >
> > I?m looking for some guidance in regards to zero-inflated models with
> > repeated measures (i.e., random effect for site). My first question is
> more
> > of a statistical one, while the second is related to R packages.
> Apologies
> > for the long post; however, I want to make sure my concerns/questions are
> > clear!
> >
> >
> > Our project and dataset:
> >
> >
> > - The aim of our project is to 1) examine associations between shoreline
> > habitat characteristics and the abundance of several shorebird species;
> and
> > 2) estimate the total abundance of each shorebird species within the
> entire
> > study region based on the models from 1) above, with confidence
> intervals.
> > Note that we will be using an information theoretic approach for 1)
> above,
> > and would like to use MMI for 2).
> >
> > - Our response dataset consists of counts of shorebirds at >150 coastal
> > sites, conducted on the second Sunday of each month between the months of
> > Oct-March, over 10 years; however, not every site was surveyed in all
> > months (we?ve limited our dataset to those with a minimum of 3 counts in
> a
> > year).  Our response variable is thus the number of birds counted in a
> > given month/year at a given site. Note that we plan to model each year
> > separately.
> >
> > -  The habitat dataset consists of shoreline units within our entire
> study
> > region, with each unit characterized by exposure, substrate type...etc.
> > Using GIS, we?ve measured the length of shoreline belonging to shoreline
> > categories (e.g., sand, rock, mud) within each survey site, the average
> > exposure for the site, and other continuous attributes, as well as one
> > presence/absence covariate.
> >
> > - Initial exploratory analysis has shown that the counts are
> zero-inflated.
> > While there may be some false zeros in our dataset (i.e., observer
> error),
> > the source of the zero-inflation is likely preference of shorebirds for
> > particular sites with particular features and avoidance of others (i.e.,
> > true zeros or ?structural zeros?). Some zeros likely also arise because
> the
> > species does not saturate its habitat (i.e., habitat suitable, but
> > unoccupied ? also a ?true? zero), though again, the majority of the zeros
> > are likely structural.
> >
> >
> > Onto my questions:
> >
> >
> > 1) I?ve been reading through the literature to decide what type of model
> > would best be suited for our dataset and questions. While all articles
> seem
> > to agree that the choice of a model needs to consider the source of
> excess
> > zeros, they seem to contradict one another in regards to what zeros are
> > being modeled in each component of a zero-inflated mixture model. Note
> that
> > I am not considering a two-part (i.e., conditional) model, because I do
> not
> > believe that all zeros arise from the occupancy process (as per Joseph et
> > al. 2009 and as noted above, zero abundance can occur by chance in our
> > system). Examples:
> >
> >
> > - Martin et al. (2005) state that when zero inflation is due to true
> zeros,
> > two-part or mixture models (ZIP or ZINB) are recommended, and that when
> > zero inflation is due to false zeros, a ZIB mixture model is recommended;
> > however, when zero inflation is due to both excess true and false zeros,
> a
> > Bayesian framework may be used, though there is no formal discussion in
> the
> > literature. NOTE: Since this article was published, Royle?s N-mixture
> model
> > has addressed this issue; however, I cannot use this approach as my data
> do
> > not meet the assumption of a closed population during the study period.
> >
> > - In contrast to Martin et al. (2005), Potts and Elith (2006) state that
> > the zero-inflated mixture model structure implies that zero observations
> > arising from the zero process are true negative observations, and that
> > those arising from the Poisson process are false negative observations
> ?that
> > is, the habitat is suitable, but unoccupied? (p.155). However, on the
> > previous page, they defined false negative as ?attributable to
> experimental
> > design? or observer error?, and habitat that is ?suitable, but
> unoccupied?
> > as a true negative, so I'm not sure which type of zero observation they
> are
> > really referring to here for the Poisson process.
> >
> > - In contrast to both sources above, Zuur et al. (2009) state that in a
> ZIP
> > or ZINB, zeros are modeled as coming from two processes ? the binomial
> > process, which models only false zeros (observer, design, and survey
> error)
> > and the Poisson (or Negbin) process  which models the true zeros and
> > counts. This is the opposite of what was stated by Potts and Elith.
> >
> > - Finally, I?ve read other sources which state that ZIPs simply treat the
> > population as a mixture, with one set of subjects having a zero response
> ?
> > in other words, there is no mention of whether the zero process is
> modeling
> > the ?true? or ?false? zeros.
> >
> >
> > Thinking about my system: there are a bunch of sites where the birds (of
> a
> > given species) never go (habitat is unsuitable), and a bunch where they
> do
> > go with varying levels of abundance (habitat is suitable, but come sites
> > are more favored than others, based on habitat features). Following the
> > last bullet above, a site that is suitable may have a count of zero
> simply
> > because the species wasn?t present there on the survey day (i.e., true
> zero
> > occurring by chance). Given the contradicting information above, and the
> > consensus on the importance of considering the source of zeros in model
> > selection, I would very much appreciate if someone could clear this up
> for
> > me - or let me know if I'm completely missing something here? Perhaps
> this
> > question should be posed on a stats forum, but given question 2 below, I
> > thought I'd try here first.
> >
> >
> > 2) Assuming that I?m on the right track with a ZIP, is there a package I
> > can use to model a ZIP with a random effect for site? I looked at
> glmmADMB;
> > however, the zero inflation can only be modeled as a constant. This
> doesn?t
> > make sense for my system, as the zero-inflation will be a function of
> > habitat covariates (see above). Likewise, glmmPQL is not an option, as
> this
> > method does not yield log-likelihoods (and thus no AIC). I?m also
> thinking
> > that the random effect will have to be included in the zero process as
> well
> > ? is this right?
> >
> Some of your jargon is unfamiliar to me--"true" and "false" zeros. I
> suppose a false zero would be the result of a "hurdle process" (as in
> the pscl package).  I've not seen a hurdle model joined in the same
> with a zero-inflation model.  Certainly not with "random effects"
> apart from the inflated zeros.
>
> Although I do not believe there is an ML solution for your problem
> within easy reach. However, there are Bayesian answers. Please see the
> package MCMCglmm.  It has a very well done pair of vignettes.
>
> MCMCglmm has a ZIP family option, and you can add random effects.
> Jarod Hadfield has been a regular contributor here and I think if you
> post your working example code he and others will be glad to help out.
>
> pj
>
>
>
> --
> Paul E. Johnson
> Professor, Political Science    Assoc. Director
> 1541 Lilac Lane, Room 504     Center for Research Methods
> University of Kansas               University of Kansas
> http://pj.freefaculty.org            http://quant.ku.edu
>



-- 
Jennifer Barrett, BSc., MRM
Research Associate
Centre for Wildlife Ecology
Simon Fraser University

	[[alternative HTML version deleted]]


From j.hadfield at ed.ac.uk  Sun Jul  8 12:20:18 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Sun, 08 Jul 2012 11:20:18 +0100
Subject: [R-sig-ME] MCMCglmm: priors for ordinal regression
In-Reply-To: <1080250.1669651341413298860.JavaMail.defaultUser@defaultHost>
References: <1080250.1669651341413298860.JavaMail.defaultUser@defaultHost>
Message-ID: <20120708112018.91972h8hinh39p6o@www.staffmail.ed.ac.uk>

Dear Massimo,

Do you mean the chain did not converge or the chain did not mix?  
Generally the former is rare, and is usually only seen with  
ordinal/categorical data with complete (or near complete) separation.   
Sometimes a prior that constrains the linear predictor away from  
extreme values on the logit/probit scale can fix this with a  
relatively minor prior influence on inferences made on the data scale.  
Sometimes not. Its not clear to me what the motivation is behind your  
prior - is it that the sum of your variance components is close to  
100? If so I would be careful. Use pl=TRUE in your call to MCMCglmm  
and make sure your latent variables are in the range -7 to 7.

Cheers,

Jarrod





Quoting "m.fenati at libero.it" <m.fenati at libero.it> on Wed, 4 Jul 2012  
16:48:18 +0200 (CEST):

>
> Dear R user,
> I have some problems about prior definition in MCMCglmm ordinal  
> regression. I've tried to use what Jarrod wrote about not  
> informative priors for ordinal probit but my model did not converge:
>
>
> prior=list(R=list(V= 1, fix=1), G=list(G1=list(V=1, nu=0)))
>
>
> where "..left the default prior for the fixed effects (not  
> explicitly specified)..".
>
>
> Then, in order to have however a similar uniform distribution for  
> the latent variable, I set prior for fixed effect  as "mu=0" and  
> "(co)variance=100":
>
>
> priorB<-rnorm(1000, 0, sqrt(100))
> priorMB<-1:1000
> for(i in 1:1000){
>   priorMB[i]<-mean(pnorm(priorB[i]+rnorm(1000,0,sqrt(100))))
>    }
> hist(priorMB)
>
>
> The model converge well but I've some dobts. Is it correct or not?
>
>
> Thank you very much for any suggestions or comments.
>
>
> Best regards
>
>
> Massimo
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From j.hadfield at ed.ac.uk  Sun Jul  8 13:03:37 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Sun, 08 Jul 2012 12:03:37 +0100
Subject: [R-sig-ME] A zero inflated Poisson model in MCMCglmm
In-Reply-To: <4FF46034.8070203@qmul.ac.uk>
References: <4FF46034.8070203@qmul.ac.uk>
Message-ID: <20120708120337.32586osul89lwkkk@www.staffmail.ed.ac.uk>

Hi,

I agree with David that an ordinal model may behave better for this  
type of data (or even a 0/1 response with deformed or not) but in case  
you want to pursue a ZIP model...

The model you have specified is probably not what you had in mind. You  
should think of the ZIP response as being two responses (traits): the  
Poisson counts (trait 1) and the probability that a zero comes from  
the zero-inflation process (trait 2). At the moment you have a common  
intercept for both and therefore assume that the probability that a  
zero is from the zero-inflation process (on the logit scale) is equal  
to the mean Poisson count (on the log scale). There is probably little  
justification for this (but see ZAP models). Moreover, you have a  
single animal term, and therefore assume that both processes have the  
same genetic variance and that the genetic correlation between them is  
1. You also assume that the residual variance for each process is  
equal (but the residual correlation is 0), although this is not  
actually that bigger deal because residual variation in the  
zero-inflation process (and residual corrletaion) is not identifiable  
from the data and so the value it could take is essentially arbitrary.  
However, I like to recognise this non-identifiability by fixing the  
variance at 1 and the covariance at zero (but see ZAP models where  
your specification is useful).

So, a more appropriate model would be:

model1.1<-MCMCglmm(Toes~trait,random=~us(trait):animal,family="zipoisson",rcov=~idh(trait):units,pedigree=ped,data=data,prior=prior1.1,nitt=500000,thin=500,burnin=200000,verbose=FALSE)

or perhaps

model1.2<-MCMCglmm(Toes~trait,random=~idh(trait):animal,family="zipoisson",rcov=~idh(trait):units,pedigree=ped,data=data,prior=prior1.1,nitt=500000,thin=500,burnin=200000,verbose=FALSE)

The difference between these two is that the genetic correlation  
between the zero-inflation and the Poisson processes is estimated, and  
in the latter it is set to 0.

I'm not sure about how much data you have, but I would think that if  
the incidence of deformed toes is very low, you probably have little  
information for some of the parameters. In this case you have to be  
VERY careful with priors. The prior recommendations in the MCMCglmm  
tutorial in Wilson et al. 2011 will generally be quite informative for  
small or even moderate sized datasets (the authors have kindly  
published an erratum as many people seem to be using their  
recommendation without realising its implications).

The prior could look like:

prior=list(R=list(V=diag(2), nu=0, fix=2), G=list(G1=G1))

which fixes the residual variance in the zero-inflation process to 1  
with a flat improper prior on the Poisson over-dispersion. For the  
prior on the genetic (co)variances (G1) I generally use parameter  
expanded priors of the form:

G1=list(V=diag(2), nu=2, alpha.mu=c(0,0), alpha.V=diag(2)*1000)

or

G1=list(V=diag(2), nu=1, alpha.mu=c(0,0), alpha.V=diag(2)*1000)

for model.2.

  These generally give posterior modes for the variance components  
that are close to (RE)ML estimates, although it is unclear to me how  
they behave for the covariances and functions of variances (e.g.  
heritability). At least for binary traits (e.g. the zero-inflation  
bit) I have seen a chi-square prior recommended because of its  prior  
properties on the heritability (de Villemereuil in MEE). This type of  
prior can also be obtained using parameter expansion:

(V=1, nu=1000, alpha.mu=0, alpha.V=1)

and I would guess that the multivariate analogue would be (V=diag(2),  
nu=1001, alpha.mu=c(0,0), alpha.V=diag(2)). However, a chi-square  
prior probably has poor properties for the Poisson variance and in  
MCMCglmm you could only mix these different priors for model.2 using a  
different syntax:

model1.2<-MCMCglmm(Toes~trait,random=~idh(at(trait,1)):animal+idh(at(trait,2)):animal,family="zipoisson",rcov=~idh(trait):units,pedigree=ped,data=data,prior=prior1.1,nitt=500000,thin=500,burnin=200000,verbose=FALSE)

G=list(
     G1=list(V=1, nu=1, alpha.mu=0, alpha.V=1000),
     G2=list(V=1, nu=1000, alpha.mu=0, alpha.V=1)
   )


Cheers,

Jarrod




Quoting Helen Ward <h.l.ward at qmul.ac.uk> on Wed, 04 Jul 2012 16:24:36 +0100:

> Dear list,
>
> I work on bats, some of which have deformed toes. I have just started
> trying to use MCMCglmm to investigate whether there is hertiable
> variation for the number of deformed toes a bat has. Since most bats
> have 0 deformed toes I am using a zero-inflated Poisson model.
>
> I have put together some code (see below) based on the MCMCglmm tutorial
> from the Wilson et al. 2011 'Ecologist's guide to the animal model'
> paper, the MCMCglmm Course Notes and trial and error. This code runs and
> I get a model and a heritability estimate. However, the heritability
> estimate I get is extremely high and this, teamed with the observations
> that my posterior distributions of the variance componants are not
> pretty as the example ones I've seen and my autocorrelation values are
> high (~0.4), makes me suspicious that I am not modelling this trait very
> well. I suspect I have over simplified the model. I based my prior on a
> model done in ASReml modelling 'Toes' as a Poisson distribution, which
> can an heritability estimate of 0.35.
>
> In addition, when I try and add a second random variable - year of birth
> (Born) - into a second model, both plots and auto correlation get worse
> and my heritability estimate for number of deformed toes (Toes) goes
> from about 0.8 to 0.05!
>
> Finally, I suspect you're not, but I would like to know if you are
> allowed to compare DIC values from models with different priors please.
>
> I have pasted my code below. If anyone can offer constructive criticism
> it would be much appreciated.
>
> Many thanks,
> Helen
>
> Model 1: With number of deformed toes (Toes) as the only random variable
>
>
> p.var<-var(data$Toes,na.rm=TRUE)
> prior1.1<-list(G=list(G1=list(V=matrix(p.var*0.35),n=1)),
> R=list(V=matrix(p.var*0.65),n=1))
>
> model1.1<-MCMCglmm(Toes~1,random=~animal,family="zipoisson",rcov=~trait:units,pedigree=ped,data=data,prior=prior1.1,nitt=500000,thin=500,burnin=200000,verbose=FALSE)
> posterior.heritability1.1<-model1.1$VCV[,"animal"]/(model1.1$VCV[,"animal"]+model1.1$VCV[,"trait:units"])
>
> HPDinterval(posterior.heritability1.1,0.95)
> posterior.mode(posterior.heritability1.1)
>
>
> Model 2: With number of toes (Toes) and year of birth (Born) as random
> variables
>
>
> p.var<-var(data$Toes,na.rm=TRUE)
> prior1.2<-list(G=list(G1=list(V=matrix(p.var/3),n=1),G2=list(V=matrix(p.var/3),n=1)),R=list(V=matrix(p.var/3),n=1))
>
> model1.2<-
> MCMCglmm(Toes~1,random=~animal+Born,family="zipoisson",rcov=~trait:units,pedigree=ped,data=data,prior=prior1.2,nitt=1000000,thin=1000,burnin=500000,verbose=FALSE)
> posterior.heritability1.2<-model1.2$VCV[,"animal"]/(model1.2$VCV[,"animal"]+model1.2$VCV[,"Born"]+model1.2$VCV[,"trait:units"])
>
> HPDinterval(posterior.heritability1.2,0.95)
> posterior.mode(posterior.heritability1.2)
>
>
>
>
>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From bbolker at gmail.com  Sun Jul  8 21:58:30 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 8 Jul 2012 19:58:30 +0000 (UTC)
Subject: [R-sig-ME] singular convergence with lmer()
References: <1341475287.26894.YahooMailNeo@web29506.mail.ird.yahoo.com>
Message-ID: <loom.20120708T215404-691@post.gmane.org>

laurent stephane <laurent_step at ...> writes:

> 
> Dear all,
> 
> Using the latest CRAN version of lme4 
>  I get the following warning from lmer() :
> 
> Warning message: 
> In mer_finalize(ans) : singular convergence (7)
 
> My model is not complicated and it works fine with SAS (if you are
> interested in the details of my model see
> forums.cirad.fr/logiciel-R/viewtopic.php?t=5071 )
 
> What argument could I change in lmer() to overcome this warning ?
> 

  This warning emerges from the nlminb optimizer used in the guts
of lme4, and I don't think there's much you can do to suppress it
or change the behavior of nlminb to avoid it.  The best you could
do would be to use other packages (SAS, other versions of lme4 or
nlme, etc.) to see if the correct answer was achieved despite the
warning.

  Ben Bolker


From reinhold.kliegl at gmail.com  Sun Jul  8 22:21:54 2012
From: reinhold.kliegl at gmail.com (Reinhold Kliegl)
Date: Sun, 8 Jul 2012 22:21:54 +0200
Subject: [R-sig-ME] singular convergence with lmer()
In-Reply-To: <loom.20120708T215404-691@post.gmane.org>
References: <1341475287.26894.YahooMailNeo@web29506.mail.ird.yahoo.com>
	<loom.20120708T215404-691@post.gmane.org>
Message-ID: <CAG+WrEwXRGywKp0hauQt6BT98tJ1b8AF+3755ZmH3WX7AFRDTg@mail.gmail.com>

It converged for me for lme4_0.999999-0.
Estimates look different from what you posted at the site.
Reinhold Kliegl

> dat$Part <- factor(dat$Part)
> ( fit <- lmer(y ~ (1|Operator)+(1|Part)+(1|Part:Operator), data=dat) )
Linear mixed model fit by REML
Formula: y ~ (1 | Operator) + (1 | Part) + (1 | Part:Operator)
   Data: dat
    AIC    BIC logLik deviance REMLdev
 -619.7 -603.4  314.9   -630.3  -629.7
Random effects:
 Groups        Name        Variance   Std.Dev.
 Part:Operator (Intercept) 0.00081854 0.028610
 Part          (Intercept) 1.06721729 1.033062
 Operator      (Intercept) 0.00031226 0.017671
 Residual                  0.00063295 0.025159
Number of obs: 192, groups: Part:Operator, 96; Part, 12; Operator, 8

Fixed effects:
            Estimate Std. Error t value
(Intercept)   2.7171     0.2983   9.109


On Sun, Jul 8, 2012 at 9:58 PM, Ben Bolker <bbolker at gmail.com> wrote:
> laurent stephane <laurent_step at ...> writes:
>
>>
>> Dear all,
>>
>> Using the latest CRAN version of lme4
>>  I get the following warning from lmer() :
>>
>> Warning message:
>> In mer_finalize(ans) : singular convergence (7)
>
>> My model is not complicated and it works fine with SAS (if you are
>> interested in the details of my model see
>> forums.cirad.fr/logiciel-R/viewtopic.php?t=5071 )
>
>> What argument could I change in lmer() to overcome this warning ?
>>
>
>   This warning emerges from the nlminb optimizer used in the guts
> of lme4, and I don't think there's much you can do to suppress it
> or change the behavior of nlminb to avoid it.  The best you could
> do would be to use other packages (SAS, other versions of lme4 or
> nlme, etc.) to see if the correct answer was achieved despite the
> warning.
>
>   Ben Bolker
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From jwiley.psych at gmail.com  Sun Jul  8 22:52:18 2012
From: jwiley.psych at gmail.com (Joshua Wiley)
Date: Sun, 8 Jul 2012 13:52:18 -0700
Subject: [R-sig-ME] singular convergence with lmer()
In-Reply-To: <1341475287.26894.YahooMailNeo@web29506.mail.ird.yahoo.com>
References: <1341475287.26894.YahooMailNeo@web29506.mail.ird.yahoo.com>
Message-ID: <CANz9Z_+LZqawXOg_CDJQEZ079Ja=z_2WP5h2HtpzTFPG_pp6UA@mail.gmail.com>

Notice that the variance of one of your random effects is estimated at
0.  I suspect that this is the source of the singular convergence.
IIRC proc mixed (which is what I assume you are using in SAS) uses a
somewhat different approach to to estimate the random effects than
does lme4.

Although it seems to work for Reinhold, again some of the variances
are vanishingly small, which seems to me like it may suggest some of
the effects are borderline on 0 and perhaps slightly different
estimation methods either get "really small" or simply "0" and if 0,
you get a warning.  I would also consider simplifying your model
(although likelihood ratio tests seem to suggest a significant
decrement in the likelihood fixing the variance at 0).

Cheers,

Josh

On Thu, Jul 5, 2012 at 1:01 AM, laurent stephane <laurent_step at yahoo.fr> wrote:
> Dear all,
>
> Using the latest CRAN version of lme4 I get the following warning from lmer() :
>
> Warning message:
> In mer_finalize(ans) : singular convergence (7)
>
> My model is not complicated and it works fine with SAS (if you are interested in the details of my model see forums.cirad.fr/logiciel-R/viewtopic.php?t=5071 )
>
> What argument could I change in lmer() to overcome this warning ?
>
> Kind regards,
> SL
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



-- 
Joshua Wiley
Ph.D. Student, Health Psychology
Programmer Analyst II, Statistical Consulting Group
University of California, Los Angeles
https://joshuawiley.com/


From kharouba at zoology.ubc.ca  Sun Jul  8 22:52:25 2012
From: kharouba at zoology.ubc.ca (Heather Kharouba)
Date: Sun, 8 Jul 2012 13:52:25 -0700
Subject: [R-sig-ME] Dealing with overdispersion with glmmadmb beta
	distribution
Message-ID: <f05cffcf74186100fead7e9a3037f2c9.squirrel@webmail.zoology.ubc.ca>

Dear R users,

In the analysis I'm doing, I'm interested in testing the importance of a
factor (with 5 levels). The response variable varies continuously from 0
to 1 so I've used a beta distribution with glmmadmb:

m1<-glmmadmb(AUC~variables+log_area+model+taxa+(1|study), family="Beta",
verbose=TRUE, debug=TRUE, data=mat);

Call:
glmmadmb(formula = AUC2 ~ variables + log_area + model2 + taxa +
    (1 | study), data = mat, family = "Beta", verbose = TRUE,
    debug = TRUE)


Coefficients:
                              Estimate Std. Error z value Pr(>|z|)
(Intercept)                    -1.1568     1.3899   -0.83   0.4052
variables                       0.0248     0.0112    2.22   0.0264 *
log_area                        0.1976     0.0859    2.30   0.0215 *
model2autologistic regression   2.2526     0.7345    3.07   0.0022 **
model2domain                   -0.3246     0.5753   -0.56   0.5726
model2GAM                       0.1757     0.4768    0.37   0.7125
model2GARP                     -0.2985     0.5626   -0.53   0.5957
model2gdm                      -0.5436     0.5691   -0.96   0.3395
model2GLM                      -0.3243     0.5860   -0.55   0.5800
model2localwghtregression       0.6689     0.4780    1.40   0.1617
model2logregression            -0.5179     0.6661   -0.78   0.4369
model2maxent                    0.0182     0.5353    0.03   0.9728
model2other                    -0.4466     0.5114   -0.87   0.3825
taxaHER                         0.3200     0.1024    3.12   0.0018 **
taxaINV                         0.6967     0.1591    4.38  1.2e-05 ***
taxaMAM                        -0.1792     0.0859   -2.09   0.0368 *
taxaP                           0.1230     0.0616    2.00   0.0459 *
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Number of observations: total=4317, study=20
Random effect variance(s):
Group=study
            Variance StdDev
(Intercept)    0.328 0.5727
Beta dispersion parameter: 14.503 (std. err.: 0.35671)

Log-likelihood: 9167.05

The dispersion parameter is 14.503 suggesting the model is overdispersed.
A recent suggestion from http://glmm.wikidot.com/faq and
r-sig-mixed-models archives to account for overdispersion is to include
individual-level random effects. However, if I include this additional
random effect, I get the following error:

Error in glmmadmb(AUC2 ~ variables + log_area + model2 + taxa + (1 |
study) +  :
  The function maximizer failed (couldn't find STD file)

suggesting that the model cannot be estimated. I'm wondering whether there
are other methods to reduce overdispersion I might have overlooked that
could affect the variance-mean relationship and that would be appropriate
for this type of response variable.

A snapshot of the data:

study                  taxa       AUC      model variables log_area Araujo
et al. 2005 BIRD 0.9156878   GAM         7 16.21771
Araujo et al. 2005 BIRD 0.9288596   GAM         7 16.21771
Araujo et al. 2005 BIRD 0.9254065   GAM         7 16.21771
Araujo et al. 2005 BIRD 0.8825593   GAM         7 16.21771
Araujo et al. 2005 BIRD 0.9388894   GAM         7 16.21771
Araujo et al. 2005 BIRD 0.9061483   GAM         7 16.21771

Thanks in advance!
Heather Kharouba


From jbaldwin at fs.fed.us  Sun Jul  8 23:58:14 2012
From: jbaldwin at fs.fed.us (Baldwin, Jim -FS)
Date: Sun, 8 Jul 2012 21:58:14 +0000
Subject: [R-sig-ME] singular convergence with lmer()
In-Reply-To: <CANz9Z_+LZqawXOg_CDJQEZ079Ja=z_2WP5h2HtpzTFPG_pp6UA@mail.gmail.com>
References: <1341475287.26894.YahooMailNeo@web29506.mail.ird.yahoo.com>
	<CANz9Z_+LZqawXOg_CDJQEZ079Ja=z_2WP5h2HtpzTFPG_pp6UA@mail.gmail.com>
Message-ID: <DDC5EC9B78340042B0D5A0C3789D45690EC9C067@001FSN2MPN1-061.001f.mgd2.msft.net>

I wonder if it is a version issue.  Using the data at forums.cirad.fr/logiciel-R/viewtopic.php?t=5071 I get the following (which matches what SAS produces):

> str(dat)
'data.frame':   192 obs. of  3 variables:
 $ Operator: Factor w/ 8 levels "A","B","C","D",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ Part    : Factor w/ 12 levels "1","2","3","4",..: 1 1 2 2 3 3 4 4 5 5 ...
 $ y       : num  0.724 0.699 1.554 1.535 1.786 ...
> fit
Linear mixed model fit by REML
Formula: y ~ (1 | Operator) + (1 | Part) + (1 | Part:Operator)
   Data: dat
    AIC    BIC logLik deviance REMLdev
 -619.7 -603.4  314.9   -630.3  -629.7
Random effects:
 Groups        Name        Variance   Std.Dev.
 Part:Operator (Intercept) 0.00081854 0.028610
 Part          (Intercept) 1.06721993 1.033063
 Operator      (Intercept) 0.00031226 0.017671
 Residual                  0.00063295 0.025159
Number of obs: 192, groups: Part:Operator, 96; Part, 12; Operator, 8

Fixed effects:
            Estimate Std. Error t value
(Intercept)   2.7171     0.2983   9.109


I'm using R 1.15.0 32-bit on Windows XP and Package lme4 version 0.999375-42.

Jim Baldwin
Pacific Southwest Research Station
USDA Forest Service
Albany, California

-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Joshua Wiley
Sent: Sunday, July 08, 2012 1:52 PM
To: laurent stephane
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] singular convergence with lmer()

Notice that the variance of one of your random effects is estimated at 0.  I suspect that this is the source of the singular convergence.
IIRC proc mixed (which is what I assume you are using in SAS) uses a somewhat different approach to to estimate the random effects than does lme4.

Although it seems to work for Reinhold, again some of the variances are vanishingly small, which seems to me like it may suggest some of the effects are borderline on 0 and perhaps slightly different estimation methods either get "really small" or simply "0" and if 0, you get a warning.  I would also consider simplifying your model (although likelihood ratio tests seem to suggest a significant decrement in the likelihood fixing the variance at 0).

Cheers,

Josh

On Thu, Jul 5, 2012 at 1:01 AM, laurent stephane <laurent_step at yahoo.fr> wrote:
> Dear all,
>
> Using the latest CRAN version of lme4 I get the following warning from lmer() :
>
> Warning message:
> In mer_finalize(ans) : singular convergence (7)
>
> My model is not complicated and it works fine with SAS (if you are
> interested in the details of my model see
> forums.cirad.fr/logiciel-R/viewtopic.php?t=5071 )
>
> What argument could I change in lmer() to overcome this warning ?
>
> Kind regards,
> SL
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



--
Joshua Wiley
Ph.D. Student, Health Psychology
Programmer Analyst II, Statistical Consulting Group University of California, Los Angeles https://joshuawiley.com/

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models





This electronic message contains information generated by the USDA solely for the intended recipients. Any unauthorized interception of this message or the use or disclosure of the information it contains may violate the law and subject the violator to civil or criminal penalties. If you believe you have received this message in error, please notify the sender and delete the email immediately.


From David.Duffy at qimr.edu.au  Mon Jul  9 00:53:26 2012
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Mon, 9 Jul 2012 08:53:26 +1000
Subject: [R-sig-ME] singular convergence with lmer()
In-Reply-To: <CAG+WrEwXRGywKp0hauQt6BT98tJ1b8AF+3755ZmH3WX7AFRDTg@mail.gmail.com>
References: <1341475287.26894.YahooMailNeo@web29506.mail.ird.yahoo.com><loom.20120708T215404-691@post.gmane.org>
	<CAG+WrEwXRGywKp0hauQt6BT98tJ1b8AF+3755ZmH3WX7AFRDTg@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.1207090848410.28378@orpheus.qimr.edu.au>

On Sun, 8 Jul 2012, Reinhold Kliegl wrote:

> It converged for me for lme4_0.999999-0.
> Estimates look different from what you posted at the site.
> Reinhold Kliegl
>
>> dat$Part <- factor(dat$Part)
>> ( fit <- lmer(y ~ (1|Operator)+(1|Part)+(1|Part:Operator), data=dat) )
> Linear mixed model fit by REML
> Formula: y ~ (1 | Operator) + (1 | Part) + (1 | Part:Operator)
>   Data: dat
>    AIC    BIC logLik deviance REMLdev
> -619.7 -603.4  314.9   -630.3  -629.7
> Random effects:
> Groups        Name        Variance   Std.Dev.
> Part:Operator (Intercept) 0.00081854 0.028610
> Part          (Intercept) 1.06721729 1.033062
> Operator      (Intercept) 0.00031226 0.017671
> Residual                  0.00063295 0.025159
> Number of obs: 192, groups: Part:Operator, 96; Part 12; Operator, 8
>
> Fixed effects:
>            Estimate Std. Error t value
> (Intercept)   2.7171     0.2983   9.109
>

The regress package (1.3-8) also gives

  r1 <- regress(y ~ 1, ~ Operator+Part+I(Operator:Part), data=dat)
  r1$sigma
         Operator             Part I(Operator:Part)               In
     0.0003122871     1.0671419239     0.0008185268     0.0006329560


From djmuser at gmail.com  Mon Jul  9 01:09:22 2012
From: djmuser at gmail.com (Dennis Murphy)
Date: Sun, 8 Jul 2012 16:09:22 -0700
Subject: [R-sig-ME] singular convergence with lmer()
In-Reply-To: <DDC5EC9B78340042B0D5A0C3789D45690EC9C067@001FSN2MPN1-061.001f.mgd2.msft.net>
References: <1341475287.26894.YahooMailNeo@web29506.mail.ird.yahoo.com>
	<CANz9Z_+LZqawXOg_CDJQEZ079Ja=z_2WP5h2HtpzTFPG_pp6UA@mail.gmail.com>
	<DDC5EC9B78340042B0D5A0C3789D45690EC9C067@001FSN2MPN1-061.001f.mgd2.msft.net>
Message-ID: <CADv2QyF0PHAWJp=hAWCojB6cybj6tfPzMJ=mSxi=eQ9se-NSiw@mail.gmail.com>

FWIW, I get the same result using
packageVersion('lme4')
[1] ?0.999999.0?

Dennis

On Sun, Jul 8, 2012 at 2:58 PM, Baldwin, Jim -FS <jbaldwin at fs.fed.us> wrote:
> I wonder if it is a version issue.  Using the data at forums.cirad.fr/logiciel-R/viewtopic.php?t=5071 I get the following (which matches what SAS produces):
>
>> str(dat)
> 'data.frame':   192 obs. of  3 variables:
>  $ Operator: Factor w/ 8 levels "A","B","C","D",..: 1 1 1 1 1 1 1 1 1 1 ...
>  $ Part    : Factor w/ 12 levels "1","2","3","4",..: 1 1 2 2 3 3 4 4 5 5 ...
>  $ y       : num  0.724 0.699 1.554 1.535 1.786 ...
>> fit
> Linear mixed model fit by REML
> Formula: y ~ (1 | Operator) + (1 | Part) + (1 | Part:Operator)
>    Data: dat
>     AIC    BIC logLik deviance REMLdev
>  -619.7 -603.4  314.9   -630.3  -629.7
> Random effects:
>  Groups        Name        Variance   Std.Dev.
>  Part:Operator (Intercept) 0.00081854 0.028610
>  Part          (Intercept) 1.06721993 1.033063
>  Operator      (Intercept) 0.00031226 0.017671
>  Residual                  0.00063295 0.025159
> Number of obs: 192, groups: Part:Operator, 96; Part, 12; Operator, 8
>
> Fixed effects:
>             Estimate Std. Error t value
> (Intercept)   2.7171     0.2983   9.109
>
>
> I'm using R 1.15.0 32-bit on Windows XP and Package lme4 version 0.999375-42.
>
> Jim Baldwin
> Pacific Southwest Research Station
> USDA Forest Service
> Albany, California
>
> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Joshua Wiley
> Sent: Sunday, July 08, 2012 1:52 PM
> To: laurent stephane
> Cc: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] singular convergence with lmer()
>
> Notice that the variance of one of your random effects is estimated at 0.  I suspect that this is the source of the singular convergence.
> IIRC proc mixed (which is what I assume you are using in SAS) uses a somewhat different approach to to estimate the random effects than does lme4.
>
> Although it seems to work for Reinhold, again some of the variances are vanishingly small, which seems to me like it may suggest some of the effects are borderline on 0 and perhaps slightly different estimation methods either get "really small" or simply "0" and if 0, you get a warning.  I would also consider simplifying your model (although likelihood ratio tests seem to suggest a significant decrement in the likelihood fixing the variance at 0).
>
> Cheers,
>
> Josh
>
> On Thu, Jul 5, 2012 at 1:01 AM, laurent stephane <laurent_step at yahoo.fr> wrote:
>> Dear all,
>>
>> Using the latest CRAN version of lme4 I get the following warning from lmer() :
>>
>> Warning message:
>> In mer_finalize(ans) : singular convergence (7)
>>
>> My model is not complicated and it works fine with SAS (if you are
>> interested in the details of my model see
>> forums.cirad.fr/logiciel-R/viewtopic.php?t=5071 )
>>
>> What argument could I change in lmer() to overcome this warning ?
>>
>> Kind regards,
>> SL
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>
> --
> Joshua Wiley
> Ph.D. Student, Health Psychology
> Programmer Analyst II, Statistical Consulting Group University of California, Los Angeles https://joshuawiley.com/
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>
>
>
> This electronic message contains information generated by the USDA solely for the intended recipients. Any unauthorized interception of this message or the use or disclosure of the information it contains may violate the law and subject the violator to civil or criminal penalties. If you believe you have received this message in error, please notify the sender and delete the email immediately.
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From laurent_step at yahoo.fr  Mon Jul  9 09:06:12 2012
From: laurent_step at yahoo.fr (laurent stephane)
Date: Mon, 9 Jul 2012 08:06:12 +0100 (BST)
Subject: [R-sig-ME] singular convergence with lmer()
In-Reply-To: <Pine.LNX.4.64.1207090848410.28378@orpheus.qimr.edu.au>
References: <1341475287.26894.YahooMailNeo@web29506.mail.ird.yahoo.com><loom.20120708T215404-691@post.gmane.org>
	<CAG+WrEwXRGywKp0hauQt6BT98tJ1b8AF+3755ZmH3WX7AFRDTg@mail.gmail.com>
	<Pine.LNX.4.64.1207090848410.28378@orpheus.qimr.edu.au>
Message-ID: <1341817572.62366.YahooMailNeo@web29505.mail.ird.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120709/ac4b2f3f/attachment.pl>

From laurent_step at yahoo.fr  Mon Jul  9 09:56:47 2012
From: laurent_step at yahoo.fr (laurent stephane)
Date: Mon, 9 Jul 2012 08:56:47 +0100 (BST)
Subject: [R-sig-ME] singular convergence with lmer()
In-Reply-To: <DAB3E96DAC8E3048A2229FCC86FCB10C11E05956@019-AM1MPN1-061.019D.MGD.MSFT.NET>
References: <DAB3E96DAC8E3048A2229FCC86FCB10C11E05956@019-AM1MPN1-061.019D.MGD.MSFT.NET>
Message-ID: <1341820607.13617.YahooMailNeo@web29506.mail.ird.yahoo.com>



Finally I have reproduced the crash. Before running lme4 I log-transform the response of the original dataset in R. Attached are the 
original dataset and the R code. 


library(lme4)
setwd("...") 

dat <- read.csv("zzz.csv", header=TRUE, na.strings = "", 
??? colClasses=c("factor", "factor","factor","factor","numeric"))
str(dat)

??? dat$Operator <- dat$Operator:dat$Day
??? dat$Part <- dat$Sample
??? dat$y <- log10(dat$Response)
??? dat <- droplevels(subset(dat, subset= !is.na(dat$y)))

lmer(y ~ (1|Operator)+(1|Part)+(1|Part:Operator),? data=dat)



The dataset "zzz.csv" : 


NumDos Day Operator Sample Response 
2010_0402 1 1 1 5.3 
2010_0402 1 1 1 5 
2010_0402 1 1 2 35.8 
2010_0402 1 1 2 34.3 
2010_0402 1 1 3 61.1 
2010_0402 1 1 3 61.6 
2010_0402 1 1 4 130.9 
2010_0402 1 1 4 135.1 
2010_0402 1 1 5 206.3 
2010_0402 1 1 5 195.2 
2010_0402 1 1 6 479.7 
2010_0402 1 1 6 462.2 
2010_0402 1 1 7 780.9 
2010_0402 1 1 7 818.2 
2010_0402 1 1 8 1522.9 
2010_0402 1 1 8 1549.8 
2010_0402 1 1 9 2443.8 
2010_0402 1 1 9 3150.9 
2010_0402 1 1 10 5406.3 
2010_0402 1 1 10 5304.8 
2010_0402 1 1 11 6686.4 
2010_0402 1 1 11 6536.1 
2010_0402 1 1 12 9864.9 
2010_0402 1 1 12 9448 
2010_0402 1 2 1 5.2 
2010_0402 1 2 1 5.2 
2010_0402 1 2 2 36 
2010_0402 1 2 2 37 
2010_0402 1 2 3 67.3 
2010_0402 1 2 3 69.2 
2010_0402 1 2 4 146.3 
2010_0402 1 2 4 138.9 
2010_0402 1 2 5 210.4 
2010_0402 1 2 5 210.8 
2010_0402 1 2 6 534.9 
2010_0402 1 2 6 506.1 
2010_0402 1 2 7 757.2 
2010_0402 1 2 7 813.2 
2010_0402 1 2 8 1659.9 
2010_0402 1 2 8 1790.3 
2010_0402 1 2 9 3478.3 
2010_0402 1 2 9 3469.4 
2010_0402 1 2 10 6377.7 
2010_0402 1 2 10 5758.9 
2010_0402 1 2 11 8258.3 
2010_0402 1 2 11 7317.2 
2010_0402 1 2 12 10461 
2010_0402 1 2 12 10155.5 
2010_0402 2 1 1 4.9 
2010_0402 2 1 1 5.2 
2010_0402 2 1 2 35 
2010_0402 2 1 2 31 
2010_0402 2 1 3 57.9 
2010_0402 2 1 3 60.1 
2010_0402 2 1 4 133.8 
2010_0402 2 1 4 136.9 
2010_0402 2 1 5 173.7 
2010_0402 2 1 5 179.9 
2010_0402 2 1 6 457.2 
2010_0402 2 1 6 489.8 
2010_0402 2 1 7 773.9 
2010_0402 2 1 7 799.2 
2010_0402 2 1 8 1435.1 
2010_0402 2 1 8 1536.5 
2010_0402 2 1 9 3714.1 
2010_0402 2 1 9 3880.5 
2010_0402 2 1 10 5327.9 
2010_0402 2 1 10 5548.3 
2010_0402 2 1 11 7548.1 
2010_0402 2 1 11 7206.5 
2010_0402 2 1 12 9947.5 
2010_0402 2 1 12 10477.1 
2010_0402 2 2 1 5.7 
2010_0402 2 2 1 5.4 
2010_0402 2 2 2 37.6 
2010_0402 2 2 2 37.3 
2010_0402 2 2 3 66.2 
2010_0402 2 2 3 51.6 
2010_0402 2 2 4 121.3 
2010_0402 2 2 4 139.8 
2010_0402 2 2 5 199 
2010_0402 2 2 5 231.7 
2010_0402 2 2 6 514.7 
2010_0402 2 2 6 605.7 
2010_0402 2 2 7 856.6 
2010_0402 2 2 7 867.6 
2010_0402 2 2 8 1539.2 
2010_0402 2 2 8 1691.8 
2010_0402 2 2 9 4337.8 
2010_0402 2 2 9 4744.2 
2010_0402 2 2 10 8121.6 
2010_0402 2 2 10 6447.1 
2010_0402 2 2 11 8577 
2010_0402 2 2 11 8148.4 
2010_0402 2 2 12 13747.7 
2010_0402 2 2 12 12335 
2010_0402 3 1 1 4.8 
2010_0402 3 1 1 4.8 
2010_0402 3 1 2 36.6 
2010_0402 3 1 2 35.6 
2010_0402 3 1 3 69.3 
2010_0402 3 1 3 70.6 
2010_0402 3 1 4 147.3 
2010_0402 3 1 4 141.4 
2010_0402 3 1 5 190.9 
2010_0402 3 1 5 162.5 
2010_0402 3 1 6 525.6 
2010_0402 3 1 6 488.7 
2010_0402 3 1 7 885.3 
2010_0402 3 1 7 866.9 
2010_0402 3 1 8 1590.9 
2010_0402 3 1 8 1662.9 
2010_0402 3 1 9 4146.9 
2010_0402 3 1 9 4962.8 
2010_0402 3 1 10 5005.8 
2010_0402 3 1 10 5787.6 
2010_0402 3 1 11 7605.6 
2010_0402 3 1 11 7996.6 
2010_0402 3 1 12 10513.9 
2010_0402 3 1 12 11256.8 
2010_0402 3 2 1 5.4 
2010_0402 3 2 1 5.4 
2010_0402 3 2 2 34.8 
2010_0402 3 2 2 37.6 
2010_0402 3 2 3 63.7 
2010_0402 3 2 3 65.4 
2010_0402 3 2 4 149.5 
2010_0402 3 2 4 152.5 
2010_0402 3 2 5 201.4 
2010_0402 3 2 5 210.9 
2010_0402 3 2 6 470 
2010_0402 3 2 6 459.5 
2010_0402 3 2 7 885.3 
2010_0402 3 2 7 829 
2010_0402 3 2 8 1781.6 
2010_0402 3 2 8 1555.1 
2010_0402 3 2 9 4215.3 
2010_0402 3 2 9 3966.9 
2010_0402 3 2 10 5063.7 
2010_0402 3 2 10 5365.4 
2010_0402 3 2 11 7441.6 
2010_0402 3 2 11 7592.1 
2010_0402 3 2 12 10769.1 
2010_0402 3 2 12 10955.4 
2010_0402 5 1 1 6 
2010_0402 5 1 1 5.8 
2010_0402 5 1 2 38.8 
2010_0402 5 1 2 38.5 
2010_0402 5 1 3 68.4 
2010_0402 5 1 3 67.4 
2010_0402 5 1 4 149.8 
2010_0402 5 1 4 158.2 
2010_0402 5 1 5 193.1 
2010_0402 5 1 5 190.6 
2010_0402 5 1 6 478.6 
2010_0402 5 1 6 499.9 
2010_0402 5 1 7 914 
2010_0402 5 1 7 897.2 
2010_0402 5 1 8 1543.3 
2010_0402 5 1 8 1387.3 
2010_0402 5 1 9 3574.1 
2010_0402 5 1 9 3640.9 
2010_0402 5 1 10 5371.4 
2010_0402 5 1 10 5583.8 
2010_0402 5 1 11 8196.4 
2010_0402 5 1 11 7754.8 
2010_0402 5 1 12 10663 
2010_0402 5 1 12 12536.7 
2010_0402 5 2 1 6.7 
2010_0402 5 2 1 6.2 
2010_0402 5 2 2 38.9 
2010_0402 5 2 2 40.4 
2010_0402 5 2 3 65.9 
2010_0402 5 2 3 65.2 
2010_0402 5 2 4 139.8 
2010_0402 5 2 4 137.3 
2010_0402 5 2 5 231.3 
2010_0402 5 2 5 217.4 
2010_0402 5 2 6 540.9 
2010_0402 5 2 6 500.9 
2010_0402 5 2 7 807.5 
2010_0402 5 2 7 866.3 
2010_0402 5 2 8 1539.8 
2010_0402 5 2 8 1531.4 
2010_0402 5 2 9 3556.9 
2010_0402 5 2 9 3328.2 
2010_0402 5 2 10 5113.4 
2010_0402 5 2 10 5553.9 
2010_0402 5 2 11 7782.7 
2010_0402 5 2 11 6404.3 
2010_0402 5 2 12 9499.8 
2010_0402 5 2 12 9876.5 



________________________________
 De?: Ben Bolker <bbolker at gmail.com>
??: r-sig-mixed-models at r-project.org 
Envoy? le : Dimanche 8 juillet 2012 21h58
Objet?: Re: [R-sig-ME] singular convergence with lmer()
 
laurent stephane <laurent_step at ...> writes:

> 
> Dear all,
> 
> Using the latest CRAN version of lme4 
>? I get the following warning from lmer() :
> 
> Warning message: 
> In mer_finalize(ans) : singular convergence (7)

> My model is not complicated and it works fine with SAS (if you are
> interested in the details of my model see
> forums.cirad.fr/logiciel-R/viewtopic.php?t=5071 )

> What argument could I change in lmer() to overcome this warning ?
> 

? This warning emerges from the nlminb optimizer used in the guts
of lme4, and I don't think there's much you can do to suppress it
or change the behavior of nlminb to avoid it.? The best you could
do would be to use other packages (SAS, other versions of lme4 or
nlme, etc.) to see if the correct answer was achieved despite the
warning.

? Ben Bolker

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From m.fenati at libero.it  Mon Jul  9 12:44:59 2012
From: m.fenati at libero.it (m.fenati at libero.it)
Date: Mon, 9 Jul 2012 12:44:59 +0200 (CEST)
Subject: [R-sig-ME] MCMCglmm: priors for ordinal regression
Message-ID: <27012705.534791341830699381.JavaMail.defaultUser@defaultHost>

Dear Jarrod,
thank you for your fast answer.
Yes, I had converegence (presence of trend of the time series). Unfortunately, 
I have ordinal data with near complete separation.
My aim is to set a poorly informative or uninformative priors for fixed effect 
in order to improve the chain convergence. Then I set piorB=list(mu=c(rep(0,6)),
V=diag(6)*(100)). The choice of V=100 is not based on other logical or 
numerical reasons. 
I try to display the posterior distribution of latent variable (pl=T), but I 
had a wide range of -25 + 25.....
How can I do? Could you help me to choose the right prior?

Thank in advance

Massimo
 


>----Messaggio originale----
>Da: j.hadfield at ed.ac.uk
>Data: 08/07/2012 12.20
>A: "m.fenati at libero.it"<m.fenati at libero.it>
>Cc: <r-sig-mixed-models at r-project.org>
>Ogg: Re: [R-sig-ME] MCMCglmm: priors for ordinal regression
>
>Dear Massimo,
>
>Do you mean the chain did not converge or the chain did not mix?  
>Generally the former is rare, and is usually only seen with  
>ordinal/categorical data with complete (or near complete) separation.   
>Sometimes a prior that constrains the linear predictor away from  
>extreme values on the logit/probit scale can fix this with a  
>relatively minor prior influence on inferences made on the data scale.  
>Sometimes not. Its not clear to me what the motivation is behind your  
>prior - is it that the sum of your variance components is close to  
>100? If so I would be careful. Use pl=TRUE in your call to MCMCglmm  
>and make sure your latent variables are in the range -7 to 7.
>
>Cheers,
>
>Jarrod
>
>
>
>
>
>Quoting "m.fenati at libero.it" <m.fenati at libero.it> on Wed, 4 Jul 2012  
>16:48:18 +0200 (CEST):
>
>>
>> Dear R user,
>> I have some problems about prior definition in MCMCglmm ordinal  
>> regression. I've tried to use what Jarrod wrote about not  
>> informative priors for ordinal probit but my model did not converge:
>>
>>
>> prior=list(R=list(V= 1, fix=1), G=list(G1=list(V=1, nu=0)))
>>
>>
>> where "..left the default prior for the fixed effects (not  
>> explicitly specified)..".
>>
>>
>> Then, in order to have however a similar uniform distribution for  
>> the latent variable, I set prior for fixed effect  as "mu=0" and  
>> "(co)variance=100":
>>
>>
>> priorB<-rnorm(1000, 0, sqrt(100))
>> priorMB<-1:1000
>> for(i in 1:1000){
>>   priorMB[i]<-mean(pnorm(priorB[i]+rnorm(1000,0,sqrt(100))))
>>    }
>> hist(priorMB)
>>
>>
>> The model converge well but I've some dobts. Is it correct or not?
>>
>>
>> Thank you very much for any suggestions or comments.
>>
>>
>> Best regards
>>
>>
>> Massimo
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>
>
>
>-- 
>The University of Edinburgh is a charitable body, registered in
>Scotland, with registration number SC005336.
>
>
>


From bbolker at gmail.com  Mon Jul  9 15:24:56 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 9 Jul 2012 13:24:56 +0000 (UTC)
Subject: [R-sig-ME]
	=?utf-8?q?Dealing_with_overdispersion_with_glmmadmb_be?=
	=?utf-8?q?ta=09distribution?=
References: <f05cffcf74186100fead7e9a3037f2c9.squirrel@webmail.zoology.ubc.ca>
Message-ID: <loom.20120709T151659-223@post.gmane.org>

Heather Kharouba <kharouba at ...> writes:

> In the analysis I'm doing, I'm interested in testing the importance of a
> factor (with 5 levels). The response variable varies continuously from 0
> to 1 so I've used a beta distribution with glmmadmb:
> 
> m1<-glmmadmb(AUC~variables+log_area+model+taxa+(1|study), family="Beta",
> verbose=TRUE, debug=TRUE, data=mat);
> 
> Call:
> glmmadmb(formula = AUC2 ~ variables + log_area + model2 + taxa +
>     (1 | study), data = mat, family = "Beta", verbose = TRUE,
>     debug = TRUE)
> 
> Beta dispersion parameter: 14.503 (std. err.: 0.35671)
> 
> The dispersion parameter is 14.503 suggesting the model is overdispersed.
> A recent suggestion from http://glmm.wikidot.com/faq and
> r-sig-mixed-models archives to account for overdispersion is to include
> individual-level random effects. However, if I include this additional
> random effect, I get the following error:

   You don't need to include an additional parameter etc. to allow
for overdispersion in a Beta model, because the Beta model already
incudes a dispersion parameter.  (Of the standard models, only
the Poisson and the binomial need the possibility of an overdispersion
factor: Gaussian, Gamma, negative binomial, ... all include an overdispersion
parameter, and Bernoulli (binomial with size=1) can't identify
overdispersion.

  Ben Bolker


From jesper at u.washington.edu  Fri Jul  6 23:38:23 2012
From: jesper at u.washington.edu (Gus Jespersen)
Date: Fri, 6 Jul 2012 14:38:23 -0700
Subject: [R-sig-ME] Calculating CI's for multiple fixed effects
Message-ID: <CAL9m74YVpDUsVgT=Kv_2DVDxDP2_0_GjBD6HWokrNWqRG2E3Ew@mail.gmail.com>

Greetings,
My questions are related to producing confidence intervals via
mcmcsamp and HPDinterval.  As you can see in the output below, my
model has several fixed effect parameters.  Of interest to me are each
of the "Treatment" vs. "Control" comparisons for each "site"(in each
fixed effect parameter name, these are specified by the text
immediately following "sitett").  As I understand it, if I want to
produce a CI for each "Treatment" parameter from the anova output, I
need to simply subtract the Control estimate from the Treatment
estimate for each site. The SE's are a little more complicated, but
that's not important here.  This is easy enough.  However, if I want
to use the mcmcsamp+HPDinterval combo to produce my CI's, I unsure how
to proceed.  The HPDinterval output below lists upper and lower bounds
for each fixed effect parameter, but I have no idea how to get the
"Treatment" vs. "Control" comparison for each site when working with
just the upper and lower bounds of the CI.  Any advice on how to
proceed here would be much appreciated.

Thanks,
Gus

#Models
Mod.NO3.1.1<-lmer(LogNO3Nyearone ~ 1 + sitett +(1 | pr), data=data.file.final)
Mod.NO3.1.2<-lmer(LogNO3Nyearone~ 1 +(1 | pr), data=data.file.final)

#Output:
anova(Mod.NO3.1.1,Mod.NO3.1.2)
Mod.NO3.1.1
Mod.NO3.1.1.mcmc<-mcmcsamp(Mod.NO3.1.1, n=1000)
HPDinterval(Mod.NO3.1.1.mcmc, prob=.95)

***********************************************************************
Data: data.file.final
Models:
Mod.NO3.1.2: LogNO3Nyearone ~ 1 + (1 | pr)
Mod.NO3.1.1: LogNO3Nyearone ~ 1 + sitett + (1 | pr)
            Df    AIC    BIC  logLik  Chisq Chi Df Pr(>Chisq)
Mod.NO3.1.2  3 99.830 108.59 -46.915
Mod.NO3.1.1 14 74.329 115.21 -23.165 47.501     11  1.752e-06 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
Linear mixed model fit by REML
Formula: LogNO3Nyearone ~ 1 + sitett + (1 | pr)
   Data: data.file.final
   AIC   BIC logLik deviance REMLdev
 110.9 151.8 -41.46    46.33   82.92
Random effects:
 Groups   Name        Variance  Std.Dev.
 pr       (Intercept) 0.0029239 0.054073
 Residual             0.0871127 0.295149
Number of obs: 137, groups: pr, 72

Fixed effects:
                             Estimate Std. Error t value
(Intercept)                   1.24011    0.09047  13.708
sitettLepAddition Treatment  -0.03859    0.12329  -0.313
sitettMossAddition Control    0.07747    0.13110   0.591
sitettMossAddition Treatment  0.13940    0.12794   1.090
sitettMossRemoval Control    -0.36994    0.12525  -2.954
sitettMossRemoval Treatment  -0.25789    0.12525  -2.059
sitettSaddle Control         -0.33039    0.12525  -2.638
sitettSaddle Treatment       -0.46379    0.12794  -3.625
sitettToeAdditions Control   -0.17356    0.12525  -1.386
sitettToeAdditions Treatment -0.33236    0.13110  -2.535
sitettToeRemoval Control     -0.38744    0.12525  -3.093
sitettToeRemoval Treatment   -0.44006    0.12525  -3.513

Correlation of Fixed Effects:
            (Intr) sttLAT sttMAC sttMAT sttMRC sttMRT stttSC stttST
sttTAC sttTAT sttTRC
stttLpAddtT -0.712
stttMssAddC -0.690  0.491
stttMssAddT -0.707  0.503  0.502
stttMssRmvC -0.722  0.514  0.498  0.511
stttMssRmvT -0.722  0.514  0.498  0.511  0.537
stttSddlCnt -0.722  0.514  0.498  0.511  0.522  0.522
stttSddlTrt -0.707  0.503  0.488  0.500  0.511  0.511  0.526
stttTAddtnC -0.722  0.514  0.498  0.511  0.522  0.522  0.522  0.511
stttTAddtnT -0.690  0.491  0.476  0.488  0.498  0.498  0.498  0.488
0.513
stttTRmvlCn -0.722  0.514  0.498  0.511  0.522  0.522  0.522  0.511
0.522  0.498
stttTRmvlTr -0.722  0.514  0.498  0.511  0.522  0.522  0.522  0.511
0.522  0.498  0.537

$fixef
                                  lower       upper
(Intercept)                   1.0680512  1.44056743
sitettLepAddition Treatment  -0.2625679  0.22422795
sitettMossAddition Control   -0.1969100  0.32171278
sitettMossAddition Treatment -0.1281604  0.38479353
sitettMossRemoval Control    -0.6237913 -0.12131695
sitettMossRemoval Treatment  -0.5529020 -0.03247241
sitettSaddle Control         -0.5796939 -0.07807756
sitettSaddle Treatment       -0.7031393 -0.20777619
sitettToeAdditions Control   -0.4209195  0.07304341
sitettToeAdditions Treatment -0.5943879 -0.05751119
sitettToeRemoval Control     -0.6358857 -0.14028991
sitettToeRemoval Treatment   -0.7043854 -0.22088608
attr(,"Probability")
[1] 0.95

$ST
     lower     upper
[1,]     0 0.2142559
attr(,"Probability")
[1] 0.95

$sigma
         lower     upper
[1,] 0.2643662 0.3430281
attr(,"Probability")
[1] 0.95

-- 
R. Gus Jespersen
PhD Candidate
College of Forest Resources
University of Washington
Box 352100
Seattle, WA 98195-2100
(206) 543-5777
jesper at u.washington.edu


From muhammad.mullah at mail.mcgill.ca  Wed Jul  4 00:54:57 2012
From: muhammad.mullah at mail.mcgill.ca (Muhammad Mullah)
Date: Tue, 3 Jul 2012 22:54:57 +0000
Subject: [R-sig-ME] Using glmer in R
In-Reply-To: <Pine.LNX.4.64.1207020837580.29665@orpheus.qimr.edu.au>
References: <1813C6762739724EA611B27328FABABF7E5877@EXMBX2010-1.campus.MCGILL.CA>,
	<Pine.LNX.4.64.1207020837580.29665@orpheus.qimr.edu.au>
Message-ID: <1813C6762739724EA611B27328FABABF7E58C2@EXMBX2010-1.campus.MCGILL.CA>

Dear Duffy,
As per your suggestion I went through the following but could not find any clue or work out example to figure out my problem; fitting a mixed model using ?glmer? or ?mler? so that all the random effects are iid with common variance parameter i.e., the variance covariance matrix will have the common value for diagonal element and zero for the off diagonal elements.

Can you please help me in this regard!

-------------------------------------
Have you read the vignettes
vignette(package="lme4")
the draft book chapters at
http://lme4.r-forge.r-project.org/
lecture notes at
http://lme4.r-forge.r-project.org/slides/
wiki at
http://glmm.wikidot.com/
------------------------------------------

With Regards,
Shadeque
PhD student
Dept. of Epidemiology and Biostatistics
McGill University
________________________________________
From: David Duffy [David.Duffy at qimr.edu.au]
Sent: Sunday, July 01, 2012 6:49 PM
To: Muhammad Mullah
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Using glmer in R

On Sun, 1 Jul 2012, Muhammad Mullah wrote:

> Can anybody helps me to figure out how ?glmer? or ?lmer? in R can be
> used to estimate the random effects in mixed model so that all random
> effects are independent and have the same variance, i.e., the variance
> covariance matrix will have same value for diagonal element and zero for
> covariance.

Usually the question is how to make them heterogenous ;)

Have you read the vignettes
vignette(package="lme4")
the draft book chapters at
http://lme4.r-forge.r-project.org/
lecture notes at
http://lme4.r-forge.r-project.org/slides/
wiki at
http://glmm.wikidot.com/


From bbolker at gmail.com  Tue Jul 10 17:18:39 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 10 Jul 2012 15:18:39 +0000 (UTC)
Subject: [R-sig-ME] Calculating CI's for multiple fixed effects
References: <CAL9m74YVpDUsVgT=Kv_2DVDxDP2_0_GjBD6HWokrNWqRG2E3Ew@mail.gmail.com>
Message-ID: <loom.20120710T171125-158@post.gmane.org>

Gus Jespersen <jesper <at> u.washington.edu> writes:

> 
> Greetings,
> My questions are related to producing confidence intervals via
> mcmcsamp and HPDinterval.  As you can see in the output below, my
> model has several fixed effect parameters.  Of interest to me are each
> of the "Treatment" vs. "Control" comparisons for each "site"(in each
> fixed effect parameter name, these are specified by the text
> immediately following "sitett").  As I understand it, if I want to
> produce a CI for each "Treatment" parameter from the anova output, I
> need to simply subtract the Control estimate from the Treatment
> estimate for each site. The SE's are a little more complicated, but
> that's not important here.  This is easy enough.  However, if I want
> to use the mcmcsamp+HPDinterval combo to produce my CI's, I unsure how
> to proceed.  The HPDinterval output below lists upper and lower bounds
> for each fixed effect parameter, but I have no idea how to get the
> "Treatment" vs. "Control" comparison for each site when working with
> just the upper and lower bounds of the CI.  Any advice on how to
> proceed here would be much appreciated.
> 
> Thanks,
> Gus

  I think you just need to take the output of mcmcsamp, manipulate,
and *then* use HPDinterval.

  m <- mcmcsamp(...)
  library(coda)
  m2 <- as.mcmc(as.matrix(m))

should give you an mcmc object -- you can then manipulate (mostly)
as though it were a matrix (e.g. subtract each Control column from
the corresponding Treatment column), then use the HPDinterval
built into coda to find the 95% CI on the differences ...


From ssimek at CFR.MsState.Edu  Tue Jul 10 22:40:21 2012
From: ssimek at CFR.MsState.Edu (Stephanie L. Simek)
Date: Tue, 10 Jul 2012 15:40:21 -0500
Subject: [R-sig-ME] zero inflated and repeat cells
Message-ID: <736BA42933F6C84698DBA828209ED41E01AD1026@mail.cfr.msstate.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120710/ee5c623b/attachment.pl>

From h.l.ward at qmul.ac.uk  Wed Jul 11 12:58:50 2012
From: h.l.ward at qmul.ac.uk (Helen Ward)
Date: Wed, 11 Jul 2012 11:58:50 +0100
Subject: [R-sig-ME] A zero inflated Poisson model in MCMCglmm
In-Reply-To: <20120708120337.32586osul89lwkkk@www.staffmail.ed.ac.uk>
References: <4FF46034.8070203@qmul.ac.uk>
	<20120708120337.32586osul89lwkkk@www.staffmail.ed.ac.uk>
Message-ID: <4FFD5C6A.80502@qmul.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120711/13423d01/attachment.pl>

From highstat at highstat.com  Wed Jul 11 13:02:32 2012
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Wed, 11 Jul 2012 12:02:32 +0100
Subject: [R-sig-ME] zero inflated and repeat cells
In-Reply-To: <mailman.7.1342000802.2953.r-sig-mixed-models@r-project.org>
References: <mailman.7.1342000802.2953.r-sig-mixed-models@r-project.org>
Message-ID: <4FFD5D48.4020801@highstat.com>




>
> ------------------------------
>
> Message: 2
> Date: Tue, 10 Jul 2012 15:40:21 -0500
> From: "Stephanie L. Simek" <ssimek at CFR.MsState.Edu>
> To: <r-sig-mixed-models at r-project.org>
> Subject: [R-sig-ME] zero inflated and repeat cells
> Message-ID:
> 	<736BA42933F6C84698DBA828209ED41E01AD1026 at mail.cfr.msstate.edu>
> Content-Type: text/plain
>
> Dear R users,
>
>   
>
> I have count data that illustrates the presences or absence of
> individuals in my study population. I created a grid of cells across the
> study area and calculated a count value for each individual per season
> per year for each cell. The count value is the number of times an
> individual was present in each cell.
>
>   
>
> My data is set up with the cell ID repeated for each individual per
> season per year. This format results in 71,000 records (of which 925
> have a count value >0 and 70,075 have a count value = 0). I would like
> to run mixed effects model with individual and year as random effects
> but I have been advised two things:
>
>   

I don't know whether the approach sketched above is good, bad or 
pragmatic (I would need to
know more details, but I did raise my eyebrow),
but as to your specific questions below:

1. Not sure if I would agree. It all depends on where the zeros are, and 
whether
some of the covariates can be used to model the zeros....in which case 
even a Poisson GLM
may do the job. But a ZIP is probably ok.



2. You need to think very careful where any dependency structures are in 
your experiment.
Within a cell...between cells? Without further information I can't say.
Don;t you have spatial correlation between these cells?


Alain
> 1.      The data are too extremely zero inflated therefore a zero
> inflated Poisson model will not work.
>
> 2.      The model cannot be run because the grid cells are repeated for
> each individual per season per year. I am told the model doesn't
> recognize that Cell ID #1 for individual "A" is the same cell for Cell
> ID #1 for individual "B".
>
>   
>
> Does anyone know if either or both of these points are true? Also, does
> anyone have any recommendations.
>
>   
>
> Thank you,
>
>   
>
> -Stephanie
>
>   
>
>   
>
> -------------------------------------------------------
>
> Stephanie L. Simek
>
> Carnivore Ecology Lab
>
> Forest and Wildlife Research Center
>
> Mississippi State University
>
> Box 9690
>
> Mississippi State, MS 39762
>
> Cell: (850) 591-1430
>
> Email: ssimek at cfr.msstate.edu
>
>
>
>
> 	[[alternative HTML version deleted]]
>
>
>
> ------------------------------
>
> _______________________________________________
> R-sig-mixed-models mailing list
> R-sig-mixed-models at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
> End of R-sig-mixed-models Digest, Vol 67, Issue 13
> **************************************************
>


-- 

Dr. Alain F. Zuur
First author of:

1. Analysing Ecological Data (2007).
Zuur, AF, Ieno, EN and Smith, GM. Springer. 680 p.
URL: www.springer.com/0-387-45967-7


2. Mixed effects models and extensions in ecology with R. (2009).
Zuur, AF, Ieno, EN, Walker, N, Saveliev, AA, and Smith, GM. Springer.
http://www.springer.com/life+sci/ecology/book/978-0-387-87457-9


3. A Beginner's Guide to R (2009).
Zuur, AF, Ieno, EN, Meesters, EHWG. Springer
http://www.springer.com/statistics/computational/book/978-0-387-93836-3


4. Zero Inflated Models and Generalized Linear Mixed Models with R. (2012) Zuur, Saveliev, Ieno.
http://www.highstat.com/book4.htm

Other books: http://www.highstat.com/books.htm


Statistical consultancy, courses, data analysis and software
Highland Statistics Ltd.
6 Laverock road
UK - AB41 6FN Newburgh
Tel: 0044 1358 788177
Email: highstat at highstat.com
URL: www.highstat.com
URL: www.brodgar.com


From chantepie at mnhn.fr  Wed Jul 11 16:01:53 2012
From: chantepie at mnhn.fr (Stephane Chantepie)
Date: Wed, 11 Jul 2012 16:01:53 +0200
Subject: [R-sig-ME] Post-test on MCMCglmm posteriors distribution
Message-ID: <201207111601.53136.chantepie@mnhn.fr>

Dear all,

I have a quite simple question but I do not find a clear answer on the 
internet nor in books.

The context : I am currently working on different aspects of bird aging. I 
have done animal models with MCMCglmm to find if there is an additive genetic 
variance (Va) in my sexual life traits. One animal model has been done by age. 
Now, I am interested in testing the differences in Va (posterior mode) between 
ages.

My question : As posterior predictions are supposed to be normally distributed 
(if the model converge), is it possible to use multiple Student test 
(corrected for unequal variance and pairwise multiple comparison) to test 
statiscal difference on posterior prediction? It seems to me that all 
assumptions are meet to use Student test but maybe there some problems that I 
am not thinking about.

If you have a more elegant way to test differences of posterior distributions, 
let me know!

Thank in advance for reply and sorry for this beginner question.

Stephane


From amitgal4 at gmail.com  Thu Jul 12 00:55:43 2012
From: amitgal4 at gmail.com (amit gal)
Date: Wed, 11 Jul 2012 17:55:43 -0500
Subject: [R-sig-ME] coding a simple multiple membership model
Message-ID: <CAHv_4K7G8GTxS-+9yP2ZFk2Ah+kYOPvQr2Xazzvancn9Y6_8BQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120711/35aaec25/attachment.pl>

From kw.stat at gmail.com  Thu Jul 12 02:17:10 2012
From: kw.stat at gmail.com (Kevin Wright)
Date: Wed, 11 Jul 2012 19:17:10 -0500
Subject: [R-sig-ME] Connectedness of rows/columns of a matrix
Message-ID: <CAKFxdiQbF+Sb5nGF0_kOn0O5=wfQXcs36kuk=HdO=10J2_Vycw@mail.gmail.com>

Suppose I have data with two factors A and B that each have two
levels.  If all combinations of the factors are observed, then an
incidence matrix showing combinations that are observed would look
like this:
   b1 b2
a1  1  1
a2  1  1
If I then fit a (mixed) model like y ~ A + B, the effects are
generally estimable.

If, however, some combinations are not observed, so that the incidence
matrix looks like this:
   b1 b2
a1  1  0
a2  0  1
then the A and B effects are aliased and non-estimable.

This is an extremely simple example, but sometimes when I am fitting
models which fail to converge, I dig into the data and find there is
not a connected path through the rows and columns of the incidence
matrix and (I assume) this is the cause of the non-estimability.

Two questions:
1. Is there a method to detect if the rows/columns of a matrix are
connected by at least one path through the matrix?
2. More generally, is there a measure of "connectedness" of the rows/columns?
3. Even more generally, what techniques do people use to diagnose
non-estimability?

Hope this is understandable and interesting.

Kevin Wright


-- 
Kevin Wright


From David.Duffy at qimr.edu.au  Thu Jul 12 02:44:57 2012
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Thu, 12 Jul 2012 10:44:57 +1000
Subject: [R-sig-ME] Connectedness of rows/columns of a matrix
In-Reply-To: <CAKFxdiQbF+Sb5nGF0_kOn0O5=wfQXcs36kuk=HdO=10J2_Vycw@mail.gmail.com>
References: <CAKFxdiQbF+Sb5nGF0_kOn0O5=wfQXcs36kuk=HdO=10J2_Vycw@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.1207121042560.27689@orpheus.qimr.edu.au>

On Wed, 11 Jul 2012, Kevin Wright wrote:

> Two questions:
> 1. Is there a method to detect if the rows/columns of a matrix are
> connected by at least one path through the matrix?
> 2. More generally, is there a measure of "connectedness" of the rows/columns?
> 3. Even more generally, what techniques do people use to diagnose
> non-estimability?

The rank of the design matrix for these kinds of problems.  For more 
elaborate SEMs, run from different start values and get different answers 
;)


From Christoph.Scherber at agr.uni-goettingen.de  Thu Jul 12 14:09:36 2012
From: Christoph.Scherber at agr.uni-goettingen.de (Christoph Scherber)
Date: Thu, 12 Jul 2012 14:09:36 +0200
Subject: [R-sig-ME] P values in summary tables from lmer objects
Message-ID: <4FFEBE80.8030601@agr.uni-goettingen.de>

Dear all,

It seems that the newest version of lme4 (0.999999-0) now provides P values in the summary tables.
Given the many previous discussions about the derivation of P values in lmer, I was just wondering
how "safe" it would now be to use these values (and how they?re calculated). I guess they?re just
taken internally from a call to glm?

Example:

##
gm1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),
              family = binomial, data = cbpp)
summary(gm1)
coef(summary(gm1))

##

Many thanks,
Christoph


[running R 2.15.1 on Windows 7 32-Bit]






-- 
PD Dr Christoph Scherber
Georg-August University Goettingen
Department of Crop Science
Agroecology
Grisebachstrasse 6
D-37077 Goettingen
Germany
phone 0049 (0)551 39 8807
fax 0049 (0)551 39 8806
http://www.gwdg.de/~cscherb1


From lborger at cebc.cnrs.fr  Thu Jul 12 15:03:00 2012
From: lborger at cebc.cnrs.fr (lborger)
Date: Thu, 12 Jul 2012 15:03:00 +0200
Subject: [R-sig-ME] P values in summary tables from lmer objects
In-Reply-To: <4FFEBE80.8030601@agr.uni-goettingen.de>
References: <4FFEBE80.8030601@agr.uni-goettingen.de>
Message-ID: <WC20120712130300.120172@cebc.cnrs.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120712/b137467c/attachment.pl>

From eric.d.stolen at nasa.gov  Thu Jul 12 15:01:26 2012
From: eric.d.stolen at nasa.gov (Stolen, D Eric  (KSC-IHA-4400)[Innovative Health Applications LLC])
Date: Thu, 12 Jul 2012 08:01:26 -0500
Subject: [R-sig-ME] why is there no the REML option for lme4::glmer
Message-ID: <C4F0189B45EA6A43903ADBB756D6F2D25175F6BD42@NDMSSCC08.ndc.nasa.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120712/1b991bd1/attachment.pl>

From j.hadfield at ed.ac.uk  Thu Jul 12 15:13:09 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Thu, 12 Jul 2012 14:13:09 +0100
Subject: [R-sig-ME] A zero inflated Poisson model in MCMCglmm
In-Reply-To: <4FFD5C6A.80502@qmul.ac.uk>
References: <4FF46034.8070203@qmul.ac.uk>
	<20120708120337.32586osul89lwkkk@www.staffmail.ed.ac.uk>
	<4FFD5C6A.80502@qmul.ac.uk>
Message-ID: <20120712141309.10586qx02atg5hwc@www.staffmail.ed.ac.uk>

Hi,

Quoting Helen Ward <h.l.ward at qmul.ac.uk> on Wed, 11 Jul 2012 11:58:50 +0100:

> Dear Jarrod and David,
>
> Thank you both very much for your responses: I'm sorry it's taken me  
> so long to say that, but I wanted to have a proper go at everything  
> before I replied, which took longer than I anticipated!
>
> Thank you especially for your zipoisson code, Jarrod. When I  
> initially attempted to use your prior and model MCMCglmm asked me  
> for a stronger prior, so I changed it to
>
>
> Prior1=list(R=list(V=diag(2),nu=1, fix=2), G=list(G1=G1)) : is that  
> a reasonable thing to have done? The model
>
>
I think it is better to diagnose the problem rather than upping nu. In  
this case I think the residual variance of the Poisson process must be  
going to zero under the flat prior? Perhaps the counts are  
under-dispersed with respect to the Poisson, further motivating an  
ordinal model?



model1.1<-MCMCglmm(Toes~trait,random=~us(trait):animal,family="zipoisson",rcov=~idh(trait):units,pedigree=ped,data=data,prior=Prior1,nitt=500000,thin=500,burnin=200000,verbose=FALSE)
>
> ran with this prior, however I admit I am now flummoxed as to how to  
> proceed and go about estimating heritability.
>
>
> For this reason, (and because you both recommended it!), I went on  
> to try an ordinal model using the following code:
>
> p.var<-var(data$Toes,na.rm=TRUE)
>
> prior2<-list(G=list(G1=list(V=matrix(p.var/2),n=1)),  
> R=list(V=matrix(p.var/2),n=1))
>
>
IMPORTANT: The residual variance is not identifiable in an ordinal  
model so all the information is coming from the prior. This is OK, but  
I would fix the variance at 1 so as to avoid interpretation  
difficulties (and numerical problems - check the range of the latent  
variables using pl=TRUE).


model2<-MCMCglmm(Toes~1,random=~animal,family="ordinal",pedigree=ped,data=data,prior=prior2,nitt=500000,thin=500,burnin=200000,verbose=FALSE)
>
>
> followed by the same model but with the prior from the Wilson paper erratum:
>
> prior2.1 <- list(G = list(G1 = list(V = 1, nu = 0.002)), R = list(V  
> = 1,nu = 0.002))
>
>
> the plots for these models look ok, and they both give a  
> heritability estimate of ~0.55, with very similar 95% HDP intervals,  
> although the auto correlation is quite high ~0.6

Is this calculated as Vanimal/(Vanimal+Vresidual+1)  or  
Vanimal/(Vanimal+Vresidual)?

>
>
> Next I tried a categorical model, treating Toes as a binary response  
> variable (deforemed or not deformed), using
>
> p.var<-var(data$ToesBinary,na.rm=TRUE)
>
> prior3= list(R = list(V = 1, n = 0, fix = 1), G = list(G1 = list(V =  
> 1,n = 0)))
>
> model3<-  
> MCMCglmm(ToesBinary~1,random=~animal,family="categorical",data=data,pedigree=ped,prior=prior3,nitt=200000,thin=250,burnin=100000,verbose =  
> FALSE)
>
>
> Heritability came out higher with this model at about ~0.85.

Is this calculated as Vanimal/(Vanimal+Vresidual+pi^2/3)  or  
Vanimal/(Vanimal+Vresidual)?
>
>
> And then last but not least a multinomial2 model:
>
> prior4=list(R=list(V=1,nu=0.002),G=list(G1=list(V=1,nu=1,alpha.mu=0,alpha.V=100)))
>
> model4<-MCMCglmm(cbind(Deformed,Fine)~1,random=~animal,family="multinomial2",data=data,pedigree=ped,prior=prior4,nitt=200000,thin=250,burnin=100000,verbose=FALSE)
>
> The variance plots for this one didn't look very pretty and the  
> autocorrelation values were still quite high, ~0.8, but it gave me a  
> pretty heritability plot and an estimate similar to my ordinal models.
>
>
> As you can probably gather from all of this, I am something of an  
> indecisive person...., but at this stage I am inlcined to stick with  
> the ordinal models and believe that there is a heritable element to  
> the number of deformed toes (which seems rather intuitive after all  
> that!). Given this decision, can you recommend anything I might do  
> to reduce the autocorrelation in these models. Also, do I have to  
> run the same model many times to confirm its conclusions?

Re: autocorrelation. Use parameter expansion and/or run for longer  
(particularly for binary animal models and/or if there is little  
overdispersion)  If the absolute value of the latent variables under  
the logit link exceed 20, or under the probit link exceed 7, then at  
the moment I would consider another program unless the latent  
variables are associated with particular levels of a fixed effect.   
Then flatish priors on the probability scale may overcome the problem.


>
>
> Finally finally, I will have to do another zipoisson model in the  
> future, can you direct me to where I might find an example of the  
> code used for estimating heritability in these cases please?

I think it makes most sense to think about the heritabilities of each  
process separately   - the first as a Poisson trait and the second as  
a threshold (binary) trait.

>
>
> Many many thanks once more,
> Helen
>
>
>
>
> On 08/07/2012 12:03, Jarrod Hadfield wrote:
>> Hi,
>>
>> I agree with David that an ordinal model may behave better for this  
>> type of data (or even a 0/1 response with deformed or not) but in  
>> case you want to pursue a ZIP model...
>>
>> The model you have specified is probably not what you had in mind.  
>> You should think of the ZIP response as being two responses  
>> (traits): the Poisson counts (trait 1) and the probability that a  
>> zero comes from the zero-inflation process (trait 2). At the moment  
>> you have a common intercept for both and therefore assume that the  
>> probability that a zero is from the zero-inflation process (on the  
>> logit scale) is equal to the mean Poisson count (on the log scale).  
>> There is probably little justification for this (but see ZAP  
>> models). Moreover, you have a single animal term, and therefore  
>> assume that both processes have the same genetic variance and that  
>> the genetic correlation between them is 1. You also assume that the  
>> residual variance for each process is equal (but the residual  
>> correlation is 0), although this is not actually that bigger deal  
>> because residual variation in the zero-inflation process (and  
>> residual corrletaion) is not identifiable from the data and so the  
>> value it could take is essentially arbitrary. However, I like to  
>> recognise this non-identifiability by fixing the variance at 1 and  
>> the covariance at zero (but see ZAP models where your specification  
>> is useful).
>>
>> So, a more appropriate model would be:
>>
>> model1.1<-MCMCglmm(Toes~trait,random=~us(trait):animal,family="zipoisson",rcov=~idh(trait):units,pedigree=ped,data=data,prior=prior1.1,nitt=500000,thin=500,burnin=200000,verbose=FALSE) or  
>> perhaps
>>
>> model1.2<-MCMCglmm(Toes~trait,random=~idh(trait):animal,family="zipoisson",rcov=~idh(trait):units,pedigree=ped,data=data,prior=prior1.1,nitt=500000,thin=500,burnin=200000,verbose=FALSE) The difference between these two is that the genetic correlation between the zero-inflation and the Poisson processes is estimated, and in the latter it is set to  
>> 0.
>>
>> I'm not sure about how much data you have, but I would think that  
>> if the incidence of deformed toes is very low, you probably have  
>> little information for some of the parameters. In this case you  
>> have to be VERY careful with priors. The prior recommendations in  
>> the MCMCglmm tutorial in Wilson et al. 2011 will generally be quite  
>> informative for small or even moderate sized datasets (the authors  
>> have kindly published an erratum as many people seem to be using  
>> their recommendation without realising its implications).
>>
>> The prior could look like:
>>
>> prior=list(R=list(V=diag(2), nu=0, fix=2), G=list(G1=G1))
>>
>> which fixes the residual variance in the zero-inflation process to  
>> 1 with a flat improper prior on the Poisson over-dispersion. For  
>> the prior on the genetic (co)variances (G1) I generally use  
>> parameter expanded priors of the form:
>>
>> G1=list(V=diag(2), nu=2, alpha.mu=c(0,0), alpha.V=diag(2)*1000)
>>
>> or
>>
>> G1=list(V=diag(2), nu=1, alpha.mu=c(0,0), alpha.V=diag(2)*1000)
>>
>> for model.2.
>>
>> These generally give posterior modes for the variance components  
>> that are close to (RE)ML estimates, although it is unclear to me  
>> how they behave for the covariances and functions of variances  
>> (e.g. heritability). At least for binary traits (e.g. the  
>> zero-inflation bit) I have seen a chi-square prior recommended  
>> because of its  prior properties on the heritability (de  
>> Villemereuil in MEE). This type of prior can also be obtained using  
>> parameter expansion:
>>
>> (V=1, nu=1000, alpha.mu=0, alpha.V=1)
>>
>> and I would guess that the multivariate analogue would be  
>> (V=diag(2), nu=1001, alpha.mu=c(0,0), alpha.V=diag(2)). However, a  
>> chi-square prior probably has poor properties for the Poisson  
>> variance and in MCMCglmm you could only mix these different priors  
>> for model.2 using a different syntax:
>>
>> model1.2<-MCMCglmm(Toes~trait,random=~idh(at(trait,1)):animal+idh(at(trait,2)):animal,family="zipoisson",rcov=~idh(trait):units,pedigree=ped,data=data,prior=prior1.1,nitt=500000,thin=500,burnin=200000,verbose=FALSE)  
>> G=list(
>>    G1=list(V=1, nu=1, alpha.mu=0, alpha.V=1000),
>>    G2=list(V=1, nu=1000, alpha.mu=0, alpha.V=1)
>>  )
>>
>>
>> Cheers,
>>
>> Jarrod
>>
>>
>>
>>
>> Quoting Helen Ward <h.l.ward at qmul.ac.uk> on Wed, 04 Jul 2012 16:24:36 +0100:
>>
>>> Dear list,
>>>
>>> I work on bats, some of which have deformed toes. I have just started
>>> trying to use MCMCglmm to investigate whether there is hertiable
>>> variation for the number of deformed toes a bat has. Since most bats
>>> have 0 deformed toes I am using a zero-inflated Poisson model.
>>>
>>> I have put together some code (see below) based on the MCMCglmm tutorial
>>> from the Wilson et al. 2011 'Ecologist's guide to the animal model'
>>> paper, the MCMCglmm Course Notes and trial and error. This code runs and
>>> I get a model and a heritability estimate. However, the heritability
>>> estimate I get is extremely high and this, teamed with the observations
>>> that my posterior distributions of the variance componants are not
>>> pretty as the example ones I've seen and my autocorrelation values are
>>> high (~0.4), makes me suspicious that I am not modelling this trait very
>>> well. I suspect I have over simplified the model. I based my prior on a
>>> model done in ASReml modelling 'Toes' as a Poisson distribution, which
>>> can an heritability estimate of 0.35.
>>>
>>> In addition, when I try and add a second random variable - year of birth
>>> (Born) - into a second model, both plots and auto correlation get worse
>>> and my heritability estimate for number of deformed toes (Toes) goes
>>> from about 0.8 to 0.05!
>>>
>>> Finally, I suspect you're not, but I would like to know if you are
>>> allowed to compare DIC values from models with different priors please.
>>>
>>> I have pasted my code below. If anyone can offer constructive criticism
>>> it would be much appreciated.
>>>
>>> Many thanks,
>>> Helen
>>>
>>> Model 1: With number of deformed toes (Toes) as the only random variable
>>>
>>>
>>> p.var<-var(data$Toes,na.rm=TRUE)
>>> prior1.1<-list(G=list(G1=list(V=matrix(p.var*0.35),n=1)),
>>> R=list(V=matrix(p.var*0.65),n=1))
>>>
>>> model1.1<-MCMCglmm(Toes~1,random=~animal,family="zipoisson",rcov=~trait:units,pedigree=ped,data=data,prior=prior1.1,nitt=500000,thin=500,burnin=200000,verbose=FALSE) posterior.heritability1.1<-model1.1$VCV[,"animal"]/(model1.1$VCV[,"animal"]+model1.1$VCV[,"trait:units"])  
>>> HPDinterval(posterior.heritability1.1,0.95)
>>> posterior.mode(posterior.heritability1.1)
>>>
>>>
>>> Model 2: With number of toes (Toes) and year of birth (Born) as random
>>> variables
>>>
>>>
>>> p.var<-var(data$Toes,na.rm=TRUE)
>>> prior1.2<-list(G=list(G1=list(V=matrix(p.var/3),n=1),G2=list(V=matrix(p.var/3),n=1)),R=list(V=matrix(p.var/3),n=1))  
>>> model1.2<-
>>> MCMCglmm(Toes~1,random=~animal+Born,family="zipoisson",rcov=~trait:units,pedigree=ped,data=data,prior=prior1.2,nitt=1000000,thin=1000,burnin=500000,verbose=FALSE) posterior.heritability1.2<-model1.2$VCV[,"animal"]/(model1.2$VCV[,"animal"]+model1.2$VCV[,"Born"]+model1.2$VCV[,"trait:units"])  
>>> HPDinterval(posterior.heritability1.2,0.95)
>>> posterior.mode(posterior.heritability1.2)
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>    [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>>
>>
>>
>>
>
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From j.hadfield at ed.ac.uk  Thu Jul 12 15:20:28 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Thu, 12 Jul 2012 14:20:28 +0100
Subject: [R-sig-ME] MCMCglmm: priors for ordinal regression
In-Reply-To: <27012705.534791341830699381.JavaMail.defaultUser@defaultHost>
References: <27012705.534791341830699381.JavaMail.defaultUser@defaultHost>
Message-ID: <20120712142028.116530uigqbt2bcw@www.staffmail.ed.ac.uk>

Hi,

If the prior variance on your fixed effects is V+1 where V is the sum  
of the variance components (including the residual) then the marginal  
prior on the fixed effects is as flat as possible on the probability  
interval (0,1). However, you have to set up the contrasts correctly.

If you still get numerical problems I'm afraid you will have to find  
another way of doing the analysis. I have no solution, and no one has  
suggested any:

https://stat.ethz.ch/pipermail/r-sig-mixed-models/2012q1/017976.html

Cheers,

Jarrod



Quoting "m.fenati at libero.it" <m.fenati at libero.it> on Mon, 9 Jul 2012  
12:44:59 +0200 (CEST):

> Dear Jarrod,
> thank you for your fast answer.
> Yes, I had converegence (presence of trend of the time series).  
> Unfortunately,
> I have ordinal data with near complete separation.
> My aim is to set a poorly informative or uninformative priors for  
> fixed effect
> in order to improve the chain convergence. Then I set  
> piorB=list(mu=c(rep(0,6)),
> V=diag(6)*(100)). The choice of V=100 is not based on other logical or
> numerical reasons.
> I try to display the posterior distribution of latent variable (pl=T), but I
> had a wide range of -25 + 25.....
> How can I do? Could you help me to choose the right prior?
>
> Thank in advance
>
> Massimo
>
>
>
>> ----Messaggio originale----
>> Da: j.hadfield at ed.ac.uk
>> Data: 08/07/2012 12.20
>> A: "m.fenati at libero.it"<m.fenati at libero.it>
>> Cc: <r-sig-mixed-models at r-project.org>
>> Ogg: Re: [R-sig-ME] MCMCglmm: priors for ordinal regression
>>
>> Dear Massimo,
>>
>> Do you mean the chain did not converge or the chain did not mix?
>> Generally the former is rare, and is usually only seen with
>> ordinal/categorical data with complete (or near complete) separation.
>> Sometimes a prior that constrains the linear predictor away from
>> extreme values on the logit/probit scale can fix this with a
>> relatively minor prior influence on inferences made on the data scale.
>> Sometimes not. Its not clear to me what the motivation is behind your
>> prior - is it that the sum of your variance components is close to
>> 100? If so I would be careful. Use pl=TRUE in your call to MCMCglmm
>> and make sure your latent variables are in the range -7 to 7.
>>
>> Cheers,
>>
>> Jarrod
>>
>>
>>
>>
>>
>> Quoting "m.fenati at libero.it" <m.fenati at libero.it> on Wed, 4 Jul 2012
>> 16:48:18 +0200 (CEST):
>>
>>>
>>> Dear R user,
>>> I have some problems about prior definition in MCMCglmm ordinal
>>> regression. I've tried to use what Jarrod wrote about not
>>> informative priors for ordinal probit but my model did not converge:
>>>
>>>
>>> prior=list(R=list(V= 1, fix=1), G=list(G1=list(V=1, nu=0)))
>>>
>>>
>>> where "..left the default prior for the fixed effects (not
>>> explicitly specified)..".
>>>
>>>
>>> Then, in order to have however a similar uniform distribution for
>>> the latent variable, I set prior for fixed effect  as "mu=0" and
>>> "(co)variance=100":
>>>
>>>
>>> priorB<-rnorm(1000, 0, sqrt(100))
>>> priorMB<-1:1000
>>> for(i in 1:1000){
>>>   priorMB[i]<-mean(pnorm(priorB[i]+rnorm(1000,0,sqrt(100))))
>>>    }
>>> hist(priorMB)
>>>
>>>
>>> The model converge well but I've some dobts. Is it correct or not?
>>>
>>>
>>> Thank you very much for any suggestions or comments.
>>>
>>>
>>> Best regards
>>>
>>>
>>> Massimo
>>> 	[[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>>
>>
>>
>>
>> --
>> The University of Edinburgh is a charitable body, registered in
>> Scotland, with registration number SC005336.
>>
>>
>>
>
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From j.hadfield at ed.ac.uk  Thu Jul 12 16:29:00 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Thu, 12 Jul 2012 15:29:00 +0100
Subject: [R-sig-ME] Post-test on MCMCglmm posteriors distribution
In-Reply-To: <201207111601.53136.chantepie@mnhn.fr>
References: <201207111601.53136.chantepie@mnhn.fr>
Message-ID: <20120712152900.12083w92ra50io8w@www.staffmail.ed.ac.uk>

Dear Stephane,

Do you mean posterior predictions or posterior distributions? If the  
latter, and you want to compare wether Va1 > Va2, for example, you can  
just use HPDinterval on the difference.

Cheers,

Jarrod





Quoting Stephane Chantepie <chantepie at mnhn.fr> on Wed, 11 Jul 2012  
16:01:53 +0200:

> Dear all,
>
> I have a quite simple question but I do not find a clear answer on the
> internet nor in books.
>
> The context : I am currently working on different aspects of bird aging. I
> have done animal models with MCMCglmm to find if there is an additive genetic
> variance (Va) in my sexual life traits. One animal model has been  
> done by age.
> Now, I am interested in testing the differences in Va (posterior  
> mode) between
> ages.
>
> My question : As posterior predictions are supposed to be normally  
> distributed
> (if the model converge), is it possible to use multiple Student test
> (corrected for unequal variance and pairwise multiple comparison) to test
> statiscal difference on posterior prediction? It seems to me that all
> assumptions are meet to use Student test but maybe there some problems that I
> am not thinking about.
>
> If you have a more elegant way to test differences of posterior  
> distributions,
> let me know!
>
> Thank in advance for reply and sorry for this beginner question.
>
> Stephane
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From pdalgd at gmail.com  Thu Jul 12 17:29:38 2012
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 12 Jul 2012 17:29:38 +0200
Subject: [R-sig-ME] why is there no the REML option for lme4::glmer
In-Reply-To: <C4F0189B45EA6A43903ADBB756D6F2D25175F6BD42@NDMSSCC08.ndc.nasa.gov>
References: <C4F0189B45EA6A43903ADBB756D6F2D25175F6BD42@NDMSSCC08.ndc.nasa.gov>
Message-ID: <BAEA4F94-6BEF-455A-B94F-14DBAF061A14@gmail.com>


On Jul 12, 2012, at 15:01 , Stolen, D Eric (KSC-IHA-4400)[Innovative Health Applications LLC] wrote:

> Can anyone explain to me why the REML option is not available when using lme4::glmer?  I believe the recommendation for fitting linear mixed effects models using lmer is to first determine the random effects structure using REML (with the full fixed effect structure), then to conduct LR tests (or model selection) for fixed effects structure using ML, and finally to make inferences from the final model selected fit with REML.  However, when fitting a GLMM in lem4 there is no longer any REML option.  I am confused about why this is and would really appreciate knowing some more of the theory behind it.

Well, it needs to be implemented before it can be made available; it needs to be defined before it can be implemented.

As my recollection goes (must admit that it's been a while), there are only select cases where there's an exact parallel to the Gaussian situation, i.e. a transformation of data which has a distribution depending only on the variance parameters where the model for the conditional distribution given the transformation is a saturated by the mean parameters. Various ideas exist, e.g. modified profile likelihood, but I am not aware that a clear consensus has ever emerged. The famous GLIMMIX SAS macro allowed you to fit mixed models with glm-style data by an algorithm that called PROC MIXED internally, and you could turn on its REML option, but I wonder if anyone fathomed exactly what that implied.

-Peter D.


> 
> Eric Stolen
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Peter Dalgaard, Professor
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From justiceaheto at yahoo.com  Fri Jul 13 10:41:01 2012
From: justiceaheto at yahoo.com (Justice Moses K. Aheto)
Date: Fri, 13 Jul 2012 01:41:01 -0700 (PDT)
Subject: [R-sig-ME] Help needed in simulating fitted multilevel model in R.
Message-ID: <1342168861.31918.YahooMailNeo@web113409.mail.gq1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120713/5d43a1b7/attachment.pl>

From Navinder.Singh at slu.se  Fri Jul 13 11:46:36 2012
From: Navinder.Singh at slu.se (Navinder Singh)
Date: Fri, 13 Jul 2012 11:46:36 +0200
Subject: [R-sig-ME] MCMCglmm R-Variable importance
Message-ID: <5BF6BF90ED743545A0B1FCFBE7E6DA29761EF30E5B@exmbx3.ad.slu.se>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120713/517b73e5/attachment.pl>

From amitgal4 at gmail.com  Thu Jul 12 00:37:03 2012
From: amitgal4 at gmail.com (amit gal)
Date: Wed, 11 Jul 2012 17:37:03 -0500
Subject: [R-sig-ME] coding a simple multiple membership model
Message-ID: <CAHv_4K58=FspJuTfKukFd_AGuWobB68DWS1iiSvpk_BLfs2xGA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120711/de589d80/attachment.pl>

From h.l.ward at qmul.ac.uk  Fri Jul 13 17:34:36 2012
From: h.l.ward at qmul.ac.uk (Helen Ward)
Date: Fri, 13 Jul 2012 16:34:36 +0100
Subject: [R-sig-ME] A zero inflated Poisson model in MCMCglmm
In-Reply-To: <20120712141309.10586qx02atg5hwc@www.staffmail.ed.ac.uk>
References: <4FF46034.8070203@qmul.ac.uk>
	<20120708120337.32586osul89lwkkk@www.staffmail.ed.ac.uk>
	<4FFD5C6A.80502@qmul.ac.uk>
	<20120712141309.10586qx02atg5hwc@www.staffmail.ed.ac.uk>
Message-ID: <5000400C.3060605@qmul.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120713/5075e066/attachment.pl>

From j.hadfield at ed.ac.uk  Fri Jul 13 17:50:26 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Fri, 13 Jul 2012 16:50:26 +0100
Subject: [R-sig-ME] A zero inflated Poisson model in MCMCglmm
In-Reply-To: <5000400C.3060605@qmul.ac.uk>
References: <4FF46034.8070203@qmul.ac.uk>
	<20120708120337.32586osul89lwkkk@www.staffmail.ed.ac.uk>
	<4FFD5C6A.80502@qmul.ac.uk>
	<20120712141309.10586qx02atg5hwc@www.staffmail.ed.ac.uk>
	<5000400C.3060605@qmul.ac.uk>
Message-ID: <20120713165026.17247ibtsq0cuogs@www.staffmail.ed.ac.uk>

Hi,

It will not be well behaved  - you need:

R=list(V=1,fix=1)

in the prior.

pl=TRUE goes into your MCMCglmm() call.

Parameter expansion is currently only implemented for the G-structure.

Cheers,

Jarrod



Quoting Helen Ward <h.l.ward at qmul.ac.uk> on Fri, 13 Jul 2012 16:34:36 +0100:

> Hello,
>
> Thank you for another thorough response.
>
> I've decided to stick with the ordinal model, although I can't work  
> out where to put the pl=TRUE to work out the range of latent  
> variables.
>
> I just used Vanimal/(Vanimal+Vresidual) to get heritability, rather  
> than Vanimal/(Vanimal+Vresidual+1). I have now tried the latter  
> though and the answer is very similar.
>
> Unfortunately running the model for longer and giving it a longer  
> burnin has not reduced the autocorrelation. When you say 'parameter  
> expansion', does that mean of G1 in the prior?
>
> I've carried on using this prior:
>
> prior2<-list(G=list(G1=list(V=matrix(p.var/2),n=1)),  
> R=list(V=matrix(p.var/2),n=1))
>
> in which I think the variance is set to 1.
>
> Thank you again: all help is very much appreciated.
>
> Helen
>
> On 12/07/2012 14:13, Jarrod Hadfield wrote:
>> Hi,
>>
>> Quoting Helen Ward <h.l.ward at qmul.ac.uk> on Wed, 11 Jul 2012 11:58:50 +0100:
>>
>>> Dear Jarrod and David,
>>>
>>> Thank you both very much for your responses: I'm sorry it's taken  
>>> me so long to say that, but I wanted to have a proper go at  
>>> everything before I replied, which took longer than I anticipated!
>>>
>>> Thank you especially for your zipoisson code, Jarrod. When I  
>>> initially attempted to use your prior and model MCMCglmm asked me  
>>> for a stronger prior, so I changed it to
>>>
>>>
>>> Prior1=list(R=list(V=diag(2),nu=1, fix=2), G=list(G1=G1)) : is  
>>> that a reasonable thing to have done? The model
>>>
>>>
>> I think it is better to diagnose the problem rather than upping nu.  
>> In this case I think the residual variance of the Poisson process  
>> must be going to zero under the flat prior? Perhaps the counts are  
>> under-dispersed with respect to the Poisson, further motivating an  
>> ordinal model?
>>
>>
>>
>> model1.1<-MCMCglmm(Toes~trait,random=~us(trait):animal,family="zipoisson",rcov=~idh(trait):units,pedigree=ped,data=data,prior=Prior1,nitt=500000,thin=500,burnin=200000,verbose=FALSE)
>>>
>>> ran with this prior, however I admit I am now flummoxed as to how  
>>> to proceed and go about estimating heritability.
>>>
>>>
>>> For this reason, (and because you both recommended it!), I went on  
>>> to try an ordinal model using the following code:
>>>
>>> p.var<-var(data$Toes,na.rm=TRUE)
>>>
>>> prior2<-list(G=list(G1=list(V=matrix(p.var/2),n=1)),  
>>> R=list(V=matrix(p.var/2),n=1))
>>>
>>>
>> IMPORTANT: The residual variance is not identifiable in an ordinal  
>> model so all the information is coming from the prior. This is OK,  
>> but I would fix the variance at 1 so as to avoid interpretation  
>> difficulties (and numerical problems - check the range of the  
>> latent variables using pl=TRUE).
>>
>>
>> model2<-MCMCglmm(Toes~1,random=~animal,family="ordinal",pedigree=ped,data=data,prior=prior2,nitt=500000,thin=500,burnin=200000,verbose=FALSE)
>>>
>>>
>>> followed by the same model but with the prior from the Wilson  
>>> paper erratum:
>>>
>>> prior2.1 <- list(G = list(G1 = list(V = 1, nu = 0.002)), R =  
>>> list(V = 1,nu = 0.002))
>>>
>>>
>>> the plots for these models look ok, and they both give a  
>>> heritability estimate of ~0.55, with very similar 95% HDP  
>>> intervals, although the auto correlation is quite high ~0.6
>>
>> Is this calculated as Vanimal/(Vanimal+Vresidual+1)  or  
>> Vanimal/(Vanimal+Vresidual)?
>>
>>>
>>>
>>> Next I tried a categorical model, treating Toes as a binary  
>>> response variable (deforemed or not deformed), using
>>>
>>> p.var<-var(data$ToesBinary,na.rm=TRUE)
>>>
>>> prior3= list(R = list(V = 1, n = 0, fix = 1), G = list(G1 = list(V  
>>> = 1,n = 0)))
>>>
>>> model3<-  
>>> MCMCglmm(ToesBinary~1,random=~animal,family="categorical",data=data,pedigree=ped,prior=prior3,nitt=200000,thin=250,burnin=100000,verbose =  
>>> FALSE)
>>>
>>>
>>> Heritability came out higher with this model at about ~0.85.
>>
>> Is this calculated as Vanimal/(Vanimal+Vresidual+pi^2/3)  or  
>> Vanimal/(Vanimal+Vresidual)?
>>>
>>>
>>> And then last but not least a multinomial2 model:
>>>
>>> prior4=list(R=list(V=1,nu=0.002),G=list(G1=list(V=1,nu=1,alpha.mu=0,alpha.V=100))) model4<-MCMCglmm(cbind(Deformed,Fine)~1,random=~animal,family="multinomial2",data=data,pedigree=ped,prior=prior4,nitt=200000,thin=250,burnin=100000,verbose=FALSE) The variance plots for this one didn't look very pretty and the autocorrelation values were still quite high, ~0.8, but it gave me a pretty heritability plot and an estimate similar to my ordinal  
>>> models.
>>>
>>>
>>> As you can probably gather from all of this, I am something of an  
>>> indecisive person...., but at this stage I am inlcined to stick  
>>> with the ordinal models and believe that there is a heritable  
>>> element to the number of deformed toes (which seems rather  
>>> intuitive after all that!). Given this decision, can you recommend  
>>> anything I might do to reduce the autocorrelation in these models.  
>>> Also, do I have to run the same model many times to confirm its  
>>> conclusions?
>>
>> Re: autocorrelation. Use parameter expansion and/or run for longer  
>> (particularly for binary animal models and/or if there is little  
>> overdispersion)  If the absolute value of the latent variables  
>> under the logit link exceed 20, or under the probit link exceed 7,  
>> then at the moment I would consider another program unless the  
>> latent variables are associated with particular levels of a fixed  
>> effect.  Then flatish priors on the probability scale may overcome  
>> the problem.
>>
>>
>>>
>>>
>>> Finally finally, I will have to do another zipoisson model in the  
>>> future, can you direct me to where I might find an example of the  
>>> code used for estimating heritability in these cases please?
>>
>> I think it makes most sense to think about the heritabilities of  
>> each process separately   - the first as a Poisson trait and the  
>> second as a threshold (binary) trait.
>>
>>>
>>>
>>> Many many thanks once more,
>>> Helen
>>>
>>>
>>>
>>>
>>> On 08/07/2012 12:03, Jarrod Hadfield wrote:
>>>> Hi,
>>>>
>>>> I agree with David that an ordinal model may behave better for  
>>>> this type of data (or even a 0/1 response with deformed or not)  
>>>> but in case you want to pursue a ZIP model...
>>>>
>>>> The model you have specified is probably not what you had in  
>>>> mind. You should think of the ZIP response as being two responses  
>>>> (traits): the Poisson counts (trait 1) and the probability that a  
>>>> zero comes from the zero-inflation process (trait 2). At the  
>>>> moment you have a common intercept for both and therefore assume  
>>>> that the probability that a zero is from the zero-inflation  
>>>> process (on the logit scale) is equal to the mean Poisson count  
>>>> (on the log scale). There is probably little justification for  
>>>> this (but see ZAP models). Moreover, you have a single animal  
>>>> term, and therefore assume that both processes have the same  
>>>> genetic variance and that the genetic correlation between them is  
>>>> 1. You also assume that the residual variance for each process is  
>>>> equal (but the residual correlation is 0), although this is not  
>>>> actually that bigger deal because residual variation in the  
>>>> zero-inflation process (and residual corrletaion) is not  
>>>> identifiable from the data and so the value it could take is  
>>>> essentially arbitrary. However, I like to recognise this  
>>>> non-identifiability by fixing the variance at 1 and the  
>>>> covariance at zero (but see ZAP models where your specification  
>>>> is useful).
>>>>
>>>> So, a more appropriate model would be:
>>>>
>>>> model1.1<-MCMCglmm(Toes~trait,random=~us(trait):animal,family="zipoisson",rcov=~idh(trait):units,pedigree=ped,data=data,prior=prior1.1,nitt=500000,thin=500,burnin=200000,verbose=FALSE) or  
>>>> perhaps
>>>>
>>>> model1.2<-MCMCglmm(Toes~trait,random=~idh(trait):animal,family="zipoisson",rcov=~idh(trait):units,pedigree=ped,data=data,prior=prior1.1,nitt=500000,thin=500,burnin=200000,verbose=FALSE) The difference between these two is that the genetic correlation between the zero-inflation and the Poisson processes is estimated, and in the latter it is set to  
>>>> 0.
>>>>
>>>> I'm not sure about how much data you have, but I would think that  
>>>> if the incidence of deformed toes is very low, you probably have  
>>>> little information for some of the parameters. In this case you  
>>>> have to be VERY careful with priors. The prior recommendations in  
>>>> the MCMCglmm tutorial in Wilson et al. 2011 will generally be  
>>>> quite informative for small or even moderate sized datasets (the  
>>>> authors have kindly published an erratum as many people seem to  
>>>> be using their recommendation without realising its implications).
>>>>
>>>> The prior could look like:
>>>>
>>>> prior=list(R=list(V=diag(2), nu=0, fix=2), G=list(G1=G1))
>>>>
>>>> which fixes the residual variance in the zero-inflation process  
>>>> to 1 with a flat improper prior on the Poisson over-dispersion.  
>>>> For the prior on the genetic (co)variances (G1) I generally use  
>>>> parameter expanded priors of the form:
>>>>
>>>> G1=list(V=diag(2), nu=2, alpha.mu=c(0,0), alpha.V=diag(2)*1000)
>>>>
>>>> or
>>>>
>>>> G1=list(V=diag(2), nu=1, alpha.mu=c(0,0), alpha.V=diag(2)*1000)
>>>>
>>>> for model.2.
>>>>
>>>> These generally give posterior modes for the variance components  
>>>> that are close to (RE)ML estimates, although it is unclear to me  
>>>> how they behave for the covariances and functions of variances  
>>>> (e.g. heritability). At least for binary traits (e.g. the  
>>>> zero-inflation bit) I have seen a chi-square prior recommended  
>>>> because of its  prior properties on the heritability (de  
>>>> Villemereuil in MEE). This type of prior can also be obtained  
>>>> using parameter expansion:
>>>>
>>>> (V=1, nu=1000, alpha.mu=0, alpha.V=1)
>>>>
>>>> and I would guess that the multivariate analogue would be  
>>>> (V=diag(2), nu=1001, alpha.mu=c(0,0), alpha.V=diag(2)). However,  
>>>> a chi-square prior probably has poor properties for the Poisson  
>>>> variance and in MCMCglmm you could only mix these different  
>>>> priors for model.2 using a different syntax:
>>>>
>>>> model1.2<-MCMCglmm(Toes~trait,random=~idh(at(trait,1)):animal+idh(at(trait,2)):animal,family="zipoisson",rcov=~idh(trait):units,pedigree=ped,data=data,prior=prior1.1,nitt=500000,thin=500,burnin=200000,verbose=FALSE)  
>>>> G=list(
>>>>   G1=list(V=1, nu=1, alpha.mu=0, alpha.V=1000),
>>>>   G2=list(V=1, nu=1000, alpha.mu=0, alpha.V=1)
>>>> )
>>>>
>>>>
>>>> Cheers,
>>>>
>>>> Jarrod
>>>>
>>>>
>>>>
>>>>
>>>> Quoting Helen Ward <h.l.ward at qmul.ac.uk> on Wed, 04 Jul 2012  
>>>> 16:24:36 +0100:
>>>>
>>>>> Dear list,
>>>>>
>>>>> I work on bats, some of which have deformed toes. I have just started
>>>>> trying to use MCMCglmm to investigate whether there is hertiable
>>>>> variation for the number of deformed toes a bat has. Since most bats
>>>>> have 0 deformed toes I am using a zero-inflated Poisson model.
>>>>>
>>>>> I have put together some code (see below) based on the MCMCglmm tutorial
>>>>> from the Wilson et al. 2011 'Ecologist's guide to the animal model'
>>>>> paper, the MCMCglmm Course Notes and trial and error. This code runs and
>>>>> I get a model and a heritability estimate. However, the heritability
>>>>> estimate I get is extremely high and this, teamed with the observations
>>>>> that my posterior distributions of the variance componants are not
>>>>> pretty as the example ones I've seen and my autocorrelation values are
>>>>> high (~0.4), makes me suspicious that I am not modelling this trait very
>>>>> well. I suspect I have over simplified the model. I based my prior on a
>>>>> model done in ASReml modelling 'Toes' as a Poisson distribution, which
>>>>> can an heritability estimate of 0.35.
>>>>>
>>>>> In addition, when I try and add a second random variable - year of birth
>>>>> (Born) - into a second model, both plots and auto correlation get worse
>>>>> and my heritability estimate for number of deformed toes (Toes) goes
>>>>> from about 0.8 to 0.05!
>>>>>
>>>>> Finally, I suspect you're not, but I would like to know if you are
>>>>> allowed to compare DIC values from models with different priors please.
>>>>>
>>>>> I have pasted my code below. If anyone can offer constructive criticism
>>>>> it would be much appreciated.
>>>>>
>>>>> Many thanks,
>>>>> Helen
>>>>>
>>>>> Model 1: With number of deformed toes (Toes) as the only random variable
>>>>>
>>>>>
>>>>> p.var<-var(data$Toes,na.rm=TRUE)
>>>>> prior1.1<-list(G=list(G1=list(V=matrix(p.var*0.35),n=1)),
>>>>> R=list(V=matrix(p.var*0.65),n=1))
>>>>>
>>>>> model1.1<-MCMCglmm(Toes~1,random=~animal,family="zipoisson",rcov=~trait:units,pedigree=ped,data=data,prior=prior1.1,nitt=500000,thin=500,burnin=200000,verbose=FALSE) posterior.heritability1.1<-model1.1$VCV[,"animal"]/(model1.1$VCV[,"animal"]+model1.1$VCV[,"trait:units"])  
>>>>> HPDinterval(posterior.heritability1.1,0.95)
>>>>> posterior.mode(posterior.heritability1.1)
>>>>>
>>>>>
>>>>> Model 2: With number of toes (Toes) and year of birth (Born) as random
>>>>> variables
>>>>>
>>>>>
>>>>> p.var<-var(data$Toes,na.rm=TRUE)
>>>>> prior1.2<-list(G=list(G1=list(V=matrix(p.var/3),n=1),G2=list(V=matrix(p.var/3),n=1)),R=list(V=matrix(p.var/3),n=1))  
>>>>> model1.2<-
>>>>> MCMCglmm(Toes~1,random=~animal+Born,family="zipoisson",rcov=~trait:units,pedigree=ped,data=data,prior=prior1.2,nitt=1000000,thin=1000,burnin=500000,verbose=FALSE) posterior.heritability1.2<-model1.2$VCV[,"animal"]/(model1.2$VCV[,"animal"]+model1.2$VCV[,"Born"]+model1.2$VCV[,"trait:units"])  
>>>>> HPDinterval(posterior.heritability1.2,0.95)
>>>>> posterior.mode(posterior.heritability1.2)
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>   [[alternative HTML version deleted]]
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>
>>>>>
>>>>
>>>>
>>>>
>>>
>>>
>>>
>>
>>
>>
>
>
>


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From jesper at u.washington.edu  Fri Jul 13 20:32:49 2012
From: jesper at u.washington.edu (Gus Jespersen)
Date: Fri, 13 Jul 2012 11:32:49 -0700
Subject: [R-sig-ME] Calculating fixed effect contrasts with log-transformed
	data
Message-ID: <CAL9m74a9LiZ4E5APMzY+oSk6c7TDvTGxrSrPubNu3o-vANRsTQ@mail.gmail.com>

Greetings,
I doubt this is a particularly interesting question for you mixed
model gurus, but here goes.  As you can see in the output below, I
have a model with twelve fixed effect parameters.  I am interested in
each of the "Treatment" vs. "Control" comparisons for each "site"(in
each fixed effect parameter name, these are specified by the text
immediately following "sitett"). To produce a 95% CI for such a
comparison I was advised to take two steps:

(1) Subtract the Control parameter estimate from the Treatment
parameter estimate for each site.
(2) Compute the SE for this comparison via:  sqrt( var(treatment) +
var(control) - 2*cov(treatmentt,control)).  To get these values I am
using the vcov matrix for the model.

When I move to log10-transformed data, I am thinking I should
backtransform the fixed effects and SE's before moving ahead  with the
Control-Treatment comparisons.  However, the calculations become more
problematic as ( var(treatment) + var(control) -
2*cov(treatmentt,control)) is consistently negative.  I am uncertain
on how to proceed here.  Any advice would be much appreciated.

Thank you,
Gus

Data: data.file.final
Models:
Mod.NO3.1.2: NO3Nyearone ~ 1 + (1 | pr)
Mod.NO3.1.1: NO3Nyearone ~ 1 + sitett + (1 | pr)
            Df    AIC    BIC  logLik  Chisq Chi Df Pr(>Chisq)
Mod.NO3.1.2  3 1163.5 1172.2 -578.72
Mod.NO3.1.1 14 1155.8 1196.7 -563.90 29.637     11   0.001806 **
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
Linear mixed model fit by REML
Formula: NO3Nyearone ~ 1 + sitett + (1 | pr)
   Data: data.file.final
  AIC  BIC logLik deviance REMLdev
 1098 1139 -534.8     1128    1070
Random effects:
 Groups   Name        Variance Std.Dev.
 pr       (Intercept)  33.348   5.7747
 Residual             210.115  14.4954
Number of obs: 137, groups: pr, 72

Fixed effects:
                             Estimate Std. Error t value
(Intercept)                    20.118      4.701   4.280
sitettLepAddition Treatment     3.032      6.069   0.500
sitettMossAddition Control      5.677      6.809   0.834
sitettMossAddition Treatment    9.418      6.648   1.417
sitettMossRemoval Control      -9.951      6.510  -1.529
sitettMossRemoval Treatment    -9.601      6.510  -1.475
sitettSaddle Control          -10.985      6.510  -1.687
sitettSaddle Treatment        -12.269      6.648  -1.846
sitettToeAdditions Control      0.932      6.510   0.143
sitettToeAdditions Treatment  -11.678      6.809  -1.715
sitettToeRemoval Control      -12.351      6.510  -1.897
sitettToeRemoval Treatment    -13.168      6.510  -2.023



-- 
R. Gus Jespersen
PhD Candidate
College of Forest Resources
University of Washington
Box 352100
Seattle, WA 98195-2100
(206) 543-5777
jesper at u.washington.edu


From highstat at highstat.com  Sat Jul 14 10:46:59 2012
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Sat, 14 Jul 2012 09:46:59 +0100
Subject: [R-sig-ME] Calculating fixed effect contrasts with
	log-transformed data
Message-ID: <50013203.1000107@highstat.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120714/9ca7d1ef/attachment.pl>

From bbolker at gmail.com  Mon Jul 16 04:29:41 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 16 Jul 2012 02:29:41 +0000 (UTC)
Subject: [R-sig-ME] P values in summary tables from lmer objects
References: <4FFEBE80.8030601@agr.uni-goettingen.de>
	<WC20120712130300.120172@cebc.cnrs.fr>
Message-ID: <loom.20120716T042820-358@post.gmane.org>

lborger <lborger at ...> writes:

> >It seems that the newest version of lme4 (0.999999-0) now provides P values 
> in the summary tables.
> 
> Nope, they were only removed for Gaussian models, and still are, try:

 [snip]
 
> whereas they were never removed for binomial/poisson GLMMs.
> 

  Also note that the characteristics of these p-values (based on
quadratic, asymptotic assumptions etc.) is  (the first topic)
covered in http://glmm.wikidot.com/faq ...


From Thierry.ONKELINX at inbo.be  Mon Jul 16 10:53:35 2012
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Mon, 16 Jul 2012 08:53:35 +0000
Subject: [R-sig-ME] Calculating fixed effect contrasts with
 log-transformed	data
In-Reply-To: <CAL9m74a9LiZ4E5APMzY+oSk6c7TDvTGxrSrPubNu3o-vANRsTQ@mail.gmail.com>
References: <CAL9m74a9LiZ4E5APMzY+oSk6c7TDvTGxrSrPubNu3o-vANRsTQ@mail.gmail.com>
Message-ID: <AA818EAD2576BC488B4F623941DA74275D67BD17@inbomail.inbo.be>

Dear Gus,

Have a look at glht() from the multcomp package. It allows you to define the contrasts that you are interested in.

Best regards,

Thierry

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Gus Jespersen
Verzonden: vrijdag 13 juli 2012 20:33
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] Calculating fixed effect contrasts with log-transformed data

Greetings,
I doubt this is a particularly interesting question for you mixed model gurus, but here goes.  As you can see in the output below, I have a model with twelve fixed effect parameters.  I am interested in each of the "Treatment" vs. "Control" comparisons for each "site"(in each fixed effect parameter name, these are specified by the text immediately following "sitett"). To produce a 95% CI for such a comparison I was advised to take two steps:

(1) Subtract the Control parameter estimate from the Treatment parameter estimate for each site.
(2) Compute the SE for this comparison via:  sqrt( var(treatment) +
var(control) - 2*cov(treatmentt,control)).  To get these values I am using the vcov matrix for the model.

When I move to log10-transformed data, I am thinking I should backtransform the fixed effects and SE's before moving ahead  with the Control-Treatment comparisons.  However, the calculations become more problematic as ( var(treatment) + var(control) -
2*cov(treatmentt,control)) is consistently negative.  I am uncertain on how to proceed here.  Any advice would be much appreciated.

Thank you,
Gus

Data: data.file.final
Models:
Mod.NO3.1.2: NO3Nyearone ~ 1 + (1 | pr)
Mod.NO3.1.1: NO3Nyearone ~ 1 + sitett + (1 | pr)
            Df    AIC    BIC  logLik  Chisq Chi Df Pr(>Chisq)
Mod.NO3.1.2  3 1163.5 1172.2 -578.72
Mod.NO3.1.1 14 1155.8 1196.7 -563.90 29.637     11   0.001806 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Linear mixed model fit by REML
Formula: NO3Nyearone ~ 1 + sitett + (1 | pr)
   Data: data.file.final
  AIC  BIC logLik deviance REMLdev
 1098 1139 -534.8     1128    1070
Random effects:
 Groups   Name        Variance Std.Dev.
 pr       (Intercept)  33.348   5.7747
 Residual             210.115  14.4954
Number of obs: 137, groups: pr, 72

Fixed effects:
                             Estimate Std. Error t value
(Intercept)                    20.118      4.701   4.280
sitettLepAddition Treatment     3.032      6.069   0.500
sitettMossAddition Control      5.677      6.809   0.834
sitettMossAddition Treatment    9.418      6.648   1.417
sitettMossRemoval Control      -9.951      6.510  -1.529
sitettMossRemoval Treatment    -9.601      6.510  -1.475
sitettSaddle Control          -10.985      6.510  -1.687
sitettSaddle Treatment        -12.269      6.648  -1.846
sitettToeAdditions Control      0.932      6.510   0.143
sitettToeAdditions Treatment  -11.678      6.809  -1.715
sitettToeRemoval Control      -12.351      6.510  -1.897
sitettToeRemoval Treatment    -13.168      6.510  -2.023



--
R. Gus Jespersen
PhD Candidate
College of Forest Resources
University of Washington
Box 352100
Seattle, WA 98195-2100
(206) 543-5777
jesper at u.washington.edu

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From m.fenati at libero.it  Mon Jul 16 11:04:45 2012
From: m.fenati at libero.it (m.fenati at libero.it)
Date: Mon, 16 Jul 2012 11:04:45 +0200 (CEST)
Subject: [R-sig-ME] MCMCglmm: priors for ordinal regression
Message-ID: <2483687.426621342429485062.JavaMail.defaultUser@defaultHost>


Thank you again for your great suggestions.

Regards

Massimo



>----Messaggio originale----
>Da: j.hadfield at ed.ac.uk
>Data: 12/07/2012 15.20
>A: "m.fenati at libero.it"<m.fenati at libero.it>
>Cc: <r-sig-mixed-models at r-project.org>
>Ogg: Re: [R-sig-ME] MCMCglmm: priors for ordinal regression
>
>Hi,
>
>If the prior variance on your fixed effects is V+1 where V is the sum  
>of the variance components (including the residual) then the marginal  
>prior on the fixed effects is as flat as possible on the probability  
>interval (0,1). However, you have to set up the contrasts correctly.
>
>If you still get numerical problems I'm afraid you will have to find  
>another way of doing the analysis. I have no solution, and no one has  
>suggested any:
>
>https://stat.ethz.ch/pipermail/r-sig-mixed-models/2012q1/017976.html
>
>Cheers,
>
>Jarrod
>
>
>
>Quoting "m.fenati at libero.it" <m.fenati at libero.it> on Mon, 9 Jul 2012  
>12:44:59 +0200 (CEST):
>
>> Dear Jarrod,
>> thank you for your fast answer.
>> Yes, I had converegence (presence of trend of the time series).  
>> Unfortunately,
>> I have ordinal data with near complete separation.
>> My aim is to set a poorly informative or uninformative priors for  
>> fixed effect
>> in order to improve the chain convergence. Then I set  
>> piorB=list(mu=c(rep(0,6)),
>> V=diag(6)*(100)). The choice of V=100 is not based on other logical or
>> numerical reasons.
>> I try to display the posterior distribution of latent variable (pl=T), but 
I
>> had a wide range of -25 + 25.....
>> How can I do? Could you help me to choose the right prior?
>>
>> Thank in advance
>>
>> Massimo
>>
>>
>>
>>> ----Messaggio originale----
>>> Da: j.hadfield at ed.ac.uk
>>> Data: 08/07/2012 12.20
>>> A: "m.fenati at libero.it"<m.fenati at libero.it>
>>> Cc: <r-sig-mixed-models at r-project.org>
>>> Ogg: Re: [R-sig-ME] MCMCglmm: priors for ordinal regression
>>>
>>> Dear Massimo,
>>>
>>> Do you mean the chain did not converge or the chain did not mix?
>>> Generally the former is rare, and is usually only seen with
>>> ordinal/categorical data with complete (or near complete) separation.
>>> Sometimes a prior that constrains the linear predictor away from
>>> extreme values on the logit/probit scale can fix this with a
>>> relatively minor prior influence on inferences made on the data scale.
>>> Sometimes not. Its not clear to me what the motivation is behind your
>>> prior - is it that the sum of your variance components is close to
>>> 100? If so I would be careful. Use pl=TRUE in your call to MCMCglmm
>>> and make sure your latent variables are in the range -7 to 7.
>>>
>>> Cheers,
>>>
>>> Jarrod
>>>
>>>
>>>
>>>
>>>
>>> Quoting "m.fenati at libero.it" <m.fenati at libero.it> on Wed, 4 Jul 2012
>>> 16:48:18 +0200 (CEST):
>>>
>>>>
>>>> Dear R user,
>>>> I have some problems about prior definition in MCMCglmm ordinal
>>>> regression. I've tried to use what Jarrod wrote about not
>>>> informative priors for ordinal probit but my model did not converge:
>>>>
>>>>
>>>> prior=list(R=list(V= 1, fix=1), G=list(G1=list(V=1, nu=0)))
>>>>
>>>>
>>>> where "..left the default prior for the fixed effects (not
>>>> explicitly specified)..".
>>>>
>>>>
>>>> Then, in order to have however a similar uniform distribution for
>>>> the latent variable, I set prior for fixed effect  as "mu=0" and
>>>> "(co)variance=100":
>>>>
>>>>
>>>> priorB<-rnorm(1000, 0, sqrt(100))
>>>> priorMB<-1:1000
>>>> for(i in 1:1000){
>>>>   priorMB[i]<-mean(pnorm(priorB[i]+rnorm(1000,0,sqrt(100))))
>>>>    }
>>>> hist(priorMB)
>>>>
>>>>
>>>> The model converge well but I've some dobts. Is it correct or not?
>>>>
>>>>
>>>> Thank you very much for any suggestions or comments.
>>>>
>>>>
>>>> Best regards
>>>>
>>>>
>>>> Massimo
>>>> 	[[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>>
>>>
>>>
>>>
>>> --
>>> The University of Edinburgh is a charitable body, registered in
>>> Scotland, with registration number SC005336.
>>>
>>>
>>>
>>
>>
>>
>
>
>
>-- 
>The University of Edinburgh is a charitable body, registered in
>Scotland, with registration number SC005336.
>
>
>


From juliekern27 at googlemail.com  Mon Jul 16 20:47:00 2012
From: juliekern27 at googlemail.com (Julie Kern)
Date: Mon, 16 Jul 2012 19:47:00 +0100
Subject: [R-sig-ME] Question: incorporating paired data into mixed models
Message-ID: <CANJi_csjiNG=0F5oxgf6FLq-degqNtN3YD7wgo95e==t=HtobA@mail.gmail.com>

Dear R gurus,

Apologies in advance if this is very simple, I?m new to R.  I?m
analysing playback data which is paired but also contains repeated
measures. Playbacks involved 2 calls, one the experimental treatment
(alarm call), one the control treatment (contact call), both of the
same individual. Each of 4 groups received 6 playback pairs of the
alarm & contact call but none of the callers were used more than once
(i.e. 1 pair). Here?s a data sample:

Treatment	        Caller.ID	Group	Latency
Control	        AM001	A	580
Experimental	AM001	A	45
Experimental	BM001	B	3
Control	        BM001	B	91
Control	        CF001	C	600
Experimental	CF001	C	22
Experimental	DM004	D	130
Control	        DM004	D	139
Experimental	AF001	A	14
Control	        AF001	A	384

At the moment I?m using group as a random term in a mixed model but
can?t work out how to specify the paired design. i.e. does my code
just test all the experimental vs all the control, rather than testing
the difference between each specific pair?
Model<-lme (Latency~Treatment, random=~1|Group, data=mydata, method="ML")

Does it work if I add pair as a random factor?

I?ve searched through the mailing lists but can?t seem to find an answer.

Many thanks for your help!

Julie


From helixed2 at yahoo.com  Mon Jul 16 22:24:16 2012
From: helixed2 at yahoo.com (Jeremy Koster)
Date: Mon, 16 Jul 2012 13:24:16 -0700 (PDT)
Subject: [R-sig-ME] In simple terms,
	how is the estimated variance of higher-level effects calculated?
Message-ID: <1342470256.38391.YahooMailNeo@web161805.mail.bf1.yahoo.com>

I'm teaching some grad students about mixed-effects modeling. ?To their credit, they're paying close attention and asking good questions.

Today, we were talking about variance components in a basic two-level binomial glmer with no fixed effects. ?The output includes these estimates:


Random effects:
?Groups? Name? ? ? ? Variance Std.Dev.
?Subject (Intercept) 0.93537? 0.96715?

They know about standard deviations and variances from their intro class, so when I showed them the dotplot (caterpillar plot) of the estimated intercepts for each of the higher-level subjects, they wondered if the standard deviation above was simply the sd of those varying intercepts. ?Well, I acknowledged that the estimated variance was going to reflect the dispersion in those estimates, but that there is no doubt some extra stuff going on behind the scenes, particularly since the estimated ranef intercepts themselves vary in their precision, often relating to different numbers of observations for each subject (they didn't fully believe me and tried it anyway, only to confirm my suspicion).

So if one were to describe in simple terms how lme4 generates a number for the estimated variance of the random effects, what might be said?


From David.Duffy at qimr.edu.au  Tue Jul 17 00:43:03 2012
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Tue, 17 Jul 2012 08:43:03 +1000
Subject: [R-sig-ME] In simple terms,
	how is the estimated variance of higher-level effects calculated?
In-Reply-To: <1342470256.38391.YahooMailNeo@web161805.mail.bf1.yahoo.com>
References: <1342470256.38391.YahooMailNeo@web161805.mail.bf1.yahoo.com>
Message-ID: <Pine.LNX.4.64.1207170759510.24433@orpheus.qimr.edu.au>

On Mon, 16 Jul 2012, Jeremy Koster wrote:

> I'm teaching some grad students about mixed-effects modeling. To their 
> credit, they're paying close attention and asking good questions.
>
> Today, we were talking about variance components in a basic two-level 
> binomial glmer with no fixed effects.
[...]
> So if one were to describe in simple terms how lme4 generates a number 
> for the estimated variance of the random effects, what might be said?

I think conceptualizing it as a latent variable model helps.  Since the 
latent variables are unobserved, we make inferences about their 
distribution based upon the distribution of the manifest variables and our 
assumptions about the nature of the latent variable distribution.

Different assumed latent variable distributions eg beta, normal, mixtures 
- and different link functions eg logit, probit, log, identity - will 
change not only your variance estimates, but your interpretation.

One useful exercise might be to simulate binary data from a threshold 
model, and demonstrate how it is that the variances of the (known) latent 
variables are estimated (in a probit-normal model), and how the 
tetrachoric correlation, Pearson correlation and odds ratio for a 2x2 
table vary by marginal probabilities and association strength.

You might also compare different models for this "classic"
boric acid teratogenicity dataset:

http://genepi.qimr.edu.au/staff/davidD/Sib-pair/Documents/Using_Sib-pair/Scripts/boricex.in

A final example might be to look at the commonly used approach of fitting 
a LMM to binary data coded as 1's and 0's (going back to Cochrane 1943), 
and whether results are deceptive or not.  In analysis of Genome Wide 
Association Scan data for a binary phenotype Y, we test the (fixed) effect 
of each measured polymorphism X (usually scored as 0,1,2) against Y, but 
we need to adjust for confounding due to unobserved relatedness of 
individuals in the study. The latter is estimated as an NxN empirical 
kinship matrix (the average pairwise correlation over M polymorphisms 
between N study participants, with M=2000000 to 5000000, and N = 1000 to 
100000).  When Y is continuous, a LMM is a very attractive approach...

-- 
| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v


From jackson.king722 at gmail.com  Tue Jul 17 00:54:31 2012
From: jackson.king722 at gmail.com (Jackson King)
Date: Mon, 16 Jul 2012 17:54:31 -0500
Subject: [R-sig-ME] MCMCglmm question
Message-ID: <CABHLxr=vFQWa+f2BfFtApoUTEqLJxzVyeJ21i1oY-et0Afxn6g@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120716/bf65ed53/attachment.pl>

From David.Duffy at qimr.edu.au  Tue Jul 17 02:09:13 2012
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Tue, 17 Jul 2012 10:09:13 +1000
Subject: [R-sig-ME] MCMCglmm question
In-Reply-To: <CABHLxr=vFQWa+f2BfFtApoUTEqLJxzVyeJ21i1oY-et0Afxn6g@mail.gmail.com>
References: <CABHLxr=vFQWa+f2BfFtApoUTEqLJxzVyeJ21i1oY-et0Afxn6g@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.1207170959230.24433@orpheus.qimr.edu.au>

On Mon, 16 Jul 2012, Jackson King wrote:

> Hello,
>
> I am trying to fit a simple ACE genetics model with twin data, but am
> having difficulties implementing it in MCMCglmm. The data consist of both
> MZ and DZ twin pairs. The MZ twin variance can be decomposed into ACE;  the
> DZ variance can be partitioned into CE and twin/family specific variance
> and unique genetic variance. I am unable to retrieve these components after
> running a model where unit and twin level variance components are
> estimated. Any thoughts on how to do this? Do I fix the DZ genetic variance
> to be half of the MZ in the prior?

You want to put the inverse A and C matrices into ginverse (see 
?MCMCglmm), though I think C could just be coded as a household effect in 
random so that the appropriate covariance matrix will be generated for 
you.


From j.hadfield at ed.ac.uk  Tue Jul 17 11:27:34 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Tue, 17 Jul 2012 10:27:34 +0100
Subject: [R-sig-ME] MCMCglmm question
In-Reply-To: <CABHLxr=vFQWa+f2BfFtApoUTEqLJxzVyeJ21i1oY-et0Afxn6g@mail.gmail.com>
References: <CABHLxr=vFQWa+f2BfFtApoUTEqLJxzVyeJ21i1oY-et0Afxn6g@mail.gmail.com>
Message-ID: <20120717102734.127936bkhblsyrk0@www.staffmail.ed.ac.uk>

Hi Jackson,

You can set up the appropriate constraint by fitting the model as an  
animal model. For MZ twins you have to give them the same identifier  
in the pedigree though.  For example, if the data (my.data) are like  
this:

animal  individual twin  family
A           A       DZ     A
B           B       DZ     A
C           C       MZ     B
C           D       MZ     B
D           E       DZ     C
E           F       DZ     C

and the parents unknown:

ped<-cbind(my.data$animal, paste("mum", my.data$family, sep="."),  
paste("dad", my.data$family, sep="."))

ped<-ped[which(!duplicated(ped[,1])),]

ped<-insertPed(ped)  # this parental records into the pedigree  
(insertPed is in the package MasterBayes)

The analysis is then

Ainv<-inverseA(ped)$Ainv

m1<-MCMCglmm(y~1, random=animal+family, ginverse=list(animal=Ainv),  
data=my.data)

where the animal variance is Va  and the family variance is Vce.

If your data do have no structure (i.e just sets of outbred unrelated  
twins) then the above is quite an inefficient set up. A better option  
is to derive Ainv without the phantom parents:


n<-nlevels(my.data$animal)
nDZ<-sum(my.data$twin=="DZ")  # makes sure DZ is level 1 and MZ is level 2

Ainv<-bandSparse(n, n, 0:1,diagonal=list(c(rep(4/3,nDZ), rep(1,  
n-nDZ)), c(rep(c(-2/3,0), nDZ/2),rep(0, n-nDZ))), symmetric=TRUE)

rownames(Ainv)<-levels(my.data$animal)[order(my.data$twin[which(!duplicated(my.data$animal))])]

This should be quicker.


Cheers,

Jarrod









Quoting Jackson King <jackson.king722 at gmail.com> on Mon, 16 Jul 2012  
17:54:31 -0500:

> Hello,
>
> I am trying to fit a simple ACE genetics model with twin data, but am
> having difficulties implementing it in MCMCglmm. The data consist of both
> MZ and DZ twin pairs. The MZ twin variance can be decomposed into ACE;  the
> DZ variance can be partitioned into CE and twin/family specific variance
> and unique genetic variance. I am unable to retrieve these components after
> running a model where unit and twin level variance components are
> estimated. Any thoughts on how to do this? Do I fix the DZ genetic variance
> to be half of the MZ in the prior?
>
> Many thanks,
> Jackson
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From Beatriz.DeFrancisco at sams.ac.uk  Tue Jul 17 14:28:07 2012
From: Beatriz.DeFrancisco at sams.ac.uk (Beatriz De Francisco)
Date: Tue, 17 Jul 2012 12:28:07 +0000
Subject: [R-sig-ME] glsControl(optimMethod = "L-BFGS-B"),
	: false convergence (8) ERROR
Message-ID: <6B2C4C7276688144852E9630133C76BA667B22E1@Verbiage2.sams.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120717/649a53be/attachment.pl>

From bbolker at gmail.com  Tue Jul 17 15:44:30 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 17 Jul 2012 13:44:30 +0000 (UTC)
Subject: [R-sig-ME] Question: incorporating paired data into mixed models
References: <CANJi_csjiNG=0F5oxgf6FLq-degqNtN3YD7wgo95e==t=HtobA@mail.gmail.com>
Message-ID: <loom.20120717T154011-958@post.gmane.org>

Julie Kern <juliekern27 at ...> writes:

> Apologies in advance if this is very simple, I?m new to R.  I?m
> analysing playback data which is paired but also contains repeated
> measures. Playbacks involved 2 calls, one the experimental treatment
> (alarm call), one the control treatment (contact call), both of the
> same individual. Each of 4 groups received 6 playback pairs of the
> alarm & contact call but none of the callers were used more than once
> (i.e. 1 pair). Here?s a data sample:

  [snip]
 
> Model<-lme (Latency~Treatment, random=~1|Group, data=mydata, method="ML")
> 
> Does it work if I add pair as a random factor?

Yes, you should add pair (which seems to be Caller.ID in your
data set), as a random factor.  I think you can do this with
a nesting statement as follows, even though the pairs are uniquely
coded:

Model<-lme (Latency~Treatment, random=~1|Group/Caller.ID, 
     data=mydata, method="ML")


From roby.joehanes at nih.gov  Tue Jul 17 17:08:34 2012
From: roby.joehanes at nih.gov (Joehanes, Roby (NIH/NHLBI) [F])
Date: Tue, 17 Jul 2012 11:08:34 -0400
Subject: [R-sig-ME] patch for pedigremm
In-Reply-To: <840AF959BC19ED47A774DB45D9CAEC1302889F@uqexmdb7.soe.uq.edu.au>
Message-ID: <CC2AF832.2760%joehanesr@mail.nih.gov>

Hi David:

I apologize for the late reply.

The patch is meant to be implemented at the source code, not at R console. That is, you need to download the source from the code tracker and retrieve the correct revision number. Then, you can apply the patch. You will need to patch both your lme4 and your pedigreemm to make it work. I have a pre-patched versions, let me know if you want them. Note that if you install my version of lme4 and pedigreemm, your version of these packages will be overwritten. Also, I only tested them in Linux. I am not aware of whether they would work on Windows or Mac.

What the patch is doing is actually to enable 1 observation per subject, which otherwise not available in the regular pedigreemm. In addition, I incorporated the latest advances of lme4 as the pedigreemm's back end.

That being said, if the matrix A you mentioned is the relationship matrix, it should always be square regardless of the length of the IDs (or the discrepancies thereof). In pedigreemm, the matrix A will be constructed automatically from the pedigree (i.e., Ped.0) in a function called "relfactor". This same function will subset the matrix A automatically to match the IDs that appear in your data. So, there is no need of extra work or even my patch for this type of problem (i.e., length(dat$id) != length(id)). The way you specified the ID discrepancies (i.e., length(dat$id) = dim(A)[1] & length(id) = dim(A)[2]) is a bit unclear to me, and I take it to mean as (length(dat$id) != length(id)).

Looking at the error message that you presented, however, it seems that there is one ID per observation (i.e., length(unique(dat$id)) == nrow(dat)). That is the use case scenario where you would need my patch.

Hope this helps,
Roby

On 6/26/12 11:50 PM, "David Aguirre-Davies" <d.aguirre at uq.edu.au> wrote:


Hi Roby,

Sorry for my incompetence, but running patches on packages is beyond my R knowledge at this stage. I would like to run the following model using pedigreemm and I get the error message below,

> fm1 <-pedigreemm(value.p ~ gen + (1|id), data=dat, pedigree = list(id=Ped.0))
Error in function (fr, FL, start, REML, verbose)  :
  Number of levels of a grouping factor for the random effects
must be less than the number of observations

I have followed the clues on the web, and I have attempted to implement the patch you have provide (https://r-forge.r-project.org/tracker/index.php?func=detail&aid=1928&group_id=60&atid=300), but unfortunately I have been unsuccessful.

So my question is, how can I implement the patch you provide to run an animal model where length(dat$id) = dim(A)[1] & length(id) = dim(A)[2]?

Is there are trick to implementing the patch as cut and paste into the R console doesn?t seem to work for me.

Any assistance would be much appreciated.

Regards
David Aguirre


From helixed2 at yahoo.com  Tue Jul 17 19:13:48 2012
From: helixed2 at yahoo.com (Jeremy Koster)
Date: Tue, 17 Jul 2012 10:13:48 -0700 (PDT)
Subject: [R-sig-ME] In simple terms,
	how is the estimated variance of higher-level effects calculated?
In-Reply-To: <Pine.LNX.4.64.1207170759510.24433@orpheus.qimr.edu.au>
References: <1342470256.38391.YahooMailNeo@web161805.mail.bf1.yahoo.com>
	<Pine.LNX.4.64.1207170759510.24433@orpheus.qimr.edu.au>
Message-ID: <1342545228.84174.YahooMailNeo@web161806.mail.bf1.yahoo.com>

Thanks for the feedback, David. ?Owing to my lack of expertise, I confess that I didn't completely follow everything, but your email inspired me to explore the varying intercepts with different exponential families, focusing particularly on the gaussian.

In short, my students were looking at the estimated varying intercepts for each higher-level group (or the "BLUP's", as some people seem to call them) -- the intercepts that one can see by entering:

ranef (fm1)

Their suggestion was that, if one calculates the variance of that vector of estimated intercepts, then one should get the number that gets reported by lme4 for the higher-level variance (for example,?0.93537 in my earlier email).

We did this for several two-level models, and although the variance we calculate from the group-specific intercepts is always in the same ballpark as the lme4 output, it's never on target.

So I've been scouring all the resources I have at hand: Gelman and Hill, Ben Bolker's book, Snijders and Bosker, etc.

Most of these seem to say: "And this is the variance estimate and how to interpret it" . . . without going into non-technical details about how exactly it's calculated.  That's where we're stuck.  I know that the intercepts are calculated via partial pooling and a shrinkage factor, but it's not clear how that relates to the estimated variance.




----- Original Message -----
From: David Duffy <David.Duffy at qimr.edu.au>
To: Jeremy Koster <helixed2 at yahoo.com>
Cc: r-sig-mixed-models at r-project.org
Sent: Monday, July 16, 2012 6:43 PM
Subject: Re: [R-sig-ME] In simple terms,how is the estimated variance of higher-level effects calculated?

On Mon, 16 Jul 2012, Jeremy Koster wrote:

> I'm teaching some grad students about mixed-effects modeling. To their credit, they're paying close attention and asking good questions.
> 
> Today, we were talking about variance components in a basic two-level binomial glmer with no fixed effects.
[...]
> So if one were to describe in simple terms how lme4 generates a number for the estimated variance of the random effects, what might be said?

I think conceptualizing it as a latent variable model helps.? Since the latent variables are unobserved, we make inferences about their distribution based upon the distribution of the manifest variables and our assumptions about the nature of the latent variable distribution.

Different assumed latent variable distributions eg beta, normal, mixtures - and different link functions eg logit, probit, log, identity - will change not only your variance estimates, but your interpretation.

One useful exercise might be to simulate binary data from a threshold model, and demonstrate how it is that the variances of the (known) latent variables are estimated (in a probit-normal model), and how the tetrachoric correlation, Pearson correlation and odds ratio for a 2x2 table vary by marginal probabilities and association strength.

You might also compare different models for this "classic"
boric acid teratogenicity dataset:

http://genepi.qimr.edu.au/staff/davidD/Sib-pair/Documents/Using_Sib-pair/Scripts/boricex.in

A final example might be to look at the commonly used approach of fitting a LMM to binary data coded as 1's and 0's (going back to Cochrane 1943), and whether results are deceptive or not.? In analysis of Genome Wide Association Scan data for a binary phenotype Y, we test the (fixed) effect of each measured polymorphism X (usually scored as 0,1,2) against Y, but we need to adjust for confounding due to unobserved relatedness of individuals in the study. The latter is estimated as an NxN empirical kinship matrix (the average pairwise correlation over M polymorphisms between N study participants, with M=2000000 to 5000000, and N = 1000 to 100000).? When Y is continuous, a LMM is a very attractive approach...

-- | David Duffy (MBBS PhD)? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?  ,-_|\
| email: davidD at qimr.edu.au? ph: INT+61+7+3362-0217 fax: -0101? /? ?  *
| Epidemiology Unit, Queensland Institute of Medical Research?  \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia? GPG 4D0B994A v



From bates at stat.wisc.edu  Tue Jul 17 20:55:40 2012
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 17 Jul 2012 13:55:40 -0500
Subject: [R-sig-ME] In simple terms,
 how is the estimated variance of higher-level effects calculated?
In-Reply-To: <1342545228.84174.YahooMailNeo@web161806.mail.bf1.yahoo.com>
References: <1342470256.38391.YahooMailNeo@web161805.mail.bf1.yahoo.com>
	<Pine.LNX.4.64.1207170759510.24433@orpheus.qimr.edu.au>
	<1342545228.84174.YahooMailNeo@web161806.mail.bf1.yahoo.com>
Message-ID: <CAO7JsnSmAoF2rP_nM8cFvFo+vVdSOtxxNEv+2BCkQH=19HiZ2A@mail.gmail.com>

On Tue, Jul 17, 2012 at 12:13 PM, Jeremy Koster <helixed2 at yahoo.com> wrote:
> Thanks for the feedback, David.  Owing to my lack of expertise, I confess that I didn't completely follow everything, but your email inspired me to explore the varying intercepts with different exponential families, focusing particularly on the gaussian.
>
> In short, my students were looking at the estimated varying intercepts for each higher-level group (or the "BLUP's", as some people seem to call them) -- the intercepts that one can see by entering:

As Alan James once said, "these values are just like the BLUPs - Best
Linear Unbiased Predictors - except that they aren't linear and they
aren't unbiased and there is no clear sense in which they are "best",
but other than that ..."

The way I think of the model there are two vector-valued random
variables, the response, Y, and the random-effects vector, B.  The
model defines the conditional distribution of Y, given B, through a
linear predictor expression, a link function and a distribution.  It
also defines the unconditional distribution of B as multivariate
Gaussian with mean 0 and a parameterized variance-covariance matrix,
Sigma.  Given these two distributions we can evaluate the joint
distribution and the conditional distribution of B, given Y.  In
practice it is tricky to evaluate the conditional distribution of B
given the observed value of Y because you need to integrate the
unscaled conditional density.  However, you can find the conditional
mode, which is the value of B that maximizes the unscaled conditional
density, without needing to integrate.  It is these conditional modes
that are returned by ranef().

> ranef (fm1)
>
> Their suggestion was that, if one calculates the variance of that vector of estimated intercepts, then one should get the number that gets reported by lme4 for the higher-level variance (for example, 0.93537 in my earlier email).
>
> We did this for several two-level models, and although the variance we calculate from the group-specific intercepts is always in the same ballpark as the lme4 output, it's never on target.

Generally you will find that the variance of the values returned by
ranef is less than the estimated variance because of shrinkage.

> So I've been scouring all the resources I have at hand: Gelman and Hill, Ben Bolker's book, Snijders and Bosker, etc.
>
> Most of these seem to say: "And this is the variance estimate and how to interpret it" . . . without going into non-technical details about how exactly it's calculated.  That's where we're stuck.  I know that the intercepts are calculated via partial pooling and a shrinkage factor, but it's not clear how that relates to the estimated variance.


>
>
>
> ----- Original Message -----
> From: David Duffy <David.Duffy at qimr.edu.au>
> To: Jeremy Koster <helixed2 at yahoo.com>
> Cc: r-sig-mixed-models at r-project.org
> Sent: Monday, July 16, 2012 6:43 PM
> Subject: Re: [R-sig-ME] In simple terms,how is the estimated variance of higher-level effects calculated?
>
> On Mon, 16 Jul 2012, Jeremy Koster wrote:
>
>> I'm teaching some grad students about mixed-effects modeling. To their credit, they're paying close attention and asking good questions.
>>
>> Today, we were talking about variance components in a basic two-level binomial glmer with no fixed effects.
> [...]
>> So if one were to describe in simple terms how lme4 generates a number for the estimated variance of the random effects, what might be said?
>
> I think conceptualizing it as a latent variable model helps.  Since the latent variables are unobserved, we make inferences about their distribution based upon the distribution of the manifest variables and our assumptions about the nature of the latent variable distribution.
>
> Different assumed latent variable distributions eg beta, normal, mixtures - and different link functions eg logit, probit, log, identity - will change not only your variance estimates, but your interpretation.
>
> One useful exercise might be to simulate binary data from a threshold model, and demonstrate how it is that the variances of the (known) latent variables are estimated (in a probit-normal model), and how the tetrachoric correlation, Pearson correlation and odds ratio for a 2x2 table vary by marginal probabilities and association strength.
>
> You might also compare different models for this "classic"
> boric acid teratogenicity dataset:
>
> http://genepi.qimr.edu.au/staff/davidD/Sib-pair/Documents/Using_Sib-pair/Scripts/boricex.in
>
> A final example might be to look at the commonly used approach of fitting a LMM to binary data coded as 1's and 0's (going back to Cochrane 1943), and whether results are deceptive or not.  In analysis of Genome Wide Association Scan data for a binary phenotype Y, we test the (fixed) effect of each measured polymorphism X (usually scored as 0,1,2) against Y, but we need to adjust for confounding due to unobserved relatedness of individuals in the study. The latter is estimated as an NxN empirical kinship matrix (the average pairwise correlation over M polymorphisms between N study participants, with M=2000000 to 5000000, and N = 1000 to 100000).  When Y is continuous, a LMM is a very attractive approach...
>
> -- | David Duffy (MBBS PhD)                                         ,-_|\
> | email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
> | Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
> | 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v
>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From benton at stat.berkeley.edu  Tue Jul 17 21:01:04 2012
From: benton at stat.berkeley.edu (benton)
Date: Tue, 17 Jul 2012 15:01:04 -0400
Subject: [R-sig-ME] predicted mean from GLMM lower than mean from GAM
Message-ID: <5005B670.5000905@stat.berkeley.edu>

Hi:

I fit a Poisson GLMM with only the intercept and two random effects, and 
the predicted mean was 1.14. When I fit a generalized additive model 
(GAM) with only the intercept, the predicted mean was 1.6. Does anyone 
know why this is happening? I'm looking for a theoretical response, as 
I've checked my code and there are no errors.

Thanks!

Katie Benton


From jesper at u.washington.edu  Tue Jul 17 21:02:42 2012
From: jesper at u.washington.edu (Gus Jespersen)
Date: Tue, 17 Jul 2012 12:02:42 -0700
Subject: [R-sig-ME] Calculating fixed effect contrasts with
 log-transformed data
In-Reply-To: <AA818EAD2576BC488B4F623941DA74275D67BD17@inbomail.inbo.be>
References: <CAL9m74a9LiZ4E5APMzY+oSk6c7TDvTGxrSrPubNu3o-vANRsTQ@mail.gmail.com>
	<AA818EAD2576BC488B4F623941DA74275D67BD17@inbomail.inbo.be>
Message-ID: <CAL9m74bp7Sz00U703xOJSDjPv_XbQa0wHZj9P6rTnBykh2SQhA@mail.gmail.com>

Thank you Thierry,
I have looked through the glht function in multcomp, and have two
further questions:

(1) For the lmer output below, I would like to go through and
calculate a 95% CI for the difference between each "Treatment" and
"Control" fixed effect.  Based on my reading of the glht instructions,
this should look like:

> glht(Mod.NO3.1.1, linfct=c("sitettMossAddition Treatment - sitettMossAddition Control=0"))

Yet I get the following error message:

Error in parse(text = ex[i]) : <text>:1:20: unexpected symbol
1: sitettMossAddition Treatment
                                       ^
Any ideas on what I'm doing wrong here?

(2) As you can see, I am working with a log10 transformed response
variable.  I'd like to stay with this for homog. of variance reasons,
and for reporting the "Treatment -Control" CI previously mentioned,
I'd like to report the backtransformed limits of the CI.  At what
point in this process should the back-transformation happen?  When
attempting this calculation without glht, I am uncertain of where in
the process to back-transform as well.  I had been hoping to simply
use 10^ for each fixed effect and its SE, as well as each element of
the vcov matrix, but I fear I am overlooking some basic math here.

Finally, I have pasted my data below the model output, in case this is
helpful.

Thank you,
Gus

###  Model Code  ###

Mod.NO3.1.1<-lmer(NO3Nyearone ~ 1 + sitett +(1 | pr), data=data.file.final)
Mod.NO3.1.2<-lmer(log10(NO3Nyearone)~ 1  + (1 | pr), data=data.file.final)

anova(Mod.NO3.1.1,Mod.NO3.1.2)
Mod.NO3.1.1

###   Output   ###
Data: data.file.final
Models:
Mod.NO3.1.2: log10(NO3Nyearone) ~ 1 + (1 | pr)
Mod.NO3.1.1: log10(NO3Nyearone) ~ 1 + sitett + (1 | pr)
            Df    AIC    BIC  logLik  Chisq Chi Df Pr(>Chisq)
Mod.NO3.1.2  3 99.830 108.59 -46.915
Mod.NO3.1.1 14 74.329 115.21 -23.165 47.501     11  1.752e-06 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
Linear mixed model fit by REML
Formula: log10(NO3Nyearone) ~ 1 + sitett + (1 | pr)
   Data: data.file.final
   AIC   BIC logLik deviance REMLdev
 110.9 151.8 -41.46    46.33   82.92
Random effects:
 Groups   Name        Variance  Std.Dev.
 pr       (Intercept) 0.0029239 0.054073
 Residual             0.0871127 0.295149
Number of obs: 137, groups: pr, 72

Fixed effects:
                             Estimate Std. Error t value
(Intercept)                                        1.24011    0.09047  13.708
sitettLepAddition Treatment       -0.03859    0.12329  -0.313
sitettMossAddition Control           0.07747    0.13110   0.591
sitettMossAddition Treatment      0.13940    0.12794   1.090
sitettMossRemoval Control        -0.36994    0.12525  -2.954
sitettMossRemoval Treatment   -0.25789    0.12525  -2.059
sitettSaddle Control                     -0.33039    0.12525  -2.638
sitettSaddle Treatment                -0.46379    0.12794  -3.625
sitettToeAdditions Control          -0.17356    0.12525  -1.386
sitettToeAdditions Treatment     -0.33236    0.13110  -2.535
sitettToeRemoval Control          -0.38744    0.12525  -3.093
sitettToeRemoval Treatment     -0.44006    0.12525  -3.513

Correlation of Fixed Effects:
            (Intr) sttLAT sttMAC sttMAT sttMRC sttMRT stttSC stttST
sttTAC sttTAT sttTRC
stttLpAddtT -0.712
stttMssAddC -0.690  0.491
stttMssAddT -0.707  0.503  0.502
stttMssRmvC -0.722  0.514  0.498  0.511
stttMssRmvT -0.722  0.514  0.498  0.511  0.537
stttSddlCnt -0.722  0.514  0.498  0.511  0.522  0.522
stttSddlTrt -0.707  0.503  0.488  0.500  0.511  0.511  0.526
stttTAddtnC -0.722  0.514  0.498  0.511  0.522  0.522  0.522  0.511
stttTAddtnT -0.690  0.491  0.476  0.488  0.498  0.498  0.498  0.488
0.513
stttTRmvlCn -0.722  0.514  0.498  0.511  0.522  0.522  0.522  0.511
0.522  0.498
stttTRmvlTr -0.722  0.514  0.498  0.511  0.522  0.522  0.522  0.511
0.522  0.498  0.537

#####   Data  ######

PlotID	Site	pr	tt	sitett	NO3Nyearone
3156	LepAddition	LeprariaAdditionone	Control	LepAddition Control	35
3155	LepAddition	LeprariaAdditionone	Treatment	LepAddition Treatment	105.6
3161	LepAddition	LeprariaAdditionfour	Treatment	LepAddition Treatment	44.8
3297	LepAddition	LeprariaAdditionseven	Control	LepAddition Control	40.6
3158	LepAddition	LeprariaAdditiontwo	Control	LepAddition Control	26.4
3162	LepAddition	LeprariaAdditionfour	Control	LepAddition Control	19.6
3293	LepAddition	LeprariaAdditioneight	Control	LepAddition Control	28.2
3157	LepAddition	LeprariaAdditiontwo	Treatment	LepAddition Treatment	21.8
2740	LepAddition	LeprariaAdditionthree	Control	LepAddition Control	21.8
2745	LepAddition	LeprariaAdditionnine	Treatment	LepAddition Treatment	20.2
2755	LepAddition	LeprariaAdditionsix	Treatment	LepAddition Treatment	7.8
3284	LepAddition	LeprariaAdditiontwelve	Treatment	LepAddition Treatment	16.4
2749	LepAddition	LeprariaAdditionfive	Control	LepAddition Control	14.6
2744	LepAddition	LeprariaAdditionnine	Control	LepAddition Control	13
2759	LepAddition	LeprariaAdditioneleven	Treatment	LepAddition Treatment	7.4
2758	LepAddition	LeprariaAdditioneleven	Control	LepAddition Control	7.4
3292	LepAddition	LeprariaAdditioneight	Treatment	LepAddition Treatment	12.8
3296	LepAddition	LeprariaAdditionseven	Treatment	LepAddition Treatment	9.6
2748	LepAddition	LeprariaAdditionfive	Treatment	LepAddition Treatment	12.6
3294	LepAddition	LeprariaAdditionthirteen	Treatment	LepAddition Treatment	7.8
3285	LepAddition	LeprariaAdditiontwelve	Control	LepAddition Control	8.4
2754	LepAddition	LeprariaAdditionsix	Control	LepAddition Control	8.4
2741	LepAddition	LeprariaAdditionthree	Treatment	LepAddition Treatment	11
3289	MossAddition	MossAddthree	Treatment	MossAddition Treatment	63.8
2751	MossAddition	MossAddsix	Control	MossAddition Control	46.8
3286	MossAddition	MossAddthree	Control	MossAddition Control	61.8
3288	MossAddition	MossAddeleven	Treatment	MossAddition Treatment	59.4
2747	MossAddition	MossAddseven	Treatment	MossAddition Treatment	54.3
2752	MossAddition	MossAddeight	Treatment	MossAddition Treatment	20.6
3280	MossAddition	MossAddtwo	Control	MossAddition Control	31.8
3163	MossAddition	MossAddtwelve	Control	MossAddition Control	37.8
2750	MossAddition	MossAddsix	Treatment	MossAddition Treatment	30.8
2743	MossAddition	MossAddfive	Treatment	MossAddition Treatment	25
3291	MossAddition	MossAddeleven	Control	MossAddition Control	20.2
3299	MossAddition	MossAddone	Control	MossAddition Control	25
3281	MossAddition	MossAddtwo	Treatment	MossAddition Treatment	16.3
2757	MossAddition	MossAddnine	Control	MossAddition Control	14.8
2753	MossAddition	MossAddeight	Control	MossAddition Control	21
3164	MossAddition	MossAddtwelve	Treatment	MossAddition Treatment	18
3160	MossAddition	MossAddfour	Treatment	MossAddition Treatment	17.6
2742	MossAddition	MossAddfive	Control	MossAddition Control	18.8
2756	MossAddition	MossAddnine	Treatment	MossAddition Treatment	11.6
3287	MossAddition	MossAddten	Control	MossAddition Control	10.8
3290	MossAddition	MossAddten	Treatment	MossAddition Treatment	7.6
2746	MossAddition	MossAddseven	Control	MossAddition Control	4.4
3242	MossRemoval	MossRemone	Control	MossRemoval Control	40.6
3245	MossRemoval	MossRemtwo	Treatment	MossRemoval Treatment	24.6
3253	MossRemoval	MossRemtwelve	Control	MossRemoval Control	7.6
3250	MossRemoval	MossRemnine	Treatment	MossRemoval Treatment	11
3240	MossRemoval	MossRemseven	Treatment	MossRemoval Treatment	6.6
3257	MossRemoval	MossRemfive	Control	MossRemoval Control	7.4
3244	MossRemoval	MossRemtwo	Control	MossRemoval Control	9.2
3252	MossRemoval	MossRemtwelve	Treatment	MossRemoval Treatment	6.8
3254	MossRemoval	MossRemsix	Treatment	MossRemoval Treatment	17.4
3241	MossRemoval	MossRemseven	Control	MossRemoval Control	9
3255	MossRemoval	MossRemsix	Control	MossRemoval Control	6
2687	MossRemoval	MossRemten	Treatment	MossRemoval Treatment	11.8
3246	MossRemoval	MossRemthree	Control	MossRemoval Control	11
3243	MossRemoval	MossRemone	Treatment	MossRemoval Treatment	10.2
3259	MossRemoval	MossRemfour	Treatment	MossRemoval Treatment	9
3247	MossRemoval	MossRemthree	Treatment	MossRemoval Treatment	6.8
3251	MossRemoval	MossRemnine	Control	MossRemoval Control	8.4
2684	MossRemoval	MossRemeleven	Treatment	MossRemoval Treatment	7.2
3248	MossRemoval	MossRemeight	Treatment	MossRemoval Treatment	7
3249	MossRemoval	MossRemeight	Control	MossRemoval Control	10
3258	MossRemoval	MossRemfour	Control	MossRemoval Control	8.8
3256	MossRemoval	MossRemfive	Treatment	MossRemoval Treatment	7.8
2686	MossRemoval	MossRemten	Control	MossRemoval Control	1
2685	MossRemoval	MossRemeleven	Control	MossRemoval Control	3
2999	Saddle	LeprariaRemovalnine	Treatment	Saddle Treatment	21.8
2998	Saddle	LeprariaRemovalnine	Control	Saddle Control	12
2956	Saddle	LeprariaRemovaltwelve	Control	Saddle Control	11.6
2485	Saddle	LeprariaRemovalthree	Control	Saddle Control	13.4
2489	Saddle	LeprariaRemovalten	Control	Saddle Control	3
2497	Saddle	LeprariaRemovalsix	Control	Saddle Control	13.2
2487	Saddle	LeprariaRemovalten	Treatment	Saddle Treatment	3.4
2958	Saddle	LeprariaRemovaleleven	Treatment	Saddle Treatment	7.4
2491	Saddle	LeprariaRemovalone	Treatment	Saddle Treatment	12.2
2483	Saddle	LeprariaRemovalfour	Control	Saddle Control	16.4
2484	Saddle	LeprariaRemovalthree	Treatment	Saddle Treatment	14.4
2498	Saddle	LeprariaRemovalseven	Control	Saddle Control	9.8
2493	Saddle	LeprariaRemovaltwo	Control	Saddle Control	8.6
2481	Saddle	LeprariaRemovaleight	Treatment	Saddle Treatment	10.8
2480	Saddle	LeprariaRemovaleight	Control	Saddle Control	5.4
2959	Saddle	LeprariaRemovaleleven	Control	Saddle Control	6.6
2496	Saddle	LeprariaRemovalsix	Treatment	Saddle Treatment	3.6
2494	Saddle	LeprariaRemovalfive	Control	Saddle Control	4.2
2490	Saddle	LeprariaRemovalone	Control	Saddle Control	5.4
2482	Saddle	LeprariaRemovalfour	Treatment	Saddle Treatment	3.2
2495	Saddle	LeprariaRemovalfive	Treatment	Saddle Treatment	3.4
2499	Saddle	LeprariaRemovalseven	Treatment	Saddle Treatment	3
2492	Saddle	LeprariaRemovaltwo	Treatment	Saddle Treatment	2.8
2577	ToeAdditions	FlavocetrariaAdditiontwelve	Control	ToeAdditions Control	121.4
2581	ToeAdditions	FlavocetrariaAdditionone	Control	ToeAdditions Control	17.2
2576	ToeAdditions	FlavocetrariaAdditionfour	Control	ToeAdditions Control	20.8
2568	ToeAdditions	FlavocetrariaAdditiontwo	Control	ToeAdditions Control	17.6
2580	ToeAdditions	FlavocetrariaAdditionthree	Control	ToeAdditions Control	18.4
2572	ToeAdditions	FlavocetrariaAdditioneight	Treatment	ToeAdditions
Treatment	14.6
2573	ToeAdditions	FlavocetrariaAdditionfour	Treatment	ToeAdditions Treatment	12
2571	ToeAdditions	FlavocetrariaAdditionfive	Control	ToeAdditions Control	10.8
2574	ToeAdditions	FlavocetrariaAdditionseven	Treatment	ToeAdditions
Treatment	13.6
2588	ToeAdditions	FlavocetrariaAdditiontwo	Treatment	ToeAdditions Treatment	10.2
2575	ToeAdditions	FlavocetrariaAdditionsix	Control	ToeAdditions Control	11.8
2579	ToeAdditions	FlavocetrariaAdditionnine	Treatment	ToeAdditions
Treatment	13.8
2584	ToeAdditions	FlavocetrariaAdditionseven	Control	ToeAdditions Control	8.6
2582	ToeAdditions	FlavocetrariaAdditionten	Control	ToeAdditions Control	10
2569	ToeAdditions	FlavocetrariaAdditionsix	Treatment	ToeAdditions Treatment	8.6
2583	ToeAdditions	FlavocetrariaAdditionone	Treatment	ToeAdditions Treatment	7.2
2578	ToeAdditions	FlavocetrariaAdditioneleven	Control	ToeAdditions Control	10.2
2570	ToeAdditions	FlavocetrariaAdditionfive	Treatment	ToeAdditions Treatment	9.8
2585	ToeAdditions	FlavocetrariaAdditionthree	Treatment	ToeAdditions
Treatment	6.6
2589	ToeAdditions	FlavocetrariaAdditionten	Treatment	ToeAdditions Treatment	4
2683	ToeAdditions	FlavocetrariaAdditionnine	Control	ToeAdditions Control	5
2586	ToeAdditions	FlavocetrariaAdditiontwelve	Treatment	ToeAdditions
Treatment	6.2
2682	ToeAdditions	FlavocetrariaAdditioneight	Control	ToeAdditions Control	0.8
2681	ToeAdditions	FlavocetrariaAdditioneleven	Treatment	ToeAdditions Treatment	5
2986	ToeRemoval	FlavocetrariaRemovalfive	Treatment	ToeRemoval Treatment	16.8
2991	ToeRemoval	FlavocetrariaRemovalfour	Control	ToeRemoval Control	16.6
2983	ToeRemoval	FlavocetrariaRemovaltwo	Control	ToeRemoval Control	9.6
2994	ToeRemoval	FlavocetrariaRemovalnine	Control	ToeRemoval Control	7.8
2989	ToeRemoval	FlavocetrariaRemovalthree	Treatment	ToeRemoval Treatment	7.2
2796	ToeRemoval	FlavocetrariaRemovaleight	Control	ToeRemoval Control	9.2
2982	ToeRemoval	FlavocetrariaRemovaltwo	Treatment	ToeRemoval Treatment	7.2
2985	ToeRemoval	FlavocetrariaRemovalone	Treatment	ToeRemoval Treatment	6.8
2984	ToeRemoval	FlavocetrariaRemovalone	Control	ToeRemoval Control	2.6
2799	ToeRemoval	FlavocetrariaRemovalten	Control	ToeRemoval Control	8
2797	ToeRemoval	FlavocetrariaRemovaleight	Treatment	ToeRemoval Treatment	9.4
2992	ToeRemoval	FlavocetrariaRemovalsix	Control	ToeRemoval Control	8.4
2981	ToeRemoval	FlavocetrariaRemovaltwelve	Control	ToeRemoval Control	7.8
2996	ToeRemoval	FlavocetrariaRemovalseven	Control	ToeRemoval Control	6.6
2980	ToeRemoval	FlavocetrariaRemovaltwelve	Treatment	ToeRemoval Treatment	7.8
2987	ToeRemoval	FlavocetrariaRemovalfive	Control	ToeRemoval Control	6.4
2988	ToeRemoval	FlavocetrariaRemovalthree	Control	ToeRemoval Control	5.4
2990	ToeRemoval	FlavocetrariaRemovalfour	Treatment	ToeRemoval Treatment	6
2993	ToeRemoval	FlavocetrariaRemovalsix	Treatment	ToeRemoval Treatment	4.2
2689	ToeRemoval	FlavocetrariaRemovaleleven	Control	ToeRemoval Control	4.8
2997	ToeRemoval	FlavocetrariaRemovalseven	Treatment	ToeRemoval Treatment	4.6
2798	ToeRemoval	FlavocetrariaRemovalten	Treatment	ToeRemoval Treatment	6
2688	ToeRemoval	FlavocetrariaRemovaleleven	Treatment	ToeRemoval Treatment	4.4
2995	ToeRemoval	FlavocetrariaRemovalnine	Treatment	ToeRemoval Treatment	3





On Mon, Jul 16, 2012 at 1:53 AM, ONKELINX, Thierry
<Thierry.ONKELINX at inbo.be> wrote:
> Dear Gus,
>
> Have a look at glht() from the multcomp package. It allows you to define the contrasts that you are interested in.
>
> Best regards,
>
> Thierry
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
> + 32 2 525 02 51
> + 32 54 43 61 85
> Thierry.Onkelinx at inbo.be
> www.inbo.be
>
> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
> ~ Sir Ronald Aylmer Fisher
>
> The plural of anecdote is not data.
> ~ Roger Brinner
>
> The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> -----Oorspronkelijk bericht-----
> Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Gus Jespersen
> Verzonden: vrijdag 13 juli 2012 20:33
> Aan: r-sig-mixed-models at r-project.org
> Onderwerp: [R-sig-ME] Calculating fixed effect contrasts with log-transformed data
>
> Greetings,
> I doubt this is a particularly interesting question for you mixed model gurus, but here goes.  As you can see in the output below, I have a model with twelve fixed effect parameters.  I am interested in each of the "Treatment" vs. "Control" comparisons for each "site"(in each fixed effect parameter name, these are specified by the text immediately following "sitett"). To produce a 95% CI for such a comparison I was advised to take two steps:
>
> (1) Subtract the Control parameter estimate from the Treatment parameter estimate for each site.
> (2) Compute the SE for this comparison via:  sqrt( var(treatment) +
> var(control) - 2*cov(treatmentt,control)).  To get these values I am using the vcov matrix for the model.
>
> When I move to log10-transformed data, I am thinking I should backtransform the fixed effects and SE's before moving ahead  with the Control-Treatment comparisons.  However, the calculations become more problematic as ( var(treatment) + var(control) -
> 2*cov(treatmentt,control)) is consistently negative.  I am uncertain on how to proceed here.  Any advice would be much appreciated.
>
> Thank you,
> Gus
>
> Data: data.file.final
> Models:
> Mod.NO3.1.2: NO3Nyearone ~ 1 + (1 | pr)
> Mod.NO3.1.1: NO3Nyearone ~ 1 + sitett + (1 | pr)
>             Df    AIC    BIC  logLik  Chisq Chi Df Pr(>Chisq)
> Mod.NO3.1.2  3 1163.5 1172.2 -578.72
> Mod.NO3.1.1 14 1155.8 1196.7 -563.90 29.637     11   0.001806 **
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Linear mixed model fit by REML
> Formula: NO3Nyearone ~ 1 + sitett + (1 | pr)
>    Data: data.file.final
>   AIC  BIC logLik deviance REMLdev
>  1098 1139 -534.8     1128    1070
> Random effects:
>  Groups   Name        Variance Std.Dev.
>  pr       (Intercept)  33.348   5.7747
>  Residual             210.115  14.4954
> Number of obs: 137, groups: pr, 72
>
> Fixed effects:
>                              Estimate Std. Error t value
> (Intercept)                    20.118      4.701   4.280
> sitettLepAddition Treatment     3.032      6.069   0.500
> sitettMossAddition Control      5.677      6.809   0.834
> sitettMossAddition Treatment    9.418      6.648   1.417
> sitettMossRemoval Control      -9.951      6.510  -1.529
> sitettMossRemoval Treatment    -9.601      6.510  -1.475
> sitettSaddle Control          -10.985      6.510  -1.687
> sitettSaddle Treatment        -12.269      6.648  -1.846
> sitettToeAdditions Control      0.932      6.510   0.143
> sitettToeAdditions Treatment  -11.678      6.809  -1.715
> sitettToeRemoval Control      -12.351      6.510  -1.897
> sitettToeRemoval Treatment    -13.168      6.510  -2.023
>
>
>
> --
> R. Gus Jespersen
> PhD Candidate
> College of Forest Resources
> University of Washington
> Box 352100
> Seattle, WA 98195-2100
> (206) 543-5777
> jesper at u.washington.edu
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> * * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
> Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
> The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.



-- 
R. Gus Jespersen
PhD Candidate
College of Forest Resources
University of Washington
Box 352100
Seattle, WA 98195-2100
(206) 543-5777
jesper at u.washington.edu


From jwiley.psych at gmail.com  Tue Jul 17 21:04:43 2012
From: jwiley.psych at gmail.com (Joshua Wiley)
Date: Tue, 17 Jul 2012 12:04:43 -0700
Subject: [R-sig-ME] In simple terms,
 how is the estimated variance of higher-level effects calculated?
In-Reply-To: <CAO7JsnSmAoF2rP_nM8cFvFo+vVdSOtxxNEv+2BCkQH=19HiZ2A@mail.gmail.com>
References: <1342470256.38391.YahooMailNeo@web161805.mail.bf1.yahoo.com>
	<Pine.LNX.4.64.1207170759510.24433@orpheus.qimr.edu.au>
	<1342545228.84174.YahooMailNeo@web161806.mail.bf1.yahoo.com>
	<CAO7JsnSmAoF2rP_nM8cFvFo+vVdSOtxxNEv+2BCkQH=19HiZ2A@mail.gmail.com>
Message-ID: <CANz9Z_K4PVaM071+RhmCAoNsTkJhADfvQXjaF+4BxuymASfWDQ@mail.gmail.com>

On Tue, Jul 17, 2012 at 11:55 AM, Douglas Bates <bates at stat.wisc.edu> wrote:
> On Tue, Jul 17, 2012 at 12:13 PM, Jeremy Koster <helixed2 at yahoo.com> wrote:
>> Thanks for the feedback, David.  Owing to my lack of expertise, I confess that I didn't completely follow everything, but your email inspired me to explore the varying intercepts with different exponential families, focusing particularly on the gaussian.
>>
>> In short, my students were looking at the estimated varying intercepts for each higher-level group (or the "BLUP's", as some people seem to call them) -- the intercepts that one can see by entering:
>
> As Alan James once said, "these values are just like the BLUPs - Best
> Linear Unbiased Predictors - except that they aren't linear and they
> aren't unbiased and there is no clear sense in which they are "best",
> but other than that ..."

Nominated for a fortune.

[snip]

-- 
Joshua Wiley
Ph.D. Student, Health Psychology
Programmer Analyst II, Statistical Consulting Group
University of California, Los Angeles
https://joshuawiley.com/


From bbolker at gmail.com  Tue Jul 17 22:57:55 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 17 Jul 2012 20:57:55 +0000 (UTC)
Subject: [R-sig-ME] predicted mean from GLMM lower than mean from GAM
References: <5005B670.5000905@stat.berkeley.edu>
Message-ID: <loom.20120717T225345-47@post.gmane.org>

benton <benton at ...> writes:

> I fit a Poisson GLMM with only the intercept and two random effects, and 
> the predicted mean was 1.14. When I fit a generalized additive model 
> (GAM) with only the intercept, the predicted mean was 1.6. Does anyone 
> know why this is happening? I'm looking for a theoretical response, as 
> I've checked my code and there are no errors.
> 

  A little more information/reproducible example would be helpful; it
would be possible for me to invent a reproducible example for myself,
but it would be easier (for me!) and more likely to answer your
specific question if you provide the example.  Have you compared the
Poisson GLMM prediction with a Poisson GLM (no random effects)
prediction to make sure there's not some funky/surprising difference
between the GAM (presumably you're using mgcv::gam()) and the GLMM
(presumably you're using lme4::glmer()) ?  How are you deriving the
predictions?  Are you definitely using the same family and link
function for both models?  In general there can be important
differences between the marginal (no-random-effects) and conditional
(including-random-effects) predictions, but off the top of my head
that should not apply to intercept-only models ...  See
http://tinyurl.com/reproducible-000 for more info on reproducible
examples ...

  Ben Bolker


From bbolker at gmail.com  Wed Jul 18 00:08:07 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 17 Jul 2012 22:08:07 +0000 (UTC)
Subject: [R-sig-ME] Calculating fixed effect contrasts with
	log-transformed data
References: <CAL9m74a9LiZ4E5APMzY+oSk6c7TDvTGxrSrPubNu3o-vANRsTQ@mail.gmail.com>
	<AA818EAD2576BC488B4F623941DA74275D67BD17@inbomail.inbo.be>
	<CAL9m74bp7Sz00U703xOJSDjPv_XbQa0wHZj9P6rTnBykh2SQhA@mail.gmail.com>
Message-ID: <loom.20120718T000504-96@post.gmane.org>

Gus Jespersen <jesper <at> u.washington.edu> writes:

> 
> Thank you Thierry,
> I have looked through the glht function in multcomp, and have two
> further questions:
> 

 [snip]

> Yet I get the following error message:
> 
> Error in parse(text = ex[i]) : <text>:1:20: unexpected symbol
> 1: sitettMossAddition Treatment
>                                        ^
> Any ideas on what I'm doing wrong here?

  It is very likely that the glht function is having trouble
with the spaces in your level names.  I would strongly suggest
that you reformulate them as legal R variable names: something like

     levels(mydata$myfactor) <- make.names(levels(mydata$myfactor))
should work.

> 
> (2) As you can see, I am working with a log10 transformed response
> variable.  I'd like to stay with this for homog. of variance reasons,
> and for reporting the "Treatment -Control" CI previously mentioned,
> I'd like to report the backtransformed limits of the CI.  At what
> point in this process should the back-transformation happen?  When
> attempting this calculation without glht, I am uncertain of where in
> the process to back-transform as well.  I had been hoping to simply
> use 10^ for each fixed effect and its SE, as well as each element of
> the vcov matrix, but I fear I am overlooking some basic math here.

  Yes, you are.  You need to back-transform the confidence intervals,
not the elements and their standard errors.  The basic math you
are thinking about is

  10^(est+1.96*stderr) !== 10^est+10^(1.96*stderr)

If you back-transform first, the RHS is what you will be doing;
you want the LHS (this is for the upper CI, the obvious parallel
applies for the lower CI)


From j.hadfield at ed.ac.uk  Wed Jul 18 09:08:16 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Wed, 18 Jul 2012 08:08:16 +0100
Subject: [R-sig-ME] predicted mean from GLMM lower than mean from GAM
In-Reply-To: <loom.20120717T225345-47@post.gmane.org>
References: <5005B670.5000905@stat.berkeley.edu>
	<loom.20120717T225345-47@post.gmane.org>
Message-ID: <20120718080816.27804ebbmnx4cyec@www.staffmail.ed.ac.uk>

Hi,

If you take exp(log(1.14)+0.5*v) where v is the sum of the estimated  
variances do the two estimates then coincide? 1.14 in the GLMM is the  
predicted modal count (i.e. when the two random effects are zero) and  
exp(log(1.14)+0.5*v) is the predicted mean count (i.e. averaged over  
random effects).

Cheers,

Jarrod



Quoting Ben Bolker <bbolker at gmail.com> on Tue, 17 Jul 2012 20:57:55  
+0000 (UTC):

> benton <benton at ...> writes:
>
>> I fit a Poisson GLMM with only the intercept and two random effects, and
>> the predicted mean was 1.14. When I fit a generalized additive model
>> (GAM) with only the intercept, the predicted mean was 1.6. Does anyone
>> know why this is happening? I'm looking for a theoretical response, as
>> I've checked my code and there are no errors.
>>
>
>   A little more information/reproducible example would be helpful; it
> would be possible for me to invent a reproducible example for myself,
> but it would be easier (for me!) and more likely to answer your
> specific question if you provide the example.  Have you compared the
> Poisson GLMM prediction with a Poisson GLM (no random effects)
> prediction to make sure there's not some funky/surprising difference
> between the GAM (presumably you're using mgcv::gam()) and the GLMM
> (presumably you're using lme4::glmer()) ?  How are you deriving the
> predictions?  Are you definitely using the same family and link
> function for both models?  In general there can be important
> differences between the marginal (no-random-effects) and conditional
> (including-random-effects) predictions, but off the top of my head
> that should not apply to intercept-only models ...  See
> http://tinyurl.com/reproducible-000 for more info on reproducible
> examples ...
>
>   Ben Bolker
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From benton at stat.berkeley.edu  Wed Jul 18 14:24:24 2012
From: benton at stat.berkeley.edu (benton)
Date: Wed, 18 Jul 2012 08:24:24 -0400
Subject: [R-sig-ME] predicted mean from GLMM lower than mean from GAM
In-Reply-To: <mailman.1.1342605601.4657.r-sig-mixed-models@r-project.org>
References: <mailman.1.1342605601.4657.r-sig-mixed-models@r-project.org>
Message-ID: <5006AAF8.3030505@stat.berkeley.edu>

Thank you for your replies, Mr. Bolker and Mr. Hadfield.

Yes, I am using LME4 and mgcv (glmer and gam, respectively). I also fit 
a GLM, and I'm getting a similar intercept in the GLM and GAM, but again 
they are both higher than that from the GLMM.

Here is my code. PV_500 is the number of clinical episodes of P. vivax 
malaria during an interval. There were 264 children from 11 villages. 
Children were not considered at risk of acquiring malaria parasites 
after a certain period following treatment, so we fit an offset for 
their time at risk.

GLMM: Pv500 <- glmer(PV_500 ~ 1 + (1|village) + (1|child), family = 
poisson, data = dat, offset=log(YEARATRISK))
GLM: glm(PV_500 ~ 1, family=poisson, data=dat, offset=log(YEARATRISK))
GAM: gam(PV_500 ~ 1, family=poisson, data=dat, offset=log(YEARATRISK))

GLMM intercept = 0.13
GLMM mean = exp(0.13) = 1.14

GLM intercept = 0.47
GLM mean = exp(0.47) = 1.60

GAM intercept = 0.47
GAM mean = exp(0.47) = 1.60

If I do as Mr. Hadfield suggested, and add 0.5*(v_child + v_village), 
then I get the following for the GLMM estimates:

exp(.13 + .5*(.68 +.12)) = 1.70

It is now slightly higher.

Does this make the problem more clear? Thanks again for any more 
suggestions.

Best,

Katie Benton



On 7/18/12 6:00 AM, r-sig-mixed-models-request at r-project.org wrote:
> Re: predicted mean from GLMM lower than mean from GAM


From a.abrain at gmail.com  Wed Jul 18 17:47:12 2012
From: a.abrain at gmail.com (=?ISO-8859-1?Q?Alejandro_Mart=EDnez_Abra=EDn?=)
Date: Wed, 18 Jul 2012 17:47:12 +0200
Subject: [R-sig-ME] overdispersion in GLMMs
Message-ID: <CAHrfa+z6kD5CeJAGnXeqQXo_Ck6Br5NHWiDgmjycWBShzftPtg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120718/25bf74fb/attachment.pl>

From a.abrain at gmail.com  Wed Jul 18 17:39:51 2012
From: a.abrain at gmail.com (=?ISO-8859-1?Q?Alejandro_Mart=EDnez_Abra=EDn?=)
Date: Wed, 18 Jul 2012 17:39:51 +0200
Subject: [R-sig-ME] (no subject)
Message-ID: <CAHrfa+zCN1TQtuFEjiwSWMHRJBU8V1cgd3u-BuF8GqzJSBJmeg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120718/48880cb8/attachment.pl>

From roby.joehanes at nih.gov  Wed Jul 18 20:08:36 2012
From: roby.joehanes at nih.gov (Joehanes, Roby (NIH/NHLBI) [F])
Date: Wed, 18 Jul 2012 14:08:36 -0400
Subject: [R-sig-ME] patch for pedigremm
In-Reply-To: <840AF959BC19ED47A774DB45D9CAEC1302DA60@uqexmdb8.soe.uq.edu.au>
Message-ID: <CC2C73E4.27A3%joehanesr@mail.nih.gov>

Hi David:

Yes, you can overwrite my version (if you install them) by reinstalling both
lme4 and pedigreemm from the CRAN website.

I will send you a copy for both lme4 and pedigreemm in a separate e-mail so
that they won't flood the mailing list. Note that they are tested in Linux
only so far.

Installation instruction: Put the two files in your home directory in Linux,
start R in Linux and execute the following lines:

install.packages(c("Rcpp", "RcppEigen", "minqa"));
install.packages("~/lme4-svn-1786b.tar.gz", repo=NULL, type="source");
install.packages("~/pedigreemm-0.2c.tar.gz", repo=NULL, type="source");

Hope this helps,
Roby Joehanes


On 7/17/12 8:05 PM, "David Aguirre-Davies" <d.aguirre at uq.edu.au> wrote:

> Hi Roby,
> 
> Thank you for the reply. Yes the model is exactly as you describe it (i.e.,
> length(unique(dat$id)) == nrow(dat)).
> 
> If I install your versions of lme4 and pedigreemm, and in doing so overwrite
> the existing versions, can I then overwrite your versions (should I need the
> old versions for some reason) by reinstalling lme4 from the cran web site?
> 
> If this is the case, could you please send me your pre-patched versions of
> lme4 and pedigreemm and some instructions on how to get them going?
> 
> Thank again for your help
> Regards
> David  Aguirre
> 
> -----Original Message-----
> From: Joehanes, Roby (NIH/NHLBI) [F] [mailto:roby.joehanes at nih.gov]
> Sent: Wednesday, 18 July 2012 1:09 AM
> To: David Aguirre-Davies
> Cc: r-sig-mixed-models at r-project.org
> Subject: Re: patch for pedigremm
> 
> Hi David:
> 
> I apologize for the late reply.
> 
> The patch is meant to be implemented at the source code, not at R console.
> That is, you need to download the source from the code tracker and retrieve
> the correct revision number. Then, you can apply the patch. You will need to
> patch both your lme4 and your pedigreemm to make it work. I have a pre-patched
> versions, let me know if you want them. Note that if you install my version of
> lme4 and pedigreemm, your version of these packages will be overwritten. Also,
> I only tested them in Linux. I am not aware of whether they would work on
> Windows or Mac.
> 
> What the patch is doing is actually to enable 1 observation per subject, which
> otherwise not available in the regular pedigreemm. In addition, I incorporated
> the latest advances of lme4 as the pedigreemm's back end.
> 
> That being said, if the matrix A you mentioned is the relationship matrix, it
> should always be square regardless of the length of the IDs (or the
> discrepancies thereof). In pedigreemm, the matrix A will be constructed
> automatically from the pedigree (i.e., Ped.0) in a function called
> "relfactor". This same function will subset the matrix A automatically to
> match the IDs that appear in your data. So, there is no need of extra work or
> even my patch for this type of problem (i.e., length(dat$id) != length(id)).
> The way you specified the ID discrepancies (i.e., length(dat$id) = dim(A)[1] &
> length(id) = dim(A)[2]) is a bit unclear to me, and I take it to mean as
> (length(dat$id) != length(id)).
> 
> Looking at the error message that you presented, however, it seems that there
> is one ID per observation (i.e., length(unique(dat$id)) == nrow(dat)). That is
> the use case scenario where you would need my patch.
> 
> Hope this helps,
> Roby
> 
> On 6/26/12 11:50 PM, "David Aguirre-Davies" <d.aguirre at uq.edu.au> wrote:
> 
> 
> Hi Roby,
> 
> Sorry for my incompetence, but running patches on packages is beyond my R
> knowledge at this stage. I would like to run the following model using
> pedigreemm and I get the error message below,
> 
>> fm1 <-pedigreemm(value.p ~ gen + (1|id), data=dat, pedigree =
>> list(id=Ped.0))
> Error in function (fr, FL, start, REML, verbose)  :
>   Number of levels of a grouping factor for the random effects must be less
> than the number of observations
> 
> I have followed the clues on the web, and I have attempted to implement the
> patch you have provide
> (https://r-forge.r-project.org/tracker/index.php?func=detail&aid=1928&group_id
> =60&atid=300), but unfortunately I have been unsuccessful.
> 
> So my question is, how can I implement the patch you provide to run an animal
> model where length(dat$id) = dim(A)[1] & length(id) = dim(A)[2]?
> 
> Is there are trick to implementing the patch as cut and paste into the R
> console doesn't seem to work for me.
> 
> Any assistance would be much appreciated.
> 
> Regards
> David Aguirre

-- 
Roby Joehanes
Research Associate
Roby.Joehanes at nih.gov
Building 12A, Room 2007
National Institutes of Health (NIH)
Bethesda, MD 20892
P: (301) 402-8702
F: (301) 480-0028 or (301) 402-2867


From jesper at u.washington.edu  Thu Jul 19 01:53:54 2012
From: jesper at u.washington.edu (Gus Jespersen)
Date: Wed, 18 Jul 2012 16:53:54 -0700
Subject: [R-sig-ME] fixed effects/log transformations question
Message-ID: <CAL9m74bdhV4PSq=eSyzfa8KaH5Nz4SATGDvAaY3CKqu9kJi_NQ@mail.gmail.com>

Dr. Bolker,
Thanks for the response.  One last question with regard to negative t
values when using log10 transformed data.  I am assuming the correct
interpretation of the following output is: if the t value is negative
and you're using log10 data, to get the fixed effect CI, you must add
your own negative sign to 10^(est.+1/96*SE), such that the
backtransformed CI from the output below would be:

([1] "95 % REML Confidence interval"
[1] -0.58261813  0.02578124

becomes

-.295       -1.05


Is this correct,

Thanks again for the help
Gus

[1] "###############NH4 Results Year Two##################"
Data: data.sub
Models:
Mod.NH4.2.2: log10(NH4Nyeartwo) ~ 1 + (1 | pr)
Mod.NH4.2.1: log10(NH4Nyeartwo) ~ 1 + sitett + (1 | pr)
            Df    AIC    BIC   logLik  Chisq Chi Df Pr(>Chisq)
Mod.NH4.2.2  3 26.427 29.700 -10.2136
Mod.NH4.2.1  4 25.243 29.607  -8.6216 3.1841      1    0.07436 .
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
Linear mixed model fit by REML
Formula: log10(NH4Nyeartwo) ~ 1 + sitett + (1 | pr)
   Data: data.sub
   AIC   BIC logLik deviance REMLdev
 30.37 34.73 -11.18    17.24   22.37
Random effects:
 Groups   Name        Variance Std.Dev.
 pr       (Intercept) 0.010942 0.10460
 Residual             0.130473 0.36121
Number of obs: 22, groups: pr, 12

Fixed effects:
                                                    Estimate Std. Error t value
(Intercept)                                    0.7305     0.1086   6.729
sitettToeAdditionsTreatment  -0.2784     0.1552  -1.794

[1] "95 % REML Confidence interval"
[1] -0.58261813  0.02578124

Gus Jespersen <jesper <at> u.washington.edu> writes:

>
> Thank you Thierry,
> I have looked through the glht function in multcomp, and have two
> further questions:
>

 [snip]

> Yet I get the following error message:
>
> Error in parse(text = ex[i]) : <text>:1:20: unexpected symbol
> 1: sitettMossAddition Treatment
>                                        ^
> Any ideas on what I'm doing wrong here?

  It is very likely that the glht function is having trouble
with the spaces in your level names.  I would strongly suggest
that you reformulate them as legal R variable names: something like

     levels(mydata$myfactor) <- make.names(levels(mydata$myfactor))
should work.

>
> (2) As you can see, I am working with a log10 transformed response
> variable.  I'd like to stay with this for homog. of variance reasons,
> and for reporting the "Treatment -Control" CI previously mentioned,
> I'd like to report the backtransformed limits of the CI.  At what
> point in this process should the back-transformation happen?  When
> attempting this calculation without glht, I am uncertain of where in
> the process to back-transform as well.  I had been hoping to simply
> use 10^ for each fixed effect and its SE, as well as each element of
> the vcov matrix, but I fear I am overlooking some basic math here.

  Yes, you are.  You need to back-transform the confidence intervals,
not the elements and their standard errors.  The basic math you
are thinking about is

  10^(est+1.96*stderr) !== 10^est+10^(1.96*stderr)

If you back-transform first, the RHS is what you will be doing;
you want the LHS (this is for the upper CI, the obvious parallel
applies for the lower CI)


-- 
R. Gus Jespersen
PhD Candidate
College of Forest Resources
University of Washington
Box 352100
Seattle, WA 98195-2100
(206) 543-5777
jesper at u.washington.edu


From jenn.s.barrett at gmail.com  Thu Jul 19 06:37:01 2012
From: jenn.s.barrett at gmail.com (Jennifer Barrett)
Date: Wed, 18 Jul 2012 21:37:01 -0700
Subject: [R-sig-ME] Zero-inflated mixed effects model - clarification of
 zeros modeled and R package questions
In-Reply-To: <4FF8C9D8.60702@gmail.com>
References: <4FF8C9D8.60702@gmail.com>
Message-ID: <CAEbqvwpsKmBbweMfEF5yS=YmZdBHCiFUHeHNsXVykt2Tfxx07Q@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120718/591ad5f7/attachment.pl>

From lborger at cebc.cnrs.fr  Thu Jul 19 09:06:10 2012
From: lborger at cebc.cnrs.fr (Luca Borger)
Date: Thu, 19 Jul 2012 09:06:10 +0200
Subject: [R-sig-ME] overdispersion in GLMMs
In-Reply-To: <CAHrfa+z6kD5CeJAGnXeqQXo_Ck6Br5NHWiDgmjycWBShzftPtg@mail.gmail.com>
References: <CAHrfa+z6kD5CeJAGnXeqQXo_Ck6Br5NHWiDgmjycWBShzftPtg@mail.gmail.com>
Message-ID: <5007B1E2.3000508@cebc.cnrs.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120719/a916a973/attachment.pl>

From highstat at highstat.com  Thu Jul 19 09:30:59 2012
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Thu, 19 Jul 2012 08:30:59 +0100
Subject: [R-sig-ME] Zero-inflated mixed effects model - clarification of,
 zeros modeled and R package questions
Message-ID: <5007B7B3.1010809@highstat.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120719/d7f9263e/attachment.pl>

From chantepie at mnhn.fr  Thu Jul 19 11:56:52 2012
From: chantepie at mnhn.fr (Stephane Chantepie)
Date: Thu, 19 Jul 2012 11:56:52 +0200
Subject: [R-sig-ME] MCMCglmm : Difference in additive genetic variance
	estimated in univariate vs bivariate models
Message-ID: <201207191156.52410.chantepie@mnhn.fr>

Dear all,

I have been running animal models to estimate whether Va of a trait was 
changing with age, and the correlation between this trait expressed in 
different age classes. The problem is I have a difference in additive genetic 
variance (Va) estimated in univariate vs bivariate models.

To be clearer: I have run an animal model on ?Spz_9? (spz trait for age 9) and 
obtained a Va = 0.15 (lower = 0.04; upper = 0.88). The intervals are pretty 
large because I do not have a lot animals in the pedigree (171 animals) but it 
seems that the model succeeds in estimating Va. When I run a bivariate model 
c(Spz_9,Spz_5), the Va estimations of spz_9 dramatically increases with Va = 
1.02 (lower = 0.35; upper = 1.71). The Va posterior distributions of the trait 
is well shaped, so I do not know whether there is a problem or not. Va 
increases when I run a bivariate model between spz_9 and some other traits 
(spz_2 or spz_3) and the Va values for spz_9 are also closed to 1.

I have used Gaussian distributions and informative prior (V =(Phenotypic 
variance/2) , nu =1 ), no fixed effect and (birth age +animal) random effects.

I am wondering what these differences mean? Could the problem come from a lack 
of information? Can the covariance between spz_9 and spz_5 be well estimated?

Many thanks in advance for your help,

All the best


From j.hadfield at ed.ac.uk  Thu Jul 19 12:16:38 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Thu, 19 Jul 2012 11:16:38 +0100
Subject: [R-sig-ME] MCMCglmm : Difference in additive genetic variance
 estimated in univariate vs bivariate models
In-Reply-To: <201207191156.52410.chantepie@mnhn.fr>
References: <201207191156.52410.chantepie@mnhn.fr>
Message-ID: <20120719111638.157216b0f5gdm1lw@www.staffmail.ed.ac.uk>

Hi,

Bivaraite analyses can increase the precision of Va estimates for  
traits with low heritability if the other trait has high heritability  
and a genetic correlation exists.

However, I would be very surprised  if this was the cause. More likely  
(if you only have so few individuals) your posterior is just  
reflecting your prior. If you have kept V and nu the same in the  
univariate and bivariate analyses then the marginal prior on the  
variances in the bivariate analysis (if V is diagonal) has nu-1 and  
V*nu/(nu-1) - if nu=1 then V*nu/(nu-1) is ....

Cheers,

Jarrod






Quoting Stephane Chantepie <chantepie at mnhn.fr> on Thu, 19 Jul 2012  
11:56:52 +0200:

> Dear all,
>
> I have been running animal models to estimate whether Va of a trait was
> changing with age, and the correlation between this trait expressed in
> different age classes. The problem is I have a difference in additive genetic
> variance (Va) estimated in univariate vs bivariate models.
>
> To be clearer: I have run an animal model on ?Spz_9? (spz trait for  
> age 9) and
> obtained a Va = 0.15 (lower = 0.04; upper = 0.88). The intervals are pretty
> large because I do not have a lot animals in the pedigree (171  
> animals) but it
> seems that the model succeeds in estimating Va. When I run a bivariate model
> c(Spz_9,Spz_5), the Va estimations of spz_9 dramatically increases with Va =
> 1.02 (lower = 0.35; upper = 1.71). The Va posterior distributions of  
> the trait
> is well shaped, so I do not know whether there is a problem or not. Va
> increases when I run a bivariate model between spz_9 and some other traits
> (spz_2 or spz_3) and the Va values for spz_9 are also closed to 1.
>
> I have used Gaussian distributions and informative prior (V =(Phenotypic
> variance/2) , nu =1 ), no fixed effect and (birth age +animal)  
> random effects.
>
> I am wondering what these differences mean? Could the problem come  
> from a lack
> of information? Can the covariance between spz_9 and spz_5 be well estimated?
>
> Many thanks in advance for your help,
>
> All the best
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From bbolker at gmail.com  Thu Jul 19 13:58:54 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 19 Jul 2012 11:58:54 +0000 (UTC)
Subject: [R-sig-ME] fixed effects/log transformations question
References: <CAL9m74bdhV4PSq=eSyzfa8KaH5Nz4SATGDvAaY3CKqu9kJi_NQ@mail.gmail.com>
Message-ID: <loom.20120719T134733-989@post.gmane.org>

Gus Jespersen <jesper <at> u.washington.edu> writes:

> 
> Dr. Bolker,
> Thanks for the response.  One last question with regard to negative t
> values when using log10 transformed data.  I am assuming the correct
> interpretation of the following output is: if the t value is negative
> and you're using log10 data, to get the fixed effect CI, you must add
> your own negative sign to 10^(est.+1/96*SE), such that the
> backtransformed CI from the output below would be:
> 
> ([1] "95 % REML Confidence interval"
> [1] -0.58261813  0.02578124
> 
> becomes
> 
> -.295       -1.05
> 
> Is this correct,
> 

  No, but the interpretation is a little bit subtle.  Here you
are working with (as far as I can tell) the back-transformed
confidence intervals on the effect of the treatment.
10^{-0.5826,0.02578} is {0.26,1.06} (where did you get 0.295??); 
this says that the lower CI is that the proportional effect of
the treatment is to multiply by 0.26 (a 74% decrease); the upper
CI is a 6% increase (you can subtract 1 from the CI values if you
want to get it in terms of proportional changes).
  If you were using the natural log (log_e) rather than the log10
scale, then you could interpret *small* (near zero) parameters as
being approximately equivalent to proportional changes (without
back-transforming), because exp(x)-1 is approximately x when
x is small ...

  For what it's worth, this isn't an R question, or a mixed-model
question, any more, it's become a general statistical question -- you
might try asking similar questions on http://stats.stackexchange.com
...

> Thanks again for the help
> Gus
> 
> [1] "###############NH4 Results Year Two##################"
> Data: data.sub
> Models:
> Mod.NH4.2.2: log10(NH4Nyeartwo) ~ 1 + (1 | pr)
> Mod.NH4.2.1: log10(NH4Nyeartwo) ~ 1 + sitett + (1 | pr)
>             Df    AIC    BIC   logLik  Chisq Chi Df Pr(>Chisq)
> Mod.NH4.2.2  3 26.427 29.700 -10.2136
> Mod.NH4.2.1  4 25.243 29.607  -8.6216 3.1841      1    0.07436 .
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> Linear mixed model fit by REML
> Formula: log10(NH4Nyeartwo) ~ 1 + sitett + (1 | pr)
>    Data: data.sub
>    AIC   BIC logLik deviance REMLdev
>  30.37 34.73 -11.18    17.24   22.37
> Random effects:
>  Groups   Name        Variance Std.Dev.
>  pr       (Intercept) 0.010942 0.10460
>  Residual             0.130473 0.36121
> Number of obs: 22, groups: pr, 12
> 
> Fixed effects:
>                                                     Estimate Std. Error t value
> (Intercept)                                    0.7305     0.1086   6.729
> sitettToeAdditionsTreatment  -0.2784     0.1552  -1.794
> 
> [1] "95 % REML Confidence interval"
> [1] -0.58261813  0.02578124


From yolande.tra at gmail.com  Thu Jul 19 15:28:22 2012
From: yolande.tra at gmail.com (Yolande Tra)
Date: Thu, 19 Jul 2012 09:28:22 -0400
Subject: [R-sig-ME] expert opinion on using lmer
Message-ID: <CAF=8oObNp+dMNbSjFEPDsGU81qfyrqZi6UzXZB9dGFK5=_q3Vg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120719/5e0b6fa3/attachment.pl>

From f.calboli at imperial.ac.uk  Thu Jul 19 15:40:54 2012
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Thu, 19 Jul 2012 14:40:54 +0100
Subject: [R-sig-ME] expert opinion on using lmer
In-Reply-To: <CAF=8oObNp+dMNbSjFEPDsGU81qfyrqZi6UzXZB9dGFK5=_q3Vg@mail.gmail.com>
References: <CAF=8oObNp+dMNbSjFEPDsGU81qfyrqZi6UzXZB9dGFK5=_q3Vg@mail.gmail.com>
Message-ID: <E1DA066B-F9AC-4FE1-894F-3440CF333727@imperial.ac.uk>

THIS IS NOT AN EXPERT OPINION but:

> I have the following design, counts were collected at different transects,
> different depths and different sites at different times. Time is continuous
> and assumed to be random, all the others are categorical fixed where
> transect is nested within depth which is nested within site.

I do find the idea that *nested* covariates should be seen as fixed to be odd.  What I would do is to create a NEW covariate for each site/depth/transect combination if I had to consider these covariates as fixed effects.  Secondly, if you are modelling 'counts of something' at time 'whatever' in your sites, I also find it odd that time is the random variable.  Basically, I would see time as fixed and the whole transect business as random.

Hence I would do something like

lmer(count ~ time + (time|a:b:c), family = 'poisson')

to have a random intercept for the nesting variable.

does it help?

BW


F
 

> 
> I would like an expert opinion about the following code where intercept is
> modeled as random (I am not sure if this is the right way).
> 
> a<-factor(transect)
> b<-factor(depth)
> c<-factor(site)
> g<-lmer(count~(1|time)+(time|a:b:c), family="poisson")
> 
> Thanks,
> Y
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

--
Federico C. F. Calboli
Neuroepidemiology and Ageing Research
Imperial College, St. Mary's Campus
Norfolk Place, London W2 1PG

Tel +44 (0)20 75941602   Fax +44 (0)20 75943193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com


From yolande.tra at gmail.com  Thu Jul 19 15:48:06 2012
From: yolande.tra at gmail.com (Yolande Tra)
Date: Thu, 19 Jul 2012 09:48:06 -0400
Subject: [R-sig-ME] expert opinion on using lmer
In-Reply-To: <E1DA066B-F9AC-4FE1-894F-3440CF333727@imperial.ac.uk>
References: <CAF=8oObNp+dMNbSjFEPDsGU81qfyrqZi6UzXZB9dGFK5=_q3Vg@mail.gmail.com>
	<E1DA066B-F9AC-4FE1-894F-3440CF333727@imperial.ac.uk>
Message-ID: <CAF=8oOa44KcULir29wfvJe5CTL3Q-vet7KGMC1fLPOOOWFj4hg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120719/c89ec763/attachment.pl>

From ecrone at fas.harvard.edu  Thu Jul 19 16:34:39 2012
From: ecrone at fas.harvard.edu (Elizabeth Crone)
Date: Thu, 19 Jul 2012 10:34:39 -0400
Subject: [R-sig-ME] overdispersion in GLMMs (Alejandro Mart?nez Abra?n)
Message-ID: <CA+YYKzAzL_Njy8L7H45K3ZmCJ0zhp-sWRsyERW=OAJ7XO_47SA@mail.gmail.com>

> 1. How can I estimate overdispersion in a Poisson GLMMM?
>
I usually fit overdispersed Poisson (or Binomial) GLMMs by adding a
unique identifier for each observation, then adding that unique ID as
a random term.  You can "test" how overdispersed the model is by
looking at the standard deviation assoicated with that random effect,
or by comparing the fit of a model with the overdispersion term to one
without it.

Alternatively, you can look at overdispersion due to random effects of
individual, plot, etc, using the same basic procedure.

I think I got the idea from the Gelman et al. Bayesian stats text.  I
would be curious to know if others do this also.

>
> 2. I am trying to run a quasipoisson GLMM using the lmer function and the
> lme4 library but I get a
> warning stating that "glmer cannot deal with quasi error families? Any tip?
> because I have seen this done.
>
My understanding is that this functionality has been removed, since it
is +/- redundant with the approach used above, but less naturally
linked to the mixed model framework.


-- 
****************************
Elizabeth E. Crone
Senior Ecologist, Harvard Forest
Harvard University
Petersham MA 01366
office: (978)756-6145
main: (978)724-3302
cell: (406)531-3498
FAX: (978)724-3595
email: ecrone at fas.harvard.edu


From chantepie at mnhn.fr  Thu Jul 19 17:20:24 2012
From: chantepie at mnhn.fr (Stephane Chantepie)
Date: Thu, 19 Jul 2012 17:20:24 +0200
Subject: [R-sig-ME] MCMCglmm : Difference in additive genetic variance
	estimated in univariate vs bivariate models
In-Reply-To: <20120719111638.157216b0f5gdm1lw@www.staffmail.ed.ac.uk>
References: <201207191156.52410.chantepie@mnhn.fr>
	<20120719111638.157216b0f5gdm1lw@www.staffmail.ed.ac.uk>
Message-ID: <201207191720.24705.chantepie@mnhn.fr>

Hi Jarrod and all other,

You said "your posterior is just reflecting your prior" : I was thinking about 
this issue but how can I test it? The V I used is V=(Phenotypic variance/2)= 
0.47 so it is bigger than the posterior mode. I have tried to used V=1 but the 
Va posterior results remains the same (Va=0.10 for the univariate model 
spz_9). 

For the bivariate model c(spz_9,spz_5) I have kept the same V=(Phenotypic 
variance/2)but nu=2.
To answer your question "if nu=1 then V*nu/(nu-1) is ..."  V*nu/0 so +? or -? 
;-)

Just to give you an idea of the my posteriors: 

The Va posterior mode resulting from the spz_9  univariate model hit the 0 
(http://ubuntuone.com/2IGcmcdqkjcVQCdZgvMhnP) whereas the Va posterior mode 
resulting from the (spz_9,spz_5) bivariate model seems to be better shaped(at 
the top http://ubuntuone.com/3PaBOJ5dF6kDIhV6ahPnlM).

To conclude, the estimation of Va with bivariate models is biologically 
consistent with senescence theories while the Va estimated with univariate 
model is not. So : Do do you think that I could use bivariate results or it is 
better to consider that I do not have enought information?

Thank a lot for your help

all the best

stephane


From matthias.suter at art.admin.ch  Thu Jul 19 11:43:23 2012
From: matthias.suter at art.admin.ch (Matthias Suter)
Date: Thu, 19 Jul 2012 09:43:23 +0000 (UTC)
Subject: [R-sig-ME] Covariance Matrix of fixed effects in lmer()
Message-ID: <loom.20120719T113435-253@post.gmane.org>

Is there a straight forward way, to get the scaled covariance matrix from a 
lmer()? 

E.g.
lmerout <- lmer(y ~ x1 + x2 + x3  + (1 | Groupingfactor), data)

summary(lmerout) gives the "Correlation of Fixed Effects"; I would like to 
have direct access to the covariance matrix.

As analogy in lm():

lmout <- lm(y ~ x1 + x2 + x3, data)

Here, the scaled covariance matrix is:

summary(lmout)$sigma^2 * summary(lmout)$cov.uns

Thanks for any helpful answer,
Matthias


From datkins at u.washington.edu  Thu Jul 19 17:57:25 2012
From: datkins at u.washington.edu (David Atkins)
Date: Thu, 19 Jul 2012 08:57:25 -0700
Subject: [R-sig-ME] overdispersion in GLMMs (Alejandro Mart?nez Abra?n)
In-Reply-To: <CA+YYKzAzL_Njy8L7H45K3ZmCJ0zhp-sWRsyERW=OAJ7XO_47SA@mail.gmail.com>
References: <CA+YYKzAzL_Njy8L7H45K3ZmCJ0zhp-sWRsyERW=OAJ7XO_47SA@mail.gmail.com>
Message-ID: <50082E65.8000901@u.washington.edu>


Elizabeth and Alejandro--

We discuss the over-dispersed Poisson mixed model (with per-observation 
random effect) in the following tutorial:

Atkins, D. C., Baldwin, S., Zheng, C., Gallop, R. J., & Neighbors, C. 
(in press). A tutorial on count regression and zero-altered count models 
for longitudinal substance use data. Psychology of Addictive Behaviors.

which you can find along with data and R code:

http://depts.washington.edu/cshrb/newweb/statstutorials.html

Hope that helps.

cheers, Dave

 > 1. How can I estimate overdispersion in a Poisson GLMMM?
 >
I usually fit overdispersed Poisson (or Binomial) GLMMs by adding a
unique identifier for each observation, then adding that unique ID as
a random term.  You can "test" how overdispersed the model is by
looking at the standard deviation assoicated with that random effect,
or by comparing the fit of a model with the overdispersion term to one
without it.

Alternatively, you can look at overdispersion due to random effects of
individual, plot, etc, using the same basic procedure.

I think I got the idea from the Gelman et al. Bayesian stats text.  I
would be curious to know if others do this also.

 >
 > 2. I am trying to run a quasipoisson GLMM using the lmer function and the
 > lme4 library but I get a
 > warning stating that "glmer cannot deal with quasi error families? 
Any tip?
 > because I have seen this done.
 >
My understanding is that this functionality has been removed, since it
is +/- redundant with the approach used above, but less naturally
linked to the mixed model framework.


-- 
****************************
Elizabeth E. Crone
Senior Ecologist, Harvard Forest
Harvard University
Petersham MA 01366
office: (978)756-6145
main: (978)724-3302
cell: (406)531-3498
FAX: (978)724-3595
email: ecrone at fas.harvard.edu


-- 
Dave Atkins, PhD
Research Associate Professor
Department of Psychiatry and Behavioral Science
University of Washington
datkins at u.washington.edu

Center for the Study of Health and Risk Behaviors (CSHRB)		
1100 NE 45th Street, Suite 300 	
Seattle, WA  98105 	
206-616-3879 	
http://depts.washington.edu/cshrb/
(Mon-Wed)	

Center for Healthcare Improvement, for Addictions, Mental Illness,
   Medically Vulnerable Populations (CHAMMP)
325 9th Avenue, 2HH-15
Box 359911
Seattle, WA 98104
http://www.chammp.org
(Thurs)


From bbolker at gmail.com  Thu Jul 19 19:52:52 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 19 Jul 2012 17:52:52 +0000 (UTC)
Subject: [R-sig-ME] Covariance Matrix of fixed effects in lmer()
References: <loom.20120719T113435-253@post.gmane.org>
Message-ID: <loom.20120719T194504-61@post.gmane.org>

Matthias Suter <matthias.suter at ...> writes:

> 
> Is there a straight forward way, to get the scaled covariance matrix from a 
> lmer()? 
> 
> E.g.
> lmerout <- lmer(y ~ x1 + x2 + x3  + (1 | Groupingfactor), data)
> 
> summary(lmerout) gives the "Correlation of Fixed Effects"; I would like to 
> have direct access to the covariance matrix.
> 
> As analogy in lm():
> 
> lmout <- lm(y ~ x1 + x2 + x3, data)
> 
> Here, the scaled covariance matrix is:
> 
> summary(lmout)$sigma^2 * summary(lmout)$cov.uns
> 
> Thanks for any helpful answer,
> Matthias
> 
> 

  It sounds like you want vcov() ... ?  (vcov() works for
lm() results, and many other model types, too ...

  methods(class="mer")
  showMethods(class="mer") ## S4 methods

## or if using development lme4 from r-forge:
  methods(class="merMod")


e.g.

(fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
vcov(fm1)

2 x 2 Matrix of class "dpoMatrix"
            (Intercept)      Days
(Intercept)   46.574978 -1.451084
Days          -1.451084  2.389469

  cheers
   Ben Bolker


From yolande.tra at gmail.com  Thu Jul 19 22:04:46 2012
From: yolande.tra at gmail.com (Yolande Tra)
Date: Thu, 19 Jul 2012 16:04:46 -0400
Subject: [R-sig-ME] lmer, no residual in the output - want REML not laplace
Message-ID: <CAF=8oOZ6QB2aM0m_JR+0c5DKo-1837D9eRdOPyiihjunFENaHA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120719/90c66fee/attachment.pl>

From bbolker at gmail.com  Thu Jul 19 23:30:54 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 19 Jul 2012 21:30:54 +0000 (UTC)
Subject: [R-sig-ME] lmer,
	no residual in the output - want REML not laplace
References: <CAF=8oOZ6QB2aM0m_JR+0c5DKo-1837D9eRdOPyiihjunFENaHA@mail.gmail.com>
Message-ID: <loom.20120719T232740-938@post.gmane.org>

Yolande Tra <yolande.tra at ...> writes:

> 
> Hello,
> 
> I run the following code.
> 1. REML fit was not tin the output

  As a recent post on this list stated, the definition of REML
is somewhat unclear for GLMMs. (Dave Fournier has given a reasonable
definition in the past, but not all researchers in this area agree
with his definition.)  If you can specify exactly what you want
the code to do in order to implement restricted ML for a GLMM, 
that might spark some discussion.

> 2. There was no residual in the output

  How about residuals(g) ?

> 3. I could not run anova(g)
> 
> > (g=lmer(total_count ~ c+(1|c:b:a), d2, REML=TRUE, family = "poisson"))
> Generalized linear mixed model fit by the Laplace approximation
> Formula: total_count ~ c + (1 | c:b:a)
>    Data: d2
>    AIC   BIC logLik deviance
>  661.9 673.1 -326.9    653.9
> Random effects:
>  Groups Name        Variance Std.Dev.
>  c:b:a  (Intercept) 1.8368   1.3553
> Number of obs: 122, groups: c:b:a, 18
> Fixed effects:
>               Estimate Std. Error z value Pr(>|z|)
> (Intercept)     1.6383     0.5605   2.923  0.00347 **
> cLovers Point   0.2080     0.7924   0.262  0.79295
> cPoint Pinos   -0.4282     0.7998  -0.535  0.59242
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> Correlation of Fixed Effects:
>             (Intr) cLvrsP
> cLoversPont -0.707
> cPointPinos -0.701  0.496
> > anova(g)
> Error in anova(g) : single argument anova for GLMMs not yet implemented
> What might be wrong?

  Maybe it's not implemented?  What do you want it to do?
  Perhaps try fitting a reduced model (g2 <- update(g2,.~.-c)
and go from there?  Or try drop1() ?

  Ben Bolker


From j.hadfield at ed.ac.uk  Fri Jul 20 11:32:00 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Fri, 20 Jul 2012 10:32:00 +0100
Subject: [R-sig-ME] MCMCglmm: priors for ordinal regression
In-Reply-To: <2483687.426621342429485062.JavaMail.defaultUser@defaultHost>
References: <2483687.426621342429485062.JavaMail.defaultUser@defaultHost>
Message-ID: <20120720103200.66280v4zqxsj7aus@www.staffmail.ed.ac.uk>

Dear Massimo,

The function below (prior.scale) may be useful. It returns a  
covariance matrix that can be passed to prior$B$V.  If the predictors  
had been scaled according to the procedure outlined in Gelman et al.  
(2006) then this would induce an identity prior covariance matrix on  
the new regression coefficients (i.e. they're iid a priori). Gelman  
recommends a scaled Cauchy or scaled t prior on these new regression  
ceofficients. I don't think this is possible in MCMCglmm* but a normal  
with variance equal to the sum of the variance components + 1 (probit)  
  pi^2/3 (logit) I think is not too unreasonable - but it does put  
less weight on very extreme probabilities than the Cauchy and t. To  
achieve this just multiply the output of prior.scale by the variance  
you want to use.

Bear in mind that the inputs are scaled, not the columns of the design  
matrix - this means that interactions are penalised more than main  
effects. In my field just-so stories involving 'significant'  
interactions  are often used to resuscitate a `failed' experiment, so  
penalising them by default may be no bad thing.

Cheers,

Jarrod

*setting a weaker prior on the non-identified residual variance rather  
than fixing it may achieve a t-prior - but really not sure

Gelman et al. A WEAKLY INFORMATIVE DEFAULT PRIOR DISTRIBUTION FOR
LOGISTIC AND OTHER REGRESSION MODELS The Annals of Applied Statistics
2008, Vol. 2, No. 4, 1360?1383

# formula = the fixed formula in the model
# data = the data set

prior.scale<-function(formula, data){
   X1<-model.matrix(formula, data)
   X2<-get_all_vars(formula, data)
   X2<-as.data.frame(lapply(X2, function(x){if(is.numeric(x)){scale(x,  
scale=sd(x)*2)}else{x}}))
   X2<-model.matrix(formula, data=X2)
   X2[,-1]<-apply(X2[,-1], 2,  
function(x){if(any(!x%in%c(0,1))){x}else{scale(x,  
center=sum(x)/length(x), scale=1)}})
   crossprod(t(solve(t(X1)%*%X1, t(X1)%*%X2)))
}


Quoting "m.fenati at libero.it" <m.fenati at libero.it> on Mon, 16 Jul 2012  
11:04:45 +0200 (CEST):

>
> Thank you again for your great suggestions.
>
> Regards
>
> Massimo
>
>
>
>> ----Messaggio originale----
>> Da: j.hadfield at ed.ac.uk
>> Data: 12/07/2012 15.20
>> A: "m.fenati at libero.it"<m.fenati at libero.it>
>> Cc: <r-sig-mixed-models at r-project.org>
>> Ogg: Re: [R-sig-ME] MCMCglmm: priors for ordinal regression
>>
>> Hi,
>>
>> If the prior variance on your fixed effects is V+1 where V is the sum
>> of the variance components (including the residual) then the marginal
>> prior on the fixed effects is as flat as possible on the probability
>> interval (0,1). However, you have to set up the contrasts correctly.
>>
>> If you still get numerical problems I'm afraid you will have to find
>> another way of doing the analysis. I have no solution, and no one has
>> suggested any:
>>
>> https://stat.ethz.ch/pipermail/r-sig-mixed-models/2012q1/017976.html
>>
>> Cheers,
>>
>> Jarrod
>>
>>
>>
>> Quoting "m.fenati at libero.it" <m.fenati at libero.it> on Mon, 9 Jul 2012
>> 12:44:59 +0200 (CEST):
>>
>>> Dear Jarrod,
>>> thank you for your fast answer.
>>> Yes, I had converegence (presence of trend of the time series).
>>> Unfortunately,
>>> I have ordinal data with near complete separation.
>>> My aim is to set a poorly informative or uninformative priors for
>>> fixed effect
>>> in order to improve the chain convergence. Then I set
>>> piorB=list(mu=c(rep(0,6)),
>>> V=diag(6)*(100)). The choice of V=100 is not based on other logical or
>>> numerical reasons.
>>> I try to display the posterior distribution of latent variable (pl=T), but
> I
>>> had a wide range of -25 + 25.....
>>> How can I do? Could you help me to choose the right prior?
>>>
>>> Thank in advance
>>>
>>> Massimo
>>>
>>>
>>>
>>>> ----Messaggio originale----
>>>> Da: j.hadfield at ed.ac.uk
>>>> Data: 08/07/2012 12.20
>>>> A: "m.fenati at libero.it"<m.fenati at libero.it>
>>>> Cc: <r-sig-mixed-models at r-project.org>
>>>> Ogg: Re: [R-sig-ME] MCMCglmm: priors for ordinal regression
>>>>
>>>> Dear Massimo,
>>>>
>>>> Do you mean the chain did not converge or the chain did not mix?
>>>> Generally the former is rare, and is usually only seen with
>>>> ordinal/categorical data with complete (or near complete) separation.
>>>> Sometimes a prior that constrains the linear predictor away from
>>>> extreme values on the logit/probit scale can fix this with a
>>>> relatively minor prior influence on inferences made on the data scale.
>>>> Sometimes not. Its not clear to me what the motivation is behind your
>>>> prior - is it that the sum of your variance components is close to
>>>> 100? If so I would be careful. Use pl=TRUE in your call to MCMCglmm
>>>> and make sure your latent variables are in the range -7 to 7.
>>>>
>>>> Cheers,
>>>>
>>>> Jarrod
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> Quoting "m.fenati at libero.it" <m.fenati at libero.it> on Wed, 4 Jul 2012
>>>> 16:48:18 +0200 (CEST):
>>>>
>>>>>
>>>>> Dear R user,
>>>>> I have some problems about prior definition in MCMCglmm ordinal
>>>>> regression. I've tried to use what Jarrod wrote about not
>>>>> informative priors for ordinal probit but my model did not converge:
>>>>>
>>>>>
>>>>> prior=list(R=list(V= 1, fix=1), G=list(G1=list(V=1, nu=0)))
>>>>>
>>>>>
>>>>> where "..left the default prior for the fixed effects (not
>>>>> explicitly specified)..".
>>>>>
>>>>>
>>>>> Then, in order to have however a similar uniform distribution for
>>>>> the latent variable, I set prior for fixed effect  as "mu=0" and
>>>>> "(co)variance=100":
>>>>>
>>>>>
>>>>> priorB<-rnorm(1000, 0, sqrt(100))
>>>>> priorMB<-1:1000
>>>>> for(i in 1:1000){
>>>>>   priorMB[i]<-mean(pnorm(priorB[i]+rnorm(1000,0,sqrt(100))))
>>>>>    }
>>>>> hist(priorMB)
>>>>>
>>>>>
>>>>> The model converge well but I've some dobts. Is it correct or not?
>>>>>
>>>>>
>>>>> Thank you very much for any suggestions or comments.
>>>>>
>>>>>
>>>>> Best regards
>>>>>
>>>>>
>>>>> Massimo
>>>>> 	[[alternative HTML version deleted]]
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>
>>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> The University of Edinburgh is a charitable body, registered in
>>>> Scotland, with registration number SC005336.
>>>>
>>>>
>>>>
>>>
>>>
>>>
>>
>>
>>
>> --
>> The University of Edinburgh is a charitable body, registered in
>> Scotland, with registration number SC005336.
>>
>>
>>
>
>
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From j.hadfield at ed.ac.uk  Fri Jul 20 11:53:03 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Fri, 20 Jul 2012 10:53:03 +0100
Subject: [R-sig-ME] MCMCglmm : Difference in additive genetic variance
 estimated in univariate vs bivariate models
In-Reply-To: <201207191720.24705.chantepie@mnhn.fr>
References: <201207191156.52410.chantepie@mnhn.fr>
	<20120719111638.157216b0f5gdm1lw@www.staffmail.ed.ac.uk>
	<201207191720.24705.chantepie@mnhn.fr>
Message-ID: <20120720105303.656167gk3o63guo8@www.staffmail.ed.ac.uk>

Hi Stephane,

If I am worried about prior influence and my model is multi-trait  
Gaussian I often run it through ASReml as a check.  Generally I find  
parameter expanded priors to be weaker than the inverse-Wishart and  
usually use them. Unfortunately I haven't seen much work done  
regarding their properties for covariance matrices. However, having

V=diag(n), nu=n, alpha.mu=c(0,0), alpha.V=diag(n)*s

where n is the dimension of the matrix and s a scale parameter  
(assuming the traits are on the same scale) seems to work well under  
at least some circumstances. Even with low replication the posterior  
modes are close to their REML estimates, and if one of the variance is  
close to zero then the posterior for the correlation is close to being  
uniform on the -1/1 interval, as you would hope  - approximate  
standard errors for this correlation obtained from REML analyses often  
seem to be anti-conservative.

Cheers,

Jarrod





Quoting Stephane Chantepie <chantepie at mnhn.fr> on Thu, 19 Jul 2012  
17:20:24 +0200:

> Hi Jarrod and all other,
>
> You said "your posterior is just reflecting your prior" : I was  
> thinking about
> this issue but how can I test it? The V I used is V=(Phenotypic variance/2)=
> 0.47 so it is bigger than the posterior mode. I have tried to used  
> V=1 but the
> Va posterior results remains the same (Va=0.10 for the univariate model
> spz_9).
>
> For the bivariate model c(spz_9,spz_5) I have kept the same V=(Phenotypic
> variance/2)but nu=2.
> To answer your question "if nu=1 then V*nu/(nu-1) is ..."  V*nu/0 so +? or -?
> ;-)
>
> Just to give you an idea of the my posteriors:
>
> The Va posterior mode resulting from the spz_9  univariate model hit the 0
> (http://ubuntuone.com/2IGcmcdqkjcVQCdZgvMhnP) whereas the Va posterior mode
> resulting from the (spz_9,spz_5) bivariate model seems to be better shaped(at
> the top http://ubuntuone.com/3PaBOJ5dF6kDIhV6ahPnlM).
>
> To conclude, the estimation of Va with bivariate models is biologically
> consistent with senescence theories while the Va estimated with univariate
> model is not. So : Do do you think that I could use bivariate  
> results or it is
> better to consider that I do not have enought information?
>
> Thank a lot for your help
>
> all the best
>
> stephane
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From m.fenati at libero.it  Fri Jul 20 15:07:02 2012
From: m.fenati at libero.it (m.fenati at libero.it)
Date: Fri, 20 Jul 2012 15:07:02 +0200 (CEST)
Subject: [R-sig-ME] MCMCglmm: priors for ordinal regression
Message-ID: <28127168.2410191342789622799.JavaMail.defaultUser@defaultHost>

I will try as soon as possible.
I am very interested to include MCMCglmm as possible tool for my analyses 
where ordinal data are very usual.
Thanks again. 
Massimo

>----Messaggio originale----
>Da: j.hadfield at ed.ac.uk
>Data: 20/07/2012 11.32
>A: "m.fenati at libero.it"<m.fenati at libero.it>
>Cc: <r-sig-mixed-models at r-project.org>
>Ogg: Re: [R-sig-ME] MCMCglmm: priors for ordinal regression
>
>Dear Massimo,
>
>The function below (prior.scale) may be useful. It returns a  
>covariance matrix that can be passed to prior$B$V.  If the predictors  
>had been scaled according to the procedure outlined in Gelman et al.  
>(2006) then this would induce an identity prior covariance matrix on  
>the new regression coefficients (i.e. they're iid a priori). Gelman  
>recommends a scaled Cauchy or scaled t prior on these new regression  
>ceofficients. I don't think this is possible in MCMCglmm* but a normal  
>with variance equal to the sum of the variance components + 1 (probit)  
>  pi^2/3 (logit) I think is not too unreasonable - but it does put  
>less weight on very extreme probabilities than the Cauchy and t. To  
>achieve this just multiply the output of prior.scale by the variance  
>you want to use.
>
>Bear in mind that the inputs are scaled, not the columns of the design  
>matrix - this means that interactions are penalised more than main  
>effects. In my field just-so stories involving 'significant'  
>interactions  are often used to resuscitate a `failed' experiment, so  
>penalising them by default may be no bad thing.
>
>Cheers,
>
>Jarrod
>
>*setting a weaker prior on the non-identified residual variance rather  
>than fixing it may achieve a t-prior - but really not sure
>
>Gelman et al. A WEAKLY INFORMATIVE DEFAULT PRIOR DISTRIBUTION FOR
>LOGISTIC AND OTHER REGRESSION MODELS The Annals of Applied Statistics
>2008, Vol. 2, No. 4, 1360?1383
>
># formula = the fixed formula in the model
># data = the data set
>
>prior.scale<-function(formula, data){
>   X1<-model.matrix(formula, data)
>   X2<-get_all_vars(formula, data)
>   X2<-as.data.frame(lapply(X2, function(x){if(is.numeric(x)){scale(x,  
>scale=sd(x)*2)}else{x}}))
>   X2<-model.matrix(formula, data=X2)
>   X2[,-1]<-apply(X2[,-1], 2,  
>function(x){if(any(!x%in%c(0,1))){x}else{scale(x,  
>center=sum(x)/length(x), scale=1)}})
>   crossprod(t(solve(t(X1)%*%X1, t(X1)%*%X2)))
>}
>
>
>Quoting "m.fenati at libero.it" <m.fenati at libero.it> on Mon, 16 Jul 2012  
>11:04:45 +0200 (CEST):
>
>>
>> Thank you again for your great suggestions.
>>
>> Regards
>>
>> Massimo
>>
>>
>>
>>> ----Messaggio originale----
>>> Da: j.hadfield at ed.ac.uk
>>> Data: 12/07/2012 15.20
>>> A: "m.fenati at libero.it"<m.fenati at libero.it>
>>> Cc: <r-sig-mixed-models at r-project.org>
>>> Ogg: Re: [R-sig-ME] MCMCglmm: priors for ordinal regression
>>>
>>> Hi,
>>>
>>> If the prior variance on your fixed effects is V+1 where V is the sum
>>> of the variance components (including the residual) then the marginal
>>> prior on the fixed effects is as flat as possible on the probability
>>> interval (0,1). However, you have to set up the contrasts correctly.
>>>
>>> If you still get numerical problems I'm afraid you will have to find
>>> another way of doing the analysis. I have no solution, and no one has
>>> suggested any:
>>>
>>> https://stat.ethz.ch/pipermail/r-sig-mixed-models/2012q1/017976.html
>>>
>>> Cheers,
>>>
>>> Jarrod
>>>
>>>
>>>
>>> Quoting "m.fenati at libero.it" <m.fenati at libero.it> on Mon, 9 Jul 2012
>>> 12:44:59 +0200 (CEST):
>>>
>>>> Dear Jarrod,
>>>> thank you for your fast answer.
>>>> Yes, I had converegence (presence of trend of the time series).
>>>> Unfortunately,
>>>> I have ordinal data with near complete separation.
>>>> My aim is to set a poorly informative or uninformative priors for
>>>> fixed effect
>>>> in order to improve the chain convergence. Then I set
>>>> piorB=list(mu=c(rep(0,6)),
>>>> V=diag(6)*(100)). The choice of V=100 is not based on other logical or
>>>> numerical reasons.
>>>> I try to display the posterior distribution of latent variable (pl=T), 
but
>> I
>>>> had a wide range of -25 + 25.....
>>>> How can I do? Could you help me to choose the right prior?
>>>>
>>>> Thank in advance
>>>>
>>>> Massimo
>>>>
>>>>
>>>>
>>>>> ----Messaggio originale----
>>>>> Da: j.hadfield at ed.ac.uk
>>>>> Data: 08/07/2012 12.20
>>>>> A: "m.fenati at libero.it"<m.fenati at libero.it>
>>>>> Cc: <r-sig-mixed-models at r-project.org>
>>>>> Ogg: Re: [R-sig-ME] MCMCglmm: priors for ordinal regression
>>>>>
>>>>> Dear Massimo,
>>>>>
>>>>> Do you mean the chain did not converge or the chain did not mix?
>>>>> Generally the former is rare, and is usually only seen with
>>>>> ordinal/categorical data with complete (or near complete) separation.
>>>>> Sometimes a prior that constrains the linear predictor away from
>>>>> extreme values on the logit/probit scale can fix this with a
>>>>> relatively minor prior influence on inferences made on the data scale.
>>>>> Sometimes not. Its not clear to me what the motivation is behind your
>>>>> prior - is it that the sum of your variance components is close to
>>>>> 100? If so I would be careful. Use pl=TRUE in your call to MCMCglmm
>>>>> and make sure your latent variables are in the range -7 to 7.
>>>>>
>>>>> Cheers,
>>>>>
>>>>> Jarrod
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> Quoting "m.fenati at libero.it" <m.fenati at libero.it> on Wed, 4 Jul 2012
>>>>> 16:48:18 +0200 (CEST):
>>>>>
>>>>>>
>>>>>> Dear R user,
>>>>>> I have some problems about prior definition in MCMCglmm ordinal
>>>>>> regression. I've tried to use what Jarrod wrote about not
>>>>>> informative priors for ordinal probit but my model did not converge:
>>>>>>
>>>>>>
>>>>>> prior=list(R=list(V= 1, fix=1), G=list(G1=list(V=1, nu=0)))
>>>>>>
>>>>>>
>>>>>> where "..left the default prior for the fixed effects (not
>>>>>> explicitly specified)..".
>>>>>>
>>>>>>
>>>>>> Then, in order to have however a similar uniform distribution for
>>>>>> the latent variable, I set prior for fixed effect  as "mu=0" and
>>>>>> "(co)variance=100":
>>>>>>
>>>>>>
>>>>>> priorB<-rnorm(1000, 0, sqrt(100))
>>>>>> priorMB<-1:1000
>>>>>> for(i in 1:1000){
>>>>>>   priorMB[i]<-mean(pnorm(priorB[i]+rnorm(1000,0,sqrt(100))))
>>>>>>    }
>>>>>> hist(priorMB)
>>>>>>
>>>>>>
>>>>>> The model converge well but I've some dobts. Is it correct or not?
>>>>>>
>>>>>>
>>>>>> Thank you very much for any suggestions or comments.
>>>>>>
>>>>>>
>>>>>> Best regards
>>>>>>
>>>>>>
>>>>>> Massimo
>>>>>> 	[[alternative HTML version deleted]]
>>>>>>
>>>>>> _______________________________________________
>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>
>>>>>>
>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> The University of Edinburgh is a charitable body, registered in
>>>>> Scotland, with registration number SC005336.
>>>>>
>>>>>
>>>>>
>>>>
>>>>
>>>>
>>>
>>>
>>>
>>> --
>>> The University of Edinburgh is a charitable body, registered in
>>> Scotland, with registration number SC005336.
>>>
>>>
>>>
>>
>>
>>
>>
>
>
>
>-- 
>The University of Edinburgh is a charitable body, registered in
>Scotland, with registration number SC005336.
>
>
>


From Javier.Perez-Barberia at hutton.ac.uk  Fri Jul 20 17:00:48 2012
From: Javier.Perez-Barberia at hutton.ac.uk (Javier Perez-Barberia)
Date: Fri, 20 Jul 2012 15:00:48 +0000
Subject: [R-sig-ME] MCMCglmm: anova table
Message-ID: <EBB9FB26C0C0184E86180D82D00279A13D82AEBF@ABEXC02.ad.hutton.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120720/4ffa6b21/attachment.pl>

From amanda_henry at eva.mpg.de  Fri Jul 20 17:08:18 2012
From: amanda_henry at eva.mpg.de (Amanda Henry)
Date: Fri, 20 Jul 2012 15:08:18 +0000 (UTC)
Subject: [R-sig-ME] nlmer "Error: object of type 'symbol' is not subsettable"
Message-ID: <loom.20120720T165302-463@post.gmane.org>

Hi all,

I want to test the effect of "species", "tech", and "area" on "types",
controlling for "microfossil", and "sampletype", with "site" as a random effect,
which should have random slope wrt "microfossil", as well as weighting the
response by "microfossil".
Microfossil and type are quantitative, the rest are factors. I have transformed
"microfossil" to approximate a uniform distribution, it appears as "z.micro" in
the formula. 

My data is structured as follows

'data.frame':	210 obs. of  9 variables:
 $ sample     : Factor w/ 207 levels "Abri Pataud left M2",..: 116 117 118 119
120 121 122 123 124 125 ...
 $ site       : Factor w/ 20 levels "Arcy","Blombos",..: 18 18 18 18 18 18 18 18
18 18 ...
 $ sampletype : Factor w/ 2 levels "calculus","tool": 2 2 2 2 2 2 2 2 2 2 ...
 $ species    : Factor w/ 2 levels "MH","N": 1 1 1 1 1 1 1 1 1 1 ...
 $ tech       : Factor w/ 3 levels "MP/MSA","N","UP/LSA": 3 3 3 3 3 3 3 3 3 3 ...
 $ area       : Factor w/ 3 levels "A","E","NE": 3 3 3 3 3 3 3 3 3 3 ...
 $ microfossil: int  7 16 0 4 5 10 6 1 25 1 ...
 $ type       : int  7 8 0 4 2 7 5 1 16 1 ...
 $ z.micro    : num  1.26 1.881 -1.061 0.853 1.013 ...


I have used the following function

res=nlmer(type~species+tech+area+sampletype+z.micro+
  (1|site)+(0+z.micro|site), weights=(microfossil+1),
         family=poisson, data=tdata)

None of the categorical predictors or control factors (species, tech, area,
sampletype) are perfectly nested or crossed with each other or with site.  

I have included "microfossil+1" as the weight because there are zero counts in
microfossil and that gives an error.

I receive the error: 
Error: object of type 'symbol' is not subsettable

I have seen only one other instance of this problem mentioned on this mailing
list, but it was not answered. 

Any suggestions?

Thanks for your help,
Amanda


From bates at stat.wisc.edu  Fri Jul 20 17:51:54 2012
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 20 Jul 2012 10:51:54 -0500
Subject: [R-sig-ME] lmer output shows laplace approximation not reml
In-Reply-To: <CAF=8oObZqK8BcoGRopkag7DH0jrO26iruW-pG2MyYU5eKqKrxw@mail.gmail.com>
References: <CAF=8oObZqK8BcoGRopkag7DH0jrO26iruW-pG2MyYU5eKqKrxw@mail.gmail.com>
Message-ID: <CAO7JsnTaEryfDg8m1TZ-=BkxPexbjFnZycQeREvJSfP1QjjZbQ@mail.gmail.com>

On Thu, Jul 19, 2012 at 9:20 PM, Yolande Tra <yolande.tra at gmail.com> wrote:
>
> Dear Douglas,
>
> I am sorry to bother you but this is very important. I posted the following question (in a slight different version) at r-sig-ME question list but it seems no one is able to answer it.

But Ben answered it.  When you specify family="poisson" you are
fitting a generalized linear mixed model.  The parameter estimates
provided for such a model by lme4 are the maximum likelihood
estimates, up to an approximation.  The default approximation is the
Laplace approximation.


 This data has quite complicated design. I did not find any example
that is similar in the literature on lme4. According to the
investigator this is a partial nested design. Counts were collected at
different transects, different depths and different sites at different
times. Time is continuous and assumed to be random, all the others are
categorical fixed where transect is nested within depth which is
nested within site. Definitely the three factors are nested within
each other but based on the the attached files and the table below, it
looks like this a repeated measurement design where time (dive_id) is
nested within the three factor level combination. So far if I am
wrong, please correct me. I believe the main effect is site (b) and
level (a) is nested within depth(b) which in turn is nested within
site(b). dive_id which represents also time is random.
> I read some examples you gave. My output is different.
> 1. The fit is done with Laplace approximation, not REML
> 2. There is no residual random effect
> 3. anova(g) did not give any output
>
> In this table the cell represents the number of times each combination was used to obtain the counts (based on the attached file).
>
>
>
>
> Hopkins
>
> Lovers Point
>
> Point Pinos
>
> Total
>
> 5
>
> B
>
> 8
>
> 6
>
> 6
>
> 20
>
> M
>
> 8
>
> 6
>
> 6
>
> 20
>
> Total
>
> 16
>
> 12
>
> 12
>
> 40
>
> 10
>
> B
>
> 7
>
> 6
>
> 7
>
> 20
>
> M
>
> 7
>
> 6
>
> 7
>
> 20
>
> Total
>
> 14
>
> 12
>
> 14
>
> 40
>
> 15
>
> B
>
> 7
>
> 6
>
> 8
>
> 21
>
> M
>
> 7
>
> 6
>
> 8
>
> 21
>
> Total
>
> 14
>
> 12
>
> 16
>
> 42
>
> Total
>
> 44
>
> 36
>
> 42
>
> 122
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> d2 <- read.csv(file.path(dataDir,"aggregate_2008.csv"), as.is=T,stringsAsFactors = FALSE)
> > a<-factor(d2$level)
> > b<-factor(d2$site)
> > c<-factor(d2$depth)
> > g=lmer(total_count ~ b+(1|b:c)+(1|b:c:a)+(1|dive_id), d2, REML=TRUE,family = "poisson")
> > summary(g)
> Generalized linear mixed model fit by the Laplace approximation
> Formula: total_count ~ b + (1 | b:c) + (1 | b:c:a) + (1 | dive_id)
>    Data: d2
>   AIC  BIC logLik deviance
>  1153 1169 -570.3     1141
> Random effects:
>  Groups  Name        Variance Std.Dev.
>  dive_id (Intercept) 0.60707  0.77915
>  b:c:a   (Intercept) 0.16273  0.40340
>  b:c     (Intercept) 0.16273  0.40340
> Number of obs: 122, groups: dive_id, 61; b:c:a, 9; b:c, 9
>
> Fixed effects:
>               Estimate Std. Error z value Pr(>|z|)
> (Intercept)    1.98724    0.37388   5.315 1.07e-07 ***
> bLovers Point  0.02358    0.53618   0.044    0.965
> bPoint Pinos  -0.43114    0.53273  -0.809    0.418
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> Correlation of Fixed Effects:
>             (Intr) bLvrsP
> bLoversPont -0.697
> bPointPinos -0.702  0.489
>
> > anova(g)
> Error in anova(g) : single argument anova for GLMMs not yet implemented
>
> I really appreciate any of your insight as author of the package lme4.
>
> Yolande


From yolande.tra at gmail.com  Fri Jul 20 18:39:36 2012
From: yolande.tra at gmail.com (Yolande Tra)
Date: Fri, 20 Jul 2012 12:39:36 -0400
Subject: [R-sig-ME] lmer output shows laplace approximation not reml
In-Reply-To: <CAO7JsnTaEryfDg8m1TZ-=BkxPexbjFnZycQeREvJSfP1QjjZbQ@mail.gmail.com>
References: <CAF=8oObZqK8BcoGRopkag7DH0jrO26iruW-pG2MyYU5eKqKrxw@mail.gmail.com>
	<CAO7JsnTaEryfDg8m1TZ-=BkxPexbjFnZycQeREvJSfP1QjjZbQ@mail.gmail.com>
Message-ID: <CAF=8oOZTnPEJ2dY8yjNbA+hpwYUDdE-s0PjEbmTWOmbAMB=n9w@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120720/4c06c001/attachment.pl>

From jwiley.psych at gmail.com  Fri Jul 20 18:56:11 2012
From: jwiley.psych at gmail.com (Joshua Wiley)
Date: Fri, 20 Jul 2012 09:56:11 -0700
Subject: [R-sig-ME] lmer output shows laplace approximation not reml
In-Reply-To: <CAF=8oOZTnPEJ2dY8yjNbA+hpwYUDdE-s0PjEbmTWOmbAMB=n9w@mail.gmail.com>
References: <CAF=8oObZqK8BcoGRopkag7DH0jrO26iruW-pG2MyYU5eKqKrxw@mail.gmail.com>
	<CAO7JsnTaEryfDg8m1TZ-=BkxPexbjFnZycQeREvJSfP1QjjZbQ@mail.gmail.com>
	<CAF=8oOZTnPEJ2dY8yjNbA+hpwYUDdE-s0PjEbmTWOmbAMB=n9w@mail.gmail.com>
Message-ID: <CANz9Z_+qoA63gcftkpZnDO7YkaXxVtrx4zkRYsAQGe++UUyieQ@mail.gmail.com>

Hi Yolande,

It is not clear what REML is with GLMMs.  In LMMs, REML maximizes the
liklihood that only depends only on the variance components by
conditioning on the fixed effects.  AFAIK (Dr. Bates will hopefully
correct me if I am wrong or step in with a more thorough explanation)
it is unclear what the conditional distribution that depends only on
variance components would be with nonlinear models.  The only
approaches I know of that still do something REMLish use iterative
linear approximations (e.g., the glimmix macro did this with repeated
underlying calls to proc mixed in SAS).

So, I do not think it is surprising that with a GLMM (which mixed
effects poisson is), REML does nothing.

Cheers,

Josh

On Fri, Jul 20, 2012 at 9:39 AM, Yolande Tra <yolande.tra at gmail.com> wrote:
> Thank you for your reply.
> Even though I specify REML=TRUE in the code, the fit was not done with REML.
> My last question also was why I could not get an ANOVA output for the fixed
> effects
>
>> anova(g)
> Error in anova(g) : single argument anova for GLMMs not yet implemented
>
> Y
> On Fri, Jul 20, 2012 at 11:51 AM, Douglas Bates <bates at stat.wisc.edu> wrote:
>
>> On Thu, Jul 19, 2012 at 9:20 PM, Yolande Tra <yolande.tra at gmail.com>
>> wrote:
>> >
>> > Dear Douglas,
>> >
>> > I am sorry to bother you but this is very important. I posted the
>> following question (in a slight different version) at r-sig-ME question
>> list but it seems no one is able to answer it.
>>
>> But Ben answered it.  When you specify family="poisson" you are
>> fitting a generalized linear mixed model.  The parameter estimates
>> provided for such a model by lme4 are the maximum likelihood
>> estimates, up to an approximation.  The default approximation is the
>> Laplace approximation.
>>
>>
>>  This data has quite complicated design. I did not find any example
>> that is similar in the literature on lme4. According to the
>> investigator this is a partial nested design. Counts were collected at
>> different transects, different depths and different sites at different
>> times. Time is continuous and assumed to be random, all the others are
>> categorical fixed where transect is nested within depth which is
>> nested within site. Definitely the three factors are nested within
>> each other but based on the the attached files and the table below, it
>> looks like this a repeated measurement design where time (dive_id) is
>> nested within the three factor level combination. So far if I am
>> wrong, please correct me. I believe the main effect is site (b) and
>> level (a) is nested within depth(b) which in turn is nested within
>> site(b). dive_id which represents also time is random.
>> > I read some examples you gave. My output is different.
>> > 1. The fit is done with Laplace approximation, not REML
>> > 2. There is no residual random effect
>> > 3. anova(g) did not give any output
>> >
>> > In this table the cell represents the number of times each combination
>> was used to obtain the counts (based on the attached file).
>> >
>> >
>> >
>> >
>> > Hopkins
>> >
>> > Lovers Point
>> >
>> > Point Pinos
>> >
>> > Total
>> >
>> > 5
>> >
>> > B
>> >
>> > 8
>> >
>> > 6
>> >
>> > 6
>> >
>> > 20
>> >
>> > M
>> >
>> > 8
>> >
>> > 6
>> >
>> > 6
>> >
>> > 20
>> >
>> > Total
>> >
>> > 16
>> >
>> > 12
>> >
>> > 12
>> >
>> > 40
>> >
>> > 10
>> >
>> > B
>> >
>> > 7
>> >
>> > 6
>> >
>> > 7
>> >
>> > 20
>> >
>> > M
>> >
>> > 7
>> >
>> > 6
>> >
>> > 7
>> >
>> > 20
>> >
>> > Total
>> >
>> > 14
>> >
>> > 12
>> >
>> > 14
>> >
>> > 40
>> >
>> > 15
>> >
>> > B
>> >
>> > 7
>> >
>> > 6
>> >
>> > 8
>> >
>> > 21
>> >
>> > M
>> >
>> > 7
>> >
>> > 6
>> >
>> > 8
>> >
>> > 21
>> >
>> > Total
>> >
>> > 14
>> >
>> > 12
>> >
>> > 16
>> >
>> > 42
>> >
>> > Total
>> >
>> > 44
>> >
>> > 36
>> >
>> > 42
>> >
>> > 122
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> > d2 <- read.csv(file.path(dataDir,"aggregate_2008.csv"), as.is=T,stringsAsFactors
>> = FALSE)
>> > > a<-factor(d2$level)
>> > > b<-factor(d2$site)
>> > > c<-factor(d2$depth)
>> > > g=lmer(total_count ~ b+(1|b:c)+(1|b:c:a)+(1|dive_id), d2,
>> REML=TRUE,family = "poisson")
>> > > summary(g)
>> > Generalized linear mixed model fit by the Laplace approximation
>> > Formula: total_count ~ b + (1 | b:c) + (1 | b:c:a) + (1 | dive_id)
>> >    Data: d2
>> >   AIC  BIC logLik deviance
>> >  1153 1169 -570.3     1141
>> > Random effects:
>> >  Groups  Name        Variance Std.Dev.
>> >  dive_id (Intercept) 0.60707  0.77915
>> >  b:c:a   (Intercept) 0.16273  0.40340
>> >  b:c     (Intercept) 0.16273  0.40340
>> > Number of obs: 122, groups: dive_id, 61; b:c:a, 9; b:c, 9
>> >
>> > Fixed effects:
>> >               Estimate Std. Error z value Pr(>|z|)
>> > (Intercept)    1.98724    0.37388   5.315 1.07e-07 ***
>> > bLovers Point  0.02358    0.53618   0.044    0.965
>> > bPoint Pinos  -0.43114    0.53273  -0.809    0.418
>> > ---
>> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>> >
>> > Correlation of Fixed Effects:
>> >             (Intr) bLvrsP
>> > bLoversPont -0.697
>> > bPointPinos -0.702  0.489
>> >
>> > > anova(g)
>> > Error in anova(g) : single argument anova for GLMMs not yet implemented
>> >
>> > I really appreciate any of your insight as author of the package lme4.
>> >
>> > Yolande
>>
>
>         [[alternative HTML version deleted]]
>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



-- 
Joshua Wiley
Ph.D. Student, Health Psychology
Programmer Analyst II, Statistical Consulting Group
University of California, Los Angeles
https://joshuawiley.com/


From bbolker at gmail.com  Fri Jul 20 19:13:49 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 20 Jul 2012 17:13:49 +0000 (UTC)
Subject: [R-sig-ME] lmer output shows laplace approximation not reml
References: <CAF=8oObZqK8BcoGRopkag7DH0jrO26iruW-pG2MyYU5eKqKrxw@mail.gmail.com>
	<CAO7JsnTaEryfDg8m1TZ-=BkxPexbjFnZycQeREvJSfP1QjjZbQ@mail.gmail.com>
	<CAF=8oOZTnPEJ2dY8yjNbA+hpwYUDdE-s0PjEbmTWOmbAMB=n9w@mail.gmail.com>
	<CANz9Z_+qoA63gcftkpZnDO7YkaXxVtrx4zkRYsAQGe++UUyieQ@mail.gmail.com>
Message-ID: <loom.20120720T191255-797@post.gmane.org>

Joshua Wiley <jwiley.psych at ...> writes:

> 
> Hi Yolande,
> 
> It is not clear what REML is with GLMMs. 

  [snip ...]

> So, I do not think it is surprising that with a GLMM (which mixed
> effects poisson is), REML does nothing.

  I have added a (short!) discussion of this issue, with some links
to mailing list threads and literature, at:

http://glmm.wikidot.com/faq#reml-glmm


From yolande.tra at gmail.com  Fri Jul 20 19:25:09 2012
From: yolande.tra at gmail.com (Yolande Tra)
Date: Fri, 20 Jul 2012 13:25:09 -0400
Subject: [R-sig-ME] lmer output shows laplace approximation not reml
In-Reply-To: <loom.20120720T191255-797@post.gmane.org>
References: <CAF=8oObZqK8BcoGRopkag7DH0jrO26iruW-pG2MyYU5eKqKrxw@mail.gmail.com>
	<CAO7JsnTaEryfDg8m1TZ-=BkxPexbjFnZycQeREvJSfP1QjjZbQ@mail.gmail.com>
	<CAF=8oOZTnPEJ2dY8yjNbA+hpwYUDdE-s0PjEbmTWOmbAMB=n9w@mail.gmail.com>
	<CANz9Z_+qoA63gcftkpZnDO7YkaXxVtrx4zkRYsAQGe++UUyieQ@mail.gmail.com>
	<loom.20120720T191255-797@post.gmane.org>
Message-ID: <CAF=8oObcJXFXxsG6GTPg9sYqSdkLzhq-EpmscEz+DM5jq5eD9g@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120720/b3a78cfc/attachment.pl>

From bbolker at gmail.com  Fri Jul 20 19:31:23 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 20 Jul 2012 17:31:23 +0000 (UTC)
Subject: [R-sig-ME] nlmer "Error: object of type 'symbol' is not
	subsettable"
References: <loom.20120720T165302-463@post.gmane.org>
Message-ID: <loom.20120720T192327-36@post.gmane.org>

Amanda Henry <amanda_henry at ...> writes:

>  Hi all, I want to test the effect of "species", "tech", and "area"
> on "types", controlling for "microfossil", and "sampletype", with
> "site" as a random effect, which should have random slope wrt
> "microfossil", as well as weighting the response by "microfossil".
> Microfossil and type are quantitative, the rest are factors. I have
> transformed "microfossil" to approximate a uniform distribution, it
> appears as "z.micro" in the formula.
 
> My data is structured as follows
> 
> 'data.frame':	210 obs. of  9 variables:
>  $ sample     : Factor w/ 207 levels "Abri Pataud left M2",..: [snip]
>  $ site       : Factor w/ 20 levels "Arcy","Blombos",..: [snip]
>  $ sampletype : Factor w/ 2 levels "calculus","tool": 2 2 2
>  $ species    : Factor w/ 2 levels "MH","N": 1 1 1 1 [snip]
>  $ tech       : Factor w/ 3 levels "MP/MSA","N","UP/LSA": 3 3
>  $ area       : Factor w/ 3 levels "A","E","NE": 3 3 3 3 3 
>  $ microfossil: int  7 16 0 4 5 10 6 1 25 1 ...
>  $ type       : int  7 8 0 4 2 7 5 1 16 1 ...
>  $ z.micro    : num  1.26 1.881 -1.061 0.853 1.013 ...
> 
> I have used the following function
> 
> res=nlmer(type~species+tech+area+sampletype+z.micro+
>   (1|site)+(0+z.micro|site), weights=(microfossil+1),
>          family=poisson, data=tdata)
> 
> None of the categorical predictors or control factors (species, tech, area,
> sampletype) are perfectly nested or crossed with each other or with site.  
> 
> I have included "microfossil+1" as the weight because there are zero counts in
> microfossil and that gives an error.
> 
> I receive the error: 
> Error: object of type 'symbol' is not subsettable
> 
> I have seen only one other instance of this problem mentioned on this mailing
> list, but it was not answered. 

  This isn't reproducible, so (rather than spend the time to make
up my own example) I'm going to make a few guesses.

 * What else did you try -- did any simpler versions work?  I.e.,
did it work without the weights specification?

 *  It's not at all clear to me why you're using 'nlmer' since you don't
appear to have a nonlinear function on the right-hand side of the
equation -- did you mean to use lmer?  If not, what non-linear function
did you intend to use?  Did you look at the examples given for ?nlmer -- ?
(i.e., this model specification looks quite different from the nlmer
example)

 * do you really want microfossil+1 and not 1/(microfossil+1) as
the weights? (i.e., an _inverse_ variance weighting is more common).
Perhaps since microfossil is a count response you could/should
use glmer with a Poisson family, which would automatically take
care of the weighting (although as usual you would have to watch
out for overdispersion)?


From juliekern27 at googlemail.com  Sat Jul 21 16:24:53 2012
From: juliekern27 at googlemail.com (Julie Kern)
Date: Sat, 21 Jul 2012 15:24:53 +0100
Subject: [R-sig-ME] GLMM Q: Categorical fixed effects with more than 2 levels
Message-ID: <CANJi_ctFGpOqxpo8HegzJs_ccC-e+1rp=YPDqch=RLqfEGx39Q@mail.gmail.com>

I?m running a lmm (with lme) to investigate the influence of several
fixed effects on call rate. The model also includes the random terms
of group & individual identity. Several of my fixed effects are
categorical & some have more than 2 categories (up to 5). E.g. The
fixed effect habitat contains the categories ?open?, ?medium? &
?dense?. The model has shown habitat to be a significant predictor of
call rate, but I would now like to test the differences between these.
E.g. Are call rates in open, medium & dense habitats different from
each other? I am unsure how best to go about this as I still have to
take into account repeated measures from the same individuals & groups
so don?t think I can simply use a Wilcoxon signed-rank test.

Any advice would be greatly appreciated, I'm new to R & have spent a
long time searching for this on the web but am going round in ever
more confusing circles!

Thank you!

Julie


From pauljohn32 at gmail.com  Sun Jul 22 01:19:05 2012
From: pauljohn32 at gmail.com (Paul Johnson)
Date: Sat, 21 Jul 2012 18:19:05 -0500
Subject: [R-sig-ME] GLMM Q: Categorical fixed effects with more than 2
	levels
In-Reply-To: <CANJi_ctFGpOqxpo8HegzJs_ccC-e+1rp=YPDqch=RLqfEGx39Q@mail.gmail.com>
References: <CANJi_ctFGpOqxpo8HegzJs_ccC-e+1rp=YPDqch=RLqfEGx39Q@mail.gmail.com>
Message-ID: <CAErODj-aOO37pYgy7HTm908JzviMv0dnSh7QnbV8dKM_KE5RDg@mail.gmail.com>

On Sat, Jul 21, 2012 at 9:24 AM, Julie Kern <juliekern27 at googlemail.com> wrote:
> I?m running a lmm (with lme) to investigate the influence of several
> fixed effects on call rate. The model also includes the random terms
> of group & individual identity. Several of my fixed effects are
> categorical & some have more than 2 categories (up to 5). E.g. The
> fixed effect habitat contains the categories ?open?, ?medium? &
> ?dense?. The model has shown habitat to be a significant predictor of
> call rate, but I would now like to test the differences between these.
> E.g. Are call rates in open, medium & dense habitats different from
> each other? I am unsure how best to go about this as I still have to
> take into account repeated measures from the same individuals & groups
> so don?t think I can simply use a Wilcoxon signed-rank test.

I don't think there is an 'honestly significant difference' test that
can check whether the 3 are different from each other.  However, I
think you could ask a more specific comparison, such as "does it make
a difference if I assume the effects of open and medium are the same
as the effect of dense"?

You can create a new factor variable that recodes open and medium to
be the same thing.  I find the procedure to do that is difficult to
describe to my students.  To make that easier, I have a little package
of regression tools called "rockchalk".  It has in there a function
combineLevels.  you could run (if dat is the data frame)

dat$hab2 <- combineLevels(dat$habitat, levs = c("open", "medium"),
newLabel = c("oOrMed"))

if you run that model again, replacing habitat by hab2, then the anova
function will calculate a likelihood ratio test to check if you have
lost explanatory power by fixing the coefficients of open and medium
to be the same.

now, if you have a 5 level variable, say
c("lo","lom","medium","mhi","hi"),  and you want to reduce the model
to a comparison that combines "lo" and "lom" versus
"medium","mhi","hi", you have to run combineLevels twice, first
putting together "lo" and "lom" and then "medium", "mhi", "hi".

You aren't required to do that with rockchalk, but otherwise you have
a somewhat conceptually difficult series of steps.

1. Take the levels of the existing variable,
2. Add the new level for the combined category
3. Create a new factor variable that uses the new levels object
4. Reassign the cases to the new label.

If the factor is ordinal, then this is a little spicy to splice the
new one into the levels list at the right spot.  But you can see how
if you read the code for combineLevels.


> Any advice would be greatly appreciated, I'm new to R & have spent a
> long time searching for this on the web but am going round in ever
> more confusing circles!
>
> Thank you!
>
> Julie
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



-- 
Paul E. Johnson
Professor, Political Science    Assoc. Director
1541 Lilac Lane, Room 504     Center for Research Methods
University of Kansas               University of Kansas
http://pj.freefaculty.org            http://quant.ku.edu


From joeking1809 at yahoo.com  Sun Jul 22 14:26:34 2012
From: joeking1809 at yahoo.com (Joe King)
Date: Sun, 22 Jul 2012 05:26:34 -0700 (PDT)
Subject: [R-sig-ME] MCMCglmm prior choice for random intercept model
In-Reply-To: <CANz9Z_+qoA63gcftkpZnDO7YkaXxVtrx4zkRYsAQGe++UUyieQ@mail.gmail.com>
References: <CAF=8oObZqK8BcoGRopkag7DH0jrO26iruW-pG2MyYU5eKqKrxw@mail.gmail.com>
	<CAO7JsnTaEryfDg8m1TZ-=BkxPexbjFnZycQeREvJSfP1QjjZbQ@mail.gmail.com>
	<CAF=8oOZTnPEJ2dY8yjNbA+hpwYUDdE-s0PjEbmTWOmbAMB=n9w@mail.gmail.com>
	<CANz9Z_+qoA63gcftkpZnDO7YkaXxVtrx4zkRYsAQGe++UUyieQ@mail.gmail.com>
Message-ID: <1342959994.83186.YahooMailNeo@web114504.mail.gq1.yahoo.com>



Hello MCMCglmm experts !

I have a random intercept model for which I am using MCMCglmm with the following type of call

MC1<-MCMCglmm(bly~x1+x2,random=~school,data=dt,family="categorical" ,prior=list(R=list(V=1,fix=1), G=list(G1=list(V=1, nu=0)))
??? ??? , slice=T, nitt=iter, ,burnin=burn )

I was told that this prior specification: ....G=list(G1=list(V=1, nu=0))) is non-informative for the random effect variance. Is this correct ?

One problem I am having with my models is, when I run a null model (with no covariates, only bly~,random=~school) then the posterior mean for random intercept ( the G-structure : ~school) is lower than when I add covariates, although the 95% credible intervals overlap. This happens with several batches of similar data. Is this a cause for concern and could it be related to the prior specification ? 

Thank you !
JK


From nilsson.henric at gmail.com  Sun Jul 22 17:23:38 2012
From: nilsson.henric at gmail.com (Henric (Nilsson) Winell)
Date: Sun, 22 Jul 2012 17:23:38 +0200
Subject: [R-sig-ME] GLMM Q: Categorical fixed effects with more than 2
 levels
In-Reply-To: <CAErODj-aOO37pYgy7HTm908JzviMv0dnSh7QnbV8dKM_KE5RDg@mail.gmail.com>
References: <CANJi_ctFGpOqxpo8HegzJs_ccC-e+1rp=YPDqch=RLqfEGx39Q@mail.gmail.com>
	<CAErODj-aOO37pYgy7HTm908JzviMv0dnSh7QnbV8dKM_KE5RDg@mail.gmail.com>
Message-ID: <500C1AFA.2040200@gmail.com>

On 2012-07-22 01:19, Paul Johnson wrote:

> On Sat, Jul 21, 2012 at 9:24 AM, Julie Kern <juliekern27 at googlemail.com> wrote:

>> I?m running a lmm (with lme) to investigate the influence of several
>> fixed effects on call rate. The model also includes the random terms
>> of group & individual identity. Several of my fixed effects are
>> categorical & some have more than 2 categories (up to 5). E.g. The
>> fixed effect habitat contains the categories ?open?, ?medium? &
>> ?dense?. The model has shown habitat to be a significant predictor of
>> call rate, but I would now like to test the differences between these.
>> E.g. Are call rates in open, medium & dense habitats different from
>> each other? I am unsure how best to go about this as I still have to
>> take into account repeated measures from the same individuals & groups
>> so don?t think I can simply use a Wilcoxon signed-rank test.
>
> I don't think there is an 'honestly significant difference' test that
> can check whether the 3 are different from each other.  However, I

Assuming a factor X with three levels called "A", "B" and "C".  Isn't 
that just the simultaneous test of

H0_A: A = (B + C) / 2  <=>  A - (B + C) / 2 = 0
H0_B: B = (A + C) / 2  <=>  B - (A + C) / 2 = 0
H0_C: C = (A + B) / 2  <=>  C - (A + B) / 2 = 0

?  If so, that could easily be set-up using the 'multcomp' package:

 > library("nlme")
 > library("multcomp")
Loading required package: mvtnorm
Loading required package: survival
Loading required package: splines
 > fm <- lme(yield ~ 0 + Variety, random = ~ nitro | Block, data = Oats)
 > K <- rbind("GR vs (M, V)" = c(   1, -1/2, -1/2),
+            "M vs (GR, V)" = c(-1/2,    1, -1/2),
+            "V vs (GR, M)" = c(-1/2, -1/2,    1))
 > fm.glht <- glht(fm, linfct = mcp(Variety = K))
 > set.seed(123) # for reproducibility
 > summary(fm.glht, test = adjusted("free")) # see ?adjusted

          Simultaneous Tests for General Linear Hypotheses

Multiple Comparisons of Means: User-defined Contrasts


Fit: lme.formula(fixed = yield ~ 0 + Variety, data = Oats, random = 
~nitro |
     Block)

Linear Hypotheses:
                   Estimate Std. Error z value Pr(>|z|)
GR vs (M, V) == 0   0.7917     3.9024   0.203   0.8392
M vs (GR, V) == 0   8.7292     3.9024   2.237   0.0470 *
V vs (GR, M) == 0  -9.5208     3.9024  -2.440   0.0391 *
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
(Adjusted p values reported -- free method)

So, "Golden Rain" does not differ significantly from the mean of the 
others, "Marvellous" has significantly higher yield than mean of the 
others, and "Victory" has significantly lower yield than mean of the 
others.  This seems consistent with

 > boxplot(yield ~ Variety, data = Oats)

> think you could ask a more specific comparison, such as "does it make
> a difference if I assume the effects of open and medium are the same
> as the effect of dense"?
>
> You can create a new factor variable that recodes open and medium to
> be the same thing.  I find the procedure to do that is difficult to
> describe to my students.  To make that easier, I have a little package
> of regression tools called "rockchalk".  It has in there a function
> combineLevels.  you could run (if dat is the data frame)
>
> dat$hab2 <- combineLevels(dat$habitat, levs = c("open", "medium"),
> newLabel = c("oOrMed"))
>
> if you run that model again, replacing habitat by hab2, then the anova
> function will calculate a likelihood ratio test to check if you have
> lost explanatory power by fixing the coefficients of open and medium
> to be the same.
>
> now, if you have a 5 level variable, say
> c("lo","lom","medium","mhi","hi"),  and you want to reduce the model
> to a comparison that combines "lo" and "lom" versus
> "medium","mhi","hi", you have to run combineLevels twice, first
> putting together "lo" and "lom" and then "medium", "mhi", "hi".
>
> You aren't required to do that with rockchalk, but otherwise you have
> a somewhat conceptually difficult series of steps.
>
> 1. Take the levels of the existing variable,
> 2. Add the new level for the combined category
> 3. Create a new factor variable that uses the new levels object
> 4. Reassign the cases to the new label.

Huh?

 > a <- gl(5, 2, labels = c("lo2", "lo1", "mid", "hi1", "hi2"))
 > a
  [1] lo2 lo2 lo1 lo1 mid mid hi1 hi1 hi2 hi2
Levels: lo2 lo1 mid hi1 hi2
 > levels(a) <- c("lo", "lo", "mid+hi", "mid+hi", "mid+hi")
 > a
  [1] lo     lo     lo     lo     mid+hi mid+hi mid+hi mid+hi mid+hi mid+hi
Levels: lo mid+hi
 >

This seems pretty straightforward.  Honestly, what's "conceptually 
difficult" here?


HTH,
Henric



>
> If the factor is ordinal, then this is a little spicy to splice the
> new one into the levels list at the right spot.  But you can see how
> if you read the code for combineLevels.
>
>
>> Any advice would be greatly appreciated, I'm new to R & have spent a
>> long time searching for this on the web but am going round in ever
>> more confusing circles!
>>
>> Thank you!
>>
>> Julie
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>


From kkelley at nd.edu  Tue Jul 24 08:29:26 2012
From: kkelley at nd.edu (Ken Kelley)
Date: Tue, 24 Jul 2012 02:29:26 -0400
Subject: [R-sig-ME] mean and variance of random effects in glmer
Message-ID: <746616B6-C9D9-47B0-A0C9-55CCD0984FE4@nd.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120724/49be064f/attachment.pl>

From Thierry.ONKELINX at inbo.be  Tue Jul 24 11:22:20 2012
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 24 Jul 2012 09:22:20 +0000
Subject: [R-sig-ME] mean and variance of random effects in glmer
In-Reply-To: <746616B6-C9D9-47B0-A0C9-55CCD0984FE4@nd.edu>
References: <746616B6-C9D9-47B0-A0C9-55CCD0984FE4@nd.edu>
Message-ID: <AA818EAD2576BC488B4F623941DA74275D680D53@inbomail.inbo.be>

Dear Ken,

Very large variance for the random effect in a binomial glmer is an indication for (quasi-)complete separation. Here is some info on that issue: http://www.ats.ucla.edu/stat/mult_pkg/faq/general/complete_separation_logit_models.htm

If the values of Problem and Across are constant within each level of PID, I would aggregate the data (sum per PID) and then use a simple glm()

Best regards,

Thierry

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Ken Kelley
Verzonden: dinsdag 24 juli 2012 8:29
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] mean and variance of random effects in glmer

Hi everyone,

I'm fitting a straightforward glmer model with the family=binomial. I expected the mean of the random effect for the intercept to be near zero, but that isn't the case, as the mean is .91:

> (model.3 <- glmer(TA ~ 1 + Problem + Across + (1|PID), data=Data.Timed, family = binomial, nAGQ=100))
Generalized linear mixed model fit by the adaptive Gaussian Hermite approximation
Formula: TA ~ 1 + Problem + Across + (1 | PID)
   Data: Data.Timed
   AIC   BIC logLik deviance
 158.8 172.9 -75.38    150.8
Random effects:
 Groups Name        Variance Std.Dev.
 PID    (Intercept) 18.869   4.3439
Number of obs: 256, groups: PID, 64

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)  -1.1328     0.7304  -1.551   0.1209
Problem      -0.5864     0.2449  -2.394   0.0167 *
Across       -1.3768     0.4280  -3.217   0.0013 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
        (Intr) Problm
Problem -0.390
Across  -0.208 -0.150

 That is, I'm doing a mixed effects logistic regression. The PID is the participant ID; there are 4 Problems (essentially timepoints: 0, 1, 2, 3) and Across is a time-varying covariate (0, 1, 2, or 3).

The mean of the random effects is:
> colMeans(ranef(model.3)$PID[])
(Intercept)
  0.9137307

Additionally, the variance of the random effect is in the model output as 18.869, yet when I calculate the variance of the random effects directly, I get a much smaller value:
> var(ranef(model.3)$PID[])
            (Intercept)
(Intercept)    7.806402

Should I be surprised by either of the issues I note above? My concern is that I was planning on plotting the model implied curves using the fixed effects (so that the curves would represent an individual specific trajectory for a participant with a random effect of 0). Yet, there are no individuals with a random effect of zero and the mean is not zero. Thus, such a plot doesn't seem as useful as I initially thought it would.

Thanks for any thoughts on this,
Ken



        [[alternative HTML version deleted]]

* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From lorenz.gygax at art.admin.ch  Tue Jul 24 16:06:24 2012
From: lorenz.gygax at art.admin.ch (lorenz.gygax at art.admin.ch)
Date: Tue, 24 Jul 2012 16:06:24 +0200
Subject: [R-sig-ME] Large data set with several nesting levels and
	autocorrelated errors
Message-ID: <E8419A009DF25046854415F529BAB731E558D1C7D6@EVD-C7000.bk.evdad.admin.ch>

Dear Mixed-Modelers,

we are measuring brain activation in animals by non-invasively tracking brain hemodynamics using near-infrared light. We have measured 24 subjects (half of which are kept in either of two housing conditions) in three experimental conditions each and present the according stimuli 12 times per condition (12 blocks). Each presentation lasts 75 seconds and we get a measurement at 1 Hz for eight measurement channels representing different locations on the head.

This results in 24 * 3 * 12 * 75 * 8 = 518'400 rows in a complete data set (some few stimulations are actually missing due to measurement artifacts resulting in a bit less than 500'000 rows in the actual data set). So far, we have usually calculated block averages across the 12 repetitions per stimulus (using the median at each time point across the 12 repetitions). In the current experiment, we have the impression that this averaging smoothes too much of the patterns that are visible in the raw data and thus, we would like to work with the complete non-averaged data set. We are specifically interested in the shape of the time-course of the signal.

A full model as we have been using would look something like this:

lme (hemo ~ expCond * housingCond * ns (time, df= 7) * locHead,
        random= ~ 1 | subj/expCond/block/locHead,
        weights= corARMA (form= ~ 1 | subj/expCond/block/locHead, p= 3))

>From what the data shows and AR(1) process would usually suffice, but somehow the ARMA model with three parameters seems to lead to more (numerically) stable estimates.

This model unfortunately fails on a 64bit Windows 7 Enterprise machine with 24 GB of RAM due to memory limits (sessionInfo is attached at the end). I have only now noticed that this is not really the most recent version of R (this is a machine at our office and could and should be updated, of course). Would that make a (the?) difference in respect to memory management?

I presume that some other functions like lmer would be more economical on memory use but looking through the archives and the internet sites in respect to R and mixed models, there does not seem to be an alternative function that is more economical and can incorporate temporal correlation in the residuals. Is this correct?

Does anyone of you happen to know whether it is possible to make virtual memory on a Windows machine visible to R? (We have set-up some virtual memory on that machine, but R only recognizes the 24 GB of actual RAM.)

Is it possible to estimate how much memory would be needed?

Any further thoughts on and thoughts on alternative approaches?

Many thanks and best wishes, Lorenz
-
Lorenz Gygax
Centre for proper housing of ruminants and pigs
T?nikon, CH-8356 Ettenhausen / Switzerland


R version 2.12.2 (2011-02-25)
Platform: x86_64-pc-mingw32/x64 (64-bit)

locale:
[1] LC_COLLATE=German_Switzerland.1252  LC_CTYPE=German_Switzerland.1252    LC_MONETARY=German_Switzerland.1252 LC_NUMERIC=C                       
[5] LC_TIME=German_Switzerland.1252    

attached base packages:
[1] splines   stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] nlme_3.1-98

loaded via a namespace (and not attached):
[1] grid_2.12.2     lattice_0.19-17 tools_2.12.2   


From kkelley at nd.edu  Tue Jul 24 20:38:20 2012
From: kkelley at nd.edu (Ken Kelley)
Date: Tue, 24 Jul 2012 14:38:20 -0400
Subject: [R-sig-ME] mean and variance of random effects in glmer
In-Reply-To: <AA818EAD2576BC488B4F623941DA74275D680D53@inbomail.inbo.be>
References: <746616B6-C9D9-47B0-A0C9-55CCD0984FE4@nd.edu>
	<AA818EAD2576BC488B4F623941DA74275D680D53@inbomail.inbo.be>
Message-ID: <A65BAFED-340C-453E-B1EF-9BE94A5C20D3@nd.edu>

Hi Thierry,

Thanks for your thoughts on this. I hadn't considered quasi-separation, but I don't think that is it. Actually, the issue is that I slipped into thinking that the random effects were the conditional means (like in a linear mixed effects model). Rather, they are the conditional modes. Thus, the mean of the random effects need not be zero as I initially expected (and as would be the case in a linear mixed effects model). 

But, I still expected the variance of the random effects to match the output (it is 18.9 in the output yet 7.8 when I calculate it on the random effects directly). 

Best wishes,
Ken





On Jul 24, 2012, at 5:23 AM, "ONKELINX, Thierry" <Thierry.ONKELINX at inbo.be> wrote:

> Dear Ken,
> 
> Very large variance for the random effect in a binomial glmer is an indication for (quasi-)complete separation. Here is some info on that issue: http://www.ats.ucla.edu/stat/mult_pkg/faq/general/complete_separation_logit_models.htm
> 
> If the values of Problem and Across are constant within each level of PID, I would aggregate the data (sum per PID) and then use a simple glm()
> 
> Best regards,
> 
> Thierry
> 
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest, 
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
> + 32 2 525 02 51
> + 32 54 43 61 85
> Thierry.Onkelinx at inbo.be
> www.inbo.be
> 
> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
> ~ Sir Ronald Aylmer Fisher
> 
> The plural of anecdote is not data.
> ~ Roger Brinner
> 
> The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
> 
> -----Oorspronkelijk bericht-----
> Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Ken Kelley
> Verzonden: dinsdag 24 juli 2012 8:29
> Aan: r-sig-mixed-models at r-project.org
> Onderwerp: [R-sig-ME] mean and variance of random effects in glmer
> 
> Hi everyone,
> 
> I'm fitting a straightforward glmer model with the family=binomial. I expected the mean of the random effect for the intercept to be near zero, but that isn't the case, as the mean is .91:
> 
>> (model.3 <- glmer(TA ~ 1 + Problem + Across + (1|PID), data=Data.Timed, family = binomial, nAGQ=100))
> Generalized linear mixed model fit by the adaptive Gaussian Hermite approximation
> Formula: TA ~ 1 + Problem + Across + (1 | PID)
>   Data: Data.Timed
>   AIC   BIC logLik deviance
> 158.8 172.9 -75.38    150.8
> Random effects:
> Groups Name        Variance Std.Dev.
> PID    (Intercept) 18.869   4.3439
> Number of obs: 256, groups: PID, 64
> 
> Fixed effects:
>            Estimate Std. Error z value Pr(>|z|)
> (Intercept)  -1.1328     0.7304  -1.551   0.1209
> Problem      -0.5864     0.2449  -2.394   0.0167 *
> Across       -1.3768     0.4280  -3.217   0.0013 **
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> Correlation of Fixed Effects:
>        (Intr) Problm
> Problem -0.390
> Across  -0.208 -0.150
> 
> That is, I'm doing a mixed effects logistic regression. The PID is the participant ID; there are 4 Problems (essentially timepoints: 0, 1, 2, 3) and Across is a time-varying covariate (0, 1, 2, or 3).
> 
> The mean of the random effects is:
>> colMeans(ranef(model.3)$PID[])
> (Intercept)
>  0.9137307
> 
> Additionally, the variance of the random effect is in the model output as 18.869, yet when I calculate the variance of the random effects directly, I get a much smaller value:
>> var(ranef(model.3)$PID[])
>            (Intercept)
> (Intercept)    7.806402
> 
> Should I be surprised by either of the issues I note above? My concern is that I was planning on plotting the model implied curves using the fixed effects (so that the curves would represent an individual specific trajectory for a participant with a random effect of 0). Yet, there are no individuals with a random effect of zero and the mean is not zero. Thus, such a plot doesn't seem as useful as I initially thought it would.
> 
> Thanks for any thoughts on this,
> Ken
> 
> 
> 
>        [[alternative HTML version deleted]]
> 
> * * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
> Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
> The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From Thierry.ONKELINX at inbo.be  Tue Jul 24 21:12:46 2012
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 24 Jul 2012 19:12:46 +0000
Subject: [R-sig-ME] mean and variance of random effects in glmer
In-Reply-To: <A65BAFED-340C-453E-B1EF-9BE94A5C20D3@nd.edu>
References: <746616B6-C9D9-47B0-A0C9-55CCD0984FE4@nd.edu>
	<AA818EAD2576BC488B4F623941DA74275D680D53@inbomail.inbo.be>,
	<A65BAFED-340C-453E-B1EF-9BE94A5C20D3@nd.edu>
Message-ID: <AA818EAD2576BC488B4F623941DA74275D68137C@inbomail.inbo.be>

Don't assume that there is not quasi-separation, but rather check it to be sure that it is not there. I'm pretty sure that it is a case of complete separation. Deal with that first.
________________________________________
Van: Ken Kelley [kkelley at nd.edu]
Verzonden: dinsdag 24 juli 2012 20:38
Aan: ONKELINX, Thierry
CC: r-sig-mixed-models at r-project.org
Onderwerp: Re: mean and variance of random effects in glmer

Hi Thierry,

Thanks for your thoughts on this. I hadn't considered quasi-separation, but I don't think that is it. Actually, the issue is that I slipped into thinking that the random effects were the conditional means (like in a linear mixed effects model). Rather, they are the conditional modes. Thus, the mean of the random effects need not be zero as I initially expected (and as would be the case in a linear mixed effects model).

But, I still expected the variance of the random effects to match the output (it is 18.9 in the output yet 7.8 when I calculate it on the random effects directly).

Best wishes,
Ken





On Jul 24, 2012, at 5:23 AM, "ONKELINX, Thierry" <Thierry.ONKELINX at inbo.be> wrote:

> Dear Ken,
>
> Very large variance for the random effect in a binomial glmer is an indication for (quasi-)complete separation. Here is some info on that issue: http://www.ats.ucla.edu/stat/mult_pkg/faq/general/complete_separation_logit_models.htm
>
> If the values of Problem and Across are constant within each level of PID, I would aggregate the data (sum per PID) and then use a simple glm()
>
> Best regards,
>
> Thierry
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest,
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
> + 32 2 525 02 51
> + 32 54 43 61 85
> Thierry.Onkelinx at inbo.be
> www.inbo.be
>
> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
> ~ Sir Ronald Aylmer Fisher
>
> The plural of anecdote is not data.
> ~ Roger Brinner
>
> The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> -----Oorspronkelijk bericht-----
> Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Ken Kelley
> Verzonden: dinsdag 24 juli 2012 8:29
> Aan: r-sig-mixed-models at r-project.org
> Onderwerp: [R-sig-ME] mean and variance of random effects in glmer
>
> Hi everyone,
>
> I'm fitting a straightforward glmer model with the family=binomial. I expected the mean of the random effect for the intercept to be near zero, but that isn't the case, as the mean is .91:
>
>> (model.3 <- glmer(TA ~ 1 + Problem + Across + (1|PID), data=Data.Timed, family = binomial, nAGQ=100))
> Generalized linear mixed model fit by the adaptive Gaussian Hermite approximation
> Formula: TA ~ 1 + Problem + Across + (1 | PID)
>   Data: Data.Timed
>   AIC   BIC logLik deviance
> 158.8 172.9 -75.38    150.8
> Random effects:
> Groups Name        Variance Std.Dev.
> PID    (Intercept) 18.869   4.3439
> Number of obs: 256, groups: PID, 64
>
> Fixed effects:
>            Estimate Std. Error z value Pr(>|z|)
> (Intercept)  -1.1328     0.7304  -1.551   0.1209
> Problem      -0.5864     0.2449  -2.394   0.0167 *
> Across       -1.3768     0.4280  -3.217   0.0013 **
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> Correlation of Fixed Effects:
>        (Intr) Problm
> Problem -0.390
> Across  -0.208 -0.150
>
> That is, I'm doing a mixed effects logistic regression. The PID is the participant ID; there are 4 Problems (essentially timepoints: 0, 1, 2, 3) and Across is a time-varying covariate (0, 1, 2, or 3).
>
> The mean of the random effects is:
>> colMeans(ranef(model.3)$PID[])
> (Intercept)
>  0.9137307
>
> Additionally, the variance of the random effect is in the model output as 18.869, yet when I calculate the variance of the random effects directly, I get a much smaller value:
>> var(ranef(model.3)$PID[])
>            (Intercept)
> (Intercept)    7.806402
>
> Should I be surprised by either of the issues I note above? My concern is that I was planning on plotting the model implied curves using the fixed effects (so that the curves would represent an individual specific trajectory for a participant with a random effect of 0). Yet, there are no individuals with a random effect of zero and the mean is not zero. Thus, such a plot doesn't seem as useful as I initially thought it would.
>
> Thanks for any thoughts on this,
> Ken
>
>
>
>        [[alternative HTML version deleted]]
>
> * * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
> Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
> The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From r01db11 at abdn.ac.uk  Mon Jul 23 11:17:52 2012
From: r01db11 at abdn.ac.uk (Brickhill, Daisy)
Date: Mon, 23 Jul 2012 10:17:52 +0100
Subject: [R-sig-ME] zero inflated poisson output
Message-ID: <EF102B09CB7FCC43952890D497B467C9370E398477@VMAILB.uoa.abdn.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120723/cb72716f/attachment.pl>

From amartinez at imedea.uib-csic.es  Tue Jul 24 15:56:59 2012
From: amartinez at imedea.uib-csic.es (=?UTF-8?B?QWxlamFuZHJvIE1hcnTDrW5leiBBYnJhw61u?=)
Date: Tue, 24 Jul 2012 15:56:59 +0200
Subject: [R-sig-ME] predict in mixed models
Message-ID: <500EA9AB.9010302@imedea.uib-csic.es>

Hi everyone,

could you please give me any clue regarding the equivalent procedure to 
plot predicted values from mixed models?

Thanks,
Alejandro
************************
Dr. Alejandro Mart?nez-Abra?n
Investigador postdoctoral Parga-Pondal
Universidade da Coru?a
Departamento de Biolox?a Animal, Biolox?a Vexetal e Ecolox?a
Facultade de Ciencias
Campus da Zapateira s/n
E-15071 A Coru?a, Spain
E-mail: a.abrain at udc.es
http://lapoesiadelconocimiento.blogspot.com/
*************************
Population Ecology Group
IMEDEA (CSIC-UIB)
C/Miquel Marqu?s 21
07190 Esporles,Mallorca,Spain
Voice:+34971611929
E-mail:amartinez at imedea.uib-csic.es
http://www.imedea.uib.es/ficha.php?pid=158
http://lapoesiadelconocimiento.blogspot.com/
**************************
"Es mejor encender una vela que maldecir la oscuridad"
"Ubi dubium ibi libertas" (Donde hay duda hay libertad)
"????? ???????" (Con?cete a ti mismo)
"Mientras el cerebro siga siendo un misterio el universo entero seguir? 
siendo un misterio"


From Thierry.ONKELINX at inbo.be  Tue Jul 24 22:09:59 2012
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 24 Jul 2012 20:09:59 +0000
Subject: [R-sig-ME] Large data set with several nesting levels
	and	autocorrelated errors
In-Reply-To: <E8419A009DF25046854415F529BAB731E558D1C7D6@EVD-C7000.bk.evdad.admin.ch>
References: <E8419A009DF25046854415F529BAB731E558D1C7D6@EVD-C7000.bk.evdad.admin.ch>
Message-ID: <AA818EAD2576BC488B4F623941DA74275D6813BD@inbomail.inbo.be>

Dear Lorenz,

Your model requires a distance matrix of 2.69e11 elements (518000 * 518000). The random effects fixes at lot of them at zero, but you still need 24 * 3 * 12 * 8 = 6912 block of each 75 * 75 = 5625 distances. Still totalling about 39M non-zero distances. So you need at least 3 * 39 M = 116M elements for the sparse correlation matrix (269G for a regular matrix). Furthermore 518k responses and 518k residuals. The design matrix for the fixed effects is about 2 * 3 * 8 * 7 * 518k = 174M elements. The design matrix for the random effects is a sparse matrix with 8k non-zero elements. The model holds the original data as wel: 518k * 6 variables)
Assuming 8 byte per element (64 bit system) I get a minimum footprint of 2.2 GB. And that is just to store one copy of the model. So you'll need a multiple of that to calculate the model.

A few suggestions:
1) update R and the packages. There might be some improvement in memory management.
2) keep your session as clean as possible. Use only the objects you can't do without
3) simplify your model. You can simplify the random effect because you use many factors in the fixed part as well (leading to an unindentifiable model)
lme (hemo ~ expCond * housingCond * ns (time, df= 7) * locHead,
        random= ~ 1 | subj/block,
        correlation = corARMA (form= ~ time | subj/block/expCond:locHead, p= 3))
4) using a simpler correlation structure might help as well. To quote Zuur et al (2009) you don't need the best correlation structure. A reasonable one will do.
5) Reduce the amount of data by aggregation.

Best regards,

Thierry


________________________________________
Van: r-sig-mixed-models-bounces at r-project.org [r-sig-mixed-models-bounces at r-project.org] namens lorenz.gygax at art.admin.ch [lorenz.gygax at art.admin.ch]
Verzonden: dinsdag 24 juli 2012 16:06
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] Large data set with several nesting levels and    autocorrelated errors

Dear Mixed-Modelers,

we are measuring brain activation in animals by non-invasively tracking brain hemodynamics using near-infrared light. We have measured 24 subjects (half of which are kept in either of two housing conditions) in three experimental conditions each and present the according stimuli 12 times per condition (12 blocks). Each presentation lasts 75 seconds and we get a measurement at 1 Hz for eight measurement channels representing different locations on the head.

This results in 24 * 3 * 12 * 75 * 8 = 518'400 rows in a complete data set (some few stimulations are actually missing due to measurement artifacts resulting in a bit less than 500'000 rows in the actual data set). So far, we have usually calculated block averages across the 12 repetitions per stimulus (using the median at each time point across the 12 repetitions). In the current experiment, we have the impression that this averaging smoothes too much of the patterns that are visible in the raw data and thus, we would like to work with the complete non-averaged data set. We are specifically interested in the shape of the time-course of the signal.

A full model as we have been using would look something like this:

lme (hemo ~ expCond * housingCond * ns (time, df= 7) * locHead,
        random= ~ 1 | subj/expCond/block/locHead,
        weights= corARMA (form= ~ 1 | subj/expCond/block/locHead, p= 3))

>From what the data shows and AR(1) process would usually suffice, but somehow the ARMA model with three parameters seems to lead to more (numerically) stable estimates.

This model unfortunately fails on a 64bit Windows 7 Enterprise machine with 24 GB of RAM due to memory limits (sessionInfo is attached at the end). I have only now noticed that this is not really the most recent version of R (this is a machine at our office and could and should be updated, of course). Would that make a (the?) difference in respect to memory management?

I presume that some other functions like lmer would be more economical on memory use but looking through the archives and the internet sites in respect to R and mixed models, there does not seem to be an alternative function that is more economical and can incorporate temporal correlation in the residuals. Is this correct?

Does anyone of you happen to know whether it is possible to make virtual memory on a Windows machine visible to R? (We have set-up some virtual memory on that machine, but R only recognizes the 24 GB of actual RAM.)

Is it possible to estimate how much memory would be needed?

Any further thoughts on and thoughts on alternative approaches?

Many thanks and best wishes, Lorenz
-
Lorenz Gygax
Centre for proper housing of ruminants and pigs
T?nikon, CH-8356 Ettenhausen / Switzerland


R version 2.12.2 (2011-02-25)
Platform: x86_64-pc-mingw32/x64 (64-bit)

locale:
[1] LC_COLLATE=German_Switzerland.1252  LC_CTYPE=German_Switzerland.1252    LC_MONETARY=German_Switzerland.1252 LC_NUMERIC=C
[5] LC_TIME=German_Switzerland.1252

attached base packages:
[1] splines   stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] nlme_3.1-98

loaded via a namespace (and not attached):
[1] grid_2.12.2     lattice_0.19-17 tools_2.12.2

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From Thierry.ONKELINX at inbo.be  Tue Jul 24 22:12:06 2012
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 24 Jul 2012 20:12:06 +0000
Subject: [R-sig-ME] predict in mixed models
In-Reply-To: <500EA9AB.9010302@imedea.uib-csic.es>
References: <500EA9AB.9010302@imedea.uib-csic.es>
Message-ID: <AA818EAD2576BC488B4F623941DA74275D6813D2@inbomail.inbo.be>

http://glmm.wikidot.com/faq

________________________________________
Van: r-sig-mixed-models-bounces at r-project.org [r-sig-mixed-models-bounces at r-project.org] namens Alejandro Mart?nez Abra?n [amartinez at imedea.uib-csic.es]
Verzonden: dinsdag 24 juli 2012 15:56
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] predict in mixed models

Hi everyone,

could you please give me any clue regarding the equivalent procedure to
plot predicted values from mixed models?

Thanks,
Alejandro
************************
Dr. Alejandro Mart?nez-Abra?n
Investigador postdoctoral Parga-Pondal
Universidade da Coru?a
Departamento de Biolox?a Animal, Biolox?a Vexetal e Ecolox?a
Facultade de Ciencias
Campus da Zapateira s/n
E-15071 A Coru?a, Spain
E-mail: a.abrain at udc.es
http://lapoesiadelconocimiento.blogspot.com/
*************************
Population Ecology Group
IMEDEA (CSIC-UIB)
C/Miquel Marqu?s 21
07190 Esporles,Mallorca,Spain
Voice:+34971611929
E-mail:amartinez at imedea.uib-csic.es
http://www.imedea.uib.es/ficha.php?pid=158
http://lapoesiadelconocimiento.blogspot.com/
**************************
"Es mejor encender una vela que maldecir la oscuridad"
"Ubi dubium ibi libertas" (Donde hay duda hay libertad)
"????? ???????" (Con?cete a ti mismo)
"Mientras el cerebro siga siendo un misterio el universo entero seguir?
siendo un misterio"

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.

From jake987722 at hotmail.com  Tue Jul 24 22:40:55 2012
From: jake987722 at hotmail.com (Jake Westfall)
Date: Tue, 24 Jul 2012 14:40:55 -0600
Subject: [R-sig-ME] mean and variance of random effects in glmer
In-Reply-To: <A65BAFED-340C-453E-B1EF-9BE94A5C20D3@nd.edu>
References: <746616B6-C9D9-47B0-A0C9-55CCD0984FE4@nd.edu>,
	<AA818EAD2576BC488B4F623941DA74275D680D53@inbomail.inbo.be>,
	<A65BAFED-340C-453E-B1EF-9BE94A5C20D3@nd.edu>
Message-ID: <SNT107-W5323B52D14E20998AEAD0ECBDC0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120724/262dec44/attachment.pl>

From juliekern27 at googlemail.com  Wed Jul 25 23:35:22 2012
From: juliekern27 at googlemail.com (Julie Kern)
Date: Wed, 25 Jul 2012 22:35:22 +0100
Subject: [R-sig-ME] Extracting means and SE from lme & lmer with random terms
Message-ID: <CANJi_csD2qzpjH5UUoaXSt=i7g700YH1Du1MavMgx4cVzP5dbA@mail.gmail.com>

Dear R gurus,

I?ve done a lot of reading around this topic but can?t seem to find a
solution that is working.

I am running linear mixed models with lme & a few glmms with lmer
(binomial response term). I would like to extract (not predict) the
means & their SE of the fixed effects from the model but am having
difficulties. For example, having run the model

Vocalising<-lmer(Vocalsing~Sex+Age+Rank+fixef4...fixef7+(1|Group/ID),
data=mydata, family=binomial, REML=FALSE)

I would get a value for the mean proportion of males and females that
vocalised during bouts as well as the standard error of the
proportion.
I have tried using allEffects in the effects package but am only
managing to get means. Is this because I have random terms in my model
as well as fixed? If so how would you recommend I proceed?
I have tried using  attr(ranef(mymodel, postVar = TRUE)[[1]],
"postVar")as recommended on http://glmm.wikidot.com/faq but this
produces a list of 40 or so numbers so I?ve clearly misunderstood
this!

Any tips or advice would be greatly appreciated!

Thank you!

Julie


From alistair.james.stewart at gmail.com  Wed Jul 25 05:20:38 2012
From: alistair.james.stewart at gmail.com (Alistair Stewart)
Date: Wed, 25 Jul 2012 03:20:38 +0000 (UTC)
Subject: [R-sig-ME] predict in mixed models
References: <500EA9AB.9010302@imedea.uib-csic.es>
Message-ID: <loom.20120725T050552-646@post.gmane.org>

Alejandro Mart?nez Abra?n <amartinez at ...> writes:

> 
> could you please give me any clue regarding the equivalent procedure to 
> plot predicted values from mixed models?
> 


I have used the "predictSE.mer" method in package "AICcmodelavg" for predicting
from binomial generalized linear mixed models (glmer).

Take care when adding/subtracting predicted SE to fitted values to ensure your
values are transformed and backtransformed appropriately for their probability
distribution.


From lorenz.gygax at art.admin.ch  Thu Jul 26 08:51:10 2012
From: lorenz.gygax at art.admin.ch (lorenz.gygax at art.admin.ch)
Date: Thu, 26 Jul 2012 08:51:10 +0200
Subject: [R-sig-ME] Large data set with several nesting levels and
 autocorrelated errors
Message-ID: <E8419A009DF25046854415F529BAB731E558D1C903@EVD-C7000.bk.evdad.admin.ch>

Dear Thierry,

many thanks for your time and input. 

> Your model requires a distance matrix of 2.69e11 elements (518000 * 518000). The
> random effects fixes at lot of them at zero, but you still need 24 * 3 * 12 * 8 = 6912
> block of each 75 * 75 = 5625 distances. Still totalling about 39M non-zero distances.
> So you need at least 3 * 39 M = 116M elements for the sparse correlation matrix
> (269G for a regular matrix). Furthermore 518k responses and 518k residuals. The
> design matrix for the fixed effects is about 2 * 3 * 8 * 7 * 518k = 174M elements.
> The design matrix for the random effects is a sparse matrix with 8k non-zero
> elements. The model holds the original data as wel: 518k * 6 variables)
> Assuming 8 byte per element (64 bit system) I get a minimum footprint of 2.2 GB.
> And that is just to store one copy of the model. So you'll need a multiple of that to
> calculate the model.

Ok, this would nevertheless imply that - with 24 GB of RAM - a calculation of such a model is not completely off limits.

> A few suggestions:
> 1) update R and the packages. There might be some improvement in memory
> management.

We will certainly do that.

> 2) keep your session as clean as possible. Use only the objects you can't do without

We will also optimise that, though I do hope that I had a proper eye on this issue already.

> 3) simplify your model. You can simplify the random effect because you use many
> factors in the fixed part as well (leading to an unindentifiable model)
> lme (hemo ~ expCond * housingCond * ns (time, df= 7) * locHead,
>         random= ~ 1 | subj/block,
>         correlation = corARMA (form= ~ time | subj/block/expCond:locHead, p= 3))

I do not agree on that. In my view, I am only using the same variables in the random as in the fixed effect as a short cut, but the internally constructed variables are actually different. If we go back to my suggestion:
 
lme (hemo ~ expCond * housingCond * ns (time, df= 7) * locHead,
        random= ~ 1 | subj/expCond/block/locHead,
        weights= corARMA (form= ~ 1 | subj/expCond/block/locHead, p= 3))

e.g. expCond is a factor with three levels (the experimental conditions) in the fixed effect, but subj/expCond is a factor with 72 levels (the combinations of the 24 subjects with the three experimental conditions). Instead of expCond I could have used the calendar date here because the different experimental conditions were applied on different days.

In my view, I have a proper hierarchically nested model: the time course is reflected by the single observations available within each of the eight location on the head. Eight measurements of these locations on the head were made at the same time (i.e. in the same block), blocks were dependent repeats because they were applied in direct temporal succession on a given day, each day belonged to a specific experimental condition and all subjects were subjected to the different experimental conditions.

I also think that I indeed need to have these levels because the fixed effects are on different hierarchical levels: housing condition is a between subject variable, experimental condition a within subject variable (and between days), the location on the head is a within block variable and the time course is on the level of the single observations.

Last but not least, I had never trouble estimating models with a similar structure in the past and thus they do not seem to be (numerically) unidentifiable and I do not see why they should be because there are repeated measurements in respect to all fixed and random effects.

> 4) using a simpler correlation structure might help as well. To quote Zuur et al (2009)
> you don't need the best correlation structure. A reasonable one will do.

Ok. We should look into this.

> 5) Reduce the amount of data by aggregation.

That is exactly what we have tried so far by block-averaging but, of course, we might need to start think about other ways of aggregation.

Again, many thanks for your thoughts and input which are highly valuable! Lorenz

> ________________________________________
> Van: r-sig-mixed-models-bounces at r-project.org [r-sig-mixed-models-bounces at r-
> project.org] namens lorenz.gygax at art.admin.ch [lorenz.gygax at art.admin.ch]
> Verzonden: dinsdag 24 juli 2012 16:06
> Aan: r-sig-mixed-models at r-project.org
> Onderwerp: [R-sig-ME] Large data set with several nesting levels and
> autocorrelated errors
> 
> Dear Mixed-Modelers,
> 
> we are measuring brain activation in animals by non-invasively tracking brain
> hemodynamics using near-infrared light. We have measured 24 subjects (half of
> which are kept in either of two housing conditions) in three experimental conditions
> each and present the according stimuli 12 times per condition (12 blocks). Each
> presentation lasts 75 seconds and we get a measurement at 1 Hz for eight
> measurement channels representing different locations on the head.
> 
> This results in 24 * 3 * 12 * 75 * 8 = 518'400 rows in a complete data set (some few
> stimulations are actually missing due to measurement artifacts resulting in a bit less
> than 500'000 rows in the actual data set). So far, we have usually calculated block
> averages across the 12 repetitions per stimulus (using the median at each time point
> across the 12 repetitions). In the current experiment, we have the impression that
> this averaging smoothes too much of the patterns that are visible in the raw data and
> thus, we would like to work with the complete non-averaged data set. We are
> specifically interested in the shape of the time-course of the signal.
> 
> A full model as we have been using would look something like this:
> 
> lme (hemo ~ expCond * housingCond * ns (time, df= 7) * locHead,
>         random= ~ 1 | subj/expCond/block/locHead,
>         weights= corARMA (form= ~ 1 | subj/expCond/block/locHead, p= 3))
> 
> >From what the data shows and AR(1) process would usually suffice, but somehow
> the ARMA model with three parameters seems to lead to more (numerically) stable
> estimates.
> 
> This model unfortunately fails on a 64bit Windows 7 Enterprise machine with 24 GB
> of RAM due to memory limits (sessionInfo is attached at the end). I have only now
> noticed that this is not really the most recent version of R (this is a machine at our
> office and could and should be updated, of course). Would that make a (the?)
> difference in respect to memory management?
> 
> I presume that some other functions like lmer would be more economical on
> memory use but looking through the archives and the internet sites in respect to R
> and mixed models, there does not seem to be an alternative function that is more
> economical and can incorporate temporal correlation in the residuals. Is this
> correct?
> 
> Does anyone of you happen to know whether it is possible to make virtual memory
> on a Windows machine visible to R? (We have set-up some virtual memory on that
> machine, but R only recognizes the 24 GB of actual RAM.)
> 
> Is it possible to estimate how much memory would be needed?
> 
> Any further thoughts on and thoughts on alternative approaches?
> 
> Many thanks and best wishes, Lorenz
> -
> Lorenz Gygax
> Centre for proper housing of ruminants and pigs
> T?nikon, CH-8356 Ettenhausen / Switzerland
> 
> 
> R version 2.12.2 (2011-02-25)
> Platform: x86_64-pc-mingw32/x64 (64-bit)
> 
> locale:
> [1] LC_COLLATE=German_Switzerland.1252
> LC_CTYPE=German_Switzerland.1252
> LC_MONETARY=German_Switzerland.1252 LC_NUMERIC=C
> [5] LC_TIME=German_Switzerland.1252
> 
> attached base packages:
> [1] splines   stats     graphics  grDevices utils     datasets  methods   base
> 
> other attached packages:
> [1] nlme_3.1-98
> 
> loaded via a namespace (and not attached):
> [1] grid_2.12.2     lattice_0.19-17 tools_2.12.2
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> * * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
> Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en
> binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door
> een geldig ondertekend document.
> The views expressed in this message and any annex are purely those of the writer
> and may not be regarded as stating an official position of INBO, as long as the
> message is not confirmed by a duly signed document.


From bbolker at gmail.com  Fri Jul 27 16:30:21 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 27 Jul 2012 14:30:21 +0000 (UTC)
Subject: [R-sig-ME] zero inflated poisson output
References: <EF102B09CB7FCC43952890D497B467C9370E398477@VMAILB.uoa.abdn.ac.uk>
Message-ID: <loom.20120727T161741-570@post.gmane.org>

Brickhill, Daisy <r01db11 at ...> writes:

>  I am using the glmm.admb package to fit a zero-inflated poisson
> mixed model. What I would like to know is why when I use
> summary(model) the output only gives (what appears to be) the count
> process result. I.e. the binomial process, that deals with the
> zero/non zero is not shown. If you use zeroinfl() from library(pscl)
> it gives both 'Count model coefficients (poisson with log link)' AND
> 'Zero-inflation model coefficients (binomial with logit link)'. The
> glmm.admb output seems to give only the count model coefficients. I
> am using glmm.admb rather than zeroinfl() because I have a random
> effect. Can anyone tell me whether is a way I could extract the
> zero-inflation model coefficients from a glmm.admb model?

One thing to note is that the current version of glmmADMB
only fits a constant model for the zero-inflation process: if you
are interested in more complex zero-inflation models, you should
ask here again.

  What version of glmmADMB are you using?
(i.e. what is the output of sessionInfo() ? )

  With the most recent version, when I run this
example ...

## simple simulated zero-inflated Poisson example
### simulate values
set.seed(101)
d <- data.frame(f=factor(rep(LETTERS[1:10],each=10)),x=runif(100))
u <- rnorm(10,sd=2)
d$eta <- with(d,u[f]+1+4*x)
pz <- 0.3
zi <- rbinom(100,size=1,prob=pz)
d$y <- ifelse(zi,0,rpois(100,lambda=exp(d$eta)))
## fit
zipmodel <- glmmadmb(y~x+(1|f),data=d,family="poisson",zeroInflation=TRUE)

  I do get information about the zero-inflation parameter estimates ...

  At the moment glmmADMB is not building on r-forge, but I 
am trying to fix it ...

  Ben Bolker


From bbolker at gmail.com  Fri Jul 27 16:40:16 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 27 Jul 2012 14:40:16 +0000 (UTC)
Subject: [R-sig-ME] Extracting means and SE from lme & lmer with random
	terms
References: <CANJi_csD2qzpjH5UUoaXSt=i7g700YH1Du1MavMgx4cVzP5dbA@mail.gmail.com>
Message-ID: <loom.20120727T163113-753@post.gmane.org>

Julie Kern <juliekern27 at ...> writes:

> 
> Dear R gurus,
> 
> I?ve done a lot of reading around this topic but can?t seem to find a
> solution that is working.
> 
> I am running linear mixed models with lme & a few glmms with lmer
> (binomial response term). I would like to extract (not predict) the
> means & their SE of the fixed effects from the model but am having
> difficulties. For example, having run the model
> 
> Vocalising<-lmer(Vocalsing~Sex+Age+Rank+fixef4...fixef7+(1|Group/ID),
> data=mydata, family=binomial, REML=FALSE)
> 
> I would get a value for the mean proportion of males and females that
> vocalised during bouts as well as the standard error of the
> proportion.

  Can you give us a (small!) reproducible example?
  Do you get the desired results (in terms of which values are
computed) if you use glm() instead of lmer() and drop the random
effects term?

  A couple of notes:

 * REML is silently ignored when fitting GLMMs (there is a bit
of commentary on this in http://glmm.wikidot.com/faq ; the development
version of lme4 produces a warning).
 * the CRAN version of lme4 silently passes control to glmer() when
'family' is specified, but it is probably better to call glmer() explicitly
when doing GLMMs (the development version requires that you call glmer()
explicitly).
  

> I have tried using allEffects in the effects package but am only
> managing to get means. Is this because I have random terms in my model
> as well as fixed? If so how would you recommend I proceed?
> I have tried using  attr(ranef(mymodel, postVar = TRUE)[[1]],
> "postVar")as recommended on http://glmm.wikidot.com/faq but this
> produces a list of 40 or so numbers so I?ve clearly misunderstood
> this!

  the incantation you reproduce here is for getting the variances
of the random effect 'estimates' (conditional modes), not for getting
the standard errors of the fixed-effect parameters.

  Ben Bolker


From stephanie.averygomm at gmail.com  Thu Jul 26 21:55:49 2012
From: stephanie.averygomm at gmail.com (Stephanie Avery-Gomm)
Date: Thu, 26 Jul 2012 12:55:49 -0700
Subject: [R-sig-ME] Specifying 'correct' degrees of freedom for
 within-subject factor in *nlme/lme* repeated measures ANOVA?
Message-ID: <CAJLbuD5YFbT3qg41ch0Jvrq9xta30Wx1EUEkAzSgTwFSJg5qrg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120726/852f87e3/attachment.pl>

From andrewobermeier at me.com  Fri Jul 27 21:05:14 2012
From: andrewobermeier at me.com (Obermeier Andrew)
Date: Sat, 28 Jul 2012 04:05:14 +0900
Subject: [R-sig-ME] lme4 t value for 3 levels of fixed factor
Message-ID: <39B0CE71-7B43-49F5-8197-94AC61EFDBD1@me.com>

Hello,

I just joined this list today, so am worried about proper protocol, but would like to post a question about lme4.

In Baayen, Davidson, and Bates (2008), Mixed-effects modeling with crossed random effects for subjects and items, the authors describe steps for a Latin Square Design (p. 402) in which they compare 3 levels of the experimental conditions. I am considering replicating this analysis for my dissertation, I would also like to investigate 3 levels of my factor, but wish to confirm how lme4 derives the t value. 

It is my understanding that t values can only be used to compare 2 means. For 3 levels, does lme4 do a series of pairwise comparisons?

Andrew Obermeier


From bbolker at gmail.com  Sat Jul 28 00:16:59 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 27 Jul 2012 22:16:59 +0000 (UTC)
Subject: [R-sig-ME] Specifying 'correct' degrees of freedom for
	within-subject factor in *nlme/lme* repeated measures ANOVA?
References: <CAJLbuD5YFbT3qg41ch0Jvrq9xta30Wx1EUEkAzSgTwFSJg5qrg@mail.gmail.com>
Message-ID: <loom.20120728T001619-533@post.gmane.org>

Stephanie Avery-Gomm <stephanie.averygomm at ...> writes:

> 
> Hello,
> 
> I am using *nlme* to do a mixed effect repeated measures ANCOVA, with two
> additional fixed factors  but a limited sample size. *I am seeking
> clarification on how to/if I should adjust the inflated degrees of freedom
> for a within-subject factor as a way of dealing with the temporal
> pseudoreplication.  *I am not using lmer so I am not sure the FAQ or Bates
> discussion re: adjusting df in lme4/lmer applies (
> https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html).*
> 
> *More information: At 3 Sites on a river I measured fish Population in
> approximately 9 stream Channel Units. Each Channel Unit was classified as a
> Habitat, with three levels (Glide, Riffle, Pool). I sampled each Channel
> Unit 3 times over the course of the summer, each time taking a Discharge
> Measurement (thus the exact Discharge differs a little from Site to Site,
> and so is a continuous variable). I want to know if fish Population in
> stream habitats (Glides, Riffles, Pools) changes as discharge decreases
> over the summer, if there an interaction and if fish Populations differ
> between habitats or between sites?
> 
> The model I have settled on looks like this:
> Pop.Model<-lme(Pop~Site+Habitat*Discharge, random=~1|ChannelUnit,
> correlation=corCAR1(),data=mydata)
> 
> Inclusion of the three repeated measurements of Population in each Channel
> Unit results in temporal pseudoreplication *and the degrees of freedom for
> the within-subjects factor (Discharge) is 42, but I only have 26 Channel
> Units, so this is obviously inflated (should be 21). I read in The R Book
> (Crawley: *(Pg. 644*)
> that I can fix this by specifying  the degrees of freedom. But how?*
> *
> Although I?ve read a ton online, including Bates info re: SAS PROC Mixed
> versus R lmer and degrees of freedom I find that I am still quite confused.
> If anyone can offer specific advice on how I can adjust my degrees of
> freedom for the within-subjects factor in nlme or explain in accessible
> terms why I don?t need to, I would be very grateful. *
> 
> Just in case I haven't provided enough information, here is my data and r
> code.
> .csv file:
> https://www.dropbox.com/s/2ijgq74di3hmo8i/R.Help.csv
> .R file:
> https://www.dropbox.com/s/puj5maifxc2rfcg/R%20Help.R
> .doc with code & diagram:
> https://www.dropbox.com/s/29dtofc62t957co/R%20Help.doc
> 
> Sincerely,
> 
> Stephanie Avery-Gomm
> MSc. Candidate, Zoology Department
> University of British Columbia
> 


  Are you sure that 42 (which is a propitious number in any case, see
_The Hitchhiker's Guide to the Galaxy_) is *not* the right number of
df for Discharge? Continuous predictors often behave differently from 
discrete ones: in particular, see the discussion at http://tinyurl.com/ntygq3
(referenced from http://glmm.wikidot.com/faq) about how lme computes
degrees of freedom: "a term is _outer_ to a grouping factor if its
value does not change within levels of the grouping factor", thus if
Discharge takes on different values within each Channel Unit then it is
estimated at the innermost level.

  Crawley doesn't actually say (AFAICT) that you ought to be manually
adjusting the df provided by lme: "You use all of the data in the
model, and you specify its structure appropriately so that the
hypotheses are tested with the correct degrees of freedom (10 in this
case, not 48)".  For the case he is examining, he is using an
interaction between the continuous predictor (week) and the grouping
factor (plant), *and* the weeks measured are the same for each plant.
I won't say that lme *always* gets the df 'right', but I don't think
I've ever seen a case where there was an unambiguous right answer
(i.e. the situation matched a classical experimental design so that
the problem could also be expressed as a standard method-of-moments
ANOVA with a well defined denominator df) *and* lme got it wrong.

  I would suggest: (a) trying out a variety of examples (cross {discrete
predictors, continuous predictors with identical values within each group,
continuous with different values in each group} with {random intercept
only, random intercept + random slope}); (b) looking in an alternative
source such as Ellison and Gotelli's _Primer of Ecological Statistics_
to try to convince yourself about the appropriate df.

 Two more issues:

 * if the qualitative and quantitative structure of your data
allow it, you should consider adding interactions of Discharge
with random (Channel Unit) and fixed effects (Site) 
in your model (see Schielzeth and Forstmeier 2009).

 * Another minor can of worms is that one might consider adjusting
the 'denominator df' for the autoregressive structure -- if the
points are not all independent, then the effective df will be
slightly smaller.  In principle one can do this with Satterthwaite
or Kenward-Roger approximations, but I don't know if anyone's
implemented them for lme models (pbkrtest implements them for
lme4 models, but those don't allow temporal autocorrelation
structures.  Have you looked at the ACF() output to see if
the temporal correlation structure is really necessary for
your data?)  However, I would be tempted to sweep this
under the rug (as Crawley seems to; he doesn't mention df
again when discussing autocorrelation structures).

(I will also point out that is is **not** kosher in my opinion to
post a public link to the entirety of a copyrighted (and non-open)
work; it would be fair use, I think, to post a copy of a relevant
page or two, or to point to it on Google Books
<http://books.google.com/books?id=8D4HVx0apZQC&pg=PA644>.)

@article{schielzeth_conclusions_2009,
	title = {Conclusions beyond support: overconfident estimates in mixed models},
	volume = {20},
	number = {2},
	journal = {Behavioral Ecology},
	author = {Schielzeth, Holger and Forstmeier, Wolfgang},
	month = mar,
	year = {2009},
	issn = {1045-2249, 1465-7279},
	shorttitle = {Conclusions beyond support},
	url = {http://beheco.oxfordjournals.org/content/20/2/416},
	doi = {10.1093/beheco/arn145},
	pages = {416--420},
}


From pdalgd at gmail.com  Sat Jul 28 12:25:02 2012
From: pdalgd at gmail.com (peter dalgaard)
Date: Sat, 28 Jul 2012 12:25:02 +0200
Subject: [R-sig-ME] Specifying 'correct' degrees of freedom for
	within-subject factor in *nlme/lme* repeated measures ANOVA?
In-Reply-To: <loom.20120728T001619-533@post.gmane.org>
References: <CAJLbuD5YFbT3qg41ch0Jvrq9xta30Wx1EUEkAzSgTwFSJg5qrg@mail.gmail.com>
	<loom.20120728T001619-533@post.gmane.org>
Message-ID: <C62688ED-0C8D-4095-9C65-B2F3E7EA5DD9@gmail.com>


On Jul 28, 2012, at 00:16 , Ben Bolker wrote:

> I won't say that lme *always* gets the df 'right', but I don't think
> I've ever seen a case where there was an unambiguous right answer
> (i.e. the situation matched a classical experimental design so that
> the problem could also be expressed as a standard method-of-moments
> ANOVA with a well defined denominator df) *and* lme got it wrong.

Haven't played with lme for a while, but models with crossed random effects is a pretty sure way to get df wrong. E.g., this one which I dug out from some 2006 slides:

> library(nlme)
> summary(aov(logDens~sample*dilut+Error(Block/(sample*dilut)), data=Assay))

Error: Block
          Df   Sum Sq  Mean Sq F value Pr(>F)
Residuals  1 0.008311 0.008311               

Error: Block:sample
          Df  Sum Sq Mean Sq F value  Pr(>F)   
sample     5 0.27615 0.05523   11.21 0.00952 **
Residuals  5 0.02463 0.00493                   
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 

Error: Block:dilut
          Df Sum Sq Mean Sq F value   Pr(>F)    
dilut      4  3.749  0.9373   420.8 1.68e-05 ***
Residuals  4  0.009  0.0022                     
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 

Error: Block:sample:dilut
             Df  Sum Sq  Mean Sq F value Pr(>F)
sample:dilut 20 0.05552 0.002776   1.607  0.149
Residuals    20 0.03455 0.001728               

This is not "officially" supported by lme, but you can cheat it to fit the model:

> as3 <- lme(logDens~sample*dilut, data=Assay,
+            random=list(Block=~1,
+                      Block=pdIdent(~sample-1),
+                      dilut=~1))
> anova(as3)
             numDF denDF  F-value p-value
(Intercept)      1    25 538.0174  <.0001
sample           5    25  11.2133  <.0001
dilut            4     4 420.7911  <.0001
sample:dilut    20    25   1.6069  0.1301

but as you see, the F-values are right but the denDF are not.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From john.maindonald at anu.edu.au  Sat Jul 28 13:08:56 2012
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Sat, 28 Jul 2012 21:08:56 +1000
Subject: [R-sig-ME] Specifying 'correct' degrees of freedom for
	within-subject factor in *nlme/lme* repeated measures ANOVA?
In-Reply-To: <loom.20120728T001619-533@post.gmane.org>
References: <CAJLbuD5YFbT3qg41ch0Jvrq9xta30Wx1EUEkAzSgTwFSJg5qrg@mail.gmail.com>
	<loom.20120728T001619-533@post.gmane.org>
Message-ID: <AB21580C-B849-408D-AE99-ED04468994BB@anu.edu.au>

Hello -
The proper degrees of freedom will depend on what error you specify.
I'd expect there is a between Channels component of error that affects
the slope estimates.  In that case your error term maybe should be
random=~Discharge|ChannelUnit

One can of course check whether the estimate of the relevant component 
of variance is greater than 0, and if so whether it is of any consequence.

In such analyses, it is commonly assumed that the variation between
slopes can be entirely explained by variation about a line whose clop
is constant (here, across ChannelUnits).  Experience with previous
such data may establish whether this is a reasonable assumption.
A cautious analyst will, if the data allow it, want to check this assumption.

The degrees of freedom are of consequence only when you want to
move from F-statistics or t-statistics or other such statistics to p-values.
They are part of a mechanism that is used to get approximate p-values.
The p-values are, in general, not well-defined -- assumptions are made
along the lines of the assumptions that underpin the Behrens-Fisher
test.  If the model is balanced they can in general, most pundits will I
think argue,  be used as a reasonable guide.  If the model is badly 
unbalanced, they should be treated with caution.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.
http://www.maths.anu.edu.au/~johnm

On 28/07/2012, at 8:16 AM, Ben Bolker wrote:

> Stephanie Avery-Gomm <stephanie.averygomm at ...> writes:
> 
>> 
>> Hello,
>> 
>> I am using *nlme* to do a mixed effect repeated measures ANCOVA, with two
>> additional fixed factors  but a limited sample size. *I am seeking
>> clarification on how to/if I should adjust the inflated degrees of freedom
>> for a within-subject factor as a way of dealing with the temporal
>> pseudoreplication.  *I am not using lmer so I am not sure the FAQ or Bates
>> discussion re: adjusting df in lme4/lmer applies (
>> https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html).*
>> 
>> *More information: At 3 Sites on a river I measured fish Population in
>> approximately 9 stream Channel Units. Each Channel Unit was classified as a
>> Habitat, with three levels (Glide, Riffle, Pool). I sampled each Channel
>> Unit 3 times over the course of the summer, each time taking a Discharge
>> Measurement (thus the exact Discharge differs a little from Site to Site,
>> and so is a continuous variable). I want to know if fish Population in
>> stream habitats (Glides, Riffles, Pools) changes as discharge decreases
>> over the summer, if there an interaction and if fish Populations differ
>> between habitats or between sites?
>> 
>> The model I have settled on looks like this:
>> Pop.Model<-lme(Pop~Site+Habitat*Discharge, random=~1|ChannelUnit,
>> correlation=corCAR1(),data=mydata)
>> 
>> Inclusion of the three repeated measurements of Population in each Channel
>> Unit results in temporal pseudoreplication *and the degrees of freedom for
>> the within-subjects factor (Discharge) is 42, but I only have 26 Channel
>> Units, so this is obviously inflated (should be 21). I read in The R Book
>> (Crawley: *(Pg. 644*)
>> that I can fix this by specifying  the degrees of freedom. But how?*
>> *
>> Although I?ve read a ton online, including Bates info re: SAS PROC Mixed
>> versus R lmer and degrees of freedom I find that I am still quite confused.
>> If anyone can offer specific advice on how I can adjust my degrees of
>> freedom for the within-subjects factor in nlme or explain in accessible
>> terms why I don?t need to, I would be very grateful. *
>> 
>> Just in case I haven't provided enough information, here is my data and r
>> code.
>> .csv file:
>> https://www.dropbox.com/s/2ijgq74di3hmo8i/R.Help.csv
>> .R file:
>> https://www.dropbox.com/s/puj5maifxc2rfcg/R%20Help.R
>> .doc with code & diagram:
>> https://www.dropbox.com/s/29dtofc62t957co/R%20Help.doc
>> 
>> Sincerely,
>> 
>> Stephanie Avery-Gomm
>> MSc. Candidate, Zoology Department
>> University of British Columbia
>> 
> 
> 
>  Are you sure that 42 (which is a propitious number in any case, see
> _The Hitchhiker's Guide to the Galaxy_) is *not* the right number of
> df for Discharge? Continuous predictors often behave differently from 
> discrete ones: in particular, see the discussion at http://tinyurl.com/ntygq3
> (referenced from http://glmm.wikidot.com/faq) about how lme computes
> degrees of freedom: "a term is _outer_ to a grouping factor if its
> value does not change within levels of the grouping factor", thus if
> Discharge takes on different values within each Channel Unit then it is
> estimated at the innermost level.
> 
>  Crawley doesn't actually say (AFAICT) that you ought to be manually
> adjusting the df provided by lme: "You use all of the data in the
> model, and you specify its structure appropriately so that the
> hypotheses are tested with the correct degrees of freedom (10 in this
> case, not 48)".  For the case he is examining, he is using an
> interaction between the continuous predictor (week) and the grouping
> factor (plant), *and* the weeks measured are the same for each plant.
> I won't say that lme *always* gets the df 'right', but I don't think
> I've ever seen a case where there was an unambiguous right answer
> (i.e. the situation matched a classical experimental design so that
> the problem could also be expressed as a standard method-of-moments
> ANOVA with a well defined denominator df) *and* lme got it wrong.
> 
>  I would suggest: (a) trying out a variety of examples (cross {discrete
> predictors, continuous predictors with identical values within each group,
> continuous with different values in each group} with {random intercept
> only, random intercept + random slope}); (b) looking in an alternative
> source such as Ellison and Gotelli's _Primer of Ecological Statistics_
> to try to convince yourself about the appropriate df.
> 
> Two more issues:
> 
> * if the qualitative and quantitative structure of your data
> allow it, you should consider adding interactions of Discharge
> with random (Channel Unit) and fixed effects (Site) 
> in your model (see Schielzeth and Forstmeier 2009).
> 
> * Another minor can of worms is that one might consider adjusting
> the 'denominator df' for the autoregressive structure -- if the
> points are not all independent, then the effective df will be
> slightly smaller.  In principle one can do this with Satterthwaite
> or Kenward-Roger approximations, but I don't know if anyone's
> implemented them for lme models (pbkrtest implements them for
> lme4 models, but those don't allow temporal autocorrelation
> structures.  Have you looked at the ACF() output to see if
> the temporal correlation structure is really necessary for
> your data?)  However, I would be tempted to sweep this
> under the rug (as Crawley seems to; he doesn't mention df
> again when discussing autocorrelation structures).
> 
> (I will also point out that is is **not** kosher in my opinion to
> post a public link to the entirety of a copyrighted (and non-open)
> work; it would be fair use, I think, to post a copy of a relevant
> page or two, or to point to it on Google Books
> <http://books.google.com/books?id=8D4HVx0apZQC&pg=PA644>.)
> 
> @article{schielzeth_conclusions_2009,
> 	title = {Conclusions beyond support: overconfident estimates in mixed models},
> 	volume = {20},
> 	number = {2},
> 	journal = {Behavioral Ecology},
> 	author = {Schielzeth, Holger and Forstmeier, Wolfgang},
> 	month = mar,
> 	year = {2009},
> 	issn = {1045-2249, 1465-7279},
> 	shorttitle = {Conclusions beyond support},
> 	url = {http://beheco.oxfordjournals.org/content/20/2/416},
> 	doi = {10.1093/beheco/arn145},
> 	pages = {416--420},
> }
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From john.maindonald at anu.edu.au  Sat Jul 28 13:45:44 2012
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Sat, 28 Jul 2012 21:45:44 +1000
Subject: [R-sig-ME] Specifying 'correct' degrees of freedom for
	within-subject factor in *nlme/lme* repeated measures ANOVA?
In-Reply-To: <CAJLbuD5YFbT3qg41ch0Jvrq9xta30Wx1EUEkAzSgTwFSJg5qrg@mail.gmail.com>
References: <CAJLbuD5YFbT3qg41ch0Jvrq9xta30Wx1EUEkAzSgTwFSJg5qrg@mail.gmail.com>
Message-ID: <622855B1-C870-4659-AFE3-75D9CE084549@anu.edu.au>

Hello -
I found it remarkably easy to dowload your data and reproduce your analysis.
Your efforts at making what you'd done thus easy to reproduce are to be
commended!

Compare your

> Pop.Model2<-lme(Pop~Site+Habitat*Discharge, random=~1|ChannelUnit,data=mydata)
> summary(Pop.Model2)
Linear mixed-effects model fit by REML
 Data: mydata 
       AIC      BIC    logLik
  640.1269 661.5583 -310.0635

Random effects:
 Formula: ~1 | ChannelUnit
        (Intercept) Residual
StdDev:    13.87812  29.8971
. . .

with:

> Pop.ModelD2<-lme(Pop~Site+Habitat*Discharge, random=~Discharge|ChannelUnit,data=mydata)
> summary(Pop.Model2)
Linear mixed-effects model fit by REML
 Data: mydata 
       AIC      BIC   logLik
  636.0179 661.7355 -306.009

Random effects:
 Formula: ~Discharge | ChannelUnit
 Structure: General positive-definite, Log-Cholesky parametrization
            StdDev    Corr  
(Intercept)  17.88118 (Intr)
Discharge   450.69768 -0.663
Residual     23.51211       
. . .

I conclude that there is a very large random slope effect,   The df
for testing the fixed effect (slope) of Discharge should then be 21,
not 42 as the nlme output suggests.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.
http://www.maths.anu.edu.au/~johnm

On 27/07/2012, at 5:55 AM, Stephanie Avery-Gomm wrote:

> Hello,
> 
> I am using *nlme* to do a mixed effect repeated measures ANCOVA, with two
> additional fixed factors  but a limited sample size. *I am seeking
> clarification on how to/if I should adjust the inflated degrees of freedom
> for a within-subject factor as a way of dealing with the temporal
> pseudoreplication.  *I am not using lmer so I am not sure the FAQ or Bates
> discussion re: adjusting df in lme4/lmer applies (
> https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html).*
> 
> *More information: At 3 Sites on a river I measured fish Population in
> approximately 9 stream Channel Units. Each Channel Unit was classified as a
> Habitat, with three levels (Glide, Riffle, Pool). I sampled each Channel
> Unit 3 times over the course of the summer, each time taking a Discharge
> Measurement (thus the exact Discharge differs a little from Site to Site,
> and so is a continuous variable). I want to know if fish Population in
> stream habitats (Glides, Riffles, Pools) changes as discharge decreases
> over the summer, if there an interaction and if fish Populations differ
> between habitats or between sites?
> 
> The model I have settled on looks like this:
> Pop.Model<-lme(Pop~Site+Habitat*Discharge, random=~1|ChannelUnit,
> correlation=corCAR1(),data=mydata)
> 
> Inclusion of the three repeated measurements of Population in each Channel
> Unit results in temporal pseudoreplication *and the degrees of freedom for
> the within-subjects factor (Discharge) is 42, but I only have 26 Channel
> Units, so this is obviously inflated (should be 21). I read in The R Book
> (Crawley: *(Pg. 644,
> https://www.dropbox.com/s/4zqewxl44btqmzo/Crawley%20The%20R%20book.pdf*)
> that I can fix this by specifying  the degrees of freedom. But how?*
> *
> Although I?ve read a ton online, including Bates info re: SAS PROC Mixed
> versus R lmer and degrees of freedom I find that I am still quite confused.
> If anyone can offer specific advice on how I can adjust my degrees of
> freedom for the within-subjects factor in nlme or explain in accessible
> terms why I don?t need to, I would be very grateful. *
> 
> Just in case I haven't provided enough information, here is my data and r
> code.
> .csv file:
> https://www.dropbox.com/s/2ijgq74di3hmo8i/R.Help.csv
> .R file:
> https://www.dropbox.com/s/puj5maifxc2rfcg/R%20Help.R
> .doc with code & diagram:
> https://www.dropbox.com/s/29dtofc62t957co/R%20Help.doc
> 
> Sincerely,
> 
> Stephanie Avery-Gomm
> MSc. Candidate, Zoology Department
> University of British Columbia
> 
> 
> -- 
> -- 
> Stephanie Avery-Gomm
> Master's Candidate
> Zoology Department,
> University of British Columbia
> #4200-6270 University Blvd.
> Vancouver, B.C. V6T 1Z4
> Email: Stephanie.AveryGomm at gmail.com
> Cell: 778 322 3483
> Web: http://ca.linkedin.com/in/stephanieaverygomm
> 
> 
> 
> -- 
> -- 
> Stephanie Avery-Gomm
> Master's Candidate
> Zoology Department,
> University of British Columbia
> #4200-6270 University Blvd.
> Vancouver, B.C. V6T 1Z4
> Email: Stephanie.AveryGomm at gmail.com
> Cell: 778 322 3483
> Web: http://ca.linkedin.com/in/stephanieaverygomm
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From andrewobermeier at me.com  Mon Jul 30 08:04:53 2012
From: andrewobermeier at me.com (Obermeier Andrew)
Date: Mon, 30 Jul 2012 15:04:53 +0900
Subject: [R-sig-ME] comparing 3 levels of fixed factor in lme4
Message-ID: <6C6D8FE4-BCF2-45A9-8585-4FFB82BE741B@me.com>

In lme4, in models with 3 levels of the fixed factor, each of these gets a t value comparing it to a reference level.

How is this done? 

It is my understanding that the t value can only be used to compare 2 means.


From pdalgd at gmail.com  Mon Jul 30 12:38:28 2012
From: pdalgd at gmail.com (peter dalgaard)
Date: Mon, 30 Jul 2012 12:38:28 +0200
Subject: [R-sig-ME] comparing 3 levels of fixed factor in lme4
In-Reply-To: <6C6D8FE4-BCF2-45A9-8585-4FFB82BE741B@me.com>
References: <6C6D8FE4-BCF2-45A9-8585-4FFB82BE741B@me.com>
Message-ID: <718FA6F0-D090-47D9-AD05-498C42905302@gmail.com>


On Jul 30, 2012, at 08:04 , Obermeier Andrew wrote:

> In lme4, in models with 3 levels of the fixed factor, each of these gets a t value comparing it to a reference level.
> 
> How is this done? 
> 
> It is my understanding that the t value can only be used to compare 2 means.
> 

Then your understanding is wrong, and you need to read a text on basic linear modelling theory. Nothing specifically mixed-model or even R relevant about that.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From john.maindonald at anu.edu.au  Mon Jul 30 13:00:18 2012
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Mon, 30 Jul 2012 21:00:18 +1000
Subject: [R-sig-ME] comparing 3 levels of fixed factor in lme4
In-Reply-To: <718FA6F0-D090-47D9-AD05-498C42905302@gmail.com>
References: <6C6D8FE4-BCF2-45A9-8585-4FFB82BE741B@me.com>
	<718FA6F0-D090-47D9-AD05-498C42905302@gmail.com>
Message-ID: <9503AC75-CD1C-459B-904E-2E4B706E09FE@anu.edu.au>

Perhaps Andrew has vaguely at the back of his mind the notion
that for comparing >2 means, one should be using a multiple
range test or an anova test, at all events if the aim is to achieve
an experiment-wise 5% level.  Tests based on the individual
t-statistics are not independent.  This is of course a somewhat
controversial area.  
John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.
http://www.maths.anu.edu.au/~johnm

On 30/07/2012, at 8:38 PM, peter dalgaard wrote:

> 
> On Jul 30, 2012, at 08:04 , Obermeier Andrew wrote:
> 
>> In lme4, in models with 3 levels of the fixed factor, each of these gets a t value comparing it to a reference level.
>> 
>> How is this done? 
>> 
>> It is my understanding that the t value can only be used to compare 2 means.
>> 
> 
> Then your understanding is wrong, and you need to read a text on basic linear modelling theory. Nothing specifically mixed-model or even R relevant about that.
> 
> -- 
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From andrewobermeier at me.com  Mon Jul 30 13:38:12 2012
From: andrewobermeier at me.com (Obermeier Andrew)
Date: Mon, 30 Jul 2012 20:38:12 +0900
Subject: [R-sig-ME] comparing 3 levels of fixed factor in lme4
In-Reply-To: <9503AC75-CD1C-459B-904E-2E4B706E09FE@anu.edu.au>
References: <6C6D8FE4-BCF2-45A9-8585-4FFB82BE741B@me.com>
	<718FA6F0-D090-47D9-AD05-498C42905302@gmail.com>
	<9503AC75-CD1C-459B-904E-2E4B706E09FE@anu.edu.au>
Message-ID: <3C4C3E48-6D10-4FA7-93C8-A3E0D9C90F0C@me.com>

Thank you John Maindonald.

For more than 2 levels of the experimental condition, I learned that usually we find an F to test mean differences across the levels of the condition.

In lme4, the model summary reports a t value, and I am replicating a study that uses lme4 to compare 3 levels of a fixed factor. My advising professor has told me that the t value can only be used to compare 2 means.

Andrew Obermeier




On Jul 30, 2012, at 8:00 PM, John Maindonald <john.maindonald at anu.edu.au> wrote:

> Perhaps Andrew has vaguely at the back of his mind the notion
> that for comparing >2 means, one should be using a multiple
> range test or an anova test, at all events if the aim is to achieve
> an experiment-wise 5% level.  Tests based on the individual
> t-statistics are not independent.  This is of course a somewhat
> controversial area.  
> John Maindonald             email: john.maindonald at anu.edu.au
> phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
> Centre for Mathematics & Its Applications, Room 1194,
> John Dedman Mathematical Sciences Building (Building 27)
> Australian National University, Canberra ACT 0200.
> http://www.maths.anu.edu.au/~johnm
> 
> On 30/07/2012, at 8:38 PM, peter dalgaard wrote:
> 
>> 
>> On Jul 30, 2012, at 08:04 , Obermeier Andrew wrote:
>> 
>>> In lme4, in models with 3 levels of the fixed factor, each of these gets a t value comparing it to a reference level.
>>> 
>>> How is this done? 
>>> 
>>> It is my understanding that the t value can only be used to compare 2 means.
>>> 
>> 
>> Then your understanding is wrong, and you need to read a text on basic linear modelling theory. Nothing specifically mixed-model or even R relevant about that.
>> 
>> -- 
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School
>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From andrewobermeier at me.com  Mon Jul 30 13:51:26 2012
From: andrewobermeier at me.com (Obermeier Andrew)
Date: Mon, 30 Jul 2012 20:51:26 +0900
Subject: [R-sig-ME] comparing 3 levels of fixed factor in lme4
In-Reply-To: <718FA6F0-D090-47D9-AD05-498C42905302@gmail.com>
References: <6C6D8FE4-BCF2-45A9-8585-4FFB82BE741B@me.com>
	<718FA6F0-D090-47D9-AD05-498C42905302@gmail.com>
Message-ID: <56B68C26-AECD-4FCE-B152-14D022BAF25F@me.com>

> Then your understanding is wrong, and you need to read a text on basic linear modelling theory. Nothing specifically mixed-model or even R relevant about that.
> 
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com

If you have a specific book to suggest I would appreciate it. I have Pinhiero and Bates (2000), Mixed Effects Models in S and S-Plus, but have not found specific mentioned of how the t value is derived there yet.

I admit my ignorance in this area, and apologize if my questions are dumb, but nevertheless would appreciate some civility.

Andrew Obermeier


From john.maindonald at anu.edu.au  Mon Jul 30 14:02:42 2012
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Mon, 30 Jul 2012 22:02:42 +1000
Subject: [R-sig-ME] comparing 3 levels of fixed factor in lme4
In-Reply-To: <3C4C3E48-6D10-4FA7-93C8-A3E0D9C90F0C@me.com>
References: <6C6D8FE4-BCF2-45A9-8585-4FFB82BE741B@me.com>
	<718FA6F0-D090-47D9-AD05-498C42905302@gmail.com>
	<9503AC75-CD1C-459B-904E-2E4B706E09FE@anu.edu.au>
	<3C4C3E48-6D10-4FA7-93C8-A3E0D9C90F0C@me.com>
Message-ID: <10C96B41-2530-4C98-BA78-FCEA0F57D956@anu.edu.au>

Well, you can use the t-statistics for comparison-wise tests!
the issue is whether one ought to do this, or whether one
should do some kind of overall test.  As I see it, all depends 
on the purpose that is in mind.

As Professor Dalgaard says, the issue is much the same as 
for lm models.  

You can for example do:

> library(DAAG)
> a1 <- lme(ShootDryMass ~ fert+variety, random=~1|Block, data=rice)
> anova(a1)
            numDF denDF F-value p-value
(Intercept)     1    67   72.39  <.0001
fert            2    67    3.94  0.0241
variety         1    67   25.48  <.0001

As the design is balanced, the order of terms does not affect the anova F-test.
But as the design is balanced, you be better to do:
> rice.aov <- aov(ShootDryMass ~ fert+variety+Error(Block), data=rice)
> summary(rice.aov)

Error: Block
          Df Sum Sq Mean Sq F value Pr(>F)
Residuals  1   3528    3528               

Error: Within
          Df Sum Sq Mean Sq F value  Pr(>F)
fert       2   7019    3509    3.94   0.024
variety    1  22685   22685   25.48 3.7e-06
Residuals 67  59657     890                

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.
http://www.maths.anu.edu.au/~johnm

On 30/07/2012, at 9:38 PM, Obermeier Andrew wrote:

> Thank you John Maindonald.
> 
> For more than 2 levels of the experimental condition, I learned that usually we find an F to test mean differences across the levels of the condition.
> 
> In lme4, the model summary reports a t value, and I am replicating a study that uses lme4 to compare 3 levels of a fixed factor. My advising professor has told me that the t value can only be used to compare 2 means.
> 
> Andrew Obermeier
> 
> 
> 
> 
> On Jul 30, 2012, at 8:00 PM, John Maindonald <john.maindonald at anu.edu.au> wrote:
> 
>> Perhaps Andrew has vaguely at the back of his mind the notion
>> that for comparing >2 means, one should be using a multiple
>> range test or an anova test, at all events if the aim is to achieve
>> an experiment-wise 5% level.  Tests based on the individual
>> t-statistics are not independent.  This is of course a somewhat
>> controversial area.  
>> John Maindonald             email: john.maindonald at anu.edu.au
>> phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
>> Centre for Mathematics & Its Applications, Room 1194,
>> John Dedman Mathematical Sciences Building (Building 27)
>> Australian National University, Canberra ACT 0200.
>> http://www.maths.anu.edu.au/~johnm
>> 
>> On 30/07/2012, at 8:38 PM, peter dalgaard wrote:
>> 
>>> 
>>> On Jul 30, 2012, at 08:04 , Obermeier Andrew wrote:
>>> 
>>>> In lme4, in models with 3 levels of the fixed factor, each of these gets a t value comparing it to a reference level.
>>>> 
>>>> How is this done? 
>>>> 
>>>> It is my understanding that the t value can only be used to compare 2 means.
>>>> 
>>> 
>>> Then your understanding is wrong, and you need to read a text on basic linear modelling theory. Nothing specifically mixed-model or even R relevant about that.
>>> 
>>> -- 
>>> Peter Dalgaard, Professor,
>>> Center for Statistics, Copenhagen Business School
>>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>>> Phone: (+45)38153501
>>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>>> 
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> 
> 


From lborger at cebc.cnrs.fr  Mon Jul 30 14:07:30 2012
From: lborger at cebc.cnrs.fr (Luca Borger)
Date: Mon, 30 Jul 2012 14:07:30 +0200
Subject: [R-sig-ME] comparing 3 levels of fixed factor in lme4
In-Reply-To: <56B68C26-AECD-4FCE-B152-14D022BAF25F@me.com>
References: <6C6D8FE4-BCF2-45A9-8585-4FFB82BE741B@me.com>
	<718FA6F0-D090-47D9-AD05-498C42905302@gmail.com>
	<56B68C26-AECD-4FCE-B152-14D022BAF25F@me.com>
Message-ID: <50167902.7010204@cebc.cnrs.fr>

Hello,

 >I have Pinhiero and Bates (2000), Mixed Effects Models in S and 
S-Plus, but have not found specific mentioned of how the t value is 
derived there yet.

Did you check ch. 2.4 "Hypothesis tests and confidence intervals"?

HTH

Cheers,
Luca



------------------------------------------------------------
Luca Borger
Postdoctoral Research Fellow
Centre d'Etudes Biologiques de Chiz?
CNRS (UPR1934); INRA (USC1339)
79360 Villiers-en-Bois, France

Tel:    +33 (0)549 09 96 13
Fax:    +33 (0)549 09 65 26
email:  lborger at cebc.cnrs.fr
Skype:  luca.borger at skype.com
Web:    http://www.cebc.cnrs.fr/Fidentite/borger/borger.htm
         http://cnrs.academia.edu/LucaBorger
------------------------------------------------------------
# Forthcoming book chapter
# Dispersal Ecology and Evolution (ch. 17)
# http://ukcatalogue.oup.com/product/9780199608904.do
###
# New reprint! Animal Migration: A synthesis (ch. 8):
# http://ukcatalogue.oup.com/product/9780199568994.do

Le 30/07/2012 13:51, Obermeier Andrew a ?crit :
>> Then your understanding is wrong, and you need to read a text on basic linear modelling theory. Nothing specifically mixed-model or even R relevant about that.
>>
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School
>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
> If you have a specific book to suggest I would appreciate it. I have Pinhiero and Bates (2000), Mixed Effects Models in S and S-Plus, but have not found specific mentioned of how the t value is derived there yet.
>
> I admit my ignorance in this area, and apologize if my questions are dumb, but nevertheless would appreciate some civility.
>
> Andrew Obermeier
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>


From Thomas.Petzoldt at TU-Dresden.de  Mon Jul 30 14:24:40 2012
From: Thomas.Petzoldt at TU-Dresden.de (Thomas Petzoldt)
Date: Mon, 30 Jul 2012 14:24:40 +0200
Subject: [R-sig-ME] comparing 3 levels of fixed factor in lme4
In-Reply-To: <56B68C26-AECD-4FCE-B152-14D022BAF25F@me.com>
References: <6C6D8FE4-BCF2-45A9-8585-4FFB82BE741B@me.com>
	<718FA6F0-D090-47D9-AD05-498C42905302@gmail.com>
	<56B68C26-AECD-4FCE-B152-14D022BAF25F@me.com>
Message-ID: <50167D08.9030308@TU-Dresden.de>

On 7/30/2012 1:51 PM, Obermeier Andrew wrote:

[...]

> If you have a specific book to suggest I would appreciate it. I have
> Pinhiero and Bates (2000), Mixed Effects Models in S and S-Plus, but
> have not found specific mentioned of how the t value is derived there
> yet.

You may consider to read Pinheiro & Bates more carefully or consult an 
introductory text about model selection and likelihood ratio tests, e.g.:

Johnson, J., G. & Omland, K. S. (2004) Model Selection in Ecology and 
Evolution. Trends in Ecology and Evolution, 19, 101-108.


> I admit my ignorance in this area, and apologize if my questions are
> dumb, but nevertheless would appreciate some civility.
 >
 > Andrew Obermeier

... so please add a little background (or an email signature) so that it 
is possible to find an appropriate scientific and/or technical level in 
our answers.

Hope it helps


Thomas

-- 
Dr. Thomas Petzoldt
Technische Universitaet Dresden
Faculty of Forest, Geo and Hydro Sciences
Institute of Hydrobiology
01062 Dresden, Germany

E-Mail: thomas.petzoldt at tu-dresden.de
http://tu-dresden.de/Members/thomas.petzoldt


From andrewobermeier at me.com  Mon Jul 30 14:28:34 2012
From: andrewobermeier at me.com (Obermeier Andrew)
Date: Mon, 30 Jul 2012 21:28:34 +0900
Subject: [R-sig-ME] comparing 3 levels of fixed factor in lme4
In-Reply-To: <50167D08.9030308@TU-Dresden.de>
References: <6C6D8FE4-BCF2-45A9-8585-4FFB82BE741B@me.com>
	<718FA6F0-D090-47D9-AD05-498C42905302@gmail.com>
	<56B68C26-AECD-4FCE-B152-14D022BAF25F@me.com>
	<50167D08.9030308@TU-Dresden.de>
Message-ID: <6D812C6B-D36D-4B6D-8EB8-17B62E4FD655@me.com>

Thank you.

I'll read Pinhiero and Bates more carefully.

Andrew Obermeier
Doctoral Candidate, Temple University, Japan.



On Jul 30, 2012, at 9:24 PM, Thomas Petzoldt <Thomas.Petzoldt at TU-Dresden.de> wrote:

> On 7/30/2012 1:51 PM, Obermeier Andrew wrote:
> 
> [...]
> 
>> If you have a specific book to suggest I would appreciate it. I have
>> Pinhiero and Bates (2000), Mixed Effects Models in S and S-Plus, but
>> have not found specific mentioned of how the t value is derived there
>> yet.
> 
> You may consider to read Pinheiro & Bates more carefully or consult an introductory text about model selection and likelihood ratio tests, e.g.:
> 
> Johnson, J., G. & Omland, K. S. (2004) Model Selection in Ecology and Evolution. Trends in Ecology and Evolution, 19, 101-108.
> 
> 
>> I admit my ignorance in this area, and apologize if my questions are
>> dumb, but nevertheless would appreciate some civility.
> >
> > Andrew Obermeier
> 
> ... so please add a little background (or an email signature) so that it is possible to find an appropriate scientific and/or technical level in our answers.
> 
> Hope it helps
> 
> 
> Thomas
> 
> -- 
> Dr. Thomas Petzoldt
> Technische Universitaet Dresden
> Faculty of Forest, Geo and Hydro Sciences
> Institute of Hydrobiology
> 01062 Dresden, Germany
> 
> E-Mail: thomas.petzoldt at tu-dresden.de
> http://tu-dresden.de/Members/thomas.petzoldt


From pdalgd at gmail.com  Mon Jul 30 14:33:27 2012
From: pdalgd at gmail.com (peter dalgaard)
Date: Mon, 30 Jul 2012 14:33:27 +0200
Subject: [R-sig-ME] comparing 3 levels of fixed factor in lme4
In-Reply-To: <56B68C26-AECD-4FCE-B152-14D022BAF25F@me.com>
References: <6C6D8FE4-BCF2-45A9-8585-4FFB82BE741B@me.com>
	<718FA6F0-D090-47D9-AD05-498C42905302@gmail.com>
	<56B68C26-AECD-4FCE-B152-14D022BAF25F@me.com>
Message-ID: <154A5C3F-0828-47DD-BC7F-CFE63015C603@gmail.com>


On Jul 30, 2012, at 13:51 , Obermeier Andrew wrote:

>> Then your understanding is wrong, and you need to read a text on basic linear modelling theory. Nothing specifically mixed-model or even R relevant about that.
>> 
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School
>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> 
> If you have a specific book to suggest I would appreciate it. I have Pinhiero and Bates (2000), Mixed Effects Models in S and S-Plus, but have not found specific mentioned of how the t value is derived there yet.
> 
> I admit my ignorance in this area, and apologize if my questions are dumb, but nevertheless would appreciate some civility.
> 
> Andrew Obermeier

You probably need something from the level just below MEMSS. There's Maindonald & Braun, although I don't have a copy to hand just now. You might also take a look at the Faraway PDF (or  buy his more recent book) at

http://cran.at.r-project.org/other-docs.html

Specifically, take a look chapter 15 and chapter 3.1--3.

(And apologies if I came across a bit abrasive, but you did ask the same question about four times...)

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From andrewobermeier at me.com  Mon Jul 30 14:54:42 2012
From: andrewobermeier at me.com (Obermeier Andrew)
Date: Mon, 30 Jul 2012 21:54:42 +0900
Subject: [R-sig-ME] comparing 3 levels of fixed factor in lme4
In-Reply-To: <154A5C3F-0828-47DD-BC7F-CFE63015C603@gmail.com>
References: <6C6D8FE4-BCF2-45A9-8585-4FFB82BE741B@me.com>
	<718FA6F0-D090-47D9-AD05-498C42905302@gmail.com>
	<56B68C26-AECD-4FCE-B152-14D022BAF25F@me.com>
	<154A5C3F-0828-47DD-BC7F-CFE63015C603@gmail.com>
Message-ID: <DD3EFE41-86FB-4272-8BB1-92C8D717A34B@me.com>

Thank you very much Professor Dalgaard. 

I'll get the books by Maindonald & Braun and also Faraway, and meanwhile read the pdf you suggested.

Yes, MEMSS is a bit difficult for me.

I'm very sorry for all the noise I have made.

Sincerely,

Andrew Obermeier
Doctoral Candidate, Temple University Japan


> You probably need something from the level just below MEMSS. There's Maindonald & Braun, although I don't have a copy to hand just now. You might also take a look at the Faraway PDF (or  buy his more recent book) at
> 
> http://cran.at.r-project.org/other-docs.html
> 
> Specifically, take a look chapter 15 and chapter 3.1--3.
> 
> (And apologies if I came across a bit abrasive, but you did ask the same question about four times...)
> 
> -- 
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> 
> 
> 
> 
> 
> 
> 
>


From daviddouter at hotmail.com  Mon Jul 30 18:50:10 2012
From: daviddouter at hotmail.com (David Douterlungne)
Date: Mon, 30 Jul 2012 18:50:10 +0200
Subject: [R-sig-ME] measure of fit with GLMM
Message-ID: <DUB104-W107783DB184E6D058CAF71ACC60@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120730/03c54c70/attachment.pl>

From jwiley.psych at gmail.com  Tue Jul 31 05:37:11 2012
From: jwiley.psych at gmail.com (Joshua Wiley)
Date: Mon, 30 Jul 2012 20:37:11 -0700
Subject: [R-sig-ME] mcmcglmm and parallel chains
In-Reply-To: <loom.20120613T100728-628@post.gmane.org>
References: <20120612142138.GA15825@ingegerdsdator>
	<loom.20120613T100728-628@post.gmane.org>
Message-ID: <CANz9Z_KBxGZRMBo=qdkPO1k-qhY4mK5XFs_HdP5jL7pZLt9gyA@mail.gmail.com>

On Wed, Jun 13, 2012 at 1:16 AM, Ben Bolker <bbolker at gmail.com> wrote:
> Hans Ekbrand <hans at ...> writes:
>> I am learning mcmcglmm in order to use it on a beowulf cluster.
>>
>> In https://stat.ethz.ch/pipermail/r-sig-mixed-models/2011q3/006558.html
>>
>> Jarrod Hadfield writes:
>>
>> "You can merge MCMC chains from multiple runs, although you should make
>> sure you start them from different initial values"
>>
>> Is it sufficient to provide differents random seeds for each run,
>> or does this refer to the start parameter of mcmcglmm()?
>>
>>    start: optional list having 4 possible elements: ?R? (R-structure)
>>           ?G? (G-structure) and ?liab? (latent variables or
>>           liabilities) should contain the starting values where ?G?
>>           itself is also a list with as many elements as random effect
>>           components. The fourth element ?QUASI? should be logical: if
>>           ?TRUE? starting latent variables are obtained heuristically,
>>           if ?FALSE? then they are sampled from a Z-distribution
>>
>
>   It depends a bit on what your computational issues are.  It would
> probably be _better_ to use multiple starting points, but if you are
> sure you have no problem with burn-in then you can start all the chains
> at the same points and rely on the different random-number seeds to allow
> the chains to explore parameter space independently.  (Using multiple
> starting points would would also allow you to use the Gelman-Rubin
> diagnostic to assess convergence.)

Is this a reasonable approach to fitting large cross classified
logistic models?  I am exploring moving a model to MCMCglmm; however,
it already runs slowly using more traditional methods like glmer().  I
have access to a cluster so it would not be difficult to split the
chains across many cores.

I guess I would like to know what are the issues with that approach
and what should I profile to see if that is a reasonable way to
improve performance?

Thanks!

Josh

>    I would do some experiments with MCMCglmm to ensure that you know
> how random seeds work with it (i.e. that you get identical answers
> if and only if random seeds are set the same).  You may also want/need
> to look at some of the comments in the high performance task view about
> random number streams for parallel computation.
>
>
> to look into
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Joshua Wiley
Ph.D. Student, Health Psychology
Programmer Analyst II, Statistical Consulting Group
University of California, Los Angeles
https://joshuawiley.com/


From bbolker at gmail.com  Tue Jul 31 15:26:01 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 31 Jul 2012 13:26:01 +0000 (UTC)
Subject: [R-sig-ME] measure of fit with GLMM
References: <DUB104-W107783DB184E6D058CAF71ACC60@phx.gbl>
Message-ID: <loom.20120731T062957-561@post.gmane.org>

David Douterlungne <daviddouter at ...> writes:


> How can
> I compute a measure of fit of Generalized Linear Mixed Models 
> (more specifically
> a lmer-object). Does a package similar to ?Lmmfit? 
> exists for glmm objects?  

  It might exist (but not that I know of); you might use the sos
package to look for it.  The problem is that the difficulties of
computing goodness-of-fit for LMMs (see references in the lmmfit
package) are compounded by the difficulties of computing
goodness-of-fit for GLMs (see e.g.
http://finzi.psych.upenn.edu/R/library/descr/html/LogRegR2.html ) ...

  Ben Bolker


From kurbyc at gvsu.edu  Tue Jul 31 18:10:15 2012
From: kurbyc at gvsu.edu (Christopher Kurby)
Date: Tue, 31 Jul 2012 16:10:15 +0000
Subject: [R-sig-ME] question about In mer_finalize(ans) : iteration limit
 reached without convergence (9)
Message-ID: <DD62888B-A04A-464E-B113-171F4B76A7E8@gvsu.edu>

Hello mixed modelers,

I have run some logistic mixed effect models recently and have received this error, "In mer_finalize(ans) : iteration limit reached without convergence (9)." My models are quite large. Here is one of them:

IndivChanges.Fine.slopes.lmer <- lmer(Bins ~ (Character + CharChar + CharObj + Space + Cause + Goal + Scene)*AgeCentered + TotalSpeed + (1+(Character + CharChar + CharObj + Space + Cause + Goal + Scene)|SubjNum) + (1+(Character + CharChar + CharObj + Space + Cause + Goal + Scene)|Clip), data=bins5000[bins5000$Grain == "Fine",], verbose=T, family=binomial)

No need to know the meaning of the regressors. As you can see, I have random effects terms that model random slopes associated with some of the fixed effects. I've read on related postings that the above error may be because the model is over-specified. This may be true because the model runs just fine if I model the intercepts only. Are the coefficients for the model with random slopes not to be trusted because of this warning? I have pasted the last few iterations below if that helps. 

Thanks much,
Chris

298:     32321.871: 0.731753 0.132115 0.137086  0.00000 0.00148956 0.0326407 0.168323 0.0525714 0.0972708 0.184065 0.0872438 0.133164 0.0684092 0.0539844 0.205175 0.851397 0.114391 0.381421 0.693689 0.158717  1.00432 0.866623 -0.206032 0.544031  1.19370 -1.50018 -0.190620 -0.0421279 -0.111096 -0.0849887 -0.174832 -0.318776 -0.346122 -0.371937 -1.33303 -0.742240 0.130250 0.116329  0.00000  0.00000 0.00425600  0.00000 0.147181 0.00293407 -0.461068 -0.557872 -0.716052 -1.19376 -1.26139  1.71680 -0.806458  1.86601 -0.594384 -0.250580 -0.0310011 -0.504158 -0.646994 0.574640 -0.586445 -0.207232 -0.950873 0.491170 0.216685 0.0412126 -0.159642 0.302386 -0.0298736 -0.242878 -0.398745 0.359931 -0.0315161 0.921738 -1.42477 0.317335 0.182896 0.376938 0.243562 0.377014 0.224896 -0.130288 0.00643905 0.0441177 0.000320275 -0.000801700 -0.00153798 0.000350855 0.000888126 -8.32440e-07 0.00235408
299:     32321.866: 0.731776 0.132129 0.137063  0.00000 0.00150241 0.0326668 0.168318 0.0525231 0.0972974 0.184061 0.0872274 0.133144 0.0684322 0.0539893 0.205210 0.851407 0.114443 0.381368 0.693660 0.158746  1.00431 0.866690 -0.206035 0.544054  1.19378 -1.50027 -0.190598 -0.0421015 -0.111112 -0.0849996 -0.174840 -0.318807 -0.346087 -0.372077 -1.33328 -0.742173 0.130232 0.116308  0.00000  0.00000 0.00423700  0.00000 0.147152 0.00295179 -0.461175 -0.557929 -0.716076 -1.19391 -1.26147  1.71705 -0.806294  1.86606 -0.594408 -0.250581 -0.0310267 -0.504231 -0.647073 0.574840 -0.586566 -0.207421 -0.951028 0.491237 0.216722 0.0412463 -0.159583 0.302432 -0.0300427 -0.242795 -0.398871 0.359955 -0.0315649 0.921779 -1.42473 0.317402 0.183039 0.376889 0.243582 0.377037 0.224942 -0.130254 0.00642826 0.0439955 0.000312418 -0.000800297 -0.00153592 0.000379481 0.000889780 8.91593e-06 0.00234924
300:     32321.482: 0.733417 0.133066 0.134693  0.00000 0.00437083 0.0390904 0.168895 0.0427666 0.101817 0.182775 0.0842960 0.129643 0.0729268 0.0538430 0.210895 0.853695 0.125644 0.370091 0.686940 0.165249  1.00100 0.881058 -0.206400 0.548838  1.21272 -1.52079 -0.185583 -0.0362476 -0.114788 -0.0873242 -0.176617 -0.325417 -0.338692 -0.403026 -1.38819 -0.727440 0.127952 0.111791  0.00000  0.00000 3.13331e-06  0.00000 0.141448 0.00714953 -0.485023 -0.570686 -0.721934 -1.22830 -1.28031  1.77333 -0.769850  1.87673 -0.599608 -0.251076 -0.0367640 -0.520249 -0.664602 0.619004 -0.613303 -0.249588 -0.985389 0.505882 0.224753 0.0486226 -0.146511 0.312606 -0.0677897 -0.224560 -0.426951 0.364995 -0.0425878 0.930731 -1.41525 0.334034 0.212488 0.367646 0.245462 0.382965 0.234438 -0.123606 0.00611488 0.0174095 0.000332695 -0.000623908 -0.00152730 0.000343676 0.000805522 4.24731e-05 0.00241407


From r01db11 at abdn.ac.uk  Tue Jul 31 17:18:50 2012
From: r01db11 at abdn.ac.uk (Brickhill, Daisy)
Date: Tue, 31 Jul 2012 16:18:50 +0100
Subject: [R-sig-ME] MCMCglmm zero-altered
Message-ID: <EF102B09CB7FCC43952890D497B467C9370E3988DA@VMAILB.uoa.abdn.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120731/fd6ef079/attachment.pl>

From belen.fresnillo at yahoo.es  Tue Jul 31 11:53:59 2012
From: belen.fresnillo at yahoo.es (=?iso-8859-1?Q?Bel=E9n_Fresnillo?=)
Date: Tue, 31 Jul 2012 10:53:59 +0100 (BST)
Subject: [R-sig-ME] Error using glmmADMB
Message-ID: <1343728439.98381.YahooMailNeo@web28904.mail.ir2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120731/6c42ce1e/attachment.pl>

From j.hadfield at ed.ac.uk  Tue Jul 31 19:52:57 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Tue, 31 Jul 2012 18:52:57 +0100
Subject: [R-sig-ME] MCMCglmm zero-altered
In-Reply-To: <EF102B09CB7FCC43952890D497B467C9370E3988DA@VMAILB.uoa.abdn.ac.uk>
References: <EF102B09CB7FCC43952890D497B467C9370E3988DA@VMAILB.uoa.abdn.ac.uk>
Message-ID: <20120731185257.40427duz637osx8o@www.staffmail.ed.ac.uk>

Hi,

To use a ZAP model to test whether there is any zero inflation or  
deflation effects you want to hold the parameters constant across the  
Poisson and zero-altered part and compare them to a model in which  
they vary.

For the random effects this comparison would be random=~colony versus  
something more complex (idh(trait):colony or us(trait):colony). For  
the fixed effects you want to compare ~1 versus ~trait and  
percent.grass2 versus trait:percent.grass2 etc.

  For the overdispersion term MCMCglmm will not allow you to have the  
same "residual" for both parts, but ~trait:units allows the  
"residuals" for both parts to have the same distribution (although  
information regarding its variance only comes from the Poisson part).   
I believe this still allows valid testing of whether there is any  
zero-alteration or not (but as always, could be wrong). In the  
trait:units model you do NOT want to fix the variance: in your second  
prior you had fix=2 despite estimating a single variance(V=diag(1)) so  
it was probably ignored anyway.  I thought I had implemented MCMCglmm  
so it would generate an error if fix>nrow(V) - did you not get this?

Cheers,

Jarrod




Quoting "Brickhill, Daisy" <r01db11 at abdn.ac.uk> on Tue, 31 Jul 2012  
16:18:50 +0100:

> Hi,
> I am currently modelling the effect of different habitat variables  
> on the numbers of tipulid larvae found in soil cores using MCMCglmm.  
> The data is slightly zero inflated so I am trying a zero-altered  
> model (among others). I have used the following priors and model:
>
> prior1ZA = list(R = list(V=diag(2), n=0.002, fix=2), G = list(G1 =  
> list(V=diag(2), n=0.002)))
>
> model1ZA <- MCMCglmm(no._tips ~trait*(percent.grass2 + mean.veg.ht +  
> mean.soil.moisture + juldate + year),random = ~  
> idh(trait):colony,rcov = ~ idh(trait):units,
> family = "zapoisson", data = data, prior = prior1ZA, burnin = 3000,  
> nitt = 1003000, thin=1000)
>
>
> However I have read in a previous post by the immensely helpful  
> Jarrod Hadfield that "It is usual in zero-altered models to have the  
> zero bit and the truncated poisson bit have the same  
> over-dispersion. You do this by fitting the  interaction  
> rcov=~traits:units."
>
> I thought that ensuring the poisson and the zero process have the  
> same over-dispersion would require priors and model of the form:
>
> prior1ZA = list(R = list(V=diag(1), n=0.002, fix=2), G = list(G1 =  
> list(V=diag(1), n=0.002)))
>
> model1ZA <- MCMCglmm(no._tips ~trait*(percent.grass2 + mean.veg.ht +  
> mean.soil.moisture + juldate + year),random = ~ trait:colony, rcov =  
> ~ trait:units,
> family = "zapoisson", data = data, prior = prior1ZA, burnin = 3000,  
> nitt = 1003000, thin=1000)
>
>
> But looking at other posts I am beginning to think I am missing  
> something and that I *can* use my priors and model (with different  
> variances for the zero and poisson parts of the model). Is this  
> true? Can anyone tell me which of the two residual variance and  
> random effect structures is most advisable?
>
> Many thanks,
> Daisy
>
>
>
>
>
> The University of Aberdeen is a charity registered in Scotland, No SC013683.
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From bbolker at gmail.com  Wed Aug  1 01:40:55 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 31 Jul 2012 23:40:55 +0000 (UTC)
Subject: [R-sig-ME] Error using glmmADMB
References: <1343728439.98381.YahooMailNeo@web28904.mail.ir2.yahoo.com>
Message-ID: <loom.20120801T013626-254@post.gmane.org>

Bel?n Fresnillo <belen.fresnillo at ...> writes:

> 
> Dear all list members,
> ?
> I?m trying to use glmmADMB package to analize my count data in interactions
> between adult and subadult
> lizards. I have repeated measures zero inflated count data from videotaped
> interactions of different
> durations, so I?m trying to use glmmADMB with an offset variable (recording
> time), but it doesn't seem
> like working. I keep finding this error message with any of the models
> including the offset: 
> ?
> >
>
> model<-glmmadmb(interactions~treatment*sex.sub*sex.adult+
>   svl+offset(recording.time)+(1|id.adult),
> zeroInflation=TRUE, family="poisson")

  It's extremely likely that you meant offset(log(recording.time)) ; offsets are
specified
on the scale of the linear predictor (log in this case).  I don't know if that
will help (of
course as you point out it doesn't help with NB1).

  What happens if you try this (1) with glm(), with neither zero inflation nor
random effects;
(2) with pscl::zeroinfl, without random effects; (3) with glmmADMB, without the
zero-inflation?
If these all give reasonable answers but the full model still fails, see some of
the debugging tips
in ?admbControl ...

  The key to giving enough information is to give a *reproducible* example if at
all possible
(see http://tinyurl.com/reproducible-000 ...) -- the results of sessionInfo()
are helpful too
so we can see what version of R/packages you're using.

  Ben Bolker


From j.hadfield at ed.ac.uk  Wed Aug  1 10:29:02 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Wed, 01 Aug 2012 09:29:02 +0100
Subject: [R-sig-ME] MCMCglmm zero-altered
In-Reply-To: <EF102B09CB7FCC43952890D497B467C9370E39890E@VMAILB.uoa.abdn.ac.uk>
References: <EF102B09CB7FCC43952890D497B467C9370E3988DA@VMAILB.uoa.abdn.ac.uk>
	<20120731185257.40427duz637osx8o@www.staffmail.ed.ac.uk>
	<EF102B09CB7FCC43952890D497B467C9370E39890E@VMAILB.uoa.abdn.ac.uk>
Message-ID: <20120801092902.4853327aieidlcsg@www.staffmail.ed.ac.uk>

Hi,

Then I think your first model is appropriate, although you may still  
want to simplify by having some terms constant over the two processes.

Cheers,

Jarrod


Quoting "Brickhill, Daisy" <r01db11 at abdn.ac.uk> on Wed, 1 Aug 2012  
09:18:14 +0100:

> Thanks Jarrod. I'm afraid it was a case of copying code without  
> testing it and, as you say, it should have read
> prior1ZA = list(R = list(V=diag(1), n=0.002), G = list(G1 =  
> list(V=diag(1), n=0.002))) without fix=2. I have tried the  
> trait:units model and I get significant traitza terms so I would  
> like to use the zapoisson rather than the overdispersed poisson.
> I suppose what I am asking is can I now go on to use my more complex  
> model with rcov  = ~ idh(trait):units?
> Thanks for your help.
>
> -----Original Message-----
> From: Jarrod Hadfield [mailto:j.hadfield at ed.ac.uk]
> Sent: 31 July 2012 18:53
> To: Brickhill, Daisy
> Cc: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] MCMCglmm zero-altered
>
> Hi,
>
> To use a ZAP model to test whether there is any zero inflation or  
> deflation effects you want to hold the parameters constant across  
> the Poisson and zero-altered part and compare them to a model in  
> which they vary.
>
> For the random effects this comparison would be random=~colony  
> versus something more complex (idh(trait):colony or  
> us(trait):colony). For the fixed effects you want to compare ~1  
> versus ~trait and
> percent.grass2 versus trait:percent.grass2 etc.
>
>   For the overdispersion term MCMCglmm will not allow you to have  
> the same "residual" for both parts, but ~trait:units allows the  
> "residuals" for both parts to have the same distribution (although
> information regarding its variance only comes from the Poisson part).
> I believe this still allows valid testing of whether there is any  
> zero-alteration or not (but as always, could be wrong). In the  
> trait:units model you do NOT want to fix the variance: in your  
> second prior you had fix=2 despite estimating a single  
> variance(V=diag(1)) so it was probably ignored anyway.  I thought I  
> had implemented MCMCglmm so it would generate an error if  
> fix>nrow(V) - did you not get this?
>
> Cheers,
>
> Jarrod
>
>
>
>
> Quoting "Brickhill, Daisy" <r01db11 at abdn.ac.uk> on Tue, 31 Jul 2012
> 16:18:50 +0100:
>
>> Hi,
>> I am currently modelling the effect of different habitat variables on
>> the numbers of tipulid larvae found in soil cores using MCMCglmm.
>> The data is slightly zero inflated so I am trying a zero-altered model
>> (among others). I have used the following priors and model:
>>
>> prior1ZA = list(R = list(V=diag(2), n=0.002, fix=2), G = list(G1 =
>> list(V=diag(2), n=0.002)))
>>
>> model1ZA <- MCMCglmm(no._tips ~trait*(percent.grass2 + mean.veg.ht +
>> mean.soil.moisture + juldate + year),random = ~ idh(trait):colony,rcov
>> = ~ idh(trait):units, family = "zapoisson", data = data, prior =
>> prior1ZA, burnin = 3000, nitt = 1003000, thin=1000)
>>
>>
>> However I have read in a previous post by the immensely helpful Jarrod
>> Hadfield that "It is usual in zero-altered models to have the zero bit
>> and the truncated poisson bit have the same over-dispersion. You do
>> this by fitting the  interaction rcov=~traits:units."
>>
>> I thought that ensuring the poisson and the zero process have the same
>> over-dispersion would require priors and model of the form:
>>
>> prior1ZA = list(R = list(V=diag(1), n=0.002, fix=2), G = list(G1 =
>> list(V=diag(1), n=0.002)))
>>
>> model1ZA <- MCMCglmm(no._tips ~trait*(percent.grass2 + mean.veg.ht +
>> mean.soil.moisture + juldate + year),random = ~ trait:colony, rcov = ~
>> trait:units, family = "zapoisson", data = data, prior = prior1ZA,
>> burnin = 3000, nitt = 1003000, thin=1000)
>>
>>
>> But looking at other posts I am beginning to think I am missing
>> something and that I *can* use my priors and model (with different
>> variances for the zero and poisson parts of the model). Is this true?
>> Can anyone tell me which of the two residual variance and random
>> effect structures is most advisable?
>>
>> Many thanks,
>> Daisy
>>
>>
>>
>>
>>
>> The University of Aberdeen is a charity registered in Scotland, No SC013683.
>>
>>       [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>
>
>
> --
> The University of Edinburgh is a charitable body, registered in  
> Scotland, with registration number SC005336.
>
>
>
>
> The University of Aberdeen is a charity registered in Scotland, No SC013683.
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From L.Crowther at uea.ac.uk  Wed Aug  1 14:27:06 2012
From: L.Crowther at uea.ac.uk (Liam Crowther (BIO))
Date: Wed, 1 Aug 2012 13:27:06 +0100
Subject: [R-sig-ME] lme4 - GLMM dispersion parameter?
Message-ID: <D914AEC8471BB443930374112C2432BCF80AEE2807@UEASTUEXCHMBX01.UEA.AC.UK>

Dear list users,

I'm using lme4 to model the densities of several bee species in response to landscape gradients. For some species I've used just a random intercept and for others I've allowed a random effect of forage quality, this is determined by comparing maximal models with the different random components before refining the fixed effects. The dependent variable is a count at at a transect of which there are repeated measures so I'm using a GLMM with Poisson errors, examples of final models below:

hy25<-glmer(Bh~+DATE+bees$X250PCURB+bees$X250PCOSR+bees$X250PCWOO+ (1|TRANSECT), data = bees, family =poisson)

b23<-glmer(BtA~FORAGE.ST+DATE+bees$X250PCURB+bees$X250TE+bees$X250PCSNA+bees$X250PCWOO+ (1+FORAGE.ST|TRANSECT), data = bees, family =poisson)

In total there are 338 observations of 42 subjects, is there a general method for extracting a dispersion parameter from models such as these (there are models for 7 spp. at 3 different scales) where there are differing numbers of predictors and random effects?

Thank you

Liam Crowther

From bbolker at gmail.com  Wed Aug  1 14:56:50 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 1 Aug 2012 12:56:50 +0000 (UTC)
Subject: [R-sig-ME] lme4 - GLMM dispersion parameter?
References: <D914AEC8471BB443930374112C2432BCF80AEE2807@UEASTUEXCHMBX01.UEA.AC.UK>
Message-ID: <loom.20120801T145222-52@post.gmane.org>

Liam Crowther (BIO <L.Crowther at ...> writes:

>  Dear list users, I'm using lme4 to model the densities of several
> bee species in response to landscape gradients. For some species
> I've used just a random intercept and for others I've allowed a
> random effect of forage quality, this is determined by comparing
> maximal models with the different random components before refining
> the fixed effects. The dependent variable is a count at a
> transect of which there are repeated measures so I'm using a GLMM
> with Poisson errors, examples of final models below:

> hy25<-glmer(Bh~+DATE+bees$X250PCURB+bees$X250PCOSR+bees$X250PCWOO+ 
> (1|TRANSECT), data = bees,
> family =poisson)

  It may work for now but it's ugly and maybe eventually problematic to use
bees$ inside the formula:

hy25<-glmer(Bh~DATE+X250PCURB+X250PCOSR+X250PCWOO+(1|TRANSECT), 
  data = bees, family =poisson)

 would be clearer.

[snip]

> In total there are 338 observations of 42 subjects, 
> is there a general method for extracting a dispersion
> parameter from models such as these 
> (there are models for 7 spp. at 3 different scales) where there are
> differing numbers of predictors and random effects?

  Don't quite understand the question.  Poisson models don't
have dispersion parameters.  Do you want to extract the random-effects
variances and covariances (?VarCorr) ? Compute an estimate
of overdispersion (sum(residuals(model,type="pearson")^2), and
and see http://glmm.wikidot.com/faq ?

  Ben Bolker


From stat.list at yahoo.co.uk  Wed Aug  1 15:03:02 2012
From: stat.list at yahoo.co.uk (Rachel Cohen)
Date: Wed, 1 Aug 2012 14:03:02 +0100 (BST)
Subject: [R-sig-ME] basic question
Message-ID: <1343826182.94874.YahooMailNeo@web132205.mail.ird.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120801/2313b139/attachment.pl>

From john.maindonald at anu.edu.au  Thu Aug  2 00:39:54 2012
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Thu, 2 Aug 2012 08:39:54 +1000
Subject: [R-sig-ME] lme4 - GLMM dispersion parameter?
In-Reply-To: <loom.20120801T145222-52@post.gmane.org>
References: <D914AEC8471BB443930374112C2432BCF80AEE2807@UEASTUEXCHMBX01.UEA.AC.UK>
	<loom.20120801T145222-52@post.gmane.org>
Message-ID: <B36907BB-B312-44BA-8B09-F229B03AC6DA@anu.edu.au>

If Liam is wanting the (approximate) equivalent of a dispersion 
parameter, then put in an observation level random effect.
Create a factor that has one level for each observation, and 
include this as a random effect (maybe +~1|obs)

Genuine poisson variation is uncommon in such contexts --
typically one can expect some clustering.  One should
accordingly check the fitting of an observation level random
effect.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.
http://www.maths.anu.edu.au/~johnm

On 01/08/2012, at 10:56 PM, Ben Bolker wrote:

> Liam Crowther (BIO <L.Crowther at ...> writes:
> 
>> Dear list users, I'm using lme4 to model the densities of several
>> bee species in response to landscape gradients. For some species
>> I've used just a random intercept and for others I've allowed a
>> random effect of forage quality, this is determined by comparing
>> maximal models with the different random components before refining
>> the fixed effects. The dependent variable is a count at a
>> transect of which there are repeated measures so I'm using a GLMM
>> with Poisson errors, examples of final models below:
> 
>> hy25<-glmer(Bh~+DATE+bees$X250PCURB+bees$X250PCOSR+bees$X250PCWOO+ 
>> (1|TRANSECT), data = bees,
>> family =poisson)
> 
>  It may work for now but it's ugly and maybe eventually problematic to use
> bees$ inside the formula:
> 
> hy25<-glmer(Bh~DATE+X250PCURB+X250PCOSR+X250PCWOO+(1|TRANSECT), 
>  data = bees, family =poisson)
> 
> would be clearer.
> 
> [snip]
> 
>> In total there are 338 observations of 42 subjects, 
>> is there a general method for extracting a dispersion
>> parameter from models such as these 
>> (there are models for 7 spp. at 3 different scales) where there are
>> differing numbers of predictors and random effects?
> 
>  Don't quite understand the question.  Poisson models don't
> have dispersion parameters.  Do you want to extract the random-effects
> variances and covariances (?VarCorr) ? Compute an estimate
> of overdispersion (sum(residuals(model,type="pearson")^2), and
> and see http://glmm.wikidot.com/faq ?
> 
>  Ben Bolker
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From bbolker at gmail.com  Thu Aug  2 03:51:57 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 2 Aug 2012 01:51:57 +0000 (UTC)
Subject: [R-sig-ME] basic question
References: <1343826182.94874.YahooMailNeo@web132205.mail.ird.yahoo.com>
Message-ID: <loom.20120802T034230-54@post.gmane.org>

Rachel Cohen <stat.list at ...> writes:

>  Hi, I have a very basic query (I think). ?If I fit a model of the
> specification given below how many model parameters are there? ?Am I
> correct in thinking that the model parameters are the fixed effects
> intercept and coefficients (in this case two of them) and also a
> parameter for within and for between group variance? ?So in total 5
> fitted parameters? or are there also covariance parameters?. ?I am
> trying to calculate the standard error of the residuals as:

  Unfortunately, this is an easy question to state but not (in my
opinion) an easy question to answer unambiguously.


> RSE = ((sum(residuals^2))/(N - no.of parameters))
> 
> and therefore need to know how many parameters my model actually has. 
> I've gotten a little confused by the
> literature definitions of what constitutes a parameter.?

It's confusing!

 [snip]

 A *reasonable* definition (although not the only one) would be
to count the number of fixed-effect parameters ('beta' in much
of the literature) and the number of random-effect parameters
(generally referred to as 'theta' in the lme4 documentation,
but varying a great deal among references):

length(fixef(model))+length(getME(model,"theta"))

  in your case that's 3 parameters for the fixed effects
and 6 RE parameters (you have a 3x3 variance-covariance matrix of the
random effects, the matrix is symmetric, so counting the
diagonal plus one triangle gives 3*(3+1)/2 = 6).  Generally one
doesn't count the residual variance since that is estimated
from the (penalized) residual sum of squares.

  It really depends what you want to use the RSE for.  It may
very well not have the properties you're expecting (i.e. the
properties that it has in a simple (non-mixed) linear model ...)

  There's a bit more about parameter-counting issues at
http://glmm.wikidot.com/faq, I think ...

> model<-lmer((log.mass)~centre.log.dbh+centre.log.height+
> (1+centre.log.dbh+centre.log.height|species_site),
> data=allometry_2,REML=T)?
> 
> ?model summary output table:
> 
> Linear mixed model fit by REML?
> Formula: (log.mass) ~ centre.log.dbh + centre.log.height + 
> (1 + centre.log.dbh + centre.log.height | species_site)?

[snip] 
> Random effects:
> ?Groups ? ? ? Name ? ? ? ? ? ? ?Variance ?Std.Dev. Corr ? ? ? ? ?
> ?species_site (Intercept) ? ? ? 0.061636 ?0.24827 ? ? ? ? ? ? ? ?
> ? ? ? ? ? ? ? centre.log.dbh ? ?0.279362 ?0.52855 ? 0.575 ? ? ? ?
> ? ? ? ? ? ? ? centre.log.height?0.285753 ?0.53456 ?-0.441 -0.755?
> ?Residual ? ? ? ? ? ? ? ? ?? ? ?0.086963 ?0.29489 ? ? ? ? ? ? ? ?
> Number of obs: 337, groups: species_site, 18
>


From r01db11 at abdn.ac.uk  Wed Aug  1 10:18:14 2012
From: r01db11 at abdn.ac.uk (Brickhill, Daisy)
Date: Wed, 1 Aug 2012 09:18:14 +0100
Subject: [R-sig-ME] MCMCglmm zero-altered
In-Reply-To: <20120731185257.40427duz637osx8o@www.staffmail.ed.ac.uk>
References: <EF102B09CB7FCC43952890D497B467C9370E3988DA@VMAILB.uoa.abdn.ac.uk>
	<20120731185257.40427duz637osx8o@www.staffmail.ed.ac.uk>
Message-ID: <EF102B09CB7FCC43952890D497B467C9370E39890E@VMAILB.uoa.abdn.ac.uk>

Thanks Jarrod. I'm afraid it was a case of copying code without testing it and, as you say, it should have read
prior1ZA = list(R = list(V=diag(1), n=0.002), G = list(G1 = list(V=diag(1), n=0.002))) without fix=2. I have tried the trait:units model and I get significant traitza terms so I would like to use the zapoisson rather than the overdispersed poisson.
I suppose what I am asking is can I now go on to use my more complex model with rcov  = ~ idh(trait):units?
Thanks for your help.

-----Original Message-----
From: Jarrod Hadfield [mailto:j.hadfield at ed.ac.uk]
Sent: 31 July 2012 18:53
To: Brickhill, Daisy
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] MCMCglmm zero-altered

Hi,

To use a ZAP model to test whether there is any zero inflation or deflation effects you want to hold the parameters constant across the Poisson and zero-altered part and compare them to a model in which they vary.

For the random effects this comparison would be random=~colony versus something more complex (idh(trait):colony or us(trait):colony). For the fixed effects you want to compare ~1 versus ~trait and
percent.grass2 versus trait:percent.grass2 etc.

  For the overdispersion term MCMCglmm will not allow you to have the same "residual" for both parts, but ~trait:units allows the "residuals" for both parts to have the same distribution (although
information regarding its variance only comes from the Poisson part).
I believe this still allows valid testing of whether there is any zero-alteration or not (but as always, could be wrong). In the trait:units model you do NOT want to fix the variance: in your second prior you had fix=2 despite estimating a single variance(V=diag(1)) so it was probably ignored anyway.  I thought I had implemented MCMCglmm so it would generate an error if fix>nrow(V) - did you not get this?

Cheers,

Jarrod




Quoting "Brickhill, Daisy" <r01db11 at abdn.ac.uk> on Tue, 31 Jul 2012
16:18:50 +0100:

> Hi,
> I am currently modelling the effect of different habitat variables on
> the numbers of tipulid larvae found in soil cores using MCMCglmm.
> The data is slightly zero inflated so I am trying a zero-altered model
> (among others). I have used the following priors and model:
>
> prior1ZA = list(R = list(V=diag(2), n=0.002, fix=2), G = list(G1 =
> list(V=diag(2), n=0.002)))
>
> model1ZA <- MCMCglmm(no._tips ~trait*(percent.grass2 + mean.veg.ht +
> mean.soil.moisture + juldate + year),random = ~ idh(trait):colony,rcov
> = ~ idh(trait):units, family = "zapoisson", data = data, prior =
> prior1ZA, burnin = 3000, nitt = 1003000, thin=1000)
>
>
> However I have read in a previous post by the immensely helpful Jarrod
> Hadfield that "It is usual in zero-altered models to have the zero bit
> and the truncated poisson bit have the same over-dispersion. You do
> this by fitting the  interaction rcov=~traits:units."
>
> I thought that ensuring the poisson and the zero process have the same
> over-dispersion would require priors and model of the form:
>
> prior1ZA = list(R = list(V=diag(1), n=0.002, fix=2), G = list(G1 =
> list(V=diag(1), n=0.002)))
>
> model1ZA <- MCMCglmm(no._tips ~trait*(percent.grass2 + mean.veg.ht +
> mean.soil.moisture + juldate + year),random = ~ trait:colony, rcov = ~
> trait:units, family = "zapoisson", data = data, prior = prior1ZA,
> burnin = 3000, nitt = 1003000, thin=1000)
>
>
> But looking at other posts I am beginning to think I am missing
> something and that I *can* use my priors and model (with different
> variances for the zero and poisson parts of the model). Is this true?
> Can anyone tell me which of the two residual variance and random
> effect structures is most advisable?
>
> Many thanks,
> Daisy
>
>
>
>
>
> The University of Aberdeen is a charity registered in Scotland, No SC013683.
>
>       [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



--
The University of Edinburgh is a charitable body, registered in Scotland, with registration number SC005336.




The University of Aberdeen is a charity registered in Scotland, No SC013683.


From longrob604 at gmail.com  Thu Aug  2 21:09:04 2012
From: longrob604 at gmail.com (W Robert Long)
Date: Thu, 02 Aug 2012 20:09:04 +0100
Subject: [R-sig-ME] MCMCglmm non-informative prior
In-Reply-To: <B36907BB-B312-44BA-8B09-F229B03AC6DA@anu.edu.au>
References: <D914AEC8471BB443930374112C2432BCF80AEE2807@UEASTUEXCHMBX01.UEA.AC.UK>
	<loom.20120801T145222-52@post.gmane.org>
	<B36907BB-B312-44BA-8B09-F229B03AC6DA@anu.edu.au>
Message-ID: <501AD050.20705@gmail.com>

Hi all,

I want to set a non-informative prior on a random effect. My simplest 
model has just one random effect.

The tutorial pdf shows how to use an improper flat prior, and a proper 
Cauchy prior.  I don't think the former is appropriate for me because I 
know the parameter will not take values above 1 (or rather values above 
1 are vanishingly small). I guess a U(0,1) prior distribution would be 
ideal but I don't see any way to parameterise that. Am I thinking about 
this properly ?

As for the proper Cauchy prior which seems to accept a
additional parameter that controls the scale , I don't really understand 
this - can anyone explain it ? Can this be used in my situation ?

Thanks

Robert Long
Postgraduate Student
University of Leeds / UK


From bbolker at gmail.com  Fri Aug  3 02:56:28 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 02 Aug 2012 20:56:28 -0400
Subject: [R-sig-ME] basic question
In-Reply-To: <1343906088.83232.YahooMailNeo@web132202.mail.ird.yahoo.com>
References: <1343826182.94874.YahooMailNeo@web132205.mail.ird.yahoo.com>
	<loom.20120802T034230-54@post.gmane.org>
	<1343906088.83232.YahooMailNeo@web132202.mail.ird.yahoo.com>
Message-ID: <501B21BC.1090404@gmail.com>

 [cc'ing back to r-sig-mixed-models]

On 12-08-02 07:14 AM, Rachel Cohen wrote:
> Hi, thanks for your reply.  I was reading the Pinheiro and Bates book
> (pg.21) which says that for a basic model the parameters are: ?, ?2b ,
> and ?2 which is why I thought the residual variance was counted as a
> parameter?  Also, on the same page: "Although the random effects, bi, i
> = 1, . . .,M may behave like parameters, formally they are just another
> level of random variation in the model so we do not ?estimate? them as
> such."  That lead me to believe that the random effects are not counted
> as parameters?

   First, you need to make the distinction between the b_i value
('BLUPs', or 'conditional modes', or 'random effect estimates' -- the
estimates of what's happening in each grouping level) -- and the theta
values (the variances of the random effects).
> 
> I am intending on using the RSE in the formula: CF = exp (RSE^2
> /2) which is supposed to correct for log-bias in estimates (in this case
> estimated tree biomass).  Each unlogged estimate is multiplied by the
> correction factor (CF).  I haven't come across any mention in the
> literature of whether it is appropriate to use the RSE from a
> mixed-effects model in this way.  I hope so?

  If you want to fit on the log scale but estimate the *mean* effects on
the unlogged scale, you should (I think) add the exp(TOTAL_variance/2);
in this case TOTAL_variance will be the sum of *all of the variance
components* in the model, including the residual variance (I'm going to
assume you have only intercept random effects in the model, otherwise
things would get a bit more complicated ...) I *think* it would be
something like sum(sapply(VarCorr(model),diag))+sigma(model)^2 ...  I
think there has been a reasonably recent post by Jarrod Hadfield
answering someone's question related to this topic ...

  If I'm right, then there's no need to go messing around with numbers
of parameters ...
  Ben Bolker





------------------------
> *From:* Ben Bolker <bbolker at gmail.com>
> *To:* r-sig-mixed-models at r-project.org
> *Sent:* Thursday, 2 August 2012, 2:51
> *Subject:* Re: [R-sig-ME] basic question
> 
> Rachel Cohen <stat.list at ...> writes:
> 
>>  Hi, I have a very basic query (I think).  If I fit a model of the
>> specification given below how many model parameters are there?  Am I
>> correct in thinking that the model parameters are the fixed effects
>> intercept and coefficients (in this case two of them) and also a
>> parameter for within and for between group variance?  So in total 5
>> fitted parameters? or are there also covariance parameters?.  I am
>> trying to calculate the standard error of the residuals as:
> 
>   Unfortunately, this is an easy question to state but not (in my
> opinion) an easy question to answer unambiguously.
> 
> 
>> RSE = ((sum(residuals^2))/(N - no.of parameters))
>>
>> and therefore need to know how many parameters my model actually has.
>> I've gotten a little confused by the
>> literature definitions of what constitutes a parameter. 
> 
> It's confusing!
> 
> [snip]
> 
> A *reasonable* definition (although not the only one) would be
> to count the number of fixed-effect parameters ('beta' in much
> of the literature) and the number of random-effect parameters
> (generally referred to as 'theta' in the lme4 documentation,
> but varying a great deal among references):
> 
> length(fixef(model))+length(getME(model,"theta"))
> 
>   in your case that's 3 parameters for the fixed effects
> and 6 RE parameters (you have a 3x3 variance-covariance matrix of the
> random effects, the matrix is symmetric, so counting the
> diagonal plus one triangle gives 3*(3+1)/2 = 6).  Generally one
> doesn't count the residual variance since that is estimated
> from the (penalized) residual sum of squares.
> 
>   It really depends what you want to use the RSE for.  It may
> very well not have the properties you're expecting (i.e. the
> properties that it has in a simple (non-mixed) linear model ...)
> 
>   There's a bit more about parameter-counting issues at
> http://glmm.wikidot.com/faq, I think ...
> 
>> model<-lmer((log.mass)~centre.log.dbh+centre.log.height+
>> (1+centre.log.dbh+centre.log.height|species_site),
>> data=allometry_2,REML=T) 
>>
>>  model summary output table:
>>
>> Linear mixed model fit by REML 
>> Formula: (log.mass) ~ centre.log.dbh + centre.log.height +
>> (1 + centre.log.dbh + centre.log.height | species_site) 
> 
> [snip]
>> Random effects:
>>  Groups       Name              Variance  Std.Dev. Corr          
>>  species_site (Intercept)       0.061636  0.24827                
>>               centre.log.dbh    0.279362  0.52855   0.575        
>>               centre.log.height 0.285753  0.53456  -0.441 -0.755 
>>  Residual                       0.086963  0.29489                
>> Number of obs: 337, groups: species_site, 18
>>
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org
> <mailto:R-sig-mixed-models at r-project.org> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
>


From jbaldwin at fs.fed.us  Fri Aug  3 03:37:59 2012
From: jbaldwin at fs.fed.us (Baldwin, Jim -FS)
Date: Fri, 3 Aug 2012 01:37:59 +0000
Subject: [R-sig-ME] basic question
In-Reply-To: <501B21BC.1090404@gmail.com>
References: <1343826182.94874.YahooMailNeo@web132205.mail.ird.yahoo.com>
	<loom.20120802T034230-54@post.gmane.org>
	<1343906088.83232.YahooMailNeo@web132202.mail.ird.yahoo.com>
	<501B21BC.1090404@gmail.com>
Message-ID: <DDC5EC9B78340042B0D5A0C3789D45690ECA3EE2@001FSN2MPN1-062.001f.mgd2.msft.net>

If the objective is to obtain an approximately unbiased estimate for the prediction of a future observation that would have the same variance components as found in the fitted model, then multiplying the backtransformed value by exp of "something like sum(sapply(VarCorr(model),diag))+sigma(model)^2 ..."/2 is a reasonable thing to do as Dr. Bolker suggests.

But if the approximately unbiased prediction is for a different situation which might not include either the same or all of the variance components in the fitted model, you'll need to think about which ones to use.

Also, (and this is likely very obvious) if the multiplicative correction is much larger than around 1.5, I'd have to wonder about the goodness-of-fit of the distributional assumptions of the model.  Sometimes this bias correction can give results that are way outside of observed values on the original scale.  (And, at least for me, I'd also check my code in such situations.  It's many times my fault and not the data's fault.)

Jim

Jim Baldwin
Station Statistician
Pacific Southwest Research Station
USDA Forest Service


-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Ben Bolker
Sent: Thursday, August 02, 2012 5:56 PM
To: Rachel Cohen; r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] basic question

 [cc'ing back to r-sig-mixed-models]

On 12-08-02 07:14 AM, Rachel Cohen wrote:
> Hi, thanks for your reply.  I was reading the Pinheiro and Bates book
> (pg.21) which says that for a basic model the parameters are: ?, ?2b ,
> and ?2 which is why I thought the residual variance was counted as a
> parameter?  Also, on the same page: "Although the random effects, bi,
> i = 1, . . .,M may behave like parameters, formally they are just
> another level of random variation in the model so we do not ?estimate?
> them as such."  That lead me to believe that the random effects are
> not counted as parameters?

   First, you need to make the distinction between the b_i value ('BLUPs', or 'conditional modes', or 'random effect estimates' -- the estimates of what's happening in each grouping level) -- and the theta values (the variances of the random effects).
>
> I am intending on using the RSE in the formula: CF = exp (RSE^2
> /2) which is supposed to correct for log-bias in estimates (in this
> case estimated tree biomass).  Each unlogged estimate is multiplied by
> the correction factor (CF).  I haven't come across any mention in the
> literature of whether it is appropriate to use the RSE from a
> mixed-effects model in this way.  I hope so?

  If you want to fit on the log scale but estimate the *mean* effects on the unlogged scale, you should (I think) add the exp(TOTAL_variance/2); in this case TOTAL_variance will be the sum of *all of the variance
components* in the model, including the residual variance (I'm going to assume you have only intercept random effects in the model, otherwise things would get a bit more complicated ...) I *think* it would be something like sum(sapply(VarCorr(model),diag))+sigma(model)^2 ...  I think there has been a reasonably recent post by Jarrod Hadfield answering someone's question related to this topic ...

  If I'm right, then there's no need to go messing around with numbers of parameters ...
  Ben Bolker





------------------------
> *From:* Ben Bolker <bbolker at gmail.com>
> *To:* r-sig-mixed-models at r-project.org
> *Sent:* Thursday, 2 August 2012, 2:51
> *Subject:* Re: [R-sig-ME] basic question
>
> Rachel Cohen <stat.list at ...> writes:
>
>>  Hi, I have a very basic query (I think).  If I fit a model of the
>> specification given below how many model parameters are there?  Am I
>> correct in thinking that the model parameters are the fixed effects
>> intercept and coefficients (in this case two of them) and also a
>> parameter for within and for between group variance?  So in total 5
>> fitted parameters? or are there also covariance parameters?.  I am
>> trying to calculate the standard error of the residuals as:
>
>   Unfortunately, this is an easy question to state but not (in my
> opinion) an easy question to answer unambiguously.
>
>
>> RSE = ((sum(residuals^2))/(N - no.of parameters))
>>
>> and therefore need to know how many parameters my model actually has.
>> I've gotten a little confused by the
>> literature definitions of what constitutes a parameter.
>
> It's confusing!
>
> [snip]
>
> A *reasonable* definition (although not the only one) would be to
> count the number of fixed-effect parameters ('beta' in much of the
> literature) and the number of random-effect parameters (generally
> referred to as 'theta' in the lme4 documentation, but varying a great
> deal among references):
>
> length(fixef(model))+length(getME(model,"theta"))
>
>   in your case that's 3 parameters for the fixed effects and 6 RE
> parameters (you have a 3x3 variance-covariance matrix of the random
> effects, the matrix is symmetric, so counting the diagonal plus one
> triangle gives 3*(3+1)/2 = 6).  Generally one doesn't count the
> residual variance since that is estimated from the (penalized)
> residual sum of squares.
>
>   It really depends what you want to use the RSE for.  It may very
> well not have the properties you're expecting (i.e. the properties
> that it has in a simple (non-mixed) linear model ...)
>
>   There's a bit more about parameter-counting issues at
> http://glmm.wikidot.com/faq, I think ...
>
>> model<-lmer((log.mass)~centre.log.dbh+centre.log.height+
>> (1+centre.log.dbh+centre.log.height|species_site),
>> data=allometry_2,REML=T)
>>
>>  model summary output table:
>>
>> Linear mixed model fit by REML
>> Formula: (log.mass) ~ centre.log.dbh + centre.log.height +
>> (1 + centre.log.dbh + centre.log.height | species_site)
>
> [snip]
>> Random effects:
>>  Groups       Name              Variance  Std.Dev. Corr
>>  species_site (Intercept)       0.061636  0.24827
>>               centre.log.dbh    0.279362  0.52855   0.575
>>               centre.log.height 0.285753  0.53456  -0.441 -0.755
>>  Residual                       0.086963  0.29489
>> Number of obs: 337, groups: species_site, 18
>>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org
> <mailto:R-sig-mixed-models at r-project.org> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models




This electronic message contains information generated by the USDA solely for the intended recipients. Any unauthorized interception of this message or the use or disclosure of the information it contains may violate the law and subject the violator to civil or criminal penalties. If you believe you have received this message in error, please notify the sender and delete the email immediately.

From msaunder at purdue.edu  Fri Aug  3 15:26:04 2012
From: msaunder at purdue.edu (Saunders, Michael R)
Date: Fri, 3 Aug 2012 09:26:04 -0400
Subject: [R-sig-ME] Question on statistical methods in manuscript
Message-ID: <A4DC70C5A3C1F740B46A6F153284A99D3CB713C24C@VPEXCH09.purdue.lcl>

All:

Although this is not specific to R, I have a technical question regarding a mixed-effects model being presented in a paper that I am reviewing.  I am not a statistician by training, but know enough of mixed-models to use them (and obviously not enough to understand some of the technical aspects of them).

The experimental design that was presented included sampling sections of a log (3 sections per tree) from 2-3 trees per treatment (2 or 3 treatments per stand) in 3 stands.  The independent variables were counts of internal structures in the log stems (e.g., structure A, structure B, etc).

The authors used SAS to "link" the structures to one another.  So for example,

Structure A = a + b*Structure B

They used random effects for stand, treatment (nested in stand), and tree (nested in treatment and experiment).  They present that only the intercept had random effects associated with it (although I question that approach from a biological standpoint).

So my question:
With only 18 trees total (54 log sections), is it even possible to fit a 3-level mixed-effects model?  My experience with multi-level models is that you have to have a large enough sample size within the innermost experiment units (maybe not all of them, but some of them) to estimate the variance of the parameter(s).  Here, there is only 3 in that innermost unit and even with a linear model, I find it hard to believe that the variance would be estimated well.

Thanks in advance,

Mike

From stat.list at yahoo.co.uk  Fri Aug  3 16:19:54 2012
From: stat.list at yahoo.co.uk (Rachel Cohen)
Date: Fri, 3 Aug 2012 15:19:54 +0100 (BST)
Subject: [R-sig-ME] basic question
In-Reply-To: <DDC5EC9B78340042B0D5A0C3789D45690ECA3EE2@001FSN2MPN1-062.001f.mgd2.msft.net>
References: <1343826182.94874.YahooMailNeo@web132205.mail.ird.yahoo.com>
	<loom.20120802T034230-54@post.gmane.org>
	<1343906088.83232.YahooMailNeo@web132202.mail.ird.yahoo.com>
	<501B21BC.1090404@gmail.com>
	<DDC5EC9B78340042B0D5A0C3789D45690ECA3EE2@001FSN2MPN1-062.001f.mgd2.msft.net>
Message-ID: <1344003594.22806.YahooMailNeo@web132201.mail.ird.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120803/321495a2/attachment.pl>

From m.fairbrother at bristol.ac.uk  Fri Aug  3 19:11:59 2012
From: m.fairbrother at bristol.ac.uk (Malcolm Fairbrother)
Date: Fri, 3 Aug 2012 18:11:59 +0100
Subject: [R-sig-ME] two questions about clmm
In-Reply-To: <CAG_uk9015VEH6yNk=j5E8anOanVM8Q-A4UEkQatus=-aifUS1w@mail.gmail.com>
References: <78678DB3-9021-44DE-899B-68D71E64E042@bristol.ac.uk>
	<CAG_uk93R0z8R-2toMV5SUes+_2c+Hs0SRNX0JGM-EUxNhLf5+Q@mail.gmail.com>
	<42206B2D-3125-4CF1-9CBC-2ED5C2A5F6CB@bristol.ac.uk>
	<CAG_uk91iBuYna+icaHkD2mn9NkW2OSQ8XVAdQ=tA8jhAP0g5kw@mail.gmail.com>
	<D094D7ED-53D2-4800-A15A-B01A729274EC@bristol.ac.uk>
	<CAG_uk9015VEH6yNk=j5E8anOanVM8Q-A4UEkQatus=-aifUS1w@mail.gmail.com>
Message-ID: <61327C50-05BF-4D7C-9743-AE3280A1EA20@bristol.ac.uk>

Dear Rune (and list),

I've been making use of clmm, and have two (potentially over-ambitious) questions. If Rune or anyone else can offer any insights about either, that would be much appreciated.

First, I noticed something intriguing in the ordinal package documentation: the "## Binomial example with data from the lme4-package example", for clmm2. This suggests a way of shortening large datasets (with one Bernoulli trial per row) into shorter ones (with counts on each row, representing many trials), rather like "glmer(cbind(incidence, size - incidence)?" does for lme4. This obviously speeds up model fitting tremendously. However, I was wondering if there's any way to do this for outcomes with more than two levels (i.e., not just binomial, but multinomial)? This may not be possible or even make sense, but I thought I'd ask, given the example that was in the documentation.

Second, I often use "simulate" with fitted mer objects (from lme4), to get confidence intervals for quantities of interest. (Using "refit" and "simulate" together is fast.) Is there any similar way to simulate and refit fitted clmm objects?

Many thanks,
Malcolm


Dr Malcolm Fairbrother
School of Geographical Sciences
University of Bristol


From Tom_Philippi at nps.gov  Fri Aug  3 20:45:21 2012
From: Tom_Philippi at nps.gov (Tom_Philippi at nps.gov)
Date: Fri, 3 Aug 2012 11:45:21 -0700
Subject: [R-sig-ME] reaped measures with unequal intervals
In-Reply-To: <F82E4A0E5D9F0D44939AC32130AE0BF42DB8BC9B@SBSSERVER.sbs.local>
References: <F82E4A0E5D9F0D44939AC32130AE0BF42DB8BC9B@SBSSERVER.sbs.local>
Message-ID: <OFD4A9E419.FA8C7478-ON85257A4F.0060C55A-88257A4F.0067074D@nps.gov>

Ahmad--

The unequal intervals part is rather simple.  Let Time be your variable
with values {6, 13,15}.  If you expect a linear trend over time within
subjects, then Time should be numeric; if not make Time a factor.  Equal
spacing of repeated measures would let you do something between these two
extremes: decompose trends over time into simple polynomial (linear,
quadratic, cubic, etc.) trends.  But, with only 3 dates, you wouldn't be
doing that anyway.

Everything else in the analyses depends on your experimental or sampling
design, response variable, etc., and the questions you are trying to
address with your analyses.  In my case of fixed islands, random sites
within islands, and balanced revisit dates (not evenly spaced because some
years are missing, but those years are missing for all sites), I need to
fit both an intercept and a temporal trend in my random effects: (1+Year |
Site).  With Year as numeric, I am fitting a single parameter linear trend
over time (estimating the variance among Plots in their linear temporal
trends).  With Year as a factor, I would be fitting nlevels(Year)
parameters for N : intercept (==Year 1) and 1 parameter for the difference
of each subsequent year (or (0+Year|Plot), which would give me
nlevels(Year) parameters, each directly interpretable as the (variance
across plots of the) mean for that Year).

I hope that this helps point you in the right direction.

Tom



                                                                           
             Ahmad Rabiee                                                  
             <AhmadR at sbscibus.                                             
             com.au>                                                    To 
             Sent by:                  "r-sig-mixed-models at r-project.org"  
             r-sig-mixed-model         <r-sig-mixed-models at r-project.org>  
             s-bounces at r-proje                                          cc 
             ct.org                                                        
                                                                   Subject 
                                       [R-sig-ME] reaped measures with     
             05/22/2012 04:51          unequal intervals                   
             AM GMT                                                        
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




Hi

I have a repeated measures dataset with unequal intervals (spacing) - the
intervals are Day 6, Day 13 and Day 15.
Anyone knows how to handle this type of data?

Thanks
Ahmad


             [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From bbolker at gmail.com  Fri Aug  3 23:37:58 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 3 Aug 2012 21:37:58 +0000 (UTC)
Subject: [R-sig-ME] basic question
References: <1343826182.94874.YahooMailNeo@web132205.mail.ird.yahoo.com>
	<loom.20120802T034230-54@post.gmane.org>
	<1343906088.83232.YahooMailNeo@web132202.mail.ird.yahoo.com>
	<501B21BC.1090404@gmail.com>
	<DDC5EC9B78340042B0D5A0C3789D45690ECA3EE2@001FSN2MPN1-062.001f.mgd2.msft.net>
	<1344003594.22806.YahooMailNeo@web132201.mail.ird.yahoo.com>
Message-ID: <loom.20120803T225428-71@post.gmane.org>

Rachel Cohen <stat.list at ...> writes:

>  Hi, thank you both for your replies. However I remain a little
> confused.  Firstly apologies as I noticed that in my original
> posting I had left out the sqrt in the formula for the residual
> standard error given by Sprugel (1983) - RSE =
> sqrt((sum(residuals^2))/(N - no.of parameters)).  In addition to a
> random effects term for the intercept my model (see below) also has
> a random effects term for both explanatory variables (dbh and
> height) so I guess this changes things and makes it more
> complicated?

  (I was a bit amused that Sprugel 1983 makes a big deal about
the denominator needing to contain N-2 instead of N-1 ; yes,
he's right, but in practical situations it should rarely make
any difference ... his point about the base of the logarithm
is much more important.)

  Yes (I should have noticed this previously).  The mean of the
predicted distribution for any particular set of predictors (e.g. a
tree of a particular size) will be exp([predicted log
mean])*exp([predicted log variance]/2); I believe (but haven't worked
it out) that when there are random effects of continuous predictors
that the variance as well as the mean will then depend on the
continuous predictors, e.g.

  log(Y) = a + eps_1 + (b+eps_2)*dbh

 var(log(Y)) = var(eps_1) + var(eps_2)*dbh^2

  so the bias correction term will depend on the predictors.

  (It's even a little bit worse than that, since some of the
random effects are correlated with each other ...)
 
> Although I understand what the code you suggested is doing (summing
> the variances) I'm afraid my understanding of statistics is perhaps
> not good enough to know how and if this is equivalent to the formula
> given by Sprugel (which seems to be the accepted way of working out
> the residual error and correcting estimates for log-bias).  Can
> Sprugels formula as it's given not be used with mixed effects
> models?   I used resid(model) to grab the residuals for use in the
> above formula, maybe this wasn't correct?   I'm unclear as to why
> all the variances should be used to correct the final estimates of
> biomass for each tree and not just the residual variance (as in
> Sprugel)?

  Sprugel's formula is definitely intended for simpler cases -- i.e.
simple log-log allometric regressions.

  The basic idea is that, if mu is the mean of the data on the log
scale, and if the data (or the predictions) have any variability,
then the mean of the data on the original scale is greater than
exp(mu) -- this is a consequence of a mathematical rule called
Jensen's inequality.  As Jim Baldwin says, you may need to think
hard about what situation you're actually trying to predict the
mean for.

> When I work out the CF using the above formula I get a value of
> ~1.0444 (very reasonable) and using the code you sent me I get a
> value of 1.529! (not very reasonable and quite worrying).  So any
> further help in explaining this would be much appreciated as I am
> now quite concerned about the goodness-of-fit of my model if the
> value of ~1.5 is correct.

  It sounds like you might need to dig a little deeper ...

> 
> Many thanks for your help,
> 
> Rachel
> 
> model<-lmer((log.mass)~centre.log.dbh+centre.log.height+ 
> (1+centre.log.dbh+centre.log.height|species_site),
>  data=allometry_2,REML=T) 
> 
> ________________________________
>  From: "Baldwin, Jim -FS" <jbaldwin at ...>
> 
> r-sig-mixed-models at ..."
> <r-sig-mixed-models at ...> 
> Sent: Friday, 3 August 2012, 2:37
> Subject: RE: [R-sig-ME] basic question


> If the objective is to obtain an approximately unbiased estimate for
> the prediction of a future observation that would have the same
> variance components as found in the fitted model, then multiplying
> the backtransformed value by exp of "something like
> sum(sapply(VarCorr(model),diag))+sigma(model)^2 ..."/2 is a
> reasonable thing to do as Dr. Bolker suggests.

> But if the approximately unbiased prediction is for a different
> situation which might not include either the same or all of the
> variance components in the fitted model, you'll need to think about
> which ones to use.
 
> Also, (and this is likely very obvious) if the multiplicative
> correction is much larger than around 1.5, I'd have to wonder about
> the goodness-of-fit of the distributional assumptions of the model.
> Sometimes this bias correction can give results that are way outside
> of observed values on the original scale.  (And, at least for me,
> I'd also check my code in such situations.  It's many times my fault
> and not the data's fault.)
 
> Jim
> 
> Jim Baldwin
> Station Statistician
> Pacific Southwest Research Station
> USDA Forest Service

  [snipping stuff to make Gmane happy]


From Victoria.Buller at paigntonzoo.org.uk  Sat Aug  4 16:19:48 2012
From: Victoria.Buller at paigntonzoo.org.uk (Victoria Buller)
Date: Sat, 4 Aug 2012 15:19:48 +0100
Subject: [R-sig-ME] confusing error message
Message-ID: <A0E8448DEED0814C812405997AB45635AFA8B6A7B5@PZOODC02EX.paignton-zoo.lan>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120804/cb09bbb6/attachment.pl>

From bbolker at gmail.com  Sat Aug  4 22:07:20 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 4 Aug 2012 20:07:20 +0000 (UTC)
Subject: [R-sig-ME] confusing error message
References: <A0E8448DEED0814C812405997AB45635AFA8B6A7B5@PZOODC02EX.paignton-zoo.lan>
Message-ID: <loom.20120804T220656-764@post.gmane.org>

Victoria Buller <Victoria.Buller at ...> writes:

> 
> 
> ________________________________
> Disclaimer:
> The information contained herein is confidential, may ...{{dropped:13}}


From jeaggu at gmail.com  Sun Aug  5 12:39:00 2012
From: jeaggu at gmail.com (Jesus)
Date: Sun, 5 Aug 2012 12:39:00 +0200
Subject: [R-sig-ME] same within group std. error
Message-ID: <CA+by4T5kuJ4NFQwQ_zYowNARd5LQ3sGnXjqW_PxnZgpPDkAq_w@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120805/4e35e155/attachment.pl>

From Victoria.Buller at paigntonzoo.org.uk  Sun Aug  5 11:17:19 2012
From: Victoria.Buller at paigntonzoo.org.uk (Victoria Buller)
Date: Sun, 5 Aug 2012 10:17:19 +0100
Subject: [R-sig-ME] confusing error message
Message-ID: <A0E8448DEED0814C812405997AB45635AFA8B6A7C8@PZOODC02EX.paignton-zoo.lan>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120805/74065fc2/attachment.pl>

From pdalgd at gmail.com  Sun Aug  5 16:37:52 2012
From: pdalgd at gmail.com (peter dalgaard)
Date: Sun, 5 Aug 2012 16:37:52 +0200
Subject: [R-sig-ME] same within group std. error
In-Reply-To: <CA+by4T5kuJ4NFQwQ_zYowNARd5LQ3sGnXjqW_PxnZgpPDkAq_w@mail.gmail.com>
References: <CA+by4T5kuJ4NFQwQ_zYowNARd5LQ3sGnXjqW_PxnZgpPDkAq_w@mail.gmail.com>
Message-ID: <0B67480C-191B-4583-8430-4C543F1EC89A@gmail.com>


On Aug 5, 2012, at 12:39 , Jesus wrote:

> Dear all,
> I have a question regarding mixed effect models and I have not been
> successful in finding the answer in the sites.
> 
> I have my response variable "a" and the explanatory variables "distance",
> "numberAni" and "factor(algorithms)---with  seven different algorithms, I
> also have my random variable "species" (with 5 different species). One of
> the algorithms is set as the intercept in the model.
> 
> I run my model:
> 
> model_1<-(lme(a)~ numberAni*factor(algorithm)+distance*factor(algorithm),
> random=~1|species,data=datamodel)
> 
> An my summary table is as follows. My question is* why* do I get the *same
> Std. Error* for all the algorithms in the first part of the table and then
> other same std. error for the first interactions and then the new one for
> the second interactions? why are the within groups std. errors the same?


Your Subject: suggests that you might not have understood that the column labeled Std.Error is the accuracy of the estimates in the preceding column. This is a function of the estimated variance parameters and of the design -- no. of replications, etc. In a balanced design, which is probably what you have, the factor levels enter interchangeably, so the Std.Error for each level is the same function of the variances. 

> 
> 
>                                    Value  *Std.Error*   DF    t-value p-value
> (Intercept)                     1.0676374 0.28408892 4677   3.758110  0.0002
> NumberAni                       -0.0001166 0.00022606   13  -0.515724  0.6147
> factor(algorithm)A1             -2.8588307 0.14268775 4677 -20.035572  0.0000
> factor(algorithm)A2             -0.3869196 0.14268775 4677  -2.711653  0.0067
> factor(algorithm)A3             -0.6668651 0.14268775 4677  -4.673597  0.0000
> factor(algorithm)A4             -0.6600452 0.14268775 4677  -4.625802  0.0000
> factor(algorithm)A5             -0.3995352 0.14268775 4677  -2.800066  0.0051
> factor(algorithm)A6             -0.8642987 0.14268775 4677  -6.057273  0.0000
> distance                       -0.0000014 0.00000204   13  -0.685703  0.5049
> 
> NumberAni:factor(algorithm)A1   0.0007324 0.00010871 4677   6.737032  0.0000
> 
> NumberAni:factor(algorithm)A2    0.0003483 0.00010871 4677   3.203698  0.0014
> 
> NumberAni:factor(algorithm)A3    0.0004839 0.00010871 4677   4.451614  0.0000
> 
> NumberAni:factor(algorithm)A4    0.0004649 0.00010871 4677   4.277108  0.0000
> 
> NumberAni:factor(algorithm)A5    -0.0000330 0.00010871 4677  -0.303855  0.7613
> 
> NumberAni:factor(algorithm)A6    0.0004812 0.00010871 4677   4.426913  0.0000
> 
> factor(algorithm)A1:distance 0.0000085 0.00000099 4677 8.577618 0.0000
> factor(algorithm)A2:distance 0.0000023 0.00000099 4677 2.309649 0.0210
> factor(algorithm)A3:distance 0.0000024 0.00000099 4677 2.468800 0.0136
> factor(algorithm)A4:distance 0.0000030 0.00000099 4677 3.008320 0.0026
> factor(algorithm)A5:distance -0.0000001 0.00000099 4677 -0.064863 0.9483
> factor(algorithm)A6:distance 0.0000033 0.00000099 4677 3.309633 0.0009
> 
> 
> 
> If someone could explain it to me it would be great. I am sorry if this is
> a question already answered before.
> 
> Regards,
> 
> Jag
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From bbolker at gmail.com  Mon Aug  6 06:19:22 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 6 Aug 2012 04:19:22 +0000 (UTC)
Subject: [R-sig-ME] confusing error message
References: <A0E8448DEED0814C812405997AB45635AFA8B6A7C8@PZOODC02EX.paignton-zoo.lan>
Message-ID: <loom.20120806T060852-502@post.gmane.org>

Victoria Buller <Victoria.Buller at ...> writes:

> Can someone explain to me what this error message means? -
> 
> () : Downdated VtV is not positive definite
> 
> I keep getting it after using this code for my data-

> > m1<-glmer(a.martiensseni~distance.tree+herbangular.trans+
    shrubangular.trans+canopyangular.trans+soilangular.trans+
    leaflitangular.trans+sandangular.trans+woodangular.trans+
    waterangular.trans+cobblesangular.trans+rocksangular.trans+
    archaangular.trans+archbangular.trans+archcangular.trans+
    archdangular.trans+archeangular.trans+alt+slope+start.time+
    (1|site)+(1|location),family=binomial)

> I get that it is to do with my random effects... but I don't
> understand why there is a problem. 'Site' is just an area known as
> A,B,C,D or E and location is the location within the site the frog
> was found, so either transect 1,2, or 3. I am trying to analyse frog
> presence or absence data with habitat characteristics...  can anyone
> help????

  Site is just on the edge of having enough levels to estimate
a random effect reliably, and transect has not enough (the rule
of thumb is a *minimum* of 5-6: see e.g. http://glmm.wikidot.com/faq ).
If you have three transects within each site, I suspect you want
(1|site/location) or (equivalently) (1|site)+(1|site/location).

  I hope you have a pretty big data set: one rule of thumb is
that you need the effective size of the data set to about
10 times the number of parameters; for binary data 'effective size'
is (approximately) the minimum of the number of presences and
the number of absences, so the minimum of # presences/# absences
should be at least 200 or so ...

   Frank Harrell's book is a good reference for what to do
if you have more predictors than your data can support.

  Ben Bolker


From longrob604 at gmail.com  Mon Aug  6 11:56:53 2012
From: longrob604 at gmail.com (Robert Long)
Date: Mon, 6 Aug 2012 10:56:53 +0100
Subject: [R-sig-ME] Extracting the posterior distribution for a random
	effect in MCMCglmm
Message-ID: <CA+3TTkPmWEqNnfV-YedR6C5gKwjiwP+ziMh170F=8MyLZuBgMQ@mail.gmail.com>

Hello

I would like to extract the data for the posterior distribution for a
random effect in MCMCglmm.  Using the example in the tutorial:

data(Traffic)
prior <- list(R = list(V = 1, nu = 0.002), G = list(G1 = list(V = 1,
nu = 0.002)))
m2a.7 <- MCMCglmm(y ~ year + limit + as.numeric(day), random = ~day,
family = "poisson", data = Traffic, prior = prior, verbose = FALSE, pr=T)

summary(m2a.7)

This gives:

 G-structure:  ~day
    post.mean l-95% CI u-95% CI eff.samp
day   0.09326  0.06076   0.1313    266.8

How can I extract the data that gives this mean and 95% BCI ?

I can see that I can obtain the results for the fixed effects by such as:
mean(m2a.7$Sol[,1]) which gives the posterior mean for the first fixed
effect. But how can I do that for the random effects ? I can see that
there are data in m2a.7$Sol[,5:96] but these don't seem to be
variances as many are negative.

A related question is: quantile(m2a.7$Sol[,1],c(0.025,0.975),type = 1)
does not give precisely the same interval as in summary(m2a.7) - I
wonder why there is a difference ?

Thanks !

Robert Long
Postgraduate student
University of Leeds / UK


From j.hadfield at ed.ac.uk  Mon Aug  6 12:11:03 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Mon, 06 Aug 2012 11:11:03 +0100
Subject: [R-sig-ME] Extracting the posterior distribution for a random
 effect in MCMCglmm
In-Reply-To: <CA+3TTkPmWEqNnfV-YedR6C5gKwjiwP+ziMh170F=8MyLZuBgMQ@mail.gmail.com>
References: <CA+3TTkPmWEqNnfV-YedR6C5gKwjiwP+ziMh170F=8MyLZuBgMQ@mail.gmail.com>
Message-ID: <20120806111103.21133nxr7mbwpsis@www.staffmail.ed.ac.uk>

Hi,

specifying pr=TRUE in the call to MCMCglmm saves the posterior  
distribution of all location effects (fixed and random). They appear  
in Sol.

summary uses HPDinterval not quantile. HPDinterval (with prob=0.95)  
finds the shortest interval which contains 95% of the posterior  
samples, which may be different from quantile which just finds the  
lowest and highest 2.5%.

Cheers,

Jarrod



Quoting Robert Long <longrob604 at gmail.com> on Mon, 6 Aug 2012 10:56:53 +0100:

> Hello
>
> I would like to extract the data for the posterior distribution for a
> random effect in MCMCglmm.  Using the example in the tutorial:
>
> data(Traffic)
> prior <- list(R = list(V = 1, nu = 0.002), G = list(G1 = list(V = 1,
> nu = 0.002)))
> m2a.7 <- MCMCglmm(y ~ year + limit + as.numeric(day), random = ~day,
> family = "poisson", data = Traffic, prior = prior, verbose = FALSE, pr=T)
>
> summary(m2a.7)
>
> This gives:
>
>  G-structure:  ~day
>     post.mean l-95% CI u-95% CI eff.samp
> day   0.09326  0.06076   0.1313    266.8
>
> How can I extract the data that gives this mean and 95% BCI ?
>
> I can see that I can obtain the results for the fixed effects by such as:
> mean(m2a.7$Sol[,1]) which gives the posterior mean for the first fixed
> effect. But how can I do that for the random effects ? I can see that
> there are data in m2a.7$Sol[,5:96] but these don't seem to be
> variances as many are negative.
>
> A related question is: quantile(m2a.7$Sol[,1],c(0.025,0.975),type = 1)
> does not give precisely the same interval as in summary(m2a.7) - I
> wonder why there is a difference ?
>
> Thanks !
>
> Robert Long
> Postgraduate student
> University of Leeds / UK
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From Thierry.ONKELINX at inbo.be  Mon Aug  6 12:17:07 2012
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Mon, 6 Aug 2012 10:17:07 +0000
Subject: [R-sig-ME] Extracting the posterior distribution for a
	random	effect in MCMCglmm
In-Reply-To: <CA+3TTkPmWEqNnfV-YedR6C5gKwjiwP+ziMh170F=8MyLZuBgMQ@mail.gmail.com>
References: <CA+3TTkPmWEqNnfV-YedR6C5gKwjiwP+ziMh170F=8MyLZuBgMQ@mail.gmail.com>
Message-ID: <AA818EAD2576BC488B4F623941DA74275E393116@inbomail.inbo.be>

Dear Robert,

m2a.7$Sol store fixed effect parameters and random effect parameters (if pr = TRUE). The variances are stored in m2a.7$VCV

Best regards,

Thierry

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Robert Long
Verzonden: maandag 6 augustus 2012 11:57
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] Extracting the posterior distribution for a random effect in MCMCglmm

Hello

I would like to extract the data for the posterior distribution for a random effect in MCMCglmm.  Using the example in the tutorial:

data(Traffic)
prior <- list(R = list(V = 1, nu = 0.002), G = list(G1 = list(V = 1, nu = 0.002)))
m2a.7 <- MCMCglmm(y ~ year + limit + as.numeric(day), random = ~day, family = "poisson", data = Traffic, prior = prior, verbose = FALSE, pr=T)

summary(m2a.7)

This gives:

 G-structure:  ~day
    post.mean l-95% CI u-95% CI eff.samp
day   0.09326  0.06076   0.1313    266.8

How can I extract the data that gives this mean and 95% BCI ?

I can see that I can obtain the results for the fixed effects by such as:
mean(m2a.7$Sol[,1]) which gives the posterior mean for the first fixed effect. But how can I do that for the random effects ? I can see that there are data in m2a.7$Sol[,5:96] but these don't seem to be variances as many are negative.

A related question is: quantile(m2a.7$Sol[,1],c(0.025,0.975),type = 1) does not give precisely the same interval as in summary(m2a.7) - I wonder why there is a difference ?

Thanks !

Robert Long
Postgraduate student
University of Leeds / UK

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From longrob604 at gmail.com  Mon Aug  6 12:28:04 2012
From: longrob604 at gmail.com (Robert Long)
Date: Mon, 6 Aug 2012 11:28:04 +0100
Subject: [R-sig-ME] Extracting the posterior distribution for a random
 effect in MCMCglmm
In-Reply-To: <20120806111103.21133nxr7mbwpsis@www.staffmail.ed.ac.uk>
References: <CA+3TTkPmWEqNnfV-YedR6C5gKwjiwP+ziMh170F=8MyLZuBgMQ@mail.gmail.com>
	<20120806111103.21133nxr7mbwpsis@www.staffmail.ed.ac.uk>
Message-ID: <CA+3TTkMefZ5WGfqY+b__Ga6vYq7HPr549iuYnDd_Mz8z8fTeEg@mail.gmail.com>

Hi Jarrod, thanks for your reply.

I understand about the interval calculation now. However, I'm sorry
that I still don't see how to get the random effects myself.  I see I
can get the posterior mean for fixed effects by mean(m2a.7$Sol[,1])
and HPDinterval(m2a.7$Sol[,1],prob=0.95) etc, I see there are data in
m2a.7$Sol in columns after the fixed effects in columns 5 through 96
for each of the days, but how do I reproduce this

>>  G-structure:  ~day
>>     post.mean l-95% CI u-95% CI
>> day   0.09326  0.06076   0.1313

from m2a.7$Sol[,5:96])  ? I would like to do
mean(something)
and
HPDinterval(something, prob = 0.95)

So what is the "something" ?

Thanks again

Robert Long
Postgraduate student
University of Leeds / UK

On Mon, Aug 6, 2012 at 11:11 AM, Jarrod Hadfield <j.hadfield at ed.ac.uk> wrote:
> Hi,
>
> specifying pr=TRUE in the call to MCMCglmm saves the posterior distribution
> of all location effects (fixed and random). They appear in Sol.
>
> summary uses HPDinterval not quantile. HPDinterval (with prob=0.95) finds
> the shortest interval which contains 95% of the posterior samples, which may
> be different from quantile which just finds the lowest and highest 2.5%.
>
> Cheers,
>
> Jarrod
>
>
>
>
> Quoting Robert Long <longrob604 at gmail.com> on Mon, 6 Aug 2012 10:56:53
> +0100:
>
>> Hello
>>
>> I would like to extract the data for the posterior distribution for a
>> random effect in MCMCglmm.  Using the example in the tutorial:
>>
>> data(Traffic)
>> prior <- list(R = list(V = 1, nu = 0.002), G = list(G1 = list(V = 1,
>> nu = 0.002)))
>> m2a.7 <- MCMCglmm(y ~ year + limit + as.numeric(day), random = ~day,
>> family = "poisson", data = Traffic, prior = prior, verbose = FALSE, pr=T)
>>
>> summary(m2a.7)
>>
>> This gives:
>>
>>  G-structure:  ~day
>>     post.mean l-95% CI u-95% CI eff.samp
>> day   0.09326  0.06076   0.1313    266.8
>>
>> How can I extract the data that gives this mean and 95% BCI ?
>>
>> I can see that I can obtain the results for the fixed effects by such as:
>> mean(m2a.7$Sol[,1]) which gives the posterior mean for the first fixed
>> effect. But how can I do that for the random effects ? I can see that
>> there are data in m2a.7$Sol[,5:96] but these don't seem to be
>> variances as many are negative.
>>
>> A related question is: quantile(m2a.7$Sol[,1],c(0.025,0.975),type = 1)
>> does not give precisely the same interval as in summary(m2a.7) - I
>> wonder why there is a difference ?
>>
>> Thanks !
>>
>> Robert Long
>> Postgraduate student
>> University of Leeds / UK
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>
>
>
> --
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
>
>


From j.hadfield at ed.ac.uk  Mon Aug  6 12:43:56 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Mon, 06 Aug 2012 11:43:56 +0100
Subject: [R-sig-ME] Extracting the posterior distribution for a random
 effect in MCMCglmm
In-Reply-To: <CA+3TTkMefZ5WGfqY+b__Ga6vYq7HPr549iuYnDd_Mz8z8fTeEg@mail.gmail.com>
References: <CA+3TTkPmWEqNnfV-YedR6C5gKwjiwP+ziMh170F=8MyLZuBgMQ@mail.gmail.com>
	<20120806111103.21133nxr7mbwpsis@www.staffmail.ed.ac.uk>
	<CA+3TTkMefZ5WGfqY+b__Ga6vYq7HPr549iuYnDd_Mz8z8fTeEg@mail.gmail.com>
Message-ID: <20120806114356.19606nc90bchqy04@www.staffmail.ed.ac.uk>

Hi,

This:

>>>  G-structure:  ~day
>>>     post.mean l-95% CI u-95% CI
>>> day   0.09326  0.06076   0.1313

summarises the posterior distribution of the variance of the normal  
distribution from which the day *effects* come from. Samples from this  
posterior distribution are stored in m2a.7$VCV[,"day"]. The effects  
themselves are in Sol.

Cheers,

Jarrod


Quoting Robert Long <longrob604 at gmail.com> on Mon, 6 Aug 2012 11:28:04 +0100:

> Hi Jarrod, thanks for your reply.
>
> I understand about the interval calculation now. However, I'm sorry
> that I still don't see how to get the random effects myself.  I see I
> can get the posterior mean for fixed effects by mean(m2a.7$Sol[,1])
> and HPDinterval(m2a.7$Sol[,1],prob=0.95) etc, I see there are data in
> m2a.7$Sol in columns after the fixed effects in columns 5 through 96
> for each of the days, but how do I reproduce this
>
>>>  G-structure:  ~day
>>>     post.mean l-95% CI u-95% CI
>>> day   0.09326  0.06076   0.1313
>
> from m2a.7$Sol[,5:96])  ? I would like to do
> mean(something)
> and
> HPDinterval(something, prob = 0.95)
>
> So what is the "something" ?
>
> Thanks again
>
> Robert Long
> Postgraduate student
> University of Leeds / UK
>
> On Mon, Aug 6, 2012 at 11:11 AM, Jarrod Hadfield <j.hadfield at ed.ac.uk> wrote:
>> Hi,
>>
>> specifying pr=TRUE in the call to MCMCglmm saves the posterior distribution
>> of all location effects (fixed and random). They appear in Sol.
>>
>> summary uses HPDinterval not quantile. HPDinterval (with prob=0.95) finds
>> the shortest interval which contains 95% of the posterior samples, which may
>> be different from quantile which just finds the lowest and highest 2.5%.
>>
>> Cheers,
>>
>> Jarrod
>>
>>
>>
>>
>> Quoting Robert Long <longrob604 at gmail.com> on Mon, 6 Aug 2012 10:56:53
>> +0100:
>>
>>> Hello
>>>
>>> I would like to extract the data for the posterior distribution for a
>>> random effect in MCMCglmm.  Using the example in the tutorial:
>>>
>>> data(Traffic)
>>> prior <- list(R = list(V = 1, nu = 0.002), G = list(G1 = list(V = 1,
>>> nu = 0.002)))
>>> m2a.7 <- MCMCglmm(y ~ year + limit + as.numeric(day), random = ~day,
>>> family = "poisson", data = Traffic, prior = prior, verbose = FALSE, pr=T)
>>>
>>> summary(m2a.7)
>>>
>>> This gives:
>>>
>>>  G-structure:  ~day
>>>     post.mean l-95% CI u-95% CI eff.samp
>>> day   0.09326  0.06076   0.1313    266.8
>>>
>>> How can I extract the data that gives this mean and 95% BCI ?
>>>
>>> I can see that I can obtain the results for the fixed effects by such as:
>>> mean(m2a.7$Sol[,1]) which gives the posterior mean for the first fixed
>>> effect. But how can I do that for the random effects ? I can see that
>>> there are data in m2a.7$Sol[,5:96] but these don't seem to be
>>> variances as many are negative.
>>>
>>> A related question is: quantile(m2a.7$Sol[,1],c(0.025,0.975),type = 1)
>>> does not give precisely the same interval as in summary(m2a.7) - I
>>> wonder why there is a difference ?
>>>
>>> Thanks !
>>>
>>> Robert Long
>>> Postgraduate student
>>> University of Leeds / UK
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>>
>>
>>
>>
>> --
>> The University of Edinburgh is a charitable body, registered in
>> Scotland, with registration number SC005336.
>>
>>
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From ned.dochtermann at gmail.com  Mon Aug  6 17:05:14 2012
From: ned.dochtermann at gmail.com (Ned Dochtermann)
Date: Mon, 06 Aug 2012 10:05:14 -0500
Subject: [R-sig-ME]  Extracting the posterior distribution for a random,
 effect in MCMCglmm
Message-ID: <501FDD2A.3090305@gmail.com>

The values you're referring to in the solution part of the output (Sol) 
are what for a linear model would be the BLUPs, hence some are negative.

The random effects variances live in VCV. If I recall correctly 
posterior modes and HPD intervals for a model like yours can be 
extracted as:
posterior.mode(m2a.7$VCV[,1])
HPDinterval(m2a.7$VCV[,1])

-- 
Ned A. Dochtermann
Assistant Professor / Department of Biological Sciences
*NORTH DAKOTA STATE UNIVERSITY*
www.ndsu.edu


Message: 5
Date: Mon, 6 Aug 2012 10:56:53 +0100
From: Robert Long<longrob604 at gmail.com>
To:r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Extracting the posterior distribution for a random
	effect in MCMCglmm
Message-ID:
	<CA+3TTkPmWEqNnfV-YedR6C5gKwjiwP+ziMh170F=8MyLZuBgMQ at mail.gmail.com>
Content-Type: text/plain; charset=ISO-8859-1

Hello

I would like to extract the data for the posterior distribution for a
random effect in MCMCglmm.  Using the example in the tutorial:

data(Traffic)
prior <- list(R = list(V = 1, nu = 0.002), G = list(G1 = list(V = 1,
nu = 0.002)))
m2a.7 <- MCMCglmm(y ~ year + limit + as.numeric(day), random = ~day,
family = "poisson", data = Traffic, prior = prior, verbose = FALSE, pr=T)

summary(m2a.7)

This gives:

  G-structure:  ~day
     post.mean l-95% CI u-95% CI eff.samp
day   0.09326  0.06076   0.1313    266.8

How can I extract the data that gives this mean and 95% BCI ?

I can see that I can obtain the results for the fixed effects by such as:
mean(m2a.7$Sol[,1]) which gives the posterior mean for the first fixed
effect. But how can I do that for the random effects ? I can see that
there are data in m2a.7$Sol[,5:96] but these don't seem to be
variances as many are negative.

A related question is: quantile(m2a.7$Sol[,1],c(0.025,0.975),type = 1)
does not give precisely the same interval as in summary(m2a.7) - I
wonder why there is a difference ?

Thanks !

Robert Long
Postgraduate student
University of Leeds / UK


From longrob604 at gmail.com  Mon Aug  6 17:07:38 2012
From: longrob604 at gmail.com (W Robert Long)
Date: Mon, 06 Aug 2012 16:07:38 +0100
Subject: [R-sig-ME] Extracting the posterior distribution for a random,
 effect in MCMCglmm
In-Reply-To: <501FDD2A.3090305@gmail.com>
References: <501FDD2A.3090305@gmail.com>
Message-ID: <501FDDBA.3090803@gmail.com>

Thanks Ned, Thierry and Jarrod

My code is working nicely now.

Regards
Rob



On 6/08/2012 4:05 PM, Ned Dochtermann wrote:
> The values you're referring to in the solution part of the output (Sol)
> are what for a linear model would be the BLUPs, hence some are negative.
>
> The random effects variances live in VCV. If I recall correctly
> posterior modes and HPD intervals for a model like yours can be
> extracted as:
> posterior.mode(m2a.7$VCV[,1])
> HPDinterval(m2a.7$VCV[,1])
>


From alexandre.m.martin at gmail.com  Mon Aug  6 19:27:08 2012
From: alexandre.m.martin at gmail.com (Alexandre Martin)
Date: Mon, 06 Aug 2012 13:27:08 -0400
Subject: [R-sig-ME] convergence slot in glmer?
Message-ID: <501FFE6C.40101@gmail.com>

Dear mixed modelers,

I am looking for a way to identify false convergence in models running 
in loops.
Warnings are : "In mer_finalize(ans) : false convergence (8)" but are 
not stored in any slot, which makes impossible their assignation to 
particular models in loops.

Did anybody already face this problem? Any advice will be appreciated.

Thank you very much.

Alexandre


From pwschmitt at gmail.com  Mon Aug  6 22:08:16 2012
From: pwschmitt at gmail.com (paul)
Date: Mon, 06 Aug 2012 13:08:16 -0700
Subject: [R-sig-ME] Model for irregular design
Message-ID: <50202430.5090300@gmail.com>

Dear All,
I am interested in using random effects/ mixed effects models for 
estimation purposes.  Suppose I study 7 treatments in each of 3 clinical 
studies with treatments allocated among studies as indicated; study 1: 
trt_1, trt_2, trt_3; study 2: trt_1, trt_4, trt_5; study 3: trt_1, 
trt_6, trt_7.  I recognize the fact that this design set up is 
asymmetric and probably has little to recommend it.  I will assume 
complete pooling is inappropriate.  Nevertheless, I want to analyze this 
data using lmer/lme or Winbugs.  I believe this study design imposes 
certain conditional exchangeability constraints.  For example, given 
study 1 it would seem to follow that trt2 and trt3 are exchangeable.  
For this reason it would seem the model formulation below is reasonable:

trt_i ~ N(s1, sigma^2); i = 2,3
trt_i ~ N(s2, sigma^2); i = 4,5
trt_i ~ N(s3, sigma^2); i = 6,7

Above s1, s2, and s3 denote random effects associated with treatment 
pairs which may be considered exchangeable.

In addition, we have:
trt_1 ~ N(s0, sigma0^2)
where s0 = 1/3*(s1 + s2 + s3)

If I went ahead and imposed vague priors on the appropriate parameters 
above, I believe I could set up a WINBUGS program to obtain posterior 
estimates for each trt_i; i = 1, 2, ..., 7.  My question: is it possible 
to set up and solve this problem using lmer/lme?  If so how would I do it?

Thank you for any assistance on this question.

Regards,
Paul


From g.leckie at bristol.ac.uk  Mon Aug  6 19:36:18 2012
From: g.leckie at bristol.ac.uk (George Leckie)
Date: Mon, 6 Aug 2012 18:36:18 +0100
Subject: [R-sig-ME]  code for multiple membership models?
Message-ID: <CAJXn_HwGzC4zhrJ7hnYF0dWvnv5QQsnAM_jW_R5NiQ3Wq1f_iw@mail.gmail.com>

Dear Doug,

I found your post on fitting multiple membership models using lme4a very helpful

    https://stat.ethz.ch/pipermail/r-sig-mixed-models/2011q2/006318.html

and I managed to get this approach on my own data.

I am now trying to follow the same approach on the new version of lme4
as this is now meant to have superceded lme4a

    http://lme4.r-forge.r-project.org/

However, the approach no longer appears to work as the the noFit
option appears not to be supported. It also no longer seems possible
to edit the Zt matrices and so on.

Might you be able to update your previous example to show us how to
fit multiple membership models using the new lme4?

I see that there have also been some recent posts about difficulties
in manually editing Zt in other contexts, so perhaps this is general
problem for lme4 which lme4a could deal with?

    https://stat.ethz.ch/pipermail/r-sig-mixed-models/2012q2/018118.html

Best wishes

George


From emmanuel.curis at parisdescartes.fr  Tue Aug  7 06:55:31 2012
From: emmanuel.curis at parisdescartes.fr (Emmanuel Curis)
Date: Tue, 7 Aug 2012 06:55:31 +0200
Subject: [R-sig-ME] convergence slot in glmer?
In-Reply-To: <501FFE6C.40101@gmail.com>
References: <501FFE6C.40101@gmail.com>
Message-ID: <20120807045531.GA30079@info124.pharmacie.univ-paris5.fr>

Hi,

When I had similar problems with nls and non-converging models for
some steps in the loop, the only way I found was to use the tryCatch
function.

It was for errors, not warnings, but according to its help page it
seems possible to catch also warnings with this function. So it should
be possible to use it with lme also in your case...

Hope this helps,

Best regards,

Emmanuel Curis

On Mon, Aug 06, 2012 at 01:27:08PM -0400, Alexandre Martin wrote:
> Dear mixed modelers,
>
> I am looking for a way to identify false convergence in models running in 
> loops.
> Warnings are : "In mer_finalize(ans) : false convergence (8)" but are not 
> stored in any slot, which makes impossible their assignation to particular 
> models in loops.
>
> Did anybody already face this problem? Any advice will be appreciated.
>
> Thank you very much.
>
> Alexandre

-- 
                                Emmanuel CURIS
                                emmanuel.curis at univ-paris5.fr

Page WWW: http://emmanuel.curis.online.fr/index.html


From j.hadfield at ed.ac.uk  Tue Aug  7 10:20:07 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Tue, 07 Aug 2012 09:20:07 +0100
Subject: [R-sig-ME] code for multiple membership models?
In-Reply-To: <CAJXn_HwGzC4zhrJ7hnYF0dWvnv5QQsnAM_jW_R5NiQ3Wq1f_iw@mail.gmail.com>
References: <CAJXn_HwGzC4zhrJ7hnYF0dWvnv5QQsnAM_jW_R5NiQ3Wq1f_iw@mail.gmail.com>
Message-ID: <20120807092007.17616pxiwngw6l0c@www.staffmail.ed.ac.uk>

Hi,

This is mainly a reply to Malcolm's earlier email which I had missed  
(I do field work from April-July and don't usually read emails).

To fit the MLWin multimembership model in MCMCglmm:

library(foreign); lips <-  
read.dta("http://www.bristol.ac.uk/cmm/media/runmlwin/lips1.dta")[,c(1,3,5,9:30)]

prior=list(R=list(V=1, nu=0), G=list(G1=list(V=1, nu=1, alpha.mu=0,  
alpha.V=1000)))

m1<-MCMCglmm(obs~perc_aff,  
random=~idv(~neigh1:weight1+neigh2:weight2+neigh3:weight3+neigh4:weight4+neigh5:weight5+neigh6:weight6+neigh7:weight7+neigh8:weight8+neigh9:weight9+neigh10:weight10+neigh11:weight11), data=lips, family="poisson",  
prior=prior)


Unfortunately the book is no longer on their server so I can't compare  
the results. However, I find little evidence for area effects once  
observation level overdispersion is accounted for (default in  
MCMCglmm, but perhaps not fitted in the original analyses).

The next version of MCMCglmm will have more efficient ways of setting  
up multimembership models, and also related models which I don't know  
the name for. Perhaps someone does? For example, imagine you want to  
fit mother and grandmother as random effects for some trait measured  
in offspring. The usual model would be:

random=~mother+gmother

However, if some mothers appear as grandmothers the covariance between  
their effects is estimable and perhaps of interest.  The next version  
will make this possible as random=~str(~mother, ~gmother).

Cheers,

Jarrod


Quoting George Leckie <g.leckie at bristol.ac.uk> on Mon, 6 Aug 2012  
18:36:18 +0100:

> Dear Doug,
>
> I found your post on fitting multiple membership models using lme4a  
> very helpful
>
>     https://stat.ethz.ch/pipermail/r-sig-mixed-models/2011q2/006318.html
>
> and I managed to get this approach on my own data.
>
> I am now trying to follow the same approach on the new version of lme4
> as this is now meant to have superceded lme4a
>
>     http://lme4.r-forge.r-project.org/
>
> However, the approach no longer appears to work as the the noFit
> option appears not to be supported. It also no longer seems possible
> to edit the Zt matrices and so on.
>
> Might you be able to update your previous example to show us how to
> fit multiple membership models using the new lme4?
>
> I see that there have also been some recent posts about difficulties
> in manually editing Zt in other contexts, so perhaps this is general
> problem for lme4 which lme4a could deal with?
>
>     https://stat.ethz.ch/pipermail/r-sig-mixed-models/2012q2/018118.html
>
> Best wishes
>
> George
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From nadine.klauke at biologie.uni-freiburg.de  Tue Aug  7 21:32:10 2012
From: nadine.klauke at biologie.uni-freiburg.de (Nadine Klauke)
Date: Tue, 07 Aug 2012 21:32:10 +0200
Subject: [R-sig-ME] pvals.fnc + lmer, discrepancy between pvalues
Message-ID: <web-186888828@uni-freiburg.de>

Dear R list members,

I?m trying to fit a mixed model with log transformed count
data in R (version 13.0). Here is the model specification:

m1<-lmer(Surv~H*E+HER+G+(1|ID),data=rep1)

The variables are:
Surv ##log-transformed count data
E ## 2-level categorical 
H ##count data
HER ##continous between 0 and 1
G ##count data
random effect: individual ID (individuals repeatedly
measured in differnet years)

The response variable is log-transformed because of
underdispersion with poisson-distribution.
When I inspected the p-values of the model given by
pvals.fnc() I realized that the pMCMC values are completely
different from the pvalues based on a t-distribution (R
output see below). I am aware that
pMCMC values are more reliable for mixed models.
Nevertheless, as far as I get it form the help lists etc,
the values usually do not differ so much. Or am I wrong?
When I calculate pvalues through logliklihood estimations
with anova() pvalues look more similar to those of the
t-distribution. Furthermore, the density plots of the fixed
effects given by pvals.fnc
look normally distributed. Might the different pvalues be
due to small sample size? Should I rather rely on the
logliklihood estimation than on pMCMC? Any advices would be
appreciated.

Thanks a lot.

Nadine


Results given by R:

m1<-lmer(Surv~H*E+HER+G+(1|ID),data=rep1)
summary(m1)
pvals.fnc(m1)

Linear mixed model fit by maximum likelihood 
Formula: Surv ~ H * E + HER + G + (1 | ID) 
Data: rep1 
AIC   BIC logLik deviance REMLdev
9.428 19.49  3.286   -6.572   17.59
Random effects:
  Groups   Name        Variance Std.Dev.
ID       (Intercept) 0.090871 0.301447
Residual             0.003334 0.057741
Number of obs: 26, groups: ID, 19

Fixed effects:
  Estimate Std. Error t value
(Intercept)   -0.87556    0.47112  -1.858
H              0.04135    0.01389   2.976
Eun           -0.52552    0.11116  -4.728
HER            3.45757    0.90229   3.832
G             -0.04023    0.03136  -1.283
H:Eun          0.22855    0.03812   5.996

Correlation of Fixed Effects:
             (Intr) H      Enrfhr HER    Gelege
H            0.467                            
Eun          0.364  0.337                     
HER         -0.950 -0.485 -0.573              
G            0.562  0.222  0.777 -0.767       
H:Eun       -0.550 -0.361 -0.864  0.712 -0.815

pvals.fnc(m1)
$fixed
               Estimate MCMCmean HPD95lower HPD95upper
 pMCMC Pr(>|t|)
(Intercept)    -0.8756  -0.6317    -2.1442     0.9813
0.4034   0.0779
H               0.0414   0.0366    -0.0518     0.1262
0.3992   0.0075
Eunerfahren    -0.5255  -0.1302    -0.5614     0.3020
0.5394   0.0001
HER             3.4576   1.6306    -0.7388     4.0931
0.1766   0.0010
Gelege         -0.0402   0.1261     0.0005     0.2523
0.0516   0.2143
H:Eunerfahren   0.2286   0.0506    -0.1330     0.2172
0.5524   0.0000

$random
Groups        Name Std.Dev. MCMCmedian MCMCmean HPD95lower
HPD95upper
1       ID (Intercept)   0.3014     0.0320   0.0464
    0.0000     0.1422
2 Residual               0.0577     0.3015   0.3076
    0.2124     0.4149

m1<-lmer(Surv~H*E+HER+G+(1|ID),REML=F,data=rep1)
m2<-lmer(Surv~H+E+HER+G+(1|ID),REML=F,data=rep1)
anova(m2,m1)

#Models:
#m2: Surv ~ H + E + HER + G + (1 | ID)
#m1: Surv ~ H * E + HER + G + (1 | ID)
#Df     AIC    BIC  logLik  Chisq Chi Df Pr(>Chisq)   
#m2  7 17.8965 26.703 -1.9482                            
#m1  8  9.4276 19.492  3.2862 10.469      1   0.001214 **
 # ---
 # Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1
? ? 1 


m1<-lmer(Surv~H*E+HER+G+(1|ID),REML=F,data=rep1)
m3<-lmer(Surv~H*E+G+(1|ID),REML=F,data=rep1)
anova(m3,m1)
#Models:
#m3: Surv ~ H * E + Gelege + (1 | ID)
#m1: Surv ~ H * E + HER + Gelege + (1 | ID)
#Df     AIC    BIC  logLik  Chisq Chi Df Pr(>Chisq)   
#m3  7 16.2181 25.025 -1.1090                            
#m1  8  9.4276 19.492  3.2862 8.7905      1   0.003028 **
  ---
#  Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1
? ? 1 

m1<-lmer(Surv~H*E+HER+G+(1|ID),REML=F,data=rep1)
m4<-lmer(Surv~H*E+HER+(1|ID),REML=F,data=rep1)
anova(m4,m1)
#m4: Surv ~ H * E + HER + (1 | ID)
#m1: Surv ~ H * E + HER + G + (1 | ID)
#Df    AIC    BIC logLik  Chisq Chi Df Pr(>Chisq)
#m4  7 8.3834 17.190 2.8083                         
#m1  8 9.4276 19.492 3.2862 0.9559      1     0.3282


From andrewdigby at mac.com  Wed Aug  8 00:21:45 2012
From: andrewdigby at mac.com (Andrew Digby)
Date: Wed, 08 Aug 2012 10:21:45 +1200
Subject: [R-sig-ME] Is there an R function for GLMM with binary response,
 nested random factors, and temporal correlation?
Message-ID: <B9623EB7-8367-4473-BA6C-97A328E18AC4@mac.com>


Despite lots of investigation, I haven't found any R packages might be suitable for the following problem. I'd be very grateful for suggestions. 

I have three-way nested data, with a series of measures (obs) taken in quick succession (equal time spacing) from each subject on different days. The measures taken on the same day are temporally correlated, so I'd like to use an AR1 correlation structure for those, but treat subjects and days as nested random factors (random intercept) since there is little temporal correlation between days. The response is binary.    

So I need a GLMM with a correlation structure. I've tried using GEE, but the R packages can't cope with multilevel nested data. The only R function I've found that can do this is glmmPQL. 

m <- glmmPQL(y ~ f1 * f2 * f3 + (1|subj/day), correlation=corAR1(form =~obsno|subj/day))

f1 - f3 are fixed factors

However, PQL estimation is not recommended for binary response data. With no AIC and unreliable p values, model selection seems impossible! So my question is:

1) are there any other functions which are suitable for a GLMM with multilevel nested random effects and a AR1 correlation structure? Or is MCMC the only option?
2) to make things more complicated, I'd also like to include a varFunc variance structure to cope with heterogeneity. Is this possible in ML methods in R? I'd also like to extend to a multinomial response at a later stage. 

GEE seems the best bet, but I come unstuck with the three-way nested factors. 

Thanks for your help.

Note: I originally posted this on R-help, but it was suggested that this list might be more appropriate.


From bates at stat.wisc.edu  Wed Aug  8 17:19:31 2012
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 8 Aug 2012 10:19:31 -0500
Subject: [R-sig-ME] Is there an R function for GLMM with binary response,
 nested random factors, and temporal correlation?
In-Reply-To: <B9623EB7-8367-4473-BA6C-97A328E18AC4@mac.com>
References: <B9623EB7-8367-4473-BA6C-97A328E18AC4@mac.com>
Message-ID: <CAO7JsnQMAObfNtYsLwM9ZG3r+yTGKdpJUoRgzoJ8qkuLMqvz=g@mail.gmail.com>

On Tue, Aug 7, 2012 at 5:21 PM, Andrew Digby <andrewdigby at mac.com> wrote:
>
> Despite lots of investigation, I haven't found any R packages might be suitable for the following problem. I'd be very grateful for suggestions.
>
> I have three-way nested data, with a series of measures (obs) taken in quick succession (equal time spacing) from each subject on different days. The measures taken on the same day are temporally correlated, so I'd like to use an AR1 correlation structure for those, but treat subjects and days as nested random factors (random intercept) since there is little temporal correlation between days. The response is binary.
>
> So I need a GLMM with a correlation structure. I've tried using GEE, but the R packages can't cope with multilevel nested data. The only R function I've found that can do this is glmmPQL.

Before you look for an R function, you should first check whether
there is indeed a statistical model with the properties that you
mention.  In the standard definition of a generalized linear mixed
model, and the only one that makes sense to me, the conditional
distribution of the response given the random effects has independent
components.

Aspects of linear mixed models that depend on being able to model the
variance-covariance of the response separately from the mean don't
carry over to generalized linear mixed models.  One of the fundamental
properties of GLMs and GLMMs is that the variance does depend on the
mean.

> m <- glmmPQL(y ~ f1 * f2 * f3 + (1|subj/day), correlation=corAR1(form =~obsno|subj/day))
>
> f1 - f3 are fixed factors
>
> However, PQL estimation is not recommended for binary response data. With no AIC and unreliable p values, model selection seems impossible! So my question is:
>
> 1) are there any other functions which are suitable for a GLMM with multilevel nested random effects and a AR1 correlation structure? Or is MCMC the only option?
> 2) to make things more complicated, I'd also like to include a varFunc variance structure to cope with heterogeneity. Is this possible in ML methods in R? I'd also like to extend to a multinomial response at a later stage.
>
> GEE seems the best bet, but I come unstuck with the three-way nested factors.
>
> Thanks for your help.
>
> Note: I originally posted this on R-help, but it was suggested that this list might be more appropriate.
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From Ulrike.Froemmel at HS-Lausitz.de  Wed Aug  8 21:12:56 2012
From: Ulrike.Froemmel at HS-Lausitz.de (=?iso-8859-1?Q?Fr=F6mmel=2C_Ulrike?=)
Date: Wed, 8 Aug 2012 21:12:56 +0200
Subject: [R-sig-ME] How to handle measurement uncertainities with lmer
In-Reply-To: <20120808185545.9dc18f4c.hm@zotac.lan>
References: <20120808185545.9dc18f4c.hm@zotac.lan>
Message-ID: <BF207FEE50E72F47B1E4CF01863783A803BF28A3381A@EGON11.HS-Lausitz.int>

Dear list members,


a result set obtained from an automated video microscope contains
adhesion density values of a particular bacterial isolate per cell
surface area (germs/mm^2). Because the device determines this density
several times, the result set also contains an associated measurement
uncertainity, given as coefficient of variation Cv = sd/? (standard deviation / mean).

In addition, each isolate was determined repeatedly on the same plate,
and also on different plates.

Independently from these experiments, the presence or absence of around 40
genes had been determined for each of the 316 bacterial isolates.

To investigate how much a particular gene influences bacterial adhesion,
the following lmer model was used:


   lmer(log(`Germs/mm^2`+1) ~ -1 + GeneA + GeneB + .... GeneZ + (1|Isolate) + (1|Plate), data=df)


> str(df):

'data.frame':   4101 obs. of  67 variables:
 $ Adhesion    : num  5.67 5.55 5.33 6.85 6.94 ...
 $ Isolate     : Factor w/ 316 levels "2164","2166",..: 183 183 183 12 12 107 107 107 246 76 ...
 $ Plate       : Factor w/ 59 levels "5637 BP01 m1",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ Germs/mm^2  : num  288 256 206 946 1031 ...
 $ Germs_cv    : num  48.49 5.51 4.56 9.07 6.78 ...
 $ GeneA       : Factor w/ 2 levels "-","+": 1 1 1 1 1 1 1 1 1 1 ...
 $ GeneB       : Factor w/ 2 levels "-","+": 1 1 1 1 1 1 1 1 1 1 ...
 $ GeneC       : Factor w/ 2 levels "-","+": 2 2 2 1 1 1 1 1 1 1 ...
 [...] <- ~ 40 Genes plus some other data)


The log transformation was applied because the residual vs. fitted diagram
strongly indicated heteroscedasticity. With the log transformation applied,
the density values are approximately normal distributed.


My questions are:

 1/ How to feed the uncertainity measures into lmer?

 2/ What would an effect of zero actually mean, considering that the
    data had to be log transformed in order to cope with
    heteroscedascity?

 3/ How can I do the inverse transform of the effects?



Best regards


From matz at mail.utexas.edu  Thu Aug  9 15:57:04 2012
From: matz at mail.utexas.edu (mikhail matz)
Date: Thu, 9 Aug 2012 08:57:04 -0500
Subject: [R-sig-ME] multiple comparisons with MCMCglmm?
Message-ID: <1493ADA9-7BD7-4355-BA1D-2F0C4783892A@mail.utexas.edu>

Hello,

Is correction for multiple comparisons ever necessary with MCMCglmm, or with any other method deriving the credible intervals from MCMC sampling?

My gut feeling is no, since we are deriving credible intervals for all the effects (or their combinations, if anyone is interested in pairwise comparisons a'la Tukey) jointly from the results of the same MCMC chain. I am, however, not sure at all whether this logic makes any sense.

The only [hopeful] bit that I managed to find on the web is the paper by Gelman et al 2012 ( http://www.stat.columbia.edu/~gelman/research/published/multiple2f.pdf ), but it is really more about multilevel (i.e., mixed) models rather than MCMC-based methods.

I would greatly appreciate any advice.

cheers

Mikhail Matz, University of Texas at Austin

From egmartins at gmail.com  Thu Aug  9 17:41:57 2012
From: egmartins at gmail.com (Eduardo Martins)
Date: Thu, 9 Aug 2012 08:41:57 -0700
Subject: [R-sig-ME] Is there an R function for GLMM with binary
Message-ID: <BA737D57-2B65-491B-AA5B-591162D0FC42@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120809/be779ef8/attachment.pl>

From bates at stat.wisc.edu  Thu Aug  9 18:21:35 2012
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 9 Aug 2012 11:21:35 -0500
Subject: [R-sig-ME] Is there an R function for GLMM with binary response,
 nested random factors, and temporal correlation?
In-Reply-To: <2AD4A3F2-5107-41BF-9775-93D01F67B144@mac.com>
References: <B9623EB7-8367-4473-BA6C-97A328E18AC4@mac.com>
	<CAO7JsnQMAObfNtYsLwM9ZG3r+yTGKdpJUoRgzoJ8qkuLMqvz=g@mail.gmail.com>
	<2AD4A3F2-5107-41BF-9775-93D01F67B144@mac.com>
Message-ID: <CAO7JsnTXcAGnqqitKvXE=hF9rk5in2FHzZiPTyHd6vY0HifQ8g@mail.gmail.com>

On Wed, Aug 8, 2012 at 7:49 PM, Andrew Digby <andrewdigby at mac.com> wrote:
>
> Thank you for your advice. Can I interpret it  to mean that it's not correct to specify a correlation structure (beyond that induced through random effects) in generalised models? I had thought that although prone to some issues, this was possible; see e.g. Ben Bolker's and Alain Zuur's reponses here: https://stat.ethz.ch/pipermail/r-sig-mixed-models/2012q1/014937.html. In my case, I think I do have 'small-scale correlation', with repeated measures of a subject taken in rapid succession on a particular day. My residuals don't show gross heterogeneity, so I may not need to alter the variance structure to allow for that.

Perhaps Ben or Alain can provide more information on models that
incorporate correlation structure in binary responses.  There may be
definitions of these using copulas or something like that but I don't
know enough about that area to be able to provide advice.

> I'd be grateful for general advice on whether a GLM with temporal correlation structure is an adequate method in this case. Or should I tackle MCMC instead? (I have Zuur et al's excellent books.) However, this is a small part of my overall analysis, so I'd prefer to keep things as simple as possible!

One way to achieve a temporal correlation structure is to have a
random effect for slope with respect to time for each subject.

> On 9/08/2012, at 03:19 , Douglas Bates <bates at stat.wisc.edu> wrote:
>
>> On Tue, Aug 7, 2012 at 5:21 PM, Andrew Digby <andrewdigby at mac.com> wrote:
>>>
>>> Despite lots of investigation, I haven't found any R packages might be suitable for the following problem. I'd be very grateful for suggestions.
>>>
>>> I have three-way nested data, with a series of measures (obs) taken in quick succession (equal time spacing) from each subject on different days. The measures taken on the same day are temporally correlated, so I'd like to use an AR1 correlation structure for those, but treat subjects and days as nested random factors (random intercept) since there is little temporal correlation between days. The response is binary.
>>>
>>> So I need a GLMM with a correlation structure. I've tried using GEE, but the R packages can't cope with multilevel nested data. The only R function I've found that can do this is glmmPQL.
>>
>> Before you look for an R function, you should first check whether
>> there is indeed a statistical model with the properties that you
>> mention.  In the standard definition of a generalized linear mixed
>> model, and the only one that makes sense to me, the conditional
>> distribution of the response given the random effects has independent
>> components.
>>
>> Aspects of linear mixed models that depend on being able to model the
>> variance-covariance of the response separately from the mean don't
>> carry over to generalized linear mixed models.  One of the fundamental
>> properties of GLMs and GLMMs is that the variance does depend on the
>> mean.
>>
>>> m <- glmmPQL(y ~ f1 * f2 * f3 + (1|subj/day), correlation=corAR1(form =~obsno|subj/day))
>>>
>>> f1 - f3 are fixed factors
>>>
>>> However, PQL estimation is not recommended for binary response data. With no AIC and unreliable p values, model selection seems impossible! So my question is:
>>>
>>> 1) are there any other functions which are suitable for a GLMM with multilevel nested random effects and a AR1 correlation structure? Or is MCMC the only option?
>>> 2) to make things more complicated, I'd also like to include a varFunc variance structure to cope with heterogeneity. Is this possible in ML methods in R? I'd also like to extend to a multinomial response at a later stage.
>>>
>>> GEE seems the best bet, but I come unstuck with the three-way nested factors.
>>>
>>> Thanks for your help.
>>>
>>> Note: I originally posted this on R-help, but it was suggested that this list might be more appropriate.
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From tonyahaff at gmail.com  Thu Aug  9 18:57:50 2012
From: tonyahaff at gmail.com (Tonya Haff)
Date: Thu, 9 Aug 2012 12:57:50 -0400
Subject: [R-sig-ME] Response variable with zero variance
Message-ID: <CACx4S4OwEBopxPbgv-nb_uma=H=ohjn1YdJ+47YmcXsPF8knVw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120809/23c4f126/attachment.pl>

From khallg at unm.edu  Thu Aug  9 19:20:18 2012
From: khallg at unm.edu (Kevin Hallgren)
Date: Thu, 9 Aug 2012 11:20:18 -0600
Subject: [R-sig-ME] Testing for significant intercept-slope correlation in
	nlme
Message-ID: <CAK7f5ZqRHP9L6k6ksOkWe6Rm-G+Z7gn=ix4CVVw87ajqL8XOEw@mail.gmail.com>

Hi R-SIG,

I'm working on a study that is predicting daily urges to drink alcohol
based on daily marital satisfaction.  My model has daily observations
nested within participants, and includes random intercepts (i.e.,
individuals may vary in how many urges they experience) and random
slopes for marital satisfaction (i.e., individuals may vary in how
much their marital satisfaction predicts their urges to drink).

I want to test whether the random intercept-slope correlation is
statistically significant using a chi-square test with nested model
comparison, but I'm having trouble specifying the random effects to do
this.

I can create a model with only random slopes and compare that against
a model with random intercepts, random slopes, and intercept-slope
correlation, but doing a nested model comparison combines the
significance test of the random slope effect and the slope-intercept
correlation into one test of significance.  Ideally, I would like to
test the significance of the slope variance and the intercept-slope
correlation separately.

Using nlme (which I selected over lme4 because it allows for temporal
autocorrelation effects), I can specify

random=~1|IDNUM,  #Random intercepts
random=~1 + PREV_URGE_CTRD|IDNUM, #Random intercepts, slopes, and
intercept-slope correlation

But I cannot figure out how to specify random intercepts and slopes
but NO intercept-slope correlation, e.g.,
random= ~(1|IDNUM) + (0 + PREV_URGE_CTRD|IDNUM),  #produces an error message

1.  Does anyone know how to specify random intercepts and random
slopes but suppress the intercept-slope correlation using nlme?  I'd
like to stick with the nlme package if possible.
2.  If that is not possible, does anyone know of a good way (or
references) to test the significance of the slopes and the
significance of the intercept-slope correlation when a nested model
comparison changes both of those random effects simultaneously?

An example of the full model (which includes other fixed effects not
described above) is here:
m1 = lme(NEXT_MAR ~ PREV_MAR + SESSNUM + TXCOND + TXCOND*SESSNUM +
PREV_URGE_CTRD + PREV_URGE_PERSON_MEAN + PREV_URGE_CTRD*TXCOND +
TXCOND*PREV_URGE_CTRD*SESSNUM+ PREV_URGE_PERSON_MEAN*TXCOND +
PREV_URGE_PERSON_MEAN*SESSNUM + TXCOND*PREV_URGE_PERSON_MEAN*SESSNUM,
	random=~1|IDNUM,
	data=d,
	na.action=na.omit,
	correlation=corAR1(0, form = ~SESSION_DAY|IDNUM),
	method="ML")


Thanks!
Kevin


From andrewdigby at mac.com  Thu Aug  9 02:49:14 2012
From: andrewdigby at mac.com (Andrew Digby)
Date: Thu, 09 Aug 2012 12:49:14 +1200
Subject: [R-sig-ME] Is there an R function for GLMM with binary response,
 nested random factors, and temporal correlation?
In-Reply-To: <CAO7JsnQMAObfNtYsLwM9ZG3r+yTGKdpJUoRgzoJ8qkuLMqvz=g@mail.gmail.com>
References: <B9623EB7-8367-4473-BA6C-97A328E18AC4@mac.com>
	<CAO7JsnQMAObfNtYsLwM9ZG3r+yTGKdpJUoRgzoJ8qkuLMqvz=g@mail.gmail.com>
Message-ID: <2AD4A3F2-5107-41BF-9775-93D01F67B144@mac.com>


Thank you for your advice. Can I interpret it  to mean that it's not correct to specify a correlation structure (beyond that induced through random effects) in generalised models? I had thought that although prone to some issues, this was possible; see e.g. Ben Bolker's and Alain Zuur's reponses here: https://stat.ethz.ch/pipermail/r-sig-mixed-models/2012q1/014937.html. In my case, I think I do have 'small-scale correlation', with repeated measures of a subject taken in rapid succession on a particular day. My residuals don't show gross heterogeneity, so I may not need to alter the variance structure to allow for that.

I'd be grateful for general advice on whether a GLM with temporal correlation structure is an adequate method in this case. Or should I tackle MCMC instead? (I have Zuur et al's excellent books.) However, this is a small part of my overall analysis, so I'd prefer to keep things as simple as possible!

Many thanks for your help,

Andrew

On 9/08/2012, at 03:19 , Douglas Bates <bates at stat.wisc.edu> wrote:

> On Tue, Aug 7, 2012 at 5:21 PM, Andrew Digby <andrewdigby at mac.com> wrote:
>> 
>> Despite lots of investigation, I haven't found any R packages might be suitable for the following problem. I'd be very grateful for suggestions.
>> 
>> I have three-way nested data, with a series of measures (obs) taken in quick succession (equal time spacing) from each subject on different days. The measures taken on the same day are temporally correlated, so I'd like to use an AR1 correlation structure for those, but treat subjects and days as nested random factors (random intercept) since there is little temporal correlation between days. The response is binary.
>> 
>> So I need a GLMM with a correlation structure. I've tried using GEE, but the R packages can't cope with multilevel nested data. The only R function I've found that can do this is glmmPQL.
> 
> Before you look for an R function, you should first check whether
> there is indeed a statistical model with the properties that you
> mention.  In the standard definition of a generalized linear mixed
> model, and the only one that makes sense to me, the conditional
> distribution of the response given the random effects has independent
> components.
> 
> Aspects of linear mixed models that depend on being able to model the
> variance-covariance of the response separately from the mean don't
> carry over to generalized linear mixed models.  One of the fundamental
> properties of GLMs and GLMMs is that the variance does depend on the
> mean.
> 
>> m <- glmmPQL(y ~ f1 * f2 * f3 + (1|subj/day), correlation=corAR1(form =~obsno|subj/day))
>> 
>> f1 - f3 are fixed factors
>> 
>> However, PQL estimation is not recommended for binary response data. With no AIC and unreliable p values, model selection seems impossible! So my question is:
>> 
>> 1) are there any other functions which are suitable for a GLMM with multilevel nested random effects and a AR1 correlation structure? Or is MCMC the only option?
>> 2) to make things more complicated, I'd also like to include a varFunc variance structure to cope with heterogeneity. Is this possible in ML methods in R? I'd also like to extend to a multinomial response at a later stage.
>> 
>> GEE seems the best bet, but I come unstuck with the three-way nested factors.
>> 
>> Thanks for your help.
>> 
>> Note: I originally posted this on R-help, but it was suggested that this list might be more appropriate.
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From highstat at highstat.com  Thu Aug  9 22:42:18 2012
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Thu, 09 Aug 2012 21:42:18 +0100
Subject: [R-sig-ME] Is there an R function for GLMM with binary
Message-ID: <502420AA.5050608@highstat.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120809/bf7e03e8/attachment.pl>

From highstat at highstat.com  Thu Aug  9 22:47:14 2012
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Thu, 09 Aug 2012 21:47:14 +0100
Subject: [R-sig-ME] Is there an R function for GLMM with binary
Message-ID: <502421D2.6080809@highstat.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120809/a4466b1b/attachment.pl>

From henrik.singmann at psychologie.uni-freiburg.de  Fri Aug 10 01:21:15 2012
From: henrik.singmann at psychologie.uni-freiburg.de (Henrik Singmann)
Date: Fri, 10 Aug 2012 01:21:15 +0200
Subject: [R-sig-ME] announcing package afex: obtain p-values for all fixed
 effects in a lmer() model via KRmodcomp
Message-ID: <k01gla$gel$1@dough.gmane.org>

Dear list members,

I am happy to announce that my new package "afex" has been accepted on 
CRAN that may be of interest to some.
In particular, it contains the function mixed() that calculates p-values 
for all fixed effects (terms) in a mixed model using the Kenward-Rogers 
approximation for degrees of freedom implemented in pbkrtest::KRmodcomp 
(after fitting all the necessary models for this using lme4::lmer).

In the current version (0.2-26, the last number denotes the r-forge 
revision it is based on) mixed() only allows to obtain type 3 tests, but 
future versions will allow type 2 tests (chances are that within August 
the type 2 tests will be implemented in the development version on 
R-Forge). The type 3 tests are based on comparing a model in which only 
the term of interest is eliminated with the full model. This 
functionality is based on a helpful discussion with Ben Bolker and 
Joshua Wiley on http://stackoverflow.com/q/11335923/289572.

Furthermore, the random-effects structure is identical for all models. 
This design decision seems debatable to me and I am happy for any input. 
An alternative would be to delete the term of interest also from the 
random-effect structure (and not only from the fixed-effects structure) 
when fitting the contrasting model. However, I am neither sure if this 
is possible with KRmodcomp nor if it is a sensible thing to do.

Running mixed() may be a lengthy process as it fits the full model and 
an additional model for each term in the model (including the intercept) 
with the full random structure using lmer(). Furthermore, obtaining the 
p-values via KRmodcomp is also a time consuming process.

Note that loading package afex sets the default contrasts to contr.sum. 
For a brief description of afex see below or 
http://cran.r-project.org/web/packages/afex/index.html.

I hope mixed() may be helpful and am interested in any discussion, 
questions, ideas, bugs, or contributions.

Best,
Henrik

afex ? Analysis of Factorial Experiments
afex was developed for the analysis of factorial experiments and 
provides convenience functions for type 3 (the default) and type 2 tests 
for ANOVA and ANCOVA (using car::Anova) and mixed models (using lmer). 
It is set up in a way (type 3 tests as defaults and setting contrasts to 
contr.sum) to replicate results from commercial software packages with 
the hope to ease the transition from such software to R. Additionally, 
afex has a formula interface for car::Anova imitating the functionality 
of aov (but works with unbalanced designs, see ?aov.car). Data for afex 
needs to be in the long format (i.e., one observation per row).  afex 
has a homepage (that currently only focuses on the ANOVA and ANCOVA 
functionality): 
http://www.psychologie.uni-freiburg.de/Members/singmann/R/afex
The design of afex was strongly influenced by Mike Lawrence? ez package.

-- 
Dipl. Psych. Henrik Singmann
PhD Student
Albert-Ludwigs-Universit?t Freiburg, Germany
http://www.psychologie.uni-freiburg.de/Members/singmann


From john.maindonald at anu.edu.au  Fri Aug 10 02:18:16 2012
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Fri, 10 Aug 2012 10:18:16 +1000
Subject: [R-sig-ME] multiple comparisons with MCMCglmm?
In-Reply-To: <1493ADA9-7BD7-4355-BA1D-2F0C4783892A@mail.utexas.edu>
References: <1493ADA9-7BD7-4355-BA1D-2F0C4783892A@mail.utexas.edu>
Message-ID: <B9C66791-1E1F-4BEA-994E-663BA99DA22C@anu.edu.au>

If you are using the MCMC results to make comparisons in 
the same manner as for frequentist results, then the sampling 
properties are entirely comparable.  If you want to move from
comparison-wise 5% CIs to overall 5% CIs, then some kind of 
multiple range correction might in principle be used.  But this
usually not the way to go; MCMC gives you sampling statistics
from which you can fairly directly extract overall 5% CIs.  You 
can directly extract the sampling distribution for, e.g., the Tukey 
HSD, if that is what you want.

(If you are in a Bayesian mindset, you might think of the frequentist 
analysis as derived using an unstated Bayesian prior -- typically 
there is a prior that will give you the frequentist results.)

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.
http://www.maths.anu.edu.au/~johnm

On 09/08/2012, at 11:57 PM, mikhail matz <matz at mail.utexas.edu> wrote:

> Hello,
> 
> Is correction for multiple comparisons ever necessary with MCMCglmm, or with any other method deriving the credible intervals from MCMC sampling?
> 
> My gut feeling is no, since we are deriving credible intervals for all the effects (or their combinations, if anyone is interested in pairwise comparisons a'la Tukey) jointly from the results of the same MCMC chain. I am, however, not sure at all whether this logic makes any sense.
> 
> The only [hopeful] bit that I managed to find on the web is the paper by Gelman et al 2012 ( http://www.stat.columbia.edu/~gelman/research/published/multiple2f.pdf ), but it is really more about multilevel (i.e., mixed) models rather than MCMC-based methods.
> 
> I would greatly appreciate any advice.
> 
> cheers
> 
> Mikhail Matz, University of Texas at Austin
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From datkins at u.washington.edu  Fri Aug 10 09:27:43 2012
From: datkins at u.washington.edu (David Atkins)
Date: Fri, 10 Aug 2012 09:27:43 +0200
Subject: [R-sig-ME] Testing for significant intercept-slope correlation
 in nlme
In-Reply-To: <CAK7f5ZqRHP9L6k6ksOkWe6Rm-G+Z7gn=ix4CVVw87ajqL8XOEw@mail.gmail.com>
References: <CAK7f5ZqRHP9L6k6ksOkWe6Rm-G+Z7gn=ix4CVVw87ajqL8XOEw@mail.gmail.com>
Message-ID: <5024B7EF.3040000@u.washington.edu>


Hi Kevin--

I have not used lme() too much recently, but I think you would want to 
use the pdDiag() function for creating a diagonal random-effects matrix. 
  For instance, using the Orthodont datset:

library(nlme)

?lme
head(Orthodont)
fm1 <- lme(distance ~ age,
            data = Orthodont,
            random = ~ 1 | Subject)
fm1.1 <- lme(distance ~ age,
            data = Orthodont,
            random = ~ -1 + age | Subject)
fm1.2 <- lme(distance ~ age,
            data = Orthodont,
            random = list(Subject = pdDiag(~ age)))
fm1.3 <- lme(distance ~ age,
              data = Orthodont,
              random = ~ age | Subject)
anova(fm1, fm1.1, fm1.2, fm1.3)

summary(fm1.2, corr = FALSE)

So, fm1.2 fits intercept and random slope for age but no covariance 
(correlation) between the two.

[BTW, you might check whether the autocorrelation term seems to matter 
in any substantive way for your results.  Pretty much every time I have 
compared with and without such terms (using daily diary data), models 
with autocorrelation terms are hugely preferred by statistical indices, 
but the parameters that I care about are virtually identical. 
Oftentimes, these terms are "fighting" with the random-effects for 
explaining the correlation due to repeated-measures, but fixed-effects 
and their SE are largely similar.]

Hope that helps.

cheers, Dave

-- 
Dave Atkins, PhD
University of Washington
datkins at u.washington.edu
http://depts.washington.edu/cshrb/

August 1 - October 30:

Universitat Zurich
Psychologisches Institut
Klinische Psychologie
Binzmuhlestrasse 14/23
CH-8050 Zurich

+41 44 635 71 75
Hi R-SIG,

I'm working on a study that is predicting daily urges to drink alcohol
based on daily marital satisfaction.  My model has daily observations
nested within participants, and includes random intercepts (i.e.,
individuals may vary in how many urges they experience) and random
slopes for marital satisfaction (i.e., individuals may vary in how
much their marital satisfaction predicts their urges to drink).

I want to test whether the random intercept-slope correlation is
statistically significant using a chi-square test with nested model
comparison, but I'm having trouble specifying the random effects to do
this.

I can create a model with only random slopes and compare that against
a model with random intercepts, random slopes, and intercept-slope
correlation, but doing a nested model comparison combines the
significance test of the random slope effect and the slope-intercept
correlation into one test of significance.  Ideally, I would like to
test the significance of the slope variance and the intercept-slope
correlation separately.

Using nlme (which I selected over lme4 because it allows for temporal
autocorrelation effects), I can specify

random=~1|IDNUM,  #Random intercepts
random=~1 + PREV_URGE_CTRD|IDNUM, #Random intercepts, slopes, and
intercept-slope correlation

But I cannot figure out how to specify random intercepts and slopes
but NO intercept-slope correlation, e.g.,
random= ~(1|IDNUM) + (0 + PREV_URGE_CTRD|IDNUM),  #produces an error message

1.  Does anyone know how to specify random intercepts and random
slopes but suppress the intercept-slope correlation using nlme?  I'd
like to stick with the nlme package if possible.
2.  If that is not possible, does anyone know of a good way (or
references) to test the significance of the slopes and the
significance of the intercept-slope correlation when a nested model
comparison changes both of those random effects simultaneously?

An example of the full model (which includes other fixed effects not
described above) is here:
m1 = lme(NEXT_MAR ~ PREV_MAR + SESSNUM + TXCOND + TXCOND*SESSNUM +
PREV_URGE_CTRD + PREV_URGE_PERSON_MEAN + PREV_URGE_CTRD*TXCOND +
TXCOND*PREV_URGE_CTRD*SESSNUM+ PREV_URGE_PERSON_MEAN*TXCOND +
PREV_URGE_PERSON_MEAN*SESSNUM + TXCOND*PREV_URGE_PERSON_MEAN*SESSNUM,
	random=~1|IDNUM,
	data=d,
	na.action=na.omit,
	correlation=corAR1(0, form = ~SESSION_DAY|IDNUM),
	method="ML")


Thanks!
Kevin


From matthias.suter at art.admin.ch  Fri Aug 10 13:27:16 2012
From: matthias.suter at art.admin.ch (matthias.suter at art.admin.ch)
Date: Fri, 10 Aug 2012 13:27:16 +0200
Subject: [R-sig-ME] covariance matrix of fixed effects in lmer()
Message-ID: <A546928D770F23429B20371B4D3B10F7EA896EE275@EVD-C7000.bk.evdad.admin.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120810/9927f880/attachment.pl>

From Thierry.ONKELINX at inbo.be  Fri Aug 10 14:29:56 2012
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Fri, 10 Aug 2012 12:29:56 +0000
Subject: [R-sig-ME] covariance matrix of fixed effects in lmer()
In-Reply-To: <A546928D770F23429B20371B4D3B10F7EA896EE275@EVD-C7000.bk.evdad.admin.ch>
References: <A546928D770F23429B20371B4D3B10F7EA896EE275@EVD-C7000.bk.evdad.admin.ch>
Message-ID: <AA818EAD2576BC488B4F623941DA74275E396AE1@inbomail.inbo.be>

Dear Matthias,

vcov(model) for the fixed effects and VarCorr(model) for the random effects

Best regards

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey


-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens matthias.suter at art.admin.ch
Verzonden: vrijdag 10 augustus 2012 13:27
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] covariance matrix of fixed effects in lmer()

Dear list members

Is there a direct access to the covariance matrix of fixed effects in lmer(). For example, with ...


summary( lmer(y ~ x1 + x2 + x3  + (1 | Groupingfactor), data) )

... we receive the    "Correlation of Fixed Effects"


Is there a straight forward way to get the covariance matrix to which the correlation matrix refers?



The analogue in a lm() would be:

lmout <- lm(y ~ x1 + x2 + x3, data)

To get the covariance matrix, we have e.g.:

summary(lmout)$sigma^2 * summary(lmout)$cov.uns


With many thanks in advance,
Matthias

--------------------------------------------------------------------------------
Matthias Suter, Dr.
Forage and Grassland
Forschungsanstalt Agroscope Reckenholz-T?nikon ART Reckenholzstrasse 191, CH-8046 Z?rich

Tel. +41 44 377 75 90
Fax +41 44 377 72 01
matthias.suter at art.admin.ch
www.agroscope.ch
--------------------------------------------------------------------------------




        [[alternative HTML version deleted]]

* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From Thierry.ONKELINX at inbo.be  Fri Aug 10 14:44:36 2012
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Fri, 10 Aug 2012 12:44:36 +0000
Subject: [R-sig-ME] Response variable with zero variance
In-Reply-To: <CACx4S4OwEBopxPbgv-nb_uma=H=ohjn1YdJ+47YmcXsPF8knVw@mail.gmail.com>
References: <CACx4S4OwEBopxPbgv-nb_uma=H=ohjn1YdJ+47YmcXsPF8knVw@mail.gmail.com>
Message-ID: <AA818EAD2576BC488B4F623941DA74275E396C53@inbomail.inbo.be>

Dear Tonya,

Can you give us the output of the model summary?

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey


-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Tonya Haff
Verzonden: donderdag 9 augustus 2012 18:58
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] Response variable with zero variance

Hi all,

Ideally, I would like to run a GLMM on my data, which are are binomial (so I'm trying to run a model with a binomial distribution and a logit link).
 I have two fixed effects, fledgling age and playback type.  I have fledging ID as a random effect.  The problem I'm running into is zero variance in my response variable - ie fledglings always respond the same way to some of the playback types.  I can run a model and it converges, but it spits out an F value of over a million. So I'm wondering (1) is it possible to use a GLMM with a zero variance response; and (2) if so, which R package is the most appropriate?


Thanks for your help!

Cheers,

Tonya
--
Tonya Haff
PhD candidate
Evolution, Ecology and Genetics
Research School of Biology
Australian National University
Mobile:+61-4-3331-2908
Lab: +61-2-6125-5651

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From smilodon2000 at hotmail.com  Fri Aug 10 15:05:03 2012
From: smilodon2000 at hotmail.com (john benson)
Date: Fri, 10 Aug 2012 13:05:03 +0000
Subject: [R-sig-ME] correlations between fixed effects: groups or
 individuals for mixed models?
Message-ID: <SNT118-W589F3BC48FAF3F0DFF742DDCB30@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120810/fa982a42/attachment.pl>

From khallg at unm.edu  Fri Aug 10 16:47:16 2012
From: khallg at unm.edu (Kevin Hallgren)
Date: Fri, 10 Aug 2012 08:47:16 -0600
Subject: [R-sig-ME] Testing for significant intercept-slope correlation
	in nlme
Message-ID: <CAK7f5Zpf3McU8pXVdN8gN33aU-+wEJWher4arpmQ4bxc0us15g@mail.gmail.com>

Thanks Dave!  The syntax you gave worked great for testing each aspect
of the random effect.

I also tested the models without the autocorrelation component and a
few of the fixed effects estimates had quite different magnitudes than
they did when autocorrelation was included (e.g., intercept of 2.0
without autocorrelation vs. 1.5 with autocorrelation).  The direction
and significance of the effects didn't appear to change much though.
That makes me inclined to retain that component, with the hope that
it's doing more good than harm.

Thanks again
Kevin


From David.Duffy at qimr.edu.au  Sat Aug 11 00:28:34 2012
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Sat, 11 Aug 2012 08:28:34 +1000
Subject: [R-sig-ME] correlations between fixed effects: groups or
	individuals for mixed models?
In-Reply-To: <SNT118-W589F3BC48FAF3F0DFF742DDCB30@phx.gbl>
References: <SNT118-W589F3BC48FAF3F0DFF742DDCB30@phx.gbl>
Message-ID: <Pine.LNX.4.64.1208110816590.14579@orpheus.qimr.edu.au>

On Fri, 10 Aug 2012, john benson wrote:

> I've been using GAMMs and GLMMs to investigate genetic ancestry in 
> relation to environmental heterogeneity.  My data come from individuals 
> (n = 85) that live in social groups (n = 44) across my study area. 
> Thus, I have included social group as a random effect and investigated 
> the influence of several environmental fixed effects. My question: Given 
> that the random effect of social group should account for 
> non-independence between individuals within groups, it seems like I 
> should only be concerned with correlations between independent variables 
> across social groups (rather than across individuals).  Is it the 
> convention with mixed models such as mine to report a correlation matrix 
> between groups, or do folks generally report a correlation matrix 
> between individuals as one would normally do with a GLM or GAM?  I 
> suppose I could report both, but it seems like the correlations between 
> individuals wouldn't be relevant given the inclusion of the random 
> effect.

Two members per group? Or are they overlapping?  And ancestry is unrelated 
to social grouping?

-- 
| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v


From smilodon2000 at hotmail.com  Sat Aug 11 00:39:28 2012
From: smilodon2000 at hotmail.com (john benson)
Date: Fri, 10 Aug 2012 22:39:28 +0000
Subject: [R-sig-ME] correlations between fixed effects: groups or
 individuals for mixed models?
In-Reply-To: <Pine.LNX.4.64.1208110816590.14579@orpheus.qimr.edu.au>
References: <SNT118-W589F3BC48FAF3F0DFF742DDCB30@phx.gbl>,
	<Pine.LNX.4.64.1208110816590.14579@orpheus.qimr.edu.au>
Message-ID: <SNT118-W59C3F00962364D8C52D274DCB30@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120810/b948c20e/attachment.pl>

From tonyahaff at gmail.com  Sat Aug 11 15:36:02 2012
From: tonyahaff at gmail.com (Tonya Haff)
Date: Sat, 11 Aug 2012 09:36:02 -0400
Subject: [R-sig-ME] Response variable with zero variance
In-Reply-To: <AA818EAD2576BC488B4F623941DA74275E396C53@inbomail.inbo.be>
References: <CACx4S4OwEBopxPbgv-nb_uma=H=ohjn1YdJ+47YmcXsPF8knVw@mail.gmail.com>
	<AA818EAD2576BC488B4F623941DA74275E396C53@inbomail.inbo.be>
Message-ID: <CACx4S4ObXZoCOmmGOA90t5wC5W8ha10ZCc81ArKSeW1ttTHh-A@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120811/6be3a216/attachment.pl>

From bbolker at gmail.com  Sun Aug 12 03:31:27 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 12 Aug 2012 01:31:27 +0000 (UTC)
Subject: [R-sig-ME] Response variable with zero variance
References: <CACx4S4OwEBopxPbgv-nb_uma=H=ohjn1YdJ+47YmcXsPF8knVw@mail.gmail.com>
	<AA818EAD2576BC488B4F623941DA74275E396C53@inbomail.inbo.be>
	<CACx4S4ObXZoCOmmGOA90t5wC5W8ha10ZCc81ArKSeW1ttTHh-A@mail.gmail.com>
Message-ID: <loom.20120812T032624-442@post.gmane.org>

Tonya Haff <tonyahaff at ...> writes:

> I used the lmer package, using the following code
> 
> FL.glmm <- lmer(RESPONSE ~ fEXPN + fPBN + fEXPN*fPBN + (1|fNID), data =
> FL.1, family = binomial)

  By the way, the fixed effect part of the formula can also
be written as fEXPN*fPBN alone (since A*B is equivalent to A+B+A:B;
unlike in some other stats packages, * means "crossed" rather
than "interaction")

   It may be worth double-checking this with glmmPQL:

  library(MASS)
  glmmPQL(RESPONSE~fEXPN*fPBN,random=~1|fNID,data=FL.1,
    family="binomial")

although if the data are Bernoulli (0/1) then PQL is a little
bit questionable.

> 
> Here is the summary
> 
> Generalized linear mixed model fit by the Laplace approximation
> Formula: RESPONSE ~ fEXPN + fPBN + fEXPN * fPBN + (1 | fNID)
>    Data: FL.1
>    AIC   BIC logLik deviance
>  104.1 163.0 -35.04    70.09
> Random effects:
>  Groups Name        Variance Std.Dev.
>  fNID   (Intercept) 0.047168 0.21718
> Number of obs: 236, groups: fNID, 15
> 
> Fixed effects:
>                Estimate Std. Error z value Pr(>|z|)
> (Intercept)     -0.4102     0.5304  -0.774    0.439
> fEXPN2           1.4329     0.7880   1.818    0.069 .
> fEXPN3          21.9715 12840.4523   0.002    0.999
> fEXPN4          21.9715 12409.4403   0.002    0.999
> fPBN2           21.9715 12409.4408   0.002    0.999
> fPBN3            0.5453     0.7390   0.738    0.461
> fPBN4           -1.4788     0.9286  -1.593    0.111
> fEXPN2:fPBN2    -1.4171 17619.4645  -1e-04    1.000
> fEXPN3:fPBN2   -21.9715 21994.2881  -0.001    0.999
> fEXPN4:fPBN2   -21.9715 21493.7828  -0.001    0.999
> fEXPN2:fPBN3    20.0155 12547.8259   0.002    0.999
> fEXPN3:fPBN3    -0.5390 18188.0409   0.000    1.000
> fEXPN4:fPBN3    -0.5390 17577.5297   0.000    1.000
> fEXPN2:fPBN4   -21.1113 12447.6988  -0.002    0.999
> fEXPN3:fPBN4   -41.6658 18265.7721  -0.002    0.998
> fEXPN4:fPBN4   -41.6660 17647.6445  -0.002    0.998

  This looks like a Hauck-Donner effect to me (you can look it
up -- it occurs in GLM(M)s when there are strong binomial
effects).  I'm a little worried about the exact equivalence
of some of the parameter estimates though, and apparent
overfitting ... if these are Bernoulli data, how many
positive responses are there overall?  Have you tried
nAGQ=8 ?

> So here I had no F statistic at all, and the results of the fixed effects
> don't make sense, when compared against the actual data (where the pattern
> between PBN (playback type) and EXPN (experiment) are quite stark.
> 
> Because this didn't make sense to me, and because I am not a whiz at R, I
> ran the same model in SPSS 20.0, and that is where I got the huge F
> statistic. Here is that model summary:
> 
>  F = 1, 229, 656.019, df1 = 15, df2=220, P<0.0001
> EXPN F = 83.83, df1=3, df2=220, P<0.0001
> PBN F=2,2280.08, df1=3, df2=220, P<0.0001
> EXPNxPNB F=82.96, df1=9, df2=220, P<0.0001

  But: what did you do in SPSS, which doesn't fit GLMMs (as
far as I know)?

> >
> > Ideally, I would like to run a GLMM on my data, which are are binomial (so
> > I'm trying to run a model with a binomial distribution and a logit link).
> >  I have two fixed effects, fledgling age and playback type.  I have
> > fledging ID as a random effect.  The problem I'm running into is zero
> > variance in my response variable - ie fledglings always respond the same
> > way to some of the playback types.  I can run a model and it converges, but
> > it spits out an F value of over a million. So I'm wondering (1) is it
> > possible to use a GLMM with a zero variance response; and (2) if so, which
> > R package is the most appropriate?
> >
> >


From jwiley.psych at gmail.com  Sun Aug 12 03:38:02 2012
From: jwiley.psych at gmail.com (Joshua Wiley)
Date: Sat, 11 Aug 2012 18:38:02 -0700
Subject: [R-sig-ME] Response variable with zero variance
In-Reply-To: <loom.20120812T032624-442@post.gmane.org>
References: <CACx4S4OwEBopxPbgv-nb_uma=H=ohjn1YdJ+47YmcXsPF8knVw@mail.gmail.com>
	<AA818EAD2576BC488B4F623941DA74275E396C53@inbomail.inbo.be>
	<CACx4S4ObXZoCOmmGOA90t5wC5W8ha10ZCc81ArKSeW1ttTHh-A@mail.gmail.com>
	<loom.20120812T032624-442@post.gmane.org>
Message-ID: <CANz9Z_LCJJuuPDgreMykV6zt8kdrBjqVyLFTSPLC_4_qcUoggw@mail.gmail.com>

On Sat, Aug 11, 2012 at 6:31 PM, Ben Bolker <bbolker at gmail.com> wrote:
> Tonya Haff <tonyahaff at ...> writes:
>
>> I used the lmer package, using the following code
>>
>> FL.glmm <- lmer(RESPONSE ~ fEXPN + fPBN + fEXPN*fPBN + (1|fNID), data =
>> FL.1, family = binomial)
>
>   By the way, the fixed effect part of the formula can also
> be written as fEXPN*fPBN alone (since A*B is equivalent to A+B+A:B;
> unlike in some other stats packages, * means "crossed" rather
> than "interaction")
>
>    It may be worth double-checking this with glmmPQL:
>
>   library(MASS)
>   glmmPQL(RESPONSE~fEXPN*fPBN,random=~1|fNID,data=FL.1,
>     family="binomial")
>
> although if the data are Bernoulli (0/1) then PQL is a little
> bit questionable.
>
>>
>> Here is the summary
>>
>> Generalized linear mixed model fit by the Laplace approximation
>> Formula: RESPONSE ~ fEXPN + fPBN + fEXPN * fPBN + (1 | fNID)
>>    Data: FL.1
>>    AIC   BIC logLik deviance
>>  104.1 163.0 -35.04    70.09
>> Random effects:
>>  Groups Name        Variance Std.Dev.
>>  fNID   (Intercept) 0.047168 0.21718
>> Number of obs: 236, groups: fNID, 15
>>
>> Fixed effects:
>>                Estimate Std. Error z value Pr(>|z|)
>> (Intercept)     -0.4102     0.5304  -0.774    0.439
>> fEXPN2           1.4329     0.7880   1.818    0.069 .
>> fEXPN3          21.9715 12840.4523   0.002    0.999
>> fEXPN4          21.9715 12409.4403   0.002    0.999
>> fPBN2           21.9715 12409.4408   0.002    0.999
>> fPBN3            0.5453     0.7390   0.738    0.461
>> fPBN4           -1.4788     0.9286  -1.593    0.111
>> fEXPN2:fPBN2    -1.4171 17619.4645  -1e-04    1.000
>> fEXPN3:fPBN2   -21.9715 21994.2881  -0.001    0.999
>> fEXPN4:fPBN2   -21.9715 21493.7828  -0.001    0.999
>> fEXPN2:fPBN3    20.0155 12547.8259   0.002    0.999
>> fEXPN3:fPBN3    -0.5390 18188.0409   0.000    1.000
>> fEXPN4:fPBN3    -0.5390 17577.5297   0.000    1.000
>> fEXPN2:fPBN4   -21.1113 12447.6988  -0.002    0.999
>> fEXPN3:fPBN4   -41.6658 18265.7721  -0.002    0.998
>> fEXPN4:fPBN4   -41.6660 17647.6445  -0.002    0.998
>
>   This looks like a Hauck-Donner effect to me (you can look it
> up -- it occurs in GLM(M)s when there are strong binomial
> effects).  I'm a little worried about the exact equivalence
> of some of the parameter estimates though, and apparent
> overfitting ... if these are Bernoulli data, how many
> positive responses are there overall?  Have you tried
> nAGQ=8 ?
>
>> So here I had no F statistic at all, and the results of the fixed effects
>> don't make sense, when compared against the actual data (where the pattern
>> between PBN (playback type) and EXPN (experiment) are quite stark.
>>
>> Because this didn't make sense to me, and because I am not a whiz at R, I
>> ran the same model in SPSS 20.0, and that is where I got the huge F
>> statistic. Here is that model summary:
>>
>>  F = 1, 229, 656.019, df1 = 15, df2=220, P<0.0001
>> EXPN F = 83.83, df1=3, df2=220, P<0.0001
>> PBN F=2,2280.08, df1=3, df2=220, P<0.0001
>> EXPNxPNB F=82.96, df1=9, df2=220, P<0.0001
>
>   But: what did you do in SPSS, which doesn't fit GLMMs (as
> far as I know)?

SPSS does fit GLMMs (using PQL, I believe).  It is not a pleasant
interface, but here are examples:

http://www.ats.ucla.edu/stat/spss/code/hdp.htm

which are meant to analyze this (simulated) dataset:

http://www.ats.ucla.edu/stat/r/pages/mesimulation.htm

although I had to give up the right censored mixed effects poisson
(just ran as poisson in SPSS) and the mixed effects beta (ran as
binomial in SPSS).

>
>> >
>> > Ideally, I would like to run a GLMM on my data, which are are binomial (so
>> > I'm trying to run a model with a binomial distribution and a logit link).
>> >  I have two fixed effects, fledgling age and playback type.  I have
>> > fledging ID as a random effect.  The problem I'm running into is zero
>> > variance in my response variable - ie fledglings always respond the same
>> > way to some of the playback types.  I can run a model and it converges, but
>> > it spits out an F value of over a million. So I'm wondering (1) is it
>> > possible to use a GLMM with a zero variance response; and (2) if so, which
>> > R package is the most appropriate?
>> >
>> >
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



-- 
Joshua Wiley
Ph.D. Student, Health Psychology
Programmer Analyst II, Statistical Consulting Group
University of California, Los Angeles
https://joshuawiley.com/


From frazer_sinclair at yahoo.co.uk  Mon Aug 13 09:19:23 2012
From: frazer_sinclair at yahoo.co.uk (Frazer Sinclair)
Date: Mon, 13 Aug 2012 08:19:23 +0100 (BST)
Subject: [R-sig-ME] Controlling for non-independence in across population
	analyasis
Message-ID: <1344842363.16121.YahooMailClassic@web132204.mail.ird.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120813/9abe7cb2/attachment.pl>

From b.pelzer at maw.ru.nl  Mon Aug 13 23:37:45 2012
From: b.pelzer at maw.ru.nl (Ben Pelzer)
Date: Mon, 13 Aug 2012 23:37:45 +0200
Subject: [R-sig-ME] lmer logistic nested random effects
In-Reply-To: <SNT118-W59C3F00962364D8C52D274DCB30@phx.gbl>
References: <SNT118-W589F3BC48FAF3F0DFF742DDCB30@phx.gbl>,
	<Pine.LNX.4.64.1208110816590.14579@orpheus.qimr.edu.au>
	<SNT118-W59C3F00962364D8C52D274DCB30@phx.gbl>
Message-ID: <502973A9.4060309@maw.ru.nl>

Dear list,

I would like to fit a logistic model with lmer, for nested factors like 
classes nested in schools, and Y being 0 or 1. So I have dichotomous Y 
values of pupils in classes in schools. My null-model reads:

lmer ( Y ~ (1|school/class), family=binomial(link=logit), nAGQ=7)

The options nAGQ=7 causes the following error:

    AGQ method requires a single grouping factor

Omitting the nAGQ=7 works fine and produces Laplace approximation of the 
loglikelihood.

Am I right in concluding that for hierarchical nested factors (like 
class nested in school) glmer can only use 1 quadrature point? Or am I 
overlooking something?

Of course, more quadrature points would be nicer, and in e.g. xtmelogit 
in stata and glimmix in sas I believe more quad. points are possible 
(i.e., for hierarchically nested factors, not for crossed factors) Hence 
I was surprised that in lmer, it seems impossible. Could anyone confirm 
this, please? Kind regards,

Ben.


From jwiley.psych at gmail.com  Tue Aug 14 05:53:16 2012
From: jwiley.psych at gmail.com (Joshua Wiley)
Date: Mon, 13 Aug 2012 20:53:16 -0700
Subject: [R-sig-ME] lmer logistic nested random effects
In-Reply-To: <502973A9.4060309@maw.ru.nl>
References: <SNT118-W589F3BC48FAF3F0DFF742DDCB30@phx.gbl>
	<Pine.LNX.4.64.1208110816590.14579@orpheus.qimr.edu.au>
	<SNT118-W59C3F00962364D8C52D274DCB30@phx.gbl>
	<502973A9.4060309@maw.ru.nl>
Message-ID: <CANz9Z_+xD_DOZCZtMf0yuHAx9mA0sDwc3qqHLh2_OfgErqA4ew@mail.gmail.com>

Hi Ben,

It seems as though you are correct that Stata uses more quadrature
points.  However, I do not believe that SAS does for more than a
single grouping factor.  Obviously this was not exhaustive, but I
tried when I wrote this page:

http://www.ats.ucla.edu/stat/sas/code/hdp.htm

and at least proc glimmix would not work with anything besides pseudo
likelihoods based on a linearization of the problem or the Laplace
approximation (quadrature with a single integration point).  I happen
to have also tried Stata and SAS with cross classified random effects
in a logistic model (this time with real data) and know that neither
of them use more integration points.

Another option you can try is to use the glmmADMB package which
implements a variety of models optimized in the backend with the AD
model builder.  Here is an example you can try and compare.  Note that
glmmadmb() was _substantially_ slower and took minutes to complete on
my machine with even this simple intercept only model.

hdp <- read.csv("http://www.ats.ucla.edu/stat/data/hdp.csv")
hdp$DID <- factor(hdp$DID)
hdp$HID <- factor(hdp$HID)
require(lme4)
m1 <- glmer(remission ~ 1 + (1 | DID) + (1 | HID), family = binomial,
data = hdp)
require(glmmADMB)
m2 <- glmmadmb(remission ~ 1 + (1 | DID) + (1 | HID), family =
"binomial", data = hdp)


which for me gives:

> summary(m1)
Generalized linear mixed model fit by maximum likelihood ['summary.mer']
 Family: binomial ( logit )
Formula: remission ~ 1 + (1 | DID) + (1 | HID)
   Data: hdp

      AIC       BIC    logLik  deviance
 7884.569  7905.721 -3939.284  7878.569

Random effects:
 Groups Name        Variance Std.Dev.
 DID    (Intercept) 3.4578   1.8595
 HID    (Intercept) 0.2015   0.4489
Number of obs: 8525, groups: DID, 407; HID, 35

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)  -1.3973     0.1269  -11.01   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> summary(m2)

Call:
glmmadmb(formula = remission ~ 1 + (1 | DID) + (1 | HID), data = hdp,
    family = "binomial")


Coefficients:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)    -1.40       0.13   -10.7   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Number of observations: total=8525, DID=407, HID=35
Random effect variance(s):
Group=DID
            Variance StdDev
(Intercept)    3.456  1.859
Group=HID
            Variance StdDev
(Intercept)   0.2036 0.4513

Log-likelihood: -3939.28

Cheers,

Josh


On Mon, Aug 13, 2012 at 2:37 PM, Ben Pelzer <b.pelzer at maw.ru.nl> wrote:
> Dear list,
>
> I would like to fit a logistic model with lmer, for nested factors like
> classes nested in schools, and Y being 0 or 1. So I have dichotomous Y
> values of pupils in classes in schools. My null-model reads:
>
> lmer ( Y ~ (1|school/class), family=binomial(link=logit), nAGQ=7)
>
> The options nAGQ=7 causes the following error:
>
>    AGQ method requires a single grouping factor
>
> Omitting the nAGQ=7 works fine and produces Laplace approximation of the
> loglikelihood.
>
> Am I right in concluding that for hierarchical nested factors (like class
> nested in school) glmer can only use 1 quadrature point? Or am I overlooking
> something?
>
> Of course, more quadrature points would be nicer, and in e.g. xtmelogit in
> stata and glimmix in sas I believe more quad. points are possible (i.e., for
> hierarchically nested factors, not for crossed factors) Hence I was
> surprised that in lmer, it seems impossible. Could anyone confirm this,
> please? Kind regards,
>
> Ben.
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



-- 
Joshua Wiley
Ph.D. Student, Health Psychology
Programmer Analyst II, Statistical Consulting Group
University of California, Los Angeles
https://joshuawiley.com/


From leila.brook at my.jcu.edu.au  Tue Aug 14 08:17:44 2012
From: leila.brook at my.jcu.edu.au (Leila Brook)
Date: Tue, 14 Aug 2012 06:17:44 +0000
Subject: [R-sig-ME] Including an offset in a binomial GLM/GLMM
Message-ID: <9BEE83C6BF59174DBD2CB4A06EB6CBEB2FDEAE87@SINPRD0111MB368.apcprd01.prod.exchangelabs.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120814/49126d7c/attachment.pl>

From jbaldwin at fs.fed.us  Tue Aug 14 15:15:14 2012
From: jbaldwin at fs.fed.us (Baldwin, Jim -FS)
Date: Tue, 14 Aug 2012 13:15:14 +0000
Subject: [R-sig-ME] Including an offset in a binomial GLM/GLMM
In-Reply-To: <9BEE83C6BF59174DBD2CB4A06EB6CBEB2FDEAE87@SINPRD0111MB368.apcprd01.prod.exchangelabs.com>
References: <9BEE83C6BF59174DBD2CB4A06EB6CBEB2FDEAE87@SINPRD0111MB368.apcprd01.prod.exchangelabs.com>
Message-ID: <DDC5EC9B78340042B0D5A0C3789D45690ECA4C8C@001FSN2MPN1-062.001f.mgd2.msft.net>

If the probability of animal presence and the detection probability are both of interest, have you considered the unmarked package?  That package will allow (and is built for) accounting for variability in sampling effort (such as the number of trap nights).

Jim Baldwin
Pacific Southwest Research Station
USDA Forest Service

-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Leila Brook
Sent: Monday, August 13, 2012 11:18 PM
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Including an offset in a binomial GLM/GLMM

Dear all,

I apologise if this is an obvious question, but I haven't been able to find reference to it in the literature so far. I was wondering whether it is possible to include an offset variable in a binomial GLM/GLMM, as well as in poisson models?



For example, I surveyed for my study species during a set time period and collected presence-absence data. On some nights the camera traps failed, so the no. trap nights is reduced, which could then influence detection.

Hence I would like to include trap night as an offset.



However, if it is possible to include the trap nights as an offset in a binomial model with logit link function, is it alright to just include it as below:



model<- glm(pres ~ a*b + c, offset=trapnight, family=binomial, data=data)



Or would it need to be transformed as in Poisson models with a log link when included in the formula:

model<- glm(count ~ a*b + c + offset(log(trapnight), family=poisson, data=data)



In this case, would the transformation be "offset(logit(trapnight))" or "offset(invlogit(trapnight))"



Thank you for your help,

Leila

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models





This electronic message contains information generated by the USDA solely for the intended recipients. Any unauthorized interception of this message or the use or disclosure of the information it contains may violate the law and subject the violator to civil or criminal penalties. If you believe you have received this message in error, please notify the sender and delete the email immediately.


From tara.zamin at queensu.ca  Tue Aug 14 17:11:41 2012
From: tara.zamin at queensu.ca (Tara Zamin)
Date: Tue, 14 Aug 2012 15:11:41 +0000
Subject: [R-sig-ME] Split plot design and repeated measures
Message-ID: <2822CC01C1504F4385C04B2C96B94EAD0983D5CA@MP-DUP-MBX-01.AD.QUEENSU.CA>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120814/7ea49e83/attachment.pl>

From Kathleen.Vancleef at ppw.kuleuven.be  Mon Aug 13 12:10:51 2012
From: Kathleen.Vancleef at ppw.kuleuven.be (Kathleen Vancleef)
Date: Mon, 13 Aug 2012 10:10:51 +0000
Subject: [R-sig-ME] error applying user-defined link function to lmer
Message-ID: <1D5D8E931D9B4342A2E9D41931B5734805C03C01@ICTS-S-MBX7.luna.kuleuven.be>


Dear R users,

I'm struggling with applying a user-defined link function in lmer. For analyzing data of a 2AFC psychophysical experiment, I would like to model my binary data with a logistic function with a lower limit at 0.5 instead of 0. In a previous question this has been described as a halflogit function. To do so I wrote my own link function and would like to submit it to lmer, however this results in error messages. Below I've listed some of my attempts to solve this problem based on previous postings on this list. I've also added the corresponding problems I encountered with each of these attempts.

1) I've used the link function as specified at https://stat.ethz.ch/pipermail/r-help/2007-August/138436.html

# define link function
halflogit=function(){
half.logit=function(mu) qlogis(2*mu-1)
half.logit.inv=function(eta) .5*plogis(eta)+.5
half.logit.deriv=function(eta) .5*(exp(eta/2)+exp(-eta/2))^-2
half.logit.inv.indicator=function(eta) TRUE
half.logit.indicator=function(mu) mu>.5 & mu<1 link <- "half.logit"
structure(list(linkfun = half.logit, linkinv = half.logit.inv,
??????????????????? mu.eta = half.logit.deriv, validmu = half.logit.indicator ,valideta = half.logit.inv.indicator, name = link),
?????????????? class = "link-glm")
}
# submit halflogit function to lmer
rcNrPSrcNxPSxP<-lmer(Correct~-1+cNoise*f_Pos+f_Shape_con+f_Shape_con:f_Pos+(-1+cNoise*f_Pos|f_Subject), data=dens_data, family=binomial(halflogit()))

#resulting error message
Error in if (!(validmu(mu) && valideta(eta))) stop("cannot find valid starting values: please specify some",? : 
??missing value where TRUE/FALSE needed
In addition: Warning message:
In qlogis(p, location, scale, lower.tail, log.p) : NaNs produced

2) Also making the variable global seems not to help (see also http://permalink.gmane.org/gmane.comp.lang.r.lme4.devel/4501)

mafc.logit <- function (.m = 2)
{
    .m <<- as.integer(.m)  # .m goes global
    if (.m < 2)
        stop(".m must be an integer > 1")
    linkfun <- function(mu) {
        mu <- pmax(mu, 1/.m + .Machine$double.eps)
        qlogis((.m * mu - 1)/(.m - 1))
    }
    linkinv <- function(eta) {
        1/.m + (.m - 1)/.m * .Call("logit_linkinv", eta, PACKAGE = "stats")
    }
    mu.eta <- function(eta) ((.m - 1)/.m) * .Call("logit_mu_eta",
        eta, PACKAGE = "stats")
    valideta <- function(eta) TRUE
    link <- paste("mafc.logit(", .m, ")", sep = "")
    structure(list(linkfun = linkfun, linkinv = linkinv, mu.eta = mu.eta,
        valideta = valideta, name = link), class = "link-glm") }

lmer(Correct~-1+cNoise*f_Pos+f_Shape_con+f_Shape_con:f_Pos+(-1+cNoise*f_Pos|f_Subject), data=dens_data, family=binomial(mafc.logit(2)))

Error in famType(glmFit$family) : unknown link: 'mafc.logit(2)'

3) The link function does work with glm so the problem is in lmer, I guess, not in the newly defined link function. Unfortunately I would like to model random effect, so I prefer to use the lmer function.

4) I've also tried with other versions of the package: lme4a and lme4.0, but lme4a seems not to work under the current R version, and lme4.0 gives the same errors as lme4.

5) I've tried to transform the data myself with my own function and fit a linear model on the transformed data. However transforming results in Inf, -Inf and NaN for proportion of 1, 0.5 and <0.5 respectively and a linear model cannot handle Inf, -Inf and NaN. In addition, I'm not sure if this is an appropriate way to do my analyses because the variance on the data would be estimated as an additional parameter in a linear model while in logistic regression the variance is prop(1-prop) and is not estimated as an additional parameter.

6) Last I tried using glmmPQL but this does not reach convergence and I'm not sure if it's suitable for my analyses.

I would appreciate any help on applying a halflogit function to lmer.

Kind regards,
Kathleen
---------------------------------------------------------------------------
Kathleen Vancleef
PhD student

Laboratory of Experimental Psychology
University of Leuven
Tiensestraat 102 box 3711
B-3000 Leuven
Belgium
Tel: +32 (0)16/32.62.83
Email:? Kathleen.Vancleef at ppw.kuleuven.be http://www.gestaltrevision.be 


From villegaskary at gmail.com  Wed Aug 15 21:08:02 2012
From: villegaskary at gmail.com (Karina Villegas)
Date: Wed, 15 Aug 2012 12:08:02 -0700
Subject: [R-sig-ME] unused argument(s) HELP
Message-ID: <CALcKDYEo9Wsy11gpR6HAchMqb-ap8t=SxydcsWmRfOBJaakkyA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120815/1641d61f/attachment.pl>

From bates at stat.wisc.edu  Wed Aug 15 21:48:14 2012
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 15 Aug 2012 14:48:14 -0500
Subject: [R-sig-ME] unused argument(s) HELP
In-Reply-To: <CALcKDYEo9Wsy11gpR6HAchMqb-ap8t=SxydcsWmRfOBJaakkyA@mail.gmail.com>
References: <CALcKDYEo9Wsy11gpR6HAchMqb-ap8t=SxydcsWmRfOBJaakkyA@mail.gmail.com>
Message-ID: <CAO7JsnR72CFdPQxFWoPH7BZoYCnENYtxNHfNnW2TDp1u+eON+A@mail.gmail.com>

On Wed, Aug 15, 2012 at 2:08 PM, Karina Villegas <villegaskary at gmail.com> wrote:
> Dear R experts:
>
> I am running R version 2.12.1 on Windows 2007. I am studying the effects
> maternal behavior in the body condition of sea lion pups from California.
>
> I specified my model as follows:
>
> LevelModel1 <- lme(PBC ~ Sex*Dur.nurse, random=~Sex|Trip, data=Dataset,
> max.iter=100)
>
> *See output below:*
> Error in lme(PBC ~ Sex * Dur.nurse, random = ~Sex | Trip, data = Dataset,  :
>   unused argument(s) (maxiter = 100)

There is no maxiter argument to the lme function.  You want to specify it as

control=list(maxIter=100)

(The upper-case 'I' in maxIter is important.)
> Anyone can help me with the error that makes the output? Please
>
> Thanks
> Karina
>
>
> --
> Biol. Karina Villegas Cervantes
> Estudiante de Maestr?a PCMyL - UNAM
>
> Laboratorio de Ecologia de Pinnipedos Burney J. Le Boueuf.
> CICIMAR-IPN
> Av. Instituto Politecnico Nacional s/n.Col.Playa Palo de Santa Rita
> La Paz Baja California Sur, Mexico.
>
>         [[alternative HTML version deleted]]
>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From villegaskary at gmail.com  Wed Aug 15 22:09:22 2012
From: villegaskary at gmail.com (Karina Villegas)
Date: Wed, 15 Aug 2012 13:09:22 -0700
Subject: [R-sig-ME] unused argument(s) HELP
In-Reply-To: <CAO7JsnR72CFdPQxFWoPH7BZoYCnENYtxNHfNnW2TDp1u+eON+A@mail.gmail.com>
References: <CALcKDYEo9Wsy11gpR6HAchMqb-ap8t=SxydcsWmRfOBJaakkyA@mail.gmail.com>
	<CAO7JsnR72CFdPQxFWoPH7BZoYCnENYtxNHfNnW2TDp1u+eON+A@mail.gmail.com>
Message-ID: <CALcKDYEm_ewuLQfj9mcE4Zv6hA2_wPLgkznMuo+JCH98HDVDgw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120815/c0007623/attachment.pl>

From bates at stat.wisc.edu  Wed Aug 15 23:12:33 2012
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 15 Aug 2012 16:12:33 -0500
Subject: [R-sig-ME] unused argument(s) HELP
In-Reply-To: <CALcKDYEm_ewuLQfj9mcE4Zv6hA2_wPLgkznMuo+JCH98HDVDgw@mail.gmail.com>
References: <CALcKDYEo9Wsy11gpR6HAchMqb-ap8t=SxydcsWmRfOBJaakkyA@mail.gmail.com>
	<CAO7JsnR72CFdPQxFWoPH7BZoYCnENYtxNHfNnW2TDp1u+eON+A@mail.gmail.com>
	<CALcKDYEm_ewuLQfj9mcE4Zv6hA2_wPLgkznMuo+JCH98HDVDgw@mail.gmail.com>
Message-ID: <CAO7JsnRi4zP3Fo_RLXsNbu9pBoZhJzY=GhtiNmG4jUCbfhcF7g@mail.gmail.com>

On Wed, Aug 15, 2012 at 3:09 PM, Karina Villegas <villegaskary at gmail.com> wrote:
> Thank you very much
>
> In the help of these functions, the maximum number of iterations by default
> is 50, but if I ask for a smaller number of iterations to my model, it shows
> me this error now:
>
>> LevelModel1 <- lme(PBC ~ Sex*Dur.nurse, random=~Sex|Trip, data=Dataset,
>> control=lmeControl(maxIter=50))
> Error in lme.formula(PBC ~ Sex * Dur.nurse, random = ~Sex | Trip, data =
> Dataset,  :
>   nlminb problem, convergence error code = 1
>   message = iteration limit reached without convergence (9)
>> LevelModel1 <- lme(PBC ~ Sex*Dur.nurse, random=~Sex|Trip, data=Dataset,
>> control=lmeControl(maxIter=49))
> Error in lme.formula(PBC ~ Sex * Dur.nurse, random = ~Sex | Trip, data =
> Dataset,  :
>   nlminb problem, convergence error code = 1
>   message = iteration limit reached without convergence (9)
>> LevelModel1 <- lme(PBC ~ Sex*Dur.nurse, random=~Sex|Trip, data=Dataset,
>> control=lmeControl(maxIter=40))
> Error in lme.formula(PBC ~ Sex * Dur.nurse, random = ~Sex | Trip, data =
> Dataset,  :
>   nlminb problem, convergence error code = 1
>   message = iteration limit reached without convergence (9)

If the algorithm reaches the maximum number of iterations then you
must increase that number from 50, not decrease it.

May I recommend that you install the lme4.0 package

install.packages("lme4.0")

and try instead

lmer(PCB ~ Sex*Dur.nurse + (Sex|Trip), Dataset)

and see what that does.  In a model like this it is unusual to need a
large number of iterations for the optimizer to converge unless the
model includes too many terms for the data available.

> 2012/8/15 Douglas Bates <bates at stat.wisc.edu>
>>
>> On Wed, Aug 15, 2012 at 2:08 PM, Karina Villegas <villegaskary at gmail.com>
>> wrote:
>> > Dear R experts:
>> >
>> > I am running R version 2.12.1 on Windows 2007. I am studying the effects
>> > maternal behavior in the body condition of sea lion pups from
>> > California.
>> >
>> > I specified my model as follows:
>> >
>> > LevelModel1 <- lme(PBC ~ Sex*Dur.nurse, random=~Sex|Trip, data=Dataset,
>> > max.iter=100)
>> >
>> > *See output below:*
>> > Error in lme(PBC ~ Sex * Dur.nurse, random = ~Sex | Trip, data =
>> > Dataset,  :
>> >   unused argument(s) (maxiter = 100)
>>
>> There is no maxiter argument to the lme function.  You want to specify it
>> as
>>
>> control=list(maxIter=100)
>>
>> (The upper-case 'I' in maxIter is important.)
>> > Anyone can help me with the error that makes the output? Please
>> >
>> > Thanks
>> > Karina
>> >
>> >
>> > --
>> > Biol. Karina Villegas Cervantes
>> > Estudiante de Maestr?a PCMyL - UNAM
>> >
>> > Laboratorio de Ecologia de Pinnipedos Burney J. Le Boueuf.
>> > CICIMAR-IPN
>> > Av. Instituto Politecnico Nacional s/n.Col.Playa Palo de Santa Rita
>> > La Paz Baja California Sur, Mexico.
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >
>
>
>
>
> --
> Biol. Karina Villegas Cervantes
> Estudiante de Maestr?a PCMyL - UNAM
>
> Laboratorio de Ecologia de Pinnipedos Burney J. Le Boueuf.
> CICIMAR-IPN
> Av. Instituto Politecnico Nacional s/n.Col.Playa Palo de Santa Rita
> La Paz Baja California Sur, Mexico.
>


From esther.schott at psy.lmu.de  Thu Aug 16 19:32:16 2012
From: esther.schott at psy.lmu.de (Esther Schott)
Date: Thu, 16 Aug 2012 19:32:16 +0200
Subject: [R-sig-ME] Setting up interaction contrasts in lme4 generalized
 linear mixed model
Message-ID: <502D2EA0.9090604@psy.lmu.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120816/30ad0f70/attachment.pl>

From roby.joehanes at nih.gov  Thu Aug 16 22:28:10 2012
From: roby.joehanes at nih.gov (Joehanes, Roby (NIH/NHLBI) [F])
Date: Thu, 16 Aug 2012 16:28:10 -0400
Subject: [R-sig-ME] lmer bug
Message-ID: <CC52D01A.2B1C%joehanesr@mail.nih.gov>

Hi all:

This appears to be a bug affecting lmer that is in the Subversion up until the current version (1788).

The following works:
   ff1 <- Reaction ~ Days + (Days|Subject)
   fm1 <- lmer(ff1, sleepstudy)
   print (anova(fm1))

But the following does not:
fun <- function () {
   ff1 <- Reaction ~ Days + (Days|Subject)
   fm1 <- lmer(ff1, sleepstudy)
   return (anova(fm1))
}
print(fun())

Error message:
Error in print(fun()) :
  error in evaluating the argument 'x' in selecting a method for function 'print': Error in eval(expr, envir, enclos) : object 'ff1' not found

This error, however, does not affect the lme4 package released in CRAN.

Thanks,
Roby


From roby.joehanes at nih.gov  Fri Aug 17 00:36:58 2012
From: roby.joehanes at nih.gov (Joehanes, Roby (NIH/NHLBI) [F])
Date: Thu, 16 Aug 2012 18:36:58 -0400
Subject: [R-sig-ME] lmer bug
In-Reply-To: <1345155182.55914.YahooMailNeo@web142602.mail.bf1.yahoo.com>
Message-ID: <CC52EE4A.2B22%joehanesr@mail.nih.gov>

Hi Arun:

Did you use the lmer from CRAN or built from Subversion source? This code
indeed works for lmer from CRAN, as I indicated in my original e-mail. I was
just pointing out that it no longer works for lmer from Subversion source.
Which Subversion release number are you using?

Roby


On 8/16/12 6:13 PM, "arun" <smartpink111 at yahoo.com> wrote:

> HI,
> 
> It's working for me.? I am using R 2.15.
> 
> 
> ?ff1 <- Reaction ~ Days + (Days|Subject)
>> ?? fm1 <- lmer(ff1, sleepstudy)
>> ?? print (anova(fm1))
> Analysis of Variance Table
> ???? Df Sum Sq Mean Sq F value
> Days? 1? 29986?? 29986? 45.785
>> fun <- function () {
> +?? ff1 <- Reaction ~ Days + (Days|Subject)
> +?? fm1 <- lmer(ff1, sleepstudy)
> +?? return (anova(fm1))
> + }
>> print(fun())
> Analysis of Variance Table
> ???? Df Sum Sq Mean Sq F value
> Days? 1? 29986?? 29986? 45.785
> A.K.
> 
> 
> 
> ----- Original Message -----
> From: "Joehanes, Roby (NIH/NHLBI) [F]" <roby.joehanes at nih.gov>
> To: "r-sig-mixed-models at r-project.org" <r-sig-mixed-models at r-project.org>
> Cc: 
> Sent: Thursday, August 16, 2012 4:28 PM
> Subject: [R-sig-ME] lmer bug
> 
> Hi all:
> 
> This appears to be a bug affecting lmer that is in the Subversion up until the
> current version (1788).
> 
> The following works:
> ?  ff1 <- Reaction ~ Days + (Days|Subject)
> ?  fm1 <- lmer(ff1, sleepstudy)
> ?  print (anova(fm1))
> 
> But the following does not:
> fun <- function () {
> ?  ff1 <- Reaction ~ Days + (Days|Subject)
> ?  fm1 <- lmer(ff1, sleepstudy)
> ?  return (anova(fm1))
> }
> print(fun())
> 
> Error message:
> Error in print(fun()) :
> ? error in evaluating the argument 'x' in selecting a method for function
> 'print': Error in eval(expr, envir, enclos) : object 'ff1' not found
> 
> This error, however, does not affect the lme4 package released in CRAN.
> 
> Thanks,
> Roby
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 

-- 
Roby Joehanes
Research Associate
Roby.Joehanes at nih.gov
Building 12A, Room 2007
National Institutes of Health (NIH)
Bethesda, MD 20892
P: (301) 402-8702
F: (301) 480-0028 or (301) 402-2867


From bates at stat.wisc.edu  Fri Aug 17 18:32:56 2012
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 17 Aug 2012 11:32:56 -0500
Subject: [R-sig-ME] lmer bug
In-Reply-To: <CC52D01A.2B1C%joehanesr@mail.nih.gov>
References: <CC52D01A.2B1C%joehanesr@mail.nih.gov>
Message-ID: <CAO7JsnRM_3avjyHLgb5vdW6uUFD_R0=ZZcd-hyzgTxJ28PxHiQ@mail.gmail.com>

On Thu, Aug 16, 2012 at 3:28 PM, Joehanes, Roby (NIH/NHLBI) [F]
<roby.joehanes at nih.gov> wrote:
> Hi all:
>
> This appears to be a bug affecting lmer that is in the Subversion up until the current version (1788).
>
> The following works:
>    ff1 <- Reaction ~ Days + (Days|Subject)
>    fm1 <- lmer(ff1, sleepstudy)
>    print (anova(fm1))
>
> But the following does not:
> fun <- function () {
>    ff1 <- Reaction ~ Days + (Days|Subject)
>    fm1 <- lmer(ff1, sleepstudy)
>    return (anova(fm1))
> }
> print(fun())
>
> Error message:
> Error in print(fun()) :
>   error in evaluating the argument 'x' in selecting a method for function 'print': Error in eval(expr, envir, enclos) : object 'ff1' not found
>
> This error, however, does not affect the lme4 package released in CRAN.

This error in the development version is probably due to the tricky
business of when exactly a formula is evaluated.  A formula records
the environment in which it is evaluated and that is the default
environment for some later evaluations.  In an attempt to accommodate
certain corner cases where the formula is given as a character string
the evaluation of the formula was changed.  Myself I would rather have
cases like this work than character strings for formulas but I think
we will need to wait until Ben is available again to look as this


From hannah.hlx at gmail.com  Sat Aug 18 03:39:48 2012
From: hannah.hlx at gmail.com (li li)
Date: Fri, 17 Aug 2012 21:39:48 -0400
Subject: [R-sig-ME] random effect nested in the fixed effect
Message-ID: <CAHLnndYW79boeEbhEJKvLWiO7NihY+UfYTKSt1jQZ66Wn4TA3w@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120817/1368b598/attachment.pl>

From hannah.hlx at gmail.com  Fri Aug 17 13:41:37 2012
From: hannah.hlx at gmail.com (li li)
Date: Fri, 17 Aug 2012 07:41:37 -0400
Subject: [R-sig-ME] Question on Mixed Models Uisng R
Message-ID: <CAHLnndb+7_wJFJibjv9ivcEg2bRvS9=j2toKzAenUHf01YnJMA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120817/360a4a48/attachment.pl>

From hans at sociologi.cjb.net  Sat Aug 18 15:32:21 2012
From: hans at sociologi.cjb.net (Hans Ekbrand)
Date: Sat, 18 Aug 2012 15:32:21 +0200
Subject: [R-sig-ME] random effect nested in the fixed effect
In-Reply-To: <CAHLnndYW79boeEbhEJKvLWiO7NihY+UfYTKSt1jQZ66Wn4TA3w@mail.gmail.com>
References: <CAHLnndYW79boeEbhEJKvLWiO7NihY+UfYTKSt1jQZ66Wn4TA3w@mail.gmail.com>
Message-ID: <20120818133207.GA12121@samir>

On Fri, Aug 17, 2012 at 09:39:48PM -0400, li li wrote:
> Dear all,
>     I am starting to use R for mixed models.
>     For example, for the date below,
>       I want to fit the model values=sample+ind(sample).
> Here "sample" is a fixed effect and "ind"
> should be a random effect nested in "sample".

for lme4 I would have tried:

lmer(values ~ sample + (sample | ind), data = y)

-- 
Hans Ekbrand (http://sociologi.cjb.net) <hans at sociologi.cjb.net>


From hans at sociologi.cjb.net  Sat Aug 18 15:42:19 2012
From: hans at sociologi.cjb.net (Hans Ekbrand)
Date: Sat, 18 Aug 2012 15:42:19 +0200
Subject: [R-sig-ME] random effect nested in the fixed effect
In-Reply-To: <CAHLnndYW79boeEbhEJKvLWiO7NihY+UfYTKSt1jQZ66Wn4TA3w@mail.gmail.com>
References: <CAHLnndYW79boeEbhEJKvLWiO7NihY+UfYTKSt1jQZ66Wn4TA3w@mail.gmail.com>
Message-ID: <20120818134219.GB12121@samir>

On Fri, Aug 17, 2012 at 09:39:48PM -0400, li li wrote:
> Dear all,
>     I am starting to use R for mixed models.
>     For example, for the date below,
>       I want to fit the model values=sample+ind(sample).
> Here "sample" is a fixed effect and "ind"
> should be a random effect nested in "sample".

[...]

>    Here is the data structure:
> 
> 
>   values ind sample
> 1  0.03325   1      1
> 2  0.03305   1      1
> 3  0.03185   1      1
> 4  0.03515   1      1
> 5  0.03375   1      1
> 6  0.01180   1      2
> 7  0.01850   1      3
> 8  0.02915   1      4
> 9  0.06200   1      5
> 10 0.03230   2      1
> 11 0.03345   2      1
> 12 0.03385   2      1
> 13 0.03605   2      1
> 14 0.03225   2      1
> 15 0.01145   2      2
> 16 0.01805   2      3
> 17 0.02950   2      4
> 18 0.05995   2      5

This looks like crossed data. It this really is nested data, then it
is implicitly nested as described in
http://lme4.r-forge.r-project.org/book/Ch2.pdf, page 39, and my
formula will not work. For my formula to work, you have to make
samples clearly unique (see page 40 in
http://lme4.r-forge.r-project.org/book/Ch2.pdf):

-- 
Hans Ekbrand (http://sociologi.cjb.net) <hans at sociologi.cjb.net>


From aghaynes at gmail.com  Sat Aug 18 21:12:53 2012
From: aghaynes at gmail.com (Alan Haynes)
Date: Sat, 18 Aug 2012 21:12:53 +0200
Subject: [R-sig-ME] Question on Mixed Models Uisng R
In-Reply-To: <CAHLnndb+7_wJFJibjv9ivcEg2bRvS9=j2toKzAenUHf01YnJMA@mail.gmail.com>
References: <CAHLnndb+7_wJFJibjv9ivcEg2bRvS9=j2toKzAenUHf01YnJMA@mail.gmail.com>
Message-ID: <CAPdSD+5ZPDn-HMYx03LH_548d1Lq-qFDu5m5qSo6Shb=xF8nyA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120818/6ebe761d/attachment.pl>

From ekaaf900 at ricv.zaq.ne.jp  Sun Aug 19 02:29:57 2012
From: ekaaf900 at ricv.zaq.ne.jp (MASAAKI)
Date: Sun, 19 Aug 2012 09:29:57 +0900
Subject: [R-sig-ME] Dear sir,
Message-ID: <DB150790230240DB908E70DC000658B2@tujitaniPC>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120819/a703a46a/attachment.pl>

From leila.brook at my.jcu.edu.au  Mon Aug 20 02:58:13 2012
From: leila.brook at my.jcu.edu.au (Leila Brook)
Date: Mon, 20 Aug 2012 00:58:13 +0000
Subject: [R-sig-ME] Including an offset in a binomial GLM/GLMM
In-Reply-To: <DDC5EC9B78340042B0D5A0C3789D45690ECA4C8C@001FSN2MPN1-062.001f.mgd2.msft.net>
References: <9BEE83C6BF59174DBD2CB4A06EB6CBEB2FDEAE87@SINPRD0111MB368.apcprd01.prod.exchangelabs.com>,
	<DDC5EC9B78340042B0D5A0C3789D45690ECA4C8C@001FSN2MPN1-062.001f.mgd2.msft.net>
Message-ID: <9BEE83C6BF59174DBD2CB4A06EB6CBEB2FDEB2CA@SINPRD0111MB368.apcprd01.prod.exchangelabs.com>

Dear Jim, 
Thanks for your suggestion about using unmarked. I was curious about whether the offset would be possible in a binary model, as it is generally only mentioned for a poisson, but there must be other cases where it would be beneficial for a binary response variable. 
I have looked at detection probability in a separate analysis and was hoping to use a GLM/M to look at presence/absence.
Thanks again,
Leila

________________________________________
From: Baldwin, Jim -FS [jbaldwin at fs.fed.us]
Sent: Tuesday, 14 August 2012 11:15 PM
To: Leila Brook; r-sig-mixed-models at r-project.org
Subject: RE: Including an offset in a binomial GLM/GLMM

If the probability of animal presence and the detection probability are both of interest, have you considered the unmarked package?  That package will allow (and is built for) accounting for variability in sampling effort (such as the number of trap nights).

Jim Baldwin
Pacific Southwest Research Station
USDA Forest Service

-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Leila Brook
Sent: Monday, August 13, 2012 11:18 PM
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Including an offset in a binomial GLM/GLMM

Dear all,

I apologise if this is an obvious question, but I haven't been able to find reference to it in the literature so far. I was wondering whether it is possible to include an offset variable in a binomial GLM/GLMM, as well as in poisson models?



For example, I surveyed for my study species during a set time period and collected presence-absence data. On some nights the camera traps failed, so the no. trap nights is reduced, which could then influence detection.

Hence I would like to include trap night as an offset.



However, if it is possible to include the trap nights as an offset in a binomial model with logit link function, is it alright to just include it as below:



model<- glm(pres ~ a*b + c, offset=trapnight, family=binomial, data=data)



Or would it need to be transformed as in Poisson models with a log link when included in the formula:

model<- glm(count ~ a*b + c + offset(log(trapnight), family=poisson, data=data)



In this case, would the transformation be "offset(logit(trapnight))" or "offset(invlogit(trapnight))"



Thank you for your help,

Leila

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models





This electronic message contains information generated by the USDA solely for the intended recipients. Any unauthorized interception of this message or the use or disclosure of the information it contains may violate the law and subject the violator to civil or criminal penalties. If you believe you have received this message in error, please notify the sender and delete the email immediately.

From jbaldwin at fs.fed.us  Mon Aug 20 06:42:11 2012
From: jbaldwin at fs.fed.us (Baldwin, Jim -FS)
Date: Mon, 20 Aug 2012 04:42:11 +0000
Subject: [R-sig-ME] Including an offset in a binomial GLM/GLMM
In-Reply-To: <9BEE83C6BF59174DBD2CB4A06EB6CBEB2FDEB2CA@SINPRD0111MB368.apcprd01.prod.exchangelabs.com>
References: <9BEE83C6BF59174DBD2CB4A06EB6CBEB2FDEAE87@SINPRD0111MB368.apcprd01.prod.exchangelabs.com>,
	<DDC5EC9B78340042B0D5A0C3789D45690ECA4C8C@001FSN2MPN1-062.001f.mgd2.msft.net>
	<9BEE83C6BF59174DBD2CB4A06EB6CBEB2FDEB2CA@SINPRD0111MB368.apcprd01.prod.exchangelabs.com>
Message-ID: <DDC5EC9B78340042B0D5A0C3789D45690ECA5374@001FSN2MPN1-062.001f.mgd2.msft.net>

If "a*b+c" (as you show below) is an effect of some experimental design (treatments for instance or availability of some desired habitat feature) that affects the probability of presence of a critter and trap nights is more of a measure of "human sampling effort" unrelated to the animal presence, then I think you'd want to estimate both of those effects simultaneously in one analysis rather than in separate analyses.  (The effects of a, b, and c should be either free and clear of the measurement process or if that isn't possible, characterized for a standardized amount of effort.)

You might need to postulate a model of the relationship among the detection probabilities for different trap nights (assuming they don't vary by too much).  A simple (but not necessarily realistic model) might be that there is a probability p to detect the animal on a single trap night (given that it is available to be trapped).  If t consecutive trap nights are independent given that the animal is available to be trapped all t nights, then the probability of a detection would be 1 - (1-p)^t.

For this specific application r-sig-ecology at r-project.org might also be a place to look for suggestions.

Jim


-----Original Message-----
From: Leila Brook [mailto:leila.brook at my.jcu.edu.au] 
Sent: Sunday, August 19, 2012 5:58 PM
To: Baldwin, Jim -FS; r-sig-mixed-models at r-project.org
Subject: RE: Including an offset in a binomial GLM/GLMM

Dear Jim, 
Thanks for your suggestion about using unmarked. I was curious about whether the offset would be possible in a binary model, as it is generally only mentioned for a poisson, but there must be other cases where it would be beneficial for a binary response variable. 
I have looked at detection probability in a separate analysis and was hoping to use a GLM/M to look at presence/absence.
Thanks again,
Leila

________________________________________
From: Baldwin, Jim -FS [jbaldwin at fs.fed.us]
Sent: Tuesday, 14 August 2012 11:15 PM
To: Leila Brook; r-sig-mixed-models at r-project.org
Subject: RE: Including an offset in a binomial GLM/GLMM

If the probability of animal presence and the detection probability are both of interest, have you considered the unmarked package?  That package will allow (and is built for) accounting for variability in sampling effort (such as the number of trap nights).

Jim Baldwin
Pacific Southwest Research Station
USDA Forest Service

-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Leila Brook
Sent: Monday, August 13, 2012 11:18 PM
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Including an offset in a binomial GLM/GLMM

Dear all,

I apologise if this is an obvious question, but I haven't been able to find reference to it in the literature so far. I was wondering whether it is possible to include an offset variable in a binomial GLM/GLMM, as well as in poisson models?



For example, I surveyed for my study species during a set time period and collected presence-absence data. On some nights the camera traps failed, so the no. trap nights is reduced, which could then influence detection.

Hence I would like to include trap night as an offset.



However, if it is possible to include the trap nights as an offset in a binomial model with logit link function, is it alright to just include it as below:



model<- glm(pres ~ a*b + c, offset=trapnight, family=binomial, data=data)



Or would it need to be transformed as in Poisson models with a log link when included in the formula:

model<- glm(count ~ a*b + c + offset(log(trapnight), family=poisson, data=data)



In this case, would the transformation be "offset(logit(trapnight))" or "offset(invlogit(trapnight))"



Thank you for your help,

Leila

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models





This electronic message contains information generated by the USDA solely for the intended recipients. Any unauthorized interception of this message or the use or disclosure of the information it contains may violate the law and subject the violator to civil or criminal penalties. If you believe you have received this message in error, please notify the sender and delete the email immediately.


From rhbc at imm.dtu.dk  Mon Aug 20 10:16:05 2012
From: rhbc at imm.dtu.dk (Rune Haubo)
Date: Mon, 20 Aug 2012 10:16:05 +0200
Subject: [R-sig-ME] two questions about clmm
In-Reply-To: <61327C50-05BF-4D7C-9743-AE3280A1EA20@bristol.ac.uk>
References: <78678DB3-9021-44DE-899B-68D71E64E042@bristol.ac.uk>
	<CAG_uk93R0z8R-2toMV5SUes+_2c+Hs0SRNX0JGM-EUxNhLf5+Q@mail.gmail.com>
	<42206B2D-3125-4CF1-9CBC-2ED5C2A5F6CB@bristol.ac.uk>
	<CAG_uk91iBuYna+icaHkD2mn9NkW2OSQ8XVAdQ=tA8jhAP0g5kw@mail.gmail.com>
	<D094D7ED-53D2-4800-A15A-B01A729274EC@bristol.ac.uk>
	<CAG_uk9015VEH6yNk=j5E8anOanVM8Q-A4UEkQatus=-aifUS1w@mail.gmail.com>
	<61327C50-05BF-4D7C-9743-AE3280A1EA20@bristol.ac.uk>
Message-ID: <CAG_uk90=ifNhcQf9kLzkhi4Gm0gyrA8UOmNWHd2yLyS7ka5shQ@mail.gmail.com>

Dear Malcolm

On 3 August 2012 19:11, Malcolm Fairbrother <m.fairbrother at bristol.ac.uk> wrote:
> Dear Rune (and list),
>
> I've been making use of clmm, and have two (potentially over-ambitious) questions. If Rune or anyone else can offer any insights about either, that would be much appreciated.
>
> First, I noticed something intriguing in the ordinal package documentation: the "## Binomial example with data from the lme4-package example", for clmm2. This suggests a way of shortening large datasets (with one Bernoulli trial per row) into shorter ones (with counts on each row, representing many trials), rather like "glmer(cbind(incidence, size - incidence)?" does for lme4. This obviously speeds up model fitting tremendously. However, I was wondering if there's any way to do this for outcomes with more than two levels (i.e., not just binomial, but multinomial)? This may not be possible or even make sense, but I thought I'd ask, given the example that was in the documentation.

It makes very good sense, but different computational strategies make
different specifications more natural.

In a binomial glm there are basically three ways to specify the data:
1) binary trials with one row for each trial, 2) binary trials with
with a row for each unique covariate setting and a weight indicating
the number of times this trial/setting was observed, and, 3) as a
two-column matrix with the number of successes and failures in each
row for each covariate setting.

All three structures extend to the ordered multinomial situation, but
clm and clmm only work with the first 2 due to the way the internal
computations are carried out. As briefly mentioned in this vignette
(http://www.cran.r-project.org/web/packages/ordinal/vignettes/clm_intro.pdf)
on page 10-11, setting up the data as in 2) is much more efficient
than in 1). Other packages like VGAM work with the matrix
representation of the multinomial response as in 3), and uses an
iterated weighted least squares estimation scheme somewhat different
from the Newton-Raphson scheme employed in clm and clmm.

>
> Second, I often use "simulate" with fitted mer objects (from lme4), to get confidence intervals for quantities of interest. (Using "refit" and "simulate" together is fast.) Is there any similar way to simulate and refit fitted clmm objects?

Not currently, though that may change at some point.

Cheers,
Rune

>
> Many thanks,
> Malcolm
>
>
> Dr Malcolm Fairbrother
> School of Geographical Sciences
> University of Bristol
>

-- 
Rune H B Christensen, PhD
DTU Informatics, Section for Statistics
Technical University of Denmark, Build. 305, Room 122,
DK-2800 Kgs. Lyngby, Denmark
Phone: (+45) 45 25 33 63
Mobile: (+45) 30 26 45 54


From kevpar66 at gmail.com  Mon Aug 20 22:08:01 2012
From: kevpar66 at gmail.com (Kevin Parsons)
Date: Mon, 20 Aug 2012 21:08:01 +0100
Subject: [R-sig-ME] mixed effects nested repeated measures MANOVA
Message-ID: <CALKFrYkD=vZ+9bHE8pXaobESCC-GfzRBYgWs0ptJRMCq9n=6=Q@mail.gmail.com>

I have what I think is a fairly complex problem that I hope can be
resolved. I've come close but not quite complete using the 'car'
package but now I've turned my attention to lmer. My experiment
involves looking at 6 traits that were measured twice for time, in two
ecomorphs, under two experimental treatments, with four different
families nested within ecomorphs (2 families per ecomorph). There are
363 individuals

I have come up with the following model for use with lmer:
model2 <- lmer(Y3 ~time*ecomorph*diet*family/ecomorph+(1|individual),
data=data1)

I obtain the following:

Linear mixed model fit by REML
Formula: Y3 ~ time * ecomorph * diet * family/ecomorph + (1 | individual)
   Data: data1
   AIC   BIC logLik deviance REMLdev
 -1207 -1124  621.5    -1349   -1243
Random effects:
 Groups   Name        Variance  Std.Dev.
 fish     (Intercept) 0.0012532 0.035401
 Residual             0.0081641 0.090355
Number of obs: 726, groups: individuals, 363

Fixed effects:
                          Estimate Std. Error t value
(Intercept)               -0.67115    0.49360  -1.360
time                       1.06044    0.30750   3.449
ecomorph                   1.18842    0.42141   2.820
diet                       0.63752    0.30018   2.124
family                     0.75811    0.21545   3.519
time:ecomorph             -0.83778    0.26252  -3.191
time:diet                 -0.47492    0.18700  -2.540
ecomorph:diet             -0.60881    0.25211  -2.415
time:family               -0.48472    0.13422  -3.611
ecomorph:family           -0.49755    0.14252  -3.491
diet:family               -0.31653    0.13614  -2.325
time:ecomorph:diet         0.39737    0.15705   2.530
time:ecomorph:family       0.31555    0.08878   3.554
time:diet:family           0.24273    0.08481   2.862
ecomorph:diet:family       0.23655    0.08813   2.684
time:ecomorph:diet:family -0.16362    0.05490  -2.980


While the model runs I'm not feeling comfortable with the results, I
have concerns about the proper F-ratios being used, and as I read more
elsewhere it seems lmer is not taking a wholly multivariate approach?

Someone else has suggested that I calculate coefficients from nested
models that can be found for each response (using either lm, lme,
lmer) . These coefficients can then be concatenated to form a matrix
of coefficients, from which multivariate test statistics can be found.
Could someone please explain how this can be carried out in more
practical terms


From m.fairbrother at bristol.ac.uk  Tue Aug 21 10:05:01 2012
From: m.fairbrother at bristol.ac.uk (Malcolm Fairbrother)
Date: Tue, 21 Aug 2012 09:05:01 +0100
Subject: [R-sig-ME] Another MCMCglmm question - fixing 	correlations
In-Reply-To: <3259EF79-BB81-4C7B-90D7-F51948B88022@bristol.ac.uk>
References: <mailman.7.1291201202.14469.r-sig-mixed-models@r-project.org>
	<65C3FF86-9E7A-449A-86E7-1096E3F480CD@bristol.ac.uk>
	<EEED6CE6-2D3D-480B-88E3-52A4C25797FA@ed.ac.uk>
	<7FC5B5A7-3E8D-43ED-B54F-7785257F3841@bristol.ac.uk>
	<20101204211150.50434f1dwuwqf7ok@www.staffmail.ed.ac.uk>
	<3259EF79-BB81-4C7B-90D7-F51948B88022@bristol.ac.uk>
Message-ID: <F430FC4F-13A6-4ED4-923D-74709B0E2711@bristol.ac.uk>

Dear Jarrod,

A while back we had a discussion on the list about spatial simultaneous autoregressive lag models, using the "sir" function in MCMCglmm. I just tried re-running the code in my message of 5 Dec 2010, which was working at the time, and it now gets hung up on the call to "sir", returning the error:

Error in model.frame.default(object, data, xlev = xlev) : 
  invalid type (S4) for variable 'sir(~W, ~units)'

I thought "sir(~W, ~diag(nrow(W)))" might help, but it doesn't.

And I get the same error if I try the code from pp. 132-3 in the MCMCglmm CourseNotes.

Did you tweak something in MCMCglmm (I'm now running version 2.16) which might have caused this problem? Any assistance appreciated.

Cheers,
Malcolm




On 5 Dec 2010, at 16:16, Malcolm Fairbrother wrote:

> Hi Jarrod,
> 
> Thanks very much for the suggestions, which did the trick. (And standardising the row weights to 1 seemed to prevent any trouble with any values of rho.)
> 
> MCMCglmm does appear to fit spatial simultaneous autoregressive lag models, and about as well as the specialised functions for such models in the "spdep" package. I've tried a few values of rho, and MCMCglmm recovers them well (the estimates from both it and "lagsarlm"  are slightly downward biased--see below for simulations code).
> 
> My real/ulterior interest here is actually estimating multilevel models where diffusion takes place across higher-level units, and some preliminary investigations suggest that MCMCglmm can fit such a model--unlike any other R function I've tried--but I'll spare the list the gory details of this work-in-progress, unless somebody wants to know.
> 
> Cheers,
> Malcolm
> 
> 
> 
> library(MCMCglmm)
> library(spdep)
> N <- 50 # set sample size
> rho <- 0.6 # set autocorrelation coefficient
> sims <- 100
> prior1 = list(R=list(V=1, nu=0.002))
> MSE <- res <- matrix(NA, nrow=sims, ncol=6)
> for (i in 1:sims) {
> 	W <- matrix(rbinom(N^2, 1, 0.25), N, N)*upper.tri(matrix(1, N, N))
> 	W <- W + t(W) # symmetrical connectivity matrix
> 	W <- W/apply(W, 1, sum) # standardise rows
> 	listw <- mat2listw(W) # convert contiguities to listw format
> 	L <- diag(N) - rho*W
> 	X1 <- runif(N, min=-5, max=5)
> 	Xbe <- X1+rnorm(N)
> 	y <- solve(L, Xbe) # generate lagged y
> 	dat <- data.frame(X1=X1, y=y, W=W)
> 	autoreg <- lagsarlm(y ~ X1, listw=listw, data=dat) # fit autoregressive model
> 	MC2 <- MCMCglmm(y ~ X1+sir(~W, ~units), data=dat, prior=prior1, verbose=F)
> 	res[i,] <- c(coefficients(autoreg), mean(MC2$Lambda), apply(MC2$Sol, 2, mean))
> 	MSE[i,] <- c((coefficients(autoreg)-c(rho, 0, 1))^2, (mean(MC2$Lambda)-rho)^2, (apply(MC2$Sol, 2, mean) - c(0, 1))^2)
> 	}
> 
> 
> 
> On 4 Dec 2010, at 21:11, Jarrod Hadfield wrote:
> 
>> Hi Malcolm,
>> 
>> I meant replace the sir function in the R directory and rebuild + reinstall MCMCglmm. However, I later realised that for your problem there is a simpler way:
>> 
>> N <- 50 # set sample size
>> W <- matrix(rbinom(N^2, 1, 0.25), N, N)*upper.tri(matrix(1, N, N))
>> W <- W + t(W) # symmetrical connectivity matrix
>> rho <- 0.2 # set autocorrelation coefficient
>> L <- diag(N) - rho*W  # my L is your M^{-1}
>> X1 <- runif(N, min=-5, max=5)
>> Xbe <- X1+rnorm(N)
>> y <- solve(L, Xbe) # generate lagged y
>> dat <- data.frame(X1=X1, y=y, W=W)
>> 
>> prior1 = list(R=list(V=1, nu=0.002))
>> MC2 <- MCMCglmm(y ~ X1+sir(~W, ~units), data=dat, prior=prior1, verbose=F)
>> 
>> This works because ~units sets up an identity matrix and W%*%t(I) = W.
>> 
>> However, I could not get sensible results from MCMCglmm with rho=0.3 and wasn't sure why (rho=0.03 for example gives sense). The problem is associated with a change in the sign of the determinant of L (or M) (i.e the Jacobian), which is perhaps not that surprising since if W=I then rho=0.3 and rho = 1.7 should be indiscriminable even with a fixed residual variance:
>> 
>> y = Xbe/(1-0.3) = -Xbe/(1-1.7)
>> 
>> I would have to think about this harder than I have to offer a solution (this is why the sir models are undocumented!), but perhaps you have some insight?
>> 
>> Also, I don't think MC3 results are meaningless, the estimate for the sir parameter overlaps zero as expected.
>> 
>> Cheers,
>> 
>> Jarrod
>> 
>> 
>> 
>> Quoting Malcolm Fairbrother <m.fairbrother at bristol.ac.uk>:
>> 
>>> Hi Jarrod,
>>> 
>>> Thanks very much for the suggestion. However, in trying what you said, I'm not getting very far (see code below). I redefine the function "sir", and then re-install the MCMCglmm package. But the MCMCglmm function still seems to use the old "sir" function, not the new one, with the result that when I try calling sir in the middle of a call to MCMCglmm, I get an error message telling me that my call to sir is problematic ("Error in sir(W) : formula not passed to formula1 in sir"). Can you please clarify what I'm doing wrong?
>>> 
>>> Thanks again for any assistance.
>>> - Malcolm
>>> 
>>> 
>>> sir <- function(W) {W}
>>> install.packages("MCMCglmm")
>>> library(MCMCglmm)
>>> 
>>> N <- 50 # set sample size
>>> W <- matrix(rbinom(N^2, 1, 0.25), N, N)*upper.tri(matrix(1, N, N))
>>> W <- W + t(W) # symmetrical connectivity matrix
>>> rho <- 0.2 # set autocorrelation coefficient
>>> M <- solve(diag(N) - rho*W)
>>> X1 <- runif(N, min=-5, max=5)
>>> Xbe <- X1+rnorm(N)
>>> y <- M %*% Xbe # generate lagged y
>>> dat <- data.frame(X1=X1, y=y)
>>> 
>>> prior1 = list(R=list(V=1, nu=0.002), G=list(G1=list(V=1, nu=0.002)))
>>> MC1 <- MCMCglmm(y ~ X1, data=dat, prior=prior1, verbose=F)
>>> # works OK, compare with summary(lm(dat$y~dat$X1))
>>> 
>>> identical(sir(W), W) # TRUE
>>> MC2 <- MCMCglmm(y ~ X1 + sir(W), data=dat, prior=prior1, verbose=F)
>>> # doesn't work
>>> 
>>> fac2 <- fac1 < -factor(sample(letters, N, TRUE), levels=letters)
>>> MC3 <- MCMCglmm(y ~ X1 + sir(~fac1, ~fac2), data=dat, prior=prior1, verbose=F)
>>> # works--results are meaningless, but doesn't return any error
>>> 
>>> 
>>> 
>>> 
>>> 
>>> On 2 Dec 2010, at 11:20, Jarrod Hadfield wrote:
>>> 
>>>> Hi Malcolm,
>>>> 
>>>> The suggestion of using SIR models for time series was a throw away remark, but I can't see anything technically incorrect with it. For example the model y[t] = \lambda*y[t-1]+..... can be treated as a SIR model, although there are certainly better ways of fitting it. I'm not familiar with spatial models and connectivity matrices but if W[i,j] is a way of specifying y[i] = \lambda*y[j] + ... then it is possible to fit it as a SIR model. You could modify the sir code to do what you would like (and then reinstall MCMCglmm) , for example:
>>>> 
>>>> sir<-function(W){W}
>>>> 
>>>> and then place W in the data.frame.
>>>> 
>>>> MCMCglmm will associate design matrices with recursive/simultaneous structures if the design matrix is returned from a function called "sir".
>>>> 
>>>> Cheers,
>>>> 
>>>> Jarrod
>>>> 
>>>> 
>>>> 
>>>> 
>>>> 
>>>> 
>>>> 
>>>> 
>>>> 
>>>> On 1 Dec 2010, at 20:10, Malcolm Fairbrother wrote:
>>>> 
>>>>> Dear Jarrod,
>>>>> 
>>>>> I'm very interested in this SIR feature of MCMCglmm, which I hadn't been aware of until your e-mail earlier today. Do I understand correctly that this should allow for feedback effects between units, reflecting the fact that y(i) and y(not-i) affect each other? If so, this could be a useful way of fitting, for example, spatial autoregressive models, where some units may affect each other by virtue of being neighbours, and we would like to know the magnitude of these effects. (You mentioned time series, but some web searches haven't turned up much on MCMCglmm and time series. And time only flows in one direction, whereas units in space can have two-way effects.)
>>>>> 
>>>>> However, in looking at the documentation and code, I've been struggling a bit with the "sir(~XX, ~XX)" part. Is there a way to specify a "connectivity" matrix directly, rather than using the "sir" function? Looking at the MCMCglmm code, as I understand it, "sir" is both a way of generating a connectivity matrix AND a way of telling MCMCglmm to treat the sir(~XX, ~XX) differently than other covariates.
>>>>> 
>>>>> In the case of spatial autoregressive models, each element on the main diagonal of the connectivity matrix is 0, and rows are often standardised to sum to 1. Such a connectivity matrix might, for example, look like:
>>>>> 
>>>>> N <- 100
>>>>> W <- matrix(rbinom(N^2, 1, 0.1), N, N)*upper.tri(matrix(1, N, N)) # chance of being neighbours is 0.1
>>>>> W <- W + t(W) # matrix is symmetrical, with zeros on the main diagonal
>>>>> 
>>>>> But I'm having a hard time figuring out how to use "sir" to specify such a matrix in the middle of a call to MCMCglmm.
>>>>> 
>>>>> If you see what I mean, can you offer any suggestions? I might take a stab at modifying MCMCglmm to make this possible, but before I do that (and I'm not certain I'll have the know-how) I wanted to check whether you had any ideas.
>>>>> 
>>>>> Many thanks for any assistance or clarification you can provide.
>>>>> - Malcolm
>>>>> 
>>>>> 
>>>>> 
>>>>>> Message: 5
>>>>>> Date: Wed, 1 Dec 2010 10:28:43 +0000
>>>>>> From: Jarrod Hadfield <j.hadfield at ed.ac.uk>
>>>>>> To: Szymek Drobniak <geralttee at gmail.com>
>>>>>> Cc: r-sig-mixed-models at r-project.org
>>>>>> Subject: Re: [R-sig-ME] Another MCMCglmm question - fixing
>>>>>> 	correlations
>>>>>> Message-ID: <0AF131D6-557C-4324-A65A-A175E41DBA8F at ed.ac.uk>
>>>>>> Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes
>>>>>> 
>>>>>> Hi,
>>>>>> 
>>>>>> There is a way, but its quite involved and you may find it easier to
>>>>>> fit this type of model in something like ASReml if you can get a
>>>>>> license. If not ......
>>>>>> 
>>>>>> I have implemented simultaneous-recursive (SIR) mixed models  in
>>>>>> MCMCglmm, although currently they are not well tested or documented
>>>>>> (See Chapter 9 of the CourseNotes) and cannot be used with non-
>>>>>> Gaussian data or certain patterns of missing data. Nevertheless, in
>>>>>> this example the existing code should work fine.  A simple SIR model
>>>>>> has the form:
>>>>>> 
>>>>>> y_{i} = mu + \lambda*y_{j} + u_{i} + e_{i}
>>>>>> 
>>>>>> where mu is the intercept, u is a random effect (e.g. breeding value
>>>>>> in your case) and e is a residual. The new part of the model is the
>>>>>> structural parameter \lambda multiplied by y_{j}, the jth element of
>>>>>> the response vector, which can be useful for modelling the effects of
>>>>>> behavioural interactions or time series etc.  However, placing y's on
>>>>>> the LHS and RHS complicates the likelihood, but MCMCglmm deals with
>>>>>> this.
>>>>>> 
>>>>>> If we set i=j then the above equation can be rearranged:
>>>>>> 
>>>>>> y_{i}  - \lambda*y_{i} = mu + u_{i} + e_{i}
>>>>>> y_{i}(1  - \lambda) = mu + u_{i} + e_{i}
>>>>>> y_{i} = (mu + u_{i} + e_{i})/(1  - \lambda)
>>>>>> 
>>>>>> from this it can be seen that mu, u, e, and \lambda cannot be uniquely
>>>>>> estimated without restrictions.  To illustrate we'll simulate some data:
>>>>>> 
>>>>>> fac<-gl(25,4)
>>>>>> y<-rnorm(25)[fac]+rnorm(100, -1, sqrt(2))
>>>>>> 
>>>>>> # mu = -1, VAR(u)=1, VAR(e) = 2, \lambda = 0
>>>>>> 
>>>>>> dat<-data.frame(y=y, fac=fac)
>>>>>> 
>>>>>> prior1 = list(R=list(V=1, nu=0.002), G=list(G1=list(V=1, nu=0.002)))
>>>>>> 
>>>>>> m1<-MCMCglmm(y~1, random=~fac,  data=dat, prior=prior1)
>>>>>> 
>>>>>> # fitting a non-SIR model gives estimates consistent with the true
>>>>>> values, which you can verify through summary(m1). Now for the SIR
>>>>>> model. Because we know all parameters cannot be uniquely estimated
>>>>>> I've set the residual variance to one arbitrarily.
>>>>>> 
>>>>>> prior2 = list(R=list(V=1, fix=1), G=list(G1=list(V=1, nu=0.002)))
>>>>>> 
>>>>>> m2<-MCMCglmm(y~1+sir(~units, ~units), random=~fac,  data=dat,
>>>>>> prior=prior2)   # sir(~units, ~units) sets i=j.
>>>>>> 
>>>>>> # The estimates do not look consistent with the real values.  However,
>>>>>> we can rescale them by 1-\lambda and the estimates  are close to being
>>>>>> identical up to Monte Carlo error (the prior will have slightly
>>>>>> different effects)
>>>>>> 
>>>>>> HPDinterval(m1$VCV[,1])
>>>>>> HPDinterval(m2$VCV[,1]/(1-m2$Lambda)^2)
>>>>>> 
>>>>>> HPDinterval(m1$VCV[,2])
>>>>>> HPDinterval(m2$VCV[,2]/(1-m2$Lambda)^2)
>>>>>> 
>>>>>> HPDinterval(m1$Sol)
>>>>>> HPDinterval(m2$Sol/(1-m2$Lambda))
>>>>> 
>>>>> 
>>>>> 	[[alternative HTML version deleted]]
>>>>> 
>>>>> _______________________________________________
>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>> 
>>>> 
>>>> 
>>>> --
>>>> The University of Edinburgh is a charitable body, registered in
>>>> Scotland, with registration number SC005336.
>>>> 
>>> 
>>> 
>>> 
>> 
>> 
>> 
>> -- 
>> The University of Edinburgh is a charitable body, registered in
>> Scotland, with registration number SC005336.
>> 
>> 
> 


From daniel at umd.edu  Tue Aug 21 22:23:43 2012
From: daniel at umd.edu (Daniel Malter)
Date: Tue, 21 Aug 2012 16:23:43 -0400
Subject: [R-sig-ME] Retrieve autocorrelation-corrected errors from gls
 (nlme) or gamm (mgcv)
Message-ID: <DF5B15061A61C2409088F43AE2C87EC6018C3F473B00@OITMXCMS02VI.AD.UMD.EDU>

Hi, 

In the example below, I am modeling the dependent variable Y as a function of X when the errors are autoregressive at the first lag.  There is a number of ways/functions with which to model this. arima (tseries), gls (nlme), and gamm should produce similar results in the simulated example below, and they do. 

However, I need the residuals from this analysis and both gls and gamm seem to return the errors before correction for autocorrelation, whereas arima returns the corrected errors (i.e., the estimate of the innovation). My question is whether there is an easy way to retrieve the corrected errors from gamm or gls. My real data are panel data with AR3 errors for which I model nonlinear effects of the independent variables on the dependent variable. Hand-computing the appropriate errors would be painful. 

#simulate data with AR1 errors 

set.seed(394857395) 
e<-rnorm(101) 
e<-e[2:101]+0.5*e[1:100] 
x<-rnorm(100) 
y<-x+e 

#OLS for comparison 
reg<-lm(y~x) 
summary(reg) 
pacf(residuals(reg)) 

#arima 
require(tseries) 
reg1<-arima(y,order=c(1,0,0),xreg=x) 
reg1 

#gls 
require(nlme) 
reg2<-gls(y~x,correlation=corAR1()) 
summary(reg2) 

#gamm 
require(mgcv) 
reg3<-gamm(y~s(x),correlation=corAR1()) 
summary(reg3$lme) 

par(mfcol=c(1,3)) 
pacf(residuals(reg1),main="ARIMA",lag.max=10) 
pacf(residuals(reg2),main="GLS",lag.max=10) 
pacf(residuals(reg3$lme),main="GAMM",lag.max=10) 


Thanks, 
Daniel 


From highstat at highstat.com  Wed Aug 22 12:22:25 2012
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Wed, 22 Aug 2012 11:22:25 +0100
Subject: [R-sig-ME] Retrieve autocorrelation-corrected errors from gls
 (nlme) or, gamm (mgcv) (Daniel Malter)
In-Reply-To: <mailman.3.1345629601.2139.r-sig-mixed-models@r-project.org>
References: <mailman.3.1345629601.2139.r-sig-mixed-models@r-project.org>
Message-ID: <5034B2E1.8080002@highstat.com>


> ----------------------------------------------------------------------
>
> Message: 1
> Date: Tue, 21 Aug 2012 16:23:43 -0400
> From: Daniel Malter <daniel at umd.edu>
> To: "r-sig-mixed-models at r-project.org"
> 	<r-sig-mixed-models at r-project.org>
> Subject: [R-sig-ME] Retrieve autocorrelation-corrected errors from gls
> 	(nlme) or gamm (mgcv)
> Message-ID:
> 	<DF5B15061A61C2409088F43AE2C87EC6018C3F473B00 at OITMXCMS02VI.AD.UMD.EDU>
> Content-Type: text/plain; charset="us-ascii"
>
> Hi,
>
> In the example below, I am modeling the dependent variable Y as a function of X when the errors are autoregressive at the first lag.  There is a number of ways/functions with which to model this. arima (tseries), gls (nlme), and gamm should produce similar results in the simulated example below, and they do.
>
> However, I need the residuals from this analysis and both gls and gamm seem to return the errors before correction for autocorrelation, whereas arima returns the corrected errors (i.e., the estimate of the innovation). My question is whether there is an easy way to retrieve the corrected errors from gamm or gls. My real data are panel data with AR3 errors for which I model nonlinear effects of the independent variables on the dependent variable. Hand-computing the appropriate errors would be painful.
>
> #simulate data with AR1 errors
>
> set.seed(394857395)
> e<-rnorm(101)
> e<-e[2:101]+0.5*e[1:100]
> x<-rnorm(100)
> y<-x+e
>
> #OLS for comparison
> reg<-lm(y~x)
> summary(reg)
> pacf(residuals(reg))


It is better to use rstandard instead of resid

> #arima
> require(tseries)
> reg1<-arima(y,order=c(1,0,0),xreg=x)
> reg1
>
> #gls
> require(nlme)
> reg2<-gls(y~x,correlation=corAR1())
> summary(reg2)
>
> #gamm
> require(mgcv)
> reg3<-gamm(y~s(x),correlation=corAR1())
> summary(reg3$lme)
>
> par(mfcol=c(1,3))
> pacf(residuals(reg1),main="ARIMA",lag.max=10)
> pacf(residuals(reg2),main="GLS",lag.max=10)
> pacf(residuals(reg3$lme),main="GAMM",lag.max=10)
>


Use:  resid(reg1, type = "n")


Alain

> Thanks,
> Daniel
>


-- 

Dr. Alain F. Zuur
First author of:

1. Analysing Ecological Data (2007).
Zuur, AF, Ieno, EN and Smith, GM. Springer. 680 p.
URL: www.springer.com/0-387-45967-7


2. Mixed effects models and extensions in ecology with R. (2009).
Zuur, AF, Ieno, EN, Walker, N, Saveliev, AA, and Smith, GM. Springer.
http://www.springer.com/life+sci/ecology/book/978-0-387-87457-9


3. A Beginner's Guide to R (2009).
Zuur, AF, Ieno, EN, Meesters, EHWG. Springer
http://www.springer.com/statistics/computational/book/978-0-387-93836-3


4. Zero Inflated Models and Generalized Linear Mixed Models with R. (2012) Zuur, Saveliev, Ieno.
http://www.highstat.com/book4.htm

Other books: http://www.highstat.com/books.htm


Statistical consultancy, courses, data analysis and software
Highland Statistics Ltd.
6 Laverock road
UK - AB41 6FN Newburgh
Tel: 0044 1358 788177
Email: highstat at highstat.com
URL: www.highstat.com
URL: www.brodgar.com


From r.m.krug at gmail.com  Wed Aug 22 16:55:26 2012
From: r.m.krug at gmail.com (Rainer M Krug)
Date: Wed, 22 Aug 2012 16:55:26 +0200
Subject: [R-sig-ME] [R] Question concerning anova()
In-Reply-To: <CACk-te1Ci4Fo1dEP7CU=S+URBN_faiBYCoz1fH0OVK1KzcY0zQ@mail.gmail.com>
References: <5034EB51.1090401@gmail.com>
	<CACk-te1Ci4Fo1dEP7CU=S+URBN_faiBYCoz1fH0OVK1KzcY0zQ@mail.gmail.com>
Message-ID: <5034F2DE.9090806@gmail.com>

On 22/08/12 16:36, Bert Gunter wrote:
> Models with different fixed effects estimated by REML cannot be
> compared by anova.

I have seen that much in "Modern Applied Statistics in S", and therefore have chosen the model = "ML"

>
> In future, please post questions on mixed effects models on the
> r-sig-mixed-effects mailing lists. You're likely to receive more
> informative replies there, too.

Thanks - wasn't aware of this sig - I'll send the reply there as well.

Thanks,

Rainer

>
> -- Bert
>
> On Wed, Aug 22, 2012 at 7:23 AM, Rainer M Krug <r.m.krug at gmail.com> wrote:
>> Hi
>>
>> I am comparing four different linear mixed effect models, derived from
>> updating the original one. To compare these, I want to use anova(). I
>> therefore do the following (not reproducible - just to illustration
>> purpose!):
>>
>> dat <- loadSPECIES(SPECIES)
>> subs <- expression(dead==FALSE & recTreat==FALSE)
>> feff <- noBefore~pHarv*year      # fixed effect in the model
>> reff <- ~year|plant              # random effect in the model, where year is
>> the
>> corr <- corAR1(form=~year|plant) # describing the within-group correlation
>> structure
>> #
>> dat.lme <- lme(
>>               fixed = feff,                           # fixed effect in the
>> model
>>               data  = dat,
>>               subset = eval(subs),
>>               method = "ML",
>>               random = reff,                          # random effect in the
>> model
>>               correlation = corr,
>>               na.action = na.omit
>>               )
>> dat.lme.r1 <- update(dat.lme, random=~1|plant)
>> dat.lme.f1 <- update(dat.lme, fixed=noBefore~year)
>> dat.lme.r1.f1 <- update(dat.lme.r1, fixed=noBefore~year)
>>
>>
>> The anova is as follow:
>>
>>> anova(dat.lme, dat.lme.r1, dat.lme.f1, dat.lme.r1.f1)
>>                Model df      AIC      BIC    logLik   Test      L.Ratio
>> p-value
>> dat.lme           1  9 1703.218 1733.719 -842.6089
>> dat.lme.r1        2  7 1699.218 1722.941 -842.6089 1 vs 2 1.019230e-07
>> 1
>> dat.lme.f1        3  7 1705.556 1729.279 -845.7779
>> dat.lme.r1.f1     4  5 1701.556 1718.501 -845.7779 3 vs 4 8.498318e-08
>> 1
>>
>> I have two questions:
>> 1) I am wondering why the "2 vs 3" does not give the Test values?
>> Is this because the two models are considered as "identical", which would be
>> strange, due to the different logLik values.
>>
>> 2) If I want to compare all models among each other - is there a "best" way?
>> I would be reluctant to do several ANOVA's, due to necessary corrections for
>> multple tests (although this should not be a problem here?)
>>
>> I can obviously select the best model based on the AIC.
>>
>> Thanks in advance,
>>
>> Rainer
>>
>> --
>> Rainer M. Krug, PhD (Conservation Ecology, SUN), MSc (Conservation Biology,
>> UCT), Dipl. Phys. (Germany)
>>
>> Centre of Excellence for Invasion Biology
>> Stellenbosch University
>> South Africa
>>
>> Tel :       +33 - (0)9 53 10 27 44
>> Cell:       +33 - (0)6 85 62 59 98
>> Fax :       +33 - (0)9 58 10 27 44
>>
>> Fax (D):    +49 - (0)3 21 21 25 22 44
>>
>> email:      Rainer at krugs.de
>>
>> Skype:      RMkrug
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
>


-- 
Rainer M. Krug, PhD (Conservation Ecology, SUN), MSc (Conservation Biology, UCT), Dipl. Phys. (Germany)

Centre of Excellence for Invasion Biology
Stellenbosch University
South Africa

Tel :       +33 - (0)9 53 10 27 44
Cell:       +33 - (0)6 85 62 59 98
Fax :       +33 - (0)9 58 10 27 44

Fax (D):    +49 - (0)3 21 21 25 22 44

email:      Rainer at krugs.de

Skype:      RMkrug


From r.m.krug at gmail.com  Wed Aug 22 17:56:11 2012
From: r.m.krug at gmail.com (Rainer M Krug)
Date: Wed, 22 Aug 2012 17:56:11 +0200
Subject: [R-sig-ME] [R] Question concerning anova()
In-Reply-To: <CAPdSD+75+ao-J9zPPqmyYM_DzywsHsrv=pqj=xgMW5mvQjmqkA@mail.gmail.com>
References: <5034EB51.1090401@gmail.com>
	<CACk-te1Ci4Fo1dEP7CU=S+URBN_faiBYCoz1fH0OVK1KzcY0zQ@mail.gmail.com>
	<5034F2DE.9090806@gmail.com>
	<CAPdSD+75+ao-J9zPPqmyYM_DzywsHsrv=pqj=xgMW5mvQjmqkA@mail.gmail.com>
Message-ID: <5035011B.6020406@gmail.com>

On 22/08/12 17:36, Alan Haynes wrote:
> Hi Rainer,
>
> 1) I *think* you dont get tests for your 2 vs 3 because your models arent nested which is a
> condition for using anova() in this way I think. I would suggest you write out your models rather
> than use update as you have been. The you'll know exactly whats nested and whats not.

OK - tried it out, and it makes sense.

>
> 2) Im not sure about a best way (you'll probably get different answers depending on who you ask). In
> any case, I believe its recommended to sort out your random effects before you start dealing with
> fixed effects. Then reduce your fixed effects if needs be.

That is what I was thinking about, effectively going along with the steps described in the Modern 
Applied Statistics book. But further suggestions are welcome.

Rainer

>
> HTH
>
> Alan
>
>
>
> --------------------------------------------------
> Email: aghaynes at gmail.com <mailto:aghaynes at gmail.com>
> Mobile: +41794385586
> Skype: aghaynes
>
>
> On 22 August 2012 16:55, Rainer M Krug <r.m.krug at gmail.com <mailto:r.m.krug at gmail.com>> wrote:
>
>     On 22/08/12 16:36, Bert Gunter wrote:
>
>         Models with different fixed effects estimated by REML cannot be
>         compared by anova.
>
>
>     I have seen that much in "Modern Applied Statistics in S", and therefore have chosen the model =
>     "ML"
>
>
>         In future, please post questions on mixed effects models on the
>         r-sig-mixed-effects mailing lists. You're likely to receive more
>         informative replies there, too.
>
>
>     Thanks - wasn't aware of this sig - I'll send the reply there as well.
>
>     Thanks,
>
>     Rainer
>
>
>         -- Bert
>
>         On Wed, Aug 22, 2012 at 7:23 AM, Rainer M Krug <r.m.krug at gmail.com
>         <mailto:r.m.krug at gmail.com>> wrote:
>
>             Hi
>
>             I am comparing four different linear mixed effect models, derived from
>             updating the original one. To compare these, I want to use anova(). I
>             therefore do the following (not reproducible - just to illustration
>             purpose!):
>
>             dat <- loadSPECIES(SPECIES)
>             subs <- expression(dead==FALSE & recTreat==FALSE)
>             feff <- noBefore~pHarv*year      # fixed effect in the model
>             reff <- ~year|plant              # random effect in the model, where year is
>             the
>             corr <- corAR1(form=~year|plant) # describing the within-group correlation
>             structure
>             #
>             dat.lme <- lme(
>                            fixed = feff,                           # fixed effect in the
>             model
>                            data  = dat,
>                            subset = eval(subs),
>                            method = "ML",
>                            random = reff,                          # random effect in the
>             model
>                            correlation = corr,
>                            na.action = na.omit
>                            )
>             dat.lme.r1 <- update(dat.lme, random=~1|plant)
>             dat.lme.f1 <- update(dat.lme, fixed=noBefore~year)
>             dat.lme.r1.f1 <- update(dat.lme.r1, fixed=noBefore~year)
>
>
>             The anova is as follow:
>
>                 anova(dat.lme, dat.lme.r1, dat.lme.f1, dat.lme.r1.f1)
>
>                             Model df      AIC      BIC    logLik   Test      L.Ratio
>             p-value
>             dat.lme           1  9 1703.218 1733.719 -842.6089
>             dat.lme.r1        2  7 1699.218 1722.941 -842.6089 1 vs 2 1.019230e-07
>             1
>             dat.lme.f1        3  7 1705.556 1729.279 -845.7779
>             dat.lme.r1.f1     4  5 1701.556 1718.501 -845.7779 3 vs 4 8.498318e-08
>             1
>
>             I have two questions:
>             1) I am wondering why the "2 vs 3" does not give the Test values?
>             Is this because the two models are considered as "identical", which would be
>             strange, due to the different logLik values.
>
>             2) If I want to compare all models among each other - is there a "best" way?
>             I would be reluctant to do several ANOVA's, due to necessary corrections for
>             multple tests (although this should not be a problem here?)
>
>             I can obviously select the best model based on the AIC.
>
>             Thanks in advance,
>
>             Rainer
>
>             --
>             Rainer M. Krug, PhD (Conservation Ecology, SUN), MSc (Conservation Biology,
>             UCT), Dipl. Phys. (Germany)
>
>             Centre of Excellence for Invasion Biology
>             Stellenbosch University
>             South Africa
>
>             Tel : +33 - (0)9 53 10 27 44 <tel:%2B33%20-%20%280%299%2053%2010%2027%2044>
>             Cell: +33 - (0)6 85 62 59 98 <tel:%2B33%20-%20%280%296%2085%2062%2059%2098>
>             Fax : +33 - (0)9 58 10 27 44 <tel:%2B33%20-%20%280%299%2058%2010%2027%2044>
>
>             Fax (D): +49 - (0)3 21 21 25 22 44 <tel:%2B49%20-%20%280%293%2021%2021%2025%2022%2044>
>
>             email: Rainer at krugs.de <mailto:Rainer at krugs.de>
>
>             Skype:      RMkrug
>
>             ________________________________________________
>             R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>             https://stat.ethz.ch/mailman/__listinfo/r-help
>             <https://stat.ethz.ch/mailman/listinfo/r-help>
>             PLEASE do read the posting guide http://www.R-project.org/__posting-guide.html
>             <http://www.R-project.org/posting-guide.html>
>             and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
>
>
>     --
>     Rainer M. Krug, PhD (Conservation Ecology, SUN), MSc (Conservation Biology, UCT), Dipl. Phys.
>     (Germany)
>
>     Centre of Excellence for Invasion Biology
>     Stellenbosch University
>     South Africa
>
>     Tel : +33 - (0)9 53 10 27 44 <tel:%2B33%20-%20%280%299%2053%2010%2027%2044>
>     Cell: +33 - (0)6 85 62 59 98 <tel:%2B33%20-%20%280%296%2085%2062%2059%2098>
>     Fax : +33 - (0)9 58 10 27 44 <tel:%2B33%20-%20%280%299%2058%2010%2027%2044>
>
>     Fax (D): +49 - (0)3 21 21 25 22 44 <tel:%2B49%20-%20%280%293%2021%2021%2025%2022%2044>
>
>     email: Rainer at krugs.de <mailto:Rainer at krugs.de>
>
>     Skype:      RMkrug
>
>     _________________________________________________
>     R-sig-mixed-models at r-project.__org <mailto:R-sig-mixed-models at r-project.org> mailing list
>     https://stat.ethz.ch/mailman/__listinfo/r-sig-mixed-models
>     <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
>
>


-- 
Rainer M. Krug, PhD (Conservation Ecology, SUN), MSc (Conservation Biology, UCT), Dipl. Phys. (Germany)

Centre of Excellence for Invasion Biology
Stellenbosch University
South Africa

Tel :       +33 - (0)9 53 10 27 44
Cell:       +33 - (0)6 85 62 59 98
Fax :       +33 - (0)9 58 10 27 44

Fax (D):    +49 - (0)3 21 21 25 22 44

email:      Rainer at krugs.de

Skype:      RMkrug


From r.m.krug at gmail.com  Wed Aug 22 18:04:55 2012
From: r.m.krug at gmail.com (Rainer M Krug)
Date: Wed, 22 Aug 2012 18:04:55 +0200
Subject: [R-sig-ME] Question concerning anova()
In-Reply-To: <CACk-te0PTUc5o6oT1Ox7SNw_mRGMmJXEDiuYar2Dr9oFjy4HrQ@mail.gmail.com>
References: <5034EB51.1090401@gmail.com>
	<CACk-te1Ci4Fo1dEP7CU=S+URBN_faiBYCoz1fH0OVK1KzcY0zQ@mail.gmail.com>
	<5034F2DE.9090806@gmail.com>
	<CACk-te0PTUc5o6oT1Ox7SNw_mRGMmJXEDiuYar2Dr9oFjy4HrQ@mail.gmail.com>
Message-ID: <50350327.5000006@gmail.com>

Further discussed on r-sig-mixed-models

Rainer

On 22/08/12 17:04, Bert Gunter wrote:
> Oops -- missed that. OTOH, my reply demonstrates the value of the
> mixed models list recommendation.
>
> -- Bert
>
> On Wed, Aug 22, 2012 at 7:55 AM, Rainer M Krug <r.m.krug at gmail.com> wrote:
>> On 22/08/12 16:36, Bert Gunter wrote:
>>>
>>> Models with different fixed effects estimated by REML cannot be
>>> compared by anova.
>>
>>
>> I have seen that much in "Modern Applied Statistics in S", and therefore
>> have chosen the model = "ML"
>>
>>>
>>> In future, please post questions on mixed effects models on the
>>> r-sig-mixed-effects mailing lists. You're likely to receive more
>>> informative replies there, too.
>>
>>
>> Thanks - wasn't aware of this sig - I'll send the reply there as well.
>>
>> Thanks,
>>
>> Rainer
>>
>>>
>>> -- Bert
>>>
>>> On Wed, Aug 22, 2012 at 7:23 AM, Rainer M Krug <r.m.krug at gmail.com> wrote:
>>>>
>>>> Hi
>>>>
>>>> I am comparing four different linear mixed effect models, derived from
>>>> updating the original one. To compare these, I want to use anova(). I
>>>> therefore do the following (not reproducible - just to illustration
>>>> purpose!):
>>>>
>>>> dat <- loadSPECIES(SPECIES)
>>>> subs <- expression(dead==FALSE & recTreat==FALSE)
>>>> feff <- noBefore~pHarv*year      # fixed effect in the model
>>>> reff <- ~year|plant              # random effect in the model, where year
>>>> is
>>>> the
>>>> corr <- corAR1(form=~year|plant) # describing the within-group
>>>> correlation
>>>> structure
>>>> #
>>>> dat.lme <- lme(
>>>>                fixed = feff,                           # fixed effect in
>>>> the
>>>> model
>>>>                data  = dat,
>>>>                subset = eval(subs),
>>>>                method = "ML",
>>>>                random = reff,                          # random effect in
>>>> the
>>>> model
>>>>                correlation = corr,
>>>>                na.action = na.omit
>>>>                )
>>>> dat.lme.r1 <- update(dat.lme, random=~1|plant)
>>>> dat.lme.f1 <- update(dat.lme, fixed=noBefore~year)
>>>> dat.lme.r1.f1 <- update(dat.lme.r1, fixed=noBefore~year)
>>>>
>>>>
>>>> The anova is as follow:
>>>>
>>>>> anova(dat.lme, dat.lme.r1, dat.lme.f1, dat.lme.r1.f1)
>>>>
>>>>                 Model df      AIC      BIC    logLik   Test      L.Ratio
>>>> p-value
>>>> dat.lme           1  9 1703.218 1733.719 -842.6089
>>>> dat.lme.r1        2  7 1699.218 1722.941 -842.6089 1 vs 2 1.019230e-07
>>>> 1
>>>> dat.lme.f1        3  7 1705.556 1729.279 -845.7779
>>>> dat.lme.r1.f1     4  5 1701.556 1718.501 -845.7779 3 vs 4 8.498318e-08
>>>> 1
>>>>
>>>> I have two questions:
>>>> 1) I am wondering why the "2 vs 3" does not give the Test values?
>>>> Is this because the two models are considered as "identical", which would
>>>> be
>>>> strange, due to the different logLik values.
>>>>
>>>> 2) If I want to compare all models among each other - is there a "best"
>>>> way?
>>>> I would be reluctant to do several ANOVA's, due to necessary corrections
>>>> for
>>>> multple tests (although this should not be a problem here?)
>>>>
>>>> I can obviously select the best model based on the AIC.
>>>>
>>>> Thanks in advance,
>>>>
>>>> Rainer
>>>>
>>>> --
>>>> Rainer M. Krug, PhD (Conservation Ecology, SUN), MSc (Conservation
>>>> Biology,
>>>> UCT), Dipl. Phys. (Germany)
>>>>
>>>> Centre of Excellence for Invasion Biology
>>>> Stellenbosch University
>>>> South Africa
>>>>
>>>> Tel :       +33 - (0)9 53 10 27 44
>>>> Cell:       +33 - (0)6 85 62 59 98
>>>> Fax :       +33 - (0)9 58 10 27 44
>>>>
>>>> Fax (D):    +49 - (0)3 21 21 25 22 44
>>>>
>>>> email:      Rainer at krugs.de
>>>>
>>>> Skype:      RMkrug
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>>
>>>
>>
>>
>> --
>> Rainer M. Krug, PhD (Conservation Ecology, SUN), MSc (Conservation Biology,
>> UCT), Dipl. Phys. (Germany)
>>
>> Centre of Excellence for Invasion Biology
>> Stellenbosch University
>> South Africa
>>
>> Tel :       +33 - (0)9 53 10 27 44
>> Cell:       +33 - (0)6 85 62 59 98
>> Fax :       +33 - (0)9 58 10 27 44
>>
>> Fax (D):    +49 - (0)3 21 21 25 22 44
>>
>> email:      Rainer at krugs.de
>>
>> Skype:      RMkrug
>
>
>


From jimmycloud at gmail.com  Wed Aug 22 19:32:12 2012
From: jimmycloud at gmail.com (Jie)
Date: Wed, 22 Aug 2012 13:32:12 -0400
Subject: [R-sig-ME] use lme4 if given effects' design matrix
Message-ID: <CACXG3GjsghQEcxqZ+y1hFDC9HUaU8sY+5m0wbVVa-GBH6Z9q_g@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120822/953f7a9d/attachment.pl>

From gunter.berton at gene.com  Wed Aug 22 17:04:00 2012
From: gunter.berton at gene.com (Bert Gunter)
Date: Wed, 22 Aug 2012 08:04:00 -0700
Subject: [R-sig-ME] [R] Question concerning anova()
In-Reply-To: <5034F2DE.9090806@gmail.com>
References: <5034EB51.1090401@gmail.com>
	<CACk-te1Ci4Fo1dEP7CU=S+URBN_faiBYCoz1fH0OVK1KzcY0zQ@mail.gmail.com>
	<5034F2DE.9090806@gmail.com>
Message-ID: <CACk-te0PTUc5o6oT1Ox7SNw_mRGMmJXEDiuYar2Dr9oFjy4HrQ@mail.gmail.com>

Oops -- missed that. OTOH, my reply demonstrates the value of the
mixed models list recommendation.

-- Bert

On Wed, Aug 22, 2012 at 7:55 AM, Rainer M Krug <r.m.krug at gmail.com> wrote:
> On 22/08/12 16:36, Bert Gunter wrote:
>>
>> Models with different fixed effects estimated by REML cannot be
>> compared by anova.
>
>
> I have seen that much in "Modern Applied Statistics in S", and therefore
> have chosen the model = "ML"
>
>>
>> In future, please post questions on mixed effects models on the
>> r-sig-mixed-effects mailing lists. You're likely to receive more
>> informative replies there, too.
>
>
> Thanks - wasn't aware of this sig - I'll send the reply there as well.
>
> Thanks,
>
> Rainer
>
>>
>> -- Bert
>>
>> On Wed, Aug 22, 2012 at 7:23 AM, Rainer M Krug <r.m.krug at gmail.com> wrote:
>>>
>>> Hi
>>>
>>> I am comparing four different linear mixed effect models, derived from
>>> updating the original one. To compare these, I want to use anova(). I
>>> therefore do the following (not reproducible - just to illustration
>>> purpose!):
>>>
>>> dat <- loadSPECIES(SPECIES)
>>> subs <- expression(dead==FALSE & recTreat==FALSE)
>>> feff <- noBefore~pHarv*year      # fixed effect in the model
>>> reff <- ~year|plant              # random effect in the model, where year
>>> is
>>> the
>>> corr <- corAR1(form=~year|plant) # describing the within-group
>>> correlation
>>> structure
>>> #
>>> dat.lme <- lme(
>>>               fixed = feff,                           # fixed effect in
>>> the
>>> model
>>>               data  = dat,
>>>               subset = eval(subs),
>>>               method = "ML",
>>>               random = reff,                          # random effect in
>>> the
>>> model
>>>               correlation = corr,
>>>               na.action = na.omit
>>>               )
>>> dat.lme.r1 <- update(dat.lme, random=~1|plant)
>>> dat.lme.f1 <- update(dat.lme, fixed=noBefore~year)
>>> dat.lme.r1.f1 <- update(dat.lme.r1, fixed=noBefore~year)
>>>
>>>
>>> The anova is as follow:
>>>
>>>> anova(dat.lme, dat.lme.r1, dat.lme.f1, dat.lme.r1.f1)
>>>
>>>                Model df      AIC      BIC    logLik   Test      L.Ratio
>>> p-value
>>> dat.lme           1  9 1703.218 1733.719 -842.6089
>>> dat.lme.r1        2  7 1699.218 1722.941 -842.6089 1 vs 2 1.019230e-07
>>> 1
>>> dat.lme.f1        3  7 1705.556 1729.279 -845.7779
>>> dat.lme.r1.f1     4  5 1701.556 1718.501 -845.7779 3 vs 4 8.498318e-08
>>> 1
>>>
>>> I have two questions:
>>> 1) I am wondering why the "2 vs 3" does not give the Test values?
>>> Is this because the two models are considered as "identical", which would
>>> be
>>> strange, due to the different logLik values.
>>>
>>> 2) If I want to compare all models among each other - is there a "best"
>>> way?
>>> I would be reluctant to do several ANOVA's, due to necessary corrections
>>> for
>>> multple tests (although this should not be a problem here?)
>>>
>>> I can obviously select the best model based on the AIC.
>>>
>>> Thanks in advance,
>>>
>>> Rainer
>>>
>>> --
>>> Rainer M. Krug, PhD (Conservation Ecology, SUN), MSc (Conservation
>>> Biology,
>>> UCT), Dipl. Phys. (Germany)
>>>
>>> Centre of Excellence for Invasion Biology
>>> Stellenbosch University
>>> South Africa
>>>
>>> Tel :       +33 - (0)9 53 10 27 44
>>> Cell:       +33 - (0)6 85 62 59 98
>>> Fax :       +33 - (0)9 58 10 27 44
>>>
>>> Fax (D):    +49 - (0)3 21 21 25 22 44
>>>
>>> email:      Rainer at krugs.de
>>>
>>> Skype:      RMkrug
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>>
>
>
> --
> Rainer M. Krug, PhD (Conservation Ecology, SUN), MSc (Conservation Biology,
> UCT), Dipl. Phys. (Germany)
>
> Centre of Excellence for Invasion Biology
> Stellenbosch University
> South Africa
>
> Tel :       +33 - (0)9 53 10 27 44
> Cell:       +33 - (0)6 85 62 59 98
> Fax :       +33 - (0)9 58 10 27 44
>
> Fax (D):    +49 - (0)3 21 21 25 22 44
>
> email:      Rainer at krugs.de
>
> Skype:      RMkrug



-- 

Bert Gunter
Genentech Nonclinical Biostatistics

Internal Contact Info:
Phone: 467-7374
Website:
http://pharmadevelopment.roche.com/index/pdb/pdb-functional-groups/pdb-biostatistics/pdb-ncb-home.htm


From henrik.singmann at psychologie.uni-freiburg.de  Wed Aug 22 20:22:40 2012
From: henrik.singmann at psychologie.uni-freiburg.de (Henrik Singmann)
Date: Wed, 22 Aug 2012 20:22:40 +0200
Subject: [R-sig-ME] How to obtain Type 2 like p-values for effects?
Message-ID: <50352370.6020403@psychologie.uni-freiburg.de>

Dear all,

I am currently trying to add the possibility to obtain Type 2 like p-values for all effects in a mixed model to function mixed() in package afex (using pbkrtest::KRmodcomp see: https://stat.ethz.ch/pipermail/r-sig-mixed-models/2012q3/018946.html) but am unsure on how to implement this correctly.

I came up with two possible solutions which both seem to have problems.

(1) Fit one full model and contrast a model for each effect with this full model. Each submodel is created by subtracting the effect of interest and all higher order effects from the model (e.g., when interested in a main effect, the submodel contains all main effects but the one of interest and no interactions).

(2) Fit a full model for each order of effects (i.e., one full model for all main effects, one full model for all two-way interactions, ...) and contrast a model for each effect with the full model of the corresponding order (e.g., when interested in a main effect, the submodel contains all main effects but the one of interest and no interactions and the full model contains all main effects and no interactions).

Both solutions seem to have problems.
The first solution seems rather dubious, why would you want to compare two models to test for a single effect when in fact more than just this effect is missing, namely all the higher order effects. Consequently, this solution does produces small p-values and many significant effects. For example, comparing a full model with many interactions with a model for the main effects in which only those are present almost inevitably needs to be significant.
The second solution seems problematic from another perspective, as it simply ignores the higher order effects. This contrasts with classical ANOVA in which even the lower order effects are tested against the MSE of the full model. Consequently, using the second approach the lower order effects of a model with interactions are identical to a model fitted without those interactions.

Any ideas would be really appreciated.

(Sidenote: I obtain type 3 tests by contrasting the full model with a model in which only the relevant effect is missing, ignoring the order of the effects)

Cheers,
Henrik

-- 
Dipl. Psych. Henrik Singmann
PhD Student
Albert-Ludwigs-Universit?t Freiburg, Germany
http://www.psychologie.uni-freiburg.de/Members/singmann


From jfox at mcmaster.ca  Wed Aug 22 22:10:25 2012
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 22 Aug 2012 16:10:25 -0400
Subject: [R-sig-ME] How to obtain Type 2 like p-values for effects?
In-Reply-To: <50352370.6020403@psychologie.uni-freiburg.de>
References: <50352370.6020403@psychologie.uni-freiburg.de>
Message-ID: <web-420216476@cgpsrv2.cis.mcmaster.ca>

Dear Henrik,

The Anova() function in the car package already does this for Wald tests of the fixed effects in a mixed model, using pbkrtest to compute df (in the development version of the car package on R-Forge). We define the "type-II" test as the maximally powerful Wald test for the hypothesis in question obeying marginality.

I hope this helps,
 John

------------------------------------------------
John Fox
Sen. William McMaster Prof. of Social Statistics
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/
	
On Wed, 22 Aug 2012 20:22:40 +0200
 Henrik Singmann <henrik.singmann at psychologie.uni-freiburg.de> wrote:
> Dear all,
> 
> I am currently trying to add the possibility to obtain Type 2 like p-values for all effects in a mixed model to function mixed() in package afex (using pbkrtest::KRmodcomp see: https://stat.ethz.ch/pipermail/r-sig-mixed-models/2012q3/018946.html) but am unsure on how to implement this correctly.
> 
> I came up with two possible solutions which both seem to have problems.
> 
> (1) Fit one full model and contrast a model for each effect with this full model. Each submodel is created by subtracting the effect of interest and all higher order effects from the model (e.g., when interested in a main effect, the submodel contains all main effects but the one of interest and no interactions).
> 
> (2) Fit a full model for each order of effects (i.e., one full model for all main effects, one full model for all two-way interactions, ...) and contrast a model for each effect with the full model of the corresponding order (e.g., when interested in a main effect, the submodel contains all main effects but the one of interest and no interactions and the full model contains all main effects and no interactions).
> 
> Both solutions seem to have problems.
> The first solution seems rather dubious, why would you want to compare two models to test for a single effect when in fact more than just this effect is missing, namely all the higher order effects. Consequently, this solution does produces small p-values and many significant effects. For example, comparing a full model with many interactions with a model for the main effects in which only those are present almost inevitably needs to be significant.
> The second solution seems problematic from another perspective, as it simply ignores the higher order effects. This contrasts with classical ANOVA in which even the lower order effects are tested against the MSE of the full model. Consequently, using the second approach the lower order effects of a model with interactions are identical to a model fitted without those interactions.
> 
> Any ideas would be really appreciated.
> 
> (Sidenote: I obtain type 3 tests by contrasting the full model with a model in which only the relevant effect is missing, ignoring the order of the effects)
> 
> Cheers,
> Henrik
> 
> -- 
> Dipl. Psych. Henrik Singmann
> PhD Student
> Albert-Ludwigs-Universit?t Freiburg, Germany
> http://www.psychologie.uni-freiburg.de/Members/singmann
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From henrik.singmann at psychologie.uni-freiburg.de  Wed Aug 22 22:56:04 2012
From: henrik.singmann at psychologie.uni-freiburg.de (Henrik Singmann)
Date: Wed, 22 Aug 2012 22:56:04 +0200
Subject: [R-sig-ME] How to obtain Type 2 like p-values for effects?
In-Reply-To: <web-420216476@cgpsrv2.cis.mcmaster.ca>
References: <50352370.6020403@psychologie.uni-freiburg.de>
	<web-420216476@cgpsrv2.cis.mcmaster.ca>
Message-ID: <50354764.8030001@psychologie.uni-freiburg.de>

Dear John,

The difference between car::Anova() and my mixed() function is (as we 
already discussed off list) that mixed() uses  a likelihood ratio (LR) 
test approach (i.e., comparing two models) and Anova() uses the Wald 
tests. As the approach is somewhat different my question is if there is 
a way to obtain reasonable "type-II" tests using the model comparison 
approach. Perhaps it would have been better to frame the question in 
such a way.
Actually I checked both of my solutions against the development versions 
of Anova() and they don't match. However, solution (2) comes close. In 
contrast, the "type-III" tests perfectly match the results obtained with 
car::Anova().

Do you think the LR approach is generally flawed when compared with a 
Wald test approach? Or is there a way to obtain reasonable test for 
effetcs obeying marginality using LR?

Best,
Henrik

John Fox schrieb:
> Dear Henrik,
>
> The Anova() function in the car package already does this for Wald tests of the fixed effects in a mixed model, using pbkrtest to compute df (in the development version of the car package on R-Forge). We define the "type-II" test as the maximally powerful Wald test for the hypothesis in question obeying marginality.
>
> I hope this helps,
>   John
>
> ------------------------------------------------
> John Fox
> Sen. William McMaster Prof. of Social Statistics
> Department of Sociology
> McMaster University
> Hamilton, Ontario, Canada
> http://socserv.mcmaster.ca/jfox/
> 	
> On Wed, 22 Aug 2012 20:22:40 +0200
>   Henrik Singmann <henrik.singmann at psychologie.uni-freiburg.de> wrote:
>> Dear all,
>>
>> I am currently trying to add the possibility to obtain Type 2 like p-values for all effects in a mixed model to function mixed() in package afex (using pbkrtest::KRmodcomp see: https://stat.ethz.ch/pipermail/r-sig-mixed-models/2012q3/018946.html) but am unsure on how to implement this correctly.
>>
>> I came up with two possible solutions which both seem to have problems.
>>
>> (1) Fit one full model and contrast a model for each effect with this full model. Each submodel is created by subtracting the effect of interest and all higher order effects from the model (e.g., when interested in a main effect, the submodel contains all main effects but the one of interest and no interactions).
>>
>> (2) Fit a full model for each order of effects (i.e., one full model for all main effects, one full model for all two-way interactions, ...) and contrast a model for each effect with the full model of the corresponding order (e.g., when interested in a main effect, the submodel contains all main effects but the one of interest and no interactions and the full model contains all main effects and no interactions).
>>
>> Both solutions seem to have problems.
>> The first solution seems rather dubious, why would you want to compare two models to test for a single effect when in fact more than just this effect is missing, namely all the higher order effects. Consequently, this solution does produces small p-values and many significant effects. For example, comparing a full model with many interactions with a model for the main effects in which only those are present almost inevitably needs to be significant.
>> The second solution seems problematic from another perspective, as it simply ignores the higher order effects. This contrasts with classical ANOVA in which even the lower order effects are tested against the MSE of the full model. Consequently, using the second approach the lower order effects of a model with interactions are identical to a model fitted without those interactions.
>>
>> Any ideas would be really appreciated.
>>
>> (Sidenote: I obtain type 3 tests by contrasting the full model with a model in which only the relevant effect is missing, ignoring the order of the effects)
>>
>> Cheers,
>> Henrik
>>
>> -- 
>> Dipl. Psych. Henrik Singmann
>> PhD Student
>> Albert-Ludwigs-Universit?t Freiburg, Germany
>> http://www.psychologie.uni-freiburg.de/Members/singmann
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Dipl. Psych. Henrik Singmann
PhD Student
Albert-Ludwigs-Universit?t Freiburg, Germany
http://www.psychologie.uni-freiburg.de/Members/singmann


From pwschmitt at gmail.com  Thu Aug 23 05:45:43 2012
From: pwschmitt at gmail.com (Paul Schmitt)
Date: Wed, 22 Aug 2012 22:45:43 -0500
Subject: [R-sig-ME] Will lmer/lme work for a model such as this?
Message-ID: <CADevZde5LHttnh7FLgu6MW+8aa7Hd5KH0up4+iPk-=+9cE7Ypw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120822/07bb5670/attachment.pl>

From leila.brook at my.jcu.edu.au  Thu Aug 23 07:58:59 2012
From: leila.brook at my.jcu.edu.au (Leila Brook)
Date: Thu, 23 Aug 2012 05:58:59 +0000
Subject: [R-sig-ME] Dealing with heteroscedasticity in a GLM/M
Message-ID: <9BEE83C6BF59174DBD2CB4A06EB6CBEB2FDEB86C@SINPRD0111MB368.apcprd01.prod.exchangelabs.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120823/20dceed4/attachment.pl>

From aghaynes at gmail.com  Thu Aug 23 08:49:15 2012
From: aghaynes at gmail.com (Alan Haynes)
Date: Thu, 23 Aug 2012 08:49:15 +0200
Subject: [R-sig-ME] Dealing with heteroscedasticity in a GLM/M
In-Reply-To: <9BEE83C6BF59174DBD2CB4A06EB6CBEB2FDEB86C@SINPRD0111MB368.apcprd01.prod.exchangelabs.com>
References: <9BEE83C6BF59174DBD2CB4A06EB6CBEB2FDEB86C@SINPRD0111MB368.apcprd01.prod.exchangelabs.com>
Message-ID: <CAPdSD+6GD8xQu7fTHqqLjuWgvaO0eS1poM1NZ-1=ni2+hrsEKw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120823/9b4225ae/attachment.pl>

From markus.jantti at iki.fi  Thu Aug 23 09:29:57 2012
From: markus.jantti at iki.fi (=?ISO-8859-1?Q?Markus_J=E4ntti?=)
Date: Thu, 23 Aug 2012 09:29:57 +0200
Subject: [R-sig-ME] Dealing with heteroscedasticity in a GLM/M
In-Reply-To: <9BEE83C6BF59174DBD2CB4A06EB6CBEB2FDEB86C@SINPRD0111MB368.apcprd01.prod.exchangelabs.com>
References: <9BEE83C6BF59174DBD2CB4A06EB6CBEB2FDEB86C@SINPRD0111MB368.apcprd01.prod.exchangelabs.com>
Message-ID: <5035DBF5.4030605@iki.fi>

This is, strictly speaking, the wrong approach, but in order to explore the 
presence of heteroscedasticity, you could try to use the linear ME functions and 
the variance objects in that.

What I suggest by way of exploration is the following. If you regress a binomial 
response as if it were a countinuous variable in a standard OLS regression 
setting many problems arise,  including out of unit interval predictions and the 
error term is heteroscedastic. That heteroscedasticity is of a known form 
however, the variance being p*(1-p) where p is x*b is the linear predictor of 
the probability.

I would suggest you compare two models, both estimated using lme in the nlme 
package. One which models the response and includes a variance function that 
takes into account the heteroscedasticity induced by having a binary rather than 
continuous dependent variable. You then compare that with a model that adds, 
using the varComb() function, the heteroscedasticity you worry about.

Markus

On 08/23/2012 07:58 AM, Leila Brook wrote:
> I am hoping to find a way to account for heterogeneity of variance between categories of explanatory variables in a generalised model.
>
> I have searched books and this forum, and haven't found any advice that I understood could help account for this assumption in a generalised model context, as I can't fit a variance structure in lme4.
>
>
>
> As background to my study:
>
> I used camera stations set up in pairs (one positioned on a track and one off the track) to record my study species, and used the same pairs in each of two seasons. As my surveys were repeated, I have specified camera pair as a random effect. I am using a binomial model in lme4 to model the proportion of nights an animal was recorded, as a function of the fixed effects of season (2 categories), position (categorical: whether on or off the track), area (categorical: one of two areas) and continuous habitat variables, plus interactions between them.
>
>
>
> I validated the GLM form of the model, including plotting the deviance residuals against my explanatory variables, and have noted that the variance of residuals for the categorical variables appears to differ.
>
> Differences in the response variable between these categories were part of my research question and are evident in the raw data, so I don't want to remove them from the analysis.
>
> I have tried converting my response variable to be continuous, but when I do, it is not normal (too many zeros), nor is the log transformed variable.
>
>
>
> I have read that nlme can fit a variance structure to a LME, but haven't heard of a way of dealing with heterogeneity in a generalised mixed effect model, nor found an R package that can fit a variance structure to a GLMM.
>
> Can anyone provide any advice on how to overcome this problem, or whether I can continue, with some form of caveat, with the GLMM?
>
>
>
> Thanks in advance,
>
> Leila
>
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


-- 
Markus Jantti
Professor of Economics
Swedish Institute for Social Research
Stockholm University


From highstat at highstat.com  Thu Aug 23 11:34:27 2012
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Thu, 23 Aug 2012 10:34:27 +0100
Subject: [R-sig-ME] Dealing with heteroscedasticity in a GLM/M
Message-ID: <5035F923.9090905@highstat.com>

Leila,

The following reference shows how you can include covariates in the 
variance structure of
a negative binomial GLM. Extending it to a GLMM is straightforward.

Ntzoufras I. (2009). Bayesian Modeling Using WinBUGS. Wiley & Sons Inc. 
New Jersey.


R code is on his website...



Alain

-- 

Dr. Alain F. Zuur
First author of:

1. Analysing Ecological Data (2007).
Zuur, AF, Ieno, EN and Smith, GM. Springer. 680 p.
URL: www.springer.com/0-387-45967-7


2. Mixed effects models and extensions in ecology with R. (2009).
Zuur, AF, Ieno, EN, Walker, N, Saveliev, AA, and Smith, GM. Springer.
http://www.springer.com/life+sci/ecology/book/978-0-387-87457-9


3. A Beginner's Guide to R (2009).
Zuur, AF, Ieno, EN, Meesters, EHWG. Springer
http://www.springer.com/statistics/computational/book/978-0-387-93836-3


4. Zero Inflated Models and Generalized Linear Mixed Models with R. (2012) Zuur, Saveliev, Ieno.
http://www.highstat.com/book4.htm

Other books: http://www.highstat.com/books.htm


Statistical consultancy, courses, data analysis and software
Highland Statistics Ltd.
6 Laverock road
UK - AB41 6FN Newburgh
Tel: 0044 1358 788177
Email: highstat at highstat.com
URL: www.highstat.com
URL: www.brodgar.com


From marie.denis at cirad.fr  Thu Aug 23 11:54:49 2012
From: marie.denis at cirad.fr (Marie Denis)
Date: Thu, 23 Aug 2012 11:54:49 +0200
Subject: [R-sig-ME] Questions about MCMCglmm and marker data
Message-ID: <5035FDE9.6010801@cirad.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120823/12ba2adb/attachment.pl>

From r.m.krug at gmail.com  Thu Aug 23 14:19:19 2012
From: r.m.krug at gmail.com (Rainer M Krug)
Date: Thu, 23 Aug 2012 14:19:19 +0200
Subject: [R-sig-ME] Question concerning anova()
In-Reply-To: <CAPdSD+79RhkYobWTLpgKcVLjR3dzd0PCuXRG6+qKqXjN6KDfNQ@mail.gmail.com>
References: <5034EB51.1090401@gmail.com>
	<CACk-te1Ci4Fo1dEP7CU=S+URBN_faiBYCoz1fH0OVK1KzcY0zQ@mail.gmail.com>
	<5034F2DE.9090806@gmail.com>
	<CACk-te0PTUc5o6oT1Ox7SNw_mRGMmJXEDiuYar2Dr9oFjy4HrQ@mail.gmail.com>
	<50350327.5000006@gmail.com>
	<CAPdSD+79RhkYobWTLpgKcVLjR3dzd0PCuXRG6+qKqXjN6KDfNQ@mail.gmail.com>
Message-ID: <50361FC7.3040404@gmail.com>

On 23/08/12 08:34, Alan Haynes wrote:
> If you have a copy available, Zuur et al 2009 Mixed effects models and extensions in ecology with R
> is a good book and describes a procedure well. Almost the whole book is based on lme and also has
> examples of variance/correlation structures which might be useful to you (although you already seem
> to what what youre doing with them...).

Great - I'll look into it. I have luckily access via the University.

>
> The book suggests doing something like:
> mod1 <- gls(noBefore~pHarv*year, data= dat) # model without random term

OK - makes sense.

> mod2 <- lme(noBefore~pHarv*year, data= dat, random=~1|plant, method="REML") # random intercept

OK.

> mod3 <- lme(noBefore~pHarv*year, data= dat, random=~year|plant, method="REML") # random intercept

Question concerning the "random intercept" you mention - I assume this should be "random effect of 
year" ?


>
> anova(mod1, mod2, mod3)
>
> then if you accept mod2:
> mod2.1 <- update(mod2, method="ML") # ML for fixed effects
> mod2.2 <- update(mod2.1, .~. - pHarv:year) # create the nested model of mod2.1
> anova(mod2.1, mod2.2
>
> beyond that you should create two more nested models (each with a fixed effect removed) and compare
> them back to mod2.2 (assuming you dont need the interaction).

OK - makes sense.

>
> Where exactly testing the correlation structures would come in im not sure though. Also, you need to
> be aware of "testing on the boundary." I forget exactly where it comes in though (testing for the
> random effect I think). Thats covered by Zuur et al too.

I'll check it out - thanks.

Cheers,

Rainer


>
>
> HTH
>
> Alan
>
>
> --------------------------------------------------
> Email: aghaynes at gmail.com <mailto:aghaynes at gmail.com>
> Mobile: +41794385586
> Skype: aghaynes
>
>
> On 22 August 2012 18:04, Rainer M Krug <r.m.krug at gmail.com <mailto:r.m.krug at gmail.com>> wrote:
>
>     Further discussed on r-sig-mixed-models
>
>     Rainer
>
>
>     On 22/08/12 17:04, Bert Gunter wrote:
>
>         Oops -- missed that. OTOH, my reply demonstrates the value of the
>         mixed models list recommendation.
>
>         -- Bert
>
>         On Wed, Aug 22, 2012 at 7:55 AM, Rainer M Krug <r.m.krug at gmail.com
>         <mailto:r.m.krug at gmail.com>> wrote:
>
>             On 22/08/12 16:36, Bert Gunter wrote:
>
>
>                 Models with different fixed effects estimated by REML cannot be
>                 compared by anova.
>
>
>
>             I have seen that much in "Modern Applied Statistics in S", and therefore
>             have chosen the model = "ML"
>
>
>                 In future, please post questions on mixed effects models on the
>                 r-sig-mixed-effects mailing lists. You're likely to receive more
>                 informative replies there, too.
>
>
>
>             Thanks - wasn't aware of this sig - I'll send the reply there as well.
>
>             Thanks,
>
>             Rainer
>
>
>                 -- Bert
>
>                 On Wed, Aug 22, 2012 at 7:23 AM, Rainer M Krug <r.m.krug at gmail.com
>                 <mailto:r.m.krug at gmail.com>> wrote:
>
>
>                     Hi
>
>                     I am comparing four different linear mixed effect models, derived from
>                     updating the original one. To compare these, I want to use anova(). I
>                     therefore do the following (not reproducible - just to illustration
>                     purpose!):
>
>                     dat <- loadSPECIES(SPECIES)
>                     subs <- expression(dead==FALSE & recTreat==FALSE)
>                     feff <- noBefore~pHarv*year      # fixed effect in the model
>                     reff <- ~year|plant              # random effect in the model, where year
>                     is
>                     the
>                     corr <- corAR1(form=~year|plant) # describing the within-group
>                     correlation
>                     structure
>                     #
>                     dat.lme <- lme(
>                                     fixed = feff,                           # fixed effect in
>                     the
>                     model
>                                     data  = dat,
>                                     subset = eval(subs),
>                                     method = "ML",
>                                     random = reff,                          # random effect in
>                     the
>                     model
>                                     correlation = corr,
>                                     na.action = na.omit
>                                     )
>                     dat.lme.r1 <- update(dat.lme, random=~1|plant)
>                     dat.lme.f1 <- update(dat.lme, fixed=noBefore~year)
>                     dat.lme.r1.f1 <- update(dat.lme.r1, fixed=noBefore~year)
>
>
>                     The anova is as follow:
>
>                         anova(dat.lme, dat.lme.r1, dat.lme.f1, dat.lme.r1.f1)
>
>
>                                      Model df      AIC      BIC    logLik   Test      L.Ratio
>                     p-value
>                     dat.lme           1  9 1703.218 1733.719 -842.6089
>                     dat.lme.r1        2  7 1699.218 1722.941 -842.6089 1 vs 2 1.019230e-07
>                     1
>                     dat.lme.f1        3  7 1705.556 1729.279 -845.7779
>                     dat.lme.r1.f1     4  5 1701.556 1718.501 -845.7779 3 vs 4 8.498318e-08
>                     1
>
>                     I have two questions:
>                     1) I am wondering why the "2 vs 3" does not give the Test values?
>                     Is this because the two models are considered as "identical", which would
>                     be
>                     strange, due to the different logLik values.
>
>                     2) If I want to compare all models among each other - is there a "best"
>                     way?
>                     I would be reluctant to do several ANOVA's, due to necessary corrections
>                     for
>                     multple tests (although this should not be a problem here?)
>
>                     I can obviously select the best model based on the AIC.
>
>                     Thanks in advance,
>
>                     Rainer
>
>                     --
>                     Rainer M. Krug, PhD (Conservation Ecology, SUN), MSc (Conservation
>                     Biology,
>                     UCT), Dipl. Phys. (Germany)
>
>                     Centre of Excellence for Invasion Biology
>                     Stellenbosch University
>                     South Africa
>
>                     Tel : +33 - (0)9 53 10 27 44 <tel:%2B33%20-%20%280%299%2053%2010%2027%2044>
>                     Cell: +33 - (0)6 85 62 59 98 <tel:%2B33%20-%20%280%296%2085%2062%2059%2098>
>                     Fax : +33 - (0)9 58 10 27 44 <tel:%2B33%20-%20%280%299%2058%2010%2027%2044>
>
>                     Fax (D): +49 - (0)3 21 21 25 22 44
>                     <tel:%2B49%20-%20%280%293%2021%2021%2025%2022%2044>
>
>                     email: Rainer at krugs.de <mailto:Rainer at krugs.de>
>
>                     Skype:      RMkrug
>
>                     ________________________________________________
>                     R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>                     https://stat.ethz.ch/mailman/__listinfo/r-help
>                     <https://stat.ethz.ch/mailman/listinfo/r-help>
>                     PLEASE do read the posting guide
>                     http://www.R-project.org/__posting-guide.html
>                     <http://www.R-project.org/posting-guide.html>
>                     and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
>
>
>
>             --
>             Rainer M. Krug, PhD (Conservation Ecology, SUN), MSc (Conservation Biology,
>             UCT), Dipl. Phys. (Germany)
>
>             Centre of Excellence for Invasion Biology
>             Stellenbosch University
>             South Africa
>
>             Tel : +33 - (0)9 53 10 27 44 <tel:%2B33%20-%20%280%299%2053%2010%2027%2044>
>             Cell: +33 - (0)6 85 62 59 98 <tel:%2B33%20-%20%280%296%2085%2062%2059%2098>
>             Fax : +33 - (0)9 58 10 27 44 <tel:%2B33%20-%20%280%299%2058%2010%2027%2044>
>
>             Fax (D): +49 - (0)3 21 21 25 22 44 <tel:%2B49%20-%20%280%293%2021%2021%2025%2022%2044>
>
>             email: Rainer at krugs.de <mailto:Rainer at krugs.de>
>
>             Skype:      RMkrug
>
>
>
>
>
>     _________________________________________________
>     R-sig-mixed-models at r-project.__org <mailto:R-sig-mixed-models at r-project.org> mailing list
>     https://stat.ethz.ch/mailman/__listinfo/r-sig-mixed-models
>     <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
>
>


-- 
Rainer M. Krug, PhD (Conservation Ecology, SUN), MSc (Conservation Biology, UCT), Dipl. Phys. (Germany)

Centre of Excellence for Invasion Biology
Stellenbosch University
South Africa

Tel :       +33 - (0)9 53 10 27 44
Cell:       +33 - (0)6 85 62 59 98
Fax :       +33 - (0)9 58 10 27 44

Fax (D):    +49 - (0)3 21 21 25 22 44

email:      Rainer at krugs.de

Skype:      RMkrug


From ihok at hotmail.com  Thu Aug 23 17:31:43 2012
From: ihok at hotmail.com (Jack Tanner)
Date: Thu, 23 Aug 2012 11:31:43 -0400
Subject: [R-sig-ME] working around glm.fit: "fitted probabilities
 numerically 0 or 1 occurred"
Message-ID: <BLU0-SMTP383A6DE38C964D8F996724CCABE0@phx.gbl>

I'm using lme4 0.999999-0 to fit some 0/1 response data with a logistic 
regression and I get glm.fit: "fitted probabilities numerically 0 or 1 
occurred". I've read Ted Harding's explanation 
<http://r.789695.n4.nabble.com/glm-fit-quot-fitted-probabilities-numerically-0-or-1-occurred-quot-td849242.html>, 
but I still don't know how to work around this.

More specifically, this is a Rasch-style model. When I fit a smaller 
model that has only effects (intercepts), not covariates, glmer() 
converges fine. When I add a matrix of covariates, which are mainly 
low-frequency counts (lots of zeros), I get "fitted probabilities 
numerically 0 or 1 occurred". I can fit the model including covariates 
under JAGS, but I was hoping to be able to fit it under lmer because 
MCMC can be very slow.

Perhaps there are some ways I could reparametrize the model?

Another idea is that calling glmer(..., verbose=TRUE) gives the error on 
the very first iteration, i.e.,
   0:           nan: 0.180462 0.0723839 ...
Warning messages:
1: glm.fit: fitted probabilities numerically 0 or 1 occurred
2: In mer_finalize(ans) : gr cannot be computed at initial par (65)

Would it be worth trying to specify good starting values? If so, is 
there a way to extract estimates from an object of class mer that can be 
easily passed to glmer(..., start=...)?


From curis at pharmacie.univ-paris5.fr  Thu Aug 23 12:10:46 2012
From: curis at pharmacie.univ-paris5.fr (Emmanuel Curis)
Date: Thu, 23 Aug 2012 12:10:46 +0200
Subject: [R-sig-ME] Questions about MCMCglmm and marker data
In-Reply-To: <5035FDE9.6010801@cirad.fr>
References: <5035FDE9.6010801@cirad.fr>
Message-ID: <20120823101046.GB26811@info124.pharmacie.univ-paris5.fr>

Hello,

I'm not familiar with this kind of genomic problems, but if the
columns are the number of a given SNP for a given individual and can
be only 0, 1 and 2, is this really possible to consider they follow a
normal distribution?

On Thu, Aug 23, 2012 at 11:54:49AM +0200, Marie Denis wrote:
? Hi,
? 
? I use the MCMCglmm function in the genomic selection context in the 
? univariate case. In fact I have for each trait one marker matrix 
? constituted of 0,1 and 2. The rows are the individuals and the columns 
? the SNPs. In a first time, we consider that each SNP follow a normal 
? distribution with the *same *variance. So I use the following model:
? 
? prior.1.1 <- list(G=list(G1=list(V=diag(x = as.numeric(scale), nrow=1, 
? ncol=1),nu=ddl),
?                            R=list(V=matrix(scale),nu=ddl))
? 
? mcmc.fit.1.1 <- MCMCglmm(P~ 1,random=~idv(SNP),prior=prior.1.1,
?                    data=data1.1,
?                    nitt=5000, burnin=1000,verbose=FALSE,
?                    thin=10,pr=TRUE)
? 
? So, I obtained a common variance associated to my SNPs.
? 
? 
? The second step is a bivariate analysis. I would like to obtain a 
? (co)variance matrix 2*2  associated to the trait1 and trait 2 and the 
? correlation between both. (one variance for the SNPs for the trait 1 and 
? one for the trait 2). But I don't know how i can do. The following model 
? give me only one variance for all SNPs and both traits.
? 
?      prior.3<- list(G=list(G1=list(V=diag(x = as.numeric(scale), nrow=1,
?                            ncol=1),n=1)),
?                R=list(V=diag(x=as.numeric(0.1),nrow=2,ncol=2),n=2))
? 
?      mcmc.fit.3 <- MCMCglmm(cbind(P1,P2)~ trait-1,random=~idv(trait:SNP),
?                    rcov=~idh(trait):units,
?                    prior=prior.3,
?                    data=data1.3,family=c("gaussian","gaussian"),
?                    nitt=2000, burnin=500,verbose=FALSE,
?                    thin=10,pr=TRUE)
? 
? 
? 
? thanks for your help,
? 
? 
? 
? 
? 	[[alternative HTML version deleted]]
? 
? _______________________________________________
? R-sig-mixed-models at r-project.org mailing list
? https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
                                Emmanuel CURIS
                                emmanuel.curis at univ-paris5.fr

Page WWW: http://emmanuel.curis.online.fr/index.html


From chris at trickysolutions.com.au  Fri Aug 24 01:07:35 2012
From: chris at trickysolutions.com.au (Chris Howden)
Date: Fri, 24 Aug 2012 09:07:35 +1000
Subject: [R-sig-ME] working around glm.fit: "fitted probabilities
 numerically 0 or 1 occurred"
In-Reply-To: <BLU0-SMTP383A6DE38C964D8F996724CCABE0@phx.gbl>
References: <BLU0-SMTP383A6DE38C964D8F996724CCABE0@phx.gbl>
Message-ID: <-2696681026722914294@unknownmsgid>

Jack this could be occurring due to separation. Which is when a
predictor perfectly separates your response into 1's or 0's.

U can check for this by running some tables, perhaps guided by
removing predictors form your model until u no longer receive the
message. The last predictor removed is then perhaps causing
separation.

Next think about what this means. We do this analysis to understand
what predictors are related to an event. Well if the event either
always occurs or never occurs for some values of a predictor than
maybe I don't need an analysis to tell me what's going on?  When I get
this I often find I can simply remove the predictor from the model,
but comment on its effect in my discussion.

Or if I am doing predictive modelling than I use a 2 stage model, that
predicts using a predictor *response table and the a logistic
regression for those cells that don't have prob 0 or 1.

Chris Howden
Founding Partner
Tricky Solutions
Tricky Solutions 4 Tricky Problems
Evidence Based Strategic Development, IP Commercialisation and
Innovation, Data Analysis, Modelling and Training

(mobile) 0410 689 945
(fax / office)
chris at trickysolutions.com.au

Disclaimer: The information in this email and any attachments to it are
confidential and may contain legally privileged information. If you are not
the named or intended recipient, please delete this communication and
contact us immediately. Please note you are not authorised to copy,
use or disclose this communication or any attachments without our
consent. Although this email has been checked by anti-virus software,
there is a risk that email messages may be corrupted or infected by
viruses or other
interferences. No responsibility is accepted for such interference. Unless
expressly stated, the views of the writer are not those of the
company. Tricky Solutions always does our best to provide accurate
forecasts and analyses based on the data supplied, however it is
possible that some important predictors were not included in the data
sent to us. Information provided by us should not be solely relied
upon when making decisions and clients should use their own judgement.

On 24/08/2012, at 1:40, Jack Tanner <ihok at hotmail.com> wrote:

> I'm using lme4 0.999999-0 to fit some 0/1 response data with a logistic regression and I get glm.fit: "fitted probabilities numerically 0 or 1 occurred". I've read Ted Harding's explanation <http://r.789695.n4.nabble.com/glm-fit-quot-fitted-probabilities-numerically-0-or-1-occurred-quot-td849242.html>, but I still don't know how to work around this.
>
> More specifically, this is a Rasch-style model. When I fit a smaller model that has only effects (intercepts), not covariates, glmer() converges fine. When I add a matrix of covariates, which are mainly low-frequency counts (lots of zeros), I get "fitted probabilities numerically 0 or 1 occurred". I can fit the model including covariates under JAGS, but I was hoping to be able to fit it under lmer because MCMC can be very slow.
>
> Perhaps there are some ways I could reparametrize the model?
>
> Another idea is that calling glmer(..., verbose=TRUE) gives the error on the very first iteration, i.e.,
>  0:           nan: 0.180462 0.0723839 ...
> Warning messages:
> 1: glm.fit: fitted probabilities numerically 0 or 1 occurred
> 2: In mer_finalize(ans) : gr cannot be computed at initial par (65)
>
> Would it be worth trying to specify good starting values? If so, is there a way to extract estimates from an object of class mer that can be easily passed to glmer(..., start=...)?
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From S.Ellison at LGCGroup.com  Fri Aug 24 12:43:39 2012
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Fri, 24 Aug 2012 11:43:39 +0100
Subject: [R-sig-ME] working around glm.fit: "fitted probabilities
 numerically 0 or 1 occurred"
In-Reply-To: <BLU0-SMTP383A6DE38C964D8F996724CCABE0@phx.gbl>
References: <BLU0-SMTP383A6DE38C964D8F996724CCABE0@phx.gbl>
Message-ID: <A4E5A0B016B8CB41A485FC629B633CED469543130A@GOLD.corp.lgc-group.com>

This does not necessarily mean the fit has not converged; it is usually just a warning that some predicted probabilities at some point during the process of fitting were so close to 0 or 1 that they cannot be properly represented in finite precision arithmetic. That does not, of itself, prevent convergence, and you have already nicely demonstrated that the problem occurred on early iterations but not later, better, estimates.

Are you sure you need to work round it?

S

*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From pdxgary163 at gmail.com  Fri Aug 24 18:02:29 2012
From: pdxgary163 at gmail.com (Gary Dong)
Date: Fri, 24 Aug 2012 09:02:29 -0700
Subject: [R-sig-ME] Pseudo R squared
Message-ID: <CAEVDvzXOD45_tktOo=_yy9GDREXrgdOyZ8RLYeZR4LSTbsm5kQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120824/31c95aa3/attachment.pl>

From ihok at hotmail.com  Sat Aug 25 06:22:45 2012
From: ihok at hotmail.com (Jack Tanner)
Date: Sat, 25 Aug 2012 00:22:45 -0400
Subject: [R-sig-ME] working around glm.fit: "fitted probabilities
 numerically 0 or 1 occurred"
In-Reply-To: <A4E5A0B016B8CB41A485FC629B633CED469543130A@GOLD.corp.lgc-group.com>
References: <BLU0-SMTP383A6DE38C964D8F996724CCABE0@phx.gbl>
	<A4E5A0B016B8CB41A485FC629B633CED469543130A@GOLD.corp.lgc-group.com>
Message-ID: <BLU0-SMTP37811A855153AC9BDE27E03CABC0@phx.gbl>

On 8/24/2012 6:43 AM, S Ellison wrote:
> This does not necessarily mean the fit has not converged; it is usually just a warning that some predicted probabilities at some point during the process of fitting were so close to 0 or 1 that they cannot be properly represented in finite precision arithmetic. That does not, of itself, prevent convergence, and you have already nicely demonstrated that the problem occurred on early iterations but not later, better, estimates.
>
> Are you sure you need to work round it?

I got it! But I still have a question. Why does the start parameter to 
lmer() only take ST and fixef, and not starting ranef values?

I did need to work around it lmer's warnings. On the one hand, lmer did 
produce a fitted mer object. On the other, it printed only a single 
iteration under verbose=TRUE. It never performed additional iterations. 
The estimates from verbose contained a lot of NaN values. The warnings I 
got were

Warning messages:
1: glm.fit: fitted probabilities numerically 0 or 1 occurred
2: In mer_finalize(ans) : gr cannot be computed at initial par (65)

What worked was that I initialized a new run of lmer(..., 
start=my_start), where my_start used ST and fixef values from a 
successful fit that did not include the covariant matrices. The first 
time I tried this, I got an additional warning.

Warning messages:
1: glm.fit: fitted probabilities numerically 0 or 1 occurred
2: In sort(names(start)) == sort(names(FL)) :
   longer object length is not a multiple of shorter object length
3: In mer_finalize(ans) : gr cannot be computed at initial par (65)

The new warning (2) made sense, because the covariant matrices require 
the estimation of additional parameters not included in the previous 
fit. I padded the fixef component of my_start with some rnorm(mean=0, 
sd=.2) values, and presently, lmer is on iteration 67!

So, why doesn't lmer's start parameter take ranef values?


From j.hadfield at ed.ac.uk  Sat Aug 25 17:55:27 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Sat, 25 Aug 2012 16:55:27 +0100
Subject: [R-sig-ME] Another MCMCglmm question - fixing 	correlations
In-Reply-To: <F430FC4F-13A6-4ED4-923D-74709B0E2711@bristol.ac.uk>
References: <mailman.7.1291201202.14469.r-sig-mixed-models@r-project.org>
	<65C3FF86-9E7A-449A-86E7-1096E3F480CD@bristol.ac.uk>
	<EEED6CE6-2D3D-480B-88E3-52A4C25797FA@ed.ac.uk>
	<7FC5B5A7-3E8D-43ED-B54F-7785257F3841@bristol.ac.uk>
	<20101204211150.50434f1dwuwqf7ok@www.staffmail.ed.ac.uk>
	<3259EF79-BB81-4C7B-90D7-F51948B88022@bristol.ac.uk>
	<F430FC4F-13A6-4ED4-923D-74709B0E2711@bristol.ac.uk>
Message-ID: <20120825165527.13616s4b0w4lfd44@www.staffmail.ed.ac.uk>

Hi Malcolm,

I'm aware of the problem - I moved to a sparseMatrix implementation  
which seems to be incompatible with with  
model.matrix/sparse.model.matrix. I will try and sort it out as soon  
as I have time.

Cheers,

Jarrod



Quoting Malcolm Fairbrother <m.fairbrother at bristol.ac.uk> on Tue, 21  
Aug 2012 09:05:01 +0100:

> Dear Jarrod,
>
> A while back we had a discussion on the list about spatial  
> simultaneous autoregressive lag models, using the "sir" function in  
> MCMCglmm. I just tried re-running the code in my message of 5 Dec  
> 2010, which was working at the time, and it now gets hung up on the  
> call to "sir", returning the error:
>
> Error in model.frame.default(object, data, xlev = xlev) :
>   invalid type (S4) for variable 'sir(~W, ~units)'
>
> I thought "sir(~W, ~diag(nrow(W)))" might help, but it doesn't.
>
> And I get the same error if I try the code from pp. 132-3 in the  
> MCMCglmm CourseNotes.
>
> Did you tweak something in MCMCglmm (I'm now running version 2.16)  
> which might have caused this problem? Any assistance appreciated.
>
> Cheers,
> Malcolm
>
>
>
>
> On 5 Dec 2010, at 16:16, Malcolm Fairbrother wrote:
>
>> Hi Jarrod,
>>
>> Thanks very much for the suggestions, which did the trick. (And  
>> standardising the row weights to 1 seemed to prevent any trouble  
>> with any values of rho.)
>>
>> MCMCglmm does appear to fit spatial simultaneous autoregressive lag  
>> models, and about as well as the specialised functions for such  
>> models in the "spdep" package. I've tried a few values of rho, and  
>> MCMCglmm recovers them well (the estimates from both it and  
>> "lagsarlm"  are slightly downward biased--see below for simulations  
>> code).
>>
>> My real/ulterior interest here is actually estimating multilevel  
>> models where diffusion takes place across higher-level units, and  
>> some preliminary investigations suggest that MCMCglmm can fit such  
>> a model--unlike any other R function I've tried--but I'll spare the  
>> list the gory details of this work-in-progress, unless somebody  
>> wants to know.
>>
>> Cheers,
>> Malcolm
>>
>>
>>
>> library(MCMCglmm)
>> library(spdep)
>> N <- 50 # set sample size
>> rho <- 0.6 # set autocorrelation coefficient
>> sims <- 100
>> prior1 = list(R=list(V=1, nu=0.002))
>> MSE <- res <- matrix(NA, nrow=sims, ncol=6)
>> for (i in 1:sims) {
>> 	W <- matrix(rbinom(N^2, 1, 0.25), N, N)*upper.tri(matrix(1, N, N))
>> 	W <- W + t(W) # symmetrical connectivity matrix
>> 	W <- W/apply(W, 1, sum) # standardise rows
>> 	listw <- mat2listw(W) # convert contiguities to listw format
>> 	L <- diag(N) - rho*W
>> 	X1 <- runif(N, min=-5, max=5)
>> 	Xbe <- X1+rnorm(N)
>> 	y <- solve(L, Xbe) # generate lagged y
>> 	dat <- data.frame(X1=X1, y=y, W=W)
>> 	autoreg <- lagsarlm(y ~ X1, listw=listw, data=dat) # fit  
>> autoregressive model
>> 	MC2 <- MCMCglmm(y ~ X1+sir(~W, ~units), data=dat, prior=prior1, verbose=F)
>> 	res[i,] <- c(coefficients(autoreg), mean(MC2$Lambda),  
>> apply(MC2$Sol, 2, mean))
>> 	MSE[i,] <- c((coefficients(autoreg)-c(rho, 0, 1))^2,  
>> (mean(MC2$Lambda)-rho)^2, (apply(MC2$Sol, 2, mean) - c(0, 1))^2)
>> 	}
>>
>>
>>
>> On 4 Dec 2010, at 21:11, Jarrod Hadfield wrote:
>>
>>> Hi Malcolm,
>>>
>>> I meant replace the sir function in the R directory and rebuild +  
>>> reinstall MCMCglmm. However, I later realised that for your  
>>> problem there is a simpler way:
>>>
>>> N <- 50 # set sample size
>>> W <- matrix(rbinom(N^2, 1, 0.25), N, N)*upper.tri(matrix(1, N, N))
>>> W <- W + t(W) # symmetrical connectivity matrix
>>> rho <- 0.2 # set autocorrelation coefficient
>>> L <- diag(N) - rho*W  # my L is your M^{-1}
>>> X1 <- runif(N, min=-5, max=5)
>>> Xbe <- X1+rnorm(N)
>>> y <- solve(L, Xbe) # generate lagged y
>>> dat <- data.frame(X1=X1, y=y, W=W)
>>>
>>> prior1 = list(R=list(V=1, nu=0.002))
>>> MC2 <- MCMCglmm(y ~ X1+sir(~W, ~units), data=dat, prior=prior1, verbose=F)
>>>
>>> This works because ~units sets up an identity matrix and W%*%t(I) = W.
>>>
>>> However, I could not get sensible results from MCMCglmm with  
>>> rho=0.3 and wasn't sure why (rho=0.03 for example gives sense).  
>>> The problem is associated with a change in the sign of the  
>>> determinant of L (or M) (i.e the Jacobian), which is perhaps not  
>>> that surprising since if W=I then rho=0.3 and rho = 1.7 should be  
>>> indiscriminable even with a fixed residual variance:
>>>
>>> y = Xbe/(1-0.3) = -Xbe/(1-1.7)
>>>
>>> I would have to think about this harder than I have to offer a  
>>> solution (this is why the sir models are undocumented!), but  
>>> perhaps you have some insight?
>>>
>>> Also, I don't think MC3 results are meaningless, the estimate for  
>>> the sir parameter overlaps zero as expected.
>>>
>>> Cheers,
>>>
>>> Jarrod
>>>
>>>
>>>
>>> Quoting Malcolm Fairbrother <m.fairbrother at bristol.ac.uk>:
>>>
>>>> Hi Jarrod,
>>>>
>>>> Thanks very much for the suggestion. However, in trying what you  
>>>> said, I'm not getting very far (see code below). I redefine the  
>>>> function "sir", and then re-install the MCMCglmm package. But the  
>>>> MCMCglmm function still seems to use the old "sir" function, not  
>>>> the new one, with the result that when I try calling sir in the  
>>>> middle of a call to MCMCglmm, I get an error message telling me  
>>>> that my call to sir is problematic ("Error in sir(W) : formula  
>>>> not passed to formula1 in sir"). Can you please clarify what I'm  
>>>> doing wrong?
>>>>
>>>> Thanks again for any assistance.
>>>> - Malcolm
>>>>
>>>>
>>>> sir <- function(W) {W}
>>>> install.packages("MCMCglmm")
>>>> library(MCMCglmm)
>>>>
>>>> N <- 50 # set sample size
>>>> W <- matrix(rbinom(N^2, 1, 0.25), N, N)*upper.tri(matrix(1, N, N))
>>>> W <- W + t(W) # symmetrical connectivity matrix
>>>> rho <- 0.2 # set autocorrelation coefficient
>>>> M <- solve(diag(N) - rho*W)
>>>> X1 <- runif(N, min=-5, max=5)
>>>> Xbe <- X1+rnorm(N)
>>>> y <- M %*% Xbe # generate lagged y
>>>> dat <- data.frame(X1=X1, y=y)
>>>>
>>>> prior1 = list(R=list(V=1, nu=0.002), G=list(G1=list(V=1, nu=0.002)))
>>>> MC1 <- MCMCglmm(y ~ X1, data=dat, prior=prior1, verbose=F)
>>>> # works OK, compare with summary(lm(dat$y~dat$X1))
>>>>
>>>> identical(sir(W), W) # TRUE
>>>> MC2 <- MCMCglmm(y ~ X1 + sir(W), data=dat, prior=prior1, verbose=F)
>>>> # doesn't work
>>>>
>>>> fac2 <- fac1 < -factor(sample(letters, N, TRUE), levels=letters)
>>>> MC3 <- MCMCglmm(y ~ X1 + sir(~fac1, ~fac2), data=dat,  
>>>> prior=prior1, verbose=F)
>>>> # works--results are meaningless, but doesn't return any error
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> On 2 Dec 2010, at 11:20, Jarrod Hadfield wrote:
>>>>
>>>>> Hi Malcolm,
>>>>>
>>>>> The suggestion of using SIR models for time series was a throw  
>>>>> away remark, but I can't see anything technically incorrect with  
>>>>> it. For example the model y[t] = \lambda*y[t-1]+..... can be  
>>>>> treated as a SIR model, although there are certainly better ways  
>>>>> of fitting it. I'm not familiar with spatial models and  
>>>>> connectivity matrices but if W[i,j] is a way of specifying y[i]  
>>>>> = \lambda*y[j] + ... then it is possible to fit it as a SIR  
>>>>> model. You could modify the sir code to do what you would like  
>>>>> (and then reinstall MCMCglmm) , for example:
>>>>>
>>>>> sir<-function(W){W}
>>>>>
>>>>> and then place W in the data.frame.
>>>>>
>>>>> MCMCglmm will associate design matrices with  
>>>>> recursive/simultaneous structures if the design matrix is  
>>>>> returned from a function called "sir".
>>>>>
>>>>> Cheers,
>>>>>
>>>>> Jarrod
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> On 1 Dec 2010, at 20:10, Malcolm Fairbrother wrote:
>>>>>
>>>>>> Dear Jarrod,
>>>>>>
>>>>>> I'm very interested in this SIR feature of MCMCglmm, which I  
>>>>>> hadn't been aware of until your e-mail earlier today. Do I  
>>>>>> understand correctly that this should allow for feedback  
>>>>>> effects between units, reflecting the fact that y(i) and  
>>>>>> y(not-i) affect each other? If so, this could be a useful way  
>>>>>> of fitting, for example, spatial autoregressive models, where  
>>>>>> some units may affect each other by virtue of being neighbours,  
>>>>>> and we would like to know the magnitude of these effects. (You  
>>>>>> mentioned time series, but some web searches haven't turned up  
>>>>>> much on MCMCglmm and time series. And time only flows in one  
>>>>>> direction, whereas units in space can have two-way effects.)
>>>>>>
>>>>>> However, in looking at the documentation and code, I've been  
>>>>>> struggling a bit with the "sir(~XX, ~XX)" part. Is there a way  
>>>>>> to specify a "connectivity" matrix directly, rather than using  
>>>>>> the "sir" function? Looking at the MCMCglmm code, as I  
>>>>>> understand it, "sir" is both a way of generating a connectivity  
>>>>>> matrix AND a way of telling MCMCglmm to treat the sir(~XX, ~XX)  
>>>>>> differently than other covariates.
>>>>>>
>>>>>> In the case of spatial autoregressive models, each element on  
>>>>>> the main diagonal of the connectivity matrix is 0, and rows are  
>>>>>> often standardised to sum to 1. Such a connectivity matrix  
>>>>>> might, for example, look like:
>>>>>>
>>>>>> N <- 100
>>>>>> W <- matrix(rbinom(N^2, 1, 0.1), N, N)*upper.tri(matrix(1, N,  
>>>>>> N)) # chance of being neighbours is 0.1
>>>>>> W <- W + t(W) # matrix is symmetrical, with zeros on the main diagonal
>>>>>>
>>>>>> But I'm having a hard time figuring out how to use "sir" to  
>>>>>> specify such a matrix in the middle of a call to MCMCglmm.
>>>>>>
>>>>>> If you see what I mean, can you offer any suggestions? I might  
>>>>>> take a stab at modifying MCMCglmm to make this possible, but  
>>>>>> before I do that (and I'm not certain I'll have the know-how) I  
>>>>>> wanted to check whether you had any ideas.
>>>>>>
>>>>>> Many thanks for any assistance or clarification you can provide.
>>>>>> - Malcolm
>>>>>>
>>>>>>
>>>>>>
>>>>>>> Message: 5
>>>>>>> Date: Wed, 1 Dec 2010 10:28:43 +0000
>>>>>>> From: Jarrod Hadfield <j.hadfield at ed.ac.uk>
>>>>>>> To: Szymek Drobniak <geralttee at gmail.com>
>>>>>>> Cc: r-sig-mixed-models at r-project.org
>>>>>>> Subject: Re: [R-sig-ME] Another MCMCglmm question - fixing
>>>>>>> 	correlations
>>>>>>> Message-ID: <0AF131D6-557C-4324-A65A-A175E41DBA8F at ed.ac.uk>
>>>>>>> Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes
>>>>>>>
>>>>>>> Hi,
>>>>>>>
>>>>>>> There is a way, but its quite involved and you may find it easier to
>>>>>>> fit this type of model in something like ASReml if you can get a
>>>>>>> license. If not ......
>>>>>>>
>>>>>>> I have implemented simultaneous-recursive (SIR) mixed models  in
>>>>>>> MCMCglmm, although currently they are not well tested or documented
>>>>>>> (See Chapter 9 of the CourseNotes) and cannot be used with non-
>>>>>>> Gaussian data or certain patterns of missing data. Nevertheless, in
>>>>>>> this example the existing code should work fine.  A simple SIR model
>>>>>>> has the form:
>>>>>>>
>>>>>>> y_{i} = mu + \lambda*y_{j} + u_{i} + e_{i}
>>>>>>>
>>>>>>> where mu is the intercept, u is a random effect (e.g. breeding value
>>>>>>> in your case) and e is a residual. The new part of the model is the
>>>>>>> structural parameter \lambda multiplied by y_{j}, the jth element of
>>>>>>> the response vector, which can be useful for modelling the effects of
>>>>>>> behavioural interactions or time series etc.  However, placing y's on
>>>>>>> the LHS and RHS complicates the likelihood, but MCMCglmm deals with
>>>>>>> this.
>>>>>>>
>>>>>>> If we set i=j then the above equation can be rearranged:
>>>>>>>
>>>>>>> y_{i}  - \lambda*y_{i} = mu + u_{i} + e_{i}
>>>>>>> y_{i}(1  - \lambda) = mu + u_{i} + e_{i}
>>>>>>> y_{i} = (mu + u_{i} + e_{i})/(1  - \lambda)
>>>>>>>
>>>>>>> from this it can be seen that mu, u, e, and \lambda cannot be uniquely
>>>>>>> estimated without restrictions.  To illustrate we'll simulate  
>>>>>>> some data:
>>>>>>>
>>>>>>> fac<-gl(25,4)
>>>>>>> y<-rnorm(25)[fac]+rnorm(100, -1, sqrt(2))
>>>>>>>
>>>>>>> # mu = -1, VAR(u)=1, VAR(e) = 2, \lambda = 0
>>>>>>>
>>>>>>> dat<-data.frame(y=y, fac=fac)
>>>>>>>
>>>>>>> prior1 = list(R=list(V=1, nu=0.002), G=list(G1=list(V=1, nu=0.002)))
>>>>>>>
>>>>>>> m1<-MCMCglmm(y~1, random=~fac,  data=dat, prior=prior1)
>>>>>>>
>>>>>>> # fitting a non-SIR model gives estimates consistent with the true
>>>>>>> values, which you can verify through summary(m1). Now for the SIR
>>>>>>> model. Because we know all parameters cannot be uniquely estimated
>>>>>>> I've set the residual variance to one arbitrarily.
>>>>>>>
>>>>>>> prior2 = list(R=list(V=1, fix=1), G=list(G1=list(V=1, nu=0.002)))
>>>>>>>
>>>>>>> m2<-MCMCglmm(y~1+sir(~units, ~units), random=~fac,  data=dat,
>>>>>>> prior=prior2)   # sir(~units, ~units) sets i=j.
>>>>>>>
>>>>>>> # The estimates do not look consistent with the real values.  However,
>>>>>>> we can rescale them by 1-\lambda and the estimates  are close to being
>>>>>>> identical up to Monte Carlo error (the prior will have slightly
>>>>>>> different effects)
>>>>>>>
>>>>>>> HPDinterval(m1$VCV[,1])
>>>>>>> HPDinterval(m2$VCV[,1]/(1-m2$Lambda)^2)
>>>>>>>
>>>>>>> HPDinterval(m1$VCV[,2])
>>>>>>> HPDinterval(m2$VCV[,2]/(1-m2$Lambda)^2)
>>>>>>>
>>>>>>> HPDinterval(m1$Sol)
>>>>>>> HPDinterval(m2$Sol/(1-m2$Lambda))
>>>>>>
>>>>>>
>>>>>> 	[[alternative HTML version deleted]]
>>>>>>
>>>>>> _______________________________________________
>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> The University of Edinburgh is a charitable body, registered in
>>>>> Scotland, with registration number SC005336.
>>>>>
>>>>
>>>>
>>>>
>>>
>>>
>>>
>>> --
>>> The University of Edinburgh is a charitable body, registered in
>>> Scotland, with registration number SC005336.
>>>
>>>
>>
>
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From pauljohn32 at gmail.com  Sun Aug 26 09:12:56 2012
From: pauljohn32 at gmail.com (Paul Johnson)
Date: Sun, 26 Aug 2012 02:12:56 -0500
Subject: [R-sig-ME] who shot the predict method (and did you shoot the
	deputy as well?)
Message-ID: <CAErODj8FZVw9owRE+U49MfLrckTjMOos3MJFDrwj1Sxw=_0E7Q@mail.gmail.com>

It seems like I can't leave for a month or two without pieces of mount
Rushmore falling off of lme4.

Remember last June, I posted about some troubles with the predict
method and you fixed it.

And I had some other trouble with fishy looking variance estimates
from models specified like (1|id) + (0 +x|id).
I stopped fighting with that because I just couldn't understand it at
all, but today I took it up again.

Now the fishy looking variance estimates are completely solved
(yeah!), but somebody stole the predict function entirely:

> m3newdat$mm4pred <- predict(mm4, newdata = m3newdat)
Error in UseMethod("predict") :
  no applicable method for 'predict' applied to an object of class "mer"

Have I gotten shunted to the wrong lme4 packaging?

> sessionInfo()
R version 2.15.1 (2012-06-22)
Platform: x86_64-pc-linux-gnu (64-bit)

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
 [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
 [7] LC_PAPER=C                 LC_NAME=C
 [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] lme4_0.999999-0 Matrix_1.0-6    lattice_0.20-10 MASS_7.3-19

loaded via a namespace (and not attached):
[1] compiler_2.15.1 grid_2.15.1     nlme_3.1-104    stats4_2.15.1
[5] tools_2.15.1
>

If you put the predict method back, you'll get a huge, colorful, and
massively entertaining payoff from mlm-2.R.

Please don't critique the code too harshly.  I'm making a coding
tutorial and all beautiful things start ugly.  Once this works, then
mlm-3.R will be the "tight, clean" version that re-organizes all the
rambling script into functions and more obviously re-usable code.  But
not predict? Really?

## Paul Johnson <pauljohn at ku.edu>
## 2012-06-24

## mlm-2. This is step 2 in the journey

## Topic: create multi-level data and use regression and lmer to
## estimate it.  This creates 3 different dependent variables,
## y1: ordinary "homoskedastic" regression (no grouping effect0
## y2: clustered "random intercepts"
## y3: clustered "random intercepts" and "random slope" for 1 predictor (x1)

## Change 1. I demonstrated mlm-1.R for James Selig, who suggested
## it was important to have clusters made up of various numbers of
## observations (unbalanced groups). Now there are M groups, but
## the variable N becomes a vector that represents the number of
## respondents in each cluster.

## Change 2. Do better book keeping on the random effects.  Now the
## random effects are drawn from a multivariate normal, allowing
## correlation between them. The random effects matrix is called
## Mreffects, its columns are (b0, b1), representing the intercept and
## slope random effects.  In my opinion, it is easier to specify the
## parameters with standard deviations and correlations.  The Covar
## matrix of the random effects "Msigma" is built up from those
## parameters.

##             | STDEb0  0   | |1  Mrho   | | STDEb0  0   |
## Msigma = |             |X|          |X|             |
##            |  0   STDEb1 | |Mrho    1 | |  0   STDEb1 |

library(MASS) ## for rmvnorm
set.seed(1234) ## for replicability

M <- 100  ## Clusters
## For equal-sized clusters with 10 observations, do this
## N <- rep(10, M)
## There are many ways to get varying cluster sizes.
##
N <- 134 + rpois(M, lambda=20)
## I add 4 because a cluster with fewer than 4 observations
## will cause a regression on the separate cluster to fail

## get M unique gray colors, will use later for plotting
## mycolors <- gray.colors(M)

## More colorful mycolors ?
mycolors <- rainbow(M)


## Create Mind and Iind indexes for book keeping.
## Mind, "M index" shows with which cluster an observation belongs.
Mind <- unlist(lapply(1:M, function(x) rep(x, each=N[x])))
## Iind, "I index" indexes cluster members.
Iind <- unlist(lapply(1:M, function(x) seq(1, N[x], by=1)))

##Create 3 variables from a multivariate Normal with these
## characteristics
Xmeans <- c(100, 200, 150)
Xsds <- c(20, 30, 40)
Xrho12 <- 0.4
Xrho13 <- 0.2
Xrho23 <- 0.0
Xcorr.mat <- diag(1, 3)
Xcorr.mat[1,2] <- Xcorr.mat[2,1] <- Xrho12
Xcorr.mat[1,3] <- Xcorr.mat[3,1] <- Xrho13
Xcorr.mat[2,3] <- Xcorr.mat[3,2] <- Xrho23
if(!isSymmetric(Xcorr.mat))stop("Xcorr.mat is not symmetric")

Xsigma <- diag(Xsds) %*% Xcorr.mat %*% diag(Xsds)
X.mat <- mvrnorm(n = sum(N), mu = Xmeans, Sigma = Xsigma)
dimnames(X.mat)[[2]] <- c("x1", "x2", "x3")
## Don't forget to insert an intercept later
X.mat <- cbind(X.mat)




## The true fixed effects of b0, b1, b2, b3 in
## y = b0 + b1*x1 + b2*x2 + b3*x3
bslopes <- c(1.5, 0.25, -0.2, 0.05)

## Standard deviation of error term at individual level
STDE <- 15

## Create a dependent variable that has no clustering effects.
##     FIXED Part                 +  RANDOM PART
y1 <- cbind(1, X.mat) %*% bslopes + rnorm(sum(N), m = 0, s = STDE)
dat <- cbind(data.frame(Mind, Iind, y1), as.data.frame(X.mat))
rownames(dat) <- paste(dat$Mind, dat$Iind, sep=".") ##may help bookkeeping later
rm(Mind, Iind, y1, X.mat) ## cleanup workspace

##Check the R-square on fitted model.
summary(lm(y1 ~ x1 + x2 + x3, data=dat))$r.square

## Layer on additive group level error in y2 and y3


## Parameters for random effects:
## STDEb0: standard deviation of clustered intercepts.
STDEb0 <- 0.5
## STDEb1: standard deviation of slopes across cluster units
STDEb1 <- 0.05
Mrho <- 0.2
## I'm tempted to get fancy here with a matrix for STDEb0, STDEb1, and
## Mrho. It would pay off, I expect. But be harder to teach.
Msigma <- diag(c(STDEb0, STDEb1)) %*% matrix(c(1, Mrho, Mrho, 1), 2,
2) %*% diag(c(STDEb0, STDEb1))
Mreffects <- mvrnorm(M, mu = c(0, 0), Sigma = Msigma)
colnames(Mreffects) <- c("b0","b1")

## In y2, include random intercept
dat$y2 <- dat$y1 + Mreffects[ ,"b0"][dat$Mind]

## In y3, add in random slope effect
dat$y3 <- dat$y2 +  dat$x1 * Mreffects[ ,"b1"][dat$Mind]

## Do some diagnostics on Mreffects
plot(Mreffects[, 1], Mreffects[,2], xlab = "Intercept Random Effect",
ylab = "Slope Random Effect",  main = "True Random Effects")
##library(rockchalk)
##summarize(Mreffects)
summary(Mreffects)
apply(Mreffects, 2, sd)
cor(Mreffects)

plot(Mreffects[ ,"b0"][dat$Mind], dat$x1 * Mreffects[
,"b1"][dat$Mind], col=dat$Mind, main = "Random Slopes and Intercepts",
xlab="Intercept Random Effect (b0)", ylab="Slope Random Effect
(b1*x)")

## The "grand" regression?
m1 <- lm(y3 ~ x1 + x2 + x3, data=dat)
summary(m1)

## The "dummy variable" regression?
m2 <- lm(y3 ~ x1 + x2 + x3 + as.factor(Mind), data=dat)
summary(m2)


library(lme4)
## M separate regressions, with 3 predictors
m3list <- lmList(y3 ~ x1 + x2 + x3 | Mind, data=dat, pool = FALSE)

## Predicted values set x2 and x3 at their cluster-specific means.

plot(y3 ~ x1, data=dat, col=mycolors[Mind], lwd = 0.6, main = "lm on clusters")
for( i in seq_along(m3list)){
    m3mf <- model.frame(m3list[[i]]) #data set for group i
    x1range <- range(m3mf$x1) ## use group-specific ranges this time
    pgroup <- predict( m3list[[i]], newdata = data.frame(x1=x1range,
x2=mean(m3mf$x2), x3=mean(m3mf$x3)))
    lines(x1range, pgroup, col=mycolors[i])
}

## If a cluster has 3 or fewer cases, this warning will appear
## Warning message:
## In predict.lm(m3list[[i]], newdata = data.frame(x1 = x1range, x2 =
mean(m3mf$x2),  :
##   prediction from a rank-deficient fit may be misleading


## Keep a record.
m3newdat <- lapply(m3list, function(x) {
    m3mf <- model.frame(x)
    ndf = data.frame(x1=range(m3mf$x1), x2=mean(m3mf$x2), x3=mean(m3mf$x3))
    ndf$m3pred <- predict(x, newdata = ndf)
    ndf} )
m3newdat <- do.call("rbind", m3newdat)
## Better add a variable Mind. This way seems like overkill, but
probably is more
## robust than rep(1:M, each=2). It capitalizes on row names
m3newdat$Mind <-  as.integer(do.call("rbind",
strsplit(row.names(m3newdat), split="\\.", perl=T))[ ,1])

## Draw new graphs on a new device, so we can compare
dev.new()


##
## Estimate this as a mixed-effects model with lme4

## mm2: Just the random intercept, no random slope
mm2 <- lmer( y2 ~ x1 + x2 + x3 + (1 | Mind), data=dat)
summary(mm2)
mm2VarCorr <- VarCorr(mm2)
mm2VarCorr

cor(fitted(mm2), dat$y2)

## Both random intercept and random slope, not correlated with each other
## Depending on Mrho, this may be a 'wrong model'.
mm3 <- lmer( y3 ~ x1 + x2 + x3 + (1|Mind) + (0 + x1 | Mind), data=dat,
verbose=3)
summary(mm3)
mm3VarCorr <- VarCorr(mm3)
mm3VarCorr

cor(fitted(mm3), dat$y3)



## Yank out the standard deviation estimates. idiom stolen from
formatVC in lmer code.
mm3reStdDev <- c(lapply(mm3VarCorr, attr, "stddev"), list(Residual =
attr(mm3VarCorr, "sc")))
mm3reStdDev


mm3reStdDev[["Mind"]]["(Intercept)"] - STDEb0
##mm3reStdDev[["Mind"]]["x1"] - STDEb1
## Can't figure how to get STD(b1)
mm3reStdDev[["Residual"]] - STDE

## The "right" model allows the random effects to be correlated (Supposing
## Mrho not 0).
mm4 <- lmer( y3 ~ x1 + x2 + x3 + (x1 | Mind), data=dat)
summary(mm4)
mm4VarCorr <- VarCorr(mm4)
mm4VarCorr

cor(fitted(mm4), dat$y3)

plot(fitted(mm4), dat$y3, col=dat$Mind)



## Yank out the standard deviation estimates
mm4reStdDev <- c(lapply(mm4VarCorr, attr, "stddev"), list(Residual =
attr(mm4VarCorr, "sc")))
mm4reStdDev


## Ever do summary stats on the predicted random effects?
mm4ranef <- ranef(mm4, postVar = TRUE) ## a ranef.mer object,  a list
that includes one data frame
apply(mm4ranef[["Mind"]], 2, mean)
apply(mm4ranef[["Mind"]], 2, sd)
#summarize(mm4ranef$Mind)
summary(mm4raner$Mind)
cor(mm4ranef$Mind)

dotplot(mm4ranef)

m3newdat$mm4pred <- predict(mm4, newdata = m3newdat)

plot(y3 ~ x1, data=dat, col=mycolors[Mind], main="lmer mixed model predictions")
by(m3newdat, m3newdat$Mind, function(x) { lines(x$x1, x$mm4pred,
col=mycolors[x$Mind])})


## Double-check prediction values
mm4b <- fixef(mm4) ## coef(summary(mm4))[ ,1]
mm4vnames <- names(mm4b)
##  b0 + raneff(intercept, j)
mm4inteffect <- mm4b["(Intercept)"] +  mm4ranef[[1]][m3newdat$Mind, 1]
mm4x1effect <- m3newdat[ , c("x1")] * (mm4b["x1"] +
mm4ranef[[1]][m3newdat$Mind, 2] )
mm4mmpred2 <- mm4inteffect + mm4x1effect +  as.matrix(m3newdat[
,c("x2","x3") ]) %*%  mm4b[c("x2","x3")]
m3newdat$mm4manualpred <-  mm4inteffect + mm4x1effect +
as.matrix(m3newdat[ ,c("x2","x3") ]) %*%  mm4b[c("x2","x3")]

cbind(m3newdat$mm4manualpred, m3newdat$mm4pred)

## OK, looks good
plot(y3 ~ x1, data=dat, col=mycolors[Mind], main="lmer mixed model predictions")
by(m3newdat, m3newdat$Mind, function(x) { lines(x$x1, x$mm4manualpred,
col=mycolors[x$Mind])})




## Now grab estimates from lm and lmer for closer inspection.
## intercepts on M separate lm fits:
m3b0  <- sapply(m3list, function(m) coef(m)["(Intercept)"])
sd(m3b0)

### The intercept "predictions" (blups) from mm4
mm4b0 <- mm4b["(Intercept)"] + mm4ranef[[1]][, "(Intercept)"]
sd(mm4b0)

###The slope estimates from M separate lm fits:
m3x1b  <- sapply(m3list, function(m) coef(m)["x1"])
sd(m3x1b)

### The slope "predictions" (blups) from mm4
mm4x1b <- mm4b["x1"] + mm4ranef[[1]][, "x1"]
sd(mm4x1b)


## Are the mm4 b's better than lm's? Closer to expected values?
Shrunken toward center?
op <- par(no.readonly = TRUE)
par(mar=c(5.1, 5.1, 4.1, 2.1))
plot(m3x1b, mm4x1b, xlab = expression(hat(b)[1] ~~from~~lm), ylab =
expression(hat(b)[1] ~~from~~lmer~~mixed~~model),
main=expression(paste("Shrinkage: Scatter of ",hat(b)[1]," from lm and
lmer" )))
mx1b <- lm( mm4x1b ~ m3x1b)
abline(mx1b)
summary(mx1b)
options(digits=2)
legend("bottomright",
as.expression(c(bquote(widehat(hat(b)[1*j]^{lmer}) == .(coef(mx1b)[1])
+ .(coef(mx1b)[2])*hat(b)[1*j]))), lty=1, lwd=1)
options(digits=6)
par(op)


## Let's compare histograms of the slope estimates
par(mfcol=c(1,2))
par(mar=c(5.1, 4.1, 6.1, 2.1), xpd=T)

## Uses rockchalk: b1breaks <- plotSeq(range(c(m3x1b, mm4x1b)), 20)
b1breaks <- seq(range(c(m3x1b, mm4x1b))[1], range(c(m3x1b,
mm4x1b))[2], length.out = 20)
m3x1bhist <- hist(m3x1b, breaks=b1breaks, plot = FALSE)
m4x1bhist <- hist(mm4x1b, breaks =b1breaks, plot = FALSE)
## Uses rockchalk: ylim <- magRange(c(0, m3x1bhist$density,
m3x1bhist$density),  c(1, 1.2))

ylim <- range(c(0, m3x1bhist$density, m3x1bhist$density))*c(1, 1.2)
hist(m3x1b, breaks=b1breaks, prob = TRUE, ylim=ylim,
xlab=expression(paste("100 lm estimates ", hat(b)[1])),
main=paste("Separate OLS (lm)"))
legend("topright", legend = as.expression(c(
bquote(mean(hat(b)[1])==.(round(mean(m3x1b),2))),
bquote(sd(hat(b)[1])==.(round(sd(m3x1b),2))))))

hist(mm4x1b,  breaks=b1breaks, prob = TRUE, ylim=ylim,
xlab=expression(paste("lmer 'blup' estimates ", hat(b)[1])),
main="Mixed Model (lmer)")
legend("topleft", legend = as.expression(c(
bquote(mean(hat(b)[1])==.(round(mean(mm4x1b),2))),
bquote(sd(hat(b)[1])==.(round(sd(mm4x1b),2))))))
par(op)


## Plot the estimated slopes against the "true" random effects from Mreffects
plot(bslopes[2] + Mreffects[ ,2], m3x1b, xlab="Cluster Slopes (True
Values)", ylab="Estimates of Slopes (across Clusters)", col="gray70",
xlim= c(1.04,1.04)*range(c(bslopes[2] + Mreffects[ ,2], m3x1b)),
ylim=c(1.04,1.04)*range((c(bslopes[2] + Mreffects[ ,2], m3x1b))))
points(bslopes[2] + Mreffects[ ,2], mm4x1b, , col="black", pch=13)
legend("topleft", c("OLS Slopes", "Mixed Model Slopes"), pch=c(1,13),
col=c("gray70","black"))



## Plot the estimated intercept effects against the "true" random
effects from Mreffects
plot(bslopes[1] + Mreffects[ ,1], m3b0, xlab="Cluster Intercepts (True
Values)", ylab="Estimates of Intercepts", col="gray70")
points(bslopes[1] + Mreffects[ ,1], mm4b0 , col="black", pch=13)
legend("topleft", c("OLS Intercepts", "Mixed Model Intercepts"),
pch=c(1,13), col=c("gray70","black"))


plot(Mreffects[, 1], m3b0)

plot(Mreffects[, 1], mm4b0)



-- 
Paul E. Johnson
Professor, Political Science    Assoc. Director
1541 Lilac Lane, Room 504     Center for Research Methods
University of Kansas               University of Kansas
http://pj.freefaculty.org            http://quant.ku.edu


From jackson.king722 at gmail.com  Sun Aug 26 16:28:45 2012
From: jackson.king722 at gmail.com (Jackson King)
Date: Sun, 26 Aug 2012 09:28:45 -0500
Subject: [R-sig-ME] MCMCglmm multinomial logit
Message-ID: <CABHLxrkeAYYzjfv1MDvKZ4erp2s_76Ns8BvqYdQx51fWZR=otw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120826/746a3ab5/attachment.pl>

From highstat at highstat.com  Mon Aug 27 09:11:53 2012
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Mon, 27 Aug 2012 08:11:53 +0100
Subject: [R-sig-ME] Dealing with heteroscedasticity in a GLM/M
In-Reply-To: <9BEE83C6BF59174DBD2CB4A06EB6CBEB2FDEBAFE@SINPRD0111MB368.apcprd01.prod.exchangelabs.com>
References: <9BEE83C6BF59174DBD2CB4A06EB6CBEB2FDEBAFE@SINPRD0111MB368.apcprd01.prod.exchangelabs.com>
Message-ID: <503B1DB9.2050203@highstat.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120827/be4fd1a3/attachment.ksh>

From andreas.boeck at tum.de  Mon Aug 27 12:25:23 2012
From: andreas.boeck at tum.de (=?ISO-8859-1?Q?Andreas_B=F6ck?=)
Date: Mon, 27 Aug 2012 12:25:23 +0200
Subject: [R-sig-ME] GLMM with mgcv::gam
In-Reply-To: <503B45DA.5050508@web.de>
References: <503B45DA.5050508@web.de>
Message-ID: <503B4B13.9030307@tum.de>

Dear Mixed-Model Experts,

my question is about the mgcv::gam function in a recent version (version
1.7-19 or 1.7-20).

Is it true, that gam(y ~ x1+ s(id, bs="re"), method ="REML",
family=binomial) does the following:
- setup the model
- call MASS::glmmPQL where
    glmmPQL somehow iterates between nlme::lme and glm.fit calls
- make the results look like a gam object

The method="REML" argument affects only the lme part in the fitting
procedure ?

If the above is true, is it prefarable to use glmer(y~x1 + (1|id),
family="binomial") or gamm4, as the approach in the lme4 package is more
reliable as the penalized-quasi-likelihood approach in package MASS,
especially for binomial families?

Thank you very much for clarification !

Best,
Andi B?ck
(PhD candidate, Munich)


From andreas.boeck at tum.de  Mon Aug 27 14:55:49 2012
From: andreas.boeck at tum.de (=?ISO-8859-1?Q?Andreas_B=F6ck?=)
Date: Mon, 27 Aug 2012 14:55:49 +0200
Subject: [R-sig-ME] GLMM with mgcv::gam
In-Reply-To: <mailman.5.1345888802.22555.r-sig-mixed-models@r-project.org>
References: <mailman.5.1345888802.22555.r-sig-mixed-models@r-project.org>
Message-ID: <503B6E55.9030803@tum.de>

Dear Mixed-Model Experts,

my question is about the mgcv::gam function in a recent version (version 
1.7-19 or 1.7-20).

Is it true, that gam(y ~ x1+ s(id, bs="re"), method ="REML", 
family=binomial) does the following:
- setup the model
- call MASS::glmmPQL where
    glmmPQL somehow iterates between nlme::lme and glm.fit calls
- make the results look like a gam object

The method="REML" argument affects only the lme part in the fitting 
procedure ?

If the above is true, is it prefarable to use glmer(y~x1 + (1|id), 
family="binomial") or gamm4, as the approach in the lme4 package is more 
reliable as the penalized-quasi-likelihood approach in package MASS, 
especially for binomial families?

Thank you very much for clarification !

Best,
Andi B?ck
(PhD candidate, Munich)

PS: Sorry for eventually double posting, got an error from naver.com, I 
didn't understand


From pauljohn32 at gmail.com  Mon Aug 27 18:53:15 2012
From: pauljohn32 at gmail.com (Paul Johnson)
Date: Mon, 27 Aug 2012 11:53:15 -0500
Subject: [R-sig-ME] Dealing with heteroscedasticity in a GLM/M
In-Reply-To: <9BEE83C6BF59174DBD2CB4A06EB6CBEB2FDEB86C@SINPRD0111MB368.apcprd01.prod.exchangelabs.com>
References: <9BEE83C6BF59174DBD2CB4A06EB6CBEB2FDEB86C@SINPRD0111MB368.apcprd01.prod.exchangelabs.com>
Message-ID: <CAErODj9NbjtwrEPkczMYT-yLmTfmmu2meDyyDi0jDzRTN9T+mg@mail.gmail.com>

On Thu, Aug 23, 2012 at 12:58 AM, Leila Brook <leila.brook at my.jcu.edu.au> wrote:
> I am hoping to find a way to account for heterogeneity of variance between categories of explanatory variables in a generalised model.
>
> I have searched books and this forum, and haven't found any advice that I understood could help account for this assumption in a generalised model context, as I can't fit a variance structure in lme4.
>
>
>
> As background to my study:
>
> I used camera stations set up in pairs (one positioned on a track and one off the track) to record my study species, and used the same pairs in each of two seasons. As my surveys were repeated, I have specified camera pair as a random effect. I am using a binomial model in lme4 to model the proportion of nights an animal was recorded, as a function of the fixed effects of season (2 categories), position (categorical: whether on or off the track), area (categorical: one of two areas) and continuous habitat variables, plus interactions between them.
>
>
>
> I validated the GLM form of the model, including plotting the deviance residuals against my explanatory variables, and have noted that the variance of residuals for the categorical variables appears to differ.
>

Stop there.

In Logit/Probit frameworks, the variance is assumed equal for all
groups. It is never estimated.  The model is not identified otherwise.
The effect of heteroskedasticity is not just inefficiency, but
parameter bias. This makes logit models much more suspicious than
previously believed. This means that all of the work you have done so
far to "validate" your model is dubious and you need to take a step
back.

We are in a bind with logit models. Either we estimate separate models
for the separate groups (to avoid heteroskedasticity), but we are not
able to compare coefficients across models because there is that
different, but un-estimated variance.  Or we fit one model that
combines the group, make the wrong assumption, and end up with wrong
parameter estimates.  I don't mean just a little off. I mean wrong.
Its discouraging.

As far as I know, this problem was first popularized by Paul Allison,
Scott Long, and Richard Williams, but it is nicely surveyed in this
review essay:

Mood, Carina. 2010. Logistic Regression: Why We Cannot Do What We
Think We Can DO, and What We Can Do About It. European Sociological
Review 26(1): 67-82.

That has cites to the earlier Allison paper and some of Williams's work.

In my opinion, there are no completely safe approaches to dealing with
the heteroskedastic group-level error.  Richard Williams at Notre Dame
gave an excellent presentation about it.  He told me he has a paper
forthcoming in the Stata journal about it, but I don't feel free to
pass it along to you. But I bet his website has more information.

It seems to me that if you try to "pin" one group as the "baseline
variance" group and then add properly structured random effects for
the other ones, you might get a handle on it. The R package dglm has
suggestions like that.

Good luck. If you get an answer, I'd really like to know what is the
state of the art now (this minute)...

-- 
Paul E. Johnson
Professor, Political Science    Assoc. Director
1541 Lilac Lane, Room 504     Center for Research Methods
University of Kansas               University of Kansas
http://pj.freefaculty.org            http://quant.ku.edu


From j.hadfield at ed.ac.uk  Mon Aug 27 19:23:41 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Mon, 27 Aug 2012 18:23:41 +0100
Subject: [R-sig-ME] MCMCglmm multinomial logit
In-Reply-To: <CABHLxrkeAYYzjfv1MDvKZ4erp2s_76Ns8BvqYdQx51fWZR=otw@mail.gmail.com>
References: <CABHLxrkeAYYzjfv1MDvKZ4erp2s_76Ns8BvqYdQx51fWZR=otw@mail.gmail.com>
Message-ID: <20120827182341.78871bjh8feszoyo@www.staffmail.ed.ac.uk>

Hi Jackson,

If x1 is the fixed effect prediction for the log contrast B-A then

plogis(x1/sqrt(1+v1*c2))

gives the expected probability of being B rather than A after  
integrating over the random effects (in your case the residuals under  
the assumption that they have unit variance v1=1). You can write it  
another way:

Have nu1 = x1+e1

plogis(x1/sqrt(1+v1*c2)) \approx  int plogis(nu1)pnorm(e1,0,sqrt(v1))de

and we can see the approximation is pretty good:

x1<-2
c2<-0.345843
v1<-1

plogis(x1/sqrt(1+v1*c2))
integrate(function(e1){plogis(x1+e1)*dnorm(e1,0,sqrt(v1))}, -Inf,Inf)

Obtaining the expected probability of being B rather than A, or C  
rather than A, is therefore relatively straightforward.

Obtaining the expected probability of B rather than C is a bit more tricky.

take nu2 = log contrast C-A

nu1-nu2 = log contrast B-C

which is

x1+e1-x2+e2

and is distributed as N(x1-x2, V[1,1]+V[2,2]-2*V[1,2]) where in you case V=R.

We can therefore use the same approximation as before replacing x1  
with x1-x2 and v1 with V[1,1]+V[2,2]-2*V[1,2].

If we chose V = IJ  then V[1,1] = V[2,2] = V[1,1]+V[2,2]-2*V[1,2] and  
so we multiply c2 by the same constant for all contrasts. This was the  
main motivation. In fact, it might make more sense to scale IJ by  
1/(j-1) rather than 1/j so all contrasts have unit variance, but  
anything could be used really as long as you are careful.

Looking at it now, the justification that these results/approximations  
extend to the case A versus (B and C) as I suggest in the CourseNotes  
seems more tenuous and I will check it out.

Cheers,

Jarrod





Quoting Jackson King <jackson.king722 at gmail.com> on Sun, 26 Aug 2012  
09:28:45 -0500:

> Hello,
>
> I am attempting to fit a multinomial logit model (3 categories) with random
> effects (across states). I have attempted to follow the advice in the
> course notes, but am a little uncertain the reason for the priors on the
> residuals, and how this is used to calculate predicted probabilities. I
> would like my covariates to predict each outcome -- cov1 predicting option3
> versus option1 and cov1 predicting option2 versus option1, rather than a
> main effect.
>
> Here is a simple version of my model
>
> j<-length(levels(data$char))
>
> I<-diag<-(j-1) #2x2 identity matrix
>
> J=matrix(rep(1, (j-1)^2), c(j-1, j-1)) #2x2 Unit Matrix
>
> IJ <- (1/3) * (diag(2) + matrix(1, 2, 2)) #Residual covriance matrix
>
> prior = list(R = list(V =IJ, fix = 1), G = list(G1 = list(V = diag(2), nu =
> 0.002))) #Why can't I use V=diag(2) for prior R?
>
>
> model<- MCMCglmm(char~-1+trait*(cov1), random=~idh(trait):state, rcov = ~us(
> trait):units, data=data, family = "categorical", prior = prior, verbose =
> TRUE)
>
>
> I find that the model converges, and the coefficients look similar to
> maximum likelihood, but then I would like to predict probabilities for
> being in each category. Typically, I think this is done by
> plogis(x/sqrt(1+c2)), so why is it necessary to multiply by the delta
> matrix (course notes p. 97)? Alternatively, if I simply use a 2x2 diagonal
> matrix for the prior for R, shouldn't I be able to use the same
> transformation -- plogis(x/sqrt(1+c2)). In short, I am a little confused
> about the IJ matrix and where it comes from. Is there a quick answer, or
> another paper that explains this? And (2) is it reasonable to predict
> probabilities from my model, based on fixed values of cov1, using this
> simple transformation above... plogis(x/sqrt(1+c2))
>
> Many thanks.
>
> Jackson
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From j.hadfield at ed.ac.uk  Mon Aug 27 19:26:25 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Mon, 27 Aug 2012 18:26:25 +0100
Subject: [R-sig-ME] Questions about MCMCglmm and marker data
In-Reply-To: <20120823101046.GB26811@info124.pharmacie.univ-paris5.fr>
References: <5035FDE9.6010801@cirad.fr>
	<20120823101046.GB26811@info124.pharmacie.univ-paris5.fr>
Message-ID: <20120827182625.8866720cbjenthoo@www.staffmail.ed.ac.uk>

Hi,

It is the effects that are to be considered normal, not the way the  
genotypes are coded.

It is not possible to fit this model in the current version of  
MCMCglmm, but it will be available in the next. I hope to submit this  
version to CRAN in the next couple of weeks.

Cheers,

Jarrod






Quoting Emmanuel Curis <curis at pharmacie.univ-paris5.fr> on Thu, 23 Aug  
2012 12:10:46 +0200:

> Hello,
>
> I'm not familiar with this kind of genomic problems, but if the
> columns are the number of a given SNP for a given individual and can
> be only 0, 1 and 2, is this really possible to consider they follow a
> normal distribution?
>
> On Thu, Aug 23, 2012 at 11:54:49AM +0200, Marie Denis wrote:
> ? Hi,
> ?
> ? I use the MCMCglmm function in the genomic selection context in the
> ? univariate case. In fact I have for each trait one marker matrix
> ? constituted of 0,1 and 2. The rows are the individuals and the columns
> ? the SNPs. In a first time, we consider that each SNP follow a normal
> ? distribution with the *same *variance. So I use the following model:
> ?
> ? prior.1.1 <- list(G=list(G1=list(V=diag(x = as.numeric(scale), nrow=1,
> ? ncol=1),nu=ddl),
> ?                            R=list(V=matrix(scale),nu=ddl))
> ?
> ? mcmc.fit.1.1 <- MCMCglmm(P~ 1,random=~idv(SNP),prior=prior.1.1,
> ?                    data=data1.1,
> ?                    nitt=5000, burnin=1000,verbose=FALSE,
> ?                    thin=10,pr=TRUE)
> ?
> ? So, I obtained a common variance associated to my SNPs.
> ?
> ?
> ? The second step is a bivariate analysis. I would like to obtain a
> ? (co)variance matrix 2*2  associated to the trait1 and trait 2 and the
> ? correlation between both. (one variance for the SNPs for the trait 1 and
> ? one for the trait 2). But I don't know how i can do. The following model
> ? give me only one variance for all SNPs and both traits.
> ?
> ?      prior.3<- list(G=list(G1=list(V=diag(x = as.numeric(scale), nrow=1,
> ?                            ncol=1),n=1)),
> ?                R=list(V=diag(x=as.numeric(0.1),nrow=2,ncol=2),n=2))
> ?
> ?      mcmc.fit.3 <- MCMCglmm(cbind(P1,P2)~ trait-1,random=~idv(trait:SNP),
> ?                    rcov=~idh(trait):units,
> ?                    prior=prior.3,
> ?                    data=data1.3,family=c("gaussian","gaussian"),
> ?                    nitt=2000, burnin=500,verbose=FALSE,
> ?                    thin=10,pr=TRUE)
> ?
> ?
> ?
> ? thanks for your help,
> ?
> ?
> ?
> ?
> ? 	[[alternative HTML version deleted]]
> ?
> ? _______________________________________________
> ? R-sig-mixed-models at r-project.org mailing list
> ? https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> --
>                                 Emmanuel CURIS
>                                 emmanuel.curis at univ-paris5.fr
>
> Page WWW: http://emmanuel.curis.online.fr/index.html
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From matz at utexas.edu  Mon Aug 27 07:10:30 2012
From: matz at utexas.edu (Mikhail Matz)
Date: Mon, 27 Aug 2012 00:10:30 -0500
Subject: [R-sig-ME] bayes factors for MCMCglmm
Message-ID: <C657AD3B-2DEE-4373-93D4-E465212DCEF4@utexas.edu>

Hello - is it possible to calculate hayes factors for MCMCglmm objects, to compare alternative models?

Mikhail


From bbolker at gmail.com  Tue Aug 28 02:21:23 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 28 Aug 2012 00:21:23 +0000 (UTC)
Subject: [R-sig-ME]
	=?utf-8?q?who_shot_the_predict_method_=28and_did_you_s?=
	=?utf-8?q?hoot_the=09deputy_as_well=3F=29?=
References: <CAErODj8FZVw9owRE+U49MfLrckTjMOos3MJFDrwj1Sxw=_0E7Q@mail.gmail.com>
Message-ID: <loom.20120828T011347-203@post.gmane.org>

Paul Johnson <pauljohn32 at ...> writes:

> 
> It seems like I can't leave for a month or two without pieces of mount
> Rushmore falling off of lme4.
> 
> Remember last June, I posted about some troubles with the predict
> method and you fixed it.

  Quick question: did predict() *ever* exist for the stable version
of lme4?  (I don't think so, although I may of course be mistaken ...)
  Were you working with the development (r-forge) version
of lme4 before?

  I will take a look at your code ...

  Ben Bolker

> 
> And I had some other trouble with fishy looking variance estimates
> from models specified like (1|id) + (0 +x|id).
> I stopped fighting with that because I just couldn't understand it at
> all, but today I took it up again.
> 
> Now the fishy looking variance estimates are completely solved
> (yeah!), but somebody stole the predict function entirely:
> 
> > m3newdat$mm4pred <- predict(mm4, newdata = m3newdat)
> Error in UseMethod("predict") :
>   no applicable method for 'predict' applied to an object of class "mer"
> 
> Have I gotten shunted to the wrong lme4 packaging?


From savfrank at hotmail.com  Tue Aug 28 03:39:26 2012
From: savfrank at hotmail.com (Frank Savage)
Date: Mon, 27 Aug 2012 20:39:26 -0500
Subject: [R-sig-ME] =?windows-1256?q?Linear_Mixed_Model_with_Twin_Data=3B_?=
 =?windows-1256?q?Heteroscedasticity=3F=FE?=
Message-ID: <COL112-W27961BE589617195DB6684DBA10@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120827/082f6ae2/attachment.pl>

From ihok at hotmail.com  Tue Aug 28 04:14:50 2012
From: ihok at hotmail.com (Jack Tanner)
Date: Mon, 27 Aug 2012 22:14:50 -0400
Subject: [R-sig-ME] lme4 book pdf
Message-ID: <BLU0-SMTP39559DE3AC72DEB68CF7B48CAA10@phx.gbl>

Building the lme4 book from source is a bit painful right now, because 
lme4a cannot be installed from CRAN. Does anyone have a pretty recent 
PDF of the book that they could be make available?


From ihok at hotmail.com  Tue Aug 28 04:14:58 2012
From: ihok at hotmail.com (Jack Tanner)
Date: Mon, 27 Aug 2012 22:14:58 -0400
Subject: [R-sig-ME] lme4 book pdf
Message-ID: <BLU0-SMTP390488C06E3A6162DC333E2CAA10@phx.gbl>

Building the lme4 book from source is a bit painful right now, because 
lme4a cannot be installed from CRAN. Does anyone have a pretty recent 
PDF of the book that they could make available?


From bbolker at gmail.com  Tue Aug 28 04:43:23 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 28 Aug 2012 02:43:23 +0000 (UTC)
Subject: [R-sig-ME] bayes factors for MCMCglmm
References: <C657AD3B-2DEE-4373-93D4-E465212DCEF4@utexas.edu>
Message-ID: <loom.20120828T043351-7@post.gmane.org>

Mikhail Matz <matz at ...> writes:

>  Hello - is it possible to calculate hayes factors for MCMCglmm
> objects, to compare alternative models?  Mikhail


Well, MCMCglmm gives you a posterior sample for the deviance, so
in principle you could calculate the harmonic mean of the likelihood,
e.g.

data(PlodiaPO)  
model1<-MCMCglmm(PO~1, random=~FSfamily, data=PlodiaPO, verbose=FALSE)

likchain <- exp(-model1$Deviance/2)
1/mean(1/likchain)

  these are ridiculously big numbers -- that may have something
to do with the way the deviance is scaled.

  However, harmonic means are known to be really, really
dodgy estimates of the Bayes factor -- see e.g.

http://scholarworks.umass.edu/cgi/viewcontent.cgi?
   article=1107&context=astro_faculty_pubs
(warning, broken URL)

  Perhaps someone else will contribute something more useful.
  You could also use DIC to compare models, although it has
its own issues ...


From savfrank at hotmail.com  Tue Aug 28 05:08:58 2012
From: savfrank at hotmail.com (Frank Savage)
Date: Mon, 27 Aug 2012 22:08:58 -0500
Subject: [R-sig-ME]
 =?windows-1256?q?Linear_Mixed_Model_with_Twin_Data=3B_?=
 =?windows-1256?q?Heteroscedasticity=3F=FE?=
In-Reply-To: <COL112-W27961BE589617195DB6684DBA10@phx.gbl>
References: <COL112-W27961BE589617195DB6684DBA10@phx.gbl>
Message-ID: <COL112-W30B2B0944E51A29F915F80DBA10@phx.gbl>


Very sorry!! -- first time poster and it looks like you have to follow an odd link to get to my question. I've relayed it here below for convenience.

(Let me preface this by stating that I am somewhat of an R newb and also a LMM newb.)

I have a very, very simple research question. I have a continuous outcome variable ("HAPPY"). I have two factors that divide my sample into 4 cells (Factor1, Factor2), and I want to assess whether there are any main effects or an interactions. A simple 2 x 2 ANOVA should do the trick.

There are two issues:

1) The individuals in my sample are family members. Every single individual has a fraternal twin or an identical twin in the dataset. I know that this throws off all the independence assumptions, so I was instructed to use linear mixed models and include the family identification variable (which identifies an individual as being part of a twin pair) ("Family") in my dataset as a random effect.

Right now, this is my R syntax:
model <- lme(HAPPY ~ Factor1*Factor2, random= ~1|Family, data=mydata, method="REML", na.action=na.omit)
summary(model)
anova(model,type="marginal")

So right now, there is only one random effect. But -- theoretically, we'd expect that identical twins and fraternal twins would differ in their degree of relatedness (the variable is called "twintype"). Should this be reflected in my model, and if so...how? Conceptually, I'm struggling with how it would be specified. And practically, I'm also struggling -- putting the above syntax together was actually quite the accomplishment for me!

2) Heteroscedasticity. What is the best way to assess if heteroscedasticity is affecting my model? Right now I'm just eyeballing --
plot(fitted(model), resid(model)) and looking for cone shapes.

And if heteroscedasticity is an issue, what can I do to fix it? From what I've gathered from the google machine, I can enter a weighting command that will account for the inconstant variance. Could I modify my syntax above to read --
model <- lme(HAPPY ~ Factor1*Factor2, random= ~1|Family, data=mydata, method="REML", na.action=na.omit, weights=varPower())


From j.hadfield at ed.ac.uk  Tue Aug 28 09:51:29 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Tue, 28 Aug 2012 08:51:29 +0100
Subject: [R-sig-ME] bayes factors for MCMCglmm
In-Reply-To: <loom.20120828T043351-7@post.gmane.org>
References: <C657AD3B-2DEE-4373-93D4-E465212DCEF4@utexas.edu>
	<loom.20120828T043351-7@post.gmane.org>
Message-ID: <20120828085129.19932k9qd841h5s0@www.staffmail.ed.ac.uk>

Hi,

The fact that the deviance is not calculated after marginalising the  
random effects may also be an issue for Bayes factors, as it can be  
with DIC. But not sure.

Cheers,

Jarrod


Quoting Ben Bolker <bbolker at gmail.com> on Tue, 28 Aug 2012 02:43:23  
+0000 (UTC):

> Mikhail Matz <matz at ...> writes:
>
>>  Hello - is it possible to calculate hayes factors for MCMCglmm
>> objects, to compare alternative models?  Mikhail
>
>
> Well, MCMCglmm gives you a posterior sample for the deviance, so
> in principle you could calculate the harmonic mean of the likelihood,
> e.g.
>
> data(PlodiaPO)
> model1<-MCMCglmm(PO~1, random=~FSfamily, data=PlodiaPO, verbose=FALSE)
>
> likchain <- exp(-model1$Deviance/2)
> 1/mean(1/likchain)
>
>   these are ridiculously big numbers -- that may have something
> to do with the way the deviance is scaled.
>
>   However, harmonic means are known to be really, really
> dodgy estimates of the Bayes factor -- see e.g.
>
> http://scholarworks.umass.edu/cgi/viewcontent.cgi?
>    article=1107&context=astro_faculty_pubs
> (warning, broken URL)
>
>   Perhaps someone else will contribute something more useful.
>   You could also use DIC to compare models, although it has
> its own issues ...
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From kumiko.fukumura at strath.ac.uk  Tue Aug 28 09:55:30 2012
From: kumiko.fukumura at strath.ac.uk (Kumiko Fukumura)
Date: Tue, 28 Aug 2012 08:55:30 +0100
Subject: [R-sig-ME] Dealing with heteroscedasticity in a GLM/M (Paul
	Johnson)
Message-ID: <1F943C50E37E194BB891501E163237A8016B115B3B51@E2K7-MS2.ds.strath.ac.uk>

Hi there,

I'm a psycholinguist, and have only recently started using logit mixed-effects to analyze categorical/binary data.

I was wondering if the issues of heteroscedasticity that have been discussed here also apply to repeated measures designs - where same participants take part in all conditions.



In experimental psycholinguistics, 2 x 2 repeated measures designs are usually favoured, and the interaction between the two factors tends to be one of the main interests of analysis. I've noticed that when the proportion of one of the conditions approaches 0 or 1, we tend to get surprising results for the interactions (something what you do not expect from patterns of means - this may be because we're looking at log-odds).  Also, if the overall mean is close to 0 or 1, coefficient estimates often do not make sense. I guess this is because the variance becomes not equaly when the proportion approaches 0 or 1.



Traditionally, ANOVAs have been the standard analytical tool for proportion data in our area (we normally apply arcsin-transformation on the proportions before submitting them to ANOVAs). But we've been recently encouraged to use mixed-effects mainly because analysing proportions violates the homogeneity of variance assumed by ANOVAs. So I'd like to know if the heterogeneity of variance also affects logit mixed-effects badly (to me, the effect seems more severe with logit mixed effects than with ANOVAs).



Any advice would be much appreciated.



Thank you for your attention.



Kumiko





Dr Kumiko Fukumura

School of Psychological Sciences and Health

University of Strathclyde

40 George Street

Glasgow, G1 1 QE









------------------------------

Message: 3
Date: Mon, 27 Aug 2012 11:53:15 -0500
From: Paul Johnson <pauljohn32 at gmail.com>
To: Leila Brook <leila.brook at my.jcu.edu.au>
Cc: "r-sig-mixed-models at r-project.org"
        <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Dealing with heteroscedasticity in a GLM/M
Message-ID:
        <CAErODj9NbjtwrEPkczMYT-yLmTfmmu2meDyyDi0jDzRTN9T+mg at mail.gmail.com>
Content-Type: text/plain; charset=ISO-8859-1

On Thu, Aug 23, 2012 at 12:58 AM, Leila Brook <leila.brook at my.jcu.edu.au> wrote:
> I am hoping to find a way to account for heterogeneity of variance between categories of explanatory variables in a generalised model.
>
> I have searched books and this forum, and haven't found any advice that I understood could help account for this assumption in a generalised model context, as I can't fit a variance structure in lme4.
>
>
>
> As background to my study:
>
> I used camera stations set up in pairs (one positioned on a track and one off the track) to record my study species, and used the same pairs in each of two seasons. As my surveys were repeated, I have specified camera pair as a random effect. I am using a binomial model in lme4 to model the proportion of nights an animal was recorded, as a function of the fixed effects of season (2 categories), position (categorical: whether on or off the track), area (categorical: one of two areas) and continuous habitat variables, plus interactions between them.
>
>
>
> I validated the GLM form of the model, including plotting the deviance residuals against my explanatory variables, and have noted that the variance of residuals for the categorical variables appears to differ.
>

Stop there.

In Logit/Probit frameworks, the variance is assumed equal for all
groups. It is never estimated.  The model is not identified otherwise.
The effect of heteroskedasticity is not just inefficiency, but
parameter bias. This makes logit models much more suspicious than
previously believed. This means that all of the work you have done so
far to "validate" your model is dubious and you need to take a step
back.

We are in a bind with logit models. Either we estimate separate models
for the separate groups (to avoid heteroskedasticity), but we are not
able to compare coefficients across models because there is that
different, but un-estimated variance.  Or we fit one model that
combines the group, make the wrong assumption, and end up with wrong
parameter estimates.  I don't mean just a little off. I mean wrong.
Its discouraging.

As far as I know, this problem was first popularized by Paul Allison,
Scott Long, and Richard Williams, but it is nicely surveyed in this
review essay:

Mood, Carina. 2010. Logistic Regression: Why We Cannot Do What We
Think We Can DO, and What We Can Do About It. European Sociological
Review 26(1): 67-82.

That has cites to the earlier Allison paper and some of Williams's work.

In my opinion, there are no completely safe approaches to dealing with
the heteroskedastic group-level error.  Richard Williams at Notre Dame
gave an excellent presentation about it.  He told me he has a paper
forthcoming in the Stata journal about it, but I don't feel free to
pass it along to you. But I bet his website has more information.

It seems to me that if you try to "pin" one group as the "baseline
variance" group and then add properly structured random effects for
the other ones, you might get a handle on it. The R package dglm has
suggestions like that.

Good luck. If you get an answer, I'd really like to know what is the
state of the art now (this minute)...

--
Paul E. Johnson
Professor, Political Science    Assoc. Director
1541 Lilac Lane, Room 504     Center for Research Methods
University of Kansas               University of Kansas
http://pj.freefaculty.org            http://quant.ku.edu



------------------------------

Message: 4
Date: Mon, 27 Aug 2012 18:23:41 +0100
From: Jarrod Hadfield <j.hadfield at ed.ac.uk>
To: Jackson King <jackson.king722 at gmail.com>
Cc: R-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] MCMCglmm multinomial logit
Message-ID: <20120827182341.78871bjh8feszoyo at www.staffmail.ed.ac.uk>
Content-Type: text/plain; charset=ISO-8859-1; DelSp="Yes";
        format="flowed"

Hi Jackson,

If x1 is the fixed effect prediction for the log contrast B-A then

plogis(x1/sqrt(1+v1*c2))

gives the expected probability of being B rather than A after
integrating over the random effects (in your case the residuals under
the assumption that they have unit variance v1=1). You can write it
another way:

Have nu1 = x1+e1

plogis(x1/sqrt(1+v1*c2)) \approx  int plogis(nu1)pnorm(e1,0,sqrt(v1))de

and we can see the approximation is pretty good:

x1<-2
c2<-0.345843
v1<-1

plogis(x1/sqrt(1+v1*c2))
integrate(function(e1){plogis(x1+e1)*dnorm(e1,0,sqrt(v1))}, -Inf,Inf)

Obtaining the expected probability of being B rather than A, or C
rather than A, is therefore relatively straightforward.

Obtaining the expected probability of B rather than C is a bit more tricky.

take nu2 = log contrast C-A

nu1-nu2 = log contrast B-C

which is

x1+e1-x2+e2

and is distributed as N(x1-x2, V[1,1]+V[2,2]-2*V[1,2]) where in you case V=R.

We can therefore use the same approximation as before replacing x1
with x1-x2 and v1 with V[1,1]+V[2,2]-2*V[1,2].

If we chose V = IJ  then V[1,1] = V[2,2] = V[1,1]+V[2,2]-2*V[1,2] and
so we multiply c2 by the same constant for all contrasts. This was the
main motivation. In fact, it might make more sense to scale IJ by
1/(j-1) rather than 1/j so all contrasts have unit variance, but
anything could be used really as long as you are careful.

Looking at it now, the justification that these results/approximations
extend to the case A versus (B and C) as I suggest in the CourseNotes
seems more tenuous and I will check it out.

Cheers,

Jarrod





Quoting Jackson King <jackson.king722 at gmail.com> on Sun, 26 Aug 2012
09:28:45 -0500:

> Hello,
>
> I am attempting to fit a multinomial logit model (3 categories) with random
> effects (across states). I have attempted to follow the advice in the
> course notes, but am a little uncertain the reason for the priors on the
> residuals, and how this is used to calculate predicted probabilities. I
> would like my covariates to predict each outcome -- cov1 predicting option3
> versus option1 and cov1 predicting option2 versus option1, rather than a
> main effect.
>
> Here is a simple version of my model
>
> j<-length(levels(data$char))
>
> I<-diag<-(j-1) #2x2 identity matrix
>
> J=matrix(rep(1, (j-1)^2), c(j-1, j-1)) #2x2 Unit Matrix
>
> IJ <- (1/3) * (diag(2) + matrix(1, 2, 2)) #Residual covriance matrix
>
> prior = list(R = list(V =IJ, fix = 1), G = list(G1 = list(V = diag(2), nu =
> 0.002))) #Why can't I use V=diag(2) for prior R?
>
>
> model<- MCMCglmm(char~-1+trait*(cov1), random=~idh(trait):state, rcov = ~us(
> trait):units, data=data, family = "categorical", prior = prior, verbose =
> TRUE)
>
>
> I find that the model converges, and the coefficients look similar to
> maximum likelihood, but then I would like to predict probabilities for
> being in each category. Typically, I think this is done by
> plogis(x/sqrt(1+c2)), so why is it necessary to multiply by the delta
> matrix (course notes p. 97)? Alternatively, if I simply use a 2x2 diagonal
> matrix for the prior for R, shouldn't I be able to use the same
> transformation -- plogis(x/sqrt(1+c2)). In short, I am a little confused
> about the IJ matrix and where it comes from. Is there a quick answer, or
> another paper that explains this? And (2) is it reasonable to predict
> probabilities from my model, based on fixed values of cov1, using this
> simple transformation above... plogis(x/sqrt(1+c2))
>
> Many thanks.
>
> Jackson
>
>       [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



--
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.



------------------------------

Message: 5
Date: Mon, 27 Aug 2012 18:26:25 +0100
From: Jarrod Hadfield <j.hadfield at ed.ac.uk>
To: Emmanuel Curis <curis at pharmacie.univ-paris5.fr>
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Questions about MCMCglmm and marker data
Message-ID: <20120827182625.8866720cbjenthoo at www.staffmail.ed.ac.uk>
Content-Type: text/plain; charset=ISO-8859-1; DelSp="Yes";
        format="flowed"

Hi,

It is the effects that are to be considered normal, not the way the
genotypes are coded.

It is not possible to fit this model in the current version of
MCMCglmm, but it will be available in the next. I hope to submit this
version to CRAN in the next couple of weeks.

Cheers,

Jarrod






Quoting Emmanuel Curis <curis at pharmacie.univ-paris5.fr> on Thu, 23 Aug
2012 12:10:46 +0200:

> Hello,
>
> I'm not familiar with this kind of genomic problems, but if the
> columns are the number of a given SNP for a given individual and can
> be only 0, 1 and 2, is this really possible to consider they follow a
> normal distribution?
>
> On Thu, Aug 23, 2012 at 11:54:49AM +0200, Marie Denis wrote:
> ? Hi,
> ?
> ? I use the MCMCglmm function in the genomic selection context in the
> ? univariate case. In fact I have for each trait one marker matrix
> ? constituted of 0,1 and 2. The rows are the individuals and the columns
> ? the SNPs. In a first time, we consider that each SNP follow a normal
> ? distribution with the *same *variance. So I use the following model:
> ?
> ? prior.1.1 <- list(G=list(G1=list(V=diag(x = as.numeric(scale), nrow=1,
> ? ncol=1),nu=ddl),
> ?                            R=list(V=matrix(scale),nu=ddl))
> ?
> ? mcmc.fit.1.1 <- MCMCglmm(P~ 1,random=~idv(SNP),prior=prior.1.1,
> ?                    data=data1.1,
> ?                    nitt=5000, burnin=1000,verbose=FALSE,
> ?                    thin=10,pr=TRUE)
> ?
> ? So, I obtained a common variance associated to my SNPs.
> ?
> ?
> ? The second step is a bivariate analysis. I would like to obtain a
> ? (co)variance matrix 2*2  associated to the trait1 and trait 2 and the
> ? correlation between both. (one variance for the SNPs for the trait 1 and
> ? one for the trait 2). But I don't know how i can do. The following model
> ? give me only one variance for all SNPs and both traits.
> ?
> ?      prior.3<- list(G=list(G1=list(V=diag(x = as.numeric(scale), nrow=1,
> ?                            ncol=1),n=1)),
> ?                R=list(V=diag(x=as.numeric(0.1),nrow=2,ncol=2),n=2))
> ?
> ?      mcmc.fit.3 <- MCMCglmm(cbind(P1,P2)~ trait-1,random=~idv(trait:SNP),
> ?                    rcov=~idh(trait):units,
> ?                    prior=prior.3,
> ?                    data=data1.3,family=c("gaussian","gaussian"),
> ?                    nitt=2000, burnin=500,verbose=FALSE,
> ?                    thin=10,pr=TRUE)
> ?
> ?
> ?
> ? thanks for your help,
> ?
> ?
> ?
> ?
> ?     [[alternative HTML version deleted]]
> ?
> ? _______________________________________________
> ? R-sig-mixed-models at r-project.org mailing list
> ? https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> --
>                                 Emmanuel CURIS
>                                 emmanuel.curis at univ-paris5.fr
>
> Page WWW: http://emmanuel.curis.online.fr/index.html
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



--
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.



------------------------------

Message: 6
Date: Mon, 27 Aug 2012 00:10:30 -0500
From: Mikhail Matz <matz at utexas.edu>
To: "r-sig-mixed-models at r-project.org"
        <r-sig-mixed-models at r-project.org>
Subject: [R-sig-ME] bayes factors for MCMCglmm
Message-ID: <C657AD3B-2DEE-4373-93D4-E465212DCEF4 at utexas.edu>
Content-Type: text/plain; charset=us-ascii

Hello - is it possible to calculate hayes factors for MCMCglmm objects, to compare alternative models?

Mikhail



------------------------------

Message: 7
Date: Tue, 28 Aug 2012 00:21:23 +0000 (UTC)
From: Ben Bolker <bbolker at gmail.com>
To: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] who shot the predict method (and did you shoot
        the     deputy as well?)
Message-ID: <loom.20120828T011347-203 at post.gmane.org>
Content-Type: text/plain; charset=us-ascii

Paul Johnson <pauljohn32 at ...> writes:

>
> It seems like I can't leave for a month or two without pieces of mount
> Rushmore falling off of lme4.
>
> Remember last June, I posted about some troubles with the predict
> method and you fixed it.

  Quick question: did predict() *ever* exist for the stable version
of lme4?  (I don't think so, although I may of course be mistaken ...)
  Were you working with the development (r-forge) version
of lme4 before?

  I will take a look at your code ...

  Ben Bolker

>
> And I had some other trouble with fishy looking variance estimates
> from models specified like (1|id) + (0 +x|id).
> I stopped fighting with that because I just couldn't understand it at
> all, but today I took it up again.
>
> Now the fishy looking variance estimates are completely solved
> (yeah!), but somebody stole the predict function entirely:
>
> > m3newdat$mm4pred <- predict(mm4, newdata = m3newdat)
> Error in UseMethod("predict") :
>   no applicable method for 'predict' applied to an object of class "mer"
>
> Have I gotten shunted to the wrong lme4 packaging?



------------------------------

_______________________________________________
R-sig-mixed-models mailing list
R-sig-mixed-models at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


End of R-sig-mixed-models Digest, Vol 68, Issue 32


From R.M.Krug at gmail.com  Tue Aug 28 10:31:28 2012
From: R.M.Krug at gmail.com (Rainer M Krug)
Date: Tue, 28 Aug 2012 10:31:28 +0200
Subject: [R-sig-ME] lme4 book pdf
In-Reply-To: <BLU0-SMTP390488C06E3A6162DC333E2CAA10@phx.gbl>
References: <BLU0-SMTP390488C06E3A6162DC333E2CAA10@phx.gbl>
Message-ID: <503C81E0.6090701@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 28/08/12 04:14, Jack Tanner wrote:
> Building the lme4 book from source is a bit painful right now, because lme4a cannot be
> installed from CRAN. Does anyone have a pretty recent PDF of the book that they could make
> available?
> 

What about: http://lme4.r-forge.r-project.org/book/

google: lme4 book

Cheers,

Rainer

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.11 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://www.enigmail.net/

iEYEARECAAYFAlA8geAACgkQoYgNqgF2ego1MwCfZCMr6MixieqBoT8Cb1qqGfCw
gBUAn3arS8BiGmnipIp+ENksEBXVC1hM
=oj3R
-----END PGP SIGNATURE-----


From bates at stat.wisc.edu  Tue Aug 28 16:25:12 2012
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 28 Aug 2012 09:25:12 -0500
Subject: [R-sig-ME] quick question regarding your "residual" command for
 your lmer command - please help me
In-Reply-To: <CAMyz1qbfyQcfjnBGJfNd8PKU0dGxNxxNjFWbTbtedT__+R2x5A@mail.gmail.com>
References: <CAMyz1qbfyQcfjnBGJfNd8PKU0dGxNxxNjFWbTbtedT__+R2x5A@mail.gmail.com>
Message-ID: <CAO7JsnTg9heVareU8daW74-zFm2w--KQF7eSGau=N83Tr2FuNg@mail.gmail.com>

I have taken the liberty of cc:'ing the
R-SIG-Mixed-Models at R-project.org mailing list on this reply.  Many of
those who read that list will be able to help you, often more quickly
than I am able to do.

Your question is a bit confusing in that you say you are using lmer
and then quote the documentation for residuals.lme.  The nlme package,
containing lme and supporting methods, and the lme4 package,
containing lmer, are different.  You should not expect the
documentation for one to apply to the other.

On Tue, Aug 28, 2012 at 8:20 AM, Yugo Nakamura <yugonakamura25 at gmail.com> wrote:
> To Professor Bates,
>
> I am terribly sorry for this impromptu email.  I have been using your lmer
> command for my dissertation (at the University of Washington) but I am
> having some difficulty to fully understand the outputs.  Your expertise will
> be deeply appreciated.  I would like to thank you in advance for your time.
>
> My question pertains to the Residual command of the lmer and the variance
> estimates of the level 1 residuals in the  lmer output.  I am not certain as
> to how these figures are calculated and what these estimates really imply.
>
> The definition of the residual are as follow.
>
> residuals.lme {nlme}
> The residuals at level i are obtained by subtracting the fitted levels at
> that level from the response vector (and dividing by the estimated
> within-group standard error, if type="pearson"). The fitted values at level
> i are obtained by adding together the population fitted values (based only
> on the fixed effects estimates) and the estimated contributions of the
> random effects to the fitted values at grouping levels less or equal to i
> http://stat.ethz.ch/R-manual/R-patched/library/nlme/html/residuals.lme.html
>
> Is this definition saying that, the residuals are defined by subtracting the
> fitted values from the fixed effects and the empirical Bayes estimate of the
> level 2 residuals?
>
> I calculated the variance of these residuals and it gave me the same
> estimate of the variance estimate of the "Residuals" (level 1) in the lmer
> output.
>
> But my question then is, is this variance estimate the "within group
> variance" explained in different textbooks of multilevel analysis?  But if
> so I find it slightly too big..  I have learned that within group variance
> are normally distributed with mean zero and constant variance for each and
> every group.   That is, the variance is calculated within/for each and every
> group and this variance is constant across all groups (quite strong
> assumption).
>
> But the variance estimate and the residual command above seems to give us
> the pooled residuals regardless of the groups.  That is, the variance is the
> estimate of the variance across all the residuals regardless/ignoring of the
> group.   Is this so?   If this is the case, isn't this variance the sum of
> all the within group variances (simply because the sum of normal
> distribution is also a normal distribution with the mean and variance also
> summed)?  Thus, to get a within group variance (for each group) I should
> divide your Residual variance estimate by the number of groups?
>
> I hope I was able to make sense.  It will be great if you could share me
> your expertise.    If there is a link that describes all the details of your
> command, that will be very helpful as well.
>
> Thank you so much in advance for your time and cooperation!
>
> yours,
>
> Yugo Nakamura
>
>
>
>


From andrewdigby at mac.com  Tue Aug 28 08:47:25 2012
From: andrewdigby at mac.com (Andrew Digby)
Date: Tue, 28 Aug 2012 18:47:25 +1200
Subject: [R-sig-ME] Is there an R function for GLMM with binary response,
 nested random factors, and temporal correlation?
In-Reply-To: <CAO7JsnTXcAGnqqitKvXE=hF9rk5in2FHzZiPTyHd6vY0HifQ8g@mail.gmail.com>
References: <B9623EB7-8367-4473-BA6C-97A328E18AC4@mac.com>
	<CAO7JsnQMAObfNtYsLwM9ZG3r+yTGKdpJUoRgzoJ8qkuLMqvz=g@mail.gmail.com>
	<2AD4A3F2-5107-41BF-9775-93D01F67B144@mac.com>
	<CAO7JsnTXcAGnqqitKvXE=hF9rk5in2FHzZiPTyHd6vY0HifQ8g@mail.gmail.com>
Message-ID: <14D7DCCA-01B9-433E-9727-672B50E559DB@mac.com>


Thanks again, and apologies for the delay. I'd prefer not to incorporate the temporal correlation structure through a random intercept/slope as I want to try auto-regressive correlation rather than compound symmetry. 

I've tried MCMCglmm, but it doesn't seem to allow AR correlation either. I don't have time to get into running WinBugs on OS X, or learning ADMB. Any further suggestions on how to do mixed logistic regression with temporal correlation would be very welcome!

Thanks,

Andrew

On 10/08/2012, at 04:21 , Douglas Bates <bates at stat.wisc.edu> wrote:

> On Wed, Aug 8, 2012 at 7:49 PM, Andrew Digby <andrewdigby at mac.com> wrote:
>> 
>> Thank you for your advice. Can I interpret it  to mean that it's not correct to specify a correlation structure (beyond that induced through random effects) in generalised models? I had thought that although prone to some issues, this was possible; see e.g. Ben Bolker's and Alain Zuur's reponses here: https://stat.ethz.ch/pipermail/r-sig-mixed-models/2012q1/014937.html. In my case, I think I do have 'small-scale correlation', with repeated measures of a subject taken in rapid succession on a particular day. My residuals don't show gross heterogeneity, so I may not need to alter the variance structure to allow for that.
> 
> Perhaps Ben or Alain can provide more information on models that
> incorporate correlation structure in binary responses.  There may be
> definitions of these using copulas or something like that but I don't
> know enough about that area to be able to provide advice.
> 
>> I'd be grateful for general advice on whether a GLM with temporal correlation structure is an adequate method in this case. Or should I tackle MCMC instead? (I have Zuur et al's excellent books.) However, this is a small part of my overall analysis, so I'd prefer to keep things as simple as possible!
> 
> One way to achieve a temporal correlation structure is to have a
> random effect for slope with respect to time for each subject.
> 
>> On 9/08/2012, at 03:19 , Douglas Bates <bates at stat.wisc.edu> wrote:
>> 
>>> On Tue, Aug 7, 2012 at 5:21 PM, Andrew Digby <andrewdigby at mac.com> wrote:
>>>> 
>>>> Despite lots of investigation, I haven't found any R packages might be suitable for the following problem. I'd be very grateful for suggestions.
>>>> 
>>>> I have three-way nested data, with a series of measures (obs) taken in quick succession (equal time spacing) from each subject on different days. The measures taken on the same day are temporally correlated, so I'd like to use an AR1 correlation structure for those, but treat subjects and days as nested random factors (random intercept) since there is little temporal correlation between days. The response is binary.
>>>> 
>>>> So I need a GLMM with a correlation structure. I've tried using GEE, but the R packages can't cope with multilevel nested data. The only R function I've found that can do this is glmmPQL.
>>> 
>>> Before you look for an R function, you should first check whether
>>> there is indeed a statistical model with the properties that you
>>> mention.  In the standard definition of a generalized linear mixed
>>> model, and the only one that makes sense to me, the conditional
>>> distribution of the response given the random effects has independent
>>> components.
>>> 
>>> Aspects of linear mixed models that depend on being able to model the
>>> variance-covariance of the response separately from the mean don't
>>> carry over to generalized linear mixed models.  One of the fundamental
>>> properties of GLMs and GLMMs is that the variance does depend on the
>>> mean.
>>> 
>>>> m <- glmmPQL(y ~ f1 * f2 * f3 + (1|subj/day), correlation=corAR1(form =~obsno|subj/day))
>>>> 
>>>> f1 - f3 are fixed factors
>>>> 
>>>> However, PQL estimation is not recommended for binary response data. With no AIC and unreliable p values, model selection seems impossible! So my question is:
>>>> 
>>>> 1) are there any other functions which are suitable for a GLMM with multilevel nested random effects and a AR1 correlation structure? Or is MCMC the only option?
>>>> 2) to make things more complicated, I'd also like to include a varFunc variance structure to cope with heterogeneity. Is this possible in ML methods in R? I'd also like to extend to a multinomial response at a later stage.
>>>> 
>>>> GEE seems the best bet, but I come unstuck with the three-way nested factors.
>>>> 
>>>> Thanks for your help.
>>>> 
>>>> Note: I originally posted this on R-help, but it was suggested that this list might be more appropriate.
>>>> 
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>


From mwpennell at gmail.com  Tue Aug 28 19:09:26 2012
From: mwpennell at gmail.com (Matt Pennell)
Date: Tue, 28 Aug 2012 13:09:26 -0400
Subject: [R-sig-ME] bayes factors for MCMCglmm
In-Reply-To: <20120828085129.19932k9qd841h5s0@www.staffmail.ed.ac.uk>
References: <C657AD3B-2DEE-4373-93D4-E465212DCEF4@utexas.edu>
	<loom.20120828T043351-7@post.gmane.org>
	<20120828085129.19932k9qd841h5s0@www.staffmail.ed.ac.uk>
Message-ID: <CACHYP6=4eG=NGBDM8Ror=JgGh6RAikN+JHS4mK=XB=XUcQL3Mw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120828/3402b8fc/attachment.pl>

From ihok at hotmail.com  Tue Aug 28 20:41:22 2012
From: ihok at hotmail.com (Jack Tanner)
Date: Tue, 28 Aug 2012 14:41:22 -0400
Subject: [R-sig-ME] lme4 book pdf
In-Reply-To: <503C81E0.6090701@gmail.com>
References: <BLU0-SMTP390488C06E3A6162DC333E2CAA10@phx.gbl>
	<503C81E0.6090701@gmail.com>
Message-ID: <BLU0-SMTP399D76F4263AB630766F4BCAA10@phx.gbl>

On 8/28/2012 4:31 AM, Rainer M Krug wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> On 28/08/12 04:14, Jack Tanner wrote:
>> Building the lme4 book from source is a bit painful right now, because lme4a cannot be
>> installed from CRAN. Does anyone have a pretty recent PDF of the book that they could make
>> available?
>>
>
> What about: http://lme4.r-forge.r-project.org/book/
>
> google: lme4 book

Thanks, that's great, but have there really been no changes in two years?


From steve.taylor at aut.ac.nz  Wed Aug 29 01:18:02 2012
From: steve.taylor at aut.ac.nz (Steve Taylor)
Date: Tue, 28 Aug 2012 23:18:02 +0000
Subject: [R-sig-ME] lme4 book pdf
In-Reply-To: <BLU0-SMTP399D76F4263AB630766F4BCAA10@phx.gbl>
References: <BLU0-SMTP390488C06E3A6162DC333E2CAA10@phx.gbl>
	<503C81E0.6090701@gmail.com>
	<BLU0-SMTP399D76F4263AB630766F4BCAA10@phx.gbl>
Message-ID: <CCE952776B6679469977532BD863C39C2309E8E5@Lewis.autuni.aut.ac.nz>

And note that chapter 3 is omitted.

-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Jack Tanner
Sent: Wednesday, 29 August 2012 6:41a
To: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] lme4 book pdf

On 8/28/2012 4:31 AM, Rainer M Krug wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> On 28/08/12 04:14, Jack Tanner wrote:
>> Building the lme4 book from source is a bit painful right now, because lme4a cannot be
>> installed from CRAN. Does anyone have a pretty recent PDF of the book that they could make
>> available?
>>
>
> What about: http://lme4.r-forge.r-project.org/book/
>
> google: lme4 book

Thanks, that's great, but have there really been no changes in two years?

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From highstat at highstat.com  Wed Aug 29 12:26:46 2012
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Wed, 29 Aug 2012 11:26:46 +0100
Subject: [R-sig-ME] Is there an R function for GLMM with binary, response,
 nested random factors, and temporal correlation?
In-Reply-To: <mailman.1.1346234401.13027.r-sig-mixed-models@r-project.org>
References: <mailman.1.1346234401.13027.r-sig-mixed-models@r-project.org>
Message-ID: <503DEE66.3070409@highstat.com>





>
> ------------------------------
>
> Message: 2
> Date: Tue, 28 Aug 2012 18:47:25 +1200
> From: Andrew Digby <andrewdigby at mac.com>
> To: Douglas Bates <bates at stat.wisc.edu>,
> 	"r-sig-mixed-models at r-project.org" <r-sig-mixed-models at r-project.org>
> Subject: Re: [R-sig-ME] Is there an R function for GLMM with binary
> 	response, nested random factors, and temporal correlation?
> Message-ID: <14D7DCCA-01B9-433E-9727-672B50E559DB at mac.com>
> Content-Type: text/plain; CHARSET=US-ASCII
>
>
> Thanks again, and apologies for the delay. I'd prefer not to incorporate the temporal correlation structure through a random intercept/slope as I want to try auto-regressive correlation rather than compound symmetry.
>
> I've tried MCMCglmm, but it doesn't seem to allow AR correlation either. I don't have time to get into running WinBugs on OS X, or

After 5 days fulltime pottering around last year, I gave up trying to 
get WinBUSG to run on OS X (actually...I did manage to run OpenBUGS on 
OS X). Now I use TeamViewer (free)...which allows me to remotely access 
a Windows machine from a Mac, and run for example WinBUGS. It takes 
about 1 minute to set up. And WinBUGS runs through the night.

All you need is remote access to an Windows computer...which can't be 
too difficult to arrange.



> learning ADMB. Any further suggestions on how to do mixed logistic regression with temporal correlation would be very welcome!
I'm afraid you will have to go the WinBUGS/OpenBUGS route. Example code 
for what you want is in our 2012 book.

Kind regards,

Alain


> Thanks,
>
> Andrew
>
> On 10/08/2012, at 04:21 , Douglas Bates <bates at stat.wisc.edu> wrote:





>> On Wed, Aug 8, 2012 at 7:49 PM, Andrew Digby <andrewdigby at mac.com> wrote:
>>> Thank you for your advice. Can I interpret it  to mean that it's not correct to specify a correlation structure (beyond that induced through random effects) in generalised models? I had thought that although prone to some issues, this was possible; see e.g. Ben Bolker's and Alain Zuur's reponses here: https://stat.ethz.ch/pipermail/r-sig-mixed-models/2012q1/014937.html. In my case, I think I do have 'small-scale correlation', with repeated measures of a subject taken in rapid succession on a particular day. My residuals don't show gross heterogeneity, so I may not need to alter the variance structure to allow for that.
>> Perhaps Ben or Alain can provide more information on models that
>> incorporate correlation structure in binary responses.  There may be
>> definitions of these using copulas or something like that but I don't
>> know enough about that area to be able to provide advice.
>>
>>> I'd be grateful for general advice on whether a GLM with temporal correlation structure is an adequate method in this case. Or should I tackle MCMC instead? (I have Zuur et al's excellent books.) However, this is a small part of my overall analysis, so I'd prefer to keep things as simple as possible!
>> One way to achieve a temporal correlation structure is to have a
>> random effect for slope with respect to time for each subject.
>>
>>> On 9/08/2012, at 03:19 , Douglas Bates <bates at stat.wisc.edu> wrote:
>>>
>>>> On Tue, Aug 7, 2012 at 5:21 PM, Andrew Digby <andrewdigby at mac.com> wrote:
>>>>> Despite lots of investigation, I haven't found any R packages might be suitable for the following problem. I'd be very grateful for suggestions.
>>>>>
>>>>> I have three-way nested data, with a series of measures (obs) taken in quick succession (equal time spacing) from each subject on different days. The measures taken on the same day are temporally correlated, so I'd like to use an AR1 correlation structure for those, but treat subjects and days as nested random factors (random intercept) since there is little temporal correlation between days. The response is binary.
>>>>>
>>>>> So I need a GLMM with a correlation structure. I've tried using GEE, but the R packages can't cope with multilevel nested data. The only R function I've found that can do this is glmmPQL.
>>>> Before you look for an R function, you should first check whether
>>>> there is indeed a statistical model with the properties that you
>>>> mention.  In the standard definition of a generalized linear mixed
>>>> model, and the only one that makes sense to me, the conditional
>>>> distribution of the response given the random effects has independent
>>>> components.
>>>>
>>>> Aspects of linear mixed models that depend on being able to model the
>>>> variance-covariance of the response separately from the mean don't
>>>> carry over to generalized linear mixed models.  One of the fundamental
>>>> properties of GLMs and GLMMs is that the variance does depend on the
>>>> mean.
>>>>
>>>>> m <- glmmPQL(y ~ f1 * f2 * f3 + (1|subj/day), correlation=corAR1(form =~obsno|subj/day))
>>>>>
>>>>> f1 - f3 are fixed factors
>>>>>
>>>>> However, PQL estimation is not recommended for binary response data. With no AIC and unreliable p values, model selection seems impossible! So my question is:
>>>>>
>>>>> 1) are there any other functions which are suitable for a GLMM with multilevel nested random effects and a AR1 correlation structure? Or is MCMC the only option?
>>>>> 2) to make things more complicated, I'd also like to include a varFunc variance structure to cope with heterogeneity. Is this possible in ML methods in R? I'd also like to extend to a multinomial response at a later stage.
>>>>>
>>>>> GEE seems the best bet, but I come unstuck with the three-way nested factors.
>>>>>
>>>>> Thanks for your help.
>>>>>
>>>>> Note: I originally posted this on R-help, but it was suggested that this list might be more appropriate.
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>




********************************





-- 

Dr. Alain F. Zuur
First author of:

1. Analysing Ecological Data (2007).
Zuur, AF, Ieno, EN and Smith, GM. Springer. 680 p.
URL: www.springer.com/0-387-45967-7


2. Mixed effects models and extensions in ecology with R. (2009).
Zuur, AF, Ieno, EN, Walker, N, Saveliev, AA, and Smith, GM. Springer.
http://www.springer.com/life+sci/ecology/book/978-0-387-87457-9


3. A Beginner's Guide to R (2009).
Zuur, AF, Ieno, EN, Meesters, EHWG. Springer
http://www.springer.com/statistics/computational/book/978-0-387-93836-3


4. Zero Inflated Models and Generalized Linear Mixed Models with R. (2012) Zuur, Saveliev, Ieno.
http://www.highstat.com/book4.htm

Other books: http://www.highstat.com/books.htm


Statistical consultancy, courses, data analysis and software
Highland Statistics Ltd.
6 Laverock road
UK - AB41 6FN Newburgh
Tel: 0044 1358 788177
Email: highstat at highstat.com
URL: www.highstat.com
URL: www.brodgar.com


From bates at stat.wisc.edu  Wed Aug 29 16:45:24 2012
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 29 Aug 2012 09:45:24 -0500
Subject: [R-sig-ME] quick question regarding your "residual" command for
 your lmer command - please help me
In-Reply-To: <CAMyz1qY_s2_xtEmNpYpM3EtQ6zoSFu2t9Qi93SYM6E-fxE8iLg@mail.gmail.com>
References: <CAMyz1qbfyQcfjnBGJfNd8PKU0dGxNxxNjFWbTbtedT__+R2x5A@mail.gmail.com>
	<CAO7JsnTg9heVareU8daW74-zFm2w--KQF7eSGau=N83Tr2FuNg@mail.gmail.com>
	<CAMyz1qY_s2_xtEmNpYpM3EtQ6zoSFu2t9Qi93SYM6E-fxE8iLg@mail.gmail.com>
Message-ID: <CAO7JsnRy9nWeMVyds6p5de--8ph10Vi+SNvon9Dn05FKi1upTw@mail.gmail.com>

Again, I will defer to the R-Sig-Mixed-Models mailing list.

On Wed, Aug 29, 2012 at 6:52 AM, Yugo Nakamura <yugonakamura25 at gmail.com> wrote:
> To Professor Bates,
>
> Thank you for your reply and for adding me on to the list.  I will check
> your commands again.
>
> To shorten my question, I essentially get confused when researches study the
> level 1 residuals pooled across the groups and not within groups separately.
>
> I just do not see why and what this analysis entail or suggest for the
> multilevel analysis and assumptions that do not take the grouping into
> consideration.
>
> Your thoughts and expertise will be deeply appreciated.
>
> Above
>
>
> On Tue, Aug 28, 2012 at 11:25 PM, Douglas Bates <bates at stat.wisc.edu> wrote:
>>
>> I have taken the liberty of cc:'ing the
>> R-SIG-Mixed-Models at R-project.org mailing list on this reply.  Many of
>> those who read that list will be able to help you, often more quickly
>> than I am able to do.
>>
>> Your question is a bit confusing in that you say you are using lmer
>> and then quote the documentation for residuals.lme.  The nlme package,
>> containing lme and supporting methods, and the lme4 package,
>> containing lmer, are different.  You should not expect the
>> documentation for one to apply to the other.
>>
>> On Tue, Aug 28, 2012 at 8:20 AM, Yugo Nakamura <yugonakamura25 at gmail.com>
>> wrote:
>> > To Professor Bates,
>> >
>> > I am terribly sorry for this impromptu email.  I have been using your
>> > lmer
>> > command for my dissertation (at the University of Washington) but I am
>> > having some difficulty to fully understand the outputs.  Your expertise
>> > will
>> > be deeply appreciated.  I would like to thank you in advance for your
>> > time.
>> >
>> > My question pertains to the Residual command of the lmer and the
>> > variance
>> > estimates of the level 1 residuals in the  lmer output.  I am not
>> > certain as
>> > to how these figures are calculated and what these estimates really
>> > imply.
>> >
>> > The definition of the residual are as follow.
>> >
>> > residuals.lme {nlme}
>> > The residuals at level i are obtained by subtracting the fitted levels
>> > at
>> > that level from the response vector (and dividing by the estimated
>> > within-group standard error, if type="pearson"). The fitted values at
>> > level
>> > i are obtained by adding together the population fitted values (based
>> > only
>> > on the fixed effects estimates) and the estimated contributions of the
>> > random effects to the fitted values at grouping levels less or equal to
>> > i
>> >
>> > http://stat.ethz.ch/R-manual/R-patched/library/nlme/html/residuals.lme.html
>> >
>> > Is this definition saying that, the residuals are defined by subtracting
>> > the
>> > fitted values from the fixed effects and the empirical Bayes estimate of
>> > the
>> > level 2 residuals?
>> >
>> > I calculated the variance of these residuals and it gave me the same
>> > estimate of the variance estimate of the "Residuals" (level 1) in the
>> > lmer
>> > output.
>> >
>> > But my question then is, is this variance estimate the "within group
>> > variance" explained in different textbooks of multilevel analysis?  But
>> > if
>> > so I find it slightly too big..  I have learned that within group
>> > variance
>> > are normally distributed with mean zero and constant variance for each
>> > and
>> > every group.   That is, the variance is calculated within/for each and
>> > every
>> > group and this variance is constant across all groups (quite strong
>> > assumption).
>> >
>> > But the variance estimate and the residual command above seems to give
>> > us
>> > the pooled residuals regardless of the groups.  That is, the variance is
>> > the
>> > estimate of the variance across all the residuals regardless/ignoring of
>> > the
>> > group.   Is this so?   If this is the case, isn't this variance the sum
>> > of
>> > all the within group variances (simply because the sum of normal
>> > distribution is also a normal distribution with the mean and variance
>> > also
>> > summed)?  Thus, to get a within group variance (for each group) I should
>> > divide your Residual variance estimate by the number of groups?
>> >
>> > I hope I was able to make sense.  It will be great if you could share me
>> > your expertise.    If there is a link that describes all the details of
>> > your
>> > command, that will be very helpful as well.
>> >
>> > Thank you so much in advance for your time and cooperation!
>> >
>> > yours,
>> >
>> > Yugo Nakamura
>> >
>> >
>> >
>> >
>
>


From matz at utexas.edu  Wed Aug 29 05:06:45 2012
From: matz at utexas.edu (Mikhail Matz)
Date: Tue, 28 Aug 2012 22:06:45 -0500
Subject: [R-sig-ME] MCMCglmm poisson / not poisson
Message-ID: <AB8E7D64-ECD8-4AB8-A1C9-BA6A64CF063D@utexas.edu>


Hello - 

I am playing with ways to justify that the MCMCglmm model fits my data well, which is quite important for me since I am hoping to be able to suggest MCMCglmm-based modeling as a general solution for a particular type of analysis. 

I am running "poisson" family on counts data, with two random effects. Following Elston, D. A., R. Moss, et al. (2001). Parasitology 122: 563-569., I am checking whether my lognormal residuals (latent variable minus predicted value) are normally distributed (check), if my random effects (saved with pr=T) are normally distributed (more or less check), and then I try to see if the observed counts really look like Poisson samples based on the latent variables. Again, following Elston et al, I am making a p-p plot using this script (expert coders, please don't judge):

pp.poisson=function(counts,latents) {
	sim=c()
	for(i in 1:length(counts)){
		if (is.na(counts[i])) next
		data=counts[i]
		low=ppois(data,exp(latents[i]))-dpois(data,exp(latents[i]))
		up=ppois(data,exp(latents[i]))
		ss=seq(low,up,(up-low)/100)
		sim=append(sim,sample(ss,1))
	}
	sims=sort(sim)
	xx=(rank(sims)-0.5)/length(sims)
	plot(sims~xx)
	abline(0,1)
}

? and unfortunately it looks really ugly, like a very strongly bent  ' ~ ' rather than a line.
The little script above seems to work; here is a sanity check:

psim=c()
nnn=rnorm(500,10,10)
for (i in 1:length(nnn)){
	psim=append(psim,rpois(1,exp(nnn[i])))
}
pp.poisson(psim,nnn)

I will be extremely grateful for any comments on this.

cheers

Misha
UT Austin


From bates at stat.wisc.edu  Wed Aug 29 16:51:38 2012
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 29 Aug 2012 09:51:38 -0500
Subject: [R-sig-ME] lme4 book pdf
In-Reply-To: <CCE952776B6679469977532BD863C39C2309E8E5@Lewis.autuni.aut.ac.nz>
References: <BLU0-SMTP390488C06E3A6162DC333E2CAA10@phx.gbl>
	<503C81E0.6090701@gmail.com>
	<BLU0-SMTP399D76F4263AB630766F4BCAA10@phx.gbl>
	<CCE952776B6679469977532BD863C39C2309E8E5@Lewis.autuni.aut.ac.nz>
Message-ID: <CAO7JsnT5kQXn9xQ4A5Wb6seaq1aCgD4DCCwe01Tt7gQYS+bS9Q@mail.gmail.com>

On Tue, Aug 28, 2012 at 6:18 PM, Steve Taylor <steve.taylor at aut.ac.nz> wrote:
> And note that chapter 3 is omitted.
>
> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Jack Tanner
> Sent: Wednesday, 29 August 2012 6:41a
> To: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] lme4 book pdf
>
> On 8/28/2012 4:31 AM, Rainer M Krug wrote:
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA1
>>
>> On 28/08/12 04:14, Jack Tanner wrote:
>>> Building the lme4 book from source is a bit painful right now, because lme4a cannot be
>>> installed from CRAN. Does anyone have a pretty recent PDF of the book that they could make
>>> available?
>>>
>>
>> What about: http://lme4.r-forge.r-project.org/book/
>>
>> google: lme4 book
>
> Thanks, that's great, but have there really been no changes in two years?

There have been many changes but I haven't uploaded them to the public
SVN archive.

I am working on another, also long-delayed, project right now and
probably won't get back to that book and the lme4 code for several
months.  I find it difficult to generate enthusiasm for working on R
packages right now.


From j.hadfield at ed.ac.uk  Wed Aug 29 17:09:45 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Wed, 29 Aug 2012 16:09:45 +0100
Subject: [R-sig-ME] MCMCglmm poisson / not poisson
In-Reply-To: <AB8E7D64-ECD8-4AB8-A1C9-BA6A64CF063D@utexas.edu>
References: <AB8E7D64-ECD8-4AB8-A1C9-BA6A64CF063D@utexas.edu>
Message-ID: <20120829160945.87654p213mg1wnms@www.staffmail.ed.ac.uk>

Hi,

The residuals (pre observation random effects) are assumed to be  
conditionaly normal. However, I think that even if this is satisfied  
then this does not imply that the posterior means/modes of the random  
effects will be normal. In fact when the expected number of counts is  
small I could imagine that the posterior means/modes could be strongly  
non-normal, perhaps even multimodal. Are the "latents" the posterior  
mean/mode of the residuals?  If you plot the distribution of  
per-observation  random-effects on an iteration by iteration basis, do  
you still see non-normality?

Cheers,

Jarrod


Quoting Mikhail Matz <matz at utexas.edu> on Tue, 28 Aug 2012 22:06:45 -0500:

>
> Hello -
>
> I am playing with ways to justify that the MCMCglmm model fits my  
> data well, which is quite important for me since I am hoping to be  
> able to suggest MCMCglmm-based modeling as a general solution for a  
> particular type of analysis.
>
> I am running "poisson" family on counts data, with two random  
> effects. Following Elston, D. A., R. Moss, et al. (2001).  
> Parasitology 122: 563-569., I am checking whether my lognormal  
> residuals (latent variable minus predicted value) are normally  
> distributed (check), if my random effects (saved with pr=T) are  
> normally distributed (more or less check), and then I try to see if  
> the observed counts really look like Poisson samples based on the  
> latent variables. Again, following Elston et al, I am making a p-p  
> plot using this script (expert coders, please don't judge):
>
> pp.poisson=function(counts,latents) {
> 	sim=c()
> 	for(i in 1:length(counts)){
> 		if (is.na(counts[i])) next
> 		data=counts[i]
> 		low=ppois(data,exp(latents[i]))-dpois(data,exp(latents[i]))
> 		up=ppois(data,exp(latents[i]))
> 		ss=seq(low,up,(up-low)/100)
> 		sim=append(sim,sample(ss,1))
> 	}
> 	sims=sort(sim)
> 	xx=(rank(sims)-0.5)/length(sims)
> 	plot(sims~xx)
> 	abline(0,1)
> }
>
> ? and unfortunately it looks really ugly, like a very strongly bent   
> ' ~ ' rather than a line.
> The little script above seems to work; here is a sanity check:
>
> psim=c()
> nnn=rnorm(500,10,10)
> for (i in 1:length(nnn)){
> 	psim=append(psim,rpois(1,exp(nnn[i])))
> }
> pp.poisson(psim,nnn)
>
> I will be extremely grateful for any comments on this.
>
> cheers
>
> Misha
> UT Austin
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From j.hadfield at ed.ac.uk  Wed Aug 29 17:16:04 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Wed, 29 Aug 2012 16:16:04 +0100
Subject: [R-sig-ME] MCMCglmm poisson / not poisson
In-Reply-To: <AB8E7D64-ECD8-4AB8-A1C9-BA6A64CF063D@utexas.edu>
References: <AB8E7D64-ECD8-4AB8-A1C9-BA6A64CF063D@utexas.edu>
Message-ID: <20120829161604.17912b2ic4umsajo@www.staffmail.ed.ac.uk>

Hi,

If you calculate latentsv (a vector of variances from the posterior  
distribution of the residuals), and replace

exp(latents[i])

with

exp(latents[i]+0.5*latentsv[i])

does this improve things?

Cheers,

Jarrod

Quoting Mikhail Matz <matz at utexas.edu> on Tue, 28 Aug 2012 22:06:45 -0500:

>
> Hello -
>
> I am playing with ways to justify that the MCMCglmm model fits my  
> data well, which is quite important for me since I am hoping to be  
> able to suggest MCMCglmm-based modeling as a general solution for a  
> particular type of analysis.
>
> I am running "poisson" family on counts data, with two random  
> effects. Following Elston, D. A., R. Moss, et al. (2001).  
> Parasitology 122: 563-569., I am checking whether my lognormal  
> residuals (latent variable minus predicted value) are normally  
> distributed (check), if my random effects (saved with pr=T) are  
> normally distributed (more or less check), and then I try to see if  
> the observed counts really look like Poisson samples based on the  
> latent variables. Again, following Elston et al, I am making a p-p  
> plot using this script (expert coders, please don't judge):
>
> pp.poisson=function(counts,latents) {
> 	sim=c()
> 	for(i in 1:length(counts)){
> 		if (is.na(counts[i])) next
> 		data=counts[i]
> 		low=ppois(data,exp(latents[i]))-dpois(data,exp(latents[i]))
> 		up=ppois(data,exp(latents[i]))
> 		ss=seq(low,up,(up-low)/100)
> 		sim=append(sim,sample(ss,1))
> 	}
> 	sims=sort(sim)
> 	xx=(rank(sims)-0.5)/length(sims)
> 	plot(sims~xx)
> 	abline(0,1)
> }
>
> ? and unfortunately it looks really ugly, like a very strongly bent   
> ' ~ ' rather than a line.
> The little script above seems to work; here is a sanity check:
>
> psim=c()
> nnn=rnorm(500,10,10)
> for (i in 1:length(nnn)){
> 	psim=append(psim,rpois(1,exp(nnn[i])))
> }
> pp.poisson(psim,nnn)
>
> I will be extremely grateful for any comments on this.
>
> cheers
>
> Misha
> UT Austin
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From torvon at gmail.com  Wed Aug 29 18:19:26 2012
From: torvon at gmail.com (Eiko Fried)
Date: Wed, 29 Aug 2012 12:19:26 -0400
Subject: [R-sig-ME] Interpretation of effects
Message-ID: <CACm_P7oyk=L4ZneB+2abpxdKkRwOk5Bt05tBuGkGM+AOWZar6w@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120829/8567276b/attachment.pl>

From matz at utexas.edu  Wed Aug 29 18:23:21 2012
From: matz at utexas.edu (Mikhail Matz)
Date: Wed, 29 Aug 2012 11:23:21 -0500
Subject: [R-sig-ME] MCMCglmm poisson / not poisson
In-Reply-To: <20120829160945.87654p213mg1wnms@www.staffmail.ed.ac.uk>
References: <AB8E7D64-ECD8-4AB8-A1C9-BA6A64CF063D@utexas.edu>
	<20120829160945.87654p213mg1wnms@www.staffmail.ed.ac.uk>
Message-ID: <61DAE81E-21C8-4481-8B38-A4E2CC414323@utexas.edu>

Hi Jarrod - 

I see how the means of random effects don't have to be normal (they just happen to be close to normal in my case, which is rather surprising). What I am concerned about is the Poisson residuals, corresponding to the difference between exp(latent variable) (latent=as.vector(apply(model$Liab,2,mean))) and the observed counts.  Each count should be a sample from a Poisson distribution with the mean=exp(corresponding latent variable), correct? This is what I still cannot convince myself about. Or is the sample truncated in some way (say, only allowing the numbers larger than or equal to the mean?)  

cheers

Misha
 

On Aug 29, 2012, at 10:09 AM, Jarrod Hadfield <j.hadfield at ed.ac.uk> wrote:

> Hi,
> 
> The residuals (pre observation random effects) are assumed to be conditionaly normal. However, I think that even if this is satisfied then this does not imply that the posterior means/modes of the random effects will be normal. In fact when the expected number of counts is small I could imagine that the posterior means/modes could be strongly non-normal, perhaps even multimodal. Are the "latents" the posterior mean/mode of the residuals?  If you plot the distribution of per-observation  random-effects on an iteration by iteration basis, do you still see non-normality?
> 
> Cheers,
> 
> Jarrod
> 
> 
> Quoting Mikhail Matz <matz at utexas.edu> on Tue, 28 Aug 2012 22:06:45 -0500:
> 
>> 
>> Hello -
>> 
>> I am playing with ways to justify that the MCMCglmm model fits my data well, which is quite important for me since I am hoping to be able to suggest MCMCglmm-based modeling as a general solution for a particular type of analysis.
>> 
>> I am running "poisson" family on counts data, with two random effects. Following Elston, D. A., R. Moss, et al. (2001). Parasitology 122: 563-569., I am checking whether my lognormal residuals (latent variable minus predicted value) are normally distributed (check), if my random effects (saved with pr=T) are normally distributed (more or less check), and then I try to see if the observed counts really look like Poisson samples based on the latent variables. Again, following Elston et al, I am making a p-p plot using this script (expert coders, please don't judge):
>> 
>> pp.poisson=function(counts,latents) {
>> 	sim=c()
>> 	for(i in 1:length(counts)){
>> 		if (is.na(counts[i])) next
>> 		data=counts[i]
>> 		low=ppois(data,exp(latents[i]))-dpois(data,exp(latents[i]))
>> 		up=ppois(data,exp(latents[i]))
>> 		ss=seq(low,up,(up-low)/100)
>> 		sim=append(sim,sample(ss,1))
>> 	}
>> 	sims=sort(sim)
>> 	xx=(rank(sims)-0.5)/length(sims)
>> 	plot(sims~xx)
>> 	abline(0,1)
>> }
>> 
>> ? and unfortunately it looks really ugly, like a very strongly bent  ' ~ ' rather than a line.
>> The little script above seems to work; here is a sanity check:
>> 
>> psim=c()
>> nnn=rnorm(500,10,10)
>> for (i in 1:length(nnn)){
>> 	psim=append(psim,rpois(1,exp(nnn[i])))
>> }
>> pp.poisson(psim,nnn)
>> 
>> I will be extremely grateful for any comments on this.
>> 
>> cheers
>> 
>> Misha
>> UT Austin
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> 
>> 
> 
> 
> 
> -- 
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
> 
> 


From matz at utexas.edu  Wed Aug 29 20:22:09 2012
From: matz at utexas.edu (Mikhail Matz)
Date: Wed, 29 Aug 2012 13:22:09 -0500
Subject: [R-sig-ME] MCMCglmm poisson / not poisson
In-Reply-To: <20120829163938.31077ep22qu8j04c@www.staffmail.ed.ac.uk>
References: <AB8E7D64-ECD8-4AB8-A1C9-BA6A64CF063D@utexas.edu>
	<20120829163938.31077ep22qu8j04c@www.staffmail.ed.ac.uk>
Message-ID: <3B8D8FD9-BC45-438A-BB96-98D377097532@utexas.edu>

Interesting!
so each mcmc realization of latent variables is perfectly poisson-compatible, but their mean is not. OK! This totally works for me. 
I now wonder why I expected the means to retain Poisson properties in the first place, actually

Thanks a lot!

Misha

On Aug 29, 2012, at 10:39 AM, Jarrod Hadfield <j.hadfield at ed.ac.uk> wrote:

> Hi,
> 
> library(MCMCglmm)
> l<-rnorm(100,1,2)
> y<-rpois(100, exp(l))
> 
> dat<-data.frame(y=y)
> 
> m1<-MCMCglmm(y~1, data=dat, family="poisson", pl=TRUE)
> 
> pp.poisson(y, colMeans(m1$Liab))
> 
> # looks bad
> 
> pp.poisson(y, colMeans(m1$Liab)+0.5*apply(m1$Liab, 2, var))
> 
> # looks better
> 
> for(i in 1:1000){
> pp.poisson(y, m1$Liab[i,])
> }
> 
> # looks good
> 
> Cheers,
> 
> Jarrod
> 
> 
> 
> 
> 
> Quoting Mikhail Matz <matz at utexas.edu> on Tue, 28 Aug 2012 22:06:45 -0500:
> 
>> 
>> Hello -
>> 
>> I am playing with ways to justify that the MCMCglmm model fits my data well, which is quite important for me since I am hoping to be able to suggest MCMCglmm-based modeling as a general solution for a particular type of analysis.
>> 
>> I am running "poisson" family on counts data, with two random effects. Following Elston, D. A., R. Moss, et al. (2001). Parasitology 122: 563-569., I am checking whether my lognormal residuals (latent variable minus predicted value) are normally distributed (check), if my random effects (saved with pr=T) are normally distributed (more or less check), and then I try to see if the observed counts really look like Poisson samples based on the latent variables. Again, following Elston et al, I am making a p-p plot using this script (expert coders, please don't judge):
>> 
>> pp.poisson=function(counts,latents) {
>> 	sim=c()
>> 	for(i in 1:length(counts)){
>> 		if (is.na(counts[i])) next
>> 		data=counts[i]
>> 		low=ppois(data,exp(latents[i]))-dpois(data,exp(latents[i]))
>> 		up=ppois(data,exp(latents[i]))
>> 		ss=seq(low,up,(up-low)/100)
>> 		sim=append(sim,sample(ss,1))
>> 	}
>> 	sims=sort(sim)
>> 	xx=(rank(sims)-0.5)/length(sims)
>> 	plot(sims~xx)
>> 	abline(0,1)
>> }
>> 
>> ? and unfortunately it looks really ugly, like a very strongly bent  ' ~ ' rather than a line.
>> The little script above seems to work; here is a sanity check:
>> 
>> psim=c()
>> nnn=rnorm(500,10,10)
>> for (i in 1:length(nnn)){
>> 	psim=append(psim,rpois(1,exp(nnn[i])))
>> }
>> pp.poisson(psim,nnn)
>> 
>> I will be extremely grateful for any comments on this.
>> 
>> cheers
>> 
>> Misha
>> UT Austin
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> 
>> 
> 
> 
> 
> -- 
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
> 
> 


From bbolker at gmail.com  Thu Aug 30 06:07:45 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 30 Aug 2012 04:07:45 +0000 (UTC)
Subject: [R-sig-ME] Interpretation of effects
References: <CACm_P7oyk=L4ZneB+2abpxdKkRwOk5Bt05tBuGkGM+AOWZar6w@mail.gmail.com>
Message-ID: <loom.20120830T055459-525@post.gmane.org>

Eiko Fried <torvon at ...> writes:

> 
> Dear Mailinglist,
> 
> I might have to solve this problem in R. MPLUS support tell me they cannot
> help me because the question is not MPLUS specific enough.

  I'm a little confused.  Are you running the model in R or MPLUS?
If the former, could you please provide a reproducible example
( e.g. see http://tinyurl.com/reproducible-000 )?  If the latter,
I don't quite understand why the question is "not MPLUS specific enough"
(maybe they consider it a general statistical question, and that the
MPLUS documentation should be sufficient to indicate how MPLUS
parameterizes models?  I don't know ...)

> I am not sure where to inquire about this, maybe you guys can help me out.
> 
> I'm running something that is basically a mixed effects model in MPLUS,
> which is set up as growth model. I allow every person to have its own
> intercept and slope.
> 
> 5 measurement points, 1 ordered dependent variable (0-3), quadratic growth
> term has better fit, so I use a quadratic slope.
> 
> I am predicting intercept and slope by two covariates, one is a centered
> metric (intelligence), the other nominal (gender).
> 
> Results:
> Intercept: gender is significant (negative estimate -0.6)
> Slope: gender is significant (positive estimate 0.2)

  So in lme your model would be

  lme(response ~ (intelligence+gender)*poly(time,2), 
     random=(poly(time,2)|individual),
    data=... )  

?

(Maybe?  I'm not quite sure I've specified the predictor by time
interaction correctly ... and I'm not sure whether you're just trying
to use a pure quadratic model, and if so whether you can justify
a pure quadratic [i.e. setting the linear term to zero] ... however,
all of that is separate from any mixed-model issues yo
 
> I want to plot this covariate now, over time (male vs female), and am not
> sure how to do this.
> 
> (1) Coding is 1=female, 0=male. Do I understand correctly that this means
> that males are higher at baseline (higher by 0.6 points on the dependent
> variable, I guess?), and that females rise higher in the slope? Do they
> rise by 0.2 per measurement point on the dependent variable, or 0.2 over
> all 5 measurement points, or something entirely different?

  That sounds right; the female slope per unit of the continuous
predictor should be 0.2 greater than the male slope.  (But I don't
know exactly what you mean by 'quadratic slope' above -- I'm now
guessing you mean pure quadratic.)

> 
> (2) How can I find out whether the two trajectories meet (males start
> higher, females rise quicker ...)? That is, do females overtake males at
> some point, or are maler going to be higher throughout?

  I think you'd need to do a bit of algebra on this.
 
> (3) If the effects were not on gender, but on a centered, metric variable,
> like intelligence - what exactly would -0.6 and 0.2 mean?

  It would mean the slope with respect to time increased or decreased
by the relevant amount per unit of intelligence.

  None of these questions are specific to mixed models, they all 
have to do with the coding of parameters in linear models more
generally.  I don't know where this is covered best: maybe see
one of Crawley's books, or Faraway's books on linear models.  Perhaps
others have suggestions.

  Ben Bolker


From datkins at u.washington.edu  Thu Aug 30 09:48:18 2012
From: datkins at u.washington.edu (David Atkins)
Date: Thu, 30 Aug 2012 09:48:18 +0200
Subject: [R-sig-ME] quick question regarding your "residual" command for
 your lmer command - please help me
In-Reply-To: <CAO7JsnRy9nWeMVyds6p5de--8ph10Vi+SNvon9Dn05FKi1upTw@mail.gmail.com>
References: <CAO7JsnRy9nWeMVyds6p5de--8ph10Vi+SNvon9Dn05FKi1upTw@mail.gmail.com>
Message-ID: <503F1AC2.4000505@u.washington.edu>


Yugo--

Most of what you write below is related to general statistical questions 
about mixed models, and hence would be more appropriate for a general 
stats listserv or local stat support (you know, there are some good stat 
consulting facilities at UW...).

With respect to the following specific question:

 >> > But my question then is, is this variance estimate the "within group
 >> > variance" explained in different textbooks of multilevel analysis? 
  But

Yes, the "Residual" variance reported by summary() form either lmer() or 
nlme() is what variously gets called "within group" or "level-1" error / 
variance.  For example, we can use this (from a random-intercept) model 
as our estimate of the within-group variance to calculate the intraclass 
correlation coefficient.

As a more general comment, you might check out UCLA's stats webpages, 
which have many helpful examples of stats procedures (including R and 
including mixed models aka multilevel models aka hierarchical linear 
models).  For example, there is R code to accompany Singer and Willett's 
book on longitudinal data analysis, which is one of the best 
introductions I know of for social science types:

http://www.ats.ucla.edu/stat/examples/alda.htm

Hope that helps.

cheers, Dave
-- 
Dave Atkins, PhD
University of Washington
datkins at u.washington.edu
http://depts.washington.edu/cshrb/

August 1 - October 30:

Universitat Zurich
Psychologisches Institut
Klinische Psychologie
Binzmuhlestrasse 14/23
CH-8050 Zurich

+41 44 635 71 75

Again, I will defer to the R-Sig-Mixed-Models mailing list.

On Wed, Aug 29, 2012 at 6:52 AM, Yugo Nakamura <yugonakamura25 at 
gmail.com> wrote:
 > To Professor Bates,
 >
 > Thank you for your reply and for adding me on to the list.  I will check
 > your commands again.
 >
 > To shorten my question, I essentially get confused when researches 
study the
 > level 1 residuals pooled across the groups and not within groups 
separately.
 >
 > I just do not see why and what this analysis entail or suggest for the
 > multilevel analysis and assumptions that do not take the grouping into
 > consideration.
 >
 > Your thoughts and expertise will be deeply appreciated.
 >
 > Above
 >
 >
 > On Tue, Aug 28, 2012 at 11:25 PM, Douglas Bates <bates at 
stat.wisc.edu> wrote:
 >>
 >> I have taken the liberty of cc:'ing the
 >> R-SIG-Mixed-Models at R-project.org mailing list on this reply.  Many of
 >> those who read that list will be able to help you, often more quickly
 >> than I am able to do.
 >>
 >> Your question is a bit confusing in that you say you are using lmer
 >> and then quote the documentation for residuals.lme.  The nlme package,
 >> containing lme and supporting methods, and the lme4 package,
 >> containing lmer, are different.  You should not expect the
 >> documentation for one to apply to the other.
 >>
 >> On Tue, Aug 28, 2012 at 8:20 AM, Yugo Nakamura <yugonakamura25 at 
gmail.com>
 >> wrote:
 >> > To Professor Bates,
 >> >
 >> > I am terribly sorry for this impromptu email.  I have been using your
 >> > lmer
 >> > command for my dissertation (at the University of Washington) but I am
 >> > having some difficulty to fully understand the outputs.  Your 
expertise
 >> > will
 >> > be deeply appreciated.  I would like to thank you in advance for your
 >> > time.
 >> >
 >> > My question pertains to the Residual command of the lmer and the
 >> > variance
 >> > estimates of the level 1 residuals in the  lmer output.  I am not
 >> > certain as
 >> > to how these figures are calculated and what these estimates really
 >> > imply.
 >> >
 >> > The definition of the residual are as follow.
 >> >
 >> > residuals.lme {nlme}
 >> > The residuals at level i are obtained by subtracting the fitted levels
 >> > at
 >> > that level from the response vector (and dividing by the estimated
 >> > within-group standard error, if type="pearson"). The fitted values at
 >> > level
 >> > i are obtained by adding together the population fitted values (based
 >> > only
 >> > on the fixed effects estimates) and the estimated contributions of the
 >> > random effects to the fitted values at grouping levels less or 
equal to
 >> > i
 >> >
 >> > 
http://stat.ethz.ch/R-manual/R-patched/library/nlme/html/residuals.lme.html
 >> >
 >> > Is this definition saying that, the residuals are defined by 
subtracting
 >> > the
 >> > fitted values from the fixed effects and the empirical Bayes 
estimate of
 >> > the
 >> > level 2 residuals?
 >> >
 >> > I calculated the variance of these residuals and it gave me the same
 >> > estimate of the variance estimate of the "Residuals" (level 1) in the
 >> > lmer
 >> > output.
 >> >
 >> > But my question then is, is this variance estimate the "within group
 >> > variance" explained in different textbooks of multilevel analysis? 
  But
 >> > if
 >> > so I find it slightly too big..  I have learned that within group
 >> > variance
 >> > are normally distributed with mean zero and constant variance for each
 >> > and
 >> > every group.   That is, the variance is calculated within/for each and
 >> > every
 >> > group and this variance is constant across all groups (quite strong
 >> > assumption).
 >> >
 >> > But the variance estimate and the residual command above seems to give
 >> > us
 >> > the pooled residuals regardless of the groups.  That is, the 
variance is
 >> > the
 >> > estimate of the variance across all the residuals 
regardless/ignoring of
 >> > the
 >> > group.   Is this so?   If this is the case, isn't this variance 
the sum
 >> > of
 >> > all the within group variances (simply because the sum of normal
 >> > distribution is also a normal distribution with the mean and variance
 >> > also
 >> > summed)?  Thus, to get a within group variance (for each group) I 
should
 >> > divide your Residual variance estimate by the number of groups?
 >> >
 >> > I hope I was able to make sense.  It will be great if you could 
share me
 >> > your expertise.    If there is a link that describes all the 
details of
 >> > your
 >> > command, that will be very helpful as well.
 >> >
 >> > Thank you so much in advance for your time and cooperation!
 >> >
 >> > yours,
 >> >
 >> > Yugo Nakamura
 >> >
 >> >
 >> >
 >> >
 >
 >


From m.fairbrother at bristol.ac.uk  Thu Aug 30 18:40:47 2012
From: m.fairbrother at bristol.ac.uk (Malcolm Fairbrother)
Date: Thu, 30 Aug 2012 17:40:47 +0100
Subject: [R-sig-ME] instrumenting for an Xj correlated with Uj
In-Reply-To: <A9597263-8FDA-4C38-BCED-9EEBB694FC45@bristol.ac.uk>
References: <2D3FF0E6-C643-4271-9F50-B0A1CA37A5ED@bristol.ac.uk>
	<4F7C11B9.2010503@iki.fi>
	<2B79B7BD-6B2C-4647-ACB6-AB2722D47FED@gmail.com>
	<A9597263-8FDA-4C38-BCED-9EEBB694FC45@bristol.ac.uk>
Message-ID: <122EFD30-65C4-428A-86CB-E20F941316B5@bristol.ac.uk>

Dear list,

A few months ago I asked about a model where a level-2 covariate (Xj) is correlated with the level-2 random effects (Uj), such that the estimated coefficient on Xj is biased--see <https://stat.ethz.ch/pipermail/r-sig-mixed-models/2012q2/018116.html>.

A couple people noted that an instrumental variable approach could help recover a less-biased estimate for the coefficient on Xj. However, I can find very little documentation about the use of instrumental variables in a multilevel/mixed modelling context. Following what I've read about the "2SLS" estimator (used above all in economics), I've adapted the original code I posted--see below--to perform a makeshift two-stage procedure which indeed performs very well in recovering the coefficient on Xj.

However, from what I've read, the SEs in the second stage of a two-stage instrumental variables approach need adjusting? yet I can't follow how, and especially in a multilevel context. Can anyone suggest any helpful resources, and/or additions to the code below? Would recourse to resampling with "refit(simulate(mod))" be a usable approach (in an lme4 context)?

Another issue is that the estimated level-2 variance (for "group") is much too high (whereas it's somewhat too low in a naive model without using an instrument).

Any input would be much appreciated.

Cheers,
Malcolm


library(lme4); library(multicore)
N <- 40 # set number of groups
T <- 10 # set number of observations per group
dgp <- function(N, T) {
	dat <- data.frame(group=1:N, Zj=rnorm(N), Uj=rnorm(N))[rep(1:N,each=T),]
	dat$Xj <- dat$Zj + dat$Uj	# Xj is correlated with Uj, but Zj is not
	dat$y <- 1 + dat$Xj + dat$Uj + rnorm(N*T)
	dat
	}
fit <- function(dat=dgp(N=N, T=T)) {
	bmod <- lmer(y ~ Xj + (1 | group), dat) # fit a naive (two-level) model to estimate the effect of Zj
	ivmod <- lm(Xj ~ Zj, dat[!duplicated(dat$group),]) # 1st stage: regress Xj on the instrument Zj (requires just one level)
	mod <- lmer(y ~ fitted(ivmod)[rep(1:N,each=T)] + (1 | group), dat) # 2nd stage: regress y on the fitted values from the 1st stage model
	c(fixef(bmod), as.numeric(summary(bmod)@REmat[,3]), fixef(mod), as.numeric(summary(mod)@REmat[,3]))
	}
res1 <- do.call("rbind", mclapply(1:100, function(yy) fit())) # run simulations
apply(res1, 2, function(x) c(mean(x),median(x))) # coefficient on Xj is much closer to 1, group variance is biased in opposite ways, residual variance is identical
apply(res1[,c(2,6)], 2, sd) # variance is higher for IV

From J.L.Baker at 2008.hull.ac.uk  Fri Aug 31 12:39:08 2012
From: J.L.Baker at 2008.hull.ac.uk (Joanna L Baker)
Date: Fri, 31 Aug 2012 11:39:08 +0100
Subject: [R-sig-ME] Phylogenetic Logistic Regression + MCMCglmm
Message-ID: <41FF6BF3BF1CD34596E05F3C0A891B4102CAB512@EXCL2VS1.adir.hull.ac.uk>

Hello all, 

I am currently trying to use MCMCglmm to carry out a phylogenetic
logistic regression, with a single binary response and one or more
continuous or categorical predictors. However, I am having issues with
the phylogenetic component, 'animal', converging. This occurs when I use
different datasets, with high, moderate and low phylogenetic signal.

I have tried improper (with the expected issues), proper, and
alpha-expanded priors but these do not seem to have much effect on the
results. I should note that I have no issues with convergence when my
response is a continuous variable. 

Does anybody have a worked example with data and tree that I could use
to get started with phylogenetic logistic regression or could somebody
point me in the direction of any published work that has used the
package for such an analysis successfully? I'll much appreciate any help
with this.

Thanks again,

Joanna Baker

University of Hull

Cottingham Road

Hull

East Yorkshire

j.l.baker at 2008.hull.ac.uk

 

-------------- next part --------------
**************************************************
To view the terms under which this email is 
distributed, please go to 
http://www2.hull.ac.uk/legal/disclaimer.aspx
**************************************************

From bates at stat.wisc.edu  Fri Aug 31 20:03:45 2012
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 31 Aug 2012 13:03:45 -0500
Subject: [R-sig-ME] lme4 with Poisson
In-Reply-To: <1346365540.60258.YahooMailNeo@web96007.mail.aue.yahoo.com>
References: <1346365540.60258.YahooMailNeo@web96007.mail.aue.yahoo.com>
Message-ID: <CAO7JsnQcWatj0ji2v5RPfXqYv97UPka2rfuz3V0P=gjA9ppuTg@mail.gmail.com>

On Thu, Aug 30, 2012 at 5:25 PM, Lynne Clay <lynne.clay at xtra.co.nz> wrote:
> Dear Prof Bates,
> I'm a doctoral candidate in NZ trying to analyse survey data with random
> effects with my outcome being a count.  I discovered your lme4 package and
> have been using this with success, however, I need to check for
> overdispersion and it is at this point I am having problems.  The formula I
> have used before has been (1/df)*deviance and if I use this my model is
> highly overdispersed.  I read on one of the discussion boards that adding an
> extra random effect (1|id#) addresses the overdispersion problem which I
> have included but overdispersion continues.
>
> Can overdispersion be calculated in this manner?

I'm sorry but I know nothing about overdispersion.  To me it is
completely artificial because there is no probability distribution on
which to base a statistical model with these properties.

> Do you have any suggestions of how to deal with this?

Sorry but I don't.  I have taken the liberty of sending a copy of this
reply to the R-SIG-Mixed-Models mailing list in the hope that readers
of that list can help you.
>
>
> Lynne
>
>
> Lynne Clay
> PhD Candidate
> School of Physiotherapy
> University of Otago
> PO Box 56
> Dunedin 9054
> New Zealand


From mmalten at gmail.com  Fri Aug 31 20:17:08 2012
From: mmalten at gmail.com (Mitchell Maltenfort)
Date: Fri, 31 Aug 2012 14:17:08 -0400
Subject: [R-sig-ME] lme4 with Poisson
In-Reply-To: <CAO7JsnQcWatj0ji2v5RPfXqYv97UPka2rfuz3V0P=gjA9ppuTg@mail.gmail.com>
References: <1346365540.60258.YahooMailNeo@web96007.mail.aue.yahoo.com>
	<CAO7JsnQcWatj0ji2v5RPfXqYv97UPka2rfuz3V0P=gjA9ppuTg@mail.gmail.com>
Message-ID: <CANOgrHabRufLAL5c6L4_fK__x+H4Ki9w78SiDvgEytSzkztANw@mail.gmail.com>

I think the SabreR package handles overdispersion.

____________________________
Ersatzistician and Chutzpahthologist
I can answer any question.  "I don't know" is an answer. "I don't know
yet" is a better answer.


On Fri, Aug 31, 2012 at 2:03 PM, Douglas Bates <bates at stat.wisc.edu> wrote:
> On Thu, Aug 30, 2012 at 5:25 PM, Lynne Clay <lynne.clay at xtra.co.nz> wrote:
>> Dear Prof Bates,
>> I'm a doctoral candidate in NZ trying to analyse survey data with random
>> effects with my outcome being a count.  I discovered your lme4 package and
>> have been using this with success, however, I need to check for
>> overdispersion and it is at this point I am having problems.  The formula I
>> have used before has been (1/df)*deviance and if I use this my model is
>> highly overdispersed.  I read on one of the discussion boards that adding an
>> extra random effect (1|id#) addresses the overdispersion problem which I
>> have included but overdispersion continues.
>>
>> Can overdispersion be calculated in this manner?
>
> I'm sorry but I know nothing about overdispersion.  To me it is
> completely artificial because there is no probability distribution on
> which to base a statistical model with these properties.
>
>> Do you have any suggestions of how to deal with this?
>
> Sorry but I don't.  I have taken the liberty of sending a copy of this
> reply to the R-SIG-Mixed-Models mailing list in the hope that readers
> of that list can help you.
>>
>>
>> Lynne
>>
>>
>> Lynne Clay
>> PhD Candidate
>> School of Physiotherapy
>> University of Otago
>> PO Box 56
>> Dunedin 9054
>> New Zealand
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From hesterlingsma at gmail.com  Fri Aug 31 20:56:21 2012
From: hesterlingsma at gmail.com (Hester Lingsma)
Date: Fri, 31 Aug 2012 20:56:21 +0200
Subject: [R-sig-ME] ordinal multilevel model
Message-ID: <3E60F6D0-BDB6-41D4-BEFF-7961E7353E71@gmail.com>

Dear R users, 
I would like to fit an ordinal (proportional odds) multilevel model. Is it possible with lme4? Or another R package? Or should I go to SAS? (Glimmix and nlmixed can do it). Thanks for any advice. 
Hester Lingsma 


From mjswat at ebi.ac.uk  Fri Aug 31 21:02:27 2012
From: mjswat at ebi.ac.uk (Maciej Swat)
Date: Fri, 31 Aug 2012 20:02:27 +0100
Subject: [R-sig-ME] lme4 with Poisson
In-Reply-To: <CAO7JsnQcWatj0ji2v5RPfXqYv97UPka2rfuz3V0P=gjA9ppuTg@mail.gmail.com>
References: <1346365540.60258.YahooMailNeo@web96007.mail.aue.yahoo.com>
	<CAO7JsnQcWatj0ji2v5RPfXqYv97UPka2rfuz3V0P=gjA9ppuTg@mail.gmail.com>
Message-ID: <6F8DFA4B-9429-4D22-9B4B-B74166DFC27D@ebi.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120831/7275f849/attachment.pl>

From lupp at uchicago.edu  Fri Aug 31 21:12:50 2012
From: lupp at uchicago.edu (Stuart Luppescu)
Date: Fri, 31 Aug 2012 14:12:50 -0500
Subject: [R-sig-ME] ordinal multilevel model
In-Reply-To: <3E60F6D0-BDB6-41D4-BEFF-7961E7353E71@gmail.com>
References: <3E60F6D0-BDB6-41D4-BEFF-7961E7353E71@gmail.com>
Message-ID: <1346440370.21102.4.camel@localhost>

On Fri, 2012-08-31 at 20:56 +0200, Hester Lingsma wrote:
> Dear R users, 
> I would like to fit an ordinal (proportional odds) multilevel model.
> Is it possible with lme4? Or another R package? Or should I go to SAS?
> (Glimmix and nlmixed can do it). Thanks for any advice. 

I have used MCMCglmm for this purpose. It works very well, and the
developer, Jarrod Hadfield, is an active participant on this list.
-- 
Stuart Luppescu -=-=- slu <AT> ccsr <DOT> uchicago <DOT> edu
CCSR at U of C ,.;-*^*-;.,  ccsr.uchicago.edu
     (^_^)/    ????????
[Crash programs] fail because they are based on the theory that, 
with nine women pregnant, you can get a baby a month.
                -- Wernher von Braun


From bbolker at gmail.com  Fri Aug 31 23:14:35 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 31 Aug 2012 21:14:35 +0000 (UTC)
Subject: [R-sig-ME] lme4 with Poisson
References: <1346365540.60258.YahooMailNeo@web96007.mail.aue.yahoo.com>
	<CAO7JsnQcWatj0ji2v5RPfXqYv97UPka2rfuz3V0P=gjA9ppuTg@mail.gmail.com>
Message-ID: <loom.20120831T230759-290@post.gmane.org>

Douglas Bates <bates at ...> writes:

> 
> On Thu, Aug 30, 2012 at 5:25 PM, Lynne Clay <lynne.clay at ...> wrote:
> > Dear Prof Bates,
> > I'm a doctoral candidate in NZ trying to analyse survey data with random
> > effects with my outcome being a count.  I discovered your lme4 package and
> > have been using this with success, however, I need to check for
> > overdispersion and it is at this point I am having problems.  The formula I
> > have used before has been (1/df)*deviance and if I use this my model is
> > highly overdispersed.  I read on one of the discussion boards that adding an
> > extra random effect (1|id#) addresses the overdispersion problem which I
> > have included but overdispersion continues.
> >
> > Can overdispersion be calculated in this manner?
> 
> I'm sorry but I know nothing about overdispersion.  To me it is
> completely artificial because there is no probability distribution on
> which to base a statistical model with these properties.
> 
> > Do you have any suggestions of how to deal with this?
> 
> Sorry but I don't.  I have taken the liberty of sending a copy of this
> reply to the R-SIG-Mixed-Models mailing list in the hope that readers
> of that list can help you.
> >


  It is probably worth checking out http://glmm.wikidot.com/faq ,
which has a variety of suggestions for handling overdispersion in lme4
(via adding observation-level random effect, as you suggest above) and
in other R packages: among other things, there are other packages such
glmmADMB that can fit negative binomial models with random effects.
One of the other responses mentions sabreR: from my brief
web-scrounging (e.g http://sabre.lancs.ac.uk/sabreR_coursebook5.pdf ),
it looks like sabre handles overdispersion by individual-level
random effects as well.

  I might also recommend Zuur et al's book on mixed models.

  Ben Bolker


From anthony.rietl at gmail.com  Sat Sep  1 00:12:52 2012
From: anthony.rietl at gmail.com (Anthony Rietl)
Date: Fri, 31 Aug 2012 17:12:52 -0500
Subject: [R-sig-ME] Split plot design with repeated measures - model errors
	(nlme)
Message-ID: <CAFR-yR17yQZ2gC4mP+Nt+AqvyXis6u+vmbmz7B2WXe2Sid=cwQ@mail.gmail.com>

Forgive me if this has been answered, but after extensive searching I
have found nothing to help me. Here is a brief overview of the
experiment:

-I have 18 tanks, each of which are split into 4 sections, each
section has a different specie of wetland plant. Tank is my whole plot
factor. Variety, or species (spp), is my split plot factor.

-I have 2 treatments, each at three levels. Nutrient addition (nut)
and clipping (clip)

-I am measuring methane (ch4) emission as my response variable. I
measured methane on 2 occasions, represented by variable t for time.

Data Structure

>str(ch4data)
'data.frame':   144 obs. of  6 variables:
$ tank: Factor w/ 18 levels "1","2","3","4",..: 1 1 1 1 2 2 2 2 3 3 ...
$ spp : Factor w/ 4 levels "Emac","Ewal",..: 4 2 3 1 3 4 1 2 1 4 ...
$ nut : Factor w/ 3 levels "1","2","3": 2 2 2 2 1 1 1 1 3 3 ...
$ clip: Factor w/ 3 levels "a","b","c": 3 3 3 3 2 2 2 2 2 2 ...
$ t: Factor w/ 2 levels "1","2": 1 1 1 1 1 1 1 1 1 1 ...
$ ch4 : num  0.382 1.642 1.529 0.245 11.482 ...

Model

ch4.model = lme(fixed = ch4 ~ spp+nut+clip+t + spp*nut+spp*clip+spp*t
+ nut*clip+nut*t + clip*t + spp*nut*clip + spp*nut*clip*t
+(nut*clip)%in%tank, random= ~1|tank, data=ch4data)

this gives the following error

Error in MEEM(object, conLin, control$niterEM) : Singularity in
backsolve at level 0, block 1

Specifically, my questions are as follows: What is wrong with the fit
of my model and what does this error mean? What in my model is
singular? Have I over specified the model? I think the problem is with
nesting the nut*clip interaction within tank, but I'm not sure.
Additional help with fitting this model correctly would be much
appreciated. I am new to R, so please take that into account.

Thanks,
Anthony


From highstat at highstat.com  Sat Sep  1 00:21:20 2012
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Fri, 31 Aug 2012 23:21:20 +0100
Subject: [R-sig-ME] lme4 with Poisson
In-Reply-To: <mailman.884.1346451197.4564.r-sig-mixed-models@r-project.org>
References: <mailman.884.1346451197.4564.r-sig-mixed-models@r-project.org>
Message-ID: <504138E0.4000604@highstat.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120831/126d8f00/attachment.pl>

From Tom_Philippi at nps.gov  Sat Sep  1 00:29:16 2012
From: Tom_Philippi at nps.gov (Tom_Philippi at nps.gov)
Date: Fri, 31 Aug 2012 15:29:16 -0700
Subject: [R-sig-ME] Split plot design with repeated measures - model
	errors	(nlme)
In-Reply-To: <CAFR-yR17yQZ2gC4mP+Nt+AqvyXis6u+vmbmz7B2WXe2Sid=cwQ@mail.gmail.com>
References: <CAFR-yR17yQZ2gC4mP+Nt+AqvyXis6u+vmbmz7B2WXe2Sid=cwQ@mail.gmail.com>
Message-ID: <OF7665FB7C.9D98E541-ON85257A6B.007AEC45-88257A6B.007B87BA@nps.gov>

Anthony--
You did not specify whether your nutrient and clipping treatments are
applied to entire tanks, quadrants within tanks, or individual plants
within quadrants, so I don't think anyone can suggest the proper model
specification.

Nonetheless, I suspect that the error message is because your formula
specifies each main effect several times.  Note that spp*nut*clip*t is not
just the 4-way interaction, but each main effect, 2-way, and 3-way
interaction.  spp:nut:clip:t would be the 4-way interaction.  See the
documentation for formula, or any of relevant resources listed at:
http://cran.r-project.org/other-docs.html

Tom




                                                                           
             Anthony Rietl                                                 
             <anthony.rietl at gm                                             
             ail.com>                                                   To 
             Sent by:                  r-sig-mixed-models at r-project.org    
             r-sig-mixed-model                                          cc 
             s-bounces at r-proje                                             
             ct.org                                                Subject 
                                       [R-sig-ME] Split plot design with   
                                       repeated measures - model errors    
             08/31/2012 05:12          (nlme)                              
             PM EST                                                        
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




Forgive me if this has been answered, but after extensive searching I
have found nothing to help me. Here is a brief overview of the
experiment:

-I have 18 tanks, each of which are split into 4 sections, each
section has a different specie of wetland plant. Tank is my whole plot
factor. Variety, or species (spp), is my split plot factor.

-I have 2 treatments, each at three levels. Nutrient addition (nut)
and clipping (clip)

-I am measuring methane (ch4) emission as my response variable. I
measured methane on 2 occasions, represented by variable t for time.

Data Structure

>str(ch4data)
'data.frame':   144 obs. of  6 variables:
$ tank: Factor w/ 18 levels "1","2","3","4",..: 1 1 1 1 2 2 2 2 3 3 ...
$ spp : Factor w/ 4 levels "Emac","Ewal",..: 4 2 3 1 3 4 1 2 1 4 ...
$ nut : Factor w/ 3 levels "1","2","3": 2 2 2 2 1 1 1 1 3 3 ...
$ clip: Factor w/ 3 levels "a","b","c": 3 3 3 3 2 2 2 2 2 2 ...
$ t: Factor w/ 2 levels "1","2": 1 1 1 1 1 1 1 1 1 1 ...
$ ch4 : num  0.382 1.642 1.529 0.245 11.482 ...

Model

ch4.model = lme(fixed = ch4 ~ spp+nut+clip+t + spp*nut+spp*clip+spp*t
+ nut*clip+nut*t + clip*t + spp*nut*clip + spp*nut*clip*t
+(nut*clip)%in%tank, random= ~1|tank, data=ch4data)

this gives the following error

Error in MEEM(object, conLin, control$niterEM) : Singularity in
backsolve at level 0, block 1

Specifically, my questions are as follows: What is wrong with the fit
of my model and what does this error mean? What in my model is
singular? Have I over specified the model? I think the problem is with
nesting the nut*clip interaction within tank, but I'm not sure.
Additional help with fitting this model correctly would be much
appreciated. I am new to R, so please take that into account.

Thanks,
Anthony

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From john.maindonald at anu.edu.au  Sat Sep  1 00:49:25 2012
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Sat, 1 Sep 2012 08:49:25 +1000
Subject: [R-sig-ME] lme4 with Poisson
In-Reply-To: <CAO7JsnQcWatj0ji2v5RPfXqYv97UPka2rfuz3V0P=gjA9ppuTg@mail.gmail.com>
References: <1346365540.60258.YahooMailNeo@web96007.mail.aue.yahoo.com>
	<CAO7JsnQcWatj0ji2v5RPfXqYv97UPka2rfuz3V0P=gjA9ppuTg@mail.gmail.com>
Message-ID: <206004BF-7384-4CB2-BA70-1DF14C78273B@anu.edu.au>

Surely, if one uses the lme4 formulation to model observation 
level random effects, this is modelling what is commonly called 
over dispersion.  If the observation level component of variance 
is greater than zero, then any over dispersion is to some extent
accounted for.

There is an issue of whether this is appropriately modelling
overdispersion -- the scale parameter may change with changes
in the fitted value.  Checking for this is a matter of some subtlety.

Now back to Lynne's specific question:

I do not see how one can use a (1/df)*deviance formula to check 
whether the over-dispersion has been accounted for.  I presume
this is the residual df from the fitted model.  For reasons that were
explained on this list quite a long time back, the df are for this
purpose inappropriate.  NB that this is a multi-level model.  Within 
the model formulation that you are using, it is the observation level 
component of variance that tells you about the extent of overdispersion.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.
http://www.maths.anu.edu.au/~johnm

On 01/09/2012, at 4:03 AM, Douglas Bates <bates at stat.wisc.edu> wrote:

> On Thu, Aug 30, 2012 at 5:25 PM, Lynne Clay <lynne.clay at xtra.co.nz> wrote:
>> Dear Prof Bates,
>> I'm a doctoral candidate in NZ trying to analyse survey data with random
>> effects with my outcome being a count.  I discovered your lme4 package and
>> have been using this with success, however, I need to check for
>> overdispersion and it is at this point I am having problems.  The formula I
>> have used before has been (1/df)*deviance and if I use this my model is
>> highly overdispersed.  I read on one of the discussion boards that adding an
>> extra random effect (1|id#) addresses the overdispersion problem which I
>> have included but overdispersion continues.
>> 
>> Can overdispersion be calculated in this manner?
> 
> I'm sorry but I know nothing about overdispersion.  To me it is
> completely artificial because there is no probability distribution on
> which to base a statistical model with these properties.
> 
>> Do you have any suggestions of how to deal with this?
> 
> Sorry but I don't.  I have taken the liberty of sending a copy of this
> reply to the R-SIG-Mixed-Models mailing list in the hope that readers
> of that list can help you.
>> 
>> 
>> Lynne
>> 
>> 
>> Lynne Clay
>> PhD Candidate
>> School of Physiotherapy
>> University of Otago
>> PO Box 56
>> Dunedin 9054
>> New Zealand
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From anthony.rietl at gmail.com  Sat Sep  1 00:54:50 2012
From: anthony.rietl at gmail.com (Anthony Rietl)
Date: Fri, 31 Aug 2012 17:54:50 -0500
Subject: [R-sig-ME] Split plot design with repeated measures - model
 errors (nlme)
In-Reply-To: <OF7665FB7C.9D98E541-ON85257A6B.007AEC45-88257A6B.007B87BA@nps.gov>
References: <CAFR-yR17yQZ2gC4mP+Nt+AqvyXis6u+vmbmz7B2WXe2Sid=cwQ@mail.gmail.com>
	<OF7665FB7C.9D98E541-ON85257A6B.007AEC45-88257A6B.007B87BA@nps.gov>
Message-ID: <CAFR-yR2FkkmV_G_U9LznzbV_OjsHg3vVt7zMoRUisqYyTwz1zQ@mail.gmail.com>

Tom,
Thanks for your input. My treatments (nut & clip) were applied to the
entire tank and all 4 species within that tank, making it a true split
plot. Each treatment combination also had a replicate - for example, 2
tanks would have the treatment combination of 1(nut) and a(clip), and
so on for other treatments. I was not aware that spp*nut*clip*t would
give me main effect, 2-way, and 3-way interactions. Thanks for
pointing that out. However, if I run the model with spp*nut*clip*t,
removing all of the other interactions, I still get the same error
regarding singularity, making me think that the nested term is still a
problem.

Thanks,
Anthony

On Fri, Aug 31, 2012 at 5:29 PM,  <Tom_Philippi at nps.gov> wrote:
> Anthony--
> You did not specify whether your nutrient and clipping treatments are
> applied to entire tanks, quadrants within tanks, or individual plants
> within quadrants, so I don't think anyone can suggest the proper model
> specification.
>
> Nonetheless, I suspect that the error message is because your formula
> specifies each main effect several times.  Note that spp*nut*clip*t is not
> just the 4-way interaction, but each main effect, 2-way, and 3-way
> interaction.  spp:nut:clip:t would be the 4-way interaction.  See the
> documentation for formula, or any of relevant resources listed at:
> http://cran.r-project.org/other-docs.html
>
> Tom
>
>
>
>
>
>              Anthony Rietl
>              <anthony.rietl at gm
>              ail.com>                                                   To
>              Sent by:                  r-sig-mixed-models at r-project.org
>              r-sig-mixed-model                                          cc
>              s-bounces at r-proje
>              ct.org                                                Subject
>                                        [R-sig-ME] Split plot design with
>                                        repeated measures - model errors
>              08/31/2012 05:12          (nlme)
>              PM EST
>
>
>
>
>
>
>
>
>
> Forgive me if this has been answered, but after extensive searching I
> have found nothing to help me. Here is a brief overview of the
> experiment:
>
> -I have 18 tanks, each of which are split into 4 sections, each
> section has a different specie of wetland plant. Tank is my whole plot
> factor. Variety, or species (spp), is my split plot factor.
>
> -I have 2 treatments, each at three levels. Nutrient addition (nut)
> and clipping (clip)
>
> -I am measuring methane (ch4) emission as my response variable. I
> measured methane on 2 occasions, represented by variable t for time.
>
> Data Structure
>
>>str(ch4data)
> 'data.frame':   144 obs. of  6 variables:
> $ tank: Factor w/ 18 levels "1","2","3","4",..: 1 1 1 1 2 2 2 2 3 3 ...
> $ spp : Factor w/ 4 levels "Emac","Ewal",..: 4 2 3 1 3 4 1 2 1 4 ...
> $ nut : Factor w/ 3 levels "1","2","3": 2 2 2 2 1 1 1 1 3 3 ...
> $ clip: Factor w/ 3 levels "a","b","c": 3 3 3 3 2 2 2 2 2 2 ...
> $ t: Factor w/ 2 levels "1","2": 1 1 1 1 1 1 1 1 1 1 ...
> $ ch4 : num  0.382 1.642 1.529 0.245 11.482 ...
>
> Model
>
> ch4.model = lme(fixed = ch4 ~ spp+nut+clip+t + spp*nut+spp*clip+spp*t
> + nut*clip+nut*t + clip*t + spp*nut*clip + spp*nut*clip*t
> +(nut*clip)%in%tank, random= ~1|tank, data=ch4data)
>
> this gives the following error
>
> Error in MEEM(object, conLin, control$niterEM) : Singularity in
> backsolve at level 0, block 1
>
> Specifically, my questions are as follows: What is wrong with the fit
> of my model and what does this error mean? What in my model is
> singular? Have I over specified the model? I think the problem is with
> nesting the nut*clip interaction within tank, but I'm not sure.
> Additional help with fitting this model correctly would be much
> appreciated. I am new to R, so please take that into account.
>
> Thanks,
> Anthony
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
Anthony Rietl
Graduate Student
Louisiana State University
Renewable Natural Resources
Baton Rouge, LA 70803
Office: (225) 578-1540


From anthony.rietl at gmail.com  Sat Sep  1 02:50:03 2012
From: anthony.rietl at gmail.com (Anthony Rietl)
Date: Fri, 31 Aug 2012 19:50:03 -0500
Subject: [R-sig-ME] Split plot design with repeated measures - model
 errors (nlme)
In-Reply-To: <CADv2QyF_7MEN9nUTfxtWeGVJoi35Sn8V2+o=od3SB4qm3quFxQ@mail.gmail.com>
References: <CAFR-yR17yQZ2gC4mP+Nt+AqvyXis6u+vmbmz7B2WXe2Sid=cwQ@mail.gmail.com>
	<OF7665FB7C.9D98E541-ON85257A6B.007AEC45-88257A6B.007B87BA@nps.gov>
	<CAFR-yR2FkkmV_G_U9LznzbV_OjsHg3vVt7zMoRUisqYyTwz1zQ@mail.gmail.com>
	<CADv2QyF_7MEN9nUTfxtWeGVJoi35Sn8V2+o=od3SB4qm3quFxQ@mail.gmail.com>
Message-ID: <CAFR-yR12LaHfR0Rh=c27jS8=SSiMA28LG6V2-C7-48i3m_u+ew@mail.gmail.com>

Dennis,
Thanks for the response. For clarification, the treatments are
assigned to tanks at random. For example, all 18 tanks were randomly
assigned a nutrient treatment (1, 2, or 3) and randomly assigned a
clip treatment (a, b, or c). That gives 9 combinations, each
replicated once in a total of 18 tanks. After trying the model you
suggest (m1 <- lme(ch4 ~ t * spp * nut * clip, random = ~ 1 | tank/nut
* clip/spp,
> data = ch4data) I get another error - "Error in getGroups.data.frame(dataMix, groups) : Invalid formula for groups"

If I run the model without specifying nut*clip nested within tank, the
model runs leaving me to still suspect that I am still unclear how to
specify my nesting correctly. This is the model that runs - ch4.model
<- lme(ch4 ~ t * spp * nut * clip, random = ~ 1 | tank, data =
ch4data)

I will look through  Pinheiro and Bates (2000), thanks for pointing me to it.

Anthony


On Fri, Aug 31, 2012 at 6:50 PM, Dennis Murphy <djmuser at gmail.com> wrote:
> Hi:
>
> It's not clear to me how nut and clip are assigned. Are two tanks assigned
> to each nut/clip combination at random? If so, then the df breakdown in the
> ANOVA should look something like
>
> Whole plot: (tank level)
>
> nut          2
> clip         2
> nut * clip  4
> Error(WP)  9
>
> Split plot:  (section level)
>
> variety               3
> variety * nut       6
> variety* clip       6
> 3fi                    12
> Error (SP)         27
>
> Split-split plot:   (occasion level)
>
> Occasion           1
> (Cross occasion with everything above, same degrees of freedom)
>
> Occasion is equivalent to a split-split plot factor when the #occasions = 2,
> since you can only get a compound symmetric covariance structure in that
> case. That's how I'd analyze it, something like
>
> m1 <- lme(ch4 ~ t * spp * nut * clip, random = ~ 1 | tank/nut * clip/spp,
> data = ch4data)
>
> For reference, see the split-plot example starting on p.45 of Pinheiro and
> Bates (2000). I'm not sure about the correctness of the random effects
> specification, but check out the example to see how split-plot designs are
> programmed in lme(). I'd try it out myself, but no data were provided...
>
> HTH,
> Dennis
>
> On Fri, Aug 31, 2012 at 3:54 PM, Anthony Rietl <anthony.rietl at gmail.com>
> wrote:
>>
>> Tom,
>> Thanks for your input. My treatments (nut & clip) were applied to the
>> entire tank and all 4 species within that tank, making it a true split
>> plot. Each treatment combination also had a replicate - for example, 2
>> tanks would have the treatment combination of 1(nut) and a(clip), and
>> so on for other treatments. I was not aware that spp*nut*clip*t would
>> give me main effect, 2-way, and 3-way interactions. Thanks for
>> pointing that out. However, if I run the model with spp*nut*clip*t,
>> removing all of the other interactions, I still get the same error
>> regarding singularity, making me think that the nested term is still a
>> problem.
>>
>> Thanks,
>> Anthony
>>
>> On Fri, Aug 31, 2012 at 5:29 PM,  <Tom_Philippi at nps.gov> wrote:
>> > Anthony--
>> > You did not specify whether your nutrient and clipping treatments are
>> > applied to entire tanks, quadrants within tanks, or individual plants
>> > within quadrants, so I don't think anyone can suggest the proper model
>> > specification.
>> >
>> > Nonetheless, I suspect that the error message is because your formula
>> > specifies each main effect several times.  Note that spp*nut*clip*t is
>> > not
>> > just the 4-way interaction, but each main effect, 2-way, and 3-way
>> > interaction.  spp:nut:clip:t would be the 4-way interaction.  See the
>> > documentation for formula, or any of relevant resources listed at:
>> > http://cran.r-project.org/other-docs.html
>> >
>> > Tom
>> >
>> >
>> >
>> >
>> >
>> >              Anthony Rietl
>> >              <anthony.rietl at gm
>> >              ail.com>
>> > To
>> >              Sent by:                  r-sig-mixed-models at r-project.org
>> >              r-sig-mixed-model
>> > cc
>> >              s-bounces at r-proje
>> >              ct.org
>> > Subject
>> >                                        [R-sig-ME] Split plot design with
>> >                                        repeated measures - model errors
>> >              08/31/2012 05:12          (nlme)
>> >              PM EST
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> > Forgive me if this has been answered, but after extensive searching I
>> > have found nothing to help me. Here is a brief overview of the
>> > experiment:
>> >
>> > -I have 18 tanks, each of which are split into 4 sections, each
>> > section has a different specie of wetland plant. Tank is my whole plot
>> > factor. Variety, or species (spp), is my split plot factor.
>> >
>> > -I have 2 treatments, each at three levels. Nutrient addition (nut)
>> > and clipping (clip)
>> >
>> > -I am measuring methane (ch4) emission as my response variable. I
>> > measured methane on 2 occasions, represented by variable t for time.
>> >
>> > Data Structure
>> >
>> >>str(ch4data)
>> > 'data.frame':   144 obs. of  6 variables:
>> > $ tank: Factor w/ 18 levels "1","2","3","4",..: 1 1 1 1 2 2 2 2 3 3 ...
>> > $ spp : Factor w/ 4 levels "Emac","Ewal",..: 4 2 3 1 3 4 1 2 1 4 ...
>> > $ nut : Factor w/ 3 levels "1","2","3": 2 2 2 2 1 1 1 1 3 3 ...
>> > $ clip: Factor w/ 3 levels "a","b","c": 3 3 3 3 2 2 2 2 2 2 ...
>> > $ t: Factor w/ 2 levels "1","2": 1 1 1 1 1 1 1 1 1 1 ...
>> > $ ch4 : num  0.382 1.642 1.529 0.245 11.482 ...
>> >
>> > Model
>> >
>> > ch4.model = lme(fixed = ch4 ~ spp+nut+clip+t + spp*nut+spp*clip+spp*t
>> > + nut*clip+nut*t + clip*t + spp*nut*clip + spp*nut*clip*t
>> > +(nut*clip)%in%tank, random= ~1|tank, data=ch4data)
>> >
>> > this gives the following error
>> >
>> > Error in MEEM(object, conLin, control$niterEM) : Singularity in
>> > backsolve at level 0, block 1
>> >
>> > Specifically, my questions are as follows: What is wrong with the fit
>> > of my model and what does this error mean? What in my model is
>> > singular? Have I over specified the model? I think the problem is with
>> > nesting the nut*clip interaction within tank, but I'm not sure.
>> > Additional help with fitting this model correctly would be much
>> > appreciated. I am new to R, so please take that into account.
>> >
>> > Thanks,
>> > Anthony
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >
>> >
>>
>>
>>
>> --
>> Anthony Rietl
>> Graduate Student
>> Louisiana State University
>> Renewable Natural Resources
>> Baton Rouge, LA 70803
>> Office: (225) 578-1540
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
Anthony Rietl
Graduate Student
Louisiana State University
Renewable Natural Resources
Baton Rouge, LA 70803
Office: (225) 578-1540


From karruo at utu.fi  Sat Sep  1 12:44:21 2012
From: karruo at utu.fi (Kari Ruohonen)
Date: Sat, 01 Sep 2012 13:44:21 +0300
Subject: [R-sig-ME] ordinal multilevel model
In-Reply-To: <3E60F6D0-BDB6-41D4-BEFF-7961E7353E71@gmail.com>
References: <3E60F6D0-BDB6-41D4-BEFF-7961E7353E71@gmail.com>
Message-ID: <5041E705.6080403@utu.fi>

On 31.08.2012 21:56, Hester Lingsma wrote:
> Dear R users,
> I would like to fit an ordinal (proportional odds) multilevel model. Is it possible with lme4? Or another R package? Or should I go to SAS? (Glimmix and nlmixed can do it). Thanks for any advice.
> Hester Lingsma
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
Ordinal package may provide what you need.

regards, Kari


From mike.lwrnc at gmail.com  Sat Sep  1 15:33:02 2012
From: mike.lwrnc at gmail.com (Mike Lawrence)
Date: Sat, 1 Sep 2012 10:33:02 -0300
Subject: [R-sig-ME] RStan: Fast, multilevel Bayesian modeling in R
Message-ID: <CAB+QPJCX1_zxzwq7CdwNMmdfo+LS1jBDx10mhsVnwO56kghzKg@mail.gmail.com>

I thought subscribers to the list might find this of interest:

http://blog.revolutionanalytics.com/2012/08/rstan-fast-multilevel-bayesian-modeling-in-r.html


From i.m.s.white at ed.ac.uk  Sat Sep  1 15:51:50 2012
From: i.m.s.white at ed.ac.uk (I.M.S.White)
Date: Sat, 01 Sep 2012 14:51:50 +0100
Subject: [R-sig-ME] Split plot design with repeated measures - model
 errors (nlme)
In-Reply-To: <CAFR-yR17yQZ2gC4mP+Nt+AqvyXis6u+vmbmz7B2WXe2Sid=cwQ@mail.gmail.com>
References: <CAFR-yR17yQZ2gC4mP+Nt+AqvyXis6u+vmbmz7B2WXe2Sid=cwQ@mail.gmail.com>
Message-ID: <20120901145150.12551l4wikdpbvvo@www.staffmail.ed.ac.uk>

This is standard split-plot design.  The lme model formula is

fixed = ch4 ~ nut*clip*spp, random = ~1|tank, ...

Specify the treatment structure as fixed formula, plot structure as  
random. Here the plot structure is simple, mainplots (tanks), and  
subplots within mainplots. The latter does not need to be specified  
because it is residual, fitted always by default.



Quoting Anthony Rietl <anthony.rietl at gmail.com> on Fri, 31 Aug 2012  
17:12:52 -0500:

> Forgive me if this has been answered, but after extensive searching I
> have found nothing to help me. Here is a brief overview of the
> experiment:
>
> -I have 18 tanks, each of which are split into 4 sections, each
> section has a different specie of wetland plant. Tank is my whole plot
> factor. Variety, or species (spp), is my split plot factor.
>
> -I have 2 treatments, each at three levels. Nutrient addition (nut)
> and clipping (clip)
>
> -I am measuring methane (ch4) emission as my response variable. I
> measured methane on 2 occasions, represented by variable t for time.
>
> Data Structure
>
>> str(ch4data)
> 'data.frame':   144 obs. of  6 variables:
> $ tank: Factor w/ 18 levels "1","2","3","4",..: 1 1 1 1 2 2 2 2 3 3 ...
> $ spp : Factor w/ 4 levels "Emac","Ewal",..: 4 2 3 1 3 4 1 2 1 4 ...
> $ nut : Factor w/ 3 levels "1","2","3": 2 2 2 2 1 1 1 1 3 3 ...
> $ clip: Factor w/ 3 levels "a","b","c": 3 3 3 3 2 2 2 2 2 2 ...
> $ t: Factor w/ 2 levels "1","2": 1 1 1 1 1 1 1 1 1 1 ...
> $ ch4 : num  0.382 1.642 1.529 0.245 11.482 ...
>
> Model
>
> ch4.model = lme(fixed = ch4 ~ spp+nut+clip+t + spp*nut+spp*clip+spp*t
> + nut*clip+nut*t + clip*t + spp*nut*clip + spp*nut*clip*t
> +(nut*clip)%in%tank, random= ~1|tank, data=ch4data)
>
> this gives the following error
>
> Error in MEEM(object, conLin, control$niterEM) : Singularity in
> backsolve at level 0, block 1
>
> Specifically, my questions are as follows: What is wrong with the fit
> of my model and what does this error mean? What in my model is
> singular? Have I over specified the model? I think the problem is with
> nesting the nut*clip interaction within tank, but I'm not sure.
> Additional help with fitting this model correctly would be much
> appreciated. I am new to R, so please take that into account.
>
> Thanks,
> Anthony
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



::::::::::::::::::::::::::::::::::::::::::
: I.White                                :
: University of Edinburgh                :
: Ashworth Laboratories, West Mains Road :
: Edinburgh EH9 3JT                      :
: Tel 0131 650 5490  Fax 0131 650 6564   :
::::::::::::::::::::::::::::::::::::::::::

-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From j.hadfield at ed.ac.uk  Sat Sep  1 21:06:22 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Sat, 01 Sep 2012 20:06:22 +0100
Subject: [R-sig-ME] Phylogenetic Logistic Regression + MCMCglmm
In-Reply-To: <41FF6BF3BF1CD34596E05F3C0A891B4102CAB512@EXCL2VS1.adir.hull.ac.uk>
References: <41FF6BF3BF1CD34596E05F3C0A891B4102CAB512@EXCL2VS1.adir.hull.ac.uk>
Message-ID: <20120901200622.19094ky48uwym6m8@www.staffmail.ed.ac.uk>

Hi,

The most likely reason is that you have not fixed the non-identified  
residual variance, although there are other possibilities. Here is an  
example with a simulated tree and binary data:

tree<-rcoal(100)
x<-rnorm(100)
l<-rbv(tree, 1, nodes="TIPS")+x+rnorm(100)
y<-rbinom(100, 1, plogis(l))

dat<-data.frame(y=y, x=x, species=tree$tip.label)

prior1=list(R=list(V=1, fix=1), G=list(G1=list(V=1, nu=1, alpha.mu=0,  
alpha.V=1000)))

# residual variance fixed at 1.


Ainv<-inverseA(tree)$Ainv
m1<-MCMCglmm(y~x, random=~species, ginverse=list(species=Ainv),  
family="categorical", prior=prior1, data=dat)

plot(m1$Sol)
# fixed effects should be zero and one

plot(m1$VCV[,1]/(rowSums(m1$VCV)+pi^2/3))

# phylogenetic ICC should be 1/(2+pi^2/3)=0.189. Note wdie credible  
intervals - you need v.large phylogenies to get precise estimates with  
binary data.

Cheers,

Jarrod






Quoting Joanna L Baker <J.L.Baker at 2008.hull.ac.uk> on Fri, 31 Aug 2012  
11:39:08 +0100:

> Hello all,
>
> I am currently trying to use MCMCglmm to carry out a phylogenetic
> logistic regression, with a single binary response and one or more
> continuous or categorical predictors. However, I am having issues with
> the phylogenetic component, 'animal', converging. This occurs when I use
> different datasets, with high, moderate and low phylogenetic signal.
>
> I have tried improper (with the expected issues), proper, and
> alpha-expanded priors but these do not seem to have much effect on the
> results. I should note that I have no issues with convergence when my
> response is a continuous variable.
>
> Does anybody have a worked example with data and tree that I could use
> to get started with phylogenetic logistic regression or could somebody
> point me in the direction of any published work that has used the
> package for such an analysis successfully? I'll much appreciate any help
> with this.
>
> Thanks again,
>
> Joanna Baker
>
> University of Hull
>
> Cottingham Road
>
> Hull
>
> East Yorkshire
>
> j.l.baker at 2008.hull.ac.uk
>
>
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From David.LeBlond at abbott.com  Sun Sep  2 15:41:34 2012
From: David.LeBlond at abbott.com (David LeBlond)
Date: Sun, 2 Sep 2012 08:41:34 -0500
Subject: [R-sig-ME] AUTO: New email address for Dave LeBlond:
	david.leblond@sbcglobal.net
Message-ID: <OF204323FA.B3182F02-ON86257A6D.004B3789-86257A6D.004B3789@abbott.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120902/b98bc163/attachment.pl>

From jackson.king722 at gmail.com  Sun Sep  2 23:58:57 2012
From: jackson.king722 at gmail.com (Jackson King)
Date: Sun, 2 Sep 2012 16:58:57 -0500
Subject: [R-sig-ME] MCMCglmm multinomial logit
In-Reply-To: <CABHLxrkeAYYzjfv1MDvKZ4erp2s_76Ns8BvqYdQx51fWZR=otw@mail.gmail.com>
References: <CABHLxrkeAYYzjfv1MDvKZ4erp2s_76Ns8BvqYdQx51fWZR=otw@mail.gmail.com>
Message-ID: <CABHLxr=vThnS6cY1oA_Y+WUiYcLe9PK0uVgZxM=s9ty_7mF_mw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120902/97e49767/attachment.pl>

From David.LeBlond at abbott.com  Mon Sep  3 13:01:28 2012
From: David.LeBlond at abbott.com (David LeBlond)
Date: Mon, 3 Sep 2012 06:01:28 -0500
Subject: [R-sig-ME] AUTO: New email address for Dave LeBlond:
	david.leblond@sbcglobal.net
Message-ID: <OF5866BC0C.797C578C-ON86257A6E.003C8F3C-86257A6E.003C8F3C@abbott.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120903/68b6f113/attachment.pl>

From J.L.Baker at 2008.hull.ac.uk  Mon Sep  3 18:10:04 2012
From: J.L.Baker at 2008.hull.ac.uk (Joanna L Baker)
Date: Mon, 3 Sep 2012 17:10:04 +0100
Subject: [R-sig-ME] Phylogenetic Logistic Regression + MCMCglmm
In-Reply-To: <20120901200622.19094ky48uwym6m8@www.staffmail.ed.ac.uk>
References: <41FF6BF3BF1CD34596E05F3C0A891B4102CAB512@EXCL2VS1.adir.hull.ac.uk>
	<20120901200622.19094ky48uwym6m8@www.staffmail.ed.ac.uk>
Message-ID: <41FF6BF3BF1CD34596E05F3C0A891B4102CAB52B@EXCL2VS1.adir.hull.ac.uk>

Dear Jarrod,

Thank you very much for your response. Unfortunately I still have issues
with convergence even when the residual variance is fixed. Part of the
problem I was having was the specification for visualizing the posterior
distributions of the phylogenetic portion of the analysis. 

This may be a silly question, but what is the difference between:

plot(my2$VCV[,1])

plot(my2$VCV[,1]/(rowSums(my2$VCV)+pi^2/3)).

The two produce very obviously different graphs, and I was wondering
what exactly the second part of the latter plot is related to?

Even looking at the correct graph for the phylogenetic part of the
analysis, it seems I am having trouble with 'animal' converging, and
have massive issues with the variance. I have attached the posterior
distributions for both your example (PhyLogRegSIM.pdf) and my own data
(PhyLogRegDAT.pdf) to illustrate the problem.  This occurs even when
running the models for a longer time.
 
I much appreciate your time and advice on this matter. 

Thanks,
Joanna Baker


Below is the code for my model specification:

prior1=list(R=list(V=1, fix=1), G=list(G1=list(V=1, nu=1, alpha.mu=0,
alpha.V=1000)))

myAinv2<-inverseA(mytree)$Ainv
my2<-MCMCglmm(DV~IV, random=~animal, ginverse=list(animal=myAinv2), 
	family="categorical", prior=prior1, data=mydata,
	nitt=10000000,thin=1000,burnin=100000)

plot(my2$Sol)
plot(my2$VCV[,1]/(rowSums(my2$VCV)+pi^2/3))


c2<-((16*sqrt(3))/(15*pi))^2
my2.int<-my2$Sol/(sqrt(1+c2))
my2.int
posterior.mode(my2.int)
summary(my2.int)

-----Original Message-----
From: Jarrod Hadfield [mailto:j.hadfield at ed.ac.uk] 
Sent: 01 September 2012 20:06
To: Joanna L Baker
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Phylogenetic Logistic Regression + MCMCglmm

Hi,

The most likely reason is that you have not fixed the non-identified
residual variance, although there are other possibilities. Here is an
example with a simulated tree and binary data:

tree<-rcoal(100)
x<-rnorm(100)
l<-rbv(tree, 1, nodes="TIPS")+x+rnorm(100) y<-rbinom(100, 1, plogis(l))

dat<-data.frame(y=y, x=x, species=tree$tip.label)

prior1=list(R=list(V=1, fix=1), G=list(G1=list(V=1, nu=1, alpha.mu=0,
alpha.V=1000)))

# residual variance fixed at 1.


Ainv<-inverseA(tree)$Ainv
m1<-MCMCglmm(y~x, random=~species, ginverse=list(species=Ainv),
family="categorical", prior=prior1, data=dat)

plot(m1$Sol)
# fixed effects should be zero and one

plot(m1$VCV[,1]/(rowSums(m1$VCV)+pi^2/3))

# phylogenetic ICC should be 1/(2+pi^2/3)=0.189. Note wdie credible
intervals - you need v.large phylogenies to get precise estimates with
binary data.

Cheers,

Jarrod






Quoting Joanna L Baker <J.L.Baker at 2008.hull.ac.uk> on Fri, 31 Aug 2012
11:39:08 +0100:

> Hello all,
>
> I am currently trying to use MCMCglmm to carry out a phylogenetic 
> logistic regression, with a single binary response and one or more 
> continuous or categorical predictors. However, I am having issues with

> the phylogenetic component, 'animal', converging. This occurs when I 
> use different datasets, with high, moderate and low phylogenetic
signal.
>
> I have tried improper (with the expected issues), proper, and 
> alpha-expanded priors but these do not seem to have much effect on the

> results. I should note that I have no issues with convergence when my 
> response is a continuous variable.
>
> Does anybody have a worked example with data and tree that I could use

> to get started with phylogenetic logistic regression or could somebody

> point me in the direction of any published work that has used the 
> package for such an analysis successfully? I'll much appreciate any 
> help with this.
>
> Thanks again,
>
> Joanna Baker
>
> University of Hull
>
> Cottingham Road
>
> Hull
>
> East Yorkshire
>
> j.l.baker at 2008.hull.ac.uk
>
>
>
>



--
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


-------------- next part --------------
**************************************************
To view the terms under which this email is 
distributed, please go to 
http://www2.hull.ac.uk/legal/disclaimer.aspx
**************************************************

From matz at utexas.edu  Sun Sep  2 05:22:14 2012
From: matz at utexas.edu (Mikhail Matz)
Date: Sat, 1 Sep 2012 22:22:14 -0500
Subject: [R-sig-ME] Fwd:  MCMCglmm poisson / not poisson
References: <3B8D8FD9-BC45-438A-BB96-98D377097532@utexas.edu>
Message-ID: <7824FDDB-79C8-46C1-9297-F703259170F6@utexas.edu>

I thought I better post the end of this discussion, for the benefit of future doubters?

Begin forwarded message:

> From: Mikhail Matz <matz at utexas.edu>
> Subject: Re: [R-sig-ME] MCMCglmm poisson / not poisson
> 
> Interesting!
> so each mcmc realization of latent variables is perfectly poisson-compatible, but their mean is not. OK! This totally works for me. 
> I now wonder why I expected the means to retain Poisson properties in the first place, actually
> 
> Thanks a lot!
> 
> Misha
> 
> On Aug 29, 2012, at 10:39 AM, Jarrod Hadfield <j.hadfield at ed.ac.uk> wrote:
> 
>> Hi,
>> 
>> library(MCMCglmm)
>> l<-rnorm(100,1,2)
>> y<-rpois(100, exp(l))
>> 
>> dat<-data.frame(y=y)
>> 
>> m1<-MCMCglmm(y~1, data=dat, family="poisson", pl=TRUE)
>> 
>> pp.poisson(y, colMeans(m1$Liab))
>> 
>> # looks bad
>> 
>> pp.poisson(y, colMeans(m1$Liab)+0.5*apply(m1$Liab, 2, var))
>> 
>> # looks better
>> 
>> for(i in 1:1000){
>> pp.poisson(y, m1$Liab[i,])
>> }
>> 
>> # looks good
>> 
>> Cheers,
>> 
>> Jarrod
>> 
>> 
>> 
>> 
>> 
>> Quoting Mikhail Matz <matz at utexas.edu> on Tue, 28 Aug 2012 22:06:45 -0500:
>> 
>>> 
>>> Hello -
>>> 
>>> I am playing with ways to justify that the MCMCglmm model fits my data well, which is quite important for me since I am hoping to be able to suggest MCMCglmm-based modeling as a general solution for a particular type of analysis.
>>> 
>>> I am running "poisson" family on counts data, with two random effects. Following Elston, D. A., R. Moss, et al. (2001). Parasitology 122: 563-569., I am checking whether my lognormal residuals (latent variable minus predicted value) are normally distributed (check), if my random effects (saved with pr=T) are normally distributed (more or less check), and then I try to see if the observed counts really look like Poisson samples based on the latent variables. Again, following Elston et al, I am making a p-p plot using this script (expert coders, please don't judge):
>>> 
>>> pp.poisson=function(counts,latents) {
>>> 	sim=c()
>>> 	for(i in 1:length(counts)){
>>> 		if (is.na(counts[i])) next
>>> 		data=counts[i]
>>> 		low=ppois(data,exp(latents[i]))-dpois(data,exp(latents[i]))
>>> 		up=ppois(data,exp(latents[i]))
>>> 		ss=seq(low,up,(up-low)/100)
>>> 		sim=append(sim,sample(ss,1))
>>> 	}
>>> 	sims=sort(sim)
>>> 	xx=(rank(sims)-0.5)/length(sims)
>>> 	plot(sims~xx)
>>> 	abline(0,1)
>>> }
>>> 
>>> ? and unfortunately it looks really ugly, like a very strongly bent  ' ~ ' rather than a line.
>>> The little script above seems to work; here is a sanity check:
>>> 
>>> psim=c()
>>> nnn=rnorm(500,10,10)
>>> for (i in 1:length(nnn)){
>>> 	psim=append(psim,rpois(1,exp(nnn[i])))
>>> }
>>> pp.poisson(psim,nnn)
>>> 
>>> I will be extremely grateful for any comments on this.
>>> 
>>> cheers
>>> 
>>> Misha
>>> UT Austin
>>> 
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>> 
>>> 
>> 
>> 
>> 
>> -- 
>> The University of Edinburgh is a charitable body, registered in
>> Scotland, with registration number SC005336.
>> 
>> 
> 


From jake987722 at hotmail.com  Mon Sep  3 21:10:24 2012
From: jake987722 at hotmail.com (Jake Westfall)
Date: Mon, 3 Sep 2012 13:10:24 -0600
Subject: [R-sig-ME] AUTO: New email address for Dave LeBlond:
 david.leblond@sbcglobal.net
In-Reply-To: <OF5866BC0C.797C578C-ON86257A6E.003C8F3C-86257A6E.003C8F3C@abbott.com>
References: <OF5866BC0C.797C578C-ON86257A6E.003C8F3C-86257A6E.003C8F3C@abbott.com>
Message-ID: <SNT107-W3125C2CEB2916C33E07CF5CBAB0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120903/2be0c12a/attachment.pl>

From j.hadfield at ed.ac.uk  Tue Sep  4 11:50:21 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Tue, 04 Sep 2012 10:50:21 +0100
Subject: [R-sig-ME] Phylogenetic Logistic Regression + MCMCglmm
In-Reply-To: <41FF6BF3BF1CD34596E05F3C0A891B4102CAB52B@EXCL2VS1.adir.hull.ac.uk>
References: <41FF6BF3BF1CD34596E05F3C0A891B4102CAB512@EXCL2VS1.adir.hull.ac.uk>
	<20120901200622.19094ky48uwym6m8@www.staffmail.ed.ac.uk>
	<41FF6BF3BF1CD34596E05F3C0A891B4102CAB52B@EXCL2VS1.adir.hull.ac.uk>
Message-ID: <20120904105021.72001375a63wcu00@www.staffmail.ed.ac.uk>

Hi,

Quoting Joanna L Baker <J.L.Baker at 2008.hull.ac.uk> on Mon, 3 Sep 2012  
17:10:04 +0100:

> Dear Jarrod,
>
> Thank you very much for your response. Unfortunately I still have issues
> with convergence even when the residual variance is fixed. Part of the
> problem I was having was the specification for visualizing the posterior
> distributions of the phylogenetic portion of the analysis.
>
> This may be a silly question, but what is the difference between:
>
> plot(my2$VCV[,1])

This is the variance of the phylogenetic effects at the tips (if the  
tree is scaled and ultrametric) on the latent scale.
>
> plot(my2$VCV[,1]/(rowSums(my2$VCV)+pi^2/3)).

my2$VCV[,1] depends on your choice of R$V. The above is the  
intra-class correlation on the latent scale and does not depend on R.   
pi^2/3 is the variance of the logistic distribution.  Think of the  
model in this way:

latent = mu + u + e + e'

where e is the normally distributed residual with variance R$V, and e'  
is an additional residual distributed as logistic (if logit link  
used), or normal (if probit link used) or extreme value (if  
complementary log-log link used).  The variance of e' with logit-link  
is pi^2/3.

Note that the outcome is not stochastic when e' is conditioned on. In  
this case if latent<0 then the outcome is 0 with probability 1.

>
> The two produce very obviously different graphs, and I was wondering
> what exactly the second part of the latter plot is related to?
>
> Even looking at the correct graph for the phylogenetic part of the
> analysis, it seems I am having trouble with 'animal' converging, and
> have massive issues with the variance. I have attached the posterior
> distributions for both your example (PhyLogRegSIM.pdf) and my own data
> (PhyLogRegDAT.pdf) to illustrate the problem.  This occurs even when
> running the models for a longer time.

I would say that the phylogenetic signal in your data is high and/or  
sample sizes are small and/or one outcome is rare (you need v. large  
sample sizes with binary data).  The chain is getting "stuck" at H2  
(or lambda) =1. You could run it longer, but I suspect you are hitting  
numerical problems (|latent variable| exceeding 20).  One option is to  
move to probit link (family="ordinal") and use the chi square prior  
(see http://permalink.gmane.org/gmane.comp.lang.r.lme4.devel/8531)  
which down weights H2 values very close to 0 or 1 compared to some  
other priors. If the absolute value of the latent variable stays below  
7, you should be OK. If not, you will probably have to move to another  
piece of software which can deal with extreme predicted probabilities  
more elegantly. I have used truncated latent variables in my own work,  
but I'm not convinced they are a general or robust solution so have  
not implemented them in the release version.

Cheers,

Jarrod




>
> I much appreciate your time and advice on this matter.
>
> Thanks,
> Joanna Baker
>
>
> Below is the code for my model specification:
>
> prior1=list(R=list(V=1, fix=1), G=list(G1=list(V=1, nu=1, alpha.mu=0,
> alpha.V=1000)))
>
> myAinv2<-inverseA(mytree)$Ainv
> my2<-MCMCglmm(DV~IV, random=~animal, ginverse=list(animal=myAinv2),
> 	family="categorical", prior=prior1, data=mydata,
> 	nitt=10000000,thin=1000,burnin=100000)
>
> plot(my2$Sol)
> plot(my2$VCV[,1]/(rowSums(my2$VCV)+pi^2/3))
>
>
> c2<-((16*sqrt(3))/(15*pi))^2
> my2.int<-my2$Sol/(sqrt(1+c2))
> my2.int
> posterior.mode(my2.int)
> summary(my2.int)
>
> -----Original Message-----
> From: Jarrod Hadfield [mailto:j.hadfield at ed.ac.uk]
> Sent: 01 September 2012 20:06
> To: Joanna L Baker
> Cc: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Phylogenetic Logistic Regression + MCMCglmm
>
> Hi,
>
> The most likely reason is that you have not fixed the non-identified
> residual variance, although there are other possibilities. Here is an
> example with a simulated tree and binary data:
>
> tree<-rcoal(100)
> x<-rnorm(100)
> l<-rbv(tree, 1, nodes="TIPS")+x+rnorm(100) y<-rbinom(100, 1, plogis(l))
>
> dat<-data.frame(y=y, x=x, species=tree$tip.label)
>
> prior1=list(R=list(V=1, fix=1), G=list(G1=list(V=1, nu=1, alpha.mu=0,
> alpha.V=1000)))
>
> # residual variance fixed at 1.
>
>
> Ainv<-inverseA(tree)$Ainv
> m1<-MCMCglmm(y~x, random=~species, ginverse=list(species=Ainv),
> family="categorical", prior=prior1, data=dat)
>
> plot(m1$Sol)
> # fixed effects should be zero and one
>
> plot(m1$VCV[,1]/(rowSums(m1$VCV)+pi^2/3))
>
> # phylogenetic ICC should be 1/(2+pi^2/3)=0.189. Note wdie credible
> intervals - you need v.large phylogenies to get precise estimates with
> binary data.
>
> Cheers,
>
> Jarrod
>
>
>
>
>
>
> Quoting Joanna L Baker <J.L.Baker at 2008.hull.ac.uk> on Fri, 31 Aug 2012
> 11:39:08 +0100:
>
>> Hello all,
>>
>> I am currently trying to use MCMCglmm to carry out a phylogenetic
>> logistic regression, with a single binary response and one or more
>> continuous or categorical predictors. However, I am having issues with
>
>> the phylogenetic component, 'animal', converging. This occurs when I
>> use different datasets, with high, moderate and low phylogenetic
> signal.
>>
>> I have tried improper (with the expected issues), proper, and
>> alpha-expanded priors but these do not seem to have much effect on the
>
>> results. I should note that I have no issues with convergence when my
>> response is a continuous variable.
>>
>> Does anybody have a worked example with data and tree that I could use
>
>> to get started with phylogenetic logistic regression or could somebody
>
>> point me in the direction of any published work that has used the
>> package for such an analysis successfully? I'll much appreciate any
>> help with this.
>>
>> Thanks again,
>>
>> Joanna Baker
>>
>> University of Hull
>>
>> Cottingham Road
>>
>> Hull
>>
>> East Yorkshire
>>
>> j.l.baker at 2008.hull.ac.uk
>>
>>
>>
>>
>
>
>
> --
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
>
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From schmettow at web.de  Wed Sep  5 09:54:56 2012
From: schmettow at web.de (Martin Schmettow)
Date: Wed, 5 Sep 2012 09:54:56 +0200
Subject: [R-sig-ME] False convergence when running lmer with Zelig
	example for gamma.mixed
In-Reply-To: <20111108010432.40d1c5a1.Hugo.Mildenberger@web.de>
References: <20111108010432.40d1c5a1.Hugo.Mildenberger@web.de>
Message-ID: <004601cd8b3b$bdb772a0$392657e0$@web.de>

Hi Hugo,

I've tried several times fitting a gamma model myself with lme4. Never
succeeded. 
Many others have reported similar problems fitting gamma models with lme4.
Seemingly, it never worked.
The easiest way of dealing with durations is log-transformation and then use
a Gaussian LMM.

CU, Martin.


> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-
> models-bounces at r-project.org] On Behalf Of Hugo Mildenberger
> Sent: Tuesday, November 08, 2011 1:05 AM
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] False convergence when running lmer with Zelig example
> for gamma.mixed
> 
> Dear list members,
> 
> while testing Zelig example code related to mixed linear models, I got a
very
> strange result from "gamma.mixed". I therefore did run lmer directly to
see if
> the convergence problem was perhaps due to a parameter problem only,
> but it is not. Since the example should have been viable in the past, I'm
> wondering what is going wrong today? The only difference is the method
> specification, which was PQL, but PQL is not supported anymore. Using nAGQ
> > 1 makes no difference.
> 
>  # for coalition2 dataset only
>  require(Zelig)
>  data(coalition2)
> 
>  require(lme4)
> 
>  l.m <- lmer(duration ~ invest +
>                         fract +
>                         polar +
>                         numst2 +
>                         crisis + (1 | country),
>              data   = coalition2,
>              family = Gamma(link = log))
>  summary(l.m)
>  packageVersion('lme4')
> 
> Here is the output:
> 
> R version 2.13.1 (2011-07-08)
> [...]
> Warning message:
> In mer_finalize(ans) : false convergence (8)
> >  summary(l.m)
> Generalized linear mixed model fit by the Laplace approximation
> Formula: duration ~ invest + fract + polar + numst2 + crisis + (1 |
country)
>    Data: coalition2
>       AIC      BIC  logLik deviance
>  -3918661 -3918632 1959339 -3918677
> Random effects:
>  Groups   Name        Variance    Std.Dev.
>  country  (Intercept) 5.1269e+154 2.2643e+77
>  Residual             4.1745e+155 6.4610e+77
> Number of obs: 304, groups: country, 14
> 
> Fixed effects:
>               Estimate Std. Error t value
> (Intercept)  4.108e+00  8.941e+77       0
> invest      -3.882e-01  2.252e+77       0
> fract        4.980e-01  1.380e+75       0
> polar       -1.850e-02  1.017e+76       0
> numst2       3.696e-01  1.431e+77       0
> crisis       2.569e-02  4.590e+75       0
> 
> Correlation of Fixed Effects:
>        (Intr) invest fract  polar  numst2
> invest -0.020
> fract  -0.984 -0.043
> polar   0.227 -0.284 -0.262
> numst2 -0.439 -0.120  0.360 -0.039
> crisis -0.019 -0.283  0.000 -0.217  0.108
> >  packageVersion('lme4')
> [1] '0.999375.42'
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From roby.joehanes at nih.gov  Wed Sep  5 16:42:56 2012
From: roby.joehanes at nih.gov (Joehanes, Roby (NIH/NHLBI) [F])
Date: Wed, 5 Sep 2012 10:42:56 -0400
Subject: [R-sig-ME] Using weights properly in lmer
Message-ID: <CC6CDD30.36F8%joehanesr@mail.nih.gov>

Hi all:

I hope this does not get asked very often, but I am somewhat confused of how weights can be used in lmer. Specifically, as shown here:
http://web.archiveorange.com/archive/v/rOz2zfpzU2udVOwySzEz

w<-rep(1,nrow(sleepstudy))
(fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy) )
(fm2 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy, weights = w) )
(fm3 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy, weights = w/sum(w)) )

I still found the same problems as in the above URL even with the latest lmer from the Subversion.

My particular goal is to use the weights to offset the heteroscedasticity. That is, I have a matrix, say, A, of m x n. Then,

y <- as.numeric(A)
wt <- rep(1.0/apply(A,1,var), n)

What I want is to run:
result <- lmer(y ~ ...some covariates..., weights = wt)

But the result is strange. The problem is like the above. Is this a bug? Or is there any convention I need to know? Please advise.

Thank you,
Roby


From F.DUYME at arvalisinstitutduvegetal.fr  Wed Sep  5 17:35:55 2012
From: F.DUYME at arvalisinstitutduvegetal.fr (DUYME Florent)
Date: Wed, 5 Sep 2012 17:35:55 +0200
Subject: [R-sig-ME] multi-environments trial : how to take into account
 different residual variances ?
Message-ID: <674BC74273529E40A236CE2FB772DE093D1F3C10B7@srv-exch-bgn.arvalis-fr.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
URL : <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120905/8443bfc1/attachment.pl>

From webbsjulie at gmail.com  Tue Sep  4 23:52:03 2012
From: webbsjulie at gmail.com (Julie Webbs)
Date: Tue, 4 Sep 2012 22:52:03 +0100
Subject: [R-sig-ME] Help
Message-ID: <CAOQbByiReYMnibZ+mOjDD5Luh7XVgCtNDdxCbJnYZb_=3kG+qA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120904/ccc8d5fb/attachment.pl>

From f.calboli at imperial.ac.uk  Wed Sep  5 19:21:01 2012
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Wed, 5 Sep 2012 18:21:01 +0100
Subject: [R-sig-ME] Help
In-Reply-To: <CAOQbByiReYMnibZ+mOjDD5Luh7XVgCtNDdxCbJnYZb_=3kG+qA@mail.gmail.com>
References: <CAOQbByiReYMnibZ+mOjDD5Luh7XVgCtNDdxCbJnYZb_=3kG+qA@mail.gmail.com>
Message-ID: <893CA64A-CD0C-4A4E-B33E-62B67C9321FD@imperial.ac.uk>

On 4 Sep 2012, at 22:52, Julie Webbs <webbsjulie at gmail.com> wrote:

> Dear Sir/madam
> 
> I am using* **Package 'lme4*'. in R, I wonder whether the variances
> reported , say in penicillin, are the variance components that we estimate
> in SPSS ( Analyze>General linear model> Variance Component).

I, and I suspect many on this list, have no access

-- to SPSS
-- to the data you are talking about

It would be helpful if you could provide

1) the data
2) the model you are using and the results from SPSS
3) the modle and results from your analysis with lme4

and then ask a reasonable question, such as, 'I assume that I am specifying the same model in R and SPSS, but my results differ, could someone help me understand whether I am wrong assuming that I am specifying the same model, or what is going on otherwise?', or, 'SPSS spits out this estimate, how do I get it with lme4?'

HTH

F


> 
> Many thanks, looking forward to hearing from you.
> 
> Julie
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

--
Federico C. F. Calboli
Neuroepidemiology and Ageing Research
Imperial College, St. Mary's Campus
Norfolk Place, London W2 1PG

Tel +44 (0)20 75941602   Fax +44 (0)20 75943193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com


From bates at stat.wisc.edu  Wed Sep  5 19:30:56 2012
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 5 Sep 2012 12:30:56 -0500
Subject: [R-sig-ME] Help
In-Reply-To: <893CA64A-CD0C-4A4E-B33E-62B67C9321FD@imperial.ac.uk>
References: <CAOQbByiReYMnibZ+mOjDD5Luh7XVgCtNDdxCbJnYZb_=3kG+qA@mail.gmail.com>
	<893CA64A-CD0C-4A4E-B33E-62B67C9321FD@imperial.ac.uk>
Message-ID: <CAO7JsnSGUvJDpHPjjt4240rsBQ3Z3Hoz84EWMg0dSPrt0V=u9w@mail.gmail.com>

On Wed, Sep 5, 2012 at 12:21 PM, Federico Calboli
<f.calboli at imperial.ac.uk> wrote:
> On 4 Sep 2012, at 22:52, Julie Webbs <webbsjulie at gmail.com> wrote:
>
>> Dear Sir/madam
>>
>> I am using* **Package 'lme4*'. in R, I wonder whether the variances
>> reported , say in penicillin, are the variance components that we estimate
>> in SPSS ( Analyze>General linear model> Variance Component).
>
> I, and I suspect many on this list, have no access
>
> -- to SPSS
> -- to the data you are talking about
>
> It would be helpful if you could provide
>
> 1) the data

Julia may be referring to the Penicillin data set from the lme4
package itself.  (Julie: R is a case-sensitive language so
"Penicillin" is different from "penicillin".)

> 2) the model you are using and the results from SPSS
> 3) the modle and results from your analysis with lme4
>
> and then ask a reasonable question, such as, 'I assume that I am specifying the same model in R and SPSS, but my results differ, could someone help me understand whether I am wrong assuming that I am specifying the same model, or what is going on otherwise?', or, 'SPSS spits out this estimate, how do I get it with lme4?'
>
> HTH
>
> F
>
>
>>
>> Many thanks, looking forward to hearing from you.
>>
>> Julie
>>
>>       [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> --
> Federico C. F. Calboli
> Neuroepidemiology and Ageing Research
> Imperial College, St. Mary's Campus
> Norfolk Place, London W2 1PG
>
> Tel +44 (0)20 75941602   Fax +44 (0)20 75943193
>
> f.calboli [.a.t] imperial.ac.uk
> f.calboli [.a.t] gmail.com
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From villegaskary at gmail.com  Wed Sep  5 20:45:24 2012
From: villegaskary at gmail.com (Karina Villegas)
Date: Wed, 5 Sep 2012 11:45:24 -0700
Subject: [R-sig-ME] help for HLM
Message-ID: <CALcKDYHZFke19gyU_18a=Dc1uxso96h3s_c0PdTDVPA6EuPZmw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120905/c99f7b35/attachment.pl>

From bates at stat.wisc.edu  Wed Sep  5 21:39:59 2012
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 5 Sep 2012 14:39:59 -0500
Subject: [R-sig-ME] help for HLM
In-Reply-To: <CALcKDYHZFke19gyU_18a=Dc1uxso96h3s_c0PdTDVPA6EuPZmw@mail.gmail.com>
References: <CALcKDYHZFke19gyU_18a=Dc1uxso96h3s_c0PdTDVPA6EuPZmw@mail.gmail.com>
Message-ID: <CAO7JsnRVyprc4c_j468pFixTgsoVZpttNuNRz6CXcrvgQd517g@mail.gmail.com>

The important part of this output is the line Number of groups: 2

You are trying to estimate three variance-covariance parameters with
only two levels of Trip.  You would need many levels of Trip to be
able to do so.

When you have only two levels of a categorical variable you must model
it with fixed-effects parameters even though from the structure of the
experiment it may seem reasonable to use random effects.

On Wed, Sep 5, 2012 at 1:45 PM, Karina Villegas <villegaskary at gmail.com> wrote:
> *Dear R experts:*
> *
> *
> *I am running R version 2.12.1 on Windows 2007. I am studying the effects
> maternal behavior in the body condition of sea lion pups from California. *
>
> *
> *
>
> *I'm trying to make a hierarchical linear model*
>
> *When I run the full model if I get these results:*
>
> *> LevelModel7 <- lme(PBC ~ Sex*Dur.nurse + Sex*Freq.interaction +
> Sex*Density.females , random=~Sex|Trip, data=Dataset)*
>
> *> summary (LevelModel7)*
>
> *Linear mixed-effects model fit by REML*
>
> * Data: Dataset*
>
> *       AIC      BIC    logLik*
>
> *  446.1461 478.1074 -211.0730*
>
> * *
>
> *Random effects:*
>
> * Formula: ~Sex | Trip*
>
> * Structure: General positive-definite, Log-Cholesky parametrization*
>
> *            StdDev    Corr *
>
> *(Intercept) 0.5250613 (Intr)*
>
> *Sex         0.0614132 0    *
>
> *Residual    1.4865472      *
>
> * *
>
> *Fixed effects: PBC ~ Sex * Dur.nurse + Sex * Freq.interaction + Sex *
> Density.females*
>
> *                         Value Std.Error  DF    t-value p-value*
>
> *(Intercept)            3.32002 13.521223 105  0.2455415  0.8065*
>
> *Sex                    0.76586  1.584268 105  0.4834144  0.6298*
>
> *Dur.nurse              1.95957  1.798351 105  1.0896502  0.2784*
>
> *Freq.interaction      -0.22624  0.219637 105 -1.0300505  0.3054*
>
> *Density.females      -32.19456 19.335512 105 -1.6650480  0.0989*
>
> *Sex:Dur.nurse         -0.21788  0.209796 105 -1.0385351  0.3014*
>
> *Sex:Freq.interaction   0.02165  0.025693 105  0.8426169  0.4014*
>
> *Sex:Density.females    3.15918  2.258348 105  1.3988881  0.1648*
>
> * Correlation:*
>
> *                     (Intr) Sex    Dr.nrs Frq.nt Dnsty. Sx:Dr. Sx:Fr.*
>
> *Sex                  -0.998                                         *
>
> *Dur.nurse            -0.928  0.924                                  *
>
> *Freq.interaction     -0.194  0.195  0.103                           *
>
> *Density.females       0.613 -0.609 -0.809 -0.346                    *
>
> *Sex:Dur.nurse         0.928 -0.928 -0.996 -0.109  0.802             *
>
> *Sex:Freq.interaction  0.196 -0.198 -0.108 -0.994  0.353  0.108      *
>
> *Sex:Density.females  -0.611  0.609  0.802  0.354 -0.991 -0.807 -0.350*
>
> * *
>
> *Standardized Within-Group Residuals:*
>
> *        Min          Q1         Med          Q3         Max*
>
> *-2.46806088 -0.47865795 -0.05134942  0.57369682  2.64445772*
>
> * *
>
> *Number of Observations: 114*
>
> *Number of Groups: 2*
>
> * *
>
> *When I run simple models (one variable included: females Density,
>  Frequency interaction) I have no problem either.*
>
> * *
>
> *> LevelModel3 <- lme(PBC ~ Sex*Density.females , random=~Sex|Trip,
> data=Dataset)*
>
> *> summary (LevelModel3)*
>
> *Linear mixed-effects model fit by REML*
>
> * Data: Dataset*
>
> *       AIC      BIC    logLik*
>
> *  428.5119 450.1158 -206.2560*
>
> * *
>
> *Random effects:*
>
> * Formula: ~Sex | Trip*
>
> * Structure: General positive-definite, Log-Cholesky parametrization*
>
> *            **StdDev       Corr *
>
> *(Intercept) 1.076517e+00 (Intr)*
>
> *Sex         5.258193e-05 0    *
>
> *Residual    1.489254e+00      *
>
> * *
>
> *Fixed effects: PBC ~ Sex * Density.females*
>
> *                         Value Std.Error  DF   t-value p-value*
>
> *(Intercept)          14.774095  4.876298 109  3.029777  0.0031*
>
> *Sex                  -0.646479  0.566328 109 -1.141528  0.2562*
>
> *Density.females     -18.980461 10.099637 109 -1.879321  0.0629*
>
> *Sex:Density.females   1.892986  1.185115 109  1.597302  0.1131*
>
> * Correlation:*
>
> *                    (Intr) Sex    Dnsty.*
>
> *Sex                 -0.984             *
>
> *Density.females     -0.857  0.862      *
>
> *Sex:Density.females  0.855 -0.869 -0.995*
>
> * *
>
> *Standardized Within-Group Residuals:*
>
> *         Min           Q1          Med           Q3          Max*
>
> *-2.678669640 -0.591855777  0.006108111  0.543417969  2.424555266*
>
> * *
>
> *Number of Observations: 114*
>
> *Number of Groups: 2*
>
>
>
> *> LevelModel4 <- lme(PBC ~ Sex*Freq.interaction , random=~Sex|Trip,
> data=Dataset)*
>
> *> summary (LevelModel4)*
>
> *Linear mixed-effects model fit by REML*
>
> * Data: Dataset*
>
> *       AIC      BIC    logLik*
>
> *  451.1833 472.7872 -217.5917*
>
> * *
>
> *Random effects:*
>
> * Formula: ~Sex | Trip*
>
> * Structure: General positive-definite, Log-Cholesky parametrization*
>
> *            **StdDev       Corr *
>
> *(Intercept) 1.934101e+00 (Intr)*
>
> *Sex         3.442453e-05 0    *
>
> *Residual    1.527767e+00      *
>
> * *
>
> *Fixed effects: PBC ~ Sex * Freq.interaction*
>
> *                         Value Std.Error  DF   t-value p-value*
>
> *(Intercept)          12.595151  4.211533 109  2.990633  0.0034*
>
> *Sex                  -0.535742  0.465667 109 -1.150484  0.2525*
>
> *Freq.interaction     -0.334393  0.199095 109 -1.679564  0.0959*
>
> *Sex:Freq.interaction  0.039834  0.023241 109  1.713935  0.0894*
>
> * Correlation:*
>
> *                     (Intr) Sex    Frq.nt*
>
> *Sex                  -0.943             *
>
> *Freq.interaction     -0.745  0.783      *
>
> *Sex:Freq.interaction  0.743 -0.788 -0.996*
>
> * *
>
> *Standardized Within-Group Residuals:*
>
> *        Min          Q1         Med          Q3         Max*
>
> *-2.79224714 -0.46765869 -0.04400343  0.66192444  2.53486123*
>
> * *
>
> *Number of Observations: 114*
>
> *Number of Groups: 2*
>
> * *
>
> * *
>
> *However, when I include only variable Dur.nurse, I get the following error
> message:*
>
>> LevelModel2 <- lme(PBC ~ Sex*Dur.nurse, random=~Sex|Trip, data=Dataset)
>
> Error in lme.formula(PBC ~ Sex * Dur.nurse, random = ~Sex | Trip, data =
> Dataset) :
>
>   nlminb problem, convergence error code = 1
>
>   message = iteration limit reached without convergence (9)
>
>
>
> *We thought it was a problem with the number of iterations and I increased
> the iterations, but I still get the error:*
>
>> LevelModel1 <- lme(PBC ~ Sex*Dur.nurse, random=~Sex|Trip, data=Dataset,
> control=lmeControl(maxIter=200))
>
> Error in lme.formula(PBC ~ Sex * Dur.nurse, random = ~Sex | Trip, data =
> Dataset,  :
>
>   nlminb problem, convergence error code = 1
>
>   message = iteration limit reached without convergence (9)
>
>
>
> *Somebody has any idea?*
>
>
>
> *Thanks and regards*
>
> *Karina*
>
> --
> Biol. Karina Villegas Cervantes
> Estudiante de Maestr?a PCMyL - UNAM
>
> Laboratorio de Ecologia de Pinnipedos Burney J. Le Boueuf.
> CICIMAR-IPN
> Av. Instituto Politecnico Nacional s/n.Col.Playa Palo de Santa Rita
> La Paz Baja California Sur, Mexico.
>
>         [[alternative HTML version deleted]]
>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From vcareau at ucr.edu  Thu Sep  6 00:26:44 2012
From: vcareau at ucr.edu (Vincent Careau)
Date: Wed, 5 Sep 2012 15:26:44 -0700
Subject: [R-sig-ME] Heterogeneous (co)variances in a bivariate model with
	MCMCglmm?
Message-ID: <6A1067D3F7C24C07B19FE7D45979E767@PCderobo>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120905/5fd66a29/attachment.pl>

From mike.lwrnc at gmail.com  Thu Sep  6 00:41:06 2012
From: mike.lwrnc at gmail.com (Mike Lawrence)
Date: Wed, 5 Sep 2012 19:41:06 -0300
Subject: [R-sig-ME] Characterizing correlation between binomial effects
Message-ID: <CAB+QPJCY56yC6QUGJrkhzs1yH=1xhgQdxK-j3OrkLEQhiG5C2A@mail.gmail.com>

I have data from multiple human participants who each completed two
tasks. In each task there were two conditions, and in each condition
there are multiple trials that yield a binomial (1/0) outcome. I would
like to evaluate whether there is a correlation between Ss' condition
effect in task 1 and their condition effect in task 2. Traditionally
in my field, this would be achieved by collapsing the raw
trial-by-trial data to proportions in each task then correlate the
resulting scores:

    task1_scores = ddply(
        .data = task1_trials
        , .variables = .(participant)
        , .fun = function(x){
            data.frame( value = mean( x$response[x$condition=='A'] ) -
mean( x$response[x$condition=='B'] ) )
        }
    )
    task2_scores = ddply(
        .data = task2_trials
        , .variables = .(participant)
        , .fun = function(x){
            data.frame( value = mean( x$response[x$condition=='A'] ) -
mean( x$response[x$condition=='B'] ) )
        }
    )
    cor(task1_scores$value,task2_scores$value)

However, this clearly ignores the binomial nature of the raw data,
with the consequence (assuming the monte carlo simulation that I ran
last night is correct) that type-I error rates will be inflated for a
NHST test on the final correlation (at least, when the raw proportions
are far from 50%, which is typically the case for data I encounter).

Does anyone have any input on how to address the "is there a
correlation between the condition effects in the tasks" question in a
generalized linear mixed effects modelling framework?

One idea I had was to combine the two task data frames to a single
data frame, code task as a variable, and fit a full glmm model:

    fit = glmer(
        family = binomial
        , data = both_tasks
        , formula = response ~ task*condition + (1+task*condition|participant)
    )

then obtain the model's predictions for each participant's data:

    r = as.matrix(ranef(fit)$participant)
    temp = expand.grid(task=c('1','2'),condition=c('A','B'),response=0)
    from_terms = terms(response~task*condition)
    mm = model.matrix(from_terms,temp)
    preds = expand.grid(ask=c('1','2'),condition=c('A','B'),participant=1:nrow(r),response=0)
    for(i in 1:nrow(r)){
        preds$response[preds$participant==i] = as.numeric(mm %*% r[i,])
    }

then use these predictions to compute each participant's condition
effect in each task and correlate the resulting scores:

    scores = ddply(
        .data = preds
        , .variables = .(participant,task)
        , .fun = function(x){
            data.frame( value = diff(x$response) )
        }
    )
    cor( scores$value[scores$task=='1'] , scores$value[scores$task=='2'] )

But it appears that this approach also suffers from a type-I error
inflation (again, according to a simulation I ran last night).

Any suggestions?

Mike


From datkins at u.washington.edu  Thu Sep  6 12:25:36 2012
From: datkins at u.washington.edu (David Atkins)
Date: Thu, 06 Sep 2012 12:25:36 +0200
Subject: [R-sig-ME] Heterogeneous (co)variances in a bivariate model
 with MCMCglmm?
In-Reply-To: <6A1067D3F7C24C07B19FE7D45979E767@PCderobo>
References: <6A1067D3F7C24C07B19FE7D45979E767@PCderobo>
Message-ID: <50487A20.5020509@u.washington.edu>


Vincent--

I've been doing a bit of work on multivariate mixed models using 
MCMCglmm with some colleagues (actually, for a tutorial paper on that 
topic...).

The following will fit random intercepts and slopes for two outcomes but 
*not* allowing correlations between outcomes (note that "j" is the 
grouping variable in our data).  Data is for simple two treatment over 
time study (common in psychiatry / psychology) with two outcomes.

# random intercept and slopes, but not correlated across outcomes
prior2 <- list(R = list(V = diag(2), nu = 1.002),
                G = list(G1 = list(V = diag(2), nu = 1.002),
                         G2 = list(V = diag(2), nu = 1.002)))

mult.mcmc3 <- MCMCglmm(cbind(y, y2) ~ -1 + trait*(tx*time),
                        data = data.wide,
                        random = ~ us(at.level(trait, 1) +
                                   at.level(trait, 1):time):j +
                                   us(at.level(trait, 2) +
                                   at.level(trait, 2):time):j,
                        rcov = ~ us(trait):units,
                        family = c("gaussian","gaussian"),
                        verbose = TRUE,
                        nitt = 25000, burnin = 5000, thin = 20,
                        prior = prior2)


Fitting correlated effects is actually a bit less "wordy" in terms of 
syntax:

# unstructured matrix between outcomes
prior3 <- list(R = list(V = .2*diag(2), nu = 3),
                G = list(G1 = list(V = diag(c(.2,.2,.1,.1)), nu = 5)))

mult.mcmc3.1 <- MCMCglmm(cbind(y, y2) ~ -1 + trait*(tx*time),
                        data = data.wide,
                        random = ~ us(-1 + trait*time):j,
                        rcov = ~ us(trait):units,
                        family = c("gaussian","gaussian"),
                        verbose = TRUE,
                        nitt = 25000, burnin = 5000, thin = 20,
                        prior = prior3)

Note that we're using simulated data and have been futzing a bit with 
priors, so I would definitely *not* suggest "prior3" for off-the-shelf 
usage.

Hope that helps.

cheers, Dave


-- 
Dave Atkins, PhD
University of Washington
datkins at u.washington.edu
http://depts.washington.edu/cshrb/

August 1 - October 30:

Universitat Zurich
Psychologisches Institut
Klinische Psychologie
Binzmuhlestrasse 14/23
CH-8050 Zurich

+41 44 635 71 75

Hi,

Is it possible to fit a bivariate model with heterogeneous variance
components in MCMCglmm?



Let's say we measured two traits (x1 and x2) on 50 individuals, twice per
year over two years (2008 and 2009). CREATE THE DATA:

ID<-c(rbind(seq(1,50),seq(1,50),seq(1,50),seq(1,50)))

YEAR<-rep(2008:2009,100)

x1<-ID/10+rnorm(200, mean = 0, sd = 2)

tmp<-data.frame(ID, YEAR,x1)

tmp$ID<-as.factor(tmp$ID)

tmp$YEAR<-as.factor(tmp$YEAR)

plot(x1~ID,data=tmp)

tmp2008<-subset(tmp,YEAR=="2008")

tmp2009<-subset(tmp,YEAR=="2009")

tmp2008$x2<-rnorm(100, mean = 0, sd = 3)+tmp2008$x1

tmp2009$x2<-rnorm(100, mean = 0, sd = 3)-tmp2009$x1+2*(mean(tmp2009$x1))

DATA<-rbind(tmp2008,tmp2009)

plot(x1~x2,DATA, col=YEAR)



In section 3.4 of the MCMCglmm course notes ("Heterogenous Residual
Variance"), we can see how to estimate a separate variance for ID and
residuals according to YEAR in a univariate model:

pr = list(R = list(V = diag(2), nu = 1), G = list(G1 = list(V = diag(2),nu =
1)))

model<-MCMCglmm(x1~1,

random=~idh(YEAR):ID,

rcov=~idh(YEAR):units,

data=DATA, nitt = 130000, thin = 100, burnin = 30000,prior=pr)



We can also fit a bivariate model:

BI.model<-MCMCglmm(cbind(x1, x2)~1,random=~us(trait):ID,

                                       rcov=~us(trait):units,

                                       family=c("gaussian","gaussian"),

                                       prior=pr,data=DATA)



However, I cannot find how to combine the two models above, where we
estimate separate variance (and covariances) in a bivariate model. Is this
possible at all? For sake of comparison, the code for such a model in
ASRreml-R is:



BI.model<-asreml(cbind(x1,x2)~trait,
random=~at(YEAR):us(trait):ID,rcov=~at(YEAR):units:us(trait),data=DATA,maxit
er=500)



cheers,

Vincent




	[[alternative HTML version deleted]]


From mike.lwrnc at gmail.com  Thu Sep  6 17:09:42 2012
From: mike.lwrnc at gmail.com (Mike Lawrence)
Date: Thu, 6 Sep 2012 12:09:42 -0300
Subject: [R-sig-ME] Characterizing correlation between binomial effects
In-Reply-To: <CAB+QPJCY56yC6QUGJrkhzs1yH=1xhgQdxK-j3OrkLEQhiG5C2A@mail.gmail.com>
References: <CAB+QPJCY56yC6QUGJrkhzs1yH=1xhgQdxK-j3OrkLEQhiG5C2A@mail.gmail.com>
Message-ID: <CAB+QPJBxvGLsa+7U3QFfzrOvpEZPN9Yqb-Fqhy=bisT6UbxPbw@mail.gmail.com>

Reinhold Kliegl provided me with the solution, which I've coded into
an example available here:

https://gist.github.com/3657148


On Wed, Sep 5, 2012 at 7:41 PM, Mike Lawrence <mike.lwrnc at gmail.com> wrote:
> I have data from multiple human participants who each completed two
> tasks. In each task there were two conditions, and in each condition
> there are multiple trials that yield a binomial (1/0) outcome. I would
> like to evaluate whether there is a correlation between Ss' condition
> effect in task 1 and their condition effect in task 2. Traditionally
> in my field, this would be achieved by collapsing the raw
> trial-by-trial data to proportions in each task then correlate the
> resulting scores:
>
>     task1_scores = ddply(
>         .data = task1_trials
>         , .variables = .(participant)
>         , .fun = function(x){
>             data.frame( value = mean( x$response[x$condition=='A'] ) -
> mean( x$response[x$condition=='B'] ) )
>         }
>     )
>     task2_scores = ddply(
>         .data = task2_trials
>         , .variables = .(participant)
>         , .fun = function(x){
>             data.frame( value = mean( x$response[x$condition=='A'] ) -
> mean( x$response[x$condition=='B'] ) )
>         }
>     )
>     cor(task1_scores$value,task2_scores$value)
>
> However, this clearly ignores the binomial nature of the raw data,
> with the consequence (assuming the monte carlo simulation that I ran
> last night is correct) that type-I error rates will be inflated for a
> NHST test on the final correlation (at least, when the raw proportions
> are far from 50%, which is typically the case for data I encounter).
>
> Does anyone have any input on how to address the "is there a
> correlation between the condition effects in the tasks" question in a
> generalized linear mixed effects modelling framework?
>
> One idea I had was to combine the two task data frames to a single
> data frame, code task as a variable, and fit a full glmm model:
>
>     fit = glmer(
>         family = binomial
>         , data = both_tasks
>         , formula = response ~ task*condition + (1+task*condition|participant)
>     )
>
> then obtain the model's predictions for each participant's data:
>
>     r = as.matrix(ranef(fit)$participant)
>     temp = expand.grid(task=c('1','2'),condition=c('A','B'),response=0)
>     from_terms = terms(response~task*condition)
>     mm = model.matrix(from_terms,temp)
>     preds = expand.grid(ask=c('1','2'),condition=c('A','B'),participant=1:nrow(r),response=0)
>     for(i in 1:nrow(r)){
>         preds$response[preds$participant==i] = as.numeric(mm %*% r[i,])
>     }
>
> then use these predictions to compute each participant's condition
> effect in each task and correlate the resulting scores:
>
>     scores = ddply(
>         .data = preds
>         , .variables = .(participant,task)
>         , .fun = function(x){
>             data.frame( value = diff(x$response) )
>         }
>     )
>     cor( scores$value[scores$task=='1'] , scores$value[scores$task=='2'] )
>
> But it appears that this approach also suffers from a type-I error
> inflation (again, according to a simulation I ran last night).
>
> Any suggestions?
>
> Mike


From h.l.ward at qmul.ac.uk  Thu Sep  6 17:13:47 2012
From: h.l.ward at qmul.ac.uk (Helen Ward)
Date: Thu, 06 Sep 2012 16:13:47 +0100
Subject: [R-sig-ME] Priors for and estimating heritability from an ordinal
 and a Poisson animal model
Message-ID: <5048BDAB.3090103@qmul.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120906/65f93ea4/attachment.pl>

From jake987722 at hotmail.com  Thu Sep  6 17:57:18 2012
From: jake987722 at hotmail.com (Jake Westfall)
Date: Thu, 6 Sep 2012 09:57:18 -0600
Subject: [R-sig-ME] Characterizing correlation between binomial effects
In-Reply-To: <CAB+QPJBxvGLsa+7U3QFfzrOvpEZPN9Yqb-Fqhy=bisT6UbxPbw@mail.gmail.com>
References: <CAB+QPJCY56yC6QUGJrkhzs1yH=1xhgQdxK-j3OrkLEQhiG5C2A@mail.gmail.com>,
	<CAB+QPJBxvGLsa+7U3QFfzrOvpEZPN9Yqb-Fqhy=bisT6UbxPbw@mail.gmail.com>
Message-ID: <SNT107-W5528603FBAFC174D240815CBA80@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120906/630fa14d/attachment.pl>

From vcareau at ucr.edu  Thu Sep  6 19:25:29 2012
From: vcareau at ucr.edu (Vincent Careau)
Date: Thu, 6 Sep 2012 10:25:29 -0700
Subject: [R-sig-ME] Heterogeneous (co)variances in a bivariate model
	with MCMCglmm?
In-Reply-To: <50487A20.5020509@u.washington.edu>
References: <6A1067D3F7C24C07B19FE7D45979E767@PCderobo>
	<50487A20.5020509@u.washington.edu>
Message-ID: <725D8D7E384F45A590085AD599C653F0@PCderobo>

Thanks Dave,
I looked at your coding and modified it to the fictive data set (see below)
to fit a model with different variances and covariances in 2008 and 2009.
The following coding seems to work fine:

pr <- list(R = list(V = diag(2), nu = 1.002),
               G = list(G1 = list(V = diag(2), nu = 1.002),
                        G2 = list(V = diag(2), nu = 1.002)))  
                    
BI.model<-MCMCglmm(cbind(x1, x2) ~ -1 + trait,
                        data = DATA,
                        random = ~ us(at.level(YEAR,1):trait):ID +
                                   us(at.level(YEAR,2):trait):ID,
                        rcov = ~ us(trait):units,
                        family = c("gaussian","gaussian"),
                        nitt = 25000, burnin = 5000, thin = 20,
                        prior = pr)  

As we can see, the covariance between x1 and x2 is different in 2008 and
2009. However, this code specifies common residual variances and covariances
in both years. I tried to modify the "rcov=" to allow heterogenous
residuals, but could not figure out how to make it work in a bivariate
context. I tried the same structure than I used in the random statement:

rcov =   ~ us(at.level(YEAR,1):trait):units+
           us(at.level(YEAR,2):trait):units,
 
but I get the following error:
"Error in MCMCglmm(cbind(x1, x2) ~ -1 + trait, data = DATA, random =
~us(at.level(YEAR,  : 
  R-structure does not define unique residual for each data point"

The following seems to work fine:
rcov =   ~ idh(at.level(YEAR,1):trait):units+
           idh(at.level(YEAR,2):trait):units,
but when I call the model with the summary function I get this message:
"Error in rep(rep(1:length(object$Random$nrt), object$Random$nrt),
object$Random$nfl^2) : 
  invalid 'times' argument"

I would be surprised this is an overfitting problem because each individual
is measured twoce per year (changing it to 4 tiomes per year yields that
same error). Could it be related to prior specification?

Cheers,
Vincent

-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org
[mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of David Atkins
Sent: Thursday, September 06, 2012 3:26 AM
To: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Heterogeneous (co)variances in a bivariate model
with MCMCglmm?


Vincent--

I've been doing a bit of work on multivariate mixed models using 
MCMCglmm with some colleagues (actually, for a tutorial paper on that 
topic...).

The following will fit random intercepts and slopes for two outcomes but 
*not* allowing correlations between outcomes (note that "j" is the 
grouping variable in our data).  Data is for simple two treatment over 
time study (common in psychiatry / psychology) with two outcomes.

# random intercept and slopes, but not correlated across outcomes
prior2 <- list(R = list(V = diag(2), nu = 1.002),
                G = list(G1 = list(V = diag(2), nu = 1.002),
                         G2 = list(V = diag(2), nu = 1.002)))

mult.mcmc3 <- MCMCglmm(cbind(y, y2) ~ -1 + trait*(tx*time),
                        data = data.wide,
                        random = ~ us(at.level(trait, 1) +
                                   at.level(trait, 1):time):j +
                                   us(at.level(trait, 2) +
                                   at.level(trait, 2):time):j,
                        rcov = ~ us(trait):units,
                        family = c("gaussian","gaussian"),
                        verbose = TRUE,
                        nitt = 25000, burnin = 5000, thin = 20,
                        prior = prior2)


Fitting correlated effects is actually a bit less "wordy" in terms of 
syntax:

# unstructured matrix between outcomes
prior3 <- list(R = list(V = .2*diag(2), nu = 3),
                G = list(G1 = list(V = diag(c(.2,.2,.1,.1)), nu = 5)))

mult.mcmc3.1 <- MCMCglmm(cbind(y, y2) ~ -1 + trait*(tx*time),
                        data = data.wide,
                        random = ~ us(-1 + trait*time):j,
                        rcov = ~ us(trait):units,
                        family = c("gaussian","gaussian"),
                        verbose = TRUE,
                        nitt = 25000, burnin = 5000, thin = 20,
                        prior = prior3)

Note that we're using simulated data and have been futzing a bit with 
priors, so I would definitely *not* suggest "prior3" for off-the-shelf 
usage.

Hope that helps.

cheers, Dave


-- 
Dave Atkins, PhD
University of Washington
datkins at u.washington.edu
http://depts.washington.edu/cshrb/

August 1 - October 30:

Universitat Zurich
Psychologisches Institut
Klinische Psychologie
Binzmuhlestrasse 14/23
CH-8050 Zurich

+41 44 635 71 75

Hi,

Is it possible to fit a bivariate model with heterogeneous variance
components in MCMCglmm?



Let's say we measured two traits (x1 and x2) on 50 individuals, twice per
year over two years (2008 and 2009). CREATE THE DATA:

ID<-c(rbind(seq(1,50),seq(1,50),seq(1,50),seq(1,50)))

YEAR<-rep(2008:2009,100)

x1<-ID/10+rnorm(200, mean = 0, sd = 2)

tmp<-data.frame(ID, YEAR,x1)

tmp$ID<-as.factor(tmp$ID)

tmp$YEAR<-as.factor(tmp$YEAR)

plot(x1~ID,data=tmp)

tmp2008<-subset(tmp,YEAR=="2008")

tmp2009<-subset(tmp,YEAR=="2009")

tmp2008$x2<-rnorm(100, mean = 0, sd = 3)+tmp2008$x1

tmp2009$x2<-rnorm(100, mean = 0, sd = 3)-tmp2009$x1+2*(mean(tmp2009$x1))

DATA<-rbind(tmp2008,tmp2009)

plot(x1~x2,DATA, col=YEAR)



In section 3.4 of the MCMCglmm course notes ("Heterogenous Residual
Variance"), we can see how to estimate a separate variance for ID and
residuals according to YEAR in a univariate model:

pr = list(R = list(V = diag(2), nu = 1), G = list(G1 = list(V = diag(2),nu =
1)))

model<-MCMCglmm(x1~1,

random=~idh(YEAR):ID,

rcov=~idh(YEAR):units,

data=DATA, nitt = 130000, thin = 100, burnin = 30000,prior=pr)



We can also fit a bivariate model:

BI.model<-MCMCglmm(cbind(x1, x2)~1,random=~us(trait):ID,

                                       rcov=~us(trait):units,

                                       family=c("gaussian","gaussian"),

                                       prior=pr,data=DATA)



However, I cannot find how to combine the two models above, where we
estimate separate variance (and covariances) in a bivariate model. Is this
possible at all? For sake of comparison, the code for such a model in
ASRreml-R is:



BI.model<-asreml(cbind(x1,x2)~trait,
random=~at(YEAR):us(trait):ID,rcov=~at(YEAR):units:us(trait),data=DATA,maxit
er=500)



cheers,

Vincent




	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From nrf5017 at psu.edu  Thu Sep  6 21:53:28 2012
From: nrf5017 at psu.edu (Nate Fronk)
Date: Thu, 6 Sep 2012 15:53:28 -0400
Subject: [R-sig-ME] mcmcsamp with Poisson distribution
Message-ID: <1346961204l.1323208l.0l@psu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120906/862262f8/attachment.pl>

From datkins at u.washington.edu  Fri Sep  7 08:50:02 2012
From: datkins at u.washington.edu (David Atkins)
Date: Fri, 07 Sep 2012 08:50:02 +0200
Subject: [R-sig-ME] Heterogeneous (co)variances in a bivariate model
 with MCMCglmm?
In-Reply-To: <725D8D7E384F45A590085AD599C653F0@PCderobo>
References: <6A1067D3F7C24C07B19FE7D45979E767@PCderobo>
	<50487A20.5020509@u.washington.edu>
	<725D8D7E384F45A590085AD599C653F0@PCderobo>
Message-ID: <5049991A.5080909@u.washington.edu>


Hi Vincent--

Hmm, we may need to let Jarrod sound off on this; you are fitting (what 
I would call) a stratified model for the random-effects, and I have 
never tried to do something similar for the residual variances.

Have you tried either of:

rcov =   ~ us(Year*trait):units  # possibly '-1 + Year*trait'

or

rcov =   ~ idh(Year*trait):units  # possibly '-1 + Year*trait'

and, I would think you would want to alter your R prior to have:

V = diag(4), nu = 3.022

perhaps as a starting place.

I *think* those should fit separate residual error terms for both 
outcomes by each year.  The former might be under-identified given your 
data (as it allows covariances among all residuals).

cheers, Dave

Dave Atkins, PhD
University of Washington
datkins at u.washington.edu
http://depts.washington.edu/cshrb/

August 1 - October 30:

Universitat Zurich
Psychologisches Institut
Klinische Psychologie
Binzmuhlestrasse 14/23
CH-8050 Zurich

+41 44 635 71 75

On 9/6/12 7:25 PM, Vincent Careau wrote:
> Thanks Dave,
> I looked at your coding and modified it to the fictive data set (see below)
> to fit a model with different variances and covariances in 2008 and 2009.
> The following coding seems to work fine:
>
> pr <- list(R = list(V = diag(2), nu = 1.002),
>                 G = list(G1 = list(V = diag(2), nu = 1.002),
>                          G2 = list(V = diag(2), nu = 1.002)))
>
> BI.model<-MCMCglmm(cbind(x1, x2) ~ -1 + trait,
>                          data = DATA,
>                          random = ~ us(at.level(YEAR,1):trait):ID +
>                                     us(at.level(YEAR,2):trait):ID,
>                          rcov = ~ us(trait):units,
>                          family = c("gaussian","gaussian"),
>                          nitt = 25000, burnin = 5000, thin = 20,
>                          prior = pr)
>
> As we can see, the covariance between x1 and x2 is different in 2008 and
> 2009. However, this code specifies common residual variances and covariances
> in both years. I tried to modify the "rcov=" to allow heterogenous
> residuals, but could not figure out how to make it work in a bivariate
> context. I tried the same structure than I used in the random statement:
>
> rcov =   ~ us(at.level(YEAR,1):trait):units+
>             us(at.level(YEAR,2):trait):units,
>
> but I get the following error:
> "Error in MCMCglmm(cbind(x1, x2) ~ -1 + trait, data = DATA, random =
> ~us(at.level(YEAR,  :
>    R-structure does not define unique residual for each data point"
>
> The following seems to work fine:
> rcov =   ~ idh(at.level(YEAR,1):trait):units+
>             idh(at.level(YEAR,2):trait):units,
> but when I call the model with the summary function I get this message:
> "Error in rep(rep(1:length(object$Random$nrt), object$Random$nrt),
> object$Random$nfl^2) :
>    invalid 'times' argument"
>
> I would be surprised this is an overfitting problem because each individual
> is measured twoce per year (changing it to 4 tiomes per year yields that
> same error). Could it be related to prior specification?
>
> Cheers,
> Vincent
>
> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org
> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of David Atkins
> Sent: Thursday, September 06, 2012 3:26 AM
> To: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Heterogeneous (co)variances in a bivariate model
> with MCMCglmm?
>
>
> Vincent--
>
> I've been doing a bit of work on multivariate mixed models using
> MCMCglmm with some colleagues (actually, for a tutorial paper on that
> topic...).
>
> The following will fit random intercepts and slopes for two outcomes but
> *not* allowing correlations between outcomes (note that "j" is the
> grouping variable in our data).  Data is for simple two treatment over
> time study (common in psychiatry / psychology) with two outcomes.
>
> # random intercept and slopes, but not correlated across outcomes
> prior2 <- list(R = list(V = diag(2), nu = 1.002),
>                  G = list(G1 = list(V = diag(2), nu = 1.002),
>                           G2 = list(V = diag(2), nu = 1.002)))
>
> mult.mcmc3 <- MCMCglmm(cbind(y, y2) ~ -1 + trait*(tx*time),
>                          data = data.wide,
>                          random = ~ us(at.level(trait, 1) +
>                                     at.level(trait, 1):time):j +
>                                     us(at.level(trait, 2) +
>                                     at.level(trait, 2):time):j,
>                          rcov = ~ us(trait):units,
>                          family = c("gaussian","gaussian"),
>                          verbose = TRUE,
>                          nitt = 25000, burnin = 5000, thin = 20,
>                          prior = prior2)
>
>
> Fitting correlated effects is actually a bit less "wordy" in terms of
> syntax:
>
> # unstructured matrix between outcomes
> prior3 <- list(R = list(V = .2*diag(2), nu = 3),
>                  G = list(G1 = list(V = diag(c(.2,.2,.1,.1)), nu = 5)))
>
> mult.mcmc3.1 <- MCMCglmm(cbind(y, y2) ~ -1 + trait*(tx*time),
>                          data = data.wide,
>                          random = ~ us(-1 + trait*time):j,
>                          rcov = ~ us(trait):units,
>                          family = c("gaussian","gaussian"),
>                          verbose = TRUE,
>                          nitt = 25000, burnin = 5000, thin = 20,
>                          prior = prior3)
>
> Note that we're using simulated data and have been futzing a bit with
> priors, so I would definitely *not* suggest "prior3" for off-the-shelf
> usage.
>
> Hope that helps.
>
> cheers, Dave
>
>


From luisd at ciencias.unam.mx  Fri Sep  7 09:02:54 2012
From: luisd at ciencias.unam.mx (Luis Verde)
Date: Fri, 7 Sep 2012 17:02:54 +1000
Subject: [R-sig-ME] sparse Matrix from phylo object in MCMCglmm
Message-ID: <CAO+Szw8Y+3-g_9kxxeCCoK3EiFhVB+3=khh_QCZ6vw5OCBFoEg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120907/879f4068/attachment.pl>

From guillaumechaumet at gmail.com  Sat Sep  8 11:46:52 2012
From: guillaumechaumet at gmail.com (guillaume chaumet)
Date: Sat, 8 Sep 2012 11:46:52 +0200
Subject: [R-sig-ME] MixMod lsmeans question
Message-ID: <CAGg8Sk+=GpABd5+9xYFY6rKhcEB8ZjO+MwzpwSQs6Ajz2vcFpQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120908/056d6ed3/attachment.pl>

From andrewdigby at mac.com  Sat Sep  8 03:29:08 2012
From: andrewdigby at mac.com (Andrew Digby)
Date: Sat, 08 Sep 2012 13:29:08 +1200
Subject: [R-sig-ME] Pearson residuals in R2jags
Message-ID: <88641DEC-6460-4DC0-9E5B-13679353D5EF@mac.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120908/df46fe59/attachment.pl>

From bbolker at gmail.com  Sun Sep  9 22:51:02 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 9 Sep 2012 20:51:02 +0000 (UTC)
Subject: [R-sig-ME] MixMod lsmeans question
References: <CAGg8Sk+=GpABd5+9xYFY6rKhcEB8ZjO+MwzpwSQs6Ajz2vcFpQ@mail.gmail.com>
Message-ID: <loom.20120909T224853-684@post.gmane.org>

guillaume chaumet <guillaumechaumet at ...> writes:

> 
> Dear mixed models list,
> I have a question concerning TotalAnalysis function from MixMod package.
> Anova results tell me that there is no interaction effect between two
> factors but lsmeans table show me estimate values and p value as though
> there was an interaction effect.
> Is it a normal behavior?

  I'm not sure, but it sounds as though you may be comparing hypothesis
tests done on the basis of Wald tests (based on estimates of curvature
of the goodness-of-fit surface) [i.e. "lsmeans" table] with those based
on explicit model comparisons (*possibly* Anova(), depending on details ...)
These can be different.

  More details would be helpful.

  Ben Bolker


From bbolker at gmail.com  Sun Sep  9 23:03:50 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 9 Sep 2012 21:03:50 +0000 (UTC)
Subject: [R-sig-ME] mcmcsamp with Poisson distribution
References: <1346961204l.1323208l.0l@psu.edu>
Message-ID: <loom.20120909T225124-802@post.gmane.org>

Nate Fronk <nrf5017 at ...> writes:

> I am using a Before-After Control-Impact study
> design to look at the effect of natural gas activity on bird species.The
> interaction in my equation tests for the effect of well pads being placed in
> the forest. 

 [snip]

> I'm aware I
> could take the Bayesian route but I just wanted to see if there may be some
> code out there somewhere to get around this. Below is an example of what my
> data looks like and my code. Any input would be greatly appreciated. 
> 
> Block    Pads  Time  Abundance    
> 34B25    2      1    8    

[snip]

> 35C74    4      1   15    
> 
> Revi <- read.table("revibaci2.txt", header = TRUE)
> library(lme4)

The following should both be unnecessary, although they don't hurt ...

> Revi$Block<-as.factor(Revi$Block)  
> Revi$Pads<-as.numeric(Revi$Pads)
 
> mm1<-lmer(Abundance~Pads*Time+(1|Block),family=poisson,data=Revi)

  mcmcsamp has never (as far as I know) worked for GLMMs (i.e.
family not equal to the default "gaussian")

> mms1<- mcmcsamp(mm1,1000)
> HPDinterval(mcmcsamp(mm1,n=1000))

  An alternative to try:

library(glmmADMB)

mm2 <- glmmadmb(Abundance~Pads*Time+(1|Block),
       family="poisson",data=Revi,mcmc=TRUE)


From David.Duffy at qimr.edu.au  Mon Sep 10 05:53:59 2012
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Mon, 10 Sep 2012 13:53:59 +1000
Subject: [R-sig-ME] Characterizing correlation between binomial effects
In-Reply-To: <CAB+QPJCY56yC6QUGJrkhzs1yH=1xhgQdxK-j3OrkLEQhiG5C2A@mail.gmail.com>
References: <CAB+QPJCY56yC6QUGJrkhzs1yH=1xhgQdxK-j3OrkLEQhiG5C2A@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.1209101344160.25050@orpheus.qimr.edu.au>

On Wed, 5 Sep 2012, Mike Lawrence wrote:

> I have data from multiple human participants who each completed two
> tasks. In each task there were two conditions, and in each condition
> there are multiple trials that yield a binomial (1/0) outcome. I would
> like to evaluate whether there is a correlation between Ss' condition
> effect in task 1 and their condition effect in task 2.
>
> One idea I had was to combine the two task data frames to a single
> data frame, code task as a variable, and fit a full glmm model:
>
>    fit = glmer(
>        family = binomial
>        , data = both_tasks
>        , formula = response ~ task*condition + (1+task*condition|participant)
>    )

If I understand this correctly, which I probably don't, you want something 
like a P-value for the likelihood ratio test comparing 
(1+task*condition|participant) with (1 + condition + task|participant). Or
should that be (1|condition) + (1 + task|participant)?


From alku at imm.dtu.dk  Mon Sep 10 12:53:43 2012
From: alku at imm.dtu.dk (Alexandra Kuznetsova)
Date: Mon, 10 Sep 2012 12:53:43 +0200
Subject: [R-sig-ME] MixMod lsmeans question
Message-ID: <AAD1945C8FDCF0488D845356BF31DB7205B0892622@WINEXCHANGE5.win.dtu.dk>

If for lsmeans table you use lsmeans function, then all the least squares means will be calculated for all effects (for non significant ones as well), if for lsmeans table you use totalAnalysis function, then by default only for significant effects lsmeans will be calculated. Does this answers your question?

From guillaumechaumet at gmail.com  Tue Sep 11 08:43:55 2012
From: guillaumechaumet at gmail.com (guillaume chaumet)
Date: Tue, 11 Sep 2012 08:43:55 +0200
Subject: [R-sig-ME] MixMod lsmeans question
In-Reply-To: <AAD1945C8FDCF0488D845356BF31DB7205B0892622@WINEXCHANGE5.win.dtu.dk>
References: <AAD1945C8FDCF0488D845356BF31DB7205B0892622@WINEXCHANGE5.win.dtu.dk>
Message-ID: <CAGg8SkJ5b-Hj6XwYL9wG63jOs1SVMyW0Dav0UOfRwOFCOcaGpA@mail.gmail.com>

Here's the data and the code :

aggTc=read.csv("aggTc.csv")

library(lme4)
library(MixMod)

CRT.lmer=lmer(log(CRT)~test*fatigue*wakefulness+ (1|subject),data=aggTc)

totalAnalysis(CRT.lmer,aggTc)

As you could observe, lmer part shows us no interaction effect except
fatigue:wakefulness but second "lsmeans" part shows every significant
effects...

Guillaume Chaumet




2012/9/10 Alexandra Kuznetsova <alku at imm.dtu.dk>

> If for lsmeans table you use lsmeans function, then all the least squares
> means will be calculated for all effects (for non significant ones as
> well), if for lsmeans table you use totalAnalysis function, then by default
> only for significant effects lsmeans will be calculated. Does this answers
> your question?
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

From alku at imm.dtu.dk  Tue Sep 11 15:24:06 2012
From: alku at imm.dtu.dk (Alexandra Kuznetsova)
Date: Tue, 11 Sep 2012 15:24:06 +0200
Subject: [R-sig-ME] MixMod lsmeans question
In-Reply-To: <CAGg8SkJ5b-Hj6XwYL9wG63jOs1SVMyW0Dav0UOfRwOFCOcaGpA@mail.gmail.com>
References: <AAD1945C8FDCF0488D845356BF31DB7205B0892622@WINEXCHANGE5.win.dtu.dk>,
	<CAGg8SkJ5b-Hj6XwYL9wG63jOs1SVMyW0Dav0UOfRwOFCOcaGpA@mail.gmail.com>
Message-ID: <AAD1945C8FDCF0488D845356BF31DB7205B0892625@WINEXCHANGE5.win.dtu.dk>

Thank you for the data and explanations! Now I understand the problem. This is a normal behaviour, and there is no error here - proc mixed of SAS program gives the same results. It could be that estimates of population means are significant and the effects of ANOVA table non-significant 
________________________________________
From: guillaume chaumet [guillaumechaumet at gmail.com]
Sent: Tuesday, September 11, 2012 8:43 AM
To: Alexandra Kuznetsova
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] MixMod lsmeans question

Here's the data and the code :

aggTc=read.csv("aggTc.csv")

library(lme4)
library(MixMod)

CRT.lmer=lmer(log(CRT)~test*fatigue*wakefulness+ (1|subject),data=aggTc)

totalAnalysis(CRT.lmer,aggTc)

As you could observe, lmer part shows us no interaction effect except fatigue:wakefulness but second "lsmeans" part shows every significant effects...

Guillaume Chaumet




2012/9/10 Alexandra Kuznetsova <alku at imm.dtu.dk<mailto:alku at imm.dtu.dk>>
If for lsmeans table you use lsmeans function, then all the least squares means will be calculated for all effects (for non significant ones as well), if for lsmeans table you use totalAnalysis function, then by default only for significant effects lsmeans will be calculated. Does this answers your question?
_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From wolfgang.viechtbauer at maastrichtuniversity.nl  Tue Sep 11 16:15:11 2012
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (STAT))
Date: Tue, 11 Sep 2012 16:15:11 +0200
Subject: [R-sig-ME] Characterizing correlation between binomial effects
In-Reply-To: <SNT107-W5528603FBAFC174D240815CBA80@phx.gbl>
References: <CAB+QPJCY56yC6QUGJrkhzs1yH=1xhgQdxK-j3OrkLEQhiG5C2A@mail.gmail.com>,
	<CAB+QPJBxvGLsa+7U3QFfzrOvpEZPN9Yqb-Fqhy=bisT6UbxPbw@mail.gmail.com>
	<SNT107-W5528603FBAFC174D240815CBA80@phx.gbl>
Message-ID: <077E31A57DA26E46AB0D493C9966AC730CC5A13B04@UM-MAIL4112.unimaas.nl>

I meant to respond to this earlier, but now finally got around to it. The nice thing is, there is (for once) an easy answer. For testing covariance parameters, the value under the null is obviously not on the boundary of the parameter space and the concerns with testing parameters on the boundary do not apply. You can actually test this out with some data and the simulate.lme() function (see also chapter 2 in Pinheiro & Bates, 2000). For example:

library(nlme)

res1 <- lme(distance ~ age, random = ~ age | Subject, data = Orthodont)
res2 <- lme(distance ~ age, random = list(Subject = pdDiag(~ age)), data = Orthodont)
res3 <- lme(distance ~ age, random = ~ 1 | Subject, data = Orthodont)

anova(res1, res2, res3)

res.sim13 <- simulate.lme(res3, res1, nsim=1000, seed=18271) ### parameter tested on the boundary
res.sim12 <- simulate.lme(res2, res1, nsim=1000, seed=18271) ### parameter tested not on boundary

plot(res.sim13, df = c(1,2), panel = function(...) {panel.xyplot(...); panel.abline(a=0,b=1, lty="dotted")})
plot(res.sim12, df = c(1,2), panel = function(...) {panel.xyplot(...); panel.abline(a=0,b=1, lty="dotted")})

Note that the 50:50 mixture of chi^2_1 and chi^2_2 seems adequate in the first case. However, in the second case, chi^2_1 is closer to giving you a nominal test (actually it's a bit conservative, but better than the 50:50 mixture).

Best,

Wolfgang

--   
Wolfgang Viechtbauer, Ph.D., Statistician   
Department of Psychiatry and Psychology   
School for Mental Health and Neuroscience   
Faculty of Health, Medicine, and Life Sciences   
Maastricht University, P.O. Box 616 (VIJV1)   
6200 MD Maastricht, The Netherlands   
+31 (43) 388-4170 | http://www.wvbauer.com   


> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-
> bounces at r-project.org] On Behalf Of Jake Westfall
> Sent: Thursday, September 06, 2012 17:57
> To: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Characterizing correlation between binomial
> effects
> 
> This brings up a related question for me. Some of the issues that arise in
> testing the significance of random variances are due to the null value
> being on the boundary of the parameter space (i.e., testing for variance =
> 0). However, since COvariances can be positive or negative, this seems to
> imply that these issues would not apply to the case of testing null
> hypotheses about random covariances, such as covariance = 0, as Mike is
> doing in his and Reinhold's solution. Is this correct?
> Jake


From j.hadfield at ed.ac.uk  Tue Sep 11 18:24:05 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Tue, 11 Sep 2012 17:24:05 +0100
Subject: [R-sig-ME] sparse Matrix from phylo object in MCMCglmm
In-Reply-To: <CAO+Szw8Y+3-g_9kxxeCCoK3EiFhVB+3=khh_QCZ6vw5OCBFoEg@mail.gmail.com>
References: <CAO+Szw8Y+3-g_9kxxeCCoK3EiFhVB+3=khh_QCZ6vw5OCBFoEg@mail.gmail.com>
Message-ID: <20120911172405.22233xh3lf6pv0kk@www.staffmail.ed.ac.uk>

Hi,

You will be best pruning the phylogeny for each data set.  You should  
get the same answer if you don't, but it may take longer (both time  
per iteration, and more iterations because of slower mixing). Just  
creating a covariance matrix for the tips is generally a bad idea  
unless the phylogeny is small. Retaining internal nodes allows a  
sparser inverse, and retains structure that algorithms for efficiently  
solving sparse linear systems can recognise and exploit.

The default is to scale the distance from root to tip to one unit. If  
the tree is non-ultrametric this is not possible, and you will get an  
error saying the tree cannot be scaled. You can leave it unscaled, or  
stretch the tree to be ultrametric and then scale.

Cheers,

Jarrod



Quoting Luis Verde <luisd at ciencias.unam.mx> on Fri, 7 Sep 2012 17:02:54 +1000:

> Hi,
>
> I've been using MCMCglmm to do ordinal regressions with phylogenetic
> structure. At the moment I've been using the ginverse argument, and a
> sparse inverse matrix created with
>
>> inverseA(phylo.object)$Ainv
>
> after running MCMCglmm, I get the warning that "some combinations in animal
> do not exist and missing records have been generated"
>
> I assume this is because internal nodes are part of the inverse matrix and
> I only have data for the tips. If I then only use a subset of my data, I
> get the same warning; but my parameter estimates seem reasonable. Does the
> program skip these rows internally? I tried to create a matrix for tips
> only and I would always end up with a matrix too large in size that would
> crash the computers. This wouldn't happen when using all nodes.
>
> My question is, should I be pruning my tree to match every subset of data
> and then generating an inverse matrix for tips only? Is the default por
> inverseA to scale branch lengths?
>
> best regards,
>
> Luis
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From j.hadfield at ed.ac.uk  Tue Sep 11 18:29:02 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Tue, 11 Sep 2012 17:29:02 +0100
Subject: [R-sig-ME] Heterogeneous (co)variances in a bivariate model
 with MCMCglmm?
In-Reply-To: <5049991A.5080909@u.washington.edu>
References: <6A1067D3F7C24C07B19FE7D45979E767@PCderobo>
	<50487A20.5020509@u.washington.edu>
	<725D8D7E384F45A590085AD599C653F0@PCderobo>
	<5049991A.5080909@u.washington.edu>
Message-ID: <20120911172902.53046nh4tqlp9ssg@www.staffmail.ed.ac.uk>

Hi,

The rcov function does not allow the specification of direct products,  
unlike the random structure which does fit what you want.

Being able to specify:

rcov=~ us(at.level(YEAR,1):trait):units + us(at.level(YEAR,2):trait):units

would be useful, but I think I have been asked this before (where YEAR  
was GENDER) and when I looked into the code I realised that it would  
actually take quite a bit of work to make possible. Sorry.

Cheers,

Jarrod





Quoting David Atkins <datkins at u.washington.edu> on Fri, 07 Sep 2012  
08:50:02 +0200:

>
> Hi Vincent--
>
> Hmm, we may need to let Jarrod sound off on this; you are fitting  
> (what I would call) a stratified model for the random-effects, and I  
> have never tried to do something similar for the residual variances.
>
> Have you tried either of:
>
> rcov =   ~ us(Year*trait):units  # possibly '-1 + Year*trait'
>
> or
>
> rcov =   ~ idh(Year*trait):units  # possibly '-1 + Year*trait'
>
> and, I would think you would want to alter your R prior to have:
>
> V = diag(4), nu = 3.022
>
> perhaps as a starting place.
>
> I *think* those should fit separate residual error terms for both  
> outcomes by each year.  The former might be under-identified given  
> your data (as it allows covariances among all residuals).
>
> cheers, Dave
>
> Dave Atkins, PhD
> University of Washington
> datkins at u.washington.edu
> http://depts.washington.edu/cshrb/
>
> August 1 - October 30:
>
> Universitat Zurich
> Psychologisches Institut
> Klinische Psychologie
> Binzmuhlestrasse 14/23
> CH-8050 Zurich
>
> +41 44 635 71 75
>
> On 9/6/12 7:25 PM, Vincent Careau wrote:
>> Thanks Dave,
>> I looked at your coding and modified it to the fictive data set (see below)
>> to fit a model with different variances and covariances in 2008 and 2009.
>> The following coding seems to work fine:
>>
>> pr <- list(R = list(V = diag(2), nu = 1.002),
>>                G = list(G1 = list(V = diag(2), nu = 1.002),
>>                         G2 = list(V = diag(2), nu = 1.002)))
>>
>> BI.model<-MCMCglmm(cbind(x1, x2) ~ -1 + trait,
>>                         data = DATA,
>>                         random = ~ us(at.level(YEAR,1):trait):ID +
>>                                    us(at.level(YEAR,2):trait):ID,
>>                         rcov = ~ us(trait):units,
>>                         family = c("gaussian","gaussian"),
>>                         nitt = 25000, burnin = 5000, thin = 20,
>>                         prior = pr)
>>
>> As we can see, the covariance between x1 and x2 is different in 2008 and
>> 2009. However, this code specifies common residual variances and covariances
>> in both years. I tried to modify the "rcov=" to allow heterogenous
>> residuals, but could not figure out how to make it work in a bivariate
>> context. I tried the same structure than I used in the random statement:
>>
>> rcov =   ~ us(at.level(YEAR,1):trait):units+
>>            us(at.level(YEAR,2):trait):units,
>>
>> but I get the following error:
>> "Error in MCMCglmm(cbind(x1, x2) ~ -1 + trait, data = DATA, random =
>> ~us(at.level(YEAR,  :
>>   R-structure does not define unique residual for each data point"
>>
>> The following seems to work fine:
>> rcov =   ~ idh(at.level(YEAR,1):trait):units+
>>            idh(at.level(YEAR,2):trait):units,
>> but when I call the model with the summary function I get this message:
>> "Error in rep(rep(1:length(object$Random$nrt), object$Random$nrt),
>> object$Random$nfl^2) :
>>   invalid 'times' argument"
>>
>> I would be surprised this is an overfitting problem because each individual
>> is measured twoce per year (changing it to 4 tiomes per year yields that
>> same error). Could it be related to prior specification?
>>
>> Cheers,
>> Vincent
>>
>> -----Original Message-----
>> From: r-sig-mixed-models-bounces at r-project.org
>> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of David Atkins
>> Sent: Thursday, September 06, 2012 3:26 AM
>> To: r-sig-mixed-models at r-project.org
>> Subject: Re: [R-sig-ME] Heterogeneous (co)variances in a bivariate model
>> with MCMCglmm?
>>
>>
>> Vincent--
>>
>> I've been doing a bit of work on multivariate mixed models using
>> MCMCglmm with some colleagues (actually, for a tutorial paper on that
>> topic...).
>>
>> The following will fit random intercepts and slopes for two outcomes but
>> *not* allowing correlations between outcomes (note that "j" is the
>> grouping variable in our data).  Data is for simple two treatment over
>> time study (common in psychiatry / psychology) with two outcomes.
>>
>> # random intercept and slopes, but not correlated across outcomes
>> prior2 <- list(R = list(V = diag(2), nu = 1.002),
>>                 G = list(G1 = list(V = diag(2), nu = 1.002),
>>                          G2 = list(V = diag(2), nu = 1.002)))
>>
>> mult.mcmc3 <- MCMCglmm(cbind(y, y2) ~ -1 + trait*(tx*time),
>>                         data = data.wide,
>>                         random = ~ us(at.level(trait, 1) +
>>                                    at.level(trait, 1):time):j +
>>                                    us(at.level(trait, 2) +
>>                                    at.level(trait, 2):time):j,
>>                         rcov = ~ us(trait):units,
>>                         family = c("gaussian","gaussian"),
>>                         verbose = TRUE,
>>                         nitt = 25000, burnin = 5000, thin = 20,
>>                         prior = prior2)
>>
>>
>> Fitting correlated effects is actually a bit less "wordy" in terms of
>> syntax:
>>
>> # unstructured matrix between outcomes
>> prior3 <- list(R = list(V = .2*diag(2), nu = 3),
>>                 G = list(G1 = list(V = diag(c(.2,.2,.1,.1)), nu = 5)))
>>
>> mult.mcmc3.1 <- MCMCglmm(cbind(y, y2) ~ -1 + trait*(tx*time),
>>                         data = data.wide,
>>                         random = ~ us(-1 + trait*time):j,
>>                         rcov = ~ us(trait):units,
>>                         family = c("gaussian","gaussian"),
>>                         verbose = TRUE,
>>                         nitt = 25000, burnin = 5000, thin = 20,
>>                         prior = prior3)
>>
>> Note that we're using simulated data and have been futzing a bit with
>> priors, so I would definitely *not* suggest "prior3" for off-the-shelf
>> usage.
>>
>> Hope that helps.
>>
>> cheers, Dave
>>
>>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From andrew.close at newcastle.ac.uk  Tue Sep 11 14:01:27 2012
From: andrew.close at newcastle.ac.uk (Andrew Close)
Date: Tue, 11 Sep 2012 12:01:27 +0000
Subject: [R-sig-ME] nlme model.matrix problem for prediction intervals
Message-ID: <D891D3A75E959E428CFDFBD258C1621E0A8D2D78@EXMBDB01.campus.ncl.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120911/c6b4554d/attachment.pl>

From andrewdigby at mac.com  Wed Sep 12 01:30:04 2012
From: andrewdigby at mac.com (Andrew Digby)
Date: Wed, 12 Sep 2012 11:30:04 +1200
Subject: [R-sig-ME] Is there an R function for GLMM with binary, response,
 nested random factors, and temporal correlation?
Message-ID: <6718A468-38E6-4D13-8713-AA4F49498910@mac.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120912/06e45479/attachment.pl>

From klemens.weigl at gmail.com  Wed Sep 12 15:58:44 2012
From: klemens.weigl at gmail.com (Klemens Weigl)
Date: Wed, 12 Sep 2012 15:58:44 +0200
Subject: [R-sig-ME] Cumulative link mixed model appropriate in a 2x4 design?
Message-ID: <CA+LmEJt6QFSZVWJ2+b5vFvDJZMdjvxTbiKPdp5OesEY9zgd1pA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120912/c6054d00/attachment.pl>

From bbolker at gmail.com  Wed Sep 12 18:58:16 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 12 Sep 2012 16:58:16 +0000 (UTC)
Subject: [R-sig-ME] nlme model.matrix problem for prediction intervals
References: <D891D3A75E959E428CFDFBD258C1621E0A8D2D78@EXMBDB01.campus.ncl.ac.uk>
Message-ID: <loom.20120912T185421-274@post.gmane.org>

Andrew Close <andrew.close at ...> writes:


> I am trying to generate prediction intervals for an nlme model by
>  following the example provided in http://glmm.wikidot.com/faq

> I have encountered a problem when  calculating "predvar" and 
> an error message is produced.
> 
> The problem, as far as I can tell, 
> is actually in creating the model matrix: "Error in Designmat %*%
> fm1$varFix : non-conformable arguments"

  The basic problem is that the code on that page is intended for
*linear* models.  In order to do the equivalent construction for
a nonlinear model, I believe you'd have to compute the linearization
(i.e. the gradient of the response variable with respect to each of 
the parameters) at each specified prediction point.  This is certainly
do-able, but doing it automatically by extracting information from
the model could be a little tricky.

  Perhaps someone will step forward with a solution ...


  Ben Bolker

> Here is the code I have used
> 
> library(nlme)
> 
> ##
> fm1 <- nlme(height ~ SSasymp(age, Asym, R0, lrc),
> data = Loblolly,
> fixed = Asym + R0 + lrc ~ 1,
> random = Asym ~ 1,
> start = c(Asym = 103, R0 = -8.5, lrc = -3.3))
> 
> newLoblolly$pred<-predict(fm1, newLoblolly)
> newLoblolly <- expand.grid(age = c(5,10,15,20,25,30), Seed = c(301,327))
> newLoblolly$pred<-predict(fm1, newLoblolly, level=0)
> 
> Designmat <- model.matrix(eval(eval(fm1$call$model)[-2]), newLoblolly[-3])
> Designmat


From llopezt2004 at yahoo.co.uk  Tue Sep 11 21:29:03 2012
From: llopezt2004 at yahoo.co.uk (Leonel Lopez)
Date: Tue, 11 Sep 2012 20:29:03 +0100 (BST)
Subject: [R-sig-ME] Question on significancy of terms
Message-ID: <1347391743.60520.YahooMailNeo@web29302.mail.ird.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120911/3da381d8/attachment.pl>

From charris at ihi.or.tz  Wed Sep 12 15:16:01 2012
From: charris at ihi.or.tz (Caroline)
Date: Wed, 12 Sep 2012 16:16:01 +0300
Subject: [R-sig-ME] model definition issues for repeat measures
Message-ID: <001401cd90e8$d85cb860$89162920$@or.tz>

Dear R, 

I am trying to analyse my count data using lmer, family=poisson, however
this model is unable to cope with my data as the count data for one of my
factor levels drops to zero for every data point. The model runs without any
error messages, however one interaction term has a huge Std error and is not
significant (corresponding to the data points which are all zero), when it
clearly should be significant from looking at the data graphically.

Please can you advise the best way forward to analyse the data. Please find
my data table and R script attached. 

Kind regards,

Caroline

 

------------------------

Caroline Harris, PhD

Vector Group

 

Ifakara Health Institute, Mlabani Passage, PO Box 53, Ifakara, Tanzania

Tel: +255 686136969 E-mail: charris at ihi.or.tz

 

Liverpool School of Tropical Medicine, Pembroke Place, Liverpool, L3 5QA, UK

http://www.lstmliverpool.ac.uk/research/academic-groups/staff-profiles/carol
ine-harris

 

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: A_E.txt
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120912/972a0269/attachment.txt>

From andrewobermeier at me.com  Thu Sep 13 03:24:00 2012
From: andrewobermeier at me.com (Obermeier Andrew)
Date: Thu, 13 Sep 2012 10:24:00 +0900
Subject: [R-sig-ME] 3-level fixed factor in lme4
Message-ID: <B6AC2F54-285D-44DA-93B4-E9773359B80D@me.com>

Hello Mixed Modelers,

I came to this list with a question a couple months ago, and will admit it was a very humbling experience. There were a few kind responses, but my question was mostly ignored or I was told I needed to read basic statistics books. I take full blame for this, and know that I was not asking my question in the right way. Now, 2 months later, I am trying again.

I am not a statistician but want to learn. Especially, I like learning statistics with R because it helps me to see the nuts and bolts of statistics much more than the big commercial user interface packages do, and I get a much better feel for how to look at data.

I am writing a dissertation in second language acquisition, analyzing my data using lme4 for a mixed model with subjects and items set as random effects. My dependent variable is reaction time (RT). My independent variable is Target Condition (TarCond). TarCond is a three-level factor: nonword, related, unrelated.

When I run the model, lmer provides the summary below. The Fixed effects section tells me that for the three factors of my independent variable I get the following mean reaction times (RT), and all of these have big t values:
nonword (Intercept): 1239.64
TarCondrelated: 1239.64 - 370.72 = 868.92
TarCondunrelated: 1239.64 + 318.34 = 1557.98

(lmer summary)
-----------------------------------------------------------------
Linear mixed model fit by REML 
Formula: RT ~ TarCond + (1 | Subject) + (1 | Item) 
   Data: postest 
   AIC   BIC logLik deviance REMLdev
 30949 30982 -15468    30966   30937
Random effects:
 Groups   Name        Variance Std.Dev.
 Item     (Intercept)  22371   149.57  
 Subject  (Intercept)  65149   255.24  
 Residual             560665   748.78  
Number of obs: 1920, groups: Item, 96; Subject, 20

Fixed effects:
                 Estimate Std. Error t value
(Intercept)       1239.64      65.61  18.894
TarCondrelated    -370.72      56.13  -6.605
TarCondunrelated   318.34      56.13   5.672

Correlation of Fixed Effects:
            (Intr) TrCndr
TarCondrltd -0.285       
TarCndnrltd -0.285  0.333
----------------------------------------------------------------------


Since the t values in the above model were all well above 2 or below -2, I read that this might mean I have a significant effect. 

I followed the analysis in Baayen, Davidson, and Bates' (2007) article titled "Mixed-effects modeling with crossed random effects for subjects and items." Though this article informs me that p-values calculated with the degrees of freedom used by pvals.fnc will be "anti-conservative", I proceeded to analyze the fixed effects in this model with Markov chain Monte Carlo sampling.
 
-------------------------------------------------------------------------------------------------
> mcmcpostest <- pvals.fnc(postest.lmer, nsim = 10000)
> mcmcpostest$fixed
                 Estimate MCMCmean HPD95lower HPD95upper  pMCMC Pr(>|t|)
(Intercept)        1239.6   1239.5     1108.9     1365.0 0.0001        0
TarCondrelated     -370.7   -371.2     -474.6     -263.8 0.0001        0
TarCondunrelated    318.3    317.8      214.3      428.8 0.0001        0
-------------------------------------------------------------------------------------------------

My apologies for this long preamble, but my questions are as follows. 
(1) Is this analytical procedure appropriate? 
(2) Specifically, is it OK for me to test the significance of a 3-level factor in this way? (My advising professor tells me that the t value can only be used to compare 2 means.)
(3) How does lme4 calculate t values for a factor that is greater than 2 levels.

Below I have attached my dataset in CSV format. If there is any other information I need to provide, please let me know and I will very gladly do so. 

My apologies if my questions are misguided or inappropriate. I am way out of my league here.

I would be very grateful for any attention that anyone can offer to my questions.

Sincerely,

Andrew Obermeier
Associate Professor, Kyoto University of Education
PhD Candidate, Temple University Japan

-------------- next part --------------




From broog731 at newschool.edu  Thu Sep 13 05:24:23 2012
From: broog731 at newschool.edu (Geoff Brookshire)
Date: Wed, 12 Sep 2012 23:24:23 -0400
Subject: [R-sig-ME] Question on significancy of terms
In-Reply-To: <1347391743.60520.YahooMailNeo@web29302.mail.ird.yahoo.com>
References: <1347391743.60520.YahooMailNeo@web29302.mail.ird.yahoo.com>
Message-ID: <CAEmOzNKUqRQmczbEmiO4inxFfYUA2PfjTRSyUTdwZqSxWhdoVg@mail.gmail.com>

Hi there,

I asked a similar question about a year ago:
https://stat.ethz.ch/pipermail/r-sig-mixed-models/2011q3/006690.html

To test for the significance of a, you could either use pvals.mcmc (in
the languageR package) to get an MCMC p-value, or do Wald chi-square
tests of model fits without the interaction term like so:

m1 <- glmer(y ~ a + b + (1|ind), family=poisson)
m2 <- glmer(y ~ a + (1|ind), family=poisson)
anova(m1, m2)

(Make sure you use the pipe character | for random effects.)

cheers,
Geoff

On Tue, Sep 11, 2012 at 3:29 PM, Leonel Lopez <llopezt2004 at yahoo.co.uk> wrote:
> Hi lme4 users:
> I am new to mixed models in R and started using lme4 and apart of all I?m not an statistician.
> I am developing a GLMM model like the one below which contains the independent effect and interaction of two factors, plus the repeated measurement effect on individuals.
> Y~a*b+(1/Ind)I am using theglmer( function, family=poisson. This model has three terms (a+b+a:ab). If I want to test the significance of the three terms I am using the likelihood ratio test (anova) comparing the model with the term and without the term
> Let say for example:
> m1<-glmer(y~a*b+(1/ind), family=poisson)
> m2<-glmer(y~a+b+(1/ind), family=poisson)
> anova(m1,m2)
> but, how to test the significancy of only "a" or only "b"
> For example to test a:
>
> m1<-glmer(y~a*b+(1/ind), family=poisson)
> m3<-glmer(y~b+a:b+(1/ind), family=poisson)
> anova(m1,m3)
>
> This seems to be a incorrect model development as the LRT gave exacttly the same values for the two models.
>
>
> I am only considering transform my variable, get a normal distribution and conduct the analysis with the lmer() function.
>
> I?ll be very grateful for your help!!
>
> Cheers
>
> Leo
>         [[alternative HTML version deleted]]
>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From j.hadfield at ed.ac.uk  Thu Sep 13 09:20:17 2012
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Thu, 13 Sep 2012 08:20:17 +0100
Subject: [R-sig-ME] Cumulative link mixed model appropriate in a 2x4
 design?
In-Reply-To: <CA+LmEJt6QFSZVWJ2+b5vFvDJZMdjvxTbiKPdp5OesEY9zgd1pA@mail.gmail.com>
References: <CA+LmEJt6QFSZVWJ2+b5vFvDJZMdjvxTbiKPdp5OesEY9zgd1pA@mail.gmail.com>
Message-ID: <20120913082017.92273mii6ycbs64g@www.staffmail.ed.ac.uk>

Hi,

With normal response, you are right in thinking that you don't need a  
cumulative link mixed model. A linear mixed model (with group as a  
random term?) should suffice.

Cheers,

Jarrod





Quoting Klemens Weigl <klemens.weigl at gmail.com> on Wed, 12 Sep 2012  
15:58:44 +0200:

> Dear R-sig-mixed-model-group!
>
> Basically I've got a fairly simple dataset with a 2x4 design (two
> independent variables = i.v.) and a continuous response variable (only one
> dependent variable).
> 1st i.v.: two different treatments
> 2nd i.v.: 4 time points: after 2, 4, 6 and 8 weeks --> at each time
> point: mice with tumor cells are killed and the tumor growth was analyzed.
> Therefore no repeated measures. Every mouse can be just in one of the two
> treatment groups in just one of the 4 time points.
> The data are normally distributed, but with unequal and small 'n' in each
> group (ranging from 8 to 14 mice per group).
>
> Objective: to test wether or not one treatment is better than the other
> treatment over the 4 time points all together?
>
> Someone was suggesting "cumulative link mixed model with Laplace
> approximation" for this task.
>
> Well I am wondering if the clmm with Laplace approximation is appropriate
> for this task, because the response variable is "continuous" and not
> ordinal (as written in the clmm2_tutorial) Am I loosing much power if I
> apply it?
>
> I'd be interested if someone might have some arguments for or against the
> application of clmm with L.a. in that design-setting - or a better solution?
>
> Kind regards,
> Klemens
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From aghaynes at gmail.com  Thu Sep 13 09:40:01 2012
From: aghaynes at gmail.com (Alan Haynes)
Date: Thu, 13 Sep 2012 09:40:01 +0200
Subject: [R-sig-ME] 3-level fixed factor in lme4
In-Reply-To: <B6AC2F54-285D-44DA-93B4-E9773359B80D@me.com>
References: <B6AC2F54-285D-44DA-93B4-E9773359B80D@me.com>
Message-ID: <CAPdSD+7fjgtk+1vvO0PDRjc6otHodyeC4daZeyig9G7hebf28g@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120913/3e47e8c9/attachment.pl>

From luisd at ciencias.unam.mx  Thu Sep 13 11:07:12 2012
From: luisd at ciencias.unam.mx (Luis Verde)
Date: Thu, 13 Sep 2012 19:07:12 +1000
Subject: [R-sig-ME] sparse Matrix from phylo object in MCMCglmm
In-Reply-To: <20120911172405.22233xh3lf6pv0kk@www.staffmail.ed.ac.uk>
References: <CAO+Szw8Y+3-g_9kxxeCCoK3EiFhVB+3=khh_QCZ6vw5OCBFoEg@mail.gmail.com>
	<20120911172405.22233xh3lf6pv0kk@www.staffmail.ed.ac.uk>
Message-ID: <CAO+Szw8N2BQ-q5Ozz8r2A1WtszRQ0R_t0dHqS6peaL_Cv8cmdw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120913/246e1f7c/attachment.pl>

From curis at pharmacie.univ-paris5.fr  Wed Sep 12 22:19:39 2012
From: curis at pharmacie.univ-paris5.fr (Emmanuel Curis)
Date: Wed, 12 Sep 2012 22:19:39 +0200
Subject: [R-sig-ME] Question on significancy of terms
In-Reply-To: <1347391743.60520.YahooMailNeo@web29302.mail.ird.yahoo.com>
References: <1347391743.60520.YahooMailNeo@web29302.mail.ird.yahoo.com>
Message-ID: <20120912201939.GA7312@info124.pharmacie.univ-paris5.fr>

Hello Leonel,

I think your problem is not specific of GLMM. The (fixed-part) models
a*b = a+b+a:b and b+a:b will lead to similar results, but expressed
with a different set of coefficients.

Assume a and b are two-levels factors (a1,a2 and b1,b2) for sake of
simplicity : the first model will have coefficients
 - associated to a   : for a2
 - associated to   b : for b2
 - associated to a:b : for a2:b2

The second model will have coefficients
 - associated to   b : for b2
 - associated to a:b : for a1:b2 and a2:b2

(you can check this also with the simple lm function, by the way).

So they have the same number of coefficients and, I think, express the
same linear relationship but in a different basis.

So the likelyhood will be the same...

I never tested that with GLMM, but I guess it is the same problem as
with lm and lmer...

Best regards,

On Tue, Sep 11, 2012 at 08:29:03PM +0100, Leonel Lopez wrote:
? Hi lme4 users:
? I am new to mixed models in R and started using lme4 and apart of all I?m not an statistician.
? I am developing a GLMM model like the one below which contains the independent effect and interaction of two factors, plus the repeated measurement effect on individuals.
? Y~a*b+(1/Ind)I am using theglmer( function, family=poisson.?This model has three terms (a+b+a:ab). If I want to test the significance of the three terms I am using the?likelihood ratio test (anova) comparing the model?with the term and without the term
? Let say for example:
? m1<-glmer(y~a*b+(1/ind), family=poisson)
? m2<-glmer(y~a+b+(1/ind),?family=poisson)
? anova(m1,m2)
? but, how to test the significancy of only "a" or only "b"
? For example to test a:
? 
? m1<-glmer(y~a*b+(1/ind),?family=poisson)
? m3<-glmer(y~b+a:b+(1/ind),?family=poisson)
? anova(m1,m3)
? 
? This seems to be a incorrect model development as the LRT gave exacttly the same values for the two models.
? 
? 
? I am only considering transform my variable, get a normal distribution and conduct the analysis with the lmer() function.
? 
? I?ll be very grateful for your help!!
? 
? Cheers
? 
? Leo
? 	[[alternative HTML version deleted]]
? 

? _______________________________________________
? R-sig-mixed-models at r-project.org mailing list
? https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
                                Emmanuel CURIS
                                emmanuel.curis at univ-paris5.fr

Page WWW: http://emmanuel.curis.online.fr/index.html


From bbolker at gmail.com  Thu Sep 13 17:32:16 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 13 Sep 2012 15:32:16 +0000 (UTC)
Subject: [R-sig-ME] model definition issues for repeat measures
References: <001401cd90e8$d85cb860$89162920$@or.tz>
Message-ID: <loom.20120913T172813-613@post.gmane.org>

Caroline <charris at ...> writes:


> 
> I am trying to analyse my count data using lmer, family=poisson, however
> this model is unable to cope with my data as the count data for one of my
> factor levels drops to zero for every data point. The model runs without any
> error messages, however one interaction term has a huge Std error and is not
> significant (corresponding to the data points which are all zero), when it
> clearly should be significant from looking at the data graphically.
> 
> Please can you advise the best way forward to analyse the data. Please find
> my data table and R script attached. 
> 

 
  Thank you for including a reproducible example! Although in this case I
think it's probably not necessary.  You should look up/Google for
"Hauck-Donner effect" (you can find a discussion in Venables and Ripley's
book), which refers to the situation where the approximation used to
compute confidence intervals on GLM(M)s breaks down for strong effects.
You should use explicit model comparison (?update, ?anova, ?drop1) to
test the difference between models with and without the intercept term.

However, you might want to be careful with the all-zero case, as it will
lead to an infinite estimate (in theory) of the interaction coefficient --
in practice it will just lead to a very large, poorly constrained estimate.
You could try a Bayesian method, or you could just try leaving out that
category and make sure that the qualitative results of your analysis
remain unchanged ...

  Ben Bolker


From m.w.palmer at newcastle.ac.uk  Fri Sep 14 14:13:01 2012
From: m.w.palmer at newcastle.ac.uk (Mike Palmer)
Date: Fri, 14 Sep 2012 12:13:01 +0000
Subject: [R-sig-ME] Split plot (with repeated measure?)
Message-ID: <6CD8E85FE4E5D44B9C540A746B2925470A8E6534@EXMBDB01.campus.ncl.ac.uk>

Hello,

I would like advice on if and how I might viably analyse this dataset: 

The experiment has 3 blocks with a three-level main plot treatment, split to a two-level treatment. In this case most parameters are also measured repeatedly at 1-3 monthly intervals. Ideally I would like to test the full response~treatment_1*treatment_2*sample_time model, but don't know what the structure should be, or even whether this is a viable analysis. I should probably add that neither the main or sub plots are randomised.

Thanks

Mike Palmer


From Paul.Thompson at SanfordHealth.org  Fri Sep 14 16:37:36 2012
From: Paul.Thompson at SanfordHealth.org (Thompson,Paul)
Date: Fri, 14 Sep 2012 14:37:36 +0000
Subject: [R-sig-ME] Split plot (with repeated measure?)
In-Reply-To: <6CD8E85FE4E5D44B9C540A746B2925470A8E6534@EXMBDB01.campus.ncl.ac.uk>
References: <6CD8E85FE4E5D44B9C540A746B2925470A8E6534@EXMBDB01.campus.ncl.ac.uk>
Message-ID: <9B75E7CF385CB94EAD6587DD96AC2D974664AFC3@SFPCEXMBX2.sanfordhealth.org>

Your description is confusing. Can you supply a diagram showing 1) arms 2) timing of assessments 3) # cases in each arm ?

-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Mike Palmer
Sent: Friday, September 14, 2012 7:13 AM
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Split plot (with repeated measure?)

Hello,

I would like advice on if and how I might viably analyse this dataset: 

The experiment has 3 blocks with a three-level main plot treatment, split to a two-level treatment. In this case most parameters are also measured repeatedly at 1-3 monthly intervals. Ideally I would like to test the full response~treatment_1*treatment_2*sample_time model, but don't know what the structure should be, or even whether this is a viable analysis. I should probably add that neither the main or sub plots are randomised.

Thanks

Mike Palmer

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
-----------------------------------------------------------------------
Confidentiality Notice: This e-mail message, including any attachments,
is for the sole use of the intended recipient(s) and may contain
privileged and confidential information.  Any unauthorized review, use,
disclosure or distribution is prohibited.  If you are not the intended
recipient, please contact the sender by reply e-mail and destroy
all copies of the original message.



From rbarbarahc at gmail.com  Fri Sep 14 19:34:28 2012
From: rbarbarahc at gmail.com (barbara costa)
Date: Fri, 14 Sep 2012 18:34:28 +0100
Subject: [R-sig-ME] mixed models with two random variables?
Message-ID: <CAOEz-Ntfyu-b+Ye+OkJeBtG7at2q8Tw1dyUpHmL8iSrV=iyDvQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120914/43801422/attachment.pl>

From m.fairbrother at bristol.ac.uk  Sat Sep 15 12:57:59 2012
From: m.fairbrother at bristol.ac.uk (Malcolm Fairbrother)
Date: Sat, 15 Sep 2012 11:57:59 +0100
Subject: [R-sig-ME] mixed models with two random variables?
In-Reply-To: <mailman.5.1347703201.4381.r-sig-mixed-models@r-project.org>
References: <mailman.5.1347703201.4381.r-sig-mixed-models@r-project.org>
Message-ID: <EBAF1258-51E1-44F6-BFCD-7226C6E157BE@bristol.ac.uk>

Hi Barbara,

The lme4 package may be a bit easier for you to use, and is more current than nlme in various ways (though nlme does do a few things lme4 can't do). I think the problem is the right hand side of the random part of your model: I don't believe you can have a "+" there, as you have in your first two models. Your third model definitely doesn't make sense, because you're treating fReserve as both a covariate and a random classification.

It's a bit hard to understand your query generally. Variables treated as fixed effects can be either categorical or continuous--not a problem. But I don't understand the random classification in your case, which is the part for which you probably want to calculate a variance term (and why you need a mixed model). A random classification is usually coded as 1 to 20, or A to Z, or something like that, whereas your Roughness and DivBoulders look like continuous variables.

However, if you are trying to model observations cross-classified in both Roughness and DivBoulders, and treating fReserve as the only covariate/predictor, then try lme4's lmer, along the lines of:

lmer(Biomass ~ fReserve + (1 | DivBoulders) + (1 | Roughness), myData)

Hope that helps. Follow up with further clarification if not.

Cheers,
Malcolm



> Date: Fri, 14 Sep 2012 18:34:28 +0100
> From: barbara costa <rbarbarahc at gmail.com>
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] mixed models with two random variables?
> 
> Dear R users,
> Does anyone knows how to run a glmm with one fixed factor and 2 random
> numeric variables (indices)? Is there any way to force in the model a
> separate interaction of those random variables with the fixed one?
> I hope you can help me.
> 
> #eg.
> Reserve <- rep(c("In","Out"), 100)
> fReserve <- factor(Reserve)
> DivBoulders <- rep (c(1.23,2.4,1.26,1.78,1.97,1.35,1.23,2.4,1.26,1.78), 20)
> Roughness <- rep(c(3.45,2.56,1.32,5.67,3.73,3.57,2.66,1.52,7.67,2.73),20)
> Biomass <- rep(c(8,5.3,3.5,12,25.4,10.1,9.8,2.4,5.6,5.3),20)
> 
> myData <- data.frame (fReserve ,DivBoulders ,Roughness ,Biomass )
> 
> #glm
> glm1 <- glm (Biomass ~ fReserve * Roughness + fReserve *  DivBoulders  ,
> family= Gamma, data= myData)
> 
> #glmm:
> library (nlme)
> 
> lme1 <- lme (Biomass  ~  fReserve , random= ~1 + fReserve | Roughness
> +  DivBoulders  , data=  myData ) # random intercept and slope  - I suspect
> it's my case
> #Error in getGroups.data.frame(dataMix, groups) :
> # Invalid formula for groups
> # if I only use one random variable I have:
> #Error in chol.default((value + t(value))/2) :
>  #the leading minor of order 2 is not positive definite
> 
> 
> lme2 <- lme( Biomass  ~  fReserve , random = ~1 | Roughness
> +  DivBoulders  ,data=myData) #random intercept
> #Error in getGroups.data.frame(dataMix, groups) :
> # Invalid formula for groups
> # if I only use one random variable my result is fine!
> 
> 
> lme3 <- lme (Biomass   ~  fReserve  , random=  ~ Roughness +   DivBoulders
> | fReserve   , data= myData ) # from help (lme)
> summary (lme3)
> # I have a result. Is the model correct for what I want?
> # BUT my fixed effect (reserve) does not have a p-value due to zero degrees
> of freedom. However in glm1 or lm2 with one random variable it has.
> 
> Can you help me please?
> 
> Thanks a lot in advance,
> Barbara


From jwiley.psych at gmail.com  Sun Sep 16 22:01:09 2012
From: jwiley.psych at gmail.com (Joshua Wiley)
Date: Sun, 16 Sep 2012 13:01:09 -0700
Subject: [R-sig-ME] examples of combining chains from MCMCglmm
Message-ID: <CANz9Z_+VQME+uyr2dxJr6GcQ3PCG9tMqScQ=M_xhb-TF=Tf1vQ@mail.gmail.com>

Hi All,

Just wondering if anyone has examples lying around of combining chains
from different runs of MCMCglmm on the same model?  If anyone does,
I'd love to look at some.  Ideally they would be generalized (i.e.,
able to combine an arbitrary number of chains).  If not, once I am
done I will probably make a little example and post it somewhere.

Also, the time to complete does not seem to be a linear function of
the number of iterations.  Does anyone have comments on that?  I am
saving a bunch of information (pr = TRUE, pl = TRUE, saveX = TRUE,
saveZ = TRUE, saveXL = TRUE) so perhaps it has to do with that.  I ran
2e4 iterations and it took about 6.5 minutes.  6e4 iterations took
39.5 minutes, or nearly twice as long as would be expected from a
linear increase.  I cannot share the actual data, but the general
structure of the model is:

MCMCglmm(outcome ~ 22 fixed predictors, family = "ordinal", data = dat,
    random = ~ var1 + var2,
    prior = list(
      B = list(mu = rep(0, 23), V = diag(23) * (1 + 1)),
      R = list(V = 1, fix = 1),
      G = list(
        G1 = list(V = 1, nu = .002),
        G2 = list(V = 1, nu = .002)
      )),
    pr=TRUE, pl=TRUE, saveX = TRUE, saveZ = TRUE, saveXL = TRUE,
    nitt = 4e5, thin = 1000, burnin = 1e4)

The thinning is high because I had problems with autocorrelation on
some parameters, possible mixing issues related to relatively
unbalanced distribution of the outcome (approximately 80%, 10%, 10%
for a three level ordered outcome).

Thanks for any thoughts or tips,

Josh


-- 
Joshua Wiley
Ph.D. Student, Health Psychology
Programmer Analyst II, Statistical Consulting Group
University of California, Los Angeles
https://joshuawiley.com/


From rbarbarahc at gmail.com  Mon Sep 17 12:15:04 2012
From: rbarbarahc at gmail.com (barbara costa)
Date: Mon, 17 Sep 2012 11:15:04 +0100
Subject: [R-sig-ME] mixed models with two random variables?
In-Reply-To: <EBAF1258-51E1-44F6-BFCD-7226C6E157BE@bristol.ac.uk>
References: <mailman.5.1347703201.4381.r-sig-mixed-models@r-project.org>
	<EBAF1258-51E1-44F6-BFCD-7226C6E157BE@bristol.ac.uk>
Message-ID: <CAOEz-NsY4Vg_5rGVXGQB-joxRN_d7cOipVNGZcZXPwqbn7gEUA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120917/75a1c71a/attachment.pl>

From ruderman at usc.edu  Mon Sep 17 17:19:57 2012
From: ruderman at usc.edu (Dan Ruderman)
Date: Mon, 17 Sep 2012 08:19:57 -0700
Subject: [R-sig-ME] Origins of difference in significance between
 lmer/pvals.fnc and aov?
Message-ID: <50573F9D.8020501@usc.edu>

Hi everyone,

I have a data set which seems amenable to analysis either through lmer
or aov, so I am comparing their results. I'm finding small p-values for
the fixed effect of interest in either case, but they differ quite a bit
(4E-5 vs 1E-3). Since I plan to perform the test across many hypotheses,
this difference will matter once multiple testing correction is performed.
I'd like to try to understand the reason for this difference and whether I
am applying these methods appropriately.

The response data are measurements of subjects who are either treated
(n=5) or untreated (n=4), with measurements taken at 3 time points in
each subject. The fixed effects are 'day' (3 levels) and 'treated' (2 
levels).
The random effect is 'subject'.



A. From lmer and pvals.fnc ('languageR' package) I get the following:

 > model.lmer <- lmer ( value ~ day + treated + (1|subject), data )
 > pvals.fnc ( model.lmer, nsim=100000, addPlot=F, ndigits=5 )
$fixed
Estimate MCMCmean HPD95lower HPD95upper pMCMC Pr(>|t|)
(Intercept) 24.9542 24.958 10.6752 38.985 0.00126 0.00100
dayday.14 24.4787 24.511 8.4761 40.253 0.00352 0.00492
dayday.21 -2.6075 -2.634 -19.0851 12.744 0.73022 0.74335
treatedTRUE 38.4092 38.409 23.9338 52.987 0.00004 0.00000

$random
Groups Name Std.Dev. MCMCmedian MCMCmean HPD95lower HPD95upper
1 subject (Intercept) 0.00014 2.40028 3.27886 0.00000 10.02878
2 Residual 16.69129 16.47799 16.79166 12.04746 22.19050

Note pMCMC for treatedTRUE is 4E-5.



B. Using aov with an Error term by subject the result is:

 > summary(aov ( value ~ treated + day + Error(subject), data))

Error: subject
Df Sum Sq Mean Sq F value Pr(>F)
treated 1 9835 9835 29.73 0.000953 ***
Residuals 7 2316 331
---
Signif. codes: 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Error: Within
Df Sum Sq Mean Sq F value Pr(>F)
day 2 4019 2009.5 7.857 0.0042 **
Residuals 16 4092 255.8
---
Signif. codes: 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1


In this case 'treated' has a p-value of 0.000953, which is quite
a bit larger than we had in lmer. I believe these two methods
have the same set of assumptions behind them. If anyone can
provide guidance as to how to interpret this difference I would
be very appreciative. A dput() of the data frame is below.

Regards,
Dan



 > dput(data)
structure(list(subject = structure(c(1L, 1L, 1L, 2L, 2L, 2L,
3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 7L, 7L, 7L, 8L,
8L, 8L, 9L, 9L, 9L), .Label = c("Subject.1", "Subject.2", "Subject.3",
"Subject.4", "Subject.5", "Subject.6", "Subject.7", "Subject.8",
"Subject.9"), class = "factor"), day = structure(c(2L, 3L, 1L,
2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L,
3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L), .Label = c("day.09", "day.14",
"day.21"), class = "factor"), treated = structure(c(1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c("FALSE", "TRUE"), class = 
"factor"),
value = c(45.0156, 30.3417, 11.294, 40.822, 17.4264, 10.0395,
55.6568, 61.0276, 43.5105, 21.7286, 12.9841, 37.0885, 115.7431,
43.5571, 63.5267, 56.357, 61.7074, 62.3348, 106.6633, 51.7871,
58.4828, 106.9284, 54.7985, 65.9771, 88.0273, 59.5362, 64.3797
)), .Names = c("subject", "day", "treated", "value"), row.names = c(NA,
-27L), class = "data.frame")


-- 
Dan Ruderman, Ph.D.
Assistant Professor of Research Medicine
Center for Applied Molecular Medicine
Keck School of Medicine of USC


From m.fairbrother at bristol.ac.uk  Tue Sep 18 12:14:52 2012
From: m.fairbrother at bristol.ac.uk (Malcolm Fairbrother)
Date: Tue, 18 Sep 2012 11:14:52 +0100
Subject: [R-sig-ME] mixed models with two random variables?
In-Reply-To: <CAOEz-NsY4Vg_5rGVXGQB-joxRN_d7cOipVNGZcZXPwqbn7gEUA@mail.gmail.com>
References: <mailman.5.1347703201.4381.r-sig-mixed-models@r-project.org>
	<EBAF1258-51E1-44F6-BFCD-7226C6E157BE@bristol.ac.uk>
	<CAOEz-NsY4Vg_5rGVXGQB-joxRN_d7cOipVNGZcZXPwqbn7gEUA@mail.gmail.com>
Message-ID: <EB95FAA3-52E0-46F6-9CEE-E6423DDE3D15@bristol.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120918/3a548d16/attachment.pl>

From rbarbarahc at gmail.com  Tue Sep 18 12:24:24 2012
From: rbarbarahc at gmail.com (barbara costa)
Date: Tue, 18 Sep 2012 11:24:24 +0100
Subject: [R-sig-ME] mixed models with two random variables?
In-Reply-To: <EB95FAA3-52E0-46F6-9CEE-E6423DDE3D15@bristol.ac.uk>
References: <mailman.5.1347703201.4381.r-sig-mixed-models@r-project.org>
	<EBAF1258-51E1-44F6-BFCD-7226C6E157BE@bristol.ac.uk>
	<CAOEz-NsY4Vg_5rGVXGQB-joxRN_d7cOipVNGZcZXPwqbn7gEUA@mail.gmail.com>
	<EB95FAA3-52E0-46F6-9CEE-E6423DDE3D15@bristol.ac.uk>
Message-ID: <CAOEz-NuM7YCBn2wpWWO5n-vf7WzbvokkDsSPf1p26JUJDhH4pQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120918/c5fd89cb/attachment.pl>

From birdlists at gmail.com  Wed Sep 19 01:09:06 2012
From: birdlists at gmail.com (Jude Phillips)
Date: Tue, 18 Sep 2012 19:09:06 -0400
Subject: [R-sig-ME] calculating number of parameters for AICc
Message-ID: <CAJT=iKdDm9njzm+BwhxmngcxwD33BL_zA85URkJcj6rFcUVxwg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120918/80fb85b8/attachment.pl>

From Paul.Johnson at glasgow.ac.uk  Wed Sep 19 11:32:17 2012
From: Paul.Johnson at glasgow.ac.uk (Paul Johnson)
Date: Wed, 19 Sep 2012 10:32:17 +0100
Subject: [R-sig-ME] MCMC mixed models for genetic parameters
In-Reply-To: <CAJT=iKdDm9njzm+BwhxmngcxwD33BL_zA85URkJcj6rFcUVxwg@mail.gmail.com>
References: <CAJT=iKdDm9njzm+BwhxmngcxwD33BL_zA85URkJcj6rFcUVxwg@mail.gmail.com>
Message-ID: <156FA9B609FA8F409121FB91C13E93DB03DCC101F2@rcb-exserv.rcbdomain.com>

Might be of interest...

Heredity (2012) 109, 235-245; doi:10.1038/hdy.2012.35; published online 18 July 2012
Bayesian adaptive Markov chain Monte Carlo estimation of genetic parameters
Mathew et al.

"a new fast adaptive Markov chain Monte Carlo (MCMC) sampling algorithm for the estimation of genetic parameters in the linear mixed model with several random effects."

Cheers,
Paul


The University of Glasgow, charity number SC004401


From bbolker at gmail.com  Wed Sep 19 14:41:10 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 19 Sep 2012 12:41:10 +0000 (UTC)
Subject: [R-sig-ME] calculating number of parameters for AICc
References: <CAJT=iKdDm9njzm+BwhxmngcxwD33BL_zA85URkJcj6rFcUVxwg@mail.gmail.com>
Message-ID: <loom.20120919T141947-677@post.gmane.org>

Jude Phillips <birdlists at ...> writes:

> 
> Hi Listers,
> 
> I am wondering about how 'K' (number of parameters in AICc equation) is
> calculated for mixed effects models. I am using aictab in the AICcmodavg
> library, to produce an AICc table for a set of models.  In the table, K
> seems to equal number of fixed effects + number of random effects + 2.  Is
> that the correct calculation?  In Vaida and Blanchard (2005), they suggest
> that for the 'population' focus (ie when you are interested in the fixed
> effects, not the random effects, which is the case for my study), K is the
> number of fixed parameters counting mean parameters and variance
> components.  I'm not sure how you calculate the number of variance
> components, so I'm not sure if this gives the same value for K as is
> supplied in aictab?  So I'm wondering if the K given in aictab is really
> correct for a 'population focus', and if not, how should K be calculated?

   Well, you can just look in the AICc.mer() function:

K <- attr(logLik(mod), "df")

this leads you (obscurely, I admit) to getMethods("logLik",sig="mer"):

  attr(val, "df") <- dims[["p"]] + dims[["np"]] + 
         as.logical(dims[["useSc"]])

To get past here, you would have to dig quite a bit deeper, but
basically the answer is that dims[["p"]] is the same as
length(fixef(model)) -- i.e., the number of fixed-effect coefficients --
and dims[["np"]] is the same as length(getME(model,"theta")) -- the
number of variance parameters.  For each random term in the model with
q components, it has q*(q+1)/2 parameters -- for example, a term of
the form (slope|group) has 3 parameters (intercept variance, slope
variance, correlation between intercept and slope).  The last term
says whether the model uses a scale parameter or not (yes for
linear mixed models, no for typical GLMMs like binomial or Poisson).

Your statement of "number of fixed effects + number of random effects + 2"
doesn't seem correct, but perhaps if you gave an example ...

Whether to add nuisance parameters or not, such as the residual
variance parameter that is estimated based on the residual variance,
is as far as I know an open question.  In the classic AIC context
it doesn't matter as long as one is consistent.  In the AICc context,
I don't think anyone really knows the answer ... adding +1 for
the residual variance parameter (as lme4 does) would make the
model selection process slightly more conservative.


From frgger372 at gmail.com  Thu Sep 20 01:46:42 2012
From: frgger372 at gmail.com (Paul York)
Date: Wed, 19 Sep 2012 19:46:42 -0400
Subject: [R-sig-ME] model averaging
Message-ID: <CAJPX4h_PpdN8k05-REDJX6FMq2AXV_E8s7XzMvqcbfp60U=F3g@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120919/3adbe4dd/attachment.pl>

From klemens.weigl at gmail.com  Thu Sep 20 12:22:11 2012
From: klemens.weigl at gmail.com (Klemens Weigl)
Date: Thu, 20 Sep 2012 12:22:11 +0200
Subject: [R-sig-ME] Cumulative link mixed model appropriate in a 2x4
	design?
In-Reply-To: <20120913082017.92273mii6ycbs64g@www.staffmail.ed.ac.uk>
References: <CA+LmEJt6QFSZVWJ2+b5vFvDJZMdjvxTbiKPdp5OesEY9zgd1pA@mail.gmail.com>
	<20120913082017.92273mii6ycbs64g@www.staffmail.ed.ac.uk>
Message-ID: <CA+LmEJvRTPrxtxF-R6C6_YhNaE_kDApVE4Q+pG-32jiVJQwmfQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120920/c547e257/attachment.pl>

From rhbc at imm.dtu.dk  Thu Sep 20 12:44:40 2012
From: rhbc at imm.dtu.dk (Rune Haubo)
Date: Thu, 20 Sep 2012 12:44:40 +0200
Subject: [R-sig-ME] Cumulative link mixed model appropriate in a 2x4
	design?
In-Reply-To: <CA+LmEJvRTPrxtxF-R6C6_YhNaE_kDApVE4Q+pG-32jiVJQwmfQ@mail.gmail.com>
References: <CA+LmEJt6QFSZVWJ2+b5vFvDJZMdjvxTbiKPdp5OesEY9zgd1pA@mail.gmail.com>
	<20120913082017.92273mii6ycbs64g@www.staffmail.ed.ac.uk>
	<CA+LmEJvRTPrxtxF-R6C6_YhNaE_kDApVE4Q+pG-32jiVJQwmfQ@mail.gmail.com>
Message-ID: <CAG_uk93dE44=zKKbMNu28eQc_8dpK75=OY=L=ykX0gJg4b9aeQ@mail.gmail.com>

On 20 September 2012 12:22, Klemens Weigl <klemens.weigl at gmail.com> wrote:
> Dear David and Jarrod,
>
> so far I tried a lot :-).
>
> 1st solution:
> To run a linear mixed model with 'the 2 groups/treatments' as fixed effect
> and 'the 4 time points' as random effect. It showed a significant group
> effect. Then it is interesting on which time point. My colleagues just want
> 4 t-tests for independent measures at each time point. If run them it
> showed - Bonferroni-corrected - only one significant time point.
>
> 1) Is this approach okay?
>
>
> 2nd solution:
> I also wanted to apply the cumlutative link mixed model with L.a..
> --> the response was 'size' (continuous)
> --> fixed effect 'treatment' (2 groups)
> --> random effect 'weeks' (4 time points).
> The problem: I always got:
> ----------------------------------------
> FEHLER:
> response needs to be a factor
> ----------------------------------------
>
> I used the following R-code:
> ------------------------------------------
>> library(ordinal)
> ...
>> fm1 <- clmm2(size ~ treatment, random=weeks, data=data)
> and then:
>> fm1 <- clmm2(size ~ treatment, random=weeks, data=data, Hess=TRUE,
> nAGQ=10)
> ------------------------------------------
>
> In the wine data example of the clmm2-tutorial the bottles of wine first
> were rated from 0 to 100 and then 'transformed' into the rating of 1=
> "least bitter", 5= "most bitter".
>
> 2) Do I also have to transform the continuous 'size' response into
> something like a 1 to 5 rating, before I can apply the clmm2-model?

Yes, essentially you would have to do that in order to apply clmm or
clmm2. I would however not use a mixed effects model for these data at
all. If you had more than one observation per mice, that could be
relevant, but not here. That means I would also treat the time
variable as fixed. I would also start out with a linear model / 2-way
ANOVA. A cumulative link model on a coarsened version of 'size' could
possibly be relevant if you are concerned about the normality
assumption (but a transformation of 'size' might just be better) or if
you are just dying to get predictions in terms of probabilities. I
would be more concerned about correlation between time points, in
which case gls from the nlme package could add an AR1 structure to the
model, but it doesn't sound as if you have a dataset large enough to
identify a correlation structure and it might not be important.

>
> First I thought there - compared to t-tests for independent samples where
> one also can easily apply the Mann-Whitney-U-test if the  data are
> non-normal... - might also be no problem just to apply clmm2 and it should
> at least show me some results.
> But it showed: response needs to be a factor.

As it says: the response needs to be a factor for a clm or a clmm to be fitted.

Hope this helps,
Rune

>
> 3) Is the problem just 2) or lies the problem somewhere else?
>
> I appreciate your answers.
>
> Kind regards,
> Klemens
>
>
>
>
>
>
> 2012/9/13 Jarrod Hadfield <j.hadfield at ed.ac.uk>
>
>> Hi,
>>
>> With normal response, you are right in thinking that you don't need a
>> cumulative link mixed model. A linear mixed model (with group as a random
>> term?) should suffice.
>>
>> Cheers,
>>
>> Jarrod
>>
>>
>>
>>
>>
>>
>> Quoting Klemens Weigl <klemens.weigl at gmail.com> on Wed, 12 Sep 2012
>> 15:58:44 +0200:
>>
>>  Dear R-sig-mixed-model-group!
>>>
>>> Basically I've got a fairly simple dataset with a 2x4 design (two
>>> independent variables = i.v.) and a continuous response variable (only one
>>> dependent variable).
>>> 1st i.v.: two different treatments
>>> 2nd i.v.: 4 time points: after 2, 4, 6 and 8 weeks --> at each time
>>> point: mice with tumor cells are killed and the tumor growth was analyzed.
>>> Therefore no repeated measures. Every mouse can be just in one of the two
>>> treatment groups in just one of the 4 time points.
>>> The data are normally distributed, but with unequal and small 'n' in each
>>> group (ranging from 8 to 14 mice per group).
>>>
>>> Objective: to test wether or not one treatment is better than the other
>>> treatment over the 4 time points all together?
>>>
>>> Someone was suggesting "cumulative link mixed model with Laplace
>>> approximation" for this task.
>>>
>>> Well I am wondering if the clmm with Laplace approximation is appropriate
>>> for this task, because the response variable is "continuous" and not
>>> ordinal (as written in the clmm2_tutorial) Am I loosing much power if I
>>> apply it?
>>>
>>> I'd be interested if someone might have some arguments for or against the
>>> application of clmm with L.a. in that design-setting - or a better
>>> solution?
>>>
>>> Kind regards,
>>> Klemens
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________**_________________
>>> R-sig-mixed-models at r-project.**org <R-sig-mixed-models at r-project.org>mailing list
>>> https://stat.ethz.ch/mailman/**listinfo/r-sig-mixed-models<https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
>>>
>>>
>>>
>>
>> --
>> The University of Edinburgh is a charitable body, registered in
>> Scotland, with registration number SC005336.
>>
>>
>>
>
>
> --
> ______________________
>
> M.Sc. Klemens Weigl
> Applied Statistics, Biostatistics
> +43 680 215 67 58
> klemens.weigl at gmail.com
> www.psychologische-statistik.at
> ______________________
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



-- 
Rune H B Christensen, PhD
DTU Informatics, Section for Statistics
Technical University of Denmark, Build. 305, Room 122,
DK-2800 Kgs. Lyngby, Denmark
Phone: (+45) 45 25 33 63
Mobile: (+45) 30 26 45 54


From maj at waikato.ac.nz  Thu Sep 20 13:23:33 2012
From: maj at waikato.ac.nz (Murray Jorgensen)
Date: Thu, 20 Sep 2012 23:23:33 +1200
Subject: [R-sig-ME] calculating number of parameters for AICc
In-Reply-To: <mailman.5.1348135202.23562.r-sig-mixed-models@r-project.org>
References: <mailman.5.1348135202.23562.r-sig-mixed-models@r-project.org>
Message-ID: <505AFCB5.2000206@waikato.ac.nz>

In reply to Jude Phillips, Ben Bolker wrote the information below. This is a very thorough answer to Jude's question but also a good example of how to tease information out of R.

Ben, do you think this would be a good addition to the Wiki? (I do.)

Murray Jorgensen

=====================================
Well, you can just look in the AICc.mer() function:

K <- attr(logLik(mod), "df")

this leads you (obscurely, I admit) to getMethods("logLik",sig="mer"):

   attr(val, "df") <- dims[["p"]] + dims[["np"]] +
          as.logical(dims[["useSc"]])

To get past here, you would have to dig quite a bit deeper, but
basically the answer is that dims[["p"]] is the same as
length(fixef(model)) -- i.e., the number of fixed-effect coefficients --
and dims[["np"]] is the same as length(getME(model,"theta")) -- the
number of variance parameters.  For each random term in the model with
q components, it has q*(q+1)/2 parameters -- for example, a term of
the form (slope|group) has 3 parameters (intercept variance, slope
variance, correlation between intercept and slope).  The last term
says whether the model uses a scale parameter or not (yes for
linear mixed models, no for typical GLMMs like binomial or Poisson).

Your statement of "number of fixed effects + number of random effects + 2"
doesn't seem correct, but perhaps if you gave an example ...

Whether to add nuisance parameters or not, such as the residual
variance parameter that is estimated based on the residual variance,
is as far as I know an open question.  In the classic AIC context
it doesn't matter as long as one is consistent.  In the AICc context,
I don't think anyone really knows the answer ... adding +1 for
the residual variance parameter (as lme4 does) would make the
model selection process slightly more conservative.

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz      majmurr at gmail.com         Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441   Mobile 021 0200 8350


From bbolker at gmail.com  Thu Sep 20 18:50:22 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 20 Sep 2012 16:50:22 +0000 (UTC)
Subject: [R-sig-ME] model averaging
References: <CAJPX4h_PpdN8k05-REDJX6FMq2AXV_E8s7XzMvqcbfp60U=F3g@mail.gmail.com>
Message-ID: <loom.20120920T175309-358@post.gmane.org>

Paul York <frgger372 at ...> writes:

> I think I understand that if you are comparing models using AICc, you
> should use ML to compare models with the same random effects but different
> fixed effects.  Therefore ML should be used during model selection.
>  However, when you present the effect sizes of your final model, you should
> use REML, because it provides better estimates of beta (please correct me
> if I'm wrong here!).  However, I am now interested in proceeding to model
> averaging, and I'm unclear whether I should be using ML or REML for this
> stage of the analysis - ML will provide better estimates of AIC weights (I
> assume?) but REML will provide better estimates of beta.  So does anyone
> know which I should be using?

  Hmmm. I'm not sure, but ... my understanding was that REML provided
unbiased (for specific categories of models) estimates of *variances*,
but that the fixed-effect (beta) coefficients were identical.  You can
test this empirically for one special case:

> (fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
> fm1ML <- update(fm1,REML=FALSE)
> fixef(fm1)
(Intercept)        Days 
  251.40510    10.46729 
> fixef(fm1ML)
(Intercept)        Days 
  251.40510    10.46729 

  The argument about ML vs REML comparisons strictly speaking applies
to likelihood ratio tests, marginal F tests, and other tests that assume
nestedness, but I think it's probably a good idea by extension to use
ML for other types of model comparison.  I would suggest using it for
model averaging as well.


From kumiko.fukumura at strath.ac.uk  Thu Sep 20 20:36:04 2012
From: kumiko.fukumura at strath.ac.uk (Kumiko Fukumura)
Date: Thu, 20 Sep 2012 19:36:04 +0100
Subject: [R-sig-ME] R-sig-mixed-models Digest, Vol 69, Issue 18
In-Reply-To: <mailman.3.1347616802.10272.r-sig-mixed-models@r-project.org>
References: <mailman.3.1347616802.10272.r-sig-mixed-models@r-project.org>
Message-ID: <1F943C50E37E194BB891501E163237A8016B139225A0@E2K7-MS2.ds.strath.ac.uk>

Hi,

RE: "Hauck-Donner effect"

I was wondering if you could possibly clarify whether Hauck-Donner effect affects coefficients for interactions only or it also affects fixed effects as well. In my experience, it seems to vary: in some situations, many zeros in one or more condition appeared to influence the estimate for the interaction only, but they also seem to influence fixed effects in other situations (though the fixed effects estimates were sensible when the interaction was removed from the model).  Another question is what we can do under such situations - having many zeros also seem to influence the results of model comparisons (using "anova" functions) as well as coefficient estimates - I noticed that some recommended the use of model comparisons to address the problem, but it doesn't seem to help much. Removing some conditions is a possibility, but it's a bit shame because we cannot take into account the whole data set in our analyses.  I'd be very grateful if you could give us any more information that you know about this phenomenon. Thank you very much in advance.

Best wishes
Kumiko


Kumiko Fukumura
University of Strathclyde


>You should look up/Google for "Hauck-Donner effect" (you can find a discussion in Venables and Ripley's book), which refers to the situation >where the approximation used to compute confidence intervals on GLM(M)s breaks down for strong effects.
>You should use explicit model comparison (?update, ?anova, ?drop1) to test the difference between models with and without the intercept term.

>However, you might want to be careful with the all-zero case, as it will lead to an infinite estimate (in theory) of the interaction coefficient -- in >practice it will just lead to a very large, poorly constrained estimate.
>You could try a Bayesian method, or you could just try leaving out that category and make sure that the qualitative results of your analysis remain >unchanged ...

  Ben Bolker



------------------------------

_______________________________________________
R-sig-mixed-models mailing list
R-sig-mixed-models at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


End of R-sig-mixed-models Digest, Vol 69, Issue 18


From helixed2 at yahoo.com  Fri Sep 21 03:26:37 2012
From: helixed2 at yahoo.com (Jeremy Koster)
Date: Thu, 20 Sep 2012 18:26:37 -0700 (PDT)
Subject: [R-sig-ME] A modeling question for the ecologists
Message-ID: <1348190797.35242.YahooMailNeo@web160306.mail.bf1.yahoo.com>

This message is directly especially toward the ecologists like Ben Bolker, but I'd welcome any advice . . .

We have data on the harvests of wildlife. ?For approximately 5,000 harvested animals (and thus 5,000 rows in the data frame), we have these variables:
1. "Species" (about 20 in all)
2. "Activity" (whether the harvested species is predominantly diurnal or nocturnal)
3. "Time" (the time of day at which the animal was harvested
4. "Dogs" (whether or not dogs were present and presumably assisting when the animal was captured)

The working hypothesis is that dogs increase harvests of nocturnal species because they can sniff them out during the day when they're sleeping and track them at night when vision is limited (almost all nocturnal hunting involves dogs in this setting).

So initially I was inclined to specify a model:

model <- glmer ( Dogs ~ Activity * Time + (1|Species) , family = binomial, data = d)

That was partly because "Activity" is essentially a species-level variable, so it felt appropriate to include it as a predictor with a random effect for "Species."

Intuitively, though, we tend to think of "Activity" as the outcome variable and the presence of dogs as a predictor, which raises or lowers the preponderance of nocturnal species in the harvest.

Is there a good way to model these data while retaining that sense of causality -- in other words, could we put Activity on the left side of the equation?



From jbaldwin at fs.fed.us  Fri Sep 21 04:23:34 2012
From: jbaldwin at fs.fed.us (Baldwin, Jim -FS)
Date: Fri, 21 Sep 2012 02:23:34 +0000
Subject: [R-sig-ME] A modeling question for the ecologists
In-Reply-To: <1348190797.35242.YahooMailNeo@web160306.mail.bf1.yahoo.com>
References: <1348190797.35242.YahooMailNeo@web160306.mail.bf1.yahoo.com>
Message-ID: <DDC5EC9B78340042B0D5A0C3789D45691996782E@001FSN2MPN1-063.001f.mgd2.msft.net>

I'm not in Ben's league (either as an ecologist or as a statistician) and proof of that might be with the following questions/comments:

1.  By having presence of dogs as the dependent variable it seems that your question (potentially) addressed by the model is "Given that we have a capture from a nocturnal species, what proportion involved dogs?"  But that would be influenced with how often dogs were used (which has nothing to do with how well dogs can detect certain species).

2.  Activity is a known and singular quantity for each species (or at least there is just a single assessment even if it might be wrong).  Wouldn't the capture rate by dogs depend on the harvested species?  I'm not understanding what an overall effect of "activity" might mean given the different numbers of each species available to be captured and how well a dog detects any particular species.

3.  It would seem that the units of replication would have to acknowledge multiple harvests (and I'm assuming that there are multiple harvests some with dogs and some without) and different areas harvested rather than the individual captures.  From your description I assume that "Dogs" are applied to individual "harvests".  (Not to mention the use of different dogs or numbers of dogs.)

Jim Baldwin


-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Jeremy Koster
Sent: Thursday, September 20, 2012 6:27 PM
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] A modeling question for the ecologists

This message is directly especially toward the ecologists like Ben Bolker, but I'd welcome any advice . . .

We have data on the harvests of wildlife.  For approximately 5,000 harvested animals (and thus 5,000 rows in the data frame), we have these variables:
1. "Species" (about 20 in all)
2. "Activity" (whether the harvested species is predominantly diurnal or nocturnal) 3. "Time" (the time of day at which the animal was harvested 4. "Dogs" (whether or not dogs were present and presumably assisting when the animal was captured)

The working hypothesis is that dogs increase harvests of nocturnal species because they can sniff them out during the day when they're sleeping and track them at night when vision is limited (almost all nocturnal hunting involves dogs in this setting).

So initially I was inclined to specify a model:

model <- glmer ( Dogs ~ Activity * Time + (1|Species) , family = binomial, data = d)

That was partly because "Activity" is essentially a species-level variable, so it felt appropriate to include it as a predictor with a random effect for "Species."

Intuitively, though, we tend to think of "Activity" as the outcome variable and the presence of dogs as a predictor, which raises or lowers the preponderance of nocturnal species in the harvest.

Is there a good way to model these data while retaining that sense of causality -- in other words, could we put Activity on the left side of the equation?


_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models





This electronic message contains information generated by the USDA solely for the intended recipients. Any unauthorized interception of this message or the use or disclosure of the information it contains may violate the law and subject the violator to civil or criminal penalties. If you believe you have received this message in error, please notify the sender and delete the email immediately.


From helixed2 at yahoo.com  Fri Sep 21 04:51:20 2012
From: helixed2 at yahoo.com (Jeremy Koster)
Date: Thu, 20 Sep 2012 19:51:20 -0700 (PDT)
Subject: [R-sig-ME] A modeling question for the ecologists
In-Reply-To: <DDC5EC9B78340042B0D5A0C3789D45691996782E@001FSN2MPN1-063.001f.mgd2.msft.net>
References: <1348190797.35242.YahooMailNeo@web160306.mail.bf1.yahoo.com>
	<DDC5EC9B78340042B0D5A0C3789D45691996782E@001FSN2MPN1-063.001f.mgd2.msft.net>
Message-ID: <1348195880.11982.YahooMailNeo@web160303.mail.bf1.yahoo.com>

Thanks, Jim. ?There are definitely some limitations to the dataset (which was collected for other purposes). ?For example, we have data only from successful hunts, so we can't really estimate how much time dogs spend on nocturnal versus diurnal hunts (speaking to your first concern). ?A correlation between the use of dogs and the activity patterns of prey species (e.g., nocturnal) wouldn't necessarily mean a whole lot if dogs were used almost exclusively at night -- hence our efforts to include the time of the harvest as a crude indicator along these lines. ?And yes, a simple binary variable to denote the presence of dogs overlooks variation in their abilities (a favorite topic of mine).

To rephrase, we're basically interested in testing for a correlation between the use of dogs and the relative preponderance of nocturnal species in the harvest (as in your first point). ?Because we have contextual data for each harvest -- specifically the time at which the animal was harvested -- I was reluctant to aggregate the data, which steered me toward a logistic regression and the need to account for the repeated harvests of the same species via a mixed model. ?But since these are all categorical predictors, it occurs to me that there might be better alternatives . . .

In any case, a colleague just suggested this model:

model <- glmer ( Activity ~ Dogs * Time + (1|Species) , family = binomial, data = d)


But that one just seems peculiar . . . albeit for reasons that I can't fully articulate.



----- Original Message -----
From: "Baldwin, Jim -FS" <jbaldwin at fs.fed.us>
To: Jeremy Koster <helixed2 at yahoo.com>; "r-sig-mixed-models at r-project.org" <r-sig-mixed-models at r-project.org>
Cc: 
Sent: Thursday, September 20, 2012 10:23 PM
Subject: RE: [R-sig-ME] A modeling question for the ecologists

I'm not in Ben's league (either as an ecologist or as a statistician) and proof of that might be with the following questions/comments:

1.? By having presence of dogs as the dependent variable it seems that your question (potentially) addressed by the model is "Given that we have a capture from a nocturnal species, what proportion involved dogs?"? But that would be influenced with how often dogs were used (which has nothing to do with how well dogs can detect certain species).

2.? Activity is a known and singular quantity for each species (or at least there is just a single assessment even if it might be wrong).? Wouldn't the capture rate by dogs depend on the harvested species?? I'm not understanding what an overall effect of "activity" might mean given the different numbers of each species available to be captured and how well a dog detects any particular species.

3.? It would seem that the units of replication would have to acknowledge multiple harvests (and I'm assuming that there are multiple harvests some with dogs and some without) and different areas harvested rather than the individual captures.? From your description I assume that "Dogs" are applied to individual "harvests".? (Not to mention the use of different dogs or numbers of dogs.)

Jim Baldwin


-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Jeremy Koster
Sent: Thursday, September 20, 2012 6:27 PM
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] A modeling question for the ecologists

This message is directly especially toward the ecologists like Ben Bolker, but I'd welcome any advice . . .

We have data on the harvests of wildlife.? For approximately 5,000 harvested animals (and thus 5,000 rows in the data frame), we have these variables:
1. "Species" (about 20 in all)
2. "Activity" (whether the harvested species is predominantly diurnal or nocturnal) 3. "Time" (the time of day at which the animal was harvested 4. "Dogs" (whether or not dogs were present and presumably assisting when the animal was captured)

The working hypothesis is that dogs increase harvests of nocturnal species because they can sniff them out during the day when they're sleeping and track them at night when vision is limited (almost all nocturnal hunting involves dogs in this setting).

So initially I was inclined to specify a model:

model <- glmer ( Dogs ~ Activity * Time + (1|Species) , family = binomial, data = d)

That was partly because "Activity" is essentially a species-level variable, so it felt appropriate to include it as a predictor with a random effect for "Species."

Intuitively, though, we tend to think of "Activity" as the outcome variable and the presence of dogs as a predictor, which raises or lowers the preponderance of nocturnal species in the harvest.

Is there a good way to model these data while retaining that sense of causality -- in other words, could we put Activity on the left side of the equation?


_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models





This electronic message contains information generated by the USDA solely for the intended recipients. Any unauthorized interception of this message or the use or disclosure of the information it contains may violate the law and subject the violator to civil or criminal penalties. If you believe you have received this message in error, please notify the sender and delete the email immediately.


From torvon at gmail.com  Fri Sep 21 19:43:42 2012
From: torvon at gmail.com (Eiko Fried)
Date: Fri, 21 Sep 2012 13:43:42 -0400
Subject: [R-sig-ME] Intra-individual variation over time
Message-ID: <CACm_P7qG3_+d5Ucnu-UVk83ns7d38KmuOwyngvTv6aH8i923rQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120921/46043292/attachment.pl>

From helixed2 at yahoo.com  Fri Sep 21 22:42:11 2012
From: helixed2 at yahoo.com (Jeremy Koster)
Date: Fri, 21 Sep 2012 13:42:11 -0700 (PDT)
Subject: [R-sig-ME] A modeling question for the ecologists
In-Reply-To: <DDC5EC9B78340042B0D5A0C3789D45691996782E@001FSN2MPN1-063.001f.mgd2.msft.net>
References: <1348190797.35242.YahooMailNeo@web160306.mail.bf1.yahoo.com>
	<DDC5EC9B78340042B0D5A0C3789D45691996782E@001FSN2MPN1-063.001f.mgd2.msft.net>
Message-ID: <1348260131.88572.YahooMailNeo@web160305.mail.bf1.yahoo.com>

As a brief update, two off-list suggestions came in today:

1. Multinomial logistic regression, with "species" as the outcome variable, as in:


model <- multinom(Species?~ Dogs?* Time, data = d.known)

But it wasn't clear how to incorporate "activity" (i.e., whether or not the species is nocturnal)

and

2. Log-linear models of a 3-way contingency table of the aggregated counts (for "Dogs," "Activity," and "Time") . . . but there were concerns about treating each harvested animal as independent of its species.

Any advice on either of these two options would be much appreciated.



----- Original Message -----
From: "Baldwin, Jim -FS" <jbaldwin at fs.fed.us>
To: Jeremy Koster <helixed2 at yahoo.com>; "r-sig-mixed-models at r-project.org" <r-sig-mixed-models at r-project.org>
Cc: 
Sent: Thursday, September 20, 2012 10:23 PM
Subject: RE: [R-sig-ME] A modeling question for the ecologists

I'm not in Ben's league (either as an ecologist or as a statistician) and proof of that might be with the following questions/comments:

1.? By having presence of dogs as the dependent variable it seems that your question (potentially) addressed by the model is "Given that we have a capture from a nocturnal species, what proportion involved dogs?"? But that would be influenced with how often dogs were used (which has nothing to do with how well dogs can detect certain species).

2.? Activity is a known and singular quantity for each species (or at least there is just a single assessment even if it might be wrong).? Wouldn't the capture rate by dogs depend on the harvested species?? I'm not understanding what an overall effect of "activity" might mean given the different numbers of each species available to be captured and how well a dog detects any particular species.

3.? It would seem that the units of replication would have to acknowledge multiple harvests (and I'm assuming that there are multiple harvests some with dogs and some without) and different areas harvested rather than the individual captures.? From your description I assume that "Dogs" are applied to individual "harvests".? (Not to mention the use of different dogs or numbers of dogs.)

Jim Baldwin


-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Jeremy Koster
Sent: Thursday, September 20, 2012 6:27 PM
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] A modeling question for the ecologists

This message is directly especially toward the ecologists like Ben Bolker, but I'd welcome any advice . . .

We have data on the harvests of wildlife.? For approximately 5,000 harvested animals (and thus 5,000 rows in the data frame), we have these variables:
1. "Species" (about 20 in all)
2. "Activity" (whether the harvested species is predominantly diurnal or nocturnal) 3. "Time" (the time of day at which the animal was harvested 4. "Dogs" (whether or not dogs were present and presumably assisting when the animal was captured)

The working hypothesis is that dogs increase harvests of nocturnal species because they can sniff them out during the day when they're sleeping and track them at night when vision is limited (almost all nocturnal hunting involves dogs in this setting).

So initially I was inclined to specify a model:

model <- glmer ( Dogs ~ Activity * Time + (1|Species) , family = binomial, data = d)

That was partly because "Activity" is essentially a species-level variable, so it felt appropriate to include it as a predictor with a random effect for "Species."

Intuitively, though, we tend to think of "Activity" as the outcome variable and the presence of dogs as a predictor, which raises or lowers the preponderance of nocturnal species in the harvest.

Is there a good way to model these data while retaining that sense of causality -- in other words, could we put Activity on the left side of the equation?


_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models





This electronic message contains information generated by the USDA solely for the intended recipients. Any unauthorized interception of this message or the use or disclosure of the information it contains may violate the law and subject the violator to civil or criminal penalties. If you believe you have received this message in error, please notify the sender and delete the email immediately.


From bbolker at gmail.com  Fri Sep 21 23:09:00 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 21 Sep 2012 21:09:00 +0000 (UTC)
Subject: [R-sig-ME] R-sig-mixed-models Digest, Vol 69, Issue 18
References: <mailman.3.1347616802.10272.r-sig-mixed-models@r-project.org>
	<1F943C50E37E194BB891501E163237A8016B139225A0@E2K7-MS2.ds.strath.ac.uk>
Message-ID: <loom.20120921T230103-246@post.gmane.org>

Kumiko Fukumura <kumiko.fukumura at ...> writes:

> 
> Hi,
> 
> RE: "Hauck-Donner effect"
> 

> I was wondering if you could possibly clarify whether Hauck-Donner
> effect affects coefficients for interactions only or it also affects
> fixed effects as well. In my experience, it seems to vary: in some
> situations, many zeros in one or more condition appeared to
> influence the estimate for the interaction only, but they also seem
> to influence fixed effects in other situations (though the fixed
> effects estimates were sensible when the interaction was removed
> from the model).  Another question is what we can do under such
> situations - having many zeros also seem to influence the results of
> model comparisons (using "anova" functions) as well as coefficient
> estimates - I noticed that some recommended the use of model
> comparisons to address the problem, but it doesn't seem to help
> much. Removing some conditions is a possibility, but it's a bit
> shame because we cannot take into account the whole data set in our
> analyses.  I'd be very grateful if you could give us any more
> informati!  on that you know about this phenomenon. Thank you very
> much in advance.

very brief thoughts:

  (1) you should be very, very careful interpreting the meaning
of main effects in the presence of interactions containing them.
  (2) it's possible that you're mixing up two different phenomena,
Hauck-Donner effects (which occur when some effects are strong
so that the likelihood surface is far from quadratic) and complete
separation (which occurs when some breakpoint or set of categories
in the data separates a region of all-zero from a region of all-one
[or all-positive] responses, leading to infinite estimates of
some parameters). (I see that my answer didn't distinguish between
these phenomena very clearly either.)

  Hauck-Donner effects are trivially handled by model comparison.

  Complete separation must be dealt with by bias-reducing algorithms
(Firth: see e.g. the logistf package), by adding Bayesian priors
(see e.g. bayesglm in the arm package or blmer in the blme package
[maybe], or use MCMCglmm, or WinBUGS).

  Neither of these is mixed-model specific, and in fact they're
not R-specific either, so you might want to ask further questions
either on R-help or on http://stats.stackexchange.com ...  
 
> Best wishes
> Kumiko
> 
> Kumiko Fukumura
> University of Strathclyde
 
> >You should look up/Google for "Hauck-Donner effect" (you can find a
> discussion in Venables and Ripley's book), which refers to the
> situation >where the approximation used to compute confidence
> intervals on GLM(M)s breaks down for strong effects.  >You should
> use explicit model comparison (?update, ?anova, ?drop1) to test the
> difference between models with and without the intercept term.

> >However, you might want to be careful with the all-zero case, as it
> will lead to an infinite estimate (in theory) of the interaction
> coefficient -- in >practice it will just lead to a very large,
> poorly constrained estimate.  >You could try a Bayesian method, or
> you could just try leaving out that category and make sure that the
> qualitative results of your analysis remain >unchanged ...


From fci201 at exeter.ac.uk  Sat Sep 22 16:10:13 2012
From: fci201 at exeter.ac.uk (Ingleby, Fiona)
Date: Sat, 22 Sep 2012 14:10:13 +0000
Subject: [R-sig-ME] Multivariate mixed models
In-Reply-To: <FD6F6B8A5C87974AA3DC10E3656878B1082671@VMEXCHANGEMBS6A.isad.isadroot.ex.ac.uk>
References: <FD6F6B8A5C87974AA3DC10E3656878B108265C@VMEXCHANGEMBS6A.isad.isadroot.ex.ac.uk>,
	<FD6F6B8A5C87974AA3DC10E3656878B1082671@VMEXCHANGEMBS6A.isad.isadroot.ex.ac.uk>
Message-ID: <FD6F6B8A5C87974AA3DC10E3656878B108269F@VMEXCHANGEMBS6A.isad.isadroot.ex.ac.uk>

Hi everyone,

I have a dataset with 3 trait measurements for individuals from different families, like this:

trait1      trait2      trait3      family
 0.5        -0.2         0.2         A
 0.2         0.7        -0.1         A
 0.3        -0.3         0.5         A
 0.1        -0.1         0.4         B
-0.4         0.5        -0.6         B
-0.1         0.8         0            B
-0.2         0.7         0.5         C

...and so on (there are 80 families with 20 individuals measured from each).

The model I want to fit is pretty straightforward, with the three traits as a multivariate response, and family as a random effect. I want to be able to extract the genetic variance-covariance matrix for the set of 3 traits. I can do this quite easily with MCMCglmm, and am aware of non-Bayesian methods in other programs (i.e. SAS), but awkwardly, I want to fit a non-Bayesian model in R.

I can run individual lmer models for each trait, e.g.:

model <- lmer (trait1 ~ 1 + (1|family) )

and extract the random effects variance for the family term, giving me the genetic variance for each trait from each model individually. But this doesn't allow me to model the genetic covariance between traits, and so I'm wondering if there is a way to run lmer with a multivariate response, or if there is another method I could use to fit this kind of model?

I did try re-arranging my dataset so that the 3 trait scores for each individual were stacked into one variable (creating new variables for trait.id (1, 2, or 3) and individual id within family) like this:

trait.score     trait.id     individual     family

and then the model:

model <- lmer (trait.score ~ 1 + (trait.id|family) + (individual|family) )

but this model makes R crash and so I'm assuming there might be some kind of convergence problem?! And I'm not even sure this model would give me the genetic covariances between traits, it would probably just give the genetic variances for each trait (as before but in one model instead of 3 separately)?
 
Any ideas/advice would be greatly appreciated.

Thanks,

Fiona Ingleby


From hans at sociologi.cjb.net  Mon Sep 24 16:16:11 2012
From: hans at sociologi.cjb.net (Hans Ekbrand)
Date: Mon, 24 Sep 2012 16:16:11 +0200
Subject: [R-sig-ME] Interpreting the output of summary() of a glmer-object
Message-ID: <20120924141611.GA4189@samir>

Dear list,

First, I have a very simple question. In the summary output of a
glmer-object, What does the "Variance" and "Std.Dev" mean for the
Random effects? What is the scale for these measures?

load(url("http://sociologi.cjb.net/temp/a.strange.df.RData"))
my.fit.1 <- glmer(MV744A ~ (1|MV024), data = a.strange.df, family = "binomial")
summary(my.fit.1)

Generalized linear mixed model fit by the Laplace approximation 
Formula: MV744A ~ (1 | MV024) 
   Data: a.strange.df 
   AIC   BIC logLik deviance
 76209 76227 -38102    76205
Random effects:
 Groups Name        Variance Std.Dev.
 MV024  (Intercept) 0.40558  0.63685 
Number of obs: 73601, groups: MV024, 29

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -1.4187     0.1191  -11.91   <2e-16 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 

I think that I understand that if the Variance term, here 0.40558 is
low relative to the Std.Dev, there is not much variation caught by the
random term (in this case where the random term represents "Regions",
few Regions would then significantly differ from the grand mean). Here
we have a big underlying n, which might explain that most Regions did
signficantly differ from the mean.

dotplot(ranef(my.fit.1, postVar = TRUE))


Secondly, after adding several fixed terms, each with a substantial effect, I
would (given my vague understanding of what the "Variance" term means)
expect the "Variance" of the random effect to decrease, but on the
contrary it increased:

summary(my.fit.5 <- glmer(MV744A ~ (1|MV024) + MV025 + MV106 + MV012 + MV130, data = a.strange.df, family = "binomial"))

Generalized linear mixed model fit by the Laplace approximation 
Formula: MV744A ~ (1 | MV024) + MV025 + MV106 + MV012 + MV130 
   Data: a.strange.df
   AIC   BIC logLik deviance
 73327 73483 -36646    73293
Random effects:
 Groups Name        Variance Std.Dev.
 MV024  (Intercept) 0.46855  0.6845  
Number of obs: 73560, groups: MV024, 29

Fixed effects:
                             Estimate Std. Error z value Pr(>|z|)    
(Intercept)                -0.7491070  0.1343065   -5.58 2.44e-08 ***
MV025Rural                  0.4423018  0.0198776   22.25  < 2e-16 ***
MV106Primary               -0.1829852  0.0306670   -5.97 2.42e-09 ***
MV106Secondary             -0.5743379  0.0263945  -21.76  < 2e-16 ***
MV106Higher                -1.2945147  0.0385589  -33.57  < 2e-16 ***
MV012                      -0.0145262  0.0008938  -16.25  < 2e-16 ***
MV130Muslim                 0.3079549  0.0276753   11.13  < 2e-16 ***
MV130Christian             -0.2949242  0.0431780   -6.83 8.47e-12 ***
MV130Sikh                  -0.2605808  0.1087276   -2.40  0.01655 *  
MV130Buddhist/Neo-Buddhist -0.0139619  0.0830028   -0.17  0.86642    
MV130Jain                  -0.3624429  0.1909181   -1.90  0.05764 .  
MV130Jewish                -8.9103460 63.6027289   -0.14  0.88859    
MV130Parsi/Zoroastrian     -9.6231061 90.8921529   -0.11  0.91568    
MV130No religion           -0.5553414  0.7918028   -0.70  0.48308    
MV130Donyi polo             0.7034558  0.2266543    3.10  0.00191 ** 
MV130Other                 -0.0718436  0.0890363   -0.81  0.41972    
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 

Sure, the Std.Dev of the random effect also increased (from 0.63685 to
0.6845) but still, isn't the increase of the variance of random effect
(from 0.40558 to 0.46855) rather odd here?

How do you interpret an increase in the random effect after adding
perfectly fine explanatory fixed terms to the model?

BTW. The dependent variable MV744A measures an attitude, and MV025 is
type of area (Urban/Rural), MV106 is educational level, MV012 is age,
MV130 is religion. 

The caterpillar plot for my.fit.5, shows all regions except 3 of them
differ signifcantly from the mean, even when controlling for the fixed
terms.

dotplot(ranef(my.fit.5, postVar = TRUE))

Any hints, or reading tips, greatly appreciated.

Kind regards,

--
Hans Ekbrand
Department of sociology and workscience
University of Gothenburg, Sweden.


From jone2093 at umn.edu  Fri Sep 21 23:36:02 2012
From: jone2093 at umn.edu (Stephen Jones)
Date: Fri, 21 Sep 2012 16:36:02 -0500
Subject: [R-sig-ME] Seemingly Unrelated Regression in lme4
Message-ID: <CAObg2BHGjH-kEXXdqEtLb4QfoShKrHc2jmadS=N-stdY=GNzNQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120921/135e0178/attachment.pl>

From michael.zombok at googlemail.com  Sat Sep 22 16:40:38 2012
From: michael.zombok at googlemail.com (michael.zombok at googlemail.com)
Date: Sat, 22 Sep 2012 16:40:38 +0200
Subject: [R-sig-ME] Random effects in a nested factorial design - Problems
	understanding the effect of reference categories
Message-ID: <souy5k2m87t.fsf@gmx.de>

Hello,

I have problems understanding the following different, but somewhat
related mixed models and would thankful for any comments or references.

Suposse you have repeated measurements from an experiment with two factors
- factor 1 treatment: notreatment/treatment
- factor 2 time: t1/t2/t3

Every subject is either in the notreatment or treatment group and is
clearly distinguished by an unique ID (named IDS) in the data. So the
data (given in the long format) looks like the following:

IDS treatment time resp
1 TR T1 2.3
1 TR T2 4.2
1 TR T3 8.2
2 TR T1 3.2
2 TR T2 3.1
.
.
10 NT T1 3.2
10 NT T2 3.5
10 NT T3 3.2
11 NT T1 1.2
11 NT T2 3.5
11 NT T3 2.2

Both factors are treated 'as.factor' in R and not as numeric. One
is interested in the effects of both, treatment and time.

As the intercept in this model is the level in the response of the
notreatment group at T1, I wonder what is modelled with the following model:

resp ~ treatment + time + (1|IDS)            #M1

? Only the variability of the subjects in the no treatment group at time
T1 or the overall variability of all subjects? 

What is the difference to the following two models: 

resp ~ treatment + time + (1 + treatment|IDS)       #M2
resp ~ treatment + time + (0 + treatment|IDS)       #M3

As every subject is in only one treatment group, the overall error
variance remains the same, but the random effect variance is somehow
divided - how? Do the individual estimates make sense? Can they be interpreted?

An additional question: How do I model the variability of subjects over
time in this setting with random effects?

resp ~ treatment + time + (1 + time|IDS) ?

If I am correct, the intercept in this model refers to the variability
of the subjects at T1? Again: Does the intercept refer to all subjects
or only the subject of the reference category?

I appreciate every comment or reference. Thank you very much!

Best,
Friedericksen


From giulia.dottisani at gmail.com  Mon Sep 24 18:11:21 2012
From: giulia.dottisani at gmail.com (Giulia Dotti Sani)
Date: Mon, 24 Sep 2012 18:11:21 +0200
Subject: [R-sig-ME] Seemingly Unrelated Regression in lme4
In-Reply-To: <CAObg2BHGjH-kEXXdqEtLb4QfoShKrHc2jmadS=N-stdY=GNzNQ@mail.gmail.com>
References: <CAObg2BHGjH-kEXXdqEtLb4QfoShKrHc2jmadS=N-stdY=GNzNQ@mail.gmail.com>
Message-ID: <CAKz8HvngVDhV_V75pgtGRm4TEbF20=pZDLUDYnii=YGCFESiEA@mail.gmail.com>

I posted the same  question a few months ago, have a look here:

https://stat.ethz.ch/pipermail/r-sig-mixed-models/2012q1/018070.html

and the following threads.
Hope it helps,

Giulia Dotti Sani,
University of Trento
Italy

On Fri, Sep 21, 2012 at 11:36 PM, Stephen Jones <jone2093 at umn.edu> wrote:
> Hello,
>
> I am using lme4 to analyze a dataset that has crossed random effects. There
> are three outcomes of interest, and currently I am using three separate
> linear mixed-effects models. I would like to use seemingly unrelated
> regression (SUR) to perform cross-equation tests on the fixed effects
> parameters.
>
> Is there a way to use SUR with lme4? Are there any papers that can serve as
> examples or documentation that may help?
>
> (This is my first time posting to the group, so please let me know if I
> should be including other info in my post.)
>
> Stephen Jones
> Carlson School of Management
> University of Minnesota
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From bbolker at gmail.com  Tue Sep 25 04:04:19 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 25 Sep 2012 02:04:19 +0000 (UTC)
Subject: [R-sig-ME] Interpreting the output of summary() of a
	glmer-object
References: <20120924141611.GA4189@samir>
Message-ID: <loom.20120925T035248-650@post.gmane.org>

Hans Ekbrand <hans at ...> writes:

> First, I have a very simple question. In the summary output of a
> glmer-object, What does the "Variance" and "Std.Dev" mean for the
> Random effects? What is the scale for these measures?

  It's a little hard to think of a way to say this that doesn't
seem redundant ... "Variance" is the estimated variance of the
random effects, "Std.Dev" is the standard deviation (i.e. the
square root of the variance -- these quantities give redundant
information; seeing the variance can be useful because of the
additivity of variances and the traditional presentation of
mixed models in terms of variance decomposition, while the
standard deviation can be useful because it is on the same scale
as the estimated fixed-effect coefficients).  The scale is the
same as the scale of the fixed-effect coefficients, i.e. the
scale of the linear predictor.

  For example, for a Poisson GLMER

glmer(y~x+(1|grp),family=poisson,...)

  the underlying statistical model is

  Y_{ij} ~ Poisson(lambda_{ij})
  log(lambda_{ij}) = b_0 + b_1*x_{ij} + eps_j
  eps_j ~ Normal(0,sigma^2_g)

  "Variance" is the estimate of sigma^2_g

the estimated 
> 
> load(url("http://sociologi.cjb.net/temp/a.strange.df.RData"))
> my.fit.1 <- glmer(MV744A ~ (1|MV024), 
>    data = a.strange.df, family = "binomial")
> summary(my.fit.1)
> 
> Generalized linear mixed model fit by the Laplace approximation 
> Formula: MV744A ~ (1 | MV024) 
>    Data: a.strange.df 
>    AIC   BIC logLik deviance
>  76209 76227 -38102    76205
> Random effects:
>  Groups Name        Variance Std.Dev.
>  MV024  (Intercept) 0.40558  0.63685 
> Number of obs: 73601, groups: MV024, 29
> 
> Fixed effects:
>             Estimate Std. Error z value Pr(>|z|)    
> (Intercept)  -1.4187     0.1191  -11.91   <2e-16 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 
> 
> I think that I understand that if the Variance term, here 0.40558 is
> low relative to the Std.Dev, 

  This is not really a meaningful statement.  The Variance reported
is always the (Std.Dev)^2.

> there is not much variation caught by the
> random term (in this case where the random term represents "Regions",
> few Regions would then significantly differ from the grand mean). 

  It's probably easiest to compare the standard deviation to
the fixed effect coefficients.  It's a little hard to know whether
it's "small" because there's not really anything else in this model
to compare it to ... (you could compare it to the intercept, but
that only tells you about the possibility that the binomial probability
is 0.5 (corresponding to 0 on the logit scale), so it's probably
not meaningful ...)

> Here
> we have a big underlying n, which might explain that most Regions did
> signficantly differ from the mean.

  Not clear to me what this means.

> dotplot(ranef(my.fit.1, postVar = TRUE))
> 
> Secondly, after adding several fixed terms, each with a substantial effect, I
> would (given my vague understanding of what the "Variance" term means)
> expect the "Variance" of the random effect to decrease, but on the
> contrary it increased:
> 
> summary(my.fit.5 <- glmer(MV744A ~ (1|MV024) + MV025 + 
> MV106 + MV012 + MV130, data = a.strange.df, family = "binomial"))
> 
> Generalized linear mixed model fit by the Laplace approximation 
> Formula: MV744A ~ (1 | MV024) + MV025 + MV106 + MV012 + MV130 
>    Data: a.strange.df
>    AIC   BIC logLik deviance
>  73327 73483 -36646    73293
> Random effects:
>  Groups Name        Variance Std.Dev.
>  MV024  (Intercept) 0.46855  0.6845  
> Number of obs: 73560, groups: MV024, 29
> 
  
 [snip]

> Sure, the Std.Dev of the random effect also increased (from 0.63685 to
> 0.6845) but still, isn't the increase of the variance of random effect
> (from 0.40558 to 0.46855) rather odd here?

  Not necessarily.  We're not necessarily talking about "explained
variance" here.

> The caterpillar plot for my.fit.5, shows all regions except 3 of them
> differ signifcantly from the mean, even when controlling for the fixed
> terms.

  In the context of a mixed model it probably doesn't make a lot
of sense to discuss regions "differing significantly from the mean" --
if you want to do hypothesis tests like that, you should treat
the region as a fixed effect ...


From h.l.ward at qmul.ac.uk  Tue Sep 25 16:37:16 2012
From: h.l.ward at qmul.ac.uk (Helen Ward)
Date: Tue, 25 Sep 2012 15:37:16 +0100
Subject: [R-sig-ME] Fwd: Priors for and estimating heritability from an
 ordinal and a Poisson animal model
In-Reply-To: <5048BDAB.3090103@qmul.ac.uk>
References: <5048BDAB.3090103@qmul.ac.uk>
Message-ID: <5061C19C.3050409@qmul.ac.uk>

Hi everyone,

I have posted this before, but I thought I'd try one more time in the 
hope that someone can help :) Sorry if you've read it twice.

Helen

....


Hello everybody,

I've just come back to some models I was working on a while ago and was
hoping someone could help me clear up a couple of issues. My goal is to
calculate the heritability of two independent traits: number of deformed
toes, and age of 1st reproduction.

Number of deformed toes is a data set of integers ranging from 0 to 6:
it looks most like a zero-inflated Poisson distribution.
Age of 1st reproduction is a data set of integers ranging from 0
upwards: it has a clear Poisson distribution.

I have built animal models for these traits in ASReml specifying that
both data sets are Poisson distributed. However, I cannot compare my
heritability estimates with those estimated for other Gaussian traits as
in a Poisson model in ASReml the residual variance is fixed at 1.

I am now trying to model these traits in MCMCglmm, deformed toes in an
ordinal model and age of 1st repro in a Poisson model.

At the moment for deformed toes I am using

prior2=list(R=list(V=1, fix=1), G=list(G1=list(V=1, nu=1,
alpha.mu=0,alpha.V=100)))

model1<-MCMCglmm(Toes~1,random=~animal,family="ordinal",pedigree=ped,data=data,prior=prior2,nitt=500000,thin=300,burnin=300000,verbose=FALSE,pl=TRUE)
then

posterior.heritability1<-model1$VCV[,"animal"]/(model1$VCV[,"animal"]+model1$VCV[,"units"]+1)to 

calculate heritabilty.

For age of 1st repro I am using

prior1.1<-list(G=list(G1=list(V=matrix(p.var/2),n=1)),
R=list(V=matrix(p.var/2),n=1))

model1.1<-MCMCglmm(Poisson1strepro~1,random=~animal,family=?poisson?,pedigree=ped,data=data,prior=prior1.1,nitt=100000,thin=75,burnin=25000,verbose=FALSE)

then

posterior.heritability1.1<-model1.1$VCV[,"animal"]/(model1.1$VCV[,"animal"]+model1.1$VCV[,"units"]) 

to calculate heritability


Please can someone comment on whether they think these priors and
methods of estimating heritability are OK?


Please can someone also tell me whether the heritability estimate from
the ordinal model means the same thing as a heritability estimated from
a model in which the residual variance is not fixed?


Finally, I would like to add a random factor (Year of birth) to both
models. Please can someone suggest how I might alter the prior in the
ordinal toes model to do so?


Many (many) thanks,
Helen


From luke.mangaliso.duncan at gmail.com  Tue Sep 25 21:38:35 2012
From: luke.mangaliso.duncan at gmail.com (Luke Duncan)
Date: Tue, 25 Sep 2012 21:38:35 +0200
Subject: [R-sig-ME] 'Missing' tests for lmer models
Message-ID: <CAE9UE+9pXiJcaCkJg+a7Bnv_QUyk-Vs01j34G-4ut_kzOOG=+w@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120925/0d0de1bc/attachment.pl>

From hans at sociologi.cjb.net  Tue Sep 25 23:20:41 2012
From: hans at sociologi.cjb.net (Hans Ekbrand)
Date: Tue, 25 Sep 2012 23:20:41 +0200
Subject: [R-sig-ME] Interpreting the output of summary() of a
 glmer-object
In-Reply-To: <loom.20120925T035248-650@post.gmane.org>
References: <20120924141611.GA4189@samir>
	<loom.20120925T035248-650@post.gmane.org>
Message-ID: <20120925212041.GA5783@samir>

On Tue, Sep 25, 2012 at 02:04:19AM +0000, Ben Bolker wrote:
> Hans Ekbrand <hans at ...> writes:
> 
> > First, I have a very simple question. In the summary output of a
> > glmer-object, What does the "Variance" and "Std.Dev" mean for the
> > Random effects? What is the scale for these measures?
> 
>   It's a little hard to think of a way to say this that doesn't
> seem redundant ... "Variance" is the estimated variance of the
> random effects, "Std.Dev" is the standard deviation (i.e. the
> square root of the variance -- these quantities give redundant
> information; seeing the variance can be useful because of the
> additivity of variances and the traditional presentation of
> mixed models in terms of variance decomposition, while the
> standard deviation can be useful because it is on the same scale
> as the estimated fixed-effect coefficients).  The scale is the
> same as the scale of the fixed-effect coefficients, i.e. the
> scale of the linear predictor.

Thanks alot Ben for taking the time to explain the basics, it really
helps me!

> > 
> > load(url("http://sociologi.cjb.net/temp/a.strange.df.RData"))
> > my.fit.1 <- glmer(MV744A ~ (1|MV024), 
> >    data = a.strange.df, family = "binomial")
> > summary(my.fit.1)
> > 
> > Generalized linear mixed model fit by the Laplace approximation 
> > Formula: MV744A ~ (1 | MV024) 
> >    Data: a.strange.df 
> >    AIC   BIC logLik deviance
> >  76209 76227 -38102    76205
> > Random effects:
> >  Groups Name        Variance Std.Dev.
> >  MV024  (Intercept) 0.40558  0.63685 
> > Number of obs: 73601, groups: MV024, 29
> > 
> > Fixed effects:
> >             Estimate Std. Error z value Pr(>|z|)    
> > (Intercept)  -1.4187     0.1191  -11.91   <2e-16 ***
> > ---
> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 


So, if I understand you - which I think I do - then 0.63685 is simply the standard deviation of ranef(my.fit.1)?

When I try to compute that manually I get a numerically close figure, but not quite the same:

sd(unlist(ranef(my.fit.1)))
[1] 0.6423346

I am on the right track?


From David.Duffy at qimr.edu.au  Tue Sep 25 23:21:13 2012
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Wed, 26 Sep 2012 07:21:13 +1000
Subject: [R-sig-ME] 'Missing' tests for lmer models
In-Reply-To: <CAE9UE+9pXiJcaCkJg+a7Bnv_QUyk-Vs01j34G-4ut_kzOOG=+w@mail.gmail.com>
References: <CAE9UE+9pXiJcaCkJg+a7Bnv_QUyk-Vs01j34G-4ut_kzOOG=+w@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.1209260717440.6759@orpheus.qimr.edu.au>

You need to provide boring details like "sessionInfo()" on old and new 
machines, so we can see which versions of the packages you actually have 
installed, and whether your locale is weird...


From bbolker at gmail.com  Tue Sep 25 23:43:48 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 25 Sep 2012 21:43:48 +0000 (UTC)
Subject: [R-sig-ME] Interpreting the output of summary() of a
	glmer-object
References: <20120924141611.GA4189@samir>
	<loom.20120925T035248-650@post.gmane.org>
	<20120925212041.GA5783@samir>
Message-ID: <loom.20120925T234007-68@post.gmane.org>

Hans Ekbrand <hans at ...> writes:

> 
> On Tue, Sep 25, 2012 at 02:04:19AM +0000, Ben Bolker wrote:
> > Hans Ekbrand <hans at ...> writes:
> > 

[snip]

> > > 
> > > load(url("http://sociologi.cjb.net/temp/a.strange.df.RData"))
> > > my.fit.1 <- glmer(MV744A ~ (1|MV024), 
> > >    data = a.strange.df, family = "binomial")
> > > summary(my.fit.1)
> > > 

[snip]

> > > Random effects:
> > >  Groups Name        Variance Std.Dev.
> > >  MV024  (Intercept) 0.40558  0.63685 
> > > Number of obs: 73601, groups: MV024, 29
> > > 

[snip]

> 
> So, if I understand you - which I think I do - then 0.63685 is 
> simply the standard deviation of ranef(my.fit.1)?
> 
> When I try to compute that manually I get a numerically 
> close figure, but not quite the same:
> 
> sd(unlist(ranef(my.fit.1)))
> [1] 0.6423346
> 
> I am on the right track?
> 

  I think this has been discussed previously on the list (although
I can't point you to a precise spot), but: 

http://permalink.gmane.org/gmane.comp.lang.r.lme4.devel/8580

says

"Generally you will find that the variance of the values returned by
ranef is less than the estimated variance because of shrinkage."

  Sorry I can't go into  more detail right now.


From hans at sociologi.cjb.net  Tue Sep 25 23:55:11 2012
From: hans at sociologi.cjb.net (Hans Ekbrand)
Date: Tue, 25 Sep 2012 23:55:11 +0200
Subject: [R-sig-ME] Interpreting the output of summary() of a
 glmer-object
In-Reply-To: <loom.20120925T234007-68@post.gmane.org>
References: <20120924141611.GA4189@samir>
	<loom.20120925T035248-650@post.gmane.org>
	<20120925212041.GA5783@samir>
	<loom.20120925T234007-68@post.gmane.org>
Message-ID: <20120925215511.GA5805@samir>

On Tue, Sep 25, 2012 at 09:43:48PM +0000, Ben Bolker wrote:
> Hans Ekbrand <hans at ...> writes:

> > So, if I understand you - which I think I do - then 0.63685 is 
> > simply the standard deviation of ranef(my.fit.1)?
> > 
> > When I try to compute that manually I get a numerically 
> > close figure, but not quite the same:
> > 
> > sd(unlist(ranef(my.fit.1)))
> > [1] 0.6423346
> > 
> > I am on the right track?
> > 
> 
>   I think this has been discussed previously on the list (although
> I can't point you to a precise spot), but: 
> 
> http://permalink.gmane.org/gmane.comp.lang.r.lme4.devel/8580
> 
> says
> 
> "Generally you will find that the variance of the values returned by
> ranef is less than the estimated variance because of shrinkage."
> 
>   Sorry I can't go into  more detail right now.

No need to, but thanks for the pointer. I take that it's not *simply*
the standard deviation, but it is the standard deviation :-)


From David.Duffy at qimr.edu.au  Wed Sep 26 01:52:24 2012
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Wed, 26 Sep 2012 09:52:24 +1000
Subject: [R-sig-ME] Fwd: Priors for and estimating heritability from an
	ordinal and a Poisson animal model
In-Reply-To: <5061C19C.3050409@qmul.ac.uk>
References: <5048BDAB.3090103@qmul.ac.uk> <5061C19C.3050409@qmul.ac.uk>
Message-ID: <Pine.LNX.4.64.1209260747540.6759@orpheus.qimr.edu.au>

On Tue, 25 Sep 2012, Helen Ward wrote:

> Please can someone also tell me whether the heritability estimate from
> the ordinal model means the same thing as a heritability estimated from
> a model in which the residual variance is not fixed?

Looks OK from here, except maybe nu in prior2.  It is probably 
worthwhile simulating some data from what you think is a sensible model 
and running these jobs, as well as tinkering with the priors, and seeing 
what effect that has.  I don't know, for instance, why age at first 
reproduction should be poisson (you could show your conclusions are robust 
to misspecification if the true model is something else).

As a check for "toes", at least, you can calculate the intralitter 
polychoric correlation (I presume you have litters), by producing all
possible sibling pairs, and using the polycor package eg

library(polycor)
litter <- paste(data$sire, data$dam, sep="x")
litter[litter == "NAxNA"] <- NA
# phenotype and sibship indicator
litter.data <- data.frame(litter=litter, trait=data$trait)
litter.data <- litter.data[complete.cases(litter.data),]
# keep those with at least two members
litter.sizes <- as.data.frame(table(litter.data$litter))
useful <- litter.sizes$Var1[litter.sizes$Freq > 1]
litter.data <- litter.data[litter.data$litter %in% useful, ]
# set up for combn() to extract all pairs of siblings
litter.data$litter <- factor(litter.data$litter)
litter.data$trait <- as.character(litter.data$trait)
allpairs <- sapply(split(litter.data$trait, litter.data$litter),
                    function(x) combn(x,2))
allpairs <- as.data.frame(matrix(unlist(unlist(allpairs)), nc=2, byrow=T))
# finally call polychor() to estimate sibling intraclass correlation
names(allpairs) <- c("sib1", "sib2")
polychor(c(allpairs$sib1, allpairs$sib2), c(allpairs$sib2, allpairs$sib1))


You could extend this to other classes of relative and make it bivariate.

Just 2c, David Duffy.


From bonamy at horus.ens.fr  Wed Sep 26 09:28:03 2012
From: bonamy at horus.ens.fr (Pierre B. de Villemereuil)
Date: Wed, 26 Sep 2012 09:28:03 +0200
Subject: [R-sig-ME] Priors for and estimating heritability from an
 ordinal and a Poisson animal model
In-Reply-To: <5048BDAB.3090103@qmul.ac.uk>
References: <5048BDAB.3090103@qmul.ac.uk>
Message-ID: <5062AE83.9070604@horus.ens.fr>

(I thought I responded to this e-mail a while ago... but my response 
might have not leave my computer !)

Concerning the priors, two things :
- For the first model, it is not very clear, but I assume you use 
"deformed toes" as a binary variable. In that case, a better prior would 
be the chi2 distribution. You can have it by using the following prior :

prior2=list(R=list(V=1, fix=1), G=list(G1=list(V=1, nu=1000, alpha.mu=0,alpha.V=1)))

This prior is a bit skewed toward little heritabilities, but the one you 
used is actually skewed toward strong heritabilities.
- For the second model, what is "p.var" ? If it is the phenotypic 
variance calculated from the data, you should not use it. It is 
ill-advised to use the data under investigation to set up the prior. I 
think you try there to use your first prior or the following one :

prior1.1<-list(G=list(G1=list(V=1,n=0.002)),R=list(V=1,n=0.002))

I hope this is answering some of your questions. Don't hesitate to check 
different priors to see if your posterior distribution is influenced or 
not by the prior.

Also, normally, the heritability being an intra-class coefficient, it 
should be independant from the fact that the residual variance is fixed. 
You should check the MCMCglmm Course Notes from Jarrod Hadfield 
concerning this point.

Finally, to add the random factor, using the chi2 distribution, you 
should just use :

prior2=list(R=list(V=1, fix=1), G=list(G1=list(V=1, nu=1000, alpha.mu=0,alpha.V=1),
G2=list(V=1, nu=1000, alpha.mu=0,alpha.V=1)))

This would not alter much the shape of the prior on the heritability.

Cheers,
Pierre.


Le 06/09/2012 17:13, Helen Ward a ?crit :
> Hello everybody,
>
> I've just come back to some models I was working on a while ago and was
> hoping someone could help me clear up a couple of issues. My goal is to
> calculate the heritability of two independent traits: number of deformed
> toes, and age of 1st reproduction.
>
> Number of deformed toes is a data set of integers ranging from 0 to 6:
> it looks most like a zero-inflated Poisson distribution.
> Age of 1st reproduction is a data set of integers ranging from 0
> upwards: it has a clear Poisson distribution.
>
> I have built animal models for these traits in ASReml specifying that
> both data sets are Poisson distributed. However, I cannot compare my
> heritability estimates with those estimated for other Gaussian traits as
> in a Poisson model in ASReml the residual variance is fixed at 1.
>
> I am now trying to model these traits in MCMCglmm, deformed toes in an
> ordinal model and age of 1st repro in a Poisson model.
>
> At the moment for deformed toes I am using
>
> prior2=list(R=list(V=1, fix=1), G=list(G1=list(V=1, nu=1,
> alpha.mu=0,alpha.V=100)))
>
> model1<-MCMCglmm(Toes~1,random=~animal,family="ordinal",pedigree=ped,data=data,prior=prior2,nitt=500000,thin=300,burnin=300000,verbose=FALSE,pl=TRUE)
> then
>
> posterior.heritability1<-model1$VCV[,"animal"]/(model1$VCV[,"animal"]+model1$VCV[,"units"]+1)to
> calculate heritabilty.
>
> For age of 1st repro I am using
>
> prior1.1<-list(G=list(G1=list(V=matrix(p.var/2),n=1)),
> R=list(V=matrix(p.var/2),n=1))
>
> model1.1<-MCMCglmm(Poisson1strepro~1,random=~animal,family="poisson",pedigree=ped,data=data,prior=prior1.1,nitt=100000,thin=75,burnin=25000,verbose=FALSE)
>
> then
>
> posterior.heritability1.1<-model1.1$VCV[,"animal"]/(model1.1$VCV[,"animal"]+model1.1$VCV[,"units"])
> to calculate heritability
>
>
> Please can someone comment on whether they think these priors and
> methods of estimating heritability are OK?
>
>
> Please can someone also tell me whether the heritability estimate from
> the ordinal model means the same thing as a heritability estimated from
> a model in which the residual variance is not fixed?
>
>
> Finally, I would like to add a random factor (Year of birth) to both
> models. Please can someone suggest how I might alter the prior in the
> ordinal toes model to do so?
>
>
> Many (many) thanks,
> Helen
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From ckluss at email.uni-kiel.de  Wed Sep 26 14:59:57 2012
From: ckluss at email.uni-kiel.de (=?ISO-8859-15?Q?Christof_Klu=DF?=)
Date: Wed, 26 Sep 2012 14:59:57 +0200
Subject: [R-sig-ME] nlme-function: Step halving factor reduced below minimum
 in PNLS step
Message-ID: <k3uu8d$mg6$1@ger.gmane.org>

Hi

can I avoid the error "Step halving factor reduced below minimum in PNLS
step" by setting a control parameter to a lower value?

If so, which one?

thx
Christof


From jone2093 at umn.edu  Thu Sep 27 03:51:22 2012
From: jone2093 at umn.edu (Stephen Jones)
Date: Wed, 26 Sep 2012 20:51:22 -0500
Subject: [R-sig-ME] Seemingly Unrelated Regression in lme4
In-Reply-To: <CAKz8HvngVDhV_V75pgtGRm4TEbF20=pZDLUDYnii=YGCFESiEA@mail.gmail.com>
References: <CAObg2BHGjH-kEXXdqEtLb4QfoShKrHc2jmadS=N-stdY=GNzNQ@mail.gmail.com>
	<CAKz8HvngVDhV_V75pgtGRm4TEbF20=pZDLUDYnii=YGCFESiEA@mail.gmail.com>
Message-ID: <CAObg2BHDgBFD7DQCKsuHtxkxZTmW2Pn-rSg4Lw0NMAZRmt9NBA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120926/538d3d05/attachment.pl>

From bates at stat.wisc.edu  Thu Sep 27 19:37:21 2012
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 27 Sep 2012 12:37:21 -0500
Subject: [R-sig-ME] questions about the nlme model in R
In-Reply-To: <3C6DAC79F44AE248924E55FB812A51AC020672A1F7A5@E2MBX02.west.esurance.com>
References: <3C6DAC79F44AE248924E55FB812A51AC020672A1F7A5@E2MBX02.west.esurance.com>
Message-ID: <CAO7JsnTM-HooZwvZVVot9k4e4X7vH9eoZKfZcrP-dz1UfEtZHg@mail.gmail.com>

On Tue, Sep 25, 2012 at 10:09 AM, Margo Law <MLaw at esurance.com> wrote:
> Hello.

> My name is Margo Law and I am new to programming in the R language.  I have
> been working within the R Studio and on the Nonlinear Mixed-Effects Models R
> documentation your name and email appear as an author.

As I indicated in our telephone conversation, it is better to send
such questions to the R-SIG-Mixed-Models at R-project.org mailing list as
several of the readers on that list can respond to you question and
often much sooner than I am able to.

> I am wondering if you would help me.  I have a data set that consists of:
>
> Daily date (each day from 2008-2012)
>
> Weekday (a factor with 7 levels)
>
> Input value
>
> Output value
>
> A variable for each individual holiday (for example new.years.day 1 for
> ?yes? and 0 for ?no?)
>
>
>
> I?m trying to run the nlme model on input and output values I have (known
> values) with ultimately using this to predict unknown output values.
>
>
>
>
>
> The program I am inputting is:
>
>
>
> model<-nlme(output~input+new.years.day+new.years.day.observed+mlk+valentines+presidents.day+
> easter+mothers.day+
>
>
> memorial.day+fathers.day+july4+july4.observed+labor.day+columbus.day+veterans.day+
>
>
> veterans.day.observed+thanksgiving.day+christmas.observed+christmas.day+christmas.eve+new.years.eve,
>
>                       data= input.dataset,
>
>
> fixed=input+new.years.day+new.years.day.observed+mlk+valentines+presidents.day+easter+
>
>
> mothers.day+memorial.day+fathers.day+july4+july4.observed+
>
>
> labor.day+columbus.day+veterans.day+veterans.day.observed+thanksgiving.day+
>
>
> christmas.observed+christmas.day+christmas.eve+new.years.eve~1,
>
>                       random=input~1|Weekday,
>
>
> start=c(0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1))

Well, first that isn't a nonlinear mixed-effects model, as far as I
can see.  You seem to be using a linear model formula as the first
argument, not a nonlinear model.  That formula will not be interpreted
as a linear model and you won't get the result that you expect.

If you do want to fit a linear mixed-effects model you would use lme
from the nlme package or lmer from the lme4 package.  However, both of
them are unlikely to be able to fit this model because you have so
many fixed-effects terms.

> When I run this in R I get the following error:
>
>
>
> Error in MEEM(object, conLin, control$niterEM) :
>
>   Singularity in backsolve at level 0, block 1
>
>
>
>
>
>
>
> I have looked everywhere I can find, but I have no idea what this error
> means or is referring to in order to know how to go about fixing it.
>
>
>
>
>
> My question is could you explain to me what this error means and what I
> would then need to do to go about fixing this?  Also would it be possible to
> provide a broader explanation of the nlme model, in particular how each
> individual piece works (if I?m saying that right so that it makes sense).
>
>
>
>
>
> Any help you could provide is greatly appreciated.
>
> Thank you so much.
>
>
>
>
>
> Margo Law
>
>
>
>
> This email may contain confidential and privileged inf...{{dropped:10}}


From MLaw at esurance.com  Thu Sep 27 21:43:44 2012
From: MLaw at esurance.com (Margo Law)
Date: Thu, 27 Sep 2012 12:43:44 -0700
Subject: [R-sig-ME] questions about the nlme model in R
Message-ID: <3C6DAC79F44AE248924E55FB812A51AC020672A1F880@E2MBX02.west.esurance.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120927/5bbef8bf/attachment.pl>

From bates at stat.wisc.edu  Thu Sep 27 22:55:30 2012
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 27 Sep 2012 15:55:30 -0500
Subject: [R-sig-ME] Fwd: cross-classified random effects model R code for
	empirical bayes
In-Reply-To: <CAO7JsnRceQHSNpb31zxom-cVdTczd0EWRnfCzn9NNSW+w8iuWQ@mail.gmail.com>
References: <1348778337.70253.YahooMailNeo@web121105.mail.ne1.yahoo.com>
	<CAO7JsnRceQHSNpb31zxom-cVdTczd0EWRnfCzn9NNSW+w8iuWQ@mail.gmail.com>
Message-ID: <CAO7JsnSKq19xoPZFJ0rNThyuc-VYQW+sU_0admxvDOv_4jsDRg@mail.gmail.com>

I forgot to cc: the list on this reply.


---------- Forwarded message ----------
From: Douglas Bates <bates at stat.wisc.edu>
Date: Thu, Sep 27, 2012 at 3:54 PM
Subject: Re: cross-classified random effects model R code for empirical bayes
To: Webster Kasongo <kasongster at yahoo.com>


It is better to send questions like this to the
R-SIG-Mixed-Models at R-project.org mailing list (see
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models for more
information) than to me directly.  Several of those who read that list
can respond to you and often do so much more quickly than I am able
to.

On Thu, Sep 27, 2012 at 3:38 PM, Webster Kasongo <kasongster at yahoo.com> wrote:
> Dear Dr. Bates
> I am womdering whether there is a way of specifying the parameter estimation
> method for empirical bayes method
> e.g.,
> summary(fit.T2 <-lmer(wordsum ~ race+GENDER+ age+ I(age^2)+ educ+
> (1|PERIOD)+(1|COHORT), data=GSSBCFINAL, REML=FALSE))
>
> In the above code, estimation method is ML.

> How can one estimate emprical
> bayesian method?

A facetious answer would be "learn to program in R".  :-)

I don't know of any R packages that provide empirical Bayes estimates.
 In fact, I'm not sure that the name "empirical Bayes" is sufficient
to define a particular estimation method.  I think it refers to a
general approach and you would need to be more specific about the
criterion.


From bates at stat.wisc.edu  Thu Sep 27 23:19:01 2012
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 27 Sep 2012 16:19:01 -0500
Subject: [R-sig-ME] nlme random effect
In-Reply-To: <342dd048.c9f1.13a07ca78b9.Coremail.lydia1011@126.com>
References: <342dd048.c9f1.13a07ca78b9.Coremail.lydia1011@126.com>
Message-ID: <CAO7JsnSf1dhGQoN2MK64SPO-8GNPcrrjVnfR7h9LS9cXzts3vg@mail.gmail.com>

Generally it is best to send questions like this to the
R-SIG-Mixed-Models at R-project.org mailing list (see instructions at
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models).  Many of
those who read that list can reply and often much faster than I am
able to.


On Thu, Sep 27, 2012 at 7:54 AM, lydia <lydia1011 at 126.com> wrote:
> Hello,
> I want build a nlme model with random effect like following formula.
> ?=??*exp(?), where ?? is  population mean and ? is the random effect, How
> should I realize it? Are there any examples?

I think we would need a bit more information to be able to respond.

> ps. I'm building a pharmacodynamic model and I can only find random effect
> in linear form. I want to try orther forms.
>
> Thanks a lot. I'm looking forward to your reply.
>
> Best Regards
>
> Lydia
>
>


From kasongster at yahoo.com  Thu Sep 27 23:16:56 2012
From: kasongster at yahoo.com (Webster Kasongo)
Date: Thu, 27 Sep 2012 14:16:56 -0700 (PDT)
Subject: [R-sig-ME] cross-classified random effect model
Message-ID: <1348780616.58165.YahooMailNeo@web121101.mail.ne1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120927/047f5b05/attachment.pl>

From webster.kasongo at Vanderbilt.Edu  Thu Sep 27 23:58:33 2012
From: webster.kasongo at Vanderbilt.Edu (Kasongo, Webster)
Date: Thu, 27 Sep 2012 21:58:33 +0000
Subject: [R-sig-ME] cross classified random effect model
Message-ID: <EC5593B1ED11B940B648821089E06133060CB5@ITS-HCWNEM108.ds.vanderbilt.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120927/87bf3714/attachment.pl>

From ckluss at email.uni-kiel.de  Fri Sep 28 17:53:41 2012
From: ckluss at email.uni-kiel.de (=?ISO-8859-15?Q?Christof_Klu=DF?=)
Date: Fri, 28 Sep 2012 17:53:41 +0200
Subject: [R-sig-ME] nlme vs nlmer
Message-ID: <k44h67$tcp$1@ger.gmane.org>

Hi

can you give me a hint how to convert a nlme(..) like

nlme(Y ~ fun(X,a,b,c),
     fixed  = list(a ~ 1, b ~ 1, c ~ 1),
     random = list(a ~ 1, b ~ 1, c ~ 1),
     groups = ~ location,
     data   = measurements,
     start  = c(a = 350, b = 0.2, c = 120))

to an equivalent nlmer(...)? The following seems to be wrong

nlmer(Y ~ fun(X,a,b,c) ~ (a  + b + c | location),
              data = measurements,
              start = c(a = 350, b = 0.2, c = 120))

it throws "gradient attribute of evaluated model must be a numeric matrix"

thx
Christof


From ims203 at exeter.ac.uk  Fri Sep 28 17:31:49 2012
From: ims203 at exeter.ac.uk (Stott, Iain)
Date: Fri, 28 Sep 2012 15:31:49 +0000
Subject: [R-sig-ME] MCMCglmm: correctly estimating phylogenetic heritability
Message-ID: <B964ED738030624680E9A6D3182E934F049836@VMEXCHANGEMBS5A.isad.isadroot.ex.ac.uk>

Dear all,

I am working with MCMCglmm models that include a phylogenetic component. The aim of the study is to estimate the influence of phylogeny on certain aspects of demography in plant populations.

The models are constructed as follows: 
Univariate response
One fixed factor (categorical, four levels)
Two nested random grouping factors (Population nested within Species)
Phylogenetic scaling of random effects (using a 'phylo' object and the 'animal' argument)
Default uninformative priors for fixed effects (mu=0, sigma^2=10^10)
Uninformative priors for random effects and residuals (V=1, nu=0.001)
Parameter expansion for random effects priors (I had some problems with chains sticking at 0)

DIC from models with phylogeny vs. without phylogeny are very similar, hence there are two separate sets of rival models. The only discernable effect that phylogeny has is to increase the credible intervals on the fixed effect posteriors.

Estimating a heritability factor (i.e. something akin to Pagel's lambda) could clear up how important phylogeny actually is. Hadfield & Nakagawa (2010) in J. Evol. Biol. state that this can be found using:

var(phylo)/(var(phylo)+var(residual)) .

Fitted variance for phylogeny is very small compared to species, population and residuals which could explain some of the strange results we're getting: phylogeny may be statistically important, but have a very small effect. BUT, the variance attributed to phylogeny seems to be partitioned primarily into Species in non-phylogenetic models (which makes sense). SO, is the above correct or should we actually be using something like:

var(phylo)/(var(phylo)+var(residual)+var(random effects)) ?


Hope someone can shed some light,

Iain 



- - - - - - - - - - - -
Dr. Iain Stott
Centre for Ecology and Conservation
University of Exeter, Cornwall Campus
Tremough, Treliever Road
Penryn, Cornwall, TR10 9EZ, UK.
Tel (office): 01326 371852
http://biosciences.exeter.ac.uk/staff/postgradresearch/iainstott/
- - - - - - - - - - - -



From Freedom.Gumedze at uct.ac.za  Sat Sep 29 11:00:04 2012
From: Freedom.Gumedze at uct.ac.za (Freedom Gumedze)
Date: Sat, 29 Sep 2012 11:00:04 +0200
Subject: [R-sig-ME] gls or lme with unstructured covariance martrix
In-Reply-To: <EC5593B1ED11B940B648821089E06133060CB5@ITS-HCWNEM108.ds.vanderbilt.edu>
References: <EC5593B1ED11B940B648821089E06133060CB5@ITS-HCWNEM108.ds.vanderbilt.edu>
Message-ID: <5066D4B4020000EE000B5216@gwiasmtp.uct.ac.za>

Dear R mixed modellers
 
I am trying to fit a gls or mixed model to meta analysis data,
specifically summary survival curve data. 
The response variable consists of survival proportions for each trial
at different time points (with associated standard errors).
Since the variances are known I fit the following model using gls:
 
> y1=prop/100
> varest=(se/100)^2
> library(nlme)
> mod <- gls(y1~1,weights=varFixed(~varest))
> summary(mod)
Generalized least squares fit by REML
  Model: y1 ~ 1 
  Data: NULL 
       AIC     BIC    logLik
  118.5856 124.336 -57.29279
 
Variance function:
 Structure: fixed weights
 Formula: ~varest 
 
Coefficients:
                Value  Std.Error  t-value p-value
(Intercept) 0.3839122 0.02509093 15.30083       0
 
Standardized residuals:
       Min         Q1        Med         Q3        Max 
-2.5493065 -0.5069545 -0.0253402  0.5264015  3.4182227 
 
Residual standard error: 6.050427 
Degrees of freedom: 132 total; 131 residual
> 
The problems with above model are
 
(i) It assumes the errors are uncorrelated, the covariance matrix for
the errors V is diagonal with variances for each proportion on the
diagonals. How can I allows the errors to be correlated?
(ii) The proportions from the same trial are independent. 
 
Pinheiro and Bates (in their book) suggest that one can account for
both heteroscedasticity in the  errors and correlation between trial
measurements. However, the heteroscedasticity of the errors assume
independence.
 
I tried the following model but is it correct?
 
study=as.factor(trial)
mod2 <-
gls(y1~1,correlation=corSymm(form=~1|study),weights=varFixed(~varest))
 
I would also welcome advice on fitting this model as a linear mixed
model but assuming the errors known but correlated.
 
kind regards,
Freeedom




###

UNIVERSITY OF CAPE TOWN 

This e-mail is subject to the UCT ICT policies and e-mail disclaimer
published on our website at
http://www.uct.ac.za/about/policies/emaildisclaimer/ or obtainable from
+27 21 650 9111. This e-mail is intended only for the person(s) to whom
it is addressed. If the e-mail has reached you in error, please notify
the author. If you are not the intended recipient of the e-mail you may
not use, disclose, copy, redirect or print the content. If this e-mail
is not related to the business of UCT it is sent by the sender in the
sender's individual capacity.

###
 

From rjeffries at ucla.edu  Sun Sep 30 00:17:33 2012
From: rjeffries at ucla.edu (Robin Jeffries)
Date: Sat, 29 Sep 2012 15:17:33 -0700
Subject: [R-sig-ME] Sampling methods for MCMCglmm using cengaussian family
Message-ID: <CAMt7gbP3Kbcvou1NM6z-X=71v_jrON1Gk8OPw2jY+nMjRPL-hg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120929/88c91370/attachment.pl>

From elofgren at email.unc.edu  Sun Sep 30 11:20:13 2012
From: elofgren at email.unc.edu (Lofgren, Eric)
Date: Sun, 30 Sep 2012 09:20:13 +0000
Subject: [R-sig-ME] Two Questions re: Piecewise Mixed Effects Models
Message-ID: <BF2CE7102E190B4FABEED85AB38539C403BB6C90@ITS-MSXMBS3M.ad.unc.edu>

I'm relatively new to mixed effects models, and have two fairly basic questions (I think) about a project I'm working on.

Essentially, I'm working on a poisson regression problem where we're trying to estimate the impact of a change in a policy on the rate of an event X. There are N sites where this change has taken place, and we also happen to know another characteristic of each site Z. It's been suggested that I look a a piecewise mixed effects model (which I've also seen referred to as an interrupted time series or "broken stick" model).

I've got two questions about this type of model:

1. As I understand it, you should have two time variables, a "Time Before" and a "Time After" variable, each of which equals 0 right at the point of the policy change. My question is what these variables should look like, specifically what they should look like after they reach zero. For example, with a simple series of 5 points and the policy change in the middle, should it look like this:

Time Before: 2, 1, 0, NA, NA
Time After: NA, NA, 0, 1, 2

or this?

Time Before: 2, 1, 0, -1, -2
TIme After: -2, -1, 0, 1, 2

Or something else entirely?

2. In terms of using something like glmer() to fit the model itself, I'm a little confused about the syntax (I normally use SAS, but am trying to get better at using R). Following some advice I've seen, I should be using something like this:

model <- glmer( rate ~ policy + time.before + time.after + (1+ time.before + time.after + Z |Site) + offset(log(people)), data=data, family="poisson")

First, is this correct? Second, what's accomplished by having the time variables in both the fixed and random effects sections - what does that produce? A fixed slope and random intercept? Something else?

Thanks,

Eric

From rubem_ceratti at yahoo.com.br  Sun Sep 30 17:43:54 2012
From: rubem_ceratti at yahoo.com.br (Rubem Kaipper Ceratti)
Date: Sun, 30 Sep 2012 08:43:54 -0700 (PDT)
Subject: [R-sig-ME] Subject-wise log-likelihood gradient and hessian
Message-ID: <1349019834.24153.YahooMailNeo@web160702.mail.bf1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120930/853fb2b1/attachment.pl>

From jwiley.psych at gmail.com  Sun Sep 30 18:08:05 2012
From: jwiley.psych at gmail.com (Joshua Wiley)
Date: Sun, 30 Sep 2012 09:08:05 -0700
Subject: [R-sig-ME] Sampling methods for MCMCglmm using cengaussian
	family
In-Reply-To: <CAMt7gbP3Kbcvou1NM6z-X=71v_jrON1Gk8OPw2jY+nMjRPL-hg@mail.gmail.com>
References: <CAMt7gbP3Kbcvou1NM6z-X=71v_jrON1Gk8OPw2jY+nMjRPL-hg@mail.gmail.com>
Message-ID: <CANz9Z_J3d2W6MHibLWJ5ekruEWWAd_gJiMijPittGsBsme_akg@mail.gmail.com>

Hi Robin,

Jarrod may be along at some point with a more definitive answer but:

If you have not already, you should read:
http://cran.r-project.org/web/packages/MCMCglmm/vignettes/CourseNotes.pdf
They are excellent and I think you may find Chapter 7 informative.

Of course you have a posterior distribution for latent scores.  You
always will, although if you had no random effects other than
residuals and all observed noncensored gaussian, it would be a
prodigiously boring posterior with no variability across draws, but
there all the same.  In your case, for censored cases it will vary by
draw.

To answer your questions.

1) I am not sure.
2) Yes, MH is being used.  Again the technical details on the course
notes provide some more information on sampling schemes.
3) As far as I can tell you cannot.  You can set it via the tune
argument.  You set NULL so the default start is used


 if (is.null(tune)) {
        AMtune = c(rep(FALSE, nG), rep(TRUE, nR))
        for (i in 1:nR) {
            tune[[i]] = diag(nfl[nG + i])
        }
    }

this is passed to .C, but as near as I can tell, is not returned from
the R MCMCglmm() function.  So it would seem you do not extract it.  A
quick poke around MCMCglmm.cc, suggests it is the 34th
argument---probing deeper there may get you some traction if you
really want.

Side comment, you are using small variances on the prior for your
fixed effects.  Just wanted to point it out if not intentional.

Cheers,

Josh


On Sat, Sep 29, 2012 at 3:17 PM, Robin Jeffries <rjeffries at ucla.edu> wrote:
> Hello,
>
> I am using MCMCglmm to model a time to event outcome where individuals can
> be either left or right censored so I am using the "cengaussian" family. I
> have successfully run the model
>
> test <- MCMCglmm(cbind(low, up) ~ g0 + age0,
>          family="cengaussian", data=sex, tune=NULL,
>          prior = list(R = list(V=1, n=0.002),
>                       B = list(mu=rep(0,3), V=diag(3))))
>
> Looking at the form of the density function from the MCMC Overview
> documentation, I don't see liabilities. Combine that with having no random
> effects, leads me to think that the program would use Gibbs sampling.
>
> However, when I add a "pl=TRUE" to the above code, I get a posterior
> distribution of liabilities, and the program displays the acceptance ratio
> for latent scores (this is displayed regardless of the value of pl). Both
> seem to indicate that MH sampling is being used.
>
> So my questions are
> 1) Where are the liabilities being used in the density?
> 2) Does the display of the acceptance ratio for latent scores definitively
> mean that MH sampling is being used?
> 3) If MH is being used, how can I extract the covariance matrix for the
> proposal distribution that was used?
>
>
> Thank you,
> -Robin
>
>
> Robin Jeffries
> MS, DrPH Candidate
> Department of Biostatistics,
> UCLA
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



-- 
Joshua Wiley
Ph.D. Student, Health Psychology
Programmer Analyst II, Statistical Consulting Group
University of California, Los Angeles
https://joshuawiley.com/


From jones at reed.edu  Sun Sep 30 19:40:50 2012
From: jones at reed.edu (Albyn Jones)
Date: Sun, 30 Sep 2012 10:40:50 -0700
Subject: [R-sig-ME] Two Questions re: Piecewise Mixed Effects Models
In-Reply-To: <BF2CE7102E190B4FABEED85AB38539C403BB6C90@ITS-MSXMBS3M.ad.unc.edu>
References: <BF2CE7102E190B4FABEED85AB38539C403BB6C90@ITS-MSXMBS3M.ad.unc.edu>
Message-ID: <20120930174050.GC25074@reed.edu>

The first question has nothing to do with mixed effects per se.
your "time before" and "time after" are perfectly colinear.

  > Time Before: 2, 1, 0, -1, -2
  > TIme After: -2, -1, 0, 1, 2

The version with NA's will drop all observations except at time 0.

I think you want something like

  Time Before: -2, -1, 0, 0, 0
  TIme After:   0,  0, 0, 1, 2

albyn

On Sun, Sep 30, 2012 at 09:20:13AM +0000, Lofgren, Eric wrote:
> I'm relatively new to mixed effects models, and have two fairly basic questions (I think) about a project I'm working on.
> 
> Essentially, I'm working on a poisson regression problem where we're trying to estimate the impact of a change in a policy on the rate of an event X. There are N sites where this change has taken place, and we also happen to know another characteristic of each site Z. It's been suggested that I look a a piecewise mixed effects model (which I've also seen referred to as an interrupted time series or "broken stick" model).
> 
> I've got two questions about this type of model:
> 
> 1. As I understand it, you should have two time variables, a "Time Before" and a "Time After" variable, each of which equals 0 right at the point of the policy change. My question is what these variables should look like, specifically what they should look like after they reach zero. For example, with a simple series of 5 points and the policy change in the middle, should it look like this:
> 
> Time Before: 2, 1, 0, NA, NA
> Time After: NA, NA, 0, 1, 2
> 
> or this?
> 
> Time Before: 2, 1, 0, -1, -2
> TIme After: -2, -1, 0, 1, 2
> 
> Or something else entirely?
> 
> 2. In terms of using something like glmer() to fit the model itself, I'm a little confused about the syntax (I normally use SAS, but am trying to get better at using R). Following some advice I've seen, I should be using something like this:
> 
> model <- glmer( rate ~ policy + time.before + time.after + (1+ time.before + time.after + Z |Site) + offset(log(people)), data=data, family="poisson")
> 
> First, is this correct? Second, what's accomplished by having the time variables in both the fixed and random effects sections - what does that produce? A fixed slope and random intercept? Something else?
> 
> Thanks,
> 
> Eric
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 

-- 
Albyn Jones
Reed College
jones at reed.edu


From rjeffries at ucla.edu  Sun Sep 30 20:50:11 2012
From: rjeffries at ucla.edu (Robin Jeffries)
Date: Sun, 30 Sep 2012 11:50:11 -0700
Subject: [R-sig-ME] Sampling methods for MCMCglmm using cengaussian
	family
In-Reply-To: <CANz9Z_J3d2W6MHibLWJ5ekruEWWAd_gJiMijPittGsBsme_akg@mail.gmail.com>
References: <CAMt7gbP3Kbcvou1NM6z-X=71v_jrON1Gk8OPw2jY+nMjRPL-hg@mail.gmail.com>
	<CANz9Z_J3d2W6MHibLWJ5ekruEWWAd_gJiMijPittGsBsme_akg@mail.gmail.com>
Message-ID: <CAMt7gbMhfSemXpJoqVmbd7xRJXGUFESZeQBW7xouAQvkSXgUaw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20120930/b004f02a/attachment.pl>

From jwiley.psych at gmail.com  Sun Sep 30 21:04:25 2012
From: jwiley.psych at gmail.com (Joshua Wiley)
Date: Sun, 30 Sep 2012 12:04:25 -0700
Subject: [R-sig-ME] Sampling methods for MCMCglmm using cengaussian
	family
In-Reply-To: <CAMt7gbMhfSemXpJoqVmbd7xRJXGUFESZeQBW7xouAQvkSXgUaw@mail.gmail.com>
References: <CAMt7gbP3Kbcvou1NM6z-X=71v_jrON1Gk8OPw2jY+nMjRPL-hg@mail.gmail.com>
	<CANz9Z_J3d2W6MHibLWJ5ekruEWWAd_gJiMijPittGsBsme_akg@mail.gmail.com>
	<CAMt7gbMhfSemXpJoqVmbd7xRJXGUFESZeQBW7xouAQvkSXgUaw@mail.gmail.com>
Message-ID: <CANz9Z_+s8BugosDJdi0+QtJakaC9TgUGOEwEYOWh9E_Hzr=p_A@mail.gmail.com>

Hmm, that makes sense, but I am not sure how to go about doing it.
Okay, I am sure because I can see the code where it is done in C++,
but I do not know an easy way and really loathe the idea of hacking
source code, recompiling, finding an error, and cycling through that
process until it works.  I could be missing something because I am not
the strongest at the theory underpinning these models.

I did edit the R MCMCglmm function so I could look at all the output
from the call to .C, but at least in the test case I created to try to
mimic your example, there did not seem to be anything useful there.

The process sounds like what the MICE package does, but in a bayesian framework.

I know Jarrod is a busy fellow, but he usually periodically gets to
emails here, and if he sees this, I am sure he would have a better
answer/direction for you to take as there is still the real
possibility I am missing something silly.

Good luck,

Josh

On Sun, Sep 30, 2012 at 11:50 AM, Robin Jeffries <rjeffries at ucla.edu> wrote:
> Hi Joshua,
>
> Thank you for your response. I do have those Course Notes, but only skimmed
> the technical details b/c I don't have any RE. I'll look further into it.
>
> Thank you for looking into extracting the proposal variance, I don't have
> enough knowledge to look into or understand the guts of most programs,
> especially if they're in C. I know I can provide a proposal distribution,
> that's the entire point. I want to run this model for enough iterations such
> that the proposal distribution is "good" in that the acceptance rate is ~25%
> or so. Then I want to know what that proposal distribution is, so I can
> restart the model using this good proposal distribution with no burnin.
>
> This probably sounds strange, but this model is only a step in a larger
> cyclical algorithm (Sequential Regression Multiple Imputation (Raghunathan
> 2001)) that models multiple variables, one iteration at a time. Y1 is
> modeled, its results fed into the model for Y2, both those results are fed
> into Y3.... until the results from Y2-Yp are fed back into a model for Y1.
> So I need to draw 1 iteration at a time using a a constant proposal
> distribution that does not adapt.
>
> I was just hoping to avoid those re-calculations this time and have MCMCglmm
> just tell me what a good proposal variance was instead of having to figure
> it out myself :)
>
> FYI, The small priors were not intentional, essentially a typo. Thank you
> for pointing it out.
>
> Anyhow, thank you again for helping me figure out how I'm going to do what i
> need to do. I appreciate the time you spent on it.
>
>
> -Robin
>
>



-- 
Joshua Wiley
Ph.D. Student, Health Psychology
Programmer Analyst II, Statistical Consulting Group
University of California, Los Angeles
https://joshuawiley.com/


From jwiley.psych at gmail.com  Sun Sep 30 21:42:13 2012
From: jwiley.psych at gmail.com (Joshua Wiley)
Date: Sun, 30 Sep 2012 12:42:13 -0700
Subject: [R-sig-ME] Two Questions re: Piecewise Mixed Effects Models
In-Reply-To: <BF2CE7102E190B4FABEED85AB38539C403BB6C90@ITS-MSXMBS3M.ad.unc.edu>
References: <BF2CE7102E190B4FABEED85AB38539C403BB6C90@ITS-MSXMBS3M.ad.unc.edu>
Message-ID: <CANz9Z_Kh_h5BN59kJezNvbNR+nQt8kiCmNF-DQ=zv9+jorvVMg@mail.gmail.com>

Hi Eric,

The first part of this code is reproducible and I suggest you run it
to see the graph and the output as I hope it will help you understand.

## simulate some data
set.seed(10)
y <- rnorm(1000, mean = (time <- sample(-2:2, 1000, TRUE)) * ((time >
0) + .5), sd = 1)

# plot time and the outcome y, you can see the 'break' sort of
plot(jitter(time), y)

# fit a non broek and broken time model
# also get the predicted values for times -2, -1, 0, 1, and 2
yhat1 <- predict(m1 <- lm(y ~ time), data.frame(time = -2:2))
yhat2 <- predict(m2 <- lm(y ~ time + time:I(time > 0)), data.frame(time = -2:2))

# add lines of predicted values from the different models
lines(-2:2, yhat1, col = "black", lwd = 3)
lines(-2:2, yhat2, col = "blue", lwd = 3)

# summaries of the models so you can see the coefficients
summary(m1)
summary(m2)

# what you have is an intercept and slope for time <= 0
# these are called intercept and time
# then for time > 0, you get an adjustment to the time slope (the
interaction term)
# the final design matrix would look something like:
cbind(Int = 1, Time = -2:2, Time2Adj = (-2:2) * (-2:2 > 0))


This part of the code is not reproducible, but I think this is more
like the model you probably want in glmer()

model <- glmer(rate ~ policy + time.before + time.after + Z +
  (1+ time.before + time.after | Site) + offset(log(people)),
  data=data, family="poisson")

# this gives you fixed effects for policy and Z
# and random effects for the intercept, and two time slopes
# that are allowed to vary across levels in Site
# all random effects have freely estimates covariances, so that is a 3
x 3 matrix
# (intercept, time.before, time.after) with all elements estimated

This is roughly (exactly?) equivalenet to the SAS model using proc glimmix:

proc glimmix data=data method=LAPLACE;
  class Site;
  model rate = policy time.before time.after Z / dist=poisson offset=people;
  random intercept time.before time.after / subject=Site type = chol;
/* you may be more familiar with type = un for unstructured */
/*  random intercept time.before time.after / subject=Site type = un; */
run;

Hope this helps,

Josh


On Sun, Sep 30, 2012 at 2:20 AM, Lofgren, Eric <elofgren at email.unc.edu> wrote:
> I'm relatively new to mixed effects models, and have two fairly basic questions (I think) about a project I'm working on.
>
> Essentially, I'm working on a poisson regression problem where we're trying to estimate the impact of a change in a policy on the rate of an event X. There are N sites where this change has taken place, and we also happen to know another characteristic of each site Z. It's been suggested that I look a a piecewise mixed effects model (which I've also seen referred to as an interrupted time series or "broken stick" model).
>
> I've got two questions about this type of model:
>
> 1. As I understand it, you should have two time variables, a "Time Before" and a "Time After" variable, each of which equals 0 right at the point of the policy change. My question is what these variables should look like, specifically what they should look like after they reach zero. For example, with a simple series of 5 points and the policy change in the middle, should it look like this:
>
> Time Before: 2, 1, 0, NA, NA
> Time After: NA, NA, 0, 1, 2
>
> or this?
>
> Time Before: 2, 1, 0, -1, -2
> TIme After: -2, -1, 0, 1, 2
>
> Or something else entirely?
>
> 2. In terms of using something like glmer() to fit the model itself, I'm a little confused about the syntax (I normally use SAS, but am trying to get better at using R). Following some advice I've seen, I should be using something like this:
>
> model <- glmer( rate ~ policy + time.before + time.after + (1+ time.before + time.after + Z |Site) + offset(log(people)), data=data, family="poisson")
>
> First, is this correct? Second, what's accomplished by having the time variables in both the fixed and random effects sections - what does that produce? A fixed slope and random intercept? Something else?
>
> Thanks,
>
> Eric
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



-- 
Joshua Wiley
Ph.D. Student, Health Psychology
Programmer Analyst II, Statistical Consulting Group
University of California, Los Angeles
https://joshuawiley.com/


From elofgren at email.unc.edu  Sun Sep 30 11:08:36 2012
From: elofgren at email.unc.edu (Lofgren, Eric)
Date: Sun, 30 Sep 2012 09:08:36 +0000
Subject: [R-sig-ME] Two Questions re: Piecewise mixed effects models
Message-ID: <BF2CE7102E190B4FABEED85AB38539C403BB6BDC@ITS-MSXMBS3M.ad.unc.edu>

I'm relatively new to mixed effects models, and have two fairly basic questions (I think) about a project I'm working on.

Essentially, I'm working on a poisson regression problem where we're trying to estimate the impact of a change in a policy on the rate of an event X. There are N sites where this change has taken place, and we also happen to know another characteristic of each site Z. It's been suggested that I look a a piecewise mixed effects model (which I've also seen referred to as an interrupted time series or "broken stick" model).

I've got two questions about this type of model:

1. As I understand it, you should have two time variables, a "Time Before" and a "Time After" variable, each of which equals 0 right at the point of the policy change. My question is what these variables should look like, specifically what they should look like after they reach zero. For example, with a simple series of 5 points and the policy change in the middle, should it look like this:

Time Before: 2, 1, 0, NA, NA
Time After: NA, NA, 0, 1, 2

or this?

Time Before: 2, 1, 0, -1, -2
TIme After: -2, -1, 0, 1, 2

Or something else entirely?

2. In terms of using something like glmer() to fit the model itself, I'm a little confused about the syntax (I normally use SAS, but am trying to get better at using R). Following some advice I've seen, I should be using something like this:

model <- glmer( rate ~ policy + time.before + time.after + (1+ time.before + time.after + Z |Site) + offset(log(people)), data=data, family="poisson")

First, is this correct? Second, what's accomplished by having the time variables in both the fixed and random effects sections - what does that produce? A fixed slope and random intercept? Something else?

Thanks,

Eric

