From @hm@ti@@ @ending from gm@il@com  Wed Jan  2 09:36:11 2019
From: @hm@ti@@ @ending from gm@il@com (Toni Hernandez-Matias)
Date: Wed, 2 Jan 2019 09:36:11 +0100
Subject: [R-sig-ME] glmulti
Message-ID: <CA+hwERk=BW=H49ZchwaTrsReFbBiiqW+hWpPUvC4bqqU1jJLyQ@mail.gmail.com>

Dear all,

I am trying to use glmulti for model selection for a set of linear models
fit with glmer. But I failed to do so, and when I searched in google I
found several people with similar problems but not solutions.
At the moment, my attempt is very simple:
output1<-glmulti(Y~1+DENS_EXT+(1|utm_code_f),data=data1,level=1,crit=aicc,fitfunction=glmer,family=binomial)

DENS_EXT is a continous variable
utm_code_f is a random factor

But it does not work:
Initialization...
Error in .subset2(x, i, exact = exact) :
  attempt to select less than one element in get1index

Could anyone help me? Do you have some information about how to apply glmer
using glmulti?

Thank you very much in advance (and happy new year!)

Toni

-- 
*********************************************************

Antonio Hernandez Matias

Equip de Biologia de la Conservaci?
Departament de Biologia Evolutiva, Ecolog?a i Ci?ncies Ambientals
Facultat de Biologia  i Institut de Recerca de la Biodiversitat (IRBio)
Universitat de Barcelona (UB)
Av. Diagonal, 643
Barcelona      08028
Spain
Telephone: +34-934035857
FAX: +34-934035740
e-mail: ahernandezmatias at ub.edu

***********************************************************

	[[alternative HTML version deleted]]


From lplough @ending from umce@@edu  Wed Jan  2 17:33:06 2019
From: lplough @ending from umce@@edu (Plough, Louis)
Date: Wed, 2 Jan 2019 11:33:06 -0500
Subject: [R-sig-ME] binary trait correlation across environments
 (experimental trials) using MCMCglmm?
Message-ID: <CAA9UjY_kEoj_5K_3CMmV4-KtFo7FQZL+_dviJnnUmcU=6tBgSw@mail.gmail.com>

Dear list,
I am trying to estimate the genetic correlation between binary traits
(survival) across trial (experimental) exposures of oysters to low
salinity.  I have a 50 family half-sib design (with replicates) that have
been deployed in a randomized design. Estimates of heritability with
MCMCglmm were reasonable in each of the two trials (same families used but
different individuals) but I am interested in estimating the genetic
correlation for survival between the two trials.

Code and suggestions I have read on this and other online forums seem to
apply to genetic correlations (rG) estimated for two traits measured on the
SAME individuals at the same time (environment) e.g. tarsus length and
color of birds.  For example, code like this from a a  2017 R-Sig-ME post
<https://stat.ethz.ch/pipermail/r-sig-mixed-models/2017q1/025532.html>:

*Bin.Bin.corr <- MCMCglmm(cbind(bin1, bin2) ~ trait - 1, random =
**~us(trait):animal, **rcov = ~corg(trait):units, family =
c("threshold",
**"threshold"), pedigree = Ped, prior = Prior1rG, **data = Data, nitt
= 4050000, burnin = 50000, thin = 2000, **verbose =**FALSE)*


Note that  Jerrod suggested different priors and modeling the
interaction of sex and animal with: *us(sex):animal *but Im not sure
this approach would applies to a cross-environment correlation
estimate?


So, my question is, what is the best way to proceed to estimate a
cross environment genetic correlation of a binary trait (survival)? My
data look like this (head and tail of file) where "Trial" is the
relevant 'environment' variable (with conditions A and B):






  animal
  Family
  rep
  sire
  dam
  Surv
  Trial


  20160010411501
  1
  A
  20135319911008
  20142889911003
  1
    A


  20160010411502
  1
  A
  20135319911008
  20142889911003
  1
    A


  20160010411503
  1
  A
  20135319911008
  20142889911003
  1
    A


  20160010411504
  1
  A
  20135319911008
  20142889911003
  1
    A


  20160010411505
  1
  A
  20135319911008
  20142889911003
  1
    A


  20160010411506
  1
  A
  20135319911008
  20142889911003
  1
    A


  .....








  animal
  Family
  rep
  sire
  dam
  Surv
  Trial


  20160880411657
  88
  C
  20140019911002
  20135319911007
  0
    B


  20160880411658
  88
  C
  20140019911002
  20135319911007
  0
    B


  20160880411659
  88
  C
  20140019911002
  20135319911007
  0
    B


  20160880411660
  88
  C
  20140019911002
  20135319911007
  0
    B


  20160880411661
  88
  C
  20140019911002
  20135319911007
  0
    B


  20160880411662
  88
  C
  20140019911002
  20135319911007
  0
    B



Here are two possible approaches I have come up with, though I am
unsure of their validity.


1) With my binary phenotype (Surv) indexed by Trial (A or B), I could
estimate the interaction of animal:Trial (e.g. ~us(Trial):animal) as
suggested by Jerrod for that previous R-Sig-Me post, however, its not
clear to me

what to do next with the outcome of that term?  Running a model like this:


bi_model_trial <- MCMCglmm(Surv ~ 1, random = ~us(Trial):animal,
family = "threshold",prior = prior, pedigree = pedigree.t, data =
trials, nitt = 5e+04, burnin = 15000, thin = 10)


the following output is achieved (I know the eff. sample sizes are WAY
too low, but just trying to understand what to do with the output)


> summary(bi_model_trial)

 Iterations = 15001:49991
 Thinning interval  = 10
 Sample size  = 3500

 DIC: 5868.55

 G-structure:  ~us(Trial):animal

                     post.mean l-95% CI u-95% CI eff.samp
TrialA:TrialA.animal    0.3803   0.1330   0.6587    27.94
TrialB:TrialA.animal    0.1074  -0.1775   0.3795    77.64
TrialA:TrialB.animal    0.1074  -0.1775   0.3795    77.64
TrialB:TrialB.animal    1.6707   0.6172   2.9186    17.74

 R-structure:  ~units

      post.mean l-95% CI u-95% CI eff.samp
units    0.4744   0.2055   0.8418    15.82

 Location effects: phen ~ 1

            post.mean l-95% CI u-95% CI eff.samp  pMCMC
(Intercept)     3.625    2.657    4.557    3.169 <3e-04 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

 Cutpoints:
                     post.mean l-95% CI u-95% CI eff.samp
cutpoint.traitphen.1     4.186    3.046    5.221    2.318


Im not sure how to interpret these interaction terms or how a genetic
correlation would be calculated from them? Is this even the right way
to setup the model for estimating genetic correlation between trials?


2) A Second approach would be to simply estimate the correlation of
family means (survival) between trials using a set of equations from
this   paper <https://onlinelibrary.wiley.com/doi/full/10.1111/j.1420-9101.2005.00997.x>.

It would seem most appropriate to use equation (5) and substitute Sire
variance in place of Family variance (half sib design), and thus
genetic correlation Rg = Variance.sire / (  Variance.sire +
Variance.Sire*Variance.Env).


The question is, how to get the appropriate interaction variance
components (Var. Sire*Env)   from the MCMCglmm output or how to code
the model appropriately? Perhaps another package is better suited for
this?


Any advice on my understanding of the problem (i.e. cross
environmental genetic correlation is different from simple bivariate
genetic correlation of traits within an environment/trial) and how to
accomplish the cross environmental genetic co

correlation ( approach 1 or 2 or something completely different),
would be most appreciated.


Thanks!


LVP




.

	[[alternative HTML version deleted]]


From mich@el@whitby @ending from gm@il@com  Tue Jan  1 19:37:13 2019
From: mich@el@whitby @ending from gm@il@com (Michael Whitby)
Date: Tue, 1 Jan 2019 12:37:13 -0600
Subject: [R-sig-ME] Calculating Confidence intervals for glmmTMB predictions
Message-ID: <CAOgUSWGuU3E6y3ThQiO_+qNzWzd=1t6y0btfXBaOYZSu7bfb1g@mail.gmail.com>

I am trying to calculate confidence intervals of predictions made from a
glmmTMB model with zero inflation using an ar1 covariance structure. The
model uses complex bases (both poly and scale) in both the model and zi
formula.

I have looked through a few issues posted on github and the original paper
describing glmmTMB. However, many methods are proposed and shown, and it
has only caused me confustion. Additionally, it appears glmmTMB has changed
slightly (e.g. ziform depricated, and addition of allow.new.levels
argurment), leading me to think that the correct method may be different.

Here is what I've looked at so far:
https://github.com/glmmTMB/glmmTMB/issues/324
https://github.com/glmmTMB/glmmTMB/issues/378
https://www.biorxiv.org/content/biorxiv/suppl/2017/05/01/132753.DC1/132753-2.pdf
https://r-sig-mixed-models.r-project.narkive.com/EOC1ab9c/r-sig-me-zero-inflated-glmmtmb-with-poly-confidence-band

Currently, I am simply taking the prediction and se on the response scale
(since this should include the ZI term as well), converting back to the log
scale, calculating the 0.95 confidence interval and converting back to the
response scale.

Here is an example using the Salamanders data set (this is somewhat
simplified as it doesn't use ar1 or complex basis, but I think it still
demonstrates my question). I would like some reassurance that I calculated
the upper and lower limits correctly.

library(glmmTMB)
data("Salamanders")

# View(Salamanders)

fit = glmmTMB(count~spp + cover + mined + poly(DOP, 3)+(1|site),
              ziformula=~spp + mined,
              dispformula = ~DOY,
              data = Salamanders,
              family=nbinom2)
# DOP
-----------------------------------------------------------------------------------------


newdata2 <- expand.grid(
  site = "new",
  spp = unique(Salamanders$spp),
  mined = factor(c("yes", "no"), levels = levels(Salamanders$mined)),
  cover = mean(Salamanders$cover),
  DOY = mean(Salamanders$DOY),
  DOP = seq(from = min(Salamanders$DOP), to=max(Salamanders$DOP), length =
25)
)

preds2 <- predict(fit, newdata2, se=T, allow.new.levels = T,
type='response')
newdata2$pred=preds2$fit
newdata2$se = preds2$se.fit
newdata2$ulimit = exp(log(newdata2$pred)+(qnorm(0.975)*log(newdata2$se)))
newdata2$llimit = exp(log(newdata2$pred)-(qnorm(0.975)*log(newdata2$se)))

library(ggplot2)

ggplot(data=newdata2, aes(x=DOP, y = pred) )+
  geom_ribbon(aes(ymin=llimit, ymax=ulimit, fill=mined), alpha = 0.25)+
  geom_line(aes(color = mined), size=1)+
  facet_wrap(~spp)



Michael Whitby
michael.whitby at gmail.com
609-923-0973

	[[alternative HTML version deleted]]


From cl@rk@kog@n @ending from w@u@edu  Wed Jan  2 20:39:58 2019
From: cl@rk@kog@n @ending from w@u@edu (Kogan, Clark)
Date: Wed, 2 Jan 2019 19:39:58 +0000
Subject: [R-sig-ME] AR1 within test bouts with random intercept
Message-ID: <CY4PR0101MB2982944CDD3E6F7DD0442D31E48C0@CY4PR0101MB2982.prod.exchangelabs.com>

Hi,

I am trying to fit a linear mixed effects model that has a random effect for subject and an AR1 correlation structure for time within period, where period is nested within subject. I am happy to treat time as either discrete or continuous - the time intervals within a period are all the same. I attempt to fit the following model and get an error.

mod <- lme(agg ~ tx_group + day + tx_group*day, random = ~1|subj, correlation = corAR1(form = ~time|subj_period), data = pw)


Error in lme.formula(agg ~ tx_group + day + tx_group * day, random = ~1 |  :
  incompatible formulas for groups in 'random' and 'correlation'

>From a little online reading, it seems that lme likes to have the same grouping factor for the random effects and the correlation. I was wondering whether there are other tools that are currently available to fit this type of correlation structure.

Thanks,
Clark Kogan

	[[alternative HTML version deleted]]


From j@h@dfield @ending from ed@@c@uk  Wed Jan  2 21:17:49 2019
From: j@h@dfield @ending from ed@@c@uk (HADFIELD Jarrod)
Date: Wed, 2 Jan 2019 20:17:49 +0000
Subject: [R-sig-ME] binary trait correlation across environments
 (experimental trials) using MCMCglmm?
In-Reply-To: <CAA9UjY_kEoj_5K_3CMmV4-KtFo7FQZL+_dviJnnUmcU=6tBgSw@mail.gmail.com>
References: <CAA9UjY_kEoj_5K_3CMmV4-KtFo7FQZL+_dviJnnUmcU=6tBgSw@mail.gmail.com>
Message-ID: <21C98FDD-D20C-44AF-BD1E-94C22DE91776@ed.ac.uk>

Hi,

Your model bi_model_trial is the correct one. However you must fix the residual variance at one in the prior. You have estimated the genetic (co)variance matrix for the two trials, from which you can obtain the genetic correlation.  Alternatively, you could fit animal+animal:Trial which assumes the genetic variances in the two trials are the same and the correlation is positive. The correlation in this latter model is obtained as VAR(animal)/(VAR(animal)+VAR(animal:Trial)).  Also, there seems to be a problem with your Surv data as it has three levels rather than 2 and so a cutpoint is being estimated.

Cheers,

Jarrod






On 2 Jan 2019, at 16:33, Plough, Louis <lplough at umces.edu<mailto:lplough at umces.edu>> wrote:

bi_model_trial

The University of Edinburgh is a charitable body, registered in Scotland, with registration number SC005336.

	[[alternative HTML version deleted]]


From lplough @ending from umce@@edu  Wed Jan  2 23:18:49 2019
From: lplough @ending from umce@@edu (Plough, Louis)
Date: Wed, 2 Jan 2019 17:18:49 -0500
Subject: [R-sig-ME] binary trait correlation across environments
 (experimental trials) using MCMCglmm?
In-Reply-To: <21C98FDD-D20C-44AF-BD1E-94C22DE91776@ed.ac.uk>
References: <CAA9UjY_kEoj_5K_3CMmV4-KtFo7FQZL+_dviJnnUmcU=6tBgSw@mail.gmail.com>
 <21C98FDD-D20C-44AF-BD1E-94C22DE91776@ed.ac.uk>
Message-ID: <CAA9UjY-_b3F1uCmhim7TSRz6BBV9NFCC6+i9ExEfH6iwBLwfuQ@mail.gmail.com>

Thanks Jarrod for the prompt response!

I think the residual variance was set at 1.   My prior  looks like this:

prior <- list(R = list(V = 1, nu = 3), G = list(G1 = list(V = diag(2), nu =
2)))

Not sure why the difference for *nu* in R vs G (3 vs 2)...might have been a
typo. Is there guidance on setting the gamma parameter for this kind of
binary trait, cross environment correlation model with the 'threshold'
family? In the low iteration (toy) run I did for the R-Sig-ME post, the
correlation was a bit lower than I would expect based on simply phenotypic
correlation of family means, so I think I might need to tweak *nu *for
both?

*Can I also confirm that I am estimating the rG correctly with the
following code:*

corr.gen<-model_trial4$VCV[,'TrialA:TrialB.animal']/sqrt(model_trial4$VCV[,'TrialA:TrialA.animal']*model_trial4$VCV[,'TrialB:TrialB.animal'])

Any reason to use 'TrialA:TrialB.animal' vs 'TrialB:TrialA.animal' in the
numerator? Or are they basically equivalent? I wouldn't want add them,
would I?

Thanks for your help!!!

LVP



On Wed, Jan 2, 2019 at 3:17 PM HADFIELD Jarrod <j.hadfield at ed.ac.uk> wrote:

> Hi,
>
> Your model bi_model_trial is the correct one. However you must fix the
> residual variance at one in the prior. You have estimated the genetic
> (co)variance matrix for the two trials, from which you can obtain the
> genetic correlation.  Alternatively, you could fit animal+animal:Trial
> which assumes the genetic variances in the two trials are the same and the
> correlation is positive. The correlation in this latter model is obtained
> as VAR(animal)/(VAR(animal)+VAR(animal:Trial)).  Also, there seems to be a
> problem with your Surv data as it has three levels rather than 2 and so a
> cutpoint is being estimated.
>
> Cheers,
>
> Jarrod
>
>
>
>
>
>
> On 2 Jan 2019, at 16:33, Plough, Louis <lplough at umces.edu> wrote:
>
> bi_model_trial
>
>
> The University of Edinburgh is a charitable body, registered in Scotland,
> with registration number SC005336.
>

	[[alternative HTML version deleted]]


From d@luedecke m@ili@g off uke@de  Thu Jan  3 00:51:49 2019
From: d@luedecke m@ili@g off uke@de (d@luedecke m@ili@g off uke@de)
Date: Thu, 3 Jan 2019 00:51:49 +0100
Subject: [R-sig-ME] 
 Calculating Confidence intervals for glmmTMB predictions
In-Reply-To: <CAOgUSWGuU3E6y3ThQiO_+qNzWzd=1t6y0btfXBaOYZSu7bfb1g@mail.gmail.com>
References: <CAOgUSWGuU3E6y3ThQiO_+qNzWzd=1t6y0btfXBaOYZSu7bfb1g@mail.gmail.com>
Message-ID: <000001d4a2f6$2145bce0$63d136a0$@uke.de>

Hi Michael,

as the zero-inflation component and count component of the model work in
"opposite directions" (a higher expected value for the zero inflation means
a lower response, but a higher value for the conditional model means a
higher response), the recommendation from the package authors (see
especially
https://github.com/glmmTMB/glmmTMB/issues/378#issuecomment-417850022, which
refers to the Appendix A in the paper) is to use a bootstrapping-method to
compute the CI (see
https://journal.r-project.org/archive/2017/RJ-2017-066/RJ-2017-066.pdf, page
391-392).

I have implemented various options lately in my ggeffects-package
(https://strengejacke.github.io/ggeffects). There are examples for the
various predict-options (count component only, conditioning on
zero-inflation, taking random effects into account etc.) in the package
vignettes, see here:
https://strengejacke.github.io/ggeffects/articles/randomeffects.html#margina
l-effects-for-zero-inflated-mixed-models

These features are currently only available in the GitHub-version of
ggeffects, but a CRAN-submission is planned in the next days. ggpredict()
will do all the work for you and allows you to easily create a plot. A
function-call would look like this:

ggpredict(fit, c("DOP", "mined", "spp"), type = "fe.zi") %>% plot()


If you want to this this manually, you could use this code:

newdata2 <- expand.grid(
  site = "new",
  spp = unique(Salamanders$spp),
  mined = factor(c("yes", "no"), levels = levels(Salamanders$mined)),
  cover = mean(Salamanders$cover),
  DOY = mean(Salamanders$DOY),
  DOP = seq(from = min(Salamanders$DOP), to=max(Salamanders$DOP), length =
25)
)

prdat.sim <- get_glmmTMB_predictions(fit, newdata2, 1000) sims <-
exp(prdat.sim$cond) * (1 - stats::plogis(prdat.sim$zi))

newdata2$predicted <- apply(sims, 1, mean) newdata2$conf.low <- apply(sims,
1, quantile, probs = .025) newdata2$conf.high <- apply(sims, 1, quantile,
probs = .975) newdata2$std.error <- apply(sims, 1, sd)

ggplot(data=newdata2, aes(x=DOP, y = predicted) )+
  geom_ribbon(aes(ymin=conf.low, ymax=conf.high, fill=mined), alpha = 0.25)+
  geom_line(aes(color = mined), size=1)+
  facet_wrap(~spp)

get_glmmTMB_predictions <- function(model, newdata, nsim) {
  tryCatch(
    {
      x.cond <- stats::model.matrix(lme4::nobars(stats::formula(model)[-2]),
newdata)
      beta.cond <- lme4::fixef(model)$cond
      
      ziformula <- model$modelInfo$allForm$ziformula
      x.zi <- stats::model.matrix(lme4::nobars(stats::formula(ziformula)),
newdata)
      beta.zi <- lme4::fixef(model)$zi
      
      pred.condpar.psim <- MASS::mvrnorm(n = nsim, mu = beta.cond, Sigma =
stats::vcov(model)$cond)
      pred.cond.psim <- x.cond %*% t(pred.condpar.psim)
      pred.zipar.psim <- MASS::mvrnorm(n = nsim, mu = beta.zi, Sigma =
stats::vcov(model)$zi)
      pred.zi.psim <- x.zi %*% t(pred.zipar.psim)
      
      list(cond = pred.cond.psim, zi = pred.zi.psim)
    },
    error = function(x) { NULL },
    warning = function(x) { NULL },
    finally = function(x) { NULL }
  )
}


Best
Daniel

-----Urspr?ngliche Nachricht-----
Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im
Auftrag von Michael Whitby
Gesendet: Dienstag, 1. Januar 2019 19:37
An: r-sig-mixed-models at r-project.org
Betreff: [R-sig-ME] Calculating Confidence intervals for glmmTMB predictions

I am trying to calculate confidence intervals of predictions made from a
glmmTMB model with zero inflation using an ar1 covariance structure. The
model uses complex bases (both poly and scale) in both the model and zi
formula.

I have looked through a few issues posted on github and the original paper
describing glmmTMB. However, many methods are proposed and shown, and it has
only caused me confustion. Additionally, it appears glmmTMB has changed
slightly (e.g. ziform depricated, and addition of allow.new.levels
argurment), leading me to think that the correct method may be different.

Here is what I've looked at so far:
https://github.com/glmmTMB/glmmTMB/issues/324
https://github.com/glmmTMB/glmmTMB/issues/378
https://www.biorxiv.org/content/biorxiv/suppl/2017/05/01/132753.DC1/132753-2
.pdf
https://r-sig-mixed-models.r-project.narkive.com/EOC1ab9c/r-sig-me-zero-infl
ated-glmmtmb-with-poly-confidence-band

Currently, I am simply taking the prediction and se on the response scale
(since this should include the ZI term as well), converting back to the log
scale, calculating the 0.95 confidence interval and converting back to the
response scale.

Here is an example using the Salamanders data set (this is somewhat
simplified as it doesn't use ar1 or complex basis, but I think it still
demonstrates my question). I would like some reassurance that I calculated
the upper and lower limits correctly.

library(glmmTMB)
data("Salamanders")

# View(Salamanders)

fit = glmmTMB(count~spp + cover + mined + poly(DOP, 3)+(1|site),
              ziformula=~spp + mined,
              dispformula = ~DOY,
              data = Salamanders,
              family=nbinom2)
# DOP
----------------------------------------------------------------------------
-------------


newdata2 <- expand.grid(
  site = "new",
  spp = unique(Salamanders$spp),
  mined = factor(c("yes", "no"), levels = levels(Salamanders$mined)),
  cover = mean(Salamanders$cover),
  DOY = mean(Salamanders$DOY),
  DOP = seq(from = min(Salamanders$DOP), to=max(Salamanders$DOP), length =
25)
)

preds2 <- predict(fit, newdata2, se=T, allow.new.levels = T,
type='response')
newdata2$pred=preds2$fit
newdata2$se = preds2$se.fit
newdata2$ulimit = exp(log(newdata2$pred)+(qnorm(0.975)*log(newdata2$se)))
newdata2$llimit = exp(log(newdata2$pred)-(qnorm(0.975)*log(newdata2$se)))

library(ggplot2)

ggplot(data=newdata2, aes(x=DOP, y = pred) )+
  geom_ribbon(aes(ymin=llimit, ymax=ulimit, fill=mined), alpha = 0.25)+
  geom_line(aes(color = mined), size=1)+
  facet_wrap(~spp)



Michael Whitby
michael.whitby at gmail.com
609-923-0973

	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Prof. Dr. Dr. Uwe Koch-Gromus, Joachim Pr?l?, Marya Verdel
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING


From j@h@dfield @ending from ed@@c@uk  Thu Jan  3 10:24:31 2019
From: j@h@dfield @ending from ed@@c@uk (HADFIELD Jarrod)
Date: Thu, 3 Jan 2019 09:24:31 +0000
Subject: [R-sig-ME] binary trait correlation across environments
 (experimental trials) using MCMCglmm?
In-Reply-To: <CAA9UjY-_b3F1uCmhim7TSRz6BBV9NFCC6+i9ExEfH6iwBLwfuQ@mail.gmail.com>
References: <CAA9UjY_kEoj_5K_3CMmV4-KtFo7FQZL+_dviJnnUmcU=6tBgSw@mail.gmail.com>
 <21C98FDD-D20C-44AF-BD1E-94C22DE91776@ed.ac.uk>
 <CAA9UjY-_b3F1uCmhim7TSRz6BBV9NFCC6+i9ExEfH6iwBLwfuQ@mail.gmail.com>
Message-ID: <5c2d79bb-ffd5-7671-e08f-d5133a6bc265@ed.ac.uk>

Hi,

The prior is not fixed at one. Also, the priors you are using are quite informative. I would use

prior <- list(R = list(V = 1, fix=1), G = list(G1 = list(V = diag(2), nu = 2, alpha.mu=c(0,0), alpha.V=diag(2)*100)))

Your equation for the genetic correlation is correct.

You shouldn't expect the correlation in family means to equal the model based estimate of the genetic correlation for a number of reasons; most importantly they are on different scales (data versus latent) and the variances of the family means contains residual variation but the covariance doesn't so the correlation in family means is a (downwardly) biased estimator.

Cheers,

Jarrod






On 02/01/2019 22:18, Plough, Louis wrote:
Thanks Jarrod for the prompt response!

I think the residual variance was set at 1.   My prior  looks like this:

prior <- list(R = list(V = 1, nu = 3), G = list(G1 = list(V = diag(2), nu = 2)))

Not sure why the difference for nu in R vs G (3 vs 2)...might have been a typo. Is there guidance on setting the gamma parameter for this kind of binary trait, cross environment correlation model with the 'threshold' family? In the low iteration (toy) run I did for the R-Sig-ME post, the correlation was a bit lower than I would expect based on simply phenotypic correlation of family means, so I think I might need to tweak nu for both?

Can I also confirm that I am estimating the rG correctly with the following code:

corr.gen<-model_trial4$VCV[,'TrialA:TrialB.animal']/sqrt(model_trial4$VCV[,'TrialA:TrialA.animal']*model_trial4$VCV[,'TrialB:TrialB.animal'])

Any reason to use 'TrialA:TrialB.animal' vs 'TrialB:TrialA.animal' in the numerator? Or are they basically equivalent? I wouldn't want add them, would I?

Thanks for your help!!!

LVP



On Wed, Jan 2, 2019 at 3:17 PM HADFIELD Jarrod <j.hadfield at ed.ac.uk<mailto:j.hadfield at ed.ac.uk>> wrote:
Hi,

Your model bi_model_trial is the correct one. However you must fix the residual variance at one in the prior. You have estimated the genetic (co)variance matrix for the two trials, from which you can obtain the genetic correlation.  Alternatively, you could fit animal+animal:Trial which assumes the genetic variances in the two trials are the same and the correlation is positive. The correlation in this latter model is obtained as VAR(animal)/(VAR(animal)+VAR(animal:Trial)).  Also, there seems to be a problem with your Surv data as it has three levels rather than 2 and so a cutpoint is being estimated.

Cheers,

Jarrod






On 2 Jan 2019, at 16:33, Plough, Louis <lplough at umces.edu<mailto:lplough at umces.edu>> wrote:

bi_model_trial

The University of Edinburgh is a charitable body, registered in Scotland, with registration number SC005336.

	[[alternative HTML version deleted]]


From w@ngji@wei92 @ending from hotm@il@com  Fri Jan  4 05:43:48 2019
From: w@ngji@wei92 @ending from hotm@il@com (=?gb2312?B?zfUgvM7svw==?=)
Date: Fri, 4 Jan 2019 04:43:48 +0000
Subject: [R-sig-ME] Error in gamm4 function in names(x) <- value: 'names'
 attribute must be the same length as the vector
In-Reply-To: <TY2PR06MB3102E814E8D460058086E90DC28E0@TY2PR06MB3102.apcprd06.prod.outlook.com>
References: <TY2PR06MB310294C5531DFB29737640AFC28E0@TY2PR06MB3102.apcprd06.prod.outlook.com>,
 <TY2PR06MB3102E814E8D460058086E90DC28E0@TY2PR06MB3102.apcprd06.prod.outlook.com>
Message-ID: <TY2PR06MB3102827A4AF693764D9448A6C28E0@TY2PR06MB3102.apcprd06.prod.outlook.com>


Dear R users,


I am using the *gamm4 package* to model the ozone pollution concentration according to some environmental covariates by generated additive mixed model. The model takes the form :

model1 <-gamm4(O3~s(X,Y,bs="tp",k=10)+wd+s(date,bs="cc",k=100)+district,data=mydata,family= gaussian(link ="log" ), random=~(1|date/district), na.action="na.omit", method="REML",gamma=1.4)

And here is the strcture of  covariates
> str(mydata)
'data.frame': 7100 obs. of  286 variables:
 $ date            : Date, format: "2016-01-01" "2016-01-01" "2016-01-01" ...
 $ O3              : num  0.0141 0.0149 0.0102 0.0159 0.0186 ...
 $ district        : Factor w/ 10 levels "bc","bh","dl",..: 1 8 7 8 2 6 4 4 10 2 ...
 $ wd              : Factor w/ 16 levels "E","ENE","ESE",..: 13 13 13 13 13 2 9 9 11 13 ...
 $ X               : num  0.389 0.365 1 0.44 0.892 ...
 $ Y               : num  0.311 0.204 0.426 0.223 0.162 ...

I am stuck on an error in R: 'names' attribute [1] must be the same length as the vector [0].

I try to find where the problem is by delete the term of "s(date,bs="cc",k=100)" from the fomular and it could work well. It seems like  there is something wrong with date field.

I'm not exactly sure how to fix this problem.  Any advice would be greatly
appreciated!
Jiawei Wang
???? Outlook<http://aka.ms/weboutlook>

	[[alternative HTML version deleted]]


From bbolker @ending from gm@il@com  Fri Jan  4 15:23:56 2019
From: bbolker @ending from gm@il@com (Ben Bolker)
Date: Fri, 4 Jan 2019 09:23:56 -0500
Subject: [R-sig-ME] 
 Error in gamm4 function in names(x) <- value: 'names'
 attribute must be the same length as the vector
In-Reply-To: <TY2PR06MB3102827A4AF693764D9448A6C28E0@TY2PR06MB3102.apcprd06.prod.outlook.com>
References: <TY2PR06MB310294C5531DFB29737640AFC28E0@TY2PR06MB3102.apcprd06.prod.outlook.com>
 <TY2PR06MB3102E814E8D460058086E90DC28E0@TY2PR06MB3102.apcprd06.prod.outlook.com>
 <TY2PR06MB3102827A4AF693764D9448A6C28E0@TY2PR06MB3102.apcprd06.prod.outlook.com>
Message-ID: <CABghstTJn=nh9EgUPKJ4Xk5c4HbqGoFqPooakfCTpB5qAd=frw@mail.gmail.com>

 Hard to say much more without a reproducible example.

 You could try to turning your date variable into a numeric value.

bs="cc" appears to be a cyclic smooth; I would probably experiment
with some very simple models (maybe in mgcv::gam() to start with) that
use a cyclic smooth on a date-valued covariate and see if that even
works ...

  can you show us the results of traceback() ?

  cheers
    Ben Bolker

On Thu, Jan 3, 2019 at 11:44 PM ? ?? <wangjiawei92 at hotmail.com> wrote:
>
>
> Dear R users,
>
>
> I am using the *gamm4 package* to model the ozone pollution concentration according to some environmental covariates by generated additive mixed model. The model takes the form :
>
> model1 <-gamm4(O3~s(X,Y,bs="tp",k=10)+wd+s(date,bs="cc",k=100)+district,data=mydata,family= gaussian(link ="log" ), random=~(1|date/district), na.action="na.omit", method="REML",gamma=1.4)
>
> And here is the strcture of  covariates
> > str(mydata)
> 'data.frame': 7100 obs. of  286 variables:
>  $ date            : Date, format: "2016-01-01" "2016-01-01" "2016-01-01" ...
>  $ O3              : num  0.0141 0.0149 0.0102 0.0159 0.0186 ...
>  $ district        : Factor w/ 10 levels "bc","bh","dl",..: 1 8 7 8 2 6 4 4 10 2 ...
>  $ wd              : Factor w/ 16 levels "E","ENE","ESE",..: 13 13 13 13 13 2 9 9 11 13 ...
>  $ X               : num  0.389 0.365 1 0.44 0.892 ...
>  $ Y               : num  0.311 0.204 0.426 0.223 0.162 ...
>
> I am stuck on an error in R: 'names' attribute [1] must be the same length as the vector [0].
>
> I try to find where the problem is by delete the term of "s(date,bs="cc",k=100)" from the fomular and it could work well. It seems like  there is something wrong with date field.
>
> I'm not exactly sure how to fix this problem.  Any advice would be greatly
> appreciated!
> Jiawei Wang
> ?? Outlook<http://aka.ms/weboutlook>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From mlevin@ @ending from purdue@edu  Sat Jan  5 19:48:59 2019
From: mlevin@ @ending from purdue@edu (Levine, Michael)
Date: Sat, 5 Jan 2019 18:48:59 +0000
Subject: [R-sig-ME] Prediction variance for GLMM
Message-ID: <1546714137197.61600@purdue.edu>

Dear all,


I would like to ask the following question. Is it possible to obtain prediction variances for GLMMs in the package lme4 , based e.g. on the function glmer? I know that it is possible to do it with "pure" GLM's but I don't see any options for GLMM's.  I realize there is a problem there because such a variance can be defined in several different ways...


Let me know and thanks a lot in advance!


Yours,

Michael Levine
Associate Professor, Statistics

Department of Statistics
Purdue University
250 North University Street
West Lafayette, IN 47907 USA

email: mlevins at purdue.edu
Phone: +1-765-496-7571
Fax:   +1-765-494-0558
URL:   www.stat.purdue.edu/~mlevins

	[[alternative HTML version deleted]]


From john@m@indon@ld @ending from @nu@edu@@u  Sat Jan  5 21:32:14 2019
From: john@m@indon@ld @ending from @nu@edu@@u (John Maindonald)
Date: Sat, 5 Jan 2019 20:32:14 +0000
Subject: [R-sig-ME] Prediction variance for GLMM
In-Reply-To: <1546714137197.61600@purdue.edu>
References: <1546714137197.61600@purdue.edu>
Message-ID: <ED82C8EE-284A-428A-99E1-F48B7DF0829B@anu.edu.au>

You might consider using glmmTMB::glmmTMB(), which has an
`se.fit=TRUE` argument.  Note the ability, with allow.new.levels=TRUE,
to predict for a new random factor level.

glmmTMB() has the advantange of allowing a wider range of error
familiies.  For betabinomial, negative binomial, and other such
families, it allows the modeling of the ?dispersion? parameter
(NB that this is not ?dispersion? as defined for glm quasi models,
albeit one can be expressed as a function of the other.)

There is the standard warning that the SEs are contingent on the
model assumptions, and on distributional theory approximations.
(E.g., what DF assumptions will give a good approximation to the
coef/SE distributions.)  The lme4 parametric bootstrap function
bootMer() should now work to give SEs that pretty much get
around issues with distributional theory approximations, but not
around model assumptions more generally.


John Maindonald             email: john.maindonald at anu.edu.au<mailto:john.maindonald at anu.edu.au>

On 6/01/2019, at 07:48, Levine, Michael <mlevins at purdue.edu<mailto:mlevins at purdue.edu>> wrote:

Dear all,


I would like to ask the following question. Is it possible to obtain prediction variances for GLMMs in the package lme4 , based e.g. on the function glmer? I know that it is possible to do it with "pure" GLM's but I don't see any options for GLMM's.  I realize there is a problem there because such a variance can be defined in several different ways...


Let me know and thanks a lot in advance!


Yours,

Michael Levine
Associate Professor, Statistics

Department of Statistics
Purdue University
250 North University Street
West Lafayette, IN 47907 USA

email: mlevins at purdue.edu<mailto:mlevins at purdue.edu>
Phone: +1-765-496-7571
Fax:   +1-765-494-0558
URL:   www.stat.purdue.edu/~mlevins

[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


	[[alternative HTML version deleted]]


From p_connolly @ending from @ling@hot@co@nz  Sat Jan  5 22:29:43 2019
From: p_connolly @ending from @ling@hot@co@nz (Patrick Connolly)
Date: Sun, 6 Jan 2019 10:29:43 +1300
Subject: [R-sig-ME] Predictions from lme and lmer models
Message-ID: <20190105212943.GA8688@slingshot.co.nz>



The call to predict with lme models has an argument 'level' which, I
understand, produces population predictions for level = 0 and group
specific predictions for level = 1.

There doesn't seem to be a similar argument for lmer (lme4 package)
models.  Is it just a matter of specifying the appropriate newdata
data frame to achieve the same end?  Or is there a more direct way?

TIA

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}                   Great minds discuss ideas    
 _( Y )_  	         Average minds discuss events 
(:_~*~_:)                  Small minds discuss people  
 (_)-(_)  	                      ..... Eleanor Roosevelt
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From d@rizopoulo@ @ending from er@@mu@mc@nl  Sat Jan  5 23:54:23 2019
From: d@rizopoulo@ @ending from er@@mu@mc@nl (D. Rizopoulos)
Date: Sat, 5 Jan 2019 22:54:23 +0000
Subject: [R-sig-ME] Prediction variance for GLMM
In-Reply-To: <1546714137197.61600@purdue.edu>
References: <1546714137197.61600@purdue.edu>
Message-ID: <7191AFC7255B4F49A30707E39BEAD05FDEC4F91B@EXCH-RX03.erasmusmc.nl>

It is not clear what type of predictions you want to calculate. In GLMMs, and because of the nonlinear link function you have three types of predictions:

(1) Using only the fixed effects, i.e., setting the random effects to zero. These are predictions for the average subject.

(2) Population predictions, i.e., predictions averaged over the subjects. Note that these are not the same as the predictions in (1).

(3) Subject-specific predictions, which are calculated conditionally on the random effects, either for existing or new subjects. In this case, you can also calculate dynamic predictions that are updated as  new measurements are recorded for the subject.

For all type of predictions you could calculate standard errors.

Using the GLMMadaptive package (https://drizopoulos.github.io/GLMMadaptive/), you can get both the predictions and the standard errors using the predict() method. For examples, check the vignettes:

https://drizopoulos.github.io/GLMMadaptive/articles/Methods_MixMod.html#predictions

and

https://drizopoulos.github.io/GLMMadaptive/articles/Dynamic_Predictions.html

Best,
Dimitris

- - - - - -
Dimitris Rizopoulos
Professor of Biostatistics
Erasmus University Medical Center
The Netherlands

From: Levine, Michael <mlevins at purdue.edu<mailto:mlevins at purdue.edu>>
Date: Saturday, 05 Jan 2019, 7:49 PM
To: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
Cc: 'Jiexin Duan' <duan32 at purdue.edu<mailto:duan32 at purdue.edu>>
Subject: [R-sig-ME] Prediction variance for GLMM

Dear all,


I would like to ask the following question. Is it possible to obtain prediction variances for GLMMs in the package lme4 , based e.g. on the function glmer? I know that it is possible to do it with "pure" GLM's but I don't see any options for GLMM's.  I realize there is a problem there because such a variance can be defined in several different ways...


Let me know and thanks a lot in advance!


Yours,

Michael Levine
Associate Professor, Statistics

Department of Statistics
Purdue University
250 North University Street
West Lafayette, IN 47907 USA

email: mlevins at purdue.edu
Phone: +1-765-496-7571
Fax:   +1-765-494-0558
URL:   www.stat.purdue.edu/~mlevins<http://www.stat.purdue.edu/~mlevins>

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From d@luedecke m@ili@g off uke@de  Sun Jan  6 00:16:14 2019
From: d@luedecke m@ili@g off uke@de (d@luedecke m@ili@g off uke@de)
Date: Sun, 6 Jan 2019 00:16:14 +0100
Subject: [R-sig-ME] Prediction variance for GLMM
In-Reply-To: <1546714137197.61600@purdue.edu>
References: <1546714137197.61600@purdue.edu>
Message-ID: <000601d4a54c$a82b8930$f8829b90$@uke.de>

Dear Michael,

For prediction intervals (in contrast to confidence intervals), I recommend
Ben Bolker's FAQ on GLMM's:
http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#predictions-andor-con
fidence-or-prediction-intervals-on-predictions

The example in the FAQ is for models with simple random effects structures.
The idea is to add the random effect variances as additional source of
uncertainty to get prediction intervals. In another context, Ben Bolker has
drafted a function to compute r-squared values for GLMM's, based on the
paper from Nakagawa et al. 2017, where Ben used a proposal from Johnson et
al. to compute the mean random effect variance for models with more complex
random effects structures. I'm using the same approach to compute
predictions with prediction intervals from glmer() or glmmTMB(), in my
package "ggeffects", which computes marginal effects for many different
models. 

See below four examples, each one for glmer() and glmmTMB(), where
predictions condition on fixed effects only, and where prediction intervals
also take the uncertainty of random effects into account. The current code
may only work in the GitHub-version from ggeffects, but a CRAN submission is
planned within the next days. You find more details at
http://strengejacke.github.io/ggeffects.

Best
Daniel


library(lme4)
library(glmmTMB)
library(ggeffects)

m1 <- glmer(
  cbind(incidence, size - incidence) ~ period + (1 | herd),
  data = cbpp, 
  family = binomial
)

m2 <- glmmTMB(
  cbind(incidence, size - incidence) ~ period + (1 | herd),
  data = cbpp, 
  family = binomial
)

ggpredict(m1, "period")
#> 
#> # Predicted probabilities of cbind(incidence, size - incidence) 
#> # x = period 
#> 
#>  x predicted std.error conf.low conf.high
#>  1     0.198     0.231    0.136     0.280
#>  2     0.084     0.307    0.048     0.143
#>  3     0.074     0.327    0.040     0.132
#>  4     0.048     0.425    0.022     0.105
#> 
#> Adjusted for:
#> * herd = 0 (population-level)
#> Standard errors are on link-scale (untransformed).

ggpredict(m2, "period")
#> 
#> # Predicted probabilities of cbind(incidence, size - incidence) 
#> # x = period 
#> 
#>  x predicted std.error conf.low conf.high
#>  1     0.198     0.232    0.135     0.280
#>  2     0.084     0.310    0.047     0.144
#>  3     0.074     0.330    0.040     0.132
#>  4     0.048     0.430    0.021     0.106
#> 
#> Adjusted for:
#> * herd = NA (population-level)
#> Standard errors are on link-scale (untransformed).

ggpredict(m1, "period", type = "re")
#> 
#> # Predicted probabilities of cbind(incidence, size - incidence) 
#> # x = period 
#> 
#>  x predicted std.error conf.low conf.high
#>  1     0.198     0.682    0.061     0.485
#>  2     0.084     0.712    0.022     0.270
#>  3     0.074     0.721    0.019     0.247
#>  4     0.048     0.770    0.011     0.187
#> 
#> Adjusted for:
#> * herd = 0 (population-level)
#> Standard errors are on link-scale (untransformed).

ggpredict(m2, "period", type = "re")
#> 
#> # Predicted probabilities of cbind(incidence, size - incidence) 
#> # x = period 
#> 
#>  x predicted std.error conf.low conf.high
#>  1     0.198     0.683    0.061     0.485
#>  2     0.084     0.713    0.022     0.270
#>  3     0.074     0.722    0.019     0.248
#>  4     0.048     0.773    0.011     0.188
#> 
#> Adjusted for:
#> * herd = NA (population-level)
#> Standard errors are on link-scale (untransformed).

Nakagawa S, Johnson P, Schielzeth H (2017) The coefficient of determination
R2 and intra-class correlation coefficient from generalized linear
mixed-effects models revisted and expanded. J. R. Soc. Interface 14. doi:
10.1098/rsif.2017.0213

Johnson PC, O'Hara RB. 2014. Extension of Nakagawa & Schielzeth's R2GLMM to
random slopes models. Methods Ecol Evol, 5: 944-946. (doi:
10.1111/2041-210X.12225)




-----Urspr?ngliche Nachricht-----
Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im
Auftrag von Levine, Michael
Gesendet: Samstag, 5. Januar 2019 19:49
An: r-sig-mixed-models at r-project.org
Cc: 'Jiexin Duan' <duan32 at purdue.edu>
Betreff: [R-sig-ME] Prediction variance for GLMM

Dear all,


I would like to ask the following question. Is it possible to obtain
prediction variances for GLMMs in the package lme4 , based e.g. on the
function glmer? I know that it is possible to do it with "pure" GLM's but I
don't see any options for GLMM's.  I realize there is a problem there
because such a variance can be defined in several different ways...


Let me know and thanks a lot in advance!


Yours,

Michael Levine
Associate Professor, Statistics

Department of Statistics
Purdue University
250 North University Street
West Lafayette, IN 47907 USA

email: mlevins at purdue.edu
Phone: +1-765-496-7571
Fax:   +1-765-494-0558
URL:   www.stat.purdue.edu/~mlevins

	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Prof. Dr. Dr. Uwe Koch-Gromus, Joachim Pr?l?, Marya Verdel
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING


From bbolker @ending from gm@il@com  Sun Jan  6 01:00:34 2019
From: bbolker @ending from gm@il@com (Ben Bolker)
Date: Sat, 5 Jan 2019 19:00:34 -0500
Subject: [R-sig-ME] Predictions from lme and lmer models
In-Reply-To: <20190105212943.GA8688@slingshot.co.nz>
References: <20190105212943.GA8688@slingshot.co.nz>
Message-ID: <2f2ee07e-bb13-3138-3f89-6da2328d44b2@gmail.com>


  See the "re.form" argument ... re.form=~0 or re.form=NA gives
population-level predictions (level-0). The interface has changed a bit
because lme4 can handle more complex models than nlme (specifically, it
can fit non-nested models, so "which random effects are included" is
potentially a more complex question than "which levels are included").

On 2019-01-05 4:29 p.m., Patrick Connolly wrote:
> 
> 
> The call to predict with lme models has an argument 'level' which, I
> understand, produces population predictions for level = 0 and group
> specific predictions for level = 1.
> 
> There doesn't seem to be a similar argument for lmer (lme4 package)
> models.  Is it just a matter of specifying the appropriate newdata
> data frame to achieve the same end?  Or is there a more direct way?
> 
> TIA
>


From p_connolly @ending from @ling@hot@co@nz  Sun Jan  6 08:21:33 2019
From: p_connolly @ending from @ling@hot@co@nz (Patrick Connolly)
Date: Sun, 6 Jan 2019 20:21:33 +1300
Subject: [R-sig-ME] Predictions from lme and lmer models
In-Reply-To: <2f2ee07e-bb13-3138-3f89-6da2328d44b2@gmail.com>
References: <20190105212943.GA8688@slingshot.co.nz>
 <2f2ee07e-bb13-3138-3f89-6da2328d44b2@gmail.com>
Message-ID: <20190106072133.GC8688@slingshot.co.nz>


So just to check that I've understood this correctly, population-level
predictions exclude random coefficients, but the coefficients it does
use were themselves calculated concurrently with those random term/s?
(As distinct from a model with only fixed terms.)

On Sat, 05-Jan-2019 at 07:00PM -0500, Ben Bolker wrote:

|> 
|>   See the "re.form" argument ... re.form=~0 or re.form=NA gives
|> population-level predictions (level-0). The interface has changed a bit
|> because lme4 can handle more complex models than nlme (specifically, it
|> can fit non-nested models, so "which random effects are included" is
|> potentially a more complex question than "which levels are included").
|> 
|> On 2019-01-05 4:29 p.m., Patrick Connolly wrote:
|> > 
|> > 
|> > The call to predict with lme models has an argument 'level' which, I
|> > understand, produces population predictions for level = 0 and group
|> > specific predictions for level = 1.
|> > 
|> > There doesn't seem to be a similar argument for lmer (lme4 package)
|> > models.  Is it just a matter of specifying the appropriate newdata
|> > data frame to achieve the same end?  Or is there a more direct way?
|> > 
|> > TIA
|> >
|> 
|> _______________________________________________
|> R-sig-mixed-models at r-project.org mailing list
|> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}                   Great minds discuss ideas    
 _( Y )_  	         Average minds discuss events 
(:_~*~_:)                  Small minds discuss people  
 (_)-(_)  	                      ..... Eleanor Roosevelt
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From li@@_1995_@ @ending from hotm@il@com  Sat Jan  5 15:20:35 2019
From: li@@_1995_@ @ending from hotm@il@com (Lisa Snel)
Date: Sat, 5 Jan 2019 14:20:35 +0000
Subject: [R-sig-ME] Mixed Design ANOVA in R after MICE imputation
Message-ID: <AM4PR0501MB225828C90D05AED978E83581AF8F0@AM4PR0501MB2258.eurprd05.prod.outlook.com>

Dear all,

I have a question about performing a Mixed Design ANOVA in R after multiple imputation using MICE. My data is as follows:

id <- c(1,2,3,4,5,6,7,8,9,10)
group <- c(0,1,1,0,0,1,0,0,0,1)
measure_1 <- c(60,80,90,54,60,61,77,67,88,90)
measure_2 <- c(55,88,88,55,70,62,78,66,65,92)
measure_3 <- c(58,88,85,56,68,62,89,62,70,99)
measure_4 <- c(64,80,78,92,65,64,87,65,67,96)
measure_5 <- c(64,85,80,65,74,69,90,65,70,99)
measure_6 <- c(70,83,80,55,73,64,91,65,91,89)
dat <- data.frame(id, group, measure_1, measure_2, measure_3, measure_4, measure_5, measure_6)
dat$group <- as.factor(dat$group)

So: we have 6 repeated measurements of diastolic blood pressure (measure 1 till 6). The grouping factor is gender, which is called group. This variable is coded 1 if male and 0 if female. Before multiple imputation, we have used the following code in R:

library(reshape)
library(reshape2)
datLong <- melt(dat, id = c("id", "group"), measured = c("measure_1", "measure_2", "measure_3", "measure_4", "measure_5", "measure_6"))
datLong

colnames(datLong) <- c("ID", "Gender", "Time", "Score")
datLong
table(datLong$Time)
datLong$ID <- as.factor(datLong$ID)

library(ez)
model_mixed <- ezANOVA(data = datLong,
               dv = Value,
               wid = ID,
               within = Time,
               between = Gender,
               detailed = TRUE,
               type = 3,
               return_aov = TRUE)
model_mixed

This worked perfectly. However, our data is not complete. We have missing values, that we impute using MICE:

id <- c(1,2,3,4,5,6,7,8,9,10)
group <- c(0,1,1,0,0,1,0,0,0,1)
measure_1 <- c(60,80,90,54,60,61,77,67,88,90)
measure_2 <- c(55,NA,88,55,70,62,78,66,65,92)
measure_3 <- c(58,88,85,56,68,62,89,62,70,99)
measure_4 <- c(64,80,78,92,NA,NA,87,65,67,96)
measure_5 <- c(64,85,80,65,74,69,90,65,70,99)
measure_6 <- c(70,NA,80,55,73,64,91,65,91,89)
dat <- data.frame(id, group, measure_1, measure_2, measure_3, measure_4, measure_5, measure_6)
dat$group <- as.factor(dat$group)

imp_anova <- mice(dat, maxit = 0)
meth <- imp_anova$method
pred <- imp_anova$predictorMatrix
imp_anova <- mice(dat, method = meth, predictorMatrix = pred, seed = 2018, maxit = 10, m = 5)

(The imputation gives logged events, because of the made-up data and the simple imputation code e.g id used as a predictor. For my real data, the imputation was correct and valid)

Now I have the imputed dataset of class ?mids?. I have searched the internet, but I cannot find how I can perform the mixed design ANOVA on this imputed set, as I did before with the complete set using ezANOVA. Is there anyone who can and wants to help me?


Best,

Lisa


	[[alternative HTML version deleted]]


From lplough @ending from umce@@edu  Sun Jan  6 20:40:23 2019
From: lplough @ending from umce@@edu (Plough, Louis)
Date: Sun, 6 Jan 2019 14:40:23 -0500
Subject: [R-sig-ME] binary trait correlation across environments
 (experimental trials) using MCMCglmm?
In-Reply-To: <5c2d79bb-ffd5-7671-e08f-d5133a6bc265@ed.ac.uk>
References: <CAA9UjY_kEoj_5K_3CMmV4-KtFo7FQZL+_dviJnnUmcU=6tBgSw@mail.gmail.com>
 <21C98FDD-D20C-44AF-BD1E-94C22DE91776@ed.ac.uk>
 <CAA9UjY-_b3F1uCmhim7TSRz6BBV9NFCC6+i9ExEfH6iwBLwfuQ@mail.gmail.com>
 <5c2d79bb-ffd5-7671-e08f-d5133a6bc265@ed.ac.uk>
Message-ID: <CAA9UjY90pFcjeGy_BHY8XKL0bmTB_b8Nr2AzTdhpPgL8Exj28A@mail.gmail.com>

Hi Jarrod et al.
Thanks for these suggestions. The model coding the interaction as
~us(Trait):animal is working (running), but convergence appears to be
a way off even after 2 or 3 million iterations, especially for the
TrialB variance ( survival in the second environment).  Would another
family (ordinal?) be worth considering over the threshold family? Or
changing the prior somehow, e.g.  bumping up the alpha.V=diag(2)*1000?
 We do expect some positive correlation between the two trials...i.e.
should not be zero.

Coding the model the other way you suggested as animal+animal:Trait
gives me the following error:

> bi_model_trial_pos <- MCMCglmm(phen ~ 1, random = ~animal+animal:Trial, family = "threshold",prior = prior_b, pedigree = pedigree.t, data = trials_up, nitt = 5e+05, burnin = 25000, thin = 20)

Error in buildZ(rmodel.terms[r], data = data, nginverse = names(ginverse)) :

  interactions not permitted between standard random effects and those
associated with ginverse

I tried used a similar prior as for the above model, but changed the
number of structures... (not sure if that is the right prior here):

> prior_b <- list(R = list(V = 1, fix=1), G = list(G1 = list(V = 1, nu = 2),G2=list(V=1,alpha.mu=0, nu=2)))

For either model, I have been studying your tutorials for how to tweak
the priors in a sensible way (for a binary trait), but I still
struggle with how the various supplementary parameters alter the prior
distribution or CDF... you had mentioned that my previous priors were
'very informative', but in which way? Towards zero or too high?

LVP


On Thu, Jan 3, 2019 at 4:24 AM HADFIELD Jarrod <j.hadfield at ed.ac.uk> wrote:
>
> Hi,
>
> The prior is not fixed at one. Also, the priors you are using are quite informative. I would use
>
> prior <- list(R = list(V = 1, fix=1), G = list(G1 = list(V = diag(2), nu = 2, alpha.mu=c(0,0), alpha.V=diag(2)*100)))
>
> Your equation for the genetic correlation is correct.
>
> You shouldn't expect the correlation in family means to equal the model based estimate of the genetic correlation for a number of reasons; most importantly they are on different scales (data versus latent) and the variances of the family means contains residual variation but the covariance doesn't so the correlation in family means is a (downwardly) biased estimator.
>
> Cheers,
>
> Jarrod
>
>
>
>
>
>
> On 02/01/2019 22:18, Plough, Louis wrote:
>
> Thanks Jarrod for the prompt response!
>
> I think the residual variance was set at 1.   My prior  looks like this:
>
> prior <- list(R = list(V = 1, nu = 3), G = list(G1 = list(V = diag(2), nu = 2)))
>
> Not sure why the difference for nu in R vs G (3 vs 2)...might have been a typo. Is there guidance on setting the gamma parameter for this kind of binary trait, cross environment correlation model with the 'threshold' family? In the low iteration (toy) run I did for the R-Sig-ME post, the correlation was a bit lower than I would expect based on simply phenotypic correlation of family means, so I think I might need to tweak nu for both?
>
> Can I also confirm that I am estimating the rG correctly with the following code:
>
> corr.gen<-model_trial4$VCV[,'TrialA:TrialB.animal']/sqrt(model_trial4$VCV[,'TrialA:TrialA.animal']*model_trial4$VCV[,'TrialB:TrialB.animal'])
>
> Any reason to use 'TrialA:TrialB.animal' vs 'TrialB:TrialA.animal' in the numerator? Or are they basically equivalent? I wouldn't want add them, would I?
>
> Thanks for your help!!!
>
> LVP
>
>
>
> On Wed, Jan 2, 2019 at 3:17 PM HADFIELD Jarrod <j.hadfield at ed.ac.uk> wrote:
>>
>> Hi,
>>
>> Your model bi_model_trial is the correct one. However you must fix the residual variance at one in the prior. You have estimated the genetic (co)variance matrix for the two trials, from which you can obtain the genetic correlation.  Alternatively, you could fit animal+animal:Trial which assumes the genetic variances in the two trials are the same and the correlation is positive. The correlation in this latter model is obtained as VAR(animal)/(VAR(animal)+VAR(animal:Trial)).  Also, there seems to be a problem with your Surv data as it has three levels rather than 2 and so a cutpoint is being estimated.
>>
>> Cheers,
>>
>> Jarrod
>>
>>
>>
>>
>>
>>
>> On 2 Jan 2019, at 16:33, Plough, Louis <lplough at umces.edu> wrote:
>>
>> bi_model_trial
>>
>>
>> The University of Edinburgh is a charitable body, registered in Scotland, with registration number SC005336.


From jungm@@rten @ending from gm@il@com  Mon Jan  7 03:01:28 2019
From: jungm@@rten @ending from gm@il@com (Maarten Jung)
Date: Mon, 7 Jan 2019 03:01:28 +0100
Subject: [R-sig-ME] Mixed Design ANOVA in R after MICE imputation
In-Reply-To: <AM4PR0501MB225828C90D05AED978E83581AF8F0@AM4PR0501MB2258.eurprd05.prod.outlook.com>
References: <AM4PR0501MB225828C90D05AED978E83581AF8F0@AM4PR0501MB2258.eurprd05.prod.outlook.com>
Message-ID: <CAHr4Dye6dCt_NfgencjPnn0qfLS3u-pwtJTbYgrXPNS7AOQMUA@mail.gmail.com>

Hi Lisa,

the mice::complete() function returns the completed data in a
specified format, after that you probably have to convert it into
long-formatted data if you want to use ez::ezANOVA().
Please note that this mailing list is about mixed models and your
question doesn't seem to be about this topic.

Best regards, Maarten

On Sun, Jan 6, 2019 at 5:18 PM Lisa Snel <lisa_1995_s at hotmail.com> wrote:
>
> Dear all,
>
> I have a question about performing a Mixed Design ANOVA in R after multiple imputation using MICE. My data is as follows:
>
> id <- c(1,2,3,4,5,6,7,8,9,10)
> group <- c(0,1,1,0,0,1,0,0,0,1)
> measure_1 <- c(60,80,90,54,60,61,77,67,88,90)
> measure_2 <- c(55,88,88,55,70,62,78,66,65,92)
> measure_3 <- c(58,88,85,56,68,62,89,62,70,99)
> measure_4 <- c(64,80,78,92,65,64,87,65,67,96)
> measure_5 <- c(64,85,80,65,74,69,90,65,70,99)
> measure_6 <- c(70,83,80,55,73,64,91,65,91,89)
> dat <- data.frame(id, group, measure_1, measure_2, measure_3, measure_4, measure_5, measure_6)
> dat$group <- as.factor(dat$group)
>
> So: we have 6 repeated measurements of diastolic blood pressure (measure 1 till 6). The grouping factor is gender, which is called group. This variable is coded 1 if male and 0 if female. Before multiple imputation, we have used the following code in R:
>
> library(reshape)
> library(reshape2)
> datLong <- melt(dat, id = c("id", "group"), measured = c("measure_1", "measure_2", "measure_3", "measure_4", "measure_5", "measure_6"))
> datLong
>
> colnames(datLong) <- c("ID", "Gender", "Time", "Score")
> datLong
> table(datLong$Time)
> datLong$ID <- as.factor(datLong$ID)
>
> library(ez)
> model_mixed <- ezANOVA(data = datLong,
>                dv = Value,
>                wid = ID,
>                within = Time,
>                between = Gender,
>                detailed = TRUE,
>                type = 3,
>                return_aov = TRUE)
> model_mixed
>
> This worked perfectly. However, our data is not complete. We have missing values, that we impute using MICE:
>
> id <- c(1,2,3,4,5,6,7,8,9,10)
> group <- c(0,1,1,0,0,1,0,0,0,1)
> measure_1 <- c(60,80,90,54,60,61,77,67,88,90)
> measure_2 <- c(55,NA,88,55,70,62,78,66,65,92)
> measure_3 <- c(58,88,85,56,68,62,89,62,70,99)
> measure_4 <- c(64,80,78,92,NA,NA,87,65,67,96)
> measure_5 <- c(64,85,80,65,74,69,90,65,70,99)
> measure_6 <- c(70,NA,80,55,73,64,91,65,91,89)
> dat <- data.frame(id, group, measure_1, measure_2, measure_3, measure_4, measure_5, measure_6)
> dat$group <- as.factor(dat$group)
>
> imp_anova <- mice(dat, maxit = 0)
> meth <- imp_anova$method
> pred <- imp_anova$predictorMatrix
> imp_anova <- mice(dat, method = meth, predictorMatrix = pred, seed = 2018, maxit = 10, m = 5)
>
> (The imputation gives logged events, because of the made-up data and the simple imputation code e.g id used as a predictor. For my real data, the imputation was correct and valid)
>
> Now I have the imputed dataset of class ?mids?. I have searched the internet, but I cannot find how I can perform the mixed design ANOVA on this imputed set, as I did before with the complete set using ezANOVA. Is there anyone who can and wants to help me?
>
>
> Best,
>
> Lisa
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From thierry@onkelinx @ending from inbo@be  Mon Jan  7 10:22:19 2019
From: thierry@onkelinx @ending from inbo@be (Thierry Onkelinx)
Date: Mon, 7 Jan 2019 10:22:19 +0100
Subject: [R-sig-ME] AR1 within test bouts with random intercept
In-Reply-To: <CY4PR0101MB2982944CDD3E6F7DD0442D31E48C0@CY4PR0101MB2982.prod.exchangelabs.com>
References: <CY4PR0101MB2982944CDD3E6F7DD0442D31E48C0@CY4PR0101MB2982.prod.exchangelabs.com>
Message-ID: <CAJuCY5x-DjSk7yMZ=tgEnfXcZsk5j_ybnB8GMCsRd4C4CwR8Hg@mail.gmail.com>

Dear Clark,

Have you tried something like corAR1(form = ~time|subj/period) This uses
explict nesting. Your code uses implicit nesting and lme doesn't handle
that.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op wo 2 jan. 2019 om 20:40 schreef Kogan, Clark <clark.kogan at wsu.edu>:

> Hi,
>
> I am trying to fit a linear mixed effects model that has a random effect
> for subject and an AR1 correlation structure for time within period, where
> period is nested within subject. I am happy to treat time as either
> discrete or continuous - the time intervals within a period are all the
> same. I attempt to fit the following model and get an error.
>
> mod <- lme(agg ~ tx_group + day + tx_group*day, random = ~1|subj,
> correlation = corAR1(form = ~time|subj_period), data = pw)
>
>
> Error in lme.formula(agg ~ tx_group + day + tx_group * day, random = ~1 |
> :
>   incompatible formulas for groups in 'random' and 'correlation'
>
> From a little online reading, it seems that lme likes to have the same
> grouping factor for the random effects and the correlation. I was wondering
> whether there are other tools that are currently available to fit this type
> of correlation structure.
>
> Thanks,
> Clark Kogan
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From y@@hree19 @ending from gm@il@com  Mon Jan  7 16:12:38 2019
From: y@@hree19 @ending from gm@il@com (Yashree Mehta)
Date: Mon, 7 Jan 2019 16:12:38 +0100
Subject: [R-sig-ME] Compound Symmetry Covariance structure
In-Reply-To: <07f27448-4ca6-3cf8-7a81-5873462ebc89@gmail.com>
References: <CAOE=hqKdo9hPGd=3gWvz8xOSdFxChXBsQg-Nc=6drEd9aFCdyQ@mail.gmail.com>
 <CAFW8BypcBQ1AFRSGcMSF1m_bZJXg=Znwqc1MzU3hSP3itaNscw@mail.gmail.com>
 <dcf7f24d-6bfd-2436-76ce-d037703f5a1f@gmail.com>
 <CAOE=hq+jzUXfwtk7F8cV7NRzjX4o2dYYZXBNi95jhfgzxK-Gjg@mail.gmail.com>
 <07f27448-4ca6-3cf8-7a81-5873462ebc89@gmail.com>
Message-ID: <CAOE=hqKuAXSooMQ-nBNhupr8ybEya9dPfkmrghE3Ndm+5uT06w@mail.gmail.com>

Hi Ben,

the log-likelihoods of the original and the
compound-symmetrized models are the same. However, there is a shift in the
location of the kernel density plots of the random intercept when I compare
the ones from the original and the compound-symmetrized models. I
understand that there are no correlation parameters to model and so the the
log-likelihoods of the original and the
compound-symmetrized models are the same. I was wondering whether the
explicit accounting for compound symmetry by updating the original model is
necessary or simply a random intercept specification is enough for inducing
a compound symmetric covariance structure?

Thank you very much,

Regards,
Yashree

On Mon, Dec 10, 2018 at 1:10 AM Ben Bolker <bbolker at gmail.com> wrote:

>
>   I don't think a compound-symmetric specification would do anything in
> this case (i.e. when only the intercept varies among groups), because
> there are no explicit correlation parameters to model. Does the model
> actually change (e.g. are the log-likelihoods of the original and the
> compound-symmetrized models the same)?
>
> On 2018-12-09 3:37 p.m., Yashree Mehta wrote:
> > Hi Ben, Joaquin and John,
> >
> > First of all, thank you very much for your responses. They are all very
> > helpful.
> >
> > Yes, I understand now that there is an induced compound -symmetry
> > covariance structure in random effects model in nlme as default. I was
> > wondering if now, if I explicitly initialize the correlation and impose
> > compound symmetry in the model code (learnt from the example in Pinheiro
> > and Bates):
> >
> > First, I estimate the intra-class correlation coefficient and the value
> > is 0.908. Then, I estimate the standard LME model,
> >
> > model <- maize ~ "covariates" + random = ~ 1|HOUSEHOLD_ID, data=farm
> >
> > Then, I impose compound symmetry explicitly:
> >
> > dependency<-corCompSymm(value=0.908, form=~1|HOUSEHOLD_ID)
> > cs<-Initialize( dependency  , data=farm)
> > new_model<-update(model, correlation=cs)
> >
> > Is this fundamentally correct or is it double accounting for compound
> > symmetry since there already is default in lme function?
> >
> > Thank you very much.
> >
> > Regards,
> > Yashree
> >
> > On Sun, Dec 9, 2018 at 8:24 PM Ben Bolker <bbolker at gmail.com
> > <mailto:bbolker at gmail.com>> wrote:
> >
> >
> >       A quick example of the induced covariance structure.
> >
> >       Suppose you set up the simplest possible (linear) mixed model,
> which
> >     has an overall intercept B; a group-level random effect on the
> intercept
> >     e1_i with variance v1; and a residual error e0_ij with variance v0.
> The
> >     value of x_{ij} = B + e1_i + e0_ij.  The variance of any observation
> >     (E[(x_{ij}-B)^2]) is v0+v1.  The covariance of observations in the
> same
> >     group is E[(x_{ij}-B)(x_{kj}-B)] = v1. The covariance of
> observations in
> >     *different* groups is 0.  If we write out the correlation matrix for
> the
> >     whole data set (assuming the observations are written out with
> samples
> >     from the same group occurring contiguously), it will consist of a
> >     block-diagonal matrix with correlation v1/(v0+v1) within each block;
> the
> >     rest of the matrix will be zero.  This is a form of induced
> >     compound-symmetric covariance structure.
> >
> >       Presumably others can give good references to where this is
> explained
> >     clearly in the literature (maybe even in Pinheiro and Bates, I don't
> >     have access to my copy right now)
> >
> >     On 2018-12-07 1:53 p.m., Poe, John wrote:
> >     > Hi Yashree,
> >     >
> >     > Can you give the citation and page number for the panel data book?
> >     >
> >     > On Fri, Dec 7, 2018 at 1:15 PM Yashree Mehta <yashree19 at gmail.com
> >     <mailto:yashree19 at gmail.com>> wrote:
> >     >
> >     >> Hi,
> >     >>
> >     >> I have a question about the random effects model (Specifically, a
> >     random
> >     >> intercept model) in its role in assuming a covariance structure in
> >     >> estimation. In a panel data textbook, I read that by estimating a
> >     random
> >     >> effects model itself, there is an induced covariance structure.
> >     >>
> >     >> In nlme package, there are several types of covariance structures
> >     such as
> >     >> Compound Symmetry (which I assume in my model) but the default
> >     value is 0.
> >     >> I initialize it and proceed with the estimation.
> >     >>
> >     >> Does this mean that if I do not specify the compound symmetry
> >     value in
> >     >> nlme, the estimation is without a covariance assumption or there
> is
> >     >> something I have missed in my understanding? That the " by
> >     estimating a
> >     >> random effects model itself, there is an induced covariance
> >     structure"
> >     >> confuses me a little.
> >     >>
> >     >> It would be very helpful to get an explanation on this.
> >     >>
> >     >> Thank you very much!
> >     >>
> >     >> Regards,
> >     >> Yashree
> >     >>
> >     >>         [[alternative HTML version deleted]]
> >     >>
> >     >> _______________________________________________
> >     >> R-sig-mixed-models at r-project.org
> >     <mailto:R-sig-mixed-models at r-project.org> mailing list
> >     >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >     >>
> >     >
> >     >
> >
> >     _______________________________________________
> >     R-sig-mixed-models at r-project.org
> >     <mailto:R-sig-mixed-models at r-project.org> mailing list
> >     https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>

	[[alternative HTML version deleted]]


From m@di671 @ending from hotm@il@com  Mon Jan  7 19:29:17 2019
From: m@di671 @ending from hotm@il@com (Naima M.)
Date: Mon, 7 Jan 2019 18:29:17 +0000
Subject: [R-sig-ME] Glmm for positive count data
Message-ID: <MWHPR22MB033383923461E1270648C16DFE890@MWHPR22MB0333.namprd22.prod.outlook.com>


Dear all

I  have a positive count data response which I fitted with glmer and Poisson distribution (all worked well with no convergence problem in lme4).
Recently, I realized that Truncated Poisson distribution may be more appropriate for my data since there is no zeros in my responses and tried to fit Truncated negative count distribution (I used truncated negative binomial because my models were overdispersed), but I have many issues with convergence and the run time is very slow using glmmTMB or glmmADMB.
I'm wondering if I can keep my first models (glmer with Poisson), even if my data is positive?

Best,

Na?ma

	[[alternative HTML version deleted]]


From bbolker @ending from gm@il@com  Mon Jan  7 19:59:31 2019
From: bbolker @ending from gm@il@com (Ben Bolker)
Date: Mon, 7 Jan 2019 13:59:31 -0500
Subject: [R-sig-ME] Glmm for positive count data
In-Reply-To: <MWHPR22MB033383923461E1270648C16DFE890@MWHPR22MB0333.namprd22.prod.outlook.com>
References: <MWHPR22MB033383923461E1270648C16DFE890@MWHPR22MB0333.namprd22.prod.outlook.com>
Message-ID: <d79b05c4-5796-e761-8f67-9801b91720c4@gmail.com>


  (1) Are the issues with convergence generally the occurrence of
singular fits? (glmmTMB handles these less gracefully than glmer, since
it fits the RE variances on a logarithmic scale and relies more heavily
on Wald approximations in reporting

  (2) How slow is very slow? How big is your data set?

All other things being equal it's better to use the most realistic model
you can, but the biases due to model misspecification might not be too
huge.  How big are the differences you observe in the coefficients that
you're interested in, between a truncated-NB and a regular NB fit?

  If you have overdispersion you should *not* rely on a regular Poisson
fit (but if you've done something like add observation-level random
effects then you should be more or less OK, although arguably NB could
be better)

  Ben Bolker

On 2019-01-07 1:29 p.m., Naima M. wrote:
> 
> Dear all
> 
> I  have a positive count data response which I fitted with glmer and Poisson distribution (all worked well with no convergence problem in lme4).
> Recently, I realized that Truncated Poisson distribution may be more appropriate for my data since there is no zeros in my responses and tried to fit Truncated negative count distribution (I used truncated negative binomial because my models were overdispersed), but I have many issues with convergence and the run time is very slow using glmmTMB or glmmADMB.
> I'm wondering if I can keep my first models (glmer with Poisson), even if my data is positive?
> 
> Best,
> 
> Na?ma
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From cl@rk@kog@n @ending from w@u@edu  Mon Jan  7 22:26:34 2019
From: cl@rk@kog@n @ending from w@u@edu (Kogan, Clark)
Date: Mon, 7 Jan 2019 21:26:34 +0000
Subject: [R-sig-ME] AR1 within test bouts with random intercept
In-Reply-To: <CAJuCY5x-DjSk7yMZ=tgEnfXcZsk5j_ybnB8GMCsRd4C4CwR8Hg@mail.gmail.com>
References: <CY4PR0101MB2982944CDD3E6F7DD0442D31E48C0@CY4PR0101MB2982.prod.exchangelabs.com>
 <CAJuCY5x-DjSk7yMZ=tgEnfXcZsk5j_ybnB8GMCsRd4C4CwR8Hg@mail.gmail.com>
Message-ID: <CY4PR0101MB2982E8CA0EEB644A4B375CF6E4890@CY4PR0101MB2982.prod.exchangelabs.com>

Thierry,

That seemed to work.

Thanks,
Clark

From: Thierry Onkelinx <thierry.onkelinx at inbo.be>
Sent: Monday, January 7, 2019 1:22 AM
To: Kogan, Clark <clark.kogan at wsu.edu>
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] AR1 within test bouts with random intercept

Dear Clark,

Have you tried something like corAR1(form = ~time|subj/period) This uses explict nesting. Your code uses implicit nesting and lme doesn't handle that.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be<https://urldefense.proofpoint.com/v2/url?u=http-3A__www.inbo.be&d=DwMFaQ&c=C3yme8gMkxg_ihJNXS06ZyWk4EJm8LdrrvxQb-Je7sw&r=oEbEbiEeg32ENfvrFY6o9qTkkdIivEvx2vWVghVoM8I&m=Infad875ZIbPiowJ57tbsFHA_gftmw8-sgq5z0hue5I&s=r1olaq9nnnxgW91JEU2Hb5pkcP3g3zbAjBMfbHtq7jA&e=>
///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

[https://inbo-website-prd-532750756126.s3-eu-west-1.amazonaws.com/inbologoleeuw_nl.png]<https://urldefense.proofpoint.com/v2/url?u=https-3A__www.inbo.be&d=DwMFaQ&c=C3yme8gMkxg_ihJNXS06ZyWk4EJm8LdrrvxQb-Je7sw&r=oEbEbiEeg32ENfvrFY6o9qTkkdIivEvx2vWVghVoM8I&m=Infad875ZIbPiowJ57tbsFHA_gftmw8-sgq5z0hue5I&s=glLvHK1d3wde4cvsto5Ok6JkzCwWnMb6Y0EAKi8TSnM&e=>


Op wo 2 jan. 2019 om 20:40 schreef Kogan, Clark <clark.kogan at wsu.edu<mailto:clark.kogan at wsu.edu>>:
Hi,

I am trying to fit a linear mixed effects model that has a random effect for subject and an AR1 correlation structure for time within period, where period is nested within subject. I am happy to treat time as either discrete or continuous - the time intervals within a period are all the same. I attempt to fit the following model and get an error.

mod <- lme(agg ~ tx_group + day + tx_group*day, random = ~1|subj, correlation = corAR1(form = ~time|subj_period), data = pw)


Error in lme.formula(agg ~ tx_group + day + tx_group * day, random = ~1 |  :
  incompatible formulas for groups in 'random' and 'correlation'

From a little online reading, it seems that lme likes to have the same grouping factor for the random effects and the correlation. I was wondering whether there are other tools that are currently available to fit this type of correlation structure.

Thanks,
Clark Kogan

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models<https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dsig-2Dmixed-2Dmodels&d=DwMFaQ&c=C3yme8gMkxg_ihJNXS06ZyWk4EJm8LdrrvxQb-Je7sw&r=oEbEbiEeg32ENfvrFY6o9qTkkdIivEvx2vWVghVoM8I&m=Infad875ZIbPiowJ57tbsFHA_gftmw8-sgq5z0hue5I&s=oScK9lsBY2syXlvS4j7GNbmUkqGBMAVjKz2ECP_DH-g&e=>

	[[alternative HTML version deleted]]


From j@h@dfield @ending from ed@@c@uk  Tue Jan  8 12:15:55 2019
From: j@h@dfield @ending from ed@@c@uk (HADFIELD Jarrod)
Date: Tue, 8 Jan 2019 11:15:55 +0000
Subject: [R-sig-ME] binary trait correlation across environments
 (experimental trials) using MCMCglmm?
In-Reply-To: <CAA9UjY90pFcjeGy_BHY8XKL0bmTB_b8Nr2AzTdhpPgL8Exj28A@mail.gmail.com>
References: <CAA9UjY_kEoj_5K_3CMmV4-KtFo7FQZL+_dviJnnUmcU=6tBgSw@mail.gmail.com>
 <21C98FDD-D20C-44AF-BD1E-94C22DE91776@ed.ac.uk>
 <CAA9UjY-_b3F1uCmhim7TSRz6BBV9NFCC6+i9ExEfH6iwBLwfuQ@mail.gmail.com>
 <5c2d79bb-ffd5-7671-e08f-d5133a6bc265@ed.ac.uk>
 <CAA9UjY90pFcjeGy_BHY8XKL0bmTB_b8Nr2AzTdhpPgL8Exj28A@mail.gmail.com>
Message-ID: <69437ede-2d76-4a38-a27f-1ffa3d06eb6e@ed.ac.uk>

Hi,

Is it possible to share the data - it is hard to troubleshoot otherwise?

Cheers,

Jarrod


On 06/01/2019 19:40, Plough, Louis wrote:
> Hi Jarrod et al.
> Thanks for these suggestions. The model coding the interaction as
> ~us(Trait):animal is working (running), but convergence appears to be
> a way off even after 2 or 3 million iterations, especially for the
> TrialB variance ( survival in the second environment).  Would another
> family (ordinal?) be worth considering over the threshold family? Or
> changing the prior somehow, e.g.  bumping up the alpha.V=diag(2)*1000?
>   We do expect some positive correlation between the two trials...i.e.
> should not be zero.
>
> Coding the model the other way you suggested as animal+animal:Trait
> gives me the following error:
>
>> bi_model_trial_pos <- MCMCglmm(phen ~ 1, random = ~animal+animal:Trial, family = "threshold",prior = prior_b, pedigree = pedigree.t, data = trials_up, nitt = 5e+05, burnin = 25000, thin = 20)
> Error in buildZ(rmodel.terms[r], data = data, nginverse = names(ginverse)) :
>
>    interactions not permitted between standard random effects and those
> associated with ginverse
>
> I tried used a similar prior as for the above model, but changed the
> number of structures... (not sure if that is the right prior here):
>
>> prior_b <- list(R = list(V = 1, fix=1), G = list(G1 = list(V = 1, nu = 2),G2=list(V=1,alpha.mu=0, nu=2)))
> For either model, I have been studying your tutorials for how to tweak
> the priors in a sensible way (for a binary trait), but I still
> struggle with how the various supplementary parameters alter the prior
> distribution or CDF... you had mentioned that my previous priors were
> 'very informative', but in which way? Towards zero or too high?
>
> LVP
>
>
> On Thu, Jan 3, 2019 at 4:24 AM HADFIELD Jarrod <j.hadfield at ed.ac.uk> wrote:
>> Hi,
>>
>> The prior is not fixed at one. Also, the priors you are using are quite informative. I would use
>>
>> prior <- list(R = list(V = 1, fix=1), G = list(G1 = list(V = diag(2), nu = 2, alpha.mu=c(0,0), alpha.V=diag(2)*100)))
>>
>> Your equation for the genetic correlation is correct.
>>
>> You shouldn't expect the correlation in family means to equal the model based estimate of the genetic correlation for a number of reasons; most importantly they are on different scales (data versus latent) and the variances of the family means contains residual variation but the covariance doesn't so the correlation in family means is a (downwardly) biased estimator.
>>
>> Cheers,
>>
>> Jarrod
>>
>>
>>
>>
>>
>>
>> On 02/01/2019 22:18, Plough, Louis wrote:
>>
>> Thanks Jarrod for the prompt response!
>>
>> I think the residual variance was set at 1.   My prior  looks like this:
>>
>> prior <- list(R = list(V = 1, nu = 3), G = list(G1 = list(V = diag(2), nu = 2)))
>>
>> Not sure why the difference for nu in R vs G (3 vs 2)...might have been a typo. Is there guidance on setting the gamma parameter for this kind of binary trait, cross environment correlation model with the 'threshold' family? In the low iteration (toy) run I did for the R-Sig-ME post, the correlation was a bit lower than I would expect based on simply phenotypic correlation of family means, so I think I might need to tweak nu for both?
>>
>> Can I also confirm that I am estimating the rG correctly with the following code:
>>
>> corr.gen<-model_trial4$VCV[,'TrialA:TrialB.animal']/sqrt(model_trial4$VCV[,'TrialA:TrialA.animal']*model_trial4$VCV[,'TrialB:TrialB.animal'])
>>
>> Any reason to use 'TrialA:TrialB.animal' vs 'TrialB:TrialA.animal' in the numerator? Or are they basically equivalent? I wouldn't want add them, would I?
>>
>> Thanks for your help!!!
>>
>> LVP
>>
>>
>>
>> On Wed, Jan 2, 2019 at 3:17 PM HADFIELD Jarrod <j.hadfield at ed.ac.uk> wrote:
>>> Hi,
>>>
>>> Your model bi_model_trial is the correct one. However you must fix the residual variance at one in the prior. You have estimated the genetic (co)variance matrix for the two trials, from which you can obtain the genetic correlation.  Alternatively, you could fit animal+animal:Trial which assumes the genetic variances in the two trials are the same and the correlation is positive. The correlation in this latter model is obtained as VAR(animal)/(VAR(animal)+VAR(animal:Trial)).  Also, there seems to be a problem with your Surv data as it has three levels rather than 2 and so a cutpoint is being estimated.
>>>
>>> Cheers,
>>>
>>> Jarrod
>>>
>>>
>>>
>>>
>>>
>>>
>>> On 2 Jan 2019, at 16:33, Plough, Louis <lplough at umces.edu> wrote:
>>>
>>> bi_model_trial
>>>
>>>
>>> The University of Edinburgh is a charitable body, registered in Scotland, with registration number SC005336.

-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From tkelleykemple @ending from g@h@rv@rd@edu  Thu Jan 10 23:38:04 2019
From: tkelleykemple @ending from g@h@rv@rd@edu (Kelley-Kemple, Thomas)
Date: Thu, 10 Jan 2019 17:38:04 -0500
Subject: [R-sig-ME] Complex Error terms With Non-nested Error Structure
Message-ID: <CABTBhVjJaiBXpcfGWoLysA8nV=xdSL5j3kM3gvhgFPRjFsLTEg@mail.gmail.com>

Hello,
I'm trying to fit a model that has observations of scores, cross nested
within students and test items, (eg 10 students each answer 6 test items),
that then get scored on 2 dimensions. There are person and item random
effects, but the dimension effects are fixed (in the sense that these are
the exact two dimensions we care about; they aren't drawn from a wider
population of ways to evaluate the scores). I can write the code to get the
random effects for persons and items within each dimension (as well as the
covariances across dimensions), but am having trouble getting the syntax
right for separate residuals for each level of the fixed effects.

Ideally, I would have an estimate of the variance of the residuals for each
of the two dimensions, as well has how those residuals co-vary across those
two dimensions (ie. for a given person/item combination, how do the two
scores covary across the dimensions).
To get the main part of the model I can write:

 lmer(score ~ 1 + (-1+v1+v2| p) +  (-1+v1+v2| i ), data=data)

Here p is a factor indexing persons, i is a factor indexing items, and v1
and v2 are the dummies for the two score dimensions.

However, I can't figure out how to get the complex error structure in lmer.
I've looked into nmle and somewhat figured out the hack suggested to get
crossed random effects (nest them within a dummy random effect), but
haven't had much success there either. I'm open to suggestions either in
lmer or other packages.


Thanks in advance for any help!

Thomas

	[[alternative HTML version deleted]]


From r@turner @ending from @uckl@nd@@c@nz  Fri Jan 11 03:30:23 2019
From: r@turner @ending from @uckl@nd@@c@nz (Rolf Turner)
Date: Fri, 11 Jan 2019 15:30:23 +1300
Subject: [R-sig-ME] Constraining error variance to 0 in a mixed model.
Message-ID: <9fbf5a81-a680-1087-fa77-84966db5a350@auckland.ac.nz>


Let me start off by apologising for beating this issue to death on this 
mailing list.  I have raised it (without getting satisfactory answers) 
on several occasions.  If you are fed up to the teeth with my maundering 
s, please just press the delete button.

This latest occasion was triggered by my trying to learn to use the 
glmmTMB function (from the package of the same name).  I noticed (to my 
initial delight) that glmmTMB() has an argument "dispformula" and the 
help says (in particular) "In Gaussian mixed models, dispformula=~0 
fixes the [dispersion] parameter to be 0, forcing variance into the 
random effects."

I said "Ah-ha!  The very thing."  So I tried it out, using the following 
data set (supplied by Ben Pelzer in a posting to this list, and 
previously used by my very good self to illustrate the problem that I am 
trying to solve):

Dat <- structure(list(person = c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L,
4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 7L, 7L, 7L, 8L, 8L, 8L, 9L,
9L, 9L, 10L, 10L, 10L, 11L, 11L, 11L, 12L, 12L, 12L, 13L, 13L,
13L, 14L, 14L, 14L, 15L, 15L, 15L, 16L, 16L, 16L, 17L, 17L, 17L,
18L, 18L, 18L, 19L, 19L, 19L, 20L, 20L, 20L), occasion = structure(c(1L,
2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L,
3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L,
1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L,
2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L), .Label = c("1",
"2", "3"), class = "factor"), test = c(25L, 21L, 27L, 27L, 19L,
23L, 20L, 18L, 18L, 26L, 33L, 46L, 25L, 36L, 47L, 36L, 35L, 41L,
30L, 30L, 37L, 23L, 21L, 19L, 39L, 37L, 33L, 29L, 36L, 49L, 29L,
34L, 44L, 39L, 32L, 30L, 26L, 32L, 36L, 33L, 21L, 8L, 30L, 34L,
34L, 27L, 34L, 39L, 41L, 40L, 44L, 37L, 34L, 34L, 23L, 26L, 30L,
31L, 28L, 27L)), row.names = c(NA, -60L), class = "data.frame")

I tried three ways of "analysing" these data:

(1) Simple-minded multivariate analysis:

X   <- matrix(Dat$test,byrow=TRUE,ncol=3)
colnames(X) <- paste0("occasion",1:3)
mu  <- apply(X,2,mean)
Sig <- var(X)/20 # X has 20 rows

(2) Using lmer():
library(lme4)
fit2 <- lmer(test ~ 0 + occasion + (0 + occasion | person),data=Dat,
              control=lmerControl(check.nobs.vs.nRE = "ignore"))

(The "control" argument was gathered from a posting by Maarten Jung, on 
which I have previously commented.)

Note that this "works" but throws (as I have previously noted)
a disconcerting warning message:

> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model is nearly unidentifiable: large eigenvalue ratio
>  - Rescale variables?

(3) Using glmmTMB():
library(glmmTMB)
fit3 <- glmmTMB(test ~ 0 + occasion + (0 + occasion | person),data=Dat,
                 dispformula = ~0)

(No warning message; initially I said "Wheeeee!!!")

The "parameter estimates" mu, fixef(fit2) and fixef(fit3) all agree:

occasion1 occasion2 occasion3
     29.80     30.05     33.30

The covariance Sig and vcov(fit2) agree modulo numerical noise:

 > Sig
           occasion1 occasion2 occasion3
occasion1 1.7821053  1.150526 0.5768421
occasion2 1.1505263  2.249868 3.0623684
occasion3 0.5768421  3.062368 5.8531579

 > vcov(fit2)
3 x 3 Matrix of class "dpoMatrix"
           occasion1 occasion2 occasion3
occasion1 1.7821054  1.150526 0.5768422
occasion2 1.1505265  2.249868 3.0623683
occasion3 0.5768422  3.062368 5.8531577

However vcov(fit3) differs; after some head scratching I realised that 
it is equal to 19/20 times the others.  After a little more head 
scratching I said ah-ha!  The glmmTMB() function sets REML=FALSE by
default.  So I'll just call it with REML=TRUE:

fit3 <- glmmTMB(test ~ 0+occasion + (0+occasion | person),data=Dat,
                 dispformula = ~0,REML=TRUE)

And now vcov(fit3) more-or-less agrees with the other results:

 > vcov(fit3)
Conditional model:
           occasion1 occasion2 occasion3
occasion1 1.7819337  1.150276 0.5765386
occasion2 1.1502758  2.249755 3.0623895
occasion3 0.5765386  3.062390 5.8534164

But, to my extreme irritation, glmmTMB() now also throws a warning:

> Warning message:
> In fitTMB(TMBStruc) :
> Model convergence problem; false convergence (8). See vignette('troubleshooting')

(I've looked at the vignette, and I can sort of see a connection, but 
not really.)

*Why* does the bloody universe always do this sort of thing to me?  Why 
on earth should dividing by 19 rather than 20, effectively, cause such a 
warning to be thrown?

Isn't something a little out of whack here?

I reiterate at this point a remark that I made in a previous post on the 
issue of constraining the error variance to 0:

> What we have here is a "boundary" or "edge" or "corner" case, and I
> have heard it asserted by someone knowledgeable and respected (can't
> remember who, it was a long while ago) that in testing your software it
> is crucial to see how it behaves with such "boundary" cases.

Am I being unreasonable?  (*Me*?  *Unreasonable*?  Perish the thought!) :-)

cheers,

Rolf Turner

P.S.  Note that setting REML=FALSE in the call to lmer() causes it to
produce a covariance matrix that agrees with that produced by glmmTMB()
with default setting.  However this has no impact upon the error message 
issued by lmer().

R. T.

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From d@rizopoulo@ @ending from er@@mu@mc@nl  Fri Jan 11 08:53:30 2019
From: d@rizopoulo@ @ending from er@@mu@mc@nl (D. Rizopoulos)
Date: Fri, 11 Jan 2019 07:53:30 +0000
Subject: [R-sig-ME] Constraining error variance to 0 in a mixed model.
In-Reply-To: <9fbf5a81-a680-1087-fa77-84966db5a350@auckland.ac.nz>
References: <9fbf5a81-a680-1087-fa77-84966db5a350@auckland.ac.nz>
Message-ID: <7191AFC7255B4F49A30707E39BEAD05FDEC56670@EXCH-RX03.erasmusmc.nl>

For what it's worth, the same model can be fitted with nlme::gls without any warning messages:

library(nlme)
fit4 <- gls(test ~ 0 + occasion, data = Dat, 
            weights = varIdent(form = ~ 1 | occasion),
            correlation = corSymm(form = ~ 1 | person))

vcov(fit4)

Best,
Dimitris


-----Original Message-----
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of Rolf Turner
Sent: Friday, January 11, 2019 3:30 AM
To: Help Mixed Models <r-sig-mixed-models at r-project.org>
Subject: [R-sig-ME] Constraining error variance to 0 in a mixed model.


Let me start off by apologising for beating this issue to death on this mailing list.  I have raised it (without getting satisfactory answers) on several occasions.  If you are fed up to the teeth with my maundering s, please just press the delete button.

This latest occasion was triggered by my trying to learn to use the glmmTMB function (from the package of the same name).  I noticed (to my initial delight) that glmmTMB() has an argument "dispformula" and the help says (in particular) "In Gaussian mixed models, dispformula=~0 fixes the [dispersion] parameter to be 0, forcing variance into the random effects."

I said "Ah-ha!  The very thing."  So I tried it out, using the following data set (supplied by Ben Pelzer in a posting to this list, and previously used by my very good self to illustrate the problem that I am trying to solve):

Dat <- structure(list(person = c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 7L, 7L, 7L, 8L, 8L, 8L, 9L, 9L, 9L, 10L, 10L, 10L, 11L, 11L, 11L, 12L, 12L, 12L, 13L, 13L, 13L, 14L, 14L, 14L, 15L, 15L, 15L, 16L, 16L, 16L, 17L, 17L, 17L, 18L, 18L, 18L, 19L, 19L, 19L, 20L, 20L, 20L), occasion = structure(c(1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L), .Label = c("1", "2", "3"), class = "factor"), test = c(25L, 21L, 27L, 27L, 19L, 23L, 20L, 18L, 18L, 26L, 33L, 46L, 25L, 36L, 47L, 36L, 35L, 41L, 30L, 30L, 37L, 23L, 21L, 19L, 39L, 37L, 33L, 29L, 36L, 49L, 29L, 34L, 44L, 39L, 32L, 30L, 26L, 32L, 36L, 33L, 21L, 8L, 30L, 34L, 34L, 27L, 34L, 39L, 41L, 40L, 44L, 37L, 34L, 34L, 23L, 26L, 30L, 31L, 28L, 27L)), row.names = c(NA, -60L), class = "data.frame")

I tried three ways of "analysing" these data:

(1) Simple-minded multivariate analysis:

X   <- matrix(Dat$test,byrow=TRUE,ncol=3)
colnames(X) <- paste0("occasion",1:3)
mu  <- apply(X,2,mean)
Sig <- var(X)/20 # X has 20 rows

(2) Using lmer():
library(lme4)
fit2 <- lmer(test ~ 0 + occasion + (0 + occasion | person),data=Dat,
              control=lmerControl(check.nobs.vs.nRE = "ignore"))

(The "control" argument was gathered from a posting by Maarten Jung, on which I have previously commented.)

Note that this "works" but throws (as I have previously noted) a disconcerting warning message:

> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model is nearly unidentifiable: large eigenvalue ratio
>  - Rescale variables?

(3) Using glmmTMB():
library(glmmTMB)
fit3 <- glmmTMB(test ~ 0 + occasion + (0 + occasion | person),data=Dat,
                 dispformula = ~0)

(No warning message; initially I said "Wheeeee!!!")

The "parameter estimates" mu, fixef(fit2) and fixef(fit3) all agree:

occasion1 occasion2 occasion3
     29.80     30.05     33.30

The covariance Sig and vcov(fit2) agree modulo numerical noise:

 > Sig
           occasion1 occasion2 occasion3
occasion1 1.7821053  1.150526 0.5768421
occasion2 1.1505263  2.249868 3.0623684
occasion3 0.5768421  3.062368 5.8531579

 > vcov(fit2)
3 x 3 Matrix of class "dpoMatrix"
           occasion1 occasion2 occasion3
occasion1 1.7821054  1.150526 0.5768422
occasion2 1.1505265  2.249868 3.0623683
occasion3 0.5768422  3.062368 5.8531577

However vcov(fit3) differs; after some head scratching I realised that it is equal to 19/20 times the others.  After a little more head scratching I said ah-ha!  The glmmTMB() function sets REML=FALSE by default.  So I'll just call it with REML=TRUE:

fit3 <- glmmTMB(test ~ 0+occasion + (0+occasion | person),data=Dat,
                 dispformula = ~0,REML=TRUE)

And now vcov(fit3) more-or-less agrees with the other results:

 > vcov(fit3)
Conditional model:
           occasion1 occasion2 occasion3
occasion1 1.7819337  1.150276 0.5765386
occasion2 1.1502758  2.249755 3.0623895
occasion3 0.5765386  3.062390 5.8534164

But, to my extreme irritation, glmmTMB() now also throws a warning:

> Warning message:
> In fitTMB(TMBStruc) :
> Model convergence problem; false convergence (8). See 
> vignette('troubleshooting')

(I've looked at the vignette, and I can sort of see a connection, but not really.)

*Why* does the bloody universe always do this sort of thing to me?  Why on earth should dividing by 19 rather than 20, effectively, cause such a warning to be thrown?

Isn't something a little out of whack here?

I reiterate at this point a remark that I made in a previous post on the issue of constraining the error variance to 0:

> What we have here is a "boundary" or "edge" or "corner" case, and I 
> have heard it asserted by someone knowledgeable and respected (can't 
> remember who, it was a long while ago) that in testing your software 
> it is crucial to see how it behaves with such "boundary" cases.

Am I being unreasonable?  (*Me*?  *Unreasonable*?  Perish the thought!) :-)

cheers,

Rolf Turner

P.S.  Note that setting REML=FALSE in the call to lmer() causes it to produce a covariance matrix that agrees with that produced by glmmTMB() with default setting.  However this has no impact upon the error message issued by lmer().

R. T.

--
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From mollieebrook@ @ending from gm@il@com  Fri Jan 11 13:40:25 2019
From: mollieebrook@ @ending from gm@il@com (Mollie Brooks)
Date: Fri, 11 Jan 2019 13:40:25 +0100
Subject: [R-sig-ME] Constraining error variance to 0 in a mixed model.
In-Reply-To: <7191AFC7255B4F49A30707E39BEAD05FDEC56670@EXCH-RX03.erasmusmc.nl>
References: <9fbf5a81-a680-1087-fa77-84966db5a350@auckland.ac.nz>
 <7191AFC7255B4F49A30707E39BEAD05FDEC56670@EXCH-RX03.erasmusmc.nl>
Message-ID: <4AAF202E-3C0B-4357-9449-3AAE4F28BE83@gmail.com>

Hi Rolf, 

Thanks for the enthusiasm. I believe the ability to force the variance into the RE in glmmTMB was Kasper Kristensen?s idea.

I had a look at your example and it seems that it could be a false positive coming from nlminb. The maximum gradient component is 2.4e-6, so it looks to me like the model converged. 

In the future, the glmmTMB developers will reconsider the convergence check that relies on nlminb?s convergence codes.

if (!is.null(fit$convergence) && fit$convergence != 0) warning()

cheers,
Mollie


> On 11Jan 2019, at 8:53, D. Rizopoulos <d.rizopoulos at erasmusmc.nl> wrote:
> 
> For what it's worth, the same model can be fitted with nlme::gls without any warning messages:
> 
> library(nlme)
> fit4 <- gls(test ~ 0 + occasion, data = Dat, 
>            weights = varIdent(form = ~ 1 | occasion),
>            correlation = corSymm(form = ~ 1 | person))
> 
> vcov(fit4)
> 
> Best,
> Dimitris
> 
> 
> -----Original Message-----
> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of Rolf Turner
> Sent: Friday, January 11, 2019 3:30 AM
> To: Help Mixed Models <r-sig-mixed-models at r-project.org>
> Subject: [R-sig-ME] Constraining error variance to 0 in a mixed model.
> 
> 
> Let me start off by apologising for beating this issue to death on this mailing list.  I have raised it (without getting satisfactory answers) on several occasions.  If you are fed up to the teeth with my maundering s, please just press the delete button.
> 
> This latest occasion was triggered by my trying to learn to use the glmmTMB function (from the package of the same name).  I noticed (to my initial delight) that glmmTMB() has an argument "dispformula" and the help says (in particular) "In Gaussian mixed models, dispformula=~0 fixes the [dispersion] parameter to be 0, forcing variance into the random effects."
> 
> I said "Ah-ha!  The very thing."  So I tried it out, using the following data set (supplied by Ben Pelzer in a posting to this list, and previously used by my very good self to illustrate the problem that I am trying to solve):
> 
> Dat <- structure(list(person = c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 7L, 7L, 7L, 8L, 8L, 8L, 9L, 9L, 9L, 10L, 10L, 10L, 11L, 11L, 11L, 12L, 12L, 12L, 13L, 13L, 13L, 14L, 14L, 14L, 15L, 15L, 15L, 16L, 16L, 16L, 17L, 17L, 17L, 18L, 18L, 18L, 19L, 19L, 19L, 20L, 20L, 20L), occasion = structure(c(1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L), .Label = c("1", "2", "3"), class = "factor"), test = c(25L, 21L, 27L, 27L, 19L, 23L, 20L, 18L, 18L, 26L, 33L, 46L, 25L, 36L, 47L, 36L, 35L, 41L, 30L, 30L, 37L, 23L, 21L, 19L, 39L, 37L, 33L, 29L, 36L, 49L, 29L, 34L, 44L, 39L, 32L, 30L, 26L, 32L, 36L, 33L, 21L, 8L, 30L, 34L, 34L, 27L, 34L, 39L, 41L, 40L, 44L, 37L, 34L, 34L, 23L, 26L, 30L, 31L, 28L, 27L)), row.names = c(NA, -60L), class = "data.frame")
> 
> I tried three ways of "analysing" these data:
> 
> (1) Simple-minded multivariate analysis:
> 
> X   <- matrix(Dat$test,byrow=TRUE,ncol=3)
> colnames(X) <- paste0("occasion",1:3)
> mu  <- apply(X,2,mean)
> Sig <- var(X)/20 # X has 20 rows
> 
> (2) Using lmer():
> library(lme4)
> fit2 <- lmer(test ~ 0 + occasion + (0 + occasion | person),data=Dat,
>              control=lmerControl(check.nobs.vs.nRE = "ignore"))
> 
> (The "control" argument was gathered from a posting by Maarten Jung, on which I have previously commented.)
> 
> Note that this "works" but throws (as I have previously noted) a disconcerting warning message:
> 
>> Warning message:
>> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>>  Model is nearly unidentifiable: large eigenvalue ratio
>> - Rescale variables?
> 
> (3) Using glmmTMB():
> library(glmmTMB)
> fit3 <- glmmTMB(test ~ 0 + occasion + (0 + occasion | person),data=Dat,
>                 dispformula = ~0)
> 
> (No warning message; initially I said "Wheeeee!!!")
> 
> The "parameter estimates" mu, fixef(fit2) and fixef(fit3) all agree:
> 
> occasion1 occasion2 occasion3
>     29.80     30.05     33.30
> 
> The covariance Sig and vcov(fit2) agree modulo numerical noise:
> 
>> Sig
>           occasion1 occasion2 occasion3
> occasion1 1.7821053  1.150526 0.5768421
> occasion2 1.1505263  2.249868 3.0623684
> occasion3 0.5768421  3.062368 5.8531579
> 
>> vcov(fit2)
> 3 x 3 Matrix of class "dpoMatrix"
>           occasion1 occasion2 occasion3
> occasion1 1.7821054  1.150526 0.5768422
> occasion2 1.1505265  2.249868 3.0623683
> occasion3 0.5768422  3.062368 5.8531577
> 
> However vcov(fit3) differs; after some head scratching I realised that it is equal to 19/20 times the others.  After a little more head scratching I said ah-ha!  The glmmTMB() function sets REML=FALSE by default.  So I'll just call it with REML=TRUE:
> 
> fit3 <- glmmTMB(test ~ 0+occasion + (0+occasion | person),data=Dat,
>                 dispformula = ~0,REML=TRUE)
> 
> And now vcov(fit3) more-or-less agrees with the other results:
> 
>> vcov(fit3)
> Conditional model:
>           occasion1 occasion2 occasion3
> occasion1 1.7819337  1.150276 0.5765386
> occasion2 1.1502758  2.249755 3.0623895
> occasion3 0.5765386  3.062390 5.8534164
> 
> But, to my extreme irritation, glmmTMB() now also throws a warning:
> 
>> Warning message:
>> In fitTMB(TMBStruc) :
>> Model convergence problem; false convergence (8). See 
>> vignette('troubleshooting')
> 
> (I've looked at the vignette, and I can sort of see a connection, but not really.)
> 
> *Why* does the bloody universe always do this sort of thing to me?  Why on earth should dividing by 19 rather than 20, effectively, cause such a warning to be thrown?
> 
> Isn't something a little out of whack here?
> 
> I reiterate at this point a remark that I made in a previous post on the issue of constraining the error variance to 0:
> 
>> What we have here is a "boundary" or "edge" or "corner" case, and I 
>> have heard it asserted by someone knowledgeable and respected (can't 
>> remember who, it was a long while ago) that in testing your software 
>> it is crucial to see how it behaves with such "boundary" cases.
> 
> Am I being unreasonable?  (*Me*?  *Unreasonable*?  Perish the thought!) :-)
> 
> cheers,
> 
> Rolf Turner
> 
> P.S.  Note that setting REML=FALSE in the call to lmer() causes it to produce a covariance matrix that agrees with that produced by glmmTMB() with default setting.  However this has no impact upon the error message issued by lmer().
> 
> R. T.
> 
> --
> Honorary Research Fellow
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


	[[alternative HTML version deleted]]


From roy@verillmurr@y @ending from gm@il@com  Fri Jan 11 19:57:13 2019
From: roy@verillmurr@y @ending from gm@il@com (Roy Averill-Murray)
Date: Fri, 11 Jan 2019 10:57:13 -0800
Subject: [R-sig-ME] Estimated dependent variable in GL(M)Ms
Message-ID: <CA+y+9g+U9UZRoGVOL5TohqQzDXx78PkZuCYmVzPbSLq5nn6ROA@mail.gmail.com>

How does one estimate beta parameters and appropriate SEs when the
dependent variable consists of estimated values with known variances? My
case involves modeling factors affecting animal home ranges that were
estimated separately (i.e., individual home ranges are the dependent
variable, each of which has its own variance). I found the paper by Lewis
and Linzer (2005. Estimating regression models in which the dependent
variable is based on estimates. Political Analysis 13:345-364), which
describes feasible generalized least squares, but I have been unable to
find R code for this or a description of how to apply it in nlme, lme4,
glmmTMB, etc.

Thanks for any help,
Roy Averill-Murray

	[[alternative HTML version deleted]]


From tor@ten@h@uffe @ending from gm@il@com  Fri Jan 11 20:08:00 2019
From: tor@ten@h@uffe @ending from gm@il@com (Torsten Hauffe)
Date: Fri, 11 Jan 2019 20:08:00 +0100
Subject: [R-sig-ME] Estimated dependent variable in GL(M)Ms
In-Reply-To: <CA+y+9g+U9UZRoGVOL5TohqQzDXx78PkZuCYmVzPbSLq5nn6ROA@mail.gmail.com>
References: <CA+y+9g+U9UZRoGVOL5TohqQzDXx78PkZuCYmVzPbSLq5nn6ROA@mail.gmail.com>
Message-ID: <CAGCrCxYvJDQSMyJXZBJ7BQKC3SKsZYYw+CnDc=EgWTTAGJtDrQ@mail.gmail.com>

Sounds like you are looking for meta-analysis. But I'm not 100% sure about
this.

Cheers!

On Fri, 11 Jan 2019 at 19:57, Roy Averill-Murray <royaverillmurray at gmail.com>
wrote:

> How does one estimate beta parameters and appropriate SEs when the
> dependent variable consists of estimated values with known variances? My
> case involves modeling factors affecting animal home ranges that were
> estimated separately (i.e., individual home ranges are the dependent
> variable, each of which has its own variance). I found the paper by Lewis
> and Linzer (2005. Estimating regression models in which the dependent
> variable is based on estimates. Political Analysis 13:345-364), which
> describes feasible generalized least squares, but I have been unable to
> find R code for this or a description of how to apply it in nlme, lme4,
> glmmTMB, etc.
>
> Thanks for any help,
> Roy Averill-Murray
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From r@turner @ending from @uckl@nd@@c@nz  Fri Jan 11 22:07:56 2019
From: r@turner @ending from @uckl@nd@@c@nz (Rolf Turner)
Date: Sat, 12 Jan 2019 10:07:56 +1300
Subject: [R-sig-ME] Constraining error variance to 0 in a mixed model.
In-Reply-To: <7191AFC7255B4F49A30707E39BEAD05FDEC56670@EXCH-RX03.erasmusmc.nl>
References: <9fbf5a81-a680-1087-fa77-84966db5a350@auckland.ac.nz>
 <7191AFC7255B4F49A30707E39BEAD05FDEC56670@EXCH-RX03.erasmusmc.nl>
Message-ID: <6750ab51-feba-df9c-a011-db7526e0e2ba@auckland.ac.nz>


On 1/11/19 8:53 PM, D. Rizopoulos wrote:

> For what it's worth, the same model can be fitted with nlme::gls without any warning messages:
> 
> library(nlme)
> fit4 <- gls(test ~ 0 + occasion, data = Dat,
>              weights = varIdent(form = ~ 1 | occasion),
>              correlation = corSymm(form = ~ 1 | person))
> 
> vcov(fit4)

Thanks.  That's interesting, but a bit obscure from the point of view of 
a naive young ( :-) ) thing like myself.  The syntax of the call to 
gls() is mind-boggling --- at least to my feeble mind.

What I really wanted (as I have remarked in similar posts in the past) 
is to be able to fit simple mixed models in a simple-minded way, to 
check that my call to the more sophisticated software is correct.  This 
desideratum kind of falls apart if the sophisticated software 
cannot/won't fit the simple model (at least not without throwing a 
hissy-fit).

I think that what I *can* take away from your fitting procedure is that 
there are/should be no insurmountable intrinsic numerical barriers to 
fitting the simple model that I propose by means of sophisticated software.

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


> -----Original Message-----
> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of Rolf Turner
> Sent: Friday, January 11, 2019 3:30 AM
> To: Help Mixed Models <r-sig-mixed-models at r-project.org>
> Subject: [R-sig-ME] Constraining error variance to 0 in a mixed model.
> 
> 
> Let me start off by apologising for beating this issue to death on this mailing list.  I have raised it (without getting satisfactory answers) on several occasions.  If you are fed up to the teeth with my maundering s, please just press the delete button.
> 
> This latest occasion was triggered by my trying to learn to use the glmmTMB function (from the package of the same name).  I noticed (to my initial delight) that glmmTMB() has an argument "dispformula" and the help says (in particular) "In Gaussian mixed models, dispformula=~0 fixes the [dispersion] parameter to be 0, forcing variance into the random effects."
> 
> I said "Ah-ha!  The very thing."  So I tried it out, using the following data set (supplied by Ben Pelzer in a posting to this list, and previously used by my very good self to illustrate the problem that I am trying to solve):
> 
> Dat <- structure(list(person = c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 7L, 7L, 7L, 8L, 8L, 8L, 9L, 9L, 9L, 10L, 10L, 10L, 11L, 11L, 11L, 12L, 12L, 12L, 13L, 13L, 13L, 14L, 14L, 14L, 15L, 15L, 15L, 16L, 16L, 16L, 17L, 17L, 17L, 18L, 18L, 18L, 19L, 19L, 19L, 20L, 20L, 20L), occasion = structure(c(1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L), .Label = c("1", "2", "3"), class = "factor"), test = c(25L, 21L, 27L, 27L, 19L, 23L, 20L, 18L, 18L, 26L, 33L, 46L, 25L, 36L, 47L, 36L, 35L, 41L, 30L, 30L, 37L, 23L, 21L, 19L, 39L, 37L, 33L, 29L, 36L, 49L, 29L, 34L, 44L, 39L, 32L, 30L, 26L, 32L, 36L, 33L, 21L, 8L, 30L, 34L, 34L, 27L, 34L, 39L, 41L, 40L, 44L, 37L, 34L, 34L, 23L, 26L, 30L, 31L, 28L, 27L)), row.names = c(NA, -60L), class = "data.frame")
> 
> I tried three ways of "analysing" these data:
> 
> (1) Simple-minded multivariate analysis:
> 
> X   <- matrix(Dat$test,byrow=TRUE,ncol=3)
> colnames(X) <- paste0("occasion",1:3)
> mu  <- apply(X,2,mean)
> Sig <- var(X)/20 # X has 20 rows
> 
> (2) Using lmer():
> library(lme4)
> fit2 <- lmer(test ~ 0 + occasion + (0 + occasion | person),data=Dat,
>                control=lmerControl(check.nobs.vs.nRE = "ignore"))
> 
> (The "control" argument was gathered from a posting by Maarten Jung, on which I have previously commented.)
> 
> Note that this "works" but throws (as I have previously noted) a disconcerting warning message:
> 
>> Warning message:
>> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>>    Model is nearly unidentifiable: large eigenvalue ratio
>>   - Rescale variables?
> 
> (3) Using glmmTMB():
> library(glmmTMB)
> fit3 <- glmmTMB(test ~ 0 + occasion + (0 + occasion | person),data=Dat,
>                   dispformula = ~0)
> 
> (No warning message; initially I said "Wheeeee!!!")
> 
> The "parameter estimates" mu, fixef(fit2) and fixef(fit3) all agree:
> 
> occasion1 occasion2 occasion3
>       29.80     30.05     33.30
> 
> The covariance Sig and vcov(fit2) agree modulo numerical noise:
> 
>   > Sig
>             occasion1 occasion2 occasion3
> occasion1 1.7821053  1.150526 0.5768421
> occasion2 1.1505263  2.249868 3.0623684
> occasion3 0.5768421  3.062368 5.8531579
> 
>   > vcov(fit2)
> 3 x 3 Matrix of class "dpoMatrix"
>             occasion1 occasion2 occasion3
> occasion1 1.7821054  1.150526 0.5768422
> occasion2 1.1505265  2.249868 3.0623683
> occasion3 0.5768422  3.062368 5.8531577
> 
> However vcov(fit3) differs; after some head scratching I realised that it is equal to 19/20 times the others.  After a little more head scratching I said ah-ha!  The glmmTMB() function sets REML=FALSE by default.  So I'll just call it with REML=TRUE:
> 
> fit3 <- glmmTMB(test ~ 0+occasion + (0+occasion | person),data=Dat,
>                   dispformula = ~0,REML=TRUE)
> 
> And now vcov(fit3) more-or-less agrees with the other results:
> 
>   > vcov(fit3)
> Conditional model:
>             occasion1 occasion2 occasion3
> occasion1 1.7819337  1.150276 0.5765386
> occasion2 1.1502758  2.249755 3.0623895
> occasion3 0.5765386  3.062390 5.8534164
> 
> But, to my extreme irritation, glmmTMB() now also throws a warning:
> 
>> Warning message:
>> In fitTMB(TMBStruc) :
>> Model convergence problem; false convergence (8). See
>> vignette('troubleshooting')
> 
> (I've looked at the vignette, and I can sort of see a connection, but not really.)
> 
> *Why* does the bloody universe always do this sort of thing to me?  Why on earth should dividing by 19 rather than 20, effectively, cause such a warning to be thrown?
> 
> Isn't something a little out of whack here?
> 
> I reiterate at this point a remark that I made in a previous post on the issue of constraining the error variance to 0:
> 
>> What we have here is a "boundary" or "edge" or "corner" case, and I
>> have heard it asserted by someone knowledgeable and respected (can't
>> remember who, it was a long while ago) that in testing your software
>> it is crucial to see how it behaves with such "boundary" cases.
> 
> Am I being unreasonable?  (*Me*?  *Unreasonable*?  Perish the thought!) :-)
> 
> cheers,
> 
> Rolf Turner
> 
> P.S.  Note that setting REML=FALSE in the call to lmer() causes it to produce a covariance matrix that agrees with that produced by glmmTMB() with default setting.  However this has no impact upon the error message issued by lmer().
> 
> R. T.
>


From r@turner @ending from @uckl@nd@@c@nz  Fri Jan 11 22:09:02 2019
From: r@turner @ending from @uckl@nd@@c@nz (Rolf Turner)
Date: Sat, 12 Jan 2019 10:09:02 +1300
Subject: [R-sig-ME] Constraining error variance to 0 in a mixed model.
In-Reply-To: <4AAF202E-3C0B-4357-9449-3AAE4F28BE83@gmail.com>
References: <9fbf5a81-a680-1087-fa77-84966db5a350@auckland.ac.nz>
 <7191AFC7255B4F49A30707E39BEAD05FDEC56670@EXCH-RX03.erasmusmc.nl>
 <4AAF202E-3C0B-4357-9449-3AAE4F28BE83@gmail.com>
Message-ID: <02433069-2e08-526f-d11e-5736e6639f13@auckland.ac.nz>


On 1/12/19 1:40 AM, Mollie Brooks wrote:

> Hi Rolf,
> 
> Thanks for the enthusiasm. I believe the ability to force the variance 
> into the RE in glmmTMB was Kasper Kristensen?s idea.
> 
> I had a look at your example and it seems that it could be a false 
> positive coming from nlminb. The maximum gradient component is 2.4e-6, 
> so it looks to me like the model converged.
> 
> In the future, the glmmTMB developers will reconsider the convergence 
> check that relies on nlminb?s convergence codes.
> 
> if (!is.null(fit$convergence) && fit$convergence != 0) warning()

Thanks. That's encouraging.

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From @vik @ending from @@vion@huji@@c@il  Sat Jan 12 08:01:02 2019
From: @vik @ending from @@vion@huji@@c@il (Avraham Kluger)
Date: Sat, 12 Jan 2019 07:01:02 +0000
Subject: [R-sig-ME] Correlations among random variables
Message-ID: <268119BD809ADB44968F39A7A86398FA017177FEC4@Pegasus2.hustaff.huji.local>

Hi,

I am struggling to analyze, in R, MLM models that specify correlations among random variables, as can be done with SPSS, SAS, or MlWin.

Consider the following code in SPSS
-----------------------------
MIXED
   Outcome  BY role  WITH focalcode partcode
   /FIXED = focalcode partcode | NOINT
   /PRINT = SOLUTION TESTCOV
   /RANDOM focalcode partcode | SUBJECT(focalid) COVTYPE(UNR)
   /REPEATED = role | SUBJECT(focalid*dyadid) COVTYPE(UNR).
-----------------------------
And a minimal code (with data) in R

-----------------------------
df <- read.csv("https://raw.githubusercontent.com/avi-kluger/RCompanion4DDABook/master/Chapter%2010/Chapter10_df.csv")
head(df)
library(lme4)

mlm <- lmer(outcome   ~ 0 + focalcode + partcode + role +
                       (0 + focalcode + partcode|| focalid/ dyadid),
                       data = df)
summary(mlm)
-----------------------------

These SPSS and R codes produce the same variance estimates.  However, SPSS also produces a correlation among "focalcode" and "partcode."  How can this be done in R?  Is it also possible to produce the correlation among the respective error variances (as in SPSS)?

Additional information


1.       MOTIVATION.  The question arises from David Kenny's work on one-with-many reciprocal designs (e.g., a manager rate all subordinates, and all subordinates rate the same manager).  These models estimate the variance stemming from the one (e.g., managers) and the many (e.g., subordinates), and the correlation among them (termed generalized reciprocity).  The data and codes for SAS etc. are available at http://davidakenny.net/kkc/c10/c10.htm.

2.       SPSS OUTPUT (download HTML file):  https://www.dropbox.com/s/eqch0kq6djtbsfx/One%20with%20many%20SPSS%20output.htm?dl=1

Sincerely,

Avi Kluger
https://www.avi-kluger.com/


	[[alternative HTML version deleted]]


From d@luedecke m@ili@g off uke@de  Sat Jan 12 10:28:01 2019
From: d@luedecke m@ili@g off uke@de (d@luedecke m@ili@g off uke@de)
Date: Sat, 12 Jan 2019 10:28:01 +0100
Subject: [R-sig-ME] Correlations among random variables
In-Reply-To: <268119BD809ADB44968F39A7A86398FA017177FEC4@Pegasus2.hustaff.huji.local>
References: <268119BD809ADB44968F39A7A86398FA017177FEC4@Pegasus2.hustaff.huji.local>
Message-ID: <000001d4aa59$1d661e70$58325b50$@uke.de>

Hi Avi,

You can find some of the numbers from the covariance parameters from the
SPSS output also in the "summary()" from your model. Other parameters don't
match, maybe the random effects structure needs to be specified in a
different way? However, I'm not sure how to translate the rather "confusing"
SPSS notation into R-syntax.

Best
Daniel

-----Urspr?ngliche Nachricht-----
Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im
Auftrag von Avraham Kluger
Gesendet: Samstag, 12. Januar 2019 08:01
An: r-sig-mixed-models at r-project.org
Cc: Michal Lehmann <chikush at gmail.com>; Kenny, David
<david.kenny at uconn.edu>; Sarit Pery <sarit at peryjoy.com>
Betreff: [R-sig-ME] Correlations among random variables

Hi,

I am struggling to analyze, in R, MLM models that specify correlations among
random variables, as can be done with SPSS, SAS, or MlWin.

Consider the following code in SPSS
-----------------------------
MIXED
   Outcome  BY role  WITH focalcode partcode
   /FIXED = focalcode partcode | NOINT
   /PRINT = SOLUTION TESTCOV
   /RANDOM focalcode partcode | SUBJECT(focalid) COVTYPE(UNR)
   /REPEATED = role | SUBJECT(focalid*dyadid) COVTYPE(UNR).
-----------------------------
And a minimal code (with data) in R

-----------------------------
df <-
read.csv("https://raw.githubusercontent.com/avi-kluger/RCompanion4DDABook/ma
ster/Chapter%2010/Chapter10_df.csv")
head(df)
library(lme4)

mlm <- lmer(outcome   ~ 0 + focalcode + partcode + role +
                       (0 + focalcode + partcode|| focalid/ dyadid),
                       data = df)
summary(mlm)
-----------------------------

These SPSS and R codes produce the same variance estimates.  However, SPSS
also produces a correlation among "focalcode" and "partcode."  How can this
be done in R?  Is it also possible to produce the correlation among the
respective error variances (as in SPSS)?

Additional information


1.       MOTIVATION.  The question arises from David Kenny's work on
one-with-many reciprocal designs (e.g., a manager rate all subordinates, and
all subordinates rate the same manager).  These models estimate the variance
stemming from the one (e.g., managers) and the many (e.g., subordinates),
and the correlation among them (termed generalized reciprocity).  The data
and codes for SAS etc. are available at
http://davidakenny.net/kkc/c10/c10.htm.

2.       SPSS OUTPUT (download HTML file):
https://www.dropbox.com/s/eqch0kq6djtbsfx/One%20with%20many%20SPSS%20output.
htm?dl=1

Sincerely,

Avi Kluger
https://www.avi-kluger.com/


	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Prof. Dr. Dr. Uwe Koch-Gromus, Joachim Pr?l?, Marya Verdel
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING


From @vik @ending from @@vion@huji@@c@il  Sat Jan 12 10:49:24 2019
From: @vik @ending from @@vion@huji@@c@il (Avraham Kluger)
Date: Sat, 12 Jan 2019 09:49:24 +0000
Subject: [R-sig-ME] Correlations among random variables
In-Reply-To: <000001d4aa59$1d661e70$58325b50$@uke.de>
References: <268119BD809ADB44968F39A7A86398FA017177FEC4@Pegasus2.hustaff.huji.local>
 <000001d4aa59$1d661e70$58325b50$@uke.de>
Message-ID: <268119BD809ADB44968F39A7A86398FA01717802BA@Pegasus2.hustaff.huji.local>

Dear Daniel,

I thank you very much for your note.  In the meantime, my student Limor Borut solved the problem with lme:

mlm <- lme(outcome ~  0 + focalcode + 0 + partcode, random = ~ 0 + focalcode + partcode|focalid/dyadid, data = df)

Best,

Avi

-----Original Message-----
From: d.luedecke at uke.de [mailto:d.luedecke at uke.de] 
Sent: Saturday, January 12, 2019 11:28 AM
To: Avraham Kluger <avik at savion.huji.ac.il>; r-sig-mixed-models at r-project.org
Subject: AW: [R-sig-ME] Correlations among random variables

Hi Avi,

You can find some of the numbers from the covariance parameters from the SPSS output also in the "summary()" from your model. Other parameters don't match, maybe the random effects structure needs to be specified in a different way? However, I'm not sure how to translate the rather "confusing"
SPSS notation into R-syntax.

Best
Daniel

-----Urspr?ngliche Nachricht-----
Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im Auftrag von Avraham Kluger
Gesendet: Samstag, 12. Januar 2019 08:01
An: r-sig-mixed-models at r-project.org
Cc: Michal Lehmann <chikush at gmail.com>; Kenny, David <david.kenny at uconn.edu>; Sarit Pery <sarit at peryjoy.com>
Betreff: [R-sig-ME] Correlations among random variables

Hi,

I am struggling to analyze, in R, MLM models that specify correlations among random variables, as can be done with SPSS, SAS, or MlWin.

Consider the following code in SPSS
-----------------------------
MIXED
   Outcome  BY role  WITH focalcode partcode
   /FIXED = focalcode partcode | NOINT
   /PRINT = SOLUTION TESTCOV
   /RANDOM focalcode partcode | SUBJECT(focalid) COVTYPE(UNR)
   /REPEATED = role | SUBJECT(focalid*dyadid) COVTYPE(UNR).
-----------------------------
And a minimal code (with data) in R

-----------------------------
df <-
read.csv("https://raw.githubusercontent.com/avi-kluger/RCompanion4DDABook/ma
ster/Chapter%2010/Chapter10_df.csv")
head(df)
library(lme4)

mlm <- lmer(outcome   ~ 0 + focalcode + partcode + role +
                       (0 + focalcode + partcode|| focalid/ dyadid),
                       data = df)
summary(mlm)
-----------------------------

These SPSS and R codes produce the same variance estimates.  However, SPSS also produces a correlation among "focalcode" and "partcode."  How can this be done in R?  Is it also possible to produce the correlation among the respective error variances (as in SPSS)?

Additional information


1.       MOTIVATION.  The question arises from David Kenny's work on
one-with-many reciprocal designs (e.g., a manager rate all subordinates, and all subordinates rate the same manager).  These models estimate the variance stemming from the one (e.g., managers) and the many (e.g., subordinates), and the correlation among them (termed generalized reciprocity).  The data and codes for SAS etc. are available at http://davidakenny.net/kkc/c10/c10.htm.

2.       SPSS OUTPUT (download HTML file):
https://www.dropbox.com/s/eqch0kq6djtbsfx/One%20with%20many%20SPSS%20output.
htm?dl=1

Sincerely,

Avi Kluger
https://www.avi-kluger.com/


	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Prof. Dr. Dr. Uwe Koch-Gromus, Joachim Pr?l?, Marya Verdel _____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING


From u@nhoro@1 @ending from buckeyem@il@o@u@edu  Sat Jan 12 14:16:45 2019
From: u@nhoro@1 @ending from buckeyem@il@o@u@edu (Uanhoro, James)
Date: Sat, 12 Jan 2019 13:16:45 +0000
Subject: [R-sig-ME] Correlations among random variables
In-Reply-To: <268119BD809ADB44968F39A7A86398FA017177FEC4@Pegasus2.hustaff.huji.local>
Message-ID: <924de780-cb25-41db-b4dc-f50000d0bc59@email.android.com>

In the lme4 syntax, you'd have to change the double pipe, ||, when specifying the random effects to a single pipe, |, to permit a correlation between random effects. lme4 is faster than nlme.

Assuming lme4 and nlme are the only options ... If you want to specify an error covariance structure beyond the covariance structure implied by standard multilevel models, you will have to use nlme. nlme has a `correlation =` argument that allows different covariance structures, corSymm (general/unstructured), corCompSymm (exchangeable), ...


On Jan 12, 2019 02:01, Avraham Kluger <avik at savion.huji.ac.il> wrote:

Hi,

I am struggling to analyze, in R, MLM models that specify correlations among random variables, as can be done with SPSS, SAS, or MlWin.

Consider the following code in SPSS
-----------------------------
MIXED
   Outcome  BY role  WITH focalcode partcode
   /FIXED = focalcode partcode | NOINT
   /PRINT = SOLUTION TESTCOV
   /RANDOM focalcode partcode | SUBJECT(focalid) COVTYPE(UNR)
   /REPEATED = role | SUBJECT(focalid*dyadid) COVTYPE(UNR).
-----------------------------
And a minimal code (with data) in R

-----------------------------
df <- read.csv("https://raw.githubusercontent.com/avi-kluger/RCompanion4DDABook/master/Chapter%2010/Chapter10_df.csv")
head(df)
library(lme4)

mlm <- lmer(outcome   ~ 0 + focalcode + partcode + role +
                       (0 + focalcode + partcode|| focalid/ dyadid),
                       data = df)
summary(mlm)
-----------------------------

These SPSS and R codes produce the same variance estimates.  However, SPSS also produces a correlation among "focalcode" and "partcode."  How can this be done in R?  Is it also possible to produce the correlation among the respective error variances (as in SPSS)?

Additional information


1.       MOTIVATION.  The question arises from David Kenny's work on one-with-many reciprocal designs (e.g., a manager rate all subordinates, and all subordinates rate the same manager).  These models estimate the variance stemming from the one (e.g., managers) and the many (e.g., subordinates), and the correlation among them (termed generalized reciprocity).  The data and codes for SAS etc. are available at http://davidakenny.net/kkc/c10/c10.htm.

2.       SPSS OUTPUT (download HTML file):  https://www.dropbox.com/s/eqch0kq6djtbsfx/One%20with%20many%20SPSS%20output.htm?dl=1

Sincerely,

Avi Kluger
https://www.avi-kluger.com/


[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


	[[alternative HTML version deleted]]


From d@rizopoulo@ @ending from er@@mu@mc@nl  Sat Jan 12 20:35:33 2019
From: d@rizopoulo@ @ending from er@@mu@mc@nl (D. Rizopoulos)
Date: Sat, 12 Jan 2019 19:35:33 +0000
Subject: [R-sig-ME] Constraining error variance to 0 in a mixed model.
In-Reply-To: <6750ab51-feba-df9c-a011-db7526e0e2ba@auckland.ac.nz>
References: <9fbf5a81-a680-1087-fa77-84966db5a350@auckland.ac.nz>
 <7191AFC7255B4F49A30707E39BEAD05FDEC56670@EXCH-RX03.erasmusmc.nl>
 <6750ab51-feba-df9c-a011-db7526e0e2ba@auckland.ac.nz>
Message-ID: <7191AFC7255B4F49A30707E39BEAD05FDEC57EAA@EXCH-RX03.erasmusmc.nl>

The call to gls() is the one that fits an unstructured covariance matrix for the error terms. Namely, the varIdent() function in the weights arguments specifies that we want a difference variance parameter per occasion, and the corSymm() function in the correlation argument specifies that we want a general symmetric correlation matrix for the measurements of each person.

In any case, a much simpler call to lme() also seems to work, i.e.,

fit5 <- lme(test ~ 0 + occasion, random = ~ 0 + occasion | person, data = Dat)

vcov(fit5)


Best,
Dimitris


-----Original Message-----
From: Rolf Turner <r.turner at auckland.ac.nz> 
Sent: Friday, January 11, 2019 10:08 PM
To: D. Rizopoulos <d.rizopoulos at erasmusmc.nl>
Cc: Help Mixed Models <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Constraining error variance to 0 in a mixed model.


On 1/11/19 8:53 PM, D. Rizopoulos wrote:

> For what it's worth, the same model can be fitted with nlme::gls without any warning messages:
> 
> library(nlme)
> fit4 <- gls(test ~ 0 + occasion, data = Dat,
>              weights = varIdent(form = ~ 1 | occasion),
>              correlation = corSymm(form = ~ 1 | person))
> 
> vcov(fit4)

Thanks.  That's interesting, but a bit obscure from the point of view of a naive young ( :-) ) thing like myself.  The syntax of the call to
gls() is mind-boggling --- at least to my feeble mind.

What I really wanted (as I have remarked in similar posts in the past) is to be able to fit simple mixed models in a simple-minded way, to check that my call to the more sophisticated software is correct.  This desideratum kind of falls apart if the sophisticated software cannot/won't fit the simple model (at least not without throwing a hissy-fit).

I think that what I *can* take away from your fitting procedure is that there are/should be no insurmountable intrinsic numerical barriers to fitting the simple model that I propose by means of sophisticated software.

cheers,

Rolf

--
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


> -----Original Message-----
> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of Rolf Turner
> Sent: Friday, January 11, 2019 3:30 AM
> To: Help Mixed Models <r-sig-mixed-models at r-project.org>
> Subject: [R-sig-ME] Constraining error variance to 0 in a mixed model.
> 
> 
> Let me start off by apologising for beating this issue to death on this mailing list.  I have raised it (without getting satisfactory answers) on several occasions.  If you are fed up to the teeth with my maundering s, please just press the delete button.
> 
> This latest occasion was triggered by my trying to learn to use the glmmTMB function (from the package of the same name).  I noticed (to my initial delight) that glmmTMB() has an argument "dispformula" and the help says (in particular) "In Gaussian mixed models, dispformula=~0 fixes the [dispersion] parameter to be 0, forcing variance into the random effects."
> 
> I said "Ah-ha!  The very thing."  So I tried it out, using the following data set (supplied by Ben Pelzer in a posting to this list, and previously used by my very good self to illustrate the problem that I am trying to solve):
> 
> Dat <- structure(list(person = c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 7L, 7L, 7L, 8L, 8L, 8L, 9L, 9L, 9L, 10L, 10L, 10L, 11L, 11L, 11L, 12L, 12L, 12L, 13L, 13L, 13L, 14L, 14L, 14L, 15L, 15L, 15L, 16L, 16L, 16L, 17L, 17L, 17L, 18L, 18L, 18L, 19L, 19L, 19L, 20L, 20L, 20L), occasion = structure(c(1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L), .Label = c("1", "2", "3"), class = "factor"), test = c(25L, 21L, 27L, 27L, 19L, 23L, 20L, 18L, 18L, 26L, 33L, 46L, 25L, 36L, 47L, 36L, 35L, 41L, 30L, 30L, 37L, 23L, 21L, 19L, 39L, 37L, 33L, 29L, 36L, 49L, 29L, 34L, 44L, 39L, 32L, 30L, 26L, 32L, 36L, 33L, 21L, 8L, 30L, 34L, 34L, 27L, 34L, 39L, 41L, 40L, 44L, 37L, 34L, 34L, 23L, 26L, 30L, 31L, 28L, 27L)), row.names = c(NA, -60L), class = "data.frame")
> 
> I tried three ways of "analysing" these data:
> 
> (1) Simple-minded multivariate analysis:
> 
> X   <- matrix(Dat$test,byrow=TRUE,ncol=3)
> colnames(X) <- paste0("occasion",1:3)
> mu  <- apply(X,2,mean)
> Sig <- var(X)/20 # X has 20 rows
> 
> (2) Using lmer():
> library(lme4)
> fit2 <- lmer(test ~ 0 + occasion + (0 + occasion | person),data=Dat,
>                control=lmerControl(check.nobs.vs.nRE = "ignore"))
> 
> (The "control" argument was gathered from a posting by Maarten Jung, on which I have previously commented.)
> 
> Note that this "works" but throws (as I have previously noted) a disconcerting warning message:
> 
>> Warning message:
>> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>>    Model is nearly unidentifiable: large eigenvalue ratio
>>   - Rescale variables?
> 
> (3) Using glmmTMB():
> library(glmmTMB)
> fit3 <- glmmTMB(test ~ 0 + occasion + (0 + occasion | person),data=Dat,
>                   dispformula = ~0)
> 
> (No warning message; initially I said "Wheeeee!!!")
> 
> The "parameter estimates" mu, fixef(fit2) and fixef(fit3) all agree:
> 
> occasion1 occasion2 occasion3
>       29.80     30.05     33.30
> 
> The covariance Sig and vcov(fit2) agree modulo numerical noise:
> 
>   > Sig
>             occasion1 occasion2 occasion3
> occasion1 1.7821053  1.150526 0.5768421
> occasion2 1.1505263  2.249868 3.0623684
> occasion3 0.5768421  3.062368 5.8531579
> 
>   > vcov(fit2)
> 3 x 3 Matrix of class "dpoMatrix"
>             occasion1 occasion2 occasion3
> occasion1 1.7821054  1.150526 0.5768422
> occasion2 1.1505265  2.249868 3.0623683
> occasion3 0.5768422  3.062368 5.8531577
> 
> However vcov(fit3) differs; after some head scratching I realised that it is equal to 19/20 times the others.  After a little more head scratching I said ah-ha!  The glmmTMB() function sets REML=FALSE by default.  So I'll just call it with REML=TRUE:
> 
> fit3 <- glmmTMB(test ~ 0+occasion + (0+occasion | person),data=Dat,
>                   dispformula = ~0,REML=TRUE)
> 
> And now vcov(fit3) more-or-less agrees with the other results:
> 
>   > vcov(fit3)
> Conditional model:
>             occasion1 occasion2 occasion3
> occasion1 1.7819337  1.150276 0.5765386
> occasion2 1.1502758  2.249755 3.0623895
> occasion3 0.5765386  3.062390 5.8534164
> 
> But, to my extreme irritation, glmmTMB() now also throws a warning:
> 
>> Warning message:
>> In fitTMB(TMBStruc) :
>> Model convergence problem; false convergence (8). See
>> vignette('troubleshooting')
> 
> (I've looked at the vignette, and I can sort of see a connection, but not really.)
> 
> *Why* does the bloody universe always do this sort of thing to me?  Why on earth should dividing by 19 rather than 20, effectively, cause such a warning to be thrown?
> 
> Isn't something a little out of whack here?
> 
> I reiterate at this point a remark that I made in a previous post on the issue of constraining the error variance to 0:
> 
>> What we have here is a "boundary" or "edge" or "corner" case, and I
>> have heard it asserted by someone knowledgeable and respected (can't
>> remember who, it was a long while ago) that in testing your software
>> it is crucial to see how it behaves with such "boundary" cases.
> 
> Am I being unreasonable?  (*Me*?  *Unreasonable*?  Perish the thought!) :-)
> 
> cheers,
> 
> Rolf Turner
> 
> P.S.  Note that setting REML=FALSE in the call to lmer() causes it to produce a covariance matrix that agrees with that produced by glmmTMB() with default setting.  However this has no impact upon the error message issued by lmer().
> 
> R. T.
> 



From @vik @ending from @@vion@huji@@c@il  Sun Jan 13 04:42:01 2019
From: @vik @ending from @@vion@huji@@c@il (Avraham Kluger)
Date: Sun, 13 Jan 2019 03:42:01 +0000
Subject: [R-sig-ME] Correlations among random variables
In-Reply-To: <268119BD809ADB44968F39A7A86398FA0171780E64@Pegasus2.hustaff.huji.local>
References: <268119BD809ADB44968F39A7A86398FA0171780A78@Pegasus2.hustaff.huji.local>,
 <1547306566.local-6baa2fdf-d400-v1.5.5-b7939d38@getmailspring.com>
 <268119BD809ADB44968F39A7A86398FA0171780E64@Pegasus2.hustaff.huji.local>
Message-ID: <268119BD809ADB44968F39A7A86398FA01717814B7@Pegasus2.hustaff.huji.local>

Hi,

Following help from James Uanhoro, I produced models with correlated random variables both with nlms and lme4.  Curiously, the estimates of the variances and their correlation are identical, but the error variances and their correlation are not.  Yet, the sum of the error variances are identical.  For example, in the nlme code below the error for focalcode + residual is .376 + .046 = .422, and in the lme4 it is .2 + .222 = .422.  Can anyone guide me to read about these different decompositions?  This may explain the different correlations among the error terms .265 vs. .451.

MLM with nlme
          Variance                            StdDev    Corr
focalid = pdLogChol(0 + focalcode + partcode)
focalcode 0.20840953                          0.4565189 foclcd
partcode  0.06089854                          0.2467763 0.699
dyadid =  pdLogChol(0 + focalcode + partcode)
focalcode 0.37674500                          0.6137956 foclcd
partcode  0.50282362                          0.7091006 0.265
Residual  0.04641050                          0.2154310

MLM with lme4

> as.data.frame(VarCorr(lme4Mlm))
             grp      var1     var2       vcov     sdcor
1 dyadid:focalid focalcode     <NA> 0.20018050 0.4474153
2 dyadid:focalid  partcode     <NA> 0.32625940 0.5711912
3 dyadid:focalid focalcode partcode 0.11523355 0.4509065
4        focalid focalcode     <NA> 0.20840930 0.4565187
5        focalid  partcode     <NA> 0.06089846 0.2467761
6        focalid focalcode partcode 0.07872740 0.6988182
7       Residual      <NA>     <NA> 0.22297497 0.4722023
> VarCorr(lme4Mlm)
Groups         Name      Std.Dev. Corr
 dyadid:focalid focalcode 0.44742
                partcode  0.57119  0.451
focalid        focalcode 0.45652
                partcode  0.24678  0.699
Residual                 0.47220

Yours,

Avi Kluger<http://avikluger.wix.com/avi-kluger>






From: Uanhoro, James<mailto:uanhoro.1 at buckeyemail.osu.edu>
Sent: Saturday, January 12, 2019 5:38 PM
To: Avraham Kluger<mailto:avik at savion.huji.ac.il>
Subject: Re: [R-sig-ME] Correlations among random variables

That is the exact lme4 syntax that replicates the nlme model. I did not attempt to run it but when I did, I noticed that the problem is you have two random effects, each having 624 values, 624 by 2 equals the sample size. lme4 will not run when this happens hence the error message. You can tell lme4 to run nevertheless

summary(mlms <- lmer(
  outcome ~ 0 + focalcode + partcode +
    (0 + focalcode + partcode | focalid/dyadid),
  Chapter10_df,
  control = lmerControl(check.nobs.vs.nRE = "warning")))

This will force the program to run the code, and will print out warnings. The log likelihood was the same with that from nlme indicating that the models are the same. But the variance-covariance matrices for the random effects by the interaction between focalid and dyadid - the inner cluster variable - are different. Which one do you trust? Probably neither - there is just not enough information to estimate this varCov matrix. See this thread for commentary on the issue: https://github.com/lme4/lme4/issues/175

Hope this helps, -James.



On Jan 12 2019, at 9:49 am, Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>> wrote:
Oops,

Actually the code produces error




Error: number of observations (=1248) <= number of random effects (=1248) for term (0 + focalcode + partcode | dyadid:focalid); the random-effects parameters and the residual variance (or scale parameter) are probably unidentifiable





The code below should run on any machine.





Best





Avi



>






################################################################################

#    **************************** R companion for **************************

#

# Kenny, D. A., Kashy, D. A., & Cook, W. L. (2006). Dyadic data analysis.

# New York: Guilford Press.

#

# lme code developed by Limor Borut: limor.borut at mail.huji.ac.il<mailto:limor.borut at mail.huji.ac.il>

# written by Avi Kluger: avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>

#

#                              CHAPTER 10 -- one with many SRM

#

################################################################################

rm(list = ls())                               # Clean the Global Environment

cat ("\014")                                  # Clean the R console

if (is.null(dev.list()) == FALSE) dev.off()   # Clean Plots



# Read (in SPSS format) from Kenny's book site and replicate Table 9.1

if (!require('foreign')) install.packages('foreign'); library('foreign')

Chapter10_df <- read.spss("http://davidakenny.net/kkc/c10/c10_recip.sav",

               to.data.frame = TRUE, use.value.labels = FALSE)



head(Chapter10_df)



if (!require("nlme")) install.packages("nlme"); suppressMessages(library(nlme))



mlm <- lme(outcome ~   0 + focalcode + 0 + partcode,

            random = ~ 0 + focalcode + partcode|focalid/dyadid,

            data = Chapter10_df)

summary(mlm)

intervals(mlm)

mlmOutput <- VarCorr(mlm)

VarCorr(mlm)



cat(

"Actor   variance = ",   round(as.numeric(VarCorr(mlm)[, "Variance"][3]), 3),

"\nPartner variance = ", round(as.numeric(VarCorr(mlm)[, "Variance"][2]), 3),

"\nGeneralized Reciprocity = ", round(as.numeric(VarCorr(mlm)[, "Corr"][3]), 3),

"\nDyadic Reciprocity = ", round(as.numeric(VarCorr(mlm)[, "Corr"][6]), 3), "\n"

)



# Very Important Note.  The original data coded with 0 the focal person.

# Therefore the first random variable above is partner variance.  Reversing

# the codes below make the results more intuitive.  I thank David Kenny for

# Clarifying this issue.

Chapter10_df$focalcode <- 1- Chapter10_df$focalcode

Chapter10_df$partcode  <- 1- Chapter10_df$partcode

mlm <- lme(outcome ~   0 + focalcode + 0 + partcode,

            random = ~ 0 + focalcode + partcode|focalid/dyadid,

            data = Chapter10_df)

VarCorr(mlm)



# An alternative suggsted by James Uanhoro <uanhoro.1 at buckeyemail.osu.edu<mailto:uanhoro.1 at buckeyemail.osu.edu>>



if (!require("lme4")) install.packages("lme4"); suppressMessages(library(lme4))

mlm <- lmer(outcome   ~ 0 + focalcode + partcode + role +

                       (0 + focalcode + partcode | focalid/ dyadid),

                       data = Chapter10_df)

summary(mlm)

VarCorr(mlm)







From: Uanhoro, James [mailto:uanhoro.1 at buckeyemail.osu.edu]
Sent: Saturday, January 12, 2019 4:41 PM
To: Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>
Subject: RE: [R-sig-ME] Correlations among random variables


My reply addressed to issues: covariance between error terms; and between random effects.
The only change to lme4 for random effects is to switch the double pipe to a single pipe in the random effects specification of the model, as I have done below:

mlm <- lmer(outcome   ~ 0 + focalcode + partcode + role +
                       (0 + focalcode + partcode | focalid/ dyadid),
                       data = df)

James.



On Jan 12, 2019 09:05, Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>> wrote:

Dear James,



As you might have seen in my second message to r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>, my student solved this problem with nlme.  Would you know how to write it in lme4?



Here is the working nlme code



mlm <- lme(outcome ~   0 + focalcode + 0 + partcode,

            random = ~ 0 + focalcode + partcode|focalid/dyadid,

            data = Chapter10_df)



Best,



Avi







From: Uanhoro, James [mailto:uanhoro.1 at buckeyemail.osu.edu]
Sent: Saturday, January 12, 2019 3:17 PM
To: Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>
Cc: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Correlations among random variables


In the lme4 syntax, you'd have to change the double pipe, ||, when specifying the random effects to a single pipe, |, to permit a correlation between random effects. lme4 is faster than nlme.

Assuming lme4 and nlme are the only options ... If you want to specify an error covariance structure beyond the covariance structure implied by standard multilevel models, you will have to use nlme. nlme has a `correlation =` argument that allows different covariance structures, corSymm (general/unstructured), corCompSymm (exchangeable), ...




On Jan 12, 2019 02:01, Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>> wrote:

Hi,

I am struggling to analyze, in R, MLM models that specify correlations among random variables, as can be done with SPSS, SAS, or MlWin.

Consider the following code in SPSS
-----------------------------
MIXED
   Outcome  BY role  WITH focalcode partcode
   /FIXED = focalcode partcode | NOINT
   /PRINT = SOLUTION TESTCOV
   /RANDOM focalcode partcode | SUBJECT(focalid) COVTYPE(UNR)
   /REPEATED = role | SUBJECT(focalid*dyadid) COVTYPE(UNR).
-----------------------------
And a minimal code (with data) in R

-----------------------------
df <- read.csv("https://raw.githubusercontent.com/avi-kluger/RCompanion4DDABook/master/Chapter%2010/Chapter10_df.csv")
head(df)
library(lme4)

mlm <- lmer(outcome   ~ 0 + focalcode + partcode + role +
                       (0 + focalcode + partcode|| focalid/ dyadid),
                       data = df)
summary(mlm)
-----------------------------

These SPSS and R codes produce the same variance estimates.  However, SPSS also produces a correlation among "focalcode" and "partcode."  How can this be done in R?  Is it also possible to produce the correlation among the respective error variances (as in SPSS)?

Additional information

1.       MOTIVATION.  The question arises from David Kenny's work on one-with-many reciprocal designs (e.g., a manager rate all subordinates, and all subordinates rate the same manager).  These models estimate the variance stemming from the one (e.g., managers) and the many (e.g., subordinates), and the correlation among them (termed generalized reciprocity).  The data and codes for SAS etc. are available at http://davidakenny.net/kkc/c10/c10.htm.

2.       SPSS OUTPUT (download HTML file):  https://www.dropbox.com/s/eqch0kq6djtbsfx/One%20with%20many%20SPSS%20output.htm?dl=1

Sincerely,

Avi Kluger
https://www.avi-kluger.com/

[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



	[[alternative HTML version deleted]]


From wolfg@ng@viechtb@uer @ending from m@@@trichtuniver@ity@nl  Sun Jan 13 22:36:12 2019
From: wolfg@ng@viechtb@uer @ending from m@@@trichtuniver@ity@nl (Viechtbauer, Wolfgang (SP))
Date: Sun, 13 Jan 2019 21:36:12 +0000
Subject: [R-sig-ME] Estimated dependent variable in GL(M)Ms
In-Reply-To: <CAGCrCxYvJDQSMyJXZBJ7BQKC3SKsZYYw+CnDc=EgWTTAGJtDrQ@mail.gmail.com>
References: <CA+y+9g+U9UZRoGVOL5TohqQzDXx78PkZuCYmVzPbSLq5nn6ROA@mail.gmail.com>
 <CAGCrCxYvJDQSMyJXZBJ7BQKC3SKsZYYw+CnDc=EgWTTAGJtDrQ@mail.gmail.com>
Message-ID: <efa3b9347c1e49388118f19e7b3a48f6@UM-MAIL3214.unimaas.nl>

Hi Roy,

I took a look at the paper by Lewis and Linzer. As Torsten already indicated, what they describe *is* meta-analysis (except that they make no reference to this at all). Meta-analytic method allow you to model outcomes that are estimates with know variances / standard errors. Packages metafor (of which I am the author) or meta (by Guido Schwarzer) are good starting points. As a simple example, let's say you have the following estimates and variances:

yi <- c(-0.889, -1.585, -1.348, -1.442, -0.218, -0.786, -1.621, 0.012, -0.469, -1.371, -0.339, 0.446, -0.017)
vi <- c(0.326, 0.195, 0.415, 0.02, 0.051, 0.007, 0.223, 0.004, 0.056, 0.073, 0.012, 0.533, 0.071)

Then you can analyze (i.e., pool) these estimates with:

library(metafor)
res <- rma(yi, vi)
res

Predictors can be added as well:

xi <- c(44, 55, 42, 52, 13, 44, 19, 13, 27, 42, 18, 33, 33)
res <- rma(yi ~ xi, vi)
res

This is essentially approach 4.3 in the paper, except that this uses REML estimation instead of the (rather inefficient) estimator of sigma^2 described in the paper.

The OLS + robust standard errors approach described in the paper would be:

id <- 1:13
res <- rma(yi ~ xi, vi, weights=1)
robust(res, cluster=id)

But the estimates of the model coefficients are not efficient (since they are estimated using OLS). This would be better:

res <- rma(yi ~ xi, vi)
robust(res, cluster=id)

Best,
Wolfgang

-- 
Wolfgang Viechtbauer, Ph.D., Statistician | Department of Psychiatry and 
Neuropsychology | Maastricht University | P.O. Box 616 (VIJV1) | 6200 MD 
Maastricht, The Netherlands | +31 (43) 388-4170 | http://www.wvbauer.com 

>-----Original Message-----
>From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-
>project.org] On Behalf Of Torsten Hauffe
>Sent: Friday, 11 January, 2019 20:08
>To: Roy Averill-Murray
>Cc: r-sig-mixed-models at r-project.org
>Subject: Re: [R-sig-ME] Estimated dependent variable in GL(M)Ms
>
>Sounds like you are looking for meta-analysis. But I'm not 100% sure
>about
>this.
>
>Cheers!
>
>On Fri, 11 Jan 2019 at 19:57, Roy Averill-Murray
><royaverillmurray at gmail.com>
>wrote:
>
>> How does one estimate beta parameters and appropriate SEs when the
>> dependent variable consists of estimated values with known variances?
>My
>> case involves modeling factors affecting animal home ranges that were
>> estimated separately (i.e., individual home ranges are the dependent
>> variable, each of which has its own variance). I found the paper by
>Lewis
>> and Linzer (2005. Estimating regression models in which the dependent
>> variable is based on estimates. Political Analysis 13:345-364), which
>> describes feasible generalized least squares, but I have been unable to
>> find R code for this or a description of how to apply it in nlme, lme4,
>> glmmTMB, etc.
>>
>> Thanks for any help,
>> Roy Averill-Murray


From wolfg@ng@viechtb@uer @ending from m@@@trichtuniver@ity@nl  Mon Jan 14 13:28:32 2019
From: wolfg@ng@viechtb@uer @ending from m@@@trichtuniver@ity@nl (Viechtbauer, Wolfgang (SP))
Date: Mon, 14 Jan 2019 12:28:32 +0000
Subject: [R-sig-ME] Correlations among random variables
In-Reply-To: <268119BD809ADB44968F39A7A86398FA01717814B7@Pegasus2.hustaff.huji.local>
References: <268119BD809ADB44968F39A7A86398FA0171780A78@Pegasus2.hustaff.huji.local>, 
 <1547306566.local-6baa2fdf-d400-v1.5.5-b7939d38@getmailspring.com>
 <268119BD809ADB44968F39A7A86398FA0171780E64@Pegasus2.hustaff.huji.local>
 <268119BD809ADB44968F39A7A86398FA01717814B7@Pegasus2.hustaff.huji.local>
Message-ID: <07379d113b434eecaa13a06bbd80dab3@UM-MAIL3214.unimaas.nl>

I did not follow the whole thread, but what you describe, Avi, clearly suggests an overparameterized model. The 'decompositions' are essentially arbitrary then and are just a matter of differences in the optimization routines. The marginal var-cov structure might still be the same (as suggested by the fact that the sum of the two components is identical in both model fits), so inferences about the fixed effects could still be valid, but I would still worry about fitting a model whose parameters are not uniquely identified (esp. if inferences of interest pertain to the variances of the random effects / errors).

Best,
Wolfgang

>-----Original Message-----
>From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-
>project.org] On Behalf Of Avraham Kluger
>Sent: Sunday, 13 January, 2019 4:42
>To: R-sig-mixed-models at r-project.org
>Cc: Kenny, David
>Subject: Re: [R-sig-ME] Correlations among random variables
>
>Hi,
>
>Following help from James Uanhoro, I produced models with correlated
>random variables both with nlms and lme4.  Curiously, the estimates of
>the variances and their correlation are identical, but the error
>variances and their correlation are not.  Yet, the sum of the error
>variances are identical.  For example, in the nlme code below the error
>for focalcode + residual is .376 + .046 = .422, and in the lme4 it is .2
>+ .222 = .422.  Can anyone guide me to read about these different
>decompositions?  This may explain the different correlations among the
>error terms .265 vs. .451.
>
>MLM with nlme
>          Variance                            StdDev    Corr
>focalid = pdLogChol(0 + focalcode + partcode)
>focalcode 0.20840953                          0.4565189 foclcd
>partcode  0.06089854                          0.2467763 0.699
>dyadid =  pdLogChol(0 + focalcode + partcode)
>focalcode 0.37674500                          0.6137956 foclcd
>partcode  0.50282362                          0.7091006 0.265
>Residual  0.04641050                          0.2154310
>
>MLM with lme4
>
>> as.data.frame(VarCorr(lme4Mlm))
>             grp      var1     var2       vcov     sdcor
>1 dyadid:focalid focalcode     <NA> 0.20018050 0.4474153
>2 dyadid:focalid  partcode     <NA> 0.32625940 0.5711912
>3 dyadid:focalid focalcode partcode 0.11523355 0.4509065
>4        focalid focalcode     <NA> 0.20840930 0.4565187
>5        focalid  partcode     <NA> 0.06089846 0.2467761
>6        focalid focalcode partcode 0.07872740 0.6988182
>7       Residual      <NA>     <NA> 0.22297497 0.4722023
>> VarCorr(lme4Mlm)
>Groups         Name      Std.Dev. Corr
> dyadid:focalid focalcode 0.44742
>                partcode  0.57119  0.451
>focalid        focalcode 0.45652
>                partcode  0.24678  0.699
>Residual                 0.47220
>
>Yours,
>
>Avi Kluger<http://avikluger.wix.com/avi-kluger>
>
>From: Uanhoro, James<mailto:uanhoro.1 at buckeyemail.osu.edu>
>Sent: Saturday, January 12, 2019 5:38 PM
>To: Avraham Kluger<mailto:avik at savion.huji.ac.il>
>Subject: Re: [R-sig-ME] Correlations among random variables
>
>That is the exact lme4 syntax that replicates the nlme model. I did not
>attempt to run it but when I did, I noticed that the problem is you have
>two random effects, each having 624 values, 624 by 2 equals the sample
>size. lme4 will not run when this happens hence the error message. You
>can tell lme4 to run nevertheless
>
>summary(mlms <- lmer(
>  outcome ~ 0 + focalcode + partcode +
>    (0 + focalcode + partcode | focalid/dyadid),
>  Chapter10_df,
>  control = lmerControl(check.nobs.vs.nRE = "warning")))
>
>This will force the program to run the code, and will print out warnings.
>The log likelihood was the same with that from nlme indicating that the
>models are the same. But the variance-covariance matrices for the random
>effects by the interaction between focalid and dyadid - the inner cluster
>variable - are different. Which one do you trust? Probably neither -
>there is just not enough information to estimate this varCov matrix. See
>this thread for commentary on the issue:
>https://github.com/lme4/lme4/issues/175
>
>Hope this helps, -James.


From thierry@onkelinx @ending from inbo@be  Mon Jan 14 13:42:24 2019
From: thierry@onkelinx @ending from inbo@be (Thierry Onkelinx)
Date: Mon, 14 Jan 2019 13:42:24 +0100
Subject: [R-sig-ME] Correlations among random variables
In-Reply-To: <268119BD809ADB44968F39A7A86398FA01717814B7@Pegasus2.hustaff.huji.local>
References: <268119BD809ADB44968F39A7A86398FA0171780A78@Pegasus2.hustaff.huji.local>
 <1547306566.local-6baa2fdf-d400-v1.5.5-b7939d38@getmailspring.com>
 <268119BD809ADB44968F39A7A86398FA0171780E64@Pegasus2.hustaff.huji.local>
 <268119BD809ADB44968F39A7A86398FA01717814B7@Pegasus2.hustaff.huji.local>
Message-ID: <CAJuCY5xvTNDaByf0YU3DNQGq_j31x0xDGX61u=UdARCKyDmfGQ@mail.gmail.com>

Dear Avraham,

Do you have a huge amount of random effects? If not, the variance estimates
have a large uncertainty. So that you precieve as a strong diverence is
actually just noise from the model uncertainty. I wrote a small blog post
on the number of random effect levels and the resulting uncertainty on the
variance estimates:
https://www.muscardinus.be/2018/09/number-random-effect-levels/

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op zo 13 jan. 2019 om 04:42 schreef Avraham Kluger <avik at savion.huji.ac.il>:

> Hi,
>
> Following help from James Uanhoro, I produced models with correlated
> random variables both with nlms and lme4.  Curiously, the estimates of the
> variances and their correlation are identical, but the error variances and
> their correlation are not.  Yet, the sum of the error variances are
> identical.  For example, in the nlme code below the error for focalcode +
> residual is .376 + .046 = .422, and in the lme4 it is .2 + .222 = .422.
> Can anyone guide me to read about these different decompositions?  This may
> explain the different correlations among the error terms .265 vs. .451.
>
> MLM with nlme
>           Variance                            StdDev    Corr
> focalid = pdLogChol(0 + focalcode + partcode)
> focalcode 0.20840953                          0.4565189 foclcd
> partcode  0.06089854                          0.2467763 0.699
> dyadid =  pdLogChol(0 + focalcode + partcode)
> focalcode 0.37674500                          0.6137956 foclcd
> partcode  0.50282362                          0.7091006 0.265
> Residual  0.04641050                          0.2154310
>
> MLM with lme4
>
> > as.data.frame(VarCorr(lme4Mlm))
>              grp      var1     var2       vcov     sdcor
> 1 dyadid:focalid focalcode     <NA> 0.20018050 0.4474153
> 2 dyadid:focalid  partcode     <NA> 0.32625940 0.5711912
> 3 dyadid:focalid focalcode partcode 0.11523355 0.4509065
> 4        focalid focalcode     <NA> 0.20840930 0.4565187
> 5        focalid  partcode     <NA> 0.06089846 0.2467761
> 6        focalid focalcode partcode 0.07872740 0.6988182
> 7       Residual      <NA>     <NA> 0.22297497 0.4722023
> > VarCorr(lme4Mlm)
> Groups         Name      Std.Dev. Corr
>  dyadid:focalid focalcode 0.44742
>                 partcode  0.57119  0.451
> focalid        focalcode 0.45652
>                 partcode  0.24678  0.699
> Residual                 0.47220
>
> Yours,
>
> Avi Kluger<http://avikluger.wix.com/avi-kluger>
>
>
>
>
>
>
> From: Uanhoro, James<mailto:uanhoro.1 at buckeyemail.osu.edu>
> Sent: Saturday, January 12, 2019 5:38 PM
> To: Avraham Kluger<mailto:avik at savion.huji.ac.il>
> Subject: Re: [R-sig-ME] Correlations among random variables
>
> That is the exact lme4 syntax that replicates the nlme model. I did not
> attempt to run it but when I did, I noticed that the problem is you have
> two random effects, each having 624 values, 624 by 2 equals the sample
> size. lme4 will not run when this happens hence the error message. You can
> tell lme4 to run nevertheless
>
> summary(mlms <- lmer(
>   outcome ~ 0 + focalcode + partcode +
>     (0 + focalcode + partcode | focalid/dyadid),
>   Chapter10_df,
>   control = lmerControl(check.nobs.vs.nRE = "warning")))
>
> This will force the program to run the code, and will print out warnings.
> The log likelihood was the same with that from nlme indicating that the
> models are the same. But the variance-covariance matrices for the random
> effects by the interaction between focalid and dyadid - the inner cluster
> variable - are different. Which one do you trust? Probably neither - there
> is just not enough information to estimate this varCov matrix. See this
> thread for commentary on the issue:
> https://github.com/lme4/lme4/issues/175
>
> Hope this helps, -James.
>
>
>
> On Jan 12 2019, at 9:49 am, Avraham Kluger <avik at savion.huji.ac.il<mailto:
> avik at savion.huji.ac.il>> wrote:
> Oops,
>
> Actually the code produces error
>
>
>
>
> Error: number of observations (=1248) <= number of random effects (=1248)
> for term (0 + focalcode + partcode | dyadid:focalid); the random-effects
> parameters and the residual variance (or scale parameter) are probably
> unidentifiable
>
>
>
>
>
> The code below should run on any machine.
>
>
>
>
>
> Best
>
>
>
>
>
> Avi
>
>
>
> >
>
>
>
>
>
>
>
> ################################################################################
>
> #    **************************** R companion for
> **************************
>
> #
>
> # Kenny, D. A., Kashy, D. A., & Cook, W. L. (2006). Dyadic data analysis.
>
> # New York: Guilford Press.
>
> #
>
> # lme code developed by Limor Borut: limor.borut at mail.huji.ac.il<mailto:
> limor.borut at mail.huji.ac.il>
>
> # written by Avi Kluger: avik at savion.huji.ac.il<mailto:
> avik at savion.huji.ac.il>
>
> #
>
> #                              CHAPTER 10 -- one with many SRM
>
> #
>
>
> ################################################################################
>
> rm(list = ls())                               # Clean the Global
> Environment
>
> cat ("\014")                                  # Clean the R console
>
> if (is.null(dev.list()) == FALSE) dev.off()   # Clean Plots
>
>
>
> # Read (in SPSS format) from Kenny's book site and replicate Table 9.1
>
> if (!require('foreign')) install.packages('foreign'); library('foreign')
>
> Chapter10_df <- read.spss("http://davidakenny.net/kkc/c10/c10_recip.sav",
>
>                to.data.frame = TRUE, use.value.labels = FALSE)
>
>
>
> head(Chapter10_df)
>
>
>
> if (!require("nlme")) install.packages("nlme");
> suppressMessages(library(nlme))
>
>
>
> mlm <- lme(outcome ~   0 + focalcode + 0 + partcode,
>
>             random = ~ 0 + focalcode + partcode|focalid/dyadid,
>
>             data = Chapter10_df)
>
> summary(mlm)
>
> intervals(mlm)
>
> mlmOutput <- VarCorr(mlm)
>
> VarCorr(mlm)
>
>
>
> cat(
>
> "Actor   variance = ",   round(as.numeric(VarCorr(mlm)[, "Variance"][3]),
> 3),
>
> "\nPartner variance = ", round(as.numeric(VarCorr(mlm)[, "Variance"][2]),
> 3),
>
> "\nGeneralized Reciprocity = ", round(as.numeric(VarCorr(mlm)[,
> "Corr"][3]), 3),
>
> "\nDyadic Reciprocity = ", round(as.numeric(VarCorr(mlm)[, "Corr"][6]),
> 3), "\n"
>
> )
>
>
>
> # Very Important Note.  The original data coded with 0 the focal person.
>
> # Therefore the first random variable above is partner variance.  Reversing
>
> # the codes below make the results more intuitive.  I thank David Kenny for
>
> # Clarifying this issue.
>
> Chapter10_df$focalcode <- 1- Chapter10_df$focalcode
>
> Chapter10_df$partcode  <- 1- Chapter10_df$partcode
>
> mlm <- lme(outcome ~   0 + focalcode + 0 + partcode,
>
>             random = ~ 0 + focalcode + partcode|focalid/dyadid,
>
>             data = Chapter10_df)
>
> VarCorr(mlm)
>
>
>
> # An alternative suggsted by James Uanhoro <uanhoro.1 at buckeyemail.osu.edu
> <mailto:uanhoro.1 at buckeyemail.osu.edu>>
>
>
>
> if (!require("lme4")) install.packages("lme4");
> suppressMessages(library(lme4))
>
> mlm <- lmer(outcome   ~ 0 + focalcode + partcode + role +
>
>                        (0 + focalcode + partcode | focalid/ dyadid),
>
>                        data = Chapter10_df)
>
> summary(mlm)
>
> VarCorr(mlm)
>
>
>
>
>
>
>
> From: Uanhoro, James [mailto:uanhoro.1 at buckeyemail.osu.edu]
> Sent: Saturday, January 12, 2019 4:41 PM
> To: Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>
> Subject: RE: [R-sig-ME] Correlations among random variables
>
>
> My reply addressed to issues: covariance between error terms; and between
> random effects.
> The only change to lme4 for random effects is to switch the double pipe to
> a single pipe in the random effects specification of the model, as I have
> done below:
>
> mlm <- lmer(outcome   ~ 0 + focalcode + partcode + role +
>                        (0 + focalcode + partcode | focalid/ dyadid),
>                        data = df)
>
> James.
>
>
>
> On Jan 12, 2019 09:05, Avraham Kluger <avik at savion.huji.ac.il<mailto:
> avik at savion.huji.ac.il>> wrote:
>
> Dear James,
>
>
>
> As you might have seen in my second message to
> r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>,
> my student solved this problem with nlme.  Would you know how to write it
> in lme4?
>
>
>
> Here is the working nlme code
>
>
>
> mlm <- lme(outcome ~   0 + focalcode + 0 + partcode,
>
>             random = ~ 0 + focalcode + partcode|focalid/dyadid,
>
>             data = Chapter10_df)
>
>
>
> Best,
>
>
>
> Avi
>
>
>
>
>
>
>
> From: Uanhoro, James [mailto:uanhoro.1 at buckeyemail.osu.edu]
> Sent: Saturday, January 12, 2019 3:17 PM
> To: Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>
> Cc: r-sig-mixed-models at r-project.org<mailto:
> r-sig-mixed-models at r-project.org>
> Subject: Re: [R-sig-ME] Correlations among random variables
>
>
> In the lme4 syntax, you'd have to change the double pipe, ||, when
> specifying the random effects to a single pipe, |, to permit a correlation
> between random effects. lme4 is faster than nlme.
>
> Assuming lme4 and nlme are the only options ... If you want to specify an
> error covariance structure beyond the covariance structure implied by
> standard multilevel models, you will have to use nlme. nlme has a
> `correlation =` argument that allows different covariance structures,
> corSymm (general/unstructured), corCompSymm (exchangeable), ...
>
>
>
>
> On Jan 12, 2019 02:01, Avraham Kluger <avik at savion.huji.ac.il<mailto:
> avik at savion.huji.ac.il>> wrote:
>
> Hi,
>
> I am struggling to analyze, in R, MLM models that specify correlations
> among random variables, as can be done with SPSS, SAS, or MlWin.
>
> Consider the following code in SPSS
> -----------------------------
> MIXED
>    Outcome  BY role  WITH focalcode partcode
>    /FIXED = focalcode partcode | NOINT
>    /PRINT = SOLUTION TESTCOV
>    /RANDOM focalcode partcode | SUBJECT(focalid) COVTYPE(UNR)
>    /REPEATED = role | SUBJECT(focalid*dyadid) COVTYPE(UNR).
> -----------------------------
> And a minimal code (with data) in R
>
> -----------------------------
> df <- read.csv("
> https://raw.githubusercontent.com/avi-kluger/RCompanion4DDABook/master/Chapter%2010/Chapter10_df.csv
> ")
> head(df)
> library(lme4)
>
> mlm <- lmer(outcome   ~ 0 + focalcode + partcode + role +
>                        (0 + focalcode + partcode|| focalid/ dyadid),
>                        data = df)
> summary(mlm)
> -----------------------------
>
> These SPSS and R codes produce the same variance estimates.  However, SPSS
> also produces a correlation among "focalcode" and "partcode."  How can this
> be done in R?  Is it also possible to produce the correlation among the
> respective error variances (as in SPSS)?
>
> Additional information
>
> 1.       MOTIVATION.  The question arises from David Kenny's work on
> one-with-many reciprocal designs (e.g., a manager rate all subordinates,
> and all subordinates rate the same manager).  These models estimate the
> variance stemming from the one (e.g., managers) and the many (e.g.,
> subordinates), and the correlation among them (termed generalized
> reciprocity).  The data and codes for SAS etc. are available at
> http://davidakenny.net/kkc/c10/c10.htm.
>
> 2.       SPSS OUTPUT (download HTML file):
> https://www.dropbox.com/s/eqch0kq6djtbsfx/One%20with%20many%20SPSS%20output.htm?dl=1
>
> Sincerely,
>
> Avi Kluger
> https://www.avi-kluger.com/
>
> [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org>
> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From RICARDO@ALVARADO @ending from ucr@@c@cr  Mon Jan 14 22:59:29 2019
From: RICARDO@ALVARADO @ending from ucr@@c@cr (RICARDO ALVARADO BARRANTES)
Date: Mon, 14 Jan 2019 15:59:29 -0600
Subject: [R-sig-ME] Calculating F values for lme function
In-Reply-To: <b576d62b04e705c87c535aaa372352d1@ucr.ac.cr>
References: <b576d62b04e705c87c535aaa372352d1@ucr.ac.cr>
Message-ID: <06823a0dae02c1d88c331ae1ecf2e403@ucr.ac.cr>

I have a question related to the funcion LME in the library NLME.  I
would like to understand how the F values are calculated, since the
output only shows the degrees of freedom but doen't show the sums of
squares involved in those calculations. 

Thanks for your attention. 

Ricardo

  
	[[alternative HTML version deleted]]


From jdnewmil @ending from dcn@d@vi@@c@@u@  Mon Jan 14 23:47:47 2019
From: jdnewmil @ending from dcn@d@vi@@c@@u@ (Jeff Newmiller)
Date: Mon, 14 Jan 2019 14:47:47 -0800
Subject: [R-sig-ME] Calculating F values for lme function
In-Reply-To: <06823a0dae02c1d88c331ae1ecf2e403@ucr.ac.cr>
References: <b576d62b04e705c87c535aaa372352d1@ucr.ac.cr>
 <06823a0dae02c1d88c331ae1ecf2e403@ucr.ac.cr>
Message-ID: <38D362F2-9510-4E20-A995-4EDB8417321C@dcn.davis.ca.us>

Fortunately, nlme is open source [1][2], so you can follow along in as much detail as you like.

Note that capitalization matters in R... NLME is not correct.

[1]  https://github.com/cran/nlme/blob/master/R/lme.R
[2] https://cran.r-project.org/package=nlme

On January 14, 2019 1:59:29 PM PST, RICARDO ALVARADO BARRANTES <RICARDO.ALVARADO at ucr.ac.cr> wrote:
>I have a question related to the funcion LME in the library NLME.  I
>would like to understand how the F values are calculated, since the
>output only shows the degrees of freedom but doen't show the sums of
>squares involved in those calculations. 
>
>Thanks for your attention. 
>
>Ricardo
>
>  
>	[[alternative HTML version deleted]]
>
>_______________________________________________
>R-sig-mixed-models at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Sent from my phone. Please excuse my brevity.


From @vik @ending from @@vion@huji@@c@il  Tue Jan 15 06:58:07 2019
From: @vik @ending from @@vion@huji@@c@il (Avraham Kluger)
Date: Tue, 15 Jan 2019 05:58:07 +0000
Subject: [R-sig-ME] Correlations among random variables
In-Reply-To: <CAJuCY5xvTNDaByf0YU3DNQGq_j31x0xDGX61u=UdARCKyDmfGQ@mail.gmail.com>
References: <268119BD809ADB44968F39A7A86398FA0171780A78@Pegasus2.hustaff.huji.local>
 <1547306566.local-6baa2fdf-d400-v1.5.5-b7939d38@getmailspring.com>
 <268119BD809ADB44968F39A7A86398FA0171780E64@Pegasus2.hustaff.huji.local>
 <268119BD809ADB44968F39A7A86398FA01717814B7@Pegasus2.hustaff.huji.local>,
 <CAJuCY5xvTNDaByf0YU3DNQGq_j31x0xDGX61u=UdARCKyDmfGQ@mail.gmail.com>
Message-ID: <268119BD809ADB44968F39A7A86398FA0171784372@Pegasus2.hustaff.huji.local>

Dear Thierry,

I thank you for the reference to your blog.  It does raise questions abo ut noise in the data.  However, if the correlations are significant in any method, would you conclude that a correlation exists in the population?

Avi

From: Thierry Onkelinx<mailto:thierry.onkelinx at inbo.be>
Sent: Monday, January 14, 2019 2:42 PM
To: Avraham Kluger<mailto:avik at savion.huji.ac.il>
Cc: R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org>; Kenny, David<mailto:david.kenny at uconn.edu>
Subject: Re: [R-sig-ME] Correlations among random variables

Dear Avraham,

Do you have a huge amount of random effects? If not, the variance estimates have a large uncertainty. So that you precieve as a strong diverence is actually just noise from the model uncertainty. I wrote a small blog post on the number of random effect levels and the resulting uncertainty on the variance estimates: https://www.muscardinus.be/2018/09/number-random-effect-levels/

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be<http://www.inbo.be>

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

[https://inbo-website-prd-532750756126.s3-eu-west-1.amazonaws.com/inbologoleeuw_nl.png]<https://www.inbo.be>


Op zo 13 jan. 2019 om 04:42 schreef Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>:
Hi,

Following help from James Uanhoro, I produced models with correlated random variables both with nlms and lme4.  Curiously, the estimates of the variances and their correlation are identical, but the error variances and their correlation are not.  Yet, the sum of the error variances are identical.  For example, in the nlme code below the error for focalcode + residual is .376 + .046 = .422, and in the lme4 it is .2 + .222 = .422.  Can anyone guide me to read about these different decompositions?  This may explain the different correlations among the error terms .265 vs. .451.

MLM with nlme
          Variance                            StdDev    Corr
focalid = pdLogChol(0 + focalcode + partcode)
focalcode 0.20840953                          0.4565189 foclcd
partcode  0.06089854                          0.2467763 0.699
dyadid =  pdLogChol(0 + focalcode + partcode)
focalcode 0.37674500                          0.6137956 foclcd
partcode  0.50282362                          0.7091006 0.265
Residual  0.04641050                          0.2154310

MLM with lme4

> as.data.frame(VarCorr(lme4Mlm))
             grp      var1     var2       vcov     sdcor
1 dyadid:focalid focalcode     <NA> 0.20018050 0.4474153
2 dyadid:focalid  partcode     <NA> 0.32625940 0.5711912
3 dyadid:focalid focalcode partcode 0.11523355 0.4509065
4        focalid focalcode     <NA> 0.20840930 0.4565187
5        focalid  partcode     <NA> 0.06089846 0.2467761
6        focalid focalcode partcode 0.07872740 0.6988182
7       Residual      <NA>     <NA> 0.22297497 0.4722023
> VarCorr(lme4Mlm)
Groups         Name      Std.Dev. Corr
 dyadid:focalid focalcode 0.44742
                partcode  0.57119  0.451
focalid        focalcode 0.45652
                partcode  0.24678  0.699
Residual                 0.47220

Yours,

Avi Kluger<http://avikluger.wix.com/avi-kluger>






From: Uanhoro, James<mailto:uanhoro.1 at buckeyemail.osu.edu<mailto:uanhoro.1 at buckeyemail.osu.edu>>
Sent: Saturday, January 12, 2019 5:38 PM
To: Avraham Kluger<mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>
Subject: Re: [R-sig-ME] Correlations among random variables

That is the exact lme4 syntax that replicates the nlme model. I did not attempt to run it but when I did, I noticed that the problem is you have two random effects, each having 624 values, 624 by 2 equals the sample size. lme4 will not run when this happens hence the error message. You can tell lme4 to run nevertheless

summary(mlms <- lmer(
  outcome ~ 0 + focalcode + partcode +
    (0 + focalcode + partcode | focalid/dyadid),
  Chapter10_df,
  control = lmerControl(check.nobs.vs.nRE = "warning")))

This will force the program to run the code, and will print out warnings. The log likelihood was the same with that from nlme indicating that the models are the same. But the variance-covariance matrices for the random effects by the interaction between focalid and dyadid - the inner cluster variable - are different. Which one do you trust? Probably neither - there is just not enough information to estimate this varCov matrix. See this thread for commentary on the issue: https://github.com/lme4/lme4/issues/175

Hope this helps, -James.



On Jan 12 2019, at 9:49 am, Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>> wrote:
Oops,

Actually the code produces error




Error: number of observations (=1248) <= number of random effects (=1248) for term (0 + focalcode + partcode | dyadid:focalid); the random-effects parameters and the residual variance (or scale parameter) are probably unidentifiable





The code below should run on any machine.





Best





Avi



>






################################################################################

#    **************************** R companion for **************************

#

# Kenny, D. A., Kashy, D. A., & Cook, W. L. (2006). Dyadic data analysis.

# New York: Guilford Press.

#

# lme code developed by Limor Borut: limor.borut at mail.huji.ac.il<mailto:limor.borut at mail.huji.ac.il><mailto:limor.borut at mail.huji.ac.il<mailto:limor.borut at mail.huji.ac.il>>

# written by Avi Kluger: avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>

#

#                              CHAPTER 10 -- one with many SRM

#

################################################################################

rm(list = ls())                               # Clean the Global Environment

cat ("\014")                                  # Clean the R console

if (is.null(dev.list()) == FALSE) dev.off()   # Clean Plots



# Read (in SPSS format) from Kenny's book site and replicate Table 9.1

if (!require('foreign')) install.packages('foreign'); library('foreign')

Chapter10_df <- read.spss("http://davidakenny.net/kkc/c10/c10_recip.sav",

               to.data.frame = TRUE, use.value.labels = FALSE)



head(Chapter10_df)



if (!require("nlme")) install.packages("nlme"); suppressMessages(library(nlme))



mlm <- lme(outcome ~   0 + focalcode + 0 + partcode,

            random = ~ 0 + focalcode + partcode|focalid/dyadid,

            data = Chapter10_df)

summary(mlm)

intervals(mlm)

mlmOutput <- VarCorr(mlm)

VarCorr(mlm)



cat(

"Actor   variance = ",   round(as.numeric(VarCorr(mlm)[, "Variance"][3]), 3),

"\nPartner variance = ", round(as.numeric(VarCorr(mlm)[, "Variance"][2]), 3),

"\nGeneralized Reciprocity = ", round(as.numeric(VarCorr(mlm)[, "Corr"][3]), 3),

"\nDyadic Reciprocity = ", round(as.numeric(VarCorr(mlm)[, "Corr"][6]), 3), "\n"

)



# Very Important Note.  The original data coded with 0 the focal person.

# Therefore the first random variable above is partner variance.  Reversing

# the codes below make the results more intuitive.  I thank David Kenny for

# Clarifying this issue.

Chapter10_df$focalcode <- 1- Chapter10_df$focalcode

Chapter10_df$partcode  <- 1- Chapter10_df$partcode

mlm <- lme(outcome ~   0 + focalcode + 0 + partcode,

            random = ~ 0 + focalcode + partcode|focalid/dyadid,

            data = Chapter10_df)

VarCorr(mlm)



# An alternative suggsted by James Uanhoro <uanhoro.1 at buckeyemail.osu.edu<mailto:uanhoro.1 at buckeyemail.osu.edu><mailto:uanhoro.1 at buckeyemail.osu.edu<mailto:uanhoro.1 at buckeyemail.osu.edu>>>



if (!require("lme4")) install.packages("lme4"); suppressMessages(library(lme4))

mlm <- lmer(outcome   ~ 0 + focalcode + partcode + role +

                       (0 + focalcode + partcode | focalid/ dyadid),

                       data = Chapter10_df)

summary(mlm)

VarCorr(mlm)







From: Uanhoro, James [mailto:uanhoro.1 at buckeyemail.osu.edu<mailto:uanhoro.1 at buckeyemail.osu.edu>]
Sent: Saturday, January 12, 2019 4:41 PM
To: Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>>
Subject: RE: [R-sig-ME] Correlations among random variables


My reply addressed to issues: covariance between error terms; and between random effects.
The only change to lme4 for random effects is to switch the double pipe to a single pipe in the random effects specification of the model, as I have done below:

mlm <- lmer(outcome   ~ 0 + focalcode + partcode + role +
                       (0 + focalcode + partcode | focalid/ dyadid),
                       data = df)

James.



On Jan 12, 2019 09:05, Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>> wrote:

Dear James,



As you might have seen in my second message to r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org><mailto:r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>, my student solved this problem with nlme.  Would you know how to write it in lme4?



Here is the working nlme code



mlm <- lme(outcome ~   0 + focalcode + 0 + partcode,

            random = ~ 0 + focalcode + partcode|focalid/dyadid,

            data = Chapter10_df)



Best,



Avi







From: Uanhoro, James [mailto:uanhoro.1 at buckeyemail.osu.edu<mailto:uanhoro.1 at buckeyemail.osu.edu>]
Sent: Saturday, January 12, 2019 3:17 PM
To: Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>>
Cc: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org><mailto:r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
Subject: Re: [R-sig-ME] Correlations among random variables


In the lme4 syntax, you'd have to change the double pipe, ||, when specifying the random effects to a single pipe, |, to permit a correlation between random effects. lme4 is faster than nlme.

Assuming lme4 and nlme are the only options ... If you want to specify an error covariance structure beyond the covariance structure implied by standard multilevel models, you will have to use nlme. nlme has a `correlation =` argument that allows different covariance structures, corSymm (general/unstructured), corCompSymm (exchangeable), ...




On Jan 12, 2019 02:01, Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>> wrote:

Hi,

I am struggling to analyze, in R, MLM models that specify correlations among random variables, as can be done with SPSS, SAS, or MlWin.

Consider the following code in SPSS
-----------------------------
MIXED
   Outcome  BY role  WITH focalcode partcode
   /FIXED = focalcode partcode | NOINT
   /PRINT = SOLUTION TESTCOV
   /RANDOM focalcode partcode | SUBJECT(focalid) COVTYPE(UNR)
   /REPEATED = role | SUBJECT(focalid*dyadid) COVTYPE(UNR).
-----------------------------
And a minimal code (with data) in R

-----------------------------
df <- read.csv("https://raw.githubusercontent.com/avi-kluger/RCompanion4DDABook/master/Chapter%2010/Chapter10_df.csv")
head(df)
library(lme4)

mlm <- lmer(outcome   ~ 0 + focalcode + partcode + role +
                       (0 + focalcode + partcode|| focalid/ dyadid),
                       data = df)
summary(mlm)
-----------------------------

These SPSS and R codes produce the same variance estimates.  However, SPSS also produces a correlation among "focalcode" and "partcode."  How can this be done in R?  Is it also possible to produce the correlation among the respective error variances (as in SPSS)?

Additional information

1.       MOTIVATION.  The question arises from David Kenny's work on one-with-many reciprocal designs (e.g., a manager rate all subordinates, and all subordinates rate the same manager).  These models estimate the variance stemming from the one (e.g., managers) and the many (e.g., subordinates), and the correlation among them (termed generalized reciprocity).  The data and codes for SAS etc. are available at http://davidakenny.net/kkc/c10/c10.htm.

2.       SPSS OUTPUT (download HTML file):  https://www.dropbox.com/s/eqch0kq6djtbsfx/One%20with%20many%20SPSS%20output.htm?dl=1

Sincerely,

Avi Kluger
https://www.avi-kluger.com/

[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org><mailto:R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org>> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From u@nhoro@1 @ending from buckeyem@il@o@u@edu  Tue Jan 15 14:39:07 2019
From: u@nhoro@1 @ending from buckeyem@il@o@u@edu (Uanhoro, James)
Date: Tue, 15 Jan 2019 13:39:07 +0000
Subject: [R-sig-ME] Correlations among random variables
In-Reply-To: <268119BD809ADB44968F39A7A86398FA0171784372@Pegasus2.hustaff.huji.local>
Message-ID: <07ad2b28-debd-40a3-a4f8-a63efb13c010@email.android.com>



***********************************************************************************
This email contains links to a hosting company that is frequented by scammers.
Please think before you click any links.

The Ohio State University will NEVER ask you for your account information by email.
If you receive such a message, please report it to report-phish at osu.edu

NEVER reply to any email asking you for your account information
or other personal details. For more information or to get help,
contact the IT Service Desk by calling 614-688-HELP (4357).
***********************************************************************************
A correlation always exists in the population, if we assume the realistic position that exactly nil effects are just not true.

The points Wolfgang Viechtbauer makes are very important. That is also the crux of this discussion: https://github.com/lme4/lme4/issues/175#issuecomment-33580591.

If your question pertains to the variance covariance matrix that changes from lme4 to nlme, then you're probably asking too much of the data. And the only information you should trust about this particular variance covariance matrix is that you should trust nothing else about it.

James.


On Jan 15, 2019 00:58, Avraham Kluger <avik at savion.huji.ac.il> wrote:

Dear Thierry,

I thank you for the reference to your blog.  It does raise questions abo ut noise in the data.  However, if the correlations are significant in any method, would you conclude that a correlation exists in the population?

Avi

From: Thierry Onkelinx<mailto:thierry.onkelinx at inbo.be>
Sent: Monday, January 14, 2019 2:42 PM
To: Avraham Kluger<mailto:avik at savion.huji.ac.il>
Cc: R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org>; Kenny, David<mailto:david.kenny at uconn.edu>
Subject: Re: [R-sig-ME] Correlations among random variables

Dear Avraham,

Do you have a huge amount of random effects? If not, the variance estimates have a large uncertainty. So that you precieve as a strong diverence is actually just noise from the model uncertainty. I wrote a small blog post on the number of random effect levels and the resulting uncertainty on the variance estimates: https://www.muscardinus.be/2018/09/number-random-effect-levels/

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be<http://www.inbo.be>

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

[https://inbo-website-prd-532750756126.s3-eu-west-1.amazonaws.com/inbologoleeuw_nl.png]<https://www.inbo.be>


Op zo 13 jan. 2019 om 04:42 schreef Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>:
Hi,

Following help from James Uanhoro, I produced models with correlated random variables both with nlms and lme4.  Curiously, the estimates of the variances and their correlation are identical, but the error variances and their correlation are not.  Yet, the sum of the error variances are identical.  For example, in the nlme code below the error for focalcode + residual is .376 + .046 = .422, and in the lme4 it is .2 + .222 = .422.  Can anyone guide me to read about these different decompositions?  This may explain the different correlations among the error terms .265 vs. .451.

MLM with nlme
          Variance                            StdDev    Corr
focalid = pdLogChol(0 + focalcode + partcode)
focalcode 0.20840953                          0.4565189 foclcd
partcode  0.06089854                          0.2467763 0.699
dyadid =  pdLogChol(0 + focalcode + partcode)
focalcode 0.37674500                          0.6137956 foclcd
partcode  0.50282362                          0.7091006 0.265
Residual  0.04641050                          0.2154310

MLM with lme4

> as.data.frame(VarCorr(lme4Mlm))
             grp      var1     var2       vcov     sdcor
1 dyadid:focalid focalcode     <NA> 0.20018050 0.4474153
2 dyadid:focalid  partcode     <NA> 0.32625940 0.5711912
3 dyadid:focalid focalcode partcode 0.11523355 0.4509065
4        focalid focalcode     <NA> 0.20840930 0.4565187
5        focalid  partcode     <NA> 0.06089846 0.2467761
6        focalid focalcode partcode 0.07872740 0.6988182
7       Residual      <NA>     <NA> 0.22297497 0.4722023
> VarCorr(lme4Mlm)
Groups         Name      Std.Dev. Corr
dyadid:focalid focalcode 0.44742
                partcode  0.57119  0.451
focalid        focalcode 0.45652
                partcode  0.24678  0.699
Residual                 0.47220

Yours,

Avi Kluger<http://avikluger.wix.com/avi-kluger>






From: Uanhoro, James<mailto:uanhoro.1 at buckeyemail.osu.edu<mailto:uanhoro.1 at buckeyemail.osu.edu>>
Sent: Saturday, January 12, 2019 5:38 PM
To: Avraham Kluger<mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>
Subject: Re: [R-sig-ME] Correlations among random variables

That is the exact lme4 syntax that replicates the nlme model. I did not attempt to run it but when I did, I noticed that the problem is you have two random effects, each having 624 values, 624 by 2 equals the sample size. lme4 will not run when this happens hence the error message. You can tell lme4 to run nevertheless

summary(mlms <- lmer(
  outcome ~ 0 + focalcode + partcode +
    (0 + focalcode + partcode | focalid/dyadid),
  Chapter10_df,
  control = lmerControl(check.nobs.vs.nRE = "warning")))

This will force the program to run the code, and will print out warnings. The log likelihood was the same with that from nlme indicating that the models are the same. But the variance-covariance matrices for the random effects by the interaction between focalid and dyadid - the inner cluster variable - are different. Which one do you trust? Probably neither - there is just not enough information to estimate this varCov matrix. See this thread for commentary on the issue: https://github.com/lme4/lme4/issues/175

Hope this helps, -James.



On Jan 12 2019, at 9:49 am, Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>> wrote:
Oops,

Actually the code produces error




Error: number of observations (=1248) <= number of random effects (=1248) for term (0 + focalcode + partcode | dyadid:focalid); the random-effects parameters and the residual variance (or scale parameter) are probably unidentifiable





The code below should run on any machine.





Best





Avi



>






################################################################################

#    **************************** R companion for **************************

#

# Kenny, D. A., Kashy, D. A., & Cook, W. L. (2006). Dyadic data analysis.

# New York: Guilford Press.

#

# lme code developed by Limor Borut: limor.borut at mail.huji.ac.il<mailto:limor.borut at mail.huji.ac.il><mailto:limor.borut at mail.huji.ac.il<mailto:limor.borut at mail.huji.ac.il>>

# written by Avi Kluger: avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>

#

#                              CHAPTER 10 -- one with many SRM

#

################################################################################

rm(list = ls())                               # Clean the Global Environment

cat ("\014")                                  # Clean the R console

if (is.null(dev.list()) == FALSE) dev.off()   # Clean Plots



# Read (in SPSS format) from Kenny's book site and replicate Table 9.1

if (!require('foreign')) install.packages('foreign'); library('foreign')

Chapter10_df <- read.spss("http://davidakenny.net/kkc/c10/c10_recip.sav",

               to.data.frame = TRUE, use.value.labels = FALSE)



head(Chapter10_df)



if (!require("nlme")) install.packages("nlme"); suppressMessages(library(nlme))



mlm <- lme(outcome ~   0 + focalcode + 0 + partcode,

            random = ~ 0 + focalcode + partcode|focalid/dyadid,

            data = Chapter10_df)

summary(mlm)

intervals(mlm)

mlmOutput <- VarCorr(mlm)

VarCorr(mlm)



cat(

"Actor   variance = ",   round(as.numeric(VarCorr(mlm)[, "Variance"][3]), 3),

"\nPartner variance = ", round(as.numeric(VarCorr(mlm)[, "Variance"][2]), 3),

"\nGeneralized Reciprocity = ", round(as.numeric(VarCorr(mlm)[, "Corr"][3]), 3),

"\nDyadic Reciprocity = ", round(as.numeric(VarCorr(mlm)[, "Corr"][6]), 3), "\n"

)



# Very Important Note.  The original data coded with 0 the focal person.

# Therefore the first random variable above is partner variance.  Reversing

# the codes below make the results more intuitive.  I thank David Kenny for

# Clarifying this issue.

Chapter10_df$focalcode <- 1- Chapter10_df$focalcode

Chapter10_df$partcode  <- 1- Chapter10_df$partcode

mlm <- lme(outcome ~   0 + focalcode + 0 + partcode,

            random = ~ 0 + focalcode + partcode|focalid/dyadid,

            data = Chapter10_df)

VarCorr(mlm)



# An alternative suggsted by James Uanhoro <uanhoro.1 at buckeyemail.osu.edu<mailto:uanhoro.1 at buckeyemail.osu.edu><mailto:uanhoro.1 at buckeyemail.osu.edu<mailto:uanhoro.1 at buckeyemail.osu.edu>>>



if (!require("lme4")) install.packages("lme4"); suppressMessages(library(lme4))

mlm <- lmer(outcome   ~ 0 + focalcode + partcode + role +

                       (0 + focalcode + partcode | focalid/ dyadid),

                       data = Chapter10_df)

summary(mlm)

VarCorr(mlm)







From: Uanhoro, James [mailto:uanhoro.1 at buckeyemail.osu.edu<mailto:uanhoro.1 at buckeyemail.osu.edu>]
Sent: Saturday, January 12, 2019 4:41 PM
To: Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>>
Subject: RE: [R-sig-ME] Correlations among random variables


My reply addressed to issues: covariance between error terms; and between random effects.
The only change to lme4 for random effects is to switch the double pipe to a single pipe in the random effects specification of the model, as I have done below:

mlm <- lmer(outcome   ~ 0 + focalcode + partcode + role +
                       (0 + focalcode + partcode | focalid/ dyadid),
                       data = df)

James.



On Jan 12, 2019 09:05, Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>> wrote:

Dear James,



As you might have seen in my second message to r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org><mailto:r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>, my student solved this problem with nlme.  Would you know how to write it in lme4?



Here is the working nlme code



mlm <- lme(outcome ~   0 + focalcode + 0 + partcode,

            random = ~ 0 + focalcode + partcode|focalid/dyadid,

            data = Chapter10_df)



Best,



Avi







From: Uanhoro, James [mailto:uanhoro.1 at buckeyemail.osu.edu<mailto:uanhoro.1 at buckeyemail.osu.edu>]
Sent: Saturday, January 12, 2019 3:17 PM
To: Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>>
Cc: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org><mailto:r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
Subject: Re: [R-sig-ME] Correlations among random variables


In the lme4 syntax, you'd have to change the double pipe, ||, when specifying the random effects to a single pipe, |, to permit a correlation between random effects. lme4 is faster than nlme.

Assuming lme4 and nlme are the only options ... If you want to specify an error covariance structure beyond the covariance structure implied by standard multilevel models, you will have to use nlme. nlme has a `correlation =` argument that allows different covariance structures, corSymm (general/unstructured), corCompSymm (exchangeable), ...




On Jan 12, 2019 02:01, Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>> wrote:

Hi,

I am struggling to analyze, in R, MLM models that specify correlations among random variables, as can be done with SPSS, SAS, or MlWin.

Consider the following code in SPSS
-----------------------------
MIXED
   Outcome  BY role  WITH focalcode partcode
   /FIXED = focalcode partcode | NOINT
   /PRINT = SOLUTION TESTCOV
   /RANDOM focalcode partcode | SUBJECT(focalid) COVTYPE(UNR)
   /REPEATED = role | SUBJECT(focalid*dyadid) COVTYPE(UNR).
-----------------------------
And a minimal code (with data) in R

-----------------------------
df <- read.csv("https://raw.githubusercontent.com/avi-kluger/RCompanion4DDABook/master/Chapter%2010/Chapter10_df.csv")
head(df)
library(lme4)

mlm <- lmer(outcome   ~ 0 + focalcode + partcode + role +
                       (0 + focalcode + partcode|| focalid/ dyadid),
                       data = df)
summary(mlm)
-----------------------------

These SPSS and R codes produce the same variance estimates.  However, SPSS also produces a correlation among "focalcode" and "partcode."  How can this be done in R?  Is it also possible to produce the correlation among the respective error variances (as in SPSS)?

Additional information

1.       MOTIVATION.  The question arises from David Kenny's work on one-with-many reciprocal designs (e.g., a manager rate all subordinates, and all subordinates rate the same manager).  These models estimate the variance stemming from the one (e.g., managers) and the many (e.g., subordinates), and the correlation among them (termed generalized reciprocity).  The data and codes for SAS etc. are available at http://davidakenny.net/kkc/c10/c10.htm.

2.       SPSS OUTPUT (download HTML file):  https://www.dropbox.com/s/eqch0kq6djtbsfx/One%20with%20many%20SPSS%20output.htm?dl=1

Sincerely,

Avi Kluger
https://www.avi-kluger.com/

[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org><mailto:R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org>> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


	[[alternative HTML version deleted]]


From @v|k @end|ng |rom @@v|on@huj|@@c@||  Wed Jan 16 11:26:34 2019
From: @v|k @end|ng |rom @@v|on@huj|@@c@|| (Avraham Kluger)
Date: Wed, 16 Jan 2019 10:26:34 +0000
Subject: [R-sig-ME] Correlations among random variables
In-Reply-To: <07ad2b28-debd-40a3-a4f8-a63efb13c010@email.android.com>
References: <268119BD809ADB44968F39A7A86398FA0171784372@Pegasus2.hustaff.huji.local>,
 <07ad2b28-debd-40a3-a4f8-a63efb13c010@email.android.com>
Message-ID: <268119BD809ADB44968F39A7A86398FA0171786471@Pegasus2.hustaff.huji.local>

Hi,

I thank James, Wolfgang , Thierry , and David for educating me.  To summarize all responses, you seem to suggest: Do not trust your data when it comes to the correlation between the error terms.  I still have some questions about intepreration.  Note that I do not care particularity about these data, but I am trying to grasp the principle.  First, I repeated the calculations with SPSS and with SEM (in the later using wide-data formal, and placing equality constraints within Focal and within Partner on intercepts, variances, and covariances).  The summary of all the results can be found below.

Error variances, covariances, and correlations  by software/analytic approach
---------------------------------------------------------------------------------------------------------------------------------
	                 Var Focal 	Var Partner	Residual	Total Var Focal	Total Var Partner  Cov (SE/CI)	        Corr (SE/CI)
---------------------------------------------------------------------------------------------------------------------------------
SPSS         	.549	                .423	              NA	        .549	                 .423	                NA 	                .239 (.046)
lme4	        .326	                .200	              .223	        .549	                 .423	                0.115(*)	        .451
nlme	        .376	                .502	              .046	        .549	                 .423	                        (*)	        .265
lavaan (SEM)	.331 	        .457	              .094	        .551	                 .425	                0.116 (.024)	.298
---------------------------------------------------------------------------------------------------------------------------------
*	cannot get confidence intervals on var-cov components: Non-positive definite approximate variance-covariance

1.  How are the correlations calculated.  Assuming that the covariance is .115 (or .116), what are the variances that their product serve as a denominator in transforming the covariance into the correlation?
2.  David Dupphy wrote:

FWIW, the raw correlations (that are being analysed) when reordered as a a bivariate setup:
partner-focal r=0.31 [dyads]
focal intraclass r=0.10 (jack SE=0.06)  [clustered on focal]
partner intraclass r=0.33 (jSE=0.07) [clustered on focal]

Given that all analyses suggest a POSITIVE covariance, would it be reasonable to conclude that there is a positive correlation in the population, although its magnitude is uncertain?  Or would you still believe these signals are noise?

I with deep gratitude to this form and all the help received,

Avi



From: Uanhoro, James [uanhoro.1 at buckeyemail.osu.edu]

Sent: Tuesday, January 15, 2019 3:39 PM

To: Avraham Kluger

Cc: R-sig-mixed-models at r-project.org

Subject: Re: [R-sig-ME] Correlations among random variables











***********************************************************************************


This email contains links to a hosting company that is frequented by scammers.

Please think before you click any links.



The Ohio State University will NEVER ask you for your account information by email.

If you receive such a message, please report it to report-phish at osu.edu



NEVER reply to any email asking you for your account information 

or other personal details. For more information or to get help,

contact the IT Service Desk by calling 614-688-HELP (4357).

***********************************************************************************






A correlation always exists in the population, if we assume the realistic position that exactly nil effects are just not true.





The points Wolfgang Viechtbauer makes are very important. That is also the crux of this discussion: https://github.com/lme4/lme4/issues/175#issuecomment-33580591.





If your question pertains to the variance covariance matrix that changes from lme4 to nlme, then you're probably asking too much of the data. And the only information you should trust about this particular variance covariance matrix is that
 you should trust nothing else about it.





James.







On Jan 15, 2019 00:58, Avraham Kluger <avik at savion.huji.ac.il> wrote:

Dear Thierry, 



I thank you for the reference to your blog.  It does raise questions abo ut noise in the data.  However, if the correlations are significant in any method, would you conclude that a correlation exists in the population?




Avi 



From: Thierry Onkelinx<mailto:thierry.onkelinx at inbo.be> 

Sent: Monday, January 14, 2019 2:42 PM 

To: Avraham Kluger<mailto:avik at savion.huji.ac.il> 

Cc: R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org>; Kenny, David<mailto:david.kenny at uconn.edu>


Subject: Re: [R-sig-ME] Correlations among random variables 



Dear Avraham, 



Do you have a huge amount of random effects? If not, the variance estimates have a large uncertainty. So that you precieve as a strong diverence is actually just noise from the model uncertainty. I wrote a small blog post on the number of random effect levels
 and the resulting uncertainty on the variance estimates: https://www.muscardinus.be/2018/09/number-random-effect-levels/




Best regards, 



ir. Thierry Onkelinx 

Statisticus / Statistician 



Vlaamse Overheid / Government of Flanders 

INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST


Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance 

thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be> 

Havenlaan 88 bus 73, 1000 Brussel 

www.inbo.be<http://www.inbo.be> 



///////////////////////////////////////////////////////////////////////////////////////////


To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher


The plural of anecdote is not data. ~ Roger Brinner 

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey


///////////////////////////////////////////////////////////////////////////////////////////




[https://inbo-website-prd-532750756126.s3-eu-west-1.amazonaws.com/inbologoleeuw_nl.png]<https://www.inbo.be>






Op zo 13 jan. 2019 om 04:42 schreef Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>:


Hi, 



Following help from James Uanhoro, I produced models with correlated random variables both with nlms and lme4.  Curiously, the estimates of the variances and their correlation are identical, but the error variances and their correlation are not.  Yet, the sum
 of the error variances are identical.  For example, in the nlme code below the error for focalcode + residual is .376 + .046 = .422, and in the lme4 it is .2 + .222 = .422.  Can anyone guide me to read about these different decompositions?  This may explain
 the different correlations among the error terms .265 vs. .451. 



MLM with nlme 

          Variance                            StdDev    Corr 

focalid = pdLogChol(0 + focalcode + partcode) 

focalcode 0.20840953                          0.4565189 foclcd 

partcode  0.06089854                          0.2467763 0.699 

dyadid =  pdLogChol(0 + focalcode + partcode) 

focalcode 0.37674500                          0.6137956 foclcd 

partcode  0.50282362                          0.7091006 0.265 

Residual  0.04641050                          0.2154310 



MLM with lme4 



> as.data.frame(VarCorr(lme4Mlm)) 

             grp      var1     var2       vcov     sdcor 

1 dyadid:focalid focalcode     <NA> 0.20018050 0.4474153 

2 dyadid:focalid  partcode     <NA> 0.32625940 0.5711912 

3 dyadid:focalid focalcode partcode 0.11523355 0.4509065 

4        focalid focalcode     <NA> 0.20840930 0.4565187 

5        focalid  partcode     <NA> 0.06089846 0.2467761 

6        focalid focalcode partcode 0.07872740 0.6988182 

7       Residual      <NA>     <NA> 0.22297497 0.4722023 

> VarCorr(lme4Mlm) 

Groups         Name      Std.Dev. Corr 

dyadid:focalid focalcode 0.44742 

                partcode  0.57119  0.451 

focalid        focalcode 0.45652 

                partcode  0.24678  0.699 

Residual                 0.47220 



Yours, 



Avi Kluger<http://avikluger.wix.com/avi-kluger> 













From: Uanhoro, James<mailto:uanhoro.1 at buckeyemail.osu.edu<mailto:uanhoro.1 at buckeyemail.osu.edu>>


Sent: Saturday, January 12, 2019 5:38 PM 

To: Avraham Kluger<mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>> 

Subject: Re: [R-sig-ME] Correlations among random variables 



That is the exact lme4 syntax that replicates the nlme model. I did not attempt to run it but when I did, I noticed that the problem is you have two random effects, each having 624 values, 624 by 2 equals the sample size. lme4 will not run when this happens
 hence the error message. You can tell lme4 to run nevertheless 



summary(mlms <- lmer( 

  outcome ~ 0 + focalcode + partcode + 

    (0 + focalcode + partcode | focalid/dyadid), 

  Chapter10_df, 

  control = lmerControl(check.nobs.vs.nRE = "warning"))) 



This will force the program to run the code, and will print out warnings. The log likelihood was the same with that from nlme indicating that the models are the same. But the variance-covariance matrices for the random effects by the interaction between focalid
 and dyadid - the inner cluster variable - are different. Which one do you trust? Probably neither - there is just not enough information to estimate this varCov matrix. See this thread for commentary on the issue: https://github.com/lme4/lme4/issues/175




Hope this helps, -James. 







On Jan 12 2019, at 9:49 am, Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>> wrote:


Oops, 



Actually the code produces error 









Error: number of observations (=1248) <= number of random effects (=1248) for term (0 + focalcode + partcode | dyadid:focalid); the random-effects parameters and the residual variance (or scale parameter) are probably unidentifiable












The code below should run on any machine. 











Best 











Avi 







> 













################################################################################ 



#    **************************** R companion for ************************** 



# 



# Kenny, D. A., Kashy, D. A., & Cook, W. L. (2006). Dyadic data analysis. 



# New York: Guilford Press. 



# 



# lme code developed by Limor Borut: limor.borut at mail.huji.ac.il<mailto:limor.borut at mail.huji.ac.il><mailto:limor.borut at mail.huji.ac.il<mailto:limor.borut at mail.huji.ac.il>>




# written by Avi Kluger: avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>




# 



#                              CHAPTER 10 -- one with many SRM 



# 



################################################################################ 



rm(list = ls())                               # Clean the Global Environment 



cat ("\014")                                  # Clean the R console 



if (is.null(dev.list()) == FALSE) dev.off()   # Clean Plots 







# Read (in SPSS format) from Kenny's book site and replicate Table 9.1 



if (!require('foreign')) install.packages('foreign'); library('foreign') 



Chapter10_df <- read.spss("http://davidakenny.net/kkc/c10/c10_recip.sav", 



               to.data.frame = TRUE, use.value.labels = FALSE) 







head(Chapter10_df) 







if (!require("nlme")) install.packages("nlme"); suppressMessages(library(nlme)) 







mlm <- lme(outcome ~   0 + focalcode + 0 + partcode, 



            random = ~ 0 + focalcode + partcode|focalid/dyadid, 



            data = Chapter10_df) 



summary(mlm) 



intervals(mlm) 



mlmOutput <- VarCorr(mlm) 



VarCorr(mlm) 







cat( 



"Actor   variance = ",   round(as.numeric(VarCorr(mlm)[, "Variance"][3]), 3), 



"\nPartner variance = ", round(as.numeric(VarCorr(mlm)[, "Variance"][2]), 3), 



"\nGeneralized Reciprocity = ", round(as.numeric(VarCorr(mlm)[, "Corr"][3]), 3), 



"\nDyadic Reciprocity = ", round(as.numeric(VarCorr(mlm)[, "Corr"][6]), 3), "\n" 



) 







# Very Important Note.  The original data coded with 0 the focal person. 



# Therefore the first random variable above is partner variance.  Reversing 



# the codes below make the results more intuitive.  I thank David Kenny for 



# Clarifying this issue. 



Chapter10_df$focalcode <- 1- Chapter10_df$focalcode 



Chapter10_df$partcode  <- 1- Chapter10_df$partcode 



mlm <- lme(outcome ~   0 + focalcode + 0 + partcode, 



            random = ~ 0 + focalcode + partcode|focalid/dyadid, 



            data = Chapter10_df) 



VarCorr(mlm) 







# An alternative suggsted by James Uanhoro <uanhoro.1 at buckeyemail.osu.edu<mailto:uanhoro.1 at buckeyemail.osu.edu><mailto:uanhoro.1 at buckeyemail.osu.edu<mailto:uanhoro.1 at buckeyemail.osu.edu>>>








if (!require("lme4")) install.packages("lme4"); suppressMessages(library(lme4)) 



mlm <- lmer(outcome   ~ 0 + focalcode + partcode + role + 



                       (0 + focalcode + partcode | focalid/ dyadid), 



                       data = Chapter10_df) 



summary(mlm) 



VarCorr(mlm) 















From: Uanhoro, James [mailto:uanhoro.1 at buckeyemail.osu.edu<mailto:uanhoro.1 at buckeyemail.osu.edu>]


Sent: Saturday, January 12, 2019 4:41 PM 

To: Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>>


Subject: RE: [R-sig-ME] Correlations among random variables 





My reply addressed to issues: covariance between error terms; and between random effects.


The only change to lme4 for random effects is to switch the double pipe to a single pipe in the random effects specification of the model, as I have done below:




mlm <- lmer(outcome   ~ 0 + focalcode + partcode + role + 

                       (0 + focalcode + partcode | focalid/ dyadid), 

                       data = df) 



James. 







On Jan 12, 2019 09:05, Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>> wrote:




Dear James, 







As you might have seen in my second message to r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org><mailto:r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>, my student solved this problem with nlme.  Would you
 know how to write it in lme4? 







Here is the working nlme code 







mlm <- lme(outcome ~   0 + focalcode + 0 + partcode, 



            random = ~ 0 + focalcode + partcode|focalid/dyadid, 



            data = Chapter10_df) 







Best, 







Avi 















From: Uanhoro, James [mailto:uanhoro.1 at buckeyemail.osu.edu<mailto:uanhoro.1 at buckeyemail.osu.edu>]


Sent: Saturday, January 12, 2019 3:17 PM 

To: Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>>


Cc: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org><mailto:r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>


Subject: Re: [R-sig-ME] Correlations among random variables 





In the lme4 syntax, you'd have to change the double pipe, ||, when specifying the random effects to a single pipe, |, to permit a correlation between random effects. lme4 is faster than nlme.




Assuming lme4 and nlme are the only options ... If you want to specify an error covariance structure beyond the covariance structure implied by standard multilevel models, you will have to use nlme. nlme has a `correlation =` argument that allows different
 covariance structures, corSymm (general/unstructured), corCompSymm (exchangeable), ...










On Jan 12, 2019 02:01, Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>> wrote:




Hi, 



I am struggling to analyze, in R, MLM models that specify correlations among random variables, as can be done with SPSS, SAS, or MlWin.




Consider the following code in SPSS 

----------------------------- 

MIXED 

   Outcome  BY role  WITH focalcode partcode 

   /FIXED = focalcode partcode | NOINT 

   /PRINT = SOLUTION TESTCOV 

   /RANDOM focalcode partcode | SUBJECT(focalid) COVTYPE(UNR) 

   /REPEATED = role | SUBJECT(focalid*dyadid) COVTYPE(UNR). 

----------------------------- 

And a minimal code (with data) in R 



----------------------------- 

df <- read.csv("https://raw.githubusercontent.com/avi-kluger/RCompanion4DDABook/master/Chapter%2010/Chapter10_df.csv")


head(df) 

library(lme4) 



mlm <- lmer(outcome   ~ 0 + focalcode + partcode + role + 

                       (0 + focalcode + partcode|| focalid/ dyadid), 

                       data = df) 

summary(mlm) 

----------------------------- 



These SPSS and R codes produce the same variance estimates.  However, SPSS also produces a correlation among "focalcode" and "partcode."  How can this be done in R?  Is it also possible to produce the correlation among the respective error variances (as in
 SPSS)? 



Additional information 



1.       MOTIVATION.  The question arises from David Kenny's work on one-with-many reciprocal designs (e.g., a manager rate all subordinates, and all subordinates rate the same manager).  These models estimate the variance stemming from the one (e.g., managers)
 and the many (e.g., subordinates), and the correlation among them (termed generalized reciprocity).  The data and codes for SAS etc. are available at http://davidakenny.net/kkc/c10/c10.htm.




2.       SPSS OUTPUT (download HTML file):  https://www.dropbox.com/s/eqch0kq6djtbsfx/One%20with%20many%20SPSS%20output.htm?dl=1




Sincerely, 



Avi Kluger 

https://www.avi-kluger.com/ 



[[alternative HTML version deleted]] 



_______________________________________________ 

R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org><mailto:R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org>> mailing list


https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models 







        [[alternative HTML version deleted]] 



_______________________________________________ 

R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list


https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models 

d

        [[alternative HTML version deleted]] 



_______________________________________________ 

R-sig-mixed-models at r-project.org mailing list 

https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models 















From m@tthew@@@hby @end|ng |rom ntu@@c@uk  Thu Jan 17 19:16:47 2019
From: m@tthew@@@hby @end|ng |rom ntu@@c@uk (Ashby, Matthew)
Date: Thu, 17 Jan 2019 18:16:47 +0000
Subject: [R-sig-ME] Predicting values from an MCMCglmm model
Message-ID: <451D2B60-FB33-4D58-8ABA-532182AFCBDD@contoso.com>

Dear all,

I am fitting an MCMCglmm with Poisson link to investigate relationships between schools and violence in the surrounding neighbourhood. The dependent variable is counts of assaults in neighbourhoods, with repeated measures across four different time periods. The model uses data from nine cities. The model is fit by:

aslt_h1 <- MCMCglmm(
  fixed = count ~ count_lag_sc,
  random = ~ us(1 + pop_total_sc + index_disadvantage + index_mobility + index_ethnic + perc_teen_sc + private_elementary_school + public_elementary_school + private_secondary_school +  public_secondary_school):city_period,
  data = data_h1,
  family = "poisson",
  prior = list(R = list(V = diag(1), nu = 0.002), G = list(G1 = list(V = diag(10), n = 10))),
  thin = 20, burnin = 5000, nitt = 50000, pr = TRUE
)

I would like to generate predicted values for the mean neighbourhood at each time period in each city. I've constructed a data frame of new data, but I get the error message

Error in dimnames(x) <- dn :
  length of 'dimnames' [2] not equal to array extent

By trial and error I think the problem is caused by my newdata data frame not being in the format expected by predict.MCMCglmm(), but having read several tutorials online and section 2.5 of the Course Notes, I can't see where I'm going wrong. I'm creating the newdata DF by:

newdata <- expand.grid(
  count = 0,
  # since all the control variables were centered and scaled, they can be set to zero here to represent block groups with the mean values
  count_lag_sc = 0,
  pop_total_sc = 0,
  perc_teen_sc = 0,
  index_disadvantage = 0,
  index_mobility = 0,
  index_ethnic = 0,
  # since we know there is only a meaningful effect for public high/middle schools, we can set all the other school indicators to FALSE
  private_elementary_school = FALSE,
  public_elementary_school = FALSE,
  private_secondary_school = FALSE,
  public_secondary_school = c(FALSE, TRUE),
  city_period = c(
    "Chicago - non-school day non-school time",
    "Chicago - non-school day school time",
    "Chicago - school day non-school time",
    "Chicago - school day school time",
    "Detroit - non-school day non-school time",
    [? levels for other cities snipped for space ?]
  )
)

And then trying to predict values by calling:

predictions <- predict(aslt_h1, newdata = newdata, marginal = NULL, type = "response", posterior = "mean", verbose = TRUE)

Does the format of the newdata DF look acceptable? Is the problem likely to be there or in the arguments I've set in the predict() call?

Thank you in advance for any tips or suggestions.

Yours,


Matt Ashby
Nottingham Trent University


DISCLAIMER: This email is intended solely for the addressee. It may contain private and confidential information. If you are not the intended addressee, please take no action based on it nor show a copy to anyone. In this case, please reply to this email to highlight the error. Opinions and information in this email that do not relate to the official business of Nottingham Trent University shall be understood as neither given nor endorsed by the University. Nottingham Trent University has taken steps to ensure that this email and any attachments are virus-free, but we do advise that the recipient should check that the email and its attachments are actually virus free. This is in keeping with good computing practice.

From bbo|ker @end|ng |rom gm@||@com  Thu Jan 17 19:28:46 2019
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Thu, 17 Jan 2019 13:28:46 -0500
Subject: [R-sig-ME] Calculating F values for lme function
In-Reply-To: <06823a0dae02c1d88c331ae1ecf2e403@ucr.ac.cr>
References: <b576d62b04e705c87c535aaa372352d1@ucr.ac.cr>
 <06823a0dae02c1d88c331ae1ecf2e403@ucr.ac.cr>
Message-ID: <7f20e0be-bafc-148f-4abe-e17ba9d5f16b@gmail.com>


  Jeff is correct, but maybe doesn't provide much context; the
documentation that he points doesn't contain details of the fitting
algorithm used in lme.  There is an entire book by Pinheiro and Bates
(Springer, 2000) that discusses the algorithms. The main thing to note,
though, is that lme uses a penalized least squares algorithm rather than
a moment-based/sum-of-squares decomposition, so technically there *are*
no "sums of squares" involved in the calculation.  For many problems
there are *equivalent* formulations in terms of sums of squares, and one
can often derive the SS involved, but lme doesn't actually use them.

On 2019-01-14 4:59 p.m., RICARDO ALVARADO BARRANTES wrote:
> I have a question related to the funcion LME in the library NLME.  I
> would like to understand how the F values are calculated, since the
> output only shows the degrees of freedom but doen't show the sums of
> squares involved in those calculations. 
> 
> Thanks for your attention. 
> 
> Ricardo
> 
>   
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From v|n|c|u@@@@m@|@ @end|ng |rom hotm@||@com  Fri Jan 18 02:11:32 2019
From: v|n|c|u@@@@m@|@ @end|ng |rom hotm@||@com (Vinicius Maia)
Date: Fri, 18 Jan 2019 01:11:32 +0000
Subject: [R-sig-ME] gamm shifting residuals in plots
Message-ID: <DM6PR14MB2892DBC21A881BA52537F162BE9C0@DM6PR14MB2892.namprd14.prod.outlook.com>

Hi folks!

When running a gamm and seing its plots I found the residuals locations are shifted , I presume its because the gam smooth is fitted after  taking  the random effects into account (obviously). But I am not understanding how its possible to change data points location in these plots. The same is valid for the lmer, I usually plot the fitted line whith the residuals, but I have never seen this discrepancy in points positions with the fitted line before. I would appreciate if anyone could clarify what is happening.

Plots are attached

codes:

a) mod1=gamm(y1_r_prop~s(areia), random = list(area=~1,chave=~1),data=dadosest)
plot(mod1$gam,residuals=TRUE,pch=1.3,shift = coef(mod1$gam)[1])

b) mod2=gamm(y1_r_prop~s(areia), random = list(chave=~1),method="ML",data=dadosest)
plot(mod2$gam,residuals=TRUE,pch=1.3,shift = coef(mod2$gam)[1])

c) mod3=gamm(y1_r_prop~s(areia), random = list(area=~1),method="ML",data=dadosest)
plot(mod3$gam,residuals=TRUE,pch=1.3,shift = coef(mod3$gam)[1])

d) mod4=gam(y1_r_prop~s(areia),method="ML",data=dadosest)
plot(mod4,residuals=TRUE,pch=1.3,shift = coef(mod4)[1])

e) plot(y1_r_prop~areia)
curve(18.3048+-2.2142*x,add=TRUE) # coefficients from a lmer with both area and chave random effects

Thank you all!


From th|erry@onke||nx @end|ng |rom |nbo@be  Fri Jan 18 10:03:53 2019
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Fri, 18 Jan 2019 10:03:53 +0100
Subject: [R-sig-ME] gamm shifting residuals in plots
In-Reply-To: <DM6PR14MB2892DBC21A881BA52537F162BE9C0@DM6PR14MB2892.namprd14.prod.outlook.com>
References: <DM6PR14MB2892DBC21A881BA52537F162BE9C0@DM6PR14MB2892.namprd14.prod.outlook.com>
Message-ID: <CAJuCY5yP+bLhHhR8j9TwsCCtkAfTJ9EQg=pq+nDec3N-9foyXw@mail.gmail.com>

The mailing accepts only a limited range of file extensions. Your images
got stripped from your mail. Put them somewhere online and send the link.
Sending a minimal reproducible example is better.

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op vr 18 jan. 2019 om 02:11 schreef Vinicius Maia <
vinicius.a.maia at hotmail.com>:

> Hi folks!
>
> When running a gamm and seing its plots I found the residuals locations
> are shifted , I presume its because the gam smooth is fitted after  taking
> the random effects into account (obviously). But I am not understanding how
> its possible to change data points location in these plots. The same is
> valid for the lmer, I usually plot the fitted line whith the residuals, but
> I have never seen this discrepancy in points positions with the fitted line
> before. I would appreciate if anyone could clarify what is happening.
>
> Plots are attached
>
> codes:
>
> a) mod1=gamm(y1_r_prop~s(areia), random =
> list(area=~1,chave=~1),data=dadosest)
> plot(mod1$gam,residuals=TRUE,pch=1.3,shift = coef(mod1$gam)[1])
>
> b) mod2=gamm(y1_r_prop~s(areia), random =
> list(chave=~1),method="ML",data=dadosest)
> plot(mod2$gam,residuals=TRUE,pch=1.3,shift = coef(mod2$gam)[1])
>
> c) mod3=gamm(y1_r_prop~s(areia), random =
> list(area=~1),method="ML",data=dadosest)
> plot(mod3$gam,residuals=TRUE,pch=1.3,shift = coef(mod3$gam)[1])
>
> d) mod4=gam(y1_r_prop~s(areia),method="ML",data=dadosest)
> plot(mod4,residuals=TRUE,pch=1.3,shift = coef(mod4)[1])
>
> e) plot(y1_r_prop~areia)
> curve(18.3048+-2.2142*x,add=TRUE) # coefficients from a lmer with both
> area and chave random effects
>
> Thank you all!
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From v|n|c|u@@@@m@|@ @end|ng |rom hotm@||@com  Fri Jan 18 12:05:30 2019
From: v|n|c|u@@@@m@|@ @end|ng |rom hotm@||@com (Vinicius Maia)
Date: Fri, 18 Jan 2019 11:05:30 +0000
Subject: [R-sig-ME] gamm shifting points locations in plots: corrected post
Message-ID: <DM6PR14MB289226ACEE6D82E59E72A325BE9C0@DM6PR14MB2892.namprd14.prod.outlook.com>

Hi folks!

When running a gamm and seing its plots I found the residuals locations are shifted, I presume its because the smooth is fitted after  taking  the random effects into account (obviously). But I am not understanding how its possible to change data points location in these plots. Oddly the fitted smooth changes with the residuals inclusion in the following zuur example. The same is valid for the lmer, I usually plot the fitted line whith the residuals, but I have never seen this discrepancy in a points positions with the fitted line before. The gamm and  lmer with both random effects show a similar linear fitted line. I would appreciate if anyone could clarify what is happening.

plots are in the link: https://uploaddeimagens.com.br/imagens/rsigd-min-jpg/

my codes:


a) mod1=gamm(y1_r_prop~s(areia), random = list(area=~1,chave=~1),data=dadosest)
plot(mod1$gam,residuals=TRUE,pch=1.3,shift = coef(mod1$gam)[1])

b) mod2=gamm(y1_r_prop~s(areia), random = list(chave=~1),method="ML",data=dadosest)
plot(mod2$gam,residuals=TRUE,pch=1.3,shift = coef(mod2$gam)[1])

c) mod3=gamm(y1_r_prop~s(areia), random = list(area=~1),method="ML",data=dadosest)
plot(mod3$gam,residuals=TRUE,pch=1.3,shift = coef(mod3$gam)[1])

d) mod4=gam(y1_r_prop~s(areia),method="ML",data=dadosest)
plot(mod4,residuals=TRUE,pch=1.3,shift = coef(mod4)[1])

e) plot(y1_r_prop~areia)
curve(18.3048+-2.2142*x,add=TRUE) # coefficients from a lmer with both area and chave random effects

as you can see y1_r_prop ranges from 0 to 50, and in gamm plots 10 to 30 sometimes.
and point positions varies with random effects inclusion.

A reproduciple example with zuur chapter 13, were a similar situation happen

library(nlme)
library(lme4)
library(mgcv)
library(glmmTMB) ; data(Owls)

Owls$NCalls <- Owls$SiblingNegotiation
Owls$fNest <- factor(Owls$Nest)

plot(Owls$NCalls~Owls$ArrivalTime)

#NCalls ranges from 0 to 30

O4.gamm <- gamm(NCalls ~  s(ArrivalTime),random = list(fNest =~ 1), data = Owls, family = poisson)

plot(O4.gamm$gam,residuals=TRUE,pch=1,cex=0.7,shift=coef(O4.gamm$gam)[1])
plot(O4.gamm$gam,residuals=TRUE,pch=1,cex=0.7)
plot(O4.gamm$gam,shift=coef(O4.gamm$gam)[1])
plot(O4.gamm$gam)

#here NCalls range changes to 1 to 5

#the fitted line also changes a lot with the inclusion of residuals

mod1=gam(NCalls~s(ArrivalTime),family = poisson, data = Owls)
plot(mod1,residuals=TRUE,pch=1,cex=0.7,shift=coef(mod1)[1])
plot(mod1,residuals=TRUE,pch=1,cex=0.7)
plot(mod1,shift=coef(mod1)[1])
plot(mod1)

#here NCalls range changes to 1 to 12
# and the fitted line also changes a lot with the inclusion of residuals


Thank you all!



	[[alternative HTML version deleted]]


From w@ngj|@we|92 @end|ng |rom hotm@||@com  Sun Jan 20 01:53:22 2019
From: w@ngj|@we|92 @end|ng |rom hotm@||@com (=?gb2312?B?zfUgvM7svw==?=)
Date: Sun, 20 Jan 2019 00:53:22 +0000
Subject: [R-sig-ME] How to add a random slope using gam package?
Message-ID: <TY2PR06MB31020B2B4BB4EF63761E5085C29E0@TY2PR06MB3102.apcprd06.prod.outlook.com>

Dear R users,


I am using the gam function in *gam package* to model the ozone pollution concentration according to some environmental covariates. I have scanned the document of this package but it seems like only random intercept is available.  Does anyone know how to add a random slope into model?

Any advice would be greatly appreciated!
Jiawei Wang


???? Outlook<http://aka.ms/weboutlook>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Tue Jan 22 18:57:43 2019
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Tue, 22 Jan 2019 12:57:43 -0500
Subject: [R-sig-ME] upcoming changes to lme4 default optimizers (lmer only)
Message-ID: <7e6e55d4-2553-d099-7c7f-b24a0055328b@gmail.com>


 tl;dr If you want your results to stay identical in the upcoming
release of lme4 (1.1-20), you'll need to use
lmerControl(optimizer="bobyqa") to specify the current (but changing)
default optimizer.

  As of 1.1-20 (headed to CRAN soon) the default optimizer for lmer
changes from "bobyqa" (Powell's BOBYQA method as implemented in the
minqa package) to "nloptwrap" (the same method as implemented in the
nloptr package).  We have found only minor changes (often for the
better) in our own tests, and we haven't seen any problems with
downstream package tests, but this modification *will* change results
from lmer; generally only by a small amount, but in some unstable cases
could lead do quite different answers. For example, here:

https://stats.stackexchange.com/questions/384528/lme-and-lmer-giving-conflicting-results/384539#384539

bobyqa gives the wrong answer (it finds a mode away from zero) while
nloptwrap gets it right. In this case the difference is a clear
improvement, but nonlinear optimization being what it is we can't
guarantee that some changes won't be for the worse.

  We have not changed the glmer default (time window is too short for
this release, which should go to CRAN by 27 Jan), but probably will in
the future.

  If you can test the development version (on GitHub) and report [here
or on the lme4 issues list] any significant problems you encounter  in
the next few days, that would be great.

  cheers
    Ben Bolker


From Ph||||p@A|d@y @end|ng |rom mp|@n|  Tue Jan 22 19:43:24 2019
From: Ph||||p@A|d@y @end|ng |rom mp|@n| (Alday, Phillip)
Date: Tue, 22 Jan 2019 18:43:24 +0000
Subject: [R-sig-ME] 
 upcoming changes to lme4 default optimizers (lmer only)
In-Reply-To: <7e6e55d4-2553-d099-7c7f-b24a0055328b@gmail.com>
References: <7e6e55d4-2553-d099-7c7f-b24a0055328b@gmail.com>
Message-ID: <6c56b62c-a147-9189-af18-0d559c2bd015@mpi.nl>

For psycholinguistics datasets (especially EEG and eye movements), I
have noticed that nloptwrap sometimes reports convergence failure when
"bobyqa" does not. I think the "problem" is with xtol_abs -- changing
maxeval has no impact, but allowing for smaller x-step changes does.
This is in itself perhaps indicative of other problems with the model
and data, but could once again lead to previously "converged" models not
converging in a new lme4 version. The non-converged nloptwrapr models
are definitely faster to compute though.

I haven't tested this systematically (neither in terms of comparing fits
of bobyqa vs non-converged vs converged nloptwrap nor in terms of
datasets) and there is no free lunch when it comes to optimizers, but it
might nonetheless be convenient to list some of these optimizer-specific
parameters in ?convergence or ?lmerControl beyond their use in the
examples. If not, then this message on a public mailing list might still
be useful hint for posterity. :)

So far I haven't noticed any issues with local optima like the
CrossValidated question.

Best,

Phillip

On 22/1/19 6:57 pm, Ben Bolker wrote:
>  tl;dr If you want your results to stay identical in the upcoming
> release of lme4 (1.1-20), you'll need to use
> lmerControl(optimizer="bobyqa") to specify the current (but changing)
> default optimizer.
>
>   As of 1.1-20 (headed to CRAN soon) the default optimizer for lmer
> changes from "bobyqa" (Powell's BOBYQA method as implemented in the
> minqa package) to "nloptwrap" (the same method as implemented in the
> nloptr package).  We have found only minor changes (often for the
> better) in our own tests, and we haven't seen any problems with
> downstream package tests, but this modification *will* change results
> from lmer; generally only by a small amount, but in some unstable cases
> could lead do quite different answers. For example, here:
>
> https://stats.stackexchange.com/questions/384528/lme-and-lmer-giving-conflicting-results/384539#384539
>
> bobyqa gives the wrong answer (it finds a mode away from zero) while
> nloptwrap gets it right. In this case the difference is a clear
> improvement, but nonlinear optimization being what it is we can't
> guarantee that some changes won't be for the worse.
>
>   We have not changed the glmer default (time window is too short for
> this release, which should go to CRAN by 27 Jan), but probably will in
> the future.
>
>   If you can test the development version (on GitHub) and report [here
> or on the lme4 issues list] any significant problems you encounter  in
> the next few days, that would be great.
>
>   cheers
>     Ben Bolker
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From ce||@@o||@more|r@ @end|ng |rom gm@||@com  Wed Jan 23 13:19:48 2019
From: ce||@@o||@more|r@ @end|ng |rom gm@||@com (=?UTF-8?Q?C=C3=A9lia_Sofia_Moreira?=)
Date: Wed, 23 Jan 2019 12:19:48 +0000
Subject: [R-sig-ME] Bayesian Conway-Maxwell-Poisson distribution
Message-ID: <CAJhU6eFxFcyfMSRdnHQoFHK_0vWPVMYKTzzoU60C_NetC3iv_g@mail.gmail.com>

Dear all,

I'm performing some mixed-modeling analyses using the
Conway-maxwell-Poisson distribution, from the glmmTMB package (my response
variables are underdispersed). I would like to complement some results from
this frequentist statistics with Bayesian results, mainly because the
significance of some (glmmTMB) results are borderline, i.e., are marginally
significant. More specifically, I would like to compute the (Bayesian)
evidence ratio in order to compare two quantities.

In the Bayesian context, I usually use the brms package. Unfortunately,
this package has no specific family to deal with underdispersed count data
(it has only for overdispersed count data). As far as I know, there are no
Bayesian packages with specific families for underdispersed count data.

So, I see two main options, and I would be grateful if you could tell me
your opinion about the best way to follow:

1) performing post-hoc MCMC with glmmTMB, as described in
https://cran.r-project.org/web/packages/glmmTMB/vignettes/mcmc.html
and then use the output model to obtain the evidence ratios (through brms,
for example). However, In this case, I don't know if it is possible to
define the priors and, in the affirmative case, how to do it. Do you?

2) using the gamma-count distribution (R code available at
https://discourse.mc-stan.org/t/brms-and-conway-maxwell-poisson-distribution/7368/6
). However, in this case, I have to work with an unfamiliar package (to me;
some difficulties may arise further)...

Can you please tell me your opinion about this problem?

Kind regards,
csm

	[[alternative HTML version deleted]]


From @v|k @end|ng |rom @@v|on@huj|@@c@||  Thu Jan 24 05:39:43 2019
From: @v|k @end|ng |rom @@v|on@huj|@@c@|| (Avraham Kluger)
Date: Thu, 24 Jan 2019 04:39:43 +0000
Subject: [R-sig-ME] Correlations among random variables
In-Reply-To: <268119BD809ADB44968F39A7A86398FA0171786471@Pegasus2.hustaff.huji.local>
References: <268119BD809ADB44968F39A7A86398FA0171784372@Pegasus2.hustaff.huji.local>,
 <07ad2b28-debd-40a3-a4f8-a63efb13c010@email.android.com>
 <268119BD809ADB44968F39A7A86398FA0171786471@Pegasus2.hustaff.huji.local>
Message-ID: <268119BD809ADB44968F39A7A86398FA017234D293@Pegasus2.hustaff.huji.local>

Hi again,

A reminder, I am trying to get a stable estimate of the correlation between two random errors in a mode like:

lme(outcome ~   0 + focalcode + partcode,  random = ~ 0 + focalcode + partcode|focalid/dyadid,   data = x)

I figured out that five different methods yield identical covariance estimate, as I show below.  My new question is CAN YOU CONSTRAIN RESIDUAL TO ZERO in either lme4 or nlme? This may allow convergence and hence estimate of standard error for the covariance.
-------------------------------------------------------------------------------------------------------------------------------------------------
Method					Var Focal	Var Partner	Residual	Cov 		Corr 
--------------------------------------------------------------------------------------------------------------------------------------------------
SPSS					.549		.423						.239 
lavaan (SEM)				.552		.425				.116 		.239
							
lme4					.326		.200		.223		.115		.451
nlme					.376		.502		.046				.265
lavaan (SEM) with latent residuals	.331 		.457		.094		.116 		.298
-------------------------------------------------------------------------------------------------------------------------------------------------
Some methods printout the covariance and some do not (or fail to converge properly), but all suggest that the covariance is .115 or .116.  For MLM in R:

lme4, cov = .451 * sqrt(.326 * .200) =  .115
nlme, cov = .265 * sqrt(.376 * .502) =  .115

for SPSS that runs without any warning, cov = .239  * sqrt(.549 * .423) =  .115

I appreciate your patience with a novice,

Best,

Avi


-----Original Message-----
From: Avraham Kluger 
Sent: Wednesday, January 16, 2019 12:27 PM
To: Uanhoro, James <uanhoro.1 at buckeyemail.osu.edu>; R-sig-mixed-models at r-project.org
Cc: Kenny, David <david.kenny at uconn.edu>
Subject: RE: [R-sig-ME] Correlations among random variables

Hi,

I thank James, Wolfgang , Thierry , and David for educating me.  To summarize all responses, you seem to suggest: Do not trust your data when it comes to the correlation between the error terms.  I still have some questions about intepreration.  Note that I do not care particularity about these data, but I am trying to grasp the principle.  First, I repeated the calculations with SPSS and with SEM (in the later using wide-data formal, and placing equality constraints within Focal and within Partner on intercepts, variances, and covariances).  The summary of all the results can be found below.

Error variances, covariances, and correlations  by software/analytic approach
---------------------------------------------------------------------------------------------------------------------------------
	                 Var Focal 	Var Partner	Residual	Total Var Focal	Total Var Partner  Cov (SE/CI)	        Corr (SE/CI)
---------------------------------------------------------------------------------------------------------------------------------
SPSS         	.549	                .423	              NA	        .549	                 .423	                NA 	                .239 (.046)
lme4	        .326	                .200	              .223	        .549	                 .423	                0.115(*)	        .451
nlme	        .376	                .502	              .046	        .549	                 .423	                        (*)	        .265
lavaan (SEM)	.331 	        .457	              .094	        .551	                 .425	                0.116 (.024)	.298
---------------------------------------------------------------------------------------------------------------------------------
*	cannot get confidence intervals on var-cov components: Non-positive definite approximate variance-covariance

1.  How are the correlations calculated.  Assuming that the covariance is .115 (or .116), what are the variances that their product serve as a denominator in transforming the covariance into the correlation?
2.  David Dupphy wrote:

FWIW, the raw correlations (that are being analysed) when reordered as a a bivariate setup:
partner-focal r=0.31 [dyads]
focal intraclass r=0.10 (jack SE=0.06)  [clustered on focal] partner intraclass r=0.33 (jSE=0.07) [clustered on focal]

Given that all analyses suggest a POSITIVE covariance, would it be reasonable to conclude that there is a positive correlation in the population, although its magnitude is uncertain?  Or would you still believe these signals are noise?

I with deep gratitude to this form and all the help received,

Avi



From: Uanhoro, James [uanhoro.1 at buckeyemail.osu.edu]

Sent: Tuesday, January 15, 2019 3:39 PM

To: Avraham Kluger

Cc: R-sig-mixed-models at r-project.org

Subject: Re: [R-sig-ME] Correlations among random variables











***********************************************************************************


This email contains links to a hosting company that is frequented by scammers.

Please think before you click any links.



The Ohio State University will NEVER ask you for your account information by email.

If you receive such a message, please report it to report-phish at osu.edu



NEVER reply to any email asking you for your account information 

or other personal details. For more information or to get help,

contact the IT Service Desk by calling 614-688-HELP (4357).

***********************************************************************************






A correlation always exists in the population, if we assume the realistic position that exactly nil effects are just not true.





The points Wolfgang Viechtbauer makes are very important. That is also the crux of this discussion: https://github.com/lme4/lme4/issues/175#issuecomment-33580591.





If your question pertains to the variance covariance matrix that changes from lme4 to nlme, then you're probably asking too much of the data. And the only information you should trust about this particular variance covariance matrix is that  you should trust nothing else about it.





James.







On Jan 15, 2019 00:58, Avraham Kluger <avik at savion.huji.ac.il> wrote:

Dear Thierry, 



I thank you for the reference to your blog.  It does raise questions abo ut noise in the data.  However, if the correlations are significant in any method, would you conclude that a correlation exists in the population?




Avi 



From: Thierry Onkelinx<mailto:thierry.onkelinx at inbo.be> 

Sent: Monday, January 14, 2019 2:42 PM 

To: Avraham Kluger<mailto:avik at savion.huji.ac.il> 

Cc: R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org>; Kenny, David<mailto:david.kenny at uconn.edu>


Subject: Re: [R-sig-ME] Correlations among random variables 



Dear Avraham, 



Do you have a huge amount of random effects? If not, the variance estimates have a large uncertainty. So that you precieve as a strong diverence is actually just noise from the model uncertainty. I wrote a small blog post on the number of random effect levels  and the resulting uncertainty on the variance estimates: https://www.muscardinus.be/2018/09/number-random-effect-levels/




Best regards, 



ir. Thierry Onkelinx 

Statisticus / Statistician 



Vlaamse Overheid / Government of Flanders 

INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST


Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance 

thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be> 

Havenlaan 88 bus 73, 1000 Brussel 

www.inbo.be<http://www.inbo.be> 



///////////////////////////////////////////////////////////////////////////////////////////


To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher


The plural of anecdote is not data. ~ Roger Brinner 

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey


///////////////////////////////////////////////////////////////////////////////////////////




[https://inbo-website-prd-532750756126.s3-eu-west-1.amazonaws.com/inbologoleeuw_nl.png]<https://www.inbo.be>






Op zo 13 jan. 2019 om 04:42 schreef Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>:


Hi, 



Following help from James Uanhoro, I produced models with correlated random variables both with nlms and lme4.  Curiously, the estimates of the variances and their correlation are identical, but the error variances and their correlation are not.  Yet, the sum  of the error variances are identical.  For example, in the nlme code below the error for focalcode + residual is .376 + .046 = .422, and in the lme4 it is .2 + .222 = .422.  Can anyone guide me to read about these different decompositions?  This may explain  the different correlations among the error terms .265 vs. .451. 



MLM with nlme 

          Variance                            StdDev    Corr 

focalid = pdLogChol(0 + focalcode + partcode) 

focalcode 0.20840953                          0.4565189 foclcd 

partcode  0.06089854                          0.2467763 0.699 

dyadid =  pdLogChol(0 + focalcode + partcode) 

focalcode 0.37674500                          0.6137956 foclcd 

partcode  0.50282362                          0.7091006 0.265 

Residual  0.04641050                          0.2154310 



MLM with lme4 



> as.data.frame(VarCorr(lme4Mlm))

             grp      var1     var2       vcov     sdcor 

1 dyadid:focalid focalcode     <NA> 0.20018050 0.4474153 

2 dyadid:focalid  partcode     <NA> 0.32625940 0.5711912 

3 dyadid:focalid focalcode partcode 0.11523355 0.4509065 

4        focalid focalcode     <NA> 0.20840930 0.4565187 

5        focalid  partcode     <NA> 0.06089846 0.2467761 

6        focalid focalcode partcode 0.07872740 0.6988182 

7       Residual      <NA>     <NA> 0.22297497 0.4722023 

> VarCorr(lme4Mlm)

Groups         Name      Std.Dev. Corr 

dyadid:focalid focalcode 0.44742 

                partcode  0.57119  0.451 

focalid        focalcode 0.45652 

                partcode  0.24678  0.699 

Residual                 0.47220 



Yours, 



Avi Kluger<http://avikluger.wix.com/avi-kluger> 













From: Uanhoro, James<mailto:uanhoro.1 at buckeyemail.osu.edu<mailto:uanhoro.1 at buckeyemail.osu.edu>>


Sent: Saturday, January 12, 2019 5:38 PM 

To: Avraham Kluger<mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>> 

Subject: Re: [R-sig-ME] Correlations among random variables 



That is the exact lme4 syntax that replicates the nlme model. I did not attempt to run it but when I did, I noticed that the problem is you have two random effects, each having 624 values, 624 by 2 equals the sample size. lme4 will not run when this happens  hence the error message. You can tell lme4 to run nevertheless 



summary(mlms <- lmer( 

  outcome ~ 0 + focalcode + partcode + 

    (0 + focalcode + partcode | focalid/dyadid), 

  Chapter10_df, 

  control = lmerControl(check.nobs.vs.nRE = "warning"))) 



This will force the program to run the code, and will print out warnings. The log likelihood was the same with that from nlme indicating that the models are the same. But the variance-covariance matrices for the random effects by the interaction between focalid  and dyadid - the inner cluster variable - are different. Which one do you trust? Probably neither - there is just not enough information to estimate this varCov matrix. See this thread for commentary on the issue: https://github.com/lme4/lme4/issues/175




Hope this helps, -James. 







On Jan 12 2019, at 9:49 am, Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>> wrote:


Oops, 



Actually the code produces error 









Error: number of observations (=1248) <= number of random effects (=1248) for term (0 + focalcode + partcode | dyadid:focalid); the random-effects parameters and the residual variance (or scale parameter) are probably unidentifiable












The code below should run on any machine. 











Best 











Avi 







> 













################################################################################ 



#    **************************** R companion for ************************** 



# 



# Kenny, D. A., Kashy, D. A., & Cook, W. L. (2006). Dyadic data analysis. 



# New York: Guilford Press. 



# 



# lme code developed by Limor Borut: limor.borut at mail.huji.ac.il<mailto:limor.borut at mail.huji.ac.il><mailto:limor.borut at mail.huji.ac.il<mailto:limor.borut at mail.huji.ac.il>>




# written by Avi Kluger: avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>




# 



#                              CHAPTER 10 -- one with many SRM 



# 



################################################################################ 



rm(list = ls())                               # Clean the Global Environment 



cat ("\014")                                  # Clean the R console 



if (is.null(dev.list()) == FALSE) dev.off()   # Clean Plots 







# Read (in SPSS format) from Kenny's book site and replicate Table 9.1 



if (!require('foreign')) install.packages('foreign'); library('foreign') 



Chapter10_df <- read.spss("http://davidakenny.net/kkc/c10/c10_recip.sav", 



               to.data.frame = TRUE, use.value.labels = FALSE) 







head(Chapter10_df) 







if (!require("nlme")) install.packages("nlme"); suppressMessages(library(nlme)) 







mlm <- lme(outcome ~   0 + focalcode + 0 + partcode, 



            random = ~ 0 + focalcode + partcode|focalid/dyadid, 



            data = Chapter10_df) 



summary(mlm) 



intervals(mlm) 



mlmOutput <- VarCorr(mlm) 



VarCorr(mlm) 







cat( 



"Actor   variance = ",   round(as.numeric(VarCorr(mlm)[, "Variance"][3]), 3), 



"\nPartner variance = ", round(as.numeric(VarCorr(mlm)[, "Variance"][2]), 3), 



"\nGeneralized Reciprocity = ", round(as.numeric(VarCorr(mlm)[, "Corr"][3]), 3), 



"\nDyadic Reciprocity = ", round(as.numeric(VarCorr(mlm)[, "Corr"][6]), 3), "\n" 



) 







# Very Important Note.  The original data coded with 0 the focal person. 



# Therefore the first random variable above is partner variance.  Reversing



# the codes below make the results more intuitive.  I thank David Kenny for



# Clarifying this issue. 



Chapter10_df$focalcode <- 1- Chapter10_df$focalcode 



Chapter10_df$partcode  <- 1- Chapter10_df$partcode 



mlm <- lme(outcome ~   0 + focalcode + 0 + partcode, 



            random = ~ 0 + focalcode + partcode|focalid/dyadid, 



            data = Chapter10_df) 



VarCorr(mlm) 







# An alternative suggsted by James Uanhoro <uanhoro.1 at buckeyemail.osu.edu<mailto:uanhoro.1 at buckeyemail.osu.edu><mailto:uanhoro.1 at buckeyemail.osu.edu<mailto:uanhoro.1 at buckeyemail.osu.edu>>>








if (!require("lme4")) install.packages("lme4"); suppressMessages(library(lme4)) 



mlm <- lmer(outcome   ~ 0 + focalcode + partcode + role + 



                       (0 + focalcode + partcode | focalid/ dyadid), 



                       data = Chapter10_df) 



summary(mlm) 



VarCorr(mlm) 















From: Uanhoro, James [mailto:uanhoro.1 at buckeyemail.osu.edu<mailto:uanhoro.1 at buckeyemail.osu.edu>]


Sent: Saturday, January 12, 2019 4:41 PM 

To: Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>>


Subject: RE: [R-sig-ME] Correlations among random variables 





My reply addressed to issues: covariance between error terms; and between random effects.


The only change to lme4 for random effects is to switch the double pipe to a single pipe in the random effects specification of the model, as I have done below:




mlm <- lmer(outcome   ~ 0 + focalcode + partcode + role + 

                       (0 + focalcode + partcode | focalid/ dyadid), 

                       data = df) 



James. 







On Jan 12, 2019 09:05, Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>> wrote:




Dear James, 







As you might have seen in my second message to r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org><mailto:r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>, my student solved this problem with nlme.  Would you  know how to write it in lme4? 







Here is the working nlme code 







mlm <- lme(outcome ~   0 + focalcode + 0 + partcode, 



            random = ~ 0 + focalcode + partcode|focalid/dyadid, 



            data = Chapter10_df) 







Best, 







Avi 















From: Uanhoro, James [mailto:uanhoro.1 at buckeyemail.osu.edu<mailto:uanhoro.1 at buckeyemail.osu.edu>]


Sent: Saturday, January 12, 2019 3:17 PM 

To: Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>>


Cc: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org><mailto:r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>


Subject: Re: [R-sig-ME] Correlations among random variables 





In the lme4 syntax, you'd have to change the double pipe, ||, when specifying the random effects to a single pipe, |, to permit a correlation between random effects. lme4 is faster than nlme.




Assuming lme4 and nlme are the only options ... If you want to specify an error covariance structure beyond the covariance structure implied by standard multilevel models, you will have to use nlme. nlme has a `correlation =` argument that allows different  covariance structures, corSymm (general/unstructured), corCompSymm (exchangeable), ...










On Jan 12, 2019 02:01, Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>> wrote:




Hi, 



I am struggling to analyze, in R, MLM models that specify correlations among random variables, as can be done with SPSS, SAS, or MlWin.




Consider the following code in SPSS 

----------------------------- 

MIXED 

   Outcome  BY role  WITH focalcode partcode 

   /FIXED = focalcode partcode | NOINT 

   /PRINT = SOLUTION TESTCOV 

   /RANDOM focalcode partcode | SUBJECT(focalid) COVTYPE(UNR) 

   /REPEATED = role | SUBJECT(focalid*dyadid) COVTYPE(UNR). 

----------------------------- 

And a minimal code (with data) in R 



----------------------------- 

df <- read.csv("https://raw.githubusercontent.com/avi-kluger/RCompanion4DDABook/master/Chapter%2010/Chapter10_df.csv")


head(df) 

library(lme4) 



mlm <- lmer(outcome   ~ 0 + focalcode + partcode + role + 

                       (0 + focalcode + partcode|| focalid/ dyadid), 

                       data = df) 

summary(mlm) 

----------------------------- 



These SPSS and R codes produce the same variance estimates.  However, SPSS also produces a correlation among "focalcode" and "partcode."  How can this be done in R?  Is it also possible to produce the correlation among the respective error variances (as in  SPSS)? 



Additional information 



1.       MOTIVATION.  The question arises from David Kenny's work on one-with-many reciprocal designs (e.g., a manager rate all subordinates, and all subordinates rate the same manager).  These models estimate the variance stemming from the one (e.g., managers)
 and the many (e.g., subordinates), and the correlation among them (termed generalized reciprocity).  The data and codes for SAS etc. are available at http://davidakenny.net/kkc/c10/c10.htm.




2.       SPSS OUTPUT (download HTML file):  https://www.dropbox.com/s/eqch0kq6djtbsfx/One%20with%20many%20SPSS%20output.htm?dl=1




Sincerely, 



Avi Kluger 

https://www.avi-kluger.com/ 



[[alternative HTML version deleted]] 



_______________________________________________ 

R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org><mailto:R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org>> mailing list


https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models 







        [[alternative HTML version deleted]] 



_______________________________________________ 

R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list


https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models 

d

        [[alternative HTML version deleted]] 



_______________________________________________ 

R-sig-mixed-models at r-project.org mailing list 

https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models 


From bbo|ker @end|ng |rom gm@||@com  Thu Jan 24 16:30:19 2019
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Thu, 24 Jan 2019 10:30:19 -0500
Subject: [R-sig-ME] Correlations among random variables
In-Reply-To: <268119BD809ADB44968F39A7A86398FA017234D293@Pegasus2.hustaff.huji.local>
References: <268119BD809ADB44968F39A7A86398FA0171784372@Pegasus2.hustaff.huji.local>
 <07ad2b28-debd-40a3-a4f8-a63efb13c010@email.android.com>
 <268119BD809ADB44968F39A7A86398FA0171786471@Pegasus2.hustaff.huji.local>
 <268119BD809ADB44968F39A7A86398FA017234D293@Pegasus2.hustaff.huji.local>
Message-ID: <5b402416-cd19-6f03-bddf-038441f8fc16@gmail.com>


   Not in lme4:
http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#setting-residual-variances-to-a-fixed-value-zero-or-other

  In lme you can set the residual std dev to a fixed small value, but
not to exactly zero:

lme(Reaction~Days,random=~1|Subject,sleepstudy,control=list(sigma=1e-8))

  If you're trying to test that the covariance is significantly
positive, I think getting the standard error is the wrong approach; the
Wald (quadratic) approximation is often very bad for random-effects
variances and covariances.  I would suggest profile confidence intervals
or a likelihood ratio test.



On 2019-01-23 11:39 p.m., Avraham Kluger wrote:
> Hi again,
> 
> A reminder, I am trying to get a stable estimate of the correlation between two random errors in a mode like:
> 
> lme(outcome ~   0 + focalcode + partcode,  random = ~ 0 + focalcode + partcode|focalid/dyadid,   data = x)
> 
> I figured out that five different methods yield identical covariance estimate, as I show below.  My new question is CAN YOU CONSTRAIN RESIDUAL TO ZERO in either lme4 or nlme? This may allow convergence and hence estimate of standard error for the covariance.
> -------------------------------------------------------------------------------------------------------------------------------------------------
> Method					Var Focal	Var Partner	Residual	Cov 		Corr 
> --------------------------------------------------------------------------------------------------------------------------------------------------
> SPSS					.549		.423						.239 
> lavaan (SEM)				.552		.425				.116 		.239
> 							
> lme4					.326		.200		.223		.115		.451
> nlme					.376		.502		.046				.265
> lavaan (SEM) with latent residuals	.331 		.457		.094		.116 		.298
> -------------------------------------------------------------------------------------------------------------------------------------------------
> Some methods printout the covariance and some do not (or fail to converge properly), but all suggest that the covariance is .115 or .116.  For MLM in R:
> 
> lme4, cov = .451 * sqrt(.326 * .200) =  .115
> nlme, cov = .265 * sqrt(.376 * .502) =  .115
> 
> for SPSS that runs without any warning, cov = .239  * sqrt(.549 * .423) =  .115
> 
> I appreciate your patience with a novice,
> 
> Best,
> 
> Avi
> 
> 
> -----Original Message-----
> From: Avraham Kluger 
> Sent: Wednesday, January 16, 2019 12:27 PM
> To: Uanhoro, James <uanhoro.1 at buckeyemail.osu.edu>; R-sig-mixed-models at r-project.org
> Cc: Kenny, David <david.kenny at uconn.edu>
> Subject: RE: [R-sig-ME] Correlations among random variables
> 
> Hi,
> 
> I thank James, Wolfgang , Thierry , and David for educating me.  To summarize all responses, you seem to suggest: Do not trust your data when it comes to the correlation between the error terms.  I still have some questions about intepreration.  Note that I do not care particularity about these data, but I am trying to grasp the principle.  First, I repeated the calculations with SPSS and with SEM (in the later using wide-data formal, and placing equality constraints within Focal and within Partner on intercepts, variances, and covariances).  The summary of all the results can be found below.
> 
> Error variances, covariances, and correlations  by software/analytic approach
> ---------------------------------------------------------------------------------------------------------------------------------
> 	                 Var Focal 	Var Partner	Residual	Total Var Focal	Total Var Partner  Cov (SE/CI)	        Corr (SE/CI)
> ---------------------------------------------------------------------------------------------------------------------------------
> SPSS         	.549	                .423	              NA	        .549	                 .423	                NA 	                .239 (.046)
> lme4	        .326	                .200	              .223	        .549	                 .423	                0.115(*)	        .451
> nlme	        .376	                .502	              .046	        .549	                 .423	                        (*)	        .265
> lavaan (SEM)	.331 	        .457	              .094	        .551	                 .425	                0.116 (.024)	.298
> ---------------------------------------------------------------------------------------------------------------------------------
> *	cannot get confidence intervals on var-cov components: Non-positive definite approximate variance-covariance
> 
> 1.  How are the correlations calculated.  Assuming that the covariance is .115 (or .116), what are the variances that their product serve as a denominator in transforming the covariance into the correlation?
> 2.  David Dupphy wrote:
> 
> FWIW, the raw correlations (that are being analysed) when reordered as a a bivariate setup:
> partner-focal r=0.31 [dyads]
> focal intraclass r=0.10 (jack SE=0.06)  [clustered on focal] partner intraclass r=0.33 (jSE=0.07) [clustered on focal]
> 
> Given that all analyses suggest a POSITIVE covariance, would it be reasonable to conclude that there is a positive correlation in the population, although its magnitude is uncertain?  Or would you still believe these signals are noise?
> 
> I with deep gratitude to this form and all the help received,
> 
> Avi
> 
> 
> 
> From: Uanhoro, James [uanhoro.1 at buckeyemail.osu.edu]
> 
> Sent: Tuesday, January 15, 2019 3:39 PM
> 
> To: Avraham Kluger
> 
> Cc: R-sig-mixed-models at r-project.org
> 
> Subject: Re: [R-sig-ME] Correlations among random variables
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> ***********************************************************************************
> 
> 
> This email contains links to a hosting company that is frequented by scammers.
> 
> Please think before you click any links.
> 
> 
> 
> The Ohio State University will NEVER ask you for your account information by email.
> 
> If you receive such a message, please report it to report-phish at osu.edu
> 
> 
> 
> NEVER reply to any email asking you for your account information 
> 
> or other personal details. For more information or to get help,
> 
> contact the IT Service Desk by calling 614-688-HELP (4357).
> 
> ***********************************************************************************
> 
> 
> 
> 
> 
> 
> A correlation always exists in the population, if we assume the realistic position that exactly nil effects are just not true.
> 
> 
> 
> 
> 
> The points Wolfgang Viechtbauer makes are very important. That is also the crux of this discussion: https://github.com/lme4/lme4/issues/175#issuecomment-33580591.
> 
> 
> 
> 
> 
> If your question pertains to the variance covariance matrix that changes from lme4 to nlme, then you're probably asking too much of the data. And the only information you should trust about this particular variance covariance matrix is that  you should trust nothing else about it.
> 
> 
> 
> 
> 
> James.
> 
> 
> 
> 
> 
> 
> 
> On Jan 15, 2019 00:58, Avraham Kluger <avik at savion.huji.ac.il> wrote:
> 
> Dear Thierry, 
> 
> 
> 
> I thank you for the reference to your blog.  It does raise questions abo ut noise in the data.  However, if the correlations are significant in any method, would you conclude that a correlation exists in the population?
> 
> 
> 
> 
> Avi 
> 
> 
> 
> From: Thierry Onkelinx<mailto:thierry.onkelinx at inbo.be> 
> 
> Sent: Monday, January 14, 2019 2:42 PM 
> 
> To: Avraham Kluger<mailto:avik at savion.huji.ac.il> 
> 
> Cc: R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org>; Kenny, David<mailto:david.kenny at uconn.edu>
> 
> 
> Subject: Re: [R-sig-ME] Correlations among random variables 
> 
> 
> 
> Dear Avraham, 
> 
> 
> 
> Do you have a huge amount of random effects? If not, the variance estimates have a large uncertainty. So that you precieve as a strong diverence is actually just noise from the model uncertainty. I wrote a small blog post on the number of random effect levels  and the resulting uncertainty on the variance estimates: https://www.muscardinus.be/2018/09/number-random-effect-levels/
> 
> 
> 
> 
> Best regards, 
> 
> 
> 
> ir. Thierry Onkelinx 
> 
> Statisticus / Statistician 
> 
> 
> 
> Vlaamse Overheid / Government of Flanders 
> 
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST
> 
> 
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance 
> 
> thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be> 
> 
> Havenlaan 88 bus 73, 1000 Brussel 
> 
> www.inbo.be<http://www.inbo.be> 
> 
> 
> 
> ///////////////////////////////////////////////////////////////////////////////////////////
> 
> 
> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> 
> 
> The plural of anecdote is not data. ~ Roger Brinner 
> 
> The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
> 
> 
> ///////////////////////////////////////////////////////////////////////////////////////////
> 
> 
> 
> 
> [https://inbo-website-prd-532750756126.s3-eu-west-1.amazonaws.com/inbologoleeuw_nl.png]<https://www.inbo.be>
> 
> 
> 
> 
> 
> 
> Op zo 13 jan. 2019 om 04:42 schreef Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>:
> 
> 
> Hi, 
> 
> 
> 
> Following help from James Uanhoro, I produced models with correlated random variables both with nlms and lme4.  Curiously, the estimates of the variances and their correlation are identical, but the error variances and their correlation are not.  Yet, the sum  of the error variances are identical.  For example, in the nlme code below the error for focalcode + residual is .376 + .046 = .422, and in the lme4 it is .2 + .222 = .422.  Can anyone guide me to read about these different decompositions?  This may explain  the different correlations among the error terms .265 vs. .451. 
> 
> 
> 
> MLM with nlme 
> 
>           Variance                            StdDev    Corr 
> 
> focalid = pdLogChol(0 + focalcode + partcode) 
> 
> focalcode 0.20840953                          0.4565189 foclcd 
> 
> partcode  0.06089854                          0.2467763 0.699 
> 
> dyadid =  pdLogChol(0 + focalcode + partcode) 
> 
> focalcode 0.37674500                          0.6137956 foclcd 
> 
> partcode  0.50282362                          0.7091006 0.265 
> 
> Residual  0.04641050                          0.2154310 
> 
> 
> 
> MLM with lme4 
> 
> 
> 
>> as.data.frame(VarCorr(lme4Mlm))
> 
>              grp      var1     var2       vcov     sdcor 
> 
> 1 dyadid:focalid focalcode     <NA> 0.20018050 0.4474153 
> 
> 2 dyadid:focalid  partcode     <NA> 0.32625940 0.5711912 
> 
> 3 dyadid:focalid focalcode partcode 0.11523355 0.4509065 
> 
> 4        focalid focalcode     <NA> 0.20840930 0.4565187 
> 
> 5        focalid  partcode     <NA> 0.06089846 0.2467761 
> 
> 6        focalid focalcode partcode 0.07872740 0.6988182 
> 
> 7       Residual      <NA>     <NA> 0.22297497 0.4722023 
> 
>> VarCorr(lme4Mlm)
> 
> Groups         Name      Std.Dev. Corr 
> 
> dyadid:focalid focalcode 0.44742 
> 
>                 partcode  0.57119  0.451 
> 
> focalid        focalcode 0.45652 
> 
>                 partcode  0.24678  0.699 
> 
> Residual                 0.47220 
> 
> 
> 
> Yours, 
> 
> 
> 
> Avi Kluger<http://avikluger.wix.com/avi-kluger> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> From: Uanhoro, James<mailto:uanhoro.1 at buckeyemail.osu.edu<mailto:uanhoro.1 at buckeyemail.osu.edu>>
> 
> 
> Sent: Saturday, January 12, 2019 5:38 PM 
> 
> To: Avraham Kluger<mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>> 
> 
> Subject: Re: [R-sig-ME] Correlations among random variables 
> 
> 
> 
> That is the exact lme4 syntax that replicates the nlme model. I did not attempt to run it but when I did, I noticed that the problem is you have two random effects, each having 624 values, 624 by 2 equals the sample size. lme4 will not run when this happens  hence the error message. You can tell lme4 to run nevertheless 
> 
> 
> 
> summary(mlms <- lmer( 
> 
>   outcome ~ 0 + focalcode + partcode + 
> 
>     (0 + focalcode + partcode | focalid/dyadid), 
> 
>   Chapter10_df, 
> 
>   control = lmerControl(check.nobs.vs.nRE = "warning"))) 
> 
> 
> 
> This will force the program to run the code, and will print out warnings. The log likelihood was the same with that from nlme indicating that the models are the same. But the variance-covariance matrices for the random effects by the interaction between focalid  and dyadid - the inner cluster variable - are different. Which one do you trust? Probably neither - there is just not enough information to estimate this varCov matrix. See this thread for commentary on the issue: https://github.com/lme4/lme4/issues/175
> 
> 
> 
> 
> Hope this helps, -James. 
> 
> 
> 
> 
> 
> 
> 
> On Jan 12 2019, at 9:49 am, Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>> wrote:
> 
> 
> Oops, 
> 
> 
> 
> Actually the code produces error 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> Error: number of observations (=1248) <= number of random effects (=1248) for term (0 + focalcode + partcode | dyadid:focalid); the random-effects parameters and the residual variance (or scale parameter) are probably unidentifiable
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> The code below should run on any machine. 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> Best 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> Avi 
> 
> 
> 
> 
> 
> 
> 
>>
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> ################################################################################ 
> 
> 
> 
> #    **************************** R companion for ************************** 
> 
> 
> 
> # 
> 
> 
> 
> # Kenny, D. A., Kashy, D. A., & Cook, W. L. (2006). Dyadic data analysis. 
> 
> 
> 
> # New York: Guilford Press. 
> 
> 
> 
> # 
> 
> 
> 
> # lme code developed by Limor Borut: limor.borut at mail.huji.ac.il<mailto:limor.borut at mail.huji.ac.il><mailto:limor.borut at mail.huji.ac.il<mailto:limor.borut at mail.huji.ac.il>>
> 
> 
> 
> 
> # written by Avi Kluger: avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>
> 
> 
> 
> 
> # 
> 
> 
> 
> #                              CHAPTER 10 -- one with many SRM 
> 
> 
> 
> # 
> 
> 
> 
> ################################################################################ 
> 
> 
> 
> rm(list = ls())                               # Clean the Global Environment 
> 
> 
> 
> cat ("\014")                                  # Clean the R console 
> 
> 
> 
> if (is.null(dev.list()) == FALSE) dev.off()   # Clean Plots 
> 
> 
> 
> 
> 
> 
> 
> # Read (in SPSS format) from Kenny's book site and replicate Table 9.1 
> 
> 
> 
> if (!require('foreign')) install.packages('foreign'); library('foreign') 
> 
> 
> 
> Chapter10_df <- read.spss("http://davidakenny.net/kkc/c10/c10_recip.sav", 
> 
> 
> 
>                to.data.frame = TRUE, use.value.labels = FALSE) 
> 
> 
> 
> 
> 
> 
> 
> head(Chapter10_df) 
> 
> 
> 
> 
> 
> 
> 
> if (!require("nlme")) install.packages("nlme"); suppressMessages(library(nlme)) 
> 
> 
> 
> 
> 
> 
> 
> mlm <- lme(outcome ~   0 + focalcode + 0 + partcode, 
> 
> 
> 
>             random = ~ 0 + focalcode + partcode|focalid/dyadid, 
> 
> 
> 
>             data = Chapter10_df) 
> 
> 
> 
> summary(mlm) 
> 
> 
> 
> intervals(mlm) 
> 
> 
> 
> mlmOutput <- VarCorr(mlm) 
> 
> 
> 
> VarCorr(mlm) 
> 
> 
> 
> 
> 
> 
> 
> cat( 
> 
> 
> 
> "Actor   variance = ",   round(as.numeric(VarCorr(mlm)[, "Variance"][3]), 3), 
> 
> 
> 
> "\nPartner variance = ", round(as.numeric(VarCorr(mlm)[, "Variance"][2]), 3), 
> 
> 
> 
> "\nGeneralized Reciprocity = ", round(as.numeric(VarCorr(mlm)[, "Corr"][3]), 3), 
> 
> 
> 
> "\nDyadic Reciprocity = ", round(as.numeric(VarCorr(mlm)[, "Corr"][6]), 3), "\n" 
> 
> 
> 
> ) 
> 
> 
> 
> 
> 
> 
> 
> # Very Important Note.  The original data coded with 0 the focal person. 
> 
> 
> 
> # Therefore the first random variable above is partner variance.  Reversing
> 
> 
> 
> # the codes below make the results more intuitive.  I thank David Kenny for
> 
> 
> 
> # Clarifying this issue. 
> 
> 
> 
> Chapter10_df$focalcode <- 1- Chapter10_df$focalcode 
> 
> 
> 
> Chapter10_df$partcode  <- 1- Chapter10_df$partcode 
> 
> 
> 
> mlm <- lme(outcome ~   0 + focalcode + 0 + partcode, 
> 
> 
> 
>             random = ~ 0 + focalcode + partcode|focalid/dyadid, 
> 
> 
> 
>             data = Chapter10_df) 
> 
> 
> 
> VarCorr(mlm) 
> 
> 
> 
> 
> 
> 
> 
> # An alternative suggsted by James Uanhoro <uanhoro.1 at buckeyemail.osu.edu<mailto:uanhoro.1 at buckeyemail.osu.edu><mailto:uanhoro.1 at buckeyemail.osu.edu<mailto:uanhoro.1 at buckeyemail.osu.edu>>>
> 
> 
> 
> 
> 
> 
> 
> 
> if (!require("lme4")) install.packages("lme4"); suppressMessages(library(lme4)) 
> 
> 
> 
> mlm <- lmer(outcome   ~ 0 + focalcode + partcode + role + 
> 
> 
> 
>                        (0 + focalcode + partcode | focalid/ dyadid), 
> 
> 
> 
>                        data = Chapter10_df) 
> 
> 
> 
> summary(mlm) 
> 
> 
> 
> VarCorr(mlm) 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> From: Uanhoro, James [mailto:uanhoro.1 at buckeyemail.osu.edu<mailto:uanhoro.1 at buckeyemail.osu.edu>]
> 
> 
> Sent: Saturday, January 12, 2019 4:41 PM 
> 
> To: Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>>
> 
> 
> Subject: RE: [R-sig-ME] Correlations among random variables 
> 
> 
> 
> 
> 
> My reply addressed to issues: covariance between error terms; and between random effects.
> 
> 
> The only change to lme4 for random effects is to switch the double pipe to a single pipe in the random effects specification of the model, as I have done below:
> 
> 
> 
> 
> mlm <- lmer(outcome   ~ 0 + focalcode + partcode + role + 
> 
>                        (0 + focalcode + partcode | focalid/ dyadid), 
> 
>                        data = df) 
> 
> 
> 
> James. 
> 
> 
> 
> 
> 
> 
> 
> On Jan 12, 2019 09:05, Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>> wrote:
> 
> 
> 
> 
> Dear James, 
> 
> 
> 
> 
> 
> 
> 
> As you might have seen in my second message to r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org><mailto:r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>, my student solved this problem with nlme.  Would you  know how to write it in lme4? 
> 
> 
> 
> 
> 
> 
> 
> Here is the working nlme code 
> 
> 
> 
> 
> 
> 
> 
> mlm <- lme(outcome ~   0 + focalcode + 0 + partcode, 
> 
> 
> 
>             random = ~ 0 + focalcode + partcode|focalid/dyadid, 
> 
> 
> 
>             data = Chapter10_df) 
> 
> 
> 
> 
> 
> 
> 
> Best, 
> 
> 
> 
> 
> 
> 
> 
> Avi 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> From: Uanhoro, James [mailto:uanhoro.1 at buckeyemail.osu.edu<mailto:uanhoro.1 at buckeyemail.osu.edu>]
> 
> 
> Sent: Saturday, January 12, 2019 3:17 PM 
> 
> To: Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>>
> 
> 
> Cc: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org><mailto:r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
> 
> 
> Subject: Re: [R-sig-ME] Correlations among random variables 
> 
> 
> 
> 
> 
> In the lme4 syntax, you'd have to change the double pipe, ||, when specifying the random effects to a single pipe, |, to permit a correlation between random effects. lme4 is faster than nlme.
> 
> 
> 
> 
> Assuming lme4 and nlme are the only options ... If you want to specify an error covariance structure beyond the covariance structure implied by standard multilevel models, you will have to use nlme. nlme has a `correlation =` argument that allows different  covariance structures, corSymm (general/unstructured), corCompSymm (exchangeable), ...
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> On Jan 12, 2019 02:01, Avraham Kluger <avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il><mailto:avik at savion.huji.ac.il<mailto:avik at savion.huji.ac.il>>> wrote:
> 
> 
> 
> 
> Hi, 
> 
> 
> 
> I am struggling to analyze, in R, MLM models that specify correlations among random variables, as can be done with SPSS, SAS, or MlWin.
> 
> 
> 
> 
> Consider the following code in SPSS 
> 
> ----------------------------- 
> 
> MIXED 
> 
>    Outcome  BY role  WITH focalcode partcode 
> 
>    /FIXED = focalcode partcode | NOINT 
> 
>    /PRINT = SOLUTION TESTCOV 
> 
>    /RANDOM focalcode partcode | SUBJECT(focalid) COVTYPE(UNR) 
> 
>    /REPEATED = role | SUBJECT(focalid*dyadid) COVTYPE(UNR). 
> 
> ----------------------------- 
> 
> And a minimal code (with data) in R 
> 
> 
> 
> ----------------------------- 
> 
> df <- read.csv("https://raw.githubusercontent.com/avi-kluger/RCompanion4DDABook/master/Chapter%2010/Chapter10_df.csv")
> 
> 
> head(df) 
> 
> library(lme4) 
> 
> 
> 
> mlm <- lmer(outcome   ~ 0 + focalcode + partcode + role + 
> 
>                        (0 + focalcode + partcode|| focalid/ dyadid), 
> 
>                        data = df) 
> 
> summary(mlm) 
> 
> ----------------------------- 
> 
> 
> 
> These SPSS and R codes produce the same variance estimates.  However, SPSS also produces a correlation among "focalcode" and "partcode."  How can this be done in R?  Is it also possible to produce the correlation among the respective error variances (as in  SPSS)? 
> 
> 
> 
> Additional information 
> 
> 
> 
> 1.       MOTIVATION.  The question arises from David Kenny's work on one-with-many reciprocal designs (e.g., a manager rate all subordinates, and all subordinates rate the same manager).  These models estimate the variance stemming from the one (e.g., managers)
>  and the many (e.g., subordinates), and the correlation among them (termed generalized reciprocity).  The data and codes for SAS etc. are available at http://davidakenny.net/kkc/c10/c10.htm.
> 
> 
> 
> 
> 2.       SPSS OUTPUT (download HTML file):  https://www.dropbox.com/s/eqch0kq6djtbsfx/One%20with%20many%20SPSS%20output.htm?dl=1
> 
> 
> 
> 
> Sincerely, 
> 
> 
> 
> Avi Kluger 
> 
> https://www.avi-kluger.com/ 
> 
> 
> 
> [[alternative HTML version deleted]] 
> 
> 
> 
> _______________________________________________ 
> 
> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org><mailto:R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org>> mailing list
> 
> 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models 
> 
> 
> 
> 
> 
> 
> 
>         [[alternative HTML version deleted]] 
> 
> 
> 
> _______________________________________________ 
> 
> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
> 
> 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models 
> 
> d
> 
>         [[alternative HTML version deleted]] 
> 
> 
> 
> _______________________________________________ 
> 
> R-sig-mixed-models at r-project.org mailing list 
> 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models 
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From @|m@pot|er @end|ng |rom gm@||@com  Tue Jan 22 17:14:53 2019
From: @|m@pot|er @end|ng |rom gm@||@com (Simon POTIER)
Date: Tue, 22 Jan 2019 17:14:53 +0100
Subject: [R-sig-ME] How to analyse data with two replicates per group
Message-ID: <CANiGZfVXyJADh4O9mgUA5YSm6vwoqXgTR0Vgs2DgU2BsS32koA@mail.gmail.com>

Dear all,

Thanks in advance for your help.
I will try to be precise, do not hesitate to tell me if you need more
details.

I have a dataset measuring some features of raptors eyes (let says here eye
size for exemple). I have measured both eyes (left and right eyes) of 47
individuals.
If I want to compare eye size with age and sex, what type of analysis
should I use:

1) Should I analyse both eye separately (which can be interesting in term
of vision)?
2) Can/should I analyse both eye together including Individual as a random
effect? In other terms, I was wondering whether including individuals as
random effect (with only two levels for each individual (left and right
eye)) is possible?

One more question, I want to compare the AIC of my models, but I get this
error message:
Warning message:
In AIC.default(mm.edge, mm.edge2, mm.edge3) :
  models are not all fitted to the same number of observations

I understand that the problem come from the fact that I have some NA in
some fixed effect. How can I deal with this please because it is not
possible to compare AIC?

Best regards,

Simon Potier

-- 
*******************************************************
Simon Potier
Department of Biology
Lund University
S?lvegatan 35
S-22362 Lund
Sweden
*******************************************************
phone +336 11 31 67 16
sim.potier at gmail.com
https://www.simonpotier.fr
*******************************************************

	[[alternative HTML version deleted]]


From ||@@@b|rnb@um @end|ng |rom |@u@de  Wed Jan 23 08:27:32 2019
From: ||@@@b|rnb@um @end|ng |rom |@u@de (Birnbaum, Lisa)
Date: Wed, 23 Jan 2019 07:27:32 +0000
Subject: [R-sig-ME] negative binomial ICC for the GLMM
Message-ID: <88721699B70CE04A9C5D0F71A8E61C1175EEF472@mbx2.exch.fau.de>

Dear Timothy,

I wanna calculate a negative binomial ICC for the null model with Rdistanc as depedent variable and schoolid as cluster variable. In the last step I get an error message. I am not sure if I did it right with the numerator. Can you please help me?

This is my R-code:

#Specify the model first

  model = glmer.nb(formula = Rdistanc ~ 1 + (1 | schoolid), data = dat)

 # Then execute this code for ICC calculation for negative binomial.
  #There is nothing you need to change to the code, it's a function independent of the data.
  #################################
  ICC.NB <- function(model, numerator){
    require(lme4)
    mout <- data.frame(VarCorr(model)) # random intercept model variances
    sigma_a2 <- sum(mout[mout$grp %in% numerator, "vcov"]) # random effect(s) in numerator

    sigma_2 <- sum(mout["vcov"]) # sum of random effects variance in denominator
    beta <- as.numeric(fixef(model)["(Intercept)"]) # fixed effect intercept
    r <- getME(object = model, "glmer.nb.theta") # theta
    icc <- (exp(sigma_a2) - 1) / ((exp(sigma_2) - 1) + (exp(sigma_2) /
                                                          r) + (exp(-beta) - (sigma_2 / 2)))
    return(icc)
  }
  ##################################
  # Lastly, run
  ICC.NB (glmer.nb(formula = dat$Rdistanc ~ 1 + (1 | dat$schoolid), data = dat, numerator = dat$schoolid))

I really would appreciate your help.
Best regards,
Lisa

--
Lisa Birnbaum, M.A.
Friedrich-Alexander-Universit?t Erlangen-N?rnberg
Lehrstuhl f?r Empirische Bildungsforschung
Regensburger Str. 160
90478 N?rnberg
Mail: lisa.birnbaum at fau.de


	[[alternative HTML version deleted]]


From wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n|  Thu Jan 24 17:42:52 2019
From: wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n| (Viechtbauer, Wolfgang (SP))
Date: Thu, 24 Jan 2019 16:42:52 +0000
Subject: [R-sig-ME] Correlations among random variables
In-Reply-To: <5b402416-cd19-6f03-bddf-038441f8fc16@gmail.com>
References: <268119BD809ADB44968F39A7A86398FA0171784372@Pegasus2.hustaff.huji.local>
 <07ad2b28-debd-40a3-a4f8-a63efb13c010@email.android.com>
 <268119BD809ADB44968F39A7A86398FA0171786471@Pegasus2.hustaff.huji.local>
 <268119BD809ADB44968F39A7A86398FA017234D293@Pegasus2.hustaff.huji.local>
 <5b402416-cd19-6f03-bddf-038441f8fc16@gmail.com>
Message-ID: <2d209e1f27dd4319a59fa206319ffbfe@UM-MAIL3214.unimaas.nl>

For the data Avi is working with, the default optimizer (nlminb) fails. Switching to 'optim' (with method 'BFGS') works. Here is the fully reproducible code:

df <- read.csv("https://raw.githubusercontent.com/avi-kluger/RCompanion4DDABook/master/Chapter%2010/Chapter10_df.csv")

library(nlme) 

df$focalcode <- 1 - df$focalcode 
df$partcode  <- 1 - df$partcode 

### overparamterized model
res1 <- lme(outcome ~ 0 + focalcode + partcode, random = ~ 0 + focalcode + partcode | focalid/dyadid, data = df)
summary(res1)

### contrain sigma to a very small value
res2 <- lme(outcome ~ 0 + focalcode + partcode, random = ~ 0 + focalcode + partcode | focalid/dyadid, data = df, control=list(sigma=1e-8, opt="optim"))
summary(res2)

Just for fun, I also fitted the same model using 'metafor'. While it was not really made for analyzing raw data like this, it can be used to fit the same model (with the devel version) and then sigma can be constrained exactly to 0:

devtools::install_github("wviechtb/metafor")
library(metafor)

df$dyadid.in.focalid <- interaction(df$focalid, df$dyadid)
res3 <- rma.mv(outcome ~ 0 + focalcode + partcode, V=0, random = list(~ 0 + focalcode + partcode | focalid, ~ 0 + focalcode + partcode | dyadid.in.focalid), struct="GEN", data = df, sparse=TRUE)
res3

(note that 'focalid/dyadid' doesn't work at the moment, so you have to create the nested factor manually first; also, model fitting can be slow with rma.mv(), so you might have to wait a bit for it to converge)

The results for res2 and res3 are quite close.

Best,
Wolfgang

-----Original Message-----
From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Ben Bolker
Sent: Thursday, 24 January, 2019 16:30
To: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Correlations among random variables

   Not in lme4:
http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#setting-residual-variances-to-a-fixed-value-zero-or-other

  In lme you can set the residual std dev to a fixed small value, but
not to exactly zero:

lme(Reaction~Days,random=~1|Subject,sleepstudy,control=list(sigma=1e-8))

  If you're trying to test that the covariance is significantly
positive, I think getting the standard error is the wrong approach; the
Wald (quadratic) approximation is often very bad for random-effects
variances and covariances.  I would suggest profile confidence intervals
or a likelihood ratio test.

On 2019-01-23 11:39 p.m., Avraham Kluger wrote:
> Hi again,
> 
> A reminder, I am trying to get a stable estimate of the correlation between two random errors in a mode like:
> 
> lme(outcome ~   0 + focalcode + partcode,  random = ~ 0 + focalcode + partcode|focalid/dyadid,   data = x)
> 
> I figured out that five different methods yield identical covariance estimate, as I show below.  My new question is CAN YOU CONSTRAIN RESIDUAL TO ZERO in either lme4 or nlme? This may allow convergence and hence estimate of standard error for the covariance.
> -------------------------------------------------------------------------------------------------------------------------------------------------
> Method					Var Focal	Var Partner	Residual	Cov 		Corr 
> --------------------------------------------------------------------------------------------------------------------------------------------------
> SPSS					.549		.423						.239 
> lavaan (SEM)				.552		.425				.116 		.239
> 							
> lme4					.326		.200		.223		.115		.451
> nlme					.376		.502		.046				.265
> lavaan (SEM) with latent residuals	.331 		.457		.094		.116 		.298
> -------------------------------------------------------------------------------------------------------------------------------------------------
> Some methods printout the covariance and some do not (or fail to converge properly), but all suggest that the covariance is .115 or .116.  For MLM in R:
> 
> lme4, cov = .451 * sqrt(.326 * .200) =  .115
> nlme, cov = .265 * sqrt(.376 * .502) =  .115
> 
> for SPSS that runs without any warning, cov = .239  * sqrt(.549 * .423) =  .115
> 
> I appreciate your patience with a novice,
> 
> Best,
> 
> Avi


From ry@n@@|mmon@ @end|ng |rom duke@edu  Thu Jan 24 18:55:21 2019
From: ry@n@@|mmon@ @end|ng |rom duke@edu (Ryan Simmons)
Date: Thu, 24 Jan 2019 17:55:21 +0000
Subject: [R-sig-ME] Bayesian Conway-Maxwell-Poisson distribution
Message-ID: <SN6PR05MB5341E1997EF158F28A7B97AAFD9A0@SN6PR05MB5341.namprd05.prod.outlook.com>

Hi csm,

There is a package called "combayes" (https://github.com/cchanialidis/combayes) for Bayesian inference of Conway-Maxwell-Poisson regression models. I don't know much about this package, but it appears to at least be well-documented, and if nothing else then looking at the source code may guide you in developing a solution. The paper describing the algorithms used in this package is called "Efficient inference for COM-Poisson regression models" published in Statistics and Computing, May 2018, Volume 8, Issue 3, p.595-608 by Charalampos Chanialdis, Ludger Evers, Tereza Neocleous, and Agostino Nobile. The doi is: https://doi.org/10.1007/s11222-017-9750-x, and you can find it online here: https://link.springer.com/article/10.1007/s11222-017-9750-x. 

Good luck!

-Ryan

-----Original Message-----

Message: 1
Date: Wed, 23 Jan 2019 12:19:48 +0000
From: =?UTF-8?Q?C=C3=A9lia_Sofia_Moreira?=
	<celiasofiamoreira at gmail.com>
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Bayesian Conway-Maxwell-Poisson distribution
Message-ID:
	<CAJhU6eFxFcyfMSRdnHQoFHK_0vWPVMYKTzzoU60C_NetC3iv_g at mail.gmail.com>
Content-Type: text/plain; charset="utf-8"

Dear all,

I'm performing some mixed-modeling analyses using the Conway-maxwell-Poisson distribution, from the glmmTMB package (my response variables are underdispersed). I would like to complement some results from this frequentist statistics with Bayesian results, mainly because the significance of some (glmmTMB) results are borderline, i.e., are marginally significant. More specifically, I would like to compute the (Bayesian) evidence ratio in order to compare two quantities.

In the Bayesian context, I usually use the brms package. Unfortunately, this package has no specific family to deal with underdispersed count data (it has only for overdispersed count data). As far as I know, there are no Bayesian packages with specific families for underdispersed count data.

So, I see two main options, and I would be grateful if you could tell me your opinion about the best way to follow:

1) performing post-hoc MCMC with glmmTMB, as described in https://urldefense.proofpoint.com/v2/url?u=https-3A__cran.r-2Dproject.org_web_packages_glmmTMB_vignettes_mcmc.html&d=DwICAg&c=imBPVzF25OnBgGmVOlcsiEgHoG1i6YHLR0Sj_gZ4adc&r=szMipnZzQMIctqnszKsX-AUNBHtRn663-NaJh5_rHBE&m=OSZv1-WYaULDAcsvdVobBHQNa1WmbZwz0TLojWBC5Y8&s=EE3SUxLx3KVRUdyPXQWLbr30Az-XaNQkp9hZzPOxYZQ&e=
and then use the output model to obtain the evidence ratios (through brms, for example). However, In this case, I don't know if it is possible to define the priors and, in the affirmative case, how to do it. Do you?

2) using the gamma-count distribution (R code available at https://urldefense.proofpoint.com/v2/url?u=https-3A__discourse.mc-2Dstan.org_t_brms-2Dand-2Dconway-2Dmaxwell-2Dpoisson-2Ddistribution_7368_6&d=DwICAg&c=imBPVzF25OnBgGmVOlcsiEgHoG1i6YHLR0Sj_gZ4adc&r=szMipnZzQMIctqnszKsX-AUNBHtRn663-NaJh5_rHBE&m=OSZv1-WYaULDAcsvdVobBHQNa1WmbZwz0TLojWBC5Y8&s=45Hcvw8fez6Qc8Ptv5ag2eHHdu5B0qKnInnoZz4Yn2o&e=
). However, in this case, I have to work with an unfamiliar package (to me; some difficulties may arise further)...

Can you please tell me your opinion about this problem?

Kind regards,
csm

	[[alternative HTML version deleted]]


From @v|k @end|ng |rom @@v|on@huj|@@c@||  Thu Jan 24 20:24:26 2019
From: @v|k @end|ng |rom @@v|on@huj|@@c@|| (Avraham Kluger)
Date: Thu, 24 Jan 2019 19:24:26 +0000
Subject: [R-sig-ME] Correlations among random variables
Message-ID: <268119BD809ADB44968F39A7A86398FA017234E825@Pegasus2.hustaff.huji.local>

Dear Wolfgang,



Your metafor solution beautifully replicates, to the dot, results from SPSS and lavann.  Can you obtain CI around the estimates, rho, and phi?



Avi

--------------------------------------------------



For the data Avi is working with, the default optimizer (nlminb) fails. Switching to 'optim' (with method 'BFGS') works. Here is the fully reproducible code:



df <- read.csv("https://raw.githubusercontent.com/avi-kluger/RCompanion4DDABook/master/Chapter%2010/Chapter10_df.csv")



library(nlme)



df$focalcode <- 1 - df$focalcode

df$partcode  <- 1 - df$partcode



### overparamterized model

res1 <- lme(outcome ~ 0 + focalcode + partcode, random = ~ 0 + focalcode + partcode | focalid/dyadid, data = df)

summary(res1)



### contrain sigma to a very small value

res2 <- lme(outcome ~ 0 + focalcode + partcode, random = ~ 0 + focalcode + partcode | focalid/dyadid, data = df, control=list(sigma=1e-8, opt="optim"))

summary(res2)



Just for fun, I also fitted the same model using 'metafor'. While it was not really made for analyzing raw data like this, it can be used to fit the same model (with the devel version) and then sigma can be constrained exactly to 0:



devtools::install_github("wviechtb/metafor")

library(metafor)



df$dyadid.in.focalid <- interaction(df$focalid, df$dyadid)

res3 <- rma.mv(outcome ~ 0 + focalcode + partcode, V=0, random = list(~ 0 + focalcode + partcode | focalid, ~ 0 + focalcode + partcode | dyadid.in.focalid), struct="GEN", data = df, sparse=TRUE)

res3



(note that 'focalid/dyadid' doesn't work at the moment, so you have to create the nested factor manually first; also, model fitting can be slow with rma.mv(), so you might have to wait a bit for it to converge)



The results for res2 and res3 are quite close.



Best,

Wolfgang


	[[alternative HTML version deleted]]


From wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n|  Thu Jan 24 22:17:53 2019
From: wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n| (Viechtbauer, Wolfgang (SP))
Date: Thu, 24 Jan 2019 21:17:53 +0000
Subject: [R-sig-ME] Correlations among random variables
In-Reply-To: <268119BD809ADB44968F39A7A86398FA017234E825@Pegasus2.hustaff.huji.local>
References: <268119BD809ADB44968F39A7A86398FA017234E825@Pegasus2.hustaff.huji.local>
Message-ID: <c44cc653c3cc42b4997c8192942242c3@UM-MAIL3214.unimaas.nl>

Happy to hear that.

Yes, you can get profile likelihood CIs with:

ci.rho <- confint(res3, rho=1, verbose=TRUE)
ci.rho
ci.phi <- confint(res3, phi=1, verbose=TRUE)
ci.phi

But this will take quite some time, so you might want to run this when you don't need your computer for a while. Here is what I get:

    estimate  ci.lb  ci.ub 
rho   0.6988 0.3660 1.0000

    estimate  ci.lb  ci.ub 
phi   0.2390 0.1465 0.3274

Best,
Wolfgang

-----Original Message-----
From: Avraham Kluger [mailto:avik at savion.huji.ac.il] 
Sent: Thursday, 24 January, 2019 20:24
To: r-sig-mixed-models at r-project.org
Cc: Viechtbauer, Wolfgang (SP)
Subject: Re: [R-sig-ME] Correlations among random variables

Dear Wolfgang,

Your metafor solution beautifully replicates, to the dot, results from SPSS and lavann.? Can you obtain CI around the estimates, rho, and phi?

Avi
--------------------------------------------------

For the data Avi is working with, the default optimizer (nlminb) fails. Switching to 'optim' (with method 'BFGS') works. Here is the fully reproducible code:

df <- read.csv("https://raw.githubusercontent.com/avi-kluger/RCompanion4DDABook/master/Chapter%2010/Chapter10_df.csv")

library(nlme) 

df$focalcode <- 1 - df$focalcode
df$partcode? <- 1 - df$partcode 

### overparamterized model
res1 <- lme(outcome ~ 0 + focalcode + partcode, random = ~ 0 + focalcode + partcode | focalid/dyadid, data = df)
summary(res1)

### contrain sigma to a very small value
res2 <- lme(outcome ~ 0 + focalcode + partcode, random = ~ 0 + focalcode + partcode | focalid/dyadid, data = df, control=list(sigma=1e-8, opt="optim"))
summary(res2)

Just for fun, I also fitted the same model using 'metafor'. While it was not really made for analyzing raw data like this, it can be used to fit the same model (with the devel version) and then sigma can be constrained exactly to 0:

devtools::install_github("wviechtb/metafor")
library(metafor)

df$dyadid.in.focalid <- interaction(df$focalid, df$dyadid)
res3 <- rma.mv(outcome ~ 0 + focalcode + partcode, V=0, random = list(~ 0 + focalcode + partcode | focalid, ~ 0 + focalcode + partcode | dyadid.in.focalid), struct="GEN", data = df, sparse=TRUE)
res3

(note that 'focalid/dyadid' doesn't work at the moment, so you have to create the nested factor manually first; also, model fitting can be slow with rma.mv(), so you might have to wait a bit for it to converge)

The results for res2 and res3 are quite close.

Best,
Wolfgang


From th|erry@onke||nx @end|ng |rom |nbo@be  Fri Jan 25 09:28:03 2019
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Fri, 25 Jan 2019 09:28:03 +0100
Subject: [R-sig-ME] How to analyse data with two replicates per group
In-Reply-To: <CANiGZfVXyJADh4O9mgUA5YSm6vwoqXgTR0Vgs2DgU2BsS32koA@mail.gmail.com>
References: <CANiGZfVXyJADh4O9mgUA5YSm6vwoqXgTR0Vgs2DgU2BsS32koA@mail.gmail.com>
Message-ID: <CAJuCY5xcCeS+uy4=iUrexn6OPCaixsP9XMheUCkXYi93vzdXEQ@mail.gmail.com>

Dear Simon,

IMHO you should use individual as a random effect. If a systematic
difference between both eyes are relevant / plausible, then add a variable
coding for left or right to the fixed effect. Otherwise, don't include it
in the model.

Create a subset of the data which has no missing values in all variables
used in your full model. Fit all your models on this subset. Then you can
compare AIC.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op do 24 jan. 2019 om 17:30 schreef Simon POTIER <sim.potier at gmail.com>:

> Dear all,
>
> Thanks in advance for your help.
> I will try to be precise, do not hesitate to tell me if you need more
> details.
>
> I have a dataset measuring some features of raptors eyes (let says here eye
> size for exemple). I have measured both eyes (left and right eyes) of 47
> individuals.
> If I want to compare eye size with age and sex, what type of analysis
> should I use:
>
> 1) Should I analyse both eye separately (which can be interesting in term
> of vision)?
> 2) Can/should I analyse both eye together including Individual as a random
> effect? In other terms, I was wondering whether including individuals as
> random effect (with only two levels for each individual (left and right
> eye)) is possible?
>
> One more question, I want to compare the AIC of my models, but I get this
> error message:
> Warning message:
> In AIC.default(mm.edge, mm.edge2, mm.edge3) :
>   models are not all fitted to the same number of observations
>
> I understand that the problem come from the fact that I have some NA in
> some fixed effect. How can I deal with this please because it is not
> possible to compare AIC?
>
> Best regards,
>
> Simon Potier
>
> --
> *******************************************************
> Simon Potier
> Department of Biology
> Lund University
> S?lvegatan 35
> S-22362 Lund
> Sweden
> *******************************************************
> phone +336 11 31 67 16
> sim.potier at gmail.com
> https://www.simonpotier.fr
> *******************************************************
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From |@t@rcev|ch @end|ng |rom we@t-|nc@com  Mon Jan 28 20:47:55 2019
From: |@t@rcev|ch @end|ng |rom we@t-|nc@com (Leigh Ann Starcevich)
Date: Mon, 28 Jan 2019 11:47:55 -0800
Subject: [R-sig-ME] Independent random effects with US and AR1 structures
 using glmmTMB
Message-ID: <CAO4izq424CLaf5kxBmVXFXVaKpJVs5tdkqxpQ3GA+RH0ZhEaqw@mail.gmail.com>

I am modeling counts collected at several Visits at several Sites.  The
data structure is similar to the "dp" data set in this document, where f =
Site and tt = Visit:

http://bbolker.github.io/mixedmodels-misc/notes/corr_braindump.html

To reproduce, use the following function from
https://github.com/bbolker/mixedmodels-misc/blob/master/notes/lme4ord_poiscorsim.R
:

simCor1 <- function(phi=0.8,sdgrp=2,sdres=1,
                    npergrp=20,ngrp=20,
                    seed=NULL,
                    ## set linkinv/simfun for GLMM sims
                    linkinv=identity,
                    simfun=identity) {
    if (!is.null(seed)) set.seed(seed)
    cmat <- sdres*phi^abs(outer(0:(npergrp-1),0:(npergrp-1),"-"))
    errs <- MASS::mvrnorm(ngrp,mu=rep(0,npergrp),Sigma=cmat)
    ranef <- rnorm(ngrp,mean=0,sd=sdgrp)
    d <- data.frame(f=rep(1:ngrp,each=npergrp))
    eta <- ranef[as.numeric(d$f)] + c(t(errs)) ## unpack errors by row
    mu <- linkinv(eta)
    d$y <- simfun(mu)
    d$tt <- factor(rep(1:npergrp,ngrp))
    return(d)
}

library(glmmTMB)
set.seed(101)


# Simulate data
dp <- simCor1(phi=0.8,sdgrp=2,sdres=1,seed=101,
              linkinv=exp,simfun=function(x) rpois(length(x),lambda=x))

This model is used:

glmmTMB_pois_fit <- glmmTMB(y~1 + (1|f) + ar1(tt-1|f), data=dp, family
= poisson)

and the following output is obtained:

#  Groups Name        Variance Std.Dev. Corr

#  f      (Intercept) 5.3065   2.3036
#  f.1    tt1         0.9996   0.9998   0.77 (ar1)
# Number of obs: 400, groups:  f, 20

# Conditional model:
#             Estimate Std. Error z value Pr(>|z|)
# (Intercept)  -0.8319     0.5626  -1.479    0.139

summary(glmmTMB_pois_fit)$varcor$cond

# only one correlation matrix provided in output

Because we are modeling the AR1 structure by each level of the grouping
factor, f, I expected to obtain a different AR1 correlation estimate for
each level of f.  However, the model summary indicates that only one AR1
correlation is estimated.  Is this a mean across levels of f?

If the AR1 correlation structure is modeled across all Sites for a
single-level group variable, ff, how are these parameterizations
different?  The results are VERY different:

# Model one AR1 correlation across levels of f using single-level grouping
factor ff
dp$ff = as.factor(rep(1,nrow(dp)))
glmmTMB_pois_fit2 <- glmmTMB(y~1 + (1|f) + ar1(tt-1|ff), data=dp,
 family=poisson)
summary(glmmTMB_pois_fit2)

# Conditional model:
#  Groups Name        Variance Std.Dev. Corr
#  f      (Intercept) 5.5511   2.3561
#  ff     tt1         0.1049   0.3238   0.04 (ar1)
# Number of obs: 400, groups:  f, 20; ff, 1
#
# Conditional model:
#             Estimate Std. Error z value Pr(>|z|)
# (Intercept)  -0.5406     0.5616  -0.963    0.336

summary(glmmTMB_pois_fit)$varcor$cond

# only one correlation matrix provided in output

Thanks for your help,

--Leigh Ann Starcevich

	[[alternative HTML version deleted]]


From d@r|zopou|o@ @end|ng |rom er@@mu@mc@n|  Wed Jan 30 14:00:48 2019
From: d@r|zopou|o@ @end|ng |rom er@@mu@mc@n| (D. Rizopoulos)
Date: Wed, 30 Jan 2019 13:00:48 +0000
Subject: [R-sig-ME] GLMMs with Adaptive Gaussian Quadrature - GLMMadaptive
 0.5-1
Message-ID: <30a9cfe3-c318-3983-61c2-0265db7b9f95@erasmusmc.nl>

Dear R mixed-model users,

A new version of GLMMadaptive (0.5-1) has been rolled out on CRAN.

Summary: GLMMadaptive fits mixed effects models using adaptive
Gaussian quadrature to approximate the integrals over the random
effects, allowing also for user-specified models.

Website: https://drizopoulos.github.io/GLMMadaptive/

New features:

- Support is provided for the **effects** and **ggeffects** packages for 
producing effect plots.

- Support is provided for the **DHARMa** package for checking the 
goodness-of-fit of fitted mixed models using scaled simulated residuals. 
Examples can be found in: vignette("Goodness_of_Fit", package = 
"GLMMadaptive")

- Function marginal_coefs() for calculating coefficients with a marginal 
/ population-averaged interpretation has a faster implementation.

- The optimizer nlminb() can now also be invoked using the new control 
argument 'optimizer'; a new vignette describes how to control the 
optimization and numerical integration procedures in the package: 
vignette("Optimization", package = "GLMMadaptive")

- New family object students.t() for fitting (robust) linear mixed 
models with a Student's t distribution for the error terms.

As always, any kind of feedback is more than welcome.

Best,
Dimitris

-- 
Dimitris Rizopoulos
Professor of Biostatistics
Department of Biostatistics
Erasmus University Medical Center

Address: PO Box 2040, 3000 CA Rotterdam, the Netherlands
Tel: +31/(0)10/7043478
Fax: +31/(0)10/7043014
Web (personal): http://www.drizopoulos.com/
Web (work): http://www.erasmusmc.nl/biostatistiek/
Blog: http://iprogn.blogspot.nl/

From ce||@@o||@more|r@ @end|ng |rom gm@||@com  Thu Jan 31 15:10:13 2019
From: ce||@@o||@more|r@ @end|ng |rom gm@||@com (=?UTF-8?Q?C=C3=A9lia_Sofia_Moreira?=)
Date: Thu, 31 Jan 2019 14:10:13 +0000
Subject: [R-sig-ME] Reporting results of a mixed-effects model,
 using the negative binomial distribution
Message-ID: <CAJhU6eErnzpDN-_adWYRrmbshYyatp-A_0TAmb5YTJGBZ1Q8Xg@mail.gmail.com>

Dear all,

I am with some difficulties in reporting the results of a mixed-effects
model, with the following main output:

Random effects:
 Groups Name        Variance Std.Dev.
 Subj   (Intercept) 0.1726   0.4155
Number of obs: 164, groups:  Subj, 50

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)  0.09118    0.14255    0.64    0.522
Time         0.11498    0.00515   22.33   <2e-16 ***

This is a negative binomial mixed-effects model (log link), Time is the
indep. variable, and Y is the dep. variable. I would like to know if I can
write that:

*Not accounting for the uncertainty of random effects (i.e., setting the
random effects to zero), the fixed effect of Time on Y is written on the
form:*
*exp(0.09118 + 0.11498 * Time).*

If it is not correct/rigorous, can you please suggest me how should I
report this fixed effect (setting random effects to 0)?

Best regards,
csm

	[[alternative HTML version deleted]]


From M@tthew@Boden @end|ng |rom v@@gov  Wed Jan 30 22:27:20 2019
From: M@tthew@Boden @end|ng |rom v@@gov (Boden, Matthew T.)
Date: Wed, 30 Jan 2019 21:27:20 +0000
Subject: [R-sig-ME] LMER - Plotting of a quadratic effect interacting with
 time
Message-ID: <BN8PR09MB35548B6E8CAC788939EA2E5A87900@BN8PR09MB3554.namprd09.prod.outlook.com>

Hello,

I have a question related to fitting and plotting a longitudinal linear mixed model that includes an interaction between a quadratic effect and time. Data attached.

I fit the following:

Q1 <- lmer(Patients ~ Time*FTE + Time*I(FTE^2) +  (FTE  | ID), data = SHARE)

#Yes, the variables are on very different scales - will take care of that later

I find a sizeable quadratic effect.

Fixed effects:
                Estimate Std. Error t value
(Intercept)    6.760e+03  5.347e+02  12.642
Time           2.033e+01  1.011e+01   2.011
FTE            9.728e+01  8.583e+00  11.335
I(FTE^2)      -5.155e-01  4.000e-02 -12.890
Time:FTE      -5.560e-01  2.254e-01  -2.467
Time:I(FTE^2)  7.371e-03  1.052e-03   7.006

To plot the quadratic interaction, I attempt to use the effects package. However, effects are displayed for Time x FTE, not time by FTE^2. Time x FTE is clearly not the plot that I want (I think...).

e1 <- effect(term="Time:I(FTE^2)", mod=Q1)
ed1<-as.data.frame(e1)
ed1
   Time FTE       fit        se     lower     upper
1     1  17  8277.635  464.3995  7366.770  9188.500
2     4  17  8316.650  463.3763  7407.792  9225.508
......

I tried a workaround, by fitting a model that included FTE^2 as a second, calculated variable in the data set. Using the effects package, I do indeed obtain Time * FTE_sq.

Q2 <- lmer(Patients ~ Time*FTE + Time*FTE_sq +  (FTE  | ID), data = SHARE)

e2 <- effect(term="Time*FTE_sq", mod=Q2)
ed2<-as.data.frame(e2)
ed2

   Time FTE_sq        fit        se      lower      upper
1     1    300 14678.1413  564.5423 13570.8582 15785.4243
2     4    300 14606.9253  563.3827 13501.9166 15711.9340
......

But the plot does not at all look like what I would expect. All lines representing FTE_sq over time are straight.

ggplot(ed2, aes(x=Time, y=fit, color=FTE_sq,group=FTE_sq)) +
   geom_point() +
   geom_line(size=1.2) +
   labs(title = "Time x FTE^2", x= "Time",
        y="Patients", color="FTE^2", fill="FTE^2") + theme_classic() +
        theme(text=element_text(size=10))

Does my problem (obtaining effects for FTE^2*Time and accurately plotting them) relate to my use of the effects package, ggplot, both?

Thank you for the feedback.

Matthew Boden, Ph.D.
Senior Evaluator
Program Evaluation & Resource Center
Office of Mental Health & Suicide Prevention
Veterans Health Administration


From j|ox @end|ng |rom mcm@@ter@c@  Thu Jan 31 20:56:42 2019
From: j|ox @end|ng |rom mcm@@ter@c@ (Fox, John)
Date: Thu, 31 Jan 2019 19:56:42 +0000
Subject: [R-sig-ME] 
 LMER - Plotting of a quadratic effect interacting with time
In-Reply-To: <5600_1548961407_x0VJ3QQo031244_BN8PR09MB35548B6E8CAC788939EA2E5A87900@BN8PR09MB3554.namprd09.prod.outlook.com>
References: <5600_1548961407_x0VJ3QQo031244_BN8PR09MB35548B6E8CAC788939EA2E5A87900@BN8PR09MB3554.namprd09.prod.outlook.com>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC836A7F3AB@FHSDB2D11-2.csu.mcmaster.ca>

Dear Mathew,

> -----Original Message-----
> From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-
> project.org] On Behalf Of Boden, Matthew T. via R-sig-mixed-models
> Sent: Wednesday, January 30, 2019 4:27 PM
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] LMER - Plotting of a quadratic effect interacting with time
> 
> Hello,
> 
> I have a question related to fitting and plotting a longitudinal linear mixed
> model that includes an interaction between a quadratic effect and time. Data
> attached.
> 
> I fit the following:
> 
> Q1 <- lmer(Patients ~ Time*FTE + Time*I(FTE^2) +  (FTE  | ID), data = SHARE)
> 
> #Yes, the variables are on very different scales - will take care of that later
> 
> I find a sizeable quadratic effect.
> 
> Fixed effects:
>                 Estimate Std. Error t value
> (Intercept)    6.760e+03  5.347e+02  12.642
> Time           2.033e+01  1.011e+01   2.011
> FTE            9.728e+01  8.583e+00  11.335
> I(FTE^2)      -5.155e-01  4.000e-02 -12.890
> Time:FTE      -5.560e-01  2.254e-01  -2.467
> Time:I(FTE^2)  7.371e-03  1.052e-03   7.006
> 
> To plot the quadratic interaction, I attempt to use the effects package.
> However, effects are displayed for Time x FTE, not time by FTE^2. Time x FTE
> is clearly not the plot that I want (I think...).
> 
> e1 <- effect(term="Time:I(FTE^2)", mod=Q1)
> ed1<-as.data.frame(e1)
> ed1
>    Time FTE       fit        se     lower     upper
> 1     1  17  8277.635  464.3995  7366.770  9188.500
> 2     4  17  8316.650  463.3763  7407.792  9225.508
> ......
> 
> I tried a workaround, by fitting a model that included FTE^2 as a second,
> calculated variable in the data set. Using the effects package, I do indeed
> obtain Time * FTE_sq.
> 
> Q2 <- lmer(Patients ~ Time*FTE + Time*FTE_sq +  (FTE  | ID), data = SHARE)
> 
> e2 <- effect(term="Time*FTE_sq", mod=Q2)
> ed2<-as.data.frame(e2)
> ed2
> 
>    Time FTE_sq        fit        se      lower      upper
> 1     1    300 14678.1413  564.5423 13570.8582 15785.4243
> 2     4    300 14606.9253  563.3827 13501.9166 15711.9340
> ......
> 
> But the plot does not at all look like what I would expect. All lines
> representing FTE_sq over time are straight.

Try, plot(Effect(c("Time", "FTE"), Q1)) .

More generally, why not fit the model as  Q1 <- lmer(Patients ~ Time*poly(FTE, 2) +  (FTE  | ID), data = SHARE)  or Q1 <- lmer(Patients ~ Time*poly(FTE, 2, raw=TRUE) +  (FTE  | ID), data = SHARE) ? Also, do you really want the linear term in FTE to be random and the quadratic term only fixed?

I hope this helps,
 John

> 
> ggplot(ed2, aes(x=Time, y=fit, color=FTE_sq,group=FTE_sq)) +
>    geom_point() +
>    geom_line(size=1.2) +
>    labs(title = "Time x FTE^2", x= "Time",
>         y="Patients", color="FTE^2", fill="FTE^2") + theme_classic() +
>         theme(text=element_text(size=10))
> 
> Does my problem (obtaining effects for FTE^2*Time and accurately plotting
> them) relate to my use of the effects package, ggplot, both?
> 
> Thank you for the feedback.
> 
> Matthew Boden, Ph.D.
> Senior Evaluator
> Program Evaluation & Resource Center
> Office of Mental Health & Suicide Prevention Veterans Health Administration
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From @ndrew@beet @end|ng |rom no@@@gov  Thu Jan 31 21:46:49 2019
From: @ndrew@beet @end|ng |rom no@@@gov (Andy Beet)
Date: Thu, 31 Jan 2019 15:46:49 -0500
Subject: [R-sig-ME] nlme::gls potential bug
Message-ID: <a8006862-7e90-c5b4-5aa6-21dabfb80cb7@noaa.gov>

Hi there,

I was referred to this group. I hope someone can help out.

I have been using the nlme::gls package in R to fit a pretty
simple model (linear with AR error)

y(t) = beta*x(t) + e(t)? ??? ??? ??? where e(t) ~ rho*e(t-1) + Z(t)
 ???? and Z(t)~ N(0,sig^2)

I call the R routine

glsObj <- nlme::gls(y ~ x -1, data=data, correlation =
nlme::corAR1(form= ~x), method="ML")

All seems fine.


In addition, I have also coded the likelihood myself and maximized it
for beta, rho and sigma.

I get the exact same estimates of beta and rho, (as nlme::gls) but the
estimate of sigma is not the same and i can not figure out why.

The maximum likelihood estimator for sigma under this model is

sig^2 = (( 1-rho^2)u(1)^2 + sum((u(t)- rho*u(t-1))^2)/n

where the sum is t=2,...,n and

u(t) = y(t) - X(t)*beta


I have read the mixed-effects models in S and S-Plus book (nlme::gls
code is based directly on this) and this problem is specified on page
204 eq (5.5). I have also calculated sigma based on (5.7) -after the
transformation documented (5.2) -and i do not get the same value as
either the package or my implementation.

Any advice would be most welcomed. Is there a bug in the estimation of
sigma in this package?

Thanks

Andy

-- 
Andy Beet
Ecosystem Dynamics & Assessment Branch
Northeast Fisheries Science Center
NOAA Fisheries Service
166 Water Street
Woods Hole, MA 02543
tel: 508-495-2073


From m@tthew@t@boden @end|ng |rom gm@||@com  Fri Feb  1 00:12:07 2019
From: m@tthew@t@boden @end|ng |rom gm@||@com (Matthew Boden)
Date: Thu, 31 Jan 2019 15:12:07 -0800
Subject: [R-sig-ME] 
 LMER - Plotting of a quadratic effect interacting with time
In-Reply-To: <ACD1644AA6C67E4FBD0C350625508EC836A7F3AB@FHSDB2D11-2.csu.mcmaster.ca>
References: <5600_1548961407_x0VJ3QQo031244_BN8PR09MB35548B6E8CAC788939EA2E5A87900@BN8PR09MB3554.namprd09.prod.outlook.com>
 <ACD1644AA6C67E4FBD0C350625508EC836A7F3AB@FHSDB2D11-2.csu.mcmaster.ca>
Message-ID: <CAE10PCxAWQa_4MAQ123_X3TBKA6Ync8_fSm4AH0ZW5JrNLCuBw@mail.gmail.com>

Thank you, John.  Worked like a charm.
Also, good catch on exclusion of quadratic FTE as a random effect. Made no
sense.

Matt

On Thu, Jan 31, 2019 at 11:56 AM Fox, John <jfox at mcmaster.ca> wrote:

> Dear Mathew,
>
> > -----Original Message-----
> > From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-
> > project.org] On Behalf Of Boden, Matthew T. via R-sig-mixed-models
> > Sent: Wednesday, January 30, 2019 4:27 PM
> > To: r-sig-mixed-models at r-project.org
> > Subject: [R-sig-ME] LMER - Plotting of a quadratic effect interacting
> with time
> >
> > Hello,
> >
> > I have a question related to fitting and plotting a longitudinal linear
> mixed
> > model that includes an interaction between a quadratic effect and time.
> Data
> > attached.
> >
> > I fit the following:
> >
> > Q1 <- lmer(Patients ~ Time*FTE + Time*I(FTE^2) +  (FTE  | ID), data =
> SHARE)
> >
> > #Yes, the variables are on very different scales - will take care of
> that later
> >
> > I find a sizeable quadratic effect.
> >
> > Fixed effects:
> >                 Estimate Std. Error t value
> > (Intercept)    6.760e+03  5.347e+02  12.642
> > Time           2.033e+01  1.011e+01   2.011
> > FTE            9.728e+01  8.583e+00  11.335
> > I(FTE^2)      -5.155e-01  4.000e-02 -12.890
> > Time:FTE      -5.560e-01  2.254e-01  -2.467
> > Time:I(FTE^2)  7.371e-03  1.052e-03   7.006
> >
> > To plot the quadratic interaction, I attempt to use the effects package.
> > However, effects are displayed for Time x FTE, not time by FTE^2. Time x
> FTE
> > is clearly not the plot that I want (I think...).
> >
> > e1 <- effect(term="Time:I(FTE^2)", mod=Q1)
> > ed1<-as.data.frame(e1)
> > ed1
> >    Time FTE       fit        se     lower     upper
> > 1     1  17  8277.635  464.3995  7366.770  9188.500
> > 2     4  17  8316.650  463.3763  7407.792  9225.508
> > ......
> >
> > I tried a workaround, by fitting a model that included FTE^2 as a second,
> > calculated variable in the data set. Using the effects package, I do
> indeed
> > obtain Time * FTE_sq.
> >
> > Q2 <- lmer(Patients ~ Time*FTE + Time*FTE_sq +  (FTE  | ID), data =
> SHARE)
> >
> > e2 <- effect(term="Time*FTE_sq", mod=Q2)
> > ed2<-as.data.frame(e2)
> > ed2
> >
> >    Time FTE_sq        fit        se      lower      upper
> > 1     1    300 14678.1413  564.5423 13570.8582 15785.4243
> > 2     4    300 14606.9253  563.3827 13501.9166 15711.9340
> > ......
> >
> > But the plot does not at all look like what I would expect. All lines
> > representing FTE_sq over time are straight.
>
> Try, plot(Effect(c("Time", "FTE"), Q1)) .
>
> More generally, why not fit the model as  Q1 <- lmer(Patients ~
> Time*poly(FTE, 2) +  (FTE  | ID), data = SHARE)  or Q1 <- lmer(Patients ~
> Time*poly(FTE, 2, raw=TRUE) +  (FTE  | ID), data = SHARE) ? Also, do you
> really want the linear term in FTE to be random and the quadratic term only
> fixed?
>
> I hope this helps,
>  John
>
> >
> > ggplot(ed2, aes(x=Time, y=fit, color=FTE_sq,group=FTE_sq)) +
> >    geom_point() +
> >    geom_line(size=1.2) +
> >    labs(title = "Time x FTE^2", x= "Time",
> >         y="Patients", color="FTE^2", fill="FTE^2") + theme_classic() +
> >         theme(text=element_text(size=10))
> >
> > Does my problem (obtaining effects for FTE^2*Time and accurately plotting
> > them) relate to my use of the effects package, ggplot, both?
> >
> > Thank you for the feedback.
> >
> > Matthew Boden, Ph.D.
> > Senior Evaluator
> > Program Evaluation & Resource Center
> > Office of Mental Health & Suicide Prevention Veterans Health
> Administration
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From Adr|@@n@de@Jong @end|ng |rom @|u@@e  Fri Feb  1 14:49:05 2019
From: Adr|@@n@de@Jong @end|ng |rom @|u@@e (Adriaan De Jong)
Date: Fri, 1 Feb 2019 13:49:05 +0000
Subject: [R-sig-ME] NaN output from mcse on a glmm model
Message-ID: <69d12654218842ab9e1ac20e07379f83@Exch2-3.slu.se>

Dear list members,
I ran the following glmm on the attached data file, guided by Christina Knudson's "An introduction to Model-Fitting with the R package glmm, 11th of December 2018. (Fresh R download and all packages recently updated. I'm aware of the fact that the cluster part of the script I redundant)
Everything appears to work fine, but when I try to extract the Monte Carlo standard errors with mcse I only receive NaN's for each of the parameters in the model.
The example in Christina Knudson's text uses Bernoulli data, while I use count data (Poisson). Is that the cause of the NaN's?
Grateful for any explanation or suggestion. Thanks in advance.
Have a nice weekend,
Adjan

Adriaan "Adjan" de Jong
Senior researcher
Dept. of Wildlife, Fish, and Environmental Studies
Swedish University of Agricultural Sciences

set.seed(2019)
clust<-makeCluster(2)
mm01<-glmm(Arter ~ Status, random=list(~0+Site), varcomps.names=c("Site"), data=ArterVis, family.glmm=poisson.glmm, m=10000, cluster=clust)
stopCluster(clust)
summary(mm01)
coef(mm01)
confint(mm01)
mcse(mm01)
se(mm01)


---
N?r du skickar e-post till SLU s? inneb?r detta att SLU behandlar dina personuppgifter. F?r att l?sa mer om hur detta g?r till, klicka h?r <https://www.slu.se/om-slu/kontakta-slu/personuppgifter/>
E-mailing SLU will result in SLU processing your personal data. For more information on how this is done, click here <https://www.slu.se/en/about-slu/contact-slu/personal-data/>

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: Arter 2002-2015 Visits Impact Short.txt
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20190201/3c92833f/attachment-0001.txt>

From M@tthew@Boden @end|ng |rom v@@gov  Fri Feb  1 00:05:35 2019
From: M@tthew@Boden @end|ng |rom v@@gov (Boden, Matthew T.)
Date: Thu, 31 Jan 2019 23:05:35 +0000
Subject: [R-sig-ME] 
 LMER - Plotting of a quadratic effect interacting with time
In-Reply-To: <ACD1644AA6C67E4FBD0C350625508EC836A7F3AB@FHSDB2D11-2.csu.mcmaster.ca>
References: <5600_1548961407_x0VJ3QQo031244_BN8PR09MB35548B6E8CAC788939EA2E5A87900@BN8PR09MB3554.namprd09.prod.outlook.com>
 <ACD1644AA6C67E4FBD0C350625508EC836A7F3AB@FHSDB2D11-2.csu.mcmaster.ca>
Message-ID: <BN8PR09MB35544641151425DA1311436187910@BN8PR09MB3554.namprd09.prod.outlook.com>

Worked like a charm!  Thanks, John.  

-----Original Message-----
From: Fox, John [mailto:jfox at mcmaster.ca] 
Sent: Thursday, January 31, 2019 11:57 AM
To: Boden, Matthew T. <Matthew.Boden at va.gov>
Cc: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
Subject: [EXTERNAL] RE: LMER - Plotting of a quadratic effect interacting with time

Dear Mathew,

> -----Original Message-----
> From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-
> project.org] On Behalf Of Boden, Matthew T. via R-sig-mixed-models
> Sent: Wednesday, January 30, 2019 4:27 PM
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] LMER - Plotting of a quadratic effect interacting 
> with time
> 
> Hello,
> 
> I have a question related to fitting and plotting a longitudinal 
> linear mixed model that includes an interaction between a quadratic 
> effect and time. Data attached.
> 
> I fit the following:
> 
> Q1 <- lmer(Patients ~ Time*FTE + Time*I(FTE^2) +  (FTE  | ID), data = 
> SHARE)
> 
> #Yes, the variables are on very different scales - will take care of 
> that later
> 
> I find a sizeable quadratic effect.
> 
> Fixed effects:
>                 Estimate Std. Error t value
> (Intercept)    6.760e+03  5.347e+02  12.642
> Time           2.033e+01  1.011e+01   2.011
> FTE            9.728e+01  8.583e+00  11.335
> I(FTE^2)      -5.155e-01  4.000e-02 -12.890
> Time:FTE      -5.560e-01  2.254e-01  -2.467
> Time:I(FTE^2)  7.371e-03  1.052e-03   7.006
> 
> To plot the quadratic interaction, I attempt to use the effects package.
> However, effects are displayed for Time x FTE, not time by FTE^2. Time 
> x FTE is clearly not the plot that I want (I think...).
> 
> e1 <- effect(term="Time:I(FTE^2)", mod=Q1)
> ed1<-as.data.frame(e1)
> ed1
>    Time FTE       fit        se     lower     upper
> 1     1  17  8277.635  464.3995  7366.770  9188.500
> 2     4  17  8316.650  463.3763  7407.792  9225.508
> ......
> 
> I tried a workaround, by fitting a model that included FTE^2 as a 
> second, calculated variable in the data set. Using the effects 
> package, I do indeed obtain Time * FTE_sq.
> 
> Q2 <- lmer(Patients ~ Time*FTE + Time*FTE_sq +  (FTE  | ID), data = 
> SHARE)
> 
> e2 <- effect(term="Time*FTE_sq", mod=Q2)
> ed2<-as.data.frame(e2)
> ed2
> 
>    Time FTE_sq        fit        se      lower      upper
> 1     1    300 14678.1413  564.5423 13570.8582 15785.4243
> 2     4    300 14606.9253  563.3827 13501.9166 15711.9340
> ......
> 
> But the plot does not at all look like what I would expect. All lines 
> representing FTE_sq over time are straight.

Try, plot(Effect(c("Time", "FTE"), Q1)) .

More generally, why not fit the model as  Q1 <- lmer(Patients ~ Time*poly(FTE, 2) +  (FTE  | ID), data = SHARE)  or Q1 <- lmer(Patients ~ Time*poly(FTE, 2, raw=TRUE) +  (FTE  | ID), data = SHARE) ? Also, do you really want the linear term in FTE to be random and the quadratic term only fixed?

I hope this helps,
 John

> 
> ggplot(ed2, aes(x=Time, y=fit, color=FTE_sq,group=FTE_sq)) +
>    geom_point() +
>    geom_line(size=1.2) +
>    labs(title = "Time x FTE^2", x= "Time",
>         y="Patients", color="FTE^2", fill="FTE^2") + theme_classic() +
>         theme(text=element_text(size=10))
> 
> Does my problem (obtaining effects for FTE^2*Time and accurately 
> plotting
> them) relate to my use of the effects package, ggplot, both?
> 
> Thank you for the feedback.
> 
> Matthew Boden, Ph.D.
> Senior Evaluator
> Program Evaluation & Resource Center
> Office of Mental Health & Suicide Prevention Veterans Health 
> Administration
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From bbo|ker @end|ng |rom gm@||@com  Fri Feb  1 18:26:05 2019
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Fri, 1 Feb 2019 12:26:05 -0500
Subject: [R-sig-ME] NaN output from mcse on a glmm model
In-Reply-To: <69d12654218842ab9e1ac20e07379f83@Exch2-3.slu.se>
References: <69d12654218842ab9e1ac20e07379f83@Exch2-3.slu.se>
Message-ID: <CABghstTEEps3jM_z6CFrz4OQZr1NT+i+ngDd5Z0ZUQABAT4RiQ@mail.gmail.com>

  I can confirm this on my system. I looked and nothing obviously
fishy pops out about the model or the data. Inside the mcvcov()
function, after calling the C-code guts of the computation, stuff[[4]]
(the "numsum" component of the list passed to the C code) is full of
Inf and NaN values.  If I were you I'd (1) try a couple of very simple
examples, one with bernoulli and one with poisson data, to support
your idea that there's something wrong with the Poisson case; (2) look
around for examples of people using the package, e.g.
<https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=knudson+glmm&btnG=>,
and see if there are Poisson examples; (3) contact the maintainer ...

On Fri, Feb 1, 2019 at 8:49 AM Adriaan De Jong <Adriaan.de.Jong at slu.se> wrote:
>
> Dear list members,
> I ran the following glmm on the attached data file, guided by Christina Knudson's "An introduction to Model-Fitting with the R package glmm, 11th of December 2018. (Fresh R download and all packages recently updated. I'm aware of the fact that the cluster part of the script I redundant)
> Everything appears to work fine, but when I try to extract the Monte Carlo standard errors with mcse I only receive NaN's for each of the parameters in the model.
> The example in Christina Knudson's text uses Bernoulli data, while I use count data (Poisson). Is that the cause of the NaN's?
> Grateful for any explanation or suggestion. Thanks in advance.
> Have a nice weekend,
> Adjan
>
> Adriaan "Adjan" de Jong
> Senior researcher
> Dept. of Wildlife, Fish, and Environmental Studies
> Swedish University of Agricultural Sciences
>
> set.seed(2019)
> clust<-makeCluster(2)
> mm01<-glmm(Arter ~ Status, random=list(~0+Site), varcomps.names=c("Site"), data=ArterVis, family.glmm=poisson.glmm, m=10000, cluster=clust)
> stopCluster(clust)
> summary(mm01)
> coef(mm01)
> confint(mm01)
> mcse(mm01)
> se(mm01)
>
>
> ---
> N?r du skickar e-post till SLU s? inneb?r detta att SLU behandlar dina personuppgifter. F?r att l?sa mer om hur detta g?r till, klicka h?r <https://www.slu.se/om-slu/kontakta-slu/personuppgifter/>
> E-mailing SLU will result in SLU processing your personal data. For more information on how this is done, click here <https://www.slu.se/en/about-slu/contact-slu/personal-data/>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From D@v|d@Du||y @end|ng |rom q|mrbergho|er@edu@@u  Sat Feb  2 00:41:41 2019
From: D@v|d@Du||y @end|ng |rom q|mrbergho|er@edu@@u (David Duffy)
Date: Fri, 1 Feb 2019 23:41:41 +0000
Subject: [R-sig-ME] NaN output from mcse on a glmm model
In-Reply-To: <CABghstTEEps3jM_z6CFrz4OQZr1NT+i+ngDd5Z0ZUQABAT4RiQ@mail.gmail.com>
References: <69d12654218842ab9e1ac20e07379f83@Exch2-3.slu.se>,
 <CABghstTEEps3jM_z6CFrz4OQZr1NT+i+ngDd5Z0ZUQABAT4RiQ@mail.gmail.com>
Message-ID: <4737E17E7C8C3C4A8B5C1CE5346371D4A84FA96F@EXCH06S.adqimr.ad.lan>

I ran it as a single instance ie no makeCluster(), with m=100, and it seems OK

Fixed Effects:
                   Estimate Std. Error z value Pr(>|z|)    
(Intercept)         3.48177    0.02022 172.180   <2e-16 ***
StatusConstruction  0.01733    0.02357   0.735   0.4621    
StatusReady         0.04639    0.02258   2.054   0.0399 *  
StatusTrafic        0.02267    0.02192   1.034   0.3010    

Variance Components for Random Effects (P-values are one-tailed):
     Estimate Std. Error z value Pr(>|z|)/2   
Site  0.06095    0.02419    2.52    0.00587 **

With glmer VC is 0.06088, and with another MCMC package, 0.066, SE=0.028

So maybe the clustering code?

Cheers, David Duffy.
________________________________________
From: R-sig-mixed-models [r-sig-mixed-models-bounces at r-project.org] on behalf of Ben Bolker [bbolker at gmail.com]
Sent: Saturday, 2 February 2019 3:26 AM
To: Adriaan De Jong
Cc: R-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] NaN output from mcse on a glmm model

  I can confirm this on my system. I looked and nothing obviously
fishy pops out about the model or the data. Inside the mcvcov()
function, after calling the C-code guts of the computation, stuff[[4]]
(the "numsum" component of the list passed to the C code) is full of
Inf and NaN values.  If I were you I'd (1) try a couple of very simple
examples, one with bernoulli and one with poisson data, to support
your idea that there's something wrong with the Poisson case; (2) look
around for examples of people using the package, e.g.
<https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=knudson+glmm&btnG=>,
and see if there are Poisson examples; (3) contact the maintainer ...

On Fri, Feb 1, 2019 at 8:49 AM Adriaan De Jong <Adriaan.de.Jong at slu.se> wrote:
>
> Dear list members,
> I ran the following glmm on the attached data file, guided by Christina Knudson's "An introduction to Model-Fitting with the R package glmm, 11th of December 2018. (Fresh R download and all packages recently updated. I'm aware of the fact that the cluster part of the script I redundant)
> Everything appears to work fine, but when I try to extract the Monte Carlo standard errors with mcse I only receive NaN's for each of the parameters in the model.
> The example in Christina Knudson's text uses Bernoulli data, while I use count data (Poisson). Is that the cause of the NaN's?
> Grateful for any explanation or suggestion. Thanks in advance.
> Have a nice weekend,
> Adjan
>
> Adriaan "Adjan" de Jong
> Senior researcher
> Dept. of Wildlife, Fish, and Environmental Studies
> Swedish University of Agricultural Sciences
>
> set.seed(2019)
> clust<-makeCluster(2)
> mm01<-glmm(Arter ~ Status, random=list(~0+Site), varcomps.names=c("Site"), data=ArterVis, family.glmm=poisson.glmm, m=10000, cluster=clust)
> stopCluster(clust)
> summary(mm01)
> coef(mm01)
> confint(mm01)
> mcse(mm01)
> se(mm01)
>
>
> ---
> N?r du skickar e-post till SLU s? inneb?r detta att SLU behandlar dina personuppgifter. F?r att l?sa mer om hur detta g?r till, klicka h?r <https://www.slu.se/om-slu/kontakta-slu/personuppgifter/>
> E-mailing SLU will result in SLU processing your personal data. For more information on how this is done, click here <https://www.slu.se/en/about-slu/contact-slu/personal-data/>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From Adr|@@n@de@Jong @end|ng |rom @|u@@e  Sat Feb  2 11:30:39 2019
From: Adr|@@n@de@Jong @end|ng |rom @|u@@e (Adriaan De Jong)
Date: Sat, 2 Feb 2019 10:30:39 +0000
Subject: [R-sig-ME] NaN output from mcse on a glmm model
In-Reply-To: <4737E17E7C8C3C4A8B5C1CE5346371D4A84FA96F@EXCH06S.adqimr.ad.lan>
References: <69d12654218842ab9e1ac20e07379f83@Exch2-3.slu.se>,
 <CABghstTEEps3jM_z6CFrz4OQZr1NT+i+ngDd5Z0ZUQABAT4RiQ@mail.gmail.com>
 <4737E17E7C8C3C4A8B5C1CE5346371D4A84FA96F@EXCH06S.adqimr.ad.lan>
Message-ID: <c11953c9c51a4998866f2d2021389513@Exch2-3.slu.se>

Dear Ben, Dear David,
Thanks for your information and suggestions.
I removed the cluster part of the script, but the mcse function continues to produce NaN's only. I'll continue along the steps suggested by Ben and report.
The mcse evaluation is important because I have no a priori information about the required number of iterations. If I cannot find my way around the mcse function problem, I may just rerun the script a couple of times with increasing m to see how things develop. If the estimates remain stable, I may choose to trust the highest level iterations. The fact that David's output for m=100 is very similar to mine with m=10000 is reassuring.
Cheers,
Adjan

-----Original Message-----
From: David Duffy [mailto:David.Duffy at qimrberghofer.edu.au]
Sent: den 2 februari 2019 00:42
To: Ben Bolker <bbolker at gmail.com>; Adriaan De Jong <Adriaan.de.Jong at slu.se>
Cc: R-sig-mixed-models at r-project.org
Subject: RE: [R-sig-ME] NaN output from mcse on a glmm model

I ran it as a single instance ie no makeCluster(), with m=100, and it seems OK

Fixed Effects:
                   Estimate Std. Error z value Pr(>|z|)
(Intercept)         3.48177    0.02022 172.180   <2e-16 ***
StatusConstruction  0.01733    0.02357   0.735   0.4621
StatusReady         0.04639    0.02258   2.054   0.0399 *
StatusTrafic        0.02267    0.02192   1.034   0.3010

Variance Components for Random Effects (P-values are one-tailed):
     Estimate Std. Error z value Pr(>|z|)/2
Site  0.06095    0.02419    2.52    0.00587 **

With glmer VC is 0.06088, and with another MCMC package, 0.066, SE=0.028

So maybe the clustering code?

Cheers, David Duffy.
________________________________________
From: R-sig-mixed-models [r-sig-mixed-models-bounces at r-project.org] on behalf of Ben Bolker [bbolker at gmail.com]
Sent: Saturday, 2 February 2019 3:26 AM
To: Adriaan De Jong
Cc: R-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] NaN output from mcse on a glmm model

  I can confirm this on my system. I looked and nothing obviously fishy pops out about the model or the data. Inside the mcvcov() function, after calling the C-code guts of the computation, stuff[[4]] (the "numsum" component of the list passed to the C code) is full of Inf and NaN values.  If I were you I'd (1) try a couple of very simple examples, one with bernoulli and one with poisson data, to support your idea that there's something wrong with the Poisson case; (2) look around for examples of people using the package, e.g.
<https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=knudson+glmm&btnG=>,
and see if there are Poisson examples; (3) contact the maintainer ...

On Fri, Feb 1, 2019 at 8:49 AM Adriaan De Jong <Adriaan.de.Jong at slu.se> wrote:
>
> Dear list members,
> I ran the following glmm on the attached data file, guided by
> Christina Knudson's "An introduction to Model-Fitting with the R package glmm, 11th of December 2018. (Fresh R download and all packages recently updated. I'm aware of the fact that the cluster part of the script I redundant) Everything appears to work fine, but when I try to extract the Monte Carlo standard errors with mcse I only receive NaN's for each of the parameters in the model.
> The example in Christina Knudson's text uses Bernoulli data, while I use count data (Poisson). Is that the cause of the NaN's?
> Grateful for any explanation or suggestion. Thanks in advance.
> Have a nice weekend,
> Adjan
>
> Adriaan "Adjan" de Jong
> Senior researcher
> Dept. of Wildlife, Fish, and Environmental Studies Swedish University
> of Agricultural Sciences
>
> set.seed(2019)
> clust<-makeCluster(2)
> mm01<-glmm(Arter ~ Status, random=list(~0+Site),
> varcomps.names=c("Site"), data=ArterVis, family.glmm=poisson.glmm,
> m=10000, cluster=clust)
> stopCluster(clust)
> summary(mm01)
> coef(mm01)
> confint(mm01)
> mcse(mm01)
> se(mm01)
>
>
> ---
> N?r du skickar e-post till SLU s? inneb?r detta att SLU behandlar dina
> personuppgifter. F?r att l?sa mer om hur detta g?r till, klicka h?r
> <https://www.slu.se/om-slu/kontakta-slu/personuppgifter/>
> E-mailing SLU will result in SLU processing your personal data. For
> more information on how this is done, click here
> <https://www.slu.se/en/about-slu/contact-slu/personal-data/>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
---
N?r du skickar e-post till SLU s? inneb?r detta att SLU behandlar dina personuppgifter. F?r att l?sa mer om hur detta g?r till, klicka h?r <https://www.slu.se/om-slu/kontakta-slu/personuppgifter/>
E-mailing SLU will result in SLU processing your personal data. For more information on how this is done, click here <https://www.slu.se/en/about-slu/contact-slu/personal-data/>

From th|erry@onke||nx @end|ng |rom |nbo@be  Mon Feb  4 12:37:46 2019
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Mon, 4 Feb 2019 12:37:46 +0100
Subject: [R-sig-ME] nlme::gls potential bug
In-Reply-To: <a8006862-7e90-c5b4-5aa6-21dabfb80cb7@noaa.gov>
References: <a8006862-7e90-c5b4-5aa6-21dabfb80cb7@noaa.gov>
Message-ID: <CAJuCY5xzxVgtv6K5U7TaP7P9w2LKCCbFTzx50PQ-ALRgDtkp3Q@mail.gmail.com>

Dear Andy,

Can you post a reproducible example?

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op do 31 jan. 2019 om 21:47 schreef Andy Beet via R-sig-mixed-models <
r-sig-mixed-models at r-project.org>:

> Hi there,
>
> I was referred to this group. I hope someone can help out.
>
> I have been using the nlme::gls package in R to fit a pretty
> simple model (linear with AR error)
>
> y(t) = beta*x(t) + e(t)              where e(t) ~ rho*e(t-1) + Z(t)
>       and Z(t)~ N(0,sig^2)
>
> I call the R routine
>
> glsObj <- nlme::gls(y ~ x -1, data=data, correlation =
> nlme::corAR1(form= ~x), method="ML")
>
> All seems fine.
>
>
> In addition, I have also coded the likelihood myself and maximized it
> for beta, rho and sigma.
>
> I get the exact same estimates of beta and rho, (as nlme::gls) but the
> estimate of sigma is not the same and i can not figure out why.
>
> The maximum likelihood estimator for sigma under this model is
>
> sig^2 = (( 1-rho^2)u(1)^2 + sum((u(t)- rho*u(t-1))^2)/n
>
> where the sum is t=2,...,n and
>
> u(t) = y(t) - X(t)*beta
>
>
> I have read the mixed-effects models in S and S-Plus book (nlme::gls
> code is based directly on this) and this problem is specified on page
> 204 eq (5.5). I have also calculated sigma based on (5.7) -after the
> transformation documented (5.2) -and i do not get the same value as
> either the package or my implementation.
>
> Any advice would be most welcomed. Is there a bug in the estimation of
> sigma in this package?
>
> Thanks
>
> Andy
>
> --
> Andy Beet
> Ecosystem Dynamics & Assessment Branch
> Northeast Fisheries Science Center
> NOAA Fisheries Service
> 166 Water Street
> Woods Hole, MA 02543
> tel: 508-495-2073
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Mon Feb  4 14:28:01 2019
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 4 Feb 2019 08:28:01 -0500
Subject: [R-sig-ME] nlme::gls potential bug
In-Reply-To: <a8006862-7e90-c5b4-5aa6-21dabfb80cb7@noaa.gov>
References: <a8006862-7e90-c5b4-5aa6-21dabfb80cb7@noaa.gov>
Message-ID: <CABghstQnL5oFq=e9H2dbuKGAdkGDnk0RVQq79icyftdFyXSohg@mail.gmail.com>

  How different are the answers? Is it possibly an issue of dividing
by n vs. (n-1) ?

On Thu, Jan 31, 2019 at 3:47 PM Andy Beet via R-sig-mixed-models
<r-sig-mixed-models at r-project.org> wrote:
>
> Hi there,
>
> I was referred to this group. I hope someone can help out.
>
> I have been using the nlme::gls package in R to fit a pretty
> simple model (linear with AR error)
>
> y(t) = beta*x(t) + e(t)              where e(t) ~ rho*e(t-1) + Z(t)
>       and Z(t)~ N(0,sig^2)
>
> I call the R routine
>
> glsObj <- nlme::gls(y ~ x -1, data=data, correlation =
> nlme::corAR1(form= ~x), method="ML")
>
> All seems fine.
>
>
> In addition, I have also coded the likelihood myself and maximized it
> for beta, rho and sigma.
>
> I get the exact same estimates of beta and rho, (as nlme::gls) but the
> estimate of sigma is not the same and i can not figure out why.
>
> The maximum likelihood estimator for sigma under this model is
>
> sig^2 = (( 1-rho^2)u(1)^2 + sum((u(t)- rho*u(t-1))^2)/n
>
> where the sum is t=2,...,n and
>
> u(t) = y(t) - X(t)*beta
>
>
> I have read the mixed-effects models in S and S-Plus book (nlme::gls
> code is based directly on this) and this problem is specified on page
> 204 eq (5.5). I have also calculated sigma based on (5.7) -after the
> transformation documented (5.2) -and i do not get the same value as
> either the package or my implementation.
>
> Any advice would be most welcomed. Is there a bug in the estimation of
> sigma in this package?
>
> Thanks
>
> Andy
>
> --
> Andy Beet
> Ecosystem Dynamics & Assessment Branch
> Northeast Fisheries Science Center
> NOAA Fisheries Service
> 166 Water Street
> Woods Hole, MA 02543
> tel: 508-495-2073
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From @v|k @end|ng |rom @@v|on@huj|@@c@||  Thu Feb  7 06:46:40 2019
From: @v|k @end|ng |rom @@v|on@huj|@@c@|| (Avraham Kluger)
Date: Thu, 7 Feb 2019 05:46:40 +0000
Subject: [R-sig-ME] Correlations among random variables
In-Reply-To: <c44cc653c3cc42b4997c8192942242c3@UM-MAIL3214.unimaas.nl>
References: <268119BD809ADB44968F39A7A86398FA017234E825@Pegasus2.hustaff.huji.local>
 <c44cc653c3cc42b4997c8192942242c3@UM-MAIL3214.unimaas.nl>
Message-ID: <268119BD809ADB44968F39A7A86398FA0172371B4C@Pegasus2.hustaff.huji.local>

Dear Wolfgang,

Your CIs seemed to have worked in the past.  I tried to replicated it (attached), but I get 

Warning messages:
1: In confint.rma.mv(res3, rho = 1, verbose = TRUE) :
  Cannot obtain lower bound of profile likelihood CI due to convergence problems.
2: In confint.rma.mv(res3, rho = 1, verbose = TRUE) :
  Cannot obtain upper bound of profile likelihood CI due to convergence problems.

I made sure that I have the most recent version of metafor with
devtools::install_github("wviechtb/metafor", force = TRUE)

to no avail.   I also tried to understand how to use control for confint.

I found out yesterday about your keynote in Dubrovnik on May 31st.  I will see if I can attend.

Best,

Avi




-----Original Message-----
From: Viechtbauer, Wolfgang (SP) [mailto:wolfgang.viechtbauer at maastrichtuniversity.nl] 
Sent: Thursday, January 24, 2019 11:18 PM
To: Avraham Kluger <avik at savion.huji.ac.il>; r-sig-mixed-models at r-project.org
Subject: RE: [R-sig-ME] Correlations among random variables

Happy to hear that.

Yes, you can get profile likelihood CIs with:

ci.rho <- confint(res3, rho=1, verbose=TRUE) ci.rho ci.phi <- confint(res3, phi=1, verbose=TRUE) ci.phi

But this will take quite some time, so you might want to run this when you don't need your computer for a while. Here is what I get:

    estimate  ci.lb  ci.ub 
rho   0.6988 0.3660 1.0000

    estimate  ci.lb  ci.ub 
phi   0.2390 0.1465 0.3274

Best,
Wolfgang

-----Original Message-----
From: Avraham Kluger [mailto:avik at savion.huji.ac.il]
Sent: Thursday, 24 January, 2019 20:24
To: r-sig-mixed-models at r-project.org
Cc: Viechtbauer, Wolfgang (SP)
Subject: Re: [R-sig-ME] Correlations among random variables

Dear Wolfgang,

Your metafor solution beautifully replicates, to the dot, results from SPSS and lavann.? Can you obtain CI around the estimates, rho, and phi?

Avi
--------------------------------------------------

For the data Avi is working with, the default optimizer (nlminb) fails. Switching to 'optim' (with method 'BFGS') works. Here is the fully reproducible code:

df <- read.csv("https://raw.githubusercontent.com/avi-kluger/RCompanion4DDABook/master/Chapter%2010/Chapter10_df.csv")

library(nlme) 

df$focalcode <- 1 - df$focalcode
df$partcode? <- 1 - df$partcode 

### overparamterized model
res1 <- lme(outcome ~ 0 + focalcode + partcode, random = ~ 0 + focalcode + partcode | focalid/dyadid, data = df)
summary(res1)

### contrain sigma to a very small value
res2 <- lme(outcome ~ 0 + focalcode + partcode, random = ~ 0 + focalcode + partcode | focalid/dyadid, data = df, control=list(sigma=1e-8, opt="optim"))
summary(res2)

Just for fun, I also fitted the same model using 'metafor'. While it was not really made for analyzing raw data like this, it can be used to fit the same model (with the devel version) and then sigma can be constrained exactly to 0:

devtools::install_github("wviechtb/metafor")
library(metafor)

df$dyadid.in.focalid <- interaction(df$focalid, df$dyadid)
res3 <- rma.mv(outcome ~ 0 + focalcode + partcode, V=0, random = list(~ 0 + focalcode + partcode | focalid, ~ 0 + focalcode + partcode | dyadid.in.focalid), struct="GEN", data = df, sparse=TRUE)
res3

(note that 'focalid/dyadid' doesn't work at the moment, so you have to create the nested factor manually first; also, model fitting can be slow with rma.mv(), so you might have to wait a bit for it to converge)

The results for res2 and res3 are quite close.

Best,
Wolfgang

From r@turner @end|ng |rom @uck|@nd@@c@nz  Sun Feb 10 10:50:15 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sun, 10 Feb 2019 22:50:15 +1300
Subject: [R-sig-ME] nlmer and the binomial distribution.
Message-ID: <2a3354c5-d1e6-6c63-e5f9-48d1c053b6a6@auckland.ac.nz>


It is not clear to me from the help file whether the nlmer() function 
from the lme4 package can be used to fit non-linear mixed models when 
the response has a discrete distribution, in particular a binomial 
distribution.  I'd like to fit a mixed binomial model in which the 
success probability *cannot* be expressed as "linkinv(linear predictor)" 
where "linkinv()" is the inverse of one of the "standard" link functions
(logit, probit, or cloglog) and the linear predictor is linear in the 
model parameters, but has to be expressed as a more complicated 
non-linear function of the parameters and the predictors.

If it is possible, how should the response appear in the formula? 
Should it be given in the form

   cbind(successes,failures) ~ ... ?

And how should the non-linear function be structured so as to 
accommodate the two-column nature of the response?

I *might* be able to figure all this out by experimenting, but the range 
of possible wrong approaches and wrong garden paths down which to lead 
myself kind of overwhelms me.

So I thought I'd ask here and maybe save myself a bit of time. :-)

cheers,

Rolf Turner

P. S.  It's quite possible that my question makes no real sense at all. 
If so, please feel free to tell me so, but a bit of elaboration as to 
why would be appreciated.

R. T.

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From bbo|ker @end|ng |rom gm@||@com  Sun Feb 10 16:38:10 2019
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sun, 10 Feb 2019 10:38:10 -0500
Subject: [R-sig-ME] nlmer and the binomial distribution.
In-Reply-To: <2a3354c5-d1e6-6c63-e5f9-48d1c053b6a6@auckland.ac.nz>
References: <2a3354c5-d1e6-6c63-e5f9-48d1c053b6a6@auckland.ac.nz>
Message-ID: <fbab4575-e3bc-777b-683d-222064c3603f@gmail.com>


  nlmer does *not* handle non-Gaussian (exponential family) models
(GNLMMs). I don't know of a mainstream, out-of-the-box solution for
frequentist fits of GNLMMs in R.

  * brms can handle nonlinear models with non-Gaussian responses
<https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html>
. It does Bayesian estimation only, but optimization *could* be hacked
if you wanted <https://github.com/paul-buerkner/brms/issues/115>

  * you could try the gnlmm function in Jim Lindsey's repeated package;
you'll have to install it and the rmutils package from source available
at <http://www.commanster.eu/rcode.html>

  * to my knowledge the TMB package would be the most
straightforward/modern way to fit GNLMMs in R, but you would have to
figure out how to write the TMB code.


On 2019-02-10 4:50 a.m., Rolf Turner wrote:
> 
> It is not clear to me from the help file whether the nlmer() function
> from the lme4 package can be used to fit non-linear mixed models when
> the response has a discrete distribution, in particular a binomial
> distribution.? I'd like to fit a mixed binomial model in which the
> success probability *cannot* be expressed as "linkinv(linear predictor)"
> where "linkinv()" is the inverse of one of the "standard" link functions
> (logit, probit, or cloglog) and the linear predictor is linear in the
> model parameters, but has to be expressed as a more complicated
> non-linear function of the parameters and the predictors.
> 
> If it is possible, how should the response appear in the formula? Should
> it be given in the form
> 
> ? cbind(successes,failures) ~ ... ?
> 
> And how should the non-linear function be structured so as to
> accommodate the two-column nature of the response?
> 
> I *might* be able to figure all this out by experimenting, but the range
> of possible wrong approaches and wrong garden paths down which to lead
> myself kind of overwhelms me.
> 
> So I thought I'd ask here and maybe save myself a bit of time. :-)
> 
> cheers,
> 
> Rolf Turner
> 
> P. S.? It's quite possible that my question makes no real sense at all.
> If so, please feel free to tell me so, but a bit of elaboration as to
> why would be appreciated.
> 
> R. T.
>


From ken@knob|@uch @end|ng |rom |n@erm@|r  Sun Feb 10 19:43:59 2019
From: ken@knob|@uch @end|ng |rom |n@erm@|r (Kenneth Knoblauch)
Date: Sun, 10 Feb 2019 19:43:59 +0100
Subject: [R-sig-ME] nlmer and the binomial distribution.
Message-ID: <c34966c13b069fa1cfd6da1a16b22e36@inserm.fr>

Just a point of note, that the repeated package is on CRAN

https://cran.r-project.org/web/packages/repeated/index.html

and the gnm package on CRAN might be able to handle this, too,

https://cran.r-project.org/web/packages/gnm/index.html

Good luck.

> nlmer does *not* handle non-Gaussian (exponential family) models
> (GNLMMs). I don't know of a mainstream, out-of-the-box solution for
> frequentist fits of GNLMMs in R.
> 
> * brms can handle nonlinear models with non-Gaussian responses
> <https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html>
> . It does Bayesian estimation only, but optimization *could* be hacked
> if you wanted <https://github.com/paul-buerkner/brms/issues/115>
> 
> * you could try the gnlmm function in Jim Lindsey's repeated package;
> you'll have to install it and the rmutils package from source 
> available
> at <http://www.commanster.eu/rcode.html>

> * to my knowledge the TMB package would be the most
> straightforward/modern way to fit GNLMMs in R, but you would have to
> figure out how to write the TMB code.
> 
> On 2019-02-10 4:50 a.m., Rolf Turner wrote:
>> 
>> It is not clear to me from the help file whether the nlmer() function
>> from the lme4 package can be used to fit non-linear mixed models when
>> the response has a discrete distribution, in particular a binomial
>> distribution. I'd like to fit a mixed binomial model in which the
>> success probability *cannot* be expressed as "linkinv(linear 
>> predictor)"
>> where "linkinv()" is the inverse of one of the "standard" link 
>> functions
>> (logit, probit, or cloglog) and the linear predictor is linear in the
>> model parameters, but has to be expressed as a more complicated
>> non-linear function of the parameters and the predictors.
>> 
>> If it is possible, how should the response appear in the formula? 
>> Should
>> it be given in the form
>> 
>> cbind(successes,failures) ~ ... ?
>> 
>> And how should the non-linear function be structured so as to
>> accommodate the two-column nature of the response?
>> 
>> I *might* be able to figure all this out by experimenting, but the 
>> range
>> of possible wrong approaches and wrong garden paths down which to 
>> lead
>> myself kind of overwhelms me.
>> 
>> So I thought I'd ask here and maybe save myself a bit of time. :-)
> 
>> cheers,
> v
>> Rolf Turner
>> 
>> P. S. It's quite possible that my question makes no real sense at 
>> all.
>> If so, please feel free to tell me so, but a bit of elaboration as to
>> why would be appreciated.
>> 
>> R. T.
>> 


-- 
Kenneth Knoblauch
Inserm U1208
Stem-cell and Brain Research Institute
18 avenue du Doyen L?pine
69500 Bron
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 91 34 61
portable: +33 (0)6 84 10 64 10
http://www.sbri.fr/user/1453


From bbo|ker @end|ng |rom gm@||@com  Sun Feb 10 20:17:21 2019
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sun, 10 Feb 2019 14:17:21 -0500
Subject: [R-sig-ME] nlmer and the binomial distribution.
In-Reply-To: <c34966c13b069fa1cfd6da1a16b22e36@inserm.fr>
References: <c34966c13b069fa1cfd6da1a16b22e36@inserm.fr>
Message-ID: <0d87ccae-f44b-dbeb-08fa-63eb30a4930f@gmail.com>



On 2019-02-10 1:43 p.m., Kenneth Knoblauch wrote:
> Just a point of note, that the repeated package is on CRAN
> 
> https://cran.r-project.org/web/packages/repeated/index.html

  Cool, thanks!


> 
> and the gnm package on CRAN might be able to handle this, too,
> 
> https://cran.r-project.org/web/packages/gnm/index.html
> 
> Good luck.
> 
>> nlmer does *not* handle non-Gaussian (exponential family) models
>> (GNLMMs). I don't know of a mainstream, out-of-the-box solution for
>> frequentist fits of GNLMMs in R.
>>
>> * brms can handle nonlinear models with non-Gaussian responses
>> <https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html>
>>
>> . It does Bayesian estimation only, but optimization *could* be hacked
>> if you wanted <https://github.com/paul-buerkner/brms/issues/115>
>>
>> * you could try the gnlmm function in Jim Lindsey's repeated package;
>> you'll have to install it and the rmutils package from source available
>> at <http://www.commanster.eu/rcode.html>
> 
>> * to my knowledge the TMB package would be the most
>> straightforward/modern way to fit GNLMMs in R, but you would have to
>> figure out how to write the TMB code.
>>
>> On 2019-02-10 4:50 a.m., Rolf Turner wrote:
>>>
>>> It is not clear to me from the help file whether the nlmer() function
>>> from the lme4 package can be used to fit non-linear mixed models when
>>> the response has a discrete distribution, in particular a binomial
>>> distribution. I'd like to fit a mixed binomial model in which the
>>> success probability *cannot* be expressed as "linkinv(linear predictor)"
>>> where "linkinv()" is the inverse of one of the "standard" link functions
>>> (logit, probit, or cloglog) and the linear predictor is linear in the
>>> model parameters, but has to be expressed as a more complicated
>>> non-linear function of the parameters and the predictors.
>>>
>>> If it is possible, how should the response appear in the formula? Should
>>> it be given in the form
>>>
>>> cbind(successes,failures) ~ ... ?
>>>
>>> And how should the non-linear function be structured so as to
>>> accommodate the two-column nature of the response?
>>>
>>> I *might* be able to figure all this out by experimenting, but the range
>>> of possible wrong approaches and wrong garden paths down which to lead
>>> myself kind of overwhelms me.
>>>
>>> So I thought I'd ask here and maybe save myself a bit of time. :-)
>>
>>> cheers,
>> v
>>> Rolf Turner
>>>
>>> P. S. It's quite possible that my question makes no real sense at all.
>>> If so, please feel free to tell me so, but a bit of elaboration as to
>>> why would be appreciated.
>>>
>>> R. T.
>>>
> 
>


From r@turner @end|ng |rom @uck|@nd@@c@nz  Sun Feb 10 22:07:48 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Mon, 11 Feb 2019 10:07:48 +1300
Subject: [R-sig-ME] [FORGED] Re:  nlmer and the binomial distribution.
In-Reply-To: <0d87ccae-f44b-dbeb-08fa-63eb30a4930f@gmail.com>
References: <c34966c13b069fa1cfd6da1a16b22e36@inserm.fr>
 <0d87ccae-f44b-dbeb-08fa-63eb30a4930f@gmail.com>
Message-ID: <50eea622-2cdd-9b18-2412-a0095a303b73@auckland.ac.nz>


Thanks to Ben Bolker and Kenneth Knoblauch for their replies.  I have 
already investigated the "repeated" package --- and got some mileage out 
of it, thanks to the assistance of the maintainer, Bruce Swihart.

I was not entirely satisfied with the results, since the software in the 
"repeated" package only accommodates a "random intercept", and there is 
evidence from investigating the more standard models that can be fitted 
using glmer() that a random *slope* is needed as well.  Bruce has said 
that he *may* be able to incorporate the "random slope" idea, in a sense 
that I can make precise in the context of the particular non-linear 
model that I have in mind.  However it is slowly dawning on me that 
making this modification is probably an overwhelmingly difficult task.

I guess that I was na?ve in thinking that nlmer might handle it for me.

cheers,

Rolf Turner

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From bbo|ker @end|ng |rom gm@||@com  Mon Feb 11 00:24:47 2019
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sun, 10 Feb 2019 18:24:47 -0500
Subject: [R-sig-ME] [FORGED] Re: nlmer and the binomial distribution.
In-Reply-To: <50eea622-2cdd-9b18-2412-a0095a303b73@auckland.ac.nz>
References: <c34966c13b069fa1cfd6da1a16b22e36@inserm.fr>
 <0d87ccae-f44b-dbeb-08fa-63eb30a4930f@gmail.com>
 <50eea622-2cdd-9b18-2412-a0095a303b73@auckland.ac.nz>
Message-ID: <CABghstTRSah_c52kRv6Gt+X_tw-XUy5mYPwRtB8Unipb__p+xg@mail.gmail.com>

  I guess overwhelmingly difficult is in the eye of the beholder.  Any
chance of a small example of what you want to do? Although TMB is
written in an extended form of C++ rather than R, it might not be too
horrible. A version of the classic orange tree example is here:
https://kaskr.github.io/adcomp/orange_big_8cpp-example.html (it could
use more comments/there are a few slightly weird-looking things ...)

On Sun, Feb 10, 2019 at 4:08 PM Rolf Turner <r.turner at auckland.ac.nz> wrote:
>
>
> Thanks to Ben Bolker and Kenneth Knoblauch for their replies.  I have
> already investigated the "repeated" package --- and got some mileage out
> of it, thanks to the assistance of the maintainer, Bruce Swihart.
>
> I was not entirely satisfied with the results, since the software in the
> "repeated" package only accommodates a "random intercept", and there is
> evidence from investigating the more standard models that can be fitted
> using glmer() that a random *slope* is needed as well.  Bruce has said
> that he *may* be able to incorporate the "random slope" idea, in a sense
> that I can make precise in the context of the particular non-linear
> model that I have in mind.  However it is slowly dawning on me that
> making this modification is probably an overwhelmingly difficult task.
>
> I guess that I was na?ve in thinking that nlmer might handle it for me.
>
> cheers,
>
> Rolf Turner
>
> --
> Honorary Research Fellow
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From r@turner @end|ng |rom @uck|@nd@@c@nz  Mon Feb 11 02:47:04 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Mon, 11 Feb 2019 14:47:04 +1300
Subject: [R-sig-ME] [FORGED] Re: nlmer and the binomial distribution.
In-Reply-To: <CABghstTRSah_c52kRv6Gt+X_tw-XUy5mYPwRtB8Unipb__p+xg@mail.gmail.com>
References: <c34966c13b069fa1cfd6da1a16b22e36@inserm.fr>
 <0d87ccae-f44b-dbeb-08fa-63eb30a4930f@gmail.com>
 <50eea622-2cdd-9b18-2412-a0095a303b73@auckland.ac.nz>
 <CABghstTRSah_c52kRv6Gt+X_tw-XUy5mYPwRtB8Unipb__p+xg@mail.gmail.com>
Message-ID: <7c3dde37-2e16-3bb2-cf7a-3b72735fd3ba@auckland.ac.nz>


On 2/11/19 12:24 PM, Ben Bolker wrote:

>    I guess overwhelmingly difficult is in the eye of the beholder.  Any
> chance of a small example of what you want to do? Although TMB is
> written in an extended form of C++ rather than R, it might not be too
> horrible. A version of the classic orange tree example is here:
> https://kaskr.github.io/adcomp/orange_big_8cpp-example.html (it could
> use more comments/there are a few slightly weird-looking things ...)

Thank you for this kind offer to investigate further.

It just so happens that I have to hand a toy data set that I constructed 
for Bruce Swihart to use in his efforts to help me.  This data set is 
attached as "simdat.txt".  This a result of a dput().  To obtain the 
data do:

     X <- dget("simdat.txt")

I have also attached what I hope is a reasonably lucid explanation of 
the model that I am trying to fit (in "problem.pdf").

Thanks again for taking a look at this.

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: simdat.txt
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20190211/f3fde7c8/attachment-0001.txt>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: problem.pdf
Type: application/pdf
Size: 88646 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20190211/f3fde7c8/attachment-0001.pdf>

From Adr|@@n@de@Jong @end|ng |rom @|u@@e  Mon Feb 11 10:07:18 2019
From: Adr|@@n@de@Jong @end|ng |rom @|u@@e (Adriaan De Jong)
Date: Mon, 11 Feb 2019 09:07:18 +0000
Subject: [R-sig-ME] Highly unbalanced design with count data
Message-ID: <144bfa8a16a74e1382e5e9e0027c3966@Exch2-3.slu.se>

Dear list members,
Birds in agricultural landscapes have been counted annually in a Before-During-After Control-Impact study of possible effects from the construction of a new railway. Obviously, much of the ?design? of this experiment was beyond my control; you cannot direct a billion ? project for the sake of a bird study ? There are six control sites and 13 impact sites of different sizes and shapes (because that?s what the landscape had to offer). All impacted patches of agricultural land were included. Counts were made 2002-2015 with a gap for 2011 and 2012. The construction of the railway became a lengthy process that occurred in various sections along the full 190 km. The sequential steps in the process were: Before, Construction (when the actual building took place), Ready (when the railway was ready but no trains were running) and Traffic (when regular train traffic occurred). Each impact site (patch of agricultural land) was subject for these steps during various years and durations. Also, not all impact sites were surveyed during all the steps. There are issues in the data structure of the count data (some over- and under-dispersion, and high proportion of zero counts), but my main concern is the irregular temporal structure of ?treatments? (variable Status below). The citation marks around the word treatment is because these are not independent treatments but rather repeated measures in a fixed sequence. It would be lovely to ?digest? the full dataset with a Poisson GLMM of some sort, e.g. glmer(response ~ Status + (1|Site), family=poisson), but I have a strong feeling that the requirements of such an analysis are violated by the highly unbalanced design. I?ve also considered a repeated measures ANOVA but I assume the same problem applies there.
I would appreciate any comments or advice that could bring the analysis of this dataset further. Thanks in advance!
Have a nice day!
Adjan

Adriaan ?Adjan? de Jong
Senior researcher
Dept of Wildlife, Fish, and Environmental Studies
Swedish University of Agricultural Sciences

---
N?r du skickar e-post till SLU s? inneb?r detta att SLU behandlar dina personuppgifter. F?r att l?sa mer om hur detta g?r till, klicka h?r <https://www.slu.se/om-slu/kontakta-slu/personuppgifter/>
E-mailing SLU will result in SLU processing your personal data. For more information on how this is done, click here <https://www.slu.se/en/about-slu/contact-slu/personal-data/>

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Mon Feb 11 11:46:25 2019
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Mon, 11 Feb 2019 11:46:25 +0100
Subject: [R-sig-ME] Highly unbalanced design with count data
In-Reply-To: <144bfa8a16a74e1382e5e9e0027c3966@Exch2-3.slu.se>
References: <144bfa8a16a74e1382e5e9e0027c3966@Exch2-3.slu.se>
Message-ID: <CAJuCY5z-zm3Q4JQZan=-iZS+dtpMu9ed6bK0Oe6eDd1wJ+FrEg@mail.gmail.com>

Dear Adriaan,

I'd at least add a year random intercept to take into account generic time
effects. glmer(response ~ Status + (1|Site) + (1|Year), family=poisson) Such
model assumes that a "status" has a constant relative effect which is
identical on all sites. One could argue that the length of a disturbance is
important too. E.g. the bird will return if the disturbance is short but
will stay away when the distrubance is longer. However, I doubt that you
have sufficient data to fit such model.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op ma 11 feb. 2019 om 10:07 schreef Adriaan De Jong <Adriaan.de.Jong at slu.se
>:

> Dear list members,
> Birds in agricultural landscapes have been counted annually in a
> Before-During-After Control-Impact study of possible effects from the
> construction of a new railway. Obviously, much of the ?design? of this
> experiment was beyond my control; you cannot direct a billion ? project for
> the sake of a bird study ? There are six control sites and 13 impact sites
> of different sizes and shapes (because that?s what the landscape had to
> offer). All impacted patches of agricultural land were included. Counts
> were made 2002-2015 with a gap for 2011 and 2012. The construction of the
> railway became a lengthy process that occurred in various sections along
> the full 190 km. The sequential steps in the process were: Before,
> Construction (when the actual building took place), Ready (when the railway
> was ready but no trains were running) and Traffic (when regular train
> traffic occurred). Each impact site (patch of agricultural land) was
> subject for these steps during various years and durations. Also, not all
> impact sites were surveyed during all the steps. There are issues in the
> data structure of the count data (some over- and under-dispersion, and high
> proportion of zero counts), but my main concern is the irregular temporal
> structure of ?treatments? (variable Status below). The citation marks
> around the word treatment is because these are not independent treatments
> but rather repeated measures in a fixed sequence. It would be lovely to
> ?digest? the full dataset with a Poisson GLMM of some sort, e.g.
> glmer(response ~ Status + (1|Site), family=poisson), but I have a strong
> feeling that the requirements of such an analysis are violated by the
> highly unbalanced design. I?ve also considered a repeated measures ANOVA
> but I assume the same problem applies there.
> I would appreciate any comments or advice that could bring the analysis of
> this dataset further. Thanks in advance!
> Have a nice day!
> Adjan
>
> Adriaan ?Adjan? de Jong
> Senior researcher
> Dept of Wildlife, Fish, and Environmental Studies
> Swedish University of Agricultural Sciences
>
> ---
> N?r du skickar e-post till SLU s? inneb?r detta att SLU behandlar dina
> personuppgifter. F?r att l?sa mer om hur detta g?r till, klicka h?r <
> https://www.slu.se/om-slu/kontakta-slu/personuppgifter/>
> E-mailing SLU will result in SLU processing your personal data. For more
> information on how this is done, click here <
> https://www.slu.se/en/about-slu/contact-slu/personal-data/>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n|  Mon Feb 11 15:27:25 2019
From: wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n| (Viechtbauer, Wolfgang (SP))
Date: Mon, 11 Feb 2019 14:27:25 +0000
Subject: [R-sig-ME] Correlations among random variables
In-Reply-To: <268119BD809ADB44968F39A7A86398FA0172371B4C@Pegasus2.hustaff.huji.local>
References: <268119BD809ADB44968F39A7A86398FA017234E825@Pegasus2.hustaff.huji.local>
 <c44cc653c3cc42b4997c8192942242c3@UM-MAIL3214.unimaas.nl>
 <268119BD809ADB44968F39A7A86398FA0172371B4C@Pegasus2.hustaff.huji.local>
Message-ID: <09f2b989b135499598607d74fed0ffb5@UM-MAIL3214.unimaas.nl>

Hi Avi,

Ah, a recent change to metafor broke something. This should be fixed now.

Best,
Wolfgang

-----Original Message-----
From: Avraham Kluger [mailto:avik at savion.huji.ac.il] 
Sent: Thursday, 07 February, 2019 6:47
To: Viechtbauer, Wolfgang (SP); r-sig-mixed-models at r-project.org
Subject: RE: [R-sig-ME] Correlations among random variables

ATTACHMENT(S) REMOVED: One with many -- indistinguishable with MLM.R 

Dear Wolfgang,

Your CIs seemed to have worked in the past.? I tried to replicated it (attached), but I get

Warning messages:
1: In confint.rma.mv(res3, rho = 1, verbose = TRUE) :
? Cannot obtain lower bound of profile likelihood CI due to convergence problems.
2: In confint.rma.mv(res3, rho = 1, verbose = TRUE) :
? Cannot obtain upper bound of profile likelihood CI due to convergence problems.

I made sure that I have the most recent version of metafor with
devtools::install_github("wviechtb/metafor", force = TRUE)

to no avail.?? I also tried to understand how to use control for confint.

I found out yesterday about your keynote in Dubrovnik on May 31st.? I will see if I can attend.

Best,

Avi

-----Original Message-----
From: Viechtbauer, Wolfgang (SP) [mailto:wolfgang.viechtbauer at maastrichtuniversity.nl]
Sent: Thursday, January 24, 2019 11:18 PM
To: Avraham Kluger <avik at savion.huji.ac.il>; r-sig-mixed-models at r-project.org
Subject: RE: [R-sig-ME] Correlations among random variables

Happy to hear that.

Yes, you can get profile likelihood CIs with:

ci.rho <- confint(res3, rho=1, verbose=TRUE) ci.rho ci.phi <- confint(res3, phi=1, verbose=TRUE) ci.phi

But this will take quite some time, so you might want to run this when you don't need your computer for a while. Here is what I get:

??? estimate? ci.lb? ci.ub
rho?? 0.6988 0.3660 1.0000

??? estimate? ci.lb? ci.ub
phi?? 0.2390 0.1465 0.3274

Best,
Wolfgang

-----Original Message-----
From: Avraham Kluger [mailto:avik at savion.huji.ac.il]
Sent: Thursday, 24 January, 2019 20:24
To: r-sig-mixed-models at r-project.org
Cc: Viechtbauer, Wolfgang (SP)
Subject: Re: [R-sig-ME] Correlations among random variables

Dear Wolfgang,

Your metafor solution beautifully replicates, to the dot, results from SPSS and lavann.? Can you obtain CI around the estimates, rho, and phi?

Avi
--------------------------------------------------

For the data Avi is working with, the default optimizer (nlminb) fails. Switching to 'optim' (with method 'BFGS') works. Here is the fully reproducible code:

df <- read.csv("https://raw.githubusercontent.com/avi-kluger/RCompanion4DDABook/master/Chapter%2010/Chapter10_df.csv")

library(nlme)

df$focalcode <- 1 - df$focalcode
df$partcode? <- 1 - df$partcode

### overparamterized model
res1 <- lme(outcome ~ 0 + focalcode + partcode, random = ~ 0 + focalcode + partcode | focalid/dyadid, data = df)
summary(res1)

### contrain sigma to a very small value
res2 <- lme(outcome ~ 0 + focalcode + partcode, random = ~ 0 + focalcode + partcode | focalid/dyadid, data = df, control=list(sigma=1e-8, opt="optim"))
summary(res2)

Just for fun, I also fitted the same model using 'metafor'. While it was not really made for analyzing raw data like this, it can be used to fit the same model (with the devel version) and then sigma can be constrained exactly to 0:

devtools::install_github("wviechtb/metafor")
library(metafor)

df$dyadid.in.focalid <- interaction(df$focalid, df$dyadid)
res3 <- rma.mv(outcome ~ 0 + focalcode + partcode, V=0, random = list(~ 0 + focalcode + partcode | focalid, ~ 0 + focalcode + partcode | dyadid.in.focalid), struct="GEN", data = df, sparse=TRUE)
res3

(note that 'focalid/dyadid' doesn't work at the moment, so you have to create the nested factor manually first; also, model fitting can be slow with rma.mv(), so you might have to wait a bit for it to converge)

The results for res2 and res3 are quite close.

Best,
Wolfgang


From |@b|o@|@non| @end|ng |rom pop@o@|t  Mon Feb 11 09:40:28 2019
From: |@b|o@|@non| @end|ng |rom pop@o@|t (Fabio Fanoni)
Date: Mon, 11 Feb 2019 08:40:28 +0000
Subject: [R-sig-ME] gls() funcion after R upgrade
Message-ID: <d0cd3fe9653e44dcbdb782bb8af4094a@popso.it>

Dear all,

I have been using the package nlme for about a year through the software R

Recently, following an update of R (and the corresponding nlme package), in some situations the gls() function does not work.


The following lines of code

require(nlme)
db<-read.csv2("G:\\Dati.csv",as.is=TRUE)
f<-"DELTA_INVNORM_STD ~ 0+ ITOD + EU_CH"
gls(as.formula(f),correlation=corARMA(p=2,q=3,fixed=FALSE),data=db,weights=varExp(),method="ML")


with the old version of R returns the following output:

Generalized least squares fit by maximum likelihood
  Model: as.formula(f)
  Data: db
  Log-likelihood: -36.13853

Coefficients:
      ITOD      EU_CH
-0.3138869 -0.5010393

Correlation Structure: ARMA(2,3)
Formula: ~1
 Parameter estimate(s):
       Phi1        Phi2      Theta1      Theta2      Theta3
 0.50054107 -0.07357577  0.91847373  0.66706344  0.51689285
Variance function:
Structure: Power of variance covariate
Formula: ~fitted(.)
 Parameter estimates:
      power
-0.01893321
Degrees of freedom: 71 total; 69 residual
Residual standard error: 0.4077908


while with the new one version of R ends with the following error message

Error in `coef<-.corARMA`(`*tmp*`, value = value[parMap[, i]]) :
  NA/NaN/Inf in foreign function call (arg 1)



here are the parameters of the old version of R

> sessionInfo()
R version 3.3.2 (2016-10-31)
Platform: i386-w64-mingw32/i386 (32-bit)
Running under: Windows >= 8 x64 (build 9200)

locale:
[1] LC_COLLATE=Italian_Italy.1252  LC_CTYPE=Italian_Italy.1252    LC_MONETARY=Italian_Italy.1252
[4] LC_NUMERIC=C                   LC_TIME=Italian_Italy.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] nlme_3.1-128

loaded via a namespace (and not attached):
[1] tools_3.3.2     grid_3.3.2      lattice_0.20-34



and the correspondents of the new one

> sessionInfo()
R version 3.5.2 (2018-12-20)
Platform: i386-w64-mingw32/i386 (32-bit)
Running under: Windows >= 8 x64 (build 9200)

Matrix products: default

locale:
[1] LC_COLLATE=Italian_Italy.1252  LC_CTYPE=Italian_Italy.1252    LC_MONETARY=Italian_Italy.1252
[4] LC_NUMERIC=C                   LC_TIME=Italian_Italy.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] nlme_3.1-137

loaded via a namespace (and not attached):
[1] compiler_3.5.2  tools_3.5.2     grid_3.5.2      lattice_0.20-38

The data used are shown below


I wonder if someone could help me to solve this problem.

Best regards,

Fabio Fanoni


DELTA_INVNORM_STD;ITOD;EU_CH
-1,01888891391635;1,11744160507933;0,164921569515933
-1,7287582158078;1,33122211515022;0,192399322297369
-1,92084919508607;1,42620153798496;0,119394703266418
-1,07481132504896;1,40851069525448;-0,21451386962765
0,368325144516397;1,143582080184;-0,541418306765893
1,60232300735139;0,877753890833926;-0,540071359447086
1,79762928248747;0,585114281768911;-0,457099318942592
0,91995670533225;0,300270733467356;-0,292232806294851
-0,0262660321428139;0,20146037523289;-0,236469135573377
-1,02121152912805;0,0419782544708825;-0,345437279939175
-1,10672650470506;0,0574913198102401;-0,366584376685452
-1,39178681241528;0,33582796272111;-0,222730255141816
-1,1454072922613;0,119188584353956;-0,00950828000562807
-0,958672816825354;-0,0667970836192611;0,4570747351185
-1,32272038017045;-0,307712992787705;0,958678420940093
-0,907400017348963;-0,781894397724971;1,3426934842003
-1,24178782808971;-0,502123817789113;1,78543551198139
-0,727279530248652;-0,198729005124588;1,65424271463061
-0,663683090224516;-0,00292614483254617;1,29016249027665
-0,870662613154294;0,284152373447782;0,852269477692287
-0,740956260890321;0,212558271927232;0,359286261984589
-0,875175106507846;0,16921613723138;0,301636857743261
-0,159691021207123;0,344902273564796;0,406160071512065
0,485359221230746;0,405193753056144;0,545299871378597
0,91806657022351;0,51772409209882;0,664639525858513
0,94182403298868;0,624398008842597;0,721750147939642
0,772810225693767;0,669380547864261;0,746938090683181
0,454079892235428;0,829860577818214;0,875841077996076
0,216198540864624;0,85936757871922;1,06778126286636
-0,390013295840405;0,820301653758161;1,32356681853436
-1,08957286663887;0,735286971796754;1,51483352772482
-1,7181964072478;0,54188741408352;1,59807495122772
-2,14142427321141;0,3080114833751;1,30188093962788
-1,73631132303729;0,242276386254216;0,821154955431617
-1,14832298161027;-0,168559611137128;0,390400772122712
-0,199657255063557;-0,673990227176314;-0,425580724204736
0,637711021315926;-1,26311924357155;-0,777807797606142
1,13372564859946;-2,22697608948676;-1,02793616079627
1,3387228312938;-2,67326030450369;-1,25476231880355
1,13410628583832;-2,41425486942835;-0,769187327492251
1,20371911407773;-1,73317141026889;-0,4868668898436
1,04245681382429;-0,627677467262121;-0,517038541303494
1,19122635858486;0,318264079861633;-0,902669938921763
1,10644115835792;0,53399823108225;-1,58826681113873
0,657027862693417;0,566924775110004;-2,16826290930735
0,391546504792817;0,382793958142103;-2,37380727704873
-0,121513593301095;0,113508212335312;-2,28814134594747
0,090587410027589;-0,10456301293743;-1,92150191643467
0,293363258062962;-0,599038773090656;-1,5279235122598
0,490646403474033;-1,16804784867814;-1,10646327070376
0,663013157660536;-1,68022387383074;-0,281457211581005
0,48006697458688;-2,17535440458421;0,0140633243390067
0,33992207153128;-2,15751594621565;0,415319331867849
0,485131796838891;-1,91164287702336;0,74289724468591
0,645057199360022;-1,64072137764755;0,715150100824384
0,830719890823037;-1,20839230550334;0,886616664628225
1,00060688160202;-0,799350150825283;0,78775063121476
0,799516770753564;-0,474799224784852;0,620190215847712
0,884527141039385;-0,111321336688718;0,400772282236831
0,77592825475446;0,41873781137028;0,214623979323192
0,897761905934407;0,618216183184323;-0,371702774880583
1,23087946317063;0,86282014652723;-1,04275259746654
1,1730433788867;1,03592979384644;-1,50933562067236
1,18413315011977;0,843466359597336;-1,89254251877396
0,522306442129412;0,786432900675309;-1,199941514842
-0,423472231576842;0,534354713733984;-0,258289703958632
-1,02160314495135;0,445296924001273;0,367637345059225
-1,2194743130469;0,791437804698057;0,831122380946011
-0,682787491324478;0,735393577499657;0,636488297397258
-0,277058822726022;0,988210808618591;0,362382895620209
-0,0283252883031908;1,18581655608866;0,466771415869385




Fabio Fanoni
SVILUPPO MODELLI DI CREDITO
Piazza Garibaldi 16
23100 Sondrio (SO)
Banca Popolare di Sondrio
Tel. +390342528920
fabio.fanoni at popso.it<mailto:fabio.fanoni at popso.it>


AVVERTENZE LEGALI - I contenuti di questo messaggio, proveniente da un indirizzo di posta elettronica aziendale della Banca Popolare di Sondrio, e gli eventuali allegati possono essere letti e utilizzati, per esigenze lavorative, da chi opera alle dipendenze o per conto della stessa, o ? comunque cointeressato nell'inerente relazione d'affari. Le dichiarazioni, ivi contenute, non impegnano contrattualmente la Banca Popolare di Sondrio se non nei limiti di quanto eventualmente previsto in accordi opportunamente formalizzati. Se il messaggio ? stato ricevuto per errore ce ne scusiamo, pregando di segnalare ci? al mittente e poi di distruggerlo senza farne alcun uso poich? l'utilizzo senza averne diritto ? vietato dalla legge e potrebbe costituire reato.

DATI SOCIETARI - BANCA POPOLARE DI SONDRIO - Societ? cooperativa per azioni - Fondata nel 1871
Sede sociale e direzione generale: I - 23100 SONDRIO SO - piazza Garibaldi, 16
Indirizzo Internet: http:[doppiabarra]www[punto]popso[punto]it - E-mail: info[chiocciola]popso[punto]it
Iscritta al Registro delle Imprese di Sondrio al n. 00053810149, all'Albo delle Banche al n. 842, all'Albo delle Societ? Cooperative al n. A160536 Capogruppo del Gruppo bancario Banca Popolare di Sondrio, iscritto all'Albo dei Gruppi bancari al n. 5696.0 Aderente al Fondo Interbancario di Tutela dei Depositi e al Fondo Nazionale di Garanzia - Codice fiscale e partita IVA: 00053810149
Capitale sociale: EUR 1.360.157.331 - Riserve: EUR 947.325.264 (Dati approvati dall'Assemblea dei soci del 29 aprile 2017).

N.B. I "filtri antivirus" e "antispam" in uso su molti sistemi di posta elettronica possono talvolta ritardare o impedire, in tutto o in parte, il recapito dei messaggi. In tali casi, salvo verifica di avvenuta ricezione, pu? essere necessario modificare i contenuti o le modalit? d'invio.


	[[alternative HTML version deleted]]


From @t@cey-h@nneb@um @end|ng |rom utu|@@@edu  Fri Feb 15 21:25:09 2019
From: @t@cey-h@nneb@um @end|ng |rom utu|@@@edu (Stacey Hannebaum)
Date: Fri, 15 Feb 2019 14:25:09 -0600
Subject: [R-sig-ME] Help with: Multivariate models with binary variables and
 variables with zipoisson distribution
Message-ID: <CAFtRfJXA5jOSX0n343Zs_wcyXnC1Dy3iw06GV513Li7GxUx8nw@mail.gmail.com>

Hi,

I am trying to build a multivariate model to test for associations between
individual variation in behavioral and other traits. Essentially, I want to
derive two calculations from the model: (1) the repeatability of each of
the three traits within individuals and (2) the correlation between each
pair of traits and between each trait and fitness and trappability (the
latter two have one value per individual).
I have been working on this for some time and have reached a standstill.
Here are the main issues I am facing. Any suggestions would be very
appreciated.
(1) How should a binary variable be modelled? For now, I have modelled it
as Gaussian (with the assumption that 0s and 1s are drawn from some
underlying Gaussian process) so that I can calculate correlations but I
don?t know if this is reasonable. Any thoughts?
(2) I tried to run the below model but received the following error:
Error in priorformat(if (NOpriorG) { :
  V is the wrong dimension for some prior$G/prior$R elements
I am sure it has to do with modelling CHARGES as zipoisson - I think I need
to specify the structure for modelling the 0?s but I don?t know where to
start with this.

Below are the variables of interest in case this is useful to know:
logitTAPERETURN: logit-transformed latency to enter a nest once a novel
stimulus was applied / total possible time allowed for trial; trials were
terminated after 5 minutes whether or not bird entered nest; multiple
trials per individual.
CHARGES: number of times an individual attacked the novel stimulus (count
data with many zeros); multiple trials per individual. This could be
expressed as a rate (number of charges / latency to enter nest) but I
decided to model length of trial as a predictor variable using log(latency
to enter nest); trial repeated multiple times per subject
STATUSAFTER: whether the bird stayed in the nest or fled from the nest
(binary)
CAPTURES: number of times an individual was captured (count data with many
ones and no zeros); only one value per individual
RELFITNESS: number of fledglings for individual / mean number of fledglings
across all individuals; only one value per individual

prior_tr_c_sa_nc_f =
list(G=list(G1=list(V=diag(c(1,1,1,0.0001,0.0001),5,5),nu=1.002, fix = 4)),
                                  R=list(V=diag(5),nu=1.002))

I fixed CAPTURES and RELFITNESS because there is not within-individual
variation in these variables.

mcmc_tr_c_sa_nc_f_us <- MCMCglmm(cbind(logitTAPERETURN, CHARGES,
STATUSAFTER, CAPTURES, RELFITNESS) ~
                                            at.level(trait,1):(COLSIZE) +

at.level(trait,1):(TAPETRIALNUM) +

at.level(trait,1):(TAPENESTSTATUS) +
                                            at.level(trait,2):(COLSIZE) +

at.level(trait,2):(stTAPERETURN) +
                                            at.level(trait,2):(stTAPETEMP) +

at.level(trait,3):(stALARMTRIALNUM) +
                                            at.level(trait,3):(stALARMTIME)
+

at.level(trait,3):(stALARMWINDSP) +

at.level(trait,3):(STATUSBEFORE) +
                                            at.level(trait,4):(COLSIZE) +
                                            at.level(trait,5):(COLSIZE) +

at.level(trait,5):(stCLUTCHSIZE) - 1,
                                          random =~ us(trait):BAND,
                                          rcov =~ us(trait):units,
                                          family =
c("gaussian","zipoisson","gaussian","ztpoisson","gaussian"),
                                          prior = prior_tr_c_sa_nc_f,
                                          nitt=420000,
                                          burn=20000,
                                          thin=100,
                                          verbose = TRUE,
                                          data =
as.data.frame(MultivariateAnalysis20172018))


Thank you,

Stacey Hannebaum
Postdoctoral Research Associate

	[[alternative HTML version deleted]]


From jdpo223 @end|ng |rom g@uky@edu  Sat Feb 16 00:46:27 2019
From: jdpo223 @end|ng |rom g@uky@edu (Poe, John)
Date: Fri, 15 Feb 2019 17:46:27 -0600
Subject: [R-sig-ME] Teaching Assistants for advanced multilevel modeling at
 ICPSR
Message-ID: <CAFW8BypZZqASQ7zdBCYFS3zBa04DPWtWebHjUVoF4cg0RXgPbg@mail.gmail.com>

I'm looking for recommendations on potential teaching assistants for my
4-week advanced multilevel modeling course at the ICPSR summer program in
Ann Arbor, Michigan. If you know of advanced graduate students or postdocs
who might be qualified and interested please let me know off the listserv
or have them email me directly. It's a great program and Ann Arbor is an
absolutely lovely place to spend some time in the summer.

The class pulls from econometrics,  biostatistics, and psychometrics and
covers both MLE and Bayesian multilevel modeling. I don't expect anyone to
be comfortable with everything so they'd need to be willing to go through
the material with me in an informal readings bootcamp before the class to
fill in gaps.

You can find last year's syllabus here:
http://www.johndavidpoe.com/teaching/multilevel-modeling-ii-advanced-topics/

	[[alternative HTML version deleted]]


From Ch|@Zh@ng @end|ng |rom UGent@be  Sat Feb 16 14:52:47 2019
From: Ch|@Zh@ng @end|ng |rom UGent@be (Chi Zhang)
Date: Sat, 16 Feb 2019 13:52:47 +0000
Subject: [R-sig-ME] Should I use full models when using Powersim?
In-Reply-To: <mailman.17284.5.1550314802.50351.r-sig-mixed-models@r-project.org>
References: <mailman.17284.5.1550314802.50351.r-sig-mixed-models@r-project.org>
Message-ID: <1550325098543.49347@UGent.be>

Dear all,

I tried using powersim from R package simr to estimate the number of participants that I need for an experiment. I performed the simulation based on the data from my pilot study. The model I used is sketched below:

fit <- glmer(B ~ a+b+a:b
             (1+a+b+a:b|Subject) +
             (1+a+b+a:b|Item),
           family = binomial(link="logit"),
           data = data,
           control = glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=50000),
                                  tol = .0001))

in which Subject and Item mean the distinct id of subjects and items from the pilot study. I want to know test how the power of the interaction term (a:b) changes with the growth of the number of participants. The code I am using is:

fit2<- extend(fit, along="Subject", n = 84)
sim <- powerCurve(fit2, test = fcompare(~a+b), along = "Subject", breaks=c(48,60,72,84), nsim = 5000)
print(sim)

But the results of the simulation was rather bizarre. To begin with, the power of the interaction grew smaller when the number of participants increased from 72 to 84, which I believe is incompatible with the normal observation that the power increases with the number of participants. Second, I tried using the full random model to perform the simulation, but it is really slow (it took me weeks to get just one result). I was wondering if I can use a simpler random model to perform the simulation.

To reiterate my question: first, why my simulated power decreased with the increase of the number of participants? Is there something wrong with my code? Second, can I use a simpler random model for the simulation in order to save time? Thanks in advance!

Best,

Chi Zhang

------------------------
Chi Zhang

PhD Student
Department of Experimental Psychology
Ghent University
Henri Dunantlaan 2, B-9000 Gent, Belgium
Tel: +32 465386530
E-mail: chi.zhang at ugent.be

________________________________________
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on behalf of r-sig-mixed-models-request at r-project.org <r-sig-mixed-models-request at r-project.org>
Sent: Saturday, February 16, 2019 12:00
To: r-sig-mixed-models at r-project.org
Subject: R-sig-mixed-models Digest, Vol 146, Issue 11

Send R-sig-mixed-models mailing list submissions to
        r-sig-mixed-models at r-project.org

To subscribe or unsubscribe via the World Wide Web, visit
        https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
or, via email, send a message with subject or body 'help' to
        r-sig-mixed-models-request at r-project.org

You can reach the person managing the list at
        r-sig-mixed-models-owner at r-project.org

When replying, please edit your Subject line so it is more specific
than "Re: Contents of R-sig-mixed-models digest..."


Today's Topics:

   1. gls() funcion after R upgrade (Fabio Fanoni)
   2. Help with: Multivariate models with binary variables and
      variables with zipoisson distribution (Stacey Hannebaum)
   3. Teaching Assistants for advanced multilevel modeling at ICPSR
      (Poe, John)

----------------------------------------------------------------------

Message: 1
Date: Mon, 11 Feb 2019 08:40:28 +0000
From: Fabio Fanoni <fabio.fanoni at popso.it>
To: "r-sig-mixed-models at r-project.org"
        <r-sig-mixed-models at r-project.org>
Subject: [R-sig-ME] gls() funcion after R upgrade
Message-ID: <d0cd3fe9653e44dcbdb782bb8af4094a at popso.it>
Content-Type: text/plain; charset="utf-8"

Dear all,

I have been using the package nlme for about a year through the software R

Recently, following an update of R (and the corresponding nlme package), in some situations the gls() function does not work.


The following lines of code

require(nlme)
db<-read.csv2("G:\\Dati.csv",as.is=TRUE)
f<-"DELTA_INVNORM_STD ~ 0+ ITOD + EU_CH"
gls(as.formula(f),correlation=corARMA(p=2,q=3,fixed=FALSE),data=db,weights=varExp(),method="ML")


with the old version of R returns the following output:

Generalized least squares fit by maximum likelihood
  Model: as.formula(f)
  Data: db
  Log-likelihood: -36.13853

Coefficients:
      ITOD      EU_CH
-0.3138869 -0.5010393

Correlation Structure: ARMA(2,3)
Formula: ~1
 Parameter estimate(s):
       Phi1        Phi2      Theta1      Theta2      Theta3
 0.50054107 -0.07357577  0.91847373  0.66706344  0.51689285
Variance function:
Structure: Power of variance covariate
Formula: ~fitted(.)
 Parameter estimates:
      power
-0.01893321
Degrees of freedom: 71 total; 69 residual
Residual standard error: 0.4077908


while with the new one version of R ends with the following error message

Error in `coef<-.corARMA`(`*tmp*`, value = value[parMap[, i]]) :
  NA/NaN/Inf in foreign function call (arg 1)



here are the parameters of the old version of R

> sessionInfo()
R version 3.3.2 (2016-10-31)
Platform: i386-w64-mingw32/i386 (32-bit)
Running under: Windows >= 8 x64 (build 9200)

locale:
[1] LC_COLLATE=Italian_Italy.1252  LC_CTYPE=Italian_Italy.1252    LC_MONETARY=Italian_Italy.1252
[4] LC_NUMERIC=C                   LC_TIME=Italian_Italy.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] nlme_3.1-128

loaded via a namespace (and not attached):
[1] tools_3.3.2     grid_3.3.2      lattice_0.20-34



and the correspondents of the new one

> sessionInfo()
R version 3.5.2 (2018-12-20)
Platform: i386-w64-mingw32/i386 (32-bit)
Running under: Windows >= 8 x64 (build 9200)

Matrix products: default

locale:
[1] LC_COLLATE=Italian_Italy.1252  LC_CTYPE=Italian_Italy.1252    LC_MONETARY=Italian_Italy.1252
[4] LC_NUMERIC=C                   LC_TIME=Italian_Italy.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] nlme_3.1-137

loaded via a namespace (and not attached):
[1] compiler_3.5.2  tools_3.5.2     grid_3.5.2      lattice_0.20-38

The data used are shown below


I wonder if someone could help me to solve this problem.

Best regards,

Fabio Fanoni


DELTA_INVNORM_STD;ITOD;EU_CH
-1,01888891391635;1,11744160507933;0,164921569515933
-1,7287582158078;1,33122211515022;0,192399322297369
-1,92084919508607;1,42620153798496;0,119394703266418
-1,07481132504896;1,40851069525448;-0,21451386962765
0,368325144516397;1,143582080184;-0,541418306765893
1,60232300735139;0,877753890833926;-0,540071359447086
1,79762928248747;0,585114281768911;-0,457099318942592
0,91995670533225;0,300270733467356;-0,292232806294851
-0,0262660321428139;0,20146037523289;-0,236469135573377
-1,02121152912805;0,0419782544708825;-0,345437279939175
-1,10672650470506;0,0574913198102401;-0,366584376685452
-1,39178681241528;0,33582796272111;-0,222730255141816
-1,1454072922613;0,119188584353956;-0,00950828000562807
-0,958672816825354;-0,0667970836192611;0,4570747351185
-1,32272038017045;-0,307712992787705;0,958678420940093
-0,907400017348963;-0,781894397724971;1,3426934842003
-1,24178782808971;-0,502123817789113;1,78543551198139
-0,727279530248652;-0,198729005124588;1,65424271463061
-0,663683090224516;-0,00292614483254617;1,29016249027665
-0,870662613154294;0,284152373447782;0,852269477692287
-0,740956260890321;0,212558271927232;0,359286261984589
-0,875175106507846;0,16921613723138;0,301636857743261
-0,159691021207123;0,344902273564796;0,406160071512065
0,485359221230746;0,405193753056144;0,545299871378597
0,91806657022351;0,51772409209882;0,664639525858513
0,94182403298868;0,624398008842597;0,721750147939642
0,772810225693767;0,669380547864261;0,746938090683181
0,454079892235428;0,829860577818214;0,875841077996076
0,216198540864624;0,85936757871922;1,06778126286636
-0,390013295840405;0,820301653758161;1,32356681853436
-1,08957286663887;0,735286971796754;1,51483352772482
-1,7181964072478;0,54188741408352;1,59807495122772
-2,14142427321141;0,3080114833751;1,30188093962788
-1,73631132303729;0,242276386254216;0,821154955431617
-1,14832298161027;-0,168559611137128;0,390400772122712
-0,199657255063557;-0,673990227176314;-0,425580724204736
0,637711021315926;-1,26311924357155;-0,777807797606142
1,13372564859946;-2,22697608948676;-1,02793616079627
1,3387228312938;-2,67326030450369;-1,25476231880355
1,13410628583832;-2,41425486942835;-0,769187327492251
1,20371911407773;-1,73317141026889;-0,4868668898436
1,04245681382429;-0,627677467262121;-0,517038541303494
1,19122635858486;0,318264079861633;-0,902669938921763
1,10644115835792;0,53399823108225;-1,58826681113873
0,657027862693417;0,566924775110004;-2,16826290930735
0,391546504792817;0,382793958142103;-2,37380727704873
-0,121513593301095;0,113508212335312;-2,28814134594747
0,090587410027589;-0,10456301293743;-1,92150191643467
0,293363258062962;-0,599038773090656;-1,5279235122598
0,490646403474033;-1,16804784867814;-1,10646327070376
0,663013157660536;-1,68022387383074;-0,281457211581005
0,48006697458688;-2,17535440458421;0,0140633243390067
0,33992207153128;-2,15751594621565;0,415319331867849
0,485131796838891;-1,91164287702336;0,74289724468591
0,645057199360022;-1,64072137764755;0,715150100824384
0,830719890823037;-1,20839230550334;0,886616664628225
1,00060688160202;-0,799350150825283;0,78775063121476
0,799516770753564;-0,474799224784852;0,620190215847712
0,884527141039385;-0,111321336688718;0,400772282236831
0,77592825475446;0,41873781137028;0,214623979323192
0,897761905934407;0,618216183184323;-0,371702774880583
1,23087946317063;0,86282014652723;-1,04275259746654
1,1730433788867;1,03592979384644;-1,50933562067236
1,18413315011977;0,843466359597336;-1,89254251877396
0,522306442129412;0,786432900675309;-1,199941514842
-0,423472231576842;0,534354713733984;-0,258289703958632
-1,02160314495135;0,445296924001273;0,367637345059225
-1,2194743130469;0,791437804698057;0,831122380946011
-0,682787491324478;0,735393577499657;0,636488297397258
-0,277058822726022;0,988210808618591;0,362382895620209
-0,0283252883031908;1,18581655608866;0,466771415869385




Fabio Fanoni
SVILUPPO MODELLI DI CREDITO
Piazza Garibaldi 16
23100 Sondrio (SO)
Banca Popolare di Sondrio
Tel. +390342528920
fabio.fanoni at popso.it<mailto:fabio.fanoni at popso.it>


AVVERTENZE LEGALI - I contenuti di questo messaggio, proveniente da un indirizzo di posta elettronica aziendale della Banca Popolare di Sondrio, e gli eventuali allegati possono essere letti e utilizzati, per esigenze lavorative, da chi opera alle dipendenze o per conto della stessa, o ? comunque cointeressato nell'inerente relazione d'affari. Le dichiarazioni, ivi contenute, non impegnano contrattualmente la Banca Popolare di Sondrio se non nei limiti di quanto eventualmente previsto in accordi opportunamente formalizzati. Se il messaggio ? stato ricevuto per errore ce ne scusiamo, pregando di segnalare ci? al mittente e poi di distruggerlo senza farne alcun uso poich? l'utilizzo senza averne diritto ? vietato dalla legge e potrebbe costituire reato.

DATI SOCIETARI - BANCA POPOLARE DI SONDRIO - Societ? cooperativa per azioni - Fondata nel 1871
Sede sociale e direzione generale: I - 23100 SONDRIO SO - piazza Garibaldi, 16
Indirizzo Internet: http:[doppiabarra]www[punto]popso[punto]it - E-mail: info[chiocciola]popso[punto]it
Iscritta al Registro delle Imprese di Sondrio al n. 00053810149, all'Albo delle Banche al n. 842, all'Albo delle Societ? Cooperative al n. A160536 Capogruppo del Gruppo bancario Banca Popolare di Sondrio, iscritto all'Albo dei Gruppi bancari al n. 5696.0 Aderente al Fondo Interbancario di Tutela dei Depositi e al Fondo Nazionale di Garanzia - Codice fiscale e partita IVA: 00053810149
Capitale sociale: EUR 1.360.157.331 - Riserve: EUR 947.325.264 (Dati approvati dall'Assemblea dei soci del 29 aprile 2017).

N.B. I "filtri antivirus" e "antispam" in uso su molti sistemi di posta elettronica possono talvolta ritardare o impedire, in tutto o in parte, il recapito dei messaggi. In tali casi, salvo verifica di avvenuta ricezione, pu? essere necessario modificare i contenuti o le modalit? d'invio.


        [[alternative HTML version deleted]]




------------------------------

Message: 2
Date: Fri, 15 Feb 2019 14:25:09 -0600
From: Stacey Hannebaum <stacey-hannebaum at utulsa.edu>
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Help with: Multivariate models with binary
        variables and variables with zipoisson distribution
Message-ID:
        <CAFtRfJXA5jOSX0n343Zs_wcyXnC1Dy3iw06GV513Li7GxUx8nw at mail.gmail.com>
Content-Type: text/plain; charset="utf-8"

Hi,

I am trying to build a multivariate model to test for associations between
individual variation in behavioral and other traits. Essentially, I want to
derive two calculations from the model: (1) the repeatability of each of
the three traits within individuals and (2) the correlation between each
pair of traits and between each trait and fitness and trappability (the
latter two have one value per individual).
I have been working on this for some time and have reached a standstill.
Here are the main issues I am facing. Any suggestions would be very
appreciated.
(1) How should a binary variable be modelled? For now, I have modelled it
as Gaussian (with the assumption that 0s and 1s are drawn from some
underlying Gaussian process) so that I can calculate correlations but I
don?t know if this is reasonable. Any thoughts?
(2) I tried to run the below model but received the following error:
Error in priorformat(if (NOpriorG) { :
  V is the wrong dimension for some prior$G/prior$R elements
I am sure it has to do with modelling CHARGES as zipoisson - I think I need
to specify the structure for modelling the 0?s but I don?t know where to
start with this.

Below are the variables of interest in case this is useful to know:
logitTAPERETURN: logit-transformed latency to enter a nest once a novel
stimulus was applied / total possible time allowed for trial; trials were
terminated after 5 minutes whether or not bird entered nest; multiple
trials per individual.
CHARGES: number of times an individual attacked the novel stimulus (count
data with many zeros); multiple trials per individual. This could be
expressed as a rate (number of charges / latency to enter nest) but I
decided to model length of trial as a predictor variable using log(latency
to enter nest); trial repeated multiple times per subject
STATUSAFTER: whether the bird stayed in the nest or fled from the nest
(binary)
CAPTURES: number of times an individual was captured (count data with many
ones and no zeros); only one value per individual
RELFITNESS: number of fledglings for individual / mean number of fledglings
across all individuals; only one value per individual

prior_tr_c_sa_nc_f =
list(G=list(G1=list(V=diag(c(1,1,1,0.0001,0.0001),5,5),nu=1.002, fix = 4)),
                                  R=list(V=diag(5),nu=1.002))

I fixed CAPTURES and RELFITNESS because there is not within-individual
variation in these variables.

mcmc_tr_c_sa_nc_f_us <- MCMCglmm(cbind(logitTAPERETURN, CHARGES,
STATUSAFTER, CAPTURES, RELFITNESS) ~
                                            at.level(trait,1):(COLSIZE) +

at.level(trait,1):(TAPETRIALNUM) +

at.level(trait,1):(TAPENESTSTATUS) +
                                            at.level(trait,2):(COLSIZE) +

at.level(trait,2):(stTAPERETURN) +
                                            at.level(trait,2):(stTAPETEMP) +

at.level(trait,3):(stALARMTRIALNUM) +
                                            at.level(trait,3):(stALARMTIME)
+

at.level(trait,3):(stALARMWINDSP) +

at.level(trait,3):(STATUSBEFORE) +
                                            at.level(trait,4):(COLSIZE) +
                                            at.level(trait,5):(COLSIZE) +

at.level(trait,5):(stCLUTCHSIZE) - 1,
                                          random =~ us(trait):BAND,
                                          rcov =~ us(trait):units,
                                          family =
c("gaussian","zipoisson","gaussian","ztpoisson","gaussian"),
                                          prior = prior_tr_c_sa_nc_f,
                                          nitt=420000,
                                          burn=20000,
                                          thin=100,
                                          verbose = TRUE,
                                          data =
as.data.frame(MultivariateAnalysis20172018))


Thank you,

Stacey Hannebaum
Postdoctoral Research Associate

        [[alternative HTML version deleted]]




------------------------------

Message: 3
Date: Fri, 15 Feb 2019 17:46:27 -0600
From: "Poe, John" <jdpo223 at g.uky.edu>
To: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
Subject: [R-sig-ME] Teaching Assistants for advanced multilevel
        modeling at ICPSR
Message-ID:
        <CAFW8BypZZqASQ7zdBCYFS3zBa04DPWtWebHjUVoF4cg0RXgPbg at mail.gmail.com>
Content-Type: text/plain; charset="utf-8"

I'm looking for recommendations on potential teaching assistants for my
4-week advanced multilevel modeling course at the ICPSR summer program in
Ann Arbor, Michigan. If you know of advanced graduate students or postdocs
who might be qualified and interested please let me know off the listserv
or have them email me directly. It's a great program and Ann Arbor is an
absolutely lovely place to spend some time in the summer.

The class pulls from econometrics,  biostatistics, and psychometrics and
covers both MLE and Bayesian multilevel modeling. I don't expect anyone to
be comfortable with everything so they'd need to be willing to go through
the material with me in an informal readings bootcamp before the class to
fill in gaps.

You can find last year's syllabus here:
http://www.johndavidpoe.com/teaching/multilevel-modeling-ii-advanced-topics/

        [[alternative HTML version deleted]]




------------------------------

Subject: Digest Footer

_______________________________________________
R-sig-mixed-models mailing list
R-sig-mixed-models at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


------------------------------

End of R-sig-mixed-models Digest, Vol 146, Issue 11
***************************************************


From Ph||||p@A|d@y @end|ng |rom mp|@n|  Mon Feb 18 22:02:31 2019
From: Ph||||p@A|d@y @end|ng |rom mp|@n| (Alday, Phillip)
Date: Mon, 18 Feb 2019 21:02:31 +0000
Subject: [R-sig-ME] Should I use full models when using Powersim?
In-Reply-To: <1550325098543.49347@UGent.be>
References: <mailman.17284.5.1550314802.50351.r-sig-mixed-models@r-project.org>
 <1550325098543.49347@UGent.be>
Message-ID: <4496af8e-5728-459e-c948-83958f6442c4@mpi.nl>

Dear Chi,

Does your maximal model converge for your observed data? Are you able to
detect all assumed "true" effects (i.e. are all of your effects of
interest significant)?

If the answer is no: then your data are too noisy to give you a reliable
estimate of the effect and this can present itself as weird results in
simulation-based power analyses. Basically, if your estimates are noisy,
then your simulation will be noisy and your power estimates won't be
reliable. This also holds for power analysis without simulation, but
won't be as obvious: if you don't have a good estimate of your effect
size, then your power analysis will be misleading.

As far as computational time: use the model in your power analysis that
you want to use in your final analysis. After all, you want to estimate
how much power your final analysis has! There is a large amount of
literature debating tradeoffs in Type I & II error when using different
random-effects structures, and you haven't revealed anything else about
your data and experimental design, so there's little specific advice I
can give. However, in my experience, removing the interaction terms from
the random effects generally speeds up the computation a lot while not
really changing model fit. Since the interaction term is the effect you
care about, I would reparameterize the model so that interaction is a
main effect, e.g.

data$ab <- interaction(data$a, data$b)

fit <- glmer(B ~ a * ab + (1 + a + ab | Subject) ....

If your original model take an hour or more to compute,then it's no
surprise that 5000 simulations * 4 points on the power curve = 20 000
model fits take weeks!

Best,

Phillip


PS: Don't name your dataframe "data"! There is a built-in function with
that name in R and if you're not careful, you'll get all sorts of weird
erros.



On 16/2/19 2:52 pm, Chi Zhang wrote:
> Dear all,
>
> I tried using powersim from R package simr to estimate the number of participants that I need for an experiment. I performed the simulation based on the data from my pilot study. The model I used is sketched below:
>
> fit <- glmer(B ~ a+b+a:b
>              (1+a+b+a:b|Subject) +
>              (1+a+b+a:b|Item),
>            family = binomial(link="logit"),
>            data = data,
>            control = glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=50000),
>                                   tol = .0001))
>
> in which Subject and Item mean the distinct id of subjects and items from the pilot study. I want to know test how the power of the interaction term (a:b) changes with the growth of the number of participants. The code I am using is:
>
> fit2<- extend(fit, along="Subject", n = 84)
> sim <- powerCurve(fit2, test = fcompare(~a+b), along = "Subject", breaks=c(48,60,72,84), nsim = 5000)
> print(sim)
>
> But the results of the simulation was rather bizarre. To begin with, the power of the interaction grew smaller when the number of participants increased from 72 to 84, which I believe is incompatible with the normal observation that the power increases with the number of participants. Second, I tried using the full random model to perform the simulation, but it is really slow (it took me weeks to get just one result). I was wondering if I can use a simpler random model to perform the simulation.
>
> To reiterate my question: first, why my simulated power decreased with the increase of the number of participants? Is there something wrong with my code? Second, can I use a simpler random model for the simulation in order to save time? Thanks in advance!
>
> Best,
>
> Chi Zhang
>
> ------------------------
> Chi Zhang
>
> PhD Student
> Department of Experimental Psychology
> Ghent University
> Henri Dunantlaan 2, B-9000 Gent, Belgium
> Tel: +32 465386530
> E-mail: chi.zhang at ugent.be
>
> ________________________________________
> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on behalf of r-sig-mixed-models-request at r-project.org <r-sig-mixed-models-request at r-project.org>
> Sent: Saturday, February 16, 2019 12:00
> To: r-sig-mixed-models at r-project.org
> Subject: R-sig-mixed-models Digest, Vol 146, Issue 11
>
> Send R-sig-mixed-models mailing list submissions to
>         r-sig-mixed-models at r-project.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> or, via email, send a message with subject or body 'help' to
>         r-sig-mixed-models-request at r-project.org
>
> You can reach the person managing the list at
>         r-sig-mixed-models-owner at r-project.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of R-sig-mixed-models digest..."
>
>
> Today's Topics:
>
>    1. gls() funcion after R upgrade (Fabio Fanoni)
>    2. Help with: Multivariate models with binary variables and
>       variables with zipoisson distribution (Stacey Hannebaum)
>    3. Teaching Assistants for advanced multilevel modeling at ICPSR
>       (Poe, John)
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Mon, 11 Feb 2019 08:40:28 +0000
> From: Fabio Fanoni <fabio.fanoni at popso.it>
> To: "r-sig-mixed-models at r-project.org"
>         <r-sig-mixed-models at r-project.org>
> Subject: [R-sig-ME] gls() funcion after R upgrade
> Message-ID: <d0cd3fe9653e44dcbdb782bb8af4094a at popso.it>
> Content-Type: text/plain; charset="utf-8"
>
> Dear all,
>
> I have been using the package nlme for about a year through the software R
>
> Recently, following an update of R (and the corresponding nlme package), in some situations the gls() function does not work.
>
>
> The following lines of code
>
> require(nlme)
> db<-read.csv2("G:\\Dati.csv",as.is=TRUE)
> f<-"DELTA_INVNORM_STD ~ 0+ ITOD + EU_CH"
> gls(as.formula(f),correlation=corARMA(p=2,q=3,fixed=FALSE),data=db,weights=varExp(),method="ML")
>
>
> with the old version of R returns the following output:
>
> Generalized least squares fit by maximum likelihood
>   Model: as.formula(f)
>   Data: db
>   Log-likelihood: -36.13853
>
> Coefficients:
>       ITOD      EU_CH
> -0.3138869 -0.5010393
>
> Correlation Structure: ARMA(2,3)
> Formula: ~1
>  Parameter estimate(s):
>        Phi1        Phi2      Theta1      Theta2      Theta3
>  0.50054107 -0.07357577  0.91847373  0.66706344  0.51689285
> Variance function:
> Structure: Power of variance covariate
> Formula: ~fitted(.)
>  Parameter estimates:
>       power
> -0.01893321
> Degrees of freedom: 71 total; 69 residual
> Residual standard error: 0.4077908
>
>
> while with the new one version of R ends with the following error message
>
> Error in `coef<-.corARMA`(`*tmp*`, value = value[parMap[, i]]) :
>   NA/NaN/Inf in foreign function call (arg 1)
>
>
>
> here are the parameters of the old version of R
>
>> sessionInfo()
> R version 3.3.2 (2016-10-31)
> Platform: i386-w64-mingw32/i386 (32-bit)
> Running under: Windows >= 8 x64 (build 9200)
>
> locale:
> [1] LC_COLLATE=Italian_Italy.1252  LC_CTYPE=Italian_Italy.1252    LC_MONETARY=Italian_Italy.1252
> [4] LC_NUMERIC=C                   LC_TIME=Italian_Italy.1252
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] nlme_3.1-128
>
> loaded via a namespace (and not attached):
> [1] tools_3.3.2     grid_3.3.2      lattice_0.20-34
>
>
>
> and the correspondents of the new one
>
>> sessionInfo()
> R version 3.5.2 (2018-12-20)
> Platform: i386-w64-mingw32/i386 (32-bit)
> Running under: Windows >= 8 x64 (build 9200)
>
> Matrix products: default
>
> locale:
> [1] LC_COLLATE=Italian_Italy.1252  LC_CTYPE=Italian_Italy.1252    LC_MONETARY=Italian_Italy.1252
> [4] LC_NUMERIC=C                   LC_TIME=Italian_Italy.1252
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] nlme_3.1-137
>
> loaded via a namespace (and not attached):
> [1] compiler_3.5.2  tools_3.5.2     grid_3.5.2      lattice_0.20-38
>
> The data used are shown below
>
>
> I wonder if someone could help me to solve this problem.
>
> Best regards,
>
> Fabio Fanoni
>
>
> DELTA_INVNORM_STD;ITOD;EU_CH
> -1,01888891391635;1,11744160507933;0,164921569515933
> -1,7287582158078;1,33122211515022;0,192399322297369
> -1,92084919508607;1,42620153798496;0,119394703266418
> -1,07481132504896;1,40851069525448;-0,21451386962765
> 0,368325144516397;1,143582080184;-0,541418306765893
> 1,60232300735139;0,877753890833926;-0,540071359447086
> 1,79762928248747;0,585114281768911;-0,457099318942592
> 0,91995670533225;0,300270733467356;-0,292232806294851
> -0,0262660321428139;0,20146037523289;-0,236469135573377
> -1,02121152912805;0,0419782544708825;-0,345437279939175
> -1,10672650470506;0,0574913198102401;-0,366584376685452
> -1,39178681241528;0,33582796272111;-0,222730255141816
> -1,1454072922613;0,119188584353956;-0,00950828000562807
> -0,958672816825354;-0,0667970836192611;0,4570747351185
> -1,32272038017045;-0,307712992787705;0,958678420940093
> -0,907400017348963;-0,781894397724971;1,3426934842003
> -1,24178782808971;-0,502123817789113;1,78543551198139
> -0,727279530248652;-0,198729005124588;1,65424271463061
> -0,663683090224516;-0,00292614483254617;1,29016249027665
> -0,870662613154294;0,284152373447782;0,852269477692287
> -0,740956260890321;0,212558271927232;0,359286261984589
> -0,875175106507846;0,16921613723138;0,301636857743261
> -0,159691021207123;0,344902273564796;0,406160071512065
> 0,485359221230746;0,405193753056144;0,545299871378597
> 0,91806657022351;0,51772409209882;0,664639525858513
> 0,94182403298868;0,624398008842597;0,721750147939642
> 0,772810225693767;0,669380547864261;0,746938090683181
> 0,454079892235428;0,829860577818214;0,875841077996076
> 0,216198540864624;0,85936757871922;1,06778126286636
> -0,390013295840405;0,820301653758161;1,32356681853436
> -1,08957286663887;0,735286971796754;1,51483352772482
> -1,7181964072478;0,54188741408352;1,59807495122772
> -2,14142427321141;0,3080114833751;1,30188093962788
> -1,73631132303729;0,242276386254216;0,821154955431617
> -1,14832298161027;-0,168559611137128;0,390400772122712
> -0,199657255063557;-0,673990227176314;-0,425580724204736
> 0,637711021315926;-1,26311924357155;-0,777807797606142
> 1,13372564859946;-2,22697608948676;-1,02793616079627
> 1,3387228312938;-2,67326030450369;-1,25476231880355
> 1,13410628583832;-2,41425486942835;-0,769187327492251
> 1,20371911407773;-1,73317141026889;-0,4868668898436
> 1,04245681382429;-0,627677467262121;-0,517038541303494
> 1,19122635858486;0,318264079861633;-0,902669938921763
> 1,10644115835792;0,53399823108225;-1,58826681113873
> 0,657027862693417;0,566924775110004;-2,16826290930735
> 0,391546504792817;0,382793958142103;-2,37380727704873
> -0,121513593301095;0,113508212335312;-2,28814134594747
> 0,090587410027589;-0,10456301293743;-1,92150191643467
> 0,293363258062962;-0,599038773090656;-1,5279235122598
> 0,490646403474033;-1,16804784867814;-1,10646327070376
> 0,663013157660536;-1,68022387383074;-0,281457211581005
> 0,48006697458688;-2,17535440458421;0,0140633243390067
> 0,33992207153128;-2,15751594621565;0,415319331867849
> 0,485131796838891;-1,91164287702336;0,74289724468591
> 0,645057199360022;-1,64072137764755;0,715150100824384
> 0,830719890823037;-1,20839230550334;0,886616664628225
> 1,00060688160202;-0,799350150825283;0,78775063121476
> 0,799516770753564;-0,474799224784852;0,620190215847712
> 0,884527141039385;-0,111321336688718;0,400772282236831
> 0,77592825475446;0,41873781137028;0,214623979323192
> 0,897761905934407;0,618216183184323;-0,371702774880583
> 1,23087946317063;0,86282014652723;-1,04275259746654
> 1,1730433788867;1,03592979384644;-1,50933562067236
> 1,18413315011977;0,843466359597336;-1,89254251877396
> 0,522306442129412;0,786432900675309;-1,199941514842
> -0,423472231576842;0,534354713733984;-0,258289703958632
> -1,02160314495135;0,445296924001273;0,367637345059225
> -1,2194743130469;0,791437804698057;0,831122380946011
> -0,682787491324478;0,735393577499657;0,636488297397258
> -0,277058822726022;0,988210808618591;0,362382895620209
> -0,0283252883031908;1,18581655608866;0,466771415869385
>
>
>
>
> Fabio Fanoni
> SVILUPPO MODELLI DI CREDITO
> Piazza Garibaldi 16
> 23100 Sondrio (SO)
> Banca Popolare di Sondrio
> Tel. +390342528920
> fabio.fanoni at popso.it<mailto:fabio.fanoni at popso.it>
>
>
> AVVERTENZE LEGALI - I contenuti di questo messaggio, proveniente da un indirizzo di posta elettronica aziendale della Banca Popolare di Sondrio, e gli eventuali allegati possono essere letti e utilizzati, per esigenze lavorative, da chi opera alle dipendenze o per conto della stessa, o ? comunque cointeressato nell'inerente relazione d'affari. Le dichiarazioni, ivi contenute, non impegnano contrattualmente la Banca Popolare di Sondrio se non nei limiti di quanto eventualmente previsto in accordi opportunamente formalizzati. Se il messaggio ? stato ricevuto per errore ce ne scusiamo, pregando di segnalare ci? al mittente e poi di distruggerlo senza farne alcun uso poich? l'utilizzo senza averne diritto ? vietato dalla legge e potrebbe costituire reato.
>
> DATI SOCIETARI - BANCA POPOLARE DI SONDRIO - Societ? cooperativa per azioni - Fondata nel 1871
> Sede sociale e direzione generale: I - 23100 SONDRIO SO - piazza Garibaldi, 16
> Indirizzo Internet: http:[doppiabarra]www[punto]popso[punto]it - E-mail: info[chiocciola]popso[punto]it
> Iscritta al Registro delle Imprese di Sondrio al n. 00053810149, all'Albo delle Banche al n. 842, all'Albo delle Societ? Cooperative al n. A160536 Capogruppo del Gruppo bancario Banca Popolare di Sondrio, iscritto all'Albo dei Gruppi bancari al n. 5696.0 Aderente al Fondo Interbancario di Tutela dei Depositi e al Fondo Nazionale di Garanzia - Codice fiscale e partita IVA: 00053810149
> Capitale sociale: EUR 1.360.157.331 - Riserve: EUR 947.325.264 (Dati approvati dall'Assemblea dei soci del 29 aprile 2017).
>
> N.B. I "filtri antivirus" e "antispam" in uso su molti sistemi di posta elettronica possono talvolta ritardare o impedire, in tutto o in parte, il recapito dei messaggi. In tali casi, salvo verifica di avvenuta ricezione, pu? essere necessario modificare i contenuti o le modalit? d'invio.
>
>
>         [[alternative HTML version deleted]]
>
>
>
>
> ------------------------------
>
> Message: 2
> Date: Fri, 15 Feb 2019 14:25:09 -0600
> From: Stacey Hannebaum <stacey-hannebaum at utulsa.edu>
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] Help with: Multivariate models with binary
>         variables and variables with zipoisson distribution
> Message-ID:
>         <CAFtRfJXA5jOSX0n343Zs_wcyXnC1Dy3iw06GV513Li7GxUx8nw at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> Hi,
>
> I am trying to build a multivariate model to test for associations between
> individual variation in behavioral and other traits. Essentially, I want to
> derive two calculations from the model: (1) the repeatability of each of
> the three traits within individuals and (2) the correlation between each
> pair of traits and between each trait and fitness and trappability (the
> latter two have one value per individual).
> I have been working on this for some time and have reached a standstill.
> Here are the main issues I am facing. Any suggestions would be very
> appreciated.
> (1) How should a binary variable be modelled? For now, I have modelled it
> as Gaussian (with the assumption that 0s and 1s are drawn from some
> underlying Gaussian process) so that I can calculate correlations but I
> don?t know if this is reasonable. Any thoughts?
> (2) I tried to run the below model but received the following error:
> Error in priorformat(if (NOpriorG) { :
>   V is the wrong dimension for some prior$G/prior$R elements
> I am sure it has to do with modelling CHARGES as zipoisson - I think I need
> to specify the structure for modelling the 0?s but I don?t know where to
> start with this.
>
> Below are the variables of interest in case this is useful to know:
> logitTAPERETURN: logit-transformed latency to enter a nest once a novel
> stimulus was applied / total possible time allowed for trial; trials were
> terminated after 5 minutes whether or not bird entered nest; multiple
> trials per individual.
> CHARGES: number of times an individual attacked the novel stimulus (count
> data with many zeros); multiple trials per individual. This could be
> expressed as a rate (number of charges / latency to enter nest) but I
> decided to model length of trial as a predictor variable using log(latency
> to enter nest); trial repeated multiple times per subject
> STATUSAFTER: whether the bird stayed in the nest or fled from the nest
> (binary)
> CAPTURES: number of times an individual was captured (count data with many
> ones and no zeros); only one value per individual
> RELFITNESS: number of fledglings for individual / mean number of fledglings
> across all individuals; only one value per individual
>
> prior_tr_c_sa_nc_f =
> list(G=list(G1=list(V=diag(c(1,1,1,0.0001,0.0001),5,5),nu=1.002, fix = 4)),
>                                   R=list(V=diag(5),nu=1.002))
>
> I fixed CAPTURES and RELFITNESS because there is not within-individual
> variation in these variables.
>
> mcmc_tr_c_sa_nc_f_us <- MCMCglmm(cbind(logitTAPERETURN, CHARGES,
> STATUSAFTER, CAPTURES, RELFITNESS) ~
>                                             at.level(trait,1):(COLSIZE) +
>
> at.level(trait,1):(TAPETRIALNUM) +
>
> at.level(trait,1):(TAPENESTSTATUS) +
>                                             at.level(trait,2):(COLSIZE) +
>
> at.level(trait,2):(stTAPERETURN) +
>                                             at.level(trait,2):(stTAPETEMP) +
>
> at.level(trait,3):(stALARMTRIALNUM) +
>                                             at.level(trait,3):(stALARMTIME)
> +
>
> at.level(trait,3):(stALARMWINDSP) +
>
> at.level(trait,3):(STATUSBEFORE) +
>                                             at.level(trait,4):(COLSIZE) +
>                                             at.level(trait,5):(COLSIZE) +
>
> at.level(trait,5):(stCLUTCHSIZE) - 1,
>                                           random =~ us(trait):BAND,
>                                           rcov =~ us(trait):units,
>                                           family =
> c("gaussian","zipoisson","gaussian","ztpoisson","gaussian"),
>                                           prior = prior_tr_c_sa_nc_f,
>                                           nitt=420000,
>                                           burn=20000,
>                                           thin=100,
>                                           verbose = TRUE,
>                                           data =
> as.data.frame(MultivariateAnalysis20172018))
>
>
> Thank you,
>
> Stacey Hannebaum
> Postdoctoral Research Associate
>
>         [[alternative HTML version deleted]]
>
>
>
>
> ------------------------------
>
> Message: 3
> Date: Fri, 15 Feb 2019 17:46:27 -0600
> From: "Poe, John" <jdpo223 at g.uky.edu>
> To: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
> Subject: [R-sig-ME] Teaching Assistants for advanced multilevel
>         modeling at ICPSR
> Message-ID:
>         <CAFW8BypZZqASQ7zdBCYFS3zBa04DPWtWebHjUVoF4cg0RXgPbg at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> I'm looking for recommendations on potential teaching assistants for my
> 4-week advanced multilevel modeling course at the ICPSR summer program in
> Ann Arbor, Michigan. If you know of advanced graduate students or postdocs
> who might be qualified and interested please let me know off the listserv
> or have them email me directly. It's a great program and Ann Arbor is an
> absolutely lovely place to spend some time in the summer.
>
> The class pulls from econometrics,  biostatistics, and psychometrics and
> covers both MLE and Bayesian multilevel modeling. I don't expect anyone to
> be comfortable with everything so they'd need to be willing to go through
> the material with me in an informal readings bootcamp before the class to
> fill in gaps.
>
> You can find last year's syllabus here:
> http://www.johndavidpoe.com/teaching/multilevel-modeling-ii-advanced-topics/
>
>         [[alternative HTML version deleted]]
>
>
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> R-sig-mixed-models mailing list
> R-sig-mixed-models at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
> ------------------------------
>
> End of R-sig-mixed-models Digest, Vol 146, Issue 11
> ***************************************************
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From bbo|ker @end|ng |rom gm@||@com  Mon Feb 18 22:19:32 2019
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 18 Feb 2019 16:19:32 -0500
Subject: [R-sig-ME] Should I use full models when using Powersim?
In-Reply-To: <4496af8e-5728-459e-c948-83958f6442c4@mpi.nl>
References: <mailman.17284.5.1550314802.50351.r-sig-mixed-models@r-project.org>
 <1550325098543.49347@UGent.be> <4496af8e-5728-459e-c948-83958f6442c4@mpi.nl>
Message-ID: <09f15a89-1bad-0ccb-39cd-9f4c7e08bf47@gmail.com>


  Agree with everything said here. A few super-low-tech partial
solutions ...

  Others may disagree, but 5000 sims seems pretty extreme for estimating
power (which is after all never better than a crude estimate); if you
reduced that 10-fold you'd probably still get an adequate estimate of power.

  It doesn't look like simr does parallel computation yet - if it did,
and if you have access to a reasonably powerful machine, you could
reduce the computation by another factor of <number of cores>. (You
could do this by brute force by manually running a bunch of jobs with
different random-number seeds, or using the parallel package's
parLapply(), then averaging the power curves you get ...)

On 2019-02-18 4:02 p.m., Alday, Phillip wrote:
> Dear Chi,
> 
> Does your maximal model converge for your observed data? Are you able to
> detect all assumed "true" effects (i.e. are all of your effects of
> interest significant)?
> 
> If the answer is no: then your data are too noisy to give you a reliable
> estimate of the effect and this can present itself as weird results in
> simulation-based power analyses. Basically, if your estimates are noisy,
> then your simulation will be noisy and your power estimates won't be
> reliable. This also holds for power analysis without simulation, but
> won't be as obvious: if you don't have a good estimate of your effect
> size, then your power analysis will be misleading.
> 
> As far as computational time: use the model in your power analysis that
> you want to use in your final analysis. After all, you want to estimate
> how much power your final analysis has! There is a large amount of
> literature debating tradeoffs in Type I & II error when using different
> random-effects structures, and you haven't revealed anything else about
> your data and experimental design, so there's little specific advice I
> can give. However, in my experience, removing the interaction terms from
> the random effects generally speeds up the computation a lot while not
> really changing model fit. Since the interaction term is the effect you
> care about, I would reparameterize the model so that interaction is a
> main effect, e.g.
> 
> data$ab <- interaction(data$a, data$b)
> 
> fit <- glmer(B ~ a * ab + (1 + a + ab | Subject) ....
> 
> If your original model take an hour or more to compute,then it's no
> surprise that 5000 simulations * 4 points on the power curve = 20 000
> model fits take weeks!
> 
> Best,
> 
> Phillip
> 
> 
> PS: Don't name your dataframe "data"! There is a built-in function with
> that name in R and if you're not careful, you'll get all sorts of weird
> erros.
> 
> 
> 
> On 16/2/19 2:52 pm, Chi Zhang wrote:
>> Dear all,
>>
>> I tried using powersim from R package simr to estimate the number of participants that I need for an experiment. I performed the simulation based on the data from my pilot study. The model I used is sketched below:
>>
>> fit <- glmer(B ~ a+b+a:b
>>              (1+a+b+a:b|Subject) +
>>              (1+a+b+a:b|Item),
>>            family = binomial(link="logit"),
>>            data = data,
>>            control = glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=50000),
>>                                   tol = .0001))
>>
>> in which Subject and Item mean the distinct id of subjects and items from the pilot study. I want to know test how the power of the interaction term (a:b) changes with the growth of the number of participants. The code I am using is:
>>
>> fit2<- extend(fit, along="Subject", n = 84)
>> sim <- powerCurve(fit2, test = fcompare(~a+b), along = "Subject", breaks=c(48,60,72,84), nsim = 5000)
>> print(sim)
>>
>> But the results of the simulation was rather bizarre. To begin with, the power of the interaction grew smaller when the number of participants increased from 72 to 84, which I believe is incompatible with the normal observation that the power increases with the number of participants. Second, I tried using the full random model to perform the simulation, but it is really slow (it took me weeks to get just one result). I was wondering if I can use a simpler random model to perform the simulation.
>>
>> To reiterate my question: first, why my simulated power decreased with the increase of the number of participants? Is there something wrong with my code? Second, can I use a simpler random model for the simulation in order to save time? Thanks in advance!
>>
>> Best,
>>
>> Chi Zhang
>>
>> ------------------------
>> Chi Zhang
>>
>> PhD Student
>> Department of Experimental Psychology
>> Ghent University
>> Henri Dunantlaan 2, B-9000 Gent, Belgium
>> Tel: +32 465386530
>> E-mail: chi.zhang at ugent.be
>>
>> ________________________________________
>> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on behalf of r-sig-mixed-models-request at r-project.org <r-sig-mixed-models-request at r-project.org>
>> Sent: Saturday, February 16, 2019 12:00
>> To: r-sig-mixed-models at r-project.org
>> Subject: R-sig-mixed-models Digest, Vol 146, Issue 11
>>
>> Send R-sig-mixed-models mailing list submissions to
>>         r-sig-mixed-models at r-project.org
>>
>> To subscribe or unsubscribe via the World Wide Web, visit
>>         https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> or, via email, send a message with subject or body 'help' to
>>         r-sig-mixed-models-request at r-project.org
>>
>> You can reach the person managing the list at
>>         r-sig-mixed-models-owner at r-project.org
>>
>> When replying, please edit your Subject line so it is more specific
>> than "Re: Contents of R-sig-mixed-models digest..."
>>
>>
>> Today's Topics:
>>
>>    1. gls() funcion after R upgrade (Fabio Fanoni)
>>    2. Help with: Multivariate models with binary variables and
>>       variables with zipoisson distribution (Stacey Hannebaum)
>>    3. Teaching Assistants for advanced multilevel modeling at ICPSR
>>       (Poe, John)
>>
>> ----------------------------------------------------------------------
>>
>> Message: 1
>> Date: Mon, 11 Feb 2019 08:40:28 +0000
>> From: Fabio Fanoni <fabio.fanoni at popso.it>
>> To: "r-sig-mixed-models at r-project.org"
>>         <r-sig-mixed-models at r-project.org>
>> Subject: [R-sig-ME] gls() funcion after R upgrade
>> Message-ID: <d0cd3fe9653e44dcbdb782bb8af4094a at popso.it>
>> Content-Type: text/plain; charset="utf-8"
>>
>> Dear all,
>>
>> I have been using the package nlme for about a year through the software R
>>
>> Recently, following an update of R (and the corresponding nlme package), in some situations the gls() function does not work.
>>
>>
>> The following lines of code
>>
>> require(nlme)
>> db<-read.csv2("G:\\Dati.csv",as.is=TRUE)
>> f<-"DELTA_INVNORM_STD ~ 0+ ITOD + EU_CH"
>> gls(as.formula(f),correlation=corARMA(p=2,q=3,fixed=FALSE),data=db,weights=varExp(),method="ML")
>>
>>
>> with the old version of R returns the following output:
>>
>> Generalized least squares fit by maximum likelihood
>>   Model: as.formula(f)
>>   Data: db
>>   Log-likelihood: -36.13853
>>
>> Coefficients:
>>       ITOD      EU_CH
>> -0.3138869 -0.5010393
>>
>> Correlation Structure: ARMA(2,3)
>> Formula: ~1
>>  Parameter estimate(s):
>>        Phi1        Phi2      Theta1      Theta2      Theta3
>>  0.50054107 -0.07357577  0.91847373  0.66706344  0.51689285
>> Variance function:
>> Structure: Power of variance covariate
>> Formula: ~fitted(.)
>>  Parameter estimates:
>>       power
>> -0.01893321
>> Degrees of freedom: 71 total; 69 residual
>> Residual standard error: 0.4077908
>>
>>
>> while with the new one version of R ends with the following error message
>>
>> Error in `coef<-.corARMA`(`*tmp*`, value = value[parMap[, i]]) :
>>   NA/NaN/Inf in foreign function call (arg 1)
>>
>>
>>
>> here are the parameters of the old version of R
>>
>>> sessionInfo()
>> R version 3.3.2 (2016-10-31)
>> Platform: i386-w64-mingw32/i386 (32-bit)
>> Running under: Windows >= 8 x64 (build 9200)
>>
>> locale:
>> [1] LC_COLLATE=Italian_Italy.1252  LC_CTYPE=Italian_Italy.1252    LC_MONETARY=Italian_Italy.1252
>> [4] LC_NUMERIC=C                   LC_TIME=Italian_Italy.1252
>>
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>> other attached packages:
>> [1] nlme_3.1-128
>>
>> loaded via a namespace (and not attached):
>> [1] tools_3.3.2     grid_3.3.2      lattice_0.20-34
>>
>>
>>
>> and the correspondents of the new one
>>
>>> sessionInfo()
>> R version 3.5.2 (2018-12-20)
>> Platform: i386-w64-mingw32/i386 (32-bit)
>> Running under: Windows >= 8 x64 (build 9200)
>>
>> Matrix products: default
>>
>> locale:
>> [1] LC_COLLATE=Italian_Italy.1252  LC_CTYPE=Italian_Italy.1252    LC_MONETARY=Italian_Italy.1252
>> [4] LC_NUMERIC=C                   LC_TIME=Italian_Italy.1252
>>
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>> other attached packages:
>> [1] nlme_3.1-137
>>
>> loaded via a namespace (and not attached):
>> [1] compiler_3.5.2  tools_3.5.2     grid_3.5.2      lattice_0.20-38
>>
>> The data used are shown below
>>
>>
>> I wonder if someone could help me to solve this problem.
>>
>> Best regards,
>>
>> Fabio Fanoni
>>
>>
>> DELTA_INVNORM_STD;ITOD;EU_CH
>> -1,01888891391635;1,11744160507933;0,164921569515933
>> -1,7287582158078;1,33122211515022;0,192399322297369
>> -1,92084919508607;1,42620153798496;0,119394703266418
>> -1,07481132504896;1,40851069525448;-0,21451386962765
>> 0,368325144516397;1,143582080184;-0,541418306765893
>> 1,60232300735139;0,877753890833926;-0,540071359447086
>> 1,79762928248747;0,585114281768911;-0,457099318942592
>> 0,91995670533225;0,300270733467356;-0,292232806294851
>> -0,0262660321428139;0,20146037523289;-0,236469135573377
>> -1,02121152912805;0,0419782544708825;-0,345437279939175
>> -1,10672650470506;0,0574913198102401;-0,366584376685452
>> -1,39178681241528;0,33582796272111;-0,222730255141816
>> -1,1454072922613;0,119188584353956;-0,00950828000562807
>> -0,958672816825354;-0,0667970836192611;0,4570747351185
>> -1,32272038017045;-0,307712992787705;0,958678420940093
>> -0,907400017348963;-0,781894397724971;1,3426934842003
>> -1,24178782808971;-0,502123817789113;1,78543551198139
>> -0,727279530248652;-0,198729005124588;1,65424271463061
>> -0,663683090224516;-0,00292614483254617;1,29016249027665
>> -0,870662613154294;0,284152373447782;0,852269477692287
>> -0,740956260890321;0,212558271927232;0,359286261984589
>> -0,875175106507846;0,16921613723138;0,301636857743261
>> -0,159691021207123;0,344902273564796;0,406160071512065
>> 0,485359221230746;0,405193753056144;0,545299871378597
>> 0,91806657022351;0,51772409209882;0,664639525858513
>> 0,94182403298868;0,624398008842597;0,721750147939642
>> 0,772810225693767;0,669380547864261;0,746938090683181
>> 0,454079892235428;0,829860577818214;0,875841077996076
>> 0,216198540864624;0,85936757871922;1,06778126286636
>> -0,390013295840405;0,820301653758161;1,32356681853436
>> -1,08957286663887;0,735286971796754;1,51483352772482
>> -1,7181964072478;0,54188741408352;1,59807495122772
>> -2,14142427321141;0,3080114833751;1,30188093962788
>> -1,73631132303729;0,242276386254216;0,821154955431617
>> -1,14832298161027;-0,168559611137128;0,390400772122712
>> -0,199657255063557;-0,673990227176314;-0,425580724204736
>> 0,637711021315926;-1,26311924357155;-0,777807797606142
>> 1,13372564859946;-2,22697608948676;-1,02793616079627
>> 1,3387228312938;-2,67326030450369;-1,25476231880355
>> 1,13410628583832;-2,41425486942835;-0,769187327492251
>> 1,20371911407773;-1,73317141026889;-0,4868668898436
>> 1,04245681382429;-0,627677467262121;-0,517038541303494
>> 1,19122635858486;0,318264079861633;-0,902669938921763
>> 1,10644115835792;0,53399823108225;-1,58826681113873
>> 0,657027862693417;0,566924775110004;-2,16826290930735
>> 0,391546504792817;0,382793958142103;-2,37380727704873
>> -0,121513593301095;0,113508212335312;-2,28814134594747
>> 0,090587410027589;-0,10456301293743;-1,92150191643467
>> 0,293363258062962;-0,599038773090656;-1,5279235122598
>> 0,490646403474033;-1,16804784867814;-1,10646327070376
>> 0,663013157660536;-1,68022387383074;-0,281457211581005
>> 0,48006697458688;-2,17535440458421;0,0140633243390067
>> 0,33992207153128;-2,15751594621565;0,415319331867849
>> 0,485131796838891;-1,91164287702336;0,74289724468591
>> 0,645057199360022;-1,64072137764755;0,715150100824384
>> 0,830719890823037;-1,20839230550334;0,886616664628225
>> 1,00060688160202;-0,799350150825283;0,78775063121476
>> 0,799516770753564;-0,474799224784852;0,620190215847712
>> 0,884527141039385;-0,111321336688718;0,400772282236831
>> 0,77592825475446;0,41873781137028;0,214623979323192
>> 0,897761905934407;0,618216183184323;-0,371702774880583
>> 1,23087946317063;0,86282014652723;-1,04275259746654
>> 1,1730433788867;1,03592979384644;-1,50933562067236
>> 1,18413315011977;0,843466359597336;-1,89254251877396
>> 0,522306442129412;0,786432900675309;-1,199941514842
>> -0,423472231576842;0,534354713733984;-0,258289703958632
>> -1,02160314495135;0,445296924001273;0,367637345059225
>> -1,2194743130469;0,791437804698057;0,831122380946011
>> -0,682787491324478;0,735393577499657;0,636488297397258
>> -0,277058822726022;0,988210808618591;0,362382895620209
>> -0,0283252883031908;1,18581655608866;0,466771415869385
>>
>>
>>
>>
>> Fabio Fanoni
>> SVILUPPO MODELLI DI CREDITO
>> Piazza Garibaldi 16
>> 23100 Sondrio (SO)
>> Banca Popolare di Sondrio
>> Tel. +390342528920
>> fabio.fanoni at popso.it<mailto:fabio.fanoni at popso.it>
>>
>>
>> AVVERTENZE LEGALI - I contenuti di questo messaggio, proveniente da un indirizzo di posta elettronica aziendale della Banca Popolare di Sondrio, e gli eventuali allegati possono essere letti e utilizzati, per esigenze lavorative, da chi opera alle dipendenze o per conto della stessa, o ? comunque cointeressato nell'inerente relazione d'affari. Le dichiarazioni, ivi contenute, non impegnano contrattualmente la Banca Popolare di Sondrio se non nei limiti di quanto eventualmente previsto in accordi opportunamente formalizzati. Se il messaggio ? stato ricevuto per errore ce ne scusiamo, pregando di segnalare ci? al mittente e poi di distruggerlo senza farne alcun uso poich? l'utilizzo senza averne diritto ? vietato dalla legge e potrebbe costituire reato.
>>
>> DATI SOCIETARI - BANCA POPOLARE DI SONDRIO - Societ? cooperativa per azioni - Fondata nel 1871
>> Sede sociale e direzione generale: I - 23100 SONDRIO SO - piazza Garibaldi, 16
>> Indirizzo Internet: http:[doppiabarra]www[punto]popso[punto]it - E-mail: info[chiocciola]popso[punto]it
>> Iscritta al Registro delle Imprese di Sondrio al n. 00053810149, all'Albo delle Banche al n. 842, all'Albo delle Societ? Cooperative al n. A160536 Capogruppo del Gruppo bancario Banca Popolare di Sondrio, iscritto all'Albo dei Gruppi bancari al n. 5696.0 Aderente al Fondo Interbancario di Tutela dei Depositi e al Fondo Nazionale di Garanzia - Codice fiscale e partita IVA: 00053810149
>> Capitale sociale: EUR 1.360.157.331 - Riserve: EUR 947.325.264 (Dati approvati dall'Assemblea dei soci del 29 aprile 2017).
>>
>> N.B. I "filtri antivirus" e "antispam" in uso su molti sistemi di posta elettronica possono talvolta ritardare o impedire, in tutto o in parte, il recapito dei messaggi. In tali casi, salvo verifica di avvenuta ricezione, pu? essere necessario modificare i contenuti o le modalit? d'invio.
>>
>>
>>         [[alternative HTML version deleted]]
>>
>>
>>
>>
>> ------------------------------
>>
>> Message: 2
>> Date: Fri, 15 Feb 2019 14:25:09 -0600
>> From: Stacey Hannebaum <stacey-hannebaum at utulsa.edu>
>> To: r-sig-mixed-models at r-project.org
>> Subject: [R-sig-ME] Help with: Multivariate models with binary
>>         variables and variables with zipoisson distribution
>> Message-ID:
>>         <CAFtRfJXA5jOSX0n343Zs_wcyXnC1Dy3iw06GV513Li7GxUx8nw at mail.gmail.com>
>> Content-Type: text/plain; charset="utf-8"
>>
>> Hi,
>>
>> I am trying to build a multivariate model to test for associations between
>> individual variation in behavioral and other traits. Essentially, I want to
>> derive two calculations from the model: (1) the repeatability of each of
>> the three traits within individuals and (2) the correlation between each
>> pair of traits and between each trait and fitness and trappability (the
>> latter two have one value per individual).
>> I have been working on this for some time and have reached a standstill.
>> Here are the main issues I am facing. Any suggestions would be very
>> appreciated.
>> (1) How should a binary variable be modelled? For now, I have modelled it
>> as Gaussian (with the assumption that 0s and 1s are drawn from some
>> underlying Gaussian process) so that I can calculate correlations but I
>> don?t know if this is reasonable. Any thoughts?
>> (2) I tried to run the below model but received the following error:
>> Error in priorformat(if (NOpriorG) { :
>>   V is the wrong dimension for some prior$G/prior$R elements
>> I am sure it has to do with modelling CHARGES as zipoisson - I think I need
>> to specify the structure for modelling the 0?s but I don?t know where to
>> start with this.
>>
>> Below are the variables of interest in case this is useful to know:
>> logitTAPERETURN: logit-transformed latency to enter a nest once a novel
>> stimulus was applied / total possible time allowed for trial; trials were
>> terminated after 5 minutes whether or not bird entered nest; multiple
>> trials per individual.
>> CHARGES: number of times an individual attacked the novel stimulus (count
>> data with many zeros); multiple trials per individual. This could be
>> expressed as a rate (number of charges / latency to enter nest) but I
>> decided to model length of trial as a predictor variable using log(latency
>> to enter nest); trial repeated multiple times per subject
>> STATUSAFTER: whether the bird stayed in the nest or fled from the nest
>> (binary)
>> CAPTURES: number of times an individual was captured (count data with many
>> ones and no zeros); only one value per individual
>> RELFITNESS: number of fledglings for individual / mean number of fledglings
>> across all individuals; only one value per individual
>>
>> prior_tr_c_sa_nc_f =
>> list(G=list(G1=list(V=diag(c(1,1,1,0.0001,0.0001),5,5),nu=1.002, fix = 4)),
>>                                   R=list(V=diag(5),nu=1.002))
>>
>> I fixed CAPTURES and RELFITNESS because there is not within-individual
>> variation in these variables.
>>
>> mcmc_tr_c_sa_nc_f_us <- MCMCglmm(cbind(logitTAPERETURN, CHARGES,
>> STATUSAFTER, CAPTURES, RELFITNESS) ~
>>                                             at.level(trait,1):(COLSIZE) +
>>
>> at.level(trait,1):(TAPETRIALNUM) +
>>
>> at.level(trait,1):(TAPENESTSTATUS) +
>>                                             at.level(trait,2):(COLSIZE) +
>>
>> at.level(trait,2):(stTAPERETURN) +
>>                                             at.level(trait,2):(stTAPETEMP) +
>>
>> at.level(trait,3):(stALARMTRIALNUM) +
>>                                             at.level(trait,3):(stALARMTIME)
>> +
>>
>> at.level(trait,3):(stALARMWINDSP) +
>>
>> at.level(trait,3):(STATUSBEFORE) +
>>                                             at.level(trait,4):(COLSIZE) +
>>                                             at.level(trait,5):(COLSIZE) +
>>
>> at.level(trait,5):(stCLUTCHSIZE) - 1,
>>                                           random =~ us(trait):BAND,
>>                                           rcov =~ us(trait):units,
>>                                           family =
>> c("gaussian","zipoisson","gaussian","ztpoisson","gaussian"),
>>                                           prior = prior_tr_c_sa_nc_f,
>>                                           nitt=420000,
>>                                           burn=20000,
>>                                           thin=100,
>>                                           verbose = TRUE,
>>                                           data =
>> as.data.frame(MultivariateAnalysis20172018))
>>
>>
>> Thank you,
>>
>> Stacey Hannebaum
>> Postdoctoral Research Associate
>>
>>         [[alternative HTML version deleted]]
>>
>>
>>
>>
>> ------------------------------
>>
>> Message: 3
>> Date: Fri, 15 Feb 2019 17:46:27 -0600
>> From: "Poe, John" <jdpo223 at g.uky.edu>
>> To: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
>> Subject: [R-sig-ME] Teaching Assistants for advanced multilevel
>>         modeling at ICPSR
>> Message-ID:
>>         <CAFW8BypZZqASQ7zdBCYFS3zBa04DPWtWebHjUVoF4cg0RXgPbg at mail.gmail.com>
>> Content-Type: text/plain; charset="utf-8"
>>
>> I'm looking for recommendations on potential teaching assistants for my
>> 4-week advanced multilevel modeling course at the ICPSR summer program in
>> Ann Arbor, Michigan. If you know of advanced graduate students or postdocs
>> who might be qualified and interested please let me know off the listserv
>> or have them email me directly. It's a great program and Ann Arbor is an
>> absolutely lovely place to spend some time in the summer.
>>
>> The class pulls from econometrics,  biostatistics, and psychometrics and
>> covers both MLE and Bayesian multilevel modeling. I don't expect anyone to
>> be comfortable with everything so they'd need to be willing to go through
>> the material with me in an informal readings bootcamp before the class to
>> fill in gaps.
>>
>> You can find last year's syllabus here:
>> http://www.johndavidpoe.com/teaching/multilevel-modeling-ii-advanced-topics/
>>
>>         [[alternative HTML version deleted]]
>>
>>
>>
>>
>> ------------------------------
>>
>> Subject: Digest Footer
>>
>> _______________________________________________
>> R-sig-mixed-models mailing list
>> R-sig-mixed-models at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>> ------------------------------
>>
>> End of R-sig-mixed-models Digest, Vol 146, Issue 11
>> ***************************************************
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From Ph||||p@A|d@y @end|ng |rom mp|@n|  Mon Feb 18 22:32:12 2019
From: Ph||||p@A|d@y @end|ng |rom mp|@n| (Alday, Phillip)
Date: Mon, 18 Feb 2019 21:32:12 +0000
Subject: [R-sig-ME] Should I use full models when using Powersim?
In-Reply-To: <09f15a89-1bad-0ccb-39cd-9f4c7e08bf47@gmail.com>
References: <mailman.17284.5.1550314802.50351.r-sig-mixed-models@r-project.org>
 <1550325098543.49347@UGent.be> <4496af8e-5728-459e-c948-83958f6442c4@mpi.nl>
 <09f15a89-1bad-0ccb-39cd-9f4c7e08bf47@gmail.com>
Message-ID: <0c3ff954-975e-2696-b28e-e2ec02b63699@mpi.nl>

On 18/2/19 10:19 pm, Ben Bolker wrote:
>   Agree with everything said here. A few super-low-tech partial
> solutions ...
>
>   Others may disagree, but 5000 sims seems pretty extreme for estimating
> power (which is after all never better than a crude estimate); if you
> reduced that 10-fold you'd probably still get an adequate estimate of power.
>
>   It doesn't look like simr does parallel computation yet - if it did,
> and if you have access to a reasonably powerful machine, you could
> reduce the computation by another factor of <number of cores>. (You
> could do this by brute force by manually running a bunch of jobs with
> different random-number seeds, or using the parallel package's
> parLapply(), then averaging the power curves you get ...)

No, simr doesn't really support parallel simulation yet, but see the
discussion on GitHub for some work arounds (e.g. using my experimental
fork or the future package + some custom code to combine things):
https://github.com/pitakakariki/simr/issues/39

Phillip

>
> On 2019-02-18 4:02 p.m., Alday, Phillip wrote:
>> Dear Chi,
>>
>> Does your maximal model converge for your observed data? Are you able to
>> detect all assumed "true" effects (i.e. are all of your effects of
>> interest significant)?
>>
>> If the answer is no: then your data are too noisy to give you a reliable
>> estimate of the effect and this can present itself as weird results in
>> simulation-based power analyses. Basically, if your estimates are noisy,
>> then your simulation will be noisy and your power estimates won't be
>> reliable. This also holds for power analysis without simulation, but
>> won't be as obvious: if you don't have a good estimate of your effect
>> size, then your power analysis will be misleading.
>>
>> As far as computational time: use the model in your power analysis that
>> you want to use in your final analysis. After all, you want to estimate
>> how much power your final analysis has! There is a large amount of
>> literature debating tradeoffs in Type I & II error when using different
>> random-effects structures, and you haven't revealed anything else about
>> your data and experimental design, so there's little specific advice I
>> can give. However, in my experience, removing the interaction terms from
>> the random effects generally speeds up the computation a lot while not
>> really changing model fit. Since the interaction term is the effect you
>> care about, I would reparameterize the model so that interaction is a
>> main effect, e.g.
>>
>> data$ab <- interaction(data$a, data$b)
>>
>> fit <- glmer(B ~ a * ab + (1 + a + ab | Subject) ....
>>
>> If your original model take an hour or more to compute,then it's no
>> surprise that 5000 simulations * 4 points on the power curve = 20 000
>> model fits take weeks!
>>
>> Best,
>>
>> Phillip
>>
>>
>> PS: Don't name your dataframe "data"! There is a built-in function with
>> that name in R and if you're not careful, you'll get all sorts of weird
>> erros.
>>
>>
>>
>> On 16/2/19 2:52 pm, Chi Zhang wrote:
>>> Dear all,
>>>
>>> I tried using powersim from R package simr to estimate the number of participants that I need for an experiment. I performed the simulation based on the data from my pilot study. The model I used is sketched below:
>>>
>>> fit <- glmer(B ~ a+b+a:b
>>>              (1+a+b+a:b|Subject) +
>>>              (1+a+b+a:b|Item),
>>>            family = binomial(link="logit"),
>>>            data = data,
>>>            control = glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=50000),
>>>                                   tol = .0001))
>>>
>>> in which Subject and Item mean the distinct id of subjects and items from the pilot study. I want to know test how the power of the interaction term (a:b) changes with the growth of the number of participants. The code I am using is:
>>>
>>> fit2<- extend(fit, along="Subject", n = 84)
>>> sim <- powerCurve(fit2, test = fcompare(~a+b), along = "Subject", breaks=c(48,60,72,84), nsim = 5000)
>>> print(sim)
>>>
>>> But the results of the simulation was rather bizarre. To begin with, the power of the interaction grew smaller when the number of participants increased from 72 to 84, which I believe is incompatible with the normal observation that the power increases with the number of participants. Second, I tried using the full random model to perform the simulation, but it is really slow (it took me weeks to get just one result). I was wondering if I can use a simpler random model to perform the simulation.
>>>
>>> To reiterate my question: first, why my simulated power decreased with the increase of the number of participants? Is there something wrong with my code? Second, can I use a simpler random model for the simulation in order to save time? Thanks in advance!
>>>
>>> Best,
>>>
>>> Chi Zhang
>>>
>>> ------------------------
>>> Chi Zhang
>>>
>>> PhD Student
>>> Department of Experimental Psychology
>>> Ghent University
>>> Henri Dunantlaan 2, B-9000 Gent, Belgium
>>> Tel: +32 465386530
>>> E-mail: chi.zhang at ugent.be
>>>
>>> ________________________________________
>>> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on behalf of r-sig-mixed-models-request at r-project.org <r-sig-mixed-models-request at r-project.org>
>>> Sent: Saturday, February 16, 2019 12:00
>>> To: r-sig-mixed-models at r-project.org
>>> Subject: R-sig-mixed-models Digest, Vol 146, Issue 11
>>>
>>> Send R-sig-mixed-models mailing list submissions to
>>>         r-sig-mixed-models at r-project.org
>>>
>>> To subscribe or unsubscribe via the World Wide Web, visit
>>>         https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>> or, via email, send a message with subject or body 'help' to
>>>         r-sig-mixed-models-request at r-project.org
>>>
>>> You can reach the person managing the list at
>>>         r-sig-mixed-models-owner at r-project.org
>>>
>>> When replying, please edit your Subject line so it is more specific
>>> than "Re: Contents of R-sig-mixed-models digest..."
>>>
>>>
>>> Today's Topics:
>>>
>>>    1. gls() funcion after R upgrade (Fabio Fanoni)
>>>    2. Help with: Multivariate models with binary variables and
>>>       variables with zipoisson distribution (Stacey Hannebaum)
>>>    3. Teaching Assistants for advanced multilevel modeling at ICPSR
>>>       (Poe, John)
>>>
>>> ----------------------------------------------------------------------
>>>
>>> Message: 1
>>> Date: Mon, 11 Feb 2019 08:40:28 +0000
>>> From: Fabio Fanoni <fabio.fanoni at popso.it>
>>> To: "r-sig-mixed-models at r-project.org"
>>>         <r-sig-mixed-models at r-project.org>
>>> Subject: [R-sig-ME] gls() funcion after R upgrade
>>> Message-ID: <d0cd3fe9653e44dcbdb782bb8af4094a at popso.it>
>>> Content-Type: text/plain; charset="utf-8"
>>>
>>> Dear all,
>>>
>>> I have been using the package nlme for about a year through the software R
>>>
>>> Recently, following an update of R (and the corresponding nlme package), in some situations the gls() function does not work.
>>>
>>>
>>> The following lines of code
>>>
>>> require(nlme)
>>> db<-read.csv2("G:\\Dati.csv",as.is=TRUE)
>>> f<-"DELTA_INVNORM_STD ~ 0+ ITOD + EU_CH"
>>> gls(as.formula(f),correlation=corARMA(p=2,q=3,fixed=FALSE),data=db,weights=varExp(),method="ML")
>>>
>>>
>>> with the old version of R returns the following output:
>>>
>>> Generalized least squares fit by maximum likelihood
>>>   Model: as.formula(f)
>>>   Data: db
>>>   Log-likelihood: -36.13853
>>>
>>> Coefficients:
>>>       ITOD      EU_CH
>>> -0.3138869 -0.5010393
>>>
>>> Correlation Structure: ARMA(2,3)
>>> Formula: ~1
>>>  Parameter estimate(s):
>>>        Phi1        Phi2      Theta1      Theta2      Theta3
>>>  0.50054107 -0.07357577  0.91847373  0.66706344  0.51689285
>>> Variance function:
>>> Structure: Power of variance covariate
>>> Formula: ~fitted(.)
>>>  Parameter estimates:
>>>       power
>>> -0.01893321
>>> Degrees of freedom: 71 total; 69 residual
>>> Residual standard error: 0.4077908
>>>
>>>
>>> while with the new one version of R ends with the following error message
>>>
>>> Error in `coef<-.corARMA`(`*tmp*`, value = value[parMap[, i]]) :
>>>   NA/NaN/Inf in foreign function call (arg 1)
>>>
>>>
>>>
>>> here are the parameters of the old version of R
>>>
>>>> sessionInfo()
>>> R version 3.3.2 (2016-10-31)
>>> Platform: i386-w64-mingw32/i386 (32-bit)
>>> Running under: Windows >= 8 x64 (build 9200)
>>>
>>> locale:
>>> [1] LC_COLLATE=Italian_Italy.1252  LC_CTYPE=Italian_Italy.1252    LC_MONETARY=Italian_Italy.1252
>>> [4] LC_NUMERIC=C                   LC_TIME=Italian_Italy.1252
>>>
>>> attached base packages:
>>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>>
>>> other attached packages:
>>> [1] nlme_3.1-128
>>>
>>> loaded via a namespace (and not attached):
>>> [1] tools_3.3.2     grid_3.3.2      lattice_0.20-34
>>>
>>>
>>>
>>> and the correspondents of the new one
>>>
>>>> sessionInfo()
>>> R version 3.5.2 (2018-12-20)
>>> Platform: i386-w64-mingw32/i386 (32-bit)
>>> Running under: Windows >= 8 x64 (build 9200)
>>>
>>> Matrix products: default
>>>
>>> locale:
>>> [1] LC_COLLATE=Italian_Italy.1252  LC_CTYPE=Italian_Italy.1252    LC_MONETARY=Italian_Italy.1252
>>> [4] LC_NUMERIC=C                   LC_TIME=Italian_Italy.1252
>>>
>>> attached base packages:
>>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>>
>>> other attached packages:
>>> [1] nlme_3.1-137
>>>
>>> loaded via a namespace (and not attached):
>>> [1] compiler_3.5.2  tools_3.5.2     grid_3.5.2      lattice_0.20-38
>>>
>>> The data used are shown below
>>>
>>>
>>> I wonder if someone could help me to solve this problem.
>>>
>>> Best regards,
>>>
>>> Fabio Fanoni
>>>
>>>
>>> DELTA_INVNORM_STD;ITOD;EU_CH
>>> -1,01888891391635;1,11744160507933;0,164921569515933
>>> -1,7287582158078;1,33122211515022;0,192399322297369
>>> -1,92084919508607;1,42620153798496;0,119394703266418
>>> -1,07481132504896;1,40851069525448;-0,21451386962765
>>> 0,368325144516397;1,143582080184;-0,541418306765893
>>> 1,60232300735139;0,877753890833926;-0,540071359447086
>>> 1,79762928248747;0,585114281768911;-0,457099318942592
>>> 0,91995670533225;0,300270733467356;-0,292232806294851
>>> -0,0262660321428139;0,20146037523289;-0,236469135573377
>>> -1,02121152912805;0,0419782544708825;-0,345437279939175
>>> -1,10672650470506;0,0574913198102401;-0,366584376685452
>>> -1,39178681241528;0,33582796272111;-0,222730255141816
>>> -1,1454072922613;0,119188584353956;-0,00950828000562807
>>> -0,958672816825354;-0,0667970836192611;0,4570747351185
>>> -1,32272038017045;-0,307712992787705;0,958678420940093
>>> -0,907400017348963;-0,781894397724971;1,3426934842003
>>> -1,24178782808971;-0,502123817789113;1,78543551198139
>>> -0,727279530248652;-0,198729005124588;1,65424271463061
>>> -0,663683090224516;-0,00292614483254617;1,29016249027665
>>> -0,870662613154294;0,284152373447782;0,852269477692287
>>> -0,740956260890321;0,212558271927232;0,359286261984589
>>> -0,875175106507846;0,16921613723138;0,301636857743261
>>> -0,159691021207123;0,344902273564796;0,406160071512065
>>> 0,485359221230746;0,405193753056144;0,545299871378597
>>> 0,91806657022351;0,51772409209882;0,664639525858513
>>> 0,94182403298868;0,624398008842597;0,721750147939642
>>> 0,772810225693767;0,669380547864261;0,746938090683181
>>> 0,454079892235428;0,829860577818214;0,875841077996076
>>> 0,216198540864624;0,85936757871922;1,06778126286636
>>> -0,390013295840405;0,820301653758161;1,32356681853436
>>> -1,08957286663887;0,735286971796754;1,51483352772482
>>> -1,7181964072478;0,54188741408352;1,59807495122772
>>> -2,14142427321141;0,3080114833751;1,30188093962788
>>> -1,73631132303729;0,242276386254216;0,821154955431617
>>> -1,14832298161027;-0,168559611137128;0,390400772122712
>>> -0,199657255063557;-0,673990227176314;-0,425580724204736
>>> 0,637711021315926;-1,26311924357155;-0,777807797606142
>>> 1,13372564859946;-2,22697608948676;-1,02793616079627
>>> 1,3387228312938;-2,67326030450369;-1,25476231880355
>>> 1,13410628583832;-2,41425486942835;-0,769187327492251
>>> 1,20371911407773;-1,73317141026889;-0,4868668898436
>>> 1,04245681382429;-0,627677467262121;-0,517038541303494
>>> 1,19122635858486;0,318264079861633;-0,902669938921763
>>> 1,10644115835792;0,53399823108225;-1,58826681113873
>>> 0,657027862693417;0,566924775110004;-2,16826290930735
>>> 0,391546504792817;0,382793958142103;-2,37380727704873
>>> -0,121513593301095;0,113508212335312;-2,28814134594747
>>> 0,090587410027589;-0,10456301293743;-1,92150191643467
>>> 0,293363258062962;-0,599038773090656;-1,5279235122598
>>> 0,490646403474033;-1,16804784867814;-1,10646327070376
>>> 0,663013157660536;-1,68022387383074;-0,281457211581005
>>> 0,48006697458688;-2,17535440458421;0,0140633243390067
>>> 0,33992207153128;-2,15751594621565;0,415319331867849
>>> 0,485131796838891;-1,91164287702336;0,74289724468591
>>> 0,645057199360022;-1,64072137764755;0,715150100824384
>>> 0,830719890823037;-1,20839230550334;0,886616664628225
>>> 1,00060688160202;-0,799350150825283;0,78775063121476
>>> 0,799516770753564;-0,474799224784852;0,620190215847712
>>> 0,884527141039385;-0,111321336688718;0,400772282236831
>>> 0,77592825475446;0,41873781137028;0,214623979323192
>>> 0,897761905934407;0,618216183184323;-0,371702774880583
>>> 1,23087946317063;0,86282014652723;-1,04275259746654
>>> 1,1730433788867;1,03592979384644;-1,50933562067236
>>> 1,18413315011977;0,843466359597336;-1,89254251877396
>>> 0,522306442129412;0,786432900675309;-1,199941514842
>>> -0,423472231576842;0,534354713733984;-0,258289703958632
>>> -1,02160314495135;0,445296924001273;0,367637345059225
>>> -1,2194743130469;0,791437804698057;0,831122380946011
>>> -0,682787491324478;0,735393577499657;0,636488297397258
>>> -0,277058822726022;0,988210808618591;0,362382895620209
>>> -0,0283252883031908;1,18581655608866;0,466771415869385
>>>
>>>
>>>
>>>
>>> Fabio Fanoni
>>> SVILUPPO MODELLI DI CREDITO
>>> Piazza Garibaldi 16
>>> 23100 Sondrio (SO)
>>> Banca Popolare di Sondrio
>>> Tel. +390342528920
>>> fabio.fanoni at popso.it<mailto:fabio.fanoni at popso.it>
>>>
>>>
>>> AVVERTENZE LEGALI - I contenuti di questo messaggio, proveniente da un indirizzo di posta elettronica aziendale della Banca Popolare di Sondrio, e gli eventuali allegati possono essere letti e utilizzati, per esigenze lavorative, da chi opera alle dipendenze o per conto della stessa, o ? comunque cointeressato nell'inerente relazione d'affari. Le dichiarazioni, ivi contenute, non impegnano contrattualmente la Banca Popolare di Sondrio se non nei limiti di quanto eventualmente previsto in accordi opportunamente formalizzati. Se il messaggio ? stato ricevuto per errore ce ne scusiamo, pregando di segnalare ci? al mittente e poi di distruggerlo senza farne alcun uso poich? l'utilizzo senza averne diritto ? vietato dalla legge e potrebbe costituire reato.
>>>
>>> DATI SOCIETARI - BANCA POPOLARE DI SONDRIO - Societ? cooperativa per azioni - Fondata nel 1871
>>> Sede sociale e direzione generale: I - 23100 SONDRIO SO - piazza Garibaldi, 16
>>> Indirizzo Internet: http:[doppiabarra]www[punto]popso[punto]it - E-mail: info[chiocciola]popso[punto]it
>>> Iscritta al Registro delle Imprese di Sondrio al n. 00053810149, all'Albo delle Banche al n. 842, all'Albo delle Societ? Cooperative al n. A160536 Capogruppo del Gruppo bancario Banca Popolare di Sondrio, iscritto all'Albo dei Gruppi bancari al n. 5696.0 Aderente al Fondo Interbancario di Tutela dei Depositi e al Fondo Nazionale di Garanzia - Codice fiscale e partita IVA: 00053810149
>>> Capitale sociale: EUR 1.360.157.331 - Riserve: EUR 947.325.264 (Dati approvati dall'Assemblea dei soci del 29 aprile 2017).
>>>
>>> N.B. I "filtri antivirus" e "antispam" in uso su molti sistemi di posta elettronica possono talvolta ritardare o impedire, in tutto o in parte, il recapito dei messaggi. In tali casi, salvo verifica di avvenuta ricezione, pu? essere necessario modificare i contenuti o le modalit? d'invio.
>>>
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>>
>>>
>>>
>>> ------------------------------
>>>
>>> Message: 2
>>> Date: Fri, 15 Feb 2019 14:25:09 -0600
>>> From: Stacey Hannebaum <stacey-hannebaum at utulsa.edu>
>>> To: r-sig-mixed-models at r-project.org
>>> Subject: [R-sig-ME] Help with: Multivariate models with binary
>>>         variables and variables with zipoisson distribution
>>> Message-ID:
>>>         <CAFtRfJXA5jOSX0n343Zs_wcyXnC1Dy3iw06GV513Li7GxUx8nw at mail.gmail.com>
>>> Content-Type: text/plain; charset="utf-8"
>>>
>>> Hi,
>>>
>>> I am trying to build a multivariate model to test for associations between
>>> individual variation in behavioral and other traits. Essentially, I want to
>>> derive two calculations from the model: (1) the repeatability of each of
>>> the three traits within individuals and (2) the correlation between each
>>> pair of traits and between each trait and fitness and trappability (the
>>> latter two have one value per individual).
>>> I have been working on this for some time and have reached a standstill.
>>> Here are the main issues I am facing. Any suggestions would be very
>>> appreciated.
>>> (1) How should a binary variable be modelled? For now, I have modelled it
>>> as Gaussian (with the assumption that 0s and 1s are drawn from some
>>> underlying Gaussian process) so that I can calculate correlations but I
>>> don?t know if this is reasonable. Any thoughts?
>>> (2) I tried to run the below model but received the following error:
>>> Error in priorformat(if (NOpriorG) { :
>>>   V is the wrong dimension for some prior$G/prior$R elements
>>> I am sure it has to do with modelling CHARGES as zipoisson - I think I need
>>> to specify the structure for modelling the 0?s but I don?t know where to
>>> start with this.
>>>
>>> Below are the variables of interest in case this is useful to know:
>>> logitTAPERETURN: logit-transformed latency to enter a nest once a novel
>>> stimulus was applied / total possible time allowed for trial; trials were
>>> terminated after 5 minutes whether or not bird entered nest; multiple
>>> trials per individual.
>>> CHARGES: number of times an individual attacked the novel stimulus (count
>>> data with many zeros); multiple trials per individual. This could be
>>> expressed as a rate (number of charges / latency to enter nest) but I
>>> decided to model length of trial as a predictor variable using log(latency
>>> to enter nest); trial repeated multiple times per subject
>>> STATUSAFTER: whether the bird stayed in the nest or fled from the nest
>>> (binary)
>>> CAPTURES: number of times an individual was captured (count data with many
>>> ones and no zeros); only one value per individual
>>> RELFITNESS: number of fledglings for individual / mean number of fledglings
>>> across all individuals; only one value per individual
>>>
>>> prior_tr_c_sa_nc_f =
>>> list(G=list(G1=list(V=diag(c(1,1,1,0.0001,0.0001),5,5),nu=1.002, fix = 4)),
>>>                                   R=list(V=diag(5),nu=1.002))
>>>
>>> I fixed CAPTURES and RELFITNESS because there is not within-individual
>>> variation in these variables.
>>>
>>> mcmc_tr_c_sa_nc_f_us <- MCMCglmm(cbind(logitTAPERETURN, CHARGES,
>>> STATUSAFTER, CAPTURES, RELFITNESS) ~
>>>                                             at.level(trait,1):(COLSIZE) +
>>>
>>> at.level(trait,1):(TAPETRIALNUM) +
>>>
>>> at.level(trait,1):(TAPENESTSTATUS) +
>>>                                             at.level(trait,2):(COLSIZE) +
>>>
>>> at.level(trait,2):(stTAPERETURN) +
>>>                                             at.level(trait,2):(stTAPETEMP) +
>>>
>>> at.level(trait,3):(stALARMTRIALNUM) +
>>>                                             at.level(trait,3):(stALARMTIME)
>>> +
>>>
>>> at.level(trait,3):(stALARMWINDSP) +
>>>
>>> at.level(trait,3):(STATUSBEFORE) +
>>>                                             at.level(trait,4):(COLSIZE) +
>>>                                             at.level(trait,5):(COLSIZE) +
>>>
>>> at.level(trait,5):(stCLUTCHSIZE) - 1,
>>>                                           random =~ us(trait):BAND,
>>>                                           rcov =~ us(trait):units,
>>>                                           family =
>>> c("gaussian","zipoisson","gaussian","ztpoisson","gaussian"),
>>>                                           prior = prior_tr_c_sa_nc_f,
>>>                                           nitt=420000,
>>>                                           burn=20000,
>>>                                           thin=100,
>>>                                           verbose = TRUE,
>>>                                           data =
>>> as.data.frame(MultivariateAnalysis20172018))
>>>
>>>
>>> Thank you,
>>>
>>> Stacey Hannebaum
>>> Postdoctoral Research Associate
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>>
>>>
>>>
>>> ------------------------------
>>>
>>> Message: 3
>>> Date: Fri, 15 Feb 2019 17:46:27 -0600
>>> From: "Poe, John" <jdpo223 at g.uky.edu>
>>> To: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
>>> Subject: [R-sig-ME] Teaching Assistants for advanced multilevel
>>>         modeling at ICPSR
>>> Message-ID:
>>>         <CAFW8BypZZqASQ7zdBCYFS3zBa04DPWtWebHjUVoF4cg0RXgPbg at mail.gmail.com>
>>> Content-Type: text/plain; charset="utf-8"
>>>
>>> I'm looking for recommendations on potential teaching assistants for my
>>> 4-week advanced multilevel modeling course at the ICPSR summer program in
>>> Ann Arbor, Michigan. If you know of advanced graduate students or postdocs
>>> who might be qualified and interested please let me know off the listserv
>>> or have them email me directly. It's a great program and Ann Arbor is an
>>> absolutely lovely place to spend some time in the summer.
>>>
>>> The class pulls from econometrics,  biostatistics, and psychometrics and
>>> covers both MLE and Bayesian multilevel modeling. I don't expect anyone to
>>> be comfortable with everything so they'd need to be willing to go through
>>> the material with me in an informal readings bootcamp before the class to
>>> fill in gaps.
>>>
>>> You can find last year's syllabus here:
>>> http://www.johndavidpoe.com/teaching/multilevel-modeling-ii-advanced-topics/
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>>
>>>
>>>
>>> ------------------------------
>>>
>>> Subject: Digest Footer
>>>
>>> _______________________________________________
>>> R-sig-mixed-models mailing list
>>> R-sig-mixed-models at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>>
>>> ------------------------------
>>>
>>> End of R-sig-mixed-models Digest, Vol 146, Issue 11
>>> ***************************************************
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From Ph||||p@A|d@y @end|ng |rom mp|@n|  Tue Feb 19 00:43:42 2019
From: Ph||||p@A|d@y @end|ng |rom mp|@n| (Alday, Phillip)
Date: Mon, 18 Feb 2019 23:43:42 +0000
Subject: [R-sig-ME] gls() funcion after R upgrade
In-Reply-To: <d0cd3fe9653e44dcbdb782bb8af4094a@popso.it>
References: <d0cd3fe9653e44dcbdb782bb8af4094a@popso.it>
Message-ID: <a1ff82fe-4f87-c923-e111-f73e9b31a4d6@mpi.nl>

Dear Fabio,

Your example runs fine on my machine, and I have a comparable R and nlme
version. Have you tried re-installing nlme and all its dependencies?

install.packages("nlme",dependencies=TRUE)

Sometimes dependencies can get out of sync and this can lead to weird
errors. (I've had this happen when lme4 was compiled against a different
Rcpp version than Matrix.)

As a sidebar, I get different estimates than your old output had:


>
gls(as.formula(f),correlation=corARMA(p=2,q=3,fixed=FALSE),data=db,weights=varExp(),method="ML")
Generalized least squares fit by maximum likelihood
? Model: as.formula(f)
? Data: db
? Log-likelihood: 11.74371

Coefficients:
????? ITOD????? EU_CH
-0.3123153 -0.4979789

Correlation Structure: ARMA(2,3)
?Formula: ~1
?Parameter estimate(s):
????? Phi1?????? Phi2???? Theta1???? Theta2???? Theta3
?0.7542011 -0.2467866? 0.8989841? 0.9444138? 0.9092150
Variance function:
?Structure: Exponential of variance covariate
?Formula: ~fitted(.)
?Parameter estimates:
????? expon
-0.01497128
Degrees of freedom: 71 total; 69 residual
Residual standard error: 0.817064

(Note that "power" is now called "expon" for "exponent".) The small
differences in the exponent and the coefficients could be numerical
issues, but the big differences in ARMA seem a bit worrying. The
log-likelihood of 'my' fit is better, but the residual standard error is
worse.

Best,

Phillip

> sessionInfo()
R version 3.5.2 (2018-12-20)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Linux Mint 19.1

Matrix products: default
BLAS: /usr/lib/x86_64-linux-gnu/openblas/libblas.so.3
LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.2.20.so

::snip::


attached base packages:
[1] stats???? graphics? grDevices utils???? datasets? methods?? base

other attached packages:
[1] nlme_3.1-137

loaded via a namespace (and not attached):
[1] compiler_3.5.2? grid_3.5.2????? lattice_0.20-38

On 11/2/19 9:40 am, Fabio Fanoni wrote:
> Dear all,
>
> I have been using the package nlme for about a year through the software R
>
> Recently, following an update of R (and the corresponding nlme package), in some situations the gls() function does not work.
>
>
> The following lines of code
>
> require(nlme)
> db<-read.csv2("G:\\Dati.csv",as.is=TRUE)
> f<-"DELTA_INVNORM_STD ~ 0+ ITOD + EU_CH"
> gls(as.formula(f),correlation=corARMA(p=2,q=3,fixed=FALSE),data=db,weights=varExp(),method="ML")
>
>
> with the old version of R returns the following output:
>
> Generalized least squares fit by maximum likelihood
>   Model: as.formula(f)
>   Data: db
>   Log-likelihood: -36.13853
>
> Coefficients:
>       ITOD      EU_CH
> -0.3138869 -0.5010393
>
> Correlation Structure: ARMA(2,3)
> Formula: ~1
>  Parameter estimate(s):
>        Phi1        Phi2      Theta1      Theta2      Theta3
>  0.50054107 -0.07357577  0.91847373  0.66706344  0.51689285
> Variance function:
> Structure: Power of variance covariate
> Formula: ~fitted(.)
>  Parameter estimates:
>       power
> -0.01893321
> Degrees of freedom: 71 total; 69 residual
> Residual standard error: 0.4077908
>
>
> while with the new one version of R ends with the following error message
>
> Error in `coef<-.corARMA`(`*tmp*`, value = value[parMap[, i]]) :
>   NA/NaN/Inf in foreign function call (arg 1)
>
>
>
> here are the parameters of the old version of R
>
>> sessionInfo()
> R version 3.3.2 (2016-10-31)
> Platform: i386-w64-mingw32/i386 (32-bit)
> Running under: Windows >= 8 x64 (build 9200)
>
> locale:
> [1] LC_COLLATE=Italian_Italy.1252  LC_CTYPE=Italian_Italy.1252    LC_MONETARY=Italian_Italy.1252
> [4] LC_NUMERIC=C                   LC_TIME=Italian_Italy.1252
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] nlme_3.1-128
>
> loaded via a namespace (and not attached):
> [1] tools_3.3.2     grid_3.3.2      lattice_0.20-34
>
>
>
> and the correspondents of the new one
>
>> sessionInfo()
> R version 3.5.2 (2018-12-20)
> Platform: i386-w64-mingw32/i386 (32-bit)
> Running under: Windows >= 8 x64 (build 9200)
>
> Matrix products: default
>
> locale:
> [1] LC_COLLATE=Italian_Italy.1252  LC_CTYPE=Italian_Italy.1252    LC_MONETARY=Italian_Italy.1252
> [4] LC_NUMERIC=C                   LC_TIME=Italian_Italy.1252
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] nlme_3.1-137
>
> loaded via a namespace (and not attached):
> [1] compiler_3.5.2  tools_3.5.2     grid_3.5.2      lattice_0.20-38
>
> The data used are shown below
>
>
> I wonder if someone could help me to solve this problem.
>
> Best regards,
>
> Fabio Fanoni
>
>
> DELTA_INVNORM_STD;ITOD;EU_CH
> -1,01888891391635;1,11744160507933;0,164921569515933
> -1,7287582158078;1,33122211515022;0,192399322297369
> -1,92084919508607;1,42620153798496;0,119394703266418
> -1,07481132504896;1,40851069525448;-0,21451386962765
> 0,368325144516397;1,143582080184;-0,541418306765893
> 1,60232300735139;0,877753890833926;-0,540071359447086
> 1,79762928248747;0,585114281768911;-0,457099318942592
> 0,91995670533225;0,300270733467356;-0,292232806294851
> -0,0262660321428139;0,20146037523289;-0,236469135573377
> -1,02121152912805;0,0419782544708825;-0,345437279939175
> -1,10672650470506;0,0574913198102401;-0,366584376685452
> -1,39178681241528;0,33582796272111;-0,222730255141816
> -1,1454072922613;0,119188584353956;-0,00950828000562807
> -0,958672816825354;-0,0667970836192611;0,4570747351185
> -1,32272038017045;-0,307712992787705;0,958678420940093
> -0,907400017348963;-0,781894397724971;1,3426934842003
> -1,24178782808971;-0,502123817789113;1,78543551198139
> -0,727279530248652;-0,198729005124588;1,65424271463061
> -0,663683090224516;-0,00292614483254617;1,29016249027665
> -0,870662613154294;0,284152373447782;0,852269477692287
> -0,740956260890321;0,212558271927232;0,359286261984589
> -0,875175106507846;0,16921613723138;0,301636857743261
> -0,159691021207123;0,344902273564796;0,406160071512065
> 0,485359221230746;0,405193753056144;0,545299871378597
> 0,91806657022351;0,51772409209882;0,664639525858513
> 0,94182403298868;0,624398008842597;0,721750147939642
> 0,772810225693767;0,669380547864261;0,746938090683181
> 0,454079892235428;0,829860577818214;0,875841077996076
> 0,216198540864624;0,85936757871922;1,06778126286636
> -0,390013295840405;0,820301653758161;1,32356681853436
> -1,08957286663887;0,735286971796754;1,51483352772482
> -1,7181964072478;0,54188741408352;1,59807495122772
> -2,14142427321141;0,3080114833751;1,30188093962788
> -1,73631132303729;0,242276386254216;0,821154955431617
> -1,14832298161027;-0,168559611137128;0,390400772122712
> -0,199657255063557;-0,673990227176314;-0,425580724204736
> 0,637711021315926;-1,26311924357155;-0,777807797606142
> 1,13372564859946;-2,22697608948676;-1,02793616079627
> 1,3387228312938;-2,67326030450369;-1,25476231880355
> 1,13410628583832;-2,41425486942835;-0,769187327492251
> 1,20371911407773;-1,73317141026889;-0,4868668898436
> 1,04245681382429;-0,627677467262121;-0,517038541303494
> 1,19122635858486;0,318264079861633;-0,902669938921763
> 1,10644115835792;0,53399823108225;-1,58826681113873
> 0,657027862693417;0,566924775110004;-2,16826290930735
> 0,391546504792817;0,382793958142103;-2,37380727704873
> -0,121513593301095;0,113508212335312;-2,28814134594747
> 0,090587410027589;-0,10456301293743;-1,92150191643467
> 0,293363258062962;-0,599038773090656;-1,5279235122598
> 0,490646403474033;-1,16804784867814;-1,10646327070376
> 0,663013157660536;-1,68022387383074;-0,281457211581005
> 0,48006697458688;-2,17535440458421;0,0140633243390067
> 0,33992207153128;-2,15751594621565;0,415319331867849
> 0,485131796838891;-1,91164287702336;0,74289724468591
> 0,645057199360022;-1,64072137764755;0,715150100824384
> 0,830719890823037;-1,20839230550334;0,886616664628225
> 1,00060688160202;-0,799350150825283;0,78775063121476
> 0,799516770753564;-0,474799224784852;0,620190215847712
> 0,884527141039385;-0,111321336688718;0,400772282236831
> 0,77592825475446;0,41873781137028;0,214623979323192
> 0,897761905934407;0,618216183184323;-0,371702774880583
> 1,23087946317063;0,86282014652723;-1,04275259746654
> 1,1730433788867;1,03592979384644;-1,50933562067236
> 1,18413315011977;0,843466359597336;-1,89254251877396
> 0,522306442129412;0,786432900675309;-1,199941514842
> -0,423472231576842;0,534354713733984;-0,258289703958632
> -1,02160314495135;0,445296924001273;0,367637345059225
> -1,2194743130469;0,791437804698057;0,831122380946011
> -0,682787491324478;0,735393577499657;0,636488297397258
> -0,277058822726022;0,988210808618591;0,362382895620209
> -0,0283252883031908;1,18581655608866;0,466771415869385
>
>
>
>
> Fabio Fanoni
> SVILUPPO MODELLI DI CREDITO
> Piazza Garibaldi 16
> 23100 Sondrio (SO)
> Banca Popolare di Sondrio
> Tel. +390342528920
> fabio.fanoni at popso.it<mailto:fabio.fanoni at popso.it>
>
>
> AVVERTENZE LEGALI - I contenuti di questo messaggio, proveniente da un indirizzo di posta elettronica aziendale della Banca Popolare di Sondrio, e gli eventuali allegati possono essere letti e utilizzati, per esigenze lavorative, da chi opera alle dipendenze o per conto della stessa, o ? comunque cointeressato nell'inerente relazione d'affari. Le dichiarazioni, ivi contenute, non impegnano contrattualmente la Banca Popolare di Sondrio se non nei limiti di quanto eventualmente previsto in accordi opportunamente formalizzati. Se il messaggio ? stato ricevuto per errore ce ne scusiamo, pregando di segnalare ci? al mittente e poi di distruggerlo senza farne alcun uso poich? l'utilizzo senza averne diritto ? vietato dalla legge e potrebbe costituire reato.
>
> DATI SOCIETARI - BANCA POPOLARE DI SONDRIO - Societ? cooperativa per azioni - Fondata nel 1871
> Sede sociale e direzione generale: I - 23100 SONDRIO SO - piazza Garibaldi, 16
> Indirizzo Internet: http:[doppiabarra]www[punto]popso[punto]it - E-mail: info[chiocciola]popso[punto]it
> Iscritta al Registro delle Imprese di Sondrio al n. 00053810149, all'Albo delle Banche al n. 842, all'Albo delle Societ? Cooperative al n. A160536 Capogruppo del Gruppo bancario Banca Popolare di Sondrio, iscritto all'Albo dei Gruppi bancari al n. 5696.0 Aderente al Fondo Interbancario di Tutela dei Depositi e al Fondo Nazionale di Garanzia - Codice fiscale e partita IVA: 00053810149
> Capitale sociale: EUR 1.360.157.331 - Riserve: EUR 947.325.264 (Dati approvati dall'Assemblea dei soci del 29 aprile 2017).
>
> N.B. I "filtri antivirus" e "antispam" in uso su molti sistemi di posta elettronica possono talvolta ritardare o impedire, in tutto o in parte, il recapito dei messaggi. In tali casi, salvo verifica di avvenuta ricezione, pu? essere necessario modificare i contenuti o le modalit? d'invio.
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From ph||||p@@|d@y @end|ng |rom mp|@n|  Tue Feb 19 17:43:13 2019
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Tue, 19 Feb 2019 17:43:13 +0100
Subject: [R-sig-ME] R:  gls() funcion after R upgrade
In-Reply-To: <5640809b8d8849edb929ab293773809b@popso.it>
References: <d0cd3fe9653e44dcbdb782bb8af4094a@popso.it>
 <a1ff82fe-4f87-c923-e111-f73e9b31a4d6@mpi.nl>
 <5640809b8d8849edb929ab293773809b@popso.it>
Message-ID: <eb21ed55-8e07-b457-6f9b-c1cfa3ff3b0f@mpi.nl>

This seems like a bug / compiler issue on 32-bit Windows. I don't have a
convenient Windows machine to test it out myself, but you should report
it (along with your minimal example):

https://cran.r-project.org/web/packages/nlme/index.html

Also: for many reasons (performance, ability to work with larger
datasets, etc.), if you have access to the 64-bit versions, you should
be using them!

Phillip

On 19/2/19 5:02 pm, Fabio Fanoni wrote:
> Dear Phillip,
> 
> thanks for the reply
> 
> I tried both to re-install the package nlme (and all its dependencies) and to re-install R as a whole but I always get the same error message
> 
> The problem seems to be related to the varExp() variance function structures on a 32-bit windows platform
> With 64-bit windows platform, the old version 3.3.2 and the new version 3.5.2 produce the same results
> At the same time, the varPower() function always produces the same results regardless of the windows platform or version of R
> The following table shows the coefficients obtained according to the platform (32 bit / 62 bit), version of R and variance function
> 
> Platform	Version	weights	   	ITOD	   	EU_CH
> windows 32 bit	3.3.2	varExp()	-0.1053291	-0.4406068
> windows 32 bit	3.5.2	varExp()	<NA>	         <NA>
> windows 64 bit	3.3.2	varExp()	-0.3011427	-0.5537158
> windows 64 bit	3.5.2	varExp()	-0.3011427	-0.5537158
> windows 32 bit	3.3.2	varPower()	-0.3138869	-0.5010393
> windows 32 bit	3.5.2	varPower()	-0.3138869	-0.5010393
> windows 64 bit	3.3.2	varPower()	-0.3138868	-0.5010394
> windows 64 bit	3.5.2	varPower()	-0.3138868	-0.5010394
> 				
> linux 64 bit	3.5.2	varExp()	-0.3123153 	-0.4979789
> 
> These values differ from what I reported in the previous e-mail. For a ?copy and paste? mistake, in the previous e-mail I reported the results obtained with the varPower() function instead of varExp() function. 
> 
> Here is the correct output
> 
>> gls(as.formula(f),correlation=corARMA(p=2,q=3,fixed=FALSE),data=db,weights=varExp(),method="ML")
> Generalized least squares fit by maximum likelihood
>   Model: as.formula(f) 
>   Data: db 
>   Log-likelihood: 12.11128
> 
> Coefficients:
>       ITOD      EU_CH 
> -0.1053291 -0.4406068 
> 
> Correlation Structure: ARMA(2,3)
>  Formula: ~1 
>  Parameter estimate(s):
>       Phi1       Phi2     Theta1     Theta2     Theta3 
>  0.6485554 -0.1750409  0.9450232  0.9566344  0.9566288 
> Variance function:
>  Structure: Exponential of variance covariate
>  Formula: ~fitted(.) 
>  Parameter estimates:
>         expon 
> -0.0007047246 
> Degrees of freedom: 71 total; 69 residual
> Residual standard error: 0.7740528
> 
> On the 3.5.2 version of R, the sessionInfo() show "Matrix products: default" whereas in the 3.3.2 version this message does not appear. 
> 
> If I set options(matprod="internal") on the 3.5.2 I avoid the error but the results I get are very different from those of version 3.3.2
> 
> If I use the REML method instead of the ML I get much more similar results but, at the same time, very different from those I would get with the same method on the version 3.2.2
> 
> Maybe these clues can inspire a solution?
> 
> 
> Fabio
> 
> 
> 
> 
> 
> -----Messaggio originale-----
> Da: Alday, Phillip <Phillip.Alday at mpi.nl> 
> Inviato: marted? 19 febbraio 2019 00:44
> A: Fabio Fanoni <fabio.fanoni at popso.it>; r-sig-mixed-models at r-project.org
> Oggetto: Re: [R-sig-ME] gls() funcion after R upgrade
> 
> Dear Fabio,
> 
> Your example runs fine on my machine, and I have a comparable R and nlme version. Have you tried re-installing nlme and all its dependencies?
> 
> install.packages("nlme",dependencies=TRUE)
> 
> Sometimes dependencies can get out of sync and this can lead to weird errors. (I've had this happen when lme4 was compiled against a different Rcpp version than Matrix.)
> 
> As a sidebar, I get different estimates than your old output had:
> 
> 
>>
> gls(as.formula(f),correlation=corARMA(p=2,q=3,fixed=FALSE),data=db,weights=varExp(),method="ML")
> Generalized least squares fit by maximum likelihood
> ? Model: as.formula(f)
> ? Data: db
> ? Log-likelihood: 11.74371
> 
> Coefficients:
> ????? ITOD????? EU_CH
> -0.3123153 -0.4979789
> 
> Correlation Structure: ARMA(2,3)
> ?Formula: ~1
> ?Parameter estimate(s):
> ????? Phi1?????? Phi2???? Theta1???? Theta2???? Theta3
> ?0.7542011 -0.2467866? 0.8989841? 0.9444138? 0.9092150 Variance function:
> ?Structure: Exponential of variance covariate
> ?Formula: ~fitted(.)
> ?Parameter estimates:
> ????? expon
> -0.01497128
> Degrees of freedom: 71 total; 69 residual Residual standard error: 0.817064
> 
> (Note that "power" is now called "expon" for "exponent".) The small differences in the exponent and the coefficients could be numerical issues, but the big differences in ARMA seem a bit worrying. The log-likelihood of 'my' fit is better, but the residual standard error is worse.
> 
> Best,
> 
> Phillip
> 
>> sessionInfo()
> R version 3.5.2 (2018-12-20)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Linux Mint 19.1
> 
> Matrix products: default
> BLAS: /usr/lib/x86_64-linux-gnu/openblas/libblas.so.3
> LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.2.20.so
> 
> ::snip::
> 
> 
> attached base packages:
> [1] stats???? graphics? grDevices utils???? datasets? methods?? base
> 
> other attached packages:
> [1] nlme_3.1-137
> 
> loaded via a namespace (and not attached):
> [1] compiler_3.5.2? grid_3.5.2????? lattice_0.20-38
> 
> On 11/2/19 9:40 am, Fabio Fanoni wrote:
>> Dear all,
>>
>> I have been using the package nlme for about a year through the 
>> software R
>>
>> Recently, following an update of R (and the corresponding nlme package), in some situations the gls() function does not work.
>>
>>
>> The following lines of code
>>
>> require(nlme)
>> db<-read.csv2("G:\\Dati.csv",as.is=TRUE)
>> f<-"DELTA_INVNORM_STD ~ 0+ ITOD + EU_CH"
>> gls(as.formula(f),correlation=corARMA(p=2,q=3,fixed=FALSE),data=db,wei
>> ghts=varExp(),method="ML")
>>
>>
>> with the old version of R returns the following output:
>>
>> Generalized least squares fit by maximum likelihood
>>   Model: as.formula(f)
>>   Data: db
>>   Log-likelihood: -36.13853
>>
>> Coefficients:
>>       ITOD      EU_CH
>> -0.3138869 -0.5010393
>>
>> Correlation Structure: ARMA(2,3)
>> Formula: ~1
>>  Parameter estimate(s):
>>        Phi1        Phi2      Theta1      Theta2      Theta3
>>  0.50054107 -0.07357577  0.91847373  0.66706344  0.51689285 Variance 
>> function:
>> Structure: Power of variance covariate
>> Formula: ~fitted(.)
>>  Parameter estimates:
>>       power
>> -0.01893321
>> Degrees of freedom: 71 total; 69 residual Residual standard error: 
>> 0.4077908
>>
>>
>> while with the new one version of R ends with the following error 
>> message
>>
>> Error in `coef<-.corARMA`(`*tmp*`, value = value[parMap[, i]]) :
>>   NA/NaN/Inf in foreign function call (arg 1)
>>
>>
>>
>> here are the parameters of the old version of R
>>
>>> sessionInfo()
>> R version 3.3.2 (2016-10-31)
>> Platform: i386-w64-mingw32/i386 (32-bit) Running under: Windows >= 8 
>> x64 (build 9200)
>>
>> locale:
>> [1] LC_COLLATE=Italian_Italy.1252  LC_CTYPE=Italian_Italy.1252    LC_MONETARY=Italian_Italy.1252
>> [4] LC_NUMERIC=C                   LC_TIME=Italian_Italy.1252
>>
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>> other attached packages:
>> [1] nlme_3.1-128
>>
>> loaded via a namespace (and not attached):
>> [1] tools_3.3.2     grid_3.3.2      lattice_0.20-34
>>
>>
>>
>> and the correspondents of the new one
>>
>>> sessionInfo()
>> R version 3.5.2 (2018-12-20)
>> Platform: i386-w64-mingw32/i386 (32-bit) Running under: Windows >= 8 
>> x64 (build 9200)
>>
>> Matrix products: default
>>
>> locale:
>> [1] LC_COLLATE=Italian_Italy.1252  LC_CTYPE=Italian_Italy.1252    LC_MONETARY=Italian_Italy.1252
>> [4] LC_NUMERIC=C                   LC_TIME=Italian_Italy.1252
>>
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>> other attached packages:
>> [1] nlme_3.1-137
>>
>> loaded via a namespace (and not attached):
>> [1] compiler_3.5.2  tools_3.5.2     grid_3.5.2      lattice_0.20-38
>>
>> The data used are shown below
>>
>>
>> I wonder if someone could help me to solve this problem.
>>
>> Best regards,
>>
>> Fabio Fanoni
>>
>>
>> DELTA_INVNORM_STD;ITOD;EU_CH
>> -1,01888891391635;1,11744160507933;0,164921569515933
>> -1,7287582158078;1,33122211515022;0,192399322297369
>> -1,92084919508607;1,42620153798496;0,119394703266418
>> -1,07481132504896;1,40851069525448;-0,21451386962765
>> 0,368325144516397;1,143582080184;-0,541418306765893
>> 1,60232300735139;0,877753890833926;-0,540071359447086
>> 1,79762928248747;0,585114281768911;-0,457099318942592
>> 0,91995670533225;0,300270733467356;-0,292232806294851
>> -0,0262660321428139;0,20146037523289;-0,236469135573377
>> -1,02121152912805;0,0419782544708825;-0,345437279939175
>> -1,10672650470506;0,0574913198102401;-0,366584376685452
>> -1,39178681241528;0,33582796272111;-0,222730255141816
>> -1,1454072922613;0,119188584353956;-0,00950828000562807
>> -0,958672816825354;-0,0667970836192611;0,4570747351185
>> -1,32272038017045;-0,307712992787705;0,958678420940093
>> -0,907400017348963;-0,781894397724971;1,3426934842003
>> -1,24178782808971;-0,502123817789113;1,78543551198139
>> -0,727279530248652;-0,198729005124588;1,65424271463061
>> -0,663683090224516;-0,00292614483254617;1,29016249027665
>> -0,870662613154294;0,284152373447782;0,852269477692287
>> -0,740956260890321;0,212558271927232;0,359286261984589
>> -0,875175106507846;0,16921613723138;0,301636857743261
>> -0,159691021207123;0,344902273564796;0,406160071512065
>> 0,485359221230746;0,405193753056144;0,545299871378597
>> 0,91806657022351;0,51772409209882;0,664639525858513
>> 0,94182403298868;0,624398008842597;0,721750147939642
>> 0,772810225693767;0,669380547864261;0,746938090683181
>> 0,454079892235428;0,829860577818214;0,875841077996076
>> 0,216198540864624;0,85936757871922;1,06778126286636
>> -0,390013295840405;0,820301653758161;1,32356681853436
>> -1,08957286663887;0,735286971796754;1,51483352772482
>> -1,7181964072478;0,54188741408352;1,59807495122772
>> -2,14142427321141;0,3080114833751;1,30188093962788
>> -1,73631132303729;0,242276386254216;0,821154955431617
>> -1,14832298161027;-0,168559611137128;0,390400772122712
>> -0,199657255063557;-0,673990227176314;-0,425580724204736
>> 0,637711021315926;-1,26311924357155;-0,777807797606142
>> 1,13372564859946;-2,22697608948676;-1,02793616079627
>> 1,3387228312938;-2,67326030450369;-1,25476231880355
>> 1,13410628583832;-2,41425486942835;-0,769187327492251
>> 1,20371911407773;-1,73317141026889;-0,4868668898436
>> 1,04245681382429;-0,627677467262121;-0,517038541303494
>> 1,19122635858486;0,318264079861633;-0,902669938921763
>> 1,10644115835792;0,53399823108225;-1,58826681113873
>> 0,657027862693417;0,566924775110004;-2,16826290930735
>> 0,391546504792817;0,382793958142103;-2,37380727704873
>> -0,121513593301095;0,113508212335312;-2,28814134594747
>> 0,090587410027589;-0,10456301293743;-1,92150191643467
>> 0,293363258062962;-0,599038773090656;-1,5279235122598
>> 0,490646403474033;-1,16804784867814;-1,10646327070376
>> 0,663013157660536;-1,68022387383074;-0,281457211581005
>> 0,48006697458688;-2,17535440458421;0,0140633243390067
>> 0,33992207153128;-2,15751594621565;0,415319331867849
>> 0,485131796838891;-1,91164287702336;0,74289724468591
>> 0,645057199360022;-1,64072137764755;0,715150100824384
>> 0,830719890823037;-1,20839230550334;0,886616664628225
>> 1,00060688160202;-0,799350150825283;0,78775063121476
>> 0,799516770753564;-0,474799224784852;0,620190215847712
>> 0,884527141039385;-0,111321336688718;0,400772282236831
>> 0,77592825475446;0,41873781137028;0,214623979323192
>> 0,897761905934407;0,618216183184323;-0,371702774880583
>> 1,23087946317063;0,86282014652723;-1,04275259746654
>> 1,1730433788867;1,03592979384644;-1,50933562067236
>> 1,18413315011977;0,843466359597336;-1,89254251877396
>> 0,522306442129412;0,786432900675309;-1,199941514842
>> -0,423472231576842;0,534354713733984;-0,258289703958632
>> -1,02160314495135;0,445296924001273;0,367637345059225
>> -1,2194743130469;0,791437804698057;0,831122380946011
>> -0,682787491324478;0,735393577499657;0,636488297397258
>> -0,277058822726022;0,988210808618591;0,362382895620209
>> -0,0283252883031908;1,18581655608866;0,466771415869385
>>
>>
>>
>>
>> Fabio Fanoni
>> SVILUPPO MODELLI DI CREDITO
>> Piazza Garibaldi 16
>> 23100 Sondrio (SO)
>> Banca Popolare di Sondrio
>> Tel. +390342528920
>> fabio.fanoni at popso.it<mailto:fabio.fanoni at popso.it>
>>
>>
>> AVVERTENZE LEGALI - I contenuti di questo messaggio, proveniente da un indirizzo di posta elettronica aziendale della Banca Popolare di Sondrio, e gli eventuali allegati possono essere letti e utilizzati, per esigenze lavorative, da chi opera alle dipendenze o per conto della stessa, o ? comunque cointeressato nell'inerente relazione d'affari. Le dichiarazioni, ivi contenute, non impegnano contrattualmente la Banca Popolare di Sondrio se non nei limiti di quanto eventualmente previsto in accordi opportunamente formalizzati. Se il messaggio ? stato ricevuto per errore ce ne scusiamo, pregando di segnalare ci? al mittente e poi di distruggerlo senza farne alcun uso poich? l'utilizzo senza averne diritto ? vietato dalla legge e potrebbe costituire reato.
>>
>> DATI SOCIETARI - BANCA POPOLARE DI SONDRIO - Societ? cooperativa per 
>> azioni - Fondata nel 1871 Sede sociale e direzione generale: I - 23100 
>> SONDRIO SO - piazza Garibaldi, 16 Indirizzo Internet: 
>> http:[doppiabarra]www[punto]popso[punto]it - E-mail: 
>> info[chiocciola]popso[punto]it Iscritta al Registro delle Imprese di Sondrio al n. 00053810149, all'Albo delle Banche al n. 842, all'Albo delle Societ? Cooperative al n. A160536 Capogruppo del Gruppo bancario Banca Popolare di Sondrio, iscritto all'Albo dei Gruppi bancari al n. 5696.0 Aderente al Fondo Interbancario di Tutela dei Depositi e al Fondo Nazionale di Garanzia - Codice fiscale e partita IVA: 00053810149 Capitale sociale: EUR 1.360.157.331 - Riserve: EUR 947.325.264 (Dati approvati dall'Assemblea dei soci del 29 aprile 2017).
>>
>> N.B. I "filtri antivirus" e "antispam" in uso su molti sistemi di posta elettronica possono talvolta ritardare o impedire, in tutto o in parte, il recapito dei messaggi. In tali casi, salvo verifica di avvenuta ricezione, pu? essere necessario modificare i contenuti o le modalit? d'invio.
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From |@b|o@|@non| @end|ng |rom pop@o@|t  Tue Feb 19 17:02:21 2019
From: |@b|o@|@non| @end|ng |rom pop@o@|t (Fabio Fanoni)
Date: Tue, 19 Feb 2019 16:02:21 +0000
Subject: [R-sig-ME] R:  gls() funcion after R upgrade
In-Reply-To: <a1ff82fe-4f87-c923-e111-f73e9b31a4d6@mpi.nl>
References: <d0cd3fe9653e44dcbdb782bb8af4094a@popso.it>
 <a1ff82fe-4f87-c923-e111-f73e9b31a4d6@mpi.nl>
Message-ID: <5640809b8d8849edb929ab293773809b@popso.it>

Dear Phillip,

thanks for the reply

I tried both to re-install the package nlme (and all its dependencies) and to re-install R as a whole but I always get the same error message

The problem seems to be related to the varExp() variance function structures on a 32-bit windows platform
With 64-bit windows platform, the old version 3.3.2 and the new version 3.5.2 produce the same results
At the same time, the varPower() function always produces the same results regardless of the windows platform or version of R
The following table shows the coefficients obtained according to the platform (32 bit / 62 bit), version of R and variance function

Platform	Version	weights	   	ITOD	   	EU_CH
windows 32 bit	3.3.2	varExp()	-0.1053291	-0.4406068
windows 32 bit	3.5.2	varExp()	<NA>	         <NA>
windows 64 bit	3.3.2	varExp()	-0.3011427	-0.5537158
windows 64 bit	3.5.2	varExp()	-0.3011427	-0.5537158
windows 32 bit	3.3.2	varPower()	-0.3138869	-0.5010393
windows 32 bit	3.5.2	varPower()	-0.3138869	-0.5010393
windows 64 bit	3.3.2	varPower()	-0.3138868	-0.5010394
windows 64 bit	3.5.2	varPower()	-0.3138868	-0.5010394
				
linux 64 bit	3.5.2	varExp()	-0.3123153 	-0.4979789

These values differ from what I reported in the previous e-mail. For a ?copy and paste? mistake, in the previous e-mail I reported the results obtained with the varPower() function instead of varExp() function. 

Here is the correct output

> gls(as.formula(f),correlation=corARMA(p=2,q=3,fixed=FALSE),data=db,weights=varExp(),method="ML")
Generalized least squares fit by maximum likelihood
  Model: as.formula(f) 
  Data: db 
  Log-likelihood: 12.11128

Coefficients:
      ITOD      EU_CH 
-0.1053291 -0.4406068 

Correlation Structure: ARMA(2,3)
 Formula: ~1 
 Parameter estimate(s):
      Phi1       Phi2     Theta1     Theta2     Theta3 
 0.6485554 -0.1750409  0.9450232  0.9566344  0.9566288 
Variance function:
 Structure: Exponential of variance covariate
 Formula: ~fitted(.) 
 Parameter estimates:
        expon 
-0.0007047246 
Degrees of freedom: 71 total; 69 residual
Residual standard error: 0.7740528

On the 3.5.2 version of R, the sessionInfo() show "Matrix products: default" whereas in the 3.3.2 version this message does not appear. 

If I set options(matprod="internal") on the 3.5.2 I avoid the error but the results I get are very different from those of version 3.3.2

If I use the REML method instead of the ML I get much more similar results but, at the same time, very different from those I would get with the same method on the version 3.2.2

Maybe these clues can inspire a solution?


Fabio





-----Messaggio originale-----
Da: Alday, Phillip <Phillip.Alday at mpi.nl> 
Inviato: marted? 19 febbraio 2019 00:44
A: Fabio Fanoni <fabio.fanoni at popso.it>; r-sig-mixed-models at r-project.org
Oggetto: Re: [R-sig-ME] gls() funcion after R upgrade

Dear Fabio,

Your example runs fine on my machine, and I have a comparable R and nlme version. Have you tried re-installing nlme and all its dependencies?

install.packages("nlme",dependencies=TRUE)

Sometimes dependencies can get out of sync and this can lead to weird errors. (I've had this happen when lme4 was compiled against a different Rcpp version than Matrix.)

As a sidebar, I get different estimates than your old output had:


>
gls(as.formula(f),correlation=corARMA(p=2,q=3,fixed=FALSE),data=db,weights=varExp(),method="ML")
Generalized least squares fit by maximum likelihood
? Model: as.formula(f)
? Data: db
? Log-likelihood: 11.74371

Coefficients:
????? ITOD????? EU_CH
-0.3123153 -0.4979789

Correlation Structure: ARMA(2,3)
?Formula: ~1
?Parameter estimate(s):
????? Phi1?????? Phi2???? Theta1???? Theta2???? Theta3
?0.7542011 -0.2467866? 0.8989841? 0.9444138? 0.9092150 Variance function:
?Structure: Exponential of variance covariate
?Formula: ~fitted(.)
?Parameter estimates:
????? expon
-0.01497128
Degrees of freedom: 71 total; 69 residual Residual standard error: 0.817064

(Note that "power" is now called "expon" for "exponent".) The small differences in the exponent and the coefficients could be numerical issues, but the big differences in ARMA seem a bit worrying. The log-likelihood of 'my' fit is better, but the residual standard error is worse.

Best,

Phillip

> sessionInfo()
R version 3.5.2 (2018-12-20)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Linux Mint 19.1

Matrix products: default
BLAS: /usr/lib/x86_64-linux-gnu/openblas/libblas.so.3
LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.2.20.so

::snip::


attached base packages:
[1] stats???? graphics? grDevices utils???? datasets? methods?? base

other attached packages:
[1] nlme_3.1-137

loaded via a namespace (and not attached):
[1] compiler_3.5.2? grid_3.5.2????? lattice_0.20-38

On 11/2/19 9:40 am, Fabio Fanoni wrote:
> Dear all,
>
> I have been using the package nlme for about a year through the 
> software R
>
> Recently, following an update of R (and the corresponding nlme package), in some situations the gls() function does not work.
>
>
> The following lines of code
>
> require(nlme)
> db<-read.csv2("G:\\Dati.csv",as.is=TRUE)
> f<-"DELTA_INVNORM_STD ~ 0+ ITOD + EU_CH"
> gls(as.formula(f),correlation=corARMA(p=2,q=3,fixed=FALSE),data=db,wei
> ghts=varExp(),method="ML")
>
>
> with the old version of R returns the following output:
>
> Generalized least squares fit by maximum likelihood
>   Model: as.formula(f)
>   Data: db
>   Log-likelihood: -36.13853
>
> Coefficients:
>       ITOD      EU_CH
> -0.3138869 -0.5010393
>
> Correlation Structure: ARMA(2,3)
> Formula: ~1
>  Parameter estimate(s):
>        Phi1        Phi2      Theta1      Theta2      Theta3
>  0.50054107 -0.07357577  0.91847373  0.66706344  0.51689285 Variance 
> function:
> Structure: Power of variance covariate
> Formula: ~fitted(.)
>  Parameter estimates:
>       power
> -0.01893321
> Degrees of freedom: 71 total; 69 residual Residual standard error: 
> 0.4077908
>
>
> while with the new one version of R ends with the following error 
> message
>
> Error in `coef<-.corARMA`(`*tmp*`, value = value[parMap[, i]]) :
>   NA/NaN/Inf in foreign function call (arg 1)
>
>
>
> here are the parameters of the old version of R
>
>> sessionInfo()
> R version 3.3.2 (2016-10-31)
> Platform: i386-w64-mingw32/i386 (32-bit) Running under: Windows >= 8 
> x64 (build 9200)
>
> locale:
> [1] LC_COLLATE=Italian_Italy.1252  LC_CTYPE=Italian_Italy.1252    LC_MONETARY=Italian_Italy.1252
> [4] LC_NUMERIC=C                   LC_TIME=Italian_Italy.1252
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] nlme_3.1-128
>
> loaded via a namespace (and not attached):
> [1] tools_3.3.2     grid_3.3.2      lattice_0.20-34
>
>
>
> and the correspondents of the new one
>
>> sessionInfo()
> R version 3.5.2 (2018-12-20)
> Platform: i386-w64-mingw32/i386 (32-bit) Running under: Windows >= 8 
> x64 (build 9200)
>
> Matrix products: default
>
> locale:
> [1] LC_COLLATE=Italian_Italy.1252  LC_CTYPE=Italian_Italy.1252    LC_MONETARY=Italian_Italy.1252
> [4] LC_NUMERIC=C                   LC_TIME=Italian_Italy.1252
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] nlme_3.1-137
>
> loaded via a namespace (and not attached):
> [1] compiler_3.5.2  tools_3.5.2     grid_3.5.2      lattice_0.20-38
>
> The data used are shown below
>
>
> I wonder if someone could help me to solve this problem.
>
> Best regards,
>
> Fabio Fanoni
>
>
> DELTA_INVNORM_STD;ITOD;EU_CH
> -1,01888891391635;1,11744160507933;0,164921569515933
> -1,7287582158078;1,33122211515022;0,192399322297369
> -1,92084919508607;1,42620153798496;0,119394703266418
> -1,07481132504896;1,40851069525448;-0,21451386962765
> 0,368325144516397;1,143582080184;-0,541418306765893
> 1,60232300735139;0,877753890833926;-0,540071359447086
> 1,79762928248747;0,585114281768911;-0,457099318942592
> 0,91995670533225;0,300270733467356;-0,292232806294851
> -0,0262660321428139;0,20146037523289;-0,236469135573377
> -1,02121152912805;0,0419782544708825;-0,345437279939175
> -1,10672650470506;0,0574913198102401;-0,366584376685452
> -1,39178681241528;0,33582796272111;-0,222730255141816
> -1,1454072922613;0,119188584353956;-0,00950828000562807
> -0,958672816825354;-0,0667970836192611;0,4570747351185
> -1,32272038017045;-0,307712992787705;0,958678420940093
> -0,907400017348963;-0,781894397724971;1,3426934842003
> -1,24178782808971;-0,502123817789113;1,78543551198139
> -0,727279530248652;-0,198729005124588;1,65424271463061
> -0,663683090224516;-0,00292614483254617;1,29016249027665
> -0,870662613154294;0,284152373447782;0,852269477692287
> -0,740956260890321;0,212558271927232;0,359286261984589
> -0,875175106507846;0,16921613723138;0,301636857743261
> -0,159691021207123;0,344902273564796;0,406160071512065
> 0,485359221230746;0,405193753056144;0,545299871378597
> 0,91806657022351;0,51772409209882;0,664639525858513
> 0,94182403298868;0,624398008842597;0,721750147939642
> 0,772810225693767;0,669380547864261;0,746938090683181
> 0,454079892235428;0,829860577818214;0,875841077996076
> 0,216198540864624;0,85936757871922;1,06778126286636
> -0,390013295840405;0,820301653758161;1,32356681853436
> -1,08957286663887;0,735286971796754;1,51483352772482
> -1,7181964072478;0,54188741408352;1,59807495122772
> -2,14142427321141;0,3080114833751;1,30188093962788
> -1,73631132303729;0,242276386254216;0,821154955431617
> -1,14832298161027;-0,168559611137128;0,390400772122712
> -0,199657255063557;-0,673990227176314;-0,425580724204736
> 0,637711021315926;-1,26311924357155;-0,777807797606142
> 1,13372564859946;-2,22697608948676;-1,02793616079627
> 1,3387228312938;-2,67326030450369;-1,25476231880355
> 1,13410628583832;-2,41425486942835;-0,769187327492251
> 1,20371911407773;-1,73317141026889;-0,4868668898436
> 1,04245681382429;-0,627677467262121;-0,517038541303494
> 1,19122635858486;0,318264079861633;-0,902669938921763
> 1,10644115835792;0,53399823108225;-1,58826681113873
> 0,657027862693417;0,566924775110004;-2,16826290930735
> 0,391546504792817;0,382793958142103;-2,37380727704873
> -0,121513593301095;0,113508212335312;-2,28814134594747
> 0,090587410027589;-0,10456301293743;-1,92150191643467
> 0,293363258062962;-0,599038773090656;-1,5279235122598
> 0,490646403474033;-1,16804784867814;-1,10646327070376
> 0,663013157660536;-1,68022387383074;-0,281457211581005
> 0,48006697458688;-2,17535440458421;0,0140633243390067
> 0,33992207153128;-2,15751594621565;0,415319331867849
> 0,485131796838891;-1,91164287702336;0,74289724468591
> 0,645057199360022;-1,64072137764755;0,715150100824384
> 0,830719890823037;-1,20839230550334;0,886616664628225
> 1,00060688160202;-0,799350150825283;0,78775063121476
> 0,799516770753564;-0,474799224784852;0,620190215847712
> 0,884527141039385;-0,111321336688718;0,400772282236831
> 0,77592825475446;0,41873781137028;0,214623979323192
> 0,897761905934407;0,618216183184323;-0,371702774880583
> 1,23087946317063;0,86282014652723;-1,04275259746654
> 1,1730433788867;1,03592979384644;-1,50933562067236
> 1,18413315011977;0,843466359597336;-1,89254251877396
> 0,522306442129412;0,786432900675309;-1,199941514842
> -0,423472231576842;0,534354713733984;-0,258289703958632
> -1,02160314495135;0,445296924001273;0,367637345059225
> -1,2194743130469;0,791437804698057;0,831122380946011
> -0,682787491324478;0,735393577499657;0,636488297397258
> -0,277058822726022;0,988210808618591;0,362382895620209
> -0,0283252883031908;1,18581655608866;0,466771415869385
>
>
>
>
> Fabio Fanoni
> SVILUPPO MODELLI DI CREDITO
> Piazza Garibaldi 16
> 23100 Sondrio (SO)
> Banca Popolare di Sondrio
> Tel. +390342528920
> fabio.fanoni at popso.it<mailto:fabio.fanoni at popso.it>
>
>
> AVVERTENZE LEGALI - I contenuti di questo messaggio, proveniente da un indirizzo di posta elettronica aziendale della Banca Popolare di Sondrio, e gli eventuali allegati possono essere letti e utilizzati, per esigenze lavorative, da chi opera alle dipendenze o per conto della stessa, o ? comunque cointeressato nell'inerente relazione d'affari. Le dichiarazioni, ivi contenute, non impegnano contrattualmente la Banca Popolare di Sondrio se non nei limiti di quanto eventualmente previsto in accordi opportunamente formalizzati. Se il messaggio ? stato ricevuto per errore ce ne scusiamo, pregando di segnalare ci? al mittente e poi di distruggerlo senza farne alcun uso poich? l'utilizzo senza averne diritto ? vietato dalla legge e potrebbe costituire reato.
>
> DATI SOCIETARI - BANCA POPOLARE DI SONDRIO - Societ? cooperativa per 
> azioni - Fondata nel 1871 Sede sociale e direzione generale: I - 23100 
> SONDRIO SO - piazza Garibaldi, 16 Indirizzo Internet: 
> http:[doppiabarra]www[punto]popso[punto]it - E-mail: 
> info[chiocciola]popso[punto]it Iscritta al Registro delle Imprese di Sondrio al n. 00053810149, all'Albo delle Banche al n. 842, all'Albo delle Societ? Cooperative al n. A160536 Capogruppo del Gruppo bancario Banca Popolare di Sondrio, iscritto all'Albo dei Gruppi bancari al n. 5696.0 Aderente al Fondo Interbancario di Tutela dei Depositi e al Fondo Nazionale di Garanzia - Codice fiscale e partita IVA: 00053810149 Capitale sociale: EUR 1.360.157.331 - Riserve: EUR 947.325.264 (Dati approvati dall'Assemblea dei soci del 29 aprile 2017).
>
> N.B. I "filtri antivirus" e "antispam" in uso su molti sistemi di posta elettronica possono talvolta ritardare o impedire, in tutto o in parte, il recapito dei messaggi. In tali casi, salvo verifica di avvenuta ricezione, pu? essere necessario modificare i contenuti o le modalit? d'invio.
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From |ou|@ebj @end|ng |rom drcmr@dk  Wed Feb 20 10:01:31 2019
From: |ou|@ebj @end|ng |rom drcmr@dk (=?utf-8?Q?Louise_Baru=C3=ABl_Johansen?=)
Date: Wed, 20 Feb 2019 10:01:31 +0100
Subject: [R-sig-ME] Interaction effects with GAMM
Message-ID: <8282C73C-0475-4012-9EC3-152CACF7E823@drcmr.dk>

I have a question on how to model interaction terms including smooths in a GAMM model (using the mgcv and nlme packages in R).

We have collected longitudinal behavioral and brain imaging data from ~100 subjects across ~6 time points, and I would like to model main effects of age, sex, brain as well as to-way interaction terms (and maybe three-way interaction terms), while correcting for education level and taking random effects into account.

Is using the ti() setup the way to do this:

M = gamm(behav ~ ti(age) + sex + education + ti(age, by = sex) + brain + ti(brain, by = age), random = list(subjectID = ~1+age), data = data)


All help will be appreciated. 

Thanks, Louise

From cr|@@|e@@@ndro @end|ng |rom gm@||@com  Thu Feb 21 19:49:30 2019
From: cr|@@|e@@@ndro @end|ng |rom gm@||@com (Cristiano Alessandro)
Date: Thu, 21 Feb 2019 12:49:30 -0600
Subject: [R-sig-ME] treatment and sum contrasts in ME models
Message-ID: <CAHhX7Wh5sz2xzBuCvth-pT0SxriA54_2G7YT4KM1=n3XE07uhQ@mail.gmail.com>

Hi all,

I am fitting a linear mixed effect models with two factors (mPair with 6
levels, and spd_des with 3 levels) and their interaction in R. I obtain
inconsistent results depending on the contrasts that I choose, and I would
like to understand why and how to deal with it.

If I use treatment contrasts, I obtain the following (I only copy the
relevant info of the results)

> options(contrasts = c("contr.treatment","contr.poly"))
> linM1 <- lme(cc_marg ~ mPair*spd_des , random = ~mPair|ratID, data=dat_trf, na.action=na.omit, method = "ML", control=lCtr )
> summary(linM1)

Fixed effects: cc_marg ~ mPair * spd_des
                         Value  Std.Error DF   t-value p-value
(Intercept)          1.4628761 0.09618167 94 15.209511  0.0000
mPairRFVI           -0.8180718 0.10454920 94 -7.824754  0.0000
mPairVLRF           -0.7990828 0.13193991 94 -6.056415  0.0000
mPairVLVI           -0.6077804 0.13734253 94 -4.425289  0.0000
mPairVMRF           -0.7444267 0.13294167 94 -5.599649  0.0000
mPairVMVI           -0.4799995 0.12194383 94 -3.936234  0.0002
spd_des15           -0.0830016 0.07990370 94 -1.038771  0.3016
spd_des20           -0.0856339 0.08321984 94 -1.029008  0.3061
mPairRFVI:spd_des15 -0.1576193 0.13500809 94 -1.167481  0.2460
mPairVLRF:spd_des15  0.0866510 0.11385875 94  0.761039  0.4485
mPairVLVI:spd_des15  0.0083311 0.13500809 94  0.061708  0.9509
mPairVMRF:spd_des15  0.0184844 0.11385875 94  0.162345  0.8714
mPairVMVI:spd_des15 -0.0672286 0.13500809 94 -0.497960  0.6197
mPairRFVI:spd_des20 -0.1705514 0.14201095 94 -1.200973  0.2328
mPairVLRF:spd_des20  0.0899629 0.11949193 94  0.752879  0.4534
mPairVLVI:spd_des20 -0.0626845 0.14359174 94 -0.436547  0.6634
mPairVMRF:spd_des20 -0.0106400 0.11969131 94 -0.088895  0.9294
mPairVMVI:spd_des20 -0.0608750 0.14286017 94 -0.426116  0.6710

I interpret this as follows: Since the interaction terms are all
non-significant, then the factor spd_des (also non-significant) does not
influence the data at any level of the factor mPair.

On the other hand, using sum contrasts I obtain the following results.

> options(contrasts = c("contr.sum","contr.poly"))
> linM2 <- lme(cc_marg ~ mPair*spd_des , random = ~mPair|ratID, data=dat_trf, na.action=na.omit, method = "ML", control=lCtr )
> summary(linM2)

Fixed effects: cc_marg ~ mPair * spd_des
                     Value  Std.Error DF   t-value p-value
(Intercept)      0.8137433 0.04791890 94 16.981678  0.0000
mPair1           0.5929117 0.06609665 94  8.970373  0.0000
mPair2          -0.3341386 0.04969616 94 -6.723629  0.0000
mPair3          -0.1472874 0.07260892 94 -2.028503  0.0453
mPair4          -0.0328631 0.08993236 94 -0.365421  0.7156
mPair5          -0.1488959 0.06991733 94 -2.129600  0.0358
spd_des1         0.0743293 0.02315254 94  3.210416  0.0018
spd_des2        -0.0272358 0.02325774 94 -1.171043  0.2445
mPair1:spd_des1 -0.0181081 0.04414399 94 -0.410206  0.6826
mPair2:spd_des1  0.0912334 0.05726538 94  1.593168  0.1145
mPair3:spd_des1 -0.0769866 0.04518813 94 -1.703691  0.0917
mPair4:spd_des1  0.0000066 0.05743544 94  0.000114  0.9999
mPair5:spd_des1 -0.0207337 0.04518548 94 -0.458859  0.6474
mPair1:spd_des2  0.0004559 0.04558473 94  0.010002  0.9920
mPair2:spd_des2 -0.0478225 0.05730295 94 -0.834555  0.4061
mPair3:spd_des2  0.0282279 0.04525282 94  0.623781  0.5343
mPair4:spd_des2  0.0269011 0.05747689 94  0.468034  0.6408
mPair5:spd_des2  0.0163141 0.04525367 94  0.360503  0.7193

> anova.lme(linM2,type="marginal")
              numDF denDF   F-value p-value
(Intercept)       1    94 288.37740  <.0001
mPair             5    94  35.30799  <.0001
spd_des           2    94   5.17279  0.0074
mPair:spd_des    10    94   0.52159  0.8710

The results are now telling me that the first level of the factor spd_des
is significant; i.e. the mean of the data at that level of spd_des is
significantly different from the grand mean (Intercept), and since the
interactions are all non-significant, this is true at all levels of mPair.

So, with treatment contrasts spd_des does not influence the data at any
level of mPair, and with sum contrast spd_des influence the data at all
level of mPair. How do I deal with this? What result should I trust?
Thanks in advance for your help

Best
Cristiano

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Thu Feb 21 19:49:53 2019
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Thu, 21 Feb 2019 13:49:53 -0500
Subject: [R-sig-ME] treatment and sum contrasts in ME models
In-Reply-To: <CAHhX7Wh5sz2xzBuCvth-pT0SxriA54_2G7YT4KM1=n3XE07uhQ@mail.gmail.com>
References: <CAHhX7Wh5sz2xzBuCvth-pT0SxriA54_2G7YT4KM1=n3XE07uhQ@mail.gmail.com>
Message-ID: <CABghstRck9kCHFbVUst67+=BtSu8WY_vyLS8sseZYPgr+jB0GQ@mail.gmail.com>

  Not totally necessary, but would it be easy to post your data somewhere?

On Thu, Feb 21, 2019 at 1:44 PM Cristiano Alessandro
<cri.alessandro at gmail.com> wrote:
>
> Hi all,
>
> I am fitting a linear mixed effect models with two factors (mPair with 6
> levels, and spd_des with 3 levels) and their interaction in R. I obtain
> inconsistent results depending on the contrasts that I choose, and I would
> like to understand why and how to deal with it.
>
> If I use treatment contrasts, I obtain the following (I only copy the
> relevant info of the results)
>
> > options(contrasts = c("contr.treatment","contr.poly"))
> > linM1 <- lme(cc_marg ~ mPair*spd_des , random = ~mPair|ratID, data=dat_trf, na.action=na.omit, method = "ML", control=lCtr )
> > summary(linM1)
>
> Fixed effects: cc_marg ~ mPair * spd_des
>                          Value  Std.Error DF   t-value p-value
> (Intercept)          1.4628761 0.09618167 94 15.209511  0.0000
> mPairRFVI           -0.8180718 0.10454920 94 -7.824754  0.0000
> mPairVLRF           -0.7990828 0.13193991 94 -6.056415  0.0000
> mPairVLVI           -0.6077804 0.13734253 94 -4.425289  0.0000
> mPairVMRF           -0.7444267 0.13294167 94 -5.599649  0.0000
> mPairVMVI           -0.4799995 0.12194383 94 -3.936234  0.0002
> spd_des15           -0.0830016 0.07990370 94 -1.038771  0.3016
> spd_des20           -0.0856339 0.08321984 94 -1.029008  0.3061
> mPairRFVI:spd_des15 -0.1576193 0.13500809 94 -1.167481  0.2460
> mPairVLRF:spd_des15  0.0866510 0.11385875 94  0.761039  0.4485
> mPairVLVI:spd_des15  0.0083311 0.13500809 94  0.061708  0.9509
> mPairVMRF:spd_des15  0.0184844 0.11385875 94  0.162345  0.8714
> mPairVMVI:spd_des15 -0.0672286 0.13500809 94 -0.497960  0.6197
> mPairRFVI:spd_des20 -0.1705514 0.14201095 94 -1.200973  0.2328
> mPairVLRF:spd_des20  0.0899629 0.11949193 94  0.752879  0.4534
> mPairVLVI:spd_des20 -0.0626845 0.14359174 94 -0.436547  0.6634
> mPairVMRF:spd_des20 -0.0106400 0.11969131 94 -0.088895  0.9294
> mPairVMVI:spd_des20 -0.0608750 0.14286017 94 -0.426116  0.6710
>
> I interpret this as follows: Since the interaction terms are all
> non-significant, then the factor spd_des (also non-significant) does not
> influence the data at any level of the factor mPair.
>
> On the other hand, using sum contrasts I obtain the following results.
>
> > options(contrasts = c("contr.sum","contr.poly"))
> > linM2 <- lme(cc_marg ~ mPair*spd_des , random = ~mPair|ratID, data=dat_trf, na.action=na.omit, method = "ML", control=lCtr )
> > summary(linM2)
>
> Fixed effects: cc_marg ~ mPair * spd_des
>                      Value  Std.Error DF   t-value p-value
> (Intercept)      0.8137433 0.04791890 94 16.981678  0.0000
> mPair1           0.5929117 0.06609665 94  8.970373  0.0000
> mPair2          -0.3341386 0.04969616 94 -6.723629  0.0000
> mPair3          -0.1472874 0.07260892 94 -2.028503  0.0453
> mPair4          -0.0328631 0.08993236 94 -0.365421  0.7156
> mPair5          -0.1488959 0.06991733 94 -2.129600  0.0358
> spd_des1         0.0743293 0.02315254 94  3.210416  0.0018
> spd_des2        -0.0272358 0.02325774 94 -1.171043  0.2445
> mPair1:spd_des1 -0.0181081 0.04414399 94 -0.410206  0.6826
> mPair2:spd_des1  0.0912334 0.05726538 94  1.593168  0.1145
> mPair3:spd_des1 -0.0769866 0.04518813 94 -1.703691  0.0917
> mPair4:spd_des1  0.0000066 0.05743544 94  0.000114  0.9999
> mPair5:spd_des1 -0.0207337 0.04518548 94 -0.458859  0.6474
> mPair1:spd_des2  0.0004559 0.04558473 94  0.010002  0.9920
> mPair2:spd_des2 -0.0478225 0.05730295 94 -0.834555  0.4061
> mPair3:spd_des2  0.0282279 0.04525282 94  0.623781  0.5343
> mPair4:spd_des2  0.0269011 0.05747689 94  0.468034  0.6408
> mPair5:spd_des2  0.0163141 0.04525367 94  0.360503  0.7193
>
> > anova.lme(linM2,type="marginal")
>               numDF denDF   F-value p-value
> (Intercept)       1    94 288.37740  <.0001
> mPair             5    94  35.30799  <.0001
> spd_des           2    94   5.17279  0.0074
> mPair:spd_des    10    94   0.52159  0.8710
>
> The results are now telling me that the first level of the factor spd_des
> is significant; i.e. the mean of the data at that level of spd_des is
> significantly different from the grand mean (Intercept), and since the
> interactions are all non-significant, this is true at all levels of mPair.
>
> So, with treatment contrasts spd_des does not influence the data at any
> level of mPair, and with sum contrast spd_des influence the data at all
> level of mPair. How do I deal with this? What result should I trust?
> Thanks in advance for your help
>
> Best
> Cristiano
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From cr|@@|e@@@ndro @end|ng |rom gm@||@com  Thu Feb 21 19:57:55 2019
From: cr|@@|e@@@ndro @end|ng |rom gm@||@com (Cristiano Alessandro)
Date: Thu, 21 Feb 2019 12:57:55 -0600
Subject: [R-sig-ME] treatment and sum contrasts in ME models
In-Reply-To: <CABghstRck9kCHFbVUst67+=BtSu8WY_vyLS8sseZYPgr+jB0GQ@mail.gmail.com>
References: <CAHhX7Wh5sz2xzBuCvth-pT0SxriA54_2G7YT4KM1=n3XE07uhQ@mail.gmail.com>
 <CABghstRck9kCHFbVUst67+=BtSu8WY_vyLS8sseZYPgr+jB0GQ@mail.gmail.com>
Message-ID: <CAHhX7WhrieoYq9R=_U39LRjv-ET1cxoZpm4UUOjRA_UQqg+9Ag@mail.gmail.com>

Unfortunately, it is a bit hard for me to do so at this point. I would have
done it otherwise, as I did in the past. Sorry.
Can a guess be made also without the dataset?

On Thu, Feb 21, 2019 at 12:50 PM Ben Bolker <bbolker at gmail.com> wrote:

>   Not totally necessary, but would it be easy to post your data somewhere?
>
> On Thu, Feb 21, 2019 at 1:44 PM Cristiano Alessandro
> <cri.alessandro at gmail.com> wrote:
> >
> > Hi all,
> >
> > I am fitting a linear mixed effect models with two factors (mPair with 6
> > levels, and spd_des with 3 levels) and their interaction in R. I obtain
> > inconsistent results depending on the contrasts that I choose, and I
> would
> > like to understand why and how to deal with it.
> >
> > If I use treatment contrasts, I obtain the following (I only copy the
> > relevant info of the results)
> >
> > > options(contrasts = c("contr.treatment","contr.poly"))
> > > linM1 <- lme(cc_marg ~ mPair*spd_des , random = ~mPair|ratID,
> data=dat_trf, na.action=na.omit, method = "ML", control=lCtr )
> > > summary(linM1)
> >
> > Fixed effects: cc_marg ~ mPair * spd_des
> >                          Value  Std.Error DF   t-value p-value
> > (Intercept)          1.4628761 0.09618167 94 15.209511  0.0000
> > mPairRFVI           -0.8180718 0.10454920 94 -7.824754  0.0000
> > mPairVLRF           -0.7990828 0.13193991 94 -6.056415  0.0000
> > mPairVLVI           -0.6077804 0.13734253 94 -4.425289  0.0000
> > mPairVMRF           -0.7444267 0.13294167 94 -5.599649  0.0000
> > mPairVMVI           -0.4799995 0.12194383 94 -3.936234  0.0002
> > spd_des15           -0.0830016 0.07990370 94 -1.038771  0.3016
> > spd_des20           -0.0856339 0.08321984 94 -1.029008  0.3061
> > mPairRFVI:spd_des15 -0.1576193 0.13500809 94 -1.167481  0.2460
> > mPairVLRF:spd_des15  0.0866510 0.11385875 94  0.761039  0.4485
> > mPairVLVI:spd_des15  0.0083311 0.13500809 94  0.061708  0.9509
> > mPairVMRF:spd_des15  0.0184844 0.11385875 94  0.162345  0.8714
> > mPairVMVI:spd_des15 -0.0672286 0.13500809 94 -0.497960  0.6197
> > mPairRFVI:spd_des20 -0.1705514 0.14201095 94 -1.200973  0.2328
> > mPairVLRF:spd_des20  0.0899629 0.11949193 94  0.752879  0.4534
> > mPairVLVI:spd_des20 -0.0626845 0.14359174 94 -0.436547  0.6634
> > mPairVMRF:spd_des20 -0.0106400 0.11969131 94 -0.088895  0.9294
> > mPairVMVI:spd_des20 -0.0608750 0.14286017 94 -0.426116  0.6710
> >
> > I interpret this as follows: Since the interaction terms are all
> > non-significant, then the factor spd_des (also non-significant) does not
> > influence the data at any level of the factor mPair.
> >
> > On the other hand, using sum contrasts I obtain the following results.
> >
> > > options(contrasts = c("contr.sum","contr.poly"))
> > > linM2 <- lme(cc_marg ~ mPair*spd_des , random = ~mPair|ratID,
> data=dat_trf, na.action=na.omit, method = "ML", control=lCtr )
> > > summary(linM2)
> >
> > Fixed effects: cc_marg ~ mPair * spd_des
> >                      Value  Std.Error DF   t-value p-value
> > (Intercept)      0.8137433 0.04791890 94 16.981678  0.0000
> > mPair1           0.5929117 0.06609665 94  8.970373  0.0000
> > mPair2          -0.3341386 0.04969616 94 -6.723629  0.0000
> > mPair3          -0.1472874 0.07260892 94 -2.028503  0.0453
> > mPair4          -0.0328631 0.08993236 94 -0.365421  0.7156
> > mPair5          -0.1488959 0.06991733 94 -2.129600  0.0358
> > spd_des1         0.0743293 0.02315254 94  3.210416  0.0018
> > spd_des2        -0.0272358 0.02325774 94 -1.171043  0.2445
> > mPair1:spd_des1 -0.0181081 0.04414399 94 -0.410206  0.6826
> > mPair2:spd_des1  0.0912334 0.05726538 94  1.593168  0.1145
> > mPair3:spd_des1 -0.0769866 0.04518813 94 -1.703691  0.0917
> > mPair4:spd_des1  0.0000066 0.05743544 94  0.000114  0.9999
> > mPair5:spd_des1 -0.0207337 0.04518548 94 -0.458859  0.6474
> > mPair1:spd_des2  0.0004559 0.04558473 94  0.010002  0.9920
> > mPair2:spd_des2 -0.0478225 0.05730295 94 -0.834555  0.4061
> > mPair3:spd_des2  0.0282279 0.04525282 94  0.623781  0.5343
> > mPair4:spd_des2  0.0269011 0.05747689 94  0.468034  0.6408
> > mPair5:spd_des2  0.0163141 0.04525367 94  0.360503  0.7193
> >
> > > anova.lme(linM2,type="marginal")
> >               numDF denDF   F-value p-value
> > (Intercept)       1    94 288.37740  <.0001
> > mPair             5    94  35.30799  <.0001
> > spd_des           2    94   5.17279  0.0074
> > mPair:spd_des    10    94   0.52159  0.8710
> >
> > The results are now telling me that the first level of the factor spd_des
> > is significant; i.e. the mean of the data at that level of spd_des is
> > significantly different from the grand mean (Intercept), and since the
> > interactions are all non-significant, this is true at all levels of
> mPair.
> >
> > So, with treatment contrasts spd_des does not influence the data at any
> > level of mPair, and with sum contrast spd_des influence the data at all
> > level of mPair. How do I deal with this? What result should I trust?
> > Thanks in advance for your help
> >
> > Best
> > Cristiano
> >
> >         [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From ph||||p@@|d@y @end|ng |rom mp|@n|  Fri Feb 22 11:29:39 2019
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Fri, 22 Feb 2019 11:29:39 +0100
Subject: [R-sig-ME] Interaction effects with GAMM
In-Reply-To: <8282C73C-0475-4012-9EC3-152CACF7E823@drcmr.dk>
References: <8282C73C-0475-4012-9EC3-152CACF7E823@drcmr.dk>
Message-ID: <36e467b0-eae1-5afc-281a-59630dd278e3@mpi.nl>

Hi Louise,

I'm somewhat curious what brain imaging data you have that can be so
neatly summed up as a single univariate value. While you can do e.g. the
EEG voltage at a given timepoint in a given channel or the BOLD signal
in a given voxel or some overall structural score derived from DTI,
these are generally very poor indices of the structural and activity
variation within and between brains. I ask because knowing more about
your data helps when giving advice about a model. I'm guessing behavior
is something like RT or maybe d-prime/sensitivity index and *not* simple
accuracy, where a Gaussian model would not be appropriate.

All that said, I do already have one comment/question ...

Your data are longitudinal, but how much so? What's the range in age
within subjects vs. between subjects? If the range within subjects is
just a few months to a year or two and the range between subjects is
several years, as is common in many studies, then having a by-subject
slope for age doesn't really make much sense. The overall by-subjects
variation (the intercept, i.e. ~1) and residual variation will probably
dominate.

And some general advice: use the various plotting functions (plot(),
vis.gam()), etc. to get an idea about what your model "thinks" the world
looks like and whether that matches your own expectations and
matches/fits the picture presented by the data.

Best,
Phillip


On 20/2/19 10:01 am, Louise Baru?l Johansen wrote:
> I have a question on how to model interaction terms including smooths in a GAMM model (using the mgcv and nlme packages in R).
> 
> We have collected longitudinal behavioral and brain imaging data from ~100 subjects across ~6 time points, and I would like to model main effects of age, sex, brain as well as to-way interaction terms (and maybe three-way interaction terms), while correcting for education level and taking random effects into account.
> 
> Is using the ti() setup the way to do this:
> 
> M = gamm(behav ~ ti(age) + sex + education + ti(age, by = sex) + brain + ti(brain, by = age), random = list(subjectID = ~1+age), data = data)
> 
> 
> All help will be appreciated. 
> 
> Thanks, Louise
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From |ou|@ebj @end|ng |rom drcmr@dk  Tue Feb 26 13:57:58 2019
From: |ou|@ebj @end|ng |rom drcmr@dk (=?utf-8?Q?Louise_Baru=C3=ABl_Johansen?=)
Date: Tue, 26 Feb 2019 13:57:58 +0100
Subject: [R-sig-ME] Interaction effects with GAMM
In-Reply-To: <36e467b0-eae1-5afc-281a-59630dd278e3@mpi.nl>
References: <8282C73C-0475-4012-9EC3-152CACF7E823@drcmr.dk>
 <36e467b0-eae1-5afc-281a-59630dd278e3@mpi.nl>
Message-ID: <7C5C755C-1B17-4829-9DB4-1D192ADB5648@drcmr.dk>

Dear Phillip,

Thank you for taking your time to look at my question.

Our data consists of up to 12 MRI scans per subject with interscan-intervals of 6 months, and the subjects were between the age of 7-13 years at baseline, which gives us a reasonable overlap between subjects. The brain data is extracted from regions of interest, and the behavioural data could be RT.

My question was more regarding how to incorporate the interaction effects in the most appropriate way statistically; by using te(), ti(), or in a completely different way?

All the best,

Louise

> On 22 Feb 2019, at 11.29, Phillip Alday <phillip.alday at mpi.nl> wrote:
> 
> Hi Louise,
> 
> I'm somewhat curious what brain imaging data you have that can be so
> neatly summed up as a single univariate value. While you can do e.g. the
> EEG voltage at a given timepoint in a given channel or the BOLD signal
> in a given voxel or some overall structural score derived from DTI,
> these are generally very poor indices of the structural and activity
> variation within and between brains. I ask because knowing more about
> your data helps when giving advice about a model. I'm guessing behavior
> is something like RT or maybe d-prime/sensitivity index and *not* simple
> accuracy, where a Gaussian model would not be appropriate.
> 
> All that said, I do already have one comment/question ...
> 
> Your data are longitudinal, but how much so? What's the range in age
> within subjects vs. between subjects? If the range within subjects is
> just a few months to a year or two and the range between subjects is
> several years, as is common in many studies, then having a by-subject
> slope for age doesn't really make much sense. The overall by-subjects
> variation (the intercept, i.e. ~1) and residual variation will probably
> dominate.
> 
> And some general advice: use the various plotting functions (plot(),
> vis.gam()), etc. to get an idea about what your model "thinks" the world
> looks like and whether that matches your own expectations and
> matches/fits the picture presented by the data.
> 
> Best,
> Phillip
> 
> 
> On 20/2/19 10:01 am, Louise Baru?l Johansen wrote:
>> I have a question on how to model interaction terms including smooths in a GAMM model (using the mgcv and nlme packages in R).
>> 
>> We have collected longitudinal behavioral and brain imaging data from ~100 subjects across ~6 time points, and I would like to model main effects of age, sex, brain as well as to-way interaction terms (and maybe three-way interaction terms), while correcting for education level and taking random effects into account.
>> 
>> Is using the ti() setup the way to do this:
>> 
>> M = gamm(behav ~ ti(age) + sex + education + ti(age, by = sex) + brain + ti(brain, by = age), random = list(subjectID = ~1+age), data = data)
>> 
>> 
>> All help will be appreciated. 
>> 
>> Thanks, Louise
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> 


From bbo|ker @end|ng |rom gm@||@com  Wed Feb 27 00:01:19 2019
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Tue, 26 Feb 2019 18:01:19 -0500
Subject: [R-sig-ME] lme4 1.1-21 going to CRAN soon
Message-ID: <f3b79c3b-e1fa-4311-b455-213993412c5a@gmail.com>


  The CRAN maintainers have asked for another lme4 update to fix some
testing issues. We've also made the following few changes

  USER-VISIBLE CHANGES:

   ? ?bootMer? now traps and stores messages, warnings, and errors
   ? ?bootMer? returns an object of class ?c("bootMer","boot")?; new
?print? and ?confint? methods for class ?bootMer?
   ? small changes to wording of singular-fit messages

---

We don't anticipate any downstream problems (we have run
reverse-dependency checks -- but we have missed problems before!), but
would be happy to hear about any problems, or relatively straightforward
bug fixes/feature requests that we should try to squeeze into the next
CRAN release (CRAN rules policies discourage us from submitting new
releases more frequently than every 1-2 months).

 [In case you're thinking of asking, "implement flexible correlation and
heteroscedasticity structures at the level of residuals and groups" is
not relatively straightforward :-( ]

  cheers
    Ben Bolker


From |@b|o@|@non| @end|ng |rom pop@o@|t  Tue Feb 26 09:12:50 2019
From: |@b|o@|@non| @end|ng |rom pop@o@|t (Fabio Fanoni)
Date: Tue, 26 Feb 2019 08:12:50 +0000
Subject: [R-sig-ME] R: R:  gls() funcion after R upgrade
In-Reply-To: <eb21ed55-8e07-b457-6f9b-c1cfa3ff3b0f@mpi.nl>
References: <d0cd3fe9653e44dcbdb782bb8af4094a@popso.it>
 <a1ff82fe-4f87-c923-e111-f73e9b31a4d6@mpi.nl>
 <5640809b8d8849edb929ab293773809b@popso.it>
 <eb21ed55-8e07-b457-6f9b-c1cfa3ff3b0f@mpi.nl>
Message-ID: <9d887a8a6c404bb4a52d9d921af1ffdf@popso.it>

On the web page

https://cran.r-project.org/web/packages/nlme/index.html

I found the following link

https://bugs.r-project.org

that re-directs me to:

https://bugs.r-project.org/bugzilla/

(Bugzilla - Main Page)

To enter a new bug report you must have an account (email address and password).

Unfortunately, I can't find, on this website, the link for creating a new account.
In the homepage header, either clicking on "New" or clicking on "Log In" I have to insert Email Address and Password

Would you have an idea on how to create an account?


Fabio Fanoni
SVILUPPO MODELLI DI CREDITO
Piazza Garibaldi 16
23100 Sondrio (SO)
Banca Popolare di Sondrio

Tel. +390342528920?| Mobile. ?| Fax. 
fabio.fanoni at popso.it

-----Messaggio originale-----
Da: Phillip Alday <phillip.alday at mpi.nl> 
Inviato: marted? 19 febbraio 2019 17:43
A: Fabio Fanoni <fabio.fanoni at popso.it>; r-sig-mixed-models at r-project.org
Oggetto: Re: R: [R-sig-ME] gls() funcion after R upgrade

This seems like a bug / compiler issue on 32-bit Windows. I don't have a convenient Windows machine to test it out myself, but you should report it (along with your minimal example):

https://cran.r-project.org/web/packages/nlme/index.html

Also: for many reasons (performance, ability to work with larger datasets, etc.), if you have access to the 64-bit versions, you should be using them!

Phillip

On 19/2/19 5:02 pm, Fabio Fanoni wrote:
> Dear Phillip,
> 
> thanks for the reply
> 
> I tried both to re-install the package nlme (and all its dependencies) 
> and to re-install R as a whole but I always get the same error message
> 
> The problem seems to be related to the varExp() variance function 
> structures on a 32-bit windows platform With 64-bit windows platform, 
> the old version 3.3.2 and the new version 3.5.2 produce the same 
> results At the same time, the varPower() function always produces the 
> same results regardless of the windows platform or version of R The 
> following table shows the coefficients obtained according to the 
> platform (32 bit / 62 bit), version of R and variance function
> 
> Platform	Version	weights	   	ITOD	   	EU_CH
> windows 32 bit	3.3.2	varExp()	-0.1053291	-0.4406068
> windows 32 bit	3.5.2	varExp()	<NA>	         <NA>
> windows 64 bit	3.3.2	varExp()	-0.3011427	-0.5537158
> windows 64 bit	3.5.2	varExp()	-0.3011427	-0.5537158
> windows 32 bit	3.3.2	varPower()	-0.3138869	-0.5010393
> windows 32 bit	3.5.2	varPower()	-0.3138869	-0.5010393
> windows 64 bit	3.3.2	varPower()	-0.3138868	-0.5010394
> windows 64 bit	3.5.2	varPower()	-0.3138868	-0.5010394
> 				
> linux 64 bit	3.5.2	varExp()	-0.3123153 	-0.4979789
> 
> These values differ from what I reported in the previous e-mail. For a ?copy and paste? mistake, in the previous e-mail I reported the results obtained with the varPower() function instead of varExp() function. 
> 
> Here is the correct output
> 
>> gls(as.formula(f),correlation=corARMA(p=2,q=3,fixed=FALSE),data=db,we
>> ights=varExp(),method="ML")
> Generalized least squares fit by maximum likelihood
>   Model: as.formula(f) 
>   Data: db 
>   Log-likelihood: 12.11128
> 
> Coefficients:
>       ITOD      EU_CH 
> -0.1053291 -0.4406068
> 
> Correlation Structure: ARMA(2,3)
>  Formula: ~1
>  Parameter estimate(s):
>       Phi1       Phi2     Theta1     Theta2     Theta3 
>  0.6485554 -0.1750409  0.9450232  0.9566344  0.9566288 Variance 
> function:
>  Structure: Exponential of variance covariate
>  Formula: ~fitted(.)
>  Parameter estimates:
>         expon
> -0.0007047246
> Degrees of freedom: 71 total; 69 residual Residual standard error: 
> 0.7740528
> 
> On the 3.5.2 version of R, the sessionInfo() show "Matrix products: default" whereas in the 3.3.2 version this message does not appear. 
> 
> If I set options(matprod="internal") on the 3.5.2 I avoid the error 
> but the results I get are very different from those of version 3.3.2
> 
> If I use the REML method instead of the ML I get much more similar 
> results but, at the same time, very different from those I would get 
> with the same method on the version 3.2.2
> 
> Maybe these clues can inspire a solution?
> 
> 
> Fabio
> 
> 
> 
> 
> 
> -----Messaggio originale-----
> Da: Alday, Phillip <Phillip.Alday at mpi.nl>
> Inviato: marted? 19 febbraio 2019 00:44
> A: Fabio Fanoni <fabio.fanoni at popso.it>; 
> r-sig-mixed-models at r-project.org
> Oggetto: Re: [R-sig-ME] gls() funcion after R upgrade
> 
> Dear Fabio,
> 
> Your example runs fine on my machine, and I have a comparable R and nlme version. Have you tried re-installing nlme and all its dependencies?
> 
> install.packages("nlme",dependencies=TRUE)
> 
> Sometimes dependencies can get out of sync and this can lead to weird 
> errors. (I've had this happen when lme4 was compiled against a 
> different Rcpp version than Matrix.)
> 
> As a sidebar, I get different estimates than your old output had:
> 
> 
>>
> gls(as.formula(f),correlation=corARMA(p=2,q=3,fixed=FALSE),data=db,wei
> ghts=varExp(),method="ML") Generalized least squares fit by maximum 
> likelihood
> ? Model: as.formula(f)
> ? Data: db
> ? Log-likelihood: 11.74371
> 
> Coefficients:
> ????? ITOD????? EU_CH
> -0.3123153 -0.4979789
> 
> Correlation Structure: ARMA(2,3)
> ?Formula: ~1
> ?Parameter estimate(s):
> ????? Phi1?????? Phi2???? Theta1???? Theta2???? Theta3
> ?0.7542011 -0.2467866? 0.8989841? 0.9444138? 0.9092150 Variance function:
> ?Structure: Exponential of variance covariate
> ?Formula: ~fitted(.)
> ?Parameter estimates:
> ????? expon
> -0.01497128
> Degrees of freedom: 71 total; 69 residual Residual standard error: 
> 0.817064
> 
> (Note that "power" is now called "expon" for "exponent".) The small differences in the exponent and the coefficients could be numerical issues, but the big differences in ARMA seem a bit worrying. The log-likelihood of 'my' fit is better, but the residual standard error is worse.
> 
> Best,
> 
> Phillip
> 
>> sessionInfo()
> R version 3.5.2 (2018-12-20)
> Platform: x86_64-pc-linux-gnu (64-bit) Running under: Linux Mint 19.1
> 
> Matrix products: default
> BLAS: /usr/lib/x86_64-linux-gnu/openblas/libblas.so.3
> LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.2.20.so
> 
> ::snip::
> 
> 
> attached base packages:
> [1] stats???? graphics? grDevices utils???? datasets? methods?? base
> 
> other attached packages:
> [1] nlme_3.1-137
> 
> loaded via a namespace (and not attached):
> [1] compiler_3.5.2? grid_3.5.2????? lattice_0.20-38
> 
> On 11/2/19 9:40 am, Fabio Fanoni wrote:
>> Dear all,
>>
>> I have been using the package nlme for about a year through the 
>> software R
>>
>> Recently, following an update of R (and the corresponding nlme package), in some situations the gls() function does not work.
>>
>>
>> The following lines of code
>>
>> require(nlme)
>> db<-read.csv2("G:\\Dati.csv",as.is=TRUE)
>> f<-"DELTA_INVNORM_STD ~ 0+ ITOD + EU_CH"
>> gls(as.formula(f),correlation=corARMA(p=2,q=3,fixed=FALSE),data=db,we
>> i
>> ghts=varExp(),method="ML")
>>
>>
>> with the old version of R returns the following output:
>>
>> Generalized least squares fit by maximum likelihood
>>   Model: as.formula(f)
>>   Data: db
>>   Log-likelihood: -36.13853
>>
>> Coefficients:
>>       ITOD      EU_CH
>> -0.3138869 -0.5010393
>>
>> Correlation Structure: ARMA(2,3)
>> Formula: ~1
>>  Parameter estimate(s):
>>        Phi1        Phi2      Theta1      Theta2      Theta3
>>  0.50054107 -0.07357577  0.91847373  0.66706344  0.51689285 Variance
>> function:
>> Structure: Power of variance covariate
>> Formula: ~fitted(.)
>>  Parameter estimates:
>>       power
>> -0.01893321
>> Degrees of freedom: 71 total; 69 residual Residual standard error: 
>> 0.4077908
>>
>>
>> while with the new one version of R ends with the following error 
>> message
>>
>> Error in `coef<-.corARMA`(`*tmp*`, value = value[parMap[, i]]) :
>>   NA/NaN/Inf in foreign function call (arg 1)
>>
>>
>>
>> here are the parameters of the old version of R
>>
>>> sessionInfo()
>> R version 3.3.2 (2016-10-31)
>> Platform: i386-w64-mingw32/i386 (32-bit) Running under: Windows >= 8
>> x64 (build 9200)
>>
>> locale:
>> [1] LC_COLLATE=Italian_Italy.1252  LC_CTYPE=Italian_Italy.1252    LC_MONETARY=Italian_Italy.1252
>> [4] LC_NUMERIC=C                   LC_TIME=Italian_Italy.1252
>>
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>> other attached packages:
>> [1] nlme_3.1-128
>>
>> loaded via a namespace (and not attached):
>> [1] tools_3.3.2     grid_3.3.2      lattice_0.20-34
>>
>>
>>
>> and the correspondents of the new one
>>
>>> sessionInfo()
>> R version 3.5.2 (2018-12-20)
>> Platform: i386-w64-mingw32/i386 (32-bit) Running under: Windows >= 8
>> x64 (build 9200)
>>
>> Matrix products: default
>>
>> locale:
>> [1] LC_COLLATE=Italian_Italy.1252  LC_CTYPE=Italian_Italy.1252    LC_MONETARY=Italian_Italy.1252
>> [4] LC_NUMERIC=C                   LC_TIME=Italian_Italy.1252
>>
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>> other attached packages:
>> [1] nlme_3.1-137
>>
>> loaded via a namespace (and not attached):
>> [1] compiler_3.5.2  tools_3.5.2     grid_3.5.2      lattice_0.20-38
>>
>> The data used are shown below
>>
>>
>> I wonder if someone could help me to solve this problem.
>>
>> Best regards,
>>
>> Fabio Fanoni
>>
>>
>> DELTA_INVNORM_STD;ITOD;EU_CH
>> -1,01888891391635;1,11744160507933;0,164921569515933
>> -1,7287582158078;1,33122211515022;0,192399322297369
>> -1,92084919508607;1,42620153798496;0,119394703266418
>> -1,07481132504896;1,40851069525448;-0,21451386962765
>> 0,368325144516397;1,143582080184;-0,541418306765893
>> 1,60232300735139;0,877753890833926;-0,540071359447086
>> 1,79762928248747;0,585114281768911;-0,457099318942592
>> 0,91995670533225;0,300270733467356;-0,292232806294851
>> -0,0262660321428139;0,20146037523289;-0,236469135573377
>> -1,02121152912805;0,0419782544708825;-0,345437279939175
>> -1,10672650470506;0,0574913198102401;-0,366584376685452
>> -1,39178681241528;0,33582796272111;-0,222730255141816
>> -1,1454072922613;0,119188584353956;-0,00950828000562807
>> -0,958672816825354;-0,0667970836192611;0,4570747351185
>> -1,32272038017045;-0,307712992787705;0,958678420940093
>> -0,907400017348963;-0,781894397724971;1,3426934842003
>> -1,24178782808971;-0,502123817789113;1,78543551198139
>> -0,727279530248652;-0,198729005124588;1,65424271463061
>> -0,663683090224516;-0,00292614483254617;1,29016249027665
>> -0,870662613154294;0,284152373447782;0,852269477692287
>> -0,740956260890321;0,212558271927232;0,359286261984589
>> -0,875175106507846;0,16921613723138;0,301636857743261
>> -0,159691021207123;0,344902273564796;0,406160071512065
>> 0,485359221230746;0,405193753056144;0,545299871378597
>> 0,91806657022351;0,51772409209882;0,664639525858513
>> 0,94182403298868;0,624398008842597;0,721750147939642
>> 0,772810225693767;0,669380547864261;0,746938090683181
>> 0,454079892235428;0,829860577818214;0,875841077996076
>> 0,216198540864624;0,85936757871922;1,06778126286636
>> -0,390013295840405;0,820301653758161;1,32356681853436
>> -1,08957286663887;0,735286971796754;1,51483352772482
>> -1,7181964072478;0,54188741408352;1,59807495122772
>> -2,14142427321141;0,3080114833751;1,30188093962788
>> -1,73631132303729;0,242276386254216;0,821154955431617
>> -1,14832298161027;-0,168559611137128;0,390400772122712
>> -0,199657255063557;-0,673990227176314;-0,425580724204736
>> 0,637711021315926;-1,26311924357155;-0,777807797606142
>> 1,13372564859946;-2,22697608948676;-1,02793616079627
>> 1,3387228312938;-2,67326030450369;-1,25476231880355
>> 1,13410628583832;-2,41425486942835;-0,769187327492251
>> 1,20371911407773;-1,73317141026889;-0,4868668898436
>> 1,04245681382429;-0,627677467262121;-0,517038541303494
>> 1,19122635858486;0,318264079861633;-0,902669938921763
>> 1,10644115835792;0,53399823108225;-1,58826681113873
>> 0,657027862693417;0,566924775110004;-2,16826290930735
>> 0,391546504792817;0,382793958142103;-2,37380727704873
>> -0,121513593301095;0,113508212335312;-2,28814134594747
>> 0,090587410027589;-0,10456301293743;-1,92150191643467
>> 0,293363258062962;-0,599038773090656;-1,5279235122598
>> 0,490646403474033;-1,16804784867814;-1,10646327070376
>> 0,663013157660536;-1,68022387383074;-0,281457211581005
>> 0,48006697458688;-2,17535440458421;0,0140633243390067
>> 0,33992207153128;-2,15751594621565;0,415319331867849
>> 0,485131796838891;-1,91164287702336;0,74289724468591
>> 0,645057199360022;-1,64072137764755;0,715150100824384
>> 0,830719890823037;-1,20839230550334;0,886616664628225
>> 1,00060688160202;-0,799350150825283;0,78775063121476
>> 0,799516770753564;-0,474799224784852;0,620190215847712
>> 0,884527141039385;-0,111321336688718;0,400772282236831
>> 0,77592825475446;0,41873781137028;0,214623979323192
>> 0,897761905934407;0,618216183184323;-0,371702774880583
>> 1,23087946317063;0,86282014652723;-1,04275259746654
>> 1,1730433788867;1,03592979384644;-1,50933562067236
>> 1,18413315011977;0,843466359597336;-1,89254251877396
>> 0,522306442129412;0,786432900675309;-1,199941514842
>> -0,423472231576842;0,534354713733984;-0,258289703958632
>> -1,02160314495135;0,445296924001273;0,367637345059225
>> -1,2194743130469;0,791437804698057;0,831122380946011
>> -0,682787491324478;0,735393577499657;0,636488297397258
>> -0,277058822726022;0,988210808618591;0,362382895620209
>> -0,0283252883031908;1,18581655608866;0,466771415869385
>>
>>
>>
>>
>> Fabio Fanoni
>> SVILUPPO MODELLI DI CREDITO
>> Piazza Garibaldi 16
>> 23100 Sondrio (SO)
>> Banca Popolare di Sondrio
>> Tel. +390342528920
>> fabio.fanoni at popso.it<mailto:fabio.fanoni at popso.it>
>>
>>
>> AVVERTENZE LEGALI - I contenuti di questo messaggio, proveniente da un indirizzo di posta elettronica aziendale della Banca Popolare di Sondrio, e gli eventuali allegati possono essere letti e utilizzati, per esigenze lavorative, da chi opera alle dipendenze o per conto della stessa, o ? comunque cointeressato nell'inerente relazione d'affari. Le dichiarazioni, ivi contenute, non impegnano contrattualmente la Banca Popolare di Sondrio se non nei limiti di quanto eventualmente previsto in accordi opportunamente formalizzati. Se il messaggio ? stato ricevuto per errore ce ne scusiamo, pregando di segnalare ci? al mittente e poi di distruggerlo senza farne alcun uso poich? l'utilizzo senza averne diritto ? vietato dalla legge e potrebbe costituire reato.
>>
>> DATI SOCIETARI - BANCA POPOLARE DI SONDRIO - Societ? cooperativa per 
>> azioni - Fondata nel 1871 Sede sociale e direzione generale: I - 
>> 23100 SONDRIO SO - piazza Garibaldi, 16 Indirizzo Internet:
>> http:[doppiabarra]www[punto]popso[punto]it - E-mail: 
>> info[chiocciola]popso[punto]it Iscritta al Registro delle Imprese di Sondrio al n. 00053810149, all'Albo delle Banche al n. 842, all'Albo delle Societ? Cooperative al n. A160536 Capogruppo del Gruppo bancario Banca Popolare di Sondrio, iscritto all'Albo dei Gruppi bancari al n. 5696.0 Aderente al Fondo Interbancario di Tutela dei Depositi e al Fondo Nazionale di Garanzia - Codice fiscale e partita IVA: 00053810149 Capitale sociale: EUR 1.360.157.331 - Riserve: EUR 947.325.264 (Dati approvati dall'Assemblea dei soci del 29 aprile 2017).
>>
>> N.B. I "filtri antivirus" e "antispam" in uso su molti sistemi di posta elettronica possono talvolta ritardare o impedire, in tutto o in parte, il recapito dei messaggi. In tali casi, salvo verifica di avvenuta ricezione, pu? essere necessario modificare i contenuti o le modalit? d'invio.
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n|  Thu Feb 28 19:09:45 2019
From: wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n| (Viechtbauer, Wolfgang (SP))
Date: Thu, 28 Feb 2019 18:09:45 +0000
Subject: [R-sig-ME] Weird problem with augPred()
Message-ID: <6a55f671b79245a3bc99a7c2848582d9@UM-MAIL3214.unimaas.nl>

Hi All,

I came across a weird problem with augPred() from nlme. Here is a reproducible example to illustrate the issue:

library(nlme)

dat <- data.frame(id = c(1,1,1,2,3,3,4,4,4,4),
                  xi = c(1,2,3,1,1,2,1,2,3,4),
                  yi = c(2,1,4,2,3,2,5,4,6,8),
                  zi = c(rep("a",9), NA))

res1 <- lme(yi ~ xi, random = ~ 1 | id, data=dat)
sav1 <- augPred(res1, primary = ~ xi) # WORKS

dat$zi <- "a"

res2 <- lme(yi ~ xi, random = ~ 1 | id, data=dat)
sav2 <- augPred(res2, primary = ~ xi) # WORKS

dat$zi[10] <- NA

res3 <- lme(yi ~ xi, random = ~ 1 | id, data=dat)
sav3 <- augPred(res3, primary = ~ xi) # ERROR

So, if 'zi' is a factor, then a missing value causes no problems (sav1). Or if 'zi' is a character variable without any missings, then augPred() also runs (sav2). But if 'zi' is a character variable and there is at least one missing value, then augPred() fails (sav3).

This aside, why does 'zi' even play a role here? It isn't used at all in the model fit.

Best,
Wolfgang


From k@@t@ke10 @end|ng |rom gm@||@com  Thu Feb 28 20:20:48 2019
From: k@@t@ke10 @end|ng |rom gm@||@com (Kasahun Takele)
Date: Thu, 28 Feb 2019 22:20:48 +0300
Subject: [R-sig-ME] Requesting for Assistance
Message-ID: <CAFsDCbs7=q248vnaXKqTby+mTiQCjY9iOyDMa_1AwV4x5Jiycg@mail.gmail.com>

Dear Sir/Madam,
I am motivated to apply the multivariate linear mixed model. May you help
me to get R packages to fit multivariate linear mixed model.

Thank you for your assistance.


_______________________________________________________________________________

With Regards,

Kasahun Takele

	[[alternative HTML version deleted]]


From ph||||p@@|d@y @end|ng |rom mp|@n|  Mon Mar  4 18:41:29 2019
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Mon, 4 Mar 2019 18:41:29 +0100
Subject: [R-sig-ME] R: R:  gls() funcion after R upgrade
In-Reply-To: <9d887a8a6c404bb4a52d9d921af1ffdf@popso.it>
References: <d0cd3fe9653e44dcbdb782bb8af4094a@popso.it>
 <a1ff82fe-4f87-c923-e111-f73e9b31a4d6@mpi.nl>
 <5640809b8d8849edb929ab293773809b@popso.it>
 <eb21ed55-8e07-b457-6f9b-c1cfa3ff3b0f@mpi.nl>
 <9d887a8a6c404bb4a52d9d921af1ffdf@popso.it>
Message-ID: <c838e389-c131-9ca7-5d54-d829f4ffeacc@mpi.nl>

I also found that page confusing! It seems that you should submit your
bug report to the mailing list r-devel at r-rpoject.org :

> If your bug is in the language, though, or the Core-supported packages, you should submit your report to R?s Bugzilla.
> NOTE: due to abuse by spammers, since 2016-07-09 only ?members? (including all who have previously submitted bugs) can submit new bugs on R?s Bugzilla. For now, either post (e-mail) your bug report to R-devel or ask an R Core member to add you manually to R?s Bugzilla members.

(from: https://www.r-project.org/bugs.html ; nlme is maintained by R-Core).

Thanks for your patience and contribution!

Phillip



On 26/2/19 9:12 am, Fabio Fanoni wrote:
> On the web page
> 
> https://cran.r-project.org/web/packages/nlme/index.html
> 
> I found the following link
> 
> https://bugs.r-project.org
> 
> that re-directs me to:
> 
> https://bugs.r-project.org/bugzilla/
> 
> (Bugzilla - Main Page)
> 
> To enter a new bug report you must have an account (email address and password).
> 
> Unfortunately, I can't find, on this website, the link for creating a new account.
> In the homepage header, either clicking on "New" or clicking on "Log In" I have to insert Email Address and Password
> 
> Would you have an idea on how to create an account?
> 
> 
> Fabio Fanoni
> SVILUPPO MODELLI DI CREDITO
> Piazza Garibaldi 16
> 23100 Sondrio (SO)
> Banca Popolare di Sondrio
> 
> Tel. +390342528920?| Mobile. ?| Fax. 
> fabio.fanoni at popso.it
> 
> -----Messaggio originale-----
> Da: Phillip Alday <phillip.alday at mpi.nl> 
> Inviato: marted? 19 febbraio 2019 17:43
> A: Fabio Fanoni <fabio.fanoni at popso.it>; r-sig-mixed-models at r-project.org
> Oggetto: Re: R: [R-sig-ME] gls() funcion after R upgrade
> 
> This seems like a bug / compiler issue on 32-bit Windows. I don't have a convenient Windows machine to test it out myself, but you should report it (along with your minimal example):
> 
> https://cran.r-project.org/web/packages/nlme/index.html
> 
> Also: for many reasons (performance, ability to work with larger datasets, etc.), if you have access to the 64-bit versions, you should be using them!
> 
> Phillip
> 
> On 19/2/19 5:02 pm, Fabio Fanoni wrote:
>> Dear Phillip,
>>
>> thanks for the reply
>>
>> I tried both to re-install the package nlme (and all its dependencies) 
>> and to re-install R as a whole but I always get the same error message
>>
>> The problem seems to be related to the varExp() variance function 
>> structures on a 32-bit windows platform With 64-bit windows platform, 
>> the old version 3.3.2 and the new version 3.5.2 produce the same 
>> results At the same time, the varPower() function always produces the 
>> same results regardless of the windows platform or version of R The 
>> following table shows the coefficients obtained according to the 
>> platform (32 bit / 62 bit), version of R and variance function
>>
>> Platform	Version	weights	   	ITOD	   	EU_CH
>> windows 32 bit	3.3.2	varExp()	-0.1053291	-0.4406068
>> windows 32 bit	3.5.2	varExp()	<NA>	         <NA>
>> windows 64 bit	3.3.2	varExp()	-0.3011427	-0.5537158
>> windows 64 bit	3.5.2	varExp()	-0.3011427	-0.5537158
>> windows 32 bit	3.3.2	varPower()	-0.3138869	-0.5010393
>> windows 32 bit	3.5.2	varPower()	-0.3138869	-0.5010393
>> windows 64 bit	3.3.2	varPower()	-0.3138868	-0.5010394
>> windows 64 bit	3.5.2	varPower()	-0.3138868	-0.5010394
>> 				
>> linux 64 bit	3.5.2	varExp()	-0.3123153 	-0.4979789
>>
>> These values differ from what I reported in the previous e-mail. For a ?copy and paste? mistake, in the previous e-mail I reported the results obtained with the varPower() function instead of varExp() function. 
>>
>> Here is the correct output
>>
>>> gls(as.formula(f),correlation=corARMA(p=2,q=3,fixed=FALSE),data=db,we
>>> ights=varExp(),method="ML")
>> Generalized least squares fit by maximum likelihood
>>   Model: as.formula(f) 
>>   Data: db 
>>   Log-likelihood: 12.11128
>>
>> Coefficients:
>>       ITOD      EU_CH 
>> -0.1053291 -0.4406068
>>
>> Correlation Structure: ARMA(2,3)
>>  Formula: ~1
>>  Parameter estimate(s):
>>       Phi1       Phi2     Theta1     Theta2     Theta3 
>>  0.6485554 -0.1750409  0.9450232  0.9566344  0.9566288 Variance 
>> function:
>>  Structure: Exponential of variance covariate
>>  Formula: ~fitted(.)
>>  Parameter estimates:
>>         expon
>> -0.0007047246
>> Degrees of freedom: 71 total; 69 residual Residual standard error: 
>> 0.7740528
>>
>> On the 3.5.2 version of R, the sessionInfo() show "Matrix products: default" whereas in the 3.3.2 version this message does not appear. 
>>
>> If I set options(matprod="internal") on the 3.5.2 I avoid the error 
>> but the results I get are very different from those of version 3.3.2
>>
>> If I use the REML method instead of the ML I get much more similar 
>> results but, at the same time, very different from those I would get 
>> with the same method on the version 3.2.2
>>
>> Maybe these clues can inspire a solution?
>>
>>
>> Fabio
>>
>>
>>
>>
>>
>> -----Messaggio originale-----
>> Da: Alday, Phillip <Phillip.Alday at mpi.nl>
>> Inviato: marted? 19 febbraio 2019 00:44
>> A: Fabio Fanoni <fabio.fanoni at popso.it>; 
>> r-sig-mixed-models at r-project.org
>> Oggetto: Re: [R-sig-ME] gls() funcion after R upgrade
>>
>> Dear Fabio,
>>
>> Your example runs fine on my machine, and I have a comparable R and nlme version. Have you tried re-installing nlme and all its dependencies?
>>
>> install.packages("nlme",dependencies=TRUE)
>>
>> Sometimes dependencies can get out of sync and this can lead to weird 
>> errors. (I've had this happen when lme4 was compiled against a 
>> different Rcpp version than Matrix.)
>>
>> As a sidebar, I get different estimates than your old output had:
>>
>>
>>>
>> gls(as.formula(f),correlation=corARMA(p=2,q=3,fixed=FALSE),data=db,wei
>> ghts=varExp(),method="ML") Generalized least squares fit by maximum 
>> likelihood
>> ? Model: as.formula(f)
>> ? Data: db
>> ? Log-likelihood: 11.74371
>>
>> Coefficients:
>> ????? ITOD????? EU_CH
>> -0.3123153 -0.4979789
>>
>> Correlation Structure: ARMA(2,3)
>> ?Formula: ~1
>> ?Parameter estimate(s):
>> ????? Phi1?????? Phi2???? Theta1???? Theta2???? Theta3
>> ?0.7542011 -0.2467866? 0.8989841? 0.9444138? 0.9092150 Variance function:
>> ?Structure: Exponential of variance covariate
>> ?Formula: ~fitted(.)
>> ?Parameter estimates:
>> ????? expon
>> -0.01497128
>> Degrees of freedom: 71 total; 69 residual Residual standard error: 
>> 0.817064
>>
>> (Note that "power" is now called "expon" for "exponent".) The small differences in the exponent and the coefficients could be numerical issues, but the big differences in ARMA seem a bit worrying. The log-likelihood of 'my' fit is better, but the residual standard error is worse.
>>
>> Best,
>>
>> Phillip
>>
>>> sessionInfo()
>> R version 3.5.2 (2018-12-20)
>> Platform: x86_64-pc-linux-gnu (64-bit) Running under: Linux Mint 19.1
>>
>> Matrix products: default
>> BLAS: /usr/lib/x86_64-linux-gnu/openblas/libblas.so.3
>> LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.2.20.so
>>
>> ::snip::
>>
>>
>> attached base packages:
>> [1] stats???? graphics? grDevices utils???? datasets? methods?? base
>>
>> other attached packages:
>> [1] nlme_3.1-137
>>
>> loaded via a namespace (and not attached):
>> [1] compiler_3.5.2? grid_3.5.2????? lattice_0.20-38
>>
>> On 11/2/19 9:40 am, Fabio Fanoni wrote:
>>> Dear all,
>>>
>>> I have been using the package nlme for about a year through the 
>>> software R
>>>
>>> Recently, following an update of R (and the corresponding nlme package), in some situations the gls() function does not work.
>>>
>>>
>>> The following lines of code
>>>
>>> require(nlme)
>>> db<-read.csv2("G:\\Dati.csv",as.is=TRUE)
>>> f<-"DELTA_INVNORM_STD ~ 0+ ITOD + EU_CH"
>>> gls(as.formula(f),correlation=corARMA(p=2,q=3,fixed=FALSE),data=db,we
>>> i
>>> ghts=varExp(),method="ML")
>>>
>>>
>>> with the old version of R returns the following output:
>>>
>>> Generalized least squares fit by maximum likelihood
>>>   Model: as.formula(f)
>>>   Data: db
>>>   Log-likelihood: -36.13853
>>>
>>> Coefficients:
>>>       ITOD      EU_CH
>>> -0.3138869 -0.5010393
>>>
>>> Correlation Structure: ARMA(2,3)
>>> Formula: ~1
>>>  Parameter estimate(s):
>>>        Phi1        Phi2      Theta1      Theta2      Theta3
>>>  0.50054107 -0.07357577  0.91847373  0.66706344  0.51689285 Variance
>>> function:
>>> Structure: Power of variance covariate
>>> Formula: ~fitted(.)
>>>  Parameter estimates:
>>>       power
>>> -0.01893321
>>> Degrees of freedom: 71 total; 69 residual Residual standard error: 
>>> 0.4077908
>>>
>>>
>>> while with the new one version of R ends with the following error 
>>> message
>>>
>>> Error in `coef<-.corARMA`(`*tmp*`, value = value[parMap[, i]]) :
>>>   NA/NaN/Inf in foreign function call (arg 1)
>>>
>>>
>>>
>>> here are the parameters of the old version of R
>>>
>>>> sessionInfo()
>>> R version 3.3.2 (2016-10-31)
>>> Platform: i386-w64-mingw32/i386 (32-bit) Running under: Windows >= 8
>>> x64 (build 9200)
>>>
>>> locale:
>>> [1] LC_COLLATE=Italian_Italy.1252  LC_CTYPE=Italian_Italy.1252    LC_MONETARY=Italian_Italy.1252
>>> [4] LC_NUMERIC=C                   LC_TIME=Italian_Italy.1252
>>>
>>> attached base packages:
>>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>>
>>> other attached packages:
>>> [1] nlme_3.1-128
>>>
>>> loaded via a namespace (and not attached):
>>> [1] tools_3.3.2     grid_3.3.2      lattice_0.20-34
>>>
>>>
>>>
>>> and the correspondents of the new one
>>>
>>>> sessionInfo()
>>> R version 3.5.2 (2018-12-20)
>>> Platform: i386-w64-mingw32/i386 (32-bit) Running under: Windows >= 8
>>> x64 (build 9200)
>>>
>>> Matrix products: default
>>>
>>> locale:
>>> [1] LC_COLLATE=Italian_Italy.1252  LC_CTYPE=Italian_Italy.1252    LC_MONETARY=Italian_Italy.1252
>>> [4] LC_NUMERIC=C                   LC_TIME=Italian_Italy.1252
>>>
>>> attached base packages:
>>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>>
>>> other attached packages:
>>> [1] nlme_3.1-137
>>>
>>> loaded via a namespace (and not attached):
>>> [1] compiler_3.5.2  tools_3.5.2     grid_3.5.2      lattice_0.20-38
>>>
>>> The data used are shown below
>>>
>>>
>>> I wonder if someone could help me to solve this problem.
>>>
>>> Best regards,
>>>
>>> Fabio Fanoni
>>>
>>>
>>> DELTA_INVNORM_STD;ITOD;EU_CH
>>> -1,01888891391635;1,11744160507933;0,164921569515933
>>> -1,7287582158078;1,33122211515022;0,192399322297369
>>> -1,92084919508607;1,42620153798496;0,119394703266418
>>> -1,07481132504896;1,40851069525448;-0,21451386962765
>>> 0,368325144516397;1,143582080184;-0,541418306765893
>>> 1,60232300735139;0,877753890833926;-0,540071359447086
>>> 1,79762928248747;0,585114281768911;-0,457099318942592
>>> 0,91995670533225;0,300270733467356;-0,292232806294851
>>> -0,0262660321428139;0,20146037523289;-0,236469135573377
>>> -1,02121152912805;0,0419782544708825;-0,345437279939175
>>> -1,10672650470506;0,0574913198102401;-0,366584376685452
>>> -1,39178681241528;0,33582796272111;-0,222730255141816
>>> -1,1454072922613;0,119188584353956;-0,00950828000562807
>>> -0,958672816825354;-0,0667970836192611;0,4570747351185
>>> -1,32272038017045;-0,307712992787705;0,958678420940093
>>> -0,907400017348963;-0,781894397724971;1,3426934842003
>>> -1,24178782808971;-0,502123817789113;1,78543551198139
>>> -0,727279530248652;-0,198729005124588;1,65424271463061
>>> -0,663683090224516;-0,00292614483254617;1,29016249027665
>>> -0,870662613154294;0,284152373447782;0,852269477692287
>>> -0,740956260890321;0,212558271927232;0,359286261984589
>>> -0,875175106507846;0,16921613723138;0,301636857743261
>>> -0,159691021207123;0,344902273564796;0,406160071512065
>>> 0,485359221230746;0,405193753056144;0,545299871378597
>>> 0,91806657022351;0,51772409209882;0,664639525858513
>>> 0,94182403298868;0,624398008842597;0,721750147939642
>>> 0,772810225693767;0,669380547864261;0,746938090683181
>>> 0,454079892235428;0,829860577818214;0,875841077996076
>>> 0,216198540864624;0,85936757871922;1,06778126286636
>>> -0,390013295840405;0,820301653758161;1,32356681853436
>>> -1,08957286663887;0,735286971796754;1,51483352772482
>>> -1,7181964072478;0,54188741408352;1,59807495122772
>>> -2,14142427321141;0,3080114833751;1,30188093962788
>>> -1,73631132303729;0,242276386254216;0,821154955431617
>>> -1,14832298161027;-0,168559611137128;0,390400772122712
>>> -0,199657255063557;-0,673990227176314;-0,425580724204736
>>> 0,637711021315926;-1,26311924357155;-0,777807797606142
>>> 1,13372564859946;-2,22697608948676;-1,02793616079627
>>> 1,3387228312938;-2,67326030450369;-1,25476231880355
>>> 1,13410628583832;-2,41425486942835;-0,769187327492251
>>> 1,20371911407773;-1,73317141026889;-0,4868668898436
>>> 1,04245681382429;-0,627677467262121;-0,517038541303494
>>> 1,19122635858486;0,318264079861633;-0,902669938921763
>>> 1,10644115835792;0,53399823108225;-1,58826681113873
>>> 0,657027862693417;0,566924775110004;-2,16826290930735
>>> 0,391546504792817;0,382793958142103;-2,37380727704873
>>> -0,121513593301095;0,113508212335312;-2,28814134594747
>>> 0,090587410027589;-0,10456301293743;-1,92150191643467
>>> 0,293363258062962;-0,599038773090656;-1,5279235122598
>>> 0,490646403474033;-1,16804784867814;-1,10646327070376
>>> 0,663013157660536;-1,68022387383074;-0,281457211581005
>>> 0,48006697458688;-2,17535440458421;0,0140633243390067
>>> 0,33992207153128;-2,15751594621565;0,415319331867849
>>> 0,485131796838891;-1,91164287702336;0,74289724468591
>>> 0,645057199360022;-1,64072137764755;0,715150100824384
>>> 0,830719890823037;-1,20839230550334;0,886616664628225
>>> 1,00060688160202;-0,799350150825283;0,78775063121476
>>> 0,799516770753564;-0,474799224784852;0,620190215847712
>>> 0,884527141039385;-0,111321336688718;0,400772282236831
>>> 0,77592825475446;0,41873781137028;0,214623979323192
>>> 0,897761905934407;0,618216183184323;-0,371702774880583
>>> 1,23087946317063;0,86282014652723;-1,04275259746654
>>> 1,1730433788867;1,03592979384644;-1,50933562067236
>>> 1,18413315011977;0,843466359597336;-1,89254251877396
>>> 0,522306442129412;0,786432900675309;-1,199941514842
>>> -0,423472231576842;0,534354713733984;-0,258289703958632
>>> -1,02160314495135;0,445296924001273;0,367637345059225
>>> -1,2194743130469;0,791437804698057;0,831122380946011
>>> -0,682787491324478;0,735393577499657;0,636488297397258
>>> -0,277058822726022;0,988210808618591;0,362382895620209
>>> -0,0283252883031908;1,18581655608866;0,466771415869385
>>>
>>>
>>>
>>>
>>> Fabio Fanoni
>>> SVILUPPO MODELLI DI CREDITO
>>> Piazza Garibaldi 16
>>> 23100 Sondrio (SO)
>>> Banca Popolare di Sondrio
>>> Tel. +390342528920
>>> fabio.fanoni at popso.it<mailto:fabio.fanoni at popso.it>
>>>
>>>
>>> AVVERTENZE LEGALI - I contenuti di questo messaggio, proveniente da un indirizzo di posta elettronica aziendale della Banca Popolare di Sondrio, e gli eventuali allegati possono essere letti e utilizzati, per esigenze lavorative, da chi opera alle dipendenze o per conto della stessa, o ? comunque cointeressato nell'inerente relazione d'affari. Le dichiarazioni, ivi contenute, non impegnano contrattualmente la Banca Popolare di Sondrio se non nei limiti di quanto eventualmente previsto in accordi opportunamente formalizzati. Se il messaggio ? stato ricevuto per errore ce ne scusiamo, pregando di segnalare ci? al mittente e poi di distruggerlo senza farne alcun uso poich? l'utilizzo senza averne diritto ? vietato dalla legge e potrebbe costituire reato.
>>>
>>> DATI SOCIETARI - BANCA POPOLARE DI SONDRIO - Societ? cooperativa per 
>>> azioni - Fondata nel 1871 Sede sociale e direzione generale: I - 
>>> 23100 SONDRIO SO - piazza Garibaldi, 16 Indirizzo Internet:
>>> http:[doppiabarra]www[punto]popso[punto]it - E-mail: 
>>> info[chiocciola]popso[punto]it Iscritta al Registro delle Imprese di Sondrio al n. 00053810149, all'Albo delle Banche al n. 842, all'Albo delle Societ? Cooperative al n. A160536 Capogruppo del Gruppo bancario Banca Popolare di Sondrio, iscritto all'Albo dei Gruppi bancari al n. 5696.0 Aderente al Fondo Interbancario di Tutela dei Depositi e al Fondo Nazionale di Garanzia - Codice fiscale e partita IVA: 00053810149 Capitale sociale: EUR 1.360.157.331 - Riserve: EUR 947.325.264 (Dati approvati dall'Assemblea dei soci del 29 aprile 2017).
>>>
>>> N.B. I "filtri antivirus" e "antispam" in uso su molti sistemi di posta elettronica possono talvolta ritardare o impedire, in tutto o in parte, il recapito dei messaggi. In tali casi, salvo verifica di avvenuta ricezione, pu? essere necessario modificare i contenuti o le modalit? d'invio.
>>>
>>>
>>> 	[[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list 
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From n|codegu|ne@ @end|ng |rom gm@||@com  Tue Mar  5 11:00:24 2019
From: n|codegu|ne@ @end|ng |rom gm@||@com (Nicolas Deguines)
Date: Tue, 5 Mar 2019 11:00:24 +0100
Subject: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5
 levels) response variable?
Message-ID: <CAFzZfa1imABAAqjozzwTpkWgVCaaHkWsGZKKC9aLKPCQ8sjMgw@mail.gmail.com>

Hello everyone,

I am investigating how engagement into a citizen science program can change
participants' behavior in terms of implementing gardening techniques
benefitting biodiversity.
There are 2362 participants, distributed into 7 cohorts (= year in which
they joined the program), and I have repeated gardening technique
information for multiple years for each participant.
So I need to use mixed modeling.

One of the response variable is a score that can takes 5 values: 0, 1, 2,
3, or 4. It's ordered, it's not continuous (there are 5 levels).
I would analyze this into a cumulative link mixed models (using clmm() from
ordinal package) but the Hessian condition I obtained with such model is >
5.0e+06. I.e. assumption is violated (simplifying my initial full model did
not help at all).

As an alternative, I am wondering if I could treat this response variable
has a continuous one into a lmer() model.
When I do:
- Normality of model residuals is nicely met
- Homoscedasticity of model residuals is met as well.
=> does meeting these two assumptions is enough to validate the use of a
lmer() model for an ordered categorical response variable?

In one of Douglas Bates' presentation (slide 3 of Jan. 2011, Madison:
http://lme4.r-forge.r-project.org/slides/2011-01-11-Madison/5GLMM.pdf), it
is said that
"When using LMMs we assume that the response being modeled is on a
continuous scale.
Sometimes we can bend this assumption a bit if the response is an ordinal
response with a moderate to large number of levels.
For example, [...a response variable taking] integer values on the scale of
1 to 10."
=> is 5 levels too few to be treated as continuous? Or would it be ok given
residuals behave nicely?

I would appreciate any help and thoughts on this.
I checked that this was not treated in a previous post and I hope I did not
miss it (sorry if I did).

Best,
Nicolas Deguines
----------------------------------
Postdoctoral Research Associate
Laboratoire Ecologie, Syst?matique et Evolution
Universit? Paris Sud, Orsay, France
Website: http://nicolasdeguines.weebly.com/

	[[alternative HTML version deleted]]


From ph||||p@@|d@y @end|ng |rom mp|@n|  Tue Mar  5 13:04:04 2019
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Tue, 5 Mar 2019 13:04:04 +0100
Subject: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5
 levels) response variable?
In-Reply-To: <CAFzZfa1imABAAqjozzwTpkWgVCaaHkWsGZKKC9aLKPCQ8sjMgw@mail.gmail.com>
References: <CAFzZfa1imABAAqjozzwTpkWgVCaaHkWsGZKKC9aLKPCQ8sjMgw@mail.gmail.com>
Message-ID: <e1e2c821-8add-7cfe-7c23-e4bea4ddb985@mpi.nl>

Hi Nicolas,

How much you can get away bending the assumptions depends in some ways
on how well the resulting model fits your data. If the resulting model
is a poor fit, then it's not a great model for performing inference. The
other problem with bending assumptions is that a lot of 'error
statistics' (standard errors, t-values, and basically anything related
to significance testings) aren't guaranteed to do what they are supposed
to do. (In your case, the good behavior of your residuals suggests that
this won't be a huge problem, but there are no promises.)

You can get around this a bit by doing things like cross-validation or
other inferential steps based on how well the model generalizes to /
predicts new data instead of significance testing of coefficients or
linear hypotheses.

John Kruschke has written about this issue at some length and seems
convinced that it's (almost) always a bad idea to bend the
metric/continuous assumption when dealing with ordinal data:

http://doingbayesiandataanalysis.blogspot.com/2017/12/which-movie-is-rated-better-dont-treat.html

http://doingbayesiandataanalysis.blogspot.com/2018/09/analyzing-ordinal-data-with-metric.html

The latter is largely a link/"press release" for the associated paper:

Liddell, T. M., & Kruschke, J. K. (2018). Analyzing ordinal data with
metric models: What could possibly go wrong? Journal of Experimental
Social Psychology , 79 , 328?348. doi:10.1016/j.jesp.2018.08.009

Finally, have you tried other link and threshold functions in clmm?
Those can make a huge difference!

Phillip

On 5/3/19 11:00 am, Nicolas Deguines wrote:
> Hello everyone,
> 
> I am investigating how engagement into a citizen science program can change
> participants' behavior in terms of implementing gardening techniques
> benefitting biodiversity.
> There are 2362 participants, distributed into 7 cohorts (= year in which
> they joined the program), and I have repeated gardening technique
> information for multiple years for each participant.
> So I need to use mixed modeling.
> 
> One of the response variable is a score that can takes 5 values: 0, 1, 2,
> 3, or 4. It's ordered, it's not continuous (there are 5 levels).
> I would analyze this into a cumulative link mixed models (using clmm() from
> ordinal package) but the Hessian condition I obtained with such model is >
> 5.0e+06. I.e. assumption is violated (simplifying my initial full model did
> not help at all).
> 
> As an alternative, I am wondering if I could treat this response variable
> has a continuous one into a lmer() model.
> When I do:
> - Normality of model residuals is nicely met
> - Homoscedasticity of model residuals is met as well.
> => does meeting these two assumptions is enough to validate the use of a
> lmer() model for an ordered categorical response variable?
> 
> In one of Douglas Bates' presentation (slide 3 of Jan. 2011, Madison:
> http://lme4.r-forge.r-project.org/slides/2011-01-11-Madison/5GLMM.pdf), it
> is said that
> "When using LMMs we assume that the response being modeled is on a
> continuous scale.
> Sometimes we can bend this assumption a bit if the response is an ordinal
> response with a moderate to large number of levels.
> For example, [...a response variable taking] integer values on the scale of
> 1 to 10."
> => is 5 levels too few to be treated as continuous? Or would it be ok given
> residuals behave nicely?
> 
> I would appreciate any help and thoughts on this.
> I checked that this was not treated in a previous post and I hope I did not
> miss it (sorry if I did).
> 
> Best,
> Nicolas Deguines
> ----------------------------------
> Postdoctoral Research Associate
> Laboratoire Ecologie, Syst?matique et Evolution
> Universit? Paris Sud, Orsay, France
> Website: http://nicolasdeguines.weebly.com/
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From ph||||p@@|d@y @end|ng |rom mp|@n|  Tue Mar  5 13:55:34 2019
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Tue, 5 Mar 2019 13:55:34 +0100
Subject: [R-sig-ME] treatment and sum contrasts in ME models
In-Reply-To: <CAHhX7WhrieoYq9R=_U39LRjv-ET1cxoZpm4UUOjRA_UQqg+9Ag@mail.gmail.com>
References: <CAHhX7Wh5sz2xzBuCvth-pT0SxriA54_2G7YT4KM1=n3XE07uhQ@mail.gmail.com>
 <CABghstRck9kCHFbVUst67+=BtSu8WY_vyLS8sseZYPgr+jB0GQ@mail.gmail.com>
 <CAHhX7WhrieoYq9R=_U39LRjv-ET1cxoZpm4UUOjRA_UQqg+9Ag@mail.gmail.com>
Message-ID: <74e420f1-b67d-d2d1-da4e-aa1996d295aa@mpi.nl>

Following up on this, wouldn't the direct comparison be with

anova.lme(linM1,type="marginal")
anova.lme(linM2,type="marginal")

?

And potentially doing the same thing with car::Anova() /

Best,
Phillip

On 21/2/19 7:57 pm, Cristiano Alessandro wrote:
> Unfortunately, it is a bit hard for me to do so at this point. I would have
> done it otherwise, as I did in the past. Sorry.
> Can a guess be made also without the dataset?
> 
> On Thu, Feb 21, 2019 at 12:50 PM Ben Bolker <bbolker at gmail.com> wrote:
> 
>>   Not totally necessary, but would it be easy to post your data somewhere?
>>
>> On Thu, Feb 21, 2019 at 1:44 PM Cristiano Alessandro
>> <cri.alessandro at gmail.com> wrote:
>>>
>>> Hi all,
>>>
>>> I am fitting a linear mixed effect models with two factors (mPair with 6
>>> levels, and spd_des with 3 levels) and their interaction in R. I obtain
>>> inconsistent results depending on the contrasts that I choose, and I
>> would
>>> like to understand why and how to deal with it.
>>>
>>> If I use treatment contrasts, I obtain the following (I only copy the
>>> relevant info of the results)
>>>
>>>> options(contrasts = c("contr.treatment","contr.poly"))
>>>> linM1 <- lme(cc_marg ~ mPair*spd_des , random = ~mPair|ratID,
>> data=dat_trf, na.action=na.omit, method = "ML", control=lCtr )
>>>> summary(linM1)
>>>
>>> Fixed effects: cc_marg ~ mPair * spd_des
>>>                          Value  Std.Error DF   t-value p-value
>>> (Intercept)          1.4628761 0.09618167 94 15.209511  0.0000
>>> mPairRFVI           -0.8180718 0.10454920 94 -7.824754  0.0000
>>> mPairVLRF           -0.7990828 0.13193991 94 -6.056415  0.0000
>>> mPairVLVI           -0.6077804 0.13734253 94 -4.425289  0.0000
>>> mPairVMRF           -0.7444267 0.13294167 94 -5.599649  0.0000
>>> mPairVMVI           -0.4799995 0.12194383 94 -3.936234  0.0002
>>> spd_des15           -0.0830016 0.07990370 94 -1.038771  0.3016
>>> spd_des20           -0.0856339 0.08321984 94 -1.029008  0.3061
>>> mPairRFVI:spd_des15 -0.1576193 0.13500809 94 -1.167481  0.2460
>>> mPairVLRF:spd_des15  0.0866510 0.11385875 94  0.761039  0.4485
>>> mPairVLVI:spd_des15  0.0083311 0.13500809 94  0.061708  0.9509
>>> mPairVMRF:spd_des15  0.0184844 0.11385875 94  0.162345  0.8714
>>> mPairVMVI:spd_des15 -0.0672286 0.13500809 94 -0.497960  0.6197
>>> mPairRFVI:spd_des20 -0.1705514 0.14201095 94 -1.200973  0.2328
>>> mPairVLRF:spd_des20  0.0899629 0.11949193 94  0.752879  0.4534
>>> mPairVLVI:spd_des20 -0.0626845 0.14359174 94 -0.436547  0.6634
>>> mPairVMRF:spd_des20 -0.0106400 0.11969131 94 -0.088895  0.9294
>>> mPairVMVI:spd_des20 -0.0608750 0.14286017 94 -0.426116  0.6710
>>>
>>> I interpret this as follows: Since the interaction terms are all
>>> non-significant, then the factor spd_des (also non-significant) does not
>>> influence the data at any level of the factor mPair.
>>>
>>> On the other hand, using sum contrasts I obtain the following results.
>>>
>>>> options(contrasts = c("contr.sum","contr.poly"))
>>>> linM2 <- lme(cc_marg ~ mPair*spd_des , random = ~mPair|ratID,
>> data=dat_trf, na.action=na.omit, method = "ML", control=lCtr )
>>>> summary(linM2)
>>>
>>> Fixed effects: cc_marg ~ mPair * spd_des
>>>                      Value  Std.Error DF   t-value p-value
>>> (Intercept)      0.8137433 0.04791890 94 16.981678  0.0000
>>> mPair1           0.5929117 0.06609665 94  8.970373  0.0000
>>> mPair2          -0.3341386 0.04969616 94 -6.723629  0.0000
>>> mPair3          -0.1472874 0.07260892 94 -2.028503  0.0453
>>> mPair4          -0.0328631 0.08993236 94 -0.365421  0.7156
>>> mPair5          -0.1488959 0.06991733 94 -2.129600  0.0358
>>> spd_des1         0.0743293 0.02315254 94  3.210416  0.0018
>>> spd_des2        -0.0272358 0.02325774 94 -1.171043  0.2445
>>> mPair1:spd_des1 -0.0181081 0.04414399 94 -0.410206  0.6826
>>> mPair2:spd_des1  0.0912334 0.05726538 94  1.593168  0.1145
>>> mPair3:spd_des1 -0.0769866 0.04518813 94 -1.703691  0.0917
>>> mPair4:spd_des1  0.0000066 0.05743544 94  0.000114  0.9999
>>> mPair5:spd_des1 -0.0207337 0.04518548 94 -0.458859  0.6474
>>> mPair1:spd_des2  0.0004559 0.04558473 94  0.010002  0.9920
>>> mPair2:spd_des2 -0.0478225 0.05730295 94 -0.834555  0.4061
>>> mPair3:spd_des2  0.0282279 0.04525282 94  0.623781  0.5343
>>> mPair4:spd_des2  0.0269011 0.05747689 94  0.468034  0.6408
>>> mPair5:spd_des2  0.0163141 0.04525367 94  0.360503  0.7193
>>>
>>>> anova.lme(linM2,type="marginal")
>>>               numDF denDF   F-value p-value
>>> (Intercept)       1    94 288.37740  <.0001
>>> mPair             5    94  35.30799  <.0001
>>> spd_des           2    94   5.17279  0.0074
>>> mPair:spd_des    10    94   0.52159  0.8710
>>>
>>> The results are now telling me that the first level of the factor spd_des
>>> is significant; i.e. the mean of the data at that level of spd_des is
>>> significantly different from the grand mean (Intercept), and since the
>>> interactions are all non-significant, this is true at all levels of
>> mPair.
>>>
>>> So, with treatment contrasts spd_des does not influence the data at any
>>> level of mPair, and with sum contrast spd_des influence the data at all
>>> level of mPair. How do I deal with this? What result should I trust?
>>> Thanks in advance for your help
>>>
>>> Best
>>> Cristiano
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From ph||||p@@|d@y @end|ng |rom mp|@n|  Tue Mar  5 17:35:47 2019
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Tue, 5 Mar 2019 17:35:47 +0100
Subject: [R-sig-ME] Interaction effects with GAMM
In-Reply-To: <7C5C755C-1B17-4829-9DB4-1D192ADB5648@drcmr.dk>
References: <8282C73C-0475-4012-9EC3-152CACF7E823@drcmr.dk>
 <36e467b0-eae1-5afc-281a-59630dd278e3@mpi.nl>
 <7C5C755C-1B17-4829-9DB4-1D192ADB5648@drcmr.dk>
Message-ID: <1ff129b0-5dd7-6f87-bfda-51e7ecfe8f38@mpi.nl>

Dear Louise,

knowing a bit about your data helps in knowing which smoother to use.

>From my basic understanding of GAMs and after looking through Wood's
"Generalized Additive Models" (2nd ed):

ti(age) : I don't think this is actually any different from s(age), and
for the examples in the book, main effects that have been separated out
are usually computed with s(). The reason I think this that ti() is a
tensor product smooth, but there is no actual tensor product when
dealing with only one covariate, so this reduces down to a 1d smoother.
For the 2+d case, this is not the case: s(x,y) is isotropic, i.e. forces
both x and y to share the same scale and wigliness, while ti(x,y) and
te(x,y) do not force that.

ti(age) + ti(age, by=sex) and brain ti(brain, by=age): seems to fit the
advice on pp. 326-327 that a main effect be included for smooth-factor
interactions, but as in above, I think you can simplify all of your ti()
to s() without any change. ti() and te() are definitely equivalent here
because there are no lower order interactions or main effects to exclude
from a single predictor -- ti() and te() only really differ when there
are two covariates not specified by the by= argument because the
predictor in by= argument isn't a smoother covariate but rather
specifies that multiple smoothers are produced (see Chapter 5, and later
p. 334 "... isotropic smooths, of the sort produced by s(X,Y) terms, are
usually good choices when the covariates of the smooth are naturally on
the same scale, and we expect that the same degree of smoothness is
appropriate with respect to both covariate axes...").

I'm also not sure that you need a smoother on the main effect of age,
but that's a question to be determined via model comparison.

The combination of your within and between subject age scale should
allow estimating both fixed and random effects of age.

So I would start with:

M = gamm(behav ~ age + sex + education + s(age, by = sex) + brain +
s(brain, by = age), random = list(subjectID = ~1+age), data = data)

and then see (via AIC) if adding a marginal smoother for age helps:

M = gamm(behav ~ s(age,id="age") + sex + education + s(age, by = sex,
id="age") + brain + s(brain, by = age), random = list(subjectID =
~1+age), data = data)

and perhaps repeating the same thing for brain. Note that id=...
argument forces the same age-related smoothers to have the same
smoothing parameter, i.e. the same amount of wigliness.

Like I said though, my understanding of and experience with GAMs is not
particularly extensive, so you can also check whether there is a
difference between s() and ti() in the overall model fit. In one example
in the book, basically equivalent models differ somewhat in their fit
because of differences in the implied penalty structure for the smooths
(p. 335).

Best,
Phillip

On 26/2/19 1:57 pm, Louise Baru?l Johansen wrote:
> Dear Phillip,
> 
> Thank you for taking your time to look at my question.
> 
> Our data consists of up to 12 MRI scans per subject with interscan-intervals of 6 months, and the subjects were between the age of 7-13 years at baseline, which gives us a reasonable overlap between subjects. The brain data is extracted from regions of interest, and the behavioural data could be RT.
> 
> My question was more regarding how to incorporate the interaction effects in the most appropriate way statistically; by using te(), ti(), or in a completely different way?
> 
> All the best,
> 
> Louise
> 
>> On 22 Feb 2019, at 11.29, Phillip Alday <phillip.alday at mpi.nl> wrote:
>>
>> Hi Louise,
>>
>> I'm somewhat curious what brain imaging data you have that can be so
>> neatly summed up as a single univariate value. While you can do e.g. the
>> EEG voltage at a given timepoint in a given channel or the BOLD signal
>> in a given voxel or some overall structural score derived from DTI,
>> these are generally very poor indices of the structural and activity
>> variation within and between brains. I ask because knowing more about
>> your data helps when giving advice about a model. I'm guessing behavior
>> is something like RT or maybe d-prime/sensitivity index and *not* simple
>> accuracy, where a Gaussian model would not be appropriate.
>>
>> All that said, I do already have one comment/question ...
>>
>> Your data are longitudinal, but how much so? What's the range in age
>> within subjects vs. between subjects? If the range within subjects is
>> just a few months to a year or two and the range between subjects is
>> several years, as is common in many studies, then having a by-subject
>> slope for age doesn't really make much sense. The overall by-subjects
>> variation (the intercept, i.e. ~1) and residual variation will probably
>> dominate.
>>
>> And some general advice: use the various plotting functions (plot(),
>> vis.gam()), etc. to get an idea about what your model "thinks" the world
>> looks like and whether that matches your own expectations and
>> matches/fits the picture presented by the data.
>>
>> Best,
>> Phillip
>>
>>
>> On 20/2/19 10:01 am, Louise Baru?l Johansen wrote:
>>> I have a question on how to model interaction terms including smooths in a GAMM model (using the mgcv and nlme packages in R).
>>>
>>> We have collected longitudinal behavioral and brain imaging data from ~100 subjects across ~6 time points, and I would like to model main effects of age, sex, brain as well as to-way interaction terms (and maybe three-way interaction terms), while correcting for education level and taking random effects into account.
>>>
>>> Is using the ti() setup the way to do this:
>>>
>>> M = gamm(behav ~ ti(age) + sex + education + ti(age, by = sex) + brain + ti(brain, by = age), random = list(subjectID = ~1+age), data = data)
>>>
>>>
>>> All help will be appreciated. 
>>>
>>> Thanks, Louise
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>


From n|codegu|ne@ @end|ng |rom gm@||@com  Wed Mar  6 15:00:32 2019
From: n|codegu|ne@ @end|ng |rom gm@||@com (Nicolas Deguines)
Date: Wed, 6 Mar 2019 15:00:32 +0100
Subject: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5
 levels) response variable?
In-Reply-To: <e1e2c821-8add-7cfe-7c23-e4bea4ddb985@mpi.nl>
References: <CAFzZfa1imABAAqjozzwTpkWgVCaaHkWsGZKKC9aLKPCQ8sjMgw@mail.gmail.com>
 <e1e2c821-8add-7cfe-7c23-e4bea4ddb985@mpi.nl>
Message-ID: <CAFzZfa3nFOxY8WyscYBad7idDv=eo_v7fXetKBKtFPZ3k0uk+w@mail.gmail.com>

Hello Phillip and all,

Thanks a lot Phillip for your very interesting and useful answer, and for
the paper from Liddell & Kruschke. It helps a lot.

About trying other link and threshold functions in clmm: no huge difference
in my case unfortunately. I tried different combinations of each.
'equidistant' did do better, but the improvement was far from enough.

I computed density plots for my response variable as observed and as
predicted from my lmer() model (similar to what Liddell and Kruschke do in
Figure 6): the linear mixed-model does pretty well in fitting the data.
=> so I'd be enclined to trust the results from my lmer models in the
present case (but Liddell and Kruschke did show very clear cases when a
linear model fit very poorly the ordinal data).

Meanwhile, I thought of another alternative for analyzing this response
variable and I would be curious to read what people may think about it.
Before presenting that alternative, I need to say more about that 5-levels
response variable.
It is a score built by Muratet and Fontaine (2015)* to assess the
naturalness of a given private backyard (it is shown to be correlated with
higher abundance of butterflies).
In the backyard: fallow area, nettles (*Urtica dioica*), ivy (*Hedera helix*),
and brambles (*Rubus spp.*) are each scored one if present, and the
naturalness index was computed as the sum of these scores.
=> it results in a 5-levels ordinal variable because it can go from 0 to 4,
and each increase in 1 means a backyard with more features of 'naturalness'.
I wonder thus if this could be modelled using a glmer() with family =
binomial and feeding to the model two columns: cbind(sum of 1's, sum of
0's) (see R documentation for family{stats}, in the Details: "*As a
two-column integer matrix: the first column gives the number of successes
and the second the number of failures.*")
I will try and see how the model fit the data. But I would be interested in
getting a theoretical opinion.

I hope this can help others too

Best regards,
Nicolas Deguines

*
https://www.sciencedirect.com/science/article/abs/pii/S0006320714004704?via%3Dihub

----------------------------------
Postdoctoral Research Associate
Laboratoire Ecologie, Syst?matique et Evolution
Universit? Paris Sud, Orsay, France
Website: http://nicolasdeguines.weebly.com/


On Tue, 5 Mar 2019 at 13:04, Phillip Alday <phillip.alday at mpi.nl> wrote:

> Hi Nicolas,
>
> How much you can get away bending the assumptions depends in some ways
> on how well the resulting model fits your data. If the resulting model
> is a poor fit, then it's not a great model for performing inference. The
> other problem with bending assumptions is that a lot of 'error
> statistics' (standard errors, t-values, and basically anything related
> to significance testings) aren't guaranteed to do what they are supposed
> to do. (In your case, the good behavior of your residuals suggests that
> this won't be a huge problem, but there are no promises.)
>
> You can get around this a bit by doing things like cross-validation or
> other inferential steps based on how well the model generalizes to /
> predicts new data instead of significance testing of coefficients or
> linear hypotheses.
>
> John Kruschke has written about this issue at some length and seems
> convinced that it's (almost) always a bad idea to bend the
> metric/continuous assumption when dealing with ordinal data:
>
>
> http://doingbayesiandataanalysis.blogspot.com/2017/12/which-movie-is-rated-better-dont-treat.html
>
>
> http://doingbayesiandataanalysis.blogspot.com/2018/09/analyzing-ordinal-data-with-metric.html
>
> The latter is largely a link/"press release" for the associated paper:
>
> Liddell, T. M., & Kruschke, J. K. (2018). Analyzing ordinal data with
> metric models: What could possibly go wrong? Journal of Experimental
> Social Psychology , 79 , 328?348. doi:10.1016/j.jesp.2018.08.009
>
> Finally, have you tried other link and threshold functions in clmm?
> Those can make a huge difference!
>
> Phillip
>
> On 5/3/19 11:00 am, Nicolas Deguines wrote:
> > Hello everyone,
> >
> > I am investigating how engagement into a citizen science program can
> change
> > participants' behavior in terms of implementing gardening techniques
> > benefitting biodiversity.
> > There are 2362 participants, distributed into 7 cohorts (= year in which
> > they joined the program), and I have repeated gardening technique
> > information for multiple years for each participant.
> > So I need to use mixed modeling.
> >
> > One of the response variable is a score that can takes 5 values: 0, 1, 2,
> > 3, or 4. It's ordered, it's not continuous (there are 5 levels).
> > I would analyze this into a cumulative link mixed models (using clmm()
> from
> > ordinal package) but the Hessian condition I obtained with such model is
> >
> > 5.0e+06. I.e. assumption is violated (simplifying my initial full model
> did
> > not help at all).
> >
> > As an alternative, I am wondering if I could treat this response variable
> > has a continuous one into a lmer() model.
> > When I do:
> > - Normality of model residuals is nicely met
> > - Homoscedasticity of model residuals is met as well.
> > => does meeting these two assumptions is enough to validate the use of a
> > lmer() model for an ordered categorical response variable?
> >
> > In one of Douglas Bates' presentation (slide 3 of Jan. 2011, Madison:
> > http://lme4.r-forge.r-project.org/slides/2011-01-11-Madison/5GLMM.pdf),
> it
> > is said that
> > "When using LMMs we assume that the response being modeled is on a
> > continuous scale.
> > Sometimes we can bend this assumption a bit if the response is an ordinal
> > response with a moderate to large number of levels.
> > For example, [...a response variable taking] integer values on the scale
> of
> > 1 to 10."
> > => is 5 levels too few to be treated as continuous? Or would it be ok
> given
> > residuals behave nicely?
> >
> > I would appreciate any help and thoughts on this.
> > I checked that this was not treated in a previous post and I hope I did
> not
> > miss it (sorry if I did).
> >
> > Best,
> > Nicolas Deguines
> > ----------------------------------
> > Postdoctoral Research Associate
> > Laboratoire Ecologie, Syst?matique et Evolution
> > Universit? Paris Sud, Orsay, France
> > Website: http://nicolasdeguines.weebly.com/
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>

	[[alternative HTML version deleted]]


From emm@nue|@cur|@ @end|ng |rom p@r|@de@c@rte@@|r  Wed Mar  6 15:11:17 2019
From: emm@nue|@cur|@ @end|ng |rom p@r|@de@c@rte@@|r (Emmanuel Curis)
Date: Wed, 6 Mar 2019 15:11:17 +0100
Subject: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5
 levels) response variable?
In-Reply-To: <CAFzZfa3nFOxY8WyscYBad7idDv=eo_v7fXetKBKtFPZ3k0uk+w@mail.gmail.com>
References: <CAFzZfa1imABAAqjozzwTpkWgVCaaHkWsGZKKC9aLKPCQ8sjMgw@mail.gmail.com>
 <e1e2c821-8add-7cfe-7c23-e4bea4ddb985@mpi.nl>
 <CAFzZfa3nFOxY8WyscYBad7idDv=eo_v7fXetKBKtFPZ3k0uk+w@mail.gmail.com>
Message-ID: <20190306141117.GD2816@info124.pharmacie.univ-paris5.fr>

Hello Nicolas,

Each of your individual item is a Bernoulli variable. So the
background seems in favor of a binomial law if you can assume

 - that probability of each of the 5 items of the score is the same
   (that is, p(? having ivy ?) = p(? having nettles ?) = ...

 - that these events are independant.

If both are true, then the total score should be exactly a binomial
variable. Otherwise, it's not clear that the result should be a
binomial. But it may be worth trying the results it gives!

On Wed, Mar 06, 2019 at 03:00:32PM +0100, Nicolas Deguines wrote:
? Hello Phillip and all,
? 
? Thanks a lot Phillip for your very interesting and useful answer, and for
? the paper from Liddell & Kruschke. It helps a lot.
? 
? About trying other link and threshold functions in clmm: no huge difference
? in my case unfortunately. I tried different combinations of each.
? 'equidistant' did do better, but the improvement was far from enough.
? 
? I computed density plots for my response variable as observed and as
? predicted from my lmer() model (similar to what Liddell and Kruschke do in
? Figure 6): the linear mixed-model does pretty well in fitting the data.
? => so I'd be enclined to trust the results from my lmer models in the
? present case (but Liddell and Kruschke did show very clear cases when a
? linear model fit very poorly the ordinal data).
? 
? Meanwhile, I thought of another alternative for analyzing this response
? variable and I would be curious to read what people may think about it.
? Before presenting that alternative, I need to say more about that 5-levels
? response variable.
? It is a score built by Muratet and Fontaine (2015)* to assess the
? naturalness of a given private backyard (it is shown to be correlated with
? higher abundance of butterflies).
? In the backyard: fallow area, nettles (*Urtica dioica*), ivy (*Hedera helix*),
? and brambles (*Rubus spp.*) are each scored one if present, and the
? naturalness index was computed as the sum of these scores.
? => it results in a 5-levels ordinal variable because it can go from 0 to 4,
? and each increase in 1 means a backyard with more features of 'naturalness'.
? I wonder thus if this could be modelled using a glmer() with family =
? binomial and feeding to the model two columns: cbind(sum of 1's, sum of
? 0's) (see R documentation for family{stats}, in the Details: "*As a
? two-column integer matrix: the first column gives the number of successes
? and the second the number of failures.*")
? I will try and see how the model fit the data. But I would be interested in
? getting a theoretical opinion.
? 
? I hope this can help others too
? 
? Best regards,
? Nicolas Deguines
? 
? *
? https://www.sciencedirect.com/science/article/abs/pii/S0006320714004704?via%3Dihub
? 
? ----------------------------------
? Postdoctoral Research Associate
? Laboratoire Ecologie, Syst?matique et Evolution
? Universit? Paris Sud, Orsay, France
? Website: http://nicolasdeguines.weebly.com/
? 
? 
? On Tue, 5 Mar 2019 at 13:04, Phillip Alday <phillip.alday at mpi.nl> wrote:
? 
? > Hi Nicolas,
? >
? > How much you can get away bending the assumptions depends in some ways
? > on how well the resulting model fits your data. If the resulting model
? > is a poor fit, then it's not a great model for performing inference. The
? > other problem with bending assumptions is that a lot of 'error
? > statistics' (standard errors, t-values, and basically anything related
? > to significance testings) aren't guaranteed to do what they are supposed
? > to do. (In your case, the good behavior of your residuals suggests that
? > this won't be a huge problem, but there are no promises.)
? >
? > You can get around this a bit by doing things like cross-validation or
? > other inferential steps based on how well the model generalizes to /
? > predicts new data instead of significance testing of coefficients or
? > linear hypotheses.
? >
? > John Kruschke has written about this issue at some length and seems
? > convinced that it's (almost) always a bad idea to bend the
? > metric/continuous assumption when dealing with ordinal data:
? >
? >
? > http://doingbayesiandataanalysis.blogspot.com/2017/12/which-movie-is-rated-better-dont-treat.html
? >
? >
? > http://doingbayesiandataanalysis.blogspot.com/2018/09/analyzing-ordinal-data-with-metric.html
? >
? > The latter is largely a link/"press release" for the associated paper:
? >
? > Liddell, T. M., & Kruschke, J. K. (2018). Analyzing ordinal data with
? > metric models: What could possibly go wrong? Journal of Experimental
? > Social Psychology , 79 , 328?348. doi:10.1016/j.jesp.2018.08.009
? >
? > Finally, have you tried other link and threshold functions in clmm?
? > Those can make a huge difference!
? >
? > Phillip
? >
? > On 5/3/19 11:00 am, Nicolas Deguines wrote:
? > > Hello everyone,
? > >
? > > I am investigating how engagement into a citizen science program can
? > change
? > > participants' behavior in terms of implementing gardening techniques
? > > benefitting biodiversity.
? > > There are 2362 participants, distributed into 7 cohorts (= year in which
? > > they joined the program), and I have repeated gardening technique
? > > information for multiple years for each participant.
? > > So I need to use mixed modeling.
? > >
? > > One of the response variable is a score that can takes 5 values: 0, 1, 2,
? > > 3, or 4. It's ordered, it's not continuous (there are 5 levels).
? > > I would analyze this into a cumulative link mixed models (using clmm()
? > from
? > > ordinal package) but the Hessian condition I obtained with such model is
? > >
? > > 5.0e+06. I.e. assumption is violated (simplifying my initial full model
? > did
? > > not help at all).
? > >
? > > As an alternative, I am wondering if I could treat this response variable
? > > has a continuous one into a lmer() model.
? > > When I do:
? > > - Normality of model residuals is nicely met
? > > - Homoscedasticity of model residuals is met as well.
? > > => does meeting these two assumptions is enough to validate the use of a
? > > lmer() model for an ordered categorical response variable?
? > >
? > > In one of Douglas Bates' presentation (slide 3 of Jan. 2011, Madison:
? > > http://lme4.r-forge.r-project.org/slides/2011-01-11-Madison/5GLMM.pdf),
? > it
? > > is said that
? > > "When using LMMs we assume that the response being modeled is on a
? > > continuous scale.
? > > Sometimes we can bend this assumption a bit if the response is an ordinal
? > > response with a moderate to large number of levels.
? > > For example, [...a response variable taking] integer values on the scale
? > of
? > > 1 to 10."
? > > => is 5 levels too few to be treated as continuous? Or would it be ok
? > given
? > > residuals behave nicely?
? > >
? > > I would appreciate any help and thoughts on this.
? > > I checked that this was not treated in a previous post and I hope I did
? > not
? > > miss it (sorry if I did).
? > >
? > > Best,
? > > Nicolas Deguines
? > > ----------------------------------
? > > Postdoctoral Research Associate
? > > Laboratoire Ecologie, Syst?matique et Evolution
? > > Universit? Paris Sud, Orsay, France
? > > Website: http://nicolasdeguines.weebly.com/
? > >
? > >       [[alternative HTML version deleted]]
? > >
? > > _______________________________________________
? > > R-sig-mixed-models at r-project.org mailing list
? > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
? > >
? >
? 
? 	[[alternative HTML version deleted]]
? 
? _______________________________________________
? R-sig-mixed-models at r-project.org mailing list
? https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
                                Emmanuel CURIS
                                emmanuel.curis at parisdescartes.fr

Page WWW: http://emmanuel.curis.online.fr/index.html


From d@r|zopou|o@ @end|ng |rom er@@mu@mc@n|  Wed Mar  6 15:11:43 2019
From: d@r|zopou|o@ @end|ng |rom er@@mu@mc@n| (D. Rizopoulos)
Date: Wed, 6 Mar 2019 14:11:43 +0000
Subject: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5
 levels) response variable?
In-Reply-To: <CAFzZfa3nFOxY8WyscYBad7idDv=eo_v7fXetKBKtFPZ3k0uk+w@mail.gmail.com>
References: <CAFzZfa1imABAAqjozzwTpkWgVCaaHkWsGZKKC9aLKPCQ8sjMgw@mail.gmail.com>
 <e1e2c821-8add-7cfe-7c23-e4bea4ddb985@mpi.nl>
 <CAFzZfa3nFOxY8WyscYBad7idDv=eo_v7fXetKBKtFPZ3k0uk+w@mail.gmail.com>
Message-ID: <df6e8e09-5348-a2f6-a388-5a15352b3c1e@erasmusmc.nl>

Hi Nicolas,

You could instead use a continuation ratio model that entails a couple 
of data management steps to transform your ordinal response variable 
into a binary one, and fit it with a mixed effects logistic regression. 
In addition, under this is model it is straightforward to assess/relax 
the ordinality assumption (i.e., that the effect of the predictors is 
the same for levels of the ordinal response).

You can find an example using the GLMMadaptive package here: 
https://drizopoulos.github.io/GLMMadaptive/articles/Ordinal_Mixed_Models.html

Best,
Dimitris


On 3/6/2019 3:00 PM, Nicolas Deguines wrote:
> Hello Phillip and all,
> 
> Thanks a lot Phillip for your very interesting and useful answer, and for
> the paper from Liddell & Kruschke. It helps a lot.
> 
> About trying other link and threshold functions in clmm: no huge difference
> in my case unfortunately. I tried different combinations of each.
> 'equidistant' did do better, but the improvement was far from enough.
> 
> I computed density plots for my response variable as observed and as
> predicted from my lmer() model (similar to what Liddell and Kruschke do in
> Figure 6): the linear mixed-model does pretty well in fitting the data.
> => so I'd be enclined to trust the results from my lmer models in the
> present case (but Liddell and Kruschke did show very clear cases when a
> linear model fit very poorly the ordinal data).
> 
> Meanwhile, I thought of another alternative for analyzing this response
> variable and I would be curious to read what people may think about it.
> Before presenting that alternative, I need to say more about that 5-levels
> response variable.
> It is a score built by Muratet and Fontaine (2015)* to assess the
> naturalness of a given private backyard (it is shown to be correlated with
> higher abundance of butterflies).
> In the backyard: fallow area, nettles (*Urtica dioica*), ivy (*Hedera helix*),
> and brambles (*Rubus spp.*) are each scored one if present, and the
> naturalness index was computed as the sum of these scores.
> => it results in a 5-levels ordinal variable because it can go from 0 to 4,
> and each increase in 1 means a backyard with more features of 'naturalness'.
> I wonder thus if this could be modelled using a glmer() with family =
> binomial and feeding to the model two columns: cbind(sum of 1's, sum of
> 0's) (see R documentation for family{stats}, in the Details: "*As a
> two-column integer matrix: the first column gives the number of successes
> and the second the number of failures.*")
> I will try and see how the model fit the data. But I would be interested in
> getting a theoretical opinion.
> 
> I hope this can help others too
> 
> Best regards,
> Nicolas Deguines
> 
> *
> https://emea01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.sciencedirect.com%2Fscience%2Farticle%2Fabs%2Fpii%2FS0006320714004704%3Fvia%253Dihub&amp;data=02%7C01%7Cd.rizopoulos%40erasmusmc.nl%7C6be346dd1ef044242f7008d6a23c415f%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C1%7C636874777051485248&amp;sdata=n7z7lU50tmKYG6goqLHnw9HrMSsIXfTPYvSO8XYE0yM%3D&amp;reserved=0
> 
> ----------------------------------
> Postdoctoral Research Associate
> Laboratoire Ecologie, Syst?matique et Evolution
> Universit? Paris Sud, Orsay, France
> Website: https://emea01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fnicolasdeguines.weebly.com%2F&amp;data=02%7C01%7Cd.rizopoulos%40erasmusmc.nl%7C6be346dd1ef044242f7008d6a23c415f%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C1%7C636874777051485248&amp;sdata=tT40Zd6AKGsntDjw2cJ1j8lXnpIsF2ILVQIkoQQ9NtA%3D&amp;reserved=0
> 
> 
> On Tue, 5 Mar 2019 at 13:04, Phillip Alday <phillip.alday at mpi.nl> wrote:
> 
>> Hi Nicolas,
>>
>> How much you can get away bending the assumptions depends in some ways
>> on how well the resulting model fits your data. If the resulting model
>> is a poor fit, then it's not a great model for performing inference. The
>> other problem with bending assumptions is that a lot of 'error
>> statistics' (standard errors, t-values, and basically anything related
>> to significance testings) aren't guaranteed to do what they are supposed
>> to do. (In your case, the good behavior of your residuals suggests that
>> this won't be a huge problem, but there are no promises.)
>>
>> You can get around this a bit by doing things like cross-validation or
>> other inferential steps based on how well the model generalizes to /
>> predicts new data instead of significance testing of coefficients or
>> linear hypotheses.
>>
>> John Kruschke has written about this issue at some length and seems
>> convinced that it's (almost) always a bad idea to bend the
>> metric/continuous assumption when dealing with ordinal data:
>>
>>
>> https://emea01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fdoingbayesiandataanalysis.blogspot.com%2F2017%2F12%2Fwhich-movie-is-rated-better-dont-treat.html&amp;data=02%7C01%7Cd.rizopoulos%40erasmusmc.nl%7C6be346dd1ef044242f7008d6a23c415f%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C1%7C636874777051485248&amp;sdata=cQnJprVCR3EvfNaD8Rb6fjs0eDipjI2heMflyne8%2F4Y%3D&amp;reserved=0
>>
>>
>> https://emea01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fdoingbayesiandataanalysis.blogspot.com%2F2018%2F09%2Fanalyzing-ordinal-data-with-metric.html&amp;data=02%7C01%7Cd.rizopoulos%40erasmusmc.nl%7C6be346dd1ef044242f7008d6a23c415f%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C1%7C636874777051495257&amp;sdata=Oy2ddDyCDIiFOWB9p%2Fwhd5x%2FImpc4cTollEqDA%2Fh1Yk%3D&amp;reserved=0
>>
>> The latter is largely a link/"press release" for the associated paper:
>>
>> Liddell, T. M., & Kruschke, J. K. (2018). Analyzing ordinal data with
>> metric models: What could possibly go wrong? Journal of Experimental
>> Social Psychology , 79 , 328?348. doi:10.1016/j.jesp.2018.08.009
>>
>> Finally, have you tried other link and threshold functions in clmm?
>> Those can make a huge difference!
>>
>> Phillip
>>
>> On 5/3/19 11:00 am, Nicolas Deguines wrote:
>>> Hello everyone,
>>>
>>> I am investigating how engagement into a citizen science program can
>> change
>>> participants' behavior in terms of implementing gardening techniques
>>> benefitting biodiversity.
>>> There are 2362 participants, distributed into 7 cohorts (= year in which
>>> they joined the program), and I have repeated gardening technique
>>> information for multiple years for each participant.
>>> So I need to use mixed modeling.
>>>
>>> One of the response variable is a score that can takes 5 values: 0, 1, 2,
>>> 3, or 4. It's ordered, it's not continuous (there are 5 levels).
>>> I would analyze this into a cumulative link mixed models (using clmm()
>> from
>>> ordinal package) but the Hessian condition I obtained with such model is
>>>
>>> 5.0e+06. I.e. assumption is violated (simplifying my initial full model
>> did
>>> not help at all).
>>>
>>> As an alternative, I am wondering if I could treat this response variable
>>> has a continuous one into a lmer() model.
>>> When I do:
>>> - Normality of model residuals is nicely met
>>> - Homoscedasticity of model residuals is met as well.
>>> => does meeting these two assumptions is enough to validate the use of a
>>> lmer() model for an ordered categorical response variable?
>>>
>>> In one of Douglas Bates' presentation (slide 3 of Jan. 2011, Madison:
>>> https://emea01.safelinks.protection.outlook.com/?url=http%3A%2F%2Flme4.r-forge.r-project.org%2Fslides%2F2011-01-11-Madison%2F5GLMM.pdf&amp;data=02%7C01%7Cd.rizopoulos%40erasmusmc.nl%7C6be346dd1ef044242f7008d6a23c415f%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C1%7C636874777051495257&amp;sdata=A0huudQxBTempIcQXjHS%2BfZWPTsHl3ZiDKMEX0B2Z%2B4%3D&amp;reserved=0),
>> it
>>> is said that
>>> "When using LMMs we assume that the response being modeled is on a
>>> continuous scale.
>>> Sometimes we can bend this assumption a bit if the response is an ordinal
>>> response with a moderate to large number of levels.
>>> For example, [...a response variable taking] integer values on the scale
>> of
>>> 1 to 10."
>>> => is 5 levels too few to be treated as continuous? Or would it be ok
>> given
>>> residuals behave nicely?
>>>
>>> I would appreciate any help and thoughts on this.
>>> I checked that this was not treated in a previous post and I hope I did
>> not
>>> miss it (sorry if I did).
>>>
>>> Best,
>>> Nicolas Deguines
>>> ----------------------------------
>>> Postdoctoral Research Associate
>>> Laboratoire Ecologie, Syst?matique et Evolution
>>> Universit? Paris Sud, Orsay, France
>>> Website: https://emea01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fnicolasdeguines.weebly.com%2F&amp;data=02%7C01%7Cd.rizopoulos%40erasmusmc.nl%7C6be346dd1ef044242f7008d6a23c415f%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C1%7C636874777051495257&amp;sdata=PFyamOaNafTF9tACqTx1y%2BNH6q34P4m%2B9waT4nng2VY%3D&amp;reserved=0
>>>
>>>        [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://emea01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-mixed-models&amp;data=02%7C01%7Cd.rizopoulos%40erasmusmc.nl%7C6be346dd1ef044242f7008d6a23c415f%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C1%7C636874777051495257&amp;sdata=Jx%2FX%2BiPkf24jsL32CWAkt48CSLnaHac9SoUwlvsEQsc%3D&amp;reserved=0
>>>
>>
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://emea01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-mixed-models&amp;data=02%7C01%7Cd.rizopoulos%40erasmusmc.nl%7C6be346dd1ef044242f7008d6a23c415f%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C1%7C636874777051495257&amp;sdata=Jx%2FX%2BiPkf24jsL32CWAkt48CSLnaHac9SoUwlvsEQsc%3D&amp;reserved=0
> 

-- 
Dimitris Rizopoulos
Professor of Biostatistics
Department of Biostatistics
Erasmus University Medical Center

Address: PO Box 2040, 3000 CA Rotterdam, the Netherlands
Tel: +31/(0)10/7043478
Fax: +31/(0)10/7043014
Web (personal): http://www.drizopoulos.com/
Web (work): http://www.erasmusmc.nl/biostatistiek/
Blog: http://iprogn.blogspot.nl/

From p|erce@1 @end|ng |rom m@u@edu  Wed Mar  6 15:37:54 2019
From: p|erce@1 @end|ng |rom m@u@edu (Pierce, Steven)
Date: Wed, 6 Mar 2019 14:37:54 +0000
Subject: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5
 levels) response variable?
In-Reply-To: <CAFzZfa3nFOxY8WyscYBad7idDv=eo_v7fXetKBKtFPZ3k0uk+w@mail.gmail.com>
References: <CAFzZfa1imABAAqjozzwTpkWgVCaaHkWsGZKKC9aLKPCQ8sjMgw@mail.gmail.com>
 <e1e2c821-8add-7cfe-7c23-e4bea4ddb985@mpi.nl>
 <CAFzZfa3nFOxY8WyscYBad7idDv=eo_v7fXetKBKtFPZ3k0uk+w@mail.gmail.com>
Message-ID: <DM6PR12MB2732EC4918306F6B1E31EC8681730@DM6PR12MB2732.namprd12.prod.outlook.com>

Nicolas,

So your score is constructed by summing 4 binary variables, each of which represents a feature of a natural environment. This is similar to how social science researchers use survey data with binary questions that get combined into scale scores. Such scale scores are usually intended to measure latent variables. 

By taking a simple sum of the binary items as the scale score, you are making strong assumptions about each item being equally important to measuring the latent variable. That's a testable hypothesis. Consider doing a confirmatory factor analysis using the 4 items as indicators of a latent variable (naturalness) that is assumed to be continuous and normally distributed. The point of this is to get a decent measurement model for the latent variable. 

Allow the factor loadings to vary across items (some may be more important to measuring the latent variable than others); also test for whether there are correlations between the item residuals that are not explained by being indicators of the latent variable. If you can achieve adequate CFA model fit and composite reliability for the latent variable (i.e., naturalness), then you have the ability to save out factor score estimates of the latent variable that should be continuous, normally distribute values. 

Then you can either take those factor score estimates and plug them into your mixed model, or just expand from a confirmatory factor analysis model into a full structural equation model that includes all the elements of the desired mixed model, with the latent "naturalness" variable as the outcome. The latter approach is more theoretically desirable because it reduces bias due to measurement error. 

Steven J. Pierce, Ph.D.
Acting Director; Associate Director
Center for Statistical Training & Consulting (CSTAT)
Michigan State University
E-mail: pierces1 at msu

-----Original Message-----
From: Nicolas Deguines <nicodeguines at gmail.com> 
Sent: Wednesday, March 6, 2019 9:01 AM
To: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5 levels) response variable?

Hello Phillip and all,

Thanks a lot Phillip for your very interesting and useful answer, and for
the paper from Liddell & Kruschke. It helps a lot.

About trying other link and threshold functions in clmm: no huge difference
in my case unfortunately. I tried different combinations of each.
'equidistant' did do better, but the improvement was far from enough.

I computed density plots for my response variable as observed and as
predicted from my lmer() model (similar to what Liddell and Kruschke do in
Figure 6): the linear mixed-model does pretty well in fitting the data.
=> so I'd be enclined to trust the results from my lmer models in the
present case (but Liddell and Kruschke did show very clear cases when a
linear model fit very poorly the ordinal data).

Meanwhile, I thought of another alternative for analyzing this response
variable and I would be curious to read what people may think about it.
Before presenting that alternative, I need to say more about that 5-levels
response variable.
It is a score built by Muratet and Fontaine (2015)* to assess the
naturalness of a given private backyard (it is shown to be correlated with
higher abundance of butterflies).
In the backyard: fallow area, nettles (*Urtica dioica*), ivy (*Hedera helix*),
and brambles (*Rubus spp.*) are each scored one if present, and the
naturalness index was computed as the sum of these scores.
=> it results in a 5-levels ordinal variable because it can go from 0 to 4,
and each increase in 1 means a backyard with more features of 'naturalness'.
I wonder thus if this could be modelled using a glmer() with family =
binomial and feeding to the model two columns: cbind(sum of 1's, sum of
0's) (see R documentation for family{stats}, in the Details: "*As a
two-column integer matrix: the first column gives the number of successes
and the second the number of failures.*")
I will try and see how the model fit the data. But I would be interested in
getting a theoretical opinion.

I hope this can help others too

Best regards,
Nicolas Deguines

*
https://urldefense.proofpoint.com/v2/url?u=https-3A__www.sciencedirect.com_science_article_abs_pii_S0006320714004704-3Fvia-253Dihub&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=y9qFrcE-42RfdxSyVs_fJu4DwgBIEjj1jh2TimnhlHI&e=

----------------------------------
Postdoctoral Research Associate
Laboratoire Ecologie, Syst?matique et Evolution
Universit? Paris Sud, Orsay, France
Website: https://urldefense.proofpoint.com/v2/url?u=http-3A__nicolasdeguines.weebly.com_&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=umRU32TJsniNhkwfbYgkiGyv_zmKT1LJU9qtyEGdVXg&e=


On Tue, 5 Mar 2019 at 13:04, Phillip Alday <phillip.alday at mpi.nl> wrote:

> Hi Nicolas,
>
> How much you can get away bending the assumptions depends in some ways
> on how well the resulting model fits your data. If the resulting model
> is a poor fit, then it's not a great model for performing inference. The
> other problem with bending assumptions is that a lot of 'error
> statistics' (standard errors, t-values, and basically anything related
> to significance testings) aren't guaranteed to do what they are supposed
> to do. (In your case, the good behavior of your residuals suggests that
> this won't be a huge problem, but there are no promises.)
>
> You can get around this a bit by doing things like cross-validation or
> other inferential steps based on how well the model generalizes to /
> predicts new data instead of significance testing of coefficients or
> linear hypotheses.
>
> John Kruschke has written about this issue at some length and seems
> convinced that it's (almost) always a bad idea to bend the
> metric/continuous assumption when dealing with ordinal data:
>
>
> https://urldefense.proofpoint.com/v2/url?u=http-3A__doingbayesiandataanalysis.blogspot.com_2017_12_which-2Dmovie-2Dis-2Drated-2Dbetter-2Ddont-2Dtreat.html&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=cTggjZQJ8ytLt2Oe_1EsE6augDSd6GEeTbNeGKtCtjA&e=
>
>
> https://urldefense.proofpoint.com/v2/url?u=http-3A__doingbayesiandataanalysis.blogspot.com_2018_09_analyzing-2Dordinal-2Ddata-2Dwith-2Dmetric.html&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=mhVwPoXW5lRnvpxuImiQd-DfcbkWvVrws6eDG9BhW4I&e=
>
> The latter is largely a link/"press release" for the associated paper:
>
> Liddell, T. M., & Kruschke, J. K. (2018). Analyzing ordinal data with
> metric models: What could possibly go wrong? Journal of Experimental
> Social Psychology , 79 , 328?348. doi:10.1016/j.jesp.2018.08.009
>
> Finally, have you tried other link and threshold functions in clmm?
> Those can make a huge difference!
>
> Phillip
>
> On 5/3/19 11:00 am, Nicolas Deguines wrote:
> > Hello everyone,
> >
> > I am investigating how engagement into a citizen science program can
> change
> > participants' behavior in terms of implementing gardening techniques
> > benefitting biodiversity.
> > There are 2362 participants, distributed into 7 cohorts (= year in which
> > they joined the program), and I have repeated gardening technique
> > information for multiple years for each participant.
> > So I need to use mixed modeling.
> >
> > One of the response variable is a score that can takes 5 values: 0, 1, 2,
> > 3, or 4. It's ordered, it's not continuous (there are 5 levels).
> > I would analyze this into a cumulative link mixed models (using clmm()
> from
> > ordinal package) but the Hessian condition I obtained with such model is
> >
> > 5.0e+06. I.e. assumption is violated (simplifying my initial full model
> did
> > not help at all).
> >
> > As an alternative, I am wondering if I could treat this response variable
> > has a continuous one into a lmer() model.
> > When I do:
> > - Normality of model residuals is nicely met
> > - Homoscedasticity of model residuals is met as well.
> > => does meeting these two assumptions is enough to validate the use of a
> > lmer() model for an ordered categorical response variable?
> >
> > In one of Douglas Bates' presentation (slide 3 of Jan. 2011, Madison:
> > https://urldefense.proofpoint.com/v2/url?u=http-3A__lme4.r-2Dforge.r-2Dproject.org_slides_2011-2D01-2D11-2DMadison_5GLMM.pdf&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=gQt2neGX64PqU4v8CFD2g0WdUb_EycwwXjSSo4dbLDY&e=),
> it
> > is said that
> > "When using LMMs we assume that the response being modeled is on a
> > continuous scale.
> > Sometimes we can bend this assumption a bit if the response is an ordinal
> > response with a moderate to large number of levels.
> > For example, [...a response variable taking] integer values on the scale
> of
> > 1 to 10."
> > => is 5 levels too few to be treated as continuous? Or would it be ok
> given
> > residuals behave nicely?
> >
> > I would appreciate any help and thoughts on this.
> > I checked that this was not treated in a previous post and I hope I did
> not
> > miss it (sorry if I did).
> >
> > Best,
> > Nicolas Deguines
> > ----------------------------------
> > Postdoctoral Research Associate
> > Laboratoire Ecologie, Syst?matique et Evolution
> > Universit? Paris Sud, Orsay, France
> > Website: https://urldefense.proofpoint.com/v2/url?u=http-3A__nicolasdeguines.weebly.com_&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=umRU32TJsniNhkwfbYgkiGyv_zmKT1LJU9qtyEGdVXg&e=
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dsig-2Dmixed-2Dmodels&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=v-ubYR1CYdeewDkWeQmOxEvkKJ4LF-vs8O0dNas2S8Q&e=
> >
>

	[[alternative HTML version deleted]]



From HDor@n @end|ng |rom @|r@org  Wed Mar  6 16:11:58 2019
From: HDor@n @end|ng |rom @|r@org (Doran, Harold)
Date: Wed, 6 Mar 2019 15:11:58 +0000
Subject: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5
 levels) response variable?
In-Reply-To: <DM6PR12MB2732EC4918306F6B1E31EC8681730@DM6PR12MB2732.namprd12.prod.outlook.com>
References: <CAFzZfa1imABAAqjozzwTpkWgVCaaHkWsGZKKC9aLKPCQ8sjMgw@mail.gmail.com>
 <e1e2c821-8add-7cfe-7c23-e4bea4ddb985@mpi.nl>
 <CAFzZfa3nFOxY8WyscYBad7idDv=eo_v7fXetKBKtFPZ3k0uk+w@mail.gmail.com>
 <DM6PR12MB2732EC4918306F6B1E31EC8681730@DM6PR12MB2732.namprd12.prod.outlook.com>
Message-ID: <BN7PR05MB585786EF2C9E4DA0AE49D9FACA730@BN7PR05MB5857.namprd05.prod.outlook.com>

@steven, no, this is not at all how survey researchers generate scaled scores. Scaled scores are MLEs of a likelihood function (or sometimes posterior mean/modes) constructed from the observed responses. 

To the OP, using lmer for the ordered response model is not an appropriate modeling strategy at all. Take a look at your predicted values. Are they < 0 or > 5? What does that mean?

-----Original Message-----
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of Pierce, Steven
Sent: Wednesday, March 06, 2019 9:38 AM
To: Nicolas Deguines <nicodeguines at gmail.com>
Cc: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5 levels) response variable?

Nicolas,

So your score is constructed by summing 4 binary variables, each of which represents a feature of a natural environment. This is similar to how social science researchers use survey data with binary questions that get combined into scale scores. Such scale scores are usually intended to measure latent variables. 

By taking a simple sum of the binary items as the scale score, you are making strong assumptions about each item being equally important to measuring the latent variable. That's a testable hypothesis. Consider doing a confirmatory factor analysis using the 4 items as indicators of a latent variable (naturalness) that is assumed to be continuous and normally distributed. The point of this is to get a decent measurement model for the latent variable. 

Allow the factor loadings to vary across items (some may be more important to measuring the latent variable than others); also test for whether there are correlations between the item residuals that are not explained by being indicators of the latent variable. If you can achieve adequate CFA model fit and composite reliability for the latent variable (i.e., naturalness), then you have the ability to save out factor score estimates of the latent variable that should be continuous, normally distribute values. 

Then you can either take those factor score estimates and plug them into your mixed model, or just expand from a confirmatory factor analysis model into a full structural equation model that includes all the elements of the desired mixed model, with the latent "naturalness" variable as the outcome. The latter approach is more theoretically desirable because it reduces bias due to measurement error. 

Steven J. Pierce, Ph.D.
Acting Director; Associate Director
Center for Statistical Training & Consulting (CSTAT) Michigan State University
E-mail: pierces1 at msu

-----Original Message-----
From: Nicolas Deguines <nicodeguines at gmail.com>
Sent: Wednesday, March 6, 2019 9:01 AM
To: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5 levels) response variable?

Hello Phillip and all,

Thanks a lot Phillip for your very interesting and useful answer, and for the paper from Liddell & Kruschke. It helps a lot.

About trying other link and threshold functions in clmm: no huge difference in my case unfortunately. I tried different combinations of each.
'equidistant' did do better, but the improvement was far from enough.

I computed density plots for my response variable as observed and as predicted from my lmer() model (similar to what Liddell and Kruschke do in Figure 6): the linear mixed-model does pretty well in fitting the data.
=> so I'd be enclined to trust the results from my lmer models in the present case (but Liddell and Kruschke did show very clear cases when a linear model fit very poorly the ordinal data).

Meanwhile, I thought of another alternative for analyzing this response variable and I would be curious to read what people may think about it.
Before presenting that alternative, I need to say more about that 5-levels response variable.
It is a score built by Muratet and Fontaine (2015)* to assess the naturalness of a given private backyard (it is shown to be correlated with higher abundance of butterflies).
In the backyard: fallow area, nettles (*Urtica dioica*), ivy (*Hedera helix*), and brambles (*Rubus spp.*) are each scored one if present, and the naturalness index was computed as the sum of these scores.
=> it results in a 5-levels ordinal variable because it can go from 0 to 4, and each increase in 1 means a backyard with more features of 'naturalness'.
I wonder thus if this could be modelled using a glmer() with family = binomial and feeding to the model two columns: cbind(sum of 1's, sum of
0's) (see R documentation for family{stats}, in the Details: "*As a two-column integer matrix: the first column gives the number of successes and the second the number of failures.*") I will try and see how the model fit the data. But I would be interested in getting a theoretical opinion.

I hope this can help others too

Best regards,
Nicolas Deguines

*
https://urldefense.proofpoint.com/v2/url?u=https-3A__www.sciencedirect.com_science_article_abs_pii_S0006320714004704-3Fvia-253Dihub&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=y9qFrcE-42RfdxSyVs_fJu4DwgBIEjj1jh2TimnhlHI&e=

----------------------------------
Postdoctoral Research Associate
Laboratoire Ecologie, Syst?matique et Evolution Universit? Paris Sud, Orsay, France
Website: https://urldefense.proofpoint.com/v2/url?u=http-3A__nicolasdeguines.weebly.com_&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=umRU32TJsniNhkwfbYgkiGyv_zmKT1LJU9qtyEGdVXg&e=


On Tue, 5 Mar 2019 at 13:04, Phillip Alday <phillip.alday at mpi.nl> wrote:

> Hi Nicolas,
>
> How much you can get away bending the assumptions depends in some ways 
> on how well the resulting model fits your data. If the resulting model 
> is a poor fit, then it's not a great model for performing inference. 
> The other problem with bending assumptions is that a lot of 'error 
> statistics' (standard errors, t-values, and basically anything related 
> to significance testings) aren't guaranteed to do what they are 
> supposed to do. (In your case, the good behavior of your residuals 
> suggests that this won't be a huge problem, but there are no 
> promises.)
>
> You can get around this a bit by doing things like cross-validation or 
> other inferential steps based on how well the model generalizes to / 
> predicts new data instead of significance testing of coefficients or 
> linear hypotheses.
>
> John Kruschke has written about this issue at some length and seems 
> convinced that it's (almost) always a bad idea to bend the 
> metric/continuous assumption when dealing with ordinal data:
>
>
> https://urldefense.proofpoint.com/v2/url?u=http-3A__doingbayesiandataa
> nalysis.blogspot.com_2017_12_which-2Dmovie-2Dis-2Drated-2Dbetter-2Ddon
> t-2Dtreat.html&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7Zip
> xyQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=cTggjZQJ8ytLt2Oe_1
> EsE6augDSd6GEeTbNeGKtCtjA&e=
>
>
> https://urldefense.proofpoint.com/v2/url?u=http-3A__doingbayesiandataa
> nalysis.blogspot.com_2018_09_analyzing-2Dordinal-2Ddata-2Dwith-2Dmetri
> c.html&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uL
> hmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=mhVwPoXW5lRnvpxuImiQd-Dfcb
> kWvVrws6eDG9BhW4I&e=
>
> The latter is largely a link/"press release" for the associated paper:
>
> Liddell, T. M., & Kruschke, J. K. (2018). Analyzing ordinal data with 
> metric models: What could possibly go wrong? Journal of Experimental 
> Social Psychology , 79 , 328?348. doi:10.1016/j.jesp.2018.08.009
>
> Finally, have you tried other link and threshold functions in clmm?
> Those can make a huge difference!
>
> Phillip
>
> On 5/3/19 11:00 am, Nicolas Deguines wrote:
> > Hello everyone,
> >
> > I am investigating how engagement into a citizen science program can
> change
> > participants' behavior in terms of implementing gardening techniques 
> > benefitting biodiversity.
> > There are 2362 participants, distributed into 7 cohorts (= year in 
> > which they joined the program), and I have repeated gardening 
> > technique information for multiple years for each participant.
> > So I need to use mixed modeling.
> >
> > One of the response variable is a score that can takes 5 values: 0, 
> > 1, 2, 3, or 4. It's ordered, it's not continuous (there are 5 levels).
> > I would analyze this into a cumulative link mixed models (using 
> > clmm()
> from
> > ordinal package) but the Hessian condition I obtained with such 
> > model is
> >
> > 5.0e+06. I.e. assumption is violated (simplifying my initial full 
> > model
> did
> > not help at all).
> >
> > As an alternative, I am wondering if I could treat this response 
> > variable has a continuous one into a lmer() model.
> > When I do:
> > - Normality of model residuals is nicely met
> > - Homoscedasticity of model residuals is met as well.
> > => does meeting these two assumptions is enough to validate the use 
> > of a
> > lmer() model for an ordered categorical response variable?
> >
> > In one of Douglas Bates' presentation (slide 3 of Jan. 2011, Madison:
> > https://urldefense.proofpoint.com/v2/url?u=http-3A__lme4.r-2Dforge.r
> > -2Dproject.org_slides_2011-2D01-2D11-2DMadison_5GLMM.pdf&d=DwIFaQ&c=
> > nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uLhmTfp88PWyhBj0bi
> > VanGF_OI9s5JNvn6ENYnsaenI&s=gQt2neGX64PqU4v8CFD2g0WdUb_EycwwXjSSo4db
> > LDY&e=),
> it
> > is said that
> > "When using LMMs we assume that the response being modeled is on a 
> > continuous scale.
> > Sometimes we can bend this assumption a bit if the response is an 
> > ordinal response with a moderate to large number of levels.
> > For example, [...a response variable taking] integer values on the 
> > scale
> of
> > 1 to 10."
> > => is 5 levels too few to be treated as continuous? Or would it be 
> > ok
> given
> > residuals behave nicely?
> >
> > I would appreciate any help and thoughts on this.
> > I checked that this was not treated in a previous post and I hope I 
> > did
> not
> > miss it (sorry if I did).
> >
> > Best,
> > Nicolas Deguines
> > ----------------------------------
> > Postdoctoral Research Associate
> > Laboratoire Ecologie, Syst?matique et Evolution Universit? Paris 
> > Sud, Orsay, France
> > Website: 
> > https://urldefense.proofpoint.com/v2/url?u=http-3A__nicolasdeguines.
> > weebly.com_&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7Zipx
> > yQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=umRU32TJsniNhkwfb
> > YgkiGyv_zmKT1LJU9qtyEGdVXg&e=
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list 
> > https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_ma
> > ilman_listinfo_r-2Dsig-2Dmixed-2Dmodels&d=DwIFaQ&c=nE__W8dFE-shTxStw
> > Xtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6E
> > NYnsaenI&s=v-ubYR1CYdeewDkWeQmOxEvkKJ4LF-vs8O0dNas2S8Q&e=
> >
>

	[[alternative HTML version deleted]]


_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From p|erce@1 @end|ng |rom m@u@edu  Wed Mar  6 17:04:09 2019
From: p|erce@1 @end|ng |rom m@u@edu (Pierce, Steven)
Date: Wed, 6 Mar 2019 16:04:09 +0000
Subject: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5
 levels) response variable?
In-Reply-To: <BN7PR05MB585786EF2C9E4DA0AE49D9FACA730@BN7PR05MB5857.namprd05.prod.outlook.com>
References: <CAFzZfa1imABAAqjozzwTpkWgVCaaHkWsGZKKC9aLKPCQ8sjMgw@mail.gmail.com>
 <e1e2c821-8add-7cfe-7c23-e4bea4ddb985@mpi.nl>
 <CAFzZfa3nFOxY8WyscYBad7idDv=eo_v7fXetKBKtFPZ3k0uk+w@mail.gmail.com>
 <DM6PR12MB2732EC4918306F6B1E31EC8681730@DM6PR12MB2732.namprd12.prod.outlook.com>
 <BN7PR05MB585786EF2C9E4DA0AE49D9FACA730@BN7PR05MB5857.namprd05.prod.outlook.com>
Message-ID: <DM6PR12MB27325EA202BF546CDE2BDAC381730@DM6PR12MB2732.namprd12.prod.outlook.com>

@Harold. There is a vast array of different ways that people analyze survey data. Some are far more appropriate and methodologically sound than others. Taking unweighted sums of binary items is indeed a very common practice for producing scale scores, though it is usually done without considering or testing the underlying measurement assumptions that it implies. I didn't endorse that overly simplistic practice but I have seen huge numbers of published papers that use it. More rigorous methods (such as I suggested) are reasonable ways to use observed responses to measure higher-order theoretical constructs while being thoughtful about the assumptions involved. My point was that one should be more thoughtful about the measurement process that yields the scores to be analyzed. A different approach to that step can change whether the score obtained is ordinal/count or legitimately continuous. 

Steve

-----Original Message-----
From: Doran, Harold <HDoran at air.org> 
Sent: Wednesday, March 6, 2019 10:12 AM
To: Pierce, Steven <pierces1 at msu.edu>; Nicolas Deguines <nicodeguines at gmail.com>
Cc: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
Subject: RE: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5 levels) response variable?

@steven, no, this is not at all how survey researchers generate scaled scores. Scaled scores are MLEs of a likelihood function (or sometimes posterior mean/modes) constructed from the observed responses. 

To the OP, using lmer for the ordered response model is not an appropriate modeling strategy at all. Take a look at your predicted values. Are they < 0 or > 5? What does that mean?

-----Original Message-----
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of Pierce, Steven
Sent: Wednesday, March 06, 2019 9:38 AM
To: Nicolas Deguines <nicodeguines at gmail.com>
Cc: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5 levels) response variable?

Nicolas,

So your score is constructed by summing 4 binary variables, each of which represents a feature of a natural environment. This is similar to how social science researchers use survey data with binary questions that get combined into scale scores. Such scale scores are usually intended to measure latent variables. 

By taking a simple sum of the binary items as the scale score, you are making strong assumptions about each item being equally important to measuring the latent variable. That's a testable hypothesis. Consider doing a confirmatory factor analysis using the 4 items as indicators of a latent variable (naturalness) that is assumed to be continuous and normally distributed. The point of this is to get a decent measurement model for the latent variable. 

Allow the factor loadings to vary across items (some may be more important to measuring the latent variable than others); also test for whether there are correlations between the item residuals that are not explained by being indicators of the latent variable. If you can achieve adequate CFA model fit and composite reliability for the latent variable (i.e., naturalness), then you have the ability to save out factor score estimates of the latent variable that should be continuous, normally distribute values. 

Then you can either take those factor score estimates and plug them into your mixed model, or just expand from a confirmatory factor analysis model into a full structural equation model that includes all the elements of the desired mixed model, with the latent "naturalness" variable as the outcome. The latter approach is more theoretically desirable because it reduces bias due to measurement error. 

Steven J. Pierce, Ph.D.
Acting Director; Associate Director
Center for Statistical Training & Consulting (CSTAT) Michigan State University
E-mail: pierces1 at msu

-----Original Message-----
From: Nicolas Deguines <nicodeguines at gmail.com>
Sent: Wednesday, March 6, 2019 9:01 AM
To: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5 levels) response variable?

Hello Phillip and all,

Thanks a lot Phillip for your very interesting and useful answer, and for the paper from Liddell & Kruschke. It helps a lot.

About trying other link and threshold functions in clmm: no huge difference in my case unfortunately. I tried different combinations of each.
'equidistant' did do better, but the improvement was far from enough.

I computed density plots for my response variable as observed and as predicted from my lmer() model (similar to what Liddell and Kruschke do in Figure 6): the linear mixed-model does pretty well in fitting the data.
=> so I'd be enclined to trust the results from my lmer models in the present case (but Liddell and Kruschke did show very clear cases when a linear model fit very poorly the ordinal data).

Meanwhile, I thought of another alternative for analyzing this response variable and I would be curious to read what people may think about it.
Before presenting that alternative, I need to say more about that 5-levels response variable.
It is a score built by Muratet and Fontaine (2015)* to assess the naturalness of a given private backyard (it is shown to be correlated with higher abundance of butterflies).
In the backyard: fallow area, nettles (*Urtica dioica*), ivy (*Hedera helix*), and brambles (*Rubus spp.*) are each scored one if present, and the naturalness index was computed as the sum of these scores.
=> it results in a 5-levels ordinal variable because it can go from 0 to 4, and each increase in 1 means a backyard with more features of 'naturalness'.
I wonder thus if this could be modelled using a glmer() with family = binomial and feeding to the model two columns: cbind(sum of 1's, sum of
0's) (see R documentation for family{stats}, in the Details: "*As a two-column integer matrix: the first column gives the number of successes and the second the number of failures.*") I will try and see how the model fit the data. But I would be interested in getting a theoretical opinion.

I hope this can help others too

Best regards,
Nicolas Deguines

*
https://urldefense.proofpoint.com/v2/url?u=https-3A__www.sciencedirect.com_science_article_abs_pii_S0006320714004704-3Fvia-253Dihub&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=y9qFrcE-42RfdxSyVs_fJu4DwgBIEjj1jh2TimnhlHI&e=

----------------------------------
Postdoctoral Research Associate
Laboratoire Ecologie, Syst?matique et Evolution Universit? Paris Sud, Orsay, France
Website: https://urldefense.proofpoint.com/v2/url?u=http-3A__nicolasdeguines.weebly.com_&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=umRU32TJsniNhkwfbYgkiGyv_zmKT1LJU9qtyEGdVXg&e=


On Tue, 5 Mar 2019 at 13:04, Phillip Alday <phillip.alday at mpi.nl> wrote:

> Hi Nicolas,
>
> How much you can get away bending the assumptions depends in some ways 
> on how well the resulting model fits your data. If the resulting model 
> is a poor fit, then it's not a great model for performing inference. 
> The other problem with bending assumptions is that a lot of 'error 
> statistics' (standard errors, t-values, and basically anything related 
> to significance testings) aren't guaranteed to do what they are 
> supposed to do. (In your case, the good behavior of your residuals 
> suggests that this won't be a huge problem, but there are no 
> promises.)
>
> You can get around this a bit by doing things like cross-validation or 
> other inferential steps based on how well the model generalizes to / 
> predicts new data instead of significance testing of coefficients or 
> linear hypotheses.
>
> John Kruschke has written about this issue at some length and seems 
> convinced that it's (almost) always a bad idea to bend the 
> metric/continuous assumption when dealing with ordinal data:
>
>
> https://urldefense.proofpoint.com/v2/url?u=http-3A__doingbayesiandataa
> nalysis.blogspot.com_2017_12_which-2Dmovie-2Dis-2Drated-2Dbetter-2Ddon
> t-2Dtreat.html&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7Zip
> xyQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=cTggjZQJ8ytLt2Oe_1
> EsE6augDSd6GEeTbNeGKtCtjA&e=
>
>
> https://urldefense.proofpoint.com/v2/url?u=http-3A__doingbayesiandataa
> nalysis.blogspot.com_2018_09_analyzing-2Dordinal-2Ddata-2Dwith-2Dmetri
> c.html&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uL
> hmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=mhVwPoXW5lRnvpxuImiQd-Dfcb
> kWvVrws6eDG9BhW4I&e=
>
> The latter is largely a link/"press release" for the associated paper:
>
> Liddell, T. M., & Kruschke, J. K. (2018). Analyzing ordinal data with 
> metric models: What could possibly go wrong? Journal of Experimental 
> Social Psychology , 79 , 328?348. doi:10.1016/j.jesp.2018.08.009
>
> Finally, have you tried other link and threshold functions in clmm?
> Those can make a huge difference!
>
> Phillip
>
> On 5/3/19 11:00 am, Nicolas Deguines wrote:
> > Hello everyone,
> >
> > I am investigating how engagement into a citizen science program can
> change
> > participants' behavior in terms of implementing gardening techniques 
> > benefitting biodiversity.
> > There are 2362 participants, distributed into 7 cohorts (= year in 
> > which they joined the program), and I have repeated gardening 
> > technique information for multiple years for each participant.
> > So I need to use mixed modeling.
> >
> > One of the response variable is a score that can takes 5 values: 0, 
> > 1, 2, 3, or 4. It's ordered, it's not continuous (there are 5 levels).
> > I would analyze this into a cumulative link mixed models (using 
> > clmm()
> from
> > ordinal package) but the Hessian condition I obtained with such 
> > model is
> >
> > 5.0e+06. I.e. assumption is violated (simplifying my initial full 
> > model
> did
> > not help at all).
> >
> > As an alternative, I am wondering if I could treat this response 
> > variable has a continuous one into a lmer() model.
> > When I do:
> > - Normality of model residuals is nicely met
> > - Homoscedasticity of model residuals is met as well.
> > => does meeting these two assumptions is enough to validate the use 
> > of a
> > lmer() model for an ordered categorical response variable?
> >
> > In one of Douglas Bates' presentation (slide 3 of Jan. 2011, Madison:
> > https://urldefense.proofpoint.com/v2/url?u=http-3A__lme4.r-2Dforge.r
> > -2Dproject.org_slides_2011-2D01-2D11-2DMadison_5GLMM.pdf&d=DwIFaQ&c=
> > nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uLhmTfp88PWyhBj0bi
> > VanGF_OI9s5JNvn6ENYnsaenI&s=gQt2neGX64PqU4v8CFD2g0WdUb_EycwwXjSSo4db
> > LDY&e=),
> it
> > is said that
> > "When using LMMs we assume that the response being modeled is on a 
> > continuous scale.
> > Sometimes we can bend this assumption a bit if the response is an 
> > ordinal response with a moderate to large number of levels.
> > For example, [...a response variable taking] integer values on the 
> > scale
> of
> > 1 to 10."
> > => is 5 levels too few to be treated as continuous? Or would it be 
> > ok
> given
> > residuals behave nicely?
> >
> > I would appreciate any help and thoughts on this.
> > I checked that this was not treated in a previous post and I hope I 
> > did
> not
> > miss it (sorry if I did).
> >
> > Best,
> > Nicolas Deguines
> > ----------------------------------
> > Postdoctoral Research Associate
> > Laboratoire Ecologie, Syst?matique et Evolution Universit? Paris 
> > Sud, Orsay, France
> > Website: 
> > https://urldefense.proofpoint.com/v2/url?u=http-3A__nicolasdeguines.
> > weebly.com_&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7Zipx
> > yQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=umRU32TJsniNhkwfb
> > YgkiGyv_zmKT1LJU9qtyEGdVXg&e=
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list 
> > https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_ma
> > ilman_listinfo_r-2Dsig-2Dmixed-2Dmodels&d=DwIFaQ&c=nE__W8dFE-shTxStw
> > Xtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6E
> > NYnsaenI&s=v-ubYR1CYdeewDkWeQmOxEvkKJ4LF-vs8O0dNas2S8Q&e=
> >
>

	[[alternative HTML version deleted]]


_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dsig-2Dmixed-2Dmodels&d=DwIGaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=bJ2WeOugjTjdNv0JHhX3mbV-NYORMqKlREE07FtWxDk&s=NIMa3Dk2PuImseyI9fe9_mHHtE_cVvs8yVALMBHOHQc&e=

From HDor@n @end|ng |rom @|r@org  Wed Mar  6 17:12:18 2019
From: HDor@n @end|ng |rom @|r@org (Doran, Harold)
Date: Wed, 6 Mar 2019 16:12:18 +0000
Subject: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5
 levels) response variable?
In-Reply-To: <DM6PR12MB27325EA202BF546CDE2BDAC381730@DM6PR12MB2732.namprd12.prod.outlook.com>
References: <CAFzZfa1imABAAqjozzwTpkWgVCaaHkWsGZKKC9aLKPCQ8sjMgw@mail.gmail.com>
 <e1e2c821-8add-7cfe-7c23-e4bea4ddb985@mpi.nl>
 <CAFzZfa3nFOxY8WyscYBad7idDv=eo_v7fXetKBKtFPZ3k0uk+w@mail.gmail.com>
 <DM6PR12MB2732EC4918306F6B1E31EC8681730@DM6PR12MB2732.namprd12.prod.outlook.com>
 <BN7PR05MB585786EF2C9E4DA0AE49D9FACA730@BN7PR05MB5857.namprd05.prod.outlook.com>
 <DM6PR12MB27325EA202BF546CDE2BDAC381730@DM6PR12MB2732.namprd12.prod.outlook.com>
Message-ID: <BN7PR05MB5857D7662034FDDED896C44ACA730@BN7PR05MB5857.namprd05.prod.outlook.com>

No, this is absolutely incorrect. Sums of the binary variables are the *raw* scores and these are *not* scaled scores. Psychometric practice for developing scaled scores does indeed have many methods (as the author of a published test scoring software system, I've included most psychometric approaches), but the sums  as you suggest are not one of them.

-----Original Message-----
From: Pierce, Steven <pierces1 at msu.edu> 
Sent: Wednesday, March 06, 2019 11:04 AM
To: Doran, Harold <HDoran at air.org>; Nicolas Deguines <nicodeguines at gmail.com>
Cc: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
Subject: RE: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5 levels) response variable?

@Harold. There is a vast array of different ways that people analyze survey data. Some are far more appropriate and methodologically sound than others. Taking unweighted sums of binary items is indeed a very common practice for producing scale scores, though it is usually done without considering or testing the underlying measurement assumptions that it implies. I didn't endorse that overly simplistic practice but I have seen huge numbers of published papers that use it. More rigorous methods (such as I suggested) are reasonable ways to use observed responses to measure higher-order theoretical constructs while being thoughtful about the assumptions involved. My point was that one should be more thoughtful about the measurement process that yields the scores to be analyzed. A different approach to that step can change whether the score obtained is ordinal/count or legitimately continuous. 

Steve

-----Original Message-----
From: Doran, Harold <HDoran at air.org>
Sent: Wednesday, March 6, 2019 10:12 AM
To: Pierce, Steven <pierces1 at msu.edu>; Nicolas Deguines <nicodeguines at gmail.com>
Cc: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
Subject: RE: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5 levels) response variable?

@steven, no, this is not at all how survey researchers generate scaled scores. Scaled scores are MLEs of a likelihood function (or sometimes posterior mean/modes) constructed from the observed responses. 

To the OP, using lmer for the ordered response model is not an appropriate modeling strategy at all. Take a look at your predicted values. Are they < 0 or > 5? What does that mean?

-----Original Message-----
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of Pierce, Steven
Sent: Wednesday, March 06, 2019 9:38 AM
To: Nicolas Deguines <nicodeguines at gmail.com>
Cc: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5 levels) response variable?

Nicolas,

So your score is constructed by summing 4 binary variables, each of which represents a feature of a natural environment. This is similar to how social science researchers use survey data with binary questions that get combined into scale scores. Such scale scores are usually intended to measure latent variables. 

By taking a simple sum of the binary items as the scale score, you are making strong assumptions about each item being equally important to measuring the latent variable. That's a testable hypothesis. Consider doing a confirmatory factor analysis using the 4 items as indicators of a latent variable (naturalness) that is assumed to be continuous and normally distributed. The point of this is to get a decent measurement model for the latent variable. 

Allow the factor loadings to vary across items (some may be more important to measuring the latent variable than others); also test for whether there are correlations between the item residuals that are not explained by being indicators of the latent variable. If you can achieve adequate CFA model fit and composite reliability for the latent variable (i.e., naturalness), then you have the ability to save out factor score estimates of the latent variable that should be continuous, normally distribute values. 

Then you can either take those factor score estimates and plug them into your mixed model, or just expand from a confirmatory factor analysis model into a full structural equation model that includes all the elements of the desired mixed model, with the latent "naturalness" variable as the outcome. The latter approach is more theoretically desirable because it reduces bias due to measurement error. 

Steven J. Pierce, Ph.D.
Acting Director; Associate Director
Center for Statistical Training & Consulting (CSTAT) Michigan State University
E-mail: pierces1 at msu

-----Original Message-----
From: Nicolas Deguines <nicodeguines at gmail.com>
Sent: Wednesday, March 6, 2019 9:01 AM
To: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5 levels) response variable?

Hello Phillip and all,

Thanks a lot Phillip for your very interesting and useful answer, and for the paper from Liddell & Kruschke. It helps a lot.

About trying other link and threshold functions in clmm: no huge difference in my case unfortunately. I tried different combinations of each.
'equidistant' did do better, but the improvement was far from enough.

I computed density plots for my response variable as observed and as predicted from my lmer() model (similar to what Liddell and Kruschke do in Figure 6): the linear mixed-model does pretty well in fitting the data.
=> so I'd be enclined to trust the results from my lmer models in the present case (but Liddell and Kruschke did show very clear cases when a linear model fit very poorly the ordinal data).

Meanwhile, I thought of another alternative for analyzing this response variable and I would be curious to read what people may think about it.
Before presenting that alternative, I need to say more about that 5-levels response variable.
It is a score built by Muratet and Fontaine (2015)* to assess the naturalness of a given private backyard (it is shown to be correlated with higher abundance of butterflies).
In the backyard: fallow area, nettles (*Urtica dioica*), ivy (*Hedera helix*), and brambles (*Rubus spp.*) are each scored one if present, and the naturalness index was computed as the sum of these scores.
=> it results in a 5-levels ordinal variable because it can go from 0 to 4, and each increase in 1 means a backyard with more features of 'naturalness'.
I wonder thus if this could be modelled using a glmer() with family = binomial and feeding to the model two columns: cbind(sum of 1's, sum of
0's) (see R documentation for family{stats}, in the Details: "*As a two-column integer matrix: the first column gives the number of successes and the second the number of failures.*") I will try and see how the model fit the data. But I would be interested in getting a theoretical opinion.

I hope this can help others too

Best regards,
Nicolas Deguines

*
https://urldefense.proofpoint.com/v2/url?u=https-3A__www.sciencedirect.com_science_article_abs_pii_S0006320714004704-3Fvia-253Dihub&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=y9qFrcE-42RfdxSyVs_fJu4DwgBIEjj1jh2TimnhlHI&e=

----------------------------------
Postdoctoral Research Associate
Laboratoire Ecologie, Syst?matique et Evolution Universit? Paris Sud, Orsay, France
Website: https://urldefense.proofpoint.com/v2/url?u=http-3A__nicolasdeguines.weebly.com_&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=umRU32TJsniNhkwfbYgkiGyv_zmKT1LJU9qtyEGdVXg&e=


On Tue, 5 Mar 2019 at 13:04, Phillip Alday <phillip.alday at mpi.nl> wrote:

> Hi Nicolas,
>
> How much you can get away bending the assumptions depends in some ways 
> on how well the resulting model fits your data. If the resulting model 
> is a poor fit, then it's not a great model for performing inference.
> The other problem with bending assumptions is that a lot of 'error 
> statistics' (standard errors, t-values, and basically anything related 
> to significance testings) aren't guaranteed to do what they are 
> supposed to do. (In your case, the good behavior of your residuals 
> suggests that this won't be a huge problem, but there are no
> promises.)
>
> You can get around this a bit by doing things like cross-validation or 
> other inferential steps based on how well the model generalizes to / 
> predicts new data instead of significance testing of coefficients or 
> linear hypotheses.
>
> John Kruschke has written about this issue at some length and seems 
> convinced that it's (almost) always a bad idea to bend the 
> metric/continuous assumption when dealing with ordinal data:
>
>
> https://urldefense.proofpoint.com/v2/url?u=http-3A__doingbayesiandataa
> nalysis.blogspot.com_2017_12_which-2Dmovie-2Dis-2Drated-2Dbetter-2Ddon
> t-2Dtreat.html&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7Zip
> xyQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=cTggjZQJ8ytLt2Oe_1
> EsE6augDSd6GEeTbNeGKtCtjA&e=
>
>
> https://urldefense.proofpoint.com/v2/url?u=http-3A__doingbayesiandataa
> nalysis.blogspot.com_2018_09_analyzing-2Dordinal-2Ddata-2Dwith-2Dmetri
> c.html&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uL
> hmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=mhVwPoXW5lRnvpxuImiQd-Dfcb
> kWvVrws6eDG9BhW4I&e=
>
> The latter is largely a link/"press release" for the associated paper:
>
> Liddell, T. M., & Kruschke, J. K. (2018). Analyzing ordinal data with 
> metric models: What could possibly go wrong? Journal of Experimental 
> Social Psychology , 79 , 328?348. doi:10.1016/j.jesp.2018.08.009
>
> Finally, have you tried other link and threshold functions in clmm?
> Those can make a huge difference!
>
> Phillip
>
> On 5/3/19 11:00 am, Nicolas Deguines wrote:
> > Hello everyone,
> >
> > I am investigating how engagement into a citizen science program can
> change
> > participants' behavior in terms of implementing gardening techniques 
> > benefitting biodiversity.
> > There are 2362 participants, distributed into 7 cohorts (= year in 
> > which they joined the program), and I have repeated gardening 
> > technique information for multiple years for each participant.
> > So I need to use mixed modeling.
> >
> > One of the response variable is a score that can takes 5 values: 0, 
> > 1, 2, 3, or 4. It's ordered, it's not continuous (there are 5 levels).
> > I would analyze this into a cumulative link mixed models (using
> > clmm()
> from
> > ordinal package) but the Hessian condition I obtained with such 
> > model is
> >
> > 5.0e+06. I.e. assumption is violated (simplifying my initial full 
> > model
> did
> > not help at all).
> >
> > As an alternative, I am wondering if I could treat this response 
> > variable has a continuous one into a lmer() model.
> > When I do:
> > - Normality of model residuals is nicely met
> > - Homoscedasticity of model residuals is met as well.
> > => does meeting these two assumptions is enough to validate the use 
> > of a
> > lmer() model for an ordered categorical response variable?
> >
> > In one of Douglas Bates' presentation (slide 3 of Jan. 2011, Madison:
> > https://urldefense.proofpoint.com/v2/url?u=http-3A__lme4.r-2Dforge.r
> > -2Dproject.org_slides_2011-2D01-2D11-2DMadison_5GLMM.pdf&d=DwIFaQ&c=
> > nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uLhmTfp88PWyhBj0bi
> > VanGF_OI9s5JNvn6ENYnsaenI&s=gQt2neGX64PqU4v8CFD2g0WdUb_EycwwXjSSo4db
> > LDY&e=),
> it
> > is said that
> > "When using LMMs we assume that the response being modeled is on a 
> > continuous scale.
> > Sometimes we can bend this assumption a bit if the response is an 
> > ordinal response with a moderate to large number of levels.
> > For example, [...a response variable taking] integer values on the 
> > scale
> of
> > 1 to 10."
> > => is 5 levels too few to be treated as continuous? Or would it be 
> > ok
> given
> > residuals behave nicely?
> >
> > I would appreciate any help and thoughts on this.
> > I checked that this was not treated in a previous post and I hope I 
> > did
> not
> > miss it (sorry if I did).
> >
> > Best,
> > Nicolas Deguines
> > ----------------------------------
> > Postdoctoral Research Associate
> > Laboratoire Ecologie, Syst?matique et Evolution Universit? Paris 
> > Sud, Orsay, France
> > Website: 
> > https://urldefense.proofpoint.com/v2/url?u=http-3A__nicolasdeguines.
> > weebly.com_&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7Zipx
> > yQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=umRU32TJsniNhkwfb
> > YgkiGyv_zmKT1LJU9qtyEGdVXg&e=
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list 
> > https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_ma
> > ilman_listinfo_r-2Dsig-2Dmixed-2Dmodels&d=DwIFaQ&c=nE__W8dFE-shTxStw
> > Xtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6E
> > NYnsaenI&s=v-ubYR1CYdeewDkWeQmOxEvkKJ4LF-vs8O0dNas2S8Q&e=
> >
>

	[[alternative HTML version deleted]]


_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dsig-2Dmixed-2Dmodels&d=DwIGaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=bJ2WeOugjTjdNv0JHhX3mbV-NYORMqKlREE07FtWxDk&s=NIMa3Dk2PuImseyI9fe9_mHHtE_cVvs8yVALMBHOHQc&e=

From p|erce@1 @end|ng |rom m@u@edu  Wed Mar  6 17:49:14 2019
From: p|erce@1 @end|ng |rom m@u@edu (Pierce, Steven)
Date: Wed, 6 Mar 2019 16:49:14 +0000
Subject: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5
 levels) response variable?
In-Reply-To: <BN7PR05MB5857D7662034FDDED896C44ACA730@BN7PR05MB5857.namprd05.prod.outlook.com>
References: <CAFzZfa1imABAAqjozzwTpkWgVCaaHkWsGZKKC9aLKPCQ8sjMgw@mail.gmail.com>
 <e1e2c821-8add-7cfe-7c23-e4bea4ddb985@mpi.nl>
 <CAFzZfa3nFOxY8WyscYBad7idDv=eo_v7fXetKBKtFPZ3k0uk+w@mail.gmail.com>
 <DM6PR12MB2732EC4918306F6B1E31EC8681730@DM6PR12MB2732.namprd12.prod.outlook.com>
 <BN7PR05MB585786EF2C9E4DA0AE49D9FACA730@BN7PR05MB5857.namprd05.prod.outlook.com>
 <DM6PR12MB27325EA202BF546CDE2BDAC381730@DM6PR12MB2732.namprd12.prod.outlook.com>
 <BN7PR05MB5857D7662034FDDED896C44ACA730@BN7PR05MB5857.namprd05.prod.outlook.com>
Message-ID: <DM6PR12MB27322BEB6813D6024258262781730@DM6PR12MB2732.namprd12.prod.outlook.com>

I also have extensive experience with psychometric modeling. We are just using the term "scale" differently. Many researchers refer to a survey instrument as a scale, refer to the sum of the items from such an instrument as a scale score, then go on to use such scores in their research. That's the sense in which I am using the term scale score. It's obviously a looser, more informal usage than you prefer but that doesn?t mean I'm wrong. It's an empirical fact that lots of people use the term the way I did. 

Again, the point was that I took the time to suggest a better measurement approach after pointing out a similarity in how the scores Nicolas described were constructed and how scores are sometimes created in social science research. 

Steve


-----Original Message-----
From: Doran, Harold <HDoran at air.org> 
Sent: Wednesday, March 6, 2019 11:12 AM
To: Pierce, Steven <pierces1 at msu.edu>; Nicolas Deguines <nicodeguines at gmail.com>
Cc: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
Subject: RE: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5 levels) response variable?

No, this is absolutely incorrect. Sums of the binary variables are the *raw* scores and these are *not* scaled scores. Psychometric practice for developing scaled scores does indeed have many methods (as the author of a published test scoring software system, I've included most psychometric approaches), but the sums  as you suggest are not one of them.

-----Original Message-----
From: Pierce, Steven <pierces1 at msu.edu> 
Sent: Wednesday, March 06, 2019 11:04 AM
To: Doran, Harold <HDoran at air.org>; Nicolas Deguines <nicodeguines at gmail.com>
Cc: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
Subject: RE: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5 levels) response variable?

@Harold. There is a vast array of different ways that people analyze survey data. Some are far more appropriate and methodologically sound than others. Taking unweighted sums of binary items is indeed a very common practice for producing scale scores, though it is usually done without considering or testing the underlying measurement assumptions that it implies. I didn't endorse that overly simplistic practice but I have seen huge numbers of published papers that use it. More rigorous methods (such as I suggested) are reasonable ways to use observed responses to measure higher-order theoretical constructs while being thoughtful about the assumptions involved. My point was that one should be more thoughtful about the measurement process that yields the scores to be analyzed. A different approach to that step can change whether the score obtained is ordinal/count or legitimately continuous. 

Steve

-----Original Message-----
From: Doran, Harold <HDoran at air.org>
Sent: Wednesday, March 6, 2019 10:12 AM
To: Pierce, Steven <pierces1 at msu.edu>; Nicolas Deguines <nicodeguines at gmail.com>
Cc: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
Subject: RE: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5 levels) response variable?

@steven, no, this is not at all how survey researchers generate scaled scores. Scaled scores are MLEs of a likelihood function (or sometimes posterior mean/modes) constructed from the observed responses. 

To the OP, using lmer for the ordered response model is not an appropriate modeling strategy at all. Take a look at your predicted values. Are they < 0 or > 5? What does that mean?

-----Original Message-----
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of Pierce, Steven
Sent: Wednesday, March 06, 2019 9:38 AM
To: Nicolas Deguines <nicodeguines at gmail.com>
Cc: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5 levels) response variable?

Nicolas,

So your score is constructed by summing 4 binary variables, each of which represents a feature of a natural environment. This is similar to how social science researchers use survey data with binary questions that get combined into scale scores. Such scale scores are usually intended to measure latent variables. 

By taking a simple sum of the binary items as the scale score, you are making strong assumptions about each item being equally important to measuring the latent variable. That's a testable hypothesis. Consider doing a confirmatory factor analysis using the 4 items as indicators of a latent variable (naturalness) that is assumed to be continuous and normally distributed. The point of this is to get a decent measurement model for the latent variable. 

Allow the factor loadings to vary across items (some may be more important to measuring the latent variable than others); also test for whether there are correlations between the item residuals that are not explained by being indicators of the latent variable. If you can achieve adequate CFA model fit and composite reliability for the latent variable (i.e., naturalness), then you have the ability to save out factor score estimates of the latent variable that should be continuous, normally distribute values. 

Then you can either take those factor score estimates and plug them into your mixed model, or just expand from a confirmatory factor analysis model into a full structural equation model that includes all the elements of the desired mixed model, with the latent "naturalness" variable as the outcome. The latter approach is more theoretically desirable because it reduces bias due to measurement error. 

Steven J. Pierce, Ph.D.
Acting Director; Associate Director
Center for Statistical Training & Consulting (CSTAT) Michigan State University
E-mail: pierces1 at msu

-----Original Message-----
From: Nicolas Deguines <nicodeguines at gmail.com>
Sent: Wednesday, March 6, 2019 9:01 AM
To: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5 levels) response variable?

Hello Phillip and all,

Thanks a lot Phillip for your very interesting and useful answer, and for the paper from Liddell & Kruschke. It helps a lot.

About trying other link and threshold functions in clmm: no huge difference in my case unfortunately. I tried different combinations of each.
'equidistant' did do better, but the improvement was far from enough.

I computed density plots for my response variable as observed and as predicted from my lmer() model (similar to what Liddell and Kruschke do in Figure 6): the linear mixed-model does pretty well in fitting the data.
=> so I'd be enclined to trust the results from my lmer models in the present case (but Liddell and Kruschke did show very clear cases when a linear model fit very poorly the ordinal data).

Meanwhile, I thought of another alternative for analyzing this response variable and I would be curious to read what people may think about it.
Before presenting that alternative, I need to say more about that 5-levels response variable.
It is a score built by Muratet and Fontaine (2015)* to assess the naturalness of a given private backyard (it is shown to be correlated with higher abundance of butterflies).
In the backyard: fallow area, nettles (*Urtica dioica*), ivy (*Hedera helix*), and brambles (*Rubus spp.*) are each scored one if present, and the naturalness index was computed as the sum of these scores.
=> it results in a 5-levels ordinal variable because it can go from 0 to 4, and each increase in 1 means a backyard with more features of 'naturalness'.
I wonder thus if this could be modelled using a glmer() with family = binomial and feeding to the model two columns: cbind(sum of 1's, sum of
0's) (see R documentation for family{stats}, in the Details: "*As a two-column integer matrix: the first column gives the number of successes and the second the number of failures.*") I will try and see how the model fit the data. But I would be interested in getting a theoretical opinion.

I hope this can help others too

Best regards,
Nicolas Deguines

*
https://urldefense.proofpoint.com/v2/url?u=https-3A__www.sciencedirect.com_science_article_abs_pii_S0006320714004704-3Fvia-253Dihub&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=y9qFrcE-42RfdxSyVs_fJu4DwgBIEjj1jh2TimnhlHI&e=

----------------------------------
Postdoctoral Research Associate
Laboratoire Ecologie, Syst?matique et Evolution Universit? Paris Sud, Orsay, France
Website: https://urldefense.proofpoint.com/v2/url?u=http-3A__nicolasdeguines.weebly.com_&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=umRU32TJsniNhkwfbYgkiGyv_zmKT1LJU9qtyEGdVXg&e=


On Tue, 5 Mar 2019 at 13:04, Phillip Alday <phillip.alday at mpi.nl> wrote:

> Hi Nicolas,
>
> How much you can get away bending the assumptions depends in some ways 
> on how well the resulting model fits your data. If the resulting model 
> is a poor fit, then it's not a great model for performing inference.
> The other problem with bending assumptions is that a lot of 'error 
> statistics' (standard errors, t-values, and basically anything related 
> to significance testings) aren't guaranteed to do what they are 
> supposed to do. (In your case, the good behavior of your residuals 
> suggests that this won't be a huge problem, but there are no
> promises.)
>
> You can get around this a bit by doing things like cross-validation or 
> other inferential steps based on how well the model generalizes to / 
> predicts new data instead of significance testing of coefficients or 
> linear hypotheses.
>
> John Kruschke has written about this issue at some length and seems 
> convinced that it's (almost) always a bad idea to bend the 
> metric/continuous assumption when dealing with ordinal data:
>
>
> https://urldefense.proofpoint.com/v2/url?u=http-3A__doingbayesiandataa
> nalysis.blogspot.com_2017_12_which-2Dmovie-2Dis-2Drated-2Dbetter-2Ddon
> t-2Dtreat.html&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7Zip
> xyQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=cTggjZQJ8ytLt2Oe_1
> EsE6augDSd6GEeTbNeGKtCtjA&e=
>
>
> https://urldefense.proofpoint.com/v2/url?u=http-3A__doingbayesiandataa
> nalysis.blogspot.com_2018_09_analyzing-2Dordinal-2Ddata-2Dwith-2Dmetri
> c.html&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uL
> hmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=mhVwPoXW5lRnvpxuImiQd-Dfcb
> kWvVrws6eDG9BhW4I&e=
>
> The latter is largely a link/"press release" for the associated paper:
>
> Liddell, T. M., & Kruschke, J. K. (2018). Analyzing ordinal data with 
> metric models: What could possibly go wrong? Journal of Experimental 
> Social Psychology , 79 , 328?348. doi:10.1016/j.jesp.2018.08.009
>
> Finally, have you tried other link and threshold functions in clmm?
> Those can make a huge difference!
>
> Phillip
>
> On 5/3/19 11:00 am, Nicolas Deguines wrote:
> > Hello everyone,
> >
> > I am investigating how engagement into a citizen science program can
> change
> > participants' behavior in terms of implementing gardening techniques 
> > benefitting biodiversity.
> > There are 2362 participants, distributed into 7 cohorts (= year in 
> > which they joined the program), and I have repeated gardening 
> > technique information for multiple years for each participant.
> > So I need to use mixed modeling.
> >
> > One of the response variable is a score that can takes 5 values: 0, 
> > 1, 2, 3, or 4. It's ordered, it's not continuous (there are 5 levels).
> > I would analyze this into a cumulative link mixed models (using
> > clmm()
> from
> > ordinal package) but the Hessian condition I obtained with such 
> > model is
> >
> > 5.0e+06. I.e. assumption is violated (simplifying my initial full 
> > model
> did
> > not help at all).
> >
> > As an alternative, I am wondering if I could treat this response 
> > variable has a continuous one into a lmer() model.
> > When I do:
> > - Normality of model residuals is nicely met
> > - Homoscedasticity of model residuals is met as well.
> > => does meeting these two assumptions is enough to validate the use 
> > of a
> > lmer() model for an ordered categorical response variable?
> >
> > In one of Douglas Bates' presentation (slide 3 of Jan. 2011, Madison:
> > https://urldefense.proofpoint.com/v2/url?u=http-3A__lme4.r-2Dforge.r
> > -2Dproject.org_slides_2011-2D01-2D11-2DMadison_5GLMM.pdf&d=DwIFaQ&c=
> > nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uLhmTfp88PWyhBj0bi
> > VanGF_OI9s5JNvn6ENYnsaenI&s=gQt2neGX64PqU4v8CFD2g0WdUb_EycwwXjSSo4db
> > LDY&e=),
> it
> > is said that
> > "When using LMMs we assume that the response being modeled is on a 
> > continuous scale.
> > Sometimes we can bend this assumption a bit if the response is an 
> > ordinal response with a moderate to large number of levels.
> > For example, [...a response variable taking] integer values on the 
> > scale
> of
> > 1 to 10."
> > => is 5 levels too few to be treated as continuous? Or would it be 
> > ok
> given
> > residuals behave nicely?
> >
> > I would appreciate any help and thoughts on this.
> > I checked that this was not treated in a previous post and I hope I 
> > did
> not
> > miss it (sorry if I did).
> >
> > Best,
> > Nicolas Deguines
> > ----------------------------------
> > Postdoctoral Research Associate
> > Laboratoire Ecologie, Syst?matique et Evolution Universit? Paris 
> > Sud, Orsay, France
> > Website: 
> > https://urldefense.proofpoint.com/v2/url?u=http-3A__nicolasdeguines.
> > weebly.com_&d=DwIFaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7Zipx
> > yQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6ENYnsaenI&s=umRU32TJsniNhkwfb
> > YgkiGyv_zmKT1LJU9qtyEGdVXg&e=
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list 
> > https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_ma
> > ilman_listinfo_r-2Dsig-2Dmixed-2Dmodels&d=DwIFaQ&c=nE__W8dFE-shTxStw
> > Xtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=uLhmTfp88PWyhBj0biVanGF_OI9s5JNvn6E
> > NYnsaenI&s=v-ubYR1CYdeewDkWeQmOxEvkKJ4LF-vs8O0dNas2S8Q&e=
> >
>

	[[alternative HTML version deleted]]


_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dsig-2Dmixed-2Dmodels&d=DwIGaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=bJ2WeOugjTjdNv0JHhX3mbV-NYORMqKlREE07FtWxDk&s=NIMa3Dk2PuImseyI9fe9_mHHtE_cVvs8yVALMBHOHQc&e=

From I|@n@Re|n@te|n @end|ng |rom nyu|@ngone@org  Wed Mar  6 19:53:34 2019
From: I|@n@Re|n@te|n @end|ng |rom nyu|@ngone@org (Reinstein, Ilan)
Date: Wed, 6 Mar 2019 18:53:34 +0000
Subject: [R-sig-ME] Model Definition and Interpretation - Interactions,
 plus Singularity
Message-ID: <2aabb3b3f0f14118800e627361e4ff15@nyulangone.org>

Hi all, I hope all is well.


I'll try to keep this as brief as possible.


I am trying to fit a mixed logistic regression to the following data structure. The names of the variables are at the end.


- 100+ Students interacted with/reviewed 100 items each, everyone saw the same items but in different order. (ID)

- The response variable is binary (correct or not) (correct)

- The order is coded by a variable from 1-100 and it is common to everyone, although the item ID is different for each person. (Sequence)

- The items belong to different categories and the number of items by category is not the same, i.e., ItemTypeA has 20 items, TypeB has 15 etc. (caseType), Total unique items = 100. As a simple, preliminary case I am sorting the items into only two categories, in such a way that they are as balanced as possible, approx. 56 vs 44.


The goal of the study is to analyze change in performance of the students when reviewing an increasing number of items of different kinds. We know theoretically that students have different initial abilities, and that they learn at different rates. Additionally, the different types of items are themselves of varying "difficulty" (we showed it using IRT), so we expect/assume that because each person reviewed the same items but in different order, we can expect to see different performance curves both by person and by case type, hence the random coefficients logistic models presented below.


1. glmer(correct ~ scale(Sequence) + (scale(Sequence) | ID:caseType), family = binomial)

2. glmer(correct ~ scale(Sequence):caseType + (scale(Sequence):caseType | ID), family = binomial)


I've fitted these two models to capture the different learning rates by person and by case type but I am not sure about, first, if the interaction is correctly specified, and second, where and how to specify the interaction given the needs of my problem (person-case or # items-case, random or fixed). Are cases nested within persons, even if the number of items by case differs? Or is the interaction of case type with the number on the sequence more informative for my purpose?


The first model's coef()/ranef() output is very attractive since I can have an Intercept and a Slope for the interaction of person and case type, however after carefully reviewing the answers in this discussion<https://stats.stackexchange.com/questions/31569/questions-about-how-random-effects-are-specified-in-lmer>, I moved to model number 2 since it made more sense in the interpretation, however I am unsure which is more appropriate for my needs. I am starting to get more inclined towards the second model but it is a singular fit (+1 correlation of random effects). I've looked for possible solutions without the need to go Bayesian, but I am not sure how to implement those either so I tried going to rstanarm. Are there any suggestions about the priors?


I will continue to try out the different suggestions presented in the different threads around singularities on lme4.


Finally, I looked for a suitable dataset for reproducibility but I hope this is more of a conceptual discussion.


Similar questions about singular fits: https://stats.stackexchange.com/questions/378939/dealing-with-singular-fit-in-mixed-models


[https://cdn.sstatic.net/Sites/stats/img/apple-touch-icon at 2.png?v=344f57aa10cc]<https://stats.stackexchange.com/questions/378939/dealing-with-singular-fit-in-mixed-models>

lme4 nlme - Dealing with singular fit in mixed models - Cross Validated<https://stats.stackexchange.com/questions/378939/dealing-with-singular-fit-in-mixed-models>
stats.stackexchange.com
Let's say we have a model mod <- Y ~ X*Condition + (X*Condition|subject) # Y = logit variable # X = continuous variable # Condition = values A and B, dummy coded; the design is repeated ...




Thank you in advance,


Best,


Ilan Reinstein



------------------------------------------------------------
This email message, including any attachments, is for th...{{dropped:14}}


From |upp @end|ng |rom uch|c@go@edu  Thu Mar  7 03:02:42 2019
From: |upp @end|ng |rom uch|c@go@edu (Stuart Luppescu)
Date: Thu, 7 Mar 2019 02:02:42 +0000
Subject: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5
 levels) response variable?
In-Reply-To: <DM6PR12MB27322BEB6813D6024258262781730@DM6PR12MB2732.namprd12.prod.outlook.com>
References: <CAFzZfa1imABAAqjozzwTpkWgVCaaHkWsGZKKC9aLKPCQ8sjMgw@mail.gmail.com>
 <e1e2c821-8add-7cfe-7c23-e4bea4ddb985@mpi.nl>
 <CAFzZfa3nFOxY8WyscYBad7idDv=eo_v7fXetKBKtFPZ3k0uk+w@mail.gmail.com>
 <DM6PR12MB2732EC4918306F6B1E31EC8681730@DM6PR12MB2732.namprd12.prod.outlook.com>
 <BN7PR05MB585786EF2C9E4DA0AE49D9FACA730@BN7PR05MB5857.namprd05.prod.outlook.com>
 <DM6PR12MB27325EA202BF546CDE2BDAC381730@DM6PR12MB2732.namprd12.prod.outlook.com>
 <BN7PR05MB5857D7662034FDDED896C44ACA730@BN7PR05MB5857.namprd05.prod.outlook.com>
 <DM6PR12MB27322BEB6813D6024258262781730@DM6PR12MB2732.namprd12.prod.outlook.com>
Message-ID: <1551924159.2701.15.camel@uchicago.edu>

On Wed, 2019-03-06 at 16:49 +0000, Pierce, Steven wrote:
> Many researchers refer to a survey instrument as a scale, refer to
> the sum of the items from such an instrument as a scale score, then
> go on to use such scores in their research. That's the sense in which
> I am using the term scale score. It's obviously a looser, more
> informal usage than you prefer but that doesn?t mean I'm wrong. It's
> an empirical fact that lots of people use the term the way I did. 

I'm with Harold Doran on this one (not for the first time by any
means). Just because a lot of people do it doesn't have anything to do
whether it's a good method or not. When I was in graduate school a long
time ago we learned that the numerical codes associated with the
response categories for survey data are category labels, nominal data,
not numeric data. And as such it is not appropriate to do arithmetic
(such as calculating sums and means) on them.
-- 
Stuart Luppescu
Chief Psychometrician (ret.)
UChicago Consortium on School Research
http://consortium.uchicago.edu

lupp at uchicago.edu

From d@kot@judo @end|ng |rom m@c@com  Thu Mar  7 03:57:05 2019
From: d@kot@judo @end|ng |rom m@c@com (Peter Claussen)
Date: Wed, 6 Mar 2019 20:57:05 -0600
Subject: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5
 levels) response variable?
In-Reply-To: <CAFzZfa3nFOxY8WyscYBad7idDv=eo_v7fXetKBKtFPZ3k0uk+w@mail.gmail.com>
References: <CAFzZfa1imABAAqjozzwTpkWgVCaaHkWsGZKKC9aLKPCQ8sjMgw@mail.gmail.com>
 <e1e2c821-8add-7cfe-7c23-e4bea4ddb985@mpi.nl>
 <CAFzZfa3nFOxY8WyscYBad7idDv=eo_v7fXetKBKtFPZ3k0uk+w@mail.gmail.com>
Message-ID: <C0B73C5A-09CF-4E95-B767-33203D3CD318@mac.com>




> In the backyard: fallow area, nettles (*Urtica dioica*), ivy (*Hedera helix*),
> and brambles (*Rubus spp.*) are each scored one if present, and the
> naturalness index was computed as the sum of these scores.
> => it results in a 5-levels ordinal variable because it can go from 0 to 4,
> and each increase in 1 means a backyard with more features of 'naturalness'.
> I wonder thus if this could be modelled using a glmer() with family =
> binomial and feeding to the model two columns: cbind(sum of 1's, sum of
> 0's) (see R documentation for family{stats}, in the Details: "*As a
> two-column integer matrix: the first column gives the number of successes
> and the second the number of failures.*")
> I will try and see how the model fit the data. But I would be interested in
> getting a theoretical opinion.


I?m not sure summing the scores is correct. Are the distributions of nettles, ivy and brambles independent, or could they be correlated? 

If you have a fallow area and find nettles, is it also likely to have ivy? If so, the the binary outcomes (nettles present and ivy present) are not really additive.  Similarly, are the outcomes equivalent. That is, is a fallow plot with nettles, and nettles only, equal in ?naturalness? to a fallow plot with ivy and only ivy?

You might want a multivariate approach. Have you considered principal components to reduce each set of (nettles, ivy, branbles) to a single score, and then analyze that score using lmer?

Cheers,

Peter




	[[alternative HTML version deleted]]


From n|codegu|ne@ @end|ng |rom gm@||@com  Thu Mar  7 14:48:41 2019
From: n|codegu|ne@ @end|ng |rom gm@||@com (Nicolas Deguines)
Date: Thu, 7 Mar 2019 14:48:41 +0100
Subject: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5
 levels) response variable?
In-Reply-To: <C0B73C5A-09CF-4E95-B767-33203D3CD318@mac.com>
References: <CAFzZfa1imABAAqjozzwTpkWgVCaaHkWsGZKKC9aLKPCQ8sjMgw@mail.gmail.com>
 <e1e2c821-8add-7cfe-7c23-e4bea4ddb985@mpi.nl>
 <CAFzZfa3nFOxY8WyscYBad7idDv=eo_v7fXetKBKtFPZ3k0uk+w@mail.gmail.com>
 <C0B73C5A-09CF-4E95-B767-33203D3CD318@mac.com>
Message-ID: <CAFzZfa2o19CUghDn6BOcZHKzA+9VDFRn6YW7uVKJLjsvg7qx8g@mail.gmail.com>

Dear Emmanuel, Dimitris, Steven, Harold, Peter, Stuart and all,

I thank you for your time and for your insights into this issue; I can see
there are important things to consider and check before I further analyze
this variable. The principal components analysis is definitely my next step
to get a better idea of whether or not these four scores are correlated.

Thanks again,
Cheers,
Nicolas


On Thu, 7 Mar 2019 at 03:57, Peter Claussen <dakotajudo at mac.com> wrote:

>
>
>
> In the backyard: fallow area, nettles (*Urtica dioica*), ivy (*Hedera
> helix*),
> and brambles (*Rubus spp.*) are each scored one if present, and the
> naturalness index was computed as the sum of these scores.
> => it results in a 5-levels ordinal variable because it can go from 0 to 4,
> and each increase in 1 means a backyard with more features of
> 'naturalness'.
> I wonder thus if this could be modelled using a glmer() with family =
> binomial and feeding to the model two columns: cbind(sum of 1's, sum of
> 0's) (see R documentation for family{stats}, in the Details: "*As a
> two-column integer matrix: the first column gives the number of successes
> and the second the number of failures.*")
> I will try and see how the model fit the data. But I would be interested in
> getting a theoretical opinion.
>
>
>
> I?m not sure summing the scores is correct. Are the distributions of
> nettles, ivy and brambles independent, or could they be correlated?
>
> If you have a fallow area and find nettles, is it also likely to have ivy?
> If so, the the binary outcomes (nettles present and ivy present) are not
> really additive.  Similarly, are the outcomes equivalent. That is, is a
> fallow plot with nettles, and nettles only, equal in ?naturalness? to a
> fallow plot with ivy and only ivy?
>
> You might want a multivariate approach. Have you considered principal
> components to reduce each set of (nettles, ivy, branbles) to a single
> score, and then analyze that score using lmer?
>
> Cheers,
>
> Peter
>
>
>
>

	[[alternative HTML version deleted]]


From F@rr@r@D@v|d @end|ng |rom ep@@gov  Thu Mar  7 14:49:42 2019
From: F@rr@r@D@v|d @end|ng |rom ep@@gov (Farrar, David)
Date: Thu, 7 Mar 2019 13:49:42 +0000
Subject: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5
 levels) response variable?
In-Reply-To: <1551924159.2701.15.camel@uchicago.edu>
References: <CAFzZfa1imABAAqjozzwTpkWgVCaaHkWsGZKKC9aLKPCQ8sjMgw@mail.gmail.com>
 <e1e2c821-8add-7cfe-7c23-e4bea4ddb985@mpi.nl>
 <CAFzZfa3nFOxY8WyscYBad7idDv=eo_v7fXetKBKtFPZ3k0uk+w@mail.gmail.com>
 <DM6PR12MB2732EC4918306F6B1E31EC8681730@DM6PR12MB2732.namprd12.prod.outlook.com>
 <BN7PR05MB585786EF2C9E4DA0AE49D9FACA730@BN7PR05MB5857.namprd05.prod.outlook.com>
 <DM6PR12MB27325EA202BF546CDE2BDAC381730@DM6PR12MB2732.namprd12.prod.outlook.com>
 <BN7PR05MB5857D7662034FDDED896C44ACA730@BN7PR05MB5857.namprd05.prod.outlook.com>
 <DM6PR12MB27322BEB6813D6024258262781730@DM6PR12MB2732.namprd12.prod.outlook.com>
 <1551924159.2701.15.camel@uchicago.edu>
Message-ID: <BN6PR09MB1411C6E8D5044EBC385C341C9A4C0@BN6PR09MB1411.namprd09.prod.outlook.com>

The simple addition has an interpretation of "number of positive indicators."  Before applying a more refined approach, I wonder if there is a quick check that some of the indicators might be handled differently,  e.g.,  do some load much differently on the first  PC (I should check my PC terminology).    
Or course, this then raises whether there is a refined  PC-like approach for binary responses.
I wonder if this might lead to a small adjustment that  would be easy to explain. 
"Hessian condition I obtained with such model is > 5.0e+06. I.e. assumption is violated (simplifying my initial full model did not help at all)."
Forgive me please if this is  a special operation for ordinal modeling;  however, the Hessian conditions I can think  of would be a measure of ill-conditioning, which might suggest whether a model is overfitted.
There is now at least one whole book on longitudinal discrete data. 
In America, BTW,  Hedera would be bad.


-----Original Message-----
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of Stuart Luppescu
Sent: Wednesday, March 06, 2019 9:03 PM
To: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5 levels) response variable?

On Wed, 2019-03-06 at 16:49 +0000, Pierce, Steven wrote:
> Many researchers refer to a survey instrument as a scale, refer to the 
> sum of the items from such an instrument as a scale score, then go on 
> to use such scores in their research. That's the sense in which I am 
> using the term scale score. It's obviously a looser, more informal 
> usage than you prefer but that doesn?t mean I'm wrong. It's an 
> empirical fact that lots of people use the term the way I did.

I'm with Harold Doran on this one (not for the first time by any means). Just because a lot of people do it doesn't have anything to do whether it's a good method or not. When I was in graduate school a long time ago we learned that the numerical codes associated with the response categories for survey data are category labels, nominal data, not numeric data. And as such it is not appropriate to do arithmetic (such as calculating sums and means) on them.
--
Stuart Luppescu
Chief Psychometrician (ret.)
UChicago Consortium on School Research
http://consortium.uchicago.edu

lupp at uchicago.edu
_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From p|erce@1 @end|ng |rom m@u@edu  Thu Mar  7 14:55:48 2019
From: p|erce@1 @end|ng |rom m@u@edu (Pierce, Steven)
Date: Thu, 7 Mar 2019 13:55:48 +0000
Subject: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5
 levels) response variable?
In-Reply-To: <1551924159.2701.15.camel@uchicago.edu>
References: <CAFzZfa1imABAAqjozzwTpkWgVCaaHkWsGZKKC9aLKPCQ8sjMgw@mail.gmail.com>
 <e1e2c821-8add-7cfe-7c23-e4bea4ddb985@mpi.nl>
 <CAFzZfa3nFOxY8WyscYBad7idDv=eo_v7fXetKBKtFPZ3k0uk+w@mail.gmail.com>
 <DM6PR12MB2732EC4918306F6B1E31EC8681730@DM6PR12MB2732.namprd12.prod.outlook.com>
 <BN7PR05MB585786EF2C9E4DA0AE49D9FACA730@BN7PR05MB5857.namprd05.prod.outlook.com>
 <DM6PR12MB27325EA202BF546CDE2BDAC381730@DM6PR12MB2732.namprd12.prod.outlook.com>
 <BN7PR05MB5857D7662034FDDED896C44ACA730@BN7PR05MB5857.namprd05.prod.outlook.com>
 <DM6PR12MB27322BEB6813D6024258262781730@DM6PR12MB2732.namprd12.prod.outlook.com>
 <1551924159.2701.15.camel@uchicago.edu>
Message-ID: <DM6PR12MB2732AD90E734E7EAE8638F11814C0@DM6PR12MB2732.namprd12.prod.outlook.com>

Again, I never said the sum scores were the right choice. That was why I recommended a measurement methodology based on using confirmatory factor analysis in my first message. If such a CFA model is estimated via methods appropriate for binary indicator variables and the model fit is adequate, you can compute meaningful factor score estimates of the latent variable from the model results. Such factor score estimates have different properties than either the raw, item-level responses or the sum of the binary items. That should make the factor score estimates more appropriate for use in a mixed model than the sum scores would have been. I further pointed out that solving the entire problem within a structural equation model framework would be better than just saving out the factor score estimates and treating them as an observed variable. 

Neither you nor Harold have (a) made a principled argument about how using CFA and SEM would be flawed, or (b) suggested a better approach. Instead you've criticized a minor point where I acknowledged a similarity between how the scores Nicolas had described were constructed and a commonly-used but flawed approach to measurement in the social sciences. I only mentioned that similarity as a bridge to suggesting the CFA approach that more statistically rigorous social scientists have developed for translating binary item-level data into decent measures of theoretical constructs. 

If you have specific suggestions for better methodology, I'd like to hear them. 

Steven J. Pierce, Ph.D.
Acting Director; Associate Director
Center for Statistical Training & Consulting (CSTAT)
Michigan State University

-----Original Message-----
From: Stuart Luppescu <lupp at uchicago.edu> 
Sent: Wednesday, March 6, 2019 9:03 PM
To: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5 levels) response variable?

On Wed, 2019-03-06 at 16:49 +0000, Pierce, Steven wrote:
> Many researchers refer to a survey instrument as a scale, refer to
> the sum of the items from such an instrument as a scale score, then
> go on to use such scores in their research. That's the sense in which
> I am using the term scale score. It's obviously a looser, more
> informal usage than you prefer but that doesn?t mean I'm wrong. It's
> an empirical fact that lots of people use the term the way I did. 

I'm with Harold Doran on this one (not for the first time by any
means). Just because a lot of people do it doesn't have anything to do
whether it's a good method or not. When I was in graduate school a long
time ago we learned that the numerical codes associated with the
response categories for survey data are category labels, nominal data,
not numeric data. And as such it is not appropriate to do arithmetic
(such as calculating sums and means) on them.
-- 
Stuart Luppescu
Chief Psychometrician (ret.)
UChicago Consortium on School Research
https://urldefense.proofpoint.com/v2/url?u=http-3A__consortium.uchicago.edu&d=DwIGaQ&c=nE__W8dFE-shTxStwXtp0A&r=91SB6keVyEb7FtX7ZipxyQ&m=wu_gseOOQ6GxLdRbKFj3lZ-oY6-ir99D2ZwFW9FqwNY&s=u_SaGEXn3sueWiFNbjnsoCpPAOGD9xbBkgxy5uw9GFY&e=

lupp at uchicago.edu

From ph||||p@@|d@y @end|ng |rom mp|@n|  Thu Mar  7 16:56:41 2019
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Thu, 7 Mar 2019 16:56:41 +0100
Subject: [R-sig-ME] Model Definition and Interpretation - Interactions,
 plus Singularity
In-Reply-To: <2aabb3b3f0f14118800e627361e4ff15@nyulangone.org>
References: <2aabb3b3f0f14118800e627361e4ff15@nyulangone.org>
Message-ID: <0f1c4fb8-ddca-dca3-3cea-dcf0409b9b5a@mpi.nl>

Hello,


On 6/3/19 7:53 pm, Reinstein, Ilan wrote:
> Hi all, I hope all is well.
> 

::snip::

> 
> 
> 1. glmer(correct ~ scale(Sequence) + (scale(Sequence) | ID:caseType), family = binomial)
> 
> 2. glmer(correct ~ scale(Sequence):caseType + (scale(Sequence):caseType | ID), family = binomial)


This model is a little unusual -- it includes interactions without main
effects, which usually doesn't make sense.

I think you want:

glmer(correct ~ 1 + scale(Sequence)*caseType +
(1+scale(Sequence)*caseType | ID), family = binomial)


(Note: I always make my intercept term explicit, both to remind that
it's there, and because some of the other software I use doesn't add an
implicit intercept.)

I wouldn't worry about achieving balance via artificial mehtods -- lme4
doesn't require it and the lack of balance will primarily show itself as
a difference in the intercept term, which isn't a big deal, and to a
lesser extent a difference in the standard errors -- they may actually
be better because you have more data overall.


Finally, you have 15ish (or at least more than 10 based on your
description and a logical leap) items per category, so you have more
than enough items to also estimate item effects. If each item belongs
uniquely to a single caseType, then you can't estimate caseType by item
effects, but you could potentially estimate sequence effects:

glmer(correct ~ 1 + scale(Sequence)*caseType +
(1+scale(Sequence)*caseType | ID) + (1+scale(Sequence)|Item), family =
binomial)

and if that fails to converge, you can try a model which just allows for
just intercept-level variation by item:

glmer(correct ~ 1 + scale(Sequence)*caseType +
(1+scale(Sequence)*caseType | ID) + (1|Item), family = binomial)


This latter model would pick up e.g. whether a given item is more or
less likely to be corrected rated, beyond the effect of its overarching
caseType or sequence position. lme4 will pick up on the "between" design
aspects of the item component without you having to be more explicit.

About your convergence problems: assuming they still linger after adding
in the main effects, I would interpret them as there not being enough
data to estimate the by-subject and by-item differences in the sequence
effect, which isn't horrible nor particularly surprising for me: I
expect the overall effect of sequence and the general variation between
subjects and items (i.e. the corresponding random intercepts) to be much
larger than the variation between subjects and items in the sequence effect.

Best,
Phillip

> 
> I've fitted these two models to capture the different learning rates by person and by case type but I am not sure about, first, if the interaction is correctly specified, and second, where and how to specify the interaction given the needs of my problem (person-case or # items-case, random or fixed). Are cases nested within persons, even if the number of items by case differs? Or is the interaction of case type with the number on the sequence more informative for my purpose?
> 
> 
> The first model's coef()/ranef() output is very attractive since I can have an Intercept and a Slope for the interaction of person and case type, however after carefully reviewing the answers in this discussion<https://stats.stackexchange.com/questions/31569/questions-about-how-random-effects-are-specified-in-lmer>, I moved to model number 2 since it made more sense in the interpretation, however I am unsure which is more appropriate for my needs. I am starting to get more inclined towards the second model but it is a singular fit (+1 correlation of random effects). I've looked for possible solutions without the need to go Bayesian, but I am not sure how to implement those either so I tried going to rstanarm. Are there any suggestions about the priors?
> 
> 
> I will continue to try out the different suggestions presented in the different threads around singularities on lme4.
> 
> 
> Finally, I looked for a suitable dataset for reproducibility but I hope this is more of a conceptual discussion.
> 
> 
> Similar questions about singular fits: https://stats.stackexchange.com/questions/378939/dealing-with-singular-fit-in-mixed-models
> 
> 
> [https://cdn.sstatic.net/Sites/stats/img/apple-touch-icon at 2.png?v=344f57aa10cc]<https://stats.stackexchange.com/questions/378939/dealing-with-singular-fit-in-mixed-models>
> 
> lme4 nlme - Dealing with singular fit in mixed models - Cross Validated<https://stats.stackexchange.com/questions/378939/dealing-with-singular-fit-in-mixed-models>
> stats.stackexchange.com
> Let's say we have a model mod <- Y ~ X*Condition + (X*Condition|subject) # Y = logit variable # X = continuous variable # Condition = values A and B, dummy coded; the design is repeated ...
> 
> 
> 
> 
> Thank you in advance,
> 
> 
> Best,
> 
> 
> Ilan Reinstein
> 
> 
> 
> ------------------------------------------------------------
> This email message, including any attachments, is for ...{{dropped:7}}


From HDor@n @end|ng |rom @|r@org  Thu Mar  7 17:05:47 2019
From: HDor@n @end|ng |rom @|r@org (Doran, Harold)
Date: Thu, 7 Mar 2019 16:05:47 +0000
Subject: [R-sig-ME] Model Definition and Interpretation - Interactions,
 plus Singularity
In-Reply-To: <0f1c4fb8-ddca-dca3-3cea-dcf0409b9b5a@mpi.nl>
References: <2aabb3b3f0f14118800e627361e4ff15@nyulangone.org>
 <0f1c4fb8-ddca-dca3-3cea-dcf0409b9b5a@mpi.nl>
Message-ID: <BN7PR05MB58573371909AF3F4CF8DF533CA4C0@BN7PR05MB5857.namprd05.prod.outlook.com>

Ilan

I was reading this a bit and I was wondering if you are estimating what you actually wanted to estimate? I was uncertain. It sounded from the description you provided that you wanted to measure change in abilities? But it appears from the lmer call you're estimating item effects (but you already have item parameters for those items?)

That is, your code reminds me of the paper we published on this some time ago, link below.

https://www.jstatsoft.org/article/view/v020i02

If you're interested in measuring change in abilities, you might consider direct estimation (like used in NAEP). 

-----Original Message-----
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of Phillip Alday
Sent: Thursday, March 07, 2019 10:57 AM
To: Reinstein, Ilan <Ilan.Reinstein at nyulangone.org>; r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Model Definition and Interpretation - Interactions, plus Singularity

Hello,


On 6/3/19 7:53 pm, Reinstein, Ilan wrote:
> Hi all, I hope all is well.
> 

::snip::

> 
> 
> 1. glmer(correct ~ scale(Sequence) + (scale(Sequence) | ID:caseType), 
> family = binomial)
> 
> 2. glmer(correct ~ scale(Sequence):caseType + 
> (scale(Sequence):caseType | ID), family = binomial)


This model is a little unusual -- it includes interactions without main effects, which usually doesn't make sense.

I think you want:

glmer(correct ~ 1 + scale(Sequence)*caseType + (1+scale(Sequence)*caseType | ID), family = binomial)


(Note: I always make my intercept term explicit, both to remind that it's there, and because some of the other software I use doesn't add an implicit intercept.)

I wouldn't worry about achieving balance via artificial mehtods -- lme4 doesn't require it and the lack of balance will primarily show itself as a difference in the intercept term, which isn't a big deal, and to a lesser extent a difference in the standard errors -- they may actually be better because you have more data overall.


Finally, you have 15ish (or at least more than 10 based on your description and a logical leap) items per category, so you have more than enough items to also estimate item effects. If each item belongs uniquely to a single caseType, then you can't estimate caseType by item effects, but you could potentially estimate sequence effects:

glmer(correct ~ 1 + scale(Sequence)*caseType + (1+scale(Sequence)*caseType | ID) + (1+scale(Sequence)|Item), family =
binomial)

and if that fails to converge, you can try a model which just allows for just intercept-level variation by item:

glmer(correct ~ 1 + scale(Sequence)*caseType + (1+scale(Sequence)*caseType | ID) + (1|Item), family = binomial)


This latter model would pick up e.g. whether a given item is more or less likely to be corrected rated, beyond the effect of its overarching caseType or sequence position. lme4 will pick up on the "between" design aspects of the item component without you having to be more explicit.

About your convergence problems: assuming they still linger after adding in the main effects, I would interpret them as there not being enough data to estimate the by-subject and by-item differences in the sequence effect, which isn't horrible nor particularly surprising for me: I expect the overall effect of sequence and the general variation between subjects and items (i.e. the corresponding random intercepts) to be much larger than the variation between subjects and items in the sequence effect.

Best,
Phillip

> 
> I've fitted these two models to capture the different learning rates by person and by case type but I am not sure about, first, if the interaction is correctly specified, and second, where and how to specify the interaction given the needs of my problem (person-case or # items-case, random or fixed). Are cases nested within persons, even if the number of items by case differs? Or is the interaction of case type with the number on the sequence more informative for my purpose?
> 
> 
> The first model's coef()/ranef() output is very attractive since I can have an Intercept and a Slope for the interaction of person and case type, however after carefully reviewing the answers in this discussion<https://stats.stackexchange.com/questions/31569/questions-about-how-random-effects-are-specified-in-lmer>, I moved to model number 2 since it made more sense in the interpretation, however I am unsure which is more appropriate for my needs. I am starting to get more inclined towards the second model but it is a singular fit (+1 correlation of random effects). I've looked for possible solutions without the need to go Bayesian, but I am not sure how to implement those either so I tried going to rstanarm. Are there any suggestions about the priors?
> 
> 
> I will continue to try out the different suggestions presented in the different threads around singularities on lme4.
> 
> 
> Finally, I looked for a suitable dataset for reproducibility but I hope this is more of a conceptual discussion.
> 
> 
> Similar questions about singular fits: 
> https://stats.stackexchange.com/questions/378939/dealing-with-singular
> -fit-in-mixed-models
> 
> 
> [https://cdn.sstatic.net/Sites/stats/img/apple-touch-icon at 2.png?v=344f
> 57aa10cc]<https://stats.stackexchange.com/questions/378939/dealing-wit
> h-singular-fit-in-mixed-models>
> 
> lme4 nlme - Dealing with singular fit in mixed models - Cross 
> Validated<https://stats.stackexchange.com/questions/378939/dealing-wit
> h-singular-fit-in-mixed-models>
> stats.stackexchange.com
> Let's say we have a model mod <- Y ~ X*Condition + (X*Condition|subject) # Y = logit variable # X = continuous variable # Condition = values A and B, dummy coded; the design is repeated ...
> 
> 
> 
> 
> Thank you in advance,
> 
> 
> Best,
> 
> 
> Ilan Reinstein
> 
> 
> 
> ------------------------------------------------------------
> This email message, including any attachments, is for ...{{dropped:6}}


From |jrhur|ey @end|ng |rom gm@||@com  Thu Mar  7 19:58:07 2019
From: |jrhur|ey @end|ng |rom gm@||@com (landon hurley)
Date: Thu, 7 Mar 2019 13:58:07 -0500
Subject: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5
 levels) response variable?
In-Reply-To: <DM6PR12MB2732AD90E734E7EAE8638F11814C0@DM6PR12MB2732.namprd12.prod.outlook.com>
References: <CAFzZfa1imABAAqjozzwTpkWgVCaaHkWsGZKKC9aLKPCQ8sjMgw@mail.gmail.com>
 <e1e2c821-8add-7cfe-7c23-e4bea4ddb985@mpi.nl>
 <CAFzZfa3nFOxY8WyscYBad7idDv=eo_v7fXetKBKtFPZ3k0uk+w@mail.gmail.com>
 <DM6PR12MB2732EC4918306F6B1E31EC8681730@DM6PR12MB2732.namprd12.prod.outlook.com>
 <BN7PR05MB585786EF2C9E4DA0AE49D9FACA730@BN7PR05MB5857.namprd05.prod.outlook.com>
 <DM6PR12MB27325EA202BF546CDE2BDAC381730@DM6PR12MB2732.namprd12.prod.outlook.com>
 <BN7PR05MB5857D7662034FDDED896C44ACA730@BN7PR05MB5857.namprd05.prod.outlook.com>
 <DM6PR12MB27322BEB6813D6024258262781730@DM6PR12MB2732.namprd12.prod.outlook.com>
 <1551924159.2701.15.camel@uchicago.edu>
 <DM6PR12MB2732AD90E734E7EAE8638F11814C0@DM6PR12MB2732.namprd12.prod.outlook.com>
Message-ID: <2e6295e0-2e10-9040-8f9a-eec476d85ae8@gmail.com>

Steven,

Since you ask:

On 3/7/19 8:55 AM, Pierce, Steven wrote:
> Neither you nor Harold have (a) made a principled argument about how 
> using CFA and SEM would be flawed, or (b) suggested a better 
> approach. Instead you've criticized a minor point where I 
> acknowledged a similarity between how the scores Nicolas had 
> described were constructed and a commonly-used but flawed approach
> to measurement in the social sciences. I only mentioned that
> similarity as a bridge to suggesting the CFA approach that more
> statistically rigorous social scientists have develoto tped for
> translating binary item-level data into decent measures of
> theoretical constructs.

The problem with traditional factor analytic models with respect to the
theta score estimation lies in that the amount of information changes
relative to each persons' location on the theta scale. This has negative
impacts as a consequence of the so-called "factor score indeterminacy."
 An item of average difficulty is most probable to be located near that
location on the scale, which is a reflection of the theta score the item
was calibrated upon (the item regularity parameters are trained).
Further, let us assume that there is a more complicated relationship
than merely a proportional equivalence between the sum score and the
theta score. If this were false, than at best we would have ordination
(i.e., theta rankings) which were equivalent to the ranks of the sum
scores. This rank equivalence is lost once the model is expanded, but it
allows for differing amounts of information contained in each item to be
allocated across an entire ability level. The discrimination parameter
of an item serves to reflect the slope of the change from one response
to the next. A perfect step function (for example, a Heaviside function)
has perfect discrimination: an ability below the threshold (the
difficulty parameter, where there is a 50% probability of endorsement)
will always respond 0, otherwise 1.

Expansion of the factor model for handling discrete items under item
response theory, would be the typical solution for determining a theta
location for any given respondent. CFA does not represent a solution to
this problem because it specifically avoids the question of the
operation of the production of factor scores themselves (by
integrating/averaging out the individual scores). Instead, it is looking
for the structure of the model, but not how or where it is a useful tool
in the sample data. IRT, on the other hand, enables us to assess how
reliably (how accurately) an estimated location is upon the sample. More
items, and higher discriminating items provide more information, and
serve to change the test information.

If scoring items were the desired approach, then IRT has been developed
for much of the existing standardised testing (e.g., SAT, ACT, GRE,
LSAT, MCAT). It would appear to be the ideal solution.

-- 
Violence is the last refuge of the incompetent.


From p|erce@1 @end|ng |rom m@u@edu  Thu Mar  7 21:47:21 2019
From: p|erce@1 @end|ng |rom m@u@edu (Pierce, Steven)
Date: Thu, 7 Mar 2019 20:47:21 +0000
Subject: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5
 levels) response variable?
In-Reply-To: <2e6295e0-2e10-9040-8f9a-eec476d85ae8@gmail.com>
References: <CAFzZfa1imABAAqjozzwTpkWgVCaaHkWsGZKKC9aLKPCQ8sjMgw@mail.gmail.com>
 <e1e2c821-8add-7cfe-7c23-e4bea4ddb985@mpi.nl>
 <CAFzZfa3nFOxY8WyscYBad7idDv=eo_v7fXetKBKtFPZ3k0uk+w@mail.gmail.com>
 <DM6PR12MB2732EC4918306F6B1E31EC8681730@DM6PR12MB2732.namprd12.prod.outlook.com>
 <BN7PR05MB585786EF2C9E4DA0AE49D9FACA730@BN7PR05MB5857.namprd05.prod.outlook.com>
 <DM6PR12MB27325EA202BF546CDE2BDAC381730@DM6PR12MB2732.namprd12.prod.outlook.com>
 <BN7PR05MB5857D7662034FDDED896C44ACA730@BN7PR05MB5857.namprd05.prod.outlook.com>
 <DM6PR12MB27322BEB6813D6024258262781730@DM6PR12MB2732.namprd12.prod.outlook.com>
 <1551924159.2701.15.camel@uchicago.edu>
 <DM6PR12MB2732AD90E734E7EAE8638F11814C0@DM6PR12MB2732.namprd12.prod.outlook.com>
 <2e6295e0-2e10-9040-8f9a-eec476d85ae8@gmail.com>
Message-ID: <DM6PR12MB27326F2658F71659E8E70848814C0@DM6PR12MB2732.namprd12.prod.outlook.com>

Landon,

I'm familiar with IRT methods as well as CFA and agree IRT also provides a good measurement approach here. Raykov & Marcoulides (2016) point out that in some situations (possibly including this one), one can compute the relevant IRT parameters from the CFA results and the CFA parameters from the IRT results. My discussions with Raykov lead me to believe that CFA and IRT models are completely interchangeable in those scenarios. They accomplish the same thing. That relationship has not been proven to generalize to all situations (to my knowledge), but it is worth noting when it applies to a given problem. 

Raykov, T., & Marcoulides, G. A. (2016). On the relationship between classical test theory and item response theory: From one to the other and back. Educational and Psychological Measurement, 76(2), 325-338. doi:10.1177/0013164415576958

Steve

-----Original Message-----
From: landon hurley <ljrhurley at gmail.com> 
Sent: Thursday, March 7, 2019 1:58 PM
To: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5 levels) response variable?

Steven,

Since you ask:

On 3/7/19 8:55 AM, Pierce, Steven wrote:
> Neither you nor Harold have (a) made a principled argument about how 
> using CFA and SEM would be flawed, or (b) suggested a better 
> approach. Instead you've criticized a minor point where I 
> acknowledged a similarity between how the scores Nicolas had 
> described were constructed and a commonly-used but flawed approach
> to measurement in the social sciences. I only mentioned that
> similarity as a bridge to suggesting the CFA approach that more
> statistically rigorous social scientists have develoto tped for
> translating binary item-level data into decent measures of
> theoretical constructs.

The problem with traditional factor analytic models with respect to the
theta score estimation lies in that the amount of information changes
relative to each persons' location on the theta scale. This has negative
impacts as a consequence of the so-called "factor score indeterminacy."
 An item of average difficulty is most probable to be located near that
location on the scale, which is a reflection of the theta score the item
was calibrated upon (the item regularity parameters are trained).
Further, let us assume that there is a more complicated relationship
than merely a proportional equivalence between the sum score and the
theta score. If this were false, than at best we would have ordination
(i.e., theta rankings) which were equivalent to the ranks of the sum
scores. This rank equivalence is lost once the model is expanded, but it
allows for differing amounts of information contained in each item to be
allocated across an entire ability level. The discrimination parameter
of an item serves to reflect the slope of the change from one response
to the next. A perfect step function (for example, a Heaviside function)
has perfect discrimination: an ability below the threshold (the
difficulty parameter, where there is a 50% probability of endorsement)
will always respond 0, otherwise 1.

Expansion of the factor model for handling discrete items under item
response theory, would be the typical solution for determining a theta
location for any given respondent. CFA does not represent a solution to
this problem because it specifically avoids the question of the
operation of the production of factor scores themselves (by
integrating/averaging out the individual scores). Instead, it is looking
for the structure of the model, but not how or where it is a useful tool
in the sample data. IRT, on the other hand, enables us to assess how
reliably (how accurately) an estimated location is upon the sample. More
items, and higher discriminating items provide more information, and
serve to change the test information.

If scoring items were the desired approach, then IRT has been developed
for much of the existing standardised testing (e.g., SAT, ACT, GRE,
LSAT, MCAT). It would appear to be the ideal solution.

-- 
Violence is the last refuge of the incompetent.



From HDor@n @end|ng |rom @|r@org  Thu Mar  7 22:20:30 2019
From: HDor@n @end|ng |rom @|r@org (Doran, Harold)
Date: Thu, 7 Mar 2019 21:20:30 +0000
Subject: [R-sig-ME] Is it ok to use lmer() for an ordered categorical (5
 levels) response variable?
In-Reply-To: <DM6PR12MB27326F2658F71659E8E70848814C0@DM6PR12MB2732.namprd12.prod.outlook.com>
References: <CAFzZfa1imABAAqjozzwTpkWgVCaaHkWsGZKKC9aLKPCQ8sjMgw@mail.gmail.com>
 <e1e2c821-8add-7cfe-7c23-e4bea4ddb985@mpi.nl>
 <CAFzZfa3nFOxY8WyscYBad7idDv=eo_v7fXetKBKtFPZ3k0uk+w@mail.gmail.com>
 <DM6PR12MB2732EC4918306F6B1E31EC8681730@DM6PR12MB2732.namprd12.prod.outlook.com>
 <BN7PR05MB585786EF2C9E4DA0AE49D9FACA730@BN7PR05MB5857.namprd05.prod.outlook.com>
 <DM6PR12MB27325EA202BF546CDE2BDAC381730@DM6PR12MB2732.namprd12.prod.outlook.com>
 <BN7PR05MB5857D7662034FDDED896C44ACA730@BN7PR05MB5857.namprd05.prod.outlook.com>
 <DM6PR12MB27322BEB6813D6024258262781730@DM6PR12MB2732.namprd12.prod.outlook.com>
 <1551924159.2701.15.camel@uchicago.edu>
 <DM6PR12MB2732AD90E734E7EAE8638F11814C0@DM6PR12MB2732.namprd12.prod.outlook.com>
 <2e6295e0-2e10-9040-8f9a-eec476d85ae8@gmail.com>
 <DM6PR12MB27326F2658F71659E8E70848814C0@DM6PR12MB2732.namprd12.prod.outlook.com>
Message-ID: <D8A6F26F.57676%hdoran@air.org>

Steven

CFA *is* IRT. IRT parameters can also be estimated using generalized
linear models as we show in this paper. I also show in this paper how IRT
models are specifically derived from the classical test theory measurement
model (see section 1.1). Social scientists often think of these as models
that somehow do different things and have different reasons to exist.
Perhaps it?s because I?m older now, but I want to view the statistical
world as a unified model that, under constraints, can be used to estimate
different things. 

https://www.jstatsoft.org/article/view/v020i02

This thread has deviated a bit to a degree that is not helpful to the OP I
think. We started with the question on whether ordered, categorical data
can be (or should be) estimated using a linear mixed effects model. I
still can?t see any reason to support that idea and recall that the OP
began with an ordered logit but got stuck on the hessian singularity
somewhere. My recommendation is to go back and figure out why that is
happening in the context of that model rather than ditching that model for
use of a different one.

In other words, don?t let software determine what model is most
appropriate. 



On 3/7/19, 3:47 PM, "Pierce, Steven" <pierces1 at msu.edu> wrote:

>Landon,
>
>I'm familiar with IRT methods as well as CFA and agree IRT also provides
>a good measurement approach here. Raykov & Marcoulides (2016) point out
>that in some situations (possibly including this one), one can compute
>the relevant IRT parameters from the CFA results and the CFA parameters
>from the IRT results. My discussions with Raykov lead me to believe that
>CFA and IRT models are completely interchangeable in those scenarios.
>They accomplish the same thing. That relationship has not been proven to
>generalize to all situations (to my knowledge), but it is worth noting
>when it applies to a given problem.
>
>Raykov, T., & Marcoulides, G. A. (2016). On the relationship between
>classical test theory and item response theory: From one to the other and
>back. Educational and Psychological Measurement, 76(2), 325-338.
>doi:10.1177/0013164415576958
>
>Steve
>
>-----Original Message-----
>From: landon hurley <ljrhurley at gmail.com>
>Sent: Thursday, March 7, 2019 1:58 PM
>To: r-sig-mixed-models at r-project.org
>Subject: Re: [R-sig-ME] Is it ok to use lmer() for an ordered categorical
>(5 levels) response variable?
>
>Steven,
>
>Since you ask:
>
>On 3/7/19 8:55 AM, Pierce, Steven wrote:
>> Neither you nor Harold have (a) made a principled argument about how
>> using CFA and SEM would be flawed, or (b) suggested a better
>> approach. Instead you've criticized a minor point where I
>> acknowledged a similarity between how the scores Nicolas had
>> described were constructed and a commonly-used but flawed approach
>> to measurement in the social sciences. I only mentioned that
>> similarity as a bridge to suggesting the CFA approach that more
>> statistically rigorous social scientists have develoto tped for
>> translating binary item-level data into decent measures of
>> theoretical constructs.
>
>The problem with traditional factor analytic models with respect to the
>theta score estimation lies in that the amount of information changes
>relative to each persons' location on the theta scale. This has negative
>impacts as a consequence of the so-called "factor score indeterminacy."
> An item of average difficulty is most probable to be located near that
>location on the scale, which is a reflection of the theta score the item
>was calibrated upon (the item regularity parameters are trained).
>Further, let us assume that there is a more complicated relationship
>than merely a proportional equivalence between the sum score and the
>theta score. If this were false, than at best we would have ordination
>(i.e., theta rankings) which were equivalent to the ranks of the sum
>scores. This rank equivalence is lost once the model is expanded, but it
>allows for differing amounts of information contained in each item to be
>allocated across an entire ability level. The discrimination parameter
>of an item serves to reflect the slope of the change from one response
>to the next. A perfect step function (for example, a Heaviside function)
>has perfect discrimination: an ability below the threshold (the
>difficulty parameter, where there is a 50% probability of endorsement)
>will always respond 0, otherwise 1.
>
>Expansion of the factor model for handling discrete items under item
>response theory, would be the typical solution for determining a theta
>location for any given respondent. CFA does not represent a solution to
>this problem because it specifically avoids the question of the
>operation of the production of factor scores themselves (by
>integrating/averaging out the individual scores). Instead, it is looking
>for the structure of the model, but not how or where it is a useful tool
>in the sample data. IRT, on the other hand, enables us to assess how
>reliably (how accurately) an estimated location is upon the sample. More
>items, and higher discriminating items provide more information, and
>serve to change the test information.
>
>If scoring items were the desired approach, then IRT has been developed
>for much of the existing standardised testing (e.g., SAT, ACT, GRE,
>LSAT, MCAT). It would appear to be the ideal solution.
>
>-- 
>Violence is the last refuge of the incompetent.
>
>
>_______________________________________________
>R-sig-mixed-models at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From h|gh@t@t @end|ng |rom h|gh@t@t@com  Fri Mar  8 11:13:33 2019
From: h|gh@t@t @end|ng |rom h|gh@t@t@com (Highland Statistics Ltd)
Date: Fri, 8 Mar 2019 10:13:33 +0000
Subject: [R-sig-ME] Statistics course in Spain: Mixed effects modelling
Message-ID: <9d752b61-0154-7e93-c6e4-990bc6410de7@highstat.com>

Apologies for cross-posting


We would like to announce the following statistics course.

Course: Linear Mixed Effects Models and GLMM with R-INLA.
Where:? University of C?diz, C?diz, Spain
When:?? 2-6 September 2019
Course website: http://highstat.com/index.php/courses-upcoming
Course flyer: 
http://highstat.com/Courses/Flyers/2019/Flyer2019_08Cadiz_GLMM_INLA.pdf


The same course will also be given in St John?s, Canada. 12-16 August 2019

Kind regards,


Alain Zuur

-- 

Dr. Alain F. Zuur
Highland Statistics Ltd.
9 St Clair Wynd
AB41 6DZ Newburgh, UK
Email: highstat at highstat.com
URL:   www.highstat.com

And:
NIOZ Royal Netherlands Institute for Sea Research,
Department of Coastal Systems, and Utrecht University,
P.O. Box 59, 1790 AB Den Burg,
Texel, The Netherlands



Author of:
1. Beginner's Guide to Spatial, Temporal and Spatial-Temporal Ecological Data Analysis with R-INLA. (2017).
2. Beginner's Guide to Zero-Inflated Models with R (2016).
3. Beginner's Guide to Data Exploration and Visualisation with R (2015).
4. Beginner's Guide to GAMM with R (2014).
5. Beginner's Guide to GLM and GLMM with R (2013).
6. Beginner's Guide to GAM with R (2012).
7. Zero Inflated Models and GLMM with R (2012).
8. A Beginner's Guide to R (2009).
9. Mixed effects models and extensions in ecology with R (2009).
10. Analysing Ecological Data (2007).


From h@nch@o @end|ng |rom ude|@edu  Sun Mar 10 03:26:51 2019
From: h@nch@o @end|ng |rom ude|@edu (Chao Han)
Date: Sat, 9 Mar 2019 21:26:51 -0500
Subject: [R-sig-ME] Model failed to converge when a factor is centered or
 releveled
Message-ID: <CALiHhPSJ6dnnJubpHFtduN38oW9S8hWf7UEhALagP0rWZVaXFg@mail.gmail.com>

Hi all,

I'm running a mixed-effects model using lmer4::glmer() function. The
modeling works with R's default dummy coding. But if I center or relevel a
factor of 2 levels, the model failed to converge. I am wondering why.

Here is the data:
https://www.dropbox.com/s/ysw5rvxowamucoh/example.csv?dl=0

Here is the code for the model without centering or releveling the factor
"Memory":

> model_default <- glmer(ACC ~ Memory * Group + (1|Subject) +
(1+Memory|item), family = "binomial", data = df)

The model converges with the above code. But the following model failed to
converge after centering the factor "Memory":

> c1 <- c(-0.5, 0.5)
> contrasts(df$Memory) = cbind(c1)
> model_center <- glmer(ACC ~ Memory * Group + (1|Subject) +
(1+Memory|item), family = "binomial", data = df)

If I relevel the factor "Memory", the following model failed to converge
either:

> df$Memory <- factor(df$Memory, levels = c("Y", "X"))
> model_relevel <- glmer(ACC ~ Memory * Group + (1|Subject) +
(1+Memory|item), family = "binomial", data = df)

Why would centering or releveling a factor make a difference in terms of
convergence? Which result should I trust?

Thank you in advance for your help.

Best,
Chao

	[[alternative HTML version deleted]]


From ph||||p@@|d@y @end|ng |rom mp|@n|  Tue Mar 12 17:21:12 2019
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Tue, 12 Mar 2019 17:21:12 +0100
Subject: [R-sig-ME] 
 Model failed to converge when a factor is centered or releveled
In-Reply-To: <CALiHhPSJ6dnnJubpHFtduN38oW9S8hWf7UEhALagP0rWZVaXFg@mail.gmail.com>
References: <CALiHhPSJ6dnnJubpHFtduN38oW9S8hWf7UEhALagP0rWZVaXFg@mail.gmail.com>
Message-ID: <34d1af22-432d-5ef9-8183-42f22ed67f9a@mpi.nl>

A few general comments without looking at your data:

1. It generally doesn't make sense to "center" a categorical variable.
For a two-level, dummy-coded variable you could in theory do this, but
it would lead to a rather odd balance-weighted contrast coding.

2. I have occasionally seen weird behavior in complex binomial models
where the choice of reference-level makes a difference in convergence. I
suspect this has to do with the whether the model is able to "pull away"
from a "solution" where the model just predicts the same response for
everything, but I have not investigated this rigorously nor I have
checked that my hand-waving explanation makes sense in terms of the
actual behavior of the optimizer.

If you really want to reverse your contrast labels, what happens when
you do that directly instead of releveling?
i.e.

c1 <- c(-0.5, 0.5)
contrasts(df$Memory) = cbind(c1)
model_center <- glmer(ACC ~ Memory * Group + (1|Subject) +
(1+Memory|item), family = "binomial", data = df)

c2 <- c(0.5, -0.5)
contrasts(df$Memory) = cbind(c2)
model_relevel <- glmer(ACC ~ Memory * Group + (1|Subject) + ...


Best,
Phillip

On 10/3/19 3:26 am, Chao Han wrote:
> Hi all,
> 
> I'm running a mixed-effects model using lmer4::glmer() function. The
> modeling works with R's default dummy coding. But if I center or relevel a
> factor of 2 levels, the model failed to converge. I am wondering why.
> 
> Here is the data:
> https://www.dropbox.com/s/ysw5rvxowamucoh/example.csv?dl=0
> 
> Here is the code for the model without centering or releveling the factor
> "Memory":
> 
>> model_default <- glmer(ACC ~ Memory * Group + (1|Subject) +
> (1+Memory|item), family = "binomial", data = df)
> 
> The model converges with the above code. But the following model failed to
> converge after centering the factor "Memory":
> 
>> c1 <- c(-0.5, 0.5)
>> contrasts(df$Memory) = cbind(c1)
>> model_center <- glmer(ACC ~ Memory * Group + (1|Subject) +
> (1+Memory|item), family = "binomial", data = df)
> 
> If I relevel the factor "Memory", the following model failed to converge
> either:
> 
>> df$Memory <- factor(df$Memory, levels = c("Y", "X"))
>> model_relevel <- glmer(ACC ~ Memory * Group + (1|Subject) +
> (1+Memory|item), family = "binomial", data = df)
> 
> Why would centering or releveling a factor make a difference in terms of
> convergence? Which result should I trust?
> 
> Thank you in advance for your help.
> 
> Best,
> Chao
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From D@v|d@Du||y @end|ng |rom q|mrbergho|er@edu@@u  Wed Mar 13 01:57:32 2019
From: D@v|d@Du||y @end|ng |rom q|mrbergho|er@edu@@u (David Duffy)
Date: Wed, 13 Mar 2019 00:57:32 +0000
Subject: [R-sig-ME] 
 Model failed to converge when a factor is centered or releveled
In-Reply-To: <CALiHhPSJ6dnnJubpHFtduN38oW9S8hWf7UEhALagP0rWZVaXFg@mail.gmail.com>
References: <CALiHhPSJ6dnnJubpHFtduN38oW9S8hWf7UEhALagP0rWZVaXFg@mail.gmail.com>
Message-ID: <4737E17E7C8C3C4A8B5C1CE5346371D4A8510E4B@EXCH06S.adqimr.ad.lan>

These jobs run for me giving those warnings, but when I plot the re's from both models they seem to be identical, and have converged to the same likelihood. I guess it is just that the contrast coding leads to numerical differences that upset the diagnostics, 



________________________________________
From: R-sig-mixed-models [r-sig-mixed-models-bounces at r-project.org] on behalf of Chao Han [hanchao at udel.edu]
Sent: Sunday, 10 March 2019 12:26 PM
To: R-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Model failed to converge when a factor is centered or releveled

Hi all,

I'm running a mixed-effects model using lmer4::glmer() function. The
modeling works with R's default dummy coding. But if I center or relevel a
factor of 2 levels, the model failed to converge. I am wondering why.

Here is the data:
https://www.dropbox.com/s/ysw5rvxowamucoh/example.csv?dl=0

Here is the code for the model without centering or releveling the factor
"Memory":

> model_default <- glmer(ACC ~ Memory * Group + (1|Subject) +
(1+Memory|item), family = "binomial", data = df)

The model converges with the above code. But the following model failed to
converge after centering the factor "Memory":

> c1 <- c(-0.5, 0.5)
> contrasts(df$Memory) = cbind(c1)
> model_center <- glmer(ACC ~ Memory * Group + (1|Subject) +
(1+Memory|item), family = "binomial", data = df)

If I relevel the factor "Memory", the following model failed to converge
either:

> df$Memory <- factor(df$Memory, levels = c("Y", "X"))
> model_relevel <- glmer(ACC ~ Memory * Group + (1|Subject) +
(1+Memory|item), family = "binomial", data = df)

Why would centering or releveling a factor make a difference in terms of
convergence? Which result should I trust?


From I|@n@Re|n@te|n @end|ng |rom nyu|@ngone@org  Wed Mar 13 02:04:04 2019
From: I|@n@Re|n@te|n @end|ng |rom nyu|@ngone@org (Reinstein, Ilan)
Date: Wed, 13 Mar 2019 01:04:04 +0000
Subject: [R-sig-ME] Model Definition and Interpretation - Interactions,
 plus Singularity
In-Reply-To: <BN7PR05MB58573371909AF3F4CF8DF533CA4C0@BN7PR05MB5857.namprd05.prod.outlook.com>
References: <2aabb3b3f0f14118800e627361e4ff15@nyulangone.org>
 <0f1c4fb8-ddca-dca3-3cea-dcf0409b9b5a@mpi.nl>,
 <BN7PR05MB58573371909AF3F4CF8DF533CA4C0@BN7PR05MB5857.namprd05.prod.outlook.com>
Message-ID: <3af1b4237bdf45a2929816a004f554d3@nyulangone.org>


Hi Harold and Phillip, thanks for your reply and sorry for the delay.


Your insight was quite helpful. Some comments as I may have not been clear before, I apologize.


Harold - the difficulty estimates I mentioned were obtained by fitting an LLTM to our data. That is how we know the case types are different, additionally, this model provides item-level difficulty estimates as well. The problem is, we are only interested in capturing the change in ability at the case type level, suggesting that the subjects actually understand things at that level (type-vs-type, not item-by-item). I read the reference you sent, and although it was interesting, we are hitting some challenges when taking the IRT path for this analysis, so I am not sure that will help us.


Phillip- Thanks for pointing out the incorrect definition of the interaction I was using, the model makes more sense as you suggested. However, for both cases there is nor convergence, in fact, sometimes it is a singular fit while in other situations it just does not converge. Will using a different optimizer and setting the calc.derivs = FALSE in glmerControl help?


>> If each item belongs uniquely to a single caseType, then you can't estimate caseType by item effects, but you could potentially estimate sequence effects:


>> This latter model would pick up e.g. whether a given item is more or less likely to be corrected rated, beyond the effect of its overarching caseType or sequence position. lme4 will pick up on the "between" design aspects of the item component without you having to be more explicit.


Also, I am not sure I understand what you mean by sequence and case type effects over items. In this particular case, the number at which the item is presented (sequence) _does_ matter, since we are looking for different changes in ability by person, which in turn are set to be different according to the case type. What is the advantage and the interpretation of the inclusion of the variation over items, whether as a slope or an intercept?


Thanks again for your valuable input, it has been quite enriching in my lme4 journey.


Best,


Ilan

________________________________
From: Doran, Harold <HDoran at air.org>
Sent: Thursday, March 7, 2019 11:05:47 AM
To: Phillip Alday; Reinstein, Ilan; r-sig-mixed-models at r-project.org
Subject: RE: [R-sig-ME] Model Definition and Interpretation - Interactions, plus Singularity

Ilan

I was reading this a bit and I was wondering if you are estimating what you actually wanted to estimate? I was uncertain. It sounded from the description you provided that you wanted to measure change in abilities? But it appears from the lmer call you're estimating item effects (but you already have item parameters for those items?)

That is, your code reminds me of the paper we published on this some time ago, link below.

https://urldefense.proofpoint.com/v2/url?u=https-3A__www.jstatsoft.org_article_view_v020i02&d=DwIF-g&c=j5oPpO0eBH1iio48DtsedeElZfc04rx3ExJHeIIZuCs&r=D5erugTFg_izZGHSPIkFaZ8YL0JMFUHxjgMAYvCYYrc&m=ldVgcdweF3eXwKk7FsT_NXn8xCq_0yaW3zrmDzjgSVU&s=kNS6-nqxgh8MEHVNpEDx9gvVkRSf5TfDutE9_P5IsLw&e=

If you're interested in measuring change in abilities, you might consider direct estimation (like used in NAEP).

-----Original Message-----
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of Phillip Alday
Sent: Thursday, March 07, 2019 10:57 AM
To: Reinstein, Ilan <Ilan.Reinstein at nyulangone.org>; r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Model Definition and Interpretation - Interactions, plus Singularity

Hello,


On 6/3/19 7:53 pm, Reinstein, Ilan wrote:
> Hi all, I hope all is well.
>

::snip::

>
>
> 1. glmer(correct ~ scale(Sequence) + (scale(Sequence) | ID:caseType),
> family = binomial)
>
> 2. glmer(correct ~ scale(Sequence):caseType +
> (scale(Sequence):caseType | ID), family = binomial)


This model is a little unusual -- it includes interactions without main effects, which usually doesn't make sense.

I think you want:

glmer(correct ~ 1 + scale(Sequence)*caseType + (1+scale(Sequence)*caseType | ID), family = binomial)


(Note: I always make my intercept term explicit, both to remind that it's there, and because some of the other software I use doesn't add an implicit intercept.)

I wouldn't worry about achieving balance via artificial mehtods -- lme4 doesn't require it and the lack of balance will primarily show itself as a difference in the intercept term, which isn't a big deal, and to a lesser extent a difference in the standard errors -- they may actually be better because you have more data overall.


Finally, you have 15ish (or at least more than 10 based on your description and a logical leap) items per category, so you have more than enough items to also estimate item effects. If each item belongs uniquely to a single caseType, then you can't estimate caseType by item effects, but you could potentially estimate sequence effects:

glmer(correct ~ 1 + scale(Sequence)*caseType + (1+scale(Sequence)*caseType | ID) + (1+scale(Sequence)|Item), family =
binomial)

and if that fails to converge, you can try a model which just allows for just intercept-level variation by item:

glmer(correct ~ 1 + scale(Sequence)*caseType + (1+scale(Sequence)*caseType | ID) + (1|Item), family = binomial)


This latter model would pick up e.g. whether a given item is more or less likely to be corrected rated, beyond the effect of its overarching caseType or sequence position. lme4 will pick up on the "between" design aspects of the item component without you having to be more explicit.

About your convergence problems: assuming they still linger after adding in the main effects, I would interpret them as there not being enough data to estimate the by-subject and by-item differences in the sequence effect, which isn't horrible nor particularly surprising for me: I expect the overall effect of sequence and the general variation between subjects and items (i.e. the corresponding random intercepts) to be much larger than the variation between subjects and items in the sequence effect.

Best,
Phillip

>
> I've fitted these two models to capture the different learning rates by person and by case type but I am not sure about, first, if the interaction is correctly specified, and second, where and how to specify the interaction given the needs of my problem (person-case or # items-case, random or fixed). Are cases nested within persons, even if the number of items by case differs? Or is the interaction of case type with the number on the sequence more informative for my purpose?
>
>
> The first model's coef()/ranef() output is very attractive since I can have an Intercept and a Slope for the interaction of person and case type, however after carefully reviewing the answers in this discussion<https://urldefense.proofpoint.com/v2/url?u=https-3A__stats.stackexchange.com_questions_31569_questions-2Dabout-2Dhow-2Drandom-2Deffects-2Dare-2Dspecified-2Din-2Dlmer&d=DwIF-g&c=j5oPpO0eBH1iio48DtsedeElZfc04rx3ExJHeIIZuCs&r=D5erugTFg_izZGHSPIkFaZ8YL0JMFUHxjgMAYvCYYrc&m=ldVgcdweF3eXwKk7FsT_NXn8xCq_0yaW3zrmDzjgSVU&s=VwS7CoKV7lTomAkPsjwT5esdPBSGnsn6JmR5tHjDRzE&e=>, I moved to model number 2 since it made more sense in the interpretation, however I am unsure which is more appropriate for my needs. I am starting to get more inclined towards the second model but it is a singular fit (+1 correlation of random effects). I've looked for possible solutions without the need to go Bayesian, but I am not sure how to implement those either so I tried going to rstanarm. Are there any suggestions about the priors?
>
>
> I will continue to try out the different suggestions presented in the different threads around singularities on lme4.
>
>
> Finally, I looked for a suitable dataset for reproducibility but I hope this is more of a conceptual discussion.
>
>
> Similar questions about singular fits:
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stats.stackexchange.com_questions_378939_dealing-2Dwith-2Dsingular&d=DwIF-g&c=j5oPpO0eBH1iio48DtsedeElZfc04rx3ExJHeIIZuCs&r=D5erugTFg_izZGHSPIkFaZ8YL0JMFUHxjgMAYvCYYrc&m=ldVgcdweF3eXwKk7FsT_NXn8xCq_0yaW3zrmDzjgSVU&s=8rAnN6AMYnzzz-dAXUaWSqX1QuRYd4jezzQgkn_21Ho&e=
> -fit-in-mixed-models
>
>
> [https://urldefense.proofpoint.com/v2/url?u=https-3A__cdn.sstatic.net_Sites_stats_img_apple-2Dtouch-2Dicon-402.png-3Fv-3D344f&d=DwIF-g&c=j5oPpO0eBH1iio48DtsedeElZfc04rx3ExJHeIIZuCs&r=D5erugTFg_izZGHSPIkFaZ8YL0JMFUHxjgMAYvCYYrc&m=ldVgcdweF3eXwKk7FsT_NXn8xCq_0yaW3zrmDzjgSVU&s=_zS-mUiv6ehsv2uVG2lPuf62u-obBPrjlM1mspZ9EvA&e=
> 57aa10cc]<https://urldefense.proofpoint.com/v2/url?u=https-3A__stats.stackexchange.com_questions_378939_dealing-2Dwit&d=DwIF-g&c=j5oPpO0eBH1iio48DtsedeElZfc04rx3ExJHeIIZuCs&r=D5erugTFg_izZGHSPIkFaZ8YL0JMFUHxjgMAYvCYYrc&m=ldVgcdweF3eXwKk7FsT_NXn8xCq_0yaW3zrmDzjgSVU&s=tWwIjahfJzsNIBdq-LumQ2TQp35LciuRwmIC5BUao00&e=
> h-singular-fit-in-mixed-models>
>
> lme4 nlme - Dealing with singular fit in mixed models - Cross
> Validated<https://urldefense.proofpoint.com/v2/url?u=https-3A__stats.stackexchange.com_questions_378939_dealing-2Dwit&d=DwIF-g&c=j5oPpO0eBH1iio48DtsedeElZfc04rx3ExJHeIIZuCs&r=D5erugTFg_izZGHSPIkFaZ8YL0JMFUHxjgMAYvCYYrc&m=ldVgcdweF3eXwKk7FsT_NXn8xCq_0yaW3zrmDzjgSVU&s=tWwIjahfJzsNIBdq-LumQ2TQp35LciuRwmIC5BUao00&e=
> h-singular-fit-in-mixed-models>
> stats.stackexchange.com
> Let's say we have a model mod <- Y ~ X*Condition + (X*Condition|subject) # Y = logit variable # X = continuous variable # Condition = values A and B, dummy coded; the design is repeated ...
>
>
>
>
> Thank you in advance,
>
>
> Best,
>
>
> Ilan Reinstein
>
>
>
> ------------------------------------------------------------
> This email message, including any attachments, is for ...{{dropped:25}}


From @dcor@o @end|ng |rom v|m@@edu  Wed Mar 13 16:08:39 2019
From: @dcor@o @end|ng |rom v|m@@edu (Andrew D. Corso)
Date: Wed, 13 Mar 2019 15:08:39 +0000
Subject: [R-sig-ME] Normalized residuals and ACF plots for glmmTMB
Message-ID: <7f3317179f164ea2a37c782414f82842@vims.edu>

Hello,

I am trying to generate normalized residuals for a glmmTMB negative binomial zero inflated model in order to create ACF and pACF plots.

My model:
mod1 = glmmTMB(count ~ a + b + offset(c) + ar1(year+ 0|group), ziformula= ~a+d, data = dat, family = nbinom2

The residuals.glmmTMB command only has "response" and "pearson" as options for type. I'm curious if anyone has any suggestions for a compatible technique for normalized?

Thanks,
Andrew

	[[alternative HTML version deleted]]


From @|ex@ndre@@nto@br @end|ng |rom y@hoo@com@br  Wed Mar 13 23:02:03 2019
From: @|ex@ndre@@nto@br @end|ng |rom y@hoo@com@br (ASANTOS)
Date: Wed, 13 Mar 2019 18:02:03 -0400
Subject: [R-sig-ME] Comparative plot between zero inflated poisson model
 (ZIP) and zero inflated poisson mixed model
Message-ID: <e154e949-60f3-6005-a5b4-11cbde8ca230@yahoo.com.br>

Dear Members,

I've like to plot zero inflated poisson model (ZIP) and zero inflated 
poisson mixed model for look the differences in the models adjusted. For 
this I try:

#Packages
library(pscl) #For zero inflated poisson
library(glmmTMB)# For mixed zero inflated poisson
library(ggplot2) #Plot the results
library(gridExtra) ## Put 2 ggplot together


# Artificial data set
set.seed(007)
n <- 100 # number of subjects
K <- 8 # number of measurements per subject
t_max <- 5 # maximum follow-up time
DF <- data.frame(id = rep(seq_len(n), each = K),
 ???????????????? time = c(replicate(n, c(0, sort(runif(K - 1, 0, 
t_max))))),
 ???????????????? sex = rep(gl(2, n/2, labels = c("male", "female")), 
each = K))
DF$y <- rnbinom(n * K, size = 2, mu = exp(1.552966))
str(DF)

# Using zero inflated poisson model with pscl package
time2<-(DF$time)^2
mZIP <- zeroinfl(y~time+time2+sex|time+sex, data=DF)
summary(mZIP)

#If I imagine that all coefficients are significant

# Y estimated
pred.data1 = data.frame(
time<-DF$time,
time2<-(DF$time)^2,
sex<-DF$sex)
pred.data1$y = predict(mZIP, newdata=pred.data1, type="response")


# Now using zero inflated poisson mixed model with glmmTMB package
mZIPmix<- glmmTMB(y~time+time2+sex+(1|id),
data=DF, ziformula=~1,family=poisson)
summary(mZIPmix)
#

# new Y estimated
pred.data2 = data.frame(
time<-DF$time,
time2<-(DF$time)^2,
sex<-DF$sex,
id<-DF$id)
pred.data2$y = predict(mZIPmix, newdata=pred.data2, type="response")


# Plot zero inflated poisson model and mixed poisson model
par(mfrow=c(1,2))
plot1<-ggplot(DF, aes(time, y, colour=sex)) +
 ? labs(title="Zero inflated model") +
 ? geom_point() +
 ? geom_line(data=pred.data1) +
 ? stat_smooth(method="glm", family=poisson(link="log"), formula = 
y~poly(x,2),fullrange=TRUE)

plot2<-ggplot(DF, aes(time, y, colour=sex)) +
 ? labs(title="Zero inflated mixed model") +
 ? geom_point() +
 ? geom_line(data=pred.data2) +
 ? stat_smooth(method="glm", family=poisson(link="log"), formula = 
y~poly(x,2),fullrange=TRUE)## here a don't find any method to mixed glm
grid.arrange(plot1, plot2, ncol=2)
#-

But doesn't work of sure. Please, how I could make this two plots for 
direct comparing purposes?

Thanks in advanced,

Alexandre

-- 
======================================================================
Alexandre dos Santos
Prote??o Florestal
IFMT - Instituto Federal de Educa??o, Ci?ncia e Tecnologia de Mato Grosso
Campus C?ceres
Caixa Postal 244
Avenida dos Ramires, s/n
Bairro: Distrito Industrial
C?ceres - MT                      CEP: 78.200-000
Fone: (+55) 65 99686-6970 (VIVO) (+55) 65 3221-2674 (FIXO)

         alexandre.santos at cas.ifmt.edu.br
Lattes: http://lattes.cnpq.br/1360403201088680
OrcID: orcid.org/0000-0001-8232-6722
Researchgate: www.researchgate.net/profile/Alexandre_Santos10
LinkedIn: br.linkedin.com/in/alexandre-dos-santos-87961635
Mendeley:www.mendeley.com/profiles/alexandre-dos-santos6/


From L@R@E|||ott @end|ng |rom exeter@@c@uk  Thu Mar 14 12:05:51 2019
From: L@R@E|||ott @end|ng |rom exeter@@c@uk (Elliott, Lewis)
Date: Thu, 14 Mar 2019 11:05:51 +0000
Subject: [R-sig-ME] Zero slope-slope covariance models in lme4
Message-ID: <LO2P265MB111706E748253D6253F3D5FF804B0@LO2P265MB1117.GBRP265.PROD.OUTLOOK.COM>

Dear all,

I have been attempting to fit glmer model objects in lme4 with correlated random intercepts and slopes. A minimal reproducible example of my situation might be:

require(tidyverse)
require(lme4)

dat <- mtcars %>%
  as_tibble() %>%
  mutate_at(vars(am,vs,gear,carb), all_vars(factor(.))) %>%
  select(am,vs,gear,carb)

fit <- glmer(am ~ vs + gear + (vs + gear|carb),
             dat=dat, family="binomial",control=glmerControl(calc.derivs=F), nAGQ=0)

However, the output of VarCorr:

Groups      Name        Std.Dev.    Corr
carb        (Intercept) 1.0938
vs1         1.0955      0.082
gear4       1.1446      0.080       0.294
gear5       1.0308      -0.037      -0.010      0.013

...fits correlations between all random slope terms. I am only interested in intercept-slope correlations and not slope-slope correlations.

In other words, I wish to constrain all correlations that are not intercept-slope correlations (i.e. all possible slope-slope correlations) to zero.

I have read the excellent advice on how to do this on Stack Overflow<https://stackoverflow.com/questions/35399127/lme4-how-to-specify-2-correlations-with-random-intercept-without-adding-correl/35401176#35401176> and in Appendix A.1 of the 2015 publication in Journal of Statistical Software ("Fitting Linear Mixed-Effects Models Using lme4") on how to do this using modular functions.

However, both of the examples given only demonstrate how to constrain one slope-slope correlation to zero. Is there a general way of constraining multiple (all) slope-slope correlations to zero using a similar method to those described on Stack Overflow and in the publication?

N.B In reality I have a large number of random slope terms specified in various models which are all factor variables with varying numbers of levels. This is why I'm looking for a general way rather than a way that is specific to each variance-covariance matrix for every model. I believe a solution might involve 'looping' over all non-first-row/first-column elements of the variance-covariance matrix and constraining these to zero, but I can't get my head around how this might be done.

My thanks in advance (and apologies for using correlation and covariance so interchangeably above).

Lewis

	[[alternative HTML version deleted]]


From he|n@v@n@||ever|oo @end|ng |rom v|@etern@@n|  Thu Mar 14 15:46:58 2019
From: he|n@v@n@||ever|oo @end|ng |rom v|@etern@@n| (Hein van Lieverloo)
Date: Thu, 14 Mar 2019 15:46:58 +0100
Subject: [R-sig-ME] Overdispersed and zero-inflated - or not - and if so,
 how to model them? #glmmTMB
Message-ID: <015801d4da74$c7397500$55ac5f00$@viaeterna.nl>

Dear all,

Keywords: #glmmTMB  #overdisp  #zero_count

I am grateful for this mailing list and in advance, for any helpful
response.
This e-mail has two related questions.
Details (summary, background, approach and results) are given below them.

Question 1: my data are zero-inflated and overdispersed, but what does the
overdispersion parameter in glmmTMB (genpois, negbin1, negbin2) tell me? 
	It is very high in genpois and negbin1 models (see question 2) and I
thought it should be near 1, like in negbin2 (>> 1 is overdispersed, <<1 is
underdispersed)
	But when I test these generalized models for overdispersion
(overdisp from sjstats), no overdispersion is indicated.

Question 2: should I use Gaussian on log(counts) with AIC 2068  or use
negbin2 with AIC 8036 and add overdispersion and zero-inflation models to
get a lower AIC (and if so, how?)
	When I use glmmTMB on counts with poisson, I get an AIC of 117 856.
Testing the model with overdisp and zero_count (from the sjstats package), I
find p = 0 (overdispersed) and zc-ratio 0.81 (probable zero-inflation).
	When I use glmmTMB on log10(counts), with 0's estimated to 0.1 so
resulting in -1, I get an AIC of 2068? (with lmer: 2122). Looks fine, but
may be wrong.
	When I use glmmTMB on counts with either genpois (dispersion par
613), negbinom1 (dispersion par 287) or negbinom2 (dispersion par 0.72), I
get AIC's over 8036. Much higher, but may be ok.

	My data are zero-inflated and overdispersed and I would think that
glmmTMB with generalized models would result in much better models (lower
AIC) than simply working with the log-transformed data.
	The p-values per variable are similar enough, by the way, see the
best two models at the end of this mail.
	Of course, simply transforming 0 counts into -1 at the log-level
could be the cause and this approach may oversimplify reality and the AIC of
2068 could be artificial.
	If overdispersion and zero-inflation really is necessary, do I need
to get the AIC ?down from 8036 to 2068 or can I accept higher AICs? I
suppose I can.
	But then: how should I approach the development of the zi-model
and/or the overdispersion models? 
	I know, from theory, but the thing is, there is little of no
research on invertebrates in drinking water distribution systems and their
structure is so different from surface water systems, that we are developing
hypotheses from this data set.


Summary of design and model
- Invertebrates in drinking water distribution systems in The Netherlands:
1993-1995 (yes, very old data!).
- glmmTMB of multilevel model ?(1 | vNr / lNr)? : 34 systems (v), 175
sampling locations (l, ~5/system), 1301 samples (~ 8 quarters from
1993-1995), a multitude of variables measured.
- One of the best model tested: lWapit (count data) ?~ pTDOC + tCa + logtMn
+ lnOType + logbS500 + bTemp + blWavlo + blRoeiNaup + blMoskr


Background
The data were collected in the '90's and basic results were published in
2012:
https://www.sciencedirect.com/science/article/abs/pii/S0043135412002217?via%
3Dihub
Dissolved organic matter is the best (causal / proxy / collinear?) predictor
for energy and carbon supply (R2 ~ 0.6 on mean estimated mean biomass at the
system level). ?
I can send you the paper if you want. Also, I can sent more details, short
of the data set.
Since, when I have time (no funding), I try to find more predictors, at more
than just the highest aggregated level (system). I followed some courses on
multilevel modeling was well.
In 2013 a statistician using GenStat told me my data were zero-inflated and
overdispersed.
So, no glmm with Poisson response possible. The only option was: first a
glmm binomial for absence - presence, then glmm Poisson on the
presence-data.

The past two weeks (finally, I found some time again) I was and am so happy
to find Ben Bolker's ?glmmTMB, able to work with zero-inflation and
overdispersion (I heard of MCMC options in 2017, no time then).
Learning from Ben Bolker's Salamanders-work, I managed to come a long way,
but I have not been able to develops stable overdispersion or zero-inflation
generalized models that significantly lower AIC in glmmTMB.
Although I teach the basics of statistics and made a lot of LM-models, I am
not a statistician (I'm a biologist happily forced toward statistics), and I
find a lot of details and mathematics hard to grasp:
https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html



Model and comparison approach
? System-level variable names start with p, location-level variables start
with l, t or log t, sample-level variables start with b or logb. Only
lnOType is a three types factor (bl are log-counts of other taxa)
? tCa and tMn = calcium and manganese in tap water (mean over time), lnOType
= village, city or rural environment, lbS500 = sediment > 500 um per sample,
bTemp = temperature sample
? blWavlo, blRoeipNaup, blMoskr = log(count(taxon; -1 <- 0)) per sample for
Cladocera, Copepoda, Ostracoda 
? 0-model contains no parameters (response ~ 1), 1-model contains major
predictor (pTDOC), full model contains 21 likely/possible predictors
? Model is kept identical in all regressions, although other versions may
have lower AIC
? Model data for comparison = all data? (during model development, systems
were randomly split approx. 60-40)
? I did not include overdispersion or zero-inflated models yet, as I am not
sure whether it is necessary and I cannot get the basic ones (e.g. just with
pTDOC) stable. I can imagine that adding empty ZI-models is not very
effective in countering zero-inflation


Results (I can send more details, if required)

AIC per model (dispersion only for best model: x-model)

multilevel model:  + (1|vNr / lNr) for all except lm

	

response = blWapit = log(count(bWapi)), where -1 <- 0)  (counts expressed
per m3)

lm		0-model	1-model	x-model	full model
Gaussian	4293.4		4014.5		3778.2		3642.9

	

lmer		0-model	1-model	x-model	full model
Gaussian	2185.8		2122		2121.9		2185.8

	

glmmTMB	0-model	1-model	x-model	full model
Gaussian	 2128.7		2116.6		2068.2		2074

	

response = b4Wapit = count(bWapi) expressed as rounded per 4 m3 (most sample
volumes are very close to that)

glmmTMB	Disp ratio (p)	Dispersion par	zc ratio		zi-model
0-model	1-model	x-model	full model	remarks
poisson		99.4 (0) *	NA		0.81 **		NA
137165		137157		117856		114773		* p (H0: not
overdispersed) **zero-inflation probable
genpois		0.34 (1)		613		NA		NA
8096.8		8088.1		8036.1		8042.7	
genpois (+ZI)	NA		603		NA		zi =~ 1
8094.1		8085.5		8036.6		8043.1	
trunc genpois	NA		701 (1-model)	NA		zi =~ 1
9109.7		9097.5		*		*		*with zi =
~1 or zi =~pTDOC, non-positive-definite Hessian matrix
nbinom1	0.53 (1)		287		NA		NA
8306.4 *	8297.7 *	8244.6 *	8251.8 *	* warnings:
In f(par, order = order, ...) : value out of range in 'lgamma'
nbinom1 (+ZI)	NA		287		NA		zi =~ 1
8306.4		8299.7		8246.6 *	8253.7		* warnings:
In f(par, order = order, ...) : value out of range in 'lgamma'
nbinom2	0.78		0.72		NA		NA
8224.1		8216.0		8165.3		8171.8	
nbinom2 (+ZI)	NA		0.787		NA		zi =~ 1
8226.1		8218.0		8165.3		8172.6	


Comparing the best generalized glmmTMB model (nbinom2) on counts with the
best Gaussian model on log10(counts, 0 -> -1)

Family: nbinom2  ( log )
Family: gaussian  ( identity )					
Formula:          b4Wapit ~ pTDOC + tCa + logtMn + lnOType + logbS500 +
bTemp +  	Formula:          blWapit ~ pTDOC + tCa + logtMn + lnOType +
logbS500 + bTemp +  					
    blWavlo + blRoeiNaup + blMoskr + (1 | vNr/lNr)
blWavlo + blRoeiNaup + blMoskr + (1 | vNr/lNr)

Data: AllData
Data: AllData					
	

     AIC      BIC   logLik deviance df.resid
AIC      BIC   logLik deviance df.resid

  8165.3   8237.7  -4068.6   8137.3     1287
2068.2   2140.6  -1020.1   2040.2     1287

	

Random effects:
Random effects:					
	

Conditional model:
Conditional model:					
 Groups  Name        Variance Std.Dev.
Groups   Name        Variance Std.Dev.					
 lNr:vNr (Intercept) 4.325    2.080
lNr:vNr  (Intercept) 0.4850   0.6964  					
 vNr     (Intercept) 5.913    2.432
vNr      (Intercept) 0.4001   0.6326  					
	
Residual             0.1794   0.4236  					
Number of obs: 1301, groups:  lNr:vNr, 175; vNr, 34
Number of obs: 1301, groups:  lNr:vNr, 175; vNr, 34

	

Overdispersion parameter for nbinom2 family (): 0.72
Dispersion estimate for gaussian family (sigma^2): 0.179

	

Conditional model:
Conditional model:					
	Estimate Std. Error z value Pr(>|z|)
Estimate Std. Error z value Pr(>|z|)

(Intercept) -5.25438    1.80820  -2.906 0.003662 **
(Intercept) -1.566112   0.487738  -3.211  0.00132 **

pTDOC        0.95740    0.26583   3.602 0.000316 ***
pTDOC        0.298345   0.070836   4.212 2.53e-05 ***

tCa          0.06371    0.01623   3.926 8.64e-05 ***
tCa          0.013963   0.004579   3.050  0.00229 **

logtMn       0.83018    0.49447   1.679 0.093164 .
logtMn       0.243523   0.135480   1.797  0.07226 .

lnOTypeland  0.97151    0.51131   1.900 0.057425 .
lnOTypeland  0.390505   0.152519   2.560  0.01046 *

lnOTypestad -0.72832    0.82751  -0.880 0.378788
lnOTypestad  0.112042   0.231978   0.483  0.62911

logbS500     0.44416    0.10870   4.086 4.39e-05 ***
logbS500     0.127756   0.029836   4.282 1.85e-05 ***

bTemp        0.03655    0.01301   2.810 0.004948 **
bTemp        0.007290   0.003264   2.234  0.02551 *

blWavlo      0.14475    0.04158   3.481 0.000500 ***
blWavlo      0.036470   0.011718   3.112  0.00186 **

blRoeiNaup   0.11404    0.05684   2.006 0.044818 *
blRoeiNaup   0.042573   0.015790   2.696  0.00701 **

blMoskr     -0.25836    0.11832  -2.184 0.028993 *
blMoskr     -0.066737   0.030350  -2.199  0.02788 *

---
---		
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1



Many thanks in advance for your help!

Kind regards,

Hein van Lieverloo



Met vriendelijke groet,

Hein van Lieverloo


From mo|||eebrook@ @end|ng |rom gm@||@com  Thu Mar 14 16:34:20 2019
From: mo|||eebrook@ @end|ng |rom gm@||@com (Mollie Brooks)
Date: Thu, 14 Mar 2019 16:34:20 +0100
Subject: [R-sig-ME] 
 Overdispersed and zero-inflated - or not - and if so,
 how to model them? #glmmTMB
In-Reply-To: <015801d4da74$c7397500$55ac5f00$@viaeterna.nl>
References: <015801d4da74$c7397500$55ac5f00$@viaeterna.nl>
Message-ID: <DE824630-F149-4D53-BCCC-5ED3E1F0480F@gmail.com>

Dear Hein,

See replies below...

> On 14Mar 2019, at 15:46, Hein van Lieverloo <hein.van.lieverloo at viaeterna.nl> wrote:
> 
> Dear all,
> 
> Keywords: #glmmTMB  #overdisp  #zero_count
> 
> I am grateful for this mailing list and in advance, for any helpful
> response.
> This e-mail has two related questions.
> Details (summary, background, approach and results) are given below them.
> 
> Question 1: my data are zero-inflated and overdispersed, but what does the
> overdispersion parameter in glmmTMB (genpois, negbin1, negbin2) tell me? 
> 	It is very high in genpois and negbin1 models (see question 2) and I
> thought it should be near 1, like in negbin2 (>> 1 is overdispersed, <<1 is
> underdispersed)
> 	But when I test these generalized models for overdispersion
> (overdisp from sjstats), no overdispersion is indicated.

The dispersion parameter in a glmmTMB model is there to handle the dispersion and it?s fine if it?s different from 1. So your tests with sjstats seemed to be correct. For descriptions of how the dispersion parameters relate to the variance, see ?sigma.glmmTMB

> 
> Question 2: should I use Gaussian on log(counts) with AIC 2068  or use
> negbin2 with AIC 8036 and add overdispersion and zero-inflation models to
> get a lower AIC (and if so, how?)
> 	When I use glmmTMB on counts with poisson, I get an AIC of 117 856.
> Testing the model with overdisp and zero_count (from the sjstats package), I
> find p = 0 (overdispersed) and zc-ratio 0.81 (probable zero-inflation).
> 	When I use glmmTMB on log10(counts), with 0's estimated to 0.1 so
> resulting in -1, I get an AIC of 2068  (with lmer: 2122). Looks fine, but
> may be wrong.
> 	When I use glmmTMB on counts with either genpois (dispersion par
> 613), negbinom1 (dispersion par 287) or negbinom2 (dispersion par 0.72), I
> get AIC's over 8036. Much higher, but may be ok.

You can?t compare the models of the log-transformed data to the raw data. For example,
> set.seed(1)
> x=rpois(100, lambda=5)
> AIC(glmmTMB(log(x)~1))
[1] 128.0742
> AIC(glmmTMB(x~1, family=poisson))
[1] 422.911

or see discussion here https://stats.stackexchange.com/questions/61332/comparing-aic-of-a-model-and-its-log-transformed-version

> 
> 	My data are zero-inflated and overdispersed and I would think that
> glmmTMB with generalized models would result in much better models (lower
> AIC) than simply working with the log-transformed data.
> 	The p-values per variable are similar enough, by the way, see the
> best two models at the end of this mail.
> 	Of course, simply transforming 0 counts into -1 at the log-level
> could be the cause and this approach may oversimplify reality and the AIC of
> 2068 could be artificial.
> 	If overdispersion and zero-inflation really is necessary, do I need
> to get the AIC  down from 8036 to 2068 or can I accept higher AICs? I
> suppose I can.
> 	But then: how should I approach the development of the zi-model
> and/or the overdispersion models? 
> 	I know, from theory, but the thing is, there is little of no
> research on invertebrates in drinking water distribution systems and their
> structure is so different from surface water systems, that we are developing
> hypotheses from this data set.
> 
> 
> Summary of design and model
> - Invertebrates in drinking water distribution systems in The Netherlands:
> 1993-1995 (yes, very old data!).
> - glmmTMB of multilevel model  (1 | vNr / lNr)  : 34 systems (v), 175
> sampling locations (l, ~5/system), 1301 samples (~ 8 quarters from
> 1993-1995), a multitude of variables measured.
> - One of the best model tested: lWapit (count data)  ~ pTDOC + tCa + logtMn
> + lnOType + logbS500 + bTemp + blWavlo + blRoeiNaup + blMoskr
> 
> 
> Background
> The data were collected in the '90's and basic results were published in
> 2012:
> https://www.sciencedirect.com/science/article/abs/pii/S0043135412002217?via%
> 3Dihub
> Dissolved organic matter is the best (causal / proxy / collinear?) predictor
> for energy and carbon supply (R2 ~ 0.6 on mean estimated mean biomass at the
> system level).  
> I can send you the paper if you want. Also, I can sent more details, short
> of the data set.
> Since, when I have time (no funding), I try to find more predictors, at more
> than just the highest aggregated level (system). I followed some courses on
> multilevel modeling was well.
> In 2013 a statistician using GenStat told me my data were zero-inflated and
> overdispersed.
> So, no glmm with Poisson response possible. The only option was: first a
> glmm binomial for absence - presence, then glmm Poisson on the
> presence-data.
> 
> The past two weeks (finally, I found some time again) I was and am so happy
> to find Ben Bolker's  glmmTMB, able to work with zero-inflation and
> overdispersion (I heard of MCMC options in 2017, no time then).
> Learning from Ben Bolker's Salamanders-work, I managed to come a long way,
> but I have not been able to develops stable overdispersion or zero-inflation
> generalized models that significantly lower AIC in glmmTMB.
> Although I teach the basics of statistics and made a lot of LM-models, I am
> not a statistician (I'm a biologist happily forced toward statistics), and I
> find a lot of details and mathematics hard to grasp:
> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html
> 

I?m glad that glmmTMB is solving some of your long-standing problems and I agree that Ben Bolker has contributed immensely to glmmTMB and GLMMs in general, but calling it "Ben Bolker?s glmmTMB" is disregarding the other developers of the package and documentation.

> 
> 
> Model and comparison approach
> ? System-level variable names start with p, location-level variables start
> with l, t or log t, sample-level variables start with b or logb. Only
> lnOType is a three types factor (bl are log-counts of other taxa)
> ? tCa and tMn = calcium and manganese in tap water (mean over time), lnOType
> = village, city or rural environment, lbS500 = sediment > 500 um per sample,
> bTemp = temperature sample
> ? blWavlo, blRoeipNaup, blMoskr = log(count(taxon; -1 <- 0)) per sample for
> Cladocera, Copepoda, Ostracoda 
> ? 0-model contains no parameters (response ~ 1), 1-model contains major
> predictor (pTDOC), full model contains 21 likely/possible predictors
> ? Model is kept identical in all regressions, although other versions may
> have lower AIC
> ? Model data for comparison = all data  (during model development, systems
> were randomly split approx. 60-40)

Make sure you?re using the same data for all models in AIC comparisons. 


> ? I did not include overdispersion or zero-inflated models yet, as I am not
> sure whether it is necessary and I cannot get the basic ones (e.g. just with
> pTDOC) stable. I can imagine that adding empty ZI-models is not very
> effective in countering zero-inflation
> 

For people in your situation, I typically recommend fitting a negative binomial model (see Warton, Environmetrics 2005), then testing for zero-inflation (I typically use DHARMa, but it sounds like sjstats does this also). Then if you have zero-inflation, you could fit a zero-inflated negative binomial. Then if the nbinom2 dispersion parameter in the conditional model gets very large, it means you might as well use a zero-inflated Poisson (see nbinom2 in ?sigma.glmmTMB for the reason). However, the best distribution could change depending on the predictors in the model because a model that explains less of the variance might have more dispersion. As you saw in the salamander examples (Brooks et al. 2017, R Journal, Appendix A), you can try different zero-inflation models.

cheers,
Mollie

> 
> Results (I can send more details, if required)
> 
> AIC per model (dispersion only for best model: x-model)
> 
> multilevel model:  + (1|vNr / lNr) for all except lm
> 
> 	
> 
> response = blWapit = log(count(bWapi)), where -1 <- 0)  (counts expressed
> per m3)
> 
> lm		0-model	1-model	x-model	full model
> Gaussian	4293.4		4014.5		3778.2		3642.9
> 
> 	
> 
> lmer		0-model	1-model	x-model	full model
> Gaussian	2185.8		2122		2121.9		2185.8
> 
> 	
> 
> glmmTMB	0-model	1-model	x-model	full model
> Gaussian	 2128.7		2116.6		2068.2		2074
> 
> 	
> 
> response = b4Wapit = count(bWapi) expressed as rounded per 4 m3 (most sample
> volumes are very close to that)
> 
> glmmTMB	Disp ratio (p)	Dispersion par	zc ratio		zi-model
> 0-model	1-model	x-model	full model	remarks
> poisson		99.4 (0) *	NA		0.81 **		NA
> 137165		137157		117856		114773		* p (H0: not
> overdispersed) **zero-inflation probable
> genpois		0.34 (1)		613		NA		NA
> 8096.8		8088.1		8036.1		8042.7	
> genpois (+ZI)	NA		603		NA		zi =~ 1
> 8094.1		8085.5		8036.6		8043.1	
> trunc genpois	NA		701 (1-model)	NA		zi =~ 1
> 9109.7		9097.5		*		*		*with zi =
> ~1 or zi =~pTDOC, non-positive-definite Hessian matrix
> nbinom1	0.53 (1)		287		NA		NA
> 8306.4 *	8297.7 *	8244.6 *	8251.8 *	* warnings:
> In f(par, order = order, ...) : value out of range in 'lgamma'
> nbinom1 (+ZI)	NA		287		NA		zi =~ 1
> 8306.4		8299.7		8246.6 *	8253.7		* warnings:
> In f(par, order = order, ...) : value out of range in 'lgamma'
> nbinom2	0.78		0.72		NA		NA
> 8224.1		8216.0		8165.3		8171.8	
> nbinom2 (+ZI)	NA		0.787		NA		zi =~ 1
> 8226.1		8218.0		8165.3		8172.6	
> 
> 
> Comparing the best generalized glmmTMB model (nbinom2) on counts with the
> best Gaussian model on log10(counts, 0 -> -1)
> 
> Family: nbinom2  ( log )
> Family: gaussian  ( identity )					
> Formula:          b4Wapit ~ pTDOC + tCa + logtMn + lnOType + logbS500 +
> bTemp +  	Formula:          blWapit ~ pTDOC + tCa + logtMn + lnOType +
> logbS500 + bTemp +  					
>    blWavlo + blRoeiNaup + blMoskr + (1 | vNr/lNr)
> blWavlo + blRoeiNaup + blMoskr + (1 | vNr/lNr)
> 
> Data: AllData
> Data: AllData					
> 	
> 
>     AIC      BIC   logLik deviance df.resid
> AIC      BIC   logLik deviance df.resid
> 
>  8165.3   8237.7  -4068.6   8137.3     1287
> 2068.2   2140.6  -1020.1   2040.2     1287
> 
> 	
> 
> Random effects:
> Random effects:					
> 	
> 
> Conditional model:
> Conditional model:					
> Groups  Name        Variance Std.Dev.
> Groups   Name        Variance Std.Dev.					
> lNr:vNr (Intercept) 4.325    2.080
> lNr:vNr  (Intercept) 0.4850   0.6964  					
> vNr     (Intercept) 5.913    2.432
> vNr      (Intercept) 0.4001   0.6326  					
> 	
> Residual             0.1794   0.4236  					
> Number of obs: 1301, groups:  lNr:vNr, 175; vNr, 34
> Number of obs: 1301, groups:  lNr:vNr, 175; vNr, 34
> 
> 	
> 
> Overdispersion parameter for nbinom2 family (): 0.72
> Dispersion estimate for gaussian family (sigma^2): 0.179
> 
> 	
> 
> Conditional model:
> Conditional model:					
> 	Estimate Std. Error z value Pr(>|z|)
> Estimate Std. Error z value Pr(>|z|)
> 
> (Intercept) -5.25438    1.80820  -2.906 0.003662 **
> (Intercept) -1.566112   0.487738  -3.211  0.00132 **
> 
> pTDOC        0.95740    0.26583   3.602 0.000316 ***
> pTDOC        0.298345   0.070836   4.212 2.53e-05 ***
> 
> tCa          0.06371    0.01623   3.926 8.64e-05 ***
> tCa          0.013963   0.004579   3.050  0.00229 **
> 
> logtMn       0.83018    0.49447   1.679 0.093164 .
> logtMn       0.243523   0.135480   1.797  0.07226 .
> 
> lnOTypeland  0.97151    0.51131   1.900 0.057425 .
> lnOTypeland  0.390505   0.152519   2.560  0.01046 *
> 
> lnOTypestad -0.72832    0.82751  -0.880 0.378788
> lnOTypestad  0.112042   0.231978   0.483  0.62911
> 
> logbS500     0.44416    0.10870   4.086 4.39e-05 ***
> logbS500     0.127756   0.029836   4.282 1.85e-05 ***
> 
> bTemp        0.03655    0.01301   2.810 0.004948 **
> bTemp        0.007290   0.003264   2.234  0.02551 *
> 
> blWavlo      0.14475    0.04158   3.481 0.000500 ***
> blWavlo      0.036470   0.011718   3.112  0.00186 **
> 
> blRoeiNaup   0.11404    0.05684   2.006 0.044818 *
> blRoeiNaup   0.042573   0.015790   2.696  0.00701 **
> 
> blMoskr     -0.25836    0.11832  -2.184 0.028993 *
> blMoskr     -0.066737   0.030350  -2.199  0.02788 *
> 
> ---
> ---		
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> 
> 
> Many thanks in advance for your help!
> 
> Kind regards,
> 
> Hein van Lieverloo
> 
> 
> 
> Met vriendelijke groet,
> 
> Hein van Lieverloo
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


???????????
Mollie E. Brooks, Ph.D.
Research Scientist
National Institute of Aquatic Resources
Technical University of Denmark


	[[alternative HTML version deleted]]


From t|m@co|e @end|ng |rom uc|@@c@uk  Thu Mar 14 18:45:05 2019
From: t|m@co|e @end|ng |rom uc|@@c@uk (Cole, Tim)
Date: Thu, 14 Mar 2019 17:45:05 +0000
Subject: [R-sig-ME] R-sig-mixed-models Digest, Vol 147, Issue 14
In-Reply-To: <mailman.17334.3168.1552577861.8486.r-sig-mixed-models@r-project.org>
References: <mailman.17334.3168.1552577861.8486.r-sig-mixed-models@r-project.org>
Message-ID: <59480C4E-6BE7-432F-A798-CAE19DCADA5F@ucl.ac.uk>

Dear Mollie,

That stack exchange link cites Akaike (1978) as saying you can compare models with differently transformed Y variables so long as the transformation?s Jacobian is included. In fact the earlier classic Box & Cox (1964) paper says the same thing and shows how to do it.

The AICadj and BICadj functions in my sitar package adjust for a Box-Cox transformed Y variable, including log(Y). The code below shows that the log transform provides a worse fit:

> library(glmmTMB)
> set.seed(1)
> x=rpois(100, lambda=5)
> AIC(glmmTMB(x~1, family=poisson))
[1] 422.9
> AIC(glmmTMB(log(x)~1))
[1] 128.1

> library(sitar)
> AICadj(glmmTMB(x~1, family=poisson))
glmmTMB(x ~ 1, family = poisson)
                           422.9
> AICadj(glmmTMB(log(x)~1))
glmmTMB(log(x) ~ 1)
              436.2

Best wishes,
Tim Cole
--
Population Policy and Practice Programme
UCL Great Ormond Street Institute of Child Health,
30 Guilford Street, London WC1N 1EH, UK

------------------------------
Date: Thu, 14 Mar 2019 16:34:20 +0100
From: Mollie Brooks <mollieebrooks at gmail.com<mailto:mollieebrooks at gmail.com>>
To: Hein van Lieverloo <hein.van.lieverloo at viaeterna.nl<mailto:hein.van.lieverloo at viaeterna.nl>>
Cc: R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME]  Overdispersed and zero-inflated - or not -
                and if so, how to model them? #glmmTMB
Message-ID: <DE824630-F149-4D53-BCCC-5ED3E1F0480F at gmail.com<mailto:DE824630-F149-4D53-BCCC-5ED3E1F0480F at gmail.com>>
Content-Type: text/plain; charset="utf-8"

Dear Hein,

See replies below...

On 14Mar 2019, at 15:46, Hein van Lieverloo <hein.van.lieverloo at viaeterna.nl<mailto:hein.van.lieverloo at viaeterna.nl>> wrote:
Dear all,
Keywords: #glmmTMB  #overdisp  #zero_count
I am grateful for this mailing list and in advance, for any helpful
response.
This e-mail has two related questions.
Details (summary, background, approach and results) are given below them.
Question 1: my data are zero-inflated and overdispersed, but what does the
overdispersion parameter in glmmTMB (genpois, negbin1, negbin2) tell me?
           It is very high in genpois and negbin1 models (see question 2) and I
thought it should be near 1, like in negbin2 (>> 1 is overdispersed, <<1 is
underdispersed)
           But when I test these generalized models for overdispersion
(overdisp from sjstats), no overdispersion is indicated.

The dispersion parameter in a glmmTMB model is there to handle the dispersion and it?s fine if it?s different from 1. So your tests with sjstats seemed to be correct. For descriptions of how the dispersion parameters relate to the variance, see ?sigma.glmmTMB

Question 2: should I use Gaussian on log(counts) with AIC 2068  or use
negbin2 with AIC 8036 and add overdispersion and zero-inflation models to
get a lower AIC (and if so, how?)
           When I use glmmTMB on counts with poisson, I get an AIC of 117 856.
Testing the model with overdisp and zero_count (from the sjstats package), I
find p = 0 (overdispersed) and zc-ratio 0.81 (probable zero-inflation).
           When I use glmmTMB on log10(counts), with 0's estimated to 0.1 so
resulting in -1, I get an AIC of 2068  (with lmer: 2122). Looks fine, but
may be wrong.
           When I use glmmTMB on counts with either genpois (dispersion par
613), negbinom1 (dispersion par 287) or negbinom2 (dispersion par 0.72), I
get AIC's over 8036. Much higher, but may be ok.

You can?t compare the models of the log-transformed data to the raw data. For example,
set.seed(1)
x=rpois(100, lambda=5)
AIC(glmmTMB(log(x)~1))
[1] 128.0742
AIC(glmmTMB(x~1, family=poisson))
[1] 422.911

or see discussion here https://stats.stackexchange.com/questions/61332/comparing-aic-of-a-model-and-its-log-transformed-version


	[[alternative HTML version deleted]]


From he|n@v@n@||ever|oo @end|ng |rom v|@etern@@n|  Thu Mar 14 19:16:47 2019
From: he|n@v@n@||ever|oo @end|ng |rom v|@etern@@n| (Hein van Lieverloo)
Date: Thu, 14 Mar 2019 19:16:47 +0100
Subject: [R-sig-ME] 
 Overdispersed and zero-inflated - or not - and if so,
 how to model them? #glmmTMB
In-Reply-To: <DE824630-F149-4D53-BCCC-5ED3E1F0480F@gmail.com>
References: <015801d4da74$c7397500$55ac5f00$@viaeterna.nl>
 <DE824630-F149-4D53-BCCC-5ED3E1F0480F@gmail.com>
Message-ID: <034301d4da92$1764a820$462df860$@viaeterna.nl>

Dear Mollie,

 

- Thank you so, so much for your great and swift reply!

- I understand now the meaning of the dispersion parameters in glmmTMB. 

- DHARMa is a great tool box, with testZeroInflation(simulateResiduals(model)) even better than zero_count from sjstats, as it also works with non-Poisson models and provides great simulation frequency distributions.

- After using this tool, it seems I do not have to use ZI-models, nor dispersion models when I'm working with the genpois model in glmmTMB. This makes life so much easier!

 

- I will study the information you sent on AIC-comparison between counts and log(counts). For now, I can accept the much higher AIC (8036) with the glmmTMB-genpois on counts compared to the 2068 with glmmTMB-gaussian with log10(counts, 1 <- 0)).

- Sorry to everyone in the glmmTMB development team for thinking Ben Bolker developed glmmTMB, I should have studied the package details. Especially as I saw Mollie's name in the Description file just now. I do not there is a plain text (nor the HTML) code for the 'foot in mouth'-emoji, but I assure you it is here ^1000 ....

 

PS I responded to Mollie earlier in more detail. I don't know the mailing etiquette and I am happy to send this more detailed mail in this group.

 

Kind regards,

 

Hein 

 

From: Mollie Brooks <mollieebrooks at gmail.com> 
Sent: donderdag 14 maart 2019 16:34
To: Hein van Lieverloo <hein.van.lieverloo at viaeterna.nl>
Cc: R-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Overdispersed and zero-inflated - or not - and if so, how to model them? #glmmTMB

 

Dear Hein,

 

See replies below...

 

On 14Mar 2019, at 15:46, Hein van Lieverloo < <mailto:hein.van.lieverloo at viaeterna.nl> hein.van.lieverloo at viaeterna.nl> wrote:

 

Dear all,

Keywords: #glmmTMB  #overdisp  #zero_count

I am grateful for this mailing list and in advance, for any helpful
response.
This e-mail has two related questions.
Details (summary, background, approach and results) are given below them.

Question 1: my data are zero-inflated and overdispersed, but what does the
overdispersion parameter in glmmTMB (genpois, negbin1, negbin2) tell me? 
               It is very high in genpois and negbin1 models (see question 2) and I
thought it should be near 1, like in negbin2 (>> 1 is overdispersed, <<1 is
underdispersed)
               But when I test these generalized models for overdispersion
(overdisp from sjstats), no overdispersion is indicated.

 

The dispersion parameter in a glmmTMB model is there to handle the dispersion and it?s fine if it?s different from 1. So your tests with sjstats seemed to be correct. For descriptions of how the dispersion parameters relate to the variance, see ?sigma.glmmTMB

 


Question 2: should I use Gaussian on log(counts) with AIC 2068  or use
negbin2 with AIC 8036 and add overdispersion and zero-inflation models to
get a lower AIC (and if so, how?)
               When I use glmmTMB on counts with poisson, I get an AIC of 117 856.
Testing the model with overdisp and zero_count (from the sjstats package), I
find p = 0 (overdispersed) and zc-ratio 0.81 (probable zero-inflation).
               When I use glmmTMB on log10(counts), with 0's estimated to 0.1 so
resulting in -1, I get an AIC of 2068  (with lmer: 2122). Looks fine, but
may be wrong.
               When I use glmmTMB on counts with either genpois (dispersion par
613), negbinom1 (dispersion par 287) or negbinom2 (dispersion par 0.72), I
get AIC's over 8036. Much higher, but may be ok.

 

You can?t compare the models of the log-transformed data to the raw data. For example,

> set.seed(1)

> x=rpois(100, lambda=5)

> AIC(glmmTMB(log(x)~1))

[1] 128.0742

> AIC(glmmTMB(x~1, family=poisson))

[1] 422.911

 

or see discussion here  <https://stats.stackexchange.com/questions/61332/comparing-aic-of-a-model-and-its-log-transformed-version> https://stats.stackexchange.com/questions/61332/comparing-aic-of-a-model-and-its-log-transformed-version

 


               My data are zero-inflated and overdispersed and I would think that
glmmTMB with generalized models would result in much better models (lower
AIC) than simply working with the log-transformed data.
               The p-values per variable are similar enough, by the way, see the
best two models at the end of this mail.
               Of course, simply transforming 0 counts into -1 at the log-level
could be the cause and this approach may oversimplify reality and the AIC of
2068 could be artificial.
               If overdispersion and zero-inflation really is necessary, do I need
to get the AIC  down from 8036 to 2068 or can I accept higher AICs? I
suppose I can.
               But then: how should I approach the development of the zi-model
and/or the overdispersion models? 
               I know, from theory, but the thing is, there is little of no
research on invertebrates in drinking water distribution systems and their
structure is so different from surface water systems, that we are developing
hypotheses from this data set.


Summary of design and model
- Invertebrates in drinking water distribution systems in The Netherlands:
1993-1995 (yes, very old data!).
- glmmTMB of multilevel model  (1 | vNr / lNr)  : 34 systems (v), 175
sampling locations (l, ~5/system), 1301 samples (~ 8 quarters from
1993-1995), a multitude of variables measured.
- One of the best model tested: lWapit (count data)  ~ pTDOC + tCa + logtMn
+ lnOType + logbS500 + bTemp + blWavlo + blRoeiNaup + blMoskr


Background
The data were collected in the '90's and basic results were published in
2012:
 <https://www.sciencedirect.com/science/article/abs/pii/S0043135412002217?via%25> https://www.sciencedirect.com/science/article/abs/pii/S0043135412002217?via%
3Dihub
Dissolved organic matter is the best (causal / proxy / collinear?) predictor
for energy and carbon supply (R2 ~ 0.6 on mean estimated mean biomass at the
system level).  
I can send you the paper if you want. Also, I can sent more details, short
of the data set.
Since, when I have time (no funding), I try to find more predictors, at more
than just the highest aggregated level (system). I followed some courses on
multilevel modeling was well.
In 2013 a statistician using GenStat told me my data were zero-inflated and
overdispersed.
So, no glmm with Poisson response possible. The only option was: first a
glmm binomial for absence - presence, then glmm Poisson on the
presence-data.

The past two weeks (finally, I found some time again) I was and am so happy
to find Ben Bolker's  glmmTMB, able to work with zero-inflation and
overdispersion (I heard of MCMC options in 2017, no time then).
Learning from Ben Bolker's Salamanders-work, I managed to come a long way,
but I have not been able to develops stable overdispersion or zero-inflation
generalized models that significantly lower AIC in glmmTMB.
Although I teach the basics of statistics and made a lot of LM-models, I am
not a statistician (I'm a biologist happily forced toward statistics), and I
find a lot of details and mathematics hard to grasp:
 <https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html

 

I?m glad that glmmTMB is solving some of your long-standing problems and I agree that Ben Bolker has contributed immensely to glmmTMB and GLMMs in general, but calling it "Ben Bolker?s glmmTMB" is disregarding the other developers of the package and documentation.

 



Model and comparison approach
? System-level variable names start with p, location-level variables start
with l, t or log t, sample-level variables start with b or logb. Only
lnOType is a three types factor (bl are log-counts of other taxa)
? tCa and tMn = calcium and manganese in tap water (mean over time), lnOType
= village, city or rural environment, lbS500 = sediment > 500 um per sample,
bTemp = temperature sample
? blWavlo, blRoeipNaup, blMoskr = log(count(taxon; -1 <- 0)) per sample for
Cladocera, Copepoda, Ostracoda 
? 0-model contains no parameters (response ~ 1), 1-model contains major
predictor (pTDOC), full model contains 21 likely/possible predictors
? Model is kept identical in all regressions, although other versions may
have lower AIC
? Model data for comparison = all data  (during model development, systems
were randomly split approx. 60-40)

 

Make sure you?re using the same data for all models in AIC comparisons. 

 





? I did not include overdispersion or zero-inflated models yet, as I am not
sure whether it is necessary and I cannot get the basic ones (e.g. just with
pTDOC) stable. I can imagine that adding empty ZI-models is not very
effective in countering zero-inflation

 

For people in your situation, I typically recommend fitting a negative binomial model (see Warton, Environmetrics 2005), then testing for zero-inflation (I typically use DHARMa, but it sounds like sjstats does this also). Then if you have zero-inflation, you could fit a zero-inflated negative binomial. Then if the nbinom2 dispersion parameter in the conditional model gets very large, it means you might as well use a zero-inflated Poisson (see nbinom2 in ?sigma.glmmTMB for the reason). However, the best distribution could change depending on the predictors in the model because a model that explains less of the variance might have more dispersion. As you saw in the salamander examples (Brooks et al. 2017, R Journal, Appendix A), you can try different zero-inflation models.

 

cheers,

Mollie

 


Results (I can send more details, if required)

AIC per model (dispersion only for best model: x-model)

multilevel model:  + (1|vNr / lNr) for all except lm

               

response = blWapit = log(count(bWapi)), where -1 <- 0)  (counts expressed
per m3)

lm                         0-model              1-model              x-model               full model
Gaussian             4293.4                 4014.5                 3778.2                 3642.9

               

lmer                     0-model              1-model              x-model               full model
Gaussian             2185.8                 2122                     2121.9                 2185.8

               

glmmTMB           0-model              1-model              x-model               full model
Gaussian             2128.7                2116.6                 2068.2                 2074

               

response = b4Wapit = count(bWapi) expressed as rounded per 4 m3 (most sample
volumes are very close to that)

glmmTMB           Disp ratio (p)      Dispersion par   zc ratio                zi-model
0-model              1-model              x-model               full model           remarks
poisson               99.4 (0) *            NA                        0.81 **                NA
137165                137157                117856                114773                * p (H0: not
overdispersed) **zero-inflation probable
genpois                              0.34 (1)                613                       NA                        NA
8096.8                 8088.1                 8036.1                 8042.7  
genpois (+ZI)      NA                        603                       NA                        zi =~ 1
8094.1                 8085.5                 8036.6                 8043.1  
trunc genpois    NA                        701 (1-model)    NA                        zi =~ 1
9109.7                 9097.5                 *                           *                           *with zi =
~1 or zi =~pTDOC, non-positive-definite Hessian matrix
nbinom1             0.53 (1)                287                       NA                        NA
8306.4 *             8297.7 *             8244.6 *             8251.8 *             * warnings:
In f(par, order = order, ...) : value out of range in 'lgamma'
nbinom1 (+ZI)    NA                        287                       NA                        zi =~ 1
8306.4                 8299.7                 8246.6 *             8253.7                 * warnings:
In f(par, order = order, ...) : value out of range in 'lgamma'
nbinom2             0.78                      0.72                      NA                        NA
8224.1                 8216.0                 8165.3                 8171.8  
nbinom2 (+ZI)    NA                        0.787                   NA                        zi =~ 1
8226.1                 8218.0                 8165.3                 8172.6  


Comparing the best generalized glmmTMB model (nbinom2) on counts with the
best Gaussian model on log10(counts, 0 -> -1)

Family: nbinom2  ( log )
Family: gaussian  ( identity )                                                                  
Formula:          b4Wapit ~ pTDOC + tCa + logtMn + lnOType + logbS500 +
bTemp +             Formula:          blWapit ~ pTDOC + tCa + logtMn + lnOType +
logbS500 + bTemp +                                                                 
   blWavlo + blRoeiNaup + blMoskr + (1 | vNr/lNr)
blWavlo + blRoeiNaup + blMoskr + (1 | vNr/lNr)

Data: AllData
Data: AllData                                                                
               

    AIC      BIC   logLik deviance df.resid
AIC      BIC   logLik deviance df.resid

 8165.3   8237.7  -4068.6   8137.3     1287
2068.2   2140.6  -1020.1   2040.2     1287

               

Random effects:
Random effects:                                                                        
               

Conditional model:
Conditional model:                                                                    
Groups  Name        Variance Std.Dev.
Groups   Name        Variance Std.Dev.                                                                
lNr:vNr (Intercept) 4.325    2.080
lNr:vNr  (Intercept) 0.4850   0.6964                                                                   
vNr     (Intercept) 5.913    2.432
vNr      (Intercept) 0.4001   0.6326                                                                      
               
Residual             0.1794   0.4236                                                                          
Number of obs: 1301, groups:  lNr:vNr, 175; vNr, 34
Number of obs: 1301, groups:  lNr:vNr, 175; vNr, 34

               

Overdispersion parameter for nbinom2 family (): 0.72
Dispersion estimate for gaussian family (sigma^2): 0.179

               

Conditional model:
Conditional model:                                                                    
               Estimate Std. Error z value Pr(>|z|)
Estimate Std. Error z value Pr(>|z|)

(Intercept) -5.25438    1.80820  -2.906 0.003662 **
(Intercept) -1.566112   0.487738  -3.211  0.00132 **

pTDOC        0.95740    0.26583   3.602 0.000316 ***
pTDOC        0.298345   0.070836   4.212 2.53e-05 ***

tCa          0.06371    0.01623   3.926 8.64e-05 ***
tCa          0.013963   0.004579   3.050  0.00229 **

logtMn       0.83018    0.49447   1.679 0.093164 .
logtMn       0.243523   0.135480   1.797  0.07226 .

lnOTypeland  0.97151    0.51131   1.900 0.057425 .
lnOTypeland  0.390505   0.152519   2.560  0.01046 *

lnOTypestad -0.72832    0.82751  -0.880 0.378788
lnOTypestad  0.112042   0.231978   0.483  0.62911

logbS500     0.44416    0.10870   4.086 4.39e-05 ***
logbS500     0.127756   0.029836   4.282 1.85e-05 ***

bTemp        0.03655    0.01301   2.810 0.004948 **
bTemp        0.007290   0.003264   2.234  0.02551 *

blWavlo      0.14475    0.04158   3.481 0.000500 ***
blWavlo      0.036470   0.011718   3.112  0.00186 **

blRoeiNaup   0.11404    0.05684   2.006 0.044818 *
blRoeiNaup   0.042573   0.015790   2.696  0.00701 **

blMoskr     -0.25836    0.11832  -2.184 0.028993 *
blMoskr     -0.066737   0.030350  -2.199  0.02788 *

---
---                         
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1



Many thanks in advance for your help!

Kind regards,

Hein van Lieverloo



Met vriendelijke groet,

Hein van Lieverloo

_______________________________________________
R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-models at r-project.org>  mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models





???????????
Mollie E. Brooks, Ph.D.
Research Scientist
National Institute of Aquatic Resources
Technical University of Denmark




	[[alternative HTML version deleted]]


From t|m@co|e @end|ng |rom uc|@@c@uk  Thu Mar 14 19:39:03 2019
From: t|m@co|e @end|ng |rom uc|@@c@uk (Cole, Tim)
Date: Thu, 14 Mar 2019 18:39:03 +0000
Subject: [R-sig-ME] R-sig-mixed-models Digest, Vol 147, Issue 14
In-Reply-To: <037e01d4da94$fc90f3c0$f5b2db40$@viaeterna.nl>
References: <mailman.17334.3168.1552577861.8486.r-sig-mixed-models@r-project.org>
 <59480C4E-6BE7-432F-A798-CAE19DCADA5F@ucl.ac.uk>
 <037e01d4da94$fc90f3c0$f5b2db40$@viaeterna.nl>
Message-ID: <F8379509-DD0E-4C3E-B0AF-7C417B4FF944@ucl.ac.uk>

Dear Hein,

I?ll send you (offline) a PDF of the Box-Cox paper.

Best wishes,
Tim
--
Population Policy and Practice Programme
UCL Great Ormond Street Institute of Child Health,
30 Guilford Street, London WC1N 1EH, UK


From: Hein van Lieverloo <hein.van.lieverloo at viaeterna.nl>
Date: Thursday, 14 March 2019 at 18:37
To: "Cole, Tim" <tim.cole at ucl.ac.uk>, "mollieebrooks at gmail.com" <mollieebrooks at gmail.com>
Cc: "r-sig-mixed-models at r-project.org" <r-sig-mixed-models at r-project.org>
Subject: RE: R-sig-mixed-models Digest, Vol 147, Issue 14

Dear Tim,

I will try to study the details of the link Mollie sent and your sitar package later (especially the AICadj and BICadj).
I understand from both your mails that I can safely use the generalized model (most likely the glmmTMB genpois, see my reply to Mollie).
Is there an appropriate article that I can refer to in the article I'm writing to support this decision (I would be obliged if you could send a copy)?

Kind regards,

Hein

From: Cole, Tim <tim.cole at ucl.ac.uk>
Sent: donderdag 14 maart 2019 18:45
To: mollieebrooks at gmail.com
Cc: r-sig-mixed-models at r-project.org; hein.van.lieverloo at viaeterna.nl
Subject: Re: R-sig-mixed-models Digest, Vol 147, Issue 14

Dear Mollie,

That stack exchange link cites Akaike (1978) as saying you can compare models with differently transformed Y variables so long as the transformation?s Jacobian is included. In fact the earlier classic Box & Cox (1964) paper says the same thing and shows how to do it.

The AICadj and BICadj functions in my sitar package adjust for a Box-Cox transformed Y variable, including log(Y). The code below shows that the log transform provides a worse fit:

> library(glmmTMB)
> set.seed(1)
> x=rpois(100, lambda=5)
> AIC(glmmTMB(x~1, family=poisson))
[1] 422.9
> AIC(glmmTMB(log(x)~1))
[1] 128.1

> library(sitar)
> AICadj(glmmTMB(x~1, family=poisson))
glmmTMB(x ~ 1, family = poisson)
                           422.9
> AICadj(glmmTMB(log(x)~1))
glmmTMB(log(x) ~ 1)
              436.2

Best wishes,
Tim Cole
--
Population Policy and Practice Programme
UCL Great Ormond Street Institute of Child Health,
30 Guilford Street, London WC1N 1EH, UK

------------------------------
Date: Thu, 14 Mar 2019 16:34:20 +0100
From: Mollie Brooks <mollieebrooks at gmail.com<mailto:mollieebrooks at gmail.com>>
To: Hein van Lieverloo <hein.van.lieverloo at viaeterna.nl<mailto:hein.van.lieverloo at viaeterna.nl>>
Cc: R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME]  Overdispersed and zero-inflated - or not -
                and if so, how to model them? #glmmTMB
Message-ID: <DE824630-F149-4D53-BCCC-5ED3E1F0480F at gmail.com<mailto:DE824630-F149-4D53-BCCC-5ED3E1F0480F at gmail.com>>
Content-Type: text/plain; charset="utf-8"

Dear Hein,

See replies below...

On 14Mar 2019, at 15:46, Hein van Lieverloo <hein.van.lieverloo at viaeterna.nl<mailto:hein.van.lieverloo at viaeterna.nl>> wrote:
Dear all,
Keywords: #glmmTMB  #overdisp  #zero_count
I am grateful for this mailing list and in advance, for any helpful
response.
This e-mail has two related questions.
Details (summary, background, approach and results) are given below them.
Question 1: my data are zero-inflated and overdispersed, but what does the
overdispersion parameter in glmmTMB (genpois, negbin1, negbin2) tell me?
           It is very high in genpois and negbin1 models (see question 2) and I
thought it should be near 1, like in negbin2 (>> 1 is overdispersed, <<1 is
underdispersed)
           But when I test these generalized models for overdispersion
(overdisp from sjstats), no overdispersion is indicated.

The dispersion parameter in a glmmTMB model is there to handle the dispersion and it?s fine if it?s different from 1. So your tests with sjstats seemed to be correct. For descriptions of how the dispersion parameters relate to the variance, see ?sigma.glmmTMB

Question 2: should I use Gaussian on log(counts) with AIC 2068  or use
negbin2 with AIC 8036 and add overdispersion and zero-inflation models to
get a lower AIC (and if so, how?)
           When I use glmmTMB on counts with poisson, I get an AIC of 117 856.
Testing the model with overdisp and zero_count (from the sjstats package), I
find p = 0 (overdispersed) and zc-ratio 0.81 (probable zero-inflation).
           When I use glmmTMB on log10(counts), with 0's estimated to 0.1 so
resulting in -1, I get an AIC of 2068  (with lmer: 2122). Looks fine, but
may be wrong.
           When I use glmmTMB on counts with either genpois (dispersion par
613), negbinom1 (dispersion par 287) or negbinom2 (dispersion par 0.72), I
get AIC's over 8036. Much higher, but may be ok.

You can?t compare the models of the log-transformed data to the raw data. For example,
set.seed(1)
x=rpois(100, lambda=5)
AIC(glmmTMB(log(x)~1))
[1] 128.0742
AIC(glmmTMB(x~1, family=poisson))
[1] 422.911

or see discussion here https://stats.stackexchange.com/questions/61332/comparing-aic-of-a-model-and-its-log-transformed-version


	[[alternative HTML version deleted]]


From h@nch@o @end|ng |rom ude|@edu  Fri Mar 15 01:33:50 2019
From: h@nch@o @end|ng |rom ude|@edu (Chao Han)
Date: Thu, 14 Mar 2019 20:33:50 -0400
Subject: [R-sig-ME] 
 Model failed to converge when a factor is centered or releveled
In-Reply-To: <CAO7JsnQx47tey1MN781ooo7NUSv1k0sF+R6cx8AK529Pp8QpKQ@mail.gmail.com>
References: <CALiHhPSJ6dnnJubpHFtduN38oW9S8hWf7UEhALagP0rWZVaXFg@mail.gmail.com>
 <29526_1552407705_0PO90061AHG9XND0_34d1af22-432d-5ef9-8183-42f22ed67f9a@mpi.nl>
 <CAO7JsnQx47tey1MN781ooo7NUSv1k0sF+R6cx8AK529Pp8QpKQ@mail.gmail.com>
Message-ID: <CALiHhPSWkwZxaxegoNV0-GmQPvDXvsJvUFYCghCOaUZtMd=3HQ@mail.gmail.com>

Thank you all for your comments.
I tried both c(0.5, -0.5) and contr.helmert contrast setting, and they both
give "failed to converge" warnings. I'm concerned if I should trust the
model with default coding.

Best,
Chao

On Tue, Mar 12, 2019 at 12:49 PM Douglas Bates <bates at stat.wisc.edu> wrote:

> For a 2-level factor like Memory I prefer to use contr.helmert rather than
> hard-coding levels like c(-0.5, 0.5).  In general statisticians code
> factors in a 2-level factorial experiment using (-1, 1), not (-0.5, 0.5)
> because the (-1, 1) levels are propagated to interactions at any level.  If
> you use (-0.5, 0.5) then 2-level interactions end up being coded as (-0.25,
> 0.25), 3-level interactions as (-0.125, 0.125), etc.  If you just assign
> the contrasts as contr.helmert to two-level factors at the beginning things
> proceed smoothly, in my opinion.
>
> On Tue, Mar 12, 2019 at 11:21 AM Phillip Alday <phillip.alday at mpi.nl>
> wrote:
>
>> A few general comments without looking at your data:
>>
>> 1. It generally doesn't make sense to "center" a categorical variable.
>> For a two-level, dummy-coded variable you could in theory do this, but
>> it would lead to a rather odd balance-weighted contrast coding.
>>
>> 2. I have occasionally seen weird behavior in complex binomial models
>> where the choice of reference-level makes a difference in convergence. I
>> suspect this has to do with the whether the model is able to "pull away"
>> from a "solution" where the model just predicts the same response for
>> everything, but I have not investigated this rigorously nor I have
>> checked that my hand-waving explanation makes sense in terms of the
>> actual behavior of the optimizer.
>>
>> If you really want to reverse your contrast labels, what happens when
>> you do that directly instead of releveling?
>> i.e.
>>
>> c1 <- c(-0.5, 0.5)
>> contrasts(df$Memory) = cbind(c1)
>> model_center <- glmer(ACC ~ Memory * Group + (1|Subject) +
>> (1+Memory|item), family = "binomial", data = df)
>>
>> c2 <- c(0.5, -0.5)
>> contrasts(df$Memory) = cbind(c2)
>> model_relevel <- glmer(ACC ~ Memory * Group + (1|Subject) + ...
>>
>>
>> Best,
>> Phillip
>>
>> On 10/3/19 3:26 am, Chao Han wrote:
>> > Hi all,
>> >
>> > I'm running a mixed-effects model using lmer4::glmer() function. The
>> > modeling works with R's default dummy coding. But if I center or
>> relevel a
>> > factor of 2 levels, the model failed to converge. I am wondering why.
>> >
>> > Here is the data:
>> > https://www.dropbox.com/s/ysw5rvxowamucoh/example.csv?dl=0
>> >
>> > Here is the code for the model without centering or releveling the
>> factor
>> > "Memory":
>> >
>> >> model_default <- glmer(ACC ~ Memory * Group + (1|Subject) +
>> > (1+Memory|item), family = "binomial", data = df)
>> >
>> > The model converges with the above code. But the following model failed
>> to
>> > converge after centering the factor "Memory":
>> >
>> >> c1 <- c(-0.5, 0.5)
>> >> contrasts(df$Memory) = cbind(c1)
>> >> model_center <- glmer(ACC ~ Memory * Group + (1|Subject) +
>> > (1+Memory|item), family = "binomial", data = df)
>> >
>> > If I relevel the factor "Memory", the following model failed to converge
>> > either:
>> >
>> >> df$Memory <- factor(df$Memory, levels = c("Y", "X"))
>> >> model_relevel <- glmer(ACC ~ Memory * Group + (1|Subject) +
>> > (1+Memory|item), family = "binomial", data = df)
>> >
>> > Why would centering or releveling a factor make a difference in terms of
>> > convergence? Which result should I trust?
>> >
>> > Thank you in advance for your help.
>> >
>> > Best,
>> > Chao
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From h@nch@o @end|ng |rom ude|@edu  Fri Mar 15 01:37:49 2019
From: h@nch@o @end|ng |rom ude|@edu (Chao Han)
Date: Thu, 14 Mar 2019 20:37:49 -0400
Subject: [R-sig-ME] 
 Model failed to converge when a factor is centered or releveled
In-Reply-To: <4737E17E7C8C3C4A8B5C1CE5346371D4A8510E4B@EXCH06S.adqimr.ad.lan>
References: <CALiHhPSJ6dnnJubpHFtduN38oW9S8hWf7UEhALagP0rWZVaXFg@mail.gmail.com>
 <4737E17E7C8C3C4A8B5C1CE5346371D4A8510E4B@EXCH06S.adqimr.ad.lan>
Message-ID: <CALiHhPSmYUWuogXCfCbbGeXTChWN-z+OGT7VR7WT75cVuVmgoQ@mail.gmail.com>

Thank you, David. Does that mean I can trust the model?

On Tue, Mar 12, 2019 at 8:57 PM David Duffy <
David.Duffy at qimrberghofer.edu.au> wrote:

> These jobs run for me giving those warnings, but when I plot the re's from
> both models they seem to be identical, and have converged to the same
> likelihood. I guess it is just that the contrast coding leads to numerical
> differences that upset the diagnostics,
>
>
>
> ________________________________________
> From: R-sig-mixed-models [r-sig-mixed-models-bounces at r-project.org] on
> behalf of Chao Han [hanchao at udel.edu]
> Sent: Sunday, 10 March 2019 12:26 PM
> To: R-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] Model failed to converge when a factor is centered or
> releveled
>
> Hi all,
>
> I'm running a mixed-effects model using lmer4::glmer() function. The
> modeling works with R's default dummy coding. But if I center or relevel a
> factor of 2 levels, the model failed to converge. I am wondering why.
>
> Here is the data:
> https://www.dropbox.com/s/ysw5rvxowamucoh/example.csv?dl=0
>
> Here is the code for the model without centering or releveling the factor
> "Memory":
>
> > model_default <- glmer(ACC ~ Memory * Group + (1|Subject) +
> (1+Memory|item), family = "binomial", data = df)
>
> The model converges with the above code. But the following model failed to
> converge after centering the factor "Memory":
>
> > c1 <- c(-0.5, 0.5)
> > contrasts(df$Memory) = cbind(c1)
> > model_center <- glmer(ACC ~ Memory * Group + (1|Subject) +
> (1+Memory|item), family = "binomial", data = df)
>
> If I relevel the factor "Memory", the following model failed to converge
> either:
>
> > df$Memory <- factor(df$Memory, levels = c("Y", "X"))
> > model_relevel <- glmer(ACC ~ Memory * Group + (1|Subject) +
> (1+Memory|item), family = "binomial", data = df)
>
> Why would centering or releveling a factor make a difference in terms of
> convergence? Which result should I trust?
>

	[[alternative HTML version deleted]]


From he|n@v@n@||ever|oo @end|ng |rom v|@etern@@n|  Fri Mar 15 20:59:15 2019
From: he|n@v@n@||ever|oo @end|ng |rom v|@etern@@n| (Hein van Lieverloo)
Date: Fri, 15 Mar 2019 20:59:15 +0100
Subject: [R-sig-ME] Testing for collinearity / variance inflation # glmmTMB
Message-ID: <005801d4db69$9259ba70$b70d2f50$@viaeterna.nl>

Dear Members,

I'm trying new variables in my glmmTMB genpois model (details below).
Adding one possibly collinear variable led to a much lower AIC (from 8031.7
to 7899.3) although the coefficient is not different from 0  (p(H1) = 0.66).

I'd like to test for variance inflation / collinearity between variables in
glmmTMB models.
For lm and lmer models, I can use VIF (I tested the same model in lmer (with
log(counts, -1 <- 0) and found no correlation, this model is given below in
the details).
Could someone help me with this (should I test for collinearity and how to
do that in glmmTMB models?)

I read about spatial and temporal autocorrelation in the GLMM FAQ on GitHub:
https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html but if I understand
that correctly, that is about independency of sampling locations and
repeated measures, which I can test using the DHARMa package Mollie Brooks
pointed me to (great). I can't find any collinearity test there either. 

Background

The model and the results are below, the new variable is logbR2A25 (log of
'plate count' bacteria on R2A medium 25 C, 10 days in water flushed from
hydrants).
The Asellidae (b4Wapit, counts filtered (100 um mesh) from 4 m3 water
flushed from hydrants) are significantly related with the concentration
dissolved organic carbon in water from treatment plants.
The bacteria (R2A plate count as index) may be an intermediary / collinear
variable though    DOC -> bacteria -> Asellidae  or  DOC -> bacteria  and
DOC -> x -> Asellidae (x are other or more trophic levels).

Thanks in advance

Kind regards,

Hein van Lieverloo



Details of the models


Family: genpois  ( log )
Formula:          b4Wapit ~ pTDOC + tCa + logtFe + lnOType + logbS500 +
bTemp +  
    logbR2A25 + blWavlo + blRoeiNaup + blMoskr + (1 | vNr/lNr)
Data: AllData

     AIC      BIC   logLik deviance df.resid 
  7899.6   7976.9  -3934.8   7869.6     1264 

Random effects:

Conditional model:
 Groups  Name        Variance Std.Dev.
 lNr:vNr (Intercept) 1.727    1.314   
 vNr     (Intercept) 2.130    1.459   
Number of obs: 1279, groups:  lNr:vNr, 175; vNr, 34

Overdispersion parameter for genpois family ():  593 

Conditional model:
             Estimate Std. Error z value Pr(>|z|)    
(Intercept) -0.712907   1.192110  -0.598 0.549826    
pTDOC        0.596179   0.162938   3.659 0.000253 ***
tCa          0.038302   0.010659   3.593 0.000326 ***
logtFe       1.084413   0.391032   2.773 0.005551 ** 
lnOTypeland  0.848745   0.341383   2.486 0.012912 *  
lnOTypestad -0.557032   0.579695  -0.961 0.336600       This is one factor
level that was and is not significantly different from the reference 
logbS500     0.413405   0.079401   5.207 1.92e-07 ***
bTemp        0.030599   0.009214   3.321 0.000897 ***

logbR2A25    0.033845   0.076619   0.442 0.658688       Coefficient mean not
significantly different from 0

blWavlo      0.044184   0.029413   1.502 0.133050          Now this one is
also not significant anymore
blRoeiNaup   0.115044   0.038998   2.950 0.003178 ** 
blMoskr     -0.116707   0.073944  -1.578 0.114493         Now this one is
also not significant anymore
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Linear mixed model fit by REML ['lmerMod']
Formula: blWapit ~ pTDOC + tCa + logtFe + lnOType + logbS500 + bTemp +  
    logbR2A25 + blWavlo + blRoeiNaup + blMoskr + (1 | vNr/lNr)
   Data: AllData

REML criterion at convergence: 2066.4

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.8586 -0.4306 -0.0618  0.3586  6.2652 

Random effects:
 Groups   Name        Variance Std.Dev.
 lNr:vNr  (Intercept) 0.4806   0.6933  
 vNr      (Intercept) 0.3921   0.6262  
 Residual             0.1810   0.4254  
Number of obs: 1279, groups:  lNr:vNr, 175; vNr, 34

Fixed effects:
             Estimate Std. Error t value
(Intercept) -1.314870   0.511630  -2.570
pTDOC        0.272967   0.069991   3.900
tCa          0.013413   0.004527   2.963
logtFe       0.543973   0.175699   3.096
lnOTypeland  0.378729   0.152018   2.491
lnOTypestad  0.058470   0.228750   0.256
logbS500     0.132559   0.030340   4.369
bTemp        0.007874   0.003296   2.389
logbR2A25    0.073598   0.032762   2.246
blWavlo      0.031790   0.011911   2.669
blRoeiNaup   0.043181   0.015952   2.707
blMoskr     -0.061650   0.030685  -2.009

Correlation of Fixed Effects:
            (Intr) pTDOC  tCa    logtFe lnOTypl lnOTyps lgS500 bTemp  lR2A25
blWavl
pTDOC       -0.326

tCa         -0.538 -0.132

logtFe       0.671 -0.047 -0.050

lnOTypeland -0.300 -0.035  0.074 -0.071

lnOTypestad -0.183  0.069 -0.096 -0.115  0.440

logbS500     0.020  0.019 -0.014 -0.001  0.036   0.001

bTemp       -0.058  0.008 -0.010  0.002  0.004  -0.001   0.078

logbR2A25   -0.219 -0.024  0.002 -0.027  0.029  -0.004  -0.080 -0.013

blWavlo      0.012 -0.015  0.013 -0.011 -0.002  -0.003   0.008 -0.257 -0.082

blRoeiNaup  -0.024 -0.012 -0.029  0.007  0.020   0.023  -0.013  0.015 -0.015
-0.185
blMoskr      0.061  0.007 -0.011  0.009  0.001   0.021  -0.046  0.026 -0.026
-0.066
            blRoNp
pTDOC             
tCa               
logtFe            
lnOTypeland       
lnOTypestad       
logbS500          
bTemp             
logbR2A25         
blWavlo           
blRoeiNaup        
blMoskr     -0.036

> vif(Mx)
               GVIF Df GVIF^(1/(2*Df))
pTDOC      1.028332  1        1.014067
tCa        1.047301  1        1.023377
logtFe     1.021198  1        1.010544
lnOType    1.052972  2        1.012988
logbS500   1.018038  1        1.008979
bTemp      1.080318  1        1.039384
logbR2A25  1.019387  1        1.009647
blWavlo    1.125894  1        1.061081
blRoeiNaup 1.042468  1        1.021013
blMoskr    1.011175  1        1.005572


From @ouhey|@@ghebghoub @end|ng |rom gm@||@com  Mon Mar 18 19:11:23 2019
From: @ouhey|@@ghebghoub @end|ng |rom gm@||@com (Souheyla GHEBGHOUB)
Date: Mon, 18 Mar 2019 18:11:23 +0000
Subject: [R-sig-ME] Random effects in multinomial regression in R?
Message-ID: <CAEA998ifF3J6fdjknpAZm_xpjTjTUrY1p5oRJqzyAGSGO0FV1w@mail.gmail.com>

I have *Change* from Pretest to Posttest (gain, no_gain, decline) as the
DV. Also *Pretest* and *Group* as covariates. This called for a multinomial
regression:

mod0 <- brm(Change ~ Pretest + Group)

*Question: *I'd like to add random effects of *Subject* and *Word*, which
may differ by time, but I don't have effect of *Time* to do:

mod1 <- brm(Change ~ Pretest + Group + (Time|Subject) + (Time|Word))

So I thought of this:

mod2 <- brm(Change ~ Pretest + Group + (1|Subject) + (1|Word))

but this also seems wrong to me. What do you think is the best way to treat
random effects in this situation, please?

Thank you

Souheyla Ghebghoub

	[[alternative HTML version deleted]]


From m@tthew@t@boden @end|ng |rom gm@||@com  Wed Mar 20 06:43:22 2019
From: m@tthew@t@boden @end|ng |rom gm@||@com (Matthew Boden)
Date: Tue, 19 Mar 2019 22:43:22 -0700
Subject: [R-sig-ME] Fwd: glm.nb convergence issues
In-Reply-To: <BN8PR09MB3554300E08C4B76B4E25FC9587410@BN8PR09MB3554.namprd09.prod.outlook.com>
References: <BN8PR09MB3554300E08C4B76B4E25FC9587410@BN8PR09MB3554.namprd09.prod.outlook.com>
Message-ID: <CAE10PCzNa4V0Hr03K4TwOeACavzFZUHoRo4Vrj_+VUDTueVCCA@mail.gmail.com>

Hello,



I?m working my way through convergence issues related to a negative
binomial mixed model with an offset and random intercept. Data attached.



A3 <- glmer.nb(wait_n ~ offset(log(wait_d)) +  scale(spr) + (1 | check),
data = ST)

summary(A3)



#Model failed to converge with max|grad| = 0.0031459 (tol = 0.001, component 1)

#Model is nearly unidentifiable: very large eigenvalue

# - Rescale variables?



I have two questions.



1) Outcome variable (wait_n) values are much larger than predictor (spr)
values.



Mean wait_n = 11,783

Mean spr = 7.34



Would transforming the predictor (e.g., multiply by 1000) and/or offset
make sense here. The offset confuses the issue (or, me) - as a log, it is
quite small relative to the outcome and on but on the same scale as the
other predictor.



2) The use of allFit to try different optimizers seems to be giving
conflicting results.



A3.all <- allFit(A3, meth.tab = NULL)

ss <- summary(A3.all)



#bobyqa : [OK]

#Nelder_Mead : [OK]

#nlminbwrap : [OK]

#nmkbw : [OK]

#optimx.L-BFGS-B : [OK]

#nloptwrap.NLOPT_LN_NELDERMEAD : [OK]

#nloptwrap.NLOPT_LN_BOBYQA : [OK]



ss$ which.OK



#bobyqa                         TRUE

#Nelder_Mead                    TRUE

#nlminbwrap                     TRUE

#nmkbw                          TRUE

#optimx.L-BFGS-B                TRUE

#nloptwrap.NLOPT_LN_NELDERMEAD  TRUE

#nloptwrap.NLOPT_LN_BOBYQA      TRUE



What do you know, all optimizers supposedly work.

However, some of them lead to a model that converges, and others don?t.



A3a <- glmer.nb(wait_n ~ offset(log(wait_d)) +  scale(spr) + (1 | check),
data = ST, glmerControl(optimizer = "bobyqa"))



#Model is nearly unidentifiable: very large eigenvalue

# - Rescale variables?



A3b <- glmer.nb(wait_n ~ offset(log(wait_d)) +  scale(spr) + (1 | check),
data = ST, glmerControl(optimizer = "Nelder_Mead"))



#Model failed to converge with max|grad| = 0.00553513 (tol = 0.001, component 1)

#Model is nearly unidentifiable: very large eigenvalue

# - Rescale variables?



A3c <- glmer.nb(wait_n ~ offset(log(wait_d)) +  scale(spr) + (1 | check),
data = ST, glmerControl(optimizer = "nlminbwrap"))



#Model is nearly unidentifiable: very large eigenvalue

# - Rescale variables?



A3d <- glmer.nb(wait_n ~ offset(log(wait_d)) +  scale(spr) + (1 | check),
data = ST, glmerControl(optimizer = "nmkbw"))



#Model failed to converge with max|grad| = 0.00177926 (tol = 0.001, component 1)

#Model is nearly unidentifiable: very large eigenvalue

# - Rescale variables?



I?m trying to understand why allFit responds that all optimizers converge
when they actually do not.



Thank you,

Matt



Matthew Boden, Ph.D.

Senior Evaluator

Program Evaluation & Resource Center

Office of Mental Health & Suicide Prevention

Veterans Health Administration

From m@tteo@red@n@ @end|ng |rom hotm@||@com  Wed Mar 20 07:42:08 2019
From: m@tteo@red@n@ @end|ng |rom hotm@||@com (Matteo Redana)
Date: Wed, 20 Mar 2019 06:42:08 +0000
Subject: [R-sig-ME] Help with a multiresponse model
Message-ID: <VI1PR06MB5920D9788CA3CD791BB1A472FA410@VI1PR06MB5920.eurprd06.prod.outlook.com>

Dear all,

I?m new in the mailing list. I?m coning to you for a problem on how approach my data:

I have a time series of Water temperature from 06-06-2017 to 28-07-2017, 1 value each hour. In the same period the 10\06, 02/07 and 25/07 were sampled larvae from the river and teste for thermotolerance. So each sampling day I have a different number of larvae with different thermotolerances. Working with GAM there are evidence of a trend in thermotolrence, given the high variability in each sample day.

What I want to do is to create a model that look at the impact of water temperature (also lagged values) on the thermotolerance.

Until now I?ve ever worked  with mgcv and dlnm package in hydraulics, but it?s the first time with this kind of data.
I don?t know how to build a model, given that I have water temp. value in a different scale (hourly) and a multiple response in my Y (CT-thermotolerance).
The next step will be to preform a mixed model to test differences in natural site and artificial (this experiment was done in different lake).
Any suggestion is welcome!

Matteo

Sent from Mail<https://go.microsoft.com/fwlink/?LinkId=550986> for Windows 10


	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Wed Mar 20 09:48:07 2019
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Wed, 20 Mar 2019 09:48:07 +0100
Subject: [R-sig-ME] Fwd: glm.nb convergence issues
In-Reply-To: <CAE10PCzNa4V0Hr03K4TwOeACavzFZUHoRo4Vrj_+VUDTueVCCA@mail.gmail.com>
References: <BN8PR09MB3554300E08C4B76B4E25FC9587410@BN8PR09MB3554.namprd09.prod.outlook.com>
 <CAE10PCzNa4V0Hr03K4TwOeACavzFZUHoRo4Vrj_+VUDTueVCCA@mail.gmail.com>
Message-ID: <CAJuCY5w74vVuATprurgR86zByRSbvztvN92a2m8e6ENjTG-+Jw@mail.gmail.com>

Dear Matthew,

The mailing list accepts only a limited number of file formats as
attachment. Your data got stripped. Can you resend the data or send a link
to the data?

The offset requires the log because the model is using the log link. Your
model fits log(E(wait_n)) = offset(log(wait_d)) + covariates which you can
rewrite as log(E(wait_n)) - offset(log(wait_d)) = covariates or
log(E(wait_n) / wait_d) = covariates. Pick a relevant magnitude of wait_d
so that the magnitude of wait_n/wait_d is somewhat near 1. E.g. we don't
express the number of inhabitats per m? but rather per km?

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op wo 20 mrt. 2019 om 06:43 schreef Matthew Boden <matthew.t.boden at gmail.com
>:

> Hello,
>
>
>
> I?m working my way through convergence issues related to a negative
> binomial mixed model with an offset and random intercept. Data attached.
>
>
>
> A3 <- glmer.nb(wait_n ~ offset(log(wait_d)) +  scale(spr) + (1 | check),
> data = ST)
>
> summary(A3)
>
>
>
> #Model failed to converge with max|grad| = 0.0031459 (tol = 0.001,
> component 1)
>
> #Model is nearly unidentifiable: very large eigenvalue
>
> # - Rescale variables?
>
>
>
> I have two questions.
>
>
>
> 1) Outcome variable (wait_n) values are much larger than predictor (spr)
> values.
>
>
>
> Mean wait_n = 11,783
>
> Mean spr = 7.34
>
>
>
> Would transforming the predictor (e.g., multiply by 1000) and/or offset
> make sense here. The offset confuses the issue (or, me) - as a log, it is
> quite small relative to the outcome and on but on the same scale as the
> other predictor.
>
>
>
> 2) The use of allFit to try different optimizers seems to be giving
> conflicting results.
>
>
>
> A3.all <- allFit(A3, meth.tab = NULL)
>
> ss <- summary(A3.all)
>
>
>
> #bobyqa : [OK]
>
> #Nelder_Mead : [OK]
>
> #nlminbwrap : [OK]
>
> #nmkbw : [OK]
>
> #optimx.L-BFGS-B : [OK]
>
> #nloptwrap.NLOPT_LN_NELDERMEAD : [OK]
>
> #nloptwrap.NLOPT_LN_BOBYQA : [OK]
>
>
>
> ss$ which.OK
>
>
>
> #bobyqa                         TRUE
>
> #Nelder_Mead                    TRUE
>
> #nlminbwrap                     TRUE
>
> #nmkbw                          TRUE
>
> #optimx.L-BFGS-B                TRUE
>
> #nloptwrap.NLOPT_LN_NELDERMEAD  TRUE
>
> #nloptwrap.NLOPT_LN_BOBYQA      TRUE
>
>
>
> What do you know, all optimizers supposedly work.
>
> However, some of them lead to a model that converges, and others don?t.
>
>
>
> A3a <- glmer.nb(wait_n ~ offset(log(wait_d)) +  scale(spr) + (1 | check),
> data = ST, glmerControl(optimizer = "bobyqa"))
>
>
>
> #Model is nearly unidentifiable: very large eigenvalue
>
> # - Rescale variables?
>
>
>
> A3b <- glmer.nb(wait_n ~ offset(log(wait_d)) +  scale(spr) + (1 | check),
> data = ST, glmerControl(optimizer = "Nelder_Mead"))
>
>
>
> #Model failed to converge with max|grad| = 0.00553513 (tol = 0.001,
> component 1)
>
> #Model is nearly unidentifiable: very large eigenvalue
>
> # - Rescale variables?
>
>
>
> A3c <- glmer.nb(wait_n ~ offset(log(wait_d)) +  scale(spr) + (1 | check),
> data = ST, glmerControl(optimizer = "nlminbwrap"))
>
>
>
> #Model is nearly unidentifiable: very large eigenvalue
>
> # - Rescale variables?
>
>
>
> A3d <- glmer.nb(wait_n ~ offset(log(wait_d)) +  scale(spr) + (1 | check),
> data = ST, glmerControl(optimizer = "nmkbw"))
>
>
>
> #Model failed to converge with max|grad| = 0.00177926 (tol = 0.001,
> component 1)
>
> #Model is nearly unidentifiable: very large eigenvalue
>
> # - Rescale variables?
>
>
>
> I?m trying to understand why allFit responds that all optimizers converge
> when they actually do not.
>
>
>
> Thank you,
>
> Matt
>
>
>
> Matthew Boden, Ph.D.
>
> Senior Evaluator
>
> Program Evaluation & Resource Center
>
> Office of Mental Health & Suicide Prevention
>
> Veterans Health Administration
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From c@c@voeten @end|ng |rom hum@|e|denun|v@n|  Wed Mar 20 10:55:19 2019
From: c@c@voeten @end|ng |rom hum@|e|denun|v@n| (Voeten, C.C.)
Date: Wed, 20 Mar 2019 09:55:19 +0000
Subject: [R-sig-ME] Help with a multiresponse model
In-Reply-To: <VI1PR06MB5920D9788CA3CD791BB1A472FA410@VI1PR06MB5920.eurprd06.prod.outlook.com>
References: <VI1PR06MB5920D9788CA3CD791BB1A472FA410@VI1PR06MB5920.eurprd06.prod.outlook.com>
Message-ID: <D14049CE02C4F54D95360EEC06CE45C50FA1FF94@SPMXM08.VUW.leidenuniv.nl>

Dear Matteo,

Since you are using GAMs anyway, have you considered using gam() with family='mvn(2)'?

Best,
Cesko

-----Oorspronkelijk bericht-----
Van: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Namens Matteo Redana
Verzonden: woensdag 20 maart 2019 07:42
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] Help with a multiresponse model

Dear all,

I?m new in the mailing list. I?m coning to you for a problem on how approach my data:

I have a time series of Water temperature from 06-06-2017 to 28-07-2017, 1 value each hour. In the same period the 10\06, 02/07 and 25/07 were sampled larvae from the river and teste for thermotolerance. So each sampling day I have a different number of larvae with different thermotolerances. Working with GAM there are evidence of a trend in thermotolrence, given the high variability in each sample day.

What I want to do is to create a model that look at the impact of water temperature (also lagged values) on the thermotolerance.

Until now I?ve ever worked  with mgcv and dlnm package in hydraulics, but it?s the first time with this kind of data.
I don?t know how to build a model, given that I have water temp. value in a different scale (hourly) and a multiple response in my Y (CT-thermotolerance).
The next step will be to preform a mixed model to test differences in natural site and artificial (this experiment was done in different lake).
Any suggestion is welcome!

Matteo

Sent from Mail<https://go.microsoft.com/fwlink/?LinkId=550986> for Windows 10


	[[alternative HTML version deleted]]


From m@tteo@red@n@ @end|ng |rom hotm@||@com  Wed Mar 20 12:28:45 2019
From: m@tteo@red@n@ @end|ng |rom hotm@||@com (Matteo Redana)
Date: Wed, 20 Mar 2019 11:28:45 +0000
Subject: [R-sig-ME] Help with a multiresponse model
In-Reply-To: <D14049CE02C4F54D95360EEC06CE45C50FA1FF94@SPMXM08.VUW.leidenuniv.nl>
References: <VI1PR06MB5920D9788CA3CD791BB1A472FA410@VI1PR06MB5920.eurprd06.prod.outlook.com>,
 <D14049CE02C4F54D95360EEC06CE45C50FA1FF94@SPMXM08.VUW.leidenuniv.nl>
Message-ID: <VI1PR06MB592073BE0D167F7B87D010F6FA410@VI1PR06MB5920.eurprd06.prod.outlook.com>

Dear Cesko,

Do you suggest this approach? (see attached table with a subset of three larvae ct1,ct2 and ct3)



new<-gam(list(ct1~s(WDS,k=3)+cros4,ct2~s(WDS,k=3)+cros4,ct3~s(WDS,k=3)+cros4),data=linrathen,method="REML",family = mvn(d=3))



where WDS is temperature of water, cros4 is a crossbasis object to look at the lagged effect (lag=240, means 10 days)

I have these problems:

I have each sampling days ca. 30 larvae: means that mvn(d=30)?

Given that in the three different sampling days the individuals are different I am assuming that in the same column the individual is the same.

I hope to be clear, if you need other info just ask.

Many Thanks

Matteo





Sent from Mail<https://go.microsoft.com/fwlink/?LinkId=550986> for Windows 10



________________________________
From: Voeten, C.C. <c.c.voeten at hum.leidenuniv.nl>
Sent: Wednesday, March 20, 2019 10:55:19 AM
To: 'Matteo Redana'; 'r-sig-mixed-models at r-project.org'
Subject: RE: Help with a multiresponse model

Dear Matteo,

Since you are using GAMs anyway, have you considered using gam() with family='mvn(2)'?

Best,
Cesko

-----Oorspronkelijk bericht-----
Van: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Namens Matteo Redana
Verzonden: woensdag 20 maart 2019 07:42
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] Help with a multiresponse model

Dear all,

I?m new in the mailing list. I?m coning to you for a problem on how approach my data:

I have a time series of Water temperature from 06-06-2017 to 28-07-2017, 1 value each hour. In the same period the 10\06, 02/07 and 25/07 were sampled larvae from the river and teste for thermotolerance. So each sampling day I have a different number of larvae with different thermotolerances. Working with GAM there are evidence of a trend in thermotolrence, given the high variability in each sample day.

What I want to do is to create a model that look at the impact of water temperature (also lagged values) on the thermotolerance.

Until now I?ve ever worked  with mgcv and dlnm package in hydraulics, but it?s the first time with this kind of data.
I don?t know how to build a model, given that I have water temp. value in a different scale (hourly) and a multiple response in my Y (CT-thermotolerance).
The next step will be to preform a mixed model to test differences in natural site and artificial (this experiment was done in different lake).
Any suggestion is welcome!

Matteo

Sent from Mail<https://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgo.microsoft.com%2Ffwlink%2F%3FLinkId%3D550986&amp;data=02%7C01%7C%7C5eadbc71ac274ccf2d3d08d6ad1a2b79%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636886725238016932&amp;sdata=u44KZixs648dMNQx6%2FDIJC4sUGOmn0rGsnY%2Bws4ekx8%3D&amp;reserved=0> for Windows 10


        [[alternative HTML version deleted]]


From c@c@voeten @end|ng |rom hum@|e|denun|v@n|  Wed Mar 20 14:09:52 2019
From: c@c@voeten @end|ng |rom hum@|e|denun|v@n| (Voeten, C.C.)
Date: Wed, 20 Mar 2019 13:09:52 +0000
Subject: [R-sig-ME] Help with a multiresponse model
In-Reply-To: <VI1PR06MB592073BE0D167F7B87D010F6FA410@VI1PR06MB5920.eurprd06.prod.outlook.com>
References: <VI1PR06MB5920D9788CA3CD791BB1A472FA410@VI1PR06MB5920.eurprd06.prod.outlook.com>,
 <D14049CE02C4F54D95360EEC06CE45C50FA1FF94@SPMXM08.VUW.leidenuniv.nl>
 <VI1PR06MB592073BE0D167F7B87D010F6FA410@VI1PR06MB5920.eurprd06.prod.outlook.com>
Message-ID: <D14049CE02C4F54D95360EEC06CE45C50FA2005F@SPMXM08.VUW.leidenuniv.nl>

Hi Matteo,

Ah, I see. I had missed that each of your larvae would be a single column of observations. I thought you had multiple dependent measures (e.g. 'ct_day1', 'ct_day2', 'ct_day3') and wanted to fit a multi-response model to those, with larvae as rows.

Would it work if you transformed your data into long format, so that larvae become rows instead of columns and you get an additional column 'larva'? In that case, you could just run something like gam(ct ~ s(WDS) + cros4 + s(larva,bs='re')) (possibly also including random slopes for larva and factor smooths for WDS by larva, and possibly also including a factor for measurement day). That wouldn't be a multi-response model anymore, but from what you've shown it seems to me that this is actually closer to what you want?

Best,
Cesko

Van: Matteo Redana <matteo.redana at hotmail.com> 
Verzonden: woensdag 20 maart 2019 12:29
Aan: Voeten, C.C. <c.c.voeten at hum.leidenuniv.nl>; 'r-sig-mixed-models at r-project.org' <r-sig-mixed-models at r-project.org>
Onderwerp: RE: Help with a multiresponse model

Dear Cesko,
Do you suggest this approach? (see attached table with a subset of three larvae ct1,ct2 and ct3)
?
new<-gam(list(ct1~s(WDS,k=3)+cros4,ct2~s(WDS,k=3)+cros4,ct3~s(WDS,k=3)+cros4),data=linrathen,method="REML",family = mvn(d=3))
?
where WDS is temperature of water, cros4 is a crossbasis object to look at the lagged effect (lag=240, means 10 days)
I have these problems:
I have each sampling days ca. 30 larvae: means that mvn(d=30)?
Given that in the three different sampling days the individuals are different I am assuming that in the same column the individual is the same.
I hope to be clear, if you need other info just ask.
Many Thanks
Matteo
?
?
Sent from https://go.microsoft.com/fwlink/?LinkId=550986 for Windows 10
?
________________________________________
From: Voeten, C.C. <mailto:c.c.voeten at hum.leidenuniv.nl>
Sent: Wednesday, March 20, 2019 10:55:19 AM
To: 'Matteo Redana'; 'r-sig-mixed-models at r-project.org'
Subject: RE: Help with a multiresponse model 
?
Dear Matteo,

Since you are using GAMs anyway, have you considered using gam() with family='mvn(2)'?

Best,
Cesko

-----Oorspronkelijk bericht-----
Van: R-sig-mixed-models <mailto:r-sig-mixed-models-bounces at r-project.org> Namens Matteo Redana
Verzonden: woensdag 20 maart 2019 07:42
Aan: mailto:r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] Help with a multiresponse model

Dear all,

I?m new in the mailing list. I?m coning to you for a problem on how approach my data:

I have a time series of Water temperature from 06-06-2017 to 28-07-2017, 1 value each hour. In the same period the 10\06, 02/07 and 25/07 were sampled larvae from the river and teste for thermotolerance. So each sampling day I have a different number of larvae with different thermotolerances. Working with GAM there are evidence of a trend in thermotolrence, given the high variability in each sample day.

What I want to do is to create a model that look at the impact of water temperature (also lagged values) on the thermotolerance.

Until now I?ve ever worked? with mgcv and dlnm package in hydraulics, but it?s the first time with this kind of data.
I don?t know how to build a model, given that I have water temp. value in a different scale (hourly) and a multiple response in my Y (CT-thermotolerance).
The next step will be to preform a mixed model to test differences in natural site and artificial (this experiment was done in different lake).
Any suggestion is welcome!

Matteo

Sent from Mail<https://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgo.microsoft.com%2Ffwlink%2F%3FLinkId%3D550986&amp;data=02%7C01%7C%7C5eadbc71ac274ccf2d3d08d6ad1a2b79%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636886725238016932&amp;sdata=u44KZixs648dMNQx6%2FDIJC4sUGOmn0rGsnY%2Bws4ekx8%3D&amp;reserved=0> for Windows 10


??????? [[alternative HTML version deleted]]

From m@tteo@red@n@ @end|ng |rom hotm@||@com  Wed Mar 20 14:31:36 2019
From: m@tteo@red@n@ @end|ng |rom hotm@||@com (Matteo Redana)
Date: Wed, 20 Mar 2019 13:31:36 +0000
Subject: [R-sig-ME] Help with a multiresponse model
In-Reply-To: <D14049CE02C4F54D95360EEC06CE45C50FA2005F@SPMXM08.VUW.leidenuniv.nl>
References: <VI1PR06MB5920D9788CA3CD791BB1A472FA410@VI1PR06MB5920.eurprd06.prod.outlook.com>,
 <D14049CE02C4F54D95360EEC06CE45C50FA1FF94@SPMXM08.VUW.leidenuniv.nl>
 <VI1PR06MB592073BE0D167F7B87D010F6FA410@VI1PR06MB5920.eurprd06.prod.outlook.com>,
 <D14049CE02C4F54D95360EEC06CE45C50FA2005F@SPMXM08.VUW.leidenuniv.nl>
Message-ID: <VI1PR06MB592018B42766937E5578F48BFA410@VI1PR06MB5920.eurprd06.prod.outlook.com>

Hi Cesko,

I?m very confused by these data. But if I set ct as row, I need WDS as columns(one for each obs. Corresponding to a specific ct value), or not?



Thanks for the help

Matteo

Sent from Mail<https://go.microsoft.com/fwlink/?LinkId=550986> for Windows 10



________________________________
From: Voeten, C.C. <c.c.voeten at hum.leidenuniv.nl>
Sent: Wednesday, March 20, 2019 2:09:52 PM
To: 'Matteo Redana'; 'r-sig-mixed-models at r-project.org'
Subject: RE: Help with a multiresponse model

Hi Matteo,

Ah, I see. I had missed that each of your larvae would be a single column of observations. I thought you had multiple dependent measures (e.g. 'ct_day1', 'ct_day2', 'ct_day3') and wanted to fit a multi-response model to those, with larvae as rows.

Would it work if you transformed your data into long format, so that larvae become rows instead of columns and you get an additional column 'larva'? In that case, you could just run something like gam(ct ~ s(WDS) + cros4 + s(larva,bs='re')) (possibly also including random slopes for larva and factor smooths for WDS by larva, and possibly also including a factor for measurement day). That wouldn't be a multi-response model anymore, but from what you've shown it seems to me that this is actually closer to what you want?

Best,
Cesko

Van: Matteo Redana <matteo.redana at hotmail.com>
Verzonden: woensdag 20 maart 2019 12:29
Aan: Voeten, C.C. <c.c.voeten at hum.leidenuniv.nl>; 'r-sig-mixed-models at r-project.org' <r-sig-mixed-models at r-project.org>
Onderwerp: RE: Help with a multiresponse model

Dear Cesko,
Do you suggest this approach? (see attached table with a subset of three larvae ct1,ct2 and ct3)

new<-gam(list(ct1~s(WDS,k=3)+cros4,ct2~s(WDS,k=3)+cros4,ct3~s(WDS,k=3)+cros4),data=linrathen,method="REML",family = mvn(d=3))

where WDS is temperature of water, cros4 is a crossbasis object to look at the lagged effect (lag=240, means 10 days)
I have these problems:
I have each sampling days ca. 30 larvae: means that mvn(d=30)?
Given that in the three different sampling days the individuals are different I am assuming that in the same column the individual is the same.
I hope to be clear, if you need other info just ask.
Many Thanks
Matteo


Sent from https://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgo.microsoft.com%2Ffwlink%2F%3FLinkId%3D550986&amp;data=02%7C01%7C%7Cedca9093a3af485c8e6d08d6ad3558e8%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636886841965855787&amp;sdata=XasCsTp88y0VJDO0I2baPljy9DjsgsQhuO4txmgKvcA%3D&amp;reserved=0 for Windows 10

________________________________________
From: Voeten, C.C. <mailto:c.c.voeten at hum.leidenuniv.nl>
Sent: Wednesday, March 20, 2019 10:55:19 AM
To: 'Matteo Redana'; 'r-sig-mixed-models at r-project.org'
Subject: RE: Help with a multiresponse model

Dear Matteo,

Since you are using GAMs anyway, have you considered using gam() with family='mvn(2)'?

Best,
Cesko

-----Oorspronkelijk bericht-----
Van: R-sig-mixed-models <mailto:r-sig-mixed-models-bounces at r-project.org> Namens Matteo Redana
Verzonden: woensdag 20 maart 2019 07:42
Aan: mailto:r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] Help with a multiresponse model

Dear all,

I?m new in the mailing list. I?m coning to you for a problem on how approach my data:

I have a time series of Water temperature from 06-06-2017 to 28-07-2017, 1 value each hour. In the same period the 10\06, 02/07 and 25/07 were sampled larvae from the river and teste for thermotolerance. So each sampling day I have a different number of larvae with different thermotolerances. Working with GAM there are evidence of a trend in thermotolrence, given the high variability in each sample day.

What I want to do is to create a model that look at the impact of water temperature (also lagged values) on the thermotolerance.

Until now I?ve ever worked  with mgcv and dlnm package in hydraulics, but it?s the first time with this kind of data.
I don?t know how to build a model, given that I have water temp. value in a different scale (hourly) and a multiple response in my Y (CT-thermotolerance).
The next step will be to preform a mixed model to test differences in natural site and artificial (this experiment was done in different lake).
Any suggestion is welcome!

Matteo

Sent from Mail<https://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgo.microsoft.com%2Ffwlink%2F%3FLinkId%3D550986&amp;data=02%7C01%7C%7Cedca9093a3af485c8e6d08d6ad3558e8%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636886841965865799&amp;sdata=XwFpCAY%2Bho7wWxEWf8baEtzD4wkw8UpW6EpWAror9nU%3D&amp;reserved=0> for Windows 10


        [[alternative HTML version deleted]]

	[[alternative HTML version deleted]]


From mon|c@@deu@eb|o @end|ng |rom gm@||@com  Wed Mar 20 17:28:35 2019
From: mon|c@@deu@eb|o @end|ng |rom gm@||@com (=?utf-8?Q?M=C3=B3nica_Eus=C3=A9bio?=)
Date: Wed, 20 Mar 2019 16:28:35 +0000
Subject: [R-sig-ME] Same model different data compare coefficients mixed
 models r
Message-ID: <AD44B11A-3309-45F4-9A00-7834AD18B2BD@gmail.com>

I am predicting the values of sales for every day/product with the information of the previous 30 days. I use the same variables to obtain the model used for each day/product. It is a 30 days temporal model with random effects at the region level. 

The values of the coefficients will be different according to the product and day.

What I am trying to figure out is if for the same product I can compare the coefficients for the different days. There is some kind of assessment/formal test to do this?

Imagine that we have this model structure, and I want to predict the sales for one day: model_day1<-lmer(y ~ x1 + x2+ x3 + (1|x4) + cos(2*pi*t/7) + sin(2*pi*t/7), data=dados) This is estimated with the 30 previous days, to know the sales of the 31st day.

Then I will use the same model structure to predict the sales of the second day, with the previous 30 days. 

model_day2<-lmer(y ~ x1 + x2+ x3 + (1|x4) + cos(2*pi*t/7) + sin(2*pi*t/7), data=dados)

There is a way to know if x1 coefficient of the model of day 1 is the same as x1 of the model of day 2? I am also trying to compare for more than just two days, and maybe figure out if working days are different from noon-working days.
	[[alternative HTML version deleted]]


From ph||||p@@|d@y @end|ng |rom mp|@n|  Wed Mar 20 18:02:14 2019
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Wed, 20 Mar 2019 18:02:14 +0100
Subject: [R-sig-ME] Random effects in multinomial regression in R?
In-Reply-To: <CAEA998ifF3J6fdjknpAZm_xpjTjTUrY1p5oRJqzyAGSGO0FV1w@mail.gmail.com>
References: <CAEA998ifF3J6fdjknpAZm_xpjTjTUrY1p5oRJqzyAGSGO0FV1w@mail.gmail.com>
Message-ID: <d9ebad5b-3b52-0ce0-05d7-6731444874a6@mpi.nl>

Generally speaking for the parameterization of mixed-effects models in
lme4/brms/the usual packages, it doesn't make sense to have a varying
slope (e.g. Time|Subject) without the corresponding fixed effect. This
is because the varying slopes are calculated as offsets from the group
mean, i.e from the fixed effect estimate. Not doing including the fixed
effect is equivalent to assuming the group mean is zero, which is
usually not the assumption you want to make.

If you fit models with random slopes without the corresponding fixed
effects, then there are two main problems:

1. The corresponding variance parameter will be mis-estimated because it
will be the average squared distance to zero and not the average squared
distance to the mean (and average squared distance to the mean is the
definition of variance).

2. The model may not converge because the numerics are set up under the
"zero mean" assumption. For lme4/nlme, this is the case, but I believe
that brms may do some internal reparameterization that may avoid these
difficulties. (And a model fit with MCMC (brms) may not have the same
numerical issues as a model fit with MLE (lme4)).

In brief: just add time as a fixed effect.

Also: why not fit your model as a continuous model with pre vs. post as
a contrast in the model rather reducing a continuous variable to a
category? You can still apply a categorical distinction afterwards if
you so desire, but in my experience, it's best to defer making things
categorical until as late as possible (see also Frank Harrel's comments
on prediction vs. classification:
http://www.fharrell.com/post/classification/). Moreover, it's a lot
easier to fit a continuous model than a multinomial one ....

Best,
Phillip

On 18/3/19 7:11 pm, Souheyla GHEBGHOUB wrote:
> I have *Change* from Pretest to Posttest (gain, no_gain, decline) as the
> DV. Also *Pretest* and *Group* as covariates. This called for a multinomial
> regression:
> 
> mod0 <- brm(Change ~ Pretest + Group)
> 
> *Question: *I'd like to add random effects of *Subject* and *Word*, which
> may differ by time, but I don't have effect of *Time* to do:
> 
> mod1 <- brm(Change ~ Pretest + Group + (Time|Subject) + (Time|Word))
> 
> So I thought of this:
> 
> mod2 <- brm(Change ~ Pretest + Group + (1|Subject) + (1|Word))
> 
> but this also seems wrong to me. What do you think is the best way to treat
> random effects in this situation, please?
> 
> Thank you
> 
> Souheyla Ghebghoub
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From @ouhey|@@ghebghoub @end|ng |rom gm@||@com  Wed Mar 20 18:39:36 2019
From: @ouhey|@@ghebghoub @end|ng |rom gm@||@com (Souheyla GHEBGHOUB)
Date: Wed, 20 Mar 2019 17:39:36 +0000
Subject: [R-sig-ME] Random effects in multinomial regression in R?
In-Reply-To: <d9ebad5b-3b52-0ce0-05d7-6731444874a6@mpi.nl>
References: <CAEA998ifF3J6fdjknpAZm_xpjTjTUrY1p5oRJqzyAGSGO0FV1w@mail.gmail.com>
 <d9ebad5b-3b52-0ce0-05d7-6731444874a6@mpi.nl>
Message-ID: <CAEA998j+S7V=1EMmPNeOqtyOXwbBUfiv-PJgqgafaXDkrwB+1w@mail.gmail.com>

Hi Philip,

Thank you for the clarification. But I might have not make it clear in my
question.

I don't have Time in my data at all because I chose to predict change
rather than having posttest and pretest responses as DV and Time as fixed
effect.

I chose this way because I have groups of subjects who were tested on
words, and I was not too sure whether, a simple regression with Responses
as DV and Time (Pretest/Posttest) as IV , will take into account
differences between Pretest and Posttest at the level of each word. That
is, I don't know whether it will sum the overall pretest score of each
subject then compare it to its posttest, while I want it to compare each
subject score of each word from pretest to posttest then base its analysis
on these score changes.

That's why I did not want to risk it and chose *score change* as the DV
instead. But I was faced with another problem which is absence of Time
effect by which subjects differ for my random slopes?

Best,
Souheyla

On Wed, 20 Mar 2019 at 17:02, Phillip Alday <phillip.alday at mpi.nl> wrote:

> Generally speaking for the parameterization of mixed-effects models in
> lme4/brms/the usual packages, it doesn't make sense to have a varying
> slope (e.g. Time|Subject) without the corresponding fixed effect. This
> is because the varying slopes are calculated as offsets from the group
> mean, i.e from the fixed effect estimate. Not doing including the fixed
> effect is equivalent to assuming the group mean is zero, which is
> usually not the assumption you want to make.
>
> If you fit models with random slopes without the corresponding fixed
> effects, then there are two main problems:
>
> 1. The corresponding variance parameter will be mis-estimated because it
> will be the average squared distance to zero and not the average squared
> distance to the mean (and average squared distance to the mean is the
> definition of variance).
>
> 2. The model may not converge because the numerics are set up under the
> "zero mean" assumption. For lme4/nlme, this is the case, but I believe
> that brms may do some internal reparameterization that may avoid these
> difficulties. (And a model fit with MCMC (brms) may not have the same
> numerical issues as a model fit with MLE (lme4)).
>
> In brief: just add time as a fixed effect.
>
> Also: why not fit your model as a continuous model with pre vs. post as
> a contrast in the model rather reducing a continuous variable to a
> category? You can still apply a categorical distinction afterwards if
> you so desire, but in my experience, it's best to defer making things
> categorical until as late as possible (see also Frank Harrel's comments
> on prediction vs. classification:
> http://www.fharrell.com/post/classification/). Moreover, it's a lot
> easier to fit a continuous model than a multinomial one ....
>
> Best,
> Phillip
>
> On 18/3/19 7:11 pm, Souheyla GHEBGHOUB wrote:
> > I have *Change* from Pretest to Posttest (gain, no_gain, decline) as the
> > DV. Also *Pretest* and *Group* as covariates. This called for a
> multinomial
> > regression:
> >
> > mod0 <- brm(Change ~ Pretest + Group)
> >
> > *Question: *I'd like to add random effects of *Subject* and *Word*, which
> > may differ by time, but I don't have effect of *Time* to do:
> >
> > mod1 <- brm(Change ~ Pretest + Group + (Time|Subject) + (Time|Word))
> >
> > So I thought of this:
> >
> > mod2 <- brm(Change ~ Pretest + Group + (1|Subject) + (1|Word))
> >
> > but this also seems wrong to me. What do you think is the best way to
> treat
> > random effects in this situation, please?
> >
> > Thank you
> >
> > Souheyla Ghebghoub
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>

	[[alternative HTML version deleted]]


From ph||||p@@|d@y @end|ng |rom mp|@n|  Wed Mar 20 18:56:38 2019
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Wed, 20 Mar 2019 18:56:38 +0100
Subject: [R-sig-ME] Random effects in multinomial regression in R?
In-Reply-To: <CAEA998j+S7V=1EMmPNeOqtyOXwbBUfiv-PJgqgafaXDkrwB+1w@mail.gmail.com>
References: <CAEA998ifF3J6fdjknpAZm_xpjTjTUrY1p5oRJqzyAGSGO0FV1w@mail.gmail.com>
 <d9ebad5b-3b52-0ce0-05d7-6731444874a6@mpi.nl>
 <CAEA998j+S7V=1EMmPNeOqtyOXwbBUfiv-PJgqgafaXDkrwB+1w@mail.gmail.com>
Message-ID: <e7bc83c6-4478-32ba-a652-88ed606300fa@mpi.nl>

On 20/3/19 6:39 pm, Souheyla GHEBGHOUB wrote:
> Hi Philip,?
> 
> Thank you for the clarification. But I might have not make it clear in
> my question.
> 
> I don't have Time in my data at all because I chose to predict change
> rather than having posttest and pretest responses as DV and Time as
> fixed effect. 

If Time isn't in your difference data, then it really makes no sense to
have it in your model anywhere ....

> I chose this way because I have groups of subjects who were tested on
> words, and I was not too sure whether, a simple regression with
> Responses as DV and Time (Pretest/Posttest) as IV , will take into
> account differences between Pretest and Posttest at the level of each
> word. That is,?I don't know whether it will sum the overall pretest
> score of each subject then compare it to its posttest, while I want it
> to?compare each subject score of each word from pretest to posttest then
> base its analysis on these score changes. 

I don't want to be too harsh, but if you were unsure about that, then
that's the question you should have asked first. (See also the XY
problem, https://en.wikipedia.org/wiki/XY_problem)

> 
> That's why I did not want to risk it and chose /score change/?as the DV
> instead. But I was faced with another problem which is absence?of Time
> effect by which subjects differ for my random slopes?

Assuming you want to compute the difference outside of the model, then
you could (and I would argue should) still use the continuous/numeric
difference and not a categorical thresholding of that difference as your
dependent variable.

In that case, I would argue that there can't be a "Time" effect by
subject because you are measuring the difference, which incorporates the
variance at each Time in the variance of the difference. Same for word.

Depending on the exact structure of the test and whether there are
multiple pretest scores by subject or by word, you could potentially
include that as a random slope, but to make a more precise
recommendation there, we need to know more about your data.

Best,
Phillip





> 
> Best,
> Souheyla?
> 
> On Wed, 20 Mar 2019 at 17:02, Phillip Alday <phillip.alday at mpi.nl
> <mailto:phillip.alday at mpi.nl>> wrote:
> 
>     Generally speaking for the parameterization of mixed-effects models in
>     lme4/brms/the usual packages, it doesn't make sense to have a varying
>     slope (e.g. Time|Subject) without the corresponding fixed effect. This
>     is because the varying slopes are calculated as offsets from the group
>     mean, i.e from the fixed effect estimate. Not doing including the fixed
>     effect is equivalent to assuming the group mean is zero, which is
>     usually not the assumption you want to make.
> 
>     If you fit models with random slopes without the corresponding fixed
>     effects, then there are two main problems:
> 
>     1. The corresponding variance parameter will be mis-estimated because it
>     will be the average squared distance to zero and not the average squared
>     distance to the mean (and average squared distance to the mean is the
>     definition of variance).
> 
>     2. The model may not converge because the numerics are set up under the
>     "zero mean" assumption. For lme4/nlme, this is the case, but I believe
>     that brms may do some internal reparameterization that may avoid these
>     difficulties. (And a model fit with MCMC (brms) may not have the same
>     numerical issues as a model fit with MLE (lme4)).
> 
>     In brief: just add time as a fixed effect.
> 
>     Also: why not fit your model as a continuous model with pre vs. post as
>     a contrast in the model rather reducing a continuous variable to a
>     category? You can still apply a categorical distinction afterwards if
>     you so desire, but in my experience, it's best to defer making things
>     categorical until as late as possible (see also Frank Harrel's comments
>     on prediction vs. classification:
>     http://www.fharrell.com/post/classification/). Moreover, it's a lot
>     easier to fit a continuous model than a multinomial one ....
> 
>     Best,
>     Phillip
> 
>     On 18/3/19 7:11 pm, Souheyla GHEBGHOUB wrote:
>     > I have *Change* from Pretest to Posttest (gain, no_gain, decline)
>     as the
>     > DV. Also *Pretest* and *Group* as covariates. This called for a
>     multinomial
>     > regression:
>     >
>     > mod0 <- brm(Change ~ Pretest + Group)
>     >
>     > *Question: *I'd like to add random effects of *Subject* and
>     *Word*, which
>     > may differ by time, but I don't have effect of *Time* to do:
>     >
>     > mod1 <- brm(Change ~ Pretest + Group + (Time|Subject) + (Time|Word))
>     >
>     > So I thought of this:
>     >
>     > mod2 <- brm(Change ~ Pretest + Group + (1|Subject) + (1|Word))
>     >
>     > but this also seems wrong to me. What do you think is the best way
>     to treat
>     > random effects in this situation, please?
>     >
>     > Thank you
>     >
>     > Souheyla Ghebghoub
>     >
>     >? ? ? ?[[alternative HTML version deleted]]
>     >
>     > _______________________________________________
>     > R-sig-mixed-models at r-project.org
>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>     >
>


From Ph||||p@A|d@y @end|ng |rom mp|@n|  Wed Mar 20 19:51:13 2019
From: Ph||||p@A|d@y @end|ng |rom mp|@n| (Alday, Phillip)
Date: Wed, 20 Mar 2019 18:51:13 +0000
Subject: [R-sig-ME] Random effects in multinomial regression in R?
In-Reply-To: <CAEA998hdxDeJr1cPSs8gQob-5J0yKk+f0DwDZTw4mfPWNqc79w@mail.gmail.com>
References: <CAEA998ifF3J6fdjknpAZm_xpjTjTUrY1p5oRJqzyAGSGO0FV1w@mail.gmail.com>
 <d9ebad5b-3b52-0ce0-05d7-6731444874a6@mpi.nl>
 <CAEA998j+S7V=1EMmPNeOqtyOXwbBUfiv-PJgqgafaXDkrwB+1w@mail.gmail.com>
 <e7bc83c6-4478-32ba-a652-88ed606300fa@mpi.nl>
 <CAEA998hdxDeJr1cPSs8gQob-5J0yKk+f0DwDZTw4mfPWNqc79w@mail.gmail.com>
Message-ID: <d741bd3a-af48-adee-2b2e-4bdcef824865@mpi.nl>

Please keep the list in CC.

I really can't provide more advice about whether to do an intercept-only model or include the Pretest score in the random effects without knowing more about your data. If you have multiple pretest scores per subject and word, then it might make sense to include them in the random effects, *depending on your data and research question*. If you don't, then it definitely doesn't make sense to estimate a slope (i.e a rate) from a single static observation.

Phillip

On 20/3/19 7:33 pm, Souheyla GHEBGHOUB wrote:
Hi again Phillip,

My question is  :  I'd like to add random effects of Subject and Word, which may differ by time from pretest to posttest, but I don't have effect of Time , so I can't do:

mod1 <- brm(Change ~ Pretest + Group + (Time|Subject) + (Time|Word))  So should I just do (1|Subject) + (1|Word))  or  (Pretest|Subject) + (Pretest|Word)) or exclude random effects?

Thank you for looking into this :)

Souheyla







On Wed, 20 Mar 2019 at 18:28, Phillip Alday <phillip.alday at mpi.nl<mailto:phillip.alday at mpi.nl>> wrote:
On 20/3/19 6:39 pm, Souheyla GHEBGHOUB wrote:
> Hi Philip,
>
> Thank you for the clarification. But I might have not make it clear in
> my question.
>
> I don't have Time in my data at all because I chose to predict change
> rather than having posttest and pretest responses as DV and Time as
> fixed effect.

If Time isn't in your difference data, then it really makes no sense to
have it in your model anywhere ....

> I chose this way because I have groups of subjects who were tested on
> words, and I was not too sure whether, a simple regression with
> Responses as DV and Time (Pretest/Posttest) as IV , will take into
> account differences between Pretest and Posttest at the level of each
> word. That is, I don't know whether it will sum the overall pretest
> score of each subject then compare it to its posttest, while I want it
> to compare each subject score of each word from pretest to posttest then
> base its analysis on these score changes.

I don't want to be too harsh, but if you were unsure about that, then
that's the question you should have asked first. (See also the XY
problem, https://en.wikipedia.org/wiki/XY_problem)

>
> That's why I did not want to risk it and chose /score change/ as the DV
> instead. But I was faced with another problem which is absence of Time
> effect by which subjects differ for my random slopes?

Assuming you want to compute the difference outside of the model, then
you could (and I would argue should) still use the continuous/numeric
difference and not a categorical thresholding of that difference as your
dependent variable.

In that case, I would argue that there can't be a "Time" effect by
subject because you are measuring the difference, which incorporates the
variance at each Time in the variance of the difference. Same for word.

Depending on the exact structure of the test and whether there are
multiple pretest scores by subject or by word, you could potentially
include that as a random slope, but to make a more precise
recommendation there, we need to know more about your data.

Best,
Phillip





>
> Best,
> Souheyla
>
> On Wed, 20 Mar 2019 at 17:02, Phillip Alday <phillip.alday at mpi.nl<mailto:phillip.alday at mpi.nl>
> <mailto:phillip.alday at mpi.nl<mailto:phillip.alday at mpi.nl>>> wrote:
>
>     Generally speaking for the parameterization of mixed-effects models in
>     lme4/brms/the usual packages, it doesn't make sense to have a varying
>     slope (e.g. Time|Subject) without the corresponding fixed effect. This
>     is because the varying slopes are calculated as offsets from the group
>     mean, i.e from the fixed effect estimate. Not doing including the fixed
>     effect is equivalent to assuming the group mean is zero, which is
>     usually not the assumption you want to make.
>
>     If you fit models with random slopes without the corresponding fixed
>     effects, then there are two main problems:
>
>     1. The corresponding variance parameter will be mis-estimated because it
>     will be the average squared distance to zero and not the average squared
>     distance to the mean (and average squared distance to the mean is the
>     definition of variance).
>
>     2. The model may not converge because the numerics are set up under the
>     "zero mean" assumption. For lme4/nlme, this is the case, but I believe
>     that brms may do some internal reparameterization that may avoid these
>     difficulties. (And a model fit with MCMC (brms) may not have the same
>     numerical issues as a model fit with MLE (lme4)).
>
>     In brief: just add time as a fixed effect.
>
>     Also: why not fit your model as a continuous model with pre vs. post as
>     a contrast in the model rather reducing a continuous variable to a
>     category? You can still apply a categorical distinction afterwards if
>     you so desire, but in my experience, it's best to defer making things
>     categorical until as late as possible (see also Frank Harrel's comments
>     on prediction vs. classification:
>     http://www.fharrell.com/post/classification/). Moreover, it's a lot
>     easier to fit a continuous model than a multinomial one ....
>
>     Best,
>     Phillip
>
>     On 18/3/19 7:11 pm, Souheyla GHEBGHOUB wrote:
>     > I have *Change* from Pretest to Posttest (gain, no_gain, decline)
>     as the
>     > DV. Also *Pretest* and *Group* as covariates. This called for a
>     multinomial
>     > regression:
>     >
>     > mod0 <- brm(Change ~ Pretest + Group)
>     >
>     > *Question: *I'd like to add random effects of *Subject* and
>     *Word*, which
>     > may differ by time, but I don't have effect of *Time* to do:
>     >
>     > mod1 <- brm(Change ~ Pretest + Group + (Time|Subject) + (Time|Word))
>     >
>     > So I thought of this:
>     >
>     > mod2 <- brm(Change ~ Pretest + Group + (1|Subject) + (1|Word))
>     >
>     > but this also seems wrong to me. What do you think is the best way
>     to treat
>     > random effects in this situation, please?
>     >
>     > Thank you
>     >
>     > Souheyla Ghebghoub
>     >
>     >       [[alternative HTML version deleted]]
>     >
>     > _______________________________________________
>     > R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org>
>     <mailto:R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org>> mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>     >
>

	[[alternative HTML version deleted]]


From w@||dm@w@@@10 @end|ng |rom gm@||@com  Wed Mar 20 20:01:01 2019
From: w@||dm@w@@@10 @end|ng |rom gm@||@com (Walid Crampton-Mawass)
Date: Wed, 20 Mar 2019 15:01:01 -0400
Subject: [R-sig-ME] MCMCglmm error
Message-ID: <CAJtCY7Uv+n13xJhR8umzferErPmRLf5HMNPKcNrQ+dbNe=fDQQ@mail.gmail.com>

Hello everyone,

I started getting this error when I run an animal model with a maternal
random effect using MCMCglmm. here is my code:

*prior1 <- list(R=list(V=1, nu=0.002), G=list(G1=list(V=1,
nu=0.002),G2=list(V=1, nu=0.002)))*

*model_afr_3_1 <- MCMCglmm(AFR~
1+OffMortality+COEFPAR+I(COEFPAR*COEFPAR)+TWIN+BIRTHYW+mortrate1 , random =
~animal + MOTHERW, rcov = ~units, data = IAC, pedigree = prunedPed, family
= "gaussian" , nitt = 3500000, burnin = 500000, thin = 3000, prior =
prior1, verbose = FALSE, pr=TRUE)*
*Error in t(ZZ[[k]]) : invalid object passed to as_cholmod_sparse*
*Calls: MCMCglmm -> buildZ -> t -> t*

I was able to find out that the variable causing the error is the second
random term "MOTHERW" which represents the id of each individual's mother
as to model maternal variation. However I started getting this error
recently even though it used to work beforehand without any errors. The
variable is set as a multi-level factor just as is done to the "animal"
variable. But there are a few missing values for some individuals (20 out
of 572)

*str(dat$MOTHERW)*
* Factor w/ 297 levels "100007","100032",..: 114 142 207 12 258 168 261 230
179 107 ...*

I tried another variable, FATHERW, which is the id of the father of the
individual. I get the same error again : *Error in t(ZZ[[k]]) : invalid
object passed to as_cholmod_sparse*

However, when using another variable in my data as a random variable
instead, BIRTHYW (birth year of individual; set as a factor), the model
runs without any errors. The only difference is that there are no missing
values in this variable compared MOTHERW  and FATHERW.

is it probably the case that we can't use a variable with a few missing
values as a random variable in MCMCglmm anymore?

Thanks for any help!
-- 
Walid Mawass
Ph.D. candidate in Cellular and Molecular Biology
Population Genetics Laboratory
University of Qu?bec at Trois-Rivi?res
3351, boul. des Forges, C.P. 500
Trois-Rivi?res (Qu?bec) G9A 5H7
Telephone: 819-376-5011 poste 3384

	[[alternative HTML version deleted]]


From j@h@d||e|d @end|ng |rom ed@@c@uk  Wed Mar 20 20:34:46 2019
From: j@h@d||e|d @end|ng |rom ed@@c@uk (HADFIELD Jarrod)
Date: Wed, 20 Mar 2019 19:34:46 +0000
Subject: [R-sig-ME] MCMCglmm error
In-Reply-To: <CAJtCY7Uv+n13xJhR8umzferErPmRLf5HMNPKcNrQ+dbNe=fDQQ@mail.gmail.com>
References: <CAJtCY7Uv+n13xJhR8umzferErPmRLf5HMNPKcNrQ+dbNe=fDQQ@mail.gmail.com>
Message-ID: <D59133B4-9B65-4064-A3FA-EA94632A7D44@ed.ac.uk>

Hi,

MCMCglmm used to issue a warning if there were NA?s in the random effect predictors. However, people seemed to ignore it when it was inappropriate. The only case I can think of where it should be ignored is in a multi membership model where the number of members varies over observations, which is probably a rare type of model. Consequently, I now just stop people doing what I *think*they didn?t intend. It may be the case that you don?t know who the mother is, but that doesn?t mean they don?t have a mother. Fitting a dummy maternal id is the standard solution.

A more informative error message will be in the next release.

Cheers,

Jarrod

> On 20 Mar 2019, at 19:01, Walid Crampton-Mawass <walidmawass10 at gmail.com> wrote:
>
> Hello everyone,
>
> I started getting this error when I run an animal model with a maternal
> random effect using MCMCglmm. here is my code:
>
> *prior1 <- list(R=list(V=1, nu=0.002), G=list(G1=list(V=1,
> nu=0.002),G2=list(V=1, nu=0.002)))*
>
> *model_afr_3_1 <- MCMCglmm(AFR~
> 1+OffMortality+COEFPAR+I(COEFPAR*COEFPAR)+TWIN+BIRTHYW+mortrate1 , random =
> ~animal + MOTHERW, rcov = ~units, data = IAC, pedigree = prunedPed, family
> = "gaussian" , nitt = 3500000, burnin = 500000, thin = 3000, prior =
> prior1, verbose = FALSE, pr=TRUE)*
> *Error in t(ZZ[[k]]) : invalid object passed to as_cholmod_sparse*
> *Calls: MCMCglmm -> buildZ -> t -> t*
>
> I was able to find out that the variable causing the error is the second
> random term "MOTHERW" which represents the id of each individual's mother
> as to model maternal variation. However I started getting this error
> recently even though it used to work beforehand without any errors. The
> variable is set as a multi-level factor just as is done to the "animal"
> variable. But there are a few missing values for some individuals (20 out
> of 572)
>
> *str(dat$MOTHERW)*
> * Factor w/ 297 levels "100007","100032",..: 114 142 207 12 258 168 261 230
> 179 107 ...*
>
> I tried another variable, FATHERW, which is the id of the father of the
> individual. I get the same error again : *Error in t(ZZ[[k]]) : invalid
> object passed to as_cholmod_sparse*
>
> However, when using another variable in my data as a random variable
> instead, BIRTHYW (birth year of individual; set as a factor), the model
> runs without any errors. The only difference is that there are no missing
> values in this variable compared MOTHERW  and FATHERW.
>
> is it probably the case that we can't use a variable with a few missing
> values as a random variable in MCMCglmm anymore?
>
> Thanks for any help!
> --
> Walid Mawass
> Ph.D. candidate in Cellular and Molecular Biology
> Population Genetics Laboratory
> University of Qu?bec at Trois-Rivi?res
> 3351, boul. des Forges, C.P. 500
> Trois-Rivi?res (Qu?bec) G9A 5H7
> Telephone: 819-376-5011 poste 3384
>
> [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

The University of Edinburgh is a charitable body, registered in Scotland, with registration number SC005336.

From m@tthew@t@boden @end|ng |rom gm@||@com  Wed Mar 20 20:50:27 2019
From: m@tthew@t@boden @end|ng |rom gm@||@com (Matthew Boden)
Date: Wed, 20 Mar 2019 12:50:27 -0700
Subject: [R-sig-ME] Fwd: glm.nb convergence issues
In-Reply-To: <CAJuCY5w74vVuATprurgR86zByRSbvztvN92a2m8e6ENjTG-+Jw@mail.gmail.com>
References: <BN8PR09MB3554300E08C4B76B4E25FC9587410@BN8PR09MB3554.namprd09.prod.outlook.com>
 <CAE10PCzNa4V0Hr03K4TwOeACavzFZUHoRo4Vrj_+VUDTueVCCA@mail.gmail.com>
 <CAJuCY5w74vVuATprurgR86zByRSbvztvN92a2m8e6ENjTG-+Jw@mail.gmail.com>
Message-ID: <CAE10PCyfie21gDQHw9EWfWKFz66JAVqSU9HHBYzEJfx18sH09g@mail.gmail.com>

Thank you, Thierry.  A csv file is attached; hopefully, that works.

The numerator divided by the denominator will always fall between 0 and 1
for these data, and is typically very close to 1.
The predictor variable ranges from about 3 to 18, but even when scaled, the
model won't converge.

Matt












On Wed, Mar 20, 2019 at 1:48 AM Thierry Onkelinx <thierry.onkelinx at inbo.be>
wrote:

> Dear Matthew,
>
> The mailing list accepts only a limited number of file formats as
> attachment. Your data got stripped. Can you resend the data or send a link
> to the data?
>
> The offset requires the log because the model is using the log link. Your
> model fits log(E(wait_n)) = offset(log(wait_d)) + covariates which you can
> rewrite as log(E(wait_n)) - offset(log(wait_d)) = covariates or
> log(E(wait_n) / wait_d) = covariates. Pick a relevant magnitude of wait_d
> so that the magnitude of wait_n/wait_d is somewhat near 1. E.g. we don't
> express the number of inhabitats per m? but rather per km?
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op wo 20 mrt. 2019 om 06:43 schreef Matthew Boden <
> matthew.t.boden at gmail.com>:
>
>> Hello,
>>
>>
>>
>> I?m working my way through convergence issues related to a negative
>> binomial mixed model with an offset and random intercept. Data attached.
>>
>>
>>
>> A3 <- glmer.nb(wait_n ~ offset(log(wait_d)) +  scale(spr) + (1 | check),
>> data = ST)
>>
>> summary(A3)
>>
>>
>>
>> #Model failed to converge with max|grad| = 0.0031459 (tol = 0.001,
>> component 1)
>>
>> #Model is nearly unidentifiable: very large eigenvalue
>>
>> # - Rescale variables?
>>
>>
>>
>> I have two questions.
>>
>>
>>
>> 1) Outcome variable (wait_n) values are much larger than predictor (spr)
>> values.
>>
>>
>>
>> Mean wait_n = 11,783
>>
>> Mean spr = 7.34
>>
>>
>>
>> Would transforming the predictor (e.g., multiply by 1000) and/or offset
>> make sense here. The offset confuses the issue (or, me) - as a log, it is
>> quite small relative to the outcome and on but on the same scale as the
>> other predictor.
>>
>>
>>
>> 2) The use of allFit to try different optimizers seems to be giving
>> conflicting results.
>>
>>
>>
>> A3.all <- allFit(A3, meth.tab = NULL)
>>
>> ss <- summary(A3.all)
>>
>>
>>
>> #bobyqa : [OK]
>>
>> #Nelder_Mead : [OK]
>>
>> #nlminbwrap : [OK]
>>
>> #nmkbw : [OK]
>>
>> #optimx.L-BFGS-B : [OK]
>>
>> #nloptwrap.NLOPT_LN_NELDERMEAD : [OK]
>>
>> #nloptwrap.NLOPT_LN_BOBYQA : [OK]
>>
>>
>>
>> ss$ which.OK
>>
>>
>>
>> #bobyqa                         TRUE
>>
>> #Nelder_Mead                    TRUE
>>
>> #nlminbwrap                     TRUE
>>
>> #nmkbw                          TRUE
>>
>> #optimx.L-BFGS-B                TRUE
>>
>> #nloptwrap.NLOPT_LN_NELDERMEAD  TRUE
>>
>> #nloptwrap.NLOPT_LN_BOBYQA      TRUE
>>
>>
>>
>> What do you know, all optimizers supposedly work.
>>
>> However, some of them lead to a model that converges, and others don?t.
>>
>>
>>
>> A3a <- glmer.nb(wait_n ~ offset(log(wait_d)) +  scale(spr) + (1 | check),
>> data = ST, glmerControl(optimizer = "bobyqa"))
>>
>>
>>
>> #Model is nearly unidentifiable: very large eigenvalue
>>
>> # - Rescale variables?
>>
>>
>>
>> A3b <- glmer.nb(wait_n ~ offset(log(wait_d)) +  scale(spr) + (1 | check),
>> data = ST, glmerControl(optimizer = "Nelder_Mead"))
>>
>>
>>
>> #Model failed to converge with max|grad| = 0.00553513 (tol = 0.001,
>> component 1)
>>
>> #Model is nearly unidentifiable: very large eigenvalue
>>
>> # - Rescale variables?
>>
>>
>>
>> A3c <- glmer.nb(wait_n ~ offset(log(wait_d)) +  scale(spr) + (1 | check),
>> data = ST, glmerControl(optimizer = "nlminbwrap"))
>>
>>
>>
>> #Model is nearly unidentifiable: very large eigenvalue
>>
>> # - Rescale variables?
>>
>>
>>
>> A3d <- glmer.nb(wait_n ~ offset(log(wait_d)) +  scale(spr) + (1 | check),
>> data = ST, glmerControl(optimizer = "nmkbw"))
>>
>>
>>
>> #Model failed to converge with max|grad| = 0.00177926 (tol = 0.001,
>> component 1)
>>
>> #Model is nearly unidentifiable: very large eigenvalue
>>
>> # - Rescale variables?
>>
>>
>>
>> I?m trying to understand why allFit responds that all optimizers converge
>> when they actually do not.
>>
>>
>>
>> Thank you,
>>
>> Matt
>>
>>
>>
>> Matthew Boden, Ph.D.
>>
>> Senior Evaluator
>>
>> Program Evaluation & Resource Center
>>
>> Office of Mental Health & Suicide Prevention
>>
>> Veterans Health Administration
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

From w@||dm@w@@@10 @end|ng |rom gm@||@com  Wed Mar 20 20:58:25 2019
From: w@||dm@w@@@10 @end|ng |rom gm@||@com (Walid Crampton-Mawass)
Date: Wed, 20 Mar 2019 15:58:25 -0400
Subject: [R-sig-ME] MCMCglmm error
In-Reply-To: <D59133B4-9B65-4064-A3FA-EA94632A7D44@ed.ac.uk>
References: <CAJtCY7Uv+n13xJhR8umzferErPmRLf5HMNPKcNrQ+dbNe=fDQQ@mail.gmail.com>
 <D59133B4-9B65-4064-A3FA-EA94632A7D44@ed.ac.uk>
Message-ID: <CAJtCY7VXuQ7CsO_rUzCRB1E8crtG1uNxRsj_3fgvw-kqC0Y9gw@mail.gmail.com>

Thank you Jarrod for your reply!

If I understand correctly then, to stop this error I should just give a
dummy maternal id for the individuals with missing id.

Thank you again for your help

Walid Mawass






On Wed, Mar 20, 2019, 3:34 PM HADFIELD Jarrod <j.hadfield at ed.ac.uk> wrote:

> Hi,
>
> MCMCglmm used to issue a warning if there were NA?s in the random effect
> predictors. However, people seemed to ignore it when it was inappropriate.
> The only case I can think of where it should be ignored is in a multi
> membership model where the number of members varies over observations,
> which is probably a rare type of model. Consequently, I now just stop
> people doing what I *think*they didn?t intend. It may be the case that you
> don?t know who the mother is, but that doesn?t mean they don?t have a
> mother. Fitting a dummy maternal id is the standard solution.
>
> A more informative error message will be in the next release.
>
> Cheers,
>
> Jarrod
>
> > On 20 Mar 2019, at 19:01, Walid Crampton-Mawass <walidmawass10 at gmail.com>
> wrote:
> >
> > Hello everyone,
> >
> > I started getting this error when I run an animal model with a maternal
> > random effect using MCMCglmm. here is my code:
> >
> > *prior1 <- list(R=list(V=1, nu=0.002), G=list(G1=list(V=1,
> > nu=0.002),G2=list(V=1, nu=0.002)))*
> >
> > *model_afr_3_1 <- MCMCglmm(AFR~
> > 1+OffMortality+COEFPAR+I(COEFPAR*COEFPAR)+TWIN+BIRTHYW+mortrate1 ,
> random =
> > ~animal + MOTHERW, rcov = ~units, data = IAC, pedigree = prunedPed,
> family
> > = "gaussian" , nitt = 3500000, burnin = 500000, thin = 3000, prior =
> > prior1, verbose = FALSE, pr=TRUE)*
> > *Error in t(ZZ[[k]]) : invalid object passed to as_cholmod_sparse*
> > *Calls: MCMCglmm -> buildZ -> t -> t*
> >
> > I was able to find out that the variable causing the error is the second
> > random term "MOTHERW" which represents the id of each individual's mother
> > as to model maternal variation. However I started getting this error
> > recently even though it used to work beforehand without any errors. The
> > variable is set as a multi-level factor just as is done to the "animal"
> > variable. But there are a few missing values for some individuals (20 out
> > of 572)
> >
> > *str(dat$MOTHERW)*
> > * Factor w/ 297 levels "100007","100032",..: 114 142 207 12 258 168 261
> 230
> > 179 107 ...*
> >
> > I tried another variable, FATHERW, which is the id of the father of the
> > individual. I get the same error again : *Error in t(ZZ[[k]]) : invalid
> > object passed to as_cholmod_sparse*
> >
> > However, when using another variable in my data as a random variable
> > instead, BIRTHYW (birth year of individual; set as a factor), the model
> > runs without any errors. The only difference is that there are no missing
> > values in this variable compared MOTHERW  and FATHERW.
> >
> > is it probably the case that we can't use a variable with a few missing
> > values as a random variable in MCMCglmm anymore?
> >
> > Thanks for any help!
> > --
> > Walid Mawass
> > Ph.D. candidate in Cellular and Molecular Biology
> > Population Genetics Laboratory
> > University of Qu?bec at Trois-Rivi?res
> > 3351, boul. des Forges, C.P. 500
> > Trois-Rivi?res (Qu?bec) G9A 5H7
> > Telephone: 819-376-5011 poste 3384
> >
> > [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> The University of Edinburgh is a charitable body, registered in Scotland,
> with registration number SC005336.
>

	[[alternative HTML version deleted]]


From j@h@d||e|d @end|ng |rom ed@@c@uk  Wed Mar 20 21:05:18 2019
From: j@h@d||e|d @end|ng |rom ed@@c@uk (HADFIELD Jarrod)
Date: Wed, 20 Mar 2019 20:05:18 +0000
Subject: [R-sig-ME] MCMCglmm error
In-Reply-To: <CAJtCY7VXuQ7CsO_rUzCRB1E8crtG1uNxRsj_3fgvw-kqC0Y9gw@mail.gmail.com>
References: <CAJtCY7Uv+n13xJhR8umzferErPmRLf5HMNPKcNrQ+dbNe=fDQQ@mail.gmail.com>
 <D59133B4-9B65-4064-A3FA-EA94632A7D44@ed.ac.uk>
 <CAJtCY7VXuQ7CsO_rUzCRB1E8crtG1uNxRsj_3fgvw-kqC0Y9gw@mail.gmail.com>
Message-ID: <80777F08-D2A9-40C7-BBE4-1942F461A577@ed.ac.uk>

Hi,

Yes. You make assumptions in doing this - that the individuals with unknown mothers aren?t really maternal siblings - but better than assuming they were virgin births.

Cheers,

Jarrod

On 20 Mar 2019, at 19:58, Walid Crampton-Mawass <walidmawass10 at gmail.com<mailto:walidmawass10 at gmail.com>> wrote:

Thank you Jarrod for your reply!

If I understand correctly then, to stop this error I should just give a dummy maternal id for the individuals with missing id.

Thank you again for your help

Walid Mawass






On Wed, Mar 20, 2019, 3:34 PM HADFIELD Jarrod <j.hadfield at ed.ac.uk<mailto:j.hadfield at ed.ac.uk>> wrote:
Hi,

MCMCglmm used to issue a warning if there were NA?s in the random effect predictors. However, people seemed to ignore it when it was inappropriate. The only case I can think of where it should be ignored is in a multi membership model where the number of members varies over observations, which is probably a rare type of model. Consequently, I now just stop people doing what I *think*they didn?t intend. It may be the case that you don?t know who the mother is, but that doesn?t mean they don?t have a mother. Fitting a dummy maternal id is the standard solution.

A more informative error message will be in the next release.

Cheers,

Jarrod

> On 20 Mar 2019, at 19:01, Walid Crampton-Mawass <walidmawass10 at gmail.com<mailto:walidmawass10 at gmail.com>> wrote:
>
> Hello everyone,
>
> I started getting this error when I run an animal model with a maternal
> random effect using MCMCglmm. here is my code:
>
> *prior1 <- list(R=list(V=1, nu=0.002), G=list(G1=list(V=1,
> nu=0.002),G2=list(V=1, nu=0.002)))*
>
> *model_afr_3_1 <- MCMCglmm(AFR~
> 1+OffMortality+COEFPAR+I(COEFPAR*COEFPAR)+TWIN+BIRTHYW+mortrate1 , random =
> ~animal + MOTHERW, rcov = ~units, data = IAC, pedigree = prunedPed, family
> = "gaussian" , nitt = 3500000, burnin = 500000, thin = 3000, prior =
> prior1, verbose = FALSE, pr=TRUE)*
> *Error in t(ZZ[[k]]) : invalid object passed to as_cholmod_sparse*
> *Calls: MCMCglmm -> buildZ -> t -> t*
>
> I was able to find out that the variable causing the error is the second
> random term "MOTHERW" which represents the id of each individual's mother
> as to model maternal variation. However I started getting this error
> recently even though it used to work beforehand without any errors. The
> variable is set as a multi-level factor just as is done to the "animal"
> variable. But there are a few missing values for some individuals (20 out
> of 572)
>
> *str(dat$MOTHERW)*
> * Factor w/ 297 levels "100007","100032",..: 114 142 207 12 258 168 261 230
> 179 107 ...*
>
> I tried another variable, FATHERW, which is the id of the father of the
> individual. I get the same error again : *Error in t(ZZ[[k]]) : invalid
> object passed to as_cholmod_sparse*
>
> However, when using another variable in my data as a random variable
> instead, BIRTHYW (birth year of individual; set as a factor), the model
> runs without any errors. The only difference is that there are no missing
> values in this variable compared MOTHERW  and FATHERW.
>
> is it probably the case that we can't use a variable with a few missing
> values as a random variable in MCMCglmm anymore?
>
> Thanks for any help!
> --
> Walid Mawass
> Ph.D. candidate in Cellular and Molecular Biology
> Population Genetics Laboratory
> University of Qu?bec at Trois-Rivi?res
> 3351, boul. des Forges, C.P. 500
> Trois-Rivi?res (Qu?bec) G9A 5H7
> Telephone: 819-376-5011 poste 3384
>
> [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

The University of Edinburgh is a charitable body, registered in Scotland, with registration number SC005336.


	[[alternative HTML version deleted]]


From w@||dm@w@@@10 @end|ng |rom gm@||@com  Wed Mar 20 21:45:29 2019
From: w@||dm@w@@@10 @end|ng |rom gm@||@com (Walid Crampton-Mawass)
Date: Wed, 20 Mar 2019 16:45:29 -0400
Subject: [R-sig-ME] MCMCglmm error
In-Reply-To: <80777F08-D2A9-40C7-BBE4-1942F461A577@ed.ac.uk>
References: <CAJtCY7Uv+n13xJhR8umzferErPmRLf5HMNPKcNrQ+dbNe=fDQQ@mail.gmail.com>
 <D59133B4-9B65-4064-A3FA-EA94632A7D44@ed.ac.uk>
 <CAJtCY7VXuQ7CsO_rUzCRB1E8crtG1uNxRsj_3fgvw-kqC0Y9gw@mail.gmail.com>
 <80777F08-D2A9-40C7-BBE4-1942F461A577@ed.ac.uk>
Message-ID: <CAJtCY7X3zvc6_kBKqZBec2m-FomPy24=wTtVVPObXUW9eccoSA@mail.gmail.com>

Definitely a better assumption.

Thank you again.

Walid

On Wed, Mar 20, 2019, 4:05 PM HADFIELD Jarrod <j.hadfield at ed.ac.uk> wrote:

> Hi,
>
> Yes. You make assumptions in doing this - that the individuals with
> unknown mothers aren?t really maternal siblings - but better than assuming
> they were virgin births.
>
> Cheers,
>
> Jarrod
>
> On 20 Mar 2019, at 19:58, Walid Crampton-Mawass <walidmawass10 at gmail.com>
> wrote:
>
> Thank you Jarrod for your reply!
>
> If I understand correctly then, to stop this error I should just give a
> dummy maternal id for the individuals with missing id.
>
> Thank you again for your help
>
> Walid Mawass
>
>
>
>
>
>
> On Wed, Mar 20, 2019, 3:34 PM HADFIELD Jarrod <j.hadfield at ed.ac.uk> wrote:
>
>> Hi,
>>
>> MCMCglmm used to issue a warning if there were NA?s in the random effect
>> predictors. However, people seemed to ignore it when it was inappropriate.
>> The only case I can think of where it should be ignored is in a multi
>> membership model where the number of members varies over observations,
>> which is probably a rare type of model. Consequently, I now just stop
>> people doing what I *think*they didn?t intend. It may be the case that you
>> don?t know who the mother is, but that doesn?t mean they don?t have a
>> mother. Fitting a dummy maternal id is the standard solution.
>>
>> A more informative error message will be in the next release.
>>
>> Cheers,
>>
>> Jarrod
>>
>> > On 20 Mar 2019, at 19:01, Walid Crampton-Mawass <
>> walidmawass10 at gmail.com> wrote:
>> >
>> > Hello everyone,
>> >
>> > I started getting this error when I run an animal model with a maternal
>> > random effect using MCMCglmm. here is my code:
>> >
>> > *prior1 <- list(R=list(V=1, nu=0.002), G=list(G1=list(V=1,
>> > nu=0.002),G2=list(V=1, nu=0.002)))*
>> >
>> > *model_afr_3_1 <- MCMCglmm(AFR~
>> > 1+OffMortality+COEFPAR+I(COEFPAR*COEFPAR)+TWIN+BIRTHYW+mortrate1 ,
>> random =
>> > ~animal + MOTHERW, rcov = ~units, data = IAC, pedigree = prunedPed,
>> family
>> > = "gaussian" , nitt = 3500000, burnin = 500000, thin = 3000, prior =
>> > prior1, verbose = FALSE, pr=TRUE)*
>> > *Error in t(ZZ[[k]]) : invalid object passed to as_cholmod_sparse*
>> > *Calls: MCMCglmm -> buildZ -> t -> t*
>> >
>> > I was able to find out that the variable causing the error is the second
>> > random term "MOTHERW" which represents the id of each individual's
>> mother
>> > as to model maternal variation. However I started getting this error
>> > recently even though it used to work beforehand without any errors. The
>> > variable is set as a multi-level factor just as is done to the "animal"
>> > variable. But there are a few missing values for some individuals (20
>> out
>> > of 572)
>> >
>> > *str(dat$MOTHERW)*
>> > * Factor w/ 297 levels "100007","100032",..: 114 142 207 12 258 168 261
>> 230
>> > 179 107 ...*
>> >
>> > I tried another variable, FATHERW, which is the id of the father of the
>> > individual. I get the same error again : *Error in t(ZZ[[k]]) : invalid
>> > object passed to as_cholmod_sparse*
>> >
>> > However, when using another variable in my data as a random variable
>> > instead, BIRTHYW (birth year of individual; set as a factor), the model
>> > runs without any errors. The only difference is that there are no
>> missing
>> > values in this variable compared MOTHERW  and FATHERW.
>> >
>> > is it probably the case that we can't use a variable with a few missing
>> > values as a random variable in MCMCglmm anymore?
>> >
>> > Thanks for any help!
>> > --
>> > Walid Mawass
>> > Ph.D. candidate in Cellular and Molecular Biology
>> > Population Genetics Laboratory
>> > University of Qu?bec at Trois-Rivi?res
>> > 3351, boul. des Forges, C.P. 500
>> > Trois-Rivi?res (Qu?bec) G9A 5H7
>> > Telephone: 819-376-5011 poste 3384
>> >
>> > [[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>> The University of Edinburgh is a charitable body, registered in Scotland,
>> with registration number SC005336.
>>
>
>

	[[alternative HTML version deleted]]


From @ouhey|@@ghebghoub @end|ng |rom gm@||@com  Wed Mar 20 22:46:01 2019
From: @ouhey|@@ghebghoub @end|ng |rom gm@||@com (Souheyla GHEBGHOUB)
Date: Wed, 20 Mar 2019 21:46:01 +0000
Subject: [R-sig-ME] Random effects in multinomial regression in R?
In-Reply-To: <d741bd3a-af48-adee-2b2e-4bdcef824865@mpi.nl>
References: <CAEA998ifF3J6fdjknpAZm_xpjTjTUrY1p5oRJqzyAGSGO0FV1w@mail.gmail.com>
 <d9ebad5b-3b52-0ce0-05d7-6731444874a6@mpi.nl>
 <CAEA998j+S7V=1EMmPNeOqtyOXwbBUfiv-PJgqgafaXDkrwB+1w@mail.gmail.com>
 <e7bc83c6-4478-32ba-a652-88ed606300fa@mpi.nl>
 <CAEA998hdxDeJr1cPSs8gQob-5J0yKk+f0DwDZTw4mfPWNqc79w@mail.gmail.com>
 <d741bd3a-af48-adee-2b2e-4bdcef824865@mpi.nl>
Message-ID: <CAEA998goObypPU7K5HmWGGHXvhKvvSJKU-dewqtDRw-QOZP4Vg@mail.gmail.com>

Dear Philip,

Thank you for the clarification. I agree.
So does your response intend that I should just do (1|Subject) ?

Thanks again
Souheyla

On Wed, 20 Mar 2019, 18:51 Alday, Phillip, <Phillip.Alday at mpi.nl> wrote:

> Please keep the list in CC.
>
> I really can't provide more advice about whether to do an intercept-only
> model or include the Pretest score in the random effects without knowing
> more about your data. If you have multiple pretest scores per subject and
> word, then it might make sense to include them in the random effects,
> *depending on your data and research question*. If you don't, then it
> definitely doesn't make sense to estimate a slope (i.e a rate) from a
> single static observation.
>
> Phillip
> On 20/3/19 7:33 pm, Souheyla GHEBGHOUB wrote:
>
> Hi again Phillip,
>
> My question is  :  I'd like to add random effects of *Subject* and *Word*,
> which may differ by time from pretest to posttest, but I don't have effect
> of *Time* , so I can't do:
>
> mod1 <- brm(Change ~ Pretest + Group + (Time|Subject) + (Time|Word))  So should I just do (1|Subject) + (1|Word))  or  (Pretest|Subject) + (Pretest|Word)) or exclude random effects?
>
> Thank you for looking into this :)
>
> Souheyla
>
>
>
>
>
> On Wed, 20 Mar 2019 at 18:28, Phillip Alday <phillip.alday at mpi.nl> wrote:
>
>> On 20/3/19 6:39 pm, Souheyla GHEBGHOUB wrote:
>> > Hi Philip,
>> >
>> > Thank you for the clarification. But I might have not make it clear in
>> > my question.
>> >
>> > I don't have Time in my data at all because I chose to predict change
>> > rather than having posttest and pretest responses as DV and Time as
>> > fixed effect.
>>
>> If Time isn't in your difference data, then it really makes no sense to
>> have it in your model anywhere ....
>>
>> > I chose this way because I have groups of subjects who were tested on
>> > words, and I was not too sure whether, a simple regression with
>> > Responses as DV and Time (Pretest/Posttest) as IV , will take into
>> > account differences between Pretest and Posttest at the level of each
>> > word. That is, I don't know whether it will sum the overall pretest
>> > score of each subject then compare it to its posttest, while I want it
>> > to compare each subject score of each word from pretest to posttest then
>> > base its analysis on these score changes.
>>
>> I don't want to be too harsh, but if you were unsure about that, then
>> that's the question you should have asked first. (See also the XY
>> problem, https://en.wikipedia.org/wiki/XY_problem)
>>
>> >
>> > That's why I did not want to risk it and chose /score change/ as the DV
>> > instead. But I was faced with another problem which is absence of Time
>> > effect by which subjects differ for my random slopes?
>>
>> Assuming you want to compute the difference outside of the model, then
>> you could (and I would argue should) still use the continuous/numeric
>> difference and not a categorical thresholding of that difference as your
>> dependent variable.
>>
>> In that case, I would argue that there can't be a "Time" effect by
>> subject because you are measuring the difference, which incorporates the
>> variance at each Time in the variance of the difference. Same for word.
>>
>> Depending on the exact structure of the test and whether there are
>> multiple pretest scores by subject or by word, you could potentially
>> include that as a random slope, but to make a more precise
>> recommendation there, we need to know more about your data.
>>
>> Best,
>> Phillip
>>
>>
>>
>>
>>
>> >
>> > Best,
>> > Souheyla
>> >
>> > On Wed, 20 Mar 2019 at 17:02, Phillip Alday <phillip.alday at mpi.nl
>> > <mailto:phillip.alday at mpi.nl>> wrote:
>> >
>> >     Generally speaking for the parameterization of mixed-effects models
>> in
>> >     lme4/brms/the usual packages, it doesn't make sense to have a
>> varying
>> >     slope (e.g. Time|Subject) without the corresponding fixed effect.
>> This
>> >     is because the varying slopes are calculated as offsets from the
>> group
>> >     mean, i.e from the fixed effect estimate. Not doing including the
>> fixed
>> >     effect is equivalent to assuming the group mean is zero, which is
>> >     usually not the assumption you want to make.
>> >
>> >     If you fit models with random slopes without the corresponding fixed
>> >     effects, then there are two main problems:
>> >
>> >     1. The corresponding variance parameter will be mis-estimated
>> because it
>> >     will be the average squared distance to zero and not the average
>> squared
>> >     distance to the mean (and average squared distance to the mean is
>> the
>> >     definition of variance).
>> >
>> >     2. The model may not converge because the numerics are set up under
>> the
>> >     "zero mean" assumption. For lme4/nlme, this is the case, but I
>> believe
>> >     that brms may do some internal reparameterization that may avoid
>> these
>> >     difficulties. (And a model fit with MCMC (brms) may not have the
>> same
>> >     numerical issues as a model fit with MLE (lme4)).
>> >
>> >     In brief: just add time as a fixed effect.
>> >
>> >     Also: why not fit your model as a continuous model with pre vs.
>> post as
>> >     a contrast in the model rather reducing a continuous variable to a
>> >     category? You can still apply a categorical distinction afterwards
>> if
>> >     you so desire, but in my experience, it's best to defer making
>> things
>> >     categorical until as late as possible (see also Frank Harrel's
>> comments
>> >     on prediction vs. classification:
>> >     http://www.fharrell.com/post/classification/). Moreover, it's a lot
>> >     easier to fit a continuous model than a multinomial one ....
>> >
>> >     Best,
>> >     Phillip
>> >
>> >     On 18/3/19 7:11 pm, Souheyla GHEBGHOUB wrote:
>> >     > I have *Change* from Pretest to Posttest (gain, no_gain, decline)
>> >     as the
>> >     > DV. Also *Pretest* and *Group* as covariates. This called for a
>> >     multinomial
>> >     > regression:
>> >     >
>> >     > mod0 <- brm(Change ~ Pretest + Group)
>> >     >
>> >     > *Question: *I'd like to add random effects of *Subject* and
>> >     *Word*, which
>> >     > may differ by time, but I don't have effect of *Time* to do:
>> >     >
>> >     > mod1 <- brm(Change ~ Pretest + Group + (Time|Subject) +
>> (Time|Word))
>> >     >
>> >     > So I thought of this:
>> >     >
>> >     > mod2 <- brm(Change ~ Pretest + Group + (1|Subject) + (1|Word))
>> >     >
>> >     > but this also seems wrong to me. What do you think is the best way
>> >     to treat
>> >     > random effects in this situation, please?
>> >     >
>> >     > Thank you
>> >     >
>> >     > Souheyla Ghebghoub
>> >     >
>> >     >       [[alternative HTML version deleted]]
>> >     >
>> >     > _______________________________________________
>> >     > R-sig-mixed-models at r-project.org
>> >     <mailto:R-sig-mixed-models at r-project.org> mailing list
>> >     > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >     >
>> >
>>
>

	[[alternative HTML version deleted]]


From b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com  Wed Mar 20 23:18:45 2019
From: b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com (Alessandra Bielli)
Date: Wed, 20 Mar 2019 22:18:45 +0000
Subject: [R-sig-ME] Fixing singularity in a generalized linear mixed effect
 model
Message-ID: <CA+6N3yUq1s=XfHo6-GS26cc_doGLMQzjBqVhCUo63Szg=xUmfg@mail.gmail.com>

Dear List

I am fitting this model using the lme4 package, in order to obtain catch
estimates using the predict function

m1 <- glmer(Count ~ CE + offset(log(Effort)) + (1|SetYear) +(1|Season) +
                  (1|Lance.N) + (1|Boat.Name) + (1|Observer.Name), data =
Data,                     glmerControl(optimizer = "bobyqa"), family=
"poisson")


where: CE is a categorical (control or treatment), Effort is numerical
(fishing effort), and all the other variables are random effects.

*My problem is that I get a warning message saying that the model is
singular*

*>summary(m1)*

Generalized linear mixed model fit by maximum likelihood (Laplace
Approximation) [glmerMod]
 Family: poisson  ( log )
Formula: Count ~ CE + offset(log(Effort)) + (1 | SetYear) + (1 |
    Season) + (1 | Lance.N) + (1 | Boat.Name) + (1 | Observer.Name)
   Data: Data
Control: glmerControl(optimizer = "bobyqa")

     AIC      BIC   logLik deviance df.resid
   148.6    174.3    -67.3    134.6      285

Scaled residuals:
    Min      1Q  Median      3Q     Max
-0.4852 -0.1758 -0.1339 -0.1227  3.5980

Random effects:
 Groups        Name        Variance  Std.Dev.
 Lance.N       (Intercept) 2.259e+00 1.503e+00
 Boat.Name     (Intercept) 0.000e+00 0.000e+00
 Observer.Name (Intercept) 0.000e+00 0.000e+00
 Season        (Intercept) 4.149e-17 6.442e-09
 SetYear       (Intercept) 0.000e+00 0.000e+00
Number of obs: 292, groups:
Lance.N, 146; Boat.Name, 21; Observer.Name, 5; Season, 4; SetYear, 4

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)  -2.5751     0.6612  -3.895 9.83e-05 ***
CEE          -0.5878     0.5003  -1.175     0.24
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Correlation of Fixed Effects:
    (Intr)
CEE -0.257
*convergence code: 0*
*singular fit*

I am aware that there are a lot of random effects and some of them have a
number of levels <5. However, this study was carried out under real fishery
conditions, so these random effects seemed all important to me.

I removed the random effects with variance zero as suggested here
https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#singular-models-random-effect-variances-estimated-as-zero-or-correlations-estimated-as---1
until I removed them all and found myself with a glm instead.

My questions are

- why the variance of Lance.N, initially positive, becomes zero after I
remove the other random effects that had variance equal zero?
- is it acceptable to fit a glm just because all the random effect
variances were zero?

I hope I gave all the information you need.

Thanks for any advice!

Alessandra

	[[alternative HTML version deleted]]


From ph||||p@@|d@y @end|ng |rom mp|@n|  Thu Mar 21 12:52:16 2019
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Thu, 21 Mar 2019 12:52:16 +0100
Subject: [R-sig-ME] Random effects in multinomial regression in R?
In-Reply-To: <CAEA998goObypPU7K5HmWGGHXvhKvvSJKU-dewqtDRw-QOZP4Vg@mail.gmail.com>
References: <CAEA998ifF3J6fdjknpAZm_xpjTjTUrY1p5oRJqzyAGSGO0FV1w@mail.gmail.com>
 <d9ebad5b-3b52-0ce0-05d7-6731444874a6@mpi.nl>
 <CAEA998j+S7V=1EMmPNeOqtyOXwbBUfiv-PJgqgafaXDkrwB+1w@mail.gmail.com>
 <e7bc83c6-4478-32ba-a652-88ed606300fa@mpi.nl>
 <CAEA998hdxDeJr1cPSs8gQob-5J0yKk+f0DwDZTw4mfPWNqc79w@mail.gmail.com>
 <d741bd3a-af48-adee-2b2e-4bdcef824865@mpi.nl>
 <CAEA998goObypPU7K5HmWGGHXvhKvvSJKU-dewqtDRw-QOZP4Vg@mail.gmail.com>
Message-ID: <7d0ac7d1-b095-63b2-3bda-fec23d6065eb@mpi.nl>

I honestly don't know because I don't know enough the structure of your
data.

Phillip

On 20/3/19 10:46 pm, Souheyla GHEBGHOUB wrote:
> Dear Philip,
> 
> Thank you for the clarification. I agree.?
> So does your response intend that I should just do (1|Subject) ?
> 
> Thanks again
> Souheyla
> 
> On Wed, 20 Mar 2019, 18:51 Alday, Phillip, <Phillip.Alday at mpi.nl
> <mailto:Phillip.Alday at mpi.nl>> wrote:
> 
>     Please keep the list in CC.
> 
>     I really can't provide more advice about whether to do an
>     intercept-only model or include the Pretest score in the random
>     effects without knowing more about your data. If you have multiple
>     pretest scores per subject and word, then it might make sense to
>     include them in the random effects, *depending on your data and
>     research question*. If you don't, then it definitely doesn't make
>     sense to estimate a slope (i.e a rate) from a single static
>     observation.
> 
>     Phillip
> 
>     On 20/3/19 7:33 pm, Souheyla GHEBGHOUB wrote:
>>     Hi again Phillip,?
>>
>>     My question is? :??I'd like to add random effects
>>     of?*/Subject/*?and?*/Word/*, which may differ by time from pretest
>>     to posttest, but I don't have effect of?*/Time/*?, so I can't do:
>>     |mod1 <-brm(Change~Pretest+Group+(Time|Subject)+(Time|Word)) |So should I just do (1|Subject)+(1|Word)) or
>>     (Pretest|Subject)+(Pretest|Word)) or exclude random effects?
>>     Thank you for looking into this :)
>>     Souheyla
>>     ||
>>
>>     On Wed, 20 Mar 2019 at 18:28, Phillip Alday <phillip.alday at mpi.nl
>>     <mailto:phillip.alday at mpi.nl>> wrote:
>>
>>         On 20/3/19 6:39 pm, Souheyla GHEBGHOUB wrote:
>>         > Hi Philip,?
>>         >
>>         > Thank you for the clarification. But I might have not make
>>         it clear in
>>         > my question.
>>         >
>>         > I don't have Time in my data at all because I chose to
>>         predict change
>>         > rather than having posttest and pretest responses as DV and
>>         Time as
>>         > fixed effect.
>>
>>         If Time isn't in your difference data, then it really makes no
>>         sense to
>>         have it in your model anywhere ....
>>
>>         > I chose this way because I have groups of subjects who were
>>         tested on
>>         > words, and I was not too sure whether, a simple regression with
>>         > Responses as DV and Time (Pretest/Posttest) as IV , will
>>         take into
>>         > account differences between Pretest and Posttest at the
>>         level of each
>>         > word. That is,?I don't know whether it will sum the overall
>>         pretest
>>         > score of each subject then compare it to its posttest, while
>>         I want it
>>         > to?compare each subject score of each word from pretest to
>>         posttest then
>>         > base its analysis on these score changes.
>>
>>         I don't want to be too harsh, but if you were unsure about
>>         that, then
>>         that's the question you should have asked first. (See also the XY
>>         problem, https://en.wikipedia.org/wiki/XY_problem)
>>
>>         >
>>         > That's why I did not want to risk it and chose /score
>>         change/?as the DV
>>         > instead. But I was faced with another problem which is
>>         absence?of Time
>>         > effect by which subjects differ for my random slopes?
>>
>>         Assuming you want to compute the difference outside of the
>>         model, then
>>         you could (and I would argue should) still use the
>>         continuous/numeric
>>         difference and not a categorical thresholding of that
>>         difference as your
>>         dependent variable.
>>
>>         In that case, I would argue that there can't be a "Time" effect by
>>         subject because you are measuring the difference, which
>>         incorporates the
>>         variance at each Time in the variance of the difference. Same
>>         for word.
>>
>>         Depending on the exact structure of the test and whether there are
>>         multiple pretest scores by subject or by word, you could
>>         potentially
>>         include that as a random slope, but to make a more precise
>>         recommendation there, we need to know more about your data.
>>
>>         Best,
>>         Phillip
>>
>>
>>
>>
>>
>>         >
>>         > Best,
>>         > Souheyla?
>>         >
>>         > On Wed, 20 Mar 2019 at 17:02, Phillip Alday
>>         <phillip.alday at mpi.nl <mailto:phillip.alday at mpi.nl>
>>         > <mailto:phillip.alday at mpi.nl <mailto:phillip.alday at mpi.nl>>>
>>         wrote:
>>         >
>>         >? ? ?Generally speaking for the parameterization of
>>         mixed-effects models in
>>         >? ? ?lme4/brms/the usual packages, it doesn't make sense to
>>         have a varying
>>         >? ? ?slope (e.g. Time|Subject) without the corresponding
>>         fixed effect. This
>>         >? ? ?is because the varying slopes are calculated as offsets
>>         from the group
>>         >? ? ?mean, i.e from the fixed effect estimate. Not doing
>>         including the fixed
>>         >? ? ?effect is equivalent to assuming the group mean is zero,
>>         which is
>>         >? ? ?usually not the assumption you want to make.
>>         >
>>         >? ? ?If you fit models with random slopes without the
>>         corresponding fixed
>>         >? ? ?effects, then there are two main problems:
>>         >
>>         >? ? ?1. The corresponding variance parameter will be
>>         mis-estimated because it
>>         >? ? ?will be the average squared distance to zero and not the
>>         average squared
>>         >? ? ?distance to the mean (and average squared distance to
>>         the mean is the
>>         >? ? ?definition of variance).
>>         >
>>         >? ? ?2. The model may not converge because the numerics are
>>         set up under the
>>         >? ? ?"zero mean" assumption. For lme4/nlme, this is the case,
>>         but I believe
>>         >? ? ?that brms may do some internal reparameterization that
>>         may avoid these
>>         >? ? ?difficulties. (And a model fit with MCMC (brms) may not
>>         have the same
>>         >? ? ?numerical issues as a model fit with MLE (lme4)).
>>         >
>>         >? ? ?In brief: just add time as a fixed effect.
>>         >
>>         >? ? ?Also: why not fit your model as a continuous model with
>>         pre vs. post as
>>         >? ? ?a contrast in the model rather reducing a continuous
>>         variable to a
>>         >? ? ?category? You can still apply a categorical distinction
>>         afterwards if
>>         >? ? ?you so desire, but in my experience, it's best to defer
>>         making things
>>         >? ? ?categorical until as late as possible (see also Frank
>>         Harrel's comments
>>         >? ? ?on prediction vs. classification:
>>         >? ? ?http://www.fharrell.com/post/classification/). Moreover,
>>         it's a lot
>>         >? ? ?easier to fit a continuous model than a multinomial one ....
>>         >
>>         >? ? ?Best,
>>         >? ? ?Phillip
>>         >
>>         >? ? ?On 18/3/19 7:11 pm, Souheyla GHEBGHOUB wrote:
>>         >? ? ?> I have *Change* from Pretest to Posttest (gain,
>>         no_gain, decline)
>>         >? ? ?as the
>>         >? ? ?> DV. Also *Pretest* and *Group* as covariates. This
>>         called for a
>>         >? ? ?multinomial
>>         >? ? ?> regression:
>>         >? ? ?>
>>         >? ? ?> mod0 <- brm(Change ~ Pretest + Group)
>>         >? ? ?>
>>         >? ? ?> *Question: *I'd like to add random effects of
>>         *Subject* and
>>         >? ? ?*Word*, which
>>         >? ? ?> may differ by time, but I don't have effect of *Time*
>>         to do:
>>         >? ? ?>
>>         >? ? ?> mod1 <- brm(Change ~ Pretest + Group + (Time|Subject)
>>         + (Time|Word))
>>         >? ? ?>
>>         >? ? ?> So I thought of this:
>>         >? ? ?>
>>         >? ? ?> mod2 <- brm(Change ~ Pretest + Group + (1|Subject) +
>>         (1|Word))
>>         >? ? ?>
>>         >? ? ?> but this also seems wrong to me. What do you think is
>>         the best way
>>         >? ? ?to treat
>>         >? ? ?> random effects in this situation, please?
>>         >? ? ?>
>>         >? ? ?> Thank you
>>         >? ? ?>
>>         >? ? ?> Souheyla Ghebghoub
>>         >? ? ?>
>>         >? ? ?>? ? ? ?[[alternative HTML version deleted]]
>>         >? ? ?>
>>         >? ? ?> _______________________________________________
>>         >? ? ?> R-sig-mixed-models at r-project.org
>>         <mailto:R-sig-mixed-models at r-project.org>
>>         >? ? ?<mailto:R-sig-mixed-models at r-project.org
>>         <mailto:R-sig-mixed-models at r-project.org>> mailing list
>>         >? ? ?> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>         >? ? ?>
>>         >
>>


From @ouhey|@@ghebghoub @end|ng |rom gm@||@com  Thu Mar 21 14:00:08 2019
From: @ouhey|@@ghebghoub @end|ng |rom gm@||@com (Souheyla GHEBGHOUB)
Date: Thu, 21 Mar 2019 13:00:08 +0000
Subject: [R-sig-ME] Random effects in multinomial regression in R?
In-Reply-To: <7d0ac7d1-b095-63b2-3bda-fec23d6065eb@mpi.nl>
References: <CAEA998ifF3J6fdjknpAZm_xpjTjTUrY1p5oRJqzyAGSGO0FV1w@mail.gmail.com>
 <d9ebad5b-3b52-0ce0-05d7-6731444874a6@mpi.nl>
 <CAEA998j+S7V=1EMmPNeOqtyOXwbBUfiv-PJgqgafaXDkrwB+1w@mail.gmail.com>
 <e7bc83c6-4478-32ba-a652-88ed606300fa@mpi.nl>
 <CAEA998hdxDeJr1cPSs8gQob-5J0yKk+f0DwDZTw4mfPWNqc79w@mail.gmail.com>
 <d741bd3a-af48-adee-2b2e-4bdcef824865@mpi.nl>
 <CAEA998goObypPU7K5HmWGGHXvhKvvSJKU-dewqtDRw-QOZP4Vg@mail.gmail.com>
 <7d0ac7d1-b095-63b2-3bda-fec23d6065eb@mpi.nl>
Message-ID: <CAEA998iF59CEzE8wBCfKFD65zoTCg=YER9eqt-mJuioR24ot=w@mail.gmail.com>

Dear Philip,

I understand , here is the structure of my data in case it could help.

I have 3 groups of participants (control, treatment1, treatment2). Each
group was tested twice, once before treatment (pretest) and once after
treatment (posttest).
In each test, they were tested on knowledge of 28 words, scores are
dichotomous (0 = unknown , 1 = known). Tests are the same.

I calculated change from pretest to posttest :
if pretest 0 and posttest 0 = no gain
if pretest 1 and posttest 1 = no gain
if pretest 0 and posttest 1 = gain
if pretest 1 and posttest 0 = decline
So I ended up with a dependent variable called Change with 3 levels
(no_gain, gain, decline) and I tried to predict it using Group and Pretest
as covariates using multinomial logit model. mod0 <- brm(Change ~ Pretest +
Group) I would like to add random effects for subjects but don't know
what's the best form when Time factor is absent.

I hope other statisticians who read this could help
Thank you
Souheyla

	[[alternative HTML version deleted]]


From b@ud-bovy@g@br|e| @end|ng |rom h@r@|t  Thu Mar 21 14:06:48 2019
From: b@ud-bovy@g@br|e| @end|ng |rom h@r@|t (Baud-Bovy Gabriel)
Date: Thu, 21 Mar 2019 13:06:48 +0000
Subject: [R-sig-ME] Fitting RT: underdispersion with gamma and identity link
In-Reply-To: <c046fa4b-0ebe-7150-2b5b-2195ca501afb@iit.it>
References: <c046fa4b-0ebe-7150-2b5b-2195ca501afb@iit.it>
Message-ID: <b19e6662-dda7-8a14-87c8-facc07dde037@hsr.it>

Dear all,

I have updated a one-month old post on stakexchange about a GLMM model that I used to fit RT with gamma
distribution.

https://stats.stackexchange.com/questions/391076/how-to-interpret-significant-factors-in-a-glmm-gamma-model-that-appears-to-be-go

I mention this question in the mailing list because there are still things that I don't understand.  I have put some info at the end of this email it is probably best to answer me on stakeexchange, where it is also possible to see the plots
(otherwise, I'll cross post the answers).

- The DHARMa residual plot suggests underdispersion and I don't know how to deal with that.

- The results of the model don't make sense. For example, all fixed effect are statistically significant
   despite the fact that these factors explain little to nothing when looking at the plots. The random
   effects also appear to be  needed (statistically significant LRTs) and supported by the data  (rePCA)
   although I double that it is the case. Note that the model fits without warning.

-  I don't understand why the results are statistically significant. Intuitively, I would expect that
   estimates to be not statistically significant if there is a lot of unexplained variability.

-   Even if gamma distribution is not a perfect model of RT variability (because of underdispersion),
    it is a better model than Gaussian noise. I would expect therefore that the result of the model
     would be more trustworthy with gamma noise than with gaussian noise.

-  I also don't understand the value of residual SD, which seems to be on a different scale
   that I  would expect.

- As I have mentioned in a previous question to the list, my general goal is to be able to fit RT
  with identity link and welcome other suggestion but I would also like to understand what
  is happening in this case.

  If anybody is interested, I might share the
data privately. Thank you for any help.

Gabriel

>fitRespLat11 <- glmer(resp.lat ~ stake.i.c + dir.c + win.prob.c +
  (stake.i.c + dir.c + win.prob.c || su),
  data=tmp, family = Gamma(link = "identity"),
  control=glmerControl(optimizer =  "bobyqa"))

> summary(fitRespLat11)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: Gamma  ( identity )
Formula: resp.lat ~ stake.i.c + dir.c + win.prob.c + (stake.i.c + dir.c + win.prob.c || su)
   Data: tmp
Control: glmerControl(optimizer = "bobyqa")

     AIC      BIC   logLik deviance df.resid
 58026.3  58083.4 -29004.2  58008.3     4160

Scaled residuals:
    Min      1Q  Median      3Q     Max
-1.7918 -0.6890 -0.2162  0.4502  4.8900

Random effects:
 Groups   Name        Variance  Std.Dev.
 su       (Intercept) 5.413e+03  73.576
 su.1     stake.i.c   2.008e+03  44.806
 su.2     dir.c       2.944e+03  54.260
 su.3     win.prob.c  7.514e+04 274.108
 Residual             2.172e-01   0.466
Number of obs: 4169, groups:  su, 120

Fixed effects:
            Estimate Std. Error t value Pr(>|z|)
(Intercept)  612.589      6.290  97.390  < 2e-16 ***
stake.i.c    -61.037      6.001 -10.171  < 2e-16 ***
dir.c        -33.530      5.481  -6.117 9.52e-10 ***
win.prob.c    34.071      8.262   4.124 3.72e-05 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Correlation of Fixed Effects:
           (Intr) stk..c dir.c
stake.i.c  -0.089
dir.c      -0.014  0.126
win.prob.c  0.191 -0.226 -0.108

> isSingular(fitRespLat11)
[1] FALSE

# Random structure checks

> rePCA(fitRespLat11)
$su
Standard deviations (1, .., p=4):
[1] 588.19493 157.88241 116.43287  96.14736

Rotation (n x k) = (4 x 4):
     [,1] [,2] [,3] [,4]
[1,]    0    1    0    0
[2,]    0    0    0    1
[3,]    0    0    1    0
[4,]    1    0    0    0

attr(,"class")
[1] "prcomplist"


> anova(fitRespLat11,fitRespLat12,fitRespLat13,fitRespLat14)
Data: tmp
Models:
fitRespLat14: resp.lat ~ stake.i.c + dir.c + win.prob.c + (1 | su)
fitRespLat13: resp.lat ~ stake.i.c + dir.c + win.prob.c + (stake.i.c || su)
fitRespLat12: resp.lat ~ stake.i.c + dir.c + win.prob.c + (stake.i.c + dir.c || su)
fitRespLat11: resp.lat ~ stake.i.c + dir.c + win.prob.c + (stake.i.c + dir.c + win.prob.c || su)
             Df   AIC   BIC logLik deviance  Chisq Chi Df Pr(>Chisq)
fitRespLat14  6 58203 58241 -29096    58191
fitRespLat13  7 58129 58173 -29057    58115 76.699      1  < 2.2e-16 ***
fitRespLat12  8 58056 58107 -29020    58040 74.820      1  < 2.2e-16 ***
fitRespLat11  9 58026 58083 -29004    58008 31.496      1  1.999e-08 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

# See stackexchange link for plots and DHARMa residuals


--
#-----------------------------------------------------------------------
Gabriel Baud-Bovy               web: www.iit.it/people/gabriel-baud-bovy<http://www.iit.it/people/gabriel-baud-bovy>
Italian Institute of Technology          email: gabriel.baud-bovy at iit.it<mailto:gabriel.baud-bovy at iit.it>
Central office: via Morego 30, 16163 Genova     tel.: (+39) 010 8172 202
Erzelli office: via Melen 73, 16152 Genova    mobile: (+39) 348 172 4045
#-----------------------------------------------------------------------


Rispetta l?ambiente: non stampare questa mail se non ? necessario.
Respect the environment: print this email only if necessary.

	[[alternative HTML version deleted]]


From b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com  Thu Mar 21 21:43:07 2019
From: b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com (Alessandra Bielli)
Date: Thu, 21 Mar 2019 20:43:07 +0000
Subject: [R-sig-ME] Fwd: Fixing singularity in a generalized linear mixed
 effect model
In-Reply-To: <CA+6N3yUq1s=XfHo6-GS26cc_doGLMQzjBqVhCUo63Szg=xUmfg@mail.gmail.com>
References: <CA+6N3yUq1s=XfHo6-GS26cc_doGLMQzjBqVhCUo63Szg=xUmfg@mail.gmail.com>
Message-ID: <CA+6N3yWgRpDAkf5Tt3vjxq7Uii6JA4BjSVpr5+sBLspCjtWMMg@mail.gmail.com>

Hi All

Sorry if I am reposting this, I am not sure it got sent to the list...

---------- Forwarded message ---------
From: Alessandra Bielli <bielli.alessandra at gmail.com>
Date: Wed, Mar 20, 2019 at 10:18 PM
Subject: Fixing singularity in a generalized linear mixed effect model
To: <r-sig-mixed-models at r-project.org>


Dear List

I am fitting this model using the lme4 package, in order to obtain catch
estimates using the predict function

m1 <- glmer(Count ~ CE + offset(log(Effort)) + (1|SetYear) +(1|Season) +
                  (1|Lance.N) + (1|Boat.Name) + (1|Observer.Name), data =
Data,                     glmerControl(optimizer = "bobyqa"), family=
"poisson")


where: CE is a categorical (control or treatment), Effort is numerical
(fishing effort), and all the other variables are random effects.

*My problem is that I get a warning message saying that the model is
singular*

*>summary(m1)*

Generalized linear mixed model fit by maximum likelihood (Laplace
Approximation) [glmerMod]
 Family: poisson  ( log )
Formula: Count ~ CE + offset(log(Effort)) + (1 | SetYear) + (1 |
    Season) + (1 | Lance.N) + (1 | Boat.Name) + (1 | Observer.Name)
   Data: Data
Control: glmerControl(optimizer = "bobyqa")

     AIC      BIC   logLik deviance df.resid
   148.6    174.3    -67.3    134.6      285

Scaled residuals:
    Min      1Q  Median      3Q     Max
-0.4852 -0.1758 -0.1339 -0.1227  3.5980

Random effects:
 Groups        Name        Variance  Std.Dev.
 Lance.N       (Intercept) 2.259e+00 1.503e+00
 Boat.Name     (Intercept) 0.000e+00 0.000e+00
 Observer.Name (Intercept) 0.000e+00 0.000e+00
 Season        (Intercept) 4.149e-17 6.442e-09
 SetYear       (Intercept) 0.000e+00 0.000e+00
Number of obs: 292, groups:
Lance.N, 146; Boat.Name, 21; Observer.Name, 5; Season, 4; SetYear, 4

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)  -2.5751     0.6612  -3.895 9.83e-05 ***
CEE          -0.5878     0.5003  -1.175     0.24
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Correlation of Fixed Effects:
    (Intr)
CEE -0.257
*convergence code: 0*
*singular fit*

I am aware that there are a lot of random effects and some of them have a
number of levels <5. However, this study was carried out under real fishery
conditions, so these random effects seemed all important to me.

I removed the random effects with variance zero as suggested here
https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#singular-models-random-effect-variances-estimated-as-zero-or-correlations-estimated-as---1
until I removed them all and found myself with a glm instead.

My questions are

- why the variance of Lance.N, initially positive, becomes zero after I
remove the other random effects that had variance equal zero?
- is it acceptable to fit a glm just because all the random effect
variances were zero?

I hope I gave all the information you need.

Thanks for any advice!

Alessandra

	[[alternative HTML version deleted]]


From b|mono@om @end|ng |rom gm@||@com  Fri Mar 22 14:42:46 2019
From: b|mono@om @end|ng |rom gm@||@com (=?UTF-8?B?UmVuw6k=?=)
Date: Fri, 22 Mar 2019 14:42:46 +0100
Subject: [R-sig-ME] Random effects in multinomial regression in R?
In-Reply-To: <CAEA998iF59CEzE8wBCfKFD65zoTCg=YER9eqt-mJuioR24ot=w@mail.gmail.com>
References: <CAEA998ifF3J6fdjknpAZm_xpjTjTUrY1p5oRJqzyAGSGO0FV1w@mail.gmail.com>
 <d9ebad5b-3b52-0ce0-05d7-6731444874a6@mpi.nl>
 <CAEA998j+S7V=1EMmPNeOqtyOXwbBUfiv-PJgqgafaXDkrwB+1w@mail.gmail.com>
 <e7bc83c6-4478-32ba-a652-88ed606300fa@mpi.nl>
 <CAEA998hdxDeJr1cPSs8gQob-5J0yKk+f0DwDZTw4mfPWNqc79w@mail.gmail.com>
 <d741bd3a-af48-adee-2b2e-4bdcef824865@mpi.nl>
 <CAEA998goObypPU7K5HmWGGHXvhKvvSJKU-dewqtDRw-QOZP4Vg@mail.gmail.com>
 <7d0ac7d1-b095-63b2-3bda-fec23d6065eb@mpi.nl>
 <CAEA998iF59CEzE8wBCfKFD65zoTCg=YER9eqt-mJuioR24ot=w@mail.gmail.com>
Message-ID: <CADcpBHNC5BJp9HmB-p-0xAuoC1yrhFTMPwM3ZXBCcH_2jRAJgA@mail.gmail.com>

Hi Souheyla,

it seems to me that you will run into problems with your coding of change
(gain, no gain and decline) because the 'change' is by
definition/calculation depending on the predictor pretest.
See, according to your coding scheme:
Change = decline can only occur if pretest=1 (not by pretest=0).
Change = gain can only occur if pretest = 0 (not by pretest=1)
Change = No Gain can occur if pretest= 1 or 0
In other words:
If pretest = 1 then the possible outcomes can be decline or no gain
If pretest = 0 then the possible outcomes can be gain or no gain

And if the model result shows you then that the pre-test is significantly
related to p(change-outcome), I guess there is no surprise in it, is it?

So the first solution to this would be simply kicking the pre-test
predictor out of the model completely, and predict:
mod1 <- brm(Change ~ Group + (1|Subject) + (1+Group|Word),...)
(Btw.: actually the first Hierarchical Bayes Model question I see on the
mixed-effects mailing list :))

Attempt for a further clarification on which random slopes would reflect
the model's design:
If you have a within-subjects design, by-subject random slopes are possible
for the within-subject variable (e.g. if there are two sets of words/lists
[e.g. abstract vs. concrete words] for each participant, and you test
whether there is a performance-difference between these word-lists, then
you can implement by-subject random slopes for words, because each
participant has seen both sets.) If each participant has seen only one list
(i.e. between subjects design) by subject random slopes for words are not
appropriate, because there is no 'slope' by participant (i.e. by
definition, having a slope requires at least two observations...). This is
always a good rule of thumb without thinking about it too heavily :)
Ans as you see: you can define a random slope for words:  (1+Group|Word),
because each word has been presented in each group (i.e. there can be a
slope for each word). And intuitively speaking the Treatment-effect can
vary depending on the stimuli you use, and the slope makes sense. (You also
see in this example that the treatment effect can also vary by subjects,
but in fact, this subject effect variation IS EQUAL to the effect you want
to test, and having by subject group random slopes would eliminate the
fixed effect...)

Anyway, there is a second possibility to define your model, depending on
how you want to interpret it. In the previous model you can say something
about the type-of-change likelihoods depending on the treatment group. But
you could implement the model as binomial as well (i.e. logistic regression)

mod2 <- brm(posttest ~ pretest*Group + (1|Subject) + (1+Group|Word),...)

And what you would expect here would be an interaction between pre-test and
Group. For instance; if pretest=0 & treatment 1 then posttest larger than
with pretest=0 & treatment 2; but not when pretest=1 (because this is a
plausible no gain situation). And so on...
(And in this model there are no also no further random slopes hidden in
your design :))
Hope this helps.

Best, Ren?


Am Do., 21. M?rz 2019 um 14:01 Uhr schrieb Souheyla GHEBGHOUB <
souheyla.ghebghoub at gmail.com>:

> Dear Philip,
>
> I understand , here is the structure of my data in case it could help.
>
> I have 3 groups of participants (control, treatment1, treatment2). Each
> group was tested twice, once before treatment (pretest) and once after
> treatment (posttest).
> In each test, they were tested on knowledge of 28 words, scores are
> dichotomous (0 = unknown , 1 = known). Tests are the same.
>
> I calculated change from pretest to posttest :
> if pretest 0 and posttest 0 = no gain
> if pretest 1 and posttest 1 = no gain
> if pretest 0 and posttest 1 = gain
> if pretest 1 and posttest 0 = decline
> So I ended up with a dependent variable called Change with 3 levels
> (no_gain, gain, decline) and I tried to predict it using Group and Pretest
> as covariates using multinomial logit model. mod0 <- brm(Change ~ Pretest +
> Group) I would like to add random effects for subjects but don't know
> what's the best form when Time factor is absent.
>
> I hope other statisticians who read this could help
> Thank you
> Souheyla
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Fri Mar 22 15:29:08 2019
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Fri, 22 Mar 2019 15:29:08 +0100
Subject: [R-sig-ME] Fwd: glm.nb convergence issues
In-Reply-To: <CAE10PCyfie21gDQHw9EWfWKFz66JAVqSU9HHBYzEJfx18sH09g@mail.gmail.com>
References: <BN8PR09MB3554300E08C4B76B4E25FC9587410@BN8PR09MB3554.namprd09.prod.outlook.com>
 <CAE10PCzNa4V0Hr03K4TwOeACavzFZUHoRo4Vrj_+VUDTueVCCA@mail.gmail.com>
 <CAJuCY5w74vVuATprurgR86zByRSbvztvN92a2m8e6ENjTG-+Jw@mail.gmail.com>
 <CAE10PCyfie21gDQHw9EWfWKFz66JAVqSU9HHBYzEJfx18sH09g@mail.gmail.com>
Message-ID: <CAJuCY5z=ENvt1ztASU2wa=BN2L3QayCTDPQ8s3-pu42OA_Sf+g@mail.gmail.com>

Dear Matthew,

The fixed effect of your model suggests following EDA plot: ggplot(ST,
aes(x = spr, y = wait_n / wait_d)) + geom_point()

Based on this plot, I doubt if the negative binomial distribution makes
sense for your data. The problem is IMHO rather conceptual (what model
makes sense) than technical (false convergence). I would strongly recommend
to consult a local statistician.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op wo 20 mrt. 2019 om 20:50 schreef Matthew Boden <matthew.t.boden at gmail.com
>:

> Thank you, Thierry.  A csv file is attached; hopefully, that works.
>
> The numerator divided by the denominator will always fall between 0 and 1
> for these data, and is typically very close to 1.
> The predictor variable ranges from about 3 to 18, but even when scaled,
> the model won't converge.
>
> Matt
>
>
>
>
>
>
>
>
>
>
>
>
> On Wed, Mar 20, 2019 at 1:48 AM Thierry Onkelinx <thierry.onkelinx at inbo.be>
> wrote:
>
>> Dear Matthew,
>>
>> The mailing list accepts only a limited number of file formats as
>> attachment. Your data got stripped. Can you resend the data or send a link
>> to the data?
>>
>> The offset requires the log because the model is using the log link. Your
>> model fits log(E(wait_n)) = offset(log(wait_d)) + covariates which you can
>> rewrite as log(E(wait_n)) - offset(log(wait_d)) = covariates or
>> log(E(wait_n) / wait_d) = covariates. Pick a relevant magnitude of wait_d
>> so that the magnitude of wait_n/wait_d is somewhat near 1. E.g. we don't
>> express the number of inhabitats per m? but rather per km?
>>
>> Best regards,
>>
>> ir. Thierry Onkelinx
>> Statisticus / Statistician
>>
>> Vlaamse Overheid / Government of Flanders
>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>> AND FOREST
>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>> thierry.onkelinx at inbo.be
>> Havenlaan 88 bus 73, 1000 Brussel
>> www.inbo.be
>>
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of data.
>> ~ John Tukey
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>>
>> <https://www.inbo.be>
>>
>>
>> Op wo 20 mrt. 2019 om 06:43 schreef Matthew Boden <
>> matthew.t.boden at gmail.com>:
>>
>>> Hello,
>>>
>>>
>>>
>>> I?m working my way through convergence issues related to a negative
>>> binomial mixed model with an offset and random intercept. Data attached.
>>>
>>>
>>>
>>> A3 <- glmer.nb(wait_n ~ offset(log(wait_d)) +  scale(spr) + (1 | check),
>>> data = ST)
>>>
>>> summary(A3)
>>>
>>>
>>>
>>> #Model failed to converge with max|grad| = 0.0031459 (tol = 0.001,
>>> component 1)
>>>
>>> #Model is nearly unidentifiable: very large eigenvalue
>>>
>>> # - Rescale variables?
>>>
>>>
>>>
>>> I have two questions.
>>>
>>>
>>>
>>> 1) Outcome variable (wait_n) values are much larger than predictor (spr)
>>> values.
>>>
>>>
>>>
>>> Mean wait_n = 11,783
>>>
>>> Mean spr = 7.34
>>>
>>>
>>>
>>> Would transforming the predictor (e.g., multiply by 1000) and/or offset
>>> make sense here. The offset confuses the issue (or, me) - as a log, it is
>>> quite small relative to the outcome and on but on the same scale as the
>>> other predictor.
>>>
>>>
>>>
>>> 2) The use of allFit to try different optimizers seems to be giving
>>> conflicting results.
>>>
>>>
>>>
>>> A3.all <- allFit(A3, meth.tab = NULL)
>>>
>>> ss <- summary(A3.all)
>>>
>>>
>>>
>>> #bobyqa : [OK]
>>>
>>> #Nelder_Mead : [OK]
>>>
>>> #nlminbwrap : [OK]
>>>
>>> #nmkbw : [OK]
>>>
>>> #optimx.L-BFGS-B : [OK]
>>>
>>> #nloptwrap.NLOPT_LN_NELDERMEAD : [OK]
>>>
>>> #nloptwrap.NLOPT_LN_BOBYQA : [OK]
>>>
>>>
>>>
>>> ss$ which.OK
>>>
>>>
>>>
>>> #bobyqa                         TRUE
>>>
>>> #Nelder_Mead                    TRUE
>>>
>>> #nlminbwrap                     TRUE
>>>
>>> #nmkbw                          TRUE
>>>
>>> #optimx.L-BFGS-B                TRUE
>>>
>>> #nloptwrap.NLOPT_LN_NELDERMEAD  TRUE
>>>
>>> #nloptwrap.NLOPT_LN_BOBYQA      TRUE
>>>
>>>
>>>
>>> What do you know, all optimizers supposedly work.
>>>
>>> However, some of them lead to a model that converges, and others don?t.
>>>
>>>
>>>
>>> A3a <- glmer.nb(wait_n ~ offset(log(wait_d)) +  scale(spr) + (1 | check),
>>> data = ST, glmerControl(optimizer = "bobyqa"))
>>>
>>>
>>>
>>> #Model is nearly unidentifiable: very large eigenvalue
>>>
>>> # - Rescale variables?
>>>
>>>
>>>
>>> A3b <- glmer.nb(wait_n ~ offset(log(wait_d)) +  scale(spr) + (1 | check),
>>> data = ST, glmerControl(optimizer = "Nelder_Mead"))
>>>
>>>
>>>
>>> #Model failed to converge with max|grad| = 0.00553513 (tol = 0.001,
>>> component 1)
>>>
>>> #Model is nearly unidentifiable: very large eigenvalue
>>>
>>> # - Rescale variables?
>>>
>>>
>>>
>>> A3c <- glmer.nb(wait_n ~ offset(log(wait_d)) +  scale(spr) + (1 | check),
>>> data = ST, glmerControl(optimizer = "nlminbwrap"))
>>>
>>>
>>>
>>> #Model is nearly unidentifiable: very large eigenvalue
>>>
>>> # - Rescale variables?
>>>
>>>
>>>
>>> A3d <- glmer.nb(wait_n ~ offset(log(wait_d)) +  scale(spr) + (1 | check),
>>> data = ST, glmerControl(optimizer = "nmkbw"))
>>>
>>>
>>>
>>> #Model failed to converge with max|grad| = 0.00177926 (tol = 0.001,
>>> component 1)
>>>
>>> #Model is nearly unidentifiable: very large eigenvalue
>>>
>>> # - Rescale variables?
>>>
>>>
>>>
>>> I?m trying to understand why allFit responds that all optimizers converge
>>> when they actually do not.
>>>
>>>
>>>
>>> Thank you,
>>>
>>> Matt
>>>
>>>
>>>
>>> Matthew Boden, Ph.D.
>>>
>>> Senior Evaluator
>>>
>>> Program Evaluation & Resource Center
>>>
>>> Office of Mental Health & Suicide Prevention
>>>
>>> Veterans Health Administration
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>

	[[alternative HTML version deleted]]


From m@tthew@t@boden @end|ng |rom gm@||@com  Fri Mar 22 17:04:38 2019
From: m@tthew@t@boden @end|ng |rom gm@||@com (Matthew Boden)
Date: Fri, 22 Mar 2019 09:04:38 -0700
Subject: [R-sig-ME] Fwd: glm.nb convergence issues
In-Reply-To: <CAJuCY5z=ENvt1ztASU2wa=BN2L3QayCTDPQ8s3-pu42OA_Sf+g@mail.gmail.com>
References: <BN8PR09MB3554300E08C4B76B4E25FC9587410@BN8PR09MB3554.namprd09.prod.outlook.com>
 <CAE10PCzNa4V0Hr03K4TwOeACavzFZUHoRo4Vrj_+VUDTueVCCA@mail.gmail.com>
 <CAJuCY5w74vVuATprurgR86zByRSbvztvN92a2m8e6ENjTG-+Jw@mail.gmail.com>
 <CAE10PCyfie21gDQHw9EWfWKFz66JAVqSU9HHBYzEJfx18sH09g@mail.gmail.com>
 <CAJuCY5z=ENvt1ztASU2wa=BN2L3QayCTDPQ8s3-pu42OA_Sf+g@mail.gmail.com>
Message-ID: <CAE10PCzC1iA1rJfxfe-C4yvJnCpDTTszV3EE+ygp5axfvioSPQ@mail.gmail.com>

Thank you, Thierry. Will do.

Matt

On Fri, Mar 22, 2019 at 7:29 AM Thierry Onkelinx <thierry.onkelinx at inbo.be>
wrote:

> Dear Matthew,
>
> The fixed effect of your model suggests following EDA plot: ggplot(ST,
> aes(x = spr, y = wait_n / wait_d)) + geom_point()
>
> Based on this plot, I doubt if the negative binomial distribution makes
> sense for your data. The problem is IMHO rather conceptual (what model
> makes sense) than technical (false convergence). I would strongly recommend
> to consult a local statistician.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op wo 20 mrt. 2019 om 20:50 schreef Matthew Boden <
> matthew.t.boden at gmail.com>:
>
>> Thank you, Thierry.  A csv file is attached; hopefully, that works.
>>
>> The numerator divided by the denominator will always fall between 0 and 1
>> for these data, and is typically very close to 1.
>> The predictor variable ranges from about 3 to 18, but even when scaled,
>> the model won't converge.
>>
>> Matt
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>> On Wed, Mar 20, 2019 at 1:48 AM Thierry Onkelinx <
>> thierry.onkelinx at inbo.be> wrote:
>>
>>> Dear Matthew,
>>>
>>> The mailing list accepts only a limited number of file formats as
>>> attachment. Your data got stripped. Can you resend the data or send a link
>>> to the data?
>>>
>>> The offset requires the log because the model is using the log link.
>>> Your model fits log(E(wait_n)) = offset(log(wait_d)) + covariates which you
>>> can rewrite as log(E(wait_n)) - offset(log(wait_d)) = covariates or
>>> log(E(wait_n) / wait_d) = covariates. Pick a relevant magnitude of wait_d
>>> so that the magnitude of wait_n/wait_d is somewhat near 1. E.g. we don't
>>> express the number of inhabitats per m? but rather per km?
>>>
>>> Best regards,
>>>
>>> ir. Thierry Onkelinx
>>> Statisticus / Statistician
>>>
>>> Vlaamse Overheid / Government of Flanders
>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>>> AND FOREST
>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>>> thierry.onkelinx at inbo.be
>>> Havenlaan 88 bus 73, 1000 Brussel
>>> www.inbo.be
>>>
>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>> To call in the statistician after the experiment is done may be no more
>>> than asking him to perform a post-mortem examination: he may be able to say
>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>> The plural of anecdote is not data. ~ Roger Brinner
>>> The combination of some data and an aching desire for an answer does not
>>> ensure that a reasonable answer can be extracted from a given body of data.
>>> ~ John Tukey
>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>
>>> <https://www.inbo.be>
>>>
>>>
>>> Op wo 20 mrt. 2019 om 06:43 schreef Matthew Boden <
>>> matthew.t.boden at gmail.com>:
>>>
>>>> Hello,
>>>>
>>>>
>>>>
>>>> I?m working my way through convergence issues related to a negative
>>>> binomial mixed model with an offset and random intercept. Data attached.
>>>>
>>>>
>>>>
>>>> A3 <- glmer.nb(wait_n ~ offset(log(wait_d)) +  scale(spr) + (1 | check),
>>>> data = ST)
>>>>
>>>> summary(A3)
>>>>
>>>>
>>>>
>>>> #Model failed to converge with max|grad| = 0.0031459 (tol = 0.001,
>>>> component 1)
>>>>
>>>> #Model is nearly unidentifiable: very large eigenvalue
>>>>
>>>> # - Rescale variables?
>>>>
>>>>
>>>>
>>>> I have two questions.
>>>>
>>>>
>>>>
>>>> 1) Outcome variable (wait_n) values are much larger than predictor (spr)
>>>> values.
>>>>
>>>>
>>>>
>>>> Mean wait_n = 11,783
>>>>
>>>> Mean spr = 7.34
>>>>
>>>>
>>>>
>>>> Would transforming the predictor (e.g., multiply by 1000) and/or offset
>>>> make sense here. The offset confuses the issue (or, me) - as a log, it
>>>> is
>>>> quite small relative to the outcome and on but on the same scale as the
>>>> other predictor.
>>>>
>>>>
>>>>
>>>> 2) The use of allFit to try different optimizers seems to be giving
>>>> conflicting results.
>>>>
>>>>
>>>>
>>>> A3.all <- allFit(A3, meth.tab = NULL)
>>>>
>>>> ss <- summary(A3.all)
>>>>
>>>>
>>>>
>>>> #bobyqa : [OK]
>>>>
>>>> #Nelder_Mead : [OK]
>>>>
>>>> #nlminbwrap : [OK]
>>>>
>>>> #nmkbw : [OK]
>>>>
>>>> #optimx.L-BFGS-B : [OK]
>>>>
>>>> #nloptwrap.NLOPT_LN_NELDERMEAD : [OK]
>>>>
>>>> #nloptwrap.NLOPT_LN_BOBYQA : [OK]
>>>>
>>>>
>>>>
>>>> ss$ which.OK
>>>>
>>>>
>>>>
>>>> #bobyqa                         TRUE
>>>>
>>>> #Nelder_Mead                    TRUE
>>>>
>>>> #nlminbwrap                     TRUE
>>>>
>>>> #nmkbw                          TRUE
>>>>
>>>> #optimx.L-BFGS-B                TRUE
>>>>
>>>> #nloptwrap.NLOPT_LN_NELDERMEAD  TRUE
>>>>
>>>> #nloptwrap.NLOPT_LN_BOBYQA      TRUE
>>>>
>>>>
>>>>
>>>> What do you know, all optimizers supposedly work.
>>>>
>>>> However, some of them lead to a model that converges, and others don?t.
>>>>
>>>>
>>>>
>>>> A3a <- glmer.nb(wait_n ~ offset(log(wait_d)) +  scale(spr) + (1 |
>>>> check),
>>>> data = ST, glmerControl(optimizer = "bobyqa"))
>>>>
>>>>
>>>>
>>>> #Model is nearly unidentifiable: very large eigenvalue
>>>>
>>>> # - Rescale variables?
>>>>
>>>>
>>>>
>>>> A3b <- glmer.nb(wait_n ~ offset(log(wait_d)) +  scale(spr) + (1 |
>>>> check),
>>>> data = ST, glmerControl(optimizer = "Nelder_Mead"))
>>>>
>>>>
>>>>
>>>> #Model failed to converge with max|grad| = 0.00553513 (tol = 0.001,
>>>> component 1)
>>>>
>>>> #Model is nearly unidentifiable: very large eigenvalue
>>>>
>>>> # - Rescale variables?
>>>>
>>>>
>>>>
>>>> A3c <- glmer.nb(wait_n ~ offset(log(wait_d)) +  scale(spr) + (1 |
>>>> check),
>>>> data = ST, glmerControl(optimizer = "nlminbwrap"))
>>>>
>>>>
>>>>
>>>> #Model is nearly unidentifiable: very large eigenvalue
>>>>
>>>> # - Rescale variables?
>>>>
>>>>
>>>>
>>>> A3d <- glmer.nb(wait_n ~ offset(log(wait_d)) +  scale(spr) + (1 |
>>>> check),
>>>> data = ST, glmerControl(optimizer = "nmkbw"))
>>>>
>>>>
>>>>
>>>> #Model failed to converge with max|grad| = 0.00177926 (tol = 0.001,
>>>> component 1)
>>>>
>>>> #Model is nearly unidentifiable: very large eigenvalue
>>>>
>>>> # - Rescale variables?
>>>>
>>>>
>>>>
>>>> I?m trying to understand why allFit responds that all optimizers
>>>> converge
>>>> when they actually do not.
>>>>
>>>>
>>>>
>>>> Thank you,
>>>>
>>>> Matt
>>>>
>>>>
>>>>
>>>> Matthew Boden, Ph.D.
>>>>
>>>> Senior Evaluator
>>>>
>>>> Program Evaluation & Resource Center
>>>>
>>>> Office of Mental Health & Suicide Prevention
>>>>
>>>> Veterans Health Administration
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>

	[[alternative HTML version deleted]]


From tr|chter @end|ng |rom un|-bremen@de  Fri Mar 22 18:16:43 2019
From: tr|chter @end|ng |rom un|-bremen@de (Tim Richter-Heitmann)
Date: Fri, 22 Mar 2019 18:16:43 +0100
Subject: [R-sig-ME] R.square in Mixed Models
Message-ID: <d22cf7f4-c560-b086-0bed-e9414d0fc89b@uni-bremen.de>

Dear List,

i have used mixed models (six groups, 30 observation each) to model 
ecological interactions of protists with their environments.

I liked my models very much, but a reviewer now asked me to give 
R.square to show the explained variance of each model. I have now read a 
bit on that topic, and realized that r2s in GLMMs have limited value.

I did not want to argue my way out this request (because reviewers 
sometimes do not like this), so i generated some values with the MuMin 
package:

r.squaredGLMM(finalfit.sand.1)
 ?????????? R2m?????? R2c
[1,] 0.2716636 0.2824504

It is understoodd that the first value represents my fixed effects, and 
the second value the sum of fixed and random effects. My task is to now 
properly interpret these values. Does this mean that my random effect 
(category, six levels) does only marginally contribute to the model fit, 
and if so, was my choice to introduce this random effect justified?

Thank you for taking your time.

-- 
Dr. Tim Richter-Heitmann

University of Bremen
Microbial Ecophysiology Group (AG Friedrich)
FB02 - Biologie/Chemie
Leobener Stra?e (NW2 A2130)
D-28359 Bremen
Tel.: 0049(0)421 218-63062
Fax: 0049(0)421 218-63069


From u@nhoro@1 @end|ng |rom buckeyem@||@o@u@edu  Fri Mar 22 18:34:40 2019
From: u@nhoro@1 @end|ng |rom buckeyem@||@o@u@edu (Uanhoro, James)
Date: Fri, 22 Mar 2019 17:34:40 +0000
Subject: [R-sig-ME] R.square in Mixed Models
In-Reply-To: <d22cf7f4-c560-b086-0bed-e9414d0fc89b@uni-bremen.de>
References: <d22cf7f4-c560-b086-0bed-e9414d0fc89b@uni-bremen.de>
Message-ID: <0B346626-3866-433C-B7A0-13775D9DD70C@getmailspring.com>

These R2 values have limited interpretations beyond the algebra that produced them; so sticking to an interpretation closest to the algebra is best: http://doi.wiley.com/10.1111/j.2041-210x.2012.00261.x.

Additionally, the choice of modeling random effects should probably not be based on the size of the random effect variance - there are additional considerations e.g., cluster sizes, see design effect formula. The best advice with these models is to model the data structure you have.

Others may have more problems with using random effects with a six-level grouping structure. But from a Bayesian perspective, this poses not so many problems; see brms package and suggestions in a paper by Andrew Gelman: http://projecteuclid.org/euclid.ba/1340371048.




On Mar 22 2019, at 1:16 pm, Tim Richter-Heitmann <trichter at uni-bremen.de> wrote:
Dear List,

i have used mixed models (six groups, 30 observation each) to model
ecological interactions of protists with their environments.

I liked my models very much, but a reviewer now asked me to give
R.square to show the explained variance of each model. I have now read a
bit on that topic, and realized that r2s in GLMMs have limited value.

I did not want to argue my way out this request (because reviewers
sometimes do not like this), so i generated some values with the MuMin
package:

r.squaredGLMM(finalfit.sand.1)
           R2m       R2c
[1,] 0.2716636 0.2824504

It is understoodd that the first value represents my fixed effects, and
the second value the sum of fixed and random effects. My task is to now
properly interpret these values. Does this mean that my random effect
(category, six levels) does only marginally contribute to the model fit,
and if so, was my choice to introduce this random effect justified?

Thank you for taking your time.

--
Dr. Tim Richter-Heitmann

University of Bremen
Microbial Ecophysiology Group (AG Friedrich)
FB02 - Biologie/Chemie
Leobener Stra?e (NW2 A2130)
D-28359 Bremen
Tel.: 0049(0)421 218-63062
Fax: 0049(0)421 218-63069

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From d@iuedecke m@iii@g oii uke@de  Fri Mar 22 19:06:27 2019
From: d@iuedecke m@iii@g oii uke@de (d@iuedecke m@iii@g oii uke@de)
Date: Fri, 22 Mar 2019 19:06:27 +0100
Subject: [R-sig-ME] R.square in Mixed Models
In-Reply-To: <d22cf7f4-c560-b086-0bed-e9414d0fc89b@uni-bremen.de>
References: <d22cf7f4-c560-b086-0bed-e9414d0fc89b@uni-bremen.de>
Message-ID: <000001d4e0d9$f8d65c70$ea831550$@uke.de>

Dear Tim,

r-squared values for mixed models are indeed a bit tricky. There have been some advancements in the past years, the most current publication on this issue I know is:

Nakagawa S, Johnson P, Schielzeth H (2017) The coefficient of determination R2 and intra-class correlation coefficient from generalized linear mixed-effects models revisted and expanded. J. R. Soc. Interface 14. doi: 10.1098/rsif.2017.0213

A function for calculating r-squared and ICC values (r2() and icc()) based on Nakagawa et al. are implemented in the sjstats-package, see an overview here:
https://strengejacke.github.io/sjstats/articles/mixedmodels-statistics.html

The latest proposal from Nakagawa et al. has an approach that computes the "mean" random effects variance, so it's also useful for models with random slopes or nested random effects.

A short side-note: I'm working on the re-organization of the sjstats-package, where some functions will be moved into other packages with specific focus. There is a new "re-implementation" of r2() in the insight-package (https://github.com/easystats/insight), called "get_variance()", which will be the basis for sjstats in the future in order to compute variance-components. "get_variance()" from the GitHub-version of the insight-package can also be used to calculate variances for model-objects from the nlme- or GLMMadaptive packages (and of course lme4, glmmTMB and rstanarm). R-squared-values or the ICC can easily be calculated once you have these variance-components extracted from a model, e.g.
  r2_marginal <- vars$var.fixed / (vars$var.fixed + vars$var.random + vars$var.residual)
  r2_conditional <- (vars$var.fixed + vars$var.random) / (vars$var.fixed + vars$var.random + vars$var.residual)

Best
Daniel


-----Urspr?ngliche Nachricht-----
Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im Auftrag von Tim Richter-Heitmann
Gesendet: Freitag, 22. M?rz 2019 18:17
An: r-sig-mixed-models at r-project.org
Betreff: [R-sig-ME] R.square in Mixed Models

Dear List,

i have used mixed models (six groups, 30 observation each) to model 
ecological interactions of protists with their environments.

I liked my models very much, but a reviewer now asked me to give 
R.square to show the explained variance of each model. I have now read a 
bit on that topic, and realized that r2s in GLMMs have limited value.

I did not want to argue my way out this request (because reviewers 
sometimes do not like this), so i generated some values with the MuMin 
package:

r.squaredGLMM(finalfit.sand.1)
            R2m       R2c
[1,] 0.2716636 0.2824504

It is understoodd that the first value represents my fixed effects, and 
the second value the sum of fixed and random effects. My task is to now 
properly interpret these values. Does this mean that my random effect 
(category, six levels) does only marginally contribute to the model fit, 
and if so, was my choice to introduce this random effect justified?

Thank you for taking your time.

-- 
Dr. Tim Richter-Heitmann

University of Bremen
Microbial Ecophysiology Group (AG Friedrich)
FB02 - Biologie/Chemie
Leobener Stra?e (NW2 A2130)
D-28359 Bremen
Tel.: 0049(0)421 218-63062
Fax: 0049(0)421 218-63069

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Prof. Dr. Dr. Uwe Koch-Gromus, Joachim Pr?l?, Marya Verdel
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING

From bbo|ker @end|ng |rom gm@||@com  Fri Mar 22 19:17:32 2019
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Fri, 22 Mar 2019 14:17:32 -0400
Subject: [R-sig-ME] R.square in Mixed Models
In-Reply-To: <d22cf7f4-c560-b086-0bed-e9414d0fc89b@uni-bremen.de>
References: <d22cf7f4-c560-b086-0bed-e9414d0fc89b@uni-bremen.de>
Message-ID: <f7b58d82-5af0-99fd-5dbc-30ce7b5597ba@gmail.com>



On 2019-03-22 1:16 p.m., Tim Richter-Heitmann wrote:
> Dear List,
> 
> i have used mixed models (six groups, 30 observation each) to model
> ecological interactions of protists with their environments.
> 
> I liked my models very much, 

  we all do :-)

but a reviewer now asked me to give
> R.square to show the explained variance of each model. I have now read a
> bit on that topic, and realized that r2s in GLMMs have limited value.
> 
> I did not want to argue my way out this request (because reviewers
> sometimes do not like this), so i generated some values with the MuMin
> package:
> 
> r.squaredGLMM(finalfit.sand.1)
> ?????????? R2m?????? R2c
> [1,] 0.2716636 0.2824504
> 
> It is understoodd that the first value represents my fixed effects, and
> the second value the sum of fixed and random effects. My task is to now
> properly interpret these values. Does this mean that my random effect
> (category, six levels) does only marginally contribute to the model fit,
> and if so, was my choice to introduce this random effect justified?

  I would say "yes" (the random effect doesn't explain much additional
variance) and "yes" (it's an intrinsic aspect of the experimental
design; there's no point in taking it out.  Hurlbert 1984 would call
that "sacrificial pseudoreplication").

> 
> Thank you for taking your time.
>


From d@iuedecke m@iii@g oii uke@de  Fri Mar 22 19:31:51 2019
From: d@iuedecke m@iii@g oii uke@de (d@iuedecke m@iii@g oii uke@de)
Date: Fri, 22 Mar 2019 19:31:51 +0100
Subject: [R-sig-ME] R.square in Mixed Models
In-Reply-To: <d22cf7f4-c560-b086-0bed-e9414d0fc89b@uni-bremen.de>
References: <d22cf7f4-c560-b086-0bed-e9414d0fc89b@uni-bremen.de>
Message-ID: <000001d4e0dd$85517790$8ff466b0$@uke.de>

Ok, I did not address your point regarding interpretation directly, but it can be derived from the two code-lines at the end of my previous answer:

I'm not quite sure what "r.squaredGLMM()" calculates, I think it's similar to the r2()-function that is based on Nakagawa et al:

  r2_marginal <- var.fixed / (var.fixed + var.random + var.residual)
  r2_conditional <- (var.fixed + var.random) / (var.fixed + var.random + var.residual)

So the marginal r2 is the model's fixed effects variance divided by it's "total" variance, while the conditional r2 is the sum of fixed and random effects divided by the total variance. Quoted from Nakagawa et al.: "Marginal R2 is concerned with variance explained by fixed factors, and conditional R2 is concerned with variance explained by both fixed and random factors."

> Does this mean that my random effect (category, six levels) does only marginally contribute to the model fit

Yes, the results suggest this statement. You can also calculate the ICC, which is a similar and useful measure to investigate the variance of a model's random effects structure.

> and if so, was my choice to introduce this random effect justified?

I would not only rely on measures like r2 or icc to decide whether I would use a mixed model or not. Sometimes, the justification is just the design or theoretical consideration. And mixed models do no harm, so if you have some kind of clustering or grouping structure in your data, a mixed model is justified.

Best
Daniel 


-----Urspr?ngliche Nachricht-----
Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im Auftrag von Tim Richter-Heitmann
Gesendet: Freitag, 22. M?rz 2019 18:17
An: r-sig-mixed-models at r-project.org
Betreff: [R-sig-ME] R.square in Mixed Models

Dear List,

i have used mixed models (six groups, 30 observation each) to model 
ecological interactions of protists with their environments.

I liked my models very much, but a reviewer now asked me to give 
R.square to show the explained variance of each model. I have now read a 
bit on that topic, and realized that r2s in GLMMs have limited value.

I did not want to argue my way out this request (because reviewers 
sometimes do not like this), so i generated some values with the MuMin 
package:

r.squaredGLMM(finalfit.sand.1)
            R2m       R2c
[1,] 0.2716636 0.2824504

It is understoodd that the first value represents my fixed effects, and 
the second value the sum of fixed and random effects. My task is to now 
properly interpret these values. Does this mean that my random effect 
(category, six levels) does only marginally contribute to the model fit, 
and if so, was my choice to introduce this random effect justified?

Thank you for taking your time.

-- 
Dr. Tim Richter-Heitmann

University of Bremen
Microbial Ecophysiology Group (AG Friedrich)
FB02 - Biologie/Chemie
Leobener Stra?e (NW2 A2130)
D-28359 Bremen
Tel.: 0049(0)421 218-63062
Fax: 0049(0)421 218-63069

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Prof. Dr. Dr. Uwe Koch-Gromus, Joachim Pr?l?, Marya Verdel
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING

From @ouhey|@@ghebghoub @end|ng |rom gm@||@com  Fri Mar 22 20:49:45 2019
From: @ouhey|@@ghebghoub @end|ng |rom gm@||@com (Souheyla GHEBGHOUB)
Date: Fri, 22 Mar 2019 19:49:45 +0000
Subject: [R-sig-ME] Random effects in multinomial regression in R?
In-Reply-To: <CADcpBHNC5BJp9HmB-p-0xAuoC1yrhFTMPwM3ZXBCcH_2jRAJgA@mail.gmail.com>
References: <CAEA998ifF3J6fdjknpAZm_xpjTjTUrY1p5oRJqzyAGSGO0FV1w@mail.gmail.com>
 <d9ebad5b-3b52-0ce0-05d7-6731444874a6@mpi.nl>
 <CAEA998j+S7V=1EMmPNeOqtyOXwbBUfiv-PJgqgafaXDkrwB+1w@mail.gmail.com>
 <e7bc83c6-4478-32ba-a652-88ed606300fa@mpi.nl>
 <CAEA998hdxDeJr1cPSs8gQob-5J0yKk+f0DwDZTw4mfPWNqc79w@mail.gmail.com>
 <d741bd3a-af48-adee-2b2e-4bdcef824865@mpi.nl>
 <CAEA998goObypPU7K5HmWGGHXvhKvvSJKU-dewqtDRw-QOZP4Vg@mail.gmail.com>
 <7d0ac7d1-b095-63b2-3bda-fec23d6065eb@mpi.nl>
 <CAEA998iF59CEzE8wBCfKFD65zoTCg=YER9eqt-mJuioR24ot=w@mail.gmail.com>
 <CADcpBHNC5BJp9HmB-p-0xAuoC1yrhFTMPwM3ZXBCcH_2jRAJgA@mail.gmail.com>
Message-ID: <CAEA998jKZ6-k5HYcZ88A2O50PrvZ-YcdK45BDqxV1RxYnUhk2w@mail.gmail.com>

Dear  Ren?,

Thank you for your feedback to me. You are right, dropping the pretest from
covariate if I predict change definitely makes sense to me! But the fact
that i need to control for the starting levels of participants makes it
obligatory for me to chose the second way, which is predicting posttest
instead of change to have pretest scores controlled for.

You also chose (1+group | word) , which is new to me. Does it intend to
assume the effect of group to vary across words, which is something
applicable to my data, right?
I will discuss all this with my supervisor, and may reply here again in few
days if you do not mind.
Thank you very much
Souheyla
University of York


On Fri, 22 Mar 2019 at 13:42, Ren? <bimonosom at gmail.com> wrote:

> Hi Souheyla,
>
> it seems to me that you will run into problems with your coding of change
> (gain, no gain and decline) because the 'change' is by
> definition/calculation depending on the predictor pretest.
> See, according to your coding scheme:
> Change = decline can only occur if pretest=1 (not by pretest=0).
> Change = gain can only occur if pretest = 0 (not by pretest=1)
> Change = No Gain can occur if pretest= 1 or 0
> In other words:
> If pretest = 1 then the possible outcomes can be decline or no gain
> If pretest = 0 then the possible outcomes can be gain or no gain
>
> And if the model result shows you then that the pre-test is significantly
> related to p(change-outcome), I guess there is no surprise in it, is it?
>
> So the first solution to this would be simply kicking the pre-test
> predictor out of the model completely, and predict:
> mod1 <- brm(Change ~ Group + (1|Subject) + (1+Group|Word),...)
> (Btw.: actually the first Hierarchical Bayes Model question I see on the
> mixed-effects mailing list :))
>
> Attempt for a further clarification on which random slopes would reflect
> the model's design:
> If you have a within-subjects design, by-subject random slopes are
> possible for the within-subject variable (e.g. if there are two sets of
> words/lists [e.g. abstract vs. concrete words] for each participant, and
> you test whether there is a performance-difference between these
> word-lists, then you can implement by-subject random slopes for words,
> because each participant has seen both sets.) If each participant has seen
> only one list (i.e. between subjects design) by subject random slopes for
> words are not appropriate, because there is no 'slope' by participant (i.e.
> by definition, having a slope requires at least two observations...). This
> is always a good rule of thumb without thinking about it too heavily :)
> Ans as you see: you can define a random slope for words:  (1+Group|Word),
> because each word has been presented in each group (i.e. there can be a
> slope for each word). And intuitively speaking the Treatment-effect can
> vary depending on the stimuli you use, and the slope makes sense. (You also
> see in this example that the treatment effect can also vary by subjects,
> but in fact, this subject effect variation IS EQUAL to the effect you want
> to test, and having by subject group random slopes would eliminate the
> fixed effect...)
>
> Anyway, there is a second possibility to define your model, depending on
> how you want to interpret it. In the previous model you can say something
> about the type-of-change likelihoods depending on the treatment group. But
> you could implement the model as binomial as well (i.e. logistic regression)
>
> mod2 <- brm(posttest ~ pretest*Group + (1|Subject) + (1+Group|Word),...)
>
> And what you would expect here would be an interaction between pre-test
> and Group. For instance; if pretest=0 & treatment 1 then posttest larger
> than with pretest=0 & treatment 2; but not when pretest=1 (because this is
> a plausible no gain situation). And so on...
> (And in this model there are no also no further random slopes hidden in
> your design :))
> Hope this helps.
>
> Best, Ren?
>
>
> Am Do., 21. M?rz 2019 um 14:01 Uhr schrieb Souheyla GHEBGHOUB <
> souheyla.ghebghoub at gmail.com>:
>
>> Dear Philip,
>>
>> I understand , here is the structure of my data in case it could help.
>>
>> I have 3 groups of participants (control, treatment1, treatment2). Each
>> group was tested twice, once before treatment (pretest) and once after
>> treatment (posttest).
>> In each test, they were tested on knowledge of 28 words, scores are
>> dichotomous (0 = unknown , 1 = known). Tests are the same.
>>
>> I calculated change from pretest to posttest :
>> if pretest 0 and posttest 0 = no gain
>> if pretest 1 and posttest 1 = no gain
>> if pretest 0 and posttest 1 = gain
>> if pretest 1 and posttest 0 = decline
>> So I ended up with a dependent variable called Change with 3 levels
>> (no_gain, gain, decline) and I tried to predict it using Group and Pretest
>> as covariates using multinomial logit model. mod0 <- brm(Change ~ Pretest
>> +
>> Group) I would like to add random effects for subjects but don't know
>> what's the best form when Time factor is absent.
>>
>> I hope other statisticians who read this could help
>> Thank you
>> Souheyla
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From @ouhey|@@ghebghoub @end|ng |rom gm@||@com  Fri Mar 22 20:51:51 2019
From: @ouhey|@@ghebghoub @end|ng |rom gm@||@com (Souheyla GHEBGHOUB)
Date: Fri, 22 Mar 2019 19:51:51 +0000
Subject: [R-sig-ME] Random effects in multinomial regression in R?
In-Reply-To: <CADcpBHNC5BJp9HmB-p-0xAuoC1yrhFTMPwM3ZXBCcH_2jRAJgA@mail.gmail.com>
References: <CAEA998ifF3J6fdjknpAZm_xpjTjTUrY1p5oRJqzyAGSGO0FV1w@mail.gmail.com>
 <d9ebad5b-3b52-0ce0-05d7-6731444874a6@mpi.nl>
 <CAEA998j+S7V=1EMmPNeOqtyOXwbBUfiv-PJgqgafaXDkrwB+1w@mail.gmail.com>
 <e7bc83c6-4478-32ba-a652-88ed606300fa@mpi.nl>
 <CAEA998hdxDeJr1cPSs8gQob-5J0yKk+f0DwDZTw4mfPWNqc79w@mail.gmail.com>
 <d741bd3a-af48-adee-2b2e-4bdcef824865@mpi.nl>
 <CAEA998goObypPU7K5HmWGGHXvhKvvSJKU-dewqtDRw-QOZP4Vg@mail.gmail.com>
 <7d0ac7d1-b095-63b2-3bda-fec23d6065eb@mpi.nl>
 <CAEA998iF59CEzE8wBCfKFD65zoTCg=YER9eqt-mJuioR24ot=w@mail.gmail.com>
 <CADcpBHNC5BJp9HmB-p-0xAuoC1yrhFTMPwM3ZXBCcH_2jRAJgA@mail.gmail.com>
Message-ID: <CAEA998j_0f8xp+1ndQNtvbEjipBKM3sdaAFmZstpu2Pdum65SQ@mail.gmail.com>

Dear Ren?,

Dear Ren?, Thank you for your feedback to me. You are right, dropping the
pretest from covariate if I predict change definitely makes sense to me!
But the fact that i need to control for the starting levels of participants
makes it obligatory for me to chose the second way, which is predicting
posttest instead of change to have pretest scores controlled for.

You also chose (1+group | word) , which is new to me. Does it intend to
assume the effect of group to vary across words, which is something
applicable to my data, right? I will discuss all this with my supervisor,
and may reply here again in few days if you do not mind.
Thank you very much
Souheyla
University of York


On Fri, 22 Mar 2019 at 13:42, Ren? <bimonosom at gmail.com> wrote:

> Hi Souheyla,
>
> it seems to me that you will run into problems with your coding of change
> (gain, no gain and decline) because the 'change' is by
> definition/calculation depending on the predictor pretest.
> See, according to your coding scheme:
> Change = decline can only occur if pretest=1 (not by pretest=0).
> Change = gain can only occur if pretest = 0 (not by pretest=1)
> Change = No Gain can occur if pretest= 1 or 0
> In other words:
> If pretest = 1 then the possible outcomes can be decline or no gain
> If pretest = 0 then the possible outcomes can be gain or no gain
>
> And if the model result shows you then that the pre-test is significantly
> related to p(change-outcome), I guess there is no surprise in it, is it?
>
> So the first solution to this would be simply kicking the pre-test
> predictor out of the model completely, and predict:
> mod1 <- brm(Change ~ Group + (1|Subject) + (1+Group|Word),...)
> (Btw.: actually the first Hierarchical Bayes Model question I see on the
> mixed-effects mailing list :))
>
> Attempt for a further clarification on which random slopes would reflect
> the model's design:
> If you have a within-subjects design, by-subject random slopes are
> possible for the within-subject variable (e.g. if there are two sets of
> words/lists [e.g. abstract vs. concrete words] for each participant, and
> you test whether there is a performance-difference between these
> word-lists, then you can implement by-subject random slopes for words,
> because each participant has seen both sets.) If each participant has seen
> only one list (i.e. between subjects design) by subject random slopes for
> words are not appropriate, because there is no 'slope' by participant (i.e.
> by definition, having a slope requires at least two observations...). This
> is always a good rule of thumb without thinking about it too heavily :)
> Ans as you see: you can define a random slope for words:  (1+Group|Word),
> because each word has been presented in each group (i.e. there can be a
> slope for each word). And intuitively speaking the Treatment-effect can
> vary depending on the stimuli you use, and the slope makes sense. (You also
> see in this example that the treatment effect can also vary by subjects,
> but in fact, this subject effect variation IS EQUAL to the effect you want
> to test, and having by subject group random slopes would eliminate the
> fixed effect...)
>
> Anyway, there is a second possibility to define your model, depending on
> how you want to interpret it. In the previous model you can say something
> about the type-of-change likelihoods depending on the treatment group. But
> you could implement the model as binomial as well (i.e. logistic regression)
>
> mod2 <- brm(posttest ~ pretest*Group + (1|Subject) + (1+Group|Word),...)
>
> And what you would expect here would be an interaction between pre-test
> and Group. For instance; if pretest=0 & treatment 1 then posttest larger
> than with pretest=0 & treatment 2; but not when pretest=1 (because this is
> a plausible no gain situation). And so on...
> (And in this model there are no also no further random slopes hidden in
> your design :))
> Hope this helps.
>
> Best, Ren?
>
>
> Am Do., 21. M?rz 2019 um 14:01 Uhr schrieb Souheyla GHEBGHOUB <
> souheyla.ghebghoub at gmail.com>:
>
>> Dear Philip,
>>
>> I understand , here is the structure of my data in case it could help.
>>
>> I have 3 groups of participants (control, treatment1, treatment2). Each
>> group was tested twice, once before treatment (pretest) and once after
>> treatment (posttest).
>> In each test, they were tested on knowledge of 28 words, scores are
>> dichotomous (0 = unknown , 1 = known). Tests are the same.
>>
>> I calculated change from pretest to posttest :
>> if pretest 0 and posttest 0 = no gain
>> if pretest 1 and posttest 1 = no gain
>> if pretest 0 and posttest 1 = gain
>> if pretest 1 and posttest 0 = decline
>> So I ended up with a dependent variable called Change with 3 levels
>> (no_gain, gain, decline) and I tried to predict it using Group and Pretest
>> as covariates using multinomial logit model. mod0 <- brm(Change ~ Pretest
>> +
>> Group) I would like to add random effects for subjects but don't know
>> what's the best form when Time factor is absent.
>>
>> I hope other statisticians who read this could help
>> Thank you
>> Souheyla
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From HDor@n @end|ng |rom @|r@org  Fri Mar 22 22:19:29 2019
From: HDor@n @end|ng |rom @|r@org (Doran, Harold)
Date: Fri, 22 Mar 2019 21:19:29 +0000
Subject: [R-sig-ME] Random effects in multinomial regression in R?
In-Reply-To: <CAEA998jKZ6-k5HYcZ88A2O50PrvZ-YcdK45BDqxV1RxYnUhk2w@mail.gmail.com>
References: <CAEA998ifF3J6fdjknpAZm_xpjTjTUrY1p5oRJqzyAGSGO0FV1w@mail.gmail.com>
 <d9ebad5b-3b52-0ce0-05d7-6731444874a6@mpi.nl>
 <CAEA998j+S7V=1EMmPNeOqtyOXwbBUfiv-PJgqgafaXDkrwB+1w@mail.gmail.com>
 <e7bc83c6-4478-32ba-a652-88ed606300fa@mpi.nl>
 <CAEA998hdxDeJr1cPSs8gQob-5J0yKk+f0DwDZTw4mfPWNqc79w@mail.gmail.com>
 <d741bd3a-af48-adee-2b2e-4bdcef824865@mpi.nl>
 <CAEA998goObypPU7K5HmWGGHXvhKvvSJKU-dewqtDRw-QOZP4Vg@mail.gmail.com>
 <7d0ac7d1-b095-63b2-3bda-fec23d6065eb@mpi.nl>
 <CAEA998iF59CEzE8wBCfKFD65zoTCg=YER9eqt-mJuioR24ot=w@mail.gmail.com>
 <CADcpBHNC5BJp9HmB-p-0xAuoC1yrhFTMPwM3ZXBCcH_2jRAJgA@mail.gmail.com>
 <CAEA998jKZ6-k5HYcZ88A2O50PrvZ-YcdK45BDqxV1RxYnUhk2w@mail.gmail.com>
Message-ID: <D8BAC970.57C3D%hdoran@air.org>

Yes, but conditioning on the pre-test means you are using a variable
measured with error and the estimates you obtain and now inconsistent, and
that?s a pretty big sin.

On 3/22/19, 3:49 PM, "Souheyla GHEBGHOUB" <souheyla.ghebghoub at gmail.com>
wrote:

>Dear  Ren?,
>
>Thank you for your feedback to me. You are right, dropping the pretest
>from
>covariate if I predict change definitely makes sense to me! But the fact
>that i need to control for the starting levels of participants makes it
>obligatory for me to chose the second way, which is predicting posttest
>instead of change to have pretest scores controlled for.
>
>You also chose (1+group | word) , which is new to me. Does it intend to
>assume the effect of group to vary across words, which is something
>applicable to my data, right?
>I will discuss all this with my supervisor, and may reply here again in
>few
>days if you do not mind.
>Thank you very much
>Souheyla
>University of York
>
>
>On Fri, 22 Mar 2019 at 13:42, Ren? <bimonosom at gmail.com> wrote:
>
>> Hi Souheyla,
>>
>> it seems to me that you will run into problems with your coding of
>>change
>> (gain, no gain and decline) because the 'change' is by
>> definition/calculation depending on the predictor pretest.
>> See, according to your coding scheme:
>> Change = decline can only occur if pretest=1 (not by pretest=0).
>> Change = gain can only occur if pretest = 0 (not by pretest=1)
>> Change = No Gain can occur if pretest= 1 or 0
>> In other words:
>> If pretest = 1 then the possible outcomes can be decline or no gain
>> If pretest = 0 then the possible outcomes can be gain or no gain
>>
>> And if the model result shows you then that the pre-test is
>>significantly
>> related to p(change-outcome), I guess there is no surprise in it, is it?
>>
>> So the first solution to this would be simply kicking the pre-test
>> predictor out of the model completely, and predict:
>> mod1 <- brm(Change ~ Group + (1|Subject) + (1+Group|Word),...)
>> (Btw.: actually the first Hierarchical Bayes Model question I see on the
>> mixed-effects mailing list :))
>>
>> Attempt for a further clarification on which random slopes would reflect
>> the model's design:
>> If you have a within-subjects design, by-subject random slopes are
>> possible for the within-subject variable (e.g. if there are two sets of
>> words/lists [e.g. abstract vs. concrete words] for each participant, and
>> you test whether there is a performance-difference between these
>> word-lists, then you can implement by-subject random slopes for words,
>> because each participant has seen both sets.) If each participant has
>>seen
>> only one list (i.e. between subjects design) by subject random slopes
>>for
>> words are not appropriate, because there is no 'slope' by participant
>>(i.e.
>> by definition, having a slope requires at least two observations...).
>>This
>> is always a good rule of thumb without thinking about it too heavily :)
>> Ans as you see: you can define a random slope for words:
>>(1+Group|Word),
>> because each word has been presented in each group (i.e. there can be a
>> slope for each word). And intuitively speaking the Treatment-effect can
>> vary depending on the stimuli you use, and the slope makes sense. (You
>>also
>> see in this example that the treatment effect can also vary by subjects,
>> but in fact, this subject effect variation IS EQUAL to the effect you
>>want
>> to test, and having by subject group random slopes would eliminate the
>> fixed effect...)
>>
>> Anyway, there is a second possibility to define your model, depending on
>> how you want to interpret it. In the previous model you can say
>>something
>> about the type-of-change likelihoods depending on the treatment group.
>>But
>> you could implement the model as binomial as well (i.e. logistic
>>regression)
>>
>> mod2 <- brm(posttest ~ pretest*Group + (1|Subject) + (1+Group|Word),...)
>>
>> And what you would expect here would be an interaction between pre-test
>> and Group. For instance; if pretest=0 & treatment 1 then posttest larger
>> than with pretest=0 & treatment 2; but not when pretest=1 (because this
>>is
>> a plausible no gain situation). And so on...
>> (And in this model there are no also no further random slopes hidden in
>> your design :))
>> Hope this helps.
>>
>> Best, Ren?
>>
>>
>> Am Do., 21. M?rz 2019 um 14:01 Uhr schrieb Souheyla GHEBGHOUB <
>> souheyla.ghebghoub at gmail.com>:
>>
>>> Dear Philip,
>>>
>>> I understand , here is the structure of my data in case it could help.
>>>
>>> I have 3 groups of participants (control, treatment1, treatment2). Each
>>> group was tested twice, once before treatment (pretest) and once after
>>> treatment (posttest).
>>> In each test, they were tested on knowledge of 28 words, scores are
>>> dichotomous (0 = unknown , 1 = known). Tests are the same.
>>>
>>> I calculated change from pretest to posttest :
>>> if pretest 0 and posttest 0 = no gain
>>> if pretest 1 and posttest 1 = no gain
>>> if pretest 0 and posttest 1 = gain
>>> if pretest 1 and posttest 0 = decline
>>> So I ended up with a dependent variable called Change with 3 levels
>>> (no_gain, gain, decline) and I tried to predict it using Group and
>>>Pretest
>>> as covariates using multinomial logit model. mod0 <- brm(Change ~
>>>Pretest
>>> +
>>> Group) I would like to add random effects for subjects but don't know
>>> what's the best form when Time factor is absent.
>>>
>>> I hope other statisticians who read this could help
>>> Thank you
>>> Souheyla
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>
>
>	[[alternative HTML version deleted]]
>
>_______________________________________________
>R-sig-mixed-models at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From @ouhey|@@ghebghoub @end|ng |rom gm@||@com  Fri Mar 22 22:39:06 2019
From: @ouhey|@@ghebghoub @end|ng |rom gm@||@com (Souheyla GHEBGHOUB)
Date: Fri, 22 Mar 2019 21:39:06 +0000
Subject: [R-sig-ME] Random effects in multinomial regression in R?
In-Reply-To: <D8BAC970.57C3D%hdoran@air.org>
References: <CAEA998ifF3J6fdjknpAZm_xpjTjTUrY1p5oRJqzyAGSGO0FV1w@mail.gmail.com>
 <d9ebad5b-3b52-0ce0-05d7-6731444874a6@mpi.nl>
 <CAEA998j+S7V=1EMmPNeOqtyOXwbBUfiv-PJgqgafaXDkrwB+1w@mail.gmail.com>
 <e7bc83c6-4478-32ba-a652-88ed606300fa@mpi.nl>
 <CAEA998hdxDeJr1cPSs8gQob-5J0yKk+f0DwDZTw4mfPWNqc79w@mail.gmail.com>
 <d741bd3a-af48-adee-2b2e-4bdcef824865@mpi.nl>
 <CAEA998goObypPU7K5HmWGGHXvhKvvSJKU-dewqtDRw-QOZP4Vg@mail.gmail.com>
 <7d0ac7d1-b095-63b2-3bda-fec23d6065eb@mpi.nl>
 <CAEA998iF59CEzE8wBCfKFD65zoTCg=YER9eqt-mJuioR24ot=w@mail.gmail.com>
 <CADcpBHNC5BJp9HmB-p-0xAuoC1yrhFTMPwM3ZXBCcH_2jRAJgA@mail.gmail.com>
 <CAEA998jKZ6-k5HYcZ88A2O50PrvZ-YcdK45BDqxV1RxYnUhk2w@mail.gmail.com>
 <D8BAC970.57C3D%hdoran@air.org>
Message-ID: <CAEA998ggNw91Y93TEyPy=_t6JuJfsTdse+77H7WAdkXQTrgVOw@mail.gmail.com>

Dear Doran,

Could you explain more this point to me, please?

Thank you,
Souheyla

On Fri, 22 Mar 2019, 21:19 Doran, Harold, <HDoran at air.org> wrote:

> Yes, but conditioning on the pre-test means you are using a variable
> measured with error and the estimates you obtain and now inconsistent, and
> that?s a pretty big sin.
>
> On 3/22/19, 3:49 PM, "Souheyla GHEBGHOUB" <souheyla.ghebghoub at gmail.com>
> wrote:
>
> >Dear  Ren?,
> >
> >Thank you for your feedback to me. You are right, dropping the pretest
> >from
> >covariate if I predict change definitely makes sense to me! But the fact
> >that i need to control for the starting levels of participants makes it
> >obligatory for me to chose the second way, which is predicting posttest
> >instead of change to have pretest scores controlled for.
> >
> >You also chose (1+group | word) , which is new to me. Does it intend to
> >assume the effect of group to vary across words, which is something
> >applicable to my data, right?
> >I will discuss all this with my supervisor, and may reply here again in
> >few
> >days if you do not mind.
> >Thank you very much
> >Souheyla
> >University of York
> >
> >
> >On Fri, 22 Mar 2019 at 13:42, Ren? <bimonosom at gmail.com> wrote:
> >
> >> Hi Souheyla,
> >>
> >> it seems to me that you will run into problems with your coding of
> >>change
> >> (gain, no gain and decline) because the 'change' is by
> >> definition/calculation depending on the predictor pretest.
> >> See, according to your coding scheme:
> >> Change = decline can only occur if pretest=1 (not by pretest=0).
> >> Change = gain can only occur if pretest = 0 (not by pretest=1)
> >> Change = No Gain can occur if pretest= 1 or 0
> >> In other words:
> >> If pretest = 1 then the possible outcomes can be decline or no gain
> >> If pretest = 0 then the possible outcomes can be gain or no gain
> >>
> >> And if the model result shows you then that the pre-test is
> >>significantly
> >> related to p(change-outcome), I guess there is no surprise in it, is it?
> >>
> >> So the first solution to this would be simply kicking the pre-test
> >> predictor out of the model completely, and predict:
> >> mod1 <- brm(Change ~ Group + (1|Subject) + (1+Group|Word),...)
> >> (Btw.: actually the first Hierarchical Bayes Model question I see on the
> >> mixed-effects mailing list :))
> >>
> >> Attempt for a further clarification on which random slopes would reflect
> >> the model's design:
> >> If you have a within-subjects design, by-subject random slopes are
> >> possible for the within-subject variable (e.g. if there are two sets of
> >> words/lists [e.g. abstract vs. concrete words] for each participant, and
> >> you test whether there is a performance-difference between these
> >> word-lists, then you can implement by-subject random slopes for words,
> >> because each participant has seen both sets.) If each participant has
> >>seen
> >> only one list (i.e. between subjects design) by subject random slopes
> >>for
> >> words are not appropriate, because there is no 'slope' by participant
> >>(i.e.
> >> by definition, having a slope requires at least two observations...).
> >>This
> >> is always a good rule of thumb without thinking about it too heavily :)
> >> Ans as you see: you can define a random slope for words:
> >>(1+Group|Word),
> >> because each word has been presented in each group (i.e. there can be a
> >> slope for each word). And intuitively speaking the Treatment-effect can
> >> vary depending on the stimuli you use, and the slope makes sense. (You
> >>also
> >> see in this example that the treatment effect can also vary by subjects,
> >> but in fact, this subject effect variation IS EQUAL to the effect you
> >>want
> >> to test, and having by subject group random slopes would eliminate the
> >> fixed effect...)
> >>
> >> Anyway, there is a second possibility to define your model, depending on
> >> how you want to interpret it. In the previous model you can say
> >>something
> >> about the type-of-change likelihoods depending on the treatment group.
> >>But
> >> you could implement the model as binomial as well (i.e. logistic
> >>regression)
> >>
> >> mod2 <- brm(posttest ~ pretest*Group + (1|Subject) + (1+Group|Word),...)
> >>
> >> And what you would expect here would be an interaction between pre-test
> >> and Group. For instance; if pretest=0 & treatment 1 then posttest larger
> >> than with pretest=0 & treatment 2; but not when pretest=1 (because this
> >>is
> >> a plausible no gain situation). And so on...
> >> (And in this model there are no also no further random slopes hidden in
> >> your design :))
> >> Hope this helps.
> >>
> >> Best, Ren?
> >>
> >>
> >> Am Do., 21. M?rz 2019 um 14:01 Uhr schrieb Souheyla GHEBGHOUB <
> >> souheyla.ghebghoub at gmail.com>:
> >>
> >>> Dear Philip,
> >>>
> >>> I understand , here is the structure of my data in case it could help.
> >>>
> >>> I have 3 groups of participants (control, treatment1, treatment2). Each
> >>> group was tested twice, once before treatment (pretest) and once after
> >>> treatment (posttest).
> >>> In each test, they were tested on knowledge of 28 words, scores are
> >>> dichotomous (0 = unknown , 1 = known). Tests are the same.
> >>>
> >>> I calculated change from pretest to posttest :
> >>> if pretest 0 and posttest 0 = no gain
> >>> if pretest 1 and posttest 1 = no gain
> >>> if pretest 0 and posttest 1 = gain
> >>> if pretest 1 and posttest 0 = decline
> >>> So I ended up with a dependent variable called Change with 3 levels
> >>> (no_gain, gain, decline) and I tried to predict it using Group and
> >>>Pretest
> >>> as covariates using multinomial logit model. mod0 <- brm(Change ~
> >>>Pretest
> >>> +
> >>> Group) I would like to add random effects for subjects but don't know
> >>> what's the best form when Time factor is absent.
> >>>
> >>> I hope other statisticians who read this could help
> >>> Thank you
> >>> Souheyla
> >>>
> >>>         [[alternative HTML version deleted]]
> >>>
> >>> _______________________________________________
> >>> R-sig-mixed-models at r-project.org mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>>
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> >_______________________________________________
> >R-sig-mixed-models at r-project.org mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>
>

	[[alternative HTML version deleted]]


From u@nhoro@1 @end|ng |rom buckeyem@||@o@u@edu  Fri Mar 22 23:02:06 2019
From: u@nhoro@1 @end|ng |rom buckeyem@||@o@u@edu (Uanhoro, James)
Date: Fri, 22 Mar 2019 22:02:06 +0000
Subject: [R-sig-ME] Random effects in multinomial regression in R?
In-Reply-To: <CAEA998ggNw91Y93TEyPy=_t6JuJfsTdse+77H7WAdkXQTrgVOw@mail.gmail.com>
References: <CAEA998ggNw91Y93TEyPy=_t6JuJfsTdse+77H7WAdkXQTrgVOw@mail.gmail.com>
Message-ID: <2AF9E03B-C3BC-4C3F-8BD7-053CB8037205@getmailspring.com>

In standard regression models, the assumption is predictor variables are measured without error. Test scores will have measurement error, hence Doran's comment when test scores are used as covariates. See: Hausman, J. (2001). Mismeasured Variables in Econometric Analysis: Problems from the Right and Problems from the Left. Journal of Economic Perspectives, 15(4), 57?67. https://doi.org/10.1257/jep.15.4.57
I will note that many practitioners ignore this issue, and it is quite common to use predictors measured with error. Consider the number of times people use polychotomized income measures, or SES measures as predictors, or some other "construct".



On Mar 22 2019, at 5:39 pm, Souheyla GHEBGHOUB <souheyla.ghebghoub at gmail.com> wrote:
Dear Doran,

Could you explain more this point to me, please?

Thank you,
Souheyla

On Fri, 22 Mar 2019, 21:19 Doran, Harold, <HDoran at air.org> wrote:

Yes, but conditioning on the pre-test means you are using a variable
measured with error and the estimates you obtain and now inconsistent, and
that?s a pretty big sin.

On 3/22/19, 3:49 PM, "Souheyla GHEBGHOUB" <souheyla.ghebghoub at gmail.com>
wrote:

Dear Ren?,

Thank you for your feedback to me. You are right, dropping the pretest
from
covariate if I predict change definitely makes sense to me! But the fact
that i need to control for the starting levels of participants makes it
obligatory for me to chose the second way, which is predicting posttest
instead of change to have pretest scores controlled for.

You also chose (1+group | word) , which is new to me. Does it intend to
assume the effect of group to vary across words, which is something
applicable to my data, right?
I will discuss all this with my supervisor, and may reply here again in
few
days if you do not mind.
Thank you very much
Souheyla
University of York


On Fri, 22 Mar 2019 at 13:42, Ren? <bimonosom at gmail.com> wrote:

Hi Souheyla,

it seems to me that you will run into problems with your coding of
change
(gain, no gain and decline) because the 'change' is by
definition/calculation depending on the predictor pretest.
See, according to your coding scheme:
Change = decline can only occur if pretest=1 (not by pretest=0).
Change = gain can only occur if pretest = 0 (not by pretest=1)
Change = No Gain can occur if pretest= 1 or 0
In other words:
If pretest = 1 then the possible outcomes can be decline or no gain
If pretest = 0 then the possible outcomes can be gain or no gain

And if the model result shows you then that the pre-test is
significantly
related to p(change-outcome), I guess there is no surprise in it, is it?

So the first solution to this would be simply kicking the pre-test
predictor out of the model completely, and predict:
mod1 <- brm(Change ~ Group + (1|Subject) + (1+Group|Word),...)
(Btw.: actually the first Hierarchical Bayes Model question I see on the
mixed-effects mailing list :))

Attempt for a further clarification on which random slopes would reflect
the model's design:
If you have a within-subjects design, by-subject random slopes are
possible for the within-subject variable (e.g. if there are two sets of
words/lists [e.g. abstract vs. concrete words] for each participant, and
you test whether there is a performance-difference between these
word-lists, then you can implement by-subject random slopes for words,
because each participant has seen both sets.) If each participant has
seen
only one list (i.e. between subjects design) by subject random slopes
for
words are not appropriate, because there is no 'slope' by participant
(i.e.
by definition, having a slope requires at least two observations...).
This
is always a good rule of thumb without thinking about it too heavily :)
Ans as you see: you can define a random slope for words:
(1+Group|Word),
because each word has been presented in each group (i.e. there can be a
slope for each word). And intuitively speaking the Treatment-effect can
vary depending on the stimuli you use, and the slope makes sense. (You
also
see in this example that the treatment effect can also vary by subjects,
but in fact, this subject effect variation IS EQUAL to the effect you
want
to test, and having by subject group random slopes would eliminate the
fixed effect...)

Anyway, there is a second possibility to define your model, depending on
how you want to interpret it. In the previous model you can say
something
about the type-of-change likelihoods depending on the treatment group.
But
you could implement the model as binomial as well (i.e. logistic
regression)

mod2 <- brm(posttest ~ pretest*Group + (1|Subject) + (1+Group|Word),...)

And what you would expect here would be an interaction between pre-test
and Group. For instance; if pretest=0 & treatment 1 then posttest larger
than with pretest=0 & treatment 2; but not when pretest=1 (because this
is
a plausible no gain situation). And so on...
(And in this model there are no also no further random slopes hidden in
your design :))
Hope this helps.

Best, Ren?


Am Do., 21. M?rz 2019 um 14:01 Uhr schrieb Souheyla GHEBGHOUB <
souheyla.ghebghoub at gmail.com>:

Dear Philip,

I understand , here is the structure of my data in case it could help.

I have 3 groups of participants (control, treatment1, treatment2). Each
group was tested twice, once before treatment (pretest) and once after
treatment (posttest).
In each test, they were tested on knowledge of 28 words, scores are
dichotomous (0 = unknown , 1 = known). Tests are the same.

I calculated change from pretest to posttest :
if pretest 0 and posttest 0 = no gain
if pretest 1 and posttest 1 = no gain
if pretest 0 and posttest 1 = gain
if pretest 1 and posttest 0 = decline
So I ended up with a dependent variable called Change with 3 levels
(no_gain, gain, decline) and I tried to predict it using Group and
Pretest
as covariates using multinomial logit model. mod0 <- brm(Change ~
Pretest
+
Group) I would like to add random effects for subjects but don't know
what's the best form when Time factor is absent.

I hope other statisticians who read this could help
Thank you
Souheyla

[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models




[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From HDor@n @end|ng |rom @|r@org  Fri Mar 22 23:23:45 2019
From: HDor@n @end|ng |rom @|r@org (Doran, Harold)
Date: Fri, 22 Mar 2019 22:23:45 +0000
Subject: [R-sig-ME] Random effects in multinomial regression in R?
In-Reply-To: <CAEA998ggNw91Y93TEyPy=_t6JuJfsTdse+77H7WAdkXQTrgVOw@mail.gmail.com>
References: <CAEA998ifF3J6fdjknpAZm_xpjTjTUrY1p5oRJqzyAGSGO0FV1w@mail.gmail.com>
 <d9ebad5b-3b52-0ce0-05d7-6731444874a6@mpi.nl>
 <CAEA998j+S7V=1EMmPNeOqtyOXwbBUfiv-PJgqgafaXDkrwB+1w@mail.gmail.com>
 <e7bc83c6-4478-32ba-a652-88ed606300fa@mpi.nl>
 <CAEA998hdxDeJr1cPSs8gQob-5J0yKk+f0DwDZTw4mfPWNqc79w@mail.gmail.com>
 <d741bd3a-af48-adee-2b2e-4bdcef824865@mpi.nl>
 <CAEA998goObypPU7K5HmWGGHXvhKvvSJKU-dewqtDRw-QOZP4Vg@mail.gmail.com>
 <7d0ac7d1-b095-63b2-3bda-fec23d6065eb@mpi.nl>
 <CAEA998iF59CEzE8wBCfKFD65zoTCg=YER9eqt-mJuioR24ot=w@mail.gmail.com>
 <CADcpBHNC5BJp9HmB-p-0xAuoC1yrhFTMPwM3ZXBCcH_2jRAJgA@mail.gmail.com>
 <CAEA998jKZ6-k5HYcZ88A2O50PrvZ-YcdK45BDqxV1RxYnUhk2w@mail.gmail.com>
 <D8BAC970.57C3D%hdoran@air.org>
 <CAEA998ggNw91Y93TEyPy=_t6JuJfsTdse+77H7WAdkXQTrgVOw@mail.gmail.com>
Message-ID: <D8BAD839.57C4E%hdoran@air.org>

This article describes the issue, but in the context of linear models. The underlying issue remains even within the GLMM, however.

https://amstat.tandfonline.com/doi/full/10.1080/2330443X.2014.955228#.XJVf0nN7m8U

My software, METRICS, implements variations of the EiV model described, but again, only for linear models

https://shiny.airast.org/METRICS/



From: Souheyla GHEBGHOUB <souheyla.ghebghoub at gmail.com<mailto:souheyla.ghebghoub at gmail.com>>
Date: Friday, March 22, 2019 at 4:39 PM
To: AIR <hdoran at air.org<mailto:hdoran at air.org>>
Cc: Ren? <bimonosom at gmail.com<mailto:bimonosom at gmail.com>>, "r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>" <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
Subject: Re: [R-sig-ME] Random effects in multinomial regression in R?

Dear Doran,

Could you explain more this point to me, please?

Thank you,
Souheyla

On Fri, 22 Mar 2019, 21:19 Doran, Harold, <HDoran at air.org<mailto:HDoran at air.org>> wrote:
Yes, but conditioning on the pre-test means you are using a variable
measured with error and the estimates you obtain and now inconsistent, and
that?s a pretty big sin.

On 3/22/19, 3:49 PM, "Souheyla GHEBGHOUB" <souheyla.ghebghoub at gmail.com<mailto:souheyla.ghebghoub at gmail.com>>
wrote:

>Dear  Ren?,
>
>Thank you for your feedback to me. You are right, dropping the pretest
>from
>covariate if I predict change definitely makes sense to me! But the fact
>that i need to control for the starting levels of participants makes it
>obligatory for me to chose the second way, which is predicting posttest
>instead of change to have pretest scores controlled for.
>
>You also chose (1+group | word) , which is new to me. Does it intend to
>assume the effect of group to vary across words, which is something
>applicable to my data, right?
>I will discuss all this with my supervisor, and may reply here again in
>few
>days if you do not mind.
>Thank you very much
>Souheyla
>University of York
>
>
>On Fri, 22 Mar 2019 at 13:42, Ren? <bimonosom at gmail.com<mailto:bimonosom at gmail.com>> wrote:
>
>> Hi Souheyla,
>>
>> it seems to me that you will run into problems with your coding of
>>change
>> (gain, no gain and decline) because the 'change' is by
>> definition/calculation depending on the predictor pretest.
>> See, according to your coding scheme:
>> Change = decline can only occur if pretest=1 (not by pretest=0).
>> Change = gain can only occur if pretest = 0 (not by pretest=1)
>> Change = No Gain can occur if pretest= 1 or 0
>> In other words:
>> If pretest = 1 then the possible outcomes can be decline or no gain
>> If pretest = 0 then the possible outcomes can be gain or no gain
>>
>> And if the model result shows you then that the pre-test is
>>significantly
>> related to p(change-outcome), I guess there is no surprise in it, is it?
>>
>> So the first solution to this would be simply kicking the pre-test
>> predictor out of the model completely, and predict:
>> mod1 <- brm(Change ~ Group + (1|Subject) + (1+Group|Word),...)
>> (Btw.: actually the first Hierarchical Bayes Model question I see on the
>> mixed-effects mailing list :))
>>
>> Attempt for a further clarification on which random slopes would reflect
>> the model's design:
>> If you have a within-subjects design, by-subject random slopes are
>> possible for the within-subject variable (e.g. if there are two sets of
>> words/lists [e.g. abstract vs. concrete words] for each participant, and
>> you test whether there is a performance-difference between these
>> word-lists, then you can implement by-subject random slopes for words,
>> because each participant has seen both sets.) If each participant has
>>seen
>> only one list (i.e. between subjects design) by subject random slopes
>>for
>> words are not appropriate, because there is no 'slope' by participant
>>(i.e.
>> by definition, having a slope requires at least two observations...).
>>This
>> is always a good rule of thumb without thinking about it too heavily :)
>> Ans as you see: you can define a random slope for words:
>>(1+Group|Word),
>> because each word has been presented in each group (i.e. there can be a
>> slope for each word). And intuitively speaking the Treatment-effect can
>> vary depending on the stimuli you use, and the slope makes sense. (You
>>also
>> see in this example that the treatment effect can also vary by subjects,
>> but in fact, this subject effect variation IS EQUAL to the effect you
>>want
>> to test, and having by subject group random slopes would eliminate the
>> fixed effect...)
>>
>> Anyway, there is a second possibility to define your model, depending on
>> how you want to interpret it. In the previous model you can say
>>something
>> about the type-of-change likelihoods depending on the treatment group.
>>But
>> you could implement the model as binomial as well (i.e. logistic
>>regression)
>>
>> mod2 <- brm(posttest ~ pretest*Group + (1|Subject) + (1+Group|Word),...)
>>
>> And what you would expect here would be an interaction between pre-test
>> and Group. For instance; if pretest=0 & treatment 1 then posttest larger
>> than with pretest=0 & treatment 2; but not when pretest=1 (because this
>>is
>> a plausible no gain situation). And so on...
>> (And in this model there are no also no further random slopes hidden in
>> your design :))
>> Hope this helps.
>>
>> Best, Ren?
>>
>>
>> Am Do., 21. M?rz 2019 um 14:01 Uhr schrieb Souheyla GHEBGHOUB <
>> souheyla.ghebghoub at gmail.com<mailto:souheyla.ghebghoub at gmail.com>>:
>>
>>> Dear Philip,
>>>
>>> I understand , here is the structure of my data in case it could help.
>>>
>>> I have 3 groups of participants (control, treatment1, treatment2). Each
>>> group was tested twice, once before treatment (pretest) and once after
>>> treatment (posttest).
>>> In each test, they were tested on knowledge of 28 words, scores are
>>> dichotomous (0 = unknown , 1 = known). Tests are the same.
>>>
>>> I calculated change from pretest to posttest :
>>> if pretest 0 and posttest 0 = no gain
>>> if pretest 1 and posttest 1 = no gain
>>> if pretest 0 and posttest 1 = gain
>>> if pretest 1 and posttest 0 = decline
>>> So I ended up with a dependent variable called Change with 3 levels
>>> (no_gain, gain, decline) and I tried to predict it using Group and
>>>Pretest
>>> as covariates using multinomial logit model. mod0 <- brm(Change ~
>>>Pretest
>>> +
>>> Group) I would like to add random effects for subjects but don't know
>>> what's the best form when Time factor is absent.
>>>
>>> I hope other statisticians who read this could help
>>> Thank you
>>> Souheyla
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>
>
>       [[alternative HTML version deleted]]
>
>_______________________________________________
>R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


	[[alternative HTML version deleted]]


From @ouhey|@@ghebghoub @end|ng |rom gm@||@com  Sat Mar 23 00:31:59 2019
From: @ouhey|@@ghebghoub @end|ng |rom gm@||@com (Souheyla GHEBGHOUB)
Date: Fri, 22 Mar 2019 23:31:59 +0000
Subject: [R-sig-ME] Random effects in multinomial regression in R?
In-Reply-To: <2AF9E03B-C3BC-4C3F-8BD7-053CB8037205@getmailspring.com>
References: <CAEA998ggNw91Y93TEyPy=_t6JuJfsTdse+77H7WAdkXQTrgVOw@mail.gmail.com>
 <2AF9E03B-C3BC-4C3F-8BD7-053CB8037205@getmailspring.com>
Message-ID: <CAEA998g4rYVZoDQikUJXQCM14jD6D54NokXbOBVAe2LqV8V+rA@mail.gmail.com>

Thank you both. I will look into this and see :)

Best,
Souheyla

On Fri, 22 Mar 2019, 22:02 Uanhoro, James, <uanhoro.1 at buckeyemail.osu.edu>
wrote:

> In standard regression models, the assumption is predictor variables are
> measured without error. Test scores will have measurement error, hence
> Doran's comment when test scores are used as covariates. See: Hausman, J.
> (2001). Mismeasured Variables in Econometric Analysis: Problems from the
> Right and Problems from the Left. *Journal of Economic Perspectives*, *15*(4),
> 57?67. https://doi.org/10.1257/jep.15.4.57
> I will note that many practitioners ignore this issue, and it is quite
> common to use predictors measured with error. Consider the number of times
> people use polychotomized income measures, or SES measures as predictors,
> or some other "construct".
> On Mar 22 2019, at 5:39 pm, Souheyla GHEBGHOUB <
> souheyla.ghebghoub at gmail.com> wrote:
>
> Dear Doran,
>
> Could you explain more this point to me, please?
>
> Thank you,
> Souheyla
>
> On Fri, 22 Mar 2019, 21:19 Doran, Harold, <HDoran at air.org> wrote:
>
> Yes, but conditioning on the pre-test means you are using a variable
> measured with error and the estimates you obtain and now inconsistent, and
> that?s a pretty big sin.
>
> On 3/22/19, 3:49 PM, "Souheyla GHEBGHOUB" <souheyla.ghebghoub at gmail.com>
> wrote:
>
> Dear Ren?,
>
> Thank you for your feedback to me. You are right, dropping the pretest
> from
> covariate if I predict change definitely makes sense to me! But the fact
> that i need to control for the starting levels of participants makes it
> obligatory for me to chose the second way, which is predicting posttest
> instead of change to have pretest scores controlled for.
>
> You also chose (1+group | word) , which is new to me. Does it intend to
> assume the effect of group to vary across words, which is something
> applicable to my data, right?
> I will discuss all this with my supervisor, and may reply here again in
> few
> days if you do not mind.
> Thank you very much
> Souheyla
> University of York
>
>
> On Fri, 22 Mar 2019 at 13:42, Ren? <bimonosom at gmail.com> wrote:
>
> Hi Souheyla,
>
> it seems to me that you will run into problems with your coding of
> change
> (gain, no gain and decline) because the 'change' is by
> definition/calculation depending on the predictor pretest.
> See, according to your coding scheme:
> Change = decline can only occur if pretest=1 (not by pretest=0).
> Change = gain can only occur if pretest = 0 (not by pretest=1)
> Change = No Gain can occur if pretest= 1 or 0
> In other words:
> If pretest = 1 then the possible outcomes can be decline or no gain
> If pretest = 0 then the possible outcomes can be gain or no gain
>
> And if the model result shows you then that the pre-test is
> significantly
> related to p(change-outcome), I guess there is no surprise in it, is it?
>
> So the first solution to this would be simply kicking the pre-test
> predictor out of the model completely, and predict:
> mod1 <- brm(Change ~ Group + (1|Subject) + (1+Group|Word),...)
> (Btw.: actually the first Hierarchical Bayes Model question I see on the
> mixed-effects mailing list :))
>
> Attempt for a further clarification on which random slopes would reflect
> the model's design:
> If you have a within-subjects design, by-subject random slopes are
> possible for the within-subject variable (e.g. if there are two sets of
> words/lists [e.g. abstract vs. concrete words] for each participant, and
> you test whether there is a performance-difference between these
> word-lists, then you can implement by-subject random slopes for words,
> because each participant has seen both sets.) If each participant has
> seen
> only one list (i.e. between subjects design) by subject random slopes
> for
> words are not appropriate, because there is no 'slope' by participant
> (i.e.
> by definition, having a slope requires at least two observations...).
> This
> is always a good rule of thumb without thinking about it too heavily :)
> Ans as you see: you can define a random slope for words:
> (1+Group|Word),
> because each word has been presented in each group (i.e. there can be a
> slope for each word). And intuitively speaking the Treatment-effect can
> vary depending on the stimuli you use, and the slope makes sense. (You
> also
> see in this example that the treatment effect can also vary by subjects,
> but in fact, this subject effect variation IS EQUAL to the effect you
> want
> to test, and having by subject group random slopes would eliminate the
> fixed effect...)
>
> Anyway, there is a second possibility to define your model, depending on
> how you want to interpret it. In the previous model you can say
> something
> about the type-of-change likelihoods depending on the treatment group.
> But
> you could implement the model as binomial as well (i.e. logistic
> regression)
>
> mod2 <- brm(posttest ~ pretest*Group + (1|Subject) + (1+Group|Word),...)
>
> And what you would expect here would be an interaction between pre-test
> and Group. For instance; if pretest=0 & treatment 1 then posttest larger
> than with pretest=0 & treatment 2; but not when pretest=1 (because this
> is
> a plausible no gain situation). And so on...
> (And in this model there are no also no further random slopes hidden in
> your design :))
> Hope this helps.
>
> Best, Ren?
>
>
> Am Do., 21. M?rz 2019 um 14:01 Uhr schrieb Souheyla GHEBGHOUB <
> souheyla.ghebghoub at gmail.com>:
>
> Dear Philip,
>
> I understand , here is the structure of my data in case it could help.
>
> I have 3 groups of participants (control, treatment1, treatment2). Each
> group was tested twice, once before treatment (pretest) and once after
> treatment (posttest).
> In each test, they were tested on knowledge of 28 words, scores are
> dichotomous (0 = unknown , 1 = known). Tests are the same.
>
> I calculated change from pretest to posttest :
> if pretest 0 and posttest 0 = no gain
> if pretest 1 and posttest 1 = no gain
> if pretest 0 and posttest 1 = gain
> if pretest 1 and posttest 0 = decline
> So I ended up with a dependent variable called Change with 3 levels
> (no_gain, gain, decline) and I tried to predict it using Group and
> Pretest
> as covariates using multinomial logit model. mod0 <- brm(Change ~
> Pretest
> +
> Group) I would like to add random effects for subjects but don't know
> what's the best form when Time factor is absent.
>
> I hope other statisticians who read this could help
> Thank you
> Souheyla
>
> [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>
> [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>
>
> [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>

	[[alternative HTML version deleted]]


From @ouhey|@@ghebghoub @end|ng |rom gm@||@com  Sat Mar 23 10:11:31 2019
From: @ouhey|@@ghebghoub @end|ng |rom gm@||@com (Souheyla GHEBGHOUB)
Date: Sat, 23 Mar 2019 09:11:31 +0000
Subject: [R-sig-ME] Random effects in multinomial regression in R?
In-Reply-To: <CAEA998g4rYVZoDQikUJXQCM14jD6D54NokXbOBVAe2LqV8V+rA@mail.gmail.com>
References: <CAEA998ggNw91Y93TEyPy=_t6JuJfsTdse+77H7WAdkXQTrgVOw@mail.gmail.com>
 <2AF9E03B-C3BC-4C3F-8BD7-053CB8037205@getmailspring.com>
 <CAEA998g4rYVZoDQikUJXQCM14jD6D54NokXbOBVAe2LqV8V+rA@mail.gmail.com>
Message-ID: <CAEA998gcf_7Tx-fBsqY76JZzK6f-WG3yURJFbb83j-boknjyyQ@mail.gmail.com>

I read that in multinomial regression, all independent variables should be
variables that we manipulate. Can I still have pretest as IV without
skewing my results?

Best,
Souheyla

On Fri, 22 Mar 2019, 23:31 Souheyla GHEBGHOUB, <souheyla.ghebghoub at gmail.com>
wrote:

> Thank you both. I will look into this and see :)
>
> Best,
> Souheyla
>
> On Fri, 22 Mar 2019, 22:02 Uanhoro, James, <uanhoro.1 at buckeyemail.osu.edu>
> wrote:
>
>> In standard regression models, the assumption is predictor variables are
>> measured without error. Test scores will have measurement error, hence
>> Doran's comment when test scores are used as covariates. See: Hausman, J.
>> (2001). Mismeasured Variables in Econometric Analysis: Problems from the
>> Right and Problems from the Left. *Journal of Economic Perspectives*,
>> *15*(4), 57?67. https://doi.org/10.1257/jep.15.4.57
>> I will note that many practitioners ignore this issue, and it is quite
>> common to use predictors measured with error. Consider the number of times
>> people use polychotomized income measures, or SES measures as predictors,
>> or some other "construct".
>> On Mar 22 2019, at 5:39 pm, Souheyla GHEBGHOUB <
>> souheyla.ghebghoub at gmail.com> wrote:
>>
>> Dear Doran,
>>
>> Could you explain more this point to me, please?
>>
>> Thank you,
>> Souheyla
>>
>> On Fri, 22 Mar 2019, 21:19 Doran, Harold, <HDoran at air.org> wrote:
>>
>> Yes, but conditioning on the pre-test means you are using a variable
>> measured with error and the estimates you obtain and now inconsistent, and
>> that?s a pretty big sin.
>>
>> On 3/22/19, 3:49 PM, "Souheyla GHEBGHOUB" <souheyla.ghebghoub at gmail.com>
>> wrote:
>>
>> Dear Ren?,
>>
>> Thank you for your feedback to me. You are right, dropping the pretest
>> from
>> covariate if I predict change definitely makes sense to me! But the fact
>> that i need to control for the starting levels of participants makes it
>> obligatory for me to chose the second way, which is predicting posttest
>> instead of change to have pretest scores controlled for.
>>
>> You also chose (1+group | word) , which is new to me. Does it intend to
>> assume the effect of group to vary across words, which is something
>> applicable to my data, right?
>> I will discuss all this with my supervisor, and may reply here again in
>> few
>> days if you do not mind.
>> Thank you very much
>> Souheyla
>> University of York
>>
>>
>> On Fri, 22 Mar 2019 at 13:42, Ren? <bimonosom at gmail.com> wrote:
>>
>> Hi Souheyla,
>>
>> it seems to me that you will run into problems with your coding of
>> change
>> (gain, no gain and decline) because the 'change' is by
>> definition/calculation depending on the predictor pretest.
>> See, according to your coding scheme:
>> Change = decline can only occur if pretest=1 (not by pretest=0).
>> Change = gain can only occur if pretest = 0 (not by pretest=1)
>> Change = No Gain can occur if pretest= 1 or 0
>> In other words:
>> If pretest = 1 then the possible outcomes can be decline or no gain
>> If pretest = 0 then the possible outcomes can be gain or no gain
>>
>> And if the model result shows you then that the pre-test is
>> significantly
>> related to p(change-outcome), I guess there is no surprise in it, is it?
>>
>> So the first solution to this would be simply kicking the pre-test
>> predictor out of the model completely, and predict:
>> mod1 <- brm(Change ~ Group + (1|Subject) + (1+Group|Word),...)
>> (Btw.: actually the first Hierarchical Bayes Model question I see on the
>> mixed-effects mailing list :))
>>
>> Attempt for a further clarification on which random slopes would reflect
>> the model's design:
>> If you have a within-subjects design, by-subject random slopes are
>> possible for the within-subject variable (e.g. if there are two sets of
>> words/lists [e.g. abstract vs. concrete words] for each participant, and
>> you test whether there is a performance-difference between these
>> word-lists, then you can implement by-subject random slopes for words,
>> because each participant has seen both sets.) If each participant has
>> seen
>> only one list (i.e. between subjects design) by subject random slopes
>> for
>> words are not appropriate, because there is no 'slope' by participant
>> (i.e.
>> by definition, having a slope requires at least two observations...).
>> This
>> is always a good rule of thumb without thinking about it too heavily :)
>> Ans as you see: you can define a random slope for words:
>> (1+Group|Word),
>> because each word has been presented in each group (i.e. there can be a
>> slope for each word). And intuitively speaking the Treatment-effect can
>> vary depending on the stimuli you use, and the slope makes sense. (You
>> also
>> see in this example that the treatment effect can also vary by subjects,
>> but in fact, this subject effect variation IS EQUAL to the effect you
>> want
>> to test, and having by subject group random slopes would eliminate the
>> fixed effect...)
>>
>> Anyway, there is a second possibility to define your model, depending on
>> how you want to interpret it. In the previous model you can say
>> something
>> about the type-of-change likelihoods depending on the treatment group.
>> But
>> you could implement the model as binomial as well (i.e. logistic
>> regression)
>>
>> mod2 <- brm(posttest ~ pretest*Group + (1|Subject) + (1+Group|Word),...)
>>
>> And what you would expect here would be an interaction between pre-test
>> and Group. For instance; if pretest=0 & treatment 1 then posttest larger
>> than with pretest=0 & treatment 2; but not when pretest=1 (because this
>> is
>> a plausible no gain situation). And so on...
>> (And in this model there are no also no further random slopes hidden in
>> your design :))
>> Hope this helps.
>>
>> Best, Ren?
>>
>>
>> Am Do., 21. M?rz 2019 um 14:01 Uhr schrieb Souheyla GHEBGHOUB <
>> souheyla.ghebghoub at gmail.com>:
>>
>> Dear Philip,
>>
>> I understand , here is the structure of my data in case it could help.
>>
>> I have 3 groups of participants (control, treatment1, treatment2). Each
>> group was tested twice, once before treatment (pretest) and once after
>> treatment (posttest).
>> In each test, they were tested on knowledge of 28 words, scores are
>> dichotomous (0 = unknown , 1 = known). Tests are the same.
>>
>> I calculated change from pretest to posttest :
>> if pretest 0 and posttest 0 = no gain
>> if pretest 1 and posttest 1 = no gain
>> if pretest 0 and posttest 1 = gain
>> if pretest 1 and posttest 0 = decline
>> So I ended up with a dependent variable called Change with 3 levels
>> (no_gain, gain, decline) and I tried to predict it using Group and
>> Pretest
>> as covariates using multinomial logit model. mod0 <- brm(Change ~
>> Pretest
>> +
>> Group) I would like to add random effects for subjects but don't know
>> what's the best form when Time factor is absent.
>>
>> I hope other statisticians who read this could help
>> Thank you
>> Souheyla
>>
>> [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>>
>> [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>>
>>
>> [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>

	[[alternative HTML version deleted]]


From b|mono@om @end|ng |rom gm@||@com  Sat Mar 23 11:00:53 2019
From: b|mono@om @end|ng |rom gm@||@com (=?UTF-8?B?UmVuw6k=?=)
Date: Sat, 23 Mar 2019 11:00:53 +0100
Subject: [R-sig-ME] Random effects in multinomial regression in R?
In-Reply-To: <CAEA998gcf_7Tx-fBsqY76JZzK6f-WG3yURJFbb83j-boknjyyQ@mail.gmail.com>
References: <CAEA998ggNw91Y93TEyPy=_t6JuJfsTdse+77H7WAdkXQTrgVOw@mail.gmail.com>
 <2AF9E03B-C3BC-4C3F-8BD7-053CB8037205@getmailspring.com>
 <CAEA998g4rYVZoDQikUJXQCM14jD6D54NokXbOBVAe2LqV8V+rA@mail.gmail.com>
 <CAEA998gcf_7Tx-fBsqY76JZzK6f-WG3yURJFbb83j-boknjyyQ@mail.gmail.com>
Message-ID: <CADcpBHN_wmSacX5=LsrMSoNwFKMZxHssMXrBW5B0VTYr68tv9g@mail.gmail.com>

Hi Souheyla,

Well, I guess in your case it is simply more elegant to leave the measured
predictor out of the fixed effects, because there is also another implied
question (i.e. about the strength of change between pre and post).

So, another possibility to re-define your model (as logistic regression)
allowing for better interpretations:
mod3<-brm(score~PrePost*Group+(1+PrePost | subject)+(1+group | words),...)

score = 0 or 1 for a given testitem
PrePost = Pre vs. Post  (basically just an indicator of the measurement
time point)
Thus, the PrePost main effect will tell, whether there is a change from pre
to post (e.g. a gaint), and you can also tell how strong it is (in odds
ratios).
And if PrePost interacts with Group, then the change (e.g. a gain) is
moderated by the treatment, which seems to be your main question.

Now in this model, you can also have by-subject random slopes for PrePost
of course (because the fixed effect of PrePost is present for every
subject).

Best, Ren?


Am Sa., 23. M?rz 2019 um 10:12 Uhr schrieb Souheyla GHEBGHOUB <
souheyla.ghebghoub at gmail.com>:

> I read that in multinomial regression, all independent variables should be
> variables that we manipulate. Can I still have pretest as IV without
> skewing my results?
>
> Best,
> Souheyla
>
> On Fri, 22 Mar 2019, 23:31 Souheyla GHEBGHOUB, <
> souheyla.ghebghoub at gmail.com>
> wrote:
>
> > Thank you both. I will look into this and see :)
> >
> > Best,
> > Souheyla
> >
> > On Fri, 22 Mar 2019, 22:02 Uanhoro, James, <
> uanhoro.1 at buckeyemail.osu.edu>
> > wrote:
> >
> >> In standard regression models, the assumption is predictor variables are
> >> measured without error. Test scores will have measurement error, hence
> >> Doran's comment when test scores are used as covariates. See: Hausman,
> J.
> >> (2001). Mismeasured Variables in Econometric Analysis: Problems from the
> >> Right and Problems from the Left. *Journal of Economic Perspectives*,
> >> *15*(4), 57?67. https://doi.org/10.1257/jep.15.4.57
> >> I will note that many practitioners ignore this issue, and it is quite
> >> common to use predictors measured with error. Consider the number of
> times
> >> people use polychotomized income measures, or SES measures as
> predictors,
> >> or some other "construct".
> >> On Mar 22 2019, at 5:39 pm, Souheyla GHEBGHOUB <
> >> souheyla.ghebghoub at gmail.com> wrote:
> >>
> >> Dear Doran,
> >>
> >> Could you explain more this point to me, please?
> >>
> >> Thank you,
> >> Souheyla
> >>
> >> On Fri, 22 Mar 2019, 21:19 Doran, Harold, <HDoran at air.org> wrote:
> >>
> >> Yes, but conditioning on the pre-test means you are using a variable
> >> measured with error and the estimates you obtain and now inconsistent,
> and
> >> that?s a pretty big sin.
> >>
> >> On 3/22/19, 3:49 PM, "Souheyla GHEBGHOUB" <souheyla.ghebghoub at gmail.com
> >
> >> wrote:
> >>
> >> Dear Ren?,
> >>
> >> Thank you for your feedback to me. You are right, dropping the pretest
> >> from
> >> covariate if I predict change definitely makes sense to me! But the fact
> >> that i need to control for the starting levels of participants makes it
> >> obligatory for me to chose the second way, which is predicting posttest
> >> instead of change to have pretest scores controlled for.
> >>
> >> You also chose (1+group | word) , which is new to me. Does it intend to
> >> assume the effect of group to vary across words, which is something
> >> applicable to my data, right?
> >> I will discuss all this with my supervisor, and may reply here again in
> >> few
> >> days if you do not mind.
> >> Thank you very much
> >> Souheyla
> >> University of York
> >>
> >>
> >> On Fri, 22 Mar 2019 at 13:42, Ren? <bimonosom at gmail.com> wrote:
> >>
> >> Hi Souheyla,
> >>
> >> it seems to me that you will run into problems with your coding of
> >> change
> >> (gain, no gain and decline) because the 'change' is by
> >> definition/calculation depending on the predictor pretest.
> >> See, according to your coding scheme:
> >> Change = decline can only occur if pretest=1 (not by pretest=0).
> >> Change = gain can only occur if pretest = 0 (not by pretest=1)
> >> Change = No Gain can occur if pretest= 1 or 0
> >> In other words:
> >> If pretest = 1 then the possible outcomes can be decline or no gain
> >> If pretest = 0 then the possible outcomes can be gain or no gain
> >>
> >> And if the model result shows you then that the pre-test is
> >> significantly
> >> related to p(change-outcome), I guess there is no surprise in it, is it?
> >>
> >> So the first solution to this would be simply kicking the pre-test
> >> predictor out of the model completely, and predict:
> >> mod1 <- brm(Change ~ Group + (1|Subject) + (1+Group|Word),...)
> >> (Btw.: actually the first Hierarchical Bayes Model question I see on the
> >> mixed-effects mailing list :))
> >>
> >> Attempt for a further clarification on which random slopes would reflect
> >> the model's design:
> >> If you have a within-subjects design, by-subject random slopes are
> >> possible for the within-subject variable (e.g. if there are two sets of
> >> words/lists [e.g. abstract vs. concrete words] for each participant, and
> >> you test whether there is a performance-difference between these
> >> word-lists, then you can implement by-subject random slopes for words,
> >> because each participant has seen both sets.) If each participant has
> >> seen
> >> only one list (i.e. between subjects design) by subject random slopes
> >> for
> >> words are not appropriate, because there is no 'slope' by participant
> >> (i.e.
> >> by definition, having a slope requires at least two observations...).
> >> This
> >> is always a good rule of thumb without thinking about it too heavily :)
> >> Ans as you see: you can define a random slope for words:
> >> (1+Group|Word),
> >> because each word has been presented in each group (i.e. there can be a
> >> slope for each word). And intuitively speaking the Treatment-effect can
> >> vary depending on the stimuli you use, and the slope makes sense. (You
> >> also
> >> see in this example that the treatment effect can also vary by subjects,
> >> but in fact, this subject effect variation IS EQUAL to the effect you
> >> want
> >> to test, and having by subject group random slopes would eliminate the
> >> fixed effect...)
> >>
> >> Anyway, there is a second possibility to define your model, depending on
> >> how you want to interpret it. In the previous model you can say
> >> something
> >> about the type-of-change likelihoods depending on the treatment group.
> >> But
> >> you could implement the model as binomial as well (i.e. logistic
> >> regression)
> >>
> >> mod2 <- brm(posttest ~ pretest*Group + (1|Subject) + (1+Group|Word),...)
> >>
> >> And what you would expect here would be an interaction between pre-test
> >> and Group. For instance; if pretest=0 & treatment 1 then posttest larger
> >> than with pretest=0 & treatment 2; but not when pretest=1 (because this
> >> is
> >> a plausible no gain situation). And so on...
> >> (And in this model there are no also no further random slopes hidden in
> >> your design :))
> >> Hope this helps.
> >>
> >> Best, Ren?
> >>
> >>
> >> Am Do., 21. M?rz 2019 um 14:01 Uhr schrieb Souheyla GHEBGHOUB <
> >> souheyla.ghebghoub at gmail.com>:
> >>
> >> Dear Philip,
> >>
> >> I understand , here is the structure of my data in case it could help.
> >>
> >> I have 3 groups of participants (control, treatment1, treatment2). Each
> >> group was tested twice, once before treatment (pretest) and once after
> >> treatment (posttest).
> >> In each test, they were tested on knowledge of 28 words, scores are
> >> dichotomous (0 = unknown , 1 = known). Tests are the same.
> >>
> >> I calculated change from pretest to posttest :
> >> if pretest 0 and posttest 0 = no gain
> >> if pretest 1 and posttest 1 = no gain
> >> if pretest 0 and posttest 1 = gain
> >> if pretest 1 and posttest 0 = decline
> >> So I ended up with a dependent variable called Change with 3 levels
> >> (no_gain, gain, decline) and I tried to predict it using Group and
> >> Pretest
> >> as covariates using multinomial logit model. mod0 <- brm(Change ~
> >> Pretest
> >> +
> >> Group) I would like to add random effects for subjects but don't know
> >> what's the best form when Time factor is absent.
> >>
> >> I hope other statisticians who read this could help
> >> Thank you
> >> Souheyla
> >>
> >> [[alternative HTML version deleted]]
> >>
> >> _______________________________________________
> >> R-sig-mixed-models at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>
> >>
> >>
> >> [[alternative HTML version deleted]]
> >>
> >> _______________________________________________
> >> R-sig-mixed-models at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>
> >>
> >>
> >>
> >> [[alternative HTML version deleted]]
> >>
> >> _______________________________________________
> >> R-sig-mixed-models at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>
> >>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From HDor@n @end|ng |rom @|r@org  Sat Mar 23 11:03:39 2019
From: HDor@n @end|ng |rom @|r@org (Doran, Harold)
Date: Sat, 23 Mar 2019 10:03:39 +0000
Subject: [R-sig-ME] Random effects in multinomial regression in R?
In-Reply-To: <CAEA998gcf_7Tx-fBsqY76JZzK6f-WG3yURJFbb83j-boknjyyQ@mail.gmail.com>
References: <CAEA998ggNw91Y93TEyPy=_t6JuJfsTdse+77H7WAdkXQTrgVOw@mail.gmail.com>
 <2AF9E03B-C3BC-4C3F-8BD7-053CB8037205@getmailspring.com>
 <CAEA998g4rYVZoDQikUJXQCM14jD6D54NokXbOBVAe2LqV8V+rA@mail.gmail.com>
 <CAEA998gcf_7Tx-fBsqY76JZzK6f-WG3yURJFbb83j-boknjyyQ@mail.gmail.com>
Message-ID: <D8BB7BD5.57C5D%hdoran@air.org>

No, the ?right? statistical term to use here is that your parameter
estimates will be inconsistent. We?re deviating some from the purpose of
this list, but it is a helpful discussion. What James notes is also true,
this is often ignored in practice, which is a huge problem. I propose it?s
often ignored not because the issue is not well-known, but because there
is not (widely distributed) software that easily implements the types of
corrections needed in this scenario.



On 3/23/19, 5:11 AM, "Souheyla GHEBGHOUB" <souheyla.ghebghoub at gmail.com>
wrote:

>I read that in multinomial regression, all independent variables should be
>variables that we manipulate. Can I still have pretest as IV without
>skewing my results?
>
>Best,
>Souheyla
>
>On Fri, 22 Mar 2019, 23:31 Souheyla GHEBGHOUB,
><souheyla.ghebghoub at gmail.com>
>wrote:
>
>> Thank you both. I will look into this and see :)
>>
>> Best,
>> Souheyla
>>
>> On Fri, 22 Mar 2019, 22:02 Uanhoro, James,
>><uanhoro.1 at buckeyemail.osu.edu>
>> wrote:
>>
>>> In standard regression models, the assumption is predictor variables
>>>are
>>> measured without error. Test scores will have measurement error, hence
>>> Doran's comment when test scores are used as covariates. See: Hausman,
>>>J.
>>> (2001). Mismeasured Variables in Econometric Analysis: Problems from
>>>the
>>> Right and Problems from the Left. *Journal of Economic Perspectives*,
>>> *15*(4), 57?67. https://doi.org/10.1257/jep.15.4.57
>>> I will note that many practitioners ignore this issue, and it is quite
>>> common to use predictors measured with error. Consider the number of
>>>times
>>> people use polychotomized income measures, or SES measures as
>>>predictors,
>>> or some other "construct".
>>> On Mar 22 2019, at 5:39 pm, Souheyla GHEBGHOUB <
>>> souheyla.ghebghoub at gmail.com> wrote:
>>>
>>> Dear Doran,
>>>
>>> Could you explain more this point to me, please?
>>>
>>> Thank you,
>>> Souheyla
>>>
>>> On Fri, 22 Mar 2019, 21:19 Doran, Harold, <HDoran at air.org> wrote:
>>>
>>> Yes, but conditioning on the pre-test means you are using a variable
>>> measured with error and the estimates you obtain and now inconsistent,
>>>and
>>> that?s a pretty big sin.
>>>
>>> On 3/22/19, 3:49 PM, "Souheyla GHEBGHOUB"
>>><souheyla.ghebghoub at gmail.com>
>>> wrote:
>>>
>>> Dear Ren?,
>>>
>>> Thank you for your feedback to me. You are right, dropping the pretest
>>> from
>>> covariate if I predict change definitely makes sense to me! But the
>>>fact
>>> that i need to control for the starting levels of participants makes it
>>> obligatory for me to chose the second way, which is predicting posttest
>>> instead of change to have pretest scores controlled for.
>>>
>>> You also chose (1+group | word) , which is new to me. Does it intend to
>>> assume the effect of group to vary across words, which is something
>>> applicable to my data, right?
>>> I will discuss all this with my supervisor, and may reply here again in
>>> few
>>> days if you do not mind.
>>> Thank you very much
>>> Souheyla
>>> University of York
>>>
>>>
>>> On Fri, 22 Mar 2019 at 13:42, Ren? <bimonosom at gmail.com> wrote:
>>>
>>> Hi Souheyla,
>>>
>>> it seems to me that you will run into problems with your coding of
>>> change
>>> (gain, no gain and decline) because the 'change' is by
>>> definition/calculation depending on the predictor pretest.
>>> See, according to your coding scheme:
>>> Change = decline can only occur if pretest=1 (not by pretest=0).
>>> Change = gain can only occur if pretest = 0 (not by pretest=1)
>>> Change = No Gain can occur if pretest= 1 or 0
>>> In other words:
>>> If pretest = 1 then the possible outcomes can be decline or no gain
>>> If pretest = 0 then the possible outcomes can be gain or no gain
>>>
>>> And if the model result shows you then that the pre-test is
>>> significantly
>>> related to p(change-outcome), I guess there is no surprise in it, is
>>>it?
>>>
>>> So the first solution to this would be simply kicking the pre-test
>>> predictor out of the model completely, and predict:
>>> mod1 <- brm(Change ~ Group + (1|Subject) + (1+Group|Word),...)
>>> (Btw.: actually the first Hierarchical Bayes Model question I see on
>>>the
>>> mixed-effects mailing list :))
>>>
>>> Attempt for a further clarification on which random slopes would
>>>reflect
>>> the model's design:
>>> If you have a within-subjects design, by-subject random slopes are
>>> possible for the within-subject variable (e.g. if there are two sets of
>>> words/lists [e.g. abstract vs. concrete words] for each participant,
>>>and
>>> you test whether there is a performance-difference between these
>>> word-lists, then you can implement by-subject random slopes for words,
>>> because each participant has seen both sets.) If each participant has
>>> seen
>>> only one list (i.e. between subjects design) by subject random slopes
>>> for
>>> words are not appropriate, because there is no 'slope' by participant
>>> (i.e.
>>> by definition, having a slope requires at least two observations...).
>>> This
>>> is always a good rule of thumb without thinking about it too heavily :)
>>> Ans as you see: you can define a random slope for words:
>>> (1+Group|Word),
>>> because each word has been presented in each group (i.e. there can be a
>>> slope for each word). And intuitively speaking the Treatment-effect can
>>> vary depending on the stimuli you use, and the slope makes sense. (You
>>> also
>>> see in this example that the treatment effect can also vary by
>>>subjects,
>>> but in fact, this subject effect variation IS EQUAL to the effect you
>>> want
>>> to test, and having by subject group random slopes would eliminate the
>>> fixed effect...)
>>>
>>> Anyway, there is a second possibility to define your model, depending
>>>on
>>> how you want to interpret it. In the previous model you can say
>>> something
>>> about the type-of-change likelihoods depending on the treatment group.
>>> But
>>> you could implement the model as binomial as well (i.e. logistic
>>> regression)
>>>
>>> mod2 <- brm(posttest ~ pretest*Group + (1|Subject) +
>>>(1+Group|Word),...)
>>>
>>> And what you would expect here would be an interaction between pre-test
>>> and Group. For instance; if pretest=0 & treatment 1 then posttest
>>>larger
>>> than with pretest=0 & treatment 2; but not when pretest=1 (because this
>>> is
>>> a plausible no gain situation). And so on...
>>> (And in this model there are no also no further random slopes hidden in
>>> your design :))
>>> Hope this helps.
>>>
>>> Best, Ren?
>>>
>>>
>>> Am Do., 21. M?rz 2019 um 14:01 Uhr schrieb Souheyla GHEBGHOUB <
>>> souheyla.ghebghoub at gmail.com>:
>>>
>>> Dear Philip,
>>>
>>> I understand , here is the structure of my data in case it could help.
>>>
>>> I have 3 groups of participants (control, treatment1, treatment2). Each
>>> group was tested twice, once before treatment (pretest) and once after
>>> treatment (posttest).
>>> In each test, they were tested on knowledge of 28 words, scores are
>>> dichotomous (0 = unknown , 1 = known). Tests are the same.
>>>
>>> I calculated change from pretest to posttest :
>>> if pretest 0 and posttest 0 = no gain
>>> if pretest 1 and posttest 1 = no gain
>>> if pretest 0 and posttest 1 = gain
>>> if pretest 1 and posttest 0 = decline
>>> So I ended up with a dependent variable called Change with 3 levels
>>> (no_gain, gain, decline) and I tried to predict it using Group and
>>> Pretest
>>> as covariates using multinomial logit model. mod0 <- brm(Change ~
>>> Pretest
>>> +
>>> Group) I would like to add random effects for subjects but don't know
>>> what's the best form when Time factor is absent.
>>>
>>> I hope other statisticians who read this could help
>>> Thank you
>>> Souheyla
>>>
>>> [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>>
>>>
>>> [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>>
>>>
>>>
>>> [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>>
>
>	[[alternative HTML version deleted]]
>
>_______________________________________________
>R-sig-mixed-models at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From @ouhey|@@ghebghoub @end|ng |rom gm@||@com  Sat Mar 23 11:58:54 2019
From: @ouhey|@@ghebghoub @end|ng |rom gm@||@com (Souheyla GHEBGHOUB)
Date: Sat, 23 Mar 2019 10:58:54 +0000
Subject: [R-sig-ME] Random effects in multinomial regression in R?
In-Reply-To: <CADcpBHN_wmSacX5=LsrMSoNwFKMZxHssMXrBW5B0VTYr68tv9g@mail.gmail.com>
References: <CAEA998ggNw91Y93TEyPy=_t6JuJfsTdse+77H7WAdkXQTrgVOw@mail.gmail.com>
 <2AF9E03B-C3BC-4C3F-8BD7-053CB8037205@getmailspring.com>
 <CAEA998g4rYVZoDQikUJXQCM14jD6D54NokXbOBVAe2LqV8V+rA@mail.gmail.com>
 <CAEA998gcf_7Tx-fBsqY76JZzK6f-WG3yURJFbb83j-boknjyyQ@mail.gmail.com>
 <CADcpBHN_wmSacX5=LsrMSoNwFKMZxHssMXrBW5B0VTYr68tv9g@mail.gmail.com>
Message-ID: <CAEA998gWs2PO1NDjfn94NWR2U_AmYHDs3K+drhRAW8GLEvvtSw@mail.gmail.com>

Dear Ren?,

Thank you for the feedback. Actually, my model was originally like you
suggested now (except for slopes I had PrePost without 1 in both words and
subjects. I called PrePost as "Time". I will read more about the 1+prepost
form you mentioned.

The reason why I gave up this model and looked for something else is the
fact that each subject has 28 words tested twice (pre&post) and I was not
too sure whether such model will take into consideration differences
between pre & post at the level of every word of each participant (which is
what I want) instead of merely comparing every participant's pre mean sores
of 28 words against his post mean score (which is what i should avoid), here
is a short example as to why:

 If a participant has  got 5 correct words out of 28 in both pretest and
posttest, there will be multiple interpretations:  e.g. They could refer to
the same words (i.e. 0 gain), or they could be totally new words (i.e. 5
gains) ...etc , hence I am not sure whether such model of pretest vs
posttest will compare each subject score of each word from pretest to
posttest then base its analysis on these score changes instead of comparing
the sum scores between pre and post and which likely skew results.

I posted about this in stackexchange 3 months ago and was told that it does
compare word to word for every participant, but I am still not confident
enough to use it because all the accurateness of the results and discussion
chapters of my PhD thesis will be based on this decision.

I look forward to receive feedback from you and any member reading this,
Souheyla
University of York

On Sat, 23 Mar 2019, 10:01 Ren?, <bimonosom at gmail.com> wrote:

> Hi Souheyla,
>
> Well, I guess in your case it is simply more elegant to leave the measured
> predictor out of the fixed effects, because there is also another implied
> question (i.e. about the strength of change between pre and post).
>
> So, another possibility to re-define your model (as logistic regression)
> allowing for better interpretations:
> mod3<-brm(score~PrePost*Group+(1+PrePost | subject)+(1+group | words),...)
>
> score = 0 or 1 for a given testitem
> PrePost = Pre vs. Post  (basically just an indicator of the measurement
> time point)
> Thus, the PrePost main effect will tell, whether there is a change from
> pre to post (e.g. a gaint), and you can also tell how strong it is (in odds
> ratios).
> And if PrePost interacts with Group, then the change (e.g. a gain) is
> moderated by the treatment, which seems to be your main question.
>
> Now in this model, you can also have by-subject random slopes for PrePost
> of course (because the fixed effect of PrePost is present for every
> subject).
>
> Best, Ren?
>
>
> Am Sa., 23. M?rz 2019 um 10:12 Uhr schrieb Souheyla GHEBGHOUB <
> souheyla.ghebghoub at gmail.com>:
>
>> I read that in multinomial regression, all independent variables should be
>> variables that we manipulate. Can I still have pretest as IV without
>> skewing my results?
>>
>> Best,
>> Souheyla
>>
>> On Fri, 22 Mar 2019, 23:31 Souheyla GHEBGHOUB, <
>> souheyla.ghebghoub at gmail.com>
>> wrote:
>>
>> > Thank you both. I will look into this and see :)
>> >
>> > Best,
>> > Souheyla
>> >
>> > On Fri, 22 Mar 2019, 22:02 Uanhoro, James, <
>> uanhoro.1 at buckeyemail.osu.edu>
>> > wrote:
>> >
>> >> In standard regression models, the assumption is predictor variables
>> are
>> >> measured without error. Test scores will have measurement error, hence
>> >> Doran's comment when test scores are used as covariates. See: Hausman,
>> J.
>> >> (2001). Mismeasured Variables in Econometric Analysis: Problems from
>> the
>> >> Right and Problems from the Left. *Journal of Economic Perspectives*,
>> >> *15*(4), 57?67. https://doi.org/10.1257/jep.15.4.57
>> >> I will note that many practitioners ignore this issue, and it is quite
>> >> common to use predictors measured with error. Consider the number of
>> times
>> >> people use polychotomized income measures, or SES measures as
>> predictors,
>> >> or some other "construct".
>> >> On Mar 22 2019, at 5:39 pm, Souheyla GHEBGHOUB <
>> >> souheyla.ghebghoub at gmail.com> wrote:
>> >>
>> >> Dear Doran,
>> >>
>> >> Could you explain more this point to me, please?
>> >>
>> >> Thank you,
>> >> Souheyla
>> >>
>> >> On Fri, 22 Mar 2019, 21:19 Doran, Harold, <HDoran at air.org> wrote:
>> >>
>> >> Yes, but conditioning on the pre-test means you are using a variable
>> >> measured with error and the estimates you obtain and now inconsistent,
>> and
>> >> that?s a pretty big sin.
>> >>
>> >> On 3/22/19, 3:49 PM, "Souheyla GHEBGHOUB" <
>> souheyla.ghebghoub at gmail.com>
>> >> wrote:
>> >>
>> >> Dear Ren?,
>> >>
>> >> Thank you for your feedback to me. You are right, dropping the pretest
>> >> from
>> >> covariate if I predict change definitely makes sense to me! But the
>> fact
>> >> that i need to control for the starting levels of participants makes it
>> >> obligatory for me to chose the second way, which is predicting posttest
>> >> instead of change to have pretest scores controlled for.
>> >>
>> >> You also chose (1+group | word) , which is new to me. Does it intend to
>> >> assume the effect of group to vary across words, which is something
>> >> applicable to my data, right?
>> >> I will discuss all this with my supervisor, and may reply here again in
>> >> few
>> >> days if you do not mind.
>> >> Thank you very much
>> >> Souheyla
>> >> University of York
>> >>
>> >>
>> >> On Fri, 22 Mar 2019 at 13:42, Ren? <bimonosom at gmail.com> wrote:
>> >>
>> >> Hi Souheyla,
>> >>
>> >> it seems to me that you will run into problems with your coding of
>> >> change
>> >> (gain, no gain and decline) because the 'change' is by
>> >> definition/calculation depending on the predictor pretest.
>> >> See, according to your coding scheme:
>> >> Change = decline can only occur if pretest=1 (not by pretest=0).
>> >> Change = gain can only occur if pretest = 0 (not by pretest=1)
>> >> Change = No Gain can occur if pretest= 1 or 0
>> >> In other words:
>> >> If pretest = 1 then the possible outcomes can be decline or no gain
>> >> If pretest = 0 then the possible outcomes can be gain or no gain
>> >>
>> >> And if the model result shows you then that the pre-test is
>> >> significantly
>> >> related to p(change-outcome), I guess there is no surprise in it, is
>> it?
>> >>
>> >> So the first solution to this would be simply kicking the pre-test
>> >> predictor out of the model completely, and predict:
>> >> mod1 <- brm(Change ~ Group + (1|Subject) + (1+Group|Word),...)
>> >> (Btw.: actually the first Hierarchical Bayes Model question I see on
>> the
>> >> mixed-effects mailing list :))
>> >>
>> >> Attempt for a further clarification on which random slopes would
>> reflect
>> >> the model's design:
>> >> If you have a within-subjects design, by-subject random slopes are
>> >> possible for the within-subject variable (e.g. if there are two sets of
>> >> words/lists [e.g. abstract vs. concrete words] for each participant,
>> and
>> >> you test whether there is a performance-difference between these
>> >> word-lists, then you can implement by-subject random slopes for words,
>> >> because each participant has seen both sets.) If each participant has
>> >> seen
>> >> only one list (i.e. between subjects design) by subject random slopes
>> >> for
>> >> words are not appropriate, because there is no 'slope' by participant
>> >> (i.e.
>> >> by definition, having a slope requires at least two observations...).
>> >> This
>> >> is always a good rule of thumb without thinking about it too heavily :)
>> >> Ans as you see: you can define a random slope for words:
>> >> (1+Group|Word),
>> >> because each word has been presented in each group (i.e. there can be a
>> >> slope for each word). And intuitively speaking the Treatment-effect can
>> >> vary depending on the stimuli you use, and the slope makes sense. (You
>> >> also
>> >> see in this example that the treatment effect can also vary by
>> subjects,
>> >> but in fact, this subject effect variation IS EQUAL to the effect you
>> >> want
>> >> to test, and having by subject group random slopes would eliminate the
>> >> fixed effect...)
>> >>
>> >> Anyway, there is a second possibility to define your model, depending
>> on
>> >> how you want to interpret it. In the previous model you can say
>> >> something
>> >> about the type-of-change likelihoods depending on the treatment group.
>> >> But
>> >> you could implement the model as binomial as well (i.e. logistic
>> >> regression)
>> >>
>> >> mod2 <- brm(posttest ~ pretest*Group + (1|Subject) +
>> (1+Group|Word),...)
>> >>
>> >> And what you would expect here would be an interaction between pre-test
>> >> and Group. For instance; if pretest=0 & treatment 1 then posttest
>> larger
>> >> than with pretest=0 & treatment 2; but not when pretest=1 (because this
>> >> is
>> >> a plausible no gain situation). And so on...
>> >> (And in this model there are no also no further random slopes hidden in
>> >> your design :))
>> >> Hope this helps.
>> >>
>> >> Best, Ren?
>> >>
>> >>
>> >> Am Do., 21. M?rz 2019 um 14:01 Uhr schrieb Souheyla GHEBGHOUB <
>> >> souheyla.ghebghoub at gmail.com>:
>> >>
>> >> Dear Philip,
>> >>
>> >> I understand , here is the structure of my data in case it could help.
>> >>
>> >> I have 3 groups of participants (control, treatment1, treatment2). Each
>> >> group was tested twice, once before treatment (pretest) and once after
>> >> treatment (posttest).
>> >> In each test, they were tested on knowledge of 28 words, scores are
>> >> dichotomous (0 = unknown , 1 = known). Tests are the same.
>> >>
>> >> I calculated change from pretest to posttest :
>> >> if pretest 0 and posttest 0 = no gain
>> >> if pretest 1 and posttest 1 = no gain
>> >> if pretest 0 and posttest 1 = gain
>> >> if pretest 1 and posttest 0 = decline
>> >> So I ended up with a dependent variable called Change with 3 levels
>> >> (no_gain, gain, decline) and I tried to predict it using Group and
>> >> Pretest
>> >> as covariates using multinomial logit model. mod0 <- brm(Change ~
>> >> Pretest
>> >> +
>> >> Group) I would like to add random effects for subjects but don't know
>> >> what's the best form when Time factor is absent.
>> >>
>> >> I hope other statisticians who read this could help
>> >> Thank you
>> >> Souheyla
>> >>
>> >> [[alternative HTML version deleted]]
>> >>
>> >> _______________________________________________
>> >> R-sig-mixed-models at r-project.org mailing list
>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >>
>> >>
>> >>
>> >> [[alternative HTML version deleted]]
>> >>
>> >> _______________________________________________
>> >> R-sig-mixed-models at r-project.org mailing list
>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >>
>> >>
>> >>
>> >>
>> >> [[alternative HTML version deleted]]
>> >>
>> >> _______________________________________________
>> >> R-sig-mixed-models at r-project.org mailing list
>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >>
>> >>
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From b|mono@om @end|ng |rom gm@||@com  Sat Mar 23 13:46:27 2019
From: b|mono@om @end|ng |rom gm@||@com (=?UTF-8?B?UmVuw6k=?=)
Date: Sat, 23 Mar 2019 13:46:27 +0100
Subject: [R-sig-ME] Random effects in multinomial regression in R?
In-Reply-To: <CAEA998gWs2PO1NDjfn94NWR2U_AmYHDs3K+drhRAW8GLEvvtSw@mail.gmail.com>
References: <CAEA998ggNw91Y93TEyPy=_t6JuJfsTdse+77H7WAdkXQTrgVOw@mail.gmail.com>
 <2AF9E03B-C3BC-4C3F-8BD7-053CB8037205@getmailspring.com>
 <CAEA998g4rYVZoDQikUJXQCM14jD6D54NokXbOBVAe2LqV8V+rA@mail.gmail.com>
 <CAEA998gcf_7Tx-fBsqY76JZzK6f-WG3yURJFbb83j-boknjyyQ@mail.gmail.com>
 <CADcpBHN_wmSacX5=LsrMSoNwFKMZxHssMXrBW5B0VTYr68tv9g@mail.gmail.com>
 <CAEA998gWs2PO1NDjfn94NWR2U_AmYHDs3K+drhRAW8GLEvvtSw@mail.gmail.com>
Message-ID: <CADcpBHMrpJ7uevVwH_4BvUn15mSaNzW865Fx77s0up8u6thy-A@mail.gmail.com>

Hey, things clear up. Thanks for the picture. I want to challenge your
concerns:
First, you see that observing a main effect of PrePost (or "Time") in mod3
can only mean two things: 1) performance goes up (positive effect); or 2)
performance goes down (negative effect), in general. Saying something like
this, is what the model is defined for. The conclusion will be this
"general increase" or "general decrease" or "no evidence here". If you have
a different question from that (i.e. on an item level), then you should
specify it in more detail (see below).
There are three issues in-between the lines of your questions:
1. is it a statistical concern you have?
2. is it an actual theoretical question you have?
3. is it a matter of making a non-result to a result?

1. The statistical view:
Counter-question: who would ever assume that a 1 score for a word in a
pre-test will remain constant until eternity. And why should it? The answer
is: Nobody, and this is the reason statistics (probability theory) exists
:). So the first simple answer to your question is you do not need to test
whether observed gains from pre-to-post are 'genuine' (only from 0 to 1,
without decline cases) because 'nature' guarantees that there will be
decline somewhere. That's "randomness" :)  But the question is, what's
stronger, gain or decline? See... and there is no problem in it: a general
main effect (e.g. an overall gain) still is an overall gain, even if some
cases decline.

2. Theoretical view:
If, however, such special item dynamics are theoretized in advance, then
simply test it :) For instance, the assumption whether the treatment leads
to gain on abstract words, but to decline on concrete words, then should
find into the model by coding the factor AbsCon (abstract vs. concrete
words)  as fixed effect (assuming a within participants manipulation).
mod4<-brm(score~PrePost*Group*AbsCon+(1+PrePost*AbsCon| subject) +
(1+group|words),...)
(Note: the 1+XX|subject just means random intercept  for subject (1+) plus
slope for XX on subject; and writing (1+group|words) is the same as
writing (group|words),
but you can estimate the slope without the (word) intercept by writing
(0+group|words))

Without having such a theoretical account testing for can also be done via:
mod4<-brm(score~PrePost*Group*words+(1+PrePost*words| subject),...)
But you will hardly be able to interpret the interactions in this model
because words alone has 28 levels.

3. But, your example seems to suggest a special case... (i.e. an actual
Null-Effect):
"If a participant has  got 5 correct words out of 28 in both pretest and
posttest"
then there would be no general effect of PrePost in the model above
(generalizing to all participants now). And searching for "deeper" model
checks looks like rescuing all effects you can get (post hoc). But of
course, making an argument like there still is an effect, namely for 5
specific words, which is not observable because there is also a detriment
for other 5 words is possible, but requires a solid theory which explicitly
predicts this interaction, and an experiment which was explicitly designed
to test this interaction. (point 2)

So... if it is point 2 you got... Then go ahead :) test it in a meaningful
way. Otherwise, simply treat this "effect by words" interaction as random
slope (1+group|words), or btw. (1+PrePost*Group|words) is also possible...,
which is basically 'statistically' integrating the idea that the
treatment*time effects vary (randomly) between stimuli. And doing this in
the random effects has the notion of "generalizing" estimation error in the
population, and should be preferred to implementing those in the fixed
effects, if the 28 words can be seen as a random (non-special) stimulus
sample. If this is not the case, then consider coding the "special" thing
about the words as fixed effects (e.g. if you want to use the same design
again, for testing something, while controlling for stimulus specifics).

Best, Ren?

Am Sa., 23. M?rz 2019 um 11:59 Uhr schrieb Souheyla GHEBGHOUB <
souheyla.ghebghoub at gmail.com>:

> Dear Ren?,
>
> Thank you for the feedback. Actually, my model was originally like you
> suggested now (except for slopes I had PrePost without 1 in both words and
> subjects. I called PrePost as "Time". I will read more about the 1+prepost
> form you mentioned.
>
> The reason why I gave up this model and looked for something else is the
> fact that each subject has 28 words tested twice (pre&post) and I was not
> too sure whether such model will take into consideration differences
> between pre & post at the level of every word of each participant (which is
> what I want) instead of merely comparing every participant's pre mean sores
> of 28 words against his post mean score (which is what i should avoid), here
> is a short example as to why:
>
>  If a participant has  got 5 correct words out of 28 in both pretest and
> posttest, there will be multiple interpretations:  e.g. They could refer to
> the same words (i.e. 0 gain), or they could be totally new words (i.e. 5
> gains) ...etc , hence I am not sure whether such model of pretest vs
> posttest will compare each subject score of each word from pretest to
> posttest then base its analysis on these score changes instead of comparing
> the sum scores between pre and post and which likely skew results.
>
> I posted about this in stackexchange 3 months ago and was told that it
> does compare word to word for every participant, but I am still not
> confident enough to use it because all the accurateness of the results and
> discussion chapters of my PhD thesis will be based on this decision.
>
> I look forward to receive feedback from you and any member reading this,
> Souheyla
> University of York
>
> On Sat, 23 Mar 2019, 10:01 Ren?, <bimonosom at gmail.com> wrote:
>
>> Hi Souheyla,
>>
>> Well, I guess in your case it is simply more elegant to leave the
>> measured predictor out of the fixed effects, because there is also another
>> implied question (i.e. about the strength of change between pre and post).
>>
>> So, another possibility to re-define your model (as logistic regression)
>> allowing for better interpretations:
>> mod3<-brm(score~PrePost*Group+(1+PrePost | subject)+(1+group | words),...)
>>
>> score = 0 or 1 for a given testitem
>> PrePost = Pre vs. Post  (basically just an indicator of the measurement
>> time point)
>> Thus, the PrePost main effect will tell, whether there is a change from
>> pre to post (e.g. a gaint), and you can also tell how strong it is (in odds
>> ratios).
>> And if PrePost interacts with Group, then the change (e.g. a gain) is
>> moderated by the treatment, which seems to be your main question.
>>
>> Now in this model, you can also have by-subject random slopes for PrePost
>> of course (because the fixed effect of PrePost is present for every
>> subject).
>>
>> Best, Ren?
>>
>>
>> Am Sa., 23. M?rz 2019 um 10:12 Uhr schrieb Souheyla GHEBGHOUB <
>> souheyla.ghebghoub at gmail.com>:
>>
>>> I read that in multinomial regression, all independent variables should
>>> be
>>> variables that we manipulate. Can I still have pretest as IV without
>>> skewing my results?
>>>
>>> Best,
>>> Souheyla
>>>
>>> On Fri, 22 Mar 2019, 23:31 Souheyla GHEBGHOUB, <
>>> souheyla.ghebghoub at gmail.com>
>>> wrote:
>>>
>>> > Thank you both. I will look into this and see :)
>>> >
>>> > Best,
>>> > Souheyla
>>> >
>>> > On Fri, 22 Mar 2019, 22:02 Uanhoro, James, <
>>> uanhoro.1 at buckeyemail.osu.edu>
>>> > wrote:
>>> >
>>> >> In standard regression models, the assumption is predictor variables
>>> are
>>> >> measured without error. Test scores will have measurement error, hence
>>> >> Doran's comment when test scores are used as covariates. See:
>>> Hausman, J.
>>> >> (2001). Mismeasured Variables in Econometric Analysis: Problems from
>>> the
>>> >> Right and Problems from the Left. *Journal of Economic Perspectives*,
>>> >> *15*(4), 57?67. https://doi.org/10.1257/jep.15.4.57
>>> >> I will note that many practitioners ignore this issue, and it is quite
>>> >> common to use predictors measured with error. Consider the number of
>>> times
>>> >> people use polychotomized income measures, or SES measures as
>>> predictors,
>>> >> or some other "construct".
>>> >> On Mar 22 2019, at 5:39 pm, Souheyla GHEBGHOUB <
>>> >> souheyla.ghebghoub at gmail.com> wrote:
>>> >>
>>> >> Dear Doran,
>>> >>
>>> >> Could you explain more this point to me, please?
>>> >>
>>> >> Thank you,
>>> >> Souheyla
>>> >>
>>> >> On Fri, 22 Mar 2019, 21:19 Doran, Harold, <HDoran at air.org> wrote:
>>> >>
>>> >> Yes, but conditioning on the pre-test means you are using a variable
>>> >> measured with error and the estimates you obtain and now
>>> inconsistent, and
>>> >> that?s a pretty big sin.
>>> >>
>>> >> On 3/22/19, 3:49 PM, "Souheyla GHEBGHOUB" <
>>> souheyla.ghebghoub at gmail.com>
>>> >> wrote:
>>> >>
>>> >> Dear Ren?,
>>> >>
>>> >> Thank you for your feedback to me. You are right, dropping the pretest
>>> >> from
>>> >> covariate if I predict change definitely makes sense to me! But the
>>> fact
>>> >> that i need to control for the starting levels of participants makes
>>> it
>>> >> obligatory for me to chose the second way, which is predicting
>>> posttest
>>> >> instead of change to have pretest scores controlled for.
>>> >>
>>> >> You also chose (1+group | word) , which is new to me. Does it intend
>>> to
>>> >> assume the effect of group to vary across words, which is something
>>> >> applicable to my data, right?
>>> >> I will discuss all this with my supervisor, and may reply here again
>>> in
>>> >> few
>>> >> days if you do not mind.
>>> >> Thank you very much
>>> >> Souheyla
>>> >> University of York
>>> >>
>>> >>
>>> >> On Fri, 22 Mar 2019 at 13:42, Ren? <bimonosom at gmail.com> wrote:
>>> >>
>>> >> Hi Souheyla,
>>> >>
>>> >> it seems to me that you will run into problems with your coding of
>>> >> change
>>> >> (gain, no gain and decline) because the 'change' is by
>>> >> definition/calculation depending on the predictor pretest.
>>> >> See, according to your coding scheme:
>>> >> Change = decline can only occur if pretest=1 (not by pretest=0).
>>> >> Change = gain can only occur if pretest = 0 (not by pretest=1)
>>> >> Change = No Gain can occur if pretest= 1 or 0
>>> >> In other words:
>>> >> If pretest = 1 then the possible outcomes can be decline or no gain
>>> >> If pretest = 0 then the possible outcomes can be gain or no gain
>>> >>
>>> >> And if the model result shows you then that the pre-test is
>>> >> significantly
>>> >> related to p(change-outcome), I guess there is no surprise in it, is
>>> it?
>>> >>
>>> >> So the first solution to this would be simply kicking the pre-test
>>> >> predictor out of the model completely, and predict:
>>> >> mod1 <- brm(Change ~ Group + (1|Subject) + (1+Group|Word),...)
>>> >> (Btw.: actually the first Hierarchical Bayes Model question I see on
>>> the
>>> >> mixed-effects mailing list :))
>>> >>
>>> >> Attempt for a further clarification on which random slopes would
>>> reflect
>>> >> the model's design:
>>> >> If you have a within-subjects design, by-subject random slopes are
>>> >> possible for the within-subject variable (e.g. if there are two sets
>>> of
>>> >> words/lists [e.g. abstract vs. concrete words] for each participant,
>>> and
>>> >> you test whether there is a performance-difference between these
>>> >> word-lists, then you can implement by-subject random slopes for words,
>>> >> because each participant has seen both sets.) If each participant has
>>> >> seen
>>> >> only one list (i.e. between subjects design) by subject random slopes
>>> >> for
>>> >> words are not appropriate, because there is no 'slope' by participant
>>> >> (i.e.
>>> >> by definition, having a slope requires at least two observations...).
>>> >> This
>>> >> is always a good rule of thumb without thinking about it too heavily
>>> :)
>>> >> Ans as you see: you can define a random slope for words:
>>> >> (1+Group|Word),
>>> >> because each word has been presented in each group (i.e. there can be
>>> a
>>> >> slope for each word). And intuitively speaking the Treatment-effect
>>> can
>>> >> vary depending on the stimuli you use, and the slope makes sense. (You
>>> >> also
>>> >> see in this example that the treatment effect can also vary by
>>> subjects,
>>> >> but in fact, this subject effect variation IS EQUAL to the effect you
>>> >> want
>>> >> to test, and having by subject group random slopes would eliminate the
>>> >> fixed effect...)
>>> >>
>>> >> Anyway, there is a second possibility to define your model, depending
>>> on
>>> >> how you want to interpret it. In the previous model you can say
>>> >> something
>>> >> about the type-of-change likelihoods depending on the treatment group.
>>> >> But
>>> >> you could implement the model as binomial as well (i.e. logistic
>>> >> regression)
>>> >>
>>> >> mod2 <- brm(posttest ~ pretest*Group + (1|Subject) +
>>> (1+Group|Word),...)
>>> >>
>>> >> And what you would expect here would be an interaction between
>>> pre-test
>>> >> and Group. For instance; if pretest=0 & treatment 1 then posttest
>>> larger
>>> >> than with pretest=0 & treatment 2; but not when pretest=1 (because
>>> this
>>> >> is
>>> >> a plausible no gain situation). And so on...
>>> >> (And in this model there are no also no further random slopes hidden
>>> in
>>> >> your design :))
>>> >> Hope this helps.
>>> >>
>>> >> Best, Ren?
>>> >>
>>> >>
>>> >> Am Do., 21. M?rz 2019 um 14:01 Uhr schrieb Souheyla GHEBGHOUB <
>>> >> souheyla.ghebghoub at gmail.com>:
>>> >>
>>> >> Dear Philip,
>>> >>
>>> >> I understand , here is the structure of my data in case it could help.
>>> >>
>>> >> I have 3 groups of participants (control, treatment1, treatment2).
>>> Each
>>> >> group was tested twice, once before treatment (pretest) and once after
>>> >> treatment (posttest).
>>> >> In each test, they were tested on knowledge of 28 words, scores are
>>> >> dichotomous (0 = unknown , 1 = known). Tests are the same.
>>> >>
>>> >> I calculated change from pretest to posttest :
>>> >> if pretest 0 and posttest 0 = no gain
>>> >> if pretest 1 and posttest 1 = no gain
>>> >> if pretest 0 and posttest 1 = gain
>>> >> if pretest 1 and posttest 0 = decline
>>> >> So I ended up with a dependent variable called Change with 3 levels
>>> >> (no_gain, gain, decline) and I tried to predict it using Group and
>>> >> Pretest
>>> >> as covariates using multinomial logit model. mod0 <- brm(Change ~
>>> >> Pretest
>>> >> +
>>> >> Group) I would like to add random effects for subjects but don't know
>>> >> what's the best form when Time factor is absent.
>>> >>
>>> >> I hope other statisticians who read this could help
>>> >> Thank you
>>> >> Souheyla
>>> >>
>>> >> [[alternative HTML version deleted]]
>>> >>
>>> >> _______________________________________________
>>> >> R-sig-mixed-models at r-project.org mailing list
>>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>> >>
>>> >>
>>> >>
>>> >> [[alternative HTML version deleted]]
>>> >>
>>> >> _______________________________________________
>>> >> R-sig-mixed-models at r-project.org mailing list
>>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>> >>
>>> >>
>>> >>
>>> >>
>>> >> [[alternative HTML version deleted]]
>>> >>
>>> >> _______________________________________________
>>> >> R-sig-mixed-models at r-project.org mailing list
>>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>> >>
>>> >>
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>

	[[alternative HTML version deleted]]


From |upp @end|ng |rom uch|c@go@edu  Sat Mar 23 17:19:58 2019
From: |upp @end|ng |rom uch|c@go@edu (Stuart Luppescu)
Date: Sat, 23 Mar 2019 16:19:58 +0000
Subject: [R-sig-ME] Random effects in multinomial regression in R?
In-Reply-To: <D8BB7BD5.57C5D%hdoran@air.org>
References: <CAEA998ggNw91Y93TEyPy=_t6JuJfsTdse+77H7WAdkXQTrgVOw@mail.gmail.com>
 <2AF9E03B-C3BC-4C3F-8BD7-053CB8037205@getmailspring.com>
 <CAEA998g4rYVZoDQikUJXQCM14jD6D54NokXbOBVAe2LqV8V+rA@mail.gmail.com>
 <CAEA998gcf_7Tx-fBsqY76JZzK6f-WG3yURJFbb83j-boknjyyQ@mail.gmail.com>
 <D8BB7BD5.57C5D%hdoran@air.org>
Message-ID: <1553357996.17274.14.camel@uchicago.edu>

On Sat, 2019-03-23 at 10:03 +0000, Doran, Harold wrote:
> I propose it?s
> often ignored not because the issue is not well-known, but because there
> is not (widely distributed) software that easily implements the types of
> corrections needed in this scenario.

I'm not sure if it's "easily" implemented, but you can do this type of
measurement error model in STAN (mc-stan.org), defining the predictor
as a combination of a fixed component and a random error component,
which are determined by the data source. (My work uses the results from
an IRT model which produces an estimate and standard error for each
person.) Information here:

https://mc-stan.org/docs/2_18/stan-users-guide/bayesian-measurement-error-model.html

-- 
Stuart Luppescu
Chief Psychometrician (ret.)
UChicago Consortium on School Research
http://consortium.uchicago.edu

lupp at uchicago.edu

From @ouhey|@@ghebghoub @end|ng |rom gm@||@com  Sat Mar 23 18:57:50 2019
From: @ouhey|@@ghebghoub @end|ng |rom gm@||@com (Souheyla GHEBGHOUB)
Date: Sat, 23 Mar 2019 17:57:50 +0000
Subject: [R-sig-ME] Random effects in multinomial regression in R?
In-Reply-To: <CADcpBHMrpJ7uevVwH_4BvUn15mSaNzW865Fx77s0up8u6thy-A@mail.gmail.com>
References: <CAEA998ggNw91Y93TEyPy=_t6JuJfsTdse+77H7WAdkXQTrgVOw@mail.gmail.com>
 <2AF9E03B-C3BC-4C3F-8BD7-053CB8037205@getmailspring.com>
 <CAEA998g4rYVZoDQikUJXQCM14jD6D54NokXbOBVAe2LqV8V+rA@mail.gmail.com>
 <CAEA998gcf_7Tx-fBsqY76JZzK6f-WG3yURJFbb83j-boknjyyQ@mail.gmail.com>
 <CADcpBHN_wmSacX5=LsrMSoNwFKMZxHssMXrBW5B0VTYr68tv9g@mail.gmail.com>
 <CAEA998gWs2PO1NDjfn94NWR2U_AmYHDs3K+drhRAW8GLEvvtSw@mail.gmail.com>
 <CADcpBHMrpJ7uevVwH_4BvUn15mSaNzW865Fx77s0up8u6thy-A@mail.gmail.com>
Message-ID: <CAEA998iO=CgnrhJNvOjkQpHBJ65NJQd4vRWM1PdWOZwe_MSUdA@mail.gmail.com>

Dear Ren? and any other member interested in this discussion,

I appreciate the long feedback I received from you. But I can tell I could
not well convey my concern.

The aim of my analysis is to predict the odds of correct/incorrect
responses based on some predictors: group (Treatment1/Treatment2/Control);
Time (pretest/posttest), verbal frequency, concreteness vs.
abstractness...etc. I have hypotheses such as, Time*Group interaction will
show a significant effect as I expect students in Treatment Group 1 will
outperform at the level of Posttest.
I have a problem of predicting Response with Time as an Independent
variable which could be summarised in this example of one participant:

Subject word Group  Pretest Posttest
1 1 control 0 1
1 2 control 0 1
1 3 control 1 1
1 4 control 1 1
1 5 control 1 0
1 6 control 0 0
1 7 control 0 0
1 8 control 0 0
=3 =4

Would the model compare the mean score of pretest against mean score of
posttest? If yes, it will not truly reflect the data, because its not about
the sum,
its about every word on its own. So it should not be about 4 in posttest
against 3 in pretest means 1 gain.
 In here, there are 2 gains in word 1,2 and one decline in word 5. But does
the model analyse at the level of every word or at the level of sum of
words?
That?s the main problem I have not solved since 3 months now.
Best,
SOUHEYLA

On Sat, 23 Mar 2019 at 12:46, Ren? <bimonosom at gmail.com> wrote:

> Hey, things clear up. Thanks for the picture. I want to challenge your
> concerns:
> First, you see that observing a main effect of PrePost (or "Time") in mod3
> can only mean two things: 1) performance goes up (positive effect); or 2)
> performance goes down (negative effect), in general. Saying something like
> this, is what the model is defined for. The conclusion will be this
> "general increase" or "general decrease" or "no evidence here". If you have
> a different question from that (i.e. on an item level), then you should
> specify it in more detail (see below).
> There are three issues in-between the lines of your questions:
> 1. is it a statistical concern you have?
> 2. is it an actual theoretical question you have?
> 3. is it a matter of making a non-result to a result?
>
> 1. The statistical view:
> Counter-question: who would ever assume that a 1 score for a word in a
> pre-test will remain constant until eternity. And why should it? The answer
> is: Nobody, and this is the reason statistics (probability theory) exists
> :). So the first simple answer to your question is you do not need to test
> whether observed gains from pre-to-post are 'genuine' (only from 0 to 1,
> without decline cases) because 'nature' guarantees that there will be
> decline somewhere. That's "randomness" :)  But the question is, what's
> stronger, gain or decline? See... and there is no problem in it: a general
> main effect (e.g. an overall gain) still is an overall gain, even if some
> cases decline.
>
> 2. Theoretical view:
> If, however, such special item dynamics are theoretized in advance, then
> simply test it :) For instance, the assumption whether the treatment
> leads to gain on abstract words, but to decline on concrete words, then
> should find into the model by coding the factor AbsCon (abstract vs.
> concrete words)  as fixed effect (assuming a within participants
> manipulation).
> mod4<-brm(score~PrePost*Group*AbsCon+(1+PrePost*AbsCon| subject) +
> (1+group|words),...)
> (Note: the 1+XX|subject just means random intercept  for subject (1+) plus
> slope for XX on subject; and writing (1+group|words) is the same as
> writing (group|words), but you can estimate the slope without the (word)
> intercept by writing (0+group|words))
>
> Without having such a theoretical account testing for can also be done via:
> mod4<-brm(score~PrePost*Group*words+(1+PrePost*words| subject),...)
> But you will hardly be able to interpret the interactions in this model
> because words alone has 28 levels.
>
> 3. But, your example seems to suggest a special case... (i.e. an actual
> Null-Effect):
> "If a participant has  got 5 correct words out of 28 in both pretest and
> posttest"
> then there would be no general effect of PrePost in the model above
> (generalizing to all participants now). And searching for "deeper" model
> checks looks like rescuing all effects you can get (post hoc). But of
> course, making an argument like there still is an effect, namely for 5
> specific words, which is not observable because there is also a detriment
> for other 5 words is possible, but requires a solid theory which explicitly
> predicts this interaction, and an experiment which was explicitly designed
> to test this interaction. (point 2)
>
> So... if it is point 2 you got... Then go ahead :) test it in a meaningful
> way. Otherwise, simply treat this "effect by words" interaction as random
> slope (1+group|words), or btw. (1+PrePost*Group|words) is also possible...,
> which is basically 'statistically' integrating the idea that the
> treatment*time effects vary (randomly) between stimuli. And doing this in
> the random effects has the notion of "generalizing" estimation error in the
> population, and should be preferred to implementing those in the fixed
> effects, if the 28 words can be seen as a random (non-special) stimulus
> sample. If this is not the case, then consider coding the "special" thing
> about the words as fixed effects (e.g. if you want to use the same design
> again, for testing something, while controlling for stimulus specifics).
>
> Best, Ren?
>
> Am Sa., 23. M?rz 2019 um 11:59 Uhr schrieb Souheyla GHEBGHOUB <
> souheyla.ghebghoub at gmail.com>:
>
>> Dear Ren?,
>>
>> Thank you for the feedback. Actually, my model was originally like you
>> suggested now (except for slopes I had PrePost without 1 in both words and
>> subjects. I called PrePost as "Time". I will read more about the 1+prepost
>> form you mentioned.
>>
>> The reason why I gave up this model and looked for something else is the
>> fact that each subject has 28 words tested twice (pre&post) and I was not
>> too sure whether such model will take into consideration differences
>> between pre & post at the level of every word of each participant (which is
>> what I want) instead of merely comparing every participant's pre mean sores
>> of 28 words against his post mean score (which is what i should avoid), here
>> is a short example as to why:
>>
>>  If a participant has  got 5 correct words out of 28 in both pretest and
>> posttest, there will be multiple interpretations:  e.g. They could refer to
>> the same words (i.e. 0 gain), or they could be totally new words (i.e. 5
>> gains) ...etc , hence I am not sure whether such model of pretest vs
>> posttest will compare each subject score of each word from pretest to
>> posttest then base its analysis on these score changes instead of comparing
>> the sum scores between pre and post and which likely skew results.
>>
>> I posted about this in stackexchange 3 months ago and was told that it
>> does compare word to word for every participant, but I am still not
>> confident enough to use it because all the accurateness of the results and
>> discussion chapters of my PhD thesis will be based on this decision.
>>
>> I look forward to receive feedback from you and any member reading this,
>> Souheyla
>> University of York
>>
>> On Sat, 23 Mar 2019, 10:01 Ren?, <bimonosom at gmail.com> wrote:
>>
>>> Hi Souheyla,
>>>
>>> Well, I guess in your case it is simply more elegant to leave the
>>> measured predictor out of the fixed effects, because there is also another
>>> implied question (i.e. about the strength of change between pre and post).
>>>
>>> So, another possibility to re-define your model (as logistic regression)
>>> allowing for better interpretations:
>>> mod3<-brm(score~PrePost*Group+(1+PrePost | subject)+(1+group |
>>> words),...)
>>>
>>> score = 0 or 1 for a given testitem
>>> PrePost = Pre vs. Post  (basically just an indicator of the measurement
>>> time point)
>>> Thus, the PrePost main effect will tell, whether there is a change from
>>> pre to post (e.g. a gaint), and you can also tell how strong it is (in odds
>>> ratios).
>>> And if PrePost interacts with Group, then the change (e.g. a gain) is
>>> moderated by the treatment, which seems to be your main question.
>>>
>>> Now in this model, you can also have by-subject random slopes for
>>> PrePost of course (because the fixed effect of PrePost is present for every
>>> subject).
>>>
>>> Best, Ren?
>>>
>>>
>>> Am Sa., 23. M?rz 2019 um 10:12 Uhr schrieb Souheyla GHEBGHOUB <
>>> souheyla.ghebghoub at gmail.com>:
>>>
>>>> I read that in multinomial regression, all independent variables should
>>>> be
>>>> variables that we manipulate. Can I still have pretest as IV without
>>>> skewing my results?
>>>>
>>>> Best,
>>>> Souheyla
>>>>
>>>> On Fri, 22 Mar 2019, 23:31 Souheyla GHEBGHOUB, <
>>>> souheyla.ghebghoub at gmail.com>
>>>> wrote:
>>>>
>>>> > Thank you both. I will look into this and see :)
>>>> >
>>>> > Best,
>>>> > Souheyla
>>>> >
>>>> > On Fri, 22 Mar 2019, 22:02 Uanhoro, James, <
>>>> uanhoro.1 at buckeyemail.osu.edu>
>>>> > wrote:
>>>> >
>>>> >> In standard regression models, the assumption is predictor variables
>>>> are
>>>> >> measured without error. Test scores will have measurement error,
>>>> hence
>>>> >> Doran's comment when test scores are used as covariates. See:
>>>> Hausman, J.
>>>> >> (2001). Mismeasured Variables in Econometric Analysis: Problems from
>>>> the
>>>> >> Right and Problems from the Left. *Journal of Economic Perspectives*,
>>>> >> *15*(4), 57?67. https://doi.org/10.1257/jep.15.4.57
>>>> >> I will note that many practitioners ignore this issue, and it is
>>>> quite
>>>> >> common to use predictors measured with error. Consider the number of
>>>> times
>>>> >> people use polychotomized income measures, or SES measures as
>>>> predictors,
>>>> >> or some other "construct".
>>>> >> On Mar 22 2019, at 5:39 pm, Souheyla GHEBGHOUB <
>>>> >> souheyla.ghebghoub at gmail.com> wrote:
>>>> >>
>>>> >> Dear Doran,
>>>> >>
>>>> >> Could you explain more this point to me, please?
>>>> >>
>>>> >> Thank you,
>>>> >> Souheyla
>>>> >>
>>>> >> On Fri, 22 Mar 2019, 21:19 Doran, Harold, <HDoran at air.org> wrote:
>>>> >>
>>>> >> Yes, but conditioning on the pre-test means you are using a variable
>>>> >> measured with error and the estimates you obtain and now
>>>> inconsistent, and
>>>> >> that?s a pretty big sin.
>>>> >>
>>>> >> On 3/22/19, 3:49 PM, "Souheyla GHEBGHOUB" <
>>>> souheyla.ghebghoub at gmail.com>
>>>> >> wrote:
>>>> >>
>>>> >> Dear Ren?,
>>>> >>
>>>> >> Thank you for your feedback to me. You are right, dropping the
>>>> pretest
>>>> >> from
>>>> >> covariate if I predict change definitely makes sense to me! But the
>>>> fact
>>>> >> that i need to control for the starting levels of participants makes
>>>> it
>>>> >> obligatory for me to chose the second way, which is predicting
>>>> posttest
>>>> >> instead of change to have pretest scores controlled for.
>>>> >>
>>>> >> You also chose (1+group | word) , which is new to me. Does it intend
>>>> to
>>>> >> assume the effect of group to vary across words, which is something
>>>> >> applicable to my data, right?
>>>> >> I will discuss all this with my supervisor, and may reply here again
>>>> in
>>>> >> few
>>>> >> days if you do not mind.
>>>> >> Thank you very much
>>>> >> Souheyla
>>>> >> University of York
>>>> >>
>>>> >>
>>>> >> On Fri, 22 Mar 2019 at 13:42, Ren? <bimonosom at gmail.com> wrote:
>>>> >>
>>>> >> Hi Souheyla,
>>>> >>
>>>> >> it seems to me that you will run into problems with your coding of
>>>> >> change
>>>> >> (gain, no gain and decline) because the 'change' is by
>>>> >> definition/calculation depending on the predictor pretest.
>>>> >> See, according to your coding scheme:
>>>> >> Change = decline can only occur if pretest=1 (not by pretest=0).
>>>> >> Change = gain can only occur if pretest = 0 (not by pretest=1)
>>>> >> Change = No Gain can occur if pretest= 1 or 0
>>>> >> In other words:
>>>> >> If pretest = 1 then the possible outcomes can be decline or no gain
>>>> >> If pretest = 0 then the possible outcomes can be gain or no gain
>>>> >>
>>>> >> And if the model result shows you then that the pre-test is
>>>> >> significantly
>>>> >> related to p(change-outcome), I guess there is no surprise in it, is
>>>> it?
>>>> >>
>>>> >> So the first solution to this would be simply kicking the pre-test
>>>> >> predictor out of the model completely, and predict:
>>>> >> mod1 <- brm(Change ~ Group + (1|Subject) + (1+Group|Word),...)
>>>> >> (Btw.: actually the first Hierarchical Bayes Model question I see on
>>>> the
>>>> >> mixed-effects mailing list :))
>>>> >>
>>>> >> Attempt for a further clarification on which random slopes would
>>>> reflect
>>>> >> the model's design:
>>>> >> If you have a within-subjects design, by-subject random slopes are
>>>> >> possible for the within-subject variable (e.g. if there are two sets
>>>> of
>>>> >> words/lists [e.g. abstract vs. concrete words] for each participant,
>>>> and
>>>> >> you test whether there is a performance-difference between these
>>>> >> word-lists, then you can implement by-subject random slopes for
>>>> words,
>>>> >> because each participant has seen both sets.) If each participant has
>>>> >> seen
>>>> >> only one list (i.e. between subjects design) by subject random slopes
>>>> >> for
>>>> >> words are not appropriate, because there is no 'slope' by participant
>>>> >> (i.e.
>>>> >> by definition, having a slope requires at least two observations...).
>>>> >> This
>>>> >> is always a good rule of thumb without thinking about it too heavily
>>>> :)
>>>> >> Ans as you see: you can define a random slope for words:
>>>> >> (1+Group|Word),
>>>> >> because each word has been presented in each group (i.e. there can
>>>> be a
>>>> >> slope for each word). And intuitively speaking the Treatment-effect
>>>> can
>>>> >> vary depending on the stimuli you use, and the slope makes sense.
>>>> (You
>>>> >> also
>>>> >> see in this example that the treatment effect can also vary by
>>>> subjects,
>>>> >> but in fact, this subject effect variation IS EQUAL to the effect you
>>>> >> want
>>>> >> to test, and having by subject group random slopes would eliminate
>>>> the
>>>> >> fixed effect...)
>>>> >>
>>>> >> Anyway, there is a second possibility to define your model,
>>>> depending on
>>>> >> how you want to interpret it. In the previous model you can say
>>>> >> something
>>>> >> about the type-of-change likelihoods depending on the treatment
>>>> group.
>>>> >> But
>>>> >> you could implement the model as binomial as well (i.e. logistic
>>>> >> regression)
>>>> >>
>>>> >> mod2 <- brm(posttest ~ pretest*Group + (1|Subject) +
>>>> (1+Group|Word),...)
>>>> >>
>>>> >> And what you would expect here would be an interaction between
>>>> pre-test
>>>> >> and Group. For instance; if pretest=0 & treatment 1 then posttest
>>>> larger
>>>> >> than with pretest=0 & treatment 2; but not when pretest=1 (because
>>>> this
>>>> >> is
>>>> >> a plausible no gain situation). And so on...
>>>> >> (And in this model there are no also no further random slopes hidden
>>>> in
>>>> >> your design :))
>>>> >> Hope this helps.
>>>> >>
>>>> >> Best, Ren?
>>>> >>
>>>> >>
>>>> >> Am Do., 21. M?rz 2019 um 14:01 Uhr schrieb Souheyla GHEBGHOUB <
>>>> >> souheyla.ghebghoub at gmail.com>:
>>>> >>
>>>> >> Dear Philip,
>>>> >>
>>>> >> I understand , here is the structure of my data in case it could
>>>> help.
>>>> >>
>>>> >> I have 3 groups of participants (control, treatment1, treatment2).
>>>> Each
>>>> >> group was tested twice, once before treatment (pretest) and once
>>>> after
>>>> >> treatment (posttest).
>>>> >> In each test, they were tested on knowledge of 28 words, scores are
>>>> >> dichotomous (0 = unknown , 1 = known). Tests are the same.
>>>> >>
>>>> >> I calculated change from pretest to posttest :
>>>> >> if pretest 0 and posttest 0 = no gain
>>>> >> if pretest 1 and posttest 1 = no gain
>>>> >> if pretest 0 and posttest 1 = gain
>>>> >> if pretest 1 and posttest 0 = decline
>>>> >> So I ended up with a dependent variable called Change with 3 levels
>>>> >> (no_gain, gain, decline) and I tried to predict it using Group and
>>>> >> Pretest
>>>> >> as covariates using multinomial logit model. mod0 <- brm(Change ~
>>>> >> Pretest
>>>> >> +
>>>> >> Group) I would like to add random effects for subjects but don't know
>>>> >> what's the best form when Time factor is absent.
>>>> >>
>>>> >> I hope other statisticians who read this could help
>>>> >> Thank you
>>>> >> Souheyla
>>>> >>
>>>> >> [[alternative HTML version deleted]]
>>>> >>
>>>> >> _______________________________________________
>>>> >> R-sig-mixed-models at r-project.org mailing list
>>>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>> >>
>>>> >>
>>>> >>
>>>> >> [[alternative HTML version deleted]]
>>>> >>
>>>> >> _______________________________________________
>>>> >> R-sig-mixed-models at r-project.org mailing list
>>>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>> >>
>>>> >>
>>>> >>
>>>> >>
>>>> >> [[alternative HTML version deleted]]
>>>> >>
>>>> >> _______________________________________________
>>>> >> R-sig-mixed-models at r-project.org mailing list
>>>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>> >>
>>>> >>
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>

	[[alternative HTML version deleted]]


From b|mono@om @end|ng |rom gm@||@com  Sat Mar 23 23:00:50 2019
From: b|mono@om @end|ng |rom gm@||@com (=?UTF-8?B?UmVuw6k=?=)
Date: Sat, 23 Mar 2019 23:00:50 +0100
Subject: [R-sig-ME] Random effects in multinomial regression in R?
In-Reply-To: <CAEA998iO=CgnrhJNvOjkQpHBJ65NJQd4vRWM1PdWOZwe_MSUdA@mail.gmail.com>
References: <CAEA998ggNw91Y93TEyPy=_t6JuJfsTdse+77H7WAdkXQTrgVOw@mail.gmail.com>
 <2AF9E03B-C3BC-4C3F-8BD7-053CB8037205@getmailspring.com>
 <CAEA998g4rYVZoDQikUJXQCM14jD6D54NokXbOBVAe2LqV8V+rA@mail.gmail.com>
 <CAEA998gcf_7Tx-fBsqY76JZzK6f-WG3yURJFbb83j-boknjyyQ@mail.gmail.com>
 <CADcpBHN_wmSacX5=LsrMSoNwFKMZxHssMXrBW5B0VTYr68tv9g@mail.gmail.com>
 <CAEA998gWs2PO1NDjfn94NWR2U_AmYHDs3K+drhRAW8GLEvvtSw@mail.gmail.com>
 <CADcpBHMrpJ7uevVwH_4BvUn15mSaNzW865Fx77s0up8u6thy-A@mail.gmail.com>
 <CAEA998iO=CgnrhJNvOjkQpHBJ65NJQd4vRWM1PdWOZwe_MSUdA@mail.gmail.com>
Message-ID: <CADcpBHPA4t9sN6gSVW6fHF3apxN6jP66O1bevC_CoFLDYszP-g@mail.gmail.com>

Souheyla, it's a solvable problem :)
 But does the model analyse at the level of every word or at the level of
sum of words?
it's both.
Yes, the model tries to predict general tendencies (means/proportions,
conditioned on factors). Observation sums are always involved.
The second answer is, if there is no factor conditioning, then that's it.
With factors, its measuring by item-differences if you want: And it seems
you want: Let 'word type' interact with 'treatment' to predict gain
proportions (0's and 1's for each participant). The relation between word
type and treatment is your domain, but your statistical issues imply the
following brms model: It measures differences between single
questions/words (pre-post on average/proportion on log-scale). If not, the
answer is still 'yes': Hierarchical Bayesian logistic regression can do
what you want :)). I assume, in terms of mixed-effect models (brm is not
mixed-effects least-mean squares, it's hierarchical Bayes) I would go with:

mod5<-brm(score~Group*Time*Stims+(1|subject)+(1+Time*Stims| words),
family=bernoulli(),...)

What you will get are Bayesian likelihood estimates for each
mean/proportion (and difference) across participants, for each Group by
every word. After estimation, you can apply traditional statistical
thinking like comparing mean estimates by word, or in general. These (both)
can be accessed via 'emmeans', or 'emmip', functions and you also can
compute Bayes Factors for a change vs no-change hypotheses (or others) for
each question/word (using the hypothesis() function, or prior_samples()
 and posterior_samples() ), if you follow your own rules ;)
Best, Ren?

Am Sa., 23. M?rz 2019 um 18:58 Uhr schrieb Souheyla GHEBGHOUB <
souheyla.ghebghoub at gmail.com>:

> Dear Ren? and any other member interested in this discussion,
>
> I appreciate the long feedback I received from you. But I can tell I could
> not well convey my concern.
>
> The aim of my analysis is to predict the odds of correct/incorrect
> responses based on some predictors: group (Treatment1/Treatment2/Control);
> Time (pretest/posttest), verbal frequency, concreteness vs.
> abstractness...etc. I have hypotheses such as, Time*Group interaction will
> show a significant effect as I expect students in Treatment Group 1 will
> outperform at the level of Posttest.
> I have a problem of predicting Response with Time as an Independent
> variable which could be summarised in this example of one participant:
>
> Subject word Group  Pretest Posttest
> 1 1 control 0 1
> 1 2 control 0 1
> 1 3 control 1 1
> 1 4 control 1 1
> 1 5 control 1 0
> 1 6 control 0 0
> 1 7 control 0 0
> 1 8 control 0 0
> =3 =4
>
> Would the model compare the mean score of pretest against mean score of
> posttest? If yes, it will not truly reflect the data, because its not about
> the sum,
> its about every word on its own. So it should not be about 4 in posttest
> against 3 in pretest means 1 gain.
>  In here, there are 2 gains in word 1,2 and one decline in word 5. But
> does the model analyse at the level of every word or at the level of sum of
> words?
> That?s the main problem I have not solved since 3 months now.
> Best,
> SOUHEYLA
>
> On Sat, 23 Mar 2019 at 12:46, Ren? <bimonosom at gmail.com> wrote:
>
>> Hey, things clear up. Thanks for the picture. I want to challenge your
>> concerns:
>> First, you see that observing a main effect of PrePost (or "Time") in
>> mod3 can only mean two things: 1) performance goes up (positive effect); or
>> 2) performance goes down (negative effect), in general. Saying something
>> like this, is what the model is defined for. The conclusion will be this
>> "general increase" or "general decrease" or "no evidence here". If you have
>> a different question from that (i.e. on an item level), then you should
>> specify it in more detail (see below).
>> There are three issues in-between the lines of your questions:
>> 1. is it a statistical concern you have?
>> 2. is it an actual theoretical question you have?
>> 3. is it a matter of making a non-result to a result?
>>
>> 1. The statistical view:
>> Counter-question: who would ever assume that a 1 score for a word in a
>> pre-test will remain constant until eternity. And why should it? The answer
>> is: Nobody, and this is the reason statistics (probability theory) exists
>> :). So the first simple answer to your question is you do not need to test
>> whether observed gains from pre-to-post are 'genuine' (only from 0 to 1,
>> without decline cases) because 'nature' guarantees that there will be
>> decline somewhere. That's "randomness" :)  But the question is, what's
>> stronger, gain or decline? See... and there is no problem in it: a general
>> main effect (e.g. an overall gain) still is an overall gain, even if some
>> cases decline.
>>
>> 2. Theoretical view:
>> If, however, such special item dynamics are theoretized in advance, then
>> simply test it :) For instance, the assumption whether the treatment
>> leads to gain on abstract words, but to decline on concrete words, then
>> should find into the model by coding the factor AbsCon (abstract vs.
>> concrete words)  as fixed effect (assuming a within participants
>> manipulation).
>> mod4<-brm(score~PrePost*Group*AbsCon+(1+PrePost*AbsCon| subject) +
>> (1+group|words),...)
>> (Note: the 1+XX|subject just means random intercept  for subject (1+)
>> plus slope for XX on subject; and writing (1+group|words) is the same as
>> writing (group|words), but you can estimate the slope without the (word)
>> intercept by writing (0+group|words))
>>
>> Without having such a theoretical account testing for can also be done
>> via:
>> mod4<-brm(score~PrePost*Group*words+(1+PrePost*words| subject),...)
>> But you will hardly be able to interpret the interactions in this model
>> because words alone has 28 levels.
>>
>> 3. But, your example seems to suggest a special case... (i.e. an actual
>> Null-Effect):
>> "If a participant has  got 5 correct words out of 28 in both pretest and
>> posttest"
>> then there would be no general effect of PrePost in the model above
>> (generalizing to all participants now). And searching for "deeper" model
>> checks looks like rescuing all effects you can get (post hoc). But of
>> course, making an argument like there still is an effect, namely for 5
>> specific words, which is not observable because there is also a detriment
>> for other 5 words is possible, but requires a solid theory which explicitly
>> predicts this interaction, and an experiment which was explicitly designed
>> to test this interaction. (point 2)
>>
>> So... if it is point 2 you got... Then go ahead :) test it in a
>> meaningful way. Otherwise, simply treat this "effect by words" interaction
>> as random slope (1+group|words), or btw. (1+PrePost*Group|words) is also
>> possible..., which is basically 'statistically' integrating the idea that
>> the treatment*time effects vary (randomly) between stimuli. And doing this
>> in the random effects has the notion of "generalizing" estimation error in
>> the population, and should be preferred to implementing those in the fixed
>> effects, if the 28 words can be seen as a random (non-special) stimulus
>> sample. If this is not the case, then consider coding the "special" thing
>> about the words as fixed effects (e.g. if you want to use the same design
>> again, for testing something, while controlling for stimulus specifics).
>>
>> Best, Ren?
>>
>> Am Sa., 23. M?rz 2019 um 11:59 Uhr schrieb Souheyla GHEBGHOUB <
>> souheyla.ghebghoub at gmail.com>:
>>
>>> Dear Ren?,
>>>
>>> Thank you for the feedback. Actually, my model was originally like you
>>> suggested now (except for slopes I had PrePost without 1 in both words and
>>> subjects. I called PrePost as "Time". I will read more about the 1+prepost
>>> form you mentioned.
>>>
>>> The reason why I gave up this model and looked for something else is the
>>> fact that each subject has 28 words tested twice (pre&post) and I was not
>>> too sure whether such model will take into consideration differences
>>> between pre & post at the level of every word of each participant (which is
>>> what I want) instead of merely comparing every participant's pre mean sores
>>> of 28 words against his post mean score (which is what i should avoid), here
>>> is a short example as to why:
>>>
>>>  If a participant has  got 5 correct words out of 28 in both pretest and
>>> posttest, there will be multiple interpretations:  e.g. They could refer to
>>> the same words (i.e. 0 gain), or they could be totally new words (i.e. 5
>>> gains) ...etc , hence I am not sure whether such model of pretest vs
>>> posttest will compare each subject score of each word from pretest to
>>> posttest then base its analysis on these score changes instead of comparing
>>> the sum scores between pre and post and which likely skew results.
>>>
>>> I posted about this in stackexchange 3 months ago and was told that it
>>> does compare word to word for every participant, but I am still not
>>> confident enough to use it because all the accurateness of the results and
>>> discussion chapters of my PhD thesis will be based on this decision.
>>>
>>> I look forward to receive feedback from you and any member reading this,
>>> Souheyla
>>> University of York
>>>
>>> On Sat, 23 Mar 2019, 10:01 Ren?, <bimonosom at gmail.com> wrote:
>>>
>>>> Hi Souheyla,
>>>>
>>>> Well, I guess in your case it is simply more elegant to leave the
>>>> measured predictor out of the fixed effects, because there is also another
>>>> implied question (i.e. about the strength of change between pre and post).
>>>>
>>>> So, another possibility to re-define your model (as logistic
>>>> regression) allowing for better interpretations:
>>>> mod3<-brm(score~PrePost*Group+(1+PrePost | subject)+(1+group |
>>>> words),...)
>>>>
>>>> score = 0 or 1 for a given testitem
>>>> PrePost = Pre vs. Post  (basically just an indicator of the measurement
>>>> time point)
>>>> Thus, the PrePost main effect will tell, whether there is a change from
>>>> pre to post (e.g. a gaint), and you can also tell how strong it is (in odds
>>>> ratios).
>>>> And if PrePost interacts with Group, then the change (e.g. a gain) is
>>>> moderated by the treatment, which seems to be your main question.
>>>>
>>>> Now in this model, you can also have by-subject random slopes for
>>>> PrePost of course (because the fixed effect of PrePost is present for every
>>>> subject).
>>>>
>>>> Best, Ren?
>>>>
>>>>
>>>> Am Sa., 23. M?rz 2019 um 10:12 Uhr schrieb Souheyla GHEBGHOUB <
>>>> souheyla.ghebghoub at gmail.com>:
>>>>
>>>>> I read that in multinomial regression, all independent variables
>>>>> should be
>>>>> variables that we manipulate. Can I still have pretest as IV without
>>>>> skewing my results?
>>>>>
>>>>> Best,
>>>>> Souheyla
>>>>>
>>>>> On Fri, 22 Mar 2019, 23:31 Souheyla GHEBGHOUB, <
>>>>> souheyla.ghebghoub at gmail.com>
>>>>> wrote:
>>>>>
>>>>> > Thank you both. I will look into this and see :)
>>>>> >
>>>>> > Best,
>>>>> > Souheyla
>>>>> >
>>>>> > On Fri, 22 Mar 2019, 22:02 Uanhoro, James, <
>>>>> uanhoro.1 at buckeyemail.osu.edu>
>>>>> > wrote:
>>>>> >
>>>>> >> In standard regression models, the assumption is predictor
>>>>> variables are
>>>>> >> measured without error. Test scores will have measurement error,
>>>>> hence
>>>>> >> Doran's comment when test scores are used as covariates. See:
>>>>> Hausman, J.
>>>>> >> (2001). Mismeasured Variables in Econometric Analysis: Problems
>>>>> from the
>>>>> >> Right and Problems from the Left. *Journal of Economic
>>>>> Perspectives*,
>>>>> >> *15*(4), 57?67. https://doi.org/10.1257/jep.15.4.57
>>>>> >> I will note that many practitioners ignore this issue, and it is
>>>>> quite
>>>>> >> common to use predictors measured with error. Consider the number
>>>>> of times
>>>>> >> people use polychotomized income measures, or SES measures as
>>>>> predictors,
>>>>> >> or some other "construct".
>>>>> >> On Mar 22 2019, at 5:39 pm, Souheyla GHEBGHOUB <
>>>>> >> souheyla.ghebghoub at gmail.com> wrote:
>>>>> >>
>>>>> >> Dear Doran,
>>>>> >>
>>>>> >> Could you explain more this point to me, please?
>>>>> >>
>>>>> >> Thank you,
>>>>> >> Souheyla
>>>>> >>
>>>>> >> On Fri, 22 Mar 2019, 21:19 Doran, Harold, <HDoran at air.org> wrote:
>>>>> >>
>>>>> >> Yes, but conditioning on the pre-test means you are using a variable
>>>>> >> measured with error and the estimates you obtain and now
>>>>> inconsistent, and
>>>>> >> that?s a pretty big sin.
>>>>> >>
>>>>> >> On 3/22/19, 3:49 PM, "Souheyla GHEBGHOUB" <
>>>>> souheyla.ghebghoub at gmail.com>
>>>>> >> wrote:
>>>>> >>
>>>>> >> Dear Ren?,
>>>>> >>
>>>>> >> Thank you for your feedback to me. You are right, dropping the
>>>>> pretest
>>>>> >> from
>>>>> >> covariate if I predict change definitely makes sense to me! But the
>>>>> fact
>>>>> >> that i need to control for the starting levels of participants
>>>>> makes it
>>>>> >> obligatory for me to chose the second way, which is predicting
>>>>> posttest
>>>>> >> instead of change to have pretest scores controlled for.
>>>>> >>
>>>>> >> You also chose (1+group | word) , which is new to me. Does it
>>>>> intend to
>>>>> >> assume the effect of group to vary across words, which is something
>>>>> >> applicable to my data, right?
>>>>> >> I will discuss all this with my supervisor, and may reply here
>>>>> again in
>>>>> >> few
>>>>> >> days if you do not mind.
>>>>> >> Thank you very much
>>>>> >> Souheyla
>>>>> >> University of York
>>>>> >>
>>>>> >>
>>>>> >> On Fri, 22 Mar 2019 at 13:42, Ren? <bimonosom at gmail.com> wrote:
>>>>> >>
>>>>> >> Hi Souheyla,
>>>>> >>
>>>>> >> it seems to me that you will run into problems with your coding of
>>>>> >> change
>>>>> >> (gain, no gain and decline) because the 'change' is by
>>>>> >> definition/calculation depending on the predictor pretest.
>>>>> >> See, according to your coding scheme:
>>>>> >> Change = decline can only occur if pretest=1 (not by pretest=0).
>>>>> >> Change = gain can only occur if pretest = 0 (not by pretest=1)
>>>>> >> Change = No Gain can occur if pretest= 1 or 0
>>>>> >> In other words:
>>>>> >> If pretest = 1 then the possible outcomes can be decline or no gain
>>>>> >> If pretest = 0 then the possible outcomes can be gain or no gain
>>>>> >>
>>>>> >> And if the model result shows you then that the pre-test is
>>>>> >> significantly
>>>>> >> related to p(change-outcome), I guess there is no surprise in it,
>>>>> is it?
>>>>> >>
>>>>> >> So the first solution to this would be simply kicking the pre-test
>>>>> >> predictor out of the model completely, and predict:
>>>>> >> mod1 <- brm(Change ~ Group + (1|Subject) + (1+Group|Word),...)
>>>>> >> (Btw.: actually the first Hierarchical Bayes Model question I see
>>>>> on the
>>>>> >> mixed-effects mailing list :))
>>>>> >>
>>>>> >> Attempt for a further clarification on which random slopes would
>>>>> reflect
>>>>> >> the model's design:
>>>>> >> If you have a within-subjects design, by-subject random slopes are
>>>>> >> possible for the within-subject variable (e.g. if there are two
>>>>> sets of
>>>>> >> words/lists [e.g. abstract vs. concrete words] for each
>>>>> participant, and
>>>>> >> you test whether there is a performance-difference between these
>>>>> >> word-lists, then you can implement by-subject random slopes for
>>>>> words,
>>>>> >> because each participant has seen both sets.) If each participant
>>>>> has
>>>>> >> seen
>>>>> >> only one list (i.e. between subjects design) by subject random
>>>>> slopes
>>>>> >> for
>>>>> >> words are not appropriate, because there is no 'slope' by
>>>>> participant
>>>>> >> (i.e.
>>>>> >> by definition, having a slope requires at least two
>>>>> observations...).
>>>>> >> This
>>>>> >> is always a good rule of thumb without thinking about it too
>>>>> heavily :)
>>>>> >> Ans as you see: you can define a random slope for words:
>>>>> >> (1+Group|Word),
>>>>> >> because each word has been presented in each group (i.e. there can
>>>>> be a
>>>>> >> slope for each word). And intuitively speaking the Treatment-effect
>>>>> can
>>>>> >> vary depending on the stimuli you use, and the slope makes sense.
>>>>> (You
>>>>> >> also
>>>>> >> see in this example that the treatment effect can also vary by
>>>>> subjects,
>>>>> >> but in fact, this subject effect variation IS EQUAL to the effect
>>>>> you
>>>>> >> want
>>>>> >> to test, and having by subject group random slopes would eliminate
>>>>> the
>>>>> >> fixed effect...)
>>>>> >>
>>>>> >> Anyway, there is a second possibility to define your model,
>>>>> depending on
>>>>> >> how you want to interpret it. In the previous model you can say
>>>>> >> something
>>>>> >> about the type-of-change likelihoods depending on the treatment
>>>>> group.
>>>>> >> But
>>>>> >> you could implement the model as binomial as well (i.e. logistic
>>>>> >> regression)
>>>>> >>
>>>>> >> mod2 <- brm(posttest ~ pretest*Group + (1|Subject) +
>>>>> (1+Group|Word),...)
>>>>> >>
>>>>> >> And what you would expect here would be an interaction between
>>>>> pre-test
>>>>> >> and Group. For instance; if pretest=0 & treatment 1 then posttest
>>>>> larger
>>>>> >> than with pretest=0 & treatment 2; but not when pretest=1 (because
>>>>> this
>>>>> >> is
>>>>> >> a plausible no gain situation). And so on...
>>>>> >> (And in this model there are no also no further random slopes
>>>>> hidden in
>>>>> >> your design :))
>>>>> >> Hope this helps.
>>>>> >>
>>>>> >> Best, Ren?
>>>>> >>
>>>>> >>
>>>>> >> Am Do., 21. M?rz 2019 um 14:01 Uhr schrieb Souheyla GHEBGHOUB <
>>>>> >> souheyla.ghebghoub at gmail.com>:
>>>>> >>
>>>>> >> Dear Philip,
>>>>> >>
>>>>> >> I understand , here is the structure of my data in case it could
>>>>> help.
>>>>> >>
>>>>> >> I have 3 groups of participants (control, treatment1, treatment2).
>>>>> Each
>>>>> >> group was tested twice, once before treatment (pretest) and once
>>>>> after
>>>>> >> treatment (posttest).
>>>>> >> In each test, they were tested on knowledge of 28 words, scores are
>>>>> >> dichotomous (0 = unknown , 1 = known). Tests are the same.
>>>>> >>
>>>>> >> I calculated change from pretest to posttest :
>>>>> >> if pretest 0 and posttest 0 = no gain
>>>>> >> if pretest 1 and posttest 1 = no gain
>>>>> >> if pretest 0 and posttest 1 = gain
>>>>> >> if pretest 1 and posttest 0 = decline
>>>>> >> So I ended up with a dependent variable called Change with 3 levels
>>>>> >> (no_gain, gain, decline) and I tried to predict it using Group and
>>>>> >> Pretest
>>>>> >> as covariates using multinomial logit model. mod0 <- brm(Change ~
>>>>> >> Pretest
>>>>> >> +
>>>>> >> Group) I would like to add random effects for subjects but don't
>>>>> know
>>>>> >> what's the best form when Time factor is absent.
>>>>> >>
>>>>> >> I hope other statisticians who read this could help
>>>>> >> Thank you
>>>>> >> Souheyla
>>>>> >>
>>>>> >> [[alternative HTML version deleted]]
>>>>> >>
>>>>> >> _______________________________________________
>>>>> >> R-sig-mixed-models at r-project.org mailing list
>>>>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>> >>
>>>>> >>
>>>>> >>
>>>>> >> [[alternative HTML version deleted]]
>>>>> >>
>>>>> >> _______________________________________________
>>>>> >> R-sig-mixed-models at r-project.org mailing list
>>>>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>> >>
>>>>> >>
>>>>> >>
>>>>> >>
>>>>> >> [[alternative HTML version deleted]]
>>>>> >>
>>>>> >> _______________________________________________
>>>>> >> R-sig-mixed-models at r-project.org mailing list
>>>>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>> >>
>>>>> >>
>>>>>
>>>>>         [[alternative HTML version deleted]]
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>
>>>>

	[[alternative HTML version deleted]]


From @ouhey|@@ghebghoub @end|ng |rom gm@||@com  Sun Mar 24 09:36:27 2019
From: @ouhey|@@ghebghoub @end|ng |rom gm@||@com (Souheyla GHEBGHOUB)
Date: Sun, 24 Mar 2019 08:36:27 +0000
Subject: [R-sig-ME] Random effects in multinomial regression in R?
In-Reply-To: <CADcpBHPA4t9sN6gSVW6fHF3apxN6jP66O1bevC_CoFLDYszP-g@mail.gmail.com>
References: <CAEA998ggNw91Y93TEyPy=_t6JuJfsTdse+77H7WAdkXQTrgVOw@mail.gmail.com>
 <2AF9E03B-C3BC-4C3F-8BD7-053CB8037205@getmailspring.com>
 <CAEA998g4rYVZoDQikUJXQCM14jD6D54NokXbOBVAe2LqV8V+rA@mail.gmail.com>
 <CAEA998gcf_7Tx-fBsqY76JZzK6f-WG3yURJFbb83j-boknjyyQ@mail.gmail.com>
 <CADcpBHN_wmSacX5=LsrMSoNwFKMZxHssMXrBW5B0VTYr68tv9g@mail.gmail.com>
 <CAEA998gWs2PO1NDjfn94NWR2U_AmYHDs3K+drhRAW8GLEvvtSw@mail.gmail.com>
 <CADcpBHMrpJ7uevVwH_4BvUn15mSaNzW865Fx77s0up8u6thy-A@mail.gmail.com>
 <CAEA998iO=CgnrhJNvOjkQpHBJ65NJQd4vRWM1PdWOZwe_MSUdA@mail.gmail.com>
 <CADcpBHPA4t9sN6gSVW6fHF3apxN6jP66O1bevC_CoFLDYszP-g@mail.gmail.com>
Message-ID: <CAEA998hgxcg7Xd4S5W+yzP75G8+QqRq6TgZ+ig5pW4wec-emmA@mail.gmail.com>

Dear Ren?,

When I do Time (PrePost), score (response) will return to be dichotomous
(0, 1) and I will be using logistic regression (glmer) using lme4 package
instead of brms.

So my previous question was about glmer model and whether its results
reflect a prepost comparison based on word to word analysis of each
participant instead of their sums; the latter will skew results.
I appreciate your constant help,
Thank you
Souheyla

On Sat, 23 Mar 2019, 22:01 Ren?, <bimonosom at gmail.com> wrote:

> Souheyla, it's a solvable problem :)
>  But does the model analyse at the level of every word or at the level of
> sum of words?
> it's both.
> Yes, the model tries to predict general tendencies (means/proportions,
> conditioned on factors). Observation sums are always involved.
> The second answer is, if there is no factor conditioning, then that's it.
> With factors, its measuring by item-differences if you want: And it seems
> you want: Let 'word type' interact with 'treatment' to predict gain
> proportions (0's and 1's for each participant). The relation between word
> type and treatment is your domain, but your statistical issues imply the
> following brms model: It measures differences between single
> questions/words (pre-post on average/proportion on log-scale). If not, the
> answer is still 'yes': Hierarchical Bayesian logistic regression can do
> what you want :)). I assume, in terms of mixed-effect models (brm is not
> mixed-effects least-mean squares, it's hierarchical Bayes) I would go with:
>
> mod5<-brm(score~Group*Time*Stims+(1|subject)+(1+Time*Stims| words),
> family=bernoulli(),...)
>
> What you will get are Bayesian likelihood estimates for each
> mean/proportion (and difference) across participants, for each Group by
> every word. After estimation, you can apply traditional statistical
> thinking like comparing mean estimates by word, or in general. These (both)
> can be accessed via 'emmeans', or 'emmip', functions and you also can
> compute Bayes Factors for a change vs no-change hypotheses (or others) for
> each question/word (using the hypothesis() function, or prior_samples()
>  and posterior_samples() ), if you follow your own rules ;)
> Best, Ren?
>
> Am Sa., 23. M?rz 2019 um 18:58 Uhr schrieb Souheyla GHEBGHOUB <
> souheyla.ghebghoub at gmail.com>:
>
>> Dear Ren? and any other member interested in this discussion,
>>
>> I appreciate the long feedback I received from you. But I can tell I
>> could not well convey my concern.
>>
>> The aim of my analysis is to predict the odds of correct/incorrect
>> responses based on some predictors: group (Treatment1/Treatment2/Control);
>> Time (pretest/posttest), verbal frequency, concreteness vs.
>> abstractness...etc. I have hypotheses such as, Time*Group interaction will
>> show a significant effect as I expect students in Treatment Group 1 will
>> outperform at the level of Posttest.
>> I have a problem of predicting Response with Time as an Independent
>> variable which could be summarised in this example of one participant:
>>
>> Subject word Group  Pretest Posttest
>> 1 1 control 0 1
>> 1 2 control 0 1
>> 1 3 control 1 1
>> 1 4 control 1 1
>> 1 5 control 1 0
>> 1 6 control 0 0
>> 1 7 control 0 0
>> 1 8 control 0 0
>> =3 =4
>>
>> Would the model compare the mean score of pretest against mean score of
>> posttest? If yes, it will not truly reflect the data, because its not about
>> the sum,
>> its about every word on its own. So it should not be about 4 in posttest
>> against 3 in pretest means 1 gain.
>>  In here, there are 2 gains in word 1,2 and one decline in word 5. But
>> does the model analyse at the level of every word or at the level of sum of
>> words?
>> That?s the main problem I have not solved since 3 months now.
>> Best,
>> SOUHEYLA
>>
>> On Sat, 23 Mar 2019 at 12:46, Ren? <bimonosom at gmail.com> wrote:
>>
>>> Hey, things clear up. Thanks for the picture. I want to challenge your
>>> concerns:
>>> First, you see that observing a main effect of PrePost (or "Time") in
>>> mod3 can only mean two things: 1) performance goes up (positive effect); or
>>> 2) performance goes down (negative effect), in general. Saying something
>>> like this, is what the model is defined for. The conclusion will be this
>>> "general increase" or "general decrease" or "no evidence here". If you have
>>> a different question from that (i.e. on an item level), then you should
>>> specify it in more detail (see below).
>>> There are three issues in-between the lines of your questions:
>>> 1. is it a statistical concern you have?
>>> 2. is it an actual theoretical question you have?
>>> 3. is it a matter of making a non-result to a result?
>>>
>>> 1. The statistical view:
>>> Counter-question: who would ever assume that a 1 score for a word in a
>>> pre-test will remain constant until eternity. And why should it? The answer
>>> is: Nobody, and this is the reason statistics (probability theory) exists
>>> :). So the first simple answer to your question is you do not need to test
>>> whether observed gains from pre-to-post are 'genuine' (only from 0 to 1,
>>> without decline cases) because 'nature' guarantees that there will be
>>> decline somewhere. That's "randomness" :)  But the question is, what's
>>> stronger, gain or decline? See... and there is no problem in it: a general
>>> main effect (e.g. an overall gain) still is an overall gain, even if some
>>> cases decline.
>>>
>>> 2. Theoretical view:
>>> If, however, such special item dynamics are theoretized in advance, then
>>> simply test it :) For instance, the assumption whether the treatment
>>> leads to gain on abstract words, but to decline on concrete words, then
>>> should find into the model by coding the factor AbsCon (abstract vs.
>>> concrete words)  as fixed effect (assuming a within participants
>>> manipulation).
>>> mod4<-brm(score~PrePost*Group*AbsCon+(1+PrePost*AbsCon| subject) +
>>> (1+group|words),...)
>>> (Note: the 1+XX|subject just means random intercept  for subject (1+)
>>> plus slope for XX on subject; and writing (1+group|words) is the same
>>> as writing (group|words), but you can estimate the slope without the
>>> (word) intercept by writing (0+group|words))
>>>
>>> Without having such a theoretical account testing for can also be done
>>> via:
>>> mod4<-brm(score~PrePost*Group*words+(1+PrePost*words| subject),...)
>>> But you will hardly be able to interpret the interactions in this model
>>> because words alone has 28 levels.
>>>
>>> 3. But, your example seems to suggest a special case... (i.e. an actual
>>> Null-Effect):
>>> "If a participant has  got 5 correct words out of 28 in both pretest and
>>> posttest"
>>> then there would be no general effect of PrePost in the model above
>>> (generalizing to all participants now). And searching for "deeper" model
>>> checks looks like rescuing all effects you can get (post hoc). But of
>>> course, making an argument like there still is an effect, namely for 5
>>> specific words, which is not observable because there is also a detriment
>>> for other 5 words is possible, but requires a solid theory which explicitly
>>> predicts this interaction, and an experiment which was explicitly designed
>>> to test this interaction. (point 2)
>>>
>>> So... if it is point 2 you got... Then go ahead :) test it in a
>>> meaningful way. Otherwise, simply treat this "effect by words" interaction
>>> as random slope (1+group|words), or btw. (1+PrePost*Group|words) is also
>>> possible..., which is basically 'statistically' integrating the idea that
>>> the treatment*time effects vary (randomly) between stimuli. And doing this
>>> in the random effects has the notion of "generalizing" estimation error in
>>> the population, and should be preferred to implementing those in the fixed
>>> effects, if the 28 words can be seen as a random (non-special) stimulus
>>> sample. If this is not the case, then consider coding the "special" thing
>>> about the words as fixed effects (e.g. if you want to use the same design
>>> again, for testing something, while controlling for stimulus specifics).
>>>
>>> Best, Ren?
>>>
>>> Am Sa., 23. M?rz 2019 um 11:59 Uhr schrieb Souheyla GHEBGHOUB <
>>> souheyla.ghebghoub at gmail.com>:
>>>
>>>> Dear Ren?,
>>>>
>>>> Thank you for the feedback. Actually, my model was originally like you
>>>> suggested now (except for slopes I had PrePost without 1 in both words and
>>>> subjects. I called PrePost as "Time". I will read more about the 1+prepost
>>>> form you mentioned.
>>>>
>>>> The reason why I gave up this model and looked for something else is
>>>> the fact that each subject has 28 words tested twice (pre&post) and I was
>>>> not too sure whether such model will take into consideration differences
>>>> between pre & post at the level of every word of each participant (which is
>>>> what I want) instead of merely comparing every participant's pre mean sores
>>>> of 28 words against his post mean score (which is what i should avoid), here
>>>> is a short example as to why:
>>>>
>>>>  If a participant has  got 5 correct words out of 28 in both pretest
>>>> and posttest, there will be multiple interpretations:  e.g. They could
>>>> refer to the same words (i.e. 0 gain), or they could be totally new words
>>>> (i.e. 5 gains) ...etc , hence I am not sure whether such model of pretest
>>>> vs posttest will compare each subject score of each word from pretest to
>>>> posttest then base its analysis on these score changes instead of comparing
>>>> the sum scores between pre and post and which likely skew results.
>>>>
>>>> I posted about this in stackexchange 3 months ago and was told that it
>>>> does compare word to word for every participant, but I am still not
>>>> confident enough to use it because all the accurateness of the results and
>>>> discussion chapters of my PhD thesis will be based on this decision.
>>>>
>>>> I look forward to receive feedback from you and any member reading this,
>>>> Souheyla
>>>> University of York
>>>>
>>>> On Sat, 23 Mar 2019, 10:01 Ren?, <bimonosom at gmail.com> wrote:
>>>>
>>>>> Hi Souheyla,
>>>>>
>>>>> Well, I guess in your case it is simply more elegant to leave the
>>>>> measured predictor out of the fixed effects, because there is also another
>>>>> implied question (i.e. about the strength of change between pre and post).
>>>>>
>>>>> So, another possibility to re-define your model (as logistic
>>>>> regression) allowing for better interpretations:
>>>>> mod3<-brm(score~PrePost*Group+(1+PrePost | subject)+(1+group |
>>>>> words),...)
>>>>>
>>>>> score = 0 or 1 for a given testitem
>>>>> PrePost = Pre vs. Post  (basically just an indicator of the
>>>>> measurement time point)
>>>>> Thus, the PrePost main effect will tell, whether there is a change
>>>>> from pre to post (e.g. a gaint), and you can also tell how strong it is (in
>>>>> odds ratios).
>>>>> And if PrePost interacts with Group, then the change (e.g. a gain) is
>>>>> moderated by the treatment, which seems to be your main question.
>>>>>
>>>>> Now in this model, you can also have by-subject random slopes for
>>>>> PrePost of course (because the fixed effect of PrePost is present for every
>>>>> subject).
>>>>>
>>>>> Best, Ren?
>>>>>
>>>>>
>>>>> Am Sa., 23. M?rz 2019 um 10:12 Uhr schrieb Souheyla GHEBGHOUB <
>>>>> souheyla.ghebghoub at gmail.com>:
>>>>>
>>>>>> I read that in multinomial regression, all independent variables
>>>>>> should be
>>>>>> variables that we manipulate. Can I still have pretest as IV without
>>>>>> skewing my results?
>>>>>>
>>>>>> Best,
>>>>>> Souheyla
>>>>>>
>>>>>> On Fri, 22 Mar 2019, 23:31 Souheyla GHEBGHOUB, <
>>>>>> souheyla.ghebghoub at gmail.com>
>>>>>> wrote:
>>>>>>
>>>>>> > Thank you both. I will look into this and see :)
>>>>>> >
>>>>>> > Best,
>>>>>> > Souheyla
>>>>>> >
>>>>>> > On Fri, 22 Mar 2019, 22:02 Uanhoro, James, <
>>>>>> uanhoro.1 at buckeyemail.osu.edu>
>>>>>> > wrote:
>>>>>> >
>>>>>> >> In standard regression models, the assumption is predictor
>>>>>> variables are
>>>>>> >> measured without error. Test scores will have measurement error,
>>>>>> hence
>>>>>> >> Doran's comment when test scores are used as covariates. See:
>>>>>> Hausman, J.
>>>>>> >> (2001). Mismeasured Variables in Econometric Analysis: Problems
>>>>>> from the
>>>>>> >> Right and Problems from the Left. *Journal of Economic
>>>>>> Perspectives*,
>>>>>> >> *15*(4), 57?67. https://doi.org/10.1257/jep.15.4.57
>>>>>> >> I will note that many practitioners ignore this issue, and it is
>>>>>> quite
>>>>>> >> common to use predictors measured with error. Consider the number
>>>>>> of times
>>>>>> >> people use polychotomized income measures, or SES measures as
>>>>>> predictors,
>>>>>> >> or some other "construct".
>>>>>> >> On Mar 22 2019, at 5:39 pm, Souheyla GHEBGHOUB <
>>>>>> >> souheyla.ghebghoub at gmail.com> wrote:
>>>>>> >>
>>>>>> >> Dear Doran,
>>>>>> >>
>>>>>> >> Could you explain more this point to me, please?
>>>>>> >>
>>>>>> >> Thank you,
>>>>>> >> Souheyla
>>>>>> >>
>>>>>> >> On Fri, 22 Mar 2019, 21:19 Doran, Harold, <HDoran at air.org> wrote:
>>>>>> >>
>>>>>> >> Yes, but conditioning on the pre-test means you are using a
>>>>>> variable
>>>>>> >> measured with error and the estimates you obtain and now
>>>>>> inconsistent, and
>>>>>> >> that?s a pretty big sin.
>>>>>> >>
>>>>>> >> On 3/22/19, 3:49 PM, "Souheyla GHEBGHOUB" <
>>>>>> souheyla.ghebghoub at gmail.com>
>>>>>> >> wrote:
>>>>>> >>
>>>>>> >> Dear Ren?,
>>>>>> >>
>>>>>> >> Thank you for your feedback to me. You are right, dropping the
>>>>>> pretest
>>>>>> >> from
>>>>>> >> covariate if I predict change definitely makes sense to me! But
>>>>>> the fact
>>>>>> >> that i need to control for the starting levels of participants
>>>>>> makes it
>>>>>> >> obligatory for me to chose the second way, which is predicting
>>>>>> posttest
>>>>>> >> instead of change to have pretest scores controlled for.
>>>>>> >>
>>>>>> >> You also chose (1+group | word) , which is new to me. Does it
>>>>>> intend to
>>>>>> >> assume the effect of group to vary across words, which is something
>>>>>> >> applicable to my data, right?
>>>>>> >> I will discuss all this with my supervisor, and may reply here
>>>>>> again in
>>>>>> >> few
>>>>>> >> days if you do not mind.
>>>>>> >> Thank you very much
>>>>>> >> Souheyla
>>>>>> >> University of York
>>>>>> >>
>>>>>> >>
>>>>>> >> On Fri, 22 Mar 2019 at 13:42, Ren? <bimonosom at gmail.com> wrote:
>>>>>> >>
>>>>>> >> Hi Souheyla,
>>>>>> >>
>>>>>> >> it seems to me that you will run into problems with your coding of
>>>>>> >> change
>>>>>> >> (gain, no gain and decline) because the 'change' is by
>>>>>> >> definition/calculation depending on the predictor pretest.
>>>>>> >> See, according to your coding scheme:
>>>>>> >> Change = decline can only occur if pretest=1 (not by pretest=0).
>>>>>> >> Change = gain can only occur if pretest = 0 (not by pretest=1)
>>>>>> >> Change = No Gain can occur if pretest= 1 or 0
>>>>>> >> In other words:
>>>>>> >> If pretest = 1 then the possible outcomes can be decline or no gain
>>>>>> >> If pretest = 0 then the possible outcomes can be gain or no gain
>>>>>> >>
>>>>>> >> And if the model result shows you then that the pre-test is
>>>>>> >> significantly
>>>>>> >> related to p(change-outcome), I guess there is no surprise in it,
>>>>>> is it?
>>>>>> >>
>>>>>> >> So the first solution to this would be simply kicking the pre-test
>>>>>> >> predictor out of the model completely, and predict:
>>>>>> >> mod1 <- brm(Change ~ Group + (1|Subject) + (1+Group|Word),...)
>>>>>> >> (Btw.: actually the first Hierarchical Bayes Model question I see
>>>>>> on the
>>>>>> >> mixed-effects mailing list :))
>>>>>> >>
>>>>>> >> Attempt for a further clarification on which random slopes would
>>>>>> reflect
>>>>>> >> the model's design:
>>>>>> >> If you have a within-subjects design, by-subject random slopes are
>>>>>> >> possible for the within-subject variable (e.g. if there are two
>>>>>> sets of
>>>>>> >> words/lists [e.g. abstract vs. concrete words] for each
>>>>>> participant, and
>>>>>> >> you test whether there is a performance-difference between these
>>>>>> >> word-lists, then you can implement by-subject random slopes for
>>>>>> words,
>>>>>> >> because each participant has seen both sets.) If each participant
>>>>>> has
>>>>>> >> seen
>>>>>> >> only one list (i.e. between subjects design) by subject random
>>>>>> slopes
>>>>>> >> for
>>>>>> >> words are not appropriate, because there is no 'slope' by
>>>>>> participant
>>>>>> >> (i.e.
>>>>>> >> by definition, having a slope requires at least two
>>>>>> observations...).
>>>>>> >> This
>>>>>> >> is always a good rule of thumb without thinking about it too
>>>>>> heavily :)
>>>>>> >> Ans as you see: you can define a random slope for words:
>>>>>> >> (1+Group|Word),
>>>>>> >> because each word has been presented in each group (i.e. there can
>>>>>> be a
>>>>>> >> slope for each word). And intuitively speaking the
>>>>>> Treatment-effect can
>>>>>> >> vary depending on the stimuli you use, and the slope makes sense.
>>>>>> (You
>>>>>> >> also
>>>>>> >> see in this example that the treatment effect can also vary by
>>>>>> subjects,
>>>>>> >> but in fact, this subject effect variation IS EQUAL to the effect
>>>>>> you
>>>>>> >> want
>>>>>> >> to test, and having by subject group random slopes would eliminate
>>>>>> the
>>>>>> >> fixed effect...)
>>>>>> >>
>>>>>> >> Anyway, there is a second possibility to define your model,
>>>>>> depending on
>>>>>> >> how you want to interpret it. In the previous model you can say
>>>>>> >> something
>>>>>> >> about the type-of-change likelihoods depending on the treatment
>>>>>> group.
>>>>>> >> But
>>>>>> >> you could implement the model as binomial as well (i.e. logistic
>>>>>> >> regression)
>>>>>> >>
>>>>>> >> mod2 <- brm(posttest ~ pretest*Group + (1|Subject) +
>>>>>> (1+Group|Word),...)
>>>>>> >>
>>>>>> >> And what you would expect here would be an interaction between
>>>>>> pre-test
>>>>>> >> and Group. For instance; if pretest=0 & treatment 1 then posttest
>>>>>> larger
>>>>>> >> than with pretest=0 & treatment 2; but not when pretest=1 (because
>>>>>> this
>>>>>> >> is
>>>>>> >> a plausible no gain situation). And so on...
>>>>>> >> (And in this model there are no also no further random slopes
>>>>>> hidden in
>>>>>> >> your design :))
>>>>>> >> Hope this helps.
>>>>>> >>
>>>>>> >> Best, Ren?
>>>>>> >>
>>>>>> >>
>>>>>> >> Am Do., 21. M?rz 2019 um 14:01 Uhr schrieb Souheyla GHEBGHOUB <
>>>>>> >> souheyla.ghebghoub at gmail.com>:
>>>>>> >>
>>>>>> >> Dear Philip,
>>>>>> >>
>>>>>> >> I understand , here is the structure of my data in case it could
>>>>>> help.
>>>>>> >>
>>>>>> >> I have 3 groups of participants (control, treatment1, treatment2).
>>>>>> Each
>>>>>> >> group was tested twice, once before treatment (pretest) and once
>>>>>> after
>>>>>> >> treatment (posttest).
>>>>>> >> In each test, they were tested on knowledge of 28 words, scores are
>>>>>> >> dichotomous (0 = unknown , 1 = known). Tests are the same.
>>>>>> >>
>>>>>> >> I calculated change from pretest to posttest :
>>>>>> >> if pretest 0 and posttest 0 = no gain
>>>>>> >> if pretest 1 and posttest 1 = no gain
>>>>>> >> if pretest 0 and posttest 1 = gain
>>>>>> >> if pretest 1 and posttest 0 = decline
>>>>>> >> So I ended up with a dependent variable called Change with 3 levels
>>>>>> >> (no_gain, gain, decline) and I tried to predict it using Group and
>>>>>> >> Pretest
>>>>>> >> as covariates using multinomial logit model. mod0 <- brm(Change ~
>>>>>> >> Pretest
>>>>>> >> +
>>>>>> >> Group) I would like to add random effects for subjects but don't
>>>>>> know
>>>>>> >> what's the best form when Time factor is absent.
>>>>>> >>
>>>>>> >> I hope other statisticians who read this could help
>>>>>> >> Thank you
>>>>>> >> Souheyla
>>>>>> >>
>>>>>> >> [[alternative HTML version deleted]]
>>>>>> >>
>>>>>> >> _______________________________________________
>>>>>> >> R-sig-mixed-models at r-project.org mailing list
>>>>>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>> >>
>>>>>> >>
>>>>>> >>
>>>>>> >> [[alternative HTML version deleted]]
>>>>>> >>
>>>>>> >> _______________________________________________
>>>>>> >> R-sig-mixed-models at r-project.org mailing list
>>>>>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>> >>
>>>>>> >>
>>>>>> >>
>>>>>> >>
>>>>>> >> [[alternative HTML version deleted]]
>>>>>> >>
>>>>>> >> _______________________________________________
>>>>>> >> R-sig-mixed-models at r-project.org mailing list
>>>>>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>> >>
>>>>>> >>
>>>>>>
>>>>>>         [[alternative HTML version deleted]]
>>>>>>
>>>>>> _______________________________________________
>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>
>>>>>

	[[alternative HTML version deleted]]


From b|mono@om @end|ng |rom gm@||@com  Sun Mar 24 14:39:26 2019
From: b|mono@om @end|ng |rom gm@||@com (=?UTF-8?B?UmVuw6k=?=)
Date: Sun, 24 Mar 2019 14:39:26 +0100
Subject: [R-sig-ME] Random effects in multinomial regression in R?
In-Reply-To: <CAEA998hgxcg7Xd4S5W+yzP75G8+QqRq6TgZ+ig5pW4wec-emmA@mail.gmail.com>
References: <CAEA998ggNw91Y93TEyPy=_t6JuJfsTdse+77H7WAdkXQTrgVOw@mail.gmail.com>
 <2AF9E03B-C3BC-4C3F-8BD7-053CB8037205@getmailspring.com>
 <CAEA998g4rYVZoDQikUJXQCM14jD6D54NokXbOBVAe2LqV8V+rA@mail.gmail.com>
 <CAEA998gcf_7Tx-fBsqY76JZzK6f-WG3yURJFbb83j-boknjyyQ@mail.gmail.com>
 <CADcpBHN_wmSacX5=LsrMSoNwFKMZxHssMXrBW5B0VTYr68tv9g@mail.gmail.com>
 <CAEA998gWs2PO1NDjfn94NWR2U_AmYHDs3K+drhRAW8GLEvvtSw@mail.gmail.com>
 <CADcpBHMrpJ7uevVwH_4BvUn15mSaNzW865Fx77s0up8u6thy-A@mail.gmail.com>
 <CAEA998iO=CgnrhJNvOjkQpHBJ65NJQd4vRWM1PdWOZwe_MSUdA@mail.gmail.com>
 <CADcpBHPA4t9sN6gSVW6fHF3apxN6jP66O1bevC_CoFLDYszP-g@mail.gmail.com>
 <CAEA998hgxcg7Xd4S5W+yzP75G8+QqRq6TgZ+ig5pW4wec-emmA@mail.gmail.com>
Message-ID: <CADcpBHNWaw=CqL8S8cRDzoou4fMconYZokijUSj1G1q8bwoGZQ@mail.gmail.com>

Ok. I try a short one :)
Also going back to your first mail:

This model is what you want:

score~Time*Group+(1+Time|subject)+(1+Time*Group | words)

No skew in here ;) (but maybe model identification issues depending on the
number of observations you got.) In other words, (random) by-word
variations and (random) by-participant variations are taken into account,
and the "average across stimuli" problem articulated here (Judd, Westfall,
& Kenny, 2012) is appropriately tackled. As you do not explicitly reference
to this paper, I just assume that you mean this kind of "skew" (alpha
inflation due to stimulus aggregation), because I am currently not aware of
other theoretically or statistically relevant 'skews' there could be.

Best, Ren?

Ps: Why not using brms... :)  You get parameter likelihood estimates and
Bayesian credible intervals for your effect estimates? The model code and
output-format are basically the same. And you don't need to report p-values
:))
Pps: For making the above formula work, you need to recode your data matrix
to:

Subject word Group  Time Score
1 1 control Pre 1
1 1 control Post 1
1 2 control Pre 1
1 2 control Post 1
1 3 control Pre 0
1 3 control Post 0
1 4 control Pre 0
1 4 control Post 0

Am So., 24. M?rz 2019 um 09:36 Uhr schrieb Souheyla GHEBGHOUB <
souheyla.ghebghoub at gmail.com>:

> Dear Ren?,
>
> When I do Time (PrePost), score (response) will return to be dichotomous
> (0, 1) and I will be using logistic regression (glmer) using lme4 package
> instead of brms.
>
> So my previous question was about glmer model and whether its results
> reflect a prepost comparison based on word to word analysis of each
> participant instead of their sums; the latter will skew results.
> I appreciate your constant help,
> Thank you
> Souheyla
>
> On Sat, 23 Mar 2019, 22:01 Ren?, <bimonosom at gmail.com> wrote:
>
>> Souheyla, it's a solvable problem :)
>>  But does the model analyse at the level of every word or at the level of
>> sum of words?
>> it's both.
>> Yes, the model tries to predict general tendencies (means/proportions,
>> conditioned on factors). Observation sums are always involved.
>> The second answer is, if there is no factor conditioning, then that's it.
>> With factors, its measuring by item-differences if you want: And it seems
>> you want: Let 'word type' interact with 'treatment' to predict gain
>> proportions (0's and 1's for each participant). The relation between word
>> type and treatment is your domain, but your statistical issues imply the
>> following brms model: It measures differences between single
>> questions/words (pre-post on average/proportion on log-scale). If not, the
>> answer is still 'yes': Hierarchical Bayesian logistic regression can do
>> what you want :)). I assume, in terms of mixed-effect models (brm is not
>> mixed-effects least-mean squares, it's hierarchical Bayes) I would go with:
>>
>> mod5<-brm(score~Group*Time*Stims+(1|subject)+(1+Time*Stims| words),
>> family=bernoulli(),...)
>>
>> What you will get are Bayesian likelihood estimates for each
>> mean/proportion (and difference) across participants, for each Group by
>> every word. After estimation, you can apply traditional statistical
>> thinking like comparing mean estimates by word, or in general. These (both)
>> can be accessed via 'emmeans', or 'emmip', functions and you also can
>> compute Bayes Factors for a change vs no-change hypotheses (or others) for
>> each question/word (using the hypothesis() function, or prior_samples()
>>  and posterior_samples() ), if you follow your own rules ;)
>> Best, Ren?
>>
>> Am Sa., 23. M?rz 2019 um 18:58 Uhr schrieb Souheyla GHEBGHOUB <
>> souheyla.ghebghoub at gmail.com>:
>>
>>> Dear Ren? and any other member interested in this discussion,
>>>
>>> I appreciate the long feedback I received from you. But I can tell I
>>> could not well convey my concern.
>>>
>>> The aim of my analysis is to predict the odds of correct/incorrect
>>> responses based on some predictors: group (Treatment1/Treatment2/Control);
>>> Time (pretest/posttest), verbal frequency, concreteness vs.
>>> abstractness...etc. I have hypotheses such as, Time*Group interaction will
>>> show a significant effect as I expect students in Treatment Group 1 will
>>> outperform at the level of Posttest.
>>> I have a problem of predicting Response with Time as an Independent
>>> variable which could be summarised in this example of one participant:
>>>
>>> Subject word Group  Pretest Posttest
>>> 1 1 control 0 1
>>> 1 2 control 0 1
>>> 1 3 control 1 1
>>> 1 4 control 1 1
>>> 1 5 control 1 0
>>> 1 6 control 0 0
>>> 1 7 control 0 0
>>> 1 8 control 0 0
>>> =3 =4
>>>
>>> Would the model compare the mean score of pretest against mean score of
>>> posttest? If yes, it will not truly reflect the data, because its not about
>>> the sum,
>>> its about every word on its own. So it should not be about 4 in posttest
>>> against 3 in pretest means 1 gain.
>>>  In here, there are 2 gains in word 1,2 and one decline in word 5. But
>>> does the model analyse at the level of every word or at the level of sum of
>>> words?
>>> That?s the main problem I have not solved since 3 months now.
>>> Best,
>>> SOUHEYLA
>>>
>>> On Sat, 23 Mar 2019 at 12:46, Ren? <bimonosom at gmail.com> wrote:
>>>
>>>> Hey, things clear up. Thanks for the picture. I want to challenge your
>>>> concerns:
>>>> First, you see that observing a main effect of PrePost (or "Time") in
>>>> mod3 can only mean two things: 1) performance goes up (positive effect); or
>>>> 2) performance goes down (negative effect), in general. Saying something
>>>> like this, is what the model is defined for. The conclusion will be this
>>>> "general increase" or "general decrease" or "no evidence here". If you have
>>>> a different question from that (i.e. on an item level), then you should
>>>> specify it in more detail (see below).
>>>> There are three issues in-between the lines of your questions:
>>>> 1. is it a statistical concern you have?
>>>> 2. is it an actual theoretical question you have?
>>>> 3. is it a matter of making a non-result to a result?
>>>>
>>>> 1. The statistical view:
>>>> Counter-question: who would ever assume that a 1 score for a word in a
>>>> pre-test will remain constant until eternity. And why should it? The answer
>>>> is: Nobody, and this is the reason statistics (probability theory) exists
>>>> :). So the first simple answer to your question is you do not need to test
>>>> whether observed gains from pre-to-post are 'genuine' (only from 0 to 1,
>>>> without decline cases) because 'nature' guarantees that there will be
>>>> decline somewhere. That's "randomness" :)  But the question is, what's
>>>> stronger, gain or decline? See... and there is no problem in it: a general
>>>> main effect (e.g. an overall gain) still is an overall gain, even if some
>>>> cases decline.
>>>>
>>>> 2. Theoretical view:
>>>> If, however, such special item dynamics are theoretized in advance,
>>>> then simply test it :) For instance, the assumption whether the
>>>> treatment leads to gain on abstract words, but to decline on concrete
>>>> words, then should find into the model by coding the factor AbsCon
>>>> (abstract vs. concrete words)  as fixed effect (assuming a within
>>>> participants manipulation).
>>>> mod4<-brm(score~PrePost*Group*AbsCon+(1+PrePost*AbsCon| subject) +
>>>> (1+group|words),...)
>>>> (Note: the 1+XX|subject just means random intercept  for subject (1+)
>>>> plus slope for XX on subject; and writing (1+group|words) is the same
>>>> as writing (group|words), but you can estimate the slope without the
>>>> (word) intercept by writing (0+group|words))
>>>>
>>>> Without having such a theoretical account testing for can also be done
>>>> via:
>>>> mod4<-brm(score~PrePost*Group*words+(1+PrePost*words| subject),...)
>>>> But you will hardly be able to interpret the interactions in this model
>>>> because words alone has 28 levels.
>>>>
>>>> 3. But, your example seems to suggest a special case... (i.e. an actual
>>>> Null-Effect):
>>>> "If a participant has  got 5 correct words out of 28 in both pretest
>>>> and posttest"
>>>> then there would be no general effect of PrePost in the model above
>>>> (generalizing to all participants now). And searching for "deeper" model
>>>> checks looks like rescuing all effects you can get (post hoc). But of
>>>> course, making an argument like there still is an effect, namely for 5
>>>> specific words, which is not observable because there is also a detriment
>>>> for other 5 words is possible, but requires a solid theory which explicitly
>>>> predicts this interaction, and an experiment which was explicitly designed
>>>> to test this interaction. (point 2)
>>>>
>>>> So... if it is point 2 you got... Then go ahead :) test it in a
>>>> meaningful way. Otherwise, simply treat this "effect by words" interaction
>>>> as random slope (1+group|words), or btw. (1+PrePost*Group|words) is also
>>>> possible..., which is basically 'statistically' integrating the idea that
>>>> the treatment*time effects vary (randomly) between stimuli. And doing this
>>>> in the random effects has the notion of "generalizing" estimation error in
>>>> the population, and should be preferred to implementing those in the fixed
>>>> effects, if the 28 words can be seen as a random (non-special) stimulus
>>>> sample. If this is not the case, then consider coding the "special" thing
>>>> about the words as fixed effects (e.g. if you want to use the same design
>>>> again, for testing something, while controlling for stimulus specifics).
>>>>
>>>> Best, Ren?
>>>>
>>>> Am Sa., 23. M?rz 2019 um 11:59 Uhr schrieb Souheyla GHEBGHOUB <
>>>> souheyla.ghebghoub at gmail.com>:
>>>>
>>>>> Dear Ren?,
>>>>>
>>>>> Thank you for the feedback. Actually, my model was originally like you
>>>>> suggested now (except for slopes I had PrePost without 1 in both words and
>>>>> subjects. I called PrePost as "Time". I will read more about the 1+prepost
>>>>> form you mentioned.
>>>>>
>>>>> The reason why I gave up this model and looked for something else is
>>>>> the fact that each subject has 28 words tested twice (pre&post) and I was
>>>>> not too sure whether such model will take into consideration differences
>>>>> between pre & post at the level of every word of each participant (which is
>>>>> what I want) instead of merely comparing every participant's pre mean sores
>>>>> of 28 words against his post mean score (which is what i should avoid), here
>>>>> is a short example as to why:
>>>>>
>>>>>  If a participant has  got 5 correct words out of 28 in both pretest
>>>>> and posttest, there will be multiple interpretations:  e.g. They could
>>>>> refer to the same words (i.e. 0 gain), or they could be totally new words
>>>>> (i.e. 5 gains) ...etc , hence I am not sure whether such model of pretest
>>>>> vs posttest will compare each subject score of each word from pretest to
>>>>> posttest then base its analysis on these score changes instead of comparing
>>>>> the sum scores between pre and post and which likely skew results.
>>>>>
>>>>> I posted about this in stackexchange 3 months ago and was told that it
>>>>> does compare word to word for every participant, but I am still not
>>>>> confident enough to use it because all the accurateness of the results and
>>>>> discussion chapters of my PhD thesis will be based on this decision.
>>>>>
>>>>> I look forward to receive feedback from you and any member reading
>>>>> this,
>>>>> Souheyla
>>>>> University of York
>>>>>
>>>>> On Sat, 23 Mar 2019, 10:01 Ren?, <bimonosom at gmail.com> wrote:
>>>>>
>>>>>> Hi Souheyla,
>>>>>>
>>>>>> Well, I guess in your case it is simply more elegant to leave the
>>>>>> measured predictor out of the fixed effects, because there is also another
>>>>>> implied question (i.e. about the strength of change between pre and post).
>>>>>>
>>>>>> So, another possibility to re-define your model (as logistic
>>>>>> regression) allowing for better interpretations:
>>>>>> mod3<-brm(score~PrePost*Group+(1+PrePost | subject)+(1+group |
>>>>>> words),...)
>>>>>>
>>>>>> score = 0 or 1 for a given testitem
>>>>>> PrePost = Pre vs. Post  (basically just an indicator of the
>>>>>> measurement time point)
>>>>>> Thus, the PrePost main effect will tell, whether there is a change
>>>>>> from pre to post (e.g. a gaint), and you can also tell how strong it is (in
>>>>>> odds ratios).
>>>>>> And if PrePost interacts with Group, then the change (e.g. a gain) is
>>>>>> moderated by the treatment, which seems to be your main question.
>>>>>>
>>>>>> Now in this model, you can also have by-subject random slopes for
>>>>>> PrePost of course (because the fixed effect of PrePost is present for every
>>>>>> subject).
>>>>>>
>>>>>> Best, Ren?
>>>>>>
>>>>>>
>>>>>> Am Sa., 23. M?rz 2019 um 10:12 Uhr schrieb Souheyla GHEBGHOUB <
>>>>>> souheyla.ghebghoub at gmail.com>:
>>>>>>
>>>>>>> I read that in multinomial regression, all independent variables
>>>>>>> should be
>>>>>>> variables that we manipulate. Can I still have pretest as IV without
>>>>>>> skewing my results?
>>>>>>>
>>>>>>> Best,
>>>>>>> Souheyla
>>>>>>>
>>>>>>> On Fri, 22 Mar 2019, 23:31 Souheyla GHEBGHOUB, <
>>>>>>> souheyla.ghebghoub at gmail.com>
>>>>>>> wrote:
>>>>>>>
>>>>>>> > Thank you both. I will look into this and see :)
>>>>>>> >
>>>>>>> > Best,
>>>>>>> > Souheyla
>>>>>>> >
>>>>>>> > On Fri, 22 Mar 2019, 22:02 Uanhoro, James, <
>>>>>>> uanhoro.1 at buckeyemail.osu.edu>
>>>>>>> > wrote:
>>>>>>> >
>>>>>>> >> In standard regression models, the assumption is predictor
>>>>>>> variables are
>>>>>>> >> measured without error. Test scores will have measurement error,
>>>>>>> hence
>>>>>>> >> Doran's comment when test scores are used as covariates. See:
>>>>>>> Hausman, J.
>>>>>>> >> (2001). Mismeasured Variables in Econometric Analysis: Problems
>>>>>>> from the
>>>>>>> >> Right and Problems from the Left. *Journal of Economic
>>>>>>> Perspectives*,
>>>>>>> >> *15*(4), 57?67. https://doi.org/10.1257/jep.15.4.57
>>>>>>> >> I will note that many practitioners ignore this issue, and it is
>>>>>>> quite
>>>>>>> >> common to use predictors measured with error. Consider the number
>>>>>>> of times
>>>>>>> >> people use polychotomized income measures, or SES measures as
>>>>>>> predictors,
>>>>>>> >> or some other "construct".
>>>>>>> >> On Mar 22 2019, at 5:39 pm, Souheyla GHEBGHOUB <
>>>>>>> >> souheyla.ghebghoub at gmail.com> wrote:
>>>>>>> >>
>>>>>>> >> Dear Doran,
>>>>>>> >>
>>>>>>> >> Could you explain more this point to me, please?
>>>>>>> >>
>>>>>>> >> Thank you,
>>>>>>> >> Souheyla
>>>>>>> >>
>>>>>>> >> On Fri, 22 Mar 2019, 21:19 Doran, Harold, <HDoran at air.org> wrote:
>>>>>>> >>
>>>>>>> >> Yes, but conditioning on the pre-test means you are using a
>>>>>>> variable
>>>>>>> >> measured with error and the estimates you obtain and now
>>>>>>> inconsistent, and
>>>>>>> >> that?s a pretty big sin.
>>>>>>> >>
>>>>>>> >> On 3/22/19, 3:49 PM, "Souheyla GHEBGHOUB" <
>>>>>>> souheyla.ghebghoub at gmail.com>
>>>>>>> >> wrote:
>>>>>>> >>
>>>>>>> >> Dear Ren?,
>>>>>>> >>
>>>>>>> >> Thank you for your feedback to me. You are right, dropping the
>>>>>>> pretest
>>>>>>> >> from
>>>>>>> >> covariate if I predict change definitely makes sense to me! But
>>>>>>> the fact
>>>>>>> >> that i need to control for the starting levels of participants
>>>>>>> makes it
>>>>>>> >> obligatory for me to chose the second way, which is predicting
>>>>>>> posttest
>>>>>>> >> instead of change to have pretest scores controlled for.
>>>>>>> >>
>>>>>>> >> You also chose (1+group | word) , which is new to me. Does it
>>>>>>> intend to
>>>>>>> >> assume the effect of group to vary across words, which is
>>>>>>> something
>>>>>>> >> applicable to my data, right?
>>>>>>> >> I will discuss all this with my supervisor, and may reply here
>>>>>>> again in
>>>>>>> >> few
>>>>>>> >> days if you do not mind.
>>>>>>> >> Thank you very much
>>>>>>> >> Souheyla
>>>>>>> >> University of York
>>>>>>> >>
>>>>>>> >>
>>>>>>> >> On Fri, 22 Mar 2019 at 13:42, Ren? <bimonosom at gmail.com> wrote:
>>>>>>> >>
>>>>>>> >> Hi Souheyla,
>>>>>>> >>
>>>>>>> >> it seems to me that you will run into problems with your coding of
>>>>>>> >> change
>>>>>>> >> (gain, no gain and decline) because the 'change' is by
>>>>>>> >> definition/calculation depending on the predictor pretest.
>>>>>>> >> See, according to your coding scheme:
>>>>>>> >> Change = decline can only occur if pretest=1 (not by pretest=0).
>>>>>>> >> Change = gain can only occur if pretest = 0 (not by pretest=1)
>>>>>>> >> Change = No Gain can occur if pretest= 1 or 0
>>>>>>> >> In other words:
>>>>>>> >> If pretest = 1 then the possible outcomes can be decline or no
>>>>>>> gain
>>>>>>> >> If pretest = 0 then the possible outcomes can be gain or no gain
>>>>>>> >>
>>>>>>> >> And if the model result shows you then that the pre-test is
>>>>>>> >> significantly
>>>>>>> >> related to p(change-outcome), I guess there is no surprise in it,
>>>>>>> is it?
>>>>>>> >>
>>>>>>> >> So the first solution to this would be simply kicking the pre-test
>>>>>>> >> predictor out of the model completely, and predict:
>>>>>>> >> mod1 <- brm(Change ~ Group + (1|Subject) + (1+Group|Word),...)
>>>>>>> >> (Btw.: actually the first Hierarchical Bayes Model question I see
>>>>>>> on the
>>>>>>> >> mixed-effects mailing list :))
>>>>>>> >>
>>>>>>> >> Attempt for a further clarification on which random slopes would
>>>>>>> reflect
>>>>>>> >> the model's design:
>>>>>>> >> If you have a within-subjects design, by-subject random slopes are
>>>>>>> >> possible for the within-subject variable (e.g. if there are two
>>>>>>> sets of
>>>>>>> >> words/lists [e.g. abstract vs. concrete words] for each
>>>>>>> participant, and
>>>>>>> >> you test whether there is a performance-difference between these
>>>>>>> >> word-lists, then you can implement by-subject random slopes for
>>>>>>> words,
>>>>>>> >> because each participant has seen both sets.) If each participant
>>>>>>> has
>>>>>>> >> seen
>>>>>>> >> only one list (i.e. between subjects design) by subject random
>>>>>>> slopes
>>>>>>> >> for
>>>>>>> >> words are not appropriate, because there is no 'slope' by
>>>>>>> participant
>>>>>>> >> (i.e.
>>>>>>> >> by definition, having a slope requires at least two
>>>>>>> observations...).
>>>>>>> >> This
>>>>>>> >> is always a good rule of thumb without thinking about it too
>>>>>>> heavily :)
>>>>>>> >> Ans as you see: you can define a random slope for words:
>>>>>>> >> (1+Group|Word),
>>>>>>> >> because each word has been presented in each group (i.e. there
>>>>>>> can be a
>>>>>>> >> slope for each word). And intuitively speaking the
>>>>>>> Treatment-effect can
>>>>>>> >> vary depending on the stimuli you use, and the slope makes sense.
>>>>>>> (You
>>>>>>> >> also
>>>>>>> >> see in this example that the treatment effect can also vary by
>>>>>>> subjects,
>>>>>>> >> but in fact, this subject effect variation IS EQUAL to the effect
>>>>>>> you
>>>>>>> >> want
>>>>>>> >> to test, and having by subject group random slopes would
>>>>>>> eliminate the
>>>>>>> >> fixed effect...)
>>>>>>> >>
>>>>>>> >> Anyway, there is a second possibility to define your model,
>>>>>>> depending on
>>>>>>> >> how you want to interpret it. In the previous model you can say
>>>>>>> >> something
>>>>>>> >> about the type-of-change likelihoods depending on the treatment
>>>>>>> group.
>>>>>>> >> But
>>>>>>> >> you could implement the model as binomial as well (i.e. logistic
>>>>>>> >> regression)
>>>>>>> >>
>>>>>>> >> mod2 <- brm(posttest ~ pretest*Group + (1|Subject) +
>>>>>>> (1+Group|Word),...)
>>>>>>> >>
>>>>>>> >> And what you would expect here would be an interaction between
>>>>>>> pre-test
>>>>>>> >> and Group. For instance; if pretest=0 & treatment 1 then posttest
>>>>>>> larger
>>>>>>> >> than with pretest=0 & treatment 2; but not when pretest=1
>>>>>>> (because this
>>>>>>> >> is
>>>>>>> >> a plausible no gain situation). And so on...
>>>>>>> >> (And in this model there are no also no further random slopes
>>>>>>> hidden in
>>>>>>> >> your design :))
>>>>>>> >> Hope this helps.
>>>>>>> >>
>>>>>>> >> Best, Ren?
>>>>>>> >>
>>>>>>> >>
>>>>>>> >> Am Do., 21. M?rz 2019 um 14:01 Uhr schrieb Souheyla GHEBGHOUB <
>>>>>>> >> souheyla.ghebghoub at gmail.com>:
>>>>>>> >>
>>>>>>> >> Dear Philip,
>>>>>>> >>
>>>>>>> >> I understand , here is the structure of my data in case it could
>>>>>>> help.
>>>>>>> >>
>>>>>>> >> I have 3 groups of participants (control, treatment1,
>>>>>>> treatment2). Each
>>>>>>> >> group was tested twice, once before treatment (pretest) and once
>>>>>>> after
>>>>>>> >> treatment (posttest).
>>>>>>> >> In each test, they were tested on knowledge of 28 words, scores
>>>>>>> are
>>>>>>> >> dichotomous (0 = unknown , 1 = known). Tests are the same.
>>>>>>> >>
>>>>>>> >> I calculated change from pretest to posttest :
>>>>>>> >> if pretest 0 and posttest 0 = no gain
>>>>>>> >> if pretest 1 and posttest 1 = no gain
>>>>>>> >> if pretest 0 and posttest 1 = gain
>>>>>>> >> if pretest 1 and posttest 0 = decline
>>>>>>> >> So I ended up with a dependent variable called Change with 3
>>>>>>> levels
>>>>>>> >> (no_gain, gain, decline) and I tried to predict it using Group and
>>>>>>> >> Pretest
>>>>>>> >> as covariates using multinomial logit model. mod0 <- brm(Change ~
>>>>>>> >> Pretest
>>>>>>> >> +
>>>>>>> >> Group) I would like to add random effects for subjects but don't
>>>>>>> know
>>>>>>> >> what's the best form when Time factor is absent.
>>>>>>> >>
>>>>>>> >> I hope other statisticians who read this could help
>>>>>>> >> Thank you
>>>>>>> >> Souheyla
>>>>>>> >>
>>>>>>> >> [[alternative HTML version deleted]]
>>>>>>> >>
>>>>>>> >> _______________________________________________
>>>>>>> >> R-sig-mixed-models at r-project.org mailing list
>>>>>>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>> >>
>>>>>>> >>
>>>>>>> >>
>>>>>>> >> [[alternative HTML version deleted]]
>>>>>>> >>
>>>>>>> >> _______________________________________________
>>>>>>> >> R-sig-mixed-models at r-project.org mailing list
>>>>>>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>> >>
>>>>>>> >>
>>>>>>> >>
>>>>>>> >>
>>>>>>> >> [[alternative HTML version deleted]]
>>>>>>> >>
>>>>>>> >> _______________________________________________
>>>>>>> >> R-sig-mixed-models at r-project.org mailing list
>>>>>>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>> >>
>>>>>>> >>
>>>>>>>
>>>>>>>         [[alternative HTML version deleted]]
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>>
>>>>>>

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Tue Mar 26 09:29:44 2019
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Tue, 26 Mar 2019 09:29:44 +0100
Subject: [R-sig-ME] 
 Fixing singularity in a generalized linear mixed effect model
In-Reply-To: <CA+6N3yUq1s=XfHo6-GS26cc_doGLMQzjBqVhCUo63Szg=xUmfg@mail.gmail.com>
References: <CA+6N3yUq1s=XfHo6-GS26cc_doGLMQzjBqVhCUo63Szg=xUmfg@mail.gmail.com>
Message-ID: <CAJuCY5z3cYdnrsE6UJctA5ULBBmWoruQgV43AM_opatdibTRXA@mail.gmail.com>

Dear Alessandra,

Your problem is hard to diagnose without the data. Can you make the data
available? Does the combination of factors lead to unique observations? Or
do some combinations have only zero's?

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op wo 20 mrt. 2019 om 23:19 schreef Alessandra Bielli <
bielli.alessandra at gmail.com>:

> Dear List
>
> I am fitting this model using the lme4 package, in order to obtain catch
> estimates using the predict function
>
> m1 <- glmer(Count ~ CE + offset(log(Effort)) + (1|SetYear) +(1|Season) +
>                   (1|Lance.N) + (1|Boat.Name) + (1|Observer.Name), data =
> Data,                     glmerControl(optimizer = "bobyqa"), family=
> "poisson")
>
>
> where: CE is a categorical (control or treatment), Effort is numerical
> (fishing effort), and all the other variables are random effects.
>
> *My problem is that I get a warning message saying that the model is
> singular*
>
> *>summary(m1)*
>
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) [glmerMod]
>  Family: poisson  ( log )
> Formula: Count ~ CE + offset(log(Effort)) + (1 | SetYear) + (1 |
>     Season) + (1 | Lance.N) + (1 | Boat.Name) + (1 | Observer.Name)
>    Data: Data
> Control: glmerControl(optimizer = "bobyqa")
>
>      AIC      BIC   logLik deviance df.resid
>    148.6    174.3    -67.3    134.6      285
>
> Scaled residuals:
>     Min      1Q  Median      3Q     Max
> -0.4852 -0.1758 -0.1339 -0.1227  3.5980
>
> Random effects:
>  Groups        Name        Variance  Std.Dev.
>  Lance.N       (Intercept) 2.259e+00 1.503e+00
>  Boat.Name     (Intercept) 0.000e+00 0.000e+00
>  Observer.Name (Intercept) 0.000e+00 0.000e+00
>  Season        (Intercept) 4.149e-17 6.442e-09
>  SetYear       (Intercept) 0.000e+00 0.000e+00
> Number of obs: 292, groups:
> Lance.N, 146; Boat.Name, 21; Observer.Name, 5; Season, 4; SetYear, 4
>
> Fixed effects:
>             Estimate Std. Error z value Pr(>|z|)
> (Intercept)  -2.5751     0.6612  -3.895 9.83e-05 ***
> CEE          -0.5878     0.5003  -1.175     0.24
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> Correlation of Fixed Effects:
>     (Intr)
> CEE -0.257
> *convergence code: 0*
> *singular fit*
>
> I am aware that there are a lot of random effects and some of them have a
> number of levels <5. However, this study was carried out under real fishery
> conditions, so these random effects seemed all important to me.
>
> I removed the random effects with variance zero as suggested here
>
> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#singular-models-random-effect-variances-estimated-as-zero-or-correlations-estimated-as---1
> until I removed them all and found myself with a glm instead.
>
> My questions are
>
> - why the variance of Lance.N, initially positive, becomes zero after I
> remove the other random effects that had variance equal zero?
> - is it acceptable to fit a glm just because all the random effect
> variances were zero?
>
> I hope I gave all the information you need.
>
> Thanks for any advice!
>
> Alessandra
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From b|mono@om @end|ng |rom gm@||@com  Tue Mar 26 10:08:25 2019
From: b|mono@om @end|ng |rom gm@||@com (=?UTF-8?B?UmVuw6k=?=)
Date: Tue, 26 Mar 2019 10:08:25 +0100
Subject: [R-sig-ME] 
 Fixing singularity in a generalized linear mixed effect model
In-Reply-To: <CAJuCY5z3cYdnrsE6UJctA5ULBBmWoruQgV43AM_opatdibTRXA@mail.gmail.com>
References: <CA+6N3yUq1s=XfHo6-GS26cc_doGLMQzjBqVhCUo63Szg=xUmfg@mail.gmail.com>
 <CAJuCY5z3cYdnrsE6UJctA5ULBBmWoruQgV43AM_opatdibTRXA@mail.gmail.com>
Message-ID: <CADcpBHMFDhDyjpixq_gZyyKKLrDHU-_JrY1vjj7jYrohjffKsw@mail.gmail.com>

Hi Allessandra,

your model output says:
"Number of obs: 292"
and your model has 2 fixed effects and 5 (!) random effects.
 - If -  all these random effects are fully crossed. Then assuming you have
19 participants (e.g. 1|observers), and 4 random effects crossed on them
with two levels each (2*2*2*2 = 16 cells), would make about 292
observations.
Now you see this math uncovers the most likely problem: A random effect
(intercept) factor is urgently recommended to have least! 6 levels to make
such a model meaningful).
If all these random effects are not fully crossed, then the model is
misspecified, i.e. defining random intercepts for factor 1 separately from
random intercepts for another factor 2, when factor 1 is nested in factor
2, is over-identifying the randomness in your model -> singular.

So,
only define random intercepts for factors with more then 6 levels (move
those factors with less levels to the fixed effects instead to control for
them)
only define separate random intercepts for factors that are crossed; for
instance the factors boat name and lance seem suspicious. I guess, there is
a world in which a 'lance 1' can only be on boat 'atlantis' to be used for
fishing, but not on boat 'moby dick'. In this case, having a random
intercept for "boat name" in addition to 'lance' would not add anything to
the model, since lances would already cover the variance of boats (lances
nested in boats).
Get more observations
Rerun the model
Should be fine :)

If singularities still occur use Bayesian models or come back here :))

Best, Ren?


Am Di., 26. M?rz 2019 um 09:30 Uhr schrieb Thierry Onkelinx via
R-sig-mixed-models <r-sig-mixed-models at r-project.org>:

> Dear Alessandra,
>
> Your problem is hard to diagnose without the data. Can you make the data
> available? Does the combination of factors lead to unique observations? Or
> do some combinations have only zero's?
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op wo 20 mrt. 2019 om 23:19 schreef Alessandra Bielli <
> bielli.alessandra at gmail.com>:
>
> > Dear List
> >
> > I am fitting this model using the lme4 package, in order to obtain catch
> > estimates using the predict function
> >
> > m1 <- glmer(Count ~ CE + offset(log(Effort)) + (1|SetYear) +(1|Season) +
> >                   (1|Lance.N) + (1|Boat.Name) + (1|Observer.Name), data =
> > Data,                     glmerControl(optimizer = "bobyqa"), family=
> > "poisson")
> >
> >
> > where: CE is a categorical (control or treatment), Effort is numerical
> > (fishing effort), and all the other variables are random effects.
> >
> > *My problem is that I get a warning message saying that the model is
> > singular*
> >
> > *>summary(m1)*
> >
> > Generalized linear mixed model fit by maximum likelihood (Laplace
> > Approximation) [glmerMod]
> >  Family: poisson  ( log )
> > Formula: Count ~ CE + offset(log(Effort)) + (1 | SetYear) + (1 |
> >     Season) + (1 | Lance.N) + (1 | Boat.Name) + (1 | Observer.Name)
> >    Data: Data
> > Control: glmerControl(optimizer = "bobyqa")
> >
> >      AIC      BIC   logLik deviance df.resid
> >    148.6    174.3    -67.3    134.6      285
> >
> > Scaled residuals:
> >     Min      1Q  Median      3Q     Max
> > -0.4852 -0.1758 -0.1339 -0.1227  3.5980
> >
> > Random effects:
> >  Groups        Name        Variance  Std.Dev.
> >  Lance.N       (Intercept) 2.259e+00 1.503e+00
> >  Boat.Name     (Intercept) 0.000e+00 0.000e+00
> >  Observer.Name (Intercept) 0.000e+00 0.000e+00
> >  Season        (Intercept) 4.149e-17 6.442e-09
> >  SetYear       (Intercept) 0.000e+00 0.000e+00
> > Number of obs: 292, groups:
> > Lance.N, 146; Boat.Name, 21; Observer.Name, 5; Season, 4; SetYear, 4
> >
> > Fixed effects:
> >             Estimate Std. Error z value Pr(>|z|)
> > (Intercept)  -2.5751     0.6612  -3.895 9.83e-05 ***
> > CEE          -0.5878     0.5003  -1.175     0.24
> > ---
> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >
> > Correlation of Fixed Effects:
> >     (Intr)
> > CEE -0.257
> > *convergence code: 0*
> > *singular fit*
> >
> > I am aware that there are a lot of random effects and some of them have a
> > number of levels <5. However, this study was carried out under real
> fishery
> > conditions, so these random effects seemed all important to me.
> >
> > I removed the random effects with variance zero as suggested here
> >
> >
> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#singular-models-random-effect-variances-estimated-as-zero-or-correlations-estimated-as---1
> > until I removed them all and found myself with a glm instead.
> >
> > My questions are
> >
> > - why the variance of Lance.N, initially positive, becomes zero after I
> > remove the other random effects that had variance equal zero?
> > - is it acceptable to fit a glm just because all the random effect
> > variances were zero?
> >
> > I hope I gave all the information you need.
> >
> > Thanks for any advice!
> >
> > Alessandra
> >
> >         [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From |uc@@cor|@tt| @end|ng |rom boku@@c@@t  Tue Mar 26 13:33:59 2019
From: |uc@@cor|@tt| @end|ng |rom boku@@c@@t (Boku mail)
Date: Tue, 26 Mar 2019 13:33:59 +0100
Subject: [R-sig-ME] Mixed model compositional data
In-Reply-To: <a93bffb0-f845-43bf-a472-f87a69e07ab1@Spark>
References: <a93bffb0-f845-43bf-a472-f87a69e07ab1@Spark>
Message-ID: <ccd08bcb-dc02-4729-b601-f186631357ca@Spark>

Dear list members,
Suppose I have a dataset like this:

FACTOR1?FACTOR2?TREATMENT?AVAILABLE?RESTING?WALKING?FEEDING?DRINKING
1?????1?????A??????66?????40????12????6?????8
1?????1?????B??????72?????28????22????8?????14
1?????1?????C??????50?????20????10????4?????16
2?????1?????A??????63?????30????12????8?????13
2?????1?????B??????59?????13????27????4?????15
2?????1?????C??????50?????18????20????3?????9
1?????2?????A??????70?????40????20????5?????5
1?????2?????B??????80?????45????15????5?????15
1?????2?????C??????60?????15????15????20?????10
2?????2?????A??????58?????28????20????4?????6
2?????2?????B??????75?????18????22????10?????15
2?????2?????C??????51?????19????21????5??????6
?

Where I have behavioral data (?RESTING?, ?WALKING?, ?FEEDING?, ?DRINKING?) collected using scan sampling (i.e., each row corresponds to one scan, where a number ?AVAILABLE? of animals have been observed, and the behavior of each animal has been categorized into ?resting?, ?walking? ?feeding? or ?drinking?). Additionally, observations are grouped in 2 factors (?FACTOR1? and ?FACTOR2?) to be considered as random. I am interested in knowing the effect of the variable ?TREATMENT? on each behavioral category.

The main problem is that the behavioral variables represent compositional data (i.e. they sum up to the ?AVAILABLE? number of animals; to put it in other words, the proportions of animals within each behavioral category sums up to 1). The packages dealing with compositional data do not appear to handle random factors, thus I am wondering what would be the most sensible way to deal with this kind of data.

Would this workaround make sense?
First,?reorganize the multivariate data?with an extra lowest level indicating the responses:

FACTOR1?FACTOR2?TREATMENT?AVAILABLE?BEHAVIOR??RESPONSE
1?????1?????A??????66?????RESTING?????40
1?????1?????B??????72?????RESTING?????28
1?????1?????C??????50?????RESTING?????20
2?????1?????A??????63?????RESTING?????30
2?????1?????B??????59?????RESTING?????13
2?????1?????C??????50?????RESTING?????20
1?????2?????A??????70?????RESTING?????18
1?????2?????B??????80?????RESTING?????45
1?????2?????C??????60?????RESTING?????15
2?????2?????A??????58?????RESTING?????28
2?????2?????B??????75?????RESTING?????18
2?????2?????C??????51?????RESTING?????19
?

Then, fit a model like:

Mod <- glmmTMB(RESPONSE ~ TREATMENT * BEHAVIOR + (1|FACTOR1) +?(1|FACTOR2) +?offset(log(AVAILABLE), family=Poisson, data=my.data)

I am not sure this workaround would deal efficiently with the compositional nature of the data.

Thanks a lot in advance!
Best,
Luca

	[[alternative HTML version deleted]]


From @ouhey|@@ghebghoub @end|ng |rom gm@||@com  Tue Mar 26 14:09:00 2019
From: @ouhey|@@ghebghoub @end|ng |rom gm@||@com (Souheyla GHEBGHOUB)
Date: Tue, 26 Mar 2019 13:09:00 +0000
Subject: [R-sig-ME] Random effects in multinomial regression in R?
In-Reply-To: <CADcpBHNWaw=CqL8S8cRDzoou4fMconYZokijUSj1G1q8bwoGZQ@mail.gmail.com>
References: <CAEA998ggNw91Y93TEyPy=_t6JuJfsTdse+77H7WAdkXQTrgVOw@mail.gmail.com>
 <2AF9E03B-C3BC-4C3F-8BD7-053CB8037205@getmailspring.com>
 <CAEA998g4rYVZoDQikUJXQCM14jD6D54NokXbOBVAe2LqV8V+rA@mail.gmail.com>
 <CAEA998gcf_7Tx-fBsqY76JZzK6f-WG3yURJFbb83j-boknjyyQ@mail.gmail.com>
 <CADcpBHN_wmSacX5=LsrMSoNwFKMZxHssMXrBW5B0VTYr68tv9g@mail.gmail.com>
 <CAEA998gWs2PO1NDjfn94NWR2U_AmYHDs3K+drhRAW8GLEvvtSw@mail.gmail.com>
 <CADcpBHMrpJ7uevVwH_4BvUn15mSaNzW865Fx77s0up8u6thy-A@mail.gmail.com>
 <CAEA998iO=CgnrhJNvOjkQpHBJ65NJQd4vRWM1PdWOZwe_MSUdA@mail.gmail.com>
 <CADcpBHPA4t9sN6gSVW6fHF3apxN6jP66O1bevC_CoFLDYszP-g@mail.gmail.com>
 <CAEA998hgxcg7Xd4S5W+yzP75G8+QqRq6TgZ+ig5pW4wec-emmA@mail.gmail.com>
 <CADcpBHNWaw=CqL8S8cRDzoou4fMconYZokijUSj1G1q8bwoGZQ@mail.gmail.com>
Message-ID: <CAEA998j7_1s==q0jJv1ziAUAQ1FynW0+NJFRQwPVEqSRgF==Dw@mail.gmail.com>

Thank you very much Rene for this feedback.

I will surely consider it.

Best,
Souheyla

On Sun, 24 Mar 2019, 13:39 Ren?, <bimonosom at gmail.com> wrote:

> Ok. I try a short one :)
> Also going back to your first mail:
>
> This model is what you want:
>
> score~Time*Group+(1+Time|subject)+(1+Time*Group | words)
>
> No skew in here ;) (but maybe model identification issues depending on the
> number of observations you got.) In other words, (random) by-word
> variations and (random) by-participant variations are taken into account,
> and the "average across stimuli" problem articulated here (Judd, Westfall,
> & Kenny, 2012) is appropriately tackled. As you do not explicitly reference
> to this paper, I just assume that you mean this kind of "skew" (alpha
> inflation due to stimulus aggregation), because I am currently not aware of
> other theoretically or statistically relevant 'skews' there could be.
>
> Best, Ren?
>
> Ps: Why not using brms... :)  You get parameter likelihood estimates and
> Bayesian credible intervals for your effect estimates? The model code and
> output-format are basically the same. And you don't need to report p-values
> :))
> Pps: For making the above formula work, you need to recode your data
> matrix to:
>
> Subject word Group  Time Score
> 1 1 control Pre 1
> 1 1 control Post 1
> 1 2 control Pre 1
> 1 2 control Post 1
> 1 3 control Pre 0
> 1 3 control Post 0
> 1 4 control Pre 0
> 1 4 control Post 0
>
> Am So., 24. M?rz 2019 um 09:36 Uhr schrieb Souheyla GHEBGHOUB <
> souheyla.ghebghoub at gmail.com>:
>
>> Dear Ren?,
>>
>> When I do Time (PrePost), score (response) will return to be dichotomous
>> (0, 1) and I will be using logistic regression (glmer) using lme4 package
>> instead of brms.
>>
>> So my previous question was about glmer model and whether its results
>> reflect a prepost comparison based on word to word analysis of each
>> participant instead of their sums; the latter will skew results.
>> I appreciate your constant help,
>> Thank you
>> Souheyla
>>
>> On Sat, 23 Mar 2019, 22:01 Ren?, <bimonosom at gmail.com> wrote:
>>
>>> Souheyla, it's a solvable problem :)
>>>  But does the model analyse at the level of every word or at the level
>>> of sum of words?
>>> it's both.
>>> Yes, the model tries to predict general tendencies (means/proportions,
>>> conditioned on factors). Observation sums are always involved.
>>> The second answer is, if there is no factor conditioning, then that's
>>> it. With factors, its measuring by item-differences if you want: And it
>>> seems you want: Let 'word type' interact with 'treatment' to predict gain
>>> proportions (0's and 1's for each participant). The relation between word
>>> type and treatment is your domain, but your statistical issues imply the
>>> following brms model: It measures differences between single
>>> questions/words (pre-post on average/proportion on log-scale). If not, the
>>> answer is still 'yes': Hierarchical Bayesian logistic regression can do
>>> what you want :)). I assume, in terms of mixed-effect models (brm is not
>>> mixed-effects least-mean squares, it's hierarchical Bayes) I would go with:
>>>
>>> mod5<-brm(score~Group*Time*Stims+(1|subject)+(1+Time*Stims| words),
>>> family=bernoulli(),...)
>>>
>>> What you will get are Bayesian likelihood estimates for each
>>> mean/proportion (and difference) across participants, for each Group by
>>> every word. After estimation, you can apply traditional statistical
>>> thinking like comparing mean estimates by word, or in general. These (both)
>>> can be accessed via 'emmeans', or 'emmip', functions and you also can
>>> compute Bayes Factors for a change vs no-change hypotheses (or others) for
>>> each question/word (using the hypothesis() function, or prior_samples()
>>>  and posterior_samples() ), if you follow your own rules ;)
>>> Best, Ren?
>>>
>>> Am Sa., 23. M?rz 2019 um 18:58 Uhr schrieb Souheyla GHEBGHOUB <
>>> souheyla.ghebghoub at gmail.com>:
>>>
>>>> Dear Ren? and any other member interested in this discussion,
>>>>
>>>> I appreciate the long feedback I received from you. But I can tell I
>>>> could not well convey my concern.
>>>>
>>>> The aim of my analysis is to predict the odds of correct/incorrect
>>>> responses based on some predictors: group (Treatment1/Treatment2/Control);
>>>> Time (pretest/posttest), verbal frequency, concreteness vs.
>>>> abstractness...etc. I have hypotheses such as, Time*Group interaction will
>>>> show a significant effect as I expect students in Treatment Group 1 will
>>>> outperform at the level of Posttest.
>>>> I have a problem of predicting Response with Time as an Independent
>>>> variable which could be summarised in this example of one participant:
>>>>
>>>> Subject word Group  Pretest Posttest
>>>> 1 1 control 0 1
>>>> 1 2 control 0 1
>>>> 1 3 control 1 1
>>>> 1 4 control 1 1
>>>> 1 5 control 1 0
>>>> 1 6 control 0 0
>>>> 1 7 control 0 0
>>>> 1 8 control 0 0
>>>> =3 =4
>>>>
>>>> Would the model compare the mean score of pretest against mean score of
>>>> posttest? If yes, it will not truly reflect the data, because its not about
>>>> the sum,
>>>> its about every word on its own. So it should not be about 4 in
>>>> posttest against 3 in pretest means 1 gain.
>>>>  In here, there are 2 gains in word 1,2 and one decline in word 5. But
>>>> does the model analyse at the level of every word or at the level of sum of
>>>> words?
>>>> That?s the main problem I have not solved since 3 months now.
>>>> Best,
>>>> SOUHEYLA
>>>>
>>>> On Sat, 23 Mar 2019 at 12:46, Ren? <bimonosom at gmail.com> wrote:
>>>>
>>>>> Hey, things clear up. Thanks for the picture. I want to challenge your
>>>>> concerns:
>>>>> First, you see that observing a main effect of PrePost (or "Time") in
>>>>> mod3 can only mean two things: 1) performance goes up (positive effect); or
>>>>> 2) performance goes down (negative effect), in general. Saying something
>>>>> like this, is what the model is defined for. The conclusion will be this
>>>>> "general increase" or "general decrease" or "no evidence here". If you have
>>>>> a different question from that (i.e. on an item level), then you should
>>>>> specify it in more detail (see below).
>>>>> There are three issues in-between the lines of your questions:
>>>>> 1. is it a statistical concern you have?
>>>>> 2. is it an actual theoretical question you have?
>>>>> 3. is it a matter of making a non-result to a result?
>>>>>
>>>>> 1. The statistical view:
>>>>> Counter-question: who would ever assume that a 1 score for a word in a
>>>>> pre-test will remain constant until eternity. And why should it? The answer
>>>>> is: Nobody, and this is the reason statistics (probability theory) exists
>>>>> :). So the first simple answer to your question is you do not need to test
>>>>> whether observed gains from pre-to-post are 'genuine' (only from 0 to 1,
>>>>> without decline cases) because 'nature' guarantees that there will be
>>>>> decline somewhere. That's "randomness" :)  But the question is, what's
>>>>> stronger, gain or decline? See... and there is no problem in it: a general
>>>>> main effect (e.g. an overall gain) still is an overall gain, even if some
>>>>> cases decline.
>>>>>
>>>>> 2. Theoretical view:
>>>>> If, however, such special item dynamics are theoretized in advance,
>>>>> then simply test it :) For instance, the assumption whether the
>>>>> treatment leads to gain on abstract words, but to decline on concrete
>>>>> words, then should find into the model by coding the factor AbsCon
>>>>> (abstract vs. concrete words)  as fixed effect (assuming a within
>>>>> participants manipulation).
>>>>> mod4<-brm(score~PrePost*Group*AbsCon+(1+PrePost*AbsCon| subject) +
>>>>> (1+group|words),...)
>>>>> (Note: the 1+XX|subject just means random intercept  for subject (1+)
>>>>> plus slope for XX on subject; and writing (1+group|words) is the same
>>>>> as writing (group|words), but you can estimate the slope without the
>>>>> (word) intercept by writing (0+group|words))
>>>>>
>>>>> Without having such a theoretical account testing for can also be done
>>>>> via:
>>>>> mod4<-brm(score~PrePost*Group*words+(1+PrePost*words| subject),...)
>>>>> But you will hardly be able to interpret the interactions in this
>>>>> model because words alone has 28 levels.
>>>>>
>>>>> 3. But, your example seems to suggest a special case... (i.e. an
>>>>> actual Null-Effect):
>>>>> "If a participant has  got 5 correct words out of 28 in both pretest
>>>>> and posttest"
>>>>> then there would be no general effect of PrePost in the model above
>>>>> (generalizing to all participants now). And searching for "deeper" model
>>>>> checks looks like rescuing all effects you can get (post hoc). But of
>>>>> course, making an argument like there still is an effect, namely for
>>>>> 5 specific words, which is not observable because there is also a detriment
>>>>> for other 5 words is possible, but requires a solid theory which explicitly
>>>>> predicts this interaction, and an experiment which was explicitly designed
>>>>> to test this interaction. (point 2)
>>>>>
>>>>> So... if it is point 2 you got... Then go ahead :) test it in a
>>>>> meaningful way. Otherwise, simply treat this "effect by words" interaction
>>>>> as random slope (1+group|words), or btw. (1+PrePost*Group|words) is also
>>>>> possible..., which is basically 'statistically' integrating the idea that
>>>>> the treatment*time effects vary (randomly) between stimuli. And doing this
>>>>> in the random effects has the notion of "generalizing" estimation error in
>>>>> the population, and should be preferred to implementing those in the fixed
>>>>> effects, if the 28 words can be seen as a random (non-special) stimulus
>>>>> sample. If this is not the case, then consider coding the "special" thing
>>>>> about the words as fixed effects (e.g. if you want to use the same design
>>>>> again, for testing something, while controlling for stimulus specifics).
>>>>>
>>>>> Best, Ren?
>>>>>
>>>>> Am Sa., 23. M?rz 2019 um 11:59 Uhr schrieb Souheyla GHEBGHOUB <
>>>>> souheyla.ghebghoub at gmail.com>:
>>>>>
>>>>>> Dear Ren?,
>>>>>>
>>>>>> Thank you for the feedback. Actually, my model was originally like
>>>>>> you suggested now (except for slopes I had PrePost without 1 in both words
>>>>>> and subjects. I called PrePost as "Time". I will read more about the
>>>>>> 1+prepost form you mentioned.
>>>>>>
>>>>>> The reason why I gave up this model and looked for something else is
>>>>>> the fact that each subject has 28 words tested twice (pre&post) and I was
>>>>>> not too sure whether such model will take into consideration differences
>>>>>> between pre & post at the level of every word of each participant (which is
>>>>>> what I want) instead of merely comparing every participant's pre mean sores
>>>>>> of 28 words against his post mean score (which is what i should avoid), here
>>>>>> is a short example as to why:
>>>>>>
>>>>>>  If a participant has  got 5 correct words out of 28 in both pretest
>>>>>> and posttest, there will be multiple interpretations:  e.g. They could
>>>>>> refer to the same words (i.e. 0 gain), or they could be totally new words
>>>>>> (i.e. 5 gains) ...etc , hence I am not sure whether such model of pretest
>>>>>> vs posttest will compare each subject score of each word from pretest to
>>>>>> posttest then base its analysis on these score changes instead of comparing
>>>>>> the sum scores between pre and post and which likely skew results.
>>>>>>
>>>>>> I posted about this in stackexchange 3 months ago and was told that
>>>>>> it does compare word to word for every participant, but I am still not
>>>>>> confident enough to use it because all the accurateness of the results and
>>>>>> discussion chapters of my PhD thesis will be based on this decision.
>>>>>>
>>>>>> I look forward to receive feedback from you and any member reading
>>>>>> this,
>>>>>> Souheyla
>>>>>> University of York
>>>>>>
>>>>>> On Sat, 23 Mar 2019, 10:01 Ren?, <bimonosom at gmail.com> wrote:
>>>>>>
>>>>>>> Hi Souheyla,
>>>>>>>
>>>>>>> Well, I guess in your case it is simply more elegant to leave the
>>>>>>> measured predictor out of the fixed effects, because there is also another
>>>>>>> implied question (i.e. about the strength of change between pre and post).
>>>>>>>
>>>>>>> So, another possibility to re-define your model (as logistic
>>>>>>> regression) allowing for better interpretations:
>>>>>>> mod3<-brm(score~PrePost*Group+(1+PrePost | subject)+(1+group |
>>>>>>> words),...)
>>>>>>>
>>>>>>> score = 0 or 1 for a given testitem
>>>>>>> PrePost = Pre vs. Post  (basically just an indicator of the
>>>>>>> measurement time point)
>>>>>>> Thus, the PrePost main effect will tell, whether there is a change
>>>>>>> from pre to post (e.g. a gaint), and you can also tell how strong it is (in
>>>>>>> odds ratios).
>>>>>>> And if PrePost interacts with Group, then the change (e.g. a gain)
>>>>>>> is moderated by the treatment, which seems to be your main question.
>>>>>>>
>>>>>>> Now in this model, you can also have by-subject random slopes for
>>>>>>> PrePost of course (because the fixed effect of PrePost is present for every
>>>>>>> subject).
>>>>>>>
>>>>>>> Best, Ren?
>>>>>>>
>>>>>>>
>>>>>>> Am Sa., 23. M?rz 2019 um 10:12 Uhr schrieb Souheyla GHEBGHOUB <
>>>>>>> souheyla.ghebghoub at gmail.com>:
>>>>>>>
>>>>>>>> I read that in multinomial regression, all independent variables
>>>>>>>> should be
>>>>>>>> variables that we manipulate. Can I still have pretest as IV without
>>>>>>>> skewing my results?
>>>>>>>>
>>>>>>>> Best,
>>>>>>>> Souheyla
>>>>>>>>
>>>>>>>> On Fri, 22 Mar 2019, 23:31 Souheyla GHEBGHOUB, <
>>>>>>>> souheyla.ghebghoub at gmail.com>
>>>>>>>> wrote:
>>>>>>>>
>>>>>>>> > Thank you both. I will look into this and see :)
>>>>>>>> >
>>>>>>>> > Best,
>>>>>>>> > Souheyla
>>>>>>>> >
>>>>>>>> > On Fri, 22 Mar 2019, 22:02 Uanhoro, James, <
>>>>>>>> uanhoro.1 at buckeyemail.osu.edu>
>>>>>>>> > wrote:
>>>>>>>> >
>>>>>>>> >> In standard regression models, the assumption is predictor
>>>>>>>> variables are
>>>>>>>> >> measured without error. Test scores will have measurement error,
>>>>>>>> hence
>>>>>>>> >> Doran's comment when test scores are used as covariates. See:
>>>>>>>> Hausman, J.
>>>>>>>> >> (2001). Mismeasured Variables in Econometric Analysis: Problems
>>>>>>>> from the
>>>>>>>> >> Right and Problems from the Left. *Journal of Economic
>>>>>>>> Perspectives*,
>>>>>>>> >> *15*(4), 57?67. https://doi.org/10.1257/jep.15.4.57
>>>>>>>> >> I will note that many practitioners ignore this issue, and it is
>>>>>>>> quite
>>>>>>>> >> common to use predictors measured with error. Consider the
>>>>>>>> number of times
>>>>>>>> >> people use polychotomized income measures, or SES measures as
>>>>>>>> predictors,
>>>>>>>> >> or some other "construct".
>>>>>>>> >> On Mar 22 2019, at 5:39 pm, Souheyla GHEBGHOUB <
>>>>>>>> >> souheyla.ghebghoub at gmail.com> wrote:
>>>>>>>> >>
>>>>>>>> >> Dear Doran,
>>>>>>>> >>
>>>>>>>> >> Could you explain more this point to me, please?
>>>>>>>> >>
>>>>>>>> >> Thank you,
>>>>>>>> >> Souheyla
>>>>>>>> >>
>>>>>>>> >> On Fri, 22 Mar 2019, 21:19 Doran, Harold, <HDoran at air.org>
>>>>>>>> wrote:
>>>>>>>> >>
>>>>>>>> >> Yes, but conditioning on the pre-test means you are using a
>>>>>>>> variable
>>>>>>>> >> measured with error and the estimates you obtain and now
>>>>>>>> inconsistent, and
>>>>>>>> >> that?s a pretty big sin.
>>>>>>>> >>
>>>>>>>> >> On 3/22/19, 3:49 PM, "Souheyla GHEBGHOUB" <
>>>>>>>> souheyla.ghebghoub at gmail.com>
>>>>>>>> >> wrote:
>>>>>>>> >>
>>>>>>>> >> Dear Ren?,
>>>>>>>> >>
>>>>>>>> >> Thank you for your feedback to me. You are right, dropping the
>>>>>>>> pretest
>>>>>>>> >> from
>>>>>>>> >> covariate if I predict change definitely makes sense to me! But
>>>>>>>> the fact
>>>>>>>> >> that i need to control for the starting levels of participants
>>>>>>>> makes it
>>>>>>>> >> obligatory for me to chose the second way, which is predicting
>>>>>>>> posttest
>>>>>>>> >> instead of change to have pretest scores controlled for.
>>>>>>>> >>
>>>>>>>> >> You also chose (1+group | word) , which is new to me. Does it
>>>>>>>> intend to
>>>>>>>> >> assume the effect of group to vary across words, which is
>>>>>>>> something
>>>>>>>> >> applicable to my data, right?
>>>>>>>> >> I will discuss all this with my supervisor, and may reply here
>>>>>>>> again in
>>>>>>>> >> few
>>>>>>>> >> days if you do not mind.
>>>>>>>> >> Thank you very much
>>>>>>>> >> Souheyla
>>>>>>>> >> University of York
>>>>>>>> >>
>>>>>>>> >>
>>>>>>>> >> On Fri, 22 Mar 2019 at 13:42, Ren? <bimonosom at gmail.com> wrote:
>>>>>>>> >>
>>>>>>>> >> Hi Souheyla,
>>>>>>>> >>
>>>>>>>> >> it seems to me that you will run into problems with your coding
>>>>>>>> of
>>>>>>>> >> change
>>>>>>>> >> (gain, no gain and decline) because the 'change' is by
>>>>>>>> >> definition/calculation depending on the predictor pretest.
>>>>>>>> >> See, according to your coding scheme:
>>>>>>>> >> Change = decline can only occur if pretest=1 (not by pretest=0).
>>>>>>>> >> Change = gain can only occur if pretest = 0 (not by pretest=1)
>>>>>>>> >> Change = No Gain can occur if pretest= 1 or 0
>>>>>>>> >> In other words:
>>>>>>>> >> If pretest = 1 then the possible outcomes can be decline or no
>>>>>>>> gain
>>>>>>>> >> If pretest = 0 then the possible outcomes can be gain or no gain
>>>>>>>> >>
>>>>>>>> >> And if the model result shows you then that the pre-test is
>>>>>>>> >> significantly
>>>>>>>> >> related to p(change-outcome), I guess there is no surprise in
>>>>>>>> it, is it?
>>>>>>>> >>
>>>>>>>> >> So the first solution to this would be simply kicking the
>>>>>>>> pre-test
>>>>>>>> >> predictor out of the model completely, and predict:
>>>>>>>> >> mod1 <- brm(Change ~ Group + (1|Subject) + (1+Group|Word),...)
>>>>>>>> >> (Btw.: actually the first Hierarchical Bayes Model question I
>>>>>>>> see on the
>>>>>>>> >> mixed-effects mailing list :))
>>>>>>>> >>
>>>>>>>> >> Attempt for a further clarification on which random slopes would
>>>>>>>> reflect
>>>>>>>> >> the model's design:
>>>>>>>> >> If you have a within-subjects design, by-subject random slopes
>>>>>>>> are
>>>>>>>> >> possible for the within-subject variable (e.g. if there are two
>>>>>>>> sets of
>>>>>>>> >> words/lists [e.g. abstract vs. concrete words] for each
>>>>>>>> participant, and
>>>>>>>> >> you test whether there is a performance-difference between these
>>>>>>>> >> word-lists, then you can implement by-subject random slopes for
>>>>>>>> words,
>>>>>>>> >> because each participant has seen both sets.) If each
>>>>>>>> participant has
>>>>>>>> >> seen
>>>>>>>> >> only one list (i.e. between subjects design) by subject random
>>>>>>>> slopes
>>>>>>>> >> for
>>>>>>>> >> words are not appropriate, because there is no 'slope' by
>>>>>>>> participant
>>>>>>>> >> (i.e.
>>>>>>>> >> by definition, having a slope requires at least two
>>>>>>>> observations...).
>>>>>>>> >> This
>>>>>>>> >> is always a good rule of thumb without thinking about it too
>>>>>>>> heavily :)
>>>>>>>> >> Ans as you see: you can define a random slope for words:
>>>>>>>> >> (1+Group|Word),
>>>>>>>> >> because each word has been presented in each group (i.e. there
>>>>>>>> can be a
>>>>>>>> >> slope for each word). And intuitively speaking the
>>>>>>>> Treatment-effect can
>>>>>>>> >> vary depending on the stimuli you use, and the slope makes
>>>>>>>> sense. (You
>>>>>>>> >> also
>>>>>>>> >> see in this example that the treatment effect can also vary by
>>>>>>>> subjects,
>>>>>>>> >> but in fact, this subject effect variation IS EQUAL to the
>>>>>>>> effect you
>>>>>>>> >> want
>>>>>>>> >> to test, and having by subject group random slopes would
>>>>>>>> eliminate the
>>>>>>>> >> fixed effect...)
>>>>>>>> >>
>>>>>>>> >> Anyway, there is a second possibility to define your model,
>>>>>>>> depending on
>>>>>>>> >> how you want to interpret it. In the previous model you can say
>>>>>>>> >> something
>>>>>>>> >> about the type-of-change likelihoods depending on the treatment
>>>>>>>> group.
>>>>>>>> >> But
>>>>>>>> >> you could implement the model as binomial as well (i.e. logistic
>>>>>>>> >> regression)
>>>>>>>> >>
>>>>>>>> >> mod2 <- brm(posttest ~ pretest*Group + (1|Subject) +
>>>>>>>> (1+Group|Word),...)
>>>>>>>> >>
>>>>>>>> >> And what you would expect here would be an interaction between
>>>>>>>> pre-test
>>>>>>>> >> and Group. For instance; if pretest=0 & treatment 1 then
>>>>>>>> posttest larger
>>>>>>>> >> than with pretest=0 & treatment 2; but not when pretest=1
>>>>>>>> (because this
>>>>>>>> >> is
>>>>>>>> >> a plausible no gain situation). And so on...
>>>>>>>> >> (And in this model there are no also no further random slopes
>>>>>>>> hidden in
>>>>>>>> >> your design :))
>>>>>>>> >> Hope this helps.
>>>>>>>> >>
>>>>>>>> >> Best, Ren?
>>>>>>>> >>
>>>>>>>> >>
>>>>>>>> >> Am Do., 21. M?rz 2019 um 14:01 Uhr schrieb Souheyla GHEBGHOUB <
>>>>>>>> >> souheyla.ghebghoub at gmail.com>:
>>>>>>>> >>
>>>>>>>> >> Dear Philip,
>>>>>>>> >>
>>>>>>>> >> I understand , here is the structure of my data in case it could
>>>>>>>> help.
>>>>>>>> >>
>>>>>>>> >> I have 3 groups of participants (control, treatment1,
>>>>>>>> treatment2). Each
>>>>>>>> >> group was tested twice, once before treatment (pretest) and once
>>>>>>>> after
>>>>>>>> >> treatment (posttest).
>>>>>>>> >> In each test, they were tested on knowledge of 28 words, scores
>>>>>>>> are
>>>>>>>> >> dichotomous (0 = unknown , 1 = known). Tests are the same.
>>>>>>>> >>
>>>>>>>> >> I calculated change from pretest to posttest :
>>>>>>>> >> if pretest 0 and posttest 0 = no gain
>>>>>>>> >> if pretest 1 and posttest 1 = no gain
>>>>>>>> >> if pretest 0 and posttest 1 = gain
>>>>>>>> >> if pretest 1 and posttest 0 = decline
>>>>>>>> >> So I ended up with a dependent variable called Change with 3
>>>>>>>> levels
>>>>>>>> >> (no_gain, gain, decline) and I tried to predict it using Group
>>>>>>>> and
>>>>>>>> >> Pretest
>>>>>>>> >> as covariates using multinomial logit model. mod0 <- brm(Change ~
>>>>>>>> >> Pretest
>>>>>>>> >> +
>>>>>>>> >> Group) I would like to add random effects for subjects but don't
>>>>>>>> know
>>>>>>>> >> what's the best form when Time factor is absent.
>>>>>>>> >>
>>>>>>>> >> I hope other statisticians who read this could help
>>>>>>>> >> Thank you
>>>>>>>> >> Souheyla
>>>>>>>> >>
>>>>>>>> >> [[alternative HTML version deleted]]
>>>>>>>> >>
>>>>>>>> >> _______________________________________________
>>>>>>>> >> R-sig-mixed-models at r-project.org mailing list
>>>>>>>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>>> >>
>>>>>>>> >>
>>>>>>>> >>
>>>>>>>> >> [[alternative HTML version deleted]]
>>>>>>>> >>
>>>>>>>> >> _______________________________________________
>>>>>>>> >> R-sig-mixed-models at r-project.org mailing list
>>>>>>>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>>> >>
>>>>>>>> >>
>>>>>>>> >>
>>>>>>>> >>
>>>>>>>> >> [[alternative HTML version deleted]]
>>>>>>>> >>
>>>>>>>> >> _______________________________________________
>>>>>>>> >> R-sig-mixed-models at r-project.org mailing list
>>>>>>>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>>> >>
>>>>>>>> >>
>>>>>>>>
>>>>>>>>         [[alternative HTML version deleted]]
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>>>
>>>>>>>

	[[alternative HTML version deleted]]


From b|mono@om @end|ng |rom gm@||@com  Tue Mar 26 14:46:53 2019
From: b|mono@om @end|ng |rom gm@||@com (=?UTF-8?B?UmVuw6k=?=)
Date: Tue, 26 Mar 2019 14:46:53 +0100
Subject: [R-sig-ME] Mixed model compositional data
In-Reply-To: <ccd08bcb-dc02-4729-b601-f186631357ca@Spark>
References: <a93bffb0-f845-43bf-a472-f87a69e07ab1@Spark>
 <ccd08bcb-dc02-4729-b601-f186631357ca@Spark>
Message-ID: <CADcpBHMw8g=FScTQQs7s1i=zqy7M3tgjAf-dAN_w1XOveFjQVg@mail.gmail.com>

Hi Luca,

as far as I know the poisson regression you got is the best way to deal
with this. In the end, your raw-data is not compositional (i.e. summing up
to 100%) but it is count data (summing up to N-AVAILABLE), which you COULD
transform to compositional (i.e. to %; introducing a linear combination
between the different response types, which would then require a Dirichlet
regression). So in the end, of course there is a basic
methodological/theoretical implication, such that observing 7 in 10 means
something different then 7 in 20 ... and that is what offset is exactly
for. What I think the model - does not- account for is, that having 100
available observations also provides more precision in the count estimate,
such that you would trust 70 out of 100 more than 7 out of 10 when
"stating" this is 70%. I guess, one could therefore think about adjusting
the "weights" option in the model, vial 'available' too, to alter the
dispersion parameter in the poisson distribution, but I think this is
rather uncommon (at least the GLM help file says so... :)).

But if you feel uncomfortable with this, then you could switch to
Hierarchical Bayesian models and the package 'brms'
For this you would use the second version of your suggested data frame, but
calculate proportions of DV=response/available (thus make it compositional)
beforehand; then the basic model code would be:
brm(DV~Behavior+(1|Factor1)+(1|Factor2), data=...,
family=dirichlet(link="logit"), etc...)
This model code pretty much reads like a mixed model (i.e. random variance
for Factor1 intercepts will be estimated via likelihood), you just have to
figure out how to define the priors :), and which question you want to test
(e.g. parameter estimation, which Bayes Factors etc).

This feature in brm seems to be pretty new (2 days old), and I am not 100%
sure how to deal with this interaction. So I guess, if you want to follow
this option, you might want to post your question in this stan forum:
https://discourse.mc-stan.org

Hope this helps.

Best, Ren?

Am Di., 26. M?rz 2019 um 13:34 Uhr schrieb Boku mail <
luca.corlatti at boku.ac.at>:

> Dear list members,
> Suppose I have a dataset like this:
>
> FACTOR1 FACTOR2 TREATMENT AVAILABLE RESTING WALKING FEEDING DRINKING
> 1 1 A 66 40 12 6 8
> 1 1 B 72 28 22 8 14
> 1 1 C 50 20 10 4 16
> 2 1 A 63 30 12 8 13
> 2 1 B 59 13 27 4 15
> 2 1 C 50 18 20 3 9
> 1 2 A 70 40 20 5 5
> 1 2 B 80 45 15 5 15
> 1 2 C 60 15 15 20 10
> 2 2 A 58 28 20 4 6
> 2 2 B 75 18 22 10 15
> 2 2 C 51 19 21 5 6
> ?
>
> Where I have behavioral data (?RESTING?, ?WALKING?, ?FEEDING?, ?DRINKING?)
> collected using scan sampling (i.e., each row corresponds to one scan,
> where a number ?AVAILABLE? of animals have been observed, and the behavior
> of each animal has been categorized into ?resting?, ?walking? ?feeding? or
> ?drinking?). Additionally, observations are grouped in 2 factors (?FACTOR1?
> and ?FACTOR2?) to be considered as random. I am interested in knowing the
> effect of the variable ?TREATMENT? on each behavioral category.
>
> The main problem is that the behavioral variables represent compositional
> data (i.e. they sum up to the ?AVAILABLE? number of animals; to put it in
> other words, the proportions of animals within each behavioral category
> sums up to 1). The packages dealing with compositional data do not appear
> to handle random factors, thus I am wondering what would be the most
> sensible way to deal with this kind of data.
>
> Would this workaround make sense?
> First, reorganize the multivariate data with an extra lowest level
> indicating the responses:
>
> FACTOR1 FACTOR2 TREATMENT AVAILABLE BEHAVIOR RESPONSE
> 1 1 A 66 RESTING 40
> 1 1 B 72 RESTING 28
> 1 1 C 50 RESTING 20
> 2 1 A 63 RESTING 30
> 2 1 B 59 RESTING 13
> 2 1 C 50 RESTING 20
> 1 2 A 70 RESTING 18
> 1 2 B 80 RESTING 45
> 1 2 C 60 RESTING 15
> 2 2 A 58 RESTING 28
> 2 2 B 75 RESTING 18
> 2 2 C 51 RESTING 19
> ?
>
> Then, fit a model like:
>
> Mod <- glmmTMB(RESPONSE ~ TREATMENT * BEHAVIOR + (1|FACTOR1) + (1|FACTOR2)
> + offset(log(AVAILABLE), family=Poisson, data=my.data)
>
> I am not sure this workaround would deal efficiently with the
> compositional nature of the data.
>
> Thanks a lot in advance!
> Best,
> Luca
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com  Tue Mar 26 15:37:02 2019
From: b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com (Alessandra Bielli)
Date: Tue, 26 Mar 2019 14:37:02 +0000
Subject: [R-sig-ME] 
 Fixing singularity in a generalized linear mixed effect model
In-Reply-To: <CADcpBHMFDhDyjpixq_gZyyKKLrDHU-_JrY1vjj7jYrohjffKsw@mail.gmail.com>
References: <CA+6N3yUq1s=XfHo6-GS26cc_doGLMQzjBqVhCUo63Szg=xUmfg@mail.gmail.com>
 <CAJuCY5z3cYdnrsE6UJctA5ULBBmWoruQgV43AM_opatdibTRXA@mail.gmail.com>
 <CADcpBHMFDhDyjpixq_gZyyKKLrDHU-_JrY1vjj7jYrohjffKsw@mail.gmail.com>
Message-ID: <CA+6N3yXNUdeoaeseiyyy3CCJN-XZQZ2-seqLv8dcSYcgGLFOGg@mail.gmail.com>

Dear Ren? and Thierry

Thank you very much for your answer.
I have to check about sharing the data.

In the mean time, I'll explain a bit more the experiment.
Lance means "set" and is the code given to a fishing set.
In the experiment, within each fishing trip there are multiple fishing sets
(i.e the fishermen soak their nets multiple times, each time on a different
day, usually consecutive days) and each fishing set consists of a pair of
nets: one control net and one experimental net. This is why I believe I
must include "Lance" as a random effect, because it is really important for
the design to have a paired experiment.
So an example from my dataset would be

Trip.Code      Lance.N      Observer.Name  start date      Effort    CE
Turtles.TOT
EC062 159       Alexis Lopez 1/7/2015 0.443103     C 0
EC062 159       Alexis Lopez 1/7/2015 0.398793     E 0
EC062 160       Alexis Lopez 1/8/2015 0.474345     C 0
EC062 160       Alexis Lopez 1/8/2015 0.426911      E 0
What confuses me is that, even if I leave "Lance" as the only random
effect, the model is still singular.
Does this happen because the number of levels for Lance is too high
compared to the number of observations?
Would it be better to have repeated Lance (set number) for each trip, i.e.
trip EC062 set 1,2,3 etc then trip EC063 set 1,2,3 etc...?

Thanks again for your help!

Alessandra


On Tue, Mar 26, 2019 at 9:08 AM Ren? <bimonosom at gmail.com> wrote:

> Hi Allessandra,
>
> your model output says:
> "Number of obs: 292"
> and your model has 2 fixed effects and 5 (!) random effects.
>  - If -  all these random effects are fully crossed. Then assuming you
> have 19 participants (e.g. 1|observers), and 4 random effects crossed on
> them with two levels each (2*2*2*2 = 16 cells), would make about 292
> observations.
> Now you see this math uncovers the most likely problem: A random effect
> (intercept) factor is urgently recommended to have least! 6 levels to make
> such a model meaningful).
> If all these random effects are not fully crossed, then the model is
> misspecified, i.e. defining random intercepts for factor 1 separately from
> random intercepts for another factor 2, when factor 1 is nested in factor
> 2, is over-identifying the randomness in your model -> singular.
>
> So,
> only define random intercepts for factors with more then 6 levels (move
> those factors with less levels to the fixed effects instead to control for
> them)
> only define separate random intercepts for factors that are crossed; for
> instance the factors boat name and lance seem suspicious. I guess, there is
> a world in which a 'lance 1' can only be on boat 'atlantis' to be used for
> fishing, but not on boat 'moby dick'. In this case, having a random
> intercept for "boat name" in addition to 'lance' would not add anything to
> the model, since lances would already cover the variance of boats (lances
> nested in boats).
> Get more observations
> Rerun the model
> Should be fine :)
>
> If singularities still occur use Bayesian models or come back here :))
>
> Best, Ren?
>
>
> Am Di., 26. M?rz 2019 um 09:30 Uhr schrieb Thierry Onkelinx via
> R-sig-mixed-models <r-sig-mixed-models at r-project.org>:
>
>> Dear Alessandra,
>>
>> Your problem is hard to diagnose without the data. Can you make the data
>> available? Does the combination of factors lead to unique observations? Or
>> do some combinations have only zero's?
>>
>> Best regards,
>>
>> ir. Thierry Onkelinx
>> Statisticus / Statistician
>>
>> Vlaamse Overheid / Government of Flanders
>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
>> FOREST
>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>> thierry.onkelinx at inbo.be
>> Havenlaan 88 bus 73, 1000 Brussel
>> www.inbo.be
>>
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to
>> say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of
>> data.
>> ~ John Tukey
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>>
>> <https://www.inbo.be>
>>
>>
>> Op wo 20 mrt. 2019 om 23:19 schreef Alessandra Bielli <
>> bielli.alessandra at gmail.com>:
>>
>> > Dear List
>> >
>> > I am fitting this model using the lme4 package, in order to obtain catch
>> > estimates using the predict function
>> >
>> > m1 <- glmer(Count ~ CE + offset(log(Effort)) + (1|SetYear) +(1|Season) +
>> >                   (1|Lance.N) + (1|Boat.Name) + (1|Observer.Name), data
>> =
>> > Data,                     glmerControl(optimizer = "bobyqa"), family=
>> > "poisson")
>> >
>> >
>> > where: CE is a categorical (control or treatment), Effort is numerical
>> > (fishing effort), and all the other variables are random effects.
>> >
>> > *My problem is that I get a warning message saying that the model is
>> > singular*
>> >
>> > *>summary(m1)*
>> >
>> > Generalized linear mixed model fit by maximum likelihood (Laplace
>> > Approximation) [glmerMod]
>> >  Family: poisson  ( log )
>> > Formula: Count ~ CE + offset(log(Effort)) + (1 | SetYear) + (1 |
>> >     Season) + (1 | Lance.N) + (1 | Boat.Name) + (1 | Observer.Name)
>> >    Data: Data
>> > Control: glmerControl(optimizer = "bobyqa")
>> >
>> >      AIC      BIC   logLik deviance df.resid
>> >    148.6    174.3    -67.3    134.6      285
>> >
>> > Scaled residuals:
>> >     Min      1Q  Median      3Q     Max
>> > -0.4852 -0.1758 -0.1339 -0.1227  3.5980
>> >
>> > Random effects:
>> >  Groups        Name        Variance  Std.Dev.
>> >  Lance.N       (Intercept) 2.259e+00 1.503e+00
>> >  Boat.Name     (Intercept) 0.000e+00 0.000e+00
>> >  Observer.Name (Intercept) 0.000e+00 0.000e+00
>> >  Season        (Intercept) 4.149e-17 6.442e-09
>> >  SetYear       (Intercept) 0.000e+00 0.000e+00
>> > Number of obs: 292, groups:
>> > Lance.N, 146; Boat.Name, 21; Observer.Name, 5; Season, 4; SetYear, 4
>> >
>> > Fixed effects:
>> >             Estimate Std. Error z value Pr(>|z|)
>> > (Intercept)  -2.5751     0.6612  -3.895 9.83e-05 ***
>> > CEE          -0.5878     0.5003  -1.175     0.24
>> > ---
>> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>> >
>> > Correlation of Fixed Effects:
>> >     (Intr)
>> > CEE -0.257
>> > *convergence code: 0*
>> > *singular fit*
>> >
>> > I am aware that there are a lot of random effects and some of them have
>> a
>> > number of levels <5. However, this study was carried out under real
>> fishery
>> > conditions, so these random effects seemed all important to me.
>> >
>> > I removed the random effects with variance zero as suggested here
>> >
>> >
>> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#singular-models-random-effect-variances-estimated-as-zero-or-correlations-estimated-as---1
>> > until I removed them all and found myself with a glm instead.
>> >
>> > My questions are
>> >
>> > - why the variance of Lance.N, initially positive, becomes zero after I
>> > remove the other random effects that had variance equal zero?
>> > - is it acceptable to fit a glm just because all the random effect
>> > variances were zero?
>> >
>> > I hope I gave all the information you need.
>> >
>> > Thanks for any advice!
>> >
>> > Alessandra
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From b|mono@om @end|ng |rom gm@||@com  Tue Mar 26 17:02:58 2019
From: b|mono@om @end|ng |rom gm@||@com (=?UTF-8?B?UmVuw6k=?=)
Date: Tue, 26 Mar 2019 17:02:58 +0100
Subject: [R-sig-ME] 
 Fixing singularity in a generalized linear mixed effect model
In-Reply-To: <CA+6N3yXNUdeoaeseiyyy3CCJN-XZQZ2-seqLv8dcSYcgGLFOGg@mail.gmail.com>
References: <CA+6N3yUq1s=XfHo6-GS26cc_doGLMQzjBqVhCUo63Szg=xUmfg@mail.gmail.com>
 <CAJuCY5z3cYdnrsE6UJctA5ULBBmWoruQgV43AM_opatdibTRXA@mail.gmail.com>
 <CADcpBHMFDhDyjpixq_gZyyKKLrDHU-_JrY1vjj7jYrohjffKsw@mail.gmail.com>
 <CA+6N3yXNUdeoaeseiyyy3CCJN-XZQZ2-seqLv8dcSYcgGLFOGg@mail.gmail.com>
Message-ID: <CADcpBHN-qauo6YS8q=b+We3wVYezMKiOQeU9GBxdWOgPkY_=8g@mail.gmail.com>

Dear Alessandra,

First, I think this is a related post (considering the separation between
residual variance from random effect variance, with few/one random effect
observation per design cell).
https://stackoverflow.com/questions/19713228/lmer-error-grouping-factor-must-be-number-of-observations

Considering your experimental set up:

"each [Lance] consists of a pair of nets: one control net and one
experimental net" (This must be the 'CE' variable)

So this model alone should indeed work:
m <- glmer(Count ~ CE +  (1|Lance.N), data =Data, glmerControl(optimizer =
"bobyqa"), family= "poisson")

but you defined it this way...
m <- glmer(Count ~ CE + offset(log(Effort))+ (1|Lance.N) data =Data,
glmerControl(optimizer = "bobyqa"), family= "poisson")
And there are two odd things:
First, offset seems to be a continuous variable, and now the above
post-link definitely matters, as residual variance falls together with
random effect variance -> not identifiable.
Second, why is the offset in the fixed effects (I am not aware of this kind
of application)?
Usually, I think, the offset in poisson models is defined in the options
(see ?glm), and also the offset usually is the number of overall
observations (from which Count is the number of successes), but effort
seems to be something else.


Best, Ren?




Am Di., 26. M?rz 2019 um 15:37 Uhr schrieb Alessandra Bielli <
bielli.alessandra at gmail.com>:

> Dear Ren? and Thierry
>
> Thank you very much for your answer.
> I have to check about sharing the data.
>
> In the mean time, I'll explain a bit more the experiment.
> Lance means "set" and is the code given to a fishing set.
> In the experiment, within each fishing trip there are multiple fishing
> sets (i.e the fishermen soak their nets multiple times, each time on a
> different day, usually consecutive days) and each fishing set consists of a
> pair of nets: one control net and one experimental net. This is why I
> believe I must include "Lance" as a random effect, because it is really
> important for the design to have a paired experiment.
> So an example from my dataset would be
>
> Trip.Code      Lance.N      Observer.Name  start date      Effort    CE
>   Turtles.TOT
> EC062 159       Alexis Lopez 1/7/2015 0.443103     C 0
> EC062 159       Alexis Lopez 1/7/2015 0.398793     E 0
> EC062 160       Alexis Lopez 1/8/2015 0.474345     C 0
> EC062 160       Alexis Lopez 1/8/2015 0.426911      E 0
> What confuses me is that, even if I leave "Lance" as the only random
> effect, the model is still singular.
> Does this happen because the number of levels for Lance is too high
> compared to the number of observations?
> Would it be better to have repeated Lance (set number) for each trip, i.e.
> trip EC062 set 1,2,3 etc then trip EC063 set 1,2,3 etc...?
>
> Thanks again for your help!
>
> Alessandra
>
>
> On Tue, Mar 26, 2019 at 9:08 AM Ren? <bimonosom at gmail.com> wrote:
>
>> Hi Allessandra,
>>
>> your model output says:
>> "Number of obs: 292"
>> and your model has 2 fixed effects and 5 (!) random effects.
>>  - If -  all these random effects are fully crossed. Then assuming you
>> have 19 participants (e.g. 1|observers), and 4 random effects crossed on
>> them with two levels each (2*2*2*2 = 16 cells), would make about 292
>> observations.
>> Now you see this math uncovers the most likely problem: A random effect
>> (intercept) factor is urgently recommended to have least! 6 levels to make
>> such a model meaningful).
>> If all these random effects are not fully crossed, then the model is
>> misspecified, i.e. defining random intercepts for factor 1 separately from
>> random intercepts for another factor 2, when factor 1 is nested in factor
>> 2, is over-identifying the randomness in your model -> singular.
>>
>> So,
>> only define random intercepts for factors with more then 6 levels (move
>> those factors with less levels to the fixed effects instead to control for
>> them)
>> only define separate random intercepts for factors that are crossed; for
>> instance the factors boat name and lance seem suspicious. I guess, there is
>> a world in which a 'lance 1' can only be on boat 'atlantis' to be used for
>> fishing, but not on boat 'moby dick'. In this case, having a random
>> intercept for "boat name" in addition to 'lance' would not add anything to
>> the model, since lances would already cover the variance of boats (lances
>> nested in boats).
>> Get more observations
>> Rerun the model
>> Should be fine :)
>>
>> If singularities still occur use Bayesian models or come back here :))
>>
>> Best, Ren?
>>
>>
>> Am Di., 26. M?rz 2019 um 09:30 Uhr schrieb Thierry Onkelinx via
>> R-sig-mixed-models <r-sig-mixed-models at r-project.org>:
>>
>>> Dear Alessandra,
>>>
>>> Your problem is hard to diagnose without the data. Can you make the data
>>> available? Does the combination of factors lead to unique observations?
>>> Or
>>> do some combinations have only zero's?
>>>
>>> Best regards,
>>>
>>> ir. Thierry Onkelinx
>>> Statisticus / Statistician
>>>
>>> Vlaamse Overheid / Government of Flanders
>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>>> AND
>>> FOREST
>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>>> thierry.onkelinx at inbo.be
>>> Havenlaan 88 bus 73, 1000 Brussel
>>> www.inbo.be
>>>
>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>> To call in the statistician after the experiment is done may be no more
>>> than asking him to perform a post-mortem examination: he may be able to
>>> say
>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>> The plural of anecdote is not data. ~ Roger Brinner
>>> The combination of some data and an aching desire for an answer does not
>>> ensure that a reasonable answer can be extracted from a given body of
>>> data.
>>> ~ John Tukey
>>>
>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>
>>> <https://www.inbo.be>
>>>
>>>
>>> Op wo 20 mrt. 2019 om 23:19 schreef Alessandra Bielli <
>>> bielli.alessandra at gmail.com>:
>>>
>>> > Dear List
>>> >
>>> > I am fitting this model using the lme4 package, in order to obtain
>>> catch
>>> > estimates using the predict function
>>> >
>>> > m1 <- glmer(Count ~ CE + offset(log(Effort)) + (1|SetYear) +(1|Season)
>>> +
>>> >                   (1|Lance.N) + (1|Boat.Name) + (1|Observer.Name),
>>> data =
>>> > Data,                     glmerControl(optimizer = "bobyqa"), family=
>>> > "poisson")
>>> >
>>> >
>>> > where: CE is a categorical (control or treatment), Effort is numerical
>>> > (fishing effort), and all the other variables are random effects.
>>> >
>>> > *My problem is that I get a warning message saying that the model is
>>> > singular*
>>> >
>>> > *>summary(m1)*
>>> >
>>> > Generalized linear mixed model fit by maximum likelihood (Laplace
>>> > Approximation) [glmerMod]
>>> >  Family: poisson  ( log )
>>> > Formula: Count ~ CE + offset(log(Effort)) + (1 | SetYear) + (1 |
>>> >     Season) + (1 | Lance.N) + (1 | Boat.Name) + (1 | Observer.Name)
>>> >    Data: Data
>>> > Control: glmerControl(optimizer = "bobyqa")
>>> >
>>> >      AIC      BIC   logLik deviance df.resid
>>> >    148.6    174.3    -67.3    134.6      285
>>> >
>>> > Scaled residuals:
>>> >     Min      1Q  Median      3Q     Max
>>> > -0.4852 -0.1758 -0.1339 -0.1227  3.5980
>>> >
>>> > Random effects:
>>> >  Groups        Name        Variance  Std.Dev.
>>> >  Lance.N       (Intercept) 2.259e+00 1.503e+00
>>> >  Boat.Name     (Intercept) 0.000e+00 0.000e+00
>>> >  Observer.Name (Intercept) 0.000e+00 0.000e+00
>>> >  Season        (Intercept) 4.149e-17 6.442e-09
>>> >  SetYear       (Intercept) 0.000e+00 0.000e+00
>>> > Number of obs: 292, groups:
>>> > Lance.N, 146; Boat.Name, 21; Observer.Name, 5; Season, 4; SetYear, 4
>>> >
>>> > Fixed effects:
>>> >             Estimate Std. Error z value Pr(>|z|)
>>> > (Intercept)  -2.5751     0.6612  -3.895 9.83e-05 ***
>>> > CEE          -0.5878     0.5003  -1.175     0.24
>>> > ---
>>> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>> >
>>> > Correlation of Fixed Effects:
>>> >     (Intr)
>>> > CEE -0.257
>>> > *convergence code: 0*
>>> > *singular fit*
>>> >
>>> > I am aware that there are a lot of random effects and some of them
>>> have a
>>> > number of levels <5. However, this study was carried out under real
>>> fishery
>>> > conditions, so these random effects seemed all important to me.
>>> >
>>> > I removed the random effects with variance zero as suggested here
>>> >
>>> >
>>> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#singular-models-random-effect-variances-estimated-as-zero-or-correlations-estimated-as---1
>>> > until I removed them all and found myself with a glm instead.
>>> >
>>> > My questions are
>>> >
>>> > - why the variance of Lance.N, initially positive, becomes zero after I
>>> > remove the other random effects that had variance equal zero?
>>> > - is it acceptable to fit a glm just because all the random effect
>>> > variances were zero?
>>> >
>>> > I hope I gave all the information you need.
>>> >
>>> > Thanks for any advice!
>>> >
>>> > Alessandra
>>> >
>>> >         [[alternative HTML version deleted]]
>>> >
>>> > _______________________________________________
>>> > R-sig-mixed-models at r-project.org mailing list
>>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>> >
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>

	[[alternative HTML version deleted]]


From b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com  Tue Mar 26 19:55:20 2019
From: b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com (Alessandra Bielli)
Date: Tue, 26 Mar 2019 18:55:20 +0000
Subject: [R-sig-ME] 
 Fixing singularity in a generalized linear mixed effect model
In-Reply-To: <CADcpBHN-qauo6YS8q=b+We3wVYezMKiOQeU9GBxdWOgPkY_=8g@mail.gmail.com>
References: <CA+6N3yUq1s=XfHo6-GS26cc_doGLMQzjBqVhCUo63Szg=xUmfg@mail.gmail.com>
 <CAJuCY5z3cYdnrsE6UJctA5ULBBmWoruQgV43AM_opatdibTRXA@mail.gmail.com>
 <CADcpBHMFDhDyjpixq_gZyyKKLrDHU-_JrY1vjj7jYrohjffKsw@mail.gmail.com>
 <CA+6N3yXNUdeoaeseiyyy3CCJN-XZQZ2-seqLv8dcSYcgGLFOGg@mail.gmail.com>
 <CADcpBHN-qauo6YS8q=b+We3wVYezMKiOQeU9GBxdWOgPkY_=8g@mail.gmail.com>
Message-ID: <CA+6N3yUVb=NJqXBr4nrjam9OPyfzeOKMGsu_NHRF5F=f60R_nA@mail.gmail.com>

Hi Ren?

To anwer your questions
1. offset is a continuous variable because it's fishing effort, measured in
km of net x day. The experiment has a paired design but within the same set
it could happen that the control net is longer than the experimental net.
So count is the number of individuals captured in the net that will be
later standardised using effort.
2. the offset is in fixed effects because we use the model to predict catch
per unit effort (CPUE) and the predict function will return a CPUE (and not
a count) if the offset is included in the formula:

pred.data <- with(Data, #your data
                  expand.grid(
                    CE=c("C","E"),
                    Effort= 1,
                    Count=0))

mm <- model.matrix(terms(m1),pred.data)

pred.data$Count <- predict(m1,pred.data,re.form=NA,type="response")

I have just tried removing the offset and running glmer(Count ~
CE +  (1|Lance.N) but the model is still singular, with Lance variance = 0
I must be missing something!

Alessandra



On Tue, Mar 26, 2019 at 4:03 PM Ren? <bimonosom at gmail.com> wrote:

> Dear Alessandra,
>
> First, I think this is a related post (considering the separation between
> residual variance from random effect variance, with few/one random effect
> observation per design cell).
>
> https://stackoverflow.com/questions/19713228/lmer-error-grouping-factor-must-be-number-of-observations
>
> Considering your experimental set up:
>
> "each [Lance] consists of a pair of nets: one control net and one
> experimental net" (This must be the 'CE' variable)
>
> So this model alone should indeed work:
> m <- glmer(Count ~ CE +  (1|Lance.N), data =Data, glmerControl(optimizer =
> "bobyqa"), family= "poisson")
>
> but you defined it this way...
> m <- glmer(Count ~ CE + offset(log(Effort))+ (1|Lance.N) data =Data,
> glmerControl(optimizer = "bobyqa"), family= "poisson")
> And there are two odd things:
> First, offset seems to be a continuous variable, and now the above
> post-link definitely matters, as residual variance falls together with
> random effect variance -> not identifiable.
> Second, why is the offset in the fixed effects (I am not aware of this
> kind of application)?
> Usually, I think, the offset in poisson models is defined in the options
> (see ?glm), and also the offset usually is the number of overall
> observations (from which Count is the number of successes), but effort
> seems to be something else.
>
>
> Best, Ren?
>
>
>
>
> Am Di., 26. M?rz 2019 um 15:37 Uhr schrieb Alessandra Bielli <
> bielli.alessandra at gmail.com>:
>
>> Dear Ren? and Thierry
>>
>> Thank you very much for your answer.
>> I have to check about sharing the data.
>>
>> In the mean time, I'll explain a bit more the experiment.
>> Lance means "set" and is the code given to a fishing set.
>> In the experiment, within each fishing trip there are multiple fishing
>> sets (i.e the fishermen soak their nets multiple times, each time on a
>> different day, usually consecutive days) and each fishing set consists of a
>> pair of nets: one control net and one experimental net. This is why I
>> believe I must include "Lance" as a random effect, because it is really
>> important for the design to have a paired experiment.
>> So an example from my dataset would be
>>
>> Trip.Code      Lance.N      Observer.Name  start date      Effort    CE
>>   Turtles.TOT
>> EC062 159       Alexis Lopez 1/7/2015 0.443103     C 0
>> EC062 159       Alexis Lopez 1/7/2015 0.398793     E 0
>> EC062 160       Alexis Lopez 1/8/2015 0.474345     C 0
>> EC062 160       Alexis Lopez 1/8/2015 0.426911      E 0
>> What confuses me is that, even if I leave "Lance" as the only random
>> effect, the model is still singular.
>> Does this happen because the number of levels for Lance is too high
>> compared to the number of observations?
>> Would it be better to have repeated Lance (set number) for each trip,
>> i.e. trip EC062 set 1,2,3 etc then trip EC063 set 1,2,3 etc...?
>>
>> Thanks again for your help!
>>
>> Alessandra
>>
>>
>> On Tue, Mar 26, 2019 at 9:08 AM Ren? <bimonosom at gmail.com> wrote:
>>
>>> Hi Allessandra,
>>>
>>> your model output says:
>>> "Number of obs: 292"
>>> and your model has 2 fixed effects and 5 (!) random effects.
>>>  - If -  all these random effects are fully crossed. Then assuming you
>>> have 19 participants (e.g. 1|observers), and 4 random effects crossed on
>>> them with two levels each (2*2*2*2 = 16 cells), would make about 292
>>> observations.
>>> Now you see this math uncovers the most likely problem: A random effect
>>> (intercept) factor is urgently recommended to have least! 6 levels to make
>>> such a model meaningful).
>>> If all these random effects are not fully crossed, then the model is
>>> misspecified, i.e. defining random intercepts for factor 1 separately from
>>> random intercepts for another factor 2, when factor 1 is nested in factor
>>> 2, is over-identifying the randomness in your model -> singular.
>>>
>>> So,
>>> only define random intercepts for factors with more then 6 levels (move
>>> those factors with less levels to the fixed effects instead to control for
>>> them)
>>> only define separate random intercepts for factors that are crossed; for
>>> instance the factors boat name and lance seem suspicious. I guess, there is
>>> a world in which a 'lance 1' can only be on boat 'atlantis' to be used for
>>> fishing, but not on boat 'moby dick'. In this case, having a random
>>> intercept for "boat name" in addition to 'lance' would not add anything to
>>> the model, since lances would already cover the variance of boats (lances
>>> nested in boats).
>>> Get more observations
>>> Rerun the model
>>> Should be fine :)
>>>
>>> If singularities still occur use Bayesian models or come back here :))
>>>
>>> Best, Ren?
>>>
>>>
>>> Am Di., 26. M?rz 2019 um 09:30 Uhr schrieb Thierry Onkelinx via
>>> R-sig-mixed-models <r-sig-mixed-models at r-project.org>:
>>>
>>>> Dear Alessandra,
>>>>
>>>> Your problem is hard to diagnose without the data. Can you make the data
>>>> available? Does the combination of factors lead to unique observations?
>>>> Or
>>>> do some combinations have only zero's?
>>>>
>>>> Best regards,
>>>>
>>>> ir. Thierry Onkelinx
>>>> Statisticus / Statistician
>>>>
>>>> Vlaamse Overheid / Government of Flanders
>>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>>>> AND
>>>> FOREST
>>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>>>> thierry.onkelinx at inbo.be
>>>> Havenlaan 88 bus 73, 1000 Brussel
>>>> www.inbo.be
>>>>
>>>>
>>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>> To call in the statistician after the experiment is done may be no more
>>>> than asking him to perform a post-mortem examination: he may be able to
>>>> say
>>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>>> The plural of anecdote is not data. ~ Roger Brinner
>>>> The combination of some data and an aching desire for an answer does not
>>>> ensure that a reasonable answer can be extracted from a given body of
>>>> data.
>>>> ~ John Tukey
>>>>
>>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>
>>>> <https://www.inbo.be>
>>>>
>>>>
>>>> Op wo 20 mrt. 2019 om 23:19 schreef Alessandra Bielli <
>>>> bielli.alessandra at gmail.com>:
>>>>
>>>> > Dear List
>>>> >
>>>> > I am fitting this model using the lme4 package, in order to obtain
>>>> catch
>>>> > estimates using the predict function
>>>> >
>>>> > m1 <- glmer(Count ~ CE + offset(log(Effort)) + (1|SetYear)
>>>> +(1|Season) +
>>>> >                   (1|Lance.N) + (1|Boat.Name) + (1|Observer.Name),
>>>> data =
>>>> > Data,                     glmerControl(optimizer = "bobyqa"), family=
>>>> > "poisson")
>>>> >
>>>> >
>>>> > where: CE is a categorical (control or treatment), Effort is numerical
>>>> > (fishing effort), and all the other variables are random effects.
>>>> >
>>>> > *My problem is that I get a warning message saying that the model is
>>>> > singular*
>>>> >
>>>> > *>summary(m1)*
>>>> >
>>>> > Generalized linear mixed model fit by maximum likelihood (Laplace
>>>> > Approximation) [glmerMod]
>>>> >  Family: poisson  ( log )
>>>> > Formula: Count ~ CE + offset(log(Effort)) + (1 | SetYear) + (1 |
>>>> >     Season) + (1 | Lance.N) + (1 | Boat.Name) + (1 | Observer.Name)
>>>> >    Data: Data
>>>> > Control: glmerControl(optimizer = "bobyqa")
>>>> >
>>>> >      AIC      BIC   logLik deviance df.resid
>>>> >    148.6    174.3    -67.3    134.6      285
>>>> >
>>>> > Scaled residuals:
>>>> >     Min      1Q  Median      3Q     Max
>>>> > -0.4852 -0.1758 -0.1339 -0.1227  3.5980
>>>> >
>>>> > Random effects:
>>>> >  Groups        Name        Variance  Std.Dev.
>>>> >  Lance.N       (Intercept) 2.259e+00 1.503e+00
>>>> >  Boat.Name     (Intercept) 0.000e+00 0.000e+00
>>>> >  Observer.Name (Intercept) 0.000e+00 0.000e+00
>>>> >  Season        (Intercept) 4.149e-17 6.442e-09
>>>> >  SetYear       (Intercept) 0.000e+00 0.000e+00
>>>> > Number of obs: 292, groups:
>>>> > Lance.N, 146; Boat.Name, 21; Observer.Name, 5; Season, 4; SetYear, 4
>>>> >
>>>> > Fixed effects:
>>>> >             Estimate Std. Error z value Pr(>|z|)
>>>> > (Intercept)  -2.5751     0.6612  -3.895 9.83e-05 ***
>>>> > CEE          -0.5878     0.5003  -1.175     0.24
>>>> > ---
>>>> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>> >
>>>> > Correlation of Fixed Effects:
>>>> >     (Intr)
>>>> > CEE -0.257
>>>> > *convergence code: 0*
>>>> > *singular fit*
>>>> >
>>>> > I am aware that there are a lot of random effects and some of them
>>>> have a
>>>> > number of levels <5. However, this study was carried out under real
>>>> fishery
>>>> > conditions, so these random effects seemed all important to me.
>>>> >
>>>> > I removed the random effects with variance zero as suggested here
>>>> >
>>>> >
>>>> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#singular-models-random-effect-variances-estimated-as-zero-or-correlations-estimated-as---1
>>>> > until I removed them all and found myself with a glm instead.
>>>> >
>>>> > My questions are
>>>> >
>>>> > - why the variance of Lance.N, initially positive, becomes zero after
>>>> I
>>>> > remove the other random effects that had variance equal zero?
>>>> > - is it acceptable to fit a glm just because all the random effect
>>>> > variances were zero?
>>>> >
>>>> > I hope I gave all the information you need.
>>>> >
>>>> > Thanks for any advice!
>>>> >
>>>> > Alessandra
>>>> >
>>>> >         [[alternative HTML version deleted]]
>>>> >
>>>> > _______________________________________________
>>>> > R-sig-mixed-models at r-project.org mailing list
>>>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>> >
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Tue Mar 26 21:23:00 2019
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Tue, 26 Mar 2019 16:23:00 -0400
Subject: [R-sig-ME] 
 Fixing singularity in a generalized linear mixed effect model
In-Reply-To: <CA+6N3yUVb=NJqXBr4nrjam9OPyfzeOKMGsu_NHRF5F=f60R_nA@mail.gmail.com>
References: <CA+6N3yUq1s=XfHo6-GS26cc_doGLMQzjBqVhCUo63Szg=xUmfg@mail.gmail.com>
 <CAJuCY5z3cYdnrsE6UJctA5ULBBmWoruQgV43AM_opatdibTRXA@mail.gmail.com>
 <CADcpBHMFDhDyjpixq_gZyyKKLrDHU-_JrY1vjj7jYrohjffKsw@mail.gmail.com>
 <CA+6N3yXNUdeoaeseiyyy3CCJN-XZQZ2-seqLv8dcSYcgGLFOGg@mail.gmail.com>
 <CADcpBHN-qauo6YS8q=b+We3wVYezMKiOQeU9GBxdWOgPkY_=8g@mail.gmail.com>
 <CA+6N3yUVb=NJqXBr4nrjam9OPyfzeOKMGsu_NHRF5F=f60R_nA@mail.gmail.com>
Message-ID: <f9b60f97-2417-a059-423f-923fca1f8cf5@gmail.com>

  A quick comment to Ren?: specifying offset as part of the
fixed-effects formula, rather than as a separate argument, is fairly
standard (see the "offset" description in ?glm). As a developer I
actually slightly prefer it -- it makes it a little easier to keep track
of the offset information.

  cheers
    Ben Bolker


On 2019-03-26 2:55 p.m., Alessandra Bielli wrote:
> Hi Ren?
> 
> To anwer your questions
> 1. offset is a continuous variable because it's fishing effort, measured in
> km of net x day. The experiment has a paired design but within the same set
> it could happen that the control net is longer than the experimental net.
> So count is the number of individuals captured in the net that will be
> later standardised using effort.
> 2. the offset is in fixed effects because we use the model to predict catch
> per unit effort (CPUE) and the predict function will return a CPUE (and not
> a count) if the offset is included in the formula:
> 
> pred.data <- with(Data, #your data
>                   expand.grid(
>                     CE=c("C","E"),
>                     Effort= 1,
>                     Count=0))
> 
> mm <- model.matrix(terms(m1),pred.data)
> 
> pred.data$Count <- predict(m1,pred.data,re.form=NA,type="response")
> 
> I have just tried removing the offset and running glmer(Count ~
> CE +  (1|Lance.N) but the model is still singular, with Lance variance = 0
> I must be missing something!
> 
> Alessandra
> 
> 
> 
> On Tue, Mar 26, 2019 at 4:03 PM Ren? <bimonosom at gmail.com> wrote:
> 
>> Dear Alessandra,
>>
>> First, I think this is a related post (considering the separation between
>> residual variance from random effect variance, with few/one random effect
>> observation per design cell).
>>
>> https://stackoverflow.com/questions/19713228/lmer-error-grouping-factor-must-be-number-of-observations
>>
>> Considering your experimental set up:
>>
>> "each [Lance] consists of a pair of nets: one control net and one
>> experimental net" (This must be the 'CE' variable)
>>
>> So this model alone should indeed work:
>> m <- glmer(Count ~ CE +  (1|Lance.N), data =Data, glmerControl(optimizer =
>> "bobyqa"), family= "poisson")
>>
>> but you defined it this way...
>> m <- glmer(Count ~ CE + offset(log(Effort))+ (1|Lance.N) data =Data,
>> glmerControl(optimizer = "bobyqa"), family= "poisson")
>> And there are two odd things:
>> First, offset seems to be a continuous variable, and now the above
>> post-link definitely matters, as residual variance falls together with
>> random effect variance -> not identifiable.
>> Second, why is the offset in the fixed effects (I am not aware of this
>> kind of application)?
>> Usually, I think, the offset in poisson models is defined in the options
>> (see ?glm), and also the offset usually is the number of overall
>> observations (from which Count is the number of successes), but effort
>> seems to be something else.
>>
>>
>> Best, Ren?
>>
>>
>>
>>
>> Am Di., 26. M?rz 2019 um 15:37 Uhr schrieb Alessandra Bielli <
>> bielli.alessandra at gmail.com>:
>>
>>> Dear Ren? and Thierry
>>>
>>> Thank you very much for your answer.
>>> I have to check about sharing the data.
>>>
>>> In the mean time, I'll explain a bit more the experiment.
>>> Lance means "set" and is the code given to a fishing set.
>>> In the experiment, within each fishing trip there are multiple fishing
>>> sets (i.e the fishermen soak their nets multiple times, each time on a
>>> different day, usually consecutive days) and each fishing set consists of a
>>> pair of nets: one control net and one experimental net. This is why I
>>> believe I must include "Lance" as a random effect, because it is really
>>> important for the design to have a paired experiment.
>>> So an example from my dataset would be
>>>
>>> Trip.Code      Lance.N      Observer.Name  start date      Effort    CE
>>>   Turtles.TOT
>>> EC062 159       Alexis Lopez 1/7/2015 0.443103     C 0
>>> EC062 159       Alexis Lopez 1/7/2015 0.398793     E 0
>>> EC062 160       Alexis Lopez 1/8/2015 0.474345     C 0
>>> EC062 160       Alexis Lopez 1/8/2015 0.426911      E 0
>>> What confuses me is that, even if I leave "Lance" as the only random
>>> effect, the model is still singular.
>>> Does this happen because the number of levels for Lance is too high
>>> compared to the number of observations?
>>> Would it be better to have repeated Lance (set number) for each trip,
>>> i.e. trip EC062 set 1,2,3 etc then trip EC063 set 1,2,3 etc...?
>>>
>>> Thanks again for your help!
>>>
>>> Alessandra
>>>
>>>
>>> On Tue, Mar 26, 2019 at 9:08 AM Ren? <bimonosom at gmail.com> wrote:
>>>
>>>> Hi Allessandra,
>>>>
>>>> your model output says:
>>>> "Number of obs: 292"
>>>> and your model has 2 fixed effects and 5 (!) random effects.
>>>>  - If -  all these random effects are fully crossed. Then assuming you
>>>> have 19 participants (e.g. 1|observers), and 4 random effects crossed on
>>>> them with two levels each (2*2*2*2 = 16 cells), would make about 292
>>>> observations.
>>>> Now you see this math uncovers the most likely problem: A random effect
>>>> (intercept) factor is urgently recommended to have least! 6 levels to make
>>>> such a model meaningful).
>>>> If all these random effects are not fully crossed, then the model is
>>>> misspecified, i.e. defining random intercepts for factor 1 separately from
>>>> random intercepts for another factor 2, when factor 1 is nested in factor
>>>> 2, is over-identifying the randomness in your model -> singular.
>>>>
>>>> So,
>>>> only define random intercepts for factors with more then 6 levels (move
>>>> those factors with less levels to the fixed effects instead to control for
>>>> them)
>>>> only define separate random intercepts for factors that are crossed; for
>>>> instance the factors boat name and lance seem suspicious. I guess, there is
>>>> a world in which a 'lance 1' can only be on boat 'atlantis' to be used for
>>>> fishing, but not on boat 'moby dick'. In this case, having a random
>>>> intercept for "boat name" in addition to 'lance' would not add anything to
>>>> the model, since lances would already cover the variance of boats (lances
>>>> nested in boats).
>>>> Get more observations
>>>> Rerun the model
>>>> Should be fine :)
>>>>
>>>> If singularities still occur use Bayesian models or come back here :))
>>>>
>>>> Best, Ren?
>>>>
>>>>
>>>> Am Di., 26. M?rz 2019 um 09:30 Uhr schrieb Thierry Onkelinx via
>>>> R-sig-mixed-models <r-sig-mixed-models at r-project.org>:
>>>>
>>>>> Dear Alessandra,
>>>>>
>>>>> Your problem is hard to diagnose without the data. Can you make the data
>>>>> available? Does the combination of factors lead to unique observations?
>>>>> Or
>>>>> do some combinations have only zero's?
>>>>>
>>>>> Best regards,
>>>>>
>>>>> ir. Thierry Onkelinx
>>>>> Statisticus / Statistician
>>>>>
>>>>> Vlaamse Overheid / Government of Flanders
>>>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>>>>> AND
>>>>> FOREST
>>>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>>>>> thierry.onkelinx at inbo.be
>>>>> Havenlaan 88 bus 73, 1000 Brussel
>>>>> www.inbo.be
>>>>>
>>>>>
>>>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>> To call in the statistician after the experiment is done may be no more
>>>>> than asking him to perform a post-mortem examination: he may be able to
>>>>> say
>>>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>>>> The plural of anecdote is not data. ~ Roger Brinner
>>>>> The combination of some data and an aching desire for an answer does not
>>>>> ensure that a reasonable answer can be extracted from a given body of
>>>>> data.
>>>>> ~ John Tukey
>>>>>
>>>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>>
>>>>> <https://www.inbo.be>
>>>>>
>>>>>
>>>>> Op wo 20 mrt. 2019 om 23:19 schreef Alessandra Bielli <
>>>>> bielli.alessandra at gmail.com>:
>>>>>
>>>>>> Dear List
>>>>>>
>>>>>> I am fitting this model using the lme4 package, in order to obtain
>>>>> catch
>>>>>> estimates using the predict function
>>>>>>
>>>>>> m1 <- glmer(Count ~ CE + offset(log(Effort)) + (1|SetYear)
>>>>> +(1|Season) +
>>>>>>                   (1|Lance.N) + (1|Boat.Name) + (1|Observer.Name),
>>>>> data =
>>>>>> Data,                     glmerControl(optimizer = "bobyqa"), family=
>>>>>> "poisson")
>>>>>>
>>>>>>
>>>>>> where: CE is a categorical (control or treatment), Effort is numerical
>>>>>> (fishing effort), and all the other variables are random effects.
>>>>>>
>>>>>> *My problem is that I get a warning message saying that the model is
>>>>>> singular*
>>>>>>
>>>>>> *>summary(m1)*
>>>>>>
>>>>>> Generalized linear mixed model fit by maximum likelihood (Laplace
>>>>>> Approximation) [glmerMod]
>>>>>>  Family: poisson  ( log )
>>>>>> Formula: Count ~ CE + offset(log(Effort)) + (1 | SetYear) + (1 |
>>>>>>     Season) + (1 | Lance.N) + (1 | Boat.Name) + (1 | Observer.Name)
>>>>>>    Data: Data
>>>>>> Control: glmerControl(optimizer = "bobyqa")
>>>>>>
>>>>>>      AIC      BIC   logLik deviance df.resid
>>>>>>    148.6    174.3    -67.3    134.6      285
>>>>>>
>>>>>> Scaled residuals:
>>>>>>     Min      1Q  Median      3Q     Max
>>>>>> -0.4852 -0.1758 -0.1339 -0.1227  3.5980
>>>>>>
>>>>>> Random effects:
>>>>>>  Groups        Name        Variance  Std.Dev.
>>>>>>  Lance.N       (Intercept) 2.259e+00 1.503e+00
>>>>>>  Boat.Name     (Intercept) 0.000e+00 0.000e+00
>>>>>>  Observer.Name (Intercept) 0.000e+00 0.000e+00
>>>>>>  Season        (Intercept) 4.149e-17 6.442e-09
>>>>>>  SetYear       (Intercept) 0.000e+00 0.000e+00
>>>>>> Number of obs: 292, groups:
>>>>>> Lance.N, 146; Boat.Name, 21; Observer.Name, 5; Season, 4; SetYear, 4
>>>>>>
>>>>>> Fixed effects:
>>>>>>             Estimate Std. Error z value Pr(>|z|)
>>>>>> (Intercept)  -2.5751     0.6612  -3.895 9.83e-05 ***
>>>>>> CEE          -0.5878     0.5003  -1.175     0.24
>>>>>> ---
>>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>>>>
>>>>>> Correlation of Fixed Effects:
>>>>>>     (Intr)
>>>>>> CEE -0.257
>>>>>> *convergence code: 0*
>>>>>> *singular fit*
>>>>>>
>>>>>> I am aware that there are a lot of random effects and some of them
>>>>> have a
>>>>>> number of levels <5. However, this study was carried out under real
>>>>> fishery
>>>>>> conditions, so these random effects seemed all important to me.
>>>>>>
>>>>>> I removed the random effects with variance zero as suggested here
>>>>>>
>>>>>>
>>>>> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#singular-models-random-effect-variances-estimated-as-zero-or-correlations-estimated-as---1
>>>>>> until I removed them all and found myself with a glm instead.
>>>>>>
>>>>>> My questions are
>>>>>>
>>>>>> - why the variance of Lance.N, initially positive, becomes zero after
>>>>> I
>>>>>> remove the other random effects that had variance equal zero?
>>>>>> - is it acceptable to fit a glm just because all the random effect
>>>>>> variances were zero?
>>>>>>
>>>>>> I hope I gave all the information you need.
>>>>>>
>>>>>> Thanks for any advice!
>>>>>>
>>>>>> Alessandra
>>>>>>
>>>>>>         [[alternative HTML version deleted]]
>>>>>>
>>>>>> _______________________________________________
>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>
>>>>>
>>>>>         [[alternative HTML version deleted]]
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>
>>>>
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From b|mono@om @end|ng |rom gm@||@com  Wed Mar 27 10:12:25 2019
From: b|mono@om @end|ng |rom gm@||@com (=?UTF-8?B?UmVuw6k=?=)
Date: Wed, 27 Mar 2019 10:12:25 +0100
Subject: [R-sig-ME] 
 Fixing singularity in a generalized linear mixed effect model
In-Reply-To: <f9b60f97-2417-a059-423f-923fca1f8cf5@gmail.com>
References: <CA+6N3yUq1s=XfHo6-GS26cc_doGLMQzjBqVhCUo63Szg=xUmfg@mail.gmail.com>
 <CAJuCY5z3cYdnrsE6UJctA5ULBBmWoruQgV43AM_opatdibTRXA@mail.gmail.com>
 <CADcpBHMFDhDyjpixq_gZyyKKLrDHU-_JrY1vjj7jYrohjffKsw@mail.gmail.com>
 <CA+6N3yXNUdeoaeseiyyy3CCJN-XZQZ2-seqLv8dcSYcgGLFOGg@mail.gmail.com>
 <CADcpBHN-qauo6YS8q=b+We3wVYezMKiOQeU9GBxdWOgPkY_=8g@mail.gmail.com>
 <CA+6N3yUVb=NJqXBr4nrjam9OPyfzeOKMGsu_NHRF5F=f60R_nA@mail.gmail.com>
 <f9b60f97-2417-a059-423f-923fca1f8cf5@gmail.com>
Message-ID: <CADcpBHORuKd8z=+xi-Q0mO8eZrke-i+UQXuoecU4Cydi3wsAFQ@mail.gmail.com>

@Ben.
Thank you for the info. For me (non-developer) it seems just odd-looking,
but also, I am not the one to judge... :)

@Alessandra.
 Alright. If this is the case, and if there is nothing "wrong" with the
data/coding... then the reason for the singular fit, indeed, seems to be
the Lance zero-(or too small)-variance intercepts.
>From reading some blog- or Q&A posts about singular fits, this seems not an
unfrequent reason, even in cases in which you naturally want to have this
random intercept. I am not really aware how a 'sanity-check' about the
zero-variance Intercept for Lance would look like in the data. But I got
similar error messages from mixed models for random slopes, then ran an
"identically" defined Hierarchical Bayes model, and everything was alright
(no indications of non-convergence or non-identification), but
interestingly with a lot narrower Bayesian CIs for the effects (if one
wants to compare frequentist confidence intervals with Bayesian credible
intervals).

So if a random effect has zero variance, the usual suggestion would be
'drop it'. But I see, that you want it in there for valid reasons.
So I would suggest trying the Bayesian way, installing the 'brms' package,
and then starting with the simplest version of your model to get a feeling
for it:
library(brms)
m1<-brm(Count ~ CE +  (1|Lance.N), data=data, family=poisson(),
prior=c(prior(student_t(1, 0, 1), class = b)), chains=3, cores=3,
iter=2000, save_all_pars = TRUE, save_model = TRUE, inits=0)
library(emmeans)
## the estimated marginal means for each level of CE:
emmeans(m1, ~CE)
## and the difference between the parameter estimates
pairs(emmeans(m1, ~CE))
### - if the HDIs do not include zero ... people tend to like the idea,
that this can be interpreted like a "classic" significant effect :))

You can also get Bayes Factors (Evidence for or against the NULL, or other
hypotheses) from this whole output, but this requires a sound (and exact)
prior definition, which is not really easy for more complex regression
models. For instance,
c(prior(student_t(1, 0, 1), class = b)) above gives every "b" parameter
(fixed effects) this prior. If you want to later have BF's for hypothesis
like : "=0" you need to define c(prior(student_t(1, 0, 1), class = b,
coef="parametertotest"))

brms has a pretty good documentation showing how to get there. There you
will also get an estimate for the Lance.N variance term. You should check
this...
pm1<-posterior_samples(m1)
hist(pm1$sd_Lance.N__Intercept)

If this looks like zero too, then the glm output might be "right" :))
But you can also have a Bayes Factor for this assumption, using
hypothesis(), which is documented in its manual.



Best, Ren?





Am Di., 26. M?rz 2019 um 21:23 Uhr schrieb Ben Bolker <bbolker at gmail.com>:

>   A quick comment to Ren?: specifying offset as part of the
> fixed-effects formula, rather than as a separate argument, is fairly
> standard (see the "offset" description in ?glm). As a developer I
> actually slightly prefer it -- it makes it a little easier to keep track
> of the offset information.
>
>   cheers
>     Ben Bolker
>
>
> On 2019-03-26 2:55 p.m., Alessandra Bielli wrote:
> > Hi Ren?
> >
> > To anwer your questions
> > 1. offset is a continuous variable because it's fishing effort, measured
> in
> > km of net x day. The experiment has a paired design but within the same
> set
> > it could happen that the control net is longer than the experimental net.
> > So count is the number of individuals captured in the net that will be
> > later standardised using effort.
> > 2. the offset is in fixed effects because we use the model to predict
> catch
> > per unit effort (CPUE) and the predict function will return a CPUE (and
> not
> > a count) if the offset is included in the formula:
> >
> > pred.data <- with(Data, #your data
> >                   expand.grid(
> >                     CE=c("C","E"),
> >                     Effort= 1,
> >                     Count=0))
> >
> > mm <- model.matrix(terms(m1),pred.data)
> >
> > pred.data$Count <- predict(m1,pred.data,re.form=NA,type="response")
> >
> > I have just tried removing the offset and running glmer(Count ~
> > CE +  (1|Lance.N) but the model is still singular, with Lance variance =
> 0
> > I must be missing something!
> >
> > Alessandra
> >
> >
> >
> > On Tue, Mar 26, 2019 at 4:03 PM Ren? <bimonosom at gmail.com> wrote:
> >
> >> Dear Alessandra,
> >>
> >> First, I think this is a related post (considering the separation
> between
> >> residual variance from random effect variance, with few/one random
> effect
> >> observation per design cell).
> >>
> >>
> https://stackoverflow.com/questions/19713228/lmer-error-grouping-factor-must-be-number-of-observations
> >>
> >> Considering your experimental set up:
> >>
> >> "each [Lance] consists of a pair of nets: one control net and one
> >> experimental net" (This must be the 'CE' variable)
> >>
> >> So this model alone should indeed work:
> >> m <- glmer(Count ~ CE +  (1|Lance.N), data =Data,
> glmerControl(optimizer =
> >> "bobyqa"), family= "poisson")
> >>
> >> but you defined it this way...
> >> m <- glmer(Count ~ CE + offset(log(Effort))+ (1|Lance.N) data =Data,
> >> glmerControl(optimizer = "bobyqa"), family= "poisson")
> >> And there are two odd things:
> >> First, offset seems to be a continuous variable, and now the above
> >> post-link definitely matters, as residual variance falls together with
> >> random effect variance -> not identifiable.
> >> Second, why is the offset in the fixed effects (I am not aware of this
> >> kind of application)?
> >> Usually, I think, the offset in poisson models is defined in the options
> >> (see ?glm), and also the offset usually is the number of overall
> >> observations (from which Count is the number of successes), but effort
> >> seems to be something else.
> >>
> >>
> >> Best, Ren?
> >>
> >>
> >>
> >>
> >> Am Di., 26. M?rz 2019 um 15:37 Uhr schrieb Alessandra Bielli <
> >> bielli.alessandra at gmail.com>:
> >>
> >>> Dear Ren? and Thierry
> >>>
> >>> Thank you very much for your answer.
> >>> I have to check about sharing the data.
> >>>
> >>> In the mean time, I'll explain a bit more the experiment.
> >>> Lance means "set" and is the code given to a fishing set.
> >>> In the experiment, within each fishing trip there are multiple fishing
> >>> sets (i.e the fishermen soak their nets multiple times, each time on a
> >>> different day, usually consecutive days) and each fishing set consists
> of a
> >>> pair of nets: one control net and one experimental net. This is why I
> >>> believe I must include "Lance" as a random effect, because it is really
> >>> important for the design to have a paired experiment.
> >>> So an example from my dataset would be
> >>>
> >>> Trip.Code      Lance.N      Observer.Name  start date      Effort    CE
> >>>   Turtles.TOT
> >>> EC062 159       Alexis Lopez 1/7/2015 0.443103     C 0
> >>> EC062 159       Alexis Lopez 1/7/2015 0.398793     E 0
> >>> EC062 160       Alexis Lopez 1/8/2015 0.474345     C 0
> >>> EC062 160       Alexis Lopez 1/8/2015 0.426911      E 0
> >>> What confuses me is that, even if I leave "Lance" as the only random
> >>> effect, the model is still singular.
> >>> Does this happen because the number of levels for Lance is too high
> >>> compared to the number of observations?
> >>> Would it be better to have repeated Lance (set number) for each trip,
> >>> i.e. trip EC062 set 1,2,3 etc then trip EC063 set 1,2,3 etc...?
> >>>
> >>> Thanks again for your help!
> >>>
> >>> Alessandra
> >>>
> >>>
> >>> On Tue, Mar 26, 2019 at 9:08 AM Ren? <bimonosom at gmail.com> wrote:
> >>>
> >>>> Hi Allessandra,
> >>>>
> >>>> your model output says:
> >>>> "Number of obs: 292"
> >>>> and your model has 2 fixed effects and 5 (!) random effects.
> >>>>  - If -  all these random effects are fully crossed. Then assuming you
> >>>> have 19 participants (e.g. 1|observers), and 4 random effects crossed
> on
> >>>> them with two levels each (2*2*2*2 = 16 cells), would make about 292
> >>>> observations.
> >>>> Now you see this math uncovers the most likely problem: A random
> effect
> >>>> (intercept) factor is urgently recommended to have least! 6 levels to
> make
> >>>> such a model meaningful).
> >>>> If all these random effects are not fully crossed, then the model is
> >>>> misspecified, i.e. defining random intercepts for factor 1 separately
> from
> >>>> random intercepts for another factor 2, when factor 1 is nested in
> factor
> >>>> 2, is over-identifying the randomness in your model -> singular.
> >>>>
> >>>> So,
> >>>> only define random intercepts for factors with more then 6 levels
> (move
> >>>> those factors with less levels to the fixed effects instead to
> control for
> >>>> them)
> >>>> only define separate random intercepts for factors that are crossed;
> for
> >>>> instance the factors boat name and lance seem suspicious. I guess,
> there is
> >>>> a world in which a 'lance 1' can only be on boat 'atlantis' to be
> used for
> >>>> fishing, but not on boat 'moby dick'. In this case, having a random
> >>>> intercept for "boat name" in addition to 'lance' would not add
> anything to
> >>>> the model, since lances would already cover the variance of boats
> (lances
> >>>> nested in boats).
> >>>> Get more observations
> >>>> Rerun the model
> >>>> Should be fine :)
> >>>>
> >>>> If singularities still occur use Bayesian models or come back here :))
> >>>>
> >>>> Best, Ren?
> >>>>
> >>>>
> >>>> Am Di., 26. M?rz 2019 um 09:30 Uhr schrieb Thierry Onkelinx via
> >>>> R-sig-mixed-models <r-sig-mixed-models at r-project.org>:
> >>>>
> >>>>> Dear Alessandra,
> >>>>>
> >>>>> Your problem is hard to diagnose without the data. Can you make the
> data
> >>>>> available? Does the combination of factors lead to unique
> observations?
> >>>>> Or
> >>>>> do some combinations have only zero's?
> >>>>>
> >>>>> Best regards,
> >>>>>
> >>>>> ir. Thierry Onkelinx
> >>>>> Statisticus / Statistician
> >>>>>
> >>>>> Vlaamse Overheid / Government of Flanders
> >>>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR
> NATURE
> >>>>> AND
> >>>>> FOREST
> >>>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> >>>>> thierry.onkelinx at inbo.be
> >>>>> Havenlaan 88 bus 73, 1000 Brussel
> >>>>> www.inbo.be
> >>>>>
> >>>>>
> >>>>>
> ///////////////////////////////////////////////////////////////////////////////////////////
> >>>>> To call in the statistician after the experiment is done may be no
> more
> >>>>> than asking him to perform a post-mortem examination: he may be able
> to
> >>>>> say
> >>>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> >>>>> The plural of anecdote is not data. ~ Roger Brinner
> >>>>> The combination of some data and an aching desire for an answer does
> not
> >>>>> ensure that a reasonable answer can be extracted from a given body of
> >>>>> data.
> >>>>> ~ John Tukey
> >>>>>
> >>>>>
> ///////////////////////////////////////////////////////////////////////////////////////////
> >>>>>
> >>>>> <https://www.inbo.be>
> >>>>>
> >>>>>
> >>>>> Op wo 20 mrt. 2019 om 23:19 schreef Alessandra Bielli <
> >>>>> bielli.alessandra at gmail.com>:
> >>>>>
> >>>>>> Dear List
> >>>>>>
> >>>>>> I am fitting this model using the lme4 package, in order to obtain
> >>>>> catch
> >>>>>> estimates using the predict function
> >>>>>>
> >>>>>> m1 <- glmer(Count ~ CE + offset(log(Effort)) + (1|SetYear)
> >>>>> +(1|Season) +
> >>>>>>                   (1|Lance.N) + (1|Boat.Name) + (1|Observer.Name),
> >>>>> data =
> >>>>>> Data,                     glmerControl(optimizer = "bobyqa"),
> family=
> >>>>>> "poisson")
> >>>>>>
> >>>>>>
> >>>>>> where: CE is a categorical (control or treatment), Effort is
> numerical
> >>>>>> (fishing effort), and all the other variables are random effects.
> >>>>>>
> >>>>>> *My problem is that I get a warning message saying that the model is
> >>>>>> singular*
> >>>>>>
> >>>>>> *>summary(m1)*
> >>>>>>
> >>>>>> Generalized linear mixed model fit by maximum likelihood (Laplace
> >>>>>> Approximation) [glmerMod]
> >>>>>>  Family: poisson  ( log )
> >>>>>> Formula: Count ~ CE + offset(log(Effort)) + (1 | SetYear) + (1 |
> >>>>>>     Season) + (1 | Lance.N) + (1 | Boat.Name) + (1 | Observer.Name)
> >>>>>>    Data: Data
> >>>>>> Control: glmerControl(optimizer = "bobyqa")
> >>>>>>
> >>>>>>      AIC      BIC   logLik deviance df.resid
> >>>>>>    148.6    174.3    -67.3    134.6      285
> >>>>>>
> >>>>>> Scaled residuals:
> >>>>>>     Min      1Q  Median      3Q     Max
> >>>>>> -0.4852 -0.1758 -0.1339 -0.1227  3.5980
> >>>>>>
> >>>>>> Random effects:
> >>>>>>  Groups        Name        Variance  Std.Dev.
> >>>>>>  Lance.N       (Intercept) 2.259e+00 1.503e+00
> >>>>>>  Boat.Name     (Intercept) 0.000e+00 0.000e+00
> >>>>>>  Observer.Name (Intercept) 0.000e+00 0.000e+00
> >>>>>>  Season        (Intercept) 4.149e-17 6.442e-09
> >>>>>>  SetYear       (Intercept) 0.000e+00 0.000e+00
> >>>>>> Number of obs: 292, groups:
> >>>>>> Lance.N, 146; Boat.Name, 21; Observer.Name, 5; Season, 4; SetYear, 4
> >>>>>>
> >>>>>> Fixed effects:
> >>>>>>             Estimate Std. Error z value Pr(>|z|)
> >>>>>> (Intercept)  -2.5751     0.6612  -3.895 9.83e-05 ***
> >>>>>> CEE          -0.5878     0.5003  -1.175     0.24
> >>>>>> ---
> >>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>>>>
> >>>>>> Correlation of Fixed Effects:
> >>>>>>     (Intr)
> >>>>>> CEE -0.257
> >>>>>> *convergence code: 0*
> >>>>>> *singular fit*
> >>>>>>
> >>>>>> I am aware that there are a lot of random effects and some of them
> >>>>> have a
> >>>>>> number of levels <5. However, this study was carried out under real
> >>>>> fishery
> >>>>>> conditions, so these random effects seemed all important to me.
> >>>>>>
> >>>>>> I removed the random effects with variance zero as suggested here
> >>>>>>
> >>>>>>
> >>>>>
> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#singular-models-random-effect-variances-estimated-as-zero-or-correlations-estimated-as---1
> >>>>>> until I removed them all and found myself with a glm instead.
> >>>>>>
> >>>>>> My questions are
> >>>>>>
> >>>>>> - why the variance of Lance.N, initially positive, becomes zero
> after
> >>>>> I
> >>>>>> remove the other random effects that had variance equal zero?
> >>>>>> - is it acceptable to fit a glm just because all the random effect
> >>>>>> variances were zero?
> >>>>>>
> >>>>>> I hope I gave all the information you need.
> >>>>>>
> >>>>>> Thanks for any advice!
> >>>>>>
> >>>>>> Alessandra
> >>>>>>
> >>>>>>         [[alternative HTML version deleted]]
> >>>>>>
> >>>>>> _______________________________________________
> >>>>>> R-sig-mixed-models at r-project.org mailing list
> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>>>>>
> >>>>>
> >>>>>         [[alternative HTML version deleted]]
> >>>>>
> >>>>> _______________________________________________
> >>>>> R-sig-mixed-models at r-project.org mailing list
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>>>>
> >>>>
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From Adr|@@n@de@Jong @end|ng |rom @|u@@e  Wed Mar 27 10:50:25 2019
From: Adr|@@n@de@Jong @end|ng |rom @|u@@e (Adriaan De Jong)
Date: Wed, 27 Mar 2019 09:50:25 +0000
Subject: [R-sig-ME] Mixed-effects modelling on data from a highly unbalanced
 design
Message-ID: <cc5b85ccf91542f4be330b5ca28f7ac5@Exch2-3.slu.se>

Dear forum,

I'm struggling with a data set from 12 years of breeding bird monitoring on 13 sites that were affected by the construction of a new railway (there are control sites, too, but these fall outside the scope of this question). I have data from (a) before, (b) during construction, (c) "ready" (= railway ready but no traffic) and (d) traffic. Obviously, these four steps came in a fixed sequence. Because the railway was built in separate sections, the numbers of years of Before, Construction and Ready (coded in variable Status) vary among sites in terms of timing and duration, but all sites were monitored for the same three years while traffic was on. In short, the "design" is highly unbalanced.

At this stage, I'm looking at the possible effect of railway construction on avian biodiversity expressed as the total number of species observed per year per site (data below). I've used the following script to prepare the data:

ArterYears$ArterI<-as.integer(ArterYears$Arter) ## Arter = number of observed species
ArterYears$Site<-as.factor(ArterYears$Site)
ArterYears$NArea<-ArterYears$Areal/mean(ArterYears$Areal) ## Areal = area in hectares
ArterYears$NArea2<-ArterYears$NArea^2
ArterYears$YearS<-as.integer(ArterYears$Year - 2001) ## the study was performed 2002-2015
ArterYears$Status<-as.factor(ArterYears$Status)
ArterYears$response<-ArterYears$ArterI

(Some of these steps are not really necessary, but I've included them just to be sure. The same goes for the assignment of NA's to the model names below.)

The data for species number (range 28 - 83) are overdispersed for all sites combined, but not for the individual sites (rather underdispersed). To be on the safe side, I chose to use a negative binomial family anyway: nbinom2 in glmmTMB (I've tested glmm.nb as well with the same kind of problematic results).

>From biological reasoning, the following model is the one I'd prefer. It uses Status, adjusted Year and log("normalized" area) as fixed effects and Site as random effect. For the avian biodiversity data, this model works technically fine. Biologically speaking, the later Status classes did not differ significantly from Before (the intercept). So far so good.

glmmTMB31B<-NA
glmmTMB31B<-glmmTMB(response ~ Status + YearS + log(NArea) + (1|Site), data=ArterYears, family = nbinom2)
summary(glmmTMB31B)
AICc(glmmTMB31B)

Now I would like to evaluate this model, either against other models or with an R2 of some sort. Now problems pile up.

The "null" model (Site needs to be in the model because numbers really vary among sites, see: plot(ArterYears$ArterI ~ ArterYears$Site) )

glmmTMB0<-NA
glmmTMB0<-glmmTMB(response ~ 1 + (1|Site), data=ArterYears, family = nbinom2)
summary(glmmTMB0)
AICc(glmmTMB0)

generates the following warning messages:


1: In fitTMB(TMBStruc) :

  Model convergence problem; non-positive-definite Hessian matrix. See vignette('troubleshooting')

2: In fitTMB(TMBStruc) :

  Model convergence problem; false convergence (8). See vignette('troubleshooting')

Most other models including combinations of YearS, NArea and Status generate these warning messages (in particular the second one). This puzzles me, but unfortunately, I'm not capable of interpreting the troubleshooting messages. If the model without fixed effects had worked fine and the complex model had failed, I would suspect overparameterization (N = 156), but now I have no clue.

When I apply r.squaredGLMM (MuMIn) to the complex model, warning messages come up even here, despite the fact that they were not generated for the model as such. How should I interpret this?

> r.squaredGLMM(glmmTMB31B)
                   R2m          R2c
delta     1.112643e-18 2.247703e-18
lognormal 4.805332e-04 9.707482e-04
trigamma  6.896496e-35 1.393194e-34
Warning messages:
1: 'r.squaredGLMM' now calculates a revised statistic. See the help page.
2: The null model is correct only if all variables used by the original model remain unchanged.
3: In fitTMB(TMBStruc) :
  Model convergence problem; non-positive-definite Hessian matrix. See vignette('troubleshooting')
4: In fitTMB(TMBStruc) :
  Model convergence problem; false convergence (8). See vignette('troubleshooting')


I assume that I either make a fundamental mistake somewhere, or the data are "problematic". Could I solve all this by abandoning family = nbinom2 and go back to family = poisson (which worked fine technically)? Or test yet another package?
Thanks in advance for any suggestions.

Cheers,
Adriaan "Adjan" de Jong
Senior researcher
Swedish University of Agricultural Sciences

Data:
Site                Year              Status            Areal             Arter
Nyland          2002              Before           33.6               54
Nyland          2003              Construction 33.6               51
Nyland          2004              Construction 33.6               56
Nyland          2005              Construction 33.6               52
Nyland          2006              Ready           33.6               48
Nyland          2007              Ready           33.6               43
Nyland          2008              Ready           33.6               64
Nyland          2009              Ready           33.6               58
Nyland          2010              Ready           33.6               57
Nyland          2013              Traffic           33.6               50
Nyland          2014              Traffic           33.6               53
Nyland          2015              Traffic           33.6               61
Kornsjo          2002              Before           82.8               59
Kornsjo          2003              Before           82.8               66
Kornsjo          2004              Construction 82.8               62
Kornsjo          2005              Construction 82.8               58
Kornsjo          2006              Ready           82.8               60
Kornsjo          2007              Ready           82.8               68
Kornsjo          2008              Ready           82.8               70
Kornsjo          2009              Ready           82.8               59
Kornsjo          2010              Ready           82.8               62
Kornsjo          2013              Traffic           82.8               57
Kornsjo          2014              Traffic           82.8               58
Kornsjo          2015              Traffic           82.8               63
Stranne         2002              Construction 37.0               53
Stranne         2003              Construction 37.0               64
Stranne         2004              Ready           37.0               60
Stranne         2005              Ready           37.0               60
Stranne         2006              Ready           37.0               59
Stranne         2007              Ready           37.0               60
Stranne         2008              Ready           37.0               55
Stranne         2009              Ready           37.0               53
Stranne         2010              Ready           37.0               46
Stranne         2013              Traffic           37.0               58
Stranne         2014              Traffic           37.0               53
Stranne         2015              Traffic           37.0               56
Strandnyland 2002              Construction 69.2               43
Strandnyland 2003              Construction 69.2               44
Strandnyland 2004              Ready           69.2               52
Strandnyland 2005              Ready           69.2               56
Strandnyland 2006              Ready           69.2               46
Strandnyland 2007              Ready           69.2               52
Strandnyland 2008              Ready           69.2               48
Strandnyland 2009              Ready           69.2               43
Strandnyland 2010              Ready           69.2               55
Strandnyland 2013              Traffic           69.2               38
Strandnyland 2014              Traffic           69.2               41
Strandnyland 2015              Traffic           69.2               46
Hjalta             2002              Ready           91.1               61
Hjalta             2003              Ready           91.1               61
Hjalta             2004              Ready           91.1               62
Hjalta             2005              Ready           91.1               61
Hjalta             2006              Ready           91.1               66
Hjalta             2007              Ready           91.1               71
Hjalta             2008              Ready           91.1               73
Hjalta             2009              Ready           91.1               58
Hjalta             2010              Ready           91.1               64
Hjalta             2013              Traffic           91.1               67
Hjalta             2014              Traffic           91.1               71
Hjalta             2015              Traffic           91.1               67
Kasa              2002              Construction 98.3               65
Kasa              2003              Ready           98.3               59
Kasa              2004              Ready           98.3               59
Kasa              2005              Ready           98.3               66
Kasa              2006              Ready           98.3               52
Kasa              2007              Ready           98.3               53
Kasa              2008              Ready           98.3               56
Kasa              2009              Ready           98.3               53
Kasa              2010              Ready           98.3               51
Kasa              2013              Traffic           98.3               51
Kasa              2014              Traffic           98.3               54
Kasa              2015              Traffic           98.3               64
Ava               2002              Before           109.5             67
Ava               2003              Before           109.5             79
Ava               2004              Construction 109.5             70
Ava               2005              Construction 109.5             77
Ava               2006              Construction 109.5             76
Ava               2007              Ready           109.5             80
Ava               2008              Ready           109.5             77
Ava               2009              Ready           109.5             81
Ava               2010              Ready           109.5             83
Ava               2013              Traffic           109.5             76
Ava               2014              Traffic           109.5             81
Ava               2015              Traffic           109.5             78
Logdea          2002              Before           67.4               54
Logdea          2003              Before           67.4               53
Logdea          2004              Before           67.4               57
Logdea          2005              Construction 67.4               55
Logdea          2006              Construction 67.4               49
Logdea          2007              Ready           67.4               61
Logdea          2008              Ready           67.4               56
Logdea          2009              Ready           67.4               56
Logdea          2010              Ready           67.4               53
Logdea          2013              Traffic           67.4               55
Logdea          2014              Traffic           67.4               53
Logdea          2015              Traffic           67.4               51
Langed          2002              Before           34.2               49
Langed          2003              Before           34.2               56
Langed          2004              Before           34.2               51
Langed          2005              Before           34.2               52
Langed          2006              Construction 34.2               47
Langed          2007              Ready           34.2               56
Langed          2008              Ready           34.2               54
Langed          2009              Ready           34.2               51
Langed          2010              Ready           34.2               54
Langed          2013              Traffic           34.2               57
Langed          2014              Traffic           34.2               51
Langed          2015              Traffic           34.2               50
Hornea          2002              Before           48.9               48
Hornea          2003              Before           48.9               49
Hornea          2004              Before           48.9               41
Hornea          2005              Before           48.9               37
Hornea          2006              Construction 48.9               40
Hornea          2007              Construction 48.9               48
Hornea          2008              Ready           48.9               48
Hornea          2009              Ready           48.9               39
Hornea          2010              Ready           48.9               47
Hornea          2013              Traffic           48.9               47
Hornea          2014              Traffic           48.9               43
Hornea          2015              Traffic           48.9               47
Stocke          2002              Before           208.4             58
Stocke          2003              Before           208.4             69
Stocke          2004              Before           208.4             73
Stocke          2005              Before           208.4             67
Stocke          2006              Before           208.4             66
Stocke          2007              Construction 208.4             71
Stocke          2008              Construction 208.4             75
Stocke          2009              Construction 208.4             60
Stocke          2010              Ready           208.4             68
Stocke          2013              Traffic           208.4             73
Stocke          2014              Traffic           208.4             67
Stocke          2015              Traffic           208.4             75
StockeNE     2002              Before           33.9               34
StockeNE     2003              Before           33.9               36
StockeNE     2004              Before           33.9               32
StockeNE     2005              Before           33.9               28
StockeNE     2006              Before           33.9               35
StockeNE     2007              Construction 33.9               43
StockeNE     2008              Construction 33.9               41
StockeNE     2009              Construction 33.9               34
StockeNE     2010              Ready           33.9               34
StockeNE     2013              Traffic           33.9               33
StockeNE     2014              Traffic           33.9               32
StockeNE     2015              Traffic           33.9               37
Degernas      2002              Before           68.8               56
Degernas      2003              Before           68.8               56
Degernas      2004              Before           68.8               66
Degernas      2005              Before           68.8               64
Degernas      2006              Before           68.8               58
Degernas      2007              Before           68.8               64
Degernas      2008              Construction 68.8               62
Degernas      2009              Construction 68.8               61
Degernas      2010              Construction 68.8               68
Degernas      2013              Traffic           68.8               69
Degernas      2014              Traffic           68.8               57
Degernas      2015              Traffic           68.8               70

---
N?r du skickar e-post till SLU s? inneb?r detta att SLU behandlar dina personuppgifter. F?r att l?sa mer om hur detta g?r till, klicka h?r <https://www.slu.se/om-slu/kontakta-slu/personuppgifter/>
E-mailing SLU will result in SLU processing your personal data. For more information on how this is done, click here <https://www.slu.se/en/about-slu/contact-slu/personal-data/>

	[[alternative HTML version deleted]]


From he|n@v@n@||ever|oo @end|ng |rom v|@etern@@n|  Wed Mar 27 11:10:24 2019
From: he|n@v@n@||ever|oo @end|ng |rom v|@etern@@n| (Hein van Lieverloo)
Date: Wed, 27 Mar 2019 10:10:24 +0000 (UTC)
Subject: [R-sig-ME] 
 Mixed-effects modelling on data from a highly unbalanced design
In-Reply-To: <cc5b85ccf91542f4be330b5ca28f7ac5@Exch2-3.slu.se>
References: <cc5b85ccf91542f4be330b5ca28f7ac5@Exch2-3.slu.se>
Message-ID: <893991A5BDBB08D7.49511f74-7e88-4015-929a-d19fef38f4d6@mail.outlook.com>

Dear Adriaan (and others),




Can you perform time-series analyses with glmm? A good friend of mine is a specialist in time-series analyses and always works with Mann-Kendall with Theil-Sen and related non-parametric methods. I know glmm kan work with repeated measures.





Kind regards,




Hein van Lieverloo









On Wed, Mar 27, 2019 at 10:51 AM +0100, "Adriaan De Jong" <Adriaan.de.Jong at slu.se> wrote:










Dear forum,

I'm struggling with a data set from 12 years of breeding bird monitoring on 13 sites that were affected by the construction of a new railway (there are control sites, too, but these fall outside the scope of this question). I have data from (a) before, (b) during construction, (c) "ready" (= railway ready but no traffic) and (d) traffic. Obviously, these four steps came in a fixed sequence. Because the railway was built in separate sections, the numbers of years of Before, Construction and Ready (coded in variable Status) vary among sites in terms of timing and duration, but all sites were monitored for the same three years while traffic was on. In short, the "design" is highly unbalanced.

At this stage, I'm looking at the possible effect of railway construction on avian biodiversity expressed as the total number of species observed per year per site (data below). I've used the following script to prepare the data:

ArterYears$ArterI<-as.integer(ArterYears$Arter) ## Arter = number of observed species
ArterYears$Site<-as.factor(ArterYears$Site)
ArterYears$NArea<-ArterYears$Areal/mean(ArterYears$Areal) ## Areal = area in hectares
ArterYears$NArea2<-ArterYears$NArea^2
ArterYears$YearS<-as.integer(ArterYears$Year - 2001) ## the study was performed 2002-2015
ArterYears$Status<-as.factor(ArterYears$Status)
ArterYears$response<-ArterYears$ArterI

(Some of these steps are not really necessary, but I've included them just to be sure. The same goes for the assignment of NA's to the model names below.)

The data for species number (range 28 - 83) are overdispersed for all sites combined, but not for the individual sites (rather underdispersed). To be on the safe side, I chose to use a negative binomial family anyway: nbinom2 in glmmTMB (I've tested glmm.nb as well with the same kind of problematic results).

>From biological reasoning, the following model is the one I'd prefer. It uses Status, adjusted Year and log("normalized" area) as fixed effects and Site as random effect. For the avian biodiversity data, this model works technically fine. Biologically speaking, the later Status classes did not differ significantly from Before (the intercept). So far so good.

glmmTMB31B<-NA
glmmTMB31B<-glmmTMB(response ~ Status + YearS + log(NArea) + (1|Site), data=ArterYears, family = nbinom2)
summary(glmmTMB31B)
AICc(glmmTMB31B)

Now I would like to evaluate this model, either against other models or with an R2 of some sort. Now problems pile up.

The "null" model (Site needs to be in the model because numbers really vary among sites, see: plot(ArterYears$ArterI ~ ArterYears$Site) )

glmmTMB0<-NA
glmmTMB0<-glmmTMB(response ~ 1 + (1|Site), data=ArterYears, family = nbinom2)
summary(glmmTMB0)
AICc(glmmTMB0)

generates the following warning messages:


1: In fitTMB(TMBStruc) :

  Model convergence problem; non-positive-definite Hessian matrix. See vignette('troubleshooting')

2: In fitTMB(TMBStruc) :

  Model convergence problem; false convergence (8). See vignette('troubleshooting')

Most other models including combinations of YearS, NArea and Status generate these warning messages (in particular the second one). This puzzles me, but unfortunately, I'm not capable of interpreting the troubleshooting messages. If the model without fixed effects had worked fine and the complex model had failed, I would suspect overparameterization (N = 156), but now I have no clue.

When I apply r.squaredGLMM (MuMIn) to the complex model, warning messages come up even here, despite the fact that they were not generated for the model as such. How should I interpret this?

> r.squaredGLMM(glmmTMB31B)
                   R2m          R2c
delta     1.112643e-18 2.247703e-18
lognormal 4.805332e-04 9.707482e-04
trigamma  6.896496e-35 1.393194e-34
Warning messages:
1: 'r.squaredGLMM' now calculates a revised statistic. See the help page.
2: The null model is correct only if all variables used by the original model remain unchanged.
3: In fitTMB(TMBStruc) :
  Model convergence problem; non-positive-definite Hessian matrix. See vignette('troubleshooting')
4: In fitTMB(TMBStruc) :
  Model convergence problem; false convergence (8). See vignette('troubleshooting')


I assume that I either make a fundamental mistake somewhere, or the data are "problematic". Could I solve all this by abandoning family = nbinom2 and go back to family = poisson (which worked fine technically)? Or test yet another package?
Thanks in advance for any suggestions.

Cheers,
Adriaan "Adjan" de Jong
Senior researcher
Swedish University of Agricultural Sciences

Data:
Site                Year              Status            Areal             Arter
Nyland          2002              Before           33.6               54
Nyland          2003              Construction 33.6               51
Nyland          2004              Construction 33.6               56
Nyland          2005              Construction 33.6               52
Nyland          2006              Ready           33.6               48
Nyland          2007              Ready           33.6               43
Nyland          2008              Ready           33.6               64
Nyland          2009              Ready           33.6               58
Nyland          2010              Ready           33.6               57
Nyland          2013              Traffic           33.6               50
Nyland          2014              Traffic           33.6               53
Nyland          2015              Traffic           33.6               61
Kornsjo          2002              Before           82.8               59
Kornsjo          2003              Before           82.8               66
Kornsjo          2004              Construction 82.8               62
Kornsjo          2005              Construction 82.8               58
Kornsjo          2006              Ready           82.8               60
Kornsjo          2007              Ready           82.8               68
Kornsjo          2008              Ready           82.8               70
Kornsjo          2009              Ready           82.8               59
Kornsjo          2010              Ready           82.8               62
Kornsjo          2013              Traffic           82.8               57
Kornsjo          2014              Traffic           82.8               58
Kornsjo          2015              Traffic           82.8               63
Stranne         2002              Construction 37.0               53
Stranne         2003              Construction 37.0               64
Stranne         2004              Ready           37.0               60
Stranne         2005              Ready           37.0               60
Stranne         2006              Ready           37.0               59
Stranne         2007              Ready           37.0               60
Stranne         2008              Ready           37.0               55
Stranne         2009              Ready           37.0               53
Stranne         2010              Ready           37.0               46
Stranne         2013              Traffic           37.0               58
Stranne         2014              Traffic           37.0               53
Stranne         2015              Traffic           37.0               56
Strandnyland 2002              Construction 69.2               43
Strandnyland 2003              Construction 69.2               44
Strandnyland 2004              Ready           69.2               52
Strandnyland 2005              Ready           69.2               56
Strandnyland 2006              Ready           69.2               46
Strandnyland 2007              Ready           69.2               52
Strandnyland 2008              Ready           69.2               48
Strandnyland 2009              Ready           69.2               43
Strandnyland 2010              Ready           69.2               55
Strandnyland 2013              Traffic           69.2               38
Strandnyland 2014              Traffic           69.2               41
Strandnyland 2015              Traffic           69.2               46
Hjalta             2002              Ready           91.1               61
Hjalta             2003              Ready           91.1               61
Hjalta             2004              Ready           91.1               62
Hjalta             2005              Ready           91.1               61
Hjalta             2006              Ready           91.1               66
Hjalta             2007              Ready           91.1               71
Hjalta             2008              Ready           91.1               73
Hjalta             2009              Ready           91.1               58
Hjalta             2010              Ready           91.1               64
Hjalta             2013              Traffic           91.1               67
Hjalta             2014              Traffic           91.1               71
Hjalta             2015              Traffic           91.1               67
Kasa              2002              Construction 98.3               65
Kasa              2003              Ready           98.3               59
Kasa              2004              Ready           98.3               59
Kasa              2005              Ready           98.3               66
Kasa              2006              Ready           98.3               52
Kasa              2007              Ready           98.3               53
Kasa              2008              Ready           98.3               56
Kasa              2009              Ready           98.3               53
Kasa              2010              Ready           98.3               51
Kasa              2013              Traffic           98.3               51
Kasa              2014              Traffic           98.3               54
Kasa              2015              Traffic           98.3               64
Ava               2002              Before           109.5             67
Ava               2003              Before           109.5             79
Ava               2004              Construction 109.5             70
Ava               2005              Construction 109.5             77
Ava               2006              Construction 109.5             76
Ava               2007              Ready           109.5             80
Ava               2008              Ready           109.5             77
Ava               2009              Ready           109.5             81
Ava               2010              Ready           109.5             83
Ava               2013              Traffic           109.5             76
Ava               2014              Traffic           109.5             81
Ava               2015              Traffic           109.5             78
Logdea          2002              Before           67.4               54
Logdea          2003              Before           67.4               53
Logdea          2004              Before           67.4               57
Logdea          2005              Construction 67.4               55
Logdea          2006              Construction 67.4               49
Logdea          2007              Ready           67.4               61
Logdea          2008              Ready           67.4               56
Logdea          2009              Ready           67.4               56
Logdea          2010              Ready           67.4               53
Logdea          2013              Traffic           67.4               55
Logdea          2014              Traffic           67.4               53
Logdea          2015              Traffic           67.4               51
Langed          2002              Before           34.2               49
Langed          2003              Before           34.2               56
Langed          2004              Before           34.2               51
Langed          2005              Before           34.2               52
Langed          2006              Construction 34.2               47
Langed          2007              Ready           34.2               56
Langed          2008              Ready           34.2               54
Langed          2009              Ready           34.2               51
Langed          2010              Ready           34.2               54
Langed          2013              Traffic           34.2               57
Langed          2014              Traffic           34.2               51
Langed          2015              Traffic           34.2               50
Hornea          2002              Before           48.9               48
Hornea          2003              Before           48.9               49
Hornea          2004              Before           48.9               41
Hornea          2005              Before           48.9               37
Hornea          2006              Construction 48.9               40
Hornea          2007              Construction 48.9               48
Hornea          2008              Ready           48.9               48
Hornea          2009              Ready           48.9               39
Hornea          2010              Ready           48.9               47
Hornea          2013              Traffic           48.9               47
Hornea          2014              Traffic           48.9               43
Hornea          2015              Traffic           48.9               47
Stocke          2002              Before           208.4             58
Stocke          2003              Before           208.4             69
Stocke          2004              Before           208.4             73
Stocke          2005              Before           208.4             67
Stocke          2006              Before           208.4             66
Stocke          2007              Construction 208.4             71
Stocke          2008              Construction 208.4             75
Stocke          2009              Construction 208.4             60
Stocke          2010              Ready           208.4             68
Stocke          2013              Traffic           208.4             73
Stocke          2014              Traffic           208.4             67
Stocke          2015              Traffic           208.4             75
StockeNE     2002              Before           33.9               34
StockeNE     2003              Before           33.9               36
StockeNE     2004              Before           33.9               32
StockeNE     2005              Before           33.9               28
StockeNE     2006              Before           33.9               35
StockeNE     2007              Construction 33.9               43
StockeNE     2008              Construction 33.9               41
StockeNE     2009              Construction 33.9               34
StockeNE     2010              Ready           33.9               34
StockeNE     2013              Traffic           33.9               33
StockeNE     2014              Traffic           33.9               32
StockeNE     2015              Traffic           33.9               37
Degernas      2002              Before           68.8               56
Degernas      2003              Before           68.8               56
Degernas      2004              Before           68.8               66
Degernas      2005              Before           68.8               64
Degernas      2006              Before           68.8               58
Degernas      2007              Before           68.8               64
Degernas      2008              Construction 68.8               62
Degernas      2009              Construction 68.8               61
Degernas      2010              Construction 68.8               68
Degernas      2013              Traffic           68.8               69
Degernas      2014              Traffic           68.8               57
Degernas      2015              Traffic           68.8               70

---
N?r du skickar e-post till SLU s? inneb?r detta att SLU behandlar dina personuppgifter. F?r att l?sa mer om hur detta g?r till, klicka h?r 
E-mailing SLU will result in SLU processing your personal data. For more information on how this is done, click here 

	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models







	[[alternative HTML version deleted]]


From Adr|@@n@de@Jong @end|ng |rom @|u@@e  Wed Mar 27 13:28:49 2019
From: Adr|@@n@de@Jong @end|ng |rom @|u@@e (Adriaan De Jong)
Date: Wed, 27 Mar 2019 12:28:49 +0000
Subject: [R-sig-ME] 
 Mixed-effects modelling on data from a highly unbalanced design
In-Reply-To: <893991A5BDBB08D7.49511f74-7e88-4015-929a-d19fef38f4d6@mail.outlook.com>
References: <cc5b85ccf91542f4be330b5ca28f7ac5@Exch2-3.slu.se>
 <893991A5BDBB08D7.49511f74-7e88-4015-929a-d19fef38f4d6@mail.outlook.com>
Message-ID: <7d296aef85774d558ed4feda408f725c@Exch2-3.slu.se>

Dear Hein,
I think I get your point. Thanks.
The time series aspect is not really what I?m after here. I used Kendall rank-order correlation tests to check for temporal trends. Overall there was a positive trend (tau = 0.56, z = 2.45, P = 0.014), but not for any of the individual sites. For the analysis of the possible effects of railway construction, this general trend is a bit of a nuisance (regardless its cause) and I hoped to separate it from the Status variable effect by adding Year-2001 as a fixed effect in the model.
Year-2001 could also be added as a nested random effect (for site-specific trends), but I?m pretty sure the data wouldn?t allow for this level of complexity. Correct me if I?m wrong.
Cheers,
Adjan


PS. I currently use R version 3.5.3 on a x86_64-w64-mingw32/x64 (64-bit) platform (Windows 10, v. 1703) on a HP Elitebook computer. I regularly update all my installed R-packages.




From: Hein van Lieverloo <hein.van.lieverloo at viaeterna.nl>
Sent: den 27 mars 2019 11:10
To: r-sig-mixed-models at r-project.org; Adriaan De Jong <Adriaan.de.Jong at slu.se>
Subject: Re: [R-sig-ME] Mixed-effects modelling on data from a highly unbalanced design

Dear Adriaan (and others),
Can you perform time-series analyses with glmm? A good friend of mine is a specialist in time-series analyses and always works with Mann-Kendall with Theil-Sen and related non-parametric methods. I know glmm kan work with repeated measures.

Kind regards,
Hein van Lieverloo



On Wed, Mar 27, 2019 at 10:51 AM +0100, "Adriaan De Jong" <Adriaan.de.Jong at slu.se<mailto:Adriaan.de.Jong at slu.se>> wrote:

Dear forum,



I'm struggling with a data set from 12 years of breeding bird monitoring on 13 sites that were affected by the construction of a new railway (there are control sites, too, but these fall outside the scope of this question). I have data from (a) before, (b) during construction, (c) "ready" (= railway ready but no traffic) and (d) traffic. Obviously, these four steps came in a fixed sequence. Because the railway was built in separate sections, the numbers of years of Before, Construction and Ready (coded in variable Status) vary among sites in terms of timing and duration, but all sites were monitored for the same three years while traffic was on. In short, the "design" is highly unbalanced.



At this stage, I'm looking at the possible effect of railway construction on avian biodiversity expressed as the total number of species observed per year per site (data below). I've used the following script to prepare the data:



ArterYears$ArterI<-as.integer(ArterYears$Arter) ## Arter = number of observed species

ArterYears$Site<-as.factor(ArterYears$Site)

ArterYears$NArea<-ArterYears$Areal/mean(ArterYears$Areal) ## Areal = area in hectares

ArterYears$NArea2<-ArterYears$NArea^2

ArterYears$YearS<-as.integer(ArterYears$Year - 2001) ## the study was performed 2002-2015

ArterYears$Status<-as.factor(ArterYears$Status)

ArterYears$response<-ArterYears$ArterI



(Some of these steps are not really necessary, but I've included them just to be sure. The same goes for the assignment of NA's to the model names below.)



The data for species number (range 28 - 83) are overdispersed for all sites combined, but not for the individual sites (rather underdispersed). To be on the safe side, I chose to use a negative binomial family anyway: nbinom2 in glmmTMB (I've tested glmm.nb as well with the same kind of problematic results).



>From biological reasoning, the following model is the one I'd prefer. It uses Status, adjusted Year and log("normalized" area) as fixed effects and Site as random effect. For the avian biodiversity data, this model works technically fine. Biologically speaking, the later Status classes did not differ significantly from Before (the intercept). So far so good.



glmmTMB31B<-NA

glmmTMB31B<-glmmTMB(response ~ Status + YearS + log(NArea) + (1|Site), data=ArterYears, family = nbinom2)

summary(glmmTMB31B)

AICc(glmmTMB31B)



Now I would like to evaluate this model, either against other models or with an R2 of some sort. Now problems pile up.



The "null" model (Site needs to be in the model because numbers really vary among sites, see: plot(ArterYears$ArterI ~ ArterYears$Site) )



glmmTMB0<-NA

glmmTMB0<-glmmTMB(response ~ 1 + (1|Site), data=ArterYears, family = nbinom2)

summary(glmmTMB0)

AICc(glmmTMB0)



generates the following warning messages:





1: In fitTMB(TMBStruc) :



  Model convergence problem; non-positive-definite Hessian matrix. See vignette('troubleshooting')



2: In fitTMB(TMBStruc) :



  Model convergence problem; false convergence (8). See vignette('troubleshooting')



Most other models including combinations of YearS, NArea and Status generate these warning messages (in particular the second one). This puzzles me, but unfortunately, I'm not capable of interpreting the troubleshooting messages. If the model without fixed effects had worked fine and the complex model had failed, I would suspect overparameterization (N = 156), but now I have no clue.



When I apply r.squaredGLMM (MuMIn) to the complex model, warning messages come up even here, despite the fact that they were not generated for the model as such. How should I interpret this?



> r.squaredGLMM(glmmTMB31B)

                   R2m          R2c

delta     1.112643e-18 2.247703e-18

lognormal 4.805332e-04 9.707482e-04

trigamma  6.896496e-35 1.393194e-34

Warning messages:

1: 'r.squaredGLMM' now calculates a revised statistic. See the help page.

2: The null model is correct only if all variables used by the original model remain unchanged.

3: In fitTMB(TMBStruc) :

  Model convergence problem; non-positive-definite Hessian matrix. See vignette('troubleshooting')

4: In fitTMB(TMBStruc) :

  Model convergence problem; false convergence (8). See vignette('troubleshooting')





I assume that I either make a fundamental mistake somewhere, or the data are "problematic". Could I solve all this by abandoning family = nbinom2 and go back to family = poisson (which worked fine technically)? Or test yet another package?

Thanks in advance for any suggestions.



Cheers,

Adriaan "Adjan" de Jong

Senior researcher

Swedish University of Agricultural Sciences



Data:

Site                Year              Status            Areal             Arter

Nyland          2002              Before           33.6               54

Nyland          2003              Construction 33.6               51

Nyland          2004              Construction 33.6               56

Nyland          2005              Construction 33.6               52

Nyland          2006              Ready           33.6               48

Nyland          2007              Ready           33.6               43

Nyland          2008              Ready           33.6               64

Nyland          2009              Ready           33.6               58

Nyland          2010              Ready           33.6               57

Nyland          2013              Traffic           33.6               50

Nyland          2014              Traffic           33.6               53

Nyland          2015              Traffic           33.6               61

Kornsjo          2002              Before           82.8               59

Kornsjo          2003              Before           82.8               66

Kornsjo          2004              Construction 82.8               62

Kornsjo          2005              Construction 82.8               58

Kornsjo          2006              Ready           82.8               60

Kornsjo          2007              Ready           82.8               68

Kornsjo          2008              Ready           82.8               70

Kornsjo          2009              Ready           82.8               59

Kornsjo          2010              Ready           82.8               62

Kornsjo          2013              Traffic           82.8               57

Kornsjo          2014              Traffic           82.8               58

Kornsjo          2015              Traffic           82.8               63

Stranne         2002              Construction 37.0               53

Stranne         2003              Construction 37.0               64

Stranne         2004              Ready           37.0               60

Stranne         2005              Ready           37.0               60

Stranne         2006              Ready           37.0               59

Stranne         2007              Ready           37.0               60

Stranne         2008              Ready           37.0               55

Stranne         2009              Ready           37.0               53

Stranne         2010              Ready           37.0               46

Stranne         2013              Traffic           37.0               58

Stranne         2014              Traffic           37.0               53

Stranne         2015              Traffic           37.0               56

Strandnyland 2002              Construction 69.2               43

Strandnyland 2003              Construction 69.2               44

Strandnyland 2004              Ready           69.2               52

Strandnyland 2005              Ready           69.2               56

Strandnyland 2006              Ready           69.2               46

Strandnyland 2007              Ready           69.2               52

Strandnyland 2008              Ready           69.2               48

Strandnyland 2009              Ready           69.2               43

Strandnyland 2010              Ready           69.2               55

Strandnyland 2013              Traffic           69.2               38

Strandnyland 2014              Traffic           69.2               41

Strandnyland 2015              Traffic           69.2               46

Hjalta             2002              Ready           91.1               61

Hjalta             2003              Ready           91.1               61

Hjalta             2004              Ready           91.1               62

Hjalta             2005              Ready           91.1               61

Hjalta             2006              Ready           91.1               66

Hjalta             2007              Ready           91.1               71

Hjalta             2008              Ready           91.1               73

Hjalta             2009              Ready           91.1               58

Hjalta             2010              Ready           91.1               64

Hjalta             2013              Traffic           91.1               67

Hjalta             2014              Traffic           91.1               71

Hjalta             2015              Traffic           91.1               67

Kasa              2002              Construction 98.3               65

Kasa              2003              Ready           98.3               59

Kasa              2004              Ready           98.3               59

Kasa              2005              Ready           98.3               66

Kasa              2006              Ready           98.3               52

Kasa              2007              Ready           98.3               53

Kasa              2008              Ready           98.3               56

Kasa              2009              Ready           98.3               53

Kasa              2010              Ready           98.3               51

Kasa              2013              Traffic           98.3               51

Kasa              2014              Traffic           98.3               54

Kasa              2015              Traffic           98.3               64

Ava               2002              Before           109.5             67

Ava               2003              Before           109.5             79

Ava               2004              Construction 109.5             70

Ava               2005              Construction 109.5             77

Ava               2006              Construction 109.5             76

Ava               2007              Ready           109.5             80

Ava               2008              Ready           109.5             77

Ava               2009              Ready           109.5             81

Ava               2010              Ready           109.5             83

Ava               2013              Traffic           109.5             76

Ava               2014              Traffic           109.5             81

Ava               2015              Traffic           109.5             78

Logdea          2002              Before           67.4               54

Logdea          2003              Before           67.4               53

Logdea          2004              Before           67.4               57

Logdea          2005              Construction 67.4               55

Logdea          2006              Construction 67.4               49

Logdea          2007              Ready           67.4               61

Logdea          2008              Ready           67.4               56

Logdea          2009              Ready           67.4               56

Logdea          2010              Ready           67.4               53

Logdea          2013              Traffic           67.4               55

Logdea          2014              Traffic           67.4               53

Logdea          2015              Traffic           67.4               51

Langed          2002              Before           34.2               49

Langed          2003              Before           34.2               56

Langed          2004              Before           34.2               51

Langed          2005              Before           34.2               52

Langed          2006              Construction 34.2               47

Langed          2007              Ready           34.2               56

Langed          2008              Ready           34.2               54

Langed          2009              Ready           34.2               51

Langed          2010              Ready           34.2               54

Langed          2013              Traffic           34.2               57

Langed          2014              Traffic           34.2               51

Langed          2015              Traffic           34.2               50

Hornea          2002              Before           48.9               48

Hornea          2003              Before           48.9               49

Hornea          2004              Before           48.9               41

Hornea          2005              Before           48.9               37

Hornea          2006              Construction 48.9               40

Hornea          2007              Construction 48.9               48

Hornea          2008              Ready           48.9               48

Hornea          2009              Ready           48.9               39

Hornea          2010              Ready           48.9               47

Hornea          2013              Traffic           48.9               47

Hornea          2014              Traffic           48.9               43

Hornea          2015              Traffic           48.9               47

Stocke          2002              Before           208.4             58

Stocke          2003              Before           208.4             69

Stocke          2004              Before           208.4             73

Stocke          2005              Before           208.4             67

Stocke          2006              Before           208.4             66

Stocke          2007              Construction 208.4             71

Stocke          2008              Construction 208.4             75

Stocke          2009              Construction 208.4             60

Stocke          2010              Ready           208.4             68

Stocke          2013              Traffic           208.4             73

Stocke          2014              Traffic           208.4             67

Stocke          2015              Traffic           208.4             75

StockeNE     2002              Before           33.9               34

StockeNE     2003              Before           33.9               36

StockeNE     2004              Before           33.9               32

StockeNE     2005              Before           33.9               28

StockeNE     2006              Before           33.9               35

StockeNE     2007              Construction 33.9               43

StockeNE     2008              Construction 33.9               41

StockeNE     2009              Construction 33.9               34

StockeNE     2010              Ready           33.9               34

StockeNE     2013              Traffic           33.9               33

StockeNE     2014              Traffic           33.9               32

StockeNE     2015              Traffic           33.9               37

Degernas      2002              Before           68.8               56

Degernas      2003              Before           68.8               56

Degernas      2004              Before           68.8               66

Degernas      2005              Before           68.8               64

Degernas      2006              Before           68.8               58

Degernas      2007              Before           68.8               64

Degernas      2008              Construction 68.8               62

Degernas      2009              Construction 68.8               61

Degernas      2010              Construction 68.8               68

Degernas      2013              Traffic           68.8               69

Degernas      2014              Traffic           68.8               57

Degernas      2015              Traffic           68.8               70



---

N?r du skickar e-post till SLU s? inneb?r detta att SLU behandlar dina personuppgifter. F?r att l?sa mer om hur detta g?r till, klicka h?r

E-mailing SLU will result in SLU processing your personal data. For more information on how this is done, click here



      [[alternative HTML version deleted]]



_______________________________________________

R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list

https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



---
N?r du skickar e-post till SLU s? inneb?r detta att SLU behandlar dina personuppgifter. F?r att l?sa mer om hur detta g?r till, klicka h?r <https://www.slu.se/om-slu/kontakta-slu/personuppgifter/>
E-mailing SLU will result in SLU processing your personal data. For more information on how this is done, click here <https://www.slu.se/en/about-slu/contact-slu/personal-data/>

	[[alternative HTML version deleted]]


From d@r|zopou|o@ @end|ng |rom er@@mu@mc@n|  Wed Mar 27 13:35:20 2019
From: d@r|zopou|o@ @end|ng |rom er@@mu@mc@n| (D. Rizopoulos)
Date: Wed, 27 Mar 2019 12:35:20 +0000
Subject: [R-sig-ME] 
 Mixed-effects modelling on data from a highly unbalanced design
In-Reply-To: <cc5b85ccf91542f4be330b5ca28f7ac5@Exch2-3.slu.se>
References: <cc5b85ccf91542f4be330b5ca28f7ac5@Exch2-3.slu.se>
Message-ID: <8bc8f896-4631-2ee9-d80a-df5eb2c8bc50@erasmusmc.nl>

You could also give a try in the GLMMadaptive package that can fit the 
same model but using the more accurate adaptive Gaussian quadrature 
method to approximate the integrals in the definition of the likelihood 
instead of the Laplace approximation.

You can find several examples in the following links:

https://drizopoulos.github.io/GLMMadaptive/

https://drizopoulos.github.io/GLMMadaptive/articles/Custom_Models.html#negative-binomial-mixed-effects-model

https://drizopoulos.github.io/GLMMadaptive/articles/ZeroInflated_and_TwoPart_Models.html

https://drizopoulos.github.io/GLMMadaptive/articles/Goodness_of_Fit.html

Best,
Dimitris


On 3/27/2019 10:50 AM, Adriaan De Jong wrote:
> Dear forum,
> 
> I'm struggling with a data set from 12 years of breeding bird monitoring on 13 sites that were affected by the construction of a new railway (there are control sites, too, but these fall outside the scope of this question). I have data from (a) before, (b) during construction, (c) "ready" (= railway ready but no traffic) and (d) traffic. Obviously, these four steps came in a fixed sequence. Because the railway was built in separate sections, the numbers of years of Before, Construction and Ready (coded in variable Status) vary among sites in terms of timing and duration, but all sites were monitored for the same three years while traffic was on. In short, the "design" is highly unbalanced.
> 
> At this stage, I'm looking at the possible effect of railway construction on avian biodiversity expressed as the total number of species observed per year per site (data below). I've used the following script to prepare the data:
> 
> ArterYears$ArterI<-as.integer(ArterYears$Arter) ## Arter = number of observed species
> ArterYears$Site<-as.factor(ArterYears$Site)
> ArterYears$NArea<-ArterYears$Areal/mean(ArterYears$Areal) ## Areal = area in hectares
> ArterYears$NArea2<-ArterYears$NArea^2
> ArterYears$YearS<-as.integer(ArterYears$Year - 2001) ## the study was performed 2002-2015
> ArterYears$Status<-as.factor(ArterYears$Status)
> ArterYears$response<-ArterYears$ArterI
> 
> (Some of these steps are not really necessary, but I've included them just to be sure. The same goes for the assignment of NA's to the model names below.)
> 
> The data for species number (range 28 - 83) are overdispersed for all sites combined, but not for the individual sites (rather underdispersed). To be on the safe side, I chose to use a negative binomial family anyway: nbinom2 in glmmTMB (I've tested glmm.nb as well with the same kind of problematic results).
> 
>>From biological reasoning, the following model is the one I'd prefer. It uses Status, adjusted Year and log("normalized" area) as fixed effects and Site as random effect. For the avian biodiversity data, this model works technically fine. Biologically speaking, the later Status classes did not differ significantly from Before (the intercept). So far so good.
> 
> glmmTMB31B<-NA
> glmmTMB31B<-glmmTMB(response ~ Status + YearS + log(NArea) + (1|Site), data=ArterYears, family = nbinom2)
> summary(glmmTMB31B)
> AICc(glmmTMB31B)
> 
> Now I would like to evaluate this model, either against other models or with an R2 of some sort. Now problems pile up.
> 
> The "null" model (Site needs to be in the model because numbers really vary among sites, see: plot(ArterYears$ArterI ~ ArterYears$Site) )
> 
> glmmTMB0<-NA
> glmmTMB0<-glmmTMB(response ~ 1 + (1|Site), data=ArterYears, family = nbinom2)
> summary(glmmTMB0)
> AICc(glmmTMB0)
> 
> generates the following warning messages:
> 
> 
> 1: In fitTMB(TMBStruc) :
> 
>    Model convergence problem; non-positive-definite Hessian matrix. See vignette('troubleshooting')
> 
> 2: In fitTMB(TMBStruc) :
> 
>    Model convergence problem; false convergence (8). See vignette('troubleshooting')
> 
> Most other models including combinations of YearS, NArea and Status generate these warning messages (in particular the second one). This puzzles me, but unfortunately, I'm not capable of interpreting the troubleshooting messages. If the model without fixed effects had worked fine and the complex model had failed, I would suspect overparameterization (N = 156), but now I have no clue.
> 
> When I apply r.squaredGLMM (MuMIn) to the complex model, warning messages come up even here, despite the fact that they were not generated for the model as such. How should I interpret this?
> 
>> r.squaredGLMM(glmmTMB31B)
>                     R2m          R2c
> delta     1.112643e-18 2.247703e-18
> lognormal 4.805332e-04 9.707482e-04
> trigamma  6.896496e-35 1.393194e-34
> Warning messages:
> 1: 'r.squaredGLMM' now calculates a revised statistic. See the help page.
> 2: The null model is correct only if all variables used by the original model remain unchanged.
> 3: In fitTMB(TMBStruc) :
>    Model convergence problem; non-positive-definite Hessian matrix. See vignette('troubleshooting')
> 4: In fitTMB(TMBStruc) :
>    Model convergence problem; false convergence (8). See vignette('troubleshooting')
> 
> 
> I assume that I either make a fundamental mistake somewhere, or the data are "problematic". Could I solve all this by abandoning family = nbinom2 and go back to family = poisson (which worked fine technically)? Or test yet another package?
> Thanks in advance for any suggestions.
> 
> Cheers,
> Adriaan "Adjan" de Jong
> Senior researcher
> Swedish University of Agricultural Sciences
> 
> Data:
> Site                Year              Status            Areal             Arter
> Nyland          2002              Before           33.6               54
> Nyland          2003              Construction 33.6               51
> Nyland          2004              Construction 33.6               56
> Nyland          2005              Construction 33.6               52
> Nyland          2006              Ready           33.6               48
> Nyland          2007              Ready           33.6               43
> Nyland          2008              Ready           33.6               64
> Nyland          2009              Ready           33.6               58
> Nyland          2010              Ready           33.6               57
> Nyland          2013              Traffic           33.6               50
> Nyland          2014              Traffic           33.6               53
> Nyland          2015              Traffic           33.6               61
> Kornsjo          2002              Before           82.8               59
> Kornsjo          2003              Before           82.8               66
> Kornsjo          2004              Construction 82.8               62
> Kornsjo          2005              Construction 82.8               58
> Kornsjo          2006              Ready           82.8               60
> Kornsjo          2007              Ready           82.8               68
> Kornsjo          2008              Ready           82.8               70
> Kornsjo          2009              Ready           82.8               59
> Kornsjo          2010              Ready           82.8               62
> Kornsjo          2013              Traffic           82.8               57
> Kornsjo          2014              Traffic           82.8               58
> Kornsjo          2015              Traffic           82.8               63
> Stranne         2002              Construction 37.0               53
> Stranne         2003              Construction 37.0               64
> Stranne         2004              Ready           37.0               60
> Stranne         2005              Ready           37.0               60
> Stranne         2006              Ready           37.0               59
> Stranne         2007              Ready           37.0               60
> Stranne         2008              Ready           37.0               55
> Stranne         2009              Ready           37.0               53
> Stranne         2010              Ready           37.0               46
> Stranne         2013              Traffic           37.0               58
> Stranne         2014              Traffic           37.0               53
> Stranne         2015              Traffic           37.0               56
> Strandnyland 2002              Construction 69.2               43
> Strandnyland 2003              Construction 69.2               44
> Strandnyland 2004              Ready           69.2               52
> Strandnyland 2005              Ready           69.2               56
> Strandnyland 2006              Ready           69.2               46
> Strandnyland 2007              Ready           69.2               52
> Strandnyland 2008              Ready           69.2               48
> Strandnyland 2009              Ready           69.2               43
> Strandnyland 2010              Ready           69.2               55
> Strandnyland 2013              Traffic           69.2               38
> Strandnyland 2014              Traffic           69.2               41
> Strandnyland 2015              Traffic           69.2               46
> Hjalta             2002              Ready           91.1               61
> Hjalta             2003              Ready           91.1               61
> Hjalta             2004              Ready           91.1               62
> Hjalta             2005              Ready           91.1               61
> Hjalta             2006              Ready           91.1               66
> Hjalta             2007              Ready           91.1               71
> Hjalta             2008              Ready           91.1               73
> Hjalta             2009              Ready           91.1               58
> Hjalta             2010              Ready           91.1               64
> Hjalta             2013              Traffic           91.1               67
> Hjalta             2014              Traffic           91.1               71
> Hjalta             2015              Traffic           91.1               67
> Kasa              2002              Construction 98.3               65
> Kasa              2003              Ready           98.3               59
> Kasa              2004              Ready           98.3               59
> Kasa              2005              Ready           98.3               66
> Kasa              2006              Ready           98.3               52
> Kasa              2007              Ready           98.3               53
> Kasa              2008              Ready           98.3               56
> Kasa              2009              Ready           98.3               53
> Kasa              2010              Ready           98.3               51
> Kasa              2013              Traffic           98.3               51
> Kasa              2014              Traffic           98.3               54
> Kasa              2015              Traffic           98.3               64
> Ava               2002              Before           109.5             67
> Ava               2003              Before           109.5             79
> Ava               2004              Construction 109.5             70
> Ava               2005              Construction 109.5             77
> Ava               2006              Construction 109.5             76
> Ava               2007              Ready           109.5             80
> Ava               2008              Ready           109.5             77
> Ava               2009              Ready           109.5             81
> Ava               2010              Ready           109.5             83
> Ava               2013              Traffic           109.5             76
> Ava               2014              Traffic           109.5             81
> Ava               2015              Traffic           109.5             78
> Logdea          2002              Before           67.4               54
> Logdea          2003              Before           67.4               53
> Logdea          2004              Before           67.4               57
> Logdea          2005              Construction 67.4               55
> Logdea          2006              Construction 67.4               49
> Logdea          2007              Ready           67.4               61
> Logdea          2008              Ready           67.4               56
> Logdea          2009              Ready           67.4               56
> Logdea          2010              Ready           67.4               53
> Logdea          2013              Traffic           67.4               55
> Logdea          2014              Traffic           67.4               53
> Logdea          2015              Traffic           67.4               51
> Langed          2002              Before           34.2               49
> Langed          2003              Before           34.2               56
> Langed          2004              Before           34.2               51
> Langed          2005              Before           34.2               52
> Langed          2006              Construction 34.2               47
> Langed          2007              Ready           34.2               56
> Langed          2008              Ready           34.2               54
> Langed          2009              Ready           34.2               51
> Langed          2010              Ready           34.2               54
> Langed          2013              Traffic           34.2               57
> Langed          2014              Traffic           34.2               51
> Langed          2015              Traffic           34.2               50
> Hornea          2002              Before           48.9               48
> Hornea          2003              Before           48.9               49
> Hornea          2004              Before           48.9               41
> Hornea          2005              Before           48.9               37
> Hornea          2006              Construction 48.9               40
> Hornea          2007              Construction 48.9               48
> Hornea          2008              Ready           48.9               48
> Hornea          2009              Ready           48.9               39
> Hornea          2010              Ready           48.9               47
> Hornea          2013              Traffic           48.9               47
> Hornea          2014              Traffic           48.9               43
> Hornea          2015              Traffic           48.9               47
> Stocke          2002              Before           208.4             58
> Stocke          2003              Before           208.4             69
> Stocke          2004              Before           208.4             73
> Stocke          2005              Before           208.4             67
> Stocke          2006              Before           208.4             66
> Stocke          2007              Construction 208.4             71
> Stocke          2008              Construction 208.4             75
> Stocke          2009              Construction 208.4             60
> Stocke          2010              Ready           208.4             68
> Stocke          2013              Traffic           208.4             73
> Stocke          2014              Traffic           208.4             67
> Stocke          2015              Traffic           208.4             75
> StockeNE     2002              Before           33.9               34
> StockeNE     2003              Before           33.9               36
> StockeNE     2004              Before           33.9               32
> StockeNE     2005              Before           33.9               28
> StockeNE     2006              Before           33.9               35
> StockeNE     2007              Construction 33.9               43
> StockeNE     2008              Construction 33.9               41
> StockeNE     2009              Construction 33.9               34
> StockeNE     2010              Ready           33.9               34
> StockeNE     2013              Traffic           33.9               33
> StockeNE     2014              Traffic           33.9               32
> StockeNE     2015              Traffic           33.9               37
> Degernas      2002              Before           68.8               56
> Degernas      2003              Before           68.8               56
> Degernas      2004              Before           68.8               66
> Degernas      2005              Before           68.8               64
> Degernas      2006              Before           68.8               58
> Degernas      2007              Before           68.8               64
> Degernas      2008              Construction 68.8               62
> Degernas      2009              Construction 68.8               61
> Degernas      2010              Construction 68.8               68
> Degernas      2013              Traffic           68.8               69
> Degernas      2014              Traffic           68.8               57
> Degernas      2015              Traffic           68.8               70
> 
> ---
> N?r du skickar e-post till SLU s? inneb?r detta att SLU behandlar dina personuppgifter. F?r att l?sa mer om hur detta g?r till, klicka h?r <https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.slu.se%2Fom-slu%2Fkontakta-slu%2Fpersonuppgifter%2F&amp;data=02%7C01%7Cd.rizopoulos%40erasmusmc.nl%7Cb3c2d62a856a4aee7cfc08d6b299c61c%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C0%7C636892770857774922&amp;sdata=VzTwp%2FME7uLLWczLBSqqVH%2BWxKZ6VSLW0OhJl6O49Rg%3D&amp;reserved=0>
> E-mailing SLU will result in SLU processing your personal data. For more information on how this is done, click here <https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.slu.se%2Fen%2Fabout-slu%2Fcontact-slu%2Fpersonal-data%2F&amp;data=02%7C01%7Cd.rizopoulos%40erasmusmc.nl%7Cb3c2d62a856a4aee7cfc08d6b299c61c%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C0%7C636892770857774922&amp;sdata=7EJIaHo321BVUl0k1sUBguSgZaALSxfyy7wKJF%2FGswk%3D&amp;reserved=0>
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-mixed-models&amp;data=02%7C01%7Cd.rizopoulos%40erasmusmc.nl%7Cb3c2d62a856a4aee7cfc08d6b299c61c%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C0%7C636892770857774922&amp;sdata=c2IvA75cdLgurjryP%2BVFdZLNaohfEb6TmQKhz8KlwVk%3D&amp;reserved=0
> .
> 

-- 
Dimitris Rizopoulos
Professor of Biostatistics
Department of Biostatistics
Erasmus University Medical Center

Address: PO Box 2040, 3000 CA Rotterdam, the Netherlands
Tel: +31/(0)10/7043478
Fax: +31/(0)10/7043014
Web (personal): http://www.drizopoulos.com/
Web (work): http://www.erasmusmc.nl/biostatistiek/
Blog: http://iprogn.blogspot.nl/

From chr|@@@ @end|ng |rom med@um|ch@edu  Wed Mar 27 13:36:30 2019
From: chr|@@@ @end|ng |rom med@um|ch@edu (Andrews, Chris)
Date: Wed, 27 Mar 2019 12:36:30 +0000
Subject: [R-sig-ME] 
 Mixed-effects modelling on data from a highly unbalanced design
In-Reply-To: <cc5b85ccf91542f4be330b5ca28f7ac5@Exch2-3.slu.se>
References: <cc5b85ccf91542f4be330b5ca28f7ac5@Exch2-3.slu.se>
Message-ID: <d3732b3808e5409baa836291c6f05757@med.umich.edu>

Given the observed data,

hist(Birds$Arter)
with(ArterYears, interaction.plot(ordered(YearS), Site, Arter)))

I would just use a normal linear mixed model.  Although the data are counts, they are not counts that would generate a Poisson distribution.  And you'll not likely have convergence issues.



-----Original Message-----
From: Adriaan De Jong [mailto:Adriaan.de.Jong at slu.se] 
Sent: Wednesday, March 27, 2019 5:50 AM
To: R-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Mixed-effects modelling on data from a highly unbalanced design

Dear forum,

I'm struggling with a data set from 12 years of breeding bird monitoring on 13 sites that were affected by the construction of a new railway (there are control sites, too, but these fall outside the scope of this question). I have data from (a) before, (b) during construction, (c) "ready" (= railway ready but no traffic) and (d) traffic. Obviously, these four steps came in a fixed sequence. Because the railway was built in separate sections, the numbers of years of Before, Construction and Ready (coded in variable Status) vary among sites in terms of timing and duration, but all sites were monitored for the same three years while traffic was on. In short, the "design" is highly unbalanced.

At this stage, I'm looking at the possible effect of railway construction on avian biodiversity expressed as the total number of species observed per year per site (data below). I've used the following script to prepare the data:

ArterYears$ArterI<-as.integer(ArterYears$Arter) ## Arter = number of observed species
ArterYears$Site<-as.factor(ArterYears$Site)
ArterYears$NArea<-ArterYears$Areal/mean(ArterYears$Areal) ## Areal = area in hectares
ArterYears$NArea2<-ArterYears$NArea^2
ArterYears$YearS<-as.integer(ArterYears$Year - 2001) ## the study was performed 2002-2015
ArterYears$Status<-as.factor(ArterYears$Status)
ArterYears$response<-ArterYears$ArterI

(Some of these steps are not really necessary, but I've included them just to be sure. The same goes for the assignment of NA's to the model names below.)

The data for species number (range 28 - 83) are overdispersed for all sites combined, but not for the individual sites (rather underdispersed). To be on the safe side, I chose to use a negative binomial family anyway: nbinom2 in glmmTMB (I've tested glmm.nb as well with the same kind of problematic results).

>From biological reasoning, the following model is the one I'd prefer. It uses Status, adjusted Year and log("normalized" area) as fixed effects and Site as random effect. For the avian biodiversity data, this model works technically fine. Biologically speaking, the later Status classes did not differ significantly from Before (the intercept). So far so good.

glmmTMB31B<-NA
glmmTMB31B<-glmmTMB(response ~ Status + YearS + log(NArea) + (1|Site), data=ArterYears, family = nbinom2)
summary(glmmTMB31B)
AICc(glmmTMB31B)

Now I would like to evaluate this model, either against other models or with an R2 of some sort. Now problems pile up.

The "null" model (Site needs to be in the model because numbers really vary among sites, see: plot(ArterYears$ArterI ~ ArterYears$Site) )

glmmTMB0<-NA
glmmTMB0<-glmmTMB(response ~ 1 + (1|Site), data=ArterYears, family = nbinom2)
summary(glmmTMB0)
AICc(glmmTMB0)

generates the following warning messages:


1: In fitTMB(TMBStruc) :

  Model convergence problem; non-positive-definite Hessian matrix. See vignette('troubleshooting')

2: In fitTMB(TMBStruc) :

  Model convergence problem; false convergence (8). See vignette('troubleshooting')

Most other models including combinations of YearS, NArea and Status generate these warning messages (in particular the second one). This puzzles me, but unfortunately, I'm not capable of interpreting the troubleshooting messages. If the model without fixed effects had worked fine and the complex model had failed, I would suspect overparameterization (N = 156), but now I have no clue.

When I apply r.squaredGLMM (MuMIn) to the complex model, warning messages come up even here, despite the fact that they were not generated for the model as such. How should I interpret this?

> r.squaredGLMM(glmmTMB31B)
                   R2m          R2c
delta     1.112643e-18 2.247703e-18
lognormal 4.805332e-04 9.707482e-04
trigamma  6.896496e-35 1.393194e-34
Warning messages:
1: 'r.squaredGLMM' now calculates a revised statistic. See the help page.
2: The null model is correct only if all variables used by the original model remain unchanged.
3: In fitTMB(TMBStruc) :
  Model convergence problem; non-positive-definite Hessian matrix. See vignette('troubleshooting')
4: In fitTMB(TMBStruc) :
  Model convergence problem; false convergence (8). See vignette('troubleshooting')


I assume that I either make a fundamental mistake somewhere, or the data are "problematic". Could I solve all this by abandoning family = nbinom2 and go back to family = poisson (which worked fine technically)? Or test yet another package?
Thanks in advance for any suggestions.

Cheers,
Adriaan "Adjan" de Jong
Senior researcher
Swedish University of Agricultural Sciences

Data:
Site                Year              Status            Areal             Arter
Nyland          2002              Before           33.6               54
Nyland          2003              Construction 33.6               51
Nyland          2004              Construction 33.6               56
Nyland          2005              Construction 33.6               52
Nyland          2006              Ready           33.6               48
Nyland          2007              Ready           33.6               43
Nyland          2008              Ready           33.6               64
Nyland          2009              Ready           33.6               58
Nyland          2010              Ready           33.6               57
Nyland          2013              Traffic           33.6               50
Nyland          2014              Traffic           33.6               53
Nyland          2015              Traffic           33.6               61
Kornsjo          2002              Before           82.8               59
Kornsjo          2003              Before           82.8               66
Kornsjo          2004              Construction 82.8               62
Kornsjo          2005              Construction 82.8               58
Kornsjo          2006              Ready           82.8               60
Kornsjo          2007              Ready           82.8               68
Kornsjo          2008              Ready           82.8               70
Kornsjo          2009              Ready           82.8               59
Kornsjo          2010              Ready           82.8               62
Kornsjo          2013              Traffic           82.8               57
Kornsjo          2014              Traffic           82.8               58
Kornsjo          2015              Traffic           82.8               63
Stranne         2002              Construction 37.0               53
Stranne         2003              Construction 37.0               64
Stranne         2004              Ready           37.0               60
Stranne         2005              Ready           37.0               60
Stranne         2006              Ready           37.0               59
Stranne         2007              Ready           37.0               60
Stranne         2008              Ready           37.0               55
Stranne         2009              Ready           37.0               53
Stranne         2010              Ready           37.0               46
Stranne         2013              Traffic           37.0               58
Stranne         2014              Traffic           37.0               53
Stranne         2015              Traffic           37.0               56
Strandnyland 2002              Construction 69.2               43
Strandnyland 2003              Construction 69.2               44
Strandnyland 2004              Ready           69.2               52
Strandnyland 2005              Ready           69.2               56
Strandnyland 2006              Ready           69.2               46
Strandnyland 2007              Ready           69.2               52
Strandnyland 2008              Ready           69.2               48
Strandnyland 2009              Ready           69.2               43
Strandnyland 2010              Ready           69.2               55
Strandnyland 2013              Traffic           69.2               38
Strandnyland 2014              Traffic           69.2               41
Strandnyland 2015              Traffic           69.2               46
Hjalta             2002              Ready           91.1               61
Hjalta             2003              Ready           91.1               61
Hjalta             2004              Ready           91.1               62
Hjalta             2005              Ready           91.1               61
Hjalta             2006              Ready           91.1               66
Hjalta             2007              Ready           91.1               71
Hjalta             2008              Ready           91.1               73
Hjalta             2009              Ready           91.1               58
Hjalta             2010              Ready           91.1               64
Hjalta             2013              Traffic           91.1               67
Hjalta             2014              Traffic           91.1               71
Hjalta             2015              Traffic           91.1               67
Kasa              2002              Construction 98.3               65
Kasa              2003              Ready           98.3               59
Kasa              2004              Ready           98.3               59
Kasa              2005              Ready           98.3               66
Kasa              2006              Ready           98.3               52
Kasa              2007              Ready           98.3               53
Kasa              2008              Ready           98.3               56
Kasa              2009              Ready           98.3               53
Kasa              2010              Ready           98.3               51
Kasa              2013              Traffic           98.3               51
Kasa              2014              Traffic           98.3               54
Kasa              2015              Traffic           98.3               64
Ava               2002              Before           109.5             67
Ava               2003              Before           109.5             79
Ava               2004              Construction 109.5             70
Ava               2005              Construction 109.5             77
Ava               2006              Construction 109.5             76
Ava               2007              Ready           109.5             80
Ava               2008              Ready           109.5             77
Ava               2009              Ready           109.5             81
Ava               2010              Ready           109.5             83
Ava               2013              Traffic           109.5             76
Ava               2014              Traffic           109.5             81
Ava               2015              Traffic           109.5             78
Logdea          2002              Before           67.4               54
Logdea          2003              Before           67.4               53
Logdea          2004              Before           67.4               57
Logdea          2005              Construction 67.4               55
Logdea          2006              Construction 67.4               49
Logdea          2007              Ready           67.4               61
Logdea          2008              Ready           67.4               56
Logdea          2009              Ready           67.4               56
Logdea          2010              Ready           67.4               53
Logdea          2013              Traffic           67.4               55
Logdea          2014              Traffic           67.4               53
Logdea          2015              Traffic           67.4               51
Langed          2002              Before           34.2               49
Langed          2003              Before           34.2               56
Langed          2004              Before           34.2               51
Langed          2005              Before           34.2               52
Langed          2006              Construction 34.2               47
Langed          2007              Ready           34.2               56
Langed          2008              Ready           34.2               54
Langed          2009              Ready           34.2               51
Langed          2010              Ready           34.2               54
Langed          2013              Traffic           34.2               57
Langed          2014              Traffic           34.2               51
Langed          2015              Traffic           34.2               50
Hornea          2002              Before           48.9               48
Hornea          2003              Before           48.9               49
Hornea          2004              Before           48.9               41
Hornea          2005              Before           48.9               37
Hornea          2006              Construction 48.9               40
Hornea          2007              Construction 48.9               48
Hornea          2008              Ready           48.9               48
Hornea          2009              Ready           48.9               39
Hornea          2010              Ready           48.9               47
Hornea          2013              Traffic           48.9               47
Hornea          2014              Traffic           48.9               43
Hornea          2015              Traffic           48.9               47
Stocke          2002              Before           208.4             58
Stocke          2003              Before           208.4             69
Stocke          2004              Before           208.4             73
Stocke          2005              Before           208.4             67
Stocke          2006              Before           208.4             66
Stocke          2007              Construction 208.4             71
Stocke          2008              Construction 208.4             75
Stocke          2009              Construction 208.4             60
Stocke          2010              Ready           208.4             68
Stocke          2013              Traffic           208.4             73
Stocke          2014              Traffic           208.4             67
Stocke          2015              Traffic           208.4             75
StockeNE     2002              Before           33.9               34
StockeNE     2003              Before           33.9               36
StockeNE     2004              Before           33.9               32
StockeNE     2005              Before           33.9               28
StockeNE     2006              Before           33.9               35
StockeNE     2007              Construction 33.9               43
StockeNE     2008              Construction 33.9               41
StockeNE     2009              Construction 33.9               34
StockeNE     2010              Ready           33.9               34
StockeNE     2013              Traffic           33.9               33
StockeNE     2014              Traffic           33.9               32
StockeNE     2015              Traffic           33.9               37
Degernas      2002              Before           68.8               56
Degernas      2003              Before           68.8               56
Degernas      2004              Before           68.8               66
Degernas      2005              Before           68.8               64
Degernas      2006              Before           68.8               58
Degernas      2007              Before           68.8               64
Degernas      2008              Construction 68.8               62
Degernas      2009              Construction 68.8               61
Degernas      2010              Construction 68.8               68
Degernas      2013              Traffic           68.8               69
Degernas      2014              Traffic           68.8               57
Degernas      2015              Traffic           68.8               70

---
N?r du skickar e-post till SLU s? inneb?r detta att SLU behandlar dina personuppgifter. F?r att l?sa mer om hur detta g?r till, klicka h?r <https://www.slu.se/om-slu/kontakta-slu/personuppgifter/>
E-mailing SLU will result in SLU processing your personal data. For more information on how this is done, click here <https://www.slu.se/en/about-slu/contact-slu/personal-data/>

	[[alternative HTML version deleted]]


**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues 

From Adr|@@n@de@Jong @end|ng |rom @|u@@e  Wed Mar 27 13:50:58 2019
From: Adr|@@n@de@Jong @end|ng |rom @|u@@e (Adriaan De Jong)
Date: Wed, 27 Mar 2019 12:50:58 +0000
Subject: [R-sig-ME] 
 Mixed-effects modelling on data from a highly unbalanced design
In-Reply-To: <d3732b3808e5409baa836291c6f05757@med.umich.edu>
References: <cc5b85ccf91542f4be330b5ca28f7ac5@Exch2-3.slu.se>
 <d3732b3808e5409baa836291c6f05757@med.umich.edu>
Message-ID: <7b0dec3f8f744c05925da8b5e2f458a1@Exch2-3.slu.se>

Dear Chris,
Thanks for your suggestion. That would be a lovely easy-way-out-of-my-misery :)
But for a non-professional statistician, how far could I go here? Would the data set below still qualify for "normal" lme's? In case of the Key's?, The Support's? The Vv's? The Mf's? The Sr's?
Is there a "rule" that I can use or a paper I can refer to?
Cheers,
Adjan


Data:
SiteSitenrYearStatusArealVvMfSrKeySupport
Nyl2012002Before33.620020
Nyl2012003Construction33.600000
Nyl2012004Construction33.600000
Nyl2012005Construction33.600000
Nyl2012006Ready33.600101
Nyl2012007Ready33.600000
Nyl2012008Ready33.620020
Nyl2012009Ready33.620121
Nyl2012010Ready33.630030
Nyl2012013Traffic33.610313
Nyl2012014Traffic33.610313
Nyl2012015Traffic33.600505
Korn2022002Before82.81511152
Korn2022003Before82.81400140
Korn2022004Construction82.81901191
Korn2022005Construction82.81311132
Korn2022006Ready82.81610161
Korn2022007Ready82.81413144
Korn2022008Ready82.81710171
Korn2022009Ready82.81001101
Korn2022010Ready82.81301131
Korn2022013Traffic82.81201121
Korn2022014Traffic82.81201121
Korn2022015Traffic82.81001101
Str2032002Construction37.000202
Str2032003Construction37.000202
Str2032004Ready37.000404
Str2032005Ready37.000202
Str2032006Ready37.000101
Str2032007Ready37.000101
Str2032008Ready37.000202
Str2032009Ready37.000202
Str2032010Ready37.000000
Str2032013Traffic37.000202
Str2032014Traffic37.000101
Str2032015Traffic37.010010
StrN2042002Construction69.231233
StrN2042003Construction69.242244
StrN2042004Ready69.273376
StrN2042005Ready69.234337
StrN2042006Ready69.274276
StrN2042007Ready69.2664610
StrN2042008Ready69.234438
StrN2042009Ready69.242345
StrN2042010Ready69.223225
StrN2042013Traffic69.213619
StrN2042014Traffic69.232436
StrN2042015Traffic69.230333
Hjalta2052002Ready91.122123
Hjalta2052003Ready91.112315
Hjalta2052004Ready91.113518
Hjalta2052005Ready91.111314
Hjalta2052006Ready91.121425
Hjalta2052007Ready91.140545
Hjalta2052008Ready91.120323
Hjalta2052009Ready91.130131
Hjalta2052010Ready91.110212
Hjalta2052013Traffic91.110111
Hjalta2052014Traffic91.130333
Hjalta2052015Traffic91.130232
Kasa2062002Construction98.357158
Kasa2062003Ready98.39131914
Kasa2062004Ready98.357057
Kasa2062005Ready98.310851013
Kasa2062006Ready98.376177
Kasa2062007Ready98.396298
Kasa2062008Ready98.31352137
Kasa2062009Ready98.393295
Kasa2062010Ready98.31011102
Kasa2062013Traffic98.31102112
Kasa2062014Traffic98.380383
Kasa2062015Traffic98.31102112
Ava2072002Before109.574276
Ava2072003Before109.553457
Ava2072004Construction109.564569
Ava2072005Construction109.5466412
Ava2072006Construction109.5264210
Ava2072007Ready109.5285213
Ava2072008Ready109.5156111
Ava2072009Ready109.5347311
Ava2072010Ready109.5565511
Ava2072013Traffic109.5537510
Ava2072014Traffic109.554559
Ava2072015Traffic109.552658
Logd2082002Before67.421324
Logd2082003Before67.401203
Logd2082004Before67.402406
Logd2082005Construction67.404408
Logd2082006Construction67.403407
Logd2082007Ready67.412416
Logd2082008Ready67.402406
Logd2082009Ready67.403609
Logd2082010Ready67.4146110
Logd2082013Traffic67.402507
Logd2082014Traffic67.400606
Logd2082015Traffic67.400404
Lang2092002Before34.200000
Lang2092003Before34.200000
Lang2092004Before34.200101
Lang2092005Before34.200101
Lang2092006Construction34.210010
Lang2092007Ready34.200000
Lang2092008Ready34.200000
Lang2092009Ready34.200000
Lang2092010Ready34.200000
Lang2092013Traffic34.200000
Lang2092014Traffic34.200000
Lang2092015Traffic34.210010
Hornea2102002Before48.920222
Hornea2102003Before48.930333
Hornea2102004Before48.910515
Hornea2102005Before48.900404
Hornea2102006Construction48.900303
Hornea2102007Construction48.900404
Hornea2102008Ready48.900303
Hornea2102009Ready48.900202
Hornea2102010Ready48.900101
Hornea2102013Traffic48.900101
Hornea2102014Traffic48.900202
Hornea2102015Traffic48.900000
Stocke2112002Before208.460262
Stocke2112003Before208.480484
Stocke2112004Before208.450353
Stocke2112005Before208.490494
Stocke2112006Before208.41404144
Stocke2112007Construction208.41605165
Stocke2112008Construction208.41001101
Stocke2112009Construction208.470373
Stocke2112010Ready208.41502152
Stocke2112013Traffic208.41302132
Stocke2112014Traffic208.460060
Stocke2112015Traffic208.450353
StNE2122002Before33.900000
StNE2122003Before33.900000
StNE2122004Before33.900000
StNE2122005Before33.900000
StNE2122006Before33.900101
StNE2122007Construction33.910010
StNE2122008Construction33.900000
StNE2122009Construction33.900000
StNE2122010Ready33.900000
StNE2122013Traffic33.900000
StNE2122014Traffic33.900000
StNE2122015Traffic33.900000
Deg2132002Before68.860161
Deg2132003Before68.850050
Deg2132004Before68.81201121
Deg2132005Before68.890292
Deg2132006Before68.81102112
Deg2132007Before68.81301131
Deg2132008Construction68.81022104
Deg2132009Construction68.81520152
Deg2132010Construction68.81022104
Deg2132013Traffic68.81152117
Deg2132014Traffic68.81145119
Deg2132015Traffic68.885388


-----Original Message-----
From: Andrews, Chris <chrisaa at med.umich.edu>
Sent: den 27 mars 2019 13:37
To: Adriaan De Jong <Adriaan.de.Jong at slu.se>; R-sig-mixed-models at r-project.org
Subject: RE: [R-sig-ME] Mixed-effects modelling on data from a highly unbalanced design

Given the observed data,

hist(Birds$Arter)
with(ArterYears, interaction.plot(ordered(YearS), Site, Arter)))

I would just use a normal linear mixed model.  Although the data are counts, they are not counts that would generate a Poisson distribution.  And you'll not likely have convergence issues.



-----Original Message-----
From: Adriaan De Jong [mailto:Adriaan.de.Jong at slu.se]
Sent: Wednesday, March 27, 2019 5:50 AM
To: R-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Mixed-effects modelling on data from a highly unbalanced design

Dear forum,

I'm struggling with a data set from 12 years of breeding bird monitoring on 13 sites that were affected by the construction of a new railway (there are control sites, too, but these fall outside the scope of this question). I have data from (a) before, (b) during construction, (c) "ready" (= railway ready but no traffic) and (d) traffic. Obviously, these four steps came in a fixed sequence. Because the railway was built in separate sections, the numbers of years of Before, Construction and Ready (coded in variable Status) vary among sites in terms of timing and duration, but all sites were monitored for the same three years while traffic was on. In short, the "design" is highly unbalanced.

At this stage, I'm looking at the possible effect of railway construction on avian biodiversity expressed as the total number of species observed per year per site (data below). I've used the following script to prepare the data:

ArterYears$ArterI<-as.integer(ArterYears$Arter) ## Arter = number of observed species
ArterYears$Site<-as.factor(ArterYears$Site)
ArterYears$NArea<-ArterYears$Areal/mean(ArterYears$Areal) ## Areal = area in hectares
ArterYears$NArea2<-ArterYears$NArea^2
ArterYears$YearS<-as.integer(ArterYears$Year - 2001) ## the study was performed 2002-2015
ArterYears$Status<-as.factor(ArterYears$Status)
ArterYears$response<-ArterYears$ArterI

(Some of these steps are not really necessary, but I've included them just to be sure. The same goes for the assignment of NA's to the model names below.)

The data for species number (range 28 - 83) are overdispersed for all sites combined, but not for the individual sites (rather underdispersed). To be on the safe side, I chose to use a negative binomial family anyway: nbinom2 in glmmTMB (I've tested glmm.nb as well with the same kind of problematic results).

>From biological reasoning, the following model is the one I'd prefer. It uses Status, adjusted Year and log("normalized" area) as fixed effects and Site as random effect. For the avian biodiversity data, this model works technically fine. Biologically speaking, the later Status classes did not differ significantly from Before (the intercept). So far so good.

glmmTMB31B<-NA
glmmTMB31B<-glmmTMB(response ~ Status + YearS + log(NArea) + (1|Site), data=ArterYears, family = nbinom2)
summary(glmmTMB31B)
AICc(glmmTMB31B)

Now I would like to evaluate this model, either against other models or with an R2 of some sort. Now problems pile up.

The "null" model (Site needs to be in the model because numbers really vary among sites, see: plot(ArterYears$ArterI ~ ArterYears$Site) )

glmmTMB0<-NA
glmmTMB0<-glmmTMB(response ~ 1 + (1|Site), data=ArterYears, family = nbinom2)
summary(glmmTMB0)
AICc(glmmTMB0)

generates the following warning messages:


1: In fitTMB(TMBStruc) :

  Model convergence problem; non-positive-definite Hessian matrix. See vignette('troubleshooting')

2: In fitTMB(TMBStruc) :

  Model convergence problem; false convergence (8). See vignette('troubleshooting')

Most other models including combinations of YearS, NArea and Status generate these warning messages (in particular the second one). This puzzles me, but unfortunately, I'm not capable of interpreting the troubleshooting messages. If the model without fixed effects had worked fine and the complex model had failed, I would suspect overparameterization (N = 156), but now I have no clue.

When I apply r.squaredGLMM (MuMIn) to the complex model, warning messages come up even here, despite the fact that they were not generated for the model as such. How should I interpret this?

> r.squaredGLMM(glmmTMB31B)
                   R2m          R2c
delta     1.112643e-18 2.247703e-18
lognormal 4.805332e-04 9.707482e-04
trigamma  6.896496e-35 1.393194e-34
Warning messages:
1: 'r.squaredGLMM' now calculates a revised statistic. See the help page.
2: The null model is correct only if all variables used by the original model remain unchanged.
3: In fitTMB(TMBStruc) :
  Model convergence problem; non-positive-definite Hessian matrix. See vignette('troubleshooting')
4: In fitTMB(TMBStruc) :
  Model convergence problem; false convergence (8). See vignette('troubleshooting')


I assume that I either make a fundamental mistake somewhere, or the data are "problematic". Could I solve all this by abandoning family = nbinom2 and go back to family = poisson (which worked fine technically)? Or test yet another package?
Thanks in advance for any suggestions.

Cheers,
Adriaan "Adjan" de Jong
Senior researcher
Swedish University of Agricultural Sciences

Data:
Site                Year              Status            Areal             Arter
Nyland          2002              Before           33.6               54
Nyland          2003              Construction 33.6               51
Nyland          2004              Construction 33.6               56
Nyland          2005              Construction 33.6               52
Nyland          2006              Ready           33.6               48
Nyland          2007              Ready           33.6               43
Nyland          2008              Ready           33.6               64
Nyland          2009              Ready           33.6               58
Nyland          2010              Ready           33.6               57
Nyland          2013              Traffic           33.6               50
Nyland          2014              Traffic           33.6               53
Nyland          2015              Traffic           33.6               61
Kornsjo          2002              Before           82.8               59
Kornsjo          2003              Before           82.8               66
Kornsjo          2004              Construction 82.8               62
Kornsjo          2005              Construction 82.8               58
Kornsjo          2006              Ready           82.8               60
Kornsjo          2007              Ready           82.8               68
Kornsjo          2008              Ready           82.8               70
Kornsjo          2009              Ready           82.8               59
Kornsjo          2010              Ready           82.8               62
Kornsjo          2013              Traffic           82.8               57
Kornsjo          2014              Traffic           82.8               58
Kornsjo          2015              Traffic           82.8               63
Stranne         2002              Construction 37.0               53
Stranne         2003              Construction 37.0               64
Stranne         2004              Ready           37.0               60
Stranne         2005              Ready           37.0               60
Stranne         2006              Ready           37.0               59
Stranne         2007              Ready           37.0               60
Stranne         2008              Ready           37.0               55
Stranne         2009              Ready           37.0               53
Stranne         2010              Ready           37.0               46
Stranne         2013              Traffic           37.0               58
Stranne         2014              Traffic           37.0               53
Stranne         2015              Traffic           37.0               56
Strandnyland 2002              Construction 69.2               43
Strandnyland 2003              Construction 69.2               44
Strandnyland 2004              Ready           69.2               52
Strandnyland 2005              Ready           69.2               56
Strandnyland 2006              Ready           69.2               46
Strandnyland 2007              Ready           69.2               52
Strandnyland 2008              Ready           69.2               48
Strandnyland 2009              Ready           69.2               43
Strandnyland 2010              Ready           69.2               55
Strandnyland 2013              Traffic           69.2               38
Strandnyland 2014              Traffic           69.2               41
Strandnyland 2015              Traffic           69.2               46
Hjalta             2002              Ready           91.1               61
Hjalta             2003              Ready           91.1               61
Hjalta             2004              Ready           91.1               62
Hjalta             2005              Ready           91.1               61
Hjalta             2006              Ready           91.1               66
Hjalta             2007              Ready           91.1               71
Hjalta             2008              Ready           91.1               73
Hjalta             2009              Ready           91.1               58
Hjalta             2010              Ready           91.1               64
Hjalta             2013              Traffic           91.1               67
Hjalta             2014              Traffic           91.1               71
Hjalta             2015              Traffic           91.1               67
Kasa              2002              Construction 98.3               65
Kasa              2003              Ready           98.3               59
Kasa              2004              Ready           98.3               59
Kasa              2005              Ready           98.3               66
Kasa              2006              Ready           98.3               52
Kasa              2007              Ready           98.3               53
Kasa              2008              Ready           98.3               56
Kasa              2009              Ready           98.3               53
Kasa              2010              Ready           98.3               51
Kasa              2013              Traffic           98.3               51
Kasa              2014              Traffic           98.3               54
Kasa              2015              Traffic           98.3               64
Ava               2002              Before           109.5             67
Ava               2003              Before           109.5             79
Ava               2004              Construction 109.5             70
Ava               2005              Construction 109.5             77
Ava               2006              Construction 109.5             76
Ava               2007              Ready           109.5             80
Ava               2008              Ready           109.5             77
Ava               2009              Ready           109.5             81
Ava               2010              Ready           109.5             83
Ava               2013              Traffic           109.5             76
Ava               2014              Traffic           109.5             81
Ava               2015              Traffic           109.5             78
Logdea          2002              Before           67.4               54
Logdea          2003              Before           67.4               53
Logdea          2004              Before           67.4               57
Logdea          2005              Construction 67.4               55
Logdea          2006              Construction 67.4               49
Logdea          2007              Ready           67.4               61
Logdea          2008              Ready           67.4               56
Logdea          2009              Ready           67.4               56
Logdea          2010              Ready           67.4               53
Logdea          2013              Traffic           67.4               55
Logdea          2014              Traffic           67.4               53
Logdea          2015              Traffic           67.4               51
Langed          2002              Before           34.2               49
Langed          2003              Before           34.2               56
Langed          2004              Before           34.2               51
Langed          2005              Before           34.2               52
Langed          2006              Construction 34.2               47
Langed          2007              Ready           34.2               56
Langed          2008              Ready           34.2               54
Langed          2009              Ready           34.2               51
Langed          2010              Ready           34.2               54
Langed          2013              Traffic           34.2               57
Langed          2014              Traffic           34.2               51
Langed          2015              Traffic           34.2               50
Hornea          2002              Before           48.9               48
Hornea          2003              Before           48.9               49
Hornea          2004              Before           48.9               41
Hornea          2005              Before           48.9               37
Hornea          2006              Construction 48.9               40
Hornea          2007              Construction 48.9               48
Hornea          2008              Ready           48.9               48
Hornea          2009              Ready           48.9               39
Hornea          2010              Ready           48.9               47
Hornea          2013              Traffic           48.9               47
Hornea          2014              Traffic           48.9               43
Hornea          2015              Traffic           48.9               47
Stocke          2002              Before           208.4             58
Stocke          2003              Before           208.4             69
Stocke          2004              Before           208.4             73
Stocke          2005              Before           208.4             67
Stocke          2006              Before           208.4             66
Stocke          2007              Construction 208.4             71
Stocke          2008              Construction 208.4             75
Stocke          2009              Construction 208.4             60
Stocke          2010              Ready           208.4             68
Stocke          2013              Traffic           208.4             73
Stocke          2014              Traffic           208.4             67
Stocke          2015              Traffic           208.4             75
StockeNE     2002              Before           33.9               34
StockeNE     2003              Before           33.9               36
StockeNE     2004              Before           33.9               32
StockeNE     2005              Before           33.9               28
StockeNE     2006              Before           33.9               35
StockeNE     2007              Construction 33.9               43
StockeNE     2008              Construction 33.9               41
StockeNE     2009              Construction 33.9               34
StockeNE     2010              Ready           33.9               34
StockeNE     2013              Traffic           33.9               33
StockeNE     2014              Traffic           33.9               32
StockeNE     2015              Traffic           33.9               37
Degernas      2002              Before           68.8               56
Degernas      2003              Before           68.8               56
Degernas      2004              Before           68.8               66
Degernas      2005              Before           68.8               64
Degernas      2006              Before           68.8               58
Degernas      2007              Before           68.8               64
Degernas      2008              Construction 68.8               62
Degernas      2009              Construction 68.8               61
Degernas      2010              Construction 68.8               68
Degernas      2013              Traffic           68.8               69
Degernas      2014              Traffic           68.8               57
Degernas      2015              Traffic           68.8               70

---
N?r du skickar e-post till SLU s? inneb?r detta att SLU behandlar dina personuppgifter. F?r att l?sa mer om hur detta g?r till, klicka h?r <https://www.slu.se/om-slu/kontakta-slu/personuppgifter/>
E-mailing SLU will result in SLU processing your personal data. For more information on how this is done, click here <https://www.slu.se/en/about-slu/contact-slu/personal-data/>

[[alternative HTML version deleted]]


**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues
---
N?r du skickar e-post till SLU s? inneb?r detta att SLU behandlar dina personuppgifter. F?r att l?sa mer om hur detta g?r till, klicka h?r <https://www.slu.se/om-slu/kontakta-slu/personuppgifter/>
E-mailing SLU will result in SLU processing your personal data. For more information on how this is done, click here <https://www.slu.se/en/about-slu/contact-slu/personal-data/>

From p@r|@m @end|ng |rom @t@n|ord@edu  Wed Mar 27 16:42:06 2019
From: p@r|@m @end|ng |rom @t@n|ord@edu (Pardis Miri)
Date: Wed, 27 Mar 2019 15:42:06 +0000
Subject: [R-sig-ME] Is multiple-hypothesis-testing correction needed when
 the goal is decision making?
Message-ID: <CA+WU=HCPQgu+oNrpTn_pVoDqoxzMBBkHSLtc+AB31t5FZCqn8A@mail.gmail.com>

Dear forum,

Based on frequentist approach.
Suppose we have an IV of body site with two levels a and b. We have three DVs dv1, dv2, and dv3.  For example, how well the participant attends to the vibrations, how well the participant differentiate the vibrations, and how well the participant synchronizes their breathing with the vibrations.
We have three hypotheses: a > b for dv1, dv2, and dv3. We run a mixed model for each DV. Each results in p-values p1, p2, and p3. We know that if we want to say that all three hypotheses are true, we multiply each p-value by 3 (eg, 3*p1 ,3*p2, and 3*p3) and test if each is less than 0.05.

However, I am instead looking for some evidence - a recommendation - that site a is better than site b. In this case, can we simply say yes if at least one of the p-values is < 0.05? That is, if p1 < 0.05 but p2 and p3 are both > 0.05, we can conclude that the dv1 hypothesis shows evidence, but the two other hypotheses are inconclusive.

Thank you all!
P


	[[alternative HTML version deleted]]


From g@@d|o @end|ng |rom po@t@bgu@@c@||  Wed Mar 27 17:21:14 2019
From: g@@d|o @end|ng |rom po@t@bgu@@c@|| (Mario Garrido)
Date: Wed, 27 Mar 2019 17:21:14 +0100
Subject: [R-sig-ME] "confidence intervals" and "nominal levels"
Message-ID: <CAHzBVpKEozqwpC_DvvvzADU2cd0PbmpU455vQ4L4AGHqU74+XQ@mail.gmail.com>

Hi,
a brief and probably too simple question.
What is mean that the "confidence intervals" is below the "nominal levels"?
Thanks

-- 
Mario Garrido Escudero, PhD
Dr. Hadas Hawlena Lab
Mitrani Department of Desert Ecology
Jacob Blaustein Institutes for Desert Research
Ben-Gurion University of the Negev
Midreshet Ben-Gurion 84990 ISRAEL

gaiarrido at gmail.com; gaadio at post.bgu.ac.il
phone: (+972) 08-659-6854

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Wed Mar 27 17:44:24 2019
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 27 Mar 2019 12:44:24 -0400
Subject: [R-sig-ME] "confidence intervals" and "nominal levels"
In-Reply-To: <CAHzBVpKEozqwpC_DvvvzADU2cd0PbmpU455vQ4L4AGHqU74+XQ@mail.gmail.com>
References: <CAHzBVpKEozqwpC_DvvvzADU2cd0PbmpU455vQ4L4AGHqU74+XQ@mail.gmail.com>
Message-ID: <7c00049e-c86b-c144-53b4-aaa49ad59c59@gmail.com>


  Context?

  I'm assume that some algorithm or piece of software is returning an
interval that claims to be a frequentist confidence interval (CI), which
by definition means that x% of the time (people usually quote 95% CIs,
but other values are possible) in a long run of repeated
experiments/observations the CIs would contain the true value of the
parameter.

  So "confidence intervals below the nominal levels" probably means that
the CIs from the algorithm/software are *undercovering* -- they contain
the true value *less* than the nominal percentage (x%) of the time, or
equivalently they are narrower (more optimistic) than they should be.

 Typically in order to know something about the coverage (i.e. the
fraction of repeated trials that *actually* continue the true value) one
has to do a simulation study (it's not impossible to derive such things
analytically, but it's usually impractical except in very simple cases).

  cheers
    Ben Bolker

On 2019-03-27 12:21 p.m., Mario Garrido wrote:
> Hi,
> a brief and probably too simple question.
> What is mean that the "confidence intervals" is below the "nominal levels"?
> Thanks
>


From b|mono@om @end|ng |rom gm@||@com  Wed Mar 27 17:46:07 2019
From: b|mono@om @end|ng |rom gm@||@com (=?UTF-8?B?UmVuw6k=?=)
Date: Wed, 27 Mar 2019 17:46:07 +0100
Subject: [R-sig-ME] 
 Is multiple-hypothesis-testing correction needed when
 the goal is decision making?
In-Reply-To: <CA+WU=HCPQgu+oNrpTn_pVoDqoxzMBBkHSLtc+AB31t5FZCqn8A@mail.gmail.com>
References: <CA+WU=HCPQgu+oNrpTn_pVoDqoxzMBBkHSLtc+AB31t5FZCqn8A@mail.gmail.com>
Message-ID: <CADcpBHOPP=RarffDjtLfKOKMzjCLAPzgFWnuRusAjeeAmr_wNw@mail.gmail.com>

Hi P,
there are several different aspects in your question. Most importantly,
your question is not a frequentist question, but a Bayesian question :))

1. if you are asking for "evidence" you are basically asking for a Bayes
Factor (evidence for a hypothesis given some data), but not a frequentist
p-value (probability of data given a hypothesis). Also,a p-value does not
confirm -a hypothesis- ( or in the most generous frequentist interpretation
does not provide evidence -for- something), but (always and only) falsifies
the Null hypothesis (e.g. if p<.05 then we decide as if the Null hypothesis
is false -> falsified; this does not tell us whether the alternative
hypothesis is true). Thus, if you want to use p-values, you indeed could
say if p2 and p3 >.05, the corresponding Null Hypotheses have not been
falsified, but it was falsified for p1. Thus, there is 1 falsification and
two inconclusive tests (i.e. p>.05 without a priori Power analyses is
inconclusive). And if we take the idea of "scientific laws" seriously, then
a falsification (-> p1 Null hypothesis) means that the "law" (-> a = b) is
not a "law", because laws do not have exceptions, therefore a is not equal
to b. Thus, you can decide as if a is unequal to b. Please note, this would
be ideal and valid, but very few scientists really act like this, because
they want answers to the non-frequencist questions like yours... But if we
are doing this right now, then, if you now want to generalize this
conclusion across different dependent variables, you may well earn some
criticism for this, because, you falsified a 'law' for "attention to
vibrations", not for (e.g.) "differentiating vibrations". Nonetheless, you
could do so, which simply means, that you implicitly make the argument that
"attention to vibrations" necessarily (but not sufficiently) precedes
"differentiating vibrations" (...), same for synchronization. This
reasoning could go on... which means. If you want to make a sound statement
about whether a>b (for all or some of the DVs) do it Bayesian.

2. So, if you are asking for "evidence" that a > b then you can quantify
this, indeed, with a Bayes Factor (apart from any p-value), if you estimate
the means of a and b separately (and in a proper way; check out the brms
package in R, or maybe anovaBF with random variables should do as well,
depending on the complexity of your design). Get the posterior samples for
a and b from the model output; then subtract the posterior samples of 'b'
from those of 'a' (i.e. a values >0 means that the estimates are a>b;
subtraction has to be made for each iteration of the sampling), then you
have a number of b-a differences, and you count how often it this is that
'b-a>0' and divide it by the count of how often this is that 'b-a< 0', and
this ratio is the evidence that a>b, versus b<a. (Note this is not the same
as testing "a-b=0", which uses a different method). You can not get closer
to the answer of your question :))

Hope this helps
Oh... almost forgot:
3. You could do this in a multivariate mixed model (i.e. testing a>b for
all three DV's simultaneously, which is possible with the brms package;
then following the steps above, or simply use the hypothesis() package
afterwards which does this for you ;)).

Best, Ren?


Am Mi., 27. M?rz 2019 um 16:42 Uhr schrieb Pardis Miri <parism at stanford.edu
>:

> Dear forum,
>
> Based on frequentist approach.
> Suppose we have an IV of body site with two levels a and b. We have three
> DVs dv1, dv2, and dv3.  For example, how well the participant attends to
> the vibrations, how well the participant differentiate the vibrations, and
> how well the participant synchronizes their breathing with the vibrations.
> We have three hypotheses: a > b for dv1, dv2, and dv3. We run a mixed
> model for each DV. Each results in p-values p1, p2, and p3. We know that if
> we want to say that all three hypotheses are true, we multiply each p-value
> by 3 (eg, 3*p1 ,3*p2, and 3*p3) and test if each is less than 0.05.
>
> However, I am instead looking for some evidence - a recommendation - that
> site a is better than site b. In this case, can we simply say yes if at
> least one of the p-values is < 0.05? That is, if p1 < 0.05 but p2 and p3
> are both > 0.05, we can conclude that the dv1 hypothesis shows evidence,
> but the two other hypotheses are inconclusive.
>
> Thank you all!
> P
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From oreg @end|ng |rom huj|@@c@||  Mon Mar 25 07:16:06 2019
From: oreg @end|ng |rom huj|@@c@|| (Shaul Oreg)
Date: Mon, 25 Mar 2019 08:16:06 +0200
Subject: [R-sig-ME] multi-level moderated mediation
In-Reply-To: <268119BD809ADB44968F39A7A86398FA01723C99C8@Pegasus2.hustaff.huji.local>
References: <CANgtsbhHHcZwnmxF-sLH2-g6J_RZp_4sTLt4zKE676cL7_Z5ug@mail.gmail.com>
 <CANgtsbj-L+iDgDrebyZr=kyYqY_qTa1+etajYMyAT0kV4MSAHg@mail.gmail.com>
 <268119BD809ADB44968F39A7A86398FA01723C99C8@Pegasus2.hustaff.huji.local>
Message-ID: <8e835240009e9a9bb8fb7409b73515c2@mail.gmail.com>

Hello,

I am trying to run a multilevel moderated mediation model in R, with data
nested in three levels (children, within classes, within schools). All of
my variables are at the individual level, but I still need to account for
the nested nature of the data.

In separate analyses of mediation and of moderation I find evidence for
indirect effects of the predictor on the outcome, and evidence that my
moderator moderates the effects of the predictor on the mediator. But I?d
also like to test if the moderator *moderates the indirect effect of the
predictor on the outcome*.



Thanks,



Shaul Oreg

	[[alternative HTML version deleted]]


From g@@d|o @end|ng |rom po@t@bgu@@c@||  Thu Mar 28 08:38:41 2019
From: g@@d|o @end|ng |rom po@t@bgu@@c@|| (Mario Garrido)
Date: Thu, 28 Mar 2019 08:38:41 +0100
Subject: [R-sig-ME] "confidence intervals" and "nominal levels"
In-Reply-To: <7c00049e-c86b-c144-53b4-aaa49ad59c59@gmail.com>
References: <CAHzBVpKEozqwpC_DvvvzADU2cd0PbmpU455vQ4L4AGHqU74+XQ@mail.gmail.com>
 <7c00049e-c86b-c144-53b4-aaa49ad59c59@gmail.com>
Message-ID: <CAHzBVp+U3q+WOha3AzCXay-PC+X3bByC3047sySxcZPovHwYxg@mail.gmail.com>

Thanks Ben for your answer,
I have read it in the famous Burnham & Anderson?s 2002 "*Model Selection
and Multimodel Inference*".
I thought it means that the CI doesnt includes zero ar something similar

Bests,
Mario

El mi?., 27 mar. 2019 a las 17:44, Ben Bolker (<bbolker at gmail.com>)
escribi?:

>
>   Context?
>
>   I'm assume that some algorithm or piece of software is returning an
> interval that claims to be a frequentist confidence interval (CI), which
> by definition means that x% of the time (people usually quote 95% CIs,
> but other values are possible) in a long run of repeated
> experiments/observations the CIs would contain the true value of the
> parameter.
>
>   So "confidence intervals below the nominal levels" probably means that
> the CIs from the algorithm/software are *undercovering* -- they contain
> the true value *less* than the nominal percentage (x%) of the time, or
> equivalently they are narrower (more optimistic) than they should be.
>
>  Typically in order to know something about the coverage (i.e. the
> fraction of repeated trials that *actually* continue the true value) one
> has to do a simulation study (it's not impossible to derive such things
> analytically, but it's usually impractical except in very simple cases).
>
>   cheers
>     Ben Bolker
>
> On 2019-03-27 12:21 p.m., Mario Garrido wrote:
> > Hi,
> > a brief and probably too simple question.
> > What is mean that the "confidence intervals" is below the "nominal
> levels"?
> > Thanks
> >
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


-- 
Mario Garrido Escudero, PhD
Dr. Hadas Hawlena Lab
Mitrani Department of Desert Ecology
Jacob Blaustein Institutes for Desert Research
Ben-Gurion University of the Negev
Midreshet Ben-Gurion 84990 ISRAEL

gaiarrido at gmail.com; gaadio at post.bgu.ac.il
phone: (+972) 08-659-6854

	[[alternative HTML version deleted]]


From h@@y|m73 @end|ng |rom out|ook@com  Thu Mar 28 15:07:07 2019
From: h@@y|m73 @end|ng |rom out|ook@com (hasyim hasyim)
Date: Thu, 28 Mar 2019 14:07:07 +0000
Subject: [R-sig-ME] The criteria of MLRA parameter
Message-ID: <VI1P192MB0351A3EAC93D1194771555FFDF590@VI1P192MB0351.EURP192.PROD.OUTLOOK.COM>

Hi everyone,

Could you please explain this in detail to me the criteria of each parameter below that use in multilevel regression logistic analysis (MLRA) such as:

the Median Odds Ratio (MOR),
80% Interval Odds Ratio (80% IOR),
Proportion of Odds Ratios of opposite direction (POOR),
Intraclass Correlation Coefficient (ICC),
the Proportional Change of the Variance (PCV), and
Area under the receiver operating characteristic curve (AU-ROC)


Thank you, I really appreciate your help.


Kind regards


Hasyim

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Thu Mar 28 15:47:06 2019
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Thu, 28 Mar 2019 15:47:06 +0100
Subject: [R-sig-ME] The criteria of MLRA parameter
In-Reply-To: <VI1P192MB0351A3EAC93D1194771555FFDF590@VI1P192MB0351.EURP192.PROD.OUTLOOK.COM>
References: <VI1P192MB0351A3EAC93D1194771555FFDF590@VI1P192MB0351.EURP192.PROD.OUTLOOK.COM>
Message-ID: <CAJuCY5zjz7fJkYQpKsv9Rdwu9gG4WMbiDOF6ebE7Kc5AD-rs8A@mail.gmail.com>

Dear Hasyim,

It is explained here:
http://lmgtfy.com/?q=Area+under+the+receiver+operating+characteristic+curve

Many universities have buildings named "library". In these buildings you
can find strange objects made of paper. Some people call them "books".
Those how possess the "reading" skill can extract useful information from
these "books".

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op do 28 mrt. 2019 om 15:07 schreef hasyim hasyim <hasyim73 at outlook.com>:

> Hi everyone,
>
> Could you please explain this in detail to me the criteria of each
> parameter below that use in multilevel regression logistic analysis (MLRA)
> such as:
>
> the Median Odds Ratio (MOR),
> 80% Interval Odds Ratio (80% IOR),
> Proportion of Odds Ratios of opposite direction (POOR),
> Intraclass Correlation Coefficient (ICC),
> the Proportional Change of the Variance (PCV), and
> Area under the receiver operating characteristic curve (AU-ROC)
>
>
> Thank you, I really appreciate your help.
>
>
> Kind regards
>
>
> Hasyim
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From r@turner @end|ng |rom @uck|@nd@@c@nz  Thu Mar 28 21:44:41 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Fri, 29 Mar 2019 09:44:41 +1300
Subject: [R-sig-ME] [FORGED] Re:  The criteria of MLRA parameter
In-Reply-To: <CAJuCY5zjz7fJkYQpKsv9Rdwu9gG4WMbiDOF6ebE7Kc5AD-rs8A@mail.gmail.com>
References: <VI1P192MB0351A3EAC93D1194771555FFDF590@VI1P192MB0351.EURP192.PROD.OUTLOOK.COM>
 <CAJuCY5zjz7fJkYQpKsv9Rdwu9gG4WMbiDOF6ebE7Kc5AD-rs8A@mail.gmail.com>
Message-ID: <28bbf76f-0273-7acb-e463-324171ba7df1@auckland.ac.nz>

On 29/03/19 3:47 AM, Thierry Onkelinx via R-sig-mixed-models wrote:

<SNIP>

> Many universities have buildings named "library". In these buildings you
> can find strange objects made of paper. Some people call them "books".
> Those who possess the "reading" skill can extract useful information from
> these "books".

<SNIP>

Fortune nomination!

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu Mar 28 22:09:53 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 28 Mar 2019 14:09:53 -0700
Subject: [R-sig-ME] The criteria of MLRA parameter
In-Reply-To: <VI1P192MB0351A3EAC93D1194771555FFDF590@VI1P192MB0351.EURP192.PROD.OUTLOOK.COM>
References: <VI1P192MB0351A3EAC93D1194771555FFDF590@VI1P192MB0351.EURP192.PROD.OUTLOOK.COM>
Message-ID: <226240B9-C8B5-4147-B4C8-8D7A68BA70BD@dcn.davis.ca.us>

This request is not about R. Therefore it is off-topic on this mailing list. In general, forum participants don't typically enjoy playing the part of a dictionary or textbook, so even referring you to a statistics forum like stats.stackexchange.com would not be doing you or them any favors. Please do some reading and if you have questions about statistical concepts then consider asking those directed questions on stats.stackexchange.com.

On March 28, 2019 7:07:07 AM PDT, hasyim hasyim <hasyim73 at outlook.com> wrote:
>Hi everyone,
>
>Could you please explain this in detail to me the criteria of each
>parameter below that use in multilevel regression logistic analysis
>(MLRA) such as:
>
>the Median Odds Ratio (MOR),
>80% Interval Odds Ratio (80% IOR),
>Proportion of Odds Ratios of opposite direction (POOR),
>Intraclass Correlation Coefficient (ICC),
>the Proportional Change of the Variance (PCV), and
>Area under the receiver operating characteristic curve (AU-ROC)
>
>
>Thank you, I really appreciate your help.
>
>
>Kind regards
>
>
>Hasyim
>
>	[[alternative HTML version deleted]]
>
>_______________________________________________
>R-sig-mixed-models at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Sent from my phone. Please excuse my brevity.


From two||r@m@e|@en@ch @end|ng |rom gm@||@com  Fri Mar 29 14:31:57 2019
From: two||r@m@e|@en@ch @end|ng |rom gm@||@com (Tobias Wolfram)
Date: Fri, 29 Mar 2019 14:31:57 +0100
Subject: [R-sig-ME] Weird behaviour of prediction intervals for sums of
 linear functions of predicted values
Message-ID: <CAF0AdRWDOtbjN6+wmyKKPv_hFZBeSkH66y9wHojGHmSeF+8a8w@mail.gmail.com>

Dear all,


I face a rather strange problem in lme4 where I would very much appreciate
the incredible expertise of the people here. Thanks in advance!


A stated, my case is rather peculiar and the motivation to what I?m doing
hard to explain, but it relates to the concept of Multilevel Regression
with Poststratification (MRP). A short explanation follows but I guess you
can skip the next paragraph it if you don?t care about why I do the
seemingly weird things I?m describing afterwards.


The basic idea of MRP is that you want to get correct (or ?representative?)
estimates of opinion (agree vs. disagree in the simplest case) in a
population using potentially skewed samples. In your sample you have
information on the opinion of interest as well as some demographic
information on each individual (like age, gender, state, etc.) for each
observation. You use the demographic variables as random effects (despite
the perhaps low number of levels) and the binary opinion as the dependent
variable in a mixed model and then predict for all possible combinations of
independent variables (which grows quite fast with the number of
independent variables) the probability of agreement, which you then
multiply with the actual number of people with that set of demographic
features in the population (ground truth which comes from an external
source). You can then aggregate over these cells as you please to get for
example the share of women in each state who agreed or the number of people
over 65 who did not. In general you like to use a full bayesian approach
for this procedure but computational considerations prohibit such a
solution in my case and that?s why I?m using lme4.


So what am I actually doing? I have a large set of categorical
(demographic) variables (most of them with not more than 10 levels) which
might be related to a binary outcome. I perform variable selection using a
kind of sparse group-LASSO to drop some of them completely (if the
coefficients for all categories of the variable are shrunk to 0) and reduce
the number of levels on others (by merging those that were reduced to 0).
By doing so I get a smaller set of categorical predictors which I then use
as random effects in a mixed logit model (utilizing glmer) if they have
more than two levels (else they are excluded as fixed effects, because as
far as I understood Gelman & Hill RE and FE are in fact identical in case
of binary variables). Based on this model I create predictions for all
possible combinations of the independent variables, weigh them by some
value and aggregate them over a selection of the independent variables.


Despite the frequently low number of levels on the random effect-variables
this approach works surprisingly well, at least as long as I am interested
in point estimates. But getting interval estimates without applying rather
expensive bootstrapping techniques is a whole other topic, which leads me
to my question: I use the sim()-function from the arm-package to sample
from the I guess you could call it posterior of each prediction a few
hundred times and aggregate these samples as described to get a rough and
fast impression of the underlying distribution which I can use to estimate
some kind of prediction interval while being aware that I most likely
underestimate the error in my predictions by doing this due to the fact
that the uncertainty of certain parameters is ignored using this method. .


Again this works better then expected but it sometimes causes some weird
idiosyncrasies, especially in combination with the variable selection. As
the LASSO-lambda is determined using cross validation there is always a
small stochastic element in the selection of variables. And that?s the
issue: If a certain level of a predictor which exhibits at most a very weak
relationship to the dependent variable is selected or not sometimes changes
the estimated width of the prediction interval by a very large margin
(200-300%), without influencing the point estimates at all. Something
similar happens using predictInterval() from the merTools-package, so it
isn't just a sim()-related issue.


So does somebody have an idea what might explain this weird behavior and
what to do about it or can point me into directions to further debug it?


Thank you so much!

Tobias

	[[alternative HTML version deleted]]


From b@ch|@w01 @end|ng |rom out|ook@com  Fri Mar 29 15:00:58 2019
From: b@ch|@w01 @end|ng |rom out|ook@com (Jonathan Judge)
Date: Fri, 29 Mar 2019 14:00:58 +0000
Subject: [R-sig-ME] Weird behaviour of prediction intervals for sums of
 linear functions of predicted values
In-Reply-To: <CAF0AdRWDOtbjN6+wmyKKPv_hFZBeSkH66y9wHojGHmSeF+8a8w@mail.gmail.com>
References: <CAF0AdRWDOtbjN6+wmyKKPv_hFZBeSkH66y9wHojGHmSeF+8a8w@mail.gmail.com>
Message-ID: <DM6PR03MB38689125B115D82918C6F109AF5A0@DM6PR03MB3868.namprd03.prod.outlook.com>

Tobias:

Tough to weigh in with detail given the high-level summary, but generally speaking, what you may be seeing is that while merTools and its adapted sim function are often useful, it makes certain assumptions (namely a perfectly normal posterior) that may not always be reasonable to make.  I have found, even with fairly simple binomial models, that the true posterior of various random effects was often far different than what the simulate function expected.

My recommendation is that you slog through at least one run of a full Bayesian estimation with the sparsity-inducing prior you are trying to impose (perhaps using brms for Stan) and see what the ?true? distribution of the random effects seems to be.  If you are mainly worried only about the random effects that have already been selected --- in other words, you think the variable selection part is working just fine --- then INLA can often give just as accurate (and sometimes more accurate) fully-Bayesian estimates ofthe posterior distribution of your random effects, provided you impose a solid exponential / pc.prior on the precision of the random effects, and it will do so at a speed that is not much longer than lme4.

If you are lucky, you?ll find that sim was getting it right and you can stick with that.  If not, then at least you know you need to use something more robust.

Good luck.

Jonathan


From: Tobias Wolfram<mailto:twolfram.eisenach at gmail.com>
Sent: Friday, March 29, 2019 8:32 AM
To: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>
Subject: [R-sig-ME] Weird behaviour of prediction intervals for sums of linear functions of predicted values

Dear all,


I face a rather strange problem in lme4 where I would very much appreciate
the incredible expertise of the people here. Thanks in advance!


A stated, my case is rather peculiar and the motivation to what I?m doing
hard to explain, but it relates to the concept of Multilevel Regression
with Poststratification (MRP). A short explanation follows but I guess you
can skip the next paragraph it if you don?t care about why I do the
seemingly weird things I?m describing afterwards.


The basic idea of MRP is that you want to get correct (or ?representative?)
estimates of opinion (agree vs. disagree in the simplest case) in a
population using potentially skewed samples. In your sample you have
information on the opinion of interest as well as some demographic
information on each individual (like age, gender, state, etc.) for each
observation. You use the demographic variables as random effects (despite
the perhaps low number of levels) and the binary opinion as the dependent
variable in a mixed model and then predict for all possible combinations of
independent variables (which grows quite fast with the number of
independent variables) the probability of agreement, which you then
multiply with the actual number of people with that set of demographic
features in the population (ground truth which comes from an external
source). You can then aggregate over these cells as you please to get for
example the share of women in each state who agreed or the number of people
over 65 who did not. In general you like to use a full bayesian approach
for this procedure but computational considerations prohibit such a
solution in my case and that?s why I?m using lme4.


So what am I actually doing? I have a large set of categorical
(demographic) variables (most of them with not more than 10 levels) which
might be related to a binary outcome. I perform variable selection using a
kind of sparse group-LASSO to drop some of them completely (if the
coefficients for all categories of the variable are shrunk to 0) and reduce
the number of levels on others (by merging those that were reduced to 0).
By doing so I get a smaller set of categorical predictors which I then use
as random effects in a mixed logit model (utilizing glmer) if they have
more than two levels (else they are excluded as fixed effects, because as
far as I understood Gelman & Hill RE and FE are in fact identical in case
of binary variables). Based on this model I create predictions for all
possible combinations of the independent variables, weigh them by some
value and aggregate them over a selection of the independent variables.


Despite the frequently low number of levels on the random effect-variables
this approach works surprisingly well, at least as long as I am interested
in point estimates. But getting interval estimates without applying rather
expensive bootstrapping techniques is a whole other topic, which leads me
to my question: I use the sim()-function from the arm-package to sample
from the I guess you could call it posterior of each prediction a few
hundred times and aggregate these samples as described to get a rough and
fast impression of the underlying distribution which I can use to estimate
some kind of prediction interval while being aware that I most likely
underestimate the error in my predictions by doing this due to the fact
that the uncertainty of certain parameters is ignored using this method. .


Again this works better then expected but it sometimes causes some weird
idiosyncrasies, especially in combination with the variable selection. As
the LASSO-lambda is determined using cross validation there is always a
small stochastic element in the selection of variables. And that?s the
issue: If a certain level of a predictor which exhibits at most a very weak
relationship to the dependent variable is selected or not sometimes changes
the estimated width of the prediction interval by a very large margin
(200-300%), without influencing the point estimates at all. Something
similar happens using predictInterval() from the merTools-package, so it
isn't just a sim()-related issue.


So does somebody have an idea what might explain this weird behavior and
what to do about it or can point me into directions to further debug it?


Thank you so much!

Tobias

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


	[[alternative HTML version deleted]]


From h@@y|m73 @end|ng |rom out|ook@com  Fri Mar 29 15:45:37 2019
From: h@@y|m73 @end|ng |rom out|ook@com (hasyim hasyim)
Date: Fri, 29 Mar 2019 14:45:37 +0000
Subject: [R-sig-ME] The criteria of MLRA parameter
In-Reply-To: <226240B9-C8B5-4147-B4C8-8D7A68BA70BD@dcn.davis.ca.us>
References: <VI1P192MB0351A3EAC93D1194771555FFDF590@VI1P192MB0351.EURP192.PROD.OUTLOOK.COM>
 <226240B9-C8B5-4147-B4C8-8D7A68BA70BD@dcn.davis.ca.us>
Message-ID: <VI1P192MB0351B3F9279F4159AE6F1BEDDF5A0@VI1P192MB0351.EURP192.PROD.OUTLOOK.COM>


Thank you for your kind feedback and gentle reminder concerning regulation of the topic discussion.

I want to ask again, is there a link, for-learning the command of each parameter below using R software?

The Median Odds Ratio (MOR),
80% Interval Odds Ratio (80% IOR),
The proportion of Odds Ratios of opposite direction (POOR),
Intraclass Correlation Coefficient (ICC),
the Proportional Change of the Variance (PCV), and
The area under the receiver operating characteristic curve (AU-ROC)

Thank you very much for offering to help out with this case for improving my knowledge to create the syntax of each parameter use R. I appreciate your willingness to assist.


Kind regards

Hasyim

On Fri, 29 Mar 2019 at 04:10, Jeff Newmiller <jdnewmil at dcn.davis.ca.us<mailto:jdnewmil at dcn.davis.ca.us>> wrote:
This request is not about R. Therefore it is off-topic on this mailing list. In general, forum participants don't typically enjoy playing the part of a dictionary or textbook, so even referring you to a statistics forum like stats.stackexchange.com<http://stats.stackexchange.com> would not be doing you or them any favors. Please do some reading and if you have questions about statistical concepts then consider asking those directed questions on stats.stackexchange.com<http://stats.stackexchange.com>.

On March 28, 2019 7:07:07 AM PDT, hasyim hasyim <hasyim73 at outlook.com<mailto:hasyim73 at outlook.com>> wrote:
>Hi everyone,
>
>Could you please explain this in detail to me the criteria of each
>parameter below that use in multilevel regression logistic analysis
>(MLRA) such as:
>
>the Median Odds Ratio (MOR),
>80% Interval Odds Ratio (80% IOR),
>Proportion of Odds Ratios of opposite direction (POOR),
>Intraclass Correlation Coefficient (ICC),
>the Proportional Change of the Variance (PCV), and
>Area under the receiver operating characteristic curve (AU-ROC)
>
>
>Thank you, I really appreciate your help.
>
>
>Kind regards
>
>
>Hasyim
>
>       [[alternative HTML version deleted]]
>
>_______________________________________________
>R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

--
Sent from my phone. Please excuse my brevity.

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From mo|||eebrook@ @end|ng |rom gm@||@com  Fri Mar 29 15:49:05 2019
From: mo|||eebrook@ @end|ng |rom gm@||@com (Mollie Brooks)
Date: Fri, 29 Mar 2019 15:49:05 +0100
Subject: [R-sig-ME] The criteria of MLRA parameter
In-Reply-To: <VI1P192MB0351B3F9279F4159AE6F1BEDDF5A0@VI1P192MB0351.EURP192.PROD.OUTLOOK.COM>
References: <VI1P192MB0351A3EAC93D1194771555FFDF590@VI1P192MB0351.EURP192.PROD.OUTLOOK.COM>
 <226240B9-C8B5-4147-B4C8-8D7A68BA70BD@dcn.davis.ca.us>
 <VI1P192MB0351B3F9279F4159AE6F1BEDDF5A0@VI1P192MB0351.EURP192.PROD.OUTLOOK.COM>
Message-ID: <FC98D0EB-D4CD-4AD2-ADC9-FFE56AEB7C88@gmail.com>

https://rseek.org <https://rseek.org/> is a useful resource. 

cheers,
Mollie

> On 29Mar 2019, at 15:45, hasyim hasyim <hasyim73 at outlook.com> wrote:
> 
> 
> Thank you for your kind feedback and gentle reminder concerning regulation of the topic discussion.
> 
> I want to ask again, is there a link, for-learning the command of each parameter below using R software?
> 
> The Median Odds Ratio (MOR),
> 80% Interval Odds Ratio (80% IOR),
> The proportion of Odds Ratios of opposite direction (POOR),
> Intraclass Correlation Coefficient (ICC),
> the Proportional Change of the Variance (PCV), and
> The area under the receiver operating characteristic curve (AU-ROC)
> 
> Thank you very much for offering to help out with this case for improving my knowledge to create the syntax of each parameter use R. I appreciate your willingness to assist.
> 
> 
> Kind regards
> 
> Hasyim
> 
> On Fri, 29 Mar 2019 at 04:10, Jeff Newmiller <jdnewmil at dcn.davis.ca.us<mailto:jdnewmil at dcn.davis.ca.us>> wrote:
> This request is not about R. Therefore it is off-topic on this mailing list. In general, forum participants don't typically enjoy playing the part of a dictionary or textbook, so even referring you to a statistics forum like stats.stackexchange.com<http://stats.stackexchange.com> would not be doing you or them any favors. Please do some reading and if you have questions about statistical concepts then consider asking those directed questions on stats.stackexchange.com<http://stats.stackexchange.com>.
> 
> On March 28, 2019 7:07:07 AM PDT, hasyim hasyim <hasyim73 at outlook.com<mailto:hasyim73 at outlook.com>> wrote:
>> Hi everyone,
>> 
>> Could you please explain this in detail to me the criteria of each
>> parameter below that use in multilevel regression logistic analysis
>> (MLRA) such as:
>> 
>> the Median Odds Ratio (MOR),
>> 80% Interval Odds Ratio (80% IOR),
>> Proportion of Odds Ratios of opposite direction (POOR),
>> Intraclass Correlation Coefficient (ICC),
>> the Proportional Change of the Variance (PCV), and
>> Area under the receiver operating characteristic curve (AU-ROC)
>> 
>> 
>> Thank you, I really appreciate your help.
>> 
>> 
>> Kind regards
>> 
>> 
>> Hasyim
>> 
>>      [[alternative HTML version deleted]]
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> --
> Sent from my phone. Please excuse my brevity.
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri Mar 29 17:00:09 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 29 Mar 2019 09:00:09 -0700
Subject: [R-sig-ME] The criteria of MLRA parameter
In-Reply-To: <FC98D0EB-D4CD-4AD2-ADC9-FFE56AEB7C88@gmail.com>
References: <VI1P192MB0351A3EAC93D1194771555FFDF590@VI1P192MB0351.EURP192.PROD.OUTLOOK.COM>
 <226240B9-C8B5-4147-B4C8-8D7A68BA70BD@dcn.davis.ca.us>
 <VI1P192MB0351B3F9279F4159AE6F1BEDDF5A0@VI1P192MB0351.EURP192.PROD.OUTLOOK.COM>
 <FC98D0EB-D4CD-4AD2-ADC9-FFE56AEB7C88@gmail.com>
Message-ID: <6E1D8DA1-864F-418C-B731-7138EFF283E9@dcn.davis.ca.us>

To expand on Mollie's suggestion... R only "contains" direct support for a very small fraction of the subjects that people use it to analyze... which is why there is a distinction between "base" and "contributed" downloads from CRAN. Thus, as mentioned before, you need to make sure you are asking the right people the right questions.

I for one have no idea what MLRA is, though rseek suggests that it might have something to do with land use? If that is the case then you might read about various packages mentioned under the Spatial Task View [1], and consider pursuing further questions in the R-sig-geo mailing list [2]. If not, then you may find more subject-specific information in other Task Views [3], and relevant package documentation will often mention appropriate forums for discussing their topics.

You should be careful not to narrow your search too much, as I would guess that some of the terms you have identified (e.g. ROC) are addressed in much more generic ways (whether by contributed or base functionality) than under your particular use case... but this is your research, not ours.

When you have a question about R rather than statistics (or land use) then read the Posting Guide and  have an R code example with sample data to discuss when you ask here about it.

[1] https://cran.r-project.org/web/views/Spatial.html
[2] https://stat.ethz.ch/mailman/listinfo/r-sig-geo
[3] https://cran.r-project.org/web/views/

On March 29, 2019 7:49:05 AM PDT, Mollie Brooks <mollieebrooks at gmail.com> wrote:
>https://rseek.org <https://rseek.org/> is a useful resource. 
>
>cheers,
>Mollie
>
>> On 29Mar 2019, at 15:45, hasyim hasyim <hasyim73 at outlook.com> wrote:
>> 
>> 
>> Thank you for your kind feedback and gentle reminder concerning
>regulation of the topic discussion.
>> 
>> I want to ask again, is there a link, for-learning the command of
>each parameter below using R software?
>> 
>> The Median Odds Ratio (MOR),
>> 80% Interval Odds Ratio (80% IOR),
>> The proportion of Odds Ratios of opposite direction (POOR),
>> Intraclass Correlation Coefficient (ICC),
>> the Proportional Change of the Variance (PCV), and
>> The area under the receiver operating characteristic curve (AU-ROC)
>> 
>> Thank you very much for offering to help out with this case for
>improving my knowledge to create the syntax of each parameter use R. I
>appreciate your willingness to assist.
>> 
>> 
>> Kind regards
>> 
>> Hasyim
>> 
>> On Fri, 29 Mar 2019 at 04:10, Jeff Newmiller
><jdnewmil at dcn.davis.ca.us<mailto:jdnewmil at dcn.davis.ca.us>> wrote:
>> This request is not about R. Therefore it is off-topic on this
>mailing list. In general, forum participants don't typically enjoy
>playing the part of a dictionary or textbook, so even referring you to
>a statistics forum like
>stats.stackexchange.com<http://stats.stackexchange.com> would not be
>doing you or them any favors. Please do some reading and if you have
>questions about statistical concepts then consider asking those
>directed questions on
>stats.stackexchange.com<http://stats.stackexchange.com>.
>> 
>> On March 28, 2019 7:07:07 AM PDT, hasyim hasyim
><hasyim73 at outlook.com<mailto:hasyim73 at outlook.com>> wrote:
>>> Hi everyone,
>>> 
>>> Could you please explain this in detail to me the criteria of each
>>> parameter below that use in multilevel regression logistic analysis
>>> (MLRA) such as:
>>> 
>>> the Median Odds Ratio (MOR),
>>> 80% Interval Odds Ratio (80% IOR),
>>> Proportion of Odds Ratios of opposite direction (POOR),
>>> Intraclass Correlation Coefficient (ICC),
>>> the Proportional Change of the Variance (PCV), and
>>> Area under the receiver operating characteristic curve (AU-ROC)
>>> 
>>> 
>>> Thank you, I really appreciate your help.
>>> 
>>> 
>>> Kind regards
>>> 
>>> 
>>> Hasyim
>>> 
>>>      [[alternative HTML version deleted]]
>>> 
>>> _______________________________________________
>>>
>R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org>
>mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> 
>> --
>> Sent from my phone. Please excuse my brevity.
>> 
>> _______________________________________________
>>
>R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org>
>mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>	[[alternative HTML version deleted]]
>
>_______________________________________________
>R-sig-mixed-models at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Sent from my phone. Please excuse my brevity.


From bbo|ker @end|ng |rom gm@||@com  Fri Mar 29 17:39:18 2019
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Fri, 29 Mar 2019 12:39:18 -0400
Subject: [R-sig-ME] The criteria of MLRA parameter
In-Reply-To: <VI1P192MB0351B3F9279F4159AE6F1BEDDF5A0@VI1P192MB0351.EURP192.PROD.OUTLOOK.COM>
References: <VI1P192MB0351A3EAC93D1194771555FFDF590@VI1P192MB0351.EURP192.PROD.OUTLOOK.COM>
 <226240B9-C8B5-4147-B4C8-8D7A68BA70BD@dcn.davis.ca.us>
 <VI1P192MB0351B3F9279F4159AE6F1BEDDF5A0@VI1P192MB0351.EURP192.PROD.OUTLOOK.COM>
Message-ID: <313e8b4d-f39d-7931-ed13-5bb5cdc7d9d1@gmail.com>


   I guess my question would be, where did you get this list of summary
measures in the first place?

  If I google (for example) "median odds ratio R" I get this

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2566165/

which at least explains what most of them are.  I also got a link to
Paul Johnson's GLMMmisc package.

  Based on the document, you may need Bayesian approaches (see MCMCglmm,
rstanarm, brms packages) to compute many of these values.



On 2019-03-29 10:45 a.m., hasyim hasyim wrote:
> 
> Thank you for your kind feedback and gentle reminder concerning regulation of the topic discussion.
> 
> I want to ask again, is there a link, for-learning the command of each parameter below using R software?
> 
> The Median Odds Ratio (MOR),
> 80% Interval Odds Ratio (80% IOR),
> The proportion of Odds Ratios of opposite direction (POOR),
> Intraclass Correlation Coefficient (ICC),
> the Proportional Change of the Variance (PCV), and
> The area under the receiver operating characteristic curve (AU-ROC)
> 
> Thank you very much for offering to help out with this case for improving my knowledge to create the syntax of each parameter use R. I appreciate your willingness to assist.
> 
> 
> Kind regards
> 
> Hasyim
> 
> On Fri, 29 Mar 2019 at 04:10, Jeff Newmiller <jdnewmil at dcn.davis.ca.us<mailto:jdnewmil at dcn.davis.ca.us>> wrote:
> This request is not about R. Therefore it is off-topic on this mailing list. In general, forum participants don't typically enjoy playing the part of a dictionary or textbook, so even referring you to a statistics forum like stats.stackexchange.com<http://stats.stackexchange.com> would not be doing you or them any favors. Please do some reading and if you have questions about statistical concepts then consider asking those directed questions on stats.stackexchange.com<http://stats.stackexchange.com>.
> 
> On March 28, 2019 7:07:07 AM PDT, hasyim hasyim <hasyim73 at outlook.com<mailto:hasyim73 at outlook.com>> wrote:
>> Hi everyone,
>>
>> Could you please explain this in detail to me the criteria of each
>> parameter below that use in multilevel regression logistic analysis
>> (MLRA) such as:
>>
>> the Median Odds Ratio (MOR),
>> 80% Interval Odds Ratio (80% IOR),
>> Proportion of Odds Ratios of opposite direction (POOR),
>> Intraclass Correlation Coefficient (ICC),
>> the Proportional Change of the Variance (PCV), and
>> Area under the receiver operating characteristic curve (AU-ROC)
>>
>>
>> Thank you, I really appreciate your help.
>>
>>
>> Kind regards
>>
>>
>> Hasyim
>>
>>       [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> --
> Sent from my phone. Please excuse my brevity.
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From motyoc@k@ @end|ng |rom y@hoo@com  Sat Mar 30 13:49:26 2019
From: motyoc@k@ @end|ng |rom y@hoo@com (Andras Farkas)
Date: Sat, 30 Mar 2019 12:49:26 +0000 (UTC)
Subject: [R-sig-ME] choice of prediction error calculations for zero
 inflated model
References: <1734950256.13655666.1553950166796.ref@mail.yahoo.com>
Message-ID: <1734950256.13655666.1553950166796@mail.yahoo.com>

Hello All,

thought I would reach out to see if you have some guidance on the following: I am working with a zero inflated data set and fitting models that should be reasonable to model such data (zeroinfl() and hurdle() from pscl, mixtures from flexmix, glm.nb, etc) and trying to compare model predictive performance based on a validation data set (70% of all data was used to train and the renaming 30% for validation)... This is not necessarily a coding question but rather a stat oriented perhaps although a working example would be helpful, if there is one: I have looked extensively to see what is in the literature for measures of predictive performance of zero inflated models based on a validation data set to compare observed vs predicted responses for count data, but could not come up with much. I am familiar with general measures of performance, like RMSE, MAE, etc but finding not much on it's appropriateness for use in my setting. Some references point towards adjusted/pseudo r squared approaches but most references are to evaluate model fit during model development vs predictive performance using a validation set... Any thoughts, directions you may be able to help me with??

much appreciate your input...

thanks,

Andras?


From d@r|zopou|o@ @end|ng |rom er@@mu@mc@n|  Sat Mar 30 14:25:18 2019
From: d@r|zopou|o@ @end|ng |rom er@@mu@mc@n| (D. Rizopoulos)
Date: Sat, 30 Mar 2019 13:25:18 +0000
Subject: [R-sig-ME] choice of prediction error calculations for zero
 inflated model
In-Reply-To: <1734950256.13655666.1553950166796@mail.yahoo.com>
References: <1734950256.13655666.1553950166796.ref@mail.yahoo.com>,
 <1734950256.13655666.1553950166796@mail.yahoo.com>
Message-ID: <7191AFC7255B4F49A30707E39BEAD05FDECF858C@EXCH-RX03.erasmusmc.nl>

You could have a look at proper scoring rules: https://en.m.wikipedia.org/wiki/Scoring_rule

For an example in mixed models check this example in the GLMMadaptive package: https://drizopoulos.github.io/GLMMadaptive/articles/Dynamic_Predictions.html

Best,
Dimitris


Sent with my iPhone - apologies for typos


From: Andras Farkas via R-sig-mixed-models <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
Date: Saturday, 30 Mar 2019, 13:49
To: R-sig-mixed-models <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
Subject: [R-sig-ME] choice of prediction error calculations for zero inflated model

Hello All,

thought I would reach out to see if you have some guidance on the following: I am working with a zero inflated data set and fitting models that should be reasonable to model such data (zeroinfl() and hurdle() from pscl, mixtures from flexmix, glm.nb, etc) and trying to compare model predictive performance based on a validation data set (70% of all data was used to train and the renaming 30% for validation)... This is not necessarily a coding question but rather a stat oriented perhaps although a working example would be helpful, if there is one: I have looked extensively to see what is in the literature for measures of predictive performance of zero inflated models based on a validation data set to compare observed vs predicted responses for count data, but could not come up with much. I am familiar with general measures of performance, like RMSE, MAE, etc but finding not much on it's appropriateness for use in my setting. Some references point towards adjusted/pseudo r squared approaches but most references are to evaluate model fit during model development vs predictive performance using a validation set... Any thoughts, directions you may be able to help me with?

much appreciate your input...

thanks,

Andras

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-mixed-models&amp;data=02%7C01%7Cd.rizopoulos%40erasmusmc.nl%7Ced28d249435a4fd20dce08d6b50e30e8%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C0%7C636895469887628746&amp;sdata=ggaw%2BsDr1lYDWqQulpcL1YdOnF7OK2VBtVp917grWtI%3D&amp;reserved=0

	[[alternative HTML version deleted]]


From motyoc@k@ @end|ng |rom y@hoo@com  Sun Mar 31 16:20:05 2019
From: motyoc@k@ @end|ng |rom y@hoo@com (Andras Farkas)
Date: Sun, 31 Mar 2019 14:20:05 +0000 (UTC)
Subject: [R-sig-ME] choice of prediction error calculations for zero
 inflated model
In-Reply-To: <7191AFC7255B4F49A30707E39BEAD05FDECF858C@EXCH-RX03.erasmusmc.nl>
References: <1734950256.13655666.1553950166796.ref@mail.yahoo.com>
 <1734950256.13655666.1553950166796@mail.yahoo.com>
 <7191AFC7255B4F49A30707E39BEAD05FDECF858C@EXCH-RX03.erasmusmc.nl>
Message-ID: <652135489.13963702.1554042005410@mail.yahoo.com>

Dimitris,

thank you, very helpful.... I took a look at your scoring_rules function and attempted to "re-write" it for zero inflated poisson based on output of pscl predict models, would you mind taking a quick look, maybe it will become useful for someone else eventually:

library(caret)
library(pscl)
data("bioChemists", package = "pscl")
split=0.7
inTrain <- createDataPartition(bioChemists[,1], p=split, list=FALSE)
training <- bioChemists[ inTrain,]
test <- bioChemists[-inTrain,]
fm_zip2 <- zeroinfl(art ~ . | ., data = training)

y <-test$art
n <- length(y)
max_count<-5000

max_count <- rep(max_count, length.out = n)
prob_fun <- function (x, mean, pis) {
? ? ind0 <- x == 0
? ? out <- (1 - pis) * dpois(x, lambda = mean)
? ? out[ind0] <- pis + out[ind0]
? ? out
? }

max_count_seq <- lapply(max_count, seq, from = 0)
pred <- predict(fm_zip2, newdata = test, type = "response")
pred_zi <- predict(fm_zip2, newdata = test, type="zero")? #zero probs equal to your zi_probs? attribute generated by predict.MixMod?.

logarithmic <- quadratic <- spherical <- numeric(n)
for (i in seq_len(n)) {
? p_y <- prob_fun(y[i], mean = pred[i], pis = pred_zi[i])
? quadrat_p <- sum(prob_fun(max_count_seq[[i]], mean = pred[i],?
? ? ? ? ? ? ? ? ? ? ? ? ? ? pis = pred_zi[i])^2)
? logarithmic[i] <- log(p_y)
? quadratic[i] <- 2 * p_y + quadrat_p
? spherical[i] <- p_y / sqrt(quadrat_p)
}
result <- data.frame(logarithmic = logarithmic, quadratic = quadratic,?
? ? ? ? ? ? ? ? ? ? ?spherical = spherical)

thanks

Andras?




On Saturday, March 30, 2019, 9:25:25 AM EDT, D. Rizopoulos <d.rizopoulos at erasmusmc.nl> wrote: 






You could have a look at proper scoring rules:?https://en.m.wikipedia.org/wiki/Scoring_rule 



For an example in mixed models check this example in the GLMMadaptive package:?https://drizopoulos.github.io/GLMMadaptive/articles/Dynamic_Predictions.html




Best,?

Dimitris?


Sent with my iPhone - apologies for typos





From: Andras Farkas via R-sig-mixed-models <r-sig-mixed-models at r-project.org>

Date: Saturday, 30 Mar 2019, 13:49

To: R-sig-mixed-models <r-sig-mixed-models at r-project.org>

Subject: [R-sig-ME] choice of prediction error calculations for zero inflated model



??
Hello All,

thought I would reach out to see if you have some guidance on the following: I am working with a zero inflated data set and fitting models that should be reasonable to model such data (zeroinfl() and hurdle() from pscl, mixtures from flexmix, glm.nb, etc) and trying to compare model predictive performance based on a validation data set (70% of all data was used to train and the renaming 30% for validation)... This is not necessarily a coding question but rather a stat oriented perhaps although a working example would be helpful, if there is one: I have looked extensively to see what is in the literature for measures of predictive performance of zero inflated models based on a validation data set to compare observed vs predicted responses for count data, but could not come up with much. I am familiar with general measures of performance, like RMSE, MAE, etc but finding not much on it's appropriateness for use in my setting. Some references point towards adjusted/pseudo r squared approaches but most references are to evaluate model fit during model development vs predictive performance using a validation set... Any thoughts, directions you may be able to help me with??

much appreciate your input...

thanks,

Andras?


_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-mixed-models&amp;data=02%7C01%7Cd.rizopoulos%40erasmusmc.nl%7Ced28d249435a4fd20dce08d6b50e30e8%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C0%7C636895469887628746&amp;sdata=ggaw%2BsDr1lYDWqQulpcL1YdOnF7OK2VBtVp917grWtI%3D&amp;reserved=0


