From h|gh@t@t @end|ng |rom h|gh@t@t@com  Thu Apr  2 09:20:22 2020
From: h|gh@t@t @end|ng |rom h|gh@t@t@com (Highland Statistics Ltd)
Date: Thu, 2 Apr 2020 08:20:22 +0100
Subject: [R-sig-ME] Online stats course with on-demand video and live
 meetings: Introduction to Linear Mixed Effects Models and GLMM with R-INLA
Message-ID: <a3d32777-a862-8fb7-0801-c61ef3547ee6@highstat.com>

We would like to announce the following online statistics course:


Online course with on-demand video and live Zoom meetings: 'Introduction 
to Linear Mixed Effects Models and GLMM with R-INLA'

Remark: The course fee includes a 1-hour face-to-face video chat with 
one or both instructors.

When: 22 June-10 July 2020
Time zone for live Zoom meetings: Multiple time zones, see the flyer.

Flyer: 
http://highstat.com/Courses/Flyers/2020/Flyer2020_06_GLMMINLA_Online.pdf

Website: http://highstat.com/index.php/courses-upcoming


Kind regards,


Alain Zuur

-- 

Dr. Alain F. Zuur
Highland Statistics Ltd.
9 St Clair Wynd
AB41 6DZ Newburgh, UK
Email:highstat at highstat.com
URL:www.highstat.com


	[[alternative HTML version deleted]]


From r@turner @end|ng |rom @uck|@nd@@c@nz  Fri Apr  3 02:35:37 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Fri, 3 Apr 2020 13:35:37 +1300
Subject: [R-sig-ME] Another predict/cloglog peculiarity --- lme4 this time.
Message-ID: <cd413f00-bd15-cfbd-e9ef-3e0216e70920@auckland.ac.nz>


I'm a bit hesitant to ask this, since the last time that I asked about a 
peculiar result from predict and the "cloglog" link, it turned out that 
the problem was due to my having installed an "unorthodox" version of 
the glmmTMB package.  So I had egg on my face.

This time the package involved is lme4 and I'm pretty sure that I am 
using the "standard" version.  (See my session info below.) If I am just 
doing something stupid, I apologise for the noise.

Now to the question:

I'm pretty sure that

    linkfun(predict(fit,type=response))

should be equal to predict(fit,type="link")  and likewise

    linkinv(predict(fit,type="link"))

should be equal to predict(fit,type="response")) .

With lme4, when the "logit" link is used, both of these equalities 
appear to hold.  When the "cloglog" link is used, the second equality 
holds, but in first instance I get a range of differences:

> [1] -4.60471443  0.00281936

I have attached a sourceable script "demo.txt" and the data set "X.txt" 
upon which it depends to demonstrate this phenomenon.

I'd appreciate any enlightenment that anyone can provide about what's 
going on here.  If I'm just being stupid, please feel free to tell me 
so, as bluntly as you like.

My session info:

> R version 3.6.3 (2020-02-29)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Ubuntu 18.04.4 LTS
> 
> Matrix products: default
> BLAS:   /usr/lib/x86_64-linux-gnu/atlas/libblas.so.3.10.3
> LAPACK: /usr/lib/x86_64-linux-gnu/atlas/liblapack.so.3.10.3
> 
> locale:
>  [1] LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C              
>  [3] LC_TIME=en_NZ.UTF-8        LC_COLLATE=en_GB.UTF-8    
>  [5] LC_MONETARY=en_NZ.UTF-8    LC_MESSAGES=en_GB.UTF-8   
>  [7] LC_PAPER=en_NZ.UTF-8       LC_NAME=C                 
>  [9] LC_ADDRESS=C               LC_TELEPHONE=C            
> [11] LC_MEASUREMENT=en_NZ.UTF-8 LC_IDENTIFICATION=C       
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base     
> 
> other attached packages:
> [1] lme4_1.1-21   Matrix_1.2-17 brev_0.0-3   
> 
> loaded via a namespace (and not attached):
>  [1] minqa_1.2.4     MASS_7.3-51.5   compiler_3.6.3  Rcpp_1.0.4     
>  [5] splines_3.6.3   nlme_3.1-144    grid_3.6.3      nloptr_1.2.2.1 
>  [9] boot_1.3-24     lattice_0.20-40

Thanks.

cheers,

Rolf Turner

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: demo.txt
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20200403/74294891/attachment.txt>

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: X.txt
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20200403/74294891/attachment-0001.txt>

From th|erry@onke||nx @end|ng |rom |nbo@be  Fri Apr  3 09:00:13 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Fri, 3 Apr 2020 09:00:13 +0200
Subject: [R-sig-ME] 
 Another predict/cloglog peculiarity --- lme4 this time.
In-Reply-To: <cd413f00-bd15-cfbd-e9ef-3e0216e70920@auckland.ac.nz>
References: <cd413f00-bd15-cfbd-e9ef-3e0216e70920@auckland.ac.nz>
Message-ID: <CAJuCY5yizZGQgOaFCr5ATQ2Fvs7KJUOmQvxfq5z_X+Y3AjOr1g@mail.gmail.com>

Dear Rolf,

This is due to the precision of floating point numbers. Let's have a look
at what happens when we get the extreme negative difference.

> i <- which.min(g(predResp) - predLink)
> c(predResp[i], 1 - exp(-exp(predLink[i])))
76 76
 1  1
> c(1 - predResp[i], exp(-exp(predLink[i])))
          76           76
2.220446e-16 0.000000e+00
> c(-log(1 - predResp[i]), exp(predLink[i]))
        76         76
  36.04365 3602.72298
> c(log(-log(1 - predResp[i])), predLink[i])
      76       76
3.584731 8.189445

Best regards,

Thierry

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op vr 3 apr. 2020 om 02:36 schreef Rolf Turner <r.turner at auckland.ac.nz>:

>
> I'm a bit hesitant to ask this, since the last time that I asked about a
> peculiar result from predict and the "cloglog" link, it turned out that
> the problem was due to my having installed an "unorthodox" version of
> the glmmTMB package.  So I had egg on my face.
>
> This time the package involved is lme4 and I'm pretty sure that I am
> using the "standard" version.  (See my session info below.) If I am just
> doing something stupid, I apologise for the noise.
>
> Now to the question:
>
> I'm pretty sure that
>
>     linkfun(predict(fit,type=response))
>
> should be equal to predict(fit,type="link")  and likewise
>
>     linkinv(predict(fit,type="link"))
>
> should be equal to predict(fit,type="response")) .
>
> With lme4, when the "logit" link is used, both of these equalities
> appear to hold.  When the "cloglog" link is used, the second equality
> holds, but in first instance I get a range of differences:
>
> > [1] -4.60471443  0.00281936
>
> I have attached a sourceable script "demo.txt" and the data set "X.txt"
> upon which it depends to demonstrate this phenomenon.
>
> I'd appreciate any enlightenment that anyone can provide about what's
> going on here.  If I'm just being stupid, please feel free to tell me
> so, as bluntly as you like.
>
> My session info:
>
> > R version 3.6.3 (2020-02-29)
> > Platform: x86_64-pc-linux-gnu (64-bit)
> > Running under: Ubuntu 18.04.4 LTS
> >
> > Matrix products: default
> > BLAS:   /usr/lib/x86_64-linux-gnu/atlas/libblas.so.3.10.3
> > LAPACK: /usr/lib/x86_64-linux-gnu/atlas/liblapack.so.3.10.3
> >
> > locale:
> >  [1] LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C
> >  [3] LC_TIME=en_NZ.UTF-8        LC_COLLATE=en_GB.UTF-8
> >  [5] LC_MONETARY=en_NZ.UTF-8    LC_MESSAGES=en_GB.UTF-8
> >  [7] LC_PAPER=en_NZ.UTF-8       LC_NAME=C
> >  [9] LC_ADDRESS=C               LC_TELEPHONE=C
> > [11] LC_MEASUREMENT=en_NZ.UTF-8 LC_IDENTIFICATION=C
> >
> > attached base packages:
> > [1] stats     graphics  grDevices utils     datasets  methods   base
> >
> > other attached packages:
> > [1] lme4_1.1-21   Matrix_1.2-17 brev_0.0-3
> >
> > loaded via a namespace (and not attached):
> >  [1] minqa_1.2.4     MASS_7.3-51.5   compiler_3.6.3  Rcpp_1.0.4
> >  [5] splines_3.6.3   nlme_3.1-144    grid_3.6.3      nloptr_1.2.2.1
> >  [9] boot_1.3-24     lattice_0.20-40
>
> Thanks.
>
> cheers,
>
> Rolf Turner
>
> --
> Honorary Research Fellow
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From Ph||||p@A|d@y @end|ng |rom mp|@n|  Fri Apr  3 23:13:23 2020
From: Ph||||p@A|d@y @end|ng |rom mp|@n| (Alday, Phillip)
Date: Fri, 3 Apr 2020 21:13:23 +0000
Subject: [R-sig-ME] Interaction terms with random slopes
In-Reply-To: <CAOE=hq+MhwXLQW5Y84+0ir3b4_5Q0Y+XSiPyuW9nQSQ8L-0-3Q@mail.gmail.com>
References: <CAOE=hq+MhwXLQW5Y84+0ir3b4_5Q0Y+XSiPyuW9nQSQ8L-0-3Q@mail.gmail.com>
Message-ID: <804b7457-794e-b215-d067-784e5260edc7@mpi.nl>

Perhaps because I never had to deal with software that relies on
explicit nesting of levels, I find it very hard to follow the level-1
and level-2 terminology. Can you provide an example of the model you're
thinking of and how you would test the corresponding hypothesis (e.g.
model comparison, etc.)?

The usual rule of thumb is that for any random slope, you should have
the corresponding fixed-effect slope in the model. There's a discussion
of this over on CrossValidated:
https://stats.stackexchange.com/a/339859/26743

Now, I can think of exceptions to this rule, much like I can think of
exceptions to the rule that you should always include the intercept in a
linear model. But in both cases, if you have to ask, it usually means
you shouldn't do it.

On 11/3/20 9:00 pm, Yashree Mehta wrote:
> Hi,
>
> I have the following question:
>
> I estimate a random intercept-random slope model. For my research question,
> I want to interact the random slope variable with another level-1
> variable(which is not necessary as a main effect in the model). I am
> interesting in observing whether this level-1 variable moderates the random
> slope on the dependent variable.
>
> Do I have to include the level-1 variable as a main effect in the model (as
> is required by some linear modelling literature)? I would prefer not to
> include this level-1 variable as a main effect due to problems of
> multicollinearity.
>
> Thank you very much!
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From ph||||p@@|d@y @end|ng |rom mp|@n|  Fri Apr  3 23:26:58 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Fri, 3 Apr 2020 23:26:58 +0200
Subject: [R-sig-ME] 
 Doubt about model structure and extraction of slopes and intercepts
In-Reply-To: <CAAPK02Aa1vKQTw6xUbgm6heCKi_JJP9PihhmDF81H78K5V2fQg@mail.gmail.com>
References: <CAAPK02Aa1vKQTw6xUbgm6heCKi_JJP9PihhmDF81H78K5V2fQg@mail.gmail.com>
Message-ID: <a16eca6c-35c6-a41d-1ab0-b31828421cbd@mpi.nl>


On 12/3/20 8:48 pm, Nicolay Cunha wrote:
> Hi everyone,
>
> I am trying to fit a mixed model with a data set that was extracted from a
> grid. We grid a certain area and counted the frequency of occurrence of three
> levels of a factor variable in each grid (fac = fac1, fac2 and fac3), I am
> basically interested in testing the relationship of the frequency of
> occurrence of each level of ?fac? with external variables (X1 and X2, for
> instance). The issue is, I am not interested in knowing if the frequency of
> occurrence of each level of ?fac? differs between each other (fac1 ? fac2 ?
> fac3) because it may be influenced by sampling bias, but I am interested if
> X1 and X2 explain the frequency of occurrence of fac1, fac2 and fac3. In
> this case, my sample unit is each level of fac per cell of the grid, and
> grid is my random factor (in my original data, there is one additional
> complication. In my case, ?fac?, is a trait and each sample is a species
> individual occurring in the grid, so I will also have to correct for this,
> but I think that it can be put aside at the moment).
>
>
> Below is a reproducible example:
>
> ############################################################
>
> set.seed(457)
> # generating the data
> dat <- data.frame(X1 = runif(150,-2,2), X2 = runif(150,-2,2), fac = gl(n =
> 3,k = 50))
> modmat <- model.matrix(~X1*X2*fac,dat)
> betas <- runif(12,-2,2)
> dat$y <- rnorm(150, modmat%*%betas, 1 ) # frequency of occurrence
> dat$rand <- rep(c(1:5), each = 30) # random effect
>
> library(lme4)
> m1 <- lmer(y ~ fac/X1 + fac/X2 + (1|rand), data = dat, REML = FALSE)
> library(car)
> Anova(m1)
> summary(m1)



Thanks for having a MWE!

> #-----------First question--------------#
> # Does it make sense to write the model the way it is ? by nesting fac with
> the predictor variables, to assess the relationship I am interested in?
> # Is there a better(or correct) way for doing this?

If I undertstand correctly, you're actually dealing with counts, right?
Wouldn't a Poisson model make more sense?

The nesting effectively works out to a particular contrast coding.
Contrast coding in mixed models works the same way as in classical
linear regression, so any resources you can find for that will also help
you here.

The problem I see here is that you need to think about whether you care
about absolute or relative differences. Right now, your interactions are
for absolute differences, e.g. you're correcting each fac for X1, but if
the baseline counts for each fac are drastically different, this may not
be the comparison you want.

> #############################################################################################
> # plotting the results
>
> with(dat, plot(X1, predict(m1), pch=21, col = "black", bg = c("green",
> "purple", "salmon")[dat$fac]))
> X1_seq = seq(min(dat$X1), max(dat$X1), by=0.001)
> ypred_X1_fac1 = (fixef(m1)[1])  + (fixef(phylo_lmm_LF4)[4])*(X1_seq)
> lines(X1_seq, ypred_X1_fac1, col = "green", lty = "solid", lwd = 1.5)
> ypred_X1_fac2 = (fixef(m1)[2])  + (fixef(phylo_lmm_LF4)[5])*(X1_seq)
> lines(X1_seq, ypred_X1_fac2, col = "purple", lty = "solid", lwd = 1.5)
> ypred_X1_fac3 = (fixef(m1)[3])  + (fixef(phylo_lmm_LF4)[6])*(X1_seq)
> lines(X1_seq, ypred_X1_fac3, col = "salmon", lty = "solid", lwd = 1.5)
>
> #-----------Second question--------------#
> # How to extract the slopes and intercepts for all combinations between X1,
> X2 and fac in model this way?
> # I tried plotting this but failed miserably and I am sure that I am
> committing some terrible mistake.
> #############################################################################################


Check out the effects and emmeans packages.

Phillip


>
> I do thank any help, advise and orientation with the model building and
> plotting the predictions.
>
> Cheers,
> Nicolay
>
>
> ------------------------------------------------------------------------
> Grupo de Ecolog?a de la Polinizaci?n
> (https://sites.google.com/view/ecopol/home)
> INIBIOMA, CONICET-Universidad Nacional del Comahue
> Quintral 1250
> 8400 San Carlos de Bariloche
> Rio Negro, Argentina
> ------------------------------------------------------------------------
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From ph||||p@@|d@y @end|ng |rom mp|@n|  Fri Apr  3 23:30:29 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Fri, 3 Apr 2020 23:30:29 +0200
Subject: [R-sig-ME] 
 Working formula for glmm model in R (bernoulli response)
In-Reply-To: <1584623675.683551355@f417.i.mail.ru>
References: <1584623675.683551355@f417.i.mail.ru>
Message-ID: <0135e2c6-b8f9-5d7d-1ead-d2d17c14bd13@mpi.nl>

Most packages in R lump Bernoulli and Binomial families together, so
this should work:

library(lme4)

glmer(occuppied ~ 1 + year + (1|territory), data=yourdata, family=binomial)


On 19/3/20 2:14 pm, Michael Romanov via R-sig-mixed-models wrote:
> Dear all,
> ?
> Sorry for the silly question, but I got stuck on it.
> ?
> My dataset is occupancy of eagle territories in different years (see the example below). I?m trying to test my data for time trend, taking into account territory quality (as a random effect).
> ?
> territory year occupied
> CHV-1 2006 0
> CHV-12 2006 0
> CHV-120 2006 1
> CHV-13 2009 0
> CHV-14 2009 1
> CHV-15 2010 1
> CHV-16 2010 1
> ?
> My thoughts are following. ?year? is a fixed effect variable and ?territory? is a random effect variable. For example, in lme4 package the formula could be following: occupied = year + (1|territory). However, lme4 doesn?t support Bernoulli family, so I chose glmm package. According to its specifications, the glmm function accepts two different formulae, separately for fixed and random effects:
> ?
> glmm(fixed, random, varcomps.names, data, family.glmm, m, varcomps.equal, doPQL = TRUE,debug=FALSE, p1=1/3,p2=1/3, p3=1/3, rmax=1000,iterlim=1000, par.init, zeta=5, cluster=NULL)
> ?
> So now I need two different formulae. I tried ?occupied ~ year?, ?occupied ~ territory?, but it doesn?t work. Namely, the RStudio gets into an infinite loop and doesn?t produce any output.
> ?
> Can anyone help me to write a working formula (or formulae) to properly run the glmm model?
> ?
> Thanks,
> ?
> Michael
> ?
> ?
> ?
> ?
> ?
> ?
> ?
> ?
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From ph||||p@@|d@y @end|ng |rom mp|@n|  Fri Apr  3 23:47:28 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Fri, 3 Apr 2020 23:47:28 +0200
Subject: [R-sig-ME] 
 Models and Power Analysis for Within-subject Design in LMER
In-Reply-To: <0f262bf6f1624850b747d37cfa7d0780@vu.nl>
References: <0f262bf6f1624850b747d37cfa7d0780@vu.nl>
Message-ID: <ea9a1335-a292-bc46-d0b1-0da87b763f9f@mpi.nl>

Hi Lei,

It's been a while, but I haven't a response to your email go by and I'm
trying small productivity tasks to get the stats juices flowing, so here
it goes ....

On 14/10/19 5:08 pm, Fan, L. via R-sig-mixed-models wrote:
> Hi,
> This is Lei from VU Amsterdam.
> Recently I am proposing a new within-subject study based on my former results of a between-subject one. As planning for the proposal, I found it is a little bit confuse in creating the model.
> In the first between-subject study, I used a basic model like this with some other random slopes:
> Emotion Value ~ Emotion Type * WTR + (1|Subject) + (1|Scenario)
> Each participant would have one WTR index and finish 2 emotion assessments based on one single scenario from the pool.  The EV and ET were created as repeated measure format originally from the emotion assessments. The effect we focused on is the interaction.
> In the new study, we would like to make the study within-subject, by asking the participants to repeat the procedure for 3 times with different scenarios and WTR conditions (here the conditions are as a manipulation for maximum the rage of the WTRs, not as an IV). Then, here comes the questions:
> 1. I tried to draw the model for the new study, but it seemed to be the same as the between-subject one. Does it mean that I make mistakes in creating the model?

In both cases, each subject and each scenario is measured multiple
times, so it makes sense to have each as a blocking variable. The
nesting/crossing structure of subjects and scenarios? don't have to be
specified explicitly, so this model won't change. Note that if you had
changed between- and within-subject manipulations (i.e. random slopes),
then the model would usually change.

In other words, your model didn't change because you didn't have the
manipulation encoded in the random effects and lme4 doesn't care about
nesting/crossing of random effects in the model specification.

> 2. Using the current model for calculating the sample size, the result should be the same for the required observations as for the between-subject design (model never changed). Should not it be smaller as the design is within-subject? The only possibility is the model did not count the within part. How can I make it work?

The between-subject and between-scenario variability didn't change, so
why would the power change? :) In real data, I suspect you will see a
change because you'll have a better estimate of the variability
introduced by subjects vs. the residual variability.

> 3. As for the between-subject study, we used a data simulation approach to calculate the sample size. Here, as it is a follow-up study, is there some other more convincing way to conduct the a-prior power analysis?
>
Nope, simulation is considered the standard for power analysis in
mixed-effects models.

Best,

Phillip


> Thanks a lot in advance!
>
> Best,
> Lei Fan
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From ph||||p@@|d@y @end|ng |rom mp|@n|  Fri Apr  3 23:53:10 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Fri, 3 Apr 2020 23:53:10 +0200
Subject: [R-sig-ME] Appropriate model reduction sequence for factorial
 design in glmmTMB
In-Reply-To: <CAHr4Dyc=7aizpvnSLKyZ7pb5a81Dj85OVNsyT5Ke4OP75tW4Jw@mail.gmail.com>
References: <CAHr4Dyc=7aizpvnSLKyZ7pb5a81Dj85OVNsyT5Ke4OP75tW4Jw@mail.gmail.com>
Message-ID: <f622e5ad-a189-3d16-7228-44882f64ee8f@mpi.nl>

Hi Maarten,

It's been a while and I still haven't had the time to give your post the
necessary thought to give you a proper answer ....

That said, Dimitri Rizopoulos posted some course notes a while back
(http://www.drizopoulos.com/courses/EMC/CE08.pdf). I found the
presentation there quite nice in terms of thinking about symmetry and
nesting structures. Emi Tanaka also has some great slides on "software
design for linear mixed model specification" which I also found great
for thinking about how these structures are represented in the syntax of
various software.

So I hope my non answer helps a bit ....

Phillip

On 22/10/19 10:01 pm, Maarten Jung wrote:
> Dear list,
>
> Sorry for basically restating my question here [1], but I think it
> might be worth a separate thread as it might well be much easier to
> answer with glmmTMB.
>
> After going through the posts [2] and [3] again, I identified the
> following nesting structure (arrows indicate nesting) as the one I
> want to go with for modelling some new data:
>
> m1 -> m2a/m2b/m2c -> m3 -> m4
>
> #######################################################
>
> library("lme4")
> data("Machines", package = "MEMSS")
> d <- Machines
> mat <- model.matrix(~ 0 + Machine, d)
> A <- mat[, 1]
> B <- mat[, 2]
> C <- mat[, 3]
>
> m1 <- lmer(score ~ Machine + (0 + Machine | Worker), d)
>
> m2a <- lmer(score ~ Machine + (1 | Worker) + (0 + dummy(Machine, "A")
> | Worker) +
>
>    (0 + dummy(Machine, "B") | Worker) +
>
>    (0 + dummy(Machine, "C") | Worker), d)
>
> m2b <- lmer(score ~ Machine + (1 | Worker) + (0 + A + B + C || Worker), d)
>
> m2c <- afex::lmer_alt(score ~ Machine + (1 | Worker) + (0 + Machine ||
> Worker), d)
>
> # m2a, m2b, and m2c are equivalent
> all.equal(logLik(m2a), logLik(m2b), logLik(m2c))
>
> m3 <- lmer(score ~ Machine + (1 | Worker) + (1 | Worker:Machine), d)
>
> m4 <- lmer(score ~ Machine + (1 | Worker), d)
>
> #######################################################
>
> In my new data there are multiple observations per cell of a (at
> least) 2x3 within-subjects design.
> I know that m1 (denoting the two factors with f1 and f2, respectively)
> would look like
>
> lmer(y ~ f1*f2 + (1 + f1*f2 | subject), data)
>
> and I think like this in glmmTMB syntax
>
> glmmTMB(y ~ f1*f2 + us(f1*f2 | subject), data, REML = TRUE)
>
> So now I'm struggling to figure out what m2a (or m2b/m2c) and m3 would
> look like in the (at least) 2-factorial case.
> My guess is that m2 would look like this in glmmTMB syntax
>
> glmmTMB(y ~ f1*f2 + (1 | subject) + diag(0 + f1*f2 | subject), data,
> REML = TRUE)
>
> and that this might correspond to the following afex:lmer_alt() model
>
> afex::lmer_alt(y ~ f1*f2 + (1 | subject) + (0 +  f1*f2 || subject ), data)
>
> But I'm not sure about m3.
>
> I would be grateful if someone could provide the appropriate glmmTMB
> syntax (additionally or alternatively, lmer/afex::lmer_alt syntax is
> still welcome in the other thread).
>
> Best,
> Maarten
>
> [1] https://stat.ethz.ch/pipermail/r-sig-mixed-models/2019q4/028222.html
> [2] https://stat.ethz.ch/pipermail/r-sig-mixed-models/2018q2/026775.html
> [3] https://stats.stackexchange.com/questions/345842/what-is-the-appropriate-zero-correlation-parameter-model-for-factors-in-lmer
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From y@@hree19 @end|ng |rom gm@||@com  Sat Apr  4 21:27:53 2020
From: y@@hree19 @end|ng |rom gm@||@com (Yashree Mehta)
Date: Sat, 4 Apr 2020 21:27:53 +0200
Subject: [R-sig-ME] Interaction terms with random slopes
In-Reply-To: <804b7457-794e-b215-d067-784e5260edc7@mpi.nl>
References: <CAOE=hq+MhwXLQW5Y84+0ir3b4_5Q0Y+XSiPyuW9nQSQ8L-0-3Q@mail.gmail.com>
 <804b7457-794e-b215-d067-784e5260edc7@mpi.nl>
Message-ID: <CAOE=hqKvjvp411GJKuna6EP1+QQZgWt7HzZP9M=QhsSPQd6uTA@mail.gmail.com>

Hi,

Thanks for your reply.

The model is as follows:

Production = seed + fertilizer + fertilizer : wheatlanduse + (1 +
fertilizer | Household)

The code implies the following: Production is the dependent variable and
the covariates are seed and fertilizer. Random intercept has been
specified at the Household level and the random slope variable is
fertilizer. Cross level interaction has been introduced with a wheat land
use dummy variable which interacts with fertilizer. As can be seen,
fertilizer has also been specified as a fixed effect.

My question is as follows: Is it necessary to also include wheatlanduse as
a fixed effect because it has been introduced as an interaction? I prefer
not to because it creates problems of multicollinearity with other
covariates in the model.

Thanks again.

Yashree






On Fri, Apr 3, 2020 at 11:13 PM Alday, Phillip <Phillip.Alday at mpi.nl> wrote:

> Perhaps because I never had to deal with software that relies on
> explicit nesting of levels, I find it very hard to follow the level-1
> and level-2 terminology. Can you provide an example of the model you're
> thinking of and how you would test the corresponding hypothesis (e.g.
> model comparison, etc.)?
>
> The usual rule of thumb is that for any random slope, you should have
> the corresponding fixed-effect slope in the model. There's a discussion
> of this over on CrossValidated:
> https://stats.stackexchange.com/a/339859/26743
>
> Now, I can think of exceptions to this rule, much like I can think of
> exceptions to the rule that you should always include the intercept in a
> linear model. But in both cases, if you have to ask, it usually means
> you shouldn't do it.
>
> On 11/3/20 9:00 pm, Yashree Mehta wrote:
> > Hi,
> >
> > I have the following question:
> >
> > I estimate a random intercept-random slope model. For my research
> question,
> > I want to interact the random slope variable with another level-1
> > variable(which is not necessary as a main effect in the model). I am
> > interesting in observing whether this level-1 variable moderates the
> random
> > slope on the dependent variable.
> >
> > Do I have to include the level-1 variable as a main effect in the model
> (as
> > is required by some linear modelling literature)? I would prefer not to
> > include this level-1 variable as a main effect due to problems of
> > multicollinearity.
> >
> > Thank you very much!
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From r@turner @end|ng |rom @uck|@nd@@c@nz  Sun Apr  5 06:07:15 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sun, 5 Apr 2020 16:07:15 +1200
Subject: [R-sig-ME] 
 Another predict/cloglog peculiarity --- lme4 this time.
In-Reply-To: <CAJuCY5yizZGQgOaFCr5ATQ2Fvs7KJUOmQvxfq5z_X+Y3AjOr1g@mail.gmail.com>
References: <cd413f00-bd15-cfbd-e9ef-3e0216e70920@auckland.ac.nz>
 <CAJuCY5yizZGQgOaFCr5ATQ2Fvs7KJUOmQvxfq5z_X+Y3AjOr1g@mail.gmail.com>
Message-ID: <9e6014f9-02f6-c7ad-bd6a-00f2d363612c@auckland.ac.nz>


On 3/04/20 8:00 pm, Thierry Onkelinx wrote:

> Dear Rolf,
> 
> This is due to the precision of floating point numbers. Let's have a 
> look at what happens when we get the extreme negative difference.
> 
>  > i <- which.min(g(predResp) - predLink)
>  > c(predResp[i], 1 - exp(-exp(predLink[i])))
> 76 76
>  ?1 ?1
>  > c(1 - predResp[i], exp(-exp(predLink[i])))
>  ? ? ? ? ? 76 ? ? ? ? ? 76
> 2.220446e-16 0.000000e+00
>  > c(-log(1 - predResp[i]), exp(predLink[i]))
>  ? ? ? ? 76 ? ? ? ? 76
>  ? 36.04365 3602.72298
>  > c(log(-log(1 - predResp[i])), predLink[i])
>  ? ? ? 76 ? ? ? 76
> 3.584731 8.189445

Thanks Thierry.  I think it is at last becoming clear to me.  Sorry for 
taking so long to reply, but I've been thrashing around, trying to 
express things in my own words, in a way that I can understand.

My attempt to do so follows.  Most of you will probably find that what I 
have to say simply obfuscates the issue. You are probably best advised 
to just read what Thierry has written above and ignore my ramblings. 
But just in case anyone is interested, here are my thoughts:

Basically predResp is exactly ginv(predLink); no problems with numerical 
delicacy there.

So when I looked at

     g(predResp) - predLink

I was actually looking at

     g(ginv(predLink)) - predLink

The problem is that g(ginv(x)) is NOT equal to x, when x gets bigger 
than about 3.5, due to numerical delicacy.  (For x < 3.5, g(ginv(x)) is
pretty close to x just as the young and na?ve, such as my very good self 
--- :-) --- would expect.)

For x > 3.5, ginv(x) becomes numerically indistinguishable from 1, 
whence g(ginv(x)) is g(1) = Inf.  Or it *would* be except for the fact 
that ginv() (for the cloglog link) is designed so that its values are 
capped at 1 - .Machine$double.eps = 2.220446e-16 (on my machine at least).

Consequently g(ginv(x)) is capped at g(1 - 2.220446e-16) = 3.584731.
So as x grows it quickly outstrips g(ginv(x)).

When x is close to 3.5 g(ginv(x)) is simply a bit numerically wobbly, 
and not necessarily smaller than x. (Which is why I got a negative 
value, explicitly -0.00281936, for the lower bound of the range of
g(predResp) - predLink.

Another person, who replied to me off-list, suggested plotting
g(predResp) ~ predLink .  This is indeed illuminating.

I think that the main thing to take away from all this is that numerical 
delicacy is always lurking in the bushes, and care should always be 
taken, especially when logs and exponentials are involved.  It is 
perilous to assume that *algebraic* identities, like g(ginv(x)) = x,
will turn out to be *numerically* true.

Thanks again to Thierry.

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From john@m@|ndon@|d @end|ng |rom @nu@edu@@u  Sun Apr  5 12:06:53 2020
From: john@m@|ndon@|d @end|ng |rom @nu@edu@@u (John Maindonald)
Date: Sun, 5 Apr 2020 10:06:53 +0000
Subject: [R-sig-ME] 
 Another predict/cloglog peculiarity --- lme4 this time.
In-Reply-To: <9e6014f9-02f6-c7ad-bd6a-00f2d363612c@auckland.ac.nz>
References: <cd413f00-bd15-cfbd-e9ef-3e0216e70920@auckland.ac.nz>
 <CAJuCY5yizZGQgOaFCr5ATQ2Fvs7KJUOmQvxfq5z_X+Y3AjOr1g@mail.gmail.com>
 <9e6014f9-02f6-c7ad-bd6a-00f2d363612c@auckland.ac.nz>
Message-ID: <FCB1FE16-7244-4230-8C26-1A33FF20E560@anu.edu.au>

The ?crude' way to calculate the inverse function, for eta=3.5, would be:

cllinv <- function(eta) <- 1-exp(-exp(eta))

Observe the following
options(digits=17)

rlinkinv <- make.link<http://make.link>('cloglog')$linkinv
cllinv <- function(eta)1-exp(-exp(eta))

cll <- function(mu)log(1-log(mu))

mat <- matrix(nrow=6, ncol=3)
colnames(mat)<- c('1-exp(1-exp(eta))','R linkinv()', 'Calc as 1 -')
rownames(mat) <- paste(seq(from=3.5, to=4, by=0.1))
i <- 0
for(eta in seq(from=3.5, to=4, by=0.1)){
  i <- i+1
  mat[i,] <- c(cllinv(eta), rlinkinv(eta), exp(1-exp(eta)))
}
print(mat)

      1-exp(1-exp(eta))                              R linkinv()                          Calc as 1 -
3.5 0.99999999999999589 0.99999999999999589 1.1283307669739036e-14
3.6 0.99999999999999989 0.99999999999999978 3.4664362337169868e-16
3.7 1.00000000000000000 0.99999999999999978 7.3833488826673409e-18
3.8 1.00000000000000000 0.99999999999999978 1.0490996011645160e-19
3.9 1.00000000000000000 0.99999999999999978 9.5297924643308894e-22
4   1.00000000000000000 0.99999999999999978 5.2798210162856661e-24

I?d judge that the maximum returned by rlinkinv = make.link<http://make.link>('cloglog')$linkinv
is set to ensure that log(-log(1-p)) with p=1 never appears in the calculations,
allowing:

cll(rlinkinv(3.7))
[1] 3.5847307979997631

It is however important that predicted values on the the scale of the linear predictor
are not thus constrained  ? it can be fatal to calculate them using rlinkinv(predict(obj,
type=?response?)) !


John Maindonald             email: john.maindonald at anu.edu.au<mailto:john.maindonald at anu.edu.au>


On 5/04/2020, at 16:07, Rolf Turner <r.turner at auckland.ac.nz<mailto:r.turner at auckland.ac.nz>> wrote:


On 3/04/20 8:00 pm, Thierry Onkelinx wrote:

Dear Rolf,
This is due to the precision of floating point numbers. Let's have a look at what happens when we get the extreme negative difference.
> i <- which.min(g(predResp) - predLink)
> c(predResp[i], 1 - exp(-exp(predLink[i])))
76 76
 1  1
> c(1 - predResp[i], exp(-exp(predLink[i])))
          76           76
2.220446e-16 0.000000e+00
> c(-log(1 - predResp[i]), exp(predLink[i]))
        76         76
  36.04365 3602.72298
> c(log(-log(1 - predResp[i])), predLink[i])
      76       76
3.584731 8.189445

Thanks Thierry.  I think it is at last becoming clear to me.  Sorry for taking so long to reply, but I've been thrashing around, trying to express things in my own words, in a way that I can understand.

My attempt to do so follows.  Most of you will probably find that what I have to say simply obfuscates the issue. You are probably best advised to just read what Thierry has written above and ignore my ramblings. But just in case anyone is interested, here are my thoughts:

Basically predResp is exactly ginv(predLink); no problems with numerical delicacy there.

So when I looked at

   g(predResp) - predLink

I was actually looking at

   g(ginv(predLink)) - predLink

The problem is that g(ginv(x)) is NOT equal to x, when x gets bigger than about 3.5, due to numerical delicacy.  (For x < 3.5, g(ginv(x)) is
pretty close to x just as the young and na?ve, such as my very good self --- :-) --- would expect.)

For x > 3.5, ginv(x) becomes numerically indistinguishable from 1, whence g(ginv(x)) is g(1) = Inf.  Or it *would* be except for the fact that ginv() (for the cloglog link) is designed so that its values are capped at 1 - .Machine$double.eps = 2.220446e-16 (on my machine at least).

Consequently g(ginv(x)) is capped at g(1 - 2.220446e-16) = 3.584731.
So as x grows it quickly outstrips g(ginv(x)).

When x is close to 3.5 g(ginv(x)) is simply a bit numerically wobbly, and not necessarily smaller than x. (Which is why I got a negative value, explicitly -0.00281936, for the lower bound of the range of
g(predResp) - predLink.

Another person, who replied to me off-list, suggested plotting
g(predResp) ~ predLink .  This is indeed illuminating.

I think that the main thing to take away from all this is that numerical delicacy is always lurking in the bushes, and care should always be taken, especially when logs and exponentials are involved.  It is perilous to assume that *algebraic* identities, like g(ginv(x)) = x,
will turn out to be *numerically* true.

Thanks again to Thierry.

cheers,

Rolf

--
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


	[[alternative HTML version deleted]]


From john@m@|ndon@|d @end|ng |rom @nu@edu@@u  Sun Apr  5 12:52:51 2020
From: john@m@|ndon@|d @end|ng |rom @nu@edu@@u (John Maindonald)
Date: Sun, 5 Apr 2020 10:52:51 +0000
Subject: [R-sig-ME] 
 Another predict/cloglog peculiarity --- lme4 this time (PS)
In-Reply-To: <9e6014f9-02f6-c7ad-bd6a-00f2d363612c@auckland.ac.nz>
References: <cd413f00-bd15-cfbd-e9ef-3e0216e70920@auckland.ac.nz>
 <CAJuCY5yizZGQgOaFCr5ATQ2Fvs7KJUOmQvxfq5z_X+Y3AjOr1g@mail.gmail.com>
 <9e6014f9-02f6-c7ad-bd6a-00f2d363612c@auckland.ac.nz>
Message-ID: <D74ECA79-0C94-4AF8-8EA8-88FEF1BD3717@anu.edu.au>

Apologies.  I see that my Mac mail software has interpreted 'make.link?
as a link that is underlined ? somewhere between going out and being
received at the other end (at least at my end), it was then mangled in
the attempt to give a presumed underlying web address.  Hopefully,
the following should be OK.

The issue under discussion related to any calculation that works with
a cloglog link function, not just mixed models.

The ?crude' way to calculate the inverse function, for eta=3.5, would be:

> cllinv <- function(eta) <- 1-exp(-exp(eta))
> 
Observe the following
> options(digits=17)
> 
> rlinkinv <- make.link('cloglog')$linkinv
> cllinv <- function(eta)1-exp(-exp(eta))
> 
> cll <- function(mu)log(1-log(mu))
> 
> mat <- matrix(nrow=6, ncol=3)
> colnames(mat)<- c('1-exp(1-exp(eta))','R linkinv()', 'Calc as 1 -')
> rownames(mat) <- paste(seq(from=3.5, to=4, by=0.1))
> i <- 0
> for(eta in seq(from=3.5, to=4, by=0.1)){
>   i <- i+1
>   mat[i,] <- c(cllinv(eta), rlinkinv(eta), exp(1-exp(eta)))
> }
> print(mat)

                 1-exp(1-exp(eta))                      R linkinv()                          Calc as 1 -
  3.5 0.99999999999999589 0.99999999999999589 1.1283307669739036e-14
  3.6 0.99999999999999989 0.99999999999999978 3.4664362337169868e-16
  3.7 1.00000000000000000 0.99999999999999978 7.3833488826673409e-18
  3.8 1.00000000000000000 0.99999999999999978 1.0490996011645160e-19
  3.9 1.00000000000000000 0.99999999999999978 9.5297924643308894e-22
  4    1.00000000000000000 0.99999999999999978 5.2798210162856661e-24

I?d judge that the maximum returned by rlinkinv = make.link('cloglog')$linkinv
is set to ensure that log(-log(1-mu)) with mu=1 never appears in the calculations,
allowing (but this should at least be documented, and perhaps generate a
warning):

> cll(rlinkinv(3.7))
  [1] 3.5847307979997631

It is important that predicted values on the scale of the linear predictor
are not thus constrained  ? it can, then, be fatal to calculate them using 
rlinkinv(predict(obj,  type?response?)) !

John Maindonald             email: john.maindonald at anu.edu.au

> On 5/04/2020, at 16:07, Rolf Turner <r.turner at auckland.ac.nz> wrote:
> 
> 
> On 3/04/20 8:00 pm, Thierry Onkelinx wrote:
> 
>> Dear Rolf,
>> This is due to the precision of floating point numbers. Let's have a look at what happens when we get the extreme negative difference.
>> > i <- which.min(g(predResp) - predLink)
>> > c(predResp[i], 1 - exp(-exp(predLink[i])))
>> 76 76
>>  1  1
>> > c(1 - predResp[i], exp(-exp(predLink[i])))
>>           76           76
>> 2.220446e-16 0.000000e+00
>> > c(-log(1 - predResp[i]), exp(predLink[i]))
>>         76         76
>>   36.04365 3602.72298
>> > c(log(-log(1 - predResp[i])), predLink[i])
>>       76       76
>> 3.584731 8.189445
> 
> Thanks Thierry.  I think it is at last becoming clear to me.  Sorry for taking so long to reply, but I've been thrashing around, trying to express things in my own words, in a way that I can understand.
> 
> My attempt to do so follows.  Most of you will probably find that what I have to say simply obfuscates the issue. You are probably best advised to just read what Thierry has written above and ignore my ramblings. But just in case anyone is interested, here are my thoughts:
> 
> Basically predResp is exactly ginv(predLink); no problems with numerical delicacy there.
> 
> So when I looked at
> 
>    g(predResp) - predLink
> 
> I was actually looking at
> 
>    g(ginv(predLink)) - predLink
> 
> The problem is that g(ginv(x)) is NOT equal to x, when x gets bigger than about 3.5, due to numerical delicacy.  (For x < 3.5, g(ginv(x)) is
> pretty close to x just as the young and na?ve, such as my very good self --- :-) --- would expect.)
> 
> For x > 3.5, ginv(x) becomes numerically indistinguishable from 1, whence g(ginv(x)) is g(1) = Inf.  Or it *would* be except for the fact that ginv() (for the cloglog link) is designed so that its values are capped at 1 - .Machine$double.eps = 2.220446e-16 (on my machine at least).
> 
> Consequently g(ginv(x)) is capped at g(1 - 2.220446e-16) = 3.584731.
> So as x grows it quickly outstrips g(ginv(x)).
> 
> When x is close to 3.5 g(ginv(x)) is simply a bit numerically wobbly, and not necessarily smaller than x. (Which is why I got a negative value, explicitly -0.00281936, for the lower bound of the range of
> g(predResp) - predLink.
> 
> Another person, who replied to me off-list, suggested plotting
> g(predResp) ~ predLink .  This is indeed illuminating.
> 
> I think that the main thing to take away from all this is that numerical delicacy is always lurking in the bushes, and care should always be taken, especially when logs and exponentials are involved.  It is perilous to assume that *algebraic* identities, like g(ginv(x)) = x,
> will turn out to be *numerically* true.
> 
> Thanks again to Thierry.
> 
> cheers,
> 
> Rolf
> 
> -- 
> Honorary Research Fellow
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From M@@rten@Jung @end|ng |rom m@||box@tu-dre@den@de  Mon Apr  6 23:15:20 2020
From: M@@rten@Jung @end|ng |rom m@||box@tu-dre@den@de (Maarten Jung)
Date: Mon, 6 Apr 2020 23:15:20 +0200
Subject: [R-sig-ME] Appropriate model reduction sequence for factorial
 design in glmmTMB
In-Reply-To: <f622e5ad-a189-3d16-7228-44882f64ee8f@mpi.nl>
References: <CAHr4Dyc=7aizpvnSLKyZ7pb5a81Dj85OVNsyT5Ke4OP75tW4Jw@mail.gmail.com>
 <f622e5ad-a189-3d16-7228-44882f64ee8f@mpi.nl>
Message-ID: <CAHr4DycwbJHfjKMKP6xhbM8W0VMKDEpi8CTd5LmvgNPNKrEn3g@mail.gmail.com>

Hi Phillip,

Emi Tanaka`s slides look promising - thanks.
However, I`m still having a hard time mapping this to my example of a
2x3 within-subjects design.

Best,
Maarten

On Fri, Apr 3, 2020 at 11:53 PM Phillip Alday <phillip.alday at mpi.nl> wrote:
>
> Hi Maarten,
>
> It's been a while and I still haven't had the time to give your post the
> necessary thought to give you a proper answer ....
>
> That said, Dimitri Rizopoulos posted some course notes a while back
> (http://www.drizopoulos.com/courses/EMC/CE08.pdf). I found the
> presentation there quite nice in terms of thinking about symmetry and
> nesting structures. Emi Tanaka also has some great slides on "software
> design for linear mixed model specification" which I also found great
> for thinking about how these structures are represented in the syntax of
> various software.
>
> So I hope my non answer helps a bit ....
>
> Phillip
>
> On 22/10/19 10:01 pm, Maarten Jung wrote:
> > Dear list,
> >
> > Sorry for basically restating my question here [1], but I think it
> > might be worth a separate thread as it might well be much easier to
> > answer with glmmTMB.
> >
> > After going through the posts [2] and [3] again, I identified the
> > following nesting structure (arrows indicate nesting) as the one I
> > want to go with for modelling some new data:
> >
> > m1 -> m2a/m2b/m2c -> m3 -> m4
> >
> > #######################################################
> >
> > library("lme4")
> > data("Machines", package = "MEMSS")
> > d <- Machines
> > mat <- model.matrix(~ 0 + Machine, d)
> > A <- mat[, 1]
> > B <- mat[, 2]
> > C <- mat[, 3]
> >
> > m1 <- lmer(score ~ Machine + (0 + Machine | Worker), d)
> >
> > m2a <- lmer(score ~ Machine + (1 | Worker) + (0 + dummy(Machine, "A")
> > | Worker) +
> >
> >    (0 + dummy(Machine, "B") | Worker) +
> >
> >    (0 + dummy(Machine, "C") | Worker), d)
> >
> > m2b <- lmer(score ~ Machine + (1 | Worker) + (0 + A + B + C || Worker), d)
> >
> > m2c <- afex::lmer_alt(score ~ Machine + (1 | Worker) + (0 + Machine ||
> > Worker), d)
> >
> > # m2a, m2b, and m2c are equivalent
> > all.equal(logLik(m2a), logLik(m2b), logLik(m2c))
> >
> > m3 <- lmer(score ~ Machine + (1 | Worker) + (1 | Worker:Machine), d)
> >
> > m4 <- lmer(score ~ Machine + (1 | Worker), d)
> >
> > #######################################################
> >
> > In my new data there are multiple observations per cell of a (at
> > least) 2x3 within-subjects design.
> > I know that m1 (denoting the two factors with f1 and f2, respectively)
> > would look like
> >
> > lmer(y ~ f1*f2 + (1 + f1*f2 | subject), data)
> >
> > and I think like this in glmmTMB syntax
> >
> > glmmTMB(y ~ f1*f2 + us(f1*f2 | subject), data, REML = TRUE)
> >
> > So now I'm struggling to figure out what m2a (or m2b/m2c) and m3 would
> > look like in the (at least) 2-factorial case.
> > My guess is that m2 would look like this in glmmTMB syntax
> >
> > glmmTMB(y ~ f1*f2 + (1 | subject) + diag(0 + f1*f2 | subject), data,
> > REML = TRUE)
> >
> > and that this might correspond to the following afex:lmer_alt() model
> >
> > afex::lmer_alt(y ~ f1*f2 + (1 | subject) + (0 +  f1*f2 || subject ), data)
> >
> > But I'm not sure about m3.
> >
> > I would be grateful if someone could provide the appropriate glmmTMB
> > syntax (additionally or alternatively, lmer/afex::lmer_alt syntax is
> > still welcome in the other thread).
> >
> > Best,
> > Maarten
> >
> > [1] https://stat.ethz.ch/pipermail/r-sig-mixed-models/2019q4/028222.html
> > [2] https://stat.ethz.ch/pipermail/r-sig-mixed-models/2018q2/026775.html
> > [3] https://stats.stackexchange.com/questions/345842/what-is-the-appropriate-zero-correlation-parameter-model-for-factors-in-lmer
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From |ch|h@r@ @end|ng |rom c@r|eton@edu  Sun Apr 12 00:59:01 2020
From: |ch|h@r@ @end|ng |rom c@r|eton@edu (Laura Chihara)
Date: Sat, 11 Apr 2020 17:59:01 -0500
Subject: [R-sig-ME] lmer warning on certain OS
Message-ID: <d903f51e-d8fc-4c04-2429-bbb18df50934@carleton.edu>

I'm teaching an undergraduate statistics class and was preparing to do 
some simple examples which ran fine the last time I taught. But this 
time, I am getting warning messages (though the summary output looks OK).

These warnings occur on a Dell desktop running Windows 10 as well as a 
Windows Surface 7. I also ran this on RStudio running on a college 
server which is a Unix server (Ubuntu 16.04.6 LTS) and saw the warning. 
However, the warning message does not occur on an Apple machine nor on 
RStudio-Cloud (https://rstudio.cloud).

(I am running the most recent version of R, RStudio and lme4).

#-----------------------------
# R Script attached has the following

library(lme4)
library(nlme)
lmer(height ~ age + (age | Subject), Oxboys) #warning

mean(Oxboys$age)
Oxboys$age2 <- Oxboys$age - mean(Oxboys$age)
lmer(height ~ age2 + (age2|Subject), Oxboys)? #no warning


lmer(distance ~ age + (age |Subject), Orthodont) #warning

#But a more complicated mode:
lmer(distance ~ age*Sex + (age|Subject), Orthodont)
#--------------------------------------------------------------------------------------

#Warning:

Warning message: In checkConv(attr(opt, "derivs"), opt$par, ctrl = 
control$checkConv, : Model failed to converge with max|grad| = 0.110724 
(tol = 0.002, component 1)

#session info

> sessionInfo() R version 3.6.3 (2020-02-29) Platform: 
x86_64-w64-mingw32/x64 (64-bit) Running under: Windows 10 x64 (build 
18362) Matrix products: default Random number generation: RNG: 
Mersenne-Twister Normal: Inversion Sample: Rounding locale: [1] 
LC_COLLATE=English_United States.1252 LC_CTYPE=English_United 
States.1252 [3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C [5] 
LC_TIME=English_United States.1252 attached base packages: [1] stats 
graphics grDevices utils datasets methods base other attached packages: 
[1] nlme_3.1-144 lme4_1.1-21 Matrix_1.2-18 loaded via a namespace (and 
not attached): [1] minqa_1.2.4 MASS_7.3-51.5 compiler_3.6.3 tools_3.6.3 
Rcpp_1.0.4 [6] splines_3.6.3 grid_3.6.3 nloptr_1.2.2.1 boot_1.3-24 
lattice_0.20-38

-- 

####################################################
Laura Chihara
Prof. of Mathematics and Statistics
Dept. of Mathematics and Statistics
One North College St.
Carleton College
Northfield Mn 55057


-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: lmerWarning.R
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20200411/a09366fc/attachment.ksh>

From bbo|ker @end|ng |rom gm@||@com  Sun Apr 12 01:03:10 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sat, 11 Apr 2020 19:03:10 -0400
Subject: [R-sig-ME] lmer warning on certain OS
In-Reply-To: <d903f51e-d8fc-4c04-2429-bbb18df50934@carleton.edu>
References: <d903f51e-d8fc-4c04-2429-bbb18df50934@carleton.edu>
Message-ID: <CABghstRaW4LQ-dToFfmO6Gtx_UuWfuCqhv8X0ujXk+v_7RERsw@mail.gmail.com>

  Does updating to the very latest lme4 version (1.1-23) help with
this?  (Some convergence tolerances have been tightened in this
version.)

  cheers
   Ben Bolker

On Sat, Apr 11, 2020 at 6:59 PM Laura Chihara via R-sig-mixed-models
<r-sig-mixed-models at r-project.org> wrote:
>
> I'm teaching an undergraduate statistics class and was preparing to do
> some simple examples which ran fine the last time I taught. But this
> time, I am getting warning messages (though the summary output looks OK).
>
> These warnings occur on a Dell desktop running Windows 10 as well as a
> Windows Surface 7. I also ran this on RStudio running on a college
> server which is a Unix server (Ubuntu 16.04.6 LTS) and saw the warning.
> However, the warning message does not occur on an Apple machine nor on
> RStudio-Cloud (https://rstudio.cloud).
>
> (I am running the most recent version of R, RStudio and lme4).
>
> #-----------------------------
> # R Script attached has the following
>
> library(lme4)
> library(nlme)
> lmer(height ~ age + (age | Subject), Oxboys) #warning
>
> mean(Oxboys$age)
> Oxboys$age2 <- Oxboys$age - mean(Oxboys$age)
> lmer(height ~ age2 + (age2|Subject), Oxboys)  #no warning
>
>
> lmer(distance ~ age + (age |Subject), Orthodont) #warning
>
> #But a more complicated mode:
> lmer(distance ~ age*Sex + (age|Subject), Orthodont)
> #--------------------------------------------------------------------------------------
>
> #Warning:
>
> Warning message: In checkConv(attr(opt, "derivs"), opt$par, ctrl =
> control$checkConv, : Model failed to converge with max|grad| = 0.110724
> (tol = 0.002, component 1)
>
> #session info
>
> > sessionInfo() R version 3.6.3 (2020-02-29) Platform:
> x86_64-w64-mingw32/x64 (64-bit) Running under: Windows 10 x64 (build
> 18362) Matrix products: default Random number generation: RNG:
> Mersenne-Twister Normal: Inversion Sample: Rounding locale: [1]
> LC_COLLATE=English_United States.1252 LC_CTYPE=English_United
> States.1252 [3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C [5]
> LC_TIME=English_United States.1252 attached base packages: [1] stats
> graphics grDevices utils datasets methods base other attached packages:
> [1] nlme_3.1-144 lme4_1.1-21 Matrix_1.2-18 loaded via a namespace (and
> not attached): [1] minqa_1.2.4 MASS_7.3-51.5 compiler_3.6.3 tools_3.6.3
> Rcpp_1.0.4 [6] splines_3.6.3 grid_3.6.3 nloptr_1.2.2.1 boot_1.3-24
> lattice_0.20-38
>
> --
>
> ####################################################
> Laura Chihara
> Prof. of Mathematics and Statistics
> Dept. of Mathematics and Statistics
> One North College St.
> Carleton College
> Northfield Mn 55057
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From KELLY@SLAUGHTER @end|ng |rom tcu@edu  Mon Apr 13 01:34:02 2020
From: KELLY@SLAUGHTER @end|ng |rom tcu@edu (Slaughter, Kelly)
Date: Sun, 12 Apr 2020 23:34:02 +0000
Subject: [R-sig-ME] Controlling for self-selection bias / endogeneity in
 mixed models
Message-ID: <24e63f45d3ff4a98b1e8b451fb8af229@tcu.edu>

Hi all -

I have a concern regarding self-selection/omitted variable bias. I have a longitudinal/repeated measures model, theorizing about a relationship between treatment/control and effort, represented in nlme syntax as:

EQ 1) log(effort measured in time) ~ treatment*scale(experience), random = ~1|subject

Treatment/control is selected by the subject, it is not randomized, thus raising endogeneity concerns. My background is applied econ, so as I learn the mixed model domain, I expected to find the mixed model equivalent of instrumental variables/inverse Mills ratio, etc. Yet there is surprisingly (to me) limited material addressing this issue. The best reference material I found is in fact a thread in this mailing list from October 2016 and the papers referenced within, leading to Bell, Fairbrother, and Jones (2019). My first impression is that I should employ a within-between random effects (REWB)model -

EQ 2) log(effort measured in time) ~ treatment*scale(experience) + experience_between + experience_within, random = experience_within + scale(experience) | subject

If I understand correctly, the intuition is that the addition of a group mean explanatory variable "breaks out" the variability that would be associated with an omitted variable / error term. Per Bell et al, "there can be no correlation between level 1 variables included in the model and the level 2 random effects...unchanging and/or unmeasured characteristics of an individual (such as intelligence, ability, etc.) will be controlled out of the estimate of the within effect."

So, no concern between the subject (level 2) and treatment (level 1) via REWB, wonderful!

Bell et al caution, "...in a REWB/Mundlak models, unmeasured level 2 characteristics can cause bias in the estimates of between effects and effects of other level 2 variables."

Not an issue for me - I am not concerned with level 2, I include subject to address the IID violation but am interested in population, not subject, performance.

Bell et al continue, "However, unobserved time-varying characteristics can still cause biases at level 1 in either an FE or a REWB/Mundlak model."

Though conceptually my treatment variable is time-varying (it can change across time within a subject), as a practical/empirical matter, the treatment is unchanging within the subject - subjects have no reason to change / would prefer to keep the choice constant. Of 80k records, treatment switches within a subject occur in about a dozen records.

So, I think I have my solution. However, if a reviewer is not happy with the with-in / between REWB solution (worried about the level 1 bias), I can further defend EQ 2 via its random coefficient/slope, if I understand the Oct 2016 thread correctly.

So, my questions are:

(1) Is the above correctly reasoned?

(2) If the random slope model is a further defense against self-selection bias, could someone provide an intuitive explanation as to why? Is the idea that by allowing slopes to vary, there is no endogeneity problem to solve as the very structure of the model makes the correlated errors concern irrelevant?

Other solutions I explore include a Mundlak model, but per Bell et al, the Mundlak models are not meaningful for repeated measures. Also, it appears that the brms package appears to support mixed modeling using instrumental variables, something I am more comfortable with per my background, but strong instrumental variables are hard to find in the wild!

Thank you! - Kelly


	[[alternative HTML version deleted]]


From jdpoe223 @end|ng |rom gm@||@com  Mon Apr 13 02:36:24 2020
From: jdpoe223 @end|ng |rom gm@||@com (John Poe)
Date: Sun, 12 Apr 2020 20:36:24 -0400
Subject: [R-sig-ME] Controlling for self-selection bias / endogeneity in
 mixed models
In-Reply-To: <24e63f45d3ff4a98b1e8b451fb8af229@tcu.edu>
References: <24e63f45d3ff4a98b1e8b451fb8af229@tcu.edu>
Message-ID: <CACDpxFD=JPhHemhZV5dyix7C6UR_37np90ff_iuqyAWa+wD5KA@mail.gmail.com>

Hi Kelly,

It sounds like you've got correct reasoning on the need for a multilevel
model if your variable of interest is time invariant.

Can you post a link to the thread you're referencing?

A bit of clarity on the flavor(s) of endogeneity that concern you might be
helpful. The omitted variable bias issues solved by group mean centering
and the Mundlak device are mostly from model mis/underspecification whereas
sample selection is a fundamentally different mechanism. Both are common
sources of endogeneity recognized as such in different pockets of econ but
they tend to be seen as fundamentally different (often conceptually
unrelated) problems in other fields. Econ subsumes omitted variables, joint
causation, measurement error, and sample selection under the endogeneity
umbrella because they all cause correlation between X and the error but
other fields don't make the same connection. For instance, early panel data
work talked about Mundlak devices as "instruments" in the same way that
dynamic panel data models talk about lags and first differences as
instruments but they aren't traditional instrumental variables that you'd
find in the wild and arguably wouldn't pass the exclusion restriction test
outside of panel data. They call them instruments because they instrument
the endogeneity but they aren't "instrumental variables" in the common
parlance.

It's not clear to me if you are referring to general omitted variable bias
whereby you don't have all the appropriate variables in the model or sample
selection bias a la Heckman whereby the sample under study is
systematically different from the population to which you would like to
make inferences and thus needs some kind of complex propensity to choose A
or B style correction like with the standard selection model. I'm not clear
specifically because you referenced the inverse mills ratio but it *sounds*
like you just think you are possibly missing some set of confounders due to
the lack of randomization. If you do have sample selection bias you can use
a multilevel variant of a heckman selection model with random effects in
the outcome and selection equations. See Grilli, L., & Rampichini, C.
(2010). Selection bias in linear mixed models. *Metron, 68*(3), 309-329 for
the best discussion of the topic that I've read. Most multilevel modeling
work with this kind of problem is based on multilevel propensity score
matching which is a close cousin of multilevel Heckman selection models as
the inverse mills ratio and the propensity score are related.

You're right that the addition of group means per Mundlak segregates the
within and between effects into two different sets of betas when they would
otherwise be a weighted average. It's just a reparamaritization of the
dummy variable version of fixed effects. It is mathematically impossible in
a linear model for a group mean centered multilevel model to return
different within group beta coefficients than the standard FE model. That
doesn't mean that both of them aren't wrong because of cross-level
interactions, measurement error, selection bias and what not but they would
both be wrong in identical ways. You can directly test that they are
identical with a version of a Hausman test comparing the within group betas
with a chi2 test. The degrees of freedom calculation will be off from the
regular test because the between effects add extra but the within effects
will be identical to rounding error so it really won't matter. You can also
just do a Mundlak variation on the test. All panel data econometrics
textbooks outline this and you can justify the modeling strategy that way
regardless of reviewer misconceptions.

If the FE or group mean centered MLM are both wrong and there's some kind
of interactive effect still at work then a random coefficient will likely
show up as mattering for model fit with something like an LR test. If beta
(X_i-Xbar_j) on Y does not vary as a function of group per an LR test or
something fancier like WAIC then it is reasonable (but not infallible)
evidence that you don't have group heterogeneity-related omitted variable
bias which is what economists would typically be concerned about in this
context. You can still have other kinds of bias at work just like with any
other kind of observational model. The random coefficient in this context
is a regularized interactive fixed effect in econ jargon whereby you are
interacting the grouping structure with whatever X you want and getting a
distribution of effects. Fundamentally, it's like saying you have some kind
of conditional relationship between group/person and X and just interacting
them. It's slightly complicated by the fact that empirical bayes shrinkage
exists but if you have balanced panels then it's mostly a non issue.



On Sun, Apr 12, 2020 at 7:34 PM Slaughter, Kelly <KELLY.SLAUGHTER at tcu.edu>
wrote:

> Hi all -
>
> I have a concern regarding self-selection/omitted variable bias. I have a
> longitudinal/repeated measures model, theorizing about a relationship
> between treatment/control and effort, represented in nlme syntax as:
>
> EQ 1) log(effort measured in time) ~ treatment*scale(experience), random =
> ~1|subject
>
> Treatment/control is selected by the subject, it is not randomized, thus
> raising endogeneity concerns. My background is applied econ, so as I learn
> the mixed model domain, I expected to find the mixed model equivalent of
> instrumental variables/inverse Mills ratio, etc. Yet there is surprisingly
> (to me) limited material addressing this issue. The best reference material
> I found is in fact a thread in this mailing list from October 2016 and the
> papers referenced within, leading to Bell, Fairbrother, and Jones (2019).
> My first impression is that I should employ a within-between random effects
> (REWB)model -
>
> EQ 2) log(effort measured in time) ~ treatment*scale(experience) +
> experience_between + experience_within, random = experience_within +
> scale(experience) | subject
>
> If I understand correctly, the intuition is that the addition of a group
> mean explanatory variable "breaks out" the variability that would be
> associated with an omitted variable / error term. Per Bell et al, "there
> can be no correlation between level 1 variables included in the model and
> the level 2 random effects...unchanging and/or unmeasured characteristics
> of an individual (such as intelligence, ability, etc.) will be controlled
> out of the estimate of the within effect."
>
> So, no concern between the subject (level 2) and treatment (level 1) via
> REWB, wonderful!
>
> Bell et al caution, "...in a REWB/Mundlak models, unmeasured level 2
> characteristics can cause bias in the estimates of between effects and
> effects of other level 2 variables."
>
> Not an issue for me - I am not concerned with level 2, I include subject
> to address the IID violation but am interested in population, not subject,
> performance.
>
> Bell et al continue, "However, unobserved time-varying characteristics can
> still cause biases at level 1 in either an FE or a REWB/Mundlak model."
>
> Though conceptually my treatment variable is time-varying (it can change
> across time within a subject), as a practical/empirical matter, the
> treatment is unchanging within the subject - subjects have no reason to
> change / would prefer to keep the choice constant. Of 80k records,
> treatment switches within a subject occur in about a dozen records.
>
> So, I think I have my solution. However, if a reviewer is not happy with
> the with-in / between REWB solution (worried about the level 1 bias), I can
> further defend EQ 2 via its random coefficient/slope, if I understand the
> Oct 2016 thread correctly.
>
> So, my questions are:
>
> (1) Is the above correctly reasoned?
>
> (2) If the random slope model is a further defense against self-selection
> bias, could someone provide an intuitive explanation as to why? Is the idea
> that by allowing slopes to vary, there is no endogeneity problem to solve
> as the very structure of the model makes the correlated errors concern
> irrelevant?
>
> Other solutions I explore include a Mundlak model, but per Bell et al, the
> Mundlak models are not meaningful for repeated measures. Also, it appears
> that the brms package appears to support mixed modeling using instrumental
> variables, something I am more comfortable with per my background, but
> strong instrumental variables are hard to find in the wild!
>
> Thank you! - Kelly
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Mon Apr 13 02:45:45 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sun, 12 Apr 2020 20:45:45 -0400
Subject: [R-sig-ME] Controlling for self-selection bias / endogeneity in
 mixed models
In-Reply-To: <CACDpxFD=JPhHemhZV5dyix7C6UR_37np90ff_iuqyAWa+wD5KA@mail.gmail.com>
References: <24e63f45d3ff4a98b1e8b451fb8af229@tcu.edu>
 <CACDpxFD=JPhHemhZV5dyix7C6UR_37np90ff_iuqyAWa+wD5KA@mail.gmail.com>
Message-ID: <CABghstR+YX=4goLUgyiMAeCB+Ker4Rcw9Jx7qsj1XjQisOd5Kg@mail.gmail.com>

  Wow, this is the kind of content I come here for.  (It will take me
a while to digest this ...) Thank you!

On Sun, Apr 12, 2020 at 8:36 PM John Poe <jdpoe223 at gmail.com> wrote:
>
> Hi Kelly,
>
> It sounds like you've got correct reasoning on the need for a multilevel
> model if your variable of interest is time invariant.
>
> Can you post a link to the thread you're referencing?
>
> A bit of clarity on the flavor(s) of endogeneity that concern you might be
> helpful. The omitted variable bias issues solved by group mean centering
> and the Mundlak device are mostly from model mis/underspecification whereas
> sample selection is a fundamentally different mechanism. Both are common
> sources of endogeneity recognized as such in different pockets of econ but
> they tend to be seen as fundamentally different (often conceptually
> unrelated) problems in other fields. Econ subsumes omitted variables, joint
> causation, measurement error, and sample selection under the endogeneity
> umbrella because they all cause correlation between X and the error but
> other fields don't make the same connection. For instance, early panel data
> work talked about Mundlak devices as "instruments" in the same way that
> dynamic panel data models talk about lags and first differences as
> instruments but they aren't traditional instrumental variables that you'd
> find in the wild and arguably wouldn't pass the exclusion restriction test
> outside of panel data. They call them instruments because they instrument
> the endogeneity but they aren't "instrumental variables" in the common
> parlance.
>
> It's not clear to me if you are referring to general omitted variable bias
> whereby you don't have all the appropriate variables in the model or sample
> selection bias a la Heckman whereby the sample under study is
> systematically different from the population to which you would like to
> make inferences and thus needs some kind of complex propensity to choose A
> or B style correction like with the standard selection model. I'm not clear
> specifically because you referenced the inverse mills ratio but it *sounds*
> like you just think you are possibly missing some set of confounders due to
> the lack of randomization. If you do have sample selection bias you can use
> a multilevel variant of a heckman selection model with random effects in
> the outcome and selection equations. See Grilli, L., & Rampichini, C.
> (2010). Selection bias in linear mixed models. *Metron, 68*(3), 309-329 for
> the best discussion of the topic that I've read. Most multilevel modeling
> work with this kind of problem is based on multilevel propensity score
> matching which is a close cousin of multilevel Heckman selection models as
> the inverse mills ratio and the propensity score are related.
>
> You're right that the addition of group means per Mundlak segregates the
> within and between effects into two different sets of betas when they would
> otherwise be a weighted average. It's just a reparamaritization of the
> dummy variable version of fixed effects. It is mathematically impossible in
> a linear model for a group mean centered multilevel model to return
> different within group beta coefficients than the standard FE model. That
> doesn't mean that both of them aren't wrong because of cross-level
> interactions, measurement error, selection bias and what not but they would
> both be wrong in identical ways. You can directly test that they are
> identical with a version of a Hausman test comparing the within group betas
> with a chi2 test. The degrees of freedom calculation will be off from the
> regular test because the between effects add extra but the within effects
> will be identical to rounding error so it really won't matter. You can also
> just do a Mundlak variation on the test. All panel data econometrics
> textbooks outline this and you can justify the modeling strategy that way
> regardless of reviewer misconceptions.
>
> If the FE or group mean centered MLM are both wrong and there's some kind
> of interactive effect still at work then a random coefficient will likely
> show up as mattering for model fit with something like an LR test. If beta
> (X_i-Xbar_j) on Y does not vary as a function of group per an LR test or
> something fancier like WAIC then it is reasonable (but not infallible)
> evidence that you don't have group heterogeneity-related omitted variable
> bias which is what economists would typically be concerned about in this
> context. You can still have other kinds of bias at work just like with any
> other kind of observational model. The random coefficient in this context
> is a regularized interactive fixed effect in econ jargon whereby you are
> interacting the grouping structure with whatever X you want and getting a
> distribution of effects. Fundamentally, it's like saying you have some kind
> of conditional relationship between group/person and X and just interacting
> them. It's slightly complicated by the fact that empirical bayes shrinkage
> exists but if you have balanced panels then it's mostly a non issue.
>
>
>
> On Sun, Apr 12, 2020 at 7:34 PM Slaughter, Kelly <KELLY.SLAUGHTER at tcu.edu>
> wrote:
>
> > Hi all -
> >
> > I have a concern regarding self-selection/omitted variable bias. I have a
> > longitudinal/repeated measures model, theorizing about a relationship
> > between treatment/control and effort, represented in nlme syntax as:
> >
> > EQ 1) log(effort measured in time) ~ treatment*scale(experience), random =
> > ~1|subject
> >
> > Treatment/control is selected by the subject, it is not randomized, thus
> > raising endogeneity concerns. My background is applied econ, so as I learn
> > the mixed model domain, I expected to find the mixed model equivalent of
> > instrumental variables/inverse Mills ratio, etc. Yet there is surprisingly
> > (to me) limited material addressing this issue. The best reference material
> > I found is in fact a thread in this mailing list from October 2016 and the
> > papers referenced within, leading to Bell, Fairbrother, and Jones (2019).
> > My first impression is that I should employ a within-between random effects
> > (REWB)model -
> >
> > EQ 2) log(effort measured in time) ~ treatment*scale(experience) +
> > experience_between + experience_within, random = experience_within +
> > scale(experience) | subject
> >
> > If I understand correctly, the intuition is that the addition of a group
> > mean explanatory variable "breaks out" the variability that would be
> > associated with an omitted variable / error term. Per Bell et al, "there
> > can be no correlation between level 1 variables included in the model and
> > the level 2 random effects...unchanging and/or unmeasured characteristics
> > of an individual (such as intelligence, ability, etc.) will be controlled
> > out of the estimate of the within effect."
> >
> > So, no concern between the subject (level 2) and treatment (level 1) via
> > REWB, wonderful!
> >
> > Bell et al caution, "...in a REWB/Mundlak models, unmeasured level 2
> > characteristics can cause bias in the estimates of between effects and
> > effects of other level 2 variables."
> >
> > Not an issue for me - I am not concerned with level 2, I include subject
> > to address the IID violation but am interested in population, not subject,
> > performance.
> >
> > Bell et al continue, "However, unobserved time-varying characteristics can
> > still cause biases at level 1 in either an FE or a REWB/Mundlak model."
> >
> > Though conceptually my treatment variable is time-varying (it can change
> > across time within a subject), as a practical/empirical matter, the
> > treatment is unchanging within the subject - subjects have no reason to
> > change / would prefer to keep the choice constant. Of 80k records,
> > treatment switches within a subject occur in about a dozen records.
> >
> > So, I think I have my solution. However, if a reviewer is not happy with
> > the with-in / between REWB solution (worried about the level 1 bias), I can
> > further defend EQ 2 via its random coefficient/slope, if I understand the
> > Oct 2016 thread correctly.
> >
> > So, my questions are:
> >
> > (1) Is the above correctly reasoned?
> >
> > (2) If the random slope model is a further defense against self-selection
> > bias, could someone provide an intuitive explanation as to why? Is the idea
> > that by allowing slopes to vary, there is no endogeneity problem to solve
> > as the very structure of the model makes the correlated errors concern
> > irrelevant?
> >
> > Other solutions I explore include a Mundlak model, but per Bell et al, the
> > Mundlak models are not meaningful for repeated measures. Also, it appears
> > that the brms package appears to support mixed modeling using instrumental
> > variables, something I am more comfortable with per my background, but
> > strong instrumental variables are hard to find in the wild!
> >
> > Thank you! - Kelly
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From KELLY@SLAUGHTER @end|ng |rom tcu@edu  Mon Apr 13 02:56:58 2020
From: KELLY@SLAUGHTER @end|ng |rom tcu@edu (Slaughter, Kelly)
Date: Mon, 13 Apr 2020 00:56:58 +0000
Subject: [R-sig-ME] Controlling for self-selection bias / endogeneity in
 mixed models
In-Reply-To: <CABghstR+YX=4goLUgyiMAeCB+Ker4Rcw9Jx7qsj1XjQisOd5Kg@mail.gmail.com>
References: <24e63f45d3ff4a98b1e8b451fb8af229@tcu.edu>
 <CACDpxFD=JPhHemhZV5dyix7C6UR_37np90ff_iuqyAWa+wD5KA@mail.gmail.com>
 <CABghstR+YX=4goLUgyiMAeCB+Ker4Rcw9Jx7qsj1XjQisOd5Kg@mail.gmail.com>
Message-ID: <489a728021ea403fb0dbd36d1d8a4787@tcu.edu>

Thanks for the extensive reply, John! Before I attempt to absorb it all, let me offer a couple of quick answers to your questions just to be sure the thread does not spiral in multiple directions :)

(1)	The beginning of the thread I reference can be found here: https://hypatia.math.ethz.ch/pipermail/r-sig-mixed-models/2016q4/025147.html

(2)	I am referring to omitted variable bias, sorry for the confusion. My treatment / control is ownership of multiple financial accounts / ownership of single accounts. So perhaps let's say IQ tends to make someone more likely to hold multiple accounts (treatment) AND allows them to expend less effort in researching financial trades (outcome variable), whereas I am theorizing that multiple accounts themselves reduce effort directly.

BTW, Ben, thank you for your extensive support across multiple sites in helping the general public with mixed models in R. I have relied upon an EXTENSIVE number of your answers to mixed model questions when developing my models.

-----Original Message-----
From: Ben Bolker <bbolker at gmail.com> 
Sent: Sunday, April 12, 2020 7:46 PM
To: John Poe <jdpoe223 at gmail.com>
Cc: Slaughter, Kelly <KELLY.SLAUGHTER at tcu.edu>; r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Controlling for self-selection bias / endogeneity in mixed models

  Wow, this is the kind of content I come here for.  (It will take me a while to digest this ...) Thank you!

On Sun, Apr 12, 2020 at 8:36 PM John Poe <jdpoe223 at gmail.com> wrote:
>
> Hi Kelly,
>
> It sounds like you've got correct reasoning on the need for a 
> multilevel model if your variable of interest is time invariant.
>
> Can you post a link to the thread you're referencing?
>
> A bit of clarity on the flavor(s) of endogeneity that concern you 
> might be helpful. The omitted variable bias issues solved by group 
> mean centering and the Mundlak device are mostly from model 
> mis/underspecification whereas sample selection is a fundamentally 
> different mechanism. Both are common sources of endogeneity recognized 
> as such in different pockets of econ but they tend to be seen as 
> fundamentally different (often conceptually
> unrelated) problems in other fields. Econ subsumes omitted variables, 
> joint causation, measurement error, and sample selection under the 
> endogeneity umbrella because they all cause correlation between X and 
> the error but other fields don't make the same connection. For 
> instance, early panel data work talked about Mundlak devices as 
> "instruments" in the same way that dynamic panel data models talk 
> about lags and first differences as instruments but they aren't 
> traditional instrumental variables that you'd find in the wild and 
> arguably wouldn't pass the exclusion restriction test outside of panel 
> data. They call them instruments because they instrument the 
> endogeneity but they aren't "instrumental variables" in the common parlance.
>
> It's not clear to me if you are referring to general omitted variable 
> bias whereby you don't have all the appropriate variables in the model 
> or sample selection bias a la Heckman whereby the sample under study 
> is systematically different from the population to which you would 
> like to make inferences and thus needs some kind of complex propensity 
> to choose A or B style correction like with the standard selection 
> model. I'm not clear specifically because you referenced the inverse 
> mills ratio but it *sounds* like you just think you are possibly 
> missing some set of confounders due to the lack of randomization. If 
> you do have sample selection bias you can use a multilevel variant of 
> a heckman selection model with random effects in the outcome and selection equations. See Grilli, L., & Rampichini, C.
> (2010). Selection bias in linear mixed models. *Metron, 68*(3), 
> 309-329 for the best discussion of the topic that I've read. Most 
> multilevel modeling work with this kind of problem is based on 
> multilevel propensity score matching which is a close cousin of 
> multilevel Heckman selection models as the inverse mills ratio and the propensity score are related.
>
> You're right that the addition of group means per Mundlak segregates 
> the within and between effects into two different sets of betas when 
> they would otherwise be a weighted average. It's just a 
> reparamaritization of the dummy variable version of fixed effects. It 
> is mathematically impossible in a linear model for a group mean 
> centered multilevel model to return different within group beta 
> coefficients than the standard FE model. That doesn't mean that both 
> of them aren't wrong because of cross-level interactions, measurement 
> error, selection bias and what not but they would both be wrong in 
> identical ways. You can directly test that they are identical with a 
> version of a Hausman test comparing the within group betas with a chi2 
> test. The degrees of freedom calculation will be off from the regular 
> test because the between effects add extra but the within effects will 
> be identical to rounding error so it really won't matter. You can also 
> just do a Mundlak variation on the test. All panel data econometrics 
> textbooks outline this and you can justify the modeling strategy that way regardless of reviewer misconceptions.
>
> If the FE or group mean centered MLM are both wrong and there's some 
> kind of interactive effect still at work then a random coefficient 
> will likely show up as mattering for model fit with something like an 
> LR test. If beta
> (X_i-Xbar_j) on Y does not vary as a function of group per an LR test 
> or something fancier like WAIC then it is reasonable (but not 
> infallible) evidence that you don't have group heterogeneity-related 
> omitted variable bias which is what economists would typically be 
> concerned about in this context. You can still have other kinds of 
> bias at work just like with any other kind of observational model. The 
> random coefficient in this context is a regularized interactive fixed 
> effect in econ jargon whereby you are interacting the grouping 
> structure with whatever X you want and getting a distribution of 
> effects. Fundamentally, it's like saying you have some kind of 
> conditional relationship between group/person and X and just 
> interacting them. It's slightly complicated by the fact that empirical bayes shrinkage exists but if you have balanced panels then it's mostly a non issue.
>
>
>
> On Sun, Apr 12, 2020 at 7:34 PM Slaughter, Kelly 
> <KELLY.SLAUGHTER at tcu.edu>
> wrote:
>
> > Hi all -
> >
> > I have a concern regarding self-selection/omitted variable bias. I 
> > have a longitudinal/repeated measures model, theorizing about a 
> > relationship between treatment/control and effort, represented in nlme syntax as:
> >
> > EQ 1) log(effort measured in time) ~ treatment*scale(experience), 
> > random = ~1|subject
> >
> > Treatment/control is selected by the subject, it is not randomized, 
> > thus raising endogeneity concerns. My background is applied econ, so 
> > as I learn the mixed model domain, I expected to find the mixed 
> > model equivalent of instrumental variables/inverse Mills ratio, etc. 
> > Yet there is surprisingly (to me) limited material addressing this 
> > issue. The best reference material I found is in fact a thread in 
> > this mailing list from October 2016 and the papers referenced within, leading to Bell, Fairbrother, and Jones (2019).
> > My first impression is that I should employ a within-between random 
> > effects (REWB)model -
> >
> > EQ 2) log(effort measured in time) ~ treatment*scale(experience) + 
> > experience_between + experience_within, random = experience_within +
> > scale(experience) | subject
> >
> > If I understand correctly, the intuition is that the addition of a 
> > group mean explanatory variable "breaks out" the variability that 
> > would be associated with an omitted variable / error term. Per Bell 
> > et al, "there can be no correlation between level 1 variables 
> > included in the model and the level 2 random effects...unchanging 
> > and/or unmeasured characteristics of an individual (such as 
> > intelligence, ability, etc.) will be controlled out of the estimate of the within effect."
> >
> > So, no concern between the subject (level 2) and treatment (level 1) 
> > via REWB, wonderful!
> >
> > Bell et al caution, "...in a REWB/Mundlak models, unmeasured level 2 
> > characteristics can cause bias in the estimates of between effects 
> > and effects of other level 2 variables."
> >
> > Not an issue for me - I am not concerned with level 2, I include 
> > subject to address the IID violation but am interested in 
> > population, not subject, performance.
> >
> > Bell et al continue, "However, unobserved time-varying 
> > characteristics can still cause biases at level 1 in either an FE or a REWB/Mundlak model."
> >
> > Though conceptually my treatment variable is time-varying (it can 
> > change across time within a subject), as a practical/empirical 
> > matter, the treatment is unchanging within the subject - subjects 
> > have no reason to change / would prefer to keep the choice constant. 
> > Of 80k records, treatment switches within a subject occur in about a dozen records.
> >
> > So, I think I have my solution. However, if a reviewer is not happy 
> > with the with-in / between REWB solution (worried about the level 1 
> > bias), I can further defend EQ 2 via its random coefficient/slope, 
> > if I understand the Oct 2016 thread correctly.
> >
> > So, my questions are:
> >
> > (1) Is the above correctly reasoned?
> >
> > (2) If the random slope model is a further defense against 
> > self-selection bias, could someone provide an intuitive explanation 
> > as to why? Is the idea that by allowing slopes to vary, there is no 
> > endogeneity problem to solve as the very structure of the model 
> > makes the correlated errors concern irrelevant?
> >
> > Other solutions I explore include a Mundlak model, but per Bell et 
> > al, the Mundlak models are not meaningful for repeated measures. 
> > Also, it appears that the brms package appears to support mixed 
> > modeling using instrumental variables, something I am more 
> > comfortable with per my background, but strong instrumental variables are hard to find in the wild!
> >
> > Thank you! - Kelly
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list 
> > https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_ma
> > ilman_listinfo_r-2Dsig-2Dmixed-2Dmodels&d=DwIBaQ&c=7Q-FWLBTAxn3T_E3H
> > WrzGYJrC4RvUoWDrzTlitGRH_A&r=t-hV_EQcvMxUUCFqXmGPFL3N6XmAH6-xWI5Xpn-
> > HlYI&m=QIwJJAou0NQyfk892Wz-BodAH5I2A4aX08LX_ruukNk&s=4wSiK6P7-7_81bm
> > iLGX2F07zLv-M28Gd-4vDdwHogyk&e=
> >
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dsig-2Dmixed-2Dmodels&d=DwIBaQ&c=7Q-FWLBTAxn3T_E3HWrzG
> YJrC4RvUoWDrzTlitGRH_A&r=t-hV_EQcvMxUUCFqXmGPFL3N6XmAH6-xWI5Xpn-HlYI&m
> =QIwJJAou0NQyfk892Wz-BodAH5I2A4aX08LX_ruukNk&s=4wSiK6P7-7_81bmiLGX2F07
> zLv-M28Gd-4vDdwHogyk&e=

From jdpoe223 @end|ng |rom gm@||@com  Mon Apr 13 03:21:30 2020
From: jdpoe223 @end|ng |rom gm@||@com (John Poe)
Date: Sun, 12 Apr 2020 21:21:30 -0400
Subject: [R-sig-ME] Controlling for self-selection bias / endogeneity in
 mixed models
In-Reply-To: <489a728021ea403fb0dbd36d1d8a4787@tcu.edu>
References: <24e63f45d3ff4a98b1e8b451fb8af229@tcu.edu>
 <CACDpxFD=JPhHemhZV5dyix7C6UR_37np90ff_iuqyAWa+wD5KA@mail.gmail.com>
 <CABghstR+YX=4goLUgyiMAeCB+Ker4Rcw9Jx7qsj1XjQisOd5Kg@mail.gmail.com>
 <489a728021ea403fb0dbd36d1d8a4787@tcu.edu>
Message-ID: <CACDpxFDz+uV8yjTbNzWtQa+zAx4L5QYTHw=mDY265cRUqAVysg@mail.gmail.com>

Ah, okay I see the problem now. This kind of multilevel causal inference
problem is a bit hard for me to conceptualize. I usually think about them
with DAGs.

I *think* you're going to end up trying to model the selection mechanism
itself via something like propensity score weighting unless you can find a
good natural IV. In this context the propensity score is an artificial
instrumental variable (much like randomization is an instrument). You can
find a good explanation of IPW in Hernan and Robins
https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/ which
includes some detail on longitudinal models though that is geared to time
varying treatments. I think you'll just be focusing on building a
propensity score at the time of the choice since it never changes which
simplifies it down to the first cross-section of data. I'm familiar with 15
or 20ish papers on multilevel propensity score modeling so they are easy to
find. One that you might look at is Arpino, B. and Mealli, F., 2011. The
specification of the propensity score in multilevel observational
studies. *Computational
Statistics & Data Analysis*, *55*(4), pp.1770-1780. Arpino has several
papers on the topic including a statistics in medicine article that's also
pretty good. Causal identification is going to be based on how good the
propensity score is and there's no real way around that. Once you get the
weighted (or matched if you want to go that route) data you can put it in a
regular multilevel model.

It's possible that you could model this with cross-level interactions
between ownership and all the level 1 stuff in the model but that would get
messy. I think the propensity score route is at least more straightforward
to interpret. If you had pre-treatment outcome data of some kind then you
could do something like a synthetic control method but I don't know if
that's feasible with what you've got.

On Sun, Apr 12, 2020 at 8:56 PM Slaughter, Kelly <KELLY.SLAUGHTER at tcu.edu>
wrote:

> Thanks for the extensive reply, John! Before I attempt to absorb it all,
> let me offer a couple of quick answers to your questions just to be sure
> the thread does not spiral in multiple directions :)
>
> (1)     The beginning of the thread I reference can be found here:
> https://hypatia.math.ethz.ch/pipermail/r-sig-mixed-models/2016q4/025147.html
>
> (2)     I am referring to omitted variable bias, sorry for the confusion.
> My treatment / control is ownership of multiple financial accounts /
> ownership of single accounts. So perhaps let's say IQ tends to make someone
> more likely to hold multiple accounts (treatment) AND allows them to expend
> less effort in researching financial trades (outcome variable), whereas I
> am theorizing that multiple accounts themselves reduce effort directly.
>
> BTW, Ben, thank you for your extensive support across multiple sites in
> helping the general public with mixed models in R. I have relied upon an
> EXTENSIVE number of your answers to mixed model questions when developing
> my models.
>
> -----Original Message-----
> From: Ben Bolker <bbolker at gmail.com>
> Sent: Sunday, April 12, 2020 7:46 PM
> To: John Poe <jdpoe223 at gmail.com>
> Cc: Slaughter, Kelly <KELLY.SLAUGHTER at tcu.edu>;
> r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Controlling for self-selection bias / endogeneity
> in mixed models
>
>   Wow, this is the kind of content I come here for.  (It will take me a
> while to digest this ...) Thank you!
>
> On Sun, Apr 12, 2020 at 8:36 PM John Poe <jdpoe223 at gmail.com> wrote:
> >
> > Hi Kelly,
> >
> > It sounds like you've got correct reasoning on the need for a
> > multilevel model if your variable of interest is time invariant.
> >
> > Can you post a link to the thread you're referencing?
> >
> > A bit of clarity on the flavor(s) of endogeneity that concern you
> > might be helpful. The omitted variable bias issues solved by group
> > mean centering and the Mundlak device are mostly from model
> > mis/underspecification whereas sample selection is a fundamentally
> > different mechanism. Both are common sources of endogeneity recognized
> > as such in different pockets of econ but they tend to be seen as
> > fundamentally different (often conceptually
> > unrelated) problems in other fields. Econ subsumes omitted variables,
> > joint causation, measurement error, and sample selection under the
> > endogeneity umbrella because they all cause correlation between X and
> > the error but other fields don't make the same connection. For
> > instance, early panel data work talked about Mundlak devices as
> > "instruments" in the same way that dynamic panel data models talk
> > about lags and first differences as instruments but they aren't
> > traditional instrumental variables that you'd find in the wild and
> > arguably wouldn't pass the exclusion restriction test outside of panel
> > data. They call them instruments because they instrument the
> > endogeneity but they aren't "instrumental variables" in the common
> parlance.
> >
> > It's not clear to me if you are referring to general omitted variable
> > bias whereby you don't have all the appropriate variables in the model
> > or sample selection bias a la Heckman whereby the sample under study
> > is systematically different from the population to which you would
> > like to make inferences and thus needs some kind of complex propensity
> > to choose A or B style correction like with the standard selection
> > model. I'm not clear specifically because you referenced the inverse
> > mills ratio but it *sounds* like you just think you are possibly
> > missing some set of confounders due to the lack of randomization. If
> > you do have sample selection bias you can use a multilevel variant of
> > a heckman selection model with random effects in the outcome and
> selection equations. See Grilli, L., & Rampichini, C.
> > (2010). Selection bias in linear mixed models. *Metron, 68*(3),
> > 309-329 for the best discussion of the topic that I've read. Most
> > multilevel modeling work with this kind of problem is based on
> > multilevel propensity score matching which is a close cousin of
> > multilevel Heckman selection models as the inverse mills ratio and the
> propensity score are related.
> >
> > You're right that the addition of group means per Mundlak segregates
> > the within and between effects into two different sets of betas when
> > they would otherwise be a weighted average. It's just a
> > reparamaritization of the dummy variable version of fixed effects. It
> > is mathematically impossible in a linear model for a group mean
> > centered multilevel model to return different within group beta
> > coefficients than the standard FE model. That doesn't mean that both
> > of them aren't wrong because of cross-level interactions, measurement
> > error, selection bias and what not but they would both be wrong in
> > identical ways. You can directly test that they are identical with a
> > version of a Hausman test comparing the within group betas with a chi2
> > test. The degrees of freedom calculation will be off from the regular
> > test because the between effects add extra but the within effects will
> > be identical to rounding error so it really won't matter. You can also
> > just do a Mundlak variation on the test. All panel data econometrics
> > textbooks outline this and you can justify the modeling strategy that
> way regardless of reviewer misconceptions.
> >
> > If the FE or group mean centered MLM are both wrong and there's some
> > kind of interactive effect still at work then a random coefficient
> > will likely show up as mattering for model fit with something like an
> > LR test. If beta
> > (X_i-Xbar_j) on Y does not vary as a function of group per an LR test
> > or something fancier like WAIC then it is reasonable (but not
> > infallible) evidence that you don't have group heterogeneity-related
> > omitted variable bias which is what economists would typically be
> > concerned about in this context. You can still have other kinds of
> > bias at work just like with any other kind of observational model. The
> > random coefficient in this context is a regularized interactive fixed
> > effect in econ jargon whereby you are interacting the grouping
> > structure with whatever X you want and getting a distribution of
> > effects. Fundamentally, it's like saying you have some kind of
> > conditional relationship between group/person and X and just
> > interacting them. It's slightly complicated by the fact that empirical
> bayes shrinkage exists but if you have balanced panels then it's mostly a
> non issue.
> >
> >
> >
> > On Sun, Apr 12, 2020 at 7:34 PM Slaughter, Kelly
> > <KELLY.SLAUGHTER at tcu.edu>
> > wrote:
> >
> > > Hi all -
> > >
> > > I have a concern regarding self-selection/omitted variable bias. I
> > > have a longitudinal/repeated measures model, theorizing about a
> > > relationship between treatment/control and effort, represented in nlme
> syntax as:
> > >
> > > EQ 1) log(effort measured in time) ~ treatment*scale(experience),
> > > random = ~1|subject
> > >
> > > Treatment/control is selected by the subject, it is not randomized,
> > > thus raising endogeneity concerns. My background is applied econ, so
> > > as I learn the mixed model domain, I expected to find the mixed
> > > model equivalent of instrumental variables/inverse Mills ratio, etc.
> > > Yet there is surprisingly (to me) limited material addressing this
> > > issue. The best reference material I found is in fact a thread in
> > > this mailing list from October 2016 and the papers referenced within,
> leading to Bell, Fairbrother, and Jones (2019).
> > > My first impression is that I should employ a within-between random
> > > effects (REWB)model -
> > >
> > > EQ 2) log(effort measured in time) ~ treatment*scale(experience) +
> > > experience_between + experience_within, random = experience_within +
> > > scale(experience) | subject
> > >
> > > If I understand correctly, the intuition is that the addition of a
> > > group mean explanatory variable "breaks out" the variability that
> > > would be associated with an omitted variable / error term. Per Bell
> > > et al, "there can be no correlation between level 1 variables
> > > included in the model and the level 2 random effects...unchanging
> > > and/or unmeasured characteristics of an individual (such as
> > > intelligence, ability, etc.) will be controlled out of the estimate of
> the within effect."
> > >
> > > So, no concern between the subject (level 2) and treatment (level 1)
> > > via REWB, wonderful!
> > >
> > > Bell et al caution, "...in a REWB/Mundlak models, unmeasured level 2
> > > characteristics can cause bias in the estimates of between effects
> > > and effects of other level 2 variables."
> > >
> > > Not an issue for me - I am not concerned with level 2, I include
> > > subject to address the IID violation but am interested in
> > > population, not subject, performance.
> > >
> > > Bell et al continue, "However, unobserved time-varying
> > > characteristics can still cause biases at level 1 in either an FE or a
> REWB/Mundlak model."
> > >
> > > Though conceptually my treatment variable is time-varying (it can
> > > change across time within a subject), as a practical/empirical
> > > matter, the treatment is unchanging within the subject - subjects
> > > have no reason to change / would prefer to keep the choice constant.
> > > Of 80k records, treatment switches within a subject occur in about a
> dozen records.
> > >
> > > So, I think I have my solution. However, if a reviewer is not happy
> > > with the with-in / between REWB solution (worried about the level 1
> > > bias), I can further defend EQ 2 via its random coefficient/slope,
> > > if I understand the Oct 2016 thread correctly.
> > >
> > > So, my questions are:
> > >
> > > (1) Is the above correctly reasoned?
> > >
> > > (2) If the random slope model is a further defense against
> > > self-selection bias, could someone provide an intuitive explanation
> > > as to why? Is the idea that by allowing slopes to vary, there is no
> > > endogeneity problem to solve as the very structure of the model
> > > makes the correlated errors concern irrelevant?
> > >
> > > Other solutions I explore include a Mundlak model, but per Bell et
> > > al, the Mundlak models are not meaningful for repeated measures.
> > > Also, it appears that the brms package appears to support mixed
> > > modeling using instrumental variables, something I am more
> > > comfortable with per my background, but strong instrumental variables
> are hard to find in the wild!
> > >
> > > Thank you! - Kelly
> > >
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > _______________________________________________
> > > R-sig-mixed-models at r-project.org mailing list
> > > https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_ma
> > > ilman_listinfo_r-2Dsig-2Dmixed-2Dmodels&d=DwIBaQ&c=7Q-FWLBTAxn3T_E3H
> > > WrzGYJrC4RvUoWDrzTlitGRH_A&r=t-hV_EQcvMxUUCFqXmGPFL3N6XmAH6-xWI5Xpn-
> > > HlYI&m=QIwJJAou0NQyfk892Wz-BodAH5I2A4aX08LX_ruukNk&s=4wSiK6P7-7_81bm
> > > iLGX2F07zLv-M28Gd-4vDdwHogyk&e=
> > >
> >
> >         [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> > man_listinfo_r-2Dsig-2Dmixed-2Dmodels&d=DwIBaQ&c=7Q-FWLBTAxn3T_E3HWrzG
> > YJrC4RvUoWDrzTlitGRH_A&r=t-hV_EQcvMxUUCFqXmGPFL3N6XmAH6-xWI5Xpn-HlYI&m
> > =QIwJJAou0NQyfk892Wz-BodAH5I2A4aX08LX_ruukNk&s=4wSiK6P7-7_81bmiLGX2F07
> > zLv-M28Gd-4vDdwHogyk&e=
>

	[[alternative HTML version deleted]]


From KELLY@SLAUGHTER @end|ng |rom tcu@edu  Mon Apr 13 13:35:15 2020
From: KELLY@SLAUGHTER @end|ng |rom tcu@edu (Slaughter, Kelly)
Date: Mon, 13 Apr 2020 11:35:15 +0000
Subject: [R-sig-ME] Controlling for self-selection bias / endogeneity in
 mixed models
In-Reply-To: <CACDpxFDz+uV8yjTbNzWtQa+zAx4L5QYTHw=mDY265cRUqAVysg@mail.gmail.com>
References: <24e63f45d3ff4a98b1e8b451fb8af229@tcu.edu>
 <CACDpxFD=JPhHemhZV5dyix7C6UR_37np90ff_iuqyAWa+wD5KA@mail.gmail.com>
 <CABghstR+YX=4goLUgyiMAeCB+Ker4Rcw9Jx7qsj1XjQisOd5Kg@mail.gmail.com>
 <489a728021ea403fb0dbd36d1d8a4787@tcu.edu>
 <CACDpxFDz+uV8yjTbNzWtQa+zAx4L5QYTHw=mDY265cRUqAVysg@mail.gmail.com>
Message-ID: <8cde1f91c4ba4c5b8c7afbf370c55d6e@tcu.edu>

Thanks yet again, John. I actually began my ?journey? with propensity score matching, using the MatchIt package. Then the authors of the package came out against propensity score matching (http://gking.harvard.edu/files/gking/files/psnot.pdf) so I turned to Coarsened Exact Matching (CEM). Further evaluation revealed what I think is a sound argument that matching is only effective if you match using the separating / omitted variable (see chrisblattman.com/2010/10/27/the-cardinal-sin-of-matching/ and projects.iq.harvard.edu/sss_blog/can_matching_so). But, if you have the missing variable, you have no need to match! In short, the argument is that while matching provides a benefit over regression wrt regression extrapolation (e.g., the control variable and treatment variables? related outcome values have little overlap), it is not a solution for addressing endogeneity. But I am quite open to returning to matching if I misunderstood the argument.

You wrote in your original reply, ??a random coefficient  will likely show up as mattering for model fit with something like an  LR test.? An anova test of models with/without a random slope did indicate a better fit with the random slope. Per a response of yours in the 2016 thread,  ?The typical response when this test shows that there is still a violation of the no correlation between a random effect and a level 1 variable assumption is to stop making that assumption and use a random coefficients model.? So in my case, random (subject) and level 1 (treatment, or perhaps the missing IQ) ? what remains to be solved?

Requoting Bell ??unchanging and/or unmeasured characteristics of an individual (such as intelligence, ability, etc.) will be controlled out of the estimate of the within effect.?  This seems to address my main concern - an omitted variable (e.g., IQ) not orthogonal with treatment and correlated with the outcome. Are you not convinced that the ?within solution? in fact solves this? Or perhaps it addresses a different problem and I am not thinking about my problem correctly?

Thanks again ? I don?t want to be lazy and ask you to think through issues I should be thinking through, but discussing this with someone more familiar with the issues and a deeper understanding of the underlying statistics is a huge help!

FYI, for anyone following this thread, there is a helpful implementation of Bell et al in R to be found at https://strengejacke.github.io/mixed-models-snippets/random-effects-within-between-effects-model.html


From: John Poe <jdpoe223 at gmail.com>
Sent: Sunday, April 12, 2020 8:22 PM
To: Slaughter, Kelly <KELLY.SLAUGHTER at tcu.edu>
Cc: Ben Bolker <bbolker at gmail.com>; r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Controlling for self-selection bias / endogeneity in mixed models

Ah, okay I see the problem now. This kind of multilevel causal inference problem is a bit hard for me to conceptualize. I usually think about them with DAGs.

I *think* you're going to end up trying to model the selection mechanism itself via something like propensity score weighting unless you can find a good natural IV. In this context the propensity score is an artificial instrumental variable (much like randomization is an instrument). You can find a good explanation of IPW in Hernan and Robins https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/<https://urldefense.proofpoint.com/v2/url?u=https-3A__www.hsph.harvard.edu_miguel-2Dhernan_causal-2Dinference-2Dbook_&d=DwMFaQ&c=7Q-FWLBTAxn3T_E3HWrzGYJrC4RvUoWDrzTlitGRH_A&r=t-hV_EQcvMxUUCFqXmGPFL3N6XmAH6-xWI5Xpn-HlYI&m=w8nTOzq9PBY5WYHICKLJZ9zkWubZcwWebZPmZfsF9Oc&s=7nKaj3-u-912u_MjyXCT3jUs8dLY2q6kbYAy2vvk1as&e=> which includes some detail on longitudinal models though that is geared to time varying treatments. I think you'll just be focusing on building a propensity score at the time of the choice since it never changes which simplifies it down to the first cross-section of data. I'm familiar with 15 or 20ish papers on multilevel propensity score modeling so they are easy to find. One that you might look at is Arpino, B. and Mealli, F., 2011. The specification of the propensity score in multilevel observational studies. Computational Statistics & Data Analysis, 55(4), pp.1770-1780. Arpino has several papers on the topic including a statistics in medicine article that's also pretty good. Causal identification is going to be based on how good the propensity score is and there's no real way around that. Once you get the weighted (or matched if you want to go that route) data you can put it in a regular multilevel model.

It's possible that you could model this with cross-level interactions between ownership and all the level 1 stuff in the model but that would get messy. I think the propensity score route is at least more straightforward to interpret. If you had pre-treatment outcome data of some kind then you could do something like a synthetic control method but I don't know if that's feasible with what you've got.

On Sun, Apr 12, 2020 at 8:56 PM Slaughter, Kelly <KELLY.SLAUGHTER at tcu.edu<mailto:KELLY.SLAUGHTER at tcu.edu>> wrote:
Thanks for the extensive reply, John! Before I attempt to absorb it all, let me offer a couple of quick answers to your questions just to be sure the thread does not spiral in multiple directions :)

(1)     The beginning of the thread I reference can be found here: https://hypatia.math.ethz.ch/pipermail/r-sig-mixed-models/2016q4/025147.html<https://urldefense.proofpoint.com/v2/url?u=https-3A__hypatia.math.ethz.ch_pipermail_r-2Dsig-2Dmixed-2Dmodels_2016q4_025147.html&d=DwMFaQ&c=7Q-FWLBTAxn3T_E3HWrzGYJrC4RvUoWDrzTlitGRH_A&r=t-hV_EQcvMxUUCFqXmGPFL3N6XmAH6-xWI5Xpn-HlYI&m=w8nTOzq9PBY5WYHICKLJZ9zkWubZcwWebZPmZfsF9Oc&s=ko7SDSV6QyHTTxwz0WlGtzSAT0DkpUH6s9xQHipJviI&e=>

(2)     I am referring to omitted variable bias, sorry for the confusion. My treatment / control is ownership of multiple financial accounts / ownership of single accounts. So perhaps let's say IQ tends to make someone more likely to hold multiple accounts (treatment) AND allows them to expend less effort in researching financial trades (outcome variable), whereas I am theorizing that multiple accounts themselves reduce effort directly.

BTW, Ben, thank you for your extensive support across multiple sites in helping the general public with mixed models in R. I have relied upon an EXTENSIVE number of your answers to mixed model questions when developing my models.

-----Original Message-----
From: Ben Bolker <bbolker at gmail.com<mailto:bbolker at gmail.com>>
Sent: Sunday, April 12, 2020 7:46 PM
To: John Poe <jdpoe223 at gmail.com<mailto:jdpoe223 at gmail.com>>
Cc: Slaughter, Kelly <KELLY.SLAUGHTER at tcu.edu<mailto:KELLY.SLAUGHTER at tcu.edu>>; r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Controlling for self-selection bias / endogeneity in mixed models

  Wow, this is the kind of content I come here for.  (It will take me a while to digest this ...) Thank you!

On Sun, Apr 12, 2020 at 8:36 PM John Poe <jdpoe223 at gmail.com<mailto:jdpoe223 at gmail.com>> wrote:
>
> Hi Kelly,
>
> It sounds like you've got correct reasoning on the need for a
> multilevel model if your variable of interest is time invariant.
>
> Can you post a link to the thread you're referencing?
>
> A bit of clarity on the flavor(s) of endogeneity that concern you
> might be helpful. The omitted variable bias issues solved by group
> mean centering and the Mundlak device are mostly from model
> mis/underspecification whereas sample selection is a fundamentally
> different mechanism. Both are common sources of endogeneity recognized
> as such in different pockets of econ but they tend to be seen as
> fundamentally different (often conceptually
> unrelated) problems in other fields. Econ subsumes omitted variables,
> joint causation, measurement error, and sample selection under the
> endogeneity umbrella because they all cause correlation between X and
> the error but other fields don't make the same connection. For
> instance, early panel data work talked about Mundlak devices as
> "instruments" in the same way that dynamic panel data models talk
> about lags and first differences as instruments but they aren't
> traditional instrumental variables that you'd find in the wild and
> arguably wouldn't pass the exclusion restriction test outside of panel
> data. They call them instruments because they instrument the
> endogeneity but they aren't "instrumental variables" in the common parlance.
>
> It's not clear to me if you are referring to general omitted variable
> bias whereby you don't have all the appropriate variables in the model
> or sample selection bias a la Heckman whereby the sample under study
> is systematically different from the population to which you would
> like to make inferences and thus needs some kind of complex propensity
> to choose A or B style correction like with the standard selection
> model. I'm not clear specifically because you referenced the inverse
> mills ratio but it *sounds* like you just think you are possibly
> missing some set of confounders due to the lack of randomization. If
> you do have sample selection bias you can use a multilevel variant of
> a heckman selection model with random effects in the outcome and selection equations. See Grilli, L., & Rampichini, C.
> (2010). Selection bias in linear mixed models. *Metron, 68*(3),
> 309-329 for the best discussion of the topic that I've read. Most
> multilevel modeling work with this kind of problem is based on
> multilevel propensity score matching which is a close cousin of
> multilevel Heckman selection models as the inverse mills ratio and the propensity score are related.
>
> You're right that the addition of group means per Mundlak segregates
> the within and between effects into two different sets of betas when
> they would otherwise be a weighted average. It's just a
> reparamaritization of the dummy variable version of fixed effects. It
> is mathematically impossible in a linear model for a group mean
> centered multilevel model to return different within group beta
> coefficients than the standard FE model. That doesn't mean that both
> of them aren't wrong because of cross-level interactions, measurement
> error, selection bias and what not but they would both be wrong in
> identical ways. You can directly test that they are identical with a
> version of a Hausman test comparing the within group betas with a chi2
> test. The degrees of freedom calculation will be off from the regular
> test because the between effects add extra but the within effects will
> be identical to rounding error so it really won't matter. You can also
> just do a Mundlak variation on the test. All panel data econometrics
> textbooks outline this and you can justify the modeling strategy that way regardless of reviewer misconceptions.
>
> If the FE or group mean centered MLM are both wrong and there's some
> kind of interactive effect still at work then a random coefficient
> will likely show up as mattering for model fit with something like an
> LR test. If beta
> (X_i-Xbar_j) on Y does not vary as a function of group per an LR test
> or something fancier like WAIC then it is reasonable (but not
> infallible) evidence that you don't have group heterogeneity-related
> omitted variable bias which is what economists would typically be
> concerned about in this context. You can still have other kinds of
> bias at work just like with any other kind of observational model. The
> random coefficient in this context is a regularized interactive fixed
> effect in econ jargon whereby you are interacting the grouping
> structure with whatever X you want and getting a distribution of
> effects. Fundamentally, it's like saying you have some kind of
> conditional relationship between group/person and X and just
> interacting them. It's slightly complicated by the fact that empirical bayes shrinkage exists but if you have balanced panels then it's mostly a non issue.
>
>
>
> On Sun, Apr 12, 2020 at 7:34 PM Slaughter, Kelly
> <KELLY.SLAUGHTER at tcu.edu<mailto:KELLY.SLAUGHTER at tcu.edu>>
> wrote:
>
> > Hi all -
> >
> > I have a concern regarding self-selection/omitted variable bias. I
> > have a longitudinal/repeated measures model, theorizing about a
> > relationship between treatment/control and effort, represented in nlme syntax as:
> >
> > EQ 1) log(effort measured in time) ~ treatment*scale(experience),
> > random = ~1|subject
> >
> > Treatment/control is selected by the subject, it is not randomized,
> > thus raising endogeneity concerns. My background is applied econ, so
> > as I learn the mixed model domain, I expected to find the mixed
> > model equivalent of instrumental variables/inverse Mills ratio, etc.
> > Yet there is surprisingly (to me) limited material addressing this
> > issue. The best reference material I found is in fact a thread in
> > this mailing list from October 2016 and the papers referenced within, leading to Bell, Fairbrother, and Jones (2019).
> > My first impression is that I should employ a within-between random
> > effects (REWB)model -
> >
> > EQ 2) log(effort measured in time) ~ treatment*scale(experience) +
> > experience_between + experience_within, random = experience_within +
> > scale(experience) | subject
> >
> > If I understand correctly, the intuition is that the addition of a
> > group mean explanatory variable "breaks out" the variability that
> > would be associated with an omitted variable / error term. Per Bell
> > et al, "there can be no correlation between level 1 variables
> > included in the model and the level 2 random effects...unchanging
> > and/or unmeasured characteristics of an individual (such as
> > intelligence, ability, etc.) will be controlled out of the estimate of the within effect."
> >
> > So, no concern between the subject (level 2) and treatment (level 1)
> > via REWB, wonderful!
> >
> > Bell et al caution, "...in a REWB/Mundlak models, unmeasured level 2
> > characteristics can cause bias in the estimates of between effects
> > and effects of other level 2 variables."
> >
> > Not an issue for me - I am not concerned with level 2, I include
> > subject to address the IID violation but am interested in
> > population, not subject, performance.
> >
> > Bell et al continue, "However, unobserved time-varying
> > characteristics can still cause biases at level 1 in either an FE or a REWB/Mundlak model."
> >
> > Though conceptually my treatment variable is time-varying (it can
> > change across time within a subject), as a practical/empirical
> > matter, the treatment is unchanging within the subject - subjects
> > have no reason to change / would prefer to keep the choice constant.
> > Of 80k records, treatment switches within a subject occur in about a dozen records.
> >
> > So, I think I have my solution. However, if a reviewer is not happy
> > with the with-in / between REWB solution (worried about the level 1
> > bias), I can further defend EQ 2 via its random coefficient/slope,
> > if I understand the Oct 2016 thread correctly.
> >
> > So, my questions are:
> >
> > (1) Is the above correctly reasoned?
> >
> > (2) If the random slope model is a further defense against
> > self-selection bias, could someone provide an intuitive explanation
> > as to why? Is the idea that by allowing slopes to vary, there is no
> > endogeneity problem to solve as the very structure of the model
> > makes the correlated errors concern irrelevant?
> >
> > Other solutions I explore include a Mundlak model, but per Bell et
> > al, the Mundlak models are not meaningful for repeated measures.
> > Also, it appears that the brms package appears to support mixed
> > modeling using instrumental variables, something I am more
> > comfortable with per my background, but strong instrumental variables are hard to find in the wild!
> >
> > Thank you! - Kelly
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
> > https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_ma
> > ilman_listinfo_r-2Dsig-2Dmixed-2Dmodels&d=DwIBaQ&c=7Q-FWLBTAxn3T_E3H
> > WrzGYJrC4RvUoWDrzTlitGRH_A&r=t-hV_EQcvMxUUCFqXmGPFL3N6XmAH6-xWI5Xpn-
> > HlYI&m=QIwJJAou0NQyfk892Wz-BodAH5I2A4aX08LX_ruukNk&s=4wSiK6P7-7_81bm
> > iLGX2F07zLv-M28Gd-4vDdwHogyk&e=
> >
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dsig-2Dmixed-2Dmodels&d=DwIBaQ&c=7Q-FWLBTAxn3T_E3HWrzG
> YJrC4RvUoWDrzTlitGRH_A&r=t-hV_EQcvMxUUCFqXmGPFL3N6XmAH6-xWI5Xpn-HlYI&m
> =QIwJJAou0NQyfk892Wz-BodAH5I2A4aX08LX_ruukNk&s=4wSiK6P7-7_81bmiLGX2F07
> zLv-M28Gd-4vDdwHogyk&e=

	[[alternative HTML version deleted]]


From KELLY@SLAUGHTER @end|ng |rom tcu@edu  Mon Apr 13 13:53:05 2020
From: KELLY@SLAUGHTER @end|ng |rom tcu@edu (Slaughter, Kelly)
Date: Mon, 13 Apr 2020 11:53:05 +0000
Subject: [R-sig-ME] Controlling for self-selection bias / endogeneity in
 mixed models
In-Reply-To: <000001d61164$60ffd8b0$22ff8a10$@uke.de>
References: <24e63f45d3ff4a98b1e8b451fb8af229@tcu.edu>
 <000001d61164$60ffd8b0$22ff8a10$@uke.de>
Message-ID: <4338ec234882450ab7ee755da35f948a@tcu.edu>

Thank you, Daniel. 

Yes, I have the time invariant "treatment" as a level 1/fixed effect, and am further hypothesizing that "treatment" is more important as one gains "experience" (thus an interaction variable). The variable I am considering de-meaning / group meaning is "experience".

I scaled "experience" originally to address convergence issues, but as the R implementation of scaling also centers, I also addressed the collinearity between the main and interaction variables. But I can center without scaling of course. 

Your first link did not work for me, but the general site referenced in the link as well as the "parameters" links look potentially quite helpful. I will review/run your gist to better understand the impact of within/between and REWB, thank you very much!

-----Original Message-----
From: Daniel L?decke <d.luedecke at uke.de> 
Sent: Monday, April 13, 2020 2:23 AM
To: Slaughter, Kelly <KELLY.SLAUGHTER at tcu.edu>; r-sig-mixed-models at r-project.org
Subject: AW: [R-sig-ME] Controlling for self-selection bias / endogeneity in mixed models

Hi Kelly,

> Not an issue for me - I am not concerned with level 2, I include 
> subject
to address the IID violation but am interested in population, not subject, performance.

If your variable is practically time constant (or time invariant), you can add it as normal predictor, and you don't need the de-mean and group-mean of it (separation into within- and between-effects). In your case, if "treatment" is practically constant over time, you just include it "as is"
in your model.

The main reason for heterogeneity bias, if I understood Bell et al.
correctly, is the weighted average of coefficients for time-varying variables (or more general: level-1 predictors that have also a level-2 effect and thus might correlate with the group variable from the random effects). Simply decomposing time-varying predictors into their within- and between-effects indeed give you the same consistent estimates as a "fixed effects" model, just that the REWB model has much more benefits.

Based on a short blog post I found
(https://urldefense.proofpoint.com/v2/url?u=https-3A__shouldbewriting.netlify.com_posts_2019-2D10-2D21-2Daccounting-2Dfor-2Dwithin-2D&d=DwIFAw&c=7Q-FWLBTAxn3T_E3HWrzGYJrC4RvUoWDrzTlitGRH_A&r=t-hV_EQcvMxUUCFqXmGPFL3N6XmAH6-xWI5Xpn-HlYI&m=VAEa7Lfqyy-8PDDuGoMNHrwE_K1t6S14Mdc5eCWNdCI&s=asNSu6KdGLAPx10e6Zrk1NQaAnQzyJUr9SWWrihKz9Q&e=
and-between-subject-effect/) I have written a small gist that produces plots and coefficient tables for teaching repeated measurement with mixed models, which shows this:
https://urldefense.proofpoint.com/v2/url?u=https-3A__gist.github.com_strengejacke_c53e1fa1d7cf41e4737f3ab044a67d09&d=DwIFAw&c=7Q-FWLBTAxn3T_E3HWrzGYJrC4RvUoWDrzTlitGRH_A&r=t-hV_EQcvMxUUCFqXmGPFL3N6XmAH6-xWI5Xpn-HlYI&m=VAEa7Lfqyy-8PDDuGoMNHrwE_K1t6S14Mdc5eCWNdCI&s=Tn8PpuouPfun3hSncpr4T3MUMZq_Fg8iFBKc9NcDyxA&e= 

One thing I would take into consideration is the interaction term. There are several ways how to do this if a time-varying predictor is used in an interaction, I would not scale it (as in your example), but probably think if you're interested in the interaction of the within- or between-effect (or both). See the 'Details' in the "parameters::demean()" help for some more explanation and references (https://urldefense.proofpoint.com/v2/url?u=https-3A__easystats.github.io_parameters_reference_demean.html&d=DwIFAw&c=7Q-FWLBTAxn3T_E3HWrzGYJrC4RvUoWDrzTlitGRH_A&r=t-hV_EQcvMxUUCFqXmGPFL3N6XmAH6-xWI5Xpn-HlYI&m=VAEa7Lfqyy-8PDDuGoMNHrwE_K1t6S14Mdc5eCWNdCI&s=7U7xH0JS_kBGMIorBDahpL-x84Gae4UmyQ6TYi0HG_0&e= ).

To your 2nd question: See my gist above. FE models model the within-effect, however, this may (or: is very likely to) vary between group levels (i.e.
subjects). Thus, including the within-effect as random slope makes sense, since it captures the variability between groups (but leads to increased SE because it accounts better for the uncertainty in the random effects). See also this vignette:
https://urldefense.proofpoint.com/v2/url?u=https-3A__easystats.github.io_parameters_articles_demean.html&d=DwIFAw&c=7Q-FWLBTAxn3T_E3HWrzGYJrC4RvUoWDrzTlitGRH_A&r=t-hV_EQcvMxUUCFqXmGPFL3N6XmAH6-xWI5Xpn-HlYI&m=VAEa7Lfqyy-8PDDuGoMNHrwE_K1t6S14Mdc5eCWNdCI&s=U7045TSlmK52uGBY4fzPGhwaCLUpD-1QZdz9fyYmF8U&e= 

Best
Daniel

-----Urspr?ngliche Nachricht-----
Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im Auftrag von Slaughter, Kelly
Gesendet: Montag, 13. April 2020 01:34
An: r-sig-mixed-models at r-project.org
Betreff: [R-sig-ME] Controlling for self-selection bias / endogeneity in mixed models

Hi all -

I have a concern regarding self-selection/omitted variable bias. I have a longitudinal/repeated measures model, theorizing about a relationship between treatment/control and effort, represented in nlme syntax as:

EQ 1) log(effort measured in time) ~ treatment*scale(experience), random = ~1|subject

Treatment/control is selected by the subject, it is not randomized, thus raising endogeneity concerns. My background is applied econ, so as I learn the mixed model domain, I expected to find the mixed model equivalent of instrumental variables/inverse Mills ratio, etc. Yet there is surprisingly (to me) limited material addressing this issue. The best reference material I found is in fact a thread in this mailing list from October 2016 and the papers referenced within, leading to Bell, Fairbrother, and Jones (2019). My first impression is that I should employ a within-between random effects (REWB)model -

EQ 2) log(effort measured in time) ~ treatment*scale(experience) + experience_between + experience_within, random = experience_within +
scale(experience) | subject

If I understand correctly, the intuition is that the addition of a group mean explanatory variable "breaks out" the variability that would be associated with an omitted variable / error term. Per Bell et al, "there can be no correlation between level 1 variables included in the model and the level 2 random effects...unchanging and/or unmeasured characteristics of an individual (such as intelligence, ability, etc.) will be controlled out of the estimate of the within effect."

So, no concern between the subject (level 2) and treatment (level 1) via REWB, wonderful!

Bell et al caution, "...in a REWB/Mundlak models, unmeasured level 2 characteristics can cause bias in the estimates of between effects and effects of other level 2 variables."

Not an issue for me - I am not concerned with level 2, I include subject to address the IID violation but am interested in population, not subject, performance.

Bell et al continue, "However, unobserved time-varying characteristics can still cause biases at level 1 in either an FE or a REWB/Mundlak model."

Though conceptually my treatment variable is time-varying (it can change across time within a subject), as a practical/empirical matter, the treatment is unchanging within the subject - subjects have no reason to change / would prefer to keep the choice constant. Of 80k records, treatment switches within a subject occur in about a dozen records.

So, I think I have my solution. However, if a reviewer is not happy with the with-in / between REWB solution (worried about the level 1 bias), I can further defend EQ 2 via its random coefficient/slope, if I understand the Oct 2016 thread correctly.

So, my questions are:

(1) Is the above correctly reasoned?

(2) If the random slope model is a further defense against self-selection bias, could someone provide an intuitive explanation as to why? Is the idea that by allowing slopes to vary, there is no endogeneity problem to solve as the very structure of the model makes the correlated errors concern irrelevant?

Other solutions I explore include a Mundlak model, but per Bell et al, the Mundlak models are not meaningful for repeated measures. Also, it appears that the brms package appears to support mixed modeling using instrumental variables, something I am more comfortable with per my background, but strong instrumental variables are hard to find in the wild!

Thank you! - Kelly


	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dsig-2Dmixed-2Dmodels&d=DwIFAw&c=7Q-FWLBTAxn3T_E3HWrzGYJrC4RvUoWDrzTlitGRH_A&r=t-hV_EQcvMxUUCFqXmGPFL3N6XmAH6-xWI5Xpn-HlYI&m=VAEa7Lfqyy-8PDDuGoMNHrwE_K1t6S14Mdc5eCWNdCI&s=tuEd99m5bw5OUB0RX6CZfHDZ5w2nTVzXy4d1wozIRRk&e= 

--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Joachim Pr?l?, Prof. Dr. Blanche Schwappach-Pignataro, Marya Verdel _____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING


From d@|uedecke @end|ng |rom uke@de  Mon Apr 13 14:54:00 2020
From: d@|uedecke @end|ng |rom uke@de (=?iso-8859-1?Q?Daniel_L=FCdecke?=)
Date: Mon, 13 Apr 2020 14:54:00 +0200
Subject: [R-sig-ME] Controlling for self-selection bias / endogeneity in
 mixed models
In-Reply-To: <4338ec234882450ab7ee755da35f948a@tcu.edu>
References: <24e63f45d3ff4a98b1e8b451fb8af229@tcu.edu>
 <000001d61164$60ffd8b0$22ff8a10$@uke.de>
 <4338ec234882450ab7ee755da35f948a@tcu.edu>
Message-ID: <001b01d61192$9aa5e8b0$cff1ba10$@uke.de>

Just a short note:
Just read your previous answer to John, where you referred to my GitHub page
dealing with the REWB model. The vignette in the parameters package is
actually just an updated version of that GitHub page.

> I scaled "experience" originally to address convergence issues, but as the
R implementation of scaling also centers, I also addressed the collinearity
between the main and interaction variables. But I can center without scaling
of course.

This is something I'm not 100% sure about to deal with interaction effects
with time-varying predictors, so my comment was rather a pointer than a
definite suggestion. In a recent paper, we are modelling the interaction
between group (or treatment) and the between-effect only, and the
within-predictor is used as further covariate. I just read one or two papers
where different ways of group- and de-meaning were suggested.

> Your first link did not work for me

It's actually the same link as in the first paragraph of the vignette I
posted.

Best
Daniel

-----Urspr?ngliche Nachricht-----
Von: Slaughter, Kelly <KELLY.SLAUGHTER at tcu.edu> 
Gesendet: Montag, 13. April 2020 13:53
An: Daniel L?decke <d.luedecke at uke.de>; r-sig-mixed-models at r-project.org
Betreff: RE: [R-sig-ME] Controlling for self-selection bias / endogeneity in
mixed models

Thank you, Daniel. 

Yes, I have the time invariant "treatment" as a level 1/fixed effect, and am
further hypothesizing that "treatment" is more important as one gains
"experience" (thus an interaction variable). The variable I am considering
de-meaning / group meaning is "experience".

I scaled "experience" originally to address convergence issues, but as the R
implementation of scaling also centers, I also addressed the collinearity
between the main and interaction variables. But I can center without scaling
of course. 

Your first link did not work for me, but the general site referenced in the
link as well as the "parameters" links look potentially quite helpful. I
will review/run your gist to better understand the impact of within/between
and REWB, thank you very much!

-----Original Message-----
From: Daniel L?decke <d.luedecke at uke.de> 
Sent: Monday, April 13, 2020 2:23 AM
To: Slaughter, Kelly <KELLY.SLAUGHTER at tcu.edu>;
r-sig-mixed-models at r-project.org
Subject: AW: [R-sig-ME] Controlling for self-selection bias / endogeneity in
mixed models

Hi Kelly,

> Not an issue for me - I am not concerned with level 2, I include 
> subject
to address the IID violation but am interested in population, not subject,
performance.

If your variable is practically time constant (or time invariant), you can
add it as normal predictor, and you don't need the de-mean and group-mean of
it (separation into within- and between-effects). In your case, if
"treatment" is practically constant over time, you just include it "as is"
in your model.

The main reason for heterogeneity bias, if I understood Bell et al.
correctly, is the weighted average of coefficients for time-varying
variables (or more general: level-1 predictors that have also a level-2
effect and thus might correlate with the group variable from the random
effects). Simply decomposing time-varying predictors into their within- and
between-effects indeed give you the same consistent estimates as a "fixed
effects" model, just that the REWB model has much more benefits.

Based on a short blog post I found
(https://urldefense.proofpoint.com/v2/url?u=https-3A__shouldbewriting.netlif
y.com_posts_2019-2D10-2D21-2Daccounting-2Dfor-2Dwithin-2D&d=DwIFAw&c=7Q-FWLB
TAxn3T_E3HWrzGYJrC4RvUoWDrzTlitGRH_A&r=t-hV_EQcvMxUUCFqXmGPFL3N6XmAH6-xWI5Xp
n-HlYI&m=VAEa7Lfqyy-8PDDuGoMNHrwE_K1t6S14Mdc5eCWNdCI&s=asNSu6KdGLAPx10e6Zrk1
NQaAnQzyJUr9SWWrihKz9Q&e=
and-between-subject-effect/) I have written a small gist that produces plots
and coefficient tables for teaching repeated measurement with mixed models,
which shows this:
https://urldefense.proofpoint.com/v2/url?u=https-3A__gist.github.com_strenge
jacke_c53e1fa1d7cf41e4737f3ab044a67d09&d=DwIFAw&c=7Q-FWLBTAxn3T_E3HWrzGYJrC4
RvUoWDrzTlitGRH_A&r=t-hV_EQcvMxUUCFqXmGPFL3N6XmAH6-xWI5Xpn-HlYI&m=VAEa7Lfqyy
-8PDDuGoMNHrwE_K1t6S14Mdc5eCWNdCI&s=Tn8PpuouPfun3hSncpr4T3MUMZq_Fg8iFBKc9NcD
yxA&e= 

One thing I would take into consideration is the interaction term. There are
several ways how to do this if a time-varying predictor is used in an
interaction, I would not scale it (as in your example), but probably think
if you're interested in the interaction of the within- or between-effect (or
both). See the 'Details' in the "parameters::demean()" help for some more
explanation and references
(https://urldefense.proofpoint.com/v2/url?u=https-3A__easystats.github.io_pa
rameters_reference_demean.html&d=DwIFAw&c=7Q-FWLBTAxn3T_E3HWrzGYJrC4RvUoWDrz
TlitGRH_A&r=t-hV_EQcvMxUUCFqXmGPFL3N6XmAH6-xWI5Xpn-HlYI&m=VAEa7Lfqyy-8PDDuGo
MNHrwE_K1t6S14Mdc5eCWNdCI&s=7U7xH0JS_kBGMIorBDahpL-x84Gae4UmyQ6TYi0HG_0&e=
).

To your 2nd question: See my gist above. FE models model the within-effect,
however, this may (or: is very likely to) vary between group levels (i.e.
subjects). Thus, including the within-effect as random slope makes sense,
since it captures the variability between groups (but leads to increased SE
because it accounts better for the uncertainty in the random effects). See
also this vignette:
https://urldefense.proofpoint.com/v2/url?u=https-3A__easystats.github.io_par
ameters_articles_demean.html&d=DwIFAw&c=7Q-FWLBTAxn3T_E3HWrzGYJrC4RvUoWDrzTl
itGRH_A&r=t-hV_EQcvMxUUCFqXmGPFL3N6XmAH6-xWI5Xpn-HlYI&m=VAEa7Lfqyy-8PDDuGoMN
HrwE_K1t6S14Mdc5eCWNdCI&s=U7045TSlmK52uGBY4fzPGhwaCLUpD-1QZdz9fyYmF8U&e= 

Best
Daniel

-----Urspr?ngliche Nachricht-----
Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im
Auftrag von Slaughter, Kelly
Gesendet: Montag, 13. April 2020 01:34
An: r-sig-mixed-models at r-project.org
Betreff: [R-sig-ME] Controlling for self-selection bias / endogeneity in
mixed models

Hi all -

I have a concern regarding self-selection/omitted variable bias. I have a
longitudinal/repeated measures model, theorizing about a relationship
between treatment/control and effort, represented in nlme syntax as:

EQ 1) log(effort measured in time) ~ treatment*scale(experience), random =
~1|subject

Treatment/control is selected by the subject, it is not randomized, thus
raising endogeneity concerns. My background is applied econ, so as I learn
the mixed model domain, I expected to find the mixed model equivalent of
instrumental variables/inverse Mills ratio, etc. Yet there is surprisingly
(to me) limited material addressing this issue. The best reference material
I found is in fact a thread in this mailing list from October 2016 and the
papers referenced within, leading to Bell, Fairbrother, and Jones (2019). My
first impression is that I should employ a within-between random effects
(REWB)model -

EQ 2) log(effort measured in time) ~ treatment*scale(experience) +
experience_between + experience_within, random = experience_within +
scale(experience) | subject

If I understand correctly, the intuition is that the addition of a group
mean explanatory variable "breaks out" the variability that would be
associated with an omitted variable / error term. Per Bell et al, "there can
be no correlation between level 1 variables included in the model and the
level 2 random effects...unchanging and/or unmeasured characteristics of an
individual (such as intelligence, ability, etc.) will be controlled out of
the estimate of the within effect."

So, no concern between the subject (level 2) and treatment (level 1) via
REWB, wonderful!

Bell et al caution, "...in a REWB/Mundlak models, unmeasured level 2
characteristics can cause bias in the estimates of between effects and
effects of other level 2 variables."

Not an issue for me - I am not concerned with level 2, I include subject to
address the IID violation but am interested in population, not subject,
performance.

Bell et al continue, "However, unobserved time-varying characteristics can
still cause biases at level 1 in either an FE or a REWB/Mundlak model."

Though conceptually my treatment variable is time-varying (it can change
across time within a subject), as a practical/empirical matter, the
treatment is unchanging within the subject - subjects have no reason to
change / would prefer to keep the choice constant. Of 80k records, treatment
switches within a subject occur in about a dozen records.

So, I think I have my solution. However, if a reviewer is not happy with the
with-in / between REWB solution (worried about the level 1 bias), I can
further defend EQ 2 via its random coefficient/slope, if I understand the
Oct 2016 thread correctly.

So, my questions are:

(1) Is the above correctly reasoned?

(2) If the random slope model is a further defense against self-selection
bias, could someone provide an intuitive explanation as to why? Is the idea
that by allowing slopes to vary, there is no endogeneity problem to solve as
the very structure of the model makes the correlated errors concern
irrelevant?

Other solutions I explore include a Mundlak model, but per Bell et al, the
Mundlak models are not meaningful for repeated measures. Also, it appears
that the brms package appears to support mixed modeling using instrumental
variables, something I am more comfortable with per my background, but
strong instrumental variables are hard to find in the wild!

Thank you! - Kelly


	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_li
stinfo_r-2Dsig-2Dmixed-2Dmodels&d=DwIFAw&c=7Q-FWLBTAxn3T_E3HWrzGYJrC4RvUoWDr
zTlitGRH_A&r=t-hV_EQcvMxUUCFqXmGPFL3N6XmAH6-xWI5Xpn-HlYI&m=VAEa7Lfqyy-8PDDuG
oMNHrwE_K1t6S14Mdc5eCWNdCI&s=tuEd99m5bw5OUB0RX6CZfHDZ5w2nTVzXy4d1wozIRRk&e= 

--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen
Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Joachim Pr?l?,
Prof. Dr. Blanche Schwappach-Pignataro, Marya Verdel
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING


--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Joachim Pr?l?, Prof. Dr. Blanche Schwappach-Pignataro, Marya Verdel
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING


From john@m@|ndon@|d @end|ng |rom @nu@edu@@u  Mon Apr 13 23:46:38 2020
From: john@m@|ndon@|d @end|ng |rom @nu@edu@@u (John Maindonald)
Date: Mon, 13 Apr 2020 21:46:38 +0000
Subject: [R-sig-ME] Controlling for self-selection bias / endogeneity in
 mixed models
In-Reply-To: <8cde1f91c4ba4c5b8c7afbf370c55d6e@tcu.edu>
References: <24e63f45d3ff4a98b1e8b451fb8af229@tcu.edu>
 <CACDpxFD=JPhHemhZV5dyix7C6UR_37np90ff_iuqyAWa+wD5KA@mail.gmail.com>
 <CABghstR+YX=4goLUgyiMAeCB+Ker4Rcw9Jx7qsj1XjQisOd5Kg@mail.gmail.com>
 <489a728021ea403fb0dbd36d1d8a4787@tcu.edu>
 <CACDpxFDz+uV8yjTbNzWtQa+zAx4L5QYTHw=mDY265cRUqAVysg@mail.gmail.com>
 <8cde1f91c4ba4c5b8c7afbf370c55d6e@tcu.edu>
Message-ID: <B8D94390-BA9E-49AF-8E46-6023621A6CFB@anu.edu.au>

Irrespective of the usefulness of propensity scores as a substitute for incorporation of
the relevant variables into the regression, plots that show the extent to which groups can
be separated on the basis of propensity can provide highly useful insight.  If the groups
can be mostly or largely separated on the basis of the propensity scores, that raises
large issues for the reliance that one can place on either the regression model or the
propensity score model.  Extreme (and less extreme?) outliers on the scores can be
removed.  Of course, propensity scores should themselves be subject to diagnostic
checks; should one or more variables be transformed, or is it impossible to say?  These
issues become a whole lot more difficult in a multi-level model setting.

A serious weakness of Morgan and Winship?s ?Counterfactuals and Causal Inference?
(2015) is that, although it has lots of DAGs, it is almost completely free of graphs that
might be used for diagnostic purposes.  Multi-level models are, from a quick check, not
discussed.  What advance, if any, is now available on Morgan and Winship?


John Maindonald             email: john.maindonald at anu.edu.au<mailto:john.maindonald at anu.edu.au>


On 13/04/2020, at 23:35, Slaughter, Kelly <KELLY.SLAUGHTER at tcu.edu<mailto:KELLY.SLAUGHTER at tcu.edu>> wrote:

Thanks yet again, John. I actually began my ?journey? with propensity score matching, using the MatchIt package. Then the authors of the package came out against propensity score matching (http://gking.harvard.edu/files/gking/files/psnot.pdf) so I turned to Coarsened Exact Matching (CEM). Further evaluation revealed what I think is a sound argument that matching is only effective if you match using the separating / omitted variable (see chrisblattman.com/2010/10/27/the-cardinal-sin-of-matching/<http://chrisblattman.com/2010/10/27/the-cardinal-sin-of-matching/> and projects.iq.harvard.edu/sss_blog/can_matching_so<http://projects.iq.harvard.edu/sss_blog/can_matching_so>). But, if you have the missing variable, you have no need to match! In short, the argument is that while matching provides a benefit over regression wrt regression extrapolation (e.g., the control variable and treatment variables? related outcome values have little overlap), it is not a solution for addressing endogeneity. But I am quite open to returning to matching if I misunderstood the argument.

You wrote in your original reply, ??a random coefficient  will likely show up as mattering for model fit with something like an  LR test.? An anova test of models with/without a random slope did indicate a better fit with the random slope. Per a response of yours in the 2016 thread,  ?The typical response when this test shows that there is still a violation of the no correlation between a random effect and a level 1 variable assumption is to stop making that assumption and use a random coefficients model.? So in my case, random (subject) and level 1 (treatment, or perhaps the missing IQ) ? what remains to be solved?

Requoting Bell ??unchanging and/or unmeasured characteristics of an individual (such as intelligence, ability, etc.) will be controlled out of the estimate of the within effect.?  This seems to address my main concern - an omitted variable (e.g., IQ) not orthogonal with treatment and correlated with the outcome. Are you not convinced that the ?within solution? in fact solves this? Or perhaps it addresses a different problem and I am not thinking about my problem correctly?

Thanks again ? I don?t want to be lazy and ask you to think through issues I should be thinking through, but discussing this with someone more familiar with the issues and a deeper understanding of the underlying statistics is a huge help!

FYI, for anyone following this thread, there is a helpful implementation of Bell et al in R to be found at https://strengejacke.github.io/mixed-models-snippets/random-effects-within-between-effects-model.html


From: John Poe <jdpoe223 at gmail.com<mailto:jdpoe223 at gmail.com>>
Sent: Sunday, April 12, 2020 8:22 PM
To: Slaughter, Kelly <KELLY.SLAUGHTER at tcu.edu<mailto:KELLY.SLAUGHTER at tcu.edu>>
Cc: Ben Bolker <bbolker at gmail.com<mailto:bbolker at gmail.com>>; r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Controlling for self-selection bias / endogeneity in mixed models

Ah, okay I see the problem now. This kind of multilevel causal inference problem is a bit hard for me to conceptualize. I usually think about them with DAGs.

I *think* you're going to end up trying to model the selection mechanism itself via something like propensity score weighting unless you can find a good natural IV. In this context the propensity score is an artificial instrumental variable (much like randomization is an instrument). You can find a good explanation of IPW in Hernan and Robins https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/<https://urldefense.proofpoint.com/v2/url?u=https-3A__www.hsph.harvard.edu_miguel-2Dhernan_causal-2Dinference-2Dbook_&d=DwMFaQ&c=7Q-FWLBTAxn3T_E3HWrzGYJrC4RvUoWDrzTlitGRH_A&r=t-hV_EQcvMxUUCFqXmGPFL3N6XmAH6-xWI5Xpn-HlYI&m=w8nTOzq9PBY5WYHICKLJZ9zkWubZcwWebZPmZfsF9Oc&s=7nKaj3-u-912u_MjyXCT3jUs8dLY2q6kbYAy2vvk1as&e=> which includes some detail on longitudinal models though that is geared to time varying treatments. I think you'll just be focusing on building a propensity score at the time of the choice since it never changes which simplifies it down to the first cross-section of data. I'm familiar with 15 or 20ish papers on multilevel propensity score modeling so they are easy to find. One that you might look at is Arpino, B. and Mealli, F., 2011. The specification of the propensity score in multilevel observational studies. Computational Statistics & Data Analysis, 55(4), pp.1770-1780. Arpino has several papers on the topic including a statistics in medicine article that's also pretty good. Causal identification is going to be based on how good the propensity score is and there's no real way around that. Once you get the weighted (or matched if you want to go that route) data you can put it in a regular multilevel model.

It's possible that you could model this with cross-level interactions between ownership and all the level 1 stuff in the model but that would get messy. I think the propensity score route is at least more straightforward to interpret. If you had pre-treatment outcome data of some kind then you could do something like a synthetic control method but I don't know if that's feasible with what you've got.

On Sun, Apr 12, 2020 at 8:56 PM Slaughter, Kelly <KELLY.SLAUGHTER at tcu.edu<mailto:KELLY.SLAUGHTER at tcu.edu><mailto:KELLY.SLAUGHTER at tcu.edu>> wrote:
Thanks for the extensive reply, John! Before I attempt to absorb it all, let me offer a couple of quick answers to your questions just to be sure the thread does not spiral in multiple directions :)

(1)     The beginning of the thread I reference can be found here: https://hypatia.math.ethz.ch/pipermail/r-sig-mixed-models/2016q4/025147.html<https://urldefense.proofpoint.com/v2/url?u=https-3A__hypatia.math.ethz.ch_pipermail_r-2Dsig-2Dmixed-2Dmodels_2016q4_025147.html&d=DwMFaQ&c=7Q-FWLBTAxn3T_E3HWrzGYJrC4RvUoWDrzTlitGRH_A&r=t-hV_EQcvMxUUCFqXmGPFL3N6XmAH6-xWI5Xpn-HlYI&m=w8nTOzq9PBY5WYHICKLJZ9zkWubZcwWebZPmZfsF9Oc&s=ko7SDSV6QyHTTxwz0WlGtzSAT0DkpUH6s9xQHipJviI&e=>

(2)     I am referring to omitted variable bias, sorry for the confusion. My treatment / control is ownership of multiple financial accounts / ownership of single accounts. So perhaps let's say IQ tends to make someone more likely to hold multiple accounts (treatment) AND allows them to expend less effort in researching financial trades (outcome variable), whereas I am theorizing that multiple accounts themselves reduce effort directly.

BTW, Ben, thank you for your extensive support across multiple sites in helping the general public with mixed models in R. I have relied upon an EXTENSIVE number of your answers to mixed model questions when developing my models.

-----Original Message-----
From: Ben Bolker <bbolker at gmail.com<mailto:bbolker at gmail.com><mailto:bbolker at gmail.com>>
Sent: Sunday, April 12, 2020 7:46 PM
To: John Poe <jdpoe223 at gmail.com<mailto:jdpoe223 at gmail.com><mailto:jdpoe223 at gmail.com>>
Cc: Slaughter, Kelly <KELLY.SLAUGHTER at tcu.edu<mailto:KELLY.SLAUGHTER at tcu.edu><mailto:KELLY.SLAUGHTER at tcu.edu>>; r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org><mailto:r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Controlling for self-selection bias / endogeneity in mixed models

 Wow, this is the kind of content I come here for.  (It will take me a while to digest this ...) Thank you!

On Sun, Apr 12, 2020 at 8:36 PM John Poe <jdpoe223 at gmail.com<mailto:jdpoe223 at gmail.com><mailto:jdpoe223 at gmail.com>> wrote:

Hi Kelly,

It sounds like you've got correct reasoning on the need for a
multilevel model if your variable of interest is time invariant.

Can you post a link to the thread you're referencing?

A bit of clarity on the flavor(s) of endogeneity that concern you
might be helpful. The omitted variable bias issues solved by group
mean centering and the Mundlak device are mostly from model
mis/underspecification whereas sample selection is a fundamentally
different mechanism. Both are common sources of endogeneity recognized
as such in different pockets of econ but they tend to be seen as
fundamentally different (often conceptually
unrelated) problems in other fields. Econ subsumes omitted variables,
joint causation, measurement error, and sample selection under the
endogeneity umbrella because they all cause correlation between X and
the error but other fields don't make the same connection. For
instance, early panel data work talked about Mundlak devices as
"instruments" in the same way that dynamic panel data models talk
about lags and first differences as instruments but they aren't
traditional instrumental variables that you'd find in the wild and
arguably wouldn't pass the exclusion restriction test outside of panel
data. They call them instruments because they instrument the
endogeneity but they aren't "instrumental variables" in the common parlance.

It's not clear to me if you are referring to general omitted variable
bias whereby you don't have all the appropriate variables in the model
or sample selection bias a la Heckman whereby the sample under study
is systematically different from the population to which you would
like to make inferences and thus needs some kind of complex propensity
to choose A or B style correction like with the standard selection
model. I'm not clear specifically because you referenced the inverse
mills ratio but it *sounds* like you just think you are possibly
missing some set of confounders due to the lack of randomization. If
you do have sample selection bias you can use a multilevel variant of
a heckman selection model with random effects in the outcome and selection equations. See Grilli, L., & Rampichini, C.
(2010). Selection bias in linear mixed models. *Metron, 68*(3),
309-329 for the best discussion of the topic that I've read. Most
multilevel modeling work with this kind of problem is based on
multilevel propensity score matching which is a close cousin of
multilevel Heckman selection models as the inverse mills ratio and the propensity score are related.

You're right that the addition of group means per Mundlak segregates
the within and between effects into two different sets of betas when
they would otherwise be a weighted average. It's just a
reparamaritization of the dummy variable version of fixed effects. It
is mathematically impossible in a linear model for a group mean
centered multilevel model to return different within group beta
coefficients than the standard FE model. That doesn't mean that both
of them aren't wrong because of cross-level interactions, measurement
error, selection bias and what not but they would both be wrong in
identical ways. You can directly test that they are identical with a
version of a Hausman test comparing the within group betas with a chi2
test. The degrees of freedom calculation will be off from the regular
test because the between effects add extra but the within effects will
be identical to rounding error so it really won't matter. You can also
just do a Mundlak variation on the test. All panel data econometrics
textbooks outline this and you can justify the modeling strategy that way regardless of reviewer misconceptions.

If the FE or group mean centered MLM are both wrong and there's some
kind of interactive effect still at work then a random coefficient
will likely show up as mattering for model fit with something like an
LR test. If beta
(X_i-Xbar_j) on Y does not vary as a function of group per an LR test
or something fancier like WAIC then it is reasonable (but not
infallible) evidence that you don't have group heterogeneity-related
omitted variable bias which is what economists would typically be
concerned about in this context. You can still have other kinds of
bias at work just like with any other kind of observational model. The
random coefficient in this context is a regularized interactive fixed
effect in econ jargon whereby you are interacting the grouping
structure with whatever X you want and getting a distribution of
effects. Fundamentally, it's like saying you have some kind of
conditional relationship between group/person and X and just
interacting them. It's slightly complicated by the fact that empirical bayes shrinkage exists but if you have balanced panels then it's mostly a non issue.



On Sun, Apr 12, 2020 at 7:34 PM Slaughter, Kelly
<KELLY.SLAUGHTER at tcu.edu<mailto:KELLY.SLAUGHTER at tcu.edu><mailto:KELLY.SLAUGHTER at tcu.edu>>
wrote:

Hi all -

I have a concern regarding self-selection/omitted variable bias. I
have a longitudinal/repeated measures model, theorizing about a
relationship between treatment/control and effort, represented in nlme syntax as:

EQ 1) log(effort measured in time) ~ treatment*scale(experience),
random = ~1|subject

Treatment/control is selected by the subject, it is not randomized,
thus raising endogeneity concerns. My background is applied econ, so
as I learn the mixed model domain, I expected to find the mixed
model equivalent of instrumental variables/inverse Mills ratio, etc.
Yet there is surprisingly (to me) limited material addressing this
issue. The best reference material I found is in fact a thread in
this mailing list from October 2016 and the papers referenced within, leading to Bell, Fairbrother, and Jones (2019).
My first impression is that I should employ a within-between random
effects (REWB)model -

EQ 2) log(effort measured in time) ~ treatment*scale(experience) +
experience_between + experience_within, random = experience_within +
scale(experience) | subject

If I understand correctly, the intuition is that the addition of a
group mean explanatory variable "breaks out" the variability that
would be associated with an omitted variable / error term. Per Bell
et al, "there can be no correlation between level 1 variables
included in the model and the level 2 random effects...unchanging
and/or unmeasured characteristics of an individual (such as
intelligence, ability, etc.) will be controlled out of the estimate of the within effect."

So, no concern between the subject (level 2) and treatment (level 1)
via REWB, wonderful!

Bell et al caution, "...in a REWB/Mundlak models, unmeasured level 2
characteristics can cause bias in the estimates of between effects
and effects of other level 2 variables."

Not an issue for me - I am not concerned with level 2, I include
subject to address the IID violation but am interested in
population, not subject, performance.

Bell et al continue, "However, unobserved time-varying
characteristics can still cause biases at level 1 in either an FE or a REWB/Mundlak model."

Though conceptually my treatment variable is time-varying (it can
change across time within a subject), as a practical/empirical
matter, the treatment is unchanging within the subject - subjects
have no reason to change / would prefer to keep the choice constant.
Of 80k records, treatment switches within a subject occur in about a dozen records.

So, I think I have my solution. However, if a reviewer is not happy
with the with-in / between REWB solution (worried about the level 1
bias), I can further defend EQ 2 via its random coefficient/slope,
if I understand the Oct 2016 thread correctly.

So, my questions are:

(1) Is the above correctly reasoned?

(2) If the random slope model is a further defense against
self-selection bias, could someone provide an intuitive explanation
as to why? Is the idea that by allowing slopes to vary, there is no
endogeneity problem to solve as the very structure of the model
makes the correlated errors concern irrelevant?

Other solutions I explore include a Mundlak model, but per Bell et
al, the Mundlak models are not meaningful for repeated measures.
Also, it appears that the brms package appears to support mixed
modeling using instrumental variables, something I am more
comfortable with per my background, but strong instrumental variables are hard to find in the wild!

Thank you! - Kelly


       [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org><mailto:R-sig-mixed-models at r-project.org> mailing list
https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_ma
ilman_listinfo_r-2Dsig-2Dmixed-2Dmodels&d=DwIBaQ&c=7Q-FWLBTAxn3T_E3H
WrzGYJrC4RvUoWDrzTlitGRH_A&r=t-hV_EQcvMxUUCFqXmGPFL3N6XmAH6-xWI5Xpn-
HlYI&m=QIwJJAou0NQyfk892Wz-BodAH5I2A4aX08LX_ruukNk&s=4wSiK6P7-7_81bm
iLGX2F07zLv-M28Gd-4vDdwHogyk&e=


       [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org><mailto:R-sig-mixed-models at r-project.org> mailing list
https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
man_listinfo_r-2Dsig-2Dmixed-2Dmodels&d=DwIBaQ&c=7Q-FWLBTAxn3T_E3HWrzG
YJrC4RvUoWDrzTlitGRH_A&r=t-hV_EQcvMxUUCFqXmGPFL3N6XmAH6-xWI5Xpn-HlYI&m
=QIwJJAou0NQyfk892Wz-BodAH5I2A4aX08LX_ruukNk&s=4wSiK6P7-7_81bmiLGX2F07
zLv-M28Gd-4vDdwHogyk&e=

[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


	[[alternative HTML version deleted]]


From ch|r|eu @end|ng |rom gm@||@com  Tue Apr 14 16:29:46 2020
From: ch|r|eu @end|ng |rom gm@||@com (=?UTF-8?Q?David_Villegas_R=C3=ADos?=)
Date: Tue, 14 Apr 2020 16:29:46 +0200
Subject: [R-sig-ME] confidence intervals for interpolated values in logistic
 regression
Message-ID: <CALC46t9MRcfR-kMfKeu2ix-gHnQHgz-mtOaZpOXg+jKkY76xUg@mail.gmail.com>

Dear list,
I?m running a gam model (package mgcv) with a binary response variable (y),
and two continuous explanatory variables (x and z), plus their interaction
(x:z). I, therefore, obtain four coefficients from my model (intercept,
slope of x, slope of z and interaction coefficient).

I?m interested in obtaining the value of one of the explanatory variables
(x) for a particular level of the response variable, i.e. for a particular
probability level, and after fixing the value of the other explanatory
variable (z). Doing simple arithmetic, I can obtain the value of x that I?m
looking for, but I wonder how I can obtain a measure of error such a
confidence interval, so I can compare that value obtained from other
analogous models.

Is bootstrapping a good option or are there better alternatives? Any
practical advice/library to do so?

Thanks in advance,
David

	[[alternative HTML version deleted]]


From je@@|c@821112 @end|ng |rom gm@||@com  Tue Apr 14 23:13:01 2020
From: je@@|c@821112 @end|ng |rom gm@||@com (Chia-Yu Chen)
Date: Tue, 14 Apr 2020 23:13:01 +0200
Subject: [R-sig-ME] Question on glmer.nb in power simulation
Message-ID: <A1E4A739-A674-42B0-B384-BCE3A8E0FAB9@gmail.com>

Hi all,

I would like to ask for solution or advice on strange result that glmer.nb from lme4 generated when simulating using simR package. I?m working on longitudinal gut microbiome abundance data (23 patients, 2 cases (equal to time points), 482 bacteria), and they?re all count data. Here I will focus on only 2 bacteria among them, bacteria A and B. 

Bacteria_A <- structure(list(Individual = c(rep(c(26, 64, 1, 35, 33, 30, 3, 24, 55, 46, 39, 34, 16, 49, 61, 52, 28, 65, 62, 68, 74, 37, 67), each = 2)), Case = c(3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 3, 3, 2), Abundance_value = c(18, 4, 10, 2, 0, 0, 0, 0, 16, 1, 0, 0, 4, 16, 10, 18, 0, 0, 8, 7, 35, 16, 2, 22, 1, 6, 16, 9, 7, 12, 38, 32, 22, 4, 17, 13, 19, 20, 0, 6, 7, 13, 1, 22, 0, 0)),  class = "data.frame", row.names = c(NA, 46L), .Names = c("Individual", "Case", "Abundance_value"))

Bacteria_B <- structure(list(Individual = c(rep(c(26, 64, 1, 35, 33, 30, 3, 24, 55, 46, 39, 34, 16, 49, 61, 52, 28, 65, 62, 68, 74, 37, 67), each = 2)), Case = c(3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 3, 3, 2), Abundance_value = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)),  class = "data.frame", row.names = c(NA, 46L), .Names = c("Individual", "Case", "Abundance_value"))


I tried both lmer and glmer.nb, and the code is as below.

lmer(formula = rank(Abundance_value) ~ Case + (1| Individual)) # I use rank() here because abundance values aren?t normal-distributed
glmer.nb(formula = Abundance_value ~ Case + (1| Individual)) # I also tried negative binomial regression
car::Anova(model, type=c("II"),  test.statistic=c("Chisq?)) # Followed by Anova to get p value of ?Case"

I found that compared to using lmer, glmer.nb gave much lower p values, and thus resulting in more significance of bacterial abundance changing over time. However, among the bacteria that showed significance, there were many having really low effect size (paired Cliff?s delta) between 2 cases. I?m wondering if glmer.nb is so sensitive that it can detect such low effect size changes. I did simulation by using the powerCurve function from simR package to check the power of glmer.nb.

library(simR)
m_NB <- glmer.nb(formula = Abundance_value ~ Case + (1| Individual), REML = F)  # Fit the negative-binomial regression model
m_NB_ext <- extend(m_NB, n=1000, along = "Individual?)    # Use the ?extend" function in simR to increases sample size
powerCurve(m_NB_ext, along = "Individual", nsim = 1000, alpha = 0.1/N,  test = simr::fixed("Case", method = "chisq?)) # Plot the simulated power at different sample size. The tested target here is whether ?Case? (which is time point) can explain the variation of abundance value.

I plotted simulated power curve for different bacteria with Cliff?s delta between 2 cases being 0.609 (bacteria A) and 0.0435 (bacteria B). The power curve of 0.609 is smooth and looks normal, but the curve of 0.0435 looks really strange. It rose fast at the beginning, peaked at n=50, and then dropped all along the way to n = 600.  

Bacteria A power curve: https://drive.google.com/open?id=1B95_2WrNiSL4w9C2O315OXLKr4o_uq3c <https://drive.google.com/open?id=1B95_2WrNiSL4w9C2O315OXLKr4o_uq3c>
Bacteria B power curve:https://drive.google.com/open?id=1dSN0TA4-BxQVNZdWKsT5LT3VSMBlO6xg <https://drive.google.com/open?id=1dSN0TA4-BxQVNZdWKsT5LT3VSMBlO6xg>

The power curve of bacteria B is really weird; nevertheless, it did reflect the truth that I got many low-effect size yet significant bacteria at n = 23 in my data set. I thought of some possible reasons and would like to ask for your  advices.

1. It is due to sparsity. Most of my data are zero-inflated, and for bacteria A there were only 24% that the abundance equals to zero, while for bacteria B, there were 66%. Could the weird power curve being resulted from the high extent of zero-inflation? Is there any assumptions of data distribution for negative binomial regression using glmer.nb (or glmmTMB) that I wasn?t aware of? 
2. Maybe there is something wrong with my simR syntax?

Thank you very much for reading through this mail. Any advices or comments are appreciated! Thank you in advance.

Best regards,
Chia-Yu Chen
	[[alternative HTML version deleted]]


From h|gh@t@t @end|ng |rom h|gh@t@t@com  Wed Apr 15 15:28:03 2020
From: h|gh@t@t @end|ng |rom h|gh@t@t@com (Highland Statistics Ltd)
Date: Wed, 15 Apr 2020 14:28:03 +0100
Subject: [R-sig-ME] Online course with on-demand video and live meetings:
 Introduction to Regression Models with Spatial Correlation using R-INLA
Message-ID: <717a4609-7f6f-56ab-4199-96dee73ce77b@highstat.com>

We would like to announce the following online statistics course:


Online course with on-demand video and live meetings: 'Introduction to 
Regression Models with Spatial Correlation using R-INLA'

Remark: The course fee includes a 1-hour face-to-face video chat with 
one or both instructors.

When: 20 July - 7 August 2020
Time zone for live meetings: Multiple time zones, see the flyer.

Flyer: 
http://highstat.com/Courses/Flyers/2020/Flyer2020_07_SpatialGLM_Online.pdf

Website: http://highstat.com/index.php/courses-upcoming


Kind regards,


Alain Zuur

-- 

Dr. Alain F. Zuur
Highland Statistics Ltd.
9 St Clair Wynd
AB41 6DZ Newburgh, UK
Email:highstat at highstat.com
URL:www.highstat.com


	[[alternative HTML version deleted]]


From S@Breuge|m@n@ @end|ng |rom @tudent@ru@n|  Thu Apr 16 15:33:54 2020
From: S@Breuge|m@n@ @end|ng |rom @tudent@ru@n| (Breugelmans, S. (Sara))
Date: Thu, 16 Apr 2020 13:33:54 +0000
Subject: [R-sig-ME] Imputation methods mixed model analysis
Message-ID: <1587044036031.3411@student.ru.nl>

Dear colleagues,


For my thesis I am conducting a mixed model analysis on longitudinal data. However, when I tried to run an intercept-only model I got the following error:


preCORT_ICC <- lmer(Data_F_long$Cortisol_pre ~ 1 + (1 | Data_F_long$ID), data = Data_F_long)
Error in KhatriRao(sm, t(mm)) : (p <- ncol(X)) == ncol(Y) is not TRUE

?

When I searched this error I saw that it might have to do something with the number of NA's. So then I thought it would be better to use some kind of imputation strategy.

I was wondering if there is a build-in function in lmer() to do this. Or is it better to manually impute the data before analysing.


I already found a function called mice(). Does anyone of you have experience with mice() and would you recommend using it?


Of course it would be more convenient if I could use some kind of build-in function in lmer().


Thank you for your response!


Kind regards


Sam

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Thu Apr 16 15:40:56 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Thu, 16 Apr 2020 15:40:56 +0200
Subject: [R-sig-ME] Imputation methods mixed model analysis
In-Reply-To: <1587044036031.3411@student.ru.nl>
References: <1587044036031.3411@student.ru.nl>
Message-ID: <CAJuCY5wHxWVJ+=YXw2rZWiRS4zrozRgkwTTQQxXV+Vn++o+iPw@mail.gmail.com>

Dear Sam,

Don't use the data.frame$ notation in your formula. Just use the code below.

preCORT_ICC <- lmer(Cortisol_pre ~ 1 + (1 | ID), data = Data_F_long)

Which values are missing: Cortisol_pre or ID?

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op do 16 apr. 2020 om 15:34 schreef Breugelmans, S. (Sara) <
S.Breugelmans at student.ru.nl>:

> Dear colleagues,
>
>
> For my thesis I am conducting a mixed model analysis on longitudinal data.
> However, when I tried to run an intercept-only model I got the following
> error:
>
>
> preCORT_ICC <- lmer(Data_F_long$Cortisol_pre ~ 1 + (1 | Data_F_long$ID),
> data = Data_F_long)
> Error in KhatriRao(sm, t(mm)) : (p <- ncol(X)) == ncol(Y) is not TRUE
>
> ?
>
> When I searched this error I saw that it might have to do something with
> the number of NA's. So then I thought it would be better to use some kind
> of imputation strategy.
>
> I was wondering if there is a build-in function in lmer() to do this. Or
> is it better to manually impute the data before analysing.
>
>
> I already found a function called mice(). Does anyone of you have
> experience with mice() and would you recommend using it?
>
>
> Of course it would be more convenient if I could use some kind of build-in
> function in lmer().
>
>
> Thank you for your response!
>
>
> Kind regards
>
>
> Sam
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From j|ox @end|ng |rom mcm@@ter@c@  Thu Apr 16 16:27:57 2020
From: j|ox @end|ng |rom mcm@@ter@c@ (Fox, John)
Date: Thu, 16 Apr 2020 14:27:57 +0000
Subject: [R-sig-ME] Imputation methods mixed model analysis
In-Reply-To: <3834_1587044058_03GDYDiu031711_1587044036031.3411@student.ru.nl>
References: <3834_1587044058_03GDYDiu031711_1587044036031.3411@student.ru.nl>
Message-ID: <6A4A5818-7ADF-46E3-A703-377B6A3D6985@mcmaster.ca>

Dear Sam,

By (redundantly) using $ indexing for the Data_F_long data frame to specify the variables in the model formula, you're preventing lmer() from handling the missing data properly. Use the formula Cortisol_pre ~ 1 + (1 | ID instead.

Whether or not you should use multiple imputation (or some other strategy) for the missing data isn't a technical issue about getting lmer() to work but rather a statistical question. Used properly and with default settings in R, lmer() will perform a complete-case analysis, deleting any row in the data set with NAs. If you have a lot of missing data, complete-case analysis can be problematic, and using another approach, such as multiple imputation, can be better. 

If you use multiple imputation, the mice package is a sound implementation. See, for example, the on-line appendix on multiple imputation from Fox and Weisberg, An R Companion to Applied Regression, at <https://socialsciences.mcmaster.ca/jfox/Books/Companion/appendices/Appendix-Multiple-Imputation.pdf>, and the references given there. There are, however, special considerations for multiple imputation in mixed-effects models that aren't discussed in this appendix.

I hope this helps,
 John

  ----------------------------- 
  John Fox, Professor Emeritus
  McMaster University
  Hamilton, Ontario, Canada
  Web: http::/socserv.mcmaster.ca/jfox

> On Apr 16, 2020, at 9:33 AM, Breugelmans, S. (Sara) <S.Breugelmans at student.ru.nl> wrote:
> 
> Dear colleagues,
> 
> 
> For my thesis I am conducting a mixed model analysis on longitudinal data. However, when I tried to run an intercept-only model I got the following error:
> 
> 
> preCORT_ICC <- lmer(Data_F_long$Cortisol_pre ~ 1 + (1 | Data_F_long$ID), data = Data_F_long)
> Error in KhatriRao(sm, t(mm)) : (p <- ncol(X)) == ncol(Y) is not TRUE

> ?
> 
> When I searched this error I saw that it might have to do something with the number of NA's. So then I thought it would be better to use some kind of imputation strategy.
> 
> I was wondering if there is a build-in function in lmer() to do this. Or is it better to manually impute the data before analysing.
> 
> 
> I already found a function called mice(). Does anyone of you have experience with mice() and would you recommend using it?
> 
> 
> Of course it would be more convenient if I could use some kind of build-in function in lmer().
> 
> 
> Thank you for your response!
> 
> 
> Kind regards
> 
> 
> Sam
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From j@ork|n @end|ng |rom @om@um@ry|@nd@edu  Thu Apr 16 17:17:12 2020
From: j@ork|n @end|ng |rom @om@um@ry|@nd@edu (Sorkin, John)
Date: Thu, 16 Apr 2020 15:17:12 +0000
Subject: [R-sig-ME] Imputation methods mixed model analysis
In-Reply-To: <1587044036031.3411@student.ru.nl>
References: <1587044036031.3411@student.ru.nl>
Message-ID: <MN2PR03MB51674AC4E51B302E35D54E37E2D80@MN2PR03MB5167.namprd03.prod.outlook.com>

MICE is an algorithm that allows imputation of data. Rather than using the same strategy to impute data, MICE allows selection of  imputation methods that are most appropriate for the datum being imputed; one variable might be imputed using linear regression, another using logistic regression, etc.
Generally one imputes data using a program (or function) that is designed for imputation and then analyzes the data using a program (or function) that is designed specifically to analyze the data. By separating imputation from analysis, once one learns imputation, one can use imputation for any data set, and needs not learn the ins and outs of a specific analysis' imputation technique.
John


John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)


________________________________
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on behalf of Breugelmans, S. (Sara) <S.Breugelmans at student.ru.nl>
Sent: Thursday, April 16, 2020 9:33 AM
To: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: [R-sig-ME] Imputation methods mixed model analysis

Dear colleagues,


For my thesis I am conducting a mixed model analysis on longitudinal data. However, when I tried to run an intercept-only model I got the following error:


preCORT_ICC <- lmer(Data_F_long$Cortisol_pre ~ 1 + (1 | Data_F_long$ID), data = Data_F_long)
Error in KhatriRao(sm, t(mm)) : (p <- ncol(X)) == ncol(Y) is not TRUE

?

When I searched this error I saw that it might have to do something with the number of NA's. So then I thought it would be better to use some kind of imputation strategy.

I was wondering if there is a build-in function in lmer() to do this. Or is it better to manually impute the data before analysing.


I already found a function called mice(). Does anyone of you have experience with mice() and would you recommend using it?


Of course it would be more convenient if I could use some kind of build-in function in lmer().


Thank you for your response!


Kind regards


Sam

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-mixed-models&amp;data=02%7C01%7C%7C9772f2b68789475ada6c08d7e20ad9ef%7C717009a620de461a88940312a395cac9%7C0%7C0%7C637226408544623841&amp;sdata=UhsJ%2FSj7ge5ef1LV3yHBDgev8npGC1q7YZVCp8guCRU%3D&amp;reserved=0

	[[alternative HTML version deleted]]


From k@r| @end|ng |rom hu|t|@@org  Thu Apr 16 20:33:50 2020
From: k@r| @end|ng |rom hu|t|@@org (Karl Ove Hufthammer)
Date: Thu, 16 Apr 2020 20:33:50 +0200
Subject: [R-sig-ME] Imputation methods mixed model analysis
In-Reply-To: <1587044036031.3411@student.ru.nl>
References: <1587044036031.3411@student.ru.nl>
Message-ID: <c288be16-bd72-24bf-a2f3-f22a73cab8ac@huftis.org>

Breugelmans, S. (Sara) skreiv 16.04.2020 15:33:
> So then I thought it would be better to use some kind of imputation strategy. [?] I was wondering if there is a build-in function in lmer() to do this. Or is it better to manually impute the data before analysing.

If you only have missing data in your response variable (LHS of the 
formula), you probably don?t need to impute your data. If you (also) 
have missing data in your explanatory variables, imputation is probably 
a good idea.


> I already found a function called mice(). Does anyone of you have experience with mice() and would you recommend using it?

Yes, the ?mice? package is great. But doing proper multiple imputation 
for mixed models can be surpringly tricky. I recommend reading the 
?Flexible Imputation of Missing Data? book, written by the author of the 
?mice? package. It?s available online, for free:

https://stefvanbuuren.name/fimd/

Chapter 7 deals with imputation for multilevel data.

-- 
Karl Ove Hufthammer


From j@de@ @end|ng |rom he@|th@uc@d@edu  Fri Apr 17 06:33:45 2020
From: j@de@ @end|ng |rom he@|th@uc@d@edu (Ades, James)
Date: Fri, 17 Apr 2020 04:33:45 +0000
Subject: [R-sig-ME] Multivariate Growth Curve Analyses in lme4
Message-ID: <BY5PR19MB38599B2B97CE5D46E24AAB60EAD90@BY5PR19MB3859.namprd19.prod.outlook.com>

Hi all,


I?ve been reading through Growth Curve Analysis and Visualization in R to model executive function growth over four time points. The book is a good launching pad; however, it falls short when it comes to multivariate growth curve analysis. Additionally, I haven?t been able to find much on the internet regarding multivariate growth curve analysis in R, let alone in lme4.


I?ve never really done any multivariate analysis. Growth curve analyses doesn?t seem too different from regular multilevel modeling but for the time points occurring as the first level within the second level of participant.


1) As a barebones bivariate model with DVs of a standardized math assessment score and an aggregate executive function score would something like


```

lmer(mvbind(math.score, mean.eff) ~ timepoint + (time | participant), dat?)

```

be correct? (I have time as a random slope given that the space in between when students were tested could differ by several months, so "time point" per se seemed biased). From that point, I could add in fixed effects of cohort, parent education level, etc.


2) Would it be possible to model three components from a PCA of executive function with math score in a similar model? How would one do that? Just create a new row with the four measures and run the row as the DV?


3) Lastly, I?m wondering whether AIC model comparison goes out the window if you were to model PCA (3 components)  and aggregate score (of EF)? This question applies to both univariate and multivariate models. There would be the same number of subjects, etc. but the number of observations would ostensibly triple, given that three components would be modeled vs a more parsimonious mean score. That is, even after accounting for the variability in PCA among subjects with a model like


```

lmer(mean.eff ~ pca + timepoint + (time | participant) + (time | pca:participant), dat?)

```

If these models aren?t comparable with AIC, would LOOCV be the next best option?


Thanks much!


James



	[[alternative HTML version deleted]]


From n|@||@m|||@r @end|ng |rom w@u@edu  Fri Apr 17 22:51:57 2020
From: n|@||@m|||@r @end|ng |rom w@u@edu (Millar, Niall)
Date: Fri, 17 Apr 2020 20:51:57 +0000
Subject: [R-sig-ME] Emmeans & effects packages: Post-hoc tests for Tweedie
 glmmTMB model
Message-ID: <MWHPR0101MB3056B76FB917FBAB04EAA9C98ED90@MWHPR0101MB3056.prod.exchangelabs.com>

Are there any problems with using the effects and emmeans packages to interpret glmmTMB models with Tweedie distributions?

I have glmmTMB models using the Tweedie distribution, and I want to draw inference about the direction and magnitude of the predicted effects. In the model given below:

glmmTMB(Shoot.weight ~ Species + N.Level + Rhiz + Species:N.Level + Species:Rhiz + N.Level:Rhiz + N.Level:Rhiz:Species + (1|Block) + (1|Acc), family=tweedie, data=DcleanSOY)

I have a significant effect of the N.Level:Rhiz:Species interaction term (based on a likelihood ratio test). N level is a continuous predictor, and Rhiz and Species are both two-level factors.

I have used the effects package to make a plot with 95% confidence intervals to interpret this 3-way interaction. I have also made separate models for each level of the N level predictor, and used emmeans to get pairwise Tukey contrasts of the Rhiz and Species predictors at each N level.

Can I base my inference on these 95% confidence intervals and/or emmeans contrasts, or are these methods not supported for Tweedie glmmTMB models?

Thanks! -Niall


Niall Millar
PhD Candidate
Porter Lab
School of Biological Sciences

Washington State University Vancouver
14204 NE Salmon Creek Ave
Vancouver, WA  98686
360-784-9816
niall.millar at wsu.edu

	[[alternative HTML version deleted]]


From j|ox @end|ng |rom mcm@@ter@c@  Fri Apr 17 23:03:38 2020
From: j|ox @end|ng |rom mcm@@ter@c@ (Fox, John)
Date: Fri, 17 Apr 2020 21:03:38 +0000
Subject: [R-sig-ME] 
 Emmeans & effects packages: Post-hoc tests for Tweedie glmmTMB model
In-Reply-To: <4153_1587156743_03HKqM51027994_MWHPR0101MB3056B76FB917FBAB04EAA9C98ED90@MWHPR0101MB3056.prod.exchangelabs.com>
References: <4153_1587156743_03HKqM51027994_MWHPR0101MB3056B76FB917FBAB04EAA9C98ED90@MWHPR0101MB3056.prod.exchangelabs.com>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC88CE25F05@FHSDB2D11-2.csu.mcmaster.ca>

Dear Niall,

I've not seen applications of GLMMs using the Tweedie Family, but don't see why that should matter. The "effects" computed by the functions in the effects package are linear functions of the estimated fixed-effects parameters, and their asymptotic standard errors are computed from the estimated covariance matrix of the fixed effects. The default (asymptotic) confidence intervals are computed pointwise and so don't adjust for simultaneous inference, but there is an option to compute Scheffe intervals.

By the way, the fixed-effects part of your model is more compactly Shoot.weight ~ N.Level*Rhiz*Species.

I hope this helps,
 John

--------------------------------------
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
Web: socialsciences.mcmaster.ca/jfox/



> -----Original Message-----
> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On
> Behalf Of Millar, Niall
> Sent: Friday, April 17, 2020 4:52 PM
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] Emmeans & effects packages: Post-hoc tests for Tweedie
> glmmTMB model
> 
> Are there any problems with using the effects and emmeans packages to
> interpret glmmTMB models with Tweedie distributions?
> 
> I have glmmTMB models using the Tweedie distribution, and I want to draw
> inference about the direction and magnitude of the predicted effects. In
> the model given below:
> 
> glmmTMB(Shoot.weight ~ Species + N.Level + Rhiz + Species:N.Level +
> Species:Rhiz + N.Level:Rhiz + N.Level:Rhiz:Species + (1|Block) + (1|Acc),
> family=tweedie, data=DcleanSOY)
> 
> I have a significant effect of the N.Level:Rhiz:Species interaction term
> (based on a likelihood ratio test). N level is a continuous predictor, and
> Rhiz and Species are both two-level factors.
> 
> I have used the effects package to make a plot with 95% confidence
> intervals to interpret this 3-way interaction. I have also made separate
> models for each level of the N level predictor, and used emmeans to get
> pairwise Tukey contrasts of the Rhiz and Species predictors at each N
> level.
> 
> Can I base my inference on these 95% confidence intervals and/or emmeans
> contrasts, or are these methods not supported for Tweedie glmmTMB models?
> 
> Thanks! -Niall
> 
> 
> Niall Millar
> PhD Candidate
> Porter Lab
> School of Biological Sciences
> 
> Washington State University Vancouver
> 14204 NE Salmon Creek Ave
> Vancouver, WA  98686
> 360-784-9816
> niall.millar at wsu.edu
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From m@tteo@@b@ @end|ng |rom gm@||@com  Sun Apr 19 15:59:12 2020
From: m@tteo@@b@ @end|ng |rom gm@||@com (Matteo Sebastianelli)
Date: Sun, 19 Apr 2020 16:59:12 +0300
Subject: [R-sig-ME] Troubleshooting glmmTMB
Message-ID: <CAHLZGq83D=GaJsoc0R+UP4ER_vqjzFX4SOJ8suz814ZbKnuj4g@mail.gmail.com>

Hi everyone!

Hope to be clear enough since this is going to be my fist quest.
I was running 3 sets of models with glmer, each set of models had a
different response variable representing number of 3 different species of
birds counted during transects. No problem for the first 2 sets of models
with glmer, which I then compared with AICc to get the best model. For the
third species, by far the less counted during the transect, I found that
the model was underdispersed (~0.6). I decided to switch to genpois glmmTMB
model which can deal with underdispersed models. The response is not zero
inflated according to DHARMa testZeroInflation. I used the same approach
i.e. a set of model with different combinations of predictors which i would
then compare. The issue is that for few models i get the warning: extreme
or very small eigenvalues detected. Continuous predictors are already
scaled. I then tried to run the function I found in the glmmTMB
troubleshooting vignette (Example 3) to detect the parameters that
contribute to the small eigenvalues but i get a warning:

diagnose_vcov(mod2)
Error in diagnose_vcov(mod2) : can't analyze vcov
In addition: Warning message:
In diagnose_vcov(mod2) : analyzing Hessian, not vcov

Do you have any idea how I can overcome the problem?

I also have another question. In the models I have an observerID random
factor with 2 levels, and I know the random factors should have at least
5-6 levels. Do you think is ok to include them in the model even though it
creates a singular fit warning (in glmmTMB i get the non-positive-definite
Hessian matrix warning)?

Looking forward to hear from you.

Best,

Matteo

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Mon Apr 20 01:48:08 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sun, 19 Apr 2020 19:48:08 -0400
Subject: [R-sig-ME] Troubleshooting glmmTMB
In-Reply-To: <CAHLZGq83D=GaJsoc0R+UP4ER_vqjzFX4SOJ8suz814ZbKnuj4g@mail.gmail.com>
References: <CAHLZGq83D=GaJsoc0R+UP4ER_vqjzFX4SOJ8suz814ZbKnuj4g@mail.gmail.com>
Message-ID: <4514f1f8-d847-b8ea-355b-0a0c5f5ac1c2@gmail.com>


On 4/19/20 9:59 AM, Matteo Sebastianelli wrote:
> Hi everyone!
>
> Hope to be clear enough since this is going to be my fist quest.
> I was running 3 sets of models with glmer, each set of models had a
> different response variable representing number of 3 different species of
> birds counted during transects.

 ?? glmer with Poisson response?

> her.)No problem for the first 2 sets of models
> with glmer, which I then compared with AICc to get the best model. For the
> third species, by far the less counted during the transect, I found that
> the model was underdispersed (~0.6). I decided to switch to genpois glmmTMB
> model which can deal with underdispersed models. The response is not zero
> inflated according to DHARMa testZeroInflation.
 ?? This is not a bad idea, but note that estimates of dispersion can be 
very imprecise (i.e., it may not be as necessary to move away from 
Poisson as you think).

 ?? Do you have reason to suspect zero-inflation or are you just trying 
to cover your bases?

> I used the same approach
> i.e. a set of model with different combinations of predictors which i would
> then compare. The issue is that for few models i get the warning: extreme
> or very small eigenvalues detected. Continuous predictors are already
> scaled.

 ?? Most often this is caused by an extreme parameter, e.g. if you are 
trying to fit zero-inflation with a data set that really doesn't have 
any, or a negative binomial model with equi- or underdispersion

> I then tried to run the function I found in the glmmTMB
> troubleshooting vignette (Example 3) to detect the parameters that
> contribute to the small eigenvalues but i get a warning:
>
> diagnose_vcov(mod2)
> Error in diagnose_vcov(mod2) : can't analyze vcov
> In addition: Warning message:
> In diagnose_vcov(mod2) : analyzing Hessian, not vcov
>
> Do you have any idea how I can overcome the problem?

 ? Try changing the missing(analyze_hessian)) clause to:


 ?????? if (missing(analyze_hessian)) {
 ??????????? warning("analyzing Hessian, not vcov")
 ??????????? analyze_hessian <- TRUE
 ??????? } else {
 ??????????? if (!analyze_hessian) stop("can't analyze vcov")
 ??????? }

>
> I also have another question. In the models I have an observerID random
> factor with 2 levels, and I know the random factors should have at least
> 5-6 levels. Do you think is ok to include them in the model even though it
> creates a singular fit warning (in glmmTMB i get the non-positive-definite
> Hessian matrix warning)?

 ?? I wouldn't, I'd include it as a fixed effect or leave it out.

>
> Looking forward to hear from you.
>
> Best,
>
> Matteo
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From emm@nue|@cur|@ @end|ng |rom p@r|@de@c@rte@@|r  Mon Apr 20 09:48:51 2020
From: emm@nue|@cur|@ @end|ng |rom p@r|@de@c@rte@@|r (Emmanuel Curis)
Date: Mon, 20 Apr 2020 09:48:51 +0200
Subject: [R-sig-ME] Precision about the glmer model for Bernoulli variables
Message-ID: <20200420074851.GA6361@info124.pharmacie.univ-paris5.fr>

Hello everyone,

I hope you're all going fine in these difficult times.

I tried to understand in details the exact model used when using glmer
for a Bernoulli experiment, by comparison with the linear mixed
effects model, and especially how it introducts correlations between
observations of a given group.  I think I finally got it, but could
you check that what I write below is correct and that I'm not missing
something?

I use a very simple case with only a single random effect, and no
fixed effects, because I guess that adding fixed effects or other
random effects does not change the idea, it "just" makes formulas more
complex.  I note i the random effect level, let's say ? patient ?, and
j the observation for this patient.

In the linear model, we have Y(i,j) = ?0 + Z(i) + epsilon( i, j ) with
Z(i) and epsilon(i,j) randoms variables having a density of
probability, independant, and each iid.

Hence, cov( Y(i,j), Y(i,j') ) = Var( Z(i) ): the model introduces a
positive correlation between observations of the same patient.



In the Bernoulli model, Y(i,j) ~ B( pi(i,j) ) and pi(i,j) = f( Z(i) ),
f being the inverse link function, typically the reciprocal of the
logit. So we have

cov( Y(i,j), Y(i,j') ) = E( Y(i,j) Y(i, j') ) - E( Y(i,j) ) E( Y(i,j') )
     = Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) - pi(i,j) * pi(i,j')

Since in practice pi(i,j) does not depend on j, pi(i,j) = pi(i,j').

Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
  integral(R) Pr( Y(i,j) = 1 inter Y(i,j') = 1 | Z(i) = z ) p( Z(i) = z ) dz

Then, we assume that conditionnally on Zi, the Yij are independant, is
this right? This is the equivalent of ? the epsilon(i, j) are
independant ?? I assume this hypothesis is also used for computing the
likelihood? If not, what is the model for the joint probability?

In that case,

Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
  integral(R) f(z) f(z) p( Z(i) = z ) dz

and since pi(i,j) = integral( R ) f(z) p( Z(i) = z ) dz we have

cov( Y(i,j), Y(i,j') ) =
 integral( R ) f?(z) p( Z(i) = z ) dz -
  ( integral( R ) f(z) p( Z(i) = z ) dz )?

which in general has no reason to be nul, hence the two observations
are correlated. Is this correct?

Is there any way to have a closed-form of the covariance, for usual f
(let's say, logit or probit) and Z distribution (let's say, Gaussian)?

Thanks a lot for reading, and your answers,

-- 
                                Emmanuel CURIS
                                emmanuel.curis at parisdescartes.fr

Page WWW: http://emmanuel.curis.online.fr/index.html


From |v@|d@ @end|ng |rom he@|th@uc@d@edu  Mon Apr 20 17:27:39 2020
From: |v@|d@ @end|ng |rom he@|th@uc@d@edu (Vaida, Florin)
Date: Mon, 20 Apr 2020 15:27:39 +0000
Subject: [R-sig-ME] 
 Precision about the glmer model for Bernoulli variables
In-Reply-To: <20200420074851.GA6361@info124.pharmacie.univ-paris5.fr>
References: <20200420074851.GA6361@info124.pharmacie.univ-paris5.fr>
Message-ID: <AB72DE88-C199-459C-B0EB-0DF13BB1659B@health.ucsd.edu>

Hi Emmanuel,

Your reasoning is correct.

As a quibble, outside a simple repeated measures experiment setup, p(i,j) *does* depend on j.
For example, if observations are collected over time, generally there is a time effect; if they are repeated measures with different experimental conditions, p(i,j) will depend on the condition j, etc.

There is almost certainly no closed form solution for the covariance under logit.
I am not sure about the probit (my guess is not).
There will be some Laplace approximations available, a la Breslow and Clayton 1993.

I'd be curious if these formulas/approximations were developed somewhere - I'd be surprised if they weren't.

Florin


> On Apr 20, 2020, at 12:48 AM, Emmanuel Curis <emmanuel.curis at parisdescartes.fr> wrote:
> 
> Hello everyone,
> 
> I hope you're all going fine in these difficult times.
> 
> I tried to understand in details the exact model used when using glmer
> for a Bernoulli experiment, by comparison with the linear mixed
> effects model, and especially how it introducts correlations between
> observations of a given group.  I think I finally got it, but could
> you check that what I write below is correct and that I'm not missing
> something?
> 
> I use a very simple case with only a single random effect, and no
> fixed effects, because I guess that adding fixed effects or other
> random effects does not change the idea, it "just" makes formulas more
> complex.  I note i the random effect level, let's say ? patient ?, and
> j the observation for this patient.
> 
> In the linear model, we have Y(i,j) = ?0 + Z(i) + epsilon( i, j ) with
> Z(i) and epsilon(i,j) randoms variables having a density of
> probability, independant, and each iid.
> 
> Hence, cov( Y(i,j), Y(i,j') ) = Var( Z(i) ): the model introduces a
> positive correlation between observations of the same patient.
> 
> 
> 
> In the Bernoulli model, Y(i,j) ~ B( pi(i,j) ) and pi(i,j) = f( Z(i) ),
> f being the inverse link function, typically the reciprocal of the
> logit. So we have
> 
> cov( Y(i,j), Y(i,j') ) = E( Y(i,j) Y(i, j') ) - E( Y(i,j) ) E( Y(i,j') )
>     = Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) - pi(i,j) * pi(i,j')
> 
> Since in practice pi(i,j) does not depend on j, pi(i,j) = pi(i,j').
> 
> Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
>  integral(R) Pr( Y(i,j) = 1 inter Y(i,j') = 1 | Z(i) = z ) p( Z(i) = z ) dz
> 
> Then, we assume that conditionnally on Zi, the Yij are independant, is
> this right? This is the equivalent of ? the epsilon(i, j) are
> independant ?? I assume this hypothesis is also used for computing the
> likelihood? If not, what is the model for the joint probability?
> 
> In that case,
> 
> Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
>  integral(R) f(z) f(z) p( Z(i) = z ) dz
> 
> and since pi(i,j) = integral( R ) f(z) p( Z(i) = z ) dz we have
> 
> cov( Y(i,j), Y(i,j') ) =
> integral( R ) f?(z) p( Z(i) = z ) dz -
>  ( integral( R ) f(z) p( Z(i) = z ) dz )?
> 
> which in general has no reason to be nul, hence the two observations
> are correlated. Is this correct?
> 
> Is there any way to have a closed-form of the covariance, for usual f
> (let's say, logit or probit) and Z distribution (let's say, Gaussian)?
> 
> Thanks a lot for reading, and your answers,
> 
> -- 
>                                Emmanuel CURIS
>                                emmanuel.curis at parisdescartes.fr
> 
> Page WWW: http://emmanuel.curis.online.fr/index.html
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From emm@nue|@cur|@ @end|ng |rom p@r|@de@c@rte@@|r  Mon Apr 20 21:32:49 2020
From: emm@nue|@cur|@ @end|ng |rom p@r|@de@c@rte@@|r (Emmanuel Curis)
Date: Mon, 20 Apr 2020 21:32:49 +0200
Subject: [R-sig-ME] 
 Precision about the glmer model for Bernoulli variables
In-Reply-To: <AB72DE88-C199-459C-B0EB-0DF13BB1659B@health.ucsd.edu>
References: <20200420074851.GA6361@info124.pharmacie.univ-paris5.fr>
 <AB72DE88-C199-459C-B0EB-0DF13BB1659B@health.ucsd.edu>
Message-ID: <20200420193249.GB16474@info124.pharmacie.univ-paris5.fr>

Hi Florin,

Thanks for the answer, the precision about p(i,j), and the reference.

A last question, that I forgot in my message: is the obtained
correlation also always positive, as in the linear case? Or may some
negative correlation appear, depending on the values of pi(i,j) and
pi(i,j')?

Best regards,

On Mon, Apr 20, 2020 at 03:27:39PM +0000, Vaida, Florin wrote:
? Hi Emmanuel,
? 
? Your reasoning is correct.
? 
? As a quibble, outside a simple repeated measures experiment setup, p(i,j) *does* depend on j.
? For example, if observations are collected over time, generally there is a time effect; if they are repeated measures with different experimental conditions, p(i,j) will depend on the condition j, etc.
? 
? There is almost certainly no closed form solution for the covariance under logit.
? I am not sure about the probit (my guess is not).
? There will be some Laplace approximations available, a la Breslow and Clayton 1993.
? 
? I'd be curious if these formulas/approximations were developed somewhere - I'd be surprised if they weren't.
? 
? Florin
? 
? 
? > On Apr 20, 2020, at 12:48 AM, Emmanuel Curis <emmanuel.curis at parisdescartes.fr> wrote:
? > 
? > Hello everyone,
? > 
? > I hope you're all going fine in these difficult times.
? > 
? > I tried to understand in details the exact model used when using glmer
? > for a Bernoulli experiment, by comparison with the linear mixed
? > effects model, and especially how it introducts correlations between
? > observations of a given group.  I think I finally got it, but could
? > you check that what I write below is correct and that I'm not missing
? > something?
? > 
? > I use a very simple case with only a single random effect, and no
? > fixed effects, because I guess that adding fixed effects or other
? > random effects does not change the idea, it "just" makes formulas more
? > complex.  I note i the random effect level, let's say ? patient ?, and
? > j the observation for this patient.
? > 
? > In the linear model, we have Y(i,j) = ?0 + Z(i) + epsilon( i, j ) with
? > Z(i) and epsilon(i,j) randoms variables having a density of
? > probability, independant, and each iid.
? > 
? > Hence, cov( Y(i,j), Y(i,j') ) = Var( Z(i) ): the model introduces a
? > positive correlation between observations of the same patient.
? > 
? > 
? > 
? > In the Bernoulli model, Y(i,j) ~ B( pi(i,j) ) and pi(i,j) = f( Z(i) ),
? > f being the inverse link function, typically the reciprocal of the
? > logit. So we have
? > 
? > cov( Y(i,j), Y(i,j') ) = E( Y(i,j) Y(i, j') ) - E( Y(i,j) ) E( Y(i,j') )
? >     = Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) - pi(i,j) * pi(i,j')
? > 
? > Since in practice pi(i,j) does not depend on j, pi(i,j) = pi(i,j').
? > 
? > Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
? >  integral(R) Pr( Y(i,j) = 1 inter Y(i,j') = 1 | Z(i) = z ) p( Z(i) = z ) dz
? > 
? > Then, we assume that conditionnally on Zi, the Yij are independant, is
? > this right? This is the equivalent of ? the epsilon(i, j) are
? > independant ?? I assume this hypothesis is also used for computing the
? > likelihood? If not, what is the model for the joint probability?
? > 
? > In that case,
? > 
? > Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
? >  integral(R) f(z) f(z) p( Z(i) = z ) dz
? > 
? > and since pi(i,j) = integral( R ) f(z) p( Z(i) = z ) dz we have
? > 
? > cov( Y(i,j), Y(i,j') ) =
? > integral( R ) f?(z) p( Z(i) = z ) dz -
? >  ( integral( R ) f(z) p( Z(i) = z ) dz )?
? > 
? > which in general has no reason to be nul, hence the two observations
? > are correlated. Is this correct?
? > 
? > Is there any way to have a closed-form of the covariance, for usual f
? > (let's say, logit or probit) and Z distribution (let's say, Gaussian)?
? > 
? > Thanks a lot for reading, and your answers,
? > 
? > -- 
? >                                Emmanuel CURIS
? >                                emmanuel.curis at parisdescartes.fr
? > 
? > Page WWW: http://emmanuel.curis.online.fr/index.html
? > 
? > _______________________________________________
? > R-sig-mixed-models at r-project.org mailing list
? > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
? 

-- 
                                Emmanuel CURIS
                                emmanuel.curis at parisdescartes.fr

Page WWW: http://emmanuel.curis.online.fr/index.html


From |v@|d@ @end|ng |rom he@|th@uc@d@edu  Mon Apr 20 23:05:40 2020
From: |v@|d@ @end|ng |rom he@|th@uc@d@edu (Vaida, Florin)
Date: Mon, 20 Apr 2020 21:05:40 +0000
Subject: [R-sig-ME] 
 Precision about the glmer model for Bernoulli variables
In-Reply-To: <20200420193249.GB16474@info124.pharmacie.univ-paris5.fr>
References: <20200420074851.GA6361@info124.pharmacie.univ-paris5.fr>
 <AB72DE88-C199-459C-B0EB-0DF13BB1659B@health.ucsd.edu>
 <20200420193249.GB16474@info124.pharmacie.univ-paris5.fr>
Message-ID: <2AD9CAD5-FABC-4A13-B684-6D0908CFBF21@health.ucsd.edu>

Hi Emmanuel,

That's a good question.  My guess is that the correlation is non-negative generally, but I wasn't able to prove it theoretically even in the simplest case when Y1, Y2 ~ Bernoulli(u) independent conditionally on u, and u ~ Normal(0, 1).  I am curious if someone has a solution.
We can't go too far down this route in this forum, since Doug wants to keep it applied.

Florin

> On Apr 20, 2020, at 12:32 PM, Emmanuel Curis <emmanuel.curis at parisdescartes.fr> wrote:
> 
> Hi Florin,
> 
> Thanks for the answer, the precision about p(i,j), and the reference.
> 
> A last question, that I forgot in my message: is the obtained
> correlation also always positive, as in the linear case? Or may some
> negative correlation appear, depending on the values of pi(i,j) and
> pi(i,j')?
> 
> Best regards,
> 
> On Mon, Apr 20, 2020 at 03:27:39PM +0000, Vaida, Florin wrote:
> ? Hi Emmanuel,
> ? 
> ? Your reasoning is correct.
> ? 
> ? As a quibble, outside a simple repeated measures experiment setup, p(i,j) *does* depend on j.
> ? For example, if observations are collected over time, generally there is a time effect; if they are repeated measures with different experimental conditions, p(i,j) will depend on the condition j, etc.
> ? 
> ? There is almost certainly no closed form solution for the covariance under logit.
> ? I am not sure about the probit (my guess is not).
> ? There will be some Laplace approximations available, a la Breslow and Clayton 1993.
> ? 
> ? I'd be curious if these formulas/approximations were developed somewhere - I'd be surprised if they weren't.
> ? 
> ? Florin
> ? 
> ? 
> ? > On Apr 20, 2020, at 12:48 AM, Emmanuel Curis <emmanuel.curis at parisdescartes.fr> wrote:
> ? > 
> ? > Hello everyone,
> ? > 
> ? > I hope you're all going fine in these difficult times.
> ? > 
> ? > I tried to understand in details the exact model used when using glmer
> ? > for a Bernoulli experiment, by comparison with the linear mixed
> ? > effects model, and especially how it introducts correlations between
> ? > observations of a given group.  I think I finally got it, but could
> ? > you check that what I write below is correct and that I'm not missing
> ? > something?
> ? > 
> ? > I use a very simple case with only a single random effect, and no
> ? > fixed effects, because I guess that adding fixed effects or other
> ? > random effects does not change the idea, it "just" makes formulas more
> ? > complex.  I note i the random effect level, let's say ? patient ?, and
> ? > j the observation for this patient.
> ? > 
> ? > In the linear model, we have Y(i,j) = ?0 + Z(i) + epsilon( i, j ) with
> ? > Z(i) and epsilon(i,j) randoms variables having a density of
> ? > probability, independant, and each iid.
> ? > 
> ? > Hence, cov( Y(i,j), Y(i,j') ) = Var( Z(i) ): the model introduces a
> ? > positive correlation between observations of the same patient.
> ? > 
> ? > 
> ? > 
> ? > In the Bernoulli model, Y(i,j) ~ B( pi(i,j) ) and pi(i,j) = f( Z(i) ),
> ? > f being the inverse link function, typically the reciprocal of the
> ? > logit. So we have
> ? > 
> ? > cov( Y(i,j), Y(i,j') ) = E( Y(i,j) Y(i, j') ) - E( Y(i,j) ) E( Y(i,j') )
> ? >     = Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) - pi(i,j) * pi(i,j')
> ? > 
> ? > Since in practice pi(i,j) does not depend on j, pi(i,j) = pi(i,j').
> ? > 
> ? > Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
> ? >  integral(R) Pr( Y(i,j) = 1 inter Y(i,j') = 1 | Z(i) = z ) p( Z(i) = z ) dz
> ? > 
> ? > Then, we assume that conditionnally on Zi, the Yij are independant, is
> ? > this right? This is the equivalent of ? the epsilon(i, j) are
> ? > independant ?? I assume this hypothesis is also used for computing the
> ? > likelihood? If not, what is the model for the joint probability?
> ? > 
> ? > In that case,
> ? > 
> ? > Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
> ? >  integral(R) f(z) f(z) p( Z(i) = z ) dz
> ? > 
> ? > and since pi(i,j) = integral( R ) f(z) p( Z(i) = z ) dz we have
> ? > 
> ? > cov( Y(i,j), Y(i,j') ) =
> ? > integral( R ) f?(z) p( Z(i) = z ) dz -
> ? >  ( integral( R ) f(z) p( Z(i) = z ) dz )?
> ? > 
> ? > which in general has no reason to be nul, hence the two observations
> ? > are correlated. Is this correct?
> ? > 
> ? > Is there any way to have a closed-form of the covariance, for usual f
> ? > (let's say, logit or probit) and Z distribution (let's say, Gaussian)?
> ? > 
> ? > Thanks a lot for reading, and your answers,
> ? > 
> ? > -- 
> ? >                                Emmanuel CURIS
> ? >                                emmanuel.curis at parisdescartes.fr
> ? > 
> ? > Page WWW: http://emmanuel.curis.online.fr/index.html
> ? > 
> ? > _______________________________________________
> ? > R-sig-mixed-models at r-project.org mailing list
> ? > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> ? 
> 
> -- 
>                                Emmanuel CURIS
>                                emmanuel.curis at parisdescartes.fr
> 
> Page WWW: http://emmanuel.curis.online.fr/index.html


From n|@||@m|||@r @end|ng |rom w@u@edu  Tue Apr 21 00:11:52 2020
From: n|@||@m|||@r @end|ng |rom w@u@edu (Millar, Niall)
Date: Mon, 20 Apr 2020 22:11:52 +0000
Subject: [R-sig-ME] 
 Emmeans & effects packages: Post-hoc tests for Tweedie glmmTMB model
In-Reply-To: <ACD1644AA6C67E4FBD0C350625508EC88CE25F05@FHSDB2D11-2.csu.mcmaster.ca>
References: <4153_1587156743_03HKqM51027994_MWHPR0101MB3056B76FB917FBAB04EAA9C98ED90@MWHPR0101MB3056.prod.exchangelabs.com>,
 <ACD1644AA6C67E4FBD0C350625508EC88CE25F05@FHSDB2D11-2.csu.mcmaster.ca>
Message-ID: <MWHPR0101MB3056EF53F7895DFF1F0F3FC08ED40@MWHPR0101MB3056.prod.exchangelabs.com>

Dear John,

Thanks very much for reply. I'd just like to clarify a few points and make sure I understand. Do you mean that the effects package calculates these asymptotic confidence intervals by assuming that the distribution of the parameters (not the data) is normal? If so, can I then generate hypothesis tests with these confidence intervals by assuming some uncertainty in the parameter values?

Thanks,
-Niall
________________________________
From: Fox, John <jfox at mcmaster.ca>
Sent: 17 April 2020 14:03
To: Millar, Niall <niall.millar at wsu.edu>; r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: RE: Emmeans & effects packages: Post-hoc tests for Tweedie glmmTMB model

Dear Niall,

I've not seen applications of GLMMs using the Tweedie Family, but don't see why that should matter. The "effects" computed by the functions in the effects package are linear functions of the estimated fixed-effects parameters, and their asymptotic standard errors are computed from the estimated covariance matrix of the fixed effects. The default (asymptotic) confidence intervals are computed pointwise and so don't adjust for simultaneous inference, but there is an option to compute Scheffe intervals.

By the way, the fixed-effects part of your model is more compactly Shoot.weight ~ N.Level*Rhiz*Species.

I hope this helps,
 John

--------------------------------------
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
Web: socialsciences.mcmaster.ca/jfox/



> -----Original Message-----
> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On
> Behalf Of Millar, Niall
> Sent: Friday, April 17, 2020 4:52 PM
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] Emmeans & effects packages: Post-hoc tests for Tweedie
> glmmTMB model
>
> Are there any problems with using the effects and emmeans packages to
> interpret glmmTMB models with Tweedie distributions?
>
> I have glmmTMB models using the Tweedie distribution, and I want to draw
> inference about the direction and magnitude of the predicted effects. In
> the model given below:
>
> glmmTMB(Shoot.weight ~ Species + N.Level + Rhiz + Species:N.Level +
> Species:Rhiz + N.Level:Rhiz + N.Level:Rhiz:Species + (1|Block) + (1|Acc),
> family=tweedie, data=DcleanSOY)
>
> I have a significant effect of the N.Level:Rhiz:Species interaction term
> (based on a likelihood ratio test). N level is a continuous predictor, and
> Rhiz and Species are both two-level factors.
>
> I have used the effects package to make a plot with 95% confidence
> intervals to interpret this 3-way interaction. I have also made separate
> models for each level of the N level predictor, and used emmeans to get
> pairwise Tukey contrasts of the Rhiz and Species predictors at each N
> level.
>
> Can I base my inference on these 95% confidence intervals and/or emmeans
> contrasts, or are these methods not supported for Tweedie glmmTMB models?
>
> Thanks! -Niall
>
>
> Niall Millar
> PhD Candidate
> Porter Lab
> School of Biological Sciences
>
> Washington State University Vancouver
> 14204 NE Salmon Creek Ave
> Vancouver, WA  98686
> 360-784-9816
> niall.millar at wsu.edu
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models__;!!JmPEgBY0HMszNaDT!50zxh0DcViMBmNRc48-t8A6IKL-e__5vWeXU9OhZ-zCCA_a--_hDvHgMgtAxcUjXug$

	[[alternative HTML version deleted]]


From j|ox @end|ng |rom mcm@@ter@c@  Tue Apr 21 05:46:25 2020
From: j|ox @end|ng |rom mcm@@ter@c@ (Fox, John)
Date: Tue, 21 Apr 2020 03:46:25 +0000
Subject: [R-sig-ME] 
 Emmeans & effects packages: Post-hoc tests for Tweedie glmmTMB model
In-Reply-To: <MWHPR0101MB3056EF53F7895DFF1F0F3FC08ED40@MWHPR0101MB3056.prod.exchangelabs.com>
References: <4153_1587156743_03HKqM51027994_MWHPR0101MB3056B76FB917FBAB04EAA9C98ED90@MWHPR0101MB3056.prod.exchangelabs.com>
 <ACD1644AA6C67E4FBD0C350625508EC88CE25F05@FHSDB2D11-2.csu.mcmaster.ca>
 <MWHPR0101MB3056EF53F7895DFF1F0F3FC08ED40@MWHPR0101MB3056.prod.exchangelabs.com>
Message-ID: <49260A60-1F79-453E-B879-FA6D6DD9BC17@mcmaster.ca>

Dear Niall,

> On Apr 20, 2020, at 6:11 PM, Millar, Niall <niall.millar at wsu.edu> wrote:
> 
> Dear John,
> 
> Thanks very much for reply. I'd just like to clarify a few points and make sure I understand. Do you mean that the effects package calculates these asymptotic confidence intervals by assuming that the distribution of the parameters (not the data) is normal? If so, can I then generate hypothesis tests with these confidence intervals by assuming some uncertainty in the parameter values?

Yes, the estimated fixed-effects coefficients and the estimated effects, which are linear combinations of the estimated coefficients, are asymptotically normal, and the confidence intervals reported are either based on the normal or t-distribution, depending on the model. 

But it's not obvious to me that the point-wise confidence intervals for the effects correspond to hypotheses that you want to test. The object returned by Effect(), however, contains not only the estimated effects, their standard errors, and confidence limits, but also their full covariance matrix. Consequently, you should be able to formulate a Wald test for *any* linear combination of the effects of interest, or even, by the delta method, a test for a nonlinear function of the fixed effects.

Best,
 John

> 
> Thanks,
> -Niall
> From: Fox, John <jfox at mcmaster.ca>
> Sent: 17 April 2020 14:03
> To: Millar, Niall <niall.millar at wsu.edu>; r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
> Subject: RE: Emmeans & effects packages: Post-hoc tests for Tweedie glmmTMB model
>  
> Dear Niall,
> 
> I've not seen applications of GLMMs using the Tweedie Family, but don't see why that should matter. The "effects" computed by the functions in the effects package are linear functions of the estimated fixed-effects parameters, and their asymptotic standard errors are computed from the estimated covariance matrix of the fixed effects. The default (asymptotic) confidence intervals are computed pointwise and so don't adjust for simultaneous inference, but there is an option to compute Scheffe intervals.
> 
> By the way, the fixed-effects part of your model is more compactly Shoot.weight ~ N.Level*Rhiz*Species.
> 
> I hope this helps,
>  John
> 
> --------------------------------------
> John Fox, Professor Emeritus
> McMaster University
> Hamilton, Ontario, Canada
> Web: socialsciences.mcmaster.ca/jfox/
> 
> 
> 
> > -----Original Message-----
> > From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On
> > Behalf Of Millar, Niall
> > Sent: Friday, April 17, 2020 4:52 PM
> > To: r-sig-mixed-models at r-project.org
> > Subject: [R-sig-ME] Emmeans & effects packages: Post-hoc tests for Tweedie
> > glmmTMB model
> > 
> > Are there any problems with using the effects and emmeans packages to
> > interpret glmmTMB models with Tweedie distributions?
> > 
> > I have glmmTMB models using the Tweedie distribution, and I want to draw
> > inference about the direction and magnitude of the predicted effects. In
> > the model given below:
> > 
> > glmmTMB(Shoot.weight ~ Species + N.Level + Rhiz + Species:N.Level +
> > Species:Rhiz + N.Level:Rhiz + N.Level:Rhiz:Species + (1|Block) + (1|Acc),
> > family=tweedie, data=DcleanSOY)
> > 
> > I have a significant effect of the N.Level:Rhiz:Species interaction term
> > (based on a likelihood ratio test). N level is a continuous predictor, and
> > Rhiz and Species are both two-level factors.
> > 
> > I have used the effects package to make a plot with 95% confidence
> > intervals to interpret this 3-way interaction. I have also made separate
> > models for each level of the N level predictor, and used emmeans to get
> > pairwise Tukey contrasts of the Rhiz and Species predictors at each N
> > level.
> > 
> > Can I base my inference on these 95% confidence intervals and/or emmeans
> > contrasts, or are these methods not supported for Tweedie glmmTMB models?
> > 
> > Thanks! -Niall
> > 
> > 
> > Niall Millar
> > PhD Candidate
> > Porter Lab
> > School of Biological Sciences
> > 
> > Washington State University Vancouver
> > 14204 NE Salmon Creek Ave
> > Vancouver, WA  98686
> > 360-784-9816
> > niall.millar at wsu.edu
> > 
> >        [[alternative HTML version deleted]]
> > 
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models__;!!JmPEgBY0HMszNaDT!50zxh0DcViMBmNRc48-t8A6IKL-e__5vWeXU9OhZ-zCCA_a--_hDvHgMgtAxcUjXug$


From D@v|d@Du||y @end|ng |rom q|mrbergho|er@edu@@u  Tue Apr 21 09:18:41 2020
From: D@v|d@Du||y @end|ng |rom q|mrbergho|er@edu@@u (David Duffy)
Date: Tue, 21 Apr 2020 07:18:41 +0000
Subject: [R-sig-ME] 
 Precision about the glmer model for Bernoulli variables
In-Reply-To: <2AD9CAD5-FABC-4A13-B684-6D0908CFBF21@health.ucsd.edu>
References: <20200420074851.GA6361@info124.pharmacie.univ-paris5.fr>
 <AB72DE88-C199-459C-B0EB-0DF13BB1659B@health.ucsd.edu>
 <20200420193249.GB16474@info124.pharmacie.univ-paris5.fr>,
 <2AD9CAD5-FABC-4A13-B684-6D0908CFBF21@health.ucsd.edu>
Message-ID: <5f78610081524e23855ff4bcccdf45a8@qimrberghofer.edu.au>

You'd know the beta-binomial constrains the range of the correlation coefficient positive, but the logit-normal and probit-normal allow a negative correlation, with a lower bound constrained by the N, in the case of regular sized clusters. For your example of 2x2 tables, the correlation can go from -1 to 1. This is the usual genetics
type setup where you model the pairwise correlations for all pairs of observations in the dataset versus their
(pairwise) genetic relatedness.  In multi-trait models, the correlations between different phenotypes (say asthma and diabetes) in the same individual can be negative, so you can estimate negative variance components.

The pedigreemm package extends lme4 to allow glmms for pedigree data (see the mastitis example for a binary trait).

Cheers, David Duffy.
________________________________________
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on behalf of Vaida, Florin <fvaida at health.ucsd.edu>
Sent: Tuesday, 21 April 2020 7:05:40 AM
To: Emmanuel Curis
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME]  Precision about the glmer model for Bernoulli variables

Hi Emmanuel,

That's a good question.  My guess is that the correlation is non-negative generally, but I wasn't able to prove it theoretically even in the simplest case when Y1, Y2 ~ Bernoulli(u) independent conditionally on u, and u ~ Normal(0, 1).  I am curious if someone has a solution.
We can't go too far down this route in this forum, since Doug wants to keep it applied.

Florin

> On Apr 20, 2020, at 12:32 PM, Emmanuel Curis <emmanuel.curis at parisdescartes.fr> wrote:
>
> Hi Florin,
>
> Thanks for the answer, the precision about p(i,j), and the reference.
>
> A last question, that I forgot in my message: is the obtained
> correlation also always positive, as in the linear case? Or may some
> negative correlation appear, depending on the values of pi(i,j) and
> pi(i,j')?
>
> Best regards,
>
> On Mon, Apr 20, 2020 at 03:27:39PM +0000, Vaida, Florin wrote:
> ? Hi Emmanuel,
> ?
> ? Your reasoning is correct.
> ?
> ? As a quibble, outside a simple repeated measures experiment setup, p(i,j) *does* depend on j.
> ? For example, if observations are collected over time, generally there is a time effect; if they are repeated measures with different experimental conditions, p(i,j) will depend on the condition j, etc.
> ?
> ? There is almost certainly no closed form solution for the covariance under logit.
> ? I am not sure about the probit (my guess is not).
> ? There will be some Laplace approximations available, a la Breslow and Clayton 1993.
> ?
> ? I'd be curious if these formulas/approximations were developed somewhere - I'd be surprised if they weren't.
> ?
> ? Florin
> ?
> ?
> ? > On Apr 20, 2020, at 12:48 AM, Emmanuel Curis <emmanuel.curis at parisdescartes.fr> wrote:
> ? >
> ? > Hello everyone,
> ? >
> ? > I hope you're all going fine in these difficult times.
> ? >
> ? > I tried to understand in details the exact model used when using glmer
> ? > for a Bernoulli experiment, by comparison with the linear mixed
> ? > effects model, and especially how it introducts correlations between
> ? > observations of a given group.  I think I finally got it, but could
> ? > you check that what I write below is correct and that I'm not missing
> ? > something?
> ? >
> ? > I use a very simple case with only a single random effect, and no
> ? > fixed effects, because I guess that adding fixed effects or other
> ? > random effects does not change the idea, it "just" makes formulas more
> ? > complex.  I note i the random effect level, let's say ? patient ?, and
> ? > j the observation for this patient.
> ? >
> ? > In the linear model, we have Y(i,j) = ?0 + Z(i) + epsilon( i, j ) with
> ? > Z(i) and epsilon(i,j) randoms variables having a density of
> ? > probability, independant, and each iid.
> ? >
> ? > Hence, cov( Y(i,j), Y(i,j') ) = Var( Z(i) ): the model introduces a
> ? > positive correlation between observations of the same patient.
> ? >
> ? >
> ? >
> ? > In the Bernoulli model, Y(i,j) ~ B( pi(i,j) ) and pi(i,j) = f( Z(i) ),
> ? > f being the inverse link function, typically the reciprocal of the
> ? > logit. So we have
> ? >
> ? > cov( Y(i,j), Y(i,j') ) = E( Y(i,j) Y(i, j') ) - E( Y(i,j) ) E( Y(i,j') )
> ? >     = Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) - pi(i,j) * pi(i,j')
> ? >
> ? > Since in practice pi(i,j) does not depend on j, pi(i,j) = pi(i,j').
> ? >
> ? > Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
> ? >  integral(R) Pr( Y(i,j) = 1 inter Y(i,j') = 1 | Z(i) = z ) p( Z(i) = z ) dz
> ? >
> ? > Then, we assume that conditionnally on Zi, the Yij are independant, is
> ? > this right? This is the equivalent of ? the epsilon(i, j) are
> ? > independant ?? I assume this hypothesis is also used for computing the
> ? > likelihood? If not, what is the model for the joint probability?
> ? >
> ? > In that case,
> ? >
> ? > Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
> ? >  integral(R) f(z) f(z) p( Z(i) = z ) dz
> ? >
> ? > and since pi(i,j) = integral( R ) f(z) p( Z(i) = z ) dz we have
> ? >
> ? > cov( Y(i,j), Y(i,j') ) =
> ? > integral( R ) f?(z) p( Z(i) = z ) dz -
> ? >  ( integral( R ) f(z) p( Z(i) = z ) dz )?
> ? >
> ? > which in general has no reason to be nul, hence the two observations
> ? > are correlated. Is this correct?
> ? >
> ? > Is there any way to have a closed-form of the covariance, for usual f
> ? > (let's say, logit or probit) and Z distribution (let's say, Gaussian)?
> ? >
> ? > Thanks a lot for reading, and your answers,
> ? >
> ? > --
> ? >                                Emmanuel CURIS
> ? >                                emmanuel.curis at parisdescartes.fr
> ? >
> ? > Page WWW: http://emmanuel.curis.online.fr/index.html
> ? >
> ? > _______________________________________________
> ? > R-sig-mixed-models at r-project.org mailing list
> ? > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> ?
>
> --
>                                Emmanuel CURIS
>                                emmanuel.curis at parisdescartes.fr
>
> Page WWW: http://emmanuel.curis.online.fr/index.html

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
[EXTERNAL EMAIL] This message originates from an external email address, please exercise caution when clicking any links or opening attachments. If you believe the sender is impersonating someone at QIMR Berghofer, please forward this message to phishing at qimrberghofer.edu.au.


From john@m@|ndon@|d @end|ng |rom @nu@edu@@u  Tue Apr 21 09:59:48 2020
From: john@m@|ndon@|d @end|ng |rom @nu@edu@@u (John Maindonald)
Date: Tue, 21 Apr 2020 07:59:48 +0000
Subject: [R-sig-ME] 
 Precision about the glmer model for Bernoulli variables
In-Reply-To: <5f78610081524e23855ff4bcccdf45a8@qimrberghofer.edu.au>
References: <20200420074851.GA6361@info124.pharmacie.univ-paris5.fr>
 <AB72DE88-C199-459C-B0EB-0DF13BB1659B@health.ucsd.edu>
 <20200420193249.GB16474@info124.pharmacie.univ-paris5.fr>
 <2AD9CAD5-FABC-4A13-B684-6D0908CFBF21@health.ucsd.edu>
 <5f78610081524e23855ff4bcccdf45a8@qimrberghofer.edu.au>
Message-ID: <55E960BE-5BC8-4BB7-B30C-0AF922C7D73C@anu.edu.au>

Actually, with a suitable parameterization, the betabinomial allows
small negative correlations.  See
  Binary Regression Using an Extended Beta-Binomial Distribution, With Discussion of
 Correlation Induced by Covariate Measurement Errors
R. L. Prentice: Journal of the American Statistical Association
Vol. 81, No. 394 (Jun., 1986), pp. 321-327

A feature of the betabinomial (BB) , which ought to be better advertised than
it is in most accounts that I have seen, is that, for positive correlation rho,
it sets a strict lower limit of pi(1-pi)(1+rho) on the variance of the estimate
of the proportion pi.  This is in marked contrast to the variance
assumptions that define quasibinomial errors.

The gamlss package implements the double binomial (as well
as the logistic normal and BB), but as far as I can tell, not in a
multi-level model context.  The double binomial does allow a
negative correlation.


John Maindonald             email: john.maindonald at anu.edu.au<mailto:john.maindonald at anu.edu.au>


On 21/04/2020, at 19:18, David Duffy <David.Duffy at qimrberghofer.edu.au<mailto:David.Duffy at qimrberghofer.edu.au>> wrote:

You'd know the beta-binomial constrains the range of the correlation coefficient positive, but the logit-normal and probit-normal allow a negative correlation, with a lower bound constrained by the N, in the case of regular sized clusters. For your example of 2x2 tables, the correlation can go from -1 to 1. This is the usual genetics
type setup where you model the pairwise correlations for all pairs of observations in the dataset versus their
(pairwise) genetic relatedness.  In multi-trait models, the correlations between different phenotypes (say asthma and diabetes) in the same individual can be negative, so you can estimate negative variance components.

The pedigreemm package extends lme4 to allow glmms for pedigree data (see the mastitis example for a binary trait).

Cheers, David Duffy.
________________________________________
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org<mailto:r-sig-mixed-models-bounces at r-project.org>> on behalf of Vaida, Florin <fvaida at health.ucsd.edu<mailto:fvaida at health.ucsd.edu>>
Sent: Tuesday, 21 April 2020 7:05:40 AM
To: Emmanuel Curis
Cc: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME]  Precision about the glmer model for Bernoulli variables

Hi Emmanuel,

That's a good question.  My guess is that the correlation is non-negative generally, but I wasn't able to prove it theoretically even in the simplest case when Y1, Y2 ~ Bernoulli(u) independent conditionally on u, and u ~ Normal(0, 1).  I am curious if someone has a solution.
We can't go too far down this route in this forum, since Doug wants to keep it applied.

Florin

On Apr 20, 2020, at 12:32 PM, Emmanuel Curis <emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr>> wrote:

Hi Florin,

Thanks for the answer, the precision about p(i,j), and the reference.

A last question, that I forgot in my message: is the obtained
correlation also always positive, as in the linear case? Or may some
negative correlation appear, depending on the values of pi(i,j) and
pi(i,j')?

Best regards,

On Mon, Apr 20, 2020 at 03:27:39PM +0000, Vaida, Florin wrote:
? Hi Emmanuel,
?
? Your reasoning is correct.
?
? As a quibble, outside a simple repeated measures experiment setup, p(i,j) *does* depend on j.
? For example, if observations are collected over time, generally there is a time effect; if they are repeated measures with different experimental conditions, p(i,j) will depend on the condition j, etc.
?
? There is almost certainly no closed form solution for the covariance under logit.
? I am not sure about the probit (my guess is not).
? There will be some Laplace approximations available, a la Breslow and Clayton 1993.
?
? I'd be curious if these formulas/approximations were developed somewhere - I'd be surprised if they weren't.
?
? Florin
?
?
? > On Apr 20, 2020, at 12:48 AM, Emmanuel Curis <emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr>> wrote:
? >
? > Hello everyone,
? >
? > I hope you're all going fine in these difficult times.
? >
? > I tried to understand in details the exact model used when using glmer
? > for a Bernoulli experiment, by comparison with the linear mixed
? > effects model, and especially how it introducts correlations between
? > observations of a given group.  I think I finally got it, but could
? > you check that what I write below is correct and that I'm not missing
? > something?
? >
? > I use a very simple case with only a single random effect, and no
? > fixed effects, because I guess that adding fixed effects or other
? > random effects does not change the idea, it "just" makes formulas more
? > complex.  I note i the random effect level, let's say ? patient ?, and
? > j the observation for this patient.
? >
? > In the linear model, we have Y(i,j) = ?0 + Z(i) + epsilon( i, j ) with
? > Z(i) and epsilon(i,j) randoms variables having a density of
? > probability, independant, and each iid.
? >
? > Hence, cov( Y(i,j), Y(i,j') ) = Var( Z(i) ): the model introduces a
? > positive correlation between observations of the same patient.
? >
? >
? >
? > In the Bernoulli model, Y(i,j) ~ B( pi(i,j) ) and pi(i,j) = f( Z(i) ),
? > f being the inverse link function, typically the reciprocal of the
? > logit. So we have
? >
? > cov( Y(i,j), Y(i,j') ) = E( Y(i,j) Y(i, j') ) - E( Y(i,j) ) E( Y(i,j') )
? >     = Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) - pi(i,j) * pi(i,j')
? >
? > Since in practice pi(i,j) does not depend on j, pi(i,j) = pi(i,j').
? >
? > Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
? >  integral(R) Pr( Y(i,j) = 1 inter Y(i,j') = 1 | Z(i) = z ) p( Z(i) = z ) dz
? >
? > Then, we assume that conditionnally on Zi, the Yij are independant, is
? > this right? This is the equivalent of ? the epsilon(i, j) are
? > independant ?? I assume this hypothesis is also used for computing the
? > likelihood? If not, what is the model for the joint probability?
? >
? > In that case,
? >
? > Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
? >  integral(R) f(z) f(z) p( Z(i) = z ) dz
? >
? > and since pi(i,j) = integral( R ) f(z) p( Z(i) = z ) dz we have
? >
? > cov( Y(i,j), Y(i,j') ) =
? > integral( R ) f?(z) p( Z(i) = z ) dz -
? >  ( integral( R ) f(z) p( Z(i) = z ) dz )?
? >
? > which in general has no reason to be nul, hence the two observations
? > are correlated. Is this correct?
? >
? > Is there any way to have a closed-form of the covariance, for usual f
? > (let's say, logit or probit) and Z distribution (let's say, Gaussian)?
? >
? > Thanks a lot for reading, and your answers,
? >
? > --
? >                                Emmanuel CURIS
? >                                emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr>
? >
? > Page WWW: http://emmanuel.curis.online.fr/index.html
? >
? > _______________________________________________
? > R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
? > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
?

--
                              Emmanuel CURIS
                              emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr>

Page WWW: http://emmanuel.curis.online.fr/index.html

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
[EXTERNAL EMAIL] This message originates from an external email address, please exercise caution when clicking any links or opening attachments. If you believe the sender is impersonating someone at QIMR Berghofer, please forward this message to phishing at qimrberghofer.edu.au.

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


	[[alternative HTML version deleted]]


From john@m@|ndon@|d @end|ng |rom @nu@edu@@u  Tue Apr 21 10:15:40 2020
From: john@m@|ndon@|d @end|ng |rom @nu@edu@@u (John Maindonald)
Date: Tue, 21 Apr 2020 08:15:40 +0000
Subject: [R-sig-ME] 
 Precision about the glmer model for Bernoulli variables
In-Reply-To: <5f78610081524e23855ff4bcccdf45a8@qimrberghofer.edu.au>
References: <20200420074851.GA6361@info124.pharmacie.univ-paris5.fr>
 <AB72DE88-C199-459C-B0EB-0DF13BB1659B@health.ucsd.edu>
 <20200420193249.GB16474@info124.pharmacie.univ-paris5.fr>
 <2AD9CAD5-FABC-4A13-B684-6D0908CFBF21@health.ucsd.edu>
 <5f78610081524e23855ff4bcccdf45a8@qimrberghofer.edu.au>
Message-ID: <DDB70217-5D47-4DF2-8FF8-0748AD8D7754@anu.edu.au>

Apologies, the lower limit for the BB variance is pi(1-pi)rho.

Actually, with a suitable parameterization, the betabinomial allows
small negative correlations.  See
  Binary Regression Using an Extended Beta-Binomial Distribution, With Discussion of
 Correlation Induced by Covariate Measurement Errors
R. L. Prentice: Journal of the American Statistical Association
Vol. 81, No. 394 (Jun., 1986), pp. 321-327

A feature of the betabinomial (BB) , which ought to be better advertised than
it is in most accounts that I have seen, is that, for positive correlation rho,
it sets a strict lower limit of pi(1-pi)rho on the variance of the estimate
of the proportion pi.  This is in marked contrast to the variance
assumptions that define quasibinomial errors.

The gamlss package implements the double binomial (as well
as the logistic normal and BB), but as far as I can tell, not in a
multi-level model context.  The double binomial does allow a
negative correlation.

John Maindonald             email: john.maindonald at anu.edu.au<mailto:john.maindonald at anu.edu.au>


On 21/04/2020, at 19:18, David Duffy <David.Duffy at qimrberghofer.edu.au<mailto:David.Duffy at qimrberghofer.edu.au>> wrote:

You'd know the beta-binomial constrains the range of the correlation coefficient positive, but the logit-normal and probit-normal allow a negative correlation, with a lower bound constrained by the N, in the case of regular sized clusters. For your example of 2x2 tables, the correlation can go from -1 to 1. This is the usual genetics
type setup where you model the pairwise correlations for all pairs of observations in the dataset versus their
(pairwise) genetic relatedness.  In multi-trait models, the correlations between different phenotypes (say asthma and diabetes) in the same individual can be negative, so you can estimate negative variance components.

The pedigreemm package extends lme4 to allow glmms for pedigree data (see the mastitis example for a binary trait).

Cheers, David Duffy.
________________________________________
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org<mailto:r-sig-mixed-models-bounces at r-project.org>> on behalf of Vaida, Florin <fvaida at health.ucsd.edu<mailto:fvaida at health.ucsd.edu>>
Sent: Tuesday, 21 April 2020 7:05:40 AM
To: Emmanuel Curis
Cc: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME]  Precision about the glmer model for Bernoulli variables

Hi Emmanuel,

That's a good question.  My guess is that the correlation is non-negative generally, but I wasn't able to prove it theoretically even in the simplest case when Y1, Y2 ~ Bernoulli(u) independent conditionally on u, and u ~ Normal(0, 1).  I am curious if someone has a solution.
We can't go too far down this route in this forum, since Doug wants to keep it applied.

Florin

On Apr 20, 2020, at 12:32 PM, Emmanuel Curis <emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr>> wrote:

Hi Florin,

Thanks for the answer, the precision about p(i,j), and the reference.

A last question, that I forgot in my message: is the obtained
correlation also always positive, as in the linear case? Or may some
negative correlation appear, depending on the values of pi(i,j) and
pi(i,j')?

Best regards,

On Mon, Apr 20, 2020 at 03:27:39PM +0000, Vaida, Florin wrote:
? Hi Emmanuel,
?
? Your reasoning is correct.
?
? As a quibble, outside a simple repeated measures experiment setup, p(i,j) *does* depend on j.
? For example, if observations are collected over time, generally there is a time effect; if they are repeated measures with different experimental conditions, p(i,j) will depend on the condition j, etc.
?
? There is almost certainly no closed form solution for the covariance under logit.
? I am not sure about the probit (my guess is not).
? There will be some Laplace approximations available, a la Breslow and Clayton 1993.
?
? I'd be curious if these formulas/approximations were developed somewhere - I'd be surprised if they weren't.
?
? Florin
?
?
? > On Apr 20, 2020, at 12:48 AM, Emmanuel Curis <emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr>> wrote:
? >
? > Hello everyone,
? >
? > I hope you're all going fine in these difficult times.
? >
? > I tried to understand in details the exact model used when using glmer
? > for a Bernoulli experiment, by comparison with the linear mixed
? > effects model, and especially how it introducts correlations between
? > observations of a given group.  I think I finally got it, but could
? > you check that what I write below is correct and that I'm not missing
? > something?
? >
? > I use a very simple case with only a single random effect, and no
? > fixed effects, because I guess that adding fixed effects or other
? > random effects does not change the idea, it "just" makes formulas more
? > complex.  I note i the random effect level, let's say ? patient ?, and
? > j the observation for this patient.
? >
? > In the linear model, we have Y(i,j) = ?0 + Z(i) + epsilon( i, j ) with
? > Z(i) and epsilon(i,j) randoms variables having a density of
? > probability, independant, and each iid.
? >
? > Hence, cov( Y(i,j), Y(i,j') ) = Var( Z(i) ): the model introduces a
? > positive correlation between observations of the same patient.
? >
? >
? >
? > In the Bernoulli model, Y(i,j) ~ B( pi(i,j) ) and pi(i,j) = f( Z(i) ),
? > f being the inverse link function, typically the reciprocal of the
? > logit. So we have
? >
? > cov( Y(i,j), Y(i,j') ) = E( Y(i,j) Y(i, j') ) - E( Y(i,j) ) E( Y(i,j') )
? >     = Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) - pi(i,j) * pi(i,j')
? >
? > Since in practice pi(i,j) does not depend on j, pi(i,j) = pi(i,j').
? >
? > Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
? >  integral(R) Pr( Y(i,j) = 1 inter Y(i,j') = 1 | Z(i) = z ) p( Z(i) = z ) dz
? >
? > Then, we assume that conditionnally on Zi, the Yij are independant, is
? > this right? This is the equivalent of ? the epsilon(i, j) are
? > independant ?? I assume this hypothesis is also used for computing the
? > likelihood? If not, what is the model for the joint probability?
? >
? > In that case,
? >
? > Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
? >  integral(R) f(z) f(z) p( Z(i) = z ) dz
? >
? > and since pi(i,j) = integral( R ) f(z) p( Z(i) = z ) dz we have
? >
? > cov( Y(i,j), Y(i,j') ) =
? > integral( R ) f?(z) p( Z(i) = z ) dz -
? >  ( integral( R ) f(z) p( Z(i) = z ) dz )?
? >
? > which in general has no reason to be nul, hence the two observations
? > are correlated. Is this correct?
? >
? > Is there any way to have a closed-form of the covariance, for usual f
? > (let's say, logit or probit) and Z distribution (let's say, Gaussian)?
? >
? > Thanks a lot for reading, and your answers,
? >
? > --
? >                                Emmanuel CURIS
? >                                emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr>
? >
? > Page WWW: http://emmanuel.curis.online.fr/index.html
? >
? > _______________________________________________
? > R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
? > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
?

--
                              Emmanuel CURIS
                              emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr>

Page WWW: http://emmanuel.curis.online.fr/index.html

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
[EXTERNAL EMAIL] This message originates from an external email address, please exercise caution when clicking any links or opening attachments. If you believe the sender is impersonating someone at QIMR Berghofer, please forward this message to phishing at qimrberghofer.edu.au<mailto:phishing at qimrberghofer.edu.au>.

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


	[[alternative HTML version deleted]]


From emm@nue|@cur|@ @end|ng |rom p@r|@de@c@rte@@|r  Tue Apr 21 11:19:21 2020
From: emm@nue|@cur|@ @end|ng |rom p@r|@de@c@rte@@|r (Emmanuel Curis)
Date: Tue, 21 Apr 2020 11:19:21 +0200
Subject: [R-sig-ME] 
 Precision about the glmer model for Bernoulli variables
In-Reply-To: <55E960BE-5BC8-4BB7-B30C-0AF922C7D73C@anu.edu.au>
References: <20200420074851.GA6361@info124.pharmacie.univ-paris5.fr>
 <AB72DE88-C199-459C-B0EB-0DF13BB1659B@health.ucsd.edu>
 <20200420193249.GB16474@info124.pharmacie.univ-paris5.fr>
 <2AD9CAD5-FABC-4A13-B684-6D0908CFBF21@health.ucsd.edu>
 <5f78610081524e23855ff4bcccdf45a8@qimrberghofer.edu.au>
 <55E960BE-5BC8-4BB7-B30C-0AF922C7D73C@anu.edu.au>
Message-ID: <20200421091921.GC9918@info124.pharmacie.univ-paris5.fr>

Thanks for all this details about correlation.

Just to be clear about the nomenclature: when a model is called
??A-B??, A is for the cumulative distribution function used as link
between pi and the linear predictor including the random effect(s) or
its reciprocal, and B is for the distribution of random effect(s), is
this right?

If this is right, for the ? double binomial ?, it would mean that the
link function is a step function, and not a monotonous strictly
increasing function. Since it would then not be a bijection between R
and [0,1], doesn't it introduce indetermination and strong constraints
on the possible values of pi? I guess I'm wrongly interpreting what
means ? double binomial ?.

Sorry for this naive questions, I'm discovering the field...

Best regards,


On Tue, Apr 21, 2020 at 07:59:48AM +0000, John Maindonald wrote:
> Actually, with a suitable parameterization, the betabinomial allows
> small negative correlations.  See
>   Binary Regression Using an Extended Beta-Binomial Distribution, With Discussion of
>  Correlation Induced by Covariate Measurement Errors
> R. L. Prentice: Journal of the American Statistical Association
> Vol. 81, No. 394 (Jun., 1986), pp. 321-327
> 
> A feature of the betabinomial (BB) , which ought to be better advertised than
> it is in most accounts that I have seen, is that, for positive correlation rho,
> it sets a strict lower limit of pi(1-pi)(1+rho) on the variance of the estimate
> of the proportion pi.  This is in marked contrast to the variance
> assumptions that define quasibinomial errors.
> 
> The gamlss package implements the double binomial (as well
> as the logistic normal and BB), but as far as I can tell, not in a
> multi-level model context.  The double binomial does allow a
> negative correlation.
> 
> 
> John Maindonald             email: john.maindonald at anu.edu.au<mailto:john.maindonald at anu.edu.au>
> 
> 
> On 21/04/2020, at 19:18, David Duffy <David.Duffy at qimrberghofer.edu.au<mailto:David.Duffy at qimrberghofer.edu.au>> wrote:
> 
> You'd know the beta-binomial constrains the range of the correlation coefficient positive, but the logit-normal and probit-normal allow a negative correlation, with a lower bound constrained by the N, in the case of regular sized clusters. For your example of 2x2 tables, the correlation can go from -1 to 1. This is the usual genetics
> type setup where you model the pairwise correlations for all pairs of observations in the dataset versus their
> (pairwise) genetic relatedness.  In multi-trait models, the correlations between different phenotypes (say asthma and diabetes) in the same individual can be negative, so you can estimate negative variance components.
> 
> The pedigreemm package extends lme4 to allow glmms for pedigree data (see the mastitis example for a binary trait).
> 
> Cheers, David Duffy.
> ________________________________________
> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org<mailto:r-sig-mixed-models-bounces at r-project.org>> on behalf of Vaida, Florin <fvaida at health.ucsd.edu<mailto:fvaida at health.ucsd.edu>>
> Sent: Tuesday, 21 April 2020 7:05:40 AM
> To: Emmanuel Curis
> Cc: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>
> Subject: Re: [R-sig-ME]  Precision about the glmer model for Bernoulli variables
> 
> Hi Emmanuel,
> 
> That's a good question.  My guess is that the correlation is non-negative generally, but I wasn't able to prove it theoretically even in the simplest case when Y1, Y2 ~ Bernoulli(u) independent conditionally on u, and u ~ Normal(0, 1).  I am curious if someone has a solution.
> We can't go too far down this route in this forum, since Doug wants to keep it applied.
> 
> Florin
> 
> On Apr 20, 2020, at 12:32 PM, Emmanuel Curis <emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr>> wrote:
> 
> Hi Florin,
> 
> Thanks for the answer, the precision about p(i,j), and the reference.
> 
> A last question, that I forgot in my message: is the obtained
> correlation also always positive, as in the linear case? Or may some
> negative correlation appear, depending on the values of pi(i,j) and
> pi(i,j')?
> 
> Best regards,
> 
> On Mon, Apr 20, 2020 at 03:27:39PM +0000, Vaida, Florin wrote:
> ? Hi Emmanuel,
> ?
> ? Your reasoning is correct.
> ?
> ? As a quibble, outside a simple repeated measures experiment setup, p(i,j) *does* depend on j.
> ? For example, if observations are collected over time, generally there is a time effect; if they are repeated measures with different experimental conditions, p(i,j) will depend on the condition j, etc.
> ?
> ? There is almost certainly no closed form solution for the covariance under logit.
> ? I am not sure about the probit (my guess is not).
> ? There will be some Laplace approximations available, a la Breslow and Clayton 1993.
> ?
> ? I'd be curious if these formulas/approximations were developed somewhere - I'd be surprised if they weren't.
> ?
> ? Florin
> ?
> ?
> ? > On Apr 20, 2020, at 12:48 AM, Emmanuel Curis <emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr>> wrote:
> ? >
> ? > Hello everyone,
> ? >
> ? > I hope you're all going fine in these difficult times.
> ? >
> ? > I tried to understand in details the exact model used when using glmer
> ? > for a Bernoulli experiment, by comparison with the linear mixed
> ? > effects model, and especially how it introducts correlations between
> ? > observations of a given group.  I think I finally got it, but could
> ? > you check that what I write below is correct and that I'm not missing
> ? > something?
> ? >
> ? > I use a very simple case with only a single random effect, and no
> ? > fixed effects, because I guess that adding fixed effects or other
> ? > random effects does not change the idea, it "just" makes formulas more
> ? > complex.  I note i the random effect level, let's say ? patient ?, and
> ? > j the observation for this patient.
> ? >
> ? > In the linear model, we have Y(i,j) = ?0 + Z(i) + epsilon( i, j ) with
> ? > Z(i) and epsilon(i,j) randoms variables having a density of
> ? > probability, independant, and each iid.
> ? >
> ? > Hence, cov( Y(i,j), Y(i,j') ) = Var( Z(i) ): the model introduces a
> ? > positive correlation between observations of the same patient.
> ? >
> ? >
> ? >
> ? > In the Bernoulli model, Y(i,j) ~ B( pi(i,j) ) and pi(i,j) = f( Z(i) ),
> ? > f being the inverse link function, typically the reciprocal of the
> ? > logit. So we have
> ? >
> ? > cov( Y(i,j), Y(i,j') ) = E( Y(i,j) Y(i, j') ) - E( Y(i,j) ) E( Y(i,j') )
> ? >     = Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) - pi(i,j) * pi(i,j')
> ? >
> ? > Since in practice pi(i,j) does not depend on j, pi(i,j) = pi(i,j').
> ? >
> ? > Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
> ? >  integral(R) Pr( Y(i,j) = 1 inter Y(i,j') = 1 | Z(i) = z ) p( Z(i) = z ) dz
> ? >
> ? > Then, we assume that conditionnally on Zi, the Yij are independant, is
> ? > this right? This is the equivalent of ? the epsilon(i, j) are
> ? > independant ?? I assume this hypothesis is also used for computing the
> ? > likelihood? If not, what is the model for the joint probability?
> ? >
> ? > In that case,
> ? >
> ? > Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
> ? >  integral(R) f(z) f(z) p( Z(i) = z ) dz
> ? >
> ? > and since pi(i,j) = integral( R ) f(z) p( Z(i) = z ) dz we have
> ? >
> ? > cov( Y(i,j), Y(i,j') ) =
> ? > integral( R ) f?(z) p( Z(i) = z ) dz -
> ? >  ( integral( R ) f(z) p( Z(i) = z ) dz )?
> ? >
> ? > which in general has no reason to be nul, hence the two observations
> ? > are correlated. Is this correct?
> ? >
> ? > Is there any way to have a closed-form of the covariance, for usual f
> ? > (let's say, logit or probit) and Z distribution (let's say, Gaussian)?
> ? >
> ? > Thanks a lot for reading, and your answers,
> ? >
> ? > --
> ? >                                Emmanuel CURIS
> ? >                                emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr>
> ? >
> ? > Page WWW: http://emmanuel.curis.online.fr/index.html
> ? >
> ? > _______________________________________________
> ? > R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
> ? > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> ?
> 
> --
>                               Emmanuel CURIS
>                               emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr>
> 
> Page WWW: http://emmanuel.curis.online.fr/index.html
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> [EXTERNAL EMAIL] This message originates from an external email address, please exercise caution when clicking any links or opening attachments. If you believe the sender is impersonating someone at QIMR Berghofer, please forward this message to phishing at qimrberghofer.edu.au.
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 

-- 
                                Emmanuel CURIS
                                emmanuel.curis at parisdescartes.fr

Page WWW: http://emmanuel.curis.online.fr/index.html


From john@m@|ndon@|d @end|ng |rom @nu@edu@@u  Tue Apr 21 11:58:34 2020
From: john@m@|ndon@|d @end|ng |rom @nu@edu@@u (John Maindonald)
Date: Tue, 21 Apr 2020 09:58:34 +0000
Subject: [R-sig-ME] 
 Precision about the glmer model for Bernoulli variables
In-Reply-To: <20200421091921.GC9918@info124.pharmacie.univ-paris5.fr>
References: <20200420074851.GA6361@info124.pharmacie.univ-paris5.fr>
 <AB72DE88-C199-459C-B0EB-0DF13BB1659B@health.ucsd.edu>
 <20200420193249.GB16474@info124.pharmacie.univ-paris5.fr>
 <2AD9CAD5-FABC-4A13-B684-6D0908CFBF21@health.ucsd.edu>
 <5f78610081524e23855ff4bcccdf45a8@qimrberghofer.edu.au>
 <55E960BE-5BC8-4BB7-B30C-0AF922C7D73C@anu.edu.au>
 <20200421091921.GC9918@info124.pharmacie.univ-paris5.fr>
Message-ID: <E3B67323-F100-4B13-B5D3-0017DE620B9D@anu.edu.au>

As I recall (but check what I say), the double binomial results from
modeling separately the ?successes? and ?failures?, with exponential
families somehow involved.  See the Efron reference on the help
page ?gamlss.dist::DBI

There?s a comparison between fits that uses the betabonimial vs use
of the double binomial in pp.119-123 of the notes that you can find at:
    https://www.dropbox.com/s/xnug9whkayd3mep/mrmar2020.pdf?dl=0
or https://bit.ly/2VP2zI4

I?ve been intending to post these notes, and supporting materials,
for whoever wants to use, but have been diverted onto other things.


John Maindonald             email: john.maindonald at anu.edu.au<mailto:john.maindonald at anu.edu.au>

On 21/04/2020, at 21:19, Emmanuel Curis <emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr>> wrote:

Thanks for all this details about correlation.

Just to be clear about the nomenclature: when a model is called
? A-B ?, A is for the cumulative distribution function used as link
between pi and the linear predictor including the random effect(s) or
its reciprocal, and B is for the distribution of random effect(s), is
this right?

If this is right, for the ? double binomial ?, it would mean that the
link function is a step function, and not a monotonous strictly
increasing function. Since it would then not be a bijection between R
and [0,1], doesn't it introduce indetermination and strong constraints
on the possible values of pi? I guess I'm wrongly interpreting what
means ? double binomial ?.

Sorry for this naive questions, I'm discovering the field...

Best regards,


On Tue, Apr 21, 2020 at 07:59:48AM +0000, John Maindonald wrote:
Actually, with a suitable parameterization, the betabinomial allows
small negative correlations.  See
 Binary Regression Using an Extended Beta-Binomial Distribution, With Discussion of
Correlation Induced by Covariate Measurement Errors
R. L. Prentice: Journal of the American Statistical Association
Vol. 81, No. 394 (Jun., 1986), pp. 321-327

A feature of the betabinomial (BB) , which ought to be better advertised than
it is in most accounts that I have seen, is that, for positive correlation rho,
it sets a strict lower limit of pi(1-pi)(1+rho) on the variance of the estimate
of the proportion pi.  This is in marked contrast to the variance
assumptions that define quasibinomial errors.

The gamlss package implements the double binomial (as well
as the logistic normal and BB), but as far as I can tell, not in a
multi-level model context.  The double binomial does allow a
negative correlation.


John Maindonald             email: john.maindonald at anu.edu.au<mailto:john.maindonald at anu.edu.au><mailto:john.maindonald at anu.edu.au>


On 21/04/2020, at 19:18, David Duffy <David.Duffy at qimrberghofer.edu.au<mailto:David.Duffy at qimrberghofer.edu.au><mailto:David.Duffy at qimrberghofer.edu.au>> wrote:

You'd know the beta-binomial constrains the range of the correlation coefficient positive, but the logit-normal and probit-normal allow a negative correlation, with a lower bound constrained by the N, in the case of regular sized clusters. For your example of 2x2 tables, the correlation can go from -1 to 1. This is the usual genetics
type setup where you model the pairwise correlations for all pairs of observations in the dataset versus their
(pairwise) genetic relatedness.  In multi-trait models, the correlations between different phenotypes (say asthma and diabetes) in the same individual can be negative, so you can estimate negative variance components.

The pedigreemm package extends lme4 to allow glmms for pedigree data (see the mastitis example for a binary trait).

Cheers, David Duffy.
________________________________________
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org<mailto:r-sig-mixed-models-bounces at r-project.org><mailto:r-sig-mixed-models-bounces at r-project.org>> on behalf of Vaida, Florin <fvaida at health.ucsd.edu<mailto:fvaida at health.ucsd.edu><mailto:fvaida at health.ucsd.edu>>
Sent: Tuesday, 21 April 2020 7:05:40 AM
To: Emmanuel Curis
Cc: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org><mailto:r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME]  Precision about the glmer model for Bernoulli variables

Hi Emmanuel,

That's a good question.  My guess is that the correlation is non-negative generally, but I wasn't able to prove it theoretically even in the simplest case when Y1, Y2 ~ Bernoulli(u) independent conditionally on u, and u ~ Normal(0, 1).  I am curious if someone has a solution.
We can't go too far down this route in this forum, since Doug wants to keep it applied.

Florin

On Apr 20, 2020, at 12:32 PM, Emmanuel Curis <emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr><mailto:emmanuel.curis at parisdescartes.fr>> wrote:

Hi Florin,

Thanks for the answer, the precision about p(i,j), and the reference.

A last question, that I forgot in my message: is the obtained
correlation also always positive, as in the linear case? Or may some
negative correlation appear, depending on the values of pi(i,j) and
pi(i,j')?

Best regards,

On Mon, Apr 20, 2020 at 03:27:39PM +0000, Vaida, Florin wrote:
? Hi Emmanuel,
?
? Your reasoning is correct.
?
? As a quibble, outside a simple repeated measures experiment setup, p(i,j) *does* depend on j.
? For example, if observations are collected over time, generally there is a time effect; if they are repeated measures with different experimental conditions, p(i,j) will depend on the condition j, etc.
?
? There is almost certainly no closed form solution for the covariance under logit.
? I am not sure about the probit (my guess is not).
? There will be some Laplace approximations available, a la Breslow and Clayton 1993.
?
? I'd be curious if these formulas/approximations were developed somewhere - I'd be surprised if they weren't.
?
? Florin
?
?
? > On Apr 20, 2020, at 12:48 AM, Emmanuel Curis <emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr><mailto:emmanuel.curis at parisdescartes.fr>> wrote:
? >
? > Hello everyone,
? >
? > I hope you're all going fine in these difficult times.
? >
? > I tried to understand in details the exact model used when using glmer
? > for a Bernoulli experiment, by comparison with the linear mixed
? > effects model, and especially how it introducts correlations between
? > observations of a given group.  I think I finally got it, but could
? > you check that what I write below is correct and that I'm not missing
? > something?
? >
? > I use a very simple case with only a single random effect, and no
? > fixed effects, because I guess that adding fixed effects or other
? > random effects does not change the idea, it "just" makes formulas more
? > complex.  I note i the random effect level, let's say ? patient ?, and
? > j the observation for this patient.
? >
? > In the linear model, we have Y(i,j) = ?0 + Z(i) + epsilon( i, j ) with
? > Z(i) and epsilon(i,j) randoms variables having a density of
? > probability, independant, and each iid.
? >
? > Hence, cov( Y(i,j), Y(i,j') ) = Var( Z(i) ): the model introduces a
? > positive correlation between observations of the same patient.
? >
? >
? >
? > In the Bernoulli model, Y(i,j) ~ B( pi(i,j) ) and pi(i,j) = f( Z(i) ),
? > f being the inverse link function, typically the reciprocal of the
? > logit. So we have
? >
? > cov( Y(i,j), Y(i,j') ) = E( Y(i,j) Y(i, j') ) - E( Y(i,j) ) E( Y(i,j') )
? >     = Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) - pi(i,j) * pi(i,j')
? >
? > Since in practice pi(i,j) does not depend on j, pi(i,j) = pi(i,j').
? >
? > Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
? >  integral(R) Pr( Y(i,j) = 1 inter Y(i,j') = 1 | Z(i) = z ) p( Z(i) = z ) dz
? >
? > Then, we assume that conditionnally on Zi, the Yij are independant, is
? > this right? This is the equivalent of ? the epsilon(i, j) are
? > independant ?? I assume this hypothesis is also used for computing the
? > likelihood? If not, what is the model for the joint probability?
? >
? > In that case,
? >
? > Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
? >  integral(R) f(z) f(z) p( Z(i) = z ) dz
? >
? > and since pi(i,j) = integral( R ) f(z) p( Z(i) = z ) dz we have
? >
? > cov( Y(i,j), Y(i,j') ) =
? > integral( R ) f?(z) p( Z(i) = z ) dz -
? >  ( integral( R ) f(z) p( Z(i) = z ) dz )?
? >
? > which in general has no reason to be nul, hence the two observations
? > are correlated. Is this correct?
? >
? > Is there any way to have a closed-form of the covariance, for usual f
? > (let's say, logit or probit) and Z distribution (let's say, Gaussian)?
? >
? > Thanks a lot for reading, and your answers,
? >
? > --
? >                                Emmanuel CURIS
? >                                emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr><mailto:emmanuel.curis at parisdescartes.fr>
? >
? > Page WWW: http://emmanuel.curis.online.fr/index.html
? >
? > _______________________________________________
? > R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org><mailto:R-sig-mixed-models at r-project.org> mailing list
? > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
?

--
                             Emmanuel CURIS
                             emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr><mailto:emmanuel.curis at parisdescartes.fr>

Page WWW: http://emmanuel.curis.online.fr/index.html

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org><mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
[EXTERNAL EMAIL] This message originates from an external email address, please exercise caution when clicking any links or opening attachments. If you believe the sender is impersonating someone at QIMR Berghofer, please forward this message to phishing at qimrberghofer.edu.au.

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


--
                               Emmanuel CURIS
                               emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr>

Page WWW: http://emmanuel.curis.online.fr/index.html


	[[alternative HTML version deleted]]


From kr@g|tcode @end|ng |rom gm@||@com  Tue Apr 21 18:25:45 2020
From: kr@g|tcode @end|ng |rom gm@||@com (Kate R)
Date: Tue, 21 Apr 2020 09:25:45 -0700
Subject: [R-sig-ME] AIC Comparison for MLM with Different Distributions
In-Reply-To: <ec5b4a11-5d67-aea2-131c-19181814c1c4@gmail.com>
References: <mailman.18133.5.1583578801.26076.r-sig-mixed-models@r-project.org>
 <CAGXFLecNWPVM7yqzvDMHu6wnPGn9dcDdWRRtrL4TWeXprKYS8w@mail.gmail.com>
 <ec5b4a11-5d67-aea2-131c-19181814c1c4@gmail.com>
Message-ID: <CAGXFLedMjjmneDWbd8LpsCWAgYoHs0BdTzOVBqBeUAj-J2_W6w@mail.gmail.com>

Hi Ben,

Thank you again for your help before!

We will be using other model assessments (including R^2) as well, but I
wanted to check if the AICs can be compared for (hurdled) gamma models with
(zero-inflated) beta models when fit in glmmTMB? Likewise, can the AICs be
compared for (zero-inflated / hurdle) negative binomial and (zero-inflated)
beta models? For the beta models, we have transformed raw counts/durations
into percentages.

Many thanks,
K

On Sat, Mar 7, 2020 at 9:17 AM Ben Bolker <bbolker at gmail.com> wrote:

>
>   Only when the response variable is transformed.
>   [please keep r-sig-mixed-models in the cc: list if possible when
> following up on questions ... ]
>
>   cheers
>     Ben Bolker
>
> On 2020-03-07 12:16 p.m., Kate R wrote:
> >
> > Hi Ben,
> >
> > Thank you for your reply! Would I also apply the Jacobian correction to
> > the Gamma with log-link, or is it only used when the response variable
> > is transformed?
> >
> > Many thanks again!
> > Katie
> >
> >
> >     ------------------------------
> >
> >     Message: 2
> >     Date: Wed, 4 Mar 2020 09:55:55 -0500
> >     From: Ben Bolker <bbolker at gmail.com <mailto:bbolker at gmail.com>>
> >     To: r-sig-mixed-models at r-project.org
> >     <mailto:r-sig-mixed-models at r-project.org>
> >     Subject: Re: [R-sig-ME] AIC Comparison for MLM with Different
> >             Distributions
> >     Message-ID: <eae1791a-e56a-7c32-b872-f6fa93157857 at gmail.com
> >     <mailto:eae1791a-e56a-7c32-b872-f6fa93157857 at gmail.com>>
> >     Content-Type: text/plain; charset="utf-8"
> >
> >
> >       I agree with Thierry's big-picture comment that you should
> generally
> >     use broader/qualitative criteria to decide on a model rather than
> >     testing all possibilities.  The only exception I can think of is if
> you
> >     are *only* interested in predictive accuracy (not in inference
> >     [confidence intervals/p-values etc.]), and you make sure to use
> >     cross-validation or a testing set to evaluate out-of-sample
> predictive
> >     error (although AIC *should* generally give a reasonable
> approximation
> >     to relative out-of-sample error).
> >
> >       Beyond that, if you still want to compute AIC (e.g. your
> supervisor or
> >     a reviewer is forcing to do it, and you don't think you're in a
> position
> >     to push back effectively):
> >
> >       * as long as you include the Jacobian correction when you transform
> >     the predictor variable (i.e. #2), these log-likelihoods (and AICs)
> >     should in principle be comparable (FWIW the robustness of the
> derivation
> >     of AIC is much weaker for non-nested models; Brian Ripley [of MASS
> fame]
> >     holds a minority opinion that one should *not* use AICs to compare
> >     non-nested models)
> >
> >       * computing log-likelihoods/AICs by hand is in principle a good
> idea,
> >     but is often difficulty for multi-level models, as various integrals
> or
> >     approximations of integrals are involved.  The lmer and glmer
> >     likelihoods (1-4) are definitely comparable. To compare across
> platforms
> >     I often try to think of a simplified model that *can* be fitted in
> both
> >     platforms (e.g. in this case I think a proportional-odds ordinal
> >     regression where the response has only two levels should be
> equivalent
> >     to a binomial model with cloglog link ...)
> >
> >       cheers
> >       Ben Bolker
> >
> >     On 2020-03-03 5:29 p.m., Kate R wrote:
> >     > Hi all,
> >     >
> >     > Thank you in advance for your time and consideration! I am a
> >     > non-mathematically-inclined graduate student in communication just
> >     learning
> >     > multilevel modeling.
> >     >
> >     > I am trying to compare the AIC for 5 different models:
> >     >
> >     >
> >     >    1. model.mn5 <- lmer(anxious ~ num.cm <http://num.cm> + num.pmc
> >     + (1|userid), data = df,
> >     >    REML = F)
> >     >    2. model.mn5.log <- lmer(log(anxious) ~ num.cm <http://num.cm>
> >     + num.pmc + (1|userid),
> >     >    data = df, REML = F)
> >     >    3. model.mn5.gamma.log <- glmer(anxious ~ num.cm
> >     <http://num.cm> + num.pmc + (1|userid),
> >     >    data = df, family = Gamma(link="log"))
> >     >    4. model.mn5.gamma.id <http://model.mn5.gamma.id> <-
> >     glmer(anxious ~ num.cm <http://num.cm> + num.pmc + (1|userid),
> >     >    data = df, family = Gamma(link="identity"))
> >     >    5. model.ord5 <- clmm(anxious ~ num.cm <http://num.cm> +
> >     num.pmc + (1|userid), data =
> >     >    df, na.action = na.omit)
> >     >
> >     > (num.cm <http://num.cm> is the group mean and num.pmc is the
> >     group-mean-centered score of
> >     > the predictor)
> >     >
> >     > Despite many posts on various help forums, I understand that it's
> >     possible
> >     > to compare non-nested models with different distributions as long
> >     as all
> >     > terms, including constants, are retained (i.e. see Burnham &
> >     Anderson, Ch
> >     > 6.7 <https://www.springer.com/gp/book/9780387953649>), but that
> >     different R
> >     > packages or model classes might handle constants differently or use
> >     > different algorithms (see point 7
> >     <https://robjhyndman.com/hyndsight/aic/>),
> >     > thus making it difficult to directly compare AIC values. To avoid
> >     > this non-comparability pitfall, it was suggested in one post to
> >     calculate
> >     > your own log-likelihood (though I'm having trouble finding this
> >     post again).
> >     >
> >
> >
> >
> >
> >     > Please could you help with the following:
> >     >
> >     >    - What is the best practice for comparing the AICs for these 5
> >     models?
> >     >    - What is the R-code for manually calculating the
> >     log-likelihood and/or
> >     >    the AIC to retain all terms, including constants?
> >     >    - Can you compare ordinal models (clmm) with the continuous
> models?
> >     >    - Do you recommend any other methods and/or packages for
> comparing
> >     >    models with different distributions and/or links?
> >     >
> >     > Many thanks in advance for your time and consideration! I greatly
> >     > appreciate any suggestions.
> >     >
> >     > Kind regards,
> >     > K
> >     >
> >     >       [[alternative HTML version deleted]]
> >     >
> >     > _______________________________________________
> >     > R-sig-mixed-models at r-project.org
> >     <mailto:R-sig-mixed-models at r-project.org> mailing list
> >     > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >     >
> >
> >
> >
> >
>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Tue Apr 21 19:08:39 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Tue, 21 Apr 2020 13:08:39 -0400
Subject: [R-sig-ME] AIC Comparison for MLM with Different Distributions
In-Reply-To: <CAGXFLedMjjmneDWbd8LpsCWAgYoHs0BdTzOVBqBeUAj-J2_W6w@mail.gmail.com>
References: <mailman.18133.5.1583578801.26076.r-sig-mixed-models@r-project.org>
 <CAGXFLecNWPVM7yqzvDMHu6wnPGn9dcDdWRRtrL4TWeXprKYS8w@mail.gmail.com>
 <ec5b4a11-5d67-aea2-131c-19181814c1c4@gmail.com>
 <CAGXFLedMjjmneDWbd8LpsCWAgYoHs0BdTzOVBqBeUAj-J2_W6w@mail.gmail.com>
Message-ID: <afc7fbeb-fe56-9b90-20e6-9bf637247ca4@gmail.com>

 ? (Note that this is a public list - not just me!)

 ? AICs are comparable across a pretty wide spectrum of models, with 
some caveats.

 ?? * they're asymptotic measures (and the AICc correction was derived 
for linear models, so may or may not be exactly applicable to other 
model types)

 ?? * comparing models with parameters on the boundary (e.g. comparing a 
zero-inflated vs a non-zero-inflated model) is not exactly correct

 ?? * there's a "level of focus" question when comparing models that 
differ in their random effects

 ?? I believe most of this is discussed in the GLMM FAQ.

On 4/21/20 12:25 PM, Kate R wrote:
> Hi Ben,
>
> Thank you again for your help before!
>
> We will be using other model assessments (including R^2) as well, but 
> I wanted to check if the AICs can be compared for (hurdled) gamma 
> models with (zero-inflated) beta models when fit in glmmTMB? Likewise, 
> can the AICs be compared for(zero-inflated / hurdle) negative binomial 
> and (zero-inflated) beta models?For the beta models, we have 
> transformed raw counts/durations into percentages.
>
> Many thanks,
> K
>
> On Sat, Mar 7, 2020 at 9:17 AM Ben Bolker <bbolker at gmail.com 
> <mailto:bbolker at gmail.com>> wrote:
>
>
>     ? Only when the response variable is transformed.
>     ? [please keep r-sig-mixed-models in the cc: list if possible when
>     following up on questions ... ]
>
>     ? cheers
>     ? ? Ben Bolker
>
>     On 2020-03-07 12:16 p.m., Kate R wrote:
>     >
>     > Hi Ben,
>     >
>     > Thank you for your reply! Would I also apply the Jacobian
>     correction to
>     > the Gamma with log-link, or is it only used when the response
>     variable
>     > is transformed?
>     >
>     > Many thanks again!
>     > Katie
>     >
>     >
>     >? ? ?------------------------------
>     >
>     >? ? ?Message: 2
>     >? ? ?Date: Wed, 4 Mar 2020 09:55:55 -0500
>     >? ? ?From: Ben Bolker <bbolker at gmail.com
>     <mailto:bbolker at gmail.com> <mailto:bbolker at gmail.com
>     <mailto:bbolker at gmail.com>>>
>     >? ? ?To: r-sig-mixed-models at r-project.org
>     <mailto:r-sig-mixed-models at r-project.org>
>     >? ? ?<mailto:r-sig-mixed-models at r-project.org
>     <mailto:r-sig-mixed-models at r-project.org>>
>     >? ? ?Subject: Re: [R-sig-ME] AIC Comparison for MLM with Different
>     >? ? ?? ? ? ? Distributions
>     >? ? ?Message-ID: <eae1791a-e56a-7c32-b872-f6fa93157857 at gmail.com
>     <mailto:eae1791a-e56a-7c32-b872-f6fa93157857 at gmail.com>
>     >? ? ?<mailto:eae1791a-e56a-7c32-b872-f6fa93157857 at gmail.com
>     <mailto:eae1791a-e56a-7c32-b872-f6fa93157857 at gmail.com>>>
>     >? ? ?Content-Type: text/plain; charset="utf-8"
>     >
>     >
>     >? ? ?? I agree with Thierry's big-picture comment that you should
>     generally
>     >? ? ?use broader/qualitative criteria to decide on a model rather
>     than
>     >? ? ?testing all possibilities.? The only exception I can think
>     of is if you
>     >? ? ?are *only* interested in predictive accuracy (not in inference
>     >? ? ?[confidence intervals/p-values etc.]), and you make sure to use
>     >? ? ?cross-validation or a testing set to evaluate out-of-sample
>     predictive
>     >? ? ?error (although AIC *should* generally give a reasonable
>     approximation
>     >? ? ?to relative out-of-sample error).
>     >
>     >? ? ?? Beyond that, if you still want to compute AIC (e.g. your
>     supervisor or
>     >? ? ?a reviewer is forcing to do it, and you don't think you're
>     in a position
>     >? ? ?to push back effectively):
>     >
>     >? ? ?? * as long as you include the Jacobian correction when you
>     transform
>     >? ? ?the predictor variable (i.e. #2), these log-likelihoods (and
>     AICs)
>     >? ? ?should in principle be comparable (FWIW the robustness of
>     the derivation
>     >? ? ?of AIC is much weaker for non-nested models; Brian Ripley
>     [of MASS fame]
>     >? ? ?holds a minority opinion that one should *not* use AICs to
>     compare
>     >? ? ?non-nested models)
>     >
>     >? ? ?? * computing log-likelihoods/AICs by hand is in principle a
>     good idea,
>     >? ? ?but is often difficulty for multi-level models, as various
>     integrals or
>     >? ? ?approximations of integrals are involved.? The lmer and glmer
>     >? ? ?likelihoods (1-4) are definitely comparable. To compare
>     across platforms
>     >? ? ?I often try to think of a simplified model that *can* be
>     fitted in both
>     >? ? ?platforms (e.g. in this case I think a proportional-odds ordinal
>     >? ? ?regression where the response has only two levels should be
>     equivalent
>     >? ? ?to a binomial model with cloglog link ...)
>     >
>     >? ? ?? cheers
>     >? ? ?? Ben Bolker
>     >
>     >? ? ?On 2020-03-03 5:29 p.m., Kate R wrote:
>     >? ? ?> Hi all,
>     >? ? ?>
>     >? ? ?> Thank you in advance for your time and consideration! I am a
>     >? ? ?> non-mathematically-inclined graduate student in
>     communication just
>     >? ? ?learning
>     >? ? ?> multilevel modeling.
>     >? ? ?>
>     >? ? ?> I am trying to compare the AIC for 5 different models:
>     >? ? ?>
>     >? ? ?>
>     >? ? ?>? ? 1. model.mn5 <- lmer(anxious ~ num.cm <http://num.cm>
>     <http://num.cm> + num.pmc
>     >? ? ?+ (1|userid), data = df,
>     >? ? ?>? ? REML = F)
>     >? ? ?>? ? 2. model.mn5.log <- lmer(log(anxious) ~ num.cm
>     <http://num.cm> <http://num.cm>
>     >? ? ?+ num.pmc + (1|userid),
>     >? ? ?>? ? data = df, REML = F)
>     >? ? ?>? ? 3. model.mn5.gamma.log <- glmer(anxious ~ num.cm
>     <http://num.cm>
>     >? ? ?<http://num.cm> + num.pmc + (1|userid),
>     >? ? ?>? ? data = df, family = Gamma(link="log"))
>     >? ? ?>? ? 4. model.mn5.gamma.id <http://model.mn5.gamma.id>
>     <http://model.mn5.gamma.id> <-
>     >? ? ?glmer(anxious ~ num.cm <http://num.cm> <http://num.cm> +
>     num.pmc + (1|userid),
>     >? ? ?>? ? data = df, family = Gamma(link="identity"))
>     >? ? ?>? ? 5. model.ord5 <- clmm(anxious ~ num.cm <http://num.cm>
>     <http://num.cm> +
>     >? ? ?num.pmc + (1|userid), data =
>     >? ? ?>? ? df, na.action = na.omit)
>     >? ? ?>
>     >? ? ?> (num.cm <http://num.cm> <http://num.cm> is the group mean
>     and num.pmc is the
>     >? ? ?group-mean-centered score of
>     >? ? ?> the predictor)
>     >? ? ?>
>     >? ? ?> Despite many posts on various help forums, I understand
>     that it's
>     >? ? ?possible
>     >? ? ?> to compare non-nested models with different distributions
>     as long
>     >? ? ?as all
>     >? ? ?> terms, including constants, are retained (i.e. see Burnham &
>     >? ? ?Anderson, Ch
>     >? ? ?> 6.7 <https://www.springer.com/gp/book/9780387953649>), but
>     that
>     >? ? ?different R
>     >? ? ?> packages or model classes might handle constants
>     differently or use
>     >? ? ?> different algorithms (see point 7
>     >? ? ?<https://robjhyndman.com/hyndsight/aic/>),
>     >? ? ?> thus making it difficult to directly compare AIC values.
>     To avoid
>     >? ? ?> this non-comparability pitfall, it was suggested in one
>     post to
>     >? ? ?calculate
>     >? ? ?> your own log-likelihood (though I'm having trouble finding
>     this
>     >? ? ?post again).
>     >? ? ?>
>     >
>     >
>     >
>     >
>     >? ? ?> Please could you help with the following:
>     >? ? ?>
>     >? ? ?>? ? - What is the best practice for comparing the AICs for
>     these 5
>     >? ? ?models?
>     >? ? ?>? ? - What is the R-code for manually calculating the
>     >? ? ?log-likelihood and/or
>     >? ? ?>? ? the AIC to retain all terms, including constants?
>     >? ? ?>? ? - Can you compare ordinal models (clmm) with the
>     continuous models?
>     >? ? ?>? ? - Do you recommend any other methods and/or packages
>     for comparing
>     >? ? ?>? ? models with different distributions and/or links?
>     >? ? ?>
>     >? ? ?> Many thanks in advance for your time and consideration! I
>     greatly
>     >? ? ?> appreciate any suggestions.
>     >? ? ?>
>     >? ? ?> Kind regards,
>     >? ? ?> K
>     >? ? ?>
>     >? ? ?>? ? ? ?[[alternative HTML version deleted]]
>     >? ? ?>
>     >? ? ?> _______________________________________________
>     >? ? ?> R-sig-mixed-models at r-project.org
>     <mailto:R-sig-mixed-models at r-project.org>
>     >? ? ?<mailto:R-sig-mixed-models at r-project.org
>     <mailto:R-sig-mixed-models at r-project.org>> mailing list
>     >? ? ?> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>     >? ? ?>
>     >
>     >
>     >
>     >
>

	[[alternative HTML version deleted]]


From |v@|d@ @end|ng |rom he@|th@uc@d@edu  Tue Apr 21 19:25:33 2020
From: |v@|d@ @end|ng |rom he@|th@uc@d@edu (Vaida, Florin)
Date: Tue, 21 Apr 2020 17:25:33 +0000
Subject: [R-sig-ME] 
 Precision about the glmer model for Bernoulli variables
In-Reply-To: <2AD9CAD5-FABC-4A13-B684-6D0908CFBF21@health.ucsd.edu>
References: <20200420074851.GA6361@info124.pharmacie.univ-paris5.fr>
 <AB72DE88-C199-459C-B0EB-0DF13BB1659B@health.ucsd.edu>
 <20200420193249.GB16474@info124.pharmacie.univ-paris5.fr>
 <2AD9CAD5-FABC-4A13-B684-6D0908CFBF21@health.ucsd.edu>
Message-ID: <0E4F6A32-CB86-4C49-AEFE-A0BEE6E16CED@health.ucsd.edu>

Hi Emmanuel,

So I can prove positive within-subject correlation for GLME logistic regression with random intercepts - assuming all observations have same mean!

Let Yj ~ Bernoulli(mu), logit(mu) = beta + u, u ~ Normal(0, tau^2).
Using the conditional covariance formula you get
Cov(Y1, Y2) = E(Cov(Y1,Y2 | u) + Cov(E(Y1|u), E(Y2|u)) = 0 + Cov( mu(u), mu(u)) = Var(mu(u)) >= 0, with 0 only if tau^2 = 0.

This proof does not extend if you let Yj have different means, i.e., replace beta by beta_j.
It also does not apply to more general random effects structures, e.g. random intercepts and slopes.
Note however that for the *linear* model with random intercepts and slopes, the correlation is not guaranteed positive.

Florin


> On Apr 20, 2020, at 2:05 PM, Vaida, Florin <fvaida at health.ucsd.edu> wrote:
> 
> Hi Emmanuel,
> 
> That's a good question.  My guess is that the correlation is non-negative generally, but I wasn't able to prove it theoretically even in the simplest case when Y1, Y2 ~ Bernoulli(u) independent conditionally on u, and u ~ Normal(0, 1).  I am curious if someone has a solution.
> We can't go too far down this route in this forum, since Doug wants to keep it applied.
> 
> Florin
> 
>> On Apr 20, 2020, at 12:32 PM, Emmanuel Curis <emmanuel.curis at parisdescartes.fr> wrote:
>> 
>> Hi Florin,
>> 
>> Thanks for the answer, the precision about p(i,j), and the reference.
>> 
>> A last question, that I forgot in my message: is the obtained
>> correlation also always positive, as in the linear case? Or may some
>> negative correlation appear, depending on the values of pi(i,j) and
>> pi(i,j')?
>> 
>> Best regards,
>> 
>> On Mon, Apr 20, 2020 at 03:27:39PM +0000, Vaida, Florin wrote:
>> ? Hi Emmanuel,
>> ? 
>> ? Your reasoning is correct.
>> ? 
>> ? As a quibble, outside a simple repeated measures experiment setup, p(i,j) *does* depend on j.
>> ? For example, if observations are collected over time, generally there is a time effect; if they are repeated measures with different experimental conditions, p(i,j) will depend on the condition j, etc.
>> ? 
>> ? There is almost certainly no closed form solution for the covariance under logit.
>> ? I am not sure about the probit (my guess is not).
>> ? There will be some Laplace approximations available, a la Breslow and Clayton 1993.
>> ? 
>> ? I'd be curious if these formulas/approximations were developed somewhere - I'd be surprised if they weren't.
>> ? 
>> ? Florin
>> ? 
>> ? 
>> ? > On Apr 20, 2020, at 12:48 AM, Emmanuel Curis <emmanuel.curis at parisdescartes.fr> wrote:
>> ? > 
>> ? > Hello everyone,
>> ? > 
>> ? > I hope you're all going fine in these difficult times.
>> ? > 
>> ? > I tried to understand in details the exact model used when using glmer
>> ? > for a Bernoulli experiment, by comparison with the linear mixed
>> ? > effects model, and especially how it introducts correlations between
>> ? > observations of a given group.  I think I finally got it, but could
>> ? > you check that what I write below is correct and that I'm not missing
>> ? > something?
>> ? > 
>> ? > I use a very simple case with only a single random effect, and no
>> ? > fixed effects, because I guess that adding fixed effects or other
>> ? > random effects does not change the idea, it "just" makes formulas more
>> ? > complex.  I note i the random effect level, let's say ? patient ?, and
>> ? > j the observation for this patient.
>> ? > 
>> ? > In the linear model, we have Y(i,j) = ?0 + Z(i) + epsilon( i, j ) with
>> ? > Z(i) and epsilon(i,j) randoms variables having a density of
>> ? > probability, independant, and each iid.
>> ? > 
>> ? > Hence, cov( Y(i,j), Y(i,j') ) = Var( Z(i) ): the model introduces a
>> ? > positive correlation between observations of the same patient.
>> ? > 
>> ? > 
>> ? > 
>> ? > In the Bernoulli model, Y(i,j) ~ B( pi(i,j) ) and pi(i,j) = f( Z(i) ),
>> ? > f being the inverse link function, typically the reciprocal of the
>> ? > logit. So we have
>> ? > 
>> ? > cov( Y(i,j), Y(i,j') ) = E( Y(i,j) Y(i, j') ) - E( Y(i,j) ) E( Y(i,j') )
>> ? >     = Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) - pi(i,j) * pi(i,j')
>> ? > 
>> ? > Since in practice pi(i,j) does not depend on j, pi(i,j) = pi(i,j').
>> ? > 
>> ? > Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
>> ? >  integral(R) Pr( Y(i,j) = 1 inter Y(i,j') = 1 | Z(i) = z ) p( Z(i) = z ) dz
>> ? > 
>> ? > Then, we assume that conditionnally on Zi, the Yij are independant, is
>> ? > this right? This is the equivalent of ? the epsilon(i, j) are
>> ? > independant ?? I assume this hypothesis is also used for computing the
>> ? > likelihood? If not, what is the model for the joint probability?
>> ? > 
>> ? > In that case,
>> ? > 
>> ? > Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
>> ? >  integral(R) f(z) f(z) p( Z(i) = z ) dz
>> ? > 
>> ? > and since pi(i,j) = integral( R ) f(z) p( Z(i) = z ) dz we have
>> ? > 
>> ? > cov( Y(i,j), Y(i,j') ) =
>> ? > integral( R ) f?(z) p( Z(i) = z ) dz -
>> ? >  ( integral( R ) f(z) p( Z(i) = z ) dz )?
>> ? > 
>> ? > which in general has no reason to be nul, hence the two observations
>> ? > are correlated. Is this correct?
>> ? > 
>> ? > Is there any way to have a closed-form of the covariance, for usual f
>> ? > (let's say, logit or probit) and Z distribution (let's say, Gaussian)?
>> ? > 
>> ? > Thanks a lot for reading, and your answers,
>> ? > 
>> ? > -- 
>> ? >                                Emmanuel CURIS
>> ? >                                emmanuel.curis at parisdescartes.fr
>> ? > 
>> ? > Page WWW: http://emmanuel.curis.online.fr/index.html
>> ? > 
>> ? > _______________________________________________
>> ? > R-sig-mixed-models at r-project.org mailing list
>> ? > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> ? 
>> 
>> -- 
>>                               Emmanuel CURIS
>>                               emmanuel.curis at parisdescartes.fr
>> 
>> Page WWW: http://emmanuel.curis.online.fr/index.html
> 


From |v@|d@ @end|ng |rom he@|th@uc@d@edu  Tue Apr 21 19:33:13 2020
From: |v@|d@ @end|ng |rom he@|th@uc@d@edu (Vaida, Florin)
Date: Tue, 21 Apr 2020 17:33:13 +0000
Subject: [R-sig-ME] 
 Precision about the glmer model for Bernoulli variables
In-Reply-To: <0E4F6A32-CB86-4C49-AEFE-A0BEE6E16CED@health.ucsd.edu>
References: <20200420074851.GA6361@info124.pharmacie.univ-paris5.fr>
 <AB72DE88-C199-459C-B0EB-0DF13BB1659B@health.ucsd.edu>
 <20200420193249.GB16474@info124.pharmacie.univ-paris5.fr>
 <2AD9CAD5-FABC-4A13-B684-6D0908CFBF21@health.ucsd.edu>
 <0E4F6A32-CB86-4C49-AEFE-A0BEE6E16CED@health.ucsd.edu>
Message-ID: <E8150DEF-961A-447D-875B-F38E78E8C228@health.ucsd.edu>

I forgot to mention that this proof works for any link function, any conditional distribution for the response, and any random effects distribution.
In other words, it's not restricted to logit link, nor to binomial data, nor to normal random effects:

If Yj ~ iid conditional on u, then Cov(Y_j, Y_k) >= 0, with equality only in certain restrictive conditions.


> On Apr 21, 2020, at 10:25 AM, Vaida, Florin <fvaida at health.ucsd.edu> wrote:
> 
> Hi Emmanuel,
> 
> So I can prove positive within-subject correlation for GLME logistic regression with random intercepts - assuming all observations have same mean!
> 
> Let Yj ~ Bernoulli(mu), logit(mu) = beta + u, u ~ Normal(0, tau^2).
> Using the conditional covariance formula you get
> Cov(Y1, Y2) = E(Cov(Y1,Y2 | u) + Cov(E(Y1|u), E(Y2|u)) = 0 + Cov( mu(u), mu(u)) = Var(mu(u)) >= 0, with 0 only if tau^2 = 0.
> 
> This proof does not extend if you let Yj have different means, i.e., replace beta by beta_j.
> It also does not apply to more general random effects structures, e.g. random intercepts and slopes.
> Note however that for the *linear* model with random intercepts and slopes, the correlation is not guaranteed positive.
> 
> Florin
> 
> 
>> On Apr 20, 2020, at 2:05 PM, Vaida, Florin <fvaida at health.ucsd.edu> wrote:
>> 
>> Hi Emmanuel,
>> 
>> That's a good question.  My guess is that the correlation is non-negative generally, but I wasn't able to prove it theoretically even in the simplest case when Y1, Y2 ~ Bernoulli(u) independent conditionally on u, and u ~ Normal(0, 1).  I am curious if someone has a solution.
>> We can't go too far down this route in this forum, since Doug wants to keep it applied.
>> 
>> Florin
>> 
>>> On Apr 20, 2020, at 12:32 PM, Emmanuel Curis <emmanuel.curis at parisdescartes.fr> wrote:
>>> 
>>> Hi Florin,
>>> 
>>> Thanks for the answer, the precision about p(i,j), and the reference.
>>> 
>>> A last question, that I forgot in my message: is the obtained
>>> correlation also always positive, as in the linear case? Or may some
>>> negative correlation appear, depending on the values of pi(i,j) and
>>> pi(i,j')?
>>> 
>>> Best regards,
>>> 
>>> On Mon, Apr 20, 2020 at 03:27:39PM +0000, Vaida, Florin wrote:
>>> ? Hi Emmanuel,
>>> ? 
>>> ? Your reasoning is correct.
>>> ? 
>>> ? As a quibble, outside a simple repeated measures experiment setup, p(i,j) *does* depend on j.
>>> ? For example, if observations are collected over time, generally there is a time effect; if they are repeated measures with different experimental conditions, p(i,j) will depend on the condition j, etc.
>>> ? 
>>> ? There is almost certainly no closed form solution for the covariance under logit.
>>> ? I am not sure about the probit (my guess is not).
>>> ? There will be some Laplace approximations available, a la Breslow and Clayton 1993.
>>> ? 
>>> ? I'd be curious if these formulas/approximations were developed somewhere - I'd be surprised if they weren't.
>>> ? 
>>> ? Florin
>>> ? 
>>> ? 
>>> ? > On Apr 20, 2020, at 12:48 AM, Emmanuel Curis <emmanuel.curis at parisdescartes.fr> wrote:
>>> ? > 
>>> ? > Hello everyone,
>>> ? > 
>>> ? > I hope you're all going fine in these difficult times.
>>> ? > 
>>> ? > I tried to understand in details the exact model used when using glmer
>>> ? > for a Bernoulli experiment, by comparison with the linear mixed
>>> ? > effects model, and especially how it introducts correlations between
>>> ? > observations of a given group.  I think I finally got it, but could
>>> ? > you check that what I write below is correct and that I'm not missing
>>> ? > something?
>>> ? > 
>>> ? > I use a very simple case with only a single random effect, and no
>>> ? > fixed effects, because I guess that adding fixed effects or other
>>> ? > random effects does not change the idea, it "just" makes formulas more
>>> ? > complex.  I note i the random effect level, let's say ? patient ?, and
>>> ? > j the observation for this patient.
>>> ? > 
>>> ? > In the linear model, we have Y(i,j) = ?0 + Z(i) + epsilon( i, j ) with
>>> ? > Z(i) and epsilon(i,j) randoms variables having a density of
>>> ? > probability, independant, and each iid.
>>> ? > 
>>> ? > Hence, cov( Y(i,j), Y(i,j') ) = Var( Z(i) ): the model introduces a
>>> ? > positive correlation between observations of the same patient.
>>> ? > 
>>> ? > 
>>> ? > 
>>> ? > In the Bernoulli model, Y(i,j) ~ B( pi(i,j) ) and pi(i,j) = f( Z(i) ),
>>> ? > f being the inverse link function, typically the reciprocal of the
>>> ? > logit. So we have
>>> ? > 
>>> ? > cov( Y(i,j), Y(i,j') ) = E( Y(i,j) Y(i, j') ) - E( Y(i,j) ) E( Y(i,j') )
>>> ? >     = Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) - pi(i,j) * pi(i,j')
>>> ? > 
>>> ? > Since in practice pi(i,j) does not depend on j, pi(i,j) = pi(i,j').
>>> ? > 
>>> ? > Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
>>> ? >  integral(R) Pr( Y(i,j) = 1 inter Y(i,j') = 1 | Z(i) = z ) p( Z(i) = z ) dz
>>> ? > 
>>> ? > Then, we assume that conditionnally on Zi, the Yij are independant, is
>>> ? > this right? This is the equivalent of ? the epsilon(i, j) are
>>> ? > independant ?? I assume this hypothesis is also used for computing the
>>> ? > likelihood? If not, what is the model for the joint probability?
>>> ? > 
>>> ? > In that case,
>>> ? > 
>>> ? > Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
>>> ? >  integral(R) f(z) f(z) p( Z(i) = z ) dz
>>> ? > 
>>> ? > and since pi(i,j) = integral( R ) f(z) p( Z(i) = z ) dz we have
>>> ? > 
>>> ? > cov( Y(i,j), Y(i,j') ) =
>>> ? > integral( R ) f?(z) p( Z(i) = z ) dz -
>>> ? >  ( integral( R ) f(z) p( Z(i) = z ) dz )?
>>> ? > 
>>> ? > which in general has no reason to be nul, hence the two observations
>>> ? > are correlated. Is this correct?
>>> ? > 
>>> ? > Is there any way to have a closed-form of the covariance, for usual f
>>> ? > (let's say, logit or probit) and Z distribution (let's say, Gaussian)?
>>> ? > 
>>> ? > Thanks a lot for reading, and your answers,
>>> ? > 
>>> ? > -- 
>>> ? >                                Emmanuel CURIS
>>> ? >                                emmanuel.curis at parisdescartes.fr
>>> ? > 
>>> ? > Page WWW: http://emmanuel.curis.online.fr/index.html
>>> ? > 
>>> ? > _______________________________________________
>>> ? > R-sig-mixed-models at r-project.org mailing list
>>> ? > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>> ? 
>>> 
>>> -- 
>>>                              Emmanuel CURIS
>>>                              emmanuel.curis at parisdescartes.fr
>>> 
>>> Page WWW: http://emmanuel.curis.online.fr/index.html
>> 
> 


From |v@|d@ @end|ng |rom he@|th@uc@d@edu  Tue Apr 21 21:09:47 2020
From: |v@|d@ @end|ng |rom he@|th@uc@d@edu (Vaida, Florin)
Date: Tue, 21 Apr 2020 19:09:47 +0000
Subject: [R-sig-ME] 
 Precision about the glmer model for Bernoulli variables
In-Reply-To: <DDB70217-5D47-4DF2-8FF8-0748AD8D7754@anu.edu.au>
References: <20200420074851.GA6361@info124.pharmacie.univ-paris5.fr>
 <AB72DE88-C199-459C-B0EB-0DF13BB1659B@health.ucsd.edu>
 <20200420193249.GB16474@info124.pharmacie.univ-paris5.fr>
 <2AD9CAD5-FABC-4A13-B684-6D0908CFBF21@health.ucsd.edu>
 <5f78610081524e23855ff4bcccdf45a8@qimrberghofer.edu.au>
 <DDB70217-5D47-4DF2-8FF8-0748AD8D7754@anu.edu.au>
Message-ID: <E2FEE0F4-BB25-449F-B027-D28633BDCFDE@health.ucsd.edu>

Hi John,

Thanks for the Prentice reference.  However, I just want to point out that the extended beta-binomial is proposed there as a marginal distribution for the observations.  It could not arise from a mixed-effects model specification.

This is exactly analogous to the situation in the linear/normal case: A linear mixed-effects random intercept model reduces to a general linear model with constant, positive within-subject correlation.  Vice-versa, one can start with a marginal specification with constant within-subject correlation; this is possible even with negative correlations (within a certain range), but negative correlations do not correspond to a mixed-effects model specification.  (I thought Geert Molenberghs had a paper to this point but I can't find it now.)

Florin


On Apr 21, 2020, at 1:15 AM, John Maindonald <john.maindonald at anu.edu.au<mailto:john.maindonald at anu.edu.au>> wrote:

Apologies, the lower limit for the BB variance is pi(1-pi)rho.

Actually, with a suitable parameterization, the betabinomial allows
small negative correlations.  See
  Binary Regression Using an Extended Beta-Binomial Distribution, With Discussion of
 Correlation Induced by Covariate Measurement Errors
R. L. Prentice: Journal of the American Statistical Association
Vol. 81, No. 394 (Jun., 1986), pp. 321-327

A feature of the betabinomial (BB) , which ought to be better advertised than
it is in most accounts that I have seen, is that, for positive correlation rho,
it sets a strict lower limit of pi(1-pi)rho on the variance of the estimate
of the proportion pi.  This is in marked contrast to the variance
assumptions that define quasibinomial errors.

The gamlss package implements the double binomial (as well
as the logistic normal and BB), but as far as I can tell, not in a
multi-level model context.  The double binomial does allow a
negative correlation.

John Maindonald             email: john.maindonald at anu.edu.au<mailto:john.maindonald at anu.edu.au>


On 21/04/2020, at 19:18, David Duffy <David.Duffy at qimrberghofer.edu.au<mailto:David.Duffy at qimrberghofer.edu.au>> wrote:

You'd know the beta-binomial constrains the range of the correlation coefficient positive, but the logit-normal and probit-normal allow a negative correlation, with a lower bound constrained by the N, in the case of regular sized clusters. For your example of 2x2 tables, the correlation can go from -1 to 1. This is the usual genetics
type setup where you model the pairwise correlations for all pairs of observations in the dataset versus their
(pairwise) genetic relatedness.  In multi-trait models, the correlations between different phenotypes (say asthma and diabetes) in the same individual can be negative, so you can estimate negative variance components.

The pedigreemm package extends lme4 to allow glmms for pedigree data (see the mastitis example for a binary trait).

Cheers, David Duffy.
________________________________________
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org<mailto:r-sig-mixed-models-bounces at r-project.org>> on behalf of Vaida, Florin <fvaida at health.ucsd.edu<mailto:fvaida at health.ucsd.edu>>
Sent: Tuesday, 21 April 2020 7:05:40 AM
To: Emmanuel Curis
Cc: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME]  Precision about the glmer model for Bernoulli variables

Hi Emmanuel,

That's a good question.  My guess is that the correlation is non-negative generally, but I wasn't able to prove it theoretically even in the simplest case when Y1, Y2 ~ Bernoulli(u) independent conditionally on u, and u ~ Normal(0, 1).  I am curious if someone has a solution.
We can't go too far down this route in this forum, since Doug wants to keep it applied.

Florin

On Apr 20, 2020, at 12:32 PM, Emmanuel Curis <emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr>> wrote:

Hi Florin,

Thanks for the answer, the precision about p(i,j), and the reference.

A last question, that I forgot in my message: is the obtained
correlation also always positive, as in the linear case? Or may some
negative correlation appear, depending on the values of pi(i,j) and
pi(i,j')?

Best regards,

On Mon, Apr 20, 2020 at 03:27:39PM +0000, Vaida, Florin wrote:
? Hi Emmanuel,
?
? Your reasoning is correct.
?
? As a quibble, outside a simple repeated measures experiment setup, p(i,j) *does* depend on j.
? For example, if observations are collected over time, generally there is a time effect; if they are repeated measures with different experimental conditions, p(i,j) will depend on the condition j, etc.
?
? There is almost certainly no closed form solution for the covariance under logit.
? I am not sure about the probit (my guess is not).
? There will be some Laplace approximations available, a la Breslow and Clayton 1993.
?
? I'd be curious if these formulas/approximations were developed somewhere - I'd be surprised if they weren't.
?
? Florin
?
?
? > On Apr 20, 2020, at 12:48 AM, Emmanuel Curis <emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr>> wrote:
? >
? > Hello everyone,
? >
? > I hope you're all going fine in these difficult times.
? >
? > I tried to understand in details the exact model used when using glmer
? > for a Bernoulli experiment, by comparison with the linear mixed
? > effects model, and especially how it introducts correlations between
? > observations of a given group.  I think I finally got it, but could
? > you check that what I write below is correct and that I'm not missing
? > something?
? >
? > I use a very simple case with only a single random effect, and no
? > fixed effects, because I guess that adding fixed effects or other
? > random effects does not change the idea, it "just" makes formulas more
? > complex.  I note i the random effect level, let's say ? patient ?, and
? > j the observation for this patient.
? >
? > In the linear model, we have Y(i,j) = ?0 + Z(i) + epsilon( i, j ) with
? > Z(i) and epsilon(i,j) randoms variables having a density of
? > probability, independant, and each iid.
? >
? > Hence, cov( Y(i,j), Y(i,j') ) = Var( Z(i) ): the model introduces a
? > positive correlation between observations of the same patient.
? >
? >
? >
? > In the Bernoulli model, Y(i,j) ~ B( pi(i,j) ) and pi(i,j) = f( Z(i) ),
? > f being the inverse link function, typically the reciprocal of the
? > logit. So we have
? >
? > cov( Y(i,j), Y(i,j') ) = E( Y(i,j) Y(i, j') ) - E( Y(i,j) ) E( Y(i,j') )
? >     = Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) - pi(i,j) * pi(i,j')
? >
? > Since in practice pi(i,j) does not depend on j, pi(i,j) = pi(i,j').
? >
? > Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
? >  integral(R) Pr( Y(i,j) = 1 inter Y(i,j') = 1 | Z(i) = z ) p( Z(i) = z ) dz
? >
? > Then, we assume that conditionnally on Zi, the Yij are independant, is
? > this right? This is the equivalent of ? the epsilon(i, j) are
? > independant ?? I assume this hypothesis is also used for computing the
? > likelihood? If not, what is the model for the joint probability?
? >
? > In that case,
? >
? > Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
? >  integral(R) f(z) f(z) p( Z(i) = z ) dz
? >
? > and since pi(i,j) = integral( R ) f(z) p( Z(i) = z ) dz we have
? >
? > cov( Y(i,j), Y(i,j') ) =
? > integral( R ) f?(z) p( Z(i) = z ) dz -
? >  ( integral( R ) f(z) p( Z(i) = z ) dz )?
? >
? > which in general has no reason to be nul, hence the two observations
? > are correlated. Is this correct?
? >
? > Is there any way to have a closed-form of the covariance, for usual f
? > (let's say, logit or probit) and Z distribution (let's say, Gaussian)?
? >
? > Thanks a lot for reading, and your answers,
? >
? > --
? >                                Emmanuel CURIS
? >                                emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr>
? >
? > Page WWW: http://emmanuel.curis.online.fr/index.html
? >
? > _______________________________________________
? > R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
? > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
?

--
                              Emmanuel CURIS
                              emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr>

Page WWW: http://emmanuel.curis.online.fr/index.html

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
[EXTERNAL EMAIL] This message originates from an external email address, please exercise caution when clicking any links or opening attachments. If you believe the sender is impersonating someone at QIMR Berghofer, please forward this message to phishing at qimrberghofer.edu.au<mailto:phishing at qimrberghofer.edu.au>.

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



	[[alternative HTML version deleted]]


From emm@nue|@cur|@ @end|ng |rom p@r|@de@c@rte@@|r  Wed Apr 22 10:13:29 2020
From: emm@nue|@cur|@ @end|ng |rom p@r|@de@c@rte@@|r (Emmanuel Curis)
Date: Wed, 22 Apr 2020 10:13:29 +0200
Subject: [R-sig-ME] 
 Precision about the glmer model for Bernoulli variables
In-Reply-To: <E8150DEF-961A-447D-875B-F38E78E8C228@health.ucsd.edu>
References: <20200420074851.GA6361@info124.pharmacie.univ-paris5.fr>
 <AB72DE88-C199-459C-B0EB-0DF13BB1659B@health.ucsd.edu>
 <20200420193249.GB16474@info124.pharmacie.univ-paris5.fr>
 <2AD9CAD5-FABC-4A13-B684-6D0908CFBF21@health.ucsd.edu>
 <0E4F6A32-CB86-4C49-AEFE-A0BEE6E16CED@health.ucsd.edu>
 <E8150DEF-961A-447D-875B-F38E78E8C228@health.ucsd.edu>
Message-ID: <20200422081329.GB9808@info124.pharmacie.univ-paris5.fr>

Thanks for the demonstration, and the explainations, and the time
spent about that!  That clarifies the interpretation of the models a
lot...

Best regards,

On Tue, Apr 21, 2020 at 05:33:13PM +0000, Vaida, Florin wrote:
> I forgot to mention that this proof works for any link function, any conditional distribution for the response, and any random effects distribution.
> In other words, it's not restricted to logit link, nor to binomial data, nor to normal random effects:
> 
> If Yj ~ iid conditional on u, then Cov(Y_j, Y_k) >= 0, with equality only in certain restrictive conditions.
> 
> 
> > On Apr 21, 2020, at 10:25 AM, Vaida, Florin <fvaida at health.ucsd.edu> wrote:
> > 
> > Hi Emmanuel,
> > 
> > So I can prove positive within-subject correlation for GLME logistic regression with random intercepts - assuming all observations have same mean!
> > 
> > Let Yj ~ Bernoulli(mu), logit(mu) = beta + u, u ~ Normal(0, tau^2).
> > Using the conditional covariance formula you get
> > Cov(Y1, Y2) = E(Cov(Y1,Y2 | u) + Cov(E(Y1|u), E(Y2|u)) = 0 + Cov( mu(u), mu(u)) = Var(mu(u)) >= 0, with 0 only if tau^2 = 0.
> > 
> > This proof does not extend if you let Yj have different means, i.e., replace beta by beta_j.
> > It also does not apply to more general random effects structures, e.g. random intercepts and slopes.
> > Note however that for the *linear* model with random intercepts and slopes, the correlation is not guaranteed positive.
> > 
> > Florin
> > 
> > 
> >> On Apr 20, 2020, at 2:05 PM, Vaida, Florin <fvaida at health.ucsd.edu> wrote:
> >> 
> >> Hi Emmanuel,
> >> 
> >> That's a good question.  My guess is that the correlation is non-negative generally, but I wasn't able to prove it theoretically even in the simplest case when Y1, Y2 ~ Bernoulli(u) independent conditionally on u, and u ~ Normal(0, 1).  I am curious if someone has a solution.
> >> We can't go too far down this route in this forum, since Doug wants to keep it applied.
> >> 
> >> Florin
> >> 
> >>> On Apr 20, 2020, at 12:32 PM, Emmanuel Curis <emmanuel.curis at parisdescartes.fr> wrote:
> >>> 
> >>> Hi Florin,
> >>> 
> >>> Thanks for the answer, the precision about p(i,j), and the reference.
> >>> 
> >>> A last question, that I forgot in my message: is the obtained
> >>> correlation also always positive, as in the linear case? Or may some
> >>> negative correlation appear, depending on the values of pi(i,j) and
> >>> pi(i,j')?
> >>> 
> >>> Best regards,
> >>> 
> >>> On Mon, Apr 20, 2020 at 03:27:39PM +0000, Vaida, Florin wrote:
> >>> ? Hi Emmanuel,
> >>> ? 
> >>> ? Your reasoning is correct.
> >>> ? 
> >>> ? As a quibble, outside a simple repeated measures experiment setup, p(i,j) *does* depend on j.
> >>> ? For example, if observations are collected over time, generally there is a time effect; if they are repeated measures with different experimental conditions, p(i,j) will depend on the condition j, etc.
> >>> ? 
> >>> ? There is almost certainly no closed form solution for the covariance under logit.
> >>> ? I am not sure about the probit (my guess is not).
> >>> ? There will be some Laplace approximations available, a la Breslow and Clayton 1993.
> >>> ? 
> >>> ? I'd be curious if these formulas/approximations were developed somewhere - I'd be surprised if they weren't.
> >>> ? 
> >>> ? Florin
> >>> ? 
> >>> ? 
> >>> ? > On Apr 20, 2020, at 12:48 AM, Emmanuel Curis <emmanuel.curis at parisdescartes.fr> wrote:
> >>> ? > 
> >>> ? > Hello everyone,
> >>> ? > 
> >>> ? > I hope you're all going fine in these difficult times.
> >>> ? > 
> >>> ? > I tried to understand in details the exact model used when using glmer
> >>> ? > for a Bernoulli experiment, by comparison with the linear mixed
> >>> ? > effects model, and especially how it introducts correlations between
> >>> ? > observations of a given group.  I think I finally got it, but could
> >>> ? > you check that what I write below is correct and that I'm not missing
> >>> ? > something?
> >>> ? > 
> >>> ? > I use a very simple case with only a single random effect, and no
> >>> ? > fixed effects, because I guess that adding fixed effects or other
> >>> ? > random effects does not change the idea, it "just" makes formulas more
> >>> ? > complex.  I note i the random effect level, let's say ? patient ?, and
> >>> ? > j the observation for this patient.
> >>> ? > 
> >>> ? > In the linear model, we have Y(i,j) = ?0 + Z(i) + epsilon( i, j ) with
> >>> ? > Z(i) and epsilon(i,j) randoms variables having a density of
> >>> ? > probability, independant, and each iid.
> >>> ? > 
> >>> ? > Hence, cov( Y(i,j), Y(i,j') ) = Var( Z(i) ): the model introduces a
> >>> ? > positive correlation between observations of the same patient.
> >>> ? > 
> >>> ? > 
> >>> ? > 
> >>> ? > In the Bernoulli model, Y(i,j) ~ B( pi(i,j) ) and pi(i,j) = f( Z(i) ),
> >>> ? > f being the inverse link function, typically the reciprocal of the
> >>> ? > logit. So we have
> >>> ? > 
> >>> ? > cov( Y(i,j), Y(i,j') ) = E( Y(i,j) Y(i, j') ) - E( Y(i,j) ) E( Y(i,j') )
> >>> ? >     = Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) - pi(i,j) * pi(i,j')
> >>> ? > 
> >>> ? > Since in practice pi(i,j) does not depend on j, pi(i,j) = pi(i,j').
> >>> ? > 
> >>> ? > Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
> >>> ? >  integral(R) Pr( Y(i,j) = 1 inter Y(i,j') = 1 | Z(i) = z ) p( Z(i) = z ) dz
> >>> ? > 
> >>> ? > Then, we assume that conditionnally on Zi, the Yij are independant, is
> >>> ? > this right? This is the equivalent of ? the epsilon(i, j) are
> >>> ? > independant ?? I assume this hypothesis is also used for computing the
> >>> ? > likelihood? If not, what is the model for the joint probability?
> >>> ? > 
> >>> ? > In that case,
> >>> ? > 
> >>> ? > Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
> >>> ? >  integral(R) f(z) f(z) p( Z(i) = z ) dz
> >>> ? > 
> >>> ? > and since pi(i,j) = integral( R ) f(z) p( Z(i) = z ) dz we have
> >>> ? > 
> >>> ? > cov( Y(i,j), Y(i,j') ) =
> >>> ? > integral( R ) f?(z) p( Z(i) = z ) dz -
> >>> ? >  ( integral( R ) f(z) p( Z(i) = z ) dz )?
> >>> ? > 
> >>> ? > which in general has no reason to be nul, hence the two observations
> >>> ? > are correlated. Is this correct?
> >>> ? > 
> >>> ? > Is there any way to have a closed-form of the covariance, for usual f
> >>> ? > (let's say, logit or probit) and Z distribution (let's say, Gaussian)?
> >>> ? > 
> >>> ? > Thanks a lot for reading, and your answers,
> >>> ? > 
> >>> ? > -- 
> >>> ? >                                Emmanuel CURIS
> >>> ? >                                emmanuel.curis at parisdescartes.fr
> >>> ? > 
> >>> ? > Page WWW: http://emmanuel.curis.online.fr/index.html
> >>> ? > 
> >>> ? > _______________________________________________
> >>> ? > R-sig-mixed-models at r-project.org mailing list
> >>> ? > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>> ? 
> >>> 
> >>> -- 
> >>>                              Emmanuel CURIS
> >>>                              emmanuel.curis at parisdescartes.fr
> >>> 
> >>> Page WWW: http://emmanuel.curis.online.fr/index.html
> >> 
> > 
> 

-- 
                                Emmanuel CURIS
                                emmanuel.curis at parisdescartes.fr

Page WWW: http://emmanuel.curis.online.fr/index.html


From emm@nue|@cur|@ @end|ng |rom p@r|@de@c@rte@@|r  Wed Apr 22 10:31:22 2020
From: emm@nue|@cur|@ @end|ng |rom p@r|@de@c@rte@@|r (Emmanuel Curis)
Date: Wed, 22 Apr 2020 10:31:22 +0200
Subject: [R-sig-ME] 
 Precision about the glmer model for Bernoulli variables
In-Reply-To: <E3B67323-F100-4B13-B5D3-0017DE620B9D@anu.edu.au>
References: <20200420074851.GA6361@info124.pharmacie.univ-paris5.fr>
 <AB72DE88-C199-459C-B0EB-0DF13BB1659B@health.ucsd.edu>
 <20200420193249.GB16474@info124.pharmacie.univ-paris5.fr>
 <2AD9CAD5-FABC-4A13-B684-6D0908CFBF21@health.ucsd.edu>
 <5f78610081524e23855ff4bcccdf45a8@qimrberghofer.edu.au>
 <55E960BE-5BC8-4BB7-B30C-0AF922C7D73C@anu.edu.au>
 <20200421091921.GC9918@info124.pharmacie.univ-paris5.fr>
 <E3B67323-F100-4B13-B5D3-0017DE620B9D@anu.edu.au>
Message-ID: <20200422083122.GD9808@info124.pharmacie.univ-paris5.fr>

Thanks for the notes, they are very rich! I'll read that quietly...

Best regards,

On Tue, Apr 21, 2020 at 09:58:34AM +0000, John Maindonald wrote:
> As I recall (but check what I say), the double binomial results from
> modeling separately the ?successes? and ?failures?, with exponential
> families somehow involved.  See the Efron reference on the help
> page ?gamlss.dist::DBI
> 
> There?s a comparison between fits that uses the betabonimial vs use
> of the double binomial in pp.119-123 of the notes that you can find at:
>     https://www.dropbox.com/s/xnug9whkayd3mep/mrmar2020.pdf?dl=0
> or https://bit.ly/2VP2zI4
> 
> I?ve been intending to post these notes, and supporting materials,
> for whoever wants to use, but have been diverted onto other things.
> 
> 
> John Maindonald             email: john.maindonald at anu.edu.au<mailto:john.maindonald at anu.edu.au>
> 
> On 21/04/2020, at 21:19, Emmanuel Curis <emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr>> wrote:
> 
> Thanks for all this details about correlation.
> 
> Just to be clear about the nomenclature: when a model is called
> ? A-B ?, A is for the cumulative distribution function used as link
> between pi and the linear predictor including the random effect(s) or
> its reciprocal, and B is for the distribution of random effect(s), is
> this right?
> 
> If this is right, for the ? double binomial ?, it would mean that the
> link function is a step function, and not a monotonous strictly
> increasing function. Since it would then not be a bijection between R
> and [0,1], doesn't it introduce indetermination and strong constraints
> on the possible values of pi? I guess I'm wrongly interpreting what
> means ? double binomial ?.
> 
> Sorry for this naive questions, I'm discovering the field...
> 
> Best regards,
> 
> 
> On Tue, Apr 21, 2020 at 07:59:48AM +0000, John Maindonald wrote:
> Actually, with a suitable parameterization, the betabinomial allows
> small negative correlations.  See
>  Binary Regression Using an Extended Beta-Binomial Distribution, With Discussion of
> Correlation Induced by Covariate Measurement Errors
> R. L. Prentice: Journal of the American Statistical Association
> Vol. 81, No. 394 (Jun., 1986), pp. 321-327
> 
> A feature of the betabinomial (BB) , which ought to be better advertised than
> it is in most accounts that I have seen, is that, for positive correlation rho,
> it sets a strict lower limit of pi(1-pi)(1+rho) on the variance of the estimate
> of the proportion pi.  This is in marked contrast to the variance
> assumptions that define quasibinomial errors.
> 
> The gamlss package implements the double binomial (as well
> as the logistic normal and BB), but as far as I can tell, not in a
> multi-level model context.  The double binomial does allow a
> negative correlation.
> 
> 
> John Maindonald             email: john.maindonald at anu.edu.au<mailto:john.maindonald at anu.edu.au><mailto:john.maindonald at anu.edu.au>
> 
> 
> On 21/04/2020, at 19:18, David Duffy <David.Duffy at qimrberghofer.edu.au<mailto:David.Duffy at qimrberghofer.edu.au><mailto:David.Duffy at qimrberghofer.edu.au>> wrote:
> 
> You'd know the beta-binomial constrains the range of the correlation coefficient positive, but the logit-normal and probit-normal allow a negative correlation, with a lower bound constrained by the N, in the case of regular sized clusters. For your example of 2x2 tables, the correlation can go from -1 to 1. This is the usual genetics
> type setup where you model the pairwise correlations for all pairs of observations in the dataset versus their
> (pairwise) genetic relatedness.  In multi-trait models, the correlations between different phenotypes (say asthma and diabetes) in the same individual can be negative, so you can estimate negative variance components.
> 
> The pedigreemm package extends lme4 to allow glmms for pedigree data (see the mastitis example for a binary trait).
> 
> Cheers, David Duffy.
> ________________________________________
> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org<mailto:r-sig-mixed-models-bounces at r-project.org><mailto:r-sig-mixed-models-bounces at r-project.org>> on behalf of Vaida, Florin <fvaida at health.ucsd.edu<mailto:fvaida at health.ucsd.edu><mailto:fvaida at health.ucsd.edu>>
> Sent: Tuesday, 21 April 2020 7:05:40 AM
> To: Emmanuel Curis
> Cc: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org><mailto:r-sig-mixed-models at r-project.org>
> Subject: Re: [R-sig-ME]  Precision about the glmer model for Bernoulli variables
> 
> Hi Emmanuel,
> 
> That's a good question.  My guess is that the correlation is non-negative generally, but I wasn't able to prove it theoretically even in the simplest case when Y1, Y2 ~ Bernoulli(u) independent conditionally on u, and u ~ Normal(0, 1).  I am curious if someone has a solution.
> We can't go too far down this route in this forum, since Doug wants to keep it applied.
> 
> Florin
> 
> On Apr 20, 2020, at 12:32 PM, Emmanuel Curis <emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr><mailto:emmanuel.curis at parisdescartes.fr>> wrote:
> 
> Hi Florin,
> 
> Thanks for the answer, the precision about p(i,j), and the reference.
> 
> A last question, that I forgot in my message: is the obtained
> correlation also always positive, as in the linear case? Or may some
> negative correlation appear, depending on the values of pi(i,j) and
> pi(i,j')?
> 
> Best regards,
> 
> On Mon, Apr 20, 2020 at 03:27:39PM +0000, Vaida, Florin wrote:
> ? Hi Emmanuel,
> ?
> ? Your reasoning is correct.
> ?
> ? As a quibble, outside a simple repeated measures experiment setup, p(i,j) *does* depend on j.
> ? For example, if observations are collected over time, generally there is a time effect; if they are repeated measures with different experimental conditions, p(i,j) will depend on the condition j, etc.
> ?
> ? There is almost certainly no closed form solution for the covariance under logit.
> ? I am not sure about the probit (my guess is not).
> ? There will be some Laplace approximations available, a la Breslow and Clayton 1993.
> ?
> ? I'd be curious if these formulas/approximations were developed somewhere - I'd be surprised if they weren't.
> ?
> ? Florin
> ?
> ?
> ? > On Apr 20, 2020, at 12:48 AM, Emmanuel Curis <emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr><mailto:emmanuel.curis at parisdescartes.fr>> wrote:
> ? >
> ? > Hello everyone,
> ? >
> ? > I hope you're all going fine in these difficult times.
> ? >
> ? > I tried to understand in details the exact model used when using glmer
> ? > for a Bernoulli experiment, by comparison with the linear mixed
> ? > effects model, and especially how it introducts correlations between
> ? > observations of a given group.  I think I finally got it, but could
> ? > you check that what I write below is correct and that I'm not missing
> ? > something?
> ? >
> ? > I use a very simple case with only a single random effect, and no
> ? > fixed effects, because I guess that adding fixed effects or other
> ? > random effects does not change the idea, it "just" makes formulas more
> ? > complex.  I note i the random effect level, let's say ? patient ?, and
> ? > j the observation for this patient.
> ? >
> ? > In the linear model, we have Y(i,j) = ?0 + Z(i) + epsilon( i, j ) with
> ? > Z(i) and epsilon(i,j) randoms variables having a density of
> ? > probability, independant, and each iid.
> ? >
> ? > Hence, cov( Y(i,j), Y(i,j') ) = Var( Z(i) ): the model introduces a
> ? > positive correlation between observations of the same patient.
> ? >
> ? >
> ? >
> ? > In the Bernoulli model, Y(i,j) ~ B( pi(i,j) ) and pi(i,j) = f( Z(i) ),
> ? > f being the inverse link function, typically the reciprocal of the
> ? > logit. So we have
> ? >
> ? > cov( Y(i,j), Y(i,j') ) = E( Y(i,j) Y(i, j') ) - E( Y(i,j) ) E( Y(i,j') )
> ? >     = Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) - pi(i,j) * pi(i,j')
> ? >
> ? > Since in practice pi(i,j) does not depend on j, pi(i,j) = pi(i,j').
> ? >
> ? > Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
> ? >  integral(R) Pr( Y(i,j) = 1 inter Y(i,j') = 1 | Z(i) = z ) p( Z(i) = z ) dz
> ? >
> ? > Then, we assume that conditionnally on Zi, the Yij are independant, is
> ? > this right? This is the equivalent of ? the epsilon(i, j) are
> ? > independant ?? I assume this hypothesis is also used for computing the
> ? > likelihood? If not, what is the model for the joint probability?
> ? >
> ? > In that case,
> ? >
> ? > Pr( Y(i,j) = 1 inter Y(i,j') = 1 ) =
> ? >  integral(R) f(z) f(z) p( Z(i) = z ) dz
> ? >
> ? > and since pi(i,j) = integral( R ) f(z) p( Z(i) = z ) dz we have
> ? >
> ? > cov( Y(i,j), Y(i,j') ) =
> ? > integral( R ) f?(z) p( Z(i) = z ) dz -
> ? >  ( integral( R ) f(z) p( Z(i) = z ) dz )?
> ? >
> ? > which in general has no reason to be nul, hence the two observations
> ? > are correlated. Is this correct?
> ? >
> ? > Is there any way to have a closed-form of the covariance, for usual f
> ? > (let's say, logit or probit) and Z distribution (let's say, Gaussian)?
> ? >
> ? > Thanks a lot for reading, and your answers,
> ? >
> ? > --
> ? >                                Emmanuel CURIS
> ? >                                emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr><mailto:emmanuel.curis at parisdescartes.fr>
> ? >
> ? > Page WWW: http://emmanuel.curis.online.fr/index.html
> ? >
> ? > _______________________________________________
> ? > R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org><mailto:R-sig-mixed-models at r-project.org> mailing list
> ? > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> ?
> 
> --
>                              Emmanuel CURIS
>                              emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr><mailto:emmanuel.curis at parisdescartes.fr>
> 
> Page WWW: http://emmanuel.curis.online.fr/index.html
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org><mailto:R-sig-mixed-models at r-project.org> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> [EXTERNAL EMAIL] This message originates from an external email address, please exercise caution when clicking any links or opening attachments. If you believe the sender is impersonating someone at QIMR Berghofer, please forward this message to phishing at qimrberghofer.edu.au.
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> --
>                                Emmanuel CURIS
>                                emmanuel.curis at parisdescartes.fr<mailto:emmanuel.curis at parisdescartes.fr>
> 
> Page WWW: http://emmanuel.curis.online.fr/index.html
> 

-- 
                                Emmanuel CURIS
                                emmanuel.curis at parisdescartes.fr

Page WWW: http://emmanuel.curis.online.fr/index.html


From thom@@merk||ng00 @end|ng |rom gm@||@com  Wed Apr 22 11:44:01 2020
From: thom@@merk||ng00 @end|ng |rom gm@||@com (Thomas Merkling)
Date: Wed, 22 Apr 2020 11:44:01 +0200
Subject: [R-sig-ME] spatial auto-correlation or more complicated
 pseudo-replication?
Message-ID: <3b3cdef0-ec91-7fcf-29ea-c33be45bedbd@gmail.com>

Hi all,

I'm wondering how to best model data from an experimental design 
involving a spatial component. This is a study on seabirds nesting on 
artificial cliffs: each nest has been attributed an experimental 
treatment (supplemented or not), while making sure that there was a 
variable proportion of surrounding nests of the opposite treatment. Our 
main goal was to investigate if laying date of a focal pair was 
influenced by its treatment and/or by the proportion of surrounding 
nests of the opposite treatment (hereafter, "Prop"), which we calculated 
at 3 different spatial scale (local, panel and global, see 
https://drive.google.com/open?id=1OrJQCkNfBO6KOBHSlkOoQyAdTrqtIdY8 for a 
visual representation).

Hence, the treatment information of a focal pair is used in the 
"Treatment" predictor variable, but also in the calculation of "Prop" 
for the surrounding pairs (the number of pairs affected depending on the 
spatial scale considered), thereby leading to some pseudo-replication.? 
Since this is dependent on the distance (i.e. "Prop" of pairs closer to 
a focal one are more influenced than pairs further away), we thought 
that accounting for spatial auto-correlation for be sufficient. We used 
the spaMM package to do so, and our models look something like:

Laying ~ Treatment * Prop + Year + (1|PairID) + Matern(Y2011|x + y) + 
Matern(Y2012|x + y)

with two Mat?rn correlation random effects (one for each year of the 
study) being included (x and y being the spatial coordinates of the nests).

My question is: Is this random effect structure taking into account the 
fact that "Prop" of a focal pair depends on the "Treatment" of the 
surrounding pairs or not ? If not, how can we account for that?

Thanks in advance for your help!
Thomas


	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Wed Apr 22 13:57:40 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Wed, 22 Apr 2020 13:57:40 +0200
Subject: [R-sig-ME] spatial auto-correlation or more complicated
 pseudo-replication?
In-Reply-To: <3b3cdef0-ec91-7fcf-29ea-c33be45bedbd@gmail.com>
References: <3b3cdef0-ec91-7fcf-29ea-c33be45bedbd@gmail.com>
Message-ID: <CAJuCY5yHdyUGr4W14+RJYddLCNxmjXs=F+c=YGKUFwhs=k0J2Q@mail.gmail.com>

Dear Thomas,

Do you have information on all the nests or only on a sample of the nests?
In case you have data on every nest, then I would look at a simple model
with only treatment and an iid nest effect. Then see if there is spatial
autocorrelation. Variation at small ranges would indicate an effect of the
treatment of the neighbouring nests.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op wo 22 apr. 2020 om 11:44 schreef Thomas Merkling <
thomasmerkling00 at gmail.com>:

> Hi all,
>
> I'm wondering how to best model data from an experimental design
> involving a spatial component. This is a study on seabirds nesting on
> artificial cliffs: each nest has been attributed an experimental
> treatment (supplemented or not), while making sure that there was a
> variable proportion of surrounding nests of the opposite treatment. Our
> main goal was to investigate if laying date of a focal pair was
> influenced by its treatment and/or by the proportion of surrounding
> nests of the opposite treatment (hereafter, "Prop"), which we calculated
> at 3 different spatial scale (local, panel and global, see
> https://drive.google.com/open?id=1OrJQCkNfBO6KOBHSlkOoQyAdTrqtIdY8 for a
> visual representation).
>
> Hence, the treatment information of a focal pair is used in the
> "Treatment" predictor variable, but also in the calculation of "Prop"
> for the surrounding pairs (the number of pairs affected depending on the
> spatial scale considered), thereby leading to some pseudo-replication.
> Since this is dependent on the distance (i.e. "Prop" of pairs closer to
> a focal one are more influenced than pairs further away), we thought
> that accounting for spatial auto-correlation for be sufficient. We used
> the spaMM package to do so, and our models look something like:
>
> Laying ~ Treatment * Prop + Year + (1|PairID) + Matern(Y2011|x + y) +
> Matern(Y2012|x + y)
>
> with two Mat?rn correlation random effects (one for each year of the
> study) being included (x and y being the spatial coordinates of the nests).
>
> My question is: Is this random effect structure taking into account the
> fact that "Prop" of a focal pair depends on the "Treatment" of the
> surrounding pairs or not ? If not, how can we account for that?
>
> Thanks in advance for your help!
> Thomas
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From thom@@merk||ng00 @end|ng |rom gm@||@com  Wed Apr 22 14:58:24 2020
From: thom@@merk||ng00 @end|ng |rom gm@||@com (Thomas Merkling)
Date: Wed, 22 Apr 2020 14:58:24 +0200
Subject: [R-sig-ME] spatial auto-correlation or more complicated
 pseudo-replication?
In-Reply-To: <CAJuCY5yHdyUGr4W14+RJYddLCNxmjXs=F+c=YGKUFwhs=k0J2Q@mail.gmail.com>
References: <3b3cdef0-ec91-7fcf-29ea-c33be45bedbd@gmail.com>
 <CAJuCY5yHdyUGr4W14+RJYddLCNxmjXs=F+c=YGKUFwhs=k0J2Q@mail.gmail.com>
Message-ID: <8c4cab12-c8c7-588c-d590-8988df25b7f4@gmail.com>

Dear Thierry,

Thanks for reply. We used a sample of the population for our experiment, 
but for this sample we have information (treatment and Prop variable at 
each scale for all the nests.
How would you suggest to test/check is there is spatial autocorrelation?
I tried with the DHARMa package (which makes a Moran's I test adapted to 
mixed models), but it doesn't show if autocorrelation changes with 
distance, it just gives a p-value. I tried with a model with PairID as 
random effect (p = 0.33), but if I include nest as a random effect (some 
pairs changed in between the 2 years of the experiment, so there are 
less Nest IDs than Pair IDs) the p-value becomes 0.054 ...

Kind regards,
Thomas

On 22/04/2020 13:57, Thierry Onkelinx wrote:
> Dear Thomas,
>
> Do you have information on all the nests or only on a sample of the 
> nests? In case you have data on every nest, then I would look at a 
> simple model with only treatment?and an iid nest effect. Then see if 
> there is spatial autocorrelation. Variation at small ranges would 
> indicate an effect of the treatment of the neighbouring nests.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE 
> AND FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be <http://www.inbo.be>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no 
> more than asking him to perform a post-mortem examination: he may be 
> able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does 
> not ensure that a reasonable answer can be extracted from a given body 
> of data. ~ John Tukey
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op wo 22 apr. 2020 om 11:44 schreef Thomas Merkling 
> <thomasmerkling00 at gmail.com <mailto:thomasmerkling00 at gmail.com>>:
>
>     Hi all,
>
>     I'm wondering how to best model data from an experimental design
>     involving a spatial component. This is a study on seabirds nesting on
>     artificial cliffs: each nest has been attributed an experimental
>     treatment (supplemented or not), while making sure that there was a
>     variable proportion of surrounding nests of the opposite
>     treatment. Our
>     main goal was to investigate if laying date of a focal pair was
>     influenced by its treatment and/or by the proportion of surrounding
>     nests of the opposite treatment (hereafter, "Prop"), which we
>     calculated
>     at 3 different spatial scale (local, panel and global, see
>     https://drive.google.com/open?id=1OrJQCkNfBO6KOBHSlkOoQyAdTrqtIdY8
>     for a
>     visual representation).
>
>     Hence, the treatment information of a focal pair is used in the
>     "Treatment" predictor variable, but also in the calculation of "Prop"
>     for the surrounding pairs (the number of pairs affected depending
>     on the
>     spatial scale considered), thereby leading to some
>     pseudo-replication.
>     Since this is dependent on the distance (i.e. "Prop" of pairs
>     closer to
>     a focal one are more influenced than pairs further away), we thought
>     that accounting for spatial auto-correlation for be sufficient. We
>     used
>     the spaMM package to do so, and our models look something like:
>
>     Laying ~ Treatment * Prop + Year + (1|PairID) + Matern(Y2011|x + y) +
>     Matern(Y2012|x + y)
>
>     with two Mat?rn correlation random effects (one for each year of the
>     study) being included (x and y being the spatial coordinates of
>     the nests).
>
>     My question is: Is this random effect structure taking into
>     account the
>     fact that "Prop" of a focal pair depends on the "Treatment" of the
>     surrounding pairs or not ? If not, how can we account for that?
>
>     Thanks in advance for your help!
>     Thomas
>
>
>     ? ? ? ? [[alternative HTML version deleted]]
>
>     _______________________________________________
>     R-sig-mixed-models at r-project.org
>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>     https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From m@|re@k|rk|@nd @end|ng |rom durh@m@@c@uk  Wed Apr 22 15:48:48 2020
From: m@|re@k|rk|@nd @end|ng |rom durh@m@@c@uk (KIRKLAND, MAIRE E.)
Date: Wed, 22 Apr 2020 13:48:48 +0000
Subject: [R-sig-ME] Spatial structure in hurdle glmmTMB
Message-ID: <480CD75C-1B8F-4A1B-A247-929927698564@durham.ac.uk>

Hi all,

I am trying to use the glmmTMB package (version 1.1.0) to run a hurdle model that includes a spatial covariance structure. However, when I try to include spatial information in the zero-inflated part of the model I get an error. 

Here is a reproducible example with the error message that I receive: 

library(glmmTMB)

d <- data.frame(count = as.vector(Salamanders$count),
                mined = as.vector(Salamanders$mined),
                x = as.vector(row(Salamanders)),
                y = as.vector(col(Salamanders)))

d$pos <- numFactor(scale(d$x), scale(d$y))
d$ID <- factor(rep(1, nrow(d)))

m <- glmmTMB(count ~ mined + mat(pos + 0 | ID), 
                zi = ~., 
                family = truncated_poisson, data = d
 )

"Error in Reduce(addForm0, list(...)) : object 'mined' not found? 

Is there a way of including a spatial structure in both parts of the hurdle model?

Any help would be much appreciated!

M?ire

From mo|||eebrook@ @end|ng |rom gm@||@com  Wed Apr 22 16:15:22 2020
From: mo|||eebrook@ @end|ng |rom gm@||@com (Mollie Brooks)
Date: Wed, 22 Apr 2020 16:15:22 +0200
Subject: [R-sig-ME] Spatial structure in hurdle glmmTMB
In-Reply-To: <480CD75C-1B8F-4A1B-A247-929927698564@durham.ac.uk>
References: <480CD75C-1B8F-4A1B-A247-929927698564@durham.ac.uk>
Message-ID: <E76CE522-362E-4F22-99C3-B7D46972406D@gmail.com>

Hi M?ire, 

It sounds like you found a bug. Thank you for the easily repeatable example. 

Would you like to file an issue on GutHub, or should I?
https://github.com/glmmTMB/glmmTMB/issues

cheers,
Mollie

> On 22Apr 2020, at 15:48, KIRKLAND, MAIRE E. <maire.kirkland at durham.ac.uk> wrote:
> 
> Hi all,
> 
> I am trying to use the glmmTMB package (version 1.1.0) to run a hurdle model that includes a spatial covariance structure. However, when I try to include spatial information in the zero-inflated part of the model I get an error. 
> 
> Here is a reproducible example with the error message that I receive: 
> 
> library(glmmTMB)
> 
> d <- data.frame(count = as.vector(Salamanders$count),
>                mined = as.vector(Salamanders$mined),
>                x = as.vector(row(Salamanders)),
>                y = as.vector(col(Salamanders)))
> 
> d$pos <- numFactor(scale(d$x), scale(d$y))
> d$ID <- factor(rep(1, nrow(d)))
> 
> m <- glmmTMB(count ~ mined + mat(pos + 0 | ID), 
>                zi = ~., 
>                family = truncated_poisson, data = d
> )
> 
> "Error in Reduce(addForm0, list(...)) : object 'mined' not found? 
> 
> Is there a way of including a spatial structure in both parts of the hurdle model?
> 
> Any help would be much appreciated!
> 
> M?ire
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From th|erry@onke||nx @end|ng |rom |nbo@be  Wed Apr 22 16:44:27 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Wed, 22 Apr 2020 16:44:27 +0200
Subject: [R-sig-ME] spatial auto-correlation or more complicated
 pseudo-replication?
In-Reply-To: <8c4cab12-c8c7-588c-d590-8988df25b7f4@gmail.com>
References: <3b3cdef0-ec91-7fcf-29ea-c33be45bedbd@gmail.com>
 <CAJuCY5yHdyUGr4W14+RJYddLCNxmjXs=F+c=YGKUFwhs=k0J2Q@mail.gmail.com>
 <8c4cab12-c8c7-588c-d590-8988df25b7f4@gmail.com>
Message-ID: <CAJuCY5xzvRv4BsVirmyQAiF7KLAsxmQd-3XYOF833Sn_ss8X3Q@mail.gmail.com>

Dear Thomas,

Extract the residuals from the model. Then use gstat::variogram() to
calculate the empirical variogram of the residuals.  If there is spatial
autocorrelation, you'll see an increase in the variance as the distance
between observations increases.

I would expect that the birds have a stronger effect than the nests. Hence
I'd use Pair ID. If the dataset would span more than 2 years you could try
both a Pair and Nest random effect.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op wo 22 apr. 2020 om 14:58 schreef Thomas Merkling <
thomasmerkling00 at gmail.com>:

> Dear Thierry,
>
> Thanks for reply. We used a sample of the population for our experiment,
> but for this sample we have information (treatment and Prop variable at
> each scale for all the nests.
> How would you suggest to test/check is there is spatial autocorrelation?
> I tried with the DHARMa package (which makes a Moran's I test adapted to
> mixed models), but it doesn't show if autocorrelation changes with
> distance, it just gives a p-value. I tried with a model with PairID as
> random effect (p = 0.33), but if I include nest as a random effect (some
> pairs changed in between the 2 years of the experiment, so there are less
> Nest IDs than Pair IDs) the p-value becomes 0.054 ...
>
> Kind regards,
> Thomas
> On 22/04/2020 13:57, Thierry Onkelinx wrote:
>
> Dear Thomas,
>
> Do you have information on all the nests or only on a sample of the nests?
> In case you have data on every nest, then I would look at a simple model
> with only treatment and an iid nest effect. Then see if there is spatial
> autocorrelation. Variation at small ranges would indicate an effect of the
> treatment of the neighbouring nests.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op wo 22 apr. 2020 om 11:44 schreef Thomas Merkling <
> thomasmerkling00 at gmail.com>:
>
>> Hi all,
>>
>> I'm wondering how to best model data from an experimental design
>> involving a spatial component. This is a study on seabirds nesting on
>> artificial cliffs: each nest has been attributed an experimental
>> treatment (supplemented or not), while making sure that there was a
>> variable proportion of surrounding nests of the opposite treatment. Our
>> main goal was to investigate if laying date of a focal pair was
>> influenced by its treatment and/or by the proportion of surrounding
>> nests of the opposite treatment (hereafter, "Prop"), which we calculated
>> at 3 different spatial scale (local, panel and global, see
>> https://drive.google.com/open?id=1OrJQCkNfBO6KOBHSlkOoQyAdTrqtIdY8 for a
>> visual representation).
>>
>> Hence, the treatment information of a focal pair is used in the
>> "Treatment" predictor variable, but also in the calculation of "Prop"
>> for the surrounding pairs (the number of pairs affected depending on the
>> spatial scale considered), thereby leading to some pseudo-replication.
>> Since this is dependent on the distance (i.e. "Prop" of pairs closer to
>> a focal one are more influenced than pairs further away), we thought
>> that accounting for spatial auto-correlation for be sufficient. We used
>> the spaMM package to do so, and our models look something like:
>>
>> Laying ~ Treatment * Prop + Year + (1|PairID) + Matern(Y2011|x + y) +
>> Matern(Y2012|x + y)
>>
>> with two Mat?rn correlation random effects (one for each year of the
>> study) being included (x and y being the spatial coordinates of the
>> nests).
>>
>> My question is: Is this random effect structure taking into account the
>> fact that "Prop" of a focal pair depends on the "Treatment" of the
>> surrounding pairs or not ? If not, how can we account for that?
>>
>> Thanks in advance for your help!
>> Thomas
>>
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From m@|re@k|rk|@nd @end|ng |rom durh@m@@c@uk  Wed Apr 22 17:51:56 2020
From: m@|re@k|rk|@nd @end|ng |rom durh@m@@c@uk (KIRKLAND, MAIRE E.)
Date: Wed, 22 Apr 2020 15:51:56 +0000
Subject: [R-sig-ME] Spatial structure in hurdle glmmTMB
In-Reply-To: <E76CE522-362E-4F22-99C3-B7D46972406D@gmail.com>
References: <480CD75C-1B8F-4A1B-A247-929927698564@durham.ac.uk>
 <E76CE522-362E-4F22-99C3-B7D46972406D@gmail.com>
Message-ID: <6E0BA1D2-C45D-4FEF-B4FF-51038A65AFEA@durham.ac.uk>

Hi Mollie,

Thanks so much for your reply. I have filed it as an issue on GitHub.

Best wishes, 

M?ire


> On 22 Apr 2020, at 15:15, Mollie Brooks <mollieebrooks at gmail.com> wrote:
> 
> Hi M?ire, 
> 
> It sounds like you found a bug. Thank you for the easily repeatable example. 
> 
> Would you like to file an issue on GutHub, or should I?
> https://github.com/glmmTMB/glmmTMB/issues
> 
> cheers,
> Mollie
> 
>> On 22Apr 2020, at 15:48, KIRKLAND, MAIRE E. <maire.kirkland at durham.ac.uk> wrote:
>> 
>> Hi all,
>> 
>> I am trying to use the glmmTMB package (version 1.1.0) to run a hurdle model that includes a spatial covariance structure. However, when I try to include spatial information in the zero-inflated part of the model I get an error. 
>> 
>> Here is a reproducible example with the error message that I receive: 
>> 
>> library(glmmTMB)
>> 
>> d <- data.frame(count = as.vector(Salamanders$count),
>>               mined = as.vector(Salamanders$mined),
>>               x = as.vector(row(Salamanders)),
>>               y = as.vector(col(Salamanders)))
>> 
>> d$pos <- numFactor(scale(d$x), scale(d$y))
>> d$ID <- factor(rep(1, nrow(d)))
>> 
>> m <- glmmTMB(count ~ mined + mat(pos + 0 | ID), 
>>               zi = ~., 
>>               family = truncated_poisson, data = d
>> )
>> 
>> "Error in Reduce(addForm0, list(...)) : object 'mined' not found? 
>> 
>> Is there a way of including a spatial structure in both parts of the hurdle model?
>> 
>> Any help would be much appreciated!
>> 
>> M?ire
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 


From thom@@merk||ng00 @end|ng |rom gm@||@com  Wed Apr 22 18:19:09 2020
From: thom@@merk||ng00 @end|ng |rom gm@||@com (Thomas Merkling)
Date: Wed, 22 Apr 2020 18:19:09 +0200
Subject: [R-sig-ME] spatial auto-correlation or more complicated
 pseudo-replication?
In-Reply-To: <CAJuCY5xzvRv4BsVirmyQAiF7KLAsxmQd-3XYOF833Sn_ss8X3Q@mail.gmail.com>
References: <3b3cdef0-ec91-7fcf-29ea-c33be45bedbd@gmail.com>
 <CAJuCY5yHdyUGr4W14+RJYddLCNxmjXs=F+c=YGKUFwhs=k0J2Q@mail.gmail.com>
 <8c4cab12-c8c7-588c-d590-8988df25b7f4@gmail.com>
 <CAJuCY5xzvRv4BsVirmyQAiF7KLAsxmQd-3XYOF833Sn_ss8X3Q@mail.gmail.com>
Message-ID: <57173e82-cab3-2371-1815-f61db29b2e50@gmail.com>

Dear Thierry,

Thanks for your answer.
Below is the piece of code I ran:
mod <- lmer(Laydate ~ Treatment + Year + (1|PairID), REML= FALSE, data = 
CRlF)
CRlF$resmod <- residuals(mod, type = "pearson")
plot(gstat::variogram(resmod ~ 1, loc = x+y, data = CRlF))

It seems like the variance is quite stable for distances up to 25 and 
then drops a bit. I did the same analysis with another response variable 
(egg weight) and got a similar pattern. (links to plot for laydate 
<https://drive.google.com/open?id=1T2n41DeSh0BlkZVX5t-E1IKdMuudujBy> and 
for eggweight 
<https://drive.google.com/open?id=10_ou4yQQkF-kVVnf7zbgHyMmUEABvnPU>)
So does it mean that there is no spatial auto-correlation then?

This would match the fact that our results don't change much if we add 
the Matern correlation random effects or not.
A reviewer suggested that spatial-autocorrelation isn't sufficient to 
account for the pseudo-replication in our data, and that we still have 
an issue of inflation of the degrees of freedom and suggested 
permutation tests to account for that, but is that really necessary?

Kind regards,
Thomas

On 22/04/2020 16:44, Thierry Onkelinx wrote:
> Dear Thomas,
>
> Extract the residuals from the model. Then use gstat::variogram() to 
> calculate the empirical variogram of the residuals.? If there is 
> spatial autocorrelation, you'll see an increase in the variance as the 
> distance between observations increases.
>
> I would expect that the birds have a stronger effect than the nests. 
> Hence I'd use Pair ID. If the dataset would span more than 2 years you 
> could try both a Pair and Nest random effect.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE 
> AND FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be <http://www.inbo.be>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no 
> more than asking him to perform a post-mortem examination: he may be 
> able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does 
> not ensure that a reasonable answer can be extracted from a given body 
> of data. ~ John Tukey
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op wo 22 apr. 2020 om 14:58 schreef Thomas Merkling 
> <thomasmerkling00 at gmail.com <mailto:thomasmerkling00 at gmail.com>>:
>
>     Dear Thierry,
>
>     Thanks for reply. We used a sample of the population for our
>     experiment, but for this sample we have information (treatment and
>     Prop variable at each scale for all the nests.
>     How would you suggest to test/check is there is spatial
>     autocorrelation?
>     I tried with the DHARMa package (which makes a Moran's I test
>     adapted to mixed models), but it doesn't show if autocorrelation
>     changes with distance, it just gives a p-value. I tried with a
>     model with PairID as random effect (p = 0.33), but if I include
>     nest as a random effect (some pairs changed in between the 2 years
>     of the experiment, so there are less Nest IDs than Pair IDs) the
>     p-value becomes 0.054 ...
>
>     Kind regards,
>     Thomas
>
>     On 22/04/2020 13:57, Thierry Onkelinx wrote:
>>     Dear Thomas,
>>
>>     Do you have information on all the nests or only on a sample of
>>     the nests? In case you have data on every nest, then I would look
>>     at a simple model with only treatment?and an iid nest effect.
>>     Then see if there is spatial autocorrelation. Variation at small
>>     ranges would indicate an effect of the treatment of the
>>     neighbouring nests.
>>
>>     Best regards,
>>
>>     ir. Thierry Onkelinx
>>     Statisticus / Statistician
>>
>>     Vlaamse Overheid / Government of Flanders
>>     INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR
>>     NATURE AND FOREST
>>     Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality
>>     Assurance
>>     thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>
>>     Havenlaan 88 bus 73, 1000 Brussel
>>     www.inbo.be <http://www.inbo.be>
>>
>>     ///////////////////////////////////////////////////////////////////////////////////////////
>>     To call in the statistician after the experiment is done may be
>>     no more than asking him to perform a post-mortem examination: he
>>     may be able to say what the experiment died of. ~ Sir Ronald
>>     Aylmer Fisher
>>     The plural of anecdote is not data. ~ Roger Brinner
>>     The combination of some data and an aching desire for an answer
>>     does not ensure that a reasonable answer can be extracted from a
>>     given body of data. ~ John Tukey
>>     ///////////////////////////////////////////////////////////////////////////////////////////
>>
>>     <https://www.inbo.be>
>>
>>
>>     Op wo 22 apr. 2020 om 11:44 schreef Thomas Merkling
>>     <thomasmerkling00 at gmail.com <mailto:thomasmerkling00 at gmail.com>>:
>>
>>         Hi all,
>>
>>         I'm wondering how to best model data from an experimental design
>>         involving a spatial component. This is a study on seabirds
>>         nesting on
>>         artificial cliffs: each nest has been attributed an experimental
>>         treatment (supplemented or not), while making sure that there
>>         was a
>>         variable proportion of surrounding nests of the opposite
>>         treatment. Our
>>         main goal was to investigate if laying date of a focal pair was
>>         influenced by its treatment and/or by the proportion of
>>         surrounding
>>         nests of the opposite treatment (hereafter, "Prop"), which we
>>         calculated
>>         at 3 different spatial scale (local, panel and global, see
>>         https://drive.google.com/open?id=1OrJQCkNfBO6KOBHSlkOoQyAdTrqtIdY8
>>         for a
>>         visual representation).
>>
>>         Hence, the treatment information of a focal pair is used in the
>>         "Treatment" predictor variable, but also in the calculation
>>         of "Prop"
>>         for the surrounding pairs (the number of pairs affected
>>         depending on the
>>         spatial scale considered), thereby leading to some
>>         pseudo-replication.
>>         Since this is dependent on the distance (i.e. "Prop" of pairs
>>         closer to
>>         a focal one are more influenced than pairs further away), we
>>         thought
>>         that accounting for spatial auto-correlation for be
>>         sufficient. We used
>>         the spaMM package to do so, and our models look something like:
>>
>>         Laying ~ Treatment * Prop + Year + (1|PairID) +
>>         Matern(Y2011|x + y) +
>>         Matern(Y2012|x + y)
>>
>>         with two Mat?rn correlation random effects (one for each year
>>         of the
>>         study) being included (x and y being the spatial coordinates
>>         of the nests).
>>
>>         My question is: Is this random effect structure taking into
>>         account the
>>         fact that "Prop" of a focal pair depends on the "Treatment"
>>         of the
>>         surrounding pairs or not ? If not, how can we account for that?
>>
>>         Thanks in advance for your help!
>>         Thomas
>>
>>
>>         ? ? ? ? [[alternative HTML version deleted]]
>>
>>         _______________________________________________
>>         R-sig-mixed-models at r-project.org
>>         <mailto:R-sig-mixed-models at r-project.org> mailing list
>>         https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Wed Apr 22 19:45:49 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Wed, 22 Apr 2020 19:45:49 +0200
Subject: [R-sig-ME] spatial auto-correlation or more complicated
 pseudo-replication?
In-Reply-To: <57173e82-cab3-2371-1815-f61db29b2e50@gmail.com>
References: <3b3cdef0-ec91-7fcf-29ea-c33be45bedbd@gmail.com>
 <CAJuCY5yHdyUGr4W14+RJYddLCNxmjXs=F+c=YGKUFwhs=k0J2Q@mail.gmail.com>
 <8c4cab12-c8c7-588c-d590-8988df25b7f4@gmail.com>
 <CAJuCY5xzvRv4BsVirmyQAiF7KLAsxmQd-3XYOF833Sn_ss8X3Q@mail.gmail.com>
 <57173e82-cab3-2371-1815-f61db29b2e50@gmail.com>
Message-ID: <CAJuCY5wMMDQT=8r=bNAx4VYNJu3UwD9d84Zhnj=4LDPm1UD_fw@mail.gmail.com>

Dear Thomas,

Have a look at the data.frame in the variogram() output. Given your
variogram I expect a high number of pairs (np variable) at short range and
a low (< 100) at large ranges. Note the width and cutoff arguments of
variogram().  The defaults are 1/3 of the diagonal of the bounding box for
cutoff and cutoff/15 for width. These are likely suboptimal for your data.
I'd set width to slightly larger than the distance between two adjacent
nests. Increase the width if the variogram is unstable.
If you still get a similar picture as the ones you send, then there then
residuals are iid and thus you don't need to correct for spatial
autocorrelation.

Given the strong correlation between pair and location, the pair random
effect will take up some of the spatial autocorrelation. You could make a
variogram of the random intercepts. There should be a pure nugget effect
too.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op wo 22 apr. 2020 om 18:19 schreef Thomas Merkling <
thomasmerkling00 at gmail.com>:

> Dear Thierry,
>
> Thanks for your answer.
> Below is the piece of code I ran:
> mod <- lmer(Laydate ~ Treatment + Year + (1|PairID), REML= FALSE, data =
> CRlF)
> CRlF$resmod <- residuals(mod, type = "pearson")
> plot(gstat::variogram(resmod ~ 1, loc = x+y, data = CRlF))
>
> It seems like the variance is quite stable for distances up to 25 and then
> drops a bit. I did the same analysis with another response variable (egg
> weight) and got a similar pattern. (links to plot for laydate
> <https://drive.google.com/open?id=1T2n41DeSh0BlkZVX5t-E1IKdMuudujBy> and
> for eggweight
> <https://drive.google.com/open?id=10_ou4yQQkF-kVVnf7zbgHyMmUEABvnPU>)
> So does it mean that there is no spatial auto-correlation then?
>
> This would match the fact that our results don't change much if we add the
> Matern correlation random effects or not.
> A reviewer suggested that spatial-autocorrelation isn't sufficient to
> account for the pseudo-replication in our data, and that we still have an
> issue of inflation of the degrees of freedom and suggested permutation
> tests to account for that, but is that really necessary?
>
> Kind regards,
> Thomas
> On 22/04/2020 16:44, Thierry Onkelinx wrote:
>
> Dear Thomas,
>
> Extract the residuals from the model. Then use gstat::variogram() to
> calculate the empirical variogram of the residuals.  If there is spatial
> autocorrelation, you'll see an increase in the variance as the distance
> between observations increases.
>
> I would expect that the birds have a stronger effect than the nests. Hence
> I'd use Pair ID. If the dataset would span more than 2 years you could try
> both a Pair and Nest random effect.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op wo 22 apr. 2020 om 14:58 schreef Thomas Merkling <
> thomasmerkling00 at gmail.com>:
>
>> Dear Thierry,
>>
>> Thanks for reply. We used a sample of the population for our experiment,
>> but for this sample we have information (treatment and Prop variable at
>> each scale for all the nests.
>> How would you suggest to test/check is there is spatial autocorrelation?
>> I tried with the DHARMa package (which makes a Moran's I test adapted to
>> mixed models), but it doesn't show if autocorrelation changes with
>> distance, it just gives a p-value. I tried with a model with PairID as
>> random effect (p = 0.33), but if I include nest as a random effect (some
>> pairs changed in between the 2 years of the experiment, so there are less
>> Nest IDs than Pair IDs) the p-value becomes 0.054 ...
>>
>> Kind regards,
>> Thomas
>> On 22/04/2020 13:57, Thierry Onkelinx wrote:
>>
>> Dear Thomas,
>>
>> Do you have information on all the nests or only on a sample of the
>> nests? In case you have data on every nest, then I would look at a simple
>> model with only treatment and an iid nest effect. Then see if there is
>> spatial autocorrelation. Variation at small ranges would indicate an effect
>> of the treatment of the neighbouring nests.
>>
>> Best regards,
>>
>> ir. Thierry Onkelinx
>> Statisticus / Statistician
>>
>> Vlaamse Overheid / Government of Flanders
>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>> AND FOREST
>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>> thierry.onkelinx at inbo.be
>> Havenlaan 88 bus 73, 1000 Brussel
>> www.inbo.be
>>
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of data.
>> ~ John Tukey
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>>
>> <https://www.inbo.be>
>>
>>
>> Op wo 22 apr. 2020 om 11:44 schreef Thomas Merkling <
>> thomasmerkling00 at gmail.com>:
>>
>>> Hi all,
>>>
>>> I'm wondering how to best model data from an experimental design
>>> involving a spatial component. This is a study on seabirds nesting on
>>> artificial cliffs: each nest has been attributed an experimental
>>> treatment (supplemented or not), while making sure that there was a
>>> variable proportion of surrounding nests of the opposite treatment. Our
>>> main goal was to investigate if laying date of a focal pair was
>>> influenced by its treatment and/or by the proportion of surrounding
>>> nests of the opposite treatment (hereafter, "Prop"), which we calculated
>>> at 3 different spatial scale (local, panel and global, see
>>> https://drive.google.com/open?id=1OrJQCkNfBO6KOBHSlkOoQyAdTrqtIdY8 for
>>> a
>>> visual representation).
>>>
>>> Hence, the treatment information of a focal pair is used in the
>>> "Treatment" predictor variable, but also in the calculation of "Prop"
>>> for the surrounding pairs (the number of pairs affected depending on the
>>> spatial scale considered), thereby leading to some pseudo-replication.
>>> Since this is dependent on the distance (i.e. "Prop" of pairs closer to
>>> a focal one are more influenced than pairs further away), we thought
>>> that accounting for spatial auto-correlation for be sufficient. We used
>>> the spaMM package to do so, and our models look something like:
>>>
>>> Laying ~ Treatment * Prop + Year + (1|PairID) + Matern(Y2011|x + y) +
>>> Matern(Y2012|x + y)
>>>
>>> with two Mat?rn correlation random effects (one for each year of the
>>> study) being included (x and y being the spatial coordinates of the
>>> nests).
>>>
>>> My question is: Is this random effect structure taking into account the
>>> fact that "Prop" of a focal pair depends on the "Treatment" of the
>>> surrounding pairs or not ? If not, how can we account for that?
>>>
>>> Thanks in advance for your help!
>>> Thomas
>>>
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>

	[[alternative HTML version deleted]]


From thom@@merk||ng00 @end|ng |rom gm@||@com  Wed Apr 22 23:24:17 2020
From: thom@@merk||ng00 @end|ng |rom gm@||@com (Thomas Merkling)
Date: Wed, 22 Apr 2020 23:24:17 +0200
Subject: [R-sig-ME] spatial auto-correlation or more complicated
 pseudo-replication?
In-Reply-To: <CAJuCY5wMMDQT=8r=bNAx4VYNJu3UwD9d84Zhnj=4LDPm1UD_fw@mail.gmail.com>
References: <3b3cdef0-ec91-7fcf-29ea-c33be45bedbd@gmail.com>
 <CAJuCY5yHdyUGr4W14+RJYddLCNxmjXs=F+c=YGKUFwhs=k0J2Q@mail.gmail.com>
 <8c4cab12-c8c7-588c-d590-8988df25b7f4@gmail.com>
 <CAJuCY5xzvRv4BsVirmyQAiF7KLAsxmQd-3XYOF833Sn_ss8X3Q@mail.gmail.com>
 <57173e82-cab3-2371-1815-f61db29b2e50@gmail.com>
 <CAJuCY5wMMDQT=8r=bNAx4VYNJu3UwD9d84Zhnj=4LDPm1UD_fw@mail.gmail.com>
Message-ID: <ca63172e-e90c-8694-b6c7-01a3608ec4a1@gmail.com>

Dear Thierry,

Thanks a lot for your precious help.

I changed the width argument of variogram() and obtained similar 
patterns (for egg 
<https://drive.google.com/open?id=1Ktinx0gm4sRNS6_r5VEAg_pdND51eHQd> and 
for laydate 
<https://drive.google.com/open?id=1YoMIK1jax1dhBe94yswbHvWF_CyKDNL5>).
Doing the same thing with the random intercepts gave similar patterns 
too (for egg 
<https://drive.google.com/open?id=1fGlXObzhRj2g9J9ZnYIO8V6q6d-PmgS8> and 
for laydate 
<https://drive.google.com/open?id=1U4Q9h7foZM6q54_yteFAFHo6SVHby9mJ>).

Can I then conclude that there is no need to correct for spatial 
auto-correlation?

Given that treatment of a focal pair influences the value of another 
predictor for surrounding pairs, is there any other random effect that I 
should add? Or is this test for spatial auto-correlation enough?

Kind regards,
Thomas

On 22/04/2020 19:45, Thierry Onkelinx wrote:
> Dear Thomas,
>
> Have a look at the data.frame in the variogram() output. Given your 
> variogram I expect a high number of pairs (np variable) at short range 
> and a low (< 100) at large ranges. Note the width and cutoff arguments 
> of variogram().? The defaults are 1/3 of the diagonal of the bounding 
> box for cutoff and cutoff/15 for width. These are likely suboptimal 
> for your data. I'd set width to slightly larger than the distance 
> between two adjacent nests. Increase the width if the variogram is 
> unstable.
> If you still get a similar picture as the ones you send, then there 
> then residuals are iid and thus you don't need to correct for spatial 
> autocorrelation.
>
> Given the strong correlation between pair and location, the pair 
> random effect will take up some of the spatial autocorrelation. You 
> could make a variogram of the random intercepts. There should be a 
> pure nugget effect too.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE 
> AND FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be <http://www.inbo.be>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no 
> more than asking him to perform a post-mortem examination: he may be 
> able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does 
> not ensure that a reasonable answer can be extracted from a given body 
> of data. ~ John Tukey
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>>>         .....
>>

	[[alternative HTML version deleted]]


From ben@go|d@te|n @end|ng |rom berke|ey@edu  Thu Apr 23 00:58:16 2020
From: ben@go|d@te|n @end|ng |rom berke|ey@edu (Ben Goldstein)
Date: Wed, 22 Apr 2020 15:58:16 -0700
Subject: [R-sig-ME] nAGQ > 1 in lme4::glmer gives unexpected likelihood
Message-ID: <CAEbSdx17yikdhCLta4oektu0K44u-jA_E=9Efb9axJ=LsNsruA@mail.gmail.com>

Hi all,

I'm using lme4::glmer to estimate Poisson mixed models in a very simple
context (single random effect). I'm interested in the model likelihood/AIC
across many simulated datasets.

To investigate whether the Laplace approximation was appropriate for my
data context, I explored using the argument nAGQ to improve the accuracy of
the likelihood estimation. When I changed nAGQ to a value > 1, I saw an
unexpectedly huge change in the likelihood; log-likelihoods tended to be
off by ~200. Other statistics packages (e.g. GLMMadaptive) yield estimates
that agree with lme4's Laplace approximation, as did a manual likelihood
estimate, and not with the nAGQ > 2 estimate.

The following code reproduces the problem I'm encountering.

*# r-sig-mixed-models GLMM question*
library(lme4)
set.seed(51)

*# Simulate some random effect-driven Poisson data*
random_effects <- rnorm(10, 0, 2)
group <- rep(1:10, 10)
simulated_data <- data.frame(y = rpois(n = 100, lambda = exp(3 +
random_effects[group])),
                             group = group)

*# Fit models with Laplace (nAGQ = 1) and nAGQ = 11*
fit_Laplace <- glmer(y ~ (1|group), data = simulated_data, family =
poisson())
fit_AGQ <- glmer(y ~ (1|group), data = simulated_data, family = poisson(),
nAGQ = 11)

logLik(fit_Laplace)
logLik(fit_AGQ)
logLik(fit_Laplace) - logLik(fit_AGQ) *# Huge difference!*

When I execute the above code, I see a difference in likelihood of
-218.8894. I've tested across many simulations and on 2 different machines
(Mac and Linux). My version of lme4 is up to date.

Has anyone run into this issue before? Am I using the glmer function wrong,
or is it possible there's something going on under the hood?

Thanks,
Ben

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Thu Apr 23 01:30:00 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 22 Apr 2020 19:30:00 -0400
Subject: [R-sig-ME] nAGQ > 1 in lme4::glmer gives unexpected likelihood
In-Reply-To: <CAEbSdx17yikdhCLta4oektu0K44u-jA_E=9Efb9axJ=LsNsruA@mail.gmail.com>
References: <CAEbSdx17yikdhCLta4oektu0K44u-jA_E=9Efb9axJ=LsNsruA@mail.gmail.com>
Message-ID: <cc1385d7-c496-eb67-9643-7c2099b3c9da@gmail.com>

 ?It's entirely possible there's something going on under the hood.? 
IIRC there's a document somewhere that talks about how deviances and 
log-likelihoods are defined, and this may(?) still differ between nAGQ=1 
and nAGQ>1 ?

On 4/22/20 6:58 PM, Ben Goldstein wrote:
> Hi all,
>
> I'm using lme4::glmer to estimate Poisson mixed models in a very simple
> context (single random effect). I'm interested in the model likelihood/AIC
> across many simulated datasets.
>
> To investigate whether the Laplace approximation was appropriate for my
> data context, I explored using the argument nAGQ to improve the accuracy of
> the likelihood estimation. When I changed nAGQ to a value > 1, I saw an
> unexpectedly huge change in the likelihood; log-likelihoods tended to be
> off by ~200. Other statistics packages (e.g. GLMMadaptive) yield estimates
> that agree with lme4's Laplace approximation, as did a manual likelihood
> estimate, and not with the nAGQ > 2 estimate.
>
> The following code reproduces the problem I'm encountering.
>
> *# r-sig-mixed-models GLMM question*
> library(lme4)
> set.seed(51)
>
> *# Simulate some random effect-driven Poisson data*
> random_effects <- rnorm(10, 0, 2)
> group <- rep(1:10, 10)
> simulated_data <- data.frame(y = rpois(n = 100, lambda = exp(3 +
> random_effects[group])),
>                               group = group)
>
> *# Fit models with Laplace (nAGQ = 1) and nAGQ = 11*
> fit_Laplace <- glmer(y ~ (1|group), data = simulated_data, family =
> poisson())
> fit_AGQ <- glmer(y ~ (1|group), data = simulated_data, family = poisson(),
> nAGQ = 11)
>
> logLik(fit_Laplace)
> logLik(fit_AGQ)
> logLik(fit_Laplace) - logLik(fit_AGQ) *# Huge difference!*
>
> When I execute the above code, I see a difference in likelihood of
> -218.8894. I've tested across many simulations and on 2 different machines
> (Mac and Linux). My version of lme4 is up to date.
>
> Has anyone run into this issue before? Am I using the glmer function wrong,
> or is it possible there's something going on under the hood?
>
> Thanks,
> Ben
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From d@r|zopou|o@ @end|ng |rom er@@mu@mc@n|  Thu Apr 23 08:29:04 2020
From: d@r|zopou|o@ @end|ng |rom er@@mu@mc@n| (D. Rizopoulos)
Date: Thu, 23 Apr 2020 06:29:04 +0000
Subject: [R-sig-ME] nAGQ > 1 in lme4::glmer gives unexpected likelihood
In-Reply-To: <cc1385d7-c496-eb67-9643-7c2099b3c9da@gmail.com>
References: <CAEbSdx17yikdhCLta4oektu0K44u-jA_E=9Efb9axJ=LsNsruA@mail.gmail.com>
 <cc1385d7-c496-eb67-9643-7c2099b3c9da@gmail.com>
Message-ID: <460daf6ef66c4437891d6bcf6ca4cfd9@erasmusmc.nl>

On a related topic, it is not clear to me whether glmer() updates the location of the quadrature points at each iteration when nAGQ > 1.



-----Original Message-----
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of Ben Bolker
Sent: Thursday, April 23, 2020 1:30 AM
To: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] nAGQ > 1 in lme4::glmer gives unexpected likelihood

 ?It's entirely possible there's something going on under the hood. IIRC there's a document somewhere that talks about how deviances and log-likelihoods are defined, and this may(?) still differ between nAGQ=1 and nAGQ>1 ?

On 4/22/20 6:58 PM, Ben Goldstein wrote:
> Hi all,
>
> I'm using lme4::glmer to estimate Poisson mixed models in a very 
> simple context (single random effect). I'm interested in the model 
> likelihood/AIC across many simulated datasets.
>
> To investigate whether the Laplace approximation was appropriate for 
> my data context, I explored using the argument nAGQ to improve the 
> accuracy of the likelihood estimation. When I changed nAGQ to a value 
> > 1, I saw an unexpectedly huge change in the likelihood; 
> log-likelihoods tended to be off by ~200. Other statistics packages 
> (e.g. GLMMadaptive) yield estimates that agree with lme4's Laplace 
> approximation, as did a manual likelihood estimate, and not with the nAGQ > 2 estimate.
>
> The following code reproduces the problem I'm encountering.
>
> *# r-sig-mixed-models GLMM question*
> library(lme4)
> set.seed(51)
>
> *# Simulate some random effect-driven Poisson data* random_effects <- 
> rnorm(10, 0, 2) group <- rep(1:10, 10) simulated_data <- data.frame(y 
> = rpois(n = 100, lambda = exp(3 + random_effects[group])),
>                               group = group)
>
> *# Fit models with Laplace (nAGQ = 1) and nAGQ = 11* fit_Laplace <- 
> glmer(y ~ (1|group), data = simulated_data, family =
> poisson())
> fit_AGQ <- glmer(y ~ (1|group), data = simulated_data, family = 
> poisson(), nAGQ = 11)
>
> logLik(fit_Laplace)
> logLik(fit_AGQ)
> logLik(fit_Laplace) - logLik(fit_AGQ) *# Huge difference!*
>
> When I execute the above code, I see a difference in likelihood of 
> -218.8894. I've tested across many simulations and on 2 different 
> machines (Mac and Linux). My version of lme4 is up to date.
>
> Has anyone run into this issue before? Am I using the glmer function 
> wrong, or is it possible there's something going on under the hood?
>
> Thanks,
> Ben
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
> .ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-mixed-models&amp;data=02%7C01%7C
> d.rizopoulos%40erasmusmc.nl%7C1bd598f4412543fa011308d7e715222e%7C52663
> 8ba6af34b0fa532a1a511f4ac80%7C0%7C0%7C637231950269193147&amp;sdata=vTY
> Io6nWnpQid01oBUizJ2hVU7JwRXgh3aRJunYO86c%3D&amp;reserved=0

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-mixed-models&amp;data=02%7C01%7Cd.rizopoulos%40erasmusmc.nl%7C1bd598f4412543fa011308d7e715222e%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C0%7C637231950269193147&amp;sdata=vTYIo6nWnpQid01oBUizJ2hVU7JwRXgh3aRJunYO86c%3D&amp;reserved=0

From th|erry@onke||nx @end|ng |rom |nbo@be  Thu Apr 23 11:44:24 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Thu, 23 Apr 2020 11:44:24 +0200
Subject: [R-sig-ME] spatial auto-correlation or more complicated
 pseudo-replication?
In-Reply-To: <ca63172e-e90c-8694-b6c7-01a3608ec4a1@gmail.com>
References: <3b3cdef0-ec91-7fcf-29ea-c33be45bedbd@gmail.com>
 <CAJuCY5yHdyUGr4W14+RJYddLCNxmjXs=F+c=YGKUFwhs=k0J2Q@mail.gmail.com>
 <8c4cab12-c8c7-588c-d590-8988df25b7f4@gmail.com>
 <CAJuCY5xzvRv4BsVirmyQAiF7KLAsxmQd-3XYOF833Sn_ss8X3Q@mail.gmail.com>
 <57173e82-cab3-2371-1815-f61db29b2e50@gmail.com>
 <CAJuCY5wMMDQT=8r=bNAx4VYNJu3UwD9d84Zhnj=4LDPm1UD_fw@mail.gmail.com>
 <ca63172e-e90c-8694-b6c7-01a3608ec4a1@gmail.com>
Message-ID: <CAJuCY5zbwbtc-JAMVCBiGYB-QycQTX1SU8WZsLbFRNu316GQZg@mail.gmail.com>

Dear Thomas,

I'd say that there is nothing that suggests spatial autocorrelation in the
residuals nor in the random effects. There might be spatial autocorrelation
in the response variable, but that is handled by the covariates in the
model. The pair random effect takes care of the pseudo-replication.

IMHO you can't state that the treatment of the focal pair influences the
surrounding pairs. If that was the case, I'd expect to see spatial
autocorrelation.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op wo 22 apr. 2020 om 23:24 schreef Thomas Merkling <
thomasmerkling00 at gmail.com>:

> Dear Thierry,
>
> Thanks a lot for your precious help.
>
> I changed the width argument of variogram() and obtained similar patterns
> (for egg
> <https://drive.google.com/open?id=1Ktinx0gm4sRNS6_r5VEAg_pdND51eHQd> and
> for laydate
> <https://drive.google.com/open?id=1YoMIK1jax1dhBe94yswbHvWF_CyKDNL5>).
> Doing the same thing with the random intercepts gave similar patterns too
> (for egg
> <https://drive.google.com/open?id=1fGlXObzhRj2g9J9ZnYIO8V6q6d-PmgS8> and
> for laydate
> <https://drive.google.com/open?id=1U4Q9h7foZM6q54_yteFAFHo6SVHby9mJ>).
>
> Can I then conclude that there is no need to correct for spatial
> auto-correlation?
>
> Given that treatment of a focal pair influences the value of another
> predictor for surrounding pairs, is there any other random effect that I
> should add? Or is this test for spatial auto-correlation enough?
>
> Kind regards,
> Thomas
> On 22/04/2020 19:45, Thierry Onkelinx wrote:
>
> Dear Thomas,
>
> Have a look at the data.frame in the variogram() output. Given your
> variogram I expect a high number of pairs (np variable) at short range and
> a low (< 100) at large ranges. Note the width and cutoff arguments of
> variogram().  The defaults are 1/3 of the diagonal of the bounding box for
> cutoff and cutoff/15 for width. These are likely suboptimal for your data.
> I'd set width to slightly larger than the distance between two adjacent
> nests. Increase the width if the variogram is unstable.
> If you still get a similar picture as the ones you send, then there then
> residuals are iid and thus you don't need to correct for spatial
> autocorrelation.
>
> Given the strong correlation between pair and location, the pair random
> effect will take up some of the spatial autocorrelation. You could make a
> variogram of the random intercepts. There should be a pure nugget effect
> too.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
> .....
>>>
>>>

	[[alternative HTML version deleted]]


From thom@@merk||ng00 @end|ng |rom gm@||@com  Thu Apr 23 13:39:16 2020
From: thom@@merk||ng00 @end|ng |rom gm@||@com (Thomas Merkling)
Date: Thu, 23 Apr 2020 13:39:16 +0200
Subject: [R-sig-ME] spatial auto-correlation or more complicated
 pseudo-replication?
In-Reply-To: <CAJuCY5zbwbtc-JAMVCBiGYB-QycQTX1SU8WZsLbFRNu316GQZg@mail.gmail.com>
References: <3b3cdef0-ec91-7fcf-29ea-c33be45bedbd@gmail.com>
 <CAJuCY5yHdyUGr4W14+RJYddLCNxmjXs=F+c=YGKUFwhs=k0J2Q@mail.gmail.com>
 <8c4cab12-c8c7-588c-d590-8988df25b7f4@gmail.com>
 <CAJuCY5xzvRv4BsVirmyQAiF7KLAsxmQd-3XYOF833Sn_ss8X3Q@mail.gmail.com>
 <57173e82-cab3-2371-1815-f61db29b2e50@gmail.com>
 <CAJuCY5wMMDQT=8r=bNAx4VYNJu3UwD9d84Zhnj=4LDPm1UD_fw@mail.gmail.com>
 <ca63172e-e90c-8694-b6c7-01a3608ec4a1@gmail.com>
 <CAJuCY5zbwbtc-JAMVCBiGYB-QycQTX1SU8WZsLbFRNu316GQZg@mail.gmail.com>
Message-ID: <c933982d-97f9-9a76-d343-729701931e17@gmail.com>

Thanks Thierry, that's reassuring.

One last question: how do the covariates take care of the potential 
spatial autocorrelation in the response variable?

Kind regards,
Thomas

On 23/04/2020 11:44, Thierry Onkelinx wrote:
> Dear Thomas,
>
> I'd say that there is nothing that suggests spatial autocorrelation in 
> the residuals nor in the random effects. There might be spatial 
> autocorrelation in the response variable, but that is handled by the 
> covariates in the model. The pair random effect takes care of the 
> pseudo-replication.
>
> IMHO you can't state that the treatment of the focal pair influences 
> the surrounding pairs. If that was the case, I'd expect to see spatial 
> autocorrelation.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE 
> AND FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be <http://www.inbo.be>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no 
> more than asking him to perform a post-mortem examination: he may be 
> able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does 
> not ensure that a reasonable answer can be extracted from a given body 
> of data. ~ John Tukey
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op wo 22 apr. 2020 om 23:24 schreef Thomas Merkling 
> <thomasmerkling00 at gmail.com <mailto:thomasmerkling00 at gmail.com>>:
>
>     Dear Thierry,
>
>     Thanks a lot for your precious help.
>
>     I changed the width argument of variogram() and obtained similar
>     patterns (for egg
>     <https://drive.google.com/open?id=1Ktinx0gm4sRNS6_r5VEAg_pdND51eHQd>
>     and for laydate
>     <https://drive.google.com/open?id=1YoMIK1jax1dhBe94yswbHvWF_CyKDNL5>).
>     Doing the same thing with the random intercepts gave similar
>     patterns too (for egg
>     <https://drive.google.com/open?id=1fGlXObzhRj2g9J9ZnYIO8V6q6d-PmgS8>
>     and for laydate
>     <https://drive.google.com/open?id=1U4Q9h7foZM6q54_yteFAFHo6SVHby9mJ>).
>
>     Can I then conclude that there is no need to correct for spatial
>     auto-correlation?
>
>     Given that treatment of a focal pair influences the value of
>     another predictor for surrounding pairs, is there any other random
>     effect that I should add? Or is this test for spatial
>     auto-correlation enough?
>
>     Kind regards,
>     Thomas
>
>     On 22/04/2020 19:45, Thierry Onkelinx wrote:
>>     Dear Thomas,
>>
>>     Have a look at the data.frame in the variogram() output. Given
>>     your variogram I expect a high number of pairs (np variable) at
>>     short range and a low (< 100) at large ranges. Note the width and
>>     cutoff arguments of variogram().? The defaults are 1/3 of the
>>     diagonal of the bounding box for cutoff and cutoff/15 for width.
>>     These are likely suboptimal for your data. I'd set width to
>>     slightly larger than the distance between two adjacent nests.
>>     Increase the width if the variogram is unstable.
>>     If you still get a similar picture as the ones you send, then
>>     there then residuals are iid and thus you don't need to correct
>>     for spatial autocorrelation.
>>
>>     Given the strong correlation between pair and location, the pair
>>     random effect will take up some of the spatial autocorrelation.
>>     You could make a variogram of the random intercepts. There should
>>     be a pure nugget effect too.
>>
>>     Best regards,
>>
>>     ir. Thierry Onkelinx
>>     Statisticus / Statistician
>>
>>     Vlaamse Overheid / Government of Flanders
>>     INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR
>>     NATURE AND FOREST
>>     Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality
>>     Assurance
>>     thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>
>>     Havenlaan 88 bus 73, 1000 Brussel
>>     www.inbo.be <http://www.inbo.be>
>>
>>     ///////////////////////////////////////////////////////////////////////////////////////////
>>     To call in the statistician after the experiment is done may be
>>     no more than asking him to perform a post-mortem examination: he
>>     may be able to say what the experiment died of. ~ Sir Ronald
>>     Aylmer Fisher
>>     The plural of anecdote is not data. ~ Roger Brinner
>>     The combination of some data and an aching desire for an answer
>>     does not ensure that a reasonable answer can be extracted from a
>>     given body of data. ~ John Tukey
>>     ///////////////////////////////////////////////////////////////////////////////////////////
>>
>>     <https://www.inbo.be>
>>
>>>>             .....
>>>

	[[alternative HTML version deleted]]


From b@te@ @end|ng |rom @t@t@w|@c@edu  Fri Apr 24 16:04:56 2020
From: b@te@ @end|ng |rom @t@t@w|@c@edu (Douglas Bates)
Date: Fri, 24 Apr 2020 09:04:56 -0500
Subject: [R-sig-ME] nAGQ > 1 in lme4::glmer gives unexpected likelihood
In-Reply-To: <15763_1587596325_0Q9700G9XP5WJU30_CAEbSdx17yikdhCLta4oektu0K44u-jA_E=9Efb9axJ=LsNsruA@mail.gmail.com>
References: <15763_1587596325_0Q9700G9XP5WJU30_CAEbSdx17yikdhCLta4oektu0K44u-jA_E=9Efb9axJ=LsNsruA@mail.gmail.com>
Message-ID: <CAO7JsnR_TDHYtQDAWOzCp9CBWw7CJ+UfsZk2iTzDWqyQz=dsFw@mail.gmail.com>

There's a lot of variability in your lambdas

> exp(3 + random_effects)
 [1]  91.5919358   6.9678749   4.1841478  78.0771666 890.6931394  20.8558107
 [7]   3.0037864   0.3049416   2.1675995  40.6209684

Do you really expect that some groups will have a mean count of nearly 900
whereas others will have a mean count less than 1?


On Wed, Apr 22, 2020 at 5:58 PM Ben Goldstein <ben.goldstein at berkeley.edu>
wrote:

> Hi all,
>
> I'm using lme4::glmer to estimate Poisson mixed models in a very simple
> context (single random effect). I'm interested in the model likelihood/AIC
> across many simulated datasets.
>
> To investigate whether the Laplace approximation was appropriate for my
> data context, I explored using the argument nAGQ to improve the accuracy of
> the likelihood estimation. When I changed nAGQ to a value > 1, I saw an
> unexpectedly huge change in the likelihood; log-likelihoods tended to be
> off by ~200. Other statistics packages (e.g. GLMMadaptive) yield estimates
> that agree with lme4's Laplace approximation, as did a manual likelihood
> estimate, and not with the nAGQ > 2 estimate.
>
> The following code reproduces the problem I'm encountering.
>
> *# r-sig-mixed-models GLMM question*
> library(lme4)
> set.seed(51)
>
> *# Simulate some random effect-driven Poisson data*
> random_effects <- rnorm(10, 0, 2)
> group <- rep(1:10, 10)
> simulated_data <- data.frame(y = rpois(n = 100, lambda = exp(3 +
> random_effects[group])),
>                              group = group)
>
> *# Fit models with Laplace (nAGQ = 1) and nAGQ = 11*
> fit_Laplace <- glmer(y ~ (1|group), data = simulated_data, family =
> poisson())
> fit_AGQ <- glmer(y ~ (1|group), data = simulated_data, family = poisson(),
> nAGQ = 11)
>
> logLik(fit_Laplace)
> logLik(fit_AGQ)
> logLik(fit_Laplace) - logLik(fit_AGQ) *# Huge difference!*
>
> When I execute the above code, I see a difference in likelihood of
> -218.8894. I've tested across many simulations and on 2 different machines
> (Mac and Linux). My version of lme4 is up to date.
>
> Has anyone run into this issue before? Am I using the glmer function wrong,
> or is it possible there's something going on under the hood?
>
> Thanks,
> Ben
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From m@tteo@@b@ @end|ng |rom gm@||@com  Fri Apr 24 16:20:36 2020
From: m@tteo@@b@ @end|ng |rom gm@||@com (Matteo Sebastianelli)
Date: Fri, 24 Apr 2020 17:20:36 +0300
Subject: [R-sig-ME] Interpretation Inverse Gamma glmmTMB with interactions
Message-ID: <CAHLZGq-X5gGg-fpc1JeNKeCE-aWeg26+FgsQJMYV=a5VCj6iqw@mail.gmail.com>

Hi all,

I have been running an inverse Gamma glmm in glmmTMB and I am facing some
interpretation issues. DHARMa validation QQplot shows strong deviation when
using a gaussian model. The full model has a 3-way interaction. Here is the
output:

>
Cand.mod[[2]]<-glmmTMB(PRP~logEVI+logLAI+logVCF+logSRTM+phenotype*zone*mindist+
(1|Location/Bird_ID),control=glmmTMBControl(profile=quote(length(parameters$beta)>=5)),family=Gamma(link="inverse"),
contact)
> summary(Cand.mod[[2]])
 Family: Gamma  ( inverse )
Formula:          PRP ~ logEVI + logLAI + logVCF + logSRTM + phenotype *
zone *      mindist + (1 | Location/Bird_ID)
Data: contact

     AIC      BIC   logLik deviance df.resid
 -2510.0  -2406.4   1278.0  -2556.0      647

Random effects:

Conditional model:
 Groups           Name        Variance Std.Dev.
 Bird_ID:Location (Intercept) 0.007765 0.08812
 Location         (Intercept) 0.001286 0.03586
Number of obs: 670, groups:  Bird_ID:Location, 607; Location, 149

Dispersion estimate for Gamma family (sigma^2): 0.00149

Conditional model:
                                        Estimate Std. Error z value
Pr(>|z|)
(Intercept)                            2.0804378  0.1703550  12.212  <
2e-16 ***
logEVI                                -0.0903489  0.0458202  -1.972
0.048631 *
logLAI                                 0.0006268  0.0237269   0.026
0.978926
logVCF                                 0.0476667  0.0186172   2.560
0.010457 *
logSRTM                                0.0181225  0.0142313   1.273
0.202867
phenotypeyellow-fronted               -0.4409140  0.0445983  -9.886  <
2e-16 ***
zone2                                 -0.1888145  0.0651436  -2.898
0.003750 **
zone3                                  0.0542872  0.0453511   1.197
0.231289
zone4                                 -0.1738887  0.0525499  -3.309
0.000936 ***
mindist                                0.0266960  0.0221012   1.208
0.227088
phenotypeyellow-fronted:zone2          0.4846604  0.0690797   7.016
2.28e-12 ***
phenotypeyellow-fronted:zone3          0.2510780  0.0497183   5.050
4.42e-07 ***
phenotypeyellow-fronted:zone4          0.0867471  0.0640485   1.354
0.175609
phenotypeyellow-fronted:mindist       -0.0367038  0.0229506  -1.599
0.109765
zone2:mindist                          0.0608414  0.0342409   1.777
0.075591 .
zone3:mindist                          0.0641069  0.0247138   2.594
0.009487 **
zone4:mindist                          0.0248087  0.0334446   0.742
0.458217
phenotypeyellow-fronted:zone2:mindist -0.0616859  0.0363812  -1.696
0.089973 .
phenotypeyellow-fronted:zone3:mindist -0.0646249  0.0276774  -2.335
0.019547 *
phenotypeyellow-fronted:zone4:mindist -0.0292554  0.0442657  -0.661
0.508673
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1


What we want to understand is how to to interpret a three way interaction
model. Which factors are meaningful? If we have a three way interaction
phenotype x zone x mindist, are the P values given correct or do we
estimate them by extrapolation from the main effects and two way
interaction estimates? Are the main effects and two-way interaction effects
telling us anything meaningful, e.g. phenotype x zone or phenotype x
mindist?

Also, when we run 4 models for each zone we get some significant effects
that we can't see in the model with the 4 zones together (e.g. there is a
strong interaction effect between distance and phenotype in one of the
zone, estimate = -0.114, st.err = .03,  p=.0001). Is there a way to
extrapolate such effects from the full model?

Looking forward to hear from you.

All the best,

Matteo

	[[alternative HTML version deleted]]


From b@te@ @end|ng |rom @t@t@w|@c@edu  Fri Apr 24 16:24:00 2020
From: b@te@ @end|ng |rom @t@t@w|@c@edu (Douglas Bates)
Date: Fri, 24 Apr 2020 09:24:00 -0500
Subject: [R-sig-ME] nAGQ > 1 in lme4::glmer gives unexpected likelihood
In-Reply-To: <31308_1587737137_0Q9A00BAKPTDVU20_CAO7JsnR_TDHYtQDAWOzCp9CBWw7CJ+UfsZk2iTzDWqyQz=dsFw@mail.gmail.com>
References: <15763_1587596325_0Q9700G9XP5WJU30_CAEbSdx17yikdhCLta4oektu0K44u-jA_E=9Efb9axJ=LsNsruA@mail.gmail.com>
 <31308_1587737137_0Q9A00BAKPTDVU20_CAO7JsnR_TDHYtQDAWOzCp9CBWw7CJ+UfsZk2iTzDWqyQz=dsFw@mail.gmail.com>
Message-ID: <CAO7JsnTNuNDFMZhJjEq=rRFT8Mj5rXFes3KKOyzFi_Wg7kF=8Q@mail.gmail.com>

Having said that, I do see that the fits in the MixedModels package for
Julia produce similar values of the deviance with the Laplace approximation
and nAGQ = 7

julia> m1 = fit(MixedModel, @formula(y ~ 1 + (1|group)), dd, Poisson())
Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)
  y ~ 1 + (1 | group)
  Distribution: Poisson{Float64}
  Link: LogLink()

  Deviance: 193.5587

Variance components:
         Column    Variance  Std.Dev.
group (Intercept)  3.9577026 1.9893975

 Number of obs: 100; levels of grouping factors: 10

Fixed-effects parameters:
??????????????????????????????????????????????????
             Estimate  Std.Error  z value  P(>|z|)
??????????????????????????????????????????????????
(Intercept)   2.65175   0.632317     4.19    <1e-4
??????????????????????????????????????????????????

julia> m1 = fit(MixedModel, @formula(y ~ 1 + (1|group)), dd, Poisson(),
nAGQ=7)
Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 7)
  y ~ 1 + (1 | group)
  Distribution: Poisson{Float64}
  Link: LogLink()

  Deviance: 193.5104

Variance components:
         Column    Variance  Std.Dev.
group (Intercept)  3.9577026 1.9893975

 Number of obs: 100; levels of grouping factors: 10

Fixed-effects parameters:
??????????????????????????????????????????????????
             Estimate  Std.Error  z value  P(>|z|)
??????????????????????????????????????????????????
(Intercept)   2.65175   0.632317     4.19    <1e-4
??????????????????????????????????????????????????

As the person who wrote the first version of the nAGQ code in R I would not
be surprised if there was a constant dropped somewhere.  It is difficult
code.

And the results here in the Julia package make me uncomfortable because the
values of the parameter estimates are identical in the two fits.  I would
expect them to be close but not identical.

Isn't it good to know that there is still room for research in this area?
:-)

On Fri, Apr 24, 2020 at 9:05 AM Douglas Bates <bates at stat.wisc.edu> wrote:

> There's a lot of variability in your lambdas
>
> > exp(3 + random_effects)
>  [1]  91.5919358   6.9678749   4.1841478  78.0771666 890.6931394
> 20.8558107
>  [7]   3.0037864   0.3049416   2.1675995  40.6209684
>
> Do you really expect that some groups will have a mean count of nearly 900
> whereas others will have a mean count less than 1?
>
>
> On Wed, Apr 22, 2020 at 5:58 PM Ben Goldstein <ben.goldstein at berkeley.edu>
> wrote:
>
> > Hi all,
> >
> > I'm using lme4::glmer to estimate Poisson mixed models in a very simple
> > context (single random effect). I'm interested in the model
> likelihood/AIC
> > across many simulated datasets.
> >
> > To investigate whether the Laplace approximation was appropriate for my
> > data context, I explored using the argument nAGQ to improve the accuracy
> of
> > the likelihood estimation. When I changed nAGQ to a value > 1, I saw an
> > unexpectedly huge change in the likelihood; log-likelihoods tended to be
> > off by ~200. Other statistics packages (e.g. GLMMadaptive) yield
> estimates
> > that agree with lme4's Laplace approximation, as did a manual likelihood
> > estimate, and not with the nAGQ > 2 estimate.
> >
> > The following code reproduces the problem I'm encountering.
> >
> > *# r-sig-mixed-models GLMM question*
> > library(lme4)
> > set.seed(51)
> >
> > *# Simulate some random effect-driven Poisson data*
> > random_effects <- rnorm(10, 0, 2)
> > group <- rep(1:10, 10)
> > simulated_data <- data.frame(y = rpois(n = 100, lambda = exp(3 +
> > random_effects[group])),
> >                              group = group)
> >
> > *# Fit models with Laplace (nAGQ = 1) and nAGQ = 11*
> > fit_Laplace <- glmer(y ~ (1|group), data = simulated_data, family =
> > poisson())
> > fit_AGQ <- glmer(y ~ (1|group), data = simulated_data, family =
> poisson(),
> > nAGQ = 11)
> >
> > logLik(fit_Laplace)
> > logLik(fit_AGQ)
> > logLik(fit_Laplace) - logLik(fit_AGQ) *# Huge difference!*
> >
> > When I execute the above code, I see a difference in likelihood of
> > -218.8894. I've tested across many simulations and on 2 different
> machines
> > (Mac and Linux). My version of lme4 is up to date.
> >
> > Has anyone run into this issue before? Am I using the glmer function
> wrong,
> > or is it possible there's something going on under the hood?
> >
> > Thanks,
> > Ben
> >
> >         [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Fri Apr 24 16:59:27 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Fri, 24 Apr 2020 10:59:27 -0400
Subject: [R-sig-ME] nAGQ > 1 in lme4::glmer gives unexpected likelihood
In-Reply-To: <CAO7JsnTNuNDFMZhJjEq=rRFT8Mj5rXFes3KKOyzFi_Wg7kF=8Q@mail.gmail.com>
References: <15763_1587596325_0Q9700G9XP5WJU30_CAEbSdx17yikdhCLta4oektu0K44u-jA_E=9Efb9axJ=LsNsruA@mail.gmail.com>
 <31308_1587737137_0Q9A00BAKPTDVU20_CAO7JsnR_TDHYtQDAWOzCp9CBWw7CJ+UfsZk2iTzDWqyQz=dsFw@mail.gmail.com>
 <CAO7JsnTNuNDFMZhJjEq=rRFT8Mj5rXFes3KKOyzFi_Wg7kF=8Q@mail.gmail.com>
Message-ID: <bcc0a4d0-efab-902a-263a-a8b9e07516eb@gmail.com>


 ?? I found the note I was looking for.? In ?deviance.glmerMod it says:

 ?If adaptive Gauss-Hermite quadrature is used, then
 ????????? ?logLik(object)? is currently only proportional to the
 ????????? absolute-unconditional log-likelihood.

(see the discussion on this page for more context); see also 
https://github.com/lme4/lme4/blob/master/misc/logLikGLMM/logLikGLMM.R


On 4/24/20 10:24 AM, Douglas Bates wrote:
> Having said that, I do see that the fits in the MixedModels package for
> Julia produce similar values of the deviance with the Laplace approximation
> and nAGQ = 7
>
> julia> m1 = fit(MixedModel, @formula(y ~ 1 + (1|group)), dd, Poisson())
> Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)
>    y ~ 1 + (1 | group)
>    Distribution: Poisson{Float64}
>    Link: LogLink()
>
>    Deviance: 193.5587
>
> Variance components:
>           Column    Variance  Std.Dev.
> group (Intercept)  3.9577026 1.9893975
>
>   Number of obs: 100; levels of grouping factors: 10
>
> Fixed-effects parameters:
> ??????????????????????????????????????????????????
>               Estimate  Std.Error  z value  P(>|z|)
> ??????????????????????????????????????????????????
> (Intercept)   2.65175   0.632317     4.19    <1e-4
> ??????????????????????????????????????????????????
>
> julia> m1 = fit(MixedModel, @formula(y ~ 1 + (1|group)), dd, Poisson(),
> nAGQ=7)
> Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 7)
>    y ~ 1 + (1 | group)
>    Distribution: Poisson{Float64}
>    Link: LogLink()
>
>    Deviance: 193.5104
>
> Variance components:
>           Column    Variance  Std.Dev.
> group (Intercept)  3.9577026 1.9893975
>
>   Number of obs: 100; levels of grouping factors: 10
>
> Fixed-effects parameters:
> ??????????????????????????????????????????????????
>               Estimate  Std.Error  z value  P(>|z|)
> ??????????????????????????????????????????????????
> (Intercept)   2.65175   0.632317     4.19    <1e-4
> ??????????????????????????????????????????????????
>
> As the person who wrote the first version of the nAGQ code in R I would not
> be surprised if there was a constant dropped somewhere.  It is difficult
> code.
>
> And the results here in the Julia package make me uncomfortable because the
> values of the parameter estimates are identical in the two fits.  I would
> expect them to be close but not identical.
>
> Isn't it good to know that there is still room for research in this area?
> :-)
>
> On Fri, Apr 24, 2020 at 9:05 AM Douglas Bates <bates at stat.wisc.edu> wrote:
>
>> There's a lot of variability in your lambdas
>>
>>> exp(3 + random_effects)
>>   [1]  91.5919358   6.9678749   4.1841478  78.0771666 890.6931394
>> 20.8558107
>>   [7]   3.0037864   0.3049416   2.1675995  40.6209684
>>
>> Do you really expect that some groups will have a mean count of nearly 900
>> whereas others will have a mean count less than 1?
>>
>>
>> On Wed, Apr 22, 2020 at 5:58 PM Ben Goldstein <ben.goldstein at berkeley.edu>
>> wrote:
>>
>>> Hi all,
>>>
>>> I'm using lme4::glmer to estimate Poisson mixed models in a very simple
>>> context (single random effect). I'm interested in the model
>> likelihood/AIC
>>> across many simulated datasets.
>>>
>>> To investigate whether the Laplace approximation was appropriate for my
>>> data context, I explored using the argument nAGQ to improve the accuracy
>> of
>>> the likelihood estimation. When I changed nAGQ to a value > 1, I saw an
>>> unexpectedly huge change in the likelihood; log-likelihoods tended to be
>>> off by ~200. Other statistics packages (e.g. GLMMadaptive) yield
>> estimates
>>> that agree with lme4's Laplace approximation, as did a manual likelihood
>>> estimate, and not with the nAGQ > 2 estimate.
>>>
>>> The following code reproduces the problem I'm encountering.
>>>
>>> *# r-sig-mixed-models GLMM question*
>>> library(lme4)
>>> set.seed(51)
>>>
>>> *# Simulate some random effect-driven Poisson data*
>>> random_effects <- rnorm(10, 0, 2)
>>> group <- rep(1:10, 10)
>>> simulated_data <- data.frame(y = rpois(n = 100, lambda = exp(3 +
>>> random_effects[group])),
>>>                               group = group)
>>>
>>> *# Fit models with Laplace (nAGQ = 1) and nAGQ = 11*
>>> fit_Laplace <- glmer(y ~ (1|group), data = simulated_data, family =
>>> poisson())
>>> fit_AGQ <- glmer(y ~ (1|group), data = simulated_data, family =
>> poisson(),
>>> nAGQ = 11)
>>>
>>> logLik(fit_Laplace)
>>> logLik(fit_AGQ)
>>> logLik(fit_Laplace) - logLik(fit_AGQ) *# Huge difference!*
>>>
>>> When I execute the above code, I see a difference in likelihood of
>>> -218.8894. I've tested across many simulations and on 2 different
>> machines
>>> (Mac and Linux). My version of lme4 is up to date.
>>>
>>> Has anyone run into this issue before? Am I using the glmer function
>> wrong,
>>> or is it possible there's something going on under the hood?
>>>
>>> Thanks,
>>> Ben
>>>
>>>          [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>          [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From |v@|d@ @end|ng |rom he@|th@uc@d@edu  Fri Apr 24 23:05:11 2020
From: |v@|d@ @end|ng |rom he@|th@uc@d@edu (Vaida, Florin)
Date: Fri, 24 Apr 2020 21:05:11 +0000
Subject: [R-sig-ME] nAGQ > 1 in lme4::glmer gives unexpected likelihood
In-Reply-To: <bcc0a4d0-efab-902a-263a-a8b9e07516eb@gmail.com>
References: <15763_1587596325_0Q9700G9XP5WJU30_CAEbSdx17yikdhCLta4oektu0K44u-jA_E=9Efb9axJ=LsNsruA@mail.gmail.com>
 <31308_1587737137_0Q9A00BAKPTDVU20_CAO7JsnR_TDHYtQDAWOzCp9CBWw7CJ+UfsZk2iTzDWqyQz=dsFw@mail.gmail.com>
 <CAO7JsnTNuNDFMZhJjEq=rRFT8Mj5rXFes3KKOyzFi_Wg7kF=8Q@mail.gmail.com>
 <bcc0a4d0-efab-902a-263a-a8b9e07516eb@gmail.com>
Message-ID: <41530176-EC7C-4857-9933-7F557A70D412@health.ucsd.edu>

One can figure out which likelihood version is correct (Laplace or nAGQ>1) by doing a simulation with random effects variance -> 0.
I'll do that if I find time.
Florin

> On Apr 24, 2020, at 7:59 AM, Ben Bolker <bbolker at gmail.com> wrote:
> 
> 
>    I found the note I was looking for.  In ?deviance.glmerMod it says:
> 
>  If adaptive Gauss-Hermite quadrature is used, then
>           ?logLik(object)? is currently only proportional to the
>           absolute-unconditional log-likelihood.
> 
> (see the discussion on this page for more context); see also https://github.com/lme4/lme4/blob/master/misc/logLikGLMM/logLikGLMM.R
> 
> 
> On 4/24/20 10:24 AM, Douglas Bates wrote:
>> Having said that, I do see that the fits in the MixedModels package for
>> Julia produce similar values of the deviance with the Laplace approximation
>> and nAGQ = 7
>> 
>> julia> m1 = fit(MixedModel, @formula(y ~ 1 + (1|group)), dd, Poisson())
>> Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)
>>   y ~ 1 + (1 | group)
>>   Distribution: Poisson{Float64}
>>   Link: LogLink()
>> 
>>   Deviance: 193.5587
>> 
>> Variance components:
>>          Column    Variance  Std.Dev.
>> group (Intercept)  3.9577026 1.9893975
>> 
>>  Number of obs: 100; levels of grouping factors: 10
>> 
>> Fixed-effects parameters:
>> ??????????????????????????????????????????????????
>>              Estimate  Std.Error  z value  P(>|z|)
>> ??????????????????????????????????????????????????
>> (Intercept)   2.65175   0.632317     4.19    <1e-4
>> ??????????????????????????????????????????????????
>> 
>> julia> m1 = fit(MixedModel, @formula(y ~ 1 + (1|group)), dd, Poisson(),
>> nAGQ=7)
>> Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 7)
>>   y ~ 1 + (1 | group)
>>   Distribution: Poisson{Float64}
>>   Link: LogLink()
>> 
>>   Deviance: 193.5104
>> 
>> Variance components:
>>          Column    Variance  Std.Dev.
>> group (Intercept)  3.9577026 1.9893975
>> 
>>  Number of obs: 100; levels of grouping factors: 10
>> 
>> Fixed-effects parameters:
>> ??????????????????????????????????????????????????
>>              Estimate  Std.Error  z value  P(>|z|)
>> ??????????????????????????????????????????????????
>> (Intercept)   2.65175   0.632317     4.19    <1e-4
>> ??????????????????????????????????????????????????
>> 
>> As the person who wrote the first version of the nAGQ code in R I would not
>> be surprised if there was a constant dropped somewhere.  It is difficult
>> code.
>> 
>> And the results here in the Julia package make me uncomfortable because the
>> values of the parameter estimates are identical in the two fits.  I would
>> expect them to be close but not identical.
>> 
>> Isn't it good to know that there is still room for research in this area?
>> :-)
>> 
>> On Fri, Apr 24, 2020 at 9:05 AM Douglas Bates <bates at stat.wisc.edu> wrote:
>> 
>>> There's a lot of variability in your lambdas
>>> 
>>>> exp(3 + random_effects)
>>>  [1]  91.5919358   6.9678749   4.1841478  78.0771666 890.6931394
>>> 20.8558107
>>>  [7]   3.0037864   0.3049416   2.1675995  40.6209684
>>> 
>>> Do you really expect that some groups will have a mean count of nearly 900
>>> whereas others will have a mean count less than 1?
>>> 
>>> 
>>> On Wed, Apr 22, 2020 at 5:58 PM Ben Goldstein <ben.goldstein at berkeley.edu>
>>> wrote:
>>> 
>>>> Hi all,
>>>> 
>>>> I'm using lme4::glmer to estimate Poisson mixed models in a very simple
>>>> context (single random effect). I'm interested in the model
>>> likelihood/AIC
>>>> across many simulated datasets.
>>>> 
>>>> To investigate whether the Laplace approximation was appropriate for my
>>>> data context, I explored using the argument nAGQ to improve the accuracy
>>> of
>>>> the likelihood estimation. When I changed nAGQ to a value > 1, I saw an
>>>> unexpectedly huge change in the likelihood; log-likelihoods tended to be
>>>> off by ~200. Other statistics packages (e.g. GLMMadaptive) yield
>>> estimates
>>>> that agree with lme4's Laplace approximation, as did a manual likelihood
>>>> estimate, and not with the nAGQ > 2 estimate.
>>>> 
>>>> The following code reproduces the problem I'm encountering.
>>>> 
>>>> *# r-sig-mixed-models GLMM question*
>>>> library(lme4)
>>>> set.seed(51)
>>>> 
>>>> *# Simulate some random effect-driven Poisson data*
>>>> random_effects <- rnorm(10, 0, 2)
>>>> group <- rep(1:10, 10)
>>>> simulated_data <- data.frame(y = rpois(n = 100, lambda = exp(3 +
>>>> random_effects[group])),
>>>>                              group = group)
>>>> 
>>>> *# Fit models with Laplace (nAGQ = 1) and nAGQ = 11*
>>>> fit_Laplace <- glmer(y ~ (1|group), data = simulated_data, family =
>>>> poisson())
>>>> fit_AGQ <- glmer(y ~ (1|group), data = simulated_data, family =
>>> poisson(),
>>>> nAGQ = 11)
>>>> 
>>>> logLik(fit_Laplace)
>>>> logLik(fit_AGQ)
>>>> logLik(fit_Laplace) - logLik(fit_AGQ) *# Huge difference!*
>>>> 
>>>> When I execute the above code, I see a difference in likelihood of
>>>> -218.8894. I've tested across many simulations and on 2 different
>>> machines
>>>> (Mac and Linux). My version of lme4 is up to date.
>>>> 
>>>> Has anyone run into this issue before? Am I using the glmer function
>>> wrong,
>>>> or is it possible there's something going on under the hood?
>>>> 
>>>> Thanks,
>>>> Ben
>>>> 
>>>>         [[alternative HTML version deleted]]
>>>> 
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>> 
>>>         [[alternative HTML version deleted]]
>>> 
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>> 
>> 	[[alternative HTML version deleted]]
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From bbo|ker @end|ng |rom gm@||@com  Fri Apr 24 23:06:28 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Fri, 24 Apr 2020 17:06:28 -0400
Subject: [R-sig-ME] nAGQ > 1 in lme4::glmer gives unexpected likelihood
In-Reply-To: <41530176-EC7C-4857-9933-7F557A70D412@health.ucsd.edu>
References: <15763_1587596325_0Q9700G9XP5WJU30_CAEbSdx17yikdhCLta4oektu0K44u-jA_E=9Efb9axJ=LsNsruA@mail.gmail.com>
 <31308_1587737137_0Q9A00BAKPTDVU20_CAO7JsnR_TDHYtQDAWOzCp9CBWw7CJ+UfsZk2iTzDWqyQz=dsFw@mail.gmail.com>
 <CAO7JsnTNuNDFMZhJjEq=rRFT8Mj5rXFes3KKOyzFi_Wg7kF=8Q@mail.gmail.com>
 <bcc0a4d0-efab-902a-263a-a8b9e07516eb@gmail.com>
 <41530176-EC7C-4857-9933-7F557A70D412@health.ucsd.edu>
Message-ID: <27717610-083b-ebba-c00e-387acf037e54@gmail.com>


 ? I think they're both 'correct' in the sense of being proportional to 
the likelihood but use different baselines, thus incommensurate (see the 
note pointed out below).

On 4/24/20 5:05 PM, Vaida, Florin wrote:
> One can figure out which likelihood version is correct (Laplace or nAGQ>1) by doing a simulation with random effects variance -> 0.
> I'll do that if I find time.
> Florin
>
>> On Apr 24, 2020, at 7:59 AM, Ben Bolker <bbolker at gmail.com> wrote:
>>
>>
>>     I found the note I was looking for.  In ?deviance.glmerMod it says:
>>
>>   If adaptive Gauss-Hermite quadrature is used, then
>>            ?logLik(object)? is currently only proportional to the
>>            absolute-unconditional log-likelihood.
>>
>> (see the discussion on this page for more context); see also https://github.com/lme4/lme4/blob/master/misc/logLikGLMM/logLikGLMM.R
>>
>>
>> On 4/24/20 10:24 AM, Douglas Bates wrote:
>>> Having said that, I do see that the fits in the MixedModels package for
>>> Julia produce similar values of the deviance with the Laplace approximation
>>> and nAGQ = 7
>>>
>>> julia> m1 = fit(MixedModel, @formula(y ~ 1 + (1|group)), dd, Poisson())
>>> Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)
>>>    y ~ 1 + (1 | group)
>>>    Distribution: Poisson{Float64}
>>>    Link: LogLink()
>>>
>>>    Deviance: 193.5587
>>>
>>> Variance components:
>>>           Column    Variance  Std.Dev.
>>> group (Intercept)  3.9577026 1.9893975
>>>
>>>   Number of obs: 100; levels of grouping factors: 10
>>>
>>> Fixed-effects parameters:
>>> ??????????????????????????????????????????????????
>>>               Estimate  Std.Error  z value  P(>|z|)
>>> ??????????????????????????????????????????????????
>>> (Intercept)   2.65175   0.632317     4.19    <1e-4
>>> ??????????????????????????????????????????????????
>>>
>>> julia> m1 = fit(MixedModel, @formula(y ~ 1 + (1|group)), dd, Poisson(),
>>> nAGQ=7)
>>> Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 7)
>>>    y ~ 1 + (1 | group)
>>>    Distribution: Poisson{Float64}
>>>    Link: LogLink()
>>>
>>>    Deviance: 193.5104
>>>
>>> Variance components:
>>>           Column    Variance  Std.Dev.
>>> group (Intercept)  3.9577026 1.9893975
>>>
>>>   Number of obs: 100; levels of grouping factors: 10
>>>
>>> Fixed-effects parameters:
>>> ??????????????????????????????????????????????????
>>>               Estimate  Std.Error  z value  P(>|z|)
>>> ??????????????????????????????????????????????????
>>> (Intercept)   2.65175   0.632317     4.19    <1e-4
>>> ??????????????????????????????????????????????????
>>>
>>> As the person who wrote the first version of the nAGQ code in R I would not
>>> be surprised if there was a constant dropped somewhere.  It is difficult
>>> code.
>>>
>>> And the results here in the Julia package make me uncomfortable because the
>>> values of the parameter estimates are identical in the two fits.  I would
>>> expect them to be close but not identical.
>>>
>>> Isn't it good to know that there is still room for research in this area?
>>> :-)
>>>
>>> On Fri, Apr 24, 2020 at 9:05 AM Douglas Bates <bates at stat.wisc.edu> wrote:
>>>
>>>> There's a lot of variability in your lambdas
>>>>
>>>>> exp(3 + random_effects)
>>>>   [1]  91.5919358   6.9678749   4.1841478  78.0771666 890.6931394
>>>> 20.8558107
>>>>   [7]   3.0037864   0.3049416   2.1675995  40.6209684
>>>>
>>>> Do you really expect that some groups will have a mean count of nearly 900
>>>> whereas others will have a mean count less than 1?
>>>>
>>>>
>>>> On Wed, Apr 22, 2020 at 5:58 PM Ben Goldstein <ben.goldstein at berkeley.edu>
>>>> wrote:
>>>>
>>>>> Hi all,
>>>>>
>>>>> I'm using lme4::glmer to estimate Poisson mixed models in a very simple
>>>>> context (single random effect). I'm interested in the model
>>>> likelihood/AIC
>>>>> across many simulated datasets.
>>>>>
>>>>> To investigate whether the Laplace approximation was appropriate for my
>>>>> data context, I explored using the argument nAGQ to improve the accuracy
>>>> of
>>>>> the likelihood estimation. When I changed nAGQ to a value > 1, I saw an
>>>>> unexpectedly huge change in the likelihood; log-likelihoods tended to be
>>>>> off by ~200. Other statistics packages (e.g. GLMMadaptive) yield
>>>> estimates
>>>>> that agree with lme4's Laplace approximation, as did a manual likelihood
>>>>> estimate, and not with the nAGQ > 2 estimate.
>>>>>
>>>>> The following code reproduces the problem I'm encountering.
>>>>>
>>>>> *# r-sig-mixed-models GLMM question*
>>>>> library(lme4)
>>>>> set.seed(51)
>>>>>
>>>>> *# Simulate some random effect-driven Poisson data*
>>>>> random_effects <- rnorm(10, 0, 2)
>>>>> group <- rep(1:10, 10)
>>>>> simulated_data <- data.frame(y = rpois(n = 100, lambda = exp(3 +
>>>>> random_effects[group])),
>>>>>                               group = group)
>>>>>
>>>>> *# Fit models with Laplace (nAGQ = 1) and nAGQ = 11*
>>>>> fit_Laplace <- glmer(y ~ (1|group), data = simulated_data, family =
>>>>> poisson())
>>>>> fit_AGQ <- glmer(y ~ (1|group), data = simulated_data, family =
>>>> poisson(),
>>>>> nAGQ = 11)
>>>>>
>>>>> logLik(fit_Laplace)
>>>>> logLik(fit_AGQ)
>>>>> logLik(fit_Laplace) - logLik(fit_AGQ) *# Huge difference!*
>>>>>
>>>>> When I execute the above code, I see a difference in likelihood of
>>>>> -218.8894. I've tested across many simulations and on 2 different
>>>> machines
>>>>> (Mac and Linux). My version of lme4 is up to date.
>>>>>
>>>>> Has anyone run into this issue before? Am I using the glmer function
>>>> wrong,
>>>>> or is it possible there's something going on under the hood?
>>>>>
>>>>> Thanks,
>>>>> Ben
>>>>>
>>>>>          [[alternative HTML version deleted]]
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>
>>>>          [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>> 	[[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From |v@|d@ @end|ng |rom he@|th@uc@d@edu  Sat Apr 25 00:31:33 2020
From: |v@|d@ @end|ng |rom he@|th@uc@d@edu (Vaida, Florin)
Date: Fri, 24 Apr 2020 22:31:33 +0000
Subject: [R-sig-ME] nAGQ > 1 in lme4::glmer gives unexpected likelihood
In-Reply-To: <bcc0a4d0-efab-902a-263a-a8b9e07516eb@gmail.com>
References: <15763_1587596325_0Q9700G9XP5WJU30_CAEbSdx17yikdhCLta4oektu0K44u-jA_E=9Efb9axJ=LsNsruA@mail.gmail.com>
 <31308_1587737137_0Q9A00BAKPTDVU20_CAO7JsnR_TDHYtQDAWOzCp9CBWw7CJ+UfsZk2iTzDWqyQz=dsFw@mail.gmail.com>
 <CAO7JsnTNuNDFMZhJjEq=rRFT8Mj5rXFes3KKOyzFi_Wg7kF=8Q@mail.gmail.com>
 <bcc0a4d0-efab-902a-263a-a8b9e07516eb@gmail.com>
Message-ID: <96E4E24F-5E51-4C7C-82A4-F00421717256@health.ucsd.edu>

I modified Ben Goldstein's code slightly to show that it's indeed the nAGQ log-likelihood that's off by a constant, rather than the Laplace.
While for model selection reasons one can argue that the constant doesn't matter, it is good to know that this distinction exists.
I'm not quite sure if/why the right log-likelihood could be computed; I could see this as needed/useful for comparison between different models, i.e. outside the Poisson or even GLME class.

Florin


#### Likelihood of Poisson Models

# r-sig-mixed-models GLMM question*
  library(lme4)
set.seed(51)

# Simulate some random effect-driven Poisson data*
# random_effects <- rnorm(10, 0, 2)
# random_effects <- rnorm(10, 0, 0.01)
random_effects <- rnorm(10, 0, 0.01)

group <- rep(1:10, 10)
simulated_data <- data.frame(y = rpois(n = 100, lambda = exp(3 +
                                                               random_effects[group])),
                             group = group)

# Fit models with Laplace (nAGQ = 1) and nAGQ = 11*
  fit_Laplace <- glmer(y ~ (1|group), data = simulated_data, family =
                         poisson())
fit_AGQ <- glmer(y ~ (1|group), data = simulated_data, family = poisson(),
                 nAGQ = 11)

fit_glm = glm(y ~ group, data=simulated_data, family=poisson)

(logLik(fit_Laplace))
(logLik(fit_AGQ))
(logLik(fit_glm)). # very similar to logLik(fit_Laplace)


> On Apr 24, 2020, at 7:59 AM, Ben Bolker <bbolker at gmail.com> wrote:
> 
> 
>    I found the note I was looking for.  In ?deviance.glmerMod it says:
> 
>  If adaptive Gauss-Hermite quadrature is used, then
>           ?logLik(object)? is currently only proportional to the
>           absolute-unconditional log-likelihood.
> 
> (see the discussion on this page for more context); see also https://github.com/lme4/lme4/blob/master/misc/logLikGLMM/logLikGLMM.R
> 
> 
> On 4/24/20 10:24 AM, Douglas Bates wrote:
>> Having said that, I do see that the fits in the MixedModels package for
>> Julia produce similar values of the deviance with the Laplace approximation
>> and nAGQ = 7
>> 
>> julia> m1 = fit(MixedModel, @formula(y ~ 1 + (1|group)), dd, Poisson())
>> Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)
>>   y ~ 1 + (1 | group)
>>   Distribution: Poisson{Float64}
>>   Link: LogLink()
>> 
>>   Deviance: 193.5587
>> 
>> Variance components:
>>          Column    Variance  Std.Dev.
>> group (Intercept)  3.9577026 1.9893975
>> 
>>  Number of obs: 100; levels of grouping factors: 10
>> 
>> Fixed-effects parameters:
>> ??????????????????????????????????????????????????
>>              Estimate  Std.Error  z value  P(>|z|)
>> ??????????????????????????????????????????????????
>> (Intercept)   2.65175   0.632317     4.19    <1e-4
>> ??????????????????????????????????????????????????
>> 
>> julia> m1 = fit(MixedModel, @formula(y ~ 1 + (1|group)), dd, Poisson(),
>> nAGQ=7)
>> Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 7)
>>   y ~ 1 + (1 | group)
>>   Distribution: Poisson{Float64}
>>   Link: LogLink()
>> 
>>   Deviance: 193.5104
>> 
>> Variance components:
>>          Column    Variance  Std.Dev.
>> group (Intercept)  3.9577026 1.9893975
>> 
>>  Number of obs: 100; levels of grouping factors: 10
>> 
>> Fixed-effects parameters:
>> ??????????????????????????????????????????????????
>>              Estimate  Std.Error  z value  P(>|z|)
>> ??????????????????????????????????????????????????
>> (Intercept)   2.65175   0.632317     4.19    <1e-4
>> ??????????????????????????????????????????????????
>> 
>> As the person who wrote the first version of the nAGQ code in R I would not
>> be surprised if there was a constant dropped somewhere.  It is difficult
>> code.
>> 
>> And the results here in the Julia package make me uncomfortable because the
>> values of the parameter estimates are identical in the two fits.  I would
>> expect them to be close but not identical.
>> 
>> Isn't it good to know that there is still room for research in this area?
>> :-)
>> 
>> On Fri, Apr 24, 2020 at 9:05 AM Douglas Bates <bates at stat.wisc.edu> wrote:
>> 
>>> There's a lot of variability in your lambdas
>>> 
>>>> exp(3 + random_effects)
>>>  [1]  91.5919358   6.9678749   4.1841478  78.0771666 890.6931394
>>> 20.8558107
>>>  [7]   3.0037864   0.3049416   2.1675995  40.6209684
>>> 
>>> Do you really expect that some groups will have a mean count of nearly 900
>>> whereas others will have a mean count less than 1?
>>> 
>>> 
>>> On Wed, Apr 22, 2020 at 5:58 PM Ben Goldstein <ben.goldstein at berkeley.edu>
>>> wrote:
>>> 
>>>> Hi all,
>>>> 
>>>> I'm using lme4::glmer to estimate Poisson mixed models in a very simple
>>>> context (single random effect). I'm interested in the model
>>> likelihood/AIC
>>>> across many simulated datasets.
>>>> 
>>>> To investigate whether the Laplace approximation was appropriate for my
>>>> data context, I explored using the argument nAGQ to improve the accuracy
>>> of
>>>> the likelihood estimation. When I changed nAGQ to a value > 1, I saw an
>>>> unexpectedly huge change in the likelihood; log-likelihoods tended to be
>>>> off by ~200. Other statistics packages (e.g. GLMMadaptive) yield
>>> estimates
>>>> that agree with lme4's Laplace approximation, as did a manual likelihood
>>>> estimate, and not with the nAGQ > 2 estimate.
>>>> 
>>>> The following code reproduces the problem I'm encountering.
>>>> 
>>>> *# r-sig-mixed-models GLMM question*
>>>> library(lme4)
>>>> set.seed(51)
>>>> 
>>>> *# Simulate some random effect-driven Poisson data*
>>>> random_effects <- rnorm(10, 0, 2)
>>>> group <- rep(1:10, 10)
>>>> simulated_data <- data.frame(y = rpois(n = 100, lambda = exp(3 +
>>>> random_effects[group])),
>>>>                              group = group)
>>>> 
>>>> *# Fit models with Laplace (nAGQ = 1) and nAGQ = 11*
>>>> fit_Laplace <- glmer(y ~ (1|group), data = simulated_data, family =
>>>> poisson())
>>>> fit_AGQ <- glmer(y ~ (1|group), data = simulated_data, family =
>>> poisson(),
>>>> nAGQ = 11)
>>>> 
>>>> logLik(fit_Laplace)
>>>> logLik(fit_AGQ)
>>>> logLik(fit_Laplace) - logLik(fit_AGQ) *# Huge difference!*
>>>> 
>>>> When I execute the above code, I see a difference in likelihood of
>>>> -218.8894. I've tested across many simulations and on 2 different
>>> machines
>>>> (Mac and Linux). My version of lme4 is up to date.
>>>> 
>>>> Has anyone run into this issue before? Am I using the glmer function
>>> wrong,
>>>> or is it possible there's something going on under the hood?
>>>> 
>>>> Thanks,
>>>> Ben
>>>> 
>>>>         [[alternative HTML version deleted]]
>>>> 
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>> 
>>>         [[alternative HTML version deleted]]
>>> 
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>> 
>> 	[[alternative HTML version deleted]]
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From er|k@@o|bu @end|ng |rom n|b|o@no  Sat Apr 25 09:24:37 2020
From: er|k@@o|bu @end|ng |rom n|b|o@no (Erik Solbu)
Date: Sat, 25 Apr 2020 07:24:37 +0000
Subject: [R-sig-ME] Zero-truncated Poisson distribution in glmmTMB
Message-ID: <HE1P189MB0473C9E025B72665158D62B0EAD10@HE1P189MB0473.EURP189.PROD.OUTLOOK.COM>

Hi everyone,

I'm trying to replace a package called 'poilog' with glmmTMB, but the results differ in such a way that I'm not sure how to interpret the results from glmmTMB.  I've written a short example below:

#### Start of code ####

# Required packages
library(poilog)
library(glmmTMB)

# Simulate using the 'poilog' package
# sim_01 <- rpoilog(S = 1000, mu = 0, sig = 2, keep0 = TRUE)
# This is the same as
sim_01 <- rpois(n = 1000, lambda = exp(rnorm(n = 1000, mean = 0, sd = 2)))

# estimate the parameters
# using the 'poilog' package, assume regular Poisson distribution
est_01 <- poilogMLE(n = sim_01, zTrunc = FALSE)
# using the 'poilog' package, assume zero truncated Poisson distribution
est_02 <- poilogMLE(n = sim_01[sim_01>0], zTrunc = TRUE)

# make data.frame for glmmTMB estimation
df_01 <- data.frame(abundance = sim_01,
                   species = 1:length(sim_01))

# using the 'glmmTMB' package, assume regular Poisson distribution
est_03 <- glmmTMB(abundance ~ (1 | species),
                      family = poisson(link = "log"),
                      data = df_01)
# using the 'glmmTMB' package, assume zero truncated Poisson distribution
est_04 <- glmmTMB(abundance ~ (1 | species),
                      family = truncated_poisson(link = "log"),
                      data = df_01[df_01$abundance>0, ])

# Compare estimates
est_df <- data.frame(method = rep(c("poilog", "glmmmTMB"), each = 2),
                     assumptions = rep(c("poisson", "truncated_poisson"), 2),
                     mu = c(est_01$par[1], est_02$par[1],
                            est_03$fit$par[1], est_04$fit$par[1]),
                     sigma = c(est_01$par[2], est_02$par[2],
                               exp(c(est_03$fit$par[2], est_04$fit$par[2]))))

est_df

#### End of code ####

When I fit the truncated_poisson model in glmmTMB, the mean increases and the standard deviation decreases, which is similar to what happens if you ignore the zeros and fit a regular poisson distribution, but it doesn't seem to be doing quite that either.

So, I'm hoping anyone knew how to interpret the mean and standard deviation from the truncated_poisson output and how they relate to the mean and standard deviation of a regular poisson model?

Thank you very much for reading this and any response is much appreciated.

Best regards,

Erik



	[[alternative HTML version deleted]]


From mo|||eebrook@ @end|ng |rom gm@||@com  Sat Apr 25 12:17:37 2020
From: mo|||eebrook@ @end|ng |rom gm@||@com (Mollie Brooks)
Date: Sat, 25 Apr 2020 12:17:37 +0200
Subject: [R-sig-ME] Zero-truncated Poisson distribution in glmmTMB
In-Reply-To: <HE1P189MB0473C9E025B72665158D62B0EAD10@HE1P189MB0473.EURP189.PROD.OUTLOOK.COM>
References: <HE1P189MB0473C9E025B72665158D62B0EAD10@HE1P189MB0473.EURP189.PROD.OUTLOOK.COM>
Message-ID: <C78B7F7F-D16F-452F-A538-5E6033881E27@gmail.com>

The poilogMLE function is fitting a Poisson-lognormal which is equivalent to a Poisson with an observation level random effect (OLRE) to account for overdispersion. So to make the models similar, you probably need to add
df_01$obs = row.names(df_01)
and then add (1|obs) to your model formulas. 
  
Here is a reference about OLREs https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4194460/ <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4194460/>

cheers,
Mollie

> On 25Apr 2020, at 9:24, Erik Solbu <erik.solbu at nibio.no> wrote:
> 
> Hi everyone,
> 
> I'm trying to replace a package called 'poilog' with glmmTMB, but the results differ in such a way that I'm not sure how to interpret the results from glmmTMB.  I've written a short example below:
> 
> #### Start of code ####
> 
> # Required packages
> library(poilog)
> library(glmmTMB)
> 
> # Simulate using the 'poilog' package
> # sim_01 <- rpoilog(S = 1000, mu = 0, sig = 2, keep0 = TRUE)
> # This is the same as
> sim_01 <- rpois(n = 1000, lambda = exp(rnorm(n = 1000, mean = 0, sd = 2)))
> 
> # estimate the parameters
> # using the 'poilog' package, assume regular Poisson distribution
> est_01 <- poilogMLE(n = sim_01, zTrunc = FALSE)
> # using the 'poilog' package, assume zero truncated Poisson distribution
> est_02 <- poilogMLE(n = sim_01[sim_01>0], zTrunc = TRUE)
> 
> # make data.frame for glmmTMB estimation
> df_01 <- data.frame(abundance = sim_01,
>                   species = 1:length(sim_01))
> 
> # using the 'glmmTMB' package, assume regular Poisson distribution
> est_03 <- glmmTMB(abundance ~ (1 | species),
>                      family = poisson(link = "log"),
>                      data = df_01)
> # using the 'glmmTMB' package, assume zero truncated Poisson distribution
> est_04 <- glmmTMB(abundance ~ (1 | species),
>                      family = truncated_poisson(link = "log"),
>                      data = df_01[df_01$abundance>0, ])
> 
> # Compare estimates
> est_df <- data.frame(method = rep(c("poilog", "glmmmTMB"), each = 2),
>                     assumptions = rep(c("poisson", "truncated_poisson"), 2),
>                     mu = c(est_01$par[1], est_02$par[1],
>                            est_03$fit$par[1], est_04$fit$par[1]),
>                     sigma = c(est_01$par[2], est_02$par[2],
>                               exp(c(est_03$fit$par[2], est_04$fit$par[2]))))
> 
> est_df
> 
> #### End of code ####
> 
> When I fit the truncated_poisson model in glmmTMB, the mean increases and the standard deviation decreases, which is similar to what happens if you ignore the zeros and fit a regular poisson distribution, but it doesn't seem to be doing quite that either.
> 
> So, I'm hoping anyone knew how to interpret the mean and standard deviation from the truncated_poisson output and how they relate to the mean and standard deviation of a regular poisson model?
> 
> Thank you very much for reading this and any response is much appreciated.
> 
> Best regards,
> 
> Erik
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


	[[alternative HTML version deleted]]


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Sat Apr 25 14:33:29 2020
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Sat, 25 Apr 2020 14:33:29 +0200
Subject: [R-sig-ME] nAGQ > 1 in lme4::glmer gives unexpected likelihood
In-Reply-To: <27717610-083b-ebba-c00e-387acf037e54@gmail.com>
References: <15763_1587596325_0Q9700G9XP5WJU30_CAEbSdx17yikdhCLta4oektu0K44u-jA_E=9Efb9axJ=LsNsruA@mail.gmail.com>
 <31308_1587737137_0Q9A00BAKPTDVU20_CAO7JsnR_TDHYtQDAWOzCp9CBWw7CJ+UfsZk2iTzDWqyQz=dsFw@mail.gmail.com>
 <CAO7JsnTNuNDFMZhJjEq=rRFT8Mj5rXFes3KKOyzFi_Wg7kF=8Q@mail.gmail.com>
 <bcc0a4d0-efab-902a-263a-a8b9e07516eb@gmail.com>
 <41530176-EC7C-4857-9933-7F557A70D412@health.ucsd.edu>
 <27717610-083b-ebba-c00e-387acf037e54@gmail.com>
Message-ID: <24228.11801.175135.545280@stat.math.ethz.ch>


>  ? I think they're both 'correct' in the sense of being proportional to 
> the likelihood but use different baselines, thus incommensurate (see the 
> note pointed out below).

Well, I'm sorry to be a spoiler here, but I think we should
acknowledge that (log) likelihood is uniquely well defined
function.
I remember how we've fought with the many definitions of
deviance and have (in our still unfinished glmm-paper -- hint!!)
very nicely tried to define the many versions
(conditional/unconditional and more), but I find it too sloppy
to claim that likelihoods are only defined up to a constant
factor -- even though of course it doesn't matter of ML (and
REML) if the objective function is "off by constant factor".

Martin

> On 4/24/20 5:05 PM, Vaida, Florin wrote:
> > One can figure out which likelihood version is correct (Laplace or nAGQ>1) by doing a simulation with random effects variance -> 0.
> > I'll do that if I find time.
> > Florin
> >
> >> On Apr 24, 2020, at 7:59 AM, Ben Bolker <bbolker at gmail.com> wrote:
> >>
> >>
> >>     I found the note I was looking for.  In ?deviance.glmerMod it says:
> >>
> >>   If adaptive Gauss-Hermite quadrature is used, then
> >>            ?logLik(object)? is currently only proportional to the
> >>            absolute-unconditional log-likelihood.
> >>
> >> (see the discussion on this page for more context); see also https://github.com/lme4/lme4/blob/master/misc/logLikGLMM/logLikGLMM.R
> >>
> >>
> >> On 4/24/20 10:24 AM, Douglas Bates wrote:
> >>> Having said that, I do see that the fits in the MixedModels package for
> >>> Julia produce similar values of the deviance with the Laplace approximation
> >>> and nAGQ = 7
> >>>
> >>> julia> m1 = fit(MixedModel, @formula(y ~ 1 + (1|group)), dd, Poisson())
> >>> Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)
> >>>    y ~ 1 + (1 | group)
> >>>    Distribution: Poisson{Float64}
> >>>    Link: LogLink()
> >>>
> >>>    Deviance: 193.5587
> >>>
> >>> Variance components:
> >>>           Column    Variance  Std.Dev.
> >>> group (Intercept)  3.9577026 1.9893975
> >>>
> >>>   Number of obs: 100; levels of grouping factors: 10
> >>>
> >>> Fixed-effects parameters:
> >>> ??????????????????????????????????????????????????
> >>>               Estimate  Std.Error  z value  P(>|z|)
> >>> ??????????????????????????????????????????????????
> >>> (Intercept)   2.65175   0.632317     4.19    <1e-4
> >>> ??????????????????????????????????????????????????
> >>>
> >>> julia> m1 = fit(MixedModel, @formula(y ~ 1 + (1|group)), dd, Poisson(),
> >>> nAGQ=7)
> >>> Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 7)
> >>>    y ~ 1 + (1 | group)
> >>>    Distribution: Poisson{Float64}
> >>>    Link: LogLink()
> >>>
> >>>    Deviance: 193.5104
> >>>
> >>> Variance components:
> >>>           Column    Variance  Std.Dev.
> >>> group (Intercept)  3.9577026 1.9893975
> >>>
> >>>   Number of obs: 100; levels of grouping factors: 10
> >>>
> >>> Fixed-effects parameters:
> >>> ??????????????????????????????????????????????????
> >>>               Estimate  Std.Error  z value  P(>|z|)
> >>> ??????????????????????????????????????????????????
> >>> (Intercept)   2.65175   0.632317     4.19    <1e-4
> >>> ??????????????????????????????????????????????????
> >>>
> >>> As the person who wrote the first version of the nAGQ code in R I would not
> >>> be surprised if there was a constant dropped somewhere.  It is difficult
> >>> code.
> >>>
> >>> And the results here in the Julia package make me uncomfortable because the
> >>> values of the parameter estimates are identical in the two fits.  I would
> >>> expect them to be close but not identical.
> >>>
> >>> Isn't it good to know that there is still room for research in this area?
> >>> :-)
> >>>
> >>> On Fri, Apr 24, 2020 at 9:05 AM Douglas Bates <bates at stat.wisc.edu> wrote:
> >>>
> >>>> There's a lot of variability in your lambdas
> >>>>
> >>>>> exp(3 + random_effects)
> >>>>   [1]  91.5919358   6.9678749   4.1841478  78.0771666 890.6931394
> >>>> 20.8558107
> >>>>   [7]   3.0037864   0.3049416   2.1675995  40.6209684
> >>>>
> >>>> Do you really expect that some groups will have a mean count of nearly 900
> >>>> whereas others will have a mean count less than 1?
> >>>>
> >>>>
> >>>> On Wed, Apr 22, 2020 at 5:58 PM Ben Goldstein <ben.goldstein at berkeley.edu>
> >>>> wrote:
> >>>>
> >>>>> Hi all,
> >>>>>
> >>>>> I'm using lme4::glmer to estimate Poisson mixed models in a very simple
> >>>>> context (single random effect). I'm interested in the model
> >>>> likelihood/AIC
> >>>>> across many simulated datasets.
> >>>>>
> >>>>> To investigate whether the Laplace approximation was appropriate for my
> >>>>> data context, I explored using the argument nAGQ to improve the accuracy
> >>>> of
> >>>>> the likelihood estimation. When I changed nAGQ to a value > 1, I saw an
> >>>>> unexpectedly huge change in the likelihood; log-likelihoods tended to be
> >>>>> off by ~200. Other statistics packages (e.g. GLMMadaptive) yield
> >>>> estimates
> >>>>> that agree with lme4's Laplace approximation, as did a manual likelihood
> >>>>> estimate, and not with the nAGQ > 2 estimate.
> >>>>>
> >>>>> The following code reproduces the problem I'm encountering.
> >>>>>
> >>>>> *# r-sig-mixed-models GLMM question*
> >>>>> library(lme4)
> >>>>> set.seed(51)
> >>>>>
> >>>>> *# Simulate some random effect-driven Poisson data*
> >>>>> random_effects <- rnorm(10, 0, 2)
> >>>>> group <- rep(1:10, 10)
> >>>>> simulated_data <- data.frame(y = rpois(n = 100, lambda = exp(3 +
> >>>>> random_effects[group])),
> >>>>>                               group = group)
> >>>>>
> >>>>> *# Fit models with Laplace (nAGQ = 1) and nAGQ = 11*
> >>>>> fit_Laplace <- glmer(y ~ (1|group), data = simulated_data, family =
> >>>>> poisson())
> >>>>> fit_AGQ <- glmer(y ~ (1|group), data = simulated_data, family =
> >>>> poisson(),
> >>>>> nAGQ = 11)
> >>>>>
> >>>>> logLik(fit_Laplace)
> >>>>> logLik(fit_AGQ)
> >>>>> logLik(fit_Laplace) - logLik(fit_AGQ) *# Huge difference!*
> >>>>>
> >>>>> When I execute the above code, I see a difference in likelihood of
> >>>>> -218.8894. I've tested across many simulations and on 2 different
> >>>> machines
> >>>>> (Mac and Linux). My version of lme4 is up to date.
> >>>>>
> >>>>> Has anyone run into this issue before? Am I using the glmer function
> >>>> wrong,
> >>>>> or is it possible there's something going on under the hood?
> >>>>>
> >>>>> Thanks,
> >>>>> Ben
> >>>>>
> >>>>>          [[alternative HTML version deleted]]
> >>>>>
> >>>>> _______________________________________________
> >>>>> R-sig-mixed-models at r-project.org mailing list
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>>>>
> >>>>          [[alternative HTML version deleted]]
> >>>>
> >>>> _______________________________________________
> >>>> R-sig-mixed-models at r-project.org mailing list
> >>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>>>
> >>> 	[[alternative HTML version deleted]]
> >>>
> >>> _______________________________________________
> >>> R-sig-mixed-models at r-project.org mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >> _______________________________________________
> >> R-sig-mixed-models at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From er|k@@o|bu @end|ng |rom n|b|o@no  Sat Apr 25 15:55:00 2020
From: er|k@@o|bu @end|ng |rom n|b|o@no (Erik Solbu)
Date: Sat, 25 Apr 2020 13:55:00 +0000
Subject: [R-sig-ME] Zero-truncated Poisson distribution in glmmTMB
In-Reply-To: <C78B7F7F-D16F-452F-A538-5E6033881E27@gmail.com>
References: <HE1P189MB0473C9E025B72665158D62B0EAD10@HE1P189MB0473.EURP189.PROD.OUTLOOK.COM>,
 <C78B7F7F-D16F-452F-A538-5E6033881E27@gmail.com>
Message-ID: <HE1P189MB04739DD6147263D9DC11049BEAD10@HE1P189MB0473.EURP189.PROD.OUTLOOK.COM>

Thank you for your reply and reference. I see that they're equivalent, but isn't the term '(1|species)' already an OLRE effect?

Regards,

Erik

Last ned Outlook for iOS<https://aka.ms/o0ukef>
________________________________
Fra: Mollie Brooks <mollieebrooks at gmail.com>
Sendt: Saturday, April 25, 2020 12:17:37 PM
Til: Erik Solbu <erik.solbu at nibio.no>
Kopi: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Emne: Re: [R-sig-ME] Zero-truncated Poisson distribution in glmmTMB

The poilogMLE function is fitting a Poisson-lognormal which is equivalent to a Poisson with an observation level random effect (OLRE) to account for overdispersion. So to make the models similar, you probably need to add
df_01$obs = row.names(df_01)
and then add (1|obs) to your model formulas.

Here is a reference about OLREs https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4194460/

cheers,
Mollie


On 25Apr 2020, at 9:24, Erik Solbu <erik.solbu at nibio.no<mailto:erik.solbu at nibio.no>> wrote:

Hi everyone,

I'm trying to replace a package called 'poilog' with glmmTMB, but the results differ in such a way that I'm not sure how to interpret the results from glmmTMB.  I've written a short example below:

#### Start of code ####

# Required packages
library(poilog)
library(glmmTMB)

# Simulate using the 'poilog' package
# sim_01 <- rpoilog(S = 1000, mu = 0, sig = 2, keep0 = TRUE)
# This is the same as
sim_01 <- rpois(n = 1000, lambda = exp(rnorm(n = 1000, mean = 0, sd = 2)))

# estimate the parameters
# using the 'poilog' package, assume regular Poisson distribution
est_01 <- poilogMLE(n = sim_01, zTrunc = FALSE)
# using the 'poilog' package, assume zero truncated Poisson distribution
est_02 <- poilogMLE(n = sim_01[sim_01>0], zTrunc = TRUE)

# make data.frame for glmmTMB estimation
df_01 <- data.frame(abundance = sim_01,
                  species = 1:length(sim_01))

# using the 'glmmTMB' package, assume regular Poisson distribution
est_03 <- glmmTMB(abundance ~ (1 | species),
                     family = poisson(link = "log"),
                     data = df_01)
# using the 'glmmTMB' package, assume zero truncated Poisson distribution
est_04 <- glmmTMB(abundance ~ (1 | species),
                     family = truncated_poisson(link = "log"),
                     data = df_01[df_01$abundance>0, ])

# Compare estimates
est_df <- data.frame(method = rep(c("poilog", "glmmmTMB"), each = 2),
                    assumptions = rep(c("poisson", "truncated_poisson"), 2),
                    mu = c(est_01$par[1], est_02$par[1],
                           est_03$fit$par[1], est_04$fit$par[1]),
                    sigma = c(est_01$par[2], est_02$par[2],
                              exp(c(est_03$fit$par[2], est_04$fit$par[2]))))

est_df

#### End of code ####

When I fit the truncated_poisson model in glmmTMB, the mean increases and the standard deviation decreases, which is similar to what happens if you ignore the zeros and fit a regular poisson distribution, but it doesn't seem to be doing quite that either.

So, I'm hoping anyone knew how to interpret the mean and standard deviation from the truncated_poisson output and how they relate to the mean and standard deviation of a regular poisson model?

Thank you very much for reading this and any response is much appreciated.

Best regards,

Erik



[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


	[[alternative HTML version deleted]]


From |v@|d@ @end|ng |rom he@|th@uc@d@edu  Sat Apr 25 16:10:14 2020
From: |v@|d@ @end|ng |rom he@|th@uc@d@edu (Vaida, Florin)
Date: Sat, 25 Apr 2020 14:10:14 +0000
Subject: [R-sig-ME] nAGQ > 1 in lme4::glmer gives unexpected likelihood
In-Reply-To: <24228.11801.175135.545280@stat.math.ethz.ch>
References: <15763_1587596325_0Q9700G9XP5WJU30_CAEbSdx17yikdhCLta4oektu0K44u-jA_E=9Efb9axJ=LsNsruA@mail.gmail.com>
 <31308_1587737137_0Q9A00BAKPTDVU20_CAO7JsnR_TDHYtQDAWOzCp9CBWw7CJ+UfsZk2iTzDWqyQz=dsFw@mail.gmail.com>
 <CAO7JsnTNuNDFMZhJjEq=rRFT8Mj5rXFes3KKOyzFi_Wg7kF=8Q@mail.gmail.com>
 <bcc0a4d0-efab-902a-263a-a8b9e07516eb@gmail.com>
 <41530176-EC7C-4857-9933-7F557A70D412@health.ucsd.edu>
 <27717610-083b-ebba-c00e-387acf037e54@gmail.com>
 <24228.11801.175135.545280@stat.math.ethz.ch>
Message-ID: <4C53DF97-E0AB-457B-B74D-5BB3272D31B5@health.ucsd.edu>

No spoiler, I agree.
When comparing across multiple models, each with their own "off by" constant, having the exact log-likelihood is important.

> On Apr 25, 2020, at 5:33 AM, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> 
> 
>>   I think they're both 'correct' in the sense of being proportional to 
>> the likelihood but use different baselines, thus incommensurate (see the 
>> note pointed out below).
> 
> Well, I'm sorry to be a spoiler here, but I think we should
> acknowledge that (log) likelihood is uniquely well defined
> function.
> I remember how we've fought with the many definitions of
> deviance and have (in our still unfinished glmm-paper -- hint!!)
> very nicely tried to define the many versions
> (conditional/unconditional and more), but I find it too sloppy
> to claim that likelihoods are only defined up to a constant
> factor -- even though of course it doesn't matter of ML (and
> REML) if the objective function is "off by constant factor".
> 
> Martin
> 
>> On 4/24/20 5:05 PM, Vaida, Florin wrote:
>>> One can figure out which likelihood version is correct (Laplace or nAGQ>1) by doing a simulation with random effects variance -> 0.
>>> I'll do that if I find time.
>>> Florin
>>> 
>>>> On Apr 24, 2020, at 7:59 AM, Ben Bolker <bbolker at gmail.com> wrote:
>>>> 
>>>> 
>>>>    I found the note I was looking for.  In ?deviance.glmerMod it says:
>>>> 
>>>>  If adaptive Gauss-Hermite quadrature is used, then
>>>>           ?logLik(object)? is currently only proportional to the
>>>>           absolute-unconditional log-likelihood.
>>>> 
>>>> (see the discussion on this page for more context); see also https://github.com/lme4/lme4/blob/master/misc/logLikGLMM/logLikGLMM.R
>>>> 
>>>> 
>>>> On 4/24/20 10:24 AM, Douglas Bates wrote:
>>>>> Having said that, I do see that the fits in the MixedModels package for
>>>>> Julia produce similar values of the deviance with the Laplace approximation
>>>>> and nAGQ = 7
>>>>> 
>>>>> julia> m1 = fit(MixedModel, @formula(y ~ 1 + (1|group)), dd, Poisson())
>>>>> Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)
>>>>>   y ~ 1 + (1 | group)
>>>>>   Distribution: Poisson{Float64}
>>>>>   Link: LogLink()
>>>>> 
>>>>>   Deviance: 193.5587
>>>>> 
>>>>> Variance components:
>>>>>          Column    Variance  Std.Dev.
>>>>> group (Intercept)  3.9577026 1.9893975
>>>>> 
>>>>>  Number of obs: 100; levels of grouping factors: 10
>>>>> 
>>>>> Fixed-effects parameters:
>>>>> ??????????????????????????????????????????????????
>>>>>              Estimate  Std.Error  z value  P(>|z|)
>>>>> ??????????????????????????????????????????????????
>>>>> (Intercept)   2.65175   0.632317     4.19    <1e-4
>>>>> ??????????????????????????????????????????????????
>>>>> 
>>>>> julia> m1 = fit(MixedModel, @formula(y ~ 1 + (1|group)), dd, Poisson(),
>>>>> nAGQ=7)
>>>>> Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 7)
>>>>>   y ~ 1 + (1 | group)
>>>>>   Distribution: Poisson{Float64}
>>>>>   Link: LogLink()
>>>>> 
>>>>>   Deviance: 193.5104
>>>>> 
>>>>> Variance components:
>>>>>          Column    Variance  Std.Dev.
>>>>> group (Intercept)  3.9577026 1.9893975
>>>>> 
>>>>>  Number of obs: 100; levels of grouping factors: 10
>>>>> 
>>>>> Fixed-effects parameters:
>>>>> ??????????????????????????????????????????????????
>>>>>              Estimate  Std.Error  z value  P(>|z|)
>>>>> ??????????????????????????????????????????????????
>>>>> (Intercept)   2.65175   0.632317     4.19    <1e-4
>>>>> ??????????????????????????????????????????????????
>>>>> 
>>>>> As the person who wrote the first version of the nAGQ code in R I would not
>>>>> be surprised if there was a constant dropped somewhere.  It is difficult
>>>>> code.
>>>>> 
>>>>> And the results here in the Julia package make me uncomfortable because the
>>>>> values of the parameter estimates are identical in the two fits.  I would
>>>>> expect them to be close but not identical.
>>>>> 
>>>>> Isn't it good to know that there is still room for research in this area?
>>>>> :-)
>>>>> 
>>>>> On Fri, Apr 24, 2020 at 9:05 AM Douglas Bates <bates at stat.wisc.edu> wrote:
>>>>> 
>>>>>> There's a lot of variability in your lambdas
>>>>>> 
>>>>>>> exp(3 + random_effects)
>>>>>>  [1]  91.5919358   6.9678749   4.1841478  78.0771666 890.6931394
>>>>>> 20.8558107
>>>>>>  [7]   3.0037864   0.3049416   2.1675995  40.6209684
>>>>>> 
>>>>>> Do you really expect that some groups will have a mean count of nearly 900
>>>>>> whereas others will have a mean count less than 1?
>>>>>> 
>>>>>> 
>>>>>> On Wed, Apr 22, 2020 at 5:58 PM Ben Goldstein <ben.goldstein at berkeley.edu>
>>>>>> wrote:
>>>>>> 
>>>>>>> Hi all,
>>>>>>> 
>>>>>>> I'm using lme4::glmer to estimate Poisson mixed models in a very simple
>>>>>>> context (single random effect). I'm interested in the model
>>>>>> likelihood/AIC
>>>>>>> across many simulated datasets.
>>>>>>> 
>>>>>>> To investigate whether the Laplace approximation was appropriate for my
>>>>>>> data context, I explored using the argument nAGQ to improve the accuracy
>>>>>> of
>>>>>>> the likelihood estimation. When I changed nAGQ to a value > 1, I saw an
>>>>>>> unexpectedly huge change in the likelihood; log-likelihoods tended to be
>>>>>>> off by ~200. Other statistics packages (e.g. GLMMadaptive) yield
>>>>>> estimates
>>>>>>> that agree with lme4's Laplace approximation, as did a manual likelihood
>>>>>>> estimate, and not with the nAGQ > 2 estimate.
>>>>>>> 
>>>>>>> The following code reproduces the problem I'm encountering.
>>>>>>> 
>>>>>>> *# r-sig-mixed-models GLMM question*
>>>>>>> library(lme4)
>>>>>>> set.seed(51)
>>>>>>> 
>>>>>>> *# Simulate some random effect-driven Poisson data*
>>>>>>> random_effects <- rnorm(10, 0, 2)
>>>>>>> group <- rep(1:10, 10)
>>>>>>> simulated_data <- data.frame(y = rpois(n = 100, lambda = exp(3 +
>>>>>>> random_effects[group])),
>>>>>>>                              group = group)
>>>>>>> 
>>>>>>> *# Fit models with Laplace (nAGQ = 1) and nAGQ = 11*
>>>>>>> fit_Laplace <- glmer(y ~ (1|group), data = simulated_data, family =
>>>>>>> poisson())
>>>>>>> fit_AGQ <- glmer(y ~ (1|group), data = simulated_data, family =
>>>>>> poisson(),
>>>>>>> nAGQ = 11)
>>>>>>> 
>>>>>>> logLik(fit_Laplace)
>>>>>>> logLik(fit_AGQ)
>>>>>>> logLik(fit_Laplace) - logLik(fit_AGQ) *# Huge difference!*
>>>>>>> 
>>>>>>> When I execute the above code, I see a difference in likelihood of
>>>>>>> -218.8894. I've tested across many simulations and on 2 different
>>>>>> machines
>>>>>>> (Mac and Linux). My version of lme4 is up to date.
>>>>>>> 
>>>>>>> Has anyone run into this issue before? Am I using the glmer function
>>>>>> wrong,
>>>>>>> or is it possible there's something going on under the hood?
>>>>>>> 
>>>>>>> Thanks,
>>>>>>> Ben
>>>>>>> 
>>>>>>>         [[alternative HTML version deleted]]
>>>>>>> 
>>>>>>> _______________________________________________
>>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>> 
>>>>>>         [[alternative HTML version deleted]]
>>>>>> 
>>>>>> _______________________________________________
>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>> 
>>>>> 	[[alternative HTML version deleted]]
>>>>> 
>>>>> _______________________________________________
>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From mo|||eebrook@ @end|ng |rom gm@||@com  Sat Apr 25 16:29:18 2020
From: mo|||eebrook@ @end|ng |rom gm@||@com (Mollie Brooks)
Date: Sat, 25 Apr 2020 16:29:18 +0200
Subject: [R-sig-ME] Zero-truncated Poisson distribution in glmmTMB
In-Reply-To: <HE1P189MB04739DD6147263D9DC11049BEAD10@HE1P189MB0473.EURP189.PROD.OUTLOOK.COM>
References: <HE1P189MB0473C9E025B72665158D62B0EAD10@HE1P189MB0473.EURP189.PROD.OUTLOOK.COM>
 <C78B7F7F-D16F-452F-A538-5E6033881E27@gmail.com>
 <HE1P189MB04739DD6147263D9DC11049BEAD10@HE1P189MB0473.EURP189.PROD.OUTLOOK.COM>
Message-ID: <1D4AD66D-B46F-4112-A5B0-74756D844698@gmail.com>

Yes, I see that now. Then I?m not sure what the difference is.

Mollie

> On 25Apr 2020, at 15:55, Erik Solbu <erik.solbu at nibio.no> wrote:
> 
> Thank you for your reply and reference. I see that they're equivalent, but isn?t the term ?(1|species)? already an OLRE effect?
>  
> Regards,
>  
> Erik
>  
> Last ned Outlook for iOS <https://aka.ms/o0ukef>
> Fra: Mollie Brooks <mollieebrooks at gmail.com>
> Sendt: Saturday, April 25, 2020 12:17:37 PM
> Til: Erik Solbu <erik.solbu at nibio.no>
> Kopi: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
> Emne: Re: [R-sig-ME] Zero-truncated Poisson distribution in glmmTMB
>  
> The poilogMLE function is fitting a Poisson-lognormal which is equivalent to a Poisson with an observation level random effect (OLRE) to account for overdispersion. So to make the models similar, you probably need to add
> df_01$obs = row.names(df_01)
> and then add (1|obs) to your model formulas. 
>   
> Here is a reference about OLREs https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4194460/ <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4194460/>
>  
> cheers,
> Mollie
> 
> 
> On 25Apr 2020, at 9:24, Erik Solbu <erik.solbu at nibio.no <mailto:erik.solbu at nibio.no>> wrote:
>  
> Hi everyone,
> 
> I'm trying to replace a package called 'poilog' with glmmTMB, but the results differ in such a way that I'm not sure how to interpret the results from glmmTMB.  I've written a short example below:
> 
> #### Start of code ####
> 
> # Required packages
> library(poilog)
> library(glmmTMB)
> 
> # Simulate using the 'poilog' package
> # sim_01 <- rpoilog(S = 1000, mu = 0, sig = 2, keep0 = TRUE)
> # This is the same as
> sim_01 <- rpois(n = 1000, lambda = exp(rnorm(n = 1000, mean = 0, sd = 2)))
> 
> # estimate the parameters
> # using the 'poilog' package, assume regular Poisson distribution
> est_01 <- poilogMLE(n = sim_01, zTrunc = FALSE)
> # using the 'poilog' package, assume zero truncated Poisson distribution
> est_02 <- poilogMLE(n = sim_01[sim_01>0], zTrunc = TRUE)
> 
> # make data.frame for glmmTMB estimation
> df_01 <- data.frame(abundance = sim_01,
>                   species = 1:length(sim_01))
> 
> # using the 'glmmTMB' package, assume regular Poisson distribution
> est_03 <- glmmTMB(abundance ~ (1 | species),
>                      family = poisson(link = "log"),
>                      data = df_01)
> # using the 'glmmTMB' package, assume zero truncated Poisson distribution
> est_04 <- glmmTMB(abundance ~ (1 | species),
>                      family = truncated_poisson(link = "log"),
>                      data = df_01[df_01$abundance>0, ])
> 
> # Compare estimates
> est_df <- data.frame(method = rep(c("poilog", "glmmmTMB"), each = 2),
>                     assumptions = rep(c("poisson", "truncated_poisson"), 2),
>                     mu = c(est_01$par[1], est_02$par[1],
>                            est_03$fit$par[1], est_04$fit$par[1]),
>                     sigma = c(est_01$par[2], est_02$par[2],
>                               exp(c(est_03$fit$par[2], est_04$fit$par[2]))))
> 
> est_df
> 
> #### End of code ####
> 
> When I fit the truncated_poisson model in glmmTMB, the mean increases and the standard deviation decreases, which is similar to what happens if you ignore the zeros and fit a regular poisson distribution, but it doesn't seem to be doing quite that either.
> 
> So, I'm hoping anyone knew how to interpret the mean and standard deviation from the truncated_poisson output and how they relate to the mean and standard deviation of a regular poisson model?
> 
> Thank you very much for reading this and any response is much appreciated.
> 
> Best regards,
> 
> Erik
> 
> 
> 
> [[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-models at r-project.org> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>

	[[alternative HTML version deleted]]


From |v@|d@ @end|ng |rom he@|th@uc@d@edu  Sat Apr 25 16:40:06 2020
From: |v@|d@ @end|ng |rom he@|th@uc@d@edu (Vaida, Florin)
Date: Sat, 25 Apr 2020 14:40:06 +0000
Subject: [R-sig-ME] nAGQ > 1 in lme4::glmer gives unexpected likelihood
In-Reply-To: <CAO7JsnTNuNDFMZhJjEq=rRFT8Mj5rXFes3KKOyzFi_Wg7kF=8Q@mail.gmail.com>
References: <15763_1587596325_0Q9700G9XP5WJU30_CAEbSdx17yikdhCLta4oektu0K44u-jA_E=9Efb9axJ=LsNsruA@mail.gmail.com>
 <31308_1587737137_0Q9A00BAKPTDVU20_CAO7JsnR_TDHYtQDAWOzCp9CBWw7CJ+UfsZk2iTzDWqyQz=dsFw@mail.gmail.com>
 <CAO7JsnTNuNDFMZhJjEq=rRFT8Mj5rXFes3KKOyzFi_Wg7kF=8Q@mail.gmail.com>
Message-ID: <B6E177DB-4D1F-4B00-8A60-41F6B2B887E8@health.ucsd.edu>

Interestingly (and reassuringly), Laplace and nAGQ give consistent results for binomial GLMM.
Could there be a glitch for Poisson GLMM?

#### Likelihood of Binomial GLMM Models

# r-sig-mixed-models GLMM question*
library(lme4)
set.seed(51)

# Simulate some random effect-driven Poisson data*
# random_effects <- rnorm(10, 0, 2)
# random_effects <- rnorm(10, 0, 0.01)
random_effects <- rnorm(10, 0, 1)

group <- rep(1:10, 10)
simulated_data <- data.frame(y = rbinom(n = 100, size = 1, 
    prob = exp(2+random_effects[group])/(1+exp(2+random_effects[group]))), group = group)

# Fit models with Laplace (nAGQ = 1) and nAGQ = 11*
fit_Laplace <- glmer(y ~ (1|group), data = simulated_data, family =
                       binomial())
fit_AGQ <- glmer(y ~ (1|group), data = simulated_data, family = binomial(),
                 nAGQ = 11)
# fit_glm = glm(y ~ group, data=simulated_data, family=binomial)

(logLik(fit_Laplace))
(logLik(fit_AGQ))  # very similar to fit_Laplace
# (logLik(fit_glm)) 


> On Apr 24, 2020, at 7:24 AM, Douglas Bates <bates at stat.wisc.edu> wrote:
> 
> Having said that, I do see that the fits in the MixedModels package for
> Julia produce similar values of the deviance with the Laplace approximation
> and nAGQ = 7
> 
> julia> m1 = fit(MixedModel, @formula(y ~ 1 + (1|group)), dd, Poisson())
> Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)
>  y ~ 1 + (1 | group)
>  Distribution: Poisson{Float64}
>  Link: LogLink()
> 
>  Deviance: 193.5587
> 
> Variance components:
>         Column    Variance  Std.Dev.
> group (Intercept)  3.9577026 1.9893975
> 
> Number of obs: 100; levels of grouping factors: 10
> 
> Fixed-effects parameters:
> ??????????????????????????????????????????????????
>             Estimate  Std.Error  z value  P(>|z|)
> ??????????????????????????????????????????????????
> (Intercept)   2.65175   0.632317     4.19    <1e-4
> ??????????????????????????????????????????????????
> 
> julia> m1 = fit(MixedModel, @formula(y ~ 1 + (1|group)), dd, Poisson(),
> nAGQ=7)
> Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 7)
>  y ~ 1 + (1 | group)
>  Distribution: Poisson{Float64}
>  Link: LogLink()
> 
>  Deviance: 193.5104
> 
> Variance components:
>         Column    Variance  Std.Dev.
> group (Intercept)  3.9577026 1.9893975
> 
> Number of obs: 100; levels of grouping factors: 10
> 
> Fixed-effects parameters:
> ??????????????????????????????????????????????????
>             Estimate  Std.Error  z value  P(>|z|)
> ??????????????????????????????????????????????????
> (Intercept)   2.65175   0.632317     4.19    <1e-4
> ??????????????????????????????????????????????????
> 
> As the person who wrote the first version of the nAGQ code in R I would not
> be surprised if there was a constant dropped somewhere.  It is difficult
> code.
> 
> And the results here in the Julia package make me uncomfortable because the
> values of the parameter estimates are identical in the two fits.  I would
> expect them to be close but not identical.
> 
> Isn't it good to know that there is still room for research in this area?
> :-)
> 
> On Fri, Apr 24, 2020 at 9:05 AM Douglas Bates <bates at stat.wisc.edu> wrote:
> 
>> There's a lot of variability in your lambdas
>> 
>>> exp(3 + random_effects)
>> [1]  91.5919358   6.9678749   4.1841478  78.0771666 890.6931394
>> 20.8558107
>> [7]   3.0037864   0.3049416   2.1675995  40.6209684
>> 
>> Do you really expect that some groups will have a mean count of nearly 900
>> whereas others will have a mean count less than 1?
>> 
>> 
>> On Wed, Apr 22, 2020 at 5:58 PM Ben Goldstein <ben.goldstein at berkeley.edu>
>> wrote:
>> 
>>> Hi all,
>>> 
>>> I'm using lme4::glmer to estimate Poisson mixed models in a very simple
>>> context (single random effect). I'm interested in the model
>> likelihood/AIC
>>> across many simulated datasets.
>>> 
>>> To investigate whether the Laplace approximation was appropriate for my
>>> data context, I explored using the argument nAGQ to improve the accuracy
>> of
>>> the likelihood estimation. When I changed nAGQ to a value > 1, I saw an
>>> unexpectedly huge change in the likelihood; log-likelihoods tended to be
>>> off by ~200. Other statistics packages (e.g. GLMMadaptive) yield
>> estimates
>>> that agree with lme4's Laplace approximation, as did a manual likelihood
>>> estimate, and not with the nAGQ > 2 estimate.
>>> 
>>> The following code reproduces the problem I'm encountering.
>>> 
>>> *# r-sig-mixed-models GLMM question*
>>> library(lme4)
>>> set.seed(51)
>>> 
>>> *# Simulate some random effect-driven Poisson data*
>>> random_effects <- rnorm(10, 0, 2)
>>> group <- rep(1:10, 10)
>>> simulated_data <- data.frame(y = rpois(n = 100, lambda = exp(3 +
>>> random_effects[group])),
>>>                             group = group)
>>> 
>>> *# Fit models with Laplace (nAGQ = 1) and nAGQ = 11*
>>> fit_Laplace <- glmer(y ~ (1|group), data = simulated_data, family =
>>> poisson())
>>> fit_AGQ <- glmer(y ~ (1|group), data = simulated_data, family =
>> poisson(),
>>> nAGQ = 11)
>>> 
>>> logLik(fit_Laplace)
>>> logLik(fit_AGQ)
>>> logLik(fit_Laplace) - logLik(fit_AGQ) *# Huge difference!*
>>> 
>>> When I execute the above code, I see a difference in likelihood of
>>> -218.8894. I've tested across many simulations and on 2 different
>> machines
>>> (Mac and Linux). My version of lme4 is up to date.
>>> 
>>> Has anyone run into this issue before? Am I using the glmer function
>> wrong,
>>> or is it possible there's something going on under the hood?
>>> 
>>> Thanks,
>>> Ben
>>> 
>>>        [[alternative HTML version deleted]]
>>> 
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>> 
>> 
>>        [[alternative HTML version deleted]]
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From |v@|d@ @end|ng |rom he@|th@uc@d@edu  Sat Apr 25 16:41:01 2020
From: |v@|d@ @end|ng |rom he@|th@uc@d@edu (Vaida, Florin)
Date: Sat, 25 Apr 2020 14:41:01 +0000
Subject: [R-sig-ME] nAGQ > 1 in lme4::glmer gives unexpected likelihood
In-Reply-To: <bcc0a4d0-efab-902a-263a-a8b9e07516eb@gmail.com>
References: <15763_1587596325_0Q9700G9XP5WJU30_CAEbSdx17yikdhCLta4oektu0K44u-jA_E=9Efb9axJ=LsNsruA@mail.gmail.com>
 <31308_1587737137_0Q9A00BAKPTDVU20_CAO7JsnR_TDHYtQDAWOzCp9CBWw7CJ+UfsZk2iTzDWqyQz=dsFw@mail.gmail.com>
 <CAO7JsnTNuNDFMZhJjEq=rRFT8Mj5rXFes3KKOyzFi_Wg7kF=8Q@mail.gmail.com>
 <bcc0a4d0-efab-902a-263a-a8b9e07516eb@gmail.com>
Message-ID: <64153415-EF54-4FFD-A6F4-53E4A2AFEE64@health.ucsd.edu>

Looks like you guys thought about deviance issues a lot:
https://github.com/lme4/lme4/issues/375

Speaking about the principle of "least surprise", it is surprising to get completely different behaviors in deviance for nAGQ=1 vs nAGQ=2 (say), in the same model - (what Ben Goldstein pointed out).

A couple further questions:
1. Is the "off by" constant the same, for a given model, if I only change nAGQ?  E.g., comparing nAGQ=5 and nAGQ=20?
2. If so, can you calculate the "off by" constant by comparing with formula-based Laplace approximation with a bona fide Adaptive Gaussian Quadrature using existing algorithm but n=1?

Florin

On Apr 24, 2020, at 7:59 AM, Ben Bolker <bbolker at gmail.com<mailto:bbolker at gmail.com>> wrote:


   I found the note I was looking for.  In ?deviance.glmerMod it says:

 If adaptive Gauss-Hermite quadrature is used, then
          ?logLik(object)? is currently only proportional to the
          absolute-unconditional log-likelihood.

(see the discussion on this page for more context); see also https://github.com/lme4/lme4/blob/master/misc/logLikGLMM/logLikGLMM.R


On 4/24/20 10:24 AM, Douglas Bates wrote:
Having said that, I do see that the fits in the MixedModels package for
Julia produce similar values of the deviance with the Laplace approximation
and nAGQ = 7

julia> m1 = fit(MixedModel, @formula(y ~ 1 + (1|group)), dd, Poisson())
Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)
  y ~ 1 + (1 | group)
  Distribution: Poisson{Float64}
  Link: LogLink()

  Deviance: 193.5587

Variance components:
         Column    Variance  Std.Dev.
group (Intercept)  3.9577026 1.9893975

 Number of obs: 100; levels of grouping factors: 10

Fixed-effects parameters:
??????????????????????????????????????????????????
             Estimate  Std.Error  z value  P(>|z|)
??????????????????????????????????????????????????
(Intercept)   2.65175   0.632317     4.19    <1e-4
??????????????????????????????????????????????????

julia> m1 = fit(MixedModel, @formula(y ~ 1 + (1|group)), dd, Poisson(),
nAGQ=7)
Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 7)
  y ~ 1 + (1 | group)
  Distribution: Poisson{Float64}
  Link: LogLink()

  Deviance: 193.5104

Variance components:
         Column    Variance  Std.Dev.
group (Intercept)  3.9577026 1.9893975

 Number of obs: 100; levels of grouping factors: 10

Fixed-effects parameters:
??????????????????????????????????????????????????
             Estimate  Std.Error  z value  P(>|z|)
??????????????????????????????????????????????????
(Intercept)   2.65175   0.632317     4.19    <1e-4
??????????????????????????????????????????????????

As the person who wrote the first version of the nAGQ code in R I would not
be surprised if there was a constant dropped somewhere.  It is difficult
code.

And the results here in the Julia package make me uncomfortable because the
values of the parameter estimates are identical in the two fits.  I would
expect them to be close but not identical.

Isn't it good to know that there is still room for research in this area?
:-)

On Fri, Apr 24, 2020 at 9:05 AM Douglas Bates <bates at stat.wisc.edu<mailto:bates at stat.wisc.edu>> wrote:

There's a lot of variability in your lambdas

exp(3 + random_effects)
 [1]  91.5919358   6.9678749   4.1841478  78.0771666 890.6931394
20.8558107
 [7]   3.0037864   0.3049416   2.1675995  40.6209684

Do you really expect that some groups will have a mean count of nearly 900
whereas others will have a mean count less than 1?


On Wed, Apr 22, 2020 at 5:58 PM Ben Goldstein <ben.goldstein at berkeley.edu<mailto:ben.goldstein at berkeley.edu>>
wrote:

Hi all,

I'm using lme4::glmer to estimate Poisson mixed models in a very simple
context (single random effect). I'm interested in the model
likelihood/AIC
across many simulated datasets.

To investigate whether the Laplace approximation was appropriate for my
data context, I explored using the argument nAGQ to improve the accuracy
of
the likelihood estimation. When I changed nAGQ to a value > 1, I saw an
unexpectedly huge change in the likelihood; log-likelihoods tended to be
off by ~200. Other statistics packages (e.g. GLMMadaptive) yield
estimates
that agree with lme4's Laplace approximation, as did a manual likelihood
estimate, and not with the nAGQ > 2 estimate.

The following code reproduces the problem I'm encountering.

*# r-sig-mixed-models GLMM question*
library(lme4)
set.seed(51)

*# Simulate some random effect-driven Poisson data*
random_effects <- rnorm(10, 0, 2)
group <- rep(1:10, 10)
simulated_data <- data.frame(y = rpois(n = 100, lambda = exp(3 +
random_effects[group])),
                             group = group)

*# Fit models with Laplace (nAGQ = 1) and nAGQ = 11*
fit_Laplace <- glmer(y ~ (1|group), data = simulated_data, family =
poisson())
fit_AGQ <- glmer(y ~ (1|group), data = simulated_data, family =
poisson(),
nAGQ = 11)

logLik(fit_Laplace)
logLik(fit_AGQ)
logLik(fit_Laplace) - logLik(fit_AGQ) *# Huge difference!*

When I execute the above code, I see a difference in likelihood of
-218.8894. I've tested across many simulations and on 2 different
machines
(Mac and Linux). My version of lme4 is up to date.

Has anyone run into this issue before? Am I using the glmer function
wrong,
or is it possible there's something going on under the hood?

Thanks,
Ben

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


	[[alternative HTML version deleted]]


From biii m@iii@g oii de@@ey@ws  Sat Apr 25 16:48:52 2020
From: biii m@iii@g oii de@@ey@ws (biii m@iii@g oii de@@ey@ws)
Date: Sat, 25 Apr 2020 10:48:52 -0400
Subject: [R-sig-ME] Partial Argument Matching in nlme::varFunc
Message-ID: <010f01d61b10$a3d48950$eb7d9bf0$@denney.ws>

Hi,

 

Recently, I have turned on warnings for partial argument name matching, and
with that, I'm finding many warnings.  One is while summarizing models
within the nlme library.

 

An example that causes this is:

 

```

library(nlme)

fm1 <- gls(follicles ~ sin(2*pi*Time) + cos(2*pi*Time), Ovary,

           correlation = corAR1(form = ~ 1 | Mare),

           weights = varIdent(form=~Mare))

summary(fm1)

```

 

Which gives the warning:

```

Warning message:

In coef.varIdent(x, uncons = FALSE, allCoef = TRUE) :

  partial argument match of 'uncons' to 'unconstrained'

```

 

This appears to be fixed by the diff I put into an R bug report here:
https://bugs.r-project.org/bugzilla/show_bug.cgi?id=17767

 

I assume that this list has the right people to review and apply the patch.


Thanks,

 

Bill


	[[alternative HTML version deleted]]


From b@te@ @end|ng |rom @t@t@w|@c@edu  Sat Apr 25 17:29:06 2020
From: b@te@ @end|ng |rom @t@t@w|@c@edu (Douglas Bates)
Date: Sat, 25 Apr 2020 10:29:06 -0500
Subject: [R-sig-ME] nAGQ > 1 in lme4::glmer gives unexpected likelihood
In-Reply-To: <10353_1587825701_0Q9C005H2M5G1530_64153415-EF54-4FFD-A6F4-53E4A2AFEE64@health.ucsd.edu>
References: <15763_1587596325_0Q9700G9XP5WJU30_CAEbSdx17yikdhCLta4oektu0K44u-jA_E=9Efb9axJ=LsNsruA@mail.gmail.com>
 <31308_1587737137_0Q9A00BAKPTDVU20_CAO7JsnR_TDHYtQDAWOzCp9CBWw7CJ+UfsZk2iTzDWqyQz=dsFw@mail.gmail.com>
 <CAO7JsnTNuNDFMZhJjEq=rRFT8Mj5rXFes3KKOyzFi_Wg7kF=8Q@mail.gmail.com>
 <bcc0a4d0-efab-902a-263a-a8b9e07516eb@gmail.com>
 <10353_1587825701_0Q9C005H2M5G1530_64153415-EF54-4FFD-A6F4-53E4A2AFEE64@health.ucsd.edu>
Message-ID: <CAO7JsnRXWUiV8KGVi6wfA2FemtWAaqHBSZ=rLfisvw2eUMGZkA@mail.gmail.com>

There shouldn't be discussions of "off by" with respect to the
log-likelihood. As Martin pointed out, if you know the probability model
then you have one and only one likelihood.  In the case of a generalized
linear mixed model the likelihood is well-defined but does not have a
closed-form expression because the integral with respect to the random
effects doesn't have a closed-form expression.  Nevertheless the likelihood
is, in the mathematical sense, well-defined.  So the only question about
different ways of approximating the log-likelihood is how close it gets to
this well-defined value.


On Sat, Apr 25, 2020 at 9:41 AM <fvaida at health.ucsd.edu> wrote:

> Looks like you guys thought about deviance issues a lot:
> https://github.com/lme4/lme4/issues/375
>
> Speaking about the principle of "least surprise", it is surprising to get
> completely different behaviors in deviance for nAGQ=1 vs nAGQ=2 (say), in
> the same model - (what Ben Goldstein pointed out).
>
> A couple further questions:
> 1. Is the "off by" constant the same, for a given model, if I only change
> nAGQ?  E.g., comparing nAGQ=5 and nAGQ=20?
> 2. If so, can you calculate the "off by" constant by comparing with
> formula-based Laplace approximation with a bona fide Adaptive Gaussian
> Quadrature using existing algorithm but n=1?
>
> Florin
>
> On Apr 24, 2020, at 7:59 AM, Ben Bolker <bbolker at gmail.com<mailto:
> bbolker at gmail.com>> wrote:
>
>
>    I found the note I was looking for.  In ?deviance.glmerMod it says:
>
>  If adaptive Gauss-Hermite quadrature is used, then
>           ?logLik(object)? is currently only proportional to the
>           absolute-unconditional log-likelihood.
>
> (see the discussion on this page for more context); see also
> https://github.com/lme4/lme4/blob/master/misc/logLikGLMM/logLikGLMM.R
>
>
> On 4/24/20 10:24 AM, Douglas Bates wrote:
> Having said that, I do see that the fits in the MixedModels package for
> Julia produce similar values of the deviance with the Laplace approximation
> and nAGQ = 7
>
> julia> m1 = fit(MixedModel, @formula(y ~ 1 + (1|group)), dd, Poisson())
> Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)
>   y ~ 1 + (1 | group)
>   Distribution: Poisson{Float64}
>   Link: LogLink()
>
>   Deviance: 193.5587
>
> Variance components:
>          Column    Variance  Std.Dev.
> group (Intercept)  3.9577026 1.9893975
>
>  Number of obs: 100; levels of grouping factors: 10
>
> Fixed-effects parameters:
> ??????????????????????????????????????????????????
>              Estimate  Std.Error  z value  P(>|z|)
> ??????????????????????????????????????????????????
> (Intercept)   2.65175   0.632317     4.19    <1e-4
> ??????????????????????????????????????????????????
>
> julia> m1 = fit(MixedModel, @formula(y ~ 1 + (1|group)), dd, Poisson(),
> nAGQ=7)
> Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 7)
>   y ~ 1 + (1 | group)
>   Distribution: Poisson{Float64}
>   Link: LogLink()
>
>   Deviance: 193.5104
>
> Variance components:
>          Column    Variance  Std.Dev.
> group (Intercept)  3.9577026 1.9893975
>
>  Number of obs: 100; levels of grouping factors: 10
>
> Fixed-effects parameters:
> ??????????????????????????????????????????????????
>              Estimate  Std.Error  z value  P(>|z|)
> ??????????????????????????????????????????????????
> (Intercept)   2.65175   0.632317     4.19    <1e-4
> ??????????????????????????????????????????????????
>
> As the person who wrote the first version of the nAGQ code in R I would not
> be surprised if there was a constant dropped somewhere.  It is difficult
> code.
>
> And the results here in the Julia package make me uncomfortable because the
> values of the parameter estimates are identical in the two fits.  I would
> expect them to be close but not identical.
>
> Isn't it good to know that there is still room for research in this area?
> :-)
>
> On Fri, Apr 24, 2020 at 9:05 AM Douglas Bates <bates at stat.wisc.edu<mailto:
> bates at stat.wisc.edu>> wrote:
>
> There's a lot of variability in your lambdas
>
> exp(3 + random_effects)
>  [1]  91.5919358   6.9678749   4.1841478  78.0771666 890.6931394
> 20.8558107
>  [7]   3.0037864   0.3049416   2.1675995  40.6209684
>
> Do you really expect that some groups will have a mean count of nearly 900
> whereas others will have a mean count less than 1?
>
>
> On Wed, Apr 22, 2020 at 5:58 PM Ben Goldstein <ben.goldstein at berkeley.edu
> <mailto:ben.goldstein at berkeley.edu>>
> wrote:
>
> Hi all,
>
> I'm using lme4::glmer to estimate Poisson mixed models in a very simple
> context (single random effect). I'm interested in the model
> likelihood/AIC
> across many simulated datasets.
>
> To investigate whether the Laplace approximation was appropriate for my
> data context, I explored using the argument nAGQ to improve the accuracy
> of
> the likelihood estimation. When I changed nAGQ to a value > 1, I saw an
> unexpectedly huge change in the likelihood; log-likelihoods tended to be
> off by ~200. Other statistics packages (e.g. GLMMadaptive) yield
> estimates
> that agree with lme4's Laplace approximation, as did a manual likelihood
> estimate, and not with the nAGQ > 2 estimate.
>
> The following code reproduces the problem I'm encountering.
>
> *# r-sig-mixed-models GLMM question*
> library(lme4)
> set.seed(51)
>
> *# Simulate some random effect-driven Poisson data*
> random_effects <- rnorm(10, 0, 2)
> group <- rep(1:10, 10)
> simulated_data <- data.frame(y = rpois(n = 100, lambda = exp(3 +
> random_effects[group])),
>                              group = group)
>
> *# Fit models with Laplace (nAGQ = 1) and nAGQ = 11*
> fit_Laplace <- glmer(y ~ (1|group), data = simulated_data, family =
> poisson())
> fit_AGQ <- glmer(y ~ (1|group), data = simulated_data, family =
> poisson(),
> nAGQ = 11)
>
> logLik(fit_Laplace)
> logLik(fit_AGQ)
> logLik(fit_Laplace) - logLik(fit_AGQ) *# Huge difference!*
>
> When I execute the above code, I see a difference in likelihood of
> -218.8894. I've tested across many simulations and on 2 different
> machines
> (Mac and Linux). My version of lme4 is up to date.
>
> Has anyone run into this issue before? Am I using the glmer function
> wrong,
> or is it possible there's something going on under the hood?
>
> Thanks,
> Ben
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org>
> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org>
> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org>
> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org>
> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From r@turner @end|ng |rom @uck|@nd@@c@nz  Sun Apr 26 01:23:22 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sun, 26 Apr 2020 11:23:22 +1200
Subject: [R-sig-ME] nAGQ > 1 in lme4::glmer gives unexpected likelihood
In-Reply-To: <24228.11801.175135.545280@stat.math.ethz.ch>
References: <15763_1587596325_0Q9700G9XP5WJU30_CAEbSdx17yikdhCLta4oektu0K44u-jA_E=9Efb9axJ=LsNsruA@mail.gmail.com>
 <31308_1587737137_0Q9A00BAKPTDVU20_CAO7JsnR_TDHYtQDAWOzCp9CBWw7CJ+UfsZk2iTzDWqyQz=dsFw@mail.gmail.com>
 <CAO7JsnTNuNDFMZhJjEq=rRFT8Mj5rXFes3KKOyzFi_Wg7kF=8Q@mail.gmail.com>
 <bcc0a4d0-efab-902a-263a-a8b9e07516eb@gmail.com>
 <41530176-EC7C-4857-9933-7F557A70D412@health.ucsd.edu>
 <27717610-083b-ebba-c00e-387acf037e54@gmail.com>
 <24228.11801.175135.545280@stat.math.ethz.ch>
Message-ID: <14a5f0ec-09eb-784a-2752-5c73ae1f93e3@auckland.ac.nz>


On 26/04/20 12:33 am, Martin Maechler wrote:

> 
>>   ? I think they're both 'correct' in the sense of being proportional to
>> the likelihood but use different baselines, thus incommensurate (see the
>> note pointed out below).
> 
> Well, I'm sorry to be a spoiler here, but I think we should
> acknowledge that (log) likelihood is uniquely well defined
> function.
> I remember how we've fought with the many definitions of
> deviance and have (in our still unfinished glmm-paper -- hint!!)
> very nicely tried to define the many versions
> (conditional/unconditional and more), but I find it too sloppy
> to claim that likelihoods are only defined up to a constant
> factor -- even though of course it doesn't matter of ML (and
> REML) if the objective function is "off by constant factor".
> 
> Martin

OK.  Let me display my ignorance:  I was always under the impression 
that likelihood is defined with respect to an *underlying measure*.
Change the measure and you get a different likelihood (with the log 
likelihoods differing by a constant).

To beat it to death:  Pr(X <= x) = \int_{-\infty}^x f(x) d\mu(x)
where f(x) is the density (which can be interpreted as the likelihood)
of X *with respect to* the measure \mu().

E.g. if you have a random variable X with probability mass function 
P(x;\theta) defined, say, on the positive integers, you could make use 
of the atomic measure \mu_1() having point mass 1 at each positive 
integer, or you could make use of the atomic measure \mu_2() having 
point mass 1/2^n at each positive integer n.

The log likelihood of \theta and an iid sample {x_1, ..., x_N} is

\sum_{i=1}^N \log P(x_i; \theta)

with respect to \mu_1() and is

\sum_{i=1}^N [\log P(x_i; \theta) + x_i \log(2)]

with respect to \mu_2(), so the two likelihoods differ by
\log(2) \times \sum_{i=1}^N x_i.

Of course \mu_1() is the "natural" measure and \mu_2() is contrived and 
artificial.  But they are both perfectly legitimate measures.  And I 
have a distinct impression that situations arise in which one might 
legitimately wish to make a change of measure.

Moreover, is it not the case that the likelihood that one gets from 
fitting a linear model with no random effects, using lm(), is not 
(directly) comparable with the likelihood obtained from fitting a linear 
mixed model using lmer()?  (Whence one cannot test for the 
"significance" of a random effect by comparing the two fits via a 
likelihood ratio test, as one might perhaps na?vely hope to do.)

Please enlighten me as to the ways in which my thinking is confused.

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From d@r|zopou|o@ @end|ng |rom er@@mu@mc@n|  Sun Apr 26 08:51:54 2020
From: d@r|zopou|o@ @end|ng |rom er@@mu@mc@n| (D. Rizopoulos)
Date: Sun, 26 Apr 2020 06:51:54 +0000
Subject: [R-sig-ME] nAGQ > 1 in lme4::glmer gives unexpected likelihood
In-Reply-To: <14a5f0ec-09eb-784a-2752-5c73ae1f93e3@auckland.ac.nz>
References: <15763_1587596325_0Q9700G9XP5WJU30_CAEbSdx17yikdhCLta4oektu0K44u-jA_E=9Efb9axJ=LsNsruA@mail.gmail.com>
 <31308_1587737137_0Q9A00BAKPTDVU20_CAO7JsnR_TDHYtQDAWOzCp9CBWw7CJ+UfsZk2iTzDWqyQz=dsFw@mail.gmail.com>
 <CAO7JsnTNuNDFMZhJjEq=rRFT8Mj5rXFes3KKOyzFi_Wg7kF=8Q@mail.gmail.com>
 <bcc0a4d0-efab-902a-263a-a8b9e07516eb@gmail.com>
 <41530176-EC7C-4857-9933-7F557A70D412@health.ucsd.edu>
 <27717610-083b-ebba-c00e-387acf037e54@gmail.com>
 <24228.11801.175135.545280@stat.math.ethz.ch>,
 <14a5f0ec-09eb-784a-2752-5c73ae1f93e3@auckland.ac.nz>
Message-ID: <AM6PR04MB6503325DF746B6DBE291572EE8AE0@AM6PR04MB6503.eurprd04.prod.outlook.com>

I would say that you can compare a linear model with a linear mixed model using a likelihood ratio test. Under maximum likelihood you integrate the random effects out. Hence, you are testing whether some variance components are zero, i.e., the linear model is nested within the linear mixed model. The technical problem is that the distribution of the statistic will not be the classic chi-squared distribution because for the variance parameters the null hypothesis lies on the boundary of the corresponding parameter space.

Best,
Dimitris

??
Dimitris Rizopoulos
Professor of Biostatistics
Erasmus University Medical Center
The Netherlands
________________________________
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on behalf of Rolf Turner <r.turner at auckland.ac.nz>
Sent: Sunday, April 26, 2020 1:23:22 AM
To: Martin Maechler <maechler at stat.math.ethz.ch>
Cc: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] nAGQ > 1 in lme4::glmer gives unexpected likelihood


On 26/04/20 12:33 am, Martin Maechler wrote:

>
>>     I think they're both 'correct' in the sense of being proportional to
>> the likelihood but use different baselines, thus incommensurate (see the
>> note pointed out below).
>
> Well, I'm sorry to be a spoiler here, but I think we should
> acknowledge that (log) likelihood is uniquely well defined
> function.
> I remember how we've fought with the many definitions of
> deviance and have (in our still unfinished glmm-paper -- hint!!)
> very nicely tried to define the many versions
> (conditional/unconditional and more), but I find it too sloppy
> to claim that likelihoods are only defined up to a constant
> factor -- even though of course it doesn't matter of ML (and
> REML) if the objective function is "off by constant factor".
>
> Martin

OK.  Let me display my ignorance:  I was always under the impression
that likelihood is defined with respect to an *underlying measure*.
Change the measure and you get a different likelihood (with the log
likelihoods differing by a constant).

To beat it to death:  Pr(X <= x) = \int_{-\infty}^x f(x) d\mu(x)
where f(x) is the density (which can be interpreted as the likelihood)
of X *with respect to* the measure \mu().

E.g. if you have a random variable X with probability mass function
P(x;\theta) defined, say, on the positive integers, you could make use
of the atomic measure \mu_1() having point mass 1 at each positive
integer, or you could make use of the atomic measure \mu_2() having
point mass 1/2^n at each positive integer n.

The log likelihood of \theta and an iid sample {x_1, ..., x_N} is

\sum_{i=1}^N \log P(x_i; \theta)

with respect to \mu_1() and is

\sum_{i=1}^N [\log P(x_i; \theta) + x_i \log(2)]

with respect to \mu_2(), so the two likelihoods differ by
\log(2) \times \sum_{i=1}^N x_i.

Of course \mu_1() is the "natural" measure and \mu_2() is contrived and
artificial.  But they are both perfectly legitimate measures.  And I
have a distinct impression that situations arise in which one might
legitimately wish to make a change of measure.

Moreover, is it not the case that the likelihood that one gets from
fitting a linear model with no random effects, using lm(), is not
(directly) comparable with the likelihood obtained from fitting a linear
mixed model using lmer()?  (Whence one cannot test for the
"significance" of a random effect by comparing the two fits via a
likelihood ratio test, as one might perhaps na?vely hope to do.)

Please enlighten me as to the ways in which my thinking is confused.

cheers,

Rolf

--
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-mixed-models&amp;data=02%7C01%7Cd.rizopoulos%40erasmusmc.nl%7Ca41b44a6ef8e474585a608d7e96fb473%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C0%7C637234538296805142&amp;sdata=ZCk%2BAO5dPQxn4MDtL9Ys%2FGkr%2F6dlDYjxOR8Q4uZdidc%3D&amp;reserved=0

	[[alternative HTML version deleted]]


From r@turner @end|ng |rom @uck|@nd@@c@nz  Sun Apr 26 11:06:04 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sun, 26 Apr 2020 21:06:04 +1200
Subject: [R-sig-ME] nAGQ > 1 in lme4::glmer gives unexpected likelihood
In-Reply-To: <AM6PR04MB6503325DF746B6DBE291572EE8AE0@AM6PR04MB6503.eurprd04.prod.outlook.com>
References: <15763_1587596325_0Q9700G9XP5WJU30_CAEbSdx17yikdhCLta4oektu0K44u-jA_E=9Efb9axJ=LsNsruA@mail.gmail.com>
 <31308_1587737137_0Q9A00BAKPTDVU20_CAO7JsnR_TDHYtQDAWOzCp9CBWw7CJ+UfsZk2iTzDWqyQz=dsFw@mail.gmail.com>
 <CAO7JsnTNuNDFMZhJjEq=rRFT8Mj5rXFes3KKOyzFi_Wg7kF=8Q@mail.gmail.com>
 <bcc0a4d0-efab-902a-263a-a8b9e07516eb@gmail.com>
 <41530176-EC7C-4857-9933-7F557A70D412@health.ucsd.edu>
 <27717610-083b-ebba-c00e-387acf037e54@gmail.com>
 <24228.11801.175135.545280@stat.math.ethz.ch>
 <14a5f0ec-09eb-784a-2752-5c73ae1f93e3@auckland.ac.nz>
 <AM6PR04MB6503325DF746B6DBE291572EE8AE0@AM6PR04MB6503.eurprd04.prod.outlook.com>
Message-ID: <57f2f03f-aa03-0fd6-3e73-5ce65a94fc32@auckland.ac.nz>


On 26/04/20 6:51 pm, D. Rizopoulos wrote:

> I would say that you can compare a linear model with a linear mixed 
> model using a likelihood ratio test. Under maximum likelihood you 
> integrate the random effects out. Hence, you are testing whether some 
> variance components are zero, i.e., the linear model is nested within 
> the linear mixed model. The technical problem is that the distribution 
> of the statistic will not be the classic chi-squared distribution 
> because for the variance parameters the null hypothesis lies on the 
> boundary of the corresponding parameter space.

<SNIP>

OK.  So the problem is that the null value is on the boundary of the
parameter space (whence the asymptotics for the distribution of the 
likelihood ratio statistic don't work) and *NOT* that the "normalising 
constants" (or underlying measures) are different.  I have a clear and 
distinct (and presumably erroneous!!!) recollection of having read that 
the problem was the latter, perhaps *in addition* to the former.

Can the wise denizens of this list confirm to me the problem is *only* 
the former?

Be that as it may, is not still true that in general log likelihood is 
well-defined only up to an additive constant?

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From ph||||p@@|d@y @end|ng |rom mp|@n|  Sun Apr 26 17:15:39 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Sun, 26 Apr 2020 17:15:39 +0200
Subject: [R-sig-ME] Partial Argument Matching in nlme::varFunc
In-Reply-To: <010f01d61b10$a3d48950$eb7d9bf0$@denney.ws>
References: <010f01d61b10$a3d48950$eb7d9bf0$@denney.ws>
Message-ID: <2752318f-d35c-3794-d352-43a27a877d39@mpi.nl>

Hi Bill,

I see that you've really been getting into the nlme source lately ....
as far as I know, nlme is in maintenance mode right now and pretty badly
neglected in terms of interest in actually maintaining it. Have you
considered offering to take on some of that stewardship? I'm sure R Core
would be happy to have someone actively engaged in keeping a core
package alive.

Best,

Phillip


On 25/04/2020 16:48, bill at denney.ws wrote:
> Hi,
>
>  
>
> Recently, I have turned on warnings for partial argument name matching, and
> with that, I'm finding many warnings.  One is while summarizing models
> within the nlme library.
>
>  
>
> An example that causes this is:
>
>  
>
> ```
>
> library(nlme)
>
> fm1 <- gls(follicles ~ sin(2*pi*Time) + cos(2*pi*Time), Ovary,
>
>            correlation = corAR1(form = ~ 1 | Mare),
>
>            weights = varIdent(form=~Mare))
>
> summary(fm1)
>
> ```
>
>  
>
> Which gives the warning:
>
> ```
>
> Warning message:
>
> In coef.varIdent(x, uncons = FALSE, allCoef = TRUE) :
>
>   partial argument match of 'uncons' to 'unconstrained'
>
> ```
>
>  
>
> This appears to be fixed by the diff I put into an R bug report here:
> https://bugs.r-project.org/bugzilla/show_bug.cgi?id=17767
>
>  
>
> I assume that this list has the right people to review and apply the patch.
>
>
> Thanks,
>
>  
>
> Bill
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From ph||||p@@|d@y @end|ng |rom mp|@n|  Sun Apr 26 17:20:08 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Sun, 26 Apr 2020 17:20:08 +0200
Subject: [R-sig-ME] nAGQ > 1 in lme4::glmer gives unexpected likelihood
In-Reply-To: <B6E177DB-4D1F-4B00-8A60-41F6B2B887E8@health.ucsd.edu>
References: <15763_1587596325_0Q9700G9XP5WJU30_CAEbSdx17yikdhCLta4oektu0K44u-jA_E=9Efb9axJ=LsNsruA@mail.gmail.com>
 <31308_1587737137_0Q9A00BAKPTDVU20_CAO7JsnR_TDHYtQDAWOzCp9CBWw7CJ+UfsZk2iTzDWqyQz=dsFw@mail.gmail.com>
 <CAO7JsnTNuNDFMZhJjEq=rRFT8Mj5rXFes3KKOyzFi_Wg7kF=8Q@mail.gmail.com>
 <B6E177DB-4D1F-4B00-8A60-41F6B2B887E8@health.ucsd.edu>
Message-ID: <ffebd98e-9ecd-9d8f-3690-3a6a001864fb@mpi.nl>

Potentially. In my experience, the best-tested portions of GLMM software
(including lme4 and MixedModels) are related to Binomial models, then
Poisson, then any model family with a dispersion parameter.

That said, families without a dispersion parameter (Binomial and
Poisson) tend to work equally well in terms of the code, though Poisson
might be less well-behaved numerically.

On 25/04/2020 16:40, Vaida, Florin wrote:
> Interestingly (and reassuringly), Laplace and nAGQ give consistent results for binomial GLMM.
> Could there be a glitch for Poisson GLMM?
>
> #### Likelihood of Binomial GLMM Models
>
> # r-sig-mixed-models GLMM question*
> library(lme4)
> set.seed(51)
>
> # Simulate some random effect-driven Poisson data*
> # random_effects <- rnorm(10, 0, 2)
> # random_effects <- rnorm(10, 0, 0.01)
> random_effects <- rnorm(10, 0, 1)
>
> group <- rep(1:10, 10)
> simulated_data <- data.frame(y = rbinom(n = 100, size = 1, 
>     prob = exp(2+random_effects[group])/(1+exp(2+random_effects[group]))), group = group)
>
> # Fit models with Laplace (nAGQ = 1) and nAGQ = 11*
> fit_Laplace <- glmer(y ~ (1|group), data = simulated_data, family =
>                        binomial())
> fit_AGQ <- glmer(y ~ (1|group), data = simulated_data, family = binomial(),
>                  nAGQ = 11)
> # fit_glm = glm(y ~ group, data=simulated_data, family=binomial)
>
> (logLik(fit_Laplace))
> (logLik(fit_AGQ))  # very similar to fit_Laplace
> # (logLik(fit_glm)) 
>
>
>> On Apr 24, 2020, at 7:24 AM, Douglas Bates <bates at stat.wisc.edu> wrote:
>>
>> Having said that, I do see that the fits in the MixedModels package for
>> Julia produce similar values of the deviance with the Laplace approximation
>> and nAGQ = 7
>>
>> julia> m1 = fit(MixedModel, @formula(y ~ 1 + (1|group)), dd, Poisson())
>> Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)
>>  y ~ 1 + (1 | group)
>>  Distribution: Poisson{Float64}
>>  Link: LogLink()
>>
>>  Deviance: 193.5587
>>
>> Variance components:
>>         Column    Variance  Std.Dev.
>> group (Intercept)  3.9577026 1.9893975
>>
>> Number of obs: 100; levels of grouping factors: 10
>>
>> Fixed-effects parameters:
>> ??????????????????????????????????????????????????
>>             Estimate  Std.Error  z value  P(>|z|)
>> ??????????????????????????????????????????????????
>> (Intercept)   2.65175   0.632317     4.19    <1e-4
>> ??????????????????????????????????????????????????
>>
>> julia> m1 = fit(MixedModel, @formula(y ~ 1 + (1|group)), dd, Poisson(),
>> nAGQ=7)
>> Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 7)
>>  y ~ 1 + (1 | group)
>>  Distribution: Poisson{Float64}
>>  Link: LogLink()
>>
>>  Deviance: 193.5104
>>
>> Variance components:
>>         Column    Variance  Std.Dev.
>> group (Intercept)  3.9577026 1.9893975
>>
>> Number of obs: 100; levels of grouping factors: 10
>>
>> Fixed-effects parameters:
>> ??????????????????????????????????????????????????
>>             Estimate  Std.Error  z value  P(>|z|)
>> ??????????????????????????????????????????????????
>> (Intercept)   2.65175   0.632317     4.19    <1e-4
>> ??????????????????????????????????????????????????
>>
>> As the person who wrote the first version of the nAGQ code in R I would not
>> be surprised if there was a constant dropped somewhere.  It is difficult
>> code.
>>
>> And the results here in the Julia package make me uncomfortable because the
>> values of the parameter estimates are identical in the two fits.  I would
>> expect them to be close but not identical.
>>
>> Isn't it good to know that there is still room for research in this area?
>> :-)
>>
>> On Fri, Apr 24, 2020 at 9:05 AM Douglas Bates <bates at stat.wisc.edu> wrote:
>>
>>> There's a lot of variability in your lambdas
>>>
>>>> exp(3 + random_effects)
>>> [1]  91.5919358   6.9678749   4.1841478  78.0771666 890.6931394
>>> 20.8558107
>>> [7]   3.0037864   0.3049416   2.1675995  40.6209684
>>>
>>> Do you really expect that some groups will have a mean count of nearly 900
>>> whereas others will have a mean count less than 1?
>>>
>>>
>>> On Wed, Apr 22, 2020 at 5:58 PM Ben Goldstein <ben.goldstein at berkeley.edu>
>>> wrote:
>>>
>>>> Hi all,
>>>>
>>>> I'm using lme4::glmer to estimate Poisson mixed models in a very simple
>>>> context (single random effect). I'm interested in the model
>>> likelihood/AIC
>>>> across many simulated datasets.
>>>>
>>>> To investigate whether the Laplace approximation was appropriate for my
>>>> data context, I explored using the argument nAGQ to improve the accuracy
>>> of
>>>> the likelihood estimation. When I changed nAGQ to a value > 1, I saw an
>>>> unexpectedly huge change in the likelihood; log-likelihoods tended to be
>>>> off by ~200. Other statistics packages (e.g. GLMMadaptive) yield
>>> estimates
>>>> that agree with lme4's Laplace approximation, as did a manual likelihood
>>>> estimate, and not with the nAGQ > 2 estimate.
>>>>
>>>> The following code reproduces the problem I'm encountering.
>>>>
>>>> *# r-sig-mixed-models GLMM question*
>>>> library(lme4)
>>>> set.seed(51)
>>>>
>>>> *# Simulate some random effect-driven Poisson data*
>>>> random_effects <- rnorm(10, 0, 2)
>>>> group <- rep(1:10, 10)
>>>> simulated_data <- data.frame(y = rpois(n = 100, lambda = exp(3 +
>>>> random_effects[group])),
>>>>                             group = group)
>>>>
>>>> *# Fit models with Laplace (nAGQ = 1) and nAGQ = 11*
>>>> fit_Laplace <- glmer(y ~ (1|group), data = simulated_data, family =
>>>> poisson())
>>>> fit_AGQ <- glmer(y ~ (1|group), data = simulated_data, family =
>>> poisson(),
>>>> nAGQ = 11)
>>>>
>>>> logLik(fit_Laplace)
>>>> logLik(fit_AGQ)
>>>> logLik(fit_Laplace) - logLik(fit_AGQ) *# Huge difference!*
>>>>
>>>> When I execute the above code, I see a difference in likelihood of
>>>> -218.8894. I've tested across many simulations and on 2 different
>>> machines
>>>> (Mac and Linux). My version of lme4 is up to date.
>>>>
>>>> Has anyone run into this issue before? Am I using the glmer function
>>> wrong,
>>>> or is it possible there's something going on under the hood?
>>>>
>>>> Thanks,
>>>> Ben
>>>>
>>>>        [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>        [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From biii m@iii@g oii de@@ey@ws  Sun Apr 26 17:24:26 2020
From: biii m@iii@g oii de@@ey@ws (biii m@iii@g oii de@@ey@ws)
Date: Sun, 26 Apr 2020 11:24:26 -0400
Subject: [R-sig-ME] Partial Argument Matching in nlme::varFunc
In-Reply-To: <2752318f-d35c-3794-d352-43a27a877d39@mpi.nl>
References: <010f01d61b10$a3d48950$eb7d9bf0$@denney.ws>
 <2752318f-d35c-3794-d352-43a27a877d39@mpi.nl>
Message-ID: <000601d61bde$c5b32530$51196f90$@denney.ws>

Hi Philip,

:)

While revisions like the one here are straight-forward, I don't think that my knowledge of underlying algorithms is sufficient for me to actively maintain the package.

I try to be a productive bug-reporter, and where I can, I try to provide patches.  But, from my previous questions (about nlme::getVarCov and nlme::varFixed), I don't know what the correct behavior would be.

Thanks,

Bill

-----Original Message-----
From: Phillip Alday <phillip.alday at mpi.nl> 
Sent: Sunday, April 26, 2020 11:16 AM
To: bill at denney.ws; r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Partial Argument Matching in nlme::varFunc

Hi Bill,

I see that you've really been getting into the nlme source lately ....
as far as I know, nlme is in maintenance mode right now and pretty badly neglected in terms of interest in actually maintaining it. Have you considered offering to take on some of that stewardship? I'm sure R Core would be happy to have someone actively engaged in keeping a core package alive.

Best,

Phillip


On 25/04/2020 16:48, bill at denney.ws wrote:
> Hi,
>
>  
>
> Recently, I have turned on warnings for partial argument name 
> matching, and with that, I'm finding many warnings.  One is while 
> summarizing models within the nlme library.
>
>  
>
> An example that causes this is:
>
>  
>
> ```
>
> library(nlme)
>
> fm1 <- gls(follicles ~ sin(2*pi*Time) + cos(2*pi*Time), Ovary,
>
>            correlation = corAR1(form = ~ 1 | Mare),
>
>            weights = varIdent(form=~Mare))
>
> summary(fm1)
>
> ```
>
>  
>
> Which gives the warning:
>
> ```
>
> Warning message:
>
> In coef.varIdent(x, uncons = FALSE, allCoef = TRUE) :
>
>   partial argument match of 'uncons' to 'unconstrained'
>
> ```
>
>  
>
> This appears to be fixed by the diff I put into an R bug report here:
> https://bugs.r-project.org/bugzilla/show_bug.cgi?id=17767
>
>  
>
> I assume that this list has the right people to review and apply the patch.
>
>
> Thanks,
>
>  
>
> Bill
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From ph||||p@@|d@y @end|ng |rom mp|@n|  Sun Apr 26 17:32:58 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Sun, 26 Apr 2020 17:32:58 +0200
Subject: [R-sig-ME] nAGQ > 1 in lme4::glmer gives unexpected likelihood
In-Reply-To: <64153415-EF54-4FFD-A6F4-53E4A2AFEE64@health.ucsd.edu>
References: <15763_1587596325_0Q9700G9XP5WJU30_CAEbSdx17yikdhCLta4oektu0K44u-jA_E=9Efb9axJ=LsNsruA@mail.gmail.com>
 <31308_1587737137_0Q9A00BAKPTDVU20_CAO7JsnR_TDHYtQDAWOzCp9CBWw7CJ+UfsZk2iTzDWqyQz=dsFw@mail.gmail.com>
 <CAO7JsnTNuNDFMZhJjEq=rRFT8Mj5rXFes3KKOyzFi_Wg7kF=8Q@mail.gmail.com>
 <bcc0a4d0-efab-902a-263a-a8b9e07516eb@gmail.com>
 <64153415-EF54-4FFD-A6F4-53E4A2AFEE64@health.ucsd.edu>
Message-ID: <100396ff-425e-d5c9-b7af-d3eb8cc21648@mpi.nl>

The nAGQ setting doesn't affect the constant in question, but rather how
accurately the deviance is evaluated. *Very coarsely*, think of nAGQ as
impacting how much "rounding" you do at intermediate steps. (It's more
complicated than that, but the general intuition is not completely off.)
So for a lot of models, changing that setting won't have much of an
impact, but there could be some situations where that setting matters more.

The "off by a constant" issue is for the deviance and not the log
likelihood per se. While we often think of the deviance as just being -2
log likelihood, that's a very special case for the "easy" models you
learn in intro to stats (like the classical fixed-effects linear
regression). The term "deviance" actually comes the fact that it
describes the "deviation" in fit (as measured by log likelihood) from a
fully saturated model, which is relatively easy to define for classical LM.

Like so many obvious and easy things from classical regression, there is
no obvious and easy equivalent in the GLMM world. (GLM breaks things
from LM, and LMM breaks things from LM, GLMM just breaks soooo much.)
However, because we usually care about differences of deviance, this
doesn't matter too much -- the additive constant just cancels out.
Likewise, it doesn't matter for treating deviance as the objective in an
optimization problem.

This is then the problem with different types of "deviance". Like the
log likelihood, the deviance is uniquely defined for a given probability
model (if we only know what the saturated model is). So we can talk
about the deviance of an unconditional or conditional model, but it's a
little bit problematic to talk about the "unconditional deviance",
although I understand the temptation to use that as a convenient shorthand.

I've simplified a bit and I'm hoping that I've cleared up more
inaccuracies than I've created, lest the stats deities smite me ....

Phillip

On 25/04/2020 16:41, Vaida, Florin wrote:
> Looks like you guys thought about deviance issues a lot:
> https://github.com/lme4/lme4/issues/375
>
> Speaking about the principle of "least surprise", it is surprising to get completely different behaviors in deviance for nAGQ=1 vs nAGQ=2 (say), in the same model - (what Ben Goldstein pointed out).
>
> A couple further questions:
> 1. Is the "off by" constant the same, for a given model, if I only change nAGQ?  E.g., comparing nAGQ=5 and nAGQ=20?
> 2. If so, can you calculate the "off by" constant by comparing with formula-based Laplace approximation with a bona fide Adaptive Gaussian Quadrature using existing algorithm but n=1?
>
> Florin
>
> On Apr 24, 2020, at 7:59 AM, Ben Bolker <bbolker at gmail.com<mailto:bbolker at gmail.com>> wrote:
>
>
>    I found the note I was looking for.  In ?deviance.glmerMod it says:
>
>  If adaptive Gauss-Hermite quadrature is used, then
>           ?logLik(object)? is currently only proportional to the
>           absolute-unconditional log-likelihood.
>
> (see the discussion on this page for more context); see also https://github.com/lme4/lme4/blob/master/misc/logLikGLMM/logLikGLMM.R
>
>
> On 4/24/20 10:24 AM, Douglas Bates wrote:
> Having said that, I do see that the fits in the MixedModels package for
> Julia produce similar values of the deviance with the Laplace approximation
> and nAGQ = 7
>
> julia> m1 = fit(MixedModel, @formula(y ~ 1 + (1|group)), dd, Poisson())
> Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)
>   y ~ 1 + (1 | group)
>   Distribution: Poisson{Float64}
>   Link: LogLink()
>
>   Deviance: 193.5587
>
> Variance components:
>          Column    Variance  Std.Dev.
> group (Intercept)  3.9577026 1.9893975
>
>  Number of obs: 100; levels of grouping factors: 10
>
> Fixed-effects parameters:
> ??????????????????????????????????????????????????
>              Estimate  Std.Error  z value  P(>|z|)
> ??????????????????????????????????????????????????
> (Intercept)   2.65175   0.632317     4.19    <1e-4
> ??????????????????????????????????????????????????
>
> julia> m1 = fit(MixedModel, @formula(y ~ 1 + (1|group)), dd, Poisson(),
> nAGQ=7)
> Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 7)
>   y ~ 1 + (1 | group)
>   Distribution: Poisson{Float64}
>   Link: LogLink()
>
>   Deviance: 193.5104
>
> Variance components:
>          Column    Variance  Std.Dev.
> group (Intercept)  3.9577026 1.9893975
>
>  Number of obs: 100; levels of grouping factors: 10
>
> Fixed-effects parameters:
> ??????????????????????????????????????????????????
>              Estimate  Std.Error  z value  P(>|z|)
> ??????????????????????????????????????????????????
> (Intercept)   2.65175   0.632317     4.19    <1e-4
> ??????????????????????????????????????????????????
>
> As the person who wrote the first version of the nAGQ code in R I would not
> be surprised if there was a constant dropped somewhere.  It is difficult
> code.
>
> And the results here in the Julia package make me uncomfortable because the
> values of the parameter estimates are identical in the two fits.  I would
> expect them to be close but not identical.
>
> Isn't it good to know that there is still room for research in this area?
> :-)
>
> On Fri, Apr 24, 2020 at 9:05 AM Douglas Bates <bates at stat.wisc.edu<mailto:bates at stat.wisc.edu>> wrote:
>
> There's a lot of variability in your lambdas
>
> exp(3 + random_effects)
>  [1]  91.5919358   6.9678749   4.1841478  78.0771666 890.6931394
> 20.8558107
>  [7]   3.0037864   0.3049416   2.1675995  40.6209684
>
> Do you really expect that some groups will have a mean count of nearly 900
> whereas others will have a mean count less than 1?
>
>
> On Wed, Apr 22, 2020 at 5:58 PM Ben Goldstein <ben.goldstein at berkeley.edu<mailto:ben.goldstein at berkeley.edu>>
> wrote:
>
> Hi all,
>
> I'm using lme4::glmer to estimate Poisson mixed models in a very simple
> context (single random effect). I'm interested in the model
> likelihood/AIC
> across many simulated datasets.
>
> To investigate whether the Laplace approximation was appropriate for my
> data context, I explored using the argument nAGQ to improve the accuracy
> of
> the likelihood estimation. When I changed nAGQ to a value > 1, I saw an
> unexpectedly huge change in the likelihood; log-likelihoods tended to be
> off by ~200. Other statistics packages (e.g. GLMMadaptive) yield
> estimates
> that agree with lme4's Laplace approximation, as did a manual likelihood
> estimate, and not with the nAGQ > 2 estimate.
>
> The following code reproduces the problem I'm encountering.
>
> *# r-sig-mixed-models GLMM question*
> library(lme4)
> set.seed(51)
>
> *# Simulate some random effect-driven Poisson data*
> random_effects <- rnorm(10, 0, 2)
> group <- rep(1:10, 10)
> simulated_data <- data.frame(y = rpois(n = 100, lambda = exp(3 +
> random_effects[group])),
>                              group = group)
>
> *# Fit models with Laplace (nAGQ = 1) and nAGQ = 11*
> fit_Laplace <- glmer(y ~ (1|group), data = simulated_data, family =
> poisson())
> fit_AGQ <- glmer(y ~ (1|group), data = simulated_data, family =
> poisson(),
> nAGQ = 11)
>
> logLik(fit_Laplace)
> logLik(fit_AGQ)
> logLik(fit_Laplace) - logLik(fit_AGQ) *# Huge difference!*
>
> When I execute the above code, I see a difference in likelihood of
> -218.8894. I've tested across many simulations and on 2 different
> machines
> (Mac and Linux). My version of lme4 is up to date.
>
> Has anyone run into this issue before? Am I using the glmer function
> wrong,
> or is it possible there's something going on under the hood?
>
> Thanks,
> Ben
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From ph||||p@@|d@y @end|ng |rom mp|@n|  Sun Apr 26 18:06:05 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Sun, 26 Apr 2020 18:06:05 +0200
Subject: [R-sig-ME] nAGQ > 1 in lme4::glmer gives unexpected likelihood
In-Reply-To: <14a5f0ec-09eb-784a-2752-5c73ae1f93e3@auckland.ac.nz>
References: <15763_1587596325_0Q9700G9XP5WJU30_CAEbSdx17yikdhCLta4oektu0K44u-jA_E=9Efb9axJ=LsNsruA@mail.gmail.com>
 <31308_1587737137_0Q9A00BAKPTDVU20_CAO7JsnR_TDHYtQDAWOzCp9CBWw7CJ+UfsZk2iTzDWqyQz=dsFw@mail.gmail.com>
 <CAO7JsnTNuNDFMZhJjEq=rRFT8Mj5rXFes3KKOyzFi_Wg7kF=8Q@mail.gmail.com>
 <bcc0a4d0-efab-902a-263a-a8b9e07516eb@gmail.com>
 <41530176-EC7C-4857-9933-7F557A70D412@health.ucsd.edu>
 <27717610-083b-ebba-c00e-387acf037e54@gmail.com>
 <24228.11801.175135.545280@stat.math.ethz.ch>
 <14a5f0ec-09eb-784a-2752-5c73ae1f93e3@auckland.ac.nz>
Message-ID: <861763df-2512-d0be-6334-e8a232b0d029@mpi.nl>

Dimitris handled some of the practical issues, so I'm going to try to
handle some of the underlying math issues based on my very rusty
collection of measure theory .... hopefully I'll fix more "wrongs" than
I create and not incur the wrath of the math-stats deities ....

On 26/04/2020 01:23, Rolf Turner wrote:
>
> <SNIP>

> OK.? Let me display my ignorance:? I was always under the impression
> that likelihood is defined with respect to an *underlying measure*.
> Change the measure and you get a different likelihood (with the log
> likelihoods differing by a constant).
>
> To beat it to death:? Pr(X <= x) = \int_{-\infty}^x f(x) d\mu(x)
> where f(x) is the density (which can be interpreted as the likelihood)
> of X *with respect to* the measure \mu().
>
> E.g. if you have a random variable X with probability mass function
> P(x;\theta) defined, say, on the positive integers, you could make use
> of the atomic measure \mu_1() having point mass 1 at each positive
> integer, or you could make use of the atomic measure \mu_2() having
> point mass 1/2^n at each positive integer n. 

While the Kolmogorov axioms are expressed in terms of general measure
theory, we almost always skip the Lebesque integral and just use the
Riemann integral, with the associated and implied usual measure on the
real line (i.e. the one corresponding to the L2-norm). While we
certainly use other norms in stats (and the Lp norms are a great way to
think about things like regularization or even the deep relationship
between mean, median and mode), we almost always think of the
probabilities in terms of the usual measure on the real line.

On top of that, the "measures" you defined aren't the usual types of
things you see in measure theory. You seem to have blurred the line
between (or even perhaps convolved) your PDF and your measure. (I'm
looking at my copy of Kolmogorov and Fomin's Introductory Real Analysis
and really not wanting to take the time to make all of my statements
rigorous). This brings me to what I suspect is the actual problem:

The entire integral, including the associated measure, is what defines
the probability model, and it is for a given probability model that the
likelihood is unique. Indeed, that's the whole point of Fisher's work on
likelihood: you create something that is a function of the data for a
given probability model, including associated parameters. Since we can't
change the data, we change the parameters of the probability model to
maximize the likelihood. (This is being slightly infelicitous about the
fine print on P(x|?)vs L(?|x)and the difference between the probability
and scoring function).

The definition of (G)LMM that we use defines our probability model and
thus unique likelihood. The real fun comes in figuring out an efficient
way to evaluate the profiled log likelihood, which is where a lot of the
innovation in lme4 and MixedModels occurs -- using sparse matrix methods
on a penalized least squares problem instead of generalized least squares.

By the way, I think the problem that you're getting at with your measure
example is actually something not dissimilar to importance sampling.

One more thing to comment on below the fold ....

> <SNIP>
> Moreover, is it not the case that the likelihood that one gets from
> fitting a linear model with no random effects, using lm(), is not
> (directly) comparable with the likelihood obtained from fitting a
> linear mixed model using lmer()?? (Whence one cannot test for the
> "significance" of a random effect by comparing the two fits via a
> likelihood ratio test, as one might perhaps na?vely hope to do.)

Yes, you can compare the likelihoods between LM and LMM fits BUT the
likelihood-ratio test will be problematic because the LM fit corresponds
to an LMM fit with values at the edge of the parameter space (one or
more variance components equal to zero). Now, you could bootstrap this
test instead of assuming that the null distribution follows a
chi-squared distribution with an appropriate number of degrees of
freedom, then everything would be fine. I even have some example code
for this in Julia that I could potentially clean up and share. AIC/BIC
don't suffer from the same issues as the LRT (because they don't assume
a null distribution), but have their own difficulties and I don't want
to spend too much time discussing model selection here as enough ink and
arXiv space has been spilled over it.

I hope that helps and that I have fixed more misunderstandings than I
created.

Phillip


>
> Please enlighten me as to the ways in which my thinking is confused.
>
> cheers,
>
> Rolf
>


	[[alternative HTML version deleted]]


From ph||||p@@|d@y @end|ng |rom mp|@n|  Sun Apr 26 18:07:16 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Sun, 26 Apr 2020 18:07:16 +0200
Subject: [R-sig-ME] nAGQ > 1 in lme4::glmer gives unexpected likelihood
In-Reply-To: <57f2f03f-aa03-0fd6-3e73-5ce65a94fc32@auckland.ac.nz>
References: <15763_1587596325_0Q9700G9XP5WJU30_CAEbSdx17yikdhCLta4oektu0K44u-jA_E=9Efb9axJ=LsNsruA@mail.gmail.com>
 <31308_1587737137_0Q9A00BAKPTDVU20_CAO7JsnR_TDHYtQDAWOzCp9CBWw7CJ+UfsZk2iTzDWqyQz=dsFw@mail.gmail.com>
 <CAO7JsnTNuNDFMZhJjEq=rRFT8Mj5rXFes3KKOyzFi_Wg7kF=8Q@mail.gmail.com>
 <bcc0a4d0-efab-902a-263a-a8b9e07516eb@gmail.com>
 <41530176-EC7C-4857-9933-7F557A70D412@health.ucsd.edu>
 <27717610-083b-ebba-c00e-387acf037e54@gmail.com>
 <24228.11801.175135.545280@stat.math.ethz.ch>
 <14a5f0ec-09eb-784a-2752-5c73ae1f93e3@auckland.ac.nz>
 <AM6PR04MB6503325DF746B6DBE291572EE8AE0@AM6PR04MB6503.eurprd04.prod.outlook.com>
 <57f2f03f-aa03-0fd6-3e73-5ce65a94fc32@auckland.ac.nz>
Message-ID: <4953e4df-c60f-a442-847c-4f6e659a2432@mpi.nl>


On 26/04/2020 11:06, Rolf Turner wrote:
> Can the wise denizens of this list confirm to me the problem is *only*
> the former?
>
> Be that as it may, is not still true that in general log likelihood is
> well-defined only up to an additive constant?

It's the deviance with the additive constant issue, but this bleeds into
the "lie" we learn in intro stats that the deviane is just -2 log
likelihood.

Phillip



>
> cheers,
>
> Rolf
>


From kr@g|tcode @end|ng |rom gm@||@com  Tue Apr 28 01:58:53 2020
From: kr@g|tcode @end|ng |rom gm@||@com (Kate R)
Date: Mon, 27 Apr 2020 16:58:53 -0700
Subject: [R-sig-ME] Multilevel zero-inflated models: model selection and
 model assumption checks
In-Reply-To: <CAGXFLeexSGWnFNC7YgmNe3d4hY_uv7N-jft-0f0=AHZVSGm_bw@mail.gmail.com>
References: <CAGXFLeexSGWnFNC7YgmNe3d4hY_uv7N-jft-0f0=AHZVSGm_bw@mail.gmail.com>
Message-ID: <CAGXFLef8NR-ioUw6i1a+U5cDAG4Uz6ZigNOpK9xA-WtNsxcP=g@mail.gmail.com>

Thank you in advance for reading my long-winded email! I appreciate any
help and guidance.

I am working on research that involves frequency of events (summed during
an hour) and duration of events (calculated a seconds during an hour). Both
data types have 0s and are positively skewed. The observations (n =
14,000+) are collected hourly for up to 2 weeks for 500 participants. A
simple version of the model is: outcome ~ explanatory +
(1|participant/date).

For the frequency data, I have fit:

   - (Hurdle) Poisson / Negative Binomial (the var > mean)
   - (Hurdle) Gamma (log link) (assuming okay to treat frequency data as
   continuous)

The AIC is best for the hurdle negative binomial model. I understand that
for AIC comparisons, the Poisson models do not get an extra scale
parameter, while the other models. However, the difference in AICs are in
the hundreds, so I imagine the small difference in k parameters would not
change interpretation, as long as the log-likelihood functions are
similar...

(Q1) Does the above sound reasonable?

For the duration data, I have fit:

   - (Hurdle) Gamma (log link)
   - Zero-Inflated Beta (after transforming duration by SECONDS/3600. I
   decided to try Beta because the hourly data is bounded at 3600 seconds and
   I was not sure if an upper bound affects the gamma distribution).

(Q2) Does an upper bound mean make it inappropriate to fit a Gamma
distribution?

N.B. If fitting regular gamma/beta, I first transformed data to remove 0s.
For both the regular and zero-inflated beta, I shrunk the 1s using EITHER
the algorithm here: https://www.ncbi.nlm.nih.gov/pubmed/16594767 OR the
inverse hyperbolic sine transformation (IHST)). I had thought about using
ZOIB, but I did not want to use BRMS or GAMLSS (since I am a beginner).

Because I transformed the data in different ways for the different models,
I understand that I cannot compare the AICs. Therefore, I thought about
using cross fold validation. However, the package I was recommended to use
(cvms https://github.com/LudvigOlsen/cvms) says it supports lmer/glmer, but
doesn't appear to support glmmTMB, which is how I fit the above models.

I thought a potential solution might be to fit two separates models
(binomial for 0/1 data and count/continuous for positive data, as suggested
in this post: assessing glmmTMB hurdle model fit using DHARMa scaled
residual plot
<https://stats.stackexchange.com/questions/400147/assessing-glmmtmb-hurdle-model-fit-using-dharma-scaled-residual-plot>).
In this way, I could fit lmer/glmer, and perform cross fold validation
using cvms on the positive data. But I'm not sure if this makes sense (to
partition out the 0 data, and only use the positive data to validate the
models)? I suppose I could simulate data to check predictive accuracy on
the glmmTMB models? Is this a reasonable method of comparing the models?

(Q3) How would you recommend comparing the models with different
distributions and family transformations?

Additionally, I considered fitting a Tweedie distribution to both the
frequency and duration. However, I was having trouble finding guidance on
how to interpret the beta coefficients.

(Q4) How does one interpret Tweedie beta coefficients? Do you exponentiate
and discuss as rates?

Finally, I have used the performance package to check the model
assumptions. The results can be found at this link:

<
https://docs.google.com/document/d/1s6qtSAvw297F_cvblAq7Gh2fWgFmz9I5Nxi1K2gjScI/edit?usp=sharing
>

Pic 1 and 2 are both for the Hurdle Gamma on Duration, but have different
explanatory variables. The pics are pretty similar to the results for the
other models/distributions. I have been reading conflicting advice as to
whether the residuals for GLM models need to be normally distributed, etc.
I understand for large datasets, non-normality may be a non-issue even for
linear regression. We are not concerned with predicting new data, but
explaining the data that we have. Therefore, given the size of my sample
and the distributions. how concerned should I be about the QQ plots and
homogeneity of variance plots below?

Thank you very much!

	[[alternative HTML version deleted]]


From D@v|d@Du||y @end|ng |rom q|mrbergho|er@edu@@u  Wed Apr 29 08:23:03 2020
From: D@v|d@Du||y @end|ng |rom q|mrbergho|er@edu@@u (David Duffy)
Date: Wed, 29 Apr 2020 06:23:03 +0000
Subject: [R-sig-ME] 
 Precision about the glmer model for Bernoulli variables
In-Reply-To: <E2FEE0F4-BB25-449F-B027-D28633BDCFDE@health.ucsd.edu>
References: <20200420074851.GA6361@info124.pharmacie.univ-paris5.fr>
 <AB72DE88-C199-459C-B0EB-0DF13BB1659B@health.ucsd.edu>
 <20200420193249.GB16474@info124.pharmacie.univ-paris5.fr>
 <2AD9CAD5-FABC-4A13-B684-6D0908CFBF21@health.ucsd.edu>
 <5f78610081524e23855ff4bcccdf45a8@qimrberghofer.edu.au>
 <DDB70217-5D47-4DF2-8FF8-0748AD8D7754@anu.edu.au>,
 <E2FEE0F4-BB25-449F-B027-D28633BDCFDE@health.ucsd.edu>
Message-ID: <794d592806b541259b620d30f04c5a3f@qimrberghofer.edu.au>

Hi Florin.

> ...but negative correlations do not correspond to a mixed-effects model specification.  (I thought Geert 
> Molenberghs had a paper to this point but I can't find it now.)

Hopefully still vaguely R-related - in the case of meta-analyses of correlations, the observed correlation for a given, say, sub-study can be negative, and _some_ mixed models will inappropriately truncate this contribution at zero, leading to inflated estimates for the global parameters. This comes up when meta-analysing heritability, where the genetic model (as you have pointed out) contrains this to be non-negative for a single trait.

Because of the computational difficulties, many geneticists still fit linear-normal mixed models to binary data (eg genome-wide association studies of large datasets eg UK Biobank), and don't usually get burnt. The "better" alternative for this has been PQL, implemented in several R packages.

Cheers, David Duffy.


From d@r|zopou|o@ @end|ng |rom er@@mu@mc@n|  Wed Apr 29 08:41:39 2020
From: d@r|zopou|o@ @end|ng |rom er@@mu@mc@n| (D. Rizopoulos)
Date: Wed, 29 Apr 2020 06:41:39 +0000
Subject: [R-sig-ME] 
 Precision about the glmer model for Bernoulli variables
In-Reply-To: <794d592806b541259b620d30f04c5a3f@qimrberghofer.edu.au>
References: <20200420074851.GA6361@info124.pharmacie.univ-paris5.fr>
 <AB72DE88-C199-459C-B0EB-0DF13BB1659B@health.ucsd.edu>
 <20200420193249.GB16474@info124.pharmacie.univ-paris5.fr>
 <2AD9CAD5-FABC-4A13-B684-6D0908CFBF21@health.ucsd.edu>
 <5f78610081524e23855ff4bcccdf45a8@qimrberghofer.edu.au>
 <DDB70217-5D47-4DF2-8FF8-0748AD8D7754@anu.edu.au>,
 <E2FEE0F4-BB25-449F-B027-D28633BDCFDE@health.ucsd.edu>,
 <794d592806b541259b620d30f04c5a3f@qimrberghofer.edu.au>
Message-ID: <AM6PR04MB6503FB597F1C45FB06EAF779E8AD0@AM6PR04MB6503.eurprd04.prod.outlook.com>

Mixed models can assume negative correlations when you include something more than random intercepts. Check

https://emcbiostatistics.shinyapps.io/Repeated_Measurements/

Chapter 3, Section 3.3 -> Select random intercepts & random slopes, and make the correlation between the intercepts and slopes negative. When including quadratic random slopes even get more negative correlations.

Best,
Dimitris


??
Dimitris Rizopoulos
Professor of Biostatistics
Erasmus University Medical Center
The Netherlands
________________________________
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on behalf of David Duffy <David.Duffy at qimrberghofer.edu.au>
Sent: Wednesday, April 29, 2020 8:23:03 AM
To: Vaida, Florin <fvaida at health.ucsd.edu>; John Maindonald <john.maindonald at anu.edu.au>
Cc: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Precision about the glmer model for Bernoulli variables

Hi Florin.

> ...but negative correlations do not correspond to a mixed-effects model specification.  (I thought Geert
> Molenberghs had a paper to this point but I can't find it now.)

Hopefully still vaguely R-related - in the case of meta-analyses of correlations, the observed correlation for a given, say, sub-study can be negative, and _some_ mixed models will inappropriately truncate this contribution at zero, leading to inflated estimates for the global parameters. This comes up when meta-analysing heritability, where the genetic model (as you have pointed out) contrains this to be non-negative for a single trait.

Because of the computational difficulties, many geneticists still fit linear-normal mixed models to binary data (eg genome-wide association studies of large datasets eg UK Biobank), and don't usually get burnt. The "better" alternative for this has been PQL, implemented in several R packages.

Cheers, David Duffy.

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-mixed-models&amp;data=02%7C01%7Cd.rizopoulos%40erasmusmc.nl%7C5b7ae5701bf24a34e2a608d7ec05d2be%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C1%7C637237382072594631&amp;sdata=O%2FiLczmjWal5AWB7GVxb0MFuotrQynWbWqOACIbNSgI%3D&amp;reserved=0

	[[alternative HTML version deleted]]


From c@meron@@o @end|ng |rom m@||@utoronto@c@  Wed Apr 29 21:00:08 2020
From: c@meron@@o @end|ng |rom m@||@utoronto@c@ (Cameron So)
Date: Wed, 29 Apr 2020 19:00:08 +0000
Subject: [R-sig-ME] MCMCglmm Contrasting h2 in univariate and bivariate model
Message-ID: <YTXPR0101MB081527A295DEBA0F27AB56F7BCAD0@YTXPR0101MB0815.CANPRD01.PROD.OUTLOOK.COM>

Hello MCMCglmm users,

I am estimating the Va and heritability of a trait in separate univariate and bivariate models.
Some background beforehand: In my experiment, individuals are exposed to separate treatments.
The pedigree for each treatment is quite similar given I am using plants + multiple replicates.
Therefore, I have two univariate models: a subset of the dataset for each treatment. This has allowed
me to compare the Va and heritability estimates between treatments using the univariate models.
In the multivariate model, I include the entire dataset so I can evaluate cross-environment COVa.

I am noticing that my heritability estimates between the univariate and bivariate models differ
substantially. Could anyone offer any advice on what I am observing?

NOTE: I am not having issues of autocorrelation, nor convergence. Additionally, the univariate
Models include a second random effect term (matID) but this shouldn?t affect the additive (animal)
term that drastically. The priors obviously do differ between the univariate and bivariate models
because of the binary character of ?treatment?.

Below are my models:

## UNIVARITE MODEL EXAMPLE ##

prior8.3b <- list(R = list(V = 1, nu = 0.002),
                 G = list(G1 = list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),
                          G2 = list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000)))

HW_model8.3b <- MCMCglmm(leaf ~ plot, random = ~animal + matID,
                     ginverse = list(animal = Ainv),
                     family = "gaussian", data = heated.leaf, prior = prior8.3b,
                     nitt = 2100000, thin = 1000, burnin = 100000, verbose = T, pr = TRUE)

HW_herit8.3b <- HW_model8.3b$VCV[, "animal"]/(HW_model8.3b$VCV[, "animal"] + HW_model8.3b$VCV[, "matID"] + HW_model8.3b$VCV[, "units"])
mean(HW_herit8.3b)
#h2 = ~ 0.176

## BIVARIATE MODEL EXAMPLE ##


prior6.2 <- list(R = list(V = diag(1), fix = 1),
                 G = list(G1 = list(V = diag(2), nu = 2, alpha.mu = c(0,0), alpha.V = diag(c(1,1)))))

PL_model6.2 <- MCMCglmm(leaf ~ plot:treatment + treatment, random = ~us(treatment):animal,
                        ginverse = list(animal = Ainv), rcov=~units,
                        family = "gaussian", data = plastic.leaf, prior = prior6.2,
                        nitt = 4100000, thin = 2000, burnin = 100000, verbose = T, pr = TRUE, trunc = TRUE)

herit_PL9.2_A <-PL_model9.2$VCV[,'treatmentA:treatmentA.animal']/  (PL_model9.2$VCV[,'treatmentA:treatmentA.animal'] + 1)
mean(herit_PL9.2_A)
#h2 = ~0.73


Thank you for any help in advance!

Cameron



	[[alternative HTML version deleted]]


From w@||dm@w@@@10 @end|ng |rom gm@||@com  Wed Apr 29 22:02:52 2020
From: w@||dm@w@@@10 @end|ng |rom gm@||@com (Walid Crampton-Mawass)
Date: Wed, 29 Apr 2020 16:02:52 -0400
Subject: [R-sig-ME] MCMCglmm Contrasting h2 in univariate and bivariate
 model
In-Reply-To: <YTXPR0101MB081527A295DEBA0F27AB56F7BCAD0@YTXPR0101MB0815.CANPRD01.PROD.OUTLOOK.COM>
References: <YTXPR0101MB081527A295DEBA0F27AB56F7BCAD0@YTXPR0101MB0815.CANPRD01.PROD.OUTLOOK.COM>
Message-ID: <CAJtCY7WHpNd3FqFb6prdQoW=-5AgPnoEwC6XDus36xnVNbqnAA@mail.gmail.com>

Hello Cameron,

Based on what you have sent, I can comment on a few things that might help.

1- the second model you are using is not a bivariate model since you only
have a single response variable. What you have constructed is a univariate
model with a random interaction, in your case its the interaction of the
random term animal with the treatment variable which I presume has only two
levels since you specified a 2x2 matrix for the random term.

2- You have different priors between the two models since you fixed the
residual term to 1 which is not needed here since you are modeling the same
response variable leaf with a Gaussian distribution. This could explain the
differences you got in the heritability estimates.

3- is there a reasoning for using trunc=TRUE in your second model?

hope this helps to clear things up for you and good luck
-- 
Walid Mawass
Ph.D. student in Evolutionary Biology
Population Genetics Laboratory
University of Qu?bec at Trois-Rivi?res
3351, boul. des Forges, C.P. 500
Trois-Rivi?res (Qu?bec) G9A 5H7
Telephone: 819-376-5011 poste 3384


On Wed, Apr 29, 2020 at 3:00 PM Cameron So <cameron.so at mail.utoronto.ca>
wrote:

> Hello MCMCglmm users,
>
> I am estimating the Va and heritability of a trait in separate univariate
> and bivariate models.
> Some background beforehand: In my experiment, individuals are exposed to
> separate treatments.
> The pedigree for each treatment is quite similar given I am using plants +
> multiple replicates.
> Therefore, I have two univariate models: a subset of the dataset for each
> treatment. This has allowed
> me to compare the Va and heritability estimates between treatments using
> the univariate models.
> In the multivariate model, I include the entire dataset so I can evaluate
> cross-environment COVa.
>
> I am noticing that my heritability estimates between the univariate and
> bivariate models differ
> substantially. Could anyone offer any advice on what I am observing?
>
> NOTE: I am not having issues of autocorrelation, nor convergence.
> Additionally, the univariate
> Models include a second random effect term (matID) but this shouldn?t
> affect the additive (animal)
> term that drastically. The priors obviously do differ between the
> univariate and bivariate models
> because of the binary character of ?treatment?.
>
> Below are my models:
>
> ## UNIVARITE MODEL EXAMPLE ##
>
> prior8.3b <- list(R = list(V = 1, nu = 0.002),
>                  G = list(G1 = list(V = 1, nu = 1, alpha.mu = 0, alpha.V
> = 1000),
>                           G2 = list(V = 1, nu = 1, alpha.mu = 0, alpha.V
> = 1000)))
>
> HW_model8.3b <- MCMCglmm(leaf ~ plot, random = ~animal + matID,
>                      ginverse = list(animal = Ainv),
>                      family = "gaussian", data = heated.leaf, prior =
> prior8.3b,
>                      nitt = 2100000, thin = 1000, burnin = 100000, verbose
> = T, pr = TRUE)
>
> HW_herit8.3b <- HW_model8.3b$VCV[, "animal"]/(HW_model8.3b$VCV[, "animal"]
> + HW_model8.3b$VCV[, "matID"] + HW_model8.3b$VCV[, "units"])
> mean(HW_herit8.3b)
> #h2 = ~ 0.176
>
> ## BIVARIATE MODEL EXAMPLE ##
>
>
> prior6.2 <- list(R = list(V = diag(1), fix = 1),
>                  G = list(G1 = list(V = diag(2), nu = 2, alpha.mu =
> c(0,0), alpha.V = diag(c(1,1)))))
>
> PL_model6.2 <- MCMCglmm(leaf ~ plot:treatment + treatment, random =
> ~us(treatment):animal,
>                         ginverse = list(animal = Ainv), rcov=~units,
>                         family = "gaussian", data = plastic.leaf, prior =
> prior6.2,
>                         nitt = 4100000, thin = 2000, burnin = 100000,
> verbose = T, pr = TRUE, trunc = TRUE)
>
> herit_PL9.2_A <-PL_model9.2$VCV[,'treatmentA:treatmentA.animal']/
> (PL_model9.2$VCV[,'treatmentA:treatmentA.animal'] + 1)
> mean(herit_PL9.2_A)
> #h2 = ~0.73
>
>
> Thank you for any help in advance!
>
> Cameron
>
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From c@ro||n@@k@rou||@@ @end|ng |rom po@tgr@d@m@nche@ter@@c@uk  Thu Apr 30 10:30:55 2020
From: c@ro||n@@k@rou||@@ @end|ng |rom po@tgr@d@m@nche@ter@@c@uk (Carolina Karoullas)
Date: Thu, 30 Apr 2020 08:30:55 +0000
Subject: [R-sig-ME] Creating a phylogenetically corrected multivariate
 linear model using MCMCglmm
Message-ID: <56C3CB6634CED94898F1E5FC8E71A09CB602BF47@MBXP03.ds.man.ac.uk>

Hi all,

I'm trying to use the package MCMCglmm to run a multivariate linear model that is phylogenetically corrected. Here is a subset of my data (there are 210 entries in total for 67 species and 6 clusters):

Names PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 PC11 tips Clusters
Accipiter_gentilis1 -1.34E-01   1.98E-02    -1.72E-02   4.00E-02    -1.93E-03   2.45E-02    -1.28E-04   1.03E-02    9.09E-03    -5.33E-03   -1.30E-02   Accipiter_gentilis  "Soaring"
Accipiter_gentilis2 -1.26E-01   1.22E-02    1.66E-02    9.22E-03    1.15E-02    1.68E-02    -1.50E-02   1.02E-02    7.93E-03    -8.47E-03   -1.02E-02   Accipiter_gentilis  "Soaring"
Accipiter_nisus1    -1.81E-01   5.76E-02    -1.82E-02   6.15E-02    -9.25E-03   3.40E-02    -1.77E-02   5.45E-03    7.01E-03    -2.07E-02   -8.78E-03   Accipiter_nisus "Soaring"
Accipiter_nisus2    -2.00E-01   7.05E-02    -1.12E-02   5.94E-02    3.49E-03    3.10E-02    -1.58E-03   -1.55E-03   6.92E-03    -3.54E-02   -1.80E-02   Accipiter_nisus "Soaring"
Accipiter_nisus3    -8.14E-02   -3.39E-04   -8.88E-03   4.25E-02    -5.48E-04   -8.51E-03   5.07E-03    4.56E-03    1.97E-02    -1.46E-02   -1.43E-02   Accipiter_nisus "Soaring"
Accipiter_nisus4    -2.06E-01   7.05E-02    -2.17E-02   6.38E-02    -1.61E-02   2.80E-02    8.70E-03    -5.96E-03   6.15E-03    -5.29E-02   -2.05E-02   Accipiter_nisus "Soaring"
Actitis_hypoleucos1 2.27E-02    -2.74E-03   4.79E-02    -2.30E-02   -2.76E-02   -2.36E-02   1.70E-02    2.43E-03    3.82E-03    1.15E-02    -9.87E-03   Actitis_hypoleucos  "Continuous flapping"
Actitis_hypoleucos2 6.67E-02    -1.05E-02   5.12E-02    -2.65E-02   -3.21E-02   -2.61E-02   3.21E-03    7.46E-03    7.29E-03    4.70E-03    -1.37E-02   Actitis_hypoleucos  "Continuous flapping"
Aix_sponsa1 -3.70E-02   -1.41E-02   1.13E-02    3.16E-02    2.32E-02    -1.70E-02   2.32E-02    1.91E-03    2.91E-02    -7.71E-03   7.40E-03    Aix_sponsa  "Continuous flapping"
Aix_sponsa2 1.03E-02    -4.08E-02   -6.62E-03   1.19E-02    2.83E-02    -1.49E-02   3.78E-02    6.98E-03    2.91E-02    -4.32E-03   2.54E-03    Aix_sponsa  "Continuous flapping"
Aix_sponsa3 1.19E-02    -3.48E-02   -1.53E-02   3.76E-03    2.17E-02    1.47E-02    8.84E-03    2.39E-02    -9.20E-03   -1.78E-02   8.76E-04    Aix_sponsa  "Continuous flapping"
Aix_sponsa4 -3.37E-02   -1.75E-02   -8.06E-03   3.64E-02    -5.50E-03   1.03E-02    2.37E-02    3.33E-03    -1.04E-03   -2.00E-02   5.89E-03    Aix_sponsa  "Continuous flapping"
Aix_sponsa5 -2.30E-02   -9.59E-03   1.06E-02    3.01E-02    7.10E-03    -1.23E-02   2.08E-02    1.17E-02    1.59E-03    2.83E-03    8.75E-03    Aix_sponsa  "Continuous flapping"
Aix_sponsa6 -1.70E-02   -2.98E-02   -1.96E-02   1.76E-02    1.23E-02    4.92E-03    5.45E-03    1.99E-02    -6.43E-03   -9.63E-04   1.99E-03    Aix_sponsa  "Continuous flapping"
Aix_sponsa7 2.57E-02    -4.22E-02   -1.60E-02   1.75E-02    3.41E-03    5.80E-03    2.89E-02    6.10E-03    7.12E-03    2.75E-03    4.99E-03    Aix_sponsa  "Continuous flapping"
Aix_sponsa8 4.09E-02    -4.46E-02   -4.49E-03   2.24E-02    2.37E-03    -5.90E-03   2.78E-02    -8.26E-04   1.17E-02    -5.71E-03   -1.77E-03   Aix_sponsa  "Continuous flapping"

I created a univariate model taking inspiration from this link:

https://github.com/JonBrommer/Multivariate-Mixed-Models-in-R/wiki/MCMCglmm-examples#organisational-level-4https://github.com/JonBrommer/Multivariate-Mixed-Models-in-R/wiki/MCMCglmm-examples#organisational-level-4

And when I try to run it, it works (code below, phylo refers to my tree):

Ainv<-inverseA(phylo,nodes="TIPS",scale=F)$Ainv
p.var=var(data[,c("PC1")])
prior1<-list(R=list(V=(p.var),nu=0.002),G=list(G1=list(V=(p.var),nu=0.002)))
m7.phylo<-MCMCglmm(PC1~Clusters,
                   random=~tips,
                   family=rep("gaussian",1),
                   ginverse=list(tips=Ainv),
                   data=data,
                   prior=prior1)

However, as soon as I try to make a multivariate model, I get an error:

Ainv<-inverseA(phylo,nodes="TIPS",scale=F)$Ainv
p.var=var(data[,c("PC1","PC2")])
prior1<-list(R=list(V=(diag(2)*p.var),nu=0.002),G=list(G1=list(V=(diag(2)*p.var),nu=0.002)))
m7.phylo<-MCMCglmm(cbind(PC1,PC2)~Clusters,
                   random=~tips,
                   family=rep("gaussian",2),
                   ginverse=list(tips=Ainv),
                   data=data,
                   prior=prior1)

Error in priorformat(if (NOpriorG) { :
  V is the wrong dimension for some prior$G/prior$R elements

I do want to use all 11 PCs in the model so it's not encouraging that I can't seem to get it to work with just 2 of them...  Does anyone have any ideas of what could have gone wrong?  I would like to create other models with the same structure using different data and trees so it would be good to understand what's going on and how to create a prior properly for next time.

Thanks,

Carolina

	[[alternative HTML version deleted]]


From hkubr@@eno| @end|ng |rom gm@||@com  Thu Apr 30 17:33:45 2020
From: hkubr@@eno| @end|ng |rom gm@||@com (HATICE T KUBRA AKDUR)
Date: Thu, 30 Apr 2020 18:33:45 +0300
Subject: [R-sig-ME] question on unit variance function
In-Reply-To: <CA+_DO+xpy0av0_vGG_QRzUrMXSxuk_m0Uh_uLt0my-PVe4gRcA@mail.gmail.com>
References: <CA+_DO+xpy0av0_vGG_QRzUrMXSxuk_m0Uh_uLt0my-PVe4gRcA@mail.gmail.com>
Message-ID: <CA+_DO+w_vHUqrwTsDwu-n7kTXtgZ-gZqs2Z1u72PGa9G-6Ncog@mail.gmail.com>

Dear Group Members,

I hope you are keeping well during this difficult time.
Thank you in advance.
I have some questions below:

In the beta regression model, it is stated V(\mu)= \mu(1-\mu).

It is known that even though beta distribution is in the exponential
family, logit link is not canonical link for beta regression.

And, V(\mu)= \mu(1-\mu) is unit variance function for binomial
distribution, logit link is canonical for binomial distribution.

 I am not sure,  V(mu)=mu(1-mu), is it still unit variance function for
beta regression model with logit link.  Even though, it is not in natural
exponential family. OR we can say that V(\mu)= \mu(1-\mu) is variance
function for beta regression with logit link.

1/g'(mu) (g(mu) is link function) can be used to obtain unit variance
function for not natural exponential family distribution? or it is only
true for natural exponential family?

For example, unit variance function of Simplex distribution is
V(mu)=mu_3(1-mu)_3
because simplex distribution is in dispersion family. So, to obtain unit
variance function, the second order derivative of the deviance function is
used.

I will be very glad, if you enlighten me.

Best,

References:

https://onlinelibrary.wiley.com/doi/abs/10.1111/j.0006-341X.2000.00496.x

https://www.tandfonline.com/doi/pdf/10.1080/0266476042000214501

On Thu, Apr 30, 2020 at 11:10 AM HATICE T KUBRA AKDUR <hkubrasenol at gmail.com>
wrote:

> Dear Group Members,
>
> I hope you are keeping well during this difficult time.
> Thank you in advance.
> I have some questions below:
>
> In the beta regression model, it is stated V(\mu)= \mu(1-\mu).
>
> It is known that even though beta distribution is in the exponential
> family, logit link is not canonical link for beta regression.
>
> And, V(\mu)= \mu(1-\mu) is unit variance function for binomial
> distribution, logit link is canonical for binomial distribution.
>
>  I am not sure,  V(mu)=mu(1-mu), is it still unit variance function for
> beta regression model with logit link.  Even though, it is not in natural
> exponential family. OR we can say that V(\mu)= \mu(1-\mu) is variance
> function for beta regression with logit link.
>
> 1/g'(mu) (g(mu) is link function) can be used to obtain unit variance
> function for not natural exponential family distribution? or it is only
> true for natural exponential family?
>
> For example, unit variance function of Simplex distribution is V(mu)=mu_3(1-mu)_3
> because simplex distribution is in dispersion family. So, to obtain unit
> variance function, the second order derivative of the deviance function
> is used.
>
> I will be very glad, if you enlighten me.
> I attached beta regression and marginal simplex model papers FYI.
>
> Best Regards,
> --
> Asst. Prof. Dr. Hatice T. Kubra AKDUR
> Department of Statistics, Faculty of Science
> Gazi University
> 06500 Teknikokullar ANKARA, TURKEY
> Phone: +90 553 324 5380
> Email: hatice_senol at wsu.edu
> Homepage: http://websitem.gazi.edu.tr/site/haticesenol
>


-- 
Asst. Prof. Dr. Hatice T. Kubra AKDUR
Department of Statistics, Faculty of Science
Gazi University
06500 Teknikokullar ANKARA, TURKEY
Phone: +90 553 324 5380
Email: hatice_senol at wsu.edu
Homepage: http://websitem.gazi.edu.tr/site/haticesenol

	[[alternative HTML version deleted]]


From ger@|ttee @end|ng |rom gm@||@com  Fri May  1 12:20:42 2020
From: ger@|ttee @end|ng |rom gm@||@com (Szymek Drobniak)
Date: Fri, 1 May 2020 12:20:42 +0200
Subject: [R-sig-ME] R-sig-mixed-models Digest, Vol 161, Issue 1
In-Reply-To: <mailman.18297.9.1588327202.19518.r-sig-mixed-models@r-project.org>
References: <mailman.18297.9.1588327202.19518.r-sig-mixed-models@r-project.org>
Message-ID: <98b1c1f9-5b7b-47a3-b2d9-9f224b301c5f@Spark>

Hi Carolina, your second model assumes homogenous tips variance across variables (random=~tips) - but in your prior you structure both tips and residual variances as 2x2 matrices. In the second model you have to use us/idh variance function for tips effects (e.g. us(trait):tips), and add rcov term to model similar structure for units (e.g. rcov=~us(trait):units).

Cheers,
Szymek



Dr hab. Szymon Drobniak

Institute of Environmental Sciences
Jagiellonian University, Krak?w, Poland

School of Biological, Environmental and Earth Sciences
University of New South Wales, Sydney, Australia

Google Scholar profile
szymekdrobniak.wordpress.com
szdrobniak.pl

>
> Message: 1
> Date: Thu, 30 Apr 2020 08:30:55 +0000
> From: Carolina Karoullas
> <carolina.karoullas at postgrad.manchester.ac.uk>
> To: "r-sig-mixed-models at r-project.org"
> <r-sig-mixed-models at r-project.org>
> Subject: [R-sig-ME] Creating a phylogenetically corrected multivariate
> linear model using MCMCglmm
> Message-ID:
> <56C3CB6634CED94898F1E5FC8E71A09CB602BF47 at MBXP03.ds.man.ac.uk>
> Content-Type: text/plain; charset="utf-8"
>
> Hi all,
>
> I'm trying to use the package MCMCglmm to run a multivariate linear model that is phylogenetically corrected. Here is a subset of my data (there are 210 entries in total for 67 species and 6 clusters):
>
> Names PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 PC11 tips Clusters
> Accipiter_gentilis1 -1.34E-01 1.98E-02 -1.72E-02 4.00E-02 -1.93E-03 2.45E-02 -1.28E-04 1.03E-02 9.09E-03 -5.33E-03 -1.30E-02 Accipiter_gentilis "Soaring"
> Accipiter_gentilis2 -1.26E-01 1.22E-02 1.66E-02 9.22E-03 1.15E-02 1.68E-02 -1.50E-02 1.02E-02 7.93E-03 -8.47E-03 -1.02E-02 Accipiter_gentilis "Soaring"
> Accipiter_nisus1 -1.81E-01 5.76E-02 -1.82E-02 6.15E-02 -9.25E-03 3.40E-02 -1.77E-02 5.45E-03 7.01E-03 -2.07E-02 -8.78E-03 Accipiter_nisus "Soaring"
> Accipiter_nisus2 -2.00E-01 7.05E-02 -1.12E-02 5.94E-02 3.49E-03 3.10E-02 -1.58E-03 -1.55E-03 6.92E-03 -3.54E-02 -1.80E-02 Accipiter_nisus "Soaring"
> Accipiter_nisus3 -8.14E-02 -3.39E-04 -8.88E-03 4.25E-02 -5.48E-04 -8.51E-03 5.07E-03 4.56E-03 1.97E-02 -1.46E-02 -1.43E-02 Accipiter_nisus "Soaring"
> Accipiter_nisus4 -2.06E-01 7.05E-02 -2.17E-02 6.38E-02 -1.61E-02 2.80E-02 8.70E-03 -5.96E-03 6.15E-03 -5.29E-02 -2.05E-02 Accipiter_nisus "Soaring"
> Actitis_hypoleucos1 2.27E-02 -2.74E-03 4.79E-02 -2.30E-02 -2.76E-02 -2.36E-02 1.70E-02 2.43E-03 3.82E-03 1.15E-02 -9.87E-03 Actitis_hypoleucos "Continuous flapping"
> Actitis_hypoleucos2 6.67E-02 -1.05E-02 5.12E-02 -2.65E-02 -3.21E-02 -2.61E-02 3.21E-03 7.46E-03 7.29E-03 4.70E-03 -1.37E-02 Actitis_hypoleucos "Continuous flapping"
> Aix_sponsa1 -3.70E-02 -1.41E-02 1.13E-02 3.16E-02 2.32E-02 -1.70E-02 2.32E-02 1.91E-03 2.91E-02 -7.71E-03 7.40E-03 Aix_sponsa "Continuous flapping"
> Aix_sponsa2 1.03E-02 -4.08E-02 -6.62E-03 1.19E-02 2.83E-02 -1.49E-02 3.78E-02 6.98E-03 2.91E-02 -4.32E-03 2.54E-03 Aix_sponsa "Continuous flapping"
> Aix_sponsa3 1.19E-02 -3.48E-02 -1.53E-02 3.76E-03 2.17E-02 1.47E-02 8.84E-03 2.39E-02 -9.20E-03 -1.78E-02 8.76E-04 Aix_sponsa "Continuous flapping"
> Aix_sponsa4 -3.37E-02 -1.75E-02 -8.06E-03 3.64E-02 -5.50E-03 1.03E-02 2.37E-02 3.33E-03 -1.04E-03 -2.00E-02 5.89E-03 Aix_sponsa "Continuous flapping"
> Aix_sponsa5 -2.30E-02 -9.59E-03 1.06E-02 3.01E-02 7.10E-03 -1.23E-02 2.08E-02 1.17E-02 1.59E-03 2.83E-03 8.75E-03 Aix_sponsa "Continuous flapping"
> Aix_sponsa6 -1.70E-02 -2.98E-02 -1.96E-02 1.76E-02 1.23E-02 4.92E-03 5.45E-03 1.99E-02 -6.43E-03 -9.63E-04 1.99E-03 Aix_sponsa "Continuous flapping"
> Aix_sponsa7 2.57E-02 -4.22E-02 -1.60E-02 1.75E-02 3.41E-03 5.80E-03 2.89E-02 6.10E-03 7.12E-03 2.75E-03 4.99E-03 Aix_sponsa "Continuous flapping"
> Aix_sponsa8 4.09E-02 -4.46E-02 -4.49E-03 2.24E-02 2.37E-03 -5.90E-03 2.78E-02 -8.26E-04 1.17E-02 -5.71E-03 -1.77E-03 Aix_sponsa "Continuous flapping"
>
> I created a univariate model taking inspiration from this link:
>
> https://github.com/JonBrommer/Multivariate-Mixed-Models-in-R/wiki/MCMCglmm-examples#organisational-level-4https://github.com/JonBrommer/Multivariate-Mixed-Models-in-R/wiki/MCMCglmm-examples#organisational-level-4
>
> And when I try to run it, it works (code below, phylo refers to my tree):
>
> Ainv<-inverseA(phylo,nodes="TIPS",scale=F)$Ainv
> p.var=var(data[,c("PC1")])
> prior1<-list(R=list(V=(p.var),nu=0.002),G=list(G1=list(V=(p.var),nu=0.002)))
> m7.phylo<-MCMCglmm(PC1~Clusters,
> random=~tips,
> family=rep("gaussian",1),
> ginverse=list(tips=Ainv),
> data=data,
> prior=prior1)
>
> However, as soon as I try to make a multivariate model, I get an error:
>
> Ainv<-inverseA(phylo,nodes="TIPS",scale=F)$Ainv
> p.var=var(data[,c("PC1","PC2")])
> prior1<-list(R=list(V=(diag(2)*p.var),nu=0.002),G=list(G1=list(V=(diag(2)*p.var),nu=0.002)))
> m7.phylo<-MCMCglmm(cbind(PC1,PC2)~Clusters,
> random=~tips,
> family=rep("gaussian",2),
> ginverse=list(tips=Ainv),
> data=data,
> prior=prior1)
>
> Error in priorformat(if (NOpriorG) { :
> V is the wrong dimension for some prior$G/prior$R elements
>
> I do want to use all 11 PCs in the model so it's not encouraging that I can't seem to get it to work with just 2 of them... Does anyone have any ideas of what could have gone wrong? I would like to create other models with the same structure using different data and trees so it would be good to understand what's going on and how to create a prior properly for next time.
>
> Thanks,
>
> Carolina
>
>

	[[alternative HTML version deleted]]


From hou@|@y @end|ng |rom gm@||@com  Fri May  1 12:28:47 2020
From: hou@|@y @end|ng |rom gm@||@com (Tom Houslay)
Date: Fri, 1 May 2020 11:28:47 +0100
Subject: [R-sig-ME] Creating a phylogenetically corrected multivariate
 linear model using MCMCglmm
In-Reply-To: <mailman.18297.9.1588327202.19518.r-sig-mixed-models@r-project.org>
References: <mailman.18297.9.1588327202.19518.r-sig-mixed-models@r-project.org>
Message-ID: <CAErKyRo0ZBpPE-rZkjk6Q7+hgji5O18vXo4SrgX8ydOeWcMcyg@mail.gmail.com>

Hi Carolina,

Looks like you have used the prior from Jon's 'multivariate
phylogenetic example' section but haven't updated the random and rcov
terms in your MCMCglmm model accordingly, which should be something
like:

random=~us(trait):tips, rcov=~us(trait):units

Hope that helps...

Cheers

Tom


>
> Date: Thu, 30 Apr 2020 08:30:55 +0000
> From: Carolina Karoullas
>         <carolina.karoullas at postgrad.manchester.ac.uk>
> To: "r-sig-mixed-models at r-project.org"
>         <r-sig-mixed-models at r-project.org>
> Subject: [R-sig-ME] Creating a phylogenetically corrected multivariate
>         linear model using MCMCglmm
> Message-ID:
>         <56C3CB6634CED94898F1E5FC8E71A09CB602BF47 at MBXP03.ds.man.ac.uk>
> Content-Type: text/plain; charset="utf-8"
>
> Hi all,
>
> I'm trying to use the package MCMCglmm to run a multivariate linear model that is phylogenetically corrected. Here is a subset of my data (there are 210 entries in total for 67 species and 6 clusters):
>
> Names PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 PC11 tips Clusters
> Accipiter_gentilis1 -1.34E-01   1.98E-02    -1.72E-02   4.00E-02    -1.93E-03   2.45E-02    -1.28E-04   1.03E-02    9.09E-03    -5.33E-03   -1.30E-02   Accipiter_gentilis  "Soaring"
> Accipiter_gentilis2 -1.26E-01   1.22E-02    1.66E-02    9.22E-03    1.15E-02    1.68E-02    -1.50E-02   1.02E-02    7.93E-03    -8.47E-03   -1.02E-02   Accipiter_gentilis  "Soaring"
> Accipiter_nisus1    -1.81E-01   5.76E-02    -1.82E-02   6.15E-02    -9.25E-03   3.40E-02    -1.77E-02   5.45E-03    7.01E-03    -2.07E-02   -8.78E-03   Accipiter_nisus "Soaring"
> Accipiter_nisus2    -2.00E-01   7.05E-02    -1.12E-02   5.94E-02    3.49E-03    3.10E-02    -1.58E-03   -1.55E-03   6.92E-03    -3.54E-02   -1.80E-02   Accipiter_nisus "Soaring"
> Accipiter_nisus3    -8.14E-02   -3.39E-04   -8.88E-03   4.25E-02    -5.48E-04   -8.51E-03   5.07E-03    4.56E-03    1.97E-02    -1.46E-02   -1.43E-02   Accipiter_nisus "Soaring"
> Accipiter_nisus4    -2.06E-01   7.05E-02    -2.17E-02   6.38E-02    -1.61E-02   2.80E-02    8.70E-03    -5.96E-03   6.15E-03    -5.29E-02   -2.05E-02   Accipiter_nisus "Soaring"
> Actitis_hypoleucos1 2.27E-02    -2.74E-03   4.79E-02    -2.30E-02   -2.76E-02   -2.36E-02   1.70E-02    2.43E-03    3.82E-03    1.15E-02    -9.87E-03   Actitis_hypoleucos  "Continuous flapping"
> Actitis_hypoleucos2 6.67E-02    -1.05E-02   5.12E-02    -2.65E-02   -3.21E-02   -2.61E-02   3.21E-03    7.46E-03    7.29E-03    4.70E-03    -1.37E-02   Actitis_hypoleucos  "Continuous flapping"
> Aix_sponsa1 -3.70E-02   -1.41E-02   1.13E-02    3.16E-02    2.32E-02    -1.70E-02   2.32E-02    1.91E-03    2.91E-02    -7.71E-03   7.40E-03    Aix_sponsa  "Continuous flapping"
> Aix_sponsa2 1.03E-02    -4.08E-02   -6.62E-03   1.19E-02    2.83E-02    -1.49E-02   3.78E-02    6.98E-03    2.91E-02    -4.32E-03   2.54E-03    Aix_sponsa  "Continuous flapping"
> Aix_sponsa3 1.19E-02    -3.48E-02   -1.53E-02   3.76E-03    2.17E-02    1.47E-02    8.84E-03    2.39E-02    -9.20E-03   -1.78E-02   8.76E-04    Aix_sponsa  "Continuous flapping"
> Aix_sponsa4 -3.37E-02   -1.75E-02   -8.06E-03   3.64E-02    -5.50E-03   1.03E-02    2.37E-02    3.33E-03    -1.04E-03   -2.00E-02   5.89E-03    Aix_sponsa  "Continuous flapping"
> Aix_sponsa5 -2.30E-02   -9.59E-03   1.06E-02    3.01E-02    7.10E-03    -1.23E-02   2.08E-02    1.17E-02    1.59E-03    2.83E-03    8.75E-03    Aix_sponsa  "Continuous flapping"
> Aix_sponsa6 -1.70E-02   -2.98E-02   -1.96E-02   1.76E-02    1.23E-02    4.92E-03    5.45E-03    1.99E-02    -6.43E-03   -9.63E-04   1.99E-03    Aix_sponsa  "Continuous flapping"
> Aix_sponsa7 2.57E-02    -4.22E-02   -1.60E-02   1.75E-02    3.41E-03    5.80E-03    2.89E-02    6.10E-03    7.12E-03    2.75E-03    4.99E-03    Aix_sponsa  "Continuous flapping"
> Aix_sponsa8 4.09E-02    -4.46E-02   -4.49E-03   2.24E-02    2.37E-03    -5.90E-03   2.78E-02    -8.26E-04   1.17E-02    -5.71E-03   -1.77E-03   Aix_sponsa  "Continuous flapping"
>
> I created a univariate model taking inspiration from this link:
>
> https://github.com/JonBrommer/Multivariate-Mixed-Models-in-R/wiki/MCMCglmm-examples#organisational-level-4https://github.com/JonBrommer/Multivariate-Mixed-Models-in-R/wiki/MCMCglmm-examples#organisational-level-4
>
> And when I try to run it, it works (code below, phylo refers to my tree):
>
> Ainv<-inverseA(phylo,nodes="TIPS",scale=F)$Ainv
> p.var=var(data[,c("PC1")])
> prior1<-list(R=list(V=(p.var),nu=0.002),G=list(G1=list(V=(p.var),nu=0.002)))
> m7.phylo<-MCMCglmm(PC1~Clusters,
>                    random=~tips,
>                    family=rep("gaussian",1),
>                    ginverse=list(tips=Ainv),
>                    data=data,
>                    prior=prior1)
>
> However, as soon as I try to make a multivariate model, I get an error:
>
> Ainv<-inverseA(phylo,nodes="TIPS",scale=F)$Ainv
> p.var=var(data[,c("PC1","PC2")])
> prior1<-list(R=list(V=(diag(2)*p.var),nu=0.002),G=list(G1=list(V=(diag(2)*p.var),nu=0.002)))
> m7.phylo<-MCMCglmm(cbind(PC1,PC2)~Clusters,
>                    random=~tips,
>                    family=rep("gaussian",2),
>                    ginverse=list(tips=Ainv),
>                    data=data,
>                    prior=prior1)
>
> Error in priorformat(if (NOpriorG) { :
>   V is the wrong dimension for some prior$G/prior$R elements
>
> I do want to use all 11 PCs in the model so it's not encouraging that I can't seem to get it to work with just 2 of them...  Does anyone have any ideas of what could have gone wrong?  I would like to create other models with the same structure using different data and trees so it would be good to understand what's going on and how to create a prior properly for next time.
>
> Thanks,
>
> Carolina
>
>


From chr|@ho|d @end|ng |rom p@yctc@org  Fri May  1 20:37:59 2020
From: chr|@ho|d @end|ng |rom p@yctc@org (Chris Evans)
Date: Fri, 1 May 2020 19:37:59 +0100 (BST)
Subject: [R-sig-ME] Struggling (probably problem in wetware not software)
 with multilevel logistic regression
Message-ID: <100063919.9989289.1588358279505.JavaMail.zimbra@psyctc.org>

I am sorry if this is embarrassingly stupid/easy but I am failing to understand something I thought would be fairly easy.

The question of interest to me and others is whether people omit some items of a questionnaire more than others.  
To give a simple example I have several datasets for a ten item self-report questionnaire where one item is about risk
and where three items are cued positively and the other seven are cued negatively.

Here is an example of some such data:

   ID       Item     Score missing missRand missReg riskItem posItem
   <chr>    <chr>    <dbl>   <dbl>    <int>   <int>    <dbl>   <dbl>
 1 ALCA0001 YPCORE01     1       0        0       0        0       0
 2 ALCA0001 YPCORE02     0       0        0       0        0       0
 3 ALCA0001 YPCORE03     0       0        0       0        0       1
 4 ALCA0001 YPCORE04     0       0        0       0        1       0
 5 ALCA0001 YPCORE05     0       0        0       0        0       1
 6 ALCA0001 YPCORE06     1       0        0       0        0       0
 7 ALCA0001 YPCORE07     0       0        0       0        0       0
 8 ALCA0001 YPCORE08     0       0        0       0        0       0
 9 ALCA0001 YPCORE09     0       0        0       0        0       0
10 ALCA0001 YPCORE10     0       0        0       0        0       1
11 ALCA0004 YPCORE01     1       0        0       0        0       0
12 ALCA0004 YPCORE02     0       0        0       0        0       0
13 ALCA0004 YPCORE03     0       0        0       0        0       1
14 ALCA0004 YPCORE04     0       0        0       0        1       0
15 ALCA0004 YPCORE05     0       0        0       0        0       1

missing is the observed omission of the item (1 = omitted)
missRand is a binomial random with probability = .5 I created
missReg is a biomial random with probability = item index number / 11, i.e. strong relationship with item
riskItem identifies the risk items (item 4)
posItem identifies the positively cued items (3, 5 and 10)

For this particular dataset there are 237 participants.  (Like Groucho Marks and principles, I have 
others if you don't like this one!)

The catch is that each participant has only completed the measure once.  It is a plausible model that 
participants will vary in their propensity to omit items, it is also highly plausible that participants
might omit the risk item more than the other item. It's a question of interest (though I think not of 
huge interest nor would I bet on it (!) that the positively cued items might be omitted more than the 
negatively cued ones or v.v.

I thought I could test for these effects using glmer().  That works for 

> fitRisk <- glmer(missing ~ riskItem + (1 | ID), family = binomial, data = longDat)
> summary(fitRisk)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: missing ~ riskItem + (1 | ID)
   Data: longDat

     AIC      BIC   logLik deviance df.resid 
    51.7     69.0    -22.9     45.7     2367 

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-0.52020 -0.00113 -0.00113 -0.00113  2.93829 

Random effects:
 Groups Name        Variance Std.Dev.
 ID     (Intercept) 205.3    14.33   
Number of obs: 2370, groups:  ID, 237

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -13.563      2.540  -5.339 9.33e-08 ***
riskItem      -2.418      3.745  -0.646    0.518    
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Correlation of Fixed Effects:
         (Intr)
riskItem 0.064 

No effect but small dataset and test of principle works.  However, when I come to test the overall difference by items I hit 

5       NA     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA
  BITE23 BITE24 BITE25 BITE26 BITE27 BITE28 BITE29 BITE30 BITE31 BITE32 BITE33 Notas origRow Centro2 transfer one
1     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA  <NA>      43  Urgell        0   1
2     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA  <NA>      68  Urgell        0   1
3     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA  <NA>      69  Urgell        0   1
4     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA  <NA>      70  Urgell        0   1
5     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA  <NA>      71  Urgell        0   1
  Exclude quaireN ID.quaireN Nquaires  ID.episodeN episodeN nEpisodes Fecha.inicio  Fecha.fin ExpCOREOM ExpSFA ExpSFB
1       0       5 URGE0003-5       27 URGE0003-001        1         1   2017-11-13 2019-02-12        NA     NA     NA
2       0       3 URGE0004-3       36 URGE0004-001        1         2   2017-11-21 2018-09-10        NA     NA     NA
3       0       4 URGE0004-4       36 URGE0004-001        1         2   2017-11-21 2018-09-10        NA     NA     NA
4       0       5 URGE0004-5       36 URGE0004-001        1         2   2017-11-21 2018-09-10        NA     NA     NA
5       0       6 URGE0004-6       36 URGE0004-001        1         2   2017-11-21 2018-09-10        NA     NA     NA
  ExpYPCORE ExpEAT ExpBITE missingCOREOM missingSFA missingSFB missingYP missingEAT missingBITE impossCOREOM impossSFA
1        NA     NA      NA            NA         NA         NA        NA         NA          NA           NA        NA
2        NA     NA      NA            NA         NA         NA        NA         NA          NA           NA        NA
3        NA     NA      NA            NA         NA         NA        NA         NA          NA           NA        NA
4        NA     NA      NA            NA         NA         NA        NA         NA          NA           NA        NA
5        NA     NA      NA            NA         NA         NA        NA         NA          NA           NA        NA
  impossSFB impossYP impossEAT impossBITE
1        NA       NA        NA         NA
2        NA       NA        NA         NA
3        NA       NA        NA         NA
4        NA       NA        NA         NA
5        NA       NA        NA         NA
 [ reached 'max' / getOption("max.print") -- omitted 672 rows ]
> datPrincip_URGE %>%
+   arrange(Fecha.evaluacion) %>% # sort by evaluation date
+    filter(Cuestionarios == "YP-CORE" &
+            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
+   dim()
[1] 677 195
> datPrincip_URGE %>%
+   arrange(Fecha.evaluacion) %>% # sort by evaluation date
+    filter(Cuestionarios == "YP-CORE" &
+            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
+   group_by(ID) %>%
+   mutate(occasion = row_number(),
+          first = if_else(occasion == 1, 1, 0)) %>% # to get first ocassion only
+   filter(first == 1) %>%
+     dim()
[1]  61 197
> dat <- bind_rows(datPrincip_ALCA,
+                  datPrincip_URGE)
> dat %>%
+   arrange(Fecha.evaluacion) %>% # sort by evaluation date
+    filter(Cuestionarios == "YP-CORE" &
+            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
+   group_by(ID) %>%
+   mutate(occasion = row_number(),
+          first = if_else(occasion == 1, 1, 0)) %>% # to get first ocassion only
+   filter(first == 1) %>%
+     dim()
[1] 118 197
> dat <- bind_rows(datPrincip_ALCA,
+                  datPrincip_ARGE,
+                  datPrincip_AVEN,
+                  datPrincip_MOSC,
+                  datPrincip_SABA,
+                  datPrincip_SEVI,
+                  datPrincip_TARR,
+                  datPrincip_URGE)
> dat %>%
+   arrange(Fecha.evaluacion) %>% # sort by evaluation date
+    filter(Cuestionarios == "YP-CORE" &
+            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
+   group_by(ID) %>%
+   mutate(occasion = row_number(),
+          first = if_else(occasion == 1, 1, 0)) %>% # to get first ocassion only
+   filter(first == 1) %>%
+     dim()
[1] 237 197
> dat %>%
+   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
+    filter(Cuestionarios == "YP-CORE" &
+            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
+   group_by(ID) %>%
+   mutate(occasion = row_number(),
+          first = if_else(occasion == 1, 1, 0)) %>% # to get first ocassion only
+   filter(first == 1) %>%
+     dim()
[1] 237 197
> dat %>%
+   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
+    filter(Cuestionarios == "YP-CORE" &
+            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
+   group_by(ID) %>%
+   mutate(occasion = row_number(),
+          first = if_else(occasion == 1, 1, 0)) %>% # to get first ocassion only
+   filter(first == 1) %>%
+   select(ID, starts_with("YP")) %>%
+   dim()
[1] 237  11
> dat %>%
+   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
+    filter(Cuestionarios == "YP-CORE" &
+            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
+   group_by(ID) %>%
+   mutate(occasion = row_number(),
+          first = if_else(occasion == 1, 1, 0)) %>% # to get first ocassion only
+   filter(first == 1) %>%
+   select(ID, starts_with("YP")) %>%
+   pivot_longer(cols = starts_with("YPCORE")) %>% 
+   dim()
[1] 2370    3
> dat %>%
+   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
+    filter(Cuestionarios == "YP-CORE" &
+            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
+   group_by(ID) %>%
+   mutate(occasion = row_number(),
+          first = if_else(occasion == 1, 1, 0)) %>% # to get first ocassion only
+   filter(first == 1) %>%
+   select(ID, starts_with("YP")) %>%
+   pivot_longer(cols = starts_with("YPCORE")) %>% 
+   mutate(missing = if_else(!is.na(value), 0, 1)) -> longDat
> head(longDat)
# A tibble: 6 x 4
# Groups:   ID [1]
  ID       name     value missing
  <chr>    <chr>    <dbl>   <dbl>
1 ALCA0001 YPCORE01     1       0
2 ALCA0001 YPCORE02     0       0
3 ALCA0001 YPCORE03     0       0
4 ALCA0001 YPCORE04     0       0
5 ALCA0001 YPCORE05     0       0
6 ALCA0001 YPCORE06     1       0
> table(longDat$missing)

   0    1 
2356   14 
> ?pivot_longer
> dat %>%
+   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
+    filter(Cuestionarios == "YP-CORE" &
+            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
+   group_by(ID) %>%
+   mutate(occasion = row_number(),
+          first = if_else(occasion == 1, 1, 0)) %>% # to get first ocassion only
+   filter(first == 1) %>%
+   select(ID, starts_with("YP")) %>%
+   pivot_longer(cols = starts_with("YPCORE"),
+                names_to = "Item") %>% 
+   mutate(missing = if_else(!is.na(value), 0, 1)) -> longDat
> head(longD)
Error in head(longD) : object 'longD' not found
> head(longDat)
# A tibble: 6 x 4
# Groups:   ID [1]
  ID       Item     value missing
  <chr>    <chr>    <dbl>   <dbl>
1 ALCA0001 YPCORE01     1       0
2 ALCA0001 YPCORE02     0       0
3 ALCA0001 YPCORE03     0       0
4 ALCA0001 YPCORE04     0       0
5 ALCA0001 YPCORE05     0       0
6 ALCA0001 YPCORE06     1       0
> dat %>%
+   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
+    filter(Cuestionarios == "YP-CORE" &
+            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
+   group_by(ID) %>%
+   mutate(occasion = row_number(),
+          first = if_else(occasion == 1, 1, 0)) %>% # to get first ocassion only
+   filter(first == 1) %>%
+   select(ID, starts_with("YP")) %>%
+   pivot_longer(cols = starts_with("YPCORE"),
+                names_to = "Item",
+                values_to = "Score") %>% 
+   mutate(missing = if_else(!is.na(Score), 0, 1)) -> longDat
> head(longDat)
# A tibble: 6 x 4
# Groups:   ID [1]
  ID       Item     Score missing
  <chr>    <chr>    <dbl>   <dbl>
1 ALCA0001 YPCORE01     1       0
2 ALCA0001 YPCORE02     0       0
3 ALCA0001 YPCORE03     0       0
4 ALCA0001 YPCORE04     0       0
5 ALCA0001 YPCORE05     0       0
6 ALCA0001 YPCORE06     1       0
> formul1 <- missing ~ Item + Item | ID
> fit <- glmer(formul1, family = binomial, data = longDat)
Error in glmer(formul1, family = binomial, data = longDat) : 
  could not find function "glmer"
> library(lme4)
Loading required package: Matrix

Attaching package: ?Matrix?

The following objects are masked from ?package:tidyr?:

    expand, pack, unpack

> formul1 <- missing ~ Item + Item | ID
> fit <- glmer(formul1, family = binomial, data = longDat)
boundary (singular) fit: see ?isSingular
Warning messages:
1: In commonArgs(par, fn, control, environment()) :
  maxfun < 10 * length(par)^2 is not recommended.
2: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,  :
  failure to converge in 10000 evaluations
> formul1 <- missing ~ Item | ID
> fit <- glmer(formul1, family = binomial, data = longDat)
boundary (singular) fit: see ?isSingular
Warning messages:
1: In commonArgs(par, fn, control, environment()) :
  maxfun < 10 * length(par)^2 is not recommended.
2: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,  :
  failure to converge in 10000 evaluations
> head(longDat)
# A tibble: 6 x 4
# Groups:   ID [1]
  ID       Item     Score missing
  <chr>    <chr>    <dbl>   <dbl>
1 ALCA0001 YPCORE01     1       0
2 ALCA0001 YPCORE02     0       0
3 ALCA0001 YPCORE03     0       0
4 ALCA0001 YPCORE04     0       0
5 ALCA0001 YPCORE05     0       0
6 ALCA0001 YPCORE06     1       0
> ?glmer
> ?mcnemar.test
> table(is.na(dat$YPCORE01), is.na(dat$YPCORE02))
       
        FALSE TRUE
  FALSE  3573    4
  TRUE      1 9499
> mcnemar.test(table(is.na(dat$YPCORE01), is.na(dat$YPCORE02)))

	McNemar's Chi-squared test with continuity correction

data:  table(is.na(dat$YPCORE01), is.na(dat$YPCORE02))
McNemar's chi-squared = 0.8, df = 1, p-value = 0.3711

> dat %>%
+   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
+    filter(Cuestionarios == "YP-CORE" &
+            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
+   group_by(ID) %>%
+   mutate(occasion = row_number(),
+          first = if_else(occasion == 1, 1, 0)) %>% # to get first ocassion only
+   filter(first == 1) %>%
+   select(ID, starts_with("YP")) %>%
+   pivot_longer(cols = c("YPCORE01", "YPCORE02")),
Error: unexpected ',' in:
"  select(ID, starts_with("YP")) %>%
  pivot_longer(cols = c("YPCORE01", "YPCORE02")),"
>                names_to = "Item",
Error: unexpected ',' in "               names_to = "Item","
>                values_to = "Score") %>% 
Error: unexpected ')' in "               values_to = "Score")"
>   mutate(missing = if_else(!is.na(Score), 0, 1)) -> longDat
Error in if_else(!is.na(Score), 0, 1) : object 'Score' not found
> head(longDat)
# A tibble: 6 x 4
# Groups:   ID [1]
  ID       Item     Score missing
  <chr>    <chr>    <dbl>   <dbl>
1 ALCA0001 YPCORE01     1       0
2 ALCA0001 YPCORE02     0       0
3 ALCA0001 YPCORE03     0       0
4 ALCA0001 YPCORE04     0       0
5 ALCA0001 YPCORE05     0       0
6 ALCA0001 YPCORE06     1       0
> dat %>%
+   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
+    filter(Cuestionarios == "YP-CORE" &
+            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
+   group_by(ID) %>%
+   mutate(occasion = row_number(),
+          first = if_else(occasion == 1, 1, 0)) %>% # to get first ocassion only
+   filter(first == 1) %>%
+   select(ID, starts_with("YP")) %>%
+   pivot_longer(cols = c("YPCORE01", "YPCORE02"),
+                names_to = "Item",
+                values_to = "Score") %>% 
+   mutate(missing = if_else(!is.na(Score), 0, 1)) -> longDat
> head(longDat)
# A tibble: 6 x 12
# Groups:   ID [3]
  ID       YPCORE03 YPCORE04 YPCORE05 YPCORE06 YPCORE07 YPCORE08 YPCORE09 YPCORE10 Item     Score missing
  <chr>       <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl> <chr>    <dbl>   <dbl>
1 ALCA0001        0        0        0        1        0        0        0        0 YPCORE01     1       0
2 ALCA0001        0        0        0        1        0        0        0        0 YPCORE02     0       0
3 ALCA0004        0        0        0        1        0        0        0        0 YPCORE01     1       0
4 ALCA0004        0        0        0        1        0        0        0        0 YPCORE02     0       0
5 ALCA0007        1        0        0        1        0        2        1        0 YPCORE01     2       0
6 ALCA0007        1        0        0        1        0        2        1        0 YPCORE02     1       0
> dat %>%
+   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
+    filter(Cuestionarios == "YP-CORE" &
+            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
+   group_by(ID) %>%
+   mutate(occasion = row_number(),
+          first = if_else(occasion == 1, 1, 0)) %>% # to get first ocassion only
+   filter(first == 1) %>%
+   select(ID, c("YPCORE01", "YPCORE02")) %>%
+   pivot_longer(cols = c("YPCORE01", "YPCORE02"),
+                names_to = "Item",
+                values_to = "Score") %>% 
+   mutate(missing = if_else(!is.na(Score), 0, 1)) -> longDat
> head(longDat)
# A tibble: 6 x 4
# Groups:   ID [3]
  ID       Item     Score missing
  <chr>    <chr>    <dbl>   <dbl>
1 ALCA0001 YPCORE01     1       0
2 ALCA0001 YPCORE02     0       0
3 ALCA0004 YPCORE01     1       0
4 ALCA0004 YPCORE02     0       0
5 ALCA0007 YPCORE01     2       0
6 ALCA0007 YPCORE02     1       0
> fit <- glmer(formul1, family = binomial, data = longDat)
boundary (singular) fit: see ?isSingular
> ?isSingular
> head(longDat)
# A tibble: 6 x 4
# Groups:   ID [3]
  ID       Item     Score missing
  <chr>    <chr>    <dbl>   <dbl>
1 ALCA0001 YPCORE01     1       0
2 ALCA0001 YPCORE02     0       0
3 ALCA0004 YPCORE01     1       0
4 ALCA0004 YPCORE02     0       0
5 ALCA0007 YPCORE01     2       0
6 ALCA0007 YPCORE02     1       0
> table(longDat$missing)

  0   1 
472   2 
> library(lme4)
> formul1 <- missing ~ (Item | ID)
> fit <- glmer(formul1, family = binomial, data = longDat)
boundary (singular) fit: see ?isSingular
> longDat$missRand <- rbinom(1, length(longDat), .5)
> head(longDat)
# A tibble: 6 x 5
# Groups:   ID [3]
  ID       Item     Score missing missRand
  <chr>    <chr>    <dbl>   <dbl>    <int>
1 ALCA0001 YPCORE01     1       0        2
2 ALCA0001 YPCORE02     0       0        2
3 ALCA0004 YPCORE01     1       0        2
4 ALCA0004 YPCORE02     0       0        2
5 ALCA0007 YPCORE01     2       0        2
6 ALCA0007 YPCORE02     1       0        2
> rbinom(1,1,.5)
[1] 0
> rbinom(1,1,.5)
[1] 0
> rbinom(1,1,.5)
[1] 0
> rbinom(1,1,.5)
[1] 0
> rbinom(1,1,.5)
[1] 0
> rbinom(1,1,.5)
[1] 0
> rbinom(1,1,.5)
[1] 0
> rbinom(1,1,.5)
[1] 0
> ?rbinom
> rbinom(1,1,.05)
[1] 0
> rbinom(1,1,.05)
[1] 0
> rbinom(1,1,.95)
[1] 1
> rbinom(1,1,.5)
[1] 0
> rbinom(1,1,.95)
[1] 1
> rbinom(1,1,.5)
[1] 1
> rbinom(1,1,.5)
[1] 0
> rbinom(1,1,.5)
[1] 0
> rbinom(1,1,.5)
[1] 1
> rbinom(1,1,.5)
[1] 0
> rbinom(1,1,.5)
[1] 1
> rbinom(1,2,.5)
[1] 2
> rbinom(2,1,.5)
[1] 0 0
> rbinom(2,1,.5)
[1] 0 0
> rbinom(2,1,.5)
[1] 0 0
> rbinom(2,1,.5)
[1] 0 1
> rbinom(2,1,.5)
[1] 1 1
> rbinom(2,1,.5)
[1] 0 1
> longDat$missRand <- rbinom(length(longDat), 1, .5)
Error: Assigned data `rbinom(length(longDat), 1, 0.5)` must be compatible with existing data.
x Existing data has 474 rows.
x Assigned data has 5 rows.
? Only vectors of size 1 are recycled.
Run `rlang::last_error()` to see where the error occurred.
> table(longDat$missing)

  0   1 
472   2 
> library(lme4)
> head(longDat)
# A tibble: 6 x 5
# Groups:   ID [3]
  ID       Item     Score missing missRand
  <chr>    <chr>    <dbl>   <dbl>    <int>
1 ALCA0001 YPCORE01     1       0        2
2 ALCA0001 YPCORE02     0       0        2
3 ALCA0004 YPCORE01     1       0        2
4 ALCA0004 YPCORE02     0       0        2
5 ALCA0007 YPCORE01     2       0        2
6 ALCA0007 YPCORE02     1       0        2
> rbinom(2,1,.5)
[1] 1 0
> rbinom(2,1,.5)
[1] 1 1
> rbinom(2,1,.5)
[1] 1 1
> longDat$missRand <- NULL
> longDat$missRand <- rbinom(length(longDat), 1, .5)
Error: Assigned data `rbinom(length(longDat), 1, 0.5)` must be compatible with existing data.
x Existing data has 474 rows.
x Assigned data has 4 rows.
? Only vectors of size 1 are recycled.
Run `rlang::last_error()` to see where the error occurred.
> longDat$missRand <- rbinom(nrow(longDat), 1, .5)
> table(longDat$missRand)

  0   1 
240 234 
> head(longDat)
# A tibble: 6 x 5
# Groups:   ID [3]
  ID       Item     Score missing missRand
  <chr>    <chr>    <dbl>   <dbl>    <int>
1 ALCA0001 YPCORE01     1       0        0
2 ALCA0001 YPCORE02     0       0        1
3 ALCA0004 YPCORE01     1       0        0
4 ALCA0004 YPCORE02     0       0        1
5 ALCA0007 YPCORE01     2       0        0
6 ALCA0007 YPCORE02     1       0        0
> formul1 <- missRand ~ (Item | ID)
> fit <- glmer(formul1, family = binomial, data = longDat)
boundary (singular) fit: see ?isSingular
> fit <- glmer(missRand ~ Item, family = binomial, data = longDat)
Error: No random effects terms specified in formula
> fit <- glmer(missRand ~ Item | ID, family = binomial, data = longDat)
boundary (singular) fit: see ?isSingular
> fit <- glmer(factor(missRand) ~ Item | ID, family = binomial, data = longDat)
boundary (singular) fit: see ?isSingular
> fit <- glmer(missRand ~ Item | ID, family = "logistic", data = longDat)
Error in get(family, mode = "function", envir = parent.frame(2)) : 
  object 'logistic' of mode 'function' was not found
> fit <- glmer(missRand ~ Item + (1 | ID), family = "binomial", data = longDat)
boundary (singular) fit: see ?isSingular
> ?glmer
> longDat
# A tibble: 474 x 5
# Groups:   ID [237]
   ID       Item     Score missing missRand
   <chr>    <chr>    <dbl>   <dbl>    <int>
 1 ALCA0001 YPCORE01     1       0        0
 2 ALCA0001 YPCORE02     0       0        1
 3 ALCA0004 YPCORE01     1       0        0
 4 ALCA0004 YPCORE02     0       0        1
 5 ALCA0007 YPCORE01     2       0        0
 6 ALCA0007 YPCORE02     1       0        0
 7 ALCA0012 YPCORE01     3       0        1
 8 ALCA0012 YPCORE02     1       0        1
 9 ALCA0013 YPCORE01     4       0        1
10 ALCA0013 YPCORE02     3       0        1
# ? with 464 more rows
> table(longDat$missRand, longDat$Item)
   
    YPCORE01 YPCORE02
  0      125      115
  1      112      122
> 

> fit <- glmer(missRand ~ factor(Item) + (1 | ID), family = "binomial", data = longDat)
boundary (singular) fit: see ?isSingular
> fit <- glmer(missRand ~ factor(Item) + (1 | factor(ID)), family = "binomial", data = longDat)
Error: couldn't evaluate grouping factor factor(ID) within model frame: try adding grouping factor to data frame explicitly if possible
> fit <- glmer(missRand ~ factor(Item) + (1 | ID), family = "binomial", data = longDat)
boundary (singular) fit: see ?isSingular
> rm(fit)
> fit <- glmer(missRand ~ factor(Item) + (1 | ID), family = "binomial", data = longDat)
boundary (singular) fit: see ?isSingular
> fit
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: missRand ~ factor(Item) + (1 | ID)
   Data: longDat
      AIC       BIC    logLik  deviance  df.resid 
 662.1833  674.6669 -328.0917  656.1833       471 
Random effects:
 Groups Name        Std.Dev. 
 ID     (Intercept) 1.943e-07
Number of obs: 474, groups:  ID, 237
Fixed Effects:
         (Intercept)  factor(Item)YPCORE02  
             -0.1098                0.1689  
convergence code 0; 0 optimizer warnings; 1 lme4 warnings 
> dat %>%
+   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
+    filter(Cuestionarios == "YP-CORE" &
+            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
+   group_by(ID) %>%
+   mutate(occasion = row_number(),
+          first = if_else(occasion == 1, 1, 0)) %>% # to get first ocassion only
+   filter(first == 1) %>%
+   select(ID, starts_with("YPCORE")) %>%
+   pivot_longer(cols = starts_with("YPCORE"),
+                names_to = "Item",
+                values_to = "Score") %>% 
+   mutate(missing = if_else(!is.na(Score), 0, 1)) -> longDat
> head(longDat)
# A tibble: 6 x 4
# Groups:   ID [1]
  ID       Item     Score missing
  <chr>    <chr>    <dbl>   <dbl>
1 ALCA0001 YPCORE01     1       0
2 ALCA0001 YPCORE02     0       0
3 ALCA0001 YPCORE03     0       0
4 ALCA0001 YPCORE04     0       0
5 ALCA0001 YPCORE05     0       0
6 ALCA0001 YPCORE06     1       0
> formul1 <- missing ~ (Item | ID)
> fit <- glmer(missing ~ factor(Item) + (1 | ID), family = "binomial", data = longDat)
Warning message:
In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.78883 (tol = 0.002, component 1)
> fit <- glmer((score == 1) ~ factor(Item) + (1 | ID), family = "binomial", data = longDat)
Error in eval(predvars, data, env) : object 'score' not found
> fit <- glmer((Score == 1) ~ factor(Item) + (1 | ID), family = "binomial", data = longDat)
Warning message:
In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.00309555 (tol = 0.002, component 1)
> fit <- glmer((Score == 1) ~ factor(Item) + (factor(Item) | ID), family = "binomial", data = longDat)
Error: number of observations (=2356) < number of random effects (=2360) for term (factor(Item) | ID); the random-effects parameters are probably unidentifiable
> fit <- glmer((Score == 1) ~ factor(Item) + (Item | ID), family = "binomial", data = longDat)
Error: number of observations (=2356) < number of random effects (=2360) for term (Item | ID); the random-effects parameters are probably unidentifiable
> fit <- glmer((Score == 1) ~ factor(Item) + (1 | ID), family = "binomial", data = longDat)
Warning message:
In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.00309555 (tol = 0.002, component 1)
> head(longDat)
# A tibble: 6 x 4
# Groups:   ID [1]
  ID       Item     Score missing
  <chr>    <chr>    <dbl>   <dbl>
1 ALCA0001 YPCORE01     1       0
2 ALCA0001 YPCORE02     0       0
3 ALCA0001 YPCORE03     0       0
4 ALCA0001 YPCORE04     0       0
5 ALCA0001 YPCORE05     0       0
6 ALCA0001 YPCORE06     1       0
> longDat$missRand <- rbinom(nrow(longDat), 1, .5)
> table(longDat$missRand)

   0    1 
1211 1159 
> fit <- glmer(missRand ~ Item + (1 | ID), family = "binomial", data = longDat)
boundary (singular) fit: see ?isSingular
> fit
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: missRand ~ Item + (1 | ID)
   Data: longDat
      AIC       BIC    logLik  deviance  df.resid 
 3300.819  3364.296 -1639.410  3278.819      2359 
Random effects:
 Groups Name        Std.Dev. 
 ID     (Intercept) 2.663e-07
Number of obs: 2370, groups:  ID, 237
Fixed Effects:
 (Intercept)  ItemYPCORE02  ItemYPCORE03  ItemYPCORE04  ItemYPCORE05  ItemYPCORE06  ItemYPCORE07  ItemYPCORE08  
  -5.909e-02     1.013e-01    -1.690e-02     1.182e-01     6.753e-02    -1.186e-01    -1.046e-15    -1.186e-01  
ItemYPCORE09  ItemYPCORE10  
   1.858e-01    -6.766e-02  
convergence code 0; 0 optimizer warnings; 1 lme4 warnings 
> anova(fit)
Analysis of Variance Table
     npar Sum Sq Mean Sq F value
Item    9 5.5453 0.61615  0.6161
> table(longDat$Item, as.numeric(longDat$Item))
< table of extent 10 x 0 >
Warning message:
In table(longDat$Item, as.numeric(longDat$Item)) :
  NAs introduced by coercion
> dim(longDat)
[1] 2370    5
> head(longDat)
# A tibble: 6 x 5
# Groups:   ID [1]
  ID       Item     Score missing missRand
  <chr>    <chr>    <dbl>   <dbl>    <int>
1 ALCA0001 YPCORE01     1       0        1
2 ALCA0001 YPCORE02     0       0        1
3 ALCA0001 YPCORE03     0       0        1
4 ALCA0001 YPCORE04     0       0        1
5 ALCA0001 YPCORE05     0       0        1
6 ALCA0001 YPCORE06     1       0        0
> table(longDat$Item) #, as.numeric(longDat$Item))

YPCORE01 YPCORE02 YPCORE03 YPCORE04 YPCORE05 YPCORE06 YPCORE07 YPCORE08 YPCORE09 YPCORE10 
     237      237      237      237      237      237      237      237      237      237 
> table(as.numeric(longDat$Item)) #, as.numeric(longDat$Item))
< table of extent 0 >
Warning message:
In table(as.numeric(longDat$Item)) : NAs introduced by coercion
> table(longDat$Item, as.numeric(factor(longDat$Item)))
          
             1   2   3   4   5   6   7   8   9  10
  YPCORE01 237   0   0   0   0   0   0   0   0   0
  YPCORE02   0 237   0   0   0   0   0   0   0   0
  YPCORE03   0   0 237   0   0   0   0   0   0   0
  YPCORE04   0   0   0 237   0   0   0   0   0   0
  YPCORE05   0   0   0   0 237   0   0   0   0   0
  YPCORE06   0   0   0   0   0 237   0   0   0   0
  YPCORE07   0   0   0   0   0   0 237   0   0   0
  YPCORE08   0   0   0   0   0   0   0 237   0   0
  YPCORE09   0   0   0   0   0   0   0   0 237   0
  YPCORE10   0   0   0   0   0   0   0   0   0 237
> longDat %>%
+   mutate(missRand = rbinom(1, 1, .5))
# A tibble: 2,370 x 5
# Groups:   ID [237]
   ID       Item     Score missing missRand
   <chr>    <chr>    <dbl>   <dbl>    <int>
 1 ALCA0001 YPCORE01     1       0        1
 2 ALCA0001 YPCORE02     0       0        1
 3 ALCA0001 YPCORE03     0       0        1
 4 ALCA0001 YPCORE04     0       0        1
 5 ALCA0001 YPCORE05     0       0        1
 6 ALCA0001 YPCORE06     1       0        1
 7 ALCA0001 YPCORE07     0       0        1
 8 ALCA0001 YPCORE08     0       0        1
 9 ALCA0001 YPCORE09     0       0        1
10 ALCA0001 YPCORE10     0       0        1
# ? with 2,360 more rows
> table(longDat$missRand)

   0    1 
1211 1159 
> longDat %>%
+   mutate(missRand = rbinom(1, 1, .5),
+          missReg = rbinom(1, 1, as.numeric(factor(Item))/11))
# A tibble: 2,370 x 6
# Groups:   ID [237]
   ID       Item     Score missing missRand missReg
   <chr>    <chr>    <dbl>   <dbl>    <int>   <int>
 1 ALCA0001 YPCORE01     1       0        1       0
 2 ALCA0001 YPCORE02     0       0        1       0
 3 ALCA0001 YPCORE03     0       0        1       0
 4 ALCA0001 YPCORE04     0       0        1       0
 5 ALCA0001 YPCORE05     0       0        1       0
 6 ALCA0001 YPCORE06     1       0        1       0
 7 ALCA0001 YPCORE07     0       0        1       0
 8 ALCA0001 YPCORE08     0       0        1       0
 9 ALCA0001 YPCORE09     0       0        1       0
10 ALCA0001 YPCORE10     0       0        1       0
# ? with 2,360 more rows
> table(longDat$missReg)
< table of extent 0 >
Warning message:
Unknown or uninitialised column: `missReg`. 
> longDat %>%
+   mutate(missRand = rbinom(1, 1, .5),
+          missReg = rbinom(1, 1, as.numeric(factor(Item))/11)) -> longDat
> table(longDat$missReg)

   0    1 
2120  250 
> fit <- glmer(missReg ~ Item + (1 | ID), family = "binomial", data = longDat)
Warning message:
In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 3.81902 (tol = 0.002, component 1)
> longDat %>%
+   mutate(missRand = rbinom(1, 1, .5),
+          missReg = rbinom(1, 1, as.numeric(factor(Item))/11)) -> longDat
> head(longDat)
# A tibble: 6 x 6
# Groups:   ID [1]
  ID       Item     Score missing missRand missReg
  <chr>    <chr>    <dbl>   <dbl>    <int>   <int>
1 ALCA0001 YPCORE01     1       0        1       1
2 ALCA0001 YPCORE02     0       0        1       1
3 ALCA0001 YPCORE03     0       0        1       1
4 ALCA0001 YPCORE04     0       0        1       1
5 ALCA0001 YPCORE05     0       0        1       1
6 ALCA0001 YPCORE06     1       0        1       1
> table(longDat$missRand)

   0    1 
1140 1230 
> table(longDat$missReg)

   0    1 
2160  210 
> library(lme4)
> fitRand <- glmer(missRand ~ Item + (1 | ID), family = "binomial", data = longDat)
Warning message:
In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 5.46528 (tol = 0.002, component 1)
> anova(fitRand)
Analysis of Variance Table
     npar   Sum Sq   Mean Sq F value
Item    9 0.050372 0.0055969  0.0056
> length(unique(longDat$ID))
[1] 237
> rm(fitRand)
> anova(fitRand)
Error in anova(fitRand) : object 'fitRand' not found
> fitRand <- glmer(missRand ~ Item + (1 | ID), family = "binomial", data = longDat)
Warning message:
In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 5.46528 (tol = 0.002, component 1)
> fitRand <- glmer(missRand ~ Item + (1 | ID), family = binomial, data = longDat)
Warning message:
In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 5.46528 (tol = 0.002, component 1)
> fitRand <- glmer(missRand ~ Item + (1 | ID), family = binomial(link = "logit"), data = longDat)
Warning message:
In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 5.46528 (tol = 0.002, component 1)
> ?glmer
> data("cbpp")
> dim(cbpp)
[1] 56  4
> head(cbpp)
  herd incidence size period
1    1         2   14      1
2    1         3   12      2
3    1         4    9      3
4    1         0    5      4
5    2         3   22      1
6    2         1   18      2
> fitRand <- glmer(cbind(missRand, 1) ~ Item + (1 | ID), family = binomial(link = "logit"), data = longDat)
Warning message:
In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.00550177 (tol = 0.002, component 1)
> fitReg <- glmer(cbind(missReg, 1) ~ Item + (1 | ID), family = "binomial", data = longDat)
Warning messages:
1: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,  :
  failure to converge in 10000 evaluations
2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.47499 (tol = 0.002, component 1)
> fitReal <- glmer(cbind(missing, 1) ~ Item + (1 | ID), family = "binomial", data = longDat)
Warning message:
In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.29013 (tol = 0.002, component 1)
> data(mlmdata)
Warning message:
In data(mlmdata) : data set ?mlmdata? not found
> library(haven)
> mlmdata <- read_dta("https://stats.idre.ucla.edu/stat/examples/imm/imm10.dta")
> head(mlmdata)
# A tibble: 6 x 19
  schid stuid    ses meanses homework white parented public ratio percmin  math   sex  race sctype  cstr scsize urban
  <dbl> <dbl>  <dbl>   <dbl>    <dbl> <dbl>    <dbl>  <dbl> <dbl>   <dbl> <dbl> <dbl> <dbl>  <dbl> <dbl>  <dbl> <dbl>
1  7472     3 -0.130  -0.483        1     1        2      1    19       0    48     2     4      1     2      3     2
2  7472     8 -0.390  -0.483        0     1        2      1    19       0    48     1     4      1     2      3     2
3  7472    13 -0.800  -0.483        0     1        2      1    19       0    53     1     4      1     2      3     2
4  7472    17 -0.720  -0.483        1     1        2      1    19       0    42     1     4      1     2      3     2
5  7472    27 -0.740  -0.483        2     1        2      1    19       0    43     2     4      1     2      3     2
6  7472    28 -0.580  -0.483        1     1        2      1    19       0    57     2     4      1     2      3     2
# ? with 2 more variables: region <dbl>, schnum <dbl>
> model <- glmer(white ~ homework + (1 + homework | schid), data=mlmdata,
+                family=binomial(link="logit"))
boundary (singular) fit: see ?isSingular
> summary(model)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: white ~ homework + (1 + homework | schid)
   Data: mlmdata

     AIC      BIC   logLik deviance df.resid 
   182.4    200.2    -86.2    172.4      255 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.3373 -0.1184  0.1112  0.3421  3.8801 

Random effects:
 Groups Name        Variance Std.Dev. Corr 
 schid  (Intercept) 16.28745 4.0358        
        homework     0.04678 0.2163   -1.00
Number of obs: 260, groups:  schid, 10

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)  1.67365    1.47324   1.136    0.256
homework     0.04508    0.18433   0.245    0.807

Correlation of Fixed Effects:
         (Intr)
homework -0.590
convergence code: 0
boundary (singular) fit: see ?isSingular

> dim(mlmdata)
[1] 260  19
> head(mlmdata)
# A tibble: 6 x 19
  schid stuid    ses meanses homework white parented public ratio percmin  math   sex  race sctype  cstr scsize urban
  <dbl> <dbl>  <dbl>   <dbl>    <dbl> <dbl>    <dbl>  <dbl> <dbl>   <dbl> <dbl> <dbl> <dbl>  <dbl> <dbl>  <dbl> <dbl>
1  7472     3 -0.130  -0.483        1     1        2      1    19       0    48     2     4      1     2      3     2
2  7472     8 -0.390  -0.483        0     1        2      1    19       0    48     1     4      1     2      3     2
3  7472    13 -0.800  -0.483        0     1        2      1    19       0    53     1     4      1     2      3     2
4  7472    17 -0.720  -0.483        1     1        2      1    19       0    42     1     4      1     2      3     2
5  7472    27 -0.740  -0.483        2     1        2      1    19       0    43     2     4      1     2      3     2
6  7472    28 -0.580  -0.483        1     1        2      1    19       0    57     2     4      1     2      3     2
# ? with 2 more variables: region <dbl>, schnum <dbl>
> length(unique(mlmdata$schid))
[1] 10
> table(white)
Error in table(white) : object 'white' not found
> table(mlmdata$white)

  0   1 
 71 189 
> fitRand <- glmer(missRand ~ Item + (1 | ID), family = binomial(link = "logit"), data = longDat)
Warning message:
In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 5.46528 (tol = 0.002, component 1)
> ,
Error: unexpected ',' in ","
> longDat %>%
+   mutate(missRand = rbinom(1, 1, .5),
+          missReg = rbinom(1, 1, as.numeric(factor(Item))/11),
+          riskItem = if_else(Item == "YPCORE04", 1, 0),
+          posItem = if_else(Item %in% c("YPCORE03", "YPCORE5", "YPCORE10"), 1, 0)) -> longDat
> head(longDat)
# A tibble: 6 x 8
# Groups:   ID [1]
  ID       Item     Score missing missRand missReg riskItem posItem
  <chr>    <chr>    <dbl>   <dbl>    <int>   <int>    <dbl>   <dbl>
1 ALCA0001 YPCORE01     1       0        0       0        0       0
2 ALCA0001 YPCORE02     0       0        0       0        0       0
3 ALCA0001 YPCORE03     0       0        0       0        0       1
4 ALCA0001 YPCORE04     0       0        0       0        1       0
5 ALCA0001 YPCORE05     0       0        0       0        0       0
6 ALCA0001 YPCORE06     1       0        0       0        0       0
> table(longDat$riskItem, longDat$posItem)
   
       0    1
  0 1659  474
  1  237    0
> table(longDat$riskItem, longDat$Item)
   
    YPCORE01 YPCORE02 YPCORE03 YPCORE04 YPCORE05 YPCORE06 YPCORE07 YPCORE08 YPCORE09 YPCORE10
  0      237      237      237        0      237      237      237      237      237      237
  1        0        0        0      237        0        0        0        0        0        0
> table(longDat$posItem, longDat$Item)
   
    YPCORE01 YPCORE02 YPCORE03 YPCORE04 YPCORE05 YPCORE06 YPCORE07 YPCORE08 YPCORE09 YPCORE10
  0      237      237        0      237      237      237      237      237      237        0
  1        0        0      237        0        0        0        0        0        0      237
> longDat %>%
+   mutate(missRand = rbinom(1, 1, .5),
+          missReg = rbinom(1, 1, as.numeric(factor(Item))/11),
+          riskItem = if_else(Item == "YPCORE04", 1, 0),
+          posItem = if_else(Item %in% c("YPCORE03", "YPCORE05", "YPCORE10"), 1, 0)) -> longDat
> head(longDat)
# A tibble: 6 x 8
# Groups:   ID [1]
  ID       Item     Score missing missRand missReg riskItem posItem
  <chr>    <chr>    <dbl>   <dbl>    <int>   <int>    <dbl>   <dbl>
1 ALCA0001 YPCORE01     1       0        0       0        0       0
2 ALCA0001 YPCORE02     0       0        0       0        0       0
3 ALCA0001 YPCORE03     0       0        0       0        0       1
4 ALCA0001 YPCORE04     0       0        0       0        1       0
5 ALCA0001 YPCORE05     0       0        0       0        0       1
6 ALCA0001 YPCORE06     1       0        0       0        0       0
> table(longDat$posItem, longDat$Item)
   
    YPCORE01 YPCORE02 YPCORE03 YPCORE04 YPCORE05 YPCORE06 YPCORE07 YPCORE08 YPCORE09 YPCORE10
  0      237      237        0      237        0      237      237      237      237        0
  1        0        0      237        0      237        0        0        0        0      237
> fitRisk <- glmer(missing ~ riskItem + (1 | ID), family = binomial, data = longDat)
> summary(fitRisk)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: missing ~ riskItem + (1 | ID)
   Data: longDat

     AIC      BIC   logLik deviance df.resid 
    51.7     69.0    -22.9     45.7     2367 

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-0.52020 -0.00113 -0.00113 -0.00113  2.93829 

Random effects:
 Groups Name        Variance Std.Dev.
 ID     (Intercept) 205.3    14.33   
Number of obs: 2370, groups:  ID, 237

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -13.563      2.540  -5.339 9.33e-08 ***
riskItem      -2.418      3.745  -0.646    0.518    
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Correlation of Fixed Effects:
         (Intr)
riskItem 0.064 
> fitRisk <- glmer(missing ~ posItem + (1 | ID), family = binomial, data = longDat)
> fitRisk <- glmer(missing ~ riskItem + (1 | ID), family = binomial, data = longDat)
> summary(fitRisk)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: missing ~ riskItem + (1 | ID)
   Data: longDat

     AIC      BIC   logLik deviance df.resid 
    51.7     69.0    -22.9     45.7     2367 

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-0.52020 -0.00113 -0.00113 -0.00113  2.93829 

Random effects:
 Groups Name        Variance Std.Dev.
 ID     (Intercept) 205.3    14.33   
Number of obs: 2370, groups:  ID, 237

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -13.563      2.540  -5.339 9.33e-08 ***
riskItem      -2.418      3.745  -0.646    0.518    
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Correlation of Fixed Effects:
         (Intr)
riskItem 0.064 
> fitPos <- glmer(missing ~ posItem + (1 | ID), family = binomial, data = longDat)
> summary(fitPos)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: missing ~ posItem + (1 | ID)
   Data: longDat

     AIC      BIC   logLik deviance df.resid 
    52.4     69.7    -23.2     46.4     2367 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-0.5111 -0.0012 -0.0012 -0.0010  3.4498 

Random effects:
 Groups Name        Variance Std.Dev.
 ID     (Intercept) 197.5    14.05   
Number of obs: 2370, groups:  ID, 237

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept) -13.5152     2.5280  -5.346 8.98e-08 ***
posItem      -0.2953     1.2396  -0.238    0.812    
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Correlation of Fixed Effects:
        (Intr)
posItem -0.110
> head(longDat,15)
# A tibble: 15 x 8
# Groups:   ID [2]
   ID       Item     Score missing missRand missReg riskItem posItem
   <chr>    <chr>    <dbl>   <dbl>    <int>   <int>    <dbl>   <dbl>
 1 ALCA0001 YPCORE01     1       0        0       0        0       0
 2 ALCA0001 YPCORE02     0       0        0       0        0       0
 3 ALCA0001 YPCORE03     0       0        0       0        0       1
 4 ALCA0001 YPCORE04     0       0        0       0        1       0
 5 ALCA0001 YPCORE05     0       0        0       0        0       1
 6 ALCA0001 YPCORE06     1       0        0       0        0       0
 7 ALCA0001 YPCORE07     0       0        0       0        0       0
 8 ALCA0001 YPCORE08     0       0        0       0        0       0
 9 ALCA0001 YPCORE09     0       0        0       0        0       0
10 ALCA0001 YPCORE10     0       0        0       0        0       1
11 ALCA0004 YPCORE01     1       0        0       0        0       0
12 ALCA0004 YPCORE02     0       0        0       0        0       0
13 ALCA0004 YPCORE03     0       0        0       0        0       1
14 ALCA0004 YPCORE04     0       0        0       0        1       0
15 ALCA0004 YPCORE05     0       0        0       0        0       1
> 
> 
> rm(fitRand)
> rm(fitReg)
> rm(fitReal)
> fitRand <- glmer(cbind(missRand, 1) ~ Item + (1 | ID), family = binomial(link = "logit"), data = longDat)
Warning message:
In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.0501701 (tol = 0.002, component 1)
> fitRand
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: cbind(missRand, 1) ~ Item + (1 | ID)
   Data: longDat
      AIC       BIC    logLik  deviance  df.resid 
 2215.017  2278.494 -1096.509  2193.017      2359 
Random effects:
 Groups Name        Std.Dev.
 ID     (Intercept) 2.509   
Number of obs: 2370, groups:  ID, 237
Fixed Effects:
 (Intercept)  ItemYPCORE02  ItemYPCORE03  ItemYPCORE04  ItemYPCORE05  ItemYPCORE06  ItemYPCORE07  ItemYPCORE08  
   -2.512456      0.004360      0.007166      0.001288      0.004078      0.002322      0.005059      0.006118  
ItemYPCORE09  ItemYPCORE10  
    0.006454      0.005057  
convergence code 0; 0 optimizer warnings; 1 lme4 warnings 

So failed to converge but does give a set of estimates.  I think I can use the 


> fitReg <- glmer(cbind(missReg, 1) ~ Item + (1 | ID), family = "binomial", data = longDat)
Warning messages:
1: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,  :
  failure to converge in 10000 evaluations
2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.66664 (tol = 0.002, component 1)
> fitReg
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: cbind(missReg, 1) ~ Item + (1 | ID)
   Data: longDat
      AIC       BIC    logLik  deviance  df.resid 
 605.3404  668.8175 -291.6702  583.3404      2359 
Random effects:
 Groups Name        Std.Dev.
 ID     (Intercept) 6.775   
Number of obs: 2370, groups:  ID, 237
Fixed Effects:
 (Intercept)  ItemYPCORE02  ItemYPCORE03  ItemYPCORE04  ItemYPCORE05  ItemYPCORE06  ItemYPCORE07  ItemYPCORE08  
     -8.6607       -0.3793       -0.3159       -0.3591       -0.4608       -0.4453       -0.4165       -0.6597  
ItemYPCORE09  ItemYPCORE10  
     -0.4304       -0.3535  
convergence code 0; 1 optimizer warnings; 1 lme4 warnings 

I am starting to realise the depths of my ignorance about all this.  It's clear to me that having only one observation per cell 
because of the nesting of items within participants and only having one completion per participant is causing the convergence 
issues (which makes sense though I have a sense that there might be ways to extract estimates despite this: this is beyond me!)

I'm puzzled that I get slightly different results for the risk analysis if I use the "cbind(missing, 1) ~ " syntax in the formula
from those I get using just "missing ~ " and different max|grad values from the nonconvergence message depending on that choice.
I suspect that's a red herring.

I have seen many comments here recently about non-convergence and ways to overcome it by specifying different methods of 
approximation (if I understood that properly) but they generally seemed to be for much more complex models than mine and 
probably weren't about this "cell size = 1" issue.  I have searched for answer or illumination for some hours and perhaps
I'm asking the wrong questions but I'm stuck.  (I think it's very unlikely that I have access to trained statisticians by 
virtue of my honorary or paid academic positions!)

So my questions: 
1) _is_ there a way to estimate such a model, i.e. estimating an effect across all ten items with only one completion per participant?
2) can someone suggest good reading matter for a dogged non-statistician who can handle a lot of continuous variable issues up to
some real complexity on his own but has never been as comfortable with counts beyond the basic old NHST, single level approaches?  I 
have an old copy of Pinheiro and Bates but confess that despite trying many times, the older it and I get, the less I'm able to cope 
with it.  The algebra is just beyond me.

TIA ... and best wishes to all for physical safety and psychological resilience in these times,

Chris

-- 
Small contribution in our coronavirus rigours: 
https://www.coresystemtrust.org.uk/home/free-options-to-replace-paper-core-forms-during-the-coronavirus-pandemic/

Chris Evans <chris at psyctc.org> Visiting Professor, University of Sheffield <chris.evans at sheffield.ac.uk>
I do some consultation work for the University of Roehampton <chris.evans at roehampton.ac.uk> and other places
but <chris at psyctc.org> remains my main Email address.  I have a work web site at:
   https://www.psyctc.org/psyctc/
and a site I manage for CORE and CORE system trust at:
   http://www.coresystemtrust.org.uk/
I have "semigrated" to France, see: 
   https://www.psyctc.org/pelerinage2016/semigrating-to-france/ 
   https://www.psyctc.org/pelerinage2016/register-to-get-updates-from-pelerinage2016/

If you want an Emeeting, I am trying to keep them to Thursdays and my diary is at:
   https://www.psyctc.org/pelerinage2016/ceworkdiary/
Beware: French time, generally an hour ahead of UK.


From p@mochungo @end|ng |rom gm@||@com  Fri May  1 21:00:06 2020
From: p@mochungo @end|ng |rom gm@||@com (Pamela Ochungo)
Date: Fri, 1 May 2020 22:00:06 +0300
Subject: [R-sig-ME] Error in initializePtr() : updateMu: Size mismatch
Message-ID: <CAD=tpYDy3uke2rhQuUbAQ0EDYU9uS+Q4pG6X0f+EegR8CPXc=g@mail.gmail.com>

Hallo,

I want to run a linear mixed model featuring 5 response variables and only
one predictor variable. I also have two random effects in the model. I am
using this code:

lm1 <- lmer(cbind(transpopbees, transbrood, transhoney, transpollen,
transeggs) ~ Varroa+(1|Site)+(1|Colony),data=pollencolony)

However I get this error message:

Error in initializePtr() : updateMu: Size mismatch

What does this mean and what am I doing wrong?

Thanks

	[[alternative HTML version deleted]]


From c@c@voeten @end|ng |rom hum@|e|denun|v@n|  Fri May  1 21:25:14 2020
From: c@c@voeten @end|ng |rom hum@|e|denun|v@n| (Voeten, C.C.)
Date: Fri, 1 May 2020 19:25:14 +0000
Subject: [R-sig-ME] Error in initializePtr() : updateMu: Size mismatch
In-Reply-To: <CAD=tpYDy3uke2rhQuUbAQ0EDYU9uS+Q4pG6X0f+EegR8CPXc=g@mail.gmail.com>
References: <CAD=tpYDy3uke2rhQuUbAQ0EDYU9uS+Q4pG6X0f+EegR8CPXc=g@mail.gmail.com>
Message-ID: <06a1ebd747844ca3b51ef15afc6d07c1@hum.leidenuniv.nl>

Hi Pamela,

lmer/glmer do not support models with multiple dependent variables via the cbind() syntax. An alternative approach is to convert your data to long format and run the model in the following way:

value ~ 0+variable/Varroa + (0+variable|Site) + (0+variable|Colony)

with 'variable' the column containing "transpopbees", "transbrood", ..., and 'value' the column containing their values.
Alternatively, function gam() from package mgcv can fit your model. You would then use something like:

gam(list(transpopbees ~ Varroa + s(Site,bs='re') + s(Colony,bs='re'), transbrood ~ Varroa + s(Site,bs='re') + s(Colony,bs='re'), .....),family=mvn(5),data=pollencolony)

Hope this helps,
Cesko

-----Original Message-----
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of Pamela Ochungo
Sent: Friday, May 1, 2020 9:00 PM
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Error in initializePtr() : updateMu: Size mismatch

Hallo,

I want to run a linear mixed model featuring 5 response variables and only one predictor variable. I also have two random effects in the model. I am using this code:

lm1 <- lmer(cbind(transpopbees, transbrood, transhoney, transpollen,
transeggs) ~ Varroa+(1|Site)+(1|Colony),data=pollencolony)

However I get this error message:

Error in initializePtr() : updateMu: Size mismatch

What does this mean and what am I doing wrong?

Thanks

	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From p@mochungo @end|ng |rom gm@||@com  Fri May  1 22:15:15 2020
From: p@mochungo @end|ng |rom gm@||@com (Pamela Ochungo)
Date: Fri, 1 May 2020 23:15:15 +0300
Subject: [R-sig-ME] Error in initializePtr() : updateMu: Size mismatch
In-Reply-To: <06a1ebd747844ca3b51ef15afc6d07c1@hum.leidenuniv.nl>
References: <CAD=tpYDy3uke2rhQuUbAQ0EDYU9uS+Q4pG6X0f+EegR8CPXc=g@mail.gmail.com>
 <06a1ebd747844ca3b51ef15afc6d07c1@hum.leidenuniv.nl>
Message-ID: <CAD=tpYDWULfP4HdsMXGmB-7bibfsrMnkz3AGpWguVF9atBsUUA@mail.gmail.com>

Hi Cesko,

Thanks for your reply! I have tried the second option, function gam() from
mgcv. However I get an unexpected result as below:

Family: Multivariate normal
Link function:

Formula:
transpopbees ~ Varroa + s(Site, bs = "re") + s(Colony,
    bs = "re")
transbrood ~ Varroa + s(Site, bs = "re") + s(Colony, bs = "re")
transhoney ~ Varroa + s(Site, bs = "re") + s(Colony, bs = "re")
transpollen ~ Varroa + s(Site, bs = "re") + s(Colony, bs = "re")
transeggs ~ Varroa + s(Site, bs = "re") + s(Colony, bs = "re")

Estimated degrees of freedom:
0.0001 0.7631 0.0000 0.0003 1.7061 0.0003 1.7192
0.0002 0.0000 0.0003  total = 29.19

REML score: 547.659

How do I interpret this? I was rather hoping to get a result showing model
coefficients and p-values for each dependent variable.


Question: Is it acceptable to carry out LMM (lmer) for each of the 5
dependent variables separately against Varroa?

Cheers

Pamela

On Fri, May 1, 2020 at 10:25 PM Voeten, C.C. <c.c.voeten at hum.leidenuniv.nl>
wrote:

> Hi Pamela,
>
> lmer/glmer do not support models with multiple dependent variables via the
> cbind() syntax. An alternative approach is to convert your data to long
> format and run the model in the following way:
>
> value ~ 0+variable/Varroa + (0+variable|Site) + (0+variable|Colony)
>
> with 'variable' the column containing "transpopbees", "transbrood", ...,
> and 'value' the column containing their values.
> Alternatively, function gam() from package mgcv can fit your model. You
> would then use something like:
>
> gam(list(transpopbees ~ Varroa + s(Site,bs='re') + s(Colony,bs='re'),
> transbrood ~ Varroa + s(Site,bs='re') + s(Colony,bs='re'),
> .....),family=mvn(5),data=pollencolony)
>
> Hope this helps,
> Cesko
>
> -----Original Message-----
> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On
> Behalf Of Pamela Ochungo
> Sent: Friday, May 1, 2020 9:00 PM
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] Error in initializePtr() : updateMu: Size mismatch
>
> Hallo,
>
> I want to run a linear mixed model featuring 5 response variables and only
> one predictor variable. I also have two random effects in the model. I am
> using this code:
>
> lm1 <- lmer(cbind(transpopbees, transbrood, transhoney, transpollen,
> transeggs) ~ Varroa+(1|Site)+(1|Colony),data=pollencolony)
>
> However I get this error message:
>
> Error in initializePtr() : updateMu: Size mismatch
>
> What does this mean and what am I doing wrong?
>
> Thanks
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From c@c@voeten @end|ng |rom hum@|e|denun|v@n|  Sat May  2 11:48:13 2020
From: c@c@voeten @end|ng |rom hum@|e|denun|v@n| (Voeten, C.C.)
Date: Sat, 2 May 2020 09:48:13 +0000
Subject: [R-sig-ME] Error in initializePtr() : updateMu: Size mismatch
In-Reply-To: <CAD=tpYDWULfP4HdsMXGmB-7bibfsrMnkz3AGpWguVF9atBsUUA@mail.gmail.com>
References: <CAD=tpYDy3uke2rhQuUbAQ0EDYU9uS+Q4pG6X0f+EegR8CPXc=g@mail.gmail.com>
 <06a1ebd747844ca3b51ef15afc6d07c1@hum.leidenuniv.nl>
 <CAD=tpYDWULfP4HdsMXGmB-7bibfsrMnkz3AGpWguVF9atBsUUA@mail.gmail.com>
Message-ID: <4e6712f8c2454ca99ca3bd749554eda7@hum.leidenuniv.nl>

Hi Pamela,

Re gam(): it looks like you merely didn't call summary() on your fitted model object?

Re separate models: whether or not this is appropriate depends entirely on your data and what you want to do with them. If you want to compare the different dependent variables to one another, or if you want to assume that there is correlation between them, then you need a multivariate model. But if the different dependent variables represent wholly separate measures and they are uncorrelated (or your substantive question permits you to leave such correlation out of the analysis), then I see no problem with running separate models instead.

Best,
Cesko

From: Pamela Ochungo <pamochungo at gmail.com> 
Sent: Friday, May 1, 2020 10:15 PM
To: Voeten, C.C. <c.c.voeten at hum.leidenuniv.nl>
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Error in initializePtr() : updateMu: Size mismatch

Hi Cesko,

Thanks for your?reply! I have tried the second option, function gam() from mgcv. However I get an unexpected result as below:

Family: Multivariate normal 
Link function: 

Formula:
transpopbees ~ Varroa + s(Site, bs = "re") + s(Colony, 
? ? bs = "re")
transbrood ~ Varroa + s(Site, bs = "re") + s(Colony, bs = "re")
transhoney ~ Varroa + s(Site, bs = "re") + s(Colony, bs = "re")
transpollen ~ Varroa + s(Site, bs = "re") + s(Colony, bs = "re")
transeggs ~ Varroa + s(Site, bs = "re") + s(Colony, bs = "re")

Estimated degrees of freedom:
0.0001 0.7631 0.0000 0.0003 1.7061 0.0003 1.7192 
0.0002 0.0000 0.0003 ?total = 29.19 

REML score: 547.659

How do I interpret this? I was rather hoping to get a result showing model coefficients and p-values for each dependent variable.


Question: Is it acceptable to carry out LMM (lmer) for each of the 5 dependent variables separately?against Varroa?

Cheers

Pamela

On Fri, May 1, 2020 at 10:25 PM Voeten, C.C. <mailto:c.c.voeten at hum.leidenuniv.nl> wrote:
Hi Pamela,

lmer/glmer do not support models with multiple dependent variables via the cbind() syntax. An alternative approach is to convert your data to long format and run the model in the following way:

value ~ 0+variable/Varroa + (0+variable|Site) + (0+variable|Colony)

with 'variable' the column containing "transpopbees", "transbrood", ..., and 'value' the column containing their values.
Alternatively, function gam() from package mgcv can fit your model. You would then use something like:

gam(list(transpopbees ~ Varroa + s(Site,bs='re') + s(Colony,bs='re'), transbrood ~ Varroa + s(Site,bs='re') + s(Colony,bs='re'), .....),family=mvn(5),data=pollencolony)

Hope this helps,
Cesko

-----Original Message-----
From: R-sig-mixed-models <mailto:r-sig-mixed-models-bounces at r-project.org> On Behalf Of Pamela Ochungo
Sent: Friday, May 1, 2020 9:00 PM
To: mailto:r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Error in initializePtr() : updateMu: Size mismatch

Hallo,

I want to run a linear mixed model featuring 5 response variables and only one predictor variable. I also have two random effects in the model. I am using this code:

lm1 <- lmer(cbind(transpopbees, transbrood, transhoney, transpollen,
transeggs) ~ Varroa+(1|Site)+(1|Colony),data=pollencolony)

However I get this error message:

Error in initializePtr() : updateMu: Size mismatch

What does this mean and what am I doing wrong?

Thanks

? ? ? ? [[alternative HTML version deleted]]

_______________________________________________
mailto:R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From p@mochungo @end|ng |rom gm@||@com  Sat May  2 12:11:03 2020
From: p@mochungo @end|ng |rom gm@||@com (Pamela Ochungo)
Date: Sat, 2 May 2020 13:11:03 +0300
Subject: [R-sig-ME] Error in initializePtr() : updateMu: Size mismatch
In-Reply-To: <4e6712f8c2454ca99ca3bd749554eda7@hum.leidenuniv.nl>
References: <CAD=tpYDy3uke2rhQuUbAQ0EDYU9uS+Q4pG6X0f+EegR8CPXc=g@mail.gmail.com>
 <06a1ebd747844ca3b51ef15afc6d07c1@hum.leidenuniv.nl>
 <CAD=tpYDWULfP4HdsMXGmB-7bibfsrMnkz3AGpWguVF9atBsUUA@mail.gmail.com>
 <4e6712f8c2454ca99ca3bd749554eda7@hum.leidenuniv.nl>
Message-ID: <CAD=tpYBNCx-DGQnjxqxpZt3mPe67Ucq5RU6k2hzhBY6deyFN4w@mail.gmail.com>

Hi Cesko,

Thanks! I see my mistake now! I have called the summary and it works
perfectly well!

How would you visualize the results? Graphing residuals?

Cheers and thanks once again!

Pamela

On Sat, May 2, 2020 at 12:48 PM Voeten, C.C. <c.c.voeten at hum.leidenuniv.nl>
wrote:

> Hi Pamela,
>
> Re gam(): it looks like you merely didn't call summary() on your fitted
> model object?
>
> Re separate models: whether or not this is appropriate depends entirely on
> your data and what you want to do with them. If you want to compare the
> different dependent variables to one another, or if you want to assume that
> there is correlation between them, then you need a multivariate model. But
> if the different dependent variables represent wholly separate measures and
> they are uncorrelated (or your substantive question permits you to leave
> such correlation out of the analysis), then I see no problem with running
> separate models instead.
>
> Best,
> Cesko
>
> From: Pamela Ochungo <pamochungo at gmail.com>
> Sent: Friday, May 1, 2020 10:15 PM
> To: Voeten, C.C. <c.c.voeten at hum.leidenuniv.nl>
> Cc: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Error in initializePtr() : updateMu: Size mismatch
>
> Hi Cesko,
>
> Thanks for your reply! I have tried the second option, function gam() from
> mgcv. However I get an unexpected result as below:
>
> Family: Multivariate normal
> Link function:
>
> Formula:
> transpopbees ~ Varroa + s(Site, bs = "re") + s(Colony,
>     bs = "re")
> transbrood ~ Varroa + s(Site, bs = "re") + s(Colony, bs = "re")
> transhoney ~ Varroa + s(Site, bs = "re") + s(Colony, bs = "re")
> transpollen ~ Varroa + s(Site, bs = "re") + s(Colony, bs = "re")
> transeggs ~ Varroa + s(Site, bs = "re") + s(Colony, bs = "re")
>
> Estimated degrees of freedom:
> 0.0001 0.7631 0.0000 0.0003 1.7061 0.0003 1.7192
> 0.0002 0.0000 0.0003  total = 29.19
>
> REML score: 547.659
>
> How do I interpret this? I was rather hoping to get a result showing model
> coefficients and p-values for each dependent variable.
>
>
> Question: Is it acceptable to carry out LMM (lmer) for each of the 5
> dependent variables separately against Varroa?
>
> Cheers
>
> Pamela
>
> On Fri, May 1, 2020 at 10:25 PM Voeten, C.C. <mailto:
> c.c.voeten at hum.leidenuniv.nl> wrote:
> Hi Pamela,
>
> lmer/glmer do not support models with multiple dependent variables via the
> cbind() syntax. An alternative approach is to convert your data to long
> format and run the model in the following way:
>
> value ~ 0+variable/Varroa + (0+variable|Site) + (0+variable|Colony)
>
> with 'variable' the column containing "transpopbees", "transbrood", ...,
> and 'value' the column containing their values.
> Alternatively, function gam() from package mgcv can fit your model. You
> would then use something like:
>
> gam(list(transpopbees ~ Varroa + s(Site,bs='re') + s(Colony,bs='re'),
> transbrood ~ Varroa + s(Site,bs='re') + s(Colony,bs='re'),
> .....),family=mvn(5),data=pollencolony)
>
> Hope this helps,
> Cesko
>
> -----Original Message-----
> From: R-sig-mixed-models <mailto:r-sig-mixed-models-bounces at r-project.org>
> On Behalf Of Pamela Ochungo
> Sent: Friday, May 1, 2020 9:00 PM
> To: mailto:r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] Error in initializePtr() : updateMu: Size mismatch
>
> Hallo,
>
> I want to run a linear mixed model featuring 5 response variables and only
> one predictor variable. I also have two random effects in the model. I am
> using this code:
>
> lm1 <- lmer(cbind(transpopbees, transbrood, transhoney, transpollen,
> transeggs) ~ Varroa+(1|Site)+(1|Colony),data=pollencolony)
>
> However I get this error message:
>
> Error in initializePtr() : updateMu: Size mismatch
>
> What does this mean and what am I doing wrong?
>
> Thanks
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> mailto:R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From c@c@voeten @end|ng |rom hum@|e|denun|v@n|  Sat May  2 12:15:40 2020
From: c@c@voeten @end|ng |rom hum@|e|denun|v@n| (Voeten, C.C.)
Date: Sat, 2 May 2020 10:15:40 +0000
Subject: [R-sig-ME] Error in initializePtr() : updateMu: Size mismatch
In-Reply-To: <CAD=tpYBNCx-DGQnjxqxpZt3mPe67Ucq5RU6k2hzhBY6deyFN4w@mail.gmail.com>
References: <CAD=tpYDy3uke2rhQuUbAQ0EDYU9uS+Q4pG6X0f+EegR8CPXc=g@mail.gmail.com>
 <06a1ebd747844ca3b51ef15afc6d07c1@hum.leidenuniv.nl>
 <CAD=tpYDWULfP4HdsMXGmB-7bibfsrMnkz3AGpWguVF9atBsUUA@mail.gmail.com>
 <4e6712f8c2454ca99ca3bd749554eda7@hum.leidenuniv.nl>
 <CAD=tpYBNCx-DGQnjxqxpZt3mPe67Ucq5RU6k2hzhBY6deyFN4w@mail.gmail.com>
Message-ID: <375b77f358554adbb45f6ffe4d151591@hum.leidenuniv.nl>

Hi Pamela,

I assume, since you mention graphing residuals, that you want to visualize for purposes of model checking? Then I recommend mgcv?s gam.check() function. (You can ignore the part where it checks the value of k, the basis dimension, as that is not relevant for your model.)

Best,
Cesko

From: Pamela Ochungo <pamochungo at gmail.com>
Sent: Saturday, May 2, 2020 12:11 PM
To: Voeten, C.C. <c.c.voeten at hum.leidenuniv.nl>
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Error in initializePtr() : updateMu: Size mismatch

Hi Cesko,

Thanks! I see my mistake now! I have called the summary and it works perfectly well!

How would you visualize the results? Graphing residuals?

Cheers and thanks once again!

Pamela

On Sat, May 2, 2020 at 12:48 PM Voeten, C.C. <c.c.voeten at hum.leidenuniv.nl<mailto:c.c.voeten at hum.leidenuniv.nl>> wrote:
Hi Pamela,

Re gam(): it looks like you merely didn't call summary() on your fitted model object?

Re separate models: whether or not this is appropriate depends entirely on your data and what you want to do with them. If you want to compare the different dependent variables to one another, or if you want to assume that there is correlation between them, then you need a multivariate model. But if the different dependent variables represent wholly separate measures and they are uncorrelated (or your substantive question permits you to leave such correlation out of the analysis), then I see no problem with running separate models instead.

Best,
Cesko

From: Pamela Ochungo <pamochungo at gmail.com<mailto:pamochungo at gmail.com>>
Sent: Friday, May 1, 2020 10:15 PM
To: Voeten, C.C. <c.c.voeten at hum.leidenuniv.nl<mailto:c.c.voeten at hum.leidenuniv.nl>>
Cc: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Error in initializePtr() : updateMu: Size mismatch

Hi Cesko,

Thanks for your reply! I have tried the second option, function gam() from mgcv. However I get an unexpected result as below:

Family: Multivariate normal
Link function:

Formula:
transpopbees ~ Varroa + s(Site, bs = "re") + s(Colony,
    bs = "re")
transbrood ~ Varroa + s(Site, bs = "re") + s(Colony, bs = "re")
transhoney ~ Varroa + s(Site, bs = "re") + s(Colony, bs = "re")
transpollen ~ Varroa + s(Site, bs = "re") + s(Colony, bs = "re")
transeggs ~ Varroa + s(Site, bs = "re") + s(Colony, bs = "re")

Estimated degrees of freedom:
0.0001 0.7631 0.0000 0.0003 1.7061 0.0003 1.7192
0.0002 0.0000 0.0003  total = 29.19

REML score: 547.659

How do I interpret this? I was rather hoping to get a result showing model coefficients and p-values for each dependent variable.


Question: Is it acceptable to carry out LMM (lmer) for each of the 5 dependent variables separately against Varroa?

Cheers

Pamela

On Fri, May 1, 2020 at 10:25 PM Voeten, C.C. <mailto:c.c.voeten at hum.leidenuniv.nl<mailto:c.c.voeten at hum.leidenuniv.nl>> wrote:
Hi Pamela,

lmer/glmer do not support models with multiple dependent variables via the cbind() syntax. An alternative approach is to convert your data to long format and run the model in the following way:

value ~ 0+variable/Varroa + (0+variable|Site) + (0+variable|Colony)

with 'variable' the column containing "transpopbees", "transbrood", ..., and 'value' the column containing their values.
Alternatively, function gam() from package mgcv can fit your model. You would then use something like:

gam(list(transpopbees ~ Varroa + s(Site,bs='re') + s(Colony,bs='re'), transbrood ~ Varroa + s(Site,bs='re') + s(Colony,bs='re'), .....),family=mvn(5),data=pollencolony)

Hope this helps,
Cesko

-----Original Message-----
From: R-sig-mixed-models <mailto:r-sig-mixed-models-bounces at r-project.org<mailto:r-sig-mixed-models-bounces at r-project.org>> On Behalf Of Pamela Ochungo
Sent: Friday, May 1, 2020 9:00 PM
To: mailto:r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>
Subject: [R-sig-ME] Error in initializePtr() : updateMu: Size mismatch

Hallo,

I want to run a linear mixed model featuring 5 response variables and only one predictor variable. I also have two random effects in the model. I am using this code:

lm1 <- lmer(cbind(transpopbees, transbrood, transhoney, transpollen,
transeggs) ~ Varroa+(1|Site)+(1|Colony),data=pollencolony)

However I get this error message:

Error in initializePtr() : updateMu: Size mismatch

What does this mean and what am I doing wrong?

Thanks

        [[alternative HTML version deleted]]

_______________________________________________
mailto:R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From juho@kr|@t|@n@ruohonen @end|ng |rom gm@||@com  Sat May  2 14:04:02 2020
From: juho@kr|@t|@n@ruohonen @end|ng |rom gm@||@com (Juho Kristian Ruohonen)
Date: Sat, 2 May 2020 15:04:02 +0300
Subject: [R-sig-ME] 
 Struggling (probably problem in wetware not software)
 with multilevel logistic regression
In-Reply-To: <100063919.9989289.1588358279505.JavaMail.zimbra@psyctc.org>
References: <100063919.9989289.1588358279505.JavaMail.zimbra@psyctc.org>
Message-ID: <CAG_dBVdk4i7K4naeeiEVgsbfxrm6ScytdoXmqyknjvBOE3sNyg@mail.gmail.com>

(reposting my response since the first version failed to include the
mailing list among the recipients)

Here?s a non-statistician?s take:

You don?t need a multilevel model for this. Given that every respondent
contributes only one data point, no one?s idiosyncracies are
overrepresented, and we can expect them to cancel each other out. Your
question can be modeled using standard logistic regression.

Since you suspect *posItem* may influence the chance of omission, you
should include it as an additional fixed effect besides *negItem*.
Something like:

mymodel <- glm(missing ~ posItem + riskItem, family = binomial, data =
mydata)

It?ll be interesting to hear whether real statisticians disagree.

Best,

J

pe 1. toukok. 2020 klo 21.38 Chris Evans (chrishold at psyctc.org) kirjoitti:

> I am sorry if this is embarrassingly stupid/easy but I am failing to
> understand something I thought would be fairly easy.
>
> The question of interest to me and others is whether people omit some
> items of a questionnaire more than others.
> To give a simple example I have several datasets for a ten item
> self-report questionnaire where one item is about risk
> and where three items are cued positively and the other seven are cued
> negatively.
>
> Here is an example of some such data:
>
>    ID       Item     Score missing missRand missReg riskItem posItem
>    <chr>    <chr>    <dbl>   <dbl>    <int>   <int>    <dbl>   <dbl>
>  1 ALCA0001 YPCORE01     1       0        0       0        0       0
>  2 ALCA0001 YPCORE02     0       0        0       0        0       0
>  3 ALCA0001 YPCORE03     0       0        0       0        0       1
>  4 ALCA0001 YPCORE04     0       0        0       0        1       0
>  5 ALCA0001 YPCORE05     0       0        0       0        0       1
>  6 ALCA0001 YPCORE06     1       0        0       0        0       0
>  7 ALCA0001 YPCORE07     0       0        0       0        0       0
>  8 ALCA0001 YPCORE08     0       0        0       0        0       0
>  9 ALCA0001 YPCORE09     0       0        0       0        0       0
> 10 ALCA0001 YPCORE10     0       0        0       0        0       1
> 11 ALCA0004 YPCORE01     1       0        0       0        0       0
> 12 ALCA0004 YPCORE02     0       0        0       0        0       0
> 13 ALCA0004 YPCORE03     0       0        0       0        0       1
> 14 ALCA0004 YPCORE04     0       0        0       0        1       0
> 15 ALCA0004 YPCORE05     0       0        0       0        0       1
>
> missing is the observed omission of the item (1 = omitted)
> missRand is a binomial random with probability = .5 I created
> missReg is a biomial random with probability = item index number / 11,
> i.e. strong relationship with item
> riskItem identifies the risk items (item 4)
> posItem identifies the positively cued items (3, 5 and 10)
>
> For this particular dataset there are 237 participants.  (Like Groucho
> Marks and principles, I have
> others if you don't like this one!)
>
> The catch is that each participant has only completed the measure once.
> It is a plausible model that
> participants will vary in their propensity to omit items, it is also
> highly plausible that participants
> might omit the risk item more than the other item. It's a question of
> interest (though I think not of
> huge interest nor would I bet on it (!) that the positively cued items
> might be omitted more than the
> negatively cued ones or v.v.
>
> I thought I could test for these effects using glmer().  That works for
>
> > fitRisk <- glmer(missing ~ riskItem + (1 | ID), family = binomial, data
> = longDat)
> > summary(fitRisk)
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: missing ~ riskItem + (1 | ID)
>    Data: longDat
>
>      AIC      BIC   logLik deviance df.resid
>     51.7     69.0    -22.9     45.7     2367
>
> Scaled residuals:
>      Min       1Q   Median       3Q      Max
> -0.52020 -0.00113 -0.00113 -0.00113  2.93829
>
> Random effects:
>  Groups Name        Variance Std.Dev.
>  ID     (Intercept) 205.3    14.33
> Number of obs: 2370, groups:  ID, 237
>
> Fixed effects:
>             Estimate Std. Error z value Pr(>|z|)
> (Intercept)  -13.563      2.540  -5.339 9.33e-08 ***
> riskItem      -2.418      3.745  -0.646    0.518
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> Correlation of Fixed Effects:
>          (Intr)
> riskItem 0.064
>
> No effect but small dataset and test of principle works.  However, when I
> come to test the overall difference by items I hit
>
> 5       NA     NA     NA     NA     NA     NA     NA     NA     NA     NA
>    NA     NA     NA     NA     NA     NA
>   BITE23 BITE24 BITE25 BITE26 BITE27 BITE28 BITE29 BITE30 BITE31 BITE32
> BITE33 Notas origRow Centro2 transfer one
> 1     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA
>  NA  <NA>      43  Urgell        0   1
> 2     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA
>  NA  <NA>      68  Urgell        0   1
> 3     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA
>  NA  <NA>      69  Urgell        0   1
> 4     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA
>  NA  <NA>      70  Urgell        0   1
> 5     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA
>  NA  <NA>      71  Urgell        0   1
>   Exclude quaireN ID.quaireN Nquaires  ID.episodeN episodeN nEpisodes
> Fecha.inicio  Fecha.fin ExpCOREOM ExpSFA ExpSFB
> 1       0       5 URGE0003-5       27 URGE0003-001        1         1
>  2017-11-13 2019-02-12        NA     NA     NA
> 2       0       3 URGE0004-3       36 URGE0004-001        1         2
>  2017-11-21 2018-09-10        NA     NA     NA
> 3       0       4 URGE0004-4       36 URGE0004-001        1         2
>  2017-11-21 2018-09-10        NA     NA     NA
> 4       0       5 URGE0004-5       36 URGE0004-001        1         2
>  2017-11-21 2018-09-10        NA     NA     NA
> 5       0       6 URGE0004-6       36 URGE0004-001        1         2
>  2017-11-21 2018-09-10        NA     NA     NA
>   ExpYPCORE ExpEAT ExpBITE missingCOREOM missingSFA missingSFB missingYP
> missingEAT missingBITE impossCOREOM impossSFA
> 1        NA     NA      NA            NA         NA         NA        NA
>        NA          NA           NA        NA
> 2        NA     NA      NA            NA         NA         NA        NA
>        NA          NA           NA        NA
> 3        NA     NA      NA            NA         NA         NA        NA
>        NA          NA           NA        NA
> 4        NA     NA      NA            NA         NA         NA        NA
>        NA          NA           NA        NA
> 5        NA     NA      NA            NA         NA         NA        NA
>        NA          NA           NA        NA
>   impossSFB impossYP impossEAT impossBITE
> 1        NA       NA        NA         NA
> 2        NA       NA        NA         NA
> 3        NA       NA        NA         NA
> 4        NA       NA        NA         NA
> 5        NA       NA        NA         NA
>  [ reached 'max' / getOption("max.print") -- omitted 672 rows ]
> > datPrincip_URGE %>%
> +   arrange(Fecha.evaluacion) %>% # sort by evaluation date
> +    filter(Cuestionarios == "YP-CORE" &
> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
> +   dim()
> [1] 677 195
> > datPrincip_URGE %>%
> +   arrange(Fecha.evaluacion) %>% # sort by evaluation date
> +    filter(Cuestionarios == "YP-CORE" &
> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
> +   group_by(ID) %>%
> +   mutate(occasion = row_number(),
> +          first = if_else(occasion == 1, 1, 0)) %>% # to get first
> ocassion only
> +   filter(first == 1) %>%
> +     dim()
> [1]  61 197
> > dat <- bind_rows(datPrincip_ALCA,
> +                  datPrincip_URGE)
> > dat %>%
> +   arrange(Fecha.evaluacion) %>% # sort by evaluation date
> +    filter(Cuestionarios == "YP-CORE" &
> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
> +   group_by(ID) %>%
> +   mutate(occasion = row_number(),
> +          first = if_else(occasion == 1, 1, 0)) %>% # to get first
> ocassion only
> +   filter(first == 1) %>%
> +     dim()
> [1] 118 197
> > dat <- bind_rows(datPrincip_ALCA,
> +                  datPrincip_ARGE,
> +                  datPrincip_AVEN,
> +                  datPrincip_MOSC,
> +                  datPrincip_SABA,
> +                  datPrincip_SEVI,
> +                  datPrincip_TARR,
> +                  datPrincip_URGE)
> > dat %>%
> +   arrange(Fecha.evaluacion) %>% # sort by evaluation date
> +    filter(Cuestionarios == "YP-CORE" &
> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
> +   group_by(ID) %>%
> +   mutate(occasion = row_number(),
> +          first = if_else(occasion == 1, 1, 0)) %>% # to get first
> ocassion only
> +   filter(first == 1) %>%
> +     dim()
> [1] 237 197
> > dat %>%
> +   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
> +    filter(Cuestionarios == "YP-CORE" &
> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
> +   group_by(ID) %>%
> +   mutate(occasion = row_number(),
> +          first = if_else(occasion == 1, 1, 0)) %>% # to get first
> ocassion only
> +   filter(first == 1) %>%
> +     dim()
> [1] 237 197
> > dat %>%
> +   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
> +    filter(Cuestionarios == "YP-CORE" &
> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
> +   group_by(ID) %>%
> +   mutate(occasion = row_number(),
> +          first = if_else(occasion == 1, 1, 0)) %>% # to get first
> ocassion only
> +   filter(first == 1) %>%
> +   select(ID, starts_with("YP")) %>%
> +   dim()
> [1] 237  11
> > dat %>%
> +   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
> +    filter(Cuestionarios == "YP-CORE" &
> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
> +   group_by(ID) %>%
> +   mutate(occasion = row_number(),
> +          first = if_else(occasion == 1, 1, 0)) %>% # to get first
> ocassion only
> +   filter(first == 1) %>%
> +   select(ID, starts_with("YP")) %>%
> +   pivot_longer(cols = starts_with("YPCORE")) %>%
> +   dim()
> [1] 2370    3
> > dat %>%
> +   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
> +    filter(Cuestionarios == "YP-CORE" &
> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
> +   group_by(ID) %>%
> +   mutate(occasion = row_number(),
> +          first = if_else(occasion == 1, 1, 0)) %>% # to get first
> ocassion only
> +   filter(first == 1) %>%
> +   select(ID, starts_with("YP")) %>%
> +   pivot_longer(cols = starts_with("YPCORE")) %>%
> +   mutate(missing = if_else(!is.na(value), 0, 1)) -> longDat
> > head(longDat)
> # A tibble: 6 x 4
> # Groups:   ID [1]
>   ID       name     value missing
>   <chr>    <chr>    <dbl>   <dbl>
> 1 ALCA0001 YPCORE01     1       0
> 2 ALCA0001 YPCORE02     0       0
> 3 ALCA0001 YPCORE03     0       0
> 4 ALCA0001 YPCORE04     0       0
> 5 ALCA0001 YPCORE05     0       0
> 6 ALCA0001 YPCORE06     1       0
> > table(longDat$missing)
>
>    0    1
> 2356   14
> > ?pivot_longer
> > dat %>%
> +   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
> +    filter(Cuestionarios == "YP-CORE" &
> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
> +   group_by(ID) %>%
> +   mutate(occasion = row_number(),
> +          first = if_else(occasion == 1, 1, 0)) %>% # to get first
> ocassion only
> +   filter(first == 1) %>%
> +   select(ID, starts_with("YP")) %>%
> +   pivot_longer(cols = starts_with("YPCORE"),
> +                names_to = "Item") %>%
> +   mutate(missing = if_else(!is.na(value), 0, 1)) -> longDat
> > head(longD)
> Error in head(longD) : object 'longD' not found
> > head(longDat)
> # A tibble: 6 x 4
> # Groups:   ID [1]
>   ID       Item     value missing
>   <chr>    <chr>    <dbl>   <dbl>
> 1 ALCA0001 YPCORE01     1       0
> 2 ALCA0001 YPCORE02     0       0
> 3 ALCA0001 YPCORE03     0       0
> 4 ALCA0001 YPCORE04     0       0
> 5 ALCA0001 YPCORE05     0       0
> 6 ALCA0001 YPCORE06     1       0
> > dat %>%
> +   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
> +    filter(Cuestionarios == "YP-CORE" &
> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
> +   group_by(ID) %>%
> +   mutate(occasion = row_number(),
> +          first = if_else(occasion == 1, 1, 0)) %>% # to get first
> ocassion only
> +   filter(first == 1) %>%
> +   select(ID, starts_with("YP")) %>%
> +   pivot_longer(cols = starts_with("YPCORE"),
> +                names_to = "Item",
> +                values_to = "Score") %>%
> +   mutate(missing = if_else(!is.na(Score), 0, 1)) -> longDat
> > head(longDat)
> # A tibble: 6 x 4
> # Groups:   ID [1]
>   ID       Item     Score missing
>   <chr>    <chr>    <dbl>   <dbl>
> 1 ALCA0001 YPCORE01     1       0
> 2 ALCA0001 YPCORE02     0       0
> 3 ALCA0001 YPCORE03     0       0
> 4 ALCA0001 YPCORE04     0       0
> 5 ALCA0001 YPCORE05     0       0
> 6 ALCA0001 YPCORE06     1       0
> > formul1 <- missing ~ Item + Item | ID
> > fit <- glmer(formul1, family = binomial, data = longDat)
> Error in glmer(formul1, family = binomial, data = longDat) :
>   could not find function "glmer"
> > library(lme4)
> Loading required package: Matrix
>
> Attaching package: ?Matrix?
>
> The following objects are masked from ?package:tidyr?:
>
>     expand, pack, unpack
>
> > formul1 <- missing ~ Item + Item | ID
> > fit <- glmer(formul1, family = binomial, data = longDat)
> boundary (singular) fit: see ?isSingular
> Warning messages:
> 1: In commonArgs(par, fn, control, environment()) :
>   maxfun < 10 * length(par)^2 is not recommended.
> 2: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,
> :
>   failure to converge in 10000 evaluations
> > formul1 <- missing ~ Item | ID
> > fit <- glmer(formul1, family = binomial, data = longDat)
> boundary (singular) fit: see ?isSingular
> Warning messages:
> 1: In commonArgs(par, fn, control, environment()) :
>   maxfun < 10 * length(par)^2 is not recommended.
> 2: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,
> :
>   failure to converge in 10000 evaluations
> > head(longDat)
> # A tibble: 6 x 4
> # Groups:   ID [1]
>   ID       Item     Score missing
>   <chr>    <chr>    <dbl>   <dbl>
> 1 ALCA0001 YPCORE01     1       0
> 2 ALCA0001 YPCORE02     0       0
> 3 ALCA0001 YPCORE03     0       0
> 4 ALCA0001 YPCORE04     0       0
> 5 ALCA0001 YPCORE05     0       0
> 6 ALCA0001 YPCORE06     1       0
> > ?glmer
> > ?mcnemar.test
> > table(is.na(dat$YPCORE01), is.na(dat$YPCORE02))
>
>         FALSE TRUE
>   FALSE  3573    4
>   TRUE      1 9499
> > mcnemar.test(table(is.na(dat$YPCORE01), is.na(dat$YPCORE02)))
>
>         McNemar's Chi-squared test with continuity correction
>
> data:  table(is.na(dat$YPCORE01), is.na(dat$YPCORE02))
> McNemar's chi-squared = 0.8, df = 1, p-value = 0.3711
>
> > dat %>%
> +   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
> +    filter(Cuestionarios == "YP-CORE" &
> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
> +   group_by(ID) %>%
> +   mutate(occasion = row_number(),
> +          first = if_else(occasion == 1, 1, 0)) %>% # to get first
> ocassion only
> +   filter(first == 1) %>%
> +   select(ID, starts_with("YP")) %>%
> +   pivot_longer(cols = c("YPCORE01", "YPCORE02")),
> Error: unexpected ',' in:
> "  select(ID, starts_with("YP")) %>%
>   pivot_longer(cols = c("YPCORE01", "YPCORE02")),"
> >                names_to = "Item",
> Error: unexpected ',' in "               names_to = "Item","
> >                values_to = "Score") %>%
> Error: unexpected ')' in "               values_to = "Score")"
> >   mutate(missing = if_else(!is.na(Score), 0, 1)) -> longDat
> Error in if_else(!is.na(Score), 0, 1) : object 'Score' not found
> > head(longDat)
> # A tibble: 6 x 4
> # Groups:   ID [1]
>   ID       Item     Score missing
>   <chr>    <chr>    <dbl>   <dbl>
> 1 ALCA0001 YPCORE01     1       0
> 2 ALCA0001 YPCORE02     0       0
> 3 ALCA0001 YPCORE03     0       0
> 4 ALCA0001 YPCORE04     0       0
> 5 ALCA0001 YPCORE05     0       0
> 6 ALCA0001 YPCORE06     1       0
> > dat %>%
> +   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
> +    filter(Cuestionarios == "YP-CORE" &
> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
> +   group_by(ID) %>%
> +   mutate(occasion = row_number(),
> +          first = if_else(occasion == 1, 1, 0)) %>% # to get first
> ocassion only
> +   filter(first == 1) %>%
> +   select(ID, starts_with("YP")) %>%
> +   pivot_longer(cols = c("YPCORE01", "YPCORE02"),
> +                names_to = "Item",
> +                values_to = "Score") %>%
> +   mutate(missing = if_else(!is.na(Score), 0, 1)) -> longDat
> > head(longDat)
> # A tibble: 6 x 12
> # Groups:   ID [3]
>   ID       YPCORE03 YPCORE04 YPCORE05 YPCORE06 YPCORE07 YPCORE08 YPCORE09
> YPCORE10 Item     Score missing
>   <chr>       <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>
>   <dbl> <chr>    <dbl>   <dbl>
> 1 ALCA0001        0        0        0        1        0        0        0
>       0 YPCORE01     1       0
> 2 ALCA0001        0        0        0        1        0        0        0
>       0 YPCORE02     0       0
> 3 ALCA0004        0        0        0        1        0        0        0
>       0 YPCORE01     1       0
> 4 ALCA0004        0        0        0        1        0        0        0
>       0 YPCORE02     0       0
> 5 ALCA0007        1        0        0        1        0        2        1
>       0 YPCORE01     2       0
> 6 ALCA0007        1        0        0        1        0        2        1
>       0 YPCORE02     1       0
> > dat %>%
> +   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
> +    filter(Cuestionarios == "YP-CORE" &
> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
> +   group_by(ID) %>%
> +   mutate(occasion = row_number(),
> +          first = if_else(occasion == 1, 1, 0)) %>% # to get first
> ocassion only
> +   filter(first == 1) %>%
> +   select(ID, c("YPCORE01", "YPCORE02")) %>%
> +   pivot_longer(cols = c("YPCORE01", "YPCORE02"),
> +                names_to = "Item",
> +                values_to = "Score") %>%
> +   mutate(missing = if_else(!is.na(Score), 0, 1)) -> longDat
> > head(longDat)
> # A tibble: 6 x 4
> # Groups:   ID [3]
>   ID       Item     Score missing
>   <chr>    <chr>    <dbl>   <dbl>
> 1 ALCA0001 YPCORE01     1       0
> 2 ALCA0001 YPCORE02     0       0
> 3 ALCA0004 YPCORE01     1       0
> 4 ALCA0004 YPCORE02     0       0
> 5 ALCA0007 YPCORE01     2       0
> 6 ALCA0007 YPCORE02     1       0
> > fit <- glmer(formul1, family = binomial, data = longDat)
> boundary (singular) fit: see ?isSingular
> > ?isSingular
> > head(longDat)
> # A tibble: 6 x 4
> # Groups:   ID [3]
>   ID       Item     Score missing
>   <chr>    <chr>    <dbl>   <dbl>
> 1 ALCA0001 YPCORE01     1       0
> 2 ALCA0001 YPCORE02     0       0
> 3 ALCA0004 YPCORE01     1       0
> 4 ALCA0004 YPCORE02     0       0
> 5 ALCA0007 YPCORE01     2       0
> 6 ALCA0007 YPCORE02     1       0
> > table(longDat$missing)
>
>   0   1
> 472   2
> > library(lme4)
> > formul1 <- missing ~ (Item | ID)
> > fit <- glmer(formul1, family = binomial, data = longDat)
> boundary (singular) fit: see ?isSingular
> > longDat$missRand <- rbinom(1, length(longDat), .5)
> > head(longDat)
> # A tibble: 6 x 5
> # Groups:   ID [3]
>   ID       Item     Score missing missRand
>   <chr>    <chr>    <dbl>   <dbl>    <int>
> 1 ALCA0001 YPCORE01     1       0        2
> 2 ALCA0001 YPCORE02     0       0        2
> 3 ALCA0004 YPCORE01     1       0        2
> 4 ALCA0004 YPCORE02     0       0        2
> 5 ALCA0007 YPCORE01     2       0        2
> 6 ALCA0007 YPCORE02     1       0        2
> > rbinom(1,1,.5)
> [1] 0
> > rbinom(1,1,.5)
> [1] 0
> > rbinom(1,1,.5)
> [1] 0
> > rbinom(1,1,.5)
> [1] 0
> > rbinom(1,1,.5)
> [1] 0
> > rbinom(1,1,.5)
> [1] 0
> > rbinom(1,1,.5)
> [1] 0
> > rbinom(1,1,.5)
> [1] 0
> > ?rbinom
> > rbinom(1,1,.05)
> [1] 0
> > rbinom(1,1,.05)
> [1] 0
> > rbinom(1,1,.95)
> [1] 1
> > rbinom(1,1,.5)
> [1] 0
> > rbinom(1,1,.95)
> [1] 1
> > rbinom(1,1,.5)
> [1] 1
> > rbinom(1,1,.5)
> [1] 0
> > rbinom(1,1,.5)
> [1] 0
> > rbinom(1,1,.5)
> [1] 1
> > rbinom(1,1,.5)
> [1] 0
> > rbinom(1,1,.5)
> [1] 1
> > rbinom(1,2,.5)
> [1] 2
> > rbinom(2,1,.5)
> [1] 0 0
> > rbinom(2,1,.5)
> [1] 0 0
> > rbinom(2,1,.5)
> [1] 0 0
> > rbinom(2,1,.5)
> [1] 0 1
> > rbinom(2,1,.5)
> [1] 1 1
> > rbinom(2,1,.5)
> [1] 0 1
> > longDat$missRand <- rbinom(length(longDat), 1, .5)
> Error: Assigned data `rbinom(length(longDat), 1, 0.5)` must be compatible
> with existing data.
> x Existing data has 474 rows.
> x Assigned data has 5 rows.
> ? Only vectors of size 1 are recycled.
> Run `rlang::last_error()` to see where the error occurred.
> > table(longDat$missing)
>
>   0   1
> 472   2
> > library(lme4)
> > head(longDat)
> # A tibble: 6 x 5
> # Groups:   ID [3]
>   ID       Item     Score missing missRand
>   <chr>    <chr>    <dbl>   <dbl>    <int>
> 1 ALCA0001 YPCORE01     1       0        2
> 2 ALCA0001 YPCORE02     0       0        2
> 3 ALCA0004 YPCORE01     1       0        2
> 4 ALCA0004 YPCORE02     0       0        2
> 5 ALCA0007 YPCORE01     2       0        2
> 6 ALCA0007 YPCORE02     1       0        2
> > rbinom(2,1,.5)
> [1] 1 0
> > rbinom(2,1,.5)
> [1] 1 1
> > rbinom(2,1,.5)
> [1] 1 1
> > longDat$missRand <- NULL
> > longDat$missRand <- rbinom(length(longDat), 1, .5)
> Error: Assigned data `rbinom(length(longDat), 1, 0.5)` must be compatible
> with existing data.
> x Existing data has 474 rows.
> x Assigned data has 4 rows.
> ? Only vectors of size 1 are recycled.
> Run `rlang::last_error()` to see where the error occurred.
> > longDat$missRand <- rbinom(nrow(longDat), 1, .5)
> > table(longDat$missRand)
>
>   0   1
> 240 234
> > head(longDat)
> # A tibble: 6 x 5
> # Groups:   ID [3]
>   ID       Item     Score missing missRand
>   <chr>    <chr>    <dbl>   <dbl>    <int>
> 1 ALCA0001 YPCORE01     1       0        0
> 2 ALCA0001 YPCORE02     0       0        1
> 3 ALCA0004 YPCORE01     1       0        0
> 4 ALCA0004 YPCORE02     0       0        1
> 5 ALCA0007 YPCORE01     2       0        0
> 6 ALCA0007 YPCORE02     1       0        0
> > formul1 <- missRand ~ (Item | ID)
> > fit <- glmer(formul1, family = binomial, data = longDat)
> boundary (singular) fit: see ?isSingular
> > fit <- glmer(missRand ~ Item, family = binomial, data = longDat)
> Error: No random effects terms specified in formula
> > fit <- glmer(missRand ~ Item | ID, family = binomial, data = longDat)
> boundary (singular) fit: see ?isSingular
> > fit <- glmer(factor(missRand) ~ Item | ID, family = binomial, data =
> longDat)
> boundary (singular) fit: see ?isSingular
> > fit <- glmer(missRand ~ Item | ID, family = "logistic", data = longDat)
> Error in get(family, mode = "function", envir = parent.frame(2)) :
>   object 'logistic' of mode 'function' was not found
> > fit <- glmer(missRand ~ Item + (1 | ID), family = "binomial", data =
> longDat)
> boundary (singular) fit: see ?isSingular
> > ?glmer
> > longDat
> # A tibble: 474 x 5
> # Groups:   ID [237]
>    ID       Item     Score missing missRand
>    <chr>    <chr>    <dbl>   <dbl>    <int>
>  1 ALCA0001 YPCORE01     1       0        0
>  2 ALCA0001 YPCORE02     0       0        1
>  3 ALCA0004 YPCORE01     1       0        0
>  4 ALCA0004 YPCORE02     0       0        1
>  5 ALCA0007 YPCORE01     2       0        0
>  6 ALCA0007 YPCORE02     1       0        0
>  7 ALCA0012 YPCORE01     3       0        1
>  8 ALCA0012 YPCORE02     1       0        1
>  9 ALCA0013 YPCORE01     4       0        1
> 10 ALCA0013 YPCORE02     3       0        1
> # ? with 464 more rows
> > table(longDat$missRand, longDat$Item)
>
>     YPCORE01 YPCORE02
>   0      125      115
>   1      112      122
> >
>
> > fit <- glmer(missRand ~ factor(Item) + (1 | ID), family = "binomial",
> data = longDat)
> boundary (singular) fit: see ?isSingular
> > fit <- glmer(missRand ~ factor(Item) + (1 | factor(ID)), family =
> "binomial", data = longDat)
> Error: couldn't evaluate grouping factor factor(ID) within model frame:
> try adding grouping factor to data frame explicitly if possible
> > fit <- glmer(missRand ~ factor(Item) + (1 | ID), family = "binomial",
> data = longDat)
> boundary (singular) fit: see ?isSingular
> > rm(fit)
> > fit <- glmer(missRand ~ factor(Item) + (1 | ID), family = "binomial",
> data = longDat)
> boundary (singular) fit: see ?isSingular
> > fit
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: missRand ~ factor(Item) + (1 | ID)
>    Data: longDat
>       AIC       BIC    logLik  deviance  df.resid
>  662.1833  674.6669 -328.0917  656.1833       471
> Random effects:
>  Groups Name        Std.Dev.
>  ID     (Intercept) 1.943e-07
> Number of obs: 474, groups:  ID, 237
> Fixed Effects:
>          (Intercept)  factor(Item)YPCORE02
>              -0.1098                0.1689
> convergence code 0; 0 optimizer warnings; 1 lme4 warnings
> > dat %>%
> +   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
> +    filter(Cuestionarios == "YP-CORE" &
> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
> +   group_by(ID) %>%
> +   mutate(occasion = row_number(),
> +          first = if_else(occasion == 1, 1, 0)) %>% # to get first
> ocassion only
> +   filter(first == 1) %>%
> +   select(ID, starts_with("YPCORE")) %>%
> +   pivot_longer(cols = starts_with("YPCORE"),
> +                names_to = "Item",
> +                values_to = "Score") %>%
> +   mutate(missing = if_else(!is.na(Score), 0, 1)) -> longDat
> > head(longDat)
> # A tibble: 6 x 4
> # Groups:   ID [1]
>   ID       Item     Score missing
>   <chr>    <chr>    <dbl>   <dbl>
> 1 ALCA0001 YPCORE01     1       0
> 2 ALCA0001 YPCORE02     0       0
> 3 ALCA0001 YPCORE03     0       0
> 4 ALCA0001 YPCORE04     0       0
> 5 ALCA0001 YPCORE05     0       0
> 6 ALCA0001 YPCORE06     1       0
> > formul1 <- missing ~ (Item | ID)
> > fit <- glmer(missing ~ factor(Item) + (1 | ID), family = "binomial",
> data = longDat)
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 1.78883 (tol = 0.002,
> component 1)
> > fit <- glmer((score == 1) ~ factor(Item) + (1 | ID), family =
> "binomial", data = longDat)
> Error in eval(predvars, data, env) : object 'score' not found
> > fit <- glmer((Score == 1) ~ factor(Item) + (1 | ID), family =
> "binomial", data = longDat)
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 0.00309555 (tol = 0.002,
> component 1)
> > fit <- glmer((Score == 1) ~ factor(Item) + (factor(Item) | ID), family =
> "binomial", data = longDat)
> Error: number of observations (=2356) < number of random effects (=2360)
> for term (factor(Item) | ID); the random-effects parameters are probably
> unidentifiable
> > fit <- glmer((Score == 1) ~ factor(Item) + (Item | ID), family =
> "binomial", data = longDat)
> Error: number of observations (=2356) < number of random effects (=2360)
> for term (Item | ID); the random-effects parameters are probably
> unidentifiable
> > fit <- glmer((Score == 1) ~ factor(Item) + (1 | ID), family =
> "binomial", data = longDat)
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 0.00309555 (tol = 0.002,
> component 1)
> > head(longDat)
> # A tibble: 6 x 4
> # Groups:   ID [1]
>   ID       Item     Score missing
>   <chr>    <chr>    <dbl>   <dbl>
> 1 ALCA0001 YPCORE01     1       0
> 2 ALCA0001 YPCORE02     0       0
> 3 ALCA0001 YPCORE03     0       0
> 4 ALCA0001 YPCORE04     0       0
> 5 ALCA0001 YPCORE05     0       0
> 6 ALCA0001 YPCORE06     1       0
> > longDat$missRand <- rbinom(nrow(longDat), 1, .5)
> > table(longDat$missRand)
>
>    0    1
> 1211 1159
> > fit <- glmer(missRand ~ Item + (1 | ID), family = "binomial", data =
> longDat)
> boundary (singular) fit: see ?isSingular
> > fit
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: missRand ~ Item + (1 | ID)
>    Data: longDat
>       AIC       BIC    logLik  deviance  df.resid
>  3300.819  3364.296 -1639.410  3278.819      2359
> Random effects:
>  Groups Name        Std.Dev.
>  ID     (Intercept) 2.663e-07
> Number of obs: 2370, groups:  ID, 237
> Fixed Effects:
>  (Intercept)  ItemYPCORE02  ItemYPCORE03  ItemYPCORE04  ItemYPCORE05
> ItemYPCORE06  ItemYPCORE07  ItemYPCORE08
>   -5.909e-02     1.013e-01    -1.690e-02     1.182e-01     6.753e-02
> -1.186e-01    -1.046e-15    -1.186e-01
> ItemYPCORE09  ItemYPCORE10
>    1.858e-01    -6.766e-02
> convergence code 0; 0 optimizer warnings; 1 lme4 warnings
> > anova(fit)
> Analysis of Variance Table
>      npar Sum Sq Mean Sq F value
> Item    9 5.5453 0.61615  0.6161
> > table(longDat$Item, as.numeric(longDat$Item))
> < table of extent 10 x 0 >
> Warning message:
> In table(longDat$Item, as.numeric(longDat$Item)) :
>   NAs introduced by coercion
> > dim(longDat)
> [1] 2370    5
> > head(longDat)
> # A tibble: 6 x 5
> # Groups:   ID [1]
>   ID       Item     Score missing missRand
>   <chr>    <chr>    <dbl>   <dbl>    <int>
> 1 ALCA0001 YPCORE01     1       0        1
> 2 ALCA0001 YPCORE02     0       0        1
> 3 ALCA0001 YPCORE03     0       0        1
> 4 ALCA0001 YPCORE04     0       0        1
> 5 ALCA0001 YPCORE05     0       0        1
> 6 ALCA0001 YPCORE06     1       0        0
> > table(longDat$Item) #, as.numeric(longDat$Item))
>
> YPCORE01 YPCORE02 YPCORE03 YPCORE04 YPCORE05 YPCORE06 YPCORE07 YPCORE08
> YPCORE09 YPCORE10
>      237      237      237      237      237      237      237      237
>   237      237
> > table(as.numeric(longDat$Item)) #, as.numeric(longDat$Item))
> < table of extent 0 >
> Warning message:
> In table(as.numeric(longDat$Item)) : NAs introduced by coercion
> > table(longDat$Item, as.numeric(factor(longDat$Item)))
>
>              1   2   3   4   5   6   7   8   9  10
>   YPCORE01 237   0   0   0   0   0   0   0   0   0
>   YPCORE02   0 237   0   0   0   0   0   0   0   0
>   YPCORE03   0   0 237   0   0   0   0   0   0   0
>   YPCORE04   0   0   0 237   0   0   0   0   0   0
>   YPCORE05   0   0   0   0 237   0   0   0   0   0
>   YPCORE06   0   0   0   0   0 237   0   0   0   0
>   YPCORE07   0   0   0   0   0   0 237   0   0   0
>   YPCORE08   0   0   0   0   0   0   0 237   0   0
>   YPCORE09   0   0   0   0   0   0   0   0 237   0
>   YPCORE10   0   0   0   0   0   0   0   0   0 237
> > longDat %>%
> +   mutate(missRand = rbinom(1, 1, .5))
> # A tibble: 2,370 x 5
> # Groups:   ID [237]
>    ID       Item     Score missing missRand
>    <chr>    <chr>    <dbl>   <dbl>    <int>
>  1 ALCA0001 YPCORE01     1       0        1
>  2 ALCA0001 YPCORE02     0       0        1
>  3 ALCA0001 YPCORE03     0       0        1
>  4 ALCA0001 YPCORE04     0       0        1
>  5 ALCA0001 YPCORE05     0       0        1
>  6 ALCA0001 YPCORE06     1       0        1
>  7 ALCA0001 YPCORE07     0       0        1
>  8 ALCA0001 YPCORE08     0       0        1
>  9 ALCA0001 YPCORE09     0       0        1
> 10 ALCA0001 YPCORE10     0       0        1
> # ? with 2,360 more rows
> > table(longDat$missRand)
>
>    0    1
> 1211 1159
> > longDat %>%
> +   mutate(missRand = rbinom(1, 1, .5),
> +          missReg = rbinom(1, 1, as.numeric(factor(Item))/11))
> # A tibble: 2,370 x 6
> # Groups:   ID [237]
>    ID       Item     Score missing missRand missReg
>    <chr>    <chr>    <dbl>   <dbl>    <int>   <int>
>  1 ALCA0001 YPCORE01     1       0        1       0
>  2 ALCA0001 YPCORE02     0       0        1       0
>  3 ALCA0001 YPCORE03     0       0        1       0
>  4 ALCA0001 YPCORE04     0       0        1       0
>  5 ALCA0001 YPCORE05     0       0        1       0
>  6 ALCA0001 YPCORE06     1       0        1       0
>  7 ALCA0001 YPCORE07     0       0        1       0
>  8 ALCA0001 YPCORE08     0       0        1       0
>  9 ALCA0001 YPCORE09     0       0        1       0
> 10 ALCA0001 YPCORE10     0       0        1       0
> # ? with 2,360 more rows
> > table(longDat$missReg)
> < table of extent 0 >
> Warning message:
> Unknown or uninitialised column: `missReg`.
> > longDat %>%
> +   mutate(missRand = rbinom(1, 1, .5),
> +          missReg = rbinom(1, 1, as.numeric(factor(Item))/11)) -> longDat
> > table(longDat$missReg)
>
>    0    1
> 2120  250
> > fit <- glmer(missReg ~ Item + (1 | ID), family = "binomial", data =
> longDat)
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 3.81902 (tol = 0.002,
> component 1)
> > longDat %>%
> +   mutate(missRand = rbinom(1, 1, .5),
> +          missReg = rbinom(1, 1, as.numeric(factor(Item))/11)) -> longDat
> > head(longDat)
> # A tibble: 6 x 6
> # Groups:   ID [1]
>   ID       Item     Score missing missRand missReg
>   <chr>    <chr>    <dbl>   <dbl>    <int>   <int>
> 1 ALCA0001 YPCORE01     1       0        1       1
> 2 ALCA0001 YPCORE02     0       0        1       1
> 3 ALCA0001 YPCORE03     0       0        1       1
> 4 ALCA0001 YPCORE04     0       0        1       1
> 5 ALCA0001 YPCORE05     0       0        1       1
> 6 ALCA0001 YPCORE06     1       0        1       1
> > table(longDat$missRand)
>
>    0    1
> 1140 1230
> > table(longDat$missReg)
>
>    0    1
> 2160  210
> > library(lme4)
> > fitRand <- glmer(missRand ~ Item + (1 | ID), family = "binomial", data =
> longDat)
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 5.46528 (tol = 0.002,
> component 1)
> > anova(fitRand)
> Analysis of Variance Table
>      npar   Sum Sq   Mean Sq F value
> Item    9 0.050372 0.0055969  0.0056
> > length(unique(longDat$ID))
> [1] 237
> > rm(fitRand)
> > anova(fitRand)
> Error in anova(fitRand) : object 'fitRand' not found
> > fitRand <- glmer(missRand ~ Item + (1 | ID), family = "binomial", data =
> longDat)
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 5.46528 (tol = 0.002,
> component 1)
> > fitRand <- glmer(missRand ~ Item + (1 | ID), family = binomial, data =
> longDat)
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 5.46528 (tol = 0.002,
> component 1)
> > fitRand <- glmer(missRand ~ Item + (1 | ID), family = binomial(link =
> "logit"), data = longDat)
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 5.46528 (tol = 0.002,
> component 1)
> > ?glmer
> > data("cbpp")
> > dim(cbpp)
> [1] 56  4
> > head(cbpp)
>   herd incidence size period
> 1    1         2   14      1
> 2    1         3   12      2
> 3    1         4    9      3
> 4    1         0    5      4
> 5    2         3   22      1
> 6    2         1   18      2
> > fitRand <- glmer(cbind(missRand, 1) ~ Item + (1 | ID), family =
> binomial(link = "logit"), data = longDat)
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 0.00550177 (tol = 0.002,
> component 1)
> > fitReg <- glmer(cbind(missReg, 1) ~ Item + (1 | ID), family =
> "binomial", data = longDat)
> Warning messages:
> 1: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,
> :
>   failure to converge in 10000 evaluations
> 2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 1.47499 (tol = 0.002,
> component 1)
> > fitReal <- glmer(cbind(missing, 1) ~ Item + (1 | ID), family =
> "binomial", data = longDat)
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 1.29013 (tol = 0.002,
> component 1)
> > data(mlmdata)
> Warning message:
> In data(mlmdata) : data set ?mlmdata? not found
> > library(haven)
> > mlmdata <- read_dta("
> https://stats.idre.ucla.edu/stat/examples/imm/imm10.dta")
> > head(mlmdata)
> # A tibble: 6 x 19
>   schid stuid    ses meanses homework white parented public ratio percmin
> math   sex  race sctype  cstr scsize urban
>   <dbl> <dbl>  <dbl>   <dbl>    <dbl> <dbl>    <dbl>  <dbl> <dbl>   <dbl>
> <dbl> <dbl> <dbl>  <dbl> <dbl>  <dbl> <dbl>
> 1  7472     3 -0.130  -0.483        1     1        2      1    19       0
>   48     2     4      1     2      3     2
> 2  7472     8 -0.390  -0.483        0     1        2      1    19       0
>   48     1     4      1     2      3     2
> 3  7472    13 -0.800  -0.483        0     1        2      1    19       0
>   53     1     4      1     2      3     2
> 4  7472    17 -0.720  -0.483        1     1        2      1    19       0
>   42     1     4      1     2      3     2
> 5  7472    27 -0.740  -0.483        2     1        2      1    19       0
>   43     2     4      1     2      3     2
> 6  7472    28 -0.580  -0.483        1     1        2      1    19       0
>   57     2     4      1     2      3     2
> # ? with 2 more variables: region <dbl>, schnum <dbl>
> > model <- glmer(white ~ homework + (1 + homework | schid), data=mlmdata,
> +                family=binomial(link="logit"))
> boundary (singular) fit: see ?isSingular
> > summary(model)
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: white ~ homework + (1 + homework | schid)
>    Data: mlmdata
>
>      AIC      BIC   logLik deviance df.resid
>    182.4    200.2    -86.2    172.4      255
>
> Scaled residuals:
>     Min      1Q  Median      3Q     Max
> -4.3373 -0.1184  0.1112  0.3421  3.8801
>
> Random effects:
>  Groups Name        Variance Std.Dev. Corr
>  schid  (Intercept) 16.28745 4.0358
>         homework     0.04678 0.2163   -1.00
> Number of obs: 260, groups:  schid, 10
>
> Fixed effects:
>             Estimate Std. Error z value Pr(>|z|)
> (Intercept)  1.67365    1.47324   1.136    0.256
> homework     0.04508    0.18433   0.245    0.807
>
> Correlation of Fixed Effects:
>          (Intr)
> homework -0.590
> convergence code: 0
> boundary (singular) fit: see ?isSingular
>
> > dim(mlmdata)
> [1] 260  19
> > head(mlmdata)
> # A tibble: 6 x 19
>   schid stuid    ses meanses homework white parented public ratio percmin
> math   sex  race sctype  cstr scsize urban
>   <dbl> <dbl>  <dbl>   <dbl>    <dbl> <dbl>    <dbl>  <dbl> <dbl>   <dbl>
> <dbl> <dbl> <dbl>  <dbl> <dbl>  <dbl> <dbl>
> 1  7472     3 -0.130  -0.483        1     1        2      1    19       0
>   48     2     4      1     2      3     2
> 2  7472     8 -0.390  -0.483        0     1        2      1    19       0
>   48     1     4      1     2      3     2
> 3  7472    13 -0.800  -0.483        0     1        2      1    19       0
>   53     1     4      1     2      3     2
> 4  7472    17 -0.720  -0.483        1     1        2      1    19       0
>   42     1     4      1     2      3     2
> 5  7472    27 -0.740  -0.483        2     1        2      1    19       0
>   43     2     4      1     2      3     2
> 6  7472    28 -0.580  -0.483        1     1        2      1    19       0
>   57     2     4      1     2      3     2
> # ? with 2 more variables: region <dbl>, schnum <dbl>
> > length(unique(mlmdata$schid))
> [1] 10
> > table(white)
> Error in table(white) : object 'white' not found
> > table(mlmdata$white)
>
>   0   1
>  71 189
> > fitRand <- glmer(missRand ~ Item + (1 | ID), family = binomial(link =
> "logit"), data = longDat)
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 5.46528 (tol = 0.002,
> component 1)
> > ,
> Error: unexpected ',' in ","
> > longDat %>%
> +   mutate(missRand = rbinom(1, 1, .5),
> +          missReg = rbinom(1, 1, as.numeric(factor(Item))/11),
> +          riskItem = if_else(Item == "YPCORE04", 1, 0),
> +          posItem = if_else(Item %in% c("YPCORE03", "YPCORE5",
> "YPCORE10"), 1, 0)) -> longDat
> > head(longDat)
> # A tibble: 6 x 8
> # Groups:   ID [1]
>   ID       Item     Score missing missRand missReg riskItem posItem
>   <chr>    <chr>    <dbl>   <dbl>    <int>   <int>    <dbl>   <dbl>
> 1 ALCA0001 YPCORE01     1       0        0       0        0       0
> 2 ALCA0001 YPCORE02     0       0        0       0        0       0
> 3 ALCA0001 YPCORE03     0       0        0       0        0       1
> 4 ALCA0001 YPCORE04     0       0        0       0        1       0
> 5 ALCA0001 YPCORE05     0       0        0       0        0       0
> 6 ALCA0001 YPCORE06     1       0        0       0        0       0
> > table(longDat$riskItem, longDat$posItem)
>
>        0    1
>   0 1659  474
>   1  237    0
> > table(longDat$riskItem, longDat$Item)
>
>     YPCORE01 YPCORE02 YPCORE03 YPCORE04 YPCORE05 YPCORE06 YPCORE07
> YPCORE08 YPCORE09 YPCORE10
>   0      237      237      237        0      237      237      237
> 237      237      237
>   1        0        0        0      237        0        0        0
> 0        0        0
> > table(longDat$posItem, longDat$Item)
>
>     YPCORE01 YPCORE02 YPCORE03 YPCORE04 YPCORE05 YPCORE06 YPCORE07
> YPCORE08 YPCORE09 YPCORE10
>   0      237      237        0      237      237      237      237
> 237      237        0
>   1        0        0      237        0        0        0        0
> 0        0      237
> > longDat %>%
> +   mutate(missRand = rbinom(1, 1, .5),
> +          missReg = rbinom(1, 1, as.numeric(factor(Item))/11),
> +          riskItem = if_else(Item == "YPCORE04", 1, 0),
> +          posItem = if_else(Item %in% c("YPCORE03", "YPCORE05",
> "YPCORE10"), 1, 0)) -> longDat
> > head(longDat)
> # A tibble: 6 x 8
> # Groups:   ID [1]
>   ID       Item     Score missing missRand missReg riskItem posItem
>   <chr>    <chr>    <dbl>   <dbl>    <int>   <int>    <dbl>   <dbl>
> 1 ALCA0001 YPCORE01     1       0        0       0        0       0
> 2 ALCA0001 YPCORE02     0       0        0       0        0       0
> 3 ALCA0001 YPCORE03     0       0        0       0        0       1
> 4 ALCA0001 YPCORE04     0       0        0       0        1       0
> 5 ALCA0001 YPCORE05     0       0        0       0        0       1
> 6 ALCA0001 YPCORE06     1       0        0       0        0       0
> > table(longDat$posItem, longDat$Item)
>
>     YPCORE01 YPCORE02 YPCORE03 YPCORE04 YPCORE05 YPCORE06 YPCORE07
> YPCORE08 YPCORE09 YPCORE10
>   0      237      237        0      237        0      237      237
> 237      237        0
>   1        0        0      237        0      237        0        0
> 0        0      237
> > fitRisk <- glmer(missing ~ riskItem + (1 | ID), family = binomial, data
> = longDat)
> > summary(fitRisk)
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: missing ~ riskItem + (1 | ID)
>    Data: longDat
>
>      AIC      BIC   logLik deviance df.resid
>     51.7     69.0    -22.9     45.7     2367
>
> Scaled residuals:
>      Min       1Q   Median       3Q      Max
> -0.52020 -0.00113 -0.00113 -0.00113  2.93829
>
> Random effects:
>  Groups Name        Variance Std.Dev.
>  ID     (Intercept) 205.3    14.33
> Number of obs: 2370, groups:  ID, 237
>
> Fixed effects:
>             Estimate Std. Error z value Pr(>|z|)
> (Intercept)  -13.563      2.540  -5.339 9.33e-08 ***
> riskItem      -2.418      3.745  -0.646    0.518
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> Correlation of Fixed Effects:
>          (Intr)
> riskItem 0.064
> > fitRisk <- glmer(missing ~ posItem + (1 | ID), family = binomial, data =
> longDat)
> > fitRisk <- glmer(missing ~ riskItem + (1 | ID), family = binomial, data
> = longDat)
> > summary(fitRisk)
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: missing ~ riskItem + (1 | ID)
>    Data: longDat
>
>      AIC      BIC   logLik deviance df.resid
>     51.7     69.0    -22.9     45.7     2367
>
> Scaled residuals:
>      Min       1Q   Median       3Q      Max
> -0.52020 -0.00113 -0.00113 -0.00113  2.93829
>
> Random effects:
>  Groups Name        Variance Std.Dev.
>  ID     (Intercept) 205.3    14.33
> Number of obs: 2370, groups:  ID, 237
>
> Fixed effects:
>             Estimate Std. Error z value Pr(>|z|)
> (Intercept)  -13.563      2.540  -5.339 9.33e-08 ***
> riskItem      -2.418      3.745  -0.646    0.518
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> Correlation of Fixed Effects:
>          (Intr)
> riskItem 0.064
> > fitPos <- glmer(missing ~ posItem + (1 | ID), family = binomial, data =
> longDat)
> > summary(fitPos)
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: missing ~ posItem + (1 | ID)
>    Data: longDat
>
>      AIC      BIC   logLik deviance df.resid
>     52.4     69.7    -23.2     46.4     2367
>
> Scaled residuals:
>     Min      1Q  Median      3Q     Max
> -0.5111 -0.0012 -0.0012 -0.0010  3.4498
>
> Random effects:
>  Groups Name        Variance Std.Dev.
>  ID     (Intercept) 197.5    14.05
> Number of obs: 2370, groups:  ID, 237
>
> Fixed effects:
>             Estimate Std. Error z value Pr(>|z|)
> (Intercept) -13.5152     2.5280  -5.346 8.98e-08 ***
> posItem      -0.2953     1.2396  -0.238    0.812
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> Correlation of Fixed Effects:
>         (Intr)
> posItem -0.110
> > head(longDat,15)
> # A tibble: 15 x 8
> # Groups:   ID [2]
>    ID       Item     Score missing missRand missReg riskItem posItem
>    <chr>    <chr>    <dbl>   <dbl>    <int>   <int>    <dbl>   <dbl>
>  1 ALCA0001 YPCORE01     1       0        0       0        0       0
>  2 ALCA0001 YPCORE02     0       0        0       0        0       0
>  3 ALCA0001 YPCORE03     0       0        0       0        0       1
>  4 ALCA0001 YPCORE04     0       0        0       0        1       0
>  5 ALCA0001 YPCORE05     0       0        0       0        0       1
>  6 ALCA0001 YPCORE06     1       0        0       0        0       0
>  7 ALCA0001 YPCORE07     0       0        0       0        0       0
>  8 ALCA0001 YPCORE08     0       0        0       0        0       0
>  9 ALCA0001 YPCORE09     0       0        0       0        0       0
> 10 ALCA0001 YPCORE10     0       0        0       0        0       1
> 11 ALCA0004 YPCORE01     1       0        0       0        0       0
> 12 ALCA0004 YPCORE02     0       0        0       0        0       0
> 13 ALCA0004 YPCORE03     0       0        0       0        0       1
> 14 ALCA0004 YPCORE04     0       0        0       0        1       0
> 15 ALCA0004 YPCORE05     0       0        0       0        0       1
> >
> >
> > rm(fitRand)
> > rm(fitReg)
> > rm(fitReal)
> > fitRand <- glmer(cbind(missRand, 1) ~ Item + (1 | ID), family =
> binomial(link = "logit"), data = longDat)
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 0.0501701 (tol = 0.002,
> component 1)
> > fitRand
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: cbind(missRand, 1) ~ Item + (1 | ID)
>    Data: longDat
>       AIC       BIC    logLik  deviance  df.resid
>  2215.017  2278.494 -1096.509  2193.017      2359
> Random effects:
>  Groups Name        Std.Dev.
>  ID     (Intercept) 2.509
> Number of obs: 2370, groups:  ID, 237
> Fixed Effects:
>  (Intercept)  ItemYPCORE02  ItemYPCORE03  ItemYPCORE04  ItemYPCORE05
> ItemYPCORE06  ItemYPCORE07  ItemYPCORE08
>    -2.512456      0.004360      0.007166      0.001288      0.004078
> 0.002322      0.005059      0.006118
> ItemYPCORE09  ItemYPCORE10
>     0.006454      0.005057
> convergence code 0; 0 optimizer warnings; 1 lme4 warnings
>
> So failed to converge but does give a set of estimates.  I think I can use
> the
>
>
> > fitReg <- glmer(cbind(missReg, 1) ~ Item + (1 | ID), family =
> "binomial", data = longDat)
> Warning messages:
> 1: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,
> :
>   failure to converge in 10000 evaluations
> 2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 1.66664 (tol = 0.002,
> component 1)
> > fitReg
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: cbind(missReg, 1) ~ Item + (1 | ID)
>    Data: longDat
>       AIC       BIC    logLik  deviance  df.resid
>  605.3404  668.8175 -291.6702  583.3404      2359
> Random effects:
>  Groups Name        Std.Dev.
>  ID     (Intercept) 6.775
> Number of obs: 2370, groups:  ID, 237
> Fixed Effects:
>  (Intercept)  ItemYPCORE02  ItemYPCORE03  ItemYPCORE04  ItemYPCORE05
> ItemYPCORE06  ItemYPCORE07  ItemYPCORE08
>      -8.6607       -0.3793       -0.3159       -0.3591       -0.4608
>  -0.4453       -0.4165       -0.6597
> ItemYPCORE09  ItemYPCORE10
>      -0.4304       -0.3535
> convergence code 0; 1 optimizer warnings; 1 lme4 warnings
>
> I am starting to realise the depths of my ignorance about all this.  It's
> clear to me that having only one observation per cell
> because of the nesting of items within participants and only having one
> completion per participant is causing the convergence
> issues (which makes sense though I have a sense that there might be ways
> to extract estimates despite this: this is beyond me!)
>
> I'm puzzled that I get slightly different results for the risk analysis if
> I use the "cbind(missing, 1) ~ " syntax in the formula
> from those I get using just "missing ~ " and different max|grad values
> from the nonconvergence message depending on that choice.
> I suspect that's a red herring.
>
> I have seen many comments here recently about non-convergence and ways to
> overcome it by specifying different methods of
> approximation (if I understood that properly) but they generally seemed to
> be for much more complex models than mine and
> probably weren't about this "cell size = 1" issue.  I have searched for
> answer or illumination for some hours and perhaps
> I'm asking the wrong questions but I'm stuck.  (I think it's very unlikely
> that I have access to trained statisticians by
> virtue of my honorary or paid academic positions!)
>
> So my questions:
> 1) _is_ there a way to estimate such a model, i.e. estimating an effect
> across all ten items with only one completion per participant?
> 2) can someone suggest good reading matter for a dogged non-statistician
> who can handle a lot of continuous variable issues up to
> some real complexity on his own but has never been as comfortable with
> counts beyond the basic old NHST, single level approaches?  I
> have an old copy of Pinheiro and Bates but confess that despite trying
> many times, the older it and I get, the less I'm able to cope
> with it.  The algebra is just beyond me.
>
> TIA ... and best wishes to all for physical safety and psychological
> resilience in these times,
>
> Chris
>
> --
> Small contribution in our coronavirus rigours:
>
> https://www.coresystemtrust.org.uk/home/free-options-to-replace-paper-core-forms-during-the-coronavirus-pandemic/
>
> Chris Evans <chris at psyctc.org> Visiting Professor, University of
> Sheffield <chris.evans at sheffield.ac.uk>
> I do some consultation work for the University of Roehampton <
> chris.evans at roehampton.ac.uk> and other places
> but <chris at psyctc.org> remains my main Email address.  I have a work web
> site at:
>    https://www.psyctc.org/psyctc/
> and a site I manage for CORE and CORE system trust at:
>    http://www.coresystemtrust.org.uk/
> I have "semigrated" to France, see:
>    https://www.psyctc.org/pelerinage2016/semigrating-to-france/
>
> https://www.psyctc.org/pelerinage2016/register-to-get-updates-from-pelerinage2016/
>
> If you want an Emeeting, I am trying to keep them to Thursdays and my
> diary is at:
>    https://www.psyctc.org/pelerinage2016/ceworkdiary/
> Beware: French time, generally an hour ahead of UK.
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From q@@hoe@yb @end|ng |rom gm@||@com  Sun May  3 18:18:15 2020
From: q@@hoe@yb @end|ng |rom gm@||@com (Shoeayb Qasemi)
Date: Sun, 3 May 2020 20:48:15 +0430
Subject: [R-sig-ME] MLE for pretest-posttest design
Message-ID: <CADRwXO+gB-pNQQ=vA8eVSwgDGCV8cAWkLkhHOFVg45chLeckiA@mail.gmail.com>

Hi All,

I have data from pre-post design. The necessary assumption of ANCOVA,
homogeniety of slopes, is not satisfied. I tried the following model
as an alternative.
#variables: group(0=control, 1=experimental); time(0=pretest, 1=posttest);
mod1<-  lmer(score ~ group*time + (1+time|id), data=dat1)

My questions are:
 1. Is the model correctly specified?

2. Which effect tests the effectiveness of treatment?

Thanks in advance,

Shoeayb

	[[alternative HTML version deleted]]


From Gr|@en|@@M@to@ @end|ng |rom utd@||@@@edu  Fri May  1 08:12:31 2020
From: Gr|@en|@@M@to@ @end|ng |rom utd@||@@@edu (Matos, Grisenia)
Date: Fri, 1 May 2020 06:12:31 +0000
Subject: [R-sig-ME] vif using GLMMadaptive
Message-ID: <DM6PR01MB3724C8B9CDC58C51C09A977BF0AB0@DM6PR01MB3724.prod.exchangelabs.com>

I am a PhD student and am working on a school  project due over the weekend.  I ran the following regression:
spending.REG <- glm.nb(spending_count ~ conservative + liberal + moderate + trust_gov + liberal*trust_gov + conservative*trust_gov + moderate*trust_gov + income + education + age + female + white + budget_difficult + democrat + republican, data = Trustdata1)

I attempted to get a vif score and got this error in R Studio: there are aliased coefficients in the model

The variables conservative, liberal and moderate are fixed effect where they are either 0 or 1.  The female variable is a 0 or 1.  There are three interactive variables:  moderate*trust_gov, liberal*trust_gov, and conservative*trust_gov.  Moreover, moderate and moderate*trust_gov are the base variables.

I would like to calculate the vif for the regression equation.  First, in order to get rid of the error and thereafter calculate the vif scores, I attempted to use your code:

library("GLMMadaptive")

fm <- mixed_model(y ~ time + sex, random = ~ 1 | id, data = <your_data>,
family = zi.negative.binomial(), zi_fixed = ~ sex, zi_random = ~ 1 | id)

it returned an error: unexpected '=' in: "erate + trust_gov + liberal*trust_gov + conservative*trust_gov + moderate*trust_gov + income + education + age + female + white + budget_difficult + democrat + republican, random = 1 | id, dat
                  +                   family ="

Please provide guidance as to what I am doing incorrectly.  I appreciate your help.

Thanks,

Grisenia





	[[alternative HTML version deleted]]


From j|ox @end|ng |rom mcm@@ter@c@  Sun May  3 22:24:03 2020
From: j|ox @end|ng |rom mcm@@ter@c@ (Fox, John)
Date: Sun, 3 May 2020 20:24:03 +0000
Subject: [R-sig-ME] vif using GLMMadaptive
In-Reply-To: <29568_1588528916_043I1n7c016794_DM6PR01MB3724C8B9CDC58C51C09A977BF0AB0@DM6PR01MB3724.prod.exchangelabs.com>
References: <29568_1588528916_043I1n7c016794_DM6PR01MB3724C8B9CDC58C51C09A977BF0AB0@DM6PR01MB3724.prod.exchangelabs.com>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC88CE809D2@FHSDB2D11-2.csu.mcmaster.ca>

Dear Grisenia,

> -----Original Message-----
> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On
> Behalf Of Matos, Grisenia
> Sent: Friday, May 1, 2020 2:13 AM
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] vif using GLMMadaptive
> 
> I am a PhD student and am working on a school  project due over the
> weekend.  I ran the following regression:
> spending.REG <- glm.nb(spending_count ~ conservative + liberal + moderate
> + trust_gov + liberal*trust_gov + conservative*trust_gov +
> moderate*trust_gov + income + education + age + female + white +
> budget_difficult + democrat + republican, data = Trustdata1)
> 
> I attempted to get a vif score and got this error in R Studio: there are
> aliased coefficients in the model

I guess that you're using the vif() function in the car package (there are, I believe, other implementations as well). The reference to RStudio is irrelevant -- you'd almost surely get the same error regardless of the which programming editor you use with R.

Aliased coefficients imply that there's either perfect collinearity among the regressors in your model or a flat likelihood at the maxium, making some coefficients unidentified. In this situation, at least some of the VIFs are infinite.

In addition, the meaning of VIFs in models with interactions is ambiguous --- subject to variation depending upon inessential changes to the model, such as how contrasts for factors like sex are defined. To understand the problem, see the Fox and Monette paper referenced in ?vif. 

I hope this helps,
 John

--------------------------------------
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
Web: socialsciences.mcmaster.ca/jfox/


> 
> The variables conservative, liberal and moderate are fixed effect where
> they are either 0 or 1.  The female variable is a 0 or 1.  There are three
> interactive variables:  moderate*trust_gov, liberal*trust_gov, and
> conservative*trust_gov.  Moreover, moderate and moderate*trust_gov are the
> base variables.
> 
> I would like to calculate the vif for the regression equation.  First, in
> order to get rid of the error and thereafter calculate the vif scores, I
> attempted to use your code:
> 
> library("GLMMadaptive")
> 
> fm <- mixed_model(y ~ time + sex, random = ~ 1 | id, data = <your_data>,
> family = zi.negative.binomial(), zi_fixed = ~ sex, zi_random = ~ 1 | id)
> 
> it returned an error: unexpected '=' in: "erate + trust_gov +
> liberal*trust_gov + conservative*trust_gov + moderate*trust_gov + income +
> education + age + female + white + budget_difficult + democrat +
> republican, random = 1 | id, dat
>                   +                   family ="
> 
> Please provide guidance as to what I am doing incorrectly.  I appreciate
> your help.
> 
> Thanks,
> 
> Grisenia
> 
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From Tom_Ph|||pp| @end|ng |rom np@@gov  Sun May  3 22:30:21 2020
From: Tom_Ph|||pp| @end|ng |rom np@@gov (Philippi, Tom)
Date: Sun, 3 May 2020 20:30:21 +0000
Subject: [R-sig-ME] [EXTERNAL]  vif using GLMMadaptive
In-Reply-To: <DM6PR01MB3724C8B9CDC58C51C09A977BF0AB0@DM6PR01MB3724.prod.exchangelabs.com>
References: <DM6PR01MB3724C8B9CDC58C51C09A977BF0AB0@DM6PR01MB3724.prod.exchangelabs.com>
Message-ID: <MW2PR0901MB3754CB3EDC4E0887762934A0F3A90@MW2PR0901MB3754.namprd09.prod.outlook.com>

At a more concrete level than John Fox's reply:

You do not show your exact code that produced the syntax error on family =, but my guess is that you're missing a comma, perhaps after the dataset name and before family, perhaps elsewhere.

More fundamentally, do your indicator variables conservative + liberal + moderate sum to 1, or is there another category that would have 0,0,0 for those three variables?  If they sum to 1, your model is attempting to fit 3 parameters with only 2 degrees of freedom.  Even if there is a 4th category coding as 0,0,0, if that category has a very low frequency the VIF would be high just from these variables.

And, looking at your model, you also have republican and democrat.  Again, do they sum to 1, or is there a substantial independent fraction that codes as 0,0 for those 2 variables?

Whatever you code as republican v democrat is very likely strongly associated with conservative v liberal.  To fit your model you would need substantial numbers of liberal republicans and conservative democrats.  As John wrote, VIF isn't that useful or interpretable given your interactions.  I suggest creating a crosstabulation of {conservative liberal moderate} vs {republican democrat}, and if all cells aren't filled with a reasonable number of observations, think about which of those 2 factors you want to include in the model: you can't obtain estimates if you include both.

Tom

-----Original Message-----
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of Matos, Grisenia
Sent: Thursday, April 30, 2020 11:13 PM
To: r-sig-mixed-models at r-project.org
Subject: [EXTERNAL] [R-sig-ME] vif using GLMMadaptive

I am a PhD student and am working on a school  project due over the weekend.  I ran the following regression:
spending.REG <- glm.nb(spending_count ~ conservative + liberal + moderate + trust_gov + liberal*trust_gov + conservative*trust_gov + moderate*trust_gov + income + education + age + female + white + budget_difficult + democrat + republican, data = Trustdata1)

I attempted to get a vif score and got this error in R Studio: there are aliased coefficients in the model

The variables conservative, liberal and moderate are fixed effect where they are either 0 or 1.  The female variable is a 0 or 1.  There are three interactive variables:  moderate*trust_gov, liberal*trust_gov, and conservative*trust_gov.  Moreover, moderate and moderate*trust_gov are the base variables.

I would like to calculate the vif for the regression equation.  First, in order to get rid of the error and thereafter calculate the vif scores, I attempted to use your code:

library("GLMMadaptive")

fm <- mixed_model(y ~ time + sex, random = ~ 1 | id, data = <your_data>, family = zi.negative.binomial(), zi_fixed = ~ sex, zi_random = ~ 1 | id)

it returned an error: unexpected '=' in: "erate + trust_gov + liberal*trust_gov + conservative*trust_gov + moderate*trust_gov + income + education + age + female + white + budget_difficult + democrat + republican, random = 1 | id, dat
                  +                   family ="

Please provide guidance as to what I am doing incorrectly.  I appreciate your help.

Thanks,

Grisenia





	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From bbo|ker @end|ng |rom gm@||@com  Sun May  3 22:34:18 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sun, 3 May 2020 16:34:18 -0400
Subject: [R-sig-ME] vif using GLMMadaptive
In-Reply-To: <DM6PR01MB3724C8B9CDC58C51C09A977BF0AB0@DM6PR01MB3724.prod.exchangelabs.com>
References: <DM6PR01MB3724C8B9CDC58C51C09A977BF0AB0@DM6PR01MB3724.prod.exchangelabs.com>
Message-ID: <cc8358fd-2079-1ed8-ef72-aa5cc8fa4880@gmail.com>

 ??? My advice would be to sort out the underlying problem (which is 
*not* at all related to mixed models) ? and not move to mixed models in 
the hope that that will fix something.? I believe (but am not sure) that 
you're using vif() from the car package?

 ?? * you may be able to un-alias your variables by eliminating the 
'main effect' terms in your model;? in R formula syntax,? A*B is 
equivalent to? 1+A+B+A:B.? So dropping the main effects that are already 
included in the * terms, or switching from * to : for interactions, may 
solve your problem.

On 5/1/20 2:12 AM, Matos, Grisenia wrote:
> I am a PhD student and am working on a school  project due over the weekend.  I ran the following regression:
> spending.REG <- glm.nb(spending_count ~ conservative + liberal + moderate + trust_gov + liberal*trust_gov + conservative*trust_gov + moderate*trust_gov + income + education + age + female + white + budget_difficult + democrat + republican, data = Trustdata1)
>
> I attempted to get a vif score and got this error in R Studio: there are aliased coefficients in the model
>
> The variables conservative, liberal and moderate are fixed effect where they are either 0 or 1.  The female variable is a 0 or 1.  There are three interactive variables:  moderate*trust_gov, liberal*trust_gov, and conservative*trust_gov.  Moreover, moderate and moderate*trust_gov are the base variables.
>
> I would like to calculate the vif for the regression equation.  First, in order to get rid of the error and thereafter calculate the vif scores, I attempted to use your code:
>
> library("GLMMadaptive")
>
> fm <- mixed_model(y ~ time + sex, random = ~ 1 | id, data = <your_data>,
> family = zi.negative.binomial(), zi_fixed = ~ sex, zi_random = ~ 1 | id)
>
> it returned an error: unexpected '=' in: "erate + trust_gov + liberal*trust_gov + conservative*trust_gov + moderate*trust_gov + income + education + age + female + white + budget_difficult + democrat + republican, random = 1 | id, dat
>                    +                   family ="
>
> Please provide guidance as to what I am doing incorrectly.  I appreciate your help.
>
> Thanks,
>
> Grisenia
>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From j|ox @end|ng |rom mcm@@ter@c@  Mon May  4 00:00:27 2020
From: j|ox @end|ng |rom mcm@@ter@c@ (Fox, John)
Date: Sun, 3 May 2020 22:00:27 +0000
Subject: [R-sig-ME] vif using GLMMadaptive
In-Reply-To: <21843_1588538075_043KYY3v008791_cc8358fd-2079-1ed8-ef72-aa5cc8fa4880@gmail.com>
References: <DM6PR01MB3724C8B9CDC58C51C09A977BF0AB0@DM6PR01MB3724.prod.exchangelabs.com>
 <21843_1588538075_043KYY3v008791_cc8358fd-2079-1ed8-ef72-aa5cc8fa4880@gmail.com>
Message-ID: <4D33D188-5752-4384-96BD-7D0FC9E3E958@mcmaster.ca>

Hi Ben,

> On May 3, 2020, at 4:34 PM, Ben Bolker <bbolker at gmail.com> wrote:
> 
>     My advice would be to sort out the underlying problem (which is *not* at all related to mixed models)   and not move to mixed models in the hope that that will fix something.  I believe (but am not sure) that you're using vif() from the car package?
> 
>    * you may be able to un-alias your variables by eliminating the 'main effect' terms in your model;  in R formula syntax,  A*B is equivalent to  1+A+B+A:B.  So dropping the main effects that are already included in the * terms, or switching from * to : for interactions, may solve your problem.

I don't think so: That is, when the formula is processed, the redundancies introduced by the mistaken use of * should automatically be accounted for, as in the following:

> D <- data.frame(a=factor(rep(c("a", "b"), 2)), b=factor(rep(c("A", "B"), each=2)))
> D
  a b
1 a A
2 b A
3 a B
4 b B

> model.matrix(~1 + a + b + a*b, data=D)
  (Intercept) ab bB ab:bB
1           1  0  0     0
2           1  1  0     0
3           1  0  1     0
4           1  1  1     1
attr(,"assign")
[1] 0 1 2 3
attr(,"contrasts")
attr(,"contrasts")$a
[1] "contr.treatment"

attr(,"contrasts")$b
[1] "contr.treatment"

The problem is probably more fundamental, as has already been suggested.

Best,
 John

> 
> On 5/1/20 2:12 AM, Matos, Grisenia wrote:
>> I am a PhD student and am working on a school  project due over the weekend.  I ran the following regression:
>> spending.REG <- glm.nb(spending_count ~ conservative + liberal + moderate + trust_gov + liberal*trust_gov + conservative*trust_gov + moderate*trust_gov + income + education + age + female + white + budget_difficult + democrat + republican, data = Trustdata1)
>> 
>> I attempted to get a vif score and got this error in R Studio: there are aliased coefficients in the model
>> 
>> The variables conservative, liberal and moderate are fixed effect where they are either 0 or 1.  The female variable is a 0 or 1.  There are three interactive variables:  moderate*trust_gov, liberal*trust_gov, and conservative*trust_gov.  Moreover, moderate and moderate*trust_gov are the base variables.
>> 
>> I would like to calculate the vif for the regression equation.  First, in order to get rid of the error and thereafter calculate the vif scores, I attempted to use your code:
>> 
>> library("GLMMadaptive")
>> 
>> fm <- mixed_model(y ~ time + sex, random = ~ 1 | id, data = <your_data>,
>> family = zi.negative.binomial(), zi_fixed = ~ sex, zi_random = ~ 1 | id)
>> 
>> it returned an error: unexpected '=' in: "erate + trust_gov + liberal*trust_gov + conservative*trust_gov + moderate*trust_gov + income + education + age + female + white + budget_difficult + democrat + republican, random = 1 | id, dat
>>                   +                   family ="
>> 
>> Please provide guidance as to what I am doing incorrectly.  I appreciate your help.
>> 
>> Thanks,
>> 
>> Grisenia
>> 
>> 
>> 
>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From bbo|ker @end|ng |rom gm@||@com  Mon May  4 00:04:04 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sun, 3 May 2020 18:04:04 -0400
Subject: [R-sig-ME] vif using GLMMadaptive
In-Reply-To: <4D33D188-5752-4384-96BD-7D0FC9E3E958@mcmaster.ca>
References: <DM6PR01MB3724C8B9CDC58C51C09A977BF0AB0@DM6PR01MB3724.prod.exchangelabs.com>
 <21843_1588538075_043KYY3v008791_cc8358fd-2079-1ed8-ef72-aa5cc8fa4880@gmail.com>
 <4D33D188-5752-4384-96BD-7D0FC9E3E958@mcmaster.ca>
Message-ID: <24584428-e21c-f068-0347-a69f09b66c62@gmail.com>

 ?? Yes.? I misunderstood the issue, and particularly the meaning in 
this context of 'aliased' (which has now been well explained by you and 
Tom Philippi).? I would still stand by my first sentence ...

 ??? cheers

 ??? Ben Bolker

On 5/3/20 6:00 PM, Fox, John wrote:
> Hi Ben,
>
>> On May 3, 2020, at 4:34 PM, Ben Bolker <bbolker at gmail.com> wrote:
>>
>>      My advice would be to sort out the underlying problem (which is *not* at all related to mixed models)   and not move to mixed models in the hope that that will fix something.  I believe (but am not sure) that you're using vif() from the car package?
>>
>>     * you may be able to un-alias your variables by eliminating the 'main effect' terms in your model;  in R formula syntax,  A*B is equivalent to  1+A+B+A:B.  So dropping the main effects that are already included in the * terms, or switching from * to : for interactions, may solve your problem.
> I don't think so: That is, when the formula is processed, the redundancies introduced by the mistaken use of * should automatically be accounted for, as in the following:
>
>> D <- data.frame(a=factor(rep(c("a", "b"), 2)), b=factor(rep(c("A", "B"), each=2)))
>> D
>    a b
> 1 a A
> 2 b A
> 3 a B
> 4 b B
>
>> model.matrix(~1 + a + b + a*b, data=D)
>    (Intercept) ab bB ab:bB
> 1           1  0  0     0
> 2           1  1  0     0
> 3           1  0  1     0
> 4           1  1  1     1
> attr(,"assign")
> [1] 0 1 2 3
> attr(,"contrasts")
> attr(,"contrasts")$a
> [1] "contr.treatment"
>
> attr(,"contrasts")$b
> [1] "contr.treatment"
>
> The problem is probably more fundamental, as has already been suggested.
>
> Best,
>   John
>
>> On 5/1/20 2:12 AM, Matos, Grisenia wrote:
>>> I am a PhD student and am working on a school  project due over the weekend.  I ran the following regression:
>>> spending.REG <- glm.nb(spending_count ~ conservative + liberal + moderate + trust_gov + liberal*trust_gov + conservative*trust_gov + moderate*trust_gov + income + education + age + female + white + budget_difficult + democrat + republican, data = Trustdata1)
>>>
>>> I attempted to get a vif score and got this error in R Studio: there are aliased coefficients in the model
>>>
>>> The variables conservative, liberal and moderate are fixed effect where they are either 0 or 1.  The female variable is a 0 or 1.  There are three interactive variables:  moderate*trust_gov, liberal*trust_gov, and conservative*trust_gov.  Moreover, moderate and moderate*trust_gov are the base variables.
>>>
>>> I would like to calculate the vif for the regression equation.  First, in order to get rid of the error and thereafter calculate the vif scores, I attempted to use your code:
>>>
>>> library("GLMMadaptive")
>>>
>>> fm <- mixed_model(y ~ time + sex, random = ~ 1 | id, data = <your_data>,
>>> family = zi.negative.binomial(), zi_fixed = ~ sex, zi_random = ~ 1 | id)
>>>
>>> it returned an error: unexpected '=' in: "erate + trust_gov + liberal*trust_gov + conservative*trust_gov + moderate*trust_gov + income + education + age + female + white + budget_difficult + democrat + republican, random = 1 | id, dat
>>>                    +                   family ="
>>>
>>> Please provide guidance as to what I am doing incorrectly.  I appreciate your help.
>>>
>>> Thanks,
>>>
>>> Grisenia
>>>
>>>
>>>
>>>
>>>
>>> 	[[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From ph||||p@@|d@y @end|ng |rom mp|@n|  Mon May  4 01:11:27 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Mon, 4 May 2020 01:11:27 +0200
Subject: [R-sig-ME] 
 Struggling (probably problem in wetware not software)
 with multilevel logistic regression
In-Reply-To: <CAG_dBVdk4i7K4naeeiEVgsbfxrm6ScytdoXmqyknjvBOE3sNyg@mail.gmail.com>
References: <100063919.9989289.1588358279505.JavaMail.zimbra@psyctc.org>
 <CAG_dBVdk4i7K4naeeiEVgsbfxrm6ScytdoXmqyknjvBOE3sNyg@mail.gmail.com>
Message-ID: <48545bce-b4b0-392c-b1af-7d6ac32ae669@mpi.nl>

It might make sense to have an item-level random effect, but you'll
probably need to exclude any covariates which are perfectly correlated
with a single item (e.g. risk item for item #4), because it will be very
hard to separate the covariate effect from the item effect.

Phillip

On 2/5/20 2:04 pm, Juho Kristian Ruohonen wrote:
> (reposting my response since the first version failed to include the
> mailing list among the recipients)
>
> Here?s a non-statistician?s take:
>
> You don?t need a multilevel model for this. Given that every respondent
> contributes only one data point, no one?s idiosyncracies are
> overrepresented, and we can expect them to cancel each other out. Your
> question can be modeled using standard logistic regression.
>
> Since you suspect *posItem* may influence the chance of omission, you
> should include it as an additional fixed effect besides *negItem*.
> Something like:
>
> mymodel <- glm(missing ~ posItem + riskItem, family = binomial, data =
> mydata)
>
> It?ll be interesting to hear whether real statisticians disagree.
>
> Best,
>
> J
>
> pe 1. toukok. 2020 klo 21.38 Chris Evans (chrishold at psyctc.org) kirjoitti:
>
>> I am sorry if this is embarrassingly stupid/easy but I am failing to
>> understand something I thought would be fairly easy.
>>
>> The question of interest to me and others is whether people omit some
>> items of a questionnaire more than others.
>> To give a simple example I have several datasets for a ten item
>> self-report questionnaire where one item is about risk
>> and where three items are cued positively and the other seven are cued
>> negatively.
>>
>> Here is an example of some such data:
>>
>>    ID       Item     Score missing missRand missReg riskItem posItem
>>    <chr>    <chr>    <dbl>   <dbl>    <int>   <int>    <dbl>   <dbl>
>>  1 ALCA0001 YPCORE01     1       0        0       0        0       0
>>  2 ALCA0001 YPCORE02     0       0        0       0        0       0
>>  3 ALCA0001 YPCORE03     0       0        0       0        0       1
>>  4 ALCA0001 YPCORE04     0       0        0       0        1       0
>>  5 ALCA0001 YPCORE05     0       0        0       0        0       1
>>  6 ALCA0001 YPCORE06     1       0        0       0        0       0
>>  7 ALCA0001 YPCORE07     0       0        0       0        0       0
>>  8 ALCA0001 YPCORE08     0       0        0       0        0       0
>>  9 ALCA0001 YPCORE09     0       0        0       0        0       0
>> 10 ALCA0001 YPCORE10     0       0        0       0        0       1
>> 11 ALCA0004 YPCORE01     1       0        0       0        0       0
>> 12 ALCA0004 YPCORE02     0       0        0       0        0       0
>> 13 ALCA0004 YPCORE03     0       0        0       0        0       1
>> 14 ALCA0004 YPCORE04     0       0        0       0        1       0
>> 15 ALCA0004 YPCORE05     0       0        0       0        0       1
>>
>> missing is the observed omission of the item (1 = omitted)
>> missRand is a binomial random with probability = .5 I created
>> missReg is a biomial random with probability = item index number / 11,
>> i.e. strong relationship with item
>> riskItem identifies the risk items (item 4)
>> posItem identifies the positively cued items (3, 5 and 10)
>>
>> For this particular dataset there are 237 participants.  (Like Groucho
>> Marks and principles, I have
>> others if you don't like this one!)
>>
>> The catch is that each participant has only completed the measure once.
>> It is a plausible model that
>> participants will vary in their propensity to omit items, it is also
>> highly plausible that participants
>> might omit the risk item more than the other item. It's a question of
>> interest (though I think not of
>> huge interest nor would I bet on it (!) that the positively cued items
>> might be omitted more than the
>> negatively cued ones or v.v.
>>
>> I thought I could test for these effects using glmer().  That works for
>>
>>> fitRisk <- glmer(missing ~ riskItem + (1 | ID), family = binomial, data
>> = longDat)
>>> summary(fitRisk)
>> Generalized linear mixed model fit by maximum likelihood (Laplace
>> Approximation) ['glmerMod']
>>  Family: binomial  ( logit )
>> Formula: missing ~ riskItem + (1 | ID)
>>    Data: longDat
>>
>>      AIC      BIC   logLik deviance df.resid
>>     51.7     69.0    -22.9     45.7     2367
>>
>> Scaled residuals:
>>      Min       1Q   Median       3Q      Max
>> -0.52020 -0.00113 -0.00113 -0.00113  2.93829
>>
>> Random effects:
>>  Groups Name        Variance Std.Dev.
>>  ID     (Intercept) 205.3    14.33
>> Number of obs: 2370, groups:  ID, 237
>>
>> Fixed effects:
>>             Estimate Std. Error z value Pr(>|z|)
>> (Intercept)  -13.563      2.540  -5.339 9.33e-08 ***
>> riskItem      -2.418      3.745  -0.646    0.518
>> ---
>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>
>> Correlation of Fixed Effects:
>>          (Intr)
>> riskItem 0.064
>>
>> No effect but small dataset and test of principle works.  However, when I
>> come to test the overall difference by items I hit
>>
>> 5       NA     NA     NA     NA     NA     NA     NA     NA     NA     NA
>>    NA     NA     NA     NA     NA     NA
>>   BITE23 BITE24 BITE25 BITE26 BITE27 BITE28 BITE29 BITE30 BITE31 BITE32
>> BITE33 Notas origRow Centro2 transfer one
>> 1     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA
>>  NA  <NA>      43  Urgell        0   1
>> 2     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA
>>  NA  <NA>      68  Urgell        0   1
>> 3     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA
>>  NA  <NA>      69  Urgell        0   1
>> 4     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA
>>  NA  <NA>      70  Urgell        0   1
>> 5     NA     NA     NA     NA     NA     NA     NA     NA     NA     NA
>>  NA  <NA>      71  Urgell        0   1
>>   Exclude quaireN ID.quaireN Nquaires  ID.episodeN episodeN nEpisodes
>> Fecha.inicio  Fecha.fin ExpCOREOM ExpSFA ExpSFB
>> 1       0       5 URGE0003-5       27 URGE0003-001        1         1
>>  2017-11-13 2019-02-12        NA     NA     NA
>> 2       0       3 URGE0004-3       36 URGE0004-001        1         2
>>  2017-11-21 2018-09-10        NA     NA     NA
>> 3       0       4 URGE0004-4       36 URGE0004-001        1         2
>>  2017-11-21 2018-09-10        NA     NA     NA
>> 4       0       5 URGE0004-5       36 URGE0004-001        1         2
>>  2017-11-21 2018-09-10        NA     NA     NA
>> 5       0       6 URGE0004-6       36 URGE0004-001        1         2
>>  2017-11-21 2018-09-10        NA     NA     NA
>>   ExpYPCORE ExpEAT ExpBITE missingCOREOM missingSFA missingSFB missingYP
>> missingEAT missingBITE impossCOREOM impossSFA
>> 1        NA     NA      NA            NA         NA         NA        NA
>>        NA          NA           NA        NA
>> 2        NA     NA      NA            NA         NA         NA        NA
>>        NA          NA           NA        NA
>> 3        NA     NA      NA            NA         NA         NA        NA
>>        NA          NA           NA        NA
>> 4        NA     NA      NA            NA         NA         NA        NA
>>        NA          NA           NA        NA
>> 5        NA     NA      NA            NA         NA         NA        NA
>>        NA          NA           NA        NA
>>   impossSFB impossYP impossEAT impossBITE
>> 1        NA       NA        NA         NA
>> 2        NA       NA        NA         NA
>> 3        NA       NA        NA         NA
>> 4        NA       NA        NA         NA
>> 5        NA       NA        NA         NA
>>  [ reached 'max' / getOption("max.print") -- omitted 672 rows ]
>>> datPrincip_URGE %>%
>> +   arrange(Fecha.evaluacion) %>% # sort by evaluation date
>> +    filter(Cuestionarios == "YP-CORE" &
>> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
>> +   dim()
>> [1] 677 195
>>> datPrincip_URGE %>%
>> +   arrange(Fecha.evaluacion) %>% # sort by evaluation date
>> +    filter(Cuestionarios == "YP-CORE" &
>> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
>> +   group_by(ID) %>%
>> +   mutate(occasion = row_number(),
>> +          first = if_else(occasion == 1, 1, 0)) %>% # to get first
>> ocassion only
>> +   filter(first == 1) %>%
>> +     dim()
>> [1]  61 197
>>> dat <- bind_rows(datPrincip_ALCA,
>> +                  datPrincip_URGE)
>>> dat %>%
>> +   arrange(Fecha.evaluacion) %>% # sort by evaluation date
>> +    filter(Cuestionarios == "YP-CORE" &
>> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
>> +   group_by(ID) %>%
>> +   mutate(occasion = row_number(),
>> +          first = if_else(occasion == 1, 1, 0)) %>% # to get first
>> ocassion only
>> +   filter(first == 1) %>%
>> +     dim()
>> [1] 118 197
>>> dat <- bind_rows(datPrincip_ALCA,
>> +                  datPrincip_ARGE,
>> +                  datPrincip_AVEN,
>> +                  datPrincip_MOSC,
>> +                  datPrincip_SABA,
>> +                  datPrincip_SEVI,
>> +                  datPrincip_TARR,
>> +                  datPrincip_URGE)
>>> dat %>%
>> +   arrange(Fecha.evaluacion) %>% # sort by evaluation date
>> +    filter(Cuestionarios == "YP-CORE" &
>> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
>> +   group_by(ID) %>%
>> +   mutate(occasion = row_number(),
>> +          first = if_else(occasion == 1, 1, 0)) %>% # to get first
>> ocassion only
>> +   filter(first == 1) %>%
>> +     dim()
>> [1] 237 197
>>> dat %>%
>> +   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
>> +    filter(Cuestionarios == "YP-CORE" &
>> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
>> +   group_by(ID) %>%
>> +   mutate(occasion = row_number(),
>> +          first = if_else(occasion == 1, 1, 0)) %>% # to get first
>> ocassion only
>> +   filter(first == 1) %>%
>> +     dim()
>> [1] 237 197
>>> dat %>%
>> +   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
>> +    filter(Cuestionarios == "YP-CORE" &
>> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
>> +   group_by(ID) %>%
>> +   mutate(occasion = row_number(),
>> +          first = if_else(occasion == 1, 1, 0)) %>% # to get first
>> ocassion only
>> +   filter(first == 1) %>%
>> +   select(ID, starts_with("YP")) %>%
>> +   dim()
>> [1] 237  11
>>> dat %>%
>> +   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
>> +    filter(Cuestionarios == "YP-CORE" &
>> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
>> +   group_by(ID) %>%
>> +   mutate(occasion = row_number(),
>> +          first = if_else(occasion == 1, 1, 0)) %>% # to get first
>> ocassion only
>> +   filter(first == 1) %>%
>> +   select(ID, starts_with("YP")) %>%
>> +   pivot_longer(cols = starts_with("YPCORE")) %>%
>> +   dim()
>> [1] 2370    3
>>> dat %>%
>> +   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
>> +    filter(Cuestionarios == "YP-CORE" &
>> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
>> +   group_by(ID) %>%
>> +   mutate(occasion = row_number(),
>> +          first = if_else(occasion == 1, 1, 0)) %>% # to get first
>> ocassion only
>> +   filter(first == 1) %>%
>> +   select(ID, starts_with("YP")) %>%
>> +   pivot_longer(cols = starts_with("YPCORE")) %>%
>> +   mutate(missing = if_else(!is.na(value), 0, 1)) -> longDat
>>> head(longDat)
>> # A tibble: 6 x 4
>> # Groups:   ID [1]
>>   ID       name     value missing
>>   <chr>    <chr>    <dbl>   <dbl>
>> 1 ALCA0001 YPCORE01     1       0
>> 2 ALCA0001 YPCORE02     0       0
>> 3 ALCA0001 YPCORE03     0       0
>> 4 ALCA0001 YPCORE04     0       0
>> 5 ALCA0001 YPCORE05     0       0
>> 6 ALCA0001 YPCORE06     1       0
>>> table(longDat$missing)
>>    0    1
>> 2356   14
>>> ?pivot_longer
>>> dat %>%
>> +   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
>> +    filter(Cuestionarios == "YP-CORE" &
>> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
>> +   group_by(ID) %>%
>> +   mutate(occasion = row_number(),
>> +          first = if_else(occasion == 1, 1, 0)) %>% # to get first
>> ocassion only
>> +   filter(first == 1) %>%
>> +   select(ID, starts_with("YP")) %>%
>> +   pivot_longer(cols = starts_with("YPCORE"),
>> +                names_to = "Item") %>%
>> +   mutate(missing = if_else(!is.na(value), 0, 1)) -> longDat
>>> head(longD)
>> Error in head(longD) : object 'longD' not found
>>> head(longDat)
>> # A tibble: 6 x 4
>> # Groups:   ID [1]
>>   ID       Item     value missing
>>   <chr>    <chr>    <dbl>   <dbl>
>> 1 ALCA0001 YPCORE01     1       0
>> 2 ALCA0001 YPCORE02     0       0
>> 3 ALCA0001 YPCORE03     0       0
>> 4 ALCA0001 YPCORE04     0       0
>> 5 ALCA0001 YPCORE05     0       0
>> 6 ALCA0001 YPCORE06     1       0
>>> dat %>%
>> +   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
>> +    filter(Cuestionarios == "YP-CORE" &
>> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
>> +   group_by(ID) %>%
>> +   mutate(occasion = row_number(),
>> +          first = if_else(occasion == 1, 1, 0)) %>% # to get first
>> ocassion only
>> +   filter(first == 1) %>%
>> +   select(ID, starts_with("YP")) %>%
>> +   pivot_longer(cols = starts_with("YPCORE"),
>> +                names_to = "Item",
>> +                values_to = "Score") %>%
>> +   mutate(missing = if_else(!is.na(Score), 0, 1)) -> longDat
>>> head(longDat)
>> # A tibble: 6 x 4
>> # Groups:   ID [1]
>>   ID       Item     Score missing
>>   <chr>    <chr>    <dbl>   <dbl>
>> 1 ALCA0001 YPCORE01     1       0
>> 2 ALCA0001 YPCORE02     0       0
>> 3 ALCA0001 YPCORE03     0       0
>> 4 ALCA0001 YPCORE04     0       0
>> 5 ALCA0001 YPCORE05     0       0
>> 6 ALCA0001 YPCORE06     1       0
>>> formul1 <- missing ~ Item + Item | ID
>>> fit <- glmer(formul1, family = binomial, data = longDat)
>> Error in glmer(formul1, family = binomial, data = longDat) :
>>   could not find function "glmer"
>>> library(lme4)
>> Loading required package: Matrix
>>
>> Attaching package: ?Matrix?
>>
>> The following objects are masked from ?package:tidyr?:
>>
>>     expand, pack, unpack
>>
>>> formul1 <- missing ~ Item + Item | ID
>>> fit <- glmer(formul1, family = binomial, data = longDat)
>> boundary (singular) fit: see ?isSingular
>> Warning messages:
>> 1: In commonArgs(par, fn, control, environment()) :
>>   maxfun < 10 * length(par)^2 is not recommended.
>> 2: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,
>> :
>>   failure to converge in 10000 evaluations
>>> formul1 <- missing ~ Item | ID
>>> fit <- glmer(formul1, family = binomial, data = longDat)
>> boundary (singular) fit: see ?isSingular
>> Warning messages:
>> 1: In commonArgs(par, fn, control, environment()) :
>>   maxfun < 10 * length(par)^2 is not recommended.
>> 2: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,
>> :
>>   failure to converge in 10000 evaluations
>>> head(longDat)
>> # A tibble: 6 x 4
>> # Groups:   ID [1]
>>   ID       Item     Score missing
>>   <chr>    <chr>    <dbl>   <dbl>
>> 1 ALCA0001 YPCORE01     1       0
>> 2 ALCA0001 YPCORE02     0       0
>> 3 ALCA0001 YPCORE03     0       0
>> 4 ALCA0001 YPCORE04     0       0
>> 5 ALCA0001 YPCORE05     0       0
>> 6 ALCA0001 YPCORE06     1       0
>>> ?glmer
>>> ?mcnemar.test
>>> table(is.na(dat$YPCORE01), is.na(dat$YPCORE02))
>>         FALSE TRUE
>>   FALSE  3573    4
>>   TRUE      1 9499
>>> mcnemar.test(table(is.na(dat$YPCORE01), is.na(dat$YPCORE02)))
>>         McNemar's Chi-squared test with continuity correction
>>
>> data:  table(is.na(dat$YPCORE01), is.na(dat$YPCORE02))
>> McNemar's chi-squared = 0.8, df = 1, p-value = 0.3711
>>
>>> dat %>%
>> +   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
>> +    filter(Cuestionarios == "YP-CORE" &
>> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
>> +   group_by(ID) %>%
>> +   mutate(occasion = row_number(),
>> +          first = if_else(occasion == 1, 1, 0)) %>% # to get first
>> ocassion only
>> +   filter(first == 1) %>%
>> +   select(ID, starts_with("YP")) %>%
>> +   pivot_longer(cols = c("YPCORE01", "YPCORE02")),
>> Error: unexpected ',' in:
>> "  select(ID, starts_with("YP")) %>%
>>   pivot_longer(cols = c("YPCORE01", "YPCORE02")),"
>>>                names_to = "Item",
>> Error: unexpected ',' in "               names_to = "Item","
>>>                values_to = "Score") %>%
>> Error: unexpected ')' in "               values_to = "Score")"
>>>   mutate(missing = if_else(!is.na(Score), 0, 1)) -> longDat
>> Error in if_else(!is.na(Score), 0, 1) : object 'Score' not found
>>> head(longDat)
>> # A tibble: 6 x 4
>> # Groups:   ID [1]
>>   ID       Item     Score missing
>>   <chr>    <chr>    <dbl>   <dbl>
>> 1 ALCA0001 YPCORE01     1       0
>> 2 ALCA0001 YPCORE02     0       0
>> 3 ALCA0001 YPCORE03     0       0
>> 4 ALCA0001 YPCORE04     0       0
>> 5 ALCA0001 YPCORE05     0       0
>> 6 ALCA0001 YPCORE06     1       0
>>> dat %>%
>> +   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
>> +    filter(Cuestionarios == "YP-CORE" &
>> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
>> +   group_by(ID) %>%
>> +   mutate(occasion = row_number(),
>> +          first = if_else(occasion == 1, 1, 0)) %>% # to get first
>> ocassion only
>> +   filter(first == 1) %>%
>> +   select(ID, starts_with("YP")) %>%
>> +   pivot_longer(cols = c("YPCORE01", "YPCORE02"),
>> +                names_to = "Item",
>> +                values_to = "Score") %>%
>> +   mutate(missing = if_else(!is.na(Score), 0, 1)) -> longDat
>>> head(longDat)
>> # A tibble: 6 x 12
>> # Groups:   ID [3]
>>   ID       YPCORE03 YPCORE04 YPCORE05 YPCORE06 YPCORE07 YPCORE08 YPCORE09
>> YPCORE10 Item     Score missing
>>   <chr>       <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>
>>   <dbl> <chr>    <dbl>   <dbl>
>> 1 ALCA0001        0        0        0        1        0        0        0
>>       0 YPCORE01     1       0
>> 2 ALCA0001        0        0        0        1        0        0        0
>>       0 YPCORE02     0       0
>> 3 ALCA0004        0        0        0        1        0        0        0
>>       0 YPCORE01     1       0
>> 4 ALCA0004        0        0        0        1        0        0        0
>>       0 YPCORE02     0       0
>> 5 ALCA0007        1        0        0        1        0        2        1
>>       0 YPCORE01     2       0
>> 6 ALCA0007        1        0        0        1        0        2        1
>>       0 YPCORE02     1       0
>>> dat %>%
>> +   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
>> +    filter(Cuestionarios == "YP-CORE" &
>> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
>> +   group_by(ID) %>%
>> +   mutate(occasion = row_number(),
>> +          first = if_else(occasion == 1, 1, 0)) %>% # to get first
>> ocassion only
>> +   filter(first == 1) %>%
>> +   select(ID, c("YPCORE01", "YPCORE02")) %>%
>> +   pivot_longer(cols = c("YPCORE01", "YPCORE02"),
>> +                names_to = "Item",
>> +                values_to = "Score") %>%
>> +   mutate(missing = if_else(!is.na(Score), 0, 1)) -> longDat
>>> head(longDat)
>> # A tibble: 6 x 4
>> # Groups:   ID [3]
>>   ID       Item     Score missing
>>   <chr>    <chr>    <dbl>   <dbl>
>> 1 ALCA0001 YPCORE01     1       0
>> 2 ALCA0001 YPCORE02     0       0
>> 3 ALCA0004 YPCORE01     1       0
>> 4 ALCA0004 YPCORE02     0       0
>> 5 ALCA0007 YPCORE01     2       0
>> 6 ALCA0007 YPCORE02     1       0
>>> fit <- glmer(formul1, family = binomial, data = longDat)
>> boundary (singular) fit: see ?isSingular
>>> ?isSingular
>>> head(longDat)
>> # A tibble: 6 x 4
>> # Groups:   ID [3]
>>   ID       Item     Score missing
>>   <chr>    <chr>    <dbl>   <dbl>
>> 1 ALCA0001 YPCORE01     1       0
>> 2 ALCA0001 YPCORE02     0       0
>> 3 ALCA0004 YPCORE01     1       0
>> 4 ALCA0004 YPCORE02     0       0
>> 5 ALCA0007 YPCORE01     2       0
>> 6 ALCA0007 YPCORE02     1       0
>>> table(longDat$missing)
>>   0   1
>> 472   2
>>> library(lme4)
>>> formul1 <- missing ~ (Item | ID)
>>> fit <- glmer(formul1, family = binomial, data = longDat)
>> boundary (singular) fit: see ?isSingular
>>> longDat$missRand <- rbinom(1, length(longDat), .5)
>>> head(longDat)
>> # A tibble: 6 x 5
>> # Groups:   ID [3]
>>   ID       Item     Score missing missRand
>>   <chr>    <chr>    <dbl>   <dbl>    <int>
>> 1 ALCA0001 YPCORE01     1       0        2
>> 2 ALCA0001 YPCORE02     0       0        2
>> 3 ALCA0004 YPCORE01     1       0        2
>> 4 ALCA0004 YPCORE02     0       0        2
>> 5 ALCA0007 YPCORE01     2       0        2
>> 6 ALCA0007 YPCORE02     1       0        2
>>> rbinom(1,1,.5)
>> [1] 0
>>> rbinom(1,1,.5)
>> [1] 0
>>> rbinom(1,1,.5)
>> [1] 0
>>> rbinom(1,1,.5)
>> [1] 0
>>> rbinom(1,1,.5)
>> [1] 0
>>> rbinom(1,1,.5)
>> [1] 0
>>> rbinom(1,1,.5)
>> [1] 0
>>> rbinom(1,1,.5)
>> [1] 0
>>> ?rbinom
>>> rbinom(1,1,.05)
>> [1] 0
>>> rbinom(1,1,.05)
>> [1] 0
>>> rbinom(1,1,.95)
>> [1] 1
>>> rbinom(1,1,.5)
>> [1] 0
>>> rbinom(1,1,.95)
>> [1] 1
>>> rbinom(1,1,.5)
>> [1] 1
>>> rbinom(1,1,.5)
>> [1] 0
>>> rbinom(1,1,.5)
>> [1] 0
>>> rbinom(1,1,.5)
>> [1] 1
>>> rbinom(1,1,.5)
>> [1] 0
>>> rbinom(1,1,.5)
>> [1] 1
>>> rbinom(1,2,.5)
>> [1] 2
>>> rbinom(2,1,.5)
>> [1] 0 0
>>> rbinom(2,1,.5)
>> [1] 0 0
>>> rbinom(2,1,.5)
>> [1] 0 0
>>> rbinom(2,1,.5)
>> [1] 0 1
>>> rbinom(2,1,.5)
>> [1] 1 1
>>> rbinom(2,1,.5)
>> [1] 0 1
>>> longDat$missRand <- rbinom(length(longDat), 1, .5)
>> Error: Assigned data `rbinom(length(longDat), 1, 0.5)` must be compatible
>> with existing data.
>> x Existing data has 474 rows.
>> x Assigned data has 5 rows.
>> ? Only vectors of size 1 are recycled.
>> Run `rlang::last_error()` to see where the error occurred.
>>> table(longDat$missing)
>>   0   1
>> 472   2
>>> library(lme4)
>>> head(longDat)
>> # A tibble: 6 x 5
>> # Groups:   ID [3]
>>   ID       Item     Score missing missRand
>>   <chr>    <chr>    <dbl>   <dbl>    <int>
>> 1 ALCA0001 YPCORE01     1       0        2
>> 2 ALCA0001 YPCORE02     0       0        2
>> 3 ALCA0004 YPCORE01     1       0        2
>> 4 ALCA0004 YPCORE02     0       0        2
>> 5 ALCA0007 YPCORE01     2       0        2
>> 6 ALCA0007 YPCORE02     1       0        2
>>> rbinom(2,1,.5)
>> [1] 1 0
>>> rbinom(2,1,.5)
>> [1] 1 1
>>> rbinom(2,1,.5)
>> [1] 1 1
>>> longDat$missRand <- NULL
>>> longDat$missRand <- rbinom(length(longDat), 1, .5)
>> Error: Assigned data `rbinom(length(longDat), 1, 0.5)` must be compatible
>> with existing data.
>> x Existing data has 474 rows.
>> x Assigned data has 4 rows.
>> ? Only vectors of size 1 are recycled.
>> Run `rlang::last_error()` to see where the error occurred.
>>> longDat$missRand <- rbinom(nrow(longDat), 1, .5)
>>> table(longDat$missRand)
>>   0   1
>> 240 234
>>> head(longDat)
>> # A tibble: 6 x 5
>> # Groups:   ID [3]
>>   ID       Item     Score missing missRand
>>   <chr>    <chr>    <dbl>   <dbl>    <int>
>> 1 ALCA0001 YPCORE01     1       0        0
>> 2 ALCA0001 YPCORE02     0       0        1
>> 3 ALCA0004 YPCORE01     1       0        0
>> 4 ALCA0004 YPCORE02     0       0        1
>> 5 ALCA0007 YPCORE01     2       0        0
>> 6 ALCA0007 YPCORE02     1       0        0
>>> formul1 <- missRand ~ (Item | ID)
>>> fit <- glmer(formul1, family = binomial, data = longDat)
>> boundary (singular) fit: see ?isSingular
>>> fit <- glmer(missRand ~ Item, family = binomial, data = longDat)
>> Error: No random effects terms specified in formula
>>> fit <- glmer(missRand ~ Item | ID, family = binomial, data = longDat)
>> boundary (singular) fit: see ?isSingular
>>> fit <- glmer(factor(missRand) ~ Item | ID, family = binomial, data =
>> longDat)
>> boundary (singular) fit: see ?isSingular
>>> fit <- glmer(missRand ~ Item | ID, family = "logistic", data = longDat)
>> Error in get(family, mode = "function", envir = parent.frame(2)) :
>>   object 'logistic' of mode 'function' was not found
>>> fit <- glmer(missRand ~ Item + (1 | ID), family = "binomial", data =
>> longDat)
>> boundary (singular) fit: see ?isSingular
>>> ?glmer
>>> longDat
>> # A tibble: 474 x 5
>> # Groups:   ID [237]
>>    ID       Item     Score missing missRand
>>    <chr>    <chr>    <dbl>   <dbl>    <int>
>>  1 ALCA0001 YPCORE01     1       0        0
>>  2 ALCA0001 YPCORE02     0       0        1
>>  3 ALCA0004 YPCORE01     1       0        0
>>  4 ALCA0004 YPCORE02     0       0        1
>>  5 ALCA0007 YPCORE01     2       0        0
>>  6 ALCA0007 YPCORE02     1       0        0
>>  7 ALCA0012 YPCORE01     3       0        1
>>  8 ALCA0012 YPCORE02     1       0        1
>>  9 ALCA0013 YPCORE01     4       0        1
>> 10 ALCA0013 YPCORE02     3       0        1
>> # ? with 464 more rows
>>> table(longDat$missRand, longDat$Item)
>>     YPCORE01 YPCORE02
>>   0      125      115
>>   1      112      122
>>> fit <- glmer(missRand ~ factor(Item) + (1 | ID), family = "binomial",
>> data = longDat)
>> boundary (singular) fit: see ?isSingular
>>> fit <- glmer(missRand ~ factor(Item) + (1 | factor(ID)), family =
>> "binomial", data = longDat)
>> Error: couldn't evaluate grouping factor factor(ID) within model frame:
>> try adding grouping factor to data frame explicitly if possible
>>> fit <- glmer(missRand ~ factor(Item) + (1 | ID), family = "binomial",
>> data = longDat)
>> boundary (singular) fit: see ?isSingular
>>> rm(fit)
>>> fit <- glmer(missRand ~ factor(Item) + (1 | ID), family = "binomial",
>> data = longDat)
>> boundary (singular) fit: see ?isSingular
>>> fit
>> Generalized linear mixed model fit by maximum likelihood (Laplace
>> Approximation) ['glmerMod']
>>  Family: binomial  ( logit )
>> Formula: missRand ~ factor(Item) + (1 | ID)
>>    Data: longDat
>>       AIC       BIC    logLik  deviance  df.resid
>>  662.1833  674.6669 -328.0917  656.1833       471
>> Random effects:
>>  Groups Name        Std.Dev.
>>  ID     (Intercept) 1.943e-07
>> Number of obs: 474, groups:  ID, 237
>> Fixed Effects:
>>          (Intercept)  factor(Item)YPCORE02
>>              -0.1098                0.1689
>> convergence code 0; 0 optimizer warnings; 1 lme4 warnings
>>> dat %>%
>> +   arrange(ID, Fecha.evaluacion) %>% # sort by evaluation date
>> +    filter(Cuestionarios == "YP-CORE" &
>> +            Codigo.datos == "RESPONDIO CORRECTAMENTE") %>%
>> +   group_by(ID) %>%
>> +   mutate(occasion = row_number(),
>> +          first = if_else(occasion == 1, 1, 0)) %>% # to get first
>> ocassion only
>> +   filter(first == 1) %>%
>> +   select(ID, starts_with("YPCORE")) %>%
>> +   pivot_longer(cols = starts_with("YPCORE"),
>> +                names_to = "Item",
>> +                values_to = "Score") %>%
>> +   mutate(missing = if_else(!is.na(Score), 0, 1)) -> longDat
>>> head(longDat)
>> # A tibble: 6 x 4
>> # Groups:   ID [1]
>>   ID       Item     Score missing
>>   <chr>    <chr>    <dbl>   <dbl>
>> 1 ALCA0001 YPCORE01     1       0
>> 2 ALCA0001 YPCORE02     0       0
>> 3 ALCA0001 YPCORE03     0       0
>> 4 ALCA0001 YPCORE04     0       0
>> 5 ALCA0001 YPCORE05     0       0
>> 6 ALCA0001 YPCORE06     1       0
>>> formul1 <- missing ~ (Item | ID)
>>> fit <- glmer(missing ~ factor(Item) + (1 | ID), family = "binomial",
>> data = longDat)
>> Warning message:
>> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>>   Model failed to converge with max|grad| = 1.78883 (tol = 0.002,
>> component 1)
>>> fit <- glmer((score == 1) ~ factor(Item) + (1 | ID), family =
>> "binomial", data = longDat)
>> Error in eval(predvars, data, env) : object 'score' not found
>>> fit <- glmer((Score == 1) ~ factor(Item) + (1 | ID), family =
>> "binomial", data = longDat)
>> Warning message:
>> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>>   Model failed to converge with max|grad| = 0.00309555 (tol = 0.002,
>> component 1)
>>> fit <- glmer((Score == 1) ~ factor(Item) + (factor(Item) | ID), family =
>> "binomial", data = longDat)
>> Error: number of observations (=2356) < number of random effects (=2360)
>> for term (factor(Item) | ID); the random-effects parameters are probably
>> unidentifiable
>>> fit <- glmer((Score == 1) ~ factor(Item) + (Item | ID), family =
>> "binomial", data = longDat)
>> Error: number of observations (=2356) < number of random effects (=2360)
>> for term (Item | ID); the random-effects parameters are probably
>> unidentifiable
>>> fit <- glmer((Score == 1) ~ factor(Item) + (1 | ID), family =
>> "binomial", data = longDat)
>> Warning message:
>> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>>   Model failed to converge with max|grad| = 0.00309555 (tol = 0.002,
>> component 1)
>>> head(longDat)
>> # A tibble: 6 x 4
>> # Groups:   ID [1]
>>   ID       Item     Score missing
>>   <chr>    <chr>    <dbl>   <dbl>
>> 1 ALCA0001 YPCORE01     1       0
>> 2 ALCA0001 YPCORE02     0       0
>> 3 ALCA0001 YPCORE03     0       0
>> 4 ALCA0001 YPCORE04     0       0
>> 5 ALCA0001 YPCORE05     0       0
>> 6 ALCA0001 YPCORE06     1       0
>>> longDat$missRand <- rbinom(nrow(longDat), 1, .5)
>>> table(longDat$missRand)
>>    0    1
>> 1211 1159
>>> fit <- glmer(missRand ~ Item + (1 | ID), family = "binomial", data =
>> longDat)
>> boundary (singular) fit: see ?isSingular
>>> fit
>> Generalized linear mixed model fit by maximum likelihood (Laplace
>> Approximation) ['glmerMod']
>>  Family: binomial  ( logit )
>> Formula: missRand ~ Item + (1 | ID)
>>    Data: longDat
>>       AIC       BIC    logLik  deviance  df.resid
>>  3300.819  3364.296 -1639.410  3278.819      2359
>> Random effects:
>>  Groups Name        Std.Dev.
>>  ID     (Intercept) 2.663e-07
>> Number of obs: 2370, groups:  ID, 237
>> Fixed Effects:
>>  (Intercept)  ItemYPCORE02  ItemYPCORE03  ItemYPCORE04  ItemYPCORE05
>> ItemYPCORE06  ItemYPCORE07  ItemYPCORE08
>>   -5.909e-02     1.013e-01    -1.690e-02     1.182e-01     6.753e-02
>> -1.186e-01    -1.046e-15    -1.186e-01
>> ItemYPCORE09  ItemYPCORE10
>>    1.858e-01    -6.766e-02
>> convergence code 0; 0 optimizer warnings; 1 lme4 warnings
>>> anova(fit)
>> Analysis of Variance Table
>>      npar Sum Sq Mean Sq F value
>> Item    9 5.5453 0.61615  0.6161
>>> table(longDat$Item, as.numeric(longDat$Item))
>> < table of extent 10 x 0 >
>> Warning message:
>> In table(longDat$Item, as.numeric(longDat$Item)) :
>>   NAs introduced by coercion
>>> dim(longDat)
>> [1] 2370    5
>>> head(longDat)
>> # A tibble: 6 x 5
>> # Groups:   ID [1]
>>   ID       Item     Score missing missRand
>>   <chr>    <chr>    <dbl>   <dbl>    <int>
>> 1 ALCA0001 YPCORE01     1       0        1
>> 2 ALCA0001 YPCORE02     0       0        1
>> 3 ALCA0001 YPCORE03     0       0        1
>> 4 ALCA0001 YPCORE04     0       0        1
>> 5 ALCA0001 YPCORE05     0       0        1
>> 6 ALCA0001 YPCORE06     1       0        0
>>> table(longDat$Item) #, as.numeric(longDat$Item))
>> YPCORE01 YPCORE02 YPCORE03 YPCORE04 YPCORE05 YPCORE06 YPCORE07 YPCORE08
>> YPCORE09 YPCORE10
>>      237      237      237      237      237      237      237      237
>>   237      237
>>> table(as.numeric(longDat$Item)) #, as.numeric(longDat$Item))
>> < table of extent 0 >
>> Warning message:
>> In table(as.numeric(longDat$Item)) : NAs introduced by coercion
>>> table(longDat$Item, as.numeric(factor(longDat$Item)))
>>              1   2   3   4   5   6   7   8   9  10
>>   YPCORE01 237   0   0   0   0   0   0   0   0   0
>>   YPCORE02   0 237   0   0   0   0   0   0   0   0
>>   YPCORE03   0   0 237   0   0   0   0   0   0   0
>>   YPCORE04   0   0   0 237   0   0   0   0   0   0
>>   YPCORE05   0   0   0   0 237   0   0   0   0   0
>>   YPCORE06   0   0   0   0   0 237   0   0   0   0
>>   YPCORE07   0   0   0   0   0   0 237   0   0   0
>>   YPCORE08   0   0   0   0   0   0   0 237   0   0
>>   YPCORE09   0   0   0   0   0   0   0   0 237   0
>>   YPCORE10   0   0   0   0   0   0   0   0   0 237
>>> longDat %>%
>> +   mutate(missRand = rbinom(1, 1, .5))
>> # A tibble: 2,370 x 5
>> # Groups:   ID [237]
>>    ID       Item     Score missing missRand
>>    <chr>    <chr>    <dbl>   <dbl>    <int>
>>  1 ALCA0001 YPCORE01     1       0        1
>>  2 ALCA0001 YPCORE02     0       0        1
>>  3 ALCA0001 YPCORE03     0       0        1
>>  4 ALCA0001 YPCORE04     0       0        1
>>  5 ALCA0001 YPCORE05     0       0        1
>>  6 ALCA0001 YPCORE06     1       0        1
>>  7 ALCA0001 YPCORE07     0       0        1
>>  8 ALCA0001 YPCORE08     0       0        1
>>  9 ALCA0001 YPCORE09     0       0        1
>> 10 ALCA0001 YPCORE10     0       0        1
>> # ? with 2,360 more rows
>>> table(longDat$missRand)
>>    0    1
>> 1211 1159
>>> longDat %>%
>> +   mutate(missRand = rbinom(1, 1, .5),
>> +          missReg = rbinom(1, 1, as.numeric(factor(Item))/11))
>> # A tibble: 2,370 x 6
>> # Groups:   ID [237]
>>    ID       Item     Score missing missRand missReg
>>    <chr>    <chr>    <dbl>   <dbl>    <int>   <int>
>>  1 ALCA0001 YPCORE01     1       0        1       0
>>  2 ALCA0001 YPCORE02     0       0        1       0
>>  3 ALCA0001 YPCORE03     0       0        1       0
>>  4 ALCA0001 YPCORE04     0       0        1       0
>>  5 ALCA0001 YPCORE05     0       0        1       0
>>  6 ALCA0001 YPCORE06     1       0        1       0
>>  7 ALCA0001 YPCORE07     0       0        1       0
>>  8 ALCA0001 YPCORE08     0       0        1       0
>>  9 ALCA0001 YPCORE09     0       0        1       0
>> 10 ALCA0001 YPCORE10     0       0        1       0
>> # ? with 2,360 more rows
>>> table(longDat$missReg)
>> < table of extent 0 >
>> Warning message:
>> Unknown or uninitialised column: `missReg`.
>>> longDat %>%
>> +   mutate(missRand = rbinom(1, 1, .5),
>> +          missReg = rbinom(1, 1, as.numeric(factor(Item))/11)) -> longDat
>>> table(longDat$missReg)
>>    0    1
>> 2120  250
>>> fit <- glmer(missReg ~ Item + (1 | ID), family = "binomial", data =
>> longDat)
>> Warning message:
>> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>>   Model failed to converge with max|grad| = 3.81902 (tol = 0.002,
>> component 1)
>>> longDat %>%
>> +   mutate(missRand = rbinom(1, 1, .5),
>> +          missReg = rbinom(1, 1, as.numeric(factor(Item))/11)) -> longDat
>>> head(longDat)
>> # A tibble: 6 x 6
>> # Groups:   ID [1]
>>   ID       Item     Score missing missRand missReg
>>   <chr>    <chr>    <dbl>   <dbl>    <int>   <int>
>> 1 ALCA0001 YPCORE01     1       0        1       1
>> 2 ALCA0001 YPCORE02     0       0        1       1
>> 3 ALCA0001 YPCORE03     0       0        1       1
>> 4 ALCA0001 YPCORE04     0       0        1       1
>> 5 ALCA0001 YPCORE05     0       0        1       1
>> 6 ALCA0001 YPCORE06     1       0        1       1
>>> table(longDat$missRand)
>>    0    1
>> 1140 1230
>>> table(longDat$missReg)
>>    0    1
>> 2160  210
>>> library(lme4)
>>> fitRand <- glmer(missRand ~ Item + (1 | ID), family = "binomial", data =
>> longDat)
>> Warning message:
>> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>>   Model failed to converge with max|grad| = 5.46528 (tol = 0.002,
>> component 1)
>>> anova(fitRand)
>> Analysis of Variance Table
>>      npar   Sum Sq   Mean Sq F value
>> Item    9 0.050372 0.0055969  0.0056
>>> length(unique(longDat$ID))
>> [1] 237
>>> rm(fitRand)
>>> anova(fitRand)
>> Error in anova(fitRand) : object 'fitRand' not found
>>> fitRand <- glmer(missRand ~ Item + (1 | ID), family = "binomial", data =
>> longDat)
>> Warning message:
>> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>>   Model failed to converge with max|grad| = 5.46528 (tol = 0.002,
>> component 1)
>>> fitRand <- glmer(missRand ~ Item + (1 | ID), family = binomial, data =
>> longDat)
>> Warning message:
>> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>>   Model failed to converge with max|grad| = 5.46528 (tol = 0.002,
>> component 1)
>>> fitRand <- glmer(missRand ~ Item + (1 | ID), family = binomial(link =
>> "logit"), data = longDat)
>> Warning message:
>> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>>   Model failed to converge with max|grad| = 5.46528 (tol = 0.002,
>> component 1)
>>> ?glmer
>>> data("cbpp")
>>> dim(cbpp)
>> [1] 56  4
>>> head(cbpp)
>>   herd incidence size period
>> 1    1         2   14      1
>> 2    1         3   12      2
>> 3    1         4    9      3
>> 4    1         0    5      4
>> 5    2         3   22      1
>> 6    2         1   18      2
>>> fitRand <- glmer(cbind(missRand, 1) ~ Item + (1 | ID), family =
>> binomial(link = "logit"), data = longDat)
>> Warning message:
>> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>>   Model failed to converge with max|grad| = 0.00550177 (tol = 0.002,
>> component 1)
>>> fitReg <- glmer(cbind(missReg, 1) ~ Item + (1 | ID), family =
>> "binomial", data = longDat)
>> Warning messages:
>> 1: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,
>> :
>>   failure to converge in 10000 evaluations
>> 2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>>   Model failed to converge with max|grad| = 1.47499 (tol = 0.002,
>> component 1)
>>> fitReal <- glmer(cbind(missing, 1) ~ Item + (1 | ID), family =
>> "binomial", data = longDat)
>> Warning message:
>> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>>   Model failed to converge with max|grad| = 1.29013 (tol = 0.002,
>> component 1)
>>> data(mlmdata)
>> Warning message:
>> In data(mlmdata) : data set ?mlmdata? not found
>>> library(haven)
>>> mlmdata <- read_dta("
>> https://stats.idre.ucla.edu/stat/examples/imm/imm10.dta")
>>> head(mlmdata)
>> # A tibble: 6 x 19
>>   schid stuid    ses meanses homework white parented public ratio percmin
>> math   sex  race sctype  cstr scsize urban
>>   <dbl> <dbl>  <dbl>   <dbl>    <dbl> <dbl>    <dbl>  <dbl> <dbl>   <dbl>
>> <dbl> <dbl> <dbl>  <dbl> <dbl>  <dbl> <dbl>
>> 1  7472     3 -0.130  -0.483        1     1        2      1    19       0
>>   48     2     4      1     2      3     2
>> 2  7472     8 -0.390  -0.483        0     1        2      1    19       0
>>   48     1     4      1     2      3     2
>> 3  7472    13 -0.800  -0.483        0     1        2      1    19       0
>>   53     1     4      1     2      3     2
>> 4  7472    17 -0.720  -0.483        1     1        2      1    19       0
>>   42     1     4      1     2      3     2
>> 5  7472    27 -0.740  -0.483        2     1        2      1    19       0
>>   43     2     4      1     2      3     2
>> 6  7472    28 -0.580  -0.483        1     1        2      1    19       0
>>   57     2     4      1     2      3     2
>> # ? with 2 more variables: region <dbl>, schnum <dbl>
>>> model <- glmer(white ~ homework + (1 + homework | schid), data=mlmdata,
>> +                family=binomial(link="logit"))
>> boundary (singular) fit: see ?isSingular
>>> summary(model)
>> Generalized linear mixed model fit by maximum likelihood (Laplace
>> Approximation) ['glmerMod']
>>  Family: binomial  ( logit )
>> Formula: white ~ homework + (1 + homework | schid)
>>    Data: mlmdata
>>
>>      AIC      BIC   logLik deviance df.resid
>>    182.4    200.2    -86.2    172.4      255
>>
>> Scaled residuals:
>>     Min      1Q  Median      3Q     Max
>> -4.3373 -0.1184  0.1112  0.3421  3.8801
>>
>> Random effects:
>>  Groups Name        Variance Std.Dev. Corr
>>  schid  (Intercept) 16.28745 4.0358
>>         homework     0.04678 0.2163   -1.00
>> Number of obs: 260, groups:  schid, 10
>>
>> Fixed effects:
>>             Estimate Std. Error z value Pr(>|z|)
>> (Intercept)  1.67365    1.47324   1.136    0.256
>> homework     0.04508    0.18433   0.245    0.807
>>
>> Correlation of Fixed Effects:
>>          (Intr)
>> homework -0.590
>> convergence code: 0
>> boundary (singular) fit: see ?isSingular
>>
>>> dim(mlmdata)
>> [1] 260  19
>>> head(mlmdata)
>> # A tibble: 6 x 19
>>   schid stuid    ses meanses homework white parented public ratio percmin
>> math   sex  race sctype  cstr scsize urban
>>   <dbl> <dbl>  <dbl>   <dbl>    <dbl> <dbl>    <dbl>  <dbl> <dbl>   <dbl>
>> <dbl> <dbl> <dbl>  <dbl> <dbl>  <dbl> <dbl>
>> 1  7472     3 -0.130  -0.483        1     1        2      1    19       0
>>   48     2     4      1     2      3     2
>> 2  7472     8 -0.390  -0.483        0     1        2      1    19       0
>>   48     1     4      1     2      3     2
>> 3  7472    13 -0.800  -0.483        0     1        2      1    19       0
>>   53     1     4      1     2      3     2
>> 4  7472    17 -0.720  -0.483        1     1        2      1    19       0
>>   42     1     4      1     2      3     2
>> 5  7472    27 -0.740  -0.483        2     1        2      1    19       0
>>   43     2     4      1     2      3     2
>> 6  7472    28 -0.580  -0.483        1     1        2      1    19       0
>>   57     2     4      1     2      3     2
>> # ? with 2 more variables: region <dbl>, schnum <dbl>
>>> length(unique(mlmdata$schid))
>> [1] 10
>>> table(white)
>> Error in table(white) : object 'white' not found
>>> table(mlmdata$white)
>>   0   1
>>  71 189
>>> fitRand <- glmer(missRand ~ Item + (1 | ID), family = binomial(link =
>> "logit"), data = longDat)
>> Warning message:
>> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>>   Model failed to converge with max|grad| = 5.46528 (tol = 0.002,
>> component 1)
>>> ,
>> Error: unexpected ',' in ","
>>> longDat %>%
>> +   mutate(missRand = rbinom(1, 1, .5),
>> +          missReg = rbinom(1, 1, as.numeric(factor(Item))/11),
>> +          riskItem = if_else(Item == "YPCORE04", 1, 0),
>> +          posItem = if_else(Item %in% c("YPCORE03", "YPCORE5",
>> "YPCORE10"), 1, 0)) -> longDat
>>> head(longDat)
>> # A tibble: 6 x 8
>> # Groups:   ID [1]
>>   ID       Item     Score missing missRand missReg riskItem posItem
>>   <chr>    <chr>    <dbl>   <dbl>    <int>   <int>    <dbl>   <dbl>
>> 1 ALCA0001 YPCORE01     1       0        0       0        0       0
>> 2 ALCA0001 YPCORE02     0       0        0       0        0       0
>> 3 ALCA0001 YPCORE03     0       0        0       0        0       1
>> 4 ALCA0001 YPCORE04     0       0        0       0        1       0
>> 5 ALCA0001 YPCORE05     0       0        0       0        0       0
>> 6 ALCA0001 YPCORE06     1       0        0       0        0       0
>>> table(longDat$riskItem, longDat$posItem)
>>        0    1
>>   0 1659  474
>>   1  237    0
>>> table(longDat$riskItem, longDat$Item)
>>     YPCORE01 YPCORE02 YPCORE03 YPCORE04 YPCORE05 YPCORE06 YPCORE07
>> YPCORE08 YPCORE09 YPCORE10
>>   0      237      237      237        0      237      237      237
>> 237      237      237
>>   1        0        0        0      237        0        0        0
>> 0        0        0
>>> table(longDat$posItem, longDat$Item)
>>     YPCORE01 YPCORE02 YPCORE03 YPCORE04 YPCORE05 YPCORE06 YPCORE07
>> YPCORE08 YPCORE09 YPCORE10
>>   0      237      237        0      237      237      237      237
>> 237      237        0
>>   1        0        0      237        0        0        0        0
>> 0        0      237
>>> longDat %>%
>> +   mutate(missRand = rbinom(1, 1, .5),
>> +          missReg = rbinom(1, 1, as.numeric(factor(Item))/11),
>> +          riskItem = if_else(Item == "YPCORE04", 1, 0),
>> +          posItem = if_else(Item %in% c("YPCORE03", "YPCORE05",
>> "YPCORE10"), 1, 0)) -> longDat
>>> head(longDat)
>> # A tibble: 6 x 8
>> # Groups:   ID [1]
>>   ID       Item     Score missing missRand missReg riskItem posItem
>>   <chr>    <chr>    <dbl>   <dbl>    <int>   <int>    <dbl>   <dbl>
>> 1 ALCA0001 YPCORE01     1       0        0       0        0       0
>> 2 ALCA0001 YPCORE02     0       0        0       0        0       0
>> 3 ALCA0001 YPCORE03     0       0        0       0        0       1
>> 4 ALCA0001 YPCORE04     0       0        0       0        1       0
>> 5 ALCA0001 YPCORE05     0       0        0       0        0       1
>> 6 ALCA0001 YPCORE06     1       0        0       0        0       0
>>> table(longDat$posItem, longDat$Item)
>>     YPCORE01 YPCORE02 YPCORE03 YPCORE04 YPCORE05 YPCORE06 YPCORE07
>> YPCORE08 YPCORE09 YPCORE10
>>   0      237      237        0      237        0      237      237
>> 237      237        0
>>   1        0        0      237        0      237        0        0
>> 0        0      237
>>> fitRisk <- glmer(missing ~ riskItem + (1 | ID), family = binomial, data
>> = longDat)
>>> summary(fitRisk)
>> Generalized linear mixed model fit by maximum likelihood (Laplace
>> Approximation) ['glmerMod']
>>  Family: binomial  ( logit )
>> Formula: missing ~ riskItem + (1 | ID)
>>    Data: longDat
>>
>>      AIC      BIC   logLik deviance df.resid
>>     51.7     69.0    -22.9     45.7     2367
>>
>> Scaled residuals:
>>      Min       1Q   Median       3Q      Max
>> -0.52020 -0.00113 -0.00113 -0.00113  2.93829
>>
>> Random effects:
>>  Groups Name        Variance Std.Dev.
>>  ID     (Intercept) 205.3    14.33
>> Number of obs: 2370, groups:  ID, 237
>>
>> Fixed effects:
>>             Estimate Std. Error z value Pr(>|z|)
>> (Intercept)  -13.563      2.540  -5.339 9.33e-08 ***
>> riskItem      -2.418      3.745  -0.646    0.518
>> ---
>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>
>> Correlation of Fixed Effects:
>>          (Intr)
>> riskItem 0.064
>>> fitRisk <- glmer(missing ~ posItem + (1 | ID), family = binomial, data =
>> longDat)
>>> fitRisk <- glmer(missing ~ riskItem + (1 | ID), family = binomial, data
>> = longDat)
>>> summary(fitRisk)
>> Generalized linear mixed model fit by maximum likelihood (Laplace
>> Approximation) ['glmerMod']
>>  Family: binomial  ( logit )
>> Formula: missing ~ riskItem + (1 | ID)
>>    Data: longDat
>>
>>      AIC      BIC   logLik deviance df.resid
>>     51.7     69.0    -22.9     45.7     2367
>>
>> Scaled residuals:
>>      Min       1Q   Median       3Q      Max
>> -0.52020 -0.00113 -0.00113 -0.00113  2.93829
>>
>> Random effects:
>>  Groups Name        Variance Std.Dev.
>>  ID     (Intercept) 205.3    14.33
>> Number of obs: 2370, groups:  ID, 237
>>
>> Fixed effects:
>>             Estimate Std. Error z value Pr(>|z|)
>> (Intercept)  -13.563      2.540  -5.339 9.33e-08 ***
>> riskItem      -2.418      3.745  -0.646    0.518
>> ---
>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>
>> Correlation of Fixed Effects:
>>          (Intr)
>> riskItem 0.064
>>> fitPos <- glmer(missing ~ posItem + (1 | ID), family = binomial, data =
>> longDat)
>>> summary(fitPos)
>> Generalized linear mixed model fit by maximum likelihood (Laplace
>> Approximation) ['glmerMod']
>>  Family: binomial  ( logit )
>> Formula: missing ~ posItem + (1 | ID)
>>    Data: longDat
>>
>>      AIC      BIC   logLik deviance df.resid
>>     52.4     69.7    -23.2     46.4     2367
>>
>> Scaled residuals:
>>     Min      1Q  Median      3Q     Max
>> -0.5111 -0.0012 -0.0012 -0.0010  3.4498
>>
>> Random effects:
>>  Groups Name        Variance Std.Dev.
>>  ID     (Intercept) 197.5    14.05
>> Number of obs: 2370, groups:  ID, 237
>>
>> Fixed effects:
>>             Estimate Std. Error z value Pr(>|z|)
>> (Intercept) -13.5152     2.5280  -5.346 8.98e-08 ***
>> posItem      -0.2953     1.2396  -0.238    0.812
>> ---
>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>
>> Correlation of Fixed Effects:
>>         (Intr)
>> posItem -0.110
>>> head(longDat,15)
>> # A tibble: 15 x 8
>> # Groups:   ID [2]
>>    ID       Item     Score missing missRand missReg riskItem posItem
>>    <chr>    <chr>    <dbl>   <dbl>    <int>   <int>    <dbl>   <dbl>
>>  1 ALCA0001 YPCORE01     1       0        0       0        0       0
>>  2 ALCA0001 YPCORE02     0       0        0       0        0       0
>>  3 ALCA0001 YPCORE03     0       0        0       0        0       1
>>  4 ALCA0001 YPCORE04     0       0        0       0        1       0
>>  5 ALCA0001 YPCORE05     0       0        0       0        0       1
>>  6 ALCA0001 YPCORE06     1       0        0       0        0       0
>>  7 ALCA0001 YPCORE07     0       0        0       0        0       0
>>  8 ALCA0001 YPCORE08     0       0        0       0        0       0
>>  9 ALCA0001 YPCORE09     0       0        0       0        0       0
>> 10 ALCA0001 YPCORE10     0       0        0       0        0       1
>> 11 ALCA0004 YPCORE01     1       0        0       0        0       0
>> 12 ALCA0004 YPCORE02     0       0        0       0        0       0
>> 13 ALCA0004 YPCORE03     0       0        0       0        0       1
>> 14 ALCA0004 YPCORE04     0       0        0       0        1       0
>> 15 ALCA0004 YPCORE05     0       0        0       0        0       1
>>>
>>> rm(fitRand)
>>> rm(fitReg)
>>> rm(fitReal)
>>> fitRand <- glmer(cbind(missRand, 1) ~ Item + (1 | ID), family =
>> binomial(link = "logit"), data = longDat)
>> Warning message:
>> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>>   Model failed to converge with max|grad| = 0.0501701 (tol = 0.002,
>> component 1)
>>> fitRand
>> Generalized linear mixed model fit by maximum likelihood (Laplace
>> Approximation) ['glmerMod']
>>  Family: binomial  ( logit )
>> Formula: cbind(missRand, 1) ~ Item + (1 | ID)
>>    Data: longDat
>>       AIC       BIC    logLik  deviance  df.resid
>>  2215.017  2278.494 -1096.509  2193.017      2359
>> Random effects:
>>  Groups Name        Std.Dev.
>>  ID     (Intercept) 2.509
>> Number of obs: 2370, groups:  ID, 237
>> Fixed Effects:
>>  (Intercept)  ItemYPCORE02  ItemYPCORE03  ItemYPCORE04  ItemYPCORE05
>> ItemYPCORE06  ItemYPCORE07  ItemYPCORE08
>>    -2.512456      0.004360      0.007166      0.001288      0.004078
>> 0.002322      0.005059      0.006118
>> ItemYPCORE09  ItemYPCORE10
>>     0.006454      0.005057
>> convergence code 0; 0 optimizer warnings; 1 lme4 warnings
>>
>> So failed to converge but does give a set of estimates.  I think I can use
>> the
>>
>>
>>> fitReg <- glmer(cbind(missReg, 1) ~ Item + (1 | ID), family =
>> "binomial", data = longDat)
>> Warning messages:
>> 1: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,
>> :
>>   failure to converge in 10000 evaluations
>> 2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>>   Model failed to converge with max|grad| = 1.66664 (tol = 0.002,
>> component 1)
>>> fitReg
>> Generalized linear mixed model fit by maximum likelihood (Laplace
>> Approximation) ['glmerMod']
>>  Family: binomial  ( logit )
>> Formula: cbind(missReg, 1) ~ Item + (1 | ID)
>>    Data: longDat
>>       AIC       BIC    logLik  deviance  df.resid
>>  605.3404  668.8175 -291.6702  583.3404      2359
>> Random effects:
>>  Groups Name        Std.Dev.
>>  ID     (Intercept) 6.775
>> Number of obs: 2370, groups:  ID, 237
>> Fixed Effects:
>>  (Intercept)  ItemYPCORE02  ItemYPCORE03  ItemYPCORE04  ItemYPCORE05
>> ItemYPCORE06  ItemYPCORE07  ItemYPCORE08
>>      -8.6607       -0.3793       -0.3159       -0.3591       -0.4608
>>  -0.4453       -0.4165       -0.6597
>> ItemYPCORE09  ItemYPCORE10
>>      -0.4304       -0.3535
>> convergence code 0; 1 optimizer warnings; 1 lme4 warnings
>>
>> I am starting to realise the depths of my ignorance about all this.  It's
>> clear to me that having only one observation per cell
>> because of the nesting of items within participants and only having one
>> completion per participant is causing the convergence
>> issues (which makes sense though I have a sense that there might be ways
>> to extract estimates despite this: this is beyond me!)
>>
>> I'm puzzled that I get slightly different results for the risk analysis if
>> I use the "cbind(missing, 1) ~ " syntax in the formula
>> from those I get using just "missing ~ " and different max|grad values
>> from the nonconvergence message depending on that choice.
>> I suspect that's a red herring.
>>
>> I have seen many comments here recently about non-convergence and ways to
>> overcome it by specifying different methods of
>> approximation (if I understood that properly) but they generally seemed to
>> be for much more complex models than mine and
>> probably weren't about this "cell size = 1" issue.  I have searched for
>> answer or illumination for some hours and perhaps
>> I'm asking the wrong questions but I'm stuck.  (I think it's very unlikely
>> that I have access to trained statisticians by
>> virtue of my honorary or paid academic positions!)
>>
>> So my questions:
>> 1) _is_ there a way to estimate such a model, i.e. estimating an effect
>> across all ten items with only one completion per participant?
>> 2) can someone suggest good reading matter for a dogged non-statistician
>> who can handle a lot of continuous variable issues up to
>> some real complexity on his own but has never been as comfortable with
>> counts beyond the basic old NHST, single level approaches?  I
>> have an old copy of Pinheiro and Bates but confess that despite trying
>> many times, the older it and I get, the less I'm able to cope
>> with it.  The algebra is just beyond me.
>>
>> TIA ... and best wishes to all for physical safety and psychological
>> resilience in these times,
>>
>> Chris
>>
>> --
>> Small contribution in our coronavirus rigours:
>>
>> https://www.coresystemtrust.org.uk/home/free-options-to-replace-paper-core-forms-during-the-coronavirus-pandemic/
>>
>> Chris Evans <chris at psyctc.org> Visiting Professor, University of
>> Sheffield <chris.evans at sheffield.ac.uk>
>> I do some consultation work for the University of Roehampton <
>> chris.evans at roehampton.ac.uk> and other places
>> but <chris at psyctc.org> remains my main Email address.  I have a work web
>> site at:
>>    https://www.psyctc.org/psyctc/
>> and a site I manage for CORE and CORE system trust at:
>>    http://www.coresystemtrust.org.uk/
>> I have "semigrated" to France, see:
>>    https://www.psyctc.org/pelerinage2016/semigrating-to-france/
>>
>> https://www.psyctc.org/pelerinage2016/register-to-get-updates-from-pelerinage2016/
>>
>> If you want an Emeeting, I am trying to keep them to Thursdays and my
>> diary is at:
>>    https://www.psyctc.org/pelerinage2016/ceworkdiary/
>> Beware: French time, generally an hour ahead of UK.
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From Ju||@n@G@v|r|@Lopez @end|ng |rom un|ge@ch  Tue May  5 13:47:33 2020
From: Ju||@n@G@v|r|@Lopez @end|ng |rom un|ge@ch (Julian Gaviria Lopez)
Date: Tue, 5 May 2020 11:47:33 +0000
Subject: [R-sig-ME] Computing pair-wise associations of fixed effects in gLMM
Message-ID: <86ac6113d19846179cf1fe193a35652e@unige.ch>

Dear list members.


I have a nested data comprised by 2 factors (conditions: A and B). Each factor has 8 levels: (clusters: c1,c2,c3,c4,c5,c6,c7,c8). N=33. Aim: To assess the pairwise association between the factors (i.e. correlation between Ac1 and   Bc1, etc.).  Although an LMM will count for the dependent nature of the data (repeated measures of the 33 participants observed in condition A, and consecutively in B), some of the  dependent variables are not normally distributed (7 out of 16) according to the shapiro test.  For this reason, I think a gLMM might be a good option:


M <-glmer(observation~condition+cluster+(1|subject),data=mDATA,family="poisson")


Questions:


1) Would anyone is aware of a better option regarding the modelling method?

2) In case gLMM is the "right" way to go, I wonder how could I compute the pairwise correlations of the "fixed effects" (e.g. Ac1-Bc1;  Ac1-Bc2; ... Ac1-Bc8), with "glmer" function, or maybe with the glmmTMB?



Thanks in advance



Julian Gaviria
Neurology and Imaging of cognition lab (Labnic)
University of Geneva. Campus Biotech.
9 Chemin des Mines, 1202 Geneva, CH
Tel: +41 22 379 0380
Email: Julian.GaviriaLopez at unige.ch

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: mData_.txt
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20200505/df284224/attachment.txt>

From Ju||@n@G@v|r|@Lopez @end|ng |rom un|ge@ch  Wed May  6 13:40:37 2020
From: Ju||@n@G@v|r|@Lopez @end|ng |rom un|ge@ch (Julian Gaviria Lopez)
Date: Wed, 6 May 2020 11:40:37 +0000
Subject: [R-sig-ME] About computing pairwise associations (corr) of random
 effects in glmms
Message-ID: <de1321c922324173a224507f8cce77f9@unige.ch>


Dear Dr. Kristensen,


I aim to assess the association of the random effects computed by a glmm. I wondered whether it was possible to carry out the analysis with the glmmTMB package, then I found your vignet about the Covariance structures with glmmTMB. I know it is about the covariances that complete the model specification, therefore it is unrelated to my analysis. Nevertheless, I wonder whether you could suggest me one way to accomplish my goal:


I have nested data comprised by 2 factors (conditions: A and B). Each factor has 8 levels: (clusters: c1,c2,c3,c4,c5,c6,c7,c8). N=33. Aim: To assess the pairwise association between the factors Then, the syntax of the model would be as follows:


gmodel <- glmmTMB(observation ~ condition  + cluster (1|subject), data=mDATA, family=poisson)

Then I wonder how I could extract the pairwise correlation of the fixed effects (i.e. correlation between Ac1 and   Bc1, etc.). Is it feasible to run the analysis with glmmmTMB?

Thanks in advance for any comment on this regard.


P.d. sorry for any multiple posting. I sent this question to the r-sig-mixed-models list as well.

Julian Gaviria
Neurology and Imaging of cognition lab (Labnic)
University of Geneva. Campus Biotech.
9 Chemin des Mines, 1202 Geneva, CH
Tel: +41 22 379 0380
Email: Julian.GaviriaLopez at unige.ch

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: mData_.txt
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20200506/5ee4f96b/attachment.txt>

From @|j|@hu@ng @end|ng |rom uc|@@edu  Thu May  7 04:12:50 2020
From: @|j|@hu@ng @end|ng |rom uc|@@edu (Sijia Huang)
Date: Wed, 6 May 2020 19:12:50 -0700
Subject: [R-sig-ME] lme4 question
Message-ID: <CAATbvGuLs_s5zZcPha7ih6riLFVNb55R736q=YLQ9s6M569=_A@mail.gmail.com>

Hi,
I am using lme4 to fit cross-classified models, and I am wondering if it is
possible to fix the residual (associated with each observation) to a
certain value in lme4 -- analogous to the *hold* argument in SAS PROC.

Thank you so much! Looking forward to your reply!

Sincerely,
Sijia Huang
------------------------------------------------------------------------------------------------
Managing Editor
Chinese/English Journal of Educational Measurement and Evaluation

Ph.D. Student in Advanced Quantitative Methodology
Department of Education, University of California, Los Angeles
------------------------------------------------------------------------------------------------

	[[alternative HTML version deleted]]


From hu@ng@jcc @end|ng |rom gm@||@com  Thu May  7 07:24:00 2020
From: hu@ng@jcc @end|ng |rom gm@||@com (Sijia Huang)
Date: Wed, 6 May 2020 22:24:00 -0700
Subject: [R-sig-ME] How to fix residuals
Message-ID: <CAPmBuzGHstUxTi5L0=PkNzkF-TXgMP2vMo5WGp8zRgg_C-vN8Q@mail.gmail.com>

Hi,
I am using lme4 to fit cross-classified models, and I am wondering if it is
possible to fix the residual (associated with each observation) to a
certain value in lme4 -- analogous to the *hold* argument in SAS PROC.

Thank you so much! Looking forward to your reply!

Best,
Sijia

	[[alternative HTML version deleted]]


From @|j|@hu@ng @end|ng |rom uc|@@edu  Thu May  7 07:17:32 2020
From: @|j|@hu@ng @end|ng |rom uc|@@edu (Sijia Huang)
Date: Wed, 6 May 2020 22:17:32 -0700
Subject: [R-sig-ME] lme4 question
Message-ID: <CAATbvGtcZgo2dCajFr45QbDnBnJrDzU9pT3_gD7_DjCtH4LKYg@mail.gmail.com>

Hi,
I am using lme4 to fit cross-classified models, and I am wondering if it is
possible to fix the residual (associated with each observation) to a
certain value in lme4 -- analogous to the *hold* argument in SAS PROC.

Thank you so much! Looking forward to your reply!

Sincerely,
Sijia Huang
------------------------------------------------------------------------------------------------
Managing Editor
Chinese/English Journal of Educational Measurement and Evaluation

Ph.D. Student in Advanced Quantitative Methodology
Department of Education, University of California, Los Angeles
------------------------------------------------------------------------------------------------

	[[alternative HTML version deleted]]


From mo|||eebrook@ @end|ng |rom gm@||@com  Fri May  8 15:53:10 2020
From: mo|||eebrook@ @end|ng |rom gm@||@com (Mollie Brooks)
Date: Fri, 8 May 2020 15:53:10 +0200
Subject: [R-sig-ME] How to fix residuals
In-Reply-To: <CAPmBuzGHstUxTi5L0=PkNzkF-TXgMP2vMo5WGp8zRgg_C-vN8Q@mail.gmail.com>
References: <CAPmBuzGHstUxTi5L0=PkNzkF-TXgMP2vMo5WGp8zRgg_C-vN8Q@mail.gmail.com>
Message-ID: <B6B007C9-28C4-4D7B-947E-AF81D64C97FE@gmail.com>

It might be possible to do that with glmmTMB using the argument `start` to specify the values and using `map` to force them to stay at the starting values. The tricky part is figuring out what parameter to fix and at what values. Can you give a reproducible example so we have more to work with? 

cheers,
Mollie

> On 7May 2020, at 7:24, Sijia Huang <huangsjcc at gmail.com> wrote:
> 
> Hi,
> I am using lme4 to fit cross-classified models, and I am wondering if it is
> possible to fix the residual (associated with each observation) to a
> certain value in lme4 -- analogous to the *hold* argument in SAS PROC.
> 
> Thank you so much! Looking forward to your reply!
> 
> Best,
> Sijia
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From vdor|e @end|ng |rom gm@||@com  Fri May  8 16:43:36 2020
From: vdor|e @end|ng |rom gm@||@com (Vincent Dorie)
Date: Fri, 8 May 2020 10:43:36 -0400
Subject: [R-sig-ME] How to fix residuals
In-Reply-To: <B6B007C9-28C4-4D7B-947E-AF81D64C97FE@gmail.com>
References: <CAPmBuzGHstUxTi5L0=PkNzkF-TXgMP2vMo5WGp8zRgg_C-vN8Q@mail.gmail.com>
 <B6B007C9-28C4-4D7B-947E-AF81D64C97FE@gmail.com>
Message-ID: <CA+++UwRe73pS4kjM2wRSYy0+uzj_abawiW=D2RY-ak-BXBhKUA@mail.gmail.com>

Hi Sijia,

You might find the section in the GLMM FAQ helpful:
https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#setting-residual-variances-to-a-fixed-value-zero-or-other.
You can use those to fix the residual variance to 1, and then use a weights
vector if necessary to give every observation its own fixed variance.

Best,
Vince

On Fri, May 8, 2020 at 9:55 AM Mollie Brooks <mollieebrooks at gmail.com>
wrote:

> It might be possible to do that with glmmTMB using the argument `start` to
> specify the values and using `map` to force them to stay at the starting
> values. The tricky part is figuring out what parameter to fix and at what
> values. Can you give a reproducible example so we have more to work with?
>
> cheers,
> Mollie
>
> > On 7May 2020, at 7:24, Sijia Huang <huangsjcc at gmail.com> wrote:
> >
> > Hi,
> > I am using lme4 to fit cross-classified models, and I am wondering if it
> is
> > possible to fix the residual (associated with each observation) to a
> > certain value in lme4 -- analogous to the *hold* argument in SAS PROC.
> >
> > Thank you so much! Looking forward to your reply!
> >
> > Best,
> > Sijia
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From hu@ng@jcc @end|ng |rom gm@||@com  Sat May  9 20:00:16 2020
From: hu@ng@jcc @end|ng |rom gm@||@com (Sijia Huang)
Date: Sat, 9 May 2020 11:00:16 -0700
Subject: [R-sig-ME] How to fix residuals
In-Reply-To: <CA+++UwRe73pS4kjM2wRSYy0+uzj_abawiW=D2RY-ak-BXBhKUA@mail.gmail.com>
References: <CAPmBuzGHstUxTi5L0=PkNzkF-TXgMP2vMo5WGp8zRgg_C-vN8Q@mail.gmail.com>
 <B6B007C9-28C4-4D7B-947E-AF81D64C97FE@gmail.com>
 <CA+++UwRe73pS4kjM2wRSYy0+uzj_abawiW=D2RY-ak-BXBhKUA@mail.gmail.com>
Message-ID: <CAPmBuzEr-rqPd_fYqJGwM4WD1LB-_goHuFd4nm4J4n=bPKfMqQ@mail.gmail.com>

Thanks so much, Vince and Mollie!

Best,
Sijia



On Fri, May 8, 2020 at 7:43 AM Vincent Dorie <vdorie at gmail.com> wrote:

> Hi Sijia,
>
> You might find the section in the GLMM FAQ helpful:
> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#setting-residual-variances-to-a-fixed-value-zero-or-other.
> You can use those to fix the residual variance to 1, and then use a weights
> vector if necessary to give every observation its own fixed variance.
>
> Best,
> Vince
>
> On Fri, May 8, 2020 at 9:55 AM Mollie Brooks <mollieebrooks at gmail.com>
> wrote:
>
>> It might be possible to do that with glmmTMB using the argument `start`
>> to specify the values and using `map` to force them to stay at the starting
>> values. The tricky part is figuring out what parameter to fix and at what
>> values. Can you give a reproducible example so we have more to work with?
>>
>> cheers,
>> Mollie
>>
>> > On 7May 2020, at 7:24, Sijia Huang <huangsjcc at gmail.com> wrote:
>> >
>> > Hi,
>> > I am using lme4 to fit cross-classified models, and I am wondering if
>> it is
>> > possible to fix the residual (associated with each observation) to a
>> > certain value in lme4 -- analogous to the *hold* argument in SAS PROC.
>> >
>> > Thank you so much! Looking forward to your reply!
>> >
>> > Best,
>> > Sijia
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com  Mon May 11 23:42:40 2020
From: b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com (Alessandra Bielli)
Date: Mon, 11 May 2020 15:42:40 -0600
Subject: [R-sig-ME] correlation of fixed effects coefficients all close to
 +/-1
Message-ID: <CA+6N3yVO+vQ-LoVn6-68sEqv4OnCbT2L62C4q8SP41276c6CbA@mail.gmail.com>

Dear list,

I am fitting the mixed effect model:
 > lmer(log(percapita_day) ~ Type_residuo + (1|boatID), data=all)

 where percapita_day is a non-negative continuous response variable (on the
log scale to have residuals normally distributed), Type_residuo is a
categorical explanatory variable and boatID is a random effect with 4
levels.

I have found values very close to +/-1 in the correlation of fixed effects
matrix below, and after some research I learnt that the coefficients are
not about the correlation of the variables but the expected correlation of
the regression coefficients.

Correlation of Fixed Effects:
            (Intr) Tp_rsM Tp_rsdOr Tp_rsdOt Tp_Pyc Tp_rsP Tp_rsR
Type_rsdMtl -0.944
Tp_rsdOrgnc -0.951  0.945
Typ_rsdOtrs -0.959  0.953  0.959
Tp_rsdPplyc -0.926  0.919  0.925    0.933
Tp_rsdPlstc -0.951  0.945  0.951    0.958    0.925
Type_resdRd -0.870  0.867  0.873    0.878    0.850  0.872
Type_rsdVdr -0.954  0.949  0.955    0.962    0.928  0.954  0.876

However I still can't explain why all coefficients are so close to +/-1 and
I was wondering if these are indicators that something is wrong with my
model?
Is that due to the presence of outlayers in the response variable (see
attached)?

Thanks,

Alessandra

From hu@ng@jcc @end|ng |rom gm@||@com  Tue May 12 00:40:15 2020
From: hu@ng@jcc @end|ng |rom gm@||@com (Sijia Huang)
Date: Mon, 11 May 2020 15:40:15 -0700
Subject: [R-sig-ME] Fixing residual variance in lme4
Message-ID: <CAPmBuzFB4NgmfNzyUOU8=yRMOPX_-g4h7J-czMsrU0JWnrvaRw@mail.gmail.com>

Hi everyone,

I am trying to fit a crossed random effect model with lme4. I want the
residual variance fixed to be 1. Below is the code I use. I am wondering
why the sigma=1 argument does not work as it does in the nlme package. Or
if there is another to fix the residual variance. Thanks so much!

> example<- lmer(formula = g ~ 0 + (1|Study) + (1|Subscale),
+                data = meta, weights=Precision,
+                control=lmerControl(optimizer="bobyqa",sigma=1))

Error in lmerControl(optimizer = "bobyqa", sigma = 1) :
  unused argument (sigma = 1)



Best,
Sijia

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Tue May 12 00:56:39 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 11 May 2020 18:56:39 -0400
Subject: [R-sig-ME] Fixing residual variance in lme4
In-Reply-To: <CAPmBuzFB4NgmfNzyUOU8=yRMOPX_-g4h7J-czMsrU0JWnrvaRw@mail.gmail.com>
References: <CAPmBuzFB4NgmfNzyUOU8=yRMOPX_-g4h7J-czMsrU0JWnrvaRw@mail.gmail.com>
Message-ID: <d81dedc7-e08e-c185-9242-361b4e26bbfe@gmail.com>

 ??? You need to use the blme package.? See 
https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#setting-residual-variances-to-a-fixed-value-zero-or-other

On 5/11/20 6:40 PM, Sijia Huang wrote:
> Hi everyone,
>
> I am trying to fit a crossed random effect model with lme4. I want the
> residual variance fixed to be 1. Below is the code I use. I am wondering
> why the sigma=1 argument does not work as it does in the nlme package. Or
> if there is another to fix the residual variance. Thanks so much!
>
>> example<- lmer(formula = g ~ 0 + (1|Study) + (1|Subscale),
> +                data = meta, weights=Precision,
> +                control=lmerControl(optimizer="bobyqa",sigma=1))
>
> Error in lmerControl(optimizer = "bobyqa", sigma = 1) :
>    unused argument (sigma = 1)
>
>
>
> Best,
> Sijia
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From bbo|ker @end|ng |rom gm@||@com  Tue May 12 16:26:32 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Tue, 12 May 2020 10:26:32 -0400
Subject: [R-sig-ME] webinar of possible interest
Message-ID: <ada72858-4f9e-063f-1a11-bbe7b219e503@gmail.com>

 ? Of possible interest: it doesn't say but I assume this is British 
Summer time (UTC + 1 hour)

https://twitter.com/RoyalStatSoc/status/1260211995774717952

We're hosting an online interactive Discussion Meeting tomorrow at 4pm. 
The paper, ?Linear mixed effects models for non-Gaussian continuous 
repeated measurement data? will be presented by authors Ozgur Asar, 
David Bolin, Peter J Diggle and Jonas Wallin.

https://rss.org.uk/training-events/events/key-events/discussion-papers/


	[[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Wed May 13 17:07:38 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Wed, 13 May 2020 10:07:38 -0500
Subject: [R-sig-ME] Multi-level Rasch Model Per Douglas Bates' paper
Message-ID: <CACgv6yXKgZXxTGwu_PrnFRFFkseJfby4N8mJyKi37m3D=-cNkA@mail.gmail.com>

Hi All!
I'm following this paper <https://www.jstatsoft.org/article/view/v020i02> (
https://www.jstatsoft.org/article/view/v020i02) by Prof. Bates where after
fitting the model (*pp. 14-15*), they obtain what they call *item
easiness* *"from
the estimates of the fixed effects and the conditional modes of the random
effects."*

In short, I wonder how to obtain item easiness estimates for each of my
models (m1 & m2) below? *Thank you, Simon*

library(glmmTMB)
dat <- read.csv('https://raw.githubusercontent.com/ilzl/i/master/d.csv')

form11 <- y ~ item_type + (1 | item_id) + (1 | person_id)

form22 <- y ~ item_type + gender + (1 | item_id) + (1 | person_id)


m1 <-  glmmTMB(form11, data = subset(dat, person_id <= 40),
                       family = beta_family())

m2 <-  glmmTMB(form22, data = subset(dat, person_id <= 40),
            family = beta_family())

	[[alternative HTML version deleted]]


From ph||||p@@|d@y @end|ng |rom mp|@n|  Wed May 13 17:28:11 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Wed, 13 May 2020 17:28:11 +0200
Subject: [R-sig-ME] Multi-level Rasch Model Per Douglas Bates' paper
In-Reply-To: <CACgv6yXKgZXxTGwu_PrnFRFFkseJfby4N8mJyKi37m3D=-cNkA@mail.gmail.com>
References: <CACgv6yXKgZXxTGwu_PrnFRFFkseJfby4N8mJyKi37m3D=-cNkA@mail.gmail.com>
Message-ID: <8dad96d1-ff79-ac26-de98-b5cc36323b53@mpi.nl>

I think I saw this go past on CrossValidated -- you should mention any
crossposting. :)

In general, it would be nice to know what the structure of your data
are. Is "gender" a property of your participants, items, or something
else? What about item_type?

In lme4, you can extract the item-level predictions with coef(m) (which
is the same as ranef(m) + fixef(m)).? You can even get a plot of these with:

library(lattice)

dotplot(ranef(m, condVar=TRUE))

The zero-point is the grand mean (i.e. the corresponding fixed effect).

I don't know if this is the same as in glmmTMB.

Best,

Phillip

On 13/5/20 5:07 pm, Simon Harmel wrote:
> Hi All!
> I'm following this paper <https://www.jstatsoft.org/article/view/v020i02> (
> https://www.jstatsoft.org/article/view/v020i02) by Prof. Bates where after
> fitting the model (*pp. 14-15*), they obtain what they call *item
> easiness* *"from
> the estimates of the fixed effects and the conditional modes of the random
> effects."*
>
> In short, I wonder how to obtain item easiness estimates for each of my
> models (m1 & m2) below? *Thank you, Simon*
>
> library(glmmTMB)
> dat <- read.csv('https://raw.githubusercontent.com/ilzl/i/master/d.csv')
>
> form11 <- y ~ item_type + (1 | item_id) + (1 | person_id)
>
> form22 <- y ~ item_type + gender + (1 | item_id) + (1 | person_id)
>
>
> m1 <-  glmmTMB(form11, data = subset(dat, person_id <= 40),
>                        family = beta_family())
>
> m2 <-  glmmTMB(form22, data = subset(dat, person_id <= 40),
>             family = beta_family())
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From bbo|ker @end|ng |rom gm@||@com  Wed May 13 17:54:14 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 13 May 2020 11:54:14 -0400
Subject: [R-sig-ME] Multi-level Rasch Model Per Douglas Bates' paper
In-Reply-To: <8dad96d1-ff79-ac26-de98-b5cc36323b53@mpi.nl>
References: <CACgv6yXKgZXxTGwu_PrnFRFFkseJfby4N8mJyKi37m3D=-cNkA@mail.gmail.com>
 <8dad96d1-ff79-ac26-de98-b5cc36323b53@mpi.nl>
Message-ID: <8e3a7aec-e1ec-453a-dbdd-457a183db72a@gmail.com>

 ?? Yes, this is cross-posted, and I was planning on getting around to 
showing how to do it in the Cross-Validated post. glmmTMB doesn't have a 
built-in dotplot method, but you can cheat pretty easily because the 
individual components of a ranef() extracted from a glmmTMB fit ($cond 
for conditional model, $zi for zero-inflation model if any) have the 
same structure as ranef() from lme4, so you can steal the plotting method:

library(glmmTMB)
example(glmmTMB)
library(lme4)
r <- ranef(m1)
library(lattice)
lme4:::dotplot.ranef.mer(r$cond)

 ? Note that it's not as easy to get dotplots with whiskers from coef() 
because of some long-standing (and deep) issues with computing standard 
deviations for the sum of a fixed and a random effect ...

On 5/13/20 11:28 AM, Phillip Alday wrote:
> I think I saw this go past on CrossValidated -- you should mention any
> crossposting. :)
>
> In general, it would be nice to know what the structure of your data
> are. Is "gender" a property of your participants, items, or something
> else? What about item_type?
>
> In lme4, you can extract the item-level predictions with coef(m) (which
> is the same as ranef(m) + fixef(m)).? You can even get a plot of these with:
>
> library(lattice)
>
> dotplot(ranef(m, condVar=TRUE))
>
> The zero-point is the grand mean (i.e. the corresponding fixed effect).
>
> I don't know if this is the same as in glmmTMB.
>
> Best,
>
> Phillip
>
> On 13/5/20 5:07 pm, Simon Harmel wrote:
>> Hi All!
>> I'm following this paper <https://www.jstatsoft.org/article/view/v020i02> (
>> https://www.jstatsoft.org/article/view/v020i02) by Prof. Bates where after
>> fitting the model (*pp. 14-15*), they obtain what they call *item
>> easiness* *"from
>> the estimates of the fixed effects and the conditional modes of the random
>> effects."*
>>
>> In short, I wonder how to obtain item easiness estimates for each of my
>> models (m1 & m2) below? *Thank you, Simon*
>>
>> library(glmmTMB)
>> dat <- read.csv('https://raw.githubusercontent.com/ilzl/i/master/d.csv')
>>
>> form11 <- y ~ item_type + (1 | item_id) + (1 | person_id)
>>
>> form22 <- y ~ item_type + gender + (1 | item_id) + (1 | person_id)
>>
>>
>> m1 <-  glmmTMB(form11, data = subset(dat, person_id <= 40),
>>                         family = beta_family())
>>
>> m2 <-  glmmTMB(form22, data = subset(dat, person_id <= 40),
>>              family = beta_family())
>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From @|m@h@rme| @end|ng |rom gm@||@com  Wed May 13 18:09:41 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Wed, 13 May 2020 11:09:41 -0500
Subject: [R-sig-ME] Multi-level Rasch Model Per Douglas Bates' paper
In-Reply-To: <8e3a7aec-e1ec-453a-dbdd-457a183db72a@gmail.com>
References: <CACgv6yXKgZXxTGwu_PrnFRFFkseJfby4N8mJyKi37m3D=-cNkA@mail.gmail.com>
 <8dad96d1-ff79-ac26-de98-b5cc36323b53@mpi.nl>
 <8e3a7aec-e1ec-453a-dbdd-457a183db72a@gmail.com>
Message-ID: <CACgv6yWHjqzpmdx0RE499XWXpX5HUeRdKgC2Yt7sS4_5MgJfFw@mail.gmail.com>

Dear Phillip,

The post you saw was about the "model" itself not the question I have asked
here:)

Dear Ben,
Thank you:) as you know my data is huge and won't fit any plot. I simply
want to make sure that "*item easiness estimates*" and "*person ability
estimates*" are correctly obtained in the following way for each model
given Prof. Bates' paper:


r11 <- ranef(m1, condVar = TRUE) # for 'm1' model in my original question
in this thread
r22 <- ranef(m2, condVar = TRUE)  # for 'm2' model in my original question
in this thread

### Person abilities and item easinesses for 'm1'
person_abilities11 <- r11$cond$person_id$`(Intercept)`
item_easiness11 <- r11$cond$item_id$`(Intercept)`

### Person abilities and item easinesses for 'm2'
person_abilities22 <- r22$cond$person_id$`(Intercept)`
item_easiness22 <- r22$cond$item_id$`(Intercept)`


On Wed, May 13, 2020 at 10:54 AM Ben Bolker <bbolker at gmail.com> wrote:

>     Yes, this is cross-posted, and I was planning on getting around to
> showing how to do it in the Cross-Validated post. glmmTMB doesn't have a
> built-in dotplot method, but you can cheat pretty easily because the
> individual components of a ranef() extracted from a glmmTMB fit ($cond
> for conditional model, $zi for zero-inflation model if any) have the
> same structure as ranef() from lme4, so you can steal the plotting method:
>
> library(glmmTMB)
> example(glmmTMB)
> library(lme4)
> r <- ranef(m1)
> library(lattice)
> lme4:::dotplot.ranef.mer(r$cond)
>
>    Note that it's not as easy to get dotplots with whiskers from coef()
> because of some long-standing (and deep) issues with computing standard
> deviations for the sum of a fixed and a random effect ...
>
> On 5/13/20 11:28 AM, Phillip Alday wrote:
> > I think I saw this go past on CrossValidated -- you should mention any
> > crossposting. :)
> >
> > In general, it would be nice to know what the structure of your data
> > are. Is "gender" a property of your participants, items, or something
> > else? What about item_type?
> >
> > In lme4, you can extract the item-level predictions with coef(m) (which
> > is the same as ranef(m) + fixef(m)).  You can even get a plot of these
> with:
> >
> > library(lattice)
> >
> > dotplot(ranef(m, condVar=TRUE))
> >
> > The zero-point is the grand mean (i.e. the corresponding fixed effect).
> >
> > I don't know if this is the same as in glmmTMB.
> >
> > Best,
> >
> > Phillip
> >
> > On 13/5/20 5:07 pm, Simon Harmel wrote:
> >> Hi All!
> >> I'm following this paper <
> https://www.jstatsoft.org/article/view/v020i02> (
> >> https://www.jstatsoft.org/article/view/v020i02) by Prof. Bates where
> after
> >> fitting the model (*pp. 14-15*), they obtain what they call *item
> >> easiness* *"from
> >> the estimates of the fixed effects and the conditional modes of the
> random
> >> effects."*
> >>
> >> In short, I wonder how to obtain item easiness estimates for each of my
> >> models (m1 & m2) below? *Thank you, Simon*
> >>
> >> library(glmmTMB)
> >> dat <- read.csv('https://raw.githubusercontent.com/ilzl/i/master/d.csv
> ')
> >>
> >> form11 <- y ~ item_type + (1 | item_id) + (1 | person_id)
> >>
> >> form22 <- y ~ item_type + gender + (1 | item_id) + (1 | person_id)
> >>
> >>
> >> m1 <-  glmmTMB(form11, data = subset(dat, person_id <= 40),
> >>                         family = beta_family())
> >>
> >> m2 <-  glmmTMB(form22, data = subset(dat, person_id <= 40),
> >>              family = beta_family())
> >>
> >>      [[alternative HTML version deleted]]
> >>
> >> _______________________________________________
> >> R-sig-mixed-models at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From jr@| @end|ng |rom po@teo@no  Wed May 13 18:35:31 2020
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Wed, 13 May 2020 18:35:31 +0200
Subject: [R-sig-ME] Multi-level Rasch Model Per Douglas Bates' paper
In-Reply-To: <8dad96d1-ff79-ac26-de98-b5cc36323b53@mpi.nl>
References: <CACgv6yXKgZXxTGwu_PrnFRFFkseJfby4N8mJyKi37m3D=-cNkA@mail.gmail.com>
 <8dad96d1-ff79-ac26-de98-b5cc36323b53@mpi.nl>
Message-ID: <20200513163531.GB1776058@posteo.no>

On 2020-05-13 17:28 +0200, Phillip Alday wrote:
> On 13/5/20 5:07 pm, Simon Harmel wrote:
> > Hi All!
> > I'm following this paper 
> > https://www.jstatsoft.org/article/view/v020i02 
> > by Prof. Bates where after fitting the 
> > model (*pp. 14-15*), they obtain what 
> > they call *item easiness* *"from the 
> > estimates of the fixed effects and the 
> > conditional modes of the random 
> > effects."*
> 

Dear Simon,

I was not even able to get past subsection 3.2 ...
how reproducible even are the examples in 
this text?

Best,
Rasmus

data("lq2002", package="multilevel")
wrk <- lq2002
# wrk[1:5,]
for (i in 3:16) wrk[[i]] <- ordered(wrk[[i]])
for (i in 17:21) wrk[[i]] <- ordered(5 - wrk[[i]])
lql <- reshape(wrk,
  varying = list(names(lq2002)[3:21]),
  v.names = "fivelev",
  idvar = "subj",
  timevar = "item",
  drop = names(lq2002)[c(2, 22:27)],
  direction = "long")
lql$itype <-
  with(lql, factor(ifelse(item < 12, "Leadership",
    ifelse(item < 15, "Task Sig.", "Hostility")
  )))
for (i in c(1, 2, 4, 5)) lql[[i]] <- factor(lql[[i]])
lql$dichot <- factor(ifelse(lql$fivelev < 4, 0, 1))
# str(lql)
# summary(lql)

## 3.2 Fitting an initial multilevel Rasch model
(fm1 <- lme4::lmer(
  dichot ~ 0 + itype + (1 | subj) + (1 | COMPID) + (1 | item),
  lql,
  binomial))
Error in mkRespMod(fr, REML = REMLpass) : response must be numeric
Calls: <Anonymous> -> do.call -> <Anonymous> -> mkRespMod
Execution halted


From bbo|ker @end|ng |rom gm@||@com  Wed May 13 18:41:44 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 13 May 2020 12:41:44 -0400
Subject: [R-sig-ME] Multi-level Rasch Model Per Douglas Bates' paper
In-Reply-To: <20200513163531.GB1776058@posteo.no>
References: <CACgv6yXKgZXxTGwu_PrnFRFFkseJfby4N8mJyKi37m3D=-cNkA@mail.gmail.com>
 <8dad96d1-ff79-ac26-de98-b5cc36323b53@mpi.nl>
 <20200513163531.GB1776058@posteo.no>
Message-ID: <7cf511fe-4c06-0992-9061-c3b6e38e065b@gmail.com>

 ?? If you change that last line to lme4::glmer() it works fine.

 ?? In the early days lmer() specified with a family argument would 
automatically convert (internally) to a glmer call, but we shut that off 
in a recent release.

On 5/13/20 12:35 PM, Rasmus Liland wrote:
> On 2020-05-13 17:28 +0200, Phillip Alday wrote:
>> On 13/5/20 5:07 pm, Simon Harmel wrote:
>>> Hi All!
>>> I'm following this paper
>>> https://www.jstatsoft.org/article/view/v020i02
>>> by Prof. Bates where after fitting the
>>> model (*pp. 14-15*), they obtain what
>>> they call *item easiness* *"from the
>>> estimates of the fixed effects and the
>>> conditional modes of the random
>>> effects."*
> Dear Simon,
>
> I was not even able to get past subsection 3.2 ...
> how reproducible even are the examples in
> this text?
>
> Best,
> Rasmus
>
> data("lq2002", package="multilevel")
> wrk <- lq2002
> # wrk[1:5,]
> for (i in 3:16) wrk[[i]] <- ordered(wrk[[i]])
> for (i in 17:21) wrk[[i]] <- ordered(5 - wrk[[i]])
> lql <- reshape(wrk,
>    varying = list(names(lq2002)[3:21]),
>    v.names = "fivelev",
>    idvar = "subj",
>    timevar = "item",
>    drop = names(lq2002)[c(2, 22:27)],
>    direction = "long")
> lql$itype <-
>    with(lql, factor(ifelse(item < 12, "Leadership",
>      ifelse(item < 15, "Task Sig.", "Hostility")
>    )))
> for (i in c(1, 2, 4, 5)) lql[[i]] <- factor(lql[[i]])
> lql$dichot <- factor(ifelse(lql$fivelev < 4, 0, 1))
> # str(lql)
> # summary(lql)
>
> ## 3.2 Fitting an initial multilevel Rasch model
> (fm1 <- lme4::lmer(
>    dichot ~ 0 + itype + (1 | subj) + (1 | COMPID) + (1 | item),
>    lql,
>    binomial))
> Error in mkRespMod(fr, REML = REMLpass) : response must be numeric
> Calls: <Anonymous> -> do.call -> <Anonymous> -> mkRespMod
> Execution halted
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From jr@| @end|ng |rom po@teo@no  Wed May 13 18:50:13 2020
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Wed, 13 May 2020 18:50:13 +0200
Subject: [R-sig-ME] Multi-level Rasch Model Per Douglas Bates' paper
In-Reply-To: <7cf511fe-4c06-0992-9061-c3b6e38e065b@gmail.com>
References: <CACgv6yXKgZXxTGwu_PrnFRFFkseJfby4N8mJyKi37m3D=-cNkA@mail.gmail.com>
 <8dad96d1-ff79-ac26-de98-b5cc36323b53@mpi.nl>
 <20200513163531.GB1776058@posteo.no>
 <7cf511fe-4c06-0992-9061-c3b6e38e065b@gmail.com>
Message-ID: <20200513165013.GC1776058@posteo.no>

On 2020-05-13 12:41 -0400, Ben Bolker wrote:
> If you change that last line to 
> lme4::glmer() it works fine.
> 
> In the early days lmer() specified with a 
> family argument would automatically convert 
> (internally) to a glmer call, but we shut 
> that off in a recent release.

Indeed it does work now!  Thanks!


From @|m@h@rme| @end|ng |rom gm@||@com  Wed May 13 19:10:18 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Wed, 13 May 2020 12:10:18 -0500
Subject: [R-sig-ME] Multi-level Rasch Model Per Douglas Bates' paper
In-Reply-To: <20200513165013.GC1776058@posteo.no>
References: <CACgv6yXKgZXxTGwu_PrnFRFFkseJfby4N8mJyKi37m3D=-cNkA@mail.gmail.com>
 <8dad96d1-ff79-ac26-de98-b5cc36323b53@mpi.nl>
 <20200513163531.GB1776058@posteo.no>
 <7cf511fe-4c06-0992-9061-c3b6e38e065b@gmail.com>
 <20200513165013.GC1776058@posteo.no>
Message-ID: <CACgv6yX829x1w40Rj3kp9r4oZ0aV9ETqTJvguKT4x05GG6mo+w@mail.gmail.com>

To be clear,  from each model in my original post in this thread (i.e., m1
& m2) I just want a set of data.frames containing difficulty of each item
per its 'id', and 'item_type'?

Also, I want a set of data.frames containing person ability of each person
per their 'id', and 'item_type'?

Many thanks,
Simon


On Wed, May 13, 2020, 11:59 AM Rasmus Liland <jral at posteo.no> wrote:

> On 2020-05-13 12:41 -0400, Ben Bolker wrote:
> > If you change that last line to
> > lme4::glmer() it works fine.
> >
> > In the early days lmer() specified with a
> > family argument would automatically convert
> > (internally) to a glmer call, but we shut
> > that off in a recent release.
>
> Indeed it does work now!  Thanks!
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From jr@| @end|ng |rom po@teo@no  Wed May 13 19:25:02 2020
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Wed, 13 May 2020 19:25:02 +0200
Subject: [R-sig-ME] Multi-level Rasch Model Per Douglas Bates' paper
In-Reply-To: <20200513165013.GC1776058@posteo.no>
References: <CACgv6yXKgZXxTGwu_PrnFRFFkseJfby4N8mJyKi37m3D=-cNkA@mail.gmail.com>
 <8dad96d1-ff79-ac26-de98-b5cc36323b53@mpi.nl>
 <20200513163531.GB1776058@posteo.no>
 <7cf511fe-4c06-0992-9061-c3b6e38e065b@gmail.com>
 <20200513165013.GC1776058@posteo.no>
Message-ID: <20200513172502.GD1776058@posteo.no>

On 2020-05-13 18:50 +0200, Rasmus Liland wrote:
> Indeed it does work now!  Thanks!

Right, so this code reproduces code until the 
easiness variable on page 15.  Perhaps this 
is useful?

data("lq2002", package="multilevel")
wrk <- lq2002
# wrk[1:5,]
for (i in 3:16) wrk[[i]] <- ordered(wrk[[i]])
for (i in 17:21) wrk[[i]] <- ordered(5 - wrk[[i]])
lql <- reshape(wrk,
  varying = list(names(lq2002)[3:21]),
  v.names = "fivelev",
  idvar = "subj",
  timevar = "item",
  drop = names(lq2002)[c(2, 22:27)],
  direction = "long")
lql$itype <-
  with(lql, factor(ifelse(item < 12, "Leadership",
    ifelse(item < 15, "Task Sig.", "Hostility")
  )))
for (i in c(1, 2, 4, 5)) lql[[i]] <- factor(lql[[i]])
lql$dichot <- factor(ifelse(lql$fivelev < 4, 0, 1))
# str(lql)
# summary(lql)

## 3.2 Fitting an initial multilevel Rasch model
(fm1 <- lme4::glmer(
  dichot ~ 0 + itype + (1 | subj) +
    (1 | COMPID) +
    (1 | item),
  lql,
  binomial))
rr <- lme4::ranef(fm1, condVar = TRUE)
str(rr$COMPID)
head(rr$COMPID)

qq <- lattice::qqmath(rr)
print(qq$subj)

## 3.3 Allowing for interactions of company and item type
(fm2 <- lme4::glmer(
  dichot ~ 0 + itype + (1 | subj) +
    (0 + itype | COMPID) +
    (1 | item),
  lql,
  binomial))

(fm3 <- lme4::glmer(
  dichot ~ 0 + itype + (1 | subj) +
    (1 | COMPID:itype) +
    (1 | item),
  lql,
  binomial))

(fm3a <- lme4::glmer(
  dichot ~ 0 + itype + (1 | subj) +
    (1 | COMPID:itype) +
    (1 | COMPID) +
    (1 | item),
  lql,
  binomial))

anova(fm3, fm3a, fm2)

str(imap <- unique(lql[, c("itype", "item")]))

(easiness <-
  lme4::ranef(fm2)$item[[1]] +
  glmmTMB::fixef(fm2)[imap$itype])

Best,
Rasmus


From bbo|ker @end|ng |rom gm@||@com  Wed May 13 19:32:37 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 13 May 2020 13:32:37 -0400
Subject: [R-sig-ME] Multi-level Rasch Model Per Douglas Bates' paper
In-Reply-To: <20200513172502.GD1776058@posteo.no>
References: <CACgv6yXKgZXxTGwu_PrnFRFFkseJfby4N8mJyKi37m3D=-cNkA@mail.gmail.com>
 <8dad96d1-ff79-ac26-de98-b5cc36323b53@mpi.nl>
 <20200513163531.GB1776058@posteo.no>
 <7cf511fe-4c06-0992-9061-c3b6e38e065b@gmail.com>
 <20200513165013.GC1776058@posteo.no> <20200513172502.GD1776058@posteo.no>
Message-ID: <b4ae839e-e2ce-6860-bce2-b6b5441130d7@gmail.com>

 ?? Apologies for not looking this over in great detail, but not sure 
why you're mixing lme4 and glmmTMB here??? Isn't item easiness just 
lme4::coef(fm2)$item ?

easiness <-
   lme4::ranef(fm2)$item[[1]] +
   glmmTMB::fixef(fm2)[imap$itype])

On 5/13/20 1:25 PM, Rasmus Liland wrote:
> On 2020-05-13 18:50 +0200, Rasmus Liland wrote:
>> Indeed it does work now!  Thanks!
> Right, so this code reproduces code until the
> easiness variable on page 15.  Perhaps this
> is useful?
>
> data("lq2002", package="multilevel")
> wrk <- lq2002
> # wrk[1:5,]
> for (i in 3:16) wrk[[i]] <- ordered(wrk[[i]])
> for (i in 17:21) wrk[[i]] <- ordered(5 - wrk[[i]])
> lql <- reshape(wrk,
>    varying = list(names(lq2002)[3:21]),
>    v.names = "fivelev",
>    idvar = "subj",
>    timevar = "item",
>    drop = names(lq2002)[c(2, 22:27)],
>    direction = "long")
> lql$itype <-
>    with(lql, factor(ifelse(item < 12, "Leadership",
>      ifelse(item < 15, "Task Sig.", "Hostility")
>    )))
> for (i in c(1, 2, 4, 5)) lql[[i]] <- factor(lql[[i]])
> lql$dichot <- factor(ifelse(lql$fivelev < 4, 0, 1))
> # str(lql)
> # summary(lql)
>
> ## 3.2 Fitting an initial multilevel Rasch model
> (fm1 <- lme4::glmer(
>    dichot ~ 0 + itype + (1 | subj) +
>      (1 | COMPID) +
>      (1 | item),
>    lql,
>    binomial))
> rr <- lme4::ranef(fm1, condVar = TRUE)
> str(rr$COMPID)
> head(rr$COMPID)
>
> qq <- lattice::qqmath(rr)
> print(qq$subj)
>
> ## 3.3 Allowing for interactions of company and item type
> (fm2 <- lme4::glmer(
>    dichot ~ 0 + itype + (1 | subj) +
>      (0 + itype | COMPID) +
>      (1 | item),
>    lql,
>    binomial))
>
> (fm3 <- lme4::glmer(
>    dichot ~ 0 + itype + (1 | subj) +
>      (1 | COMPID:itype) +
>      (1 | item),
>    lql,
>    binomial))
>
> (fm3a <- lme4::glmer(
>    dichot ~ 0 + itype + (1 | subj) +
>      (1 | COMPID:itype) +
>      (1 | COMPID) +
>      (1 | item),
>    lql,
>    binomial))
>
> anova(fm3, fm3a, fm2)
>
> str(imap <- unique(lql[, c("itype", "item")]))
>
> (easiness <-
>    lme4::ranef(fm2)$item[[1]] +
>    glmmTMB::fixef(fm2)[imap$itype])
>
> Best,
> Rasmus
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From jr@| @end|ng |rom po@teo@no  Wed May 13 19:44:59 2020
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Wed, 13 May 2020 19:44:59 +0200
Subject: [R-sig-ME] Multi-level Rasch Model Per Douglas Bates' paper
In-Reply-To: <b4ae839e-e2ce-6860-bce2-b6b5441130d7@gmail.com>
References: <CACgv6yXKgZXxTGwu_PrnFRFFkseJfby4N8mJyKi37m3D=-cNkA@mail.gmail.com>
 <8dad96d1-ff79-ac26-de98-b5cc36323b53@mpi.nl>
 <20200513163531.GB1776058@posteo.no>
 <7cf511fe-4c06-0992-9061-c3b6e38e065b@gmail.com>
 <20200513165013.GC1776058@posteo.no>
 <20200513172502.GD1776058@posteo.no>
 <b4ae839e-e2ce-6860-bce2-b6b5441130d7@gmail.com>
Message-ID: <20200513174459.GE1776058@posteo.no>

On 2020-05-13 13:32 -0400, Ben Bolker wrote:
> Apologies for not looking this over in 
> great detail, but not sure why you're 
> mixing lme4 and glmmTMB here??

Sorry, I am not so familiar with all these 
functions ...

Perhaps you mean using glmmTMB::ranef.glmmTMB 
instead of lme4::ranef?  

The easiness becomes similar to page 15:

itypeLeadership itypeLeadership itypeLeadership itypeLeadership 
    -0.39327704      0.35181351     -1.37287890     -0.66120397 
itypeLeadership itypeLeadership itypeLeadership itypeLeadership 
    -1.05406831      0.19744727     -0.81676266      0.35181351 
itypeLeadership itypeLeadership itypeLeadership  itypeTask Sig. 
    -1.17155598     -0.01968142     -0.72344875     -0.70175382 
 itypeTask Sig.  itypeTask Sig.  itypeHostility  itypeHostility 
     0.10980934      0.17308785      0.57989244      2.35224505 
 itypeHostility  itypeHostility  itypeHostility 
     1.34381476      1.64930542      2.37480280

> Isn't item easiness just 
> lme4::coef(fm2)$item?

Then I get this error: 

Error: 'coef' is not an exported object from 'namespace:lme4'
Execution halted

Best,
Rasmus


From @|m@h@rme| @end|ng |rom gm@||@com  Wed May 13 19:43:50 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Wed, 13 May 2020 12:43:50 -0500
Subject: [R-sig-ME] Multi-level Rasch Model Per Douglas Bates' paper
In-Reply-To: <b4ae839e-e2ce-6860-bce2-b6b5441130d7@gmail.com>
References: <CACgv6yXKgZXxTGwu_PrnFRFFkseJfby4N8mJyKi37m3D=-cNkA@mail.gmail.com>
 <8dad96d1-ff79-ac26-de98-b5cc36323b53@mpi.nl>
 <20200513163531.GB1776058@posteo.no>
 <7cf511fe-4c06-0992-9061-c3b6e38e065b@gmail.com>
 <20200513165013.GC1776058@posteo.no>
 <20200513172502.GD1776058@posteo.no>
 <b4ae839e-e2ce-6860-bce2-b6b5441130d7@gmail.com>
Message-ID: <CACgv6yWF+d1U=wkr6BWxMSN1MUwHFDSuw+MVQDbD_CDodQNguA@mail.gmail.com>

Ben,

This is exactly what I'm trying to understand for my glmmTMB models in my
original post ('m1' & 'm2').

So to get the item easiness I think we need to use: *easiness *<-
*coef(f11)[[1]][[1]][1]*
*
        persons <- coef(f11)[[1]][[2]][1]*

Am I right Ben?

On Wed, May 13, 2020 at 12:32 PM Ben Bolker <bbolker at gmail.com> wrote:

>     Apologies for not looking this over in great detail, but not sure
> why you're mixing lme4 and glmmTMB here??  Isn't item easiness just
> lme4::coef(fm2)$item ?
>
> easiness <-
>    lme4::ranef(fm2)$item[[1]] +
>    glmmTMB::fixef(fm2)[imap$itype])
>
> On 5/13/20 1:25 PM, Rasmus Liland wrote:
> > On 2020-05-13 18:50 +0200, Rasmus Liland wrote:
> >> Indeed it does work now!  Thanks!
> > Right, so this code reproduces code until the
> > easiness variable on page 15.  Perhaps this
> > is useful?
> >
> > data("lq2002", package="multilevel")
> > wrk <- lq2002
> > # wrk[1:5,]
> > for (i in 3:16) wrk[[i]] <- ordered(wrk[[i]])
> > for (i in 17:21) wrk[[i]] <- ordered(5 - wrk[[i]])
> > lql <- reshape(wrk,
> >    varying = list(names(lq2002)[3:21]),
> >    v.names = "fivelev",
> >    idvar = "subj",
> >    timevar = "item",
> >    drop = names(lq2002)[c(2, 22:27)],
> >    direction = "long")
> > lql$itype <-
> >    with(lql, factor(ifelse(item < 12, "Leadership",
> >      ifelse(item < 15, "Task Sig.", "Hostility")
> >    )))
> > for (i in c(1, 2, 4, 5)) lql[[i]] <- factor(lql[[i]])
> > lql$dichot <- factor(ifelse(lql$fivelev < 4, 0, 1))
> > # str(lql)
> > # summary(lql)
> >
> > ## 3.2 Fitting an initial multilevel Rasch model
> > (fm1 <- lme4::glmer(
> >    dichot ~ 0 + itype + (1 | subj) +
> >      (1 | COMPID) +
> >      (1 | item),
> >    lql,
> >    binomial))
> > rr <- lme4::ranef(fm1, condVar = TRUE)
> > str(rr$COMPID)
> > head(rr$COMPID)
> >
> > qq <- lattice::qqmath(rr)
> > print(qq$subj)
> >
> > ## 3.3 Allowing for interactions of company and item type
> > (fm2 <- lme4::glmer(
> >    dichot ~ 0 + itype + (1 | subj) +
> >      (0 + itype | COMPID) +
> >      (1 | item),
> >    lql,
> >    binomial))
> >
> > (fm3 <- lme4::glmer(
> >    dichot ~ 0 + itype + (1 | subj) +
> >      (1 | COMPID:itype) +
> >      (1 | item),
> >    lql,
> >    binomial))
> >
> > (fm3a <- lme4::glmer(
> >    dichot ~ 0 + itype + (1 | subj) +
> >      (1 | COMPID:itype) +
> >      (1 | COMPID) +
> >      (1 | item),
> >    lql,
> >    binomial))
> >
> > anova(fm3, fm3a, fm2)
> >
> > str(imap <- unique(lql[, c("itype", "item")]))
> >
> > (easiness <-
> >    lme4::ranef(fm2)$item[[1]] +
> >    glmmTMB::fixef(fm2)[imap$itype])
> >
> > Best,
> > Rasmus
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Wed May 13 19:45:05 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Wed, 13 May 2020 12:45:05 -0500
Subject: [R-sig-ME] Multi-level Rasch Model Per Douglas Bates' paper
In-Reply-To: <CACgv6yWF+d1U=wkr6BWxMSN1MUwHFDSuw+MVQDbD_CDodQNguA@mail.gmail.com>
References: <CACgv6yXKgZXxTGwu_PrnFRFFkseJfby4N8mJyKi37m3D=-cNkA@mail.gmail.com>
 <8dad96d1-ff79-ac26-de98-b5cc36323b53@mpi.nl>
 <20200513163531.GB1776058@posteo.no>
 <7cf511fe-4c06-0992-9061-c3b6e38e065b@gmail.com>
 <20200513165013.GC1776058@posteo.no>
 <20200513172502.GD1776058@posteo.no>
 <b4ae839e-e2ce-6860-bce2-b6b5441130d7@gmail.com>
 <CACgv6yWF+d1U=wkr6BWxMSN1MUwHFDSuw+MVQDbD_CDodQNguA@mail.gmail.com>
Message-ID: <CACgv6yWyy-e=MuMW97_54mqb-VxRpyhEvxojV3d5pQ68Nn+dPQ@mail.gmail.com>

I meant   *easiness *<- *coef(m1)[[1]][[1]][1]*      *persons
<- coef(m1)[[1]][[2]][1]*

On Wed, May 13, 2020 at 12:43 PM Simon Harmel <sim.harmel at gmail.com> wrote:

> Ben,
>
> This is exactly what I'm trying to understand for my glmmTMB models in my
> original post ('m1' & 'm2').
>
> So to get the item easiness I think we need to use: *easiness *<-
> *coef(f11)[[1]][[1]][1]*
> *
>         persons <- coef(f11)[[1]][[2]][1]*
>
> Am I right Ben?
>
> On Wed, May 13, 2020 at 12:32 PM Ben Bolker <bbolker at gmail.com> wrote:
>
>>     Apologies for not looking this over in great detail, but not sure
>> why you're mixing lme4 and glmmTMB here??  Isn't item easiness just
>> lme4::coef(fm2)$item ?
>>
>> easiness <-
>>    lme4::ranef(fm2)$item[[1]] +
>>    glmmTMB::fixef(fm2)[imap$itype])
>>
>> On 5/13/20 1:25 PM, Rasmus Liland wrote:
>> > On 2020-05-13 18:50 +0200, Rasmus Liland wrote:
>> >> Indeed it does work now!  Thanks!
>> > Right, so this code reproduces code until the
>> > easiness variable on page 15.  Perhaps this
>> > is useful?
>> >
>> > data("lq2002", package="multilevel")
>> > wrk <- lq2002
>> > # wrk[1:5,]
>> > for (i in 3:16) wrk[[i]] <- ordered(wrk[[i]])
>> > for (i in 17:21) wrk[[i]] <- ordered(5 - wrk[[i]])
>> > lql <- reshape(wrk,
>> >    varying = list(names(lq2002)[3:21]),
>> >    v.names = "fivelev",
>> >    idvar = "subj",
>> >    timevar = "item",
>> >    drop = names(lq2002)[c(2, 22:27)],
>> >    direction = "long")
>> > lql$itype <-
>> >    with(lql, factor(ifelse(item < 12, "Leadership",
>> >      ifelse(item < 15, "Task Sig.", "Hostility")
>> >    )))
>> > for (i in c(1, 2, 4, 5)) lql[[i]] <- factor(lql[[i]])
>> > lql$dichot <- factor(ifelse(lql$fivelev < 4, 0, 1))
>> > # str(lql)
>> > # summary(lql)
>> >
>> > ## 3.2 Fitting an initial multilevel Rasch model
>> > (fm1 <- lme4::glmer(
>> >    dichot ~ 0 + itype + (1 | subj) +
>> >      (1 | COMPID) +
>> >      (1 | item),
>> >    lql,
>> >    binomial))
>> > rr <- lme4::ranef(fm1, condVar = TRUE)
>> > str(rr$COMPID)
>> > head(rr$COMPID)
>> >
>> > qq <- lattice::qqmath(rr)
>> > print(qq$subj)
>> >
>> > ## 3.3 Allowing for interactions of company and item type
>> > (fm2 <- lme4::glmer(
>> >    dichot ~ 0 + itype + (1 | subj) +
>> >      (0 + itype | COMPID) +
>> >      (1 | item),
>> >    lql,
>> >    binomial))
>> >
>> > (fm3 <- lme4::glmer(
>> >    dichot ~ 0 + itype + (1 | subj) +
>> >      (1 | COMPID:itype) +
>> >      (1 | item),
>> >    lql,
>> >    binomial))
>> >
>> > (fm3a <- lme4::glmer(
>> >    dichot ~ 0 + itype + (1 | subj) +
>> >      (1 | COMPID:itype) +
>> >      (1 | COMPID) +
>> >      (1 | item),
>> >    lql,
>> >    binomial))
>> >
>> > anova(fm3, fm3a, fm2)
>> >
>> > str(imap <- unique(lql[, c("itype", "item")]))
>> >
>> > (easiness <-
>> >    lme4::ranef(fm2)$item[[1]] +
>> >    glmmTMB::fixef(fm2)[imap$itype])
>> >
>> > Best,
>> > Rasmus
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Wed May 13 19:55:10 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 13 May 2020 13:55:10 -0400
Subject: [R-sig-ME] Multi-level Rasch Model Per Douglas Bates' paper
In-Reply-To: <CACgv6yWyy-e=MuMW97_54mqb-VxRpyhEvxojV3d5pQ68Nn+dPQ@mail.gmail.com>
References: <CACgv6yXKgZXxTGwu_PrnFRFFkseJfby4N8mJyKi37m3D=-cNkA@mail.gmail.com>
 <8dad96d1-ff79-ac26-de98-b5cc36323b53@mpi.nl>
 <20200513163531.GB1776058@posteo.no>
 <7cf511fe-4c06-0992-9061-c3b6e38e065b@gmail.com>
 <20200513165013.GC1776058@posteo.no> <20200513172502.GD1776058@posteo.no>
 <b4ae839e-e2ce-6860-bce2-b6b5441130d7@gmail.com>
 <CACgv6yWF+d1U=wkr6BWxMSN1MUwHFDSuw+MVQDbD_CDodQNguA@mail.gmail.com>
 <CACgv6yWyy-e=MuMW97_54mqb-VxRpyhEvxojV3d5pQ68Nn+dPQ@mail.gmail.com>
Message-ID: <cdd67f33-a0a6-3977-cd40-0276662cca5e@gmail.com>

 ? I'd have to look at this more carefully, but something like that.? 
It's more robust and easier to understand if you use labels rather than 
numbers, e.g.

 ?? coef(m1)$cond$item[["(Intercept)"]]

?? (if in doubt, use str(coef(m1)) to see how the pieces are arranged 
and labeled)

On 5/13/20 1:45 PM, Simon Harmel wrote:
> I meant *easiness *<- *coef(m1)[[1]][[1]][1]* *persons 
> <-?coef(m1)[[1]][[2]][1]*
>
> On Wed, May 13, 2020 at 12:43 PM Simon Harmel <sim.harmel at gmail.com 
> <mailto:sim.harmel at gmail.com>> wrote:
>
>     Ben,
>
>     This is exactly what I'm trying to understand?for my glmmTMB
>     models in my original post ('m1' & 'm2').
>
>     So to get the item easiness I think we need to use: *easiness *<-
>     *coef(f11)[[1]][[1]][1]*
>     *? ? ? ? ? ? ? ? ? ? ? ? ? ? ? persons <-?coef(f11)[[1]][[2]][1]*
>     *
>     *
>     Am I right Ben?
>
>     On Wed, May 13, 2020 at 12:32 PM Ben Bolker <bbolker at gmail.com
>     <mailto:bbolker at gmail.com>> wrote:
>
>         ??? Apologies for not looking this over in great detail, but
>         not sure
>         why you're mixing lme4 and glmmTMB here??? Isn't item easiness
>         just
>         lme4::coef(fm2)$item ?
>
>         easiness <-
>         ? ?lme4::ranef(fm2)$item[[1]] +
>         ? ?glmmTMB::fixef(fm2)[imap$itype])
>
>         On 5/13/20 1:25 PM, Rasmus Liland wrote:
>         > On 2020-05-13 18:50 +0200, Rasmus Liland wrote:
>         >> Indeed it does work now!? Thanks!
>         > Right, so this code reproduces code until the
>         > easiness variable on page 15.? Perhaps this
>         > is useful?
>         >
>         > data("lq2002", package="multilevel")
>         > wrk <- lq2002
>         > # wrk[1:5,]
>         > for (i in 3:16) wrk[[i]] <- ordered(wrk[[i]])
>         > for (i in 17:21) wrk[[i]] <- ordered(5 - wrk[[i]])
>         > lql <- reshape(wrk,
>         >? ? varying = list(names(lq2002)[3:21]),
>         >? ? v.names = "fivelev",
>         >? ? idvar = "subj",
>         >? ? timevar = "item",
>         >? ? drop = names(lq2002)[c(2, 22:27)],
>         >? ? direction = "long")
>         > lql$itype <-
>         >? ? with(lql, factor(ifelse(item < 12, "Leadership",
>         >? ? ? ifelse(item < 15, "Task Sig.", "Hostility")
>         >? ? )))
>         > for (i in c(1, 2, 4, 5)) lql[[i]] <- factor(lql[[i]])
>         > lql$dichot <- factor(ifelse(lql$fivelev < 4, 0, 1))
>         > # str(lql)
>         > # summary(lql)
>         >
>         > ## 3.2 Fitting an initial multilevel Rasch model
>         > (fm1 <- lme4::glmer(
>         >? ? dichot ~ 0 + itype + (1 | subj) +
>         >? ? ? (1 | COMPID) +
>         >? ? ? (1 | item),
>         >? ? lql,
>         >? ? binomial))
>         > rr <- lme4::ranef(fm1, condVar = TRUE)
>         > str(rr$COMPID)
>         > head(rr$COMPID)
>         >
>         > qq <- lattice::qqmath(rr)
>         > print(qq$subj)
>         >
>         > ## 3.3 Allowing for interactions of company and item type
>         > (fm2 <- lme4::glmer(
>         >? ? dichot ~ 0 + itype + (1 | subj) +
>         >? ? ? (0 + itype | COMPID) +
>         >? ? ? (1 | item),
>         >? ? lql,
>         >? ? binomial))
>         >
>         > (fm3 <- lme4::glmer(
>         >? ? dichot ~ 0 + itype + (1 | subj) +
>         >? ? ? (1 | COMPID:itype) +
>         >? ? ? (1 | item),
>         >? ? lql,
>         >? ? binomial))
>         >
>         > (fm3a <- lme4::glmer(
>         >? ? dichot ~ 0 + itype + (1 | subj) +
>         >? ? ? (1 | COMPID:itype) +
>         >? ? ? (1 | COMPID) +
>         >? ? ? (1 | item),
>         >? ? lql,
>         >? ? binomial))
>         >
>         > anova(fm3, fm3a, fm2)
>         >
>         > str(imap <- unique(lql[, c("itype", "item")]))
>         >
>         > (easiness <-
>         >? ? lme4::ranef(fm2)$item[[1]] +
>         >? ? glmmTMB::fixef(fm2)[imap$itype])
>         >
>         > Best,
>         > Rasmus
>         >
>         > _______________________________________________
>         > R-sig-mixed-models at r-project.org
>         <mailto:R-sig-mixed-models at r-project.org> mailing list
>         > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>         _______________________________________________
>         R-sig-mixed-models at r-project.org
>         <mailto:R-sig-mixed-models at r-project.org> mailing list
>         https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From jr@| @end|ng |rom po@teo@no  Wed May 13 20:06:17 2020
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Wed, 13 May 2020 20:06:17 +0200
Subject: [R-sig-ME] Multi-level Rasch Model Per Douglas Bates' paper
In-Reply-To: <20200513174459.GE1776058@posteo.no>
References: <CACgv6yXKgZXxTGwu_PrnFRFFkseJfby4N8mJyKi37m3D=-cNkA@mail.gmail.com>
 <8dad96d1-ff79-ac26-de98-b5cc36323b53@mpi.nl>
 <20200513163531.GB1776058@posteo.no>
 <7cf511fe-4c06-0992-9061-c3b6e38e065b@gmail.com>
 <20200513165013.GC1776058@posteo.no>
 <20200513172502.GD1776058@posteo.no>
 <b4ae839e-e2ce-6860-bce2-b6b5441130d7@gmail.com>
 <20200513174459.GE1776058@posteo.no>
Message-ID: <20200513180617.GF1776058@posteo.no>

On 2020-05-13 13:32 -0400, Ben Bolker wrote:
> Isn't item easiness just 
> lme4::coef(fm2)$item?

Dear Ben,

both stats::coef(fm2)$item and 
stats::coef(fm2)$cond$item[["(Intercept)"]] 
outputs something at least ... but these 
objects are lists, while the variable 
easiness becomes a named numerical vector ... 

Best,
Rasmus


From hu@ng@jcc @end|ng |rom gm@||@com  Thu May 14 02:53:58 2020
From: hu@ng@jcc @end|ng |rom gm@||@com (Sijia Huang)
Date: Wed, 13 May 2020 17:53:58 -0700
Subject: [R-sig-ME] blme optimizer warnings
Message-ID: <CAPmBuzFQdYFkx1qmj-i9x0CJLOyhXuUh+djpdUnY08xYAzJy9A@mail.gmail.com>

Hi everyone,
I am fitting a cross-classified model with blme, but getting 1 optimizer
warning. The code and output are shown below. Any suggestions regarding
fixing the estimation issue? Thanks!


> meta.example <- blmer(g~0+(1|Study)+(1|Subscale)+
1|Outcome:Study:Subscale),
+                       data=meta, weights = Variance,
+                       resid.prior = point(1),
+                       control = lmerControl(optimizer="bobyqa"))

> meta.example
Cov prior  : Outcome:Study:Subscale ~ wishart(df = 3.5, scale = Inf,
posterior.scale = cov, common.scale = TRUE)
           : Study ~ wishart(df = 3.5, scale = Inf, posterior.scale = cov,
common.scale = TRUE)
           : Subscale ~ wishart(df = 3.5, scale = Inf, posterior.scale =
cov, common.scale = TRUE)
Resid prior: point(value = 1)
Prior dev  : NaN

Linear mixed model fit by maximum likelihood  ['blmerMod']
Formula: g ~ 0 + (1 | Study) + (1 | Subscale) + (1 | Outcome:Study:Subscale)
   Data: meta
Weights: Variance
     AIC      BIC   logLik deviance df.resid
     Inf      Inf     -Inf      Inf       64
Random effects:
 Groups                 Name        Std.Dev.
 Outcome:Study:Subscale (Intercept) 1
 Study                  (Intercept) 1
 Subscale               (Intercept) 1
 Residual                           1
Number of obs: 68, groups:  Outcome:Study:Subscale, 68; Study, 57;
Subscale, 7
No fixed effect coefficients
convergence code 0; 1 optimizer warnings; 0 lme4 warnings




Best,
Sijia

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Thu May 14 02:59:45 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 13 May 2020 20:59:45 -0400
Subject: [R-sig-ME] blme optimizer warnings
In-Reply-To: <CAPmBuzFQdYFkx1qmj-i9x0CJLOyhXuUh+djpdUnY08xYAzJy9A@mail.gmail.com>
References: <CAPmBuzFQdYFkx1qmj-i9x0CJLOyhXuUh+djpdUnY08xYAzJy9A@mail.gmail.com>
Message-ID: <ee3fa987-da28-e327-6b8f-a54beefc7439@gmail.com>

 ?? Without looking very carefully at this:

* unless your response variable is somehow already centered at zero by 
design, a model with no intercept at all is going to be 
weird/problematic (random effects are always zero-centered by definition).

* is it really OK to have an infinite scale in your wishart prior?? (It 
may be fine, I'm not immediately familiar with the blme 
parameterizations, it just looks weird)

* the fact that your standard devs are all exactly 1 suggests that the 
optimizer bailed out before actually doing anything (these are the 
default starting values).

 ? Can you provide a reproducible example?

On 5/13/20 8:53 PM, Sijia Huang wrote:
> Hi everyone,
> I am fitting a cross-classified model with blme, but getting 1 optimizer
> warning. The code and output are shown below. Any suggestions regarding
> fixing the estimation issue? Thanks!
>
>
>> meta.example <- blmer(g~0+(1|Study)+(1|Subscale)+
> 1|Outcome:Study:Subscale),
> +                       data=meta, weights = Variance,
> +                       resid.prior = point(1),
> +                       control = lmerControl(optimizer="bobyqa"))
>
>> meta.example
> Cov prior  : Outcome:Study:Subscale ~ wishart(df = 3.5, scale = Inf,
> posterior.scale = cov, common.scale = TRUE)
>             : Study ~ wishart(df = 3.5, scale = Inf, posterior.scale = cov,
> common.scale = TRUE)
>             : Subscale ~ wishart(df = 3.5, scale = Inf, posterior.scale =
> cov, common.scale = TRUE)
> Resid prior: point(value = 1)
> Prior dev  : NaN
>
> Linear mixed model fit by maximum likelihood  ['blmerMod']
> Formula: g ~ 0 + (1 | Study) + (1 | Subscale) + (1 | Outcome:Study:Subscale)
>     Data: meta
> Weights: Variance
>       AIC      BIC   logLik deviance df.resid
>       Inf      Inf     -Inf      Inf       64
> Random effects:
>   Groups                 Name        Std.Dev.
>   Outcome:Study:Subscale (Intercept) 1
>   Study                  (Intercept) 1
>   Subscale               (Intercept) 1
>   Residual                           1
> Number of obs: 68, groups:  Outcome:Study:Subscale, 68; Study, 57;
> Subscale, 7
> No fixed effect coefficients
> convergence code 0; 1 optimizer warnings; 0 lme4 warnings
>
>
>
>
> Best,
> Sijia
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From hu@ng@jcc @end|ng |rom gm@||@com  Thu May 14 03:36:45 2020
From: hu@ng@jcc @end|ng |rom gm@||@com (Sijia Huang)
Date: Wed, 13 May 2020 18:36:45 -0700
Subject: [R-sig-ME] blme optimizer warnings
In-Reply-To: <ee3fa987-da28-e327-6b8f-a54beefc7439@gmail.com>
References: <CAPmBuzFQdYFkx1qmj-i9x0CJLOyhXuUh+djpdUnY08xYAzJy9A@mail.gmail.com>
 <ee3fa987-da28-e327-6b8f-a54beefc7439@gmail.com>
Message-ID: <CAPmBuzGnb3_dO2KSopPi9TVc5H=d2PkQP50KuyS-ANtTT6Zatg@mail.gmail.com>

Thanks for the quick reply, Ben!

I am replicating the Fern?ndez-Castilla et al. (2018) article. Below are
the data they have in the article. Anything I can do to resolve the issue?
Thanks!

> meta
   Study Outcome Subscale      g Variance Precision
1      1       1        1 -0.251    0.024    41.455
2      2       1        1 -0.069    0.001  1361.067
3      3       1        5  0.138    0.001   957.620
4      4       1        1 -0.754    0.085    11.809
5      5       1        1 -0.228    0.020    49.598
6      6       1        6 -0.212    0.004   246.180
7      6       2        7  0.219    0.004   246.095
8      7       1        1  0.000    0.012    83.367
9      8       1        2 -0.103    0.006   162.778
10     8       2        3  0.138    0.006   162.612
11     8       3        4 -0.387    0.006   160.133
12     9       1        1 -0.032    0.023    44.415
13    10       1        5 -0.020    0.058    17.110
14    11       1        1  0.128    0.017    59.999
15    12       1        1 -0.262    0.032    31.505
16    13       1        1 -0.046    0.071    14.080
17    14       1        6 -0.324    0.003   381.620
18    14       2        6 -0.409    0.003   378.611
19    14       3        7  0.080    0.003   385.319
20    14       4        7 -0.140    0.003   385.542
21    15       1        1  0.311    0.005   185.364
22    16       1        1  0.036    0.005   205.063
23    17       1        6 -0.259    0.001   925.643
24    17       2        7  0.196    0.001   928.897
25    18       1        1  0.157    0.013    74.094
26    19       1        1  0.000    0.056    17.985
27    20       1        1  0.000    0.074    13.600
28    21       1        6 -0.013    0.039    25.425
29    21       2        7 -0.004    0.039    25.426
30    22       1        1 -0.202    0.001  1487.992
31    23       1        1  0.000    0.086    11.628
32    24       1        1 -0.221    0.001   713.110
33    25       1        1 -0.099    0.001   749.964
34    26       1        5 -0.165    0.000  6505.024
35    27       1        1 -0.523    0.063    15.856
36    28       1        1  0.000    0.001  1611.801
37    29       1        6  0.377    0.045    22.045
38    29       2        7  0.575    0.046    21.677
39    30       1        1  0.590    0.074    13.477
40    31       1        1  0.020    0.001  1335.991
41    32       1        1  0.121    0.043    23.489
42    33       1        1 -0.101    0.003   363.163
43    34       1        1 -0.101    0.003   369.507
44    35       1        1 -0.104    0.004   255.507
45    36       1        1 -0.270    0.003   340.761
46    37       1        1  0.179    0.150     6.645
47    38       1        2  0.468    0.020    51.255
48    38       2        4 -0.479    0.020    51.193
49    39       1        5 -0.081    0.024    42.536
50    40       1        1 -0.071    0.043    23.519
51    41       1        1  0.201    0.077    13.036
52    42       1        6 -0.070    0.006   180.844
53    42       2        7  0.190    0.006   180.168
54    43       1        1  0.277    0.013    79.220
55    44       1        5 -0.086    0.001   903.924
56    45       1        5 -0.338    0.002   469.260
57    46       1        1  0.262    0.003   290.330
58    47       1        5  0.000    0.003   304.959
59    48       1        1 -0.645    0.055    18.192
60    49       1        5 -0.120    0.002   461.802
61    50       1        5 -0.286    0.009   106.189
62    51       1        1 -0.124    0.006   172.261
63    52       1        1  0.023    0.028    35.941
64    53       1        5 -0.064    0.001   944.600
65    54       1        1  0.000    0.043    23.010
66    55       1        1  0.000    0.014    72.723
67    56       1        5  0.000    0.012    85.832
68    57       1        1  0.000    0.012    85.832


On Wed, May 13, 2020 at 6:00 PM Ben Bolker <bbolker at gmail.com> wrote:

>     Without looking very carefully at this:
>
> * unless your response variable is somehow already centered at zero by
> design, a model with no intercept at all is going to be
> weird/problematic (random effects are always zero-centered by definition).
>
> * is it really OK to have an infinite scale in your wishart prior?  (It
> may be fine, I'm not immediately familiar with the blme
> parameterizations, it just looks weird)
>
> * the fact that your standard devs are all exactly 1 suggests that the
> optimizer bailed out before actually doing anything (these are the
> default starting values).
>
>    Can you provide a reproducible example?
>
> On 5/13/20 8:53 PM, Sijia Huang wrote:
> > Hi everyone,
> > I am fitting a cross-classified model with blme, but getting 1 optimizer
> > warning. The code and output are shown below. Any suggestions regarding
> > fixing the estimation issue? Thanks!
> >
> >
> >> meta.example <- blmer(g~0+(1|Study)+(1|Subscale)+
> > 1|Outcome:Study:Subscale),
> > +                       data=meta, weights = Variance,
> > +                       resid.prior = point(1),
> > +                       control = lmerControl(optimizer="bobyqa"))
> >
> >> meta.example
> > Cov prior  : Outcome:Study:Subscale ~ wishart(df = 3.5, scale = Inf,
> > posterior.scale = cov, common.scale = TRUE)
> >             : Study ~ wishart(df = 3.5, scale = Inf, posterior.scale =
> cov,
> > common.scale = TRUE)
> >             : Subscale ~ wishart(df = 3.5, scale = Inf, posterior.scale =
> > cov, common.scale = TRUE)
> > Resid prior: point(value = 1)
> > Prior dev  : NaN
> >
> > Linear mixed model fit by maximum likelihood  ['blmerMod']
> > Formula: g ~ 0 + (1 | Study) + (1 | Subscale) + (1 |
> Outcome:Study:Subscale)
> >     Data: meta
> > Weights: Variance
> >       AIC      BIC   logLik deviance df.resid
> >       Inf      Inf     -Inf      Inf       64
> > Random effects:
> >   Groups                 Name        Std.Dev.
> >   Outcome:Study:Subscale (Intercept) 1
> >   Study                  (Intercept) 1
> >   Subscale               (Intercept) 1
> >   Residual                           1
> > Number of obs: 68, groups:  Outcome:Study:Subscale, 68; Study, 57;
> > Subscale, 7
> > No fixed effect coefficients
> > convergence code 0; 1 optimizer warnings; 0 lme4 warnings
> >
> >
> >
> >
> > Best,
> > Sijia
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Thu May 14 03:57:02 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 13 May 2020 21:57:02 -0400
Subject: [R-sig-ME] blme optimizer warnings
In-Reply-To: <CAPmBuzGnb3_dO2KSopPi9TVc5H=d2PkQP50KuyS-ANtTT6Zatg@mail.gmail.com>
References: <CAPmBuzFQdYFkx1qmj-i9x0CJLOyhXuUh+djpdUnY08xYAzJy9A@mail.gmail.com>
 <ee3fa987-da28-e327-6b8f-a54beefc7439@gmail.com>
 <CAPmBuzGnb3_dO2KSopPi9TVc5H=d2PkQP50KuyS-ANtTT6Zatg@mail.gmail.com>
Message-ID: <a02c2f89-1dbb-14e3-00df-4db6cf81a158@gmail.com>


 ? Can you give a more specific reference? I can't immediately guess 
from Fern?ndez-Castilla's google scholar page which article it is ...

On 5/13/20 9:36 PM, Sijia Huang wrote:
> Thanks for the quick reply, Ben!
>
> I am replicating the Fern?ndez-Castilla et al. (2018) article. Below 
> are the data they have in the article. Anything I can do to resolve 
> the issue? Thanks!
>
> > meta
> ? ?Study Outcome Subscale ? ? ?g Variance Precision
> 1 ? ? ?1 ? ? ? 1 ? ? ? ?1 -0.251 ? ?0.024 ? ?41.455
> 2 ? ? ?2 ? ? ? 1 ? ? ? ?1 -0.069 ? ?0.001 ?1361.067
> 3 ? ? ?3 ? ? ? 1 ? ? ? ?5 ?0.138 ? ?0.001 ? 957.620
> 4 ? ? ?4 ? ? ? 1 ? ? ? ?1 -0.754 ? ?0.085 ? ?11.809
> 5 ? ? ?5 ? ? ? 1 ? ? ? ?1 -0.228 ? ?0.020 ? ?49.598
> 6 ? ? ?6 ? ? ? 1 ? ? ? ?6 -0.212 ? ?0.004 ? 246.180
> 7 ? ? ?6 ? ? ? 2 ? ? ? ?7 ?0.219 ? ?0.004 ? 246.095
> 8 ? ? ?7 ? ? ? 1 ? ? ? ?1 ?0.000 ? ?0.012 ? ?83.367
> 9 ? ? ?8 ? ? ? 1 ? ? ? ?2 -0.103 ? ?0.006 ? 162.778
> 10 ? ? 8 ? ? ? 2 ? ? ? ?3 ?0.138 ? ?0.006 ? 162.612
> 11 ? ? 8 ? ? ? 3 ? ? ? ?4 -0.387 ? ?0.006 ? 160.133
> 12 ? ? 9 ? ? ? 1 ? ? ? ?1 -0.032 ? ?0.023 ? ?44.415
> 13 ? ?10 ? ? ? 1 ? ? ? ?5 -0.020 ? ?0.058 ? ?17.110
> 14 ? ?11 ? ? ? 1 ? ? ? ?1 ?0.128 ? ?0.017 ? ?59.999
> 15 ? ?12 ? ? ? 1 ? ? ? ?1 -0.262 ? ?0.032 ? ?31.505
> 16 ? ?13 ? ? ? 1 ? ? ? ?1 -0.046 ? ?0.071 ? ?14.080
> 17 ? ?14 ? ? ? 1 ? ? ? ?6 -0.324 ? ?0.003 ? 381.620
> 18 ? ?14 ? ? ? 2 ? ? ? ?6 -0.409 ? ?0.003 ? 378.611
> 19 ? ?14 ? ? ? 3 ? ? ? ?7 ?0.080 ? ?0.003 ? 385.319
> 20 ? ?14 ? ? ? 4 ? ? ? ?7 -0.140 ? ?0.003 ? 385.542
> 21 ? ?15 ? ? ? 1 ? ? ? ?1 ?0.311 ? ?0.005 ? 185.364
> 22 ? ?16 ? ? ? 1 ? ? ? ?1 ?0.036 ? ?0.005 ? 205.063
> 23 ? ?17 ? ? ? 1 ? ? ? ?6 -0.259 ? ?0.001 ? 925.643
> 24 ? ?17 ? ? ? 2 ? ? ? ?7 ?0.196 ? ?0.001 ? 928.897
> 25 ? ?18 ? ? ? 1 ? ? ? ?1 ?0.157 ? ?0.013 ? ?74.094
> 26 ? ?19 ? ? ? 1 ? ? ? ?1 ?0.000 ? ?0.056 ? ?17.985
> 27 ? ?20 ? ? ? 1 ? ? ? ?1 ?0.000 ? ?0.074 ? ?13.600
> 28 ? ?21 ? ? ? 1 ? ? ? ?6 -0.013 ? ?0.039 ? ?25.425
> 29 ? ?21 ? ? ? 2 ? ? ? ?7 -0.004 ? ?0.039 ? ?25.426
> 30 ? ?22 ? ? ? 1 ? ? ? ?1 -0.202 ? ?0.001 ?1487.992
> 31 ? ?23 ? ? ? 1 ? ? ? ?1 ?0.000 ? ?0.086 ? ?11.628
> 32 ? ?24 ? ? ? 1 ? ? ? ?1 -0.221 ? ?0.001 ? 713.110
> 33 ? ?25 ? ? ? 1 ? ? ? ?1 -0.099 ? ?0.001 ? 749.964
> 34 ? ?26 ? ? ? 1 ? ? ? ?5 -0.165 ? ?0.000 ?6505.024
> 35 ? ?27 ? ? ? 1 ? ? ? ?1 -0.523 ? ?0.063 ? ?15.856
> 36 ? ?28 ? ? ? 1 ? ? ? ?1 ?0.000 ? ?0.001 ?1611.801
> 37 ? ?29 ? ? ? 1 ? ? ? ?6 ?0.377 ? ?0.045 ? ?22.045
> 38 ? ?29 ? ? ? 2 ? ? ? ?7 ?0.575 ? ?0.046 ? ?21.677
> 39 ? ?30 ? ? ? 1 ? ? ? ?1 ?0.590 ? ?0.074 ? ?13.477
> 40 ? ?31 ? ? ? 1 ? ? ? ?1 ?0.020 ? ?0.001 ?1335.991
> 41 ? ?32 ? ? ? 1 ? ? ? ?1 ?0.121 ? ?0.043 ? ?23.489
> 42 ? ?33 ? ? ? 1 ? ? ? ?1 -0.101 ? ?0.003 ? 363.163
> 43 ? ?34 ? ? ? 1 ? ? ? ?1 -0.101 ? ?0.003 ? 369.507
> 44 ? ?35 ? ? ? 1 ? ? ? ?1 -0.104 ? ?0.004 ? 255.507
> 45 ? ?36 ? ? ? 1 ? ? ? ?1 -0.270 ? ?0.003 ? 340.761
> 46 ? ?37 ? ? ? 1 ? ? ? ?1 ?0.179 ? ?0.150 ? ? 6.645
> 47 ? ?38 ? ? ? 1 ? ? ? ?2 ?0.468 ? ?0.020 ? ?51.255
> 48 ? ?38 ? ? ? 2 ? ? ? ?4 -0.479 ? ?0.020 ? ?51.193
> 49 ? ?39 ? ? ? 1 ? ? ? ?5 -0.081 ? ?0.024 ? ?42.536
> 50 ? ?40 ? ? ? 1 ? ? ? ?1 -0.071 ? ?0.043 ? ?23.519
> 51 ? ?41 ? ? ? 1 ? ? ? ?1 ?0.201 ? ?0.077 ? ?13.036
> 52 ? ?42 ? ? ? 1 ? ? ? ?6 -0.070 ? ?0.006 ? 180.844
> 53 ? ?42 ? ? ? 2 ? ? ? ?7 ?0.190 ? ?0.006 ? 180.168
> 54 ? ?43 ? ? ? 1 ? ? ? ?1 ?0.277 ? ?0.013 ? ?79.220
> 55 ? ?44 ? ? ? 1 ? ? ? ?5 -0.086 ? ?0.001 ? 903.924
> 56 ? ?45 ? ? ? 1 ? ? ? ?5 -0.338 ? ?0.002 ? 469.260
> 57 ? ?46 ? ? ? 1 ? ? ? ?1 ?0.262 ? ?0.003 ? 290.330
> 58 ? ?47 ? ? ? 1 ? ? ? ?5 ?0.000 ? ?0.003 ? 304.959
> 59 ? ?48 ? ? ? 1 ? ? ? ?1 -0.645 ? ?0.055 ? ?18.192
> 60 ? ?49 ? ? ? 1 ? ? ? ?5 -0.120 ? ?0.002 ? 461.802
> 61 ? ?50 ? ? ? 1 ? ? ? ?5 -0.286 ? ?0.009 ? 106.189
> 62 ? ?51 ? ? ? 1 ? ? ? ?1 -0.124 ? ?0.006 ? 172.261
> 63 ? ?52 ? ? ? 1 ? ? ? ?1 ?0.023 ? ?0.028 ? ?35.941
> 64 ? ?53 ? ? ? 1 ? ? ? ?5 -0.064 ? ?0.001 ? 944.600
> 65 ? ?54 ? ? ? 1 ? ? ? ?1 ?0.000 ? ?0.043 ? ?23.010
> 66 ? ?55 ? ? ? 1 ? ? ? ?1 ?0.000 ? ?0.014 ? ?72.723
> 67 ? ?56 ? ? ? 1 ? ? ? ?5 ?0.000 ? ?0.012 ? ?85.832
> 68 ? ?57 ? ? ? 1 ? ? ? ?1 ?0.000 ? ?0.012 ? ?85.832
>
>
> On Wed, May 13, 2020 at 6:00 PM Ben Bolker <bbolker at gmail.com 
> <mailto:bbolker at gmail.com>> wrote:
>
>     Without looking very carefully at this:
>
>     * unless your response variable is somehow already centered at
>     zero by
>     design, a model with no intercept at all is going to be
>     weird/problematic (random effects are always zero-centered by
>     definition).
>
>     * is it really OK to have an infinite scale in your wishart
>     prior?? (It
>     may be fine, I'm not immediately familiar with the blme
>     parameterizations, it just looks weird)
>
>     * the fact that your standard devs are all exactly 1 suggests that
>     the
>     optimizer bailed out before actually doing anything (these are the
>     default starting values).
>
>     ?? Can you provide a reproducible example?
>
>     On 5/13/20 8:53 PM, Sijia Huang wrote:
>     > Hi everyone,
>     > I am fitting a cross-classified model with blme, but getting 1
>     optimizer
>     > warning. The code and output are shown below. Any suggestions
>     regarding
>     > fixing the estimation issue? Thanks!
>     >
>     >
>     >> meta.example <- blmer(g~0+(1|Study)+(1|Subscale)+
>     > 1|Outcome:Study:Subscale),
>     > +? ? ? ? ? ? ? ? ? ? ? ?data=meta, weights = Variance,
>     > +? ? ? ? ? ? ? ? ? ? ? ?resid.prior = point(1),
>     > +? ? ? ? ? ? ? ? ? ? ? ?control = lmerControl(optimizer="bobyqa"))
>     >
>     >> meta.example
>     > Cov prior? : Outcome:Study:Subscale ~ wishart(df = 3.5, scale = Inf,
>     > posterior.scale = cov, common.scale = TRUE)
>     >? ? ? ? ? ? ?: Study ~ wishart(df = 3.5, scale = Inf,
>     posterior.scale = cov,
>     > common.scale = TRUE)
>     >? ? ? ? ? ? ?: Subscale ~ wishart(df = 3.5, scale = Inf,
>     posterior.scale =
>     > cov, common.scale = TRUE)
>     > Resid prior: point(value = 1)
>     > Prior dev? : NaN
>     >
>     > Linear mixed model fit by maximum likelihood ['blmerMod']
>     > Formula: g ~ 0 + (1 | Study) + (1 | Subscale) + (1 |
>     Outcome:Study:Subscale)
>     >? ? ?Data: meta
>     > Weights: Variance
>     >? ? ? ?AIC? ? ? BIC? ?logLik deviance df.resid
>     >? ? ? ?Inf? ? ? Inf? ? ?-Inf? ? ? Inf? ? ? ?64
>     > Random effects:
>     >? ?Groups? ? ? ? ? ? ? ? ?Name? ? ? ? Std.Dev.
>     >? ?Outcome:Study:Subscale (Intercept) 1
>     >? ?Study? ? ? ? ? ? ? ? ? (Intercept) 1
>     >? ?Subscale? ? ? ? ? ? ? ?(Intercept) 1
>     >? ?Residual? ? ? ? ? ? ? ? ? ? ? ? ? ?1
>     > Number of obs: 68, groups:? Outcome:Study:Subscale, 68; Study, 57;
>     > Subscale, 7
>     > No fixed effect coefficients
>     > convergence code 0; 1 optimizer warnings; 0 lme4 warnings
>     >
>     >
>     >
>     >
>     > Best,
>     > Sijia
>     >
>     >? ? ? ?[[alternative HTML version deleted]]
>     >
>     > _______________________________________________
>     > R-sig-mixed-models at r-project.org
>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>     _______________________________________________
>     R-sig-mixed-models at r-project.org
>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>     https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From hu@ng@jcc @end|ng |rom gm@||@com  Thu May 14 04:04:01 2020
From: hu@ng@jcc @end|ng |rom gm@||@com (Sijia Huang)
Date: Wed, 13 May 2020 19:04:01 -0700
Subject: [R-sig-ME] blme optimizer warnings
In-Reply-To: <a02c2f89-1dbb-14e3-00df-4db6cf81a158@gmail.com>
References: <CAPmBuzFQdYFkx1qmj-i9x0CJLOyhXuUh+djpdUnY08xYAzJy9A@mail.gmail.com>
 <ee3fa987-da28-e327-6b8f-a54beefc7439@gmail.com>
 <CAPmBuzGnb3_dO2KSopPi9TVc5H=d2PkQP50KuyS-ANtTT6Zatg@mail.gmail.com>
 <a02c2f89-1dbb-14e3-00df-4db6cf81a158@gmail.com>
Message-ID: <CAPmBuzHgH7647ho2fi=kJKn0tgBLaiVfjdURjR9s6DRbmJWcfQ@mail.gmail.com>

Here it is. Thanks!

A demonstration and evaluation of the use of cross-classified
random-effects models for meta-analysis

On Wed, May 13, 2020 at 6:57 PM Ben Bolker <bbolker at gmail.com> wrote:

>
>   Can you give a more specific reference? I can't immediately guess from
> Fern?ndez-Castilla's google scholar page which article it is ...
> On 5/13/20 9:36 PM, Sijia Huang wrote:
>
> Thanks for the quick reply, Ben!
>
> I am replicating the Fern?ndez-Castilla et al. (2018) article. Below are
> the data they have in the article. Anything I can do to resolve the issue?
> Thanks!
>
> > meta
>    Study Outcome Subscale      g Variance Precision
> 1      1       1        1 -0.251    0.024    41.455
> 2      2       1        1 -0.069    0.001  1361.067
> 3      3       1        5  0.138    0.001   957.620
> 4      4       1        1 -0.754    0.085    11.809
> 5      5       1        1 -0.228    0.020    49.598
> 6      6       1        6 -0.212    0.004   246.180
> 7      6       2        7  0.219    0.004   246.095
> 8      7       1        1  0.000    0.012    83.367
> 9      8       1        2 -0.103    0.006   162.778
> 10     8       2        3  0.138    0.006   162.612
> 11     8       3        4 -0.387    0.006   160.133
> 12     9       1        1 -0.032    0.023    44.415
> 13    10       1        5 -0.020    0.058    17.110
> 14    11       1        1  0.128    0.017    59.999
> 15    12       1        1 -0.262    0.032    31.505
> 16    13       1        1 -0.046    0.071    14.080
> 17    14       1        6 -0.324    0.003   381.620
> 18    14       2        6 -0.409    0.003   378.611
> 19    14       3        7  0.080    0.003   385.319
> 20    14       4        7 -0.140    0.003   385.542
> 21    15       1        1  0.311    0.005   185.364
> 22    16       1        1  0.036    0.005   205.063
> 23    17       1        6 -0.259    0.001   925.643
> 24    17       2        7  0.196    0.001   928.897
> 25    18       1        1  0.157    0.013    74.094
> 26    19       1        1  0.000    0.056    17.985
> 27    20       1        1  0.000    0.074    13.600
> 28    21       1        6 -0.013    0.039    25.425
> 29    21       2        7 -0.004    0.039    25.426
> 30    22       1        1 -0.202    0.001  1487.992
> 31    23       1        1  0.000    0.086    11.628
> 32    24       1        1 -0.221    0.001   713.110
> 33    25       1        1 -0.099    0.001   749.964
> 34    26       1        5 -0.165    0.000  6505.024
> 35    27       1        1 -0.523    0.063    15.856
> 36    28       1        1  0.000    0.001  1611.801
> 37    29       1        6  0.377    0.045    22.045
> 38    29       2        7  0.575    0.046    21.677
> 39    30       1        1  0.590    0.074    13.477
> 40    31       1        1  0.020    0.001  1335.991
> 41    32       1        1  0.121    0.043    23.489
> 42    33       1        1 -0.101    0.003   363.163
> 43    34       1        1 -0.101    0.003   369.507
> 44    35       1        1 -0.104    0.004   255.507
> 45    36       1        1 -0.270    0.003   340.761
> 46    37       1        1  0.179    0.150     6.645
> 47    38       1        2  0.468    0.020    51.255
> 48    38       2        4 -0.479    0.020    51.193
> 49    39       1        5 -0.081    0.024    42.536
> 50    40       1        1 -0.071    0.043    23.519
> 51    41       1        1  0.201    0.077    13.036
> 52    42       1        6 -0.070    0.006   180.844
> 53    42       2        7  0.190    0.006   180.168
> 54    43       1        1  0.277    0.013    79.220
> 55    44       1        5 -0.086    0.001   903.924
> 56    45       1        5 -0.338    0.002   469.260
> 57    46       1        1  0.262    0.003   290.330
> 58    47       1        5  0.000    0.003   304.959
> 59    48       1        1 -0.645    0.055    18.192
> 60    49       1        5 -0.120    0.002   461.802
> 61    50       1        5 -0.286    0.009   106.189
> 62    51       1        1 -0.124    0.006   172.261
> 63    52       1        1  0.023    0.028    35.941
> 64    53       1        5 -0.064    0.001   944.600
> 65    54       1        1  0.000    0.043    23.010
> 66    55       1        1  0.000    0.014    72.723
> 67    56       1        5  0.000    0.012    85.832
> 68    57       1        1  0.000    0.012    85.832
>
>
> On Wed, May 13, 2020 at 6:00 PM Ben Bolker <bbolker at gmail.com> wrote:
>
>>     Without looking very carefully at this:
>>
>> * unless your response variable is somehow already centered at zero by
>> design, a model with no intercept at all is going to be
>> weird/problematic (random effects are always zero-centered by definition).
>>
>> * is it really OK to have an infinite scale in your wishart prior?  (It
>> may be fine, I'm not immediately familiar with the blme
>> parameterizations, it just looks weird)
>>
>> * the fact that your standard devs are all exactly 1 suggests that the
>> optimizer bailed out before actually doing anything (these are the
>> default starting values).
>>
>>    Can you provide a reproducible example?
>>
>> On 5/13/20 8:53 PM, Sijia Huang wrote:
>> > Hi everyone,
>> > I am fitting a cross-classified model with blme, but getting 1 optimizer
>> > warning. The code and output are shown below. Any suggestions regarding
>> > fixing the estimation issue? Thanks!
>> >
>> >
>> >> meta.example <- blmer(g~0+(1|Study)+(1|Subscale)+
>> > 1|Outcome:Study:Subscale),
>> > +                       data=meta, weights = Variance,
>> > +                       resid.prior = point(1),
>> > +                       control = lmerControl(optimizer="bobyqa"))
>> >
>> >> meta.example
>> > Cov prior  : Outcome:Study:Subscale ~ wishart(df = 3.5, scale = Inf,
>> > posterior.scale = cov, common.scale = TRUE)
>> >             : Study ~ wishart(df = 3.5, scale = Inf, posterior.scale =
>> cov,
>> > common.scale = TRUE)
>> >             : Subscale ~ wishart(df = 3.5, scale = Inf, posterior.scale
>> =
>> > cov, common.scale = TRUE)
>> > Resid prior: point(value = 1)
>> > Prior dev  : NaN
>> >
>> > Linear mixed model fit by maximum likelihood  ['blmerMod']
>> > Formula: g ~ 0 + (1 | Study) + (1 | Subscale) + (1 |
>> Outcome:Study:Subscale)
>> >     Data: meta
>> > Weights: Variance
>> >       AIC      BIC   logLik deviance df.resid
>> >       Inf      Inf     -Inf      Inf       64
>> > Random effects:
>> >   Groups                 Name        Std.Dev.
>> >   Outcome:Study:Subscale (Intercept) 1
>> >   Study                  (Intercept) 1
>> >   Subscale               (Intercept) 1
>> >   Residual                           1
>> > Number of obs: 68, groups:  Outcome:Study:Subscale, 68; Study, 57;
>> > Subscale, 7
>> > No fixed effect coefficients
>> > convergence code 0; 1 optimizer warnings; 0 lme4 warnings
>> >
>> >
>> >
>> >
>> > Best,
>> > Sijia
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From vdor|e @end|ng |rom gm@||@com  Thu May 14 04:53:54 2020
From: vdor|e @end|ng |rom gm@||@com (Vincent Dorie)
Date: Wed, 13 May 2020 22:53:54 -0400
Subject: [R-sig-ME] blme optimizer warnings
In-Reply-To: <CAPmBuzHgH7647ho2fi=kJKn0tgBLaiVfjdURjR9s6DRbmJWcfQ@mail.gmail.com>
References: <CAPmBuzFQdYFkx1qmj-i9x0CJLOyhXuUh+djpdUnY08xYAzJy9A@mail.gmail.com>
 <ee3fa987-da28-e327-6b8f-a54beefc7439@gmail.com>
 <CAPmBuzGnb3_dO2KSopPi9TVc5H=d2PkQP50KuyS-ANtTT6Zatg@mail.gmail.com>
 <a02c2f89-1dbb-14e3-00df-4db6cf81a158@gmail.com>
 <CAPmBuzHgH7647ho2fi=kJKn0tgBLaiVfjdURjR9s6DRbmJWcfQ@mail.gmail.com>
Message-ID: <CA+++UwR10CKhGDktCjNZ5o0V4mvER9ayf6g2_i4KwW7mzZu3iA@mail.gmail.com>

A couple of guesses here in addition to what Ben mentioned, but you
likely don't want a prior on the covariance of the random effects and
the weights should be on the scale of inverse variances. The following
replicates the numbers for the CCREM column from table 1:

blmer(g ~ 1 + (1 | Study) + (1 | Subscale) + (1 | Outcome:Study:Subscale),
      data = meta, weights = Precision, control =
lmerControl(optimizer = "bobyqa"),
      resid.prior = point(1), cov.prior = NULL)


On Wed, May 13, 2020 at 10:04 PM Sijia Huang <huangsjcc at gmail.com> wrote:
>
> Here it is. Thanks!
>
> A demonstration and evaluation of the use of cross-classified
> random-effects models for meta-analysis
>
> On Wed, May 13, 2020 at 6:57 PM Ben Bolker <bbolker at gmail.com> wrote:
>
> >
> >   Can you give a more specific reference? I can't immediately guess from
> > Fern?ndez-Castilla's google scholar page which article it is ...
> > On 5/13/20 9:36 PM, Sijia Huang wrote:
> >
> > Thanks for the quick reply, Ben!
> >
> > I am replicating the Fern?ndez-Castilla et al. (2018) article. Below are
> > the data they have in the article. Anything I can do to resolve the issue?
> > Thanks!
> >
> > > meta
> >    Study Outcome Subscale      g Variance Precision
> > 1      1       1        1 -0.251    0.024    41.455
> > 2      2       1        1 -0.069    0.001  1361.067
> > 3      3       1        5  0.138    0.001   957.620
> > 4      4       1        1 -0.754    0.085    11.809
> > 5      5       1        1 -0.228    0.020    49.598
> > 6      6       1        6 -0.212    0.004   246.180
> > 7      6       2        7  0.219    0.004   246.095
> > 8      7       1        1  0.000    0.012    83.367
> > 9      8       1        2 -0.103    0.006   162.778
> > 10     8       2        3  0.138    0.006   162.612
> > 11     8       3        4 -0.387    0.006   160.133
> > 12     9       1        1 -0.032    0.023    44.415
> > 13    10       1        5 -0.020    0.058    17.110
> > 14    11       1        1  0.128    0.017    59.999
> > 15    12       1        1 -0.262    0.032    31.505
> > 16    13       1        1 -0.046    0.071    14.080
> > 17    14       1        6 -0.324    0.003   381.620
> > 18    14       2        6 -0.409    0.003   378.611
> > 19    14       3        7  0.080    0.003   385.319
> > 20    14       4        7 -0.140    0.003   385.542
> > 21    15       1        1  0.311    0.005   185.364
> > 22    16       1        1  0.036    0.005   205.063
> > 23    17       1        6 -0.259    0.001   925.643
> > 24    17       2        7  0.196    0.001   928.897
> > 25    18       1        1  0.157    0.013    74.094
> > 26    19       1        1  0.000    0.056    17.985
> > 27    20       1        1  0.000    0.074    13.600
> > 28    21       1        6 -0.013    0.039    25.425
> > 29    21       2        7 -0.004    0.039    25.426
> > 30    22       1        1 -0.202    0.001  1487.992
> > 31    23       1        1  0.000    0.086    11.628
> > 32    24       1        1 -0.221    0.001   713.110
> > 33    25       1        1 -0.099    0.001   749.964
> > 34    26       1        5 -0.165    0.000  6505.024
> > 35    27       1        1 -0.523    0.063    15.856
> > 36    28       1        1  0.000    0.001  1611.801
> > 37    29       1        6  0.377    0.045    22.045
> > 38    29       2        7  0.575    0.046    21.677
> > 39    30       1        1  0.590    0.074    13.477
> > 40    31       1        1  0.020    0.001  1335.991
> > 41    32       1        1  0.121    0.043    23.489
> > 42    33       1        1 -0.101    0.003   363.163
> > 43    34       1        1 -0.101    0.003   369.507
> > 44    35       1        1 -0.104    0.004   255.507
> > 45    36       1        1 -0.270    0.003   340.761
> > 46    37       1        1  0.179    0.150     6.645
> > 47    38       1        2  0.468    0.020    51.255
> > 48    38       2        4 -0.479    0.020    51.193
> > 49    39       1        5 -0.081    0.024    42.536
> > 50    40       1        1 -0.071    0.043    23.519
> > 51    41       1        1  0.201    0.077    13.036
> > 52    42       1        6 -0.070    0.006   180.844
> > 53    42       2        7  0.190    0.006   180.168
> > 54    43       1        1  0.277    0.013    79.220
> > 55    44       1        5 -0.086    0.001   903.924
> > 56    45       1        5 -0.338    0.002   469.260
> > 57    46       1        1  0.262    0.003   290.330
> > 58    47       1        5  0.000    0.003   304.959
> > 59    48       1        1 -0.645    0.055    18.192
> > 60    49       1        5 -0.120    0.002   461.802
> > 61    50       1        5 -0.286    0.009   106.189
> > 62    51       1        1 -0.124    0.006   172.261
> > 63    52       1        1  0.023    0.028    35.941
> > 64    53       1        5 -0.064    0.001   944.600
> > 65    54       1        1  0.000    0.043    23.010
> > 66    55       1        1  0.000    0.014    72.723
> > 67    56       1        5  0.000    0.012    85.832
> > 68    57       1        1  0.000    0.012    85.832
> >
> >
> > On Wed, May 13, 2020 at 6:00 PM Ben Bolker <bbolker at gmail.com> wrote:
> >
> >>     Without looking very carefully at this:
> >>
> >> * unless your response variable is somehow already centered at zero by
> >> design, a model with no intercept at all is going to be
> >> weird/problematic (random effects are always zero-centered by definition).
> >>
> >> * is it really OK to have an infinite scale in your wishart prior?  (It
> >> may be fine, I'm not immediately familiar with the blme
> >> parameterizations, it just looks weird)
> >>
> >> * the fact that your standard devs are all exactly 1 suggests that the
> >> optimizer bailed out before actually doing anything (these are the
> >> default starting values).
> >>
> >>    Can you provide a reproducible example?
> >>
> >> On 5/13/20 8:53 PM, Sijia Huang wrote:
> >> > Hi everyone,
> >> > I am fitting a cross-classified model with blme, but getting 1 optimizer
> >> > warning. The code and output are shown below. Any suggestions regarding
> >> > fixing the estimation issue? Thanks!
> >> >
> >> >
> >> >> meta.example <- blmer(g~0+(1|Study)+(1|Subscale)+
> >> > 1|Outcome:Study:Subscale),
> >> > +                       data=meta, weights = Variance,
> >> > +                       resid.prior = point(1),
> >> > +                       control = lmerControl(optimizer="bobyqa"))
> >> >
> >> >> meta.example
> >> > Cov prior  : Outcome:Study:Subscale ~ wishart(df = 3.5, scale = Inf,
> >> > posterior.scale = cov, common.scale = TRUE)
> >> >             : Study ~ wishart(df = 3.5, scale = Inf, posterior.scale =
> >> cov,
> >> > common.scale = TRUE)
> >> >             : Subscale ~ wishart(df = 3.5, scale = Inf, posterior.scale
> >> =
> >> > cov, common.scale = TRUE)
> >> > Resid prior: point(value = 1)
> >> > Prior dev  : NaN
> >> >
> >> > Linear mixed model fit by maximum likelihood  ['blmerMod']
> >> > Formula: g ~ 0 + (1 | Study) + (1 | Subscale) + (1 |
> >> Outcome:Study:Subscale)
> >> >     Data: meta
> >> > Weights: Variance
> >> >       AIC      BIC   logLik deviance df.resid
> >> >       Inf      Inf     -Inf      Inf       64
> >> > Random effects:
> >> >   Groups                 Name        Std.Dev.
> >> >   Outcome:Study:Subscale (Intercept) 1
> >> >   Study                  (Intercept) 1
> >> >   Subscale               (Intercept) 1
> >> >   Residual                           1
> >> > Number of obs: 68, groups:  Outcome:Study:Subscale, 68; Study, 57;
> >> > Subscale, 7
> >> > No fixed effect coefficients
> >> > convergence code 0; 1 optimizer warnings; 0 lme4 warnings
> >> >
> >> >
> >> >
> >> >
> >> > Best,
> >> > Sijia
> >> >
> >> >       [[alternative HTML version deleted]]
> >> >
> >> > _______________________________________________
> >> > R-sig-mixed-models at r-project.org mailing list
> >> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>
> >> _______________________________________________
> >> R-sig-mixed-models at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>
> >
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From hu@ng@jcc @end|ng |rom gm@||@com  Thu May 14 04:57:54 2020
From: hu@ng@jcc @end|ng |rom gm@||@com (Sijia Huang)
Date: Wed, 13 May 2020 19:57:54 -0700
Subject: [R-sig-ME] blme optimizer warnings
In-Reply-To: <CA+++UwR10CKhGDktCjNZ5o0V4mvER9ayf6g2_i4KwW7mzZu3iA@mail.gmail.com>
References: <CAPmBuzFQdYFkx1qmj-i9x0CJLOyhXuUh+djpdUnY08xYAzJy9A@mail.gmail.com>
 <ee3fa987-da28-e327-6b8f-a54beefc7439@gmail.com>
 <CAPmBuzGnb3_dO2KSopPi9TVc5H=d2PkQP50KuyS-ANtTT6Zatg@mail.gmail.com>
 <a02c2f89-1dbb-14e3-00df-4db6cf81a158@gmail.com>
 <CAPmBuzHgH7647ho2fi=kJKn0tgBLaiVfjdURjR9s6DRbmJWcfQ@mail.gmail.com>
 <CA+++UwR10CKhGDktCjNZ5o0V4mvER9ayf6g2_i4KwW7mzZu3iA@mail.gmail.com>
Message-ID: <CAPmBuzEc6ush39HBj6iOXiZuNuMSnYWK3FfkEvtupt0oNct_ww@mail.gmail.com>

Thank you so much, Vincent!

On Wed, May 13, 2020 at 7:54 PM Vincent Dorie <vdorie at gmail.com> wrote:

> A couple of guesses here in addition to what Ben mentioned, but you
> likely don't want a prior on the covariance of the random effects and
> the weights should be on the scale of inverse variances. The following
> replicates the numbers for the CCREM column from table 1:
>
> blmer(g ~ 1 + (1 | Study) + (1 | Subscale) + (1 | Outcome:Study:Subscale),
>       data = meta, weights = Precision, control =
> lmerControl(optimizer = "bobyqa"),
>       resid.prior = point(1), cov.prior = NULL)
>
>
> On Wed, May 13, 2020 at 10:04 PM Sijia Huang <huangsjcc at gmail.com> wrote:
> >
> > Here it is. Thanks!
> >
> > A demonstration and evaluation of the use of cross-classified
> > random-effects models for meta-analysis
> >
> > On Wed, May 13, 2020 at 6:57 PM Ben Bolker <bbolker at gmail.com> wrote:
> >
> > >
> > >   Can you give a more specific reference? I can't immediately guess
> from
> > > Fern?ndez-Castilla's google scholar page which article it is ...
> > > On 5/13/20 9:36 PM, Sijia Huang wrote:
> > >
> > > Thanks for the quick reply, Ben!
> > >
> > > I am replicating the Fern?ndez-Castilla et al. (2018) article. Below
> are
> > > the data they have in the article. Anything I can do to resolve the
> issue?
> > > Thanks!
> > >
> > > > meta
> > >    Study Outcome Subscale      g Variance Precision
> > > 1      1       1        1 -0.251    0.024    41.455
> > > 2      2       1        1 -0.069    0.001  1361.067
> > > 3      3       1        5  0.138    0.001   957.620
> > > 4      4       1        1 -0.754    0.085    11.809
> > > 5      5       1        1 -0.228    0.020    49.598
> > > 6      6       1        6 -0.212    0.004   246.180
> > > 7      6       2        7  0.219    0.004   246.095
> > > 8      7       1        1  0.000    0.012    83.367
> > > 9      8       1        2 -0.103    0.006   162.778
> > > 10     8       2        3  0.138    0.006   162.612
> > > 11     8       3        4 -0.387    0.006   160.133
> > > 12     9       1        1 -0.032    0.023    44.415
> > > 13    10       1        5 -0.020    0.058    17.110
> > > 14    11       1        1  0.128    0.017    59.999
> > > 15    12       1        1 -0.262    0.032    31.505
> > > 16    13       1        1 -0.046    0.071    14.080
> > > 17    14       1        6 -0.324    0.003   381.620
> > > 18    14       2        6 -0.409    0.003   378.611
> > > 19    14       3        7  0.080    0.003   385.319
> > > 20    14       4        7 -0.140    0.003   385.542
> > > 21    15       1        1  0.311    0.005   185.364
> > > 22    16       1        1  0.036    0.005   205.063
> > > 23    17       1        6 -0.259    0.001   925.643
> > > 24    17       2        7  0.196    0.001   928.897
> > > 25    18       1        1  0.157    0.013    74.094
> > > 26    19       1        1  0.000    0.056    17.985
> > > 27    20       1        1  0.000    0.074    13.600
> > > 28    21       1        6 -0.013    0.039    25.425
> > > 29    21       2        7 -0.004    0.039    25.426
> > > 30    22       1        1 -0.202    0.001  1487.992
> > > 31    23       1        1  0.000    0.086    11.628
> > > 32    24       1        1 -0.221    0.001   713.110
> > > 33    25       1        1 -0.099    0.001   749.964
> > > 34    26       1        5 -0.165    0.000  6505.024
> > > 35    27       1        1 -0.523    0.063    15.856
> > > 36    28       1        1  0.000    0.001  1611.801
> > > 37    29       1        6  0.377    0.045    22.045
> > > 38    29       2        7  0.575    0.046    21.677
> > > 39    30       1        1  0.590    0.074    13.477
> > > 40    31       1        1  0.020    0.001  1335.991
> > > 41    32       1        1  0.121    0.043    23.489
> > > 42    33       1        1 -0.101    0.003   363.163
> > > 43    34       1        1 -0.101    0.003   369.507
> > > 44    35       1        1 -0.104    0.004   255.507
> > > 45    36       1        1 -0.270    0.003   340.761
> > > 46    37       1        1  0.179    0.150     6.645
> > > 47    38       1        2  0.468    0.020    51.255
> > > 48    38       2        4 -0.479    0.020    51.193
> > > 49    39       1        5 -0.081    0.024    42.536
> > > 50    40       1        1 -0.071    0.043    23.519
> > > 51    41       1        1  0.201    0.077    13.036
> > > 52    42       1        6 -0.070    0.006   180.844
> > > 53    42       2        7  0.190    0.006   180.168
> > > 54    43       1        1  0.277    0.013    79.220
> > > 55    44       1        5 -0.086    0.001   903.924
> > > 56    45       1        5 -0.338    0.002   469.260
> > > 57    46       1        1  0.262    0.003   290.330
> > > 58    47       1        5  0.000    0.003   304.959
> > > 59    48       1        1 -0.645    0.055    18.192
> > > 60    49       1        5 -0.120    0.002   461.802
> > > 61    50       1        5 -0.286    0.009   106.189
> > > 62    51       1        1 -0.124    0.006   172.261
> > > 63    52       1        1  0.023    0.028    35.941
> > > 64    53       1        5 -0.064    0.001   944.600
> > > 65    54       1        1  0.000    0.043    23.010
> > > 66    55       1        1  0.000    0.014    72.723
> > > 67    56       1        5  0.000    0.012    85.832
> > > 68    57       1        1  0.000    0.012    85.832
> > >
> > >
> > > On Wed, May 13, 2020 at 6:00 PM Ben Bolker <bbolker at gmail.com> wrote:
> > >
> > >>     Without looking very carefully at this:
> > >>
> > >> * unless your response variable is somehow already centered at zero by
> > >> design, a model with no intercept at all is going to be
> > >> weird/problematic (random effects are always zero-centered by
> definition).
> > >>
> > >> * is it really OK to have an infinite scale in your wishart prior?
> (It
> > >> may be fine, I'm not immediately familiar with the blme
> > >> parameterizations, it just looks weird)
> > >>
> > >> * the fact that your standard devs are all exactly 1 suggests that the
> > >> optimizer bailed out before actually doing anything (these are the
> > >> default starting values).
> > >>
> > >>    Can you provide a reproducible example?
> > >>
> > >> On 5/13/20 8:53 PM, Sijia Huang wrote:
> > >> > Hi everyone,
> > >> > I am fitting a cross-classified model with blme, but getting 1
> optimizer
> > >> > warning. The code and output are shown below. Any suggestions
> regarding
> > >> > fixing the estimation issue? Thanks!
> > >> >
> > >> >
> > >> >> meta.example <- blmer(g~0+(1|Study)+(1|Subscale)+
> > >> > 1|Outcome:Study:Subscale),
> > >> > +                       data=meta, weights = Variance,
> > >> > +                       resid.prior = point(1),
> > >> > +                       control = lmerControl(optimizer="bobyqa"))
> > >> >
> > >> >> meta.example
> > >> > Cov prior  : Outcome:Study:Subscale ~ wishart(df = 3.5, scale = Inf,
> > >> > posterior.scale = cov, common.scale = TRUE)
> > >> >             : Study ~ wishart(df = 3.5, scale = Inf,
> posterior.scale =
> > >> cov,
> > >> > common.scale = TRUE)
> > >> >             : Subscale ~ wishart(df = 3.5, scale = Inf,
> posterior.scale
> > >> =
> > >> > cov, common.scale = TRUE)
> > >> > Resid prior: point(value = 1)
> > >> > Prior dev  : NaN
> > >> >
> > >> > Linear mixed model fit by maximum likelihood  ['blmerMod']
> > >> > Formula: g ~ 0 + (1 | Study) + (1 | Subscale) + (1 |
> > >> Outcome:Study:Subscale)
> > >> >     Data: meta
> > >> > Weights: Variance
> > >> >       AIC      BIC   logLik deviance df.resid
> > >> >       Inf      Inf     -Inf      Inf       64
> > >> > Random effects:
> > >> >   Groups                 Name        Std.Dev.
> > >> >   Outcome:Study:Subscale (Intercept) 1
> > >> >   Study                  (Intercept) 1
> > >> >   Subscale               (Intercept) 1
> > >> >   Residual                           1
> > >> > Number of obs: 68, groups:  Outcome:Study:Subscale, 68; Study, 57;
> > >> > Subscale, 7
> > >> > No fixed effect coefficients
> > >> > convergence code 0; 1 optimizer warnings; 0 lme4 warnings
> > >> >
> > >> >
> > >> >
> > >> >
> > >> > Best,
> > >> > Sijia
> > >> >
> > >> >       [[alternative HTML version deleted]]
> > >> >
> > >> > _______________________________________________
> > >> > R-sig-mixed-models at r-project.org mailing list
> > >> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > >>
> > >> _______________________________________________
> > >> R-sig-mixed-models at r-project.org mailing list
> > >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > >>
> > >
> >
> >         [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n|  Thu May 14 09:22:08 2020
From: wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n| (Viechtbauer, Wolfgang (SP))
Date: Thu, 14 May 2020 07:22:08 +0000
Subject: [R-sig-ME] blme optimizer warnings
In-Reply-To: <CAPmBuzEc6ush39HBj6iOXiZuNuMSnYWK3FfkEvtupt0oNct_ww@mail.gmail.com>
References: <CAPmBuzFQdYFkx1qmj-i9x0CJLOyhXuUh+djpdUnY08xYAzJy9A@mail.gmail.com>
 <ee3fa987-da28-e327-6b8f-a54beefc7439@gmail.com>
 <CAPmBuzGnb3_dO2KSopPi9TVc5H=d2PkQP50KuyS-ANtTT6Zatg@mail.gmail.com>
 <a02c2f89-1dbb-14e3-00df-4db6cf81a158@gmail.com>
 <CAPmBuzHgH7647ho2fi=kJKn0tgBLaiVfjdURjR9s6DRbmJWcfQ@mail.gmail.com>
 <CA+++UwR10CKhGDktCjNZ5o0V4mvER9ayf6g2_i4KwW7mzZu3iA@mail.gmail.com>
 <CAPmBuzEc6ush39HBj6iOXiZuNuMSnYWK3FfkEvtupt0oNct_ww@mail.gmail.com>
Message-ID: <5963855da6794fedb245c8a00e15dba3@UM-MAIL3214.unimaas.nl>

A while ago, I looked at this article and replicated the results with metafor.

library(metafor)

meta$Variance <- 1/meta$Precision

res <- rma.mv(g, Variance, random = list(~ 1 | Study/Outcome, ~ 1 | Subscale), data=meta)
res

The variances given in the article are quite heavily rounded and one value is even given as 0. With meta$Variance <- 1/meta$Precision, we can recreate the variances to avoid this issue.

Best,
Wolfgang

>-----Original Message-----
>From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org]
>On Behalf Of Sijia Huang
>Sent: Thursday, 14 May, 2020 4:58
>To: Vincent Dorie
>Cc: r-sig-mixed-models
>Subject: Re: [R-sig-ME] blme optimizer warnings
>
>Thank you so much, Vincent!
>
>On Wed, May 13, 2020 at 7:54 PM Vincent Dorie <vdorie at gmail.com> wrote:
>
>> A couple of guesses here in addition to what Ben mentioned, but you
>> likely don't want a prior on the covariance of the random effects and
>> the weights should be on the scale of inverse variances. The following
>> replicates the numbers for the CCREM column from table 1:
>>
>> blmer(g ~ 1 + (1 | Study) + (1 | Subscale) + (1 | Outcome:Study:Subscale),
>>       data = meta, weights = Precision, control =
>> lmerControl(optimizer = "bobyqa"),
>>       resid.prior = point(1), cov.prior = NULL)
>>
>> On Wed, May 13, 2020 at 10:04 PM Sijia Huang <huangsjcc at gmail.com> wrote:
>> >
>> > Here it is. Thanks!
>> >
>> > A demonstration and evaluation of the use of cross-classified
>> > random-effects models for meta-analysis

From d@@iei@schiu@egger m@iii@g oii psy@u@ibe@ch  Thu May 14 18:42:26 2020
From: d@@iei@schiu@egger m@iii@g oii psy@u@ibe@ch (d@@iei@schiu@egger m@iii@g oii psy@u@ibe@ch)
Date: Thu, 14 May 2020 16:42:26 +0000
Subject: [R-sig-ME] Prediction/classification & variable selection
Message-ID: <cea49d095185425fa766109f9760ddd3@psy.unibe.ch>

Dear people of r-sig-mixed-models at r-project.org

My name is Daniel Schlunegger, PhD-student in Psychology at the University of Bern, Switzerland. 

I?m new here and I wondered if somebody can help me. 

My goal is to predict subjects responses based on their previous responses in a one-interval two-alternative forced choice auditory discrimination task (Was it tone A or tone B sort of task). I?ve ran an experiment with 24 subjects, each subject performed 1200 trials ( = 28800 trials). There are no missing values, all data is ?clean?. 

The main idea of my work is:
1) Take subjects? responses
2) Compute some statistics with those responses
3) Use these statistics to predict the next response (in a trial-by-trial fashion)

Goal: Prediction / Classification (binary outcome)

>From three different learning models I derived three predictors. More clearly, three different sets of predictors. Within each set, there are n predictors (normally distributed). The predictors within each set are of very similar nature. I need a model with three predictors, one of each set of predictors. From each set of predictors, there is one predictor in the model:

y ~ predictor1_n + predictor2_n + predictor3_n


Problem: Theoretically it is possible (or rather probable) that for each subject a different combination of predictors (e.g. predictor1_2 + predictor2_1 + predictor3_3 vs. predictor1_1 + predictor2_2 + predictor3_3) results in a better classification accuracy. On the other hand I would like to keep the model as simple as possible. Let?s say, having the same three predictors for all subjects, while accounting for differences with a random intercept (1 | subject) or random intercept and random slope. 

I?ve seen a lot of work where they perform subject-level and group-level analyses, but I think that?s actually not correct, right? 

Do you have any suggestions how to do this the proper way? I assume that just running n * n * n different GLMM?s (lme4::glmer()) is not the proper way to do it. Because that is what I did so far, and then checked what combination gives me the best prediction. 

(I have another dataset from a slightly different version of the experiment. This dataset contains 91200 trials from 76 subjects, if number of observations is an issue here)

Thanks for considering my request. 

Kind regards, 
Daniel Schlunegger

From c@c@voeten @end|ng |rom hum@|e|denun|v@n|  Thu May 14 21:41:00 2020
From: c@c@voeten @end|ng |rom hum@|e|denun|v@n| (Voeten, C.C.)
Date: Thu, 14 May 2020 19:41:00 +0000
Subject: [R-sig-ME] Prediction/classification & variable selection
In-Reply-To: <cea49d095185425fa766109f9760ddd3@psy.unibe.ch>
References: <cea49d095185425fa766109f9760ddd3@psy.unibe.ch>
Message-ID: <fb219ae846bc42c5b29117d6b5c69933@hum.leidenuniv.nl>

Dear Daniel,

Maybe my understanding of your situation is a bit too simplistic, but it sounds like you have a classic case of model selection / feature selection? There are many approaches for that. The easiest would be likelihood-ratio tests (or AIC, or BIC, or some other criterion). Start with a full model (or as full as you can get while still achieving convergence) containing all combinations of predictors, remove one term, see if the model improves according to your criterion... repeat until no terms are left to be eliminated. There are many packages that can automate this procedure for you. Another option could be lasso or ridge regression, which are commonly used for feature selection in the classification literature. I don't know if the lasso has been implemented for mixed models, but I know that package mgcv allows you to specify ridge penalties via (see the documentation related to the paraPen argument).

HTH,
Cesko

> -----Original Message-----
> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On
> Behalf Of daniel.schlunegger at psy.unibe.ch
> Sent: Thursday, May 14, 2020 6:42 PM
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] Prediction/classification & variable selection
> 
> Dear people of r-sig-mixed-models at r-project.org
> 
> My name is Daniel Schlunegger, PhD-student in Psychology at the University
> of Bern, Switzerland.
> 
> I?m new here and I wondered if somebody can help me.
> 
> My goal is to predict subjects responses based on their previous responses in
> a one-interval two-alternative forced choice auditory discrimination task
> (Was it tone A or tone B sort of task). I?ve ran an experiment with 24
> subjects, each subject performed 1200 trials ( = 28800 trials). There are no
> missing values, all data is ?clean?.
> 
> The main idea of my work is:
> 1) Take subjects? responses
> 2) Compute some statistics with those responses
> 3) Use these statistics to predict the next response (in a trial-by-trial fashion)
> 
> Goal: Prediction / Classification (binary outcome)
> 
> From three different learning models I derived three predictors. More
> clearly, three different sets of predictors. Within each set, there are n
> predictors (normally distributed). The predictors within each set are of very
> similar nature. I need a model with three predictors, one of each set of
> predictors. From each set of predictors, there is one predictor in the model:
> 
> y ~ predictor1_n + predictor2_n + predictor3_n
> 
> 
> Problem: Theoretically it is possible (or rather probable) that for each subject
> a different combination of predictors (e.g. predictor1_2 + predictor2_1 +
> predictor3_3 vs. predictor1_1 + predictor2_2 + predictor3_3) results in a
> better classification accuracy. On the other hand I would like to keep the
> model as simple as possible. Let?s say, having the same three predictors for
> all subjects, while accounting for differences with a random intercept (1 |
> subject) or random intercept and random slope.
> 
> I?ve seen a lot of work where they perform subject-level and group-level
> analyses, but I think that?s actually not correct, right?
> 
> Do you have any suggestions how to do this the proper way? I assume that
> just running n * n * n different GLMM?s (lme4::glmer()) is not the proper way
> to do it. Because that is what I did so far, and then checked what combination
> gives me the best prediction.
> 
> (I have another dataset from a slightly different version of the experiment.
> This dataset contains 91200 trials from 76 subjects, if number of observations
> is an issue here)
> 
> Thanks for considering my request.
> 
> Kind regards,
> Daniel Schlunegger
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From tr|chter @end|ng |rom un|-bremen@de  Thu May 14 22:02:18 2020
From: tr|chter @end|ng |rom un|-bremen@de (Tim Richter-Heitmann | Universitaet Bremen)
Date: Thu, 14 May 2020 22:02:18 +0200
Subject: [R-sig-ME] Prediction/classification & variable selection
In-Reply-To: <fb219ae846bc42c5b29117d6b5c69933@hum.leidenuniv.nl>
References: <cea49d095185425fa766109f9760ddd3@psy.unibe.ch>
 <fb219ae846bc42c5b29117d6b5c69933@hum.leidenuniv.nl>
Message-ID: <20200514220218.Horde.9BIHac-GmCK8p-CxH1f4SK-@webmail.uni-bremen.de>

Dear Daniel,
to build upon Cesko's comment, if your problem is indeed a problem of  
"classic" model selection, the package MuMin does this by basically  
testing every combination of predictor variables:

https://cran.r-project.org/web/packages/MuMIn/MuMIn.pdf

It also takes mixed models.

Cheers, Tim


Zitat von "Voeten, C.C." <c.c.voeten at hum.leidenuniv.nl>:

> Dear Daniel,
>
> Maybe my understanding of your situation is a bit too simplistic,  
> but it sounds like you have a classic case of model selection /  
> feature selection? There are many approaches for that. The easiest  
> would be likelihood-ratio tests (or AIC, or BIC, or some other  
> criterion). Start with a full model (or as full as you can get while  
> still achieving convergence) containing all combinations of  
> predictors, remove one term, see if the model improves according to  
> your criterion... repeat until no terms are left to be eliminated.  
> There are many packages that can automate this procedure for you.  
> Another option could be lasso or ridge regression, which are  
> commonly used for feature selection in the classification  
> literature. I don't know if the lasso has been implemented for mixed  
> models, but I know that package mgcv allows you to specify ridge  
> penalties via (see the documentation related to the paraPen argument).
>
> HTH,
> Cesko
>
>> -----Original Message-----
>> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On
>> Behalf Of daniel.schlunegger at psy.unibe.ch
>> Sent: Thursday, May 14, 2020 6:42 PM
>> To: r-sig-mixed-models at r-project.org
>> Subject: [R-sig-ME] Prediction/classification & variable selection
>>
>> Dear people of r-sig-mixed-models at r-project.org
>>
>> My name is Daniel Schlunegger, PhD-student in Psychology at the University
>> of Bern, Switzerland.
>>
>> I?m new here and I wondered if somebody can help me.
>>
>> My goal is to predict subjects responses based on their previous  
>> responses in
>> a one-interval two-alternative forced choice auditory discrimination task
>> (Was it tone A or tone B sort of task). I?ve ran an experiment with 24
>> subjects, each subject performed 1200 trials ( = 28800 trials). There are no
>> missing values, all data is ?clean?.
>>
>> The main idea of my work is:
>> 1) Take subjects? responses
>> 2) Compute some statistics with those responses
>> 3) Use these statistics to predict the next response (in a  
>> trial-by-trial fashion)
>>
>> Goal: Prediction / Classification (binary outcome)
>>
>> From three different learning models I derived three predictors. More
>> clearly, three different sets of predictors. Within each set, there are n
>> predictors (normally distributed). The predictors within each set  
>> are of very
>> similar nature. I need a model with three predictors, one of each set of
>> predictors. From each set of predictors, there is one predictor in  
>> the model:
>>
>> y ~ predictor1_n + predictor2_n + predictor3_n
>>
>>
>> Problem: Theoretically it is possible (or rather probable) that for  
>> each subject
>> a different combination of predictors (e.g. predictor1_2 + predictor2_1 +
>> predictor3_3 vs. predictor1_1 + predictor2_2 + predictor3_3) results in a
>> better classification accuracy. On the other hand I would like to keep the
>> model as simple as possible. Let?s say, having the same three predictors for
>> all subjects, while accounting for differences with a random intercept (1 |
>> subject) or random intercept and random slope.
>>
>> I?ve seen a lot of work where they perform subject-level and group-level
>> analyses, but I think that?s actually not correct, right?
>>
>> Do you have any suggestions how to do this the proper way? I assume that
>> just running n * n * n different GLMM?s (lme4::glmer()) is not the  
>> proper way
>> to do it. Because that is what I did so far, and then checked what  
>> combination
>> gives me the best prediction.
>>
>> (I have another dataset from a slightly different version of the experiment.
>> This dataset contains 91200 trials from 76 subjects, if number of  
>> observations
>> is an issue here)
>>
>> Thanks for considering my request.
>>
>> Kind regards,
>> Daniel Schlunegger
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



-- 
Tim Richter-Heitmann
Universit?t Bremen


From c@c@voeten @end|ng |rom hum@|e|denun|v@n|  Fri May 15 14:53:02 2020
From: c@c@voeten @end|ng |rom hum@|e|denun|v@n| (Voeten, C.C.)
Date: Fri, 15 May 2020 12:53:02 +0000
Subject: [R-sig-ME] Prediction/classification & variable selection
In-Reply-To: <fde0ed9b1d7e4434a88560ac249b9555@psy.unibe.ch>
References: <cea49d095185425fa766109f9760ddd3@psy.unibe.ch>,
 <fb219ae846bc42c5b29117d6b5c69933@hum.leidenuniv.nl>
 <fde0ed9b1d7e4434a88560ac249b9555@psy.unibe.ch>
Message-ID: <702c6d70253a4ec49074fc7ddd5f60b3@hum.leidenuniv.nl>

Dear Daniel,

Please keep the list in cc.

I know exactly what you mean, writing a loop and manually extracting AIC/BIC... that is exactly how I ended up writing package buildmer, which automates precisely that. Tim already suggested MuMIn to you as well; you would need to see which of the many packages (e.g. I can also think of lmerTest::step) serves your needs best. Buildmer's advantage is that it first tries to build up the maximal model from zero before doing backward elimination, so if your maximal model isn't capable of converging, buildmer will automatically give you the largest subset that does converge (plus removing terms that are not significant in backward elimination based on LRT/AIC/BIC/etc). I can't comment on the other packages, you would really need to experiment which of them works best for your own purposes.

Re multiple testing: in my view, for hypothesis testing based on p-values, you are only using one model, which you just happened to have done some pruning on first. In that sense, you wouldn't need to apply any corrections. However it is also well known that model selection will amplify spurious effects, and I also do see where authors like Hastie & Tibshirani are coming from, saying in their book that the standard errors of a pruned model are invalid because they don't take into account the selection procedure. Ultimately, this is a highly contested issue and you'd best either follow whatever is customary in your field, or use some kind of simulation-based approach to obtain p-values that do take into account the selection procedure. (I wouldn't really know how, and I am not aware of any literature giving a clear recipe for that, but maybe others have ideas here.)

For lasso/ridge, you definitely would not need to perform any manual corrections, as in those cases selection takes place as part of the fitting process as well, so the p-values will be correct in any case (well, barring of course the general issue of p-values in mixed models...).

Best,
Cesko

> -----Original Message-----
> From: daniel.schlunegger at psy.unibe.ch <daniel.schlunegger at psy.unibe.ch>
> Sent: Friday, May 15, 2020 1:35 PM
> To: Voeten, C.C. <c.c.voeten at hum.leidenuniv.nl>
> Subject: AW: Prediction/classification & variable selection
> 
> Dear Cesko
> 
> Thank you for your reply. Based on your comment I feel confident that I'm
> not completely off the track. Because this is what I did before, looking at
> information criteria. Thank you also for the hint that there exist packages to
> do that. Because before i did this with a loop and extracting AIC, BIC, etc. I
> know horrible.
> 
> I also wondered if I have to correct for multiple testing in one way or the
> other since I'm testing all possible combinations? Thank you also for the tip
> with lasso and ridge regression, that also crossed my mind lately. I'm not too
> familiar yet with these two though.
> 
> Thank you for taking the time.
> 
> Best wishes,
> Daniel
> 
> ________________________________________
> Von: Voeten, C.C. [c.c.voeten at hum.leidenuniv.nl]
> Gesendet: Donnerstag, 14. Mai 2020 21:41
> An: Schlunegger, Daniel (PSY); r-sig-mixed-models at r-project.org
> Betreff: RE: Prediction/classification & variable selection
> 
> Dear Daniel,
> 
> Maybe my understanding of your situation is a bit too simplistic, but it sounds
> like you have a classic case of model selection / feature selection? There are
> many approaches for that. The easiest would be likelihood-ratio tests (or AIC,
> or BIC, or some other criterion). Start with a full model (or as full as you can
> get while still achieving convergence) containing all combinations of
> predictors, remove one term, see if the model improves according to your
> criterion... repeat until no terms are left to be eliminated. There are many
> packages that can automate this procedure for you. Another option could be
> lasso or ridge regression, which are commonly used for feature selection in
> the classification literature. I don't know if the lasso has been implemented
> for mixed models, but I know that package mgcv allows you to specify ridge
> penalties via (see the documentation related to the paraPen argument).
> 
> HTH,
> Cesko
> 
> > -----Original Message-----
> > From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org>
> On
> > Behalf Of daniel.schlunegger at psy.unibe.ch
> > Sent: Thursday, May 14, 2020 6:42 PM
> > To: r-sig-mixed-models at r-project.org
> > Subject: [R-sig-ME] Prediction/classification & variable selection
> >
> > Dear people of r-sig-mixed-models at r-project.org
> >
> > My name is Daniel Schlunegger, PhD-student in Psychology at the
> University
> > of Bern, Switzerland.
> >
> > I?m new here and I wondered if somebody can help me.
> >
> > My goal is to predict subjects responses based on their previous responses
> in
> > a one-interval two-alternative forced choice auditory discrimination task
> > (Was it tone A or tone B sort of task). I?ve ran an experiment with 24
> > subjects, each subject performed 1200 trials ( = 28800 trials). There are no
> > missing values, all data is ?clean?.
> >
> > The main idea of my work is:
> > 1) Take subjects? responses
> > 2) Compute some statistics with those responses
> > 3) Use these statistics to predict the next response (in a trial-by-trial
> fashion)
> >
> > Goal: Prediction / Classification (binary outcome)
> >
> > From three different learning models I derived three predictors. More
> > clearly, three different sets of predictors. Within each set, there are n
> > predictors (normally distributed). The predictors within each set are of very
> > similar nature. I need a model with three predictors, one of each set of
> > predictors. From each set of predictors, there is one predictor in the model:
> >
> > y ~ predictor1_n + predictor2_n + predictor3_n
> >
> >
> > Problem: Theoretically it is possible (or rather probable) that for each
> subject
> > a different combination of predictors (e.g. predictor1_2 + predictor2_1 +
> > predictor3_3 vs. predictor1_1 + predictor2_2 + predictor3_3) results in a
> > better classification accuracy. On the other hand I would like to keep the
> > model as simple as possible. Let?s say, having the same three predictors for
> > all subjects, while accounting for differences with a random intercept (1 |
> > subject) or random intercept and random slope.
> >
> > I?ve seen a lot of work where they perform subject-level and group-level
> > analyses, but I think that?s actually not correct, right?
> >
> > Do you have any suggestions how to do this the proper way? I assume that
> > just running n * n * n different GLMM?s (lme4::glmer()) is not the proper
> way
> > to do it. Because that is what I did so far, and then checked what
> combination
> > gives me the best prediction.
> >
> > (I have another dataset from a slightly different version of the experiment.
> > This dataset contains 91200 trials from 76 subjects, if number of
> observations
> > is an issue here)
> >
> > Thanks for considering my request.
> >
> > Kind regards,
> > Daniel Schlunegger
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From @|exboth@555 @end|ng |rom gm@||@com  Fri May 15 15:28:19 2020
From: @|exboth@555 @end|ng |rom gm@||@com (Alexander Botha)
Date: Fri, 15 May 2020 15:28:19 +0200
Subject: [R-sig-ME] mcmcglmm Priors: Auto correlation and extreme post mean
 values
Message-ID: <CAO3yAjM_6B9tn+fTMaS93WGDV78h35HNXH5qEBhB0ZNLNc+5Sw@mail.gmail.com>

Good day List,
My name is Alex, I am currently using the package mcmcglmm to determine the
impact of lunar cycles, human presence and land use type (agricultural vs
protected) on the trapping success of meso-predators for my PhD. I am new
to MCMCglmm and I was wondering if you could assist with my problem.

Structure of data: I am testing if and how the change in lunar cycle
(factor) and human presence ( factor) impact trapping success. Trapping
success is count data but because we only trapped 1 individual at most, the
data can also be considered binary. Human presence (HP) is split into 5
categories.

Model: I am running Human presence, Moon phase and Land use type as fixed
effects and the site name as a random effect.

Problem: I have quantified human presence in various locations, with areas
exposed to intense human pressures, there is a complete absence of any
successful trappings (HP2 and HP4 have no trapping success resulting in
only zeros). Post mcmcglmm, these parameters display auto correlation in
their graphs (if i set them as fixed or random effects) as well as when
using the geweke and gelman tests, but almost perfect mixing for the
others, especially when I run it with 5-20 million iterations. I also see
poorly mixed VCV graphs if the degree of belief is less than 20. I have
used a variety of priors, see below, with no success. I have also increased
the amount of itterations to 20 million, decreased the thinning factor,
increased the burnin and used poisson, zapoisson, zipoisson, ordinal and
threshold distributions, with threshold and ordinal having the best DIC
values. Together with this, I am also seeing extreme post mean values for
the models that are displaying the lowest DIC values, which is not
something that I see in any of the literature (see below).

I have read Jarrod Hadfields awesome course and tutorial notes, as well as
other literature in my field, online tutorials, information documents and
the vignettes and help functions in R as well as the correspondence on this
email list between Jarrod Hadfield and other users but I cannot seem to
figure out why the above is happening. My data set is quite small (176
entries in total, with about 40 entries per HP category) and according to
what I have read, priors can have a large impact on small to moderately
sized data sets.

Questions:
1. Does the problem lie with my priors? And if so, do you have any tips on
how to solve it?
2. Should I be using ordinal or threshold family? I have read that the
mixing of zi and za poisson models can be poor, and from my tries with
them, this seems to be the case.
3. Are either the extreme post mean values or the auto correlation in
certain parameters avoidable in this case?

*Examples of priors*
prior1<-list(R=list(V=diag(1)*1e-8, nu=0.2),G=list(G1=list(V=diag(1)*1e-6,
nu=0.2))
prior2<-list(R=list(V=diag(1), nu=0.2),G=list(G1=list(V=diag(1), nu=2)))
prior3<-list(R=list(V=diag(2), nu=0, fix=2), G=list(G1=G1))
G1=list(V=diag(2), nu=2, alpha.mu=c(0,0), alpha.V=diag(2)*1000)
prior4:
A note: I have these with degree belief ranging from 0.0002 to 20, with the
best mixing (apart from the auto correlation in certain parameters) with nu
set at 20 for the R structure. If i set the G structures degree of belief
to less than 20, it mixes the random effect extremely poorly. I have also
used ranging values from 1e-1 to 1e-20 and fixed the variances at 1 and 2
using various different priors.
*Examples of model scripts:*
OM1.1<-MCMCglmm(Jackal_trapped ~ Human_presence +  MP + Landuse, random
=~Site, data = data, prior = prior1,
family = "ordinal", nitt = 5000000, thin = 500, burnin = 500000, verbose =
FALSE, pl = TRUE, DIC=TRUE)
model1.1<-MCMCglmm(Jackal_trapped~trait,random=~us(trait):Human_presence +
us(trait):Landuse,family="zapoisson",rcov=~idh(trait):units,data=data,prior=prior,nitt=500000,thin=500,burnin=200000,verbose=FALSE)

model1.2<-MCMCglmm(Jackal_trapped~trait,random=~idh(trait):Human_presence +
idh(trait):Landuse,family="zapoisson",rcov=~idh(trait):units,data=data,prior=prior,nitt=500000,thin=500,burnin=200000,verbose=FALSE)

*Example of large post mean summary*
 family: I have used zapoisson, ordinal and zipoisson distributions
prior1<-list(R=list(V=diag(1)*1e-4, nu=0.2),G=list(G1=list(V=diag(1)*1e-2,
nu=2)))
OM1.1<-MCMCglmm(Jackal_trapped ~ Human_presence +  MP + Landuse, random
=~Site, data = data, prior = prior1, family = "family", nitt = 5000000,
thin = 500, burnin = 500000, verbose = FALSE, pl = TRUE, DIC=TRUE)
 Iterations = 500001:4999501
 Thinning interval  = 500
 Sample size  = 9000

 DIC: 0.003093406

 G-structure:  ~Site

     post.mean l-95% CI u-95% CI eff.samp
Site    0.1943 0.001154   0.1913     9000

 R-structure:  ~units

      post.mean l-95% CI u-95% CI eff.samp
units 1.755e+10 1.72e+09 3.97e+10     3632

 Location effects: Jackal_trapped ~ Human_presence + MP + Landuse
                              post.mean    l-95% CI  u-95% CI eff.samp
 pMCMC
(Intercept)                  -107789  -229797     9915     4666
0.0498 *
Human_presence1     49459   -12241      117809     6957         0.1002
Human_presence2    -49280  -153682      46279     8478          0.3184
Human_presence3     18642   -66603      104880     8529          0.6684
Human_presence4   -214029  -340008     -89259     6098          <1e-04 ***
MPFQ                         -44627  -119345      24385       7328
0.1856
MPNM                          12362   -52404      72882       9000
0.6691
MPTQ                           -43907  -122103    22055     8357
 0.1942
Landusereserve            -51105  -138603    35033     9000
 0.2302

*Example of model outputs with auto correlation*
family: I have used zapoisson, ordinal and zipoisson distributions
prior2<-list(R=list(V=diag(1)*1e-6, nu=20),G=list(G1=list(V=diag(1)*1e-6,
nu=20)))
OM2.1<-MCMCglmm(Jackal_trapped ~ Human_presence +  MP + Landuse, random
=~Site, data = data, prior = prior2,
family = "family", nitt = 5000000, thin = 500, burnin = 500000, verbose =
FALSE, pl = TRUE, DIC=TRUE)
 Iterations = 500001:4999501
 Thinning interval  = 500
 Sample size  = 9000

 DIC: 182.6921

 G-structure:  ~Site
     post.mean  l-95% CI  u-95% CI eff.samp
Site 1.114e-06 5.171e-07 1.911e-06     8515

 R-structure:  ~units

      post.mean  l-95% CI u-95% CI eff.samp
units 1.106e-06 5.129e-07 1.87e-06     9000

 Location effects: Jackal_trapped ~ Human_presence + MP + Landuse

                             post.mean l-95% CI u-95% CI eff.samp   pMCMC

(Intercept)               -0.70349 -1.20259 -0.28723    2.027     < 1e-04
***
Human_presence1   0.53752  0.41033  0.67240    6.030    < 1e-04 ***
Human_presence2  -0.44909 -0.65858 -0.03042    2.600    0.00289 **
Human_presence3  -0.11236 -0.48808  0.30079    1.683     0.53556
Human_presence4  -1.38806 -1.74144 -0.93892    2.829    < 1e-04 ***
MPFQ                        -0.38918 -0.57513 -0.23967    2.300    < 1e-04
***
MPNM                         0.14273 -0.05171  0.28830    1.625    0.14356

MPTQ                         -0.25338 -0.52899 -0.03162    1.477    0.01222
*
Landusereserve          -0.64469 -1.06522 -0.18880    2.239    < 1e-04 ***

*Example: Using my variables as random effects*
*family: I have used zapoisson, ordinal and zipoisson distributions*
model1.1<-MCMCglmm(Jackal_trapped~trait,random=~us(trait):Human_presence +
us(trait):Landuse,family="family",rcov=~idh(trait):units,data=data,prior=prior,nitt=500000,thin=500,burnin=200000,verbose=FALSE)
Iterations = 200001:499501
Thinning interval  = 500
Sample size  = 600

 DIC: 175.5933

 G-structure:  ~us(trait):Human_presence


        post.mean   l-95% CI  u-95% CI eff.samp
traitJackal_trapped:traitJackal_trapped.Human_presence 1.179e-06  5.584e-07
2.051e-06    472.7
traitza_Jackal_trapped:traitJackal_trapped.Human_presence    -1.243e-08
-5.964e-07 5.494e-07    600.0
traitJackal_trapped:traitza_Jackal_trapped.Human_presence    -1.243e-08
-5.964e-07 5.494e-07    600.0
traitza_Jackal_trapped:traitza_Jackal_trapped.Human_presence  1.172e-06
 5.252e-07 1.883e-06    507.3

 ~us(trait):Landuse

      post.mean   l-95% CI  u-95% CI eff.samp
traitJackal_trapped:traitJackal_trapped.Landuse       1.143e-06  4.876e-07
2.029e-06      600
traitza_Jackal_trapped:traitJackal_trapped.Landuse    7.237e-09 -5.758e-07
4.980e-07      600
traitJackal_trapped:traitza_Jackal_trapped.Landuse    7.237e-09 -5.758e-07
4.980e-07      600
traitza_Jackal_trapped:traitza_Jackal_trapped.Landuse 1.157e-06  4.896e-07
2.022e-06      600

 R-structure:  ~idh(trait):units

                             post.mean  l-95% CI  u-95% CI eff.samp
traitJackal_trapped.units    9.855e-07 4.689e-07 1.685e-06    704.6
traitza_Jackal_trapped.units 1.017e-06 4.490e-07 1.625e-06    600.0

 Location effects: Jackal_trapped ~ trait

                       post.mean l-95% CI u-95% CI eff.samp  pMCMC
(Intercept)               0.7789   0.7730   0.7875    8.419 <0.002 **
traitza_Jackal_trapped   -2.8911  -2.9052  -2.8741    6.982 <0.002 **

I hope I have shared enough info regarding my problem and that it makes
sense. I hope my post meets the requirements of the list and that it does
not seem like I am making my problem somebody elses, I am honestly just
lost at the moment.
I welcome any constructive criticism and any other help you can provide.

Thank you for your help.
I look forward to your responses.

With kind regards,

Alexander Edward Botha
alexbotha555 at gmail.com 082 414 9030
PhD candidate
Mammal ecology

	[[alternative HTML version deleted]]


From @reedt@8 @end|ng |rom gm@||@com  Fri May 15 16:24:44 2020
From: @reedt@8 @end|ng |rom gm@||@com (sree datta)
Date: Fri, 15 May 2020 10:24:44 -0400
Subject: [R-sig-ME] 
 mcmcglmm Priors: Auto correlation and extreme post mean values
In-Reply-To: <CAO3yAjM_6B9tn+fTMaS93WGDV78h35HNXH5qEBhB0ZNLNc+5Sw@mail.gmail.com>
References: <CAO3yAjM_6B9tn+fTMaS93WGDV78h35HNXH5qEBhB0ZNLNc+5Sw@mail.gmail.com>
Message-ID: <CAHftDbgvGUtnEWJDG4ohnb6XOZgvHLeocB=S_OPkZoirK8aw6Q@mail.gmail.com>

Hi Alexander,

Prior to running the model with "MCMCglmm", have you attempted to run a
log-linear model / a mixed-model with "nlme" or "lme4" / a decision tree? I
ask these questions to better understand what the simpler univariate and
multivariate approaches would reveal in terms of association between your
dependent variable and your predictor variables.

These are the exploratory models I would run to understand what would
represent reasonable priors to use for Bayesian based models. If you could
share more on some of your explorations with the data, that would be
helpful.

Sree

On Fri, May 15, 2020 at 9:29 AM Alexander Botha <alexbotha555 at gmail.com>
wrote:

> Good day List,
> My name is Alex, I am currently using the package mcmcglmm to determine the
> impact of lunar cycles, human presence and land use type (agricultural vs
> protected) on the trapping success of meso-predators for my PhD. I am new
> to MCMCglmm and I was wondering if you could assist with my problem.
>
> Structure of data: I am testing if and how the change in lunar cycle
> (factor) and human presence ( factor) impact trapping success. Trapping
> success is count data but because we only trapped 1 individual at most, the
> data can also be considered binary. Human presence (HP) is split into 5
> categories.
>
> Model: I am running Human presence, Moon phase and Land use type as fixed
> effects and the site name as a random effect.
>
> Problem: I have quantified human presence in various locations, with areas
> exposed to intense human pressures, there is a complete absence of any
> successful trappings (HP2 and HP4 have no trapping success resulting in
> only zeros). Post mcmcglmm, these parameters display auto correlation in
> their graphs (if i set them as fixed or random effects) as well as when
> using the geweke and gelman tests, but almost perfect mixing for the
> others, especially when I run it with 5-20 million iterations. I also see
> poorly mixed VCV graphs if the degree of belief is less than 20. I have
> used a variety of priors, see below, with no success. I have also increased
> the amount of itterations to 20 million, decreased the thinning factor,
> increased the burnin and used poisson, zapoisson, zipoisson, ordinal and
> threshold distributions, with threshold and ordinal having the best DIC
> values. Together with this, I am also seeing extreme post mean values for
> the models that are displaying the lowest DIC values, which is not
> something that I see in any of the literature (see below).
>
> I have read Jarrod Hadfields awesome course and tutorial notes, as well as
> other literature in my field, online tutorials, information documents and
> the vignettes and help functions in R as well as the correspondence on this
> email list between Jarrod Hadfield and other users but I cannot seem to
> figure out why the above is happening. My data set is quite small (176
> entries in total, with about 40 entries per HP category) and according to
> what I have read, priors can have a large impact on small to moderately
> sized data sets.
>
> Questions:
> 1. Does the problem lie with my priors? And if so, do you have any tips on
> how to solve it?
> 2. Should I be using ordinal or threshold family? I have read that the
> mixing of zi and za poisson models can be poor, and from my tries with
> them, this seems to be the case.
> 3. Are either the extreme post mean values or the auto correlation in
> certain parameters avoidable in this case?
>
> *Examples of priors*
> prior1<-list(R=list(V=diag(1)*1e-8, nu=0.2),G=list(G1=list(V=diag(1)*1e-6,
> nu=0.2))
> prior2<-list(R=list(V=diag(1), nu=0.2),G=list(G1=list(V=diag(1), nu=2)))
> prior3<-list(R=list(V=diag(2), nu=0, fix=2), G=list(G1=G1))
> G1=list(V=diag(2), nu=2, alpha.mu=c(0,0), alpha.V=diag(2)*1000)
> prior4:
> A note: I have these with degree belief ranging from 0.0002 to 20, with the
> best mixing (apart from the auto correlation in certain parameters) with nu
> set at 20 for the R structure. If i set the G structures degree of belief
> to less than 20, it mixes the random effect extremely poorly. I have also
> used ranging values from 1e-1 to 1e-20 and fixed the variances at 1 and 2
> using various different priors.
> *Examples of model scripts:*
> OM1.1<-MCMCglmm(Jackal_trapped ~ Human_presence +  MP + Landuse, random
> =~Site, data = data, prior = prior1,
> family = "ordinal", nitt = 5000000, thin = 500, burnin = 500000, verbose =
> FALSE, pl = TRUE, DIC=TRUE)
> model1.1<-MCMCglmm(Jackal_trapped~trait,random=~us(trait):Human_presence +
>
> us(trait):Landuse,family="zapoisson",rcov=~idh(trait):units,data=data,prior=prior,nitt=500000,thin=500,burnin=200000,verbose=FALSE)
>
> model1.2<-MCMCglmm(Jackal_trapped~trait,random=~idh(trait):Human_presence +
>
> idh(trait):Landuse,family="zapoisson",rcov=~idh(trait):units,data=data,prior=prior,nitt=500000,thin=500,burnin=200000,verbose=FALSE)
>
> *Example of large post mean summary*
>  family: I have used zapoisson, ordinal and zipoisson distributions
> prior1<-list(R=list(V=diag(1)*1e-4, nu=0.2),G=list(G1=list(V=diag(1)*1e-2,
> nu=2)))
> OM1.1<-MCMCglmm(Jackal_trapped ~ Human_presence +  MP + Landuse, random
> =~Site, data = data, prior = prior1, family = "family", nitt = 5000000,
> thin = 500, burnin = 500000, verbose = FALSE, pl = TRUE, DIC=TRUE)
>  Iterations = 500001:4999501
>  Thinning interval  = 500
>  Sample size  = 9000
>
>  DIC: 0.003093406
>
>  G-structure:  ~Site
>
>      post.mean l-95% CI u-95% CI eff.samp
> Site    0.1943 0.001154   0.1913     9000
>
>  R-structure:  ~units
>
>       post.mean l-95% CI u-95% CI eff.samp
> units 1.755e+10 1.72e+09 3.97e+10     3632
>
>  Location effects: Jackal_trapped ~ Human_presence + MP + Landuse
>                               post.mean    l-95% CI  u-95% CI eff.samp
>  pMCMC
> (Intercept)                  -107789  -229797     9915     4666
> 0.0498 *
> Human_presence1     49459   -12241      117809     6957         0.1002
> Human_presence2    -49280  -153682      46279     8478          0.3184
> Human_presence3     18642   -66603      104880     8529          0.6684
> Human_presence4   -214029  -340008     -89259     6098          <1e-04 ***
> MPFQ                         -44627  -119345      24385       7328
> 0.1856
> MPNM                          12362   -52404      72882       9000
> 0.6691
> MPTQ                           -43907  -122103    22055     8357
>  0.1942
> Landusereserve            -51105  -138603    35033     9000
>  0.2302
>
> *Example of model outputs with auto correlation*
> family: I have used zapoisson, ordinal and zipoisson distributions
> prior2<-list(R=list(V=diag(1)*1e-6, nu=20),G=list(G1=list(V=diag(1)*1e-6,
> nu=20)))
> OM2.1<-MCMCglmm(Jackal_trapped ~ Human_presence +  MP + Landuse, random
> =~Site, data = data, prior = prior2,
> family = "family", nitt = 5000000, thin = 500, burnin = 500000, verbose =
> FALSE, pl = TRUE, DIC=TRUE)
>  Iterations = 500001:4999501
>  Thinning interval  = 500
>  Sample size  = 9000
>
>  DIC: 182.6921
>
>  G-structure:  ~Site
>      post.mean  l-95% CI  u-95% CI eff.samp
> Site 1.114e-06 5.171e-07 1.911e-06     8515
>
>  R-structure:  ~units
>
>       post.mean  l-95% CI u-95% CI eff.samp
> units 1.106e-06 5.129e-07 1.87e-06     9000
>
>  Location effects: Jackal_trapped ~ Human_presence + MP + Landuse
>
>                              post.mean l-95% CI u-95% CI eff.samp   pMCMC
>
> (Intercept)               -0.70349 -1.20259 -0.28723    2.027     < 1e-04
> ***
> Human_presence1   0.53752  0.41033  0.67240    6.030    < 1e-04 ***
> Human_presence2  -0.44909 -0.65858 -0.03042    2.600    0.00289 **
> Human_presence3  -0.11236 -0.48808  0.30079    1.683     0.53556
> Human_presence4  -1.38806 -1.74144 -0.93892    2.829    < 1e-04 ***
> MPFQ                        -0.38918 -0.57513 -0.23967    2.300    < 1e-04
> ***
> MPNM                         0.14273 -0.05171  0.28830    1.625    0.14356
>
> MPTQ                         -0.25338 -0.52899 -0.03162    1.477    0.01222
> *
> Landusereserve          -0.64469 -1.06522 -0.18880    2.239    < 1e-04 ***
>
> *Example: Using my variables as random effects*
> *family: I have used zapoisson, ordinal and zipoisson distributions*
> model1.1<-MCMCglmm(Jackal_trapped~trait,random=~us(trait):Human_presence +
>
> us(trait):Landuse,family="family",rcov=~idh(trait):units,data=data,prior=prior,nitt=500000,thin=500,burnin=200000,verbose=FALSE)
> Iterations = 200001:499501
> Thinning interval  = 500
> Sample size  = 600
>
>  DIC: 175.5933
>
>  G-structure:  ~us(trait):Human_presence
>
>
>         post.mean   l-95% CI  u-95% CI eff.samp
> traitJackal_trapped:traitJackal_trapped.Human_presence 1.179e-06  5.584e-07
> 2.051e-06    472.7
> traitza_Jackal_trapped:traitJackal_trapped.Human_presence    -1.243e-08
> -5.964e-07 5.494e-07    600.0
> traitJackal_trapped:traitza_Jackal_trapped.Human_presence    -1.243e-08
> -5.964e-07 5.494e-07    600.0
> traitza_Jackal_trapped:traitza_Jackal_trapped.Human_presence  1.172e-06
>  5.252e-07 1.883e-06    507.3
>
>  ~us(trait):Landuse
>
>       post.mean   l-95% CI  u-95% CI eff.samp
> traitJackal_trapped:traitJackal_trapped.Landuse       1.143e-06  4.876e-07
> 2.029e-06      600
> traitza_Jackal_trapped:traitJackal_trapped.Landuse    7.237e-09 -5.758e-07
> 4.980e-07      600
> traitJackal_trapped:traitza_Jackal_trapped.Landuse    7.237e-09 -5.758e-07
> 4.980e-07      600
> traitza_Jackal_trapped:traitza_Jackal_trapped.Landuse 1.157e-06  4.896e-07
> 2.022e-06      600
>
>  R-structure:  ~idh(trait):units
>
>                              post.mean  l-95% CI  u-95% CI eff.samp
> traitJackal_trapped.units    9.855e-07 4.689e-07 1.685e-06    704.6
> traitza_Jackal_trapped.units 1.017e-06 4.490e-07 1.625e-06    600.0
>
>  Location effects: Jackal_trapped ~ trait
>
>                        post.mean l-95% CI u-95% CI eff.samp  pMCMC
> (Intercept)               0.7789   0.7730   0.7875    8.419 <0.002 **
> traitza_Jackal_trapped   -2.8911  -2.9052  -2.8741    6.982 <0.002 **
>
> I hope I have shared enough info regarding my problem and that it makes
> sense. I hope my post meets the requirements of the list and that it does
> not seem like I am making my problem somebody elses, I am honestly just
> lost at the moment.
> I welcome any constructive criticism and any other help you can provide.
>
> Thank you for your help.
> I look forward to your responses.
>
> With kind regards,
>
> Alexander Edward Botha
> alexbotha555 at gmail.com 082 414 9030
> PhD candidate
> Mammal ecology
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From @reedt@8 @end|ng |rom gm@||@com  Fri May 15 18:32:49 2020
From: @reedt@8 @end|ng |rom gm@||@com (sree datta)
Date: Fri, 15 May 2020 12:32:49 -0400
Subject: [R-sig-ME] 
 mcmcglmm Priors: Auto correlation and extreme post mean values
In-Reply-To: <CAHftDbj8MX_ivViFaq-y88a7-u1ME31_mafs=b0UoX4RqWFy=w@mail.gmail.com>
References: <CAO3yAjM_6B9tn+fTMaS93WGDV78h35HNXH5qEBhB0ZNLNc+5Sw@mail.gmail.com>
 <CAHftDbgvGUtnEWJDG4ohnb6XOZgvHLeocB=S_OPkZoirK8aw6Q@mail.gmail.com>
 <CAO3yAjPFjJqSwp-CHa0942XvCzmYLbBTbi3_XmpY4W01uJpyqA@mail.gmail.com>
 <CAHftDbj8MX_ivViFaq-y88a7-u1ME31_mafs=b0UoX4RqWFy=w@mail.gmail.com>
Message-ID: <CAHftDbhQoa5JTpo+-FT3Y4wqtBHSgWKD=ykvSDJg74G_+r4oxg@mail.gmail.com>

I forgot to copy the group on my original reply: here is the text of my
notes:

Thanks for sharing the relevant data and model details and output. The
first and biggest concern I have is the use of Site variable for a Random
Intercept. There are only 175 observations, but you have 86 different
sites. On an average, without seeing the actual data, that means you have
~2 observations per site. There is not enough data to use Site for a Random
intercept as it exists.

You should use site only after you can reduce 86 sites into some meaningful
groupings based on criteria you use in Ecology (reduce to 3 or 4 groups).
Additionally, the remaining factors create 2 * 5 * 4 = 40 cells and you
have only 175 observations ~ 4.4 observations per cell - this number is
also too low for you to use all of these factors in your analysis.

All of the problems you are facing mostly likely are arising from the wrong
application of the techniques to your data. I would take the following
steps:
1. Run a bivariate means comparison using Anova after you declare your
Landuse, Human_presence etc as factors using ( aov( trappings ~ landuse)
2. In doing so you are going to change your assumptions of trappings from
count to continuous, but I think for exploration, it is okay.
3. Make sure you examine the means of the trappings for each level and
combine levels within a factor, when the sub-groups are not different from
each other.
4. This will reduce the number of groups you are working with since your
records are limited.

Only after undertaking the steps above should you consider, running any
models in lme4 or nlme. Ideally, if you could reduce those factors that
have 4 and 5 groups to having 2 each (if the differences are not
significant - see # 3) then you could do meaningful analyses. Similarly,
with Site, the 86 levels should be reduced to either 3 or 4 for you to use
them even for a descriptive analysis. Without adding in more data or
reducing your number of levels, you should not undertake any further
modeling.

I also wanted to add that for Site variable, you should first find a
theoretical way to reduce the number of sites to 3 or 4 broad groups. You
cannot use Means comparison to do that.

On Fri, May 15, 2020 at 12:08 PM sree datta <sreedta8 at gmail.com> wrote:

>
> Thanks for sharing the relevant data and model details and output. The
> first and biggest concern I have is the use of Site variable for a Random
> Intercept. There are only 175 observations, but you have 86 different
> sites. On an average, without seeing the actual data, that means you have
> ~2 observations per site. There is not enough data to use Site for a Random
> intercept as it exists.
>
> You should use site only after you can reduce 86 sites into some
> meaningful groupings based on criteria you use in Ecology (reduce to 3 or 4
> groups). Additionally, the remaining factors create 2 * 5 * 4 = 40 cells
> and you have only 175 observations ~ 4.4 observations per cell - this
> number is also too low for you to use all of these factors in your
> analysis.
>
> All of the problems you are facing mostly likely are arising from the
> wrong application of the techniques to your data. I would take the
> following steps:
> 1. Run a bivariate means comparison using Anova after you declare your
> Landuse, Human_presence etc as factors using ( aov( trappings ~ landuse)
> 2. In doing so you are going to change your assumptions of trappings from
> count to continuous, but I think for exploration, it is okay.
> 3. Make sure you examine the means of the trappings for each level and
> combine levels within a factor, when the sub-groups are not different from
> each other.
> 4. This will reduce the number of groups you are working with since your
> records are limited.
>
> Only after undertaking the steps above should you consider, running any
> models in lme4 or nlme. Ideally, if you could reduce those factors that
> have 4 and 5 groups to having 2 each (if the differences are not
> significant - see # 3) then you could do meaningful analyses. Similarly,
> with Site, the 86 levels should be reduced to either 3 or 4 for you to use
> them even for a descriptive analysis. Without adding in more data or
> reducing your number of levels, you should not undertake any further
> modeling.
>
> Sree
>
> On Fri, May 15, 2020 at 11:33 AM Alexander Botha <alexbotha555 at gmail.com>
> wrote:
>
>> Hi Sree.
>> I have run multiple models with lmer and glmer using a combination of
>> predictor variables as either random or fixed effects. I was worried about
>> the collinearity in fixed effects (singular fit) when using lme4.
>> I hope these answer your questions. Please see below summaries of some of
>> the models I ran.
>> I have also attached the structure of my data in case it might help.
>> Thank you for your help.
>> Please let me know if you require anything else or if you have any
>> suggestions for data exploration.
>>
>> *Data structure*
>> data.frame': 175 obs. of  8 variables:
>>  $ Site          : Factor w/ 86 levels "AA1","carcass",..: 66 66 66 68 68
>> 68 21 21 21 32 ...
>>  $ Landuse       : Factor w/ 2 levels "farmland","reserve": 1 1 1 1 1 1 1
>> 1 1 1 ...
>>  $ MP            : Factor w/ 4 levels "FM","FQ","NM",..: 3 1 4 3 2 4 3 2
>> 1 3 ...
>>  $ Human_presence: Factor w/ 5 levels "0","1","2","3",..: 4 4 4 4 4 4 5 5
>> 5 5 ...
>>  $ Jackal_trapped: int  1 0 0 0 1 0 0 0 0 0 ...
>>
>> *model1<-lmer(Jackal_trapped~ Human_presence + (1|Site), data=data)*
>> REML criterion at convergence: 77.7
>>
>> Scaled residuals:
>>     Min      1Q  Median      3Q     Max
>> -1.3261 -0.7568  0.0000  0.0000  2.6909
>>
>> Random effects:
>>  Groups   Name        Variance Std.Dev.
>>  Site     (Intercept) 0.00000  0.00
>>  Residual             0.08413  0.29
>> Number of obs: 175, groups:  Site, 86
>>
>> Fixed effects:
>>                 Estimate Std. Error t value
>> (Intercept)      0.21951    0.04530   4.846
>> Human_presence1  0.16510    0.09232   1.788
>> Human_presence2 -0.21951    0.10230  -2.146
>> Human_presence3  0.08049    0.07911   1.017
>> Human_presence4 -0.21951    0.05456  -4.024
>>
>> Correlation of Fixed Effects:
>>             (Intr) Hmn_p1 Hmn_p2 Hmn_p3
>> Humn_prsnc1 -0.491
>> Humn_prsnc2 -0.443  0.217
>> Humn_prsnc3 -0.573  0.281  0.254
>> Humn_prsnc4 -0.830  0.407  0.368  0.475
>> convergence code: 0
>>
>> *model1.1<-lmer(Jackal_trapped~ Human_presence + MP + Landuse + (1|Site),
>> data=data)*
>> REML criterion at convergence: 88.5
>>
>> Scaled residuals:
>>      Min       1Q   Median       3Q      Max
>> -1.36759 -0.73677 -0.00991  0.03404  2.68699
>>
>> Random effects:
>>  Groups   Name        Variance Std.Dev.
>>  Site     (Intercept) 0.00000  0.0000
>>  Residual             0.08531  0.2921
>> Number of obs: 175, groups:  Site, 86
>>
>> Fixed effects:
>>                  Estimate Std. Error t value
>> (Intercept)      0.344378   0.181977   1.892
>> Human_presence1  0.161919   0.094159   1.720
>> Human_presence2 -0.228924   0.103472  -2.212
>> Human_presence3  0.014851   0.154207   0.096
>> Human_presence4 -0.341485   0.179179  -1.906
>> MPFQ            -0.012836   0.060249  -0.213
>> MPNM             0.009494   0.060567   0.157
>> MPTQ            -0.122941   0.114023  -1.078
>> Landusereserve  -0.116350   0.169306  -0.687
>>
>> Correlation of Fixed Effects:
>>             (Intr) Hmn_p1 Hmn_p2 Hmn_p3 Hmn_p4 MPFQ   MPNM   MPTQ
>> Humn_prsnc1 -0.159
>> Humn_prsnc2 -0.144  0.221
>> Humn_prsnc3 -0.868  0.159  0.138
>> Humn_prsnc4 -0.963  0.131  0.134  0.868
>> MPFQ        -0.248  0.158  0.052  0.094  0.052
>> MPNM        -0.178  0.082  0.047  0.003  0.002  0.594
>> MPTQ        -0.327  0.040  0.090  0.063  0.260  0.293  0.322
>> Landusersrv -0.929  0.004  0.017  0.838  0.947  0.021 -0.080  0.207
>> convergence code: 0
>> boundary (singular) fit: see ?isSingular
>>
>> *model2<-lmer(Jackal_trapped~ Human_presence + Landuse + (1|Site),
>> data=data)*
>> Linear mixed model fit by REML ['lmerMod']
>> Formula: Jackal_trapped ~ Human_presence + Landuse + (1 | Site)
>>    Data: data
>>
>> REML criterion at convergence: 79.4
>>
>> Scaled residuals:
>>     Min      1Q  Median      3Q     Max
>> -1.3227 -0.7549  0.0000  0.0000  2.6842
>>
>> Random effects:
>>  Groups   Name        Variance Std.Dev.
>>  Site     (Intercept) 0.00000  0.0000
>>  Residual             0.08455  0.2908
>> Number of obs: 175, groups:  Site, 86
>>
>> Fixed effects:
>>                 Estimate Std. Error t value
>> (Intercept)      0.28201    0.16877   1.671
>> Human_presence1  0.16510    0.09255   1.784
>> Human_presence2 -0.21951    0.10255  -2.140
>> Human_presence3  0.03049    0.15231   0.200
>> Human_presence4 -0.28201    0.17150  -1.644
>> Landusereserve  -0.06250    0.16255  -0.385
>>
>> Correlation of Fixed Effects:
>>             (Intr) Hmn_p1 Hmn_p2 Hmn_p3 Hmn_p4
>> Humn_prsnc1 -0.132
>> Humn_prsnc2 -0.119  0.217
>> Humn_prsnc3 -0.902  0.146  0.132
>> Humn_prsnc4 -0.984  0.130  0.117  0.888
>> Landusersrv -0.963  0.000  0.000  0.854  0.948
>> convergence code: 0
>>
>> *model3<-lmer(Jackal_trapped~ Human_presence + MP + (1|Landuse) (1|Site),
>> data=data)*
>> Formula: Jackal_trapped ~ Human_presence + MP + (1 | Landuse) + (1 | Site)
>>    Data: data
>>
>> REML criterion at convergence: 87.2
>>
>> Scaled residuals:
>>      Min       1Q   Median       3Q      Max
>> -1.35994 -0.74172 -0.01178  0.02922  2.68746
>>
>> Random effects:
>>  Groups   Name        Variance Std.Dev.
>>  Site     (Intercept) 0.00000  0.0000
>>  Landuse  (Intercept) 0.00000  0.0000
>>  Residual             0.08504  0.2916
>> Number of obs: 175, groups:  Site, 86; Landuse, 2
>>
>> Fixed effects:
>>                  Estimate Std. Error t value
>> (Intercept)      0.228251   0.067428   3.385
>> Human_presence1  0.162180   0.094010   1.725
>> Human_presence2 -0.227740   0.103294  -2.205
>> Human_presence3  0.103705   0.083907   1.236
>> Human_presence4 -0.224817   0.057214  -3.929
>> MPFQ            -0.011955   0.060140  -0.199
>> MPNM             0.006149   0.060276   0.102
>> MPTQ            -0.106692   0.111368  -0.958
>>
>> Correlation of Fixed Effects:
>>             (Intr) Hmn_p1 Hmn_p2 Hmn_p3 Hmn_p4 MPFQ   MPNM
>> Humn_prsnc1 -0.418
>> Humn_prsnc2 -0.347  0.221
>> Humn_prsnc3 -0.443  0.285  0.227
>> Humn_prsnc4 -0.705  0.398  0.371  0.422
>> MPFQ        -0.614  0.158  0.052  0.139  0.100
>> MPNM        -0.682  0.083  0.049  0.130  0.245  0.598
>> MPTQ        -0.370  0.040  0.088 -0.208  0.202  0.295  0.347
>> convergence code: 0
>> boundary (singular) fit: see ?isSingular
>> *model4<-glmer(Jackal_trapped~Human_presence + MP + Landuse +(1|Site),
>> data = data, family = binomial)*
>>
>> Generalized linear mixed model fit by maximum likelihood (Laplace
>> Approximation) ['glmerMod']
>>  Family: binomial  ( logit )
>> Formula: Jackal_trapped ~ Human_presence + MP + Landuse + (1 | Site)
>>    Data: data
>>
>>      AIC      BIC   logLik deviance df.resid
>>    104.1    135.7    -42.0     84.1      165
>>
>> Scaled residuals:
>>     Min      1Q  Median      3Q     Max
>> -0.8317 -0.4895  0.0000  0.0000  2.0429
>>
>> Random effects:
>>  Groups Name        Variance Std.Dev.
>>  Site   (Intercept) 0        0
>> Number of obs: 175, groups:  Site, 86
>>
>> Fixed effects:
>>                   Estimate Std. Error z value Pr(>|z|)
>> (Intercept)     -5.455e-01  1.511e+00  -0.361    0.718
>> Human_presence1  7.428e-01  7.188e-01   1.033    0.301
>> Human_presence2 -3.302e+02  2.122e+07   0.000    1.000
>> Human_presence3  3.143e-02  1.238e+00   0.025    0.980
>> Human_presence4 -4.416e+01  7.035e+06   0.000    1.000
>> MPFQ            -2.413e-01  8.942e-01  -0.270    0.787
>> MPNM             7.619e-02  7.622e-01   0.100    0.920
>> MPTQ            -7.063e-01  1.039e+00  -0.680    0.497
>> Landusereserve  -6.420e-01  1.331e+00  -0.482    0.630
>>
>> Correlation of Fixed Effects:
>>             (Intr) Hmn_p1 Hmn_p2 Hmn_p3 Hmn_p4 MPFQ   MPNM   MPTQ
>> Humn_prsnc1 -0.251
>> Humn_prsnc2  0.000  0.000
>> Humn_prsnc3 -0.861  0.210  0.000
>> Humn_prsnc4  0.000  0.000  0.000  0.000
>> MPFQ            -0.413  0.296  0.000  0.166  0.000
>> MPNM            -0.317  0.192  0.000  0.044  0.000  0.646
>> MPTQ             -0.383  0.116  0.000  0.083  0.000  0.430  0.459
>> Landusersrv -0.860  0.007  0.000  0.838  0.000  0.035 -0.115  0.158
>> convergence code: 0
>>
>> *model5<-glmer(Jackal_trapped~Human_presence + MP + Landuse +(1|Site),
>> data = data, family = poisson)*
>> Generalized linear mixed model fit by maximum likelihood (Laplace
>> Approximation) ['glmerMod']
>>  Family: poisson  ( log )
>> Formula: Jackal_trapped ~ Human_presence + MP + Landuse + (1 | Site)
>>    Data: data
>>
>>      AIC      BIC   logLik deviance df.resid
>>    110.7    142.3    -45.3     90.7      165
>>
>> Scaled residuals:
>>     Min      1Q  Median      3Q     Max
>> -0.6418 -0.4413  0.0000  0.0000  1.8248
>>
>> Random effects:
>>  Groups Name        Variance Std.Dev.
>>  Site   (Intercept) 0        0
>> Number of obs: 175, groups:  Site, 86
>>
>> Fixed effects:
>>                   Estimate Std. Error z value Pr(>|z|)
>> (Intercept)     -1.008e+00  1.278e+00  -0.789    0.430
>> Human_presence1  5.166e-01  5.851e-01   0.883    0.377
>> Human_presence2 -3.688e+01  2.122e+07   0.000    1.000
>> Human_presence3  3.101e-02  1.072e+00   0.029    0.977
>> Human_presence4 -3.153e+01  1.255e+06   0.000    1.000
>> MPFQ            -1.762e-01  7.501e-01  -0.235    0.814
>> MPNM             5.648e-02  6.197e-01   0.091    0.927
>> MPTQ            -5.145e-01  8.896e-01  -0.578    0.563
>> Landusereserve  -4.524e-01  1.134e+00  -0.399    0.690
>>
>> Correlation of Fixed Effects:
>>             (Intr) Hmn_p1 Hmn_p2 Hmn_p3 Hmn_p4 MPFQ   MPNM   MPTQ
>> Humn_prsnc1 -0.267
>> Humn_prsnc2  0.000  0.000
>> Humn_prsnc3 -0.875  0.226  0.000
>> Humn_prsnc4  0.000  0.000  0.000  0.000
>> MPFQ        -0.392  0.302  0.000  0.160  0.000
>> MPNM        -0.299  0.185  0.000  0.039  0.000  0.618
>> MPTQ        -0.345  0.119  0.000  0.076  0.000  0.395  0.431
>> Landusersrv -0.865  0.010  0.000  0.848  0.000  0.033 -0.116  0.139
>> convergence code: 0
>>
>> With kind regards,
>>
>> Alexander Edward Botha
>> alexbotha555 at gmail.com 082 414 9030
>> PhD candidate
>> Mammal ecology
>>
>>
>> On Fri, May 15, 2020 at 4:25 PM sree datta <sreedta8 at gmail.com> wrote:
>>
>>> Hi Alexander,
>>>
>>> Prior to running the model with "MCMCglmm", have you attempted to run a
>>> log-linear model / a mixed-model with "nlme" or "lme4" / a decision tree? I
>>> ask these questions to better understand what the simpler univariate and
>>> multivariate approaches would reveal in terms of association between your
>>> dependent variable and your predictor variables.
>>>
>>> These are the exploratory models I would run to understand what would
>>> represent reasonable priors to use for Bayesian based models. If you could
>>> share more on some of your explorations with the data, that would be
>>> helpful.
>>>
>>> Sree
>>>
>>> On Fri, May 15, 2020 at 9:29 AM Alexander Botha <alexbotha555 at gmail.com>
>>> wrote:
>>>
>>>> Good day List,
>>>> My name is Alex, I am currently using the package mcmcglmm to determine
>>>> the
>>>> impact of lunar cycles, human presence and land use type (agricultural
>>>> vs
>>>> protected) on the trapping success of meso-predators for my PhD. I am
>>>> new
>>>> to MCMCglmm and I was wondering if you could assist with my problem.
>>>>
>>>> Structure of data: I am testing if and how the change in lunar cycle
>>>> (factor) and human presence ( factor) impact trapping success. Trapping
>>>> success is count data but because we only trapped 1 individual at most,
>>>> the
>>>> data can also be considered binary. Human presence (HP) is split into 5
>>>> categories.
>>>>
>>>> Model: I am running Human presence, Moon phase and Land use type as
>>>> fixed
>>>> effects and the site name as a random effect.
>>>>
>>>> Problem: I have quantified human presence in various locations, with
>>>> areas
>>>> exposed to intense human pressures, there is a complete absence of any
>>>> successful trappings (HP2 and HP4 have no trapping success resulting in
>>>> only zeros). Post mcmcglmm, these parameters display auto correlation in
>>>> their graphs (if i set them as fixed or random effects) as well as when
>>>> using the geweke and gelman tests, but almost perfect mixing for the
>>>> others, especially when I run it with 5-20 million iterations. I also
>>>> see
>>>> poorly mixed VCV graphs if the degree of belief is less than 20. I have
>>>> used a variety of priors, see below, with no success. I have also
>>>> increased
>>>> the amount of itterations to 20 million, decreased the thinning factor,
>>>> increased the burnin and used poisson, zapoisson, zipoisson, ordinal and
>>>> threshold distributions, with threshold and ordinal having the best DIC
>>>> values. Together with this, I am also seeing extreme post mean values
>>>> for
>>>> the models that are displaying the lowest DIC values, which is not
>>>> something that I see in any of the literature (see below).
>>>>
>>>> I have read Jarrod Hadfields awesome course and tutorial notes, as well
>>>> as
>>>> other literature in my field, online tutorials, information documents
>>>> and
>>>> the vignettes and help functions in R as well as the correspondence on
>>>> this
>>>> email list between Jarrod Hadfield and other users but I cannot seem to
>>>> figure out why the above is happening. My data set is quite small (176
>>>> entries in total, with about 40 entries per HP category) and according
>>>> to
>>>> what I have read, priors can have a large impact on small to moderately
>>>> sized data sets.
>>>>
>>>> Questions:
>>>> 1. Does the problem lie with my priors? And if so, do you have any tips
>>>> on
>>>> how to solve it?
>>>> 2. Should I be using ordinal or threshold family? I have read that the
>>>> mixing of zi and za poisson models can be poor, and from my tries with
>>>> them, this seems to be the case.
>>>> 3. Are either the extreme post mean values or the auto correlation in
>>>> certain parameters avoidable in this case?
>>>>
>>>> *Examples of priors*
>>>> prior1<-list(R=list(V=diag(1)*1e-8,
>>>> nu=0.2),G=list(G1=list(V=diag(1)*1e-6,
>>>> nu=0.2))
>>>> prior2<-list(R=list(V=diag(1), nu=0.2),G=list(G1=list(V=diag(1), nu=2)))
>>>> prior3<-list(R=list(V=diag(2), nu=0, fix=2), G=list(G1=G1))
>>>> G1=list(V=diag(2), nu=2, alpha.mu=c(0,0), alpha.V=diag(2)*1000)
>>>> prior4:
>>>> A note: I have these with degree belief ranging from 0.0002 to 20, with
>>>> the
>>>> best mixing (apart from the auto correlation in certain parameters)
>>>> with nu
>>>> set at 20 for the R structure. If i set the G structures degree of
>>>> belief
>>>> to less than 20, it mixes the random effect extremely poorly. I have
>>>> also
>>>> used ranging values from 1e-1 to 1e-20 and fixed the variances at 1 and
>>>> 2
>>>> using various different priors.
>>>> *Examples of model scripts:*
>>>> OM1.1<-MCMCglmm(Jackal_trapped ~ Human_presence +  MP + Landuse, random
>>>> =~Site, data = data, prior = prior1,
>>>> family = "ordinal", nitt = 5000000, thin = 500, burnin = 500000,
>>>> verbose =
>>>> FALSE, pl = TRUE, DIC=TRUE)
>>>> model1.1<-MCMCglmm(Jackal_trapped~trait,random=~us(trait):Human_presence
>>>> +
>>>>
>>>> us(trait):Landuse,family="zapoisson",rcov=~idh(trait):units,data=data,prior=prior,nitt=500000,thin=500,burnin=200000,verbose=FALSE)
>>>>
>>>> model1.2<-MCMCglmm(Jackal_trapped~trait,random=~idh(trait):Human_presence
>>>> +
>>>>
>>>> idh(trait):Landuse,family="zapoisson",rcov=~idh(trait):units,data=data,prior=prior,nitt=500000,thin=500,burnin=200000,verbose=FALSE)
>>>>
>>>> *Example of large post mean summary*
>>>>  family: I have used zapoisson, ordinal and zipoisson distributions
>>>> prior1<-list(R=list(V=diag(1)*1e-4,
>>>> nu=0.2),G=list(G1=list(V=diag(1)*1e-2,
>>>> nu=2)))
>>>> OM1.1<-MCMCglmm(Jackal_trapped ~ Human_presence +  MP + Landuse, random
>>>> =~Site, data = data, prior = prior1, family = "family", nitt = 5000000,
>>>> thin = 500, burnin = 500000, verbose = FALSE, pl = TRUE, DIC=TRUE)
>>>>  Iterations = 500001:4999501
>>>>  Thinning interval  = 500
>>>>  Sample size  = 9000
>>>>
>>>>  DIC: 0.003093406
>>>>
>>>>  G-structure:  ~Site
>>>>
>>>>      post.mean l-95% CI u-95% CI eff.samp
>>>> Site    0.1943 0.001154   0.1913     9000
>>>>
>>>>  R-structure:  ~units
>>>>
>>>>       post.mean l-95% CI u-95% CI eff.samp
>>>> units 1.755e+10 1.72e+09 3.97e+10     3632
>>>>
>>>>  Location effects: Jackal_trapped ~ Human_presence + MP + Landuse
>>>>                               post.mean    l-95% CI  u-95% CI eff.samp
>>>>  pMCMC
>>>> (Intercept)                  -107789  -229797     9915     4666
>>>> 0.0498 *
>>>> Human_presence1     49459   -12241      117809     6957         0.1002
>>>> Human_presence2    -49280  -153682      46279     8478          0.3184
>>>> Human_presence3     18642   -66603      104880     8529          0.6684
>>>> Human_presence4   -214029  -340008     -89259     6098          <1e-04
>>>> ***
>>>> MPFQ                         -44627  -119345      24385       7328
>>>> 0.1856
>>>> MPNM                          12362   -52404      72882       9000
>>>> 0.6691
>>>> MPTQ                           -43907  -122103    22055     8357
>>>>  0.1942
>>>> Landusereserve            -51105  -138603    35033     9000
>>>>  0.2302
>>>>
>>>> *Example of model outputs with auto correlation*
>>>> family: I have used zapoisson, ordinal and zipoisson distributions
>>>> prior2<-list(R=list(V=diag(1)*1e-6,
>>>> nu=20),G=list(G1=list(V=diag(1)*1e-6,
>>>> nu=20)))
>>>> OM2.1<-MCMCglmm(Jackal_trapped ~ Human_presence +  MP + Landuse, random
>>>> =~Site, data = data, prior = prior2,
>>>> family = "family", nitt = 5000000, thin = 500, burnin = 500000, verbose
>>>> =
>>>> FALSE, pl = TRUE, DIC=TRUE)
>>>>  Iterations = 500001:4999501
>>>>  Thinning interval  = 500
>>>>  Sample size  = 9000
>>>>
>>>>  DIC: 182.6921
>>>>
>>>>  G-structure:  ~Site
>>>>      post.mean  l-95% CI  u-95% CI eff.samp
>>>> Site 1.114e-06 5.171e-07 1.911e-06     8515
>>>>
>>>>  R-structure:  ~units
>>>>
>>>>       post.mean  l-95% CI u-95% CI eff.samp
>>>> units 1.106e-06 5.129e-07 1.87e-06     9000
>>>>
>>>>  Location effects: Jackal_trapped ~ Human_presence + MP + Landuse
>>>>
>>>>                              post.mean l-95% CI u-95% CI eff.samp
>>>>  pMCMC
>>>>
>>>> (Intercept)               -0.70349 -1.20259 -0.28723    2.027     <
>>>> 1e-04
>>>> ***
>>>> Human_presence1   0.53752  0.41033  0.67240    6.030    < 1e-04 ***
>>>> Human_presence2  -0.44909 -0.65858 -0.03042    2.600    0.00289 **
>>>> Human_presence3  -0.11236 -0.48808  0.30079    1.683     0.53556
>>>> Human_presence4  -1.38806 -1.74144 -0.93892    2.829    < 1e-04 ***
>>>> MPFQ                        -0.38918 -0.57513 -0.23967    2.300    <
>>>> 1e-04
>>>> ***
>>>> MPNM                         0.14273 -0.05171  0.28830    1.625
>>>> 0.14356
>>>>
>>>> MPTQ                         -0.25338 -0.52899 -0.03162    1.477
>>>> 0.01222
>>>> *
>>>> Landusereserve          -0.64469 -1.06522 -0.18880    2.239    < 1e-04
>>>> ***
>>>>
>>>> *Example: Using my variables as random effects*
>>>> *family: I have used zapoisson, ordinal and zipoisson distributions*
>>>> model1.1<-MCMCglmm(Jackal_trapped~trait,random=~us(trait):Human_presence
>>>> +
>>>>
>>>> us(trait):Landuse,family="family",rcov=~idh(trait):units,data=data,prior=prior,nitt=500000,thin=500,burnin=200000,verbose=FALSE)
>>>> Iterations = 200001:499501
>>>> Thinning interval  = 500
>>>> Sample size  = 600
>>>>
>>>>  DIC: 175.5933
>>>>
>>>>  G-structure:  ~us(trait):Human_presence
>>>>
>>>>
>>>>         post.mean   l-95% CI  u-95% CI eff.samp
>>>> traitJackal_trapped:traitJackal_trapped.Human_presence 1.179e-06
>>>> 5.584e-07
>>>> 2.051e-06    472.7
>>>> traitza_Jackal_trapped:traitJackal_trapped.Human_presence    -1.243e-08
>>>> -5.964e-07 5.494e-07    600.0
>>>> traitJackal_trapped:traitza_Jackal_trapped.Human_presence    -1.243e-08
>>>> -5.964e-07 5.494e-07    600.0
>>>> traitza_Jackal_trapped:traitza_Jackal_trapped.Human_presence  1.172e-06
>>>>  5.252e-07 1.883e-06    507.3
>>>>
>>>>  ~us(trait):Landuse
>>>>
>>>>       post.mean   l-95% CI  u-95% CI eff.samp
>>>> traitJackal_trapped:traitJackal_trapped.Landuse       1.143e-06
>>>> 4.876e-07
>>>> 2.029e-06      600
>>>> traitza_Jackal_trapped:traitJackal_trapped.Landuse    7.237e-09
>>>> -5.758e-07
>>>> 4.980e-07      600
>>>> traitJackal_trapped:traitza_Jackal_trapped.Landuse    7.237e-09
>>>> -5.758e-07
>>>> 4.980e-07      600
>>>> traitza_Jackal_trapped:traitza_Jackal_trapped.Landuse 1.157e-06
>>>> 4.896e-07
>>>> 2.022e-06      600
>>>>
>>>>  R-structure:  ~idh(trait):units
>>>>
>>>>                              post.mean  l-95% CI  u-95% CI eff.samp
>>>> traitJackal_trapped.units    9.855e-07 4.689e-07 1.685e-06    704.6
>>>> traitza_Jackal_trapped.units 1.017e-06 4.490e-07 1.625e-06    600.0
>>>>
>>>>  Location effects: Jackal_trapped ~ trait
>>>>
>>>>                        post.mean l-95% CI u-95% CI eff.samp  pMCMC
>>>> (Intercept)               0.7789   0.7730   0.7875    8.419 <0.002 **
>>>> traitza_Jackal_trapped   -2.8911  -2.9052  -2.8741    6.982 <0.002 **
>>>>
>>>> I hope I have shared enough info regarding my problem and that it makes
>>>> sense. I hope my post meets the requirements of the list and that it
>>>> does
>>>> not seem like I am making my problem somebody elses, I am honestly just
>>>> lost at the moment.
>>>> I welcome any constructive criticism and any other help you can provide.
>>>>
>>>> Thank you for your help.
>>>> I look forward to your responses.
>>>>
>>>> With kind regards,
>>>>
>>>> Alexander Edward Botha
>>>> alexbotha555 at gmail.com 082 414 9030
>>>> PhD candidate
>>>> Mammal ecology
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>

	[[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Sun May 17 01:09:17 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Sat, 16 May 2020 18:09:17 -0500
Subject: [R-sig-ME] longitudinal analysis when one group switched from
 control to treatment
Message-ID: <CACgv6yUwJSAm9DPpYWY8YwMZjCZY1JrTE5QJTtxXszGC3x02yg@mail.gmail.com>

Hello All,

I have a 3-year longitudinal dataset (*see link below the table*). Up to
year 2 (coded "1"), 8 schools (4 in Treatment, 4 in Control) cooperated
with the study. But in year 3 (coded "2"), one of the Treatment schools
(named "good") dropped out.

Also in year 3 (coded "2"), we were made to move one of the *Control *schools
(named "*orange*") to the *Treatment *group. The full design of the study
is shown in the Table below.

I want to regress "year" and "group" on "y" (a continuous response) in lme4
package in R. But is there a way to capture the switch of one of the
control schools to the treatment group?

Thank you very much, Simon

?       *Switched from control to treatment*

?       *Out as of year coded 2*

*SCHOOL NAMES*

*Year*

*Codes*

*Control*

*Treatment*

0

har

john

bus

orange

caro

good

bla

carm

1

har

john

bus

*orange*

caro

good

bla

carm

2

har

john

bus

X

caro

*orange*

bla

carm

*library(lme4)*
*dat <- read.csv('https://raw.githubusercontent.com/hkil/m/master/z.csv
<https://raw.githubusercontent.com/hkil/m/master/z.csv>')*

*m1 <- lmer(y~ year*group + (1|stid), data = dat)      #### 'stid' =
student id                m2 <- lmer(y~ year*group + (1|scid/stid), data =
dat) #### 'scid' = school id*

	[[alternative HTML version deleted]]


From hu@ng@jcc @end|ng |rom gm@||@com  Mon May 18 07:31:46 2020
From: hu@ng@jcc @end|ng |rom gm@||@com (Sijia Huang)
Date: Sun, 17 May 2020 22:31:46 -0700
Subject: [R-sig-ME] Multiple-membership models with lme4
Message-ID: <CAPmBuzF8aQi_V0VYsy+R=FGJvrjysuyP=wU3rOxM_Qvuff=S1w@mail.gmail.com>

Hi everyone,
I am working on estimating multiple membership models with lme4, following
the instructions posted here
https://bbolker.github.io/mixedmodels-misc/notes/multimember.html

Below is my code, in which J2 is the number of clusters (in my case, the
clusters are clique-2s, and J2=1345) and N is the number of participants
(N=968). These participants belong to 0 to 11 of the clique-2s.

I got the below error. Could anyone help? Thanks!

> fake2 <- rep(1:J2, length.out=N)
> lmod  <- lFormula(formula=y~1+(1|fake2), data=data)
Error: number of levels of each grouping factor must be < number of
observations



Best,
Sijia

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Mon May 18 08:35:41 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Mon, 18 May 2020 08:35:41 +0200
Subject: [R-sig-ME] Multiple-membership models with lme4
In-Reply-To: <CAPmBuzF8aQi_V0VYsy+R=FGJvrjysuyP=wU3rOxM_Qvuff=S1w@mail.gmail.com>
References: <CAPmBuzF8aQi_V0VYsy+R=FGJvrjysuyP=wU3rOxM_Qvuff=S1w@mail.gmail.com>
Message-ID: <CAJuCY5zH6VoO-hc2hzBijDzGvvJZM3G+O75fW=u=R3OGrjs1QA@mail.gmail.com>

Dear Sijia,

The error message seems clear to me. The number of random effect levels
must be less than the number of observations. Otherwise the can't distinct
between the random effect and the residual.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op ma 18 mei 2020 om 07:32 schreef Sijia Huang <huangsjcc at gmail.com>:

> Hi everyone,
> I am working on estimating multiple membership models with lme4, following
> the instructions posted here
> https://bbolker.github.io/mixedmodels-misc/notes/multimember.html
>
> Below is my code, in which J2 is the number of clusters (in my case, the
> clusters are clique-2s, and J2=1345) and N is the number of participants
> (N=968). These participants belong to 0 to 11 of the clique-2s.
>
> I got the below error. Could anyone help? Thanks!
>
> > fake2 <- rep(1:J2, length.out=N)
> > lmod  <- lFormula(formula=y~1+(1|fake2), data=data)
> Error: number of levels of each grouping factor must be < number of
> observations
>
>
>
> Best,
> Sijia
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Mon May 18 09:01:31 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Mon, 18 May 2020 09:01:31 +0200
Subject: [R-sig-ME] longitudinal analysis when one group switched from
 control to treatment
In-Reply-To: <CACgv6yUwJSAm9DPpYWY8YwMZjCZY1JrTE5QJTtxXszGC3x02yg@mail.gmail.com>
References: <CACgv6yUwJSAm9DPpYWY8YwMZjCZY1JrTE5QJTtxXszGC3x02yg@mail.gmail.com>
Message-ID: <CAJuCY5xSWa7haNi0Ojn=nfjQB2rwTqDTBPg-BJ+nvWqUtkpFgw@mail.gmail.com>

Dear Simon,

The question is rather if the model is able to capture this change. Have a
look at the residuals. If they look OK, then the model handles the change
in treatment.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op zo 17 mei 2020 om 01:09 schreef Simon Harmel <sim.harmel at gmail.com>:

> Hello All,
>
> I have a 3-year longitudinal dataset (*see link below the table*). Up to
> year 2 (coded "1"), 8 schools (4 in Treatment, 4 in Control) cooperated
> with the study. But in year 3 (coded "2"), one of the Treatment schools
> (named "good") dropped out.
>
> Also in year 3 (coded "2"), we were made to move one of the *Control
> *schools
> (named "*orange*") to the *Treatment *group. The full design of the study
> is shown in the Table below.
>
> I want to regress "year" and "group" on "y" (a continuous response) in lme4
> package in R. But is there a way to capture the switch of one of the
> control schools to the treatment group?
>
> Thank you very much, Simon
>
> ?       *Switched from control to treatment*
>
> ?       *Out as of year coded 2*
>
> *SCHOOL NAMES*
>
> *Year*
>
> *Codes*
>
> *Control*
>
> *Treatment*
>
> 0
>
> har
>
> john
>
> bus
>
> orange
>
> caro
>
> good
>
> bla
>
> carm
>
> 1
>
> har
>
> john
>
> bus
>
> *orange*
>
> caro
>
> good
>
> bla
>
> carm
>
> 2
>
> har
>
> john
>
> bus
>
> X
>
> caro
>
> *orange*
>
> bla
>
> carm
>
> *library(lme4)*
> *dat <- read.csv('https://raw.githubusercontent.com/hkil/m/master/z.csv
> <https://raw.githubusercontent.com/hkil/m/master/z.csv>')*
>
> *m1 <- lmer(y~ year*group + (1|stid), data = dat)      #### 'stid' =
> student id                m2 <- lmer(y~ year*group + (1|scid/stid), data =
> dat) #### 'scid' = school id*
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From hu@ng@jcc @end|ng |rom gm@||@com  Mon May 18 17:25:11 2020
From: hu@ng@jcc @end|ng |rom gm@||@com (Sijia Huang)
Date: Mon, 18 May 2020 08:25:11 -0700
Subject: [R-sig-ME] Multiple-membership models with lme4
In-Reply-To: <CAJuCY5zH6VoO-hc2hzBijDzGvvJZM3G+O75fW=u=R3OGrjs1QA@mail.gmail.com>
References: <CAPmBuzF8aQi_V0VYsy+R=FGJvrjysuyP=wU3rOxM_Qvuff=S1w@mail.gmail.com>
 <CAJuCY5zH6VoO-hc2hzBijDzGvvJZM3G+O75fW=u=R3OGrjs1QA@mail.gmail.com>
Message-ID: <CAPmBuzEvriWL1KPaHHwKrRypTrYS00xpiPBiU2kP6JTnJLGwBw@mail.gmail.com>

Thanks for the response, Thierry!

I am fitting a multiple-membership model. Since a participant can belong to
more than one cluster, it is possible that the cluster number (number of
random effects) is larger than the number of observations. I wonder if
there is any trick I can perform in lme4 if I want to fit this model with
lme4.

Thank you all!


Best,
sIJIA

On Sun, May 17, 2020 at 11:35 PM Thierry Onkelinx <thierry.onkelinx at inbo.be>
wrote:

> Dear Sijia,
>
> The error message seems clear to me. The number of random effect levels
> must be less than the number of observations. Otherwise the can't distinct
> between the random effect and the residual.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op ma 18 mei 2020 om 07:32 schreef Sijia Huang <huangsjcc at gmail.com>:
>
>> Hi everyone,
>> I am working on estimating multiple membership models with lme4, following
>> the instructions posted here
>> https://bbolker.github.io/mixedmodels-misc/notes/multimember.html
>>
>> Below is my code, in which J2 is the number of clusters (in my case, the
>> clusters are clique-2s, and J2=1345) and N is the number of participants
>> (N=968). These participants belong to 0 to 11 of the clique-2s.
>>
>> I got the below error. Could anyone help? Thanks!
>>
>> > fake2 <- rep(1:J2, length.out=N)
>> > lmod  <- lFormula(formula=y~1+(1|fake2), data=data)
>> Error: number of levels of each grouping factor must be < number of
>> observations
>>
>>
>>
>> Best,
>> Sijia
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Mon May 18 17:44:50 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Mon, 18 May 2020 10:44:50 -0500
Subject: [R-sig-ME] longitudinal analysis when one group switched from
 control to treatment
In-Reply-To: <CAJuCY5xSWa7haNi0Ojn=nfjQB2rwTqDTBPg-BJ+nvWqUtkpFgw@mail.gmail.com>
References: <CACgv6yUwJSAm9DPpYWY8YwMZjCZY1JrTE5QJTtxXszGC3x02yg@mail.gmail.com>
 <CAJuCY5xSWa7haNi0Ojn=nfjQB2rwTqDTBPg-BJ+nvWqUtkpFgw@mail.gmail.com>
Message-ID: <CACgv6yVz9mDO2+2bqkZ=85aF-MLL_peL3A=kCS7z++r_b3_Yng@mail.gmail.com>

Dear Thierry,

By "Have a look at the residuals" you mean something like the following
(below)? So no other adjustment is required for the switching that occurred?

plot(m1, type = c("p","smooth"), col.line = 2)

plot(m1, sqrt(abs(resid(.)))~fitted(.), type = c("p","smooth"), col.line =
2)

On Mon, May 18, 2020 at 2:01 AM Thierry Onkelinx <thierry.onkelinx at inbo.be>
wrote:

> Dear Simon,
>
> The question is rather if the model is able to capture this change. Have a
> look at the residuals. If they look OK, then the model handles the change
> in treatment.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op zo 17 mei 2020 om 01:09 schreef Simon Harmel <sim.harmel at gmail.com>:
>
>> Hello All,
>>
>> I have a 3-year longitudinal dataset (*see link below the table*). Up to
>> year 2 (coded "1"), 8 schools (4 in Treatment, 4 in Control) cooperated
>> with the study. But in year 3 (coded "2"), one of the Treatment schools
>> (named "good") dropped out.
>>
>> Also in year 3 (coded "2"), we were made to move one of the *Control
>> *schools
>> (named "*orange*") to the *Treatment *group. The full design of the study
>> is shown in the Table below.
>>
>> I want to regress "year" and "group" on "y" (a continuous response) in
>> lme4
>> package in R. But is there a way to capture the switch of one of the
>> control schools to the treatment group?
>>
>> Thank you very much, Simon
>>
>> ?       *Switched from control to treatment*
>>
>> ?       *Out as of year coded 2*
>>
>> *SCHOOL NAMES*
>>
>> *Year*
>>
>> *Codes*
>>
>> *Control*
>>
>> *Treatment*
>>
>> 0
>>
>> har
>>
>> john
>>
>> bus
>>
>> orange
>>
>> caro
>>
>> good
>>
>> bla
>>
>> carm
>>
>> 1
>>
>> har
>>
>> john
>>
>> bus
>>
>> *orange*
>>
>> caro
>>
>> good
>>
>> bla
>>
>> carm
>>
>> 2
>>
>> har
>>
>> john
>>
>> bus
>>
>> X
>>
>> caro
>>
>> *orange*
>>
>> bla
>>
>> carm
>>
>> *library(lme4)*
>> *dat <- read.csv('https://raw.githubusercontent.com/hkil/m/master/z.csv
>> <https://raw.githubusercontent.com/hkil/m/master/z.csv>')*
>>
>> *m1 <- lmer(y~ year*group + (1|stid), data = dat)      #### 'stid' =
>> student id                m2 <- lmer(y~ year*group + (1|scid/stid), data =
>> dat) #### 'scid' = school id*
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Mon May 18 19:13:25 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Mon, 18 May 2020 19:13:25 +0200
Subject: [R-sig-ME] longitudinal analysis when one group switched from
 control to treatment
In-Reply-To: <CACgv6yVz9mDO2+2bqkZ=85aF-MLL_peL3A=kCS7z++r_b3_Yng@mail.gmail.com>
References: <CACgv6yUwJSAm9DPpYWY8YwMZjCZY1JrTE5QJTtxXszGC3x02yg@mail.gmail.com>
 <CAJuCY5xSWa7haNi0Ojn=nfjQB2rwTqDTBPg-BJ+nvWqUtkpFgw@mail.gmail.com>
 <CACgv6yVz9mDO2+2bqkZ=85aF-MLL_peL3A=kCS7z++r_b3_Yng@mail.gmail.com>
Message-ID: <CAJuCY5wwoYNRxLLtLT3uMUX+GTP_NEW2GKG1DBwJLuAW9XbqQQ@mail.gmail.com>

Dear Simon,

You are wondering if the model captures the change in treatment for a
school. Therefore you need to plot the residuals vs every combination of
school and year. A boxplot for every combination would be useful. If the
change in treatment triggers a shift in residuals, then the current model
fails.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op ma 18 mei 2020 om 17:45 schreef Simon Harmel <sim.harmel at gmail.com>:

> Dear Thierry,
>
> By "Have a look at the residuals" you mean something like the following
> (below)? So no other adjustment is required for the switching that occurred?
>
> plot(m1, type = c("p","smooth"), col.line = 2)
>
> plot(m1, sqrt(abs(resid(.)))~fitted(.), type = c("p","smooth"), col.line =
> 2)
>
> On Mon, May 18, 2020 at 2:01 AM Thierry Onkelinx <thierry.onkelinx at inbo.be>
> wrote:
>
>> Dear Simon,
>>
>> The question is rather if the model is able to capture this change. Have
>> a look at the residuals. If they look OK, then the model handles the change
>> in treatment.
>>
>> Best regards,
>>
>> ir. Thierry Onkelinx
>> Statisticus / Statistician
>>
>> Vlaamse Overheid / Government of Flanders
>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>> AND FOREST
>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>> thierry.onkelinx at inbo.be
>> Havenlaan 88 bus 73, 1000 Brussel
>> www.inbo.be
>>
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of data.
>> ~ John Tukey
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>>
>> <https://www.inbo.be>
>>
>>
>> Op zo 17 mei 2020 om 01:09 schreef Simon Harmel <sim.harmel at gmail.com>:
>>
>>> Hello All,
>>>
>>> I have a 3-year longitudinal dataset (*see link below the table*). Up to
>>> year 2 (coded "1"), 8 schools (4 in Treatment, 4 in Control) cooperated
>>> with the study. But in year 3 (coded "2"), one of the Treatment schools
>>> (named "good") dropped out.
>>>
>>> Also in year 3 (coded "2"), we were made to move one of the *Control
>>> *schools
>>> (named "*orange*") to the *Treatment *group. The full design of the study
>>> is shown in the Table below.
>>>
>>> I want to regress "year" and "group" on "y" (a continuous response) in
>>> lme4
>>> package in R. But is there a way to capture the switch of one of the
>>> control schools to the treatment group?
>>>
>>> Thank you very much, Simon
>>>
>>> ?       *Switched from control to treatment*
>>>
>>> ?       *Out as of year coded 2*
>>>
>>> *SCHOOL NAMES*
>>>
>>> *Year*
>>>
>>> *Codes*
>>>
>>> *Control*
>>>
>>> *Treatment*
>>>
>>> 0
>>>
>>> har
>>>
>>> john
>>>
>>> bus
>>>
>>> orange
>>>
>>> caro
>>>
>>> good
>>>
>>> bla
>>>
>>> carm
>>>
>>> 1
>>>
>>> har
>>>
>>> john
>>>
>>> bus
>>>
>>> *orange*
>>>
>>> caro
>>>
>>> good
>>>
>>> bla
>>>
>>> carm
>>>
>>> 2
>>>
>>> har
>>>
>>> john
>>>
>>> bus
>>>
>>> X
>>>
>>> caro
>>>
>>> *orange*
>>>
>>> bla
>>>
>>> carm
>>>
>>> *library(lme4)*
>>> *dat <- read.csv('https://raw.githubusercontent.com/hkil/m/master/z.csv
>>> <https://raw.githubusercontent.com/hkil/m/master/z.csv>')*
>>>
>>> *m1 <- lmer(y~ year*group + (1|stid), data = dat)      #### 'stid' =
>>> student id                m2 <- lmer(y~ year*group + (1|scid/stid), data
>>> =
>>> dat) #### 'scid' = school id*
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Tue May 19 00:52:01 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 18 May 2020 18:52:01 -0400
Subject: [R-sig-ME] Multiple-membership models with lme4
In-Reply-To: <CAPmBuzEvriWL1KPaHHwKrRypTrYS00xpiPBiU2kP6JTnJLGwBw@mail.gmail.com>
References: <CAPmBuzF8aQi_V0VYsy+R=FGJvrjysuyP=wU3rOxM_Qvuff=S1w@mail.gmail.com>
 <CAJuCY5zH6VoO-hc2hzBijDzGvvJZM3G+O75fW=u=R3OGrjs1QA@mail.gmail.com>
 <CAPmBuzEvriWL1KPaHHwKrRypTrYS00xpiPBiU2kP6JTnJLGwBw@mail.gmail.com>
Message-ID: <cd3f4c7d-bf9a-69b1-87ad-706edb1e806e@gmail.com>


 ?????? I haven't looked at this carefully, but it's sometimes the case 
that when you do 'fancy' stuff like multi-membership models, you have to 
override the default checks that lme4 does.? You can check out the 
"check.*" arguments in ?lme4::lmerControl, e.g. specifying

 ? control=lmerControl(check.nobs.vs.nlev = "ignore")


(you may also need to adjust check.nobs.vs.nRE)


 ? cheers

 ?? Ben Bolker


On 5/18/20 11:25 AM, Sijia Huang wrote:
> Thanks for the response, Thierry!
>
> I am fitting a multiple-membership model. Since a participant can belong to
> more than one cluster, it is possible that the cluster number (number of
> random effects) is larger than the number of observations. I wonder if
> there is any trick I can perform in lme4 if I want to fit this model with
> lme4.
>
> Thank you all!
>
>
> Best,
> sIJIA
>
> On Sun, May 17, 2020 at 11:35 PM Thierry Onkelinx <thierry.onkelinx at inbo.be>
> wrote:
>
>> Dear Sijia,
>>
>> The error message seems clear to me. The number of random effect levels
>> must be less than the number of observations. Otherwise the can't distinct
>> between the random effect and the residual.
>>
>> Best regards,
>>
>> ir. Thierry Onkelinx
>> Statisticus / Statistician
>>
>> Vlaamse Overheid / Government of Flanders
>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
>> FOREST
>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>> thierry.onkelinx at inbo.be
>> Havenlaan 88 bus 73, 1000 Brussel
>> www.inbo.be
>>
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of data.
>> ~ John Tukey
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>>
>> <https://www.inbo.be>
>>
>>
>> Op ma 18 mei 2020 om 07:32 schreef Sijia Huang <huangsjcc at gmail.com>:
>>
>>> Hi everyone,
>>> I am working on estimating multiple membership models with lme4, following
>>> the instructions posted here
>>> https://bbolker.github.io/mixedmodels-misc/notes/multimember.html
>>>
>>> Below is my code, in which J2 is the number of clusters (in my case, the
>>> clusters are clique-2s, and J2=1345) and N is the number of participants
>>> (N=968). These participants belong to 0 to 11 of the clique-2s.
>>>
>>> I got the below error. Could anyone help? Thanks!
>>>
>>>> fake2 <- rep(1:J2, length.out=N)
>>>> lmod  <- lFormula(formula=y~1+(1|fake2), data=data)
>>> Error: number of levels of each grouping factor must be < number of
>>> observations
>>>
>>>
>>>
>>> Best,
>>> Sijia
>>>
>>>          [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From |do@@@nto@ @end|ng |rom @mph|b|@n@@org  Wed May 20 17:19:38 2020
From: |do@@@nto@ @end|ng |rom @mph|b|@n@@org (Leida Dos Santos)
Date: Wed, 20 May 2020 16:19:38 +0100
Subject: [R-sig-ME] a GlmmTMB advice
Message-ID: <CAFnE7VBnRp01i6MMHmeDCKCBqrkJqqTRZsJLLiJSJm5X9AXXgQ@mail.gmail.com>

Hello there, I was wondering if you help. I am still learning how to work
with GlmmTMB and I have fitted GlmmTMB before but for categorical data . I
currently am working on a paper and have a data set where I am trying to
fit a GlmmTMB. I want to show the effect climate data on species richness
and abundance . I have fitted the predictor response with species "Richness
Index" and another with "abundance ", and predictor variables "climate
data" such as tmax, tmin, precipitation (Richness index I calculated using
vegan package). I have fitted (site and Month) as random intercept, because
the data was collected with no consistency but random days and month, and
years (2010-2019). There are 19 different specimens, and n= 467. All
variables are numerical. #Global Model example: Abun_2<- glmmTMB(Richness ~
(tmin +ppt1 + tmax1 + tmax2 +tmin2+ ppt2 + Year)^2+ (1|Site/Month),
data=Main_data, family="nbinom2"). However when I run this model, I come
across some warning messages:

"Found more than one class "Matrix" in cache; using the first, from
namespace 'Matrix'
Also defined by ?arkhe?
Warning messages:
_1: In glmmTMB(Abundance ~ (tmin + ppt + tmax2 + tmax + tmin2 + ppt2 + :
non-integer counts in a nbinom2 model
2: In fitTMB(TMBStruc) :
Model convergence problem; non-positive-definite Hessian matrix. See
vignette('troubleshooting')
3: In fitTMB(TMBStruc) :
Model convergence problem; function evaluation limit reached without
convergence (9). See vignette('troubleshooting')".

I checked vignette, but the truth is, this is too advanced for and I do not
understand what it is saying:) I would love to have some feedback with
regards to the model. Additionally, should I use just count for Richness
instead of the Menhinick index for richness? Or should I use a completely
different model? I would be very grateful for any feedback please:):) Thank
you in advance. :x
*Leida Dos Santos*
*BSc**,QTS,MSc,**PhD*
*IUCN SSC ASG Programme Officer*
*ldossantos at amphibians.org <ldossantos at amphibians.org>*
*leidamphibian at gmail.com <leidamphibian at gmail.com>*
*@anfileida*
*http://www.amphibians.org/ <http://www.amphibians.org/>*
*http://www.nzfrogs.org <http://www.nzfrogs.org>*
       *0  0*



*      ( -- )  /\(      )/\^^  ^^  ^^  ^^**PS: Please consider the
environment before printing this E-mail*

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Wed May 20 17:22:54 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Wed, 20 May 2020 17:22:54 +0200
Subject: [R-sig-ME] longitudinal analysis when one group switched from
 control to treatment
In-Reply-To: <CACgv6yVTKV+j=bsbDq3cqS9R9hZfHs8_DE0JHVaJ_-=-jyqi7g@mail.gmail.com>
References: <CACgv6yUwJSAm9DPpYWY8YwMZjCZY1JrTE5QJTtxXszGC3x02yg@mail.gmail.com>
 <CAJuCY5xSWa7haNi0Ojn=nfjQB2rwTqDTBPg-BJ+nvWqUtkpFgw@mail.gmail.com>
 <CACgv6yVz9mDO2+2bqkZ=85aF-MLL_peL3A=kCS7z++r_b3_Yng@mail.gmail.com>
 <CAJuCY5wwoYNRxLLtLT3uMUX+GTP_NEW2GKG1DBwJLuAW9XbqQQ@mail.gmail.com>
 <CACgv6yVznsyErRKYcAgp6K1Wbw8tEkrq9qxX1FCkYy8H=Yj_-g@mail.gmail.com>
 <CAJuCY5xuYXuFV6rK3eQjFjWoVqBG9zMYPq3NpJSsF8pNmO5HUw@mail.gmail.com>
 <CACgv6yVTKV+j=bsbDq3cqS9R9hZfHs8_DE0JHVaJ_-=-jyqi7g@mail.gmail.com>
Message-ID: <CAJuCY5wvs_w5EoJ5Tm_HpfN=JjOFpe_HFw9if9Cw2H73PUny_Q@mail.gmail.com>

Please keep the mailing list in cc

1) Both boxplots display the same information. Choose the one you find the
most clear.
2) In an ideal world, all boxplots should look similar. Determining if they
are sufficiently similar is the point where statistics becomes an art
instead of science.
3) You could consider a random effect of the year-school interaction
(1|scid:year). This has the downside that each year-school combination gets
an independent estimate, ignoring a common school effect. It's up to you to
compare an improvement in fit vs the interpretability of the model.

Best regards,

Thierry

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op di 19 mei 2020 om 15:26 schreef Simon Harmel <sim.harmel at gmail.com>:

> Dear Thierry,
>
> Thank you very much for the demonstration. Three questions:
>
> (1) Are two different sets of boxplots needed?
> (2) From these boxplots, how can we exactly determine the the failure of "
> m2" in capturing the switch of a school from control to treatment (what
> are the criteria)?
> (3) Suppose "m2" failed, what is generally an alternative to account for
> the switch  of a school from control to treatment?
>
> Thank you, Simon
>
> On Tue, May 19, 2020 at 2:27 AM Thierry Onkelinx <thierry.onkelinx at inbo.be>
> wrote:
>
>> Dear Simon,
>>
>> Something like this
>>
>> library(lme4)
>> dat <- read.csv('https://raw.githubusercontent.com/hkil/m/master/z.csv')
>> dat$year <- factor(dat$year)
>> m2 <- lmer(y~ year*group + (1|scid/stid), data =dat)
>> dat$resid <- resid(m2)
>> library(ggplot2)
>> ggplot(dat, aes(x = year, y = resid, colour = group)) +
>>   geom_boxplot() +
>>   facet_wrap(~scid)
>> ggplot(dat, aes(x = scid, y = resid, colour = year, fill = group)) +
>>   geom_boxplot()
>>
>> Best regards,
>>
>> ir. Thierry Onkelinx
>> Statisticus / Statistician
>>
>> Vlaamse Overheid / Government of Flanders
>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>> AND FOREST
>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>> thierry.onkelinx at inbo.be
>> Havenlaan 88 bus 73, 1000 Brussel
>> www.inbo.be
>>
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of data.
>> ~ John Tukey
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>>
>> <https://www.inbo.be>
>>
>>
>> Op ma 18 mei 2020 om 19:35 schreef Simon Harmel <sim.harmel at gmail.com>:
>>
>>> Dear Thierry,
>>>
>>> Would you possibly demonstrate what you exactly mean perhaps in R?
>>>
>>> Thank you,  Simon
>>>
>>> On Mon, May 18, 2020, 12:13 PM Thierry Onkelinx <
>>> thierry.onkelinx at inbo.be> wrote:
>>>
>>>> Dear Simon,
>>>>
>>>> You are wondering if the model captures the change in treatment for a
>>>> school. Therefore you need to plot the residuals vs every combination of
>>>> school and year. A boxplot for every combination would be useful. If the
>>>> change in treatment triggers a shift in residuals, then the current model
>>>> fails.
>>>>
>>>> Best regards,
>>>>
>>>> ir. Thierry Onkelinx
>>>> Statisticus / Statistician
>>>>
>>>> Vlaamse Overheid / Government of Flanders
>>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>>>> AND FOREST
>>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>>>> thierry.onkelinx at inbo.be
>>>> Havenlaan 88 bus 73, 1000 Brussel
>>>> www.inbo.be
>>>>
>>>>
>>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>> To call in the statistician after the experiment is done may be no more
>>>> than asking him to perform a post-mortem examination: he may be able to say
>>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>>> The plural of anecdote is not data. ~ Roger Brinner
>>>> The combination of some data and an aching desire for an answer does
>>>> not ensure that a reasonable answer can be extracted from a given body of
>>>> data. ~ John Tukey
>>>>
>>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>
>>>> <https://www.inbo.be>
>>>>
>>>>
>>>> Op ma 18 mei 2020 om 17:45 schreef Simon Harmel <sim.harmel at gmail.com>:
>>>>
>>>>> Dear Thierry,
>>>>>
>>>>> By "Have a look at the residuals" you mean something like the
>>>>> following (below)? So no other adjustment is required for the switching
>>>>> that occurred?
>>>>>
>>>>> plot(m1, type = c("p","smooth"), col.line = 2)
>>>>>
>>>>> plot(m1, sqrt(abs(resid(.)))~fitted(.), type = c("p","smooth"),
>>>>> col.line = 2)
>>>>>
>>>>> On Mon, May 18, 2020 at 2:01 AM Thierry Onkelinx <
>>>>> thierry.onkelinx at inbo.be> wrote:
>>>>>
>>>>>> Dear Simon,
>>>>>>
>>>>>> The question is rather if the model is able to capture this change.
>>>>>> Have a look at the residuals. If they look OK, then the model handles the
>>>>>> change in treatment.
>>>>>>
>>>>>> Best regards,
>>>>>>
>>>>>> ir. Thierry Onkelinx
>>>>>> Statisticus / Statistician
>>>>>>
>>>>>> Vlaamse Overheid / Government of Flanders
>>>>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR
>>>>>> NATURE AND FOREST
>>>>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>>>>>> thierry.onkelinx at inbo.be
>>>>>> Havenlaan 88 bus 73, 1000 Brussel
>>>>>> www.inbo.be
>>>>>>
>>>>>>
>>>>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>>> To call in the statistician after the experiment is done may be no
>>>>>> more than asking him to perform a post-mortem examination: he may be able
>>>>>> to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>>>>> The plural of anecdote is not data. ~ Roger Brinner
>>>>>> The combination of some data and an aching desire for an answer does
>>>>>> not ensure that a reasonable answer can be extracted from a given body of
>>>>>> data. ~ John Tukey
>>>>>>
>>>>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>>>
>>>>>> <https://www.inbo.be>
>>>>>>
>>>>>>
>>>>>> Op zo 17 mei 2020 om 01:09 schreef Simon Harmel <sim.harmel at gmail.com
>>>>>> >:
>>>>>>
>>>>>>> Hello All,
>>>>>>>
>>>>>>> I have a 3-year longitudinal dataset (*see link below the table*).
>>>>>>> Up to
>>>>>>> year 2 (coded "1"), 8 schools (4 in Treatment, 4 in Control)
>>>>>>> cooperated
>>>>>>> with the study. But in year 3 (coded "2"), one of the Treatment
>>>>>>> schools
>>>>>>> (named "good") dropped out.
>>>>>>>
>>>>>>> Also in year 3 (coded "2"), we were made to move one of the *Control
>>>>>>> *schools
>>>>>>> (named "*orange*") to the *Treatment *group. The full design of the
>>>>>>> study
>>>>>>> is shown in the Table below.
>>>>>>>
>>>>>>> I want to regress "year" and "group" on "y" (a continuous response)
>>>>>>> in lme4
>>>>>>> package in R. But is there a way to capture the switch of one of the
>>>>>>> control schools to the treatment group?
>>>>>>>
>>>>>>> Thank you very much, Simon
>>>>>>>
>>>>>>> ?       *Switched from control to treatment*
>>>>>>>
>>>>>>> ?       *Out as of year coded 2*
>>>>>>>
>>>>>>> *SCHOOL NAMES*
>>>>>>>
>>>>>>> *Year*
>>>>>>>
>>>>>>> *Codes*
>>>>>>>
>>>>>>> *Control*
>>>>>>>
>>>>>>> *Treatment*
>>>>>>>
>>>>>>> 0
>>>>>>>
>>>>>>> har
>>>>>>>
>>>>>>> john
>>>>>>>
>>>>>>> bus
>>>>>>>
>>>>>>> orange
>>>>>>>
>>>>>>> caro
>>>>>>>
>>>>>>> good
>>>>>>>
>>>>>>> bla
>>>>>>>
>>>>>>> carm
>>>>>>>
>>>>>>> 1
>>>>>>>
>>>>>>> har
>>>>>>>
>>>>>>> john
>>>>>>>
>>>>>>> bus
>>>>>>>
>>>>>>> *orange*
>>>>>>>
>>>>>>> caro
>>>>>>>
>>>>>>> good
>>>>>>>
>>>>>>> bla
>>>>>>>
>>>>>>> carm
>>>>>>>
>>>>>>> 2
>>>>>>>
>>>>>>> har
>>>>>>>
>>>>>>> john
>>>>>>>
>>>>>>> bus
>>>>>>>
>>>>>>> X
>>>>>>>
>>>>>>> caro
>>>>>>>
>>>>>>> *orange*
>>>>>>>
>>>>>>> bla
>>>>>>>
>>>>>>> carm
>>>>>>>
>>>>>>> *library(lme4)*
>>>>>>> *dat <- read.csv('
>>>>>>> https://raw.githubusercontent.com/hkil/m/master/z.csv
>>>>>>> <https://raw.githubusercontent.com/hkil/m/master/z.csv>')*
>>>>>>>
>>>>>>> *m1 <- lmer(y~ year*group + (1|stid), data = dat)      #### 'stid' =
>>>>>>> student id                m2 <- lmer(y~ year*group + (1|scid/stid),
>>>>>>> data =
>>>>>>> dat) #### 'scid' = school id*
>>>>>>>
>>>>>>>         [[alternative HTML version deleted]]
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>>
>>>>>>

	[[alternative HTML version deleted]]


From o||verhooker @end|ng |rom p@@t@t|@t|c@@com  Thu May 21 16:46:40 2020
From: o||verhooker @end|ng |rom p@@t@t|@t|c@@com (Oliver Hooker)
Date: Thu, 21 May 2020 15:46:40 +0100
Subject: [R-sig-ME] (no subject)
Message-ID: <CAO2T93CwK-=sPf4q2TZ2v7WXNeJA77Oh3aH0ERHq4C3ScMztgw@mail.gmail.com>

Generalised Linear (MIXED) (GLMM), Nonlinear (NLGLM) And General Additive
Models (MIXED) (GAMM) (GNAM02) This course will be delivered live


25 May 2020 - 29 May 2020



-- 
Oliver Hooker PhD.
PS statistics

2020 publications;
Parallelism in eco-morphology and gene expression despite variable
evolutionary and genomic backgrounds in a Holarctic fish. PLOS GENETICS
(2020). IN PRESS

www.PSstatistics.com
facebook.com/PSstatistics/
twitter.com/PSstatistics

53 Morrison Street
Glasgow
G5 8LB
+44 (0) 7966500340

	[[alternative HTML version deleted]]


From j@ork|n @end|ng |rom @om@um@ry|@nd@edu  Fri May 22 15:19:00 2020
From: j@ork|n @end|ng |rom @om@um@ry|@nd@edu (Sorkin, John)
Date: Fri, 22 May 2020 13:19:00 +0000
Subject: [R-sig-ME] Covariance structure used in lme
Message-ID: <MN2PR03MB5167541AE317A1869F38A8FBE2B40@MN2PR03MB5167.namprd03.prod.outlook.com>

I am running the following random slope, random intercept model:
#  Model 3
fitRSlope1 <- lme(distance~age+Sex+age*Sex, random=~1+age|Subject,data=Orthodont)
summary(fitRSlope1)
ranef(fitRSlope1)

When I run the model, I get the following output

Random effects:
 Formula: ~1 + age | Subject
 Structure: General positive-definite, Log-Cholesky parametrization
            StdDev    Corr
(Intercept) 2.4055009 (Intr)
age         0.1803455 -0.668
Residual    1.3100396

Is the general positive-definite, Log-Cholesky parametrization a description of what one might call an unstructured variance-covariance matrix with as a particular paramaterization?

Thank  you,
John





John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)


	[[alternative HTML version deleted]]


From @@c@|h|m @end|ng |rom gm@||@com  Fri May 22 13:55:09 2020
From: @@c@|h|m @end|ng |rom gm@||@com (Sara Calhim)
Date: Fri, 22 May 2020 14:55:09 +0300
Subject: [R-sig-ME] MCMCglmmRAM error
Message-ID: <A7D22A2F-78D5-4098-8C46-2B12ED7D1527@gmail.com>

Dear Jarrod (and list)

I am trying to run generalized mixed effects models where
(i) the dependent variable is a non-ordinal 4-level category
(ii) I have to control for phylogeny using a rooted, non ultrametric phylo object with n = 32 species and 26 internal nodes; with branch length info; I remove the node labels  that denote bayesian consensus support because of the earlier error "phylogeny tip/node labels are not unique? while running MCMcglmm()
(iii) fixed effects structure is a single categorical predictor (with 2, 3 or 4 levels depending on the model)
(iv) I have complete separation between Y categ and some of the X1 categories; there is also a strong 'clumping' of the Y_categ on the phylogenetic tree

> str(dummydat)
'data.frame':	32 obs. of  8 variables:
 $ animal : num  1 2 3 4 5 6 7 8 9 10 ...
 $ X1     : Factor w/ 4 levels "Habitat1","Habitat2",..: 4 3 4 4 4 3 4 4 3 4 ...
 $ X2     : Factor w/ 2 levels "Mode1","Mode2": 1 2 1 1 1 1 1 1 1 1 ...
 $ Y_categ: Factor w/ 4 levels "A","B","C","D": 4 4 4 4 4 4 4 2 2 4 ...
 $ Y_A    : num  0 0 0 0 0 0 0 0 0 0 ...
 $ Y_B    : num  0 0 0 0 0 0 0 1 1 0 ...
 $ Y_C    : num  0 0 0 0 0 0 0 0 0 0 ...
 $ Y_D    : num  1 1 1 1 1 1 1 0 0 1 ...

> table(dummydat$Y_categ, dummydat$X1)

    Habitat1 Habitat2 Habitat3 Habitat4
  A        0        3        1        2
  B        0        0        2        1
  C        5        0        0        6
  D        1        0        3        8

> table(dummydat$Y_categ, dummydat$X2)
   
    Mode1 Mode2
  A     4     2
  B     2     1
  C    10     1
  D    11     1

I have been trying to apply Hadfield's (2015) reduced phylogenetic model (MCMCglmm_RAM2) to fix phylogenetic heritability at 1 but I get the error message that 
"non-reduced nodes do not appear first? in my models mod3 and mod4 below.

I have three questions:

(1) What does this error mean? Are the priors correctly set? Is it due to having polytomies in my phylogeny?

(2) What is the difference between models with random = ~us(trait):animal and random = ~cors(trait):animal?

(3) Alternative models using family = categorical and Y_categ as the dependent variable (mod1 and mod2) run. But even long runs (>1e7 iterations) with considerable thinning failed to give well mixed  posterior chains for X1$Habitat1 and X1$Habitat2 levels of the predictor; also effective sample sizes are tiny. This is why I was trying to used the reduced = T option, with cbind() object as the multinomial dependent variable and family = threshold.


Thanks!
Sara


Code:



# # # model with Y_categ

# prior_cat

k <- length(levels(dummydat$Y_categ))
I <- diag(k-1)
J <- matrix(rep(1, (k-1)^2), c(k-1, k-1))

prior_cat <- list(R = list(fix=1, V=(1/k) * (I + J), n = k - 1),
               G = list(G1 = list(V = diag(k - 1), n = k - 1, fix=2)))

# random = ~us

mod1 <- MCMCglmm(Y_categ ~ -1 + X1, random = ~us(trait):animal,  rcov = ~us(trait):units, data = dummydat, pedigree = dummyphylo, scale = FALSE, family = "categorical", prior = prior_cat, nitt = 1e5)

# random = ~cors

mod2 <- MCMCglmm(Y_categ ~ -1 + X1, random = ~cors(trait):animal,  rcov = ~us(trait):units, data = dummydat, pedigree = dummyphylo, scale = FALSE, family = "categorical", prior = prior_cat)


# # # model with cbind(Y_A, Y_B, Y_C, Y_D)

# prior_cbind

prior_cbind <- list(R=list(V=diag(4)*1e-15, fix=1), G=list(G1=list(V=diag(4), nu=0.002, fix=2)))

# random = ~us
 
mod3 <- MCMCglmm(cbind(Y_A, Y_B, Y_C, Y_D) ~ -1 + X1, random = ~us(trait):animal,  rcov = ~us(trait):units, data = dummydat, pedigree = dummyphylo, scale = FALSE, family = c(rep( "threshold", 4)), reduced = TRUE, prior = prior_cbind)

# random = ~cors

mod4 <- MCMCglmm(cbind(Y_A, Y_B, Y_C, Y_D) ~ -1 + X1, random = ~cors(trait):animal,  rcov = ~us(trait):units, data = dummydat, pedigree = dummyphylo, scale = FALSE, family = c(rep( "threshold", 4)), reduced = TRUE, prior = prior_cbind)







	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Fri May 22 17:53:49 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Fri, 22 May 2020 17:53:49 +0200
Subject: [R-sig-ME] Covariance structure used in lme
In-Reply-To: <MN2PR03MB5167541AE317A1869F38A8FBE2B40@MN2PR03MB5167.namprd03.prod.outlook.com>
References: <MN2PR03MB5167541AE317A1869F38A8FBE2B40@MN2PR03MB5167.namprd03.prod.outlook.com>
Message-ID: <CAJuCY5xfPugbRFeZgTsW0vScta6dS1YZE_-jZrAYyx65AtA=BQ@mail.gmail.com>

Dear John,

Yes. Some people call a positive-definite matrix unstructured.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op vr 22 mei 2020 om 15:19 schreef Sorkin, John <jsorkin at som.umaryland.edu>:

> I am running the following random slope, random intercept model:
> #  Model 3
> fitRSlope1 <- lme(distance~age+Sex+age*Sex,
> random=~1+age|Subject,data=Orthodont)
> summary(fitRSlope1)
> ranef(fitRSlope1)
>
> When I run the model, I get the following output
>
> Random effects:
>  Formula: ~1 + age | Subject
>  Structure: General positive-definite, Log-Cholesky parametrization
>             StdDev    Corr
> (Intercept) 2.4055009 (Intr)
> age         0.1803455 -0.668
> Residual    1.3100396
>
> Is the general positive-definite, Log-Cholesky parametrization a
> description of what one might call an unstructured variance-covariance
> matrix with as a particular paramaterization?
>
> Thank  you,
> John
>
>
>
>
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and
> Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From |v@|d@ @end|ng |rom he@|th@uc@d@edu  Fri May 22 18:00:29 2020
From: |v@|d@ @end|ng |rom he@|th@uc@d@edu (Vaida, Florin)
Date: Fri, 22 May 2020 16:00:29 +0000
Subject: [R-sig-ME] Covariance structure used in lme
In-Reply-To: <MN2PR03MB5167541AE317A1869F38A8FBE2B40@MN2PR03MB5167.namprd03.prod.outlook.com>
References: <MN2PR03MB5167541AE317A1869F38A8FBE2B40@MN2PR03MB5167.namprd03.prod.outlook.com>
Message-ID: <3DDE2546-6085-4176-B511-971D2A270991@health.ucsd.edu>

John, this is indeed an unstructured variance-covariance matrix for the random effects.
Not to be confused with the covariance matrix of the longitudinal vector of observations, which in the context of the general linear model can also be modeled in a variety of ways, including unstructured (gls() function in R).

Florin

> On May 22, 2020, at 6:19 AM, Sorkin, John <jsorkin at som.umaryland.edu> wrote:
> 
> I am running the following random slope, random intercept model:
> #  Model 3
> fitRSlope1 <- lme(distance~age+Sex+age*Sex, random=~1+age|Subject,data=Orthodont)
> summary(fitRSlope1)
> ranef(fitRSlope1)
> 
> When I run the model, I get the following output
> 
> Random effects:
> Formula: ~1 + age | Subject
> Structure: General positive-definite, Log-Cholesky parametrization
>            StdDev    Corr
> (Intercept) 2.4055009 (Intr)
> age         0.1803455 -0.668
> Residual    1.3100396
> 
> Is the general positive-definite, Log-Cholesky parametrization a description of what one might call an unstructured variance-covariance matrix with as a particular paramaterization?
> 
> Thank  you,
> John
> 
> 
> 
> 
> 
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From orch|dn @end|ng |rom ||ve@com  Fri May 22 20:27:29 2020
From: orch|dn @end|ng |rom ||ve@com (dani jones)
Date: Fri, 22 May 2020 18:27:29 +0000
Subject: [R-sig-ME] compare a neighbourhood value with the city mean
Message-ID: <MW3PR13MB4092333482CCE36766E21B3DD6B40@MW3PR13MB4092.namprd13.prod.outlook.com>

Hi everyone,

I have a dataset with all neighbourhoods in a city and a value for each neighbourhood.

I am not sure how to test whether the value for each neighbourhood is statistically different from the city value.

Thank you very much!

Dani-Orchid

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri May 22 20:39:50 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 22 May 2020 11:39:50 -0700
Subject: [R-sig-ME] compare a neighbourhood value with the city mean
In-Reply-To: <MW3PR13MB4092333482CCE36766E21B3DD6B40@MW3PR13MB4092.namprd13.prod.outlook.com>
References: <MW3PR13MB4092333482CCE36766E21B3DD6B40@MW3PR13MB4092.namprd13.prod.outlook.com>
Message-ID: <D6D6AC27-A0B0-4F36-8F79-7C77F4DA545A@dcn.davis.ca.us>

That is... mildly interesting, but off topic. Please read the Posting Guide for more on what this list is about. Consider looking at/asking in a statistics forum like Cross Validated until you know what analysis you want to apply to this data and then search the web on how to apply that with R and if that leaves you puzzled then come back and explain what trouble you are having getting R to do what you want it to.

On May 22, 2020 11:27:29 AM PDT, dani jones <orchidn at live.com> wrote:
>Hi everyone,
>
>I have a dataset with all neighbourhoods in a city and a value for each
>neighbourhood.
>
>I am not sure how to test whether the value for each neighbourhood is
>statistically different from the city value.
>
>Thank you very much!
>
>Dani-Orchid
>
>	[[alternative HTML version deleted]]
>
>_______________________________________________
>R-sig-mixed-models at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Sent from my phone. Please excuse my brevity.


From bbo|ker @end|ng |rom gm@||@com  Fri May 22 23:17:03 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Fri, 22 May 2020 17:17:03 -0400
Subject: [R-sig-ME] MCMCglmmRAM error
In-Reply-To: <A7D22A2F-78D5-4098-8C46-2B12ED7D1527@gmail.com>
References: <A7D22A2F-78D5-4098-8C46-2B12ED7D1527@gmail.com>
Message-ID: <b3fcc6a1-0f44-9fb5-31d2-1881ff99dfac@gmail.com>

 ?? I didn't see if this got answered or not.? I don't know the answer 
(sorry): if no-one comes forward here, you might also try the 
r-sig-phylo at r-project.org list ...? Polytomies seem like a good guess - 
shouldn't be too hard to split them with a little bit of random noise 
and see if the problem persists ... ??


 ?? I don't know how cors() works, but in general us() will estimate 
separate variances (<>1) for each component, as well as all the pairwise 
correlations, while cor*() variants will assume the variances are 1 (or 
something fixed): from ?MCMCglmm.

 > ?corg?? fixes the variances along the diagonal to one and ?corgh? 
fixes the variances along the diagonal to those specified in the prior.? 
?cors? allows correlation submatrices.


 ?? Do you mean cor(trait) or cors(trait)?? The difference between us() 
and cor() is that cor() restricts the variances to 1 while us() 
estimates separate variances for each component ...

On 5/22/20 7:55 AM, Sara Calhim wrote:
> Dear Jarrod (and list)
>
> I am trying to run generalized mixed effects models where
> (i) the dependent variable is a non-ordinal 4-level category
> (ii) I have to control for phylogeny using a rooted, non ultrametric phylo object with n = 32 species and 26 internal nodes; with branch length info; I remove the node labels  that denote bayesian consensus support because of the earlier error "phylogeny tip/node labels are not unique? while running MCMcglmm()
> (iii) fixed effects structure is a single categorical predictor (with 2, 3 or 4 levels depending on the model)
> (iv) I have complete separation between Y categ and some of the X1 categories; there is also a strong 'clumping' of the Y_categ on the phylogenetic tree
>
>> str(dummydat)
> 'data.frame':	32 obs. of  8 variables:
>   $ animal : num  1 2 3 4 5 6 7 8 9 10 ...
>   $ X1     : Factor w/ 4 levels "Habitat1","Habitat2",..: 4 3 4 4 4 3 4 4 3 4 ...
>   $ X2     : Factor w/ 2 levels "Mode1","Mode2": 1 2 1 1 1 1 1 1 1 1 ...
>   $ Y_categ: Factor w/ 4 levels "A","B","C","D": 4 4 4 4 4 4 4 2 2 4 ...
>   $ Y_A    : num  0 0 0 0 0 0 0 0 0 0 ...
>   $ Y_B    : num  0 0 0 0 0 0 0 1 1 0 ...
>   $ Y_C    : num  0 0 0 0 0 0 0 0 0 0 ...
>   $ Y_D    : num  1 1 1 1 1 1 1 0 0 1 ...
>
>> table(dummydat$Y_categ, dummydat$X1)
>      Habitat1 Habitat2 Habitat3 Habitat4
>    A        0        3        1        2
>    B        0        0        2        1
>    C        5        0        0        6
>    D        1        0        3        8
>
>> table(dummydat$Y_categ, dummydat$X2)
>     
>      Mode1 Mode2
>    A     4     2
>    B     2     1
>    C    10     1
>    D    11     1
>
> I have been trying to apply Hadfield's (2015) reduced phylogenetic model (MCMCglmm_RAM2) to fix phylogenetic heritability at 1 but I get the error message that
> "non-reduced nodes do not appear first? in my models mod3 and mod4 below.
>
> I have three questions:
>
> (1) What does this error mean? Are the priors correctly set? Is it due to having polytomies in my phylogeny?
>
> (2) What is the difference between models with random = ~us(trait):animal and random = ~cors(trait):animal?
>
> (3) Alternative models using family = categorical and Y_categ as the dependent variable (mod1 and mod2) run. But even long runs (>1e7 iterations) with considerable thinning failed to give well mixed  posterior chains for X1$Habitat1 and X1$Habitat2 levels of the predictor; also effective sample sizes are tiny. This is why I was trying to used the reduced = T option, with cbind() object as the multinomial dependent variable and family = threshold.
>
>
> Thanks!
> Sara
>
>
> Code:
>
>
>
> # # # model with Y_categ
>
> # prior_cat
>
> k <- length(levels(dummydat$Y_categ))
> I <- diag(k-1)
> J <- matrix(rep(1, (k-1)^2), c(k-1, k-1))
>
> prior_cat <- list(R = list(fix=1, V=(1/k) * (I + J), n = k - 1),
>                 G = list(G1 = list(V = diag(k - 1), n = k - 1, fix=2)))
>
> # random = ~us
>
> mod1 <- MCMCglmm(Y_categ ~ -1 + X1, random = ~us(trait):animal,  rcov = ~us(trait):units, data = dummydat, pedigree = dummyphylo, scale = FALSE, family = "categorical", prior = prior_cat, nitt = 1e5)
>
> # random = ~cors
>
> mod2 <- MCMCglmm(Y_categ ~ -1 + X1, random = ~cors(trait):animal,  rcov = ~us(trait):units, data = dummydat, pedigree = dummyphylo, scale = FALSE, family = "categorical", prior = prior_cat)
>
>
> # # # model with cbind(Y_A, Y_B, Y_C, Y_D)
>
> # prior_cbind
>
> prior_cbind <- list(R=list(V=diag(4)*1e-15, fix=1), G=list(G1=list(V=diag(4), nu=0.002, fix=2)))
>
> # random = ~us
>   
> mod3 <- MCMCglmm(cbind(Y_A, Y_B, Y_C, Y_D) ~ -1 + X1, random = ~us(trait):animal,  rcov = ~us(trait):units, data = dummydat, pedigree = dummyphylo, scale = FALSE, family = c(rep( "threshold", 4)), reduced = TRUE, prior = prior_cbind)
>
> # random = ~cors
>
> mod4 <- MCMCglmm(cbind(Y_A, Y_B, Y_C, Y_D) ~ -1 + X1, random = ~cors(trait):animal,  rcov = ~us(trait):units, data = dummydat, pedigree = dummyphylo, scale = FALSE, family = c(rep( "threshold", 4)), reduced = TRUE, prior = prior_cbind)
>
>
>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From @|m@h@rme| @end|ng |rom gm@||@com  Fri May 22 23:57:27 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Fri, 22 May 2020 16:57:27 -0500
Subject: [R-sig-ME] same model runs in nlme but not lme4
Message-ID: <CACgv6yVmFT3fQ9CS7vh0KmXZKJ_retymaGut+7C2qPEjy5bJew@mail.gmail.com>

Hi All,

I was wondering why my model runs ok when I use `nlme` package but it fails
when I use the `lme4` package, am I missing something?

Thanks, Simon

#===================================
library(lme4)
library(nlme)

dat <- read.csv('https://raw.githubusercontent.com/hkil/m/master/z.csv')

m1 <- lmer(y~ group*year + (year|stid), data = dat)       ## Fails ###

m2 <- lme(y~ group*year, random = ~year|stid, data = dat) ## Runs ###

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Sat May 23 00:25:05 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Fri, 22 May 2020 18:25:05 -0400
Subject: [R-sig-ME] same model runs in nlme but not lme4
In-Reply-To: <CACgv6yVmFT3fQ9CS7vh0KmXZKJ_retymaGut+7C2qPEjy5bJew@mail.gmail.com>
References: <CACgv6yVmFT3fQ9CS7vh0KmXZKJ_retymaGut+7C2qPEjy5bJew@mail.gmail.com>
Message-ID: <77d17263-8f4a-224f-414a-e2ad60030144@gmail.com>

 ? Because lme4 is fussier than lme.? lme will fit models where the 
variance components are jointly unidentifiable; lmer tries to detect 
these problems and complains about them.? It's possible that this is a 
false positive.? You can make it run by specifying

m1 <- lmer(y~ group*year + (year|stid), data = dat, 
control=lmerControl(check.nobs.vs.nRE="ignore"))

 ? but I strongly recommend that you think about whether this might be 
exposing problems.

 ?calculating the profile suggests a little bit of weirdness.

pp <- profile(m1,signames=FALSE)

dd <- as.data.frame(pp)

library(ggplot2)
ggplot(dd,aes(.focal,.zeta)) + geom_point() + geom_line() + 
facet_wrap(~.par,scale="free_x")

You can compare confint(pp) to intervals(m2); they're mostly consistent, 
but some caution is suggested for the CIs on the correlation and the year SD


On 5/22/20 5:57 PM, Simon Harmel wrote:
> Hi All,
>
> I was wondering why my model runs ok when I use `nlme` package but it fails
> when I use the `lme4` package, am I missing something?
>
> Thanks, Simon
>
> #===================================
> library(lme4)
> library(nlme)
>
> dat <- read.csv('https://raw.githubusercontent.com/hkil/m/master/z.csv')
>
> m1 <- lmer(y~ group*year + (year|stid), data = dat)       ## Fails ###
>
> m2 <- lme(y~ group*year, random = ~year|stid, data = dat) ## Runs ###
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From @|m@h@rme| @end|ng |rom gm@||@com  Sat May 23 00:35:56 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Fri, 22 May 2020 17:35:56 -0500
Subject: [R-sig-ME] same model runs in nlme but not lme4
In-Reply-To: <77d17263-8f4a-224f-414a-e2ad60030144@gmail.com>
References: <CACgv6yVmFT3fQ9CS7vh0KmXZKJ_retymaGut+7C2qPEjy5bJew@mail.gmail.com>
 <77d17263-8f4a-224f-414a-e2ad60030144@gmail.com>
Message-ID: <CACgv6yXtC9L5Os+hc7PB_wPaHBN0Lh-A5HRbT=EesUjt_A29zQ@mail.gmail.com>

Many thanks, Ben. Just curious, what information do the plots at the end of
your exactly convey?

I also appreciate it if there if you could point me to a documentation in
lme4 where I can learn more about `profile()` and its output.

Many thanks, Simon

On Fri, May 22, 2020 at 5:25 PM Ben Bolker <bbolker at gmail.com> wrote:

>    Because lme4 is fussier than lme.  lme will fit models where the
> variance components are jointly unidentifiable; lmer tries to detect
> these problems and complains about them.  It's possible that this is a
> false positive.  You can make it run by specifying
>
> m1 <- lmer(y~ group*year + (year|stid), data = dat,
> control=lmerControl(check.nobs.vs.nRE="ignore"))
>
>    but I strongly recommend that you think about whether this might be
> exposing problems.
>
>   calculating the profile suggests a little bit of weirdness.
>
> pp <- profile(m1,signames=FALSE)
>
> dd <- as.data.frame(pp)
>
> library(ggplot2)
> ggplot(dd,aes(.focal,.zeta)) + geom_point() + geom_line() +
> facet_wrap(~.par,scale="free_x")
>
> You can compare confint(pp) to intervals(m2); they're mostly consistent,
> but some caution is suggested for the CIs on the correlation and the year
> SD
>
>
> On 5/22/20 5:57 PM, Simon Harmel wrote:
> > Hi All,
> >
> > I was wondering why my model runs ok when I use `nlme` package but it
> fails
> > when I use the `lme4` package, am I missing something?
> >
> > Thanks, Simon
> >
> > #===================================
> > library(lme4)
> > library(nlme)
> >
> > dat <- read.csv('https://raw.githubusercontent.com/hkil/m/master/z.csv')
> >
> > m1 <- lmer(y~ group*year + (year|stid), data = dat)       ## Fails ###
> >
> > m2 <- lme(y~ group*year, random = ~year|stid, data = dat) ## Runs ###
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Sat May 23 00:41:28 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Fri, 22 May 2020 18:41:28 -0400
Subject: [R-sig-ME] same model runs in nlme but not lme4
In-Reply-To: <CACgv6yXtC9L5Os+hc7PB_wPaHBN0Lh-A5HRbT=EesUjt_A29zQ@mail.gmail.com>
References: <CACgv6yVmFT3fQ9CS7vh0KmXZKJ_retymaGut+7C2qPEjy5bJew@mail.gmail.com>
 <77d17263-8f4a-224f-414a-e2ad60030144@gmail.com>
 <CACgv6yXtC9L5Os+hc7PB_wPaHBN0Lh-A5HRbT=EesUjt_A29zQ@mail.gmail.com>
Message-ID: <f7c5c225-c9b4-2ae9-8be1-e4df55e98365@gmail.com>


 ?? Profile plots expressed in terms of the signed square root are 
straight lines if the log-likelihood surface is quadratic (in which case 
the Wald confidence intervals will be reliable). (I know that's very 
terse but I'm composing in haste.)

 ? vignette("lmer", package="lme4") has a little bit.? More generally 
you can read in any advanced stats book about likelihood profiles and 
what they are/mean (section 4 of 
https://ms.mcmaster.ca/~bolker/emdbook/chap6A.pdf gives one such 
introduction).

On 5/22/20 6:35 PM, Simon Harmel wrote:
> Many thanks, Ben. Just curious, what information do the plots at the 
> end of your exactly convey?
>
> I also appreciate it if there if you could point me to a 
> documentation?in lme4 where I can learn more about `profile()` and its 
> output.
>
> Many thanks, Simon
>
> On Fri, May 22, 2020 at 5:25 PM Ben Bolker <bbolker at gmail.com 
> <mailto:bbolker at gmail.com>> wrote:
>
>     Because lme4 is fussier than lme.? lme will fit models where the
>     variance components are jointly unidentifiable; lmer tries to detect
>     these problems and complains about them.? It's possible that this
>     is a
>     false positive.? You can make it run by specifying
>
>     m1 <- lmer(y~ group*year + (year|stid), data = dat,
>     control=lmerControl(check.nobs.vs.nRE="ignore"))
>
>     ?? but I strongly recommend that you think about whether this
>     might be
>     exposing problems.
>
>     ??calculating the profile suggests a little bit of weirdness.
>
>     pp <- profile(m1,signames=FALSE)
>
>     dd <- as.data.frame(pp)
>
>     library(ggplot2)
>     ggplot(dd,aes(.focal,.zeta)) + geom_point() + geom_line() +
>     facet_wrap(~.par,scale="free_x")
>
>     You can compare confint(pp) to intervals(m2); they're mostly
>     consistent,
>     but some caution is suggested for the CIs on the correlation and
>     the year SD
>
>
>     On 5/22/20 5:57 PM, Simon Harmel wrote:
>     > Hi All,
>     >
>     > I was wondering why my model runs ok when I use `nlme` package
>     but it fails
>     > when I use the `lme4` package, am I missing something?
>     >
>     > Thanks, Simon
>     >
>     > #===================================
>     > library(lme4)
>     > library(nlme)
>     >
>     > dat <-
>     read.csv('https://raw.githubusercontent.com/hkil/m/master/z.csv')
>     >
>     > m1 <- lmer(y~ group*year + (year|stid), data = dat) ? ?## Fails ###
>     >
>     > m2 <- lme(y~ group*year, random = ~year|stid, data = dat) ##
>     Runs ###
>     >
>     >? ? ? ?[[alternative HTML version deleted]]
>     >
>     > _______________________________________________
>     > R-sig-mixed-models at r-project.org
>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>     _______________________________________________
>     R-sig-mixed-models at r-project.org
>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>     https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Sat May 23 01:24:57 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Fri, 22 May 2020 18:24:57 -0500
Subject: [R-sig-ME] same model runs in nlme but not lme4
In-Reply-To: <f7c5c225-c9b4-2ae9-8be1-e4df55e98365@gmail.com>
References: <CACgv6yVmFT3fQ9CS7vh0KmXZKJ_retymaGut+7C2qPEjy5bJew@mail.gmail.com>
 <77d17263-8f4a-224f-414a-e2ad60030144@gmail.com>
 <CACgv6yXtC9L5Os+hc7PB_wPaHBN0Lh-A5HRbT=EesUjt_A29zQ@mail.gmail.com>
 <f7c5c225-c9b4-2ae9-8be1-e4df55e98365@gmail.com>
Message-ID: <CACgv6yWtxbAFm4MYEXydzV5n51PST+gvB_-ADj_b7KQq_P0NmQ@mail.gmail.com>

Short but very clear. Appreciate it very much. Don't mean to make this
long, but how this likelihood profile analysis relates with fitted vs.
residual relation? Can they be at odds?

On Fri, May 22, 2020 at 5:41 PM Ben Bolker <bbolker at gmail.com> wrote:

>
>    Profile plots expressed in terms of the signed square root are straight
> lines if the log-likelihood surface is quadratic (in which case the Wald
> confidence intervals will be reliable). (I know that's very terse but I'm
> composing in haste.)
>
>   vignette("lmer", package="lme4") has a little bit.  More generally you
> can read in any advanced stats book about likelihood profiles and what they
> are/mean (section 4 of https://ms.mcmaster.ca/~bolker/emdbook/chap6A.pdf
> gives one such introduction).
> On 5/22/20 6:35 PM, Simon Harmel wrote:
>
> Many thanks, Ben. Just curious, what information do the plots at the end
> of your exactly convey?
>
> I also appreciate it if there if you could point me to a documentation in
> lme4 where I can learn more about `profile()` and its output.
>
> Many thanks, Simon
>
> On Fri, May 22, 2020 at 5:25 PM Ben Bolker <bbolker at gmail.com> wrote:
>
>>    Because lme4 is fussier than lme.  lme will fit models where the
>> variance components are jointly unidentifiable; lmer tries to detect
>> these problems and complains about them.  It's possible that this is a
>> false positive.  You can make it run by specifying
>>
>> m1 <- lmer(y~ group*year + (year|stid), data = dat,
>> control=lmerControl(check.nobs.vs.nRE="ignore"))
>>
>>    but I strongly recommend that you think about whether this might be
>> exposing problems.
>>
>>   calculating the profile suggests a little bit of weirdness.
>>
>> pp <- profile(m1,signames=FALSE)
>>
>> dd <- as.data.frame(pp)
>>
>> library(ggplot2)
>> ggplot(dd,aes(.focal,.zeta)) + geom_point() + geom_line() +
>> facet_wrap(~.par,scale="free_x")
>>
>> You can compare confint(pp) to intervals(m2); they're mostly consistent,
>> but some caution is suggested for the CIs on the correlation and the year
>> SD
>>
>>
>> On 5/22/20 5:57 PM, Simon Harmel wrote:
>> > Hi All,
>> >
>> > I was wondering why my model runs ok when I use `nlme` package but it
>> fails
>> > when I use the `lme4` package, am I missing something?
>> >
>> > Thanks, Simon
>> >
>> > #===================================
>> > library(lme4)
>> > library(nlme)
>> >
>> > dat <- read.csv('https://raw.githubusercontent.com/hkil/m/master/z.csv
>> ')
>> >
>> > m1 <- lmer(y~ group*year + (year|stid), data = dat)       ## Fails ###
>> >
>> > m2 <- lme(y~ group*year, random = ~year|stid, data = dat) ## Runs ###
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Sat May 23 01:35:39 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Fri, 22 May 2020 19:35:39 -0400
Subject: [R-sig-ME] same model runs in nlme but not lme4
In-Reply-To: <CACgv6yWtxbAFm4MYEXydzV5n51PST+gvB_-ADj_b7KQq_P0NmQ@mail.gmail.com>
References: <CACgv6yVmFT3fQ9CS7vh0KmXZKJ_retymaGut+7C2qPEjy5bJew@mail.gmail.com>
 <77d17263-8f4a-224f-414a-e2ad60030144@gmail.com>
 <CACgv6yXtC9L5Os+hc7PB_wPaHBN0Lh-A5HRbT=EesUjt_A29zQ@mail.gmail.com>
 <f7c5c225-c9b4-2ae9-8be1-e4df55e98365@gmail.com>
 <CACgv6yWtxbAFm4MYEXydzV5n51PST+gvB_-ADj_b7KQq_P0NmQ@mail.gmail.com>
Message-ID: <32f62a14-1907-1863-fa28-a75f3d5ab715@gmail.com>

 ?? They're pretty separate things.? The? likelihood profile is 
completely conditional on the model.? I suppose if the data are 
completely insane then the profile will probably be weird too. The 
profile has to do with the shape of the likelihood surface rather than 
the distribution of the variation around the model.

On 5/22/20 7:24 PM, Simon Harmel wrote:
> Short but very clear. Appreciate it very much. Don't mean to make this 
> long, but how this likelihood profile analysis relates with fitted?vs. 
> residual relation? Can they be at odds?
>
> On Fri, May 22, 2020 at 5:41 PM Ben Bolker <bbolker at gmail.com 
> <mailto:bbolker at gmail.com>> wrote:
>
>
>     ?? Profile plots expressed in terms of the signed square root are
>     straight lines if the log-likelihood surface is quadratic (in
>     which case the Wald confidence intervals will be reliable). (I
>     know that's very terse but I'm composing in haste.)
>
>     ? vignette("lmer", package="lme4") has a little bit. More
>     generally you can read in any advanced stats book about likelihood
>     profiles and what they are/mean (section 4 of
>     https://ms.mcmaster.ca/~bolker/emdbook/chap6A.pdf gives one such
>     introduction).
>
>     On 5/22/20 6:35 PM, Simon Harmel wrote:
>>     Many thanks, Ben. Just curious, what information do the plots at
>>     the end of your exactly convey?
>>
>>     I also appreciate it if there if you could point me to a
>>     documentation?in lme4 where I can learn more about `profile()`
>>     and its output.
>>
>>     Many thanks, Simon
>>
>>     On Fri, May 22, 2020 at 5:25 PM Ben Bolker <bbolker at gmail.com
>>     <mailto:bbolker at gmail.com>> wrote:
>>
>>         ?? Because lme4 is fussier than lme.? lme will fit models
>>         where the
>>         variance components are jointly unidentifiable; lmer tries to
>>         detect
>>         these problems and complains about them.? It's possible that
>>         this is a
>>         false positive.? You can make it run by specifying
>>
>>         m1 <- lmer(y~ group*year + (year|stid), data = dat,
>>         control=lmerControl(check.nobs.vs.nRE="ignore"))
>>
>>         ?? but I strongly recommend that you think about whether this
>>         might be
>>         exposing problems.
>>
>>         ??calculating the profile suggests a little bit of weirdness.
>>
>>         pp <- profile(m1,signames=FALSE)
>>
>>         dd <- as.data.frame(pp)
>>
>>         library(ggplot2)
>>         ggplot(dd,aes(.focal,.zeta)) + geom_point() + geom_line() +
>>         facet_wrap(~.par,scale="free_x")
>>
>>         You can compare confint(pp) to intervals(m2); they're mostly
>>         consistent,
>>         but some caution is suggested for the CIs on the correlation
>>         and the year SD
>>
>>
>>         On 5/22/20 5:57 PM, Simon Harmel wrote:
>>         > Hi All,
>>         >
>>         > I was wondering why my model runs ok when I use `nlme`
>>         package but it fails
>>         > when I use the `lme4` package, am I missing something?
>>         >
>>         > Thanks, Simon
>>         >
>>         > #===================================
>>         > library(lme4)
>>         > library(nlme)
>>         >
>>         > dat <-
>>         read.csv('https://raw.githubusercontent.com/hkil/m/master/z.csv')
>>         >
>>         > m1 <- lmer(y~ group*year + (year|stid), data = dat)? ? ?
>>         ?## Fails ###
>>         >
>>         > m2 <- lme(y~ group*year, random = ~year|stid, data = dat)
>>         ## Runs ###
>>         >
>>         >? ? ? ?[[alternative HTML version deleted]]
>>         >
>>         > _______________________________________________
>>         > R-sig-mixed-models at r-project.org
>>         <mailto:R-sig-mixed-models at r-project.org> mailing list
>>         > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>         _______________________________________________
>>         R-sig-mixed-models at r-project.org
>>         <mailto:R-sig-mixed-models at r-project.org> mailing list
>>         https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>

	[[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Sat May 23 01:42:00 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Fri, 22 May 2020 18:42:00 -0500
Subject: [R-sig-ME] same model runs in nlme but not lme4
In-Reply-To: <32f62a14-1907-1863-fa28-a75f3d5ab715@gmail.com>
References: <CACgv6yVmFT3fQ9CS7vh0KmXZKJ_retymaGut+7C2qPEjy5bJew@mail.gmail.com>
 <77d17263-8f4a-224f-414a-e2ad60030144@gmail.com>
 <CACgv6yXtC9L5Os+hc7PB_wPaHBN0Lh-A5HRbT=EesUjt_A29zQ@mail.gmail.com>
 <f7c5c225-c9b4-2ae9-8be1-e4df55e98365@gmail.com>
 <CACgv6yWtxbAFm4MYEXydzV5n51PST+gvB_-ADj_b7KQq_P0NmQ@mail.gmail.com>
 <32f62a14-1907-1863-fa28-a75f3d5ab715@gmail.com>
Message-ID: <CACgv6yUZRUr60AAVM9ZKtc6-6tFAdMV6pCE-L0W3ZU83xv8GLQ@mail.gmail.com>

Many thanks!

On Fri, May 22, 2020 at 6:35 PM Ben Bolker <bbolker at gmail.com> wrote:

>    They're pretty separate things.  The  likelihood profile is completely
> conditional on the model.  I suppose if the data are completely insane then
> the profile will probably be weird too.  The profile has to do with the
> shape of the likelihood surface rather than the distribution of the
> variation around the model.
> On 5/22/20 7:24 PM, Simon Harmel wrote:
>
> Short but very clear. Appreciate it very much. Don't mean to make this
> long, but how this likelihood profile analysis relates with fitted vs.
> residual relation? Can they be at odds?
>
> On Fri, May 22, 2020 at 5:41 PM Ben Bolker <bbolker at gmail.com> wrote:
>
>>
>>    Profile plots expressed in terms of the signed square root are
>> straight lines if the log-likelihood surface is quadratic (in which case
>> the Wald confidence intervals will be reliable). (I know that's very terse
>> but I'm composing in haste.)
>>
>>   vignette("lmer", package="lme4") has a little bit.  More generally you
>> can read in any advanced stats book about likelihood profiles and what they
>> are/mean (section 4 of https://ms.mcmaster.ca/~bolker/emdbook/chap6A.pdf
>> gives one such introduction).
>> On 5/22/20 6:35 PM, Simon Harmel wrote:
>>
>> Many thanks, Ben. Just curious, what information do the plots at the end
>> of your exactly convey?
>>
>> I also appreciate it if there if you could point me to a documentation in
>> lme4 where I can learn more about `profile()` and its output.
>>
>> Many thanks, Simon
>>
>> On Fri, May 22, 2020 at 5:25 PM Ben Bolker <bbolker at gmail.com> wrote:
>>
>>>    Because lme4 is fussier than lme.  lme will fit models where the
>>> variance components are jointly unidentifiable; lmer tries to detect
>>> these problems and complains about them.  It's possible that this is a
>>> false positive.  You can make it run by specifying
>>>
>>> m1 <- lmer(y~ group*year + (year|stid), data = dat,
>>> control=lmerControl(check.nobs.vs.nRE="ignore"))
>>>
>>>    but I strongly recommend that you think about whether this might be
>>> exposing problems.
>>>
>>>   calculating the profile suggests a little bit of weirdness.
>>>
>>> pp <- profile(m1,signames=FALSE)
>>>
>>> dd <- as.data.frame(pp)
>>>
>>> library(ggplot2)
>>> ggplot(dd,aes(.focal,.zeta)) + geom_point() + geom_line() +
>>> facet_wrap(~.par,scale="free_x")
>>>
>>> You can compare confint(pp) to intervals(m2); they're mostly consistent,
>>> but some caution is suggested for the CIs on the correlation and the
>>> year SD
>>>
>>>
>>> On 5/22/20 5:57 PM, Simon Harmel wrote:
>>> > Hi All,
>>> >
>>> > I was wondering why my model runs ok when I use `nlme` package but it
>>> fails
>>> > when I use the `lme4` package, am I missing something?
>>> >
>>> > Thanks, Simon
>>> >
>>> > #===================================
>>> > library(lme4)
>>> > library(nlme)
>>> >
>>> > dat <- read.csv('https://raw.githubusercontent.com/hkil/m/master/z.csv
>>> ')
>>> >
>>> > m1 <- lmer(y~ group*year + (year|stid), data = dat)       ## Fails ###
>>> >
>>> > m2 <- lme(y~ group*year, random = ~year|stid, data = dat) ## Runs ###
>>> >
>>> >       [[alternative HTML version deleted]]
>>> >
>>> > _______________________________________________
>>> > R-sig-mixed-models at r-project.org mailing list
>>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>

	[[alternative HTML version deleted]]


From @@|@h@d|n@|ot|| @end|ng |rom gm@||@com  Sat May 23 09:11:59 2020
From: @@|@h@d|n@|ot|| @end|ng |rom gm@||@com (Salahadin Lotfi)
Date: Sat, 23 May 2020 02:11:59 -0500
Subject: [R-sig-ME] lme approximation method for dfs
Message-ID: <CAJTPX_XhATv7AvzLYyTfESsY8VGvBikapUD+Q9gkMqFQES9mEw@mail.gmail.com>

Dear all,
I have a very simple question but, have been having a hard time to figure
it out.
I am using a mixed model with random intercept and slope using lme function
with an unstructured covariance matrix. I know lmer uses Satterthwaite's
approximation method to approximate dfs of fixed effects, but I am not sure
what is the preferred method that lme uses. Is it Wald or Likelihood ratio?
I don't think lme offers such an option to specify an approximation method
for dfs of fixed effects. Does it?

I appreciate any response in advance.
Sala


*************
Salahadin (Sala) Lotfi

PhD Candidate of Cognitive Neuroscience

University of Wisconsin-Milwaukee

Anxiety Disorders Laboratory

President, Association of Clinical and Cognitive Neuroscience, UWM

	[[alternative HTML version deleted]]


From ph||||p@@|d@y @end|ng |rom mp|@n|  Sat May 23 17:07:22 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Sat, 23 May 2020 17:07:22 +0200
Subject: [R-sig-ME] lme approximation method for dfs
In-Reply-To: <CAJTPX_XhATv7AvzLYyTfESsY8VGvBikapUD+Q9gkMqFQES9mEw@mail.gmail.com>
References: <CAJTPX_XhATv7AvzLYyTfESsY8VGvBikapUD+Q9gkMqFQES9mEw@mail.gmail.com>
Message-ID: <421276a8-6922-9f3f-2161-e5497ecbe3f0@mpi.nl>

Hi,

On 23/5/20 9:11 am, Salahadin Lotfi wrote:
> Dear all,
> I have a very simple question but, have been having a hard time to figure
> it out.
> I am using a mixed model with random intercept and slope using lme function
> with an unstructured covariance matrix. I know lmer uses Satterthwaite's
> approximation method to approximate dfs of fixed effects,

This is not accurate. lme4 by default doesn't even try to figure out the
df and doesn't report p-values. The lmerTest package adds in options to
use Satterthwaite or Kenward-Roger approximations for p-values, but
depending on who you ask around here, the sentiment for those
approximations ranges from "of course" to "hmrpf, why would you bother?"
to "the heretics must be purged!".?

The GLMM FAQ (https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html)
has some info on each of these, but I'll copy and paste something
relevant that I wrote on a different mailing list:

Treating the t-values as z-values is as reasonable as using the
t-distribution with some estimated degrees of freedom for studies with
20-30 subjects and 10s of observations per condition per subject for two
reasons. One is that a t-distribution with dozens of degrees of freedom
is essentially a normal distribution, and so even if you could figure
out what the "right" number of degrees of freedom were, it wouldn't be
far off from the number you get from the normal distribution. The other
reason is that none of these asymptotic results are guaranteed to be
particularly great for anything other than very well behaved linear
mixed models, which is why things like parametric bootstrap are the gold
standard for figuring out coverage intervals. And for large models,
bootstrapping is about as fast as KR (because KR as implemented in
pbkrmodcomp, which lmerTest depends on, computes the inverse of a large
n x matrix).

> but I am not sure
> what is the preferred method that lme uses. Is it Wald or Likelihood ratio?

Wald and likelihood ratio are not degrees of freedom estimates. The
likelihood-ratio tests do have a df, which corresponds to the difference
in the number of free parameters between the models, but this not the
relevant df. (It's numerator degrees of freedom in the ANOVA framework,
while what you need are the denominator degrees of freedom.) The Wald
tests are just the things you see in the table of the fixed effects,
i.e. the tests corresponding to the t- or z-values (or more generally
the ANOVA-style tests / tests of linear hypotheses you then construct
from the fixed effects).


> I don't think lme offers such an option to specify an approximation method
> for dfs of fixed effects. Does it?

The dfs in nlme are computed using the "inner-outer" rule which doesn't
work well for many types of designs common in cognitive neuroscience.
More information on this is in the GLMM FAQ, search for "Df
alternatives" on that page.


Hope that helps!

Phillip


>
> I appreciate any response in advance.
> Sala
>
>
> *************
> Salahadin (Sala) Lotfi
>
> PhD Candidate of Cognitive Neuroscience
>
> University of Wisconsin-Milwaukee
>
> Anxiety Disorders Laboratory
>
> President, Association of Clinical and Cognitive Neuroscience, UWM
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From @@c@|h|m @end|ng |rom gm@||@com  Sat May 23 23:28:32 2020
From: @@c@|h|m @end|ng |rom gm@||@com (Sara Calhim)
Date: Sun, 24 May 2020 00:28:32 +0300
Subject: [R-sig-ME] MCMCglmmRAM error
In-Reply-To: <b3fcc6a1-0f44-9fb5-31d2-1881ff99dfac@gmail.com>
References: <A7D22A2F-78D5-4098-8C46-2B12ED7D1527@gmail.com>
 <b3fcc6a1-0f44-9fb5-31d2-1881ff99dfac@gmail.com>
Message-ID: <A6DFC1A7-1F61-461C-98C9-220497A68F1F@gmail.com>

Hi Ben

Thanks for the info and suggestions.

My Bayesian background is using JAGS and therefore the notation of MCMCglmm still confuses me. My second question was about the random argument in MCMCglmm, where both random = ~us(trait):animal and random = ~cors(trait):animal are options in the code provided in Hadfield (2015) Meth Ecol Evol ? where the the reduced phylogeny option for MCMCglmm is presented.

I?ll try my luck in the r-sig-phylo list. And also force dichotomous branching to my phylogenetic tree to check if the error disappears.

Cheers,
Sara



> On 23 May 2020, at 00:17, Ben Bolker <bbolker at gmail.com> wrote:
> 
>    I didn't see if this got answered or not.  I don't know the answer (sorry): if no-one comes forward here, you might also try the r-sig-phylo at r-project.org list ...  Polytomies seem like a good guess - shouldn't be too hard to split them with a little bit of random noise and see if the problem persists ... ??
> 
> 
>    I don't know how cors() works, but in general us() will estimate separate variances (<>1) for each component, as well as all the pairwise correlations, while cor*() variants will assume the variances are 1 (or something fixed): from ?MCMCglmm.
> 
> > ?corg?  fixes the variances along the diagonal to one and ?corgh? fixes the variances along the diagonal to those specified in the prior.  ?cors? allows correlation submatrices.
> 
> 
>    Do you mean cor(trait) or cors(trait)?  The difference between us() and cor() is that cor() restricts the variances to 1 while us() estimates separate variances for each component ...
> 
> On 5/22/20 7:55 AM, Sara Calhim wrote:
>> Dear Jarrod (and list)
>> 
>> I am trying to run generalized mixed effects models where
>> (i) the dependent variable is a non-ordinal 4-level category
>> (ii) I have to control for phylogeny using a rooted, non ultrametric phylo object with n = 32 species and 26 internal nodes; with branch length info; I remove the node labels  that denote bayesian consensus support because of the earlier error "phylogeny tip/node labels are not unique? while running MCMcglmm()
>> (iii) fixed effects structure is a single categorical predictor (with 2, 3 or 4 levels depending on the model)
>> (iv) I have complete separation between Y categ and some of the X1 categories; there is also a strong 'clumping' of the Y_categ on the phylogenetic tree
>> 
>>> str(dummydat)
>> 'data.frame':	32 obs. of  8 variables:
>>  $ animal : num  1 2 3 4 5 6 7 8 9 10 ...
>>  $ X1     : Factor w/ 4 levels "Habitat1","Habitat2",..: 4 3 4 4 4 3 4 4 3 4 ...
>>  $ X2     : Factor w/ 2 levels "Mode1","Mode2": 1 2 1 1 1 1 1 1 1 1 ...
>>  $ Y_categ: Factor w/ 4 levels "A","B","C","D": 4 4 4 4 4 4 4 2 2 4 ...
>>  $ Y_A    : num  0 0 0 0 0 0 0 0 0 0 ...
>>  $ Y_B    : num  0 0 0 0 0 0 0 1 1 0 ...
>>  $ Y_C    : num  0 0 0 0 0 0 0 0 0 0 ...
>>  $ Y_D    : num  1 1 1 1 1 1 1 0 0 1 ...
>> 
>>> table(dummydat$Y_categ, dummydat$X1)
>>     Habitat1 Habitat2 Habitat3 Habitat4
>>   A        0        3        1        2
>>   B        0        0        2        1
>>   C        5        0        0        6
>>   D        1        0        3        8
>> 
>>> table(dummydat$Y_categ, dummydat$X2)
>>         Mode1 Mode2
>>   A     4     2
>>   B     2     1
>>   C    10     1
>>   D    11     1
>> 
>> I have been trying to apply Hadfield's (2015) reduced phylogenetic model (MCMCglmm_RAM2) to fix phylogenetic heritability at 1 but I get the error message that
>> "non-reduced nodes do not appear first? in my models mod3 and mod4 below.
>> 
>> I have three questions:
>> 
>> (1) What does this error mean? Are the priors correctly set? Is it due to having polytomies in my phylogeny?
>> 
>> (2) What is the difference between models with random = ~us(trait):animal and random = ~cors(trait):animal?
>> 
>> (3) Alternative models using family = categorical and Y_categ as the dependent variable (mod1 and mod2) run. But even long runs (>1e7 iterations) with considerable thinning failed to give well mixed  posterior chains for X1$Habitat1 and X1$Habitat2 levels of the predictor; also effective sample sizes are tiny. This is why I was trying to used the reduced = T option, with cbind() object as the multinomial dependent variable and family = threshold.
>> 
>> 
>> Thanks!
>> Sara
>> 
>> 
>> Code:
>> 
>> 
>> 
>> # # # model with Y_categ
>> 
>> # prior_cat
>> 
>> k <- length(levels(dummydat$Y_categ))
>> I <- diag(k-1)
>> J <- matrix(rep(1, (k-1)^2), c(k-1, k-1))
>> 
>> prior_cat <- list(R = list(fix=1, V=(1/k) * (I + J), n = k - 1),
>>                G = list(G1 = list(V = diag(k - 1), n = k - 1, fix=2)))
>> 
>> # random = ~us
>> 
>> mod1 <- MCMCglmm(Y_categ ~ -1 + X1, random = ~us(trait):animal,  rcov = ~us(trait):units, data = dummydat, pedigree = dummyphylo, scale = FALSE, family = "categorical", prior = prior_cat, nitt = 1e5)
>> 
>> # random = ~cors
>> 
>> mod2 <- MCMCglmm(Y_categ ~ -1 + X1, random = ~cors(trait):animal,  rcov = ~us(trait):units, data = dummydat, pedigree = dummyphylo, scale = FALSE, family = "categorical", prior = prior_cat)
>> 
>> 
>> # # # model with cbind(Y_A, Y_B, Y_C, Y_D)
>> 
>> # prior_cbind
>> 
>> prior_cbind <- list(R=list(V=diag(4)*1e-15, fix=1), G=list(G1=list(V=diag(4), nu=0.002, fix=2)))
>> 
>> # random = ~us
>>  mod3 <- MCMCglmm(cbind(Y_A, Y_B, Y_C, Y_D) ~ -1 + X1, random = ~us(trait):animal,  rcov = ~us(trait):units, data = dummydat, pedigree = dummyphylo, scale = FALSE, family = c(rep( "threshold", 4)), reduced = TRUE, prior = prior_cbind)
>> 
>> # random = ~cors
>> 
>> mod4 <- MCMCglmm(cbind(Y_A, Y_B, Y_C, Y_D) ~ -1 + X1, random = ~cors(trait):animal,  rcov = ~us(trait):units, data = dummydat, pedigree = dummyphylo, scale = FALSE, family = c(rep( "threshold", 4)), reduced = TRUE, prior = prior_cbind)
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From M@@rten@Jung @end|ng |rom m@||box@tu-dre@den@de  Sun May 24 00:01:07 2020
From: M@@rten@Jung @end|ng |rom m@||box@tu-dre@den@de (Maarten Jung)
Date: Sun, 24 May 2020 00:01:07 +0200
Subject: [R-sig-ME] lme approximation method for dfs
In-Reply-To: <421276a8-6922-9f3f-2161-e5497ecbe3f0@mpi.nl>
References: <CAJTPX_XhATv7AvzLYyTfESsY8VGvBikapUD+Q9gkMqFQES9mEw@mail.gmail.com>
 <421276a8-6922-9f3f-2161-e5497ecbe3f0@mpi.nl>
Message-ID: <CAHr4DydTGERuf4tzR-HQAB-kpRuwJXk6vNqs32cHB_kTWf3HSg@mail.gmail.com>

Dear Sala,

As Phillip Alday explained, there is no implementation of the Satterthwaite
approximation in the nlme package. If you want to stick with this package,
the only way I know to get something similar (for lme objects) is to use
functions of the emmeans package with the argument "mode" set to
"appx-satterthwaite" (see [1]).

[1] https://cran.r-project.org/web/packages/emmeans/vignettes/models.html#K

Best,
Maarten


On Sat, 23 May 2020, 17:07 Phillip Alday <phillip.alday at mpi.nl> wrote:

> Hi,
>
> On 23/5/20 9:11 am, Salahadin Lotfi wrote:
> > Dear all,
> > I have a very simple question but, have been having a hard time to figure
> > it out.
> > I am using a mixed model with random intercept and slope using lme
> function
> > with an unstructured covariance matrix. I know lmer uses Satterthwaite's
> > approximation method to approximate dfs of fixed effects,
>
> This is not accurate. lme4 by default doesn't even try to figure out the
> df and doesn't report p-values. The lmerTest package adds in options to
> use Satterthwaite or Kenward-Roger approximations for p-values, but
> depending on who you ask around here, the sentiment for those
> approximations ranges from "of course" to "hmrpf, why would you bother?"
> to "the heretics must be purged!".
>
> The GLMM FAQ (https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html)
> has some info on each of these, but I'll copy and paste something
> relevant that I wrote on a different mailing list:
>
> Treating the t-values as z-values is as reasonable as using the
> t-distribution with some estimated degrees of freedom for studies with
> 20-30 subjects and 10s of observations per condition per subject for two
> reasons. One is that a t-distribution with dozens of degrees of freedom
> is essentially a normal distribution, and so even if you could figure
> out what the "right" number of degrees of freedom were, it wouldn't be
> far off from the number you get from the normal distribution. The other
> reason is that none of these asymptotic results are guaranteed to be
> particularly great for anything other than very well behaved linear
> mixed models, which is why things like parametric bootstrap are the gold
> standard for figuring out coverage intervals. And for large models,
> bootstrapping is about as fast as KR (because KR as implemented in
> pbkrmodcomp, which lmerTest depends on, computes the inverse of a large
> n x matrix).
>
> > but I am not sure
> > what is the preferred method that lme uses. Is it Wald or Likelihood
> ratio?
>
> Wald and likelihood ratio are not degrees of freedom estimates. The
> likelihood-ratio tests do have a df, which corresponds to the difference
> in the number of free parameters between the models, but this not the
> relevant df. (It's numerator degrees of freedom in the ANOVA framework,
> while what you need are the denominator degrees of freedom.) The Wald
> tests are just the things you see in the table of the fixed effects,
> i.e. the tests corresponding to the t- or z-values (or more generally
> the ANOVA-style tests / tests of linear hypotheses you then construct
> from the fixed effects).
>
>
> > I don't think lme offers such an option to specify an approximation
> method
> > for dfs of fixed effects. Does it?
>
> The dfs in nlme are computed using the "inner-outer" rule which doesn't
> work well for many types of designs common in cognitive neuroscience.
> More information on this is in the GLMM FAQ, search for "Df
> alternatives" on that page.
>
>
> Hope that helps!
>
> Phillip
>
>
> >
> > I appreciate any response in advance.
> > Sala
> >
> >
> > *************
> > Salahadin (Sala) Lotfi
> >
> > PhD Candidate of Cognitive Neuroscience
> >
> > University of Wisconsin-Milwaukee
> >
> > Anxiety Disorders Laboratory
> >
> > President, Association of Clinical and Cognitive Neuroscience, UWM
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Sun May 24 00:44:43 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Sat, 23 May 2020 17:44:43 -0500
Subject: [R-sig-ME] same model runs in nlme but not lme4
In-Reply-To: <CACgv6yUZRUr60AAVM9ZKtc6-6tFAdMV6pCE-L0W3ZU83xv8GLQ@mail.gmail.com>
References: <CACgv6yVmFT3fQ9CS7vh0KmXZKJ_retymaGut+7C2qPEjy5bJew@mail.gmail.com>
 <77d17263-8f4a-224f-414a-e2ad60030144@gmail.com>
 <CACgv6yXtC9L5Os+hc7PB_wPaHBN0Lh-A5HRbT=EesUjt_A29zQ@mail.gmail.com>
 <f7c5c225-c9b4-2ae9-8be1-e4df55e98365@gmail.com>
 <CACgv6yWtxbAFm4MYEXydzV5n51PST+gvB_-ADj_b7KQq_P0NmQ@mail.gmail.com>
 <32f62a14-1907-1863-fa28-a75f3d5ab715@gmail.com>
 <CACgv6yUZRUr60AAVM9ZKtc6-6tFAdMV6pCE-L0W3ZU83xv8GLQ@mail.gmail.com>
Message-ID: <CACgv6yXr3UHSkgspTvcaVM-0PcdG-1BsiZ0b8FE3wWRN=2rE8Q@mail.gmail.com>

Just curious, *as.data.frame(profile(fitted glmmTMB model))* e.g., as
demonstrated HERE
<https://rdrr.io/cran/glmmTMB/man/profile.glmmTMB.html>, doesn't
return a "zeta" column as in lme4 models, rather it contains a column
called "value", what is "value" and why we take its square root when
plotting the likelihood profile?

(p.s. I'm assuming a V pattern in the likelihood profile plot would confirm
the health of Wald CIs for glmTMB models, right?)

Thanks, Simon

On Fri, May 22, 2020 at 6:42 PM Simon Harmel <sim.harmel at gmail.com> wrote:

> Many thanks!
>
> On Fri, May 22, 2020 at 6:35 PM Ben Bolker <bbolker at gmail.com> wrote:
>
>>    They're pretty separate things.  The  likelihood profile is completely
>> conditional on the model.  I suppose if the data are completely insane then
>> the profile will probably be weird too.  The profile has to do with the
>> shape of the likelihood surface rather than the distribution of the
>> variation around the model.
>> On 5/22/20 7:24 PM, Simon Harmel wrote:
>>
>> Short but very clear. Appreciate it very much. Don't mean to make this
>> long, but how this likelihood profile analysis relates with fitted vs.
>> residual relation? Can they be at odds?
>>
>> On Fri, May 22, 2020 at 5:41 PM Ben Bolker <bbolker at gmail.com> wrote:
>>
>>>
>>>    Profile plots expressed in terms of the signed square root are
>>> straight lines if the log-likelihood surface is quadratic (in which case
>>> the Wald confidence intervals will be reliable). (I know that's very terse
>>> but I'm composing in haste.)
>>>
>>>   vignette("lmer", package="lme4") has a little bit.  More generally you
>>> can read in any advanced stats book about likelihood profiles and what they
>>> are/mean (section 4 of https://ms.mcmaster.ca/~bolker/emdbook/chap6A.pdf
>>> gives one such introduction).
>>> On 5/22/20 6:35 PM, Simon Harmel wrote:
>>>
>>> Many thanks, Ben. Just curious, what information do the plots at the end
>>> of your exactly convey?
>>>
>>> I also appreciate it if there if you could point me to a
>>> documentation in lme4 where I can learn more about `profile()` and its
>>> output.
>>>
>>> Many thanks, Simon
>>>
>>> On Fri, May 22, 2020 at 5:25 PM Ben Bolker <bbolker at gmail.com> wrote:
>>>
>>>>    Because lme4 is fussier than lme.  lme will fit models where the
>>>> variance components are jointly unidentifiable; lmer tries to detect
>>>> these problems and complains about them.  It's possible that this is a
>>>> false positive.  You can make it run by specifying
>>>>
>>>> m1 <- lmer(y~ group*year + (year|stid), data = dat,
>>>> control=lmerControl(check.nobs.vs.nRE="ignore"))
>>>>
>>>>    but I strongly recommend that you think about whether this might be
>>>> exposing problems.
>>>>
>>>>   calculating the profile suggests a little bit of weirdness.
>>>>
>>>> pp <- profile(m1,signames=FALSE)
>>>>
>>>> dd <- as.data.frame(pp)
>>>>
>>>> library(ggplot2)
>>>> ggplot(dd,aes(.focal,.zeta)) + geom_point() + geom_line() +
>>>> facet_wrap(~.par,scale="free_x")
>>>>
>>>> You can compare confint(pp) to intervals(m2); they're mostly
>>>> consistent,
>>>> but some caution is suggested for the CIs on the correlation and the
>>>> year SD
>>>>
>>>>
>>>> On 5/22/20 5:57 PM, Simon Harmel wrote:
>>>> > Hi All,
>>>> >
>>>> > I was wondering why my model runs ok when I use `nlme` package but it
>>>> fails
>>>> > when I use the `lme4` package, am I missing something?
>>>> >
>>>> > Thanks, Simon
>>>> >
>>>> > #===================================
>>>> > library(lme4)
>>>> > library(nlme)
>>>> >
>>>> > dat <- read.csv('
>>>> https://raw.githubusercontent.com/hkil/m/master/z.csv')
>>>> >
>>>> > m1 <- lmer(y~ group*year + (year|stid), data = dat)       ## Fails ###
>>>> >
>>>> > m2 <- lme(y~ group*year, random = ~year|stid, data = dat) ## Runs ###
>>>> >
>>>> >       [[alternative HTML version deleted]]
>>>> >
>>>> > _______________________________________________
>>>> > R-sig-mixed-models at r-project.org mailing list
>>>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>

	[[alternative HTML version deleted]]


From d@|uedecke @end|ng |rom uke@de  Sun May 24 11:24:54 2020
From: d@|uedecke @end|ng |rom uke@de (=?iso-8859-1?Q?Daniel_L=FCdecke?=)
Date: Sun, 24 May 2020 11:24:54 +0200
Subject: [R-sig-ME] lme approximation method for dfs
In-Reply-To: <CAHr4DydTGERuf4tzR-HQAB-kpRuwJXk6vNqs32cHB_kTWf3HSg@mail.gmail.com>
References: <CAJTPX_XhATv7AvzLYyTfESsY8VGvBikapUD+Q9gkMqFQES9mEw@mail.gmail.com>
 <421276a8-6922-9f3f-2161-e5497ecbe3f0@mpi.nl>
 <CAHr4DydTGERuf4tzR-HQAB-kpRuwJXk6vNqs32cHB_kTWf3HSg@mail.gmail.com>
Message-ID: <000a01d631ad$2f8273b0$8e875b10$@uke.de>

Dear Sala,

you could use the parameters package
(https://easystats.github.io/parameters), which provides functions to
extract degrees of freedom, p-values, or in general summaries of model
parameters (see example below), which also gives you Satterthwaite
approximated degrees of freedom for models from package nlme. Internally,
the lavaSearch2 package is used to calculate the degrees of freedom. Note,
however, that the approximated degrees of freedom slightly differ from those
that are given by lmerTest::lmer(). This *might* be due to different default
settings in optimization etc. between nlme::lme() and lme4::lmer().

Best
Daniel

library(parameters)
library(nlme)
data(iris)

m1 <- lme(
  Sepal.Length ~ Petal.Width * Petal.Length + Sepal.Width,
  data = iris,
  random = ~ 1 | Species
)

model_parameters(m1)
#> Parameter                  | Coefficient |   SE |         95% CI |     t
|  df |      p
#>
----------------------------------------------------------------------------
-----------
#> (Intercept)                |        2.33 | 0.37 | [ 1.59,  3.06] |  6.27
| 143 | < .001
#> Petal.Width                |       -0.93 | 0.24 | [-1.41, -0.45] | -3.83
| 143 | < .001
#> Petal.Length               |        0.61 | 0.09 | [ 0.44,  0.78] |  7.14
| 143 | < .001
#> Sepal.Width                |        0.56 | 0.08 | [ 0.40,  0.71] |  7.15
| 143 | < .001
#> Petal.Width * Petal.Length |        0.11 | 0.05 | [ 0.01,  0.21] |  2.21
| 143 | 0.029
model_parameters(m1, df_method = "satterthwaite")
#> Warning in sCorrect.lme(x, ..., adjust.Omega = value, adjust.n = value):
Small sample corrections were derived for ML not for REML

#> Parameter                  | Coefficient |   SE |         95% CI |     t
|     df |      p
#>
----------------------------------------------------------------------------
--------------
#> (Intercept)                |        2.33 | 0.37 | [ 1.46,  3.20] |  6.27
|  38.44 | < .001
#> Petal.Width                |       -0.93 | 0.24 | [-1.52, -0.35] | -3.83
|  46.10 | < .001
#> Petal.Length               |        0.61 | 0.09 | [ 0.40,  0.82] |  7.14
|  35.46 | < .001
#> Sepal.Width                |        0.56 | 0.08 | [ 0.39,  0.73] |  7.15
| 171.78 | < .001
#> Petal.Width * Petal.Length |        0.11 | 0.05 | [-0.01,  0.23] |  2.21
|  66.29 | 0.031


-----Urspr?ngliche Nachricht-----
Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im
Auftrag von Maarten Jung
Gesendet: Sonntag, 24. Mai 2020 00:01
Cc: Help Mixed Models <r-sig-mixed-models at r-project.org>
Betreff: Re: [R-sig-ME] lme approximation method for dfs

Dear Sala,

As Phillip Alday explained, there is no implementation of the Satterthwaite
approximation in the nlme package. If you want to stick with this package,
the only way I know to get something similar (for lme objects) is to use
functions of the emmeans package with the argument "mode" set to
"appx-satterthwaite" (see [1]).

[1] https://cran.r-project.org/web/packages/emmeans/vignettes/models.html#K

Best,
Maarten


On Sat, 23 May 2020, 17:07 Phillip Alday <phillip.alday at mpi.nl> wrote:

> Hi,
>
> On 23/5/20 9:11 am, Salahadin Lotfi wrote:
> > Dear all,
> > I have a very simple question but, have been having a hard time to
figure
> > it out.
> > I am using a mixed model with random intercept and slope using lme
> function
> > with an unstructured covariance matrix. I know lmer uses Satterthwaite's
> > approximation method to approximate dfs of fixed effects,
>
> This is not accurate. lme4 by default doesn't even try to figure out the
> df and doesn't report p-values. The lmerTest package adds in options to
> use Satterthwaite or Kenward-Roger approximations for p-values, but
> depending on who you ask around here, the sentiment for those
> approximations ranges from "of course" to "hmrpf, why would you bother?"
> to "the heretics must be purged!".
>
> The GLMM FAQ (https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html)
> has some info on each of these, but I'll copy and paste something
> relevant that I wrote on a different mailing list:
>
> Treating the t-values as z-values is as reasonable as using the
> t-distribution with some estimated degrees of freedom for studies with
> 20-30 subjects and 10s of observations per condition per subject for two
> reasons. One is that a t-distribution with dozens of degrees of freedom
> is essentially a normal distribution, and so even if you could figure
> out what the "right" number of degrees of freedom were, it wouldn't be
> far off from the number you get from the normal distribution. The other
> reason is that none of these asymptotic results are guaranteed to be
> particularly great for anything other than very well behaved linear
> mixed models, which is why things like parametric bootstrap are the gold
> standard for figuring out coverage intervals. And for large models,
> bootstrapping is about as fast as KR (because KR as implemented in
> pbkrmodcomp, which lmerTest depends on, computes the inverse of a large
> n x matrix).
>
> > but I am not sure
> > what is the preferred method that lme uses. Is it Wald or Likelihood
> ratio?
>
> Wald and likelihood ratio are not degrees of freedom estimates. The
> likelihood-ratio tests do have a df, which corresponds to the difference
> in the number of free parameters between the models, but this not the
> relevant df. (It's numerator degrees of freedom in the ANOVA framework,
> while what you need are the denominator degrees of freedom.) The Wald
> tests are just the things you see in the table of the fixed effects,
> i.e. the tests corresponding to the t- or z-values (or more generally
> the ANOVA-style tests / tests of linear hypotheses you then construct
> from the fixed effects).
>
>
> > I don't think lme offers such an option to specify an approximation
> method
> > for dfs of fixed effects. Does it?
>
> The dfs in nlme are computed using the "inner-outer" rule which doesn't
> work well for many types of designs common in cognitive neuroscience.
> More information on this is in the GLMM FAQ, search for "Df
> alternatives" on that page.
>
>
> Hope that helps!
>
> Phillip
>
>
> >
> > I appreciate any response in advance.
> > Sala
> >
> >
> > *************
> > Salahadin (Sala) Lotfi
> >
> > PhD Candidate of Cognitive Neuroscience
> >
> > University of Wisconsin-Milwaukee
> >
> > Anxiety Disorders Laboratory
> >
> > President, Association of Clinical and Cognitive Neuroscience, UWM
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Joachim Pr?l?, Prof. Dr. Blanche Schwappach-Pignataro, Marya Verdel
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING


From M@@rten@Jung @end|ng |rom m@||box@tu-dre@den@de  Sun May 24 11:56:40 2020
From: M@@rten@Jung @end|ng |rom m@||box@tu-dre@den@de (Maarten Jung)
Date: Sun, 24 May 2020 11:56:40 +0200
Subject: [R-sig-ME] lme approximation method for dfs
In-Reply-To: <cf770a5d8e494b9b9520a3b9d81dfb5b@MSX-L104.msx.ad.zih.tu-dresden.de>
References: <CAJTPX_XhATv7AvzLYyTfESsY8VGvBikapUD+Q9gkMqFQES9mEw@mail.gmail.com>
 <421276a8-6922-9f3f-2161-e5497ecbe3f0@mpi.nl>
 <CAHr4DydTGERuf4tzR-HQAB-kpRuwJXk6vNqs32cHB_kTWf3HSg@mail.gmail.com>
 <cf770a5d8e494b9b9520a3b9d81dfb5b@MSX-L104.msx.ad.zih.tu-dresden.de>
Message-ID: <CAHr4Dye26Ru6+zwk7MUGoAtTGpy7bMg7LX6N0J+ZchgtZZnBBQ@mail.gmail.com>

Dear Sala,

I *think* the containment method that is mentioned in the "Models
supported by emmeans" vignette is something similar to the containment
degrees of freedom approximation in SAS [1] but if you want to use
this method, you should ask the author of emmeans (Russell Lenth) to
make sure that my guess is correct.

I guess your emmeans() call throws an error because you are missing
the "specs" argument which specifies the predictor variables over
which the estimated marginal means should be calculated. Also, the
last argument is "sigmaAdjust" instead of "adjustSigma" but it
defaults to TRUE anyway. This should work (if the emmeans package is
loaded, otherwise use emmeans:emmeans()):
emmeans(lme_fit, ~ Group * TIME, mode = "appx-satterthwaite",
sigmaAdjust = TRUE)

The function joint_tests() which constructs Type-III-ANOVA-like tables
of all predictors and the function contrast() could be useful for your
analysis, too. See also the nice vignettes here [2].

[1]  https://documentation.sas.com/?docsetId=statug&docsetTarget=statug_glimmix_details38.htm&docsetVersion=15.1&locale=en
[2]  https://cran.r-project.org/web/packages/emmeans/vignettes/

Best,
Maarten


On Sun, May 24, 2020 at 4:55 AM Salahadin Lotfi
<salahadin.lotfi at gmail.com> wrote:
>
> Wow! Such a thorough answer, Phillip. Thanks very much.
> I misspoken when I said the lmer would generate p values for fixed effects (yes it is indeed lmerTest package which does it).
> Just to make sure I understand it correctly:
> lme uses ML/REML to approximate beta weights of fixed effects,
> it uses Wald to approximate t/z values (fixed effects), and
> it uses "inner-outer" rule to get denominator of df (ddf) which is apparently the least optimal method to estimate ddf among other methods (etc., Satterthwaite's). Lastly, is this "inner-outer" rule the same as "containment method"?
>
> Maarten,
> First of all thank you for the nice pointer. The emmeans is elegant. But, I could not get it to work. Could you point out what I am missing in the emmeans function?
>   Here is my code:
> ## Group and Time are categorical IVs with three levels.
> lme_fit<- lme(DV~  factor (Group) * factor(TIME), random = ~ TIME | SubjectID, # factor(Group) * factor(TIME)
>                   correlation = corSymm (form = ~ 1 | SubjectID),
>                   weights = varIdent (form = ~ 1 | TIME),
>                   data = my_data,
>                   method= "REML",
>                   na.action = "na.omit")
> emmeans::emmeans(lme.fit, mode= "appx-satterthwaite", adjustSigma = TRUE) ## NOT working
>
>
> I appreciate for taking time and replying.
> Sala
>
> *************
> Salahadin (Sala) Lotfi
>
> PhD Candidate of Cognitive Neuroscience
>
> University of Wisconsin-Milwaukee
>
> Anxiety Disorders Laboratory
>
> President, Association of Clinical and Cognitive Neuroscience, UWM
>
>
> On Sat, May 23, 2020 at 5:01 PM Maarten Jung <Maarten.Jung at mailbox.tu-dresden.de> wrote:
>>
>> Dear Sala,
>>
>> As Phillip Alday explained, there is no implementation of the Satterthwaite approximation in the nlme package. If you want to stick with this package, the only way I know to get something similar (for lme objects) is to use functions of the emmeans package with the argument "mode" set to "appx-satterthwaite" (see [1]).
>>
>> [1] https://cran.r-project.org/web/packages/emmeans/vignettes/models.html#K
>>
>> Best,
>> Maarten
>>
>>
>> On Sat, 23 May 2020, 17:07 Phillip Alday <phillip.alday at mpi.nl> wrote:
>>>
>>> Hi,
>>>
>>> On 23/5/20 9:11 am, Salahadin Lotfi wrote:
>>> > Dear all,
>>> > I have a very simple question but, have been having a hard time to figure
>>> > it out.
>>> > I am using a mixed model with random intercept and slope using lme function
>>> > with an unstructured covariance matrix. I know lmer uses Satterthwaite's
>>> > approximation method to approximate dfs of fixed effects,
>>>
>>> This is not accurate. lme4 by default doesn't even try to figure out the
>>> df and doesn't report p-values. The lmerTest package adds in options to
>>> use Satterthwaite or Kenward-Roger approximations for p-values, but
>>> depending on who you ask around here, the sentiment for those
>>> approximations ranges from "of course" to "hmrpf, why would you bother?"
>>> to "the heretics must be purged!".
>>>
>>> The GLMM FAQ (https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html)
>>> has some info on each of these, but I'll copy and paste something
>>> relevant that I wrote on a different mailing list:
>>>
>>> Treating the t-values as z-values is as reasonable as using the
>>> t-distribution with some estimated degrees of freedom for studies with
>>> 20-30 subjects and 10s of observations per condition per subject for two
>>> reasons. One is that a t-distribution with dozens of degrees of freedom
>>> is essentially a normal distribution, and so even if you could figure
>>> out what the "right" number of degrees of freedom were, it wouldn't be
>>> far off from the number you get from the normal distribution. The other
>>> reason is that none of these asymptotic results are guaranteed to be
>>> particularly great for anything other than very well behaved linear
>>> mixed models, which is why things like parametric bootstrap are the gold
>>> standard for figuring out coverage intervals. And for large models,
>>> bootstrapping is about as fast as KR (because KR as implemented in
>>> pbkrmodcomp, which lmerTest depends on, computes the inverse of a large
>>> n x matrix).
>>>
>>> > but I am not sure
>>> > what is the preferred method that lme uses. Is it Wald or Likelihood ratio?
>>>
>>> Wald and likelihood ratio are not degrees of freedom estimates. The
>>> likelihood-ratio tests do have a df, which corresponds to the difference
>>> in the number of free parameters between the models, but this not the
>>> relevant df. (It's numerator degrees of freedom in the ANOVA framework,
>>> while what you need are the denominator degrees of freedom.) The Wald
>>> tests are just the things you see in the table of the fixed effects,
>>> i.e. the tests corresponding to the t- or z-values (or more generally
>>> the ANOVA-style tests / tests of linear hypotheses you then construct
>>> from the fixed effects).
>>>
>>>
>>> > I don't think lme offers such an option to specify an approximation method
>>> > for dfs of fixed effects. Does it?
>>>
>>> The dfs in nlme are computed using the "inner-outer" rule which doesn't
>>> work well for many types of designs common in cognitive neuroscience.
>>> More information on this is in the GLMM FAQ, search for "Df
>>> alternatives" on that page.
>>>
>>>
>>> Hope that helps!
>>>
>>> Phillip
>>>
>>>
>>> >
>>> > I appreciate any response in advance.
>>> > Sala
>>> >
>>> >
>>> > *************
>>> > Salahadin (Sala) Lotfi
>>> >
>>> > PhD Candidate of Cognitive Neuroscience
>>> >
>>> > University of Wisconsin-Milwaukee
>>> >
>>> > Anxiety Disorders Laboratory
>>> >
>>> > President, Association of Clinical and Cognitive Neuroscience, UWM
>>> >
>>> >       [[alternative HTML version deleted]]
>>> >
>>> > _______________________________________________
>>> > R-sig-mixed-models at r-project.org mailing list
>>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From ph||||p@@|d@y @end|ng |rom mp|@n|  Sun May 24 23:15:39 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Sun, 24 May 2020 23:15:39 +0200
Subject: [R-sig-ME] 
 correlation of fixed effects coefficients all close to +/-1
In-Reply-To: <CA+6N3yVO+vQ-LoVn6-68sEqv4OnCbT2L62C4q8SP41276c6CbA@mail.gmail.com>
References: <CA+6N3yVO+vQ-LoVn6-68sEqv4OnCbT2L62C4q8SP41276c6CbA@mail.gmail.com>
Message-ID: <1e3db6d1-411c-22ff-1106-34328257d51b@mpi.nl>

Hi,

Very high correlations of the fixed-effects estimates can indicate two
problems (which are actually just different manifestations of the same
deeper problem):

1. Multicollinearity -- this is the same as multicollinearity in
classical/standard/non mixed-effects regression. Basically this means
that some of your variables are expressing the same thing and so you
have some redundancies that could be eliminated. Perfect
multicollinearity leads to a rank-deficient model matrix, which R will
catch and correct, but near multicollinearity may not be caught.

2. You don't have enough data to get good estimates of all your
coefficients.

The bigger problem for your inference is that both of these problems
will inflate your standard errors. In both cases, there isn't enough
information to full tease apart the contribution from the different
variables, which means that you have a lot of variability in your
estimates and thus large standard errors.

Note that some correlation between estimates is expected. If you think
of a very simple case with the intercept and one slope/predictor then
you'll see that if you change the intercept, then you have to change the
slope a bit to get the line to stay close to the observed data.

(Once again, I worry that I've oversimplified and said something
horribly infelicitous, but I'm always happy to be corrected and learn
something myself!)

Best,

Phillip

On 11/5/20 11:42 pm, Alessandra Bielli wrote:
> Dear list,
>
> I am fitting the mixed effect model:
>  > lmer(log(percapita_day) ~ Type_residuo + (1|boatID), data=all)
>
>  where percapita_day is a non-negative continuous response variable (on the
> log scale to have residuals normally distributed), Type_residuo is a
> categorical explanatory variable and boatID is a random effect with 4
> levels.
>
> I have found values very close to +/-1 in the correlation of fixed effects
> matrix below, and after some research I learnt that the coefficients are
> not about the correlation of the variables but the expected correlation of
> the regression coefficients.
>
> Correlation of Fixed Effects:
>             (Intr) Tp_rsM Tp_rsdOr Tp_rsdOt Tp_Pyc Tp_rsP Tp_rsR
> Type_rsdMtl -0.944
> Tp_rsdOrgnc -0.951  0.945
> Typ_rsdOtrs -0.959  0.953  0.959
> Tp_rsdPplyc -0.926  0.919  0.925    0.933
> Tp_rsdPlstc -0.951  0.945  0.951    0.958    0.925
> Type_resdRd -0.870  0.867  0.873    0.878    0.850  0.872
> Type_rsdVdr -0.954  0.949  0.955    0.962    0.928  0.954  0.876
>
> However I still can't explain why all coefficients are so close to +/-1 and
> I was wondering if these are indicators that something is wrong with my
> model?
> Is that due to the presence of outlayers in the response variable (see
> attached)?
>
> Thanks,
>
> Alessandra
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From ph||||p@@|d@y @end|ng |rom mp|@n|  Mon May 25 00:45:54 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Mon, 25 May 2020 00:45:54 +0200
Subject: [R-sig-ME] question on unit variance function
In-Reply-To: <CA+_DO+w_vHUqrwTsDwu-n7kTXtgZ-gZqs2Z1u72PGa9G-6Ncog@mail.gmail.com>
References: <CA+_DO+xpy0av0_vGG_QRzUrMXSxuk_m0Uh_uLt0my-PVe4gRcA@mail.gmail.com>
 <CA+_DO+w_vHUqrwTsDwu-n7kTXtgZ-gZqs2Z1u72PGa9G-6Ncog@mail.gmail.com>
Message-ID: <5d1fcb0e-941d-1a44-9e1d-3caf222de9f4@mpi.nl>

I haven't seen an answer go by yet, so I'll take a small stab. But
first: I don't see anything here related to *mixed* models, which is the
topic of this list; all of these questions seem to be related to more
foundational GLM / regression issues. You might have better luck on
CrossValidated (https://stats.stackexchange.com/).

On the second page of your second reference, there is a claim that the
beta distribution is not a member of the exponential family. This is
half accurate -- the beta distribution is an exponential distribution,
is not in the expoential dispersion family (see e.g.
https://stats.stackexchange.com/questions/435669/can-distributions-that-are-in-the-exponential-family-but-not-the-natural-expone
and
https://stats.stackexchange.com/questions/304538/why-beta-dirichlet-regression-are-not-considered-generalized-linear-models).
This means that it doesn't fit well with the techniques usually used in
the GLM framework (such as glm() and glmer() in R).

The variance of the binomial distribution is a function of its mean
(mu*(1-mu)), which is why the binomial distribution is parameterized
purely by the mean (and number of observations)? and there is no
"residual variance" in a binomial model.

The beta distribution is parameterized either by two shape parameters or
by the mean and precision (which is like variance a scale parameter),
see e.g. the abbreviated discussion here:
https://cran.r-project.org/web/packages/brms/vignettes/brms_customfamilies.html#the-beta-binomial-distribution.
These two very different parameterizations hint at some of the problems
with working with the beta distribution.

For all of these things, the canonical reference is McCullagh and
Nelder's Generalized Linear Models.

That doesn't answer your question directly, but hopefully it gives you
enough to continue your search.

Phillip

On 30/4/20 5:33 pm, HATICE T KUBRA AKDUR wrote:
> Dear Group Members,
>
> I hope you are keeping well during this difficult time.
> Thank you in advance.
> I have some questions below:
>
> In the beta regression model, it is stated V(\mu)= \mu(1-\mu).
>
> It is known that even though beta distribution is in the exponential
> family, logit link is not canonical link for beta regression.
>
> And, V(\mu)= \mu(1-\mu) is unit variance function for binomial
> distribution, logit link is canonical for binomial distribution.
>
>  I am not sure,  V(mu)=mu(1-mu), is it still unit variance function for
> beta regression model with logit link.  Even though, it is not in natural
> exponential family. OR we can say that V(\mu)= \mu(1-\mu) is variance
> function for beta regression with logit link.
>
> 1/g'(mu) (g(mu) is link function) can be used to obtain unit variance
> function for not natural exponential family distribution? or it is only
> true for natural exponential family?
>
> For example, unit variance function of Simplex distribution is
> V(mu)=mu_3(1-mu)_3
> because simplex distribution is in dispersion family. So, to obtain unit
> variance function, the second order derivative of the deviance function is
> used.
>
> I will be very glad, if you enlighten me.
>
> Best,
>
> References:
>
> https://onlinelibrary.wiley.com/doi/abs/10.1111/j.0006-341X.2000.00496.x
>
> https://www.tandfonline.com/doi/pdf/10.1080/0266476042000214501
>
> On Thu, Apr 30, 2020 at 11:10 AM HATICE T KUBRA AKDUR <hkubrasenol at gmail.com>
> wrote:
>
>> Dear Group Members,
>>
>> I hope you are keeping well during this difficult time.
>> Thank you in advance.
>> I have some questions below:
>>
>> In the beta regression model, it is stated V(\mu)= \mu(1-\mu).
>>
>> It is known that even though beta distribution is in the exponential
>> family, logit link is not canonical link for beta regression.
>>
>> And, V(\mu)= \mu(1-\mu) is unit variance function for binomial
>> distribution, logit link is canonical for binomial distribution.
>>
>>  I am not sure,  V(mu)=mu(1-mu), is it still unit variance function for
>> beta regression model with logit link.  Even though, it is not in natural
>> exponential family. OR we can say that V(\mu)= \mu(1-\mu) is variance
>> function for beta regression with logit link.
>>
>> 1/g'(mu) (g(mu) is link function) can be used to obtain unit variance
>> function for not natural exponential family distribution? or it is only
>> true for natural exponential family?
>>
>> For example, unit variance function of Simplex distribution is V(mu)=mu_3(1-mu)_3
>> because simplex distribution is in dispersion family. So, to obtain unit
>> variance function, the second order derivative of the deviance function
>> is used.
>>
>> I will be very glad, if you enlighten me.
>> I attached beta regression and marginal simplex model papers FYI.
>>
>> Best Regards,
>> --
>> Asst. Prof. Dr. Hatice T. Kubra AKDUR
>> Department of Statistics, Faculty of Science
>> Gazi University
>> 06500 Teknikokullar ANKARA, TURKEY
>> Phone: +90 553 324 5380
>> Email: hatice_senol at wsu.edu
>> Homepage: http://websitem.gazi.edu.tr/site/haticesenol
>>
>


From ph||||p@@|d@y @end|ng |rom mp|@n|  Mon May 25 00:58:54 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Mon, 25 May 2020 00:58:54 +0200
Subject: [R-sig-ME] Computing pair-wise associations of fixed effects in
 gLMM
In-Reply-To: <86ac6113d19846179cf1fe193a35652e@unige.ch>
References: <86ac6113d19846179cf1fe193a35652e@unige.ch>
Message-ID: <d5d059f7-1e61-c9a0-d282-da2a10601150@mpi.nl>

I'm a bit confused by your question -- you "suddenly" introduce multiple
response variables but don't describe what they represent. This is just
as important as describing your predictors! Otherwise we have no way of
knowing where e.g. the Poisson distribution is a reasonable assumption.

Also, note that really shouldn't test normality of your response
variable for two reasons. First, as the size of your data increases, it
becomes easier and easier to reject the null hypothesis of normality for
trivial reasons. No real data is perfectly normally distributed, even
data generated from a normal distribution and so the test rejects more
and more data that really would be fine. Second, it's not the "absolute"
(or more precisely, marginal) distribution of your data that matters,
but rather the distribution of the residuals (or equivalently, the
conditional distribution).

I'm also not clear what you mean with pairwise correlation of
categorical predictors -- do you mean the correlation of fixed-effects
estimates? lme4 will give you that information, but I don't know if
that's what you're looking for. Are you looking for how much the effects
of the different (levels of the different) factors correlate with each
other?

That doesn't yet help you that much, but if you clarify a bit, maybe we
can help you more! :)

Best,
Phillip

On 5/5/20 1:47 pm, Julian Gaviria Lopez wrote:
> Dear list members.
>
>
> I have a nested data comprised by 2 factors (conditions: A and B). Each factor has 8 levels: (clusters: c1,c2,c3,c4,c5,c6,c7,c8). N=33. Aim: To assess the pairwise association between the factors (i.e. correlation between Ac1 and   Bc1, etc.).  Although an LMM will count for the dependent nature of the data (repeated measures of the 33 participants observed in condition A, and consecutively in B), some of the  dependent variables are not normally distributed (7 out of 16) according to the shapiro test.  For this reason, I think a gLMM might be a good option:
>
>
> M <-glmer(observation~condition+cluster+(1|subject),data=mDATA,family="poisson")
>
>
> Questions:
>
>
> 1) Would anyone is aware of a better option regarding the modelling method?
>
> 2) In case gLMM is the "right" way to go, I wonder how could I compute the pairwise correlations of the "fixed effects" (e.g. Ac1-Bc1;  Ac1-Bc2; ... Ac1-Bc8), with "glmer" function, or maybe with the glmmTMB?
>
>
>
> Thanks in advance
>
>
>
> Julian Gaviria
> Neurology and Imaging of cognition lab (Labnic)
> University of Geneva. Campus Biotech.
> 9 Chemin des Mines, 1202 Geneva, CH
> Tel: +41 22 379 0380
> Email: Julian.GaviriaLopez at unige.ch
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From ph||||p@@|d@y @end|ng |rom mp|@n|  Mon May 25 01:06:45 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Mon, 25 May 2020 01:06:45 +0200
Subject: [R-sig-ME] 
 confidence intervals for interpolated values in logistic regression
In-Reply-To: <CALC46t9MRcfR-kMfKeu2ix-gHnQHgz-mtOaZpOXg+jKkY76xUg@mail.gmail.com>
References: <CALC46t9MRcfR-kMfKeu2ix-gHnQHgz-mtOaZpOXg+jKkY76xUg@mail.gmail.com>
Message-ID: <773ec509-f363-6849-1409-b66d72aff8ab@mpi.nl>

This isn't a mixed-models issue, so it's not quite on-topic for the
list, but I'll go ahead and give a few hints:

1. Don't do the linear algebra yourself -- use predict(). This is
especially true for GAMs where you need to worry about the smoother
terms (and where the necessary matrices for the linear algebra isn't
immediately obvious from the model summaries). (Also,l you mention GAMs,
but then you don't mention any smoothers .... )

2. I think the functionality you're looking for is more or less the
effects package.

3. There is some fine print on that though: there are confidence
intervals (which summarize your model and its uncertainty and are what
are shown in effects plots) and prediction intervals (which show how
much variability you would expect in new data -- and this is more than
the confidence intervals, which summarize the uncertainty in your
parameters, not total variability).

4. mgcv may have a relevant parametric boostrap method, but I don't
think is what you're looking for. mgcv does have some nice plotting
methods built-in though in addition to the methods in the effects package.

Best,
Phillip

On 14/4/20 4:29 pm, David Villegas R?os wrote:
> Dear list,
> I?m running a gam model (package mgcv) with a binary response variable (y),
> and two continuous explanatory variables (x and z), plus their interaction
> (x:z). I, therefore, obtain four coefficients from my model (intercept,
> slope of x, slope of z and interaction coefficient).
>
> I?m interested in obtaining the value of one of the explanatory variables
> (x) for a particular level of the response variable, i.e. for a particular
> probability level, and after fixing the value of the other explanatory
> variable (z). Doing simple arithmetic, I can obtain the value of x that I?m
> looking for, but I wonder how I can obtain a measure of error such a
> confidence interval, so I can compare that value obtained from other
> analogous models.
>
> Is bootstrapping a good option or are there better alternatives? Any
> practical advice/library to do so?
>
> Thanks in advance,
> David
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From ph||||p@@|d@y @end|ng |rom mp|@n|  Mon May 25 01:15:44 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Mon, 25 May 2020 01:15:44 +0200
Subject: [R-sig-ME] Multilevel zero-inflated models: model selection and
 model assumption checks
In-Reply-To: <CAGXFLef8NR-ioUw6i1a+U5cDAG4Uz6ZigNOpK9xA-WtNsxcP=g@mail.gmail.com>
References: <CAGXFLeexSGWnFNC7YgmNe3d4hY_uv7N-jft-0f0=AHZVSGm_bw@mail.gmail.com>
 <CAGXFLef8NR-ioUw6i1a+U5cDAG4Uz6ZigNOpK9xA-WtNsxcP=g@mail.gmail.com>
Message-ID: <1515e268-f5f7-9218-bc62-fc65f5e33a07@mpi.nl>

I won't address everything, but will give a few short pointers

On 28/4/20 1:58 am, Kate R wrote:
> Thank you in advance for reading my long-winded email! I appreciate any
> help and guidance.
>
> I am working on research that involves frequency of events (summed during
> an hour) and duration of events (calculated a seconds during an hour). Both
> data types have 0s and are positively skewed. The observations (n =
> 14,000+) are collected hourly for up to 2 weeks for 500 participants. A
> simple version of the model is: outcome ~ explanatory +
> (1|participant/date).
>
> For the frequency data, I have fit:
>
>    - (Hurdle) Poisson / Negative Binomial (the var > mean)
>    - (Hurdle) Gamma (log link) (assuming okay to treat frequency data as
>    continuous)
>
> The AIC is best for the hurdle negative binomial model. I understand that
> for AIC comparisons, the Poisson models do not get an extra scale
> parameter, while the other models. However, the difference in AICs are in
> the hundreds, so I imagine the small difference in k parameters would not
> change interpretation, as long as the log-likelihood functions are
> similar...
>
> (Q1) Does the above sound reasonable?
>
> For the duration data, I have fit:
>
>    - (Hurdle) Gamma (log link)
>    - Zero-Inflated Beta (after transforming duration by SECONDS/3600. I
>    decided to try Beta because the hourly data is bounded at 3600 seconds and
>    I was not sure if an upper bound affects the gamma distribution).
>
> (Q2) Does an upper bound mean make it inappropriate to fit a Gamma
> distribution?


For this type of thing, my advise is always "plot the model against the
data!". You can use an effects-type plot overlaid with the data or you
can plot both the fitted and observed values against the predictors (in
addition to the classical fitted vs. observed plot). Which models
capture which aspects of your data? Which models break down in
unacceptable ways? As George Box said, "all models are wrong, but some
models are useful." Which models are most useful (i.e. wrong in ways you
can accept and right in ways you need)?

> N.B. If fitting regular gamma/beta, I first transformed data to remove 0s.
> For both the regular and zero-inflated beta, I shrunk the 1s using EITHER
> the algorithm here: https://www.ncbi.nlm.nih.gov/pubmed/16594767 OR the
> inverse hyperbolic sine transformation (IHST)). I had thought about using
> ZOIB, but I did not want to use BRMS or GAMLSS (since I am a beginner).
>
> Because I transformed the data in different ways for the different models,
> I understand that I cannot compare the AICs. Therefore, I thought about
> using cross fold validation. However, the package I was recommended to use
> (cvms https://github.com/LudvigOlsen/cvms) says it supports lmer/glmer, but
> doesn't appear to support glmmTMB, which is how I fit the above models.
>
> I thought a potential solution might be to fit two separates models
> (binomial for 0/1 data and count/continuous for positive data, as suggested
> in this post: assessing glmmTMB hurdle model fit using DHARMa scaled
> residual plot
> <https://stats.stackexchange.com/questions/400147/assessing-glmmtmb-hurdle-model-fit-using-dharma-scaled-residual-plot>).
> In this way, I could fit lmer/glmer, and perform cross fold validation
> using cvms on the positive data. But I'm not sure if this makes sense (to
> partition out the 0 data, and only use the positive data to validate the
> models)? I suppose I could simulate data to check predictive accuracy on
> the glmmTMB models? Is this a reasonable method of comparing the models?

This is in some sense exactly what a hurdle model does. :)

>
> (Q3) How would you recommend comparing the models with different
> distributions and family transformations?

With difficulty if you want classical tests (they aren't nested so you
can't do likelihood-ratio tests and I find even AIC/BIC less than ideal
here because the different link functions means that you're not quite
comparing the same aspects of the data). You could use stratified
cross-validation to get about predictive power, but this can be
challenging depending on the exact nesting/crossing structure of your
data (sorry, I didn't have time to read and think about your particular
data in detail).? Otherwise, I recommend the graphical comparison I
mentioned above.

Hope that helps,
Phillip


>
> Additionally, I considered fitting a Tweedie distribution to both the
> frequency and duration. However, I was having trouble finding guidance on
> how to interpret the beta coefficients.
>
> (Q4) How does one interpret Tweedie beta coefficients? Do you exponentiate
> and discuss as rates?
>
> Finally, I have used the performance package to check the model
> assumptions. The results can be found at this link:
>
> <
> https://docs.google.com/document/d/1s6qtSAvw297F_cvblAq7Gh2fWgFmz9I5Nxi1K2gjScI/edit?usp=sharing
> Pic 1 and 2 are both for the Hurdle Gamma on Duration, but have different
> explanatory variables. The pics are pretty similar to the results for the
> other models/distributions. I have been reading conflicting advice as to
> whether the residuals for GLM models need to be normally distributed, etc.
> I understand for large datasets, non-normality may be a non-issue even for
> linear regression. We are not concerned with predicting new data, but
> explaining the data that we have. Therefore, given the size of my sample
> and the distributions. how concerned should I be about the QQ plots and
> homogeneity of variance plots below?
>
> Thank you very much!
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From M@@rten@Jung @end|ng |rom m@||box@tu-dre@den@de  Mon May 25 10:40:59 2020
From: M@@rten@Jung @end|ng |rom m@||box@tu-dre@den@de (Maarten Jung)
Date: Mon, 25 May 2020 10:40:59 +0200
Subject: [R-sig-ME] lme approximation method for dfs
In-Reply-To: <6495710cfbff426098246322066c8ad8@MSX-L104.msx.ad.zih.tu-dresden.de>
References: <CAJTPX_XhATv7AvzLYyTfESsY8VGvBikapUD+Q9gkMqFQES9mEw@mail.gmail.com>
 <421276a8-6922-9f3f-2161-e5497ecbe3f0@mpi.nl>
 <CAHr4DydTGERuf4tzR-HQAB-kpRuwJXk6vNqs32cHB_kTWf3HSg@mail.gmail.com>
 <cf770a5d8e494b9b9520a3b9d81dfb5b@MSX-L104.msx.ad.zih.tu-dresden.de>
 <CAHr4Dye26Ru6+zwk7MUGoAtTGpy7bMg7LX6N0J+ZchgtZZnBBQ@mail.gmail.com>
 <6495710cfbff426098246322066c8ad8@MSX-L104.msx.ad.zih.tu-dresden.de>
Message-ID: <CAHr4DydVWxQc6m2DKxrj3bqamCwPXTOKBq51jQR1nCZ=QTBRUA@mail.gmail.com>

Hi Sala,

I'm glad I could help.
Please keep the mailing list in cc.

Just a small addition to my last mail: If you call joint_tests()
directly on your model (and not on an emmGrid object which is returned
by the functions emmeans() or ref_grid()), you also have to pass the
"mode" argument, otherwise you won't get the degrees of freedom you
want (i.e., use joint_tests(lme_fit, mode = "appx-satterthwaite") and
not just joint_tests(lme_fit)).

Best,
Maarten



On Mon, May 25, 2020 at 7:09 AM Salahadin Lotfi
<salahadin.lotfi at gmail.com> wrote:
>
> Maarten,
>
> Both suggestions were great. I used them and they are perfectly working.
>
> Sala
>
> On Sun, May 24, 2020 at 4:57 AM Maarten Jung <Maarten.Jung at mailbox.tu-dresden.de> wrote:
>>
>> Dear Sala,
>>
>> I *think* the containment method that is mentioned in the "Models
>> supported by emmeans" vignette is something similar to the containment
>> degrees of freedom approximation in SAS [1] but if you want to use
>> this method, you should ask the author of emmeans (Russell Lenth) to
>> make sure that my guess is correct.
>>
>> I guess your emmeans() call throws an error because you are missing
>> the "specs" argument which specifies the predictor variables over
>> which the estimated marginal means should be calculated. Also, the
>> last argument is "sigmaAdjust" instead of "adjustSigma" but it
>> defaults to TRUE anyway. This should work (if the emmeans package is
>> loaded, otherwise use emmeans:emmeans()):
>> emmeans(lme_fit, ~ Group * TIME, mode = "appx-satterthwaite",
>> sigmaAdjust = TRUE)
>>
>> The function joint_tests() which constructs Type-III-ANOVA-like tables
>> of all predictors and the function contrast() could be useful for your
>> analysis, too. See also the nice vignettes here [2].
>>
>> [1]  https://documentation.sas.com/?docsetId=statug&docsetTarget=statug_glimmix_details38.htm&docsetVersion=15.1&locale=en
>> [2]  https://cran.r-project.org/web/packages/emmeans/vignettes/
>>
>> Best,
>> Maarten
>>
>>
>> On Sun, May 24, 2020 at 4:55 AM Salahadin Lotfi
>> <salahadin.lotfi at gmail.com> wrote:
>> >
>> > Wow! Such a thorough answer, Phillip. Thanks very much.
>> > I misspoken when I said the lmer would generate p values for fixed effects (yes it is indeed lmerTest package which does it).
>> > Just to make sure I understand it correctly:
>> > lme uses ML/REML to approximate beta weights of fixed effects,
>> > it uses Wald to approximate t/z values (fixed effects), and
>> > it uses "inner-outer" rule to get denominator of df (ddf) which is apparently the least optimal method to estimate ddf among other methods (etc., Satterthwaite's). Lastly, is this "inner-outer" rule the same as "containment method"?
>> >
>> > Maarten,
>> > First of all thank you for the nice pointer. The emmeans is elegant. But, I could not get it to work. Could you point out what I am missing in the emmeans function?
>> >   Here is my code:
>> > ## Group and Time are categorical IVs with three levels.
>> > lme_fit<- lme(DV~  factor (Group) * factor(TIME), random = ~ TIME | SubjectID, # factor(Group) * factor(TIME)
>> >                   correlation = corSymm (form = ~ 1 | SubjectID),
>> >                   weights = varIdent (form = ~ 1 | TIME),
>> >                   data = my_data,
>> >                   method= "REML",
>> >                   na.action = "na.omit")
>> > emmeans::emmeans(lme.fit, mode= "appx-satterthwaite", adjustSigma = TRUE) ## NOT working
>> >
>> >
>> > I appreciate for taking time and replying.
>> > Sala
>> >
>> > *************
>> > Salahadin (Sala) Lotfi
>> >
>> > PhD Candidate of Cognitive Neuroscience
>> >
>> > University of Wisconsin-Milwaukee
>> >
>> > Anxiety Disorders Laboratory
>> >
>> > President, Association of Clinical and Cognitive Neuroscience, UWM
>> >
>> >
>> > On Sat, May 23, 2020 at 5:01 PM Maarten Jung <Maarten.Jung at mailbox.tu-dresden.de> wrote:
>> >>
>> >> Dear Sala,
>> >>
>> >> As Phillip Alday explained, there is no implementation of the Satterthwaite approximation in the nlme package. If you want to stick with this package, the only way I know to get something similar (for lme objects) is to use functions of the emmeans package with the argument "mode" set to "appx-satterthwaite" (see [1]).
>> >>
>> >> [1] https://cran.r-project.org/web/packages/emmeans/vignettes/models.html#K
>> >>
>> >> Best,
>> >> Maarten
>> >>
>> >>
>> >> On Sat, 23 May 2020, 17:07 Phillip Alday <phillip.alday at mpi.nl> wrote:
>> >>>
>> >>> Hi,
>> >>>
>> >>> On 23/5/20 9:11 am, Salahadin Lotfi wrote:
>> >>> > Dear all,
>> >>> > I have a very simple question but, have been having a hard time to figure
>> >>> > it out.
>> >>> > I am using a mixed model with random intercept and slope using lme function
>> >>> > with an unstructured covariance matrix. I know lmer uses Satterthwaite's
>> >>> > approximation method to approximate dfs of fixed effects,
>> >>>
>> >>> This is not accurate. lme4 by default doesn't even try to figure out the
>> >>> df and doesn't report p-values. The lmerTest package adds in options to
>> >>> use Satterthwaite or Kenward-Roger approximations for p-values, but
>> >>> depending on who you ask around here, the sentiment for those
>> >>> approximations ranges from "of course" to "hmrpf, why would you bother?"
>> >>> to "the heretics must be purged!".
>> >>>
>> >>> The GLMM FAQ (https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html)
>> >>> has some info on each of these, but I'll copy and paste something
>> >>> relevant that I wrote on a different mailing list:
>> >>>
>> >>> Treating the t-values as z-values is as reasonable as using the
>> >>> t-distribution with some estimated degrees of freedom for studies with
>> >>> 20-30 subjects and 10s of observations per condition per subject for two
>> >>> reasons. One is that a t-distribution with dozens of degrees of freedom
>> >>> is essentially a normal distribution, and so even if you could figure
>> >>> out what the "right" number of degrees of freedom were, it wouldn't be
>> >>> far off from the number you get from the normal distribution. The other
>> >>> reason is that none of these asymptotic results are guaranteed to be
>> >>> particularly great for anything other than very well behaved linear
>> >>> mixed models, which is why things like parametric bootstrap are the gold
>> >>> standard for figuring out coverage intervals. And for large models,
>> >>> bootstrapping is about as fast as KR (because KR as implemented in
>> >>> pbkrmodcomp, which lmerTest depends on, computes the inverse of a large
>> >>> n x matrix).
>> >>>
>> >>> > but I am not sure
>> >>> > what is the preferred method that lme uses. Is it Wald or Likelihood ratio?
>> >>>
>> >>> Wald and likelihood ratio are not degrees of freedom estimates. The
>> >>> likelihood-ratio tests do have a df, which corresponds to the difference
>> >>> in the number of free parameters between the models, but this not the
>> >>> relevant df. (It's numerator degrees of freedom in the ANOVA framework,
>> >>> while what you need are the denominator degrees of freedom.) The Wald
>> >>> tests are just the things you see in the table of the fixed effects,
>> >>> i.e. the tests corresponding to the t- or z-values (or more generally
>> >>> the ANOVA-style tests / tests of linear hypotheses you then construct
>> >>> from the fixed effects).
>> >>>
>> >>>
>> >>> > I don't think lme offers such an option to specify an approximation method
>> >>> > for dfs of fixed effects. Does it?
>> >>>
>> >>> The dfs in nlme are computed using the "inner-outer" rule which doesn't
>> >>> work well for many types of designs common in cognitive neuroscience.
>> >>> More information on this is in the GLMM FAQ, search for "Df
>> >>> alternatives" on that page.
>> >>>
>> >>>
>> >>> Hope that helps!
>> >>>
>> >>> Phillip
>> >>>
>> >>>
>> >>> >
>> >>> > I appreciate any response in advance.
>> >>> > Sala
>> >>> >
>> >>> >
>> >>> > *************
>> >>> > Salahadin (Sala) Lotfi
>> >>> >
>> >>> > PhD Candidate of Cognitive Neuroscience
>> >>> >
>> >>> > University of Wisconsin-Milwaukee
>> >>> >
>> >>> > Anxiety Disorders Laboratory
>> >>> >
>> >>> > President, Association of Clinical and Cognitive Neuroscience, UWM
>> >>> >
>> >>> >       [[alternative HTML version deleted]]
>> >>> >
>> >>> > _______________________________________________
>> >>> > R-sig-mixed-models at r-project.org mailing list
>> >>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >>>
>> >>> _______________________________________________
>> >>> R-sig-mixed-models at r-project.org mailing list
>> >>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From x|nx|813 @end|ng |rom 126@com  Mon May 25 11:43:42 2020
From: x|nx|813 @end|ng |rom 126@com (=?gb18030?B?WUE=?=)
Date: Mon, 25 May 2020 17:43:42 +0800
Subject: [R-sig-ME] questions on MCMCglmm
Message-ID: <tencent_6108C57938AF2DDD8036849507C60B312405@qq.com>

Dear list,


I am using MCMCglmm package in R for my multilevel multinomial logistic regression model. I have a three category level1 unordered multinomial outcome 'nomial', which was coded as 0,1,2, a level1 continuous predictor 'IT1', and a level2 continuous predictor 'age', the ID variable is 'ID'. My questions are:


1. The regression coeffecient of IT1 is 0.120220, but which category of 'nomial' does the coefficient concern? how to interpret it? 

2.  Although I have got the model run properly, I dont quite understand  what the 'rcov = ~us(trait):units' do in the model, only that the model  cant run without this part. I have read the coursenote on the website  several times, but still have not figured it out. 

3. What  does 'us','idh', 'ihv' stands for (the author uses them to refer to  covariance structure, are they initials of some terminologies)? 

4. I tried to make predictions with IT1=3 and age=23, I got error suggesting "object 'nomial' not found", but 'nomial' is exactly what I am trying to predict. How to understand the logic behind this?



 So the model fit and prediction are as below:


&gt; m10=MCMCglmm(as.factor(nomial)~IT1+age,random=~ID,data=dat,rcov = ~us(trait):units,family='categorical') 
&gt; summary(m10)

&nbsp;Iterations = 3001:12991
&nbsp;Thinning interval&nbsp; = 10
&nbsp;Sample size&nbsp; = 1000 

&nbsp;DIC: 358.2906 

&nbsp;G-structure:&nbsp; ~ID

&nbsp;&nbsp; post.mean l-95% CI u-95% CI eff.samp
ID&nbsp;&nbsp; &nbsp; 28.75&nbsp; &nbsp; 18.64&nbsp; &nbsp; 38.81&nbsp; &nbsp; 767.4

&nbsp;R-structure:&nbsp; ~us(trait):units

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; post.mean l-95% CI u-95% CI eff.samp
traitnomial.1:traitnomial.1.units &nbsp; 0.04237&nbsp; 0.01597&nbsp; 0.13210&nbsp; &nbsp; 10.77
traitnomial.2:traitnomial.1.units&nbsp; -0.21941 -0.39132 -0.07699&nbsp;&nbsp; &nbsp; 6.04
traitnomial.1:traitnomial.2.units&nbsp; -0.21941 -0.39132 -0.07699&nbsp;&nbsp; &nbsp; 6.04
traitnomial.2:traitnomial.2.units &nbsp; 1.69265&nbsp; 1.21517&nbsp; 2.21141&nbsp; &nbsp; 23.81

&nbsp;Location effects: as.factor(nomial) ~ IT1 + age 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; post.mean &nbsp; l-95% CI &nbsp; u-95% CI eff.samp pMCMC&nbsp; &nbsp;
(Intercept)&nbsp; 34.541008 -15.319976&nbsp; 78.613352&nbsp; 931.637 0.120&nbsp; &nbsp;
IT1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; 0.120220 &nbsp; 0.009121 &nbsp; 0.343275&nbsp; &nbsp; 6.204 0.004 **
age&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; -1.404272&nbsp; -3.267662 &nbsp; 0.659993&nbsp; 931.605 0.130&nbsp; &nbsp;
---
Signif. codes:&nbsp; 0 ??***?? 0.001 ??**?? 0.01 ??*?? 0.05 ??.?? 0.1 ?? ?? 1


&gt; predict.MCMCglmm(m10,data.frame(IT1=3,age=23),type='response')
Error in eval(inp, data, env) : object 'nomial' not found


Thank you very much.


Best regards,


YA
	[[alternative HTML version deleted]]


From b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com  Mon May 25 18:52:11 2020
From: b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com (Alessandra Bielli)
Date: Mon, 25 May 2020 10:52:11 -0600
Subject: [R-sig-ME] 
 correlation of fixed effects coefficients all close to +/-1
In-Reply-To: <1e3db6d1-411c-22ff-1106-34328257d51b@mpi.nl>
References: <CA+6N3yVO+vQ-LoVn6-68sEqv4OnCbT2L62C4q8SP41276c6CbA@mail.gmail.com>
 <1e3db6d1-411c-22ff-1106-34328257d51b@mpi.nl>
Message-ID: <CA+6N3yUEncW9-RxTNAEwMRA6qd132zRXJ_XiBXQZPVJiA1FaXQ@mail.gmail.com>

Hi Phillip

Thank you so much for your explanation.

I have a couple more questions

1.In my model, the regression coefficients of each one of the categories of
my predictor are correlated, but I just have one categorical predictor.  In
case of collinearity I would usually drop one predictor, but here I only
have one and my goal is to use the model to predict the dependent variable.
What's the procedure here?

2. Is there a test or visual way to determine if I have enough data to get
good estimates?

3. A couple days ago I came across this post on Cross validated that states
that the correlation of fixed effect part of the outpout is only useful in
special cases,
https://stats.stackexchange.com/questions/57240/how-do-i-interpret-the-correlations-of-fixed-effects-in-my-glmer-output.
The post references the book
http://www.sfs.uni-tuebingen.de/~hbaayen/publications/baayenCUPstats.pdf,
page 268,

"The summary concludes with a table listing the correlations of the fixed
effects. The numbers listed here can be used to construct confidence
elipses for pairs of fixed-effects parameters, and should not be confused
with the normal correlation obtained by applying cor() to pairs of
predictor vectors in the input data. Since constructing confidence ellipses
is beyond the scope of the book we will often suppress this table".

What I understand is that the correlation matrix is useful for prediction
of future values, which is also my case, but I am not entirely sure I am
interpreting this correctly.

I really appreciate your advice!

Alessandra


On Sun, May 24, 2020 at 3:15 PM Phillip Alday <phillip.alday at mpi.nl> wrote:

> Hi,
>
> Very high correlations of the fixed-effects estimates can indicate two
> problems (which are actually just different manifestations of the same
> deeper problem):
>
> 1. Multicollinearity -- this is the same as multicollinearity in
> classical/standard/non mixed-effects regression. Basically this means
> that some of your variables are expressing the same thing and so you
> have some redundancies that could be eliminated. Perfect
> multicollinearity leads to a rank-deficient model matrix, which R will
> catch and correct, but near multicollinearity may not be caught.
>
> 2. You don't have enough data to get good estimates of all your
> coefficients.
>
> The bigger problem for your inference is that both of these problems
> will inflate your standard errors. In both cases, there isn't enough
> information to full tease apart the contribution from the different
> variables, which means that you have a lot of variability in your
> estimates and thus large standard errors.
>
> Note that some correlation between estimates is expected. If you think
> of a very simple case with the intercept and one slope/predictor then
> you'll see that if you change the intercept, then you have to change the
> slope a bit to get the line to stay close to the observed data.
>
> (Once again, I worry that I've oversimplified and said something
> horribly infelicitous, but I'm always happy to be corrected and learn
> something myself!)
>
> Best,
>
> Phillip
>
> On 11/5/20 11:42 pm, Alessandra Bielli wrote:
> > Dear list,
> >
> > I am fitting the mixed effect model:
> >  > lmer(log(percapita_day) ~ Type_residuo + (1|boatID), data=all)
> >
> >  where percapita_day is a non-negative continuous response variable (on
> the
> > log scale to have residuals normally distributed), Type_residuo is a
> > categorical explanatory variable and boatID is a random effect with 4
> > levels.
> >
> > I have found values very close to +/-1 in the correlation of fixed
> effects
> > matrix below, and after some research I learnt that the coefficients are
> > not about the correlation of the variables but the expected correlation
> of
> > the regression coefficients.
> >
> > Correlation of Fixed Effects:
> >             (Intr) Tp_rsM Tp_rsdOr Tp_rsdOt Tp_Pyc Tp_rsP Tp_rsR
> > Type_rsdMtl -0.944
> > Tp_rsdOrgnc -0.951  0.945
> > Typ_rsdOtrs -0.959  0.953  0.959
> > Tp_rsdPplyc -0.926  0.919  0.925    0.933
> > Tp_rsdPlstc -0.951  0.945  0.951    0.958    0.925
> > Type_resdRd -0.870  0.867  0.873    0.878    0.850  0.872
> > Type_rsdVdr -0.954  0.949  0.955    0.962    0.928  0.954  0.876
> >
> > However I still can't explain why all coefficients are so close to +/-1
> and
> > I was wondering if these are indicators that something is wrong with my
> > model?
> > Is that due to the presence of outlayers in the response variable (see
> > attached)?
> >
> > Thanks,
> >
> > Alessandra
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>

	[[alternative HTML version deleted]]


From b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com  Mon May 25 19:12:15 2020
From: b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com (Alessandra Bielli)
Date: Mon, 25 May 2020 11:12:15 -0600
Subject: [R-sig-ME] 
 correlation of fixed effects coefficients all close to +/-1
In-Reply-To: <CA+6N3yUEncW9-RxTNAEwMRA6qd132zRXJ_XiBXQZPVJiA1FaXQ@mail.gmail.com>
References: <CA+6N3yVO+vQ-LoVn6-68sEqv4OnCbT2L62C4q8SP41276c6CbA@mail.gmail.com>
 <1e3db6d1-411c-22ff-1106-34328257d51b@mpi.nl>
 <CA+6N3yUEncW9-RxTNAEwMRA6qd132zRXJ_XiBXQZPVJiA1FaXQ@mail.gmail.com>
Message-ID: <CA+6N3yVk6q+zDQ=Rx8QHvKMvR-bV470dzVqHC9aFQ7yy7XfAXg@mail.gmail.com>

UPDATE

Dear Phillip and list

As you can see from the graph attached, one of the categories of the
predictor variable ("madera") only has one observation.
I decided to remove this observation and I ran the model again, this is the
corr matrix I get:

Correlation of Fixed Effects:
            (Intr) Tp_rsdOr Tp_rsdOt Tp_Pyc Tp_rsP Tp_rsR
Tp_rsdOrgnc -0.725
Tip_rsdOtrs -0.747  0.593
Tp_rsdPplyc -0.575  0.458    0.470
Tp_rsdPlstc -0.659  0.526    0.542    0.419
Tipo_resdRd -0.445  0.356    0.367    0.282  0.328
Tipo_rsdVdr -0.747  0.593    0.612    0.470  0.542  0.367

I am aware that modifying a dataset is unacceptable, but I think it showed
that the source of the problem was lack of observations, am I correct?
Is there a better way to deal with this? I would rather not delete a line
of my dataset, even though it is a very uncommon observation for which I do
not aim to get predictions.

Thank you again for your advice


On Mon, May 25, 2020 at 10:52 AM Alessandra Bielli <
bielli.alessandra at gmail.com> wrote:

> Hi Phillip
>
> Thank you so much for your explanation.
>
> I have a couple more questions
>
> 1.In my model, the regression coefficients of each one of the categories
> of my predictor are correlated, but I just have one categorical predictor.
> In case of collinearity I would usually drop one predictor, but here I only
> have one and my goal is to use the model to predict the dependent variable.
> What's the procedure here?
>
> 2. Is there a test or visual way to determine if I have enough data to get
> good estimates?
>
> 3. A couple days ago I came across this post on Cross validated that
> states that the correlation of fixed effect part of the outpout is only
> useful in special cases,
> https://stats.stackexchange.com/questions/57240/how-do-i-interpret-the-correlations-of-fixed-effects-in-my-glmer-output.
> The post references the book
> http://www.sfs.uni-tuebingen.de/~hbaayen/publications/baayenCUPstats.pdf,
> page 268,
>
> "The summary concludes with a table listing the correlations of the fixed
> effects. The numbers listed here can be used to construct confidence
> elipses for pairs of fixed-effects parameters, and should not be confused
> with the normal correlation obtained by applying cor() to pairs of
> predictor vectors in the input data. Since constructing confidence ellipses
> is beyond the scope of the book we will often suppress this table".
>
> What I understand is that the correlation matrix is useful for prediction
> of future values, which is also my case, but I am not entirely sure I am
> interpreting this correctly.
>
> I really appreciate your advice!
>
> Alessandra
>
>
> On Sun, May 24, 2020 at 3:15 PM Phillip Alday <phillip.alday at mpi.nl>
> wrote:
>
>> Hi,
>>
>> Very high correlations of the fixed-effects estimates can indicate two
>> problems (which are actually just different manifestations of the same
>> deeper problem):
>>
>> 1. Multicollinearity -- this is the same as multicollinearity in
>> classical/standard/non mixed-effects regression. Basically this means
>> that some of your variables are expressing the same thing and so you
>> have some redundancies that could be eliminated. Perfect
>> multicollinearity leads to a rank-deficient model matrix, which R will
>> catch and correct, but near multicollinearity may not be caught.
>>
>> 2. You don't have enough data to get good estimates of all your
>> coefficients.
>>
>> The bigger problem for your inference is that both of these problems
>> will inflate your standard errors. In both cases, there isn't enough
>> information to full tease apart the contribution from the different
>> variables, which means that you have a lot of variability in your
>> estimates and thus large standard errors.
>>
>> Note that some correlation between estimates is expected. If you think
>> of a very simple case with the intercept and one slope/predictor then
>> you'll see that if you change the intercept, then you have to change the
>> slope a bit to get the line to stay close to the observed data.
>>
>> (Once again, I worry that I've oversimplified and said something
>> horribly infelicitous, but I'm always happy to be corrected and learn
>> something myself!)
>>
>> Best,
>>
>> Phillip
>>
>> On 11/5/20 11:42 pm, Alessandra Bielli wrote:
>> > Dear list,
>> >
>> > I am fitting the mixed effect model:
>> >  > lmer(log(percapita_day) ~ Type_residuo + (1|boatID), data=all)
>> >
>> >  where percapita_day is a non-negative continuous response variable (on
>> the
>> > log scale to have residuals normally distributed), Type_residuo is a
>> > categorical explanatory variable and boatID is a random effect with 4
>> > levels.
>> >
>> > I have found values very close to +/-1 in the correlation of fixed
>> effects
>> > matrix below, and after some research I learnt that the coefficients are
>> > not about the correlation of the variables but the expected correlation
>> of
>> > the regression coefficients.
>> >
>> > Correlation of Fixed Effects:
>> >             (Intr) Tp_rsM Tp_rsdOr Tp_rsdOt Tp_Pyc Tp_rsP Tp_rsR
>> > Type_rsdMtl -0.944
>> > Tp_rsdOrgnc -0.951  0.945
>> > Typ_rsdOtrs -0.959  0.953  0.959
>> > Tp_rsdPplyc -0.926  0.919  0.925    0.933
>> > Tp_rsdPlstc -0.951  0.945  0.951    0.958    0.925
>> > Type_resdRd -0.870  0.867  0.873    0.878    0.850  0.872
>> > Type_rsdVdr -0.954  0.949  0.955    0.962    0.928  0.954  0.876
>> >
>> > However I still can't explain why all coefficients are so close to +/-1
>> and
>> > I was wondering if these are indicators that something is wrong with my
>> > model?
>> > Is that due to the presence of outlayers in the response variable (see
>> > attached)?
>> >
>> > Thanks,
>> >
>> > Alessandra
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Rplot01.pdf
Type: application/pdf
Size: 11255 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20200525/5e845415/attachment-0001.pdf>

From bbo|ker @end|ng |rom gm@||@com  Mon May 25 18:33:51 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 25 May 2020 12:33:51 -0400
Subject: [R-sig-ME] 
 correlation of fixed effects coefficients all close to +/-1
In-Reply-To: <CA+6N3yVk6q+zDQ=Rx8QHvKMvR-bV470dzVqHC9aFQ7yy7XfAXg@mail.gmail.com>
References: <CA+6N3yVO+vQ-LoVn6-68sEqv4OnCbT2L62C4q8SP41276c6CbA@mail.gmail.com>
 <1e3db6d1-411c-22ff-1106-34328257d51b@mpi.nl>
 <CA+6N3yUEncW9-RxTNAEwMRA6qd132zRXJ_XiBXQZPVJiA1FaXQ@mail.gmail.com>
 <CA+6N3yVk6q+zDQ=Rx8QHvKMvR-bV470dzVqHC9aFQ7yy7XfAXg@mail.gmail.com>
Message-ID: <f7a70082-be42-7684-ec2f-dc7c96753fa7@gmail.com>

 ??? In this case it seems to make sense to drop the singleton value.? 
You're not going to get very much information out of it anyway.? Other 
things you could try:

 ? * make sure the singleton category is not the first level of your 
factor. (Since effects of levels are by default quantified with respect 
to the first level, having a wonky first level will make your results 
look worse/crazier overall.)

 ? * compare the results with/without the extra observation; if you get 
substantively similar results both ways, you can pick one to present as 
primary and mention that the results are similar with the other choice. 
(But be careful about cherry-picking.)

On 5/25/20 1:12 PM, Alessandra Bielli wrote:
> UPDATE
>
> Dear Phillip and list
>
> As you can see from the graph attached, one of the categories of the
> predictor variable ("madera") only has one observation.
> I decided to remove this observation and I ran the model again, this is the
> corr matrix I get:
>
> Correlation of Fixed Effects:
>              (Intr) Tp_rsdOr Tp_rsdOt Tp_Pyc Tp_rsP Tp_rsR
> Tp_rsdOrgnc -0.725
> Tip_rsdOtrs -0.747  0.593
> Tp_rsdPplyc -0.575  0.458    0.470
> Tp_rsdPlstc -0.659  0.526    0.542    0.419
> Tipo_resdRd -0.445  0.356    0.367    0.282  0.328
> Tipo_rsdVdr -0.747  0.593    0.612    0.470  0.542  0.367
>
> I am aware that modifying a dataset is unacceptable, but I think it showed
> that the source of the problem was lack of observations, am I correct?
> Is there a better way to deal with this? I would rather not delete a line
> of my dataset, even though it is a very uncommon observation for which I do
> not aim to get predictions.
>
> Thank you again for your advice
>
>
> On Mon, May 25, 2020 at 10:52 AM Alessandra Bielli <
> bielli.alessandra at gmail.com> wrote:
>
>> Hi Phillip
>>
>> Thank you so much for your explanation.
>>
>> I have a couple more questions
>>
>> 1.In my model, the regression coefficients of each one of the categories
>> of my predictor are correlated, but I just have one categorical predictor.
>> In case of collinearity I would usually drop one predictor, but here I only
>> have one and my goal is to use the model to predict the dependent variable.
>> What's the procedure here?
>>
>> 2. Is there a test or visual way to determine if I have enough data to get
>> good estimates?
>>
>> 3. A couple days ago I came across this post on Cross validated that
>> states that the correlation of fixed effect part of the outpout is only
>> useful in special cases,
>> https://stats.stackexchange.com/questions/57240/how-do-i-interpret-the-correlations-of-fixed-effects-in-my-glmer-output.
>> The post references the book
>> http://www.sfs.uni-tuebingen.de/~hbaayen/publications/baayenCUPstats.pdf,
>> page 268,
>>
>> "The summary concludes with a table listing the correlations of the fixed
>> effects. The numbers listed here can be used to construct confidence
>> elipses for pairs of fixed-effects parameters, and should not be confused
>> with the normal correlation obtained by applying cor() to pairs of
>> predictor vectors in the input data. Since constructing confidence ellipses
>> is beyond the scope of the book we will often suppress this table".
>>
>> What I understand is that the correlation matrix is useful for prediction
>> of future values, which is also my case, but I am not entirely sure I am
>> interpreting this correctly.
>>
>> I really appreciate your advice!
>>
>> Alessandra
>>
>>
>> On Sun, May 24, 2020 at 3:15 PM Phillip Alday <phillip.alday at mpi.nl>
>> wrote:
>>
>>> Hi,
>>>
>>> Very high correlations of the fixed-effects estimates can indicate two
>>> problems (which are actually just different manifestations of the same
>>> deeper problem):
>>>
>>> 1. Multicollinearity -- this is the same as multicollinearity in
>>> classical/standard/non mixed-effects regression. Basically this means
>>> that some of your variables are expressing the same thing and so you
>>> have some redundancies that could be eliminated. Perfect
>>> multicollinearity leads to a rank-deficient model matrix, which R will
>>> catch and correct, but near multicollinearity may not be caught.
>>>
>>> 2. You don't have enough data to get good estimates of all your
>>> coefficients.
>>>
>>> The bigger problem for your inference is that both of these problems
>>> will inflate your standard errors. In both cases, there isn't enough
>>> information to full tease apart the contribution from the different
>>> variables, which means that you have a lot of variability in your
>>> estimates and thus large standard errors.
>>>
>>> Note that some correlation between estimates is expected. If you think
>>> of a very simple case with the intercept and one slope/predictor then
>>> you'll see that if you change the intercept, then you have to change the
>>> slope a bit to get the line to stay close to the observed data.
>>>
>>> (Once again, I worry that I've oversimplified and said something
>>> horribly infelicitous, but I'm always happy to be corrected and learn
>>> something myself!)
>>>
>>> Best,
>>>
>>> Phillip
>>>
>>> On 11/5/20 11:42 pm, Alessandra Bielli wrote:
>>>> Dear list,
>>>>
>>>> I am fitting the mixed effect model:
>>>>   > lmer(log(percapita_day) ~ Type_residuo + (1|boatID), data=all)
>>>>
>>>>   where percapita_day is a non-negative continuous response variable (on
>>> the
>>>> log scale to have residuals normally distributed), Type_residuo is a
>>>> categorical explanatory variable and boatID is a random effect with 4
>>>> levels.
>>>>
>>>> I have found values very close to +/-1 in the correlation of fixed
>>> effects
>>>> matrix below, and after some research I learnt that the coefficients are
>>>> not about the correlation of the variables but the expected correlation
>>> of
>>>> the regression coefficients.
>>>>
>>>> Correlation of Fixed Effects:
>>>>              (Intr) Tp_rsM Tp_rsdOr Tp_rsdOt Tp_Pyc Tp_rsP Tp_rsR
>>>> Type_rsdMtl -0.944
>>>> Tp_rsdOrgnc -0.951  0.945
>>>> Typ_rsdOtrs -0.959  0.953  0.959
>>>> Tp_rsdPplyc -0.926  0.919  0.925    0.933
>>>> Tp_rsdPlstc -0.951  0.945  0.951    0.958    0.925
>>>> Type_resdRd -0.870  0.867  0.873    0.878    0.850  0.872
>>>> Type_rsdVdr -0.954  0.949  0.955    0.962    0.928  0.954  0.876
>>>>
>>>> However I still can't explain why all coefficients are so close to +/-1
>>> and
>>>> I was wondering if these are indicators that something is wrong with my
>>>> model?
>>>> Is that due to the presence of outlayers in the response variable (see
>>>> attached)?
>>>>
>>>> Thanks,
>>>>
>>>> Alessandra
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Mon May 25 18:41:30 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 25 May 2020 12:41:30 -0400
Subject: [R-sig-ME] a GlmmTMB advice
In-Reply-To: <CAFnE7VBnRp01i6MMHmeDCKCBqrkJqqTRZsJLLiJSJm5X9AXXgQ@mail.gmail.com>
References: <CAFnE7VBnRp01i6MMHmeDCKCBqrkJqqTRZsJLLiJSJm5X9AXXgQ@mail.gmail.com>
Message-ID: <daa6059b-a543-e4d3-2c1a-b3c5790756bb@gmail.com>


On 5/20/20 11:19 AM, Leida Dos Santos wrote:
> Hello there, I was wondering if you help. I am still learning how to work
> with GlmmTMB and I have fitted GlmmTMB before but for categorical data . I
> currently am working on a paper and have a data set where I am trying to
> fit a GlmmTMB. I want to show the effect climate data on species richness
> and abundance . I have fitted the predictor response with species "Richness
> Index" and another with "abundance ", and predictor variables "climate
> data" such as tmax, tmin, precipitation (Richness index I calculated using
> vegan package). I have fitted (site and Month) as random intercept, because
> the data was collected with no consistency but random days and month, and
> years (2010-2019).

 ?? does the Month variable include both month and year (e.g. 
2010.April, 2018.May)?

>   There are 19 different specimens

 ??? Not sure what this means ...

>   and n= 467. All
> variables are numerical. #Global Model example: Abun_2<- glmmTMB(Richness ~
> (tmin +ppt1 + tmax1 + tmax2 +tmin2+ ppt2 + Year)^2+ (1|Site/Month),
> data=Main_data, family="nbinom2"). However when I run this model, I come
> across some warning messages:
>
> "Found more than one class "Matrix" in cache; using the first, from
> namespace 'Matrix'
> Also defined by ?arkhe?

 ?? This is harmless is in this case (although it seems like a 
questionable decision on the part of the arkhe package developers).

> Warning messages:
> _1: In glmmTMB(Abundance ~ (tmin + ppt + tmax2 + tmax + tmin2 + ppt2 + :
> non-integer counts in a nbinom2 model
> 2: In fitTMB(TMBStruc) :
> Model convergence problem; non-positive-definite Hessian matrix. See
> vignette('troubleshooting')
> 3: In fitTMB(TMBStruc) :
> Model convergence problem; function evaluation limit reached without
> convergence (9). See vignette('troubleshooting')".
>
> I checked vignette, but the truth is, this is too advanced for and I do not
> understand what it is saying:) I would love to have some feedback with
> regards to the model. Additionally, should I use just count for Richness
> instead of the Menhinick index for richness? Or should I use a completely
> different model? I

 ?? The main issue here is that it almost never makes sense to use a 
count-based model (nbinom1, nbinom2, Poisson) for data that are not 
actual counts (i.e. integers).? I'm not going to weigh in on which 
index/model you should use; there are long discussions about that, and 
you should decide as much on biological grounds (what question are you 
trying to answer?) as statistical grounds.? As long as the model 
converges to a stable answer, and the assumptions of linearity and 
homoscedasticity are reasonably well met, you should be OK statistically.

>   would be very grateful for any feedback please:):) Thank
> you in advance. :x



> *Leida Dos Santos*
> *BSc**,QTS,MSc,**PhD*
> *IUCN SSC ASG Programme Officer*
> *ldossantos at amphibians.org <ldossantos at amphibians.org>*
> *leidamphibian at gmail.com <leidamphibian at gmail.com>*
> *@anfileida*
> *http://www.amphibians.org/ <http://www.amphibians.org/>*
> *http://www.nzfrogs.org <http://www.nzfrogs.org>*
>         *0  0*
>
>
>
> *      ( -- )  /\(      )/\^^  ^^  ^^  ^^**PS: Please consider the
> environment before printing this E-mail*
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From x|nx|813 @end|ng |rom 126@com  Mon May 25 06:50:39 2020
From: x|nx|813 @end|ng |rom 126@com (=?gb18030?B?WUE=?=)
Date: Mon, 25 May 2020 12:50:39 +0800
Subject: [R-sig-ME] questions on MCMCglmm
Message-ID: <tencent_41200367873CFAF4385A47D602982044DA07@qq.com>

Dear list,


I am using MCMCglmm package in R for my multilevel multinomial logistic regression model. I have a three category level1 outcome 'nomial', which was coded as 0,1,2, a level1 continuous predictor 'IT1', and a level2 continuous predictor 'age', the ID variable is 'ID'. My questions are:


1. The regression coeffecient of IT1 is 0.120220, but which category of 'nomial' does the coefficient concern? how to interpret it? 

2.  Although I have got the model run properly, I dont quite understand  what the 'rcov = ~us(trait):units' do in the model, only that the model  cant run without this part. I have read the coursenote on the website  several times, but still have not figured it out. 

3. What  does 'us','idh', 'ihv' stands for (the author uses them to refer to  covariance structure, are they initials of some terminologies)? 

4. I tried to make predictions with IT1=3 and age=23, I got error suggesting "object 'nomial' not found", but 'nomial' is exactly what I am trying to predict. How to understand the logic behind this?



 So the model fit and prediction are as below:


&gt; m10=MCMCglmm(as.factor(nomial)~IT1+age,random=~ID,data=dat,rcov = ~us(trait):units,family='categorical') 
&gt; summary(m10)

&nbsp;Iterations = 3001:12991
&nbsp;Thinning interval&nbsp; = 10
&nbsp;Sample size&nbsp; = 1000 

&nbsp;DIC: 358.2906 

&nbsp;G-structure:&nbsp; ~ID

&nbsp;&nbsp; post.mean l-95% CI u-95% CI eff.samp
ID&nbsp;&nbsp;&nbsp;&nbsp; 28.75&nbsp;&nbsp;&nbsp; 18.64&nbsp;&nbsp;&nbsp; 38.81&nbsp;&nbsp;&nbsp; 767.4

&nbsp;R-structure:&nbsp; ~us(trait):units

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; post.mean l-95% CI u-95% CI eff.samp
traitnomial.1:traitnomial.1.units&nbsp;&nbsp; 0.04237&nbsp; 0.01597&nbsp; 0.13210&nbsp;&nbsp;&nbsp; 10.77
traitnomial.2:traitnomial.1.units&nbsp; -0.21941 -0.39132 -0.07699&nbsp;&nbsp;&nbsp;&nbsp; 6.04
traitnomial.1:traitnomial.2.units&nbsp; -0.21941 -0.39132 -0.07699&nbsp;&nbsp;&nbsp;&nbsp; 6.04
traitnomial.2:traitnomial.2.units&nbsp;&nbsp; 1.69265&nbsp; 1.21517&nbsp; 2.21141&nbsp;&nbsp;&nbsp; 23.81

&nbsp;Location effects: as.factor(nomial) ~ IT1 + age 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; post.mean&nbsp;&nbsp; l-95% CI&nbsp;&nbsp; u-95% CI eff.samp pMCMC&nbsp; &nbsp;
(Intercept)&nbsp; 34.541008 -15.319976&nbsp; 78.613352&nbsp; 931.637 0.120&nbsp; &nbsp;
IT1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.120220&nbsp;&nbsp; 0.009121&nbsp;&nbsp; 0.343275&nbsp;&nbsp;&nbsp; 6.204 0.004 **
age&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -1.404272&nbsp; -3.267662&nbsp;&nbsp; 0.659993&nbsp; 931.605 0.130&nbsp; &nbsp;
---
Signif. codes:&nbsp; 0 ??***?? 0.001 ??**?? 0.01 ??*?? 0.05 ??.?? 0.1 ?? ?? 1


&gt; predict.MCMCglmm(m10,data.frame(IT1=3,age=23),type='response')
Error in eval(inp, data, env) : object 'nomial' not found


Thank you very much.


Best regards,


YA
	[[alternative HTML version deleted]]


From b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com  Mon May 25 20:52:49 2020
From: b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com (Alessandra Bielli)
Date: Mon, 25 May 2020 12:52:49 -0600
Subject: [R-sig-ME] 
 correlation of fixed effects coefficients all close to +/-1
In-Reply-To: <f7a70082-be42-7684-ec2f-dc7c96753fa7@gmail.com>
References: <CA+6N3yVO+vQ-LoVn6-68sEqv4OnCbT2L62C4q8SP41276c6CbA@mail.gmail.com>
 <1e3db6d1-411c-22ff-1106-34328257d51b@mpi.nl>
 <CA+6N3yUEncW9-RxTNAEwMRA6qd132zRXJ_XiBXQZPVJiA1FaXQ@mail.gmail.com>
 <CA+6N3yVk6q+zDQ=Rx8QHvKMvR-bV470dzVqHC9aFQ7yy7XfAXg@mail.gmail.com>
 <f7a70082-be42-7684-ec2f-dc7c96753fa7@gmail.com>
Message-ID: <CA+6N3yXyGY8HscRRNv9PM+i3xLQPh16eFztgQTh2LCzTmY=dwQ@mail.gmail.com>

Dear Ben

I compared the predictions from a) the model with the singleton category as
last level of my factor vs b) the model that excluded the singleton
category. They are extremely similar.
Also, in both cases the correlations of fixed effects are weaker than in
the initial model, although in the case of a) coeff are > -0.660, while in
b) they are > -0.750.

So I am thinking that the best option is to use model a), because the
correlation is weaker and because it avoids deleting a category. But
correct me if I am wrong!

Thank you very much!

Alessandra

On Mon, May 25, 2020 at 10:34 AM Ben Bolker <bbolker at gmail.com> wrote:

>      In this case it seems to make sense to drop the singleton value.
> You're not going to get very much information out of it anyway.  Other
> things you could try:
>
>    * make sure the singleton category is not the first level of your
> factor. (Since effects of levels are by default quantified with respect
> to the first level, having a wonky first level will make your results
> look worse/crazier overall.)
>
>    * compare the results with/without the extra observation; if you get
> substantively similar results both ways, you can pick one to present as
> primary and mention that the results are similar with the other choice.
> (But be careful about cherry-picking.)
>
> On 5/25/20 1:12 PM, Alessandra Bielli wrote:
> > UPDATE
> >
> > Dear Phillip and list
> >
> > As you can see from the graph attached, one of the categories of the
> > predictor variable ("madera") only has one observation.
> > I decided to remove this observation and I ran the model again, this is
> the
> > corr matrix I get:
> >
> > Correlation of Fixed Effects:
> >              (Intr) Tp_rsdOr Tp_rsdOt Tp_Pyc Tp_rsP Tp_rsR
> > Tp_rsdOrgnc -0.725
> > Tip_rsdOtrs -0.747  0.593
> > Tp_rsdPplyc -0.575  0.458    0.470
> > Tp_rsdPlstc -0.659  0.526    0.542    0.419
> > Tipo_resdRd -0.445  0.356    0.367    0.282  0.328
> > Tipo_rsdVdr -0.747  0.593    0.612    0.470  0.542  0.367
> >
> > I am aware that modifying a dataset is unacceptable, but I think it
> showed
> > that the source of the problem was lack of observations, am I correct?
> > Is there a better way to deal with this? I would rather not delete a line
> > of my dataset, even though it is a very uncommon observation for which I
> do
> > not aim to get predictions.
> >
> > Thank you again for your advice
> >
> >
> > On Mon, May 25, 2020 at 10:52 AM Alessandra Bielli <
> > bielli.alessandra at gmail.com> wrote:
> >
> >> Hi Phillip
> >>
> >> Thank you so much for your explanation.
> >>
> >> I have a couple more questions
> >>
> >> 1.In my model, the regression coefficients of each one of the categories
> >> of my predictor are correlated, but I just have one categorical
> predictor.
> >> In case of collinearity I would usually drop one predictor, but here I
> only
> >> have one and my goal is to use the model to predict the dependent
> variable.
> >> What's the procedure here?
> >>
> >> 2. Is there a test or visual way to determine if I have enough data to
> get
> >> good estimates?
> >>
> >> 3. A couple days ago I came across this post on Cross validated that
> >> states that the correlation of fixed effect part of the outpout is only
> >> useful in special cases,
> >>
> https://stats.stackexchange.com/questions/57240/how-do-i-interpret-the-correlations-of-fixed-effects-in-my-glmer-output
> .
> >> The post references the book
> >>
> http://www.sfs.uni-tuebingen.de/~hbaayen/publications/baayenCUPstats.pdf,
> >> page 268,
> >>
> >> "The summary concludes with a table listing the correlations of the
> fixed
> >> effects. The numbers listed here can be used to construct confidence
> >> elipses for pairs of fixed-effects parameters, and should not be
> confused
> >> with the normal correlation obtained by applying cor() to pairs of
> >> predictor vectors in the input data. Since constructing confidence
> ellipses
> >> is beyond the scope of the book we will often suppress this table".
> >>
> >> What I understand is that the correlation matrix is useful for
> prediction
> >> of future values, which is also my case, but I am not entirely sure I am
> >> interpreting this correctly.
> >>
> >> I really appreciate your advice!
> >>
> >> Alessandra
> >>
> >>
> >> On Sun, May 24, 2020 at 3:15 PM Phillip Alday <phillip.alday at mpi.nl>
> >> wrote:
> >>
> >>> Hi,
> >>>
> >>> Very high correlations of the fixed-effects estimates can indicate two
> >>> problems (which are actually just different manifestations of the same
> >>> deeper problem):
> >>>
> >>> 1. Multicollinearity -- this is the same as multicollinearity in
> >>> classical/standard/non mixed-effects regression. Basically this means
> >>> that some of your variables are expressing the same thing and so you
> >>> have some redundancies that could be eliminated. Perfect
> >>> multicollinearity leads to a rank-deficient model matrix, which R will
> >>> catch and correct, but near multicollinearity may not be caught.
> >>>
> >>> 2. You don't have enough data to get good estimates of all your
> >>> coefficients.
> >>>
> >>> The bigger problem for your inference is that both of these problems
> >>> will inflate your standard errors. In both cases, there isn't enough
> >>> information to full tease apart the contribution from the different
> >>> variables, which means that you have a lot of variability in your
> >>> estimates and thus large standard errors.
> >>>
> >>> Note that some correlation between estimates is expected. If you think
> >>> of a very simple case with the intercept and one slope/predictor then
> >>> you'll see that if you change the intercept, then you have to change
> the
> >>> slope a bit to get the line to stay close to the observed data.
> >>>
> >>> (Once again, I worry that I've oversimplified and said something
> >>> horribly infelicitous, but I'm always happy to be corrected and learn
> >>> something myself!)
> >>>
> >>> Best,
> >>>
> >>> Phillip
> >>>
> >>> On 11/5/20 11:42 pm, Alessandra Bielli wrote:
> >>>> Dear list,
> >>>>
> >>>> I am fitting the mixed effect model:
> >>>>   > lmer(log(percapita_day) ~ Type_residuo + (1|boatID), data=all)
> >>>>
> >>>>   where percapita_day is a non-negative continuous response variable
> (on
> >>> the
> >>>> log scale to have residuals normally distributed), Type_residuo is a
> >>>> categorical explanatory variable and boatID is a random effect with 4
> >>>> levels.
> >>>>
> >>>> I have found values very close to +/-1 in the correlation of fixed
> >>> effects
> >>>> matrix below, and after some research I learnt that the coefficients
> are
> >>>> not about the correlation of the variables but the expected
> correlation
> >>> of
> >>>> the regression coefficients.
> >>>>
> >>>> Correlation of Fixed Effects:
> >>>>              (Intr) Tp_rsM Tp_rsdOr Tp_rsdOt Tp_Pyc Tp_rsP Tp_rsR
> >>>> Type_rsdMtl -0.944
> >>>> Tp_rsdOrgnc -0.951  0.945
> >>>> Typ_rsdOtrs -0.959  0.953  0.959
> >>>> Tp_rsdPplyc -0.926  0.919  0.925    0.933
> >>>> Tp_rsdPlstc -0.951  0.945  0.951    0.958    0.925
> >>>> Type_resdRd -0.870  0.867  0.873    0.878    0.850  0.872
> >>>> Type_rsdVdr -0.954  0.949  0.955    0.962    0.928  0.954  0.876
> >>>>
> >>>> However I still can't explain why all coefficients are so close to
> +/-1
> >>> and
> >>>> I was wondering if these are indicators that something is wrong with
> my
> >>>> model?
> >>>> Is that due to the presence of outlayers in the response variable (see
> >>>> attached)?
> >>>>
> >>>> Thanks,
> >>>>
> >>>> Alessandra
> >>>> _______________________________________________
> >>>> R-sig-mixed-models at r-project.org mailing list
> >>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>>
> >>>
> >>> _______________________________________________
> >>> R-sig-mixed-models at r-project.org mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From M@@rten@Jung @end|ng |rom m@||box@tu-dre@den@de  Mon May 25 22:11:15 2020
From: M@@rten@Jung @end|ng |rom m@||box@tu-dre@den@de (Maarten Jung)
Date: Mon, 25 May 2020 22:11:15 +0200
Subject: [R-sig-ME] lme approximation method for dfs
In-Reply-To: <b00727dde4b14450a66f56a425c1766a@MSX-L104.msx.ad.zih.tu-dresden.de>
References: <CAJTPX_XhATv7AvzLYyTfESsY8VGvBikapUD+Q9gkMqFQES9mEw@mail.gmail.com>
 <421276a8-6922-9f3f-2161-e5497ecbe3f0@mpi.nl>
 <CAHr4DydTGERuf4tzR-HQAB-kpRuwJXk6vNqs32cHB_kTWf3HSg@mail.gmail.com>
 <cf770a5d8e494b9b9520a3b9d81dfb5b@MSX-L104.msx.ad.zih.tu-dresden.de>
 <CAHr4Dye26Ru6+zwk7MUGoAtTGpy7bMg7LX6N0J+ZchgtZZnBBQ@mail.gmail.com>
 <6495710cfbff426098246322066c8ad8@MSX-L104.msx.ad.zih.tu-dresden.de>
 <CAHr4DydVWxQc6m2DKxrj3bqamCwPXTOKBq51jQR1nCZ=QTBRUA@mail.gmail.com>
 <b00727dde4b14450a66f56a425c1766a@MSX-L104.msx.ad.zih.tu-dresden.de>
Message-ID: <CAHr4DyfqqWD0KdivGvTWMVfxY6qw=jxW0xy1ESpXHOf-J-tJRQ@mail.gmail.com>

Hi Sala,

Please keep the mailing list in cc.

Looks like you don't have the latest emmeans version. In previous versions
"appx-satterthwaite" was termed "boot-satterthwaite" (see [1]). But
according to [1] just "satterthwaite" should also work; however, this
misses the point that the degrees of freedom that emmeans calculates are
*approximations to* the Satterthwaite approximation (again, see [1]).

[1] https://cran.r-project.org/web/packages/emmeans/vignettes/models.html#K

Best,
Maarten

On Mon, 25 May 2020, 21:18 Salahadin Lotfi <salahadin.lotfi at gmail.com>
wrote:

> Hi Maarten,
> Thanks again for additional information. I am actually started to have
> issues with *joint_tests**() *function. When I ask for  *mode =
> "appx-satterthwaite" *, I keep receiving the below error:
> Error in match.arg(mode) :   'arg' should be one of ?containment?,
> ?satterthwaite?, ?boot-satterthwaite?, ?auto?
> Precisely, when I use lmer function for fitting my model, it works with no
> issues (I don't specify mode for that as sattherthwaite is default with
> lmerTest()). But, when I fit the model with lme with an unstructured
> covariance matrix (corSymm), I get the error above.
> Would you have any input on this?
>
> Thanks very much,
> Sala
>
> On Mon, May 25, 2020 at 3:41 AM Maarten Jung <
> Maarten.Jung at mailbox.tu-dresden.de> wrote:
>
>> Hi Sala,
>>
>> I'm glad I could help.
>> Please keep the mailing list in cc.
>>
>> Just a small addition to my last mail: If you call joint_tests()
>> directly on your model (and not on an emmGrid object which is returned
>> by the functions emmeans() or ref_grid()), you also have to pass the
>> "mode" argument, otherwise you won't get the degrees of freedom you
>> want (i.e., use joint_tests(lme_fit, mode = "appx-satterthwaite") and
>> not just joint_tests(lme_fit)).
>>
>> Best,
>> Maarten
>>
>>
>>
>> On Mon, May 25, 2020 at 7:09 AM Salahadin Lotfi
>> <salahadin.lotfi at gmail.com> wrote:
>> >
>> > Maarten,
>> >
>> > Both suggestions were great. I used them and they are perfectly working.
>> >
>> > Sala
>> >
>> > On Sun, May 24, 2020 at 4:57 AM Maarten Jung <
>> Maarten.Jung at mailbox.tu-dresden.de> wrote:
>> >>
>> >> Dear Sala,
>> >>
>> >> I *think* the containment method that is mentioned in the "Models
>> >> supported by emmeans" vignette is something similar to the containment
>> >> degrees of freedom approximation in SAS [1] but if you want to use
>> >> this method, you should ask the author of emmeans (Russell Lenth) to
>> >> make sure that my guess is correct.
>> >>
>> >> I guess your emmeans() call throws an error because you are missing
>> >> the "specs" argument which specifies the predictor variables over
>> >> which the estimated marginal means should be calculated. Also, the
>> >> last argument is "sigmaAdjust" instead of "adjustSigma" but it
>> >> defaults to TRUE anyway. This should work (if the emmeans package is
>> >> loaded, otherwise use emmeans:emmeans()):
>> >> emmeans(lme_fit, ~ Group * TIME, mode = "appx-satterthwaite",
>> >> sigmaAdjust = TRUE)
>> >>
>> >> The function joint_tests() which constructs Type-III-ANOVA-like tables
>> >> of all predictors and the function contrast() could be useful for your
>> >> analysis, too. See also the nice vignettes here [2].
>> >>
>> >> [1]
>> https://documentation.sas.com/?docsetId=statug&docsetTarget=statug_glimmix_details38.htm&docsetVersion=15.1&locale=en
>> >> [2]  https://cran.r-project.org/web/packages/emmeans/vignettes/
>> >>
>> >> Best,
>> >> Maarten
>> >>
>> >>
>> >> On Sun, May 24, 2020 at 4:55 AM Salahadin Lotfi
>> >> <salahadin.lotfi at gmail.com> wrote:
>> >> >
>> >> > Wow! Such a thorough answer, Phillip. Thanks very much.
>> >> > I misspoken when I said the lmer would generate p values for fixed
>> effects (yes it is indeed lmerTest package which does it).
>> >> > Just to make sure I understand it correctly:
>> >> > lme uses ML/REML to approximate beta weights of fixed effects,
>> >> > it uses Wald to approximate t/z values (fixed effects), and
>> >> > it uses "inner-outer" rule to get denominator of df (ddf) which is
>> apparently the least optimal method to estimate ddf among other methods
>> (etc., Satterthwaite's). Lastly, is this "inner-outer" rule the same as
>> "containment method"?
>> >> >
>> >> > Maarten,
>> >> > First of all thank you for the nice pointer. The emmeans is elegant.
>> But, I could not get it to work. Could you point out what I am missing in
>> the emmeans function?
>> >> >   Here is my code:
>> >> > ## Group and Time are categorical IVs with three levels.
>> >> > lme_fit<- lme(DV~  factor (Group) * factor(TIME), random = ~ TIME |
>> SubjectID, # factor(Group) * factor(TIME)
>> >> >                   correlation = corSymm (form = ~ 1 | SubjectID),
>> >> >                   weights = varIdent (form = ~ 1 | TIME),
>> >> >                   data = my_data,
>> >> >                   method= "REML",
>> >> >                   na.action = "na.omit")
>> >> > emmeans::emmeans(lme.fit, mode= "appx-satterthwaite", adjustSigma =
>> TRUE) ## NOT working
>> >> >
>> >> >
>> >> > I appreciate for taking time and replying.
>> >> > Sala
>> >> >
>> >> > *************
>> >> > Salahadin (Sala) Lotfi
>> >> >
>> >> > PhD Candidate of Cognitive Neuroscience
>> >> >
>> >> > University of Wisconsin-Milwaukee
>> >> >
>> >> > Anxiety Disorders Laboratory
>> >> >
>> >> > President, Association of Clinical and Cognitive Neuroscience, UWM
>> >> >
>> >> >
>> >> > On Sat, May 23, 2020 at 5:01 PM Maarten Jung <
>> Maarten.Jung at mailbox.tu-dresden.de> wrote:
>> >> >>
>> >> >> Dear Sala,
>> >> >>
>> >> >> As Phillip Alday explained, there is no implementation of the
>> Satterthwaite approximation in the nlme package. If you want to stick with
>> this package, the only way I know to get something similar (for lme
>> objects) is to use functions of the emmeans package with the argument
>> "mode" set to "appx-satterthwaite" (see [1]).
>> >> >>
>> >> >> [1]
>> https://cran.r-project.org/web/packages/emmeans/vignettes/models.html#K
>> >> >>
>> >> >> Best,
>> >> >> Maarten
>> >> >>
>> >> >>
>> >> >> On Sat, 23 May 2020, 17:07 Phillip Alday <phillip.alday at mpi.nl>
>> wrote:
>> >> >>>
>> >> >>> Hi,
>> >> >>>
>> >> >>> On 23/5/20 9:11 am, Salahadin Lotfi wrote:
>> >> >>> > Dear all,
>> >> >>> > I have a very simple question but, have been having a hard time
>> to figure
>> >> >>> > it out.
>> >> >>> > I am using a mixed model with random intercept and slope using
>> lme function
>> >> >>> > with an unstructured covariance matrix. I know lmer uses
>> Satterthwaite's
>> >> >>> > approximation method to approximate dfs of fixed effects,
>> >> >>>
>> >> >>> This is not accurate. lme4 by default doesn't even try to figure
>> out the
>> >> >>> df and doesn't report p-values. The lmerTest package adds in
>> options to
>> >> >>> use Satterthwaite or Kenward-Roger approximations for p-values, but
>> >> >>> depending on who you ask around here, the sentiment for those
>> >> >>> approximations ranges from "of course" to "hmrpf, why would you
>> bother?"
>> >> >>> to "the heretics must be purged!".
>> >> >>>
>> >> >>> The GLMM FAQ (
>> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html)
>> >> >>> has some info on each of these, but I'll copy and paste something
>> >> >>> relevant that I wrote on a different mailing list:
>> >> >>>
>> >> >>> Treating the t-values as z-values is as reasonable as using the
>> >> >>> t-distribution with some estimated degrees of freedom for studies
>> with
>> >> >>> 20-30 subjects and 10s of observations per condition per subject
>> for two
>> >> >>> reasons. One is that a t-distribution with dozens of degrees of
>> freedom
>> >> >>> is essentially a normal distribution, and so even if you could
>> figure
>> >> >>> out what the "right" number of degrees of freedom were, it
>> wouldn't be
>> >> >>> far off from the number you get from the normal distribution. The
>> other
>> >> >>> reason is that none of these asymptotic results are guaranteed to
>> be
>> >> >>> particularly great for anything other than very well behaved linear
>> >> >>> mixed models, which is why things like parametric bootstrap are
>> the gold
>> >> >>> standard for figuring out coverage intervals. And for large models,
>> >> >>> bootstrapping is about as fast as KR (because KR as implemented in
>> >> >>> pbkrmodcomp, which lmerTest depends on, computes the inverse of a
>> large
>> >> >>> n x matrix).
>> >> >>>
>> >> >>> > but I am not sure
>> >> >>> > what is the preferred method that lme uses. Is it Wald or
>> Likelihood ratio?
>> >> >>>
>> >> >>> Wald and likelihood ratio are not degrees of freedom estimates. The
>> >> >>> likelihood-ratio tests do have a df, which corresponds to the
>> difference
>> >> >>> in the number of free parameters between the models, but this not
>> the
>> >> >>> relevant df. (It's numerator degrees of freedom in the ANOVA
>> framework,
>> >> >>> while what you need are the denominator degrees of freedom.) The
>> Wald
>> >> >>> tests are just the things you see in the table of the fixed
>> effects,
>> >> >>> i.e. the tests corresponding to the t- or z-values (or more
>> generally
>> >> >>> the ANOVA-style tests / tests of linear hypotheses you then
>> construct
>> >> >>> from the fixed effects).
>> >> >>>
>> >> >>>
>> >> >>> > I don't think lme offers such an option to specify an
>> approximation method
>> >> >>> > for dfs of fixed effects. Does it?
>> >> >>>
>> >> >>> The dfs in nlme are computed using the "inner-outer" rule which
>> doesn't
>> >> >>> work well for many types of designs common in cognitive
>> neuroscience.
>> >> >>> More information on this is in the GLMM FAQ, search for "Df
>> >> >>> alternatives" on that page.
>> >> >>>
>> >> >>>
>> >> >>> Hope that helps!
>> >> >>>
>> >> >>> Phillip
>> >> >>>
>> >> >>>
>> >> >>> >
>> >> >>> > I appreciate any response in advance.
>> >> >>> > Sala
>> >> >>> >
>> >> >>> >
>> >> >>> > *************
>> >> >>> > Salahadin (Sala) Lotfi
>> >> >>> >
>> >> >>> > PhD Candidate of Cognitive Neuroscience
>> >> >>> >
>> >> >>> > University of Wisconsin-Milwaukee
>> >> >>> >
>> >> >>> > Anxiety Disorders Laboratory
>> >> >>> >
>> >> >>> > President, Association of Clinical and Cognitive Neuroscience,
>> UWM
>> >> >>> >
>> >> >>> >       [[alternative HTML version deleted]]
>> >> >>> >
>> >> >>> > _______________________________________________
>> >> >>> > R-sig-mixed-models at r-project.org mailing list
>> >> >>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >> >>>
>> >> >>> _______________________________________________
>> >> >>> R-sig-mixed-models at r-project.org mailing list
>> >> >>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From @@|@h@d|n@|ot|| @end|ng |rom gm@||@com  Mon May 25 23:12:24 2020
From: @@|@h@d|n@|ot|| @end|ng |rom gm@||@com (Salahadin Lotfi)
Date: Mon, 25 May 2020 16:12:24 -0500
Subject: [R-sig-ME] lme approximation method for dfs
In-Reply-To: <CAHr4DyfqqWD0KdivGvTWMVfxY6qw=jxW0xy1ESpXHOf-J-tJRQ@mail.gmail.com>
References: <CAJTPX_XhATv7AvzLYyTfESsY8VGvBikapUD+Q9gkMqFQES9mEw@mail.gmail.com>
 <421276a8-6922-9f3f-2161-e5497ecbe3f0@mpi.nl>
 <CAHr4DydTGERuf4tzR-HQAB-kpRuwJXk6vNqs32cHB_kTWf3HSg@mail.gmail.com>
 <cf770a5d8e494b9b9520a3b9d81dfb5b@MSX-L104.msx.ad.zih.tu-dresden.de>
 <CAHr4Dye26Ru6+zwk7MUGoAtTGpy7bMg7LX6N0J+ZchgtZZnBBQ@mail.gmail.com>
 <6495710cfbff426098246322066c8ad8@MSX-L104.msx.ad.zih.tu-dresden.de>
 <CAHr4DydVWxQc6m2DKxrj3bqamCwPXTOKBq51jQR1nCZ=QTBRUA@mail.gmail.com>
 <b00727dde4b14450a66f56a425c1766a@MSX-L104.msx.ad.zih.tu-dresden.de>
 <CAHr4DyfqqWD0KdivGvTWMVfxY6qw=jxW0xy1ESpXHOf-J-tJRQ@mail.gmail.com>
Message-ID: <CAJTPX_WnNwS2GM2w6S3mGA+35sH13e+Mxx9SnK=bsjJKH4HnTQ@mail.gmail.com>

Hi Maarten,
Sorry I kept forgetting to CC the list.
Hmm! The approximations of the approximations (the source [1] indeed
clearly mentions that). So, I figured what was the source of the error when
I tried "appx-satterthwaite" and it didn't work. I summarize that down here
for future reference if folks run into the same problem.
1) Make sure emmeans library is the most update to date (thanks Maarten to
pointing out to that). Otherwise, you might get this error below:

*Error in match.arg(mode) :   'arg' should be one of ?containment?,
?satterthwaite?, ?boot-satterthwaite?, ?auto?*

2) If the model specification is too complex and you get *"non-positive
definite approximate variance-covariance"* for the covariance matrix, the
function* joint_tests()* with *mode =   "appx-satterthwaite" *won't work
and it throws this error:



*Error in emm_basis.lme(object, trms, xlev, grid, misc = attr(data,
"misc"),  :   Unable to estimate Satterthwaite parametersIn addition:
Warning message:In seq_len(nrow(B)) : first element used of 'length.out'
argument*


This is completely making sense (I "think" the
sattherthwaite approximation would not be possible with second-order finite
differences). Thus, this error is actually a good sign that the model is
not appropriate for the data (small data?), particularly because non-dp
approximation goes undetected by *summary() function*. Hence, run a less
complicated model.

I hope the summary is accurate.
Thanks a bunch, Maarten.
Sala

[1] https://cran.r-project.org/web/packages/emmeans/vignettes/models.html#K


*************
Salahadin (Sala) Lotfi

PhD Candidate of Cognitive Neuroscience

University of Wisconsin-Milwaukee

Anxiety Disorders Laboratory

President, Association of Clinical and Cognitive Neuroscience, UWM

On Mon, May 25, 2020 at 3:11 PM Maarten Jung <
Maarten.Jung at mailbox.tu-dresden.de> wrote:

> Hi Sala,
>
> Please keep the mailing list in cc.
>
> Looks like you don't have the latest emmeans version. In previous versions
> "appx-satterthwaite" was termed "boot-satterthwaite" (see [1]). But
> according to [1] just "satterthwaite" should also work; however, this
> misses the point that the degrees of freedom that emmeans calculates are
> *approximations to* the Satterthwaite approximation (again, see [1]).
>
> [1]
> https://cran.r-project.org/web/packages/emmeans/vignettes/models.html#K
>
> Best,
> Maarten
>
> On Mon, 25 May 2020, 21:18 Salahadin Lotfi <salahadin.lotfi at gmail.com>
> wrote:
>
>> Hi Maarten,
>> Thanks again for additional information. I am actually started to have
>> issues with *joint_tests**() *function. When I ask for  *mode =
>> "appx-satterthwaite" *, I keep receiving the below error:
>> Error in match.arg(mode) :   'arg' should be one of ?containment?,
>> ?satterthwaite?, ?boot-satterthwaite?, ?auto?
>> Precisely, when I use lmer function for fitting my model, it works with
>> no issues (I don't specify mode for that as sattherthwaite is default with
>> lmerTest()). But, when I fit the model with lme with an unstructured
>> covariance matrix (corSymm), I get the error above.
>> Would you have any input on this?
>>
>> Thanks very much,
>> Sala
>>
>> On Mon, May 25, 2020 at 3:41 AM Maarten Jung <
>> Maarten.Jung at mailbox.tu-dresden.de> wrote:
>>
>>> Hi Sala,
>>>
>>> I'm glad I could help.
>>> Please keep the mailing list in cc.
>>>
>>> Just a small addition to my last mail: If you call joint_tests()
>>> directly on your model (and not on an emmGrid object which is returned
>>> by the functions emmeans() or ref_grid()), you also have to pass the
>>> "mode" argument, otherwise you won't get the degrees of freedom you
>>> want (i.e., use joint_tests(lme_fit, mode = "appx-satterthwaite") and
>>> not just joint_tests(lme_fit)).
>>>
>>> Best,
>>> Maarten
>>>
>>>
>>>
>>> On Mon, May 25, 2020 at 7:09 AM Salahadin Lotfi
>>> <salahadin.lotfi at gmail.com> wrote:
>>> >
>>> > Maarten,
>>> >
>>> > Both suggestions were great. I used them and they are perfectly
>>> working.
>>> >
>>> > Sala
>>> >
>>> > On Sun, May 24, 2020 at 4:57 AM Maarten Jung <
>>> Maarten.Jung at mailbox.tu-dresden.de> wrote:
>>> >>
>>> >> Dear Sala,
>>> >>
>>> >> I *think* the containment method that is mentioned in the "Models
>>> >> supported by emmeans" vignette is something similar to the containment
>>> >> degrees of freedom approximation in SAS [1] but if you want to use
>>> >> this method, you should ask the author of emmeans (Russell Lenth) to
>>> >> make sure that my guess is correct.
>>> >>
>>> >> I guess your emmeans() call throws an error because you are missing
>>> >> the "specs" argument which specifies the predictor variables over
>>> >> which the estimated marginal means should be calculated. Also, the
>>> >> last argument is "sigmaAdjust" instead of "adjustSigma" but it
>>> >> defaults to TRUE anyway. This should work (if the emmeans package is
>>> >> loaded, otherwise use emmeans:emmeans()):
>>> >> emmeans(lme_fit, ~ Group * TIME, mode = "appx-satterthwaite",
>>> >> sigmaAdjust = TRUE)
>>> >>
>>> >> The function joint_tests() which constructs Type-III-ANOVA-like tables
>>> >> of all predictors and the function contrast() could be useful for your
>>> >> analysis, too. See also the nice vignettes here [2].
>>> >>
>>> >> [1]
>>> https://documentation.sas.com/?docsetId=statug&docsetTarget=statug_glimmix_details38.htm&docsetVersion=15.1&locale=en
>>> >> [2]  https://cran.r-project.org/web/packages/emmeans/vignettes/
>>> >>
>>> >> Best,
>>> >> Maarten
>>> >>
>>> >>
>>> >> On Sun, May 24, 2020 at 4:55 AM Salahadin Lotfi
>>> >> <salahadin.lotfi at gmail.com> wrote:
>>> >> >
>>> >> > Wow! Such a thorough answer, Phillip. Thanks very much.
>>> >> > I misspoken when I said the lmer would generate p values for fixed
>>> effects (yes it is indeed lmerTest package which does it).
>>> >> > Just to make sure I understand it correctly:
>>> >> > lme uses ML/REML to approximate beta weights of fixed effects,
>>> >> > it uses Wald to approximate t/z values (fixed effects), and
>>> >> > it uses "inner-outer" rule to get denominator of df (ddf) which is
>>> apparently the least optimal method to estimate ddf among other methods
>>> (etc., Satterthwaite's). Lastly, is this "inner-outer" rule the same as
>>> "containment method"?
>>> >> >
>>> >> > Maarten,
>>> >> > First of all thank you for the nice pointer. The emmeans is
>>> elegant. But, I could not get it to work. Could you point out what I am
>>> missing in the emmeans function?
>>> >> >   Here is my code:
>>> >> > ## Group and Time are categorical IVs with three levels.
>>> >> > lme_fit<- lme(DV~  factor (Group) * factor(TIME), random = ~ TIME |
>>> SubjectID, # factor(Group) * factor(TIME)
>>> >> >                   correlation = corSymm (form = ~ 1 | SubjectID),
>>> >> >                   weights = varIdent (form = ~ 1 | TIME),
>>> >> >                   data = my_data,
>>> >> >                   method= "REML",
>>> >> >                   na.action = "na.omit")
>>> >> > emmeans::emmeans(lme.fit, mode= "appx-satterthwaite", adjustSigma =
>>> TRUE) ## NOT working
>>> >> >
>>> >> >
>>> >> > I appreciate for taking time and replying.
>>> >> > Sala
>>> >> >
>>> >> > *************
>>> >> > Salahadin (Sala) Lotfi
>>> >> >
>>> >> > PhD Candidate of Cognitive Neuroscience
>>> >> >
>>> >> > University of Wisconsin-Milwaukee
>>> >> >
>>> >> > Anxiety Disorders Laboratory
>>> >> >
>>> >> > President, Association of Clinical and Cognitive Neuroscience, UWM
>>> >> >
>>> >> >
>>> >> > On Sat, May 23, 2020 at 5:01 PM Maarten Jung <
>>> Maarten.Jung at mailbox.tu-dresden.de> wrote:
>>> >> >>
>>> >> >> Dear Sala,
>>> >> >>
>>> >> >> As Phillip Alday explained, there is no implementation of the
>>> Satterthwaite approximation in the nlme package. If you want to stick with
>>> this package, the only way I know to get something similar (for lme
>>> objects) is to use functions of the emmeans package with the argument
>>> "mode" set to "appx-satterthwaite" (see [1]).
>>> >> >>
>>> >> >> [1]
>>> https://cran.r-project.org/web/packages/emmeans/vignettes/models.html#K
>>> >> >>
>>> >> >> Best,
>>> >> >> Maarten
>>> >> >>
>>> >> >>
>>> >> >> On Sat, 23 May 2020, 17:07 Phillip Alday <phillip.alday at mpi.nl>
>>> wrote:
>>> >> >>>
>>> >> >>> Hi,
>>> >> >>>
>>> >> >>> On 23/5/20 9:11 am, Salahadin Lotfi wrote:
>>> >> >>> > Dear all,
>>> >> >>> > I have a very simple question but, have been having a hard time
>>> to figure
>>> >> >>> > it out.
>>> >> >>> > I am using a mixed model with random intercept and slope using
>>> lme function
>>> >> >>> > with an unstructured covariance matrix. I know lmer uses
>>> Satterthwaite's
>>> >> >>> > approximation method to approximate dfs of fixed effects,
>>> >> >>>
>>> >> >>> This is not accurate. lme4 by default doesn't even try to figure
>>> out the
>>> >> >>> df and doesn't report p-values. The lmerTest package adds in
>>> options to
>>> >> >>> use Satterthwaite or Kenward-Roger approximations for p-values,
>>> but
>>> >> >>> depending on who you ask around here, the sentiment for those
>>> >> >>> approximations ranges from "of course" to "hmrpf, why would you
>>> bother?"
>>> >> >>> to "the heretics must be purged!".
>>> >> >>>
>>> >> >>> The GLMM FAQ (
>>> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html)
>>> >> >>> has some info on each of these, but I'll copy and paste something
>>> >> >>> relevant that I wrote on a different mailing list:
>>> >> >>>
>>> >> >>> Treating the t-values as z-values is as reasonable as using the
>>> >> >>> t-distribution with some estimated degrees of freedom for studies
>>> with
>>> >> >>> 20-30 subjects and 10s of observations per condition per subject
>>> for two
>>> >> >>> reasons. One is that a t-distribution with dozens of degrees of
>>> freedom
>>> >> >>> is essentially a normal distribution, and so even if you could
>>> figure
>>> >> >>> out what the "right" number of degrees of freedom were, it
>>> wouldn't be
>>> >> >>> far off from the number you get from the normal distribution. The
>>> other
>>> >> >>> reason is that none of these asymptotic results are guaranteed to
>>> be
>>> >> >>> particularly great for anything other than very well behaved
>>> linear
>>> >> >>> mixed models, which is why things like parametric bootstrap are
>>> the gold
>>> >> >>> standard for figuring out coverage intervals. And for large
>>> models,
>>> >> >>> bootstrapping is about as fast as KR (because KR as implemented in
>>> >> >>> pbkrmodcomp, which lmerTest depends on, computes the inverse of a
>>> large
>>> >> >>> n x matrix).
>>> >> >>>
>>> >> >>> > but I am not sure
>>> >> >>> > what is the preferred method that lme uses. Is it Wald or
>>> Likelihood ratio?
>>> >> >>>
>>> >> >>> Wald and likelihood ratio are not degrees of freedom estimates.
>>> The
>>> >> >>> likelihood-ratio tests do have a df, which corresponds to the
>>> difference
>>> >> >>> in the number of free parameters between the models, but this not
>>> the
>>> >> >>> relevant df. (It's numerator degrees of freedom in the ANOVA
>>> framework,
>>> >> >>> while what you need are the denominator degrees of freedom.) The
>>> Wald
>>> >> >>> tests are just the things you see in the table of the fixed
>>> effects,
>>> >> >>> i.e. the tests corresponding to the t- or z-values (or more
>>> generally
>>> >> >>> the ANOVA-style tests / tests of linear hypotheses you then
>>> construct
>>> >> >>> from the fixed effects).
>>> >> >>>
>>> >> >>>
>>> >> >>> > I don't think lme offers such an option to specify an
>>> approximation method
>>> >> >>> > for dfs of fixed effects. Does it?
>>> >> >>>
>>> >> >>> The dfs in nlme are computed using the "inner-outer" rule which
>>> doesn't
>>> >> >>> work well for many types of designs common in cognitive
>>> neuroscience.
>>> >> >>> More information on this is in the GLMM FAQ, search for "Df
>>> >> >>> alternatives" on that page.
>>> >> >>>
>>> >> >>>
>>> >> >>> Hope that helps!
>>> >> >>>
>>> >> >>> Phillip
>>> >> >>>
>>> >> >>>
>>> >> >>> >
>>> >> >>> > I appreciate any response in advance.
>>> >> >>> > Sala
>>> >> >>> >
>>> >> >>> >
>>> >> >>> > *************
>>> >> >>> > Salahadin (Sala) Lotfi
>>> >> >>> >
>>> >> >>> > PhD Candidate of Cognitive Neuroscience
>>> >> >>> >
>>> >> >>> > University of Wisconsin-Milwaukee
>>> >> >>> >
>>> >> >>> > Anxiety Disorders Laboratory
>>> >> >>> >
>>> >> >>> > President, Association of Clinical and Cognitive Neuroscience,
>>> UWM
>>> >> >>> >
>>> >> >>> >       [[alternative HTML version deleted]]
>>> >> >>> >
>>> >> >>> > _______________________________________________
>>> >> >>> > R-sig-mixed-models at r-project.org mailing list
>>> >> >>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>> >> >>>
>>> >> >>> _______________________________________________
>>> >> >>> R-sig-mixed-models at r-project.org mailing list
>>> >> >>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Tue May 26 02:38:09 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 25 May 2020 20:38:09 -0400
Subject: [R-sig-ME] a GlmmTMB advice
In-Reply-To: <CAFnE7VDMh5spG-j5WhnvHd5LjivmxPPBUh0cUTBVN7k1h1ME2Q@mail.gmail.com>
References: <CAFnE7VBnRp01i6MMHmeDCKCBqrkJqqTRZsJLLiJSJm5X9AXXgQ@mail.gmail.com>
 <daa6059b-a543-e4d3-2c1a-b3c5790756bb@gmail.com>
 <CAFnE7VDMh5spG-j5WhnvHd5LjivmxPPBUh0cUTBVN7k1h1ME2Q@mail.gmail.com>
Message-ID: <d784184a-87cc-560e-28b6-d984eb83cf2b@gmail.com>

 ?? [please keep the mailing list in Cc:]

On 5/25/20 8:31 PM, Leida Dos Santos wrote:
> Thank very much Ben,
> >There are 19 different specimens<- I mean 19 different species 
> (specimens in portuguese, I am sorry!)


 ? OK.

>
> Ben I am concerned because after checking the data carefully, I have 
> noticed?that some 13 of the 16 different species?there are less than 
> 10 individuals?per species. Would it be ok to run the Glmm anyways? I 
> am modelling the abundance as response variable and 
> predictor?variables year/month (the month of sampling for each year), 
> species, climate data (tmax, tmin, precipitation, and species life 
> traits?history (size, habitat, habit, etc).

 ?? I don't see immediately why this would be a problem with the 
species-richness analysis.

 ?? Are you running 19 (or 16, I can't tell how many species there 
really are) separate abundance analyses?? That's going to be very 
difficult if you have small numbers of individuals per species. Also, if 
you run 19 analyses with 8 covariates each, the chances of getting a lot 
of false positives/need for some kind of shrinkage or 
multiple-comparisons correction goes up.? (This is not really a mixed 
modeling question, more a generic question of what to do with relatively 
small, noisy ecological data sets ...)


>
> I am sorry for not being able to formulate my questions properly.
>
> Kind regards,
>
>
>
>
>
> *Leida Dos Santos*
> */BSc/**/,QTS,MSc,/**/PhD/*
> /_**_*IUCN SSC ASG***?Programme *Officer*/
> /ldossantos at amphibians.org <mailto:ldossantos at amphibians.org>/
> _leidamphibian at gmail.com <mailto:leidamphibian at gmail.com>_
> */@anfileida/*
> */http://www.amphibians.org//*
> **/http://www.nzfrogs.org/**
> *0 0*
> *( -- )
> /\(????? )/\
> ^^? ^^? ^^? ^^
> **PS: Please consider the environment before printing this E-mail*
>
>
>
>
>
> On Mon, 25 May 2020 at 17:44, Ben Bolker <bbolker at gmail.com 
> <mailto:bbolker at gmail.com>> wrote:
>
>
>     On 5/20/20 11:19 AM, Leida Dos Santos wrote:
>     > Hello there, I was wondering if you help. I am still learning
>     how to work
>     > with GlmmTMB and I have fitted GlmmTMB before but for
>     categorical data . I
>     > currently am working on a paper and have a data set where I am
>     trying to
>     > fit a GlmmTMB. I want to show the effect climate data on species
>     richness
>     > and abundance . I have fitted the predictor response with
>     species "Richness
>     > Index" and another with "abundance ", and predictor variables
>     "climate
>     > data" such as tmax, tmin, precipitation (Richness index I
>     calculated using
>     > vegan package). I have fitted (site and Month) as random
>     intercept, because
>     > the data was collected with no consistency but random days and
>     month, and
>     > years (2010-2019).
>
>     ??? does the Month variable include both month and year (e.g.
>     2010.April, 2018.May)?
>
>     >? ?There are 19 different specimens
>
>     ???? Not sure what this means ...
>
>     >? ?and n= 467. All
>     > variables are numerical. #Global Model example: Abun_2<-
>     glmmTMB(Richness ~
>     > (tmin +ppt1 + tmax1 + tmax2 +tmin2+ ppt2 + Year)^2+ (1|Site/Month),
>     > data=Main_data, family="nbinom2"). However when I run this
>     model, I come
>     > across some warning messages:
>     >
>     > "Found more than one class "Matrix" in cache; using the first, from
>     > namespace 'Matrix'
>     > Also defined by ?arkhe?
>
>     ??? This is harmless is in this case (although it seems like a
>     questionable decision on the part of the arkhe package developers).
>
>     > Warning messages:
>     > _1: In glmmTMB(Abundance ~ (tmin + ppt + tmax2 + tmax + tmin2 +
>     ppt2 + :
>     > non-integer counts in a nbinom2 model
>     > 2: In fitTMB(TMBStruc) :
>     > Model convergence problem; non-positive-definite Hessian matrix. See
>     > vignette('troubleshooting')
>     > 3: In fitTMB(TMBStruc) :
>     > Model convergence problem; function evaluation limit reached without
>     > convergence (9). See vignette('troubleshooting')".
>     >
>     > I checked vignette, but the truth is, this is too advanced for
>     and I do not
>     > understand what it is saying:) I would love to have some
>     feedback with
>     > regards to the model. Additionally, should I use just count for
>     Richness
>     > instead of the Menhinick index for richness? Or should I use a
>     completely
>     > different model? I
>
>     ??? The main issue here is that it almost never makes sense to use a
>     count-based model (nbinom1, nbinom2, Poisson) for data that are not
>     actual counts (i.e. integers).? I'm not going to weigh in on which
>     index/model you should use; there are long discussions about that,
>     and
>     you should decide as much on biological grounds (what question are
>     you
>     trying to answer?) as statistical grounds.? As long as the model
>     converges to a stable answer, and the assumptions of linearity and
>     homoscedasticity are reasonably well met, you should be OK
>     statistically.
>
>     >? ?would be very grateful for any feedback please:):) Thank
>     > you in advance. :x
>
>
>
>     > *Leida Dos Santos*
>     > *BSc**,QTS,MSc,**PhD*
>     > *IUCN SSC ASG Programme Officer*
>     > *ldossantos at amphibians.org <mailto:ldossantos at amphibians.org>
>     <ldossantos at amphibians.org <mailto:ldossantos at amphibians.org>>*
>     > *leidamphibian at gmail.com <mailto:leidamphibian at gmail.com>
>     <leidamphibian at gmail.com <mailto:leidamphibian at gmail.com>>*
>     > *@anfileida*
>     > *http://www.amphibians.org/ <http://www.amphibians.org/>*
>     > *http://www.nzfrogs.org <http://www.nzfrogs.org>*
>     >? ? ? ? ?*0? 0*
>     >
>     >
>     >
>     > *? ? ? ( -- )? /\(? ? ? )/\^^? ^^? ^^? ^^**PS: Please consider the
>     > environment before printing this E-mail*
>     >
>     >? ? ? ?[[alternative HTML version deleted]]
>     >
>     > _______________________________________________
>     > R-sig-mixed-models at r-project.org
>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>     _______________________________________________
>     R-sig-mixed-models at r-project.org
>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>     https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Tue May 26 02:48:54 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 25 May 2020 20:48:54 -0400
Subject: [R-sig-ME] a GlmmTMB advice
In-Reply-To: <DB8P194MB0485CB3C4CB6F2157AD6A6CDF9B00@DB8P194MB0485.EURP194.PROD.OUTLOOK.COM>
References: <CAFnE7VBnRp01i6MMHmeDCKCBqrkJqqTRZsJLLiJSJm5X9AXXgQ@mail.gmail.com>
 <daa6059b-a543-e4d3-2c1a-b3c5790756bb@gmail.com>
 <CAFnE7VDMh5spG-j5WhnvHd5LjivmxPPBUh0cUTBVN7k1h1ME2Q@mail.gmail.com>
 <d784184a-87cc-560e-28b6-d984eb83cf2b@gmail.com>
 <DB8P194MB0485CB3C4CB6F2157AD6A6CDF9B00@DB8P194MB0485.EURP194.PROD.OUTLOOK.COM>
Message-ID: <439f0884-0259-4ddb-7ef9-11447b96c3a2@gmail.com>


 ?? No need to apologize (but do, please, keep the mailing? list in the Cc:)

 ?? Are you running/planning to run 16 separate abundance analyses?? If 
so, see comments below ...


On 5/25/20 8:40 PM, Leida Dos Santos wrote:
> There are 486 individuals of 16 different species ! Sorry !
>
> Get Outlook for iOS <https://aka.ms/o0ukef>
> ------------------------------------------------------------------------
> *From:* Ben Bolker <bbolker at gmail.com>
> *Sent:* Tuesday, May 26, 2020 1:38:09 AM
> *To:* Leida Dos Santos <ldossantos at amphibians.org>; 
> r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
> *Subject:* Re: [R-sig-ME] a GlmmTMB advice
>
> ?? [please keep the mailing list in Cc:]
>
> On 5/25/20 8:31 PM, Leida Dos Santos wrote:
>> Thank very much Ben,
>> >There are 19 different specimens<- I mean 19 different species 
>> (specimens in portuguese, I am sorry!)
>
>
> ? OK.
>
>>
>> Ben I am concerned because after checking the data carefully, I have 
>> noticed?that some 13 of the 16 different species?there are less than 
>> 10 individuals?per species. Would it be ok to run the Glmm anyways? I 
>> am modelling the abundance as response variable and 
>> predictor?variables year/month (the month of sampling for each year), 
>> species, climate data (tmax, tmin, precipitation, and species life 
>> traits?history (size, habitat, habit, etc).
>
> ?? I don't see immediately why this would be a problem with the 
> species-richness analysis.
>
> ?? Are you running 19 (or 16, I can't tell how many species there 
> really are) separate abundance analyses?? That's going to be very 
> difficult if you have small numbers of individuals per species.? Also, 
> if you run 19 analyses with 8 covariates each, the chances of getting 
> a lot of false positives/need for some kind of shrinkage or 
> multiple-comparisons correction goes up.? (This is not really a mixed 
> modeling question, more a generic question of what to do with 
> relatively small, noisy ecological data sets ...)
>
>
>>
>> I am sorry for not being able to formulate my questions properly.
>>
>> Kind regards,
>>
>>
>>
>>
>>
>> *Leida Dos Santos*
>> */BSc/**/,QTS,MSc,/**/PhD/*
>> /_**_*IUCN SSC ASG***?Programme *Officer*/
>> /ldossantos at amphibians.org <mailto:ldossantos at amphibians.org>/
>> _leidamphibian at gmail.com <mailto:leidamphibian at gmail.com>_
>> */@anfileida/*
>> */http://www.amphibians.org//*
>> **/http://www.nzfrogs.org/**
>> *0? 0*
>> *????? ( -- )
>> /\(????? )/\
>> ^^? ^^? ^^? ^^
>> **PS: Please consider the environment before printing this E-mail*
>>
>>
>>
>>
>>
>> On Mon, 25 May 2020 at 17:44, Ben Bolker <bbolker at gmail.com 
>> <mailto:bbolker at gmail.com>> wrote:
>>
>>
>>     On 5/20/20 11:19 AM, Leida Dos Santos wrote:
>>     > Hello there, I was wondering if you help. I am still learning
>>     how to work
>>     > with GlmmTMB and I have fitted GlmmTMB before but for
>>     categorical data . I
>>     > currently am working on a paper and have a data set where I am
>>     trying to
>>     > fit a GlmmTMB. I want to show the effect climate data on
>>     species richness
>>     > and abundance . I have fitted the predictor response with
>>     species "Richness
>>     > Index" and another with "abundance ", and predictor variables
>>     "climate
>>     > data" such as tmax, tmin, precipitation (Richness index I
>>     calculated using
>>     > vegan package). I have fitted (site and Month) as random
>>     intercept, because
>>     > the data was collected with no consistency but random days and
>>     month, and
>>     > years (2010-2019).
>>
>>     ??? does the Month variable include both month and year (e.g.
>>     2010.April, 2018.May)?
>>
>>     >? ?There are 19 different specimens
>>
>>     ???? Not sure what this means ...
>>
>>     >? ?and n= 467. All
>>     > variables are numerical. #Global Model example: Abun_2<-
>>     glmmTMB(Richness ~
>>     > (tmin +ppt1 + tmax1 + tmax2 +tmin2+ ppt2 + Year)^2+ (1|Site/Month),
>>     > data=Main_data, family="nbinom2"). However when I run this
>>     model, I come
>>     > across some warning messages:
>>     >
>>     > "Found more than one class "Matrix" in cache; using the first, from
>>     > namespace 'Matrix'
>>     > Also defined by ?arkhe?
>>
>>     ??? This is harmless is in this case (although it seems like a
>>     questionable decision on the part of the arkhe package developers).
>>
>>     > Warning messages:
>>     > _1: In glmmTMB(Abundance ~ (tmin + ppt + tmax2 + tmax + tmin2 +
>>     ppt2 + :
>>     > non-integer counts in a nbinom2 model
>>     > 2: In fitTMB(TMBStruc) :
>>     > Model convergence problem; non-positive-definite Hessian
>>     matrix. See
>>     > vignette('troubleshooting')
>>     > 3: In fitTMB(TMBStruc) :
>>     > Model convergence problem; function evaluation limit reached
>>     without
>>     > convergence (9). See vignette('troubleshooting')".
>>     >
>>     > I checked vignette, but the truth is, this is too advanced for
>>     and I do not
>>     > understand what it is saying:) I would love to have some
>>     feedback with
>>     > regards to the model. Additionally, should I use just count for
>>     Richness
>>     > instead of the Menhinick index for richness? Or should I use a
>>     completely
>>     > different model? I
>>
>>     ??? The main issue here is that it almost never makes sense to use a
>>     count-based model (nbinom1, nbinom2, Poisson) for data that are not
>>     actual counts (i.e. integers).? I'm not going to weigh in on which
>>     index/model you should use; there are long discussions about
>>     that, and
>>     you should decide as much on biological grounds (what question
>>     are you
>>     trying to answer?) as statistical grounds.? As long as the model
>>     converges to a stable answer, and the assumptions of linearity and
>>     homoscedasticity are reasonably well met, you should be OK
>>     statistically.
>>
>>     >? ?would be very grateful for any feedback please:):) Thank
>>     > you in advance. :x
>>
>>
>>
>>     > *Leida Dos Santos*
>>     > *BSc**,QTS,MSc,**PhD*
>>     > *IUCN SSC ASG Programme Officer*
>>     > *ldossantos at amphibians.org <mailto:ldossantos at amphibians.org>
>>     <ldossantos at amphibians.org <mailto:ldossantos at amphibians.org>>*
>>     > *leidamphibian at gmail.com <mailto:leidamphibian at gmail.com>
>>     <leidamphibian at gmail.com <mailto:leidamphibian at gmail.com>>*
>>     > *@anfileida*
>>     > *http://www.amphibians.org/ <http://www.amphibians.org/>*
>>     > *http://www.nzfrogs.org <http://www.nzfrogs.org>*
>>     >? ? ? ? ?*0? 0*
>>     >
>>     >
>>     >
>>     > *? ? ? ( -- )? /\(? ? ? )/\^^? ^^? ^^? ^^**PS: Please consider the
>>     > environment before printing this E-mail*
>>     >
>>     >? ? ? ?[[alternative HTML version deleted]]
>>     >
>>     > _______________________________________________
>>     > R-sig-mixed-models at r-project.org
>>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>>     > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>     <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
>>
>>     _______________________________________________
>>     R-sig-mixed-models at r-project.org
>>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>>     https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>

	[[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Wed May 27 00:27:26 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Tue, 26 May 2020 17:27:26 -0500
Subject: [R-sig-ME] Latent variable regression in lme4 as in HLM
Message-ID: <CACgv6yWJdDpVCCAnpe+FoQZqL83L=qPPWEkGwFJmXUNLomYRSw@mail.gmail.com>

Dear All,

I know that in the HLM software, it is possible to use "intercept" (e.g.,
initial place of students at year "0") as the *predictor *of "slope" (e.g.,
fixed rate of change in years) under the *Latent Variable Regression *tab.

I was wondering if this is also possible in "lme4" or any other
mixed-modeling packages in R?  *Thanks, Simon*

*## Here is an example dataset for demonstration:*
library(lme4)
dat <- read.csv('https://raw.githubusercontent.com/hkil/m/master/z.csv')
m1 <- lmer(y ~ year + (1|stid), data = dat)      #### 'stid' = student id

	[[alternative HTML version deleted]]


From u@nhoro@1 @end|ng |rom buckeyem@||@o@u@edu  Wed May 27 01:12:01 2020
From: u@nhoro@1 @end|ng |rom buckeyem@||@o@u@edu (Uanhoro, James)
Date: Tue, 26 May 2020 23:12:01 +0000
Subject: [R-sig-ME] Latent variable regression in lme4 as in HLM
In-Reply-To: <CACgv6yWJdDpVCCAnpe+FoQZqL83L=qPPWEkGwFJmXUNLomYRSw@mail.gmail.com>
References: <CACgv6yWJdDpVCCAnpe+FoQZqL83L=qPPWEkGwFJmXUNLomYRSw@mail.gmail.com>
Message-ID: <df16541525bb3cea1d766f941bde9dd7abaafcb2.camel@buckeyemail.osu.edu>

Hello Simon,

I'm not sure what HLM does. However: if your question is about using
the random intercepts (individuals' starting points) to predict the
random slopes (their linear growth rate), then the model you need is:

summary(m2 <- lmer(y ~ year + (1 + year | stid), dat))

whcih returns the random intercept and a random slope on time.

The correlation between both random effects is the regression
coefficient from regressing the slope on the intercept (or vice-versa)
when both variables are standardized.

More generally, you can always obtain regression coefficients from a
correlation/covariance matrix of random effects. With a two-by-two
correlation matrix, the single correlation is the coefficient (in both
directions). In a larger matrix of random effects, you can use the
solve() function in R to obtain coefficients from the matrix. See here:
https://stackoverflow.com/questions/40762865/how-do-i-get-regression-
coefficients-from-a-variance-covariance-matrix-in-r

I tried your exact example, and m2 above will not fit because some of
your participants have under 2 time points while the maximum number of
time points is 3, resulting in a situation where the software is
attempting to compute more random effect values than there are rows in
the data - the software complains. Also, it is a good idea to rescale
that y variable prior to data analysis. I was able to get the model to
run by limiting the data to cases with more than 1 recorded time point:

summary(m3 <- lmer(y.s ~ year + (1 + year | stid), data = t.dat, subset
= n > 1))

I arrived at a correlation/coefficient of -0.06.

Hope this helps, -James.

Sent from Outlook Mobile

From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on
behalf of Simon Harmel <sim.harmel at gmail.com>
Sent: Tuesday, May 26, 2020, 18:27
To: r-sig-mixed-models
Subject: [R-sig-ME] Latent variable regression in lme4 as in HLM

Dear All,

I know that in the HLM software, it is possible to use "intercept"
(e.g.,
initial place of students at year "0") as the *predictor *of "slope"
(e.g.,
fixed rate of change in years) under the *Latent Variable Regression
*tab.

I was wondering if this is also possible in "lme4" or any other
mixed-modeling packages in R?  *Thanks, Simon*

*## Here is an example dataset for demonstration:*
library(lme4)
dat <- read.csv('
https://urldefense.com/v3/__https://raw.githubusercontent.com/hkil/m/master/z.csv__;!!KGKeukY!gjIgidLro6PaJJUHZOY1gk9IW8FfrzGWzo9IEaCRgFwkorvpE1tkLXqn3ujcsmy6OvxsB2xlUIM$
 ')
m1 <- lmer(y ~ year + (1|stid), data = dat)      #### 'stid' = student
id

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list

https://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models__;!!KGKeukY!gjIgidLro6PaJJUHZOY1gk9IW8FfrzGWzo9IEaCRgFwkorvpE1tkLXqn3ujcsmy6Ovxs517Hn00$


From bbo|ker @end|ng |rom gm@||@com  Wed May 27 02:38:30 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Tue, 26 May 2020 20:38:30 -0400
Subject: [R-sig-ME] Latent variable regression in lme4 as in HLM
In-Reply-To: <df16541525bb3cea1d766f941bde9dd7abaafcb2.camel@buckeyemail.osu.edu>
References: <CACgv6yWJdDpVCCAnpe+FoQZqL83L=qPPWEkGwFJmXUNLomYRSw@mail.gmail.com>
 <df16541525bb3cea1d766f941bde9dd7abaafcb2.camel@buckeyemail.osu.edu>
Message-ID: <CABghstR8rxvN6Wu-ZNaOhZj5+h14ypeiMD8W9etPzoQW-wqZpw@mail.gmail.com>

   For what it's worth, *if* you're sufficiently sure that your model
is identifiable, you can override the checks that test the relative
numbers of observations/levels/etc.; see the "check.*" options in
?lme4::lmerControl, and set the relevant ones to "ignore"

On Tue, May 26, 2020 at 7:12 PM Uanhoro, James
<uanhoro.1 at buckeyemail.osu.edu> wrote:
>
> Hello Simon,
>
> I'm not sure what HLM does. However: if your question is about using
> the random intercepts (individuals' starting points) to predict the
> random slopes (their linear growth rate), then the model you need is:
>
> summary(m2 <- lmer(y ~ year + (1 + year | stid), dat))
>
> whcih returns the random intercept and a random slope on time.
>
> The correlation between both random effects is the regression
> coefficient from regressing the slope on the intercept (or vice-versa)
> when both variables are standardized.
>
> More generally, you can always obtain regression coefficients from a
> correlation/covariance matrix of random effects. With a two-by-two
> correlation matrix, the single correlation is the coefficient (in both
> directions). In a larger matrix of random effects, you can use the
> solve() function in R to obtain coefficients from the matrix. See here:
> https://stackoverflow.com/questions/40762865/how-do-i-get-regression-
> coefficients-from-a-variance-covariance-matrix-in-r
>
> I tried your exact example, and m2 above will not fit because some of
> your participants have under 2 time points while the maximum number of
> time points is 3, resulting in a situation where the software is
> attempting to compute more random effect values than there are rows in
> the data - the software complains. Also, it is a good idea to rescale
> that y variable prior to data analysis. I was able to get the model to
> run by limiting the data to cases with more than 1 recorded time point:
>
> summary(m3 <- lmer(y.s ~ year + (1 + year | stid), data = t.dat, subset
> = n > 1))
>
> I arrived at a correlation/coefficient of -0.06.
>
> Hope this helps, -James.
>
> Sent from Outlook Mobile
>
> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on
> behalf of Simon Harmel <sim.harmel at gmail.com>
> Sent: Tuesday, May 26, 2020, 18:27
> To: r-sig-mixed-models
> Subject: [R-sig-ME] Latent variable regression in lme4 as in HLM
>
> Dear All,
>
> I know that in the HLM software, it is possible to use "intercept"
> (e.g.,
> initial place of students at year "0") as the *predictor *of "slope"
> (e.g.,
> fixed rate of change in years) under the *Latent Variable Regression
> *tab.
>
> I was wondering if this is also possible in "lme4" or any other
> mixed-modeling packages in R?  *Thanks, Simon*
>
> *## Here is an example dataset for demonstration:*
> library(lme4)
> dat <- read.csv('
> https://urldefense.com/v3/__https://raw.githubusercontent.com/hkil/m/master/z.csv__;!!KGKeukY!gjIgidLro6PaJJUHZOY1gk9IW8FfrzGWzo9IEaCRgFwkorvpE1tkLXqn3ujcsmy6OvxsB2xlUIM$
>  ')
> m1 <- lmer(y ~ year + (1|stid), data = dat)      #### 'stid' = student
> id
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
>
> https://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models__;!!KGKeukY!gjIgidLro6PaJJUHZOY1gk9IW8FfrzGWzo9IEaCRgFwkorvpE1tkLXqn3ujcsmy6Ovxs517Hn00$
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From @|m@h@rme| @end|ng |rom gm@||@com  Wed May 27 04:12:58 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Tue, 26 May 2020 21:12:58 -0500
Subject: [R-sig-ME] Latent variable regression in lme4 as in HLM
In-Reply-To: <CABghstR8rxvN6Wu-ZNaOhZj5+h14ypeiMD8W9etPzoQW-wqZpw@mail.gmail.com>
References: <CACgv6yWJdDpVCCAnpe+FoQZqL83L=qPPWEkGwFJmXUNLomYRSw@mail.gmail.com>
 <df16541525bb3cea1d766f941bde9dd7abaafcb2.camel@buckeyemail.osu.edu>
 <CABghstR8rxvN6Wu-ZNaOhZj5+h14ypeiMD8W9etPzoQW-wqZpw@mail.gmail.com>
Message-ID: <CACgv6yXwNcjLMqJ7TnoFe7cu3D3vEG=J17KaaRXhFDH8DbdZPQ@mail.gmail.com>

Dear James,

Thanks for your response. I should research this a bit more. I have the
feeling that HLM software under the *Latent Variable Regression *tab might
be doing something different. But I highly appreciate you insightful
response.

Dear Ben,  lmerControl(check.nobs.vs.nRE="ignore") didn't work.

library(lme4)
dat <- read.csv('https://raw.githubusercontent.com/hkil/m/master/z.csv')
m1 <- lmer(y~ year + (year|stid), data = dat,
           control=lmerControl(check.nobs.vs.nRE="ignore"))

On Tue, May 26, 2020 at 7:38 PM Ben Bolker <bbolker at gmail.com> wrote:

>    For what it's worth, *if* you're sufficiently sure that your model
> is identifiable, you can override the checks that test the relative
> numbers of observations/levels/etc.; see the "check.*" options in
> ?lme4::lmerControl, and set the relevant ones to "ignore"
>
> On Tue, May 26, 2020 at 7:12 PM Uanhoro, James
> <uanhoro.1 at buckeyemail.osu.edu> wrote:
> >
> > Hello Simon,
> >
> > I'm not sure what HLM does. However: if your question is about using
> > the random intercepts (individuals' starting points) to predict the
> > random slopes (their linear growth rate), then the model you need is:
> >
> > summary(m2 <- lmer(y ~ year + (1 + year | stid), dat))
> >
> > whcih returns the random intercept and a random slope on time.
> >
> > The correlation between both random effects is the regression
> > coefficient from regressing the slope on the intercept (or vice-versa)
> > when both variables are standardized.
> >
> > More generally, you can always obtain regression coefficients from a
> > correlation/covariance matrix of random effects. With a two-by-two
> > correlation matrix, the single correlation is the coefficient (in both
> > directions). In a larger matrix of random effects, you can use the
> > solve() function in R to obtain coefficients from the matrix. See here:
> > https://stackoverflow.com/questions/40762865/how-do-i-get-regression-
> > coefficients-from-a-variance-covariance-matrix-in-r
> >
> > I tried your exact example, and m2 above will not fit because some of
> > your participants have under 2 time points while the maximum number of
> > time points is 3, resulting in a situation where the software is
> > attempting to compute more random effect values than there are rows in
> > the data - the software complains. Also, it is a good idea to rescale
> > that y variable prior to data analysis. I was able to get the model to
> > run by limiting the data to cases with more than 1 recorded time point:
> >
> > summary(m3 <- lmer(y.s ~ year + (1 + year | stid), data = t.dat, subset
> > = n > 1))
> >
> > I arrived at a correlation/coefficient of -0.06.
> >
> > Hope this helps, -James.
> >
> > Sent from Outlook Mobile
> >
> > From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on
> > behalf of Simon Harmel <sim.harmel at gmail.com>
> > Sent: Tuesday, May 26, 2020, 18:27
> > To: r-sig-mixed-models
> > Subject: [R-sig-ME] Latent variable regression in lme4 as in HLM
> >
> > Dear All,
> >
> > I know that in the HLM software, it is possible to use "intercept"
> > (e.g.,
> > initial place of students at year "0") as the *predictor *of "slope"
> > (e.g.,
> > fixed rate of change in years) under the *Latent Variable Regression
> > *tab.
> >
> > I was wondering if this is also possible in "lme4" or any other
> > mixed-modeling packages in R?  *Thanks, Simon*
> >
> > *## Here is an example dataset for demonstration:*
> > library(lme4)
> > dat <- read.csv('
> >
> https://urldefense.com/v3/__https://raw.githubusercontent.com/hkil/m/master/z.csv__;!!KGKeukY!gjIgidLro6PaJJUHZOY1gk9IW8FfrzGWzo9IEaCRgFwkorvpE1tkLXqn3ujcsmy6OvxsB2xlUIM$
> >  ')
> > m1 <- lmer(y ~ year + (1|stid), data = dat)      #### 'stid' = student
> > id
> >
> >         [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> >
> >
> https://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models__;!!KGKeukY!gjIgidLro6PaJJUHZOY1gk9IW8FfrzGWzo9IEaCRgFwkorvpE1tkLXqn3ujcsmy6Ovxs517Hn00$
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Thu May 28 03:17:06 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 27 May 2020 21:17:06 -0400
Subject: [R-sig-ME] Latent variable regression in lme4 as in HLM
In-Reply-To: <CACgv6yXwNcjLMqJ7TnoFe7cu3D3vEG=J17KaaRXhFDH8DbdZPQ@mail.gmail.com>
References: <CACgv6yWJdDpVCCAnpe+FoQZqL83L=qPPWEkGwFJmXUNLomYRSw@mail.gmail.com>
 <df16541525bb3cea1d766f941bde9dd7abaafcb2.camel@buckeyemail.osu.edu>
 <CABghstR8rxvN6Wu-ZNaOhZj5+h14ypeiMD8W9etPzoQW-wqZpw@mail.gmail.com>
 <CACgv6yXwNcjLMqJ7TnoFe7cu3D3vEG=J17KaaRXhFDH8DbdZPQ@mail.gmail.com>
Message-ID: <88369ea4-9768-736d-1dd1-d4cb21ca6fb2@gmail.com>


On 5/26/20 10:12 PM, Simon Harmel wrote:
> Dear James,
>
> Thanks for your response. I should research this a bit more. I have 
> the feeling that HLM software under the /Latent Variable Regression 
> /tab might be doing something different. But I highly appreciate you 
> insightful response.
>
> Dear Ben, lmerControl(check.nobs.vs.nRE="ignore") didn't work.


 ?? Really?? (Whenever you say "didn't work" you should give more 
detail; as I show below, it worked on my platform.? That it didn't work 
for you could mean a lot of different things [e.g. it really didn't work 
on your computer due to differences in lme4 version or OS or R version 
or ... ; you misinterpreted the results; ... Knowing what didn't work 
gives us a big head-start in helping you resolve the problem ...)

 ?? I think it did what it was intended to do, which is bypass the check 
of numbers of obs vs number of random effect levels.

 ? I get:

 ?? Warning message:
In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,? :
 ? Model failed to converge with max|grad| = 0.0028078 (tol = 0.002, 
component 1)

 ? However, the results look sensible.? Further exploration:

 ?? aa <- allFit(m1)

 > aa
original model:
y ~ year + (year | stid)
data:? dat
optimizers (7): bobyqa, Nelder_Mead, nlminbwrap, nmkbw, 
optimx.L-BFGS-B,nloptwrap.NLOPT_LN_N...
differences in negative log-likelihoods:
max= 4.25e-06 ; std dev= 1.58e-06


summary(aa)

 ? Differences among parameter estimates, log-likelihoods, etc. are all 
very small, so the convergence warning is a false positive.

 ? cheers

 ??? Ben Bolker


>
> library(lme4)
> dat <- read.csv('https://raw.githubusercontent.com/hkil/m/master/z.csv')
> m1 <- lmer(y~ year + (year|stid), data = dat,
> ?control=lmerControl(check.nobs.vs.nRE="ignore"))
>
> On Tue, May 26, 2020 at 7:38 PM Ben Bolker <bbolker at gmail.com 
> <mailto:bbolker at gmail.com>> wrote:
>
>     ?For what it's worth, *if* you're sufficiently sure that your model
>     is identifiable, you can override the checks that test the relative
>     numbers of observations/levels/etc.; see the "check.*" options in
>     ?lme4::lmerControl, and set the relevant ones to "ignore"
>
>     On Tue, May 26, 2020 at 7:12 PM Uanhoro, James
>     <uanhoro.1 at buckeyemail.osu.edu
>     <mailto:uanhoro.1 at buckeyemail.osu.edu>> wrote:
>     >
>     > Hello Simon,
>     >
>     > I'm not sure what HLM does. However: if your question is about using
>     > the random intercepts (individuals' starting points) to predict the
>     > random slopes (their linear growth rate), then the model you
>     need is:
>     >
>     > summary(m2 <- lmer(y ~ year + (1 + year | stid), dat))
>     >
>     > whcih returns the random intercept and a random slope on time.
>     >
>     > The correlation between both random effects is the regression
>     > coefficient from regressing the slope on the intercept (or
>     vice-versa)
>     > when both variables are standardized.
>     >
>     > More generally, you can always obtain regression coefficients from a
>     > correlation/covariance matrix of random effects. With a two-by-two
>     > correlation matrix, the single correlation is the coefficient
>     (in both
>     > directions). In a larger matrix of random effects, you can use the
>     > solve() function in R to obtain coefficients from the matrix.
>     See here:
>     >
>     https://stackoverflow.com/questions/40762865/how-do-i-get-regression-
>     > coefficients-from-a-variance-covariance-matrix-in-r
>     >
>     > I tried your exact example, and m2 above will not fit because
>     some of
>     > your participants have under 2 time points while the maximum
>     number of
>     > time points is 3, resulting in a situation where the software is
>     > attempting to compute more random effect values than there are
>     rows in
>     > the data - the software complains. Also, it is a good idea to
>     rescale
>     > that y variable prior to data analysis. I was able to get the
>     model to
>     > run by limiting the data to cases with more than 1 recorded time
>     point:
>     >
>     > summary(m3 <- lmer(y.s ~ year + (1 + year | stid), data = t.dat,
>     subset
>     > = n > 1))
>     >
>     > I arrived at a correlation/coefficient of -0.06.
>     >
>     > Hope this helps, -James.
>     >
>     > Sent from Outlook Mobile
>     >
>     > From: R-sig-mixed-models
>     <r-sig-mixed-models-bounces at r-project.org
>     <mailto:r-sig-mixed-models-bounces at r-project.org>> on
>     > behalf of Simon Harmel <sim.harmel at gmail.com
>     <mailto:sim.harmel at gmail.com>>
>     > Sent: Tuesday, May 26, 2020, 18:27
>     > To: r-sig-mixed-models
>     > Subject: [R-sig-ME] Latent variable regression in lme4 as in HLM
>     >
>     > Dear All,
>     >
>     > I know that in the HLM software, it is possible to use "intercept"
>     > (e.g.,
>     > initial place of students at year "0") as the *predictor *of "slope"
>     > (e.g.,
>     > fixed rate of change in years) under the *Latent Variable Regression
>     > *tab.
>     >
>     > I was wondering if this is also possible in "lme4" or any other
>     > mixed-modeling packages in R?? *Thanks, Simon*
>     >
>     > *## Here is an example dataset for demonstration:*
>     > library(lme4)
>     > dat <- read.csv('
>     >
>     https://urldefense.com/v3/__https://raw.githubusercontent.com/hkil/m/master/z.csv__;!!KGKeukY!gjIgidLro6PaJJUHZOY1gk9IW8FfrzGWzo9IEaCRgFwkorvpE1tkLXqn3ujcsmy6OvxsB2xlUIM$
>     >? ')
>     > m1 <- lmer(y ~ year + (1|stid), data = dat)? ? ? #### 'stid' =
>     student
>     > id
>     >
>     >? ? ? ? ?[[alternative HTML version deleted]]
>     >
>     > _______________________________________________
>     > R-sig-mixed-models at r-project.org
>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>     >
>     >
>     https://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models__;!!KGKeukY!gjIgidLro6PaJJUHZOY1gk9IW8FfrzGWzo9IEaCRgFwkorvpE1tkLXqn3ujcsmy6Ovxs517Hn00$
>     >
>     > _______________________________________________
>     > R-sig-mixed-models at r-project.org
>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From x|nx|813 @end|ng |rom 126@com  Fri May 29 11:25:16 2020
From: x|nx|813 @end|ng |rom 126@com (=?ISO-8859-1?B?WUE=?=)
Date: Fri, 29 May 2020 17:25:16 +0800
Subject: [R-sig-ME] making predictions with MCMCglmm
Message-ID: <tencent_5701AD3CAB070C10E27910A2F3481E517A06@qq.com>

Dear list,


I am still working on the MCMCglmm predictions. I realized that I didnt provide a reproducible code in my last email, which makes people here lack of clues for helping me. I am providing a reproducible example this time using datasets from the nlme package, so if you have any experience on this package, please give me some advice on programming the predictions. As you can see below, same error occured on different models, I guess something is wrong with my code. Thank you very much.


&gt; library(MCMCglmm)
&gt; library(nlme)
&gt; data(MathAchieve,package='nlme')
&gt; data(MathAchSchool,package='nlme')
&gt; dat=merge(MathAchSchool,MathAchieve,by='School')
&gt; dat$mathach[dat$MathAch<5]=0
&gt; dat$mathach[dat$MathAch&gt;=5 &amp; dat$MathAch<15]=1
&gt; dat$mathach[dat$MathAch&gt;=15]=2
&gt; dat$mathach=as.factor(dat$mathach)
&gt; str(dat)


# prediction on a continous outcome 'MathAch'

&gt; m1=MCMCglmm(MathAch~Sex+SES,random=~School+SES,dat=dat,verbose=F)
&gt; summary(m1)
&gt; predict(m1,data.frame(Sex='Male',SES=c(0.3,-0.8)),type='response')
Error in FUN(X[[i]], ...) : object 'MathAch' not found


# prediction on a binary outcome 'Sex'
&gt; m2=MCMCglmm(Sex~SES,random=~School+SES,data=dat,family='categorical',verbose=F) 
&gt; summary(m2)
&gt; predict(m2,data.frame(SES=0.5,School='1224'),marginal=NULL,type='response')
Error in FUN(X[[i]], ...) : object 'Sex' not found



# prediction on a three category unordered multinomial outcome 'mathach'

&gt; m3=MCMCglmm(mathach~SES,random=~School+SES,data=dat,rcov=~us(trait):units,family='categorical',verbose=F) 
&gt; summary(m3)
&gt; predict(m3,data.frame(SES=0.5,School='1224'),marginal=NULL,type='response')
Error in FUN(X[[i]], ...) : object 'mathach' not found


Best regards,


YA
	[[alternative HTML version deleted]]


From hou@|@y @end|ng |rom gm@||@com  Fri May 29 13:38:34 2020
From: hou@|@y @end|ng |rom gm@||@com (Tom Houslay)
Date: Fri, 29 May 2020 12:38:34 +0100
Subject: [R-sig-ME] making predictions with MCMCglmm
In-Reply-To: <mailman.18412.11.1590746402.44732.r-sig-mixed-models@r-project.org>
References: <mailman.18412.11.1590746402.44732.r-sig-mixed-models@r-project.org>
Message-ID: <CAErKyRrYzrwQ794H_znpvJxBMCKnEeGZLsOt=6YgvNXbtZyfZw@mail.gmail.com>

Hi YA,

In your new data frame you just need to add a column for the response
variable as well (set it to 0 or similar). You may have additional issues
but that should get you over that first hurdle.

Speaking of which, I've found when predicting from MCMCglmm that it doesn't
like it when you only have a single value for any of your fixed effects --
so you may want to expand your prediction data frames, for example for m1
you could predict on those 2 SES values for both sexes (even if you are
only interested in males, you can simply subset the predictions afterwards).

Cheers

Tom


On Fri, 29 May 2020 at 11:01, <r-sig-mixed-models-request at r-project.org>
wrote:
>
>
>
> Message: 1
> Date: Fri, 29 May 2020 17:25:16 +0800
> From: "YA" <xinxi813 at 126.com>
> To: "r-sig-mixed-models" <r-sig-mixed-models at r-project.org>
> Subject: [R-sig-ME] making predictions with MCMCglmm
> Message-ID: <tencent_5701AD3CAB070C10E27910A2F3481E517A06 at qq.com>
> Content-Type: text/plain; charset="utf-8"
>
> Dear list,
>
>
> I am still working on the MCMCglmm predictions. I realized that I didnt
provide a reproducible code in my last email, which makes people here lack
of clues for helping me. I am providing a reproducible example this time
using datasets from the nlme package, so if you have any experience on this
package, please give me some advice on programming the predictions. As you
can see below, same error occured on different models, I guess something is
wrong with my code. Thank you very much.
>
>
> &gt; library(MCMCglmm)
> &gt; library(nlme)
> &gt; data(MathAchieve,package='nlme')
> &gt; data(MathAchSchool,package='nlme')
> &gt; dat=merge(MathAchSchool,MathAchieve,by='School')
> &gt; dat$mathach[dat$MathAch<5]=0
> &gt; dat$mathach[dat$MathAch&gt;=5 &amp; dat$MathAch<15]=1
> &gt; dat$mathach[dat$MathAch&gt;=15]=2
> &gt; dat$mathach=as.factor(dat$mathach)
> &gt; str(dat)
>
>
> # prediction on a continous outcome 'MathAch'
>
> &gt; m1=MCMCglmm(MathAch~Sex+SES,random=~School+SES,dat=dat,verbose=F)
> &gt; summary(m1)
> &gt; predict(m1,data.frame(Sex='Male',SES=c(0.3,-0.8)),type='response')
> Error in FUN(X[[i]], ...) : object 'MathAch' not found
>
>
> # prediction on a binary outcome 'Sex'
> &gt;
m2=MCMCglmm(Sex~SES,random=~School+SES,data=dat,family='categorical',verbose=F)
> &gt; summary(m2)
> &gt;
predict(m2,data.frame(SES=0.5,School='1224'),marginal=NULL,type='response')
> Error in FUN(X[[i]], ...) : object 'Sex' not found
>
>
>
> # prediction on a three category unordered multinomial outcome 'mathach'
>
> &gt;
m3=MCMCglmm(mathach~SES,random=~School+SES,data=dat,rcov=~us(trait):units,family='categorical',verbose=F)
> &gt; summary(m3)
> &gt;
predict(m3,data.frame(SES=0.5,School='1224'),marginal=NULL,type='response')
> Error in FUN(X[[i]], ...) : object 'mathach' not found
>
>
> Best regards,
>
>
> YA
>         [[alternative HTML version deleted]]
>
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> R-sig-mixed-models mailing list
> R-sig-mixed-models at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
> ------------------------------
>
> End of R-sig-mixed-models Digest, Vol 161, Issue 44
> ***************************************************

	[[alternative HTML version deleted]]


From @reedt@8 @end|ng |rom gm@||@com  Fri May 29 18:21:32 2020
From: @reedt@8 @end|ng |rom gm@||@com (sree datta)
Date: Fri, 29 May 2020 12:21:32 -0400
Subject: [R-sig-ME] making predictions with MCMCglmm
In-Reply-To: <CAErKyRrYzrwQ794H_znpvJxBMCKnEeGZLsOt=6YgvNXbtZyfZw@mail.gmail.com>
References: <mailman.18412.11.1590746402.44732.r-sig-mixed-models@r-project.org>
 <CAErKyRrYzrwQ794H_znpvJxBMCKnEeGZLsOt=6YgvNXbtZyfZw@mail.gmail.com>
Message-ID: <CAHftDbhUaYCB3wAhMJCKhaqFKhnQ4=cK6ceajHT4Po1QwHf+Xw@mail.gmail.com>

Dear YA

I noticed  the following in your code:

> &gt; dat$mathach[dat$MathAch&gt;=15]=2
> &gt; dat$mathach=as.factor(dat$mathach

the dat$MathAch variable in line 1 will be processed as a variable
different from dat$mathach on line 2 - Is this what you planned for? Since
R is case-sensitive, this can create the problems you are seeing -
The error "*object mathach not found*" is likely a direct result of that.

Sree

On Fri, May 29, 2020 at 7:39 AM Tom Houslay <houslay at gmail.com> wrote:

> Hi YA,
>
> In your new data frame you just need to add a column for the response
> variable as well (set it to 0 or similar). You may have additional issues
> but that should get you over that first hurdle.
>
> Speaking of which, I've found when predicting from MCMCglmm that it doesn't
> like it when you only have a single value for any of your fixed effects --
> so you may want to expand your prediction data frames, for example for m1
> you could predict on those 2 SES values for both sexes (even if you are
> only interested in males, you can simply subset the predictions
> afterwards).
>
> Cheers
>
> Tom
>
>
> On Fri, 29 May 2020 at 11:01, <r-sig-mixed-models-request at r-project.org>
> wrote:
> >
> >
> >
> > Message: 1
> > Date: Fri, 29 May 2020 17:25:16 +0800
> > From: "YA" <xinxi813 at 126.com>
> > To: "r-sig-mixed-models" <r-sig-mixed-models at r-project.org>
> > Subject: [R-sig-ME] making predictions with MCMCglmm
> > Message-ID: <tencent_5701AD3CAB070C10E27910A2F3481E517A06 at qq.com>
> > Content-Type: text/plain; charset="utf-8"
> >
> > Dear list,
> >
> >
> > I am still working on the MCMCglmm predictions. I realized that I didnt
> provide a reproducible code in my last email, which makes people here lack
> of clues for helping me. I am providing a reproducible example this time
> using datasets from the nlme package, so if you have any experience on this
> package, please give me some advice on programming the predictions. As you
> can see below, same error occured on different models, I guess something is
> wrong with my code. Thank you very much.
> >
> >
> > &gt; library(MCMCglmm)
> > &gt; library(nlme)
> > &gt; data(MathAchieve,package='nlme')
> > &gt; data(MathAchSchool,package='nlme')
> > &gt; dat=merge(MathAchSchool,MathAchieve,by='School')
> > &gt; dat$mathach[dat$MathAch<5]=0
> > &gt; dat$mathach[dat$MathAch&gt;=5 &amp; dat$MathAch<15]=1
> > &gt; dat$mathach[dat$MathAch&gt;=15]=2
> > &gt; dat$mathach=as.factor(dat$mathach)
> > &gt; str(dat)
> >
> >
> > # prediction on a continous outcome 'MathAch'
> >
> > &gt; m1=MCMCglmm(MathAch~Sex+SES,random=~School+SES,dat=dat,verbose=F)
> > &gt; summary(m1)
> > &gt; predict(m1,data.frame(Sex='Male',SES=c(0.3,-0.8)),type='response')
> > Error in FUN(X[[i]], ...) : object 'MathAch' not found
> >
> >
> > # prediction on a binary outcome 'Sex'
> > &gt;
>
> m2=MCMCglmm(Sex~SES,random=~School+SES,data=dat,family='categorical',verbose=F)
> > &gt; summary(m2)
> > &gt;
> predict(m2,data.frame(SES=0.5,School='1224'),marginal=NULL,type='response')
> > Error in FUN(X[[i]], ...) : object 'Sex' not found
> >
> >
> >
> > # prediction on a three category unordered multinomial outcome 'mathach'
> >
> > &gt;
>
> m3=MCMCglmm(mathach~SES,random=~School+SES,data=dat,rcov=~us(trait):units,family='categorical',verbose=F)
> > &gt; summary(m3)
> > &gt;
> predict(m3,data.frame(SES=0.5,School='1224'),marginal=NULL,type='response')
> > Error in FUN(X[[i]], ...) : object 'mathach' not found
> >
> >
> > Best regards,
> >
> >
> > YA
> >         [[alternative HTML version deleted]]
> >
> >
> >
> > ------------------------------
> >
> > Subject: Digest Footer
> >
> > _______________________________________________
> > R-sig-mixed-models mailing list
> > R-sig-mixed-models at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
> >
> > ------------------------------
> >
> > End of R-sig-mixed-models Digest, Vol 161, Issue 44
> > ***************************************************
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com  Sat May 30 02:15:54 2020
From: b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com (Alessandra Bielli)
Date: Fri, 29 May 2020 18:15:54 -0600
Subject: [R-sig-ME] DHARMa package diagnostic plots give different results
Message-ID: <CA+6N3yVtHA9KpQZ6odkERhRPaUuqLZs5WZShkKCMmteFn18GqA@mail.gmail.com>

Dear list.

I wonder if anyone has experience in using the DHARMa package for model
diagnostics?

I am fitting a poisson GLMM of the type

glmer(captured ~ treatment + offset(log(Effort)) + (1|Trip_Code),
      data=x, family="poisson", glmerControl(optimizer = "bobyqa"))

where captured is a count and treatment is a categorical variable with two
levels (control and experiment). The dependent variable contains mainly
zeros (Fig 1).

I am running some diagnostic tests using the DHARMa package and I did not
detect any singularity, overdispersion or zero inflation.

When I check the quantile residuals plots though, using two different
methods a) and b) I obtain two different results.

a) simulationOutput <- simulateResiduals(fittedModel = m1)
plot(simulationOutput)

b) testQuantiles(simulationOutput)

With method a) no significant quantile deviation is detected (Fig 2, right
plot), while it is detected with method b) (Fig 3). However, I was
expecting a) and b) to give the same results since the R documentation
for testQuantiles {DHARMa} explains:
"the quantile test is automatically performed in
## Not run:
plot(simulationOutput)
plotResiduals(simulationOutput)"

I understand that method a) plots rank transformed model predictions, while
b) doesn't but I wouldn't think this affects the p-value, am I correct?
I also understand that method a) automatically adds some noise to the
residuals to maintain a uniform response (see:  integerResponse parameter).
Does method b) do the same? If it doesn't, could that be the reason for the
different results?

Many thanks,

Alessandra

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Fig1.pdf
Type: application/pdf
Size: 4436 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20200529/1db2cd05/attachment-0003.pdf>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Fig2.pdf
Type: application/pdf
Size: 38202 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20200529/1db2cd05/attachment-0004.pdf>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Fig3.pdf
Type: application/pdf
Size: 31505 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20200529/1db2cd05/attachment-0005.pdf>

From x|nx|813 @end|ng |rom 126@com  Sat May 30 03:54:21 2020
From: x|nx|813 @end|ng |rom 126@com (=?utf-8?B?WUE=?=)
Date: Sat, 30 May 2020 09:54:21 +0800
Subject: [R-sig-ME] making predictions with MCMCglmm
Message-ID: <tencent_0C3B982AC4872050E7C33CA4742F5A36D607@qq.com>

Hi guys,


Thank you very much for the response.


Sree:


MathAch is different from mathach, these are two different variables. The continuous 'MathAch' in the orignial dataset was used for the linear model m1, whereas the multinomial categorical 'mathach' I intentionally created was used for the subsequent multinomial logistic regression m3. Any other ideas?



Tom:
?

I tried add both value of Sex and created a value of 0 for the outcome MathAch in m1, I still got errors. It looks like MCMCglmm prediction has a different logic than the glm prediction? Please see the codes and errors blow:


&gt; predict(m1,data.frame(Sex='Male',SES=c(0.3,-0.8),MathAch=0),type='response')
Error in buildZ(rmodel.terms[r], data = data, nginverse = names(ginverse)) : 
&nbsp; object School not found


&gt; predict(m1,data.frame(Sex=c('Male','Female'),SES=c(0.3,-0.8),MathAch=0,School='1288'),type='response')
Error in predict.MCMCglmm(m1, data.frame(Sex = c("Male", "Female"), SES = c(0.3,&nbsp; : 
&nbsp; model for newdata has fixed effects not present in original model


&gt; predict(m1,data.frame(Sex=c('Male'),SES=c(0.3,-0.8),MathAch=0,School='1288'),type='response')
Error in `contrasts<-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]) : 
&nbsp; contrasts can be applied only to factors with 2 or more levels


&gt; predict(m1,data.frame(Sex=c('Male','Female'),SES=c(0.3,-0.8),School='1288'),type='response')
Error in FUN(X[[i]], ...) : object 'MathAch' not found







Cheers,


YA




------------------&nbsp;Original&nbsp;------------------
From:&nbsp;"sree datta"<sreedta8 at gmail.com&gt;;
Date:&nbsp;Sat, May 30, 2020 00:21 AM
To:&nbsp;"Tom Houslay"<houslay at gmail.com&gt;;
Cc:&nbsp;"r-sig-mixed-models"<r-sig-mixed-models at r-project.org&gt;;"xinxi813"<xinxi813 at 126.com&gt;;
Subject:&nbsp;Re: [R-sig-ME] making predictions with MCMCglmm



Dear YA

I noticed&nbsp; the following in your code:


&gt; &amp;gt; dat$mathach[dat$MathAch&amp;gt;=15]=2
&gt; &amp;gt; dat$mathach=as.factor(dat$mathach&nbsp;&nbsp;



the dat$MathAch variable in line 1 will be processed as a variable different from dat$mathach on line&nbsp;2 - Is this what you planned for? Since R is case-sensitive, this can create the problems&nbsp;you are seeing -&nbsp;
The error "object mathach not found" is likely a direct result of that.


Sree


On Fri, May 29, 2020 at 7:39 AM Tom Houslay <houslay at gmail.com&gt; wrote:

Hi YA,
 
 In your new data frame you just need to add a column for the response
 variable as well (set it to 0 or similar). You may have additional issues
 but that should get you over that first hurdle.
 
 Speaking of which, I've found when predicting from MCMCglmm that it doesn't
 like it when you only have a single value for any of your fixed effects --
 so you may want to expand your prediction data frames, for example for m1
 you could predict on those 2 SES values for both sexes (even if you are
 only interested in males, you can simply subset the predictions afterwards).
 
 Cheers
 
 Tom
 
 
 On Fri, 29 May 2020 at 11:01, <r-sig-mixed-models-request at r-project.org&gt;
 wrote:
 &gt;
 &gt;
 &gt;
 &gt; Message: 1
 &gt; Date: Fri, 29 May 2020 17:25:16 +0800
 &gt; From: "YA" <xinxi813 at 126.com&gt;
 &gt; To: "r-sig-mixed-models" <r-sig-mixed-models at r-project.org&gt;
 &gt; Subject: [R-sig-ME] making predictions with MCMCglmm
 &gt; Message-ID: <tencent_5701AD3CAB070C10E27910A2F3481E517A06 at qq.com&gt;
 &gt; Content-Type: text/plain; charset="utf-8"
 &gt;
 &gt; Dear list,
 &gt;
 &gt;
 &gt; I am still working on the MCMCglmm predictions. I realized that I didnt
 provide a reproducible code in my last email, which makes people here lack
 of clues for helping me. I am providing a reproducible example this time
 using datasets from the nlme package, so if you have any experience on this
 package, please give me some advice on programming the predictions. As you
 can see below, same error occured on different models, I guess something is
 wrong with my code. Thank you very much.
 &gt;
 &gt;
 &gt; &amp;gt; library(MCMCglmm)
 &gt; &amp;gt; library(nlme)
 &gt; &amp;gt; data(MathAchieve,package='nlme')
 &gt; &amp;gt; data(MathAchSchool,package='nlme')
 &gt; &amp;gt; dat=merge(MathAchSchool,MathAchieve,by='School')
 &gt; &amp;gt; dat$mathach[dat$MathAch<5]=0
 &gt; &amp;gt; dat$mathach[dat$MathAch&amp;gt;=5 &amp;amp; dat$MathAch<15]=1
 &gt; &amp;gt; dat$mathach[dat$MathAch&amp;gt;=15]=2
 &gt; &amp;gt; dat$mathach=as.factor(dat$mathach)
 &gt; &amp;gt; str(dat)
 &gt;
 &gt;
 &gt; # prediction on a continous outcome 'MathAch'
 &gt;
 &gt; &amp;gt; m1=MCMCglmm(MathAch~Sex+SES,random=~School+SES,dat=dat,verbose=F)
 &gt; &amp;gt; summary(m1)
 &gt; &amp;gt; predict(m1,data.frame(Sex='Male',SES=c(0.3,-0.8)),type='response')
 &gt; Error in FUN(X[[i]], ...) : object 'MathAch' not found
 &gt;
 &gt;
 &gt; # prediction on a binary outcome 'Sex'
 &gt; &amp;gt;
 m2=MCMCglmm(Sex~SES,random=~School+SES,data=dat,family='categorical',verbose=F)
 &gt; &amp;gt; summary(m2)
 &gt; &amp;gt;
 predict(m2,data.frame(SES=0.5,School='1224'),marginal=NULL,type='response')
 &gt; Error in FUN(X[[i]], ...) : object 'Sex' not found
 &gt;
 &gt;
 &gt;
 &gt; # prediction on a three category unordered multinomial outcome 'mathach'
 &gt;
 &gt; &amp;gt;
 m3=MCMCglmm(mathach~SES,random=~School+SES,data=dat,rcov=~us(trait):units,family='categorical',verbose=F)
 &gt; &amp;gt; summary(m3)
 &gt; &amp;gt;
 predict(m3,data.frame(SES=0.5,School='1224'),marginal=NULL,type='response')
 &gt; Error in FUN(X[[i]], ...) : object 'mathach' not found
 &gt;
 &gt;
 &gt; Best regards,
 &gt;
 &gt;
 &gt; YA
 &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[[alternative HTML version deleted]]
 &gt;
 &gt;
 &gt;
 &gt; ------------------------------
 &gt;
 &gt; Subject: Digest Footer
 &gt;
 &gt; _______________________________________________
 &gt; R-sig-mixed-models mailing list
 &gt; R-sig-mixed-models at r-project.org
 &gt; https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
 &gt;
 &gt;
 &gt; ------------------------------
 &gt;
 &gt; End of R-sig-mixed-models Digest, Vol 161, Issue 44
 &gt; ***************************************************
 
 &nbsp; &nbsp; &nbsp; &nbsp; [[alternative HTML version deleted]]
 
 _______________________________________________
 R-sig-mixed-models at r-project.org mailing list
 https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
	[[alternative HTML version deleted]]


From m@tteo@@b@ @end|ng |rom gm@||@com  Mon Jun  1 12:37:54 2020
From: m@tteo@@b@ @end|ng |rom gm@||@com (Matteo Sebastianelli)
Date: Mon, 1 Jun 2020 13:37:54 +0300
Subject: [R-sig-ME] Negative binomial glmm estimates backtransformation and
 95% CI
Message-ID: <CAHLZGq-uw4A8M77ZQoi8BjjEAc_WR99Sm3jNVjdgmy=5qigGeg@mail.gmail.com>

Hi everyone,

I've been running a nbinom1 model in glmmTMB and need to show real values
for the estimates and 95% CI. Are estimates form nbinom1 glmmTMB
log-values? If so, is the transformation done by:

emmeans(model, "predictor", type="response")

Alternatively, calculating confidence intervals with "confint" and then
exponentiate them could be a possibility?

Best,

Matteo

	[[alternative HTML version deleted]]


From ecruh@ @end|ng |rom u@|@edu  Thu Jun  4 22:05:46 2020
From: ecruh@ @end|ng |rom u@|@edu (Ruhs, Emily)
Date: Thu, 4 Jun 2020 20:05:46 +0000
Subject: [R-sig-ME] Specifying starting point for MCMCglmm()
Message-ID: <60C2B040-F16F-4A66-B963-109BE574119B@usf.edu>

Hi everyone~

I am running a series of large models in R using MCMCglmm. After running the models for 14 days (complicated, multivariate models), I found that the models have not converged yet and I need to run more iterations. I know you can use the start= to specify a starting function, but I?m having difficulties getting the model to run.

model_EC <- parLapply(cl=cl,listEC, function(i) {
  MCMCglmm(i[[1]],
           random = ~us(trait):phylo, family=rep("gaussian", 6), rcov=~us(trait):units,
           ginverse=list(phylo=inv.phyloEC$Ainv),prior=prior.1,data=i[[2]], start=1820000,
           nitt=2600000,thin=1400,burnin=0)})

As the code is written, I have the start=1820000, which is where the last iteration left off. Can anyone explain what I?m doing wrong in specifying the start function?

Any help or suggestions would be greatly appreciated!

Best,
Emily Cornelius Ruhs
Postdoctoral Scholar
University of South Florida



	[[alternative HTML version deleted]]


From @@nder@8 @end|ng |rom y@hoo@com  Thu Jun  4 23:48:41 2020
From: @@nder@8 @end|ng |rom y@hoo@com (Austen Anderson)
Date: Thu, 4 Jun 2020 21:48:41 +0000 (UTC)
Subject: [R-sig-ME] Mixed effects model with many zeros
References: <1959298208.1021672.1591307321515.ref@mail.yahoo.com>
Message-ID: <1959298208.1021672.1591307321515@mail.yahoo.com>

Hi, I've got a set of longitudinal data with negative affect as the dependent variable. Negative affect was measured by 10 items asking about how much of the day the participant felt 10 different negative emotions (ordinal scale from 0-4). The modal response to that survey was 0 for all ten items, resulting in a large number of zero's for that variable along with a strong right skew. I've been exploring CrossValidated and other sources to get a sense of what my options are for modeling this data. I've read about Tweedie models, Tobit (censored) models, hurdle models, beta distribution models, and zero-inflated gamma models. As far as I could understand, the Tweedie model seemed reasonable and I modeled it this way:
neg_nat_mod_tweed <- glmmTMB(negaff ~ enjoynat_c + enjoynat_mean + daynum + (1|MRID),?? ? ? ? ? ? ? ? ? ? ? ? ?data = daily,? ? ? ? ? ? ? ? ? ? ? ? ?family = tweedie)summary(neg_nat_mod_tweed)

Family: tweedie? ( log )
Formula:? ? ? ? ? negaff ~ enjoynat_c + enjoynat_mean + daynum + (1 | MRID)Data: daily
? ? ?AIC? ? ? BIC? ?logLik deviance df.resid?? 4637.0? ?4683.6? -2311.5? ?4623.0? ? ?5753?
Random effects:
Conditional model:?Groups Name? ? ? ? Variance Std.Dev.?MRID? ?(Intercept) 1.19? ? ?1.091? ?Number of obs: 5760, groups:? MRID, 782
Overdispersion parameter for tweedie family (): 0.248?
Conditional model:? ? ? ? ? ? ? ?Estimate Std. Error z value Pr(>|z|)? ??(Intercept)? ?-1.653574? ?0.075059 -22.030? < 2e-16 ***enjoynat_c? ? -0.132666? ?0.033037? -4.016 5.93e-05 ***enjoynat_mean? 0.009201? ?0.134737? ?0.068? ? 0.946? ??daynum? ? ? ? -0.087213? ?0.005578 -15.636? < 2e-16 ***---Signif. codes:? 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

I have a few questions. First, does it seem like this is a reasonable way to analyze this data? If not, do you have other recommendations? Second, while the manual for GLMMtmb provides the Tweedie model as an option, here (https://cran.r-project.org/web/packages/glmmTMB/vignettes/glmmTMB.pdf) it says it is not yet implemented. Does anyone know if this model is trustworthy? Lastly, it mentions that the link function is log. I am still learning about how link functions work and I am not sure how to make sense of the coefficients because in their current form the negative intercept makes no sense. Can you offer some guidance on interpretation?
Thank you for your time,Austen
	[[alternative HTML version deleted]]


From Tom_Ph|||pp| @end|ng |rom np@@gov  Fri Jun  5 00:35:49 2020
From: Tom_Ph|||pp| @end|ng |rom np@@gov (Philippi, Tom)
Date: Thu, 4 Jun 2020 22:35:49 +0000
Subject: [R-sig-ME] Mixed effects model with many zeros
Message-ID: <BYAPR09MB3189D507568F22FB2C42C297F3890@BYAPR09MB3189.namprd09.prod.outlook.com>

No, to me  that does not seem like a reasonable way to analyze the data you describe, although I'm coming from a very different field so I could be off base.  The zero inflation and skew are artifacts of treating ordinal categories as numeric, and category "0" as numeric 0.

Is there a reason you don't want to model your ordered categorical response as ordered categorical?  There could be something about patterns across the 10 questions I'm missing, but for what you describe, perhaps ordinal::clmm() or something in the mixor package would do what you need.  

Note that glmmTMB is unlikely to add ordinal responses:
https://github.com/glmmTMB/glmmTMB/issues/514

Tom


-----Original Message-----
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of Austen Anderson via R-sig-mixed-models
Sent: Thursday, June 4, 2020 2:49 PM
To: r-sig-mixed-models at r-project.org
Subject: [EXTERNAL] [R-sig-ME] Mixed effects model with many zeros

Hi, I've got a set of longitudinal data with negative affect as the dependent variable. Negative affect was measured by 10 items asking about how much of the day the participant felt 10 different negative emotions (ordinal scale from 0-4). The modal response to that survey was 0 for all ten items, resulting in a large number of zero's for that variable along with a strong right skew. I've been exploring CrossValidated and other sources to get a sense of what my options are for modeling this data. I've read about Tweedie models, Tobit (censored) models, hurdle models, beta distribution models, and zero-inflated gamma models. As far as I could understand, the Tweedie model seemed reasonable and I modeled it this way:
neg_nat_mod_tweed <- glmmTMB(negaff ~ enjoynat_c + enjoynat_mean + daynum + (1|MRID),?? ? ? ? ? ? ? ? ? ? ? ? ?data = daily,? ? ? ? ? ? ? ? ? ? ? ? ?family = tweedie)summary(neg_nat_mod_tweed)

Family: tweedie? ( log )
Formula:? ? ? ? ? negaff ~ enjoynat_c + enjoynat_mean + daynum + (1 | MRID)Data: daily
? ? ?AIC? ? ? BIC? ?logLik deviance df.resid?? 4637.0? ?4683.6? -2311.5? ?4623.0? ? ?5753 Random effects:
Conditional model:?Groups Name? ? ? ? Variance Std.Dev.?MRID? ?(Intercept) 1.19? ? ?1.091? ?Number of obs: 5760, groups:? MRID, 782 Overdispersion parameter for tweedie family (): 0.248 Conditional model:? ? ? ? ? ? ? ?Estimate Std. Error z value Pr(>|z|)? ??(Intercept)? ?-1.653574? ?0.075059 -22.030? < 2e-16 ***enjoynat_c? ? -0.132666? ?0.033037? -4.016 5.93e-05 ***enjoynat_mean? 0.009201? ?0.134737? ?0.068? ? 0.946? ??daynum? ? ? ? -0.087213? ?0.005578 -15.636? < 2e-16 ***---Signif. codes:? 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

I have a few questions. First, does it seem like this is a reasonable way to analyze this data? If not, do you have other recommendations? Second, while the manual for GLMMtmb provides the Tweedie model as an option, here (https://cran.r-project.org/web/packages/glmmTMB/vignettes/glmmTMB.pdf) it says it is not yet implemented. Does anyone know if this model is trustworthy? Lastly, it mentions that the link function is log. I am still learning about how link functions work and I am not sure how to make sense of the coefficients because in their current form the negative intercept makes no sense. Can you offer some guidance on interpretation?
Thank you for your time,Austen
	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From p|erre@dev|||emereu|| @end|ng |rom ephe@p@|@eu  Fri Jun  5 09:44:38 2020
From: p|erre@dev|||emereu|| @end|ng |rom ephe@p@|@eu (Pierre de Villemereuil)
Date: Fri, 05 Jun 2020 09:44:38 +0200
Subject: [R-sig-ME] Specifying starting point for MCMCglmm()
In-Reply-To: <60C2B040-F16F-4A66-B963-109BE574119B@usf.edu>
References: <60C2B040-F16F-4A66-B963-109BE574119B@usf.edu>
Message-ID: <1609093.26T6xgk9g4@flyosfixe>

Hi,

The "start" argument of MCMCglmm() cannot be used to restart at a given iteration (and if it did, it would require a saved model to do so, since the algorithm is not deterministic).

>From the function help and "start" section:

> optional list having 4 possible elements: ?R? (R-structure)
> ?G? (G-structure) and ?liab? (latent variables or
> liabilities) should contain the starting values where ?G?
> itself is also a list with as many elements as random effect
> components. The fourth element ?QUASI? should be logical: if
> ?TRUE? starting latent variables are obtained heuristically,
> if ?FALSE? then they are sampled from a Z-distribution

So, you would need to extract such components from an already saved model. R and G components would be located in "model$VCV". Unless I'm misguided, you need to set "pl = TRUE" to save the latent variables that the "liab" component needs, which are then saved in the "model$Liab" component.

Note that I don't believe the information you can provide to "start" will be enough to restore your chain *exactly* at the state it was, but it should place you near enough a convergent state if drawn from a convergent state.

Cheers,
Pierre.


Le jeudi 4 juin 2020, 22:05:46 CEST Ruhs, Emily a ?crit :
> Hi everyone~
> 
> I am running a series of large models in R using MCMCglmm. After running the models for 14 days (complicated, multivariate models), I found that the models have not converged yet and I need to run more iterations. I know you can use the start= to specify a starting function, but I?m having difficulties getting the model to run.
> 
> model_EC <- parLapply(cl=cl,listEC, function(i) {
>   MCMCglmm(i[[1]],
>            random = ~us(trait):phylo, family=rep("gaussian", 6), rcov=~us(trait):units,
>            ginverse=list(phylo=inv.phyloEC$Ainv),prior=prior.1,data=i[[2]], start=1820000,
>            nitt=2600000,thin=1400,burnin=0)})
> 
> As the code is written, I have the start=1820000, which is where the last iteration left off. Can anyone explain what I?m doing wrong in specifying the start function?
> 
> Any help or suggestions would be greatly appreciated!
> 
> Best,
> Emily Cornelius Ruhs
> Postdoctoral Scholar
> University of South Florida
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 


From @reedt@8 @end|ng |rom gm@||@com  Fri Jun  5 21:47:52 2020
From: @reedt@8 @end|ng |rom gm@||@com (sree datta)
Date: Fri, 5 Jun 2020 15:47:52 -0400
Subject: [R-sig-ME] Specifying starting point for MCMCglmm()
In-Reply-To: <60C2B040-F16F-4A66-B963-109BE574119B@usf.edu>
References: <60C2B040-F16F-4A66-B963-109BE574119B@usf.edu>
Message-ID: <CAHftDbitZBKPCi9BDge8J6ZqYEZA=ECZVZLfAL51JvbAi7Xydw@mail.gmail.com>

Hi Emily

What are the specifications of your data you are using in the model that it
is taking so long (14 days)? How many variables and records are you using?
Have you attempted to run the model with a smaller subset of the data? If
yes, what were the results?

Sree


On Thu, Jun 4, 2020 at 4:10 PM Ruhs, Emily <ecruhs at usf.edu> wrote:

> Hi everyone~
>
> I am running a series of large models in R using MCMCglmm. After running
> the models for 14 days (complicated, multivariate models), I found that the
> models have not converged yet and I need to run more iterations. I know you
> can use the start= to specify a starting function, but I?m having
> difficulties getting the model to run.
>
> model_EC <- parLapply(cl=cl,listEC, function(i) {
>   MCMCglmm(i[[1]],
>            random = ~us(trait):phylo, family=rep("gaussian", 6),
> rcov=~us(trait):units,
>
>  ginverse=list(phylo=inv.phyloEC$Ainv),prior=prior.1,data=i[[2]],
> start=1820000,
>            nitt=2600000,thin=1400,burnin=0)})
>
> As the code is written, I have the start=1820000, which is where the last
> iteration left off. Can anyone explain what I?m doing wrong in specifying
> the start function?
>
> Any help or suggestions would be greatly appreciated!
>
> Best,
> Emily Cornelius Ruhs
> Postdoctoral Scholar
> University of South Florida
>
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From ecruh@ @end|ng |rom u@|@edu  Fri Jun  5 21:59:20 2020
From: ecruh@ @end|ng |rom u@|@edu (Ruhs, Emily)
Date: Fri, 5 Jun 2020 19:59:20 +0000
Subject: [R-sig-ME] Specifying starting point for MCMCglmm()
In-Reply-To: <CAHftDbitZBKPCi9BDge8J6ZqYEZA=ECZVZLfAL51JvbAi7Xydw@mail.gmail.com>
References: <60C2B040-F16F-4A66-B963-109BE574119B@usf.edu>
 <CAHftDbitZBKPCi9BDge8J6ZqYEZA=ECZVZLfAL51JvbAi7Xydw@mail.gmail.com>
Message-ID: <68F68E95-F41F-4C4F-8A73-19B2DEEF5EC0@usf.edu>

Hi Sree~
Thank you for reaching out.

I am running 5 models that are near the form:
EC1<-MCMCglmm(cbind(Infl.X, Infl.Y, Prop.Bottom, Prop.Top, Log.Slope, Coef) ~ (trait):log10Br+(trait):log10Bo,
              random = ~us(trait):phylo, family=rep("gaussian", 6), rcov=~us(trait):units,
              ginverse=list(phylo=inv.phyloEC$Ainv), prior=prior.1, data=speciesEC,
              nitt=NITT*Mult,thin=THIN*Mult,burnin=BURN*Mult)

I have 6 response variables and our models cover the full sweet of possibilities with log10Br and log10Bo as predictor variables (i.e. null model, log10Br alone, log10Bo alone, log10Br+log10Bo, log10Br*log10Bo). I have about 200 records (species) in the dataset.

I?ve gotten results from the models using Mult=7;NITT=260000;THIN=200;BURN=60000, but when we plot the trace and density, they still do not appear to be converging. Therefore I would like to extend to a Mult = 10.

Therefore, I was hoping to use the start= command to extend the models, without having to restart them; however, maybe that isn?t possible. Any advice you can provide would be greatly appreciated!
Best,
From: sree datta <sreedta8 at gmail.com>
Date: Friday, June 5, 2020 at 3:48 PM
To: "Ruhs, Emily" <ecruhs at usf.edu>
Cc: "r-sig-mixed-models at r-project.org" <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Specifying starting point for MCMCglmm()

This email originated from outside of USF. Do not click links or open attachments unless you recognize the sender or understand the content is safe.

Hi Emily

What are the specifications of your data you are using in the model that it is taking so long (14 days)? How many variables and records are you using? Have you attempted to run the model with a smaller subset of the data? If yes, what were the results?

Sree


On Thu, Jun 4, 2020 at 4:10 PM Ruhs, Emily <ecruhs at usf.edu<mailto:ecruhs at usf.edu>> wrote:
Hi everyone~

I am running a series of large models in R using MCMCglmm. After running the models for 14 days (complicated, multivariate models), I found that the models have not converged yet and I need to run more iterations. I know you can use the start= to specify a starting function, but I?m having difficulties getting the model to run.

model_EC <- parLapply(cl=cl,listEC, function(i) {
  MCMCglmm(i[[1]],
           random = ~us(trait):phylo, family=rep("gaussian", 6), rcov=~us(trait):units,
           ginverse=list(phylo=inv.phyloEC$Ainv),prior=prior.1,data=i[[2]], start=1820000,
           nitt=2600000,thin=1400,burnin=0)})

As the code is written, I have the start=1820000, which is where the last iteration left off. Can anyone explain what I?m doing wrong in specifying the start function?

Any help or suggestions would be greatly appreciated!

Best,
Emily Cornelius Ruhs
Postdoctoral Scholar
University of South Florida



        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models<https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-mixed-models&data=02%7C01%7Cecruhs%40usf.edu%7C23f1361618b44cd58c4508d809897102%7C741bf7dee2e546df8d6782607df9deaa%7C0%7C0%7C637269833193807480&sdata=yhwaEv0ogZ%2F4pcptmo1nC%2F2iGniKk3nzpXQ9VYmg%2B%2F8%3D&reserved=0>

	[[alternative HTML version deleted]]


From @reedt@8 @end|ng |rom gm@||@com  Fri Jun  5 22:45:58 2020
From: @reedt@8 @end|ng |rom gm@||@com (sree datta)
Date: Fri, 5 Jun 2020 16:45:58 -0400
Subject: [R-sig-ME] Specifying starting point for MCMCglmm()
In-Reply-To: <68F68E95-F41F-4C4F-8A73-19B2DEEF5EC0@usf.edu>
References: <60C2B040-F16F-4A66-B963-109BE574119B@usf.edu>
 <CAHftDbitZBKPCi9BDge8J6ZqYEZA=ECZVZLfAL51JvbAi7Xydw@mail.gmail.com>
 <68F68E95-F41F-4C4F-8A73-19B2DEEF5EC0@usf.edu>
Message-ID: <CAHftDbjhrHSUWWGQiM0XYnq66Q03wdWTDPmTGuWNg1dSkuvBiw@mail.gmail.com>

When you say 200 records, is this the total number of rows in the dataset?
What distribution are you assuming for your dependent variables? What
frequentist approach would you use instead of MCMC to model this data?

Sree

On Fri, Jun 5, 2020 at 3:59 PM Ruhs, Emily <ecruhs at usf.edu> wrote:

> Hi Sree~
>
> Thank you for reaching out.
>
>
>
> I am running 5 models that are near the form:
>
> EC1<-MCMCglmm(cbind(Infl.X, Infl.Y, Prop.Bottom, Prop.Top, Log.Slope,
> Coef) ~ (trait):log10Br+(trait):log10Bo,
>
>               random = ~us(trait):phylo, family=rep("gaussian", 6),
> rcov=~us(trait):units,
>
>               ginverse=list(phylo=inv.phyloEC$Ainv), prior=prior.1,
> data=speciesEC,
>
>               nitt=NITT*Mult,thin=THIN*Mult,burnin=BURN*Mult)
>
>
>
> I have 6 response variables and our models cover the full sweet of
> possibilities with log10Br and log10Bo as predictor variables (i.e. null
> model, log10Br alone, log10Bo alone, log10Br+log10Bo, log10Br*log10Bo). I
> have about 200 records (species) in the dataset.
>
>
>
> I?ve gotten results from the models using
> Mult=7;NITT=260000;THIN=200;BURN=60000, but when we plot the trace and
> density, they still do not appear to be converging. Therefore I would like
> to extend to a Mult = 10.
>
>
>
> Therefore, I was hoping to use the start= command to extend the models,
> without having to restart them; however, maybe that isn?t possible. Any
> advice you can provide would be greatly appreciated!
>
> Best,
>
> *From: *sree datta <sreedta8 at gmail.com>
> *Date: *Friday, June 5, 2020 at 3:48 PM
> *To: *"Ruhs, Emily" <ecruhs at usf.edu>
> *Cc: *"r-sig-mixed-models at r-project.org" <r-sig-mixed-models at r-project.org
> >
> *Subject: *Re: [R-sig-ME] Specifying starting point for MCMCglmm()
>
>
>
> This email originated from outside of USF. Do not click links or open
> attachments unless you recognize the sender or understand the content is
> safe.
>
>
>
> Hi Emily
>
>
>
> What are the specifications of your data you are using in the model that
> it is taking so long (14 days)? How many variables and records are you
> using? Have you attempted to run the model with a smaller subset of the
> data? If yes, what were the results?
>
>
>
> Sree
>
>
>
>
>
> On Thu, Jun 4, 2020 at 4:10 PM Ruhs, Emily <ecruhs at usf.edu> wrote:
>
> Hi everyone~
>
> I am running a series of large models in R using MCMCglmm. After running
> the models for 14 days (complicated, multivariate models), I found that the
> models have not converged yet and I need to run more iterations. I know you
> can use the start= to specify a starting function, but I?m having
> difficulties getting the model to run.
>
> model_EC <- parLapply(cl=cl,listEC, function(i) {
>   MCMCglmm(i[[1]],
>            random = ~us(trait):phylo, family=rep("gaussian", 6),
> rcov=~us(trait):units,
>
>  ginverse=list(phylo=inv.phyloEC$Ainv),prior=prior.1,data=i[[2]],
> start=1820000,
>            nitt=2600000,thin=1400,burnin=0)})
>
> As the code is written, I have the start=1820000, which is where the last
> iteration left off. Can anyone explain what I?m doing wrong in specifying
> the start function?
>
> Any help or suggestions would be greatly appreciated!
>
> Best,
> Emily Cornelius Ruhs
> Postdoctoral Scholar
> University of South Florida
>
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> <https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-mixed-models&data=02%7C01%7Cecruhs%40usf.edu%7C23f1361618b44cd58c4508d809897102%7C741bf7dee2e546df8d6782607df9deaa%7C0%7C0%7C637269833193807480&sdata=yhwaEv0ogZ%2F4pcptmo1nC%2F2iGniKk3nzpXQ9VYmg%2B%2F8%3D&reserved=0>
>
>

	[[alternative HTML version deleted]]


From ecruh@ @end|ng |rom u@|@edu  Mon Jun  8 14:49:31 2020
From: ecruh@ @end|ng |rom u@|@edu (Ruhs, Emily)
Date: Mon, 8 Jun 2020 12:49:31 +0000
Subject: [R-sig-ME] Specifying starting point for MCMCglmm()
In-Reply-To: <CAHftDbjhrHSUWWGQiM0XYnq66Q03wdWTDPmTGuWNg1dSkuvBiw@mail.gmail.com>
References: <60C2B040-F16F-4A66-B963-109BE574119B@usf.edu>
 <CAHftDbitZBKPCi9BDge8J6ZqYEZA=ECZVZLfAL51JvbAi7Xydw@mail.gmail.com>
 <68F68E95-F41F-4C4F-8A73-19B2DEEF5EC0@usf.edu>
 <CAHftDbjhrHSUWWGQiM0XYnq66Q03wdWTDPmTGuWNg1dSkuvBiw@mail.gmail.com>
Message-ID: <3D342AE1-D4F4-4976-B645-538298E58522@usf.edu>

Dear Sree~
Yes, there are 200 rows of data in the dataset. We are assuming a gaussian distribution in our models. If we were taking a frequentist approach, it would be a glmm (mixed model).

Thank you,
From: sree datta <sreedta8 at gmail.com>
Date: Friday, June 5, 2020 at 4:46 PM
To: "Ruhs, Emily" <ecruhs at usf.edu>
Cc: "r-sig-mixed-models at r-project.org" <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Specifying starting point for MCMCglmm()

This email originated from outside of USF. Do not click links or open attachments unless you recognize the sender or understand the content is safe.

When you say 200 records, is this the total number of rows in the dataset? What distribution are you assuming for your dependent variables? What frequentist approach would you use instead of MCMC to model this data?

Sree

On Fri, Jun 5, 2020 at 3:59 PM Ruhs, Emily <ecruhs at usf.edu<mailto:ecruhs at usf.edu>> wrote:
Hi Sree~
Thank you for reaching out.

I am running 5 models that are near the form:
EC1<-MCMCglmm(cbind(Infl.X, Infl.Y, Prop.Bottom, Prop.Top, Log.Slope, Coef) ~ (trait):log10Br+(trait):log10Bo,
              random = ~us(trait):phylo, family=rep("gaussian", 6), rcov=~us(trait):units,
              ginverse=list(phylo=inv.phyloEC$Ainv), prior=prior.1, data=speciesEC,
              nitt=NITT*Mult,thin=THIN*Mult,burnin=BURN*Mult)

I have 6 response variables and our models cover the full sweet of possibilities with log10Br and log10Bo as predictor variables (i.e. null model, log10Br alone, log10Bo alone, log10Br+log10Bo, log10Br*log10Bo). I have about 200 records (species) in the dataset.

I?ve gotten results from the models using Mult=7;NITT=260000;THIN=200;BURN=60000, but when we plot the trace and density, they still do not appear to be converging. Therefore I would like to extend to a Mult = 10.

Therefore, I was hoping to use the start= command to extend the models, without having to restart them; however, maybe that isn?t possible. Any advice you can provide would be greatly appreciated!
Best,
From: sree datta <sreedta8 at gmail.com<mailto:sreedta8 at gmail.com>>
Date: Friday, June 5, 2020 at 3:48 PM
To: "Ruhs, Emily" <ecruhs at usf.edu<mailto:ecruhs at usf.edu>>
Cc: "r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>" <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
Subject: Re: [R-sig-ME] Specifying starting point for MCMCglmm()

This email originated from outside of USF. Do not click links or open attachments unless you recognize the sender or understand the content is safe.

Hi Emily

What are the specifications of your data you are using in the model that it is taking so long (14 days)? How many variables and records are you using? Have you attempted to run the model with a smaller subset of the data? If yes, what were the results?

Sree


On Thu, Jun 4, 2020 at 4:10 PM Ruhs, Emily <ecruhs at usf.edu<mailto:ecruhs at usf.edu>> wrote:
Hi everyone~

I am running a series of large models in R using MCMCglmm. After running the models for 14 days (complicated, multivariate models), I found that the models have not converged yet and I need to run more iterations. I know you can use the start= to specify a starting function, but I?m having difficulties getting the model to run.

model_EC <- parLapply(cl=cl,listEC, function(i) {
  MCMCglmm(i[[1]],
           random = ~us(trait):phylo, family=rep("gaussian", 6), rcov=~us(trait):units,
           ginverse=list(phylo=inv.phyloEC$Ainv),prior=prior.1,data=i[[2]], start=1820000,
           nitt=2600000,thin=1400,burnin=0)})

As the code is written, I have the start=1820000, which is where the last iteration left off. Can anyone explain what I?m doing wrong in specifying the start function?

Any help or suggestions would be greatly appreciated!

Best,
Emily Cornelius Ruhs
Postdoctoral Scholar
University of South Florida



        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models<https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-mixed-models&data=02%7C01%7Cecruhs%40usf.edu%7Cd4f17c041141495449f608d809918cfe%7C741bf7dee2e546df8d6782607df9deaa%7C0%7C0%7C637269868027355896&sdata=DD3L3yG54YsZVnAUp8F3WkrayALdVVQosaUGZDow8Kc%3D&reserved=0>

	[[alternative HTML version deleted]]


From @reedt@8 @end|ng |rom gm@||@com  Tue Jun  9 00:54:58 2020
From: @reedt@8 @end|ng |rom gm@||@com (sree datta)
Date: Mon, 8 Jun 2020 18:54:58 -0400
Subject: [R-sig-ME] Specifying starting point for MCMCglmm()
In-Reply-To: <3D342AE1-D4F4-4976-B645-538298E58522@usf.edu>
References: <60C2B040-F16F-4A66-B963-109BE574119B@usf.edu>
 <CAHftDbitZBKPCi9BDge8J6ZqYEZA=ECZVZLfAL51JvbAi7Xydw@mail.gmail.com>
 <68F68E95-F41F-4C4F-8A73-19B2DEEF5EC0@usf.edu>
 <CAHftDbjhrHSUWWGQiM0XYnq66Q03wdWTDPmTGuWNg1dSkuvBiw@mail.gmail.com>
 <3D342AE1-D4F4-4976-B645-538298E58522@usf.edu>
Message-ID: <CAHftDbiv63Mzg7csksZ3fnCBFvyJPSxemv6HX_vMXTHB=HttpA@mail.gmail.com>

That is helpful to know. Did you run the glmm model? What results did you
get?

Sree

On Mon, Jun 8, 2020 at 8:49 AM Ruhs, Emily <ecruhs at usf.edu> wrote:

> Dear Sree~
>
> Yes, there are 200 rows of data in the dataset. We are assuming a gaussian
> distribution in our models. If we were taking a frequentist approach, it
> would be a glmm (mixed model).
>
>
>
> Thank you,
>
> *From: *sree datta <sreedta8 at gmail.com>
> *Date: *Friday, June 5, 2020 at 4:46 PM
> *To: *"Ruhs, Emily" <ecruhs at usf.edu>
> *Cc: *"r-sig-mixed-models at r-project.org" <r-sig-mixed-models at r-project.org
> >
> *Subject: *Re: [R-sig-ME] Specifying starting point for MCMCglmm()
>
>
>
> This email originated from outside of USF. Do not click links or open
> attachments unless you recognize the sender or understand the content is
> safe.
>
>
>
> When you say 200 records, is this the total number of rows in the dataset?
> What distribution are you assuming for your dependent variables? What
> frequentist approach would you use instead of MCMC to model this data?
>
>
>
> Sree
>
>
>
> On Fri, Jun 5, 2020 at 3:59 PM Ruhs, Emily <ecruhs at usf.edu> wrote:
>
> Hi Sree~
>
> Thank you for reaching out.
>
>
>
> I am running 5 models that are near the form:
>
> EC1<-MCMCglmm(cbind(Infl.X, Infl.Y, Prop.Bottom, Prop.Top, Log.Slope,
> Coef) ~ (trait):log10Br+(trait):log10Bo,
>
>               random = ~us(trait):phylo, family=rep("gaussian", 6),
> rcov=~us(trait):units,
>
>               ginverse=list(phylo=inv.phyloEC$Ainv), prior=prior.1,
> data=speciesEC,
>
>               nitt=NITT*Mult,thin=THIN*Mult,burnin=BURN*Mult)
>
>
>
> I have 6 response variables and our models cover the full sweet of
> possibilities with log10Br and log10Bo as predictor variables (i.e. null
> model, log10Br alone, log10Bo alone, log10Br+log10Bo, log10Br*log10Bo). I
> have about 200 records (species) in the dataset.
>
>
>
> I?ve gotten results from the models using
> Mult=7;NITT=260000;THIN=200;BURN=60000, but when we plot the trace and
> density, they still do not appear to be converging. Therefore I would like
> to extend to a Mult = 10.
>
>
>
> Therefore, I was hoping to use the start= command to extend the models,
> without having to restart them; however, maybe that isn?t possible. Any
> advice you can provide would be greatly appreciated!
>
> Best,
>
> *From: *sree datta <sreedta8 at gmail.com>
> *Date: *Friday, June 5, 2020 at 3:48 PM
> *To: *"Ruhs, Emily" <ecruhs at usf.edu>
> *Cc: *"r-sig-mixed-models at r-project.org" <r-sig-mixed-models at r-project.org
> >
> *Subject: *Re: [R-sig-ME] Specifying starting point for MCMCglmm()
>
>
>
> This email originated from outside of USF. Do not click links or open
> attachments unless you recognize the sender or understand the content is
> safe.
>
>
>
> Hi Emily
>
>
>
> What are the specifications of your data you are using in the model that
> it is taking so long (14 days)? How many variables and records are you
> using? Have you attempted to run the model with a smaller subset of the
> data? If yes, what were the results?
>
>
>
> Sree
>
>
>
>
>
> On Thu, Jun 4, 2020 at 4:10 PM Ruhs, Emily <ecruhs at usf.edu> wrote:
>
> Hi everyone~
>
> I am running a series of large models in R using MCMCglmm. After running
> the models for 14 days (complicated, multivariate models), I found that the
> models have not converged yet and I need to run more iterations. I know you
> can use the start= to specify a starting function, but I?m having
> difficulties getting the model to run.
>
> model_EC <- parLapply(cl=cl,listEC, function(i) {
>   MCMCglmm(i[[1]],
>            random = ~us(trait):phylo, family=rep("gaussian", 6),
> rcov=~us(trait):units,
>
>  ginverse=list(phylo=inv.phyloEC$Ainv),prior=prior.1,data=i[[2]],
> start=1820000,
>            nitt=2600000,thin=1400,burnin=0)})
>
> As the code is written, I have the start=1820000, which is where the last
> iteration left off. Can anyone explain what I?m doing wrong in specifying
> the start function?
>
> Any help or suggestions would be greatly appreciated!
>
> Best,
> Emily Cornelius Ruhs
> Postdoctoral Scholar
> University of South Florida
>
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> <https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-mixed-models&data=02%7C01%7Cecruhs%40usf.edu%7Cd4f17c041141495449f608d809918cfe%7C741bf7dee2e546df8d6782607df9deaa%7C0%7C0%7C637269868027355896&sdata=DD3L3yG54YsZVnAUp8F3WkrayALdVVQosaUGZDow8Kc%3D&reserved=0>
>
>

	[[alternative HTML version deleted]]


From v|t@|@he|m @end|ng |rom gm@||@com  Tue Jun  9 12:49:09 2020
From: v|t@|@he|m @end|ng |rom gm@||@com (Vital Heim)
Date: Tue, 9 Jun 2020 12:49:09 +0200
Subject: [R-sig-ME] Poor mixing and autocorrelation of ZIP data in MCMCglmm
Message-ID: <CAJkG0X9wdN=UakXkyBUeCfDaQm4uR5r72QORZB8zEdTvKbk-Zw@mail.gmail.com>

Good morning,

Currently, I am working on a Zero-Inflated Poisson Generalized Linear Mixed
Model using the MCMCglmm package in R. I am looking at the food uptake of
animal at a tourism provisioning site. I try to understand what variables
impact the amount of food/bait the animals consume.

The response variable (bait uptake) is zero-inflated. I have 6 fixed
effects (nr. tourists, nr. operators at the site, nr. conspecifics,
temperature, presence time of the animal at the site, and gender of the
animal). Presence time of the animal and temperature are only entered
at.level(trait, 1), all the other fixed effects are entered into the ZI
part and the count part of the model. I have two random effects for ID of
the animal and the provisioning event.

Here is the model specification:


modelZI.9 <- MCMCglmm(bait ~ trait - 1

                      + trait:conspecifics + trait:tourists +
trait:operators + at.level(trait,1):presence +
at.level(trait,1):temperature

                      + trait:gender

                      , random = ~idh(trait):id + idh(trait):event

                      , rcov =~ idh(trait):units

                      , data=df, family="zipoisson", prior=prior6

                      , nitt=NITT*MULT, burnin=BURN*MULT, thin = THIN*MULT,
verbose=FALSE)




I started with the chain parameters: nitt = 120000, burn = 20000, thin =
100 to get an effective sample size of 1000. My first prior was:



prior.base<-list(R=list(V=diag(c(1,1)), nu=0.002, fix=2)

            ,G=list(G1=list(V=diag(c(1,1)), nu=0.002, fix=2)

            ,G2=list(V=diag(c(1,1)), nu=0.002, fix=2)))



The chosen parameters and prior resulted in a very poor mixing of the ZI
effects as well as the both random effects. The effective sample sizes for
the ZI effects were very low (less than 100). I adjusted the chain
parameters (see below) I also adjusted the prior to account for
overdispersion in the fixed effects and making the prior stronger for the
random intercepts. There was autocorrelation in my ZI fixed effects so I
increased the thinning while increasing nitt and burn accordingly to get an
effective size of 1100. These measures improved the model in some parts.
However, I tried to validate my prior choice and parameters settings by
reading the literature to see if I am not doing anything ?illegal? with my
model. By now the parameters and prior gives me slightly better mixing. I
was able to get the random effect ID mixing nicely but only when I set the
nu = 100.002, which seems very high to me.

 These are my current chain parameters and my prior:

NITT <- 185000

BURN <- 15000

THIN <- 150

MULT <- 10



prior6<-list(R=list(V=diag(c(1e-6,1)), nu=10.002, fix=2)

             ,G=list(G1=list(V=diag(c(1,1)), nu=100.002, fix=2)

             ,G2=list(V=diag(c(1,1)), nu=0.002, fix=2)))



The autocorrelation diagnostics are:

0.075907049                 NaN

 > autocorr.diag(modelZI.9$Sol)
           bait          zi_bait      bait:conspecifics
zi_bait:conspecifics   bait:tourists     zi_bait:tourists   bait:operators
Lag 0      1.00000000    1.00000000      1.0000000000           1.00000000
       1.00000000          1.000000000      1.000000000
Lag 1500  -0.03158767    0.45992662     -0.0216380764           0.52372246
      -0.02650593          0.546651545     -0.027195064
Lag 7500   0.02284612    0.15705391      0.0114730973           0.12436415
       0.03518200          0.122140444     -0.010875426
Lag 15000  0.04558767    0.06901158     -0.0131587465           0.05700078
       0.05371373          0.054479522      0.024634961
Lag 75000  0.03567369    0.04099224     -0.0005695797           0.02014238
      -0.02546435         -0.006299296     -0.005569035
            zi_bait:operators     presence temperature
 bait:gendermale       zi_bait:gendermale
Lag 0              1.00000000  1.000000000  1.00000000
1.000000000               1.00000000
Lag 1500           0.43279808 -0.004789598 -0.03151510
 -0.002708028               0.33387297
Lag 7500           0.05697190  0.012426457  0.02221331
 -0.025070273               0.11236772
Lag 15000          0.05472752 -0.043356418  0.04499879
 -0.003589775               0.05618167
Lag 75000          0.03342174 -0.007475513  0.03201374
0.015790374              -0.03167048

> autocorr.diag(modelZI.9$VCV)
                bait.id       zi_bait.id       bait.dive
 zi_bait.dive       bait.units       zi_bait.units
Lag 0       1.000000000              NaN      1.00000000                NaN
     1.000000000                 NaN
Lag 1500   -0.026735489              NaN      0.03018431                NaN
     0.003349623                 NaN
Lag 7500   -0.051666715              NaN     -0.02011165                NaN
    -0.048462744                 NaN
Lag 15000   0.003918555              NaN     -0.02777388                NaN
    -0.001752476                 NaN
Lag 75000   0.028123317              NaN     -0.01640397                NaN
     0.075907049                 NaN



I therefore wanted to ask if there are suggestions on how I could improve
the mixing of my model without choosing these drastic changes or are these
changes in the range of what I am allowed to do to my model?


Thank you,

Vital
--
*Vital Heim *

*PhD student*
University of Basel, Switzerland

Bimini Biological Field Station Foundation
Bimini, Bahamas

+41 (0)79 732 05 57
vital.heim at gmail.com

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Tue Jun  9 15:17:57 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Tue, 9 Jun 2020 09:17:57 -0400
Subject: [R-sig-ME] 
 Poor mixing and autocorrelation of ZIP data in MCMCglmm
In-Reply-To: <CAJkG0X9wdN=UakXkyBUeCfDaQm4uR5r72QORZB8zEdTvKbk-Zw@mail.gmail.com>
References: <CAJkG0X9wdN=UakXkyBUeCfDaQm4uR5r72QORZB8zEdTvKbk-Zw@mail.gmail.com>
Message-ID: <b9bca7f7-e483-cafd-d262-7f3fd36dc8ff@gmail.com>

 ? Silly question: did you center all of your continuous predictors?

On 6/9/20 6:49 AM, Vital Heim wrote:
> Good morning,
>
> Currently, I am working on a Zero-Inflated Poisson Generalized Linear Mixed
> Model using the MCMCglmm package in R. I am looking at the food uptake of
> animal at a tourism provisioning site. I try to understand what variables
> impact the amount of food/bait the animals consume.
>
> The response variable (bait uptake) is zero-inflated. I have 6 fixed
> effects (nr. tourists, nr. operators at the site, nr. conspecifics,
> temperature, presence time of the animal at the site, and gender of the
> animal). Presence time of the animal and temperature are only entered
> at.level(trait, 1), all the other fixed effects are entered into the ZI
> part and the count part of the model. I have two random effects for ID of
> the animal and the provisioning event.
>
> Here is the model specification:
>
>
> modelZI.9 <- MCMCglmm(bait ~ trait - 1
>
>                        + trait:conspecifics + trait:tourists +
> trait:operators + at.level(trait,1):presence +
> at.level(trait,1):temperature
>
>                        + trait:gender
>
>                        , random = ~idh(trait):id + idh(trait):event
>
>                        , rcov =~ idh(trait):units
>
>                        , data=df, family="zipoisson", prior=prior6
>
>                        , nitt=NITT*MULT, burnin=BURN*MULT, thin = THIN*MULT,
> verbose=FALSE)
>
>
>
>
> I started with the chain parameters: nitt = 120000, burn = 20000, thin =
> 100 to get an effective sample size of 1000. My first prior was:
>
>
>
> prior.base<-list(R=list(V=diag(c(1,1)), nu=0.002, fix=2)
>
>              ,G=list(G1=list(V=diag(c(1,1)), nu=0.002, fix=2)
>
>              ,G2=list(V=diag(c(1,1)), nu=0.002, fix=2)))
>
>
>
> The chosen parameters and prior resulted in a very poor mixing of the ZI
> effects as well as the both random effects. The effective sample sizes for
> the ZI effects were very low (less than 100). I adjusted the chain
> parameters (see below) I also adjusted the prior to account for
> overdispersion in the fixed effects and making the prior stronger for the
> random intercepts. There was autocorrelation in my ZI fixed effects so I
> increased the thinning while increasing nitt and burn accordingly to get an
> effective size of 1100. These measures improved the model in some parts.
> However, I tried to validate my prior choice and parameters settings by
> reading the literature to see if I am not doing anything ?illegal? with my
> model. By now the parameters and prior gives me slightly better mixing. I
> was able to get the random effect ID mixing nicely but only when I set the
> nu = 100.002, which seems very high to me.
>
>   These are my current chain parameters and my prior:
>
> NITT <- 185000
>
> BURN <- 15000
>
> THIN <- 150
>
> MULT <- 10
>
>
>
> prior6<-list(R=list(V=diag(c(1e-6,1)), nu=10.002, fix=2)
>
>               ,G=list(G1=list(V=diag(c(1,1)), nu=100.002, fix=2)
>
>               ,G2=list(V=diag(c(1,1)), nu=0.002, fix=2)))
>
>
>
> The autocorrelation diagnostics are:
>
> 0.075907049                 NaN
>
>   > autocorr.diag(modelZI.9$Sol)
>             bait          zi_bait      bait:conspecifics
> zi_bait:conspecifics   bait:tourists     zi_bait:tourists   bait:operators
> Lag 0      1.00000000    1.00000000      1.0000000000           1.00000000
>         1.00000000          1.000000000      1.000000000
> Lag 1500  -0.03158767    0.45992662     -0.0216380764           0.52372246
>        -0.02650593          0.546651545     -0.027195064
> Lag 7500   0.02284612    0.15705391      0.0114730973           0.12436415
>         0.03518200          0.122140444     -0.010875426
> Lag 15000  0.04558767    0.06901158     -0.0131587465           0.05700078
>         0.05371373          0.054479522      0.024634961
> Lag 75000  0.03567369    0.04099224     -0.0005695797           0.02014238
>        -0.02546435         -0.006299296     -0.005569035
>              zi_bait:operators     presence temperature
>   bait:gendermale       zi_bait:gendermale
> Lag 0              1.00000000  1.000000000  1.00000000
> 1.000000000               1.00000000
> Lag 1500           0.43279808 -0.004789598 -0.03151510
>   -0.002708028               0.33387297
> Lag 7500           0.05697190  0.012426457  0.02221331
>   -0.025070273               0.11236772
> Lag 15000          0.05472752 -0.043356418  0.04499879
>   -0.003589775               0.05618167
> Lag 75000          0.03342174 -0.007475513  0.03201374
> 0.015790374              -0.03167048
>
>> autocorr.diag(modelZI.9$VCV)
>                  bait.id       zi_bait.id       bait.dive
>   zi_bait.dive       bait.units       zi_bait.units
> Lag 0       1.000000000              NaN      1.00000000                NaN
>       1.000000000                 NaN
> Lag 1500   -0.026735489              NaN      0.03018431                NaN
>       0.003349623                 NaN
> Lag 7500   -0.051666715              NaN     -0.02011165                NaN
>      -0.048462744                 NaN
> Lag 15000   0.003918555              NaN     -0.02777388                NaN
>      -0.001752476                 NaN
> Lag 75000   0.028123317              NaN     -0.01640397                NaN
>       0.075907049                 NaN
>
>
>
> I therefore wanted to ask if there are suggestions on how I could improve
> the mixing of my model without choosing these drastic changes or are these
> changes in the range of what I am allowed to do to my model?
>
>
> Thank you,
>
> Vital
> --
> *Vital Heim *
>
> *PhD student*
> University of Basel, Switzerland
>
> Bimini Biological Field Station Foundation
> Bimini, Bahamas
>
> +41 (0)79 732 05 57
> vital.heim at gmail.com
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From p|erre@dev|||emereu|| @end|ng |rom ephe@p@|@eu  Tue Jun  9 18:30:37 2020
From: p|erre@dev|||emereu|| @end|ng |rom ephe@p@|@eu (Pierre de Villemereuil)
Date: Tue, 09 Jun 2020 18:30:37 +0200
Subject: [R-sig-ME] 
 Poor mixing and autocorrelation of ZIP data in MCMCglmm
In-Reply-To: <CAJkG0X9wdN=UakXkyBUeCfDaQm4uR5r72QORZB8zEdTvKbk-Zw@mail.gmail.com>
References: <CAJkG0X9wdN=UakXkyBUeCfDaQm4uR5r72QORZB8zEdTvKbk-Zw@mail.gmail.com>
Message-ID: <5024751.FOpQYTZURJ@flyosflip>

Hi,

I'm very not sure about the prior you used: why fix all the random effects variances to 1 in the ZI part? I suspect at least a part of your problem might stem from there.

Here's the kind of prior I would suggest for such a model:
list(R = list(V = diag(2), nu = 2, fix = 2),
    G = list(G1 = list(V = diag(2), nu = 2, alpha.mu = c(0,0), alpha.V = diag(2)),
            G2 = list(V = diag(2), nu = 2, alpha.mu = c(0,0), alpha.V = diag(2)))
It's informative toward small variances, but you a priori know variances will be small in such models, because of the link functions involved.

Hope this helps,
Pierre.

Le mardi 9 juin 2020, 12:49:09 CEST Vital Heim a ?crit :
> Good morning,
> 
> Currently, I am working on a Zero-Inflated Poisson Generalized Linear Mixed
> Model using the MCMCglmm package in R. I am looking at the food uptake of
> animal at a tourism provisioning site. I try to understand what variables
> impact the amount of food/bait the animals consume.
> 
> The response variable (bait uptake) is zero-inflated. I have 6 fixed
> effects (nr. tourists, nr. operators at the site, nr. conspecifics,
> temperature, presence time of the animal at the site, and gender of the
> animal). Presence time of the animal and temperature are only entered
> at.level(trait, 1), all the other fixed effects are entered into the ZI
> part and the count part of the model. I have two random effects for ID of
> the animal and the provisioning event.
> 
> Here is the model specification:
> 
> 
> modelZI.9 <- MCMCglmm(bait ~ trait - 1
> 
>                       + trait:conspecifics + trait:tourists +
> trait:operators + at.level(trait,1):presence +
> at.level(trait,1):temperature
> 
>                       + trait:gender
> 
>                       , random = ~idh(trait):id + idh(trait):event
> 
>                       , rcov =~ idh(trait):units
> 
>                       , data=df, family="zipoisson", prior=prior6
> 
>                       , nitt=NITT*MULT, burnin=BURN*MULT, thin = THIN*MULT,
> verbose=FALSE)
> 
> 
> 
> 
> I started with the chain parameters: nitt = 120000, burn = 20000, thin =
> 100 to get an effective sample size of 1000. My first prior was:
> 
> 
> 
> prior.base<-list(R=list(V=diag(c(1,1)), nu=0.002, fix=2)
> 
>             ,G=list(G1=list(V=diag(c(1,1)), nu=0.002, fix=2)
> 
>             ,G2=list(V=diag(c(1,1)), nu=0.002, fix=2)))
> 
> 
> 
> The chosen parameters and prior resulted in a very poor mixing of the ZI
> effects as well as the both random effects. The effective sample sizes for
> the ZI effects were very low (less than 100). I adjusted the chain
> parameters (see below) I also adjusted the prior to account for
> overdispersion in the fixed effects and making the prior stronger for the
> random intercepts. There was autocorrelation in my ZI fixed effects so I
> increased the thinning while increasing nitt and burn accordingly to get an
> effective size of 1100. These measures improved the model in some parts.
> However, I tried to validate my prior choice and parameters settings by
> reading the literature to see if I am not doing anything ?illegal? with my
> model. By now the parameters and prior gives me slightly better mixing. I
> was able to get the random effect ID mixing nicely but only when I set the
> nu = 100.002, which seems very high to me.
> 
>  These are my current chain parameters and my prior:
> 
> NITT <- 185000
> 
> BURN <- 15000
> 
> THIN <- 150
> 
> MULT <- 10
> 
> 
> 
> prior6<-list(R=list(V=diag(c(1e-6,1)), nu=10.002, fix=2)
> 
>              ,G=list(G1=list(V=diag(c(1,1)), nu=100.002, fix=2)
> 
>              ,G2=list(V=diag(c(1,1)), nu=0.002, fix=2)))
> 
> 
> 
> The autocorrelation diagnostics are:
> 
> 0.075907049                 NaN
> 
>  > autocorr.diag(modelZI.9$Sol)
>            bait          zi_bait      bait:conspecifics
> zi_bait:conspecifics   bait:tourists     zi_bait:tourists   bait:operators
> Lag 0      1.00000000    1.00000000      1.0000000000           1.00000000
>        1.00000000          1.000000000      1.000000000
> Lag 1500  -0.03158767    0.45992662     -0.0216380764           0.52372246
>       -0.02650593          0.546651545     -0.027195064
> Lag 7500   0.02284612    0.15705391      0.0114730973           0.12436415
>        0.03518200          0.122140444     -0.010875426
> Lag 15000  0.04558767    0.06901158     -0.0131587465           0.05700078
>        0.05371373          0.054479522      0.024634961
> Lag 75000  0.03567369    0.04099224     -0.0005695797           0.02014238
>       -0.02546435         -0.006299296     -0.005569035
>             zi_bait:operators     presence temperature
>  bait:gendermale       zi_bait:gendermale
> Lag 0              1.00000000  1.000000000  1.00000000
> 1.000000000               1.00000000
> Lag 1500           0.43279808 -0.004789598 -0.03151510
>  -0.002708028               0.33387297
> Lag 7500           0.05697190  0.012426457  0.02221331
>  -0.025070273               0.11236772
> Lag 15000          0.05472752 -0.043356418  0.04499879
>  -0.003589775               0.05618167
> Lag 75000          0.03342174 -0.007475513  0.03201374
> 0.015790374              -0.03167048
> 
> > autocorr.diag(modelZI.9$VCV)
>                 bait.id       zi_bait.id       bait.dive
>  zi_bait.dive       bait.units       zi_bait.units
> Lag 0       1.000000000              NaN      1.00000000                NaN
>      1.000000000                 NaN
> Lag 1500   -0.026735489              NaN      0.03018431                NaN
>      0.003349623                 NaN
> Lag 7500   -0.051666715              NaN     -0.02011165                NaN
>     -0.048462744                 NaN
> Lag 15000   0.003918555              NaN     -0.02777388                NaN
>     -0.001752476                 NaN
> Lag 75000   0.028123317              NaN     -0.01640397                NaN
>      0.075907049                 NaN
> 
> 
> 
> I therefore wanted to ask if there are suggestions on how I could improve
> the mixing of my model without choosing these drastic changes or are these
> changes in the range of what I am allowed to do to my model?
> 
> 
> Thank you,
> 
> Vital
> --
> *Vital Heim *
> 
> *PhD student*
> University of Basel, Switzerland
> 
> Bimini Biological Field Station Foundation
> Bimini, Bahamas
> 
> +41 (0)79 732 05 57
> vital.heim at gmail.com
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 


From |@g|ne @end|ng |rom p@@jd@org  Wed Jun 10 12:40:33 2020
From: |@g|ne @end|ng |rom p@@jd@org (=?iso-8859-1?Q?IAGO_GIN=C9_V=C1ZQUEZ?=)
Date: Wed, 10 Jun 2020 10:40:33 +0000
Subject: [R-sig-ME] glmmTMB convergence with unique outcome count 0 for a
 factor regressor category
Message-ID: <PR1PR02MB4682C94CBAB1E05DFF010D0A92830@PR1PR02MB4682.eurprd02.prod.outlook.com>

Dear all,

I am trying to fit a Negative Binomial mixed model, possibly zero-inflated with glmmTMB. I tried multiple offsets, removing covariates, including and removing the ziformula option, but it didn't converge, usually with output message
Error in (function (start, objective, gradient = NULL, hessian = NULL,  :
  NA/NaN gradient evaluation
In addition: Warning messages:
1: In (function (start, objective, gradient = NULL, hessian = NULL,  :
  NA/NaN function evaluation
2: In (function (start, objective, gradient = NULL, hessian = NULL,  :
  NA/NaN function evaluation
etc.

Now I found a fact on the data. For a factor regressor, the observations restricted to one of its values (in fact, the reference value) have a unique value for the outcome count, which is 0.

Is it the reason why the models don't converge?, so any model with a factor regressor with a category for which the unique outcome is 0 will never converge?

In that case:
Is relevant that this category for which the outcome is exclusively 0 is the reference one? If I change the reference category, could the model converge?
Is there any (other) solution to this, keeping the factor in the model?

Thank you in advance!
Iago


	[[alternative HTML version deleted]]


From j@de@ @end|ng |rom he@|th@uc@d@edu  Thu Jun 11 04:09:30 2020
From: j@de@ @end|ng |rom he@|th@uc@d@edu (Ades, James)
Date: Thu, 11 Jun 2020 02:09:30 +0000
Subject: [R-sig-ME] Modeling and Interpretation Question for Interaction in
 LMER Output.
Message-ID: <BY5PR19MB38591F98849BCAC5079D34C3EA800@BY5PR19MB3859.namprd19.prod.outlook.com>

This is both a question on modeling and output interpretation.

I'm looking at growth in a working memory precision task with five separate conditions. I've used the chi-sq test to develop this final model--with the five separate conditions + an interaction between time and grade. The Lmer model is mod.1 <- lmer(level ~ task + t4 * grade + (t4|pid) + (t4|pid:task), na.action = 'na.exclude', control = lmerControl(optimizer = "nloptwrap", calc.derivs = FALSE), REML = FALSE, filter.dataset.rw)

The hypothesis is that precision provides a better indication of working memory capacity than merely counting items that one can remember.

First, I want to make sure that I am interpreting the lmer output correctly. (In the MRE you can see the output; with an emphasis on minimal, I've only included two of the conditions...I did however, have to include enough participants so that the model wouldn't be "rank deficient"). I changed grade to character because the study was two-year longitudinal (with 4 timepoints), so there are three cohorts (3rd grade becomes the 4th grade, 5th grade becomes the 6th grade, and 7th grade becomes the eighth grade). For the interactions--t4:grade5 and t4:grade6, for instance--would be asking whether the difference between the change in time between grade5 and grade3 is significant and whether the change in grade6 and grade3 is significant for all tasks. Is that correct? Then, since the question I'm really asking is whether there is a significant difference between grade5 and grade6, I'd need to use a package like emmeans to do a pairwise comparison (between 3/4, 5/6, 7/8). Is that correct?

The modeling question regards my depiction of time. The amount of elapsed time between participants and testing points is not equal equal. Students in the same classroom will be measured at equal timepoints, but while one class might have 4 months in between testing, another class might have 7 months. So the way I accounted for this difference was to make the date of everyone's first testing point "1" and then allow that to be a continuous measure for their second, third, and fourth timepoints. My question is whether this is the best way of accounting for the discrepancies in time between participants and testing points and whether I've modeled this in a way that will yield the most information. For instance, while I'd really want to see the difference in performance between all four timepoints for the 3rd, 5th, and 7th grade cohorts; with my current model, I can only see the difference between performance in grades 3/4, 5/6, 7/8. Is there a way, specifically, that I could have categorical output of the four timepoints, accounting for the discepancies in time among the four testing points? Or would it work to model the four timepoints as a fixed effect (just 1,2,3,4), but then to model the random slope of time?

Thanks much!

James



```

filter.dataset.rw <-

structure(list(pid = c("ADMIN-UCSF-bo004", "ADMIN-UCSF-bo008",
"ADMIN-UCSF-br045", "ADMIN-UCSF-ca133", "ADMIN-UCSF-cp042", "ADMIN-UCSF-la074",
"ADMIN-UCSF-pe167", "ADMIN-UCSF-pe300", "ADMIN-UCSF-bo004", "ADMIN-UCSF-bo008",
"ADMIN-UCSF-br045", "ADMIN-UCSF-ca133", "ADMIN-UCSF-cp042", "ADMIN-UCSF-la074",
"ADMIN-UCSF-bo008", "ADMIN-UCSF-br045", "ADMIN-UCSF-ca133", "ADMIN-UCSF-cp042",
"ADMIN-UCSF-la074", "ADMIN-UCSF-pe167", "ADMIN-UCSF-bo004", "ADMIN-UCSF-bo008",
"ADMIN-UCSF-br045", "ADMIN-UCSF-ca133", "ADMIN-UCSF-la074", "ADMIN-UCSF-pe167",
"ADMIN-UCSF-pe300", "ADMIN-UCSF-bo004", "ADMIN-UCSF-bo008", "ADMIN-UCSF-br045",
"ADMIN-UCSF-ca133", "ADMIN-UCSF-cp042", "ADMIN-UCSF-la074", "ADMIN-UCSF-pe167",
"ADMIN-UCSF-pe300", "ADMIN-UCSF-bo004", "ADMIN-UCSF-bo008", "ADMIN-UCSF-br045",
"ADMIN-UCSF-ca133", "ADMIN-UCSF-cp042", "ADMIN-UCSF-la074", "ADMIN-UCSF-bo008",
"ADMIN-UCSF-br045", "ADMIN-UCSF-ca133", "ADMIN-UCSF-cp042", "ADMIN-UCSF-la074",
"ADMIN-UCSF-pe167", "ADMIN-UCSF-bo004", "ADMIN-UCSF-bo008", "ADMIN-UCSF-br045",
"ADMIN-UCSF-ca133", "ADMIN-UCSF-la074", "ADMIN-UCSF-pe167", "ADMIN-UCSF-pe300"
), task = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L), .Label = c("R2B0",
"R2B2", "R2B4", "R4B0", "R4B2"), class = "factor"), grade = c("3",
"3", "5", "7", "3", "5", "7", "7", "3", "3", "5", "7", "3", "5",
"4", "6", "8", "4", "6", "8", "4", "4", "6", "8", "6", "8", "8",
"3", "3", "5", "7", "3", "5", "7", "7", "3", "3", "5", "7", "3",
"5", "4", "6", "8", "4", "6", "8", "4", "4", "6", "8", "6", "8",
"8"), level = c(-0.13232044198895, -0.0489871086556202, -0.13232044198895,
-0.13232044198895, 0.03434622467772, 0.03434622467772, -0.0489871086556202,
-0.13232044198895, -0.13232044198895, -0.13232044198895, 0.11767955801105,
0.03434622467772, -0.0489871086556202, 0.03434622467772, 0.03434622467772,
-0.13232044198895, -0.0489871086556202, -0.13232044198895, 0.20101289134438,
-0.0489871086556202, -0.0489871086556202, -0.0489871086556202,
0.03434622467772, -0.13232044198895, 0.03434622467772, -0.13232044198895,
0.20101289134438, -0.167127071823204, -0.000460405156534227,
-0.0837937384898744, -0.167127071823204, -0.167127071823204,
-0.167127071823204, 0.0828729281767957, -0.000460405156534227,
-0.167127071823204, -0.0837937384898744, -0.000460405156534227,
-0.0837937384898744, -0.0837937384898744, 0.416206261510126,
-0.0837937384898744, -0.000460405156534227, -0.0837937384898744,
-0.167127071823204, -0.0837937384898744, -0.000460405156534227,
0.0828729281767957, -0.0837937384898744, -0.000460405156534227,
-0.167127071823204, -0.000460405156534227, 0.332872928176796,
-0.000460405156534227), t4 = c(-10.9112585334942, -10.9112585334942,
-10.9112585334942, -10.9112585334942, -10.9112585334942, -10.9112585334942,
-10.9112585334942, -10.9112585334942, -10.9112585334942, -10.9112585334942,
-10.9112585334942, 3.68357690127109, 3.67897919202246, 3.67897919202246,
3.62695800727158, 3.62695800727158, 4.30543499808373, 4.30296890558854,
4.30789502392459, 4.30789502392459, 4.30543499808373, 4.30543499808373,
4.28804325537186, 4.68764324402763, 4.55866330254692, 4.67580878638063,
4.67580878638063, -10.9112585334942, -10.9112585334942, -10.9112585334942,
-10.9112585334942, -10.9112585334942, -10.9112585334942, -10.9112585334942,
-10.9112585334942, -10.9112585334942, -10.9112585334942, -10.9112585334942,
3.68357690127109, 3.67897919202246, 3.67897919202246, 3.62695800727158,
3.62695800727158, 4.30543499808373, 4.30296890558854, 4.30789502392459,
4.30789502392459, 4.30543499808373, 4.30543499808373, 4.28804325537186,
4.68764324402763, 4.55866330254692, 4.67580878638063, 4.67580878638063
), timepoint = c(1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3,
3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 2,
2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4)), row.names = c(NA,
-54L), class = "data.frame")

```


	[[alternative HTML version deleted]]


From v|t@|@he|m @end|ng |rom gm@||@com  Thu Jun 11 07:46:34 2020
From: v|t@|@he|m @end|ng |rom gm@||@com (Vital Heim)
Date: Thu, 11 Jun 2020 07:46:34 +0200
Subject: [R-sig-ME] 
 Poor mixing and autocorrelation of ZIP data in MCMCglmm
Message-ID: <CAJkG0X_FGgX2m-OfabPQ7XkRH9=D6OhyOxNWi6_FKpa8n3zLrA@mail.gmail.com>

Dear Ben and Pierre,

Thank you very much for the quick response - that is amazing and very
helpful!

I have not centered the continuous predictors as I had the feeling that
this would not be necessary in my model. Would you recommend to center them?

Thank you for the prior suggestion. From reading the course notes to
MCMCglmm I thought that I have to fix all the random effects variances to 1
in the ZI part. I am very new to Bayesian statistics and MCMCglmm and
therefore used the MCMCglmm course notes to write and understand my model.
There was one part in the course notes that said that there is no residual
variance for the zero-inflated process and that we cannot estimate the
residual covariance between the zero-inflation and the Poisson process but
that we can deal with this by fixing the residual variance for the ZI
portion at 1. Therefore I thought I have to do the same for my model.

I used the new prior and the model ran fine and showed some improvement.
The mixing of the random effects improved but the ZI portion of the random
effects did not mix well. Do you think the mixing could be/needs to be
further improved? Would you increase nu? I added you the model summary and
the autocorrelation diagnostics below:

> summary(model10)

 Iterations = 150001:1849501
 Thinning interval  = 1500
 Sample size  = 1134

 DIC: 4039.85

 G-structure:  ~idh(trait):id

                 post.mean l-95% CI u-95% CI eff.samp
baitIntake.id       0.2097  0.01955   0.5446  1134.00
zi_baitIntake.id  140.2144 10.39550 412.5714    16.88

               ~idh(trait):dive

                   post.mean  l-95% CI u-95% CI eff.samp
baitIntake.event      0.2386 1.113e-01   0.3711    544.1
zi_baitIntake.event   1.6603 1.054e-07   5.7216     64.6

 R-structure:  ~idh(trait):units

                    post.mean l-95% CI u-95% CI eff.samp
baitIntake.units       0.5084    0.423   0.5999     1134
zi_baitIntake.units    1.0000    1.000   1.0000        0

 Location effects: bait ~ trait - 1 + trait:sharks + trait:divers +
trait:boats + at.level(trait, 1):presence + at.level(trait, 1):temperature
+ trait:gender

                          post.mean   l-95% CI   u-95% CI eff.samp   pMCMC

baitIntake                -0.989171  -8.506002   7.022529   1036.3 0.80071

zi_baitIntake              5.726646  -0.731875  14.671429    118.8 0.09171
.
baitIntake:conspecifics   -0.049999  -0.115503   0.015834   1134.0 0.13757

zi_baitIntake:conspecifics  0.185816  -0.129883   0.601750    437.8 0.31922

baitIntake:tourists        0.044321   0.010587   0.075382   1134.0 0.00529
**
zi_baitIntake:tourists    -0.154735  -0.341183   0.021306    258.0 0.05996
.
baitIntake:operators       0.157988   0.069429   0.255920   1134.0 0.00176
**
zi_baitIntake:operators   -0.232157  -0.776007   0.249766    472.4 0.35979

presence                   0.010091   0.008344   0.011620   1134.0 < 9e-04
***
temperature                0.260196  -0.062494   0.536471   1027.0 0.08466
.
baitIntake:gendermale      0.355938  -0.329717   1.143065   1134.0 0.28571

zi_baitIntake:gendermale  -1.787142 -16.777974  10.576166    200.1 0.80952

---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

> autocorr.diag(model10$Sol)
           baitIntake zi_baitIntake baitIntake:conspec zi_baitIntake:consp.
baitIntake:tourists zi_baitIntake:tourists
Lag 0      1.00000000    1.00000000       1.000000000          1.000000000
         1.00000000           1.0000000000
Lag 1500   0.04457168    0.32629918       0.011215824          0.400247111
        -0.02362611           0.4065199561
Lag 7500  -0.01118381    0.21355704      -0.056422233          0.050903050
        -0.06575857           0.1543329508
Lag 15000 -0.01182550    0.14366161      -0.004787545          0.048794655
         0.01045932           0.1180500281
Lag 75000 -0.06205008    0.05428778      -0.012689660          0.004470857
        -0.01587028          -0.0001966966
          baitIntake:operators zi_baitIntake:operators     presence
 temperature baitIntake:gendermale zi_baitIntake:gendermale
Lag 0              1.000000000              1.00000000  1.000000000
 1.000000000           1.000000000               1.00000000
Lag 1500           0.018702637              0.34990109 -0.005337675
 0.049056906          -0.008540371               0.11797192
Lag 7500           0.029576245              0.04236116  0.051737798
-0.004486268          -0.043298182               0.10059317
Lag 15000         -0.006467939              0.07262102  0.020471250
-0.011604859           0.010096511               0.04657296
Lag 75000          0.050869606             -0.01289837  0.034904725
-0.064627241          -0.019433089               0.04871585

> autocorr.diag(model10$VCV)
          baitIntake.id zi_baitIntake.id baitIntake.event
zi_baitIntake.event baitIntake.units zi_baitIntake.units
Lag 0        1.00000000        1.0000000       1.00000000
 1.000000000        1.00000000                 NaN
Lag 1500    -0.01347780        0.6908849       0.03201474
 0.789290686        0.02634913                 NaN
Lag 7500    -0.04617914        0.5013689      -0.03223097
 0.519696189       -0.02148069                 NaN
Lag 15000    0.01414618        0.5099142      -0.04454962
 0.341925156       -0.04412612                 NaN
Lag 75000   -0.03180043        0.2266740      -0.01645174
-0.003025432       -0.01177512                 NaN

And then I would have another question regarding the prior which I am not
sure if that is ok to post here. If not feel free to let me know. I have
been looking up papers to learn how to properly report my prior choice and
details in a publication. I found some papers that just wrote "we chose a
inverse gamma prior" but I am concerned that some reviewers would want more
specific information. Do you have a recommendation on how to correctly
report a prior in a publication?

Best wishes,
Vital
--
*Vital Heim *

*PhD student*
University of Basel, Switzerland

Bimini Biological Field Station Foundation
Bimini, Bahamas

+41 (0)79 732 05 57
vital.heim at gmail.com

	[[alternative HTML version deleted]]


From h|gh@t@t @end|ng |rom h|gh@t@t@com  Thu Jun 11 14:40:46 2020
From: h|gh@t@t @end|ng |rom h|gh@t@t@com (Highland Statistics Ltd)
Date: Thu, 11 Jun 2020 13:40:46 +0100
Subject: [R-sig-ME] Course: Introduction to Linear Mixed Effects Models and
 GLMM with R-INLA
Message-ID: <a98ae952-bc60-d530-17c9-ae3f473be149@highstat.com>

We would like to announce the following online statistics course:

Introduction to Linear Mixed Effects Models and GLMM with R-INLA


This is an on-demand course with around 35-40 videos (each is 15-60 
minutes) with live (optional) Zoom summary sessions scheduled in 2 
different time zones:

  * Time zone 1: 09.00-11.00 British Summer Time.
  * Time zone 2: 19.00-21.00 British Summer Time.

The course represents around 40 hours of work.

The course fee includes an (optional) 1-hour face-to-face video chat? 
with one or both instructors (you can discuss your own data).

Starting date: 22 June

Flyer: 
http://highstat.com/Courses/Flyers/2020/Flyer2020_06_GLMMINLA_Online.pdf

Website: http://highstat.com/index.php/courses-upcoming


Kind regards,


Alain Zuur

-- 

Dr. Alain F. Zuur
Highland Statistics Ltd.
9 St Clair Wynd
AB41 6DZ Newburgh, UK
Email:highstat at highstat.com
URL:www.highstat.com


	[[alternative HTML version deleted]]


From mo|||eebrook@ @end|ng |rom gm@||@com  Thu Jun 11 15:58:17 2020
From: mo|||eebrook@ @end|ng |rom gm@||@com (Mollie Brooks)
Date: Thu, 11 Jun 2020 15:58:17 +0200
Subject: [R-sig-ME] 
 Course: Introduction to Linear Mixed Effects Models and
 GLMM with R-INLA
In-Reply-To: <a98ae952-bc60-d530-17c9-ae3f473be149@highstat.com>
References: <a98ae952-bc60-d530-17c9-ae3f473be149@highstat.com>
Message-ID: <6C162F61-CF6E-4A7C-97CD-61A3F2BFB24A@gmail.com>

The flyer says "Nested data means multiple observations from the same animal, site, area, nest, patient, hospital, vessel, lake, hive, transect, etc.", but this doesn?t agree with my understanding (https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#nested-or-crossed <https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#nested-or-crossed>). What if animals move from one site to another, or patients visit multiple hospitals. I encounter a lot of scientists who have a misconception of the meaning of nested data, so it would be good to be careful when teaching the terminology. Does R-INLA require random effects to be nested? 

Kind regards,
Mollie

> On 11Jun 2020, at 14:40, Highland Statistics Ltd <highstat at highstat.com> wrote:
> 
> We would like to announce the following online statistics course:
> 
> Introduction to Linear Mixed Effects Models and GLMM with R-INLA
> 
> 
> This is an on-demand course with around 35-40 videos (each is 15-60 
> minutes) with live (optional) Zoom summary sessions scheduled in 2 
> different time zones:
> 
>  * Time zone 1: 09.00-11.00 British Summer Time.
>  * Time zone 2: 19.00-21.00 British Summer Time.
> 
> The course represents around 40 hours of work.
> 
> The course fee includes an (optional) 1-hour face-to-face video chat  
> with one or both instructors (you can discuss your own data).
> 
> Starting date: 22 June
> 
> Flyer: 
> http://highstat.com/Courses/Flyers/2020/Flyer2020_06_GLMMINLA_Online.pdf
> 
> Website: http://highstat.com/index.php/courses-upcoming
> 
> 
> Kind regards,
> 
> 
> Alain Zuur
> 
> -- 
> 
> Dr. Alain F. Zuur
> Highland Statistics Ltd.
> 9 St Clair Wynd
> AB41 6DZ Newburgh, UK
> Email:highstat at highstat.com
> URL:www.highstat.com
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


	[[alternative HTML version deleted]]


From h|gh@t@t @end|ng |rom h|gh@t@t@com  Thu Jun 11 16:20:05 2020
From: h|gh@t@t @end|ng |rom h|gh@t@t@com (Highland Statistics Ltd)
Date: Thu, 11 Jun 2020 15:20:05 +0100
Subject: [R-sig-ME] 
 Course: Introduction to Linear Mixed Effects Models and
 GLMM with R-INLA
In-Reply-To: <6C162F61-CF6E-4A7C-97CD-61A3F2BFB24A@gmail.com>
References: <a98ae952-bc60-d530-17c9-ae3f473be149@highstat.com>
 <6C162F61-CF6E-4A7C-97CD-61A3F2BFB24A@gmail.com>
Message-ID: <d3c0d358-f66f-59c6-14a5-5a5f81b99d64@highstat.com>


On 11/06/2020 14:58, Mollie Brooks wrote:
> The flyer says "Nested data means multiple observations?from the same 
> animal, site, area, nest, patient, hospital, vessel, 
> lake,?hive,?transect, etc.", but this doesn?t agree with my 
> understanding 
> (https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#nested-or-crossed).?What 
> if animals move from one site to another, or patients visit multiple 
> hospitals.

Then it is not nested anymore.


> I encounter a lot of scientists who have a misconception of the 
> meaning of nested data, so it would be good to be careful when 
> teaching the terminology.?Does R-INLA require random effects to be 
> nested?

No. They can even be spatially correlated....or temporally correlated.

Alain

>
> Kind regards,
> Mollie
>
>> On 11Jun 2020, at 14:40, Highland Statistics Ltd 
>> <highstat at highstat.com <mailto:highstat at highstat.com>> wrote:
>>
>> We would like to announce the following online statistics course:
>>
>> Introduction to Linear Mixed Effects Models and GLMM with R-INLA
>>
>>
>> This is an on-demand course with around 35-40 videos (each is 15-60
>> minutes) with live (optional) Zoom summary sessions scheduled in 2
>> different time zones:
>>
>> ?* Time zone 1: 09.00-11.00 British Summer Time.
>> ?* Time zone 2: 19.00-21.00 British Summer Time.
>>
>> The course represents around 40 hours of work.
>>
>> The course fee includes an (optional) 1-hour face-to-face video chat
>> with one or both instructors (you can discuss your own data).
>>
>> Starting date: 22 June
>>
>> Flyer:
>> http://highstat.com/Courses/Flyers/2020/Flyer2020_06_GLMMINLA_Online.pdf
>>
>> Website: http://highstat.com/index.php/courses-upcoming
>>
>>
>> Kind regards,
>>
>>
>> Alain Zuur
>>
>> -- 
>>
>> Dr. Alain F. Zuur
>> Highland Statistics Ltd.
>> 9 St Clair Wynd
>> AB41 6DZ Newburgh, UK
>> Email:highstat at highstat.com
>> URL:www.highstat.com
>>
>>
>> [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
-- 

Dr. Alain F. Zuur
Highland Statistics Ltd.
9 St Clair Wynd
AB41 6DZ Newburgh, UK
Email: highstat at highstat.com
URL:   www.highstat.com


	[[alternative HTML version deleted]]


From @|ngm@nn @end|ng |rom gm@||@com  Thu Jun 11 16:51:08 2020
From: @|ngm@nn @end|ng |rom gm@||@com (Henrik Singmann)
Date: Thu, 11 Jun 2020 16:51:08 +0200
Subject: [R-sig-ME] 
 Course: Introduction to Linear Mixed Effects Models and
 GLMM with R-INLA
In-Reply-To: <d3c0d358-f66f-59c6-14a5-5a5f81b99d64@highstat.com>
References: <a98ae952-bc60-d530-17c9-ae3f473be149@highstat.com>
 <6C162F61-CF6E-4A7C-97CD-61A3F2BFB24A@gmail.com>
 <d3c0d358-f66f-59c6-14a5-5a5f81b99d64@highstat.com>
Message-ID: <CA+rDMKKSfaf0hkL7iP2ovWpGC3ZfVL8ZnEnL5-G2FhZaJuNi1w@mail.gmail.com>

But isn't that exactly Mollie's point? You write "Nested data means
multiple observations from the same [unit of observation]". And then she
gave an example where you can have multiple observations from the same unit
of observation without the data being nested.

I also completely agree with her criticism that this terminology is
critical to get right. When I teach mixed models one of the things that
always comes up is that people misunderstand the concept of nested factors:
A factor A is nested in another factor B if certain levels of A only appear
with certain levels of B and not with all levels of B (the latter would be
called crossed). In other words, whether or not we have repeated measures
or multiple observations is unrelated to whether or not there exists
nesting in the data.

Maybe it would make more sense to use "clustered" in that context instead
of "nested".


Am Do., 11. Juni 2020 um 16:20 Uhr schrieb Highland Statistics Ltd <
highstat at highstat.com>:

>
> On 11/06/2020 14:58, Mollie Brooks wrote:
> > The flyer says "Nested data means multiple observations from the same
> > animal, site, area, nest, patient, hospital, vessel,
> > lake, hive, transect, etc.", but this doesn?t agree with my
> > understanding
> > (
> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#nested-or-crossed). What
>
> > if animals move from one site to another, or patients visit multiple
> > hospitals.
>
> Then it is not nested anymore.
>
>
> > I encounter a lot of scientists who have a misconception of the
> > meaning of nested data, so it would be good to be careful when
> > teaching the terminology. Does R-INLA require random effects to be
> > nested?
>
> No. They can even be spatially correlated....or temporally correlated.
>
> Alain
>
> >
> > Kind regards,
> > Mollie
> >
> >> On 11Jun 2020, at 14:40, Highland Statistics Ltd
> >> <highstat at highstat.com <mailto:highstat at highstat.com>> wrote:
> >>
> >> We would like to announce the following online statistics course:
> >>
> >> Introduction to Linear Mixed Effects Models and GLMM with R-INLA
> >>
> >>
> >> This is an on-demand course with around 35-40 videos (each is 15-60
> >> minutes) with live (optional) Zoom summary sessions scheduled in 2
> >> different time zones:
> >>
> >>  * Time zone 1: 09.00-11.00 British Summer Time.
> >>  * Time zone 2: 19.00-21.00 British Summer Time.
> >>
> >> The course represents around 40 hours of work.
> >>
> >> The course fee includes an (optional) 1-hour face-to-face video chat
> >> with one or both instructors (you can discuss your own data).
> >>
> >> Starting date: 22 June
> >>
> >> Flyer:
> >>
> http://highstat.com/Courses/Flyers/2020/Flyer2020_06_GLMMINLA_Online.pdf
> >>
> >> Website: http://highstat.com/index.php/courses-upcoming
> >>
> >>
> >> Kind regards,
> >>
> >>
> >> Alain Zuur
> >>
> >> --
> >>
> >> Dr. Alain F. Zuur
> >> Highland Statistics Ltd.
> >> 9 St Clair Wynd
> >> AB41 6DZ Newburgh, UK
> >> Email:highstat at highstat.com
> >> URL:www.highstat.com
> >>
> >>
> >> [[alternative HTML version deleted]]
> >>
> >> _______________________________________________
> >> R-sig-mixed-models at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
> --
>
> Dr. Alain F. Zuur
> Highland Statistics Ltd.
> 9 St Clair Wynd
> AB41 6DZ Newburgh, UK
> Email: highstat at highstat.com
> URL:   www.highstat.com
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


-- 
Dr. Henrik Singmann
Assistant Professor, Department of Psychology
University of Warwick, UK
http://singmann.org

	[[alternative HTML version deleted]]


From jdpoe223 @end|ng |rom gm@||@com  Thu Jun 11 17:04:54 2020
From: jdpoe223 @end|ng |rom gm@||@com (John Poe)
Date: Thu, 11 Jun 2020 11:04:54 -0400
Subject: [R-sig-ME] 
 Course: Introduction to Linear Mixed Effects Models and
 GLMM with R-INLA
In-Reply-To: <CA+rDMKKSfaf0hkL7iP2ovWpGC3ZfVL8ZnEnL5-G2FhZaJuNi1w@mail.gmail.com>
References: <a98ae952-bc60-d530-17c9-ae3f473be149@highstat.com>
 <6C162F61-CF6E-4A7C-97CD-61A3F2BFB24A@gmail.com>
 <d3c0d358-f66f-59c6-14a5-5a5f81b99d64@highstat.com>
 <CA+rDMKKSfaf0hkL7iP2ovWpGC3ZfVL8ZnEnL5-G2FhZaJuNi1w@mail.gmail.com>
Message-ID: <CACDpxFAoUxOETyYWbiSKvH00fLoQB8Wdr4On91jTs_J7536Bkg@mail.gmail.com>

I think it's a little bit pedantic to criticize the flyer here since I
assume that the distinction between nested and cross classified grouping
structures is made pretty clear in the class itself.

I haven't seen the course material obviously so that's an assumption on my
part given how I teach it. I do spend a lot of time on it whenever I teach
mixed effects models because it is an important point of confusion for
people. I tend to use either "clustered" or the term "grouping structure"
as generic and differentiate between nested, crossed, and multiple
membership personally. So i understand the point Mollie and Henrik are
making but it might be unfair to expect that level of nuance in a flyer?







On Thu, Jun 11, 2020, 10:51 AM Henrik Singmann <singmann at gmail.com> wrote:

> But isn't that exactly Mollie's point? You write "Nested data means
> multiple observations from the same [unit of observation]". And then she
> gave an example where you can have multiple observations from the same unit
> of observation without the data being nested.
>
> I also completely agree with her criticism that this terminology is
> critical to get right. When I teach mixed models one of the things that
> always comes up is that people misunderstand the concept of nested factors:
> A factor A is nested in another factor B if certain levels of A only appear
> with certain levels of B and not with all levels of B (the latter would be
> called crossed). In other words, whether or not we have repeated measures
> or multiple observations is unrelated to whether or not there exists
> nesting in the data.
>
> Maybe it would make more sense to use "clustered" in that context instead
> of "nested".
>
>
> Am Do., 11. Juni 2020 um 16:20 Uhr schrieb Highland Statistics Ltd <
> highstat at highstat.com>:
>
> >
> > On 11/06/2020 14:58, Mollie Brooks wrote:
> > > The flyer says "Nested data means multiple observations from the same
> > > animal, site, area, nest, patient, hospital, vessel,
> > > lake, hive, transect, etc.", but this doesn?t agree with my
> > > understanding
> > > (
> >
> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#nested-or-crossed).
> What
> >
> > > if animals move from one site to another, or patients visit multiple
> > > hospitals.
> >
> > Then it is not nested anymore.
> >
> >
> > > I encounter a lot of scientists who have a misconception of the
> > > meaning of nested data, so it would be good to be careful when
> > > teaching the terminology. Does R-INLA require random effects to be
> > > nested?
> >
> > No. They can even be spatially correlated....or temporally correlated.
> >
> > Alain
> >
> > >
> > > Kind regards,
> > > Mollie
> > >
> > >> On 11Jun 2020, at 14:40, Highland Statistics Ltd
> > >> <highstat at highstat.com <mailto:highstat at highstat.com>> wrote:
> > >>
> > >> We would like to announce the following online statistics course:
> > >>
> > >> Introduction to Linear Mixed Effects Models and GLMM with R-INLA
> > >>
> > >>
> > >> This is an on-demand course with around 35-40 videos (each is 15-60
> > >> minutes) with live (optional) Zoom summary sessions scheduled in 2
> > >> different time zones:
> > >>
> > >>  * Time zone 1: 09.00-11.00 British Summer Time.
> > >>  * Time zone 2: 19.00-21.00 British Summer Time.
> > >>
> > >> The course represents around 40 hours of work.
> > >>
> > >> The course fee includes an (optional) 1-hour face-to-face video chat
> > >> with one or both instructors (you can discuss your own data).
> > >>
> > >> Starting date: 22 June
> > >>
> > >> Flyer:
> > >>
> > http://highstat.com/Courses/Flyers/2020/Flyer2020_06_GLMMINLA_Online.pdf
> > >>
> > >> Website: http://highstat.com/index.php/courses-upcoming
> > >>
> > >>
> > >> Kind regards,
> > >>
> > >>
> > >> Alain Zuur
> > >>
> > >> --
> > >>
> > >> Dr. Alain F. Zuur
> > >> Highland Statistics Ltd.
> > >> 9 St Clair Wynd
> > >> AB41 6DZ Newburgh, UK
> > >> Email:highstat at highstat.com
> > >> URL:www.highstat.com
> > >>
> > >>
> > >> [[alternative HTML version deleted]]
> > >>
> > >> _______________________________________________
> > >> R-sig-mixed-models at r-project.org mailing list
> > >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > >
> > --
> >
> > Dr. Alain F. Zuur
> > Highland Statistics Ltd.
> > 9 St Clair Wynd
> > AB41 6DZ Newburgh, UK
> > Email: highstat at highstat.com
> > URL:   www.highstat.com
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>
>
> --
> Dr. Henrik Singmann
> Assistant Professor, Department of Psychology
> University of Warwick, UK
> http://singmann.org
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From p|erre@dev|||emereu|| @end|ng |rom ephe@p@|@eu  Thu Jun 11 18:17:23 2020
From: p|erre@dev|||emereu|| @end|ng |rom ephe@p@|@eu (Pierre de Villemereuil)
Date: Thu, 11 Jun 2020 18:17:23 +0200
Subject: [R-sig-ME] 
 Poor mixing and autocorrelation of ZIP data in MCMCglmm
In-Reply-To: <CAJkG0X_FGgX2m-OfabPQ7XkRH9=D6OhyOxNWi6_FKpa8n3zLrA@mail.gmail.com>
References: <CAJkG0X_FGgX2m-OfabPQ7XkRH9=D6OhyOxNWi6_FKpa8n3zLrA@mail.gmail.com>
Message-ID: <2006873.6oUOiI1mWd@flyosflip>

Hi,

Yes, you need to fix the "residual" variance of the ZI part to 1 and cannot estimate the "residual" covariance (as far as I know for the latter), but you still can estimate the variances of the random effects.

But this output doesn't seem right. The variance for "zi_baitIntake.id" (140!!) is way too high and the uncertainty around the fixed effects (especially the intercepts for each trait) is considerable.

Are you sure you have enough data to fit such a complicated model?

You can maybe be even more informative toward small values for the variances by setting nu = 1000 for the random effects part:
list(R = list(V = diag(2), nu = 2, fix = 2),
    G = list(G1 = list(V = diag(2), nu = 1000, alpha.mu = c(0,0), alpha.V = diag(2)),
            G2 = list(V = diag(2), nu = 1000, alpha.mu = c(0,0), alpha.V = diag(2)))

But PLEASE NOTE this prior above would not be valid if you wanted to estimate a covariance between the Poisson and ZI part for the random effects (i.e. if you used "us()" instead of "idh()" in your model definition).

The priors (previous and the above one) are not an inverse Gamma distribution, so do not state that. You can state the values for the parameters V, nu, alpha.mu and alpha.V, or even better, share the analysis code with your Methods.

Cheers,
Pierre

Le jeudi 11 juin 2020, 07:46:34 CEST Vital Heim a ?crit :
> Dear Ben and Pierre,
> 
> Thank you very much for the quick response - that is amazing and very
> helpful!
> 
> I have not centered the continuous predictors as I had the feeling that
> this would not be necessary in my model. Would you recommend to center them?
> 
> Thank you for the prior suggestion. From reading the course notes to
> MCMCglmm I thought that I have to fix all the random effects variances to 1
> in the ZI part. I am very new to Bayesian statistics and MCMCglmm and
> therefore used the MCMCglmm course notes to write and understand my model.
> There was one part in the course notes that said that there is no residual
> variance for the zero-inflated process and that we cannot estimate the
> residual covariance between the zero-inflation and the Poisson process but
> that we can deal with this by fixing the residual variance for the ZI
> portion at 1. Therefore I thought I have to do the same for my model.
> 
> I used the new prior and the model ran fine and showed some improvement.
> The mixing of the random effects improved but the ZI portion of the random
> effects did not mix well. Do you think the mixing could be/needs to be
> further improved? Would you increase nu? I added you the model summary and
> the autocorrelation diagnostics below:
> 
> > summary(model10)
> 
>  Iterations = 150001:1849501
>  Thinning interval  = 1500
>  Sample size  = 1134
> 
>  DIC: 4039.85
> 
>  G-structure:  ~idh(trait):id
> 
>                  post.mean l-95% CI u-95% CI eff.samp
> baitIntake.id       0.2097  0.01955   0.5446  1134.00
> zi_baitIntake.id  140.2144 10.39550 412.5714    16.88
> 
>                ~idh(trait):dive
> 
>                    post.mean  l-95% CI u-95% CI eff.samp
> baitIntake.event      0.2386 1.113e-01   0.3711    544.1
> zi_baitIntake.event   1.6603 1.054e-07   5.7216     64.6
> 
>  R-structure:  ~idh(trait):units
> 
>                     post.mean l-95% CI u-95% CI eff.samp
> baitIntake.units       0.5084    0.423   0.5999     1134
> zi_baitIntake.units    1.0000    1.000   1.0000        0
> 
>  Location effects: bait ~ trait - 1 + trait:sharks + trait:divers +
> trait:boats + at.level(trait, 1):presence + at.level(trait, 1):temperature
> + trait:gender
> 
>                           post.mean   l-95% CI   u-95% CI eff.samp   pMCMC
> 
> baitIntake                -0.989171  -8.506002   7.022529   1036.3 0.80071
> 
> zi_baitIntake              5.726646  -0.731875  14.671429    118.8 0.09171
> .
> baitIntake:conspecifics   -0.049999  -0.115503   0.015834   1134.0 0.13757
> 
> zi_baitIntake:conspecifics  0.185816  -0.129883   0.601750    437.8 0.31922
> 
> baitIntake:tourists        0.044321   0.010587   0.075382   1134.0 0.00529
> **
> zi_baitIntake:tourists    -0.154735  -0.341183   0.021306    258.0 0.05996
> .
> baitIntake:operators       0.157988   0.069429   0.255920   1134.0 0.00176
> **
> zi_baitIntake:operators   -0.232157  -0.776007   0.249766    472.4 0.35979
> 
> presence                   0.010091   0.008344   0.011620   1134.0 < 9e-04
> ***
> temperature                0.260196  -0.062494   0.536471   1027.0 0.08466
> .
> baitIntake:gendermale      0.355938  -0.329717   1.143065   1134.0 0.28571
> 
> zi_baitIntake:gendermale  -1.787142 -16.777974  10.576166    200.1 0.80952
> 
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> > autocorr.diag(model10$Sol)
>            baitIntake zi_baitIntake baitIntake:conspec zi_baitIntake:consp.
> baitIntake:tourists zi_baitIntake:tourists
> Lag 0      1.00000000    1.00000000       1.000000000          1.000000000
>          1.00000000           1.0000000000
> Lag 1500   0.04457168    0.32629918       0.011215824          0.400247111
>         -0.02362611           0.4065199561
> Lag 7500  -0.01118381    0.21355704      -0.056422233          0.050903050
>         -0.06575857           0.1543329508
> Lag 15000 -0.01182550    0.14366161      -0.004787545          0.048794655
>          0.01045932           0.1180500281
> Lag 75000 -0.06205008    0.05428778      -0.012689660          0.004470857
>         -0.01587028          -0.0001966966
>           baitIntake:operators zi_baitIntake:operators     presence
>  temperature baitIntake:gendermale zi_baitIntake:gendermale
> Lag 0              1.000000000              1.00000000  1.000000000
>  1.000000000           1.000000000               1.00000000
> Lag 1500           0.018702637              0.34990109 -0.005337675
>  0.049056906          -0.008540371               0.11797192
> Lag 7500           0.029576245              0.04236116  0.051737798
> -0.004486268          -0.043298182               0.10059317
> Lag 15000         -0.006467939              0.07262102  0.020471250
> -0.011604859           0.010096511               0.04657296
> Lag 75000          0.050869606             -0.01289837  0.034904725
> -0.064627241          -0.019433089               0.04871585
> 
> > autocorr.diag(model10$VCV)
>           baitIntake.id zi_baitIntake.id baitIntake.event
> zi_baitIntake.event baitIntake.units zi_baitIntake.units
> Lag 0        1.00000000        1.0000000       1.00000000
>  1.000000000        1.00000000                 NaN
> Lag 1500    -0.01347780        0.6908849       0.03201474
>  0.789290686        0.02634913                 NaN
> Lag 7500    -0.04617914        0.5013689      -0.03223097
>  0.519696189       -0.02148069                 NaN
> Lag 15000    0.01414618        0.5099142      -0.04454962
>  0.341925156       -0.04412612                 NaN
> Lag 75000   -0.03180043        0.2266740      -0.01645174
> -0.003025432       -0.01177512                 NaN
> 
> And then I would have another question regarding the prior which I am not
> sure if that is ok to post here. If not feel free to let me know. I have
> been looking up papers to learn how to properly report my prior choice and
> details in a publication. I found some papers that just wrote "we chose a
> inverse gamma prior" but I am concerned that some reviewers would want more
> specific information. Do you have a recommendation on how to correctly
> report a prior in a publication?
> 
> Best wishes,
> Vital
> --
> *Vital Heim *
> 
> *PhD student*
> University of Basel, Switzerland
> 
> Bimini Biological Field Station Foundation
> Bimini, Bahamas
> 
> +41 (0)79 732 05 57
> vital.heim at gmail.com
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 


From D@v|d@Du||y @end|ng |rom q|mrbergho|er@edu@@u  Fri Jun 12 04:53:40 2020
From: D@v|d@Du||y @end|ng |rom q|mrbergho|er@edu@@u (David Duffy)
Date: Fri, 12 Jun 2020 02:53:40 +0000
Subject: [R-sig-ME] 
 Modeling and Interpretation Question for Interaction in LMER Output.
In-Reply-To: <BY5PR19MB38591F98849BCAC5079D34C3EA800@BY5PR19MB3859.namprd19.prod.outlook.com>
References: <BY5PR19MB38591F98849BCAC5079D34C3EA800@BY5PR19MB3859.namprd19.prod.outlook.com>
Message-ID: <7790fe97d6884417a73823c8ea28f1ab@qimrberghofer.edu.au>

> The modeling question regards my depiction of time. The amount of elapsed time 
> between participants and testing points is not equal equal. Students in the same classroom 
> will be measured at equal time points, but while one class might have 4 months 
> in between testing, another class might have 7 months. 

In lme, AIUI this would be comparing different correlation structures, as per the example for 
corCAR1().

From hu@ng@jcc @end|ng |rom gm@||@com  Fri Jun 12 05:07:10 2020
From: hu@ng@jcc @end|ng |rom gm@||@com (Sijia Huang)
Date: Thu, 11 Jun 2020 20:07:10 -0700
Subject: [R-sig-ME] Estimating multiple membership models with lme4
Message-ID: <CAPmBuzG1HggVsrZ57d4gf9KJNP9K4rrT5_FFXrbKhgXHooj=Ow@mail.gmail.com>

Hi all,
I am trying to estimate a multiple membership multiple classification model
with lme4. What I have in the model is that individuals are
cross-classified by ego nets and schools, and each individual can belong to
more than one ego nets.
I have following the instruction,
https://bbolker.github.io/mixedmodels-misc/notes/multimember.html

Below are my codes, where W is a matrix of 0s and 1s that indicates if an
individual belongs to an ego net.

lmod <- lFormula(y~(1|fake.ego.id)+(1|sch.id), data=data)
lmod$reTrms$Zt <- lmod$reTrms$Ztlist[[1]] <- Matrix(t(W))
devfun <- do.call(mkLmerDevfun, lmod)

Error in Lambdat %*% Ut :
  Cholmod error 'A and B inner dimensions must match' at file
../MatrixOps/cholmod_ssmult.c, line 82

Anyone have an idea how to fix this error? Thanks!


Best,
Sijia

	[[alternative HTML version deleted]]


From @v|tk|n32 @end|ng |rom gm@||@com  Fri Jun 12 06:58:41 2020
From: @v|tk|n32 @end|ng |rom gm@||@com (Sol Vitkin)
Date: Fri, 12 Jun 2020 00:58:41 -0400
Subject: [R-sig-ME] Checking Model Assumptions for Poisson Mixed Effects
 Model
Message-ID: <CAO7S7sa4CfjYM+-FcTiP6CRySCBr5kXP8jWKfzY_H9m5t9cwNA@mail.gmail.com>

Hello!

tldr; Aside from equidispersion, what are the model assumptions I should be
checking for a poisson mixed effects model that has a random intercept,
group mean centered transformations of its explanatory variables, and group
means included as variables?

I have a county-year dataset with 21 counties that each have 8 years of
data (N = 168). I am attempting to model the number of prescription opioid
related hospitalizations for this data using the number of prescription
opioid pills supplied to each county in a year, the prescription rate of
each county in a year, and demographic and economic (unemployment rate,
median household income) variables also at the county-year level. I am
using a poisson distribution (with a log link) and the lme4 package to
estimate this model with a random intercept, group mean centered
transformations of each variable, and the group mean of each variable in
line with the specification found in Bell and Jones, 2015
<https://www.cambridge.org/core/services/aop-cambridge-core/content/view/0334A27557D15848549120FE8ECD8D63/S2049847014000077a.pdf/explaining_fixed_effects_random_effects_modeling_of_timeseries_crosssectional_and_panel_data.pdf>
. Additionally, I have been using the DHARMa package to visually examine
the relationships between my explanatory variables and the randomized
quantile residuals from the model and to test for overdispersion. I am not
formally trained in mixed effects modeling and want to be as sure as one
can be that the estimates from my model are not biased. What are the
assumptions of a poisson mixed effects model and is there a rigorous set of
steps for testing these assumptions (either by looking at residuals or any
other part of the model output)?

Thank you in advance for any help!

-Sol

	[[alternative HTML version deleted]]


From mo|||eebrook@ @end|ng |rom gm@||@com  Fri Jun 12 11:34:39 2020
From: mo|||eebrook@ @end|ng |rom gm@||@com (Mollie Brooks)
Date: Fri, 12 Jun 2020 11:34:39 +0200
Subject: [R-sig-ME] 
 Course: Introduction to Linear Mixed Effects Models and
 GLMM with R-INLA
In-Reply-To: <CACDpxFAoUxOETyYWbiSKvH00fLoQB8Wdr4On91jTs_J7536Bkg@mail.gmail.com>
References: <a98ae952-bc60-d530-17c9-ae3f473be149@highstat.com>
 <6C162F61-CF6E-4A7C-97CD-61A3F2BFB24A@gmail.com>
 <d3c0d358-f66f-59c6-14a5-5a5f81b99d64@highstat.com>
 <CA+rDMKKSfaf0hkL7iP2ovWpGC3ZfVL8ZnEnL5-G2FhZaJuNi1w@mail.gmail.com>
 <CACDpxFAoUxOETyYWbiSKvH00fLoQB8Wdr4On91jTs_J7536Bkg@mail.gmail.com>
Message-ID: <886A4C5A-BBDC-43FD-974C-9DC57633A028@gmail.com>

Yes, I haven?t seen any of the course material other than the flyer. I?m just surprised by how many people think they know what "nesting" means and are wrong (e.g. professors who have been using the methods for years and PhD students who just learned the methods). I?m tired of having to correct them because it makes my work harder and I?m not really paid to do that work, so it would be helpful to me if instructors who are paid to do it could be more pedantic. 

I realize that this is off topic for the list, so I?ll drop it.


> On 11Jun 2020, at 17:04, John Poe <jdpoe223 at gmail.com> wrote:
> 
> I think it's a little bit pedantic to criticize the flyer here since I
> assume that the distinction between nested and cross classified grouping
> structures is made pretty clear in the class itself.
> 
> I haven't seen the course material obviously so that's an assumption on my
> part given how I teach it. I do spend a lot of time on it whenever I teach
> mixed effects models because it is an important point of confusion for
> people. I tend to use either "clustered" or the term "grouping structure"
> as generic and differentiate between nested, crossed, and multiple
> membership personally. So i understand the point Mollie and Henrik are
> making but it might be unfair to expect that level of nuance in a flyer?
> 
> 
> 
> 
> 
> 
> 
> On Thu, Jun 11, 2020, 10:51 AM Henrik Singmann <singmann at gmail.com> wrote:
> 
>> But isn't that exactly Mollie's point? You write "Nested data means
>> multiple observations from the same [unit of observation]". And then she
>> gave an example where you can have multiple observations from the same unit
>> of observation without the data being nested.
>> 
>> I also completely agree with her criticism that this terminology is
>> critical to get right. When I teach mixed models one of the things that
>> always comes up is that people misunderstand the concept of nested factors:
>> A factor A is nested in another factor B if certain levels of A only appear
>> with certain levels of B and not with all levels of B (the latter would be
>> called crossed). In other words, whether or not we have repeated measures
>> or multiple observations is unrelated to whether or not there exists
>> nesting in the data.
>> 
>> Maybe it would make more sense to use "clustered" in that context instead
>> of "nested".
>> 
>> 
>> Am Do., 11. Juni 2020 um 16:20 Uhr schrieb Highland Statistics Ltd <
>> highstat at highstat.com>:
>> 
>>> 
>>> On 11/06/2020 14:58, Mollie Brooks wrote:
>>>> The flyer says "Nested data means multiple observations from the same
>>>> animal, site, area, nest, patient, hospital, vessel,
>>>> lake, hive, transect, etc.", but this doesn?t agree with my
>>>> understanding
>>>> (
>>> 
>> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#nested-or-crossed).
>> What
>>> 
>>>> if animals move from one site to another, or patients visit multiple
>>>> hospitals.
>>> 
>>> Then it is not nested anymore.
>>> 
>>> 
>>>> I encounter a lot of scientists who have a misconception of the
>>>> meaning of nested data, so it would be good to be careful when
>>>> teaching the terminology. Does R-INLA require random effects to be
>>>> nested?
>>> 
>>> No. They can even be spatially correlated....or temporally correlated.
>>> 
>>> Alain
>>> 
>>>> 
>>>> Kind regards,
>>>> Mollie
>>>> 
>>>>> On 11Jun 2020, at 14:40, Highland Statistics Ltd
>>>>> <highstat at highstat.com <mailto:highstat at highstat.com>> wrote:
>>>>> 
>>>>> We would like to announce the following online statistics course:
>>>>> 
>>>>> Introduction to Linear Mixed Effects Models and GLMM with R-INLA
>>>>> 
>>>>> 
>>>>> This is an on-demand course with around 35-40 videos (each is 15-60
>>>>> minutes) with live (optional) Zoom summary sessions scheduled in 2
>>>>> different time zones:
>>>>> 
>>>>> * Time zone 1: 09.00-11.00 British Summer Time.
>>>>> * Time zone 2: 19.00-21.00 British Summer Time.
>>>>> 
>>>>> The course represents around 40 hours of work.
>>>>> 
>>>>> The course fee includes an (optional) 1-hour face-to-face video chat
>>>>> with one or both instructors (you can discuss your own data).
>>>>> 
>>>>> Starting date: 22 June
>>>>> 
>>>>> Flyer:
>>>>> 
>>> http://highstat.com/Courses/Flyers/2020/Flyer2020_06_GLMMINLA_Online.pdf
>>>>> 
>>>>> Website: http://highstat.com/index.php/courses-upcoming
>>>>> 
>>>>> 
>>>>> Kind regards,
>>>>> 
>>>>> 
>>>>> Alain Zuur
>>>>> 
>>>>> --
>>>>> 
>>>>> Dr. Alain F. Zuur
>>>>> Highland Statistics Ltd.
>>>>> 9 St Clair Wynd
>>>>> AB41 6DZ Newburgh, UK
>>>>> Email:highstat at highstat.com
>>>>> URL:www.highstat.com
>>>>> 
>>>>> 
>>>>> [[alternative HTML version deleted]]
>>>>> 
>>>>> _______________________________________________
>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>> 
>>> --
>>> 
>>> Dr. Alain F. Zuur
>>> Highland Statistics Ltd.
>>> 9 St Clair Wynd
>>> AB41 6DZ Newburgh, UK
>>> Email: highstat at highstat.com
>>> URL:   www.highstat.com
>>> 
>>> 
>>>        [[alternative HTML version deleted]]
>>> 
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>> 
>> 
>> 
>> --
>> Dr. Henrik Singmann
>> Assistant Professor, Department of Psychology
>> University of Warwick, UK
>> http://singmann.org
>> 
>>        [[alternative HTML version deleted]]
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From h|gh@t@t @end|ng |rom h|gh@t@t@com  Fri Jun 12 14:49:34 2020
From: h|gh@t@t @end|ng |rom h|gh@t@t@com (Highland Statistics Ltd)
Date: Fri, 12 Jun 2020 13:49:34 +0100
Subject: [R-sig-ME] 
 Course: Introduction to Linear Mixed Effects Models and
 GLMM with R-INLA
In-Reply-To: <886A4C5A-BBDC-43FD-974C-9DC57633A028@gmail.com>
References: <a98ae952-bc60-d530-17c9-ae3f473be149@highstat.com>
 <6C162F61-CF6E-4A7C-97CD-61A3F2BFB24A@gmail.com>
 <d3c0d358-f66f-59c6-14a5-5a5f81b99d64@highstat.com>
 <CA+rDMKKSfaf0hkL7iP2ovWpGC3ZfVL8ZnEnL5-G2FhZaJuNi1w@mail.gmail.com>
 <CACDpxFAoUxOETyYWbiSKvH00fLoQB8Wdr4On91jTs_J7536Bkg@mail.gmail.com>
 <886A4C5A-BBDC-43FD-974C-9DC57633A028@gmail.com>
Message-ID: <e7c40211-7917-e553-0d60-289a64a632a1@highstat.com>

On 12/06/2020 10:34, Mollie Brooks wrote:
> Yes, I haven?t seen any of the course material other than the flyer. I?m just surprised by how many people think they know what "nesting" means and are wrong (e.g. professors who have been using the methods for years and PhD students who just learned the methods).

Multiple monkeys in an enclosure, with multiple enclosures in a zoo, 
with multiple zoos....that would be nested...wouldn't it? And if some 
monkeys decide to take the bus and move to another zoo because the 
bananas are better, then it is crossed (like your patients going to two 
hospitals).

If don't think that a flyer is the right place to confuse people with 
"what-if" scenarios. Otherwise, I might as well attached a book.


> I?m tired of having to correct them because it makes my work harder
>   and I?m not really paid to do that work, so it would be helpful to me if instructors who are paid to do it could be more pedantic.

The art (or skill) of teaching/communicating statistics to a 
non-statistical audience requires switching regularly between informal 
language (which may not be 100% statistically correct) and formal 
statistical language. If everything is pedantic, then only a few will 
enjoy statistics and the majority will hate it (speaking from 20 years 
experience of teaching it).


> I realize that this is off topic for the list,
Is it? If instructors teach it properly, then you get fewer questions on 
this mailing list.

Alain

>   so I?ll drop it.




>
>
>> On 11Jun 2020, at 17:04, John Poe <jdpoe223 at gmail.com> wrote:
>>
>> I think it's a little bit pedantic to criticize the flyer here since I
>> assume that the distinction between nested and cross classified grouping
>> structures is made pretty clear in the class itself.
>>
>> I haven't seen the course material obviously so that's an assumption on my
>> part given how I teach it. I do spend a lot of time on it whenever I teach
>> mixed effects models because it is an important point of confusion for
>> people. I tend to use either "clustered" or the term "grouping structure"
>> as generic and differentiate between nested, crossed, and multiple
>> membership personally. So i understand the point Mollie and Henrik are
>> making but it might be unfair to expect that level of nuance in a flyer?
>>
>>
>>
>>
>>
>>
>>
>> On Thu, Jun 11, 2020, 10:51 AM Henrik Singmann <singmann at gmail.com> wrote:
>>
>>> But isn't that exactly Mollie's point? You write "Nested data means
>>> multiple observations from the same [unit of observation]". And then she
>>> gave an example where you can have multiple observations from the same unit
>>> of observation without the data being nested.
>>>
>>> I also completely agree with her criticism that this terminology is
>>> critical to get right. When I teach mixed models one of the things that
>>> always comes up is that people misunderstand the concept of nested factors:
>>> A factor A is nested in another factor B if certain levels of A only appear
>>> with certain levels of B and not with all levels of B (the latter would be
>>> called crossed). In other words, whether or not we have repeated measures
>>> or multiple observations is unrelated to whether or not there exists
>>> nesting in the data.
>>>
>>> Maybe it would make more sense to use "clustered" in that context instead
>>> of "nested".
>>>
>>>
>>> Am Do., 11. Juni 2020 um 16:20 Uhr schrieb Highland Statistics Ltd <
>>> highstat at highstat.com>:
>>>
>>>> On 11/06/2020 14:58, Mollie Brooks wrote:
>>>>> The flyer says "Nested data means multiple observations from the same
>>>>> animal, site, area, nest, patient, hospital, vessel,
>>>>> lake, hive, transect, etc.", but this doesn?t agree with my
>>>>> understanding
>>>>> (
>>> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#nested-or-crossed).
>>> What
>>>>> if animals move from one site to another, or patients visit multiple
>>>>> hospitals.
>>>> Then it is not nested anymore.
>>>>
>>>>
>>>>> I encounter a lot of scientists who have a misconception of the
>>>>> meaning of nested data, so it would be good to be careful when
>>>>> teaching the terminology. Does R-INLA require random effects to be
>>>>> nested?
>>>> No. They can even be spatially correlated....or temporally correlated.
>>>>
>>>> Alain
>>>>
>>>>> Kind regards,
>>>>> Mollie
>>>>>
>>>>>> On 11Jun 2020, at 14:40, Highland Statistics Ltd
>>>>>> <highstat at highstat.com <mailto:highstat at highstat.com>> wrote:
>>>>>>
>>>>>> We would like to announce the following online statistics course:
>>>>>>
>>>>>> Introduction to Linear Mixed Effects Models and GLMM with R-INLA
>>>>>>
>>>>>>
>>>>>> This is an on-demand course with around 35-40 videos (each is 15-60
>>>>>> minutes) with live (optional) Zoom summary sessions scheduled in 2
>>>>>> different time zones:
>>>>>>
>>>>>> * Time zone 1: 09.00-11.00 British Summer Time.
>>>>>> * Time zone 2: 19.00-21.00 British Summer Time.
>>>>>>
>>>>>> The course represents around 40 hours of work.
>>>>>>
>>>>>> The course fee includes an (optional) 1-hour face-to-face video chat
>>>>>> with one or both instructors (you can discuss your own data).
>>>>>>
>>>>>> Starting date: 22 June
>>>>>>
>>>>>> Flyer:
>>>>>>
>>>> http://highstat.com/Courses/Flyers/2020/Flyer2020_06_GLMMINLA_Online.pdf
>>>>>> Website: http://highstat.com/index.php/courses-upcoming
>>>>>>
>>>>>>
>>>>>> Kind regards,
>>>>>>
>>>>>>
>>>>>> Alain Zuur
>>>>>>
>>>>>> --
>>>>>>
>>>>>> Dr. Alain F. Zuur
>>>>>> Highland Statistics Ltd.
>>>>>> 9 St Clair Wynd
>>>>>> AB41 6DZ Newburgh, UK
>>>>>> Email:highstat at highstat.com
>>>>>> URL:www.highstat.com
>>>>>>
>>>>>>
>>>>>> [[alternative HTML version deleted]]
>>>>>>
>>>>>> _______________________________________________
>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>> --
>>>>
>>>> Dr. Alain F. Zuur
>>>> Highland Statistics Ltd.
>>>> 9 St Clair Wynd
>>>> AB41 6DZ Newburgh, UK
>>>> Email: highstat at highstat.com
>>>> URL:   www.highstat.com
>>>>
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>
>>> --
>>> Dr. Henrik Singmann
>>> Assistant Professor, Department of Psychology
>>> University of Warwick, UK
>>> http://singmann.org
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 

Dr. Alain F. Zuur
Highland Statistics Ltd.
9 St Clair Wynd
AB41 6DZ Newburgh, UK
Email: highstat at highstat.com
URL:   www.highstat.com


From th|erry@onke||nx @end|ng |rom |nbo@be  Fri Jun 12 16:07:03 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Fri, 12 Jun 2020 16:07:03 +0200
Subject: [R-sig-ME] 
 Course: Introduction to Linear Mixed Effects Models and
 GLMM with R-INLA
In-Reply-To: <e7c40211-7917-e553-0d60-289a64a632a1@highstat.com>
References: <a98ae952-bc60-d530-17c9-ae3f473be149@highstat.com>
 <6C162F61-CF6E-4A7C-97CD-61A3F2BFB24A@gmail.com>
 <d3c0d358-f66f-59c6-14a5-5a5f81b99d64@highstat.com>
 <CA+rDMKKSfaf0hkL7iP2ovWpGC3ZfVL8ZnEnL5-G2FhZaJuNi1w@mail.gmail.com>
 <CACDpxFAoUxOETyYWbiSKvH00fLoQB8Wdr4On91jTs_J7536Bkg@mail.gmail.com>
 <886A4C5A-BBDC-43FD-974C-9DC57633A028@gmail.com>
 <e7c40211-7917-e553-0d60-289a64a632a1@highstat.com>
Message-ID: <CAJuCY5wc37-E=39o-+1APO8roUFrRjRHGPnE3T8V7k7Sk-Zo4g@mail.gmail.com>

The flyer mentions nested _data_. IMHO that refers to multiple
_observations_ nested in a group (the structure within a group). The
example is correct from that perspective.

Mollie seems to refer to nested and crossed _random effects_, which is a
statement about the structure among the groups. Note that the second page
of the flyer does mention nested and crossed _random effects_. So the
distinction between nested and crossed random effects is handled in the
course.

The discussion becomes somewhat irrelevant if the grouping variables are
defined properly. Give each student, class, school, ... an ID that is
unique within the dataset. Then you only need a single variable to define a
specific group. Specify a random effect for every relevant grouping
variable as if they are crossed. The penalisation term in the likelihood
will take care of any implicit nesting.

Just my ?0.02

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op vr 12 jun. 2020 om 14:50 schreef Highland Statistics Ltd <
highstat at highstat.com>:

> On 12/06/2020 10:34, Mollie Brooks wrote:
> > Yes, I haven?t seen any of the course material other than the flyer. I?m
> just surprised by how many people think they know what "nesting" means and
> are wrong (e.g. professors who have been using the methods for years and
> PhD students who just learned the methods).
>
> Multiple monkeys in an enclosure, with multiple enclosures in a zoo,
> with multiple zoos....that would be nested...wouldn't it? And if some
> monkeys decide to take the bus and move to another zoo because the
> bananas are better, then it is crossed (like your patients going to two
> hospitals).
>
> If don't think that a flyer is the right place to confuse people with
> "what-if" scenarios. Otherwise, I might as well attached a book.
>
>
> > I?m tired of having to correct them because it makes my work harder
> >   and I?m not really paid to do that work, so it would be helpful to me
> if instructors who are paid to do it could be more pedantic.
>
> The art (or skill) of teaching/communicating statistics to a
> non-statistical audience requires switching regularly between informal
> language (which may not be 100% statistically correct) and formal
> statistical language. If everything is pedantic, then only a few will
> enjoy statistics and the majority will hate it (speaking from 20 years
> experience of teaching it).
>
>
> > I realize that this is off topic for the list,
> Is it? If instructors teach it properly, then you get fewer questions on
> this mailing list.
>
> Alain
>
> >   so I?ll drop it.
>
>
>
>
> >
> >
> >> On 11Jun 2020, at 17:04, John Poe <jdpoe223 at gmail.com> wrote:
> >>
> >> I think it's a little bit pedantic to criticize the flyer here since I
> >> assume that the distinction between nested and cross classified grouping
> >> structures is made pretty clear in the class itself.
> >>
> >> I haven't seen the course material obviously so that's an assumption on
> my
> >> part given how I teach it. I do spend a lot of time on it whenever I
> teach
> >> mixed effects models because it is an important point of confusion for
> >> people. I tend to use either "clustered" or the term "grouping
> structure"
> >> as generic and differentiate between nested, crossed, and multiple
> >> membership personally. So i understand the point Mollie and Henrik are
> >> making but it might be unfair to expect that level of nuance in a flyer?
> >>
> >>
> >>
> >>
> >>
> >>
> >>
> >> On Thu, Jun 11, 2020, 10:51 AM Henrik Singmann <singmann at gmail.com>
> wrote:
> >>
> >>> But isn't that exactly Mollie's point? You write "Nested data means
> >>> multiple observations from the same [unit of observation]". And then
> she
> >>> gave an example where you can have multiple observations from the same
> unit
> >>> of observation without the data being nested.
> >>>
> >>> I also completely agree with her criticism that this terminology is
> >>> critical to get right. When I teach mixed models one of the things that
> >>> always comes up is that people misunderstand the concept of nested
> factors:
> >>> A factor A is nested in another factor B if certain levels of A only
> appear
> >>> with certain levels of B and not with all levels of B (the latter
> would be
> >>> called crossed). In other words, whether or not we have repeated
> measures
> >>> or multiple observations is unrelated to whether or not there exists
> >>> nesting in the data.
> >>>
> >>> Maybe it would make more sense to use "clustered" in that context
> instead
> >>> of "nested".
> >>>
> >>>
> >>> Am Do., 11. Juni 2020 um 16:20 Uhr schrieb Highland Statistics Ltd <
> >>> highstat at highstat.com>:
> >>>
> >>>> On 11/06/2020 14:58, Mollie Brooks wrote:
> >>>>> The flyer says "Nested data means multiple observations from the same
> >>>>> animal, site, area, nest, patient, hospital, vessel,
> >>>>> lake, hive, transect, etc.", but this doesn?t agree with my
> >>>>> understanding
> >>>>> (
> >>>
> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#nested-or-crossed
> ).
> >>> What
> >>>>> if animals move from one site to another, or patients visit multiple
> >>>>> hospitals.
> >>>> Then it is not nested anymore.
> >>>>
> >>>>
> >>>>> I encounter a lot of scientists who have a misconception of the
> >>>>> meaning of nested data, so it would be good to be careful when
> >>>>> teaching the terminology. Does R-INLA require random effects to be
> >>>>> nested?
> >>>> No. They can even be spatially correlated....or temporally correlated.
> >>>>
> >>>> Alain
> >>>>
> >>>>> Kind regards,
> >>>>> Mollie
> >>>>>
> >>>>>> On 11Jun 2020, at 14:40, Highland Statistics Ltd
> >>>>>> <highstat at highstat.com <mailto:highstat at highstat.com>> wrote:
> >>>>>>
> >>>>>> We would like to announce the following online statistics course:
> >>>>>>
> >>>>>> Introduction to Linear Mixed Effects Models and GLMM with R-INLA
> >>>>>>
> >>>>>>
> >>>>>> This is an on-demand course with around 35-40 videos (each is 15-60
> >>>>>> minutes) with live (optional) Zoom summary sessions scheduled in 2
> >>>>>> different time zones:
> >>>>>>
> >>>>>> * Time zone 1: 09.00-11.00 British Summer Time.
> >>>>>> * Time zone 2: 19.00-21.00 British Summer Time.
> >>>>>>
> >>>>>> The course represents around 40 hours of work.
> >>>>>>
> >>>>>> The course fee includes an (optional) 1-hour face-to-face video chat
> >>>>>> with one or both instructors (you can discuss your own data).
> >>>>>>
> >>>>>> Starting date: 22 June
> >>>>>>
> >>>>>> Flyer:
> >>>>>>
> >>>>
> http://highstat.com/Courses/Flyers/2020/Flyer2020_06_GLMMINLA_Online.pdf
> >>>>>> Website: http://highstat.com/index.php/courses-upcoming
> >>>>>>
> >>>>>>
> >>>>>> Kind regards,
> >>>>>>
> >>>>>>
> >>>>>> Alain Zuur
> >>>>>>
> >>>>>> --
> >>>>>>
> >>>>>> Dr. Alain F. Zuur
> >>>>>> Highland Statistics Ltd.
> >>>>>> 9 St Clair Wynd
> >>>>>> AB41 6DZ Newburgh, UK
> >>>>>> Email:highstat at highstat.com
> >>>>>> URL:www.highstat.com
> >>>>>>
> >>>>>>
> >>>>>> [[alternative HTML version deleted]]
> >>>>>>
> >>>>>> _______________________________________________
> >>>>>> R-sig-mixed-models at r-project.org mailing list
> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>>> --
> >>>>
> >>>> Dr. Alain F. Zuur
> >>>> Highland Statistics Ltd.
> >>>> 9 St Clair Wynd
> >>>> AB41 6DZ Newburgh, UK
> >>>> Email: highstat at highstat.com
> >>>> URL:   www.highstat.com
> >>>>
> >>>>
> >>>>         [[alternative HTML version deleted]]
> >>>>
> >>>> _______________________________________________
> >>>> R-sig-mixed-models at r-project.org mailing list
> >>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>>>
> >>>
> >>> --
> >>> Dr. Henrik Singmann
> >>> Assistant Professor, Department of Psychology
> >>> University of Warwick, UK
> >>> http://singmann.org
> >>>
> >>>         [[alternative HTML version deleted]]
> >>>
> >>> _______________________________________________
> >>> R-sig-mixed-models at r-project.org mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>>
> >>      [[alternative HTML version deleted]]
> >>
> >> _______________________________________________
> >> R-sig-mixed-models at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> --
>
> Dr. Alain F. Zuur
> Highland Statistics Ltd.
> 9 St Clair Wynd
> AB41 6DZ Newburgh, UK
> Email: highstat at highstat.com
> URL:   www.highstat.com
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From j@de@ @end|ng |rom he@|th@uc@d@edu  Sun Jun 14 03:29:38 2020
From: j@de@ @end|ng |rom he@|th@uc@d@edu (Ades, James)
Date: Sun, 14 Jun 2020 01:29:38 +0000
Subject: [R-sig-ME] 
 Modeling and Interpretation Question for Interaction in LMER Output.
In-Reply-To: <7790fe97d6884417a73823c8ea28f1ab@qimrberghofer.edu.au>
References: <BY5PR19MB38591F98849BCAC5079D34C3EA800@BY5PR19MB3859.namprd19.prod.outlook.com>,
 <7790fe97d6884417a73823c8ea28f1ab@qimrberghofer.edu.au>
Message-ID: <BY5PR19MB38595798993A6941BC8C292EEA9F0@BY5PR19MB3859.namprd19.prod.outlook.com>

Hi David, what exactly are you referring to when you say that they're comparing different correlation structures?
________________________________
From: David Duffy <David.Duffy at qimrberghofer.edu.au>
Sent: Thursday, June 11, 2020 7:53 PM
To: Ades, James <jades at health.ucsd.edu>; r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: Re: Modeling and Interpretation Question for Interaction in LMER Output.

> The modeling question regards my depiction of time. The amount of elapsed time
> between participants and testing points is not equal equal. Students in the same classroom
> will be measured at equal time points, but while one class might have 4 months
> in between testing, another class might have 7 months.

In lme, AIUI this would be comparing different correlation structures, as per the example for
corCAR1().

	[[alternative HTML version deleted]]


From v|t@|@he|m @end|ng |rom gm@||@com  Wed Jun 17 10:47:27 2020
From: v|t@|@he|m @end|ng |rom gm@||@com (Vital Heim)
Date: Wed, 17 Jun 2020 10:47:27 +0200
Subject: [R-sig-ME] ZIP MCMCglmm model structure error if at.level(trait,
 2) is used
Message-ID: <CAJkG0X_cJ1Ecp+KUt940MEzE4Fs=PoobaoO2jf2+GAKwF2fKXA@mail.gmail.com>

Dear list,

Currently, I am working on analysing behavioural data from wild animals
attending touristic provisioning events. One model looks at how much bait
these animals consume and the other at how much time the animals spend
attending events and what variables could impact that. I already used this
forum to ask questions about the first model and got some great advice and
decided that I would like to ask a question in regards to the second model
I could not yet find an answer to.

I have data for 28 animals for 104 events. Every animal has 1 observation
per event (if present, presence time > 0, and if absent there is still an
observation in the df for the animal but the presence time = 0). This
resulted in 2912 observations but I had to remove 112 observations due to
missing values in the "tourists" column resulting in a df of 2800
observations.I tried to fit the following model.

prior <- list(R = list(V = diag(2), nu = 2, fix = 2),

               G = list(

               G1 = list(V = diag(2), nu = 2, alpha.mu = c(0,0), alpha.V =
diag(2))

              ,G2 = list(V = diag(2), nu = 2, alpha.mu = c(0,0), alpha.V =
diag(2))))



NITT <- 320000

BURN <- 20000

THIN <- 200

MULT <- 10


model2.1<-MCMCglmm(presence ~ trait - 1
                 + trait:(animals*philo) + trait:(tourists*philo) +
trait:(operators*philo) + trait:observed + at.level(trait,1):counter
                 , random = ~idh(trait):id + idh(trait):event
                 , rcov =~ idh(trait):units
                 , data=df, family="zipoisson", prior=prior
                 , nitt=NITT*MULT, burnin=BURN*MULT, thin = THIN*MULT,
verbose=FALSE)

This model ran fine, the mixing was ok and I could not see any warning
signs in regards to the output.

The predictors are:
- animals: number of other animals attending the event, numeric
- philo: the animals are seasonal residents, and some animals have been
observed at the tourism site the years before my study period already
(philo = y) and some have been identified for the first time during my
study period (philo = n), factor, 2 levels: "y", "n".
- operators: nr. touristic operators at the site
- observed: the touristic season lasts approximately 4 months. However,
some animals would only arrive halfway through the season, towards the end
of the season, etc. I wanted to take into account that before a shark has
been observed the first time during the season, the observations before
that particular event will have presence time = 0 because the animal has
not yet arrived in the area at all and therefore could not have been
attending the provisioning event, factor, 2 levels ("n", "y").
- counter: Once the animal has been observed for the first time that season
I then labelled every further observation with +1 as a count, numeric.

I have two random effects: one for the id of the animal and the
provisioning event.

At that point I reconsidered the model and thought that the variable
"observed" should only be taken into account in the ZI part of the model. I
re-wrote the model to:

modelnew<-MCMCglmm(presence ~ trait - 1

                   + trait:(animals*philo) + trait:(divers*philo) +
trait:(boats*philo)

                   + at.level(trait,1):counter + at.level(trait,2):observed

                   , random = ~idh(trait):id + idh(trait):dive

                   , rcov =~ idh(trait):units

                   , data=df, family="zipoisson", prior=prior

                   , nitt=NITT*MULT, burnin=BURN*MULT, thin = THIN*MULT,
verbose=FALSE)

However, when I did that I got the following warning:

Warning message:

In MCMCglmm(presence ~ trait - 1 + trait:(animals* philo) +
trait:(tourists*  :

  some fixed effects are not estimable and have been removed. Use
singular.ok=TRUE to sample these effects, but use an informative prior!

I am not sure what causes this warning. Did I wrongly specify the model
formula or the prior for that particular model? This is the first time that
I tried to write a model that has the at.level(trait,2) part in it and I
therefore am not sure if that is how I would use it, i.e. when I would like
to have a predictor estimated in the ZI part of the model only. I found an
article in the list that says that the warning is issued when effects are
confounded rather than just strongly correlated but that it could
potentially be ignored. However, I do not understand why the warning
appears and feel a bit uncomfortable ignoring a warning that I do not fully
understand.

Kind regards, Vital

	[[alternative HTML version deleted]]


From v|t@|@he|m @end|ng |rom gm@||@com  Wed Jun 17 11:02:49 2020
From: v|t@|@he|m @end|ng |rom gm@||@com (Vital Heim)
Date: Wed, 17 Jun 2020 11:02:49 +0200
Subject: [R-sig-ME] 
 Poor mixing and autocorrelation of ZIP data in MCMCglmm
In-Reply-To: <CAJkG0X-3ATUHBDVbuMJRgruV1woW+2Fv-XKuDDzBrZoYoMJjpg@mail.gmail.com>
References: <CAJkG0X_FGgX2m-OfabPQ7XkRH9=D6OhyOxNWi6_FKpa8n3zLrA@mail.gmail.com>
 <2006873.6oUOiI1mWd@flyosflip>
 <CAJkG0X-3ATUHBDVbuMJRgruV1woW+2Fv-XKuDDzBrZoYoMJjpg@mail.gmail.com>
Message-ID: <CAJkG0X850TWzOC-Qv_wqFp=fGqMdRsbnUfxaJOU=_KBzvJ7R1Q@mail.gmail.com>

Dear Pierre,

Thank you again for the answer and the new prior!

I have been working on the model last week and your question if I have
enough data got me thinking. I therefore reconsidered my available data and
also looked into if I can simplify my model.

The model I submitted before had 378 observations. The observations came
from 28 different individuals. However, I think one problem could have been
that most of these individuals only had 1 observation because they only
appeared at a provisioning event once. I therefore decided to remove all
the individuals that have less than 5 total observations from the data set
so that theoretically every animal had at least 5 opportunities to make a
decision to attend (and for how long) a provisioning event or not. That
left me with 13 individuals and a total of 405 observations (as I do not
have temperature as a fixed effect I could keep the rows with temperature
== NA, resulting in more observations than before). The nr. observations
per individual are:

id   12 13 14 16 17 19 20 21 34 38 39 40 42
#obs 27 54 67 52 44 34 26 35 41  7  7  5  6

Trying to simplify my model had me removing two fixed effects (temperature
and gender) from my model formula. That left me with the model:

priorP2 <- list(R = list(V = diag(2), nu = 2, fix = 2),
                G = list(
                G1 = list(V = diag(2), nu = 1000, alpha.mu = c(0,0),
alpha.V = diag(2))
               ,G2 = list(V = diag(2), nu = 1000, alpha.mu = c(0,0),
alpha.V = diag(2))))

NITT <- 320000
BURN <- 20000
THIN <- 200
MULT <- 10

model3.1 <- MCMCglmm(bait ~ trait - 1
                     + trait:individuals + trait:tourists + trait:operators
+ at.level(trait,1):presence
                     , random = ~idh(trait):id + idh(trait):event
                     , rcov =~ idh(trait):units
                     , data=df, family="zipoisson", prior=priorP2
                     , nitt=NITT*MULT, burnin=BURN*MULT, thin = THIN*MULT,
verbose=FALSE)

The model resulted in the following summary:

> summary(model3.1)



 Iterations = 200001:3198001

 Thinning interval  = 2000

 Sample size  = 1500



 DIC: 4295.048



 G-structure:  ~idh(trait):id



                 post.mean l-95% CI u-95% CI eff.samp

baitIntake.id       0.1969   0.0204   0.5101     1500

zi_baitIntake.id    9.6323   4.3901  17.0756      151



               ~idh(trait):dive



                   post.mean  l-95% CI u-95% CI eff.samp

baitIntake.event       0.2955 1.501e-01   0.4423  1720.47

zi_baitIntake.event    0.6166 1.873e-06   2.2227    91.55



 R-structure:  ~idh(trait):units



                    post.mean l-95% CI u-95% CI eff.samp

baitIntake.units       0.5085   0.4217   0.6023     1641

zi_baitIntake.units    1.0000   1.0000   1.0000        0



 Location effects: bait ~ trait - 1 + trait:individuals + trait:tourists +
trait:operators + at.level(trait, 1):presence



                           post.mean  l-95% CI  u-95% CI eff.samp  pMCMC

baitIntake                  5.800698  5.169356  6.423403   1500.0 <7e-04 ***

zi_baitIntake              -0.607708 -3.430961  2.148515    719.3 0.6467

baitIntake:individuals     -0.053430 -0.120854  0.018248   1500.0 0.1347

zi_baitIntake:individuals   0.070798 -0.207224  0.358939    340.2 0.6307

baitIntake:tourists         0.054909  0.024764  0.089098   1500.0 <7e-04 ***

zi_baitIntake:tourists     -0.111844 -0.248268  0.023882    222.7 0.0840 .

baitIntake:operators        0.119909  0.016259  0.208378   1500.0 0.0253 *

zi_baitIntake:operators    -0.181109 -0.552908  0.211214    316.1 0.3520

presence                    0.009881  0.008174  0.011563   1500.0 <7e-04 ***

---

Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

The new model formula and adjusting the df for only animals that have
attended the events at least 5 times helped I think. The variance in the
zi.baitintake.id is a lot smaller now (from 140 in the previous model to 9
in this model). I think there is still a rather large uncertainty in some
of the fixed effects and the intercept for each trait but am not sure if
that would be within the range that is acceptable?

The autocorrelation diagnostics looked as follows:

> autocorr.diag(model3.1$Sol)

            baitIntake zi_baitIntake baitIntake:individuals
zi_baitIntake:individuals baitIntake:tourists zi_baitIntake:tourists

Lag 0      1.000000000    1.00000000            1.000000000
1.000000000         1.000000000              1.0000000

Lag 2000   0.026012481    0.26311228            0.011550676
0.566133816        -0.006107718              0.5590814

Lag 10000 -0.036331924    0.01332843            0.003078598
0.132021786        -0.023601403              0.1855635

Lag 20000  0.026546529   -0.01065644            0.005180587
0.003356647        -0.006827257              0.0705329

Lag 1e+05  0.009254395    0.02910151            0.003647482
 -0.045349648        -0.033278001             -0.0276361

          baitIntake:operators zi_baitIntake:operators      presence

Lag 0              1.000000000              1.00000000  1.0000000000

Lag 2000           0.017853409              0.53269135  0.0309542019

Lag 10000         -0.001788713              0.15219674  0.0039462749

Lag 20000         -0.067415168              0.04973919 -0.0084329378

Lag 1e+05         -0.010173980              0.03747686 -0.0009911261



> autocorr.diag(model3.1$VCV)

          baitIntake.id zi_baitIntake.id baitIntake.event
zi_baitIntake.event baitIntake.units zi_baitIntake.units

Lag 0       1.000000000      1.000000000      1.000000000
1.00000000       1.00000000                 NaN

Lag 2000    0.010988496      0.279553940     -0.006822039
0.78582472      -0.04529223                 NaN

Lag 10000   0.005850551      0.255547273     -0.020832316
0.48146906       0.01942578                 NaN

Lag 20000   0.006862719      0.175722325     -0.080220078
0.29263378      -0.01864084                 NaN

Lag 1e+05   0.036323891      0.003939182     -0.005860269
 -0.01147487      -0.01569073                 NaN

I attached the trace and density plots to this message. I think overall the
mixing improved. The mixing of the random effects improved compared to the
previous model but is still looking a bit off when I compare it to the
mixing of the fixed effects.

Thank you for your suggestion how to correctly report the prior choice in a
paper - that is very helpful.

Kind regards,
Vital

	[[alternative HTML version deleted]]


From @@ud|@@d|q @end|ng |rom gm@||@com  Wed Jun 17 13:50:12 2020
From: @@ud|@@d|q @end|ng |rom gm@||@com (Saudi Sadiq)
Date: Wed, 17 Jun 2020 13:50:12 +0200
Subject: [R-sig-ME] Help with a mixed effects model
Message-ID: <CAB_D3mrG1Uj-tzm0-sWnj3cehHV3OB-JnptE_vwWoLrndvrbsg@mail.gmail.com>

Hope everyone is safe and sound. I appreciate your help a lot.

I am evaluating two Arabic subtitles of a humorous English scene and asked
263 participants (part) to evaluate the two subtitles (named Standard
Arabic, SA, and Egyptian Arabic, EA) via a questionnaire that asked them to
rank the two subtitles in terms of how much each subtitle is

2) more humorous (hum),

5) closer to Egyptian culture (cul)



The questionnaire contained two 1-10 linear scale questions regarding the 2
points clarified, with 1 meaning the most humorous and closest to Egyptian
culture, and 10 meaning the least humorous and furthest from Egyptian
culture. Also, the questionnaire had a general multiple-choice question
regarding which subtitle is better in general (better). General information
about the participants were also collected concerning gender (categorical
factor), age (numeric factor) and education (categorical factor).

To rule out the effect of which subtitle was watched first, two versions of
the questionnaire were relied on: one showing the ?SA subtitle first? and
another showing the ?EA subtitle first?. Nearly half the participants
answered the first and nearly half answered the latter. The info regarding
this is called WF (watched first).

I am focusing on which social factor/s lead/s the participants to evaluate
one of the two subtitles as generally better and which subtitle is more
humorous and closer to Egyptian culture. Actually, I wanted better to be
the only dependent factor and asking participants 'which subtitle is
better?' could be enough, but I wanted to have detailed information of why
a subtitle is better by asking participants specific questions (regarding
which subtitle is more humorous and closer to Egyptian culture). Most of
the time, the total of the hum + cul = better, but sometimes it is not
(e.g. the sum for subtitle EA could be bigger than for SA, but the
participant prefers SA in the better column).

I thought that mixed effects analyses would clarify the picture and answer
the research questions (which  factor/s lead/s participants to favour a
subtitle over another?) and, so,  tried the lme4 package in R and ran many
models but all the codes I have used are not working.

I ran the following codes, which yielded Error messages, like:

model1<- lmer (better ~ gender + age + education + WF + (1 | part),
data=sub_data)

Error: number of levels of each grouping factor must be < number of
observations (problems: part)



Model2 <- glmer (better ~ gender + age + education + WF + (1 | part), data
= sub_data, family='binomial')

Error in mkRespMod(fr, family = family) :

  response must be numeric or factor



Model3 <- glmer (better ~ age + gender + education + WF + (1 | part), data
= sub_data, family='binomial', control=glmerControl(optimizer=c("bobyqa")))

Error in mkRespMod(fr, family = family) :

  response must be numeric or factor



Why do the models crash? Does the problem lie in the random factor part (which
is a code for participants)? Or is it something related to the mixed
effects analysis?

I hope I am not violating the rules here as I am attaching the dataset (
sub_data) just in case someone would like to have a look at it.

 All the best

--
Saudi Sadiq,

-- 
Saudi Sadiq,

Lecturer, Minia University, Egypt

Academia <http://york.academia.edu/SaudiSadiq>, Reserachgate
<https://www.researchgate.net/profile/Saudi_Sadiq>, Google Scholar
<https://scholar.google.co.uk/citations?user=h0latzcAAAAJ&hl=en>, Publons
<https://publons.com/researcher/2950905/saudi-sadiq/>

Certified Translator by (Egyta) <https://www.egyta.com/>

Associate Fellow of the Higher Education Academy, UK
<https://www.heacademy.ac.uk/>

From y|h@|uc42 @end|ng |rom gm@||@com  Wed Jun 17 19:12:08 2020
From: y|h@|uc42 @end|ng |rom gm@||@com (Yi-Hsiu Chen)
Date: Thu, 18 Jun 2020 01:12:08 +0800
Subject: [R-sig-ME] Including binary and Gaussian responses in one
 multivariate mixed model
Message-ID: <408FBEE9-216E-42B7-AF90-BDF70FCF4EF8@gmail.com>

Dear list members,

I am trying to fit a multivariate mixed model that includes a binary and a Gaussian response variables with MCMCglmm and met some problems I couldn?t figure out. As I couldn't find much discussion on these topics, I was wondering if I could have some help from the list.

This question is also posted on Cross Validated (sorry for cross-posting), so please visit this link for a more readable version:  https://stats.stackexchange.com/questions/472662/including-both-binary-and-gaussian-responses-in-one-multivariate-mixed-model-in <https://stats.stackexchange.com/questions/472662/including-both-binary-and-gaussian-responses-in-one-multivariate-mixed-model-in> . 

Here is my question: 

I would like to fit a multivariate mixed model to a dataset that contains: a binary Direction variable showing the direction the individuals moved toward, a continuous PropRangeChange_s variable showing the scaled proportion of the occupied range increased to old occupied range (OldRange), a continuous Dist variable showing the distance between new range centre and a key location, a categorical Type stating type of the individuals; categorical variables SiteID and Lat_band showing where the observations were recorded, in which SiteID is nested within Lat_band (more than one SiteID in one Lat_band level). The example data can be downloaded at this link: https://drive.google.com/drive/folders/16sjYpBSBnu3XCNcJDPpXcJQuo4UDj8cc?usp=sharing

What I am interested in is: how Direction and PropRangeChange_s change in response to OldRange, Dist, and Type ? I am also interested in whether the probability of moving toward certain direction is related to PropRangeChange_s.

I therefore tried fitting a multivariate mixed model with MCMCglmm with below code (also included in the shared folder):

library(MCMCglmm)
library(coda) 
# Read in data required
test.data<-read.csv(file="D:/Data/ExampleData.csv", header=TRUE, sep=",")

# iteration setting
niterations <- 50000 
nburnin <- 10000
nthin <- 50 

# set priors
prior.op1 <- list(R = list(R1 = list(V = diag(2), nu = 0.002)), 
                  G = list(G1 = list(V = diag(2), nu = 0.01, alpha.mu = rep(0, 2), alpha.V = diag(2) * 1000),
                           G2 = list(V = diag(2), nu = 0.01, alpha.mu = rep(0, 2), alpha.V = diag(2) * 1000)))

# Model test 1 - include Type as fixed effect
set.seed(1)
test.mmm.c1 <- MCMCglmm(cbind(factor(Direction), PropRangeChange_s) ~ 0 + trait + trait:(scale(OldRange)*scale(Dist)*Type),
                            random = ~us(trait):Lat_band + us(trait):Lat_band:SiteID , rcov = ~us(trait):units,
                            data = test.data, family = c("categorical", "gaussian"), prior = prior.op1, nitt = niterations, burnin = nburnin, thin = nthin, verbose = T, pr = T,pl=T) 

set.seed(2)
test.mmm.c2 <- MCMCglmm(cbind(factor(Direction), PropRangeChange_s) ~ 0 + trait + trait:(scale(OldRange)*scale(Dist)*Type),
                        random = ~us(trait):Lat_band + us(trait):Lat_band:SiteID , rcov = ~us(trait):units,
                        data = test.data, family = c("categorical", "gaussian"), prior = prior.op1, nitt = niterations, burnin = nburnin, thin = nthin, verbose = T, pr = T,pl=T) 

#extract fixed effects
test.mmm.FIX <- mcmc.list(test.mmm.c1$Sol,test.mmm.c2$Sol)

#extract random effects
test.mmm.VCV <- mcmc.list(test.mmm.c1$VCV,test.mmm.c2$VCV) 

# Check convergence
max(gelman.diag(test.mmm.FIX[, 1:48])$psrf)        #[Return] 1.027717

max(gelman.diag(test.mmm.VCV)$psrf)                # Error shown

# Check estimate for fixed effects
summary(test.mmm.FIX[, 1:48])                      # Very large estimate

# Check the corelations between two responses 
test.mmm.Lat_band<-mMULTIVCV[, 1:4]
test.mmm.SiteID<-mMULTIVCV[, 5:8]
test.mmm.units<-mMULTIVCV[, 9:12]

summary(test.mmm.Lat_band)                         # Very large estimate 
summary(test.mmm.units)  

test.mmm.Lat_band.summary<-summary(mcmc.list(posterior.cor(test.mmm.c1$VCV[, 1:4]), posterior.cor(test.mmm.c2$VCV[, 1:4])))
# Q: weird quantiles that ranges from -1 to 1, which is the entire parameter range for correlation
test.mmm.units.summary<-summary(mcmc.list(posterior.cor(test.mmm.c1$VCV[, 9:12]), posterior.cor(test.mmm.c2$VCV[, 9:12])))

The script run smoothly with no warnings showing up. However, when I checked the estimates, I found all the estimates for Direction-related coefficients are very large (such as, more than 10^4), which doesn't look correct. I therefore was wondering if I can have some help on following questions:

	? Is it statistically acceptable to include both binary and continuous variables as response variables?

	? When I check the fixed effects, for Direction-related coefficients, it only states "traitDirection.2" in the output, how could I know which Direction level it is for?

	? As the chains looks converging well for the fixed effects (Gelman?Rubin diagnostic = 1.027), what might be the causes for the error message ? Error in chol.default(W) : the leading minor of order 3 is not positive definite ? shown when checking the convergence for random effects and the seemly unlikely overly large coefficient estimates?

	? Is it possible to obtain marginal R^2 and conditional R^2 with MCMCglmm?

	? Can I extract the latent variable, the probability of moving upward, with function: predict(test.mmm.c1, marginal = NULL, posterior = "mean", type = "response") 


Sorry for the long post. This is my first multivariate mixed model as well as my first MCMCglmm model, hope I didn?t make a fundamental mistake. Any suggestions would be appreciated.


Best wishes
Yi-Hsiu



 


	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Wed Jun 17 19:12:34 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Wed, 17 Jun 2020 19:12:34 +0200
Subject: [R-sig-ME] Help with a mixed effects model
In-Reply-To: <CAB_D3mrG1Uj-tzm0-sWnj3cehHV3OB-JnptE_vwWoLrndvrbsg@mail.gmail.com>
References: <CAB_D3mrG1Uj-tzm0-sWnj3cehHV3OB-JnptE_vwWoLrndvrbsg@mail.gmail.com>
Message-ID: <CAJuCY5zT_uW5quTKnskK7TBvruOfz0J1SPTaejG6v52btXgBKw@mail.gmail.com>

Dear Saudi,

Your attachment got stripped from the mail. This is explained under the
general instructions at https://www.r-project.org/mail.html#instructions.

The problem seems to be in the data. So post the data in a suitable format.
Or send us the output of `str(sub_data)`

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op wo 17 jun. 2020 om 16:14 schreef Saudi Sadiq <saudisadiq at gmail.com>:

> Hope everyone is safe and sound. I appreciate your help a lot.
>
> I am evaluating two Arabic subtitles of a humorous English scene and asked
> 263 participants (part) to evaluate the two subtitles (named Standard
> Arabic, SA, and Egyptian Arabic, EA) via a questionnaire that asked them to
> rank the two subtitles in terms of how much each subtitle is
>
> 2) more humorous (hum),
>
> 5) closer to Egyptian culture (cul)
>
>
>
> The questionnaire contained two 1-10 linear scale questions regarding the 2
> points clarified, with 1 meaning the most humorous and closest to Egyptian
> culture, and 10 meaning the least humorous and furthest from Egyptian
> culture. Also, the questionnaire had a general multiple-choice question
> regarding which subtitle is better in general (better). General information
> about the participants were also collected concerning gender (categorical
> factor), age (numeric factor) and education (categorical factor).
>
> To rule out the effect of which subtitle was watched first, two versions of
> the questionnaire were relied on: one showing the ?SA subtitle first? and
> another showing the ?EA subtitle first?. Nearly half the participants
> answered the first and nearly half answered the latter. The info regarding
> this is called WF (watched first).
>
> I am focusing on which social factor/s lead/s the participants to evaluate
> one of the two subtitles as generally better and which subtitle is more
> humorous and closer to Egyptian culture. Actually, I wanted better to be
> the only dependent factor and asking participants 'which subtitle is
> better?' could be enough, but I wanted to have detailed information of why
> a subtitle is better by asking participants specific questions (regarding
> which subtitle is more humorous and closer to Egyptian culture). Most of
> the time, the total of the hum + cul = better, but sometimes it is not
> (e.g. the sum for subtitle EA could be bigger than for SA, but the
> participant prefers SA in the better column).
>
> I thought that mixed effects analyses would clarify the picture and answer
> the research questions (which  factor/s lead/s participants to favour a
> subtitle over another?) and, so,  tried the lme4 package in R and ran many
> models but all the codes I have used are not working.
>
> I ran the following codes, which yielded Error messages, like:
>
> model1<- lmer (better ~ gender + age + education + WF + (1 | part),
> data=sub_data)
>
> Error: number of levels of each grouping factor must be < number of
> observations (problems: part)
>
>
>
> Model2 <- glmer (better ~ gender + age + education + WF + (1 | part), data
> = sub_data, family='binomial')
>
> Error in mkRespMod(fr, family = family) :
>
>   response must be numeric or factor
>
>
>
> Model3 <- glmer (better ~ age + gender + education + WF + (1 | part), data
> = sub_data, family='binomial', control=glmerControl(optimizer=c("bobyqa")))
>
> Error in mkRespMod(fr, family = family) :
>
>   response must be numeric or factor
>
>
>
> Why do the models crash? Does the problem lie in the random factor part
> (which
> is a code for participants)? Or is it something related to the mixed
> effects analysis?
>
> I hope I am not violating the rules here as I am attaching the dataset (
> sub_data) just in case someone would like to have a look at it.
>
>  All the best
>
> --
> Saudi Sadiq,
>
> --
> Saudi Sadiq,
>
> Lecturer, Minia University, Egypt
>
> Academia <http://york.academia.edu/SaudiSadiq>, Reserachgate
> <https://www.researchgate.net/profile/Saudi_Sadiq>, Google Scholar
> <https://scholar.google.co.uk/citations?user=h0latzcAAAAJ&hl=en>, Publons
> <https://publons.com/researcher/2950905/saudi-sadiq/>
>
> Certified Translator by (Egyta) <https://www.egyta.com/>
>
> Associate Fellow of the Higher Education Academy, UK
> <https://www.heacademy.ac.uk/>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Wed Jun 17 19:26:08 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 17 Jun 2020 13:26:08 -0400
Subject: [R-sig-ME] 
 Poor mixing and autocorrelation of ZIP data in MCMCglmm
In-Reply-To: <CAJkG0X850TWzOC-Qv_wqFp=fGqMdRsbnUfxaJOU=_KBzvJ7R1Q@mail.gmail.com>
References: <CAJkG0X_FGgX2m-OfabPQ7XkRH9=D6OhyOxNWi6_FKpa8n3zLrA@mail.gmail.com>
 <2006873.6oUOiI1mWd@flyosflip>
 <CAJkG0X-3ATUHBDVbuMJRgruV1woW+2Fv-XKuDDzBrZoYoMJjpg@mail.gmail.com>
 <CAJkG0X850TWzOC-Qv_wqFp=fGqMdRsbnUfxaJOU=_KBzvJ7R1Q@mail.gmail.com>
Message-ID: <4b42c94b-b6ca-7362-8fae-e92c34fcf4aa@gmail.com>

 ? This is a very superficial comment that doesn't really engage with 
the details of your struggle, but in general it makes me sad when people 
drop observations, especially from mixed models, and especially in a 
Bayesian context.? In principle (I know it doesn't always work out this 
way in practice), observations that are only weakly informative are 
still giving you *some* information ... is there any way you can solve 
the problem by setting stronger (i.e. more strongly regularizing) priors ?

On 6/17/20 5:02 AM, Vital Heim wrote:
> Dear Pierre,
>
> Thank you again for the answer and the new prior!
>
> I have been working on the model last week and your question if I have
> enough data got me thinking. I therefore reconsidered my available data and
> also looked into if I can simplify my model.
>
> The model I submitted before had 378 observations. The observations came
> from 28 different individuals. However, I think one problem could have been
> that most of these individuals only had 1 observation because they only
> appeared at a provisioning event once. I therefore decided to remove all
> the individuals that have less than 5 total observations from the data set
> so that theoretically every animal had at least 5 opportunities to make a
> decision to attend (and for how long) a provisioning event or not. That
> left me with 13 individuals and a total of 405 observations (as I do not
> have temperature as a fixed effect I could keep the rows with temperature
> == NA, resulting in more observations than before). The nr. observations
> per individual are:
>
> id   12 13 14 16 17 19 20 21 34 38 39 40 42
> #obs 27 54 67 52 44 34 26 35 41  7  7  5  6
>
> Trying to simplify my model had me removing two fixed effects (temperature
> and gender) from my model formula. That left me with the model:
>
> priorP2 <- list(R = list(V = diag(2), nu = 2, fix = 2),
>                  G = list(
>                  G1 = list(V = diag(2), nu = 1000, alpha.mu = c(0,0),
> alpha.V = diag(2))
>                 ,G2 = list(V = diag(2), nu = 1000, alpha.mu = c(0,0),
> alpha.V = diag(2))))
>
> NITT <- 320000
> BURN <- 20000
> THIN <- 200
> MULT <- 10
>
> model3.1 <- MCMCglmm(bait ~ trait - 1
>                       + trait:individuals + trait:tourists + trait:operators
> + at.level(trait,1):presence
>                       , random = ~idh(trait):id + idh(trait):event
>                       , rcov =~ idh(trait):units
>                       , data=df, family="zipoisson", prior=priorP2
>                       , nitt=NITT*MULT, burnin=BURN*MULT, thin = THIN*MULT,
> verbose=FALSE)
>
> The model resulted in the following summary:
>
>> summary(model3.1)
>
>
>   Iterations = 200001:3198001
>
>   Thinning interval  = 2000
>
>   Sample size  = 1500
>
>
>
>   DIC: 4295.048
>
>
>
>   G-structure:  ~idh(trait):id
>
>
>
>                   post.mean l-95% CI u-95% CI eff.samp
>
> baitIntake.id       0.1969   0.0204   0.5101     1500
>
> zi_baitIntake.id    9.6323   4.3901  17.0756      151
>
>
>
>                 ~idh(trait):dive
>
>
>
>                     post.mean  l-95% CI u-95% CI eff.samp
>
> baitIntake.event       0.2955 1.501e-01   0.4423  1720.47
>
> zi_baitIntake.event    0.6166 1.873e-06   2.2227    91.55
>
>
>
>   R-structure:  ~idh(trait):units
>
>
>
>                      post.mean l-95% CI u-95% CI eff.samp
>
> baitIntake.units       0.5085   0.4217   0.6023     1641
>
> zi_baitIntake.units    1.0000   1.0000   1.0000        0
>
>
>
>   Location effects: bait ~ trait - 1 + trait:individuals + trait:tourists +
> trait:operators + at.level(trait, 1):presence
>
>
>
>                             post.mean  l-95% CI  u-95% CI eff.samp  pMCMC
>
> baitIntake                  5.800698  5.169356  6.423403   1500.0 <7e-04 ***
>
> zi_baitIntake              -0.607708 -3.430961  2.148515    719.3 0.6467
>
> baitIntake:individuals     -0.053430 -0.120854  0.018248   1500.0 0.1347
>
> zi_baitIntake:individuals   0.070798 -0.207224  0.358939    340.2 0.6307
>
> baitIntake:tourists         0.054909  0.024764  0.089098   1500.0 <7e-04 ***
>
> zi_baitIntake:tourists     -0.111844 -0.248268  0.023882    222.7 0.0840 .
>
> baitIntake:operators        0.119909  0.016259  0.208378   1500.0 0.0253 *
>
> zi_baitIntake:operators    -0.181109 -0.552908  0.211214    316.1 0.3520
>
> presence                    0.009881  0.008174  0.011563   1500.0 <7e-04 ***
>
> ---
>
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> The new model formula and adjusting the df for only animals that have
> attended the events at least 5 times helped I think. The variance in the
> zi.baitintake.id is a lot smaller now (from 140 in the previous model to 9
> in this model). I think there is still a rather large uncertainty in some
> of the fixed effects and the intercept for each trait but am not sure if
> that would be within the range that is acceptable?
>
> The autocorrelation diagnostics looked as follows:
>
>> autocorr.diag(model3.1$Sol)
>              baitIntake zi_baitIntake baitIntake:individuals
> zi_baitIntake:individuals baitIntake:tourists zi_baitIntake:tourists
>
> Lag 0      1.000000000    1.00000000            1.000000000
> 1.000000000         1.000000000              1.0000000
>
> Lag 2000   0.026012481    0.26311228            0.011550676
> 0.566133816        -0.006107718              0.5590814
>
> Lag 10000 -0.036331924    0.01332843            0.003078598
> 0.132021786        -0.023601403              0.1855635
>
> Lag 20000  0.026546529   -0.01065644            0.005180587
> 0.003356647        -0.006827257              0.0705329
>
> Lag 1e+05  0.009254395    0.02910151            0.003647482
>   -0.045349648        -0.033278001             -0.0276361
>
>            baitIntake:operators zi_baitIntake:operators      presence
>
> Lag 0              1.000000000              1.00000000  1.0000000000
>
> Lag 2000           0.017853409              0.53269135  0.0309542019
>
> Lag 10000         -0.001788713              0.15219674  0.0039462749
>
> Lag 20000         -0.067415168              0.04973919 -0.0084329378
>
> Lag 1e+05         -0.010173980              0.03747686 -0.0009911261
>
>
>
>> autocorr.diag(model3.1$VCV)
>            baitIntake.id zi_baitIntake.id baitIntake.event
> zi_baitIntake.event baitIntake.units zi_baitIntake.units
>
> Lag 0       1.000000000      1.000000000      1.000000000
> 1.00000000       1.00000000                 NaN
>
> Lag 2000    0.010988496      0.279553940     -0.006822039
> 0.78582472      -0.04529223                 NaN
>
> Lag 10000   0.005850551      0.255547273     -0.020832316
> 0.48146906       0.01942578                 NaN
>
> Lag 20000   0.006862719      0.175722325     -0.080220078
> 0.29263378      -0.01864084                 NaN
>
> Lag 1e+05   0.036323891      0.003939182     -0.005860269
>   -0.01147487      -0.01569073                 NaN
>
> I attached the trace and density plots to this message. I think overall the
> mixing improved. The mixing of the random effects improved compared to the
> previous model but is still looking a bit off when I compare it to the
> mixing of the fixed effects.
>
> Thank you for your suggestion how to correctly report the prior choice in a
> paper - that is very helpful.
>
> Kind regards,
> Vital
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From th|erry@onke||nx @end|ng |rom |nbo@be  Thu Jun 18 09:06:09 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Thu, 18 Jun 2020 09:06:09 +0200
Subject: [R-sig-ME] Help with a mixed effects model
In-Reply-To: <CAB_D3moAT7ykK9c5aiqY0k3dk3qJGuXA-JkhbvtS4+-WfA2FUA@mail.gmail.com>
References: <CAB_D3mrG1Uj-tzm0-sWnj3cehHV3OB-JnptE_vwWoLrndvrbsg@mail.gmail.com>
 <CAJuCY5zT_uW5quTKnskK7TBvruOfz0J1SPTaejG6v52btXgBKw@mail.gmail.com>
 <CAB_D3moAT7ykK9c5aiqY0k3dk3qJGuXA-JkhbvtS4+-WfA2FUA@mail.gmail.com>
Message-ID: <CAJuCY5ysLg1HsHpS8QgCUtiGSQ7DK-UrFJLwoJKNdxeVXCnkww@mail.gmail.com>

Dear Saudi,

Please keep the mailing list in cc.

It is best to convert the characters to factor. The response variable of a
binomial is most clear when it is either a logical (TRUE = success, FALSE =
failure) or a two column integer value (cbind(number of success, number of
failures)).

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op wo 17 jun. 2020 om 22:54 schreef Saudi Sadiq <saudisadiq at gmail.com>:

> Sorry for the mistake, and thanks for the reply.
> Here are the results of > str(sub_data)
> tibble [263 x 10] (S3: tbl_df/tbl/data.frame)
>  $ part     : chr [1:263] "Part1" "Part2" "Part3" "Part4" ...
>  $ WF       : chr [1:263] "SA" "SA" "SA" "SA" ...
>  $ gender   : chr [1:263] "male" "female" "female" "female" ...
>  $ age      : num [1:263] 37 28 44 16 33 20 26 50 40 49 ...
>  $ education: chr [1:263] "postgrad" "postgrad" "postgrad" "seconadry or
> below" ...
>  $ SA_hum   : num [1:263] 10 8 9 4 2 8 9 5 10 7 ...
>  $ EA_hum   : num [1:263] 6 7 10 8 8 9 10 9 9 10 ...
>  $ SA_cul   : num [1:263] 7 8 9 5 1 8 9 4 10 9 ...
>  $ EA_cul   : num [1:263] 10 7 9 9 9 8 10 8 10 9 ...
>  $ better   : chr [1:263] "SA" "SA" "EA" "EA" ...
>
> Hope this is okay.
> Best
>
> On Wed, 17 Jun 2020 at 19:12, Thierry Onkelinx <thierry.onkelinx at inbo.be>
> wrote:
>
>> Dear Saudi,
>>
>> Your attachment got stripped from the mail. This is explained under the
>> general instructions at https://www.r-project.org/mail.html#instructions.
>>
>> The problem seems to be in the data. So post the data in a suitable
>> format. Or send us the output of `str(sub_data)`
>>
>> Best regards,
>>
>> ir. Thierry Onkelinx
>> Statisticus / Statistician
>>
>> Vlaamse Overheid / Government of Flanders
>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>> AND FOREST
>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>> thierry.onkelinx at inbo.be
>> Havenlaan 88 bus 73, 1000 Brussel
>> www.inbo.be
>>
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of data.
>> ~ John Tukey
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>>
>> <https://www.inbo.be>
>>
>>
>> Op wo 17 jun. 2020 om 16:14 schreef Saudi Sadiq <saudisadiq at gmail.com>:
>>
>>> Hope everyone is safe and sound. I appreciate your help a lot.
>>>
>>> I am evaluating two Arabic subtitles of a humorous English scene and
>>> asked
>>> 263 participants (part) to evaluate the two subtitles (named Standard
>>> Arabic, SA, and Egyptian Arabic, EA) via a questionnaire that asked them
>>> to
>>> rank the two subtitles in terms of how much each subtitle is
>>>
>>> 2) more humorous (hum),
>>>
>>> 5) closer to Egyptian culture (cul)
>>>
>>>
>>>
>>> The questionnaire contained two 1-10 linear scale questions regarding
>>> the 2
>>> points clarified, with 1 meaning the most humorous and closest to
>>> Egyptian
>>> culture, and 10 meaning the least humorous and furthest from Egyptian
>>> culture. Also, the questionnaire had a general multiple-choice question
>>> regarding which subtitle is better in general (better). General
>>> information
>>> about the participants were also collected concerning gender (categorical
>>> factor), age (numeric factor) and education (categorical factor).
>>>
>>> To rule out the effect of which subtitle was watched first, two versions
>>> of
>>> the questionnaire were relied on: one showing the ?SA subtitle first? and
>>> another showing the ?EA subtitle first?. Nearly half the participants
>>> answered the first and nearly half answered the latter. The info
>>> regarding
>>> this is called WF (watched first).
>>>
>>> I am focusing on which social factor/s lead/s the participants to
>>> evaluate
>>> one of the two subtitles as generally better and which subtitle is more
>>> humorous and closer to Egyptian culture. Actually, I wanted better to be
>>> the only dependent factor and asking participants 'which subtitle is
>>> better?' could be enough, but I wanted to have detailed information of
>>> why
>>> a subtitle is better by asking participants specific questions (regarding
>>> which subtitle is more humorous and closer to Egyptian culture). Most of
>>> the time, the total of the hum + cul = better, but sometimes it is not
>>> (e.g. the sum for subtitle EA could be bigger than for SA, but the
>>> participant prefers SA in the better column).
>>>
>>> I thought that mixed effects analyses would clarify the picture and
>>> answer
>>> the research questions (which  factor/s lead/s participants to favour a
>>> subtitle over another?) and, so,  tried the lme4 package in R and ran
>>> many
>>> models but all the codes I have used are not working.
>>>
>>> I ran the following codes, which yielded Error messages, like:
>>>
>>> model1<- lmer (better ~ gender + age + education + WF + (1 | part),
>>> data=sub_data)
>>>
>>> Error: number of levels of each grouping factor must be < number of
>>> observations (problems: part)
>>>
>>>
>>>
>>> Model2 <- glmer (better ~ gender + age + education + WF + (1 | part),
>>> data
>>> = sub_data, family='binomial')
>>>
>>> Error in mkRespMod(fr, family = family) :
>>>
>>>   response must be numeric or factor
>>>
>>>
>>>
>>> Model3 <- glmer (better ~ age + gender + education + WF + (1 | part),
>>> data
>>> = sub_data, family='binomial',
>>> control=glmerControl(optimizer=c("bobyqa")))
>>>
>>> Error in mkRespMod(fr, family = family) :
>>>
>>>   response must be numeric or factor
>>>
>>>
>>>
>>> Why do the models crash? Does the problem lie in the random factor part
>>> (which
>>> is a code for participants)? Or is it something related to the mixed
>>> effects analysis?
>>>
>>> I hope I am not violating the rules here as I am attaching the dataset (
>>> sub_data) just in case someone would like to have a look at it.
>>>
>>>  All the best
>>>
>>> --
>>> Saudi Sadiq,
>>>
>>> --
>>> Saudi Sadiq,
>>>
>>> Lecturer, Minia University, Egypt
>>>
>>> Academia <http://york.academia.edu/SaudiSadiq>, Reserachgate
>>> <https://www.researchgate.net/profile/Saudi_Sadiq>, Google Scholar
>>> <https://scholar.google.co.uk/citations?user=h0latzcAAAAJ&hl=en>,
>>> Publons
>>> <https://publons.com/researcher/2950905/saudi-sadiq/>
>>>
>>> Certified Translator by (Egyta) <https://www.egyta.com/>
>>>
>>> Associate Fellow of the Higher Education Academy, UK
>>> <https://www.heacademy.ac.uk/>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>
>
> --
> Saudi Sadiq,
>
> Lecturer, Minia University, Egypt
>
> Academia <http://york.academia.edu/SaudiSadiq>, Reserachgate
> <https://www.researchgate.net/profile/Saudi_Sadiq>, Google Scholar
> <https://scholar.google.co.uk/citations?user=h0latzcAAAAJ&hl=en>, Publons
> <https://publons.com/researcher/2950905/saudi-sadiq/>
>
> Certified Translator by (Egyta) <https://www.egyta.com/>
>
> Associate Fellow of the Higher Education Academy, UK
> <https://www.heacademy.ac.uk/>
>

	[[alternative HTML version deleted]]


From p|erre@dev|||emereu|| @end|ng |rom ephe@p@|@eu  Thu Jun 18 09:23:45 2020
From: p|erre@dev|||emereu|| @end|ng |rom ephe@p@|@eu (Pierre de Villemereuil)
Date: Thu, 18 Jun 2020 09:23:45 +0200
Subject: [R-sig-ME] 
 Poor mixing and autocorrelation of ZIP data in MCMCglmm
In-Reply-To: <CAJkG0X850TWzOC-Qv_wqFp=fGqMdRsbnUfxaJOU=_KBzvJ7R1Q@mail.gmail.com>
References: <CAJkG0X_FGgX2m-OfabPQ7XkRH9=D6OhyOxNWi6_FKpa8n3zLrA@mail.gmail.com>
 <CAJkG0X-3ATUHBDVbuMJRgruV1woW+2Fv-XKuDDzBrZoYoMJjpg@mail.gmail.com>
 <CAJkG0X850TWzOC-Qv_wqFp=fGqMdRsbnUfxaJOU=_KBzvJ7R1Q@mail.gmail.com>
Message-ID: <3089202.s3P4qfXpdV@flyosfixe>

Hi,

I agree with Ben, I believe it is a shame to waste that much information. With multiple measurements, *sometimes* removing non-repeated individuals might help improve things, but not always. I would try to include all individuals, and only if you still have issues, start by keeping individuals with at least 2 measurements (5 is too high a threshold in my humble opinion).

Now, for your results, it's nice to see the variance is a bit more reasonable (9 is still a high value, but it is not an impossible one if "id" explains the zeros a lot). However, your effective sample sizes for the zi-related variances is still quite low, so you need to run the MCMC for longer, maybe try to reach at least 200 or 500 (1000 would be a more comfortable target, but it might be difficult to reach if auto-correlation is high).

Cheers,
Pierre.

Le mercredi 17 juin 2020, 11:02:49 CEST Vital Heim a ?crit :
> Dear Pierre,
> 
> Thank you again for the answer and the new prior!
> 
> I have been working on the model last week and your question if I have
> enough data got me thinking. I therefore reconsidered my available data and
> also looked into if I can simplify my model.
> 
> The model I submitted before had 378 observations. The observations came
> from 28 different individuals. However, I think one problem could have been
> that most of these individuals only had 1 observation because they only
> appeared at a provisioning event once. I therefore decided to remove all
> the individuals that have less than 5 total observations from the data set
> so that theoretically every animal had at least 5 opportunities to make a
> decision to attend (and for how long) a provisioning event or not. That
> left me with 13 individuals and a total of 405 observations (as I do not
> have temperature as a fixed effect I could keep the rows with temperature
> == NA, resulting in more observations than before). The nr. observations
> per individual are:
> 
> id   12 13 14 16 17 19 20 21 34 38 39 40 42
> #obs 27 54 67 52 44 34 26 35 41  7  7  5  6
> 
> Trying to simplify my model had me removing two fixed effects (temperature
> and gender) from my model formula. That left me with the model:
> 
> priorP2 <- list(R = list(V = diag(2), nu = 2, fix = 2),
>                 G = list(
>                 G1 = list(V = diag(2), nu = 1000, alpha.mu = c(0,0),
> alpha.V = diag(2))
>                ,G2 = list(V = diag(2), nu = 1000, alpha.mu = c(0,0),
> alpha.V = diag(2))))
> 
> NITT <- 320000
> BURN <- 20000
> THIN <- 200
> MULT <- 10
> 
> model3.1 <- MCMCglmm(bait ~ trait - 1
>                      + trait:individuals + trait:tourists + trait:operators
> + at.level(trait,1):presence
>                      , random = ~idh(trait):id + idh(trait):event
>                      , rcov =~ idh(trait):units
>                      , data=df, family="zipoisson", prior=priorP2
>                      , nitt=NITT*MULT, burnin=BURN*MULT, thin = THIN*MULT,
> verbose=FALSE)
> 
> The model resulted in the following summary:
> 
> > summary(model3.1)
> 
> 
> 
>  Iterations = 200001:3198001
> 
>  Thinning interval  = 2000
> 
>  Sample size  = 1500
> 
> 
> 
>  DIC: 4295.048
> 
> 
> 
>  G-structure:  ~idh(trait):id
> 
> 
> 
>                  post.mean l-95% CI u-95% CI eff.samp
> 
> baitIntake.id       0.1969   0.0204   0.5101     1500
> 
> zi_baitIntake.id    9.6323   4.3901  17.0756      151
> 
> 
> 
>                ~idh(trait):dive
> 
> 
> 
>                    post.mean  l-95% CI u-95% CI eff.samp
> 
> baitIntake.event       0.2955 1.501e-01   0.4423  1720.47
> 
> zi_baitIntake.event    0.6166 1.873e-06   2.2227    91.55
> 
> 
> 
>  R-structure:  ~idh(trait):units
> 
> 
> 
>                     post.mean l-95% CI u-95% CI eff.samp
> 
> baitIntake.units       0.5085   0.4217   0.6023     1641
> 
> zi_baitIntake.units    1.0000   1.0000   1.0000        0
> 
> 
> 
>  Location effects: bait ~ trait - 1 + trait:individuals + trait:tourists +
> trait:operators + at.level(trait, 1):presence
> 
> 
> 
>                            post.mean  l-95% CI  u-95% CI eff.samp  pMCMC
> 
> baitIntake                  5.800698  5.169356  6.423403   1500.0 <7e-04 ***
> 
> zi_baitIntake              -0.607708 -3.430961  2.148515    719.3 0.6467
> 
> baitIntake:individuals     -0.053430 -0.120854  0.018248   1500.0 0.1347
> 
> zi_baitIntake:individuals   0.070798 -0.207224  0.358939    340.2 0.6307
> 
> baitIntake:tourists         0.054909  0.024764  0.089098   1500.0 <7e-04 ***
> 
> zi_baitIntake:tourists     -0.111844 -0.248268  0.023882    222.7 0.0840 .
> 
> baitIntake:operators        0.119909  0.016259  0.208378   1500.0 0.0253 *
> 
> zi_baitIntake:operators    -0.181109 -0.552908  0.211214    316.1 0.3520
> 
> presence                    0.009881  0.008174  0.011563   1500.0 <7e-04 ***
> 
> ---
> 
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> The new model formula and adjusting the df for only animals that have
> attended the events at least 5 times helped I think. The variance in the
> zi.baitintake.id is a lot smaller now (from 140 in the previous model to 9
> in this model). I think there is still a rather large uncertainty in some
> of the fixed effects and the intercept for each trait but am not sure if
> that would be within the range that is acceptable?
> 
> The autocorrelation diagnostics looked as follows:
> 
> > autocorr.diag(model3.1$Sol)
> 
>             baitIntake zi_baitIntake baitIntake:individuals
> zi_baitIntake:individuals baitIntake:tourists zi_baitIntake:tourists
> 
> Lag 0      1.000000000    1.00000000            1.000000000
> 1.000000000         1.000000000              1.0000000
> 
> Lag 2000   0.026012481    0.26311228            0.011550676
> 0.566133816        -0.006107718              0.5590814
> 
> Lag 10000 -0.036331924    0.01332843            0.003078598
> 0.132021786        -0.023601403              0.1855635
> 
> Lag 20000  0.026546529   -0.01065644            0.005180587
> 0.003356647        -0.006827257              0.0705329
> 
> Lag 1e+05  0.009254395    0.02910151            0.003647482
>  -0.045349648        -0.033278001             -0.0276361
> 
>           baitIntake:operators zi_baitIntake:operators      presence
> 
> Lag 0              1.000000000              1.00000000  1.0000000000
> 
> Lag 2000           0.017853409              0.53269135  0.0309542019
> 
> Lag 10000         -0.001788713              0.15219674  0.0039462749
> 
> Lag 20000         -0.067415168              0.04973919 -0.0084329378
> 
> Lag 1e+05         -0.010173980              0.03747686 -0.0009911261
> 
> 
> 
> > autocorr.diag(model3.1$VCV)
> 
>           baitIntake.id zi_baitIntake.id baitIntake.event
> zi_baitIntake.event baitIntake.units zi_baitIntake.units
> 
> Lag 0       1.000000000      1.000000000      1.000000000
> 1.00000000       1.00000000                 NaN
> 
> Lag 2000    0.010988496      0.279553940     -0.006822039
> 0.78582472      -0.04529223                 NaN
> 
> Lag 10000   0.005850551      0.255547273     -0.020832316
> 0.48146906       0.01942578                 NaN
> 
> Lag 20000   0.006862719      0.175722325     -0.080220078
> 0.29263378      -0.01864084                 NaN
> 
> Lag 1e+05   0.036323891      0.003939182     -0.005860269
>  -0.01147487      -0.01569073                 NaN
> 
> I attached the trace and density plots to this message. I think overall the
> mixing improved. The mixing of the random effects improved compared to the
> previous model but is still looking a bit off when I compare it to the
> mixing of the fixed effects.
> 
> Thank you for your suggestion how to correctly report the prior choice in a
> paper - that is very helpful.
> 
> Kind regards,
> Vital
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 


From p|erre@dev|||emereu|| @end|ng |rom ephe@p@|@eu  Thu Jun 18 09:45:12 2020
From: p|erre@dev|||emereu|| @end|ng |rom ephe@p@|@eu (Pierre de Villemereuil)
Date: Thu, 18 Jun 2020 09:45:12 +0200
Subject: [R-sig-ME] Including binary and Gaussian responses in one
 multivariate mixed model
In-Reply-To: <408FBEE9-216E-42B7-AF90-BDF70FCF4EF8@gmail.com>
References: <408FBEE9-216E-42B7-AF90-BDF70FCF4EF8@gmail.com>
Message-ID: <6987531.jHliUfeiLT@flyosfixe>

Hi,

First thing: I don't think your prior is right. You need to place the "categorical" trait in second (you might want to use "threshold" family instead by the way, unless you insist on having a logit link function) and then fix its residual variance by using "fix = 2". Also, even if you do this, I believe your current prior will impose a strong prior into covariances near 0. You can play a bit with the priors using the following script:
https://github.com/devillemereuil/prior-MCMCglmm/

As for the other questions
1. Yes.
2. "2" should relate to a level in your factor when you call "factor(Direction)", MCMCglmm is simply following R formula rules here I believe. You can name your levels using the "levels" argument of the factor() function.
3. I believe the source of these errors is the fact that the residual variance needs to be fixed for the binary trait as I mentioned above.
4. Yes. I don't know of any package performing this out-of-the-box but I never looked. Computing the "fixed-effects variance" as in Nakagawa & Schielzeth can be performed like this:

compute_varpred <- function(beta, design_matrix) {
  var(as.vector(design_matrix %*% beta)) 
}
vf <- apply(test.mmm$Sol, 1, compute_varpred, design_matrix = test.mmm$X)

5. I believe the latent variables are stored directly in test.mmm$Liab if "pl = TRUE" was set.

Hope this helps,
Pierre

Le mercredi 17 juin 2020, 19:12:08 CEST Yi-Hsiu Chen a ?crit :
> Dear list members,
> 
> I am trying to fit a multivariate mixed model that includes a binary and a Gaussian response variables with MCMCglmm and met some problems I couldn?t figure out. As I couldn't find much discussion on these topics, I was wondering if I could have some help from the list.
> 
> This question is also posted on Cross Validated (sorry for cross-posting), so please visit this link for a more readable version:  https://stats.stackexchange.com/questions/472662/including-both-binary-and-gaussian-responses-in-one-multivariate-mixed-model-in <https://stats.stackexchange.com/questions/472662/including-both-binary-and-gaussian-responses-in-one-multivariate-mixed-model-in> . 
> 
> Here is my question: 
> 
> I would like to fit a multivariate mixed model to a dataset that contains: a binary Direction variable showing the direction the individuals moved toward, a continuous PropRangeChange_s variable showing the scaled proportion of the occupied range increased to old occupied range (OldRange), a continuous Dist variable showing the distance between new range centre and a key location, a categorical Type stating type of the individuals; categorical variables SiteID and Lat_band showing where the observations were recorded, in which SiteID is nested within Lat_band (more than one SiteID in one Lat_band level). The example data can be downloaded at this link: https://drive.google.com/drive/folders/16sjYpBSBnu3XCNcJDPpXcJQuo4UDj8cc?usp=sharing
> 
> What I am interested in is: how Direction and PropRangeChange_s change in response to OldRange, Dist, and Type ? I am also interested in whether the probability of moving toward certain direction is related to PropRangeChange_s.
> 
> I therefore tried fitting a multivariate mixed model with MCMCglmm with below code (also included in the shared folder):
> 
> library(MCMCglmm)
> library(coda) 
> # Read in data required
> test.data<-read.csv(file="D:/Data/ExampleData.csv", header=TRUE, sep=",")
> 
> # iteration setting
> niterations <- 50000 
> nburnin <- 10000
> nthin <- 50 
> 
> # set priors
> prior.op1 <- list(R = list(R1 = list(V = diag(2), nu = 0.002)), 
>                   G = list(G1 = list(V = diag(2), nu = 0.01, alpha.mu = rep(0, 2), alpha.V = diag(2) * 1000),
>                            G2 = list(V = diag(2), nu = 0.01, alpha.mu = rep(0, 2), alpha.V = diag(2) * 1000)))
> 
> # Model test 1 - include Type as fixed effect
> set.seed(1)
> test.mmm.c1 <- MCMCglmm(cbind(factor(Direction), PropRangeChange_s) ~ 0 + trait + trait:(scale(OldRange)*scale(Dist)*Type),
>                             random = ~us(trait):Lat_band + us(trait):Lat_band:SiteID , rcov = ~us(trait):units,
>                             data = test.data, family = c("categorical", "gaussian"), prior = prior.op1, nitt = niterations, burnin = nburnin, thin = nthin, verbose = T, pr = T,pl=T) 
> 
> set.seed(2)
> test.mmm.c2 <- MCMCglmm(cbind(factor(Direction), PropRangeChange_s) ~ 0 + trait + trait:(scale(OldRange)*scale(Dist)*Type),
>                         random = ~us(trait):Lat_band + us(trait):Lat_band:SiteID , rcov = ~us(trait):units,
>                         data = test.data, family = c("categorical", "gaussian"), prior = prior.op1, nitt = niterations, burnin = nburnin, thin = nthin, verbose = T, pr = T,pl=T) 
> 
> #extract fixed effects
> test.mmm.FIX <- mcmc.list(test.mmm.c1$Sol,test.mmm.c2$Sol)
> 
> #extract random effects
> test.mmm.VCV <- mcmc.list(test.mmm.c1$VCV,test.mmm.c2$VCV) 
> 
> # Check convergence
> max(gelman.diag(test.mmm.FIX[, 1:48])$psrf)        #[Return] 1.027717
> 
> max(gelman.diag(test.mmm.VCV)$psrf)                # Error shown
> 
> # Check estimate for fixed effects
> summary(test.mmm.FIX[, 1:48])                      # Very large estimate
> 
> # Check the corelations between two responses 
> test.mmm.Lat_band<-mMULTIVCV[, 1:4]
> test.mmm.SiteID<-mMULTIVCV[, 5:8]
> test.mmm.units<-mMULTIVCV[, 9:12]
> 
> summary(test.mmm.Lat_band)                         # Very large estimate 
> summary(test.mmm.units)  
> 
> test.mmm.Lat_band.summary<-summary(mcmc.list(posterior.cor(test.mmm.c1$VCV[, 1:4]), posterior.cor(test.mmm.c2$VCV[, 1:4])))
> # Q: weird quantiles that ranges from -1 to 1, which is the entire parameter range for correlation
> test.mmm.units.summary<-summary(mcmc.list(posterior.cor(test.mmm.c1$VCV[, 9:12]), posterior.cor(test.mmm.c2$VCV[, 9:12])))
> 
> The script run smoothly with no warnings showing up. However, when I checked the estimates, I found all the estimates for Direction-related coefficients are very large (such as, more than 10^4), which doesn't look correct. I therefore was wondering if I can have some help on following questions:
> 
> 	? Is it statistically acceptable to include both binary and continuous variables as response variables?
> 
> 	? When I check the fixed effects, for Direction-related coefficients, it only states "traitDirection.2" in the output, how could I know which Direction level it is for?
> 
> 	? As the chains looks converging well for the fixed effects (Gelman?Rubin diagnostic = 1.027), what might be the causes for the error message ? Error in chol.default(W) : the leading minor of order 3 is not positive definite ? shown when checking the convergence for random effects and the seemly unlikely overly large coefficient estimates?
> 
> 	? Is it possible to obtain marginal R^2 and conditional R^2 with MCMCglmm?
> 
> 	? Can I extract the latent variable, the probability of moving upward, with function: predict(test.mmm.c1, marginal = NULL, posterior = "mean", type = "response") 
> 
> 
> Sorry for the long post. This is my first multivariate mixed model as well as my first MCMCglmm model, hope I didn?t make a fundamental mistake. Any suggestions would be appreciated.
> 
> 
> Best wishes
> Yi-Hsiu
> 
> 
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 


From y|h@|uc42 @end|ng |rom gm@||@com  Thu Jun 18 13:11:18 2020
From: y|h@|uc42 @end|ng |rom gm@||@com (Yi-Hsiu Chen)
Date: Thu, 18 Jun 2020 19:11:18 +0800
Subject: [R-sig-ME] Including binary and Gaussian responses in one
 multivariate mixed model
In-Reply-To: <6987531.jHliUfeiLT@flyosfixe>
References: <408FBEE9-216E-42B7-AF90-BDF70FCF4EF8@gmail.com>
 <6987531.jHliUfeiLT@flyosfixe>
Message-ID: <55E76AE7-08A3-4FF9-BC15-718593EAC1C5@gmail.com>

Dear Pierre,

Thank you very much for the reply. The suggestions are helpful, these unlikely, very large estimates vanish after the categorical trait was placed in second and the prior for the residual variance was fixed. Unfortunately the error messages ? Error in cholesterols.default (W): the leading minor of order 3 is not positive definite? were still shown when I checked the convergence for random effects with german.diag(), I wonder if it indicates that more iterations are required.     

Before I proceed for a longer run, I was wondering would you (or any other list member) mind further explaining for me why my priors are strong?

From my understanding about the priors in MCMCglmm, the probability density distributions yielded by the prior setting are mainly controlled by nu, which indicates the degree of our belief on the prior values. Therefore, the lower the nu is, the flatter the probability density distribution it yields, namely the less informative it is. With the R script you provided via the link, it seems to show the same - the lower the nu is, the flatter the resulting density distributions look like. Since my priors were set to nu=0.002 and alpha.V=diag(2)*1000, I thought they are informative and rather flat priors. 

I was wondering is my understanding about the priors in MCMCglmm incorrect?    


Cheers,
Yi-Hsiu


> Pierre de Villemereuil <pierre.devillemereuil at ephe.psl.eu> ? 2020?6?18? 15:45 ???
> 
> Hi,
> 
> First thing: I don't think your prior is right. You need to place the "categorical" trait in second (you might want to use "threshold" family instead by the way, unless you insist on having a logit link function) and then fix its residual variance by using "fix = 2". Also, even if you do this, I believe your current prior will impose a strong prior into covariances near 0. You can play a bit with the priors using the following script:
> https://github.com/devillemereuil/prior-MCMCglmm/
> 
> As for the other questions
> 1. Yes.
> 2. "2" should relate to a level in your factor when you call "factor(Direction)", MCMCglmm is simply following R formula rules here I believe. You can name your levels using the "levels" argument of the factor() function.
> 3. I believe the source of these errors is the fact that the residual variance needs to be fixed for the binary trait as I mentioned above.
> 4. Yes. I don't know of any package performing this out-of-the-box but I never looked. Computing the "fixed-effects variance" as in Nakagawa & Schielzeth can be performed like this:
> 
> compute_varpred <- function(beta, design_matrix) {
>  var(as.vector(design_matrix %*% beta)) 
> }
> vf <- apply(test.mmm$Sol, 1, compute_varpred, design_matrix = test.mmm$X)
> 
> 5. I believe the latent variables are stored directly in test.mmm$Liab if "pl = TRUE" was set.
> 
> Hope this helps,
> Pierre
> 
> Le mercredi 17 juin 2020, 19:12:08 CEST Yi-Hsiu Chen a ?crit :
>> Dear list members,
>> 
>> I am trying to fit a multivariate mixed model that includes a binary and a Gaussian response variables with MCMCglmm and met some problems I couldn?t figure out. As I couldn't find much discussion on these topics, I was wondering if I could have some help from the list.
>> 
>> This question is also posted on Cross Validated (sorry for cross-posting), so please visit this link for a more readable version:  https://stats.stackexchange.com/questions/472662/including-both-binary-and-gaussian-responses-in-one-multivariate-mixed-model-in <https://stats.stackexchange.com/questions/472662/including-both-binary-and-gaussian-responses-in-one-multivariate-mixed-model-in> . 
>> 
>> Here is my question: 
>> 
>> I would like to fit a multivariate mixed model to a dataset that contains: a binary Direction variable showing the direction the individuals moved toward, a continuous PropRangeChange_s variable showing the scaled proportion of the occupied range increased to old occupied range (OldRange), a continuous Dist variable showing the distance between new range centre and a key location, a categorical Type stating type of the individuals; categorical variables SiteID and Lat_band showing where the observations were recorded, in which SiteID is nested within Lat_band (more than one SiteID in one Lat_band level). The example data can be downloaded at this link: https://drive.google.com/drive/folders/16sjYpBSBnu3XCNcJDPpXcJQuo4UDj8cc?usp=sharing
>> 
>> What I am interested in is: how Direction and PropRangeChange_s change in response to OldRange, Dist, and Type ? I am also interested in whether the probability of moving toward certain direction is related to PropRangeChange_s.
>> 
>> I therefore tried fitting a multivariate mixed model with MCMCglmm with below code (also included in the shared folder):
>> 
>> library(MCMCglmm)
>> library(coda) 
>> # Read in data required
>> test.data<-read.csv(file="D:/Data/ExampleData.csv", header=TRUE, sep=",")
>> 
>> # iteration setting
>> niterations <- 50000 
>> nburnin <- 10000
>> nthin <- 50 
>> 
>> # set priors
>> prior.op1 <- list(R = list(R1 = list(V = diag(2), nu = 0.002)), 
>>                  G = list(G1 = list(V = diag(2), nu = 0.01, alpha.mu = rep(0, 2), alpha.V = diag(2) * 1000),
>>                           G2 = list(V = diag(2), nu = 0.01, alpha.mu = rep(0, 2), alpha.V = diag(2) * 1000)))
>> 
>> # Model test 1 - include Type as fixed effect
>> set.seed(1)
>> test.mmm.c1 <- MCMCglmm(cbind(factor(Direction), PropRangeChange_s) ~ 0 + trait + trait:(scale(OldRange)*scale(Dist)*Type),
>>                            random = ~us(trait):Lat_band + us(trait):Lat_band:SiteID , rcov = ~us(trait):units,
>>                            data = test.data, family = c("categorical", "gaussian"), prior = prior.op1, nitt = niterations, burnin = nburnin, thin = nthin, verbose = T, pr = T,pl=T) 
>> 
>> set.seed(2)
>> test.mmm.c2 <- MCMCglmm(cbind(factor(Direction), PropRangeChange_s) ~ 0 + trait + trait:(scale(OldRange)*scale(Dist)*Type),
>>                        random = ~us(trait):Lat_band + us(trait):Lat_band:SiteID , rcov = ~us(trait):units,
>>                        data = test.data, family = c("categorical", "gaussian"), prior = prior.op1, nitt = niterations, burnin = nburnin, thin = nthin, verbose = T, pr = T,pl=T) 
>> 
>> #extract fixed effects
>> test.mmm.FIX <- mcmc.list(test.mmm.c1$Sol,test.mmm.c2$Sol)
>> 
>> #extract random effects
>> test.mmm.VCV <- mcmc.list(test.mmm.c1$VCV,test.mmm.c2$VCV) 
>> 
>> # Check convergence
>> max(gelman.diag(test.mmm.FIX[, 1:48])$psrf)        #[Return] 1.027717
>> 
>> max(gelman.diag(test.mmm.VCV)$psrf)                # Error shown
>> 
>> # Check estimate for fixed effects
>> summary(test.mmm.FIX[, 1:48])                      # Very large estimate
>> 
>> # Check the corelations between two responses 
>> test.mmm.Lat_band<-mMULTIVCV[, 1:4]
>> test.mmm.SiteID<-mMULTIVCV[, 5:8]
>> test.mmm.units<-mMULTIVCV[, 9:12]
>> 
>> summary(test.mmm.Lat_band)                         # Very large estimate 
>> summary(test.mmm.units)  
>> 
>> test.mmm.Lat_band.summary<-summary(mcmc.list(posterior.cor(test.mmm.c1$VCV[, 1:4]), posterior.cor(test.mmm.c2$VCV[, 1:4])))
>> # Q: weird quantiles that ranges from -1 to 1, which is the entire parameter range for correlation
>> test.mmm.units.summary<-summary(mcmc.list(posterior.cor(test.mmm.c1$VCV[, 9:12]), posterior.cor(test.mmm.c2$VCV[, 9:12])))
>> 
>> The script run smoothly with no warnings showing up. However, when I checked the estimates, I found all the estimates for Direction-related coefficients are very large (such as, more than 10^4), which doesn't look correct. I therefore was wondering if I can have some help on following questions:
>> 
>> 	? Is it statistically acceptable to include both binary and continuous variables as response variables?
>> 
>> 	? When I check the fixed effects, for Direction-related coefficients, it only states "traitDirection.2" in the output, how could I know which Direction level it is for?
>> 
>> 	? As the chains looks converging well for the fixed effects (Gelman?Rubin diagnostic = 1.027), what might be the causes for the error message ? Error in chol.default(W) : the leading minor of order 3 is not positive definite ? shown when checking the convergence for random effects and the seemly unlikely overly large coefficient estimates?
>> 
>> 	? Is it possible to obtain marginal R^2 and conditional R^2 with MCMCglmm?
>> 
>> 	? Can I extract the latent variable, the probability of moving upward, with function: predict(test.mmm.c1, marginal = NULL, posterior = "mean", type = "response") 
>> 
>> 
>> Sorry for the long post. This is my first multivariate mixed model as well as my first MCMCglmm model, hope I didn?t make a fundamental mistake. Any suggestions would be appreciated.
>> 
>> 
>> Best wishes
>> Yi-Hsiu
>> 
>> 
>> 
>> 
>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> 
> 
> 
> 


From p|erre@dev|||emereu|| @end|ng |rom ephe@p@|@eu  Thu Jun 18 13:34:20 2020
From: p|erre@dev|||emereu|| @end|ng |rom ephe@p@|@eu (Pierre de Villemereuil)
Date: Thu, 18 Jun 2020 13:34:20 +0200
Subject: [R-sig-ME] Including binary and Gaussian responses in one
 multivariate mixed model
In-Reply-To: <55E76AE7-08A3-4FF9-BC15-718593EAC1C5@gmail.com>
References: <408FBEE9-216E-42B7-AF90-BDF70FCF4EF8@gmail.com>
 <6987531.jHliUfeiLT@flyosfixe>
 <55E76AE7-08A3-4FF9-BC15-718593EAC1C5@gmail.com>
Message-ID: <2456774.KsEx7aiSro@flyosfixe>

Hi,

Yes, sorry, I wasn't thinking straight when I mentioned the strong prior on the covariance, I was thinking about something else.

I have no idea what is generating this error in the gelman.diag() function however. Sorry.

Cheers,
Pierre.

Le jeudi 18 juin 2020, 13:11:18 CEST Yi-Hsiu Chen a ?crit :
> Dear Pierre,
> 
> Thank you very much for the reply. The suggestions are helpful, these unlikely, very large estimates vanish after the categorical trait was placed in second and the prior for the residual variance was fixed. Unfortunately the error messages ? Error in cholesterols.default (W): the leading minor of order 3 is not positive definite? were still shown when I checked the convergence for random effects with german.diag(), I wonder if it indicates that more iterations are required.     
> 
> Before I proceed for a longer run, I was wondering would you (or any other list member) mind further explaining for me why my priors are strong?
> 
> From my understanding about the priors in MCMCglmm, the probability density distributions yielded by the prior setting are mainly controlled by nu, which indicates the degree of our belief on the prior values. Therefore, the lower the nu is, the flatter the probability density distribution it yields, namely the less informative it is. With the R script you provided via the link, it seems to show the same - the lower the nu is, the flatter the resulting density distributions look like. Since my priors were set to nu=0.002 and alpha.V=diag(2)*1000, I thought they are informative and rather flat priors. 
> 
> I was wondering is my understanding about the priors in MCMCglmm incorrect?    
> 
> 
> Cheers,
> Yi-Hsiu
> 
> 
> > Pierre de Villemereuil <pierre.devillemereuil at ephe.psl.eu> ? 2020?6?18? 15:45 ???
> > 
> > Hi,
> > 
> > First thing: I don't think your prior is right. You need to place the "categorical" trait in second (you might want to use "threshold" family instead by the way, unless you insist on having a logit link function) and then fix its residual variance by using "fix = 2". Also, even if you do this, I believe your current prior will impose a strong prior into covariances near 0. You can play a bit with the priors using the following script:
> > https://github.com/devillemereuil/prior-MCMCglmm/
> > 
> > As for the other questions
> > 1. Yes.
> > 2. "2" should relate to a level in your factor when you call "factor(Direction)", MCMCglmm is simply following R formula rules here I believe. You can name your levels using the "levels" argument of the factor() function.
> > 3. I believe the source of these errors is the fact that the residual variance needs to be fixed for the binary trait as I mentioned above.
> > 4. Yes. I don't know of any package performing this out-of-the-box but I never looked. Computing the "fixed-effects variance" as in Nakagawa & Schielzeth can be performed like this:
> > 
> > compute_varpred <- function(beta, design_matrix) {
> >  var(as.vector(design_matrix %*% beta)) 
> > }
> > vf <- apply(test.mmm$Sol, 1, compute_varpred, design_matrix = test.mmm$X)
> > 
> > 5. I believe the latent variables are stored directly in test.mmm$Liab if "pl = TRUE" was set.
> > 
> > Hope this helps,
> > Pierre
> > 
> > Le mercredi 17 juin 2020, 19:12:08 CEST Yi-Hsiu Chen a ?crit :
> >> Dear list members,
> >> 
> >> I am trying to fit a multivariate mixed model that includes a binary and a Gaussian response variables with MCMCglmm and met some problems I couldn?t figure out. As I couldn't find much discussion on these topics, I was wondering if I could have some help from the list.
> >> 
> >> This question is also posted on Cross Validated (sorry for cross-posting), so please visit this link for a more readable version:  https://stats.stackexchange.com/questions/472662/including-both-binary-and-gaussian-responses-in-one-multivariate-mixed-model-in <https://stats.stackexchange.com/questions/472662/including-both-binary-and-gaussian-responses-in-one-multivariate-mixed-model-in> . 
> >> 
> >> Here is my question: 
> >> 
> >> I would like to fit a multivariate mixed model to a dataset that contains: a binary Direction variable showing the direction the individuals moved toward, a continuous PropRangeChange_s variable showing the scaled proportion of the occupied range increased to old occupied range (OldRange), a continuous Dist variable showing the distance between new range centre and a key location, a categorical Type stating type of the individuals; categorical variables SiteID and Lat_band showing where the observations were recorded, in which SiteID is nested within Lat_band (more than one SiteID in one Lat_band level). The example data can be downloaded at this link: https://drive.google.com/drive/folders/16sjYpBSBnu3XCNcJDPpXcJQuo4UDj8cc?usp=sharing
> >> 
> >> What I am interested in is: how Direction and PropRangeChange_s change in response to OldRange, Dist, and Type ? I am also interested in whether the probability of moving toward certain direction is related to PropRangeChange_s.
> >> 
> >> I therefore tried fitting a multivariate mixed model with MCMCglmm with below code (also included in the shared folder):
> >> 
> >> library(MCMCglmm)
> >> library(coda) 
> >> # Read in data required
> >> test.data<-read.csv(file="D:/Data/ExampleData.csv", header=TRUE, sep=",")
> >> 
> >> # iteration setting
> >> niterations <- 50000 
> >> nburnin <- 10000
> >> nthin <- 50 
> >> 
> >> # set priors
> >> prior.op1 <- list(R = list(R1 = list(V = diag(2), nu = 0.002)), 
> >>                  G = list(G1 = list(V = diag(2), nu = 0.01, alpha.mu = rep(0, 2), alpha.V = diag(2) * 1000),
> >>                           G2 = list(V = diag(2), nu = 0.01, alpha.mu = rep(0, 2), alpha.V = diag(2) * 1000)))
> >> 
> >> # Model test 1 - include Type as fixed effect
> >> set.seed(1)
> >> test.mmm.c1 <- MCMCglmm(cbind(factor(Direction), PropRangeChange_s) ~ 0 + trait + trait:(scale(OldRange)*scale(Dist)*Type),
> >>                            random = ~us(trait):Lat_band + us(trait):Lat_band:SiteID , rcov = ~us(trait):units,
> >>                            data = test.data, family = c("categorical", "gaussian"), prior = prior.op1, nitt = niterations, burnin = nburnin, thin = nthin, verbose = T, pr = T,pl=T) 
> >> 
> >> set.seed(2)
> >> test.mmm.c2 <- MCMCglmm(cbind(factor(Direction), PropRangeChange_s) ~ 0 + trait + trait:(scale(OldRange)*scale(Dist)*Type),
> >>                        random = ~us(trait):Lat_band + us(trait):Lat_band:SiteID , rcov = ~us(trait):units,
> >>                        data = test.data, family = c("categorical", "gaussian"), prior = prior.op1, nitt = niterations, burnin = nburnin, thin = nthin, verbose = T, pr = T,pl=T) 
> >> 
> >> #extract fixed effects
> >> test.mmm.FIX <- mcmc.list(test.mmm.c1$Sol,test.mmm.c2$Sol)
> >> 
> >> #extract random effects
> >> test.mmm.VCV <- mcmc.list(test.mmm.c1$VCV,test.mmm.c2$VCV) 
> >> 
> >> # Check convergence
> >> max(gelman.diag(test.mmm.FIX[, 1:48])$psrf)        #[Return] 1.027717
> >> 
> >> max(gelman.diag(test.mmm.VCV)$psrf)                # Error shown
> >> 
> >> # Check estimate for fixed effects
> >> summary(test.mmm.FIX[, 1:48])                      # Very large estimate
> >> 
> >> # Check the corelations between two responses 
> >> test.mmm.Lat_band<-mMULTIVCV[, 1:4]
> >> test.mmm.SiteID<-mMULTIVCV[, 5:8]
> >> test.mmm.units<-mMULTIVCV[, 9:12]
> >> 
> >> summary(test.mmm.Lat_band)                         # Very large estimate 
> >> summary(test.mmm.units)  
> >> 
> >> test.mmm.Lat_band.summary<-summary(mcmc.list(posterior.cor(test.mmm.c1$VCV[, 1:4]), posterior.cor(test.mmm.c2$VCV[, 1:4])))
> >> # Q: weird quantiles that ranges from -1 to 1, which is the entire parameter range for correlation
> >> test.mmm.units.summary<-summary(mcmc.list(posterior.cor(test.mmm.c1$VCV[, 9:12]), posterior.cor(test.mmm.c2$VCV[, 9:12])))
> >> 
> >> The script run smoothly with no warnings showing up. However, when I checked the estimates, I found all the estimates for Direction-related coefficients are very large (such as, more than 10^4), which doesn't look correct. I therefore was wondering if I can have some help on following questions:
> >> 
> >> 	? Is it statistically acceptable to include both binary and continuous variables as response variables?
> >> 
> >> 	? When I check the fixed effects, for Direction-related coefficients, it only states "traitDirection.2" in the output, how could I know which Direction level it is for?
> >> 
> >> 	? As the chains looks converging well for the fixed effects (Gelman?Rubin diagnostic = 1.027), what might be the causes for the error message ? Error in chol.default(W) : the leading minor of order 3 is not positive definite ? shown when checking the convergence for random effects and the seemly unlikely overly large coefficient estimates?
> >> 
> >> 	? Is it possible to obtain marginal R^2 and conditional R^2 with MCMCglmm?
> >> 
> >> 	? Can I extract the latent variable, the probability of moving upward, with function: predict(test.mmm.c1, marginal = NULL, posterior = "mean", type = "response") 
> >> 
> >> 
> >> Sorry for the long post. This is my first multivariate mixed model as well as my first MCMCglmm model, hope I didn?t make a fundamental mistake. Any suggestions would be appreciated.
> >> 
> >> 
> >> Best wishes
> >> Yi-Hsiu
> >> 
> >> 
> >> 
> >> 
> >> 
> >> 
> >> 	[[alternative HTML version deleted]]
> >> 
> >> _______________________________________________
> >> R-sig-mixed-models at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >> 
> > 
> > 
> > 
> 
> 


From v|t@|@he|m @end|ng |rom gm@||@com  Thu Jun 18 21:02:27 2020
From: v|t@|@he|m @end|ng |rom gm@||@com (Vital Heim)
Date: Thu, 18 Jun 2020 21:02:27 +0200
Subject: [R-sig-ME] 
 Poor mixing and autocorrelation of ZIP data in MCMCglmm
Message-ID: <CAJkG0X9O+HzZM2pG6B=85Etb8++z_xRQAhUVH-_C+TYKSM4PQw@mail.gmail.com>

Dear Ben and Pierre,

Thank you to you both for the new solution suggestions and inputs. These
help a lot and have improved my model a lot! I tried to apply your newest
suggestions to my model.

I wanted to double-check if I understand the prior suggestion correctly.
While I do understand the basics of the priors to some extent, I still have
difficulties to look at a model and say which prior would be appropriate. I
thought that the probability density distribution of the prior is flatter
the smaller nu is, i.e. the smaller nu, the weaker the prior. By
recommending to make the prior stronger, do you mean to increase nu? If so,
I have nu=1000 in the G part of the prior formula, which I thought was a
very strong prior already? Or did you mean to increase nu in the R-part of
the prior specification?

I ran the model with the full data set, i.e. none of the animals have been
removed, resulting in 424 observations across 28 individuals.

I read somewhere that the ratio of (NITT-BURN)/THIN should be kept between
1000-2000. So, my question is, if as long as this ratio is between
1000-2000 is it ok to run the model for very long ? or is that trying too
hard to fit the model?

I ran a model with NITT = 920000, BURN = 20000, THIN = 600, i.e.
(NITT-BURN)/THIN = 1500. I attached the details below and added you the
trace and density plots (I removed them for the list email as it would make
the message too large). The ZI-related variance in the baitIntake is
slightly larger again (12 compared to 9 in the previous model where I
excluded some animals).  The effective sample sizes are larger than 200 for
all ZI-related variances but still rather small for zi_baitIntake.id (412)
and zi_baitIntake.event (303). The autocorrelation is also rather large for
some of the ZI-related variances.

a)       Formula, chain parameters and prior

priorP2 <- list(R  = list(V = diag(2), nu = 2, fix = 2),

                G  = list(

                G1 = list(V = diag(2), nu = 1000, alpha.mu = c(0,0), alpha.V
= diag(2))

               ,G2 = list(V = diag(2), nu = 1000, alpha.mu = c(0,0), alpha.V
= diag(2))))



NITT <- 920000

BURN <- 20000

THIN <- 600

MULT <- 10



model6.0 <- MCMCglmm(bait ~ trait - 1

                     + trait:animals + trait:tourists + trait:operators

                     + at.level(trait,1):presence

                     , random = ~idh(trait):id + idh(trait):event

                     , rcov =~ idh(trait):units

                     , data=df, family="zipoisson", prior=priorP2

                     , nitt=NITT*MULT, burnin=BURN*MULT, thin = THIN*MULT

                     , verbose=FALSE)



b)      Summary

> summary(model6.0)



 Iterations = 200001:9194001

 Thinning interval  = 6000

 Sample size  = 1500



 DIC: 4323.011



 G-structure:  ~idh(trait):id



                 post.mean l-95% CI u-95% CI eff.samp

baitIntake.id        0.192  0.01674   0.5017   1500.0

zi_baitIntake.id    12.147  6.01228  19.7226    412.3



               ~idh(trait):event



                    post.mean  l-95% CI u-95% CI eff.samp

baitIntake.event       0.2935 1.474e-01   0.4503   1500.0

zi_baitIntake.event    0.3593 1.969e-07   1.3672    303.2



 R-structure:  ~idh(trait):units



                    post.mean l-95% CI u-95% CI eff.samp

baitIntake.units       0.5073   0.4178   0.5971     1500

zi_baitIntake.units    1.0000   1.0000   1.0000        0



 Location effects: bait ~ trait - 1 + trait:animals + trait:tourists +
trait:operators + at.level(trait, 1):presence



                         post.mean  l-95% CI  u-95% CI eff.samp   pMCMC

baitIntake                5.791012  5.123558  6.393609   1613.3 < 7e-04 ***

zi_baitIntake             1.856890 -0.743995  4.380864    760.7 0.18000

baitIntake:animals       -0.053035 -0.127511  0.014003   1500.0 0.13867

zi_baitIntake:animals     0.094574 -0.191786  0.367077    552.0 0.50267

baitIntake:tourists       0.055122  0.020721  0.086323   1500.0 0.00133 **

zi_baitIntake:tourists   -0.105934 -0.228323  0.009986    541.7 0.06800 .

baitIntake:operators      0.123296  0.026268  0.215146   1625.0 0.01467 *

zi_baitIntake:operators  -0.128753 -0.457240  0.238039    604.3 0.43067

presence                  0.009855  0.008144  0.011535   1500.0 < 7e-04 ***

---

Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1



c)       Effective sample sizes

> round(sort(effectiveSize(model6.0$Sol)))

zi_baitIntake:tourists zi_baitIntake:animals  zi_baitIntake:operators
zi_baitIntake    baitIntake:animals

                   542                   552                      604
            761                  1500

            presence    baitIntake:tourists           baitIntake
baitIntake:operators

                1500                   1500                 1613
         1625



> effectiveSize(vv6.0)

      baitIntake.id    zi_baitIntake.id     baitIntake.event
zi_baitIntake.event    baitIntake.units zi_baitIntake.units

          1500.0000            412.2620           1500.0000
303.1673           1500.0000              0.0000



d)      Autocorrelation diagnostics

> autocorr.diag(model6.0$Sol)

           baitIntake zi_baitIntake baitIntake:animals zi_baitIntake:animals
 baitIntake:tourists zi_baitIntake:tourists

Lag 0      1.00000000   1.000000000        1.000000000
 1.000000000         1.000000000             1.00000000

Lag 6000  -0.03673458   0.205319459        0.011391819
 0.461760213         0.011959233             0.41034495

Lag 30000  0.00868412   0.009355555       -0.004042761
-0.015145382        -0.006931932             0.07049628

Lag 60000 -0.03167273   0.051729288        0.003874329
 0.031838052        -0.017647052             0.02593432

Lag 3e+05  0.02904094   0.034563568        0.011200624
-0.005617972         0.029031181             0.01240007

          baitIntake:operators zi_baitIntake:operators     presence

Lag 0             1.0000000000            1.0000000000  1.000000000

Lag 6000         -0.0403305740            0.3661869833 -0.008982195

Lag 30000        -0.0583269879           -0.0004637806 -0.011342973

Lag 60000        -0.0075806959            0.0026802367  0.019267725

Lag 3e+05        -0.0009343694            0.0035771025  0.065613628



> autocorr.diag(model6.0$VCV)

          baitIntake.id zi_baitIntake.id baitIntake.event
zi_baitIntake.event baitIntake.units zi_baitIntake.units

Lag 0        1.00000000       1.00000000     1.000000000
1.00000000      1.000000000                 NaN

Lag 6000     0.01695959       0.19236401    -0.002904061
0.61268299      0.001951758                 NaN

Lag 30000    0.02745489       0.11551187    -0.004205527
0.16867050     -0.018719149                 NaN

Lag 60000   -0.01129109       0.09528737     0.009901908
0.05404658      0.004567198                 NaN

Lag 3e+05    0.03131679       0.03816832    -0.031074398
0.02954608      0.021244502                 NaN



However, I also read some more publications this morning that used MCMCglmm
and looked at their nitt, burn and thin parameters. A lot of them result in
(NITT-BURN)/THIN = 1000. I therefore re-run my model with parameters that
also had 1000 as a ratio.

The model showed some increase again in the variance (it is at 12 now for
the ZI-related variance). I think the ID does explain the 0?s a lot. Nearly
half of the animals never ate and within the animals that ate at least
once, values from 0.16 to 16.16 were observed. The autocorrelation
diagnostics showed less autocorrelation than in the model yesterday and in
the model above. The smallest effective sample size was 496.  I added you
all the details below.

a)       Model formula, prior and chain parameters

priorP2 <- list(R  = list(V = diag(2), nu = 2, fix = 2),

                G  = list(

                  G1 = list(V = diag(2), nu = 1000, alpha.mu = c(0,0),
alpha.V = diag(2))

                  ,G2 = list(V = diag(2), nu = 1000, alpha.mu = c(0,0),
alpha.V = diag(2))))



NITT <- 920000

BURN <- 20000

THIN <- 900

MULT <- 10



model7.0 <- MCMCglmm(bait ~ trait - 1

                     + trait:animals + trait:tourists + trait:operators +
at.level(trait,1):presence

                     , random = ~idh(trait):id + idh(trait):event

                     , rcov =~ idh(trait):units

                     , data=df, family="zipoisson", prior=priorP2

                     , nitt=NITT*MULT, burnin=BURN*MULT, thin = THIN*MULT,
verbose=FALSE)





b)      Model summary

> summary(model7.0)



 Iterations = 200001:9191001

 Thinning interval  = 9000

 Sample size  = 1000



 DIC: 4322.127



 G-structure:  ~idh(trait):id



                 post.mean l-95% CI u-95% CI eff.samp

baitIntake.id       0.1926   0.0205   0.4951   1000.0

zi_baitIntake.id   12.4161   5.6704  19.7886    757.5



               ~idh(trait):evemt



                   post.mean  l-95% CI u-95% CI eff.samp

baitIntake.event       0.2980 1.496e-01   0.4401   1230.0

zi_baitIntake.event    0.4127 7.273e-09   1.5063    452.8



 R-structure:  ~idh(trait):units



                    post.mean l-95% CI u-95% CI eff.samp

baitIntake.units       0.5032   0.4137   0.5967     1000

zi_baitIntake.units    1.0000   1.0000   1.0000        0



 Location effects: bait ~ trait - 1 + trait:animals + trait:tourists +
trait:operators + at.level(trait, 1):presence



                         post.mean  l-95% CI  u-95% CI eff.samp  pMCMC

baitIntake                5.804482  5.153366  6.375957    971.8 <0.001 ***

zi_baitIntake             1.868819 -0.432166  4.639872   1000.0  0.142

baitIntake:animals       -0.052230 -0.125020  0.014938   1000.0  0.170

zi_baitIntake:animals     0.091327 -0.182633  0.337763    809.5  0.484

baitIntake:tourists       0.054381  0.023034  0.088047   1000.0  0.002 **

zi_baitIntake:tourists   -0.109589 -0.231218  0.017220    784.7  0.084 .

baitIntake:operators      0.119350  0.026822  0.211514   1000.0  0.008 **

zi_baitIntake:operators  -0.125314 -0.468336  0.244574    717.2  0.522

presence                  0.009888  0.008221  0.011422   1000.0 <0.001 ***

---

Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1



c)       Effective sample sizes

> round(sort(effectiveSize(model7.0$Sol)))

 zi_baitIntake:operators zi_baitIntake:tourists zi_baitIntake:animals

                     717                    785                   809

              baitIntake          zi_baitIntake              presence

                     972                   1000                  1000

    baitIntake:operators    baitIntake:tourists    baitIntake:animals

                    1000                   1000                  1000



> effectiveSize(vv7.0)

      baitIntake.id    zi_baitIntake.id     baitIntake.event

          1000.0000            757.4588            1230.0098

 zi_baitIntake.event    baitIntake.units zi_baitIntake.units

            452.8034           1000.0000              0.0000



d)      Autocorrelation diagnostics

> autocorr.diag(model7.0$Sol)

             baitIntake zi_baitIntake baitIntake:animals zi_baitIntake:
animals

Lag 0       1.000000000    1.00000000        1.000000000
1.00000000

Lag 9000    0.017938814    0.02981790        0.006431397
0.10481115

Lag 45000  -0.006022882   -0.03825681       -0.009933080
0.07381003

Lag 90000   0.005797923   -0.02302556        0.008195491
0.01082001

Lag 450000 -0.011994835   -0.06558056       -0.060988406
 -0.04143047

           baitIntake:tourists zi_baitIntake:tourists baitIntake:operators

Lag 0              1.000000000             1.00000000         1.0000000000

Lag 9000           0.013111341             0.12011385         0.0017622903

Lag 45000         -0.006995284             0.02981600        -0.0484513234

Lag 90000         -0.015289329             0.01696414         0.0001681332

Lag 450000        -0.064725489             0.01717594         0.0335091484

           zi_baitIntake:operators     presence

Lag 0                  1.000000000  1.000000000

Lag 9000               0.164230628 -0.007679376

Lag 45000             -0.008430448 -0.020076515

Lag 90000              0.006900147 -0.020648916

Lag 450000            -0.016715936 -0.011170416



> autocorr.diag(model7.0$VCV)

           baitIntake.id zi_baitIntake.id baitIntake.event
zi_baitIntake.event

Lag 0         1.00000000       1.00000000      1.00000000       1.0000000000

Lag 9000     -0.02161770       0.13751550      0.01040345       0.3762189835

Lag 45000     0.01833775      -0.02772668     -0.04564059       0.0273746776

Lag 90000     0.02075539       0.01042728      0.02602572       0.0171497956

Lag 450000   -0.02695780      -0.04212894     -0.02191762       0.0009894756

           baitIntake.units zi_baitIntake.units

Lag 0           1.000000000                 NaN

Lag 9000        0.011209318                 NaN

Lag 45000      -0.013619054                 NaN

Lag 90000       0.030874665                 NaN

Lag 450000      0.002752061                 NaN





However, when I compared the plots to the plots from model 6.0 where I used
(NITT-BURN)/THIN = 1500, the plots of model 6.0 show better mixing. Do you
think that one of these two models would be more suitable than the other to
continue working with?

On a different note: the prior question at the beginning of the message
might not be appropriate for the list as I realize it is a rather basic
question. I tried to find some more resources online to get a better
understanding about choosing and defining appropriate priors in the future.
I found some webpages and forum entries but wanted to ask if you have
resources you would recommend in regards to better understanding priors and
prior choice?

Again, thank you very much for all the help! I appreciate it a lot.

Kind regards,
Vital
--
*Vital Heim *

*PhD student*
University of Basel, Switzerland

Bimini Biological Field Station Foundation
Bimini, Bahamas

+41 (0)79 732 05 57
vital.heim at gmail.com

	[[alternative HTML version deleted]]


From p|erre@dev|||emereu|| @end|ng |rom ephe@p@|@eu  Fri Jun 19 10:43:55 2020
From: p|erre@dev|||emereu|| @end|ng |rom ephe@p@|@eu (Pierre de Villemereuil)
Date: Fri, 19 Jun 2020 10:43:55 +0200
Subject: [R-sig-ME] 
 Poor mixing and autocorrelation of ZIP data in MCMCglmm
In-Reply-To: <CAJkG0X9O+HzZM2pG6B=85Etb8++z_xRQAhUVH-_C+TYKSM4PQw@mail.gmail.com>
References: <CAJkG0X9O+HzZM2pG6B=85Etb8++z_xRQAhUVH-_C+TYKSM4PQw@mail.gmail.com>
Message-ID: <1755325.TiFbkifcGI@flyosfixe>

Hi,

> I wanted to double-check if I understand the prior suggestion correctly.
> While I do understand the basics of the priors to some extent, I still have
> difficulties to look at a model and say which prior would be appropriate. I
> thought that the probability density distribution of the prior is flatter
> the smaller nu is, i.e. the smaller nu, the weaker the prior. By
> recommending to make the prior stronger, do you mean to increase nu? If so,
> I have nu=1000 in the G part of the prior formula, which I thought was a
> very strong prior already? Or did you mean to increase nu in the R-part of
> the prior specification?

I think the prior is OK as it is.

> I read somewhere that the ratio of (NITT-BURN)/THIN should be kept between
> 1000-2000. So, my question is, if as long as this ratio is between
> 1000-2000 is it ok to run the model for very long ? or is that trying too
> hard to fit the model?

That's not the best kind of advice in my opinion. The number of iterations is a bit meaningless without accounting for auto-correlation, which is why effective sample size was invented. Simply decide on a minimal target for effective sample size, depending on the kind of precision you would like for your estimates.

> I ran a model with NITT = 920000, BURN = 20000, THIN = 600, i.e.
> (NITT-BURN)/THIN = 1500. I attached the details below and added you the
> trace and density plots (I removed them for the list email as it would make
> the message too large). The ZI-related variance in the baitIntake is
> slightly larger again (12 compared to 9 in the previous model where I
> excluded some animals).  The effective sample sizes are larger than 200 for
> all ZI-related variances but still rather small for zi_baitIntake.id (412)
> and zi_baitIntake.event (303). The autocorrelation is also rather large for
> some of the ZI-related variances.

If you are still unsatisfied by the effective sample size, you simply need to run the model for longer.

> However, I also read some more publications this morning that used MCMCglmm
> and looked at their nitt, burn and thin parameters. A lot of them result in
> (NITT-BURN)/THIN = 1000. I therefore re-run my model with parameters that
> also had 1000 as a ratio.

Again, this is somewhat meaningless as it depends on your autocorrelation level, which in turns depends on both the model and the data. If anything, I believe you should save much more than 1000 iterations if you want to increase your effective sample size without having to run your MCMC for eons. The way you set up everything with this MULT variable might not be the best way forward.

> On a different note: the prior question at the beginning of the message
> might not be appropriate for the list as I realize it is a rather basic
> question. I tried to find some more resources online to get a better
> understanding about choosing and defining appropriate priors in the future.
> I found some webpages and forum entries but wanted to ask if you have
> resources you would recommend in regards to better understanding priors and
> prior choice?

There is this page from Andrew Gelman, but not much it the priors presented here apply to MCMCglmm:
https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations

Cheers,
Pierre


From y|h@|uc42 @end|ng |rom gm@||@com  Fri Jun 19 12:27:00 2020
From: y|h@|uc42 @end|ng |rom gm@||@com (Yi-Hsiu Chen)
Date: Fri, 19 Jun 2020 18:27:00 +0800
Subject: [R-sig-ME] Model selection for multivariate mixed models
Message-ID: <CCEC8DE6-4E2B-477D-A3E1-2D0F888370C7@gmail.com>

Dear List members,


I am trying to figure out what is sensible model/variable selection procedure for multivariate mixed model, especially with MCMCglmm. It seems to be a fundamental question but unfortunately I couldn?t find much discussion on this topic. I was wondering if I could have some help here.    

My major question is: is there a difference between model selection procedure for univariate mixed model and for multivariate mixed model?  

More specifically, I meant, when fitting a univariate mixed model, it has been suggested that (such as in Bolker et al. 2009 <https://www.sciencedirect.com/science/article/pii/S0169534709000196> and Zuur et al 2009 <https://www.springer.com/gp/book/9780387874579>  a 2-step model selection procedure should be performed to avoid biased estimates; that is, starting with a full model but varying random effect to determine an optimal random effect structure first, and then varying the fixed effects included with the optimal random effects to find the best fixed-effect structure. Both the optimal random- and fixed-effects structure and be determined by comparing AIC.

For multivariate mixed models, I was just wondering if the same model selection procedure (i.e. starting with a full model) should be followed? Or it is less of a concern for multivariate mixed models, especially with Bayesian-based MCMCglmm?  

Thank you very much in advance. 

Best wishes
Yi-Hsiu
	[[alternative HTML version deleted]]


From wonde@@en@y@|ew9 @end|ng |rom gm@||@com  Fri Jun 19 13:20:44 2020
From: wonde@@en@y@|ew9 @end|ng |rom gm@||@com (Wondessen Ayalew)
Date: Fri, 19 Jun 2020 19:20:44 +0800
Subject: [R-sig-ME] NCMCglmm technical help
Message-ID: <CAJc1x6Sv0MzW+h8xUO9ag-VpJDSCjda09oX0ffnsWsZ0hnnehQ@mail.gmail.com>

Dear Professor Jarrod Hadfield,
It is my absolute hope that you are doing well in this pandemic. Thank you
for taking your precious time to read my email. My name is Wondessen
Ayalew. In the meantime, I am M.Sc. student at Bahir Dar University,
Ethiopia, and trying to fit a bivariate animal model based on repeated
records using the MCMCglmm package of R. However, I have faced technical
error to ordering the pedigree and writing code for bivariate repeatability
model. So far, even though both MasterBayes and pedantics are removed from
CRAN, I tried to install the latest archive of both packages but both of
them were not integrated to my R v.3.6.1. Software.
If you have some time, please help me with how to fix the problem and where
to get the codes for bivariate repeatability animal models.
I know you have a very busy schedule, thank you in advance for your
professional assistance! I look forward to your positive response.
For your convenience, I have attached a pedigree, full data, and the error
messages below.

Sincerely,

Wondessen Ayalew
Animal Breeding and Genetics MSc Student

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: mergped.txt
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20200619/40684da9/attachment-0002.txt>

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: finadata.txt
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20200619/40684da9/attachment-0003.txt>

From D@v|d@Du||y @end|ng |rom q|mrbergho|er@edu@@u  Tue Jun 23 02:11:26 2020
From: D@v|d@Du||y @end|ng |rom q|mrbergho|er@edu@@u (David Duffy)
Date: Tue, 23 Jun 2020 00:11:26 +0000
Subject: [R-sig-ME] NCMCglmm technical help
In-Reply-To: <CAJc1x6Sv0MzW+h8xUO9ag-VpJDSCjda09oX0ffnsWsZ0hnnehQ@mail.gmail.com>
References: <CAJc1x6Sv0MzW+h8xUO9ag-VpJDSCjda09oX0ffnsWsZ0hnnehQ@mail.gmail.com>
Message-ID: <ce51415e82b4485dada34ec69cf8c0d8@qimrberghofer.edu.au>

In finadata.txt. you have ANIMID 3178 in twice, once as offspring of 1156  and 3104, and later as offspring of
1156  and 3065 (mergped as only the latter).

From w@||dm@w@@@10 @end|ng |rom gm@||@com  Tue Jun 23 02:57:13 2020
From: w@||dm@w@@@10 @end|ng |rom gm@||@com (Walid Crampton-Mawass)
Date: Mon, 22 Jun 2020 20:57:13 -0400
Subject: [R-sig-ME] NCMCglmm technical help
In-Reply-To: <CAJc1x6Sv0MzW+h8xUO9ag-VpJDSCjda09oX0ffnsWsZ0hnnehQ@mail.gmail.com>
References: <CAJc1x6Sv0MzW+h8xUO9ag-VpJDSCjda09oX0ffnsWsZ0hnnehQ@mail.gmail.com>
Message-ID: <CAJtCY7WpAut6=6gzcTzoFPq3n9oE0AgfUq-gk9Ceh2WQ10bqHA@mail.gmail.com>

Hey,

To solve the package installation problem, you should have R devtools
installed on your machine and this will allow you to install packages
archived by CRAN (there is a more detailed answer on how to achieve this on
the stack exchange forum if you look it up). I suggest to you as well to
install the archived package pedantics as well which has solutions for
fixing pedigrees.

Concerning the bivariate animal model you want to model, I recall there
being an example of both a bivariate model and one with repeated records in
the MCMCglmm tutorial pdf file for constructing animal models - it's
available online.

Good luck,

Walid

On Fri., Jun. 19, 2020, 10:10 a.m. Wondessen Ayalew, <
wondessenayalew9 at gmail.com> wrote:

> Dear Professor Jarrod Hadfield,
> It is my absolute hope that you are doing well in this pandemic. Thank you
> for taking your precious time to read my email. My name is Wondessen
> Ayalew. In the meantime, I am M.Sc. student at Bahir Dar University,
> Ethiopia, and trying to fit a bivariate animal model based on repeated
> records using the MCMCglmm package of R. However, I have faced technical
> error to ordering the pedigree and writing code for bivariate repeatability
> model. So far, even though both MasterBayes and pedantics are removed from
> CRAN, I tried to install the latest archive of both packages but both of
> them were not integrated to my R v.3.6.1. Software.
> If you have some time, please help me with how to fix the problem and where
> to get the codes for bivariate repeatability animal models.
> I know you have a very busy schedule, thank you in advance for your
> professional assistance! I look forward to your positive response.
> For your convenience, I have attached a pedigree, full data, and the error
> messages below.
>
> Sincerely,
>
> Wondessen Ayalew
> Animal Breeding and Genetics MSc Student
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From j@h@d||e|d @end|ng |rom ed@@c@uk  Tue Jun 23 07:06:31 2020
From: j@h@d||e|d @end|ng |rom ed@@c@uk (Jarrod)
Date: Tue, 23 Jun 2020 06:06:31 +0100
Subject: [R-sig-ME] NCMCglmm technical help
In-Reply-To: <CAJtCY7WpAut6=6gzcTzoFPq3n9oE0AgfUq-gk9Ceh2WQ10bqHA@mail.gmail.com>
References: <CAJc1x6Sv0MzW+h8xUO9ag-VpJDSCjda09oX0ffnsWsZ0hnnehQ@mail.gmail.com>
 <CAJtCY7WpAut6=6gzcTzoFPq3n9oE0AgfUq-gk9Ceh2WQ10bqHA@mail.gmail.com>
Message-ID: <8F40208A-7525-421B-AC74-5A51719333EB@ed.ac.uk>

Hi Walid,

I have replaced the deprecated C++ code in MasterBayes and hopefully MasterBayes and pedantics will be back on CRAN soon.

Cheers,

Jarrod



> On 23 Jun 2020, at 01:57, Walid Crampton-Mawass <walidmawass10 at gmail.com> wrote:
>
> Hey,
>
> To solve the package installation problem, you should have R devtools
> installed on your machine and this will allow you to install packages
> archived by CRAN (there is a more detailed answer on how to achieve this on
> the stack exchange forum if you look it up). I suggest to you as well to
> install the archived package pedantics as well which has solutions for
> fixing pedigrees.
>
> Concerning the bivariate animal model you want to model, I recall there
> being an example of both a bivariate model and one with repeated records in
> the MCMCglmm tutorial pdf file for constructing animal models - it's
> available online.
>
> Good luck,
>
> Walid
>
> On Fri., Jun. 19, 2020, 10:10 a.m. Wondessen Ayalew, <
> wondessenayalew9 at gmail.com> wrote:
>
>> Dear Professor Jarrod Hadfield,
>> It is my absolute hope that you are doing well in this pandemic. Thank you
>> for taking your precious time to read my email. My name is Wondessen
>> Ayalew. In the meantime, I am M.Sc. student at Bahir Dar University,
>> Ethiopia, and trying to fit a bivariate animal model based on repeated
>> records using the MCMCglmm package of R. However, I have faced technical
>> error to ordering the pedigree and writing code for bivariate repeatability
>> model. So far, even though both MasterBayes and pedantics are removed from
>> CRAN, I tried to install the latest archive of both packages but both of
>> them were not integrated to my R v.3.6.1. Software.
>> If you have some time, please help me with how to fix the problem and where
>> to get the codes for bivariate repeatability animal models.
>> I know you have a very busy schedule, thank you in advance for your
>> professional assistance! I look forward to your positive response.
>> For your convenience, I have attached a pedigree, full data, and the error
>> messages below.
>>
>> Sincerely,
>>
>> Wondessen Ayalew
>> Animal Breeding and Genetics MSc Student
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>       [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

The University of Edinburgh is a charitable body, registered in Scotland, with registration number SC005336.


From wonde@@en@y@|ew9 @end|ng |rom gm@||@com  Tue Jun 23 08:00:58 2020
From: wonde@@en@y@|ew9 @end|ng |rom gm@||@com (Wondessen Ayalew)
Date: Tue, 23 Jun 2020 14:00:58 +0800
Subject: [R-sig-ME] MCMCglmm technical help
Message-ID: <CAJc1x6RG+EX2aWciMM9CDQEThcdyHBjvLy8x2y0nDoRFXF=LiQ@mail.gmail.com>

Dear Professors,

I am so thankful for the time you took to help with my R packages
installation. I'm also very grateful for the replay and humbled by your
support in MCMCglmm technical support.
In the meantime, i use kinship2  package to solve the problem and I am very
much grateful for the code provided by professor Timoth?e Bonnet.

Still your decisions to load MasterBayes on CRAN is very helpful for a wide
range of researchers in the area of genetics.

Finally, thank you once again for all.

Regards,

Wondossen

On Tuesday, June 23, 2020, Jarrod <j.hadfield at ed.ac.uk> wrote:

> Hi Walid,
>
> I have replaced the deprecated C++ code in MasterBayes and hopefully
> MasterBayes and pedantics will be back on CRAN soon.
>
> Cheers,
>
> Jarrod
>
>
>
> > On 23 Jun 2020, at 01:57, Walid Crampton-Mawass <walidmawass10 at gmail.com>
> wrote:
> >
> > Hey,
> >
> > To solve the package installation problem, you should have R devtools
> > installed on your machine and this will allow you to install packages
> > archived by CRAN (there is a more detailed answer on how to achieve this
> on
> > the stack exchange forum if you look it up). I suggest to you as well to
> > install the archived package pedantics as well which has solutions for
> > fixing pedigrees.
> >
> > Concerning the bivariate animal model you want to model, I recall there
> > being an example of both a bivariate model and one with repeated records
> in
> > the MCMCglmm tutorial pdf file for constructing animal models - it's
> > available online.
> >
> > Good luck,
> >
> > Walid
> >
> > On Fri., Jun. 19, 2020, 10:10 a.m. Wondessen Ayalew, <
> > wondessenayalew9 at gmail.com> wrote:
> >
> >> Dear Professor Jarrod Hadfield,
> >> It is my absolute hope that you are doing well in this pandemic. Thank
> you
> >> for taking your precious time to read my email. My name is Wondessen
> >> Ayalew. In the meantime, I am M.Sc. student at Bahir Dar University,
> >> Ethiopia, and trying to fit a bivariate animal model based on repeated
> >> records using the MCMCglmm package of R. However, I have faced technical
> >> error to ordering the pedigree and writing code for bivariate
> repeatability
> >> model. So far, even though both MasterBayes and pedantics are removed
> from
> >> CRAN, I tried to install the latest archive of both packages but both of
> >> them were not integrated to my R v.3.6.1. Software.
> >> If you have some time, please help me with how to fix the problem and
> where
> >> to get the codes for bivariate repeatability animal models.
> >> I know you have a very busy schedule, thank you in advance for your
> >> professional assistance! I look forward to your positive response.
> >> For your convenience, I have attached a pedigree, full data, and the
> error
> >> messages below.
> >>
> >> Sincerely,
> >>
> >> Wondessen Ayalew
> >> Animal Breeding and Genetics MSc Student
> >> _______________________________________________
> >> R-sig-mixed-models at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> The University of Edinburgh is a charitable body, registered in Scotland,
> with registration number SC005336.
>

	[[alternative HTML version deleted]]


From w@||dm@w@@@10 @end|ng |rom gm@||@com  Tue Jun 23 17:14:19 2020
From: w@||dm@w@@@10 @end|ng |rom gm@||@com (Walid Crampton-Mawass)
Date: Tue, 23 Jun 2020 11:14:19 -0400
Subject: [R-sig-ME] NCMCglmm technical help
In-Reply-To: <8F40208A-7525-421B-AC74-5A51719333EB@ed.ac.uk>
References: <CAJc1x6Sv0MzW+h8xUO9ag-VpJDSCjda09oX0ffnsWsZ0hnnehQ@mail.gmail.com>
 <CAJtCY7WpAut6=6gzcTzoFPq3n9oE0AgfUq-gk9Ceh2WQ10bqHA@mail.gmail.com>
 <8F40208A-7525-421B-AC74-5A51719333EB@ed.ac.uk>
Message-ID: <CAJtCY7U7z6RSmMTiH+NHdN4xr2RHd2-pk2qRkFeC6_8u_NmVoA@mail.gmail.com>

Thank you Jarrod for the update, it's greatly appreciated!

Cheers,
-- 
Walid Mawass
Ph.D. student in Evolutionary Biology
Population Genetics Laboratory
University of Qu?bec at Trois-Rivi?res
3351, boul. des Forges, C.P. 500
Trois-Rivi?res (Qu?bec) G9A 5H7
Telephone: 819-376-5011 poste 3384


On Tue, Jun 23, 2020 at 1:06 AM Jarrod <j.hadfield at ed.ac.uk> wrote:

> Hi Walid,
>
> I have replaced the deprecated C++ code in MasterBayes and hopefully
> MasterBayes and pedantics will be back on CRAN soon.
>
> Cheers,
>
> Jarrod
>
>
>
> > On 23 Jun 2020, at 01:57, Walid Crampton-Mawass <walidmawass10 at gmail.com>
> wrote:
> >
> > Hey,
> >
> > To solve the package installation problem, you should have R devtools
> > installed on your machine and this will allow you to install packages
> > archived by CRAN (there is a more detailed answer on how to achieve this
> on
> > the stack exchange forum if you look it up). I suggest to you as well to
> > install the archived package pedantics as well which has solutions for
> > fixing pedigrees.
> >
> > Concerning the bivariate animal model you want to model, I recall there
> > being an example of both a bivariate model and one with repeated records
> in
> > the MCMCglmm tutorial pdf file for constructing animal models - it's
> > available online.
> >
> > Good luck,
> >
> > Walid
> >
> > On Fri., Jun. 19, 2020, 10:10 a.m. Wondessen Ayalew, <
> > wondessenayalew9 at gmail.com> wrote:
> >
> >> Dear Professor Jarrod Hadfield,
> >> It is my absolute hope that you are doing well in this pandemic. Thank
> you
> >> for taking your precious time to read my email. My name is Wondessen
> >> Ayalew. In the meantime, I am M.Sc. student at Bahir Dar University,
> >> Ethiopia, and trying to fit a bivariate animal model based on repeated
> >> records using the MCMCglmm package of R. However, I have faced technical
> >> error to ordering the pedigree and writing code for bivariate
> repeatability
> >> model. So far, even though both MasterBayes and pedantics are removed
> from
> >> CRAN, I tried to install the latest archive of both packages but both of
> >> them were not integrated to my R v.3.6.1. Software.
> >> If you have some time, please help me with how to fix the problem and
> where
> >> to get the codes for bivariate repeatability animal models.
> >> I know you have a very busy schedule, thank you in advance for your
> >> professional assistance! I look forward to your positive response.
> >> For your convenience, I have attached a pedigree, full data, and the
> error
> >> messages below.
> >>
> >> Sincerely,
> >>
> >> Wondessen Ayalew
> >> Animal Breeding and Genetics MSc Student
> >> _______________________________________________
> >> R-sig-mixed-models at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> The University of Edinburgh is a charitable body, registered in Scotland,
> with registration number SC005336.
>

	[[alternative HTML version deleted]]


From j@de@ @end|ng |rom he@|th@uc@d@edu  Tue Jun 23 20:31:04 2020
From: j@de@ @end|ng |rom he@|th@uc@d@edu (Ades, James)
Date: Tue, 23 Jun 2020 18:31:04 +0000
Subject: [R-sig-ME] 
 Modeling and Interpretation Question for Interaction in LMER Output.
In-Reply-To: <7790fe97d6884417a73823c8ea28f1ab@qimrberghofer.edu.au>
References: <BY5PR19MB38591F98849BCAC5079D34C3EA800@BY5PR19MB3859.namprd19.prod.outlook.com>,
 <7790fe97d6884417a73823c8ea28f1ab@qimrberghofer.edu.au>
Message-ID: <BY5PR19MB3859EAC6BD80FADF2D83095DEA940@BY5PR19MB3859.namprd19.prod.outlook.com>

Hi all,

I looked through several different nlme sources and this is what seemed to be the equivalent model using corCAR1(). I have two questions: is it legitimate to put timepoint (character, 4 "levels") as a fixed effect while keeping t4 (time, numeric, days from first testing point) as the random slope, so that I can have a more interpretable outcome to assess the effects of time per cohort with emmeans? 2) As you can see in the lmer() model, there are random effects of pid as well as an interaction between pid:task; however, what I've coded above seemed to be the equivalent in nlme. Is that accurate, or is there a different way of coding that interaction?

lme(level ~ task + timepoint * cohort,
    random = list(pid = ~t4, task = ~t4),
    correlation = corCAR1(),
    dat)


lmer(level ~ task + t4 * cohort + (t4|pid) + (t4|pid:task))

Thanks!

James


________________________________
From: David Duffy <David.Duffy at qimrberghofer.edu.au>
Sent: Thursday, June 11, 2020 7:53 PM
To: Ades, James <jades at health.ucsd.edu>; r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: Re: Modeling and Interpretation Question for Interaction in LMER Output.

> The modeling question regards my depiction of time. The amount of elapsed time
> between participants and testing points is not equal equal. Students in the same classroom
> will be measured at equal time points, but while one class might have 4 months
> in between testing, another class might have 7 months.

In lme, AIUI this would be comparing different correlation structures, as per the example for
corCAR1().

	[[alternative HTML version deleted]]


From m@@r@mez@nkh@n| @end|ng |rom gm@||@com  Wed Jun 24 14:30:34 2020
From: m@@r@mez@nkh@n| @end|ng |rom gm@||@com (Azra Ramezankhani)
Date: Wed, 24 Jun 2020 17:00:34 +0430
Subject: [R-sig-ME] Fwd: lme4
In-Reply-To: <92f56320-f6de-40fd-3b7b-be199044c9d0@mcmaster.ca>
References: <CAKiRC7j6Ccw+Bn9kY+DnnZPFn_hnmK5_wnRXmMOOkn_jRK6iVQ@mail.gmail.com>
 <92f56320-f6de-40fd-3b7b-be199044c9d0@mcmaster.ca>
Message-ID: <CAKiRC7g=phQngjx3k2LCcKX1T8cHss7Q+9JR5DYUww0atLSWYw@mail.gmail.com>

> Hi
>
>
> How can we calculate standard errors for variance components of random
> effects when fitting a mixed-effects model using the lme4 package?
>
>
> Azra Ramezankhani
>

	[[alternative HTML version deleted]]


From ph||||p@@|d@y @end|ng |rom mp|@n|  Wed Jun 24 17:00:04 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Wed, 24 Jun 2020 17:00:04 +0200
Subject: [R-sig-ME] Fwd: lme4
In-Reply-To: <CAKiRC7g=phQngjx3k2LCcKX1T8cHss7Q+9JR5DYUww0atLSWYw@mail.gmail.com>
References: <CAKiRC7j6Ccw+Bn9kY+DnnZPFn_hnmK5_wnRXmMOOkn_jRK6iVQ@mail.gmail.com>
 <92f56320-f6de-40fd-3b7b-be199044c9d0@mcmaster.ca>
 <CAKiRC7g=phQngjx3k2LCcKX1T8cHss7Q+9JR5DYUww0atLSWYw@mail.gmail.com>
Message-ID: <f789703a-557a-8c23-c6a5-95a0c2284216@mpi.nl>

This is generally not advisable because the sampling distribution of the
variances is highly skewed. This means that the standard error isn't a
particularly informative statistic.?

This has been discussed in various amounts of detail in various places,
but a good starting point (with examples and links) is this question on
CrossValidated: https://stats.stackexchange.com/q/161225/26743

For a concrete recommendation, I would use either profile or bootstrap
confidence intervals instead of standard errors. You can compute these
with confint(model, method="profile") or confint(model, method="boot").
For more information, see:
http://search.r-project.org/R/library/lme4/html/confint.merMod.html

Best,

Phillip


On 24/6/20 2:30 pm, Azra Ramezankhani wrote:
>> Hi
>>
>>
>> How can we calculate standard errors for variance components of random
>> effects when fitting a mixed-effects model using the lme4 package?
>>
>>
>> Azra Ramezankhani
>>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From bbo|ker @end|ng |rom gm@||@com  Wed Jun 24 18:58:54 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 24 Jun 2020 12:58:54 -0400
Subject: [R-sig-ME] Fwd: lme4
In-Reply-To: <f789703a-557a-8c23-c6a5-95a0c2284216@mpi.nl>
References: <CAKiRC7j6Ccw+Bn9kY+DnnZPFn_hnmK5_wnRXmMOOkn_jRK6iVQ@mail.gmail.com>
 <92f56320-f6de-40fd-3b7b-be199044c9d0@mcmaster.ca>
 <CAKiRC7g=phQngjx3k2LCcKX1T8cHss7Q+9JR5DYUww0atLSWYw@mail.gmail.com>
 <f789703a-557a-8c23-c6a5-95a0c2284216@mpi.nl>
Message-ID: <440a0667-2933-cfe0-e9e6-d390ba73befb@gmail.com>

 ?? If you really, really want these standard errors, I believe you can 
now get the from the merDeriv package (on CRAN). See ?merDeriv::vcov.lmerMod


I'd be very careful with this though!? It gives a standard


On 6/24/20 11:00 AM, Phillip Alday wrote:
> This is generally not advisable because the sampling distribution of the
> variances is highly skewed. This means that the standard error isn't a
> particularly informative statistic.
>
> This has been discussed in various amounts of detail in various places,
> but a good starting point (with examples and links) is this question on
> CrossValidated: https://stats.stackexchange.com/q/161225/26743
>
> For a concrete recommendation, I would use either profile or bootstrap
> confidence intervals instead of standard errors. You can compute these
> with confint(model, method="profile") or confint(model, method="boot").
> For more information, see:
> http://search.r-project.org/R/library/lme4/html/confint.merMod.html
>
> Best,
>
> Phillip
>
>
> On 24/6/20 2:30 pm, Azra Ramezankhani wrote:
>>> Hi
>>>
>>>
>>> How can we calculate standard errors for variance components of random
>>> effects when fitting a mixed-effects model using the lme4 package?
>>>
>>>
>>> Azra Ramezankhani
>>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From th|erry@onke||nx @end|ng |rom |nbo@be  Wed Jun 24 19:06:19 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Wed, 24 Jun 2020 19:06:19 +0200
Subject: [R-sig-ME] 
 Modeling and Interpretation Question for Interaction in LMER Output.
In-Reply-To: <BY5PR19MB3859EAC6BD80FADF2D83095DEA940@BY5PR19MB3859.namprd19.prod.outlook.com>
References: <BY5PR19MB38591F98849BCAC5079D34C3EA800@BY5PR19MB3859.namprd19.prod.outlook.com>
 <7790fe97d6884417a73823c8ea28f1ab@qimrberghofer.edu.au>
 <BY5PR19MB3859EAC6BD80FADF2D83095DEA940@BY5PR19MB3859.namprd19.prod.outlook.com>
Message-ID: <CAJuCY5xG9W=p_oTDUHvY7OvqSHHu-4CCMc_FSOxhLBbH+ycxxg@mail.gmail.com>

Dear James,

I wrote a blog post on using a variable both as fixed and random (
https://www.muscardinus.be/2017/08/fixed-and-random/). Your case isn't
handled in that blog post. But the same reasoning applies. Both categorical
doesn't make sense. One continuous and one categorical can make sense
(depends on the structure in the data). The blog post provides some clues
to determine it on your data.

Don't use explicit nested random effects, use implicit nesting: create a
new variable pid2 = interaction(pid, task). Then update the model to
(1|pid) + (1|pid2) The model will figure out the structure from the data.

Note that the lmer() model doesn't handle correlated residuals like lme()
does.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op di 23 jun. 2020 om 20:31 schreef Ades, James <jades at health.ucsd.edu>:

> Hi all,
>
> I looked through several different nlme sources and this is what seemed to
> be the equivalent model using corCAR1(). I have two questions: is it
> legitimate to put timepoint (character, 4 "levels") as a fixed effect while
> keeping t4 (time, numeric, days from first testing point) as the random
> slope, so that I can have a more interpretable outcome to assess the
> effects of time per cohort with emmeans? 2) As you can see in the lmer()
> model, there are random effects of pid as well as an interaction between
> pid:task; however, what I've coded above seemed to be the equivalent in
> nlme. Is that accurate, or is there a different way of coding that
> interaction?
>
> lme(level ~ task + timepoint * cohort,
>     random = list(pid = ~t4, task = ~t4),
>     correlation = corCAR1(),
>     dat)
>
>
> lmer(level ~ task + t4 * cohort + (t4|pid) + (t4|pid:task))
>
> Thanks!
>
> James
>
>
> ________________________________
> From: David Duffy <David.Duffy at qimrberghofer.edu.au>
> Sent: Thursday, June 11, 2020 7:53 PM
> To: Ades, James <jades at health.ucsd.edu>; r-sig-mixed-models at r-project.org
> <r-sig-mixed-models at r-project.org>
> Subject: Re: Modeling and Interpretation Question for Interaction in LMER
> Output.
>
> > The modeling question regards my depiction of time. The amount of
> elapsed time
> > between participants and testing points is not equal equal. Students in
> the same classroom
> > will be measured at equal time points, but while one class might have 4
> months
> > in between testing, another class might have 7 months.
>
> In lme, AIUI this would be comparing different correlation structures, as
> per the example for
> corCAR1().
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From c@nott|ngh@m @end|ng |rom @uck|@nd@@c@nz  Tue Jun 30 07:06:07 2020
From: c@nott|ngh@m @end|ng |rom @uck|@nd@@c@nz (Christopher Nottingham)
Date: Tue, 30 Jun 2020 05:06:07 +0000
Subject: [R-sig-ME] Incorrect output from nested model with mapped pars
Message-ID: <71c307e350b349c1b7f25ce79ac3704f@uxcn13-tdc-b.UoA.auckland.ac.nz>

I have a dataset with a variable labelled n_comm that is not relevant to some factor level combinations. I am fitting a nested model to this data and fixing betas representing the irrelevant factor combinations to 0 using map. As shown following, the model output from the summary table does not match what should be produced.

> map_names = list(beta = factor(c(1:6, NA, 8)))
> fit = glmmTMB(log(Err) ~ model + n_surv + species + n_comm:geostatistical + intensity,
+               data = Bhat_all.df,
+               start = list(beta = ifelse(is.na(map_names$beta), 0, 1)),
+               map = map_names)
> summary(fit)
Family: gaussian  ( identity )
Formula:          log(Err) ~ model + n_surv + species + n_comm:geostatistical +      intensity
Data: Bhat_all.df

     AIC      BIC   logLik deviance df.resid
 18340.8  18394.7  -9162.4  18324.8     6212


Dispersion estimate for gaussian family (sigma^2): 1.11

Conditional model:
                                    Estimate Std. Error z value Pr(>|z|)
(Intercept)                        9.329e+00  5.461e-02  170.82   <2e-16 ***
modelbiomass-dynamics             -4.082e+00  5.089e-02  -80.22   <2e-16 ***
modelsize-structured              -4.092e+00  5.056e-02  -80.93   <2e-16 ***
n_surv                            -8.895e-04  8.146e-05  -10.92   <2e-16 ***
species$\\mathit{S. aequilatera}$  5.596e-01  2.677e-02   20.90   <2e-16 ***
intensityFishing intensity: high   3.589e-01  2.681e-02   13.39   <2e-16 ***
n_comm:geostatisticalFALSE         0.000e+00  7.450e-05    0.00    1.000
n_comm:geostatisticalTRUE         -3.245e-04  5.461e-02   -0.01    0.995
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Warning message:
In cbind(Estimate = coefs, `Std. Error` = sqrt(diag(vcov))) :
  number of rows of result is not a multiple of vector length (arg 2)
> rbind(sqrt(diag(solve(fit$obj$he()))))
           [,1]       [,2]       [,3]         [,4]       [,5]       [,6]         [,7]       [,8]
[1,] 0.05458617 0.05086506 0.05053495 8.142355e-05 0.02675588 0.02679859 7.446289e-05 0.01792267
> fit$sdr
sdreport(.) result
           Estimate   Std. Error
beta   9.3291879616 5.461347e-02
beta  -4.0822725635 5.089050e-02
beta  -4.0920631052 5.056022e-02
beta  -0.0008895195 8.146427e-05
beta   0.5595933469 2.676926e-02
beta   0.3589326271 2.681199e-02
beta  -0.0003244617 7.450013e-05
betad  0.1082286532 1.793163e-02
Maximum gradient component: 0.001913387

The output below is wrong (there should be no std err, on a mapped value and the other values are incorrect.),
n_comm:geostatisticalTRUE         -3.245e-04  5.461e-02   -0.01    0.995

The dataset is attached as a rds for reproducibility.

From @dh@n333 @end|ng |rom gm@||@com  Tue Jun 30 22:08:12 2020
From: @dh@n333 @end|ng |rom gm@||@com (=?UTF-8?B?U2HDomQgSEFOQU5F?=)
Date: Tue, 30 Jun 2020 21:08:12 +0100
Subject: [R-sig-ME] PCA-GLMM
Message-ID: <CA+QAJT7k8dy=COdY03t8pazp8Zq4V_r4wx=dH=ixh3=8nfBv1A@mail.gmail.com>

Hi,
I have performed a PCA analysis which highlighted two axes PC1 and PC2. I
want to perform a GLMM analysis considering the PC1 axis as a dependent
variable (variable to explain) with Poisson or negative binomial family,
but it was not possible since the values of this axis are negative. In such
a case what should I do?
I also need to perform glmmPQL to consider spatial autocorrelation. LME
could be a solution (instead of GLMM)? If yes, how could I perform glmmPQL?
Thank you for your precious help.
Regards.

-- 
Sa?d Hanane, PhD
Service d'Ecologie, de Biodiversit? et de Conservation des Sols
Centre de Recherche Foresti?re
Chariae Omar Ibn Al Khattab, BP 763, Rabat-Agdal/Maroc.




[image: Mailtrack]
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
Sender
notified by
Mailtrack
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
30/06/20
? 21:07:24

	[[alternative HTML version deleted]]


