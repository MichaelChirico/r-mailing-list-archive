From srjafarzadeh at gmail.com  Sun Apr  1 08:04:10 2007
From: srjafarzadeh at gmail.com (Seyed Reza Jafarzadeh)
Date: Sat, 31 Mar 2007 23:04:10 -0700
Subject: [R-sig-ME] lmer, CHOLMOD warning: matrix not positive definite
Message-ID: <83217d00703312304x6de1382ejc0b87c802b585b5d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20070331/2c5cbb31/attachment.pl>

From srjafarzadeh at gmail.com  Sun Apr  1 11:12:10 2007
From: srjafarzadeh at gmail.com (Seyed Reza Jafarzadeh)
Date: Sun, 1 Apr 2007 02:12:10 -0700
Subject: [R-sig-ME] HPDinterval problem
Message-ID: <83217d00704010212y7fce1fc4u57d320b2dfc54273@mail.gmail.com>

Hi,

Can anyone tell me why I am not getting the correct intervals for
fixed effect terms for the following generalized linear mixed model
from HPDinterval:

> sessionInfo()
R version 2.4.1 (2006-12-18)
i386-pc-mingw32

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
States.1252;LC_MONETARY=English_United
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"
"methods"   "base"

other attached packages:
       coda        lme4      Matrix     lattice
   "0.10-7" "0.9975-13" "0.9975-11"   "0.14-17"

> summary(o[1:1392])
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
    0.0     0.0     1.0     2.3     3.0    30.0

> summary(pv1o[1:1392])
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  0.000   0.000   1.000   2.322   3.000  30.000

> summary(pv2o[1:1392])
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  0.000   0.000   1.000   2.315   3.000  30.000

> summary(pv1toa[1:1392])
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
   0.00    4.00    7.00   11.99   15.00  108.00

> summary(pv2toa[1:1392])
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
   0.00    4.00    7.00   11.94   15.00  108.00

> m1.16 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | prov) + (pv1toa | prov) + (pv1o | pm) + (pv1toa | pm), data = mydata[1:1392,], family = quasipoisson)

> m1.16
Generalized linear mixed model fit using Laplace
Formula: o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | prov) +
(pv1toa | prov) + (pv1o | pm) + (pv1toa | pm)
   Data: mydata[1:1392, ]
 Family: quasipoisson(log link)
  AIC  BIC logLik deviance
 2285 2390  -1123     2245
Random effects:
 Groups   Name        Variance   Std.Dev. Corr
 prov     (Intercept) 0.68110734 0.825292
          pv1o        0.01182079 0.108723 -0.927
 prov     (Intercept) 0.09896363 0.314585
          pv1toa      0.00029002 0.017030 -0.182
 pm       (Intercept) 0.05023998 0.224143
          pv1o        0.00234292 0.048404 -0.789
 pm       (Intercept) 0.01918719 0.138518
          pv1toa      0.00011984 0.010947 -0.086
 Residual             1.54376281 1.242483
number of obs: 1392, groups: prov, 24; prov, 24; pm, 12; pm, 12

Fixed effects:
             Estimate Std. Error t value
(Intercept) -0.372258   0.233326  -1.595
pv1o         0.151591   0.028778   5.268
pv2o         0.029524   0.007247   4.074
pv1toa       0.025669   0.006221   4.126
pv2toa       0.004344   0.003755   1.157
sesblf2      0.074507   0.186258   0.400
sesblf3     -0.037145   0.188021  -0.198
sesblf4      0.155999   0.187175   0.833

Correlation of Fixed Effects:
        (Intr) pv1o   pv2o   pv1toa pv2toa ssblf2 ssblf3
pv1o    -0.638
pv2o    -0.036 -0.099
pv1toa  -0.073 -0.050 -0.029
pv2toa  -0.043 -0.035 -0.156 -0.458
sesblf2 -0.411 -0.009  0.040  0.002  0.012
sesblf3 -0.412 -0.005  0.039 -0.002  0.037  0.516
sesblf4 -0.413 -0.006  0.044  0.003  0.028  0.513  0.514

> s1.16 <- mcmcsamp(m1.16, n = 100000)

> HPDinterval(s1.16)
                           lower        upper
(Intercept)         -0.372258256 -0.372258256
pv1o                 0.151590854  0.151590854
pv2o                 0.029524315  0.029524315
pv1toa               0.025668727  0.025668727
pv2toa               0.004343653  0.004343653
sesblf2              0.074507427  0.074507427
sesblf3             -0.037144631 -0.037144631
sesblf4              0.155998825  0.155998825
log(prov.(In))      -1.547675380 -0.345723770
log(prov.pv1o)      -5.610048117 -4.407086692
atanh(prv.(I).pv1)  -2.509960360 -1.663905782
log(prov.(In))      -4.030294678 -2.823797787
log(prov.pv1t)      -9.370781684 -8.165302813
atanh(prv.(I).pv1)  -1.146944941 -0.289800204
log(pm.(In))        -4.420270387 -2.597929912
log(pm.pv1o)        -7.227500164 -5.401277510
atanh(pm.(I).pv1)   -2.172644329 -0.886969199
log(pm.(In))        -6.071675906 -4.252728431
log(pm.pv1t)       -10.230334351 -8.403082501
atanh(pm.(I).pv1)   -0.810182999  0.503799332
attr(,"Probability")
[1] 0.95




Thanks,
Reza



From jkrobert at bcm.tmc.edu  Mon Apr  2 16:52:56 2007
From: jkrobert at bcm.tmc.edu (Roberts, J. Kyle)
Date: Mon, 2 Apr 2007 09:52:56 -0500
Subject: [R-sig-ME] MCMC Paper
Message-ID: <3FC0430478C30B4A9AF0AFF7863418F130F0CD@BCMEVS6.ad.bcm.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20070402/e19b3587/attachment.pl>

From Wayne.Hallstrom at WorleyParsons.com  Mon Apr  2 21:23:20 2007
From: Wayne.Hallstrom at WorleyParsons.com (Hallstrom, Wayne (Calgary))
Date: Mon, 2 Apr 2007 13:23:20 -0600
Subject: [R-sig-ME] (no subject)
In-Reply-To: <mailman.11.1175421606.20361.r-sig-mixed-models@r-project.org>
Message-ID: <C672AABF34CB964A95F5E12A80134996292C0A@cacalwpexm01.WorleyParsons.com>

 
Hi,
I am wondering what the difference between fixed effect estimates by the
fixef() and mcmcsamp() routines? I noticed that these two seem to be
aimed at determining that same fixed effect coefficients, but that the
resutls differ. Is one prefereable to the other? I have been assuming
tha the mcmcsamp() routie ints the one to go with based on reading
previous discussions about it and lmer.

Thnak you,
Wayne Hallstrom
*** WORLEYPARSONS GROUP NOTICE ***
"This email is confidential.  If you are not the intended recipient, you must not disclose  or  use the  information  contained in it.  If you have received this email in error,  please notify us immediately by return email and delete the email and any attachments. Any personal views or opinions expressed by the writer may not necessarily reflect the views or opinions of any company in the WorleyParsons Group of Companies."



From bates at stat.wisc.edu  Mon Apr  2 22:17:15 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 2 Apr 2007 15:17:15 -0500
Subject: [R-sig-ME] (no subject)
In-Reply-To: <C672AABF34CB964A95F5E12A80134996292C0A@cacalwpexm01.WorleyParsons.com>
References: <mailman.11.1175421606.20361.r-sig-mixed-models@r-project.org>
	<C672AABF34CB964A95F5E12A80134996292C0A@cacalwpexm01.WorleyParsons.com>
Message-ID: <40e66e0b0704021317s147ab8d7oa41a29814b9cf092@mail.gmail.com>

The values returned by fixef(fm) are the maximum likelihood (ML) or
restricted maximum likelihood (REML) estimates when fm is a linear
mixed-effects model.  For a generalized linear mixed model the
fixef(fm) values are those values that maximize the Laplacian
approximation to the likelihood.  That is, they are the ML estimates
up to approximation of likelihood function.

The purpose of mcmcsamp is to provide a sample from the posterior
distribution of the model parameters.  These samples are typically
used to assess precision of parameter estimates or to assess the
significance of certain terms in the model.

The summary of the result returned by mcmcsamp does give raw means of
the sample from the posterior distribution but I wouldn't quote those
as parameter estimate.  I would use the ML or REML estimates of the
parameters as the "best guess" at the parameter values.

On 4/2/07, Hallstrom, Wayne (Calgary) <Wayne.Hallstrom at worleyparsons.com> wrote:
>
> Hi,
> I am wondering what the difference between fixed effect estimates by the
> fixef() and mcmcsamp() routines? I noticed that these two seem to be
> aimed at determining that same fixed effect coefficients, but that the
> resutls differ. Is one prefereable to the other? I have been assuming
> tha the mcmcsamp() routie ints the one to go with based on reading
> previous discussions about it and lmer.
>
> Thnak you,
> Wayne Hallstrom
> *** WORLEYPARSONS GROUP NOTICE ***
> "This email is confidential.  If you are not the intended recipient, you must not disclose  or  use the  information  contained in it.  If you have received this email in error,  please notify us immediately by return email and delete the email and any attachments. Any personal views or opinions expressed by the writer may not necessarily reflect the views or opinions of any company in the WorleyParsons Group of Companies."
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From srjafarzadeh at gmail.com  Wed Apr  4 00:11:49 2007
From: srjafarzadeh at gmail.com (Seyed Reza Jafarzadeh)
Date: Tue, 3 Apr 2007 15:11:49 -0700
Subject: [R-sig-ME] [R] lmer,
	CHOLMOD warning: matrix not positive definite
In-Reply-To: <40e66e0b0704031003i3c519afbw42f71ffb7b115065@mail.gmail.com>
References: <83217d00703312304x6de1382ejc0b87c802b585b5d@mail.gmail.com>
	<83217d00704021729t5252db68oa59194788899e4fc@mail.gmail.com>
	<40e66e0b0704031003i3c519afbw42f71ffb7b115065@mail.gmail.com>
Message-ID: <83217d00704031511s4629cbbjd66a0ed6873e07c1@mail.gmail.com>

Dear Douglas Bates,

Thanks for your attention. Please see the warnings with additional
control argument for the models that failed.


> m1.2 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | prov) + (1 | pm), data = mydata[1:1392,], family = quasipoisson, control = list(msVerbose = 1))
relative tolerance set to 3.97418414349626e-06
  0      2516.24: -0.0343767 0.0799244 0.0463578 0.0173111 -0.00385332
0.330649 0.0896930 0.185739 0.0459770 0.00241710  0.00000 0.0229885
CHOLMOD warning: matrix not positive definite
Error in objective(.par, ...) : Cholmod error `matrix not positive
definite' at file:../Supernodal/t_cholmod_super_numeric.c, line 614


> m1.4 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (1 | prov) + (pv1o | pm), data = mydata[1:1392,], family = quasipoisson, control = list(msVerbose = 1))
relative tolerance set to 3.82209043930558e-06
  0      2616.37: -0.0343767 0.0799244 0.0463578 0.0173111 -0.00385332
0.330649 0.0896930 0.185739 0.0459770 0.0229885 0.00120855  0.00000
CHOLMOD warning: matrix not positive definite
Error in objective(.par, ...) : Cholmod error `matrix not positive
definite' at file:../Supernodal/t_cholmod_super_numeric.c, line 614


> m1.6 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | prov) + (pv1o | pm), data = mydata[1:1392,], family = quasipoisson, control = list(msVerbose = 1))
relative tolerance set to 4.08445979897786e-06
  0      2448.30: -0.0343767 0.0799244 0.0463578 0.0173111 -0.00385332
0.330649 0.0896930 0.185739 0.0459770 0.00241710  0.00000 0.0229885
0.00120855  0.00000
CHOLMOD warning: matrix not positive definite
Error in objective(.par, ...) : Cholmod error `matrix not positive
definite' at file:../Supernodal/t_cholmod_super_numeric.c, line 614


> m1.23 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1toa | pm), data =  mydata[1:1392, ], family = quasipoisson, control = list(msVerbose = 1))
relative tolerance set to 3.35608419754339e-06
  0      2979.66: -0.0343767 0.0799244 0.0463578 0.0173111 -0.00385332
0.330649 0.0896930 0.185739 0.0229885 7.10587e-05  0.00000
CHOLMOD warning: matrix not positive definite
Error in objective(.par, ...) : Cholmod error `matrix not positive
definite' at file:../Supernodal/t_cholmod_super_numeric.c, line 614


Thanks,
Reza



On 4/3/07, Douglas Bates <bates at stat.wisc.edu> wrote:
> On 4/2/07, Seyed Reza Jafarzadeh <srjafarzadeh at gmail.com> wrote:
> > Hi,
> >
> > I am getting a warning message when I am fitting a generalized linear
> > mixed model (m1.2 below).
> >
> >
> > CHOLMOD warning: matrix not positive definite
> >  Error in objective(.par, ...) : Cholmod error `matrix not positive
> > definite' at file:../Supernodal/t_cholmod_super_numeric.c, line 614
> >
> >
> > Any idea?
> >
> > Thanks for your help,
> > Reza
> >
> >
> >
> >  > sessionInfo()
> > R version 2.4.1 (2006-12-18)
> >  i386-pc-mingw32
> >
> > locale:
> >  LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
> > States.1252;LC_MONETARY=English_United
> > States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
> >
> > attached base packages:
> >  [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"
> > "methods"   "base"
> >
> >  other attached packages:
> >        lme4      Matrix     lattice
> > "0.9975-13" "0.9975-11"   "0.14-17"
> >
> >
> >  > m1.1 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (1 |
> > prov) + (1 | pm), data = mydata[1:1392,], family = quasipoisson)
> >  > m1.1
> > Generalized linear mixed model fit using Laplace
> >  Formula: o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (1 | prov) + (1 | pm)
> >    Data: mydata[1:1392, ]
> >  Family: quasipoisson(log link)
> >   AIC  BIC logLik deviance
> >  2622 2675  -1301     2602
> > Random effects:
> >  Groups   Name        Variance Std.Dev.
> >  prov     (Intercept) 0.495021 0.70358
> >  pm       (Intercept) 0.047755 0.21853
> >  Residual             1.935867 1.39135
> > number of obs: 1392, groups: prov, 24; pm, 12
> >
> > Fixed effects:
> >              Estimate Std. Error t value
> > (Intercept) -0.207562   0.202715  -1.024
> >  pv1o         0.056575   0.006861   8.246
> > pv2o         0.023042   0.007216   3.193
> >  pv1toa       0.026497   0.003309   8.008
> > pv2toa      -0.001075   0.003467   -0.310
> > sesblf2      0.307805   0.195291   1.576
> >  sesblf3      0.067867   0.194097   0.350
> > sesblf4      0.232652   0.194075   1.199
> >
> > Correlation of Fixed Effects:
> >          (Intr) pv1o   pv2o   pv1toa pv2toa ssblf2 ssblf3
> > pv1o     0.035
> > pv2o    -0.014 -0.568
> >  pv1toa   0.017 -0.239  0.050
> > pv2toa  -0.104  0.035 -0.267 -0.772
> > sesblf2 -0.498 -0.038  0.037 -0.067  0.102
> >  sesblf3 -0.483 -0.042  0.021 -0.096  0.085  0.508
> > sesblf4 -0.483 -0.052  0.029 -0.099  0.083  0.508  0.518
> >
> >
> > > m1.2 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | prov) + (1 | pm), data = mydata[1:1392,], family = quasipoisson)
> > CHOLMOD warning: matrix not positive definite
> >  Error in objective(.par, ...) : Cholmod error `matrix not positive
> > definite' at file:../Supernodal/t_cholmod_super_numeric.c, line 614
> >
> >
> > > m1.3 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1toa | prov) + (1 | pm), data = mydata[1:1392,], family = quasipoisson)
> > > m1.3
> > Generalized linear mixed model fit using Laplace
> > Formula: o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1toa | prov)
> > +      (1 | pm)
> >    Data: mydata[1:1392, ]
> >  Family: quasipoisson(log link)
> >   AIC  BIC logLik deviance
> >  2459 2522  -1218     2435
> > Random effects:
> >  Groups   Name        Variance   Std.Dev. Corr
> >  prov     (Intercept) 0.88071235 0.938463
> >           pv1toa      0.00065327 0.025559 -0.712
> >  pm       (Intercept) 0.04588791 0.214215
> >  Residual             1.76873748 1.329939
> > number of obs: 1392, groups: prov, 24; pm, 12
> >
> > Fixed effects:
> >               Estimate Std. Error t value
> > (Intercept) -0.3854583  0.2384690  -1.616
> > pv1o         0.0545945  0.0069507   7.855
> > pv2o         0.0266912  0.0072733   3.670
> > pv1toa       0.0369315  0.0063083   5.854
> > pv2toa      -0.0008906  0.0034099  -0.261
> > sesblf2      0.3326815  0.1908401   1.743
> > sesblf3      0.1012759  0.1896883   0.534
> > sesblf4      0.1968002  0.1899092   1.036
> >
> > Correlation of Fixed Effects:
> >         (Intr) pv1o   pv2o   pv1toa pv2toa ssblf2 ssblf3
> > pv1o     0.016
> > pv2o    -0.023 -0.306
> > pv1toa  -0.485 -0.143 -0.014
> > pv2toa  -0.080 -0.055 -0.306 -0.367
> > sesblf2 -0.414 -0.037  0.033 -0.028  0.099
> > sesblf3 -0.402 -0.034  0.027 -0.046  0.080  0.509
> > sesblf4 -0.399 -0.039  0.031 -0.056  0.079  0.507  0.517
>
> Thanks for including the information that you did.  Would you be kind
> enough to rerun the model that failed with the additional argument
> control = list(msVerbose = 1).  That will help to show exactly what is
> happening to the parameter estimates in the model during the course of
> the iterations.
>



From srjafarzadeh at gmail.com  Wed Apr  4 07:00:55 2007
From: srjafarzadeh at gmail.com (Seyed Reza Jafarzadeh)
Date: Tue, 3 Apr 2007 22:00:55 -0700
Subject: [R-sig-ME] HPDinterval problem
In-Reply-To: <83217d00704010212y7fce1fc4u57d320b2dfc54273@mail.gmail.com>
References: <83217d00704010212y7fce1fc4u57d320b2dfc54273@mail.gmail.com>
Message-ID: <83217d00704032200t20e21a28kf667c4713cab8361@mail.gmail.com>

Hi,

I am providing more examples where HPDinterval failed. It seems to be
working OK for (generalized linear mixed) models without crossed
random-effects (m1.17, m1.18, m1.19, m1.20, m1.21, m1.22, and m1.24
below).

Thank you,
Reza


> m1.1 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (1 | prov) + (1 | pm), data = mydata[1:1392, ], family = quasipoisson)
> HPDinterval(mcmcsamp(m1.1, n = 10000))
                      lower        upper
(Intercept)    -0.207561922 -0.207561922
pv1o            0.056574609  0.056574609
pv2o            0.023042057  0.023042057
pv1toa          0.026497315  0.026497315
pv2toa         -0.001074887 -0.001074887
sesblf2         0.307805373  0.307805373
sesblf3         0.067866694  0.067866694
sesblf4         0.232652035  0.232652035
log(prov.(In)) -1.948540913 -0.815550437
log(pm.(In))   -4.609549269 -3.008113214
attr(,"Probability")
[1] 0.95


> m1.3 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1toa | prov) + (1 | pm), data = mydata[1:1392, ], family = quasipoisson)
> HPDinterval(mcmcsamp(m1.3, n = 10000))
                           lower         upper
(Intercept)        -0.3854582560 -0.3854582560
pv1o                0.0545945359  0.0545945359
pv2o                0.0266911717  0.0266911717
pv1toa              0.0369314516  0.0369314516
pv2toa             -0.0008906397 -0.0008906397
sesblf2             0.3326814534  0.3326814534
sesblf3             0.1012759194  0.1012759194
sesblf4             0.1968001587  0.1968001587
log(prov.(In))     -1.2423994216 -0.0463231047
log(prov.pv1t)     -8.5013756480 -7.3008649434
atanh(prv.(I).pv1) -1.3266358579 -0.4613822430
log(pm.(In))       -4.5813293741 -2.9416249086
attr(,"Probability")
[1] 0.95


> m1.5 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (1 | prov) + (pv1toa | pm), data = mydata[1:1392, ], family = quasipoisson)
> HPDinterval(mcmcsamp(m1.5, n = 10000))
                         lower        upper
(Intercept)       -0.298634101 -0.298634101
pv1o               0.056017516  0.056017516
pv2o               0.021658991  0.021658991
pv1toa             0.028086682  0.028086682
pv2toa             0.003447681  0.003447681
sesblf2            0.413727463  0.413727463
sesblf3            0.046766676  0.046766676
sesblf4            0.255977008  0.255977008
log(prov.(In))    -1.875689638 -0.751072995
log(pm.(In))      -3.556592560 -1.722182602
log(pm.pv1t)      -9.709527247 -7.885488338
atanh(pm.(I).pv1) -1.901663364 -0.616765080
attr(,"Probability")
[1] 0.95


> m1.7 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1toa | prov) + (pv1o | pm), data = mydata[1:1392, ], family = quasipoisson)
> HPDinterval(mcmcsamp(m1.7, n = 10000))
                          lower        upper
(Intercept)        -0.389923132 -0.389923132
pv1o                0.063098026  0.063098026
pv2o                0.034944761  0.034944761
pv1toa              0.032622126  0.032622126
pv2toa              0.003154919  0.003154919
sesblf2             0.300371141  0.300371141
sesblf3             0.020146759  0.020146759
sesblf4             0.187532056  0.187532056
log(prov.(In))     -1.322210629 -0.131769983
log(prov.pv1t)     -8.411977395 -7.219326685
atanh(prv.(I).pv1) -1.257375358 -0.396660253
log(pm.(In))       -3.671581481 -1.835388518
log(pm.pv1o)       -7.107693068 -5.275740794
atanh(pm.(I).pv1)  -1.679642476 -0.382000422
attr(,"Probability")
[1] 0.95


> m1.8 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | prov) + (pv1toa | pm), data = mydata[1:1392, ], family = quasipoisson)
> HPDinterval(mcmcsamp(m1.8, n = 10000))
                          lower        upper
(Intercept)        -0.457054707 -0.457054707
pv1o                0.156145534  0.156145534
pv2o                0.024773645  0.024773645
pv1toa              0.024579764  0.024579764
pv2toa              0.001907060  0.001907060
sesblf2             0.394565315  0.394565315
sesblf3             0.061645816  0.061645816
sesblf4             0.259691274  0.259691274
log(prov.(In))     -1.301168272 -0.105499439
log(prov.pv1o)     -5.255142692 -4.057937301
atanh(prv.(I).pv1) -1.851880731 -0.999139040
log(pm.(In))       -3.798743689 -1.968443546
log(pm.pv1t)       -9.788997900 -7.962938034
atanh(pm.(I).pv1)  -1.761670102 -0.480121774
attr(,"Probability")
[1] 0.95


> m1.9 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1toa | prov) + (pv1toa | pm), data = mydata[1:1392, ], family = quasipoisson)
> HPDinterval(mcmcsamp(m1.9, n = 10000))
                          lower        upper
(Intercept)        -0.485035838 -0.485035838
pv1o                0.053927806  0.053927806
pv2o                0.027072754  0.027072754
pv1toa              0.036558311  0.036558311
pv2toa              0.004437478  0.004437478
sesblf2             0.470407622  0.470407622
sesblf3             0.094079640  0.094079640
sesblf4             0.240104293  0.240104293
log(prov.(In))     -1.195701809  0.010070569
log(prov.pv1t)     -8.449706678 -7.242412741
atanh(prv.(I).pv1) -1.288497599 -0.436028040
log(pm.(In))       -3.343337299 -1.524187369
log(pm.pv1t)       -9.575747795 -7.757473396
atanh(pm.(I).pv1)  -2.020459923 -0.729784182
attr(,"Probability")
[1] 0.95


> m1.10 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | prov) + (pv1toa | prov) + (1 | pm), data = mydata[1:1392, ], family = quasipoisson)
> HPDinterval(mcmcsamp(m1.10, n = 10000))
                           lower         upper
(Intercept)        -0.4456071894 -0.4456071894
pv1o                0.1464835742  0.1464835742
pv2o                0.0229765752  0.0229765752
pv1toa              0.0278685330  0.0278685330
pv2toa             -0.0003038598 -0.0003038598
sesblf2             0.3443156778  0.3443156778
sesblf3             0.0944738799  0.0944738799
sesblf4             0.2254927178  0.2254927178
log(prov.(In))     -1.6491223578 -0.4377761632
log(prov.pv1o)     -5.6182267338 -4.4035977132
atanh(prv.(I).pv1) -2.5384179212 -1.6957391764
log(prov.(In))     -2.5659880481 -1.3799410576
log(prov.pv1t)     -9.0668805185 -7.8753819065
atanh(prv.(I).pv1) -2.0180897439 -1.1751174649
log(pm.(In))       -4.4612011753 -2.8135021093
attr(,"Probability")
[1] 0.95


> m1.11 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (1 | prov) + (pv1o | pm) + (pv1toa | pm), data = mydata[1:1392, ], family = quasipoisson)
> HPDinterval(mcmcsamp(m1.11, n = 10000))
                        lower       upper
(Intercept)       -0.29985554 -0.29985554
pv1o               0.07375569  0.07375569
pv2o               0.02568464  0.02568464
pv1toa             0.02426277  0.02426277
pv2toa             0.00551527  0.00551527
sesblf2            0.35060701  0.35060701
sesblf3            0.01707800  0.01707800
sesblf4            0.23239228  0.23239228
log(prov.(In))    -2.01160823 -0.87660101
log(pm.(In))      -4.16132557 -2.35116260
log(pm.pv1o)      -6.98272087 -5.17284124
atanh(pm.(I).pv1) -7.98465439 -6.71075618
log(pm.(In))      -3.74016652 -1.93693950
log(pm.pv1t)      -9.66383022 -7.88527146
atanh(pm.(I).pv1) -1.73988647 -0.46569668
attr(,"Probability")
[1] 0.95


> m1.12 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | prov) + (pv1toa | prov) + (pv1o | pm), data = mydata[1:1392, ], family = quasipoisson)
> HPDinterval(mcmcsamp(m1.12, n = 10000))
                          lower        upper
(Intercept)        -0.443061575 -0.443061575
pv1o                0.145784210  0.145784210
pv2o                0.032899568  0.032899568
pv1toa              0.026326962  0.026326962
pv2toa              0.002251301  0.002251301
sesblf2             0.321052133  0.321052133
sesblf3             0.016162887  0.016162887
sesblf4             0.213160215  0.213160215
log(prov.(In))     -1.558074949 -0.356582122
log(prov.pv1o)     -5.675655643 -4.488932586
atanh(prv.(I).pv1) -2.680508143 -1.837073359
log(prov.(In))     -2.879667257 -1.665153073
log(prov.pv1t)     -8.939603075 -7.728957475
atanh(prv.(I).pv1) -1.979096998 -1.125821675
log(pm.(In))       -3.715644215 -1.907578884
log(pm.pv1o)       -7.149593475 -5.332610229
atanh(pm.(I).pv1)  -1.662717526 -0.365192495
attr(,"Probability")
[1] 0.95


> m1.13 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | prov) + (pv1toa | prov) + (pv1toa | pm), data = mydata[1:1392, ], family = quasipoisson)
> HPDinterval(mcmcsamp(m1.13, n = 10000))
                          lower        upper
(Intercept)        -0.520671363 -0.520671363
pv1o                0.143483258  0.143483258
pv2o                0.023551563  0.023551563
pv1toa              0.028365257  0.028365257
pv2toa              0.004494341  0.004494341
sesblf2             0.400505707  0.400505707
sesblf3             0.072760610  0.072760610
sesblf4             0.252793971  0.252793971
log(prov.(In))     -1.664999620 -0.457571686
log(prov.pv1o)     -5.676457356 -4.482666134
atanh(prv.(I).pv1) -2.422760733 -1.569174953
log(prov.(In))     -2.466638158 -1.254008900
log(prov.pv1t)     -8.913571866 -7.705391788
atanh(prv.(I).pv1) -1.971407906 -1.105821462
log(pm.(In))       -3.708401541 -1.878728047
log(pm.pv1t)       -9.633734032 -7.816379065
atanh(pm.(I).pv1)  -1.760071642 -0.488676259
attr(,"Probability")
[1] 0.95


> m1.14 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | prov) + (pv1o | pm) + (pv1toa | pm), data = mydata[1:1392, ], family = quasipoisson)
> HPDinterval(mcmcsamp(m1.14, n = 10000))
                          lower        upper
(Intercept)        -0.443656150 -0.443656150
pv1o                0.162193691  0.162193691
pv2o                0.031649149  0.031649149
pv1toa              0.022429405  0.022429405
pv2toa              0.002443997  0.002443997
sesblf2             0.360466084  0.360466084
sesblf3             0.016536949  0.016536949
sesblf4             0.231307640  0.231307640
log(prov.(In))     -1.339206184 -0.149150781
log(prov.pv1o)     -5.335011824 -4.152221345
atanh(prv.(I).pv1) -1.943320599 -1.087748000
log(pm.(In))       -4.267623575 -2.442556980
log(pm.pv1o)       -7.190055027 -5.371607393
atanh(pm.(I).pv1)  -3.831335751 -2.554657428
log(pm.(In))       -4.045019561 -2.242185600
log(pm.pv1t)       -9.952372879 -8.157867980
atanh(pm.(I).pv1)  -1.606404138 -0.325092337
attr(,"Probability")
[1] 0.95


> m1.15 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1toa | prov) + (pv1o | pm) + (pv1toa | pm), data = mydata[1:1392, ], family = quasipoisson)
> HPDinterval(mcmcsamp(m1.15, n = 10000))
                          lower        upper
(Intercept)        -0.492628035 -0.492628035
pv1o                0.066932526  0.066932526
pv2o                0.032558697  0.032558697
pv1toa              0.031964223  0.031964223
pv2toa              0.006872102  0.006872102
sesblf2             0.462985660  0.462985660
sesblf3             0.059477673  0.059477673
sesblf4             0.228330298  0.228330298
log(prov.(In))     -1.284303557 -0.103539433
log(prov.pv1t)     -8.417909708 -7.204367435
atanh(prv.(I).pv1) -1.282230449 -0.436899596
log(pm.(In))       -3.964190389 -2.154068579
log(pm.pv1o)       -7.288337695 -5.491624377
atanh(pm.(I).pv1)  -2.373590093 -1.106568121
log(pm.(In))       -3.689622102 -1.867271754
log(pm.pv1t)       -9.611438322 -7.798522903
atanh(pm.(I).pv1)  -2.542602654 -1.259583953
attr(,"Probability")
[1] 0.95





> m1.17 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (1 | prov), data =  mydata[1:1392, ], family = quasipoisson)
> HPDinterval(mcmcsamp(m1.17, n = 10000))
                       lower        upper
(Intercept)    -0.4556673441  0.013126565
pv1o            0.0453480105  0.064682544
pv2o            0.0151423969  0.034835504
pv1toa          0.0183202799  0.026733287
pv2toa         -0.0020464788  0.006746995
sesblf2         0.2216241893  0.441465720
sesblf3        -0.0005681114  0.209583281
sesblf4         0.1468518151  0.356648966
log(prov.(In)) -1.9257817242 -0.683522096
attr(,"Probability")
[1] 0.95


> m1.18 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (1 | pm), data =  mydata[1:1392, ], family = quasipoisson)
> HPDinterval(mcmcsamp(m1.18, n = 10000))
                   lower        upper
(Intercept)  -0.30685638  0.228099238
pv1o          0.07232589  0.090089888
pv2o          0.03628791  0.055208902
pv1toa        0.01722480  0.026059233
pv2toa       -0.01216203 -0.002886872
sesblf2      -0.06651472  0.679455774
sesblf3      -0.32185743  0.425081143
sesblf4      -0.19286322  0.550831225
log(pm.(In)) -4.34513788 -2.000295046
attr(,"Probability")
[1] 0.95


> m1.19 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | prov), data =  mydata[1:1392, ], family = quasipoisson)
> HPDinterval(mcmcsamp(m1.19, n = 10000))
                          lower        upper
(Intercept)        -0.732466301 -0.095762021
pv1o                0.109101825  0.205227082
pv2o                0.014971677  0.035573998
pv1toa              0.015082087  0.024262828
pv2toa             -0.003380130  0.005867948
sesblf2             0.252479830  0.479557058
sesblf3             0.023607887  0.241040795
sesblf4             0.140224883  0.358214042
log(prov.(In))     -1.188516571  0.105022406
log(prov.pv1o)     -5.250761065 -3.673800387
atanh(prv.(I).pv1) -1.895520991 -0.894677411
attr(,"Probability")
[1] 0.95


> m1.20 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1toa | prov), data =  mydata[1:1392, ], family = quasipoisson)
> HPDinterval(mcmcsamp(m1.20, n = 10000))
                          lower        upper
(Intercept)        -0.654205909 -0.062846199
pv1o                0.044454141  0.063085006
pv2o                0.018996380  0.038600912
pv1toa              0.023692650  0.041228419
pv2toa             -0.001622876  0.006471938
sesblf2             0.253318503  0.460385395
sesblf3             0.041646544  0.232981172
sesblf4             0.114749796  0.309772745
log(prov.(In))     -1.179070095  0.115172146
log(prov.pv1t)     -8.502052140 -7.093668453
atanh(prv.(I).pv1) -1.406595407 -0.442011756
attr(,"Probability")
[1] 0.95


> m1.21 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | prov) + (pv1toa | prov), data =  mydata[1:1392, ], family = quasipoisson)
> HPDinterval(mcmcsamp(m1.21, n = 10000))
                          lower        upper
(Intercept)        -0.809795120 -0.092818083
pv1o                0.102320721  0.195798084
pv2o                0.013958996  0.035777516
pv1toa              0.015285309  0.032155176
pv2toa             -0.001270502  0.008386427
sesblf2             0.265843858  0.488668321
sesblf3             0.022096224  0.246423841
sesblf4             0.146323603  0.368723416
log(prov.(In))     -2.053426653 -0.003235248
log(prov.pv1o)     -5.494391785 -3.713515236
atanh(prv.(I).pv1) -6.542210732 -0.849163554
log(prov.(In))     -2.693832187 -0.335722676
log(prov.pv1t)     -9.432610814 -7.470030780
atanh(prv.(I).pv1) -4.334208695 -0.366701490
attr(,"Probability")
[1] 0.95


> m1.22 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | pm), data =  mydata[1:1392, ], family = quasipoisson)
> HPDinterval(mcmcsamp(m1.22, n = 10000))
                         lower        upper
(Intercept)       -0.464495795  0.251100466
pv1o               0.058394653  0.150951592
pv2o               0.044325579  0.064938826
pv1toa             0.012090092  0.022003334
pv2toa            -0.007826689  0.002510138
sesblf2           -0.198332573  0.732862207
sesblf3           -0.451325552  0.428932303
sesblf4           -0.282425435  0.600581823
log(pm.(In))      -3.340172245 -0.892699307
log(pm.pv1o)      -6.211385523 -4.186685594
atanh(pm.(I).pv1) -1.804149382 -0.051858193
attr(,"Probability")
[1] 0.95


> m1.24 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | pm) + (pv1toa | pm), data =  mydata[1:1392, ], family = quasipoisson)
> HPDinterval(mcmcsamp(m1.24, n = 10000))
                          lower        upper
(Intercept)        -0.618999654  0.371571883
pv1o                0.058341562  0.153168044
pv2o                0.041622796  0.062613888
pv1toa              0.006833982  0.025585021
pv2toa             -0.006074951  0.004975857
sesblf2            -0.703034251  1.379272724
sesblf3            -0.494385351  0.459530213
sesblf4            -0.296029582  0.674519473
log(pm.(In))       -3.918382813 -0.627437580
log(pm.pv1o)       -6.248238619 -4.142139977
atanh(pm.(I).pv1)  -7.304481224 -0.036363295
log(pm.(In))       -6.083394919 -0.025413685
log(pm.pv1t)      -10.436085207 -7.435829249
atanh(pm.(I).pv1)  -6.066684183  3.950585563
attr(,"Probability")
[1] 0.95






On 4/1/07, Seyed Reza Jafarzadeh <srjafarzadeh at gmail.com> wrote:
> Hi,
>
> Can anyone tell me why I am not getting the correct intervals for
> fixed effect terms for the following generalized linear mixed model
> from HPDinterval:
>
> > sessionInfo()
> R version 2.4.1 (2006-12-18)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
> States.1252;LC_MONETARY=English_United
> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
>
> attached base packages:
> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"
> "methods"   "base"
>
> other attached packages:
>        coda        lme4      Matrix     lattice
>    "0.10-7" "0.9975-13" "0.9975-11"   "0.14-17"
>
> > summary(o[1:1392])
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>     0.0     0.0     1.0     2.3     3.0    30.0
>
> > summary(pv1o[1:1392])
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>   0.000   0.000   1.000   2.322   3.000  30.000
>
> > summary(pv2o[1:1392])
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>   0.000   0.000   1.000   2.315   3.000  30.000
>
> > summary(pv1toa[1:1392])
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>    0.00    4.00    7.00   11.99   15.00  108.00
>
> > summary(pv2toa[1:1392])
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>    0.00    4.00    7.00   11.94   15.00  108.00
>
> > m1.16 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | prov) + (pv1toa | prov) + (pv1o | pm) + (pv1toa | pm), data = mydata[1:1392,], family = quasipoisson)
>
> > m1.16
> Generalized linear mixed model fit using Laplace
> Formula: o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | prov) +
> (pv1toa | prov) + (pv1o | pm) + (pv1toa | pm)
>    Data: mydata[1:1392, ]
>  Family: quasipoisson(log link)
>   AIC  BIC logLik deviance
>  2285 2390  -1123     2245
> Random effects:
>  Groups   Name        Variance   Std.Dev. Corr
>  prov     (Intercept) 0.68110734 0.825292
>           pv1o        0.01182079 0.108723 -0.927
>  prov     (Intercept) 0.09896363 0.314585
>           pv1toa      0.00029002 0.017030 -0.182
>  pm       (Intercept) 0.05023998 0.224143
>           pv1o        0.00234292 0.048404 -0.789
>  pm       (Intercept) 0.01918719 0.138518
>           pv1toa      0.00011984 0.010947 -0.086
>  Residual             1.54376281 1.242483
> number of obs: 1392, groups: prov, 24; prov, 24; pm, 12; pm, 12
>
> Fixed effects:
>              Estimate Std. Error t value
> (Intercept) -0.372258   0.233326  -1.595
> pv1o         0.151591   0.028778   5.268
> pv2o         0.029524   0.007247   4.074
> pv1toa       0.025669   0.006221   4.126
> pv2toa       0.004344   0.003755   1.157
> sesblf2      0.074507   0.186258   0.400
> sesblf3     -0.037145   0.188021  -0.198
> sesblf4      0.155999   0.187175   0.833
>
> Correlation of Fixed Effects:
>         (Intr) pv1o   pv2o   pv1toa pv2toa ssblf2 ssblf3
> pv1o    -0.638
> pv2o    -0.036 -0.099
> pv1toa  -0.073 -0.050 -0.029
> pv2toa  -0.043 -0.035 -0.156 -0.458
> sesblf2 -0.411 -0.009  0.040  0.002  0.012
> sesblf3 -0.412 -0.005  0.039 -0.002  0.037  0.516
> sesblf4 -0.413 -0.006  0.044  0.003  0.028  0.513  0.514
>
> > s1.16 <- mcmcsamp(m1.16, n = 100000)
>
> > HPDinterval(s1.16)
>                            lower        upper
> (Intercept)         -0.372258256 -0.372258256
> pv1o                 0.151590854  0.151590854
> pv2o                 0.029524315  0.029524315
> pv1toa               0.025668727  0.025668727
> pv2toa               0.004343653  0.004343653
> sesblf2              0.074507427  0.074507427
> sesblf3             -0.037144631 -0.037144631
> sesblf4              0.155998825  0.155998825
> log(prov.(In))      -1.547675380 -0.345723770
> log(prov.pv1o)      -5.610048117 -4.407086692
> atanh(prv.(I).pv1)  -2.509960360 -1.663905782
> log(prov.(In))      -4.030294678 -2.823797787
> log(prov.pv1t)      -9.370781684 -8.165302813
> atanh(prv.(I).pv1)  -1.146944941 -0.289800204
> log(pm.(In))        -4.420270387 -2.597929912
> log(pm.pv1o)        -7.227500164 -5.401277510
> atanh(pm.(I).pv1)   -2.172644329 -0.886969199
> log(pm.(In))        -6.071675906 -4.252728431
> log(pm.pv1t)       -10.230334351 -8.403082501
> atanh(pm.(I).pv1)   -0.810182999  0.503799332
> attr(,"Probability")
> [1] 0.95
>
>
>
>
> Thanks,
> Reza
>



From srjafarzadeh at gmail.com  Wed Apr  4 08:59:47 2007
From: srjafarzadeh at gmail.com (Seyed Reza Jafarzadeh)
Date: Tue, 3 Apr 2007 23:59:47 -0700
Subject: [R-sig-ME] HPDinterval problem
In-Reply-To: <83217d00704032200t20e21a28kf667c4713cab8361@mail.gmail.com>
References: <83217d00704010212y7fce1fc4u57d320b2dfc54273@mail.gmail.com>
	<83217d00704032200t20e21a28kf667c4713cab8361@mail.gmail.com>
Message-ID: <83217d00704032359q1bbc5b48r2ce4a773bf62fb65@mail.gmail.com>

Hi,

Sorry for multiple postings. Please consider the following reproducible example:


> grp1 <- rep(1:24, times = 24, each = 1)
> grp2 <- rep(1:12, times = 2, each = 24)
> set.seed(1)
> out <- as.integer(abs(rnorm(576))*10)
> x1 <- out [25:552]
> x2 <- out [49:576]
> mydf <- data.frame(cbind(out[1:528], x1, x2, grp1[49:576], grp2[49:576]))
> names(mydf) <- c("out", "x1", "x2", "grp1", "grp2")

> me1 <- lmer(out ~ x1 + x2 + (x1 | grp1) + (x2 | grp2), data = mydf, family = quasipoisson)

> me1
Generalized linear mixed model fit using Laplace
Formula: out ~ x1 + x2 + (x1 | grp1) + (x2 | grp2)
   Data: mydf
 Family: quasipoisson(log link)
  AIC  BIC logLik deviance
 2559 2598  -1271     2541
Random effects:
 Groups   Name        Variance  Std.Dev. Corr
 grp1     (Intercept) 0.3250162 0.570102
          x1          0.0021856 0.046750 -0.813
 grp2     (Intercept) 0.3183619 0.564236
          x2          0.0038276 0.061868 -0.940
 Residual             4.3279637 2.080376
number of obs: 528, groups: grp1, 24; grp2, 12

Fixed effects:
             Estimate Std. Error t value
(Intercept)  2.036473   0.212687   9.575
x1          -0.001159   0.011234  -0.103
x2          -0.006830   0.018860  -0.362

Correlation of Fixed Effects:
   (Intr) x1
x1 -0.487
x2 -0.751  0.007

> HPDinterval(mcmcsamp(me1, 20000))
                         lower        upper
(Intercept)        2.036472792  2.036472792
x1                -0.001158922 -0.001158922
x2                -0.006830165 -0.006830165
log(grp1.(In))    -3.254548062 -2.052213965
log(grp1.x1)      -8.351051505 -7.149742494
atanh(gr1.(I).x1) -1.630982865 -0.783008788
log(grp2.(In))    -3.312492769 -1.509030356
log(grp2.x2)      -7.739344906 -5.926156515
atanh(gr2.(I).x2) -2.564511232 -1.291401604
attr(,"Probability")
[1] 0.95


Thanks,
Reza



On 4/3/07, Seyed Reza Jafarzadeh <srjafarzadeh at gmail.com> wrote:
> Hi,
>
> I am providing more examples where HPDinterval failed. It seems to be
> working OK for (generalized linear mixed) models without crossed
> random-effects (m1.17, m1.18, m1.19, m1.20, m1.21, m1.22, and m1.24
> below).
>
> Thank you,
> Reza
>
>
> > m1.1 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (1 | prov) + (1 | pm), data = mydata[1:1392, ], family = quasipoisson)
> > HPDinterval(mcmcsamp(m1.1, n = 10000))
>                       lower        upper
> (Intercept)    -0.207561922 -0.207561922
> pv1o            0.056574609  0.056574609
> pv2o            0.023042057  0.023042057
> pv1toa          0.026497315  0.026497315
> pv2toa         -0.001074887 -0.001074887
> sesblf2         0.307805373  0.307805373
> sesblf3         0.067866694  0.067866694
> sesblf4         0.232652035  0.232652035
> log(prov.(In)) -1.948540913 -0.815550437
> log(pm.(In))   -4.609549269 -3.008113214
> attr(,"Probability")
> [1] 0.95
>
>
> > m1.3 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1toa | prov) + (1 | pm), data = mydata[1:1392, ], family = quasipoisson)
> > HPDinterval(mcmcsamp(m1.3, n = 10000))
>                            lower         upper
> (Intercept)        -0.3854582560 -0.3854582560
> pv1o                0.0545945359  0.0545945359
> pv2o                0.0266911717  0.0266911717
> pv1toa              0.0369314516  0.0369314516
> pv2toa             -0.0008906397 -0.0008906397
> sesblf2             0.3326814534  0.3326814534
> sesblf3             0.1012759194  0.1012759194
> sesblf4             0.1968001587  0.1968001587
> log(prov.(In))     -1.2423994216 -0.0463231047
> log(prov.pv1t)     -8.5013756480 -7.3008649434
> atanh(prv.(I).pv1) -1.3266358579 -0.4613822430
> log(pm.(In))       -4.5813293741 -2.9416249086
> attr(,"Probability")
> [1] 0.95
>
>
> > m1.5 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (1 | prov) + (pv1toa | pm), data = mydata[1:1392, ], family = quasipoisson)
> > HPDinterval(mcmcsamp(m1.5, n = 10000))
>                          lower        upper
> (Intercept)       -0.298634101 -0.298634101
> pv1o               0.056017516  0.056017516
> pv2o               0.021658991  0.021658991
> pv1toa             0.028086682  0.028086682
> pv2toa             0.003447681  0.003447681
> sesblf2            0.413727463  0.413727463
> sesblf3            0.046766676  0.046766676
> sesblf4            0.255977008  0.255977008
> log(prov.(In))    -1.875689638 -0.751072995
> log(pm.(In))      -3.556592560 -1.722182602
> log(pm.pv1t)      -9.709527247 -7.885488338
> atanh(pm.(I).pv1) -1.901663364 -0.616765080
> attr(,"Probability")
> [1] 0.95
>
>
> > m1.7 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1toa | prov) + (pv1o | pm), data = mydata[1:1392, ], family = quasipoisson)
> > HPDinterval(mcmcsamp(m1.7, n = 10000))
>                           lower        upper
> (Intercept)        -0.389923132 -0.389923132
> pv1o                0.063098026  0.063098026
> pv2o                0.034944761  0.034944761
> pv1toa              0.032622126  0.032622126
> pv2toa              0.003154919  0.003154919
> sesblf2             0.300371141  0.300371141
> sesblf3             0.020146759  0.020146759
> sesblf4             0.187532056  0.187532056
> log(prov.(In))     -1.322210629 -0.131769983
> log(prov.pv1t)     -8.411977395 -7.219326685
> atanh(prv.(I).pv1) -1.257375358 -0.396660253
> log(pm.(In))       -3.671581481 -1.835388518
> log(pm.pv1o)       -7.107693068 -5.275740794
> atanh(pm.(I).pv1)  -1.679642476 -0.382000422
> attr(,"Probability")
> [1] 0.95
>
>
> > m1.8 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | prov) + (pv1toa | pm), data = mydata[1:1392, ], family = quasipoisson)
> > HPDinterval(mcmcsamp(m1.8, n = 10000))
>                           lower        upper
> (Intercept)        -0.457054707 -0.457054707
> pv1o                0.156145534  0.156145534
> pv2o                0.024773645  0.024773645
> pv1toa              0.024579764  0.024579764
> pv2toa              0.001907060  0.001907060
> sesblf2             0.394565315  0.394565315
> sesblf3             0.061645816  0.061645816
> sesblf4             0.259691274  0.259691274
> log(prov.(In))     -1.301168272 -0.105499439
> log(prov.pv1o)     -5.255142692 -4.057937301
> atanh(prv.(I).pv1) -1.851880731 -0.999139040
> log(pm.(In))       -3.798743689 -1.968443546
> log(pm.pv1t)       -9.788997900 -7.962938034
> atanh(pm.(I).pv1)  -1.761670102 -0.480121774
> attr(,"Probability")
> [1] 0.95
>
>
> > m1.9 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1toa | prov) + (pv1toa | pm), data = mydata[1:1392, ], family = quasipoisson)
> > HPDinterval(mcmcsamp(m1.9, n = 10000))
>                           lower        upper
> (Intercept)        -0.485035838 -0.485035838
> pv1o                0.053927806  0.053927806
> pv2o                0.027072754  0.027072754
> pv1toa              0.036558311  0.036558311
> pv2toa              0.004437478  0.004437478
> sesblf2             0.470407622  0.470407622
> sesblf3             0.094079640  0.094079640
> sesblf4             0.240104293  0.240104293
> log(prov.(In))     -1.195701809  0.010070569
> log(prov.pv1t)     -8.449706678 -7.242412741
> atanh(prv.(I).pv1) -1.288497599 -0.436028040
> log(pm.(In))       -3.343337299 -1.524187369
> log(pm.pv1t)       -9.575747795 -7.757473396
> atanh(pm.(I).pv1)  -2.020459923 -0.729784182
> attr(,"Probability")
> [1] 0.95
>
>
> > m1.10 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | prov) + (pv1toa | prov) + (1 | pm), data = mydata[1:1392, ], family = quasipoisson)
> > HPDinterval(mcmcsamp(m1.10, n = 10000))
>                            lower         upper
> (Intercept)        -0.4456071894 -0.4456071894
> pv1o                0.1464835742  0.1464835742
> pv2o                0.0229765752  0.0229765752
> pv1toa              0.0278685330  0.0278685330
> pv2toa             -0.0003038598 -0.0003038598
> sesblf2             0.3443156778  0.3443156778
> sesblf3             0.0944738799  0.0944738799
> sesblf4             0.2254927178  0.2254927178
> log(prov.(In))     -1.6491223578 -0.4377761632
> log(prov.pv1o)     -5.6182267338 -4.4035977132
> atanh(prv.(I).pv1) -2.5384179212 -1.6957391764
> log(prov.(In))     -2.5659880481 -1.3799410576
> log(prov.pv1t)     -9.0668805185 -7.8753819065
> atanh(prv.(I).pv1) -2.0180897439 -1.1751174649
> log(pm.(In))       -4.4612011753 -2.8135021093
> attr(,"Probability")
> [1] 0.95
>
>
> > m1.11 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (1 | prov) + (pv1o | pm) + (pv1toa | pm), data = mydata[1:1392, ], family = quasipoisson)
> > HPDinterval(mcmcsamp(m1.11, n = 10000))
>                         lower       upper
> (Intercept)       -0.29985554 -0.29985554
> pv1o               0.07375569  0.07375569
> pv2o               0.02568464  0.02568464
> pv1toa             0.02426277  0.02426277
> pv2toa             0.00551527  0.00551527
> sesblf2            0.35060701  0.35060701
> sesblf3            0.01707800  0.01707800
> sesblf4            0.23239228  0.23239228
> log(prov.(In))    -2.01160823 -0.87660101
> log(pm.(In))      -4.16132557 -2.35116260
> log(pm.pv1o)      -6.98272087 -5.17284124
> atanh(pm.(I).pv1) -7.98465439 -6.71075618
> log(pm.(In))      -3.74016652 -1.93693950
> log(pm.pv1t)      -9.66383022 -7.88527146
> atanh(pm.(I).pv1) -1.73988647 -0.46569668
> attr(,"Probability")
> [1] 0.95
>
>
> > m1.12 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | prov) + (pv1toa | prov) + (pv1o | pm), data = mydata[1:1392, ], family = quasipoisson)
> > HPDinterval(mcmcsamp(m1.12, n = 10000))
>                           lower        upper
> (Intercept)        -0.443061575 -0.443061575
> pv1o                0.145784210  0.145784210
> pv2o                0.032899568  0.032899568
> pv1toa              0.026326962  0.026326962
> pv2toa              0.002251301  0.002251301
> sesblf2             0.321052133  0.321052133
> sesblf3             0.016162887  0.016162887
> sesblf4             0.213160215  0.213160215
> log(prov.(In))     -1.558074949 -0.356582122
> log(prov.pv1o)     -5.675655643 -4.488932586
> atanh(prv.(I).pv1) -2.680508143 -1.837073359
> log(prov.(In))     -2.879667257 -1.665153073
> log(prov.pv1t)     -8.939603075 -7.728957475
> atanh(prv.(I).pv1) -1.979096998 -1.125821675
> log(pm.(In))       -3.715644215 -1.907578884
> log(pm.pv1o)       -7.149593475 -5.332610229
> atanh(pm.(I).pv1)  -1.662717526 -0.365192495
> attr(,"Probability")
> [1] 0.95
>
>
> > m1.13 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | prov) + (pv1toa | prov) + (pv1toa | pm), data = mydata[1:1392, ], family = quasipoisson)
> > HPDinterval(mcmcsamp(m1.13, n = 10000))
>                           lower        upper
> (Intercept)        -0.520671363 -0.520671363
> pv1o                0.143483258  0.143483258
> pv2o                0.023551563  0.023551563
> pv1toa              0.028365257  0.028365257
> pv2toa              0.004494341  0.004494341
> sesblf2             0.400505707  0.400505707
> sesblf3             0.072760610  0.072760610
> sesblf4             0.252793971  0.252793971
> log(prov.(In))     -1.664999620 -0.457571686
> log(prov.pv1o)     -5.676457356 -4.482666134
> atanh(prv.(I).pv1) -2.422760733 -1.569174953
> log(prov.(In))     -2.466638158 -1.254008900
> log(prov.pv1t)     -8.913571866 -7.705391788
> atanh(prv.(I).pv1) -1.971407906 -1.105821462
> log(pm.(In))       -3.708401541 -1.878728047
> log(pm.pv1t)       -9.633734032 -7.816379065
> atanh(pm.(I).pv1)  -1.760071642 -0.488676259
> attr(,"Probability")
> [1] 0.95
>
>
> > m1.14 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | prov) + (pv1o | pm) + (pv1toa | pm), data = mydata[1:1392, ], family = quasipoisson)
> > HPDinterval(mcmcsamp(m1.14, n = 10000))
>                           lower        upper
> (Intercept)        -0.443656150 -0.443656150
> pv1o                0.162193691  0.162193691
> pv2o                0.031649149  0.031649149
> pv1toa              0.022429405  0.022429405
> pv2toa              0.002443997  0.002443997
> sesblf2             0.360466084  0.360466084
> sesblf3             0.016536949  0.016536949
> sesblf4             0.231307640  0.231307640
> log(prov.(In))     -1.339206184 -0.149150781
> log(prov.pv1o)     -5.335011824 -4.152221345
> atanh(prv.(I).pv1) -1.943320599 -1.087748000
> log(pm.(In))       -4.267623575 -2.442556980
> log(pm.pv1o)       -7.190055027 -5.371607393
> atanh(pm.(I).pv1)  -3.831335751 -2.554657428
> log(pm.(In))       -4.045019561 -2.242185600
> log(pm.pv1t)       -9.952372879 -8.157867980
> atanh(pm.(I).pv1)  -1.606404138 -0.325092337
> attr(,"Probability")
> [1] 0.95
>
>
> > m1.15 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1toa | prov) + (pv1o | pm) + (pv1toa | pm), data = mydata[1:1392, ], family = quasipoisson)
> > HPDinterval(mcmcsamp(m1.15, n = 10000))
>                           lower        upper
> (Intercept)        -0.492628035 -0.492628035
> pv1o                0.066932526  0.066932526
> pv2o                0.032558697  0.032558697
> pv1toa              0.031964223  0.031964223
> pv2toa              0.006872102  0.006872102
> sesblf2             0.462985660  0.462985660
> sesblf3             0.059477673  0.059477673
> sesblf4             0.228330298  0.228330298
> log(prov.(In))     -1.284303557 -0.103539433
> log(prov.pv1t)     -8.417909708 -7.204367435
> atanh(prv.(I).pv1) -1.282230449 -0.436899596
> log(pm.(In))       -3.964190389 -2.154068579
> log(pm.pv1o)       -7.288337695 -5.491624377
> atanh(pm.(I).pv1)  -2.373590093 -1.106568121
> log(pm.(In))       -3.689622102 -1.867271754
> log(pm.pv1t)       -9.611438322 -7.798522903
> atanh(pm.(I).pv1)  -2.542602654 -1.259583953
> attr(,"Probability")
> [1] 0.95
>
>
>
>
>
> > m1.17 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (1 | prov), data =  mydata[1:1392, ], family = quasipoisson)
> > HPDinterval(mcmcsamp(m1.17, n = 10000))
>                        lower        upper
> (Intercept)    -0.4556673441  0.013126565
> pv1o            0.0453480105  0.064682544
> pv2o            0.0151423969  0.034835504
> pv1toa          0.0183202799  0.026733287
> pv2toa         -0.0020464788  0.006746995
> sesblf2         0.2216241893  0.441465720
> sesblf3        -0.0005681114  0.209583281
> sesblf4         0.1468518151  0.356648966
> log(prov.(In)) -1.9257817242 -0.683522096
> attr(,"Probability")
> [1] 0.95
>
>
> > m1.18 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (1 | pm), data =  mydata[1:1392, ], family = quasipoisson)
> > HPDinterval(mcmcsamp(m1.18, n = 10000))
>                    lower        upper
> (Intercept)  -0.30685638  0.228099238
> pv1o          0.07232589  0.090089888
> pv2o          0.03628791  0.055208902
> pv1toa        0.01722480  0.026059233
> pv2toa       -0.01216203 -0.002886872
> sesblf2      -0.06651472  0.679455774
> sesblf3      -0.32185743  0.425081143
> sesblf4      -0.19286322  0.550831225
> log(pm.(In)) -4.34513788 -2.000295046
> attr(,"Probability")
> [1] 0.95
>
>
> > m1.19 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | prov), data =  mydata[1:1392, ], family = quasipoisson)
> > HPDinterval(mcmcsamp(m1.19, n = 10000))
>                           lower        upper
> (Intercept)        -0.732466301 -0.095762021
> pv1o                0.109101825  0.205227082
> pv2o                0.014971677  0.035573998
> pv1toa              0.015082087  0.024262828
> pv2toa             -0.003380130  0.005867948
> sesblf2             0.252479830  0.479557058
> sesblf3             0.023607887  0.241040795
> sesblf4             0.140224883  0.358214042
> log(prov.(In))     -1.188516571  0.105022406
> log(prov.pv1o)     -5.250761065 -3.673800387
> atanh(prv.(I).pv1) -1.895520991 -0.894677411
> attr(,"Probability")
> [1] 0.95
>
>
> > m1.20 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1toa | prov), data =  mydata[1:1392, ], family = quasipoisson)
> > HPDinterval(mcmcsamp(m1.20, n = 10000))
>                           lower        upper
> (Intercept)        -0.654205909 -0.062846199
> pv1o                0.044454141  0.063085006
> pv2o                0.018996380  0.038600912
> pv1toa              0.023692650  0.041228419
> pv2toa             -0.001622876  0.006471938
> sesblf2             0.253318503  0.460385395
> sesblf3             0.041646544  0.232981172
> sesblf4             0.114749796  0.309772745
> log(prov.(In))     -1.179070095  0.115172146
> log(prov.pv1t)     -8.502052140 -7.093668453
> atanh(prv.(I).pv1) -1.406595407 -0.442011756
> attr(,"Probability")
> [1] 0.95
>
>
> > m1.21 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | prov) + (pv1toa | prov), data =  mydata[1:1392, ], family = quasipoisson)
> > HPDinterval(mcmcsamp(m1.21, n = 10000))
>                           lower        upper
> (Intercept)        -0.809795120 -0.092818083
> pv1o                0.102320721  0.195798084
> pv2o                0.013958996  0.035777516
> pv1toa              0.015285309  0.032155176
> pv2toa             -0.001270502  0.008386427
> sesblf2             0.265843858  0.488668321
> sesblf3             0.022096224  0.246423841
> sesblf4             0.146323603  0.368723416
> log(prov.(In))     -2.053426653 -0.003235248
> log(prov.pv1o)     -5.494391785 -3.713515236
> atanh(prv.(I).pv1) -6.542210732 -0.849163554
> log(prov.(In))     -2.693832187 -0.335722676
> log(prov.pv1t)     -9.432610814 -7.470030780
> atanh(prv.(I).pv1) -4.334208695 -0.366701490
> attr(,"Probability")
> [1] 0.95
>
>
> > m1.22 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | pm), data =  mydata[1:1392, ], family = quasipoisson)
> > HPDinterval(mcmcsamp(m1.22, n = 10000))
>                          lower        upper
> (Intercept)       -0.464495795  0.251100466
> pv1o               0.058394653  0.150951592
> pv2o               0.044325579  0.064938826
> pv1toa             0.012090092  0.022003334
> pv2toa            -0.007826689  0.002510138
> sesblf2           -0.198332573  0.732862207
> sesblf3           -0.451325552  0.428932303
> sesblf4           -0.282425435  0.600581823
> log(pm.(In))      -3.340172245 -0.892699307
> log(pm.pv1o)      -6.211385523 -4.186685594
> atanh(pm.(I).pv1) -1.804149382 -0.051858193
> attr(,"Probability")
> [1] 0.95
>
>
> > m1.24 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | pm) + (pv1toa | pm), data =  mydata[1:1392, ], family = quasipoisson)
> > HPDinterval(mcmcsamp(m1.24, n = 10000))
>                           lower        upper
> (Intercept)        -0.618999654  0.371571883
> pv1o                0.058341562  0.153168044
> pv2o                0.041622796  0.062613888
> pv1toa              0.006833982  0.025585021
> pv2toa             -0.006074951  0.004975857
> sesblf2            -0.703034251  1.379272724
> sesblf3            -0.494385351  0.459530213
> sesblf4            -0.296029582  0.674519473
> log(pm.(In))       -3.918382813 -0.627437580
> log(pm.pv1o)       -6.248238619 -4.142139977
> atanh(pm.(I).pv1)  -7.304481224 -0.036363295
> log(pm.(In))       -6.083394919 -0.025413685
> log(pm.pv1t)      -10.436085207 -7.435829249
> atanh(pm.(I).pv1)  -6.066684183  3.950585563
> attr(,"Probability")
> [1] 0.95
>
>
>
>
>
>
> On 4/1/07, Seyed Reza Jafarzadeh <srjafarzadeh at gmail.com> wrote:
> > Hi,
> >
> > Can anyone tell me why I am not getting the correct intervals for
> > fixed effect terms for the following generalized linear mixed model
> > from HPDinterval:
> >
> > > sessionInfo()
> > R version 2.4.1 (2006-12-18)
> > i386-pc-mingw32
> >
> > locale:
> > LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
> > States.1252;LC_MONETARY=English_United
> > States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
> >
> > attached base packages:
> > [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"
> > "methods"   "base"
> >
> > other attached packages:
> >        coda        lme4      Matrix     lattice
> >    "0.10-7" "0.9975-13" "0.9975-11"   "0.14-17"
> >
> > > summary(o[1:1392])
> >    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
> >     0.0     0.0     1.0     2.3     3.0    30.0
> >
> > > summary(pv1o[1:1392])
> >    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
> >   0.000   0.000   1.000   2.322   3.000  30.000
> >
> > > summary(pv2o[1:1392])
> >    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
> >   0.000   0.000   1.000   2.315   3.000  30.000
> >
> > > summary(pv1toa[1:1392])
> >    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
> >    0.00    4.00    7.00   11.99   15.00  108.00
> >
> > > summary(pv2toa[1:1392])
> >    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
> >    0.00    4.00    7.00   11.94   15.00  108.00
> >
> > > m1.16 <- lmer(o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | prov) + (pv1toa | prov) + (pv1o | pm) + (pv1toa | pm), data = mydata[1:1392,], family = quasipoisson)
> >
> > > m1.16
> > Generalized linear mixed model fit using Laplace
> > Formula: o ~ pv1o + pv2o + pv1toa + pv2toa + sesblf + (pv1o | prov) +
> > (pv1toa | prov) + (pv1o | pm) + (pv1toa | pm)
> >    Data: mydata[1:1392, ]
> >  Family: quasipoisson(log link)
> >   AIC  BIC logLik deviance
> >  2285 2390  -1123     2245
> > Random effects:
> >  Groups   Name        Variance   Std.Dev. Corr
> >  prov     (Intercept) 0.68110734 0.825292
> >           pv1o        0.01182079 0.108723 -0.927
> >  prov     (Intercept) 0.09896363 0.314585
> >           pv1toa      0.00029002 0.017030 -0.182
> >  pm       (Intercept) 0.05023998 0.224143
> >           pv1o        0.00234292 0.048404 -0.789
> >  pm       (Intercept) 0.01918719 0.138518
> >           pv1toa      0.00011984 0.010947 -0.086
> >  Residual             1.54376281 1.242483
> > number of obs: 1392, groups: prov, 24; prov, 24; pm, 12; pm, 12
> >
> > Fixed effects:
> >              Estimate Std. Error t value
> > (Intercept) -0.372258   0.233326  -1.595
> > pv1o         0.151591   0.028778   5.268
> > pv2o         0.029524   0.007247   4.074
> > pv1toa       0.025669   0.006221   4.126
> > pv2toa       0.004344   0.003755   1.157
> > sesblf2      0.074507   0.186258   0.400
> > sesblf3     -0.037145   0.188021  -0.198
> > sesblf4      0.155999   0.187175   0.833
> >
> > Correlation of Fixed Effects:
> >         (Intr) pv1o   pv2o   pv1toa pv2toa ssblf2 ssblf3
> > pv1o    -0.638
> > pv2o    -0.036 -0.099
> > pv1toa  -0.073 -0.050 -0.029
> > pv2toa  -0.043 -0.035 -0.156 -0.458
> > sesblf2 -0.411 -0.009  0.040  0.002  0.012
> > sesblf3 -0.412 -0.005  0.039 -0.002  0.037  0.516
> > sesblf4 -0.413 -0.006  0.044  0.003  0.028  0.513  0.514
> >
> > > s1.16 <- mcmcsamp(m1.16, n = 100000)
> >
> > > HPDinterval(s1.16)
> >                            lower        upper
> > (Intercept)         -0.372258256 -0.372258256
> > pv1o                 0.151590854  0.151590854
> > pv2o                 0.029524315  0.029524315
> > pv1toa               0.025668727  0.025668727
> > pv2toa               0.004343653  0.004343653
> > sesblf2              0.074507427  0.074507427
> > sesblf3             -0.037144631 -0.037144631
> > sesblf4              0.155998825  0.155998825
> > log(prov.(In))      -1.547675380 -0.345723770
> > log(prov.pv1o)      -5.610048117 -4.407086692
> > atanh(prv.(I).pv1)  -2.509960360 -1.663905782
> > log(prov.(In))      -4.030294678 -2.823797787
> > log(prov.pv1t)      -9.370781684 -8.165302813
> > atanh(prv.(I).pv1)  -1.146944941 -0.289800204
> > log(pm.(In))        -4.420270387 -2.597929912
> > log(pm.pv1o)        -7.227500164 -5.401277510
> > atanh(pm.(I).pv1)   -2.172644329 -0.886969199
> > log(pm.(In))        -6.071675906 -4.252728431
> > log(pm.pv1t)       -10.230334351 -8.403082501
> > atanh(pm.(I).pv1)   -0.810182999  0.503799332
> > attr(,"Probability")
> > [1] 0.95
> >
> >
> >
> >
> > Thanks,
> > Reza
> >
>



From jorge.gonzalez at psy.kuleuven.be  Wed Apr  4 09:04:58 2007
From: jorge.gonzalez at psy.kuleuven.be (=?windows-1252?Q?Jorge_Gonz=E1lez?=)
Date: Wed, 04 Apr 2007 09:04:58 +0200
Subject: [R-sig-ME] bVar slot of lmer objects and standard errors
Message-ID: <46134E1A.1000602@psy.kuleuven.be>

Dear all,

Sorry if this is not the correct list for this kind of questions.

In the well known example from lme4 library 

fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
 
if you consider that (s1^2)*fm1 at bVar$Subject is the posterior covariance
matrix where s1<-attr(VarCorr(fm1),"sc") (as indicated in the following post 
(http://tolstoy.newcastle.edu.au/R/help/05/12/17977.html), then, what
would attr(p.bs[[1]], "postVar") with p.bs <- ranef(fm1, postVar = TRUE)
be? According to help(ranef), postVar is an optional logical argument
indicating if the conditional variance covariance matrices, also called
the ?posterior variances?, of the random effects should be included.

You can check that the results are certainly not the same. Then, which
one is the correct posterior covariance matrix for the empirical bayes
estimates?

Thank you very much in advance.

Jorge

-- 
 _-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_
|Jorge Gonz?lez                                                       |
|Faculty of Psychology                                                |
|Research Group of Quantitative Psychology and Individual Differences |
|jorge.gonzalez at psy.kuleuven.be                                       |
|http://perswww.kuleuven.be/jorge_gonzalez                            |
|http://www.kuleuven.be/cv/u0045204e.htm                              |
|_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_|




Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From HDoran at air.org  Wed Apr  4 15:44:44 2007
From: HDoran at air.org (Doran, Harold)
Date: Wed, 4 Apr 2007 09:44:44 -0400
Subject: [R-sig-ME] bVar slot of lmer objects and standard errors
In-Reply-To: <46134E1A.1000602@psy.kuleuven.be>
Message-ID: <2323A6D37908A847A7C32F1E3662C80EB9FA2F@dc1ex01.air.org>

The proper way to do this is to use the extractor function

attr(ranef(fm1, postVar = TRUE)[[1]], "postVar") 

Reaching into the slots, as I've learned, can give unstable results. Now, the results are different, and assuming that the unscaled results coming out by reaching into the bvar slot are stable, it appears the difference is that the scale factor used in the extractor function is slightly larger than the scale factor obtained from your s1.

I'm scratching my head a bit still because the scale factor using the extractor function appears to be different for the intercept and slope.

Of course, this is assuming the unscaled estimates coming out of the bVar slot are correct, and this may be a totally false assumption. 

> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org 
> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf 
> Of Jorge Gonz?lez
> Sent: Wednesday, April 04, 2007 3:05 AM
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] bVar slot of lmer objects and standard errors
> 
> Dear all,
> 
> Sorry if this is not the correct list for this kind of questions.
> 
> In the well known example from lme4 library 
> 
> fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
>  
> if you consider that (s1^2)*fm1 at bVar$Subject is the posterior 
> covariance matrix where s1<-attr(VarCorr(fm1),"sc") (as 
> indicated in the following post 
> (http://tolstoy.newcastle.edu.au/R/help/05/12/17977.html), 
> then, what would attr(p.bs[[1]], "postVar") with p.bs <- 
> ranef(fm1, postVar = TRUE) be? According to help(ranef), 
> postVar is an optional logical argument indicating if the 
> conditional variance covariance matrices, also called the 
> "posterior variances", of the random effects should be included.
> 
> You can check that the results are certainly not the same. 
> Then, which one is the correct posterior covariance matrix 
> for the empirical bayes estimates?
> 
> Thank you very much in advance.
> 
> Jorge
> 
> --
>  _-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_
> |Jorge Gonz?lez                                               
>         |
> |Faculty of Psychology                                        
>         |
> |Research Group of Quantitative Psychology and Individual 
> Differences |
> |jorge.gonzalez at psy.kuleuven.be                               
>         |
> |http://perswww.kuleuven.be/jorge_gonzalez                    
>         |
> |http://www.kuleuven.be/cv/u0045204e.htm                      
>         |
> |_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_
> -_-_-_-_|
> 
> 
> 
> 
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Wed Apr 11 16:15:21 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 11 Apr 2007 09:15:21 -0500
Subject: [R-sig-ME] [R] negative variances
In-Reply-To: <BAY115-F3D2156174CEED193491DCC15F0@phx.gbl>
References: <BAY115-F3D2156174CEED193491DCC15F0@phx.gbl>
Message-ID: <40e66e0b0704110715t518a3ea0r4b6541732fc1c6f2@mail.gmail.com>

On 4/11/07, Tu Yu-Kang <yukangtu at hotmail.com> wrote:
> Dear R experts,
>
> I had a question which may not be directly relevant to R but I will be
> grateful if you can give me some advices.
>
> I ran a two-level multilevel model for data with repeated measurements over
> time, i.e. level-1 the repeated measures and level-2 subjects. I could not
> get convergence using lme(), so I tried MLwiN, which eventually showed the
> level-2 variances (random effects for the intercept and slope) were
> negative values. I know this is known as Heywood cases in the structural
> equation modeling literature, but the only discussion on this problem in
> the literature of multilevel models and random effects models I can find is
> in the book by Prescott and Brown.
>
> Any suggestion on how to solve this problem will be highly appreciated.

It is possible that the ML or REML estimates for a variance component
can be zero.  The algorithm used in lme doesn't perform well in this
situation which is one reason that the lmer and lmer2 functions in the
lme4 package were created.  Could you try fitting the model with those
or provide us with the data so we can check it out?

I recommend moving this discussion to the R-SIG-mixed-models mailing
list which I am copying on this reply.



From yukangtu at hotmail.com  Thu Apr 12 10:54:15 2007
From: yukangtu at hotmail.com (Tu Yu-Kang)
Date: Thu, 12 Apr 2007 08:54:15 +0000
Subject: [R-sig-ME] [R] negative variances
In-Reply-To: <40e66e0b0704110715t518a3ea0r4b6541732fc1c6f2@mail.gmail.com>
Message-ID: <BAY115-F36C15EAA87043D2A68257C15E0@phx.gbl>


Dear Prof Bates,

Many thanks for your email. I tried lmer() and received the following 
messages:

>  lm2<-lmer(ppd~month+(month|id))
Warning message:
Estimated variance-covariance for factor 'id' is singular
 in: `LMEoptimize<-`(`*tmp*`, value = list(maxIter = 200, tolerance = 
1.49011611938477e-08,  

I then tried lmer2():

>  lm3<-lmer2(ppd~month+(month|id))
> summary(lm3)
Linear mixed-effects model fit by REML 
  AIC  BIC logLik MLdeviance REMLdeviance
 1146 1166 -567.9       1126         1136
Random effects:
 Groups   Name        Variance  Std.Dev. Corr  
 id       (Intercept) 0.1566631 0.395807       
          month       0.0022331 0.047255 1.000 
 Residual             0.6391120 0.799445       
Number of obs: 420, groups: id, 140

Fixed effects:
            Estimate Std. Error t value
(Intercept)  6.43595    0.07017   91.72
month       -0.36619    0.01642  -22.30

Correlation of Fixed Effects:
      (Intr)
month -0.544


However, I am not sure about the results, because MLwiN showed both random 
effects were negative values (-0.196 and -0.023).

I start to notice this problems of negative variances when I am learning 
how to use structural equation modeling software to run multilevel models 
for longitudinal data. To my great surprise, it occurs quite frequently. In 
SEM, this problem sometimes may be overcome by estimating  a nonlinear 
model by freeing the factor loadings. For example, in this data, PPD 
(probing pocket depth) was measured three times at month 0, 3 and 6. I only 
fixed the first and last factor loadings to be 0 and 6 to get a non-linear 
relation, and I also allow the level-1 residuals to be different on each 
occasion. However, in some data, I failed to get a satifactory model no 
matter how I modified my models.

I looked for the discussion in several multilevel modeling textbooks but 
only found one short discussion in the book by Brown and Prescott. SEM 
literature usually suggest fixing the negative variances to 0. However, I 
wander whether this is the only way to get around this problem or the 
sensible way because if the random effects are fixed to 0 the model is no 
longer a random effects model.

With best regards,

Yu-Kang

>From: "Douglas Bates" <bates at stat.wisc.edu>
>To: "Tu Yu-Kang" <yukangtu at hotmail.com>
>CC: r-help at stat.math.ethz.ch, r-sig-mixed-models at r-project.org
>Subject: Re: [R] negative variances
>Date: Wed, 11 Apr 2007 09:15:21 -0500
>
>On 4/11/07, Tu Yu-Kang <yukangtu at hotmail.com> wrote:
>>Dear R experts,
>>
>>I had a question which may not be directly relevant to R but I will 
>>be
>>grateful if you can give me some advices.
>>
>>I ran a two-level multilevel model for data with repeated 
>>measurements over
>>time, i.e. level-1 the repeated measures and level-2 subjects. I 
>>could not
>>get convergence using lme(), so I tried MLwiN, which eventually 
>>showed the
>>level-2 variances (random effects for the intercept and slope) were
>>negative values. I know this is known as Heywood cases in the 
>>structural
>>equation modeling literature, but the only discussion on this 
>>problem in
>>the literature of multilevel models and random effects models I can 
>>find is
>>in the book by Prescott and Brown.
>>
>>Any suggestion on how to solve this problem will be highly 
>>appreciated.
>
>It is possible that the ML or REML estimates for a variance 
>component
>can be zero.  The algorithm used in lme doesn't perform well in this
>situation which is one reason that the lmer and lmer2 functions in 
>the
>lme4 package were created.  Could you try fitting the model with 
>those
>or provide us with the data so we can check it out?
>
>I recommend moving this discussion to the R-SIG-mixed-models mailing
>list which I am copying on this reply.



From john.maindonald at anu.edu.au  Thu Apr 12 12:17:20 2007
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Thu, 12 Apr 2007 20:17:20 +1000
Subject: [R-sig-ME] [R] negative variances
In-Reply-To: <BAY115-F36C15EAA87043D2A68257C15E0@phx.gbl>
References: <BAY115-F36C15EAA87043D2A68257C15E0@phx.gbl>
Message-ID: <367D3598-4377-4388-B1AF-72A2E285BA7A@anu.edu.au>

Negative variances, unless they can be explained as statistical error,
surely indicate that the model that is formulated expecting variances
to be positive is inappropriate.  My preference is to do what MLwiN
does, and allow the estimates to go negative, just so that the user is
alerted to problems with the model, or at least with its formulation.

The model that has negative "variance" components may from the
point of view of getting the variance-covariance structure correct be
OK, just formulated using parameters that, notwithstanding a fervent
wish to interpret them as variances, cannot be so interpreted.

I once heard, from people providing a data analysis service, of
receiving randomized block treatment comparison data where it
turned out that the blocks had been chosen, e.g. with their plots
at increasing distances from a stream.  A negative between block
component of "variance" was, given the design, to be expected.
I did myself once encounter a scientist who thought that blocks
should be chosen, as far as possible, so that each individual block
spanned as large a part of the variation as possible.

A problem with SEMs is that this stuff typically hides under the hood.
It is not obvious how the user might get to look under the hood
(at the layout of the blocks, maybe) and comment, "Ah, I might have
guessed as much."

Graphical models (the name is not as revealing as one might like
of the nature of these models) can be a huge advance because they
do try to pull apart what goes into SEMs, to look under the hood.
See the gR task view for R, that you can get to from a CRAN mirror.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.


On 12 Apr 2007, at 6:54 PM, Tu Yu-Kang wrote:

> Dear Prof Bates,
>
> Many thanks for your email. I tried lmer() and received the  
> following messages:
>
>>  lm2<-lmer(ppd~month+(month|id))
> Warning message:
> Estimated variance-covariance for factor 'id' is singular
> in: `LMEoptimize<-`(`*tmp*`, value = list(maxIter = 200, tolerance  
> = 1.49011611938477e-08,
> I then tried lmer2():
>
>>  lm3<-lmer2(ppd~month+(month|id))
>> summary(lm3)
> Linear mixed-effects model fit by REML  AIC  BIC logLik MLdeviance  
> REMLdeviance
> 1146 1166 -567.9       1126         1136
> Random effects:
> Groups   Name        Variance  Std.Dev. Corr  id       (Intercept)  
> 0.1566631 0.395807                month       0.0022331 0.047255  
> 1.000 Residual             0.6391120 0.799445       Number of obs:  
> 420, groups: id, 140
>
> Fixed effects:
>            Estimate Std. Error t value
> (Intercept)  6.43595    0.07017   91.72
> month       -0.36619    0.01642  -22.30
>
> Correlation of Fixed Effects:
>      (Intr)
> month -0.544
>
>
> However, I am not sure about the results, because MLwiN showed both  
> random effects were negative values (-0.196 and -0.023).
>
> I start to notice this problems of negative variances when I am  
> learning how to use structural equation modeling software to run  
> multilevel models for longitudinal data. To my great surprise, it  
> occurs quite frequently. In SEM, this problem sometimes may be  
> overcome by estimating  a nonlinear model by freeing the factor  
> loadings. For example, in this data, PPD (probing pocket depth) was  
> measured three times at month 0, 3 and 6. I only fixed the first  
> and last factor loadings to be 0 and 6 to get a non-linear  
> relation, and I also allow the level-1 residuals to be different on  
> each occasion. However, in some data, I failed to get a satifactory  
> model no matter how I modified my models.
>
> I looked for the discussion in several multilevel modeling  
> textbooks but only found one short discussion in the book by Brown  
> and Prescott. SEM literature usually suggest fixing the negative  
> variances to 0. However, I wander whether this is the only way to  
> get around this problem or the sensible way because if the random  
> effects are fixed to 0 the model is no longer a random effects model.
>
> With best regards,
>
> Yu-Kang
>
>> From: "Douglas Bates" <bates at stat.wisc.edu>
>> To: "Tu Yu-Kang" <yukangtu at hotmail.com>
>> CC: r-help at stat.math.ethz.ch, r-sig-mixed-models at r-project.org
>> Subject: Re: [R] negative variances
>> Date: Wed, 11 Apr 2007 09:15:21 -0500
>>
>> On 4/11/07, Tu Yu-Kang <yukangtu at hotmail.com> wrote:
>>> Dear R experts,
>>>
>>> I had a question which may not be directly relevant to R but I  
>>> will be
>>> grateful if you can give me some advices.
>>>
>>> I ran a two-level multilevel model for data with repeated  
>>> measurements over
>>> time, i.e. level-1 the repeated measures and level-2 subjects. I  
>>> could not
>>> get convergence using lme(), so I tried MLwiN, which eventually  
>>> showed the
>>> level-2 variances (random effects for the intercept and slope) were
>>> negative values. I know this is known as Heywood cases in the  
>>> structural
>>> equation modeling literature, but the only discussion on this  
>>> problem in
>>> the literature of multilevel models and random effects models I  
>>> can find is
>>> in the book by Prescott and Brown.
>>>
>>> Any suggestion on how to solve this problem will be highly  
>>> appreciated.
>>
>> It is possible that the ML or REML estimates for a variance component
>> can be zero.  The algorithm used in lme doesn't perform well in this
>> situation which is one reason that the lmer and lmer2 functions in  
>> the
>> lme4 package were created.  Could you try fitting the model with  
>> those
>> or provide us with the data so we can check it out?
>>
>> I recommend moving this discussion to the R-SIG-mixed-models mailing
>> list which I am copying on this reply.
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From bates at stat.wisc.edu  Thu Apr 12 15:07:57 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 12 Apr 2007 08:07:57 -0500
Subject: [R-sig-ME] [R] negative variances
In-Reply-To: <BAY115-F36C15EAA87043D2A68257C15E0@phx.gbl>
References: <40e66e0b0704110715t518a3ea0r4b6541732fc1c6f2@mail.gmail.com>
	<BAY115-F36C15EAA87043D2A68257C15E0@phx.gbl>
Message-ID: <40e66e0b0704120607x26ffb9c0p7fe008457c327990@mail.gmail.com>

On 4/12/07, Tu Yu-Kang <yukangtu at hotmail.com> wrote:
>
> Dear Prof Bates,
>
> Many thanks for your email. I tried lmer() and received the following
> messages:
>
> >  lm2<-lmer(ppd~month+(month|id))
> Warning message:
> Estimated variance-covariance for factor 'id' is singular
>  in: `LMEoptimize<-`(`*tmp*`, value = list(maxIter = 200, tolerance =
> 1.49011611938477e-08,
>
> I then tried lmer2():
>
> >  lm3<-lmer2(ppd~month+(month|id))
> > summary(lm3)
> Linear mixed-effects model fit by REML
>   AIC  BIC logLik MLdeviance REMLdeviance
>  1146 1166 -567.9       1126         1136
> Random effects:
>  Groups   Name        Variance  Std.Dev. Corr
>  id       (Intercept) 0.1566631 0.395807
>           month       0.0022331 0.047255 1.000
>  Residual             0.6391120 0.799445
> Number of obs: 420, groups: id, 140
>
> Fixed effects:
>             Estimate Std. Error t value
> (Intercept)  6.43595    0.07017   91.72
> month       -0.36619    0.01642  -22.30
>
> Correlation of Fixed Effects:
>       (Intr)
> month -0.544

I'm a bit pressed for time right now so this will be brief.  I suggest
that you add the optional argument

control = list(msVerbose = TRUE)

to the calls to lmer and to lmer2 and look at the progress of the
iterations and also at the value of the deviance.  I have seen
situations where lmer2 converges to a fit with a significantly smaller
deviance than can lmer because it progresses through the near-singular
region of the parameter space.

What was the deviance (or the log-likelihood) from the MLWin fit that
gave negative variance components?

Did you plot the data as ppd versus month by id using xyplot from the
lattice package?  That should always be the first step in the analysis
of longitudinal data.

> However, I am not sure about the results, because MLwiN showed both random
> effects were negative values (-0.196 and -0.023).
>
> I start to notice this problems of negative variances when I am learning
> how to use structural equation modeling software to run multilevel models
> for longitudinal data. To my great surprise, it occurs quite frequently. In
> SEM, this problem sometimes may be overcome by estimating  a nonlinear
> model by freeing the factor loadings. For example, in this data, PPD
> (probing pocket depth) was measured three times at month 0, 3 and 6. I only
> fixed the first and last factor loadings to be 0 and 6 to get a non-linear
> relation, and I also allow the level-1 residuals to be different on each
> occasion. However, in some data, I failed to get a satifactory model no
> matter how I modified my models.
>
> I looked for the discussion in several multilevel modeling textbooks but
> only found one short discussion in the book by Brown and Prescott. SEM
> literature usually suggest fixing the negative variances to 0. However, I
> wander whether this is the only way to get around this problem or the
> sensible way because if the random effects are fixed to 0 the model is no
> longer a random effects model.
>
> With best regards,
>
> Yu-Kang
>
> >From: "Douglas Bates" <bates at stat.wisc.edu>
> >To: "Tu Yu-Kang" <yukangtu at hotmail.com>
> >CC: r-help at stat.math.ethz.ch, r-sig-mixed-models at r-project.org
> >Subject: Re: [R] negative variances
> >Date: Wed, 11 Apr 2007 09:15:21 -0500
> >
> >On 4/11/07, Tu Yu-Kang <yukangtu at hotmail.com> wrote:
> >>Dear R experts,
> >>
> >>I had a question which may not be directly relevant to R but I will
> >>be
> >>grateful if you can give me some advices.
> >>
> >>I ran a two-level multilevel model for data with repeated
> >>measurements over
> >>time, i.e. level-1 the repeated measures and level-2 subjects. I
> >>could not
> >>get convergence using lme(), so I tried MLwiN, which eventually
> >>showed the
> >>level-2 variances (random effects for the intercept and slope) were
> >>negative values. I know this is known as Heywood cases in the
> >>structural
> >>equation modeling literature, but the only discussion on this
> >>problem in
> >>the literature of multilevel models and random effects models I can
> >>find is
> >>in the book by Prescott and Brown.
> >>
> >>Any suggestion on how to solve this problem will be highly
> >>appreciated.
> >
> >It is possible that the ML or REML estimates for a variance
> >component
> >can be zero.  The algorithm used in lme doesn't perform well in this
> >situation which is one reason that the lmer and lmer2 functions in
> >the
> >lme4 package were created.  Could you try fitting the model with
> >those
> >or provide us with the data so we can check it out?
> >
> >I recommend moving this discussion to the R-SIG-mixed-models mailing
> >list which I am copying on this reply.
>
> _________________________________________________________________
> Windows Live Messenger ???????????????????????
> http://get.live.com/messenger/overview
>
>



From yukangtu at hotmail.com  Thu Apr 12 16:10:25 2007
From: yukangtu at hotmail.com (Tu Yu-Kang)
Date: Thu, 12 Apr 2007 14:10:25 +0000
Subject: [R-sig-ME] [R] negative variances
In-Reply-To: <40e66e0b0704120607x26ffb9c0p7fe008457c327990@mail.gmail.com>
Message-ID: <BAY115-F279895C35E63148657A5E6C15E0@phx.gbl>

Dear Prof Bates,

> lm2<-lmer(ppd~month+(month|id)) 
Warning message:
Estimated variance-covariance for factor 'id' is singular
 in: `LMEoptimize<-`(`*tmp*`, value = list(maxIter = 200, tolerance = 
1.49011611938477e-08,  
> lm2<-lmer(ppd~month+(month|id),control = list(msVerbose = TRUE) ) 
  0      1191.73: 0.888889 0.0592593  0.00000
  1      1174.58: 5.00000e-10 5.00000e-10 0.281419
  2      1158.10: 0.956617 0.00924174 -0.00978256
  3      1151.88: 0.875101 5.00000e-10 0.550741
  4      1144.22: 0.605105 5.00000e-10 0.550741
  5      1142.59: 0.335108 5.00000e-10 0.550741
  6      1142.06: 0.452561 5.00000e-10 0.550741
  7      1141.97: 0.423318 5.00000e-10 0.550740
  8      1141.97: 0.416524 5.00000e-10 0.550740
  9      1141.97: 0.417039 5.00000e-10 0.550740
Warning message:
Estimated variance-covariance for factor 'id' is singular
 in: `LMEoptimize<-`(`*tmp*`, value = list(maxIter = 200, tolerance = 
1.49011611938477e-08,  
> lm3<-lmer2(ppd~month+(month|id),control = list(msVerbose = TRUE) ) 
  0      1191.73: 0.942809 0.243432  0.00000
  1      1181.40: 0.870948  0.00000 -0.124082
  2      1146.91: 0.855545 2.18615e-05 0.0163376
  3      1140.17: 0.413096  0.00000 0.0798584
  4      1137.48: 0.484553 2.13939e-09 0.179021
  5      1136.23: 0.542748  0.00000 0.0799814
  6      1135.87: 0.508893 0.0117775 0.110609
  7      1135.79: 0.496375  0.00000 0.115248
  8      1135.79: 0.497595  0.00000 0.117998
  9      1135.79: 0.494810 2.92361e-08 0.119135
 10      1135.79: 0.494925 1.82196e-08 0.119321
 11      1135.79: 0.495102  0.00000 0.119390
 12      1135.79: 0.495102  0.00000 0.119390
> 

The -2loglikelihhood from MLwiN is 1104.594. I did plot the data and they 
looked fine (at least to me).

With best regards,

Yu-Kang




>From: "Douglas Bates" <bates at stat.wisc.edu>
>To: "Tu Yu-Kang" <yukangtu at hotmail.com>
>CC: r-sig-mixed-models at r-project.org
>Subject: Re: [R] negative variances
>Date: Thu, 12 Apr 2007 08:07:57 -0500
>
>On 4/12/07, Tu Yu-Kang <yukangtu at hotmail.com> wrote:
>>
>>Dear Prof Bates,
>>
>>Many thanks for your email. I tried lmer() and received the 
>>following
>>messages:
>>
>> >  lm2<-lmer(ppd~month+(month|id))
>>Warning message:
>>Estimated variance-covariance for factor 'id' is singular
>>  in: `LMEoptimize<-`(`*tmp*`, value = list(maxIter = 200, 
>>tolerance =
>>1.49011611938477e-08,
>>
>>I then tried lmer2():
>>
>> >  lm3<-lmer2(ppd~month+(month|id))
>> > summary(lm3)
>>Linear mixed-effects model fit by REML
>>   AIC  BIC logLik MLdeviance REMLdeviance
>>  1146 1166 -567.9       1126         1136
>>Random effects:
>>  Groups   Name        Variance  Std.Dev. Corr
>>  id       (Intercept) 0.1566631 0.395807
>>           month       0.0022331 0.047255 1.000
>>  Residual             0.6391120 0.799445
>>Number of obs: 420, groups: id, 140
>>
>>Fixed effects:
>>             Estimate Std. Error t value
>>(Intercept)  6.43595    0.07017   91.72
>>month       -0.36619    0.01642  -22.30
>>
>>Correlation of Fixed Effects:
>>       (Intr)
>>month -0.544
>
>I'm a bit pressed for time right now so this will be brief.  I 
>suggest
>that you add the optional argument
>
>control = list(msVerbose = TRUE)
>
>to the calls to lmer and to lmer2 and look at the progress of the
>iterations and also at the value of the deviance.  I have seen
>situations where lmer2 converges to a fit with a significantly 
>smaller
>deviance than can lmer because it progresses through the 
>near-singular
>region of the parameter space.
>
>What was the deviance (or the log-likelihood) from the MLWin fit 
>that
>gave negative variance components?
>
>Did you plot the data as ppd versus month by id using xyplot from 
>the
>lattice package?  That should always be the first step in the 
>analysis
>of longitudinal data.
>
>>However, I am not sure about the results, because MLwiN showed both 
>>random
>>effects were negative values (-0.196 and -0.023).
>>
>>I start to notice this problems of negative variances when I am 
>>learning
>>how to use structural equation modeling software to run multilevel 
>>models
>>for longitudinal data. To my great surprise, it occurs quite 
>>frequently. In
>>SEM, this problem sometimes may be overcome by estimating  a 
>>nonlinear
>>model by freeing the factor loadings. For example, in this data, 
>>PPD
>>(probing pocket depth) was measured three times at month 0, 3 and 
>>6. I only
>>fixed the first and last factor loadings to be 0 and 6 to get a 
>>non-linear
>>relation, and I also allow the level-1 residuals to be different on 
>>each
>>occasion. However, in some data, I failed to get a satifactory 
>>model no
>>matter how I modified my models.
>>
>>I looked for the discussion in several multilevel modeling 
>>textbooks but
>>only found one short discussion in the book by Brown and Prescott. 
>>SEM
>>literature usually suggest fixing the negative variances to 0. 
>>However, I
>>wander whether this is the only way to get around this problem or 
>>the
>>sensible way because if the random effects are fixed to 0 the model 
>>is no
>>longer a random effects model.
>>
>>With best regards,
>>
>>Yu-Kang



From sundar.dorai-raj at pdf.com  Thu Apr 12 20:16:24 2007
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 12 Apr 2007 11:16:24 -0700
Subject: [R-sig-ME] weights and lmer2
Message-ID: <461E7778.1050404@pdf.com>

Hi, all,

Is the weights argument supposed to work with lmer2? I'm trying to 
replicate some analysis from nlme::lme but am having difficulty with the 
weights argument. Here's some funny data to work with:

set.seed(42)
z <- expand.grid(A = 1:5, B = 1:4, r = 1:2)
n <- nrow(z)
z$w <- rpois(n, 100)
z$y <- rnorm(n, ifelse(z$A%%2, 1, 2) + ifelse(z$A%%2 & z$B%%2, -1, 1))
z[1:2] <- lapply(z[1:2], factor)

## Session 1 with lmer2
library(lme4)
fit <- lmer2(y ~ (1 | A) + (1 | A:B), z)
wfit <- lmer2(y ~ (1 | A) + (1 | A:B), z, weights = w)

## Session 2 with lme
library(nlme)
fit <- lme(y ~ 1, z, ~1 | A/B)
wfit <- lme(y ~ 1, z, ~1 | A/B, weights = ~w)
vfit <- lme(y ~ 1, z, ~1 | A/B, correlation = varFixed(~w))

## Standard deviations from Session 1 (lmer2)
                fit      wfit
A:B      0.9371798 0.9371798
A        0.8673610 0.8673610
Residual 0.7908478 0.7908478

## Standard deviations from Session 2 (lme)
                              fit       wfit       vfit
(Intercept) | B %in% A 0.9371788 0.90186678 0.90186678
(Intercept) | A        0.8673608 0.87420501 0.87420501
Residual               0.7908482 0.07878061 0.07878061

It appears that the weights argument has no effect for lmer2 (or for 
lmer, which I also tried).

Thanks,

--sundar



From bates at stat.wisc.edu  Thu Apr 12 20:35:44 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 12 Apr 2007 13:35:44 -0500
Subject: [R-sig-ME] weights and lmer2
In-Reply-To: <461E7778.1050404@pdf.com>
References: <461E7778.1050404@pdf.com>
Message-ID: <40e66e0b0704121135j35ec200ga1b64958973a314b@mail.gmail.com>

On 4/12/07, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
> Hi, all,

> Is the weights argument supposed to work with lmer2? I'm trying to
> replicate some analysis from nlme::lme but am having difficulty with the
> weights argument. Here's some funny data to work with:

> set.seed(42)
> z <- expand.grid(A = 1:5, B = 1:4, r = 1:2)
> n <- nrow(z)
> z$w <- rpois(n, 100)
> z$y <- rnorm(n, ifelse(z$A%%2, 1, 2) + ifelse(z$A%%2 & z$B%%2, -1, 1))
> z[1:2] <- lapply(z[1:2], factor)

> ## Session 1 with lmer2
> library(lme4)
> fit <- lmer2(y ~ (1 | A) + (1 | A:B), z)
> wfit <- lmer2(y ~ (1 | A) + (1 | A:B), z, weights = w)

It is supposed to work but I'm not sure if it was working in the last
released version of the lme4 package. I don't plan on releasing a new
version of lme4 until after R-2.5.0 is released.  There as changes in
the development version of lme4 that require R >= 2.5.0

I'll run your test in the development version later today if someone
else doesn't get a chance to do so before me.  I'm tied up with
classes and midterm exams right now.

> ## Session 2 with lme
> library(nlme)
> fit <- lme(y ~ 1, z, ~1 | A/B)
> wfit <- lme(y ~ 1, z, ~1 | A/B, weights = ~w)
> vfit <- lme(y ~ 1, z, ~1 | A/B, correlation = varFixed(~w))
>
> ## Standard deviations from Session 1 (lmer2)
>                 fit      wfit
> A:B      0.9371798 0.9371798
> A        0.8673610 0.8673610
> Residual 0.7908478 0.7908478
>
> ## Standard deviations from Session 2 (lme)
>                               fit       wfit       vfit
> (Intercept) | B %in% A 0.9371788 0.90186678 0.90186678
> (Intercept) | A        0.8673608 0.87420501 0.87420501
> Residual               0.7908482 0.07878061 0.07878061
>
> It appears that the weights argument has no effect for lmer2 (or for
> lmer, which I also tried).
>
> Thanks,
>
> --sundar
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From sundar.dorai-raj at pdf.com  Thu Apr 12 22:01:07 2007
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 12 Apr 2007 13:01:07 -0700
Subject: [R-sig-ME] weights and lmer2
In-Reply-To: <40e66e0b0704121135j35ec200ga1b64958973a314b@mail.gmail.com>
References: <461E7778.1050404@pdf.com>
	<40e66e0b0704121135j35ec200ga1b64958973a314b@mail.gmail.com>
Message-ID: <461E9003.8040805@pdf.com>


Douglas Bates said the following on 4/12/2007 11:35 AM:
> On 4/12/07, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
>> Hi, all,
> 
>> Is the weights argument supposed to work with lmer2? I'm trying to
>> replicate some analysis from nlme::lme but am having difficulty with the
>> weights argument. Here's some funny data to work with:
> 
>> set.seed(42)
>> z <- expand.grid(A = 1:5, B = 1:4, r = 1:2)
>> n <- nrow(z)
>> z$w <- rpois(n, 100)
>> z$y <- rnorm(n, ifelse(z$A%%2, 1, 2) + ifelse(z$A%%2 & z$B%%2, -1, 1))
>> z[1:2] <- lapply(z[1:2], factor)
> 
>> ## Session 1 with lmer2
>> library(lme4)
>> fit <- lmer2(y ~ (1 | A) + (1 | A:B), z)
>> wfit <- lmer2(y ~ (1 | A) + (1 | A:B), z, weights = w)
> 
> It is supposed to work but I'm not sure if it was working in the last
> released version of the lme4 package. I don't plan on releasing a new
> version of lme4 until after R-2.5.0 is released.  There as changes in
> the development version of lme4 that require R >= 2.5.0
> 
> I'll run your test in the development version later today if someone
> else doesn't get a chance to do so before me.  I'm tied up with
> classes and midterm exams right now.
> 
>> ## Session 2 with lme
>> library(nlme)
>> fit <- lme(y ~ 1, z, ~1 | A/B)
>> wfit <- lme(y ~ 1, z, ~1 | A/B, weights = ~w)
>> vfit <- lme(y ~ 1, z, ~1 | A/B, correlation = varFixed(~w))
>>
>> ## Standard deviations from Session 1 (lmer2)
>>                 fit      wfit
>> A:B      0.9371798 0.9371798
>> A        0.8673610 0.8673610
>> Residual 0.7908478 0.7908478
>>
>> ## Standard deviations from Session 2 (lme)
>>                               fit       wfit       vfit
>> (Intercept) | B %in% A 0.9371788 0.90186678 0.90186678
>> (Intercept) | A        0.8673608 0.87420501 0.87420501
>> Residual               0.7908482 0.07878061 0.07878061
>>
>> It appears that the weights argument has no effect for lmer2 (or for
>> lmer, which I also tried).
>>
>> Thanks,
>>
>> --sundar
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>

Hi, Prof. Bates,

Thanks for the reply. I can wait for next release. I'll just continue 
using lme for now. I also forgot to add my sessionInfo if that may be 
useful:

 > sessionInfo()
R version 2.5.0 beta (2007-04-10 r41119)
i386-pc-mingw32

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United 
States.1252;LC_MONETARY=English_United 
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
[7] "base"

other attached packages:
        lme4      Matrix     lattice
"0.9975-13" "0.9975-11"    "0.15-3"

Thanks,

--sundar



From bates at stat.wisc.edu  Sat Apr 14 17:33:46 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 14 Apr 2007 10:33:46 -0500
Subject: [R-sig-ME] [R] negative variances
In-Reply-To: <BAY115-F279895C35E63148657A5E6C15E0@phx.gbl>
References: <40e66e0b0704120607x26ffb9c0p7fe008457c327990@mail.gmail.com>
	<BAY115-F279895C35E63148657A5E6C15E0@phx.gbl>
Message-ID: <40e66e0b0704140833u425a3e74t85353ea21406edd2@mail.gmail.com>

On 4/12/07, Tu Yu-Kang <yukangtu at hotmail.com> wrote:
> Dear Prof Bates,
>
> > lm2<-lmer(ppd~month+(month|id))
> Warning message:
> Estimated variance-covariance for factor 'id' is singular
>  in: `LMEoptimize<-`(`*tmp*`, value = list(maxIter = 200, tolerance =
> 1.49011611938477e-08,
> > lm2<-lmer(ppd~month+(month|id),control = list(msVerbose = TRUE) )
>   0      1191.73: 0.888889 0.0592593  0.00000
>   1      1174.58: 5.00000e-10 5.00000e-10 0.281419
>   2      1158.10: 0.956617 0.00924174 -0.00978256
>   3      1151.88: 0.875101 5.00000e-10 0.550741
>   4      1144.22: 0.605105 5.00000e-10 0.550741
>   5      1142.59: 0.335108 5.00000e-10 0.550741
>   6      1142.06: 0.452561 5.00000e-10 0.550741
>   7      1141.97: 0.423318 5.00000e-10 0.550740
>   8      1141.97: 0.416524 5.00000e-10 0.550740
>   9      1141.97: 0.417039 5.00000e-10 0.550740
> Warning message:
> Estimated variance-covariance for factor 'id' is singular
>  in: `LMEoptimize<-`(`*tmp*`, value = list(maxIter = 200, tolerance =
> 1.49011611938477e-08,
> > lm3<-lmer2(ppd~month+(month|id),control = list(msVerbose = TRUE) )
>   0      1191.73: 0.942809 0.243432  0.00000
>   1      1181.40: 0.870948  0.00000 -0.124082
>   2      1146.91: 0.855545 2.18615e-05 0.0163376
>   3      1140.17: 0.413096  0.00000 0.0798584
>   4      1137.48: 0.484553 2.13939e-09 0.179021
>   5      1136.23: 0.542748  0.00000 0.0799814
>   6      1135.87: 0.508893 0.0117775 0.110609
>   7      1135.79: 0.496375  0.00000 0.115248
>   8      1135.79: 0.497595  0.00000 0.117998
>   9      1135.79: 0.494810 2.92361e-08 0.119135
>  10      1135.79: 0.494925 1.82196e-08 0.119321
>  11      1135.79: 0.495102  0.00000 0.119390
>  12      1135.79: 0.495102  0.00000 0.119390
> >

Both of those optimization paths show demonstrate convergence to a
singular variance-covariance matrix.  The first two parameters being
optimized are on the scale of variances (lmer) or standard deviations
(lmer2) and are constrained to be non-negative.  In the case of lmer
they need to be constrained to be slightly positive (>= 1e-10).  You
can see that both case have converged on the boundary although lmer2
achieved a better minimum (lower deviance).

The first parameter represents the relative variance (or relative
standard deviation for lmer2) of the intercept random effect.  The
second parameter is a relative variance (standard deviation) of a
linear combination of the random effects.  The third parameter
determines the linear combination.

To me this indicates model failure.  There are finite variances for
both the slope and intercept random effects but they  are perfectly
correlated.
> The -2loglikelihhood from MLwiN is 1104.594. I did plot the data and they
> looked fine (at least to me).
>
> With best regards,
>
> Yu-Kang
>
>
>
>
> >From: "Douglas Bates" <bates at stat.wisc.edu>
> >To: "Tu Yu-Kang" <yukangtu at hotmail.com>
> >CC: r-sig-mixed-models at r-project.org
> >Subject: Re: [R] negative variances
> >Date: Thu, 12 Apr 2007 08:07:57 -0500
> >
> >On 4/12/07, Tu Yu-Kang <yukangtu at hotmail.com> wrote:
> >>
> >>Dear Prof Bates,
> >>
> >>Many thanks for your email. I tried lmer() and received the
> >>following
> >>messages:
> >>
> >> >  lm2<-lmer(ppd~month+(month|id))
> >>Warning message:
> >>Estimated variance-covariance for factor 'id' is singular
> >>  in: `LMEoptimize<-`(`*tmp*`, value = list(maxIter = 200,
> >>tolerance =
> >>1.49011611938477e-08,
> >>
> >>I then tried lmer2():
> >>
> >> >  lm3<-lmer2(ppd~month+(month|id))
> >> > summary(lm3)
> >>Linear mixed-effects model fit by REML
> >>   AIC  BIC logLik MLdeviance REMLdeviance
> >>  1146 1166 -567.9       1126         1136
> >>Random effects:
> >>  Groups   Name        Variance  Std.Dev. Corr
> >>  id       (Intercept) 0.1566631 0.395807
> >>           month       0.0022331 0.047255 1.000
> >>  Residual             0.6391120 0.799445
> >>Number of obs: 420, groups: id, 140
> >>
> >>Fixed effects:
> >>             Estimate Std. Error t value
> >>(Intercept)  6.43595    0.07017   91.72
> >>month       -0.36619    0.01642  -22.30
> >>
> >>Correlation of Fixed Effects:
> >>       (Intr)
> >>month -0.544
> >
> >I'm a bit pressed for time right now so this will be brief.  I
> >suggest
> >that you add the optional argument
> >
> >control = list(msVerbose = TRUE)
> >
> >to the calls to lmer and to lmer2 and look at the progress of the
> >iterations and also at the value of the deviance.  I have seen
> >situations where lmer2 converges to a fit with a significantly
> >smaller
> >deviance than can lmer because it progresses through the
> >near-singular
> >region of the parameter space.
> >
> >What was the deviance (or the log-likelihood) from the MLWin fit
> >that
> >gave negative variance components?
> >
> >Did you plot the data as ppd versus month by id using xyplot from
> >the
> >lattice package?  That should always be the first step in the
> >analysis
> >of longitudinal data.
> >
> >>However, I am not sure about the results, because MLwiN showed both
> >>random
> >>effects were negative values (-0.196 and -0.023).
> >>
> >>I start to notice this problems of negative variances when I am
> >>learning
> >>how to use structural equation modeling software to run multilevel
> >>models
> >>for longitudinal data. To my great surprise, it occurs quite
> >>frequently. In
> >>SEM, this problem sometimes may be overcome by estimating  a
> >>nonlinear
> >>model by freeing the factor loadings. For example, in this data,
> >>PPD
> >>(probing pocket depth) was measured three times at month 0, 3 and
> >>6. I only
> >>fixed the first and last factor loadings to be 0 and 6 to get a
> >>non-linear
> >>relation, and I also allow the level-1 residuals to be different on
> >>each
> >>occasion. However, in some data, I failed to get a satifactory
> >>model no
> >>matter how I modified my models.
> >>
> >>I looked for the discussion in several multilevel modeling
> >>textbooks but
> >>only found one short discussion in the book by Brown and Prescott.
> >>SEM
> >>literature usually suggest fixing the negative variances to 0.
> >>However, I
> >>wander whether this is the only way to get around this problem or
> >>the
> >>sensible way because if the random effects are fixed to 0 the model
> >>is no
> >>longer a random effects model.
> >>
> >>With best regards,
> >>
> >>Yu-Kang
>
> _________________________________________________________________
> ???? MSN Mobile ??????? MSN Messenger ??????
> http://msn.com.tw/msnmobile
>
>



From bates at stat.wisc.edu  Sat Apr 14 17:49:58 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 14 Apr 2007 10:49:58 -0500
Subject: [R-sig-ME] weights and lmer2
In-Reply-To: <461E9003.8040805@pdf.com>
References: <461E7778.1050404@pdf.com>
	<40e66e0b0704121135j35ec200ga1b64958973a314b@mail.gmail.com>
	<461E9003.8040805@pdf.com>
Message-ID: <40e66e0b0704140849m460a8f7bm6fa1be84b01f27c1@mail.gmail.com>

On 4/12/07, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
>
> Douglas Bates said the following on 4/12/2007 11:35 AM:
> > On 4/12/07, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
> >> Hi, all,
> >
> >> Is the weights argument supposed to work with lmer2? I'm trying to
> >> replicate some analysis from nlme::lme but am having difficulty with the
> >> weights argument. Here's some funny data to work with:
> >
> >> set.seed(42)
> >> z <- expand.grid(A = 1:5, B = 1:4, r = 1:2)
> >> n <- nrow(z)
> >> z$w <- rpois(n, 100)
> >> z$y <- rnorm(n, ifelse(z$A%%2, 1, 2) + ifelse(z$A%%2 & z$B%%2, -1, 1))
> >> z[1:2] <- lapply(z[1:2], factor)
> >
> >> ## Session 1 with lmer2
> >> library(lme4)
> >> fit <- lmer2(y ~ (1 | A) + (1 | A:B), z)
> >> wfit <- lmer2(y ~ (1 | A) + (1 | A:B), z, weights = w)
> >
> > It is supposed to work but I'm not sure if it was working in the last
> > released version of the lme4 package. I don't plan on releasing a new
> > version of lme4 until after R-2.5.0 is released.  There as changes in
> > the development version of lme4 that require R >= 2.5.0
> >
> > I'll run your test in the development version later today if someone
> > else doesn't get a chance to do so before me.  I'm tied up with
> > classes and midterm exams right now.
> >
> >> ## Session 2 with lme
> >> library(nlme)
> >> fit <- lme(y ~ 1, z, ~1 | A/B)
> >> wfit <- lme(y ~ 1, z, ~1 | A/B, weights = ~w)
> >> vfit <- lme(y ~ 1, z, ~1 | A/B, correlation = varFixed(~w))
> >>
> >> ## Standard deviations from Session 1 (lmer2)
> >>                 fit      wfit
> >> A:B      0.9371798 0.9371798
> >> A        0.8673610 0.8673610
> >> Residual 0.7908478 0.7908478
> >>
> >> ## Standard deviations from Session 2 (lme)
> >>                               fit       wfit       vfit
> >> (Intercept) | B %in% A 0.9371788 0.90186678 0.90186678
> >> (Intercept) | A        0.8673608 0.87420501 0.87420501
> >> Residual               0.7908482 0.07878061 0.07878061
> >>
> >> It appears that the weights argument has no effect for lmer2 (or for
> >> lmer, which I also tried).

>
> Hi, Prof. Bates,
>
> Thanks for the reply. I can wait for next release. I'll just continue
> using lme for now. I also forgot to add my sessionInfo if that may be
> useful:
>
>  > sessionInfo()
> R version 2.5.0 beta (2007-04-10 r41119)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
> States.1252;LC_MONETARY=English_United
> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
>
> attached base packages:
> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
> [7] "base"
>
> other attached packages:
>         lme4      Matrix     lattice
> "0.9975-13" "0.9975-11"    "0.15-3"

Well the good news is that in the development version of lme4 the
weights argument is used.  The bad news is that it may not be used on
the proper scale.  I believe that it is being used as if it were on
the scale of the inverse of the standard deviation and the actual
usage should be on the scale of the inverse of the variance.  I'll
need the check the documentation versus the code.



From pierces1 at msu.edu  Sat Apr 21 05:01:37 2007
From: pierces1 at msu.edu (Steven J. Pierce)
Date: Fri, 20 Apr 2007 23:01:37 -0400
Subject: [R-sig-ME] Using lmer to  do multilevel survival models
Message-ID: <000301c783c1$61651610$0a00a8c0@TheVoid>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20070420/03e5f376/attachment.pl>

From vmuggeo at dssm.unipa.it  Fri Apr 27 14:56:39 2007
From: vmuggeo at dssm.unipa.it (vito muggeo)
Date: Fri, 27 Apr 2007 14:56:39 +0200
Subject: [R-sig-ME] blockdiagonal variance re in lmer
Message-ID: <4631F307.2040607@dssm.unipa.it>

Hi all,
I am interested in usign lmer/lmer2 to fit (G)LMM where the variance of 
the random effects is blockdiagonal or simply multiple of identity. Namely:

G=blockdiag(s1*I_1,...,s_kI_k)
G=s*I

In the lme() from the nlme package I can use the random argument

#multiple of the identity:
lme(..,random=list(id=pdIdent(~Z-1)))

#blockdiagonal
lme(..,random=list(id=pdBlocked(list(pdIdent(~Z1-1),pdIdent(~Z2-1))))
or even
random=list(id=pdIdent(~Z1-1),id=pdIdent(~Z2-1))

I cannot figure out how I can attain the same with lmer/lmer2..
Please, could anyone help me?

many thanks,
vito


-- 
====================================
Vito M.R. Muggeo
Dip.to Sc Statist e Matem `Vianelli'
Universit? di Palermo
viale delle Scienze, edificio 13
90128 Palermo - ITALY
tel: 091 6626240
fax: 091 485726/485612



From shademan at ucla.edu  Tue May  8 20:46:54 2007
From: shademan at ucla.edu (shabnam shademan)
Date: Tue, 8 May 2007 11:46:54 -0700
Subject: [R-sig-ME] comparing mixed models that share some factors
Message-ID: <200705081846.l48Ikq5J024124@mail.ucla.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20070508/c33cc9b8/attachment.pl>

From srjafarzadeh at gmail.com  Wed May  9 03:12:48 2007
From: srjafarzadeh at gmail.com (Seyed Reza Jafarzadeh)
Date: Tue, 8 May 2007 18:12:48 -0700
Subject: [R-sig-ME] comparing mixed models that share some factors
In-Reply-To: <200705081846.l48Ikq5J024124@mail.ucla.edu>
References: <200705081846.l48Ikq5J024124@mail.ucla.edu>
Message-ID: <83217d00705081812v58b2529bjecd11aec4ee1d4b4@mail.gmail.com>

Hi,

anova(model1, model2) will allow you to compare models with the same
fixed effects, but different specifications of random effects. For
comparing two non-nested models (like your case) you can use AIC or
BIC. Here is a good paper
(http://www.mpi.nl/world/persons/private/baayen/publications/baayenDavidsonBates.pdf)


Reza



On 5/8/07, shabnam shademan <shademan at ucla.edu> wrote:
> Hi all -
>
>
>
> I am unsophisticated user in need of some help.  I have a study in which I
> have two random factors: subject and item.  I also have 3 fixed effects: 1.
> X1 (continuous values), 2. Y (continuous values), and 3. Age (has two levels
> "young" and "old").  My dependent variable is Z.
>
>
>
> I am using the following formula to predict the effect of each factor:
>
> fit1 <- lmer(Z ~Age * X1 * Y + (1 | Subject) + (1 | Stim), method = "ML",
> data = d)
>
>
>
> furthermore, I have the option of using a different model (theoretical, not
> statistical) in order to calculate values for X.  I will call this X2.  This
> means that I could also get a fit in the following way:
>
> fit2 <- lmer(Z ~Age * X2 * Y + (1 | Subject) + (1 | Stim), method = "ML",
> data = d)
>
>
>
>
>
> Here is the question:
>
> Is there anyway to compare fit1 and fit2?  would anova (fit1, fit2) be
> appropriate in this case?  (if possible, would you be kind enough to give me
> references on the answer?)
>
>
>
> Any help is greatly appreciated.
>
> -shabnam
>
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From gavin.simpson at ucl.ac.uk  Wed May  9 15:23:47 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 09 May 2007 14:23:47 +0100
Subject: [R-sig-ME] Specifying random effects in lme()
Message-ID: <1178717027.20901.18.camel@gsimpson.geog.ucl.ac.uk>

Dear List,

I am modelling repeated measures data from 11 sites in the UK using
lme() from the nlme package. I have the following call for a random
intercept model with an AR1 error structure:

mod <- lme(doc ~ temp + log.so4 + log.cl, random = ~ 1 | site,
           correlation = corAR1(), data = hydro)

I would like to fit a model that has random effects for log.so4 and
log.cl within site, so that I am fitting a random slop and intercept
model --- we want the effect of log.so4 and log.cl to vary between sites
and to compare the fits of the two models.

If I am following lmer() in lme4 correctly, I believe I could fit this
model as:

hydro.lmer <- lmer(doc ~ temp + log.so4 + log.cl +
                   (log.so4 | site) + (log.cl | site),
                   data = hydro)

but without the AR1 correlation.

I am struggling to get the syntax correct for random in lme() to add
random effects to log.so4 and log.cl. I have only managed to get a model
where one of the coefficients (log.cl) also has a random effect.

I would be most grateful if someone could explain how to fit the model
in lme() by showing me the correct form for the random argument.

Many thanks,

Gav

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From Fredrik.X.Nilsson at skane.se  Wed May  9 16:58:16 2007
From: Fredrik.X.Nilsson at skane.se (Nilsson Fredrik X)
Date: Wed, 9 May 2007 16:58:16 +0200
Subject: [R-sig-ME] Specifying random effects in lme()
In-Reply-To: <1178717027.20901.18.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <87A0C64299B27148B40BE0DB83EDE2DBC66533@RSMAIL002.REG.SKANE.SE>

Dear Gavin, 

It SHOULD work if you add
random = ~log.so4+log.cl|site

I made a script which is able to do so, but perhaps the addition of this extra variability in your case over-specifies the model? 

Hope this helps. Otherwise I suppose that you have to supply more details.

Cheers,

Fredrik
--------------------------------------------------------------------------
Example:

nsite<-10
nwithin<-10
prosw<-nsite*nwithin

conc<-vector(length=prosw)
temp<-rnorm(prosw)
log.so4<-seq(-5,5, length=nwithin)
log.cl<-rnorm(prosw)
int<-17
acl<-sqrt(2)
aso4<-21
atemp<- -14
rho<--0.9
for (i in 1:nsite)
{
sf<-0.1*rnorm(1)
sc<-2*rnorm(1)
ss<-rnorm(1)
err<-0.1*rnorm(1)
for (j in 1:nwithin)
{
conc[nwithin*(i-1)+j]<-sf +  int + (acl+sc)*log.cl[nwithin*(i-1)+j] + (aso4+ss)*log.so4[j] +
                  atemp*temp[nwithin*(i-1)+j] + err
err<-err*rho + 0.1*rnorm(1)
}
}

temp<-rep(temp,nsite)
log.so4<-rep(log.so4, nsite)
log.cl<-rep(log.cl,nsite)
site<-rep(seq(1,nsite), each=nwithin)

DF<-data.frame(conc,temp, log.so4, log.cl, site)
DF$site<-as.factor(site)

DFgd<-groupedData(conc~temp|site, data=DF)
plot(DFgd)

DF.lme<-lme(conc~log.so4+temp+log.cl, random=~log.so4+log.cl|site, data=DF)
plot(ACF(DF.lme, resType="n", form=~1|site, maxLag=floor(nwithin/2-.5)), alpha=0.05)
DF2.lme<-update(DF.lme, correlation=corAR1(form=~1|site))



-----Ursprungligt meddelande-----
Fr?n: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] F?r Gavin Simpson
Skickat: den 9 maj 2007 15:24
Till: R-SIG-Mixed-Model
?mne: [R-sig-ME] Specifying random effects in lme()

Dear List,

I am modelling repeated measures data from 11 sites in the UK using
lme() from the nlme package. I have the following call for a random
intercept model with an AR1 error structure:

mod <- lme(doc ~ temp + log.so4 + log.cl, random = ~ 1 | site,
           correlation = corAR1(), data = hydro)

I would like to fit a model that has random effects for log.so4 and
log.cl within site, so that I am fitting a random slop and intercept
model --- we want the effect of log.so4 and log.cl to vary between sites
and to compare the fits of the two models.

If I am following lmer() in lme4 correctly, I believe I could fit this
model as:

hydro.lmer <- lmer(doc ~ temp + log.so4 + log.cl +
                   (log.so4 | site) + (log.cl | site),
                   data = hydro)

but without the AR1 correlation.

I am struggling to get the syntax correct for random in lme() to add
random effects to log.so4 and log.cl. I have only managed to get a model
where one of the coefficients (log.cl) also has a random effect.

I would be most grateful if someone could explain how to fit the model
in lme() by showing me the correct form for the random argument.

Many thanks,

Gav

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From mdu at ceh.ac.uk  Wed May  9 17:05:43 2007
From: mdu at ceh.ac.uk (Mike Dunbar)
Date: Wed, 09 May 2007 16:05:43 +0100
Subject: [R-sig-ME] Specifying random effects in lme()
Message-ID: <s641f175.058@wpo.nerc.ac.uk>

Gav

I don't know the answer to your question, but it may not be possible. If it is possible to specify it, then you may still run into problems fitting it. You're trying to fit quite a complex model here with two random slopes and an AR term, are you really sure your model needs to be that complicated? 

Can you fit the lmer model you mention? If so, is there evidence for autocorrelation of residuals?

cheers

Mike




>>> Gavin Simpson <gavin.simpson at ucl.ac.uk> 09/05/2007 14:23 >>>
Dear List,

I am modelling repeated measures data from 11 sites in the UK using
lme() from the nlme package. I have the following call for a random
intercept model with an AR1 error structure:

mod <- lme(doc ~ temp + log.so4 + log.cl, random = ~ 1 | site,
           correlation = corAR1(), data = hydro)

I would like to fit a model that has random effects for log.so4 and
log.cl within site, so that I am fitting a random slop and intercept
model --- we want the effect of log.so4 and log.cl to vary between sites
and to compare the fits of the two models.

If I am following lmer() in lme4 correctly, I believe I could fit this
model as:

hydro.lmer <- lmer(doc ~ temp + log.so4 + log.cl +
                   (log.so4 | site) + (log.cl | site),
                   data = hydro)

but without the AR1 correlation.

I am struggling to get the syntax correct for random in lme() to add
random effects to log.so4 and log.cl. I have only managed to get a model
where one of the coefficients (log.cl) also has a random effect.

I would be most grateful if someone could explain how to fit the model
in lme() by showing me the correct form for the random argument.

Many thanks,

Gav

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/ 
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
This message (and any attachments) is for the recipient only...{{dropped}}



From gavin.simpson at ucl.ac.uk  Wed May  9 18:20:36 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 09 May 2007 17:20:36 +0100
Subject: [R-sig-ME] Specifying random effects in lme()
In-Reply-To: <s641f175.059@wpo.nerc.ac.uk>
References: <s641f175.059@wpo.nerc.ac.uk>
Message-ID: <1178727636.20901.32.camel@gsimpson.geog.ucl.ac.uk>

On Wed, 2007-05-09 at 16:05 +0100, Mike Dunbar wrote:
> Gav
> 
> I don't know the answer to your question, but it may not be possible.
> If it is possible to specify it, then you may still run into problems
> fitting it. You're trying to fit quite a complex model here with two
> random slopes and an AR term, are you really sure your model needs to
> be that complicated? 

Hi Mike,

That's one of the things we want to test.

The random effects just take up an extra df or eleven. I have plenty to
spare in the model, with many years data. For the extra 22 df used up,
does the model fit significantly better using AIC/BIC and friends?

I can fit the model in lmer very easily, converges nicely and quickly.
But I wanted to have the AR1 term in there to mop up some of the
residual autocorrelation in the error structure. There is some
indication of autocorrelation in the residuals, and testing models
without the AR term and with the AR show that the model with the AR term
is a big improvement as measured by AIC, BIC and the likelihood ratio
test.

There is some evidence that the effects of the two covariates does
differ amongst sites based on analysis of the model output from the
model with only random intercepts. We'd like to test if the more complex
model fits the data better.

Cheers Mike,

G

> 
> Can you fit the lmer model you mention? If so, is there evidence for
> autocorrelation of residuals?
> 
> cheers
> 
> Mike
> 
> 
> 
> 
> >>> Gavin Simpson <gavin.simpson at ucl.ac.uk> 09/05/2007 14:23 >>>
> Dear List,
> 
> I am modelling repeated measures data from 11 sites in the UK using
> lme() from the nlme package. I have the following call for a random
> intercept model with an AR1 error structure:
> 
> mod <- lme(doc ~ temp + log.so4 + log.cl, random = ~ 1 | site,
>            correlation = corAR1(), data = hydro)
> 
> I would like to fit a model that has random effects for log.so4 and
> log.cl within site, so that I am fitting a random slop and intercept
> model --- we want the effect of log.so4 and log.cl to vary between sites
> and to compare the fits of the two models.
> 
> If I am following lmer() in lme4 correctly, I believe I could fit this
> model as:
> 
> hydro.lmer <- lmer(doc ~ temp + log.so4 + log.cl +
>                    (log.so4 | site) + (log.cl | site),
>                    data = hydro)
> 
> but without the AR1 correlation.
> 
> I am struggling to get the syntax correct for random in lme() to add
> random effects to log.so4 and log.cl. I have only managed to get a model
> where one of the coefficients (log.cl) also has a random effect.
> 
> I would be most grateful if someone could explain how to fit the model
> in lme() by showing me the correct form for the random argument.
> 
> Many thanks,
> 
> Gav
> 
> -- 
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
>  Gavin Simpson                 [t] +44 (0)20 7679 0522
>  ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
>  Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
>  Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/ 
>  UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk 
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From gavin.simpson at ucl.ac.uk  Wed May  9 19:40:10 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 09 May 2007 18:40:10 +0100
Subject: [R-sig-ME] Specifying random effects in lme()
In-Reply-To: <87A0C64299B27148B40BE0DB83EDE2DBC66533@RSMAIL002.REG.SKANE.SE>
References: <87A0C64299B27148B40BE0DB83EDE2DBC66533@RSMAIL002.REG.SKANE.SE>
Message-ID: <1178732411.20901.42.camel@gsimpson.geog.ucl.ac.uk>

On Wed, 2007-05-09 at 16:58 +0200, Nilsson Fredrik X wrote:
> Dear Gavin, 
> 
> It SHOULD work if you add
> random = ~log.so4+log.cl|site

Hi Nilsson,

Many thanks for this. I had actually tried this but when I tried coef()
on the resulting model, I got what looked like only random effects for
the log.cl parameter. Now that I have looked at the resulting object
more closely after your email, I have noticed that the magnitudes of the
random effects for log.so4 are very small indeed, and so when coef()
prints the values for log.so4, they do all look constant, but only
because they are printed to finite decimal places.

I should look more closely next time!

Many thanks, I now have some indication that the effect of log.cl does
vary between sites, but not log.so4.

All the best,

G

> 
> I made a script which is able to do so, but perhaps the addition of
> this extra variability in your case over-specifies the model? 
> 
> Hope this helps. Otherwise I suppose that you have to supply more details.
> 
> Cheers,
> 
> Fredrik
> --------------------------------------------------------------------------
> Example:
> 
> nsite<-10
> nwithin<-10
> prosw<-nsite*nwithin
> 
> conc<-vector(length=prosw)
> temp<-rnorm(prosw)
> log.so4<-seq(-5,5, length=nwithin)
> log.cl<-rnorm(prosw)
> int<-17
> acl<-sqrt(2)
> aso4<-21
> atemp<- -14
> rho<--0.9
> for (i in 1:nsite)
> {
> sf<-0.1*rnorm(1)
> sc<-2*rnorm(1)
> ss<-rnorm(1)
> err<-0.1*rnorm(1)
> for (j in 1:nwithin)
> {
> conc[nwithin*(i-1)+j]<-sf +  int + (acl+sc)*log.cl[nwithin*(i-1)+j] + (aso4+ss)*log.so4[j] +
>                   atemp*temp[nwithin*(i-1)+j] + err
> err<-err*rho + 0.1*rnorm(1)
> }
> }
> 
> temp<-rep(temp,nsite)
> log.so4<-rep(log.so4, nsite)
> log.cl<-rep(log.cl,nsite)
> site<-rep(seq(1,nsite), each=nwithin)
> 
> DF<-data.frame(conc,temp, log.so4, log.cl, site)
> DF$site<-as.factor(site)
> 
> DFgd<-groupedData(conc~temp|site, data=DF)
> plot(DFgd)
> 
> DF.lme<-lme(conc~log.so4+temp+log.cl, random=~log.so4+log.cl|site, data=DF)
> plot(ACF(DF.lme, resType="n", form=~1|site, maxLag=floor(nwithin/2-.5)), alpha=0.05)
> DF2.lme<-update(DF.lme, correlation=corAR1(form=~1|site))
> 
> 
> 
> -----Ursprungligt meddelande-----
> Fr?n: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] F?r Gavin Simpson
> Skickat: den 9 maj 2007 15:24
> Till: R-SIG-Mixed-Model
> ?mne: [R-sig-ME] Specifying random effects in lme()
> 
> Dear List,
> 
> I am modelling repeated measures data from 11 sites in the UK using
> lme() from the nlme package. I have the following call for a random
> intercept model with an AR1 error structure:
> 
> mod <- lme(doc ~ temp + log.so4 + log.cl, random = ~ 1 | site,
>            correlation = corAR1(), data = hydro)
> 
> I would like to fit a model that has random effects for log.so4 and
> log.cl within site, so that I am fitting a random slop and intercept
> model --- we want the effect of log.so4 and log.cl to vary between sites
> and to compare the fits of the two models.
> 
> If I am following lmer() in lme4 correctly, I believe I could fit this
> model as:
> 
> hydro.lmer <- lmer(doc ~ temp + log.so4 + log.cl +
>                    (log.so4 | site) + (log.cl | site),
>                    data = hydro)
> 
> but without the AR1 correlation.
> 
> I am struggling to get the syntax correct for random in lme() to add
> random effects to log.so4 and log.cl. I have only managed to get a model
> where one of the coefficients (log.cl) also has a random effect.
> 
> I would be most grateful if someone could explain how to fit the model
> in lme() by showing me the correct form for the random argument.
> 
> Many thanks,
> 
> Gav
> 
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From markus.jantti at iki.fi  Wed May  9 23:49:21 2007
From: markus.jantti at iki.fi (=?ISO-8859-1?Q?Markus_J=E4ntti?=)
Date: Thu, 10 May 2007 00:49:21 +0300
Subject: [R-sig-ME] Extracting the coefficients from corStruct objects
Message-ID: <464241E1.6060908@iki.fi>

Dear All:

I have been trying to get the coefficients from corAR1 objects in lme objects in 
then lme package (3.1-80), but coef(obj$modelStruct$corStruct) returns something 
rather different that summary(object). I have tried to follow the various 
summary. methods in the source code, but have kind of gotten lost. Would be 
greatful if someone could point to a simple solution.

It would be helpful to manually extract the AR(1) coefficients, because I am 
data sets consisting of very large cohorts of individuals. I extract the 
information I need from each estimated object and store them in small matrices. 
I have not been able to figure out a way to extract the AR(1) parameters, however.

I include example code below, which produces the following results:
 > summary(fm2)
<snip>
Correlation Structure: AR(1)
  Formula: ~year | ind
  Parameter estimate(s):
   Phi
0.424
<snip>

but

 > coef(fm2$modelStruct$corStruct)
[1] 0.906

Version info for nlme:

 > packageDescription("nlme")
Package: nlme
Version: 3.1-80
Date: 2007-03-30
Priority: recommended
Title: Linear and Nonlinear Mixed Effects Models
Author: Jose Pinheiro <Jose.Pinheiro at pharma.novartis.com>, Douglas
         Bates <bates at stat.wisc.edu>, Saikat DebRoy
         <saikat at stat.wisc.edu>, and Deepayan Sarkar
         <deepayan at stat.wisc.edu>
Maintainer: R-core <R-core at R-project.org>
Description: Fit and compare Gaussian linear and nonlinear
         mixed-effects models.
Depends: graphics, stats, R (>= 2.3.0)
Imports: lattice
LazyLoad: yes
LazyData: yes
License: GPL version 2 or later
Packaged: Fri Mar 30 07:37:02 2007; ripley
Built: R 2.5.0; i486-pc-linux-gnu; 2007-04-25 03:13:00; unix

Example code:
<code>
library(nlme)
## number of time periods
years <- 5
## individuals
n <- 100

## the random effects
a <- as.vector(sapply(rnorm(n), rep, years))

## indicators for individuals
ind <- as.vector(sapply(1:n, rep, years))
year <- rep(1:years, n)

## the white noise
v <- rnorm(n*years)
## the AR(1) parameter
rho <- .5
## generate the AR1 erroer
## randomly draw the starting value of the AR(1) error for each unit
u1 <- rnorm(n)
## store values here:
u <- numeric(n*years)
## a counter:
k <- 1
## there has to be a much more elegant way to do this!
for(i in 1:n)
   {
     u[k] <- u1[i]
     for(j in 2:years)
       {
         k <- k+1
         u[k] <- rho*u[k-1] + v[k]
       }
     k <- k+1
   }

## the covariate
x <- runif(n*years)
## the intercept and slope
b0 <- 10
b1 <- 4
## and the dependent variable
y <- b0 + b1*x + a + u

test.d <- data.frame(y=y, x=x, ind=ind, year=year)
rm("x", "y", "ind", "years")

fm1 <- lme(y ~ x, random = ~ 1 | ind,
            data=test.d)
summary(fm1)
fm2 <- lme(y ~ x, random = ~ 1 | ind, correlation=corAR1(form=~year),
            data=test.d)
summary(fm2)
coef(fm2$modelStruct$corStruct)
</code>

-- 
Markus Jantti
Abo Akademi University
markus.jantti at iki.fi
http://www.iki.fi/~mjantti



From Fredrik.X.Nilsson at skane.se  Thu May 10 11:50:55 2007
From: Fredrik.X.Nilsson at skane.se (Nilsson Fredrik X)
Date: Thu, 10 May 2007 11:50:55 +0200
Subject: [R-sig-ME] Extracting the coefficients from corStruct objects
In-Reply-To: <464241E1.6060908@iki.fi>
Message-ID: <87A0C64299B27148B40BE0DB83EDE2DBC6657F@RSMAIL002.REG.SKANE.SE>

Dear Markus,

I don't realize why one gets these strange results and I would like to understand this too!

But here's a solution while waiting from the enlightenment:

Look at:
intervals(ModelwAR1.lme)$corStruct

And then 

intervals(ModelwAR1.lme)$corStruct[2]

is what you look for.

Cheers,

Fredrik

-----Ursprungligt meddelande-----
Fr?n: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] F?r Markus J?ntti
Skickat: den 9 maj 2007 23:49
Till: R-SIG-Mixed-Model
?mne: [R-sig-ME] Extracting the coefficients from corStruct objects

Dear All:

I have been trying to get the coefficients from corAR1 objects in lme objects in 
then lme package (3.1-80), but coef(obj$modelStruct$corStruct) returns something 
rather different that summary(object). I have tried to follow the various 
summary. methods in the source code, but have kind of gotten lost. Would be 
greatful if someone could point to a simple solution.

It would be helpful to manually extract the AR(1) coefficients, because I am 
data sets consisting of very large cohorts of individuals. I extract the 
information I need from each estimated object and store them in small matrices. 
I have not been able to figure out a way to extract the AR(1) parameters, however.

I include example code below, which produces the following results:
 > summary(fm2)
<snip>
Correlation Structure: AR(1)
  Formula: ~year | ind
  Parameter estimate(s):
   Phi
0.424
<snip>

but

 > coef(fm2$modelStruct$corStruct)
[1] 0.906

Version info for nlme:

 > packageDescription("nlme")
Package: nlme
Version: 3.1-80
Date: 2007-03-30
Priority: recommended
Title: Linear and Nonlinear Mixed Effects Models
Author: Jose Pinheiro <Jose.Pinheiro at pharma.novartis.com>, Douglas
         Bates <bates at stat.wisc.edu>, Saikat DebRoy
         <saikat at stat.wisc.edu>, and Deepayan Sarkar
         <deepayan at stat.wisc.edu>
Maintainer: R-core <R-core at R-project.org>
Description: Fit and compare Gaussian linear and nonlinear
         mixed-effects models.
Depends: graphics, stats, R (>= 2.3.0)
Imports: lattice
LazyLoad: yes
LazyData: yes
License: GPL version 2 or later
Packaged: Fri Mar 30 07:37:02 2007; ripley
Built: R 2.5.0; i486-pc-linux-gnu; 2007-04-25 03:13:00; unix

Example code:
<code>
library(nlme)
## number of time periods
years <- 5
## individuals
n <- 100

## the random effects
a <- as.vector(sapply(rnorm(n), rep, years))

## indicators for individuals
ind <- as.vector(sapply(1:n, rep, years))
year <- rep(1:years, n)

## the white noise
v <- rnorm(n*years)
## the AR(1) parameter
rho <- .5
## generate the AR1 erroer
## randomly draw the starting value of the AR(1) error for each unit
u1 <- rnorm(n)
## store values here:
u <- numeric(n*years)
## a counter:
k <- 1
## there has to be a much more elegant way to do this!
for(i in 1:n)
   {
     u[k] <- u1[i]
     for(j in 2:years)
       {
         k <- k+1
         u[k] <- rho*u[k-1] + v[k]
       }
     k <- k+1
   }

## the covariate
x <- runif(n*years)
## the intercept and slope
b0 <- 10
b1 <- 4
## and the dependent variable
y <- b0 + b1*x + a + u

test.d <- data.frame(y=y, x=x, ind=ind, year=year)
rm("x", "y", "ind", "years")

fm1 <- lme(y ~ x, random = ~ 1 | ind,
            data=test.d)
summary(fm1)
fm2 <- lme(y ~ x, random = ~ 1 | ind, correlation=corAR1(form=~year),
            data=test.d)
summary(fm2)
coef(fm2$modelStruct$corStruct)
</code>

-- 
Markus Jantti
Abo Akademi University
markus.jantti at iki.fi
http://www.iki.fi/~mjantti

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From HStevens at muohio.edu  Thu May 10 19:51:20 2007
From: HStevens at muohio.edu (Martin Henry H. Stevens)
Date: Thu, 10 May 2007 13:51:20 -0400
Subject: [R-sig-ME] log Likelihood depends on contrasts?
Message-ID: <CCCF1634-5AB3-4203-AFDC-7702DA762E43@muohio.edu>

Hi folks,
I didn't realize log likelihood of a model depended on the particular  
specification of contrasts.

Example

 > str(cake)
'data.frame':	270 obs. of  5 variables:
$ replicate  : Factor w/ 15 levels "1","2","3","4",..: 1 1 1 1 1 1 1  
1 1 1 ...
$ batch      : Factor w/ 3 levels "1","2","3": 1 1 1 1 1 1 2 2 2 2 ...
$ recipe     : Factor w/ 3 levels "1","2","3": 1 1 1 1 1 1 2 2 2 2 ...
$ temperature: Ord.factor w/ 6 levels "175"<"185"<"195"<..: 1 2 3 4 5  
6 1 2 3 4 ...
$ angle      : int  42 46 47 39 53 42 39 46 51 49 ...
 > options(contrasts=c("contr.treatment", "contr.poly"))
 > mod.t <- lmer(angle ~ recipe + (1|batch), data=cake)
 > options(contrasts=c("contr.sum", "contr.poly"))
 > mod.s <- lmer(angle ~ recipe + (1|batch), data=cake)
 > logLik(mod.t)
'log Lik.' -947.82 (df=4)
 > logLik(mod.s)
'log Lik.' -948.92 (df=4)
 >

Any insight is appreciated.

Many thanks,
Hank


Dr. Hank Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/

"E Pluribus Unum"



From bates at stat.wisc.edu  Thu May 10 21:24:06 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 10 May 2007 14:24:06 -0500
Subject: [R-sig-ME] log Likelihood depends on contrasts?
In-Reply-To: <CCCF1634-5AB3-4203-AFDC-7702DA762E43@muohio.edu>
References: <CCCF1634-5AB3-4203-AFDC-7702DA762E43@muohio.edu>
Message-ID: <40e66e0b0705101224l724219f0y3ccbae6370de2fb2@mail.gmail.com>

On 5/10/07, Martin Henry H. Stevens <HStevens at muohio.edu> wrote:
> Hi folks,
> I didn't realize log likelihood of a model depended on the particular
> specification of contrasts.
>
> Example
>
>  > str(cake)
> 'data.frame':   270 obs. of  5 variables:
> $ replicate  : Factor w/ 15 levels "1","2","3","4",..: 1 1 1 1 1 1 1
> 1 1 1 ...
> $ batch      : Factor w/ 3 levels "1","2","3": 1 1 1 1 1 1 2 2 2 2 ...
> $ recipe     : Factor w/ 3 levels "1","2","3": 1 1 1 1 1 1 2 2 2 2 ...
> $ temperature: Ord.factor w/ 6 levels "175"<"185"<"195"<..: 1 2 3 4 5
> 6 1 2 3 4 ...
> $ angle      : int  42 46 47 39 53 42 39 46 51 49 ...
>  > options(contrasts=c("contr.treatment", "contr.poly"))
>  > mod.t <- lmer(angle ~ recipe + (1|batch), data=cake)
>  > options(contrasts=c("contr.sum", "contr.poly"))
>  > mod.s <- lmer(angle ~ recipe + (1|batch), data=cake)
>  > logLik(mod.t)
> 'log Lik.' -947.82 (df=4)
>  > logLik(mod.s)
> 'log Lik.' -948.92 (df=4)


What you are seeing is the log-restricted-likelihood which does depend
upon that parameterization used for the fixed effects, hence on the
contrasts used for a factor.  If you were to fit both models with
method = "ML" then you would get the same value of the log-likelihood
as shown in the enclosed.

The REML criterion does not always behave like a likelihood.  In
particular there is a term in the REML criterion that depends on the
form of the model matrix for the fixed effects.

(By the way, the model fit in your example doesn't make sense.  It
appears that batch and recipe are the same factor and should not be
included as a fixed effect and as a random effect.)
-------------- next part --------------
> getOption("contrasts")
[1] "contr.treatment"  "contr.poly"
> logLik(fm1 <- lmer(angle ~ recipe + temp + (1|recipe:replicate), cake, method = "ML"))
'log Lik.' -848.0789 (df=5)
> options(contrasts = c("contr.sum", "contr.poly"))
> logLik(fm2 <- lmer(angle ~ recipe + temp + (1|recipe:replicate), cake, method = "ML"))
'log Lik.' -848.0789 (df=5)

From bates at stat.wisc.edu  Fri May 11 16:29:03 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 11 May 2007 09:29:03 -0500
Subject: [R-sig-ME] Extracting the coefficients from corStruct objects
In-Reply-To: <87A0C64299B27148B40BE0DB83EDE2DBC6657F@RSMAIL002.REG.SKANE.SE>
References: <464241E1.6060908@iki.fi>
	<87A0C64299B27148B40BE0DB83EDE2DBC6657F@RSMAIL002.REG.SKANE.SE>
Message-ID: <40e66e0b0705110729tb41590k744e4bec0d446da9@mail.gmail.com>

On 5/10/07, Nilsson Fredrik X <Fredrik.X.Nilsson at skane.se> wrote:
> Dear Markus,

> I don't realize why one gets these strange results and I would like to understand this too!

The reason for the strange parameterization is that the nlme package
always uses unconstrained parameters for the optimization.  When the
parameters have natural constraints it is necessary to transform them
to produce unconstrained versions.

> But here's a solution while waiting from the enlightenment:
>
> Look at:
> intervals(ModelwAR1.lme)$corStruct
>
> And then
>
> intervals(ModelwAR1.lme)$corStruct[2]
>
> is what you look for.
>
> Cheers,
>
> Fredrik
>
> -----Ursprungligt meddelande-----
> Fr?n: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] F?r Markus J?ntti
> Skickat: den 9 maj 2007 23:49
> Till: R-SIG-Mixed-Model
> ?mne: [R-sig-ME] Extracting the coefficients from corStruct objects
>
> Dear All:
>
> I have been trying to get the coefficients from corAR1 objects in lme objects in
> then lme package (3.1-80), but coef(obj$modelStruct$corStruct) returns something
> rather different that summary(object). I have tried to follow the various
> summary. methods in the source code, but have kind of gotten lost. Would be
> greatful if someone could point to a simple solution.
>
> It would be helpful to manually extract the AR(1) coefficients, because I am
> data sets consisting of very large cohorts of individuals. I extract the
> information I need from each estimated object and store them in small matrices.
> I have not been able to figure out a way to extract the AR(1) parameters, however.
>
> I include example code below, which produces the following results:
>  > summary(fm2)
> <snip>
> Correlation Structure: AR(1)
>   Formula: ~year | ind
>   Parameter estimate(s):
>    Phi
> 0.424
> <snip>
>
> but
>
>  > coef(fm2$modelStruct$corStruct)
> [1] 0.906
>
> Version info for nlme:
>
>  > packageDescription("nlme")
> Package: nlme
> Version: 3.1-80
> Date: 2007-03-30
> Priority: recommended
> Title: Linear and Nonlinear Mixed Effects Models
> Author: Jose Pinheiro <Jose.Pinheiro at pharma.novartis.com>, Douglas
>          Bates <bates at stat.wisc.edu>, Saikat DebRoy
>          <saikat at stat.wisc.edu>, and Deepayan Sarkar
>          <deepayan at stat.wisc.edu>
> Maintainer: R-core <R-core at R-project.org>
> Description: Fit and compare Gaussian linear and nonlinear
>          mixed-effects models.
> Depends: graphics, stats, R (>= 2.3.0)
> Imports: lattice
> LazyLoad: yes
> LazyData: yes
> License: GPL version 2 or later
> Packaged: Fri Mar 30 07:37:02 2007; ripley
> Built: R 2.5.0; i486-pc-linux-gnu; 2007-04-25 03:13:00; unix
>
> Example code:
> <code>
> library(nlme)
> ## number of time periods
> years <- 5
> ## individuals
> n <- 100
>
> ## the random effects
> a <- as.vector(sapply(rnorm(n), rep, years))
>
> ## indicators for individuals
> ind <- as.vector(sapply(1:n, rep, years))
> year <- rep(1:years, n)
>
> ## the white noise
> v <- rnorm(n*years)
> ## the AR(1) parameter
> rho <- .5
> ## generate the AR1 erroer
> ## randomly draw the starting value of the AR(1) error for each unit
> u1 <- rnorm(n)
> ## store values here:
> u <- numeric(n*years)
> ## a counter:
> k <- 1
> ## there has to be a much more elegant way to do this!
> for(i in 1:n)
>    {
>      u[k] <- u1[i]
>      for(j in 2:years)
>        {
>          k <- k+1
>          u[k] <- rho*u[k-1] + v[k]
>        }
>      k <- k+1
>    }
>
> ## the covariate
> x <- runif(n*years)
> ## the intercept and slope
> b0 <- 10
> b1 <- 4
> ## and the dependent variable
> y <- b0 + b1*x + a + u
>
> test.d <- data.frame(y=y, x=x, ind=ind, year=year)
> rm("x", "y", "ind", "years")
>
> fm1 <- lme(y ~ x, random = ~ 1 | ind,
>             data=test.d)
> summary(fm1)
> fm2 <- lme(y ~ x, random = ~ 1 | ind, correlation=corAR1(form=~year),
>             data=test.d)
> summary(fm2)
> coef(fm2$modelStruct$corStruct)
> </code>
>
> --
> Markus Jantti
> Abo Akademi University
> markus.jantti at iki.fi
> http://www.iki.fi/~mjantti
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From Fredrik.X.Nilsson at skane.se  Fri May 11 16:48:16 2007
From: Fredrik.X.Nilsson at skane.se (Nilsson Fredrik X)
Date: Fri, 11 May 2007 16:48:16 +0200
Subject: [R-sig-ME] Extracting the coefficients from corStruct objects
In-Reply-To: <40e66e0b0705110729tb41590k744e4bec0d446da9@mail.gmail.com>
Message-ID: <87A0C64299B27148B40BE0DB83EDE2DBC6661F@RSMAIL002.REG.SKANE.SE>

> The reason for the strange parameterization is that the nlme package
> always uses unconstrained parameters for the optimization.  When the
> parameters have natural constraints it is necessary to transform them
> to produce unconstrained versions.
>
It would be interesting to know what the transformation is or how to find out. Is it tanh(x/2)?

Best regards,
Fredrik

>> But here's a solution while waiting from the enlightenment:
>>
>> Look at:
>> intervals(ModelwAR1.lme)$corStruct
>>
>> And then
>>
>> intervals(ModelwAR1.lme)$corStruct[2]
>>
>> is what you look for.
>>
>> Cheers,
>>
>> Fredrik
>>
>> -----Ursprungligt meddelande-----
>> Fr?n: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] F?r Markus J?ntti
>> Skickat: den 9 maj 2007 23:49
>> Till: R-SIG-Mixed-Model
>> ?mne: [R-sig-ME] Extracting the coefficients from corStruct objects
>>
>> Dear All:
>>
>> I have been trying to get the coefficients from corAR1 objects in lme objects in
>> then lme package (3.1-80), but coef(obj$modelStruct$corStruct) returns something
>> rather different that summary(object). I have tried to follow the various
>> summary. methods in the source code, but have kind of gotten lost. Would be
>> greatful if someone could point to a simple solution.
>>
>> It would be helpful to manually extract the AR(1) coefficients, because I am
>> data sets consisting of very large cohorts of individuals. I extract the
>> information I need from each estimated object and store them in small matrices.
>> I have not been able to figure out a way to extract the AR(1) parameters, however.
>>
>> I include example code below, which produces the following results:
>>  > summary(fm2)
>> <snip>
>> Correlation Structure: AR(1)
>>   Formula: ~year | ind
>>   Parameter estimate(s):
>>    Phi
>> 0.424
>> <snip>
>>
>> but
>>
>>  > coef(fm2$modelStruct$corStruct)
>> [1] 0.906
>>
>> Version info for nlme:
>>
>>  > packageDescription("nlme")
>> Package: nlme
>> Version: 3.1-80
>> Date: 2007-03-30
>> Priority: recommended
>> Title: Linear and Nonlinear Mixed Effects Models
>> Author: Jose Pinheiro <Jose.Pinheiro at pharma.novartis.com>, Douglas
>>          Bates <bates at stat.wisc.edu>, Saikat DebRoy
>>          <saikat at stat.wisc.edu>, and Deepayan Sarkar
>>          <deepayan at stat.wisc.edu>
>> Maintainer: R-core <R-core at R-project.org>
>> Description: Fit and compare Gaussian linear and nonlinear
>>          mixed-effects models.
>> Depends: graphics, stats, R (>= 2.3.0)
>> Imports: lattice
>> LazyLoad: yes
>> LazyData: yes
>> License: GPL version 2 or later
>> Packaged: Fri Mar 30 07:37:02 2007; ripley
>> Built: R 2.5.0; i486-pc-linux-gnu; 2007-04-25 03:13:00; unix
>>
>> Example code:
>> <code>
>> library(nlme)
>> ## number of time periods
>> years <- 5
>> ## individuals
>> n <- 100
>>
>> ## the random effects
>> a <- as.vector(sapply(rnorm(n), rep, years))
>>
>> ## indicators for individuals
>> ind <- as.vector(sapply(1:n, rep, years))
>> year <- rep(1:years, n)
>>
>> ## the white noise
>> v <- rnorm(n*years)
>> ## the AR(1) parameter
>> rho <- .5
>> ## generate the AR1 erroer
>> ## randomly draw the starting value of the AR(1) error for each unit
>> u1 <- rnorm(n)
>> ## store values here:
>> u <- numeric(n*years)
>> ## a counter:
>> k <- 1
>> ## there has to be a much more elegant way to do this!
>> for(i in 1:n)
>>    {
>>      u[k] <- u1[i]
>>      for(j in 2:years)
>>        {
>>          k <- k+1
>>          u[k] <- rho*u[k-1] + v[k]
>>        }
>>      k <- k+1
>>    }
>>
>> ## the covariate
>> x <- runif(n*years)
>> ## the intercept and slope
>> b0 <- 10
>> b1 <- 4
>> ## and the dependent variable
>> y <- b0 + b1*x + a + u
>>
>> test.d <- data.frame(y=y, x=x, ind=ind, year=year)
>> rm("x", "y", "ind", "years")
>>
>> fm1 <- lme(y ~ x, random = ~ 1 | ind,
>>             data=test.d)
>> summary(fm1)
>> fm2 <- lme(y ~ x, random = ~ 1 | ind, correlation=corAR1(form=~year),
>>             data=test.d)
>> summary(fm2)
>> coef(fm2$modelStruct$corStruct)
>> </code>
>>
>> --
>> Markus Jantti
>> Abo Akademi University
>> markus.jantti at iki.fi
>> http://www.iki.fi/~mjantti
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>



From bates at stat.wisc.edu  Fri May 11 22:16:58 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 11 May 2007 15:16:58 -0500
Subject: [R-sig-ME] Extracting the coefficients from corStruct objects
In-Reply-To: <87A0C64299B27148B40BE0DB83EDE2DBC6661F@RSMAIL002.REG.SKANE.SE>
References: <40e66e0b0705110729tb41590k744e4bec0d446da9@mail.gmail.com>
	<87A0C64299B27148B40BE0DB83EDE2DBC6661F@RSMAIL002.REG.SKANE.SE>
Message-ID: <40e66e0b0705111316k23159a20u90e5566128f77a73@mail.gmail.com>

On 5/11/07, Nilsson Fredrik X <Fredrik.X.Nilsson at skane.se> wrote:
> > The reason for the strange parameterization is that the nlme package
> > always uses unconstrained parameters for the optimization.  When the
> > parameters have natural constraints it is necessary to transform them
> > to produce unconstrained versions.

> It would be interesting to know what the transformation is or how to find out. Is it tanh(x/2)?

Just look at the code for corAR1. The unconstrained parameter is
log((1+r)/(1-r)).

> Best regards,
> Fredrik
>
> >> But here's a solution while waiting from the enlightenment:
> >>
> >> Look at:
> >> intervals(ModelwAR1.lme)$corStruct
> >>
> >> And then
> >>
> >> intervals(ModelwAR1.lme)$corStruct[2]
> >>
> >> is what you look for.
> >>
> >> Cheers,
> >>
> >> Fredrik
> >>
> >> -----Ursprungligt meddelande-----
> >> Fr?n: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] F?r Markus J?ntti
> >> Skickat: den 9 maj 2007 23:49
> >> Till: R-SIG-Mixed-Model
> >> ?mne: [R-sig-ME] Extracting the coefficients from corStruct objects
> >>
> >> Dear All:
> >>
> >> I have been trying to get the coefficients from corAR1 objects in lme objects in
> >> then lme package (3.1-80), but coef(obj$modelStruct$corStruct) returns something
> >> rather different that summary(object). I have tried to follow the various
> >> summary. methods in the source code, but have kind of gotten lost. Would be
> >> greatful if someone could point to a simple solution.
> >>
> >> It would be helpful to manually extract the AR(1) coefficients, because I am
> >> data sets consisting of very large cohorts of individuals. I extract the
> >> information I need from each estimated object and store them in small matrices.
> >> I have not been able to figure out a way to extract the AR(1) parameters, however.
> >>
> >> I include example code below, which produces the following results:
> >>  > summary(fm2)
> >> <snip>
> >> Correlation Structure: AR(1)
> >>   Formula: ~year | ind
> >>   Parameter estimate(s):
> >>    Phi
> >> 0.424
> >> <snip>
> >>
> >> but
> >>
> >>  > coef(fm2$modelStruct$corStruct)
> >> [1] 0.906
> >>
> >> Version info for nlme:
> >>
> >>  > packageDescription("nlme")
> >> Package: nlme
> >> Version: 3.1-80
> >> Date: 2007-03-30
> >> Priority: recommended
> >> Title: Linear and Nonlinear Mixed Effects Models
> >> Author: Jose Pinheiro <Jose.Pinheiro at pharma.novartis.com>, Douglas
> >>          Bates <bates at stat.wisc.edu>, Saikat DebRoy
> >>          <saikat at stat.wisc.edu>, and Deepayan Sarkar
> >>          <deepayan at stat.wisc.edu>
> >> Maintainer: R-core <R-core at R-project.org>
> >> Description: Fit and compare Gaussian linear and nonlinear
> >>          mixed-effects models.
> >> Depends: graphics, stats, R (>= 2.3.0)
> >> Imports: lattice
> >> LazyLoad: yes
> >> LazyData: yes
> >> License: GPL version 2 or later
> >> Packaged: Fri Mar 30 07:37:02 2007; ripley
> >> Built: R 2.5.0; i486-pc-linux-gnu; 2007-04-25 03:13:00; unix
> >>
> >> Example code:
> >> <code>
> >> library(nlme)
> >> ## number of time periods
> >> years <- 5
> >> ## individuals
> >> n <- 100
> >>
> >> ## the random effects
> >> a <- as.vector(sapply(rnorm(n), rep, years))
> >>
> >> ## indicators for individuals
> >> ind <- as.vector(sapply(1:n, rep, years))
> >> year <- rep(1:years, n)
> >>
> >> ## the white noise
> >> v <- rnorm(n*years)
> >> ## the AR(1) parameter
> >> rho <- .5
> >> ## generate the AR1 erroer
> >> ## randomly draw the starting value of the AR(1) error for each unit
> >> u1 <- rnorm(n)
> >> ## store values here:
> >> u <- numeric(n*years)
> >> ## a counter:
> >> k <- 1
> >> ## there has to be a much more elegant way to do this!
> >> for(i in 1:n)
> >>    {
> >>      u[k] <- u1[i]
> >>      for(j in 2:years)
> >>        {
> >>          k <- k+1
> >>          u[k] <- rho*u[k-1] + v[k]
> >>        }
> >>      k <- k+1
> >>    }
> >>
> >> ## the covariate
> >> x <- runif(n*years)
> >> ## the intercept and slope
> >> b0 <- 10
> >> b1 <- 4
> >> ## and the dependent variable
> >> y <- b0 + b1*x + a + u
> >>
> >> test.d <- data.frame(y=y, x=x, ind=ind, year=year)
> >> rm("x", "y", "ind", "years")
> >>
> >> fm1 <- lme(y ~ x, random = ~ 1 | ind,
> >>             data=test.d)
> >> summary(fm1)
> >> fm2 <- lme(y ~ x, random = ~ 1 | ind, correlation=corAR1(form=~year),
> >>             data=test.d)
> >> summary(fm2)
> >> coef(fm2$modelStruct$corStruct)
> >> </code>
> >>
> >> --
> >> Markus Jantti
> >> Abo Akademi University
> >> markus.jantti at iki.fi
> >> http://www.iki.fi/~mjantti
> >>
> >> _______________________________________________
> >> R-sig-mixed-models at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>
> >> _______________________________________________
> >> R-sig-mixed-models at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>
>



From markus.jantti at iki.fi  Sat May 12 10:02:51 2007
From: markus.jantti at iki.fi (=?ISO-8859-1?Q?Markus_J=E4ntti?=)
Date: Sat, 12 May 2007 11:02:51 +0300
Subject: [R-sig-ME] Extracting the coefficients from corStruct objects
In-Reply-To: <40e66e0b0705111316k23159a20u90e5566128f77a73@mail.gmail.com>
References: <40e66e0b0705110729tb41590k744e4bec0d446da9@mail.gmail.com>	
	<87A0C64299B27148B40BE0DB83EDE2DBC6661F@RSMAIL002.REG.SKANE.SE>
	<40e66e0b0705111316k23159a20u90e5566128f77a73@mail.gmail.com>
Message-ID: <464574AB.7040305@iki.fi>

Douglas Bates wrote:
> On 5/11/07, Nilsson Fredrik X <Fredrik.X.Nilsson at skane.se> wrote:
>> > The reason for the strange parameterization is that the nlme package
>> > always uses unconstrained parameters for the optimization.  When the
>> > parameters have natural constraints it is necessary to transform them
>> > to produce unconstrained versions.
> 
>> It would be interesting to know what the transformation is or how to 
>> find out. Is it tanh(x/2)?
> 
> Just look at the code for corAR1. The unconstrained parameter is
> log((1+r)/(1-r)).

So for the AR1, the transform is  "Fishers z-tranform"?

I was trying to figure this out by looking at the summary methods. Embarasingly, 
it never occurred to me to look in the corAR1 code itself.

Thanks for the help to both Fredrik and Doug.

Markus

> 
>> Best regards,
>> Fredrik
>>
>> >> But here's a solution while waiting from the enlightenment:
>> >>
>> >> Look at:
>> >> intervals(ModelwAR1.lme)$corStruct
>> >>
>> >> And then
>> >>
>> >> intervals(ModelwAR1.lme)$corStruct[2]
>> >>
>> >> is what you look for.
>> >>
>> >> Cheers,
>> >>
>> >> Fredrik
>> >>
>> >> -----Ursprungligt meddelande-----
>> >> Fr?n: r-sig-mixed-models-bounces at r-project.org 
>> [mailto:r-sig-mixed-models-bounces at r-project.org] F?r Markus J?ntti
>> >> Skickat: den 9 maj 2007 23:49
>> >> Till: R-SIG-Mixed-Model
>> >> ?mne: [R-sig-ME] Extracting the coefficients from corStruct objects
>> >>
>> >> Dear All:
>> >>
>> >> I have been trying to get the coefficients from corAR1 objects in 
>> lme objects in
>> >> then lme package (3.1-80), but coef(obj$modelStruct$corStruct) 
>> returns something
>> >> rather different that summary(object). I have tried to follow the 
>> various
>> >> summary. methods in the source code, but have kind of gotten lost. 
>> Would be
>> >> greatful if someone could point to a simple solution.
>> >>
>> >> It would be helpful to manually extract the AR(1) coefficients, 
>> because I am
>> >> data sets consisting of very large cohorts of individuals. I 
>> extract the
>> >> information I need from each estimated object and store them in 
>> small matrices.
>> >> I have not been able to figure out a way to extract the AR(1) 
>> parameters, however.
>> >>
>> >> I include example code below, which produces the following results:
>> >>  > summary(fm2)
>> >> <snip>
>> >> Correlation Structure: AR(1)
>> >>   Formula: ~year | ind
>> >>   Parameter estimate(s):
>> >>    Phi
>> >> 0.424
>> >> <snip>
>> >>
>> >> but
>> >>
>> >>  > coef(fm2$modelStruct$corStruct)
>> >> [1] 0.906
>> >>
>> >> Version info for nlme:
>> >>
>> >>  > packageDescription("nlme")
>> >> Package: nlme
>> >> Version: 3.1-80
>> >> Date: 2007-03-30
>> >> Priority: recommended
>> >> Title: Linear and Nonlinear Mixed Effects Models
>> >> Author: Jose Pinheiro <Jose.Pinheiro at pharma.novartis.com>, Douglas
>> >>          Bates <bates at stat.wisc.edu>, Saikat DebRoy
>> >>          <saikat at stat.wisc.edu>, and Deepayan Sarkar
>> >>          <deepayan at stat.wisc.edu>
>> >> Maintainer: R-core <R-core at R-project.org>
>> >> Description: Fit and compare Gaussian linear and nonlinear
>> >>          mixed-effects models.
>> >> Depends: graphics, stats, R (>= 2.3.0)
>> >> Imports: lattice
>> >> LazyLoad: yes
>> >> LazyData: yes
>> >> License: GPL version 2 or later
>> >> Packaged: Fri Mar 30 07:37:02 2007; ripley
>> >> Built: R 2.5.0; i486-pc-linux-gnu; 2007-04-25 03:13:00; unix
>> >>
>> >> Example code:
>> >> <code>
>> >> library(nlme)
>> >> ## number of time periods
>> >> years <- 5
>> >> ## individuals
>> >> n <- 100
>> >>
>> >> ## the random effects
>> >> a <- as.vector(sapply(rnorm(n), rep, years))
>> >>
>> >> ## indicators for individuals
>> >> ind <- as.vector(sapply(1:n, rep, years))
>> >> year <- rep(1:years, n)
>> >>
>> >> ## the white noise
>> >> v <- rnorm(n*years)
>> >> ## the AR(1) parameter
>> >> rho <- .5
>> >> ## generate the AR1 erroer
>> >> ## randomly draw the starting value of the AR(1) error for each unit
>> >> u1 <- rnorm(n)
>> >> ## store values here:
>> >> u <- numeric(n*years)
>> >> ## a counter:
>> >> k <- 1
>> >> ## there has to be a much more elegant way to do this!
>> >> for(i in 1:n)
>> >>    {
>> >>      u[k] <- u1[i]
>> >>      for(j in 2:years)
>> >>        {
>> >>          k <- k+1
>> >>          u[k] <- rho*u[k-1] + v[k]
>> >>        }
>> >>      k <- k+1
>> >>    }
>> >>
>> >> ## the covariate
>> >> x <- runif(n*years)
>> >> ## the intercept and slope
>> >> b0 <- 10
>> >> b1 <- 4
>> >> ## and the dependent variable
>> >> y <- b0 + b1*x + a + u
>> >>
>> >> test.d <- data.frame(y=y, x=x, ind=ind, year=year)
>> >> rm("x", "y", "ind", "years")
>> >>
>> >> fm1 <- lme(y ~ x, random = ~ 1 | ind,
>> >>             data=test.d)
>> >> summary(fm1)
>> >> fm2 <- lme(y ~ x, random = ~ 1 | ind, correlation=corAR1(form=~year),
>> >>             data=test.d)
>> >> summary(fm2)
>> >> coef(fm2$modelStruct$corStruct)
>> >> </code>
>> >>
>> >> --
>> >> Markus Jantti
>> >> Abo Akademi University
>> >> markus.jantti at iki.fi
>> >> http://www.iki.fi/~mjantti
>> >>
>> >> _______________________________________________
>> >> R-sig-mixed-models at r-project.org mailing list
>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >>
>> >> _______________________________________________
>> >> R-sig-mixed-models at r-project.org mailing list
>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >>
>>
> 


-- 
Markus Jantti
Abo Akademi University
markus.jantti at iki.fi
http://www.iki.fi/~mjantti



From parn at nt.ntnu.no  Fri May 18 13:19:04 2007
From: parn at nt.ntnu.no (parn at nt.ntnu.no)
Date: Fri, 18 May 2007 13:19:04 +0200
Subject: [R-sig-ME] post hoc tests following lmer
Message-ID: <1179487144.464d8ba88eba6@webmail.ntnu.no>

Dear lmer-users,

I have struggled to find a good way to perform a post hoc test on the
fixed factors in a mixed model using lmer. I found some threads on the
R-list:
http://www.nabble.com/investigating-interactions-with-mixed-models-tf3272509.html#a9099081
http://www.nabble.com/How-to-use-lmer-function-and-multicomp-package--tf1730665.html#a4702360

...but I am really curious if anyone have any additional thoughts on
this subject.

I provide you some dummy data of 200 bird nests.
The male and female attending the nest could be either young or old.
Each chick in a clutch could either have a disease or not.
Some individuals occur in the data set several times and 'identity' of a
bird is included as a random variable.

I would like to test if the proportion of chicks that carries the
disease in a clutch depend on age of the male and female, and the
interaction thereof.
And (hopefully) be able to perform a posthoc test of which
age-combination of the parents that differs in proportion of sick
chicks.


# Dummy data

# male age
male.age <- rep(c("Y", "O"), c(100, 100))

# female age
female.age <- rep(c("Y", "O", "Y", "O"), c(50, 50, 50, 50))

# male id
# 100 unique young males
male.id.y <- (1:100)

# 70 unique old males, and 30 of the old captured before
male.id.o <- sample(c(sample(male.id.y, 30, replace=F), 101:170))
male.id <- c(male.id.y, male.id.o)

# female id
female.id.y <- 1:100
female.id.o <- sample(c(sample(female.id.y, 40, replace=F), 101:160))
female.id <- c(female.id.y[1:50], female.id.o[1:50],
female.id.y[51:100], female.id.o[51:100])

# clutch size
clutch <- floor(rnorm(200, mean=9, sd=1.35))

# number of chicks with disease in pairs with of a young male (ym) and a
# young female (yf):
n.epo.ym.yf <- rbinom(50, clutch[1:50], p=0.09)

# and so on (different probabilities of disease in each combination of
male and female age.
n.epo.ym.of <- rbinom(50, clutch[51:100], p=0.25)
n.epo.om.yf <- rbinom(50, clutch[101:150], p=0.075)
n.epo.om.of <- rbinom(50, clutch[151:200], p=0.06)

n.epo <- c(n.epo.ym.yf, n.epo.ym.of, n.epo.om.yf, n.epo.om.of)

# number of healthy offspring
n.wpo <- clutch - n.epo


ep.data <- data.frame(male.id, female.id, male.age, female.age, clutch,
n.epo, n.wpo)


# create response variable
# two-column matrix with the columns giving the numbers of 'successes'
# (n.epo) and 'failures' (n.wpo)

y <- cbind(n.epo, n.wpo)


# plot proportion of epo in the clutch for the
# different age combinations in pairs

p.epo <- n.epo/clutch
pairage <- as.factor(paste(male.age, female.age, sep=""))
plot(p.epo ~ pairage)



# mixed model.
# response: proportion epo in each clutch: y
# fixed factors: male age, female age and the interaction thereof:
# male.age*female.age
# random factors: male.id, female.id


model <- lmer(y ~ male.age*female.age + (1|male.id) + (1|female.id),
family=binomial, data=ep.data)
summary(model)


But what would be the appropriate way to make a post hoc test of which
age combinations that differs, an lmer-equivalent of
'multicomp', perhaps an mcmcsamp-approach!?


Thanks in advance for any comments!

Best regards,

Henrik

--



From stevenmh at muohio.edu  Fri May 18 14:38:21 2007
From: stevenmh at muohio.edu (Martin Henry H. Stevens)
Date: Fri, 18 May 2007 08:38:21 -0400
Subject: [R-sig-ME] post hoc tests following lmer
In-Reply-To: <1179487144.464d8ba88eba6@webmail.ntnu.no>
References: <1179487144.464d8ba88eba6@webmail.ntnu.no>
Message-ID: <D418B330-BD82-49C5-B9A1-6833EA5673DC@muohio.edu>

Hi Henrik,
See the languageR package, and a paper by Baayen, Davidson and Bates  
(submitted), at Baayen's web site. This may be helpful, as it gives a  
couple more bells-and-whistles.

Also, if I understand you correctly, you have two ages each of female  
and male? How about creating no-intercept model, generating Bayesian  
credible intervals with mcmcsamp and HPDintervals and then comparing  
then comparing the four combinations that way? I would be interested  
to here the thoughts of others on this.
Hank
On May 18, 2007, at 7:19 AM, parn at nt.ntnu.no wrote:

> Dear lmer-users,
>
> I have struggled to find a good way to perform a post hoc test on the
> fixed factors in a mixed model using lmer. I found some threads on the
> R-list:
> http://www.nabble.com/investigating-interactions-with-mixed-models- 
> tf3272509.html#a9099081
> http://www.nabble.com/How-to-use-lmer-function-and-multicomp- 
> package--tf1730665.html#a4702360
>
> ...but I am really curious if anyone have any additional thoughts on
> this subject.
>
> I provide you some dummy data of 200 bird nests.
> The male and female attending the nest could be either young or old.
> Each chick in a clutch could either have a disease or not.
> Some individuals occur in the data set several times and 'identity'  
> of a
> bird is included as a random variable.
>
> I would like to test if the proportion of chicks that carries the
> disease in a clutch depend on age of the male and female, and the
> interaction thereof.
> And (hopefully) be able to perform a posthoc test of which
> age-combination of the parents that differs in proportion of sick
> chicks.
>
>
> # Dummy data
>
> # male age
> male.age <- rep(c("Y", "O"), c(100, 100))
>
> # female age
> female.age <- rep(c("Y", "O", "Y", "O"), c(50, 50, 50, 50))
>
> # male id
> # 100 unique young males
> male.id.y <- (1:100)
>
> # 70 unique old males, and 30 of the old captured before
> male.id.o <- sample(c(sample(male.id.y, 30, replace=F), 101:170))
> male.id <- c(male.id.y, male.id.o)
>
> # female id
> female.id.y <- 1:100
> female.id.o <- sample(c(sample(female.id.y, 40, replace=F), 101:160))
> female.id <- c(female.id.y[1:50], female.id.o[1:50],
> female.id.y[51:100], female.id.o[51:100])
>
> # clutch size
> clutch <- floor(rnorm(200, mean=9, sd=1.35))
>
> # number of chicks with disease in pairs with of a young male (ym)  
> and a
> # young female (yf):
> n.epo.ym.yf <- rbinom(50, clutch[1:50], p=0.09)
>
> # and so on (different probabilities of disease in each combination of
> male and female age.
> n.epo.ym.of <- rbinom(50, clutch[51:100], p=0.25)
> n.epo.om.yf <- rbinom(50, clutch[101:150], p=0.075)
> n.epo.om.of <- rbinom(50, clutch[151:200], p=0.06)
>
> n.epo <- c(n.epo.ym.yf, n.epo.ym.of, n.epo.om.yf, n.epo.om.of)
>
> # number of healthy offspring
> n.wpo <- clutch - n.epo
>
>
> ep.data <- data.frame(male.id, female.id, male.age, female.age,  
> clutch,
> n.epo, n.wpo)
>
>
> # create response variable
> # two-column matrix with the columns giving the numbers of 'successes'
> # (n.epo) and 'failures' (n.wpo)
>
> y <- cbind(n.epo, n.wpo)
>
>
> # plot proportion of epo in the clutch for the
> # different age combinations in pairs
>
> p.epo <- n.epo/clutch
> pairage <- as.factor(paste(male.age, female.age, sep=""))
> plot(p.epo ~ pairage)
>
>
>
> # mixed model.
> # response: proportion epo in each clutch: y
> # fixed factors: male age, female age and the interaction thereof:
> # male.age*female.age
> # random factors: male.id, female.id
>
>
> model <- lmer(y ~ male.age*female.age + (1|male.id) + (1|female.id),
> family=binomial, data=ep.data)
> summary(model)
>
>
> But what would be the appropriate way to make a post hoc test of which
> age combinations that differs, an lmer-equivalent of
> 'multicomp', perhaps an mcmcsamp-approach!?
>
>
> Thanks in advance for any comments!
>
> Best regards,
>
> Henrik
>
> --
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From Jonathan.Bartlett at lshtm.ac.uk  Tue May 22 19:13:44 2007
From: Jonathan.Bartlett at lshtm.ac.uk (Jonathan Bartlett)
Date: Tue, 22 May 2007 18:13:44 +0100
Subject: [R-sig-ME] Different variance estimates from lmer and lmer2
Message-ID: <465332D7.AD5C.0057.0@lshtm.ac.uk>

Dear all

For some mixed models, I get different variance estimates if I use lmer
compared to using lmer2. Is there a reason why the two commands are
giving quite different estimates?

The analysis below is from page 168 of "Extending the linear model with
R" by Faraway, using the dataframe irrigation from the faraway package.
Strangely, the results using lmer2 agree with the book, whereas lmer
gives slightly different estimates.

I have am running R 2.5.0 with lme4 version 0.99875-0 on WinXP.

lmod <- lmer(yield ~ irrigation * variety + (1|field),data=irrigation)
> summary(lmod)
Linear mixed-effects model fit by REML 
Formula: yield ~ irrigation * variety + (1 | field) 
   Data: irrigation 
  AIC   BIC logLik MLdeviance REMLdeviance
 63.4 70.35  -22.7      68.62         45.4
Random effects:
 Groups   Name        Variance Std.Dev.
 field    (Intercept) 15.5182  3.9393  
 Residual              2.1919  1.4805  
number of obs: 16, groups: field, 8

....

> lmod <- lmer2(yield ~ irrigation * variety +
(1|field),data=irrigation)
> summary(lmod)
Linear mixed-effects model fit by REML 
Formula: yield ~ irrigation * variety + (1 | field) 
   Data: irrigation 
  AIC   BIC logLik MLdeviance REMLdeviance
 63.4 70.35 -22.70      68.61        45.39
Random effects:
 Groups   Name        Variance Std.Dev.
 field    (Intercept) 16.1991  4.0248  
 Residual              2.1076  1.4518  
Number of obs: 16, groups: field, 8

lmer2, the book and Stata's xtmixed give the estimate of the random
intercept SD as 4.02, whereas lmer gives it as 3.94.

My apologies if the reason for such differences is down to a mistake on
my part - I was unable to find any postings on the list regarding this
issue.

Many thanks
Jonathan
London School of Hygiene and Tropical Medicine
www.lshtm.ac.uk



From bates at stat.wisc.edu  Tue May 22 22:25:51 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 22 May 2007 15:25:51 -0500
Subject: [R-sig-ME] Different variance estimates from lmer and lmer2
In-Reply-To: <465332D7.AD5C.0057.0@lshtm.ac.uk>
References: <465332D7.AD5C.0057.0@lshtm.ac.uk>
Message-ID: <40e66e0b0705221325m4b30df33t99759cab1d11acdb@mail.gmail.com>

On 5/22/07, Jonathan Bartlett <Jonathan.Bartlett at lshtm.ac.uk> wrote:


> For some mixed models, I get different variance estimates if I use lmer
> compared to using lmer2. Is there a reason why the two commands are
> giving quite different estimates?

> The analysis below is from page 168 of "Extending the linear model with
> R" by Faraway, using the dataframe irrigation from the faraway package.
> Strangely, the results using lmer2 agree with the book, whereas lmer
> gives slightly different estimates.

> I have am running R 2.5.0 with lme4 version 0.99875-0 on WinXP.

> lmod <- lmer(yield ~ irrigation * variety + (1|field),data=irrigation)
> > summary(lmod)
> Linear mixed-effects model fit by REML
> Formula: yield ~ irrigation * variety + (1 | field)
>    Data: irrigation
>   AIC   BIC logLik MLdeviance REMLdeviance
>  63.4 70.35  -22.7      68.62         45.4
> Random effects:
>  Groups   Name        Variance Std.Dev.
>  field    (Intercept) 15.5182  3.9393
>  Residual              2.1919  1.4805
> number of obs: 16, groups: field, 8
>
> ....
>
> > lmod <- lmer2(yield ~ irrigation * variety +
> (1|field),data=irrigation)
> > summary(lmod)
> Linear mixed-effects model fit by REML
> Formula: yield ~ irrigation * variety + (1 | field)
>    Data: irrigation
>   AIC   BIC logLik MLdeviance REMLdeviance
>  63.4 70.35 -22.70      68.61        45.39
> Random effects:
>  Groups   Name        Variance Std.Dev.
>  field    (Intercept) 16.1991  4.0248
>  Residual              2.1076  1.4518
> Number of obs: 16, groups: field, 8

These differences are just reflecting different parameterizations and
different convergence criteria for lmer and lmer2.  Notice that the
REML deviance for the model fit by lmer2 is slightly smaller than that
fit by lmer (45.39 vs 45.4).  If you add the optional argument control
= list(msVerbose = 1) to the call to lmer and to lmer2 you will see
that lmer2 takes more iterations and, as shown above, produces a
slightly better criterion for the fit.

Having said all this, I would note that a difference of 0.01 in the
deviance is not going to be in any way significant.  Essentially what
all this is indicating is that the estimates of the variance
components are not very precisely determined.

It was probably after Julian fitted the models for his book that I
made the change in lmer to loosen the convergence criterion in lmer
somewhat.  In retrospect that may not have been a good idea.

> lmer2, the book and Stata's xtmixed give the estimate of the random
> intercept SD as 4.02, whereas lmer gives it as 3.94.
>
> My apologies if the reason for such differences is down to a mistake on
> my part - I was unable to find any postings on the list regarding this
> issue.
>
> Many thanks
> Jonathan
> London School of Hygiene and Tropical Medicine
> www.lshtm.ac.uk
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From sundar.dorai-raj at pdf.com  Tue May 22 22:56:38 2007
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 22 May 2007 13:56:38 -0700
Subject: [R-sig-ME] incorrect weights type in lmer
Message-ID: <46535906.7060405@pdf.com>

Hi, all,

I'm receiving an error I don't understand regarding a weights argument 
to lmer. Here's an example:

library(lme4)
set.seed(1)
tmp.df <- expand.grid(A = factor(1:3), B = 1:3, C = 1:10)
tmp.df$w <- as.integer(rpois(nrow(tmp.df), 50))
tmp.df$p <- 0.5 + with(tmp.df, ifelse(B%%2, -0.1, 0.1))
tmp.df$p <- with(tmp.df, p + ifelse(B%%2 & C%%2, -0.1, 0.1))
tmp.df$y <- rbinom(nrow(tmp.df), tmp.df$w, tmp.df$p)/tmp.df$w
tmp.df[2:3] <- lapply(tmp.df[2:3], factor)
fit <- lmer(y ~ A + (1 | B) + (1 | B:C), tmp.df, binomial, weights = w)

Error in lmer(y ~ A + (1 | B) + (1 | B:C), tmp.df, binomial, weights = w) :
	object `weights' of incorrect type

What sort of object is lmer expecting for weights?


 > sessionInfo()
R version 2.5.0 (2007-04-23)
i386-pc-mingw32

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United 
States.1252;LC_MONETARY=English_United 
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
[7] "base"

other attached packages:
        lme4      Matrix     lattice
"0.99875-0" "0.99875-1"    "0.15-6"

Thanks,

--sundar



From bates at stat.wisc.edu  Tue May 22 23:44:08 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 22 May 2007 16:44:08 -0500
Subject: [R-sig-ME] incorrect weights type in lmer
In-Reply-To: <46535906.7060405@pdf.com>
References: <46535906.7060405@pdf.com>
Message-ID: <40e66e0b0705221444l51cd5341r2664f81b87c60bb3@mail.gmail.com>

On 5/22/07, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
> Hi, all,
>
> I'm receiving an error I don't understand regarding a weights argument
> to lmer. Here's an example:
>
> library(lme4)
> set.seed(1)
> tmp.df <- expand.grid(A = factor(1:3), B = 1:3, C = 1:10)
> tmp.df$w <- as.integer(rpois(nrow(tmp.df), 50))
> tmp.df$p <- 0.5 + with(tmp.df, ifelse(B%%2, -0.1, 0.1))
> tmp.df$p <- with(tmp.df, p + ifelse(B%%2 & C%%2, -0.1, 0.1))
> tmp.df$y <- rbinom(nrow(tmp.df), tmp.df$w, tmp.df$p)/tmp.df$w
> tmp.df[2:3] <- lapply(tmp.df[2:3], factor)
> fit <- lmer(y ~ A + (1 | B) + (1 | B:C), tmp.df, binomial, weights = w)
>
> Error in lmer(y ~ A + (1 | B) + (1 | B:C), tmp.df, binomial, weights = w) :
>         object `weights' of incorrect type
>
> What sort of object is lmer expecting for weights?

Thanks for pointing out the problem, Sundar.  Although this is not
documented the weights need to be double precision and you are using
integers.  Your example works with weights = as.double(w) or weights =
as.numeric(w).

Most of the time R will quietly coerce integers to numeric as needed
but not in this case.



From sundar.dorai-raj at pdf.com  Wed May 23 03:15:11 2007
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 22 May 2007 18:15:11 -0700
Subject: [R-sig-ME] Poor convergence when using binomial response
Message-ID: <4653959F.1010908@pdf.com>

Hi,

I'm seeing poor convergence of a model using the binomial family. In one 
case, I have a binomial response for which I use the 'weights' argument. 
In a second case, I expand the binomial response to Bernoulli trials and 
refit. Theoretically, they should be the same model. However, that does 
not appear to be the case because the residual scale for the binomial 
case is unbelievably large. Could someone (Prof. Bates) please help 
diagnose this problem?

And thank you so much for such wonderful software. I truly appreciate 
the effort.

Thanks,

--sundar

tmp <- read.csv(url("http://sdorairaj.googlepages.com/tmp.csv"))
tmp[2:4] <- lapply(tmp[2:4], factor)
library(lme4)
fit <- lmer(1 - y ~ z + (1 | A) + (1 | A:B), tmp,
             binomial("cloglog"), weights = as.numeric(w))
## see below for expand.bin
tmp2 <- expand.bin(tmp, "x", "w")
fit2 <- lmer(x ~ z + (1 | A) + (1 | A:B), tmp2, binomial("cloglog"))

fit3 <- glm(1 - y ~ z, binomial("cloglog"), tmp, w)
fit4 <- glm(x ~ z, binomial("cloglog"), tmp2)

cbind(sapply(list(fit, fit2), fixef),
       sapply(list(fit3, fit4), coef))
#                  [,1]       [,2]       [,3]       [,4]
#(Intercept) -2.8227711 -3.1247457 -2.8227711 -2.8227711
#z2          -0.2680992 -0.2694113 -0.2680992 -0.2680992
#z3           0.3109447  0.3233432  0.3109447  0.3109447


expand.bin <- function (data, x, n) {
   char.x <- x
   char.n <- n
   x <- data[[x]]
   n <- data[[n]]
   i <- rep(seq(nrow(data)), n)
   data <- data[i, , drop = FALSE]
   expand <- function(z) c(rep(0, diff(z)), rep(1, z[1]))
   x <- apply(cbind(x, n), 1, expand)
   data[[char.x]] <- if(is.matrix(x)) c(x) else unlist(x)
   row.names(data) <- seq(nrow(data))
   data
}

 > sessionInfo()
R version 2.5.0 (2007-04-23)
i386-pc-mingw32

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United 
States.1252;LC_MONETARY=English_United 
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
[7] "base"

other attached packages:
        lme4      Matrix     lattice
"0.99875-0" "0.99875-1"    "0.15-6"



From bates at stat.wisc.edu  Wed May 23 18:49:05 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 23 May 2007 11:49:05 -0500
Subject: [R-sig-ME] Poor convergence when using binomial response
In-Reply-To: <4653959F.1010908@pdf.com>
References: <4653959F.1010908@pdf.com>
Message-ID: <40e66e0b0705230949q2da18af2y6aefac4de03accfd@mail.gmail.com>

Thanks for sending the example, Sundar, and also for the idea of the
expand.bin function.

There is a similar example in the lme4 package using the cbpp data
set.  I modified your expand.bin function to apply to that example
with the enclosed results.  As you can see the parameter estimates in
this example are not exactly the same for the "count out of" form
versus the "expanded binary responses" form but they are reasonably
close.  In general I would recommend using cbind(successes, failures)
as the response rather than messing around with the weights.  I always
manage to confuse myself about what the weights should be and need to
go back to the cbind(successes, failures) form to check.

As always, it is a good idea to set control = list(msVerbose = 1) to
see exactly what is going on with the parameter estimates during the
iterations.

I hope this helps.  If necessary I will look in more detail at your
example but I hope this is enough to get you started.


On 5/22/07, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
> Hi,
>
> I'm seeing poor convergence of a model using the binomial family. In one
> case, I have a binomial response for which I use the 'weights' argument.
> In a second case, I expand the binomial response to Bernoulli trials and
> refit. Theoretically, they should be the same model. However, that does
> not appear to be the case because the residual scale for the binomial
> case is unbelievably large. Could someone (Prof. Bates) please help
> diagnose this problem?
>
> And thank you so much for such wonderful software. I truly appreciate
> the effort.
>
> Thanks,
>
> --sundar
>
> tmp <- read.csv(url("http://sdorairaj.googlepages.com/tmp.csv"))
> tmp[2:4] <- lapply(tmp[2:4], factor)
> library(lme4)
> fit <- lmer(1 - y ~ z + (1 | A) + (1 | A:B), tmp,
>              binomial("cloglog"), weights = as.numeric(w))
> ## see below for expand.bin
> tmp2 <- expand.bin(tmp, "x", "w")
> fit2 <- lmer(x ~ z + (1 | A) + (1 | A:B), tmp2, binomial("cloglog"))
>
> fit3 <- glm(1 - y ~ z, binomial("cloglog"), tmp, w)
> fit4 <- glm(x ~ z, binomial("cloglog"), tmp2)
>
> cbind(sapply(list(fit, fit2), fixef),
>        sapply(list(fit3, fit4), coef))
> #                  [,1]       [,2]       [,3]       [,4]
> #(Intercept) -2.8227711 -3.1247457 -2.8227711 -2.8227711
> #z2          -0.2680992 -0.2694113 -0.2680992 -0.2680992
> #z3           0.3109447  0.3233432  0.3109447  0.3109447
>
>
> expand.bin <- function (data, x, n) {
>    char.x <- x
>    char.n <- n
>    x <- data[[x]]
>    n <- data[[n]]
>    i <- rep(seq(nrow(data)), n)
>    data <- data[i, , drop = FALSE]
>    expand <- function(z) c(rep(0, diff(z)), rep(1, z[1]))
>    x <- apply(cbind(x, n), 1, expand)
>    data[[char.x]] <- if(is.matrix(x)) c(x) else unlist(x)
>    row.names(data) <- seq(nrow(data))
>    data
> }
>
>  > sessionInfo()
> R version 2.5.0 (2007-04-23)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
> States.1252;LC_MONETARY=English_United
> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
>
> attached base packages:
> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
> [7] "base"
>
> other attached packages:
>         lme4      Matrix     lattice
> "0.99875-0" "0.99875-1"    "0.15-6"
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
-------------- next part --------------
expand.bin <- function(data, x, n)
{
    stopifnot(is.character(x), is.character(n),
              length(x) == 1, length(n) == 1,
              inherits(data, "data.frame"),
              all(c(x, n) %in% names(data)))
    nn <- as.integer(data[[n]])
    ans <- data[rep.int(seq_along(nn), nn), ]
    ans[[n]] <- NULL
    xx <- as.integer(data[[x]])
    ans[[x]] <- rep.int(rep.int(c(0,1), length(xx)),
                        as.vector(matrix(c(nn - xx, xx), nrow = 2, byrow = TRUE)))
    row.names(ans) <- seq(nrow(ans))
    ans
}
options(show.signif.stars = FALSE)
library(lme4)
example(cbpp)
m3 <- lmer(incidence ~ period + (1|herd),
           expand.bin(cbpp, "incidence", "size"), binomial)
m3
-------------- next part --------------

R version 2.5.0 Patched (2007-05-23 r41687)
Copyright (C) 2007 The R Foundation for Statistical Computing
ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

[Previously saved workspace restored]

> expand.bin <- function(data, x, n)
+ {
+     stopifnot(is.character(x), is.character(n),
+               length(x) == 1, length(n) == 1,
+               inherits(data, "data.frame"),
+               all(c(x, n) %in% names(data)))
+     nn <- as.integer(data[[n]])
+     ans <- data[rep.int(seq_along(nn), nn), ]
+     ans[[n]] <- NULL
+     xx <- as.integer(data[[x]])
+     ans[[x]] <- rep.int(rep.int(c(0,1), length(xx)),
+                         as.vector(matrix(c(nn - xx, xx), nrow = 2, byrow = TRUE)))
+     row.names(ans) <- seq(nrow(ans))
+     ans
+ }
> options(show.signif.stars = FALSE)
> library(lme4)
Loading required package: Matrix
Loading required package: lattice
> example(cbpp)

cbpp> ## response as a matrix
cbpp> (m1 <- lmer(cbind(incidence, size - incidence) ~ period + (1 | herd),
cbpp+             family = binomial, data = cbpp))
Generalized linear mixed model fit using Laplace 
Formula: cbind(incidence, size - incidence) ~ period + (1 | herd) 
   Data: cbpp 
 Family: binomial(logit link)
   AIC   BIC logLik deviance
 110.1 120.2 -50.05    100.1
Random effects:
 Groups Name        Variance Std.Dev.
 herd   (Intercept) 0.41804  0.64656 
number of obs: 56, groups: herd, 15

Estimated scale (compare to  1 )  1.138075 

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)  -1.3981     0.2287  -6.113 9.76e-10
period2      -0.9959     0.3056  -3.259 0.001119
period3      -1.1350     0.3266  -3.475 0.000510
period4      -1.5798     0.4286  -3.686 0.000228

Correlation of Fixed Effects:
        (Intr) perid2 perid3
period2 -0.350              
period3 -0.327  0.267       
period4 -0.248  0.202  0.186

cbpp> ## response as a vector of probabilities and usage of argument "weights"
cbpp> m2 <- lmer(incidence / size ~ period + (1 | herd), weights = size,
cbpp+            family = binomial, data = cbpp)

cbpp> ## Confirm that these are equivalent:
cbpp> stopifnot(all.equal(coef(m1), coef(m2)),
cbpp+           all.equal(ranef(m1), ranef(m2)))
> m3 <- lmer(incidence ~ period + (1|herd),
+            expand.bin(cbpp, "incidence", "size"), binomial)
> m3
Generalized linear mixed model fit using Laplace 
Formula: incidence ~ period + (1 | herd) 
   Data: expand.bin(cbpp, "incidence", "size") 
 Family: binomial(logit link)
 AIC   BIC logLik deviance
 565 588.7 -277.5      555
Random effects:
 Groups Name        Variance Std.Dev.
 herd   (Intercept) 0.41448  0.6438  
number of obs: 842, groups: herd, 15

Estimated scale (compare to  1 )  0.9832878 

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)  -1.3984     0.2282  -6.129 8.86e-10
period2      -0.9934     0.3054  -3.253 0.001144
period3      -1.1332     0.3264  -3.471 0.000518
period4      -1.5805     0.4287  -3.686 0.000227

Correlation of Fixed Effects:
        (Intr) perid2 perid3
period2 -0.351              
period3 -0.328  0.267       
period4 -0.248  0.202  0.186
> 
> proc.time()
   user  system elapsed 
  7.208   0.120   7.322 

From sundar.dorai-raj at pdf.com  Wed May 23 21:17:15 2007
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 23 May 2007 12:17:15 -0700
Subject: [R-sig-ME] Poor convergence when using binomial response
In-Reply-To: <40e66e0b0705230949q2da18af2y6aefac4de03accfd@mail.gmail.com>
References: <4653959F.1010908@pdf.com>
	<40e66e0b0705230949q2da18af2y6aefac4de03accfd@mail.gmail.com>
Message-ID: <4654933B.3070908@pdf.com>

Thanks for the quick reply. Here's an interesting twist.

 > # use cbind(failures, successes) which is appropriate
 > # with the 'cloglog' link but does not converge
 > fit <- lmer(cbind(x, w - x) ~ z + (1 | A) + (1 | A:B), tmp,
+             binomial("cloglog"), control = list(msVerbose = 1))
relative tolerance set to 5.56268464626788e-311
 > # now use cbind(sucesses, failures) which is
 > # inappropriate with the 'cloglog' link but does converge
 > fit <- lmer(cbind(w - x, x) ~ z + (1 | A) + (1 | A:B), tmp,
+             binomial("cloglog"), control = list(msVerbose = 1))
relative tolerance set to 1.23153502616496e-05
   0:     811.99477:  1.04814 0.0876112 -0.111220 0.888889 0.110250
...
  36:     702.77184:  1.14326 0.0930921 -0.150606 0.0958980 0.0605783
 >

So it seems the tolerance level is too small and no steps are taken in 
nlminb. Using debug the problem seems to be in devLaplace.

Browse[1]> PQLpars
(Intercept)          z2          z3
  -2.8227711  -0.2680992   0.3109447   0.8888889   0.1102498
Browse[1]> devLaplace(PQLpars)
[1] 1.797693e+308
Browse[1]> abs(0.01/devLaplace(PQLpars))
[1] 5.562685e-311

Also, this isn't an urgent issue for me. I'm satisfied using the 
expand.bin (thanks also for the modification) to do the analysis.

Thanks again,

--sundar

Douglas Bates said the following on 5/23/2007 9:49 AM:
> Thanks for sending the example, Sundar, and also for the idea of the
> expand.bin function.
> 
> There is a similar example in the lme4 package using the cbpp data
> set.  I modified your expand.bin function to apply to that example
> with the enclosed results.  As you can see the parameter estimates in
> this example are not exactly the same for the "count out of" form
> versus the "expanded binary responses" form but they are reasonably
> close.  In general I would recommend using cbind(successes, failures)
> as the response rather than messing around with the weights.  I always
> manage to confuse myself about what the weights should be and need to
> go back to the cbind(successes, failures) form to check.
> 
> As always, it is a good idea to set control = list(msVerbose = 1) to
> see exactly what is going on with the parameter estimates during the
> iterations.
> 
> I hope this helps.  If necessary I will look in more detail at your
> example but I hope this is enough to get you started.
> 
> 
> On 5/22/07, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
>> Hi,
>>
>> I'm seeing poor convergence of a model using the binomial family. In one
>> case, I have a binomial response for which I use the 'weights' argument.
>> In a second case, I expand the binomial response to Bernoulli trials and
>> refit. Theoretically, they should be the same model. However, that does
>> not appear to be the case because the residual scale for the binomial
>> case is unbelievably large. Could someone (Prof. Bates) please help
>> diagnose this problem?
>>
>> And thank you so much for such wonderful software. I truly appreciate
>> the effort.
>>
>> Thanks,
>>
>> --sundar
>>
>> tmp <- read.csv(url("http://sdorairaj.googlepages.com/tmp.csv"))
>> tmp[2:4] <- lapply(tmp[2:4], factor)
>> library(lme4)
>> fit <- lmer(1 - y ~ z + (1 | A) + (1 | A:B), tmp,
>>              binomial("cloglog"), weights = as.numeric(w))
>> ## see below for expand.bin
>> tmp2 <- expand.bin(tmp, "x", "w")
>> fit2 <- lmer(x ~ z + (1 | A) + (1 | A:B), tmp2, binomial("cloglog"))
>>
>> fit3 <- glm(1 - y ~ z, binomial("cloglog"), tmp, w)
>> fit4 <- glm(x ~ z, binomial("cloglog"), tmp2)
>>
>> cbind(sapply(list(fit, fit2), fixef),
>>        sapply(list(fit3, fit4), coef))
>> #                  [,1]       [,2]       [,3]       [,4]
>> #(Intercept) -2.8227711 -3.1247457 -2.8227711 -2.8227711
>> #z2          -0.2680992 -0.2694113 -0.2680992 -0.2680992
>> #z3           0.3109447  0.3233432  0.3109447  0.3109447
>>
>>
>> expand.bin <- function (data, x, n) {
>>    char.x <- x
>>    char.n <- n
>>    x <- data[[x]]
>>    n <- data[[n]]
>>    i <- rep(seq(nrow(data)), n)
>>    data <- data[i, , drop = FALSE]
>>    expand <- function(z) c(rep(0, diff(z)), rep(1, z[1]))
>>    x <- apply(cbind(x, n), 1, expand)
>>    data[[char.x]] <- if(is.matrix(x)) c(x) else unlist(x)
>>    row.names(data) <- seq(nrow(data))
>>    data
>> }
>>
>>  > sessionInfo()
>> R version 2.5.0 (2007-04-23)
>> i386-pc-mingw32
>>
>> locale:
>> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
>> States.1252;LC_MONETARY=English_United
>> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
>>
>> attached base packages:
>> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
>> [7] "base"
>>
>> other attached packages:
>>         lme4      Matrix     lattice
>> "0.99875-0" "0.99875-1"    "0.15-6"
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
> 
> ------------------------------------------------------------------------
> 
> expand.bin <- function(data, x, n)
> {
>     stopifnot(is.character(x), is.character(n),
>               length(x) == 1, length(n) == 1,
>               inherits(data, "data.frame"),
>               all(c(x, n) %in% names(data)))
>     nn <- as.integer(data[[n]])
>     ans <- data[rep.int(seq_along(nn), nn), ]
>     ans[[n]] <- NULL
>     xx <- as.integer(data[[x]])
>     ans[[x]] <- rep.int(rep.int(c(0,1), length(xx)),
>                         as.vector(matrix(c(nn - xx, xx), nrow = 2, byrow = TRUE)))
>     row.names(ans) <- seq(nrow(ans))
>     ans
> }
> options(show.signif.stars = FALSE)
> library(lme4)
> example(cbpp)
> m3 <- lmer(incidence ~ period + (1|herd),
>            expand.bin(cbpp, "incidence", "size"), binomial)
> m3
> 
> 
> ------------------------------------------------------------------------
> 
> 
> R version 2.5.0 Patched (2007-05-23 r41687)
> Copyright (C) 2007 The R Foundation for Statistical Computing
> ISBN 3-900051-07-0
> 
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under certain conditions.
> Type 'license()' or 'licence()' for distribution details.
> 
>   Natural language support but running in an English locale
> 
> R is a collaborative project with many contributors.
> Type 'contributors()' for more information and
> 'citation()' on how to cite R or R packages in publications.
> 
> Type 'demo()' for some demos, 'help()' for on-line help, or
> 'help.start()' for an HTML browser interface to help.
> Type 'q()' to quit R.
> 
> [Previously saved workspace restored]
> 
>> expand.bin <- function(data, x, n)
> + {
> +     stopifnot(is.character(x), is.character(n),
> +               length(x) == 1, length(n) == 1,
> +               inherits(data, "data.frame"),
> +               all(c(x, n) %in% names(data)))
> +     nn <- as.integer(data[[n]])
> +     ans <- data[rep.int(seq_along(nn), nn), ]
> +     ans[[n]] <- NULL
> +     xx <- as.integer(data[[x]])
> +     ans[[x]] <- rep.int(rep.int(c(0,1), length(xx)),
> +                         as.vector(matrix(c(nn - xx, xx), nrow = 2, byrow = TRUE)))
> +     row.names(ans) <- seq(nrow(ans))
> +     ans
> + }
>> options(show.signif.stars = FALSE)
>> library(lme4)
> Loading required package: Matrix
> Loading required package: lattice
>> example(cbpp)
> 
> cbpp> ## response as a matrix
> cbpp> (m1 <- lmer(cbind(incidence, size - incidence) ~ period + (1 | herd),
> cbpp+             family = binomial, data = cbpp))
> Generalized linear mixed model fit using Laplace 
> Formula: cbind(incidence, size - incidence) ~ period + (1 | herd) 
>    Data: cbpp 
>  Family: binomial(logit link)
>    AIC   BIC logLik deviance
>  110.1 120.2 -50.05    100.1
> Random effects:
>  Groups Name        Variance Std.Dev.
>  herd   (Intercept) 0.41804  0.64656 
> number of obs: 56, groups: herd, 15
> 
> Estimated scale (compare to  1 )  1.138075 
> 
> Fixed effects:
>             Estimate Std. Error z value Pr(>|z|)
> (Intercept)  -1.3981     0.2287  -6.113 9.76e-10
> period2      -0.9959     0.3056  -3.259 0.001119
> period3      -1.1350     0.3266  -3.475 0.000510
> period4      -1.5798     0.4286  -3.686 0.000228
> 
> Correlation of Fixed Effects:
>         (Intr) perid2 perid3
> period2 -0.350              
> period3 -0.327  0.267       
> period4 -0.248  0.202  0.186
> 
> cbpp> ## response as a vector of probabilities and usage of argument "weights"
> cbpp> m2 <- lmer(incidence / size ~ period + (1 | herd), weights = size,
> cbpp+            family = binomial, data = cbpp)
> 
> cbpp> ## Confirm that these are equivalent:
> cbpp> stopifnot(all.equal(coef(m1), coef(m2)),
> cbpp+           all.equal(ranef(m1), ranef(m2)))
>> m3 <- lmer(incidence ~ period + (1|herd),
> +            expand.bin(cbpp, "incidence", "size"), binomial)
>> m3
> Generalized linear mixed model fit using Laplace 
> Formula: incidence ~ period + (1 | herd) 
>    Data: expand.bin(cbpp, "incidence", "size") 
>  Family: binomial(logit link)
>  AIC   BIC logLik deviance
>  565 588.7 -277.5      555
> Random effects:
>  Groups Name        Variance Std.Dev.
>  herd   (Intercept) 0.41448  0.6438  
> number of obs: 842, groups: herd, 15
> 
> Estimated scale (compare to  1 )  0.9832878 
> 
> Fixed effects:
>             Estimate Std. Error z value Pr(>|z|)
> (Intercept)  -1.3984     0.2282  -6.129 8.86e-10
> period2      -0.9934     0.3054  -3.253 0.001144
> period3      -1.1332     0.3264  -3.471 0.000518
> period4      -1.5805     0.4287  -3.686 0.000227
> 
> Correlation of Fixed Effects:
>         (Intr) perid2 perid3
> period2 -0.351              
> period3 -0.328  0.267       
> period4 -0.248  0.202  0.186
>> proc.time()
>    user  system elapsed 
>   7.208   0.120   7.322



From HStevens at MUOhio.edu  Tue May 29 17:08:13 2007
From: HStevens at MUOhio.edu (Martin Henry H. Stevens)
Date: Tue, 29 May 2007 11:08:13 -0400
Subject: [R-sig-ME] [R] exemples, tutorial on lmer
In-Reply-To: <465C3A48.5000303@avignon.inra.fr>
References: <465C3A48.5000303@avignon.inra.fr>
Message-ID: <0F4D9CC0-FE4F-4DD6-A162-C865CDEADEEA@MUOhio.edu>

Hi Oliver,
You could start with R News 2005, no. 1. Also the PDF associated with  
lme4, "Implementation.pdf."
Hnak
On May 29, 2007, at 10:35 AM, Olivier MARTIN wrote:

> Hi all,
>
> I have some difficulties to work with the function lmer from lme4
> Does somebody have a tutorial or different examples to use this  
> function?
>
> Thanks,
> Oliver.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.



Dr. Hank Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/

"E Pluribus Unum"



From olivier.martin at avignon.inra.fr  Tue May 29 16:35:52 2007
From: olivier.martin at avignon.inra.fr (Olivier MARTIN)
Date: Tue, 29 May 2007 16:35:52 +0200
Subject: [R-sig-ME] exemples, tutorial on lmer
Message-ID: <465C3A48.5000303@avignon.inra.fr>

Hi all,

I have some difficulties to work with the function lmer from lme4
Does somebody have a tutorial or different examples to use this function?

Thanks,
Oliver.



From jkrobert at bcm.tmc.edu  Tue May 29 18:45:56 2007
From: jkrobert at bcm.tmc.edu (Roberts, J. Kyle)
Date: Tue, 29 May 2007 11:45:56 -0500
Subject: [R-sig-ME] exemples, tutorial on lmer
In-Reply-To: <465C3A48.5000303@avignon.inra.fr>
Message-ID: <3FC0430478C30B4A9AF0AFF7863418F130F173@BCMEVS6.ad.bcm.edu>

 
This is very short, but it is an excerpt from a chapter that I just wrote about software options for multilevel analysis.  Hope it helps,
Kyle

	Those who are familiar with the R programming language probably know it as the free open-sourced version of S-PLUS.  In fact, both packages are based on the S programming language and use commands that are remarkably similar in both form and execution.  R was originally written by Gentleman
and Ihaka at the University of Auckland with the first version released in 2000.  The original goal of R was to make a "leaner" S language-based system which did not have the same memory hampering problems that S-PLUS had in its early stages of development.  As R has evolved, its strength has come
in that it is a powerful free software package with a network of users who write code which is openly shared among other users.  Although S-PLUS supports more of a GUI (Graphical User Interface) design, recent programs in R such as R-Commander developed by John Fox (Fox, 2005) give it the same feel
as the S-PLUS interface.
	As the code for running a linear multilevel model in R is exactly the same as running the model in S-PLUS, it does not bear repeating here.  The nlme package run in R uses the same set of commands used by the nlme library in S-PLUS (available at http://cran.r-project.org/).  There are some
subtle differences, however.  First, R does not allow the user to run the plot(augPred(lme.object)) command to produce the graph seen in Figure 6.  This graph can still be produced, but it must be done with a groupedData object.  Second, the commands for running more complicated models like binary
response models are different in R.  In S-PLUS, the correlatedData library is used to fit binary response models, whereas the lme4 package in R which is used to fit these models.
	Recently, new routines have been introduced to R that also allow for the fitting of multilevel models.  In addition to the nlme library, the Matrix package contains the routine "lmer" which is also capable of running these models.  This library is very similar to nlme in terms of its
functionality, but the main benefit that it has is that the computations are based on analysis of sparse matrices.  This allows lmer to analyze data at much faster rates than previous versions of nlme (Bates, 2006a).
	The lmer command syntax is slightly different than the nlme syntax.  In lmer  the command line takes the form:
lmer(formula, data, family, method, control , start, subset, weights, na.action, offset, model, x, y, ...)
A thorough description of all of the arguments in this command line can be obtained by typing ?lmer at any command prompt in R.  The format of the command line is mostly similar to lme, with the exception of the fact that lme splits the commands for the fixed and random parts of the command, where
lmer simply has a single "formula" command line.  Part of the reasoning behind this change is because multilevel models are sometimes better thought of as single linear (in this case) equation in stead of a series of two equations (or levels) estimated separately (D. Bates, personal communication,
April 6, 2006).  Therefore, the syntax necessary to run the Roberts (2004) data would be:
	ex1<-lmer(SCIENCE ~ URBAN + (URBAN|GROUP), example.data)
The above command line may be read as the variable "SCIENCE" being estimated by "urban" plus the random effects for both "URBAN" and for the intercept defined by the level-2 grouping variable "GROUP".  Just as with S-PLUS, if a three level model were to be specified with level-2 and level-3 grouping
structures of classrooms and schools, respectively (these are not real variables in the dataset, but meant as heuristic), the command line for this model would read:
	ex1<-lmer(SCIENCE ~ URBAN + (URBAN|SCHOOL/CLASSROOM), example.data)
Typing the command summary(ex1) will produce the following:
Linear mixed-effects model fit by REML
Formula: SCIENCE ~ URBAN + (URBAN | GROUP) 
   Data: example.data 
      AIC      BIC    logLik MLdeviance REMLdeviance
 424.1713 442.6223 -206.0857   413.2216     412.1713
Random effects:
 Groups   Name        Variance  Std.Dev. Corr   
 GROUP    (Intercept) 113.60403 10.65852        
          URBAN         0.25200  0.50200 -0.625 
 Residual               0.27066  0.52025        
# of obs: 160, groups: GROUP, 16

Fixed effects:
             Estimate Std. Error  DF t value  Pr(>|t|)    
(Intercept)  22.39124    2.71703 158  8.2411 6.152e-14 ***
URBAN        -0.86700    0.12981 158 -6.6790 3.884e-10 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Correlation of Fixed Effects:
      (Intr)
URBAN -0.641
	This output is also remarkably similar to the lme output.  One notable difference is that the df has changed from previous versions of the package (see the S-PLUS output from above).  The reasoning behind this df change is a discussion that could easily take up another volume, and a problem
that probably won't be resolved in the near future.  For an interesting discussion of the problems related to df and p-values in multilevel models, see Bates (2006b).
	The only disadvantage to using R, as opposed to S-PLUS, is that it doesn't support a graphical user interface (GUI) design.  However, even this drawback has been partially overcome with the development of the R-Commander program by Fox (2005).  Regardless, R is an extremely powerful package
using object oriented language.  Couple this with the fact that the software is free and R makes a very inviting package.  The one caveat is that this package, like S-PLUS, has a steep learning curve.  Learning new code is like learning a new language; but the payoff may well be worth the effort.

***************************************
J. Kyle Roberts, Ph.D.
Baylor College of Medicine
Center for Educational Outreach
One Baylor Plaza, MS:  BCM411
Houston, TX   77030-3411
713-798-6672 - 713-798-8201 Fax
jkrobert at bcm.edu
***************************************

-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Olivier MARTIN
Sent: Tuesday, May 29, 2007 9:36 AM
To: r-help at stat.math.ethz.ch; r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] exemples, tutorial on lmer

Hi all,

I have some difficulties to work with the function lmer from lme4 Does somebody have a tutorial or different examples to use this function?

Thanks,
Oliver.

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From bernd.weiss at uni-koeln.de  Tue May 29 18:57:38 2007
From: bernd.weiss at uni-koeln.de (Bernd Weiss)
Date: Tue, 29 May 2007 18:57:38 +0200
Subject: [R-sig-ME] [R] exemples, tutorial on lmer
In-Reply-To: <0F4D9CC0-FE4F-4DD6-A162-C865CDEADEEA@MUOhio.edu>
References: <465C3A48.5000303@avignon.inra.fr>,
	<0F4D9CC0-FE4F-4DD6-A162-C865CDEADEEA@MUOhio.edu>
Message-ID: <465C77A2.30190.21B91C2@bernd.weiss.uni-koeln.de>

Am 29 May 2007 um 11:08 hat Martin Henry H. Stevens geschrieben:

From:           	"Martin Henry H. Stevens" <HStevens at muohio.edu>
Date sent:      	Tue, 29 May 2007 11:08:13 -0400
To:             	Olivier MARTIN <olivier.martin at avignon.inra.fr>
Copies to:      	r-sig-mixed-models at r-project.org, r-
help at stat.math.ethz.ch
Subject:        	Re: [R-sig-ME] [R] exemples, tutorial on lmer

> Hi Oliver,
> You could start with R News 2005, no. 1. Also the PDF associated with  
> lme4, "Implementation.pdf."
> Hnak
> On May 29, 2007, at 10:35 AM, Olivier MARTIN wrote:
> 
> > Hi all,
> >
> > I have some difficulties to work with the function lmer from lme4
> > Does somebody have a tutorial or different examples to use this  
> > function?
> >

I found Baayen et al. "Mixed-effects modeling with crossed random
effects for subjects and items" very useful. 
<http://www.mpi.nl/world/persons/private/baayen/publications/baayenDav
idsonBates.pdf>. 

You also might consider having a look at  
<http://www.mpi.nl/world/persons/private/baayen/publications/baayenCUP
stats.pdf>, pp. 263.

HTH,

Bernd



From tobias.sing at gmail.com  Tue Jun  5 19:56:52 2007
From: tobias.sing at gmail.com (Tobias Sing)
Date: Tue, 5 Jun 2007 19:56:52 +0200
Subject: [R-sig-ME] nlme: "Singularity in backsolve" error in ODE-defined,
	but not in closed-form model
Message-ID: <c3ca233a0706051056s3c38cc41xcf48060be01f3eab@mail.gmail.com>

Dear all,

I would appreciate very much some help/advice on a problem related to
using nlme together with odesolve. I am aware that the issue might
also be related to odesolve rather than to nlme. Still, the error
message comes from nmle, and I know several people working with nlme
who have run into similar problems, without knowing a solution
(platform: nlme 3.1-79, R 2.4.1, Win XP).

I have been trying to come up with a minimalistic example of using
analytical versus ODE-defined models in nlme. Specifically, I am
sampling artificial data from the model
y_ij = exp(-(beta + b_i) * t) + eps_ij
(i = number of individuals, j=observations for an individual),
which corresponds to the ODE dy/dt = -beta * y(t).

As in the examples in "the book", I am fitting three models:
1.) Pooling the data from all individuals
2.) Separately for all individuals
3.) Mixed-effects model

All three steps work fine for the _analytically_ defined models (and
the estimates are all good), but when I use the equivalent
_ODE_-defined models, only the first two steps are fine, and the call
to nlme (for fitting model 3) always stops with

--> "Error in MEEM(...): Singularity in backsolve at level 0, block 1)".

Here is what I tried to solve this problem, without success:
a.) Setting atol and rtol in odesolve as low as possible to be
virtually identical to the output of the analytical expression. In R's
output, I don't see any remaining differences. For example, I have
analytical(TIME=10, beta=0.75) = ode(TIME=10, beta=0.75) =
0.0005530844
b.) Playing around with the nlme control options msVerbose, tolerance,
msTol, pnlsTol, and maxiter.
c.) Googling for possible solutions

I would appreciate any help/advice to solve this problem.
Many thanks,
  Tobias

P.S. Here is the full code to reproduce the problem:

## sample artificial exponential decay data with noise:
library(nlme)
library(odesolve)
beta <- 1
sigma <- 0.025
sigma.b <- 0.2
times <- seq(0,10, by=0.5)

n.individuals <- 10
b <- rnorm(n.individuals, 0, sigma.b)
eps <- matrix(rnorm(n.individuals * length(times), 0, sigma),
length(times), n.individuals)

my.data <- as.list(1:n.individuals)
for (i in 1:n.individuals) {
  my.data[[i]] <- data.frame(ID=rep(i, length(times)),
                       TIME=times,
                       DV=exp(- (beta + b[i]) * times) + eps[, i])
  my.data[[i]][ my.data[[i]] <= 0] <- 10^(-5)
}
my.data <- do.call(rbind, my.data)

## fit analytical models (pooled, individual, and mixed):
## EVERYTHING WORKS FINE HERE!
analytical <- function(TIME, beta) {
  exp(-beta * TIME)
}
nls(DV ~ analytical(TIME, beta), data=my.data, start=list(beta=2))
model.list <- nlsList(DV ~ analytical(TIME, beta) | ID, data=my.data,
start=list(beta=2))
model.nlme <- nlme(model.list)

## fit equivalent ODE-defined models (pooled, individual, and mixed):
## --> ERROR "Singularity in backsolve" for mixed model
yprime <- function(t, y, parms) {
  dydt <- -parms['beta'] * y
  list(dydt)
}
ode <- function(TIME, beta) {
  y0 <- 1
  ans <- lsoda(y0, c(0,TIME), yprime, c(beta=beta), atol=10^(-10),
rtol=10^(-10))
  ans[2,2]
}
ode.vectorized <- Vectorize(ode, 'TIME')

nls(DV ~ ode.vectorized(TIME, beta), data=my.data, start=list(beta=2))
model.list <- nlsList(DV ~ ode.vectorized(TIME, beta) | ID,
data=my.data, start=list(beta=2))
model.nlme <- nlme(model.list)

"Error in MEEM(object, conLin, control$niterEM) :
        Singularity in backsolve at level 0, block 1"



From Cougar_711 at msn.com  Wed Jun  6 22:15:42 2007
From: Cougar_711 at msn.com (Cougar)
Date: Wed, 6 Jun 2007 16:15:42 -0400
Subject: [R-sig-ME] anova
Message-ID: <BAY144-DAV10F5F85FE3F9BE83739A6A0270@phx.gbl>

I wonder if it is possible to nest a generalized least squares model within
an lmer model for the purpose of testing the hypothesis that sigma.sq_beta_0
= 0.

Example: R 2.5.0, WinXP

>library(lme4)
>data(sleepstudy)
>library(nlme)
>m0 <- gls(Reaction ~ Days, sleepstudy)
>m1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
>anova(m0,m1)

I do not get any result from the anova.  I assume it is because the model
objects are of different classes.  Is that correct?

Respectfully,

Frank Lawrence



From bates at stat.wisc.edu  Wed Jun  6 22:35:36 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 6 Jun 2007 15:35:36 -0500
Subject: [R-sig-ME] anova
In-Reply-To: <BAY144-DAV10F5F85FE3F9BE83739A6A0270@phx.gbl>
References: <BAY144-DAV10F5F85FE3F9BE83739A6A0270@phx.gbl>
Message-ID: <40e66e0b0706061335s364f51ffwfb287f5db194726d@mail.gmail.com>

On 6/6/07, Cougar <Cougar_711 at msn.com> wrote:
> I wonder if it is possible to nest a generalized least squares model within
> an lmer model for the purpose of testing the hypothesis that sigma.sq_beta_0
> = 0.
>
> Example: R 2.5.0, WinXP
>
> >library(lme4)
> >data(sleepstudy)
> >library(nlme)
> >m0 <- gls(Reaction ~ Days, sleepstudy)
> >m1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
> >anova(m0,m1)

It is supposed to work is you use lm, not gls.  However, it doesn't.
I am reminded of one of my favorite quotes that "In theory, theory and
practice are the same.  In practice they're not.".

> library(lme4)
Loading required package: Matrix
Loading required package: lattice
> fm1 <- lm(Reaction ~ Days, sleepstudy)
> fm2 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
> anova(fm2, fm1)
Error in FUN(X[[1L]], ...) :
  no slot of name "call" for this object of class "lm"

I need to be a bit more careful in the code to get this to work but I
will do so.

>
> I do not get any result from the anova.  I assume it is because the model
> objects are of different classes.  Is that correct?
>
> Respectfully,
>
> Frank Lawrence
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From jkrobert at bcm.tmc.edu  Thu Jun  7 17:03:22 2007
From: jkrobert at bcm.tmc.edu (Roberts, J. Kyle)
Date: Thu, 7 Jun 2007 10:03:22 -0500
Subject: [R-sig-ME] NAEP Data and lmer
In-Reply-To: <40e66e0b0706061335s364f51ffwfb287f5db194726d@mail.gmail.com>
Message-ID: <3FC0430478C30B4A9AF0AFF7863418F130F1A4@BCMEVS6.ad.bcm.edu>

 
Friends,

Have any of you used either lme or lmer to analyze NAEP data? I have done some things with HLM (the software), but wondered if there was already a set of routines for handling the whole "plausible values" thing in R. Also, for those of you familiar with the NAEP and HLM, does HLM just average the
results of the plausible values, or is the final estimate a maximized estimate based on the five plausible values?

Thanks,
Kyle

P.S. - Be on the watch for the first ever "North American Conference on Multilevel Analysis" coming late Spring 2008!!

***************************************
J. Kyle Roberts, Ph.D.
Baylor College of Medicine
Center for Educational Outreach
One Baylor Plaza, MS:  BCM411
Houston, TX   77030-3411
713-798-6672 - 713-798-8201 Fax
jkrobert at bcm.edu



From bates at stat.wisc.edu  Thu Jun  7 17:14:22 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 7 Jun 2007 10:14:22 -0500
Subject: [R-sig-ME] NAEP Data and lmer
In-Reply-To: <3FC0430478C30B4A9AF0AFF7863418F130F1A4@BCMEVS6.ad.bcm.edu>
References: <40e66e0b0706061335s364f51ffwfb287f5db194726d@mail.gmail.com>
	<3FC0430478C30B4A9AF0AFF7863418F130F1A4@BCMEVS6.ad.bcm.edu>
Message-ID: <40e66e0b0706070814g7e1b47a4u9ad35208748b05e2@mail.gmail.com>

On 6/7/07, Roberts, J. Kyle <jkrobert at bcm.tmc.edu> wrote:

> Have any of you used either lme or lmer to analyze NAEP data? I have done some things with HLM (the software), but wondered if there was already a set of routines for handling the whole "plausible values" thing in R. Also, for those of you familiar with the NAEP and HLM, does HLM just average the
> results of the plausible values, or is the final estimate a maximized estimate based on the five plausible values?

My guess from the google hits is that NAEP stands for the National
Assessment of Educational Progress.  Can you be more specific about
the issues involved in analyzing NAEP data?

> P.S. - Be on the watch for the first ever "North American Conference on Multilevel Analysis" coming late Spring 2008!!

Sounds interesting.  Has the location been set?



From HDoran at air.org  Thu Jun  7 17:13:26 2007
From: HDoran at air.org (Doran, Harold)
Date: Thu, 7 Jun 2007 11:13:26 -0400
Subject: [R-sig-ME] NAEP Data and lmer
In-Reply-To: <3FC0430478C30B4A9AF0AFF7863418F130F1A4@BCMEVS6.ad.bcm.edu>
Message-ID: <2323A6D37908A847A7C32F1E3662C80EBA085D@dc1ex01.air.org>

Kyle:

What do you mean by "handle the plausible values thing in R". The
class.acc() function in the MiscPsycho package is essentially the same
expression used for calculating NAEP plausble values. I can send the
vignette that shows the computational details. Since for each student
you have multiple plausible values, can you give us a sense of what your
data structure looks like and how you're analyzing it in order to answer
your question? 

Harold


> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org 
> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf 
> Of Roberts, J. Kyle
> Sent: Thursday, June 07, 2007 11:03 AM
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] NAEP Data and lmer
> 
>  
> Friends,
> 
> Have any of you used either lme or lmer to analyze NAEP data? 
> I have done some things with HLM (the software), but wondered 
> if there was already a set of routines for handling the whole 
> "plausible values" thing in R. Also, for those of you 
> familiar with the NAEP and HLM, does HLM just average the 
> results of the plausible values, or is the final estimate a 
> maximized estimate based on the five plausible values?
> 
> Thanks,
> Kyle
> 
> P.S. - Be on the watch for the first ever "North American 
> Conference on Multilevel Analysis" coming late Spring 2008!!
> 
> ***************************************
> J. Kyle Roberts, Ph.D.
> Baylor College of Medicine
> Center for Educational Outreach
> One Baylor Plaza, MS:  BCM411
> Houston, TX   77030-3411
> 713-798-6672 - 713-798-8201 Fax
> jkrobert at bcm.edu
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From jkrobert at bcm.tmc.edu  Thu Jun  7 17:40:54 2007
From: jkrobert at bcm.tmc.edu (Roberts, J. Kyle)
Date: Thu, 7 Jun 2007 10:40:54 -0500
Subject: [R-sig-ME] NAEP Data and lmer
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80EBA085D@dc1ex01.air.org>
Message-ID: <3FC0430478C30B4A9AF0AFF7863418F130F1A5@BCMEVS6.ad.bcm.edu>

Harold,

I just want to use R to analyze NAEP data in a multilevel framework.  I don't want to actually calculate plausible values (if I understood your question correctly).  I was wondering if I would need to run 5 separate analyses and then average across those values or if there was some built-in function
in R that would let me run all five analyses simultaneously (like how SPSS handles NAEP data with regression and ANOVA).  For my analyses, I am wanting to look at student-level science scores nested within states.  I am wanting to do this to see the effect of states adding science components to
their state mandated standardized assessments (like Texas did a few years back).  This is more curiosity on my part than any rigorous study.  Regardless, I would be very interested in seeing the vignette for the class.acc() function.

Thanks,
Kyle


***************************************
J. Kyle Roberts, Ph.D.
Baylor College of Medicine
Center for Educational Outreach
One Baylor Plaza, MS:  BCM411
Houston, TX   77030-3411
713-798-6672 - 713-798-8201 Fax
jkrobert at bcm.edu
***************************************

-----Original Message-----
From: Doran, Harold [mailto:HDoran at air.org] 
Sent: Thursday, June 07, 2007 10:13 AM
To: Roberts, J. Kyle; r-sig-mixed-models at r-project.org
Subject: RE: [R-sig-ME] NAEP Data and lmer

Kyle:

What do you mean by "handle the plausible values thing in R". The
class.acc() function in the MiscPsycho package is essentially the same expression used for calculating NAEP plausble values. I can send the vignette that shows the computational details. Since for each student you have multiple plausible values, can you give us a sense of what your data structure
looks like and how you're analyzing it in order to answer your question? 

Harold


> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org
> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of 
> Roberts, J. Kyle
> Sent: Thursday, June 07, 2007 11:03 AM
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] NAEP Data and lmer
> 
>  
> Friends,
> 
> Have any of you used either lme or lmer to analyze NAEP data? 
> I have done some things with HLM (the software), but wondered if there 
> was already a set of routines for handling the whole "plausible 
> values" thing in R. Also, for those of you familiar with the NAEP and 
> HLM, does HLM just average the results of the plausible values, or is 
> the final estimate a maximized estimate based on the five plausible 
> values?
> 
> Thanks,
> Kyle
> 
> P.S. - Be on the watch for the first ever "North American Conference 
> on Multilevel Analysis" coming late Spring 2008!!
> 
> ***************************************
> J. Kyle Roberts, Ph.D.
> Baylor College of Medicine
> Center for Educational Outreach
> One Baylor Plaza, MS:  BCM411
> Houston, TX   77030-3411
> 713-798-6672 - 713-798-8201 Fax
> jkrobert at bcm.edu
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From HDoran at air.org  Thu Jun  7 18:19:32 2007
From: HDoran at air.org (Doran, Harold)
Date: Thu, 7 Jun 2007 12:19:32 -0400
Subject: [R-sig-ME] NAEP Data and lmer
In-Reply-To: <3FC0430478C30B4A9AF0AFF7863418F130F1A5@BCMEVS6.ad.bcm.edu>
Message-ID: <2323A6D37908A847A7C32F1E3662C80EBA0869@dc1ex01.air.org>

Kyle:

I think this needs much more context before I can be helpful, or maybe I
am missing something. Our software program AM (that is AIR's software)
is designed to handle some NAEP analyses (and it is also free). It also
has a small component for MLMs, but it is not able to do what lmer does.

Are you asking about 5 analyses because you want to do 1 for each of the
5 plausible values? 


> -----Original Message-----
> From: Roberts, J. Kyle [mailto:jkrobert at bcm.tmc.edu] 
> Sent: Thursday, June 07, 2007 11:41 AM
> To: Doran, Harold; r-sig-mixed-models at r-project.org
> Subject: RE: [R-sig-ME] NAEP Data and lmer
> 
> Harold,
> 
> I just want to use R to analyze NAEP data in a multilevel 
> framework.  I don't want to actually calculate plausible 
> values (if I understood your question correctly).  I was 
> wondering if I would need to run 5 separate analyses and then 
> average across those values or if there was some built-in 
> function in R that would let me run all five analyses 
> simultaneously (like how SPSS handles NAEP data with 
> regression and ANOVA).  For my analyses, I am wanting to look 
> at student-level science scores nested within states.  I am 
> wanting to do this to see the effect of states adding science 
> components to their state mandated standardized assessments 
> (like Texas did a few years back).  This is more curiosity on 
> my part than any rigorous study.  Regardless, I would be very 
> interested in seeing the vignette for the class.acc() function.
> 
> Thanks,
> Kyle
> 
> 
> ***************************************
> J. Kyle Roberts, Ph.D.
> Baylor College of Medicine
> Center for Educational Outreach
> One Baylor Plaza, MS:  BCM411
> Houston, TX   77030-3411
> 713-798-6672 - 713-798-8201 Fax
> jkrobert at bcm.edu
> ***************************************
> 
> -----Original Message-----
> From: Doran, Harold [mailto:HDoran at air.org]
> Sent: Thursday, June 07, 2007 10:13 AM
> To: Roberts, J. Kyle; r-sig-mixed-models at r-project.org
> Subject: RE: [R-sig-ME] NAEP Data and lmer
> 
> Kyle:
> 
> What do you mean by "handle the plausible values thing in R". The
> class.acc() function in the MiscPsycho package is essentially 
> the same expression used for calculating NAEP plausble 
> values. I can send the vignette that shows the computational 
> details. Since for each student you have multiple plausible 
> values, can you give us a sense of what your data structure 
> looks like and how you're analyzing it in order to answer 
> your question? 
> 
> Harold
> 
> 
> > -----Original Message-----
> > From: r-sig-mixed-models-bounces at r-project.org
> > [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of 
> > Roberts, J. Kyle
> > Sent: Thursday, June 07, 2007 11:03 AM
> > To: r-sig-mixed-models at r-project.org
> > Subject: [R-sig-ME] NAEP Data and lmer
> > 
> >  
> > Friends,
> > 
> > Have any of you used either lme or lmer to analyze NAEP data? 
> > I have done some things with HLM (the software), but 
> wondered if there 
> > was already a set of routines for handling the whole "plausible 
> > values" thing in R. Also, for those of you familiar with 
> the NAEP and 
> > HLM, does HLM just average the results of the plausible 
> values, or is 
> > the final estimate a maximized estimate based on the five plausible 
> > values?
> > 
> > Thanks,
> > Kyle
> > 
> > P.S. - Be on the watch for the first ever "North American 
> Conference 
> > on Multilevel Analysis" coming late Spring 2008!!
> > 
> > ***************************************
> > J. Kyle Roberts, Ph.D.
> > Baylor College of Medicine
> > Center for Educational Outreach
> > One Baylor Plaza, MS:  BCM411
> > Houston, TX   77030-3411
> > 713-798-6672 - 713-798-8201 Fax
> > jkrobert at bcm.edu
> > 
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > 
>



From bates at stat.wisc.edu  Mon Jun 11 20:46:09 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 11 Jun 2007 13:46:09 -0500
Subject: [R-sig-ME] Release 0.99875-1 of the lme4 package uploaded to CRAN
Message-ID: <40e66e0b0706111146i3cfb5877v249739ca040cec49@mail.gmail.com>

I have just uploaded the source package for version 0.99875-1 of the
lme4 package to CRAN.  There are two big additions to this version of
the package:

 - a new function nlmer to fit nonlinear mixed models
 - a vignette on "Theory and computational methods for mixed models"

The vignette is not in its finished form but it is a good portion of
the way along to a explaining a uniform approach to linear mixed
models, generalized linear mixed models and nonlinear mixed models
with crossed and/or nested random effects.

If you read this vignette and the other vignette on implementation in
the lme4 package then compare them to the code you will see that they
describe the lmer2 approach, not the approach that is currently
implemented in lmer.  Right now I have some implementation issues with
generalized linear mixed models and the mcmcsamp function in the lmer2
formulation but I think they are implementation issues and not
theoretical roadblocks.  Once I resolve those implementation issues I
will switch what is now called lmer2 to become lmer.

If you have books or papers that record the results of an lmer fit I
will be happy to work with you to ensure that the results from the new
version are at least as good as the results from the previous version.

If you have code that depends on the availability of specific slots in
an lmer object you should be aware that your code will break after
that switch takes place.  I can describe how the lmer2 object
represents the model and data but I don't guarantee to fix your code
for you if you reached in and grabbed particular slots (as opposed to
using extractor functions).  Some peculiar results in testing on CRAN
in the last few days caused Kurt, Martin and me to look at the
function display.mer in package arm.  That code makes a lot of
assumptions that are going to fail in the near future.



From jkrobert at bcm.tmc.edu  Mon Jun 11 22:37:56 2007
From: jkrobert at bcm.tmc.edu (Roberts, J. Kyle)
Date: Mon, 11 Jun 2007 15:37:56 -0500
Subject: [R-sig-ME] Multilevel Probability Question
Message-ID: <3FC0430478C30B4A9AF0AFF7863418F130F1B5@BCMEVS6.ad.bcm.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20070611/92c040e2/attachment.pl>

From stevenmh at muohio.edu  Tue Jun 12 13:51:38 2007
From: stevenmh at muohio.edu (Martin Henry H. Stevens)
Date: Tue, 12 Jun 2007 07:51:38 -0400
Subject: [R-sig-ME] Multilevel Probability Question
In-Reply-To: <3FC0430478C30B4A9AF0AFF7863418F130F1B5@BCMEVS6.ad.bcm.edu>
References: <3FC0430478C30B4A9AF0AFF7863418F130F1B5@BCMEVS6.ad.bcm.edu>
Message-ID: <A8863C39-57E9-46AA-9905-223A2BA4947E@muohio.edu>

If you aren't stratifying by school, then isn't it just the  
probability of drawing 120 teachers out of 2000?
-Hank
On Jun 11, 2007, at 4:37 PM, Roberts, J. Kyle wrote:

> Dear Friends,
>
> I sent the following to the multilevel listserv, but thought that  
> some of you might actually have code to compute this in R.  If you  
> do, please let me know.
>
> I have an odd question. I am trying to compute the probability of  
> drawing 120 teachers from a sample of 2000 teachers in 400 schools  
> where no two teachers work at the same school. Assume that there  
> are 5 teachers at each school, evenly spread. I was trying to do  
> this with an "n choose k" type
> situation, but I can't figure out how to include the part about  
> only 5 teachers can be at any given school. Any ideas? I am not  
> stratifying on school, just sampling teachers.
>
> Thanks,
>
> Kyle
>
>
> ***************************************
> J. Kyle Roberts, Ph.D.
> Baylor College of Medicine
> Center for Educational Outreach
> One Baylor Plaza, MS:  BCM411
> Houston, TX   77030-3411
> 713-798-6672 - 713-798-8201 Fax
> jkrobert at bcm.edu
> ***************************************
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From ccleland at optonline.net  Tue Jun 12 14:51:42 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 12 Jun 2007 08:51:42 -0400
Subject: [R-sig-ME] Multilevel Probability Question
In-Reply-To: <A8863C39-57E9-46AA-9905-223A2BA4947E@muohio.edu>
References: <3FC0430478C30B4A9AF0AFF7863418F130F1B5@BCMEVS6.ad.bcm.edu>
	<A8863C39-57E9-46AA-9905-223A2BA4947E@muohio.edu>
Message-ID: <466E96DE.6000100@optonline.net>

Martin Henry H. Stevens wrote:
> If you aren't stratifying by school, then isn't it just the  
> probability of drawing 120 teachers out of 2000?
> -Hank

  Creating the population described and taking a simple random sample of
120 teachers, it seems it would be very rare for no two teachers to come
from the same school:

pop <- data.frame(TEACHER = 1:2000, SCHOOL = rep(1:400, 5))

table(replicate(1000000, any(table(pop[sample(2000, size = 120,
replace=FALSE),]$SCHOOL) > 1)))

   TRUE
1000000

  There are obviously some samples where every teacher would come from a
different school, but I'm not sure how you might find the specific
probability.

hope this helps,

Chuck

> On Jun 11, 2007, at 4:37 PM, Roberts, J. Kyle wrote:
> 
>> Dear Friends,
>>
>> I sent the following to the multilevel listserv, but thought that  
>> some of you might actually have code to compute this in R.  If you  
>> do, please let me know.
>>
>> I have an odd question. I am trying to compute the probability of  
>> drawing 120 teachers from a sample of 2000 teachers in 400 schools  
>> where no two teachers work at the same school. Assume that there  
>> are 5 teachers at each school, evenly spread. I was trying to do  
>> this with an "n choose k" type
>> situation, but I can't figure out how to include the part about  
>> only 5 teachers can be at any given school. Any ideas? I am not  
>> stratifying on school, just sampling teachers.
>>
>> Thanks,
>>
>> Kyle
>>
>>
>> ***************************************
>> J. Kyle Roberts, Ph.D.
>> Baylor College of Medicine
>> Center for Educational Outreach
>> One Baylor Plaza, MS:  BCM411
>> Houston, TX   77030-3411
>> 713-798-6672 - 713-798-8201 Fax
>> jkrobert at bcm.edu
>> ***************************************
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894



From tring at gvdnet.dk  Tue Jun 12 16:17:27 2007
From: tring at gvdnet.dk (Troels Ring)
Date: Tue, 12 Jun 2007 16:17:27 +0200
Subject: [R-sig-ME] [Fwd: Re:  Multilevel Probability Question]
Message-ID: <466EAAF7.6010908@gvdnet.dk>


-- 

Troels Ring - -
Department of nephrology - - 
Aalborg Hospital 9100 Aalborg, Denmark - -
+45 99326629 - -
tring at gvdnet.dk

-------------- next part --------------
An embedded message was scrubbed...
From: Chuck Cleland <ccleland at optonline.net>
Subject: Re: [R-sig-ME] Multilevel Probability Question
Date: Tue, 12 Jun 2007 08:51:42 -0400
Size: 5246
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20070612/54ff7878/attachment.mht>

From robert.h.creecy at census.gov  Tue Jun 12 16:52:15 2007
From: robert.h.creecy at census.gov (robert.h.creecy at census.gov)
Date: Tue, 12 Jun 2007 10:52:15 -0400
Subject: [R-sig-ME] Multilevel Probability Question
In-Reply-To: <466E96DE.6000100@optonline.net>
Message-ID: <OF084CAA89.69E644C2-ON852572F8.005001AF-852572F8.0051AE84@census.gov>

Yes, very rare

For a simple random sample without replacement of 120 teachers from the
2000,
the probability that every teacher comes from a different school is:

#  probs[k] is the probability that the kth drawn teacher is not from any
of the previously drawn teacher's schools
# and equals 1 - the probability that the teacher drawn IS from one of the
previously drawn teacher's schools.
# After drawing one teacher from a school there are 4 left from that
schoold, so after drawing k-1 teachers there are 4*(k-1) teachers
# out of 2000-k-1 teachers that could be drawn

> probs<-1 - 4*(0:199)/(2000-0:199)
# The probability that all of the 120 teachers are from different schools
is the product
> prod(probs[1:120])
[1] 8.258304e-08
# picking 27 teachers from different schools is reasonably likely
> prod(probs[1:27])
[1] 0.4861363

Rob



                                                                           
             Chuck Cleland                                                 
             <ccleland at optonli                                             
             ne.net>                                                    To 
             Sent by:                  "Martin Henry H. Stevens"           
             r-sig-mixed-model         <stevenmh at muohio.edu>               
             s-bounces at r-proje                                          cc 
             ct.org                    r-sig-mixed-models at r-project.org    
                                                                   Subject 
                                       Re: [R-sig-ME] Multilevel           
             06/12/2007 08:51          Probability Question                
             AM                                                            
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




Martin Henry H. Stevens wrote:
> If you aren't stratifying by school, then isn't it just the
> probability of drawing 120 teachers out of 2000?
> -Hank

  Creating the population described and taking a simple random sample of
120 teachers, it seems it would be very rare for no two teachers to come
from the same school:

pop <- data.frame(TEACHER = 1:2000, SCHOOL = rep(1:400, 5))

table(replicate(1000000, any(table(pop[sample(2000, size = 120,
replace=FALSE),]$SCHOOL) > 1)))

   TRUE
1000000

  There are obviously some samples where every teacher would come from a
different school, but I'm not sure how you might find the specific
probability.

hope this helps,

Chuck

> On Jun 11, 2007, at 4:37 PM, Roberts, J. Kyle wrote:
>
>> Dear Friends,
>>
>> I sent the following to the multilevel listserv, but thought that
>> some of you might actually have code to compute this in R.  If you
>> do, please let me know.
>>
>> I have an odd question. I am trying to compute the probability of
>> drawing 120 teachers from a sample of 2000 teachers in 400 schools
>> where no two teachers work at the same school. Assume that there
>> are 5 teachers at each school, evenly spread. I was trying to do
>> this with an "n choose k" type
>> situation, but I can't figure out how to include the part about
>> only 5 teachers can be at any given school. Any ideas? I am not
>> stratifying on school, just sampling teachers.
>>
>> Thanks,
>>
>> Kyle
>>
>>
>> ***************************************
>> J. Kyle Roberts, Ph.D.
>> Baylor College of Medicine
>> Center for Educational Outreach
>> One Baylor Plaza, MS:  BCM411
>> Houston, TX   77030-3411
>> 713-798-6672 - 713-798-8201 Fax
>> jkrobert at bcm.edu
>> ***************************************
>>
>>
>>           [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

--
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From robert.h.creecy at census.gov  Tue Jun 12 17:21:00 2007
From: robert.h.creecy at census.gov (robert.h.creecy at census.gov)
Date: Tue, 12 Jun 2007 11:21:00 -0400
Subject: [R-sig-ME] Multilevel Probability Question
In-Reply-To: <OF084CAA89.69E644C2-ON852572F8.005001AF-852572F8.0051AE84@census.gov>
Message-ID: <OF12CDE65F.5D76E14E-ON852572F8.005381D6-852572F8.0054502F@census.gov>

Actually rereading the original post, using choose produces the same answer
The 5 teachers per school term is 5^120

> choose(400,120)*(5^120)/choose(2000,120)
[1] 8.258304e-08

or more reliably for bigger problems using logs

> exp(lchoose(400,120)+120*log(5)-lchoose(2000,120))
[1] 8.258304e-08

Rob



                                                                           
             robert.h.creecy at c                                             
             ensus.gov                                                     
             Sent by:                                                   To 
             r-sig-mixed-model         Chuck Cleland                       
             s-bounces at r-proje         <ccleland at optonline.net>            
             ct.org                                                     cc 
                                       r-sig-mixed-models at r-project.org,   
                                       "Martin Henry H. Stevens"           
             06/12/2007 10:52          <stevenmh at muohio.edu>               
             AM                                                    Subject 
                                       Re: [R-sig-ME] Multilevel           
                                       Probability Question                
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




Yes, very rare

For a simple random sample without replacement of 120 teachers from the
2000,
the probability that every teacher comes from a different school is:

#  probs[k] is the probability that the kth drawn teacher is not from any
of the previously drawn teacher's schools
# and equals 1 - the probability that the teacher drawn IS from one of the
previously drawn teacher's schools.
# After drawing one teacher from a school there are 4 left from that
schoold, so after drawing k-1 teachers there are 4*(k-1) teachers
# out of 2000-k-1 teachers that could be drawn

> probs<-1 - 4*(0:199)/(2000-0:199)
# The probability that all of the 120 teachers are from different schools
is the product
> prod(probs[1:120])
[1] 8.258304e-08
# picking 27 teachers from different schools is reasonably likely
> prod(probs[1:27])
[1] 0.4861363

Rob




             Chuck Cleland
             <ccleland at optonli
             ne.net>                                                    To
             Sent by:                  "Martin Henry H. Stevens"
             r-sig-mixed-model         <stevenmh at muohio.edu>
             s-bounces at r-proje                                          cc
             ct.org                    r-sig-mixed-models at r-project.org
                                                                   Subject
                                       Re: [R-sig-ME] Multilevel
             06/12/2007 08:51          Probability Question
             AM









Martin Henry H. Stevens wrote:
> If you aren't stratifying by school, then isn't it just the
> probability of drawing 120 teachers out of 2000?
> -Hank

  Creating the population described and taking a simple random sample of
120 teachers, it seems it would be very rare for no two teachers to come
from the same school:

pop <- data.frame(TEACHER = 1:2000, SCHOOL = rep(1:400, 5))

table(replicate(1000000, any(table(pop[sample(2000, size = 120,
replace=FALSE),]$SCHOOL) > 1)))

   TRUE
1000000

  There are obviously some samples where every teacher would come from a
different school, but I'm not sure how you might find the specific
probability.

hope this helps,

Chuck

> On Jun 11, 2007, at 4:37 PM, Roberts, J. Kyle wrote:
>
>> Dear Friends,
>>
>> I sent the following to the multilevel listserv, but thought that
>> some of you might actually have code to compute this in R.  If you
>> do, please let me know.
>>
>> I have an odd question. I am trying to compute the probability of
>> drawing 120 teachers from a sample of 2000 teachers in 400 schools
>> where no two teachers work at the same school. Assume that there
>> are 5 teachers at each school, evenly spread. I was trying to do
>> this with an "n choose k" type
>> situation, but I can't figure out how to include the part about
>> only 5 teachers can be at any given school. Any ideas? I am not
>> stratifying on school, just sampling teachers.
>>
>> Thanks,
>>
>> Kyle
>>
>>
>> ***************************************
>> J. Kyle Roberts, Ph.D.
>> Baylor College of Medicine
>> Center for Educational Outreach
>> One Baylor Plaza, MS:  BCM411
>> Houston, TX   77030-3411
>> 713-798-6672 - 713-798-8201 Fax
>> jkrobert at bcm.edu
>> ***************************************
>>
>>
>>           [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

--
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From bates at stat.wisc.edu  Wed Jun 13 19:25:51 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 13 Jun 2007 12:25:51 -0500
Subject: [R-sig-ME] Questions on migrating from nlme to lme4
Message-ID: <40e66e0b0706131025g7920b2a0n91e6d513b66c21c3@mail.gmail.com>

Thanks for your messages, Robert.  You mentioned posting to the
R-SIG-Mixed-Models list so I am taking the liberty of cc:ing the list
on this reply.

It is best to think of lme4 as a reimplementation of mixed models from
the ground up.  I have been seeking a way of generalizing the theory
and computational methods for mixed models in a flexible, efficient
and extensible way.  At the risk of being immodest I will say that I
think I have done so.

For linear mixed models the big difference between lme and lmer is
that lmer can handle models with crossed or partially crossed random
effects efficiently.  In areas like the analysis of scores on
state-wide tests mandated by the No Child Left Behind act or in the
analysis of "breeding values" by animal scientists, large data sets
with a complex structure are the norm.  The state-of-the-art methods
for sparse matrices that are used in lmer make fitting mixed models to
such data feasible.

In the process of examining linear mixed models in detail I eventually
arrived at a formulation of the linear mixed model that generalizes to
generalized linear mixed models and nonlinear mixed models in a way
that, to me at least, makes sense.  The basic computational steps and
the representation of the models are sufficiently similar to share a
lot of the code.

Thus the lme4 package is not an upgrade of the nlme package.  It
shares some characteristics with nlme but it is best to think of it as
a reimplementation, not an extension.

By the way, the theoretical development that I mentioned will be
described in the vignette "Theory and Computational Methods for Mixed
Models".  The first few sections of that vignette are included in the
lme4_0.99875-1 package and there is more to come.  Sorry for releasing
snapshots of a paper (or a book chapter, in this case).  Because the
document changes between releases of the package there is not a stable
reference to cite.  Once I have covered all the ground I want to cover
I will release a stable version as a technical report.

On 6/13/07, Robert Kushler <kushler at oakland.edu> wrote:

> Follow-up to previous message (included below):

> I discovered that "confint" can be used in place of "intervals", so item 2
> is taken care of.

There is a "confint" method for "lmList" objects (also discussed
below).  I'm not sure I will produce one for "lmer" objects if the
degrees of freedom controversy is not resolved.  Also I don't think
that symmetric intervals on variance components make sense and would
prefer not to produce them.

Generally I recommend using mcmcsamp to produce "confidence intervals"
on the variance components.  Approximate (and symmetric) confidence
intervals on the fixed effects are reasonably accurate but such
intervals for the random effects can be poor approximations.

> I also gather that you allow (expect?) users to use the
> two libraries simultaneously.  I assume you recommend loading nlme first,
> so the lmer version of common functions will be used.

No, I don't think it is a good idea to use nlme and lme4
simultaneously.   At one point I had code in lme4 to give a warning if
you tried to load the lme4 package when the nlme package was already
loaded.

> The windows version of 0.99875-1 appeared today.

Good.

> I have a small bit of user feedback:  lmList requires the "data" argument
> even when all of the needed variables exist in the global environment.
> I know it's good practice to construct a data frame (and advantageous to
> exploit the groupedData structure), but since lmer lets the user be lazy,
> why doesn't lmList do the same?

I haven't looked at that version of lmList for a long time.  I suppose
that I am not generating a call to 'model.frame' and I should.  I can
put that on the "To Do" list but it will be a long way down the list.

The answer to another of your questions about using groupedData
objects is that I do not encourage the use of groupedData objects with
lme4.  They were an attempt to incorporate information on the grouping
structure with the data but they got too complicated.  I'm in a "keep
it simple" mode in the development of the lme4 package.  Also, the
groupedData objects use S3 classes and one of the purposes of the lme4
package is to explore the use of S4 classes and methods.

Another aspect of the groupedData objects is that the formulation,
like the formulation of lme itself, makes sense for nested groupings
but not for crossed or partially crossed groupings.

To some extent the purpose of defining the groupedData classes was to
make it easier to generate meaningful and informative graphics with
the trellis functions in S-PLUS.  Deepayan Sarkar's development of
lattice package for R has incorporated so many new features that many
of the operations that required custom code in trellis are possible
transparently in lattice.

> Thanks again for whatever time you can spare,   Rob Kushler

> Text of previous message:

> I am belatedly transitioning to lme4 and have some questions.  Some of
> the following are things I used to show my students with the nlme library
> but have not yet figured out how to do with lme4.  If some of this is on
> your to-do list, perhaps you could post a message to [R-sig-ME] about your
> plans.  (I am eagerly awaiting the appearance of the 0.99875-1 zip file.)

Ahem, I believe you mean the nlme *package*.  (Bear in mind that
Martin Maechler reads this list. :-)

> My general question is whether any of the following can be done in the current
> version of lme4 (and if so how), or if these features might be added ("soon",
> not so soon, or never).  I know that I can show my students a "mixed" (sic)
> strategy using both the old and new libraries, but that's a bit awkward, and
> I'd like to convince them (and myself) that we can someday leave nlme behind
> with no (or few) regrets.

> If you have addressed these issues elsewhere, forgive me for not locating it.

> 1) The "correlation" and "weights" arguments of gls and lme were very useful.
>     In particular, I'd like to fit the AR1 and "unstructured" models, and be
>     able to "model heteroscedasticity".

The correlation and weights arguments haven't made it past the "keep
it simple" filter yet.

> 2) I understand your position on Wald inference for variance components, but
>     the "intervals" function had other uses.  In particular, I like to show
>     the students "plot(intervals(lmListobject)) as a diagnostic tool.  Do you
>     no longer recommend this?

I do think that a plot of the confidence intervals from an lmList
object is a good idea.  (For one thing, those confidence intervals are
well-defined.)  I have code that looks like

###################################################
### chunk number 3: Sl1
###################################################
print(plot(confint(lmList(Reaction ~ Days | Subject, sleepstudy),
                   pooled = TRUE), order = 1))

and it appears to work in current versions of lme4.

> 3) Do you plan to give the inference tools in "languageR" an official blessing
>     by incorporating them into lme4?

I certainly think they are a good idea and I worked with Harald Baayen
a bit on the development of them last January when I visited with him.
 However, I still want to think through the inference issues before
incorporating my final word about inference in the lme4 package.  One
of the annoying aspects others find in working with me is that I am
very slow on design and theoretical development.  I like to think that
the good side of this is that I am thorough.

The lme4 package and the theoretical work behind it have had a long
gestation period - one that probably will extend for a long time to
come.   However, I like the results and I am comfortable with the idea
that I have carefully considered many, many aspects of the model and
computational methods before incorporating them.

> 4) Your R-news article in May 2005 shows "anova" producing F's and p-values,
>     but that no longer seems to be the case.

Indeed.  I realize that calculation of p-values is immensely important
to some people but doing it properly for unbalanced designs is
extremely complicated and, in the areas that I am most concerned with
(large data sets with complex structure), it's a non-issue.  When you
have data on a half-million students in 2500 schools whatever
calculation you use for the degrees of freedom will produce a number
so large that the exact number is irrelevant.

If someone wants to work out how to calculate an approximate
denomination degrees of freedom, and they can get the "degrees of
freedom police" (John Maindonald and Peter Dalgaard) to approve the
result, I'll put the degrees of freedom back in.  Until then, you just
have to do what several people in the psycholinguistics area have
done, which is to uncomment the lines for the denominator degrees of
freedom upper bound in the code for the anova method.

>     In this connection, do you have
>     plans to implement the "potentially useful approximate D.F." mentioned
>     in mcmcsamp.R?

I'm not sure what code you are referring to there.  Is it in the
languageR package?

I imagine the approximation in question is from the trace of the "hat"
matrix.  If you look at the Theory vignette you will see I view the
basic calculation in mixed models as the solution of a penalized least
squares problem.  For an unpenalized least squares problem the
residual degrees of freedom is n minus the trace of the hat matrix.
Frequently in penalized least squares problems the same approximation
is used.

That quantity also appears in the definition of the Deviance
Information Criterion (DIC) by Spiegelhalter et al. for this model.
However, it does not reproduce the results from balanced experiments
with small data sets and I am beginning to have some doubts about
whether this calculation is the best way to go.

> 5) Do you no longer recommend the use of groupedData objects, or do you plan
>     to add them to lme4?

See above.

> I'm sure I'll think of additional questions, but that's (more than) enough
> for now.

> Thanks and regards,   Rob Kushler (Oakland University)

Thanks for your questions.  I hope my answers were helpful.



From John.Maindonald at anu.edu.au  Wed Jun 13 20:07:37 2007
From: John.Maindonald at anu.edu.au (John Maindonald)
Date: Thu, 14 Jun 2007 04:07:37 +1000 (EST)
Subject: [R-sig-ME] Questions on migrating from nlme to lme4
In-Reply-To: <40e66e0b0706131025g7920b2a0n91e6d513b66c21c3@mail.gmail.com>
References: <40e66e0b0706131025g7920b2a0n91e6d513b66c21c3@mail.gmail.com>
Message-ID: <45168.81.231.86.209.1181758057.squirrel@sqmail.anu.edu.au>

Bill, it is a little over 24 hours since Peter Dalgaard
and myself were locked, though briefly, in conversation
about degrees of freedom in lme4.

More seriously, I am impressed with the strides that
you have been making with lme4.

John Maindonald.


> Thanks for your messages, Robert.  You mentioned posting to the
> R-SIG-Mixed-Models list so I am taking the liberty of cc:ing the list
> on this reply.
>
> It is best to think of lme4 as a reimplementation of mixed models from
> the ground up.  I have been seeking a way of generalizing the theory
> and computational methods for mixed models in a flexible, efficient
> and extensible way.  At the risk of being immodest I will say that I
> think I have done so.
>
> For linear mixed models the big difference between lme and lmer is
> that lmer can handle models with crossed or partially crossed random
> effects efficiently.  In areas like the analysis of scores on
> state-wide tests mandated by the No Child Left Behind act or in the
> analysis of "breeding values" by animal scientists, large data sets
> with a complex structure are the norm.  The state-of-the-art methods
> for sparse matrices that are used in lmer make fitting mixed models to
> such data feasible.
>
> In the process of examining linear mixed models in detail I eventually
> arrived at a formulation of the linear mixed model that generalizes to
> generalized linear mixed models and nonlinear mixed models in a way
> that, to me at least, makes sense.  The basic computational steps and
> the representation of the models are sufficiently similar to share a
> lot of the code.
>
> Thus the lme4 package is not an upgrade of the nlme package.  It
> shares some characteristics with nlme but it is best to think of it as
> a reimplementation, not an extension.
>
> By the way, the theoretical development that I mentioned will be
> described in the vignette "Theory and Computational Methods for Mixed
> Models".  The first few sections of that vignette are included in the
> lme4_0.99875-1 package and there is more to come.  Sorry for releasing
> snapshots of a paper (or a book chapter, in this case).  Because the
> document changes between releases of the package there is not a stable
> reference to cite.  Once I have covered all the ground I want to cover
> I will release a stable version as a technical report.
>
> On 6/13/07, Robert Kushler <kushler at oakland.edu> wrote:
>
>> Follow-up to previous message (included below):
>
>> I discovered that "confint" can be used in place of "intervals", so item
>> 2
>> is taken care of.
>
> There is a "confint" method for "lmList" objects (also discussed
> below).  I'm not sure I will produce one for "lmer" objects if the
> degrees of freedom controversy is not resolved.  Also I don't think
> that symmetric intervals on variance components make sense and would
> prefer not to produce them.
>
> Generally I recommend using mcmcsamp to produce "confidence intervals"
> on the variance components.  Approximate (and symmetric) confidence
> intervals on the fixed effects are reasonably accurate but such
> intervals for the random effects can be poor approximations.
>
>> I also gather that you allow (expect?) users to use the
>> two libraries simultaneously.  I assume you recommend loading nlme
>> first,
>> so the lmer version of common functions will be used.
>
> No, I don't think it is a good idea to use nlme and lme4
> simultaneously.   At one point I had code in lme4 to give a warning if
> you tried to load the lme4 package when the nlme package was already
> loaded.
>
>> The windows version of 0.99875-1 appeared today.
>
> Good.
>
>> I have a small bit of user feedback:  lmList requires the "data"
>> argument
>> even when all of the needed variables exist in the global environment.
>> I know it's good practice to construct a data frame (and advantageous to
>> exploit the groupedData structure), but since lmer lets the user be
>> lazy,
>> why doesn't lmList do the same?
>
> I haven't looked at that version of lmList for a long time.  I suppose
> that I am not generating a call to 'model.frame' and I should.  I can
> put that on the "To Do" list but it will be a long way down the list.
>
> The answer to another of your questions about using groupedData
> objects is that I do not encourage the use of groupedData objects with
> lme4.  They were an attempt to incorporate information on the grouping
> structure with the data but they got too complicated.  I'm in a "keep
> it simple" mode in the development of the lme4 package.  Also, the
> groupedData objects use S3 classes and one of the purposes of the lme4
> package is to explore the use of S4 classes and methods.
>
> Another aspect of the groupedData objects is that the formulation,
> like the formulation of lme itself, makes sense for nested groupings
> but not for crossed or partially crossed groupings.
>
> To some extent the purpose of defining the groupedData classes was to
> make it easier to generate meaningful and informative graphics with
> the trellis functions in S-PLUS.  Deepayan Sarkar's development of
> lattice package for R has incorporated so many new features that many
> of the operations that required custom code in trellis are possible
> transparently in lattice.
>
>> Thanks again for whatever time you can spare,   Rob Kushler
>
>> Text of previous message:
>
>> I am belatedly transitioning to lme4 and have some questions.  Some of
>> the following are things I used to show my students with the nlme
>> library
>> but have not yet figured out how to do with lme4.  If some of this is on
>> your to-do list, perhaps you could post a message to [R-sig-ME] about
>> your
>> plans.  (I am eagerly awaiting the appearance of the 0.99875-1 zip
>> file.)
>
> Ahem, I believe you mean the nlme *package*.  (Bear in mind that
> Martin Maechler reads this list. :-)
>
>> My general question is whether any of the following can be done in the
>> current
>> version of lme4 (and if so how), or if these features might be added
>> ("soon",
>> not so soon, or never).  I know that I can show my students a "mixed"
>> (sic)
>> strategy using both the old and new libraries, but that's a bit awkward,
>> and
>> I'd like to convince them (and myself) that we can someday leave nlme
>> behind
>> with no (or few) regrets.
>
>> If you have addressed these issues elsewhere, forgive me for not
>> locating it.
>
>> 1) The "correlation" and "weights" arguments of gls and lme were very
>> useful.
>>     In particular, I'd like to fit the AR1 and "unstructured" models,
>> and be
>>     able to "model heteroscedasticity".
>
> The correlation and weights arguments haven't made it past the "keep
> it simple" filter yet.
>
>> 2) I understand your position on Wald inference for variance components,
>> but
>>     the "intervals" function had other uses.  In particular, I like to
>> show
>>     the students "plot(intervals(lmListobject)) as a diagnostic tool.
>> Do you
>>     no longer recommend this?
>
> I do think that a plot of the confidence intervals from an lmList
> object is a good idea.  (For one thing, those confidence intervals are
> well-defined.)  I have code that looks like
>
> ###################################################
> ### chunk number 3: Sl1
> ###################################################
> print(plot(confint(lmList(Reaction ~ Days | Subject, sleepstudy),
>                    pooled = TRUE), order = 1))
>
> and it appears to work in current versions of lme4.
>
>> 3) Do you plan to give the inference tools in "languageR" an official
>> blessing
>>     by incorporating them into lme4?
>
> I certainly think they are a good idea and I worked with Harald Baayen
> a bit on the development of them last January when I visited with him.
>  However, I still want to think through the inference issues before
> incorporating my final word about inference in the lme4 package.  One
> of the annoying aspects others find in working with me is that I am
> very slow on design and theoretical development.  I like to think that
> the good side of this is that I am thorough.
>
> The lme4 package and the theoretical work behind it have had a long
> gestation period - one that probably will extend for a long time to
> come.   However, I like the results and I am comfortable with the idea
> that I have carefully considered many, many aspects of the model and
> computational methods before incorporating them.
>
>> 4) Your R-news article in May 2005 shows "anova" producing F's and
>> p-values,
>>     but that no longer seems to be the case.
>
> Indeed.  I realize that calculation of p-values is immensely important
> to some people but doing it properly for unbalanced designs is
> extremely complicated and, in the areas that I am most concerned with
> (large data sets with complex structure), it's a non-issue.  When you
> have data on a half-million students in 2500 schools whatever
> calculation you use for the degrees of freedom will produce a number
> so large that the exact number is irrelevant.
>
> If someone wants to work out how to calculate an approximate
> denomination degrees of freedom, and they can get the "degrees of
> freedom police" (John Maindonald and Peter Dalgaard) to approve the
> result, I'll put the degrees of freedom back in.  Until then, you just
> have to do what several people in the psycholinguistics area have
> done, which is to uncomment the lines for the denominator degrees of
> freedom upper bound in the code for the anova method.
>
>>     In this connection, do you have
>>     plans to implement the "potentially useful approximate D.F."
>> mentioned
>>     in mcmcsamp.R?
>
> I'm not sure what code you are referring to there.  Is it in the
> languageR package?
>
> I imagine the approximation in question is from the trace of the "hat"
> matrix.  If you look at the Theory vignette you will see I view the
> basic calculation in mixed models as the solution of a penalized least
> squares problem.  For an unpenalized least squares problem the
> residual degrees of freedom is n minus the trace of the hat matrix.
> Frequently in penalized least squares problems the same approximation
> is used.
>
> That quantity also appears in the definition of the Deviance
> Information Criterion (DIC) by Spiegelhalter et al. for this model.
> However, it does not reproduce the results from balanced experiments
> with small data sets and I am beginning to have some doubts about
> whether this calculation is the best way to go.
>
>> 5) Do you no longer recommend the use of groupedData objects, or do you
>> plan
>>     to add them to lme4?
>
> See above.
>
>> I'm sure I'll think of additional questions, but that's (more than)
>> enough
>> for now.
>
>> Thanks and regards,   Rob Kushler (Oakland University)
>
> Thanks for your questions.  I hope my answers were helpful.
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Sun Jun 17 16:15:55 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 17 Jun 2007 09:15:55 -0500
Subject: [R-sig-ME] more comments and questions
In-Reply-To: <4672D70B.7060409@oakland.edu>
References: <4672D70B.7060409@oakland.edu>
Message-ID: <40e66e0b0706170715o4d58f770h68ebac5fdb4857@mail.gmail.com>

As we agreed i am sending a copy of this reply to the R-SIG-Mixed-Models list

On 6/15/07, Robert Kushler <kushler at oakland.edu> wrote:

> Further follow-up:

> 1) "mcmcsamp.R" is in the R-ex directory of the lme4 libr.. oops package
>     (hey, it's stored under "library" and you have to call the "library"
>      function to load it - can Martin blame us for slipping up?  :-)

Ah, I see.  In that case it is easier to refer to it as being in

example(mcmcsamp)

>     Here's the line I was referencing:

> ## potentially useful approximate D.F. :
> (eDF <- mean(samp1[,"deviance"]) - deviance(fm1, REML=FALSE))

That statement refers to the Deviance Information Criterion (DIC) of
Spiegelhalter et al.  As I said in my earlier reply I am having my
doubts about the value of that criterion for this model - hence the
phrase "potentially useful".

> 2) Thanks for clarifying the "deprecation" of groupedData objects.  Here's
>     why I was surprised by the behavior of "lmList" (quote from lme4 help
>     page on lmList):

> Arguments

> formula For lmList, a linear formula object of the form y ~ x1+...+xn | g.
>          In the formula object, y represents the response, x1,...,xn the covariates,
>          and g the grouping factor specifying the partitioning of the data according
>          to which different lm fits should be performed. The grouping factor g may be
>          omitted from the formula, in which case the grouping structure will be obtained
>          from data, which must inherit from class groupedData.

Thanks for pointing that out.  The sources for the help page were
copied from the lmList function in the nlme package and insufficiently
edited.  I'll correct that.

> 3) I know you have many high-priority items on the to-do list, but I want to pitch
>     the idea of adding the "extensions" of chapter 5 of Pinheiro and Bates sooner
>     rather than later.  The AR1 structure in particular is a very plausible and
>     useful model that seems to occur fairly often in applications - the weights data
>     from the SAS mixed book (first edition) provides one example.

This is a case where the design, in the sense of deciding how these
add-on's should be specified by the user, is much more difficult than
the implementation.

One of the aspects of lmer and nlmer that I particularly like,
relative to lme and nlme ,is the model specification.  I think the new
specification is much simpler.  In the process of rethinking the model
specification I have eliminated the pdMat classes and all of the
methods associated with them.  At the same time the new formulation
allows for specification of models with crossed or partially crossed
random effects in what i feel is a simple and straightforward way.

My priorities now are to finish and document lmer, nlmer, glmer and
mcmcsamp.  The "documentation" part includes finishing a book with the
working title "Multilevel Modeling in R".

So I'm sorry to say that, barring unforeseen developments, having the
ability to specify and fit correlation structures in addition to those
induced by the random effects in not about to happen soon.

> 4) Can the "unstructured" model be fit using the current version of lmer?  I thought
>     I could do it (at least for balanced data) by using a factor version of the "inner"
>     data structure (i.e., the repeated measures) as a random effect, but I can't get
>     it to work.  Any advice you can offer would be appreciated.

I'm not sure what the "unstructured" model is.  If you are referring a
model for repeated measures data based on, say, factors "occasion" and
"subject", then I believe you want a random-effects term of the form

(occasion|subject)

or, equivalently,

(0+occasion|subject)

The parameterization of the random effects and the variance-covariance
matrix from the second specification may be easier to interpret.

By the way, the SAS-speak term "unstructured variance matrix" is
nonsense.  An "unstructured, symmetric, positive-definite matrix" is
an oxymoron.

My approach to mixed-model specification is different from that in SAS
and in many text books.  Instead of allowing for complex structures of
the variance-covariance matrix of the random effects or, even worse
from my point of view, the induced variance-covariance matrix of the
responses, I try to keep the variance-covariance matrix's structure
simple and move the complexity, when needed, to the structure of the
model matrices for the linear predictor.
Basically, if you can express the model in terms of model matrices
plus a variance-covariance structure for the random effects where
elements associated with different terms are independent as are
elements associated with different levels of the same term, then you
can write the model in lmer.

> 5) You may not want to answer this, but I'll ask anyway.  The listing produced by

>            getMethod(anova,signature="mer")

>     (or "lmer2") doesn't contain any commented out lines for denominator df.  Am I
>     looking in the wrong place?  Can you give me another hint?

Ah, I see I left out a couple of steps.  Let me be more specific.

Step 1: Take your Windows computer and install Linux on it.
Step 2: Install R under Linux.
Step 3: Download the source package for lme4 and expand the tar file.
Step 4: Find the commented lines in lme4/R/lmer.R and uncomment them.
Step 5: Install the lme4 package.

You could avoid step 1 if you go through the steps necessary to
compile an R source package, including C source files, under Windows.
Once you have done so you would probably agree that converting to
Linux is easier.

> Thanks again for your time (and for all the terrific software).

> Regards,    Rob Kushler



From Sofie.VanGijsel at arts.kuleuven.ac.be  Tue Jun 19 14:59:36 2007
From: Sofie.VanGijsel at arts.kuleuven.ac.be (Sofie Van Gijsel)
Date: Tue, 19 Jun 2007 14:59:36 +0200
Subject: [R-sig-ME] nlme4 vs. nlme question
Message-ID: <20070619125937.2CDCEF3862@smtps02.kuleuven.be>


>Dear List members,
>
>I have a question regarding the difference between nlme & lme4 to
>which I don't seem to find an answer in the previous posts about this topic.
>
>I fitted a mixed-effects model with the different observations in my
>dataset as random effect, in a by-subject analysis. Basically, I have
>the following model:
>
>set_dimb.lmer_pois <- lmer(type_wf  ~dim + region + edu + sex +
>(1|subcorp), family = "poisson", data = set_dimb)
>
>In this model, dim, region, edu & sex are all categorical variables.
>subcorp = the subcorpora or observations in the dataset. Type_wf is
>the "number of types per subcorpus".
>
>With nlme, this worked fine, and plotting the ranefs gives insight in
>which subcorpora behave in an anomolous way.
>However, if I attempt to do the same with lme4, the error message
>tells me that the model cannot fit:
>
>Error in lmerFactorList(formula, mf, fltype) :
>          number of levels in grouping factor(s) 'subcorp' is too large
>
>I think the problem might be that for different combinations of the
>factor levels, I have more than one sample, so for example for one
>level of dim, region, education & sex, the dataset contains several
>subcorpora. In fact, if I include the subcorpus types as random
>effect (so not on the individual level of the subcorpora but on the
>'higher' level of the different types of combinations of the
>independent variables), the analysis does work and gives 
>interpretable results.
>
>So my question is: why does nlme allow this, but lme4 not? And if
>lme4 does not allow this analysis, is there a theoretical reason,
>viz. is it "wrong" to fit this type of by-subject analysis? Could this
>indicate a problem with the sampling method (viz. with the dataset)?
>
>I hope this is clear (I am not exactly a statistician :-)),
>Many thanks,
>
>Kind regards,
>Sofie VG


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From bates at stat.wisc.edu  Wed Jun 20 16:07:12 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 20 Jun 2007 09:07:12 -0500
Subject: [R-sig-ME] nlme4 vs. nlme question
In-Reply-To: <20070619125937.2CDCEF3862@smtps02.kuleuven.be>
References: <20070619125937.2CDCEF3862@smtps02.kuleuven.be>
Message-ID: <40e66e0b0706200707u2148f5f6g82dc23490eca5faf@mail.gmail.com>

On 6/19/07, Sofie Van Gijsel <Sofie.VanGijsel at arts.kuleuven.ac.be> wrote:

> >Dear List members,

> >I have a question regarding the difference between nlme & lme4 to
> >which I don't seem to find an answer in the previous posts about this topic.

> >I fitted a mixed-effects model with the different observations in my
> >dataset as random effect, in a by-subject analysis. Basically, I have
> >the following model:

I would say that the problem is using a model with random effects for
each observation.  The "per-observation" noise term already provides a
random effect for each observation.  For a linear mixed model or a
nonlinear mixed model this "residual variance" has a Gaussian
distribution, the same as the random effect.  Thus the random effect
variance is completely confounded with the residual variance.

In the nlme package there was no check on this situation and the
algorithm would apparently converge to an estimate of the variance
components.  However, this convergence would be spurious because any
combination of the residual variance and the random effect variance
that had the same sum would produce the same predictions and the same
value of the log-likelihood.

For the lme4 package I added a check on the number of levels of the
grouping factor for a random effects term and throw an error if that
equals the number of observations.  This is not declared to be an
error in a generalized linear mixed model with a binomial family.

Someone with more experience in mathematical statistics may be able to
tell us if this should or should not be an error for the Poisson
family.  It seems that this may not be necessarily be a case of
confounding of two variance components because the mean of the Poisson
determines the variance.

> >set_dimb.lmer_pois <- lmer(type_wf  ~dim + region + edu + sex +
> >(1|subcorp), family = "poisson", data = set_dimb)
> >
> >In this model, dim, region, edu & sex are all categorical variables.
> >subcorp = the subcorpora or observations in the dataset. Type_wf is
> >the "number of types per subcorpus".

I have created version 0.99875-2 of the lme4 package with this error
removed for the Poisson family.  I have tested the new version under
Linux.  Once it has been tested on Windows I will upload it to CRAN.
A preview version of the source package is at

http://www.stat.wisc.edu/~bates/lme4_0.99875-2.tar.gz

and it will be joined by the Windows binary package at

http://www.stat.wisc.edu/~bates/lme4_0.99875-2.zip

when the package has been compiled for Windows.  If you are running
under Windows you will need to download the .zip file and save it to
disk then select Packages->Install from local zip file  (or some name
like that, I don't use Windows so I am writing this from memory) to
install it.

> >With nlme, this worked fine, and plotting the ranefs gives insight in
> >which subcorpora behave in an anomolous way.
> >However, if I attempt to do the same with lme4, the error message
> >tells me that the model cannot fit:
> >
> >Error in lmerFactorList(formula, mf, fltype) :
> >          number of levels in grouping factor(s) 'subcorp' is too large
> >
> >I think the problem might be that for different combinations of the
> >factor levels, I have more than one sample, so for example for one
> >level of dim, region, education & sex, the dataset contains several
> >subcorpora. In fact, if I include the subcorpus types as random
> >effect (so not on the individual level of the subcorpora but on the
> >'higher' level of the different types of combinations of the
> >independent variables), the analysis does work and gives
> >interpretable results.
> >
> >So my question is: why does nlme allow this, but lme4 not? And if
> >lme4 does not allow this analysis, is there a theoretical reason,
> >viz. is it "wrong" to fit this type of by-subject analysis? Could this
> >indicate a problem with the sampling method (viz. with the dataset)?
> >
> >I hope this is clear (I am not exactly a statistician :-)),
> >Many thanks,
> >
> >Kind regards,
> >Sofie VG
>
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Wed Jun 20 16:16:41 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 20 Jun 2007 09:16:41 -0500
Subject: [R-sig-ME] nlme4 vs. nlme question
In-Reply-To: <40e66e0b0706200707u2148f5f6g82dc23490eca5faf@mail.gmail.com>
References: <20070619125937.2CDCEF3862@smtps02.kuleuven.be>
	<40e66e0b0706200707u2148f5f6g82dc23490eca5faf@mail.gmail.com>
Message-ID: <40e66e0b0706200716x48611688m22634ad824578490@mail.gmail.com>

On 6/20/07, Douglas Bates <bates at stat.wisc.edu> wrote:
...

> A preview version of the source package is at
>
> http://www.stat.wisc.edu/~bates/lme4_0.99875-2.tar.gz
>
> and it will be joined by the Windows binary package at
>
> http://www.stat.wisc.edu/~bates/lme4_0.99875-2.zip
>
> when the package has been compiled for Windows.

Both the source and the Windows binary packages are now available at
those URL's.  Let me know if there are any difficulties.



From mchaudhari at deltadentalwa.com  Thu Jun 21 00:26:59 2007
From: mchaudhari at deltadentalwa.com (Chaudhari, Monica)
Date: Wed, 20 Jun 2007 15:26:59 -0700
Subject: [R-sig-ME] nlme4 vs. nlme question
In-Reply-To: <20070619125937.2CDCEF3862@smtps02.kuleuven.be>
References: <20070619125937.2CDCEF3862@smtps02.kuleuven.be>
Message-ID: <06C1E76E03FE9C4B85BFA9C75365D9DA090AAC69@tiger.deltadentalwa.com>

I think, the problem is that the variable 'subcorp' is not defined as a
factor variable. You could check it using is.factor(set_dimb$subcorp).
If it returns False, then you could make it a factor variable by using
set_dimb$subcorp<-as.factor(set_dimb$subcorp)
Now try using lmer. It should work.

Thanks,
Monica

-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org
[mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Sofie Van
Gijsel
Sent: Tuesday, June 19, 2007 6:00 AM
To: R-SIG-Mixed-Models at r-project.org
Subject: [R-sig-ME] nlme4 vs. nlme question


>Dear List members,
>
>I have a question regarding the difference between nlme & lme4 to
>which I don't seem to find an answer in the previous posts about this
topic.
>
>I fitted a mixed-effects model with the different observations in my
>dataset as random effect, in a by-subject analysis. Basically, I have
>the following model:
>
>set_dimb.lmer_pois <- lmer(type_wf  ~dim + region + edu + sex +
>(1|subcorp), family = "poisson", data = set_dimb)
>
>In this model, dim, region, edu & sex are all categorical variables.
>subcorp = the subcorpora or observations in the dataset. Type_wf is
>the "number of types per subcorpus".
>
>With nlme, this worked fine, and plotting the ranefs gives insight in
>which subcorpora behave in an anomolous way.
>However, if I attempt to do the same with lme4, the error message
>tells me that the model cannot fit:
>
>Error in lmerFactorList(formula, mf, fltype) :
>          number of levels in grouping factor(s) 'subcorp' is too large
>
>I think the problem might be that for different combinations of the
>factor levels, I have more than one sample, so for example for one
>level of dim, region, education & sex, the dataset contains several
>subcorpora. In fact, if I include the subcorpus types as random
>effect (so not on the individual level of the subcorpora but on the
>'higher' level of the different types of combinations of the
>independent variables), the analysis does work and gives 
>interpretable results.
>
>So my question is: why does nlme allow this, but lme4 not? And if
>lme4 does not allow this analysis, is there a theoretical reason,
>viz. is it "wrong" to fit this type of by-subject analysis? Could this
>indicate a problem with the sampling method (viz. with the dataset)?
>
>I hope this is clear (I am not exactly a statistician :-)),
>Many thanks,
>
>Kind regards,
>Sofie VG


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


#########################################################
The information contained in this e-mail and subsequent attachments may be privileged, 
confidential and protected from disclosure.  This transmission is intended for the sole 
use of the individual and entity to whom it is addressed.  If you are not the intended 
recipient, any dissemination, distribution or copying is strictly prohibited.  If you 
think that you have received this message in error, please e-mail the sender at the above 
e-mail address.
#########################################################



From dieter.menne at menne-biomed.de  Fri Jun 22 11:28:33 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 22 Jun 2007 09:28:33 +0000 (UTC)
Subject: [R-sig-ME] Questions on migrating from nlme to lme4
References: <40e66e0b0706131025g7920b2a0n91e6d513b66c21c3@mail.gmail.com>
Message-ID: <loom.20070622T112329-758@post.gmane.org>

Douglas Bates wrote:

> Generally I recommend using mcmcsamp to produce "confidence intervals"
> on the variance components.  Approximate (and symmetric) confidence
> intervals on the fixed effects are reasonably accurate but such
> intervals for the random effects can be poor approximations.

Problem is that referees who don't read the regular Douglas B columns tend to
say "mcmc ... ha?", and, after explanation, 'we do not publish poker games' (<-
slightly paraphrased from the original comment).

Dieter



From maechler at stat.math.ethz.ch  Fri Jun 22 18:05:29 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 22 Jun 2007 18:05:29 +0200
Subject: [R-sig-ME] Questions on migrating from nlme to lme4
In-Reply-To: <loom.20070622T112329-758@post.gmane.org>
References: <40e66e0b0706131025g7920b2a0n91e6d513b66c21c3@mail.gmail.com>
	<loom.20070622T112329-758@post.gmane.org>
Message-ID: <18043.62281.715620.174754@stat.math.ethz.ch>

>>>>> "DM" == Dieter Menne <dieter.menne at menne-biomed.de>
>>>>>     on Fri, 22 Jun 2007 09:28:33 +0000 (UTC) writes:

    DM> Douglas Bates wrote:
    >> Generally I recommend using mcmcsamp to produce "confidence intervals"
    >> on the variance components.  Approximate (and symmetric) confidence
    >> intervals on the fixed effects are reasonably accurate but such
    >> intervals for the random effects can be poor approximations.

    DM> Problem is that referees who don't read the regular Douglas B columns tend to
    DM> say "mcmc ... ha?", and, after explanation, 'we do not publish poker games' (<-
    DM> slightly paraphrased from the original comment).

Hmm, we are getting off-topic here, but I think these referees
are not quite fit for the 21st century.

MANY modern statistical procedures depend on random numbers to
some extent:

- neural nets  solve a high dimensional minimization and the
  solution depends on the random starting values.
  {Some silly people would therefore always use the same random
  seed before starting the nnet}

- The good old K-Means algorithm very often starts with random
  centers {and again: people use versions of the algorithm that, e.g.,
  	  always start with same indices of observations as starting
	  values ==> their algorithm depends on the *ordering*
	  of the observations --- which I think is worse}
  
- All high-breakdown robust statistics procedures ...

- All K-fold cross-validation  ...

- All bootstrapping / bagging / bragging / ...

depend on random (sub)sampling.

If referees ask researchers to refrain from all such methods, 
a good journal editor should switch referees,
or a good author should switch to a better journal  :-)

Martin



From dieter.menne at menne-biomed.de  Fri Jun 22 19:03:34 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 22 Jun 2007 17:03:34 +0000 (UTC)
Subject: [R-sig-ME] Questions on migrating from nlme to lme4
References: <40e66e0b0706131025g7920b2a0n91e6d513b66c21c3@mail.gmail.com>
	<loom.20070622T112329-758@post.gmane.org>
	<18043.62281.715620.174754@stat.math.ethz.ch>
Message-ID: <loom.20070622T184744-348@post.gmane.org>

Martin Maechler <maechler at ...> writes:

 
> Hmm, we are getting off-topic here, but I think these referees
> are not quite fit for the 21st century.

Only partially off-topic, because I think it's important that some accepted
solution in the df discussion of lmer should be found. I rarely had problems
with professional statistical referees in Journals like Lancet or B.Journal of
Dermatology; or if so, they had damned goods reasons to complain.

The problem is that you rarely publish in such high-ranked journals with
statistical referees. Normally you get trouble with medical referees who argue
in a complex cross-over design with several nested repeats "why don't you give
simple standard error of mean instead of these incomprehensible stuff?". For
example, at the University Hospital around your corner.

> 
> MANY modern statistical procedures depend on random numbers to
> some extent:
...
... True, but
 
> If referees ask researchers to refrain from all such methods, 
> a good journal editor should switch referees,
> or a good author should switch to a better journal  

If you publish in statistical journals, no problem. If you publish statistics in
a medical journal: good luck. They want Michelin-guide style *** tables.

Dieter



From Cougar_711 at msn.com  Sat Jun 23 00:21:18 2007
From: Cougar_711 at msn.com (Cougar)
Date: Fri, 22 Jun 2007 18:21:18 -0400
Subject: [R-sig-ME] lsmeans
Message-ID: <BAY144-DAV1694BA43F2711121FFE3F9A0170@phx.gbl>

I was wondering if someone could explain how to obtain LSmeans from lmer.  I
could not find any information in the R-archives.

Respectfully,

Frank Lawrence



From HStevens at muohio.edu  Sat Jun 23 14:39:02 2007
From: HStevens at muohio.edu (Martin Henry H. Stevens)
Date: Sat, 23 Jun 2007 08:39:02 -0400
Subject: [R-sig-ME] lsmeans
In-Reply-To: <BAY144-DAV1694BA43F2711121FFE3F9A0170@phx.gbl>
References: <BAY144-DAV1694BA43F2711121FFE3F9A0170@phx.gbl>
Message-ID: <9A45B32E-3998-46B8-B3F2-D642C7743F96@muohio.edu>

Dear Frank,
As lmer doesn't use least squares means, I am not sure they are  
readily obtained. Predicted values may be obtained in several ways,  
including the following from Spencer Graves:

### From Spencer Graves
### For getting the predicted values from the fixed effects only.
predict.lmer <- function(object, X){
## From: Spencer Graves <spencer.graves>
## Date: Sun, 07 May 2006 08:07:07 -0700
# object has class "lmer"
# X = model matrix with columns
# matching object at X
    if(missing(X))
      X <- object @ X
    b <- fixef(object)
    X %*% b
}

Cheers,
Hank
On Jun 22, 2007, at 6:21 PM, Cougar wrote:

> I was wondering if someone could explain how to obtain LSmeans from  
> lmer.  I
> could not find any information in the R-archives.
>
> Respectfully,
>
> Frank Lawrence
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From bates at stat.wisc.edu  Sat Jun 23 17:30:28 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 23 Jun 2007 10:30:28 -0500
Subject: [R-sig-ME] lsmeans
In-Reply-To: <9A45B32E-3998-46B8-B3F2-D642C7743F96@muohio.edu>
References: <BAY144-DAV1694BA43F2711121FFE3F9A0170@phx.gbl>
	<9A45B32E-3998-46B8-B3F2-D642C7743F96@muohio.edu>
Message-ID: <40e66e0b0706230830n72085a95h6391f7e47093f395@mail.gmail.com>

On 6/23/07, Martin Henry H. Stevens <HStevens at muohio.edu> wrote:
> Dear Frank,
> As lmer doesn't use least squares means, I am not sure they are
> readily obtained. Predicted values may be obtained in several ways,
> including the following from Spencer Graves:

> ### From Spencer Graves
> ### For getting the predicted values from the fixed effects only.
> predict.lmer <- function(object, X){
> ## From: Spencer Graves <spencer.graves>
> ## Date: Sun, 07 May 2006 08:07:07 -0700
> # object has class "lmer"
> # X = model matrix with columns
> # matching object at X
>     if(missing(X))
>       X <- object @ X
>     b <- fixef(object)
>     X %*% b
> }

That works for class "lmer".  At the risk of confusing everyone I will
say that it doesn't work for the current "lmer2" class but it will in
the near future.

It is a dangerous practice to reach into an object and grab the
contents of slots (i.e. using the @ operator on S4 objects) or
components (using $ on S3 objects).  I continue to tweak the form of
the lmer and lmer2 objects seeking to optimize the range of
applicability, the performance and the storage requirements of the
algorithms.  Sometimes certain parts of the structure are represented
in one way and sometimes another.

The use of extractor functions such as fixef is much preferred.  I am
responsible for ensuring that the code is consistent and that such
extractor do the right thing for the current structure.
I will ensure that the model.matrix extractor works with upcoming
releases of the lme4 package and that it allows an optional argument
"which" to specify "fixed", "random" or "both".  I will also allow
such an argument for the "fitted" extractor.

> On Jun 22, 2007, at 6:21 PM, Cougar wrote:

> > I was wondering if someone could explain how to obtain LSmeans from
> > lmer.  I
> > could not find any information in the R-archives.

Thanks for checking the archive before asking, Frank.  I would
suggest, not entirely facetiously, that you also check

library(fortunes); fortune("lsmeans")

If you could explain what you want to calculate from a fitted model,
without using the term "lsmeans", I, at least, would be better able to
give advice on how to calculate it.  My big problem with lsmeans is
that I have never been able to understand how they should be
calculated and, more importantly, why one should want to calculate
them.  In other words, what do lsmeans represent and why should I be
interested in these particular values?

I have tried to understand what they are.  I have read the
explanations and looked at the formulas in, e.g. "SAS System for Mixed
Models".  Anyone who can understand that explanation and decide how
the corresponding quantities should be formulated in the
representation used in lmer is welcome to explain it to me.

It may seem surprising that I say "how the corresponding quantities
are calculated".  Many people believe that the concept corresponds to
a formula and all one needs to do is to "code up the formula".  That's
not the case.

Pardon me while I digress a bit.

The statistical concept represented by parameter estimates must be
associated with some kind of an objective function, such as "sum of
squares of residuals" or "the log-likelihood", and there are many
routes that the person designing the algorithm can take when
optimizing the objective function to obtain the parameter estimates.
This also applies to the representation of the model as data
structures.  The data structures used in lmer are very different from
those used by other software for fitting mixed models and they did not
come about by chance.  They are the result of intensive study and
extensive experimentation.  I can (and do) explain what the structures
are and how they are used but if you say, e.g. "evaluate this summary
quantity using G-inverse" that doesn't work for me because I don't
have a G matrix.  I need to go back to an explanation of what does it
represent before I can decide how to evaluate it.

This doesn't mean that everyone gets to make up their own estimation
criterion.  Consistency between implementations is a matter of
agreeing on the objective function and checking that it is evaluated
consistently.  In other words lmer should agree with SAS PROC MIXED
and HLM and MLWin and ... on what the log-likelihood or the REML
criterion is for a given model, data, parameter-value combination.
However the different implementations do not need to agree on how they
represent the information from the model and the data nor on how they
optimize the objective.

I'm sorry for such a long-winded response to your simple question.  As
you might guess I have been thinking a lot about the general issues
because I am writing the "Theory and computational methods for mixed
models" document that explains exactly how the log-likelihood in lmer,
and the Laplace approximation to the log-likelihood in nlmer and
glmer, are defined and evaluated.  One of the interesting things about
the description is that it does not deal with how the log-likelihood
or its approximation is to be optimized.  Not so much for linear mixed
models but definitely for nonlinear mixed models and generalized
linear mixed models, people confuse the algorithm and the objective.
I have been participating in a discussion (http://www.nlme.org/) of
some draft specifications for software to fit nonlinear mixed models
to population pharmacokinetic and pharmacodynamic data.  The draft
specifications state that the software should implement a variety of
methods that go by names like FO (first-order), FOCE (first-order
conditional estimates), ...  I claim that you can't tell if you have
such a method implemented properly because those methods (even the
Lindstrom-Bates method, which I know well) just tell you how you go
from one set of parameter estimates to the next.  Different
implementations, or even the same implementation but using different
hardware and systems software, may produce different answers and who
can say which is the best answer.  I advocate agreeing on a "gold
standard" evaluation of the likelihood which can be evaluated based
solely on the model and the data and parameter values.  That is, it
does not depend on how you got to those parameter values.  Then you
can compare different sets of estimates.

The situation for generalized linear mixed models is similar.  People
speak of PQL or PQL1 or PQL2 estimates as if these names could
meaningfully be applied to estimates.  These names refer to methods of
evaluating an iterative step, not an objective.  (For generalized
linear models without random effects you may say that people refer to
IRLS estimates and that just describes an iterative step.  However,
one can demonstrate that a fixed point of the IRLS iterations is the
maximum likelihood estimate so IRLS is justified on the basis of
optimizing an objective function, which can be, and is, evaluated at
the converged values.)

Returning to lsmeans, as best I can understand it, these are an
attempt to characterize a "typical" fitted response for a level of a
factor in the model, even when other terms in the model represent
interactions with that factor.  If that is the case, and let me
reiterate that I don't claim to understand exactly what they are
supposed to be, then I would claim that you need to be awfully damned
careful about what you do regarding those interactions.  There is no
universally applicable way of defining such a beast.  You must
carefully state how you are going to marginalize over the levels of
other factors involved in the interactions, which, of course, implies
that you first need to think about the marginalization and decide how
it should be done in your particular situation.

Sorry for going on at such length.  Please don't interpret the length
of my response as a criticism of your question.  I realize that those
applying statistical methods in their research to be published in
their field's literature need things like p-values and lmeans and so
on, to satisfy editors and referees in their field who believe that
these values are perfectly well-defined.  Unfortunately, I'm a purist
with extensive training in mathematics  and I only put things in my
software when I am satisfied that they are meaningful and useful and
evaluated the best way I know or can derive.  This is one of the great
pleasures of working on Open Source software.  The R Project doesn't
have a marketing department or senior management to tell us, "It
doesn't matter what you think, customers claim they need Type III sums
of squares so you have to add them whether you like it or not".  To
paraphrase Bob Dylan,  "When you ain't paid nothing, you got nothing
to lose."



From kw.statr at gmail.com  Mon Jun 25 10:42:05 2007
From: kw.statr at gmail.com (Kevin Wright)
Date: Mon, 25 Jun 2007 10:42:05 +0200
Subject: [R-sig-ME] lsmeans
In-Reply-To: <40e66e0b0706230830n72085a95h6391f7e47093f395@mail.gmail.com>
References: <BAY144-DAV1694BA43F2711121FFE3F9A0170@phx.gbl>
	<9A45B32E-3998-46B8-B3F2-D642C7743F96@muohio.edu>
	<40e66e0b0706230830n72085a95h6391f7e47093f395@mail.gmail.com>
Message-ID: <c968588d0706250142nbf4291nfc6e8f797eab1e1@mail.gmail.com>

As a statistician, I need to be careful to fit a good model to data.
"Good" can have varied meanings from "it didn't crash" to "it
completed in a tolerable amount of time" to "the variance parameters
look like what I was expecting to see" to many other criteria.
Modeling the data well can be very time-consuming, especially for
complicated data, and almost always involves finding a balance between
writing down a theoretical model and finding a model that is useful,
meaningful, and calculable.

My clients, however, don't care a whit about models.  They care about
making decisions, and the second part of my job after fitting a model
is to provide information to facilitate decision making.  The client
might say, " I have three treatments.  I don't care about significant
differences.  I have to pick one of the three, which one is my best
choice?"  Often that means providing some type of overall estimate of
the treatment effects and standard errors.  There is a nice paper that
discusses some of the art of making predictions:

@Article{Welham2004,
  author =       {Sue Welham and Brian Cullis and Beverley Gogel and Arthur
                  Gilmour and Robin Thompson},
  title =        {Predictions in Linear Mixed Models},
  journal =      {Aust. N. Z. J. Stat.},
  year =         2004,
  pages =        {325--347}
}

I called it an art, because as Doug mentioned, you have to be very
careful about what you mean by an "lsmean" or any other type of
prediction.  For example, here are three ways (not necessarily for
identical models) to calculate one particular estimate:

In PROC MIXED:
estimate "Trt1 BLUP narrow" intercept 3 |trt 3 0 0 0 0
                    loc*trt 1 0 0 0 0 1 0 0 0 0 1 0/divisor=3;

In ASREML:
predict trt  !average loc*trt  # Narrow inference space predictions

In lme
coefs=unlist(ranef(m1=lme(y ~ trt, random=~loc))
coefs[1:5]+c(mean(coefs[c(6,11,16)]),
                mean(coefs[c(7,12,17)]),
                mean(coefs[c(8,13,18)]),
                mean(coefs[c(9,14,19)]),
                mean(coefs[c(10,15,20)])) + mean(dat$y)

These three methods have pros and cons.  Extracting the right
coefficients in MIXED and lme can be excruciatingly tedious (even for
simple models), but a good rite of passage to force you to think about
what you are doing.  After learning that, ASREML provides a cleaner
method (as does PROC GLIMMIX, I believe).

It behooves decision makers to remember that the the theoretical and
computational aspects of mixed models are quite complex.  It behooves
the software developers to help people fit models and then help them
make decisions.

K Wright



On 6/23/07, Douglas Bates <bates at stat.wisc.edu> wrote:
> On 6/23/07, Martin Henry H. Stevens <HStevens at muohio.edu> wrote:
> > Dear Frank,
> > As lmer doesn't use least squares means, I am not sure they are
> > readily obtained. Predicted values may be obtained in several ways,
> > including the following from Spencer Graves:
>
> > ### From Spencer Graves
> > ### For getting the predicted values from the fixed effects only.
> > predict.lmer <- function(object, X){
> > ## From: Spencer Graves <spencer.graves>
> > ## Date: Sun, 07 May 2006 08:07:07 -0700
> > # object has class "lmer"
> > # X = model matrix with columns
> > # matching object at X
> >     if(missing(X))
> >       X <- object @ X
> >     b <- fixef(object)
> >     X %*% b
> > }
>
> That works for class "lmer".  At the risk of confusing everyone I will
> say that it doesn't work for the current "lmer2" class but it will in
> the near future.
>
> It is a dangerous practice to reach into an object and grab the
> contents of slots (i.e. using the @ operator on S4 objects) or
> components (using $ on S3 objects).  I continue to tweak the form of
> the lmer and lmer2 objects seeking to optimize the range of
> applicability, the performance and the storage requirements of the
> algorithms.  Sometimes certain parts of the structure are represented
> in one way and sometimes another.
>
> The use of extractor functions such as fixef is much preferred.  I am
> responsible for ensuring that the code is consistent and that such
> extractor do the right thing for the current structure.
> I will ensure that the model.matrix extractor works with upcoming
> releases of the lme4 package and that it allows an optional argument
> "which" to specify "fixed", "random" or "both".  I will also allow
> such an argument for the "fitted" extractor.
>
> > On Jun 22, 2007, at 6:21 PM, Cougar wrote:
>
> > > I was wondering if someone could explain how to obtain LSmeans from
> > > lmer.  I
> > > could not find any information in the R-archives.
>
> Thanks for checking the archive before asking, Frank.  I would
> suggest, not entirely facetiously, that you also check
>
> library(fortunes); fortune("lsmeans")
>
> If you could explain what you want to calculate from a fitted model,
> without using the term "lsmeans", I, at least, would be better able to
> give advice on how to calculate it.  My big problem with lsmeans is
> that I have never been able to understand how they should be
> calculated and, more importantly, why one should want to calculate
> them.  In other words, what do lsmeans represent and why should I be
> interested in these particular values?
>
> I have tried to understand what they are.  I have read the
> explanations and looked at the formulas in, e.g. "SAS System for Mixed
> Models".  Anyone who can understand that explanation and decide how
> the corresponding quantities should be formulated in the
> representation used in lmer is welcome to explain it to me.
>
> It may seem surprising that I say "how the corresponding quantities
> are calculated".  Many people believe that the concept corresponds to
> a formula and all one needs to do is to "code up the formula".  That's
> not the case.
>
> Pardon me while I digress a bit.
>
> The statistical concept represented by parameter estimates must be
> associated with some kind of an objective function, such as "sum of
> squares of residuals" or "the log-likelihood", and there are many
> routes that the person designing the algorithm can take when
> optimizing the objective function to obtain the parameter estimates.
> This also applies to the representation of the model as data
> structures.  The data structures used in lmer are very different from
> those used by other software for fitting mixed models and they did not
> come about by chance.  They are the result of intensive study and
> extensive experimentation.  I can (and do) explain what the structures
> are and how they are used but if you say, e.g. "evaluate this summary
> quantity using G-inverse" that doesn't work for me because I don't
> have a G matrix.  I need to go back to an explanation of what does it
> represent before I can decide how to evaluate it.
>
> This doesn't mean that everyone gets to make up their own estimation
> criterion.  Consistency between implementations is a matter of
> agreeing on the objective function and checking that it is evaluated
> consistently.  In other words lmer should agree with SAS PROC MIXED
> and HLM and MLWin and ... on what the log-likelihood or the REML
> criterion is for a given model, data, parameter-value combination.
> However the different implementations do not need to agree on how they
> represent the information from the model and the data nor on how they
> optimize the objective.
>
> I'm sorry for such a long-winded response to your simple question.  As
> you might guess I have been thinking a lot about the general issues
> because I am writing the "Theory and computational methods for mixed
> models" document that explains exactly how the log-likelihood in lmer,
> and the Laplace approximation to the log-likelihood in nlmer and
> glmer, are defined and evaluated.  One of the interesting things about
> the description is that it does not deal with how the log-likelihood
> or its approximation is to be optimized.  Not so much for linear mixed
> models but definitely for nonlinear mixed models and generalized
> linear mixed models, people confuse the algorithm and the objective.
> I have been participating in a discussion (http://www.nlme.org/) of
> some draft specifications for software to fit nonlinear mixed models
> to population pharmacokinetic and pharmacodynamic data.  The draft
> specifications state that the software should implement a variety of
> methods that go by names like FO (first-order), FOCE (first-order
> conditional estimates), ...  I claim that you can't tell if you have
> such a method implemented properly because those methods (even the
> Lindstrom-Bates method, which I know well) just tell you how you go
> from one set of parameter estimates to the next.  Different
> implementations, or even the same implementation but using different
> hardware and systems software, may produce different answers and who
> can say which is the best answer.  I advocate agreeing on a "gold
> standard" evaluation of the likelihood which can be evaluated based
> solely on the model and the data and parameter values.  That is, it
> does not depend on how you got to those parameter values.  Then you
> can compare different sets of estimates.
>
> The situation for generalized linear mixed models is similar.  People
> speak of PQL or PQL1 or PQL2 estimates as if these names could
> meaningfully be applied to estimates.  These names refer to methods of
> evaluating an iterative step, not an objective.  (For generalized
> linear models without random effects you may say that people refer to
> IRLS estimates and that just describes an iterative step.  However,
> one can demonstrate that a fixed point of the IRLS iterations is the
> maximum likelihood estimate so IRLS is justified on the basis of
> optimizing an objective function, which can be, and is, evaluated at
> the converged values.)
>
> Returning to lsmeans, as best I can understand it, these are an
> attempt to characterize a "typical" fitted response for a level of a
> factor in the model, even when other terms in the model represent
> interactions with that factor.  If that is the case, and let me
> reiterate that I don't claim to understand exactly what they are
> supposed to be, then I would claim that you need to be awfully damned
> careful about what you do regarding those interactions.  There is no
> universally applicable way of defining such a beast.  You must
> carefully state how you are going to marginalize over the levels of
> other factors involved in the interactions, which, of course, implies
> that you first need to think about the marginalization and decide how
> it should be done in your particular situation.
>
> Sorry for going on at such length.  Please don't interpret the length
> of my response as a criticism of your question.  I realize that those
> applying statistical methods in their research to be published in
> their field's literature need things like p-values and lmeans and so
> on, to satisfy editors and referees in their field who believe that
> these values are perfectly well-defined.  Unfortunately, I'm a purist
> with extensive training in mathematics  and I only put things in my
> software when I am satisfied that they are meaningful and useful and
> evaluated the best way I know or can derive.  This is one of the great
> pleasures of working on Open Source software.  The R Project doesn't
> have a marketing department or senior management to tell us, "It
> doesn't matter what you think, customers claim they need Type III sums
> of squares so you have to add them whether you like it or not".  To
> paraphrase Bob Dylan,  "When you ain't paid nothing, you got nothing
> to lose."
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Tue Jun 26 23:29:43 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 26 Jun 2007 16:29:43 -0500
Subject: [R-sig-ME] Follow-up on lme4
In-Reply-To: <6.2.1.2.2.20070626152502.03eb6730@email.psu.edu>
References: <6.2.1.2.2.20070623164417.01e52e88@email.psu.edu>
	<40e66e0b0706231416x259dd4d8h181d5e01695374a4@mail.gmail.com>
	<6.2.1.2.2.20070626143625.03db05a0@email.psu.edu>
	<40e66e0b0706261222y3fbd16c0i3e2069350c54299b@mail.gmail.com>
	<6.2.1.2.2.20070626152502.03eb6730@email.psu.edu>
Message-ID: <40e66e0b0706261429k15947422i7f3c559f7ea7c3a8@mail.gmail.com>

I thought that this reply may be of interest to the members of the
list.  Because I did not ask permission of the questioner before
sending a copy to the list, I have removed the questioner's name.

The short answer to your question is that the lme4 package is a work
in progress but it will converge to a 1.0 release "real soon".

By the 1.0 release there will not be an lmer2 function.  The
model-fitting functions will be lmer, for linear mixed models, glmer,
for generalized linear mixed models, and nlmer, for nonlinear mixed
models.  (The official pronunciations are "Elmer", "glimmer" and
"Nellmer", for those who care about such things.)

It is not that the lmer2 representation will go away; it will become
the lmer representation in the 1.0 version and will also be the
"linear mixed-effects engine" on which glmer and nlmer are built.  The
current lmer is the one that will go away.

The model specification for the 1.0 lmer, glmer and nlmer will be the
same as for the current lmer and nlmer.  (In the current version there
is not a separate function for generalized linear mixed models, you
just call lmer with a non-default "family" argument.  This behavior
will be "grandfather'ed" although all lmer will end up doing with a
glmm specification is turn around and call glmer.)

The internal representation of the fitted models will *not* be the
same as in the current version of the lme4 package.  This is why it
would be a very good idea to save the data and the model
specifications from current lmer fits.  I will make preview versions
of the package available and I would greatly appreciate if those of
you who have published results from lmer fits could check that the 1.0
version of lme4 gives a fit that is at least as good as the one you
reported.  If not, please inform me and I will do whatever I can to
ensure suitable behavior on models fit previously before I release the
1.0 version of lme4.

The reason that I have not already switched the package to the lmer2
representation is because I was having difficulty getting two pieces
of code, generalized linear mixed models and mcmcsamp, to work
properly in the new formulation.  I use two scales for the random
effects, the original scale and a transformed "orthogonal" scale, in
the lmer2 representation and at some point I must have thought of the
form of the calculation in one scale but used the other.

Eventually I decided to bite the bullet and write out the gory details
of the computational methods so I could be sure that I understood all
the steps and hadn't missed any steps when writing the code. As my
friend Steve Ziskind from graduate school said, "Every once in a while
you need to roll up your sleeves and write it out."  I expect that by
the time I am finished writing it all out and honing the steps in the
presentation, the actual writing of the code will be straightforward.
I know that the code to create the representation is very clean
relative to previous versions lmer (and certainly relative to lme and
nlme).  The development version of the code, currently broken, is at

https://svn.r-project.org/R-packages/branches/gappy-lmer/

Those of you who liked the scenes in the Frankenstein movies before
the Doctor proclaimed "It's alive!" can satisfy your curiosity there.
There's a reason why this representation can be considered to have
gaps but it would take too long to explain.

The "version du jour" of the paper explaining the theory and the
computational methods is available at

http://www.stat.wisc.edu/~bates/Mixed-Model-Theory.pdf

On 6/26/07, an lme4 user wrote:

>  Is lmer2 an integrated component of the lme4 package, or does it have to be
> downloaded separately ?

It is part of the current lme4 package.  Eventually it will become lmer.

>  Also -- as I understand it -- lme4 is intended to eventually expand the
> capabilities and performance of your older nlme library ?

Functions in the lme4 package extend the capabilities of lme and nlme
in some directions - specifically the ability to fit models with
crossed or partially crossed random effects, to fit generalized linear
mixed models and to fit nonlinear mixed models using the Laplace
approximation to the log-likelihood.  The model-fitting functions also
handle the boundary cases, corresponding to variance components of
zero or more general, but degenerate, forms of the
variance-covariance.

However, lme4 does not include some capabilities from the nlme package
- specifically the ability to model correlation structures or variance
structures in the responses in addition to those induced by the random
effects.



From mchaudhari at deltadentalwa.com  Wed Jun 27 23:46:30 2007
From: mchaudhari at deltadentalwa.com (Chaudhari, Monica)
Date: Wed, 27 Jun 2007 14:46:30 -0700
Subject: [R-sig-ME] Sample size estimation
In-Reply-To: <40e66e0b0706261429k15947422i7f3c559f7ea7c3a8@mail.gmail.com>
References: <6.2.1.2.2.20070623164417.01e52e88@email.psu.edu>
	<40e66e0b0706231416x259dd4d8h181d5e01695374a4@mail.gmail.com>
	<6.2.1.2.2.20070626143625.03db05a0@email.psu.edu>
	<40e66e0b0706261222y3fbd16c0i3e2069350c54299b@mail.gmail.com>
	<6.2.1.2.2.20070626152502.03eb6730@email.psu.edu>
	<40e66e0b0706261429k15947422i7f3c559f7ea7c3a8@mail.gmail.com>
Message-ID: <06C1E76E03FE9C4B85BFA9C75365D9DA0925D52F@tiger.deltadentalwa.com>

Hello List,

Can anybody guide me through the steps of sample size estimation for
seeing the effect (at 5% level of significance) of the difference in the
growth of outcome variable between two groups using Generalized Linear
Mixed Model with quasipoisson family. The model is as given below:

log(mu)= b0 + b1 time + b2 group + b3 time:group

Any clue will be highly appreciated.

Thanks,
Monica

#########################################################
The information contained in this e-mail and subsequent attachments may be privileged, 
confidential and protected from disclosure.  This transmission is intended for the sole 
use of the individual and entity to whom it is addressed.  If you are not the intended 
recipient, any dissemination, distribution or copying is strictly prohibited.  If you 
think that you have received this message in error, please e-mail the sender at the above 
e-mail address.
#########################################################



From HDoran at air.org  Thu Jun 28 14:46:55 2007
From: HDoran at air.org (Doran, Harold)
Date: Thu, 28 Jun 2007 08:46:55 -0400
Subject: [R-sig-ME] Sample size estimation
In-Reply-To: <06C1E76E03FE9C4B85BFA9C75365D9DA0925D52F@tiger.deltadentalwa.com>
Message-ID: <2323A6D37908A847A7C32F1E3662C80EE57882@dc1ex01.air.org>

The steps are pretty straightforward if you choose simulation.

1) Simulate data for a sample size s that meets your assumptions (e.g.,
effect size, variances, etc) based on the generating model
2) Run these data through lmer N times
3) Save the p-value at each iteration for the effect of interest in step
2. Save this as a dichotomous variable where 1 means p < .05
4) Compute the mean of the dichotomous variable in step 3. This is your
power at sample size s.
5) Repeat with a new sample size s.

With lmer, you don't get p-values. So, what I do is divide the fixed
effects by their standard errors to get the t-value and compare it to
the critical value associated with my level of significance desired.

You might find Gelman and Hill's book on multilevel models and a paper
you can google called "R as a tool for mathematical statistics" useful
since they both have the process and examples presented.

Harold


> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org 
> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf 
> Of Chaudhari, Monica
> Sent: Wednesday, June 27, 2007 5:47 PM
> To: Douglas Bates; R-SIG-Mixed-Models
> Subject: [R-sig-ME] Sample size estimation
> 
> Hello List,
> 
> Can anybody guide me through the steps of sample size 
> estimation for seeing the effect (at 5% level of 
> significance) of the difference in the growth of outcome 
> variable between two groups using Generalized Linear Mixed 
> Model with quasipoisson family. The model is as given below:
> 
> log(mu)= b0 + b1 time + b2 group + b3 time:group
> 
> Any clue will be highly appreciated.
> 
> Thanks,
> Monica
> 
> #########################################################
> The information contained in this e-mail and subsequent 
> attachments may be privileged, confidential and protected 
> from disclosure.  This transmission is intended for the sole 
> use of the individual and entity to whom it is addressed.  If 
> you are not the intended recipient, any dissemination, 
> distribution or copying is strictly prohibited.  If you think 
> that you have received this message in error, please e-mail 
> the sender at the above e-mail address.
> #########################################################
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



