From gregor.gorjanc at bfro.uni-lj.si  Tue Jan  1 06:27:29 2008
From: gregor.gorjanc at bfro.uni-lj.si (Gregor GORJANC)
Date: Tue, 1 Jan 2008 06:27:29 +0100
Subject: [R-sig-ME] Leading minor of order 4 in downdated X'X is not
	positive definite
In-Reply-To: <b20ae6290712312119yf7aa270jc9b6cc42f060b49f@mail.gmail.com>
References: <b20ae6290712312119yf7aa270jc9b6cc42f060b49f@mail.gmail.com>
Message-ID: <b20ae6290712312127i52b123d7r72b84c1dbe1c3700@mail.gmail.com>

Hi!

First of all happy new year and all the best in 2008!

Does anyone have any idea how to trick lmer() in the same way as
lm() to constrain one regression slope to zero in nested regression.
The following code shows what I want to do:

## Generate data

x1 <- runif(n=100, min=-4, max=1)
y1 <- rep(606, length(x1)) + rnorm(n=100, sd=10)
x1 <- x1 + 1995

x2 <- runif(n=100, min=0, max=8)
y2 <- (606 + 0.14) - 51.8 * x2 + rnorm(n=100, sd=10)
x2 <- x2 + 1997

y <- c(y1, y2)
x <- c(x1, x2)
f <- factor(rep(c("A", "B"), each=100))
g <- factor(rep(letters[1:20], times=10))

## Plot

plot(y ~ x, type="p", xlim=c(1991, 2005), ylim=c(0, 700))

## Fit model - (nested) regression for each "period"

lm(y ~ f * x)
## OK

lmer(y ~ f * x + (1 | g))
## OK

## Fit model - (nested) regression for each "period", but constrain
## slope to zero in first "period"

x2 <- x
x2[1:100] <- 0

lm(y ~ f * x2)
## OK

lmer(y ~ f * x2 + (1 | g))
Error in lmer(y ~ f * x2 + (1 | g)) :
  Leading minor of order 4 in downdated X'X is not positive definite

Thanks!

--
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------------------------------------
University of Ljubljana          PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                            mail: gregor.gorjanc <at> bfro.uni-lj.si
SI-1230 Domzale                tel: +386 (0)1 72 17 861
Slovenia, Europe                 fax: +386 (0)1 72 17 888
----------------------------------------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.
----------------------------------------------------------------------------------------------------



-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------------------------------------
University of Ljubljana          PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                            mail: gregor.gorjanc <at> bfro.uni-lj.si
SI-1230 Domzale                tel: +386 (0)1 72 17 861
Slovenia, Europe                 fax: +386 (0)1 72 17 888
----------------------------------------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.



From reinhold.kliegl at gmail.com  Tue Jan  1 16:42:31 2008
From: reinhold.kliegl at gmail.com (Reinhold Kliegl)
Date: Tue, 1 Jan 2008 16:42:31 +0100
Subject: [R-sig-ME] Leading minor of order 4 in downdated X'X is not
	positive definite
In-Reply-To: <b20ae6290712312127i52b123d7r72b84c1dbe1c3700@mail.gmail.com>
References: <b20ae6290712312119yf7aa270jc9b6cc42f060b49f@mail.gmail.com>
	<b20ae6290712312127i52b123d7r72b84c1dbe1c3700@mail.gmail.com>
Message-ID: <aefe4d0a0801010742n7953f7d8l76ed9ac2d90af4c2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080101/416d31bd/attachment.pl>

From bates at stat.wisc.edu  Sun Jan  6 21:24:26 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 6 Jan 2008 14:24:26 -0600
Subject: [R-sig-ME] Development version of lme4 now passes its tests
Message-ID: <40e66e0b0801061224t1ac75199m3aceb48f32fb323a@mail.gmail.com>

The development version of the lme4 package on R-forge now passes its
tests with linear mixed models, nonlinear mixed models and generalized
linear mixed models.  It should also be capable of fitting generalized
nonlinear mixed models but I haven't written the user-callable
functions for those models yet.

This version represents a substantial redesign of the code (the last
one, I promise) and should be both faster and simpler than previous
versions.  I would recommend trying it if you have large data sets and
complex models (e.g. models with crossed or partially crossed grouping
factors) or if you are working with generalized linear mixed models or
nonlinear mixed models.  For example, Sharon Goldwater recently wrote
to the R-help list about peculiar relationships in her fitted GLMM
models (removing a parameter decreased the deviance) from the
currently released version of lme4.  She was kind enough to provide me
with the data and I could fit all her models, with consistent results,
in the development version of the package.

I'm still working on mcmcsamp so don't expect to get reasonable
results from that function yet.  (But soon, Ben, soon.)

This version of glmer allows the quasibinomial and quasipoisson
families.  I plan to add the Gamma family soon.  Are there any other
important families that I have missed?

I have added web pages documenting the C source code at
http://lme4.R-forge.R-project.org/doxygen.  In general you can use
http://lme4.R-forge.R-project.org/ as the 'home page' for the project.
 There is a link there to the project summary page.  You can browse
the source code for the package under the SCM (source code management)
tab on the project summary page.  The SCM tab provides information on
how you can access the svn repository.  Alternatively, you can wait
for the package builds to occur overnight (in Vienna) and install the
new version with

install.packages("lme4", repos = "http://r-forge.r-project.org")

I would appreciate reports of problems with this version.  Once I get
mcmcsamp running I will call this the alpha release of lme4_1.0.0 It
is much better to catch the bugs during the alpha and beta test phases
than to need to create bug-fix releases so please test it.

I think you should be able to submit bug reports directly if you have
an R-forge login.  There is a link to the Bugs tracker and the Feature
Request tracker on the project summary page.



From austin.frank at gmail.com  Sun Jan  6 23:50:23 2008
From: austin.frank at gmail.com (Austin Frank)
Date: Sun, 06 Jan 2008 17:50:23 -0500
Subject: [R-sig-ME] Development version of lme4 now passes its tests
References: <40e66e0b0801061224t1ac75199m3aceb48f32fb323a@mail.gmail.com>
Message-ID: <m0zlvik1qo.fsf@gmail.com>

Dr. Bates--

Hello!  Happy New Year!

> This version of glmer allows the quasibinomial and quasipoisson
> families.  I plan to add the Gamma family soon.  Are there any other
> important families that I have missed?

I've asked on this list before[1], but since you've raised the issue
again, would it be possible to model a dependent variable drawn from a
multinomial distribution?  I've seen the relevant link function called
the "generalized logit".  That function and its log likelihood are
described at [2] and [3].

While the multinomial case is the one I'm most eager to see implemented
the, but there are related functions that could be considered.  Also
useful would be the generalized probit linking function ([4]) for
polychotomous choices, and the cumulative logit and probit linking
functions ([5]) for ordinal multinomial responses.

Thanks for considering these requests!
/au

Footnotes: 
[1]  http://thread.gmane.org/gmane.comp.lang.r.lme4.devel/230

[2]  http://www.statsoft.com/textbook/stglz.html#approach

[3]  http://support.sas.com/rnd/app/da/new/802ce/stat/chap8/sect5.htm

[4]  http://en.wikipedia.org/wiki/Multivariate_probit

[5]  http://www.asu.edu/sas/sasdoc/sashtml/stat/chap29/sect37.htm

-- 
Austin Frank
http://aufrank.net
GPG Public Key (D7398C2F): http://aufrank.net/personal.asc



From bernd.weiss at uni-koeln.de  Mon Jan  7 15:18:10 2008
From: bernd.weiss at uni-koeln.de (Bernd Weiss)
Date: Mon, 07 Jan 2008 15:18:10 +0100
Subject: [R-sig-ME] Development version of lme4 now passes its tests
In-Reply-To: <40e66e0b0801061224t1ac75199m3aceb48f32fb323a@mail.gmail.com>
References: <40e66e0b0801061224t1ac75199m3aceb48f32fb323a@mail.gmail.com>
Message-ID: <478234A2.4040700@uni-koeln.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Douglas Bates schrieb:

[...]

| I have added web pages documenting the C source code at
| http://lme4.R-forge.R-project.org/doxygen.  In general you can use
| http://lme4.R-forge.R-project.org/ as the 'home page' for the project.
|  There is a link there to the project summary page.  You can browse
| the source code for the package under the SCM (source code management)
| tab on the project summary page.  The SCM tab provides information on
| how you can access the svn repository.  Alternatively, you can wait
| for the package builds to occur overnight (in Vienna) and install the
| new version with
|
| install.packages("lme4", repos = "http://r-forge.r-project.org")


I am sorry if I am missing anything but I was unable to download the
windows version of the development version of lme4. Neither
<install.packages("lme4", repos ...> nor the SCM approach
("win32-latest.zip") works properly.

Some of my datasets are very strange... with respect to multilevel
modelling, of course (N ~ 80.000, K = 15, binary dependent variable).
So, it would be interesting to (1) replicate former results. In
addition, (2) I would like to compare lmer results with MLWwiN and Stata
(xtlogit, gllamm, xtmelogit).

Regards,

Bernd

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.7 (MingW32)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFHgjSiUsbvfbd00+ERAsrwAJ4kxHLdn7i39V8Edo40zfIAoweyYQCcDb/m
v95Gz8Rw4RKLM3q4pnDtwmc=
=MX4w
-----END PGP SIGNATURE-----



From maechler at stat.math.ethz.ch  Mon Jan  7 15:47:50 2008
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 7 Jan 2008 15:47:50 +0100
Subject: [R-sig-ME] R-forge problems
In-Reply-To: <478234A2.4040700@uni-koeln.de>
References: <40e66e0b0801061224t1ac75199m3aceb48f32fb323a@mail.gmail.com>
	<478234A2.4040700@uni-koeln.de>
Message-ID: <18306.15254.661325.48128@stat.math.ethz.ch>

BTW: 

Congratulations, Doug, on your lme4 "completion" !!

>>>>> "BW" == Bernd Weiss <bernd.weiss at uni-koeln.de>
>>>>>     on Mon, 07 Jan 2008 15:18:10 +0100 writes:

    BW> -----BEGIN PGP SIGNED MESSAGE-----
    BW> Hash: SHA1

    BW> [...]

    BW> | I have added web pages documenting the C source code at
    BW> | http://lme4.R-forge.R-project.org/doxygen.  In general you can use
    BW> | http://lme4.R-forge.R-project.org/ as the 'home page' for the project.
    BW> |  There is a link there to the project summary page.  You can browse
    BW> | the source code for the package under the SCM (source code management)
    BW> | tab on the project summary page.  The SCM tab provides information on
    BW> | how you can access the svn repository.  Alternatively, you can wait
    BW> | for the package builds to occur overnight (in Vienna) and install the
    BW> | new version with
    BW> |
    BW> | install.packages("lme4", repos = "http://r-forge.r-project.org")


    BW> I am sorry if I am missing anything but I was unable to download the
    BW> windows version of the development version of lme4. Neither
    BW> <install.packages("lme4", repos ...> nor the SCM approach
    BW> ("win32-latest.zip") works properly.

Hmm, yes indeed; the link (on the SCM web page) goes to the
"build log" instead of to the zip file,
and the bin/windows/contrib/2.6/ or current/ directory is
completely empty.

===> a bug in the R-forge setup (or maintenance or ...),
     and I think we've seen the problem before.

--> CC'ing one of the R-forge maintainers..

Martin Maechler, ETH ZUrich



From bates at stat.wisc.edu  Mon Jan  7 15:54:03 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 7 Jan 2008 08:54:03 -0600
Subject: [R-sig-ME] Development version of lme4 now passes its tests
In-Reply-To: <478234A2.4040700@uni-koeln.de>
References: <40e66e0b0801061224t1ac75199m3aceb48f32fb323a@mail.gmail.com>
	<478234A2.4040700@uni-koeln.de>
Message-ID: <40e66e0b0801070654y1a54947tcd0d1e582d60f15e@mail.gmail.com>

You're correct, Bernd, that the versions available by install.packages
from the repository at R-forge.R-packages.org appear to be out of
date.  Even the tar.gz file scm-latest.tar.gz is out of date
(apparently from January 04).

Those who can build a package from the source files can obtain the
latest version from the SVN archive at R-forge (instructions are given
at the SCM tab on the main project page).  I have just submitted the
package to Uwe's win-builder.R-project.org site to create a binary
Windows package, which I will make available on my web site as
http://www.stat.wisc.edu/~bates/lme4-2008-01-06-wiin32.zip

Stefan: Will someone be able to check on what is happening to the
nightly builds?

On Jan 7, 2008 8:18 AM, Bernd Weiss <bernd.weiss at uni-koeln.de> wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Douglas Bates schrieb:
>
> [...]
>
> | I have added web pages documenting the C source code at
> | http://lme4.R-forge.R-project.org/doxygen.  In general you can use
> | http://lme4.R-forge.R-project.org/ as the 'home page' for the project.
> |  There is a link there to the project summary page.  You can browse
> | the source code for the package under the SCM (source code management)
> | tab on the project summary page.  The SCM tab provides information on
> | how you can access the svn repository.  Alternatively, you can wait
> | for the package builds to occur overnight (in Vienna) and install the
> | new version with
> |
> | install.packages("lme4", repos = "http://r-forge.r-project.org")
>
>
> I am sorry if I am missing anything but I was unable to download the
> windows version of the development version of lme4. Neither
> <install.packages("lme4", repos ...> nor the SCM approach
> ("win32-latest.zip") works properly.
>
> Some of my datasets are very strange... with respect to multilevel
> modelling, of course (N ~ 80.000, K = 15, binary dependent variable).
> So, it would be interesting to (1) replicate former results. In
> addition, (2) I would like to compare lmer results with MLWwiN and Stata
> (xtlogit, gllamm, xtmelogit).
>
> Regards,
>
> Bernd
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.7 (MingW32)
> Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org
>
> iD8DBQFHgjSiUsbvfbd00+ERAsrwAJ4kxHLdn7i39V8Edo40zfIAoweyYQCcDb/m
> v95Gz8Rw4RKLM3q4pnDtwmc=
> =MX4w
> -----END PGP SIGNATURE-----
>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From stefan.theussl at wu-wien.ac.at  Mon Jan  7 16:44:45 2008
From: stefan.theussl at wu-wien.ac.at (Stefan Theussl)
Date: Mon, 07 Jan 2008 16:44:45 +0100
Subject: [R-sig-ME] Development version of lme4 now passes its tests
In-Reply-To: <40e66e0b0801070654y1a54947tcd0d1e582d60f15e@mail.gmail.com>
References: <40e66e0b0801061224t1ac75199m3aceb48f32fb323a@mail.gmail.com>	
	<478234A2.4040700@uni-koeln.de>
	<40e66e0b0801070654y1a54947tcd0d1e582d60f15e@mail.gmail.com>
Message-ID: <478248ED.9080200@wu-wien.ac.at>

Douglas Bates wrote:
> You're correct, Bernd, that the versions available by install.packages
> from the repository at R-forge.R-packages.org appear to be out of
> date.  Even the tar.gz file scm-latest.tar.gz is out of date
> (apparently from January 04).
>
> Those who can build a package from the source files can obtain the
> latest version from the SVN archive at R-forge (instructions are given
> at the SCM tab on the main project page).  I have just submitted the
> package to Uwe's win-builder.R-project.org site to create a binary
> Windows package, which I will make available on my web site as
> http://www.stat.wisc.edu/~bates/lme4-2008-01-06-wiin32.zip
>
> Stefan: Will someone be able to check on what is happening to the
> nightly builds?
>   
this is fixed. the permissions were wrong in the svn export directory. 
This was because of some experimenting a few days ago.

the latest binaries are now available as usual.

I'm sorry for the inconvenience caused.

Best,
Stefan
> On Jan 7, 2008 8:18 AM, Bernd Weiss <bernd.weiss at uni-koeln.de> wrote:
>   
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA1
>>
>> Douglas Bates schrieb:
>>
>> [...]
>>
>> | I have added web pages documenting the C source code at
>> | http://lme4.R-forge.R-project.org/doxygen.  In general you can use
>> | http://lme4.R-forge.R-project.org/ as the 'home page' for the project.
>> |  There is a link there to the project summary page.  You can browse
>> | the source code for the package under the SCM (source code management)
>> | tab on the project summary page.  The SCM tab provides information on
>> | how you can access the svn repository.  Alternatively, you can wait
>> | for the package builds to occur overnight (in Vienna) and install the
>> | new version with
>> |
>> | install.packages("lme4", repos = "http://r-forge.r-project.org")
>>
>>
>> I am sorry if I am missing anything but I was unable to download the
>> windows version of the development version of lme4. Neither
>> <install.packages("lme4", repos ...> nor the SCM approach
>> ("win32-latest.zip") works properly.
>>
>> Some of my datasets are very strange... with respect to multilevel
>> modelling, of course (N ~ 80.000, K = 15, binary dependent variable).
>> So, it would be interesting to (1) replicate former results. In
>> addition, (2) I would like to compare lmer results with MLWwiN and Stata
>> (xtlogit, gllamm, xtmelogit).
>>
>> Regards,
>>
>> Bernd
>>
>> -----BEGIN PGP SIGNATURE-----
>> Version: GnuPG v1.4.7 (MingW32)
>> Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org
>>
>> iD8DBQFHgjSiUsbvfbd00+ERAsrwAJ4kxHLdn7i39V8Edo40zfIAoweyYQCcDb/m
>> v95Gz8Rw4RKLM3q4pnDtwmc=
>> =MX4w
>> -----END PGP SIGNATURE-----
>>
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>



From bates at stat.wisc.edu  Mon Jan  7 18:13:56 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 7 Jan 2008 11:13:56 -0600
Subject: [R-sig-ME] Development version of lme4 now passes its tests
In-Reply-To: <40e66e0b0801070654y1a54947tcd0d1e582d60f15e@mail.gmail.com>
References: <40e66e0b0801061224t1ac75199m3aceb48f32fb323a@mail.gmail.com>
	<478234A2.4040700@uni-koeln.de>
	<40e66e0b0801070654y1a54947tcd0d1e582d60f15e@mail.gmail.com>
Message-ID: <40e66e0b0801070913y5bab0556seb3274549f03764d@mail.gmail.com>

On Jan 7, 2008 8:54 AM, Douglas Bates <bates at stat.wisc.edu> wrote:
> You're correct, Bernd, that the versions available by install.packages
> from the repository at R-forge.R-packages.org appear to be out of
> date.  Even the tar.gz file scm-latest.tar.gz is out of date
> (apparently from January 04).
>
> Those who can build a package from the source files can obtain the
> latest version from the SVN archive at R-forge (instructions are given
> at the SCM tab on the main project page).  I have just submitted the
> package to Uwe's win-builder.R-project.org site to create a binary
> Windows package, which I will make available on my web site as
> http://www.stat.wisc.edu/~bates/lme4-2008-01-06-wiin32.zip

The Windows package is now available as

http://www.stat.wisc.edu/~bates/lme4-2008-01-06-win32.zip

(The URL I previously gave has win32 spelled wiin32 and won't work.)



From Gregor.Gorjanc at bfro.uni-lj.si  Mon Jan  7 23:16:32 2008
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Mon, 7 Jan 2008 23:16:32 +0100
Subject: [R-sig-ME] Bug?: var estimates on exp scale for gaussian(link=log)
Message-ID: <F189E18BBAA8B6479F618B9044CF4E9C06A7992766@owa.bfro.uni-lj.si>

Hi!

I would like to fit log-normal fixed model with lmer. Fixed effects estimates
are returned on log scale, while variance components are returned on exp scale.
Why is this the case?

Here is my example. I fit several models to show the behaviour.

Thanks, Gregor

> ## --- GENERATE DATA ---
>
> set.seed(54763278)
> z <- rnorm(n=1000, mean=10, sd=1)
>
> ai <- rep(c(1, 2), each=500)
> a <- c(-2, 2)[ai]
>
> bi <- gl(n=10, k=1, length=1000)
> b <- seq(-5, 5, length.out=10)[bi]
>
> ci <- factor(round(runif(n=1000, min=1, max=100)))
> c <- rnorm(n=100)[ci]
>
> di <- factor(round(runif(n=1000, min=1, max=50)))
> d <- rnorm(n=50)[di]
>
> z <- z + a + b + c + d
> y <- exp(z)
>
> pod <- data.frame(y, ai, bi, ci, di)
>

> ## --- FIT NORMAL MIXED MODEL ---
>
> ## z ~ Normal
> tmp <- lmer(z ~ ai + bi + (1 | ci) + (1 | di), data=pod)
> tmp
...
Random effects:
 Groups   Name        Variance Std.Dev.
 ci       (Intercept) 1.17     1.080
 di       (Intercept) 0.99     0.995
 Residual             1.09     1.042
number of obs: 1000, groups: ci, 100; di, 50

Fixed effects:
            Estimate Std. Error t value
(Intercept)  -1.3399     0.2354    -5.7
ai            4.0293     0.0706    57.1
...
> ## y ~ Log-Normal
> tmp <- lmer(y ~ ai + bi + (1 | ci) + (1 | di), data=pod)
> tmp
...
Random effects:
 Groups   Name        Variance Std.Dev.
 ci       (Intercept) 1.33e+14 11531505
 di       (Intercept) 6.10e+13  7809115
 Residual             7.35e+15 85757196
number of obs: 1000, groups: ci, 100; di, 50

Fixed effects:
             Estimate Std. Error t value
(Intercept) -30997885   12065854   -2.57
ai           20613508    5480798    3.76
...

> ## --- FIT LOG-NORMAL MIXED MODEL ---
>
> ## y ~ Log-Normal
> tmp <- lmer(y ~ ai + bi + (1 | ci) + (1 | di),
+             data=pod, family=gaussian(link="log"))
CHOLMOD warning: w"
??L_? ??<_? ??z
Error in devLaplace(PQLpars) :
  Cholmod error `matrix not positive definite' at
file:../Supernodal/t_cholmod_super_numeric.c, line 613

btw, why is this happening, there is no such problem with identity
link?

> ## y ~ Log-Normal
> tmp <- lmer(y ~ ai + bi + (1 | ci),
+             data=pod, family=gaussian(link="log"))
...
Random effects:
 Groups   Name        Variance Std.Dev.
 ci       (Intercept) 4.39e+13  6622360
 Residual             1.64e+14 12824146
number of obs: 1000, groups: ci, 100

Fixed effects:
             Estimate Std. Error  t value
(Intercept) -2.16e+00   2.83e+05 -7.6e-06
ai           4.91e+00   3.99e-01    12.33
...

Here fixed effect estimates are on log scale, while variance component
estimates are huge --> is this on exp scale?



From gregor.gorjanc at bfro.uni-lj.si  Mon Jan  7 23:23:46 2008
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Mon, 7 Jan 2008 22:23:46 +0000 (UTC)
Subject: [R-sig-ME] Bug?: var estimates on exp scale for
	gaussian(link=log)
References: <F189E18BBAA8B6479F618B9044CF4E9C06A7992766@owa.bfro.uni-lj.si>
Message-ID: <loom.20080107T222111-611@post.gmane.org>

Gorjanc Gregor <Gregor.Gorjanc at ...> writes:
> 
> Hi!
> 
> I would like to fit log-normal fixed model with lmer. Fixed effects estimates
> are returned on log scale, while variance components are returned on exp scale.
> Why is this the case?

I also tried with newest version from R-Forge (svn checkout), but I can not fit
the log-normal mixed model at all. Using the same code as in previous mail I get

> tmp <- lmer(y ~ ai + bi + (1 | ci),
+             data=pod,
+             family=gaussian(link="log"))
Error in mer_finalize(ans, verbose) : 
  General form of glmer_linkinv not yet written

Gregor



From gregor.gorjanc at bfro.uni-lj.si  Mon Jan  7 23:26:01 2008
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Mon, 7 Jan 2008 22:26:01 +0000 (UTC)
Subject: [R-sig-ME] Bug?: var estimates on exp scale for
	gaussian(link=log)
References: <F189E18BBAA8B6479F618B9044CF4E9C06A7992766@owa.bfro.uni-lj.si>
Message-ID: <loom.20080107T222435-396@post.gmane.org>

Gorjanc Gregor <Gregor.Gorjanc at ...> writes:
> Hi!
> 
> I would like to fit log-normal fixed model with lmer. Fixed effects estimates
> are returned on log scale, while variance components are returned on exp scale.
> Why is this the case?

I would just like to add that I expected to get variance estimates on link (log)
scale as it is done for poisson (log link) or binomial (logit or probit link)
families.

Regards, Gregor



From bates at stat.wisc.edu  Mon Jan  7 23:32:26 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 7 Jan 2008 16:32:26 -0600
Subject: [R-sig-ME] Bug?: var estimates on exp scale for
	gaussian(link=log)
In-Reply-To: <loom.20080107T222111-611@post.gmane.org>
References: <F189E18BBAA8B6479F618B9044CF4E9C06A7992766@owa.bfro.uni-lj.si>
	<loom.20080107T222111-611@post.gmane.org>
Message-ID: <40e66e0b0801071432qd6c9df1y16bd771765104c6c@mail.gmail.com>

The development version of the lme4 package has some of the families
(binomial or quasibinomial with logit or probit link, poisson or
quasipoisson with log link) coded in C for efficiency.  The general
form will need to allow for evaluation of the family from R functions
and I haven't incorporated that yet.

On Jan 7, 2008 4:23 PM, Gregor Gorjanc <gregor.gorjanc at bfro.uni-lj.si> wrote:
> Gorjanc Gregor <Gregor.Gorjanc at ...> writes:
> >
> > Hi!
> >
> > I would like to fit log-normal fixed model with lmer. Fixed effects estimates
> > are returned on log scale, while variance components are returned on exp scale.
> > Why is this the case?
>
> I also tried with newest version from R-Forge (svn checkout), but I can not fit
> the log-normal mixed model at all. Using the same code as in previous mail I get
>
> > tmp <- lmer(y ~ ai + bi + (1 | ci),
> +             data=pod,
> +             family=gaussian(link="log"))
> Error in mer_finalize(ans, verbose) :
>   General form of glmer_linkinv not yet written
>
> Gregor
>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Mon Jan  7 23:33:53 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 7 Jan 2008 16:33:53 -0600
Subject: [R-sig-ME] Bug?: var estimates on exp scale for
	gaussian(link=log)
In-Reply-To: <loom.20080107T222435-396@post.gmane.org>
References: <F189E18BBAA8B6479F618B9044CF4E9C06A7992766@owa.bfro.uni-lj.si>
	<loom.20080107T222435-396@post.gmane.org>
Message-ID: <40e66e0b0801071433h205e6d7exfdec6b1163ab93e9@mail.gmail.com>

On Jan 7, 2008 4:26 PM, Gregor Gorjanc <gregor.gorjanc at bfro.uni-lj.si> wrote:
> Gorjanc Gregor <Gregor.Gorjanc at ...> writes:
> > Hi!
> >
> > I would like to fit log-normal fixed model with lmer. Fixed effects estimates
> > are returned on log scale, while variance components are returned on exp scale.
> > Why is this the case?
>
> I would just like to add that I expected to get variance estimates on link (log)
> scale as it is done for poisson (log link) or binomial (logit or probit link)
> families.

I'd need to look at the code more closely but I suspect that the
results you are getting are spurious.



From gregor.gorjanc at bfro.uni-lj.si  Mon Jan  7 23:42:24 2008
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Mon, 7 Jan 2008 22:42:24 +0000 (UTC)
Subject: [R-sig-ME] Bug?: var estimates on exp scale for
	gaussian(link=log)
References: <F189E18BBAA8B6479F618B9044CF4E9C06A7992766@owa.bfro.uni-lj.si>
	<loom.20080107T222435-396@post.gmane.org>
	<40e66e0b0801071433h205e6d7exfdec6b1163ab93e9@mail.gmail.com>
Message-ID: <loom.20080107T224158-672@post.gmane.org>

Douglas Bates <bates at ...> writes:
> I'd need to look at the code more closely but I suspect that the
> results you are getting are spurious.

Thanks!



From kubovy at virginia.edu  Tue Jan  8 11:51:50 2008
From: kubovy at virginia.edu (Michael Kubovy)
Date: Tue, 8 Jan 2008 05:51:50 -0500
Subject: [R-sig-ME] Development version of lme4 now passes its tests
In-Reply-To: <478248ED.9080200@wu-wien.ac.at>
References: <40e66e0b0801061224t1ac75199m3aceb48f32fb323a@mail.gmail.com>	
	<478234A2.4040700@uni-koeln.de>
	<40e66e0b0801070654y1a54947tcd0d1e582d60f15e@mail.gmail.com>
	<478248ED.9080200@wu-wien.ac.at>
Message-ID: <3F02D5BB-6E2A-4569-B023-932B66391CBD@virginia.edu>

Hello all,

I'm not sure that things are fixed. See below.

On Jan 7, 2008, at 10:44 AM, Stefan Theussl wrote:

> Douglas Bates wrote:
>> You're correct, Bernd, that the versions available by  
>> install.packages
>> from the repository at R-forge.R-packages.org appear to be out of
>> date.  Even the tar.gz file scm-latest.tar.gz is out of date
>> (apparently from January 04).
>>
>> Those who can build a package from the source files can obtain the
>> latest version from the SVN archive at R-forge (instructions are  
>> given
>> at the SCM tab on the main project page).  I have just submitted the
>> package to Uwe's win-builder.R-project.org site to create a binary
>> Windows package, which I will make available on my web site as
>> http://www.stat.wisc.edu/~bates/lme4-2008-01-06-wiin32.zip
>>
>> Stefan: Will someone be able to check on what is happening to the
>> nightly builds?
>>
> this is fixed. the permissions were wrong in the svn export directory.
> This was because of some experimenting a few days ago.
>
> the latest binaries are now available as usual.
>
> I'm sorry for the inconvenience caused.
>
> Best,
> Stefan


 > install.packages("lme4", repos = "http://r-forge.r-project.org")
Warning: unable to access index for repository http://r-forge.r-project.org/bin/macosx/universal/contrib/2.6
Warning message:
package 'lme4' is not available
 > sessionInfo()
R version 2.6.1 (2007-11-26)
i386-apple-darwin8.10.1

locale:
C

attached base packages:
[1] grid      splines   grDevices graphics  stats     utils      
methods   base

other attached packages:
  [1] Hmisc_3.4-3          coda_0.13-1          gmodels_2.14.1        
lme4_0.99875-9
  [5] Matrix_0.999375-3    lattice_0.17-4       anacor_0.9-0          
car_1.2-7
  [9] colorspace_0.95      fda_1.2.3            zoo_1.4-1             
scatterplot3d_0.3-25
[13] rgl_0.76             JGR_1.5-8            iplots_1.1-1          
JavaGD_0.4-3
[17] rJava_0.5-1

loaded via a namespace (and not attached):
[1] MASS_7.2-39    cluster_1.11.9 gdata_2.3.1    gtools_2.4.0    
tools_2.6.1

_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/



From fauna at pngp.it  Tue Jan  8 12:38:51 2008
From: fauna at pngp.it (Achaz von Hardenberg)
Date: Tue, 8 Jan 2008 12:38:51 +0100
Subject: [R-sig-ME] testing fixed effects in binomial lmer...again?
Message-ID: <0CA67590-6602-4DD5-BA58-9643C9835EA8@pngp.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080108/74a6fe32/attachment.pl>

From stefan.theussl at wu-wien.ac.at  Tue Jan  8 12:33:10 2008
From: stefan.theussl at wu-wien.ac.at (Stefan Theussl)
Date: Tue, 08 Jan 2008 12:33:10 +0100
Subject: [R-sig-ME] Development version of lme4 now passes its tests
In-Reply-To: <3F02D5BB-6E2A-4569-B023-932B66391CBD@virginia.edu>
References: <40e66e0b0801061224t1ac75199m3aceb48f32fb323a@mail.gmail.com>	
	<478234A2.4040700@uni-koeln.de>
	<40e66e0b0801070654y1a54947tcd0d1e582d60f15e@mail.gmail.com>
	<478248ED.9080200@wu-wien.ac.at>
	<3F02D5BB-6E2A-4569-B023-932B66391CBD@virginia.edu>
Message-ID: <47835F76.6020500@wu-wien.ac.at>

Michael Kubovy wrote:
> Hello all,
>
> I'm not sure that things are fixed. See below.
Currently we don't build mac binaries.
The directory
http://r-forge.r-project.org/bin/macosx/universal/contrib/2.6
does not exist yet.

You may install the package from its sources in the meantime.

Best regards,
Stefan

>
> On Jan 7, 2008, at 10:44 AM, Stefan Theussl wrote:
>
>> Douglas Bates wrote:
>>> You're correct, Bernd, that the versions available by install.packages
>>> from the repository at R-forge.R-packages.org appear to be out of
>>> date.  Even the tar.gz file scm-latest.tar.gz is out of date
>>> (apparently from January 04).
>>>
>>> Those who can build a package from the source files can obtain the
>>> latest version from the SVN archive at R-forge (instructions are given
>>> at the SCM tab on the main project page).  I have just submitted the
>>> package to Uwe's win-builder.R-project.org site to create a binary
>>> Windows package, which I will make available on my web site as
>>> http://www.stat.wisc.edu/~bates/lme4-2008-01-06-wiin32.zip
>>>
>>> Stefan: Will someone be able to check on what is happening to the
>>> nightly builds?
>>>
>> this is fixed. the permissions were wrong in the svn export directory.
>> This was because of some experimenting a few days ago.
>>
>> the latest binaries are now available as usual.
>>
>> I'm sorry for the inconvenience caused.
>>
>> Best,
>> Stefan
>
>
> > install.packages("lme4", repos = "http://r-forge.r-project.org")
> Warning: unable to access index for repository 
> http://r-forge.r-project.org/bin/macosx/universal/contrib/2.6
> Warning message:
> package 'lme4' is not available
> > sessionInfo()
> R version 2.6.1 (2007-11-26)
> i386-apple-darwin8.10.1
>
> locale:
> C
>
> attached base packages:
> [1] grid      splines   grDevices graphics  stats     utils     
> methods   base
>
> other attached packages:
>  [1] Hmisc_3.4-3          coda_0.13-1          gmodels_2.14.1       
> lme4_0.99875-9
>  [5] Matrix_0.999375-3    lattice_0.17-4       anacor_0.9-0         
> car_1.2-7
>  [9] colorspace_0.95      fda_1.2.3            zoo_1.4-1            
> scatterplot3d_0.3-25
> [13] rgl_0.76             JGR_1.5-8            iplots_1.1-1         
> JavaGD_0.4-3
> [17] rJava_0.5-1
>
> loaded via a namespace (and not attached):
> [1] MASS_7.2-39    cluster_1.11.9 gdata_2.3.1    gtools_2.4.0   
> tools_2.6.1
>
> _____________________________
> Professor Michael Kubovy
> University of Virginia
> Department of Psychology
> USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
> Parcels:    Room 102        Gilmer Hall
>         McCormick Road    Charlottesville, VA 22903
> Office:    B011    +1-434-982-4729
> Lab:        B019    +1-434-982-4751
> Fax:        +1-434-982-4766
> WWW:    http://www.people.virginia.edu/~mk9y/
>
>



From bates at stat.wisc.edu  Tue Jan  8 15:10:45 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 8 Jan 2008 08:10:45 -0600
Subject: [R-sig-ME] testing fixed effects in binomial lmer...again?
In-Reply-To: <0CA67590-6602-4DD5-BA58-9643C9835EA8@pngp.it>
References: <0CA67590-6602-4DD5-BA58-9643C9835EA8@pngp.it>
Message-ID: <40e66e0b0801080610v252f016ajc0f4bee7adfb6557@mail.gmail.com>

On Jan 8, 2008 5:38 AM, Achaz von Hardenberg <fauna at pngp.it> wrote:
> Dear all,

> I know that this may be a already debated topic, but even searching
> the R-help and the r-sig-mixed-models archives I can not find a reply
> to my doubts...(but see Ben Bolkers' reply to my similar quest in r-
> help).

> I am performing a binomial glmm analysis using the lmer function in
> the lme4 package (last release, just downloaded). I am using the
> "Laplace method".

Yes, that is the best choice in lmer.  (In the development version it
is, in fact, the only choice.)

> However, I am not sure about what I should do to test for the
> significance of fixed effects in the binomial case: Is it correct to
> test a full model against a model from which I remove the fixed
> effect I want to test using the anova(mod1.lmer, mod2.lmer) method
> and then relying on the model with the lower AIC (or on the Log-
> likelihood test?)?

 The change in the log-likelihood between two nested models is, in my
opinion, the most sensible test statistic for comparing the models.
However, it is not clear how one should convert this test statistic to
a p-value.  The use of the chi-squared distribution is based on
asymptotic results and can give an "anti-conservative" (i.e. lower
than would be obtained through a randomization test or via simulation)
p-value for small samples.  As far as I can see, the justification for
the use of AIC as a comparison criterion is even more vague.

For linear fixed-effects models one can compensate for small samples
by changing from z-tests to t-tests and from chi-squared tests to F
tests.  The exact theory breaks down for mixed-effects models or for
generalized linear models and is even more questionable for
generalized linear mixed models.  As Ben Bolker mentioned, I think
that one way to deal with the hypothesis testing question while
preserving the integrity of the model is to base inferences on a
Markov-chain Monte Carlo sample from the (Bayesian) posterior
distribution of the parameters.

Code for MCMC samples for parameters in GLMMs is not yet fully
developed (or documented).  In the meantime I would use the likelihood
ratio tests but exercise caution in reporting p-values for
small-sample cases.

> Would you advice me to use the glmmML function instead? (I am not
> sure where the differences are with lmer)
>
> I thank in advance for your help!
>
> best regards,
> Achaz von Hardenberg
>
> Ben Bolker wrote:
>  >The short answer is that testing fixed effects in GLMMs
>  >is difficult and dangerous.  Likelihood ratio tests on fixed
>  >effect differences [which is generally what anova() does]
>  >in a random-effects model are unreliable
>  >(see Pinheiro and Bates 2000).  Most of the world does
>  >F tests with various corrections on the denominator
>  >degrees of freedom, but this is contentious (in particular,
>  >Doug Bates, the author of lme4, disagrees).  lme4 will
>  >eventually let you use an MCMC sampling method to test
>  >fixed effects but that may or may not be working
>  >in the current version.
>
>  >I would try this question again on the r-sig-mixed
>  >mailing list.
>
>  > good luck,
>  > Ben Bolker
>
> Dr. Achaz von Hardenberg
> ------------------------------------------------------------------------
> --------------------------------
> Centro Studi Fauna Alpina - Alpine Wildlife Research Centre
> Servizio Sanitario e della Ricerca Scientifica
> Parco Nazionale Gran Paradiso, Degioz, 11, 11010-Valsavarenche (Ao),
> Italy
> ------------------------------------------------------------------------
> --------------------------------
>
>
>
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From dimitris.rizopoulos at med.kuleuven.be  Tue Jan  8 18:13:47 2008
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 8 Jan 2008 18:13:47 +0100
Subject: [R-sig-ME] testing fixed effects in binomial lmer...again?
References: <0CA67590-6602-4DD5-BA58-9643C9835EA8@pngp.it>
	<40e66e0b0801080610v252f016ajc0f4bee7adfb6557@mail.gmail.com>
Message-ID: <01b701c85219$d4de5620$0540210a@www.domain>

----- Original Message ----- 
From: "Douglas Bates" <bates at stat.wisc.edu>
To: "Achaz von Hardenberg" <fauna at pngp.it>
Cc: <r-sig-mixed-models at r-project.org>
Sent: Tuesday, January 08, 2008 3:10 PM
Subject: Re: [R-sig-ME] testing fixed effects in binomial 
lmer...again?


> On Jan 8, 2008 5:38 AM, Achaz von Hardenberg <fauna at pngp.it> wrote:
>> Dear all,
>
>> I know that this may be a already debated topic, but even searching
>> the R-help and the r-sig-mixed-models archives I can not find a 
>> reply
>> to my doubts...(but see Ben Bolkers' reply to my similar quest in 
>> r-
>> help).
>
>> I am performing a binomial glmm analysis using the lmer function in
>> the lme4 package (last release, just downloaded). I am using the
>> "Laplace method".
>
> Yes, that is the best choice in lmer.  (In the development version 
> it
> is, in fact, the only choice.)
>
>> However, I am not sure about what I should do to test for the
>> significance of fixed effects in the binomial case: Is it correct 
>> to
>> test a full model against a model from which I remove the fixed
>> effect I want to test using the anova(mod1.lmer, mod2.lmer) method
>> and then relying on the model with the lower AIC (or on the Log-
>> likelihood test?)?
>
> The change in the log-likelihood between two nested models is, in my
> opinion, the most sensible test statistic for comparing the models.
> However, it is not clear how one should convert this test statistic 
> to
> a p-value.  The use of the chi-squared distribution is based on
> asymptotic results and can give an "anti-conservative" (i.e. lower
> than would be obtained through a randomization test or via 
> simulation)
> p-value for small samples.  As far as I can see, the justification 
> for
> the use of AIC as a comparison criterion is even more vague.
>
> For linear fixed-effects models one can compensate for small samples
> by changing from z-tests to t-tests and from chi-squared tests to F
> tests.  The exact theory breaks down for mixed-effects models or for
> generalized linear models and is even more questionable for
> generalized linear mixed models.  As Ben Bolker mentioned, I think
> that one way to deal with the hypothesis testing question while
> preserving the integrity of the model is to base inferences on a
> Markov-chain Monte Carlo sample from the (Bayesian) posterior
> distribution of the parameters.
>
> Code for MCMC samples for parameters in GLMMs is not yet fully
> developed (or documented).  In the meantime I would use the 
> likelihood
> ratio tests but exercise caution in reporting p-values for
> small-sample cases.


What about Bootstrap (parametric or not)? Would it be useful in this 
case?

(For instance, something along the following lines:

library(lme4)

form.null <- # formula under null
form.altr <- # formula under alternative
fm1 <- lmer(form.null, family = binomial, data = data)
fm2 <- lmer(form.altr, family = binomial, data = data)

# observed value of the LRT
Tobs <- anova(fm1, fm2)$Chisq[2]

B <- 199
Tvals <- numeric(B)
# 'id' is the grouping variable
unq.ids <- unique(data$id)
for (b in 1:B) {
    dat.new <- # a sample with replacement from the original subjects
    fm1 <- lmer(form.null, family = binomial, data = data.new)
    fm2 <- lmer(form.altr, family = binomial, data = data.new)
    Tvals[b] <- anova(fm1, fm2)$Chisq[2]
}
# estimated p-value
(1 + sum(Tvals >= Tobs)) / (B + 1)


if the estimated p-value is near the significance level, 'B' can be 
increased accordingly.)

Best,
Dimitris



>> Would you advice me to use the glmmML function instead? (I am not
>> sure where the differences are with lmer)
>>
>> I thank in advance for your help!
>>
>> best regards,
>> Achaz von Hardenberg
>>
>> Ben Bolker wrote:
>>  >The short answer is that testing fixed effects in GLMMs
>>  >is difficult and dangerous.  Likelihood ratio tests on fixed
>>  >effect differences [which is generally what anova() does]
>>  >in a random-effects model are unreliable
>>  >(see Pinheiro and Bates 2000).  Most of the world does
>>  >F tests with various corrections on the denominator
>>  >degrees of freedom, but this is contentious (in particular,
>>  >Doug Bates, the author of lme4, disagrees).  lme4 will
>>  >eventually let you use an MCMC sampling method to test
>>  >fixed effects but that may or may not be working
>>  >in the current version.
>>
>>  >I would try this question again on the r-sig-mixed
>>  >mailing list.
>>
>>  > good luck,
>>  > Ben Bolker
>>
>> Dr. Achaz von Hardenberg
>> ------------------------------------------------------------------------
>> --------------------------------
>> Centro Studi Fauna Alpina - Alpine Wildlife Research Centre
>> Servizio Sanitario e della Ricerca Scientifica
>> Parco Nazionale Gran Paradiso, Degioz, 11, 11010-Valsavarenche 
>> (Ao),
>> Italy
>> ------------------------------------------------------------------------
>> --------------------------------
>>
>>
>>
>>
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From David.Duffy at qimr.edu.au  Tue Jan  8 23:12:42 2008
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Wed, 9 Jan 2008 08:12:42 +1000 (EST)
Subject: [R-sig-ME] testing fixed effects in binomial lmer...again?
In-Reply-To: <01b701c85219$d4de5620$0540210a@www.domain>
References: <0CA67590-6602-4DD5-BA58-9643C9835EA8@pngp.it><40e66e0b0801080610v252f016ajc0f4bee7adfb6557@mail.gmail.com>
	<01b701c85219$d4de5620$0540210a@www.domain>
Message-ID: <Pine.LNX.4.64.0801090746380.31988@orpheus.qimr.edu.au>

On Tue, 8 Jan 2008, Dimitris Rizopoulos wrote:
>> On Jan 8, 2008 5:38 AM, Achaz von Hardenberg <fauna at pngp.it> wrote:
>>
>>> However, I am not sure about what I should do to test for the
>>> significance of fixed effects in the binomial case
>
> What about Bootstrap (parametric or not)? Would it be useful in this
> case?
>

The only problem is specifying a bootstrap mechanism that respects the
structure of the random effects.  So for time series data, your bootstrap
samples have to remain AR1 or whatever (ie you don't want gaps appearing that
aren't in the observed data), and for genetic type data (the kind I have),
that pseudosample people are appropriately related to one another.  Resampling
clusters works for that kind of data, though I think you need many clusters.
There are several papers in the area of genetic linkage analysis that have 
validated bootstrapping for a test that a variance component is zero.

But for testing simple hypotheses about particular fixed effects, 
a permutation/randomization test should work, I think.

David Duffy.
-- 
| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v



From bolker at ufl.edu  Tue Jan  8 23:21:31 2008
From: bolker at ufl.edu (Ben Bolker)
Date: Tue, 08 Jan 2008 17:21:31 -0500
Subject: [R-sig-ME] testing fixed effects in binomial lmer...again?
In-Reply-To: <Pine.LNX.4.64.0801090746380.31988@orpheus.qimr.edu.au>
References: <0CA67590-6602-4DD5-BA58-9643C9835EA8@pngp.it><40e66e0b0801080610v252f016ajc0f4bee7adfb6557@mail.gmail.com>	<01b701c85219$d4de5620$0540210a@www.domain>
	<Pine.LNX.4.64.0801090746380.31988@orpheus.qimr.edu.au>
Message-ID: <4783F76B.7000302@ufl.edu>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

David Duffy wrote:
| On Tue, 8 Jan 2008, Dimitris Rizopoulos wrote:
|>> On Jan 8, 2008 5:38 AM, Achaz von Hardenberg <fauna at pngp.it> wrote:
|>>
|>>> However, I am not sure about what I should do to test for the
|>>> significance of fixed effects in the binomial case
|> What about Bootstrap (parametric or not)? Would it be useful in this
|> case?
|>
|
| The only problem is specifying a bootstrap mechanism that respects the
| structure of the random effects.  So for time series data, your bootstrap
| samples have to remain AR1 or whatever (ie you don't want gaps
appearing that
| aren't in the observed data), and for genetic type data (the kind I have),
| that pseudosample people are appropriately related to one another.
Resampling
| clusters works for that kind of data, though I think you need many
clusters.
| There are several papers in the area of genetic linkage analysis that
have
| validated bootstrapping for a test that a variance component is zero.
|
| But for testing simple hypotheses about particular fixed effects,
| a permutation/randomization test should work, I think.
|
| David Duffy.

~  My favorite solution (which worked in nlme, I think, but might
take some time to get for lme4 ...) would to be able to generate
posterior simulations from the reduced model, then use these to
generate a null distribution for F statistics (or whatever) for
the model comparison.  This seems as though it would actually be
a relatively straightforward extension of mcmcsamp, once it exists --
although arguably once you have mcmcsamp you wouldn't need it
any more ...

~   Ben Bolker

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.6 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFHg/drc5UpGjwzenMRAgtdAJ9iill9KFLLGOAUnLyXvCRVEWthEQCdFLb9
s+A2eFe1JfDrAy/0zW9MFhA=
=Aitb
-----END PGP SIGNATURE-----



From reinhold.kliegl at gmail.com  Wed Jan  9 00:45:36 2008
From: reinhold.kliegl at gmail.com (Reinhold Kliegl)
Date: Wed, 9 Jan 2008 00:45:36 +0100
Subject: [R-sig-ME] testing fixed effects in binomial lmer...again?
In-Reply-To: <4783F76B.7000302@ufl.edu>
References: <0CA67590-6602-4DD5-BA58-9643C9835EA8@pngp.it>
	<40e66e0b0801080610v252f016ajc0f4bee7adfb6557@mail.gmail.com>
	<01b701c85219$d4de5620$0540210a@www.domain>
	<Pine.LNX.4.64.0801090746380.31988@orpheus.qimr.edu.au>
	<4783F76B.7000302@ufl.edu>
Message-ID: <aefe4d0a0801081545u1d52b6a8wf3e1891c14f97dee@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080109/2d29b8cc/attachment.pl>

From vmuggeo at dssm.unipa.it  Fri Jan 11 11:39:42 2008
From: vmuggeo at dssm.unipa.it (vito muggeo)
Date: Fri, 11 Jan 2008 11:39:42 +0100
Subject: [R-sig-ME] fitting method in glmer and var for the cond. modes
Message-ID: <4787476E.9030306@dssm.unipa.it>

dear all,
I 've just installed the last version of lme4.
Thanks to prof Bates for his excellent work.

I have an issue and a question.

1)issue:
I tried to fit binomial GLMM, everything works but it appears that the 
method="PQL" is not implemented..Namely the following two calls yield
exactly the same results
glmer(formula,family=binomial,data=d,method="Laplace")
glmer(formula,family=binomial,data=d,method="PQL")

The summary method on the relevant fits prints in each case (correctly?) 
``Generalized linear mixed model fit by the Laplace approximation''

The estimates are very similar to those coming from MASS::glmmPQL(). For 
instance the est. st.dev for the interc is 0.1613318 (via glmmPQL()) vs. 
0.16119 (via glmer()+laplace). Since the response is binary, I expected 
the estimates to be somewhat different..


2)question:
I am interested in obtaining the variances of the predictions (i.e. the 
variances of conditional modes \tilde{b}_i) from a simple LMM (fitted 
via lme or lmer)

As far as I remember correctly, there was the `bVar' slot in the early 
versions of lmer..Am I right?

Also, how I can extract the same quantities from a "lme" fits?



many thanks,
vito


-- 
====================================
Vito M.R. Muggeo
Dip.to Sc Statist e Matem `Vianelli'
Universit? di Palermo
viale delle Scienze, edificio 13
90128 Palermo - ITALY
tel: 091 6626240
fax: 091 485726/485612



From ima at difres.dk  Fri Jan 11 17:41:44 2008
From: ima at difres.dk (Irene Mantzouni)
Date: Fri, 11 Jan 2008 17:41:44 +0100
Subject: [R-sig-ME] autocorrelation by group in mixed model
Message-ID: <8A96F09B52875349A6B34475EDCCC277044B09@lu-mail-san.dfu.local>

Hi all!
 
(How) is it possible to fit a mixed model with group specific auto-correlation structure ? For instance, not all my groups display auto-correlation so I would like to use a corMatrix (corAR1) only for the auto-correlated ones. If I construct manually a
the corMatrix, is it possible to use it  as input somehow?
 
thank you!
 
Irene



From bates at stat.wisc.edu  Fri Jan 11 18:56:04 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 11 Jan 2008 11:56:04 -0600
Subject: [R-sig-ME] fitting method in glmer and var for the cond. modes
In-Reply-To: <4787476E.9030306@dssm.unipa.it>
References: <4787476E.9030306@dssm.unipa.it>
Message-ID: <40e66e0b0801110956l490d699eib948cf9863a48a97@mail.gmail.com>

On Jan 11, 2008 4:39 AM, vito muggeo <vmuggeo at dssm.unipa.it> wrote:
> dear all,
> I 've just installed the last version of lme4.
> Thanks to prof Bates for his excellent work.

You're welcome.

> I have an issue and a question.

> 1)issue:
> I tried to fit binomial GLMM, everything works but it appears that the
> method="PQL" is not implemented..Namely the following two calls yield
> exactly the same results
> glmer(formula,family=binomial,data=d,method="Laplace")
> glmer(formula,family=binomial,data=d,method="PQL")

> The summary method on the relevant fits prints in each case (correctly?)
> ``Generalized linear mixed model fit by the Laplace approximation''

Yes.  After the seemingly endless task of development of the software
comes the even more seemingly endless task of documenting it.

The current scheme omits PQL iterations entirely.  For all types of
models: LMMs, GLMMs, NLMMs and GNLMMs  the ML estimates are those
obtained by direct optimization of the Laplace approximation to the
log-likelihood.  It happens that for LMMs the Laplace approximation is
exactly the log-likelihood.  Also, for those models the default
criterion is REML rather than ML but one can choose ML if desired.

I do plan to allow for optimization of the Adaptive Gauss-Hermite
Quadrature (AGQ) evaluation of the log-likelihood, when feasible.  The
Laplace approximation is a special case of AGQ, corresponding to a
single quadrature point per dimension.  Generally when we speak of AGQ
we are referring to cases of more than one quadrature point per
dimension.

The lmer/glmer/nlmer functions in the development version have an
argument "verbose".  If you want to get a better idea of how the
optimization is proceeding, set verbose = 1 (output on every
iteration) or verbose = 2 (output every second iteration), etc.  The
extraordinarily curious can set verbose = -1 and get even more output
from the penalized iteratively reweighted least squares (PIRLS)
algorithm to determine the conditional modes of the random effects at
each evaluation of the log-likelihood.

> The estimates are very similar to those coming from MASS::glmmPQL(). For
> instance the est. st.dev for the interc is 0.1613318 (via glmmPQL()) vs.
> 0.16119 (via glmer()+laplace). Since the response is binary, I expected
> the estimates to be somewhat different..

> 2)question:

> I am interested in obtaining the variances of the predictions (i.e. the
> variances of conditional modes \tilde{b}_i) from a simple LMM (fitted
> via lme or lmer)

Those can be obtained from

ranef(fm, postVar = TRUE)

Again, I may change the terminology from postVar (posterior variances)
to condVar (conditional variances) at some point because it is a
misnomer to call these posterior variances.  With non-nested grouping
factors these are incomplete in that they only give the parts of the
conditional variance-covariance matrix of the random effects that are
on or close to the diagonal.


> As far as I remember correctly, there was the `bVar' slot in the early
> versions of lmer..Am I right?

The bVar slot has gone away.  The computational methods in the
development version are much simpler than in previous versions.  The
complexity is all hidden in the sparse Cholesky decomposition in the L
slot.

> Also, how I can extract the same quantities from a "lme" fits?
>
>
>
> many thanks,
> vito
>
>
> --
> ====================================
> Vito M.R. Muggeo
> Dip.to Sc Statist e Matem `Vianelli'
> Universit? di Palermo
> viale delle Scienze, edificio 13
> 90128 Palermo - ITALY
> tel: 091 6626240
> fax: 091 485726/485612
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From kjbeath at kagi.com  Mon Jan 14 00:47:56 2008
From: kjbeath at kagi.com (Ken Beath)
Date: Mon, 14 Jan 2008 10:47:56 +1100
Subject: [R-sig-ME] testing fixed effects in binomial lmer...again?
In-Reply-To: <aefe4d0a0801081545u1d52b6a8wf3e1891c14f97dee@mail.gmail.com>
References: <0CA67590-6602-4DD5-BA58-9643C9835EA8@pngp.it>
	<40e66e0b0801080610v252f016ajc0f4bee7adfb6557@mail.gmail.com>
	<01b701c85219$d4de5620$0540210a@www.domain>
	<Pine.LNX.4.64.0801090746380.31988@orpheus.qimr.edu.au>
	<4783F76B.7000302@ufl.edu>
	<aefe4d0a0801081545u1d52b6a8wf3e1891c14f97dee@mail.gmail.com>
Message-ID: <72EA8B57-660C-4C7A-BF42-4152D2AD4BE6@kagi.com>

On 09/01/2008, at 10:45 AM, Reinhold Kliegl wrote:

> Not sure it applies exactly to the cases discussed so far here, but  
> there is
> analytical and simulation evidence against using BLUP-based  
> residuals of
> mixed models for bootstrapping. See:Morris, J.S. (2002). The BLUPs  
> are not
> "best" when it comes to bootstrapping.
> Statistics & Probability Letters 56, 425-450.
>
> A PDF is available on his website.
>
> Reinhold
>

A solution to this problem is

Carpenter, JR; Goldstein, H; Rasbash, J "A novel bootstrap procedure  
for assessing the relationship between class size and achievement",
JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C-APPLIED STATISTICS,  
52: 431-443 Part 4 2003

but I expect that parametric bootstrapping is enough for most problems.

Ken



From mdu at ceh.ac.uk  Mon Jan 14 11:24:04 2008
From: mdu at ceh.ac.uk (Mike Dunbar)
Date: Mon, 14 Jan 2008 10:24:04 +0000
Subject: [R-sig-ME] autocorrelation by group in mixed model
Message-ID: <s78b389c.002@wpo.nerc.ac.uk>

Dear Irene

How about making a new column in your dataset which is the response variable lagged by 1 time step. Then use this term as a predictor in your model and look at the BLUPs. This would assume that the group by group autocorrelation would follow a normal distribution, I'm not sure if this is valid from theory or not. Is this what you need?

Further suggestions. 
Emailing both R-help and R-sig-ME with the same question does not encourage replies.
Read the posting guide and post a reproducible example, or at least give some more information on what you are trying to do. We know nothing about your data so it is difficult to give meaningful help. For example you probably need quite alot of data within each group in order to estimate AR terms well.
When you say some groups don't display autocorrelation how did you determine this? If there are sufficient data to determine this precisely then you probably don't need a mixed model, just model each group separately. Alternatively the differences in group-specific AR could be sampling variability, in which case a mixed model would be appropriate, but you can't say "not all my groups display auto-correlation"

regards

Mike


>>> "Irene Mantzouni" <ima at difres.dk> 11/01/2008 16:41 >>>
Hi all!
 
(How) is it possible to fit a mixed model with group specific auto-correlation structure ? For instance, not all my groups display auto-correlation so I would like to use a corMatrix (corAR1) only for the auto-correlated ones. If I construct manually a
the corMatrix, is it possible to use it  as input somehow?
 
thank you!
 
Irene

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
This message (and any attachments) is for the recipient ...{{dropped:6}}



From xiaozh at WPI.EDU  Mon Jan 14 16:59:11 2008
From: xiaozh at WPI.EDU (Zhong, Xiao)
Date: Mon, 14 Jan 2008 10:59:11 -0500
Subject: [R-sig-ME] Can we do b-spline within lmer function?
Message-ID: <80714657CA27844AAEC7E733FD3FF9760A08C0AD70@EXCHANGEMAIL.admin.wpi.edu>

Hello,

I am working on a project that uses glmm. I would like to consider b-spline in the original glmm but I couldn't find how to add b-spline terms to the normal lmer function. Is there anybody who could help me with that?

Thanks very much,

Xiao



From kevin.thorpe at utoronto.ca  Mon Jan 14 21:21:20 2008
From: kevin.thorpe at utoronto.ca (Kevin E. Thorpe)
Date: Mon, 14 Jan 2008 15:21:20 -0500
Subject: [R-sig-ME] Some Basic lmer Questions
Message-ID: <478BC440.3090305@utoronto.ca>

I am taking my first cautious steps into the mixed-models pool and I
have a few, probably basic questions.

The data I am faced with are lab values taken at regular time intervals
(0, 4, 8, 12 and 24 hours) following a surgery.

> str(trop)
'data.frame':	790 obs. of  6 variables:
 $ pid  : int  0 0 0 0 0 1 1 1 1 1 ...
 $ ittrx: int  1 1 1 1 1 2 2 2 2 2 ...
 $ pprx : int  1 1 1 1 1 2 2 2 2 2 ...
 $ rx3  : Factor w/ 3 levels "On","Off","Converted": 1 1 1 1 1 2 2 2 2 2 ...
 $ hours: num  0 4 8 12 24 0 4 8 12 24 ...
 $ trop : num  2.12 9.51 5.79 4.37 1.8 NA NA NA NA NA ...
 - attr(*, "reshapeLong")=List of 4
  ..$ varying:List of 1
  .. ..$ : chr  "Trop0" "Trop4" "Trop8" "Trop12" ...
  ..$ v.names: chr "trop"
  ..$ idvar  : chr "pid"
  ..$ timevar: chr "hours"


Of interest is whether or not there are differences among the groups
represented by rx3 above.  If we pretend for the moment that the time
effect is linear and there is no treatment by time interaction, I
would be inclined to test for differences as follows (also
ignoring any correlation structure).

trop.lme0 <- lmer(trop~hours+(1|pid),data=trop,method="ML")
trop.lme1 <- lmer(trop~rx3+hours+(1|pid),data=trop,method="ML")
anova(trop.lme0,trop.lme1)

I seem to recall hearing/reading that the LRT from anova() is
appropriate for maximum-likelihood but not REML which is why
I used method="ML".  So, is this the right approach or have
I seriously misunderstood something?

Next, assuming I have not done anything egregious, I want to
turn to the non-linearity of the time effect.  I found that
I can use ns() in the splines package to include a spline term
for hours, but is this the right approach?  I could also see
making hours a factor to allow for non-linearity as well.

Finally, (and feel free to point me at suitable references)
how does one determine the appropriate correlation structure
to use in these models?

Thanks for your time.

-- 
Kevin E. Thorpe
Biostatistician/Trialist, Knowledge Translation Program
Assistant Professor, Department of Public Health Sciences
Faculty of Medicine, University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.864.5776  Fax: 416.864.6057



From kjbeath at kagi.com  Mon Jan 14 21:29:22 2008
From: kjbeath at kagi.com (Ken Beath)
Date: Tue, 15 Jan 2008 07:29:22 +1100
Subject: [R-sig-ME] Can we do b-spline within lmer function?
In-Reply-To: <80714657CA27844AAEC7E733FD3FF9760A08C0AD70@EXCHANGEMAIL.admin.wpi.edu>
References: <80714657CA27844AAEC7E733FD3FF9760A08C0AD70@EXCHANGEMAIL.admin.wpi.edu>
Message-ID: <5DE30F50-9A9D-4A28-817F-E3CC9AB4C8C5@kagi.com>

On 15/01/2008, at 2:59 AM, Zhong, Xiao wrote:

> Hello,
>
> I am working on a project that uses glmm. I would like to consider b- 
> spline in the original glmm but I couldn't find how to add b-spline  
> terms to the normal lmer function. Is there anybody who could help  
> me with that?
>

library(splines)
fm2 <- lmer(Reaction ~ bs(Days) + (1|Subject), sleepstudy)

Ken



From A.Robinson at ms.unimelb.edu.au  Mon Jan 14 21:50:24 2008
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Tue, 15 Jan 2008 07:50:24 +1100
Subject: [R-sig-ME] Some Basic lmer Questions
In-Reply-To: <478BC440.3090305@utoronto.ca>
References: <478BC440.3090305@utoronto.ca>
Message-ID: <20080114205024.GJ995@ms.unimelb.edu.au>

Hi Kevin,

welcome to the wonderful world of mixed-models!  I heartily recommend
that you read the following books: Gelman and Hill, and Pinehiro and
Bates. 

On Mon, Jan 14, 2008 at 03:21:20PM -0500, Kevin E. Thorpe wrote:
> I am taking my first cautious steps into the mixed-models pool and I
> have a few, probably basic questions.
> 
> The data I am faced with are lab values taken at regular time intervals
> (0, 4, 8, 12 and 24 hours) following a surgery.
> 
> > str(trop)
> 'data.frame':	790 obs. of  6 variables:
>  $ pid  : int  0 0 0 0 0 1 1 1 1 1 ...
>  $ ittrx: int  1 1 1 1 1 2 2 2 2 2 ...
>  $ pprx : int  1 1 1 1 1 2 2 2 2 2 ...
>  $ rx3  : Factor w/ 3 levels "On","Off","Converted": 1 1 1 1 1 2 2 2 2 2 ...
>  $ hours: num  0 4 8 12 24 0 4 8 12 24 ...
>  $ trop : num  2.12 9.51 5.79 4.37 1.8 NA NA NA NA NA ...
>  - attr(*, "reshapeLong")=List of 4
>   ..$ varying:List of 1
>   .. ..$ : chr  "Trop0" "Trop4" "Trop8" "Trop12" ...
>   ..$ v.names: chr "trop"
>   ..$ idvar  : chr "pid"
>   ..$ timevar: chr "hours"
> 
> 
> Of interest is whether or not there are differences among the groups
> represented by rx3 above.  If we pretend for the moment that the time
> effect is linear and there is no treatment by time interaction, I
> would be inclined to test for differences as follows (also
> ignoring any correlation structure).
> 
> trop.lme0 <- lmer(trop~hours+(1|pid),data=trop,method="ML")
> trop.lme1 <- lmer(trop~rx3+hours+(1|pid),data=trop,method="ML")
> anova(trop.lme0,trop.lme1)
> 
> I seem to recall hearing/reading that the LRT from anova() is
> appropriate for maximum-likelihood but not REML which is why
> I used method="ML".  So, is this the right approach or have
> I seriously misunderstood something?

It's under discussion.  The current recommendation is to use
mcmcsamp to sample the posterior distribution.  

> Next, assuming I have not done anything egregious, I want to
> turn to the non-linearity of the time effect.  I found that
> I can use ns() in the splines package to include a spline term
> for hours, but is this the right approach?  I could also see
> making hours a factor to allow for non-linearity as well.

It's plausible.  You could also try GAMM in mgcv.
 
> Finally, (and feel free to point me at suitable references)
> how does one determine the appropriate correlation structure
> to use in these models?

See above!

Andrew
-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/



From sundar.dorai-raj at pdf.com  Mon Jan 14 22:13:06 2008
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 14 Jan 2008 13:13:06 -0800
Subject: [R-sig-ME] Some Basic lmer Questions
In-Reply-To: <20080114205024.GJ995@ms.unimelb.edu.au>
References: <478BC440.3090305@utoronto.ca>
	<20080114205024.GJ995@ms.unimelb.edu.au>
Message-ID: <478BD062.3000208@pdf.com>



Andrew Robinson said the following on 1/14/2008 12:50 PM:
> On Mon, Jan 14, 2008 at 03:21:20PM -0500, Kevin E. Thorpe wrote:
>> Finally, (and feel free to point me at suitable references)
>> how does one determine the appropriate correlation structure
>> to use in these models?
> 
> See above!
> 
> Andrew

With the exception that lmer currently does not support correlation 
structures. If you really need to estimate correlation parameters then 
you may need to use the nlme package.

--sundar



From kjbeath at kagi.com  Mon Jan 14 22:26:53 2008
From: kjbeath at kagi.com (Ken Beath)
Date: Tue, 15 Jan 2008 08:26:53 +1100
Subject: [R-sig-ME] Some Basic lmer Questions
In-Reply-To: <20080114205024.GJ995@ms.unimelb.edu.au>
References: <478BC440.3090305@utoronto.ca>
	<20080114205024.GJ995@ms.unimelb.edu.au>
Message-ID: <6486FDE3-2E6C-4FF2-84A6-AF013CBEC5C0@kagi.com>

On 15/01/2008, at 7:50 AM, Andrew Robinson wrote:

> Hi Kevin,
>
> welcome to the wonderful world of mixed-models!  I heartily recommend
> that you read the following books: Gelman and Hill, and Pinehiro and
> Bates.
>

For linear mixed models I would also suggest Verbeke and Molenberghs  
"Linear Mixed Models for Longitudinal Data"

> On Mon, Jan 14, 2008 at 03:21:20PM -0500, Kevin E. Thorpe wrote:
>>
>> Finally, (and feel free to point me at suitable references)
>> how does one determine the appropriate correlation structure
>> to use in these models?

While you can make life more complicated AIC is a reasonable method.  
This is criticised as it is not the same as statistical significance  
but all we need is a reasonable model for the correlation so that  
inferences about the other parameters are correct.

Ken



From xiaozh at WPI.EDU  Mon Jan 14 23:42:00 2008
From: xiaozh at WPI.EDU (Zhong, Xiao)
Date: Mon, 14 Jan 2008 17:42:00 -0500
Subject: [R-sig-ME] Can we do b-spline within lmer function?
In-Reply-To: <5DE30F50-9A9D-4A28-817F-E3CC9AB4C8C5@kagi.com>
References: <80714657CA27844AAEC7E733FD3FF9760A08C0AD70@EXCHANGEMAIL.admin.wpi.edu>,
	<5DE30F50-9A9D-4A28-817F-E3CC9AB4C8C5@kagi.com>
Message-ID: <80714657CA27844AAEC7E733FD3FF9760A08C0AD73@EXCHANGEMAIL.admin.wpi.edu>

Thanks, Ken.

I tried your advice on my model:

model2.growth.mcas5 <- lmer(response ~ bs(monthElapsed) + skills + (1|studentID), data= mcas5, family=binomial(link="logit"), control =
list(msVerbose = 1, usePQL = FALSE))

It worked! What it gave me is:

                      (Intercept)                  bs(monthElapsed)1                  bs(monthElapsed)2                  bs(monthElapsed)3
                        -0.1509712                          0.7019262                         -0.1934738                          0.6257330
                  skillsG-Geometry                skillsM-Measurement    skillsN-Number-Sense-Operations skillsP-Patterns-Relations-Algebra
                        -0.4333880                         -0.9468761                          0.5263370                          0.1897903

Since I am trying to use design matrix to predict after that glmm fitting, could you tell me if there is some function/command that I can use to find what the three "bs" did to my "monthElapsed" variable?

Thanks!

Xiao


________________________________________
From: Ken Beath [kjbeath at kagi.com]
Sent: Monday, January 14, 2008 3:29 PM
To: Zhong, Xiao
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Can we do b-spline within lmer function?

On 15/01/2008, at 2:59 AM, Zhong, Xiao wrote:

> Hello,
>
> I am working on a project that uses glmm. I would like to consider b-
> spline in the original glmm but I couldn't find how to add b-spline
> terms to the normal lmer function. Is there anybody who could help
> me with that?
>

library(splines)
fm2 <- lmer(Reaction ~ bs(Days) + (1|Subject), sleepstudy)

Ken



From vmuggeo at dssm.unipa.it  Tue Jan 15 09:57:28 2008
From: vmuggeo at dssm.unipa.it (vito muggeo)
Date: Tue, 15 Jan 2008 09:57:28 +0100
Subject: [R-sig-ME] Can we do b-spline within lmer function?
In-Reply-To: <80714657CA27844AAEC7E733FD3FF9760A08C0AD73@EXCHANGEMAIL.admin.wpi.edu>
References: <80714657CA27844AAEC7E733FD3FF9760A08C0AD70@EXCHANGEMAIL.admin.wpi.edu>,
	<5DE30F50-9A9D-4A28-817F-E3CC9AB4C8C5@kagi.com>
	<80714657CA27844AAEC7E733FD3FF9760A08C0AD73@EXCHANGEMAIL.admin.wpi.edu>
Message-ID: <478C7578.501@dssm.unipa.it>

Dear Zhong,
Two (probably) minor comments:
(i) As stated in ?bs, "bs(x)" defaults to a simple third degree 
polynomial, therefore if you wants real splines you have to play with 
the argument df and/or knots.

(ii) In theory you can move to penalized B-splines (i.e., P-splines 
which appear to be preferable to simple b-splines) in a mixed model 
framework. However lmer and friends may not be used because it does not 
appear possible to specify the proper covariance matrix for the random 
effects (you should use a multiple of identity matrix as pdDiag in 
nlme::lme())

all the best,
vito



Zhong, Xiao ha scritto:
> Thanks, Ken.
> 
> I tried your advice on my model:
> 
> model2.growth.mcas5 <- lmer(response ~ bs(monthElapsed) + skills + (1|studentID), data= mcas5, family=binomial(link="logit"), control =
> list(msVerbose = 1, usePQL = FALSE))
> 
> It worked! What it gave me is:
> 
>                       (Intercept)                  bs(monthElapsed)1                  bs(monthElapsed)2                  bs(monthElapsed)3
>                         -0.1509712                          0.7019262                         -0.1934738                          0.6257330
>                   skillsG-Geometry                skillsM-Measurement    skillsN-Number-Sense-Operations skillsP-Patterns-Relations-Algebra
>                         -0.4333880                         -0.9468761                          0.5263370                          0.1897903
> 
> Since I am trying to use design matrix to predict after that glmm fitting, could you tell me if there is some function/command that I can use to find what the three "bs" did to my "monthElapsed" variable?
> 
> Thanks!
> 
> Xiao
> 
> 
> ________________________________________
> From: Ken Beath [kjbeath at kagi.com]
> Sent: Monday, January 14, 2008 3:29 PM
> To: Zhong, Xiao
> Cc: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Can we do b-spline within lmer function?
> 
> On 15/01/2008, at 2:59 AM, Zhong, Xiao wrote:
> 
>> Hello,
>>
>> I am working on a project that uses glmm. I would like to consider b-
>> spline in the original glmm but I couldn't find how to add b-spline
>> terms to the normal lmer function. Is there anybody who could help
>> me with that?
>>
> 
> library(splines)
> fm2 <- lmer(Reaction ~ bs(Days) + (1|Subject), sleepstudy)
> 
> Ken
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 

-- 
====================================
Vito M.R. Muggeo
Dip.to Sc Statist e Matem `Vianelli'
Universit? di Palermo
viale delle Scienze, edificio 13
90128 Palermo - ITALY
tel: 091 6626240
fax: 091 485726/485612



From bates at stat.wisc.edu  Wed Jan 16 18:58:51 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 16 Jan 2008 11:58:51 -0600
Subject: [R-sig-ME] [R] degrees of freedom and random effects in lmer
In-Reply-To: <497D588A106806408FB0792E4E9A16580157660D@mail2.noble.org>
References: <497D588A106806408FB0792E4E9A16580157660D@mail2.noble.org>
Message-ID: <40e66e0b0801160958h1b563214o78448459a19cba2@mail.gmail.com>

I suggest this discussion be moved to the R-SIG-mixed-models mailing
list which I am cc:ing on this reply.  Please delete the R-help
mailing list from replies to this message.

On Jan 16, 2008 11:44 AM, Feldman, Tracy <tsfeldman at noble.org> wrote:
> Dear All,
>
>
>
> I used lmer for data with non-normally distributed error and both fixed
> and random effects.  I tried to calculate a "Type III" sums of squares
> result, by I conducting likelihood ratio tests of the full model against
> a model reduced by one variable at a time (for each variable
> separately). These tests gave appropriate degrees of freedom for each of
> the two fixed effects, but when I left out one of two random effects
> (each random effect is a categorical variable with 5 and 8 levels,
> respectively) and tested that reduced model against the full model,
> output showed that the test degrees of freedom = 1, which was incorrect.

Why is that incorrect?  The degrees of freedom for a likelihood ratio
test is usually defined as the difference in the number of parameters
and random effects are not parameters.  They are an unobserved level
of random variation.  The parameter associated with the random effects
is, in the simple cases, the variance of the random effects.

> Since I used an experimental design with spatial and temporal
> "blocks"-where I repeated the same experiment several times, with a
> different treatments in each spatial block each time (and with different
> combinations of individuals in each treatment)-I am now thinking that I
> should leave the random effects in the model no matter what (and only
> test for fixed effects).  This leaves me with three related questions:

> 1.      Why do Likelihood Ratio Tests of a full model against a model
> with one less random effect report the incorrect degrees of freedom?

You are more likely to get helpful responses if you avoid value
judgements in your questions.

> Are such tests treating each random variable as one combined entity?  I
> can provide code and data if this would help.
>
>
>
> 2.      In a publication, is it reasonable to report that I only tested
> models that included random effects?  Do I need to report results of a
> test of significance of these random effects (i.e., I am not sure how or
> if I should include any information about the random effects in my
> "ANOVA-type" tables)?
>
>
>
> 3.      If I should test for the significance of random effects, per se
> (and report these), is it more appropriate to simply fit models with and
> without random effects to see if the pattern of fixed effects is
> different?  I can look at random effects using "ranef(model_name)", but
> this function does not assess their significance.
>
>
>
> I am not subscribed to this list, so if possible, please reply to me
> directly at tsfeldman at noble.org .  Thank you for your time and help.
>
>
>
> Sincerely,
>
>
>
> Tracy Feldman
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



From bates at stat.wisc.edu  Wed Jan 16 19:47:55 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 16 Jan 2008 12:47:55 -0600
Subject: [R-sig-ME] [R] degrees of freedom and random effects in lmer
In-Reply-To: <497D588A106806408FB0792E4E9A165801576614@mail2.noble.org>
References: <497D588A106806408FB0792E4E9A165801576614@mail2.noble.org>
Message-ID: <40e66e0b0801161047u485aaaa3l3606a36fce98b560@mail.gmail.com>

I also apologize because I sent an incomplete reply.  I hit the "send"
key sequence before I planned to.

I was going to say that it is not entirely clear exactly what one
should regard as "the degrees of freedom" for random effects terms.
Fixed-effects models have a solid geometric interpretation that gives
an unambiguous definition of degrees of freedom.  Models with random
effects don't have nearly the same clarity.

If one counts parameters to be estimated then random effects for the
levels of a factor cost only 1 degree of freedom, regardless of the
number of levels.  This is the lowest number one could imagine for the
degrees of freedom and, if you regard degrees of freedom as measuring
the explanatory power of a model, this can be a severe underestimate.

If one goes with the geometric argument and measures something like
the dimension of the predictor space then the degrees of freedom would
be the number of levels or that number minus 1, which is what you were
assuming.  This is the same as counting the number of coefficients in
the linear predictor.  The problem here is the predictor doesn't have
all of the degrees of freedom associated with the geometric subspace.
The "estimates" of the random effects are not the solution of a least
squares problem.  They are the solution of a penalized least squares
problem and the penalty has a damping effect on the coefficients.

An argument can be made that the effective degrees of freedom lies
between these two extremes and can be measured according to the trace
of the "hat" matrix.

I really don't know what the best answer is.  In a way I think it is
best to avoid trying to force a definition of degrees of freedom in
mixed models.


On Jan 16, 2008 12:17 PM, Feldman, Tracy <tsfeldman at noble.org> wrote:
> Dear Dr. Bates,
>
> Thank you for your response.  Also, I did not intend to make value
> judgements in my questions below.  I had simply misunderstood what was
> the parameter in the Likelihood Ratio Test (and I assumed that I knew).
> I apologize.
>
> Take care,
> Tracy
>
>
> -----Original Message-----
> From: dmbates at gmail.com [mailto:dmbates at gmail.com] On Behalf Of Douglas
> Bates
> Sent: Wednesday, January 16, 2008 11:59 AM
> To: Feldman, Tracy
> Cc: r-help at r-project.org; R-SIG-Mixed-Models
> Subject: Re: [R] degrees of freedom and random effects in lmer
>
> I suggest this discussion be moved to the R-SIG-mixed-models mailing
> list which I am cc:ing on this reply.  Please delete the R-help
> mailing list from replies to this message.
>
> On Jan 16, 2008 11:44 AM, Feldman, Tracy <tsfeldman at noble.org> wrote:
> > Dear All,
> >
> >
> >
> > I used lmer for data with non-normally distributed error and both
> fixed
> > and random effects.  I tried to calculate a "Type III" sums of squares
> > result, by I conducting likelihood ratio tests of the full model
> against
> > a model reduced by one variable at a time (for each variable
> > separately). These tests gave appropriate degrees of freedom for each
> of
> > the two fixed effects, but when I left out one of two random effects
> > (each random effect is a categorical variable with 5 and 8 levels,
> > respectively) and tested that reduced model against the full model,
> > output showed that the test degrees of freedom = 1, which was
> incorrect.
>
> Why is that incorrect?  The degrees of freedom for a likelihood ratio
> test is usually defined as the difference in the number of parameters
> and random effects are not parameters.  They are an unobserved level
> of random variation.  The parameter associated with the random effects
> is, in the simple cases, the variance of the random effects.
>
> > Since I used an experimental design with spatial and temporal
> > "blocks"-where I repeated the same experiment several times, with a
> > different treatments in each spatial block each time (and with
> different
> > combinations of individuals in each treatment)-I am now thinking that
> I
> > should leave the random effects in the model no matter what (and only
> > test for fixed effects).  This leaves me with three related questions:
>
> > 1.      Why do Likelihood Ratio Tests of a full model against a model
> > with one less random effect report the incorrect degrees of freedom?
>
> You are more likely to get helpful responses if you avoid value
> judgements in your questions.
>
> > Are such tests treating each random variable as one combined entity?
> I
> > can provide code and data if this would help.
> >
> >
> >
> > 2.      In a publication, is it reasonable to report that I only
> tested
> > models that included random effects?  Do I need to report results of a
> > test of significance of these random effects (i.e., I am not sure how
> or
> > if I should include any information about the random effects in my
> > "ANOVA-type" tables)?
> >
> >
> >
> > 3.      If I should test for the significance of random effects, per
> se
> > (and report these), is it more appropriate to simply fit models with
> and
> > without random effects to see if the pattern of fixed effects is
> > different?  I can look at random effects using "ranef(model_name)",
> but
> > this function does not assess their significance.
> >
> >
> >
> > I am not subscribed to this list, so if possible, please reply to me
> > directly at tsfeldman at noble.org .  Thank you for your time and help.
> >
> >
> >
> > Sincerely,
> >
> >
> >
> > Tracy Feldman
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>



From kjbeath at kagi.com  Thu Jan 17 00:07:56 2008
From: kjbeath at kagi.com (Ken Beath)
Date: Thu, 17 Jan 2008 10:07:56 +1100
Subject: [R-sig-ME] Can we do b-spline within lmer function?
In-Reply-To: <80714657CA27844AAEC7E733FD3FF9760A08C0AD73@EXCHANGEMAIL.admin.wpi.edu>
References: <80714657CA27844AAEC7E733FD3FF9760A08C0AD70@EXCHANGEMAIL.admin.wpi.edu>,
	<5DE30F50-9A9D-4A28-817F-E3CC9AB4C8C5@kagi.com>
	<80714657CA27844AAEC7E733FD3FF9760A08C0AD73@EXCHANGEMAIL.admin.wpi.edu>
Message-ID: <BEB0506E-D3F1-4A44-8C39-7A18342B97ED@kagi.com>

On 15/01/2008, at 9:42 AM, Zhong, Xiao wrote:

> Thanks, Ken.
>
> I tried your advice on my model:
>
> model2.growth.mcas5 <- lmer(response ~ bs(monthElapsed) + skills +  
> (1|studentID), data= mcas5, family=binomial(link="logit"), control =
> list(msVerbose = 1, usePQL = FALSE))
>
> It worked! What it gave me is:
>
>                      (Intercept)                   
> bs(monthElapsed)1                   
> bs(monthElapsed)2                  bs(monthElapsed)3
>                        -0.1509712                           
> 0.7019262                          
> -0.1934738                          0.6257330
>                  skillsG-Geometry                skillsM- 
> Measurement    skillsN-Number-Sense-Operations skillsP-Patterns- 
> Relations-Algebra
>                        -0.4333880                          
> -0.9468761                           
> 0.5263370                          0.1897903
>
> Since I am trying to use design matrix to predict after that glmm  
> fitting, could you tell me if there is some function/command that I  
> can use to find what the three "bs" did to my "monthElapsed" variable?
>

As Vito mentioned you probably need a few more knots, see the help on  
bs. The spline models I have fitted have been in lme and have used  
predict to obtain the shape of the curves, but this doesn't seem  
available in lmer.

Ken



From xiaozh at WPI.EDU  Thu Jan 17 02:27:18 2008
From: xiaozh at WPI.EDU (Zhong, Xiao)
Date: Wed, 16 Jan 2008 20:27:18 -0500
Subject: [R-sig-ME] Can we do b-spline within lmer function?
In-Reply-To: <BEB0506E-D3F1-4A44-8C39-7A18342B97ED@kagi.com>
References: <80714657CA27844AAEC7E733FD3FF9760A08C0AD70@EXCHANGEMAIL.admin.wpi.edu>,
	<5DE30F50-9A9D-4A28-817F-E3CC9AB4C8C5@kagi.com>
	<80714657CA27844AAEC7E733FD3FF9760A08C0AD73@EXCHANGEMAIL.admin.wpi.edu>,
	<BEB0506E-D3F1-4A44-8C39-7A18342B97ED@kagi.com>
Message-ID: <80714657CA27844AAEC7E733FD3FF9760A08C0AD93@EXCHANGEMAIL.admin.wpi.edu>

I see. Thanks.

xiao
________________________________________
From: Ken Beath [kjbeath at kagi.com]
Sent: Wednesday, January 16, 2008 6:07 PM
To: Zhong, Xiao
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Can we do b-spline within lmer function?

On 15/01/2008, at 9:42 AM, Zhong, Xiao wrote:

> Thanks, Ken.
>
> I tried your advice on my model:
>
> model2.growth.mcas5 <- lmer(response ~ bs(monthElapsed) + skills +
> (1|studentID), data= mcas5, family=binomial(link="logit"), control =
> list(msVerbose = 1, usePQL = FALSE))
>
> It worked! What it gave me is:
>
>                      (Intercept)
> bs(monthElapsed)1
> bs(monthElapsed)2                  bs(monthElapsed)3
>                        -0.1509712
> 0.7019262
> -0.1934738                          0.6257330
>                  skillsG-Geometry                skillsM-
> Measurement    skillsN-Number-Sense-Operations skillsP-Patterns-
> Relations-Algebra
>                        -0.4333880
> -0.9468761
> 0.5263370                          0.1897903
>
> Since I am trying to use design matrix to predict after that glmm
> fitting, could you tell me if there is some function/command that I
> can use to find what the three "bs" did to my "monthElapsed" variable?
>

As Vito mentioned you probably need a few more knots, see the help on
bs. The spline models I have fitted have been in lme and have used
predict to obtain the shape of the curves, but this doesn't seem
available in lmer.

Ken



From rlevy at ucsd.edu  Thu Jan 17 04:51:13 2008
From: rlevy at ucsd.edu (Roger Levy)
Date: Wed, 16 Jan 2008 19:51:13 -0800
Subject: [R-sig-ME] Development version of lme4 now passes its tests
In-Reply-To: <40e66e0b0801061224t1ac75199m3aceb48f32fb323a@mail.gmail.com>
References: <40e66e0b0801061224t1ac75199m3aceb48f32fb323a@mail.gmail.com>
Message-ID: <478ED0B1.8070102@ucsd.edu>

Douglas Bates wrote:

> This version of glmer allows the quasibinomial and quasipoisson
> families.  I plan to add the Gamma family soon.  Are there any other
> important families that I have missed?

This is very exciting!

I would respectfully add the inverse gaussian as an important family (at 
least to us!).  I take it from the error messages I get when using the 
R-forge version that this family is not supported yet?

Thank you once again for all this remarkable work.

Best

Roger

-- 

Roger Levy                      Email: rlevy at ucsd.edu
Assistant Professor             Phone: 858-534-7219
Department of Linguistics       Fax:   858-534-4789
UC San Diego                    Web:   http://ling.ucsd.edu/~rlevy



From njbisaac at googlemail.com  Thu Jan 17 12:11:31 2008
From: njbisaac at googlemail.com (Nick Isaac)
Date: Thu, 17 Jan 2008 11:11:31 +0000
Subject: [R-sig-ME] Random slopes without random intercepts
Message-ID: <a072ed700801170311x13b1b948x5dafb64ea03b4ebb@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080117/82c12897/attachment.pl>

From njs at pobox.com  Thu Jan 17 12:25:11 2008
From: njs at pobox.com (Nathaniel Smith)
Date: Thu, 17 Jan 2008 11:25:11 +0000
Subject: [R-sig-ME] Random slopes without random intercepts
In-Reply-To: <a072ed700801170311x13b1b948x5dafb64ea03b4ebb@mail.gmail.com>
References: <a072ed700801170311x13b1b948x5dafb64ea03b4ebb@mail.gmail.com>
Message-ID: <20080117112511.GA18354@frances.vorpus.org>

On Thu, Jan 17, 2008 at 11:11:31AM +0000, Nick Isaac wrote:
> Basically, I want to explore the full volume of parameter space for my
> dataset. There's only one fixed effect, but several levels of random
> variation. I can fit all combinations of random intercept and random slope
> models as:
> 
> 1) lmer( y ~ x + (1|A) ...)
> 2) lmer(y ~ x + (x|A) ...)
> 
> The logical way to remove the random intercept is to remove "A" from formula
> 2. But then lmer wouldn't know which level at which to fit the random
> slopes. I tried "A-1" and "(A-1)" but both returned errors.

You want (x-1|A), or (x+0|A) -- the two are identical to each other.

(The trick is that in R, formulas are generally assumed to contain an
intercept term by default.  The intercept term is named "1", so if you
write y ~ x, it's assumed you mean y ~ 1 + x.  You can override this
by explicitly subtracting the 1 off again, or by explicitly putting in
a 0, either way.  The same applies to the specification of random
effect terms.)

-- Nathaniel

-- 
Electrons find their paths in subtle ways.



From vmuggeo at dssm.unipa.it  Thu Jan 17 12:39:35 2008
From: vmuggeo at dssm.unipa.it (vito muggeo)
Date: Thu, 17 Jan 2008 12:39:35 +0100
Subject: [R-sig-ME] a problem with lme4_0.999375-0?
Message-ID: <478F3E77.9030300@dssm.unipa.it>

Dear all,

I'm running data from "Gelman&Hill Data Analysis Using regression.. 
pag259-261" (you can get data by running the code below). In particular 
I'm interested in getting variances of the predictions of the random 
intercepts. I get "right" (as compared with those from the book) results 
  using lme 0.99875-9 (downloaded from CRAN).

However using the latest lme4_0.999375-0 obtained via

install.packages("lme4", repos = "http://r-forge.r-project.org")

the variances of the predictions are wrong..(they are constant 
regardless of different cluster sizes. You can check this running the 
code below..

Many thanks,
vito


srrs2 <- 
read.table("http://www.stat.columbia.edu/~gelman/arm/examples/radon/srrs2.dat",
    header=T, sep=",")

mn <- srrs2$state=="MN"
radon <- srrs2$activity[mn]
log.radon <- log (ifelse (radon==0, .1, radon))
floor <- srrs2$floor[mn]       # 0 for basement, 1 for first floor
n <- length(radon)
y <- log.radon
x <- floor
# get county index variable
county.name <- as.vector(srrs2$county[mn])
uniq <- unique(county.name)
J <- length(uniq)
county <- rep (NA, J)
for (i in 1:J){
   county[county.name==uniq[i]] <- i
}
library(arm)

M1<-lmer(y~1+x+(1|county))
ranef(M1)[[1]][1:10,] #OK

se.ranef(M1)[[1]][1:10,] #wrong results: they are constant!!!??!!!



 > sessionInfo()
R version 2.6.1 (2007-11-26)
i386-pc-mingw32

locale:
LC_COLLATE=Italian_Italy.1252;LC_CTYPE=Italian_Italy.1252;LC_MONETARY=Italian_Italy.1252;LC_NUMERIC=C;LC_TIME=Italian_Italy.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] foreign_0.8-23    car_1.2-7         arm_1.1-1 
R2WinBUGS_2.1-6   coda_0.13-1       lme4_0.999375-0   Matrix_0.999375-4 
lattice_0.17-2
[9] MASS_7.2-38

loaded via a namespace (and not attached):
[1] grid_2.6.1


-- 
====================================
Vito M.R. Muggeo
Dip.to Sc Statist e Matem `Vianelli'
Universit? di Palermo
viale delle Scienze, edificio 13
90128 Palermo - ITALY
tel: 091 6626240
fax: 091 485726/485612



From kjbeath at kagi.com  Thu Jan 17 12:45:54 2008
From: kjbeath at kagi.com (Ken Beath)
Date: Thu, 17 Jan 2008 22:45:54 +1100
Subject: [R-sig-ME] Random slopes without random intercepts
In-Reply-To: <20080117112511.GA18354@frances.vorpus.org>
References: <a072ed700801170311x13b1b948x5dafb64ea03b4ebb@mail.gmail.com>
	<20080117112511.GA18354@frances.vorpus.org>
Message-ID: <6BA0C6B0-6DFD-4930-851A-CBC06E3F74C4@kagi.com>


On 17/01/2008, at 10:25 PM, Nathaniel Smith wrote:

> On Thu, Jan 17, 2008 at 11:11:31AM +0000, Nick Isaac wrote:
>> Basically, I want to explore the full volume of parameter space for  
>> my
>> dataset. There's only one fixed effect, but several levels of random
>> variation. I can fit all combinations of random intercept and  
>> random slope
>> models as:
>>
>> 1) lmer( y ~ x + (1|A) ...)
>> 2) lmer(y ~ x + (x|A) ...)
>>
>> The logical way to remove the random intercept is to remove "A"  
>> from formula
>> 2. But then lmer wouldn't know which level at which to fit the random
>> slopes. I tried "A-1" and "(A-1)" but both returned errors.
>
> You want (x-1|A), or (x+0|A) -- the two are identical to each other.
>


It is very unusual to find data requiring a random slope but no random  
intercept, as it requires that all lines pass through the same point  
on the y axis.

Ken



From vmuggeo at dssm.unipa.it  Thu Jan 17 13:14:17 2008
From: vmuggeo at dssm.unipa.it (vito muggeo)
Date: Thu, 17 Jan 2008 13:14:17 +0100
Subject: [R-sig-ME] a problem with lme4_0.999375-0?
In-Reply-To: <478F3E77.9030300@dssm.unipa.it>
References: <478F3E77.9030300@dssm.unipa.it>
Message-ID: <478F4699.9030107@dssm.unipa.it>

In previous message I used arm::se.ranef() to get st.errs of the 
predictions from the lmer fit.
However the problem occurs also without using arm::se.ranef(), and it 
seems to depend on ranef()..

o<-ranef(M1,postVar=TRUE)
sqrt(attr(o[[1]],"postVar")[,,1:5])

regards,
vito

vito muggeo ha scritto:
> Dear all,
> 
> I'm running data from "Gelman&Hill Data Analysis Using regression.. 
> pag259-261" (you can get data by running the code below). In particular 
> I'm interested in getting variances of the predictions of the random 
> intercepts. I get "right" (as compared with those from the book) results 
>   using lme 0.99875-9 (downloaded from CRAN).
> 
> However using the latest lme4_0.999375-0 obtained via
> 
> install.packages("lme4", repos = "http://r-forge.r-project.org")
> 
> the variances of the predictions are wrong..(they are constant 
> regardless of different cluster sizes. You can check this running the 
> code below..
> 
> Many thanks,
> vito
> 
> 
> srrs2 <- 
> read.table("http://www.stat.columbia.edu/~gelman/arm/examples/radon/srrs2.dat",
>     header=T, sep=",")
> 
> mn <- srrs2$state=="MN"
> radon <- srrs2$activity[mn]
> log.radon <- log (ifelse (radon==0, .1, radon))
> floor <- srrs2$floor[mn]       # 0 for basement, 1 for first floor
> n <- length(radon)
> y <- log.radon
> x <- floor
> # get county index variable
> county.name <- as.vector(srrs2$county[mn])
> uniq <- unique(county.name)
> J <- length(uniq)
> county <- rep (NA, J)
> for (i in 1:J){
>    county[county.name==uniq[i]] <- i
> }
> library(arm)
> 
> M1<-lmer(y~1+x+(1|county))
> ranef(M1)[[1]][1:10,] #OK
> 
> se.ranef(M1)[[1]][1:10,] #wrong results: they are constant!!!??!!!
> 
> 
> 
>  > sessionInfo()
> R version 2.6.1 (2007-11-26)
> i386-pc-mingw32
> 
> locale:
> LC_COLLATE=Italian_Italy.1252;LC_CTYPE=Italian_Italy.1252;LC_MONETARY=Italian_Italy.1252;LC_NUMERIC=C;LC_TIME=Italian_Italy.1252
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> 
> other attached packages:
> [1] foreign_0.8-23    car_1.2-7         arm_1.1-1 
> R2WinBUGS_2.1-6   coda_0.13-1       lme4_0.999375-0   Matrix_0.999375-4 
> lattice_0.17-2
> [9] MASS_7.2-38
> 
> loaded via a namespace (and not attached):
> [1] grid_2.6.1
> 
> 

-- 
====================================
Vito M.R. Muggeo
Dip.to Sc Statist e Matem `Vianelli'
Universit? di Palermo
viale delle Scienze, edificio 13
90128 Palermo - ITALY
tel: 091 6626240
fax: 091 485726/485612



From bates at stat.wisc.edu  Thu Jan 17 14:42:31 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 17 Jan 2008 07:42:31 -0600
Subject: [R-sig-ME] a problem with lme4_0.999375-0?
In-Reply-To: <478F3E77.9030300@dssm.unipa.it>
References: <478F3E77.9030300@dssm.unipa.it>
Message-ID: <40e66e0b0801170542l119aa4a2hba3625f75ce15246@mail.gmail.com>

Thanks for the thorough bug report, Vito.  I have added it to the bug
report list at http://r-forge.r-project.org/projects/lme4 (under the
Tracker tab).  Those with R-forge logins can add bug reports there
directly if they wish.  Those who do not have an R-forge login should
consider getting one.

I appreciate the example.  It will make it a lot easier to isolate the problem.

On Jan 17, 2008 5:39 AM, vito muggeo <vmuggeo at dssm.unipa.it> wrote:
> Dear all,
>
> I'm running data from "Gelman&Hill Data Analysis Using regression..
> pag259-261" (you can get data by running the code below). In particular
> I'm interested in getting variances of the predictions of the random
> intercepts. I get "right" (as compared with those from the book) results
>   using lme 0.99875-9 (downloaded from CRAN).
>
> However using the latest lme4_0.999375-0 obtained via
>
> install.packages("lme4", repos = "http://r-forge.r-project.org")
>
> the variances of the predictions are wrong..(they are constant
> regardless of different cluster sizes. You can check this running the
> code below..
>
> Many thanks,
> vito
>
>
> srrs2 <-
> read.table("http://www.stat.columbia.edu/~gelman/arm/examples/radon/srrs2.dat",
>     header=T, sep=",")
>
> mn <- srrs2$state=="MN"
> radon <- srrs2$activity[mn]
> log.radon <- log (ifelse (radon==0, .1, radon))
> floor <- srrs2$floor[mn]       # 0 for basement, 1 for first floor
> n <- length(radon)
> y <- log.radon
> x <- floor
> # get county index variable
> county.name <- as.vector(srrs2$county[mn])
> uniq <- unique(county.name)
> J <- length(uniq)
> county <- rep (NA, J)
> for (i in 1:J){
>    county[county.name==uniq[i]] <- i
> }
> library(arm)
>
> M1<-lmer(y~1+x+(1|county))
> ranef(M1)[[1]][1:10,] #OK
>
> se.ranef(M1)[[1]][1:10,] #wrong results: they are constant!!!??!!!
>
>
>
>  > sessionInfo()
> R version 2.6.1 (2007-11-26)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=Italian_Italy.1252;LC_CTYPE=Italian_Italy.1252;LC_MONETARY=Italian_Italy.1252;LC_NUMERIC=C;LC_TIME=Italian_Italy.1252
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] foreign_0.8-23    car_1.2-7         arm_1.1-1
> R2WinBUGS_2.1-6   coda_0.13-1       lme4_0.999375-0   Matrix_0.999375-4
> lattice_0.17-2
> [9] MASS_7.2-38
>
> loaded via a namespace (and not attached):
> [1] grid_2.6.1
>
>
> --
> ====================================
> Vito M.R. Muggeo
> Dip.to Sc Statist e Matem `Vianelli'
> Universit? di Palermo
> viale delle Scienze, edificio 13
> 90128 Palermo - ITALY
> tel: 091 6626240
> fax: 091 485726/485612
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Thu Jan 17 19:02:38 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 17 Jan 2008 12:02:38 -0600
Subject: [R-sig-ME] a problem with lme4_0.999375-0?
In-Reply-To: <40e66e0b0801170542l119aa4a2hba3625f75ce15246@mail.gmail.com>
References: <478F3E77.9030300@dssm.unipa.it>
	<40e66e0b0801170542l119aa4a2hba3625f75ce15246@mail.gmail.com>
Message-ID: <40e66e0b0801171002m3ade0fc2gc0e485eea6f02007@mail.gmail.com>

Thanks again for spotting that problem, Vito.  Because of the need for
generality, the C code in the mer_postVar function ends up being
fairly involved.  Once I traced through the example, however, I could
readily spot that I was running a loop over the levels of each of the
grouping factors but I wasn't using that loop index to change the
input values. Therefore, the result was exactly what you observed,
those values all ended up corresponding to the results for the first
level of the grouping factor.

Do the results in the enclosed file look like what you expect? I
rewrote the code that you sent because I found some aspects of that
code to be rather frightening.  (Is that code directly from the book?)

I will commit the changes to the SVN archive on R-forge by this
evening (North American time).  I have been tearing apart my code
again and right now I have lmer and glmer running but nlmer is having
problems.  I'll close out the bug report after I commit the changes.

On Jan 17, 2008 7:42 AM, Douglas Bates <bates at stat.wisc.edu> wrote:
> Thanks for the thorough bug report, Vito.  I have added it to the bug
> report list at http://r-forge.r-project.org/projects/lme4 (under the
> Tracker tab).  Those with R-forge logins can add bug reports there
> directly if they wish.  Those who do not have an R-forge login should
> consider getting one.
>
> I appreciate the example.  It will make it a lot easier to isolate the problem.
>
>
> On Jan 17, 2008 5:39 AM, vito muggeo <vmuggeo at dssm.unipa.it> wrote:
> > Dear all,
> >
> > I'm running data from "Gelman&Hill Data Analysis Using regression..
> > pag259-261" (you can get data by running the code below). In particular
> > I'm interested in getting variances of the predictions of the random
> > intercepts. I get "right" (as compared with those from the book) results
> >   using lme 0.99875-9 (downloaded from CRAN).
> >
> > However using the latest lme4_0.999375-0 obtained via
> >
> > install.packages("lme4", repos = "http://r-forge.r-project.org")
> >
> > the variances of the predictions are wrong..(they are constant
> > regardless of different cluster sizes. You can check this running the
> > code below..
> >
> > Many thanks,
> > vito
> >
> >
> > srrs2 <-
> > read.table("http://www.stat.columbia.edu/~gelman/arm/examples/radon/srrs2.dat",
> >     header=T, sep=",")
> >
> > mn <- srrs2$state=="MN"
> > radon <- srrs2$activity[mn]
> > log.radon <- log (ifelse (radon==0, .1, radon))
> > floor <- srrs2$floor[mn]       # 0 for basement, 1 for first floor
> > n <- length(radon)
> > y <- log.radon
> > x <- floor
> > # get county index variable
> > county.name <- as.vector(srrs2$county[mn])
> > uniq <- unique(county.name)
> > J <- length(uniq)
> > county <- rep (NA, J)
> > for (i in 1:J){
> >    county[county.name==uniq[i]] <- i
> > }
> > library(arm)
> >
> > M1<-lmer(y~1+x+(1|county))
> > ranef(M1)[[1]][1:10,] #OK
> >
> > se.ranef(M1)[[1]][1:10,] #wrong results: they are constant!!!??!!!
> >
> >
> >
> >  > sessionInfo()
> > R version 2.6.1 (2007-11-26)
> > i386-pc-mingw32
> >
> > locale:
> > LC_COLLATE=Italian_Italy.1252;LC_CTYPE=Italian_Italy.1252;LC_MONETARY=Italian_Italy.1252;LC_NUMERIC=C;LC_TIME=Italian_Italy.1252
> >
> > attached base packages:
> > [1] stats     graphics  grDevices utils     datasets  methods   base
> >
> > other attached packages:
> > [1] foreign_0.8-23    car_1.2-7         arm_1.1-1
> > R2WinBUGS_2.1-6   coda_0.13-1       lme4_0.999375-0   Matrix_0.999375-4
> > lattice_0.17-2
> > [9] MASS_7.2-38
> >
> > loaded via a namespace (and not attached):
> > [1] grid_2.6.1
> >
> >
> > --
> > ====================================
> > Vito M.R. Muggeo
> > Dip.to Sc Statist e Matem `Vianelli'
> > Universit? di Palermo
> > viale delle Scienze, edificio 13
> > 90128 Palermo - ITALY
> > tel: 091 6626240
> > fax: 091 485726/485612
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: Vito.txt
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080117/3e211b9b/attachment.txt>

From Sophie.Bestley at csiro.au  Fri Jan 18 08:26:38 2008
From: Sophie.Bestley at csiro.au (Sophie.Bestley at csiro.au)
Date: Fri, 18 Jan 2008 18:26:38 +1100
Subject: [R-sig-ME] Prediction for "new" data from a binomial glmm fitted
	using lmer
Message-ID: <FA7C85FEEB131B4EA3B1CA66D94E7E8002923E@extas4-hba.tas.csiro.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080118/7eae5977/attachment.pl>

From vmuggeo at dssm.unipa.it  Fri Jan 18 10:18:36 2008
From: vmuggeo at dssm.unipa.it (vito muggeo)
Date: Fri, 18 Jan 2008 10:18:36 +0100
Subject: [R-sig-ME] a problem with lme4_0.999375-0?
In-Reply-To: <40e66e0b0801171002m3ade0fc2gc0e485eea6f02007@mail.gmail.com>
References: <478F3E77.9030300@dssm.unipa.it>	
	<40e66e0b0801170542l119aa4a2hba3625f75ce15246@mail.gmail.com>
	<40e66e0b0801171002m3ade0fc2gc0e485eea6f02007@mail.gmail.com>
Message-ID: <47906EEC.60800@dssm.unipa.it>

Dear Douglas,
Many thanks for finding out the error and fixing it.

Yes the results mach those from the book and the code comes from

http://www.stat.columbia.edu/~gelman/arm/examples/radon/radon_setup.R

I look forward to dowloading the new version,
many thanks again
all the best,

vito



Douglas Bates ha scritto:
> Thanks again for spotting that problem, Vito.  Because of the need for
> generality, the C code in the mer_postVar function ends up being
> fairly involved.  Once I traced through the example, however, I could
> readily spot that I was running a loop over the levels of each of the
> grouping factors but I wasn't using that loop index to change the
> input values. Therefore, the result was exactly what you observed,
> those values all ended up corresponding to the results for the first
> level of the grouping factor.
> 
> Do the results in the enclosed file look like what you expect? I
> rewrote the code that you sent because I found some aspects of that
> code to be rather frightening.  (Is that code directly from the book?)
> 
> I will commit the changes to the SVN archive on R-forge by this
> evening (North American time).  I have been tearing apart my code
> again and right now I have lmer and glmer running but nlmer is having
> problems.  I'll close out the bug report after I commit the changes.
> 
> On Jan 17, 2008 7:42 AM, Douglas Bates <bates at stat.wisc.edu> wrote:
>> Thanks for the thorough bug report, Vito.  I have added it to the bug
>> report list at http://r-forge.r-project.org/projects/lme4 (under the
>> Tracker tab).  Those with R-forge logins can add bug reports there
>> directly if they wish.  Those who do not have an R-forge login should
>> consider getting one.
>>
>> I appreciate the example.  It will make it a lot easier to isolate the problem.
>>
>>
>> On Jan 17, 2008 5:39 AM, vito muggeo <vmuggeo at dssm.unipa.it> wrote:
>>> Dear all,
>>>
>>> I'm running data from "Gelman&Hill Data Analysis Using regression..
>>> pag259-261" (you can get data by running the code below). In particular
>>> I'm interested in getting variances of the predictions of the random
>>> intercepts. I get "right" (as compared with those from the book) results
>>>   using lme 0.99875-9 (downloaded from CRAN).
>>>
>>> However using the latest lme4_0.999375-0 obtained via
>>>
>>> install.packages("lme4", repos = "http://r-forge.r-project.org")
>>>
>>> the variances of the predictions are wrong..(they are constant
>>> regardless of different cluster sizes. You can check this running the
>>> code below..
>>>
>>> Many thanks,
>>> vito
>>>
>>>
>>> srrs2 <-
>>> read.table("http://www.stat.columbia.edu/~gelman/arm/examples/radon/srrs2.dat",
>>>     header=T, sep=",")
>>>
>>> mn <- srrs2$state=="MN"
>>> radon <- srrs2$activity[mn]
>>> log.radon <- log (ifelse (radon==0, .1, radon))
>>> floor <- srrs2$floor[mn]       # 0 for basement, 1 for first floor
>>> n <- length(radon)
>>> y <- log.radon
>>> x <- floor
>>> # get county index variable
>>> county.name <- as.vector(srrs2$county[mn])
>>> uniq <- unique(county.name)
>>> J <- length(uniq)
>>> county <- rep (NA, J)
>>> for (i in 1:J){
>>>    county[county.name==uniq[i]] <- i
>>> }
>>> library(arm)
>>>
>>> M1<-lmer(y~1+x+(1|county))
>>> ranef(M1)[[1]][1:10,] #OK
>>>
>>> se.ranef(M1)[[1]][1:10,] #wrong results: they are constant!!!??!!!
>>>
>>>
>>>
>>>  > sessionInfo()
>>> R version 2.6.1 (2007-11-26)
>>> i386-pc-mingw32
>>>
>>> locale:
>>> LC_COLLATE=Italian_Italy.1252;LC_CTYPE=Italian_Italy.1252;LC_MONETARY=Italian_Italy.1252;LC_NUMERIC=C;LC_TIME=Italian_Italy.1252
>>>
>>> attached base packages:
>>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>>
>>> other attached packages:
>>> [1] foreign_0.8-23    car_1.2-7         arm_1.1-1
>>> R2WinBUGS_2.1-6   coda_0.13-1       lme4_0.999375-0   Matrix_0.999375-4
>>> lattice_0.17-2
>>> [9] MASS_7.2-38
>>>
>>> loaded via a namespace (and not attached):
>>> [1] grid_2.6.1
>>>
>>>
>>> --
>>> ====================================
>>> Vito M.R. Muggeo
>>> Dip.to Sc Statist e Matem `Vianelli'
>>> Universit? di Palermo
>>> viale delle Scienze, edificio 13
>>> 90128 Palermo - ITALY
>>> tel: 091 6626240
>>> fax: 091 485726/485612
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>

-- 
====================================
Vito M.R. Muggeo
Dip.to Sc Statist e Matem `Vianelli'
Universit? di Palermo
viale delle Scienze, edificio 13
90128 Palermo - ITALY
tel: 091 6626240
fax: 091 485726/485612



From vlp1 at CDC.GOV  Fri Jan 18 14:08:00 2008
From: vlp1 at CDC.GOV (Parsons, Van L. (CDC/CCHIS/NCHS))
Date: Fri, 18 Jan 2008 08:08:00 -0500
Subject: [R-sig-ME] rgui crash with lme4_0.999375-0
In-Reply-To: <CB6AFE613082DD4FB037471AE7A1EDAF1A43E7@LTA3VS013.ees.hhs.gov>
References: <CB6AFE613082DD4FB037471AE7A1EDAF1A43E7@LTA3VS013.ees.hhs.gov>
Message-ID: <CB6AFE613082DD4FB037471AE7A1EDAF1A43E9@LTA3VS013.ees.hhs.gov>

 
 
Hello,
 I have experienced the Rgui for Windows crashing  when using weights in
the lme4_0.999375-0  lmer function. 
 
  The lme4_0.99875-9  version works fine. 
 
  I use the lme4 provided sleepstudy data with weights = Days to
reproduce the problem.
  
  
 
> sessionInfo()
R version 2.6.1 (2007-11-26)
i386-pc-mingw32 

locale:
LC_COLLATE=English_United States.1252;
LC_CTYPE=English_United States.1252;
LC_MONETARY=English_United States.1252;LC_NUMERIC=C;
LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base


other attached packages:
[1] lme4_0.999375-0   Matrix_0.999375-4 lattice_0.17-4   

loaded via a namespace (and not attached):
[1] grid_2.6.1
> 
 # lmer example   with weights that crashes 
 
  junk <- lmer(Reaction ~   (1|Subject), sleepstudy, weights=Days)

### crash results below
#################################################################
R for Windows GUI front-end has encountered a problem and needs to
close.  
We are sorry for the inconvenience.

Error signature
AppName: rgui.exe    AppVer: 2.61.43537.0    ModName: lme4.dll
ModVer: 2.61.44000.0     Offset: 00006209
#
###################################################

Thanks,

Van



From bates at stat.wisc.edu  Fri Jan 18 16:53:40 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 18 Jan 2008 09:53:40 -0600
Subject: [R-sig-ME] rgui crash with lme4_0.999375-0
In-Reply-To: <CB6AFE613082DD4FB037471AE7A1EDAF1A43E9@LTA3VS013.ees.hhs.gov>
References: <CB6AFE613082DD4FB037471AE7A1EDAF1A43E7@LTA3VS013.ees.hhs.gov>
	<CB6AFE613082DD4FB037471AE7A1EDAF1A43E9@LTA3VS013.ees.hhs.gov>
Message-ID: <40e66e0b0801180753r51011cf2vd967aa9f2091b57f@mail.gmail.com>

Thanks for the report with a reproducible example.  I have entered it
in the bug tracker at http://r-forge.r-project.org/projects/lme4

I am working on a fix to this now.

On Jan 18, 2008 7:08 AM, Parsons, Van L. (CDC/CCHIS/NCHS) <vlp1 at cdc.gov> wrote:
>
>
> Hello,
>  I have experienced the Rgui for Windows crashing  when using weights in
> the lme4_0.999375-0  lmer function.
>
>   The lme4_0.99875-9  version works fine.
>
>   I use the lme4 provided sleepstudy data with weights = Days to
> reproduce the problem.
>
>
>
> > sessionInfo()
> R version 2.6.1 (2007-11-26)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=English_United States.1252;
> LC_CTYPE=English_United States.1252;
> LC_MONETARY=English_United States.1252;LC_NUMERIC=C;
> LC_TIME=English_United States.1252
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
>
> other attached packages:
> [1] lme4_0.999375-0   Matrix_0.999375-4 lattice_0.17-4
>
> loaded via a namespace (and not attached):
> [1] grid_2.6.1
> >
>  # lmer example   with weights that crashes
>
>   junk <- lmer(Reaction ~   (1|Subject), sleepstudy, weights=Days)
>
> ### crash results below
> #################################################################
> R for Windows GUI front-end has encountered a problem and needs to
> close.
> We are sorry for the inconvenience.
>
> Error signature
> AppName: rgui.exe    AppVer: 2.61.43537.0    ModName: lme4.dll
> ModVer: 2.61.44000.0     Offset: 00006209
> #
> ###################################################
>
> Thanks,
>
> Van
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From a.fugard at ed.ac.uk  Fri Jan 18 20:19:59 2008
From: a.fugard at ed.ac.uk (Andy Fugard)
Date: Fri, 18 Jan 2008 19:19:59 +0000
Subject: [R-sig-ME] Can we do b-spline within lmer function?
In-Reply-To: <BEB0506E-D3F1-4A44-8C39-7A18342B97ED@kagi.com>
References: <80714657CA27844AAEC7E733FD3FF9760A08C0AD70@EXCHANGEMAIL.admin.wpi.edu>,
	<5DE30F50-9A9D-4A28-817F-E3CC9AB4C8C5@kagi.com>	<80714657CA27844AAEC7E733FD3FF9760A08C0AD73@EXCHANGEMAIL.admin.wpi.edu>
	<BEB0506E-D3F1-4A44-8C39-7A18342B97ED@kagi.com>
Message-ID: <4790FBDF.4020703@ed.ac.uk>

Ken Beath wrote:
> On 15/01/2008, at 9:42 AM, Zhong, Xiao wrote:
> 
>> Thanks, Ken.
>>
>> I tried your advice on my model:
>>
>> model2.growth.mcas5 <- lmer(response ~ bs(monthElapsed) + skills +  
>> (1|studentID), data= mcas5, family=binomial(link="logit"), control =
>> list(msVerbose = 1, usePQL = FALSE))
>>
>> It worked! What it gave me is:
>>
>>                      (Intercept)                   
>> bs(monthElapsed)1                   
>> bs(monthElapsed)2                  bs(monthElapsed)3
>>                        -0.1509712                           
>> 0.7019262                          
>> -0.1934738                          0.6257330
>>                  skillsG-Geometry                skillsM- 
>> Measurement    skillsN-Number-Sense-Operations skillsP-Patterns- 
>> Relations-Algebra
>>                        -0.4333880                          
>> -0.9468761                           
>> 0.5263370                          0.1897903
>>
>> Since I am trying to use design matrix to predict after that glmm  
>> fitting, could you tell me if there is some function/command that I  
>> can use to find what the three "bs" did to my "monthElapsed" variable?
>>
> 
> As Vito mentioned you probably need a few more knots, see the help on  
> bs. The spline models I have fitted have been in lme and have used  
> predict to obtain the shape of the curves, but this doesn't seem  
> available in lmer.

I have been reading this thread with interest.  When bs uses only a 
cubic, presumably it's easy to wire up a custom predict function based 
on the fixed effects, using coef - for this example piping the output 
through the inverse logit function to get back something in [0,1].  But 
what does the formula look like when you use proper splines?  I can't 
get that from the help.

Andy



From bates at stat.wisc.edu  Tue Jan 22 15:51:01 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 22 Jan 2008 08:51:01 -0600
Subject: [R-sig-ME] Alpha version of lme4_1.0.0 now on R-forge
Message-ID: <40e66e0b0801220651ic7c6a33h62f78bed44061c0c@mail.gmail.com>

I have updated the svn archive on R-forge to release 0.999375-1 of the
lme4 package.  The mcmcsamp function and the nlmer function still need
work but the lmer and glmer functions and the summaries of models fit
by these functions are stable, I think.

I would appreciate feedback and bug reports.  Please check the Bugs
tracker and the Feature Requests tracker at
http://r-forge.r-project.org/projects/lme4 to see what is currently in
the queue.

Doxygen documentation for the underlying C code is available at
http://lme4.r-forge.r-project.org/www/doxygen/



From bates at stat.wisc.edu  Tue Jan 22 20:27:59 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 22 Jan 2008 13:27:59 -0600
Subject: [R-sig-ME] Alpha version of lme4_1.0.0 now on R-forge
In-Reply-To: <40e66e0b0801220651ic7c6a33h62f78bed44061c0c@mail.gmail.com>
References: <40e66e0b0801220651ic7c6a33h62f78bed44061c0c@mail.gmail.com>
Message-ID: <40e66e0b0801221127ybe495dch3c2fd99655d191ab@mail.gmail.com>

I should have added that you can install this alpha version with

install.packages("lme4", repos = "http://r-forge.r-project.org")

There is a Windows binary package but no Mac OS X binary.  If you are
running OS X and have installed the tools for compiling R packages
then you can specify type = "source" in the call to
install.packages().

On Jan 22, 2008 8:51 AM, Douglas Bates <bates at stat.wisc.edu> wrote:
> I have updated the svn archive on R-forge to release 0.999375-1 of the
> lme4 package.  The mcmcsamp function and the nlmer function still need
> work but the lmer and glmer functions and the summaries of models fit
> by these functions are stable, I think.
>
> I would appreciate feedback and bug reports.  Please check the Bugs
> tracker and the Feature Requests tracker at
> http://r-forge.r-project.org/projects/lme4 to see what is currently in
> the queue.
>
> Doxygen documentation for the underlying C code is available at
> http://lme4.r-forge.r-project.org/www/doxygen/
>



From fauna at pngp.it  Wed Jan 23 13:22:10 2008
From: fauna at pngp.it (Achaz von Hardenberg)
Date: Wed, 23 Jan 2008 13:22:10 +0100
Subject: [R-sig-ME] Lme doubt...
Message-ID: <6683BAF7-0A1D-4E1A-B8DE-962E06B00AEB@pngp.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080123/8b2ae25c/attachment.pl>

From mdu at ceh.ac.uk  Wed Jan 23 13:42:07 2008
From: mdu at ceh.ac.uk (Mike Dunbar)
Date: Wed, 23 Jan 2008 12:42:07 +0000
Subject: [R-sig-ME] Lme doubt...
Message-ID: <s797362f.091@wpo.nerc.ac.uk>

Dear Achaz

Two suggestions:
If you include the extra lake level predictors and they are significant then you will reduce the magnitude of the lake random effect, and hence you may improve the estimation of the fish effect which is also at the lake level. I'm guessing that reducing the lake random effect won't affect the testing of the month and year factors. 
Secondly, if you didn't have any lake-level predictors that you were interested in a priori then it simply depends on whether you are interested in the magitude of these additional effects on the zooplankton, if you are then include them, if not then their effects will be mopped up into the random effect.

regards

Mike Dunbar



>>> Achaz von Hardenberg <fauna at pngp.it> 23/01/2008 12:22 >>>
Dear all,

I am analyzing data regarding the effects of introduced fish on the  
biodiversity of 12 alpine lakes. I have 6 lakes with fish and 6  
without and my dependent variables are various repeated density  
measurements of zooplankton taxa.
As we repeated the biodiversity measurements twice a year for two  
years (2006 and 2007) I am analyzing this data with  the lme function  
of the nlme package with lake identity as random grouping factor  
(random = ~1|as.factor(Cod)), and the presence/absence of fish as  
fixed effect. I included also the year and month of sampling as fixed  
effects.
The full model specification is the following:

zoopcrost.lme<-lme(Logcrostacei~as.factor(year)+as.factor(month) 
+as.factor(fish),method="ML", random=~1|as.factor(Cod),  
data=zooplankton, na.action=na.omit)

Now, I have a doubt....
The lakes differ among each other for some physical characteristics  
such as altitude, maximum depth etc....Obviously these parameters do  
not vary within each sampling as they are specific for each lake  
(they do not vary from one year to the next or from one month to the  
next) but they may have an influence on the densities of zooplankton  
taxa, regardless of the presence/absence of introduced fish. Should I  
nonetheless consider these variables among the fixed effects? Or  
should the fact that I impose the lake identity as grouping factor  
take care of the fact that there are individual differences among the  
lakes?

Sorry for possibly a naive question and thank you for your help!

Dr. Achaz von Hardenberg
------------------------------------------------------------------------ 
--------------------------------
Centro Studi Fauna Alpina - Alpine Wildlife Research Centre
Servizio Sanitario e della Ricerca Scientifica
Parco Nazionale Gran Paradiso, Degioz, 11, 11010-Valsavarenche (Ao),  
Italy
-----------------------------------------------------------------





	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
This message (and any attachments) is for the recipient ...{{dropped:6}}



From vlp1 at CDC.GOV  Wed Jan 23 21:14:00 2008
From: vlp1 at CDC.GOV (Parsons, Van L. (CDC/CCHIS/NCHS))
Date: Wed, 23 Jan 2008 15:14:00 -0500
Subject: [R-sig-ME] Binomial problem for lme4_0.999375-1: 0,1 vs xbar n
In-Reply-To: <40e66e0b0801221127ybe495dch3c2fd99655d191ab@mail.gmail.com>
References: <40e66e0b0801220651ic7c6a33h62f78bed44061c0c@mail.gmail.com>
	<40e66e0b0801221127ybe495dch3c2fd99655d191ab@mail.gmail.com>
Message-ID: <CB6AFE613082DD4FB037471AE7A1EDAF52A9DB@LTA3VS013.ees.hhs.gov>

Hello,
  The lme4_0.999375-1 lmer run on a  full response (0,1) dataset
  is not consistent with the run on an equivalent, but condensed,
  (pbar,n) dataset for the binomial.  
  
  For lme4_0.99875-9, the outputs appear to be order-of-magnitude
  consistent, so the alpha version seems to have the problem.  
  ( I am focusing on the random and fixed effects output )

  Here are some sample code and outputs generated from sleepstudy
  in lme4 using 2 datasets and both new and old lme4. 
  
  Thanks, 
  Van
  
#=======================================================
# R code 
library(lme4)
library(doBy)
sessionInfo() 


 sleep1 = sleepstudy   # from lme4 pkg

     # add binary variable
 sleep1$p1 = ifelse(sleep1$Reaction < mean(sleep1$Reaction),0,1)

   # condense binary to mean and count within Subject 
   # use doBy package 
sleep1c= summaryBy(p1~Subject, data = sleep1,FUN = c(mean,length) )   
   names(sleep1c)[ c(2,3)] = c("p1", "n1")

   

 #--------------------------------------------
  # simple model

mod1  =p1~ 1 + (1| Subject)     

# lmer run   on full   data  
mfull   =  lmer(mod1, data=sleep1,  family=binomial )
summary(mfull)

# lmer run   on   condensed data  
mcond   =  lmer(mod1, data=sleep1c, weights= n1,family=binomial )
summary(mcond) 

#--------------------------------------------
#====================================================================



## 4  partial outputs  

#====================================================================

lme4_0.999375-1  full data
mfull   =  lmer(mod1, data=sleep1,  family=binomial )
   Data: sleep1 
   AIC   BIC logLik deviance
 238.5 244.9 -117.2    234.5
Random effects:
 Groups  Name        Variance Std.Dev.
 Subject (Intercept) 1.0231   1.0115  
Number of obs: 180, groups: Subject, 18

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)  -0.3213     0.2887  -1.113    0.266
#====================================================================
# this next  run is not consistent with the other 3 

lme4_0.999375-1  condensed  data

mcond   =  lmer(mod1, data=sleep1c, weights= n1,family=binomial )
   Data: sleep1c 
   AIC   BIC logLik deviance
 9.622 11.40 -2.811    5.622
Random effects:
 Groups  Name        Variance Std.Dev.
 Subject (Intercept)  0        0      
Number of obs: 18, groups: Subject, 18

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)  -0.2457     0.4750 -0.5173    0.605
#====================================================================
lme4_0.99875-9 full 

> mfull   =  lmer(mod1, data=sleep1,  family=binomial )
   AIC   BIC logLik deviance
 238.5 244.9 -117.2    234.5
Random effects:
 Groups  Name        Variance Std.Dev.
 Subject (Intercept) 1.0420   1.0208  
number of obs: 180, groups: Subject, 18

Estimated scale (compare to  1 )  0.9369863 

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)  -0.3309     0.2906  -1.139    0.255



#====================================================================
lme4_0.99875-9 condensed 
 mcond   =  lmer(mod1, data=sleep1c, weights= n1,family=binomial )

   AIC   BIC logLik deviance
 47.88 49.66 -21.94    43.88
Random effects:
 Groups  Name        Variance Std.Dev.
 Subject (Intercept) 1.0241   1.012   
number of obs: 18, groups: Subject, 18

Estimated scale (compare to  1 )  1.012022 

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)  -0.3183     0.2888  -1.102    0.270



#====================================================================
> sessionInfo() 
R version 2.6.1 (2007-11-26) 
i386-pc-mingw32 

locale:
LC_COLLATE=English_United States.1252;
LC_CTYPE=English_United States.1252;
LC_MONETARY=English_United States.1252;
LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices utils   datasets  methods   base     

other attached packages:
[1] doBy_2.1    lme4_0.999375-1   Matrix_0.999375-4 lattice_0.17-4   

loaded via a namespace (and not attached):
[1] cluster_1.11.9 grid_2.6.1     Hmisc_3.4-3   
 
 



From bates at stat.wisc.edu  Wed Jan 23 21:30:04 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 23 Jan 2008 14:30:04 -0600
Subject: [R-sig-ME] Binomial problem for lme4_0.999375-1: 0,1 vs xbar n
In-Reply-To: <CB6AFE613082DD4FB037471AE7A1EDAF52A9DB@LTA3VS013.ees.hhs.gov>
References: <40e66e0b0801220651ic7c6a33h62f78bed44061c0c@mail.gmail.com>
	<40e66e0b0801221127ybe495dch3c2fd99655d191ab@mail.gmail.com>
	<CB6AFE613082DD4FB037471AE7A1EDAF52A9DB@LTA3VS013.ees.hhs.gov>
Message-ID: <40e66e0b0801231230p63616bi65f84d7df46ed481@mail.gmail.com>

Thanks for the report, Van.  I have entered it in the bug tracker list.

I had thought that I had checked such results for consistency but
apparently I didn't.

On Jan 23, 2008 2:14 PM, Parsons, Van L. (CDC/CCHIS/NCHS) <vlp1 at cdc.gov> wrote:
> Hello,
>   The lme4_0.999375-1 lmer run on a  full response (0,1) dataset
>   is not consistent with the run on an equivalent, but condensed,
>   (pbar,n) dataset for the binomial.
>
>   For lme4_0.99875-9, the outputs appear to be order-of-magnitude
>   consistent, so the alpha version seems to have the problem.
>   ( I am focusing on the random and fixed effects output )
>
>   Here are some sample code and outputs generated from sleepstudy
>   in lme4 using 2 datasets and both new and old lme4.
>
>   Thanks,
>   Van
>
> #=======================================================
> # R code
> library(lme4)
> library(doBy)
> sessionInfo()
>
>
>  sleep1 = sleepstudy   # from lme4 pkg
>
>      # add binary variable
>  sleep1$p1 = ifelse(sleep1$Reaction < mean(sleep1$Reaction),0,1)
>
>    # condense binary to mean and count within Subject
>    # use doBy package
> sleep1c= summaryBy(p1~Subject, data = sleep1,FUN = c(mean,length) )
>    names(sleep1c)[ c(2,3)] = c("p1", "n1")
>
>
>
>  #--------------------------------------------
>   # simple model
>
> mod1  =p1~ 1 + (1| Subject)
>
> # lmer run   on full   data
> mfull   =  lmer(mod1, data=sleep1,  family=binomial )
> summary(mfull)
>
> # lmer run   on   condensed data
> mcond   =  lmer(mod1, data=sleep1c, weights= n1,family=binomial )
> summary(mcond)
>
> #--------------------------------------------
> #====================================================================
>
>
>
> ## 4  partial outputs
>
> #====================================================================
>
> lme4_0.999375-1  full data
> mfull   =  lmer(mod1, data=sleep1,  family=binomial )
>    Data: sleep1
>    AIC   BIC logLik deviance
>  238.5 244.9 -117.2    234.5
> Random effects:
>  Groups  Name        Variance Std.Dev.
>  Subject (Intercept) 1.0231   1.0115
> Number of obs: 180, groups: Subject, 18
>
> Fixed effects:
>             Estimate Std. Error z value Pr(>|z|)
> (Intercept)  -0.3213     0.2887  -1.113    0.266
> #====================================================================
> # this next  run is not consistent with the other 3
>
> lme4_0.999375-1  condensed  data
>
> mcond   =  lmer(mod1, data=sleep1c, weights= n1,family=binomial )
>    Data: sleep1c
>    AIC   BIC logLik deviance
>  9.622 11.40 -2.811    5.622
> Random effects:
>  Groups  Name        Variance Std.Dev.
>  Subject (Intercept)  0        0
> Number of obs: 18, groups: Subject, 18
>
> Fixed effects:
>             Estimate Std. Error z value Pr(>|z|)
> (Intercept)  -0.2457     0.4750 -0.5173    0.605
> #====================================================================
> lme4_0.99875-9 full
>
> > mfull   =  lmer(mod1, data=sleep1,  family=binomial )
>    AIC   BIC logLik deviance
>  238.5 244.9 -117.2    234.5
> Random effects:
>  Groups  Name        Variance Std.Dev.
>  Subject (Intercept) 1.0420   1.0208
> number of obs: 180, groups: Subject, 18
>
> Estimated scale (compare to  1 )  0.9369863
>
> Fixed effects:
>             Estimate Std. Error z value Pr(>|z|)
> (Intercept)  -0.3309     0.2906  -1.139    0.255
>
>
>
> #====================================================================
> lme4_0.99875-9 condensed
>  mcond   =  lmer(mod1, data=sleep1c, weights= n1,family=binomial )
>
>    AIC   BIC logLik deviance
>  47.88 49.66 -21.94    43.88
> Random effects:
>  Groups  Name        Variance Std.Dev.
>  Subject (Intercept) 1.0241   1.012
> number of obs: 18, groups: Subject, 18
>
> Estimated scale (compare to  1 )  1.012022
>
> Fixed effects:
>             Estimate Std. Error z value Pr(>|z|)
> (Intercept)  -0.3183     0.2888  -1.102    0.270
>
>
>
> #====================================================================
> > sessionInfo()
> R version 2.6.1 (2007-11-26)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=English_United States.1252;
> LC_CTYPE=English_United States.1252;
> LC_MONETARY=English_United States.1252;
> LC_NUMERIC=C;LC_TIME=English_United States.1252
>
> attached base packages:
> [1] stats     graphics  grDevices utils   datasets  methods   base
>
> other attached packages:
> [1] doBy_2.1    lme4_0.999375-1   Matrix_0.999375-4 lattice_0.17-4
>
> loaded via a namespace (and not attached):
> [1] cluster_1.11.9 grid_2.6.1     Hmisc_3.4-3
>
>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Wed Jan 23 21:38:54 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 23 Jan 2008 14:38:54 -0600
Subject: [R-sig-ME] Binomial problem for lme4_0.999375-1: 0,1 vs xbar n
In-Reply-To: <40e66e0b0801231230p63616bi65f84d7df46ed481@mail.gmail.com>
References: <40e66e0b0801220651ic7c6a33h62f78bed44061c0c@mail.gmail.com>
	<40e66e0b0801221127ybe495dch3c2fd99655d191ab@mail.gmail.com>
	<CB6AFE613082DD4FB037471AE7A1EDAF52A9DB@LTA3VS013.ees.hhs.gov>
	<40e66e0b0801231230p63616bi65f84d7df46ed481@mail.gmail.com>
Message-ID: <40e66e0b0801231238n3891c001wbc8a28d589d61d3@mail.gmail.com>

It appears that the problem is more with the handling of the weights
argument than with the binomial per se.  Look at the examples in

?cbpp

The first example runs properly but the second, which is inside a
\dontrun block, doesn't.

On Jan 23, 2008 2:30 PM, Douglas Bates <bates at stat.wisc.edu> wrote:
> Thanks for the report, Van.  I have entered it in the bug tracker list.
>
> I had thought that I had checked such results for consistency but
> apparently I didn't.
>
>
> On Jan 23, 2008 2:14 PM, Parsons, Van L. (CDC/CCHIS/NCHS) <vlp1 at cdc.gov> wrote:
> > Hello,
> >   The lme4_0.999375-1 lmer run on a  full response (0,1) dataset
> >   is not consistent with the run on an equivalent, but condensed,
> >   (pbar,n) dataset for the binomial.
> >
> >   For lme4_0.99875-9, the outputs appear to be order-of-magnitude
> >   consistent, so the alpha version seems to have the problem.
> >   ( I am focusing on the random and fixed effects output )
> >
> >   Here are some sample code and outputs generated from sleepstudy
> >   in lme4 using 2 datasets and both new and old lme4.
> >
> >   Thanks,
> >   Van
> >
> > #=======================================================
> > # R code
> > library(lme4)
> > library(doBy)
> > sessionInfo()
> >
> >
> >  sleep1 = sleepstudy   # from lme4 pkg
> >
> >      # add binary variable
> >  sleep1$p1 = ifelse(sleep1$Reaction < mean(sleep1$Reaction),0,1)
> >
> >    # condense binary to mean and count within Subject
> >    # use doBy package
> > sleep1c= summaryBy(p1~Subject, data = sleep1,FUN = c(mean,length) )
> >    names(sleep1c)[ c(2,3)] = c("p1", "n1")
> >
> >
> >
> >  #--------------------------------------------
> >   # simple model
> >
> > mod1  =p1~ 1 + (1| Subject)
> >
> > # lmer run   on full   data
> > mfull   =  lmer(mod1, data=sleep1,  family=binomial )
> > summary(mfull)
> >
> > # lmer run   on   condensed data
> > mcond   =  lmer(mod1, data=sleep1c, weights= n1,family=binomial )
> > summary(mcond)
> >
> > #--------------------------------------------
> > #====================================================================
> >
> >
> >
> > ## 4  partial outputs
> >
> > #====================================================================
> >
> > lme4_0.999375-1  full data
> > mfull   =  lmer(mod1, data=sleep1,  family=binomial )
> >    Data: sleep1
> >    AIC   BIC logLik deviance
> >  238.5 244.9 -117.2    234.5
> > Random effects:
> >  Groups  Name        Variance Std.Dev.
> >  Subject (Intercept) 1.0231   1.0115
> > Number of obs: 180, groups: Subject, 18
> >
> > Fixed effects:
> >             Estimate Std. Error z value Pr(>|z|)
> > (Intercept)  -0.3213     0.2887  -1.113    0.266
> > #====================================================================
> > # this next  run is not consistent with the other 3
> >
> > lme4_0.999375-1  condensed  data
> >
> > mcond   =  lmer(mod1, data=sleep1c, weights= n1,family=binomial )
> >    Data: sleep1c
> >    AIC   BIC logLik deviance
> >  9.622 11.40 -2.811    5.622
> > Random effects:
> >  Groups  Name        Variance Std.Dev.
> >  Subject (Intercept)  0        0
> > Number of obs: 18, groups: Subject, 18
> >
> > Fixed effects:
> >             Estimate Std. Error z value Pr(>|z|)
> > (Intercept)  -0.2457     0.4750 -0.5173    0.605
> > #====================================================================
> > lme4_0.99875-9 full
> >
> > > mfull   =  lmer(mod1, data=sleep1,  family=binomial )
> >    AIC   BIC logLik deviance
> >  238.5 244.9 -117.2    234.5
> > Random effects:
> >  Groups  Name        Variance Std.Dev.
> >  Subject (Intercept) 1.0420   1.0208
> > number of obs: 180, groups: Subject, 18
> >
> > Estimated scale (compare to  1 )  0.9369863
> >
> > Fixed effects:
> >             Estimate Std. Error z value Pr(>|z|)
> > (Intercept)  -0.3309     0.2906  -1.139    0.255
> >
> >
> >
> > #====================================================================
> > lme4_0.99875-9 condensed
> >  mcond   =  lmer(mod1, data=sleep1c, weights= n1,family=binomial )
> >
> >    AIC   BIC logLik deviance
> >  47.88 49.66 -21.94    43.88
> > Random effects:
> >  Groups  Name        Variance Std.Dev.
> >  Subject (Intercept) 1.0241   1.012
> > number of obs: 18, groups: Subject, 18
> >
> > Estimated scale (compare to  1 )  1.012022
> >
> > Fixed effects:
> >             Estimate Std. Error z value Pr(>|z|)
> > (Intercept)  -0.3183     0.2888  -1.102    0.270
> >
> >
> >
> > #====================================================================
> > > sessionInfo()
> > R version 2.6.1 (2007-11-26)
> > i386-pc-mingw32
> >
> > locale:
> > LC_COLLATE=English_United States.1252;
> > LC_CTYPE=English_United States.1252;
> > LC_MONETARY=English_United States.1252;
> > LC_NUMERIC=C;LC_TIME=English_United States.1252
> >
> > attached base packages:
> > [1] stats     graphics  grDevices utils   datasets  methods   base
> >
> > other attached packages:
> > [1] doBy_2.1    lme4_0.999375-1   Matrix_0.999375-4 lattice_0.17-4
> >
> > loaded via a namespace (and not attached):
> > [1] cluster_1.11.9 grid_2.6.1     Hmisc_3.4-3
> >
> >
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>



From kubovy at virginia.edu  Thu Jan 24 03:59:47 2008
From: kubovy at virginia.edu (Michael Kubovy)
Date: Wed, 23 Jan 2008 21:59:47 -0500
Subject: [R-sig-ME] lme4_0.999375-1: not clear what works and what doesn't
Message-ID: <72E963FB-E980-459A-BF51-778F5E5305A7@virginia.edu>

Dear Mixed modelers,

I realize that the package is in transition; I see that some examples  
have a comment #not run, which I take to mean "won't run for  
now" (e.g., mcmcsamp() ).

But I'm confused about some functions that don't work:

What seems not to work:

1. simulate() "Error: 'simulate' is not implemented yet";

2. The examples in lme4::lmer that call nlmer() all say "Warning  
message: In mer_finalize(ans, verbose) : false convergence (8)"

3. plot(fm2 <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject),  
data = sleepstudy)))
complains: "Error in as.double(y) : cannot coerce to vector"

***********************************************************************
R version 2.6.1 (2007-11-26)
i386-apple-darwin8.10.1

locale:
C

attached base packages:
[1] datasets  grDevices graphics  stats     utils     methods   base

other attached packages:
  [1] foreign_0.8-23    car_1.2-7         arm_1.1-1          
R2WinBUGS_2.1-7   MASS_7.2-39
  [6] coda_0.13-1       lme4_0.999375-1   Matrix_0.999375-4  
lattice_0.17-4    JGR_1.5-8
[11] iplots_1.1-1      JavaGD_0.4-3      rJava_0.5-1

loaded via a namespace (and not attached):
[1] grid_2.6.1  tools_2.6.1

_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/



From vlp1 at CDC.GOV  Thu Jan 24 13:02:39 2008
From: vlp1 at CDC.GOV (Parsons, Van L. (CDC/CCHIS/NCHS))
Date: Thu, 24 Jan 2008 07:02:39 -0500
Subject: [R-sig-ME] Binomial problem for lme4_0.999375-1: 0,1 vs xbar n
In-Reply-To: <40e66e0b0801231238n3891c001wbc8a28d589d61d3@mail.gmail.com>
References: <40e66e0b0801220651ic7c6a33h62f78bed44061c0c@mail.gmail.com>
	<40e66e0b0801221127ybe495dch3c2fd99655d191ab@mail.gmail.com>
	<CB6AFE613082DD4FB037471AE7A1EDAF52A9DB@LTA3VS013.ees.hhs.gov>
	<40e66e0b0801231230p63616bi65f84d7df46ed481@mail.gmail.com>
	<40e66e0b0801231238n3891c001wbc8a28d589d61d3@mail.gmail.com>
Message-ID: <CB6AFE613082DD4FB037471AE7A1EDAF52AA10@LTA3VS013.ees.hhs.gov>


 The suggestion of Prof. Bates to try the (success, failure)
representation
in lmer for the condensed Bernoulli data gave results consistent with
the full data. 

  For the example discussed, this code worked: 

  lmer( n1*cbind( p1,(1- p1))~ 1 + (1| Subject)  ,
         data=sleep1c,  family=binomial )

 Van



From bates at stat.wisc.edu  Thu Jan 24 15:15:21 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 24 Jan 2008 08:15:21 -0600
Subject: [R-sig-ME] lme4_0.999375-1: not clear what works and what
	doesn't
In-Reply-To: <72E963FB-E980-459A-BF51-778F5E5305A7@virginia.edu>
References: <72E963FB-E980-459A-BF51-778F5E5305A7@virginia.edu>
Message-ID: <40e66e0b0801240615y4038f06dl55a261dd21e467f3@mail.gmail.com>

On Jan 23, 2008 8:59 PM, Michael Kubovy <kubovy at virginia.edu> wrote:
> Dear Mixed modelers,

> I realize that the package is in transition; I see that some examples
> have a comment #not run, which I take to mean "won't run for
> now" (e.g., mcmcsamp() ).

Yes.  I still need to work out some details on mcmcsamp.  They may be
easy to do or they may not - I can't tell yet.

> But I'm confused about some functions that don't work:

> What seems not to work:
>
> 1. simulate() "Error: 'simulate' is not implemented yet";

Indeed.  The innards of the simulate function are closely tied to the
representation of the model.  This has stabilized, relative to recent
major upheavals, but it is not yet completely stable.  There isn't
much purpose in devoting a lot of energy to functions like simulate()
and, as shown below, plot() until the representation and the
optimization are set.

> 2. The examples in lme4::lmer that call nlmer() all say "Warning
> message: In mer_finalize(ans, verbose) : false convergence (8)"

That, and the handling of the weights argument, are the main hangups
at present.  Updating weights for the residuals is slightly different
from updating the weighted cross products of model matrices and that
part of the code apparently still has some "infelicities".  I suspect
the problem is in the logic of updating the model matrix weighted
crossproducts.

> 3. plot(fm2 <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject),
> data = sleepstudy)))
> complains: "Error in as.double(y) : cannot coerce to vector"

To summarize.  There are known problems in

 nlmer
 mcmcsamp
 use of weights argument for LMMs and GLMMs

Collateral damage from changing the class representation occurs in
simulate, plot and possibly other extractors.



From kevin.thorpe at utoronto.ca  Fri Jan 25 15:07:33 2008
From: kevin.thorpe at utoronto.ca (Kevin E. Thorpe)
Date: Fri, 25 Jan 2008 09:07:33 -0500
Subject: [R-sig-ME] Interpreting mcmcsamp results
Message-ID: <4799ED25.4050404@utoronto.ca>

When I run summary (from the coda package) on the result of mcmcsamp I
get, among other things, some quantiles for the model parameters fit in
my lmer() call.

Is it correct to interpret say the 2.5% to 97.5% as a 95% credible interval?

Thanks,

Kevin

-- 
Kevin E. Thorpe
Biostatistician/Trialist, Knowledge Translation Program
Assistant Professor, Department of Public Health Sciences
Faculty of Medicine, University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.864.5776  Fax: 416.864.6057



From bolker at zoo.ufl.edu  Fri Jan 25 19:28:45 2008
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Fri, 25 Jan 2008 13:28:45 -0500
Subject: [R-sig-ME] Interpreting mcmcsamp results
Message-ID: <479A2A5D.50404@zoo.ufl.edu>

hits=-2.6 tests=BAYES_00,SPF_HELO_PASS
X-USF-Spam-Flag: NO


Kevin E. Thorpe wrote:
> When I run summary (from the coda package) on the result of mcmcsamp I
> get, among other things, some quantiles for the model parameters fit in
> my lmer() call.
> 
> Is it correct to interpret say the 2.5% to 97.5% as a 95% credible interval?
> 
> Thanks,
> 
> Kevin
> 


    Since I haven't seen any other answers I will take a crack at
this.

    coda:::summary.mcmc (which is the function that gets called when
you ask for a summary of an mcmc) gives quantiles, not a credible
interval.  If you want credible intervals, try HPDinterval from
the coda package (which will give 95% credible intervals by default).
For symmetric distributions the credible interval and the quantile
should be the same.

    Ben Bolker



From henrik.parn at bio.ntnu.no  Tue Jan 29 15:26:26 2008
From: henrik.parn at bio.ntnu.no (Henrik Parn)
Date: Tue, 29 Jan 2008 15:26:26 +0100
Subject: [R-sig-ME] random factor of type integer
Message-ID: <479F3792.7060905@bio.ntnu.no>

Dear all,

I have a dataset where the variables that I later will use as random 
grouping factors in a lmer-model are integers. Do I first need to 
convert them to factors?

I just tried if the example on the help page for lmer (where the 
grouping factor 'Subject' actually is a factor)...

str(sleepstudy$Subject)
fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
summary(fm1)

...differed from:

Subject2 <- as.integer(as.character(sleepstudy$Subject))
fm2 <- lmer(Reaction ~ Days + (Days|Subject2), sleepstudy)
summary(fm2)

...but it did not.

Is this a general case or could the use of a random factor of integer 
type may cause problems in lmer?

Thanks in advance!

Henrik



From bates at stat.wisc.edu  Tue Jan 29 20:47:00 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 29 Jan 2008 13:47:00 -0600
Subject: [R-sig-ME] random factor of type integer
In-Reply-To: <479F3792.7060905@bio.ntnu.no>
References: <479F3792.7060905@bio.ntnu.no>
Message-ID: <40e66e0b0801291147td1d0ee1g402a83f3ff5c6e21@mail.gmail.com>

On Jan 29, 2008 8:26 AM, Henrik Parn <henrik.parn at bio.ntnu.no> wrote:
> Dear all,

> I have a dataset where the variables that I later will use as random
> grouping factors in a lmer-model are integers. Do I first need to
> convert them to factors?

> I just tried if the example on the help page for lmer (where the
> grouping factor 'Subject' actually is a factor)...

> str(sleepstudy$Subject)
> fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
> summary(fm1)

> ...differed from:

> Subject2 <- as.integer(as.character(sleepstudy$Subject))
> fm2 <- lmer(Reaction ~ Days + (Days|Subject2), sleepstudy)
> summary(fm2)

> ...but it did not.

The short answer is that you do not need to have grouping factors
represented as factors for lmer, but it is advisable.

Internally the variables used as grouping factors for random effects
are converted to factors before being incorporated into the flist
slot.

> Is this a general case or could the use of a random factor of integer
> type may cause problems in lmer?



From David_Hinds at perlegen.com  Wed Jan 30 21:02:10 2008
From: David_Hinds at perlegen.com (David Hinds)
Date: Wed, 30 Jan 2008 12:02:10 -0800
Subject: [R-sig-ME] A glmm prediction problem
Message-ID: <ABFA245EB97DD74EA88C78E71937E78104D4A449@ilpostino.perlegen.com>

I'm using lmer with a generalized linear (binomial) mixed model with
nested
random effects, like:

	y ~ (1 | a / b / c)

There are no fixed effects.  After fitting the model, I would like to
make 
predictions for a new set of y values: specifically, I want to predict
BLUPs
for the random effects, and I would like to compute likelihoods for sets
of
y values under the fitted model.

I don't see a completely straightforward way of doing this since it
isn't
the usual sort of prediction problem.  Is this even a sensible thing to
do?

-- David Hinds



From kw.statr at gmail.com  Wed Jan 30 21:17:57 2008
From: kw.statr at gmail.com (Kevin Wright)
Date: Wed, 30 Jan 2008 14:17:57 -0600
Subject: [R-sig-ME] Factor analysis with lme?
Message-ID: <c968588d0801301217n4481254fxc16b2e81fded52ab@mail.gmail.com>

I'm looking for an example of how (if?) lme can be used with factor
analysis.  By that, I mean modeling the residual variance/covariance
matrix as

Sigma = (Gamma)(Gamma)' + D

where Gamma is a vector (or matrix) corresponding to latent variables
and D is a diagonal matrix.

I have looked around but have not found any examples.  Pointers,
comments, and code are all welcome.

K Wright



From bates at stat.wisc.edu  Wed Jan 30 21:36:18 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 30 Jan 2008 14:36:18 -0600
Subject: [R-sig-ME] A glmm prediction problem
In-Reply-To: <ABFA245EB97DD74EA88C78E71937E78104D4A449@ilpostino.perlegen.com>
References: <ABFA245EB97DD74EA88C78E71937E78104D4A449@ilpostino.perlegen.com>
Message-ID: <40e66e0b0801301236u4854fd64k447fd8bae2189cf8@mail.gmail.com>

On Jan 30, 2008 2:02 PM, David Hinds <David_Hinds at perlegen.com> wrote:
> I'm using lmer with a generalized linear (binomial) mixed model with
> nested random effects, like:

>         y ~ (1 | a / b / c)

> There are no fixed effects.

I would check that.  The model will include a constant term in the
fixed effects.

I don't think it is possible in the current formulation to fit a model
without any fixed effects.  There is nothing in the theory or
computational methods that would preclude that but I be hard pressed
to think of a situation where it would be sensible to do so.  The
distribution of the random effects assumes a mean of zero for all the
random effects terms.  If you don't have any fixed effects terms then
you are assuming that the population mean probability of success is
exactly 0.5, which seems like a pretty strong assumption.

> After fitting the model, I would like to make
> predictions for a new set of y values: specifically, I want to predict
> BLUPs
> for the random effects, and I would like to compute likelihoods for sets
> of
> y values under the fitted model.

Are the new y values to be at some set of observed levels for the a, b
and c factors?  For example, suppose that factor a is the field,
factor b is the plant selected from the field, factor c is the seed
pod selected from the plant and the observational unit is the seed
within the seed pod.  Do you want to predict for another seed from
that particular seed pod or for a seed in another, as yet unobserved,
seed pod from that plant or for a seed from a seed pod from another,
as yet unobserved, plant from one of the fields you observed or ...

Essentially what will happen is that you will need to use the marginal
variance for units that are as yet unobserved and the conditional
variance for the units that have been observed when determining the
variance of the linear predictor.  Because you have a generalized
linear mixed model you need to convert the variance of the linear
predictor to an associated interval on the mean response through the
inverse link function.  Somewhere along the line what would be the
"residual variance" in a model for a Gaussian family would need to be
incorporated for the binomial family.  I'm not sure exactly how that
would be done.  Suffice it to say that formulating the prediction
interval would not be trivial.


> I don't see a completely straightforward way of doing this since it
> isn't
> the usual sort of prediction problem.  Is this even a sensible thing to
> do?
>
> -- David Hinds
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Thu Jan 31 00:59:09 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 30 Jan 2008 17:59:09 -0600
Subject: [R-sig-ME] New alpha release of lme4 available on R-forge
Message-ID: <40e66e0b0801301559l364c04f7j12ced4719dfcc618@mail.gmail.com>

There is a new alpha release (0.999375-2) of the lme4 package on
R-forge.R-project.org.  Major changes in this release are:

- The effect of the link function and the variance function in the
family argument have been separated.  Families beyond the binomial and
poisson are now allowed although I would advise caution.  I need to do
some more research on whether a common scale parameter changes the
calculation of the deviance.  (Feature request #81).

- The flist slot is now a data frame.  (Part of Feature request #96).

- the weights argument is now handled properly.  In particular, its
use no longer causes a segfault (Bug #92)

- some slots have been renamed and code simplified.



From David_Hinds at perlegen.com  Thu Jan 31 00:56:52 2008
From: David_Hinds at perlegen.com (David Hinds)
Date: Wed, 30 Jan 2008 15:56:52 -0800
Subject: [R-sig-ME] A glmm prediction problem
In-Reply-To: <40e66e0b0801301236u4854fd64k447fd8bae2189cf8@mail.gmail.com>
References: <ABFA245EB97DD74EA88C78E71937E78104D4A449@ilpostino.perlegen.com>
	<40e66e0b0801301236u4854fd64k447fd8bae2189cf8@mail.gmail.com>
Message-ID: <ABFA245EB97DD74EA88C78E71937E78104D4A45B@ilpostino.perlegen.com>

A couple clarifications:

- It is a little bit of a digression, but for the data I'm looking at,
there really are no fixed effects and the population mean probability of
success is just 0.5: any deviation from that belongs in a random effect.
I can't fit that model directly, so you're right, I do end up with an
estimate for a fixed intercept term, which is always very close to zero.
For many purposes it can be ignored, because it accounts for so little
variance compared to the fitted random effects.

(the data consists of experimental estimates of the proportion of each
allele at genetic markers where an individual is known to have an "AB"
genotype, so we know with certainty that the underlying ratio is 1:1.
I'm using random effects to model sources of variance in the experiment
that cause overdispersion in the observed ratios.)  

- Using your terminology, I want to make predictions for y values for a
previously unseen field (new value of 'a').  I will essentially always
have data for two seed pods (two values of 'c'), and may have several
values of 'b' available, but often just one.

I was thinking to get a likelihood for a new set of y/a/b/c values by
numerically integrating p(random effects)*p(y|random effects) over the
fitted multivariate normal distribution of random effects.  I could
estimate marginal distributions for the individual random effects at the
same time.  This seems manageable for the important case of one new
field, one plant, two pods (integrating over 4 random effects) but gets
out of hand when there are multiple plants.

Alternatively it seems that 'lmer' should be able to do the work for me
if I could prespecify a model.  I found a hint on how to lock down the
random effects from an archived posting on this list (via the start= and
control= parameters) but the intercept would be a problem.

-- Dave


-----Original Message-----
From: dmbates at gmail.com [mailto:dmbates at gmail.com] On Behalf Of Douglas
Bates
Sent: Wednesday, January 30, 2008 12:36 PM
To: David Hinds
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] A glmm prediction problem

On Jan 30, 2008 2:02 PM, David Hinds <David_Hinds at perlegen.com> wrote:
> I'm using lmer with a generalized linear (binomial) mixed model with
> nested random effects, like:

>         y ~ (1 | a / b / c)

> There are no fixed effects.

I would check that.  The model will include a constant term in the
fixed effects.

I don't think it is possible in the current formulation to fit a model
without any fixed effects.  There is nothing in the theory or
computational methods that would preclude that but I be hard pressed
to think of a situation where it would be sensible to do so.  The
distribution of the random effects assumes a mean of zero for all the
random effects terms.  If you don't have any fixed effects terms then
you are assuming that the population mean probability of success is
exactly 0.5, which seems like a pretty strong assumption.

> After fitting the model, I would like to make
> predictions for a new set of y values: specifically, I want to predict
> BLUPs
> for the random effects, and I would like to compute likelihoods for
sets
> of
> y values under the fitted model.

Are the new y values to be at some set of observed levels for the a, b
and c factors?  For example, suppose that factor a is the field,
factor b is the plant selected from the field, factor c is the seed
pod selected from the plant and the observational unit is the seed
within the seed pod.  Do you want to predict for another seed from
that particular seed pod or for a seed in another, as yet unobserved,
seed pod from that plant or for a seed from a seed pod from another,
as yet unobserved, plant from one of the fields you observed or ...

Essentially what will happen is that you will need to use the marginal
variance for units that are as yet unobserved and the conditional
variance for the units that have been observed when determining the
variance of the linear predictor.  Because you have a generalized
linear mixed model you need to convert the variance of the linear
predictor to an associated interval on the mean response through the
inverse link function.  Somewhere along the line what would be the
"residual variance" in a model for a Gaussian family would need to be
incorporated for the binomial family.  I'm not sure exactly how that
would be done.  Suffice it to say that formulating the prediction
interval would not be trivial.


> I don't see a completely straightforward way of doing this since it
> isn't
> the usual sort of prediction problem.  Is this even a sensible thing
to
> do?
>
> -- David Hinds
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Thu Jan 31 20:46:24 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 31 Jan 2008 13:46:24 -0600
Subject: [R-sig-ME] New alpha release of lme4 available on R-forge
In-Reply-To: <40e66e0b0801311007n2b58d25cq8a602c4bebec8da@mail.gmail.com>
References: <40e66e0b0801301559l364c04f7j12ced4719dfcc618@mail.gmail.com>
	<Pine.LNX.4.64.0801311606360.32168@orpheus.qimr.edu.au>
	<40e66e0b0801311007n2b58d25cq8a602c4bebec8da@mail.gmail.com>
Message-ID: <40e66e0b0801311146l783f1e42lc60c1f93925fce1b@mail.gmail.com>

On Jan 31, 2008 12:07 PM, Douglas Bates <bates at stat.wisc.edu> wrote:
> Thanks for the report David.

> As you have seen there are two problems here.  One is that the offset
> slot is being assigned an integer vector but I try to use it as a
> double vector.  (One of the latent issues in the S4 class system is
> whether "numeric" is an atomic class and whether "integer" inherits
> from "numeric".)  I will simply add a coercion of the offset to a
> double before installing it in the slot.

Now fixed and committed.

> Even with that corrected there seems to be a problem with the
> evaluation of the deviance.  I'll take a longer look at that.

The problem is with the values of the offset.  Perhaps I am
misunderstanding the use of offset but I add the offset, when
specified, to the value of the linear predictor.  Because the values
of the offset are sometimes in the thousands, this makes for very
large values of the linear predictor (the 'eta' slot) and when the
inverse link function is applied some of the elements of mu become
infinite.

If I read the section in McCullagh and Nelder correctly the offset for
this example should be log(service), not service.

I enclose a sample run of how I think the model should be fit, both in
glm and in lmer/glmer.

I am grateful for the example because it exposed a bug in the call to
glm.fit within the glmer function.  That too is now fixed and
committed.

> On Jan 31, 2008 12:35 AM, David Duffy <David.Duffy at qimr.edu.au> wrote:
> > On Wed, 30 Jan 2008, Douglas Bates wrote:
> >
> > > There is a new alpha release (0.999375-2) of the lme4 package on
> > > R-forge.R-project.org.  Major changes in this release are:
> > >
> >
> > I haven't had much luck with the ships example
> >
> > R version 2.6.1 Patched (2007-12-18 r43730)
> > i686-pc-linux-gnu
> >
> > attached base packages:
> > [1] stats     graphics  grDevices utils     datasets  methods   base
> >
> > other attached packages:
> > [1] MASS_7.2-38       lme4_0.999375-2   Matrix_0.999375-4 lattice_0.17-2
> >
> > library(lme4)
> > library(MASS)
> > data(ships)
> > ships$period <- as.factor(ships$period)
> > ships$year <- as.factor(ships$year)
> > lmer(formula = incidents ~ type + (1 | period/year) + (1 | year),
> >       data = ships[ships$service>0,], family = poisson, offset = service,
> >       verbose=TRUE)
> >
> > Error in mer_finalize(ans, verbose) :
> >    REAL() can only be applied to a 'numeric', not a 'integer'
> >
> > > debug(glmer)
> > ...
> > debug: mer_finalize(ans, verbose)
> > Browse[1]> ans
> > Error in chol2inv(object at RXy, size = object at dims["p"]) :
> >    element (1, 1) is zero, so the inverse cannot be computed
> >
> > Browse[1]> ans at fixef
> > (Intercept)       typeB       typeC       typeD       typeE
> >    1.7917595   1.7957199  -1.2527630  -0.9044563  -0.1177830
> >
> >
> > Which used to be something like:
> >
> > # Fixed effects:
> > #              Estimate Std. Error  z value  Pr(>|z|)
> > # (Intercept) -5.573280   0.217482 -25.6264 < 2.2e-16 ***
> > # typeB       -0.605271   0.176118  -3.4367 0.0005888 ***
> > # typeC       -0.716560   0.328634  -2.1804 0.0292262 *
> > # typeD       -0.061478   0.289935  -0.2120 0.8320749
> > # typeE        0.309430   0.235577   1.3135 0.1890160
> > #
> > # Solution for Fixed Effects (SAS GLMMIX)
> > #   Effect       type    Estimate       Error      DF    t Value    Pr > |t|
> > #   Intercept             -5.6799      0.3286       1     -17.28      0.0368
> > #   type         B        -0.5798      0.2277      23      -2.55      0.0180
> > #   type         C        -0.6984      0.4248      23      -1.64      0.1138
> > #   type         D       -0.08703      0.3746      23      -0.23      0.8183
> > #   type         E         0.3301      0.3046      23       1.08      0.2897
> >
> > And:
> >
> > Browse[1]> ans at deviance
> >     ML  REML  ldL2 ldRX2 pwrss  disc  usqr  wrss
> >        NA    NA    NA    NA    NA    NA    NA    NA
> >
> > Hopefully, I haven't made any obvious errors.
> >
> > Cheers, David Duffy.
> >
> > --
> > | David Duffy (MBBS PhD)                                         ,-_|\
> > | email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
> > | Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
> > | 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v
> >
>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: ships_Rout.txt
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080131/61859a2c/attachment.txt>

From Alan_Bergland at brown.edu  Thu Jan 31 21:28:15 2008
From: Alan_Bergland at brown.edu (Alan Bergland)
Date: Thu, 31 Jan 2008 15:28:15 -0500
Subject: [R-sig-ME] Discrepancy between mcmcsamp value and variance
	component esitamate
Message-ID: <305A7D33-9D78-417E-BBA1-EA8AE9E18CFD@brown.edu>

Hi all,

	I am getting a funny result using the mcmcsamp function, perhaps it  
is a bug or perhaps there are convergence errors.  I get this result  
using the latest stable version of lme4 on CRAN and the alpha version  
on R-forge, on both a Mac and linux machines.  I apologize for not  
being able to construct a self contained example, so perhaps my  
output will suffice.

My data is structured in the following way:
 > str(ov)
'data.frame':   7958 obs. of  7 variables:
  $ ovn  : num  27 16 29 22 13 18 17 12 16 18 ...
  $ geno : Factor w/ 214 levels "2","3","10","12",..: 52 99 25 25 41  
32 24 32 99 207 ...
  $ food : Factor w/ 4 levels "0.2","0.4","0.6",..: 3 3 3 3 3 3 3 3 3  
3 ...
  $ block: Factor w/ 3 levels "1","2","3": 1 1 1 1 1 1 1 1 1 1 ...
  $ gf   : Factor w/ 856 levels "2:0.8","2:0.6",..: 206 394 98 98 162  
126 94 126 394 826 ...
  $ gb   : Factor w/ 642 levels "2:1","2:2","2:3",..: 154 295 73 73  
121 94 70 94 295 619 ...
  $ bf   : Factor w/ 12 levels "1:0.8","1:0.6",..: 2 2 2 2 2 2 2 2 2  
2 ...

where gf is the interaction of 'geno' and 'food'; gb is the  
interaction of 'geno' and 'block'; and 'bf' is the interaction of  
'block' and 'food'.  There is no geno:food:block interaction.


I fit the following model.  I should also note that lmer2 produces  
the same results.

 > full.ovn<-lmer(ovn~1+(1|block)+(1|gb)+(1|gf)+(1|bf)+(1|geno)+(1| 
food), ov)
 > summary(full.ovn)
Linear mixed-effects model fit by REML
Formula: ovn ~ 1 + (1 | block) + (1 | gb) + (1 | gf) + (1 | bf) + (1  
|      geno) + (1 | food)
    Data: ov
    AIC   BIC logLik MLdeviance REMLdeviance
  49097 49146 -24542      49087        49083
Random effects:
  Groups   Name        Variance Std.Dev.
  gf       (Intercept)  6.43894 2.53751
  gb       (Intercept)  6.00979 2.45149
  geno     (Intercept)  3.37417 1.83689
  bf       (Intercept)  0.84951 0.92169
  food     (Intercept) 19.30673 4.39394
  block    (Intercept)  0.35833 0.59861
  Residual             22.38789 4.73158
number of obs: 7958, groups: gf, 787; gb, 527; geno, 214; bf, 12;  
food, 4; block, 3

Fixed effects:
             Estimate Std. Error t value
(Intercept)   21.009      2.249   9.342


And, then run mcmcsamp:
 > full.mcmc<-mcmcsamp(full.ovn, n=10000)
 > summary(full.mcmc)

Iterations = 1:10000
Thinning interval = 1
Number of chains = 1
Sample size per chain = 10000

1. Empirical mean and standard deviation for each variable,
    plus standard error of the mean:

                    Mean       SD  Naive SE Time-series SE
(Intercept)     21.0016  3.82131 0.0382131      0.0350391
log(sigma^2)     3.1106  0.01734 0.0001734      0.0003096
log(gf.(In))     1.7228  0.08343 0.0008343      0.0019563
log(gb.(In))     1.8292  0.09193 0.0009193      0.0019939
log(geno.(In))   1.6577  0.13624 0.0013624      0.0015901
log(bf.(In))     0.5115  0.62757 0.0062757      0.0119208
log(food.(In))   3.2965  1.01289 0.0101289      0.0154601
log(blck.(In)) -50.0119 43.12205 0.4312205      5.3344899

2. Quantiles for each variable:

                     2.5%       25%      50%      75%   97.5%
(Intercept)      13.7807  19.34548  21.0349  22.6964 27.9320
log(sigma^2)      3.0763   3.09890   3.1105   3.1222  3.1446
log(gf.(In))      1.5619   1.66684   1.7226   1.7782  1.8891
log(gb.(In))      1.6491   1.76648   1.8300   1.8926  2.0060
log(geno.(In))    1.3956   1.56445   1.6588   1.7480  1.9267
log(bf.(In))     -0.6863   0.08243   0.5012   0.9256  1.7489
log(food.(In))    1.6572   2.58508   3.1775   3.8774  5.6251
log(blck.(In)) -152.0792 -61.58789 -38.9542 -19.1747 -0.4419

Warning message:
In glm.fit(x = X, y = Y, weights = weights, start = start, etastart =  
etastart,  :
   algorithm did not converge

and then, the HPD interval:

 > ovn.hpd
                      lower     upper
(Intercept)      13.744164 27.856893
log(sigma^2)      3.075533  3.143706
log(gf.(In))      1.564024  1.890378
log(gb.(In))      1.650405  2.006905
log(geno.(In))    1.399007  1.929362
log(bf.(In))     -0.721077  1.706577
log(food.(In))    1.518141  5.352884
log(blck.(In)) -142.946749  1.565227
attr(,"Probability")
[1] 0.95
 >

As you can see, the HPD interval for the 'geno' variance component do  
no enclose the estimate for the variance component itself.  Further,  
the mean value of 'geno' from the MCMC run does not equal the  
variance component for 'geno'.  For other variance components there  
is no problem.

Is the warning message at the end of the summary(full.mcmc) a clue  
that there is just poor convergence?  Or, is this a bug?

Thanks for the help,
Alan



From bates at stat.wisc.edu  Fri Feb  1 00:59:12 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 31 Jan 2008 17:59:12 -0600
Subject: [R-sig-ME] Structure of an mer object and steps in the computation
	within lmer
Message-ID: <40e66e0b0801311559y41e58b86k5922c6c5fb143522@mail.gmail.com>

This is in response to David Duffy's query about creating specialized
Z matrices for lmer.  I am responding in a separate message to
generate a new thread, rather than having the discussion hidden in an
misleading topic.

All of lmer, glmer and nlmer (which now works, by the way) now create
the same class of object, class "mer".  The declaration of the class,
in the file lme4/R/AllClass.R, has some comments regarding each of the
slots.  They are arranged so that the original data, model matrices,
etc. come first, then the slots that vary during the optimization.

X, the model matrix for the fixed effects, is a (dense) matrix in the
usual orientation (rows corresponding to observations).  Zt is the
transpose of the model matrix for the random effects stored as a
sparse matrix of class "dgCMatrix" defined in the Matrix package. The
y slot is, unsurprisingly, the response.  The prior weights and the
offset, if used, are stored as numeric (double precision, not integer)
slots of length n.  If they are not used they must still be numeric
vectors but they are of length 0.  This is for the S4 class
definitions.  In S3 these would typically be NULL if not used but in
S4 they must always be the same class so numeric(0) means "not used".

Random effects are associated with different terms in the model
formula.  The terms not only define sections of the model matrix Z but
also determine the structure of the variance-covariance of the random
effects.  The Gp slot is the "groups pointer" that indicates which
elements of the random effects vector are associated with each random
effects term.  To get the association of levels and covariates to
random effects within the terms, think of the result of
ranef(fittedModel).  It is a list of matrices, one for each term.  The
number of rows of the ith matrix is the number of levels of the
grouping factor for the ith term.

Try fitting models with different numbers of terms and different
expressions on the left hand side of the | to see how this all plays
out.  For a fitted model fm the expression

all.equal(unname(unlist(ranef(fm))), fm at ranef)

should be TRUE.  The form of Gp is an integer vector whose first
element is 0 and successive elements are the cumulative sum of the
length of the contents of the ranef matrix for each term.  If that
explanation is too obscure, try a few examples to see what I mean.

The elements of the dims slot and deviance slots are documented at the
beginning of the lmer.c page at lme4.r-forge.r-project.org/doxygen/

The ST slot is a list (one element per term) of square matrices.  The
ith element is of size ncol(ranef(fm)[[i]]).  They contain the
elements of the relative variance-covariance matrices of the random
effects coded in a particular way.  You don't have to know the details
if you just .Call the C function "mer_ST_initialize" to create initial
values.

The slot A is another sparse matrix and, except for nonlinear models,
it has the same strucuture as Zt so you can initialize it to that
value.  There is a C function "mer_create_L" to create the L slot from
the Zt matrix.  The L slot is the sparse Cholesky factor of a matrix
derived from Z'Z.  It is a CHMfactor object from the Matrix package.

There is another sparse matrix C that is used for generalized linear
mixed models and for nonlinear mixed models but in slightly different
forms.  For a GLMM it has the same structure as A so only the
numerical values are stored in the Cx slot.  For a NLMM the entire
sparse matrix is stored in the Cm slot.

The simple form of the optimization is to create an mer object, run
the validity checks on it

validObject(ans)

then call mer_finalize on the object with an integer value of verbose
(0 for no output during iterations, 1 for a 1-line summary every
iteration, -1 for even more output from GLMMs and NLMMs).

The details of exactly what happens inside mer_finalize are different
for linear mixed models than for other types of mixed models.  For an
LMM the fixed effects and the common scale parameter (i.e. sigma) are
profiled out of the optimization so only the parameters in ST are
varied.  The ST parameters are updated with "mer_ST_setPars" and the
profiled deviance for both ML and REML estimation is evaluated by
"mer_update_RX".

For GLMMs or NLMMs you must set both the ST parameters and the fixef
slot, then call "mer_update_u" to get the conditional modes of the
transformed random effects u.  For a GLMM the deviance is evaluated
from the deviance residuals in "mer_update_dev".

I hope this helps.  I'm sure there will be questions, assuming that
anyone is crazy enough and brave enough to want to mess with this
stuff.



From dafshartous at med.miami.edu  Fri Feb  1 18:15:51 2008
From: dafshartous at med.miami.edu (David Afshartous)
Date: Fri, 01 Feb 2008 12:15:51 -0500
Subject: [R-sig-ME] getVarCov for lmer()
In-Reply-To: <BEB0506E-D3F1-4A44-8C39-7A18342B97ED@kagi.com>
Message-ID: <C3C8BDF7.42A3%dafshartous@med.miami.edu>


All,

I've fitted some models using lme() and now I want to check some things by
fitting them in lmer() (latest version loaded from CRAN).  Under the listed
methods for lmer() I don't seem to find functionality similar to that of
getVarCov() to obtain the variance-covariance matrix of the fitted model.
Does such functionality exist?


Thanks,
David

PS - running R v2.6.0.

PPS - if it must be computed manually, how does one access information
stored in slots (e.g., Z', the transpose of the model matrix for the random
effects)?



From bates at stat.wisc.edu  Fri Feb  1 19:29:07 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 1 Feb 2008 12:29:07 -0600
Subject: [R-sig-ME] getVarCov for lmer()
In-Reply-To: <C3C8BDF7.42A3%dafshartous@med.miami.edu>
References: <BEB0506E-D3F1-4A44-8C39-7A18342B97ED@kagi.com>
	<C3C8BDF7.42A3%dafshartous@med.miami.edu>
Message-ID: <40e66e0b0802011029u4b3e3040qb4aa214e668a9003@mail.gmail.com>

On Feb 1, 2008 11:15 AM, David Afshartous <dafshartous at med.miami.edu> wrote:

> All,

> I've fitted some models using lme() and now I want to check some things by
> fitting them in lmer() (latest version loaded from CRAN).  Under the listed
> methods for lmer() I don't seem to find functionality similar to that of
> getVarCov() to obtain the variance-covariance matrix of the fitted model.
> Does such functionality exist?

I would need to go back and check what getVarCov  produces.  Can you
tell me what you want to be able to calculate?

Be aware that it is not always possible to reproduce all the
capabilities of the nlme package in lme4 because nlme does not handle
(except through egregious kludges) crossed or partially crossed random
effects specifications.  Some of the quantities that can be calculated
for models with a single level of random effects or with multiple
nested levels of random effects cannot even be defined for models with
crossed random effects.

> PS - running R v2.6.0.

> PPS - if it must be computed manually, how does one access information
> stored in slots (e.g., Z', the transpose of the model matrix for the random
> effects)?

If the fitted model is fm then Z' is available as

fm at Zt

It is stored as a compressed, column-oriented sparse matrix of class
"dgCMatrix", defined in the Matrix package.  Linear algebra operations
with such objects should magically do the right thing.  If you are
working with very large models, of course, you will find that you need
to be very careful about exactly what operations are done with Z.  In
many books on mixed models there are formulas for many calculations
based on a generalized least squares representation of the model.
Don't even consider using such formulas except for trivial examples.

I've seen far too many papers where the authors write down an n by n
matrix like V = sigma^2 I + Z Sigma Z' and then proceed to blithely
describe calculations involving the inverse of V.  I usually stop
reading at that point because I know that the author has never tried
to perform the calculation on a non-trivial example.  When n is in the
hundreds of thousands or in the millions you do not even construct n
by n matrices, let alone try to decompose them.  And anyone with
experience in numerical linear algebra knows that you never, ever,
ever expect to evaluate a formula that involves the inverse of a large
matrix.

This is all to say that you could write down an expression involving
solve() applied to some matrix of size n by n derived from Z but you
will be disappointed in the speed.



From dafshartous at med.miami.edu  Fri Feb  1 20:55:16 2008
From: dafshartous at med.miami.edu (David Afshartous)
Date: Fri, 01 Feb 2008 14:55:16 -0500
Subject: [R-sig-ME] getVarCov for lmer()
In-Reply-To: <40e66e0b0802011029u4b3e3040qb4aa214e668a9003@mail.gmail.com>
Message-ID: <C3C8E354.42AC%dafshartous@med.miami.edu>


RE what I'd like to calculate, I'd like to calculate the marginal covariance
matrix, i.e., tr(Z_i) Psi Z_I + sigma^2 I, in the notation of your book.

I've done this for a series of models estimated with lme() using
getVarCov(model, type = "marginal"), but for the last model there is an
error message and I wanted to see if this is also the case for lmer().

Here is the problem on simulated data called dat.new (formed below;
longitudinal data from a crossover design; 20 patients, 4 measurements each,
both for drug and placebo, thus n=160 observations total):

# basic random intercept model, marginal covariance matrix is compound
# symmetric as expected:

> fm <- lme( dv ~ time.num*drug,  random = ~ 1 | Patient.new,  data=dat.new )
> getVarCov(fm, type = "marginal")
Patient.new 1 
Marginal variance covariance matrix
        1       2       3       4       5       6       7       8
1 18.8570  8.5199  8.5199  8.5199  8.5199  8.5199  8.5199  8.5199
2  8.5199 18.8570  8.5199  8.5199  8.5199  8.5199  8.5199  8.5199
3  8.5199  8.5199 18.8570  8.5199  8.5199  8.5199  8.5199  8.5199
4  8.5199  8.5199  8.5199 18.8570  8.5199  8.5199  8.5199  8.5199
5  8.5199  8.5199  8.5199  8.5199 18.8570  8.5199  8.5199  8.5199
6  8.5199  8.5199  8.5199  8.5199  8.5199 18.8570  8.5199  8.5199
7  8.5199  8.5199  8.5199  8.5199  8.5199  8.5199 18.8570  8.5199
8  8.5199  8.5199  8.5199  8.5199  8.5199  8.5199  8.5199 18.8570
  Standard Deviations: 4.3424 4.3424 4.3424 4.3424 4.3424 4.3424 4.3424
4.3424 


## now I make random effect variance stratified by treatment.
## as expected, the 8x8 matrix has the upper left 4x4 block
## and the lower right 4x4 block represent the compound symmetric
## covariance matrix for the different treatment conditions (recall
## that the same patient will experience both since it's a crossover).
## whereas the upper right and lower left 4x4 blocks represent
## covariance of measurements across treatment condition, e.g.,
## the covariance of measurement i on treatment to measurement j on control.
## such a covariance exists since although we have stratified the intercept
## random effect by treatment, a covariance of these random variables is
specified by default ( check via summary(fm1) ).

> fm1 <- lme( dv ~ time.num*drug,  random = ~ 0 + Pind + Dind | Patient.new,
data=dat.new )
> getVarCov(fm1, type = "marginal")
Patient.new 1 
Marginal variance covariance matrix
        1       2       3       4      5      6      7      8
1 31.3030 29.3380 29.3380 29.3380 2.0209 2.0209 2.0209 2.0209
2 29.3380 31.3030 29.3380 29.3380 2.0209 2.0209 2.0209 2.0209
3 29.3380 29.3380 31.3030 29.3380 2.0209 2.0209 2.0209 2.0209
4 29.3380 29.3380 29.3380 31.3030 2.0209 2.0209 2.0209 2.0209
5  2.0209  2.0209  2.0209  2.0209 6.8509 4.8858 4.8858 4.8858
6  2.0209  2.0209  2.0209  2.0209 4.8858 6.8509 4.8858 4.8858
7  2.0209  2.0209  2.0209  2.0209 4.8858 4.8858 6.8509 4.8858
8  2.0209  2.0209  2.0209  2.0209 4.8858 4.8858 4.8858 6.8509
  Standard Deviations: 5.5949 5.5949 5.5949 5.5949 2.6174 2.6174 2.6174
2.6174 

## now, presumably the off-diagonal 4x4 blocks should go to zero if the
## covariance between the intercept random effects for treatment/control can
## be constrained to 0.  This can be done via:

> fm1.no.cov <- lme( dv ~ time.num*drug,  random = list(~ 0 + Pind  |
Patient.new, ~ 0 + Dind | Patient.new),  data=dat.new )

# But the following error message arises when trying to get the marginal
# covariance:

> getVarCov(fm1.no.cov, type = "marginal")
Error in getVarCov.lme(fm1.no.cov, type = "marginal") :
  Not implemented for multiple levels of nesting

Does there exist a different model statement for fm1.no.cov such that this
problem doesn't arise?  In any event, this is why I sought getVarCov in
lmer().

##################################
## code to generate simulated data above:

set.seed(500)
n.timepoints <- 4
n.subj.per.tx <- 20
sd.d <- 5;
sd.p <- 2;
sd.res <- 1.3
drug <- factor(rep(c("D", "P"), each = n.timepoints, times =
n.subj.per.tx))
drug.baseline <- rep( c(0,5), each=n.timepoints, times=n.subj.per.tx )
Patient.baseline <- rep( rnorm( n.subj.per.tx*2, sd=c(sd.d, sd.p) ),
each=n.timepoints )
time.baseline <- rep(1:n.timepoints,n.subj.per.tx*2)*as.numeric(drug=="D")
dv <- rnorm( n.subj.per.tx*n.timepoints*2,
mean=time.baseline+Patient.baseline+drug.baseline, sd=sd.res )

dat.new <- data.frame(drug, dv)
dat.new$Dind <- as.numeric(dat.new$drug == "D")
dat.new$Pind <- as.numeric(dat.new$drug == "P")
dat.new$time.num = rep(1:n.timepoints, n.subj.per.tx*2)
dat.new$Patient.new = rep(1:20, each=8)


On 2/1/08 1:29 PM, "Douglas Bates" <bates at stat.wisc.edu> wrote:

> On Feb 1, 2008 11:15 AM, David Afshartous <dafshartous at med.miami.edu> wrote:
> 
>> All,
> 
>> I've fitted some models using lme() and now I want to check some things by
>> fitting them in lmer() (latest version loaded from CRAN).  Under the listed
>> methods for lmer() I don't seem to find functionality similar to that of
>> getVarCov() to obtain the variance-covariance matrix of the fitted model.
>> Does such functionality exist?
> 
> I would need to go back and check what getVarCov  produces.  Can you
> tell me what you want to be able to calculate?
> 
> Be aware that it is not always possible to reproduce all the
> capabilities of the nlme package in lme4 because nlme does not handle
> (except through egregious kludges) crossed or partially crossed random
> effects specifications.  Some of the quantities that can be calculated
> for models with a single level of random effects or with multiple
> nested levels of random effects cannot even be defined for models with
> crossed random effects.
> 
>> PS - running R v2.6.0.
> 
>> PPS - if it must be computed manually, how does one access information
>> stored in slots (e.g., Z', the transpose of the model matrix for the random
>> effects)?
> 
> If the fitted model is fm then Z' is available as
> 
> fm at Zt
> 
> It is stored as a compressed, column-oriented sparse matrix of class
> "dgCMatrix", defined in the Matrix package.  Linear algebra operations
> with such objects should magically do the right thing.  If you are
> working with very large models, of course, you will find that you need
> to be very careful about exactly what operations are done with Z.  In
> many books on mixed models there are formulas for many calculations
> based on a generalized least squares representation of the model.
> Don't even consider using such formulas except for trivial examples.
> 
> I've seen far too many papers where the authors write down an n by n
> matrix like V = sigma^2 I + Z Sigma Z' and then proceed to blithely
> describe calculations involving the inverse of V.  I usually stop
> reading at that point because I know that the author has never tried
> to perform the calculation on a non-trivial example.  When n is in the
> hundreds of thousands or in the millions you do not even construct n
> by n matrices, let alone try to decompose them.  And anyone with
> experience in numerical linear algebra knows that you never, ever,
> ever expect to evaluate a formula that involves the inverse of a large
> matrix.
> 
> This is all to say that you could write down an expression involving
> solve() applied to some matrix of size n by n derived from Z but you
> will be disappointed in the speed.



From bates at stat.wisc.edu  Sun Feb  3 00:47:35 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 2 Feb 2008 17:47:35 -0600
Subject: [R-sig-ME] getVarCov for lmer()
In-Reply-To: <C3C8E354.42AC%dafshartous@med.miami.edu>
References: <40e66e0b0802011029u4b3e3040qb4aa214e668a9003@mail.gmail.com>
	<C3C8E354.42AC%dafshartous@med.miami.edu>
Message-ID: <40e66e0b0802021547v63a44145s467f1c568edbe495@mail.gmail.com>

Thanks for including the code to generate the example, David, and I
apologize for grumbling at you.  Sometimes my reaction to a feature
request for the lme4 package is "Oh God, I am going to have to think
about how to do this in the general case".

Once I thought about it the answer was comparatively straightforward,
although I can imagine circumstances in which one would choke even a
powerful computer trying to construct it.

The problem with trying to construct this in the general case is that
you can't make sense of Z_i.  However, you can construct tr(Z) %*%
Sigma %*% Z which, in this case will be a block diagonal matrix.

I need to explain a couple of relationships in the slots from a fitted
model.  As I mentioned previously, the Zt slot is a sparse
representation of the transpose of Z.  Let n be the number of
observations and q be the total number of random effects.  There are
(virtual) q by q matrices S and T where S is diagonal with
non-negative diagonal elements and T is unit lower triangular that
define the q by q variance-covariance matrix Sigma of the
q-dimensional random effects vector b.

Sigma = sigma^2 * T %*% S %*% S %*% t(T)

The A slot is

A = t(T) %*% S %*% t(Z)

So the matrix you want is

sigma^2 * (crossprod(fm at A) + I)

The matrix crossprod(fm at A) will be stored as a sparse symmetric
matrix.  There may be a slick way of adding an identity to it  but I
can't recall offhand how to do so (Martin would know better than I but
I think he is about to go on vacation somewhere where he doesn't have
email access).  Then you need to multiply by s^2 to get the estimated
marginal covariance matrix.

Before doing that you should check that it really is block diagonal.
Deepayan Sarkar wrote an image method for sparse matrices that helps
in visualization of the pattern.  Try

library(lme4)
fm1.no.cov <- lmer(dv ~ time.num * drug + (0+Pind|Patient.new) +
(0+Dind|Patient.new), dat.new)

image(crossprod(fm1.no.cov at A))
image(drop0(crossprod(fm1.no.cov at A))

as(crossprod(fm1.no.cov at A)[1:8,1:8], "matrix")

Remember that you need to add 1 to the diagonal elements then multiply
the matrix by s^2.

This method works for this example.  In general the matrix
crossprod(fm at A) has the potential of being a sparse matrix with a huge
number of nonzeros.


On Feb 1, 2008 1:55 PM, David Afshartous <dafshartous at med.miami.edu> wrote:
>
> RE what I'd like to calculate, I'd like to calculate the marginal covariance
> matrix, i.e., tr(Z_i) Psi Z_I + sigma^2 I, in the notation of your book.
>
> I've done this for a series of models estimated with lme() using
> getVarCov(model, type = "marginal"), but for the last model there is an
> error message and I wanted to see if this is also the case for lmer().
>
> Here is the problem on simulated data called dat.new (formed below;
> longitudinal data from a crossover design; 20 patients, 4 measurements each,
> both for drug and placebo, thus n=160 observations total):
>
> # basic random intercept model, marginal covariance matrix is compound
> # symmetric as expected:
>
> > fm <- lme( dv ~ time.num*drug,  random = ~ 1 | Patient.new,  data=dat.new )
> > getVarCov(fm, type = "marginal")
> Patient.new 1
> Marginal variance covariance matrix
>         1       2       3       4       5       6       7       8
> 1 18.8570  8.5199  8.5199  8.5199  8.5199  8.5199  8.5199  8.5199
> 2  8.5199 18.8570  8.5199  8.5199  8.5199  8.5199  8.5199  8.5199
> 3  8.5199  8.5199 18.8570  8.5199  8.5199  8.5199  8.5199  8.5199
> 4  8.5199  8.5199  8.5199 18.8570  8.5199  8.5199  8.5199  8.5199
> 5  8.5199  8.5199  8.5199  8.5199 18.8570  8.5199  8.5199  8.5199
> 6  8.5199  8.5199  8.5199  8.5199  8.5199 18.8570  8.5199  8.5199
> 7  8.5199  8.5199  8.5199  8.5199  8.5199  8.5199 18.8570  8.5199
> 8  8.5199  8.5199  8.5199  8.5199  8.5199  8.5199  8.5199 18.8570
>   Standard Deviations: 4.3424 4.3424 4.3424 4.3424 4.3424 4.3424 4.3424
> 4.3424
>
>
> ## now I make random effect variance stratified by treatment.
> ## as expected, the 8x8 matrix has the upper left 4x4 block
> ## and the lower right 4x4 block represent the compound symmetric
> ## covariance matrix for the different treatment conditions (recall
> ## that the same patient will experience both since it's a crossover).
> ## whereas the upper right and lower left 4x4 blocks represent
> ## covariance of measurements across treatment condition, e.g.,
> ## the covariance of measurement i on treatment to measurement j on control.
> ## such a covariance exists since although we have stratified the intercept
> ## random effect by treatment, a covariance of these random variables is
> specified by default ( check via summary(fm1) ).
>
> > fm1 <- lme( dv ~ time.num*drug,  random = ~ 0 + Pind + Dind | Patient.new,
> data=dat.new )
> > getVarCov(fm1, type = "marginal")
> Patient.new 1
> Marginal variance covariance matrix
>         1       2       3       4      5      6      7      8
> 1 31.3030 29.3380 29.3380 29.3380 2.0209 2.0209 2.0209 2.0209
> 2 29.3380 31.3030 29.3380 29.3380 2.0209 2.0209 2.0209 2.0209
> 3 29.3380 29.3380 31.3030 29.3380 2.0209 2.0209 2.0209 2.0209
> 4 29.3380 29.3380 29.3380 31.3030 2.0209 2.0209 2.0209 2.0209
> 5  2.0209  2.0209  2.0209  2.0209 6.8509 4.8858 4.8858 4.8858
> 6  2.0209  2.0209  2.0209  2.0209 4.8858 6.8509 4.8858 4.8858
> 7  2.0209  2.0209  2.0209  2.0209 4.8858 4.8858 6.8509 4.8858
> 8  2.0209  2.0209  2.0209  2.0209 4.8858 4.8858 4.8858 6.8509
>   Standard Deviations: 5.5949 5.5949 5.5949 5.5949 2.6174 2.6174 2.6174
> 2.6174
>
> ## now, presumably the off-diagonal 4x4 blocks should go to zero if the
> ## covariance between the intercept random effects for treatment/control can
> ## be constrained to 0.  This can be done via:
>
> > fm1.no.cov <- lme( dv ~ time.num*drug,  random = list(~ 0 + Pind  |
> Patient.new, ~ 0 + Dind | Patient.new),  data=dat.new )
>
> # But the following error message arises when trying to get the marginal
> # covariance:
>
> > getVarCov(fm1.no.cov, type = "marginal")
> Error in getVarCov.lme(fm1.no.cov, type = "marginal") :
>   Not implemented for multiple levels of nesting
>
> Does there exist a different model statement for fm1.no.cov such that this
> problem doesn't arise?  In any event, this is why I sought getVarCov in
> lmer().
>
> ##################################
> ## code to generate simulated data above:
>
> set.seed(500)
> n.timepoints <- 4
> n.subj.per.tx <- 20
> sd.d <- 5;
> sd.p <- 2;
> sd.res <- 1.3
> drug <- factor(rep(c("D", "P"), each = n.timepoints, times =
> n.subj.per.tx))
> drug.baseline <- rep( c(0,5), each=n.timepoints, times=n.subj.per.tx )
> Patient.baseline <- rep( rnorm( n.subj.per.tx*2, sd=c(sd.d, sd.p) ),
> each=n.timepoints )
> time.baseline <- rep(1:n.timepoints,n.subj.per.tx*2)*as.numeric(drug=="D")
> dv <- rnorm( n.subj.per.tx*n.timepoints*2,
> mean=time.baseline+Patient.baseline+drug.baseline, sd=sd.res )
>
> dat.new <- data.frame(drug, dv)
> dat.new$Dind <- as.numeric(dat.new$drug == "D")
> dat.new$Pind <- as.numeric(dat.new$drug == "P")
> dat.new$time.num = rep(1:n.timepoints, n.subj.per.tx*2)
> dat.new$Patient.new = rep(1:20, each=8)
>
>
>
> On 2/1/08 1:29 PM, "Douglas Bates" <bates at stat.wisc.edu> wrote:
>
> > On Feb 1, 2008 11:15 AM, David Afshartous <dafshartous at med.miami.edu> wrote:
> >
> >> All,
> >
> >> I've fitted some models using lme() and now I want to check some things by
> >> fitting them in lmer() (latest version loaded from CRAN).  Under the listed
> >> methods for lmer() I don't seem to find functionality similar to that of
> >> getVarCov() to obtain the variance-covariance matrix of the fitted model.
> >> Does such functionality exist?
> >
> > I would need to go back and check what getVarCov  produces.  Can you
> > tell me what you want to be able to calculate?
> >
> > Be aware that it is not always possible to reproduce all the
> > capabilities of the nlme package in lme4 because nlme does not handle
> > (except through egregious kludges) crossed or partially crossed random
> > effects specifications.  Some of the quantities that can be calculated
> > for models with a single level of random effects or with multiple
> > nested levels of random effects cannot even be defined for models with
> > crossed random effects.
> >
> >> PS - running R v2.6.0.
> >
> >> PPS - if it must be computed manually, how does one access information
> >> stored in slots (e.g., Z', the transpose of the model matrix for the random
> >> effects)?
> >
> > If the fitted model is fm then Z' is available as
> >
> > fm at Zt
> >
> > It is stored as a compressed, column-oriented sparse matrix of class
> > "dgCMatrix", defined in the Matrix package.  Linear algebra operations
> > with such objects should magically do the right thing.  If you are
> > working with very large models, of course, you will find that you need
> > to be very careful about exactly what operations are done with Z.  In
> > many books on mixed models there are formulas for many calculations
> > based on a generalized least squares representation of the model.
> > Don't even consider using such formulas except for trivial examples.
> >
> > I've seen far too many papers where the authors write down an n by n
> > matrix like V = sigma^2 I + Z Sigma Z' and then proceed to blithely
> > describe calculations involving the inverse of V.  I usually stop
> > reading at that point because I know that the author has never tried
> > to perform the calculation on a non-trivial example.  When n is in the
> > hundreds of thousands or in the millions you do not even construct n
> > by n matrices, let alone try to decompose them.  And anyone with
> > experience in numerical linear algebra knows that you never, ever,
> > ever expect to evaluate a formula that involves the inverse of a large
> > matrix.
> >
> > This is all to say that you could write down an expression involving
> > solve() applied to some matrix of size n by n derived from Z but you
> > will be disappointed in the speed.
>
>



From reinhold.kliegl at gmail.com  Sun Feb  3 10:14:43 2008
From: reinhold.kliegl at gmail.com (Reinhold Kliegl)
Date: Sun, 3 Feb 2008 10:14:43 +0100
Subject: [R-sig-ME] getVarCov for lmer()
In-Reply-To: <40e66e0b0802021547v63a44145s467f1c568edbe495@mail.gmail.com>
References: <40e66e0b0802011029u4b3e3040qb4aa214e668a9003@mail.gmail.com>
	<C3C8E354.42AC%dafshartous@med.miami.edu>
	<40e66e0b0802021547v63a44145s467f1c568edbe495@mail.gmail.com>
Message-ID: <aefe4d0a0802030114x445c8a78u8c1932af905c1047@mail.gmail.com>

Amazing.

Adding 1 to the diagonal is easy indeed:
> as(crossprod(fm1.no.cov at A)[1:8,1:8], "matrix")+diag(8)
         [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]
[1,] 15.92742 14.92742 14.92742 14.92742 0.000000 0.000000 0.000000 0.000000
[2,] 14.92742 15.92742 14.92742 14.92742 0.000000 0.000000 0.000000 0.000000
[3,] 14.92742 14.92742 15.92742 14.92742 0.000000 0.000000 0.000000 0.000000
[4,] 14.92742 14.92742 14.92742 15.92742 0.000000 0.000000 0.000000 0.000000
[5,]  0.00000  0.00000  0.00000  0.00000 3.486305 2.486305 2.486305 2.486305
[6,]  0.00000  0.00000  0.00000  0.00000 2.486305 3.486305 2.486305 2.486305
[7,]  0.00000  0.00000  0.00000  0.00000 2.486305 2.486305 3.486305 2.486305
[8,]  0.00000  0.00000  0.00000  0.00000 2.486305 2.486305 2.486305 3.486305

Reinhold

On Feb 3, 2008 12:47 AM, Douglas Bates <bates at stat.wisc.edu> wrote:
> Thanks for including the code to generate the example, David, and I
> apologize for grumbling at you.  Sometimes my reaction to a feature
> request for the lme4 package is "Oh God, I am going to have to think
> about how to do this in the general case".
>
> Once I thought about it the answer was comparatively straightforward,
> although I can imagine circumstances in which one would choke even a
> powerful computer trying to construct it.
>
> The problem with trying to construct this in the general case is that
> you can't make sense of Z_i.  However, you can construct tr(Z) %*%
> Sigma %*% Z which, in this case will be a block diagonal matrix.
>
> I need to explain a couple of relationships in the slots from a fitted
> model.  As I mentioned previously, the Zt slot is a sparse
> representation of the transpose of Z.  Let n be the number of
> observations and q be the total number of random effects.  There are
> (virtual) q by q matrices S and T where S is diagonal with
> non-negative diagonal elements and T is unit lower triangular that
> define the q by q variance-covariance matrix Sigma of the
> q-dimensional random effects vector b.
>
> Sigma = sigma^2 * T %*% S %*% S %*% t(T)
>
> The A slot is
>
> A = t(T) %*% S %*% t(Z)
>
> So the matrix you want is
>
> sigma^2 * (crossprod(fm at A) + I)
>
> The matrix crossprod(fm at A) will be stored as a sparse symmetric
> matrix.  There may be a slick way of adding an identity to it  but I
> can't recall offhand how to do so (Martin would know better than I but
> I think he is about to go on vacation somewhere where he doesn't have
> email access).  Then you need to multiply by s^2 to get the estimated
> marginal covariance matrix.
>
> Before doing that you should check that it really is block diagonal.
> Deepayan Sarkar wrote an image method for sparse matrices that helps
> in visualization of the pattern.  Try
>
> library(lme4)
> fm1.no.cov <- lmer(dv ~ time.num * drug + (0+Pind|Patient.new) +
> (0+Dind|Patient.new), dat.new)
>
> image(crossprod(fm1.no.cov at A))
> image(drop0(crossprod(fm1.no.cov at A))
>
> as(crossprod(fm1.no.cov at A)[1:8,1:8], "matrix")
>
> Remember that you need to add 1 to the diagonal elements then multiply
> the matrix by s^2.
>
> This method works for this example.  In general the matrix
> crossprod(fm at A) has the potential of being a sparse matrix with a huge
> number of nonzeros.
>
>
>
> On Feb 1, 2008 1:55 PM, David Afshartous <dafshartous at med.miami.edu> wrote:
> >
> > RE what I'd like to calculate, I'd like to calculate the marginal covariance
> > matrix, i.e., tr(Z_i) Psi Z_I + sigma^2 I, in the notation of your book.
> >
> > I've done this for a series of models estimated with lme() using
> > getVarCov(model, type = "marginal"), but for the last model there is an
> > error message and I wanted to see if this is also the case for lmer().
> >
> > Here is the problem on simulated data called dat.new (formed below;
> > longitudinal data from a crossover design; 20 patients, 4 measurements each,
> > both for drug and placebo, thus n=160 observations total):
> >
> > # basic random intercept model, marginal covariance matrix is compound
> > # symmetric as expected:
> >
> > > fm <- lme( dv ~ time.num*drug,  random = ~ 1 | Patient.new,  data=dat.new )
> > > getVarCov(fm, type = "marginal")
> > Patient.new 1
> > Marginal variance covariance matrix
> >         1       2       3       4       5       6       7       8
> > 1 18.8570  8.5199  8.5199  8.5199  8.5199  8.5199  8.5199  8.5199
> > 2  8.5199 18.8570  8.5199  8.5199  8.5199  8.5199  8.5199  8.5199
> > 3  8.5199  8.5199 18.8570  8.5199  8.5199  8.5199  8.5199  8.5199
> > 4  8.5199  8.5199  8.5199 18.8570  8.5199  8.5199  8.5199  8.5199
> > 5  8.5199  8.5199  8.5199  8.5199 18.8570  8.5199  8.5199  8.5199
> > 6  8.5199  8.5199  8.5199  8.5199  8.5199 18.8570  8.5199  8.5199
> > 7  8.5199  8.5199  8.5199  8.5199  8.5199  8.5199 18.8570  8.5199
> > 8  8.5199  8.5199  8.5199  8.5199  8.5199  8.5199  8.5199 18.8570
> >   Standard Deviations: 4.3424 4.3424 4.3424 4.3424 4.3424 4.3424 4.3424
> > 4.3424
> >
> >
> > ## now I make random effect variance stratified by treatment.
> > ## as expected, the 8x8 matrix has the upper left 4x4 block
> > ## and the lower right 4x4 block represent the compound symmetric
> > ## covariance matrix for the different treatment conditions (recall
> > ## that the same patient will experience both since it's a crossover).
> > ## whereas the upper right and lower left 4x4 blocks represent
> > ## covariance of measurements across treatment condition, e.g.,
> > ## the covariance of measurement i on treatment to measurement j on control.
> > ## such a covariance exists since although we have stratified the intercept
> > ## random effect by treatment, a covariance of these random variables is
> > specified by default ( check via summary(fm1) ).
> >
> > > fm1 <- lme( dv ~ time.num*drug,  random = ~ 0 + Pind + Dind | Patient.new,
> > data=dat.new )
> > > getVarCov(fm1, type = "marginal")
> > Patient.new 1
> > Marginal variance covariance matrix
> >         1       2       3       4      5      6      7      8
> > 1 31.3030 29.3380 29.3380 29.3380 2.0209 2.0209 2.0209 2.0209
> > 2 29.3380 31.3030 29.3380 29.3380 2.0209 2.0209 2.0209 2.0209
> > 3 29.3380 29.3380 31.3030 29.3380 2.0209 2.0209 2.0209 2.0209
> > 4 29.3380 29.3380 29.3380 31.3030 2.0209 2.0209 2.0209 2.0209
> > 5  2.0209  2.0209  2.0209  2.0209 6.8509 4.8858 4.8858 4.8858
> > 6  2.0209  2.0209  2.0209  2.0209 4.8858 6.8509 4.8858 4.8858
> > 7  2.0209  2.0209  2.0209  2.0209 4.8858 4.8858 6.8509 4.8858
> > 8  2.0209  2.0209  2.0209  2.0209 4.8858 4.8858 4.8858 6.8509
> >   Standard Deviations: 5.5949 5.5949 5.5949 5.5949 2.6174 2.6174 2.6174
> > 2.6174
> >
> > ## now, presumably the off-diagonal 4x4 blocks should go to zero if the
> > ## covariance between the intercept random effects for treatment/control can
> > ## be constrained to 0.  This can be done via:
> >
> > > fm1.no.cov <- lme( dv ~ time.num*drug,  random = list(~ 0 + Pind  |
> > Patient.new, ~ 0 + Dind | Patient.new),  data=dat.new )
> >
> > # But the following error message arises when trying to get the marginal
> > # covariance:
> >
> > > getVarCov(fm1.no.cov, type = "marginal")
> > Error in getVarCov.lme(fm1.no.cov, type = "marginal") :
> >   Not implemented for multiple levels of nesting
> >
> > Does there exist a different model statement for fm1.no.cov such that this
> > problem doesn't arise?  In any event, this is why I sought getVarCov in
> > lmer().
> >
> > ##################################
> > ## code to generate simulated data above:
> >
> > set.seed(500)
> > n.timepoints <- 4
> > n.subj.per.tx <- 20
> > sd.d <- 5;
> > sd.p <- 2;
> > sd.res <- 1.3
> > drug <- factor(rep(c("D", "P"), each = n.timepoints, times =
> > n.subj.per.tx))
> > drug.baseline <- rep( c(0,5), each=n.timepoints, times=n.subj.per.tx )
> > Patient.baseline <- rep( rnorm( n.subj.per.tx*2, sd=c(sd.d, sd.p) ),
> > each=n.timepoints )
> > time.baseline <- rep(1:n.timepoints,n.subj.per.tx*2)*as.numeric(drug=="D")
> > dv <- rnorm( n.subj.per.tx*n.timepoints*2,
> > mean=time.baseline+Patient.baseline+drug.baseline, sd=sd.res )
> >
> > dat.new <- data.frame(drug, dv)
> > dat.new$Dind <- as.numeric(dat.new$drug == "D")
> > dat.new$Pind <- as.numeric(dat.new$drug == "P")
> > dat.new$time.num = rep(1:n.timepoints, n.subj.per.tx*2)
> > dat.new$Patient.new = rep(1:20, each=8)
> >
> >
> >
> > On 2/1/08 1:29 PM, "Douglas Bates" <bates at stat.wisc.edu> wrote:
> >
> > > On Feb 1, 2008 11:15 AM, David Afshartous <dafshartous at med.miami.edu> wrote:
> > >
> > >> All,
> > >
> > >> I've fitted some models using lme() and now I want to check some things by
> > >> fitting them in lmer() (latest version loaded from CRAN).  Under the listed
> > >> methods for lmer() I don't seem to find functionality similar to that of
> > >> getVarCov() to obtain the variance-covariance matrix of the fitted model.
> > >> Does such functionality exist?
> > >
> > > I would need to go back and check what getVarCov  produces.  Can you
> > > tell me what you want to be able to calculate?
> > >
> > > Be aware that it is not always possible to reproduce all the
> > > capabilities of the nlme package in lme4 because nlme does not handle
> > > (except through egregious kludges) crossed or partially crossed random
> > > effects specifications.  Some of the quantities that can be calculated
> > > for models with a single level of random effects or with multiple
> > > nested levels of random effects cannot even be defined for models with
> > > crossed random effects.
> > >
> > >> PS - running R v2.6.0.
> > >
> > >> PPS - if it must be computed manually, how does one access information
> > >> stored in slots (e.g., Z', the transpose of the model matrix for the random
> > >> effects)?
> > >
> > > If the fitted model is fm then Z' is available as
> > >
> > > fm at Zt
> > >
> > > It is stored as a compressed, column-oriented sparse matrix of class
> > > "dgCMatrix", defined in the Matrix package.  Linear algebra operations
> > > with such objects should magically do the right thing.  If you are
> > > working with very large models, of course, you will find that you need
> > > to be very careful about exactly what operations are done with Z.  In
> > > many books on mixed models there are formulas for many calculations
> > > based on a generalized least squares representation of the model.
> > > Don't even consider using such formulas except for trivial examples.
> > >
> > > I've seen far too many papers where the authors write down an n by n
> > > matrix like V = sigma^2 I + Z Sigma Z' and then proceed to blithely
> > > describe calculations involving the inverse of V.  I usually stop
> > > reading at that point because I know that the author has never tried
> > > to perform the calculation on a non-trivial example.  When n is in the
> > > hundreds of thousands or in the millions you do not even construct n
> > > by n matrices, let alone try to decompose them.  And anyone with
> > > experience in numerical linear algebra knows that you never, ever,
> > > ever expect to evaluate a formula that involves the inverse of a large
> > > matrix.
> > >
> > > This is all to say that you could write down an expression involving
> > > solve() applied to some matrix of size n by n derived from Z but you
> > > will be disappointed in the speed.
> >
> >
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From nfyllas at gmail.com  Mon Feb  4 13:15:32 2008
From: nfyllas at gmail.com (Nikos Fyllas)
Date: Mon, 4 Feb 2008 12:15:32 +0000
Subject: [R-sig-ME] crossed effects formulation
Message-ID: <70eb75240802040415t3a52a16l361e307e96cfceef@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080204/ecaaea2d/attachment.pl>

From bates at stat.wisc.edu  Mon Feb  4 15:05:27 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 4 Feb 2008 08:05:27 -0600
Subject: [R-sig-ME] crossed effects formulation
In-Reply-To: <70eb75240802040415t3a52a16l361e307e96cfceef@mail.gmail.com>
References: <70eb75240802040415t3a52a16l361e307e96cfceef@mail.gmail.com>
Message-ID: <40e66e0b0802040605n224f9108sb7c824de34f488d5@mail.gmail.com>

On Feb 4, 2008 6:15 AM, Nikos Fyllas <nfyllas at gmail.com> wrote:
> Dear all,

> I have a model like: y.m1 <- lmer(y~1+factor(Site)+(1|Species))

> so a fixed site effect is crossed with a random effect of  a species.

I'm not sure I understand what you mean.  That model is an additive
model with fixed effects for Site and random effects for Species.

The random effects have an expected value of zero and, by default, the
Site factor will be represented as an "Intercept" and N-1 contrasts
labelled Site2, Site3, ..., SiteN.  The "Intercept" coefficient is the
prediction of the mean response at Site1 for a Species effect of 0.
The coefficient labeled Site2 is the difference between Site2 and
Site1.  That is, Site 1 is the reference level and all the other
coefficients are relative to that reference level.

> Does the actual equation yield to:
>
> y(site, species) = mean + SIte1_effect + Site2_effect + ...... SiteN_effect
> + Species_effect + error

Not quite.  The parameters are as I described them above.

> ie allowing different Sites to have different variances?

No, the different Sites have different predicted means but not
different variances.



From kw.statr at gmail.com  Mon Feb  4 16:23:37 2008
From: kw.statr at gmail.com (Kevin Wright)
Date: Mon, 4 Feb 2008 09:23:37 -0600
Subject: [R-sig-ME] Factor analysis with lme?
In-Reply-To: <c968588d0801301217n4481254fxc16b2e81fded52ab@mail.gmail.com>
References: <c968588d0801301217n4481254fxc16b2e81fded52ab@mail.gmail.com>
Message-ID: <c968588d0802040723x6d775025pa220739eb7e484d4@mail.gmail.com>

Just to follow up, I received no information regarding the factor
analytic model in lme, but learned that it can be done with asreml-r,
e.g.

d1=d1[order(d1$B, d1$A),]
m1 = asreml(y ~ A, data=d1, rcov=~B:facv(A, 1))

K Wright



On Jan 30, 2008 2:17 PM, Kevin Wright <kw.statr at gmail.com> wrote:
> I'm looking for an example of how (if?) lme can be used with factor
> analysis.  By that, I mean modeling the residual variance/covariance
> matrix as
>
> Sigma = (Gamma)(Gamma)' + D
>
> where Gamma is a vector (or matrix) corresponding to latent variables
> and D is a diagonal matrix.
>
> I have looked around but have not found any examples.  Pointers,
> comments, and code are all welcome.
>
> K Wright
>



From bernd.weiss at uni-koeln.de  Wed Feb  6 17:12:29 2008
From: bernd.weiss at uni-koeln.de (Bernd Weiss)
Date: Wed, 06 Feb 2008 17:12:29 +0100
Subject: [R-sig-ME] Weird behaviour of mcmcsamp & glmer
Message-ID: <47A9DC6D.8050007@uni-koeln.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

library(lme4)
m1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),
~            family = binomial, data = cbpp)
fixef(m1)  ## first call
tmp <- mcmcsamp(m1, 1000)
fixef(m1)  ## second call

The fixed effects (m1) before and after the use of mcmcsamp differ
considerably. Why?

TIA,

Bernd

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.7 (MingW32)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFHqdxtUsbvfbd00+ERAgVXAKCS3oNVMK5nTo05IARZ6d+UmGotqQCgqSpQ
vIYoubaJl20NVQRjNzmSohg=
=8Ixm
-----END PGP SIGNATURE-----



From bates at stat.wisc.edu  Wed Feb  6 17:25:12 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 6 Feb 2008 10:25:12 -0600
Subject: [R-sig-ME] Weird behaviour of mcmcsamp & glmer
In-Reply-To: <47A9DC6D.8050007@uni-koeln.de>
References: <47A9DC6D.8050007@uni-koeln.de>
Message-ID: <40e66e0b0802060825j5c9e7842w800ec082d37745ae@mail.gmail.com>

Which version of the lme4 package?

Probably what has happened is that I modify the fitted model object
during the mcmcsamp process and have forgotten to reinstall the
original version of the fixef parameters when done.

On Feb 6, 2008 10:12 AM, Bernd Weiss <bernd.weiss at uni-koeln.de> wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> library(lme4)
> m1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),
> ~            family = binomial, data = cbpp)
> fixef(m1)  ## first call
> tmp <- mcmcsamp(m1, 1000)
> fixef(m1)  ## second call
>
> The fixed effects (m1) before and after the use of mcmcsamp differ
> considerably. Why?
>
> TIA,
>
> Bernd
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.7 (MingW32)
> Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org
>
> iD8DBQFHqdxtUsbvfbd00+ERAgVXAKCS3oNVMK5nTo05IARZ6d+UmGotqQCgqSpQ
> vIYoubaJl20NVQRjNzmSohg=
> =8Ixm
> -----END PGP SIGNATURE-----
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bernd.weiss at uni-koeln.de  Wed Feb  6 17:31:10 2008
From: bernd.weiss at uni-koeln.de (Bernd Weiss)
Date: Wed, 06 Feb 2008 17:31:10 +0100
Subject: [R-sig-ME] Weird behaviour of mcmcsamp & glmer
In-Reply-To: <40e66e0b0802060825j5c9e7842w800ec082d37745ae@mail.gmail.com>
References: <47A9DC6D.8050007@uni-koeln.de>
	<40e66e0b0802060825j5c9e7842w800ec082d37745ae@mail.gmail.com>
Message-ID: <47A9E0CE.2040403@uni-koeln.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Douglas Bates schrieb:
| Which version of the lme4 package?
|
| Probably what has happened is that I modify the fitted model object
| during the mcmcsamp process and have forgotten to reinstall the
| original version of the fixef parameters when done.



Sorry, what a stupid beginner's mistake...


| sessionInfo()
R version 2.6.1 Patched (2007-12-04 r43588)
i386-pc-mingw32

locale:
LC_COLLATE=German_Germany.1252;LC_CTYPE=German_Germany.1252;LC_MONETARY=German_Germany.1252;LC_NUMERIC=C;LC_TIME=German_Germany.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] lme4_0.999375-0   Matrix_0.999375-4 lattice_0.17-4

loaded via a namespace (and not attached):
[1] grid_2.6.1


Thanks for your help,

Bernd
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.7 (MingW32)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFHqeDOUsbvfbd00+ERAr1FAJ94AgszP9m9sB9UuVWMk+984eYwowCcCTPC
vQat6/TEjYVmQXVAly+MHW0=
=jDDH
-----END PGP SIGNATURE-----



From dafshartous at med.miami.edu  Wed Feb  6 23:09:31 2008
From: dafshartous at med.miami.edu (David Afshartous)
Date: Wed, 06 Feb 2008 17:09:31 -0500
Subject: [R-sig-ME] Behavior of coef() for lmer() model w/ level-2 variance
	stratified
In-Reply-To: <40e66e0b0802021547v63a44145s467f1c568edbe495@mail.gmail.com>
Message-ID: <C3CF9A4B.4371%dafshartous@med.miami.edu>



Prof Bates:

FYI, coef() does not seem to work for models fit w/ lmer() where the level-2
variance is stratified, e.g., depends on treatment group.  Is there a quick
fix, or should I simply calculate the desired coefficients manually?
Example code below.

Thanks,
David


J = 10; K = 4
set.seed(500)
fake = Strat.mean.var.simple (J,K)
lme.no.strat = lmer (y ~  treatment.ind + (1 | person1) , data = fake)
lme.strat = lmer (y ~  treatment.ind + ( 0 + placebo.ind | person1) + (0 +
treatment.ind | person1), data = fake)
coef(lme.strat)
Error in coef(lme.strat) : unable to align random and fixed effects

coef(lme.no.strat)

An object of class ?coef.lmer?
[[1]]
   (Intercept) treatment.ind
1          6.0           2.9
2          7.9           2.9
3          7.4           2.9
4          5.1           2.9
5          6.7           2.9
6          4.0           2.9
7          8.1           2.9
8          8.2           2.9
9          6.4           2.9
10         6.4           2.9

#####################################################
J = 10
K = 4
Strat.mean.var.simple <- function (J, K){
    time <- rep(seq(0,1, ,length=K), J) # K measurements
    person <- rep(1:(J/2), each = K)
    treatment <- rep(0:1, each = J/2)
    treatment.ind <- rep(0:1, each = (J/2)*K)
    person1 <- rep(1:J, ,each = K)
    placebo.ind.1 <- treatment.ind < 1
    placebo.ind = ifelse( placebo.ind.1, 1, 0)
#
    mu.a.true.P = 4.8
    mu.a.true.T = 8.8
    sigma.a.true.P = 2.2
    sigma.a.true.T = 4.2
    sigma.y.true = 1.2
#
    a.true.P = rnorm (J/2, mu.a.true.P, sigma.a.true.P)
    a.true.T = rnorm (J/2, mu.a.true.T, sigma.a.true.T)
#
    y.P <- rnorm( (J/2)*K, a.true.P[person], sigma.y.true)
    y.T <- rnorm( (J/2)*K, a.true.T[person], sigma.y.true)
    y <- c(y.P, y.T)
    return ( data.frame (y, time, person1, treatment.ind, placebo.ind))
}



From fauna at pngp.it  Thu Feb  7 15:32:52 2008
From: fauna at pngp.it (Achaz von Hardenberg)
Date: Thu, 7 Feb 2008 15:32:52 +0100
Subject: [R-sig-ME] error msg in nlme
Message-ID: <2F49A2E4-D450-42BF-BEB8-BBA5B0A96895@pngp.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080207/a9216966/attachment.pl>

From bates at stat.wisc.edu  Thu Feb  7 18:16:28 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 7 Feb 2008 11:16:28 -0600
Subject: [R-sig-ME] error msg in nlme
In-Reply-To: <2F49A2E4-D450-42BF-BEB8-BBA5B0A96895@pngp.it>
References: <2F49A2E4-D450-42BF-BEB8-BBA5B0A96895@pngp.it>
Message-ID: <40e66e0b0802070916p51941d88w6e3248a097706646@mail.gmail.com>

On Feb 7, 2008 8:32 AM, Achaz von Hardenberg <fauna at pngp.it> wrote:
> hi all,
> Using the nlme package, I am doing some lme analyses on the effect of
> introduced fish on zooplankton biodiversity in alpine lakes.
> Everything going fine with all zooplankton taxa analyzed so far but
> one for which I get an error message which I do not know how to
> interpret:

>  > zoop.lme<-lme(Logcrpic ~ as.factor(year)+as.factor(month) + FAC1 +
> FAC2 + FAC3 + as.factor(fish),random=~1|
> Cod,data=zooplankton,na.action=na.omit)

> Error in solve.default(estimates[dimE[1] - (p:1), dimE[2] - (p:1),
> drop = FALSE]) :
>         system is computationally singular: reciprocal condition number =
> 2.21155e-16

The error is occurring in an attempt to invert a matrix - probably
when evaluating the standard errors of the estimates of the
fixed-effects coefficients.

I would suggest trying to fit the model with the lmer function in the
lme4 package instead.  Use

library(lme4)
zoop.lmer <- lmerlme(Logcrpic ~ as.factor(year)+as.factor(month) +
FAC1 +FAC2 + FAC3 + as.factor(fish) + (1|
Cod), data=zooplankton, verbose = TRUE)

> I would be thankful to anybody who could assist me in understanding
> what is going wrong with this particular variable...
>
> best regards,
> Achaz
>
> Dr. Achaz von Hardenberg
> ------------------------------------------------------------------------
> --------------------------------
> Centro Studi Fauna Alpina - Alpine Wildlife Research Centre
> Servizio Sanitario e della Ricerca Scientifica
> Parco Nazionale Gran Paradiso, Degioz, 11, 11010-Valsavarenche (Ao),
> Italy
>
>
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From kedwards at ucdavis.edu  Thu Feb  7 23:30:23 2008
From: kedwards at ucdavis.edu (Kyle Edwards)
Date: Thu, 7 Feb 2008 14:30:23 -0800
Subject: [R-sig-ME] hatTrace and effective degrees of freedom
Message-ID: <478AB7E6-AF1F-42B5-B021-3F0C5313A567@ucdavis.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080207/85edeaa7/attachment.pl>

From austin.frank at gmail.com  Fri Feb  8 00:53:44 2008
From: austin.frank at gmail.com (Austin Frank)
Date: Thu, 07 Feb 2008 18:53:44 -0500
Subject: [R-sig-ME] New alpha release of lme4 available on R-forge
References: <40e66e0b0801301559l364c04f7j12ced4719dfcc618@mail.gmail.com>
Message-ID: <m0bq6snx13.fsf@gmail.com>

On Wed, Jan 30 2008, Douglas Bates wrote:

> - The effect of the link function and the variance function in the
> family argument have been separated.  Families beyond the binomial and
> poisson are now allowed although I would advise caution.  I need to do
> some more research on whether a common scale parameter changes the
> calculation of the deviance.  (Feature request #81).

Dr. Bates--

Reading through the feature request and your response to it, I was not
quite clear on how to interpret this message.  Above you say "families
beyond the binomial and poisson are now allowed...".  Does this mean
that you've added the families and linking functions that were in the
previous version of lme4 (gamma, etc.), or that any family may be used?

Unsurprisingly, I'm wondering whether the above change will enable me to
model a multinomial dependent variable using glmer.  Is this now
possible?

Thanks for your help,
/au

-- 
Austin Frank
http://aufrank.net
GPG Public Key (D7398C2F): http://aufrank.net/personal.asc



From bates at stat.wisc.edu  Fri Feb  8 01:22:04 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 7 Feb 2008 18:22:04 -0600
Subject: [R-sig-ME] New alpha release of lme4 available on R-forge
In-Reply-To: <m0bq6snx13.fsf@gmail.com>
References: <40e66e0b0801301559l364c04f7j12ced4719dfcc618@mail.gmail.com>
	<m0bq6snx13.fsf@gmail.com>
Message-ID: <40e66e0b0802071622v3df1c808j3baf311705ed7c2f@mail.gmail.com>

On Feb 7, 2008 5:53 PM, Austin Frank <austin.frank at gmail.com> wrote:
> On Wed, Jan 30 2008, Douglas Bates wrote:
>
> > - The effect of the link function and the variance function in the
> > family argument have been separated.  Families beyond the binomial and
> > poisson are now allowed although I would advise caution.  I need to do
> > some more research on whether a common scale parameter changes the
> > calculation of the deviance.  (Feature request #81).

> Dr. Bates--

> Reading through the feature request and your response to it, I was not
> quite clear on how to interpret this message.  Above you say "families
> beyond the binomial and poisson are now allowed...".  Does this mean
> that you've added the families and linking functions that were in the
> previous version of lme4 (gamma, etc.), or that any family may be used?

I should have been more explicit.  One can use any of the families
defined in the stats package for R, including the quasi family.
Essentially those families define the link and inverse link functions
and the variance function.  Because these functions can end up being
called many, many times in optimizing the log-likelihood for model
they are evaluated in compiled code.

They could be extended but that would need a definition of inverse
link, the derivative of the inverse link and the variance function for
any new families that are to be incorporated.

> Unsurprisingly, I'm wondering whether the above change will enable me to
> model a multinomial dependent variable using glmer.  Is this now
> possible?
>
> Thanks for your help,
> /au
>
> --
> Austin Frank
> http://aufrank.net
> GPG Public Key (D7398C2F): http://aufrank.net/personal.asc
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From David.Duffy at qimr.edu.au  Fri Feb  8 03:16:17 2008
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Fri, 8 Feb 2008 12:16:17 +1000 (EST)
Subject: [R-sig-ME] Multinomial models
In-Reply-To: <40e66e0b0802071622v3df1c808j3baf311705ed7c2f@mail.gmail.com>
References: <40e66e0b0801301559l364c04f7j12ced4719dfcc618@mail.gmail.com><m0bq6snx13.fsf@gmail.com>
	<40e66e0b0802071622v3df1c808j3baf311705ed7c2f@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0802081159420.19550@orpheus.qimr.edu.au>

On Thu, 7 Feb 2008, Douglas Bates wrote:

> I should have been more explicit.  One can use any of the families
> defined in the stats package for R, including the quasi family.
>
> They could be extended but that would need a definition of inverse
> link, the derivative of the inverse link and the variance function for
> any new families that are to be incorporated.
>
Austin Frank asked

>> Unsurprisingly, I'm wondering whether the above change will enable me to
>> model a multinomial dependent variable using glmer.  Is this now
>> possible?

I can imagine a proportional odds/multiple threshold mixed model in lmer, 
but don't see how Gaussian distributed random effects could be included 
in a multinomial model without going to some type of a multivariate model.

To AF: May I ask how you currently deal with such data, LCA/LSA etc with 
latent categorical or continuous variables?

To DB: Will lmer eventually allow multivariate models and arbitrary 
distributions for the random effects (eg mixtures of normals, other 
parametric distributions)?  Are they already in there somewhere ;)

Cheers, David Duffy.

-- 
| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v



From bates at stat.wisc.edu  Fri Feb  8 18:59:26 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 8 Feb 2008 11:59:26 -0600
Subject: [R-sig-ME] Multinomial models
In-Reply-To: <Pine.LNX.4.64.0802081159420.19550@orpheus.qimr.edu.au>
References: <40e66e0b0801301559l364c04f7j12ced4719dfcc618@mail.gmail.com>
	<m0bq6snx13.fsf@gmail.com>
	<40e66e0b0802071622v3df1c808j3baf311705ed7c2f@mail.gmail.com>
	<Pine.LNX.4.64.0802081159420.19550@orpheus.qimr.edu.au>
Message-ID: <40e66e0b0802080959q255997e8vdc3f6cc22a820c45@mail.gmail.com>

On Feb 7, 2008 8:16 PM, David Duffy <David.Duffy at qimr.edu.au> wrote:
> On Thu, 7 Feb 2008, Douglas Bates wrote:
>
> > I should have been more explicit.  One can use any of the families
> > defined in the stats package for R, including the quasi family.
> >
> > They could be extended but that would need a definition of inverse
> > link, the derivative of the inverse link and the variance function for
> > any new families that are to be incorporated.
> >
> Austin Frank asked
>
> >> Unsurprisingly, I'm wondering whether the above change will enable me to
> >> model a multinomial dependent variable using glmer.  Is this now
> >> possible?

> I can imagine a proportional odds/multiple threshold mixed model in lmer,
> but don't see how Gaussian distributed random effects could be included
> in a multinomial model without going to some type of a multivariate model.

Thanks, David.  That is pretty much what I was thinking but I didn't
know how to articulate it or whether it is just my lack of experience
with the models that leaves me unable to imagine how they could be fit
into the framework.

> To AF: May I ask how you currently deal with such data, LCA/LSA etc with
> latent categorical or continuous variables?

> To DB: Will lmer eventually allow multivariate models and arbitrary
> distributions for the random effects (eg mixtures of normals, other
> parametric distributions)?  Are they already in there somewhere ;)

I can imagine incorporating multivariate models at some point but not
non-Gaussian distributions of the random effects.  A multivariate
Gaussian distribution of the random effects is welded into lmer at a
very low level.

The thing that allows lmer to fit difficult models, such as those with
crossed or partially crossed random effects, to very large data sets
is a common representation of the model and a set of C functions that
manipulate that representation.  The common representation is the
"mer" class.  The critical calculation performed on these structures
when fitting a model is the evaluation of the conditional modes of the
random effects.  The performance of lmer, glmer and nlmer hinges on
being able to evaluate the conditional modes rapidly.

There is a simplification used in lmer because you don't really need
the conditional modes - you only need some of the properties of the
conditional modes.  That is why the evaluation of the deviance is
performed in the C function mer_update_RX.

However for glmer and nlmer models you need to evaluate the
conditional modes explicitly, which is done in the C function
mer_update_u.  If you compare the structure of that code to the
pseudo-code in Appendix A3.1 of Bates and Watts (Wiley, 1988),
"Nonlinear Regression Analysis and Its Applications" you will see that
I am up to my old tricks - apparently I haven't learned much over the
past 25 years :-).  Scratch the surface of that code and you will find
an iterative, penalized nonlinear least squares optimization with
re-weighting in the "generalized" cases.

The representation of the distribution of the random effects, which is
encoded in the ST slot, a list of matrices, and the Gp slot, an
integer vector of "group pointers", and the penalized least squares
criterion satisfied by the conditional modes are derived from the
multivariate normal distribution of the random effects.  If you want
to change that you would pretty much need to start from scratch
redesigning and rewriting the software and that won't be done by me
:-)

I view the mer class and the C functions whose names start with mer_
as the underlying engine that allows fitting of a variety of models
involving random effects by evaluating the deviance, or the Laplace
approximation to it, given the data and values of the parameters.  You
can wrap that engine in code that allows a multivariate response but
you can't easily modify the distribution of the random effects.



From kedwards at ucdavis.edu  Sat Feb  9 22:21:36 2008
From: kedwards at ucdavis.edu (Kyle Edwards)
Date: Sat, 9 Feb 2008 13:21:36 -0800
Subject: [R-sig-ME] Conditional Akaike information with lmer
Message-ID: <823B2711-A31E-4490-9868-5B1C9B078E95@ucdavis.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080209/ace12c74/attachment.pl>

From hskaug at gmail.com  Tue Feb 12 13:31:49 2008
From: hskaug at gmail.com (H. Skaug)
Date: Tue, 12 Feb 2008 13:31:49 +0100
Subject: [R-sig-ME]  Multinomial models
Message-ID: <ed96c8240802120431x499a147cqf2f1f05775149b14@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080212/97f99853/attachment.pl>

From marcioestat at pop.com.br  Tue Feb 12 17:59:55 2008
From: marcioestat at pop.com.br (marcioestat at pop.com.br)
Date: Tue, 12 Feb 2008 14:59:55 -0200 (BRST)
Subject: [R-sig-ME] Anova Random Effect Nested
Message-ID: <39346.132.204.53.21.1202835595.squirrel@nwebmail.pop.com.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080212/cf237ec3/attachment.pl>

From bates at stat.wisc.edu  Tue Feb 12 19:10:48 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 12 Feb 2008 12:10:48 -0600
Subject: [R-sig-ME] Anova Random Effect Nested
In-Reply-To: <39346.132.204.53.21.1202835595.squirrel@nwebmail.pop.com.br>
References: <39346.132.204.53.21.1202835595.squirrel@nwebmail.pop.com.br>
Message-ID: <40e66e0b0802121010m1898a346m9e74098a2765734d@mail.gmail.com>

On Feb 12, 2008 10:59 AM,  <marcioestat at pop.com.br> wrote:

>  Hi listers,

> I am trying to do an anova with random
> effects and
> my data is nested...
> I didn't find what procedure
> for the random
> effects in anova... The AOV procedure doesn't tell
> about random effects,
> in fact there is no information about nested
> data too... I found the
> function LME, but I am not sure if I can use
> this function... Could
> anybody would send me any example code of an
> anova with random effect and
> nested data...

It is often easier if you could provide the list with a small example
data set and describe the type of model that you want to fit.  The lme
function in the nlme package can fit models with nested random
effects.  Depending on exactly the type of model that you want to fit
the lmer function in the lme4 package can be faster and more reliable
than lme.

One data set that may illustrate fitting nested random effects is the
Oats data in the nlme package (also provided in a somewhat simpler
form in the MEMSS package)

> library(lme4)
Loading required package: Matrix
Loading required package: lattice

Attaching package: 'Matrix'


	The following object(s) are masked from package:stats :

	 xtabs

> data(Oats, package = "MEMSS")
> str(Oats)
'data.frame':	72 obs. of  4 variables:
 $ Block  : Factor w/ 6 levels "I","II","III",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ Variety: Factor w/ 3 levels "Golden Rain",..: 3 3 3 3 1 1 1 1 2 2 ...
 $ nitro  : num  0 0.2 0.4 0.6 0 0.2 0.4 0.6 0 0.2 ...
 $ yield  : num  111 130 157 174 117 114 161 141 105 140 ...

The yield is the response for various varieties of oats planted at
different farms (the Block factor) and with different amounts of
nitrogen applied.  In agricultural experimental design we would say
that the farm is a blocking factor, the variety is the whole plot
factor and the level of nitrogen is the subplot factor.  We model
Block and the whole plot as nested random effects.

The enclosed plot of the data, produced by

> xyplot(yield ~ nitro | Block, Oats, groups = Variety, aspect = 'xy',
+        auto.key = list(columns = 3, lines = TRUE), type = "o")

shows whole plot differences but not strong trends for Variety.

To fit a model with Block and Variety within Block as random effects we use

> print(Om1 <- lmer(yield ~ nitro + (1|Block/Variety), Oats))
Linear mixed model fit by REML
Formula: yield ~ nitro + (1 | Block/Variety)
   Data: Oats
 AIC   BIC logLik deviance REMLdev
 603 614.4 -296.5    604.3     593
Random effects:
 Groups        Name        Variance Std.Dev.
 Variety:Block (Intercept) 121.11   11.005
 Block         (Intercept) 210.37   14.504
 Residual                  165.56   12.867
Number of obs: 72, groups: Variety.Block, 18; Block, 6

Fixed effects:
            Estimate Std. Error t value
(Intercept)   81.872      6.945   11.79
nitro         73.667      6.782   10.86

Correlation of Fixed Effects:
      (Intr)
nitro -0.293

The conditional modes (also called the BLUPs) of the random effects are

> ranef(Om1)
An object of class "ranef.mer"
[[1]]
                (Intercept)
Golden Rain:I     3.2331282
Golden Rain:II    4.9719015
Golden Rain:III  -8.0610704
Golden Rain:IV    6.4426777
Golden Rain:V     1.4235656
Golden Rain:VI   -5.6501375
Marvellous:I      0.6246351
Marvellous:II    10.9341715
Marvellous:III   15.6016885
Marvellous:IV    -3.2460109
Marvellous:V     -6.2155928
Marvellous:VI     8.3239327
Victory:I        10.4996447
Victory:II      -14.4054759
Victory:III     -11.2285263
Victory:IV       -5.8545041
Victory:V        -1.1849275
Victory:VI       -6.2091003

[[2]]
    (Intercept)
I     24.939661
II     2.606625
III   -6.406113
IV    -4.616819
V    -10.382321
VI    -6.141032
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Oats.pdf
Type: application/pdf
Size: 21114 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080212/eb8312c6/attachment.pdf>

From kedwards at ucdavis.edu  Wed Feb 13 19:28:11 2008
From: kedwards at ucdavis.edu (Kyle Edwards)
Date: Wed, 13 Feb 2008 10:28:11 -0800
Subject: [R-sig-ME] Conditional AIC with lmer
Message-ID: <377CBC82-F8D4-40EE-B8C5-5B5A1DD27F91@ucdavis.edu>

Hi all,

I posted on this topic last week, but someone pointed out to me that  
my post was not in plain text. Apologies for that error.

With a colleague, I have been trying to implement the Conditional AIC  
described by Vaida and Blanchard 2005 Biometrika, "Conditional Akaike  
information for mixed-effects models". This quantity is derived in a  
way analogous to the AIC, but is appropriate for scenarios where one  
is interested in the particular coefficient estimates for individual  
random effects. The formula for the asymptotic CAIC is given as

-2*log(likelihood of observed values, conditional on ML estimates of  
fixed effects and empirical Bayes estimates of random effects) + 2*K

where K = rho + 1, and rho = "effective degrees of freedom" = trace  
of the hat matrix mapping predicted values onto observed values.

After some thinking and some off-list advice, we have decided that  
appropriate code for CAIC is

CAIC <- function(model) {

	sigma <- attr(VarCorr(model), 'sc')
	observed <- attr(model, 'y')
	predicted <- fitted(model)	
	cond.loglik <- sum(dnorm(observed, predicted, sigma, log=TRUE))
	
	rho <- hatTrace(model)
	p <- length(fixef(model))
	N <- nrow(attr(model, 'X'))
	K.corr <- N*(N-p-1)*(rho+1)/((N-p)*(N-p-2)) + N*(p+1)/((N-p)*(N-p-2))
	
	CAIC <- -2*cond.loglik + 2*K.corr

	return(CAIC)

	}

where K.corr is the finite-sample correction for K, for ML model fits.

I am posting this so that 1) This code can be of use to any other  
souls in the statistical wilderness trying to do model selection with  
mixed models, and 2) So that wiser minds can point out any errors in  
our approach.

Thanks,

Kyle



From kari.ruohonen at utu.fi  Thu Feb 14 08:44:01 2008
From: kari.ruohonen at utu.fi (Kari Ruohonen)
Date: Thu, 14 Feb 2008 09:44:01 +0200
Subject: [R-sig-ME] problem in installing the alpha version of lme
Message-ID: <1202975041.11262.23.camel@wittgenstein>

Hi,

I tried just to install the alpha version of lme4 from R-forge with

install.packages("lme4", repos = "http://r-forge.r-project.org")

But I got an error saying:

Warning message:
In install.packages("lme4", repos = "http://r-forge.r-project.org") :
  package ?lme4? is not available

My session info is as follows:

> sessionInfo()
R version 2.6.2 (2008-02-08) 
i486-pc-linux-gnu 

locale:
LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods
base     

loaded via a namespace (and not attached):
[1] rcompgen_0.1-17 tools_2.6.2    

Thanks for your help.

regards, Kari



From p.dalgaard at biostat.ku.dk  Thu Feb 14 09:38:24 2008
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 14 Feb 2008 09:38:24 +0100
Subject: [R-sig-ME] problem in installing the alpha version of lme
In-Reply-To: <1202975041.11262.23.camel@wittgenstein>
References: <1202975041.11262.23.camel@wittgenstein>
Message-ID: <47B3FE00.9080406@biostat.ku.dk>

Kari Ruohonen wrote:
> Hi,
>
> I tried just to install the alpha version of lme4 from R-forge with
>
> install.packages("lme4", repos = "http://r-forge.r-project.org")
>
> But I got an error saying:
>
> Warning message:
> In install.packages("lme4", repos = "http://r-forge.r-project.org") :
>   package ?lme4? is not available
>
>   

Looks like the reason is that, er, the lme4 package is not available....

http://r-forge.r-project.org/src/contrib


Thing is, you can only get packages that way if they have passed the 
quality check, and for a package in alpha status, that is not a given. 
If you go to

http://r-forge.r-project.org/scm/?group_id=60

then you see two alternate downloads: A package snapshot and an SVN tree 
snapshot. The former currently leads to a build log that seems to have 
failed to create a vignette.

> My session info is as follows:
>
>   
>> sessionInfo()
>>     
> R version 2.6.2 (2008-02-08) 
> i486-pc-linux-gnu 
>
> locale:
> LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods
> base     
>
> loaded via a namespace (and not attached):
> [1] rcompgen_0.1-17 tools_2.6.2    
>
> Thanks for your help.
>
> regards, Kari
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From julien.martin2 at usherbrooke.ca  Thu Feb 14 16:02:47 2008
From: julien.martin2 at usherbrooke.ca (Julien)
Date: Thu, 14 Feb 2008 10:02:47 -0500
Subject: [R-sig-ME] Overdispersion,  AIC and mixed models
Message-ID: <000101c86f1a$aa1cb4e0$5a1ad284@Calypso>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080214/7f9ac97e/attachment.pl>

From lewin-koh.nicholas at gene.com  Thu Feb 14 01:36:27 2008
From: lewin-koh.nicholas at gene.com (Nicholas Lewin-Koh)
Date: Wed, 13 Feb 2008 16:36:27 -0800
Subject: [R-sig-ME] Problems withestimationg model in lmer
Message-ID: <006401c86ea1$a2cd6960$c50b2c0a@gne.windows.gene.com>

Consider the attached tumor growth data:
dat<-read.table("ListDat.txt",header=TRUE)
library(lme4)
fit<-lmer(tVol~Dose*poly(Day,2)+(Day|ID),dat)
coef(fit)
Error in coef(fit) : unable to align random and fixed effects
mcmcsamp(fit)
Error: no positive eigenvalues!
Error in t(.Call(mer_MCMCsamp, object, saveb, n, trans, verbose, deviance))
: 
  error in evaluating the argument 'x' in selecting a method for function
't'
detach(package:lme4)
library(nlme)
fit.lme<-lme(tVol~Dose*poly(Day,2),dat,random=~1+Day|ID)
coef(fit.lme)
summary(fit.lme)

works fine in lme. I think this model should be identified, is there an
estimation instability I am unaware of? My interest is in prediction not
inference, so I am not too concerned about the large number of parameters. I
am using the current R-forge version of lme4.

Thanks

Nicholas
Statistician, Genentech Inc.




 "Seek the company of those who seek the truth, and run away from those who
have found it." - Vaclav Havel
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: ListDat.txt
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080213/40f9860e/attachment.txt>

From bates at stat.wisc.edu  Thu Feb 14 16:14:39 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 14 Feb 2008 09:14:39 -0600
Subject: [R-sig-ME] problem in installing the alpha version of lme
In-Reply-To: <47B3FE00.9080406@biostat.ku.dk>
References: <1202975041.11262.23.camel@wittgenstein>
	<47B3FE00.9080406@biostat.ku.dk>
Message-ID: <40e66e0b0802140714s53c53213q65b48eca5c3e0309@mail.gmail.com>

Thanks for bringing it to my attention.  I'll correct it as soon as I
am back from teaching a class.

On Thu, Feb 14, 2008 at 2:38 AM, Peter Dalgaard
<p.dalgaard at biostat.ku.dk> wrote:
> Kari Ruohonen wrote:
>  > Hi,
>  >
>  > I tried just to install the alpha version of lme4 from R-forge with
>  >
>  > install.packages("lme4", repos = "http://r-forge.r-project.org")
>  >
>  > But I got an error saying:
>  >
>  > Warning message:
>  > In install.packages("lme4", repos = "http://r-forge.r-project.org") :
>  >   package 'lme4' is not available
>  >
>  >
>
>  Looks like the reason is that, er, the lme4 package is not available....
>
>  http://r-forge.r-project.org/src/contrib
>
>
>  Thing is, you can only get packages that way if they have passed the
>  quality check, and for a package in alpha status, that is not a given.
>  If you go to
>
>  http://r-forge.r-project.org/scm/?group_id=60
>
>  then you see two alternate downloads: A package snapshot and an SVN tree
>  snapshot. The former currently leads to a build log that seems to have
>  failed to create a vignette.
>
>
>  > My session info is as follows:
>  >
>  >
>  >> sessionInfo()
>  >>
>  > R version 2.6.2 (2008-02-08)
>  > i486-pc-linux-gnu
>  >
>  > locale:
>  > LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C
>  >
>  > attached base packages:
>  > [1] stats     graphics  grDevices utils     datasets  methods
>  > base
>  >
>  > loaded via a namespace (and not attached):
>  > [1] rcompgen_0.1-17 tools_2.6.2
>  >
>  > Thanks for your help.
>  >
>  > regards, Kari
>  >
>  > _______________________________________________
>  > R-sig-mixed-models at r-project.org mailing list
>  > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>  >
>
>
>  --
>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>   (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
>  ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
>
>
>
>  _______________________________________________
>  R-sig-mixed-models at r-project.org mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Thu Feb 14 16:19:51 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 14 Feb 2008 09:19:51 -0600
Subject: [R-sig-ME] Problems withestimationg model in lmer
In-Reply-To: <006401c86ea1$a2cd6960$c50b2c0a@gne.windows.gene.com>
References: <006401c86ea1$a2cd6960$c50b2c0a@gne.windows.gene.com>
Message-ID: <40e66e0b0802140719ydd3675dheff3b021250a3802@mail.gmail.com>

2008/2/13 Nicholas Lewin-Koh <lewin-koh.nicholas at gene.com>:
> Consider the attached tumor growth data:
>  dat<-read.table("ListDat.txt",header=TRUE)
>  library(lme4)
>  fit<-lmer(tVol~Dose*poly(Day,2)+(Day|ID),dat)
>  coef(fit)
>  Error in coef(fit) : unable to align random and fixed effects
>  mcmcsamp(fit)
>  Error: no positive eigenvalues!
>  Error in t(.Call(mer_MCMCsamp, object, saveb, n, trans, verbose, deviance))
>  :
>   error in evaluating the argument 'x' in selecting a method for function
>  't'

Could you send us the output of sessionInfo() please?  A call to coef
should not generate .Call(mer_MCMCsamp, ...)

The error message about "unable to align" will go away if you use

tVol ~ Dose*(Day + Day^2) + (Day|ID)

>  detach(package:lme4)
>  library(nlme)
>  fit.lme<-lme(tVol~Dose*poly(Day,2),dat,random=~1+Day|ID)
>  coef(fit.lme)
>  summary(fit.lme)
>
>  works fine in lme. I think this model should be identified, is there an
>  estimation instability I am unaware of? My interest is in prediction not
>  inference, so I am not too concerned about the large number of parameters. I
>  am using the current R-forge version of lme4.
>
>  Thanks
>
>  Nicholas
>  Statistician, Genentech Inc.
>
>
>
>
>   "Seek the company of those who seek the truth, and run away from those who
>  have found it." - Vaclav Havel
>
> _______________________________________________
>  R-sig-mixed-models at r-project.org mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



From lewin-koh.nicholas at gene.com  Thu Feb 14 17:10:07 2008
From: lewin-koh.nicholas at gene.com (Nicholas Lewin-Koh)
Date: Thu, 14 Feb 2008 08:10:07 -0800
Subject: [R-sig-ME] Problems withestimationg model in lmer
In-Reply-To: <40e66e0b0802140719ydd3675dheff3b021250a3802@mail.gmail.com>
References: <006401c86ea1$a2cd6960$c50b2c0a@gne.windows.gene.com>
	<40e66e0b0802140719ydd3675dheff3b021250a3802@mail.gmail.com>
Message-ID: <006e01c86f24$114772a0$c50b2c0a@gne.windows.gene.com>

Hi,
Mia culpa,
> sessionInfo()
R version 2.6.1 Patched (2007-12-16 r43718) 
i386-pc-mingw32 

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
States.1252;LC_MONETARY=English_United
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices datasets  utils     methods   base     

other attached packages:
[1] lme4_0.999375-3   Matrix_0.999375-4 lattice_0.17-4   

loaded via a namespace (and not attached):
[1] grid_2.6.1  mgcv_1.3-29 nlme_3.1-86
>

Nicholas

-----Original Message-----
From: dmbates at gmail.com [mailto:dmbates at gmail.com] On Behalf Of Douglas
Bates
Sent: Thursday, February 14, 2008 7:20 AM
To: Nicholas Lewin-Koh
Cc: r-sig-mixed-models at r-project.org; Bert Gunter
Subject: Re: [R-sig-ME] Problems withestimationg model in lmer

2008/2/13 Nicholas Lewin-Koh <lewin-koh.nicholas at gene.com>:
> Consider the attached tumor growth data:
>  dat<-read.table("ListDat.txt",header=TRUE)
>  library(lme4)
>  fit<-lmer(tVol~Dose*poly(Day,2)+(Day|ID),dat)
>  coef(fit)
>  Error in coef(fit) : unable to align random and fixed effects
>  mcmcsamp(fit)
>  Error: no positive eigenvalues!
>  Error in t(.Call(mer_MCMCsamp, object, saveb, n, trans, verbose,
deviance))
>  :
>   error in evaluating the argument 'x' in selecting a method for function
>  't'

Could you send us the output of sessionInfo() please?  A call to coef
should not generate .Call(mer_MCMCsamp, ...)

The error message about "unable to align" will go away if you use

tVol ~ Dose*(Day + Day^2) + (Day|ID)

>  detach(package:lme4)
>  library(nlme)
>  fit.lme<-lme(tVol~Dose*poly(Day,2),dat,random=~1+Day|ID)
>  coef(fit.lme)
>  summary(fit.lme)
>
>  works fine in lme. I think this model should be identified, is there an
>  estimation instability I am unaware of? My interest is in prediction not
>  inference, so I am not too concerned about the large number of
parameters. I
>  am using the current R-forge version of lme4.
>
>  Thanks
>
>  Nicholas
>  Statistician, Genentech Inc.
>
>
>
>
>   "Seek the company of those who seek the truth, and run away from those
who
>  have found it." - Vaclav Havel
>
> _______________________________________________
>  R-sig-mixed-models at r-project.org mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



From bates at stat.wisc.edu  Thu Feb 14 18:55:57 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 14 Feb 2008 11:55:57 -0600
Subject: [R-sig-ME] Fwd:  Problems withestimationg model in lmer
In-Reply-To: <40e66e0b0802140941v5f60acb2p2c769022a3e996b2@mail.gmail.com>
References: <006401c86ea1$a2cd6960$c50b2c0a@gne.windows.gene.com>
	<40e66e0b0802140719ydd3675dheff3b021250a3802@mail.gmail.com>
	<006e01c86f24$114772a0$c50b2c0a@gne.windows.gene.com>
	<002e01c86f29$a700f820$6501a8c0@gne.windows.gene.com>
	<006f01c86f2d$13c38010$c50b2c0a@gne.windows.gene.com>
	<40e66e0b0802140941v5f60acb2p2c769022a3e996b2@mail.gmail.com>
Message-ID: <40e66e0b0802140955u461f4328u256e1ac89859ab27@mail.gmail.com>

I meant to send a copy of this reply to the list.


---------- Forwarded message ----------
From: Douglas Bates <bates at stat.wisc.edu>
Date: Thu, Feb 14, 2008 at 11:41 AM
Subject: Re: [R-sig-ME] Problems withestimationg model in lmer
To: Nicholas Lewin-Koh <lewin-koh.nicholas at gene.com>
Cc: Bert Gunter <gunter.berton at gene.com>


On Thu, Feb 14, 2008 at 11:14 AM, Nicholas Lewin-Koh
 <lewin-koh.nicholas at gene.com> wrote:
 > Hi,
 >  Yes, and to follow up further, if I do
 >  > fit2<-lmer(tVol~Dose*(Day + I(Day^2))+(1|ID),dat,na.action=na.exclude)
 >  the coef(fit2) works, as your reply suggests.

 Thanks for checking that (and thanks for quietly correcting my writing
 Day^2 when I should have written I(Day^2)).  You have switched the
 random effect specification in this case from (Day|ID) to (1|ID).  The
 problem in the previous specification was that there isn't a
 fixed-effect coefficient named 'Day' when you specify the model as


 tVol ~ Dose*poly(Day, 2) + (Day|ID)

 The two columns in the matrix of random effects for the ID factor (the
 first, and only, component of the list returned by ranef) are labelled
 '(Intercept)' and 'Day'.  To calculate the linear coefficients for
 each group these must be aligned with elements of the fixed-effects
 vector.  It works for '(Intercept)' but not for 'Day' when you use
 poly(Day, 2).


 > But I still get the same error from
 >  > mcmcsamp(fit2)
 >
 > Error: no positive eigenvalues!
 >  Error in t(.Call(mer_MCMCsamp, object, saveb, n, trans, verbose, deviance))
 >  :
 >   error in evaluating the argument 'x' in selecting a method for function
 >  't'
 >  The first error comes from the check for a positive definite covariance,

 That is a check on the positive definite covariance matrix within the
 MCMC sampling scheme and is indeed a bug.  Thanks for the report.  I
 will disable mcmcsamp in the alpha test version until I can fix that
 bug.

 By the way, in your sessionInfo() output I see that you have both the
 nlme and lme4 packages loaded at the same time.  That is not a good
 idea.  They interfere with each other's definitions of some of
 accessor functions like VarCorr.



 >  But I don't think that is the source of the problem. Let me know if there is
 >  any other information I can send.
 >
 >  Nicholas
 >
 >
 >
 >  -----Original Message-----
 >  From: Bert Gunter [mailto:gunter.berton at gene.com]
 >  Sent: Thursday, February 14, 2008 8:50 AM
 >  To: 'Nicholas Lewin-Koh'; 'Douglas Bates'
 >  Subject: RE: [R-sig-ME] Problems withestimationg model in lmer
 >
 >  Doug. Note that mcmcsamp() generated .Call(mer_MCMCsamp), not coef().
 >
 >  -- Bert
 >
 >  -------Original Message-----
 >  From: dmbates at gmail.com [mailto:dmbates at gmail.com] On Behalf Of Douglas
 >  Bates
 >  Sent: Thursday, February 14, 2008 7:20 AM
 >  To: Nicholas Lewin-Koh
 >  Cc: r-sig-mixed-models at r-project.org; Bert Gunter
 >  Subject: Re: [R-sig-ME] Problems withestimationg model in lmer
 >
 >  2008/2/13 Nicholas Lewin-Koh <lewin-koh.nicholas at gene.com>:
 >  > Consider the attached tumor growth data:
 >  >  dat<-read.table("ListDat.txt",header=TRUE)
 >  >  library(lme4)
 >  >  fit<-lmer(tVol~Dose*poly(Day,2)+(Day|ID),dat)
 >  >  coef(fit)
 >  >  Error in coef(fit) : unable to align random and fixed effects
 >  >  mcmcsamp(fit)
 >  >  Error: no positive eigenvalues!
 >  >  Error in t(.Call(mer_MCMCsamp, object, saveb, n, trans, verbose,
 >  deviance))
 >  >  :
 >  >   error in evaluating the argument 'x' in selecting a method for function
 >  >  't'
 >
 >  Could you send us the output of sessionInfo() please?  A call to coef
 >  should not generate .Call(mer_MCMCsamp, ...)
 >
 >  The error message about "unable to align" will go away if you use
 >
 >  tVol ~ Dose*(Day + Day^2) + (Day|ID)
 >
 >  >  detach(package:lme4)
 >  >  library(nlme)
 >  >  fit.lme<-lme(tVol~Dose*poly(Day,2),dat,random=~1+Day|ID)
 >  >  coef(fit.lme)
 >  >  summary(fit.lme)
 >  >
 >  >  works fine in lme. I think this model should be identified, is there an
 >  >  estimation instability I am unaware of? My interest is in prediction not
 >  >  inference, so I am not too concerned about the large number of
 >  parameters. I
 >  >  am using the current R-forge version of lme4.
 >  >
 >  >  Thanks
 >  >
 >  >  Nicholas
 >  >  Statistician, Genentech Inc.
 >  >
 >  >
 >  >
 >  >
 >  >   "Seek the company of those who seek the truth, and run away from those
 >  who
 >  >  have found it." - Vaclav Havel
 >  >
 >  > _______________________________________________
 >  >  R-sig-mixed-models at r-project.org mailing list
 >  >  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
 >  >
 >  >
 >
 >
 >
 >



From bates at stat.wisc.edu  Thu Feb 14 19:17:32 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 14 Feb 2008 12:17:32 -0600
Subject: [R-sig-ME] Problems withestimationg model in lmer
In-Reply-To: <006401c86ea1$a2cd6960$c50b2c0a@gne.windows.gene.com>
References: <006401c86ea1$a2cd6960$c50b2c0a@gne.windows.gene.com>
Message-ID: <40e66e0b0802141017i1786e910g624ddf1ca52aa7bc@mail.gmail.com>

Thanks for sending the data with your report,  Nicholas.  I found it
very interesting, especially as a group of us are conducting a reading
course using Deepayan Sarkar's forthcoming book "Lattice: Multivariate
Data Visualization with R" (Springer, 2008).  (The book is due out
next month - Deepayan was kind enough to let us use a preprint copy
for the course.)  I enclose a plot of these data and the R code that
generates the plot.  It is interesting because it shows a thresholding
kind of behavior.  There is not much effect for dose until about 1.25
mg/kg.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: tVol.pdf
Type: application/pdf
Size: 43889 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080214/40c07f6b/attachment.pdf>

From lewin-koh.nicholas at gene.com  Thu Feb 14 19:35:03 2008
From: lewin-koh.nicholas at gene.com (Nicholas Lewin-Koh)
Date: Thu, 14 Feb 2008 10:35:03 -0800
Subject: [R-sig-ME] Problems withestimationg model in lmer
In-Reply-To: <40e66e0b0802141017i1786e910g624ddf1ca52aa7bc@mail.gmail.com>
References: <006401c86ea1$a2cd6960$c50b2c0a@gne.windows.gene.com>
	<40e66e0b0802141017i1786e910g624ddf1ca52aa7bc@mail.gmail.com>
Message-ID: <007801c86f38$505441d0$c50b2c0a@gne.windows.gene.com>

Hi Doug,
Actually the effect is subtle. If you look at slice by day 
and look at the dose response curves from the model there is 
a small effect at small doses in the slope, as the dose increases there is 
actual suppression.

By the way, even though I tried to strip off anything that might be
informative about which compound this is, please understand that this is
company data. I would ask that it not be distributed beyond your class. I do
not have the authority to ok its release to a broader audience.

Thanks for your understanding
Nicholas

-----Original Message-----
From: dmbates at gmail.com [mailto:dmbates at gmail.com] On Behalf Of Douglas
Bates
Sent: Thursday, February 14, 2008 10:18 AM
To: Nicholas Lewin-Koh
Cc: r-sig-mixed-models at r-project.org; Bert Gunter;
lattice-seminar at cs.wisc.edu
Subject: Re: [R-sig-ME] Problems withestimationg model in lmer

Thanks for sending the data with your report,  Nicholas.  I found it
very interesting, especially as a group of us are conducting a reading
course using Deepayan Sarkar's forthcoming book "Lattice: Multivariate
Data Visualization with R" (Springer, 2008).  (The book is due out
next month - Deepayan was kind enough to let us use a preprint copy
for the course.)  I enclose a plot of these data and the R code that
generates the plot.  It is interesting because it shows a thresholding
kind of behavior.  There is not much effect for dose until about 1.25
mg/kg.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: doseresp.pdf
Type: application/pdf
Size: 26176 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080214/ba4e1885/attachment.pdf>

From bates at stat.wisc.edu  Thu Feb 14 20:46:25 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 14 Feb 2008 13:46:25 -0600
Subject: [R-sig-ME] problem in installing the alpha version of lme
In-Reply-To: <40e66e0b0802140714s53c53213q65b48eca5c3e0309@mail.gmail.com>
References: <1202975041.11262.23.camel@wittgenstein>
	<47B3FE00.9080406@biostat.ku.dk>
	<40e66e0b0802140714s53c53213q65b48eca5c3e0309@mail.gmail.com>
Message-ID: <40e66e0b0802141146n3b22d3e8j10bcc9e0698202f1@mail.gmail.com>

I have just committed several changes to the SVN archive for version
0.999375-4, which passes the quality checks on my machine at least.  I
hope that a Windows binary will be successfully built tonight.

On Thu, Feb 14, 2008 at 9:14 AM, Douglas Bates <bates at stat.wisc.edu> wrote:
> Thanks for bringing it to my attention.  I'll correct it as soon as I
>  am back from teaching a class.
>
>
>
>  On Thu, Feb 14, 2008 at 2:38 AM, Peter Dalgaard
>  <p.dalgaard at biostat.ku.dk> wrote:
>  > Kari Ruohonen wrote:
>  >  > Hi,
>  >  >
>  >  > I tried just to install the alpha version of lme4 from R-forge with
>  >  >
>  >  > install.packages("lme4", repos = "http://r-forge.r-project.org")
>  >  >
>  >  > But I got an error saying:
>  >  >
>  >  > Warning message:
>  >  > In install.packages("lme4", repos = "http://r-forge.r-project.org") :
>  >  >   package 'lme4' is not available
>  >  >
>  >  >
>  >
>  >  Looks like the reason is that, er, the lme4 package is not available....
>  >
>  >  http://r-forge.r-project.org/src/contrib
>  >
>  >
>  >  Thing is, you can only get packages that way if they have passed the
>  >  quality check, and for a package in alpha status, that is not a given.
>  >  If you go to
>  >
>  >  http://r-forge.r-project.org/scm/?group_id=60
>  >
>  >  then you see two alternate downloads: A package snapshot and an SVN tree
>  >  snapshot. The former currently leads to a build log that seems to have
>  >  failed to create a vignette.
>  >
>  >
>  >  > My session info is as follows:
>  >  >
>  >  >
>  >  >> sessionInfo()
>  >  >>
>  >  > R version 2.6.2 (2008-02-08)
>  >  > i486-pc-linux-gnu
>  >  >
>  >  > locale:
>  >  > LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C
>  >  >
>  >  > attached base packages:
>  >  > [1] stats     graphics  grDevices utils     datasets  methods
>  >  > base
>  >  >
>  >  > loaded via a namespace (and not attached):
>  >  > [1] rcompgen_0.1-17 tools_2.6.2
>  >  >
>  >  > Thanks for your help.
>  >  >
>  >  > regards, Kari
>  >  >
>  >  > _______________________________________________
>  >  > R-sig-mixed-models at r-project.org mailing list
>  >  > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>  >  >
>  >
>  >
>  >  --
>  >    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>  >   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  >   (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
>  >  ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
>  >
>  >
>  >
>  >  _______________________________________________
>  >  R-sig-mixed-models at r-project.org mailing list
>  >  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>  >
>



From kari.ruohonen at utu.fi  Fri Feb 15 07:18:44 2008
From: kari.ruohonen at utu.fi (Kari Ruohonen)
Date: Fri, 15 Feb 2008 08:18:44 +0200
Subject: [R-sig-ME] problem in installing the alpha version of lme
In-Reply-To: <40e66e0b0802141146n3b22d3e8j10bcc9e0698202f1@mail.gmail.com>
References: <1202975041.11262.23.camel@wittgenstein>
	<47B3FE00.9080406@biostat.ku.dk>
	<40e66e0b0802140714s53c53213q65b48eca5c3e0309@mail.gmail.com>
	<40e66e0b0802141146n3b22d3e8j10bcc9e0698202f1@mail.gmail.com>
Message-ID: <1203056324.7040.5.camel@wittgenstein>

Thanks for that. Looks like it works again.

regards, Kari

On Thu, 2008-02-14 at 13:46 -0600, Douglas Bates wrote:
> I have just committed several changes to the SVN archive for version
> 0.999375-4, which passes the quality checks on my machine at least.  I
> hope that a Windows binary will be successfully built tonight.
> 
> On Thu, Feb 14, 2008 at 9:14 AM, Douglas Bates <bates at stat.wisc.edu> wrote:
> > Thanks for bringing it to my attention.  I'll correct it as soon as I
> >  am back from teaching a class.
> >
> >
> >
> >  On Thu, Feb 14, 2008 at 2:38 AM, Peter Dalgaard
> >  <p.dalgaard at biostat.ku.dk> wrote:
> >  > Kari Ruohonen wrote:
> >  >  > Hi,
> >  >  >
> >  >  > I tried just to install the alpha version of lme4 from R-forge with
> >  >  >
> >  >  > install.packages("lme4", repos = "http://r-forge.r-project.org")
> >  >  >
> >  >  > But I got an error saying:
> >  >  >
> >  >  > Warning message:
> >  >  > In install.packages("lme4", repos = "http://r-forge.r-project.org") :
> >  >  >   package 'lme4' is not available
> >  >  >
> >  >  >
> >  >
> >  >  Looks like the reason is that, er, the lme4 package is not available....
> >  >
> >  >  http://r-forge.r-project.org/src/contrib
> >  >
> >  >
> >  >  Thing is, you can only get packages that way if they have passed the
> >  >  quality check, and for a package in alpha status, that is not a given.
> >  >  If you go to
> >  >
> >  >  http://r-forge.r-project.org/scm/?group_id=60
> >  >
> >  >  then you see two alternate downloads: A package snapshot and an SVN tree
> >  >  snapshot. The former currently leads to a build log that seems to have
> >  >  failed to create a vignette.
> >  >
> >  >
> >  >  > My session info is as follows:
> >  >  >
> >  >  >
> >  >  >> sessionInfo()
> >  >  >>
> >  >  > R version 2.6.2 (2008-02-08)
> >  >  > i486-pc-linux-gnu
> >  >  >
> >  >  > locale:
> >  >  > LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C
> >  >  >
> >  >  > attached base packages:
> >  >  > [1] stats     graphics  grDevices utils     datasets  methods
> >  >  > base
> >  >  >
> >  >  > loaded via a namespace (and not attached):
> >  >  > [1] rcompgen_0.1-17 tools_2.6.2
> >  >  >
> >  >  > Thanks for your help.
> >  >  >
> >  >  > regards, Kari
> >  >  >
> >  >  > _______________________________________________
> >  >  > R-sig-mixed-models at r-project.org mailing list
> >  >  > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >  >  >
> >  >
> >  >
> >  >  --
> >  >    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
> >  >   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
> >  >   (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> >  >  ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
> >  >
> >  >
> >  >
> >  >  _______________________________________________
> >  >  R-sig-mixed-models at r-project.org mailing list
> >  >  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >  >
> >



From pvanmantgem at usgs.gov  Fri Feb 15 20:38:24 2008
From: pvanmantgem at usgs.gov (Phillip J van Mantgem)
Date: Fri, 15 Feb 2008 11:38:24 -0800
Subject: [R-sig-ME] Modeling rates with unequally spaced census intervals
Message-ID: <OF97A7C159.4AE02D70-ON882573F0.006BE299-882573F0.006BE2F3@usgs.gov>


Dear R-sig-mixed-modelers:

I?m wondering how I would write a model to estimate changes in an average
rate among groups with unequally spaced census intervals.

I am interested in modeling changes in survivorship rates by calendar year,
where the census interval has an exponential effect on survival (i.e., p =
annual survival, and p^census interval).

# ---------------------------------------------------------------------
# An example, modifying the cbpp dataset provided in the lme4 package...

library(lme4)

cbpp2 <- cbpp

# treating the period as annual measurements
cbpp2$equal.year <- as.numeric(cbpp$period)

# unequally spaced measurements
cbpp2$unequal.year <- c( c(1,3,7,10), c(1,5,10), rep(c(1,4,6,8), 5),
c(1), rep(c(1,3,7,10), 7))

m2 <- lmer(incidence / size ~ period + (1 | herd), weights = size,
family = binomial, data = cbpp2)

m3 <- lmer(incidence / size ~ equal.year + (1 | herd), weights = size,
family = binomial, data = cbpp2)

m4 <- lmer(incidence / size ~ unequal.year + (1 | herd), weights = size,
family = binomial, data = cbpp2)
# ---------------------------------------------------------------------

I don't think that the model specification for m4 is written so that I
could interpret the fixed effect for "unequal.year" as showing the average
annual change to the incidence rate. How would I do this?

Many thanks,
Phil

Phillip van Mantgem
USGS Western Ecological Research Center
Sequoia and Kings Canyon Field Station
47050 Generals Highway #4
Three Rivers, CA  93271-9651     USA
email: pvanmantgem at usgs.gov

From msfeno at yahoo.com.ar  Sat Feb 16 15:34:46 2008
From: msfeno at yahoo.com.ar (Maria Silvina Fenoglio)
Date: Sat, 16 Feb 2008 11:34:46 -0300 (ART)
Subject: [R-sig-ME] testing random effect
Message-ID: <676019.78339.qm@web53609.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080216/216e9b8e/attachment.pl>

From bates at stat.wisc.edu  Sat Feb 16 16:08:52 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 16 Feb 2008 09:08:52 -0600
Subject: [R-sig-ME] testing random effect
In-Reply-To: <676019.78339.qm@web53609.mail.re2.yahoo.com>
References: <676019.78339.qm@web53609.mail.re2.yahoo.com>
Message-ID: <40e66e0b0802160708g69a40eb1h867563d99a77d8c3@mail.gmail.com>

2008/2/16 Maria Silvina Fenoglio <msfeno at yahoo.com.ar>:
> Hi all,
>   I am trying to test the significance of the random effect in the following model but I didn't find the way to do that.
> I think that I can do it with LRT, removing the random effect and comparing both models, but an error appears...
>   Can anybody help me with that?
>
>   mod<-lmer(Riqsp~Distpza+logtam+logaisl+Densmin+(1|Sitio),sil,family=poisson,method="ML")

Could you give us a bit more detail, please?  How did you try to
perform the likelihood ratio test?

If possible, could you make your data available so we can check the
fits against different versions of the lme4 package?



From DAfshartous at med.miami.edu  Sun Feb 17 23:47:31 2008
From: DAfshartous at med.miami.edu (Afshartous, David)
Date: Sun, 17 Feb 2008 17:47:31 -0500
Subject: [R-sig-ME] testing random effect
References: <676019.78339.qm@web53609.mail.re2.yahoo.com>
Message-ID: <26E98BC2373DC14FA7432179C6CF7B620420D4@MEDEX07.ad.med.miami.edu>


If you are testing multiple random effects, the article below might be of interest:
Afshartous, David, and Wolf, Michael (2007). `` Avoiding Data Snooping in Multilevel and Mixed Effects Models,'' Journal of the Royal Statistical Society - Series A, 170: 1035-1059.


-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org on behalf of Maria Silvina Fenoglio
Sent: Sat 2/16/2008 9:34 AM
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] testing random effect
 
Hi all, 
  I am trying to test the significance of the random effect in the following model but I didn't find the way to do that.
I think that I can do it with LRT, removing the random effect and comparing both models, but an error appears...
  Can anybody help me with that?
   
  mod<-lmer(Riqsp~Distpza+logtam+logaisl+Densmin+(1|Sitio),sil,family=poisson,method="ML")

  Thanks,
  Silvina Fenoglio


Biol. Mar?a Silvina Fenoglio
Centro de Investigaciones Entomol?gicas
de C?rdoba
Edificio de Investigaciones Biol?gicas y Tecnol?gicas
Fac. de Cs. Exactas, F?s. y Naturales- 
Universidad Nacional De C?rdoba
Av. V?lez Sarsfield 1611-(X5016GCA)
C?rdoba-ARGENTINA
TEL:00-54-351-4334141/4334152 int.418
       
---------------------------------

Tarjeta de cr?dito Yahoo! de Banco Supervielle.Solicit? tu nueva Tarjeta de cr?dito. De tu PC directo a tu casa. 
 Visit? www.tuprimeratarjeta.com.ar
	[[alternative HTML version deleted]]



From sundar.dorai-raj at pdf.com  Mon Feb 18 17:31:09 2008
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 18 Feb 2008 08:31:09 -0800
Subject: [R-sig-ME] testing random effect
In-Reply-To: <40e66e0b0802160708g69a40eb1h867563d99a77d8c3@mail.gmail.com>
References: <676019.78339.qm@web53609.mail.re2.yahoo.com>
	<40e66e0b0802160708g69a40eb1h867563d99a77d8c3@mail.gmail.com>
Message-ID: <47B9B2CD.703@pdf.com>



Douglas Bates said the following on 2/16/2008 7:08 AM:
> 2008/2/16 Maria Silvina Fenoglio <msfeno at yahoo.com.ar>:
>> Hi all,
>>   I am trying to test the significance of the random effect in the following model but I didn't find the way to do that.
>> I think that I can do it with LRT, removing the random effect and comparing both models, but an error appears...
>>   Can anybody help me with that?
>>
>>   mod<-lmer(Riqsp~Distpza+logtam+logaisl+Densmin+(1|Sitio),sil,family=poisson,method="ML")
> 
> Could you give us a bit more detail, please?  How did you try to
> perform the likelihood ratio test?
> 
> If possible, could you make your data available so we can check the
> fits against different versions of the lme4 package?
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


Hi, Prof. Bates.

Here's an example that may help. "sessionInfo" at bottom.

library(lme4)
set.seed(1)
b <- rnorm(10, sd = 0.5)
z <- factor(rep(1:10, each = 10))
eta <- 1 + b[z]
y <- rpois(100, exp(eta))

## fit glmm model
(fit <- lmer(y ~ 1 + (1 | z), family = poisson, method = "ML"))
## fit null model without random effect
(fit0 <- glm(y ~ 1, family = poisson))

## this fails
anova(fit, fit0)
# Error in FUN(X[[1L]], ...) :
#   no slot of name "call" for this object of class "glm"
# In addition: Warning message:
# In logLik.glm(X[[2L]], ...) : extra arguments discarded

## compare log-likelihoods
(ll1.a <- logLik(fit))
## 'log Lik.' -65.58766 (df=2)
## the latter appears to be missing a constant
## making it impossible to compare to the following
(ll0.a <- logLik(fit0))
## 'log Lik.' -1164.184 (df=1)
## compute manually
(ll1.b <- sum(dpois(y, exp(fitted(fit)), log = TRUE)))
## [1] -187.6246
(ll0.b <- sum(dpois(y, fitted(fit0), log = TRUE)))
## [1] -1164.184

(LR <- -2 * (ll0.b - ll1.b))
## [1] 1953.119

## sorry haven't upgraded to 2.6.2 yet
 > sessionInfo()
R version 2.6.1 (2007-11-26)
i386-pc-mingw32

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United 
States.1252;LC_MONETARY=English_United 
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] lme4_0.99875-9    Matrix_0.999375-4 lattice_0.17-4

loaded via a namespace (and not attached):
[1] grid_2.6.1  tools_2.6.1
 >



From demont at access.uzh.ch  Tue Feb 19 16:01:53 2008
From: demont at access.uzh.ch (demont at access.uzh.ch)
Date: Tue, 19 Feb 2008 16:01:53 +0100
Subject: [R-sig-ME] =?iso-8859-1?q?How_to_=93weight=94_measurements_=28fam?=
 =?iso-8859-1?q?ily_means=29=3F?=
Message-ID: <web-11848736@idmailbe1b.unizh.ch>

Dear all,

I have recently started working with the lme4 package and its lmer function. 
My data file is based on family means. I do not have the row data for the 
individuals but I do know for every family on how many individuals the 
family means are based (i.e. how many individuals per family I have 
measured).
How can I fit a mixed-effects model to this data with lmer so that my family 
means are weighted with an appropriate weighting factor (i.e. weights my 
family means according to the number of offspring per family measured for 
this family = more weight for family means that are based upon many 
individuals, because I have more confidence in this measurements)?? Does 
this involve using the optional vector ?weights???
Despite going through the reference manual, the vignettes and the 
R-sig-mixed-models archives I did not find a clear answer to this very 
fundamental problem and would hence be deeply grateful for a specific and 
detailed answer!

Sincerely,
Marco





**************************************************************
Marco Demont
Zoological Museum
University of Zuerich
Winterthurerstrasse 190
CH-8057 Zuerich
Switzerland

Tel.: +41 44 635 47 79
demont at access.uzh.ch
demont at gmx.ch
www.unizh.ch/zoolmus/zmneu/englisch/forschung_e/demont_marco_e.html



From bates at stat.wisc.edu  Tue Feb 19 16:11:18 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 19 Feb 2008 09:11:18 -0600
Subject: [R-sig-ME]
	=?windows-1252?q?How_to_=93weight=94_measurements_=28f?=
	=?windows-1252?q?amily_means=29=3F?=
In-Reply-To: <web-11848736@idmailbe1b.unizh.ch>
References: <web-11848736@idmailbe1b.unizh.ch>
Message-ID: <40e66e0b0802190711l4ce8b77dpc3174316c6dcfdeb@mail.gmail.com>

On Feb 19, 2008 9:01 AM,  <demont at access.uzh.ch> wrote:
> Dear all,

> I have recently started working with the lme4 package and its lmer function.
> My data file is based on family means. I do not have the row data for the
> individuals but I do know for every family on how many individuals the
> family means are based (i.e. how many individuals per family I have
> measured).

> How can I fit a mixed-effects model to this data with lmer so that my family
> means are weighted with an appropriate weighting factor (i.e. weights my
> family means according to the number of offspring per family measured for
> this family = more weight for family means that are based upon many
> individuals, because I have more confidence in this measurements)?? Does
> this involve using the optional vector "weights"??

Yes.  You should be able to use the vector of family sizes as the
argument to weights - at least in the development version - the one
accessible via

install.packages("lme4", repos = "http://r-forge.r-project.org")

Please check which version of the lme4 package you are using with
sessionInfo() and report that if you encounter difficulties.

> Despite going through the reference manual, the vignettes and the
> R-sig-mixed-models archives I did not find a clear answer to this very
> fundamental problem and would hence be deeply grateful for a specific and
> detailed answer!

Yes, the current documentation for the lme4 package is, shall we say,
"casual".  I'm working on it.



From njbisaac at googlemail.com  Wed Feb 20 18:18:45 2008
From: njbisaac at googlemail.com (Nick Isaac)
Date: Wed, 20 Feb 2008 17:18:45 +0000
Subject: [R-sig-ME] Crash fitting lmer in lme4_0.999375-4
Message-ID: <a072ed700802200918v1dfd65f1m48f4e91b16b85bab@mail.gmail.com>

Dear all,

I just installed lme4_0.999375-4 and am sad to report unexpected behaviour
the first time I ran lmer.

> sessionInfo()
R version 2.6.2 (2008-02-08)
i386-apple-darwin8.10.1

locale:
en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] lme4_0.999375-4   Matrix_0.999375-4 lattice_0.17-4

loaded via a namespace (and not attached):
[1] grid_2.6.2



The dataset is attached. A call to:

data<-read.table("ESW_080220.txt",sep="\t",header=T)
data$W<- 5-log(1+data$CV)
full1.w.mm <- lmer(ESW ~ Wingspan * Dataset * VCS + (1|Site) + (1|Species),
data=data, weights=W)



Produces a crash with the following message:

 *** caught bus error ***
address 0x0, cause 'non-existent physical address'

Traceback:
 1: .Call(mer_optimize, ans, verbose)
 2: mer_finalize(ans, verbose)
 3: lmer(ESW ~ Wingspan * Dataset * VCS + (1 | Site) + (1 | Species),
data = data, weights = W)



I repeated the exercise on another machine, running  the version on CRAN:


> sessionInfo()
R version 2.6.2 (2008-02-08)
i386-apple-darwin8.10.1

locale:
en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] lme4_0.99875-9    Matrix_0.999375-4 lattice_0.17-4




The lmer fit the model with a warning:

Warning message:
In .local(x, ..., value) :
  Estimated variance for factor 'Site' is effectively zero



The warning reflects the fact that this model is rather over-specified.
However, the behaviour in the development version is rather unexpected.
Best wishes, Nick Isaac
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: ESW_080220.txt
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080220/41231288/attachment.txt>

From xiaozh at WPI.EDU  Wed Feb 20 18:54:01 2008
From: xiaozh at WPI.EDU (Zhong, Xiao)
Date: Wed, 20 Feb 2008 12:54:01 -0500
Subject: [R-sig-ME] one question about "loess"
Message-ID: <80714657CA27844AAEC7E733FD3FF9760A0A3ED440@EXCHANGEMAIL.admin.wpi.edu>

Dear all,

I tried to use "loess" to fit some longitudinal data as:

**********************************************************************
loess.fit<-by(mcas5.square.one.knot,mcas5.square.one.knot$skills,function(x) loess(response~month_elapsed,data=x,span=1,degree=1))
**********************************************************************

Then I tried to draw the loess fitting lines for each "skill". But for the FIRST "skill", I got the following error message as:

*************************************************
> time<-(260:800)/100
> xx<-seq(min(time),max(time),0.1)
> yy<-predict(loess.fit[[1]],data.frame(time=xx))
Error in predict(loess.fit[[1]], data.frame(time = xx)) :
        no applicable method for "predict"
> yy2<-predict(loess.fit.qua[[1]],data.frame(time=xx))
Error in predict(loess.fit.qua[[1]], data.frame(time = xx)) :
        no applicable method for "predict"
***************************************************

Actually, I can still get the loess curve corresponding to this skill, but I don't know if I can trust it... Could any of you please give me a clue of what's the problem of my code?

Thanks very much,

Xiao




________________________________________
From: r-sig-mixed-models-bounces at r-project.org [r-sig-mixed-models-bounces at r-project.org] On Behalf Of demont at access.uzh.ch [demont at access.uzh.ch]
Sent: Tuesday, February 19, 2008 10:01 AM
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] How to ?weight? measurements (family means)?

Dear all,

I have recently started working with the lme4 package and its lmer function.
My data file is based on family means. I do not have the row data for the
individuals but I do know for every family on how many individuals the
family means are based (i.e. how many individuals per family I have
measured).
How can I fit a mixed-effects model to this data with lmer so that my family
means are weighted with an appropriate weighting factor (i.e. weights my
family means according to the number of offspring per family measured for
this family = more weight for family means that are based upon many
individuals, because I have more confidence in this measurements)?? Does
this involve using the optional vector ?weights???
Despite going through the reference manual, the vignettes and the
R-sig-mixed-models archives I did not find a clear answer to this very
fundamental problem and would hence be deeply grateful for a specific and
detailed answer!

Sincerely,
Marco





**************************************************************
Marco Demont
Zoological Museum
University of Zuerich
Winterthurerstrasse 190
CH-8057 Zuerich
Switzerland

Tel.: +41 44 635 47 79
demont at access.uzh.ch
demont at gmx.ch
www.unizh.ch/zoolmus/zmneu/englisch/forschung_e/demont_marco_e.html

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From bates at stat.wisc.edu  Thu Feb 21 00:47:38 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 20 Feb 2008 17:47:38 -0600
Subject: [R-sig-ME] Crash fitting lmer in lme4_0.999375-4
In-Reply-To: <a072ed700802200918v1dfd65f1m48f4e91b16b85bab@mail.gmail.com>
References: <a072ed700802200918v1dfd65f1m48f4e91b16b85bab@mail.gmail.com>
Message-ID: <40e66e0b0802201547m6fc4bac1m8c8b31849b45cb32@mail.gmail.com>

Thanks for the report Nick and thanks for including the data so that
we can reproduce the problem.

I don't think I will have time to look at it until next week.  I am
travelling for the rest of the week (and, naturally, still need to
finish my presentation).

On Feb 20, 2008 11:18 AM, Nick Isaac <njbisaac at googlemail.com> wrote:
> Dear all,
>
> I just installed lme4_0.999375-4 and am sad to report unexpected behaviour
> the first time I ran lmer.
>
> > sessionInfo()
> R version 2.6.2 (2008-02-08)
> i386-apple-darwin8.10.1
>
> locale:
> en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] lme4_0.999375-4   Matrix_0.999375-4 lattice_0.17-4
>
> loaded via a namespace (and not attached):
> [1] grid_2.6.2
>
>
>
> The dataset is attached. A call to:
>
> data<-read.table("ESW_080220.txt",sep="\t",header=T)
> data$W<- 5-log(1+data$CV)
> full1.w.mm <- lmer(ESW ~ Wingspan * Dataset * VCS + (1|Site) + (1|Species),
> data=data, weights=W)
>
>
>
> Produces a crash with the following message:
>
>  *** caught bus error ***
> address 0x0, cause 'non-existent physical address'
>
> Traceback:
>  1: .Call(mer_optimize, ans, verbose)
>  2: mer_finalize(ans, verbose)
>  3: lmer(ESW ~ Wingspan * Dataset * VCS + (1 | Site) + (1 | Species),
> data = data, weights = W)
>
>
>
> I repeated the exercise on another machine, running  the version on CRAN:
>
>
> > sessionInfo()
> R version 2.6.2 (2008-02-08)
> i386-apple-darwin8.10.1
>
> locale:
> en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] lme4_0.99875-9    Matrix_0.999375-4 lattice_0.17-4
>
>
>
>
> The lmer fit the model with a warning:
>
> Warning message:
> In .local(x, ..., value) :
>   Estimated variance for factor 'Site' is effectively zero
>
>
>
> The warning reflects the fact that this model is rather over-specified.
> However, the behaviour in the development version is rather unexpected.
> Best wishes, Nick Isaac
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



From jebyrnes at ucdavis.edu  Fri Feb 22 01:43:50 2008
From: jebyrnes at ucdavis.edu (Jarrett Byrnes)
Date: Thu, 21 Feb 2008 16:43:50 -0800
Subject: [R-sig-ME] Strange mcmcsamp issue
Message-ID: <3E66F350-1147-4E24-956E-6668B0B13B2D@ucdavis.edu>

I'm attempting to pull out the simple effects from a mixed model with  
two crossed treatments.  The model structure is such that

a.lmer<-lmer(response ~ trta*trtb+(1|pot))

In the experiment, I have an array of pots.  Each pot has a type of  
treatment A applied to it.  Within the pot, there are two types of  
treatment B that are applied, one on either side.  I am using a mixed  
model as I wanted to account for non-independence within a pot.

There is an interaction between a and b, but I want to look at the 95%  
credible intervals of the simple effects to see which treatment  
combinations overlap 0, are greater than 0, or are less than 0.  While  
mcmcsamp works great on this object I am unclear on how to then  
combine parameter values and error to get this interval.

So, I attempted a model such as the following

a.lmer<-lmer(response ~ trta*trtb+(1|pot))

Which yielded the following error:
  Leading minor of order 15 in downdated X'X is not positive definite

Thinking that this might be an intercept issue, I fit the following  
model:

a.lmer<-lmer(response ~ trta*trtb+0+(1|pot))

This fit just fine.  summary() showed me a table of parameter values  
that seemed about what I would expect, although the correlation of  
fixed effects matrix was populated largely by 0's However,

a.mcmcsamp<-(a.lmer, 1000)
yielded the following error

Error: Omega[[1]] is not positive definite
Error in t(.Call(mer_MCMCsamp, object, saveb, n, trans, verbose,  
deviance)) :
   error in evaluating the argument 'x' in selecting a method for  
function 't'

However, if I try for roughly 30 or fewer replicates, everything works  
just fine.

Even more strange, when I next looked at a.lmer using summary() all of  
the error values for parameters had become 0, and the matrix for  
correlation of fixed effects was filled with NaNs.  This strikes me as  
rather odd.

1) Perhaps this has been fixed in later releases - I'm working off of  
lme4 version 0.99875-9 on R 2.6.2.  Should I try these instead?

2) Or, am I going about this attempt to get simple effect estimates  
all wrong?  Is it possible to use the output from the first model with  
mcmcsamp to get estimates of the simple effects?

Thanks for any advice you might have!



From sven.demaeyer at ua.ac.be  Fri Feb 22 09:57:15 2008
From: sven.demaeyer at ua.ac.be (De Maeyer Sven)
Date: Fri, 22 Feb 2008 09:57:15 +0100
Subject: [R-sig-ME] mixed models without residual term
Message-ID: <930B1A45F446404FA4D99A46F09209C4625B8C@xmail05.ad.ua.ac.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080222/741ac82d/attachment.pl>

From nasi0009 at umn.edu  Fri Feb 22 08:04:08 2008
From: nasi0009 at umn.edu (nasi0009 at umn.edu)
Date: 22 Feb 2008 01:04:08 -0600
Subject: [R-sig-ME] Problem with binomial-normal model in lmer
Message-ID: <Prayer.1.0.16.0802220104080.7118@wm1.tc.umn.edu>

I have a question about the possibility of fitting a binomial-normal model 
with lmer. I explain my problem using notation used in "Linear mixed model 
implementation in lme4" by Prof. Bates 
(http://stat.ethz.ch/CRAN/doc/vignettes/lme4/Implementation.pdf).

By binomial-normal, I mean a model that another term is added to Equation 
(29) (on page 28) of the paper, i.e. \eta=X\beta+Zb+\epsilon where \epsilon 
is N(0,\sigma_e). I thought that by modifying Z, \epsilon can be absorbed 
into Z. However, when I tried to test this on a simple simulated data set I 
received an error "Error in mer_finalize(ans, verbose) : q = > n = ". 
Basically, it seems that the basic assumption in lmer for GLMM models is 
that Z should be a thin matrix (more rows than columns). Naturally, this 
data-level normal error term can not be absorbed as another random effect 
since the total number of random effects exceeds the number of 
observations. Is there any other way around this problem? Am I doing 
something nonsense?

I appreciate if Prof. Bates or anyone who used lmer for GLMM comments on 
this.

Thanks,
Ali



From bolker at ufl.edu  Fri Feb 22 14:53:34 2008
From: bolker at ufl.edu (Ben Bolker)
Date: Fri, 22 Feb 2008 08:53:34 -0500
Subject: [R-sig-ME] Problem with binomial-normal model in lmer
Message-ID: <47BED3DE.3060501@ufl.edu>


    I have encountered this issue too (I wanted to fit a
lognormal-Poisson model as in Elston et al 2001, Parasitology
122:563-569), but didn't want to bother Doug Bates (having
bugged him about mcmcsamp so often in the past ...)  I too
would be curious to know whether this is something that could
be changed or a deeply rooted computational assumption.

    In the meantime, you could consider fitting a quasibinomial
model ...

     cheers
      Ben Bolker

  [trying again -- last one was rejected ... because of signature ? ]


nasi0009 at umn.edu wrote:
> I have a question about the possibility of fitting a binomial-normal model 
> with lmer. I explain my problem using notation used in "Linear mixed model 
> implementation in lme4" by Prof. Bates 
> (http://stat.ethz.ch/CRAN/doc/vignettes/lme4/Implementation.pdf).
> 
> By binomial-normal, I mean a model that another term is added to Equation 
> (29) (on page 28) of the paper, i.e. \eta=X\beta+Zb+\epsilon where \epsilon 
> is N(0,\sigma_e). I thought that by modifying Z, \epsilon can be absorbed 
> into Z. However, when I tried to test this on a simple simulated data set I 
> received an error "Error in mer_finalize(ans, verbose) : q = > n = ". 
> Basically, it seems that the basic assumption in lmer for GLMM models is 
> that Z should be a thin matrix (more rows than columns). Naturally, this 
> data-level normal error term can not be absorbed as another random effect 
> since the total number of random effects exceeds the number of 
> observations. Is there any other way around this problem? Am I doing 
> something nonsense?
> 
> I appreciate if Prof. Bates or anyone who used lmer for GLMM comments on 
> this.
> 
> Thanks,
> Ali
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From Manuel.A.Morales at williams.edu  Fri Feb 22 16:14:17 2008
From: Manuel.A.Morales at williams.edu (Manuel Morales)
Date: Fri, 22 Feb 2008 15:14:17 +0000
Subject: [R-sig-ME] Extracting variance components when using weights?
Message-ID: <1203693257.8529.13.camel@mrubra.localdomain>

I have a question about how to extract variance components from an lme
object when using weights. In particular, it appears that the w/i group
SD that is returned is estimated for group 1 only. Does that mean I need
to calculate the mean of the sds from all groups to estimate the overall
within-group variance component?

An example may clarify my question:

library(nlme)

### Create data
mns <- rnorm(10, 1500, 100)
sds <- rlnorm(10, log(300), .5)
obs <- mapply(function(x, y) rnorm(20, x, y), mns, sds)
ID <- rep(1:10, each=20)
resp <- c(as.vector(obs))

### Fit models
m1 <- lme(resp~1, rand=~1|ID)
m2 <- lme(resp~1, rand=~1|ID, weights=varIdent(form=~1|ID))
anova(m1,m2) # Model 2 much better than model 1

### Extract w/i grp variance
as.numeric(VarCorr(m1)[2,2])         # w/i grp sd for m1
(s1 <- as.numeric(VarCorr(m2)[2,2])) # w/i grp sd for grp 1 of m2???
var.wts <- varWeights(m2$modelStruct$varStruct) # var weights for m2
(m2.wi.sds <- s1*1/unique(var.wts)) # w/i grp sds for all grps of m2???
(mean(m2.wi.sds)) # w/i grp sd for m2???

-- 
http://mutualism.williams.edu



From mwkimpel at gmail.com  Fri Feb 22 18:57:20 2008
From: mwkimpel at gmail.com (Mark W Kimpel)
Date: Fri, 22 Feb 2008 12:57:20 -0500
Subject: [R-sig-ME] need help with mixed effects model
Message-ID: <47BF0D00.2030506@gmail.com>

This is my first foray into in mixed models and, while awaiting the 
arrival of:

Extending the Linear Model with R: Generalized Linear, Mixed Effects 
and 	Nonparametric Regression Models
Mixed Effects Models in S and S-Plus

I am in need to some advice.

I would like to look at gene-gene correlations within a multi-factorial,
mixed effects experiment. Here are the factors, with levels:

Gene Expression: 2 different genes per Animal, continuous variable
Animals: 6 per Strain
Tissues: 3 per animal
Strain: 2

I thus have 6*3*2 = 36 samples

I do not care, for this analysis, about differences between Tissues, 
Strains, or Animals, in fact, I want to control for them while examining 
the correlation of expression of the two genes. In other words, I want 
look at something very much like the Pearson correlation coefficient 
controlled for these other factors.

I guess the first question I should ask is: "is a mixed model the way to 
go, and, if not, what would be the correct approach?"

Assuming mixed models will work, as I see it through my newbie eyes, 
Tissue and strain are fixed effects and animals are random effects.

Any suggestions for an approach and a model?

Mark

Mark W. Kimpel MD  ** Neuroinformatics ** Dept. of Psychiatry
Indiana University School of Medicine

15032 Hunter Court, Westfield, IN  46074

(317) 490-5129 Work, & Mobile & VoiceMail
(317) 204-4202 Home (no voice mail please)

mwkimpel<at>gmail<dot>com



From bates at stat.wisc.edu  Sun Feb 24 20:04:41 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 24 Feb 2008 13:04:41 -0600
Subject: [R-sig-ME] Problem with binomial-normal model in lmer
In-Reply-To: <Prayer.1.0.16.0802220104080.7118@wm1.tc.umn.edu>
References: <Prayer.1.0.16.0802220104080.7118@wm1.tc.umn.edu>
Message-ID: <40e66e0b0802241104y679e0ff4i70c3e48258d4f4ff@mail.gmail.com>

On 22 Feb 2008 01:04:08 -0600,  <nasi0009 at umn.edu> wrote:
> I have a question about the possibility of fitting a binomial-normal model
>  with lmer. I explain my problem using notation used in "Linear mixed model
>  implementation in lme4" by Prof. Bates
>  (http://stat.ethz.ch/CRAN/doc/vignettes/lme4/Implementation.pdf).
>
>  By binomial-normal, I mean a model that another term is added to Equation
>  (29) (on page 28) of the paper, i.e. \eta=X\beta+Zb+\epsilon where \epsilon
>  is N(0,\sigma_e). I thought that by modifying Z, \epsilon can be absorbed
>  into Z. However, when I tried to test this on a simple simulated data set I
>  received an error "Error in mer_finalize(ans, verbose) : q = > n = ".

Did it really give that error message?  The way I had intended the
message to display is including the values of q and n.  Looking at the
code I think it should give the values.

>  Basically, it seems that the basic assumption in lmer for GLMM models is
>  that Z should be a thin matrix (more rows than columns). Naturally, this
>  data-level normal error term can not be absorbed as another random effect
>  since the total number of random effects exceeds the number of
>  observations. Is there any other way around this problem? Am I doing
>  something nonsense?

The reason for checking if the dimension of the random effects vector
exceed the number of observations is because the conditional modes of
the random effects are determined via iteratively reweighted least
squares.  It happens that the least squares problem is a penalized
least squares problem and a solution may exist even when q > n.  You
may want to try suppressing the check (near the beginning of the C
function called "update_u" in the file lme4/src/lmer.c) and seeing if
things still work for your example.

If you aren't comfortable modifying the source code and rebuilding the
package then please supply an example.

>  I appreciate if Prof. Bates or anyone who used lmer for GLMM comments on
>  this.
>
>  Thanks,
>  Ali
>
>  _______________________________________________
>  R-sig-mixed-models at r-project.org mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Sun Feb 24 20:21:39 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 24 Feb 2008 13:21:39 -0600
Subject: [R-sig-ME] mixed models without residual term
In-Reply-To: <930B1A45F446404FA4D99A46F09209C4625B8C@xmail05.ad.ua.ac.be>
References: <930B1A45F446404FA4D99A46F09209C4625B8C@xmail05.ad.ua.ac.be>
Message-ID: <40e66e0b0802241121u4e1e6f88p53db17fbc7478baa@mail.gmail.com>

On Fri, Feb 22, 2008 at 2:57 AM, De Maeyer Sven <sven.demaeyer at ua.ac.be> wrote:
> Dear List members,

>  I'm a recently new user of R and lme4. In some of my research designs I used to estimate mixed models without a residual term. Only random effects were added, but no residual. I did this in MlwiN. Now I'm trying to do the same analyses with lmer, but I can't seem to fix the residual to zero. Is this currently possible in lme4?

I don't think so.  The models that can be fit with the lme4 package
are all based on the assumption that the response vector is an n
dimensional random variable.  I'm not sure how to define maximum
likelihood estimates for parameters based on observed data without a
probability model for the distribution of the response vector.



From bates at stat.wisc.edu  Sun Feb 24 20:34:10 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 24 Feb 2008 13:34:10 -0600
Subject: [R-sig-ME] Strange mcmcsamp issue
In-Reply-To: <3E66F350-1147-4E24-956E-6668B0B13B2D@ucdavis.edu>
References: <3E66F350-1147-4E24-956E-6668B0B13B2D@ucdavis.edu>
Message-ID: <40e66e0b0802241134x397272fcv2114f23c67c5f115@mail.gmail.com>

On Thu, Feb 21, 2008 at 6:43 PM, Jarrett Byrnes <jebyrnes at ucdavis.edu> wrote:
> I'm attempting to pull out the simple effects from a mixed model with
>  two crossed treatments.  The model structure is such that
>
>  a.lmer<-lmer(response ~ trta*trtb+(1|pot))
>
>  In the experiment, I have an array of pots.  Each pot has a type of
>  treatment A applied to it.  Within the pot, there are two types of
>  treatment B that are applied, one on either side.  I am using a mixed
>  model as I wanted to account for non-independence within a pot.
>
>  There is an interaction between a and b, but I want to look at the 95%
>  credible intervals of the simple effects to see which treatment
>  combinations overlap 0, are greater than 0, or are less than 0.  While
>  mcmcsamp works great on this object I am unclear on how to then
>  combine parameter values and error to get this interval.
>
>  So, I attempted a model such as the following
>
>  a.lmer<-lmer(response ~ trta*trtb+(1|pot))

I would suggest trying to fit that model in the development version of
the lme4 package, available via

install.packages("lme4", repos = "http://r-forge.r-project.org")

requesting verbose output.  That is, fit it as

a.lmer <- lmer(response ~ trta * trtb + (1|pot), verbose = TRUE)

For each iteration of the optimizer the iteration number, the value of
the deviance and the current estimate of the relative standard
deviation of the random effects (i.e. the ratio of the standard
deviation of the random effects and the standard deviation of the
residuals) are printed.  Check if the relative standard deviation is
being driven to zero.

>  Which yielded the following error:
>   Leading minor of order 15 in downdated X'X is not positive definite


>  Thinking that this might be an intercept issue, I fit the following
>  model:
>
>  a.lmer<-lmer(response ~ trta*trtb+0+(1|pot))
>
>  This fit just fine.  summary() showed me a table of parameter values
>  that seemed about what I would expect, although the correlation of
>  fixed effects matrix was populated largely by 0's However,
>
>  a.mcmcsamp<-(a.lmer, 1000)
>  yielded the following error
>
>  Error: Omega[[1]] is not positive definite
>  Error in t(.Call(mer_MCMCsamp, object, saveb, n, trans, verbose,
>  deviance)) :
>    error in evaluating the argument 'x' in selecting a method for
>  function 't'
>
>  However, if I try for roughly 30 or fewer replicates, everything works
>  just fine.
>
>  Even more strange, when I next looked at a.lmer using summary() all of
>  the error values for parameters had become 0, and the matrix for
>  correlation of fixed effects was filled with NaNs.  This strikes me as
>  rather odd.
>
>  1) Perhaps this has been fixed in later releases - I'm working off of
>  lme4 version 0.99875-9 on R 2.6.2.  Should I try these instead?
>
>  2) Or, am I going about this attempt to get simple effect estimates
>  all wrong?  Is it possible to use the output from the first model with
>  mcmcsamp to get estimates of the simple effects?
>
>  Thanks for any advice you might have!
>
>  _______________________________________________
>  R-sig-mixed-models at r-project.org mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Sun Feb 24 20:45:42 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 24 Feb 2008 13:45:42 -0600
Subject: [R-sig-ME] need help with mixed effects model
In-Reply-To: <47BF0D00.2030506@gmail.com>
References: <47BF0D00.2030506@gmail.com>
Message-ID: <40e66e0b0802241145i7ff2888al44cfa6101b735986@mail.gmail.com>

On Fri, Feb 22, 2008 at 11:57 AM, Mark W Kimpel <mwkimpel at gmail.com> wrote:
> This is my first foray into in mixed models and, while awaiting the
>  arrival of:

>  Extending the Linear Model with R: Generalized Linear, Mixed Effects
>  and     Nonparametric Regression Models
>  Mixed Effects Models in S and S-Plus

>  I am in need to some advice.

>  I would like to look at gene-gene correlations within a multi-factorial,
>  mixed effects experiment. Here are the factors, with levels:

>  Gene Expression: 2 different genes per Animal, continuous variable
>  Animals: 6 per Strain
>  Tissues: 3 per animal

> Strain: 2

>  I thus have 6*3*2 = 36 samples

>  I do not care, for this analysis, about differences between Tissues,
>  Strains, or Animals, in fact, I want to control for them while examining
>  the correlation of expression of the two genes. In other words, I want
>  look at something very much like the Pearson correlation coefficient
>  controlled for these other factors.

>  I guess the first question I should ask is: "is a mixed model the way to
>  go, and, if not, what would be the correct approach?"

Perhaps.  How do you plan to incorporate the two genes?

>  Assuming mixed models will work, as I see it through my newbie eyes,
>  Tissue and strain are fixed effects and animals are random effects.

If you were interested in just 1 gene than I would say that this looks
like a good approach.  I'm just not sure what to do about the multiple
genes.

>  Any suggestions for an approach and a model?

The model specification (assuming that each animal has a distinct
number) would be something like

gene1 ~ Tissue * Strain + (1|Animal)

In your earlier message to the Bioconductor list you had a
specification that looked like

gene1 ~ gene2 + ...

which makes me a little queasy because you are assuming that gene2 is
"known" relative to the variability in gene1 and most of the time that
is not a reasonable approach.



From mwkimpel at gmail.com  Mon Feb 25 07:17:18 2008
From: mwkimpel at gmail.com (Mark W Kimpel)
Date: Mon, 25 Feb 2008 01:17:18 -0500
Subject: [R-sig-ME] need help with mixed effects model
In-Reply-To: <40e66e0b0802241145i7ff2888al44cfa6101b735986@mail.gmail.com>
References: <47BF0D00.2030506@gmail.com>
	<40e66e0b0802241145i7ff2888al44cfa6101b735986@mail.gmail.com>
Message-ID: <47C25D6E.9030101@gmail.com>

Doug,

What I have been persuing over the past 2 days is this model, which is 
different from what I posted to BioC and similar to yours except it 
includes the second gene:

  mod <- gene2.expression ~ gene1.expression + Strain + (1|Rat))

I understand your concern and I rely on your expertise as to whether 
this above model is tenable, but let me clarify and give background to 
the question I am exploring.

It is very common in gene expression analysis to perform hierarchical 
clustering to see which genes are most closely correlated in regards to 
expression over a number of samples. The problem with clustering is that 
all genes will cluster somewhere, and it if often hard to make sense of 
the result. I wish to assign some level of significance to the 
correlation between the expression of two genes, admittedly not 
independent of one another. If I just had a bunch of rats under no 
experimental conditions and took one sample per rat, I would think it 
would be reasonable to apply the Pearson Correlation approach and look 
at the p value of the slope.

I, however, have not been presented with such a simple experiment and 
recognize that I cannot simply correlate all 36 samples as if they were 
independent and came from the same population. In fact the genes that 
are most interesting are those that are differentially expressed between 
the two strains, and from each rat I have samples from each of 3 brain 
regions, which I know from prior work are correlated within rat but 
different between brain regions. In fact, it would be nice to be able to 
take brain region into account rather than looking it as a random effect 
within animal, but I am pretty sure I run out of degrees of freedom if I 
do that.

So, it is necessary for me to have both genes in the model to accomplish 
what I need to do, and the other factors are, in fact, are there merely 
to improve power to detect gene-gene correlation by accounting for the 
variance induced by these other factors.

So, is this legit or is there a better approach? Thanks so much for yoru 
help.

Mark

Mark W. Kimpel MD  ** Neuroinformatics ** Dept. of Psychiatry
Indiana University School of Medicine

15032 Hunter Court, Westfield, IN  46074

(317) 490-5129 Work, & Mobile & VoiceMail
(317) 204-4202 Home (no voice mail please)

mwkimpel<at>gmail<dot>com

******************************************************************


Douglas Bates wrote:
> On Fri, Feb 22, 2008 at 11:57 AM, Mark W Kimpel <mwkimpel at gmail.com> wrote:
>> This is my first foray into in mixed models and, while awaiting the
>>  arrival of:
> 
>>  Extending the Linear Model with R: Generalized Linear, Mixed Effects
>>  and     Nonparametric Regression Models
>>  Mixed Effects Models in S and S-Plus
> 
>>  I am in need to some advice.
> 
>>  I would like to look at gene-gene correlations within a multi-factorial,
>>  mixed effects experiment. Here are the factors, with levels:
> 
>>  Gene Expression: 2 different genes per Animal, continuous variable
>>  Animals: 6 per Strain
>>  Tissues: 3 per animal
> 
>> Strain: 2
> 
>>  I thus have 6*3*2 = 36 samples
> 
>>  I do not care, for this analysis, about differences between Tissues,
>>  Strains, or Animals, in fact, I want to control for them while examining
>>  the correlation of expression of the two genes. In other words, I want
>>  look at something very much like the Pearson correlation coefficient
>>  controlled for these other factors.
> 
>>  I guess the first question I should ask is: "is a mixed model the way to
>>  go, and, if not, what would be the correct approach?"
> 
> Perhaps.  How do you plan to incorporate the two genes?
> 
>>  Assuming mixed models will work, as I see it through my newbie eyes,
>>  Tissue and strain are fixed effects and animals are random effects.
> 
> If you were interested in just 1 gene than I would say that this looks
> like a good approach.  I'm just not sure what to do about the multiple
> genes.
> 
>>  Any suggestions for an approach and a model?
> 
> The model specification (assuming that each animal has a distinct
> number) would be something like
> 
> gene1 ~ Tissue * Strain + (1|Animal)
> 
> In your earlier message to the Bioconductor list you had a
> specification that looked like
> 
> gene1 ~ gene2 + ...
> 
> which makes me a little queasy because you are assuming that gene2 is
> "known" relative to the variability in gene1 and most of the time that
> is not a reasonable approach.
>



From HStevens at MUOhio.edu  Mon Feb 25 23:42:48 2008
From: HStevens at MUOhio.edu (Hank Stevens)
Date: Mon, 25 Feb 2008 17:42:48 -0500
Subject: [R-sig-ME] mcmcpvalue and contrasts
Message-ID: <3745691C-08C0-43CA-8530-DEF1E263A66E@MUOhio.edu>

Hi Folks,
I wanted to double check that my intuition makes sense.

Examples of mcmcpvalue that I have seen use treatment "contrast" coding.
However, in more complex designs, testing overall effects of a factor  
might be better done with other contrasts, such as sum or Helmert  
contrasts.

My Contention:
Different contrasts test different hypothesis, and therefore result in  
different P-values. This consequence of contrasts differs from  
analysis of variance, as in anova( lm(Y ~ X1*X2) ).

*** This is right, isn't it? ***

I provide a self-contained example.

library(lme4) # v. 0.99875-9
library(coda)
library(SASmixed)

mcmcpvalue <- function(samp) {
    ##From Bates pers comm. September 14, 2006 2:59:23 PM EDT
    ## elementary version that creates an empirical p-value for the
    ## hypothesis that the columns of samp have mean zero versus a
    ## general multivariate distribution with elliptical contours.
# samp <- rnorm(10000, m=3)
    ## differences from the mean standardized by the observed
    ## variance-covariance factor
    std <- backsolve(chol(var(samp)),
                     cbind(0, t(samp)) - colMeans(samp),
                     transpose = TRUE)
    sqdist <- colSums(std * std)
    sum(sqdist[-1] > sqdist[1])/nrow(samp)
}


options(contrasts=c("contr.treatment","contr.poly"))
print(fm1 <- lmer(resistance ~ ET * position + (1|Grp), Semiconductor),
       corr=F)
c1 <- mcmcsamp(fm1, 50000)

options(contrasts=c("contr.helmert","contr.poly"))
print(fm2 <- lmer(resistance ~ ET * position + (1|Grp), Semiconductor),
       corr=F)
c2 <- mcmcsamp(fm2, 50000)

  mcmcpvalue(c1[, 2:4 ])
[1] 0.32458
 > mcmcpvalue(c2[, 2:4 ])
[1] 0.0249
 > mcmcpvalue(c1[, 5:7 ])
[1] 0.45376
 > mcmcpvalue(c2[, 5:7 ])
[1] 0.16302
 > mcmcpvalue(c1[, 8:16 ])
[1] 0.6373
 > mcmcpvalue(c2[, 8:16 ])
[1] 0.88808
 >
 > sessionInfo()
R version 2.6.2 (2008-02-08)
i386-apple-darwin8.10.1

locale:
C

attached base packages:
[1] stats     graphics  utils     datasets  grDevices methods
[7] base

other attached packages:
[1] SASmixed_0.4-2    xtable_1.5-2      reshape_0.8.0
[4] coda_0.13-1       lme4_0.99875-9    Matrix_0.999375-4
[7] lattice_0.17-4    CarbonEL_0.1-4

loaded via a namespace (and not attached):
[1] grid_2.6.2
 >

Dr. Hank Stevens, Assoicate Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.cas.muohio.edu/ecology
http://www.muohio.edu/botany/

"If the stars should appear one night in a thousand years, how would men
believe and adore." -Ralph Waldo Emerson, writer and philosopher  
(1803-1882)



From kjbeath at kagi.com  Tue Feb 26 01:05:02 2008
From: kjbeath at kagi.com (Ken Beath)
Date: Tue, 26 Feb 2008 11:05:02 +1100
Subject: [R-sig-ME] mcmcpvalue and contrasts
In-Reply-To: <3745691C-08C0-43CA-8530-DEF1E263A66E@MUOhio.edu>
References: <3745691C-08C0-43CA-8530-DEF1E263A66E@MUOhio.edu>
Message-ID: <955BDCBE-F62D-49EB-BD22-94B3676AF007@kagi.com>

On 26/02/2008, at 9:42 AM, Hank Stevens wrote:

> Hi Folks,
> I wanted to double check that my intuition makes sense.
>
> Examples of mcmcpvalue that I have seen use treatment "contrast"  
> coding.
> However, in more complex designs, testing overall effects of a factor
> might be better done with other contrasts, such as sum or Helmert
> contrasts.
>
> My Contention:
> Different contrasts test different hypothesis, and therefore result in
> different P-values. This consequence of contrasts differs from
> analysis of variance, as in anova( lm(Y ~ X1*X2) ).
>
> *** This is right, isn't it? ***
>

The main problem is testing for a main effect in the presence of  
interaction. While it looks like it gives sensible results in some  
cases like balanced ANOVA, they really aren't sensible and the effect  
of parameterisation in other cases makes that clear.

The difference for the interaction is probably just sampling  
variation, increasing samples fixes this.

Ken



> I provide a self-contained example.
>
> library(lme4) # v. 0.99875-9
> library(coda)
> library(SASmixed)
>
> mcmcpvalue <- function(samp) {
>    ##From Bates pers comm. September 14, 2006 2:59:23 PM EDT
>    ## elementary version that creates an empirical p-value for the
>    ## hypothesis that the columns of samp have mean zero versus a
>    ## general multivariate distribution with elliptical contours.
> # samp <- rnorm(10000, m=3)
>    ## differences from the mean standardized by the observed
>    ## variance-covariance factor
>    std <- backsolve(chol(var(samp)),
>                     cbind(0, t(samp)) - colMeans(samp),
>                     transpose = TRUE)
>    sqdist <- colSums(std * std)
>    sum(sqdist[-1] > sqdist[1])/nrow(samp)
> }
>
>
> options(contrasts=c("contr.treatment","contr.poly"))
> print(fm1 <- lmer(resistance ~ ET * position + (1|Grp),  
> Semiconductor),
>       corr=F)
> c1 <- mcmcsamp(fm1, 50000)
>
> options(contrasts=c("contr.helmert","contr.poly"))
> print(fm2 <- lmer(resistance ~ ET * position + (1|Grp),  
> Semiconductor),
>       corr=F)
> c2 <- mcmcsamp(fm2, 50000)
>
>  mcmcpvalue(c1[, 2:4 ])
> [1] 0.32458
>> mcmcpvalue(c2[, 2:4 ])
> [1] 0.0249
>> mcmcpvalue(c1[, 5:7 ])
> [1] 0.45376
>> mcmcpvalue(c2[, 5:7 ])
> [1] 0.16302
>> mcmcpvalue(c1[, 8:16 ])
> [1] 0.6373
>> mcmcpvalue(c2[, 8:16 ])
> [1] 0.88808
>>
>> sessionInfo()
> R version 2.6.2 (2008-02-08)
> i386-apple-darwin8.10.1
>
> locale:
> C
>
> attached base packages:
> [1] stats     graphics  utils     datasets  grDevices methods
> [7] base
>
> other attached packages:
> [1] SASmixed_0.4-2    xtable_1.5-2      reshape_0.8.0
> [4] coda_0.13-1       lme4_0.99875-9    Matrix_0.999375-4
> [7] lattice_0.17-4    CarbonEL_0.1-4
>
> loaded via a namespace (and not attached):
> [1] grid_2.6.2
>>
>
> Dr. Hank Stevens, Assoicate Professor
> 338 Pearson Hall
> Botany Department
> Miami University
> Oxford, OH 45056
>
> Office: (513) 529-4206
> Lab: (513) 529-4262
> FAX: (513) 529-4243
> http://www.cas.muohio.edu/~stevenmh/
> http://www.cas.muohio.edu/ecology
> http://www.muohio.edu/botany/
>
> "If the stars should appear one night in a thousand years, how would  
> men
> believe and adore." -Ralph Waldo Emerson, writer and philosopher
> (1803-1882)
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From smckinney at bccrc.ca  Tue Feb 26 03:22:46 2008
From: smckinney at bccrc.ca (Steven McKinney)
Date: Mon, 25 Feb 2008 18:22:46 -0800
Subject: [R-sig-ME] mcmcpvalue and contrasts
References: <3745691C-08C0-43CA-8530-DEF1E263A66E@MUOhio.edu>
	<955BDCBE-F62D-49EB-BD22-94B3676AF007@kagi.com>
Message-ID: <0BE438149FF2254DB4199E2682C8DFEB0328A1B7@crcmail1.BCCRC.CA>


Hi Hank,

> 
> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org on behalf of Ken Beath
> Sent: Mon 2/25/2008 4:05 PM
> To: Hank Stevens
> Cc: Help Mixed Models
> Subject: Re: [R-sig-ME] mcmcpvalue and contrasts
>  
> On 26/02/2008, at 9:42 AM, Hank Stevens wrote:
> 
> > Hi Folks,
> > I wanted to double check that my intuition makes sense.
> >
> > Examples of mcmcpvalue that I have seen use treatment "contrast"  
> > coding.
> > However, in more complex designs, testing overall effects of a factor
> > might be better done with other contrasts, such as sum or Helmert
> > contrasts.
> >
> > My Contention:
> > Different contrasts test different hypothesis, and therefore result in
> > different P-values. This consequence of contrasts differs from
> > analysis of variance, as in anova( lm(Y ~ X1*X2) ).
> >
> > *** This is right, isn't it? ***
> >
> 
> The main problem is testing for a main effect in the presence of  
> interaction. While it looks like it gives sensible results in some  
> cases like balanced ANOVA, they really aren't sensible and the effect  
> of parameterisation in other cases makes that clear.
> 
> The difference for the interaction is probably just sampling  
> variation, increasing samples fixes this.
> 
> Ken

Ken is correct - testing some of the main effect terms resulting from
different parameterizations due to the differing contrast structures
will yield different results (though they in general will be somewhat 
meaningless if the corresponding interaction term is in the model
and you do not have a balanced orthogonal design).

In general, the choice of contrast *** SHOULD NOT *** yield different
p-values for logically equivalent hypothesis tests, so the answer to
Hank's question is 'no - this is not right'.  Testing an interaction
term (with all main effects terms comprising the interaction included
in the model as is in general necessary) should not depend on the
choice of contrast terms.  Here is an example.  (Unfortunately I can
not illustrate using your lmer example, as my lmer produces output
that won't print - some minor bug - but that's another issue.)

I'll illustrate the statistical principle using regular linear
models.  First with the usual treatment 'contrasts'.

    > options(contrasts=c("contr.treatment","contr.poly"))
    > lm1 <- lm(resistance ~ ET * position, data = Semiconductor)
    > summary(lm1)
    
    Call:
    lm(formula = resistance ~ ET * position, data = Semiconductor)
    
    Residuals:
         Min       1Q   Median       3Q      Max 
    -1.01333 -0.25750  0.04333  0.28333  0.74667 
    
    Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
    (Intercept)    5.61333    0.26891  20.874   <2e-16 ***
    ET2            0.38000    0.38030   0.999    0.325    
    ET3            0.52333    0.38030   1.376    0.178    
    ET4            0.72667    0.38030   1.911    0.065 .  
    position2     -0.16333    0.38030  -0.429    0.670    
    position3     -0.06000    0.38030  -0.158    0.876    
    position4      0.27333    0.38030   0.719    0.478    
    ET2:position2  0.35667    0.53782   0.663    0.512    
    ET3:position2  0.37333    0.53782   0.694    0.493    
    ET4:position2  0.37667    0.53782   0.700    0.489    
    ET2:position3 -0.16667    0.53782  -0.310    0.759    
    ET3:position3 -0.30333    0.53782  -0.564    0.577    
    ET4:position3 -0.38333    0.53782  -0.713    0.481    
    ET2:position4 -0.35000    0.53782  -0.651    0.520    
    ET3:position4 -0.31667    0.53782  -0.589    0.560    
    ET4:position4 -0.07333    0.53782  -0.136    0.892    
    ---
    Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 
    
    Residual standard error: 0.4658 on 32 degrees of freedom
    Multiple R-squared: 0.4211,	Adjusted R-squared: 0.1498 
    F-statistic: 1.552 on 15 and 32 DF,  p-value: 0.1449 


Now true contrasts with helmert polynomials:

    > options(contrasts=c("contr.helmert","contr.poly"))
    > lm2 <- lm(resistance ~ ET * position, data = Semiconductor)
    > summary(lm2)
    
    Call:
    lm(formula = resistance ~ ET * position, data = Semiconductor)
    
    Residuals:
         Min       1Q   Median       3Q      Max 
    -1.01333 -0.25750  0.04333  0.28333  0.74667 
    
    Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
    (Intercept)    6.00292    0.06723  89.292  < 2e-16 ***
    ET1            0.17000    0.09507   1.788  0.08324 .  
    ET2            0.09722    0.05489   1.771  0.08606 .  
    ET3            0.10986    0.03881   2.830  0.00797 ** 
    position1      0.05667    0.09507   0.596  0.55535    
    position2     -0.11000    0.05489  -2.004  0.05360 .  
    position3      0.03542    0.03881   0.912  0.36834    
    ET1:position1  0.08917    0.13446   0.663  0.51197    
    ET2:position1  0.03250    0.07763   0.419  0.67826    
    ET3:position1  0.01667    0.05489   0.304  0.76337    
    ET1:position2 -0.05750    0.07763  -0.741  0.46427    
    ET2:position2 -0.03528    0.04482  -0.787  0.43700    
    ET3:position2 -0.02444    0.03169  -0.771  0.44617    
    ET1:position3 -0.05167    0.05489  -0.941  0.35363    
    ET2:position3 -0.01111    0.03169  -0.351  0.72818    
    ET3:position3  0.01125    0.02241   0.502  0.61909    
    ---
    Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 
    
    Residual standard error: 0.4658 on 32 degrees of freedom
    Multiple R-squared: 0.4211,	Adjusted R-squared: 0.1498 
    F-statistic: 1.552 on 15 and 32 DF,  p-value: 0.1449 
    
Things to notice: The two model fits above show the same R-squared,
the same F-statistic and its associated p-value.  This is good - the
two models are mathematically equivalent, so the overall model fit
statistics should not differ.

The two model parameter estimate sets do differ, as the
parameterization is different (though the vector spaces in which the
data and models live are the same - a linear algebra book will show
the details).  Though the variables 'ET' and 'position' each have 4
levels, we only ever see 3 parameter estimates due to the linear
constraints imposed by projecting from the high dimensional space
containing the data to the lower dimensional space containing the
model.  

Now to test the interaction term:

    > options(contrasts=c("contr.treatment","contr.poly"))
    > lmr1 <- lm(resistance ~ ET + position, data = Semiconductor) # Fit reduced model
    > anova(lm1, lmr1)
    Analysis of Variance Table
    
    Model 1: resistance ~ ET * position
    Model 2: resistance ~ ET + position
      Res.Df     RSS Df Sum of Sq      F Pr(>F)
    1     32  6.9421                           
    2     41  7.7515 -9   -0.8095 0.4146 0.9178
    
    > options(contrasts=c("contr.helmert","contr.poly"))
    > lmr2 <- lm(resistance ~ ET + position, data = Semiconductor)
    > anova(lm2, lmr2)
    Analysis of Variance Table
    
    Model 1: resistance ~ ET * position
    Model 2: resistance ~ ET + position
      Res.Df     RSS Df Sum of Sq      F Pr(>F)
    1     32  6.9421                           
    2     41  7.7515 -9   -0.8095 0.4146 0.9178
    > 

Things to notice:  The ANOVA tables for the two different contrast
parameterizations are identical.  Dropping the interaction term
results in the model space now occupying 9 fewer dimensions (indicated
by the '-9' in the degrees of freedom column).  As measured by the
F-statistic, not much predictive capability was sacrificed by this
reduction in model dimension - the simpler model fits the data about
as well as the more complex model so we would in general opt for the
simpler model (caveat - random effects issues being ignored here in
this illustration). 

If these two logically equivalent tests had produced non-identical
results, I would next look for numerical instability in the fitting
algorithms.  Numerical instability is a hallmark of 'treatment
contrasts' (but not in R - see below *) and is the reason that those
new to statistics are reminded not to hand fit regression models using
the
Beta_hat = ((X'X)^-1)X'Y 
formula which is mathematically valid but does not perform well on 
finite precision computing devices. 
 
(* 'Treatment contrasts' computed in R are fine because under the
covers R is always using numerically stable algorithms, then results
are converted to the appropriate user-requested contrast format.)


> 
> 
> 
> > I provide a self-contained example.
> >
> > library(lme4) # v. 0.99875-9
> > library(coda)
> > library(SASmixed)
> >
> > mcmcpvalue <- function(samp) {
> >    ##From Bates pers comm. September 14, 2006 2:59:23 PM EDT
> >    ## elementary version that creates an empirical p-value for the
> >    ## hypothesis that the columns of samp have mean zero versus a
> >    ## general multivariate distribution with elliptical contours.
> > # samp <- rnorm(10000, m=3)
> >    ## differences from the mean standardized by the observed
> >    ## variance-covariance factor
> >    std <- backsolve(chol(var(samp)),
> >                     cbind(0, t(samp)) - colMeans(samp),
> >                     transpose = TRUE)
> >    sqdist <- colSums(std * std)
> >    sum(sqdist[-1] > sqdist[1])/nrow(samp)
> > }
> >
> >
> > options(contrasts=c("contr.treatment","contr.poly"))
> > print(fm1 <- lmer(resistance ~ ET * position + (1|Grp),  
> > Semiconductor),
> >       corr=F)
> > c1 <- mcmcsamp(fm1, 50000)
> >
> > options(contrasts=c("contr.helmert","contr.poly"))
> > print(fm2 <- lmer(resistance ~ ET * position + (1|Grp),  
> > Semiconductor),
> >       corr=F)
> > c2 <- mcmcsamp(fm2, 50000)
> >
> >  mcmcpvalue(c1[, 2:4 ])
> > [1] 0.32458
> >> mcmcpvalue(c2[, 2:4 ])
> > [1] 0.0249
> >> mcmcpvalue(c1[, 5:7 ])
> > [1] 0.45376
> >> mcmcpvalue(c2[, 5:7 ])
> > [1] 0.16302
> >> mcmcpvalue(c1[, 8:16 ])
> > [1] 0.6373
> >> mcmcpvalue(c2[, 8:16 ])
> > [1] 0.88808
> >>
> >> sessionInfo()
> > R version 2.6.2 (2008-02-08)
> > i386-apple-darwin8.10.1
> >
> > locale:
> > C
> >
> > attached base packages:
> > [1] stats     graphics  utils     datasets  grDevices methods
> > [7] base
> >
> > other attached packages:
> > [1] SASmixed_0.4-2    xtable_1.5-2      reshape_0.8.0
> > [4] coda_0.13-1       lme4_0.99875-9    Matrix_0.999375-4
> > [7] lattice_0.17-4    CarbonEL_0.1-4
> >
> > loaded via a namespace (and not attached):
> > [1] grid_2.6.2
> >>
> >
> > Dr. Hank Stevens, Assoicate Professor
> > 338 Pearson Hall
> > Botany Department
> > Miami University
> > Oxford, OH 45056
> >
> > Office: (513) 529-4206
> > Lab: (513) 529-4262
> > FAX: (513) 529-4243
> > http://www.cas.muohio.edu/~stevenmh/
> > http://www.cas.muohio.edu/ecology
> > http://www.muohio.edu/botany/
> >
> > "If the stars should appear one night in a thousand years, how would  
> > men
> > believe and adore." -Ralph Waldo Emerson, writer and philosopher
> > (1803-1882)
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
> 
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 


Steven McKinney

Statistician
Molecular Oncology and Breast Cancer Program
British Columbia Cancer Research Centre

email: smckinney +at+ bccrc +dot+ ca

tel: 604-675-8000 x7561

BCCRC
Molecular Oncology
675 West 10th Ave, Floor 4
Vancouver B.C. 
V5Z 1L3
Canada


My lmer problem:

> options(contrasts=c("contr.treatment","contr.poly"))
> fm1 <- lmer(resistance ~ ET * position + (1|Grp), Semiconductor)
> print(fm1, corr = FALSE)
Error in .local(x, ...) : 
  no slot of name "status" for this object of class "table"

This is probably fixed in newer configurations and is probably of no
concern.  

> sessionInfo()
R version 2.6.2 (2008-02-08) 
powerpc-apple-darwin8.10.1 

locale:
en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] Hmisc_3.4-3       SASmixed_0.4-2    coda_0.13-1       lme4_0.99875-9    Matrix_0.999375-4 lattice_0.17-4    reshape_0.8.0    
 [8] R.utils_0.9.8     R.oo_1.4.1        R.methodsS3_1.0.0 RMySQL_0.6-0      DBI_0.2-4         RODBC_1.2-3       CGIwithR_0.72    
[15] GDD_0.1-8        

loaded via a namespace (and not attached):
[1] cluster_1.11.9 grid_2.6.2     tools_2.6.2   
> 



From kjbeath at kagi.com  Tue Feb 26 05:46:22 2008
From: kjbeath at kagi.com (Ken Beath)
Date: Tue, 26 Feb 2008 15:46:22 +1100
Subject: [R-sig-ME] mcmcpvalue and contrasts
In-Reply-To: <0BE438149FF2254DB4199E2682C8DFEB0328A1B7@crcmail1.BCCRC.CA>
References: <3745691C-08C0-43CA-8530-DEF1E263A66E@MUOhio.edu>
	<955BDCBE-F62D-49EB-BD22-94B3676AF007@kagi.com>
	<0BE438149FF2254DB4199E2682C8DFEB0328A1B7@crcmail1.BCCRC.CA>
Message-ID: <976245B6-4845-4C74-839E-C606E44ABEBF@kagi.com>

On 26/02/2008, at 1:22 PM, Steven McKinney wrote:

>
> Hi Hank,
>
>>
>> -----Original Message-----
>> From: r-sig-mixed-models-bounces at r-project.org on behalf of Ken Beath
>> Sent: Mon 2/25/2008 4:05 PM
>> To: Hank Stevens
>> Cc: Help Mixed Models
>> Subject: Re: [R-sig-ME] mcmcpvalue and contrasts
>>
>> On 26/02/2008, at 9:42 AM, Hank Stevens wrote:
>>
>>> Hi Folks,
>>> I wanted to double check that my intuition makes sense.
>>>
>>> Examples of mcmcpvalue that I have seen use treatment "contrast"
>>> coding.
>>> However, in more complex designs, testing overall effects of a  
>>> factor
>>> might be better done with other contrasts, such as sum or Helmert
>>> contrasts.
>>>
>>> My Contention:
>>> Different contrasts test different hypothesis, and therefore  
>>> result in
>>> different P-values. This consequence of contrasts differs from
>>> analysis of variance, as in anova( lm(Y ~ X1*X2) ).
>>>
>>> *** This is right, isn't it? ***
>>>
>>
>> The main problem is testing for a main effect in the presence of
>> interaction. While it looks like it gives sensible results in some
>> cases like balanced ANOVA, they really aren't sensible and the effect
>> of parameterisation in other cases makes that clear.
>>
>> The difference for the interaction is probably just sampling
>> variation, increasing samples fixes this.
>>
>> Ken
>
> Ken is correct - testing some of the main effect terms resulting from
> different parameterizations due to the differing contrast structures
> will yield different results (though they in general will be somewhat
> meaningless if the corresponding interaction term is in the model
> and you do not have a balanced orthogonal design).
>

Even with orthogonal designs there is still a problem with  
interpretation. If we have a model with A*B and the interaction and B  
are significant, then it seems that the conclusion about B is limited  
to the choice of A in the experiment.  An assumption that the effect  
of B will be the same with a different set of A seems rather risky,  
although it seems to be what the FDA expect for multi-centre trials.  
If there is some need to generalise then a model allowing for  a  
random effect for B seems more sensible.

Ken



From njbisaac at googlemail.com  Tue Feb 26 11:09:08 2008
From: njbisaac at googlemail.com (Nick Isaac)
Date: Tue, 26 Feb 2008 10:09:08 +0000
Subject: [R-sig-ME] Strange mcmcsamp issue
In-Reply-To: <40e66e0b0802241134x397272fcv2114f23c67c5f115@mail.gmail.com>
References: <3E66F350-1147-4E24-956E-6668B0B13B2D@ucdavis.edu>
	<40e66e0b0802241134x397272fcv2114f23c67c5f115@mail.gmail.com>
Message-ID: <a072ed700802260209o53e306b0o4adfb41a6c8bb8b0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080226/ff9ca8cc/attachment.pl>

From bates at stat.wisc.edu  Tue Feb 26 18:51:23 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 26 Feb 2008 11:51:23 -0600
Subject: [R-sig-ME] Strange mcmcsamp issue
In-Reply-To: <a072ed700802260209o53e306b0o4adfb41a6c8bb8b0@mail.gmail.com>
References: <3E66F350-1147-4E24-956E-6668B0B13B2D@ucdavis.edu>
	<40e66e0b0802241134x397272fcv2114f23c67c5f115@mail.gmail.com>
	<a072ed700802260209o53e306b0o4adfb41a6c8bb8b0@mail.gmail.com>
Message-ID: <40e66e0b0802260951i20b741f0w1ae0b2fbb228af5a@mail.gmail.com>

On Tue, Feb 26, 2008 at 4:09 AM, Nick Isaac <njbisaac at googlemail.com> wrote:
> Dear Prof Bates,

> Would you mind clarifying the status of mcmcsamp()?

> When lme4 version 0.999375-1 was released, you stated that mcmcsamp() was
> one of a few functions that did not work properly. You didn't mention
> mcmcsamp() when 0.999375-2 was released, but your recommendation to Jarrett
> Byrnes suggests that it's working as it should. So I'm a little confused.

> Your comments will be gratefully received.

Version 0.999375-4, the latest version on R-forge, has mcmcsamp
disabled.  I plan to have mcmcsamp restored in version 0.999375-5, at
least for linear mixed models.  It is somewhat more complicated to
create mcmcsamp for generalized linear mixed models and for nonlinear
mixed models.



From smckinney at bccrc.ca  Tue Feb 26 20:21:23 2008
From: smckinney at bccrc.ca (Steven McKinney)
Date: Tue, 26 Feb 2008 11:21:23 -0800
Subject: [R-sig-ME] mcmcpvalue and contrasts
References: <3745691C-08C0-43CA-8530-DEF1E263A66E@MUOhio.edu>
	<955BDCBE-F62D-49EB-BD22-94B3676AF007@kagi.com>
	<0BE438149FF2254DB4199E2682C8DFEB0328A1B7@crcmail1.BCCRC.CA>
	<976245B6-4845-4C74-839E-C606E44ABEBF@kagi.com>
Message-ID: <0BE438149FF2254DB4199E2682C8DFEB0328A1BB@crcmail1.BCCRC.CA>


> -----Original Message-----
> From: Ken Beath [mailto:kjbeath at kagi.com]
> Sent: Mon 2/25/2008 8:46 PM
> To: Steven McKinney
> Cc: Hank Stevens; Help Mixed Models
> Subject: Re: [R-sig-ME] mcmcpvalue and contrasts
>  
> On 26/02/2008, at 1:22 PM, Steven McKinney wrote:
> 
> >
> > Hi Hank,
> >
> >>
> >> -----Original Message-----
> >> From: r-sig-mixed-models-bounces at r-project.org on behalf of Ken Beath
> >> Sent: Mon 2/25/2008 4:05 PM
> >> To: Hank Stevens
> >> Cc: Help Mixed Models
> >> Subject: Re: [R-sig-ME] mcmcpvalue and contrasts
> >>
> >> On 26/02/2008, at 9:42 AM, Hank Stevens wrote:
> >>
> >>> Hi Folks,
> >>> I wanted to double check that my intuition makes sense.
> >>>
> >>> Examples of mcmcpvalue that I have seen use treatment "contrast"
> >>> coding.
> >>> However, in more complex designs, testing overall effects of a  
> >>> factor
> >>> might be better done with other contrasts, such as sum or Helmert
> >>> contrasts.
> >>>
> >>> My Contention:
> >>> Different contrasts test different hypothesis, and therefore  
> >>> result in
> >>> different P-values. This consequence of contrasts differs from
> >>> analysis of variance, as in anova( lm(Y ~ X1*X2) ).
> >>>
> >>> *** This is right, isn't it? ***
> >>>
> >>
> >> The main problem is testing for a main effect in the presence of
> >> interaction. While it looks like it gives sensible results in some
> >> cases like balanced ANOVA, they really aren't sensible and the effect
> >> of parameterisation in other cases makes that clear.
> >>
> >> The difference for the interaction is probably just sampling
> >> variation, increasing samples fixes this.
> >>
> >> Ken
> >
> > Ken is correct - testing some of the main effect terms resulting from
> > different parameterizations due to the differing contrast structures
> > will yield different results (though they in general will be somewhat
> > meaningless if the corresponding interaction term is in the model
> > and you do not have a balanced orthogonal design).
> >
> 
> Even with orthogonal designs there is still a problem with  
> interpretation. If we have a model with A*B and the interaction and B  
> are significant, then it seems that the conclusion about B is limited  
> to the choice of A in the experiment.  An assumption that the effect  
> of B will be the same with a different set of A seems rather risky,  
> although it seems to be what the FDA expect for multi-centre trials.  
> If there is some need to generalise then a model allowing for  a  
> random effect for B seems more sensible.
> 
> Ken
> 

Yes, in general if an interaction A*B is significant, then both
main effects that comprise the interaction are 'significant' in that
they are the variables that define the significant interaction.  The
interaction is significant because the relationship between the
reponse and one of the main effect terms depends on the level of the
other main effect term, so it is important that both main effects
remain in the model to allow the model to properly characterize this
complex relationship among the three variables involved.

Deleting either main effect from a model containing the interaction is
rarely advisable as doing so can yield biased estimates of the
interaction terms.  If the interaction term is significant, no more
testing need be done - both the main effects that comprise the
interaction are important and necessary.  Testing the main effects
that comprise a significant interaction using models that contain the
interaction seldom makes sense.

The second issue you raise involves extrapolating beyond the range of
the available data.  Conclusions about A and B are indeed limited to
the range of values covered by variables A and B.  An assumption that
the effect of B will be the same with a different set of A is indeed
rather risky.  Only if one can soundly argue that the set of
institutions in a multicentre trial truly reflects the full range of
populations that will be offered the treatment under study can one
make such generalizations.

Steve



From HStevens at MUOhio.edu  Wed Feb 27 18:42:38 2008
From: HStevens at MUOhio.edu (Hank Stevens)
Date: Wed, 27 Feb 2008 12:42:38 -0500
Subject: [R-sig-ME] mcmcpvalue and contrasts
In-Reply-To: <0BE438149FF2254DB4199E2682C8DFEB0328A1BB@crcmail1.BCCRC.CA>
References: <3745691C-08C0-43CA-8530-DEF1E263A66E@MUOhio.edu>
	<955BDCBE-F62D-49EB-BD22-94B3676AF007@kagi.com>
	<0BE438149FF2254DB4199E2682C8DFEB0328A1B7@crcmail1.BCCRC.CA>
	<976245B6-4845-4C74-839E-C606E44ABEBF@kagi.com>
	<0BE438149FF2254DB4199E2682C8DFEB0328A1BB@crcmail1.BCCRC.CA>
Message-ID: <CBADC985-322C-4BDB-8EAB-70B6D116B08B@MUOhio.edu>


On Feb 26, 2008, at 2:21 PM, Steven McKinney wrote:

>
>> -----Original Message-----
>> From: Ken Beath [mailto:kjbeath at kagi.com]
>> Sent: Mon 2/25/2008 8:46 PM
>> To: Steven McKinney
>> Cc: Hank Stevens; Help Mixed Models
>> Subject: Re: [R-sig-ME] mcmcpvalue and contrasts
>>
>> On 26/02/2008, at 1:22 PM, Steven McKinney wrote:
>>
>>>
>>> Hi Hank,
>>>
>>>>
>>>> -----Original Message-----
>>>> From: r-sig-mixed-models-bounces at r-project.org on behalf of Ken  
>>>> Beath
>>>> Sent: Mon 2/25/2008 4:05 PM
>>>> To: Hank Stevens
>>>> Cc: Help Mixed Models
>>>> Subject: Re: [R-sig-ME] mcmcpvalue and contrasts
>>>>
>>>> On 26/02/2008, at 9:42 AM, Hank Stevens wrote:
>>>>
>>>>> Hi Folks,
>>>>> I wanted to double check that my intuition makes sense.
>>>>>
>>>>> Examples of mcmcpvalue that I have seen use treatment "contrast"
>>>>> coding.
>>>>> However, in more complex designs, testing overall effects of a
>>>>> factor
>>>>> might be better done with other contrasts, such as sum or Helmert
>>>>> contrasts.
>>>>>
>>>>> My Contention:
>>>>> Different contrasts test different hypothesis, and therefore
>>>>> result in
>>>>> different P-values. This consequence of contrasts differs from
>>>>> analysis of variance, as in anova( lm(Y ~ X1*X2) ).
>>>>>
>>>>> *** This is right, isn't it? ***
>>>>>
>>>>
>>>> The main problem is testing for a main effect in the presence of
>>>> interaction. While it looks like it gives sensible results in some
>>>> cases like balanced ANOVA, they really aren't sensible and the  
>>>> effect
>>>> of parameterisation in other cases makes that clear.
>>>>
>>>> The difference for the interaction is probably just sampling
>>>> variation, increasing samples fixes this.
>>>>
>>>> Ken
>>>
>>> Ken is correct - testing some of the main effect terms resulting  
>>> from
>>> different parameterizations due to the differing contrast structures
>>> will yield different results (though they in general will be  
>>> somewhat
>>> meaningless if the corresponding interaction term is in the model
>>> and you do not have a balanced orthogonal design).
>>>
>>
>> Even with orthogonal designs there is still a problem with
>> interpretation. If we have a model with A*B and the interaction and B
>> are significant, then it seems that the conclusion about B is limited
>> to the choice of A in the experiment.  An assumption that the effect
>> of B will be the same with a different set of A seems rather risky,
>> although it seems to be what the FDA expect for multi-centre trials.
>> If there is some need to generalise then a model allowing for  a
>> random effect for B seems more sensible.
>>
>> Ken
>>
>
> Yes, in general if an interaction A*B is significant, then both
> main effects that comprise the interaction are 'significant' in that
> they are the variables that define the significant interaction.  The
> interaction is significant because the relationship between the
> reponse and one of the main effect terms depends on the level of the
> other main effect term, so it is important that both main effects
> remain in the model to allow the model to properly characterize this
> complex relationship among the three variables involved.
>
> Deleting either main effect from a model containing the interaction is
> rarely advisable as doing so can yield biased estimates of the
> interaction terms.  If the interaction term is significant, no more
> testing need be done - both the main effects that comprise the
> interaction are important and necessary.  Testing the main effects
> that comprise a significant interaction using models that contain the
> interaction seldom makes sense.

I have found, however, that it is often, yea, even more often, the  
case that one factor, A, has a large independent effect, and that  
another factor, B, moderates the effect of A to a small, albeit  
detectable,  degree. In these cases, it makes biological sense to  
discuss the "independent" effect of A. Whether one cloaks this in an  
"average" effect of A, or states that the effect of A is "at least b0,  
and as high as b0+b1, given some value of B."  I understand the  
caution you express above -- I just did't want to see the baby go  
along with the bath water.

More importantly, My own ignorance typically leaves me uncertain  
regarding the best approaches for estimating the independent  
contributions of A, B, and A:B, and inferring statistical significance  
using mcmcpvalue.  Using dummy coding (treatment 'contrasts') is  
frequently a useful approach for me with very simple models (e.g. A  
and B, each with two levels, plus only one random effect). However,   
my informal trial-and-error suggests that Helmert contrasts, and to a  
lesser extent sum-to-zero contrasts, provide more stable (and maybe  
reliable) inference when trying to test whether factors and their  
interactions have detectable effects under various selected models.

For instance, given nested models

m3 <- lmer(y ~ (A + B + C)^3 + (1|D) )
m2 <- lmer(y ~ (A + B + C)^2 + (1|D) )
m1 <- lmer(y ~ A + B + C + (1|D) )

Helmert contrasts seem to more precisely estimate the same beta's and  
the same P values in mcmcpvalue than sum contrasts, and these are both  
less variable across models than treatment contrasts. Perhaps this is  
a function of my particular selection of data sets (see previous  
posts), but I didn't think so, based on the different interpretations   
of the different (non)orthogonal contrasts.

Hank

>
>
> The second issue you raise involves extrapolating beyond the range of
> the available data.  Conclusions about A and B are indeed limited to
> the range of values covered by variables A and B.  An assumption that
> the effect of B will be the same with a different set of A is indeed
> rather risky.  Only if one can soundly argue that the set of
> institutions in a multicentre trial truly reflects the full range of
> populations that will be offered the treatment under study can one
> make such generalizations.
>
> Steve



Dr. Hank Stevens, Assoicate Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.cas.muohio.edu/ecology
http://www.muohio.edu/botany/

"If the stars should appear one night in a thousand years, how would men
believe and adore." -Ralph Waldo Emerson, writer and philosopher  
(1803-1882)



From r.j.forsyth at newcastle.ac.uk  Thu Feb 28 11:56:38 2008
From: r.j.forsyth at newcastle.ac.uk (Rob Forsyth)
Date: Thu, 28 Feb 2008 10:56:38 +0000
Subject: [R-sig-ME] Richards growth curve function
In-Reply-To: <mailman.5.1204110001.11439.r-sig-mixed-models@r-project.org>
References: <mailman.5.1204110001.11439.r-sig-mixed-models@r-project.org>
Message-ID: <0C2792B2-F8DE-40E8-8DC0-FA7976E94186@newcastle.ac.uk>

Have any list members experience of creating an nlme SelfStarting  
function based on the Richards growth curve function W=A(1-b*Exp(- 
kt))^M which is a more general form of the logistic and other functions?

Thanks

Rob Forsyth



From nikko at hailmail.net  Fri Feb 29 01:28:13 2008
From: nikko at hailmail.net (Nicholas Lewin-Koh)
Date: Thu, 28 Feb 2008 19:28:13 -0500
Subject: [R-sig-ME] Richards growth curve function
In-Reply-To: <mailman.3.1204196401.30457.r-sig-mixed-models@r-project.org>
References: <mailman.3.1204196401.30457.r-sig-mixed-models@r-project.org>
Message-ID: <1204244893.24788.1239730453@webmail.messagingengine.com>


Look in the package drc. they have their own version of a selfstart
function
that can easily be adapted to a SelfStart function.

Nicholas
> ------------------------------
> 
> Message: 2
> Date: Thu, 28 Feb 2008 10:56:38 +0000
> From: Rob Forsyth <r.j.forsyth at newcastle.ac.uk>
> Subject: [R-sig-ME] Richards growth curve function
> To: r-sig-mixed-models at r-project.org
> Message-ID: <0C2792B2-F8DE-40E8-8DC0-FA7976E94186 at newcastle.ac.uk>
> Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes
> 
> Have any list members experience of creating an nlme SelfStarting  
> function based on the Richards growth curve function W=A(1-b*Exp(- 
> kt))^M which is a more general form of the logistic and other functions?
> 
> Thanks
> 
> Rob Forsyth
> 
> 
> 
> ------------------------------
> 
> _______________________________________________
> R-sig-mixed-models mailing list
> R-sig-mixed-models at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> End of R-sig-mixed-models Digest, Vol 14, Issue 26
> **************************************************



From mwkimpel at gmail.com  Fri Feb 29 18:29:25 2008
From: mwkimpel at gmail.com (Mark W Kimpel)
Date: Fri, 29 Feb 2008 12:29:25 -0500
Subject: [R-sig-ME] need help with mixed effects model
In-Reply-To: <40e66e0b0802241145i7ff2888al44cfa6101b735986@mail.gmail.com>
References: <47BF0D00.2030506@gmail.com>
	<40e66e0b0802241145i7ff2888al44cfa6101b735986@mail.gmail.com>
Message-ID: <47C840F5.60200@gmail.com>

Doug and other mixed-models aficionados,

I have made some progress on my own on the problem I posted in this 
thread. Briefly, I am analyzing a multifactoral genomic experiment and 
wish to look at gene-gene correlations independent of Strain. Because 
multiple measurements are taken per rat, I wish to use lmer. What seems 
to be working is the following.

mod1 <- lmer(gene2 ~ -1 + Strain + (1|Rat) + gene1)
mod2 <- lmer(gene2 ~ -1 + Strain + (1|Rat))
anova.sum <- anova(mod1, mod2)

I look to see if adding the expression of the other gene of interest as 
a covariate significantly improves the model, if it does, then I take 
that as an indicator of gene-gene correlation/dependence.

I am not doing this, of course for just two genes, but build an 
adjacency matrix out of the p-values for all gene-gene interactions in a 
list of about 400 sig. genes. I then adjust the p-values for FDR and 
pick a suitable FDR (0.001 in this case) as a threshold and create 
another adjacency matrix with 1's for significant correlation and 0's 
for non-significant. I then visualize this using Rgraphviz.

As I was tearing my hair out trying to make sure this was sensical, it 
occurred to me that within my list of 400 genes I have positive 
controls. About 40 of the genes are represented by 2 or more probesets, 
which should be highly correlated if they are measuring the same thing. 
So, I subjected just genes with duplicate probesets to the above 
procedure and, sure enough, in an overwhelming number of cases, 
probesets from the same gene plot next to each other.

My conclusion from this exercise is that what I am doing is empirically 
correct, although I am open to suggestions as to how it could be 
improved or comments as to how I may be just plain wrong.

Doug, I am reading your book and appreciate your contributions.

Mark

Mark W. Kimpel MD  ** Neuroinformatics ** Dept. of Psychiatry
Indiana University School of Medicine

15032 Hunter Court, Westfield, IN  46074

(317) 490-5129 Work, & Mobile & VoiceMail
(317) 204-4202 Home (no voice mail please)

mwkimpel<at>gmail<dot>com

******************************************************************


Douglas Bates wrote:
> On Fri, Feb 22, 2008 at 11:57 AM, Mark W Kimpel <mwkimpel at gmail.com> wrote:
>> This is my first foray into in mixed models and, while awaiting the
>>  arrival of:
> 
>>  Extending the Linear Model with R: Generalized Linear, Mixed Effects
>>  and     Nonparametric Regression Models
>>  Mixed Effects Models in S and S-Plus
> 
>>  I am in need to some advice.
> 
>>  I would like to look at gene-gene correlations within a multi-factorial,
>>  mixed effects experiment. Here are the factors, with levels:
> 
>>  Gene Expression: 2 different genes per Animal, continuous variable
>>  Animals: 6 per Strain
>>  Tissues: 3 per animal
> 
>> Strain: 2
> 
>>  I thus have 6*3*2 = 36 samples
> 
>>  I do not care, for this analysis, about differences between Tissues,
>>  Strains, or Animals, in fact, I want to control for them while examining
>>  the correlation of expression of the two genes. In other words, I want
>>  look at something very much like the Pearson correlation coefficient
>>  controlled for these other factors.
> 
>>  I guess the first question I should ask is: "is a mixed model the way to
>>  go, and, if not, what would be the correct approach?"
> 
> Perhaps.  How do you plan to incorporate the two genes?
> 
>>  Assuming mixed models will work, as I see it through my newbie eyes,
>>  Tissue and strain are fixed effects and animals are random effects.
> 
> If you were interested in just 1 gene than I would say that this looks
> like a good approach.  I'm just not sure what to do about the multiple
> genes.
> 
>>  Any suggestions for an approach and a model?
> 
> The model specification (assuming that each animal has a distinct
> number) would be something like
> 
> gene1 ~ Tissue * Strain + (1|Animal)
> 
> In your earlier message to the Bioconductor list you had a
> specification that looked like
> 
> gene1 ~ gene2 + ...
> 
> which makes me a little queasy because you are assuming that gene2 is
> "known" relative to the variability in gene1 and most of the time that
> is not a reasonable approach.
>



From kjbeath at kagi.com  Fri Feb 29 23:25:15 2008
From: kjbeath at kagi.com (Ken Beath)
Date: Sat, 1 Mar 2008 09:25:15 +1100
Subject: [R-sig-ME] mcmcpvalue and contrasts
In-Reply-To: <CBADC985-322C-4BDB-8EAB-70B6D116B08B@MUOhio.edu>
References: <3745691C-08C0-43CA-8530-DEF1E263A66E@MUOhio.edu>
	<955BDCBE-F62D-49EB-BD22-94B3676AF007@kagi.com>
	<0BE438149FF2254DB4199E2682C8DFEB0328A1B7@crcmail1.BCCRC.CA>
	<976245B6-4845-4C74-839E-C606E44ABEBF@kagi.com>
	<0BE438149FF2254DB4199E2682C8DFEB0328A1BB@crcmail1.BCCRC.CA>
	<CBADC985-322C-4BDB-8EAB-70B6D116B08B@MUOhio.edu>
Message-ID: <DC4A4082-77FE-4F60-9869-5A2D8D0F3C5B@kagi.com>

On 28/02/2008, at 4:42 AM, Hank Stevens wrote:
>

<snip>

> I have found, however, that it is often, yea, even more often, the  
> case that one factor, A, has a large independent effect, and that  
> another factor, B, moderates the effect of A to a small, albeit  
> detectable,  degree. In these cases, it makes biological sense to  
> discuss the "independent" effect of A. Whether one cloaks this in an  
> "average" effect of A, or states that the effect of A is "at least  
> b0, and as high as b0+b1, given some value of B."  I understand the  
> caution you express above -- I just did't want to see the baby go  
> along with the bath water.
>

There is nothing wrong with calculating an average effect, just that  
it needs to be explicit that it is for a certain population. Most  
programs allow for calculating combinations of parameter estimates,  
but I can't see anything to do this directly in R, so maybe it's  
expected that the user works out the linear algebra. One thing that  
helps is to reparameterise the model, so rather than fitting an  
interaction, the estimates are for the effect of A for the levels of  
B, so that the calculations are for a weighted sum.

Ken



From bates at stat.wisc.edu  Sat Mar  1 14:04:24 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 1 Mar 2008 07:04:24 -0600
Subject: [R-sig-ME] New alpha test release (0.999375-5) with mcmcsamp for
	linear mixed models
Message-ID: <40e66e0b0803010504n5c37b901s36036bd1abeaaf26@mail.gmail.com>

Yesterday I uploaded the files for alpha test release 0.999375-5 to
R-forge.R-project.org

Packages should be built and tested overnight.  I understand from
announcements by the R-forge administration team that binary packages
for Mac OS X should now be available through

install.packages("lme4", repos = "http://r-forge.r-project.org")

although I haven't tested this on a Mac myself.

The "show stopper" for the release of version 1.0.0 of the lme4
package has been getting mcmcsamp running again.  I have made dramatic
changes to the underlying structure of the objects representing
mixed-effects models and I wanted to be able to have mcmcsamp running
in this new formulation before an official release of a new version.

The good news is that I have mcmcsamp running for linear mixed models
and I think the samples look as they should.  I would, however, very
much appreciate others testing this function and checking if the
results are consistent with earlier implementations.  Because they are
generated by a pseudo-random mechanism in a different algorithm they
will not be identical to earlier results but the distribution should
be similar.

The bad news is that I changed the representation of the result of
mcmcsamp and this will have ramifications for code maintained by
others.  Harald Baayen and Andrew Gelman's group in particular may
want to check the new representation of the results from MCMCsamp.
The merMCMC class is an S4 class whose slots are described in the
documentation

class?merMCMC

The sample from the distribution of the fixed-effects parameters are a
matrix in a slot called "fixef".  Right now the iterations correspond
to columns - it may be more natural to transpose that matrix when
examining it.  The first sample is the fitted value of the fixed
effects.  The "sigma" slot is the corresponding sample of the common
scale parameter.  The variance-covariance of the random effects is
derived from the "sigma" slot and the "ST" slot.

To decode the representation in the ST slot you need the "nc" (number
of columns) slot which is an integer vector giving the number of
columns in the random effects matrices corresponding to each random
effects term in the model. I write those as q_i, i = 1, ..., k.  A
column of the matrix in the ST slot is of length np = \sum_{i=1}^k
q_i(q_i + 1)/2.  When divided into these subvectors, the first q_i
elements of a subvector are the non-negative scale factors from the
diagonal of the diagonal matrix S_i and the remaining elements are the
elements below the diagonal in the unit lower triangular matrix T_i.
S_i, T_i and sigma generate the variance-covariance matrix for the
random effects corresponding to the ith term as

sigma^2 * T_i %*% S_i %*% S_i %*% t(T_i)

See http://www.stat.wisc.edu/~bates/2008-02-22-Emory.pdf for a
worked-out example of this calculation.

I will create the support functions such as HPDinterval and methods
for generics like xyplot and densityplot for this class.  First I want
to get mcmcsamp working for GLMMs and NLMMs.



From HStevens at MUOhio.edu  Mon Mar  3 14:44:43 2008
From: HStevens at MUOhio.edu (Hank Stevens)
Date: Mon, 3 Mar 2008 08:44:43 -0500
Subject: [R-sig-ME] mcmcpvalue and contrasts
In-Reply-To: <DC4A4082-77FE-4F60-9869-5A2D8D0F3C5B@kagi.com>
References: <3745691C-08C0-43CA-8530-DEF1E263A66E@MUOhio.edu>
	<955BDCBE-F62D-49EB-BD22-94B3676AF007@kagi.com>
	<0BE438149FF2254DB4199E2682C8DFEB0328A1B7@crcmail1.BCCRC.CA>
	<976245B6-4845-4C74-839E-C606E44ABEBF@kagi.com>
	<0BE438149FF2254DB4199E2682C8DFEB0328A1BB@crcmail1.BCCRC.CA>
	<CBADC985-322C-4BDB-8EAB-70B6D116B08B@MUOhio.edu>
	<DC4A4082-77FE-4F60-9869-5A2D8D0F3C5B@kagi.com>
Message-ID: <D32B82B4-87D5-4A45-9955-3D2B28B4A655@MUOhio.edu>

Hi Ken,
Have you seen the multicomp package? I found it pretty useful. Others  
have written packages that include their favorite ways of coding  
contrasts including packages Hmisc, gmodels, and contrast.
Hank
On Feb 29, 2008, at 5:25 PM, Ken Beath wrote:

> On 28/02/2008, at 4:42 AM, Hank Stevens wrote:
>>
>
> <snip>
>
>> I have found, however, that it is often, yea, even more often, the
>> case that one factor, A, has a large independent effect, and that
>> another factor, B, moderates the effect of A to a small, albeit
>> detectable,  degree. In these cases, it makes biological sense to
>> discuss the "independent" effect of A. Whether one cloaks this in an
>> "average" effect of A, or states that the effect of A is "at least
>> b0, and as high as b0+b1, given some value of B."  I understand the
>> caution you express above -- I just did't want to see the baby go
>> along with the bath water.
>>
>
> There is nothing wrong with calculating an average effect, just that
> it needs to be explicit that it is for a certain population. Most
> programs allow for calculating combinations of parameter estimates,
> but I can't see anything to do this directly in R, so maybe it's
> expected that the user works out the linear algebra. One thing that
> helps is to reparameterise the model, so rather than fitting an
> interaction, the estimates are for the effect of A for the levels of
> B, so that the calculations are for a weighted sum.
>
> Ken



Dr. Hank Stevens, Associate Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.cas.muohio.edu/ecology
http://www.muohio.edu/botany/

"If the stars should appear one night in a thousand years, how would men
believe and adore." -Ralph Waldo Emerson, writer and philosopher  
(1803-1882)



From bates at stat.wisc.edu  Mon Mar  3 19:09:19 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 3 Mar 2008 12:09:19 -0600
Subject: [R-sig-ME] Crash fitting lmer in lme4_0.999375-4
In-Reply-To: <40e66e0b0803030816r3cce3482r2fea7b74c4160dc7@mail.gmail.com>
References: <a072ed700802200918v1dfd65f1m48f4e91b16b85bab@mail.gmail.com>
	<40e66e0b0802201547m6fc4bac1m8c8b31849b45cb32@mail.gmail.com>
	<a072ed700803030418k5ac9124w10da799bd2d62cb0@mail.gmail.com>
	<40e66e0b0803030816r3cce3482r2fea7b74c4160dc7@mail.gmail.com>
Message-ID: <40e66e0b0803031009g179b635mf9297b7aa1f0a0a6@mail.gmail.com>

On Mon, Mar 3, 2008 at 10:16 AM, Douglas Bates <bates at stat.wisc.edu> wrote:
> On Mon, Mar 3, 2008 at 6:18 AM, Nick Isaac <njbisaac at googlemail.com> wrote:
>  > I just installed lme4_0.999375-5 and the problem has disappeared. Thanks!
>
>  Hmm - interesting.  When I try the example using that version of the
>  lme4 package it segfaults.
>
>  I'm debugging now.  Thanks for the example.  I'm pretty sure it has to
>  do with the use of the weights and that should help to isolate the
>  problem.

There were indeed a couple of dumb programming mistakes that caused
segfaults when fitting a linear mixed model with weights.  After
having fixed those I can get your example to run but the results
(enclosed) don't look correct.  I think the problem is that I am using
the penalized, weighted residual sum of squares to evaluate the
conditional estimate of the common scale parameter (i.e. sigma)
without taking into account the sum of the weights.

I already have a FIXME comment in the code that I should evaluate and
store sigma in a different way so I will make that change and check on
the way that sigma should be estimated at the same time.



>  > On 20/02/2008, Douglas Bates <bates at stat.wisc.edu> wrote:
>  > > Thanks for the report Nick and thanks for including the data so that
>  > > we can reproduce the problem.
>  > >
>  > > I don't think I will have time to look at it until next week.  I am
>  > > travelling for the rest of the week (and, naturally, still need to
>  > > finish my presentation).
>  > >
>  > >
>  > > On Feb 20, 2008 11:18 AM, Nick Isaac <njbisaac at googlemail.com> wrote:
>  > > > Dear all,
>  > > >
>  > > > I just installed lme4_0.999375-4 and am sad to report unexpected
>  > behaviour
>  > > > the first time I ran lmer.
>  > > >
>  > > > > sessionInfo()
>  > > > R version 2.6.2 (2008-02-08)
>  > > > i386-apple-darwin8.10.1
>  > > >
>  > > > locale:
>  > > > en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8
>  > > >
>  > > > attached base packages:
>  > > > [1] stats     graphics  grDevices utils     datasets  methods   base
>  > > >
>  > > > other attached packages:
>  > > > [1] lme4_0.999375-4   Matrix_0.999375-4 lattice_0.17-4
>  > > >
>  > > > loaded via a namespace (and not attached):
>  > > > [1] grid_2.6.2
>  > > >
>  > > >
>  > > >
>  > > > The dataset is attached. A call to:
>  > > >
>  > > > data<-read.table("ESW_080220.txt",sep="\t",header=T)
>  > > > data$W<- 5-log(1+data$CV)
>  > > > full1.w.mm <- lmer(ESW ~ Wingspan * Dataset * VCS + (1|Site) +
>  > (1|Species),
>  > > > data=data, weights=W)
>  > > >
>  > > >
>  > > >
>  > > > Produces a crash with the following message:
>  > > >
>  > > >  *** caught bus error ***
>  > > > address 0x0, cause 'non-existent physical address'
>  > > >
>  > > > Traceback:
>  > > >  1: .Call(mer_optimize, ans, verbose)
>  > > >  2: mer_finalize(ans, verbose)
>  > > >  3: lmer(ESW ~ Wingspan * Dataset * VCS + (1 | Site) + (1 | Species),
>  > > > data = data, weights = W)
>  > > >
>  > > >
>  > > >
>  > > > I repeated the exercise on another machine, running  the version on
>  > CRAN:
>  > > >
>  > > >
>  > > > > sessionInfo()
>  > > > R version 2.6.2 (2008-02-08)
>  > > > i386-apple-darwin8.10.1
>  > > >
>  > > > locale:
>  > > > en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8
>  > > >
>  > > > attached base packages:
>  > > > [1] stats     graphics  grDevices utils     datasets  methods   base
>  > > >
>  > > > other attached packages:
>  > > > [1] lme4_0.99875-9    Matrix_0.999375-4 lattice_0.17-4
>  > > >
>  > > >
>  > > >
>  > > >
>  > > > The lmer fit the model with a warning:
>  > > >
>  > > > Warning message:
>  > > > In .local(x, ..., value) :
>  > > >   Estimated variance for factor 'Site' is effectively zero
>  > > >
>  > > >
>  > > >
>  > > > The warning reflects the fact that this model is rather over-specified.
>  > > > However, the behaviour in the development version is rather unexpected.
>  > > > Best wishes, Nick Isaac
>  > > >
>  > >
>  > > > _______________________________________________
>  > > > R-sig-mixed-models at r-project.org mailing list
>  > > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>  > > >
>  > > >
>  > >
>  >
>  >
>

From martin.eklund at farmbio.uu.se  Mon Mar  3 23:21:05 2008
From: martin.eklund at farmbio.uu.se (Martin Eklund)
Date: Mon, 3 Mar 2008 23:21:05 +0100
Subject: [R-sig-ME]  / vs : operators in model specifications
Message-ID: <74C8B4D5-0AA5-4145-92A3-DD47A35AE1CD@farmbio.uu.se>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080303/a8ad0d49/attachment.pl>

From bates at stat.wisc.edu  Tue Mar  4 00:12:35 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 3 Mar 2008 17:12:35 -0600
Subject: [R-sig-ME] / vs : operators in model specifications
In-Reply-To: <74C8B4D5-0AA5-4145-92A3-DD47A35AE1CD@farmbio.uu.se>
References: <74C8B4D5-0AA5-4145-92A3-DD47A35AE1CD@farmbio.uu.se>
Message-ID: <40e66e0b0803031512y1f96c4adqeb826c3516e85d8e@mail.gmail.com>

On Mon, Mar 3, 2008 at 4:21 PM, Martin Eklund
<martin.eklund at farmbio.uu.se> wrote:
> Hi all,

>  I'm new to mixed models and lme4 and would appreciate any help on
>  specifying models in lme4. Specifically I'm wondering about
>  specifying nested variables. As I understand it lmer is capable of
>  determining the nesting structure by itself (as long as the nesting
>  is not implicit in the factors), so is there ever a need for
>  specifying the nesting structure in the model specification when
>  using lmer? Assuming A and B are random factors, B is nested in A,
>  and Y is a response. What would be the difference between the
>  following model specifications?
>
>  (i)     lmer(Y ~ (1|A) + (1|B))
>  (ii)    lmer(Y ~ (1|A) + (1|A:B))
>  (iii)   lmer(Y ~ (1|A) + (1|A/B))

They should be the same provided that the levels of B are not
implicitly nested in the levels of A, which is what you indicated
would be the case.

The specification (1|A) + (1|A/B) is redundant as (1|A/B) expands to
(1|A) + (1|A:B) so (iii) could be written as

lmer(Y ~ 1|A/B)

>  On a different note: using the mcmcsamp method, is it possible to
>  change the choice of priors? I understand that this is a longshot and
>  that it is most likely not possible, but I'm still asking just to be
>  sure. Is there another way this can be done (i.e. sampling from the
>  posterior distribution of a mixed model but with a general choice of
>  prior)?

Unfortunately, no, there is no convenient way of specifying an
arbitrary prior for the mcmcsamp function.  To make it practical to
use that function routinely, the sampling scheme is hard-wired into
compiled code and based upon having an improper, locally uniform prior
on the fixed effects and improper, locally uniform priors on the
logarithms of the variance components.
>  Thank you very much!
>
>  Best regards,
>
>  Martin.
>
>  ========================================
>  Martin Eklund
>  PhD Student
>  Department of Pharmaceutical Biosciences
>  Uppsala University, Sweden
>  Ph: +46-18-4714281
>  ========================================
>
>
>
>
>
>         [[alternative HTML version deleted]]
>
>  _______________________________________________
>  R-sig-mixed-models at r-project.org mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From kjbeath at kagi.com  Tue Mar  4 01:00:45 2008
From: kjbeath at kagi.com (Ken Beath)
Date: Tue, 4 Mar 2008 11:00:45 +1100
Subject: [R-sig-ME] need help with mixed effects model
In-Reply-To: <47C840F5.60200@gmail.com>
References: <47BF0D00.2030506@gmail.com>
	<40e66e0b0802241145i7ff2888al44cfa6101b735986@mail.gmail.com>
	<47C840F5.60200@gmail.com>
Message-ID: <372C3D38-56B9-4B2D-A1D5-07344A7C0C93@kagi.com>

On 01/03/2008, at 4:29 AM, Mark W Kimpel wrote:

> Doug and other mixed-models aficionados,
>
> I have made some progress on my own on the problem I posted in this
> thread. Briefly, I am analyzing a multifactoral genomic experiment and
> wish to look at gene-gene correlations independent of Strain. Because
> multiple measurements are taken per rat, I wish to use lmer. What  
> seems
> to be working is the following.
>
> mod1 <- lmer(gene2 ~ -1 + Strain + (1|Rat) + gene1)
> mod2 <- lmer(gene2 ~ -1 + Strain + (1|Rat))
> anova.sum <- anova(mod1, mod2)
>
> I look to see if adding the expression of the other gene of interest  
> as
> a covariate significantly improves the model, if it does, then I take
> that as an indicator of gene-gene correlation/dependence.
>

The concern that Doug had is I assume that gene1 and gene2 are both  
measured with error, and this type of model assumes that the  
covariates are measured without error or for practical purposes much  
lower than the error in the dependent variable. Ignoring this problem  
biases the coefficients towards zero with consequent loss of power. I  
don't have any idea how important this is, it all depends on the error  
of your measurements. The usual solution is structural equation  
modelling (SEM). This is something I haven't tried, so I have no idea  
how easy or how well it will work.

Ken

> I am not doing this, of course for just two genes, but build an
> adjacency matrix out of the p-values for all gene-gene interactions  
> in a
> list of about 400 sig. genes. I then adjust the p-values for FDR and
> pick a suitable FDR (0.001 in this case) as a threshold and create
> another adjacency matrix with 1's for significant correlation and 0's
> for non-significant. I then visualize this using Rgraphviz.
>
> As I was tearing my hair out trying to make sure this was sensical, it
> occurred to me that within my list of 400 genes I have positive
> controls. About 40 of the genes are represented by 2 or more  
> probesets,
> which should be highly correlated if they are measuring the same  
> thing.
> So, I subjected just genes with duplicate probesets to the above
> procedure and, sure enough, in an overwhelming number of cases,
> probesets from the same gene plot next to each other.
>
> My conclusion from this exercise is that what I am doing is  
> empirically
> correct, although I am open to suggestions as to how it could be
> improved or comments as to how I may be just plain wrong.
>
> Doug, I am reading your book and appreciate your contributions.
>
> Mark
>
> Mark W. Kimpel MD  ** Neuroinformatics ** Dept. of Psychiatry
> Indiana University School of Medicine
>
> 15032 Hunter Court, Westfield, IN  46074
>
> (317) 490-5129 Work, & Mobile & VoiceMail
> (317) 204-4202 Home (no voice mail please)
>
> mwkimpel<at>gmail<dot>com
>
> ******************************************************************
>
>
> Douglas Bates wrote:
>> On Fri, Feb 22, 2008 at 11:57 AM, Mark W Kimpel  
>> <mwkimpel at gmail.com> wrote:
>>> This is my first foray into in mixed models and, while awaiting the
>>> arrival of:
>>
>>> Extending the Linear Model with R: Generalized Linear, Mixed Effects
>>> and     Nonparametric Regression Models
>>> Mixed Effects Models in S and S-Plus
>>
>>> I am in need to some advice.
>>
>>> I would like to look at gene-gene correlations within a multi- 
>>> factorial,
>>> mixed effects experiment. Here are the factors, with levels:
>>
>>> Gene Expression: 2 different genes per Animal, continuous variable
>>> Animals: 6 per Strain
>>> Tissues: 3 per animal
>>
>>> Strain: 2
>>
>>> I thus have 6*3*2 = 36 samples
>>
>>> I do not care, for this analysis, about differences between Tissues,
>>> Strains, or Animals, in fact, I want to control for them while  
>>> examining
>>> the correlation of expression of the two genes. In other words, I  
>>> want
>>> look at something very much like the Pearson correlation coefficient
>>> controlled for these other factors.
>>
>>> I guess the first question I should ask is: "is a mixed model the  
>>> way to
>>> go, and, if not, what would be the correct approach?"
>>
>> Perhaps.  How do you plan to incorporate the two genes?
>>
>>> Assuming mixed models will work, as I see it through my newbie eyes,
>>> Tissue and strain are fixed effects and animals are random effects.
>>
>> If you were interested in just 1 gene than I would say that this  
>> looks
>> like a good approach.  I'm just not sure what to do about the  
>> multiple
>> genes.
>>
>>> Any suggestions for an approach and a model?
>>
>> The model specification (assuming that each animal has a distinct
>> number) would be something like
>>
>> gene1 ~ Tissue * Strain + (1|Animal)
>>
>> In your earlier message to the Bioconductor list you had a
>> specification that looked like
>>
>> gene1 ~ gene2 + ...
>>
>> which makes me a little queasy because you are assuming that gene2 is
>> "known" relative to the variability in gene1 and most of the time  
>> that
>> is not a reasonable approach.
>>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From martin.eklund at farmbio.uu.se  Tue Mar  4 18:46:52 2008
From: martin.eklund at farmbio.uu.se (Martin Eklund)
Date: Tue, 4 Mar 2008 18:46:52 +0100
Subject: [R-sig-ME] Another question about model specification
Message-ID: <E043950C-0886-4708-BFFC-E2AD74C32D80@farmbio.uu.se>

Hello again,

Thank you very much for the quick reply on my earlier question!  
Unfortunately, I still have problems to fully understand how to  
specify models in lme4. Specifically, how do you specify interactions  
between factors?

Consider the example data below (also attached for convenience).  
'treatment' and 'family' are factors, 'treatment' is fixed and  
'family' is random. Fitting the models

(i)	lmer(weight ~ treatment + (1|family))
(ii)	lmer(weight ~ treatment * (1|family))
(iii)	lmer(weight ~ (treatment + (1|family))^2)
(iv)	lmer(weight ~ treatment : (1|family))

produce exactly the same results. I assume that this is the expected  
behavior, but to me it is a bit confusing. If we pretend that  
'family' is a fixed factor and use lm to fit the models above we get  
different results (or, to be precise, (ii) = (iii) ? (i) ? (iv)),  
which is the expected behavior to me. So, my question really boils  
down to: if I want to fit the model

weight ~ treatment + (1|family) + (an interaction term between  
treatment and (1|family))

how would I specify that? And in general, how are interactions  
specified?

Thank you!

Best regards,

Martin.

==========
R commands
==========
data <- read.delim(file="/path/to/test.txt", sep="\t", header=TRUE)
data$treatment <- as.factor(data$treatment)
data$family <- as.factor(data$family)
attach(data)
lmer(weight ~ treatment + (1|family))
lmer(weight ~ treatment * (1|family))
lmer(weight ~ (treatment + (1|family))^2)
lmer(weight ~ treatment : (1|family))

===========
Example data
===========
treatment	family	weight
2	1	0.099
3	1	0.099
1	1	0.105
1	1	0.112
1	1	0.113
1	1	0.114
1	1	0.119
3	1	0.121
1	1	0.123
1	1	0.124
3	1	0.130
3	1	0.130
2	1	0.142
2	1	0.143
2	1	0.144
2	1	0.164
2	2	0.086
1	2	0.097
1	2	0.097
1	2	0.105
1	2	0.106
2	2	0.107
1	2	0.109
2	2	0.113
2	2	0.114
1	2	0.114
3	2	0.115
2	2	0.132
3	2	0.141
2	2	0.131

?

========================================
Martin Eklund
PhD Student
Department of Pharmaceutical Biosciences
Uppsala University, Sweden
Ph: +46-18-4714281
========================================






From bates at stat.wisc.edu  Tue Mar  4 21:42:42 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 4 Mar 2008 14:42:42 -0600
Subject: [R-sig-ME] Another question about model specification
In-Reply-To: <E043950C-0886-4708-BFFC-E2AD74C32D80@farmbio.uu.se>
References: <E043950C-0886-4708-BFFC-E2AD74C32D80@farmbio.uu.se>
Message-ID: <40e66e0b0803041242q3ded4b8k73cf6e0b556aaff6@mail.gmail.com>

On Tue, Mar 4, 2008 at 11:46 AM, Martin Eklund
<martin.eklund at farmbio.uu.se> wrote:
> Hello again,

>  Thank you very much for the quick reply on my earlier question!
>  Unfortunately, I still have problems to fully understand how to
>  specify models in lme4. Specifically, how do you specify interactions
>  between factors?

>  Consider the example data below (also attached for convenience).
>  'treatment' and 'family' are factors, 'treatment' is fixed and
>  'family' is random. Fitting the models

>  (i)     lmer(weight ~ treatment + (1|family))
>  (ii)    lmer(weight ~ treatment * (1|family))
>  (iii)   lmer(weight ~ (treatment + (1|family))^2)
>  (iv)    lmer(weight ~ treatment : (1|family))

>  produce exactly the same results. I assume that this is the expected
>  behavior, but to me it is a bit confusing.

I started writing a kind of a grumpy response to this inquiry because
the documentation for lmer specifically states that terms in the
formula are separated by '+' symbols, so exploring the possible
effects of other types of operators is not likely to be productive.

Nevertheless I will concede that specifying an interaction between a
fixed-effects factor and a random factor is not a transparent
operation.  There are two ways that such an interaction can be
specified.

First, an interaction between a fixed factor and a random factor is a
random effect.  That is, randomness propagates through interactions.
To specify an interaction between a fixed factor and a random factor
you must consider not only the form of the model matrix that results
but also the form of the variance-covariance matrix for the resulting
random effects.  You can specify the interaction as

lmer(weight ~ treatment + (1| family) + (1|family:treatment))

or as

lmer(weight ~ treatment + (treatment | family))

In the first specification you will estimate two variance components,
one for family and one for the family:treatment interaction.  This
assumes that the random effects for each level of the treatment within
a family are independent and have the same variance.

In the second specification you estimate a vector of random effects
for each family where the length of the vector is the number of
treatments.  Let's say there are m treatments.  Then you will estimate
a full m by m variance-covariance matrix for the random effects.  This
becomes problematic when m is large.

That is, the first model is simpler than the second - in fact, it is a
submodel of the second model.

Here is an example using the (probably fictitious) Machines data from
the MEMSS package.

> data(Machines, package = "MEMSS")
> str(Machines)
'data.frame':	54 obs. of  3 variables:
 $ Worker : Factor w/ 6 levels "1","2","3","4",..: 1 1 1 2 2 2 3 3 3 4 ...
 $ Machine: Factor w/ 3 levels "A","B","C": 1 1 1 1 1 1 1 1 1 1 ...
 $ score  : num  52 52.8 53.1 51.8 52.8 53.1 60 60.2 58.4 51.1 ...
> (Mm1 <- lmer(score ~ Machine + (1|Worker) + (1|Worker:Machine), Machines))
Linear mixed model fit by REML
Formula: score ~ Machine + (1 | Worker) + (1 | Worker:Machine)
   Data: Machines
   AIC   BIC logLik deviance REMLdev
 227.7 239.6 -107.8    225.5   215.7
Random effects:
 Groups         Name        Variance Std.Dev.
 Worker         (Intercept) 22.85892 4.78110
 Worker:Machine (Intercept) 13.90845 3.72940
 Residual                    0.92465 0.96159
Number of obs: 54, groups: Worker, 6; Worker:Machine, 18

Fixed effects:
            Estimate Std. Error t value
(Intercept)   52.356      2.486  21.062
MachineB       7.967      2.177   3.660
MachineC      13.917      2.177   6.393

Correlation of Fixed Effects:
         (Intr) MachnB
MachineB -0.438
MachineC -0.438  0.500
> (Mm2 <- lmer(score ~ Machine + (Machine|Worker), Machines))
Linear mixed model fit by REML
Formula: score ~ Machine + (Machine | Worker)
   Data: Machines
   AIC   BIC logLik deviance REMLdev
 228.3 248.2 -104.2    216.6   208.3
Random effects:
 Groups   Name        Variance Std.Dev. Corr
 Worker   (Intercept) 16.64051 4.07928
          MachineB    34.54670 5.87764   0.484
          MachineC    13.61398 3.68971  -0.365  0.297
 Residual              0.92463 0.96158
Number of obs: 54, groups: Worker, 6

Fixed effects:
            Estimate Std. Error t value
(Intercept)   52.356      1.681  31.151
MachineB       7.967      2.421   3.291
MachineC      13.917      1.540   9.037

Correlation of Fixed Effects:
         (Intr) MachnB
MachineB  0.463
MachineC -0.374  0.301
> anova(Mm2, Mm1)
Data: Machines
Models:
Mm1: score ~ Machine + (1 | Worker) + (1 | Worker:Machine)
Mm2: score ~ Machine + (Machine | Worker)
      Df     AIC     BIC  logLik  Chisq Chi Df Pr(>Chisq)
Mm1.p  6  237.47  249.40 -112.73
Mm2.p 10  236.61  256.50 -108.31 8.8515      4    0.06492


>  If we pretend that
>  'family' is a fixed factor and use lm to fit the models above we get
>  different results (or, to be precise, (ii) = (iii) ? (i) ? (iv)),
>  which is the expected behavior to me. So, my question really boils
>  down to: if I want to fit the model
>
>  weight ~ treatment + (1|family) + (an interaction term between
>  treatment and (1|family))
>
>  how would I specify that? And in general, how are interactions
>  specified?
>
>  Thank you!
>
>  Best regards,
>
>  Martin.
>
>  ==========
>  R commands
>  ==========
>  data <- read.delim(file="/path/to/test.txt", sep="\t", header=TRUE)
>  data$treatment <- as.factor(data$treatment)
>  data$family <- as.factor(data$family)
>  attach(data)
>  lmer(weight ~ treatment + (1|family))
>  lmer(weight ~ treatment * (1|family))
>  lmer(weight ~ (treatment + (1|family))^2)
>  lmer(weight ~ treatment : (1|family))
>
>  ===========
>  Example data
>  ===========
>  treatment       family  weight
>  2       1       0.099
>  3       1       0.099
>  1       1       0.105
>  1       1       0.112
>  1       1       0.113
>  1       1       0.114
>  1       1       0.119
>  3       1       0.121
>  1       1       0.123
>  1       1       0.124
>  3       1       0.130
>  3       1       0.130
>  2       1       0.142
>  2       1       0.143
>  2       1       0.144
>  2       1       0.164
>  2       2       0.086
>  1       2       0.097
>  1       2       0.097
>  1       2       0.105
>  1       2       0.106
>  2       2       0.107
>  1       2       0.109
>  2       2       0.113
>  2       2       0.114
>  1       2       0.114
>  3       2       0.115
>  2       2       0.132
>  3       2       0.141
>  2       2       0.131
>
>  ?
>
>  ========================================
>  Martin Eklund
>  PhD Student
>  Department of Pharmaceutical Biosciences
>  Uppsala University, Sweden
>  Ph: +46-18-4714281
>  ========================================
>
>
>
>
>  _______________________________________________
>  R-sig-mixed-models at r-project.org mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

From njbisaac at googlemail.com  Wed Mar  5 12:02:42 2008
From: njbisaac at googlemail.com (Nick Isaac)
Date: Wed, 5 Mar 2008 11:02:42 +0000
Subject: [R-sig-ME] need help with mixed effects model
In-Reply-To: <372C3D38-56B9-4B2D-A1D5-07344A7C0C93@kagi.com>
References: <47BF0D00.2030506@gmail.com>
	<40e66e0b0802241145i7ff2888al44cfa6101b735986@mail.gmail.com>
	<47C840F5.60200@gmail.com>
	<372C3D38-56B9-4B2D-A1D5-07344A7C0C93@kagi.com>
Message-ID: <a072ed700803050302g35631f48g13324d21fd721c26@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080305/d7161c7a/attachment.pl>

From HStevens at muohio.edu  Wed Mar  5 12:21:49 2008
From: HStevens at muohio.edu (MHH Stevens)
Date: Wed, 5 Mar 2008 06:21:49 -0500
Subject: [R-sig-ME] need help with mixed effects model
In-Reply-To: <a072ed700803050302g35631f48g13324d21fd721c26@mail.gmail.com>
References: <47BF0D00.2030506@gmail.com>
	<40e66e0b0802241145i7ff2888al44cfa6101b735986@mail.gmail.com>
	<47C840F5.60200@gmail.com>	<372C3D38-56B9-4B2D-A1D5-07344A7C0C93@kagi.com>
	<a072ed700803050302g35631f48g13324d21fd721c26@mail.gmail.com>
Message-ID: <D2E8BC88-67BC-433F-B419-1B90B1F3206D@muohio.edu>

Hi Nick,
You might want to look at the sem package.
Hank
On Mar 5, 2008, at 6:02 AM, Nick Isaac wrote:

> You can do SEM-type models using the smatr package. Only drawback  
> is that
> you'd have to treat Rat as a fixed effect. This is a general class of
> problem that afflicts several of my current projects, and I'm  
> having a tough
> time choosing between mixed effect and structural equation models. The
> former is most appropriate for partitioning the variance, but the  
> latter is
> most appropriate for modelling error variance in the observations.  
> I don't
> see an obvious solution with the available tools and would  
> appreciate any
> general insights.
>
> Cheers, Nick
>
>
> On 04/03/2008, Ken Beath <kjbeath at kagi.com> wrote:
>>
>> The concern that Doug had is I assume that gene1 and gene2 are both
>> measured with error, and this type of model assumes that the
>> covariates are measured without error or for practical purposes much
>> lower than the error in the dependent variable. Ignoring this problem
>> biases the coefficients towards zero with consequent loss of power. I
>> don't have any idea how important this is, it all depends on the  
>> error
>> of your measurements. The usual solution is structural equation
>> modelling (SEM). This is something I haven't tried, so I have no idea
>> how easy or how well it will work.
>>
>>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

Dr. Hank Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.cas.muohio.edu/ecology
http://www.muohio.edu/botany/

"If the stars should appear one night in a thousand years, how would men
believe and adore." -Ralph Waldo Emerson, writer and philosopher  
(1803-1882)



From David.Duffy at qimr.edu.au  Thu Mar  6 01:58:01 2008
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Thu, 6 Mar 2008 10:58:01 +1000 (EST)
Subject: [R-sig-ME] need help with mixed effects model
In-Reply-To: <a072ed700803050302g35631f48g13324d21fd721c26@mail.gmail.com>
References: <47BF0D00.2030506@gmail.com><40e66e0b0802241145i7ff2888al44cfa6101b735986@mail.gmail.com><47C840F5.60200@gmail.com><372C3D38-56B9-4B2D-A1D5-07344A7C0C93@kagi.com>
	<a072ed700803050302g35631f48g13324d21fd721c26@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0803060951080.695@orpheus.qimr.edu.au>

On Wed, 5 Mar 2008, Nick Isaac wrote:

> You can do SEM-type models using the smatr package. Only drawback is that
> you'd have to treat Rat as a fixed effect. This is a general class of
> problem that afflicts several of my current projects, and I'm having a tough
> time choosing between mixed effect and structural equation models. The
> former is most appropriate for partitioning the variance, but the latter is
> most appropriate for modelling error variance in the observations. I don't
> see an obvious solution with the available tools and would appreciate any
> general insights.
>
> Cheers, Nick
>

I don't know if smatr supports multiple groups (so you can have unbalanced 
type data).  We generally use the Mx SEM package, which allows both 
multiple groups as well as "irregular" data, and our main interest is always
in partitioning the variance.  Every mixed model (indeed 
every linear model) is expressible as a SEM (it just runs slower ;)).

Generally, if you want to include a random effect you set up a latent 
variable with appropriate path(s) to the observed value(s).  Your
relationships will be between the latent variables.  Often, you'll fix
the path coefficient to unity.  Have a look at Mx book/manual (Mike Neale and
Hermine Maes) and you'll get some ideas.

For the expression example, one model would be something like
                              +--------------------------+
                 Strain1-Rg1  |                          |
              --------------------------    Rat16        |
              |               |        |                 |
                    r1        |                 r1       |
Latent    Gene111 <-->  Gene112       Gene161 <-->  Gene162
          /b1   | \b3   b4/ | \       /b1    | \ 
Manif  Tissue1 T2 T3               Tissue1 T2 T3

With appropriate equality constraints (r1=r1..=r1, b1=b1..) or 
random regressions on the path coefficientsi and covariance matrix of latent
variables.  So this model states (and
can test) that tissue expressions are imperfect measures of
overall gene expression in the ith animal.

But after saying that, for this particular example, I would use a
program such as ASREML or Wombat or MENDEL that easily fit multivariate
mixed models.  Our group has used ASREML to fit mixture models to
deal with error in DNA pooling experiments for 50000 SNPs.

Hope that's clear (and even right), David Duffy.

-- 
| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v



From david.paez.1 at ulaval.ca  Thu Mar  6 17:42:54 2008
From: david.paez.1 at ulaval.ca (David Paez)
Date: Thu,  6 Mar 2008 11:42:54 -0500
Subject: [R-sig-ME] using R to estimate heritability
Message-ID: <20080306114254.33cocgksc0s8ocgc@agora.ulaval.ca>

Hi,

I would like to know if anyone has used linear mixed modelling in R to estimate
the genetic variance components of a phenotype. I would greatly appreciate any
sample data sets, as well as the codes used or any other comments.

Cheers

David



From mwkimpel at gmail.com  Thu Mar  6 20:30:28 2008
From: mwkimpel at gmail.com (Mark W Kimpel)
Date: Thu, 06 Mar 2008 14:30:28 -0500
Subject: [R-sig-ME] need help with mixed effects model
In-Reply-To: <Pine.LNX.4.64.0803060951080.695@orpheus.qimr.edu.au>
References: <47BF0D00.2030506@gmail.com><40e66e0b0802241145i7ff2888al44cfa6101b735986@mail.gmail.com><47C840F5.60200@gmail.com><372C3D38-56B9-4B2D-A1D5-07344A7C0C93@kagi.com>	<a072ed700803050302g35631f48g13324d21fd721c26@mail.gmail.com>
	<Pine.LNX.4.64.0803060951080.695@orpheus.qimr.edu.au>
Message-ID: <47D04654.8000605@gmail.com>

First, I want to thank everyone on this SIG list for their generous time 
and thoughts on my problem. I am particularly grateful for learning that 
my problem is "multivariate" and that lmer is not (yet) set up for that. 
I will certainly be paying close attention to the list for the day when 
it is I think it is a terrific package.

Upon learning that what I am interested in is multi-variate correlation, 
not multi-factorial, a search of R packages turned up CORREP, which 
seems tailor made to my situation. In fact, perhaps once again a Nobel 
prize has slipped through my grasp as I find that yet another great idea 
of mine is not very original after all ;)

So, I'll give lmer a rest for now and see how CORREP does, but I do look 
forward to hearing about lmer's multivariate capabilities in the future.

Mark

Mark W. Kimpel MD  ** Neuroinformatics ** Dept. of Psychiatry
Indiana University School of Medicine

15032 Hunter Court, Westfield, IN  46074

(317) 490-5129 Work, & Mobile & VoiceMail
(317) 204-4202 Home (no voice mail please)

mwkimpel<at>gmail<dot>com

******************************************************************


David Duffy wrote:
> On Wed, 5 Mar 2008, Nick Isaac wrote:
> 
>> You can do SEM-type models using the smatr package. Only drawback is that
>> you'd have to treat Rat as a fixed effect. This is a general class of
>> problem that afflicts several of my current projects, and I'm having a tough
>> time choosing between mixed effect and structural equation models. The
>> former is most appropriate for partitioning the variance, but the latter is
>> most appropriate for modelling error variance in the observations. I don't
>> see an obvious solution with the available tools and would appreciate any
>> general insights.
>>
>> Cheers, Nick
>>
> 
> I don't know if smatr supports multiple groups (so you can have unbalanced 
> type data).  We generally use the Mx SEM package, which allows both 
> multiple groups as well as "irregular" data, and our main interest is always
> in partitioning the variance.  Every mixed model (indeed 
> every linear model) is expressible as a SEM (it just runs slower ;)).
> 
> Generally, if you want to include a random effect you set up a latent 
> variable with appropriate path(s) to the observed value(s).  Your
> relationships will be between the latent variables.  Often, you'll fix
> the path coefficient to unity.  Have a look at Mx book/manual (Mike Neale and
> Hermine Maes) and you'll get some ideas.
> 
> For the expression example, one model would be something like
>                               +--------------------------+
>                  Strain1-Rg1  |                          |
>               --------------------------    Rat16        |
>               |               |        |                 |
>                     r1        |                 r1       |
> Latent    Gene111 <-->  Gene112       Gene161 <-->  Gene162
>           /b1   | \b3   b4/ | \       /b1    | \ 
> Manif  Tissue1 T2 T3               Tissue1 T2 T3
> 
> With appropriate equality constraints (r1=r1..=r1, b1=b1..) or 
> random regressions on the path coefficientsi and covariance matrix of latent
> variables.  So this model states (and
> can test) that tissue expressions are imperfect measures of
> overall gene expression in the ith animal.
> 
> But after saying that, for this particular example, I would use a
> program such as ASREML or Wombat or MENDEL that easily fit multivariate
> mixed models.  Our group has used ASREML to fit mixture models to
> deal with error in DNA pooling experiments for 50000 SNPs.
> 
> Hope that's clear (and even right), David Duffy.
>



From David.Duffy at qimr.edu.au  Thu Mar  6 21:55:54 2008
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Fri, 7 Mar 2008 06:55:54 +1000 (EST)
Subject: [R-sig-ME] using R to estimate heritability
In-Reply-To: <20080306114254.33cocgksc0s8ocgc@agora.ulaval.ca>
References: <20080306114254.33cocgksc0s8ocgc@agora.ulaval.ca>
Message-ID: <Pine.LNX.4.64.0803070653360.13545@orpheus.qimr.edu.au>

On Thu, 6 Mar 2008, David Paez wrote:

> Hi,
>
> I would like to know if anyone has used linear mixed modelling in R to estimate
> the genetic variance components of a phenotype. I would greatly appreciate any
> sample data sets, as well as the codes used or any other comments.
>
> Cheers
>
> David

See the kinship package, and read the CRAN Genetics task view.

-- 
| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v



From mwkimpel at gmail.com  Sat Mar  8 21:42:20 2008
From: mwkimpel at gmail.com (Mark W Kimpel)
Date: Sat, 08 Mar 2008 15:42:20 -0500
Subject: [R-sig-ME] help with plotting fitted values
Message-ID: <47D2FA2C.9090106@gmail.com>

I would like to plot fitted values from a model vs. the original values. 
Is this possible?

My model is: mod <- lmer(gene1 ~  Treatment + Tissue + (1|Rat))

In Pinheiro and Bates, pg. 178, there is code to do this for nlme and on 
pg. 179 is the resulting graph.

When I do:
plot(mod, gene1 ~ fitted(.))

I get:
 > plot(mod, gene1 ~ fitted(.))
Error in xy.coords(x, y, xlabel, ylabel, log) :
   'x' and 'y' lengths differ

On a more general note, how does one get help for an overloaded function 
like plot? help("plot", package = "lme4") yields "no documentation", I 
could not find any info in the lme4 manual, and help(package = "lme4") 
did not list any plot methods.

Session info follows. Thanks,
Mark

 > sessionInfo()
R version 2.7.0 Under development (unstable) (2008-03-05 r44683)
x86_64-unknown-linux-gnu

locale:
LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C

attached base packages:
[1] tools     stats     graphics  grDevices datasets  utils     methods
[8] base

other attached packages:
[1] hints_1.0.1          lme4_0.99875-9       Matrix_0.999375-4
[4] lattice_0.17-6       affy_1.17.8          preprocessCore_1.1.5
[7] affyio_1.7.14        Biobase_1.99.1

loaded via a namespace (and not attached):
[1] grid_2.7.0
-- 

Mark W. Kimpel MD  ** Neuroinformatics ** Dept. of Psychiatry
Indiana University School of Medicine

15032 Hunter Court, Westfield, IN  46074

(317) 490-5129 Work, & Mobile & VoiceMail
(317) 204-4202 Home (no voice mail please)

mwkimpel<at>gmail<dot>com



From bates at stat.wisc.edu  Mon Mar 10 16:31:14 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 10 Mar 2008 10:31:14 -0500
Subject: [R-sig-ME] help with plotting fitted values
In-Reply-To: <47D2FA2C.9090106@gmail.com>
References: <47D2FA2C.9090106@gmail.com>
Message-ID: <40e66e0b0803100831k1d0b91a5g82fbf2e8b834b260@mail.gmail.com>

On Sat, Mar 8, 2008 at 3:42 PM, Mark W Kimpel <mwkimpel at gmail.com> wrote:
> I would like to plot fitted values from a model vs. the original values.
>  Is this possible?

Yes.

>  My model is: mod <- lmer(gene1 ~  Treatment + Tissue + (1|Rat))

Then the simplest way to plot the results would be

plot(gene1 ~ fitted(mod))

wouldn't it?

>  In Pinheiro and Bates, pg. 178, there is code to do this for nlme and on
>  pg. 179 is the resulting graph.

>  When I do:
>  plot(mod, gene1 ~ fitted(.))

>  I get:
>   > plot(mod, gene1 ~ fitted(.))
>  Error in xy.coords(x, y, xlabel, ylabel, log) :
>    'x' and 'y' lengths differ

Well, the nlme package uses S3 classes and methods which must dispatch
on the first argument, which is why the first argument is the fitted
model.  That is not the style for S4 classes and methods on which the
lme4 package is based.

In your case it doesn't look as if you are using the data argument in
the call to lmer, which is the preferred style, so your gene1 vector
is apparently accessible in the global environment.  If so, it is
simplest just to use the name directly as I showed above.  The model
general style of using the . and having access to variables in the
model frame is implemented using methods for the with generic so you
could use

> library(lme4)
Loading required package: Matrix
Loading required package: lattice

Attaching package: 'Matrix'


	The following object(s) are masked from package:stats :

	 xtabs

> fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
> with(fm1, xyplot(Reaction ~ fitted(.)))

As I said in response to another inquiry, there is no guarantee that
all code for the nlme package will work with the lme4 package.
Because lme4 extends the range of the mixed-effects models that can be
fit and because it is based on S4 classes and methods some things will
be done differently in lme4.

>  On a more general note, how does one get help for an overloaded function
>  like plot? help("plot", package = "lme4") yields "no documentation", I
>  could not find any info in the lme4 manual, and help(package = "lme4")
>  did not list any plot methods.
>
>  Session info follows. Thanks,
>  Mark
>
>   > sessionInfo()
>  R version 2.7.0 Under development (unstable) (2008-03-05 r44683)
>  x86_64-unknown-linux-gnu
>
>  locale:
>  LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C
>
>  attached base packages:
>  [1] tools     stats     graphics  grDevices datasets  utils     methods
>  [8] base
>
>  other attached packages:
>  [1] hints_1.0.1          lme4_0.99875-9       Matrix_0.999375-4
>  [4] lattice_0.17-6       affy_1.17.8          preprocessCore_1.1.5
>  [7] affyio_1.7.14        Biobase_1.99.1
>
>  loaded via a namespace (and not attached):
>  [1] grid_2.7.0
>  --
>
>  Mark W. Kimpel MD  ** Neuroinformatics ** Dept. of Psychiatry
>  Indiana University School of Medicine
>
>  15032 Hunter Court, Westfield, IN  46074
>
>  (317) 490-5129 Work, & Mobile & VoiceMail
>  (317) 204-4202 Home (no voice mail please)
>
>  mwkimpel<at>gmail<dot>com
>
>  _______________________________________________
>  R-sig-mixed-models at r-project.org mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From ima at aqua.dtu.dk  Mon Mar 10 19:58:35 2008
From: ima at aqua.dtu.dk (Irene Mantzouni)
Date: Mon, 10 Mar 2008 19:58:35 +0100
Subject: [R-sig-ME] multivariate mixed model
References: <FF93947C067EE443923B993DE110009245150F@lu-mail-san.dfu.local>
Message-ID: <E7163EE5889C364389C921C0DE04152401F085@lu-mail-san.dfu.local>

 

________________________________

From: Irene Mantzouni
Sent: Mon 10/03/2008 7:17 PM
To: R-sig-mixed-models at r-project.org
Subject: multivariate mixed model 



Hi all!



Just a quick question: 

Is it possible to fit a normal multivariate linear mixed model either with nlme or lme4? 

Reference to an example would be very helpful!



Thank you! 



From bates at stat.wisc.edu  Mon Mar 10 20:11:05 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 10 Mar 2008 14:11:05 -0500
Subject: [R-sig-ME] multivariate mixed model
In-Reply-To: <E7163EE5889C364389C921C0DE04152401F085@lu-mail-san.dfu.local>
References: <FF93947C067EE443923B993DE110009245150F@lu-mail-san.dfu.local>
	<E7163EE5889C364389C921C0DE04152401F085@lu-mail-san.dfu.local>
Message-ID: <40e66e0b0803101211n119a48cfpfb3941bbf2e49a45@mail.gmail.com>

On Mon, Mar 10, 2008 at 1:58 PM, Irene Mantzouni <ima at aqua.dtu.dk> wrote:

>  From: Irene Mantzouni
>  Sent: Mon 10/03/2008 7:17 PM
>  To: R-sig-mixed-models at r-project.org
>  Subject: multivariate mixed model

>  Is it possible to fit a normal multivariate linear mixed model either with nlme or lme4?

Questions like that can precipitate answers like

install.packages("fortunes")
library(fortunes)
fortune("Yoda")

However, I don't know of an easy way of fitting such models using the
nlme or the lme4 packages.

>  Reference to an example would be very helpful!



From A.Robinson at ms.unimelb.edu.au  Mon Mar 10 21:27:29 2008
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Tue, 11 Mar 2008 07:27:29 +1100
Subject: [R-sig-ME] multivariate mixed model
In-Reply-To: <E7163EE5889C364389C921C0DE04152401F085@lu-mail-san.dfu.local>
References: <FF93947C067EE443923B993DE110009245150F@lu-mail-san.dfu.local>
	<E7163EE5889C364389C921C0DE04152401F085@lu-mail-san.dfu.local>
Message-ID: <20080310202729.GB1566@ms.unimelb.edu.au>

It's not easy, but it can be done.  The approach is to define each
multivariate response as a cluster.  I did it in nlme for the
following article:

Robinson, A.P., 2004. Preserving correlation while modelling diameter
distributions.  Canadian Journal of Forest Research 34, 221

Daniel Hall (University of Georgia, I think) did it for non-linear
models, in a paper published in Biometrics (I think) about 5 years ago
...

Andrew

On Mon, Mar 10, 2008 at 07:58:35PM +0100, Irene Mantzouni wrote:
>  
> 
> ________________________________
> 
> From: Irene Mantzouni
> Sent: Mon 10/03/2008 7:17 PM
> To: R-sig-mixed-models at r-project.org
> Subject: multivariate mixed model 
> 
> 
> 
> Hi all!
> 
> 
> 
> Just a quick question: 
> 
> Is it possible to fit a normal multivariate linear mixed model either with nlme or lme4? 
> 
> Reference to an example would be very helpful!
> 
> 
> 
> Thank you! 
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-6410
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/



From bates at stat.wisc.edu  Mon Mar 10 23:44:54 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 10 Mar 2008 17:44:54 -0500
Subject: [R-sig-ME] New alpha version of lme4 code on R-forge
Message-ID: <40e66e0b0803101544r4e6c0f87n7340749fec120f9c@mail.gmail.com>

I have just uploaded a new version of the lme4 package to
R-forge.R-project.org.  I hope that binary packages for
lme4_0.999875-6 will be available tomorrow.

This version has an mcmcsamp function for linear mixed models fit by
lmer.  I have not yet incorporated the Metropolis-Hastings step for
sampling from the fixed effects and random effects in generalized
linear mixed models or nonlinear mixed models.

I have methods for the HPDinterval function for these objects.  You
can apply it to the entire sample or to the fixef slot only or to the
sigma slot only.  There is another slot called ST from which the
variances and covariances of the random effects can be derived.

I think I found a glich in earlier versions of mcmcsamp that made them
overly conservative.  When I sampled from the conditional distribution
of the sigma parameter I think I used the wrong number of degrees of
freedom for the chi-square distribution.  I have modified that and
think the results look more like they should.  However, I would
appreciate feedback on whether these results look sensible to others.



From bates at stat.wisc.edu  Wed Mar 12 21:01:29 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 12 Mar 2008 15:01:29 -0500
Subject: [R-sig-ME] Fix of HPDinterval methods in development lme4
Message-ID: <40e66e0b0803121301h2b38021bn1cbdf75515759906@mail.gmail.com>

Robert Kushler caught an "infelicity" in the HPDinterval methods so I
have uploaded a fix for that and bumped the version number to
0.999375-8 on the R-forge repository.  Binary package versions should
be available tomorrow.



From baron at psych.upenn.edu  Wed Mar 12 23:50:03 2008
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Wed, 12 Mar 2008 18:50:03 -0400
Subject: [R-sig-ME] Fix of HPDinterval methods in development lme4
In-Reply-To: <40e66e0b0803121301h2b38021bn1cbdf75515759906@mail.gmail.com>
References: <40e66e0b0803121301h2b38021bn1cbdf75515759906@mail.gmail.com>
Message-ID: <20080312225003.GA5902@psych.upenn.edu>

I tried to get this but could find only 0.999375-7.  I must have
missed some crucial message but I can't find it.  (I even created an
account at r-forge, which didn't do any good, and now I can't delete
it.)

On 03/12/08 15:01, Douglas Bates wrote:
> Robert Kushler caught an "infelicity" in the HPDinterval methods so I
> have uploaded a fix for that and bumped the version number to
> 0.999375-8 on the R-forge repository.  Binary package versions should
> be available tomorrow.
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From bates at stat.wisc.edu  Thu Mar 13 14:21:14 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 13 Mar 2008 08:21:14 -0500
Subject: [R-sig-ME] Fix of HPDinterval methods in development lme4
In-Reply-To: <20080312225003.GA5902@psych.upenn.edu>
References: <40e66e0b0803121301h2b38021bn1cbdf75515759906@mail.gmail.com>
	<20080312225003.GA5902@psych.upenn.edu>
Message-ID: <40e66e0b0803130621p22e33332t5aabfe8bd0042259@mail.gmail.com>

On Wed, Mar 12, 2008 at 5:50 PM, Jonathan Baron <baron at psych.upenn.edu> wrote:
> I tried to get this but could find only 0.999375-7.  I must have
>  missed some crucial message but I can't find it.

I think you missed the part about "Binary packages should be available
tomorrow."

Check
  http://r-forge.r-project.org/R/?group_id=60
for the latest status of the package.  The packages were rebuilt about
19:30 yesterday (European time).

> (I even created an
>  account at r-forge, which didn't do any good, and now I can't delete
>  it.)
>
>
>
>  On 03/12/08 15:01, Douglas Bates wrote:
>  > Robert Kushler caught an "infelicity" in the HPDinterval methods so I
>  > have uploaded a fix for that and bumped the version number to
>  > 0.999375-8 on the R-forge repository.  Binary package versions should
>  > be available tomorrow.
>  >
>  > _______________________________________________
>  > R-sig-mixed-models at r-project.org mailing list
>  > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>  --
>  Jonathan Baron, Professor of Psychology, University of Pennsylvania
>  Home page: http://www.sas.upenn.edu/~baron
>
>  _______________________________________________
>  R-sig-mixed-models at r-project.org mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Thu Mar 13 19:33:47 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 13 Mar 2008 13:33:47 -0500
Subject: [R-sig-ME] [R] lmer and correlation
In-Reply-To: <c84ed6950803130643v3c7f5506l794e2c66bc656475@mail.gmail.com>
References: <c84ed6950803130643v3c7f5506l794e2c66bc656475@mail.gmail.com>
Message-ID: <40e66e0b0803131133x75f20bb5tcf60e08c3d49282f@mail.gmail.com>

On Thu, Mar 13, 2008 at 8:43 AM, R Help <rhelp.stats at gmail.com> wrote:
> Hello list,
>      I've been reading through the archives and it seems as though, as
>  of right now, there is no way to specify the correlation structure in
>  lmer.  I was wondering if anyone knows if this is going to be
>  implemented?  I'm using mixed-effects models within a tree structure,
>  so I make a lot of calls to lme to get the resulting deviance, and
>  lmer2 is almost 5 times faster than lme on my test data, so if it may
>  be implemented later I'd be willing to wait, otherwise I might look
>  into trying to hack it myself.

I don't have any near-term plans for implementing a separate
correlation argument for the lmer function so I would recommend that
you hack it yourself.

I suggest starting with the development version of the lme4 sources,
available at http://r-forge.r-project.org/projects/lme4/.  See the SCM
tab for instructions on how to check out a read-only copy of the svn
archive or go to the R Packages tag for a .tar.gz file of the current
source package.

The way that Jose and I implemented correlation structures for the
nlme package is to form a pre-whitened set of responses and the
corresponding model matrices then use the standard lme representation
on these pre-whitened components to optimize the log-likelihood with
respect to the correlation parameters and the random effects
parameters simultaneously.

You need to work out how to evaluate the log-likelihood (or,
equivalently, the deviance) for the combined set of parameters.  The
slides at http://www.stat.wisc.edu/~bates/2008-02-22-Emory.pdf will
give you some idea of what the slots in the "mer" class of objects in
lme4 represent and how they are changed to accommodate changes in the
model.  Once the mer object is created most of the work is done in a
set of C functions that are documented at
http://lme4.r-forge.r-project.org/www/doxygen.  Be aware that many of
those functions are primarily for extensions to the linear mixed
model, such as generalized linear mixed models and nonlinear mixed
models and even generalized nonlinear mixed models.  The ability to
profile out the fixed-effects parameters in the case of linear mixed
models provides a massive simplification in the computational approach
and means that many of the more difficult steps for the other models
can be skipped.  Inside the mer_optimize function you will see that
there is one path for linear mixed models and another path for
everything else.

May I suggest that further discussion of this be moved to the
R-SIG-Mixed-Models at R-project.org list, which I have cc:'d on this
reply?



From kevin.thorpe at utoronto.ca  Thu Mar 13 19:53:40 2008
From: kevin.thorpe at utoronto.ca (Kevin E. Thorpe)
Date: Thu, 13 Mar 2008 14:53:40 -0400
Subject: [R-sig-ME] lme4 versus nlme
Message-ID: <47D97834.2060200@utoronto.ca>

I'm a bit confused.

I have started reading Pinheiro and Bates, which is excellent.  I am
trying to rationalize when one would use lmer() and when one would use
lme() (I know lmer() is not covered in the book).

I have read a number of the treads about why p-values have been taken
out of lmer() stuff.  Naturally, they still exist in nlme.  Presumably,
I should not trust those p-values either.

The problem still remains that I am encountered more and more situations
where it seems I need a mixed model.  The investigator wants to know if
the groups differ.  What is the recommended approach to answering those
kinds of questions?

Thank you,

Kevin

-- 
Kevin E. Thorpe
Biostatistician/Trialist, Knowledge Translation Program
Assistant Professor, Department of Public Health Sciences
Faculty of Medicine, University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.864.5776  Fax: 416.864.6057



From HDoran at air.org  Thu Mar 13 20:02:18 2008
From: HDoran at air.org (Doran, Harold)
Date: Thu, 13 Mar 2008 15:02:18 -0400
Subject: [R-sig-ME] lme4 versus nlme
In-Reply-To: <47D97834.2060200@utoronto.ca>
Message-ID: <2323A6D37908A847A7C32F1E3662C80E017BD8FC@dc1ex01.air.org>

Kevin:

lmer is a bit more general, and a heck of a lot faster. The nlme
function is designed for cases where there is a strict nesting structure
to the data. You can code for situations in which there are crossed
random effects, but it is clunky and slooooooow. lmer, on the other
hand, is specifically optimized for fully crossed or partially crossed
data and is very fast. It also works for data that are strictly nested.

There are some vignettes in the lme4 package that you can read to see
examples of how to use the lmer function. There are also some nifty
functions that go along with lmer that do not exists for nlme like
mcmcsamp.

Harold


> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org 
> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf 
> Of Kevin E. Thorpe
> Sent: Thursday, March 13, 2008 1:54 PM
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] lme4 versus nlme
> 
> I'm a bit confused.
> 
> I have started reading Pinheiro and Bates, which is 
> excellent.  I am trying to rationalize when one would use 
> lmer() and when one would use
> lme() (I know lmer() is not covered in the book).
> 
> I have read a number of the treads about why p-values have 
> been taken out of lmer() stuff.  Naturally, they still exist 
> in nlme.  Presumably, I should not trust those p-values either.
> 
> The problem still remains that I am encountered more and more 
> situations where it seems I need a mixed model.  The 
> investigator wants to know if the groups differ.  What is the 
> recommended approach to answering those kinds of questions?
> 
> Thank you,
> 
> Kevin
> 
> --
> Kevin E. Thorpe
> Biostatistician/Trialist, Knowledge Translation Program 
> Assistant Professor, Department of Public Health Sciences 
> Faculty of Medicine, University of Toronto
> email: kevin.thorpe at utoronto.ca  Tel: 416.864.5776  Fax: 416.864.6057
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 



From s.blomberg1 at uq.edu.au  Fri Mar 14 05:25:55 2008
From: s.blomberg1 at uq.edu.au (Simon Blomberg)
Date: Fri, 14 Mar 2008 14:25:55 +1000
Subject: [R-sig-ME] lme4 versus nlme
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E017BD8FC@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E017BD8FC@dc1ex01.air.org>
Message-ID: <1205468755.14437.13.camel@sib-sblomber01d.sib.uq.edu.au>

lmer is really good for crossed random effects models. They are easy to
fit and fast, as Harold said. However, lmer still (? Doug?) lacks the
ability to incorporate complex covariance structures for the random
effects and the residuals. I find I need that facility a lot, so I
generally stick with the nlme package. You should only trust the F tests
for the fixed effects for nested, balanced designs. lme is set up to
make nested designs easy to code, and crossed random designs a lot more
difficult to code. So including p-values for the fixed effects in lme
seems reasonable.

HTH,

Simon.

On Thu, 2008-03-13 at 15:02 -0400, Doran, Harold wrote:
> Kevin:
> 
> lmer is a bit more general, and a heck of a lot faster. The nlme
> function is designed for cases where there is a strict nesting structure
> to the data. You can code for situations in which there are crossed
> random effects, but it is clunky and slooooooow. lmer, on the other
> hand, is specifically optimized for fully crossed or partially crossed
> data and is very fast. It also works for data that are strictly nested.
> 
> There are some vignettes in the lme4 package that you can read to see
> examples of how to use the lmer function. There are also some nifty
> functions that go along with lmer that do not exists for nlme like
> mcmcsamp.
> 
> Harold
> 
> 
> > -----Original Message-----
> > From: r-sig-mixed-models-bounces at r-project.org 
> > [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf 
> > Of Kevin E. Thorpe
> > Sent: Thursday, March 13, 2008 1:54 PM
> > To: r-sig-mixed-models at r-project.org
> > Subject: [R-sig-ME] lme4 versus nlme
> > 
> > I'm a bit confused.
> > 
> > I have started reading Pinheiro and Bates, which is 
> > excellent.  I am trying to rationalize when one would use 
> > lmer() and when one would use
> > lme() (I know lmer() is not covered in the book).
> > 
> > I have read a number of the treads about why p-values have 
> > been taken out of lmer() stuff.  Naturally, they still exist 
> > in nlme.  Presumably, I should not trust those p-values either.
> > 
> > The problem still remains that I am encountered more and more 
> > situations where it seems I need a mixed model.  The 
> > investigator wants to know if the groups differ.  What is the 
> > recommended approach to answering those kinds of questions?
> > 
> > Thank you,
> > 
> > Kevin
> > 
> > --
> > Kevin E. Thorpe
> > Biostatistician/Trialist, Knowledge Translation Program 
> > Assistant Professor, Department of Public Health Sciences 
> > Faculty of Medicine, University of Toronto
> > email: kevin.thorpe at utoronto.ca  Tel: 416.864.5776  Fax: 416.864.6057
> > 
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > 
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
-- 
Simon Blomberg, BSc (Hons), PhD, MAppStat. 
Lecturer and Consultant Statistician 
Faculty of Biological and Chemical Sciences 
The University of Queensland 
St. Lucia Queensland 4072 
Australia
Room 320 Goddard Building (8)
T: +61 7 3365 2506
http://www.uq.edu.au/~uqsblomb
email: S.Blomberg1_at_uq.edu.au

Policies:
1.  I will NOT analyse your data for you.
2.  Your deadline is your problem.

The combination of some data and an aching desire for 
an answer does not ensure that a reasonable answer can 
be extracted from a given body of data. - John Tukey.



From a.fugard at ed.ac.uk  Fri Mar 14 13:31:41 2008
From: a.fugard at ed.ac.uk (Andy Fugard)
Date: Fri, 14 Mar 2008 12:31:41 +0000
Subject: [R-sig-ME] Fitted values in lmer
Message-ID: <47DA702D.7050802@ed.ac.uk>

Hi,

Is there any way to get fitted values out of lmer so that any NAs in the 
predictors/outcome pop out in the correct position?

 From what I can tell, the "fitted" function doesn't have access to 
missingness in the inputs.

Thanks,

Andy

-- 
Andy Fugard, Postgraduate Research Student
Psychology (Room F3), The University of Edinburgh,
   7 George Square, Edinburgh EH8 9JZ, UK
Mobile: +44 (0)78 123 87190   http://www.possibly.me.uk



From a.fugard at ed.ac.uk  Fri Mar 14 13:37:09 2008
From: a.fugard at ed.ac.uk (Andy Fugard)
Date: Fri, 14 Mar 2008 12:37:09 +0000
Subject: [R-sig-ME] Residuals for a binomial lmer model
Message-ID: <47DA7175.5020509@ed.ac.uk>

Hello,

I was wondering how to get residuals out of binomial lmers as the 
"residuals" function isn't implemented.  I've noticed a few other people 
ask the question too but get no response.  (Or at least I haven't found 
a response.)

I guess the answer is just to use the "fitted" function, which is 
implemented for binomial GLMMs.

Take the sleepstudy data, dichotomize Reaction (just to give us a 
dataset), and fit a multilevel logistic model:


med = median(sleepstudy$Reaction)
sleepstudy$bin = (sleepstudy$Reaction > med) + 0
M2 = lmer(bin ~ Days + (1|Subject) + (0+Days|Subject),
           data = sleepstudy, family = binomial)


We can pull out the fitted values and, say, plot fitted (post-inverse
logit) against data using a boxplot:


ilog = function(x) { 1/(1 + exp(-x)) }
boxplot(ilog(fitted(fm3)) ~ bin, data = sleepstudy)


Not sure now how useful this is, but I had some reason for wanting to peek!

Andy

-- 
Andy Fugard, Postgraduate Research Student
Psychology (Room F3), The University of Edinburgh,
   7 George Square, Edinburgh EH8 9JZ, UK
Mobile: +44 (0)78 123 87190   http://www.possibly.me.uk



From a.fugard at ed.ac.uk  Fri Mar 14 13:41:02 2008
From: a.fugard at ed.ac.uk (Andy Fugard)
Date: Fri, 14 Mar 2008 12:41:02 +0000
Subject: [R-sig-ME] Residuals for a binomial lmer model
In-Reply-To: <47DA7175.5020509@ed.ac.uk>
References: <47DA7175.5020509@ed.ac.uk>
Message-ID: <47DA725E.4080609@ed.ac.uk>

Sorry, that works modulo variable names...  Second try.

med = median(sleepstudy$Reaction)
sleepstudy$bin = (sleepstudy$Reaction > med) + 0
mod = lmer(bin ~ Days + (1|Subject) + (0+Days|Subject),
            data = sleepstudy, family = binomial)

ilog = function(x) { 1/(1 + exp(-x)) }
boxplot(ilog(fitted(mod)) ~ bin, data = sleepstudy)

Andy Fugard wrote:
> Hello,
> 
> I was wondering how to get residuals out of binomial lmers as the 
> "residuals" function isn't implemented.  I've noticed a few other people 
> ask the question too but get no response.  (Or at least I haven't found 
> a response.)
> 
> I guess the answer is just to use the "fitted" function, which is 
> implemented for binomial GLMMs.
> 
> Take the sleepstudy data, dichotomize Reaction (just to give us a 
> dataset), and fit a multilevel logistic model:
> 
> 
> med = median(sleepstudy$Reaction)
> sleepstudy$bin = (sleepstudy$Reaction > med) + 0
> M2 = lmer(bin ~ Days + (1|Subject) + (0+Days|Subject),
>            data = sleepstudy, family = binomial)
> 
> 
> We can pull out the fitted values and, say, plot fitted (post-inverse
> logit) against data using a boxplot:
> 
> 
> ilog = function(x) { 1/(1 + exp(-x)) }
> boxplot(ilog(fitted(fm3)) ~ bin, data = sleepstudy)
> 
> 
> Not sure now how useful this is, but I had some reason for wanting to peek!
> 
> Andy
> 


-- 
Andy Fugard, Postgraduate Research Student
Psychology (Room F3), The University of Edinburgh,
   7 George Square, Edinburgh EH8 9JZ, UK
Mobile: +44 (0)78 123 87190   http://www.possibly.me.uk



From bates at stat.wisc.edu  Fri Mar 14 15:43:38 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 14 Mar 2008 09:43:38 -0500
Subject: [R-sig-ME] Fitted values in lmer
In-Reply-To: <47DA702D.7050802@ed.ac.uk>
References: <47DA702D.7050802@ed.ac.uk>
Message-ID: <40e66e0b0803140743g269c027as9784c34b51d845ca@mail.gmail.com>

On Fri, Mar 14, 2008 at 7:31 AM, Andy Fugard <a.fugard at ed.ac.uk> wrote:

>  Is there any way to get fitted values out of lmer so that any NAs in the
>  predictors/outcome pop out in the correct position?

Yes.

>   From what I can tell, the "fitted" function doesn't have access to
>  missingness in the inputs.

It does not, in its current implementation.  I believe there is
information regarding the pattern of missingness in the model frame
(stored in the "frame" slot of the fitted model) and that can be used
to generate an appropriate fitted response.

Does anyone know exactly how this is done for linear models?  If so,
are you willing to provide a patch to the fitted method for mer
objects?  I can put it on my ToDo list but when the first item on the
list is "finish the book" you almost never get to the second and
subsequent items.



From a.fugard at ed.ac.uk  Fri Mar 14 16:47:35 2008
From: a.fugard at ed.ac.uk (Andy Fugard)
Date: Fri, 14 Mar 2008 15:47:35 +0000
Subject: [R-sig-ME] Fitted values in lmer
In-Reply-To: <40e66e0b0803140743g269c027as9784c34b51d845ca@mail.gmail.com>
References: <47DA702D.7050802@ed.ac.uk>
	<40e66e0b0803140743g269c027as9784c34b51d845ca@mail.gmail.com>
Message-ID: <47DA9E17.3060108@ed.ac.uk>

Thanks, rownames(mod at frame) gives indices I can use.  The following 
fitted.na code does the right sort of thing, but relies on the length of 
the original vector of outcomes (so I use the whole vector in the output 
anyway).  And it uses set differencing and merging, which might not be 
most computational efficient way to do things.  Any other suggestions 
for a way to do this gratefully received...


fitted.na = function(origvec,mod) {
   hits = as.numeric(rownames(mod at frame))
   misses = setdiff(1:length(origvec), hits)
   fits  = fitted(mod)
   d.hits   = data.frame(i = hits
                        ,fits = fitted(mod))
   d.misses = data.frame(i = misses
                        ,fits = rep(NA, length(misses)))
   res = merge(d.hits, d.misses, all.x = T, all.y=T)
   res$y = origvec
   res
}

# For instance, try adding missing data to the sleepstudy

breakit = function(x,p) {
   breakv = sample(c(NA,0), length(x), T, prob = c(p,1-p))
   x + breakv
}

sleep2 = sleepstudy
sleep2$Reaction = breakit(sleepstudy$Reaction,0.1)

mod = lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject),
            data = sleep2)

fitted.na(sleep2$Reaction, mod)




Andy



Douglas Bates wrote:
> On Fri, Mar 14, 2008 at 7:31 AM, Andy Fugard <a.fugard at ed.ac.uk> wrote:
> 
>>  Is there any way to get fitted values out of lmer so that any NAs in the
>>  predictors/outcome pop out in the correct position?
> 
> Yes.
> 
>>   From what I can tell, the "fitted" function doesn't have access to
>>  missingness in the inputs.
> 
> It does not, in its current implementation.  I believe there is
> information regarding the pattern of missingness in the model frame
> (stored in the "frame" slot of the fitted model) and that can be used
> to generate an appropriate fitted response.
> 
> Does anyone know exactly how this is done for linear models?  If so,
> are you willing to provide a patch to the fitted method for mer
> objects?  I can put it on my ToDo list but when the first item on the
> list is "finish the book" you almost never get to the second and
> subsequent items.
> 


-- 
Andy Fugard, Postgraduate Research Student
Psychology (Room F3), The University of Edinburgh,
   7 George Square, Edinburgh EH8 9JZ, UK
Mobile: +44 (0)78 123 87190   http://www.possibly.me.uk



From bates at stat.wisc.edu  Fri Mar 14 21:06:02 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 14 Mar 2008 15:06:02 -0500
Subject: [R-sig-ME] Residuals for a binomial lmer model
In-Reply-To: <47DA725E.4080609@ed.ac.uk>
References: <47DA7175.5020509@ed.ac.uk> <47DA725E.4080609@ed.ac.uk>
Message-ID: <40e66e0b0803141306x7adf38b6l84218b6b59b523e5@mail.gmail.com>

The problem with defining residuals for a generalized linear model or
a generalized linear mixed model is that you have so many possible
definitions.  Take a look at

?residuals.glm

Well, actually that isn't good advice because that help page refers
you to "the reference", of which there are two, and presumably you
need to sift through the descriptions in the reference to decide what
all these different types of residuals are.

In the development version of the lme4 package, the fitted model is an
object of class "mer" and has slots named "y", "eta", "mu", "resid",
"var", "pWt" and "sqrtrWt" which, for a generalized linear mixed
model, correspond to

y -> observed responses (remember that in the case of a binomial
response this is the fraction of positive responses, not the count)

eta -> linear predictor, depending on the fixed-effects parameters and
the random effects

mu -> conditional means of the responses, given the parameters and the
values of the random effects

resid -> sqrtrWt * (y - mu), i.e. the residuals whose squared length
is the weighted residual sum of squares

var -> the values of the variance function for the glm family.  This
is somewhat of a misnomer because it is the variance of the response
up to the prior weights.

pWt -> the prior weights.  For example, in a binomial response where
each "observation" is the number of successes in several trials, this
is the number of trials for each observation.

sqrtrWt -> the square root of the weights used to define the weighted
sum of squares of the residuals, sqrt(pWt/var)

Using these you can get several of the types of residuals defined in
residuals.glm.  The deviance residuals can also be calculated but I
never use them externally so I don't have an extractor for them.

So, what type of residuals do you want?

On Fri, Mar 14, 2008 at 7:41 AM, Andy Fugard <a.fugard at ed.ac.uk> wrote:
> Sorry, that works modulo variable names...  Second try.
>
>
>  med = median(sleepstudy$Reaction)
>  sleepstudy$bin = (sleepstudy$Reaction > med) + 0
>  mod = lmer(bin ~ Days + (1|Subject) + (0+Days|Subject),
>             data = sleepstudy, family = binomial)
>
>
>  ilog = function(x) { 1/(1 + exp(-x)) }
>  boxplot(ilog(fitted(mod)) ~ bin, data = sleepstudy)
>
>
>
>  Andy Fugard wrote:
>  > Hello,
>  >
>  > I was wondering how to get residuals out of binomial lmers as the
>  > "residuals" function isn't implemented.  I've noticed a few other people
>  > ask the question too but get no response.  (Or at least I haven't found
>  > a response.)
>  >
>  > I guess the answer is just to use the "fitted" function, which is
>  > implemented for binomial GLMMs.
>  >
>  > Take the sleepstudy data, dichotomize Reaction (just to give us a
>  > dataset), and fit a multilevel logistic model:
>  >
>  >
>  > med = median(sleepstudy$Reaction)
>  > sleepstudy$bin = (sleepstudy$Reaction > med) + 0
>  > M2 = lmer(bin ~ Days + (1|Subject) + (0+Days|Subject),
>  >            data = sleepstudy, family = binomial)
>  >
>  >
>  > We can pull out the fitted values and, say, plot fitted (post-inverse
>  > logit) against data using a boxplot:
>  >
>  >
>  > ilog = function(x) { 1/(1 + exp(-x)) }
>  > boxplot(ilog(fitted(fm3)) ~ bin, data = sleepstudy)
>  >
>  >
>  > Not sure now how useful this is, but I had some reason for wanting to peek!
>  >
>  > Andy
>  >
>
>
>  --
>  Andy Fugard, Postgraduate Research Student
>  Psychology (Room F3), The University of Edinburgh,
>    7 George Square, Edinburgh EH8 9JZ, UK
>  Mobile: +44 (0)78 123 87190   http://www.possibly.me.uk
>
>  _______________________________________________
>  R-sig-mixed-models at r-project.org mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Fri Mar 14 21:28:16 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 14 Mar 2008 15:28:16 -0500
Subject: [R-sig-ME] Fitted values in lmer
In-Reply-To: <40e66e0b0803140743g269c027as9784c34b51d845ca@mail.gmail.com>
References: <47DA702D.7050802@ed.ac.uk>
	<40e66e0b0803140743g269c027as9784c34b51d845ca@mail.gmail.com>
Message-ID: <40e66e0b0803141328u1e0c94f6ke5792229e94a6f1a@mail.gmail.com>

What I was referring to is the na.action attribute of the model frame

> library(lme4)
Loading required package: Matrix
Loading required package: lattice

Attaching package: 'Matrix'


	The following object(s) are masked from package:stats :

	 xtabs

> slps <- sleepstudy
> slps$Reaction[c(14, 34, 92)] <- NA
> fm1 <- lmer(Reaction ~ Days + (Days|Subject), slps)
> str(fm1 at frame)
'data.frame':	177 obs. of  3 variables:
 $ Reaction: num  250 259 251 321 357 ...
 $ Days    : num  0 1 2 3 4 5 6 7 8 9 ...
 $ Subject : Factor w/ 18 levels "308","309","310",..: 1 1 1 1 1 1 1 1 1 1 ...
 - attr(*, "terms")=Classes 'terms', 'formula' length 3 Reaction ~ Days
  .. ..- attr(*, "variables")= language list(Reaction, Days)
  .. ..- attr(*, "factors")= int [1:2, 1] 0 1
  .. .. ..- attr(*, "dimnames")=List of 2
  .. .. .. ..$ : chr [1:2] "Reaction" "Days"
  .. .. .. ..$ : chr "Days"
  .. ..- attr(*, "term.labels")= chr "Days"
  .. ..- attr(*, "order")= int 1
  .. ..- attr(*, "intercept")= int 1
  .. ..- attr(*, "response")= int 1
  .. ..- attr(*, ".Environment")=<R_GlobalEnv>
  .. ..- attr(*, "predvars")= language list(Reaction, Days)
  .. ..- attr(*, "dataClasses")= Named chr [1:2] "numeric" "numeric"
  .. .. ..- attr(*, "names")= chr [1:2] "Reaction" "Days"
 - attr(*, "na.action")=Class 'omit'  Named int [1:3] 14 34 92
  .. ..- attr(*, "names")= chr [1:3] "14" "34" "92"

At this point one can get the fitted values with the NA's in the
correct positions via the thoroughly unobvious

> stats:::naresid.exclude(attr(fm1 at frame, "na.action"), fm1 at mu)
  [1] 253.4849 273.1808 292.8766 312.5725 332.2684 351.9642 371.6601 391.3559
  [9] 411.0518 430.7476 213.1921 214.8561 216.5201       NA 219.8481 221.5121
 [17] 223.1761 224.8401 226.5041 228.1681 212.8984 217.8564 222.8143 227.7723
 [25] 232.7302 237.6882 242.6462 247.6041 252.5621 257.5200 275.9163 281.5164
 [33] 287.1166       NA 298.3168 303.9170 309.5171 315.1173 320.7174 326.3175
 [41] 273.5320 280.9433 288.3545 295.7658 303.1771 310.5884 317.9997 325.4110
 [49] 332.8223 340.2336 260.3778 270.5809 280.7841 290.9872 301.1903 311.3934
 [57] 321.5965 331.7996 342.0027 352.2058 268.1086 278.3689 288.6293 298.8896
 [65] 309.1500 319.4103 329.6707 339.9310 350.1914 360.4517 244.2257 255.7619
 [73] 267.2981 278.8342 290.3704 301.9066 313.4427 324.9789 336.5151 348.0512
 [81] 251.2783 250.9591 250.6398 250.3205 250.0013 249.6820 249.3627 249.0435
 [89] 248.7242 248.4050 283.8511       NA 322.6857 342.1031 361.5204 380.9377
 [97] 400.3550 419.7723 439.1897 458.6070 226.4061 238.0217 249.6373 261.2529
[105] 272.8685 284.4841 296.0997 307.7153 319.3310 330.9466 238.3395 255.4259
[113] 272.5123 289.5986 306.6850 323.7714 340.8578 357.9442 375.0305 392.1169
[121] 256.0056 263.4524 270.8991 278.3458 285.7925 293.2393 300.6860 308.1327
[129] 315.5795 323.0262 272.0277 286.0636 300.0995 314.1354 328.1714 342.2073
[137] 356.2432 370.2791 384.3150 398.3510 254.6441 265.9888 277.3335 288.6782
[145] 300.0229 311.3676 322.7123 334.0570 345.4017 356.7464 225.9405 241.2159
[153] 256.4914 271.7668 287.0422 302.3176 317.5930 332.8684 348.1438 363.4192
[161] 252.2315 261.7074 271.1833 280.6592 290.1351 299.6110 309.0870 318.5629
[169] 328.0388 337.5147 263.5955 275.3631 287.1308 298.8985 310.6662 322.4338
[177] 334.2015 345.9692 357.7368 369.5045

There is some subtlety about the difference between na.omit and
na.exclude that eludes me.  I'm sure it is documented and I could,
with time, unravel the mysteries but I really would prefer not to
spend that time.  I was hoping that someone else knew what the
distinction was and how one would construct an extractor that would
return the desired value and not be such an abominable kludge as the
above.


On Fri, Mar 14, 2008 at 9:43 AM, Douglas Bates <bates at stat.wisc.edu> wrote:
> On Fri, Mar 14, 2008 at 7:31 AM, Andy Fugard <a.fugard at ed.ac.uk> wrote:
>
>  >  Is there any way to get fitted values out of lmer so that any NAs in the
>  >  predictors/outcome pop out in the correct position?
>
>  Yes.
>
>
>  >   From what I can tell, the "fitted" function doesn't have access to
>  >  missingness in the inputs.
>
>  It does not, in its current implementation.  I believe there is
>  information regarding the pattern of missingness in the model frame
>  (stored in the "frame" slot of the fitted model) and that can be used
>  to generate an appropriate fitted response.
>
>  Does anyone know exactly how this is done for linear models?  If so,
>  are you willing to provide a patch to the fitted method for mer
>  objects?  I can put it on my ToDo list but when the first item on the
>  list is "finish the book" you almost never get to the second and
>  subsequent items.
>



From nilsson.henric at gmail.com  Fri Mar 14 22:28:37 2008
From: nilsson.henric at gmail.com (Henric Nilsson (Public))
Date: Fri, 14 Mar 2008 22:28:37 +0100
Subject: [R-sig-ME] Fitted values in lmer
In-Reply-To: <40e66e0b0803141328u1e0c94f6ke5792229e94a6f1a@mail.gmail.com>
References: <47DA702D.7050802@ed.ac.uk>	<40e66e0b0803140743g269c027as9784c34b51d845ca@mail.gmail.com>
	<40e66e0b0803141328u1e0c94f6ke5792229e94a6f1a@mail.gmail.com>
Message-ID: <47DAEE05.8050602@gmail.com>

Douglas Bates wrote:

> What I was referring to is the na.action attribute of the model frame
> 
>> library(lme4)
> Loading required package: Matrix
> Loading required package: lattice
> 
> Attaching package: 'Matrix'
> 
> 
> 	The following object(s) are masked from package:stats :
> 
> 	 xtabs
> 
>> slps <- sleepstudy
>> slps$Reaction[c(14, 34, 92)] <- NA
>> fm1 <- lmer(Reaction ~ Days + (Days|Subject), slps)
>> str(fm1 at frame)
> 'data.frame':	177 obs. of  3 variables:
>  $ Reaction: num  250 259 251 321 357 ...
>  $ Days    : num  0 1 2 3 4 5 6 7 8 9 ...
>  $ Subject : Factor w/ 18 levels "308","309","310",..: 1 1 1 1 1 1 1 1 1 1 ...
>  - attr(*, "terms")=Classes 'terms', 'formula' length 3 Reaction ~ Days
>   .. ..- attr(*, "variables")= language list(Reaction, Days)
>   .. ..- attr(*, "factors")= int [1:2, 1] 0 1
>   .. .. ..- attr(*, "dimnames")=List of 2
>   .. .. .. ..$ : chr [1:2] "Reaction" "Days"
>   .. .. .. ..$ : chr "Days"
>   .. ..- attr(*, "term.labels")= chr "Days"
>   .. ..- attr(*, "order")= int 1
>   .. ..- attr(*, "intercept")= int 1
>   .. ..- attr(*, "response")= int 1
>   .. ..- attr(*, ".Environment")=<R_GlobalEnv>
>   .. ..- attr(*, "predvars")= language list(Reaction, Days)
>   .. ..- attr(*, "dataClasses")= Named chr [1:2] "numeric" "numeric"
>   .. .. ..- attr(*, "names")= chr [1:2] "Reaction" "Days"
>  - attr(*, "na.action")=Class 'omit'  Named int [1:3] 14 34 92
>   .. ..- attr(*, "names")= chr [1:3] "14" "34" "92"
> 
> At this point one can get the fitted values with the NA's in the
> correct positions via the thoroughly unobvious
> 
>> stats:::naresid.exclude(attr(fm1 at frame, "na.action"), fm1 at mu)
>   [1] 253.4849 273.1808 292.8766 312.5725 332.2684 351.9642 371.6601 391.3559
>   [9] 411.0518 430.7476 213.1921 214.8561 216.5201       NA 219.8481 221.5121
>  [17] 223.1761 224.8401 226.5041 228.1681 212.8984 217.8564 222.8143 227.7723
>  [25] 232.7302 237.6882 242.6462 247.6041 252.5621 257.5200 275.9163 281.5164
>  [33] 287.1166       NA 298.3168 303.9170 309.5171 315.1173 320.7174 326.3175
>  [41] 273.5320 280.9433 288.3545 295.7658 303.1771 310.5884 317.9997 325.4110
>  [49] 332.8223 340.2336 260.3778 270.5809 280.7841 290.9872 301.1903 311.3934
>  [57] 321.5965 331.7996 342.0027 352.2058 268.1086 278.3689 288.6293 298.8896
>  [65] 309.1500 319.4103 329.6707 339.9310 350.1914 360.4517 244.2257 255.7619
>  [73] 267.2981 278.8342 290.3704 301.9066 313.4427 324.9789 336.5151 348.0512
>  [81] 251.2783 250.9591 250.6398 250.3205 250.0013 249.6820 249.3627 249.0435
>  [89] 248.7242 248.4050 283.8511       NA 322.6857 342.1031 361.5204 380.9377
>  [97] 400.3550 419.7723 439.1897 458.6070 226.4061 238.0217 249.6373 261.2529
> [105] 272.8685 284.4841 296.0997 307.7153 319.3310 330.9466 238.3395 255.4259
> [113] 272.5123 289.5986 306.6850 323.7714 340.8578 357.9442 375.0305 392.1169
> [121] 256.0056 263.4524 270.8991 278.3458 285.7925 293.2393 300.6860 308.1327
> [129] 315.5795 323.0262 272.0277 286.0636 300.0995 314.1354 328.1714 342.2073
> [137] 356.2432 370.2791 384.3150 398.3510 254.6441 265.9888 277.3335 288.6782
> [145] 300.0229 311.3676 322.7123 334.0570 345.4017 356.7464 225.9405 241.2159
> [153] 256.4914 271.7668 287.0422 302.3176 317.5930 332.8684 348.1438 363.4192
> [161] 252.2315 261.7074 271.1833 280.6592 290.1351 299.6110 309.0870 318.5629
> [169] 328.0388 337.5147 263.5955 275.3631 287.1308 298.8985 310.6662 322.4338
> [177] 334.2015 345.9692 357.7368 369.5045
> 
> There is some subtlety about the difference between na.omit and
> na.exclude that eludes me.  I'm sure it is documented and I could,
> with time, unravel the mysteries but I really would prefer not to
> spend that time.  I was hoping that someone else knew what the
> distinction was and how one would construct an extractor that would
> return the desired value and not be such an abominable kludge as the
> above.

Yes. I've prepared and tested the following patch:

Index: C:/Users/hennil/Desktop/R_packages/source/lme4_0.999375-8/R/lmer.R
===================================================================
--- C:/Users/hennil/Desktop/R_packages/source/lme4_0.999375-8/R/lmer.R 
(revision 99)
+++ C:/Users/hennil/Desktop/R_packages/source/lme4_0.999375-8/R/lmer.R 
(working copy)
@@ -882,7 +882,14 @@
        })

  setMethod("fitted", signature(object = "mer"),
-	  function(object, ...) object at eta)
+	  function(object, ...)
+      {
+          eta <- object at eta
+          na.action <- attr(object at frame, "na.action")
+          if(!is.null(na.action))
+              napredict(na.action, eta)
+          else eta
+      })

  setMethod("formula", signature(x = "mer"),
  	  function(x, ...)


After applying the patch, the distinction between `na.omit' and 
`na.exclude' is easily shown:

 > fm1 <- lmer(Reaction ~ Days + (Days|Subject), slps)
 > fitted(fm1)
   [1] 253.4849 273.1808 292.8766 312.5725 332.2684 351.9642
   [7] 371.6601 391.3559 411.0518 430.7476 213.1921 214.8561
  [13] 216.5201 219.8481 221.5121 223.1761 224.8401 226.5041
  [19] 228.1681 212.8984 217.8564 222.8143 227.7723 232.7302
  [25] 237.6882 242.6462 247.6041 252.5621 257.5200 275.9163
  [31] 281.5164 287.1166 298.3168 303.9170 309.5171 315.1173
  [37] 320.7174 326.3175 273.5320 280.9433 288.3545 295.7658
  [43] 303.1771 310.5884 317.9997 325.4110 332.8223 340.2336
  [49] 260.3778 270.5809 280.7841 290.9872 301.1903 311.3934
  [55] 321.5965 331.7996 342.0027 352.2058 268.1086 278.3689
  [61] 288.6293 298.8896 309.1500 319.4103 329.6707 339.9310
  [67] 350.1914 360.4517 244.2257 255.7619 267.2981 278.8342
  [73] 290.3704 301.9066 313.4427 324.9789 336.5151 348.0512
  [79] 251.2783 250.9591 250.6398 250.3205 250.0013 249.6820
  [85] 249.3627 249.0435 248.7242 248.4050 283.8511 322.6857
  [91] 342.1031 361.5204 380.9377 400.3550 419.7723 439.1897
  [97] 458.6070 226.4061 238.0217 249.6373 261.2529 272.8685
[103] 284.4841 296.0997 307.7153 319.3310 330.9466 238.3395
[109] 255.4259 272.5123 289.5986 306.6850 323.7714 340.8578
[115] 357.9442 375.0305 392.1169 256.0056 263.4524 270.8991
[121] 278.3458 285.7925 293.2393 300.6860 308.1327 315.5795
[127] 323.0262 272.0277 286.0636 300.0995 314.1354 328.1714
[133] 342.2073 356.2432 370.2791 384.3150 398.3510 254.6441
[139] 265.9888 277.3335 288.6782 300.0229 311.3676 322.7123
[145] 334.0570 345.4017 356.7464 225.9405 241.2159 256.4914
[151] 271.7668 287.0422 302.3176 317.5930 332.8684 348.1438
[157] 363.4192 252.2315 261.7074 271.1833 280.6592 290.1351
[163] 299.6110 309.0870 318.5629 328.0388 337.5147 263.5955
[169] 275.3631 287.1308 298.8985 310.6662 322.4338 334.2015
[175] 345.9692 357.7368 369.5045
 >
 > fm2 <- lmer(Reaction ~ Days + (Days|Subject), slps,
+             na.action = "na.exclude")
 > fitted(fm2)
   [1] 253.4849 273.1808 292.8766 312.5725 332.2684 351.9642
   [7] 371.6601 391.3559 411.0518 430.7476 213.1921 214.8561
  [13] 216.5201       NA 219.8481 221.5121 223.1761 224.8401
  [19] 226.5041 228.1681 212.8984 217.8564 222.8143 227.7723
  [25] 232.7302 237.6882 242.6462 247.6041 252.5621 257.5200
  [31] 275.9163 281.5164 287.1166       NA 298.3168 303.9170
  [37] 309.5171 315.1173 320.7174 326.3175 273.5320 280.9433
  [43] 288.3545 295.7658 303.1771 310.5884 317.9997 325.4110
  [49] 332.8223 340.2336 260.3778 270.5809 280.7841 290.9872
  [55] 301.1903 311.3934 321.5965 331.7996 342.0027 352.2058
  [61] 268.1086 278.3689 288.6293 298.8896 309.1500 319.4103
  [67] 329.6707 339.9310 350.1914 360.4517 244.2257 255.7619
  [73] 267.2981 278.8342 290.3704 301.9066 313.4427 324.9789
  [79] 336.5151 348.0512 251.2783 250.9591 250.6398 250.3205
  [85] 250.0013 249.6820 249.3627 249.0435 248.7242 248.4050
  [91] 283.8511       NA 322.6857 342.1031 361.5204 380.9377
  [97] 400.3550 419.7723 439.1897 458.6070 226.4061 238.0217
[103] 249.6373 261.2529 272.8685 284.4841 296.0997 307.7153
[109] 319.3310 330.9466 238.3395 255.4259 272.5123 289.5986
[115] 306.6850 323.7714 340.8578 357.9442 375.0305 392.1169
[121] 256.0056 263.4524 270.8991 278.3458 285.7925 293.2393
[127] 300.6860 308.1327 315.5795 323.0262 272.0277 286.0636
[133] 300.0995 314.1354 328.1714 342.2073 356.2432 370.2791
[139] 384.3150 398.3510 254.6441 265.9888 277.3335 288.6782
[145] 300.0229 311.3676 322.7123 334.0570 345.4017 356.7464
[151] 225.9405 241.2159 256.4914 271.7668 287.0422 302.3176
[157] 317.5930 332.8684 348.1438 363.4192 252.2315 261.7074
[163] 271.1833 280.6592 290.1351 299.6110 309.0870 318.5629
[169] 328.0388 337.5147 263.5955 275.3631 287.1308 298.8985
[175] 310.6662 322.4338 334.2015 345.9692 357.7368 369.5045
 >

I hope you find this useful.


Henric



> 
> 
> On Fri, Mar 14, 2008 at 9:43 AM, Douglas Bates <bates at stat.wisc.edu> wrote:
>> On Fri, Mar 14, 2008 at 7:31 AM, Andy Fugard <a.fugard at ed.ac.uk> wrote:
>>
>>  >  Is there any way to get fitted values out of lmer so that any NAs in the
>>  >  predictors/outcome pop out in the correct position?
>>
>>  Yes.
>>
>>
>>  >   From what I can tell, the "fitted" function doesn't have access to
>>  >  missingness in the inputs.
>>
>>  It does not, in its current implementation.  I believe there is
>>  information regarding the pattern of missingness in the model frame
>>  (stored in the "frame" slot of the fitted model) and that can be used
>>  to generate an appropriate fitted response.
>>
>>  Does anyone know exactly how this is done for linear models?  If so,
>>  are you willing to provide a patch to the fitted method for mer
>>  objects?  I can put it on my ToDo list but when the first item on the
>>  list is "finish the book" you almost never get to the second and
>>  subsequent items.
>>
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Fri Mar 14 22:38:53 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 14 Mar 2008 16:38:53 -0500
Subject: [R-sig-ME] Fitted values in lmer
In-Reply-To: <47DAEE05.8050602@gmail.com>
References: <47DA702D.7050802@ed.ac.uk>
	<40e66e0b0803140743g269c027as9784c34b51d845ca@mail.gmail.com>
	<40e66e0b0803141328u1e0c94f6ke5792229e94a6f1a@mail.gmail.com>
	<47DAEE05.8050602@gmail.com>
Message-ID: <40e66e0b0803141438s3fd585bal36453a8ab037f411@mail.gmail.com>

Thank you, Henric.  That's exactly what I wanted.  I will change one
glich, which is my glich, not yours.  The original method should have
returned the mu slot, not the eta slot.  These are the same numerical
values for linear mixed models but not for generalized linear mixed
models.

On Fri, Mar 14, 2008 at 4:28 PM, Henric Nilsson (Public)
<nilsson.henric at gmail.com> wrote:
>
> Douglas Bates wrote:
>
>  > What I was referring to is the na.action attribute of the model frame
>  >
>  >> library(lme4)
>  > Loading required package: Matrix
>  > Loading required package: lattice
>  >
>  > Attaching package: 'Matrix'
>  >
>  >
>  >       The following object(s) are masked from package:stats :
>  >
>  >        xtabs
>  >
>  >> slps <- sleepstudy
>  >> slps$Reaction[c(14, 34, 92)] <- NA
>  >> fm1 <- lmer(Reaction ~ Days + (Days|Subject), slps)
>  >> str(fm1 at frame)
>  > 'data.frame': 177 obs. of  3 variables:
>  >  $ Reaction: num  250 259 251 321 357 ...
>  >  $ Days    : num  0 1 2 3 4 5 6 7 8 9 ...
>  >  $ Subject : Factor w/ 18 levels "308","309","310",..: 1 1 1 1 1 1 1 1 1 1 ...
>  >  - attr(*, "terms")=Classes 'terms', 'formula' length 3 Reaction ~ Days
>  >   .. ..- attr(*, "variables")= language list(Reaction, Days)
>  >   .. ..- attr(*, "factors")= int [1:2, 1] 0 1
>  >   .. .. ..- attr(*, "dimnames")=List of 2
>  >   .. .. .. ..$ : chr [1:2] "Reaction" "Days"
>  >   .. .. .. ..$ : chr "Days"
>  >   .. ..- attr(*, "term.labels")= chr "Days"
>  >   .. ..- attr(*, "order")= int 1
>  >   .. ..- attr(*, "intercept")= int 1
>  >   .. ..- attr(*, "response")= int 1
>  >   .. ..- attr(*, ".Environment")=<R_GlobalEnv>
>  >   .. ..- attr(*, "predvars")= language list(Reaction, Days)
>  >   .. ..- attr(*, "dataClasses")= Named chr [1:2] "numeric" "numeric"
>  >   .. .. ..- attr(*, "names")= chr [1:2] "Reaction" "Days"
>  >  - attr(*, "na.action")=Class 'omit'  Named int [1:3] 14 34 92
>  >   .. ..- attr(*, "names")= chr [1:3] "14" "34" "92"
>  >
>  > At this point one can get the fitted values with the NA's in the
>  > correct positions via the thoroughly unobvious
>  >
>  >> stats:::naresid.exclude(attr(fm1 at frame, "na.action"), fm1 at mu)
>  >   [1] 253.4849 273.1808 292.8766 312.5725 332.2684 351.9642 371.6601 391.3559
>  >   [9] 411.0518 430.7476 213.1921 214.8561 216.5201       NA 219.8481 221.5121
>  >  [17] 223.1761 224.8401 226.5041 228.1681 212.8984 217.8564 222.8143 227.7723
>  >  [25] 232.7302 237.6882 242.6462 247.6041 252.5621 257.5200 275.9163 281.5164
>  >  [33] 287.1166       NA 298.3168 303.9170 309.5171 315.1173 320.7174 326.3175
>  >  [41] 273.5320 280.9433 288.3545 295.7658 303.1771 310.5884 317.9997 325.4110
>  >  [49] 332.8223 340.2336 260.3778 270.5809 280.7841 290.9872 301.1903 311.3934
>  >  [57] 321.5965 331.7996 342.0027 352.2058 268.1086 278.3689 288.6293 298.8896
>  >  [65] 309.1500 319.4103 329.6707 339.9310 350.1914 360.4517 244.2257 255.7619
>  >  [73] 267.2981 278.8342 290.3704 301.9066 313.4427 324.9789 336.5151 348.0512
>  >  [81] 251.2783 250.9591 250.6398 250.3205 250.0013 249.6820 249.3627 249.0435
>  >  [89] 248.7242 248.4050 283.8511       NA 322.6857 342.1031 361.5204 380.9377
>  >  [97] 400.3550 419.7723 439.1897 458.6070 226.4061 238.0217 249.6373 261.2529
>  > [105] 272.8685 284.4841 296.0997 307.7153 319.3310 330.9466 238.3395 255.4259
>  > [113] 272.5123 289.5986 306.6850 323.7714 340.8578 357.9442 375.0305 392.1169
>  > [121] 256.0056 263.4524 270.8991 278.3458 285.7925 293.2393 300.6860 308.1327
>  > [129] 315.5795 323.0262 272.0277 286.0636 300.0995 314.1354 328.1714 342.2073
>  > [137] 356.2432 370.2791 384.3150 398.3510 254.6441 265.9888 277.3335 288.6782
>  > [145] 300.0229 311.3676 322.7123 334.0570 345.4017 356.7464 225.9405 241.2159
>  > [153] 256.4914 271.7668 287.0422 302.3176 317.5930 332.8684 348.1438 363.4192
>  > [161] 252.2315 261.7074 271.1833 280.6592 290.1351 299.6110 309.0870 318.5629
>  > [169] 328.0388 337.5147 263.5955 275.3631 287.1308 298.8985 310.6662 322.4338
>  > [177] 334.2015 345.9692 357.7368 369.5045
>  >
>  > There is some subtlety about the difference between na.omit and
>  > na.exclude that eludes me.  I'm sure it is documented and I could,
>  > with time, unravel the mysteries but I really would prefer not to
>  > spend that time.  I was hoping that someone else knew what the
>  > distinction was and how one would construct an extractor that would
>  > return the desired value and not be such an abominable kludge as the
>  > above.
>
>  Yes. I've prepared and tested the following patch:
>
>  Index: C:/Users/hennil/Desktop/R_packages/source/lme4_0.999375-8/R/lmer.R
>  ===================================================================
>  --- C:/Users/hennil/Desktop/R_packages/source/lme4_0.999375-8/R/lmer.R
>  (revision 99)
>  +++ C:/Users/hennil/Desktop/R_packages/source/lme4_0.999375-8/R/lmer.R
>  (working copy)
>  @@ -882,7 +882,14 @@
>         })
>
>   setMethod("fitted", signature(object = "mer"),
>  -         function(object, ...) object at eta)
>  +         function(object, ...)
>  +      {
>  +          eta <- object at eta
>  +          na.action <- attr(object at frame, "na.action")
>  +          if(!is.null(na.action))
>  +              napredict(na.action, eta)
>  +          else eta
>  +      })
>
>   setMethod("formula", signature(x = "mer"),
>           function(x, ...)
>
>
>  After applying the patch, the distinction between `na.omit' and
>  `na.exclude' is easily shown:
>
>
>   > fm1 <- lmer(Reaction ~ Days + (Days|Subject), slps)
>   > fitted(fm1)
>
>    [1] 253.4849 273.1808 292.8766 312.5725 332.2684 351.9642
>    [7] 371.6601 391.3559 411.0518 430.7476 213.1921 214.8561
>   [13] 216.5201 219.8481 221.5121 223.1761 224.8401 226.5041
>   [19] 228.1681 212.8984 217.8564 222.8143 227.7723 232.7302
>   [25] 237.6882 242.6462 247.6041 252.5621 257.5200 275.9163
>   [31] 281.5164 287.1166 298.3168 303.9170 309.5171 315.1173
>   [37] 320.7174 326.3175 273.5320 280.9433 288.3545 295.7658
>   [43] 303.1771 310.5884 317.9997 325.4110 332.8223 340.2336
>   [49] 260.3778 270.5809 280.7841 290.9872 301.1903 311.3934
>   [55] 321.5965 331.7996 342.0027 352.2058 268.1086 278.3689
>   [61] 288.6293 298.8896 309.1500 319.4103 329.6707 339.9310
>   [67] 350.1914 360.4517 244.2257 255.7619 267.2981 278.8342
>   [73] 290.3704 301.9066 313.4427 324.9789 336.5151 348.0512
>   [79] 251.2783 250.9591 250.6398 250.3205 250.0013 249.6820
>   [85] 249.3627 249.0435 248.7242 248.4050 283.8511 322.6857
>   [91] 342.1031 361.5204 380.9377 400.3550 419.7723 439.1897
>   [97] 458.6070 226.4061 238.0217 249.6373 261.2529 272.8685
>  [103] 284.4841 296.0997 307.7153 319.3310 330.9466 238.3395
>  [109] 255.4259 272.5123 289.5986 306.6850 323.7714 340.8578
>  [115] 357.9442 375.0305 392.1169 256.0056 263.4524 270.8991
>  [121] 278.3458 285.7925 293.2393 300.6860 308.1327 315.5795
>  [127] 323.0262 272.0277 286.0636 300.0995 314.1354 328.1714
>  [133] 342.2073 356.2432 370.2791 384.3150 398.3510 254.6441
>  [139] 265.9888 277.3335 288.6782 300.0229 311.3676 322.7123
>  [145] 334.0570 345.4017 356.7464 225.9405 241.2159 256.4914
>  [151] 271.7668 287.0422 302.3176 317.5930 332.8684 348.1438
>  [157] 363.4192 252.2315 261.7074 271.1833 280.6592 290.1351
>  [163] 299.6110 309.0870 318.5629 328.0388 337.5147 263.5955
>  [169] 275.3631 287.1308 298.8985 310.6662 322.4338 334.2015
>  [175] 345.9692 357.7368 369.5045
>   >
>   > fm2 <- lmer(Reaction ~ Days + (Days|Subject), slps,
>  +             na.action = "na.exclude")
>   > fitted(fm2)
>
>    [1] 253.4849 273.1808 292.8766 312.5725 332.2684 351.9642
>    [7] 371.6601 391.3559 411.0518 430.7476 213.1921 214.8561
>   [13] 216.5201       NA 219.8481 221.5121 223.1761 224.8401
>   [19] 226.5041 228.1681 212.8984 217.8564 222.8143 227.7723
>
>   [25] 232.7302 237.6882 242.6462 247.6041 252.5621 257.5200
>   [31] 275.9163 281.5164 287.1166       NA 298.3168 303.9170
>   [37] 309.5171 315.1173 320.7174 326.3175 273.5320 280.9433
>   [43] 288.3545 295.7658 303.1771 310.5884 317.9997 325.4110
>
>   [49] 332.8223 340.2336 260.3778 270.5809 280.7841 290.9872
>   [55] 301.1903 311.3934 321.5965 331.7996 342.0027 352.2058
>   [61] 268.1086 278.3689 288.6293 298.8896 309.1500 319.4103
>   [67] 329.6707 339.9310 350.1914 360.4517 244.2257 255.7619
>
>   [73] 267.2981 278.8342 290.3704 301.9066 313.4427 324.9789
>   [79] 336.5151 348.0512 251.2783 250.9591 250.6398 250.3205
>   [85] 250.0013 249.6820 249.3627 249.0435 248.7242 248.4050
>   [91] 283.8511       NA 322.6857 342.1031 361.5204 380.9377
>
>   [97] 400.3550 419.7723 439.1897 458.6070 226.4061 238.0217
>  [103] 249.6373 261.2529 272.8685 284.4841 296.0997 307.7153
>  [109] 319.3310 330.9466 238.3395 255.4259 272.5123 289.5986
>  [115] 306.6850 323.7714 340.8578 357.9442 375.0305 392.1169
>
> [121] 256.0056 263.4524 270.8991 278.3458 285.7925 293.2393
>  [127] 300.6860 308.1327 315.5795 323.0262 272.0277 286.0636
>  [133] 300.0995 314.1354 328.1714 342.2073 356.2432 370.2791
>  [139] 384.3150 398.3510 254.6441 265.9888 277.3335 288.6782
>
> [145] 300.0229 311.3676 322.7123 334.0570 345.4017 356.7464
>  [151] 225.9405 241.2159 256.4914 271.7668 287.0422 302.3176
>  [157] 317.5930 332.8684 348.1438 363.4192 252.2315 261.7074
>  [163] 271.1833 280.6592 290.1351 299.6110 309.0870 318.5629
>
> [169] 328.0388 337.5147 263.5955 275.3631 287.1308 298.8985
>  [175] 310.6662 322.4338 334.2015 345.9692 357.7368 369.5045
>   >
>
>  I hope you find this useful.
>
>
>  Henric
>
>
>
>
>  >
>  >
>  > On Fri, Mar 14, 2008 at 9:43 AM, Douglas Bates <bates at stat.wisc.edu> wrote:
>  >> On Fri, Mar 14, 2008 at 7:31 AM, Andy Fugard <a.fugard at ed.ac.uk> wrote:
>  >>
>  >>  >  Is there any way to get fitted values out of lmer so that any NAs in the
>  >>  >  predictors/outcome pop out in the correct position?
>  >>
>  >>  Yes.
>  >>
>  >>
>  >>  >   From what I can tell, the "fitted" function doesn't have access to
>  >>  >  missingness in the inputs.
>  >>
>  >>  It does not, in its current implementation.  I believe there is
>  >>  information regarding the pattern of missingness in the model frame
>  >>  (stored in the "frame" slot of the fitted model) and that can be used
>  >>  to generate an appropriate fitted response.
>  >>
>  >>  Does anyone know exactly how this is done for linear models?  If so,
>  >>  are you willing to provide a patch to the fitted method for mer
>  >>  objects?  I can put it on my ToDo list but when the first item on the
>  >>  list is "finish the book" you almost never get to the second and
>  >>  subsequent items.
>  >>
>  >
>
>
> > _______________________________________________
>  > R-sig-mixed-models at r-project.org mailing list
>  > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>  >
>



From bates at stat.wisc.edu  Fri Mar 14 23:11:34 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 14 Mar 2008 17:11:34 -0500
Subject: [R-sig-ME] Fitted values in lmer
In-Reply-To: <40e66e0b0803141438s3fd585bal36453a8ab037f411@mail.gmail.com>
References: <47DA702D.7050802@ed.ac.uk>
	<40e66e0b0803140743g269c027as9784c34b51d845ca@mail.gmail.com>
	<40e66e0b0803141328u1e0c94f6ke5792229e94a6f1a@mail.gmail.com>
	<47DAEE05.8050602@gmail.com>
	<40e66e0b0803141438s3fd585bal36453a8ab037f411@mail.gmail.com>
Message-ID: <40e66e0b0803141511u6927784cj83f662d5e3b895a8@mail.gmail.com>

Patch is now committed.  I took the liberty of shortening it to a
one-liner and making similar fixes for the residuals and resid
methods.

Does anyone know enough about the definitions of the different types
of residuals for generalized linear models to implement a "type"
argument for the residuals method?

On Fri, Mar 14, 2008 at 4:38 PM, Douglas Bates <bates at stat.wisc.edu> wrote:
> Thank you, Henric.  That's exactly what I wanted.  I will change one
>  glich, which is my glich, not yours.  The original method should have
>  returned the mu slot, not the eta slot.  These are the same numerical
>  values for linear mixed models but not for generalized linear mixed
>  models.
>
>
>
>  On Fri, Mar 14, 2008 at 4:28 PM, Henric Nilsson (Public)
>  <nilsson.henric at gmail.com> wrote:
>  >
>  > Douglas Bates wrote:
>  >
>  >  > What I was referring to is the na.action attribute of the model frame
>  >  >
>  >  >> library(lme4)
>  >  > Loading required package: Matrix
>  >  > Loading required package: lattice
>  >  >
>  >  > Attaching package: 'Matrix'
>  >  >
>  >  >
>  >  >       The following object(s) are masked from package:stats :
>  >  >
>  >  >        xtabs
>  >  >
>  >  >> slps <- sleepstudy
>  >  >> slps$Reaction[c(14, 34, 92)] <- NA
>  >  >> fm1 <- lmer(Reaction ~ Days + (Days|Subject), slps)
>  >  >> str(fm1 at frame)
>  >  > 'data.frame': 177 obs. of  3 variables:
>  >  >  $ Reaction: num  250 259 251 321 357 ...
>  >  >  $ Days    : num  0 1 2 3 4 5 6 7 8 9 ...
>  >  >  $ Subject : Factor w/ 18 levels "308","309","310",..: 1 1 1 1 1 1 1 1 1 1 ...
>  >  >  - attr(*, "terms")=Classes 'terms', 'formula' length 3 Reaction ~ Days
>  >  >   .. ..- attr(*, "variables")= language list(Reaction, Days)
>  >  >   .. ..- attr(*, "factors")= int [1:2, 1] 0 1
>  >  >   .. .. ..- attr(*, "dimnames")=List of 2
>  >  >   .. .. .. ..$ : chr [1:2] "Reaction" "Days"
>  >  >   .. .. .. ..$ : chr "Days"
>  >  >   .. ..- attr(*, "term.labels")= chr "Days"
>  >  >   .. ..- attr(*, "order")= int 1
>  >  >   .. ..- attr(*, "intercept")= int 1
>  >  >   .. ..- attr(*, "response")= int 1
>  >  >   .. ..- attr(*, ".Environment")=<R_GlobalEnv>
>  >  >   .. ..- attr(*, "predvars")= language list(Reaction, Days)
>  >  >   .. ..- attr(*, "dataClasses")= Named chr [1:2] "numeric" "numeric"
>  >  >   .. .. ..- attr(*, "names")= chr [1:2] "Reaction" "Days"
>  >  >  - attr(*, "na.action")=Class 'omit'  Named int [1:3] 14 34 92
>  >  >   .. ..- attr(*, "names")= chr [1:3] "14" "34" "92"
>  >  >
>  >  > At this point one can get the fitted values with the NA's in the
>  >  > correct positions via the thoroughly unobvious
>  >  >
>  >  >> stats:::naresid.exclude(attr(fm1 at frame, "na.action"), fm1 at mu)
>  >  >   [1] 253.4849 273.1808 292.8766 312.5725 332.2684 351.9642 371.6601 391.3559
>  >  >   [9] 411.0518 430.7476 213.1921 214.8561 216.5201       NA 219.8481 221.5121
>  >  >  [17] 223.1761 224.8401 226.5041 228.1681 212.8984 217.8564 222.8143 227.7723
>  >  >  [25] 232.7302 237.6882 242.6462 247.6041 252.5621 257.5200 275.9163 281.5164
>  >  >  [33] 287.1166       NA 298.3168 303.9170 309.5171 315.1173 320.7174 326.3175
>  >  >  [41] 273.5320 280.9433 288.3545 295.7658 303.1771 310.5884 317.9997 325.4110
>  >  >  [49] 332.8223 340.2336 260.3778 270.5809 280.7841 290.9872 301.1903 311.3934
>  >  >  [57] 321.5965 331.7996 342.0027 352.2058 268.1086 278.3689 288.6293 298.8896
>  >  >  [65] 309.1500 319.4103 329.6707 339.9310 350.1914 360.4517 244.2257 255.7619
>  >  >  [73] 267.2981 278.8342 290.3704 301.9066 313.4427 324.9789 336.5151 348.0512
>  >  >  [81] 251.2783 250.9591 250.6398 250.3205 250.0013 249.6820 249.3627 249.0435
>  >  >  [89] 248.7242 248.4050 283.8511       NA 322.6857 342.1031 361.5204 380.9377
>  >  >  [97] 400.3550 419.7723 439.1897 458.6070 226.4061 238.0217 249.6373 261.2529
>  >  > [105] 272.8685 284.4841 296.0997 307.7153 319.3310 330.9466 238.3395 255.4259
>  >  > [113] 272.5123 289.5986 306.6850 323.7714 340.8578 357.9442 375.0305 392.1169
>  >  > [121] 256.0056 263.4524 270.8991 278.3458 285.7925 293.2393 300.6860 308.1327
>  >  > [129] 315.5795 323.0262 272.0277 286.0636 300.0995 314.1354 328.1714 342.2073
>  >  > [137] 356.2432 370.2791 384.3150 398.3510 254.6441 265.9888 277.3335 288.6782
>  >  > [145] 300.0229 311.3676 322.7123 334.0570 345.4017 356.7464 225.9405 241.2159
>  >  > [153] 256.4914 271.7668 287.0422 302.3176 317.5930 332.8684 348.1438 363.4192
>  >  > [161] 252.2315 261.7074 271.1833 280.6592 290.1351 299.6110 309.0870 318.5629
>  >  > [169] 328.0388 337.5147 263.5955 275.3631 287.1308 298.8985 310.6662 322.4338
>  >  > [177] 334.2015 345.9692 357.7368 369.5045
>  >  >
>  >  > There is some subtlety about the difference between na.omit and
>  >  > na.exclude that eludes me.  I'm sure it is documented and I could,
>  >  > with time, unravel the mysteries but I really would prefer not to
>  >  > spend that time.  I was hoping that someone else knew what the
>  >  > distinction was and how one would construct an extractor that would
>  >  > return the desired value and not be such an abominable kludge as the
>  >  > above.
>  >
>  >  Yes. I've prepared and tested the following patch:
>  >
>  >  Index: C:/Users/hennil/Desktop/R_packages/source/lme4_0.999375-8/R/lmer.R
>  >  ===================================================================
>  >  --- C:/Users/hennil/Desktop/R_packages/source/lme4_0.999375-8/R/lmer.R
>  >  (revision 99)
>  >  +++ C:/Users/hennil/Desktop/R_packages/source/lme4_0.999375-8/R/lmer.R
>  >  (working copy)
>  >  @@ -882,7 +882,14 @@
>  >         })
>  >
>  >   setMethod("fitted", signature(object = "mer"),
>  >  -         function(object, ...) object at eta)
>  >  +         function(object, ...)
>  >  +      {
>  >  +          eta <- object at eta
>  >  +          na.action <- attr(object at frame, "na.action")
>  >  +          if(!is.null(na.action))
>  >  +              napredict(na.action, eta)
>  >  +          else eta
>  >  +      })
>  >
>  >   setMethod("formula", signature(x = "mer"),
>  >           function(x, ...)
>  >
>  >
>  >  After applying the patch, the distinction between `na.omit' and
>  >  `na.exclude' is easily shown:
>  >
>  >
>  >   > fm1 <- lmer(Reaction ~ Days + (Days|Subject), slps)
>  >   > fitted(fm1)
>  >
>  >    [1] 253.4849 273.1808 292.8766 312.5725 332.2684 351.9642
>  >    [7] 371.6601 391.3559 411.0518 430.7476 213.1921 214.8561
>  >   [13] 216.5201 219.8481 221.5121 223.1761 224.8401 226.5041
>  >   [19] 228.1681 212.8984 217.8564 222.8143 227.7723 232.7302
>  >   [25] 237.6882 242.6462 247.6041 252.5621 257.5200 275.9163
>  >   [31] 281.5164 287.1166 298.3168 303.9170 309.5171 315.1173
>  >   [37] 320.7174 326.3175 273.5320 280.9433 288.3545 295.7658
>  >   [43] 303.1771 310.5884 317.9997 325.4110 332.8223 340.2336
>  >   [49] 260.3778 270.5809 280.7841 290.9872 301.1903 311.3934
>  >   [55] 321.5965 331.7996 342.0027 352.2058 268.1086 278.3689
>  >   [61] 288.6293 298.8896 309.1500 319.4103 329.6707 339.9310
>  >   [67] 350.1914 360.4517 244.2257 255.7619 267.2981 278.8342
>  >   [73] 290.3704 301.9066 313.4427 324.9789 336.5151 348.0512
>  >   [79] 251.2783 250.9591 250.6398 250.3205 250.0013 249.6820
>  >   [85] 249.3627 249.0435 248.7242 248.4050 283.8511 322.6857
>  >   [91] 342.1031 361.5204 380.9377 400.3550 419.7723 439.1897
>  >   [97] 458.6070 226.4061 238.0217 249.6373 261.2529 272.8685
>  >  [103] 284.4841 296.0997 307.7153 319.3310 330.9466 238.3395
>  >  [109] 255.4259 272.5123 289.5986 306.6850 323.7714 340.8578
>  >  [115] 357.9442 375.0305 392.1169 256.0056 263.4524 270.8991
>  >  [121] 278.3458 285.7925 293.2393 300.6860 308.1327 315.5795
>  >  [127] 323.0262 272.0277 286.0636 300.0995 314.1354 328.1714
>  >  [133] 342.2073 356.2432 370.2791 384.3150 398.3510 254.6441
>  >  [139] 265.9888 277.3335 288.6782 300.0229 311.3676 322.7123
>  >  [145] 334.0570 345.4017 356.7464 225.9405 241.2159 256.4914
>  >  [151] 271.7668 287.0422 302.3176 317.5930 332.8684 348.1438
>  >  [157] 363.4192 252.2315 261.7074 271.1833 280.6592 290.1351
>  >  [163] 299.6110 309.0870 318.5629 328.0388 337.5147 263.5955
>  >  [169] 275.3631 287.1308 298.8985 310.6662 322.4338 334.2015
>  >  [175] 345.9692 357.7368 369.5045
>  >   >
>  >   > fm2 <- lmer(Reaction ~ Days + (Days|Subject), slps,
>  >  +             na.action = "na.exclude")
>  >   > fitted(fm2)
>  >
>  >    [1] 253.4849 273.1808 292.8766 312.5725 332.2684 351.9642
>  >    [7] 371.6601 391.3559 411.0518 430.7476 213.1921 214.8561
>  >   [13] 216.5201       NA 219.8481 221.5121 223.1761 224.8401
>  >   [19] 226.5041 228.1681 212.8984 217.8564 222.8143 227.7723
>  >
>  >   [25] 232.7302 237.6882 242.6462 247.6041 252.5621 257.5200
>  >   [31] 275.9163 281.5164 287.1166       NA 298.3168 303.9170
>  >   [37] 309.5171 315.1173 320.7174 326.3175 273.5320 280.9433
>  >   [43] 288.3545 295.7658 303.1771 310.5884 317.9997 325.4110
>  >
>  >   [49] 332.8223 340.2336 260.3778 270.5809 280.7841 290.9872
>  >   [55] 301.1903 311.3934 321.5965 331.7996 342.0027 352.2058
>  >   [61] 268.1086 278.3689 288.6293 298.8896 309.1500 319.4103
>  >   [67] 329.6707 339.9310 350.1914 360.4517 244.2257 255.7619
>  >
>  >   [73] 267.2981 278.8342 290.3704 301.9066 313.4427 324.9789
>  >   [79] 336.5151 348.0512 251.2783 250.9591 250.6398 250.3205
>  >   [85] 250.0013 249.6820 249.3627 249.0435 248.7242 248.4050
>  >   [91] 283.8511       NA 322.6857 342.1031 361.5204 380.9377
>  >
>  >   [97] 400.3550 419.7723 439.1897 458.6070 226.4061 238.0217
>  >  [103] 249.6373 261.2529 272.8685 284.4841 296.0997 307.7153
>  >  [109] 319.3310 330.9466 238.3395 255.4259 272.5123 289.5986
>  >  [115] 306.6850 323.7714 340.8578 357.9442 375.0305 392.1169
>  >
>  > [121] 256.0056 263.4524 270.8991 278.3458 285.7925 293.2393
>  >  [127] 300.6860 308.1327 315.5795 323.0262 272.0277 286.0636
>  >  [133] 300.0995 314.1354 328.1714 342.2073 356.2432 370.2791
>  >  [139] 384.3150 398.3510 254.6441 265.9888 277.3335 288.6782
>  >
>  > [145] 300.0229 311.3676 322.7123 334.0570 345.4017 356.7464
>  >  [151] 225.9405 241.2159 256.4914 271.7668 287.0422 302.3176
>  >  [157] 317.5930 332.8684 348.1438 363.4192 252.2315 261.7074
>  >  [163] 271.1833 280.6592 290.1351 299.6110 309.0870 318.5629
>  >
>  > [169] 328.0388 337.5147 263.5955 275.3631 287.1308 298.8985
>  >  [175] 310.6662 322.4338 334.2015 345.9692 357.7368 369.5045
>  >   >
>  >
>  >  I hope you find this useful.
>  >
>  >
>  >  Henric
>  >
>  >
>  >
>  >
>  >  >
>  >  >
>  >  > On Fri, Mar 14, 2008 at 9:43 AM, Douglas Bates <bates at stat.wisc.edu> wrote:
>  >  >> On Fri, Mar 14, 2008 at 7:31 AM, Andy Fugard <a.fugard at ed.ac.uk> wrote:
>  >  >>
>  >  >>  >  Is there any way to get fitted values out of lmer so that any NAs in the
>  >  >>  >  predictors/outcome pop out in the correct position?
>  >  >>
>  >  >>  Yes.
>  >  >>
>  >  >>
>  >  >>  >   From what I can tell, the "fitted" function doesn't have access to
>  >  >>  >  missingness in the inputs.
>  >  >>
>  >  >>  It does not, in its current implementation.  I believe there is
>  >  >>  information regarding the pattern of missingness in the model frame
>  >  >>  (stored in the "frame" slot of the fitted model) and that can be used
>  >  >>  to generate an appropriate fitted response.
>  >  >>
>  >  >>  Does anyone know exactly how this is done for linear models?  If so,
>  >  >>  are you willing to provide a patch to the fitted method for mer
>  >  >>  objects?  I can put it on my ToDo list but when the first item on the
>  >  >>  list is "finish the book" you almost never get to the second and
>  >  >>  subsequent items.
>  >  >>
>  >  >
>  >
>  >
>  > > _______________________________________________
>  >  > R-sig-mixed-models at r-project.org mailing list
>  >  > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>  >  >
>  >
>



From a.fugard at ed.ac.uk  Sat Mar 15 13:59:39 2008
From: a.fugard at ed.ac.uk (Andy Fugard)
Date: Sat, 15 Mar 2008 12:59:39 +0000
Subject: [R-sig-ME] Fitted values in lmer
In-Reply-To: <40e66e0b0803141328u1e0c94f6ke5792229e94a6f1a@mail.gmail.com>
References: <47DA702D.7050802@ed.ac.uk>	
	<40e66e0b0803140743g269c027as9784c34b51d845ca@mail.gmail.com>
	<40e66e0b0803141328u1e0c94f6ke5792229e94a6f1a@mail.gmail.com>
Message-ID: <47DBC83B.3020009@ed.ac.uk>

Douglas Bates wrote:
> What I was referring to is the na.action attribute of the model frame

Aha, I didn't wade that deeply.

> 
> At this point one can get the fitted values with the NA's in the
> correct positions via the thoroughly unobvious
> 
>> stats:::naresid.exclude(attr(fm1 at frame, "na.action"), fm1 at mu)

:-)

Many thanks.

Andy

-- 
Andy Fugard, Postgraduate Research Student
Psychology (Room F3), The University of Edinburgh,
   7 George Square, Edinburgh EH8 9JZ, UK
Mobile: +44 (0)78 123 87190   http://www.possibly.me.uk



From martin.eklund at farmbio.uu.se  Sat Mar 15 14:42:32 2008
From: martin.eklund at farmbio.uu.se (Martin Eklund)
Date: Sat, 15 Mar 2008 14:42:32 +0100
Subject: [R-sig-ME] Yet another question about model specification and
	interpretation
In-Reply-To: <40e66e0b0803140926g3d4fc772r9d62115d1d30bba5@mail.gmail.com>
References: <E043950C-0886-4708-BFFC-E2AD74C32D80@farmbio.uu.se>
	<40e66e0b0803041242q3ded4b8k73cf6e0b556aaff6@mail.gmail.com>
	<DE12A0D4-5791-43A3-9555-4F4A60521759@farmbio.uu.se>
	<40e66e0b0803140926g3d4fc772r9d62115d1d30bba5@mail.gmail.com>
Message-ID: <36E8152C-0165-4657-90A8-96BDE9623441@farmbio.uu.se>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080315/961c90ac/attachment.pl>

From bates at stat.wisc.edu  Sat Mar 15 18:38:06 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 15 Mar 2008 12:38:06 -0500
Subject: [R-sig-ME] Yet another question about model specification and
	interpretation
In-Reply-To: <36E8152C-0165-4657-90A8-96BDE9623441@farmbio.uu.se>
References: <E043950C-0886-4708-BFFC-E2AD74C32D80@farmbio.uu.se>
	<40e66e0b0803041242q3ded4b8k73cf6e0b556aaff6@mail.gmail.com>
	<DE12A0D4-5791-43A3-9555-4F4A60521759@farmbio.uu.se>
	<40e66e0b0803140926g3d4fc772r9d62115d1d30bba5@mail.gmail.com>
	<36E8152C-0165-4657-90A8-96BDE9623441@farmbio.uu.se>
Message-ID: <40e66e0b0803151038l2c0b13adqd3bfa12e2586a971@mail.gmail.com>

On Sat, Mar 15, 2008 at 8:42 AM, Martin Eklund
<martin.eklund at farmbio.uu.se> wrote:
> Hi,
>
>  The pasted-in email below was intended for this mailing list, but I
>  accidently only sent it to Douglas Bates (very sorry!). Please see
>  below the pasted-in email for some more specifications about my
>  question.
>
>  >> I'm sorry if the following question is a wee bit stupid, but I
>  >> have to ask
>  >> to get this properly figured out.
>  >>
>  >> Continuing with the Machines data from the MEMSS package (see
>  >> below) as an
>  >> example. Suppose the models:
>  >>
>  >> (i) lmer(score ~ Machine + (Machine | Worker))
>  >> (ii) lmer(score ~ Machine + (1 | Worker) + (0 + Machine | Worker))
>  >
>  > Model (ii) cannot be estimated (or if it can, it shouldn't be).  The
>  > model that makes the most sense is
>  >
>  > lmer(score ~ Machine + (0+Machine | Worker))
>  >
>  > May I suggest that you look at the model matrices for the different
>  > models to see why?  I am really pushed for time right now and I just
>  > don't have the time to write long explanations.  Alternatively, if you
>  > were to copy this discussion to the R-SIG-Mixed-Models at R-project.org
>  > list I think there are others reading the list who could explain why.
>  >
>  >> In (i), score is modeled by the Machine fixed factor and the random
>  >> interaction between Machine and the Worker factor (which is
>  >> considered to be
>  >> random). (i) uses 9 degrees of freedom. Here the Worker effect is
>  >> confounded
>  >> within the intercept of the (Machine | Worker) term.
>  >> In (ii), score is modeled by the Machine fixed factor, the random
>  >> Worker
>  >> factor, and the random interaction between Machine and Worker.
>  >> (ii) uses 10
>  >> degrees of freedom. Here we can separate the Worker effect from the
>  >> Machine-Worker interaction, which costs us one degree of freedom.
>  >>
>  >> My question is simply: Have I interpreted this correctly? If not,
>  >> any help
>  >> on how to interpret the difference between model (i) and (ii) is
>  >> greatly
>  >> appreciated!
>  >>
>  >> Thank you!
>  >>
>  >> Best regards,
>  >>
>  >> Martin.
>
>  It is indeed possible to estimate model (ii), the output is:
>
>  Linear mixed-effects model fit by REML
>  Formula: score ~ Machine + (1 | Worker) + (0 + Machine | Worker)
>     Data: Machines
>     AIC   BIC logLik MLdeviance REMLdeviance
>   232.3 252.2 -106.2      221.4        212.3
>  Random effects:
>   Groups   Name        Variance Std.Dev. Corr
>   Worker   (Intercept) 18.40914 4.29059
>   Worker   MachineA    10.77633 3.28273
>            MachineB    34.51234 5.87472   0.299
>            MachineC     0.10678 0.32678  -0.937 -0.614
>   Residual              0.92999 0.96436
>  number of obs: 54, groups: Worker, 6; Worker, 6
>
>  Fixed effects:
>              Estimate Std. Error t value
>  (Intercept)   52.356      2.217  23.614
>  MachineB       7.967      2.393   3.329
>  MachineC      13.917      1.501   9.273
>
>  Correlation of Fixed Effects:
>           (Intr) MachnB
>  MachineB -0.167
>  MachineC -0.606  0.238
>
>  I'd like to to get the Worker effect estimated as well as the
>  interaction effects between Machine and Worker. The output above, in
>  my eyes, seems to provide exactly this. I understand that there's
>  something that I don't understand, and I'd very much appreciate any
>  help on interpreting the different models (i) and (ii) and why model
>  (ii) doesn't make sense.
>
>  The output from fitting model (i) is:
>
>  Linear mixed-effects model fit by REML
>  Formula: score ~ Machine + (Machine | Worker)
>     Data: Machines
>     AIC   BIC logLik MLdeviance REMLdeviance
>   226.3 244.2 -104.2      216.6        208.3
>  Random effects:
>   Groups   Name        Variance Std.Dev. Corr
>   Worker   (Intercept) 16.61570 4.0762
>            MachineB    34.47572 5.8716    0.483
>            MachineC    13.64510 3.6939   -0.365  0.298
>   Residual              0.92485 0.9617
>  number of obs: 54, groups: Worker, 6
>
>  Fixed effects:
>              Estimate Std. Error t value
>  (Intercept)   52.356      1.679  31.174
>  MachineB       7.967      2.418   3.294
>  MachineC      13.917      1.542   9.027
>
>  Correlation of Fixed Effects:
>           (Intr) MachnB
>  MachineB  0.461
>  MachineC -0.374  0.303
>
>  To me, it here looks as we get the interactions between Machine and
>  Worker, but that the Worker effect is confounded in the intercept of
>  the interaction effect. If we look at the BLUP:s for the models (i)
>  and (ii) we get the following:
>
>   > ranef("model (ii)")
>  An object of class "ranef.lmer"
>  [[1]]
>    (Intercept)
>  1   0.8924564
>  2  -3.8789789
>  3   4.7753576
>  4  -1.3654599
>  5   4.7893765
>  6  -5.2127517>
>
>  [[2]]
>      MachineA     MachineB    MachineC
>  1 -0.5889748   1.66535371  0.01455685
>  2  3.9941506   3.11719442 -0.39242270
>  3  2.3434019   2.91923878 -0.25246004
>  4  0.2583581   3.74188503 -0.09753681
>  5 -5.6182390  -0.07808906  0.46421605
>  6 -0.3886967 -11.36558288  0.26364665
>
>   > ranef("model (i)")
>  An object of class "ranef.lmer"
>  [[1]]
>    (Intercept)    MachineB   MachineC
>  1   0.3119782   2.2411827  0.6184094
>  2   0.1841904  -0.9876673 -4.4665295
>  3   6.9691957   0.8101625 -2.4958603
>  4  -1.0237402   3.3520102 -0.3907095
>  5  -0.8499415   5.5760567  6.1734022
>  6  -5.5916826 -10.9917448  0.5612878
>
>  I interpret this as that ranef("model (ii)")[[1]] is the BLUP for the
>  Worker factor and ranef("model (ii)")[[2]] the BLUP for the
>  interactions. as.matrix(ranef("model (ii)")[[1]]) + as.matrix(ranef
>  ("model (ii)")[[2]][1]) produces pretty much as.matrix(ranef("model
>  (i)")[[1]][1]), which is why I thought that the Worker effect in
>  model (i) is confounded in the intercept of the interaction effects.
>
>  Further, considering the model
>
>  (iii)   lmer(score ~ Machine + (0 + Machine | Worker)
>
>  we get the following output:
>
>  Linear mixed-effects model fit by REML
>  Formula: score ~ Machine + (0 + Machine | Worker)
>     Data: Machines
>     AIC   BIC logLik MLdeviance REMLdeviance
>   226.3 244.2 -104.2      216.6        208.3
>  Random effects:
>   Groups   Name     Variance Std.Dev. Corr
>   Worker   MachineA 16.64234 4.07950
>            MachineB 74.37306 8.62398  0.803
>            MachineC 19.26441 4.38912  0.623 0.771
>   Residual           0.92464 0.96158
>  number of obs: 54, groups: Worker, 6
>
>  Fixed effects:
>              Estimate Std. Error t value
>  (Intercept)   52.356      1.681  31.149
>  MachineB       7.967      2.420   3.292
>  MachineC      13.917      1.540   9.037
>
>  Correlation of Fixed Effects:
>           (Intr) MachnB
>  MachineB  0.463
>  MachineC -0.374  0.301
>
>  I (probably incorrectly) interpret this as that we don't get an
>  estimate of the Worker effect, but only the interaction between the
>  Worker and the Machine.
>
>  If anyone can tell me where I go wrong in my reasoning about the
>  models (i), (ii), and (ii), I'd be very happy and thankful.

Take a look at the deviances of the three models.  To do a safer
comparison it would be better to refit all three models with method =
"ML" before comparing values like AIC and BIC but you can notice that
the deviance (both ML and REML) and the AIC and BIC of models (ii) and
(iii) are essentially identical, which is as it should be because the
only differences are in the parameterization of the random effects.
However, model (i), which has an additional parameter compared to
models (ii) and (iii), gives higher AIC, BIC and deviances.  This is a
sign that something is going wrong in that model.

As I said in my private reply, fitting that model should fail but
apparently it doesn't.  The software converges to a spurious result
and it takes a bit of digging to discover that the result is indeed
spurious.  It would be better if the software weree to detect such
problems and indeed it is possible but difficult to do this. It is
also the case that all of the programming effort to do so and time
spent checking models for problems like this diverts from other
enhancements that could be made to the software.

As an editorial comment, those who would like to have the ability to
impose correlation structures or variance functions, in addition to
the correlations generated by the random effects, on the conditional
distribution of the response given the random effects (i.e. make lmer
more like lme in the range of models that can be fit) may want to bear
this case of getting a spurious fit to a model that is not well
defined in mind.  Surprisingly the majority of the effort in allowing
specifications like that is not in designing and implementing the data
structures and numerical algorithms.  That part is comparatively easy.
 The difficult part is deciding how the user should specify the model
and how the specification is translated into the structures and
algorithms.  You may think, "But that's easy - just use the lme
specification" but that won't work.  The nlme package was created for
S and S-PLUS then ported to R and it does not follow the set of steps
for model-fitting functions that is native to R.  Most of the time
that is not a problem but every once in a while it breaks and it
breaks in ugly ways.  Think of the recent discussion on this list
about getting fitted values and residuals that appropriately take
account of the pattern of missingness in the original data.  How does
the information on the pattern of missingness get into the model frame
in the first place?  How are the "subset", "na.action", "offset", etc.
arguments handled by model-fitting functions in R?

These are subtle and difficult issues.  Furthermore, the definition of
the models themselves must be done with care.  It is not possible to
independently specify correlations, variance structures, conditional
distributions (in the sense of models for binomial or Poisson
responses) and arbitrary random effects structures.  Certain
combinations don't make sense.  It is actually stronger than that,
certain combinations are impossible.  Witness the recent suggestion
that the lme4 package is somehow inferior because it doesn't allow
specification of "the" repeated measures model for a binary response.
Apparently "the" repeated measures model specifies a certain
conditional variance-covariance structure in the responses and the
fact that this structure is incompatible with the variances implied by
the binomial response should not prevent specification and fitting of
such a model.  Mathematical impossibility is apparently a rather
flimsy excuse for not providing software that fits such a model.

Later I'll write more on the issue of variance structures and
correlation structures.  I plan to provide classes for extensions to
the mixed models (in fact if you look at the AllClasses.R file on the
archive you will see they are already there) and show how they could
be used to fit certain simple extensions, like a model with stratified
conditional variances.  Then a person who wants to create software to
do a "one off" fit of a mixed model  including various "mix-in"
specifications can do so.  However, I don't plan on offering general
model specifications in lmer until I, or someone else, can work out
end-to-end how to do it without painting yourself into a corner where
impossible model specifications can be fit.  And also I want it to be
done in an Rish way so that all the other model arguments work as they
should.  (The main stumbling block there is that a call to model.frame
allows for only 1 formula and you must be very careful how you create
that formula.  If you create an omnibus formula from a collection of
other formulas you have to decide what the environment of the omnibus
formula should be.)



From martin.eklund at farmbio.uu.se  Sun Mar 16 13:40:17 2008
From: martin.eklund at farmbio.uu.se (Martin Eklund)
Date: Sun, 16 Mar 2008 13:40:17 +0100
Subject: [R-sig-ME] Yet another question about model specification and
	interpretation
In-Reply-To: <40e66e0b0803151038l2c0b13adqd3bfa12e2586a971@mail.gmail.com>
References: <E043950C-0886-4708-BFFC-E2AD74C32D80@farmbio.uu.se>
	<40e66e0b0803041242q3ded4b8k73cf6e0b556aaff6@mail.gmail.com>
	<DE12A0D4-5791-43A3-9555-4F4A60521759@farmbio.uu.se>
	<40e66e0b0803140926g3d4fc772r9d62115d1d30bba5@mail.gmail.com>
	<36E8152C-0165-4657-90A8-96BDE9623441@farmbio.uu.se>
	<40e66e0b0803151038l2c0b13adqd3bfa12e2586a971@mail.gmail.com>
Message-ID: <83AF5394-10B8-480F-9FB2-F31B701C9432@farmbio.uu.se>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080316/b380940b/attachment.pl>

From bates at stat.wisc.edu  Sun Mar 16 14:09:32 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 16 Mar 2008 08:09:32 -0500
Subject: [R-sig-ME] Yet another question about model specification and
	interpretation
In-Reply-To: <83AF5394-10B8-480F-9FB2-F31B701C9432@farmbio.uu.se>
References: <E043950C-0886-4708-BFFC-E2AD74C32D80@farmbio.uu.se>
	<40e66e0b0803041242q3ded4b8k73cf6e0b556aaff6@mail.gmail.com>
	<DE12A0D4-5791-43A3-9555-4F4A60521759@farmbio.uu.se>
	<40e66e0b0803140926g3d4fc772r9d62115d1d30bba5@mail.gmail.com>
	<36E8152C-0165-4657-90A8-96BDE9623441@farmbio.uu.se>
	<40e66e0b0803151038l2c0b13adqd3bfa12e2586a971@mail.gmail.com>
	<83AF5394-10B8-480F-9FB2-F31B701C9432@farmbio.uu.se>
Message-ID: <40e66e0b0803160609t79230ce4s3af9c664a4286012@mail.gmail.com>

On Sun, Mar 16, 2008 at 7:40 AM, Martin Eklund
<martin.eklund at farmbio.uu.se> wrote:

>  [snip]

>  > Take a look at the deviances of the three models.  To do a safer
>  > comparison it would be better to refit all three models with method =
>  > "ML" before comparing values like AIC and BIC but you can notice that
>  > the deviance (both ML and REML) and the AIC and BIC of models (ii) and
>  > (iii) are essentially identical, which is as it should be because the
>  > only differences are in the parameterization of the random effects.
>  > However, model (i), which has an additional parameter compared to
>  > models (ii) and (iii), gives higher AIC, BIC and deviances.  This is a
>  > sign that something is going wrong in that model.
>
>  Yes, I do understand that something goes wrong in the model lmer
>  (score ~ Machine + (1 | Worker) + (0 + Machine | Worker)). Indeed the
>  fit is better for the models lmer(score ~ Machine + (0 + Machine |
>  Worker)) and lmer(score ~ Machine + (Machine | Worker)), which
>  naturally are just reparametrizations of one another and therefore
>  produce similar deviances, AICs, and BICs. However, if I do want an
>  estimate of the Machine, the Worker and all interaction effects, how
>  would I in a correct way specify such a model? I'm not really after
>  the best model here, I'm just trying to understand and learn how to
>  do things in lme4.
>
>  Do I understand the model lmer(score ~ Machine + (0 + Machine |
>  Worker)) correctly when I say that we don't get an estimate of the
>  random effect of Worker, only estimates of the random interaction
>  effect between Worker and Machine? If so, how can I get an estimate
>  of the random Worker effect at the same time as estimating all the
>  random interaction effects? If not, how do I from the output from lmer
>  (score ~ Machine + (0 + Machine | Worker)) (see below) determine the
>  random Worker effect?

I'm not sure if we have discussed it in this thread but there is
another model specification that may make more sense to you

lmer(score ~ Machine + (1|Worker) + (1|Worker:Machine), Machines)

or, equivalently,

lmer(score ~ Machine + (1|Worker/Machine), Machines)

This model allows for an additive random effect per worker and for an
additive random effect for the Worker:Machine interaction.  It differs
from the models described earlier in the form of the
variance-covariance matrix for the interaction terms.  In this model
the interaction terms have constant variance and are independent.  In
the previous models the effects for each Worker-Machine combination
are allowed to be correlated within worker.

>  ====================================
>
> Linear mixed-effects model fit by REML
>  Formula: score ~ Machine + (0 + Machine | Worker)
>     Data: Machines
>     AIC   BIC logLik MLdeviance REMLdeviance
>   226.3 244.2 -104.2      216.6        208.3
>  Random effects:
>   Groups   Name     Variance Std.Dev. Corr
>   Worker   MachineA 16.64234 4.07950
>            MachineB 74.37306 8.62398  0.803
>            MachineC 19.26441 4.38912  0.623 0.771
>   Residual           0.92464 0.96158
>  number of obs: 54, groups: Worker, 6
>
>  Fixed effects:
>              Estimate Std. Error t value
>  (Intercept)   52.356      1.681  31.149
>  MachineB       7.967      2.420   3.292
>  MachineC      13.917      1.540   9.037
>
>  Correlation of Fixed Effects:
>           (Intr) MachnB
>  MachineB  0.463
>  MachineC -0.374  0.301
>  ====================================
>
>
>  Thank you!
>
>  Best regards,
>
>  Martin.
>
>  P.S. I'm sorry to keep bugging you guys, but I need to understand
>  this in order to make some sense of some experimental data. I really
>  like the idea of R in general and lme4 in particular and in order to
>  avoid using e.g. SPSS or some other commercial package I fully have
>  to understand the model specifications and interpretations. For me,
>  who are used to regression and the lm command family, this is a bit
>  unfamiliar and tricky.
>
>
>
>
>  > As I said in my private reply, fitting that model should fail but
>  > apparently it doesn't.  The software converges to a spurious result
>  > and it takes a bit of digging to discover that the result is indeed
>  > spurious.  It would be better if the software weree to detect such
>  > problems and indeed it is possible but difficult to do this. It is
>  > also the case that all of the programming effort to do so and time
>  > spent checking models for problems like this diverts from other
>  > enhancements that could be made to the software.
>  >
>  > As an editorial comment, those who would like to have the ability to
>  > impose correlation structures or variance functions, in addition to
>  > the correlations generated by the random effects, on the conditional
>  > distribution of the response given the random effects (i.e. make lmer
>  > more like lme in the range of models that can be fit) may want to bear
>  > this case of getting a spurious fit to a model that is not well
>  > defined in mind.  Surprisingly the majority of the effort in allowing
>  > specifications like that is not in designing and implementing the data
>  > structures and numerical algorithms.  That part is comparatively easy.
>  >  The difficult part is deciding how the user should specify the model
>  > and how the specification is translated into the structures and
>  > algorithms.  You may think, "But that's easy - just use the lme
>  > specification" but that won't work.  The nlme package was created for
>  > S and S-PLUS then ported to R and it does not follow the set of steps
>  > for model-fitting functions that is native to R.  Most of the time
>  > that is not a problem but every once in a while it breaks and it
>  > breaks in ugly ways.  Think of the recent discussion on this list
>  > about getting fitted values and residuals that appropriately take
>  > account of the pattern of missingness in the original data.  How does
>  > the information on the pattern of missingness get into the model frame
>  > in the first place?  How are the "subset", "na.action", "offset", etc.
>  > arguments handled by model-fitting functions in R?
>  >
>  > These are subtle and difficult issues.  Furthermore, the definition of
>  > the models themselves must be done with care.  It is not possible to
>  > independently specify correlations, variance structures, conditional
>  > distributions (in the sense of models for binomial or Poisson
>  > responses) and arbitrary random effects structures.  Certain
>  > combinations don't make sense.  It is actually stronger than that,
>  > certain combinations are impossible.  Witness the recent suggestion
>  > that the lme4 package is somehow inferior because it doesn't allow
>  > specification of "the" repeated measures model for a binary response.
>  > Apparently "the" repeated measures model specifies a certain
>  > conditional variance-covariance structure in the responses and the
>  > fact that this structure is incompatible with the variances implied by
>  > the binomial response should not prevent specification and fitting of
>  > such a model.  Mathematical impossibility is apparently a rather
>  > flimsy excuse for not providing software that fits such a model.
>  >
>  > Later I'll write more on the issue of variance structures and
>  > correlation structures.  I plan to provide classes for extensions to
>  > the mixed models (in fact if you look at the AllClasses.R file on the
>  > archive you will see they are already there) and show how they could
>  > be used to fit certain simple extensions, like a model with stratified
>  > conditional variances.  Then a person who wants to create software to
>  > do a "one off" fit of a mixed model  including various "mix-in"
>  > specifications can do so.  However, I don't plan on offering general
>  > model specifications in lmer until I, or someone else, can work out
>  > end-to-end how to do it without painting yourself into a corner where
>  > impossible model specifications can be fit.  And also I want it to be
>  > done in an Rish way so that all the other model arguments work as they
>  > should.  (The main stumbling block there is that a call to model.frame
>  > allows for only 1 formula and you must be very careful how you create
>  > that formula.  If you create an omnibus formula from a collection of
>  > other formulas you have to decide what the environment of the omnibus
>  > formula should be.)
>
>
>
> ========================================
>  Martin Eklund
>  PhD Student
>  Department of Pharmaceutical Biosciences
>  Uppsala University, Sweden
>  Ph: +46-18-4714281
>  ========================================
>
>
>
>
>
>         [[alternative HTML version deleted]]
>
>  _______________________________________________
>  R-sig-mixed-models at r-project.org mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From marcioestat at pop.com.br  Thu Mar 20 18:39:26 2008
From: marcioestat at pop.com.br (marcioestat at pop.com.br)
Date: Thu, 20 Mar 2008 14:39:26 -0300 (BRT)
Subject: [R-sig-ME] Split-plot Design
Message-ID: <51881.132.204.255.85.1206034766.squirrel@nwebmail.pop.com.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080320/fa5530a1/attachment.pl>

From mwkimpel at gmail.com  Thu Mar 20 18:57:12 2008
From: mwkimpel at gmail.com (Mark W Kimpel)
Date: Thu, 20 Mar 2008 13:57:12 -0400
Subject: [R-sig-ME] lmer in R-devel suddenly failing using code that worked
 just a few days ago
Message-ID: <47E2A578.4000000@gmail.com>

lmer is producing an error using code that was working for me just a few 
days ago. Output and sessionInfo() follows. Note that the warning 
message has always been present. Thanks, Mark

 >           Treatment <- factor(pData(AOP$eSet)$Treatment)
 >           Tissue <- factor(pData(AOP$eSet)$Tissue)
 >           Rat <- factor(pData(AOP$eSet)$Rat)
 >           Tissue <- factor(pData(AOP$eSet)$Tissue)
 >           mod1 <- lmer(gene.mat[1,] ~  Treatment + Tissue + (1|Rat))
Warning message:
In .local(x, ..., value) :
   Estimated variance for factor ?Rat? is effectively zero

 > mod1
Error in printMer(object) :
   no slot of name "status" for this object of class "table"

Enter a frame number, or 0 to exit

1: function (object)
2: function (object)
3: printMer(object)

Selection: c
Enter an item from the menu, or 0 to exit
Selection: 0
 > sessionInfo()
R version 2.7.0 Under development (unstable) (2008-03-05 r44683)
x86_64-unknown-linux-gnu

locale:
LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C

attached base packages:
[1] splines   tools     stats     graphics  grDevices datasets  utils
[8] methods   base

other attached packages:
  [1] lme4_0.99875-9       Matrix_0.999375-5    lattice_0.17-6
  [4] affycoretools_1.11.4 annaffy_1.11.5       KEGG.db_2.1.3
  [7] gcrma_2.11.4         matchprobes_1.11.1   biomaRt_1.13.9
[10] RCurl_0.8-3          GOstats_2.5.2        Category_2.5.7
[13] genefilter_1.17.12   survival_2.34        RBGL_1.15.7
[16] annotate_1.17.11     xtable_1.5-2         GO.db_2.1.3
[19] AnnotationDbi_1.1.26 RSQLite_0.6-8        DBI_0.2-4
[22] graph_1.17.17        limma_2.13.6         affy_1.17.9
[25] preprocessCore_1.1.5 affyio_1.7.17        Biobase_1.99.3

loaded via a namespace (and not attached):
[1] cluster_1.11.10 grid_2.7.0      XML_1.93-2
-- 

Mark W. Kimpel MD  ** Neuroinformatics ** Dept. of Psychiatry
Indiana University School of Medicine

15032 Hunter Court, Westfield, IN  46074

(317) 490-5129 Work, & Mobile & VoiceMail
(317) 204-4202 Home (no voice mail please)

mwkimpel<at>gmail<dot>com



From bates at stat.wisc.edu  Thu Mar 20 21:33:04 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 20 Mar 2008 15:33:04 -0500
Subject: [R-sig-ME] lmer in R-devel suddenly failing using code that
	worked just a few days ago
In-Reply-To: <47E2A578.4000000@gmail.com>
References: <47E2A578.4000000@gmail.com>
Message-ID: <40e66e0b0803201333g43d93e9cw29afbd6abb7707c6@mail.gmail.com>

On Thu, Mar 20, 2008 at 12:57 PM, Mark W Kimpel <mwkimpel at gmail.com> wrote:
> lmer is producing an error using code that was working for me just a few
>  days ago. Output and sessionInfo() follows. Note that the warning
>  message has always been present. Thanks, Mark
>
>   >           Treatment <- factor(pData(AOP$eSet)$Treatment)
>   >           Tissue <- factor(pData(AOP$eSet)$Tissue)
>   >           Rat <- factor(pData(AOP$eSet)$Rat)
>   >           Tissue <- factor(pData(AOP$eSet)$Tissue)
>   >           mod1 <- lmer(gene.mat[1,] ~  Treatment + Tissue + (1|Rat))
>  Warning message:
>  In .local(x, ..., value) :
>    Estimated variance for factor 'Rat' is effectively zero
>
>   > mod1
>  Error in printMer(object) :
>    no slot of name "status" for this object of class "table"

I don't think we can debug this without a reproducible example, Mark.
The message indicates that inside printMer an object that should be of
class summary.mer is of class "table".  My guess is that this occurs
right at the beginning of that function

printMer <- function(x, digits = max(3, getOption("digits") - 3),
                     correlation = TRUE, symbolic.cor = FALSE,
                     signif.stars = getOption("show.signif.stars"), ...)
{
    so <- summary(x)
    REML <- so at status["REML"]

during that first access to the status slot, but without a
reproducible example we can't decide what has gone wrong getting to
that point.


>  Enter a frame number, or 0 to exit
>
>  1: function (object)
>  2: function (object)
>  3: printMer(object)
>
>  Selection: c
>  Enter an item from the menu, or 0 to exit
>  Selection: 0
>   > sessionInfo()
>  R version 2.7.0 Under development (unstable) (2008-03-05 r44683)
>  x86_64-unknown-linux-gnu
>
>  locale:
>  LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C
>
>  attached base packages:
>  [1] splines   tools     stats     graphics  grDevices datasets  utils
>  [8] methods   base
>
>  other attached packages:
>   [1] lme4_0.99875-9       Matrix_0.999375-5    lattice_0.17-6
>   [4] affycoretools_1.11.4 annaffy_1.11.5       KEGG.db_2.1.3
>   [7] gcrma_2.11.4         matchprobes_1.11.1   biomaRt_1.13.9
>  [10] RCurl_0.8-3          GOstats_2.5.2        Category_2.5.7
>  [13] genefilter_1.17.12   survival_2.34        RBGL_1.15.7
>  [16] annotate_1.17.11     xtable_1.5-2         GO.db_2.1.3
>  [19] AnnotationDbi_1.1.26 RSQLite_0.6-8        DBI_0.2-4
>  [22] graph_1.17.17        limma_2.13.6         affy_1.17.9
>  [25] preprocessCore_1.1.5 affyio_1.7.17        Biobase_1.99.3
>
>  loaded via a namespace (and not attached):
>  [1] cluster_1.11.10 grid_2.7.0      XML_1.93-2
>  --
>
>  Mark W. Kimpel MD  ** Neuroinformatics ** Dept. of Psychiatry
>  Indiana University School of Medicine
>
>  15032 Hunter Court, Westfield, IN  46074
>
>  (317) 490-5129 Work, & Mobile & VoiceMail
>  (317) 204-4202 Home (no voice mail please)
>
>  mwkimpel<at>gmail<dot>com
>
>  _______________________________________________
>  R-sig-mixed-models at r-project.org mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From kw.statr at gmail.com  Thu Mar 20 23:23:19 2008
From: kw.statr at gmail.com (Kevin Wright)
Date: Thu, 20 Mar 2008 17:23:19 -0500
Subject: [R-sig-ME] Split-plot Design
In-Reply-To: <51881.132.204.255.85.1206034766.squirrel@nwebmail.pop.com.br>
References: <51881.132.204.255.85.1206034766.squirrel@nwebmail.pop.com.br>
Message-ID: <c968588d0803201523x3912d216i8a4220886336780b@mail.gmail.com>

Your question is not very clear, but if you are trying to match the
results in Kuehl, you need a fixed-effects model:

dat <- read.table("expl14-1.txt", header=TRUE)
dat$block <- factor(dat$block)
dat$nitro <- factor(dat$nitro)
dat$thatch <- factor(dat$thatch)

colnames(dat) <- c("block","nitro","thatch","chlor")
m1 <- aov(chlor~nitro*thatch+Error(block/nitro), data=dat)
summary(m1)

Mixed-effects models and degrees of freedom have been discussed many
times on this list....search the archives.

K Wright


On Thu, Mar 20, 2008 at 12:39 PM,  <marcioestat at pop.com.br> wrote:
>
>
>
>  Hi listers,
>
>  I've been studying anova and at the book of Kuehl at the chapter
>  about split-plot there is a experiment with the results... I am trying to
>  understand the experiments and make the code in order to obtain the
>  results... But there is something that I didn't understand yet...
>  I have a split-plot design (2 blocks) with two facteurs, one
>  facteur has 4 treatments and the other facteur is a measure
>  taken in three years...
>  I organize my data set as:
>
>  Nitro Bloc Year Measure
>  a
>  x
>  1         3.8
>  a
>   x
>  2         3.9
>  a         x         3         2.0
>  a         y         1         3.7
>  a         y         2
>  2.4
>  a         y         3
>  1.2
>  b         x
>   1         4.0
>  b         x
>  2         2.5
>  and so on...
>
>
>  So, I am trying this code, because I want to test each factor and the
>  interaction...
>  lme=lme(measure ~ bloc + nitro + bloc*nitro, random= ~ 1|year,
>  data=lme)
>  summary(lme)
>  The results that I am obtaining are not correct, because
>  I calculated the degrees of fredom and they are not
>  correct... According to this design I will get two errors one for the
>  whole plot and other for the subplot....
>
>  Well, as I told you, I am still learning... Any suggestions...
>
>  Thanks in advance,
>
>  Ribeiro
>
>
>         [[alternative HTML version deleted]]
>
>
> _______________________________________________
>  R-sig-mixed-models at r-project.org mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



From john.maindonald at anu.edu.au  Fri Mar 21 01:04:10 2008
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Fri, 21 Mar 2008 11:04:10 +1100
Subject: [R-sig-ME] Split-plot Design
In-Reply-To: <c968588d0803201523x3912d216i8a4220886336780b@mail.gmail.com>
References: <51881.132.204.255.85.1206034766.squirrel@nwebmail.pop.com.br>
	<c968588d0803201523x3912d216i8a4220886336780b@mail.gmail.com>
Message-ID: <64B59060-3A82-41E8-90FE-E4AFEF36E6A7@anu.edu.au>

I do not think it quite true that the aov model that has an
Error() term is a fixed effects model.  The use of the word
"stratum" implies that a mixed effects model is lurking
somewhere.  The F-tests surely assume such a model.

Some little time ago, Doug Bates invested me, along with
Peter Dalgaard, a member of the degrees of freedom police.
Problem is, I am unsure of the responsibilities, but maybe
they include commenting on a case such as this.

lme() makes a stab at an appropriate choice of degrees
of freedom, but does not always get it right, to the extent
that there is a right answer.  [lmer() has for the time being
given up on giving degrees of freedom and p-values for
fixed effects estimates.]  This part of the output from lme()
should, accordingly, be used with discretion.  In case of
doubt, check against a likelihood ratio test.  In a simple
enough experimental design, users who understand how
to calculate degrees of freedom will reason them out for
themselves.
John Maindonald.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.


On 21 Mar 2008, at 9:23 AM, Kevin Wright wrote:

> Your question is not very clear, but if you are trying to match the
> results in Kuehl, you need a fixed-effects model:
>
> dat <- read.table("expl14-1.txt", header=TRUE)
> dat$block <- factor(dat$block)
> dat$nitro <- factor(dat$nitro)
> dat$thatch <- factor(dat$thatch)
>
> colnames(dat) <- c("block","nitro","thatch","chlor")
> m1 <- aov(chlor~nitro*thatch+Error(block/nitro), data=dat)
> summary(m1)
>
> Mixed-effects models and degrees of freedom have been discussed many
> times on this list....search the archives.
>
> K Wright
>
>
> On Thu, Mar 20, 2008 at 12:39 PM,  <marcioestat at pop.com.br> wrote:
>>
>>
>>
>> Hi listers,
>>
>> I've been studying anova and at the book of Kuehl at the chapter
>> about split-plot there is a experiment with the results... I am  
>> trying to
>> understand the experiments and make the code in order to obtain the
>> results... But there is something that I didn't understand yet...
>> I have a split-plot design (2 blocks) with two facteurs, one
>> facteur has 4 treatments and the other facteur is a measure
>> taken in three years...
>> I organize my data set as:
>>
>> Nitro Bloc Year Measure
>> a
>> x
>> 1         3.8
>> a
>>  x
>> 2         3.9
>> a         x         3         2.0
>> a         y         1         3.7
>> a         y         2
>> 2.4
>> a         y         3
>> 1.2
>> b         x
>>  1         4.0
>> b         x
>> 2         2.5
>> and so on...
>>
>>
>> So, I am trying this code, because I want to test each factor and the
>> interaction...
>> lme=lme(measure ~ bloc + nitro + bloc*nitro, random= ~ 1|year,
>> data=lme)
>> summary(lme)
>> The results that I am obtaining are not correct, because
>> I calculated the degrees of fredom and they are not
>> correct... According to this design I will get two errors one for the
>> whole plot and other for the subplot....
>>
>> Well, as I told you, I am still learning... Any suggestions...
>>
>> Thanks in advance,
>>
>> Ribeiro
>>
>>
>>        [[alternative HTML version deleted]]
>>
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From mwkimpel at gmail.com  Fri Mar 21 04:08:44 2008
From: mwkimpel at gmail.com (Mark W Kimpel)
Date: Thu, 20 Mar 2008 23:08:44 -0400
Subject: [R-sig-ME] lmer in R-devel suddenly failing using code that
 worked just a few days ago
In-Reply-To: <40e66e0b0803201333g43d93e9cw29afbd6abb7707c6@mail.gmail.com>
References: <47E2A578.4000000@gmail.com>
	<40e66e0b0803201333g43d93e9cw29afbd6abb7707c6@mail.gmail.com>
Message-ID: <47E326BC.90907@gmail.com>

Doug,

When I went to make a self-contained example, it worked. When I went 
back to my original code, it too worked. Only difference was restarting 
R. Wonder if there was a conflict with another package???

Solved for now at least, sorry for bothering the list,

Mark

Mark W. Kimpel MD  ** Neuroinformatics ** Dept. of Psychiatry
Indiana University School of Medicine

15032 Hunter Court, Westfield, IN  46074

(317) 490-5129 Work, & Mobile & VoiceMail
(317) 204-4202 Home (no voice mail please)

mwkimpel<at>gmail<dot>com

******************************************************************


Douglas Bates wrote:
> On Thu, Mar 20, 2008 at 12:57 PM, Mark W Kimpel <mwkimpel at gmail.com> wrote:
>> lmer is producing an error using code that was working for me just a few
>>  days ago. Output and sessionInfo() follows. Note that the warning
>>  message has always been present. Thanks, Mark
>>
>>   >           Treatment <- factor(pData(AOP$eSet)$Treatment)
>>   >           Tissue <- factor(pData(AOP$eSet)$Tissue)
>>   >           Rat <- factor(pData(AOP$eSet)$Rat)
>>   >           Tissue <- factor(pData(AOP$eSet)$Tissue)
>>   >           mod1 <- lmer(gene.mat[1,] ~  Treatment + Tissue + (1|Rat))
>>  Warning message:
>>  In .local(x, ..., value) :
>>    Estimated variance for factor 'Rat' is effectively zero
>>
>>   > mod1
>>  Error in printMer(object) :
>>    no slot of name "status" for this object of class "table"
> 
> I don't think we can debug this without a reproducible example, Mark.
> The message indicates that inside printMer an object that should be of
> class summary.mer is of class "table".  My guess is that this occurs
> right at the beginning of that function
> 
> printMer <- function(x, digits = max(3, getOption("digits") - 3),
>                      correlation = TRUE, symbolic.cor = FALSE,
>                      signif.stars = getOption("show.signif.stars"), ...)
> {
>     so <- summary(x)
>     REML <- so at status["REML"]
> 
> during that first access to the status slot, but without a
> reproducible example we can't decide what has gone wrong getting to
> that point.
> 
> 
>>  Enter a frame number, or 0 to exit
>>
>>  1: function (object)
>>  2: function (object)
>>  3: printMer(object)
>>
>>  Selection: c
>>  Enter an item from the menu, or 0 to exit
>>  Selection: 0
>>   > sessionInfo()
>>  R version 2.7.0 Under development (unstable) (2008-03-05 r44683)
>>  x86_64-unknown-linux-gnu
>>
>>  locale:
>>  LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C
>>
>>  attached base packages:
>>  [1] splines   tools     stats     graphics  grDevices datasets  utils
>>  [8] methods   base
>>
>>  other attached packages:
>>   [1] lme4_0.99875-9       Matrix_0.999375-5    lattice_0.17-6
>>   [4] affycoretools_1.11.4 annaffy_1.11.5       KEGG.db_2.1.3
>>   [7] gcrma_2.11.4         matchprobes_1.11.1   biomaRt_1.13.9
>>  [10] RCurl_0.8-3          GOstats_2.5.2        Category_2.5.7
>>  [13] genefilter_1.17.12   survival_2.34        RBGL_1.15.7
>>  [16] annotate_1.17.11     xtable_1.5-2         GO.db_2.1.3
>>  [19] AnnotationDbi_1.1.26 RSQLite_0.6-8        DBI_0.2-4
>>  [22] graph_1.17.17        limma_2.13.6         affy_1.17.9
>>  [25] preprocessCore_1.1.5 affyio_1.7.17        Biobase_1.99.3
>>
>>  loaded via a namespace (and not attached):
>>  [1] cluster_1.11.10 grid_2.7.0      XML_1.93-2
>>  --
>>
>>  Mark W. Kimpel MD  ** Neuroinformatics ** Dept. of Psychiatry
>>  Indiana University School of Medicine
>>
>>  15032 Hunter Court, Westfield, IN  46074
>>
>>  (317) 490-5129 Work, & Mobile & VoiceMail
>>  (317) 204-4202 Home (no voice mail please)
>>
>>  mwkimpel<at>gmail<dot>com
>>
>>  _______________________________________________
>>  R-sig-mixed-models at r-project.org mailing list
>>  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>



From a.fugard at ed.ac.uk  Fri Mar 21 14:51:44 2008
From: a.fugard at ed.ac.uk (Andy Fugard)
Date: Fri, 21 Mar 2008 13:51:44 +0000
Subject: [R-sig-ME] Split-plot Design
In-Reply-To: <64B59060-3A82-41E8-90FE-E4AFEF36E6A7@anu.edu.au>
References: <51881.132.204.255.85.1206034766.squirrel@nwebmail.pop.com.br>	<c968588d0803201523x3912d216i8a4220886336780b@mail.gmail.com>
	<64B59060-3A82-41E8-90FE-E4AFEF36E6A7@anu.edu.au>
Message-ID: <47E3BD70.3070906@ed.ac.uk>

John Maindonald wrote:
> I do not think it quite true that the aov model that has an
> Error() term is a fixed effects model.  The use of the word
> "stratum" implies that a mixed effects model is lurking
> somewhere.  The F-tests surely assume such a model.

This is something that has been bothering me for a while.  Often it's 
argued that ANOVA is just regression; clearly this is not true when it's 
a repeated measures ANOVA, unless "regression" is interpreted broadly. 
I think Andrew Gelman argues this somewhere.  I don't see how to get aov 
to give me a formula, and lm doesn't fit stuff with an Error() term, but 
if it could, logically I would expect the formula to resemble closely 
the sort of thing you get with a mixed effects models.

The closest analogy I can find is that doing this...


 > aov1 = aov(yield ~  N+P+K + Error(block), npk)
 > summary(aov1)

Error: block
           Df Sum Sq Mean Sq F value Pr(>F)
Residuals  5    343      69

Error: Within
           Df Sum Sq Mean Sq F value Pr(>F)
N          1  189.3   189.3   11.82 0.0037 **
P          1    8.4     8.4    0.52 0.4800
K          1   95.2    95.2    5.95 0.0277 *
Residuals 15  240.2    16.0
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1


[I believe the F's and p's here come from a linear construction of 
nested models (i.e., N versus intercept only; N+P versus N; plus N+P+K 
versus N+P).]

... is a bit like doing this with lmer...


 > lmer0 = lmer(yield ~  1 + (1|block), npk)
 > lmer1 = lmer(yield ~  N + (1|block), npk)
 > lmer2 = lmer(yield ~  N+P + (1|block), npk)
 > lmer3 = lmer(yield ~  N+P+K + (1|block), npk)
 > anova(lmer0,lmer1)
Data: npk
Models:
lmer0: yield ~ 1 + (1 | block)
lmer1: yield ~ N + (1 | block)
       Df   AIC   BIC logLik Chisq Chi Df Pr(>Chisq)
lmer0  2 157.5 159.8  -76.7
lmer1  3 151.5 155.1  -72.8  7.93      1     0.0049 **
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
 > anova(lmer1,lmer2)
Data: npk
Models:
lmer1: yield ~ N + (1 | block)
lmer2: yield ~ N + P + (1 | block)
       Df   AIC   BIC logLik Chisq Chi Df Pr(>Chisq)
lmer1  3 151.5 155.1  -72.8
lmer2  4 153.0 157.8  -72.5  0.47      1       0.49
 > anova(lmer2,lmer3)
Data: npk
Models:
lmer2: yield ~ N + P + (1 | block)
lmer3: yield ~ N + P + K + (1 | block)
       Df   AIC   BIC logLik Chisq Chi Df Pr(>Chisq)
lmer2  4 153.0 157.8  -72.5
lmer3  5 149.0 154.9  -69.5  6.02      1      0.014 *
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1


Namely, fitting the models, and making comparisons using likelihood 
ratio tests.  Okay, the LLR is asymptotically chisq distributed with df 
the difference in parameters whereas... well F is different - I always 
get confused with the df and what bits of the residuals plug in where.

But, is this vaguely right?  (I'm aware that order matters when the 
design isn't balanced, and have read much about the type I errors versus 
type III errors disagreements.)

Actually this does lead to one question: following through the analogy, 
are repeated measures ANOVAs (those fitted with aov) always random 
intercept models, i.e. with no random slopes?


Cheers,

Andy


-- 
Andy Fugard, Postgraduate Research Student
Psychology (Room F3), The University of Edinburgh,
   7 George Square, Edinburgh EH8 9JZ, UK
Mobile: +44 (0)78 123 87190   http://www.possibly.me.uk



From kushler at oakland.edu  Fri Mar 21 15:22:33 2008
From: kushler at oakland.edu (Robert Kushler)
Date: Fri, 21 Mar 2008 10:22:33 -0400
Subject: [R-sig-ME] Split-plot Design
In-Reply-To: <47E3BD70.3070906@ed.ac.uk>
References: <51881.132.204.255.85.1206034766.squirrel@nwebmail.pop.com.br>	<c968588d0803201523x3912d216i8a4220886336780b@mail.gmail.com>	<64B59060-3A82-41E8-90FE-E4AFEF36E6A7@anu.edu.au>
	<47E3BD70.3070906@ed.ac.uk>
Message-ID: <47E3C4A9.3090300@oakland.edu>


The following (which uses the variable names in the original query)
shows that "lme" and "aov" produce the same results:

(mod <- lme(measure ~ nitro*year,random= ~ 1|bloc/nitro))
anova(mod)
summary(aov(measure ~ nitro*year + Error(bloc/nitro)))

(In the text, Kuehl rounded off the mean squares before computing
the F statistics, so they don't match.)

This is the classical "split plot" analysis, and for balanced data
there is no controversy over the F statistics.  Of course as always
we must assume (I prefer "pretend") that the random effects, including
the "errors", have normal distributions with constant variances.

The trouble arises with unbalanced data and/or more complex models.
Hopefully, reliable and convenient inference methods for those cases
will be developed, but until then those of us who continue to use
"nlme" (or SAS, or ...) must understand that we do so at our own risk.

Regards,   Rob Kushler


Andy Fugard wrote:
> John Maindonald wrote:
>> I do not think it quite true that the aov model that has an
>> Error() term is a fixed effects model.  The use of the word
>> "stratum" implies that a mixed effects model is lurking
>> somewhere.  The F-tests surely assume such a model.
> 
> This is something that has been bothering me for a while.  Often it's 
> argued that ANOVA is just regression; clearly this is not true when it's 
> a repeated measures ANOVA, unless "regression" is interpreted broadly. 
> I think Andrew Gelman argues this somewhere.  I don't see how to get aov 
> to give me a formula, and lm doesn't fit stuff with an Error() term, but 
> if it could, logically I would expect the formula to resemble closely 
> the sort of thing you get with a mixed effects models.
> 
> The closest analogy I can find is that doing this...
> 
> 
>  > aov1 = aov(yield ~  N+P+K + Error(block), npk)
>  > summary(aov1)
> 
> Error: block
>            Df Sum Sq Mean Sq F value Pr(>F)
> Residuals  5    343      69
> 
> Error: Within
>            Df Sum Sq Mean Sq F value Pr(>F)
> N          1  189.3   189.3   11.82 0.0037 **
> P          1    8.4     8.4    0.52 0.4800
> K          1   95.2    95.2    5.95 0.0277 *
> Residuals 15  240.2    16.0
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> 
> [I believe the F's and p's here come from a linear construction of 
> nested models (i.e., N versus intercept only; N+P versus N; plus N+P+K 
> versus N+P).]
> 
> ... is a bit like doing this with lmer...
> 
> 
>  > lmer0 = lmer(yield ~  1 + (1|block), npk)
>  > lmer1 = lmer(yield ~  N + (1|block), npk)
>  > lmer2 = lmer(yield ~  N+P + (1|block), npk)
>  > lmer3 = lmer(yield ~  N+P+K + (1|block), npk)
>  > anova(lmer0,lmer1)
> Data: npk
> Models:
> lmer0: yield ~ 1 + (1 | block)
> lmer1: yield ~ N + (1 | block)
>        Df   AIC   BIC logLik Chisq Chi Df Pr(>Chisq)
> lmer0  2 157.5 159.8  -76.7
> lmer1  3 151.5 155.1  -72.8  7.93      1     0.0049 **
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>  > anova(lmer1,lmer2)
> Data: npk
> Models:
> lmer1: yield ~ N + (1 | block)
> lmer2: yield ~ N + P + (1 | block)
>        Df   AIC   BIC logLik Chisq Chi Df Pr(>Chisq)
> lmer1  3 151.5 155.1  -72.8
> lmer2  4 153.0 157.8  -72.5  0.47      1       0.49
>  > anova(lmer2,lmer3)
> Data: npk
> Models:
> lmer2: yield ~ N + P + (1 | block)
> lmer3: yield ~ N + P + K + (1 | block)
>        Df   AIC   BIC logLik Chisq Chi Df Pr(>Chisq)
> lmer2  4 153.0 157.8  -72.5
> lmer3  5 149.0 154.9  -69.5  6.02      1      0.014 *
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> 
> Namely, fitting the models, and making comparisons using likelihood 
> ratio tests.  Okay, the LLR is asymptotically chisq distributed with df 
> the difference in parameters whereas... well F is different - I always 
> get confused with the df and what bits of the residuals plug in where.
> 
> But, is this vaguely right?  (I'm aware that order matters when the 
> design isn't balanced, and have read much about the type I errors versus 
> type III errors disagreements.)
> 
> Actually this does lead to one question: following through the analogy, 
> are repeated measures ANOVAs (those fitted with aov) always random 
> intercept models, i.e. with no random slopes?
> 
> 
> Cheers,
> 
> Andy
> 
>



From bates at stat.wisc.edu  Fri Mar 21 15:31:35 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 21 Mar 2008 09:31:35 -0500
Subject: [R-sig-ME] Split-plot Design
In-Reply-To: <64B59060-3A82-41E8-90FE-E4AFEF36E6A7@anu.edu.au>
References: <51881.132.204.255.85.1206034766.squirrel@nwebmail.pop.com.br>
	<c968588d0803201523x3912d216i8a4220886336780b@mail.gmail.com>
	<64B59060-3A82-41E8-90FE-E4AFEF36E6A7@anu.edu.au>
Message-ID: <40e66e0b0803210731h2fa09bbaqcb9e4fbb3794fc3a@mail.gmail.com>

On Thu, Mar 20, 2008 at 7:04 PM, John Maindonald
<john.maindonald at anu.edu.au> wrote:
> I do not think it quite true that the aov model that has an
>  Error() term is a fixed effects model.  The use of the word
>  "stratum" implies that a mixed effects model is lurking
>  somewhere.  The F-tests surely assume such a model.

>  Some little time ago, Doug Bates invested me, along with
>  Peter Dalgaard, a member of the degrees of freedom police.
>  Problem is, I am unsure of the responsibilities, but maybe
>  they include commenting on a case such as this.

Indeed, I should have been more explicit regarding the duties of a
member of the degrees of freedom police when I appointed you :-)  They
are exactly what you are doing - to weigh in on discussions of degrees
of freedom.

Perhaps I can expand a bit on why I think that the degrees of freedom
issue is not a good basis for approaching mixed models in general.

Duncan Temple Lang used to have a quote about "Language shapes the way
we think." in his email signature and I think similar ideas apply to
how we think about models.  Certainly the concept of error strata and
the associated degrees of freedom are one way of thinking about
mixed-effects models and they are effective in the case of completely
balanced designs.  If one has a completely balanced design and the
number of units at various strata is small then decomposition of the
variability into error strata is possible and the calculation of
degrees of freedom is important (because the degrees of freedom are
small - exact values of degrees of freedom are unimportant when they
are large).  Such data occur frequently - in text books.  In fact,
they are the only type of data that occur in most text books and hence
they have shaped the way we think about the models.  Generations of
statisticians have looked at techniques for small, balanced data sets
and decided that these define "the" approach to the model.  So when
confronted with large observational (and, hence, unbalanced or
"messy") data sets they approach the modeling with this mind set.

Certainly such data did occur in some agricultural trials (I'm not
sure if this is the state of the art in current agricultural trials)
and this form of analysis was important but it depends strongly on
balanced data and balance is a very fragile characteristic in most
data.  The one exception is data in text books - most often data for
examples in older texts were added as an afterthought and, for space
reasons and to illustrate the calculations, were small data sets which
just happened to be completely balanced so that all the calculations
worked out.   Fortunately this is no longer the case in books like
your book with Braun where the data sets are real data and available
separately from the book in R packages but it will take many years to
flush the way of thinking about models induced by those small,
balanced data sets from statistical "knowledge".  (There is an
enormous amount of inertia built into the system.  Most introductory
statistics textbooks published today, and probably those published for
the next couple of decades, will include "statistical tables" as
appendices because, as everyone knows, all of the statistical analysis
we do in practice involves doing a few calculations on a hand
calculator and looking up tabulated values of distributions.)

Getting back to the "language shapes the way we think" issue, I would
compare it to strategy versus tactics.  I spent a sabbatical in
Australia visiting Bill Venables.  I needed to adjust to driving on
the left hand side of the road and was able to do that after a short
initial period of confusion and discomfort.  If I think of places in
Adelaide now it is natural for me to think of myself sitting on the
right hand side of the front seat and driving on the left hand side of
the road.  That's tactics.  However, even after living in Adelaide for
several months I remember leaving the parking lot of a shopping mall
and choosing an indirect route out the back instead of the direct
route ahead because the direct route would involve a left hand turn
onto a busy road.  Instead I chose to make two right hand turns to get
to an intersection with traffic lights where I could make the left
turn.  At the level of strategy I still "knew" that left hand turns
are difficult and right hand turns are easy - exactly the wrong
approach in Australia.

This semester I am attending a seminar on hierarchical linear models
organized in our social sciences school by a statistician whose
background is in agricultural statistics.  I make myself unpopular
because I keep challenging the ideas of the social scientists and of
the organizer.  The typical kinds of studies we discuss are
longitudinal studies where the "observational units" (i.e. people) are
grouped within some social contexts.  These are usually large, highly
unbalanced data  sets. Because they are longitudinal they inevitably
involve some migration of people from one group to another.

The migration means that random effects for person and for the various
types of groups are not nested.  The models are not hierarchical or at
least they should not be.  Trying to jam the analysis of such data
into the hierarchical/multilevel framework of software like HLM or
MLwin doesn't work well but that certainly won't stop people from
trying.  The hierarchical language in which they learned the model
affects their strategy.

The agricultural statistician, by contrast, doesn't worry about the
nesting etc.,  For him the sole issue of important is to determine the
number of degrees of freedom associated with the various error strata.
 These studies involve tens of thousands of subjects and are nowhere
close to being balanced but his strategy is still based on having a
certain number of blocks and plots within the blocks and subplots
within the plots and everything is exactly balanced.

So I am not opposed to approaches based on error strata and computing
degrees of freedom where appropriate and where important.  But try not
to let the particular case of completely balanced small data sets
determine the strategy of the approach to all models involving random
effects.  It's fragile and does not generalize well, just as assuming
a strict hierarchy of factors associated with random effects is
fragile.  Balance is the special case.  Nesting is the special case.
It is a bad idea to base strategy on what happens in very special
cases.


they are always balanced and small data sets

Another very fragile

>  lme() makes a stab at an appropriate choice of degrees
>  of freedom, but does not always get it right, to the extent
>  that there is a right answer.  [lmer() has for the time being
>  given up on giving degrees of freedom and p-values for
>  fixed effects estimates.]  This part of the output from lme()
>  should, accordingly, be used with discretion.  In case of
>  doubt, check against a likelihood ratio test.  In a simple
>  enough experimental design, users who understand how
>  to calculate degrees of freedom will reason them out for
>  themselves.
>  John Maindonald.
>
>  John Maindonald             email: john.maindonald at anu.edu.au
>  phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
>  Centre for Mathematics & Its Applications, Room 1194,
>  John Dedman Mathematical Sciences Building (Building 27)
>  Australian National University, Canberra ACT 0200.
>
>
>
>
>  On 21 Mar 2008, at 9:23 AM, Kevin Wright wrote:
>
>  > Your question is not very clear, but if you are trying to match the
>  > results in Kuehl, you need a fixed-effects model:
>  >
>  > dat <- read.table("expl14-1.txt", header=TRUE)
>  > dat$block <- factor(dat$block)
>  > dat$nitro <- factor(dat$nitro)
>  > dat$thatch <- factor(dat$thatch)
>  >
>  > colnames(dat) <- c("block","nitro","thatch","chlor")
>  > m1 <- aov(chlor~nitro*thatch+Error(block/nitro), data=dat)
>  > summary(m1)
>  >
>  > Mixed-effects models and degrees of freedom have been discussed many
>  > times on this list....search the archives.
>  >
>  > K Wright
>  >
>  >
>  > On Thu, Mar 20, 2008 at 12:39 PM,  <marcioestat at pop.com.br> wrote:
>  >>
>  >>
>  >>
>  >> Hi listers,
>  >>
>  >> I've been studying anova and at the book of Kuehl at the chapter
>  >> about split-plot there is a experiment with the results... I am
>  >> trying to
>  >> understand the experiments and make the code in order to obtain the
>  >> results... But there is something that I didn't understand yet...
>  >> I have a split-plot design (2 blocks) with two facteurs, one
>  >> facteur has 4 treatments and the other facteur is a measure
>  >> taken in three years...
>  >> I organize my data set as:
>  >>
>  >> Nitro Bloc Year Measure
>  >> a
>  >> x
>  >> 1         3.8
>  >> a
>  >>  x
>  >> 2         3.9
>  >> a         x         3         2.0
>  >> a         y         1         3.7
>  >> a         y         2
>  >> 2.4
>  >> a         y         3
>  >> 1.2
>  >> b         x
>  >>  1         4.0
>  >> b         x
>  >> 2         2.5
>  >> and so on...
>  >>
>  >>
>  >> So, I am trying this code, because I want to test each factor and the
>  >> interaction...
>  >> lme=lme(measure ~ bloc + nitro + bloc*nitro, random= ~ 1|year,
>  >> data=lme)
>  >> summary(lme)
>  >> The results that I am obtaining are not correct, because
>  >> I calculated the degrees of fredom and they are not
>  >> correct... According to this design I will get two errors one for the
>  >> whole plot and other for the subplot....
>  >>
>  >> Well, as I told you, I am still learning... Any suggestions...
>  >>
>  >> Thanks in advance,
>  >>
>  >> Ribeiro
>  >>
>  >>
>  >>        [[alternative HTML version deleted]]
>  >>
>  >>
>  >> _______________________________________________
>  >> R-sig-mixed-models at r-project.org mailing list
>  >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>  >>
>  >>
>  >
>  > _______________________________________________
>  > R-sig-mixed-models at r-project.org mailing list
>  > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>  _______________________________________________
>  R-sig-mixed-models at r-project.org mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From John.Maindonald at anu.edu.au  Sat Mar 22 07:40:17 2008
From: John.Maindonald at anu.edu.au (John Maindonald)
Date: Sat, 22 Mar 2008 17:40:17 +1100
Subject: [R-sig-ME] Split-plot Design
In-Reply-To: <40e66e0b0803210731h2fa09bbaqcb9e4fbb3794fc3a@mail.gmail.com>
References: <51881.132.204.255.85.1206034766.squirrel@nwebmail.pop.com.br>
	<c968588d0803201523x3912d216i8a4220886336780b@mail.gmail.com>
	<64B59060-3A82-41E8-90FE-E4AFEF36E6A7@anu.edu.au>
	<40e66e0b0803210731h2fa09bbaqcb9e4fbb3794fc3a@mail.gmail.com>
Message-ID: <9450DEBA-94B9-4BB6-A258-A03F3B60F4D9@anu.edu.au>

I like this - a good robust defence!  Let the debate go on.

Designs that are in some sense balanced are, as far as I can judge,  
still the staple of agricultural trials.  The sorts of issues that  
arise in time series modeling must also be increasingly important.   
The thinking behind those designs is important, and ought to penetrate  
more widely.

That general style of design is not however limited to agricultural  
experimentation.  It is used widely, with different wrinkles, in  
psychology in industrial experimentation, in medicine, in laboratory  
experimentation, and with internet-based experimentation a new major  
area of application.  It ought to be used much more widely outside of  
agriculture.  Agriculture has been somewhat unique in having  
professional statisticians working alongside agricultural scientists  
for many decades now.

The wrinkles can be important.  Fisher famously commented on the  
frequent need to ask Nature more than one question at a time:
"No aphorism is more frequently repeated in connection with field  
trials, than that we must ask Nature few questions, or, ideally, one  
question, at a time. The writer is convinced that this view is wholly  
mistaken. Nature, he suggests, will best respond to a logical and  
carefully thought out questionnaire"

For each fixed effect and interaction, it is however necessary to be  
clear what is the relevant error term.  Some time ago, someone posted  
a question about the analysis of a psychology experiment where the  
number of possible error terms was very large, where some pooling was  
desirable, and where it was not at all clear what terms in the anova  
table to pool, and where d.f, were so small that the mean squares did  
not provide much clue.  (I looked for the email exchange, but could  
not immediately find it.) That is not a good design.  There may be  
much more potential for this sort of difficultly in psychology as  
compared to agriculture.  In agriculture the sites/blocks/plots/ 
subplots hierarchy is the order of the day, albeit often crossed with  
seasons to ensure that the analysis is not totally obvious.

The data sets are often larger than formerly. (But not always, again  
note some psychology experiments. Or they may not be large in the  
sense of allowing many degrees of freedom for error) Large datasets  
readily arise when, as for the internet-based experimentation,  
randomization can be done and data collected automatically.  (Some  
advertisers are randomizing their pop-up ads, to see which gives the  
best response.) With largish degrees of freedom for error, it is no  
longer necessary to worry about exact balance.  I consider Doug that  
you are too tough on the textbooks.  Maybe they ought to be branching  
out from agricultural experimentation more than they do; that is as  
much as I'd want to say.  There are any number of examples from  
psychology, some of them very interesting, in the psychology books on  
experimental design.  (Data from published papers ought nowadays as a  
matter of course go into web-based archives - aside from other  
considerations this would provide useful teaching resources. In some  
areas, this is already happening.)

Degrees of freedom make sense in crossed designs also; it is the F- 
statistics for SEs for fixed effects that can be problematic.  It may  
happen that one or two sources of error (maybe treatments by years)  
will dominate to such an extent that other sources can pretty much be  
ignored.  The conceptual simplification is worth having; its utility  
may not be evident from a general multi-level modeling perspective.

Maybe one does not want statisticians to be too dyed in the wool  
agricultural.  Still, a bit of that thinking goes a long way, not  
least among social scientists.  The arguments in Rosenbaum's  
"Observational Data" are much easier to follow if one comes to them  
with some knowledge and practical experience of old-fashioned  
experimental design.  That seems to me a good indication of the quite  
fundamental role of those ideas, even if one will never do a  
randomized experiment.

I'd have every novice statistician do apprenticeship's that include  
experience in horticultural science (they do not have a long tradition  
of working alongside professional statisticians), medicine and  
epidemiology, business statistics, and somewhere between health social  
science and psychology.  It is unfortunate that I did not myself have  
this broad training, which may go some way to explaining why I am not  
in complete agreement with Doug's sentiments!!

This is not to defend, in any large variety of places and  
circumstances, inference that relies on degrees of freedom.  Not that  
my point relate directly to degrees of freedom.  On degrees of  
freedom, I judge them to have a somewhat wider usefulness than Doug  
will allow.  It would be nice if lmer() were to provide Kenward &  
Rogers style degrees of freedom ( as the commercial ASReml-R software  
does), but Doug is right to press the dangers of giving information  
that can for very unbalanced designs be misleading.  Given a choice  
between mcmcsamp() and degrees of freedom, maybe I would choose  
mcmcsamp().

There's enlightening discussion of the opportunities that internet- 
based business offers for automated data collection and for  
experimentation on website visitors, in Ian Ayres "SuperCrunchers. Why  
thinking by numbers is the new way to be smart" (Bantam).   
Fortunately, the hype of the title pretty much goes once on gets into  
the text.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.


On 22 Mar 2008, at 1:31 AM, Douglas Bates wrote:

> On Thu, Mar 20, 2008 at 7:04 PM, John Maindonald
> <john.maindonald at anu.edu.au> wrote:
>> I do not think it quite true that the aov model that has an
>> Error() term is a fixed effects model.  The use of the word
>> "stratum" implies that a mixed effects model is lurking
>> somewhere.  The F-tests surely assume such a model.
>
>> Some little time ago, Doug Bates invested me, along with
>> Peter Dalgaard, a member of the degrees of freedom police.
>> Problem is, I am unsure of the responsibilities, but maybe
>> they include commenting on a case such as this.
>
> Indeed, I should have been more explicit regarding the duties of a
> member of the degrees of freedom police when I appointed you :-)  They
> are exactly what you are doing - to weigh in on discussions of degrees
> of freedom.
>
> Perhaps I can expand a bit on why I think that the degrees of freedom
> issue is not a good basis for approaching mixed models in general.
>
> Duncan Temple Lang used to have a quote about "Language shapes the way
> we think." in his email signature and I think similar ideas apply to
> how we think about models.  Certainly the concept of error strata and
> the associated degrees of freedom are one way of thinking about
> mixed-effects models and they are effective in the case of completely
> balanced designs.  If one has a completely balanced design and the
> number of units at various strata is small then decomposition of the
> variability into error strata is possible and the calculation of
> degrees of freedom is important (because the degrees of freedom are
> small - exact values of degrees of freedom are unimportant when they
> are large).  Such data occur frequently - in text books.  In fact,
> they are the only type of data that occur in most text books and hence
> they have shaped the way we think about the models.  Generations of
> statisticians have looked at techniques for small, balanced data sets
> and decided that these define "the" approach to the model.  So when
> confronted with large observational (and, hence, unbalanced or
> "messy") data sets they approach the modeling with this mind set.
>
> Certainly such data did occur in some agricultural trials (I'm not
> sure if this is the state of the art in current agricultural trials)
> and this form of analysis was important but it depends strongly on
> balanced data and balance is a very fragile characteristic in most
> data.  The one exception is data in text books - most often data for
> examples in older texts were added as an afterthought and, for space
> reasons and to illustrate the calculations, were small data sets which
> just happened to be completely balanced so that all the calculations
> worked out.   Fortunately this is no longer the case in books like
> your book with Braun where the data sets are real data and available
> separately from the book in R packages but it will take many years to
> flush the way of thinking about models induced by those small,
> balanced data sets from statistical "knowledge".  (There is an
> enormous amount of inertia built into the system.  Most introductory
> statistics textbooks published today, and probably those published for
> the next couple of decades, will include "statistical tables" as
> appendices because, as everyone knows, all of the statistical analysis
> we do in practice involves doing a few calculations on a hand
> calculator and looking up tabulated values of distributions.)
>
> Getting back to the "language shapes the way we think" issue, I would
> compare it to strategy versus tactics.  I spent a sabbatical in
> Australia visiting Bill Venables.  I needed to adjust to driving on
> the left hand side of the road and was able to do that after a short
> initial period of confusion and discomfort.  If I think of places in
> Adelaide now it is natural for me to think of myself sitting on the
> right hand side of the front seat and driving on the left hand side of
> the road.  That's tactics.  However, even after living in Adelaide for
> several months I remember leaving the parking lot of a shopping mall
> and choosing an indirect route out the back instead of the direct
> route ahead because the direct route would involve a left hand turn
> onto a busy road.  Instead I chose to make two right hand turns to get
> to an intersection with traffic lights where I could make the left
> turn.  At the level of strategy I still "knew" that left hand turns
> are difficult and right hand turns are easy - exactly the wrong
> approach in Australia.
>
> This semester I am attending a seminar on hierarchical linear models
> organized in our social sciences school by a statistician whose
> background is in agricultural statistics.  I make myself unpopular
> because I keep challenging the ideas of the social scientists and of
> the organizer.  The typical kinds of studies we discuss are
> longitudinal studies where the "observational units" (i.e. people) are
> grouped within some social contexts.  These are usually large, highly
> unbalanced data  sets. Because they are longitudinal they inevitably
> involve some migration of people from one group to another.
>
> The migration means that random effects for person and for the various
> types of groups are not nested.  The models are not hierarchical or at
> least they should not be.  Trying to jam the analysis of such data
> into the hierarchical/multilevel framework of software like HLM or
> MLwin doesn't work well but that certainly won't stop people from
> trying.  The hierarchical language in which they learned the model
> affects their strategy.
>
> The agricultural statistician, by contrast, doesn't worry about the
> nesting etc.,  For him the sole issue of important is to determine the
> number of degrees of freedom associated with the various error strata.
> These studies involve tens of thousands of subjects and are nowhere
> close to being balanced but his strategy is still based on having a
> certain number of blocks and plots within the blocks and subplots
> within the plots and everything is exactly balanced.
>
> So I am not opposed to approaches based on error strata and computing
> degrees of freedom where appropriate and where important.  But try not
> to let the particular case of completely balanced small data sets
> determine the strategy of the approach to all models involving random
> effects.  It's fragile and does not generalize well, just as assuming
> a strict hierarchy of factors associated with random effects is
> fragile.  Balance is the special case.  Nesting is the special case.
> It is a bad idea to base strategy on what happens in very special
> cases.
>
>
> they are always balanced and small data sets
>
> Another very fragile
>
>> lme() makes a stab at an appropriate choice of degrees
>> of freedom, but does not always get it right, to the extent
>> that there is a right answer.  [lmer() has for the time being
>> given up on giving degrees of freedom and p-values for
>> fixed effects estimates.]  This part of the output from lme()
>> should, accordingly, be used with discretion.  In case of
>> doubt, check against a likelihood ratio test.  In a simple
>> enough experimental design, users who understand how
>> to calculate degrees of freedom will reason them out for
>> themselves.
>> John Maindonald.
>>
>> John Maindonald             email: john.maindonald at anu.edu.au
>> phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
>> Centre for Mathematics & Its Applications, Room 1194,
>> John Dedman Mathematical Sciences Building (Building 27)
>> Australian National University, Canberra ACT 0200.
>>
>>
>>
>>
>> On 21 Mar 2008, at 9:23 AM, Kevin Wright wrote:
>>
>>> Your question is not very clear, but if you are trying to match the
>>> results in Kuehl, you need a fixed-effects model:
>>>
>>> dat <- read.table("expl14-1.txt", header=TRUE)
>>> dat$block <- factor(dat$block)
>>> dat$nitro <- factor(dat$nitro)
>>> dat$thatch <- factor(dat$thatch)
>>>
>>> colnames(dat) <- c("block","nitro","thatch","chlor")
>>> m1 <- aov(chlor~nitro*thatch+Error(block/nitro), data=dat)
>>> summary(m1)
>>>
>>> Mixed-effects models and degrees of freedom have been discussed many
>>> times on this list....search the archives.
>>>
>>> K Wright
>>>
>>>
>>> On Thu, Mar 20, 2008 at 12:39 PM,  <marcioestat at pop.com.br> wrote:
>>>>
>>>>
>>>>
>>>> Hi listers,
>>>>
>>>> I've been studying anova and at the book of Kuehl at the chapter
>>>> about split-plot there is a experiment with the results... I am
>>>> trying to
>>>> understand the experiments and make the code in order to obtain the
>>>> results... But there is something that I didn't understand yet...
>>>> I have a split-plot design (2 blocks) with two facteurs, one
>>>> facteur has 4 treatments and the other facteur is a measure
>>>> taken in three years...
>>>> I organize my data set as:
>>>>
>>>> Nitro Bloc Year Measure
>>>> a
>>>> x
>>>> 1         3.8
>>>> a
>>>> x
>>>> 2         3.9
>>>> a         x         3         2.0
>>>> a         y         1         3.7
>>>> a         y         2
>>>> 2.4
>>>> a         y         3
>>>> 1.2
>>>> b         x
>>>> 1         4.0
>>>> b         x
>>>> 2         2.5
>>>> and so on...
>>>>
>>>>
>>>> So, I am trying this code, because I want to test each factor and  
>>>> the
>>>> interaction...
>>>> lme=lme(measure ~ bloc + nitro + bloc*nitro, random= ~ 1|year,
>>>> data=lme)
>>>> summary(lme)
>>>> The results that I am obtaining are not correct, because
>>>> I calculated the degrees of fredom and they are not
>>>> correct... According to this design I will get two errors one for  
>>>> the
>>>> whole plot and other for the subplot....
>>>>
>>>> Well, as I told you, I am still learning... Any suggestions...
>>>>
>>>> Thanks in advance,
>>>>
>>>> Ribeiro
>>>>
>>>>
>>>>      [[alternative HTML version deleted]]
>>>>
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>>
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>



From John.Maindonald at anu.edu.au  Sat Mar 22 09:15:38 2008
From: John.Maindonald at anu.edu.au (John Maindonald)
Date: Sat, 22 Mar 2008 19:15:38 +1100
Subject: [R-sig-ME] Split-plot Design
In-Reply-To: <47E3BD70.3070906@ed.ac.uk>
References: <51881.132.204.255.85.1206034766.squirrel@nwebmail.pop.com.br>
	<c968588d0803201523x3912d216i8a4220886336780b@mail.gmail.com>
	<64B59060-3A82-41E8-90FE-E4AFEF36E6A7@anu.edu.au>
	<47E3BD70.3070906@ed.ac.uk>
Message-ID: <9EAE07DB-1747-4D2B-96C1-ED1FDF742AED@anu.edu.au>

A response to your final paragraph.  There's no reason in principle
why aov() should not allow for random slopes.  In fact, one can
put continuous variables into the Error() term, but without taking
time to study the output with some care, it is not obvious to me what
the results mean.

[For reasons that are not clear to me, you mentioned Type III
"errors".  The controversy over Type III versus Type I sums of
squares relates to models with fixed effect interactions.

With models that have a single error term, drop1() will example
the effect of single term deletions, often in unbalanced designs
more relevant than the sequential information in the anova table.
Importantly, drop1() respects hierarchy, whereas Type III sums of
squares do not.]

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.


On 22 Mar 2008, at 12:51 AM, Andy Fugard wrote:

> John Maindonald wrote:
>> I do not think it quite true that the aov model that has an
>> Error() term is a fixed effects model.  The use of the word
>> "stratum" implies that a mixed effects model is lurking
>> somewhere.  The F-tests surely assume such a model.
>
> This is something that has been bothering me for a while.  Often  
> it's argued that ANOVA is just regression; clearly this is not true  
> when it's a repeated measures ANOVA, unless "regression" is  
> interpreted broadly. I think Andrew Gelman argues this somewhere.  I  
> don't see how to get aov to give me a formula, and lm doesn't fit  
> stuff with an Error() term, but if it could, logically I would  
> expect the formula to resemble closely the sort of thing you get  
> with a mixed effects models.
>
> The closest analogy I can find is that doing this...
>
>
> > aov1 = aov(yield ~  N+P+K + Error(block), npk)
> > summary(aov1)
>
> Error: block
>          Df Sum Sq Mean Sq F value Pr(>F)
> Residuals  5    343      69
>
> Error: Within
>          Df Sum Sq Mean Sq F value Pr(>F)
> N          1  189.3   189.3   11.82 0.0037 **
> P          1    8.4     8.4    0.52 0.4800
> K          1   95.2    95.2    5.95 0.0277 *
> Residuals 15  240.2    16.0
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
>
> [I believe the F's and p's here come from a linear construction of  
> nested models (i.e., N versus intercept only; N+P versus N; plus N+P 
> +K versus N+P).]
>
> ... is a bit like doing this with lmer...
>
>
> > lmer0 = lmer(yield ~  1 + (1|block), npk)
> > lmer1 = lmer(yield ~  N + (1|block), npk)
> > lmer2 = lmer(yield ~  N+P + (1|block), npk)
> > lmer3 = lmer(yield ~  N+P+K + (1|block), npk)
> > anova(lmer0,lmer1)
> Data: npk
> Models:
> lmer0: yield ~ 1 + (1 | block)
> lmer1: yield ~ N + (1 | block)
>      Df   AIC   BIC logLik Chisq Chi Df Pr(>Chisq)
> lmer0  2 157.5 159.8  -76.7
> lmer1  3 151.5 155.1  -72.8  7.93      1     0.0049 **
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > anova(lmer1,lmer2)
> Data: npk
> Models:
> lmer1: yield ~ N + (1 | block)
> lmer2: yield ~ N + P + (1 | block)
>      Df   AIC   BIC logLik Chisq Chi Df Pr(>Chisq)
> lmer1  3 151.5 155.1  -72.8
> lmer2  4 153.0 157.8  -72.5  0.47      1       0.49
> > anova(lmer2,lmer3)
> Data: npk
> Models:
> lmer2: yield ~ N + P + (1 | block)
> lmer3: yield ~ N + P + K + (1 | block)
>      Df   AIC   BIC logLik Chisq Chi Df Pr(>Chisq)
> lmer2  4 153.0 157.8  -72.5
> lmer3  5 149.0 154.9  -69.5  6.02      1      0.014 *
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
>
> Namely, fitting the models, and making comparisons using likelihood  
> ratio tests.  Okay, the LLR is asymptotically chisq distributed with  
> df the difference in parameters whereas... well F is different - I  
> always get confused with the df and what bits of the residuals plug  
> in where.
>
> But, is this vaguely right?  (I'm aware that order matters when the  
> design isn't balanced, and have read much about the type I errors  
> versus type III errors disagreements.)
>
> Actually this does lead to one question: following through the  
> analogy, are repeated measures ANOVAs (those fitted with aov) always  
> random intercept models, i.e. with no random slopes?
>
>
> Cheers,
>
> Andy
>
>
> -- 
> Andy Fugard, Postgraduate Research Student
> Psychology (Room F3), The University of Edinburgh,
>  7 George Square, Edinburgh EH8 9JZ, UK
> Mobile: +44 (0)78 123 87190   http://www.possibly.me.uk



From a.fugard at ed.ac.uk  Sat Mar 22 12:28:17 2008
From: a.fugard at ed.ac.uk (Andy Fugard)
Date: Sat, 22 Mar 2008 11:28:17 +0000
Subject: [R-sig-ME] Split-plot Design
In-Reply-To: <9EAE07DB-1747-4D2B-96C1-ED1FDF742AED@anu.edu.au>
References: <51881.132.204.255.85.1206034766.squirrel@nwebmail.pop.com.br>
	<c968588d0803201523x3912d216i8a4220886336780b@mail.gmail.com>
	<64B59060-3A82-41E8-90FE-E4AFEF36E6A7@anu.edu.au>
	<47E3BD70.3070906@ed.ac.uk>
	<9EAE07DB-1747-4D2B-96C1-ED1FDF742AED@anu.edu.au>
Message-ID: <20080322112817.yo5su8468gk0w48s@www.sms.ed.ac.uk>

Quoting John Maindonald <John.Maindonald at anu.edu.au>:

> A response to your final paragraph.  There's no reason in principle
> why aov() should not allow for random slopes.  In fact, one can
> put continuous variables into the Error() term, but without taking
> time to study the output with some care, it is not obvious to me what
> the results mean.

Aha, thanks.

>
> [For reasons that are not clear to me, you mentioned Type III
> "errors".  The controversy over Type III versus Type I sums of
> squares relates to models with fixed effect interactions.

Interference effect; I meant to say "sums of squares", not "errors".   
I brought this up as I wanted to flag awareness that it's not always  
appropriate (with respect to one's guiding hypotheses/questions) to  
compare nested models where terms are added sequentialy in some  
arbitrary order.

Thanks,

Andy

>
> John Maindonald             email: john.maindonald at anu.edu.au
> phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
> Centre for Mathematics & Its Applications, Room 1194,
> John Dedman Mathematical Sciences Building (Building 27)
> Australian National University, Canberra ACT 0200.
>
>
> On 22 Mar 2008, at 12:51 AM, Andy Fugard wrote:
>
>> John Maindonald wrote:
>>> I do not think it quite true that the aov model that has an
>>> Error() term is a fixed effects model.  The use of the word
>>> "stratum" implies that a mixed effects model is lurking
>>> somewhere.  The F-tests surely assume such a model.
>>
>> This is something that has been bothering me for a while.  Often   
>> it's argued that ANOVA is just regression; clearly this is not true  
>>  when it's a repeated measures ANOVA, unless "regression" is   
>> interpreted broadly. I think Andrew Gelman argues this somewhere.    
>> I don't see how to get aov to give me a formula, and lm doesn't fit  
>>  stuff with an Error() term, but if it could, logically I would   
>> expect the formula to resemble closely the sort of thing you get   
>> with a mixed effects models.
>>
>> The closest analogy I can find is that doing this...
>>
>>
>>> aov1 = aov(yield ~  N+P+K + Error(block), npk)
>>> summary(aov1)
>>
>> Error: block
>>         Df Sum Sq Mean Sq F value Pr(>F)
>> Residuals  5    343      69
>>
>> Error: Within
>>         Df Sum Sq Mean Sq F value Pr(>F)
>> N          1  189.3   189.3   11.82 0.0037 **
>> P          1    8.4     8.4    0.52 0.4800
>> K          1   95.2    95.2    5.95 0.0277 *
>> Residuals 15  240.2    16.0
>> ---
>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>
>>
>> [I believe the F's and p's here come from a linear construction of   
>> nested models (i.e., N versus intercept only; N+P versus N; plus   
>> N+P+K versus N+P).]
>>
>> ... is a bit like doing this with lmer...
>>
>>
>>> lmer0 = lmer(yield ~  1 + (1|block), npk)
>>> lmer1 = lmer(yield ~  N + (1|block), npk)
>>> lmer2 = lmer(yield ~  N+P + (1|block), npk)
>>> lmer3 = lmer(yield ~  N+P+K + (1|block), npk)
>>> anova(lmer0,lmer1)
>> Data: npk
>> Models:
>> lmer0: yield ~ 1 + (1 | block)
>> lmer1: yield ~ N + (1 | block)
>>     Df   AIC   BIC logLik Chisq Chi Df Pr(>Chisq)
>> lmer0  2 157.5 159.8  -76.7
>> lmer1  3 151.5 155.1  -72.8  7.93      1     0.0049 **
>> ---
>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>> anova(lmer1,lmer2)
>> Data: npk
>> Models:
>> lmer1: yield ~ N + (1 | block)
>> lmer2: yield ~ N + P + (1 | block)
>>     Df   AIC   BIC logLik Chisq Chi Df Pr(>Chisq)
>> lmer1  3 151.5 155.1  -72.8
>> lmer2  4 153.0 157.8  -72.5  0.47      1       0.49
>>> anova(lmer2,lmer3)
>> Data: npk
>> Models:
>> lmer2: yield ~ N + P + (1 | block)
>> lmer3: yield ~ N + P + K + (1 | block)
>>     Df   AIC   BIC logLik Chisq Chi Df Pr(>Chisq)
>> lmer2  4 153.0 157.8  -72.5
>> lmer3  5 149.0 154.9  -69.5  6.02      1      0.014 *
>> ---
>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>
>>
>> Namely, fitting the models, and making comparisons using likelihood  
>>  ratio tests.  Okay, the LLR is asymptotically chisq distributed   
>> with df the difference in parameters whereas... well F is different  
>>  - I always get confused with the df and what bits of the residuals  
>>  plug in where.
>>
>> But, is this vaguely right?  (I'm aware that order matters when the  
>>  design isn't balanced, and have read much about the type I errors   
>> versus type III errors disagreements.)
>>
>> Actually this does lead to one question: following through the   
>> analogy, are repeated measures ANOVAs (those fitted with aov)   
>> always random intercept models, i.e. with no random slopes?
>>
>>
>> Cheers,
>>
>> Andy
>>
>>
>> -- 
>> Andy Fugard, Postgraduate Research Student
>> Psychology (Room F3), The University of Edinburgh,
>> 7 George Square, Edinburgh EH8 9JZ, UK
>> Mobile: +44 (0)78 123 87190   http://www.possibly.me.uk



-- 
Andy Fugard, Postgraduate Research Student
Psychology (Room F3), The University of Edinburgh,
   7 George Square, Edinburgh EH8 9JZ, UK
Mobile: +44 (0)78 123 87190   http://www.possibly.me.uk


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.



From bates at stat.wisc.edu  Sat Mar 22 16:40:08 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 22 Mar 2008 10:40:08 -0500
Subject: [R-sig-ME] Split-plot Design
In-Reply-To: <9450DEBA-94B9-4BB6-A258-A03F3B60F4D9@anu.edu.au>
References: <51881.132.204.255.85.1206034766.squirrel@nwebmail.pop.com.br>
	<c968588d0803201523x3912d216i8a4220886336780b@mail.gmail.com>
	<64B59060-3A82-41E8-90FE-E4AFEF36E6A7@anu.edu.au>
	<40e66e0b0803210731h2fa09bbaqcb9e4fbb3794fc3a@mail.gmail.com>
	<9450DEBA-94B9-4BB6-A258-A03F3B60F4D9@anu.edu.au>
Message-ID: <40e66e0b0803220840na0a96a8h3465d6995573730b@mail.gmail.com>

Thanks for your response John.  I just have one quick comment about
the Kenward-Roger degrees of freedom calculation.  It has been some
time since I looked at that paper but my impression at the time was
that the equations would not easily translate into the formulation
used in lmer.  The approach used in lmer is like that of Henderson's
mixed model equations (with many modifications).  That is, it is based
on a penalized least squares problem, not a generalized least squares
problem.  My recollection is that Kenward and Roger wrote their
equations in terms of the generalized least squares problem.

You are not the first person to suggest that incorporating the
Kenward-Roger calculation would enhance lmer.  The reason I haven't
done so is that I believe it is far from trivial to do so and I have
many, many other enhancements I would prefer to spend my time on.
However, this is open source software and any enterprising person who
wants to implement it is more than welcome to do so.  It may be
sufficiently involved to be a thesis topic - I don't know because I
haven't studied it carefully enough.

Any person considering doing that should read or reread Bill Venables'
"Exegeses on Linear Models" before embarking on it.  As he points out
in his discussion of modifications made in S-PLUS to emulate some
calculations in SAS, the "brute force" approach of taking a set of
equations and implementing them literally is rarely a good approach.
So I am not talking about a "pidgin R" implementation here where a
linear least squares calculation is written

XpX <- t(X) %*% X
XpXinv <- solve(XpX)
Xpy <- t(X) %*% y
betahat <- XpXinv %*% Xpy

An mer object includes slots L, RZX and RX that define the Cholesky
decomposition of the crossproduct matrix that is more-or-less like
that in the Henderson mixed model equations.  The L slot itself has a
Perm slot that gives the fill-reducing permutation P for the random
effects.  That may not be relevant - I'm not sure.  The original model
matrices are available as the X and Zt (transpose of Z) slots.  The
(transpose of the) derived model matrix for the orthogonal random
effects is available as A.  The terms attribute of the model matrix X
and the assign attribute of the model frame (in the slot named
"frame") should be used to associate terms with columns of the model
matrix.

It's possible that the calculation would be straightforward.  As i
said, I don't know.  My gut feeling is that it is not, which is why I
haven't embarked on it.



From marcioestat at pop.com.br  Sun Mar 23 22:15:21 2008
From: marcioestat at pop.com.br (marcioestat at pop.com.br)
Date: Sun, 23 Mar 2008 18:15:21 -0300 (BRT)
Subject: [R-sig-ME] Split-plot Design
In-Reply-To: <40e66e0b0803220840na0a96a8h3465d6995573730b@mail.gmail.com>
References: <51881.132.204.255.85.1206034766.squirrel@nwebmail.pop.com.br>
	<c968588d0803201523x3912d216i8a4220886336780b@mail.gmail.com>
	<64B59060-3A82-41E8-90FE-E4AFEF36E6A7@anu.edu.au>
	<40e66e0b0803210731h2fa09bbaqcb9e4fbb3794fc3a@mail.gmail.com>
	<9450DEBA-94B9-4BB6-A258-A03F3B60F4D9@anu.edu.au>
	<40e66e0b0803220840na0a96a8h3465d6995573730b@mail.gmail.com>
Message-ID: <53987.70.52.44.141.1206306921.squirrel@nwebmail.pop.com.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080323/15c5464c/attachment.pl>

From A.Robinson at ms.unimelb.edu.au  Sun Mar 23 23:09:43 2008
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Mon, 24 Mar 2008 09:09:43 +1100
Subject: [R-sig-ME] Split-plot Design
In-Reply-To: <53987.70.52.44.141.1206306921.squirrel@nwebmail.pop.com.br>
References: <51881.132.204.255.85.1206034766.squirrel@nwebmail.pop.com.br>
	<c968588d0803201523x3912d216i8a4220886336780b@mail.gmail.com>
	<64B59060-3A82-41E8-90FE-E4AFEF36E6A7@anu.edu.au>
	<40e66e0b0803210731h2fa09bbaqcb9e4fbb3794fc3a@mail.gmail.com>
	<9450DEBA-94B9-4BB6-A258-A03F3B60F4D9@anu.edu.au>
	<40e66e0b0803220840na0a96a8h3465d6995573730b@mail.gmail.com>
	<53987.70.52.44.141.1206306921.squirrel@nwebmail.pop.com.br>
Message-ID: <20080323220943.GB1331@ms.unimelb.edu.au>

Hi Ribeiro,

try section 1.6 of Pinheiro and Bates, as a starting point, and/or
section 10.2 of Venables and Ripley.

Andrew


On Sun, Mar 23, 2008 at 06:15:21PM -0300, marcioestat at pop.com.br wrote:
> 
> 
> 
> Hey
> listers,
> 
> ?
> 
> 
> 
> ?
> 
> It&rsquo;s good to know that I still
> have a lot of search to do&hellip;
> 
> ?
> 
> According to the two procedures AOV
> and LME, I got two different results and I didn&rsquo;t understand at all
> the results of LME&hellip; There is a coefficient estimate of each level
> and I just pretend to test if the effects of the factors and interaction
> are significant or not&hellip;So I would like to learn more about this
> function, because it's adequate for split-plot design...
> test.anova=
> aov(mes ~ nitro*thatch + Error(block/nitro), data=test)
> 
> ?
> 
> summary(test.anova)
> 
> ?
> 
> test.lme
> <- lme(mes ~ nitro*thatch,random= ~ 1|block/nitro, data=test)
> 
> ?
> 
> summary(test.lme)
> 
> ?
> 
> Does anybody has any document or
> reference that explain this results in details of the lme&hellip;
> I&rsquo;ve looked for, but I didn&rsquo;t find any good
> explication&hellip; I notice that there is a lot of information about the
> lmer, but I am going step by step&hellip;
> 
> ?
> 
> 
> 
> ?
> 
> Thanks,
> 
> ?
> 
> 
> 
> ?
> 
> Ribeiro
> 
> ?
> 
> 
> ?
> > Thanks for your response John. I just have one quick comment
> about 
> > the Kenward-Roger degrees of freedom calculation. It has
> been some 
> > time since I looked at that paper but my impression
> at the time was 
> > that the equations would not easily translate
> into the formulation 
> > used in lmer. The approach used in lmer is
> like that of Henderson's 
> > mixed model equations (with many
> modifications). That is, it is based 
> > on a penalized least
> squares problem, not a generalized least squares 
> > problem. My
> recollection is that Kenward and Roger wrote their 
> > equations in
> terms of the generalized least squares problem. 
> > 
> > You
> are not the first person to suggest that incorporating the 
> >
> Kenward-Roger calculation would enhance lmer. The reason I haven't 
> > done so is that I believe it is far from trivial to do so and I
> have 
> > many, many other enhancements I would prefer to spend my
> time on. 
> > However, this is open source software and any
> enterprising person who 
> > wants to implement it is more than
> welcome to do so. It may be 
> > sufficiently involved to be a
> thesis topic - I don't know because I 
> > haven't studied it
> carefully enough. 
> > 
> > Any person considering doing that
> should read or reread Bill Venables' 
> > "Exegeses on Linear
> Models" before embarking on it. As he points out 
> > in his
> discussion of modifications made in S-PLUS to emulate some 
> >
> calculations in SAS, the "brute force" approach of taking a set
> of 
> > equations and implementing them literally is rarely a good
> approach. 
> > So I am not talking about a "pidgin R"
> implementation here where a 
> > linear least squares calculation is
> written 
> > 
> > XpX <- t(X) %*% X 
> > XpXinv <-
> solve(XpX) 
> > Xpy <- t(X) %*% y 
> > betahat <- XpXinv
> %*% Xpy 
> > 
> > An mer object includes slots L, RZX and RX
> that define the Cholesky 
> > decomposition of the crossproduct
> matrix that is more-or-less like 
> > that in the Henderson mixed
> model equations. The L slot itself has a 
> > Perm slot that gives
> the fill-reducing permutation P for the random 
> > effects. That
> may not be relevant - I'm not sure. The original model 
> > matrices
> are available as the X and Zt (transpose of Z) slots. The 
> >
> (transpose of the) derived model matrix for the orthogonal random 
> > effects is available as A. The terms attribute of the model matrix
> X 
> > and the assign attribute of the model frame (in the slot
> named 
> > "frame") should be used to associate terms with
> columns of the model 
> > matrix. 
> > 
> > It's
> possible that the calculation would be straightforward. As i 
> >
> said, I don't know. My gut feeling is that it is not, which is why I 
> > haven't embarked on it. 
> > 
> >
> _______________________________________________ 
> >
> R-sig-mixed-models at r-project.org mailing list 
> >
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models 
> > 
> 
> 
> 	[[alternative HTML version deleted]]
> 

> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-6410
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/



From fajardo.alex at gmail.com  Mon Mar 24 22:56:14 2008
From: fajardo.alex at gmail.com (Alex Fajardo)
Date: Mon, 24 Mar 2008 14:56:14 -0700
Subject: [R-sig-ME] read.csv: uploading files
Message-ID: <f69dbed20803241456o2dbe4cc2t839b858bbcf74510@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080324/3df0da2d/attachment.pl>

From p.dalgaard at biostat.ku.dk  Mon Mar 24 23:18:11 2008
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 24 Mar 2008 23:18:11 +0100
Subject: [R-sig-ME] read.csv: uploading files
In-Reply-To: <f69dbed20803241456o2dbe4cc2t839b858bbcf74510@mail.gmail.com>
References: <f69dbed20803241456o2dbe4cc2t839b858bbcf74510@mail.gmail.com>
Message-ID: <47E828A3.4040109@biostat.ku.dk>

Alex Fajardo wrote:
> To whom it may concern,
> please, help me out.
> I am learning how to proceed with mixed-effects models analysis and
> downloaded the last R version, R-2.6.2., which comes with "lme4" package.
> However, I have a simple problem to upload my file. With older versions I
> import files via the "read.csv" command, like:
> SLA <- read.csv("SLA.csv")
>
> but now it doesn't recognize the file at all:
>
>   
>> SLA = read.csv("SLA.csv")
>>     
> *Error in file(file, "r") : unable to open connection
> In addition: Warning message:
> In file(file, "r") :
> cannot open file 'SLA.csv', reason 'No such file or directory'
> *
> It seems to me I need to save the file somewhere else and not just at the "
> R-2.6.2" directory.
> I will appreciate your help. Cheers,
>
>   
You ensure that the file is in the CURRENT directory, as always.

Now, the default for that has changed, because the install directory was 
not writable for some installs, and dumping random files there was never 
a good idea. To find out where the current directory is, type getwd(), 
to change it, use setwd() or "File/Change dir" from the menu.

Or, of course, you can give the complete path to the file. If you don't 
know what it is, file.choose() is your friend.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark      Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)              FAX: (+45) 35327907



From syadp at yahoo.com.mx  Tue Mar 25 17:23:07 2008
From: syadp at yahoo.com.mx (Anaid Diaz)
Date: Tue, 25 Mar 2008 11:23:07 -0500 (CDT)
Subject: [R-sig-ME] Mixed-effects models: question about the syntax to
	include interactions
Message-ID: <889902.99187.qm@web51905.mail.re2.yahoo.com>

Hello everyone,

I would like to as for advice on the use of ?lmer?
(package ?lme4?) and writing the proper syntax to best
describe my data using a mixed-effects model.

I have just started using these models, and although I
have read some good examples (Extending the Linear
Model with R, Faraway 2005; and the R book, Crawley
2007), I am still not sure what is the best syntax to
test my hypothesis.

Thanks in advance for reading me.

Briefly, I describe the data and the situation:

I want to describe the age-specific fecundity of the
ith individual from the jth replicate (or line) from
the kth strain.

Variables:

Categorical factors:

A[a] = Age (1,2,3
n=8) #Because the fecundity is not
linear, I decided to include it in the model as a
factor
s[k] =strain (A and B, n=2) # for the moment two, but
it?s likely to increase as the work progresses
l[j]   = line (1,2,..n=10)
i[i]   = Ind(1,2
 n=50)

(Note: I use capital letters for fixed factors and low
case for random effects)

Because the experimental design, the data follows a
hierarchical structure: where the ith individual is
nested within the jth line, and line within the kth
strain

Continuous (response) variable:
Y =Age-specific fecundity (362 observations)

Models:

Because I was (I am still) not sure of how to include
all the variables in a single model, I started by
splitting up the data and assessing which is the best
model for each strain, therefore the ?Simplest? model
for each strain is:

Linear model.

Y[aij] = A[a] + error[aij]

R code:

m1 <- lm(fecudnity ~ Age)

Reduced mixed-effect model:

Y[aij] = A[a] + l[j] + i[i] + error[aij]

R code:

m2 <- lmer(fecudnity ~ Age + (1 | line/ind),
method=?ML?)

And a ?Full mixed-effects model? model (looking for
interactions between Age and line/ind)

Y[aij] = A[a] + A[a]*l[j] + A[a]*i[i] + error[aij]

R code:

m3 <- lmer(fecudnity ~ Age + (Age | line/ind),
method=?ML?)

I have used Likelihood test ratio (LTR) to compare
between models, and I have found that for strain A the
best model is m3 (X^2 [36 d.f] =164.8, p-value=
4.73e-13), whereas for strain B, the best one is m1
(X^2 [2 d.f] =1.47, p-value= 0.473). Therefore, I
interpret these results as follow:

-	The variance between individuals in strain A is
large, and it is best described when I include
information about the line where the individuals come
from. Moreover, there is a significant interaction
between age and line/ind. Thus, some individuals have
higher fecundity at later ages compared to others.

-	The variance between individuals in strain B is low;
therefore the variance between ind/lines and
interactions can be ignored. 

These results, on their own, are quite interesting,
but I would like to have a model where I include both
strains (and still can make some interpretations)

My first guess is

m4 <- lmer(fecudnity ~ Age + (1 | strain/line/ind),
method=?ML?)
m5 <- lmer(fecudnity ~ Age + (Age | strain/line/ind),
method=?ML?)

Using LTR, I find that m5 describes better the data
(X^2 [105 d.f] = 347.15, p-value  < 2.2 e-16), but I
feel like I cannot say much of which strain has more
individual variance (or perhaps I am wrong and not
looking in the right place).

Then I though about using strain as a fixed factor,
because I am now interested in the differences between
strains:

m6 <- lmer(fecudnity ~ Age * Strain +  (Age
|strain/line/ind), method=?ML?)

or perhaps include it in the random interaction?

m7 <- lmer(fecudnity ~ Age +  (Age * Strain
|strain/line/ind), method=?ML?)

I have to be honest, at this point, I am just not sure
of how to write the model to describe the age-specific
fecundity and test the hypothesis of whether one
strain shows more variance between individuals and
lines or not. I hope some one could give some advice.

Thanks in advance

Anaid Diaz

~~~~~~~~~~~ ~ ~~~ ~ ~   ~   ~  ~   ~  ~
S.Anaid Diaz
Rm 303, Graham Kerr Bldg
Theoretical Ecology Group
Division of Environmental & Evolutionary Biology
University of Glasgow
Glasgow G12 8QQ

tel: 0141 330 2430
fax: 0141 330 5971

http://www.gla.ac.uk/ibls/DEEB/teg/people/diaz.htm 


      ____________________________________________________________________________________
?Capacidad ilimitada de almacenamiento en tu correo!
No te preocupes m?s por el espacio de tu cuenta con Correo Yahoo!:



From bates at stat.wisc.edu  Wed Mar 26 23:40:54 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 26 Mar 2008 17:40:54 -0500
Subject: [R-sig-ME] New alpha version of lme4 code on R-forge
Message-ID: <40e66e0b0803261540w1d4bee7n256c32bb75d2d81a@mail.gmail.com>

We have had a bit of grid lock on versions of the Matrix and lme4
packages for the last week or so.  I believe the problem is now
resolved and the development version (0.999375-10) of the lme4 package
on R-forge should be available as a binary package tomorrow.  The
objects produced by mcmcsamp in this version have several kinds of
plots (xyplot, densityplot, qqmath) available for them.  The mcmcsamp
function itself still has the problem of getting stuck at near-zero
variances.  I have been corresponding with Andrew Gelman about methods
that he and co-authors recently described in J. of Computational and
Graphical Statistics to avoid this problem.  These will be implemented
"in the fullness of time", as Bill Venables would say.



From danw at sussex.ac.uk  Thu Mar 27 10:30:31 2008
From: danw at sussex.ac.uk (Dan Wright)
Date: Thu, 27 Mar 2008 09:30:31 +0000
Subject: [R-sig-ME] New alpha version of lme4 code on R-forge
In-Reply-To: <40e66e0b0803261540w1d4bee7n256c32bb75d2d81a@mail.gmail.com>
References: <40e66e0b0803261540w1d4bee7n256c32bb75d2d81a@mail.gmail.com>
Message-ID: <2E8789E97F7D4AD4F0C48F78@psycho934.lifesci.susx.ac.uk>

Dear Professor Bates,

Thank you for this! I have some questions that I assume others on the list 
are interested in. You mentioned in one message that you are working on a 
book. What is it on? When out? Is it appropriate to cite when using lme4?

Much thanks,

Dan


Daniel B. Wright

from August
Psychology Department
Florida Internation University
Miami, FL

danw at sussex.ac.uk
http://www.sussex.ac.uk/Users/danw/



From bates at stat.wisc.edu  Thu Mar 27 14:17:59 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 27 Mar 2008 08:17:59 -0500
Subject: [R-sig-ME] New alpha version of lme4 code on R-forge
In-Reply-To: <2E8789E97F7D4AD4F0C48F78@psycho934.lifesci.susx.ac.uk>
References: <40e66e0b0803261540w1d4bee7n256c32bb75d2d81a@mail.gmail.com>
	<2E8789E97F7D4AD4F0C48F78@psycho934.lifesci.susx.ac.uk>
Message-ID: <40e66e0b0803270617v4bd360a6j9987e01f005a9a27@mail.gmail.com>

On Thu, Mar 27, 2008 at 4:30 AM, Dan Wright <danw at sussex.ac.uk> wrote:
> Dear Professor Bates,

>  Thank you for this! I have some questions that I assume others on the list
>  are interested in. You mentioned in one message that you are working on a
>  book. What is it on? When out? Is it appropriate to cite when using lme4?

Thanks for the inquiry.  The working title of the book is "Multilevel
modeling in R" although that may change before publication.  It is on
the practice of using the lme4 package to fit and analyze
mixed-effects models, including linear mixed models, generalized
linear mixed models, nonlinear mixed models and generalized nonlinear
mixed models.  I emphasize graphical displays of the data and aspects
of the fitted models and inferences based on MCMC samples from the
posterior distribution of the model parameters.  The statistical
theory and computational methods for fitting and analyzing the models
are also described in detail.

I have given up on issuing predictions of when the book will be
available.  "Not soon" is about the best I can say.  There is a lot of
writing still to be done.  Thanks for the suggestion of citing the
book when using lme4 but that would be premature.



From i.m.s.white at ed.ac.uk  Thu Mar 27 13:40:48 2008
From: i.m.s.white at ed.ac.uk (ian white)
Date: Thu, 27 Mar 2008 12:40:48 +0000
Subject: [R-sig-ME] use of pedigree structures
Message-ID: <47EB95D0.3070602@ed.ac.uk>

The lmer function includes random terms as

(expression for a model matrix|grouping factor)

and I wonder how general the expression can be. For example, a pedigree 
object can be converted to an A matrix (in the language of animal 
breeding), but can the A matrix be incorporated into an lmer analysis?

I thought that

(chol(pedigree)|ones)

might be OK, where 'ones' is a constant vector of value 1. With the only 
example I have tried, the syntax is accepted but the analysis ends with 
a forced exit from R (core dump).


-- 
  I White
  University of Edinburgh
  Ashworth Laboratories, West Mains Road
  Edinburgh EH9 3JT  (Tel 0131 650 5490)



From bates at stat.wisc.edu  Thu Mar 27 17:11:45 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 27 Mar 2008 11:11:45 -0500
Subject: [R-sig-ME] use of pedigree structures
In-Reply-To: <47EB95D0.3070602@ed.ac.uk>
References: <47EB95D0.3070602@ed.ac.uk>
Message-ID: <40e66e0b0803270911o114c58dcv72b5b1f14f2332bf@mail.gmail.com>

The pedigree class has been moved to the pedigreemm package, which is
also on R-forge

http://r-forge.r-project.org/projects/pedigreemm

In that package Ana Vazquez and I are developing methods to
incorporate pedigrees using animal models or sire models.  That
package is still under development.  If you are particularly anxious
to use those methods you may want to contact Ana directly.

On Thu, Mar 27, 2008 at 7:40 AM, ian white <i.m.s.white at ed.ac.uk> wrote:
> The lmer function includes random terms as
>
>  (expression for a model matrix|grouping factor)
>
>  and I wonder how general the expression can be. For example, a pedigree
>  object can be converted to an A matrix (in the language of animal
>  breeding), but can the A matrix be incorporated into an lmer analysis?
>
>  I thought that
>
>  (chol(pedigree)|ones)
>
>  might be OK, where 'ones' is a constant vector of value 1. With the only
>  example I have tried, the syntax is accepted but the analysis ends with
>  a forced exit from R (core dump).
>
>
>  --
>   I White
>   University of Edinburgh
>   Ashworth Laboratories, West Mains Road
>   Edinburgh EH9 3JT  (Tel 0131 650 5490)
>
>  _______________________________________________
>  R-sig-mixed-models at r-project.org mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Thu Mar 27 17:18:36 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 27 Mar 2008 11:18:36 -0500
Subject: [R-sig-ME] how to locate standard errors in lmer output
In-Reply-To: <47EB71BD.2020101@univ-provence.fr>
References: <47EB71BD.2020101@univ-provence.fr>
Message-ID: <40e66e0b0803270918n2d422fe3ta6cd1f7e322911af@mail.gmail.com>

 Thu, Mar 27, 2008 at 5:06 AM, F.-Xavier ALARIO
<Francois-Xavier.Alario at univ-provence.fr> wrote:
>  I have been looking in various documentations (e.g. in
>  http://rweb.stat.umn.edu/R/library/lme4/doc/) for a way to access
>  Standard Errors of fixed effects in the output of the 'lmer' function.
>  slotNames() didn't help.

>  Is there a way to do it similar to fixef(), ranef() or with a slot name?

The preferred way would be

stderr <- sqrt(diag(vcov(fm)))

for a single evaluation.  (If you were putting such a calculation
within a large simulation you may want to work harder to tune it up
but this is fine for a single evaluation.)

>  I have the same question for t-values.

fixef(fm)/stderr

The obvious next question is about the p-values but that's a sore
point so don't ask. :-)



From njbisaac at googlemail.com  Fri Mar 28 12:11:12 2008
From: njbisaac at googlemail.com (Nick Isaac)
Date: Fri, 28 Mar 2008 11:11:12 +0000
Subject: [R-sig-ME] Weights in lmer
Message-ID: <a072ed700803280411s3a16115ah1c895401ba29c9e2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080328/f697cf7b/attachment.pl>

From njbisaac at googlemail.com  Fri Mar 28 12:35:10 2008
From: njbisaac at googlemail.com (Nick Isaac)
Date: Fri, 28 Mar 2008 11:35:10 +0000
Subject: [R-sig-ME] Weights in lmer
Message-ID: <a072ed700803280435y11c6a7c1sf96d70b9bd5469d3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080328/40586de4/attachment.pl>

From lamprianou at yahoo.com  Fri Mar 28 13:05:01 2008
From: lamprianou at yahoo.com (Iasonas Lamprianou)
Date: Fri, 28 Mar 2008 05:05:01 -0700 (PDT)
Subject: [R-sig-ME] R-sig-mixed-models Digest, Vol 15, Issue 23
Message-ID: <936225.67095.qm@web54105.mail.re2.yahoo.com>

Prof. Bates,
we all thank you for investing so much time to write the book (Multilevel models in R). A polite request: please, please include as many practical and detailed examples as possible, Not just one example per chapter. And what we need, is detailed guides on how to interpret the results. And how to defend our models: i.e. to prove that they have satisfactory fit and that we have not violated their assumptions.
Thanks
 
Dr. Iasonas Lamprianou
Department of Education
The University of Manchester
Oxford Road, Manchester M13 9PL, UK
Tel. 0044 161 275 3485
iasonas.lamprianou at manchester.ac.uk


----- Original Message ----
From: "r-sig-mixed-models-request at r-project.org" <r-sig-mixed-models-request at r-project.org>
To: r-sig-mixed-models at r-project.org
Sent: Friday, 28 March, 2008 1:00:02 PM
Subject: R-sig-mixed-models Digest, Vol 15, Issue 23

Send R-sig-mixed-models mailing list submissions to
    r-sig-mixed-models at r-project.org

To subscribe or unsubscribe via the World Wide Web, visit
    https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
or, via email, send a message with subject or body 'help' to
    r-sig-mixed-models-request at r-project.org

You can reach the person managing the list at
    r-sig-mixed-models-owner at r-project.org

When replying, please edit your Subject line so it is more specific
than "Re: Contents of R-sig-mixed-models digest..."


Today's Topics:

  1. Re: New alpha version of lme4 code on R-forge (Douglas Bates)
  2. use of pedigree structures (ian white)
  3. Re: use of pedigree structures (Douglas Bates)
  4. Re: how to locate standard errors in lmer output (Douglas Bates)


----------------------------------------------------------------------

Message: 1
Date: Thu, 27 Mar 2008 08:17:59 -0500
From: "Douglas Bates" <bates at stat.wisc.edu>
Subject: Re: [R-sig-ME] New alpha version of lme4 code on R-forge
To: "Dan Wright" <danw at sussex.ac.uk>
Cc: R Mixed Models <r-sig-mixed-models at r-project.org>
Message-ID:
    <40e66e0b0803270617v4bd360a6j9987e01f005a9a27 at mail.gmail.com>
Content-Type: text/plain; charset=ISO-8859-1

On Thu, Mar 27, 2008 at 4:30 AM, Dan Wright <danw at sussex.ac.uk> wrote:
> Dear Professor Bates,

>  Thank you for this! I have some questions that I assume others on the list
>  are interested in. You mentioned in one message that you are working on a
>  book. What is it on? When out? Is it appropriate to cite when using lme4?

Thanks for the inquiry.  The working title of the book is "Multilevel
modeling in R" although that may change before publication.  It is on
the practice of using the lme4 package to fit and analyze
mixed-effects models, including linear mixed models, generalized
linear mixed models, nonlinear mixed models and generalized nonlinear
mixed models.  I emphasize graphical displays of the data and aspects
of the fitted models and inferences based on MCMC samples from the
posterior distribution of the model parameters.  The statistical
theory and computational methods for fitting and analyzing the models
are also described in detail.

I have given up on issuing predictions of when the book will be
available.  "Not soon" is about the best I can say.  There is a lot of
writing still to be done.  Thanks for the suggestion of citing the
book when using lme4 but that would be premature.



------------------------------

Message: 2
Date: Thu, 27 Mar 2008 12:40:48 +0000
From: ian white <i.m.s.white at ed.ac.uk>
Subject: [R-sig-ME] use of pedigree structures
To: r-sig-mixed-models at r-project.org
Message-ID: <47EB95D0.3070602 at ed.ac.uk>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

The lmer function includes random terms as

(expression for a model matrix|grouping factor)

and I wonder how general the expression can be. For example, a pedigree 
object can be converted to an A matrix (in the language of animal 
breeding), but can the A matrix be incorporated into an lmer analysis?

I thought that

(chol(pedigree)|ones)

might be OK, where 'ones' is a constant vector of value 1. With the only 
example I have tried, the syntax is accepted but the analysis ends with 
a forced exit from R (core dump).


-- 
  I White
  University of Edinburgh
  Ashworth Laboratories, West Mains Road
  Edinburgh EH9 3JT  (Tel 0131 650 5490)



------------------------------

Message: 3
Date: Thu, 27 Mar 2008 11:11:45 -0500
From: "Douglas Bates" <bates at stat.wisc.edu>
Subject: Re: [R-sig-ME] use of pedigree structures
To: "ian white" <i.m.s.white at ed.ac.uk>
Cc: r-sig-mixed-models at r-project.org, Ana Vazquez
    <anainesvs at gmail.com>
Message-ID:
    <40e66e0b0803270911o114c58dcv72b5b1f14f2332bf at mail.gmail.com>
Content-Type: text/plain; charset=ISO-8859-1

The pedigree class has been moved to the pedigreemm package, which is
also on R-forge

http://r-forge.r-project.org/projects/pedigreemm

In that package Ana Vazquez and I are developing methods to
incorporate pedigrees using animal models or sire models.  That
package is still under development.  If you are particularly anxious
to use those methods you may want to contact Ana directly.

On Thu, Mar 27, 2008 at 7:40 AM, ian white <i.m.s.white at ed.ac.uk> wrote:
> The lmer function includes random terms as
>
>  (expression for a model matrix|grouping factor)
>
>  and I wonder how general the expression can be. For example, a pedigree
>  object can be converted to an A matrix (in the language of animal
>  breeding), but can the A matrix be incorporated into an lmer analysis?
>
>  I thought that
>
>  (chol(pedigree)|ones)
>
>  might be OK, where 'ones' is a constant vector of value 1. With the only
>  example I have tried, the syntax is accepted but the analysis ends with
>  a forced exit from R (core dump).
>
>
>  --
>  I White
>  University of Edinburgh
>  Ashworth Laboratories, West Mains Road
>  Edinburgh EH9 3JT  (Tel 0131 650 5490)
>
>  _______________________________________________
>  R-sig-mixed-models at r-project.org mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



------------------------------

Message: 4
Date: Thu, 27 Mar 2008 11:18:36 -0500
From: "Douglas Bates" <bates at stat.wisc.edu>
Subject: Re: [R-sig-ME] how to locate standard errors in lmer output
To: "F.-Xavier ALARIO" <Francois-Xavier.Alario at univ-provence.fr>
Cc: R Mixed Models <r-sig-mixed-models at r-project.org>
Message-ID:
    <40e66e0b0803270918n2d422fe3ta6cd1f7e322911af at mail.gmail.com>
Content-Type: text/plain; charset=ISO-8859-1

Thu, Mar 27, 2008 at 5:06 AM, F.-Xavier ALARIO
<Francois-Xavier.Alario at univ-provence.fr> wrote:
>  I have been looking in various documentations (e.g. in
>  http://rweb.stat.umn.edu/R/library/lme4/doc/) for a way to access
>  Standard Errors of fixed effects in the output of the 'lmer' function.
>  slotNames() didn't help.

>  Is there a way to do it similar to fixef(), ranef() or with a slot name?

The preferred way would be

stderr <- sqrt(diag(vcov(fm)))

for a single evaluation.  (If you were putting such a calculation
within a large simulation you may want to work harder to tune it up
but this is fine for a single evaluation.)

>  I have the same question for t-values.

fixef(fm)/stderr

The obvious next question is about the p-values but that's a sore
point so don't ask. :-)



------------------------------

_______________________________________________
R-sig-mixed-models mailing list
R-sig-mixed-models at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


End of R-sig-mixed-models Digest, Vol 15, Issue 23
**************************************************


      _______________________________________
cs.yahoo.com/nowyoucan.html



From Francois-Xavier.Alario at univ-provence.fr  Fri Mar 28 08:28:39 2008
From: Francois-Xavier.Alario at univ-provence.fr (F.-Xavier ALARIO)
Date: Fri, 28 Mar 2008 08:28:39 +0100
Subject: [R-sig-ME] how to locate standard errors in lmer output
In-Reply-To: <40e66e0b0803270918n2d422fe3ta6cd1f7e322911af@mail.gmail.com>
References: <47EB71BD.2020101@univ-provence.fr>
	<40e66e0b0803270918n2d422fe3ta6cd1f7e322911af@mail.gmail.com>
Message-ID: <47EC9E27.9060600@univ-provence.fr>

Perfect, thank you very much for the prompt response.

I use the values to generate graphic representations of the final
version of a given model, so single evaluations

Douglas Bates wrote:
>  Thu, Mar 27, 2008 at 5:06 AM, F.-Xavier ALARIO
> <Francois-Xavier.Alario at univ-provence.fr> wrote:
>   
>>  I have been looking in various documentations (e.g. in
>>  http://rweb.stat.umn.edu/R/library/lme4/doc/) for a way to access
>>  Standard Errors of fixed effects in the output of the 'lmer' function.
>>  slotNames() didn't help.
>>     
>
>   
>>  Is there a way to do it similar to fixef(), ranef() or with a slot name?
>>     
>
> The preferred way would be
>
> stderr <- sqrt(diag(vcov(fm)))
>
> for a single evaluation.  (If you were putting such a calculation
> within a large simulation you may want to work harder to tune it up
> but this is fine for a single evaluation.)
>
>   
>>  I have the same question for t-values.
>>     
>
> fixef(fm)/stderr
>
> The obvious next question is about the p-values but that's a sore
> point so don't ask. :-)
>
>   

-- 
F.-Xavier ALARIO
http://www.up.univ-mrs.fr/wlpc/alario
http://www.lang-prod.org/
+33 4 88 57 69 00



From dwarren1 at cyrus.psych.uiuc.edu  Sat Mar 29 05:39:51 2008
From: dwarren1 at cyrus.psych.uiuc.edu (Dave Warren)
Date: Fri, 28 Mar 2008 23:39:51 -0500
Subject: [R-sig-ME] Regarding mcmcsamp  in the most recent alpha release
Message-ID: <47EDC817.1020707@cyrus.psych.uiuc.edu>

Hi all,

    I've been trying out the most recent alpha release and keep running 
into the following exception when I try using mcmcsamp.

 > path.lmer = lmer( log( PathLength ) ~ Trial * Status + (Trial | 
Patient), data = path_agg )
 > summary( path.lmer )
Linear mixed model fit by REML
Formula: log(PathLength) ~ Trial * Status + (Trial | Patient)
   Data: path_agg
   AIC   BIC logLik deviance REMLdev
 640.4 674.1 -312.2    594.9   624.4
Random effects:
 Groups   Name        Variance   Std.Dev.  Corr  
 Patient  (Intercept) 1.3567e-03 0.0368331       
          Trial       2.1394e-05 0.0046254 -1.000
 Residual             1.9524e-01 0.4418560       
Number of obs: 502, groups: Patient, 10

Fixed effects:
                    Estimate Std. Error t value
(Intercept)         2.995689   0.060844   49.24
Trial              -0.001977   0.002406   -0.82
StatusNormal        0.245692   0.084061    2.92
Trial:StatusNormal  0.008709   0.003382    2.58

Correlation of Fixed Effects:
            (Intr) Trial  SttsNr
Trial       -0.661             
StatusNorml -0.724  0.479      
Trl:SttsNrm  0.470 -0.711 -0.660

 > a = mcmcsamp( path.lmer, 10000 )
Error in .local(object, n, verbose, ...) :
  crossproduct matrix 1 is not positive definite

    Have I broken something?  Is my model revealed to be invalid?  
Thanks for any thoughts,

Dave



From s.blomberg1 at uq.edu.au  Sat Mar 29 14:21:52 2008
From: s.blomberg1 at uq.edu.au (Simon Blomberg)
Date: Sat, 29 Mar 2008 23:21:52 +1000
Subject: [R-sig-ME] Regarding mcmcsamp in the most recent alpha release
References: <47EDC817.1020707@cyrus.psych.uiuc.edu>
Message-ID: <DE3D1F203DAF7A4CB259560D2801DF8B3B2B44@UQEXMB2.soe.uq.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080329/842c24fd/attachment.pl>

From bates at stat.wisc.edu  Sat Mar 29 16:34:29 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 29 Mar 2008 10:34:29 -0500
Subject: [R-sig-ME] Regarding mcmcsamp in the most recent alpha release
In-Reply-To: <DE3D1F203DAF7A4CB259560D2801DF8B3B2B44@UQEXMB2.soe.uq.edu.au>
References: <47EDC817.1020707@cyrus.psych.uiuc.edu>
	<DE3D1F203DAF7A4CB259560D2801DF8B3B2B44@UQEXMB2.soe.uq.edu.au>
Message-ID: <40e66e0b0803290834i6acea840k1ce215626a068c8d@mail.gmail.com>

The first thing to notice is that the estimated correlation of the
random effects is -1.000 which, as Simon indicates, calls into
question the model that you are fitting.  Also, does Trial vary within
Patient?  You can only fit a random Trial:Patient interaction if Trial
varies within Patient.

You only have 10 different Patients so it is unlikely that you will be
able to estimate many variance parameters for the random effects. It
may not be obvious but the model that you have fit is somewhat
complicated.  I would start with simpler models.  First fit a model
with the random effect term (1|Patient).  The next model to attempt to
fit is (1|Patient/Trial), which is what I think that Simon meant to
write.  This provides a random effect for Patient and a random effect
for the Patient:Trial interaction.  It is easier to fit this model
than to fit the other version of a "random interaction" which is
(1+Trial|Patient).

Compare the AIC for those models with the AIC for the model that you
fit.  (To be more confident of the results you should fit all three
models with method = "ML")

That error message can occur even when the model is well-defined.  It
can be caused by the MCMC sampler getting stuck in regions with low,
but very flat, posterior probability. I have seen it on samples like

mcmcsamp(fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy), 10000)

but it is a low probability event for such models and I haven't found
a seed where I can reproduce the problem repeatably.


On Sat, Mar 29, 2008 at 8:21 AM, Simon Blomberg <s.blomberg1 at uq.edu.au> wrote:
> Why do you have a random coefficient for Trial? Perhaps (1|Trial/Patient) might work?

>  -----Original Message-----
>  From: r-sig-mixed-models-bounces at r-project.org on behalf of Dave Warren
>  Sent: Sat 29/03/2008 2:39 PM
>  To: r-sig-mixed-models at r-project.org
>  Subject: [R-sig-ME] Regarding mcmcsamp  in the most recent alpha release
>
>  Hi all,
>
>     I've been trying out the most recent alpha release and keep running
>  into the following exception when I try using mcmcsamp.
>
>   > path.lmer = lmer( log( PathLength ) ~ Trial * Status + (Trial |
>  Patient), data = path_agg )
>   > summary( path.lmer )
>  Linear mixed model fit by REML
>  Formula: log(PathLength) ~ Trial * Status + (Trial | Patient)
>    Data: path_agg
>    AIC   BIC logLik deviance REMLdev
>   640.4 674.1 -312.2    594.9   624.4
>  Random effects:
>   Groups   Name        Variance   Std.Dev.  Corr
>   Patient  (Intercept) 1.3567e-03 0.0368331
>           Trial       2.1394e-05 0.0046254 -1.000
>   Residual             1.9524e-01 0.4418560
>  Number of obs: 502, groups: Patient, 10
>
>  Fixed effects:
>                     Estimate Std. Error t value
>  (Intercept)         2.995689   0.060844   49.24
>  Trial              -0.001977   0.002406   -0.82
>  StatusNormal        0.245692   0.084061    2.92
>  Trial:StatusNormal  0.008709   0.003382    2.58
>
>  Correlation of Fixed Effects:
>             (Intr) Trial  SttsNr
>  Trial       -0.661
>  StatusNorml -0.724  0.479
>  Trl:SttsNrm  0.470 -0.711 -0.660
>
>   > a = mcmcsamp( path.lmer, 10000 )
>  Error in .local(object, n, verbose, ...) :
>   crossproduct matrix 1 is not positive definite
>
>     Have I broken something?  Is my model revealed to be invalid?
>  Thanks for any thoughts,
>
>  Dave
>
>  _______________________________________________
>  R-sig-mixed-models at r-project.org mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>         [[alternative HTML version deleted]]
>
>
>
>  _______________________________________________
>  R-sig-mixed-models at r-project.org mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From marianamartinez at hotmail.co.uk  Sun Mar 30 01:32:03 2008
From: marianamartinez at hotmail.co.uk (Mariana Martinez)
Date: Sun, 30 Mar 2008 00:32:03 +0000
Subject: [R-sig-ME] Comparing lme4 and glm?
Message-ID: <BAY143-W22EDC173CA54D9EE0F1519F9FB0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080330/768e6c4e/attachment.pl>

From john.maindonald at anu.edu.au  Sun Mar 30 03:13:37 2008
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Sun, 30 Mar 2008 12:13:37 +1100
Subject: [R-sig-ME] Comparing lme4 and glm?
In-Reply-To: <BAY143-W22EDC173CA54D9EE0F1519F9FB0@phx.gbl>
References: <BAY143-W22EDC173CA54D9EE0F1519F9FB0@phx.gbl>
Message-ID: <8D3B432F-213E-4725-80CD-36B51855A938@anu.edu.au>

Comments are interspersed.
John.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.


On 30 Mar 2008, at 11:32 AM, Mariana Martinez wrote:

> Dears lme4 and R users: I'm new to lme4 and starting to explore it.  
> I have two questions on regard to model simplification and hope you  
> can help me. I?m modelling the effect of timber harvesting on the  
> number of individuals left in a forest and using two fixed factors  
> and SITE as random factor with poisson family. My questions: 1.- How  
> do I know a random factor is significant or not. I found very small  
> variance and st dev for a random factor (SITE) and based on that I  
> suppose random effect is not significant, but is there any test to  
> determine it. I have seen some papers where chi square and p-values  
> are shown (Svenning et al 2008 Oecologiqa), but do not know how they  
> are obtained.

For reasons given under 2., maybe you do not want such tests.

> 2.- If the random factor is not significant can I remove it from the  
> model? and run a simple glm with poisson family?

If this makes a difference to the inference that you draw, you should  
worry.  I take the view that if such a random component is likely to  
be present, then the model should accommodate it, irrespective of  
whether is it statistically significant.  This is an especially  
important point when the component in question is estimated with a  
rather small number of degrees of freedom, i.e., not much information  
to go on.

If you omit the component then you have to contemplate the alternatives
1) the effect really was present but undetectable
2) the effect was not present, or so small that it could be ignored,  
and my inference is valid.

If (1) has a modest probability, and it matters whether you go with  
(1) or (2), going with (2) leads to a very insecure inference.  Your p- 
value is not, in truth, the one that comes out of the analysis!

Historical data can be useful.  Have you analysed other such data in  
the past?  With enough such samples, you can look at the distribution  
of estimates of the random component.

> 3.- Can I compare with ?anova? the lme4 model against that one from  
> glm? I?ll be very grateful for your help. Mariana

If the random effects all reduce to zero, then you have a glm model.

> _________________________________________________________________
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From alanc at umit.maine.edu  Sat Mar 29 20:52:58 2008
From: alanc at umit.maine.edu (Alan Cobo-Lewis)
Date: Sat, 29 Mar 2008 15:52:58 -0400
Subject: [R-sig-ME] autocorrelation
In-Reply-To: <mailman.3.1206788401.18876.r-sig-mixed-models@r-project.org>
References: <mailman.3.1206788401.18876.r-sig-mixed-models@r-project.org>
Message-ID: <fc.004c4d19360e1b553b9aca0039871ee2.360e22a5@umit.maine.edu>

Doug Bates writes on r-sig-mixed-models at r-project.org on Saturday, March 29, 2008 at 7:00 AM -0500 wrote about his planned book on multilevel modelling in R:

>I emphasize graphical displays of the data and aspects
>of the fitted models and inferences based on MCMC samples from the
>posterior distribution of the model parameters.

(n)lme handled correlated error terms, but lme4 does not. Leaving aside the superior algorithms in lme4, this appears to be the major impediment to considering lme4 capabilities as a superset of (n)lme capabilities.

But what do I do if I've got, for example, autocorrelated error terms? Is there a way to "trick" lme4 into handling that (perhaps something analogous to the "random effect variance per treatment group in lmer" thread that David Afshartous and I
participated in)? Is there instead a good argument for ignoring it? It seems like something that would arise in practice in a non-negligible amount of problems in real data. Will the upcoming book give some advice on how to address this?

I can produce self-contained reproducible code if necessary, but I don't think it is.

thanks
alan



From dwarren1 at cyrus.psych.uiuc.edu  Sat Mar 29 21:15:50 2008
From: dwarren1 at cyrus.psych.uiuc.edu (Dave Warren)
Date: Sat, 29 Mar 2008 15:15:50 -0500
Subject: [R-sig-ME] Regarding mcmcsamp in the most recent alpha release
In-Reply-To: <40e66e0b0803290834i6acea840k1ce215626a068c8d@mail.gmail.com>
References: <47EDC817.1020707@cyrus.psych.uiuc.edu>	
	<DE3D1F203DAF7A4CB259560D2801DF8B3B2B44@UQEXMB2.soe.uq.edu.au>
	<40e66e0b0803290834i6acea840k1ce215626a068c8d@mail.gmail.com>
Message-ID: <47EEA376.9080801@cyrus.psych.uiuc.edu>

Hi Simon, Doug, and list,

    Thanks for the quick responses.  Regarding the dataset, Trial does 
indeed vary within Patient; I've got different numbers of trials 
(observations, really) for each.  The simpler fit you recommended does 
the trick (ML here to improve the AIC estimate as suggested):

RT.lmer = lmer( log( RT ) ~ Trial * Status + (1 | Patient), data = 
rt_agg, method = "ML" )
 > summary( RT.lmer )
Linear mixed model fit by maximum likelihood
Formula: log(RT) ~ Trial * Status + (Trial | Patient)
   Data: rt_agg
    AIC    BIC logLik deviance REMLdev
 -71.24 -37.49  43.62   -87.24  -55.81
Random effects:
 Groups   Name        Variance   Std.Dev.   Corr 
 Patient  (Intercept) 4.0945e-02 0.20234777      
          Trial       4.4598e-07 0.00066781 0.432
 Residual             4.5319e-02 0.21288152      
Number of obs: 502, groups: Patient, 10

Fixed effects:
                     Estimate Std. Error t value
(Intercept)        10.9447820  0.0947941  115.46
Trial              -0.0038163  0.0006622   -5.76
StatusNormal       -0.2812827  0.1337640   -2.10
Trial:StatusNormal  0.0001416  0.0009196    0.15

Correlation of Fixed Effects:
            (Intr) Trial  SttsNr
Trial       -0.046             
StatusNorml -0.709  0.033      
Trl:SttsNrm  0.033 -0.720 -0.036

    Running mcmcsamp on this model yields no exceptions, which is 
terrific.  I should take a moment to praise the default xyplot output 
that mcmcsamp objects now provide.  It's very clear and concise.  
HPDinterval works just fine with that output as well.

    My original model formulation with a random coefficient for Trial 
yields an AIC value of -71.24, slightly greater than the suggested 
model.  I naively modeled a random coefficient for Trial following a 
suggestion of Doug's from earlier this month:
"""
Re: Repeated measures using lme

I'm not sure that those are equivalent specifications.  If I read the
SAS code correctly (and I don't have a lot of experience with SAS) the
equivalent call to lme would have random = ~ 1 | individual

I would be more inclined to start with a model that does have a random
effect for time but does not have the additional correlation
structure.  In lme this would be as in your specification but omitting
the correlation argument.  In lmer it would be

lmer(IL6 ~ dust + time * company + (time | individual), data)
"""
    It's arguable whether my data are truly repeated measures in the 
strictest sense, but they are collected from unique individuals 
performing very similar if not identical tasks at several dozen time 
points ("Trial"s) each.  At any rate, it seemed easy enough to try.  
Thanks very much for the feedback!

Dave

Douglas Bates wrote:
> The first thing to notice is that the estimated correlation of the
> random effects is -1.000 which, as Simon indicates, calls into
> question the model that you are fitting.  Also, does Trial vary within
> Patient?  You can only fit a random Trial:Patient interaction if Trial
> varies within Patient.
>
> You only have 10 different Patients so it is unlikely that you will be
> able to estimate many variance parameters for the random effects. It
> may not be obvious but the model that you have fit is somewhat
> complicated.  I would start with simpler models.  First fit a model
> with the random effect term (1|Patient).  The next model to attempt to
> fit is (1|Patient/Trial), which is what I think that Simon meant to
> write.  This provides a random effect for Patient and a random effect
> for the Patient:Trial interaction.  It is easier to fit this model
> than to fit the other version of a "random interaction" which is
> (1+Trial|Patient).
>
> Compare the AIC for those models with the AIC for the model that you
> fit.  (To be more confident of the results you should fit all three
> models with method = "ML")
>
> That error message can occur even when the model is well-defined.  It
> can be caused by the MCMC sampler getting stuck in regions with low,
> but very flat, posterior probability. I have seen it on samples like
>
> mcmcsamp(fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy), 10000)
>
> but it is a low probability event for such models and I haven't found
> a seed where I can reproduce the problem repeatably.
>
>
> On Sat, Mar 29, 2008 at 8:21 AM, Simon Blomberg <s.blomberg1 at uq.edu.au> wrote:
>   
>> Why do you have a random coefficient for Trial? Perhaps (1|Trial/Patient) might work?
>>     
>
>   
>>  -----Original Message-----
>>  From: r-sig-mixed-models-bounces at r-project.org on behalf of Dave Warren
>>  Sent: Sat 29/03/2008 2:39 PM
>>  To: r-sig-mixed-models at r-project.org
>>  Subject: [R-sig-ME] Regarding mcmcsamp  in the most recent alpha release
>>
>>  Hi all,
>>
>>     I've been trying out the most recent alpha release and keep running
>>  into the following exception when I try using mcmcsamp.
>>
>>   > path.lmer = lmer( log( PathLength ) ~ Trial * Status + (Trial |
>>  Patient), data = path_agg )
>>   > summary( path.lmer )
>>  Linear mixed model fit by REML
>>  Formula: log(PathLength) ~ Trial * Status + (Trial | Patient)
>>    Data: path_agg
>>    AIC   BIC logLik deviance REMLdev
>>   640.4 674.1 -312.2    594.9   624.4
>>  Random effects:
>>   Groups   Name        Variance   Std.Dev.  Corr
>>   Patient  (Intercept) 1.3567e-03 0.0368331
>>           Trial       2.1394e-05 0.0046254 -1.000
>>   Residual             1.9524e-01 0.4418560
>>  Number of obs: 502, groups: Patient, 10
>>
>>  Fixed effects:
>>                     Estimate Std. Error t value
>>  (Intercept)         2.995689   0.060844   49.24
>>  Trial              -0.001977   0.002406   -0.82
>>  StatusNormal        0.245692   0.084061    2.92
>>  Trial:StatusNormal  0.008709   0.003382    2.58
>>
>>  Correlation of Fixed Effects:
>>             (Intr) Trial  SttsNr
>>  Trial       -0.661
>>  StatusNorml -0.724  0.479
>>  Trl:SttsNrm  0.470 -0.711 -0.660
>>
>>   > a = mcmcsamp( path.lmer, 10000 )
>>  Error in .local(object, n, verbose, ...) :
>>   crossproduct matrix 1 is not positive definite
>>
>>     Have I broken something?  Is my model revealed to be invalid?
>>  Thanks for any thoughts,
>>
>>  Dave
>>
>>  _______________________________________________
>>  R-sig-mixed-models at r-project.org mailing list
>>  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>>         [[alternative HTML version deleted]]
>>
>>
>>
>>  _______________________________________________
>>  R-sig-mixed-models at r-project.org mailing list
>>  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>     
>
>



From mwkimpel at gmail.com  Sun Mar 30 19:40:12 2008
From: mwkimpel at gmail.com (Mark Kimpel)
Date: Sun, 30 Mar 2008 13:40:12 -0400
Subject: [R-sig-ME] Question of modeling genomics "high dimensional" data
Message-ID: <6b93d1830803301040x4a591e05p2220983ffbcc1012@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080330/44e223ef/attachment.pl>

From bates at stat.wisc.edu  Sun Mar 30 23:43:32 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 30 Mar 2008 16:43:32 -0500
Subject: [R-sig-ME] autocorrelation
In-Reply-To: <fc.004c4d19360e1b553b9aca0039871ee2.360e22a5@umit.maine.edu>
References: <mailman.3.1206788401.18876.r-sig-mixed-models@r-project.org>
	<fc.004c4d19360e1b553b9aca0039871ee2.360e22a5@umit.maine.edu>
Message-ID: <40e66e0b0803301443s32ecae81p31aed0feeb1682fa@mail.gmail.com>

On Sat, Mar 29, 2008 at 2:52 PM, Alan Cobo-Lewis <alanc at umit.maine.edu> wrote:
> Doug Bates writes on r-sig-mixed-models at r-project.org on Saturday, March 29, 2008 at 7:00 AM -0500 wrote about his planned book on multilevel modelling in R:
>
>  >I emphasize graphical displays of the data and aspects
>  >of the fitted models and inferences based on MCMC samples from the
>  >posterior distribution of the model parameters.
>
>  (n)lme handled correlated error terms, but lme4 does not.

So if you want a model with correlated error terms (in addition to the
correlation induced by the random effects) then you should use the
nlme package.

> Leaving aside the superior algorithms in lme4, this appears to be the major impediment to considering lme4 capabilities as a superset of (n)lme capabilities.

I don't recall any statements to the effect that the lme4 capabilities
would be a superset of the nlme capabilities.  It seems that whoever
made that decision should have informed me of it.

The development of the lme4 package has been generously funded by
several grants, the most important of which was an STTR contract that
we had for 3 years.  The purpose of that contract was to develop a
package that could fit generalized linear mixed models using the
Laplace approximation and allowing for crossed or partially crossed
grouping factors for the random effects.  The development is currently
funded by another grant specifically to provide for fitting models
with crossed and partially crossed random effects and with carryover
of random effects from one time period to another.

Neither generalized linear mixed models nor models with crossed or
partially crossed random effects can be fit (well without resorting to
egregious kludges) with the nlme package.  Even nonlinear mixed models
as fit by nlme are sub-optimal compared to the methods in lme4.  (lme4
uses direct optimization of the Laplace approximation to the
log-likelihood whereas nlme uses an alternating algorithm that Mary
Lindstrom and I proposed.)

My priorities are to fulfill the tasks that I proposed for these
grants and to build the best software that I can.  The beauty of open
source software is that if your priorities are different, you have
full access to the sources and you can modify them to fulfill your
objectives.  So I suggest that you
 - Continue to use the nlme package if you wish to incorporate
(additional) correlation structures in models
 - Design, code, test and document extensions to the lme4 package to
do so and then submit these changes as patches
 - Develop your own package so you can have things done the way you
want them to be done.  You already have access to the lme4 sources so
a lot of the heavy lifting has been done for you.

This developing the software is not as easy as it may seem.  There are
many trade-offs and, at least for me, it takes a lot of effort to
determine even if it is possible to incorporate various extensions
harmoniously.  It is possible to model the mean and variance of the
conditional distribution of the response separately when that
distribution is multivariate normal.  It is not as easy to do so when
that distribution is binomial or Poisson or some other distribution
for a generalized linear mixed model.  Because the first purpose of
the lme4 package was to allow for generalized linear mixed models I
did not incorporate (additional) correlation structures and variance
functions.  I'm not even sure it could be done consistently for GLMMs
but you are welcome to show us how.

>  But what do I do if I've got, for example, autocorrelated error terms? Is there a way to "trick" lme4 into handling that (perhaps something analogous to the "random effect variance per treatment group in lmer" thread that David Afshartous and I
>  participated in)? Is there instead a good argument for ignoring it? It seems like something that would arise in practice in a non-negligible amount of problems in real data. Will the upcoming book give some advice on how to address this?
>
>  I can produce self-contained reproducible code if necessary, but I don't think it is.
>
>  thanks
>  alan
>
>  _______________________________________________
>  R-sig-mixed-models at r-project.org mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From alanc at umit.maine.edu  Mon Mar 31 00:48:08 2008
From: alanc at umit.maine.edu (Alan Cobo-Lewis)
Date: Sun, 30 Mar 2008 18:48:08 -0400
Subject: [R-sig-ME] autocorrelation
In-Reply-To: <40e66e0b0803301443s32ecae81p31aed0feeb1682fa@mail.gmail.com>
References: <mailman.3.1206788401.18876.r-sig-mixed-models@r-project.org> <	>
	<fc.004c4d19360e1b553b9aca0039871ee2.360e22a5@umit.maine.edu>
	<40e66e0b0803301443s32ecae81p31aed0feeb1682fa@mail.gmail.com>
Message-ID: <fc.004c4d19361150393b9aca0039871ee2.36115998@umit.maine.edu>


Doug,

Fair enough.

I hope I didn't offend. I guess I was hoping that there was a reason to neglect autocorrelation, transform it away, or "trick" lme4 into handling it. I couldn't see a solution in any of those three categories, but I figured that posting the query
was the way to find out if I missed something.

I do infer from your response that there is no obvious way to "trick" lme4 into handling autocorrelated error terms without modifying the source. But I'd be happy for someone to demonstrate that I'm incorrect (for some value of "obvious").

Even though the lme4 software is excellent regardless of the funding source, I do appreciate that you shared the background regarding the funding, as puts the design decisions into a very helpful perspective. You didn't have to justify your design
decisions, but I'm glad you did. Thank you.

I'm looking forward to the book. Everyone on this list knows that nlme + Pinheiro & Bates were excellent contributions, and I'm sure that lme4 + the book on multilevel modeling in R will be as well.

thanks
alan



"Douglas Bates" <bates at stat.wisc.edu> on Sunday, March 30, 2008 at 5:43 PM -0500 wrote:
>On Sat, Mar 29, 2008 at 2:52 PM, Alan Cobo-Lewis <alanc at umit.maine.edu> wrote:
>> Doug Bates writes on r-sig-mixed-models at r-project.org on Saturday, March 29, 2008 at 7:00 AM -0500 wrote about his planned book on multilevel modelling in R:
>>
>>  >I emphasize graphical displays of the data and aspects
>>  >of the fitted models and inferences based on MCMC samples from the
>>  >posterior distribution of the model parameters.
>>
>>  (n)lme handled correlated error terms, but lme4 does not.
>
>So if you want a model with correlated error terms (in addition to the
>correlation induced by the random effects) then you should use the
>nlme package.
>
>> Leaving aside the superior algorithms in lme4, this appears to be the major impediment to considering lme4 capabilities as a superset of (n)lme capabilities.
>
>I don't recall any statements to the effect that the lme4 capabilities
>would be a superset of the nlme capabilities.  It seems that whoever
>made that decision should have informed me of it.
>
>The development of the lme4 package has been generously funded by
>several grants, the most important of which was an STTR contract that
>we had for 3 years.  The purpose of that contract was to develop a
>package that could fit generalized linear mixed models using the
>Laplace approximation and allowing for crossed or partially crossed
>grouping factors for the random effects.  The development is currently
>funded by another grant specifically to provide for fitting models
>with crossed and partially crossed random effects and with carryover
>of random effects from one time period to another.
>
>Neither generalized linear mixed models nor models with crossed or
>partially crossed random effects can be fit (well without resorting to
>egregious kludges) with the nlme package.  Even nonlinear mixed models
>as fit by nlme are sub-optimal compared to the methods in lme4.  (lme4
>uses direct optimization of the Laplace approximation to the
>log-likelihood whereas nlme uses an alternating algorithm that Mary
>Lindstrom and I proposed.)
>
>My priorities are to fulfill the tasks that I proposed for these
>grants and to build the best software that I can.  The beauty of open
>source software is that if your priorities are different, you have
>full access to the sources and you can modify them to fulfill your
>objectives.  So I suggest that you
> - Continue to use the nlme package if you wish to incorporate
>(additional) correlation structures in models
> - Design, code, test and document extensions to the lme4 package to
>do so and then submit these changes as patches
> - Develop your own package so you can have things done the way you
>want them to be done.  You already have access to the lme4 sources so
>a lot of the heavy lifting has been done for you.
>
>This developing the software is not as easy as it may seem.  There are
>many trade-offs and, at least for me, it takes a lot of effort to
>determine even if it is possible to incorporate various extensions
>harmoniously.  It is possible to model the mean and variance of the
>conditional distribution of the response separately when that
>distribution is multivariate normal.  It is not as easy to do so when
>that distribution is binomial or Poisson or some other distribution
>for a generalized linear mixed model.  Because the first purpose of
>the lme4 package was to allow for generalized linear mixed models I
>did not incorporate (additional) correlation structures and variance
>functions.  I'm not even sure it could be done consistently for GLMMs
>but you are welcome to show us how.
>
>>  But what do I do if I've got, for example, autocorrelated error terms? Is there a way to "trick" lme4 into handling that (perhaps something analogous to the "random effect variance per treatment group in lmer" thread that David Afshartous and I
>>  participated in)? Is there instead a good argument for ignoring it? It seems like something that would arise in practice in a non-negligible amount of problems in real data. Will the upcoming book give some advice on how to address this?
>>
>>  I can produce self-contained reproducible code if necessary, but I don't think it is.
>>
>>  thanks
>>  alan
>>
>>  _______________________________________________
>>  R-sig-mixed-models at r-project.org mailing list
>>  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>



--
Alan B. Cobo-Lewis, Ph.D.		(207) 581-3840 tel
Department of Psychology		(207) 581-6128 fax
University of Maine
Orono, ME 04469-5742     		alanc at maine.edu

http://www.umaine.edu/visualperception



From mdu at ceh.ac.uk  Mon Mar 31 16:06:31 2008
From: mdu at ceh.ac.uk (Mike Dunbar)
Date: Mon, 31 Mar 2008 15:06:31 +0100
Subject: [R-sig-ME] autocorrelation
Message-ID: <s7f0fe0b.027@wpo.nerc.ac.uk>

Am I missing something here? If you need to estimate an autocorrelation parameter at say lag 1, why not make yourself a new column with your response variable lagged by 1 time unit. Then include that as a fixed effect. Clearly there are issues with missing data, but I'm not aware that nlme does anything more than you can do manually. In fact I wonder if this approach is slightly more flexible, as including a random slope for that lagged variable allows it to vary between groups, and I'm not aware that this is allowed using the in-built structures in nlme. Anything more complex, as I'm continually told, there's always Winbugs...

regards

Mike D



>>> "Douglas Bates" <bates at stat.wisc.edu> 30/03/2008 22:43:32 >>>
On Sat, Mar 29, 2008 at 2:52 PM, Alan Cobo-Lewis <alanc at umit.maine.edu> wrote:
> Doug Bates writes on r-sig-mixed-models at r-project.org on Saturday, March 29, 2008 at 7:00 AM -0500 wrote about his planned book on multilevel modelling in R:
>
>  >I emphasize graphical displays of the data and aspects
>  >of the fitted models and inferences based on MCMC samples from the
>  >posterior distribution of the model parameters.
>
>  (n)lme handled correlated error terms, but lme4 does not.

So if you want a model with correlated error terms (in addition to the
correlation induced by the random effects) then you should use the
nlme package.

> Leaving aside the superior algorithms in lme4, this appears to be the major impediment to considering lme4 capabilities as a superset of (n)lme capabilities.

I don't recall any statements to the effect that the lme4 capabilities
would be a superset of the nlme capabilities.  It seems that whoever
made that decision should have informed me of it.

The development of the lme4 package has been generously funded by
several grants, the most important of which was an STTR contract that
we had for 3 years.  The purpose of that contract was to develop a
package that could fit generalized linear mixed models using the
Laplace approximation and allowing for crossed or partially crossed
grouping factors for the random effects.  The development is currently
funded by another grant specifically to provide for fitting models
with crossed and partially crossed random effects and with carryover
of random effects from one time period to another.

Neither generalized linear mixed models nor models with crossed or
partially crossed random effects can be fit (well without resorting to
egregious kludges) with the nlme package.  Even nonlinear mixed models
as fit by nlme are sub-optimal compared to the methods in lme4.  (lme4
uses direct optimization of the Laplace approximation to the
log-likelihood whereas nlme uses an alternating algorithm that Mary
Lindstrom and I proposed.)

My priorities are to fulfill the tasks that I proposed for these
grants and to build the best software that I can.  The beauty of open
source software is that if your priorities are different, you have
full access to the sources and you can modify them to fulfill your
objectives.  So I suggest that you
 - Continue to use the nlme package if you wish to incorporate
(additional) correlation structures in models
 - Design, code, test and document extensions to the lme4 package to
do so and then submit these changes as patches
 - Develop your own package so you can have things done the way you
want them to be done.  You already have access to the lme4 sources so
a lot of the heavy lifting has been done for you.

This developing the software is not as easy as it may seem.  There are
many trade-offs and, at least for me, it takes a lot of effort to
determine even if it is possible to incorporate various extensions
harmoniously.  It is possible to model the mean and variance of the
conditional distribution of the response separately when that
distribution is multivariate normal.  It is not as easy to do so when
that distribution is binomial or Poisson or some other distribution
for a generalized linear mixed model.  Because the first purpose of
the lme4 package was to allow for generalized linear mixed models I
did not incorporate (additional) correlation structures and variance
functions.  I'm not even sure it could be done consistently for GLMMs
but you are welcome to show us how.

>  But what do I do if I've got, for example, autocorrelated error terms? Is there a way to "trick" lme4 into handling that (perhaps something analogous to the "random effect variance per treatment group in lmer" thread that David Afshartous and I
>  participated in)? Is there instead a good argument for ignoring it? It seems like something that would arise in practice in a non-negligible amount of problems in real data. Will the upcoming book give some advice on how to address this?
>
>  I can produce self-contained reproducible code if necessary, but I don't think it is.
>
>  thanks
>  alan
>
>  _______________________________________________
>  R-sig-mixed-models at r-project.org mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models 
>

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
This message (and any attachments) is for the recipient ...{{dropped:6}}



From gavin.simpson at ucl.ac.uk  Mon Mar 31 16:35:50 2008
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 31 Mar 2008 15:35:50 +0100
Subject: [R-sig-ME] autocorrelation
In-Reply-To: <s7f0fe0b.027@wpo.nerc.ac.uk>
References: <s7f0fe0b.027@wpo.nerc.ac.uk>
Message-ID: <1206974150.15208.83.camel@prometheus.geog.ucl.ac.uk>

[Sorry Mike - not really a reply to your post, but I was unsure where to
jump in with my contribution - hope things are well in CEH-land?]

There was an interesting thread on the main R-Help list, which also
discussed this topic GLMM with some form of correlation in the
residuals.

You can read the thread here:

http://thread.gmane.org/gmane.comp.lang.r.general/109461

At the time I wanted to pipe up and contribute the suggestion that one
could also look at gamm() in package mgcv. I suspect that this would
fail Doug's "egregious kludges" test, but it does allow one to fit
GAMM/GLMM's with the correlation structures and variance functions
familiar to nlme users. It even allows you to use it as a gls()-like
function because of the way the GAM bit is represented in lme - at least
as far as I understand it. You can also specify parametric terms, not
just smooth terms in the model, hence getting GLMM, but I might be
mis-remembering if you need at least 1 smooth term in gamm() here.

At the time I was reading that thread, I wanted to ask about the
approach of gamm() and whether comments made in that thread by Doug
applied more generally (i.e. to gamm() and other "kludges") or
specifically to the lme4 approach to fitting GLMM. But then i got busy
and so left that for a rainy day.

I would very much appreciate any comments either in regard to using
gamm() in the case Alan presents or in regard to the thread I link to
above. As an ecologist this type of thing crops up all the time with our
data, and as such, I found the R-Help thread above very informative
indeed.

All the best,

G

On Mon, 2008-03-31 at 15:06 +0100, Mike Dunbar wrote:
> Am I missing something here? If you need to estimate an
> autocorrelation parameter at say lag 1, why not make yourself a new
> column with your response variable lagged by 1 time unit. Then include
> that as a fixed effect. Clearly there are issues with missing data,
> but I'm not aware that nlme does anything more than you can do
> manually. In fact I wonder if this approach is slightly more flexible,
> as including a random slope for that lagged variable allows it to vary
> between groups, and I'm not aware that this is allowed using the
> in-built structures in nlme. Anything more complex, as I'm continually
> told, there's always Winbugs...
> 
> regards
> 
> Mike D
> 
> 
> 
> >>> "Douglas Bates" <bates at stat.wisc.edu> 30/03/2008 22:43:32 >>>
> On Sat, Mar 29, 2008 at 2:52 PM, Alan Cobo-Lewis <alanc at umit.maine.edu> wrote:
> > Doug Bates writes on r-sig-mixed-models at r-project.org on Saturday, March 29, 2008 at 7:00 AM -0500 wrote about his planned book on multilevel modelling in R:
> >
> >  >I emphasize graphical displays of the data and aspects
> >  >of the fitted models and inferences based on MCMC samples from the
> >  >posterior distribution of the model parameters.
> >
> >  (n)lme handled correlated error terms, but lme4 does not.
> 
> So if you want a model with correlated error terms (in addition to the
> correlation induced by the random effects) then you should use the
> nlme package.
> 
> > Leaving aside the superior algorithms in lme4, this appears to be the major impediment to considering lme4 capabilities as a superset of (n)lme capabilities.
> 
> I don't recall any statements to the effect that the lme4 capabilities
> would be a superset of the nlme capabilities.  It seems that whoever
> made that decision should have informed me of it.
> 
> The development of the lme4 package has been generously funded by
> several grants, the most important of which was an STTR contract that
> we had for 3 years.  The purpose of that contract was to develop a
> package that could fit generalized linear mixed models using the
> Laplace approximation and allowing for crossed or partially crossed
> grouping factors for the random effects.  The development is currently
> funded by another grant specifically to provide for fitting models
> with crossed and partially crossed random effects and with carryover
> of random effects from one time period to another.
> 
> Neither generalized linear mixed models nor models with crossed or
> partially crossed random effects can be fit (well without resorting to
> egregious kludges) with the nlme package.  Even nonlinear mixed models
> as fit by nlme are sub-optimal compared to the methods in lme4.  (lme4
> uses direct optimization of the Laplace approximation to the
> log-likelihood whereas nlme uses an alternating algorithm that Mary
> Lindstrom and I proposed.)
> 
> My priorities are to fulfill the tasks that I proposed for these
> grants and to build the best software that I can.  The beauty of open
> source software is that if your priorities are different, you have
> full access to the sources and you can modify them to fulfill your
> objectives.  So I suggest that you
>  - Continue to use the nlme package if you wish to incorporate
> (additional) correlation structures in models
>  - Design, code, test and document extensions to the lme4 package to
> do so and then submit these changes as patches
>  - Develop your own package so you can have things done the way you
> want them to be done.  You already have access to the lme4 sources so
> a lot of the heavy lifting has been done for you.
> 
> This developing the software is not as easy as it may seem.  There are
> many trade-offs and, at least for me, it takes a lot of effort to
> determine even if it is possible to incorporate various extensions
> harmoniously.  It is possible to model the mean and variance of the
> conditional distribution of the response separately when that
> distribution is multivariate normal.  It is not as easy to do so when
> that distribution is binomial or Poisson or some other distribution
> for a generalized linear mixed model.  Because the first purpose of
> the lme4 package was to allow for generalized linear mixed models I
> did not incorporate (additional) correlation structures and variance
> functions.  I'm not even sure it could be done consistently for GLMMs
> but you are welcome to show us how.
> 
> >  But what do I do if I've got, for example, autocorrelated error terms? Is there a way to "trick" lme4 into handling that (perhaps something analogous to the "random effect variance per treatment group in lmer" thread that David Afshartous and I
> >  participated in)? Is there instead a good argument for ignoring it? It seems like something that would arise in practice in a non-negligible amount of problems in real data. Will the upcoming book give some advice on how to address this?
> >
> >  I can produce self-contained reproducible code if necessary, but I don't think it is.
> >
> >  thanks
> >  alan
> >
> >  _______________________________________________
> >  R-sig-mixed-models at r-project.org mailing list
> >  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models 
> >
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Dr. Gavin Simpson             [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From bates at stat.wisc.edu  Mon Mar 31 17:09:44 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 31 Mar 2008 10:09:44 -0500
Subject: [R-sig-ME] autocorrelation
In-Reply-To: <s7f0fe0b.027@wpo.nerc.ac.uk>
References: <s7f0fe0b.027@wpo.nerc.ac.uk>
Message-ID: <40e66e0b0803310809xbfd6d44i9c3682976cd7e8a5@mail.gmail.com>

On Mon, Mar 31, 2008 at 9:06 AM, Mike Dunbar <mdu at ceh.ac.uk> wrote:
> Am I missing something here? If you need to estimate an autocorrelation parameter at say lag 1, why not make yourself a new column with your response variable lagged by 1 time unit. Then include that as a fixed effect. Clearly there are issues with missing data, but I'm not aware that nlme does anything more than you can do manually. In fact I wonder if this approach is slightly more flexible, as including a random slope for that lagged variable allows it to vary between groups, and I'm not aware that this is allowed using the in-built structures in nlme. Anything more complex, as I'm continually told, there's always Winbugs...

The approach of incorporating a function of the response as a
predictor in a model generally leads you into a quagmire of modeling
the noise term.  The approach in nlme is to "pre-whiten" the response
and the model matrices.  Given a value of the autocorrelation
parameter, the response is transformed by the inverse of the
corresponding correlation matrix and the model matrices are similarly
transformed.  Following that the log-likelihood is evaluated and
optimized with respect to the variance components and the
autocorrelation parameter simultaneously.

>  >>> "Douglas Bates" <bates at stat.wisc.edu> 30/03/2008 22:43:32 >>>
>
>
> On Sat, Mar 29, 2008 at 2:52 PM, Alan Cobo-Lewis <alanc at umit.maine.edu> wrote:
>  > Doug Bates writes on r-sig-mixed-models at r-project.org on Saturday, March 29, 2008 at 7:00 AM -0500 wrote about his planned book on multilevel modelling in R:
>  >
>  >  >I emphasize graphical displays of the data and aspects
>  >  >of the fitted models and inferences based on MCMC samples from the
>  >  >posterior distribution of the model parameters.
>  >
>  >  (n)lme handled correlated error terms, but lme4 does not.
>
>  So if you want a model with correlated error terms (in addition to the
>  correlation induced by the random effects) then you should use the
>  nlme package.
>
>  > Leaving aside the superior algorithms in lme4, this appears to be the major impediment to considering lme4 capabilities as a superset of (n)lme capabilities.
>
>  I don't recall any statements to the effect that the lme4 capabilities
>  would be a superset of the nlme capabilities.  It seems that whoever
>  made that decision should have informed me of it.
>
>  The development of the lme4 package has been generously funded by
>  several grants, the most important of which was an STTR contract that
>  we had for 3 years.  The purpose of that contract was to develop a
>  package that could fit generalized linear mixed models using the
>  Laplace approximation and allowing for crossed or partially crossed
>  grouping factors for the random effects.  The development is currently
>  funded by another grant specifically to provide for fitting models
>  with crossed and partially crossed random effects and with carryover
>  of random effects from one time period to another.
>
>  Neither generalized linear mixed models nor models with crossed or
>  partially crossed random effects can be fit (well without resorting to
>  egregious kludges) with the nlme package.  Even nonlinear mixed models
>  as fit by nlme are sub-optimal compared to the methods in lme4.  (lme4
>  uses direct optimization of the Laplace approximation to the
>  log-likelihood whereas nlme uses an alternating algorithm that Mary
>  Lindstrom and I proposed.)
>
>  My priorities are to fulfill the tasks that I proposed for these
>  grants and to build the best software that I can.  The beauty of open
>  source software is that if your priorities are different, you have
>  full access to the sources and you can modify them to fulfill your
>  objectives.  So I suggest that you
>   - Continue to use the nlme package if you wish to incorporate
>  (additional) correlation structures in models
>   - Design, code, test and document extensions to the lme4 package to
>  do so and then submit these changes as patches
>   - Develop your own package so you can have things done the way you
>  want them to be done.  You already have access to the lme4 sources so
>  a lot of the heavy lifting has been done for you.
>
>  This developing the software is not as easy as it may seem.  There are
>  many trade-offs and, at least for me, it takes a lot of effort to
>  determine even if it is possible to incorporate various extensions
>  harmoniously.  It is possible to model the mean and variance of the
>  conditional distribution of the response separately when that
>  distribution is multivariate normal.  It is not as easy to do so when
>  that distribution is binomial or Poisson or some other distribution
>  for a generalized linear mixed model.  Because the first purpose of
>  the lme4 package was to allow for generalized linear mixed models I
>  did not incorporate (additional) correlation structures and variance
>  functions.  I'm not even sure it could be done consistently for GLMMs
>  but you are welcome to show us how.
>
>  >  But what do I do if I've got, for example, autocorrelated error terms? Is there a way to "trick" lme4 into handling that (perhaps something analogous to the "random effect variance per treatment group in lmer" thread that David Afshartous and I
>  >  participated in)? Is there instead a good argument for ignoring it? It seems like something that would arise in practice in a non-negligible amount of problems in real data. Will the upcoming book give some advice on how to address this?
>  >
>  >  I can produce self-contained reproducible code if necessary, but I don't think it is.
>  >
>  >  thanks
>  >  alan
>  >
>  >  _______________________________________________
>  >  R-sig-mixed-models at r-project.org mailing list
>  >  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>  >
>
>  _______________________________________________
>  R-sig-mixed-models at r-project.org mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>  --
>  This message (and any attachments) is for the recipient ...{{dropped:6}}
>
>
>
>  _______________________________________________
>  R-sig-mixed-models at r-project.org mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Mon Mar 31 23:58:29 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 31 Mar 2008 16:58:29 -0500
Subject: [R-sig-ME] New version of lme4 on R-forge
Message-ID: <40e66e0b0803311458l29cd8555r893acebe2fe68910@mail.gmail.com>

I just committed files for lme4 version 0.999375-11 to the R-forge
source code repository.  If the package builds tonight are successful
then

install.packages("lme4", repos = "http://r-forge.r-project.org/")

should pick up the new version tomorrow.

The major change in this version is formalizing the distinction
between random effects terms and grouping factors for the random
effects.   Variance component parameters, specifically the parameters
represented in the list of matrices in the ST slot, are associated
with random-effects terms in the model formula.  So, using our
favorite examples,

fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)

has one random effects term while

fm2 <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject), sleepstudy)

has two independent random effects terms.  However there is only one
grouping factor, "Subject", in both fm1 and fm2.

In previous versions of the lme4 package the slot fm2 at flist would be a
data frame of two columns, both of which were Subject.  Now it has
just one column with an "assign" attribute that maps terms (in ST) to
factors (in flist).

This allows changing the value of ranef(fm2) to have the same form as
ranef(fm1).  In both cases it is a named list of matrices, one for
each grouping factor.  Indexing the value of ranef() by grouping
factors, not by terms, allows some of the peculiarities of the random
effects in models like fm2 to be eliminated.

To get this committed I needed to disable to effect of "postVar =
TRUE" in the ranef method.  I will reinstate that option after I work
out the details of the code.

There is also a new vignette in this version.  Like many of my
descriptions these Notes.pdf are incomplete but may be of interest to
some who are familiar with a generalized least squares (GLS)
representation of mixed-models (such as used in MLWin and HLM).  The
lme4 package uses a penalized least squares (PLS) representation
instead.  A PLS representation always has a corresponding GLS form
but, especially for models with crossed or partially crossed grouping
factors, the PLS representation is much more consise and easily
managed than the GLS representation.

I wrote these notes to describe to Andrew Robinson how to rewrite a
couple of expressions in the 1997 Biometrics paper by Kenward and
Roger.  This is the paper that introduced some approximate
distributions of some t and F statistics for mixed models that many
would like to see implemented in lme4.  As is typical in such papers,
there several examples included but all of these examples are for data
sets of modest size (not surprisingly - the importance of such
approximations is usually much less important in models fit to large
data sets).  Many people appear perplexed as to why I don't get off my
butt and implement these approximations.  Explanations of laziness
and/or incompetence suggest themselves.  Before opting for these
explanations, however, I would ask you to look at the formulas in that
paper.  The important matrices, written $\bm{\Sigma}$, $\bm{P}_i$ and
$\bm{Q}_{ij}$ (where i and j index the variance component parameters)
are all n by n, symmetric matrices ($n$ is the number of
observations).  I have fit models where $n$ is in the millions.
Working with n by n matrices with n in the millions is, shall we say,
problematic.

You may be able to evaluate expressions involving matrices like these
if you could count on them being very, very sparse.  Unfortunately, if
the grouping factors do not form a nested sequence the matrix
$\bm{\Sigma}$ would not be very sparse.  Even worse, $\bm{P}_i$ and
$\bm{Q}_{ij}$ are defined in terms of $\bm{\Sigma}^{-1}$, which would
likely end up being dense.  If my math is correct it would require
several terabytes of memory to store a dense, symmetric n by n matrix
when n is in the millions and you would apparently need several such
matrices.  It will require a few more iterations of Moore's Law before
we can compute such quantities using the equations in that form.
While we are waiting for computers with several terabytes of memory we
may want to see if those formulas can be rewritten in a more
computable form.

The PLS representation has the advantage relative to GLS of being
based on much smaller matrices.  In particular the Cholesky factor L
is the key matrix for PLS.  L is comparatively small (q by q, where q
is the total number of random effects) and it can be kept sparse, even
for models with crossed grouping factors.



