From logodall at yahoo.fr  Wed Jul  2 17:46:06 2008
From: logodall at yahoo.fr (logodall)
Date: Wed, 2 Jul 2008 15:46:06 +0000 (GMT)
Subject: [R-sig-ME] multiple random effects in lmer and glmmPQL
Message-ID: <454634.50066.qm@web26004.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080702/d140f056/attachment.pl>

From mdu at ceh.ac.uk  Thu Jul  3 10:32:51 2008
From: mdu at ceh.ac.uk (Mike Dunbar)
Date: Thu, 03 Jul 2008 09:32:51 +0100
Subject: [R-sig-ME] multiple random effects in lmer and glmmPQL
Message-ID: <s86c9ce3.068@wpo.nerc.ac.uk>

Dear Pablo

You may be trying to make things too complicated. Are you interested purely in random intercept components or random slopes with respect to particular covariates as well? Providing you are just interested in random intercepts then your model structure then your model would simply be:

lmer(Y~X+W+Z + (1|region/route), method="REML", data=ac, family=poisson)

lmer will correctly deduce from the way that the covariates vary with the groups that X is at a different level from W and Z. If you really are interested in testing for random slopes then it looks like you have quite a complicated model, it is still worth starting with the simpler models to explore the data then you may need to consult with a professional statistican face to face.

regards

Mike


>>> logodall <logodall at yahoo.fr> 02/07/2008 16:46 >>>
I was hoping to obtain some guidance for the specification of a mixed
model in the following analysis that I have been trying to do with
glmmPQL. My problem is that I am unsure on how to specify multiple random effects associated with different
covariates at different spatial scales. 
* Response variable: a integer variable Y that are counts of birds in a
route over time (12 years, one count per year, there might be temporal
autocorrelation, hence my interest in using the library nlme)
* Explanatory variables : three continuous variables measured over
time: X is measured at the route level and W and Z are measured at the
regional level (there are many  >10 routes in each of the 24 regions)
* The goal: to determine the extent to which Y (at the route level) is
determined by X,W and Z, knowing that each  covariate was estimated at
different spatial scales (route, region), and that  each of these
scales are organized in a nested manner (routes within regions)

I have been trying to fit the model with lmer:
lmer(Y~X+W+Z + (X|region/route)+(W|region)+(Z|region), method="REML", data=ac, family=poisson)
and it seems to be doing the right things, though I am not 100% sure
that I am correctly specifying that each explanatory variable is
measured at different spatial resolution. Any words of wisdom would be
appreciated.

However, when passing to glmmPQL (because I need to test for temporal
autocorrelation), I am encountering problems to fit the very same model
(asumming that it is correct). I have tried:
glmmPQL(Y~X+W+Z, random= list(~X|region/route, ~W|region,~Z|region) and I obtain an error message
Error in logLik.reStruct(object, conLin) : 
  NAs in foreign function call (arg 2)
In addition: Warning messages:
1: In ncols * isLast :
  longer object length is not a multiple of shorter object length
2: In ncols * c(rep(1, Q), 0, 0) :
  longer object length is not a multiple of shorter object length
I have also tried : 
glmmPQL(Y~X+W+Z, random= (X|region/route +W|region+Z|region) and though
it gives an answer, I am far from certain to know what it does.

Before posting this message, I have read most of the threads of this
list, searched for help in general forums of R, and looked at the main
textbooks (Pinheiro & Bates and others) without much success. 

To rephrase the question: how to specify the structure of random
effects to specify multiple random effects associated with different
covariates at different spatial scales?
Many thanks for any help/suggestions
Sincerely,
Pablo Inchausti



      _____________________________________________________________________________ 

o.fr
	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
This message (and any attachments) is for the recipient ...{{dropped:6}}



From bates at stat.wisc.edu  Thu Jul  3 14:53:18 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 3 Jul 2008 08:53:18 -0400
Subject: [R-sig-ME] multiple random effects in lmer and glmmPQL
In-Reply-To: <s86c9ce3.068@wpo.nerc.ac.uk>
References: <s86c9ce3.068@wpo.nerc.ac.uk>
Message-ID: <40e66e0b0807030553x54670c3eo382d6ff9d4b41a20@mail.gmail.com>

Just a quick comment that REML is not allowed as an estimation
criterion for generalized linear mixed models.  The only option, and
hence the default, is maximum likelihood estimation of the parameters
based on the Laplace approximation to the log-likelihood.  Bin Dai is
working on incorporating adaptive Gauss-Hermite quadrature (AGQ) as an
alternative approximation in the cases where it makes sense to
evaluate it.

I will take this opportunity to give a pre-vacation message.  I will
be off the net for the next week, relaxing in a beautiful home on a
lake in Canada, sleeping and reading novels.  Do enjoy yourselves
while I am gone.

On Thu, Jul 3, 2008 at 4:32 AM, Mike Dunbar <mdu at ceh.ac.uk> wrote:
> Dear Pablo
>
> You may be trying to make things too complicated. Are you interested purely in random intercept components or random slopes with respect to particular covariates as well? Providing you are just interested in random intercepts then your model structure then your model would simply be:
>
> lmer(Y~X+W+Z + (1|region/route), method="REML", data=ac, family=poisson)
>
> lmer will correctly deduce from the way that the covariates vary with the groups that X is at a different level from W and Z. If you really are interested in testing for random slopes then it looks like you have quite a complicated model, it is still worth starting with the simpler models to explore the data then you may need to consult with a professional statistican face to face.
>
> regards
>
> Mike
>
>
>>>> logodall <logodall at yahoo.fr> 02/07/2008 16:46 >>>
> I was hoping to obtain some guidance for the specification of a mixed
> model in the following analysis that I have been trying to do with
> glmmPQL. My problem is that I am unsure on how to specify multiple random effects associated with different
> covariates at different spatial scales.
> * Response variable: a integer variable Y that are counts of birds in a
> route over time (12 years, one count per year, there might be temporal
> autocorrelation, hence my interest in using the library nlme)
> * Explanatory variables : three continuous variables measured over
> time: X is measured at the route level and W and Z are measured at the
> regional level (there are many  >10 routes in each of the 24 regions)
> * The goal: to determine the extent to which Y (at the route level) is
> determined by X,W and Z, knowing that each  covariate was estimated at
> different spatial scales (route, region), and that  each of these
> scales are organized in a nested manner (routes within regions)
>
> I have been trying to fit the model with lmer:
> lmer(Y~X+W+Z + (X|region/route)+(W|region)+(Z|region), method="REML", data=ac, family=poisson)
> and it seems to be doing the right things, though I am not 100% sure
> that I am correctly specifying that each explanatory variable is
> measured at different spatial resolution. Any words of wisdom would be
> appreciated.
>
> However, when passing to glmmPQL (because I need to test for temporal
> autocorrelation), I am encountering problems to fit the very same model
> (asumming that it is correct). I have tried:
> glmmPQL(Y~X+W+Z, random= list(~X|region/route, ~W|region,~Z|region) and I obtain an error message
> Error in logLik.reStruct(object, conLin) :
>  NAs in foreign function call (arg 2)
> In addition: Warning messages:
> 1: In ncols * isLast :
>  longer object length is not a multiple of shorter object length
> 2: In ncols * c(rep(1, Q), 0, 0) :
>  longer object length is not a multiple of shorter object length
> I have also tried :
> glmmPQL(Y~X+W+Z, random= (X|region/route +W|region+Z|region) and though
> it gives an answer, I am far from certain to know what it does.
>
> Before posting this message, I have read most of the threads of this
> list, searched for help in general forums of R, and looked at the main
> textbooks (Pinheiro & Bates and others) without much success.
>
> To rephrase the question: how to specify the structure of random
> effects to specify multiple random effects associated with different
> covariates at different spatial scales?
> Many thanks for any help/suggestions
> Sincerely,
> Pablo Inchausti
>
>
>
>      _____________________________________________________________________________
>
> o.fr
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
> --
> This message (and any attachments) is for the recipient ...{{dropped:6}}
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From lborger at uoguelph.ca  Thu Jul  3 15:32:49 2008
From: lborger at uoguelph.ca (Luca Borger)
Date: Thu, 3 Jul 2008 09:32:49 -0400
Subject: [R-sig-ME] multiple random effects in lmer and glmmPQL
References: <454634.50066.qm@web26004.mail.ukl.yahoo.com>
Message-ID: <4186EE94FDA2482392C5EDC9F5FC3C23@ZooAnnex2Luca>

Hello,

furthermore, given your goal, it might be informative to compare a model 
with only random intercepts - (X|region/route) - to models with one or more 
of your fixed effects added in (do the fixefs account for some of the 
variance of the ranefs, and at the right level? etc.).

Hope this makes sense.


Cheers,

Luca

---------------------------
Luca B?rger, PhD
Postdoctoral Research Fellow
Department of Integrative Biology
University of Guelph
Guelph, Ontario, Canada N1G 2W1
phone: +1 519 824 4120 ext. 52975
fax:     +1 519 767 1656


----- Original Message ----- 
From: "logodall" <logodall at yahoo.fr>
To: <r-sig-mixed-models at r-project.org>
Sent: Wednesday, July 02, 2008 11:46 AM
Subject: [R-sig-ME] multiple random effects in lmer and glmmPQL


>I was hoping to obtain some guidance for the specification of a mixed
> model in the following analysis that I have been trying to do with
> glmmPQL. My problem is that I am unsure on how to specify multiple random 
> effects associated with different
> covariates at different spatial scales.
> * Response variable: a integer variable Y that are counts of birds in a
> route over time (12 years, one count per year, there might be temporal
> autocorrelation, hence my interest in using the library nlme)
> * Explanatory variables : three continuous variables measured over
> time: X is measured at the route level and W and Z are measured at the
> regional level (there are many  >10 routes in each of the 24 regions)
> * The goal: to determine the extent to which Y (at the route level) is
> determined by X,W and Z, knowing that each  covariate was estimated at
> different spatial scales (route, region), and that  each of these
> scales are organized in a nested manner (routes within regions)
>
> I have been trying to fit the model with lmer:
> lmer(Y~X+W+Z + (X|region/route)+(W|region)+(Z|region), method="REML", 
> data=ac, family=poisson)
> and it seems to be doing the right things, though I am not 100% sure
> that I am correctly specifying that each explanatory variable is
> measured at different spatial resolution. Any words of wisdom would be
> appreciated.
>
> However, when passing to glmmPQL (because I need to test for temporal
> autocorrelation), I am encountering problems to fit the very same model
> (asumming that it is correct). I have tried:
> glmmPQL(Y~X+W+Z, random= list(~X|region/route, ~W|region,~Z|region) and I 
> obtain an error message
> Error in logLik.reStruct(object, conLin) :
>  NAs in foreign function call (arg 2)
> In addition: Warning messages:
> 1: In ncols * isLast :
>  longer object length is not a multiple of shorter object length
> 2: In ncols * c(rep(1, Q), 0, 0) :
>  longer object length is not a multiple of shorter object length
> I have also tried :
> glmmPQL(Y~X+W+Z, random= (X|region/route +W|region+Z|region) and though
> it gives an answer, I am far from certain to know what it does.
>
> Before posting this message, I have read most of the threads of this
> list, searched for help in general forums of R, and looked at the main
> textbooks (Pinheiro & Bates and others) without much success.
>
> To rephrase the question: how to specify the structure of random
> effects to specify multiple random effects associated with different
> covariates at different spatial scales?
> Many thanks for any help/suggestions
> Sincerely,
> Pablo Inchausti
>
>
>
> 
> _____________________________________________________________________________
>
> o.fr
> [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From lborger at uoguelph.ca  Thu Jul  3 16:58:18 2008
From: lborger at uoguelph.ca (Luca Borger)
Date: Thu, 3 Jul 2008 10:58:18 -0400
Subject: [R-sig-ME] multiple random effects in lmer and glmmPQL
References: <454634.50066.qm@web26004.mail.ukl.yahoo.com>
	<4186EE94FDA2482392C5EDC9F5FC3C23@ZooAnnex2Luca>
Message-ID: <A93CA57104DD48AC9ADDA0012E75EF0C@ZooAnnex2Luca>

sorry, I meant to write (1|region/route)


Cheers,

Luca
----- Original Message ----- 
From: "Luca Borger" <lborger at uoguelph.ca>
To: "logodall" <logodall at yahoo.fr>; <r-sig-mixed-models at r-project.org>
Sent: Thursday, July 03, 2008 9:32 AM
Subject: Re: [R-sig-ME] multiple random effects in lmer and glmmPQL


Hello,

furthermore, given your goal, it might be informative to compare a model
with only random intercepts - (X|region/route) - to models with one or more
of your fixed effects added in (do the fixefs account for some of the
variance of the ranefs, and at the right level? etc.).

Hope this makes sense.


Cheers,

Luca

---------------------------
Luca B?rger, PhD
Postdoctoral Research Fellow
Department of Integrative Biology
University of Guelph
Guelph, Ontario, Canada N1G 2W1
phone: +1 519 824 4120 ext. 52975
fax:     +1 519 767 1656


----- Original Message ----- 
From: "logodall" <logodall at yahoo.fr>
To: <r-sig-mixed-models at r-project.org>
Sent: Wednesday, July 02, 2008 11:46 AM
Subject: [R-sig-ME] multiple random effects in lmer and glmmPQL


>I was hoping to obtain some guidance for the specification of a mixed
> model in the following analysis that I have been trying to do with
> glmmPQL. My problem is that I am unsure on how to specify multiple random 
> effects associated with different
> covariates at different spatial scales.
> * Response variable: a integer variable Y that are counts of birds in a
> route over time (12 years, one count per year, there might be temporal
> autocorrelation, hence my interest in using the library nlme)
> * Explanatory variables : three continuous variables measured over
> time: X is measured at the route level and W and Z are measured at the
> regional level (there are many  >10 routes in each of the 24 regions)
> * The goal: to determine the extent to which Y (at the route level) is
> determined by X,W and Z, knowing that each  covariate was estimated at
> different spatial scales (route, region), and that  each of these
> scales are organized in a nested manner (routes within regions)
>
> I have been trying to fit the model with lmer:
> lmer(Y~X+W+Z + (X|region/route)+(W|region)+(Z|region), method="REML", 
> data=ac, family=poisson)
> and it seems to be doing the right things, though I am not 100% sure
> that I am correctly specifying that each explanatory variable is
> measured at different spatial resolution. Any words of wisdom would be
> appreciated.
>
> However, when passing to glmmPQL (because I need to test for temporal
> autocorrelation), I am encountering problems to fit the very same model
> (asumming that it is correct). I have tried:
> glmmPQL(Y~X+W+Z, random= list(~X|region/route, ~W|region,~Z|region) and I 
> obtain an error message
> Error in logLik.reStruct(object, conLin) :
>  NAs in foreign function call (arg 2)
> In addition: Warning messages:
> 1: In ncols * isLast :
>  longer object length is not a multiple of shorter object length
> 2: In ncols * c(rep(1, Q), 0, 0) :
>  longer object length is not a multiple of shorter object length
> I have also tried :
> glmmPQL(Y~X+W+Z, random= (X|region/route +W|region+Z|region) and though
> it gives an answer, I am far from certain to know what it does.
>
> Before posting this message, I have read most of the threads of this
> list, searched for help in general forums of R, and looked at the main
> textbooks (Pinheiro & Bates and others) without much success.
>
> To rephrase the question: how to specify the structure of random
> effects to specify multiple random effects associated with different
> covariates at different spatial scales?
> Many thanks for any help/suggestions
> Sincerely,
> Pablo Inchausti
>
>
>
>
> _____________________________________________________________________________
>
> o.fr
> [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From luis.tedeschi at gmail.com  Thu Jul  3 23:23:56 2008
From: luis.tedeschi at gmail.com (Luis Orlindo Tedeschi)
Date: Thu, 03 Jul 2008 16:23:56 -0500
Subject: [R-sig-ME] Model comparison using BIC, AIC, -2Log
Message-ID: <1215120236.28982.3.camel@localhost.localdomain>

Folks; I have a quick question about model comparison. Is it ok to use
BIC/AIC/-2log to compare models with different fixed and random effects
and even different var-(co)var structure? How can I accomplish this
using R? Will Anova do the correct comparison of different models?
Thanks in advance. Luis

-- 
Luis Orlindo Tedeschi <luis.tedeschi at gmail.com>



From mdu at ceh.ac.uk  Fri Jul  4 10:28:57 2008
From: mdu at ceh.ac.uk (Mike Dunbar)
Date: Fri, 04 Jul 2008 09:28:57 +0100
Subject: [R-sig-ME] Model comparison using BIC, AIC, -2Log
Message-ID: <s86ded67.061@wpo.nerc.ac.uk>

Dear Luis

It is not necessarily straightforward but there is alot of information out there that can help you. Take a look at http://wiki.r-project.org/rwiki/doku.php?id=guides:lmer-tests and also look through the archives of this list, e.g. the thread entitled "[R-sig-ME] interpreting significance from lmer results for dummies (like me)"

regards

Mike


>>> Luis Orlindo Tedeschi <luis.tedeschi at gmail.com> 03/07/2008 22:23 >>>
Folks; I have a quick question about model comparison. Is it ok to use
BIC/AIC/-2log to compare models with different fixed and random effects
and even different var-(co)var structure? How can I accomplish this
using R? Will Anova do the correct comparison of different models?
Thanks in advance. Luis

-- 
Luis Orlindo Tedeschi <luis.tedeschi at gmail.com>

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
This message (and any attachments) is for the recipient ...{{dropped:6}}



From luis.tedeschi at gmail.com  Fri Jul  4 15:23:35 2008
From: luis.tedeschi at gmail.com (Luis Orlindo Tedeschi)
Date: Fri, 04 Jul 2008 08:23:35 -0500
Subject: [R-sig-ME] Model comparison using BIC, AIC, -2Log
In-Reply-To: <s86ded67.060@wpo.nerc.ac.uk>
References: <s86ded67.060@wpo.nerc.ac.uk>
Message-ID: <1215177815.3179.3.camel@localhost.localdomain>

Thanks Mike... and I thought it would have a single answer... I glanced
over the link you provided; it will take me some time to digest it. My
current problem is comparing a model with variable A as random effect vs
a model with variable A as fixed effect. It gets vary confusing. Thanks
again. Luis

On Fri, 2008-07-04 at 09:28 +0100, Mike Dunbar wrote:
> Dear Luis
> 
> It is not necessarily straightforward but there is alot of information out there that can help you. Take a look at http://wiki.r-project.org/rwiki/doku.php?id=guides:lmer-tests and also look through the archives of this list, e.g. the thread entitled "[R-sig-ME] interpreting significance from lmer results for dummies (like me)"
> 
> regards
> 
> Mike
> 
> 
> >>> Luis Orlindo Tedeschi <luis.tedeschi at gmail.com> 03/07/2008 22:23 >>>
> Folks; I have a quick question about model comparison. Is it ok to use
> BIC/AIC/-2log to compare models with different fixed and random effects
> and even different var-(co)var structure? How can I accomplish this
> using R? Will Anova do the correct comparison of different models?
> Thanks in advance. Luis
> 
> -- 
> Luis Orlindo Tedeschi <luis.tedeschi at gmail.com>
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
-- 

??+----------------------------------------------------+
              ?Luis O. Tedeschi, PhD, PAS
                 Assistant Professor
                 Texas A&M University

230 Kleberg Center               p. (+1) 979-845-5065
2471 TAMU                        f. (+1) 979-845-5292
College Station, TX 77843-2471

?http://nutritionmodels.tamu.edu
http://nutr.tamu.edu
http://people.tamu.edu/~luis.tedeschi
+----------------------------------------------------+



From mdu at ceh.ac.uk  Fri Jul  4 15:38:31 2008
From: mdu at ceh.ac.uk (Mike Dunbar)
Date: Fri, 04 Jul 2008 14:38:31 +0100
Subject: [R-sig-ME] Model comparison using BIC, AIC, -2Log
Message-ID: <s86e35eb.033@wpo.nerc.ac.uk>

Hi Luis

I'm not really qualified to comment on this, but I don't think that deciding whether to make an effect fixed or random is a statistical model selection issue. Given that there are issues trying to compare nested models correctly, I can see problems. It's more philosophical, either may be right depending on what you use your model for. Are the levels of A drawn from a larger sample of levels for which you want to make inferences? One issue that may arise is that for factors that are strictly speaking fixed but have alot of levels, it may be easier from a modelling perspective to consider as random. Also you might want to look at some of the papers by Andrew Gelman.

regards

Mike


>>> Luis Orlindo Tedeschi <luis.tedeschi at gmail.com> 04/07/2008 14:23 >>>
Thanks Mike... and I thought it would have a single answer... I glanced
over the link you provided; it will take me some time to digest it. My
current problem is comparing a model with variable A as random effect vs
a model with variable A as fixed effect. It gets vary confusing. Thanks
again. Luis

On Fri, 2008-07-04 at 09:28 +0100, Mike Dunbar wrote:
> Dear Luis
> 
> It is not necessarily straightforward but there is alot of information out there that can help you. Take a look at http://wiki.r-project.org/rwiki/doku.php?id=guides:lmer-tests and also look through the archives of this list, e.g. the thread entitled "[R-sig-ME] interpreting significance from lmer results for dummies (like me)"
> 
> regards
> 
> Mike
> 
> 
> >>> Luis Orlindo Tedeschi <luis.tedeschi at gmail.com> 03/07/2008 22:23 >>>
> Folks; I have a quick question about model comparison. Is it ok to use
> BIC/AIC/-2log to compare models with different fixed and random effects
> and even different var-(co)var structure? How can I accomplish this
> using R? Will Anova do the correct comparison of different models?
> Thanks in advance. Luis
> 
> -- 
> Luis Orlindo Tedeschi <luis.tedeschi at gmail.com>
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models 
> 
> 
-- 

**+----------------------------------------------------+
              *Luis O. Tedeschi, PhD, PAS
                 Assistant Professor
                 Texas A&M University

230 Kleberg Center               p. (+1) 979-845-5065
2471 TAMU                        f. (+1) 979-845-5292
College Station, TX 77843-2471

*http://nutritionmodels.tamu.edu
http://nutr.tamu.edu 
http://people.tamu.edu/~luis.tedeschi 
+----------------------------------------------------+



-- 
This message (and any attachments) is for the recipient ...{{dropped:6}}



From rune.haubo at gmail.com  Fri Jul  4 16:16:17 2008
From: rune.haubo at gmail.com (Rune Haubo)
Date: Fri, 4 Jul 2008 16:16:17 +0200
Subject: [R-sig-ME] Model comparison using BIC, AIC, -2Log
In-Reply-To: <1215177815.3179.3.camel@localhost.localdomain>
References: <s86ded67.060@wpo.nerc.ac.uk>
	<1215177815.3179.3.camel@localhost.localdomain>
Message-ID: <4949c7e60807040716q51ad8d7fycb4aacfeaff77ef1@mail.gmail.com>

Hi Luis

I largely agree with Mike's answer and have the following additional
comments: The decision of whether a variable is taken as fixed or
random often rests on subject specific matter. An important question
is: Can the levels of the variable be considered as coming from a
normal distribution? But other aspects also play a role, such as the
number of realized levels of the variable (with only few levels, it
will often be appropriate to treat the variable as fixed anyhow). The
models rests on different distributional assumptions, so the decision
is often based on weighing the appropriateness of these assumptions.

To give more specific advise on the actual model comparison (ignoring
the question of the appropriateness of the comparison), it matters
whether you are thinking in terms of linear mixed models or
generalized linear mixed models. In the former case assuming you have
only one random effect and assuming lme is sufficient, you can do

fm.lme <- lme(....)
fm.lm <- lm(...)
anova(fm.lme, fm.lm)

If you are thinking in terms of generalized linear mixed models, and
you are using lmer, then maybe you can use something like

deviance(fm.lmer <- lmer(...))
deviance(fm.glm <- glm(...))

however, the reference distribution for the difference in deviance
depends on the actual body of the function calls.

Regards
Rune

2008/7/4 Luis Orlindo Tedeschi <luis.tedeschi at gmail.com>:
> Thanks Mike... and I thought it would have a single answer... I glanced
> over the link you provided; it will take me some time to digest it. My
> current problem is comparing a model with variable A as random effect vs
> a model with variable A as fixed effect. It gets vary confusing. Thanks
> again. Luis
>
> On Fri, 2008-07-04 at 09:28 +0100, Mike Dunbar wrote:
>> Dear Luis
>>
>> It is not necessarily straightforward but there is alot of information out there that can help you. Take a look at http://wiki.r-project.org/rwiki/doku.php?id=guides:lmer-tests and also look through the archives of this list, e.g. the thread entitled "[R-sig-ME] interpreting significance from lmer results for dummies (like me)"
>>
>> regards
>>
>> Mike
>>
>>
>> >>> Luis Orlindo Tedeschi <luis.tedeschi at gmail.com> 03/07/2008 22:23 >>>
>> Folks; I have a quick question about model comparison. Is it ok to use
>> BIC/AIC/-2log to compare models with different fixed and random effects
>> and even different var-(co)var structure? How can I accomplish this
>> using R? Will Anova do the correct comparison of different models?
>> Thanks in advance. Luis
>>
>> --
>> Luis Orlindo Tedeschi <luis.tedeschi at gmail.com>
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
> --
>
> ??+----------------------------------------------------+
>              ?Luis O. Tedeschi, PhD, PAS
>                 Assistant Professor
>                 Texas A&M University
>
> 230 Kleberg Center               p. (+1) 979-845-5065
> 2471 TAMU                        f. (+1) 979-845-5292
> College Station, TX 77843-2471
>
> ?http://nutritionmodels.tamu.edu
> http://nutr.tamu.edu
> http://people.tamu.edu/~luis.tedeschi
> +----------------------------------------------------+
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



-- 
Rune Haubo Bojesen Christensen

Master Student, M.Sc. Eng.
Phone: (+45) 30 26 45 54
Mail: rhbc at imm.dtu.dk, rune.haubo at gmail.com

DTU Informatics, Section for Statistics
Technical University of Denmark, Build.321, DK-2800 Kgs. Lyngby, Denmark

From HStevens at MUOhio.edu  Fri Jul  4 22:32:32 2008
From: HStevens at MUOhio.edu (Hank Stevens)
Date: Fri, 4 Jul 2008 16:32:32 -0400
Subject: [R-sig-ME] hatTrace
Message-ID: <D02C0C5C-E0F9-4FBA-9061-AC8505F0229B@MUOhio.edu>

Hi folks,
What happened to hatTrace?
Hank

 > sessionInfo()
R version 2.7.1 (2008-06-23)
i386-apple-darwin8.10.1

locale:
C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods
[7] base

other attached packages:
[1] foreign_0.8-26     Hmisc_3.4-3        lme4_0.999375-20
[4] Matrix_0.999375-10 lattice_0.17-8

loaded via a namespace (and not attached):
[1] cluster_1.11.11 grid_2.7.1
 >


Dr. Hank Stevens, Associate Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.cas.muohio.edu/ecology
http://www.muohio.edu/botany/

"If the stars should appear one night in a thousand years, how would men
believe and adore." -Ralph Waldo Emerson, writer and philosopher  
(1803-1882)



From john.maindonald at anu.edu.au  Sat Jul  5 01:51:20 2008
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Sat, 5 Jul 2008 09:51:20 +1000
Subject: [R-sig-ME] Random versus fixed effects
In-Reply-To: <4949c7e60807040716q51ad8d7fycb4aacfeaff77ef1@mail.gmail.com>
References: <s86ded67.060@wpo.nerc.ac.uk>
	<1215177815.3179.3.camel@localhost.localdomain>
	<4949c7e60807040716q51ad8d7fycb4aacfeaff77ef1@mail.gmail.com>
Message-ID: <C1A633C0-01F6-4DE9-9578-67933E2E59F0@anu.edu.au>

I have made this sort of comment before, but I think it
important enough to have another go, in a bit more detail.

The extent of generalization
~~~~~~~~~~~~~~~~~~~~~~
Surely the key issue is: "To what population do you wish to
generalize?"  If one wants to generalize to other schools,
then (as in the science data set in DAAG) one must have
data that can be treated as a random sample of schools.

For the science data, it turns out that the schools component
of variance is so small that it can be treated as zero --
differences between classes seems, apart from individual
variation, the only random effect needed.  Moreover degrees
of freedom = 39 for the schools component of variance is
large enough that omission of this ~0 component makes little
difference to the inference.  Thus it may reasonably be omitted,
simplifying the analysis.
[Those who want to avoid talk of degrees of freedom might
go directly to comparison of the two inferences, one with the
schools component of variance, and the other without.
Degrees of freedom are a rough, but often useful, information
measure.]

What if degrees of freedom for the schools component had
been small, and omission of this component did affect the
inference?  Subject area knowledge and experience must
then come into play - is a schools component of variance
likely?, do other studies show evidence of it?, if so what
magnitude?, and so on.

Normality
~~~~~~~
This, while sometimes important, is a second order issue.
The Central Limit Theorem comes to our aid if there is
some modest number of degrees of freedom at the relevant
level.  One can always try transforming the data if it seems
grossly non-normal, at the relevant level of variation.
(Checking this is however non-trivial;  plots of residuals
typically mix in other not-all-that-relevant levels of variation.)

vs fixed effect
~~~~~~~~~~
If the intention is to make statements only about the specific
schools included in the study, then schools may be treated
as fixed effects.  In this case, for the science data set, there
is no detectable difference between schools, and such a
fixed effect can be omitted.

Other reasons for use of fixed effects
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
As has been mentioned, there may be other reasons, other
than the wish to generalize appropriately, for modeling an
effect as random.  If one is comparing 50 varieties of wheat,
the estimates that are at the extremes will likely over-estimate
the relevant effects.  The BLUPs that are calculated from an
analysis that treats the variety effects as random pull the
estimates in towards the mean by amounts that, under often
plausible model assumptions, are appropriate.]

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.


On 5 Jul 2008, at 12:16 AM, Rune Haubo wrote:

> Hi Luis
>
> I largely agree with Mike's answer and have the following additional
> comments: The decision of whether a variable is taken as fixed or
> random often rests on subject specific matter. An important question
> is: Can the levels of the variable be considered as coming from a
> normal distribution? But other aspects also play a role, such as the
> number of realized levels of the variable (with only few levels, it
> will often be appropriate to treat the variable as fixed anyhow). The
> models rests on different distributional assumptions, so the decision
> is often based on weighing the appropriateness of these assumptions.
>
> To give more specific advise on the actual model comparison (ignoring
> the question of the appropriateness of the comparison), it matters
> whether you are thinking in terms of linear mixed models or
> generalized linear mixed models. In the former case assuming you have
> only one random effect and assuming lme is sufficient, you can do
>
> fm.lme <- lme(....)
> fm.lm <- lm(...)
> anova(fm.lme, fm.lm)
>
> If you are thinking in terms of generalized linear mixed models, and
> you are using lmer, then maybe you can use something like
>
> deviance(fm.lmer <- lmer(...))
> deviance(fm.glm <- glm(...))
>
> however, the reference distribution for the difference in deviance
> depends on the actual body of the function calls.
>
> Regards
> Rune
>
> 2008/7/4 Luis Orlindo Tedeschi <luis.tedeschi at gmail.com>:
>> Thanks Mike... and I thought it would have a single answer... I  
>> glanced
>> over the link you provided; it will take me some time to digest it.  
>> My
>> current problem is comparing a model with variable A as random  
>> effect vs
>> a model with variable A as fixed effect. It gets vary confusing.  
>> Thanks
>> again. Luis
>>
>> On Fri, 2008-07-04 at 09:28 +0100, Mike Dunbar wrote:
>>> Dear Luis
>>>
>>> It is not necessarily straightforward but there is alot of  
>>> information out there that can help you. Take a look at http://wiki.r-project.org/rwiki/doku.php?id=guides:lmer-tests 
>>>  and also look through the archives of this list, e.g. the thread  
>>> entitled "[R-sig-ME] interpreting significance from lmer results  
>>> for dummies (like me)"
>>>
>>> regards
>>>
>>> Mike
>>>
>>>
>>>>>> Luis Orlindo Tedeschi <luis.tedeschi at gmail.com> 03/07/2008  
>>>>>> 22:23 >>>
>>> Folks; I have a quick question about model comparison. Is it ok to  
>>> use
>>> BIC/AIC/-2log to compare models with different fixed and random  
>>> effects
>>> and even different var-(co)var structure? How can I accomplish this
>>> using R? Will Anova do the correct comparison of different models?
>>> Thanks in advance. Luis
>>>
>>> --
>>> Luis Orlindo Tedeschi <luis.tedeschi at gmail.com>
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>>
>> --
>>
>> +----------------------------------------------------+
>>            Luis O. Tedeschi, PhD, PAS
>>               Assistant Professor
>>               Texas A&M University
>>
>> 230 Kleberg Center               p. (+1) 979-845-5065
>> 2471 TAMU                        f. (+1) 979-845-5292
>> College Station, TX 77843-2471
>>
>> http://nutritionmodels.tamu.edu
>> http://nutr.tamu.edu
>> http://people.tamu.edu/~luis.tedeschi
>> +----------------------------------------------------+
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>
>
> -- 
> Rune Haubo Bojesen Christensen
>
> Master Student, M.Sc. Eng.
> Phone: (+45) 30 26 45 54
> Mail: rhbc at imm.dtu.dk, rune.haubo at gmail.com
>
> DTU Informatics, Section for Statistics
> Technical University of Denmark, Build.321, DK-2800 Kgs. Lyngby,  
> Denmark
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From maechler at stat.math.ethz.ch  Sat Jul  5 11:40:27 2008
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 5 Jul 2008 11:40:27 +0200
Subject: [R-sig-ME] hatTrace
In-Reply-To: <D02C0C5C-E0F9-4FBA-9061-AC8505F0229B@MUOhio.edu>
References: <D02C0C5C-E0F9-4FBA-9061-AC8505F0229B@MUOhio.edu>
Message-ID: <18543.16779.10472.148767@cmath-5.math.ethz.ch>

>>>>> "HS" == Hank Stevens <HStevens at muohio.edu>
>>>>>     on Fri, 4 Jul 2008 16:32:32 -0400 writes:

    HS> Hi folks,
    HS> What happened to hatTrace?

Well, Doug Bates is currently enjoying a real vacation, so
hopefully will not see this for a few days...

Short answer:  hatTrace() used smart C code that was using the
      "old" parametrization of "mer" objects,
      and it has not yet been implemented for the new 
      (0.999375-* and newer) one.

Longer answer: Wait for Doug's return from vacations.

Martin


    HS> Hank

    >> sessionInfo()
    HS> R version 2.7.1 (2008-06-23)
    HS> i386-apple-darwin8.10.1

    HS> locale:
    HS> C

    HS> attached base packages:
    HS> [1] stats     graphics  grDevices utils     datasets  methods
    HS> [7] base

    HS> other attached packages:
    HS> [1] foreign_0.8-26     Hmisc_3.4-3        lme4_0.999375-20
    HS> [4] Matrix_0.999375-10 lattice_0.17-8

    HS> loaded via a namespace (and not attached):
    HS> [1] cluster_1.11.11 grid_2.7.1
    >> 


    HS> Dr. Hank Stevens, Associate Professor
    HS> 338 Pearson Hall
    HS> Botany Department
    HS> Miami University
    HS> Oxford, OH 45056

    HS> Office: (513) 529-4206
    HS> Lab: (513) 529-4262
    HS> FAX: (513) 529-4243
    HS> http://www.cas.muohio.edu/~stevenmh/
    HS> http://www.cas.muohio.edu/ecology
    HS> http://www.muohio.edu/botany/

    HS> "If the stars should appear one night in a thousand years, how would men
    HS> believe and adore." -Ralph Waldo Emerson, writer and philosopher  
    HS> (1803-1882)

    HS> _______________________________________________
    HS> R-sig-mixed-models at r-project.org mailing list
    HS> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From HStevens at MUOhio.edu  Sun Jul  6 18:54:30 2008
From: HStevens at MUOhio.edu (Hank Stevens)
Date: Sun, 6 Jul 2008 12:54:30 -0400
Subject: [R-sig-ME] F test vs. mcmcpvalue
Message-ID: <6A8E3108-BB3B-4481-97B6-4927FCAA7304@MUOhio.edu>

Hi folks,

Are there general situations in which we might expect very different  
answers from F tests vs. mcmcpvalue with orthogonal contrasts (Helmert)?

I helping someone with a normal linear model with a moderate sized,  
noisy data set, and I am getting very different probabilities between  
F-tests and mcmcpvalue for some interactions.

I get similar F-test results whether I use lm (and ignore the random  
effect of subject), lme, and lmer with an DDF approximation.

When I use mcmcpvalue, I get huge changes in P-value of a main effect  
(0.6 to 0.01) when I remove its interactions. In contrast, the F-test  
(using trace of the hat matrix DF's) are much more consistent when I  
change the fixed effect structure.

I think mcmcpvalue is much more sensitive to overfitting the model. In  
some cases, removing the interactions results in a lower AIC (with ML  
fits).



In the full model, we have 28 fixed coefs (22 continuous variables or  
slope interactions) and about 500 obs.

The data are VERY unbalanced.

sessionInfo()
R version 2.7.1 (2008-06-23)
i386-apple-darwin8.10.1

locale:
C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] foreign_0.8-26     Hmisc_3.4-3        lme4_0.999375-20    
Matrix_0.999375-10
[5] lattice_0.17-8

loaded via a namespace (and not attached):
[1] cluster_1.11.11 grid_2.7.1      tools_2.7.1
 >


Hank



From spencer.graves at pdf.com  Mon Jul  7 00:16:08 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 06 Jul 2008 15:16:08 -0700
Subject: [R-sig-ME] F test vs. mcmcpvalue
In-Reply-To: <6A8E3108-BB3B-4481-97B6-4927FCAA7304@MUOhio.edu>
References: <6A8E3108-BB3B-4481-97B6-4927FCAA7304@MUOhio.edu>
Message-ID: <48714428.9050004@pdf.com>

      I haven't used 'mcmcpvalue', but I would naively expect that MCMC 
should in most cases give better answers in violations of the standard 
assumptions.  This is NOT necessarily the case, however, because it is 
known that MCMC does not always converge to a unique answer.  For 
example, with multimodal posteriors, sufficiently distinct modes may not 
be adequately explored, leading possibly to inappropriate estimates of p 
values.  In such cases, if there are no gross violations of the standard 
assumptions, I would trust the F test more. 

      Have you done normal probability plots and other plots of the 
response variable, residuals, and random coefficients?  This might help 
you identify gross violations of the standard assumptions.  It's 
possible that removing an outlier might substantially reduce the 
difference between your F tests and 'mcmcpvalue'. 
    
      Hope this helps. 
      Spencer

Hank Stevens wrote:
> Hi folks,
>
> Are there general situations in which we might expect very different 
> answers from F tests vs. mcmcpvalue with orthogonal contrasts (Helmert)?
>
> I helping someone with a normal linear model with a moderate sized, 
> noisy data set, and I am getting very different probabilities between 
> F-tests and mcmcpvalue for some interactions.
>
> I get similar F-test results whether I use lm (and ignore the random 
> effect of subject), lme, and lmer with an DDF approximation.
>
> When I use mcmcpvalue, I get huge changes in P-value of a main effect 
> (0.6 to 0.01) when I remove its interactions. In contrast, the F-test 
> (using trace of the hat matrix DF's) are much more consistent when I 
> change the fixed effect structure.
>
> I think mcmcpvalue is much more sensitive to overfitting the model. In 
> some cases, removing the interactions results in a lower AIC (with ML 
> fits).
>
>
>
> In the full model, we have 28 fixed coefs (22 continuous variables or 
> slope interactions) and about 500 obs.
>
> The data are VERY unbalanced.
>
> sessionInfo()
> R version 2.7.1 (2008-06-23)
> i386-apple-darwin8.10.1
>
> locale:
> C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] foreign_0.8-26     Hmisc_3.4-3        lme4_0.999375-20   
> Matrix_0.999375-10
> [5] lattice_0.17-8
>
> loaded via a namespace (and not attached):
> [1] cluster_1.11.11 grid_2.7.1      tools_2.7.1
> >
>
>
> Hank
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From b.glaser at bristol.ac.uk  Mon Jul  7 13:53:05 2008
From: b.glaser at bristol.ac.uk (Beate Glaser)
Date: Mon, 07 Jul 2008 12:53:05 +0100
Subject: [R-sig-ME] power simulations for lmer
Message-ID: <12433718.1215435185@epi-pc32.ads.bris.ac.uk>

Dear lmer users,

I have been reading the mailing list for a while and came across many good 
advice/suggestions/ideas from all the correspondence. I was wondering if I 
could ask your opinion about a DNA methylation project we wanted to set up.

We have DNA collected at different time points (time) and want to determine 
associations between quantitative methylation status (met) and traits 
(height/weight etc). The data will be measured for different loci (locus) 
across the genome. The DNA was collected in different tubes (tube) and 
extracted with different methods (extr). As we are in the planning phase, I 
don't have real data yet, but this is how it will look like:

sub	locus	time 		tube		extr		met	sex
1	1		0		Heparin	Phenol	0.2	1
1	1		40		EDTA		Phenol	0.4	1
1	1		60		EDTA		Phenol	0.6	1
1	1		80		EDTA		SO		0.7	1
1	1		100		CPD		Cells		0.8	1
1	2		0		Heparin	Phenol	0.2	1
1	2		40		EDTA		Phenol	0.4	1
1	2		60		EDTA		Phenol	0.5	1
1	2		80		EDTA		SO		0.4	1
1	2		100		CPD		Cells		0.4	1
1	3		0		Heparin	Phenol	0.2	1
1	3		40		EDTA		Phenol	0.6	1
1	3		60		EDTA		Phenol	0.3	1
1	3		80		EDTA		SO		0.4	1
1	3		100		CPD		Cells		0.5	1
1	4		0		Heparin	Phenol	0.3	1
1	4		40		EDTA		Phenol	0.4	1
1	4		60		EDTA		Phenol	0.4	1
1	4		80		EDTA		SO		0.7	1
1	4		100		CPD		Cells		0.3	1


1) We wanted to run a pilot project without trait analysis across many loci 
(100 - 1000) to assess the DNA handling effects. Analysis would be 
performed with a crossed effect mixed model:

fit <- lmer(met ~ poly(I(time),2)*sex + (1|extr) + (1|tube) + (1|locus) + 
(poly(I(time),2)| subj), data, method="REML")

We hope to see that the correlation within individuals is stronger than the 
one between samples isolated with identical methods, and if not we need to 
account for this in our main experiment. For each factor combination in the 
pilot project we have around 5 individuals, and the only variable we can 
truly influence is the number of met loci we want to analyse. Could anyone 
point me in the right direction of how to set up power simulations to 
determine the number of met loci we would ideally need, in order to assess 
the effect of handling (extr and tube)?

Another problem is that the tube factor is not balanced although it is 
nested within extr; so (1|extr/tube) deemed unreasonable.

2) For our main experiment we truly cannot afford to measure the DNA 
methylation status in all individuals across 1000 loci; We will concentrate 
on a specific locus (locus_of_interest) and determine its methylation 
pattern in many people who have a specific trait.

fit2 <- lmer(met ~ poly(I(time),2)*sex*locus_of_interest*trait + (1|extr) + 
(1|tube) + (1|locus_of_interest) + (poly(I(time),2)| subj), data)

Would anyone know if there is a way to include the more precise random 
effects for (extr and tube) from the pilot experiment into the main model? 
(would weights be a possibility?)

It would be great if you could let me know any 
comments/suggestions/questions/simplifications. This would help quite a lot,
Many thanks,

Beate


----------------------
Beate Glaser
Dept Social Medicine
Canynge Hall
Room 3.5
Whiteladies Road
Bristol BS8 2PR
UK

++44-117-331-3901



From david.paez.1 at ulaval.ca  Mon Jul  7 15:35:17 2008
From: david.paez.1 at ulaval.ca (David Paez)
Date: Mon,  7 Jul 2008 09:35:17 -0400
Subject: [R-sig-ME] (no subject)
Message-ID: <20080707093517.zgo4wckswgokocwg@agora.ulaval.ca>

Hi all,

In the newest version of lme4 the function hatTrace seems to be hidden away so
that R doesn't recognize it anymore.

Does someone know another method that gets to the trace of the hat matrix so
that I can calculate its DF?

cheers in advance

David

--
David James Paez
PhD candidate
Universit? Laval
D?pt. de biologie, Pavillon Vachon
1045, Avenue de la M?decine
Qu?bec, Qc. G1V 0A6
T?l. (418) 656 2681



From yufeng at nsm.umass.edu  Thu Jul  3 15:30:16 2008
From: yufeng at nsm.umass.edu (yufeng at nsm.umass.edu)
Date: Thu, 03 Jul 2008 09:30:16 -0400
Subject: [R-sig-ME] [R] lme4 : lmer : convergence problem and other errors
Message-ID: <1215091816.486cd468ef63c@mail-www.oit.umass.edu>

Dear R-user,

I am trying to use the R "lmer" function in lme4 package to fit a non linear
mixed effects model. The model I wand to fit is at an individual level with 4
parameters. For all parameters both fixed and random effects have to be
estimated, as well as their covariance matrix (see the formula bellow).
y~x1+x1^2+x2+x2^2.


I tried to fit the model with my data sets, but most of the time, R returns an
error message.
there are three main types of errors :

- In mer_finalize(ans, verbose) :
  function evaluation limit reached without convergence (9)

-  there are false convergence (8)

-there are singular convergence (7)

Do you know how to resolve these problems. Is there a way to modify the
parameters of the maximization algorithm to avoid these error messages?

Thank you for your help and answers.

Regards,

Yufeng Zhang



From bolker at zoology.ufl.edu  Mon Jul  7 19:16:07 2008
From: bolker at zoology.ufl.edu (Ben Bolker)
Date: Mon, 07 Jul 2008 13:16:07 -0400
Subject: [R-sig-ME] F test vs. mcmcpvalue
Message-ID: <48724F57.1060800@zoology.ufl.edu>


Hank Stevens wrote:
d> HI Ben and Spencer,
| Thank you very much for your help.
|
| 1. The QQ plots look normal, but highlight the lack of balance (from one
| to dozens of reps per treatment combo).
| 2. The MCMC sample traces look (in my limited experience) without
| peculiarities, and the densityplots are all quite symmetrical and
| normal-ish.
| 3. Simulations (lmer::simulate) of the null hypothesis indicate that
| F-stats as large (or larger) than my observed F-stats are VERY unlikely,
| under the null hypothesis.
|
| As I learn anything else useful, I will be happy to share.
| Cheers,
| Hank
|

~  #3 pretty much seals it for me -- since that is really what
the F test is trying to test.

~  It's a little hard to reconcile #2 and #3, though ... I would
think you could move on at this point, but just for laughs --
are you using mcmcpvalue on a single contrast, or multiple
parameters?  If the former, does it seem to agree with the results of
HPDinterval() or quantile()?  If the latter, is there something
about the _combinations_ of parameters that is wonky?

~  Don't forget my earlier comment about whether what you are testing
(p-values of main effects in the presence of interactions) actually
makes sense ...

~  cheers
~   Ben



From gillian.raab at googlemail.com  Tue Jul  8 09:40:03 2008
From: gillian.raab at googlemail.com (Gillian Raab)
Date: Tue, 8 Jul 2008 08:40:03 +0100
Subject: [R-sig-ME] [R] lme4 : lmer : convergence problem and other
	errors
In-Reply-To: <1215091816.486cd468ef63c@mail-www.oit.umass.edu>
References: <1215091816.486cd468ef63c@mail-www.oit.umass.edu>
Message-ID: <df33d1f90807080040s29bd33f8paf083d6a1799aaab@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080708/9e75e5da/attachment.pl>

From HDoran at air.org  Tue Jul  8 14:12:53 2008
From: HDoran at air.org (Doran, Harold)
Date: Tue, 8 Jul 2008 08:12:53 -0400
Subject: [R-sig-ME] [R] lme4 : lmer : convergence problem and other
	errors
In-Reply-To: <1215091816.486cd468ef63c@mail-www.oit.umass.edu>
Message-ID: <ED7B522EE00C9A4FA515AA71724D61EE4E4B14@DC1EXCL01.air.org>

There is no way to know what is going on given your description below.
You say you're fitting a non-linear model, but your notation is of a
linear model. I see you square a covariate, but this doesn't make it a
non-linear model, it is still linear in the parameters.

We don't know anything about your data, your lmer syntax, all of which
would be useful in helping you out. 

> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org 
> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf 
> Of yufeng at nsm.umass.edu
> Sent: Thursday, July 03, 2008 9:30 AM
> To: R-SIG-Mixed-Models at r-project.org
> Subject: [R-sig-ME] [R] lme4 : lmer : convergence problem and 
> other errors
> 
> Dear R-user,
> 
> I am trying to use the R "lmer" function in lme4 package to 
> fit a non linear mixed effects model. The model I wand to fit 
> is at an individual level with 4 parameters. For all 
> parameters both fixed and random effects have to be 
> estimated, as well as their covariance matrix (see the 
> formula bellow).
> y~x1+x1^2+x2+x2^2.
> 
> 
> I tried to fit the model with my data sets, but most of the 
> time, R returns an error message.
> there are three main types of errors :
> 
> - In mer_finalize(ans, verbose) :
>   function evaluation limit reached without convergence (9)
> 
> -  there are false convergence (8)
> 
> -there are singular convergence (7)
> 
> Do you know how to resolve these problems. Is there a way to 
> modify the parameters of the maximization algorithm to avoid 
> these error messages?
> 
> Thank you for your help and answers.
> 
> Regards,
> 
> Yufeng Zhang
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 



From yufeng at nsm.umass.edu  Tue Jul  8 15:45:55 2008
From: yufeng at nsm.umass.edu (yufeng at nsm.umass.edu)
Date: Tue, 08 Jul 2008 09:45:55 -0400
Subject: [R-sig-ME] [R] lme4 : lmer : convergence problem and other
	errors
In-Reply-To: <df33d1f90807080040s29bd33f8paf083d6a1799aaab@mail.gmail.com>
References: <1215091816.486cd468ef63c@mail-www.oit.umass.edu>
	<df33d1f90807080040s29bd33f8paf083d6a1799aaab@mail.gmail.com>
Message-ID: <1215524755.48736f9313e4d@mail-www.oit.umass.edu>

Please see below:

> 1) What version of lmer are you running? The new version post 23/6/08 copes
> with difficult likelihoods better.

I used the most up-to-date version.

> 2) Have you changed anything in the elements of the control parameter. The
> pre 23/6 verion had several parameters and the later one fewer. In
> particular you can increase the iterations

How could I increase the iterations?

> 3) Have you centred your x variables so they have means around zero. If not
> you should always do this as it will make the fitting easier especially with
> quadratic terms. This ought to have been my first suggestion.

Do u mean the random effects of x's should be centered around 0? I didn't do
that and I don't know how to do that in R? Could U tell me how? Thanks!



> 4) Having another look at your model you say it is non-linear, but it looks
> linear to me if you set the squared terms as covariates too.

You are right the model should be linear.

>
> Good luck
>
> Gillian Raab
> Edinburgh
>
> On 03/07/2008, yufeng at nsm.umass.edu <yufeng at nsm.umass.edu> wrote:
> >
> > Dear R-user,
> >
> > I am trying to use the R "lmer" function in lme4 package to fit a non
> > linear
> > mixed effects model. The model I wand to fit is at an individual level with
> > 4
> > parameters. For all parameters both fixed and random effects have to be
> > estimated, as well as their covariance matrix (see the formula bellow).
> > y~x1+x1^2+x2+x2^2.
> >
> >
> > I tried to fit the model with my data sets, but most of the time, R returns
> > an
> > error message.
> > there are three main types of errors :
> >
> > - In mer_finalize(ans, verbose) :
> >   function evaluation limit reached without convergence (9)
> >
> > -  there are false convergence (8)
> >
> > -there are singular convergence (7)
> >
> > Do you know how to resolve these problems. Is there a way to modify the
> > parameters of the maximization algorithm to avoid these error messages?
> >
> > Thank you for your help and answers.
> >
> > Regards,
> >
> > Yufeng Zhang
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>
>
>
> --
> Gillian M Raab
> 10 Ainslie Place EH3 6AS
> tel 0131 226 6234
> mobile 07748 678 551
>



From gillian.raab at googlemail.com  Tue Jul  8 15:53:20 2008
From: gillian.raab at googlemail.com (Gillian Raab)
Date: Tue, 8 Jul 2008 14:53:20 +0100
Subject: [R-sig-ME] [R] lme4 : lmer : convergence problem and other
	errors
In-Reply-To: <1215524755.48736f9313e4d@mail-www.oit.umass.edu>
References: <1215091816.486cd468ef63c@mail-www.oit.umass.edu>
	<df33d1f90807080040s29bd33f8paf083d6a1799aaab@mail.gmail.com>
	<1215524755.48736f9313e4d@mail-www.oit.umass.edu>
Message-ID: <df33d1f90807080653m1fc38c53oedcf1f85aab05a08@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080708/fc5a9bde/attachment.pl>

From yufeng at nsm.umass.edu  Tue Jul  8 17:47:34 2008
From: yufeng at nsm.umass.edu (yufeng at nsm.umass.edu)
Date: Tue, 08 Jul 2008 11:47:34 -0400
Subject: [R-sig-ME] [R] lme4 : lmer : convergence problem and other
	errors
In-Reply-To: <df33d1f90807080653m1fc38c53oedcf1f85aab05a08@mail.gmail.com>
References: <1215091816.486cd468ef63c@mail-www.oit.umass.edu>
	<df33d1f90807080040s29bd33f8paf083d6a1799aaab@mail.gmail.com>
	<1215524755.48736f9313e4d@mail-www.oit.umass.edu>
	<df33d1f90807080653m1fc38c53oedcf1f85aab05a08@mail.gmail.com>
Message-ID: <1215532054.48738c163e165@mail-www.oit.umass.edu>

Thank for your suggestions but why u want to center x variables around zero in
that way? I just don't understand what is the advantage by doing that.

Yufeng

Quoting Gillian Raab <gillian.raab at googlemail.com>:

> 2008/7/8 <yufeng at nsm.umass.edu>:
>
> > Please see below:
> >
> > > 1) What version of lmer are you running? The new version post 23/6/08
> > copes
> > > with difficult likelihoods better.
> >
> > I used the most up-to-date version.
> >
> > > 2) Have you changed anything in the elements of the control parameter.
> > The
> > > pre 23/6 verion had several parameters and the later one fewer. In
> > > particular you can increase the iterations
> >
> > How could I increase the iterations?
>
> READ THE HELP FILE UNDER LMER AND THE CONTROL  PARAMETER
>
> >
> >
> > > 3) Have you centred your x variables so they have means around zero. If
> > not
> > > you should always do this as it will make the fitting easier especially
> > with
> > > quadratic terms. This ought to have been my first suggestion.
> >
> > Do u mean the random effects of x's should be centered around 0? I didn't
> > do
> > that and I don't know how to do that in R? Could U tell me how? Thanks!
>
> NOTHING FANCY JUST CALCULATE NEW X VARIABLES BY SUBTRACTING THE MEAN VALUES
>
> >
> >
> >
> >
> > > 4) Having another look at your model you say it is non-linear, but it
> > looks
> > > linear to me if you set the squared terms as covariates too.
> >
> > You are right the model should be linear.
> >
> > >
> > > Good luck
> > >
> > > Gillian Raab
> > > Edinburgh
> > >
> > > On 03/07/2008, yufeng at nsm.umass.edu <yufeng at nsm.umass.edu> wrote:
> > > >
> > > > Dear R-user,
> > > >
> > > > I am trying to use the R "lmer" function in lme4 package to fit a non
> > > > linear
> > > > mixed effects model. The model I wand to fit is at an individual level
> > with
> > > > 4
> > > > parameters. For all parameters both fixed and random effects have to be
> > > > estimated, as well as their covariance matrix (see the formula bellow).
> > > > y~x1+x1^2+x2+x2^2.
> > > >
> > > >
> > > > I tried to fit the model with my data sets, but most of the time, R
> > returns
> > > > an
> > > > error message.
> > > > there are three main types of errors :
> > > >
> > > > - In mer_finalize(ans, verbose) :
> > > >   function evaluation limit reached without convergence (9)
> > > >
> > > > -  there are false convergence (8)
> > > >
> > > > -there are singular convergence (7)
> > > >
> > > > Do you know how to resolve these problems. Is there a way to modify the
> > > > parameters of the maximization algorithm to avoid these error messages?
> > > >
> > > > Thank you for your help and answers.
> > > >
> > > > Regards,
> > > >
> > > > Yufeng Zhang
> > > >
> > > > _______________________________________________
> > > > R-sig-mixed-models at r-project.org mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > > >
> > >
> > >
> > >
> > > --
> > > Gillian M Raab
> > > 10 Ainslie Place EH3 6AS
> > > tel 0131 226 6234
> > > mobile 07748 678 551
> > >
> >
> >
> >
> >
>
>
> --
> Gillian M Raab
> 10 Ainslie Place EH3 6AS
> tel 0131 226 6234
> mobile 07748 678 551
>



From gillian.raab at googlemail.com  Tue Jul  8 18:32:11 2008
From: gillian.raab at googlemail.com (Gillian Raab)
Date: Tue, 8 Jul 2008 17:32:11 +0100
Subject: [R-sig-ME] [R] lme4 : lmer : convergence problem and other
	errors
In-Reply-To: <1215532054.48738c163e165@mail-www.oit.umass.edu>
References: <1215091816.486cd468ef63c@mail-www.oit.umass.edu>
	<df33d1f90807080040s29bd33f8paf083d6a1799aaab@mail.gmail.com>
	<1215524755.48736f9313e4d@mail-www.oit.umass.edu>
	<df33d1f90807080653m1fc38c53oedcf1f85aab05a08@mail.gmail.com>
	<1215532054.48738c163e165@mail-www.oit.umass.edu>
Message-ID: <df33d1f90807080932r23a494e3m3d465196c8d3bc43@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080708/6a8b97b9/attachment.pl>

From HStevens at MUOhio.edu  Tue Jul  8 18:33:46 2008
From: HStevens at MUOhio.edu (Hank Stevens)
Date: Tue, 8 Jul 2008 12:33:46 -0400
Subject: [R-sig-ME] New mcmcpvalue?
Message-ID: <D9152988-D0F3-4631-A388-CA63D6EB8C5B@MUOhio.edu>

Hi folks,
It seems that with lme4_0.999375-21, and -20, lmer objects (mer class)  
require a new mcmcpvalue function, because of the values created. This  
(I think) works properly, because it converts the rows of the output  
to columns used by the original mcmcpvalue.

mcmcpvalue <- function(X) {
    ##From Bates pers comm. September 14, 2006 2:59:23 PM EDT
    ## elementary version that creates an empirical p-value for the
    ## hypothesis that the columns of samp have mean zero versus a
    ## general multivariate distribution with elliptical contours.
# samp <- rnorm(10000, m=3)
    ## differences from the mean standardized by the observed
    ## variance-covariance factor

## NEW PART
if(class(X) == "numeric") samp <- as.matrix(X) else{
   samp <- t( X ) }
## END new part

std <- backsolve(chol(var(samp)),
                     cbind(0, t(samp)) - colMeans(samp),
                     transpose = TRUE)
    sqdist <- colSums(std * std)
    sum(sqdist[-1] > sqdist[1])/nrow(samp)
}



Dr. Hank Stevens, Associate Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.cas.muohio.edu/ecology
http://www.muohio.edu/botany/

"If the stars should appear one night in a thousand years, how would men
believe and adore." -Ralph Waldo Emerson, writer and philosopher  
(1803-1882)



From kushler at oakland.edu  Tue Jul  8 19:17:34 2008
From: kushler at oakland.edu (Robert Kushler)
Date: Tue, 08 Jul 2008 13:17:34 -0400
Subject: [R-sig-ME] [R] lme4 : lmer : convergence problem and
	other	errors
In-Reply-To: <df33d1f90807080932r23a494e3m3d465196c8d3bc43@mail.gmail.com>
References: <1215091816.486cd468ef63c@mail-www.oit.umass.edu>	<df33d1f90807080040s29bd33f8paf083d6a1799aaab@mail.gmail.com>	<1215524755.48736f9313e4d@mail-www.oit.umass.edu>	<df33d1f90807080653m1fc38c53oedcf1f85aab05a08@mail.gmail.com>	<1215532054.48738c163e165@mail-www.oit.umass.edu>
	<df33d1f90807080932r23a494e3m3d465196c8d3bc43@mail.gmail.com>
Message-ID: <4873A12E.8060009@oakland.edu>


I'll add 2 cents worth here:  I have long advocated subtracting off a nice
round number near the middle of the range of x values, rather than using the
mean of the x values in the current data set.  This has a number of advantages,
including ease of interpretation and the ability of others to easily reproduce
the results and/or generate predicted values from the fitted model.  The same
argument applies to rescaling data by e.g. shifting the decimal point rather
than "standardizing" x.

Regards,    Rob Kushler


Gillian Raab wrote:
> Very basic statistical principal. It reduces the correlation between the x
> variables especially when you have quadratic terms. Models with correlated
> covariates are ill conditioned and fit badly and are subject to rounding
> errors.
> 
> Those of us old enough to have to have done regressions with calculators
> know this very well but you young folk don't seem to know about it. Give it
> a try and see if it helps.
> 
> Gillian
> 
> 2008/7/8 <yufeng at nsm.umass.edu>:
> 
>> Thank for your suggestions but why u want to center x variables around zero
>> in
>> that way? I just don't understand what is the advantage by doing that.
>>
>> Yufeng
>>
>> Quoting Gillian Raab <gillian.raab at googlemail.com>:
>>
>>> 2008/7/8 <yufeng at nsm.umass.edu>:
>>>
>>>> Please see below:
>>>>
>>>>> 1) What version of lmer are you running? The new version post 23/6/08
>>>> copes
>>>>> with difficult likelihoods better.
>>>> I used the most up-to-date version.
>>>>
>>>>> 2) Have you changed anything in the elements of the control
>> parameter.
>>>> The
>>>>> pre 23/6 verion had several parameters and the later one fewer. In
>>>>> particular you can increase the iterations
>>>> How could I increase the iterations?
>>> READ THE HELP FILE UNDER LMER AND THE CONTROL  PARAMETER
>>>
>>>>
>>>>> 3) Have you centred your x variables so they have means around zero.
>> If
>>>> not
>>>>> you should always do this as it will make the fitting easier
>> especially
>>>> with
>>>>> quadratic terms. This ought to have been my first suggestion.
>>>> Do u mean the random effects of x's should be centered around 0? I
>> didn't
>>>> do
>>>> that and I don't know how to do that in R? Could U tell me how? Thanks!
>>> NOTHING FANCY JUST CALCULATE NEW X VARIABLES BY SUBTRACTING THE MEAN
>> VALUES
>>>>
>>>>
>>>>
>>>>> 4) Having another look at your model you say it is non-linear, but it
>>>> looks
>>>>> linear to me if you set the squared terms as covariates too.
>>>> You are right the model should be linear.
>>>>
>>>>> Good luck
>>>>>
>>>>> Gillian Raab
>>>>> Edinburgh
>>>>>
>>>>> On 03/07/2008, yufeng at nsm.umass.edu <yufeng at nsm.umass.edu> wrote:
>>>>>> Dear R-user,
>>>>>>
>>>>>> I am trying to use the R "lmer" function in lme4 package to fit a
>> non
>>>>>> linear
>>>>>> mixed effects model. The model I wand to fit is at an individual
>> level
>>>> with
>>>>>> 4
>>>>>> parameters. For all parameters both fixed and random effects have
>> to be
>>>>>> estimated, as well as their covariance matrix (see the formula
>> bellow).
>>>>>> y~x1+x1^2+x2+x2^2.
>>>>>>
>>>>>>
>>>>>> I tried to fit the model with my data sets, but most of the time, R
>>>> returns
>>>>>> an
>>>>>> error message.
>>>>>> there are three main types of errors :
>>>>>>
>>>>>> - In mer_finalize(ans, verbose) :
>>>>>>   function evaluation limit reached without convergence (9)
>>>>>>
>>>>>> -  there are false convergence (8)
>>>>>>
>>>>>> -there are singular convergence (7)
>>>>>>
>>>>>> Do you know how to resolve these problems. Is there a way to modify
>> the
>>>>>> parameters of the maximization algorithm to avoid these error
>> messages?
>>>>>> Thank you for your help and answers.
>>>>>>
>>>>>> Regards,
>>>>>>
>>>>>> Yufeng Zhang
>>>>>>
>>>>>> _______________________________________________
>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> Gillian M Raab
>>>>> 10 Ainslie Place EH3 6AS
>>>>> tel 0131 226 6234
>>>>> mobile 07748 678 551
>>>>>
>>>>
>>>>
>>>>
>>>
>>> --
>>> Gillian M Raab
>>> 10 Ainslie Place EH3 6AS
>>> tel 0131 226 6234
>>> mobile 07748 678 551
>>>
>>
>>
>>
> 
>



From gillian.raab at googlemail.com  Tue Jul  8 19:35:54 2008
From: gillian.raab at googlemail.com (Gillian Raab)
Date: Tue, 8 Jul 2008 18:35:54 +0100
Subject: [R-sig-ME] [R] lme4 : lmer : convergence problem and other
	errors
In-Reply-To: <4873A12E.8060009@oakland.edu>
References: <1215091816.486cd468ef63c@mail-www.oit.umass.edu>
	<df33d1f90807080040s29bd33f8paf083d6a1799aaab@mail.gmail.com>
	<1215524755.48736f9313e4d@mail-www.oit.umass.edu>
	<df33d1f90807080653m1fc38c53oedcf1f85aab05a08@mail.gmail.com>
	<1215532054.48738c163e165@mail-www.oit.umass.edu>
	<df33d1f90807080932r23a494e3m3d465196c8d3bc43@mail.gmail.com>
	<4873A12E.8060009@oakland.edu>
Message-ID: <df33d1f90807081035s107e44cet661094c7d4447bfe@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080708/1ad5ff86/attachment.pl>

From kjbeath at kagi.com  Wed Jul  9 13:03:33 2008
From: kjbeath at kagi.com (Ken Beath)
Date: Wed, 9 Jul 2008 21:03:33 +1000
Subject: [R-sig-ME] F test vs. mcmcpvalue
In-Reply-To: <48724F57.1060800@zoology.ufl.edu>
References: <48724F57.1060800@zoology.ufl.edu>
Message-ID: <13B019C0-F96F-43AC-8FC5-B165143DA3BB@kagi.com>

On 08/07/2008, at 3:16 AM, Ben Bolker wrote:

>
> Hank Stevens wrote:
> d> HI Ben and Spencer,
> | Thank you very much for your help.
> |
> | 1. The QQ plots look normal, but highlight the lack of balance  
> (from one
> | to dozens of reps per treatment combo).
> | 2. The MCMC sample traces look (in my limited experience) without
> | peculiarities, and the densityplots are all quite symmetrical and
> | normal-ish.
> | 3. Simulations (lmer::simulate) of the null hypothesis indicate that
> | F-stats as large (or larger) than my observed F-stats are VERY  
> unlikely,
> | under the null hypothesis.
> |
> | As I learn anything else useful, I will be happy to share.
> | Cheers,
> | Hank
> |
>
> ~  #3 pretty much seals it for me -- since that is really what
> the F test is trying to test.
>
> ~  It's a little hard to reconcile #2 and #3, though ... I would
> think you could move on at this point, but just for laughs --
> are you using mcmcpvalue on a single contrast, or multiple
> parameters?  If the former, does it seem to agree with the results of
> HPDinterval() or quantile()?  If the latter, is there something
> about the _combinations_ of parameters that is wonky?
>

If an MCMC isn't traversing the parameter space properly the traces  
will probably still look OK until it shifts into a new region which  
may take a while.

Also it was mentioned that there were 500 observations. For clustered  
data it is the number of clusters that is more important.

Ken



From mark.lyman at atk.com  Wed Jul  9 23:24:30 2008
From: mark.lyman at atk.com (Mark Lyman)
Date: Wed, 9 Jul 2008 21:24:30 +0000 (UTC)
Subject: [R-sig-ME] lme4 0.999375-20 installation error
Message-ID: <loom.20080709T212307-148@post.gmane.org>

I am attempting to install lme4 0.999375-20 on a SUSE 9.1 machine. I have 
already installed Matrix 0.999375-10. However, the lme4 install fails. Do you 
have any suggestions on what might be wrong? My output follows:

* Installing *source* package 'lme4' ...
** libs
gcc -std=gnu99 -I/apps/R/R270/lib64/R/include  -I/usr/local/include    -fpic  -
g -O2 -c init.c -o init.o
init.c:3:20: Matrix.h: No such file or directory
In file included from init.c:4:
Syms.h:3: error: parse error before "lme4_ASym"
Syms.h:3: warning: type defaults to `int' in declaration of `lme4_ASym'
Syms.h:4: warning: type defaults to `int' in declaration of `lme4_CmSym'
Syms.h:5: warning: type defaults to `int' in declaration of `lme4_CxSym'
Syms.h:6: warning: type defaults to `int' in declaration of `lme4_DimSym'
Syms.h:7: warning: type defaults to `int' in declaration of `lme4_GpSym'
Syms.h:8: warning: type defaults to `int' in declaration of `lme4_LSym'
Syms.h:9: warning: type defaults to `int' in declaration of `lme4_RXSym'
Syms.h:10: warning: type defaults to `int' in declaration of `lme4_RZXSym'
Syms.h:11: warning: type defaults to `int' in declaration of `lme4_STSym'
Syms.h:12: warning: type defaults to `int' in declaration of `lme4_VSym'
Syms.h:13: warning: type defaults to `int' in declaration of `lme4_XSym'
Syms.h:14: warning: type defaults to `int' in declaration of `lme4_ZtSym'
Syms.h:15: warning: type defaults to `int' in declaration of `lme4_devianceSym'
Syms.h:16: warning: type defaults to `int' in declaration of `lme4_dimsSym'
Syms.h:17: warning: type defaults to `int' in declaration of `lme4_envSym'
Syms.h:18: warning: type defaults to `int' in declaration of `lme4_etaSym'
Syms.h:19: warning: type defaults to `int' in declaration of `lme4_fixefSym'
Syms.h:20: warning: type defaults to `int' in declaration of `lme4_flistSym'
Syms.h:21: warning: type defaults to `int' in declaration of `lme4_gradientSym'
Syms.h:22: warning: type defaults to `int' in declaration of `lme4_iSym'
Syms.h:23: warning: type defaults to `int' in declaration of `lme4_muEtaSym'
Syms.h:24: warning: type defaults to `int' in declaration of `lme4_muSym'
Syms.h:25: warning: type defaults to `int' in declaration of `lme4_ncSym'
Syms.h:26: warning: type defaults to `int' in declaration of `lme4_nlmodelSym'
Syms.h:27: warning: type defaults to `int' in declaration of `lme4_offsetSym'
Syms.h:28: warning: type defaults to `int' in declaration of `lme4_pSym'
Syms.h:29: warning: type defaults to `int' in declaration of `lme4_pWtSym'
Syms.h:30: warning: type defaults to `int' in declaration of `lme4_permSym'
Syms.h:31: warning: type defaults to `int' in declaration of `lme4_ranefSym'
Syms.h:32: warning: type defaults to `int' in declaration of `lme4_residSym'
Syms.h:33: warning: type defaults to `int' in declaration of `lme4_sigmaSym'
Syms.h:34: warning: type defaults to `int' in declaration of `lme4_sqrtXWtSym'
Syms.h:35: warning: type defaults to `int' in declaration of `lme4_sqrtrWtSym'
Syms.h:36: warning: type defaults to `int' in declaration of `lme4_uSym'
Syms.h:37: warning: type defaults to `int' in declaration of `lme4_varSym'
Syms.h:38: warning: type defaults to `int' in declaration of `lme4_xSym'
Syms.h:39: warning: type defaults to `int' in declaration of `lme4_ySym'
Syms.h:39: warning: data definition has no type or storage class
init.c:37: error: parse error before "c"
init.c:37: warning: type defaults to `int' in declaration of `c'
init.c:37: warning: data definition has no type or storage class
init.c: In function `R_init_lme4':
init.c:54: warning: implicit declaration of function `M_R_cholmod_start'
init.c:55: error: request for member `final_ll' in something not a structure or 
union
init.c:57: warning: assignment makes integer from pointer without a cast
init.c:58: warning: assignment makes integer from pointer without a cast
init.c:59: warning: assignment makes integer from pointer without a cast
init.c:60: warning: assignment makes integer from pointer without a cast
init.c:61: warning: assignment makes integer from pointer without a cast
init.c:62: warning: assignment makes integer from pointer without a cast
init.c:63: warning: assignment makes integer from pointer without a cast
init.c:64: warning: assignment makes integer from pointer without a cast
init.c:65: warning: assignment makes integer from pointer without a cast
init.c:66: warning: assignment makes integer from pointer without a cast
init.c:67: warning: assignment makes integer from pointer without a cast
init.c:68: warning: assignment makes integer from pointer without a cast
init.c:69: warning: assignment makes integer from pointer without a cast
init.c:70: warning: assignment makes integer from pointer without a cast
init.c:71: warning: assignment makes integer from pointer without a cast
init.c:72: warning: assignment makes integer from pointer without a cast
init.c:73: warning: assignment makes integer from pointer without a cast
init.c:74: warning: assignment makes integer from pointer without a cast
init.c:75: warning: assignment makes integer from pointer without a cast
init.c:76: warning: assignment makes integer from pointer without a cast
init.c:77: warning: assignment makes integer from pointer without a cast
init.c:78: warning: assignment makes integer from pointer without a cast
init.c:79: warning: assignment makes integer from pointer without a cast
init.c:80: warning: assignment makes integer from pointer without a cast
init.c:81: warning: assignment makes integer from pointer without a cast
init.c:82: warning: assignment makes integer from pointer without a cast
init.c:83: warning: assignment makes integer from pointer without a cast
init.c:84: warning: assignment makes integer from pointer without a cast
init.c:85: warning: assignment makes integer from pointer without a cast
init.c:86: warning: assignment makes integer from pointer without a cast
init.c:87: warning: assignment makes integer from pointer without a cast
init.c:88: warning: assignment makes integer from pointer without a cast
init.c:89: warning: assignment makes integer from pointer without a cast
init.c:90: warning: assignment makes integer from pointer without a cast
init.c:91: warning: assignment makes integer from pointer without a cast
init.c:92: warning: assignment makes integer from pointer without a cast
init.c:93: warning: assignment makes integer from pointer without a cast
init.c: In function `R_unload_lme4':
init.c:100: warning: implicit declaration of function `M_cholmod_finish'
make: *** [init.o] Error 1
ERROR: compilation failed for package 'lme4'

Mark Lyman, Statistician
ATK Launch Systems



From bates at stat.wisc.edu  Wed Jul  9 23:55:16 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 9 Jul 2008 16:55:16 -0500
Subject: [R-sig-ME] lme4 0.999375-20 installation error
In-Reply-To: <loom.20080709T212307-148@post.gmane.org>
References: <loom.20080709T212307-148@post.gmane.org>
Message-ID: <40e66e0b0807091455o4df4be61i88b72f017fcf91af@mail.gmail.com>

I'm not sure exactly what is happening on the Suse system but there
should be an additional -I clause in the call to gcc.  When I compile
on a Red Hat system the transcript includes

gcc -I/unsup/R-2.7.0/lib/R/include  -I/usr/local/include
-I"/u/b/a/bates/R/i686-pc-linux-gnu-library/2.7/Matrix/include"
-fpic  -Wall -pedantic -g -O3 -std=gnu99 -c init.c -o init.o

It is the last -I clause that provides the location to find Matrix.h,
which is what you are missing here.

On Wed, Jul 9, 2008 at 4:24 PM, Mark Lyman <mark.lyman at atk.com> wrote:
> I am attempting to install lme4 0.999375-20 on a SUSE 9.1 machine. I have
> already installed Matrix 0.999375-10. However, the lme4 install fails. Do you
> have any suggestions on what might be wrong? My output follows:
>
> * Installing *source* package 'lme4' ...
> ** libs
> gcc -std=gnu99 -I/apps/R/R270/lib64/R/include  -I/usr/local/include    -fpic  -
> g -O2 -c init.c -o init.o
> init.c:3:20: Matrix.h: No such file or directory
> In file included from init.c:4:
> Syms.h:3: error: parse error before "lme4_ASym"
> Syms.h:3: warning: type defaults to `int' in declaration of `lme4_ASym'
> Syms.h:4: warning: type defaults to `int' in declaration of `lme4_CmSym'
> Syms.h:5: warning: type defaults to `int' in declaration of `lme4_CxSym'
> Syms.h:6: warning: type defaults to `int' in declaration of `lme4_DimSym'
> Syms.h:7: warning: type defaults to `int' in declaration of `lme4_GpSym'
> Syms.h:8: warning: type defaults to `int' in declaration of `lme4_LSym'
> Syms.h:9: warning: type defaults to `int' in declaration of `lme4_RXSym'
> Syms.h:10: warning: type defaults to `int' in declaration of `lme4_RZXSym'
> Syms.h:11: warning: type defaults to `int' in declaration of `lme4_STSym'
> Syms.h:12: warning: type defaults to `int' in declaration of `lme4_VSym'
> Syms.h:13: warning: type defaults to `int' in declaration of `lme4_XSym'
> Syms.h:14: warning: type defaults to `int' in declaration of `lme4_ZtSym'
> Syms.h:15: warning: type defaults to `int' in declaration of `lme4_devianceSym'
> Syms.h:16: warning: type defaults to `int' in declaration of `lme4_dimsSym'
> Syms.h:17: warning: type defaults to `int' in declaration of `lme4_envSym'
> Syms.h:18: warning: type defaults to `int' in declaration of `lme4_etaSym'
> Syms.h:19: warning: type defaults to `int' in declaration of `lme4_fixefSym'
> Syms.h:20: warning: type defaults to `int' in declaration of `lme4_flistSym'
> Syms.h:21: warning: type defaults to `int' in declaration of `lme4_gradientSym'
> Syms.h:22: warning: type defaults to `int' in declaration of `lme4_iSym'
> Syms.h:23: warning: type defaults to `int' in declaration of `lme4_muEtaSym'
> Syms.h:24: warning: type defaults to `int' in declaration of `lme4_muSym'
> Syms.h:25: warning: type defaults to `int' in declaration of `lme4_ncSym'
> Syms.h:26: warning: type defaults to `int' in declaration of `lme4_nlmodelSym'
> Syms.h:27: warning: type defaults to `int' in declaration of `lme4_offsetSym'
> Syms.h:28: warning: type defaults to `int' in declaration of `lme4_pSym'
> Syms.h:29: warning: type defaults to `int' in declaration of `lme4_pWtSym'
> Syms.h:30: warning: type defaults to `int' in declaration of `lme4_permSym'
> Syms.h:31: warning: type defaults to `int' in declaration of `lme4_ranefSym'
> Syms.h:32: warning: type defaults to `int' in declaration of `lme4_residSym'
> Syms.h:33: warning: type defaults to `int' in declaration of `lme4_sigmaSym'
> Syms.h:34: warning: type defaults to `int' in declaration of `lme4_sqrtXWtSym'
> Syms.h:35: warning: type defaults to `int' in declaration of `lme4_sqrtrWtSym'
> Syms.h:36: warning: type defaults to `int' in declaration of `lme4_uSym'
> Syms.h:37: warning: type defaults to `int' in declaration of `lme4_varSym'
> Syms.h:38: warning: type defaults to `int' in declaration of `lme4_xSym'
> Syms.h:39: warning: type defaults to `int' in declaration of `lme4_ySym'
> Syms.h:39: warning: data definition has no type or storage class
> init.c:37: error: parse error before "c"
> init.c:37: warning: type defaults to `int' in declaration of `c'
> init.c:37: warning: data definition has no type or storage class
> init.c: In function `R_init_lme4':
> init.c:54: warning: implicit declaration of function `M_R_cholmod_start'
> init.c:55: error: request for member `final_ll' in something not a structure or
> union
> init.c:57: warning: assignment makes integer from pointer without a cast
> init.c:58: warning: assignment makes integer from pointer without a cast
> init.c:59: warning: assignment makes integer from pointer without a cast
> init.c:60: warning: assignment makes integer from pointer without a cast
> init.c:61: warning: assignment makes integer from pointer without a cast
> init.c:62: warning: assignment makes integer from pointer without a cast
> init.c:63: warning: assignment makes integer from pointer without a cast
> init.c:64: warning: assignment makes integer from pointer without a cast
> init.c:65: warning: assignment makes integer from pointer without a cast
> init.c:66: warning: assignment makes integer from pointer without a cast
> init.c:67: warning: assignment makes integer from pointer without a cast
> init.c:68: warning: assignment makes integer from pointer without a cast
> init.c:69: warning: assignment makes integer from pointer without a cast
> init.c:70: warning: assignment makes integer from pointer without a cast
> init.c:71: warning: assignment makes integer from pointer without a cast
> init.c:72: warning: assignment makes integer from pointer without a cast
> init.c:73: warning: assignment makes integer from pointer without a cast
> init.c:74: warning: assignment makes integer from pointer without a cast
> init.c:75: warning: assignment makes integer from pointer without a cast
> init.c:76: warning: assignment makes integer from pointer without a cast
> init.c:77: warning: assignment makes integer from pointer without a cast
> init.c:78: warning: assignment makes integer from pointer without a cast
> init.c:79: warning: assignment makes integer from pointer without a cast
> init.c:80: warning: assignment makes integer from pointer without a cast
> init.c:81: warning: assignment makes integer from pointer without a cast
> init.c:82: warning: assignment makes integer from pointer without a cast
> init.c:83: warning: assignment makes integer from pointer without a cast
> init.c:84: warning: assignment makes integer from pointer without a cast
> init.c:85: warning: assignment makes integer from pointer without a cast
> init.c:86: warning: assignment makes integer from pointer without a cast
> init.c:87: warning: assignment makes integer from pointer without a cast
> init.c:88: warning: assignment makes integer from pointer without a cast
> init.c:89: warning: assignment makes integer from pointer without a cast
> init.c:90: warning: assignment makes integer from pointer without a cast
> init.c:91: warning: assignment makes integer from pointer without a cast
> init.c:92: warning: assignment makes integer from pointer without a cast
> init.c:93: warning: assignment makes integer from pointer without a cast
> init.c: In function `R_unload_lme4':
> init.c:100: warning: implicit declaration of function `M_cholmod_finish'
> make: *** [init.o] Error 1
> ERROR: compilation failed for package 'lme4'
>
> Mark Lyman, Statistician
> ATK Launch Systems
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From lamprianou at yahoo.com  Thu Jul 10 12:00:41 2008
From: lamprianou at yahoo.com (Iasonas Lamprianou)
Date: Thu, 10 Jul 2008 03:00:41 -0700 (PDT)
Subject: [R-sig-ME] Rasch models in Greek
Message-ID: <269756.78079.qm@web54104.mail.re2.yahoo.com>

Dear friends and experts,
I have been watching this space for many years - more than a decade. I have benefitted a lot. This time, I need your advice on something important to me - and probably to other people. 
I have been flirting with the idea of publishing an introductory text about Rasch models in the Greek language. The Rasch model may just be a differnt formulation of a mixed effects model. Thats whay I am writing this message here.
No?Greek language Rasch book?is available at the moment. The audience, of course, is small. Instead of focusing on the Educational aspect, I would like to cover examples from all sorts of disciplines (in order to increase the audience and in order to show the flexibility and the usefulness of the model). I'll also offer a windows-based Rasch software for free to accompany the book. The software is very user-friendly, but not as fast and not as rich as Facets, Winsteps, RUMM etc. Still, it is great for beginners because it is only menu-based. The target group will be researchers as well as pre- and post-graduate students of Education, Psychology, Sociology, Medicine, Nursing etc. It will be the first book of its kind. 
However, I am NOT one of the great names in the world of Rasch models (or mixed-effects models). Therefore, I would like this book to be peer-reviewed. In other words, I would politely ask the members of this list to kindly introduce me to Greek-speaking persons/academics who are experienced with Rasch (if any of them attend this list). I would like to negotiate with them to act as reviewers. 
Please forward this message to Greek-speaking Rasch-people you know. 
Also, any of you out there who do not speak Greek but would like to contribute in any other way are welcome.
Thank you for your time
D


      __________________________________________________________
Not happy with your email address?.
Get the one you really want - millions of new email addresses available now at Yahoo! http://uk.docs.yahoo.com/ymail/new.html



From kubovy at virginia.edu  Thu Jul 10 13:51:11 2008
From: kubovy at virginia.edu (Michael Kubovy)
Date: Thu, 10 Jul 2008 07:51:11 -0400
Subject: [R-sig-ME] State of lme4
Message-ID: <344BADEF-1290-47FD-B416-C932A8146E55@virginia.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080710/6117fc06/attachment.pl>

From baron at psych.upenn.edu  Thu Jul 10 14:05:09 2008
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Thu, 10 Jul 2008 08:05:09 -0400
Subject: [R-sig-ME] State of lme4
In-Reply-To: <344BADEF-1290-47FD-B416-C932A8146E55@virginia.edu>
References: <344BADEF-1290-47FD-B416-C932A8146E55@virginia.edu>
Message-ID: <20080710120509.GA914@psych.upenn.edu>

On 07/10/08 07:51, Michael Kubovy wrote:
> Dear Friends,
> 
> I have become confused as to which set-up of lme4, arm, gmodels, etc  
> will produce CIs on the fixed effects by simulation.

In the latest version of lme4, mcmcsamp and HPDinterval work.  And
languageR has a new version as of yesterday, which seems to deal
correctly with the current version of lme4.  Specifically, pvals.fnc()
works.  I don't know about arm and gmodels.  I haven't seen new
versions, so I suspect they will not deal with the new format of lmer.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From HDoran at air.org  Thu Jul 10 14:38:20 2008
From: HDoran at air.org (Doran, Harold)
Date: Thu, 10 Jul 2008 08:38:20 -0400
Subject: [R-sig-ME] State of lme4
In-Reply-To: <20080710120509.GA914@psych.upenn.edu>
Message-ID: <ED7B522EE00C9A4FA515AA71724D61EE4E4C1F@DC1EXCL01.air.org>

Out of curiousity, why not just use the asymptotic standards errors of
the fixed effects to get Cis rather than via simulation? 

> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org 
> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf 
> Of Jonathan Baron
> Sent: Thursday, July 10, 2008 8:05 AM
> To: Michael Kubovy
> Cc: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] State of lme4
> 
> On 07/10/08 07:51, Michael Kubovy wrote:
> > Dear Friends,
> > 
> > I have become confused as to which set-up of lme4, arm, 
> gmodels, etc 
> > will produce CIs on the fixed effects by simulation.
> 
> In the latest version of lme4, mcmcsamp and HPDinterval work. 
>  And languageR has a new version as of yesterday, which seems 
> to deal correctly with the current version of lme4.  
> Specifically, pvals.fnc() works.  I don't know about arm and 
> gmodels.  I haven't seen new versions, so I suspect they will 
> not deal with the new format of lmer.
> 
> Jon
> --
> Jonathan Baron, Professor of Psychology, University of 
> Pennsylvania Home page: http://www.sas.upenn.edu/~baron
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 



From gillian.raab at googlemail.com  Thu Jul 10 15:29:19 2008
From: gillian.raab at googlemail.com (Gillian Raab)
Date: Thu, 10 Jul 2008 14:29:19 +0100
Subject: [R-sig-ME] State of lme4
In-Reply-To: <ED7B522EE00C9A4FA515AA71724D61EE4E4C1F@DC1EXCL01.air.org>
References: <20080710120509.GA914@psych.upenn.edu>
	<ED7B522EE00C9A4FA515AA71724D61EE4E4C1F@DC1EXCL01.air.org>
Message-ID: <df33d1f90807100629x31626515p713787737ee72e2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080710/295066ca/attachment.pl>

From bates at stat.wisc.edu  Thu Jul 10 16:19:07 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 10 Jul 2008 10:19:07 -0400
Subject: [R-sig-ME] State of the lme4 package
Message-ID: <40e66e0b0807100719r5792fd77wc4bd9229048d8173@mail.gmail.com>

There have been questions about the new release of lme4 and how it
relates to other packages such as languageR, arm and gmodels.

The version numbers that Martin and I use for the Matrix and lme4
packages are getting ever closer to 1.0, although those who examine
the sequence will see that it will never get there.  We hope that we
will break the sequence and actually hit 1.0 in the not too distant
future.  Matrix will hit 1.0 first.  Most of the development on that
package is being done by Martin and I think he just wants to tidy up a
few tests, etc. then release the 1.0 version.  The plan is that Matrix
will become a recommended package, perhaps as early as R-2.8.0

There are parts of the lme4 package that you can regard as being
stable.  The underlying representation of mixed-effects models (the
"mer" class) is stable.  This representation encompasses linear mixed
models, generalized linear mixed models, nonlinear mixed models and
generalized nonlinear mixed models.    The functions for fitting
linear mixed models and generalized linear mixed models also can be
regarded as stable.  I plan on adding one more argument to those
functions but it will not change the effect of current calls.
Nonlinear mixed models are not yet stable.  At present the deviance is
incorrect.  I think I know what the problem is but will need to check
whether the fix that I have in mind works.  Bin Dai is working on
adding adaptive Gauss-Hermite quadrature for GLMMs and NLMMs.  The
form of the argument to glmer and nlmer to use AGQ instead of the
Laplace approximation is now set - it should simply be a matter of
activating those switches.

The implementation of mcmcsamp is incomplete.  Currently the
implementation only allows linear mixed models with scalar random
effects.  Even that part of the implementation has, I suspect, some
"infelicities".  The chains produced for some models have peculiar
properties.  I hope the infelicities are in the implementation and not
in the theory.  Of course, it will be easier to check when I actually
write down the theory.

As has been mentioned, hatTrace is not currently active. It is the
age-old problem -- it could be implemented fairly easily in a form
that would work well for simpler models fit to small to medium-sized
data sets but that implementation would blow up when applied to
complex models fit to large data sets.  It will take some thought to
be able to create an implementation that works well on complex models
and large data sets.

So the good news is that lme4 has a stable foundation.  The bad news
is that most of the functions related to inferences for fixed-effects
parameters in linear mixed models (i.e. mcmcsamp, hatTrace,
Kenward-Roger approximation to degrees of freedom and multiplier
factors) are not yet stable.



From Mark.Lyman at atk.com  Thu Jul 10 15:43:04 2008
From: Mark.Lyman at atk.com (Lyman, Mark)
Date: Thu, 10 Jul 2008 07:43:04 -0600
Subject: [R-sig-ME] lme4 0.999375-20 installation error
In-Reply-To: <40e66e0b0807091455o4df4be61i88b72f017fcf91af@mail.gmail.com>
References: <loom.20080709T212307-148@post.gmane.org>
	<40e66e0b0807091455o4df4be61i88b72f017fcf91af@mail.gmail.com>
Message-ID: <A6BB278845329C41A08CB3C52A0E2C109D91A1@ut40se02.atk.com>

On previous post I had tried to install with R CMD INSTALL and failed.
This morning I installed using install.packages and everything worked
fine for some reason. Thanks for your time.

> sessionInfo()
R version 2.7.0 (2008-04-22) 
x86_64-unknown-linux-gnu 

locale:
C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

> install.packages("lme4_0.999375-20.tar.gz", repos=NULL)
Warning in install.packages("lme4_0.999375-20.tar.gz", repos = NULL) :
  argument 'lib' is missing: using '/home/e38302/r_library'
* Installing *source* package 'lme4' ...
** libs
gcc -std=gnu99 -I/apps/R/R270/lib64/R/include  -I/usr/local/include
-I"/home/lyman/r_library/Matrix/include"   -fpic  -g -O2 -c init.c -o
init.o
gcc -std=gnu99 -I/apps/R/R270/lib64/R/include  -I/usr/local/include
-I"/home/lyman/r_library/Matrix/include"   -fpic  -g -O2 -c lmer.c -o
lmer.o
gcc -std=gnu99 -I/apps/R/R270/lib64/R/include  -I/usr/local/include
-I"/home/lyman/r_library/Matrix/include"   -fpic  -g -O2 -c
local_stubs.c -o local_stubs.o
gcc -std=gnu99 -shared -L/usr/local/lib64 -o lme4.so init.o lmer.o
local_stubs.o -L/apps/R/R270/lib64/R/lib -lRlapack
-L/apps/R/R270/lib64/R/lib -lRblas -lg2c -lm  

Mark Lyman, Statistician
ATK Launch Systems
mark.lyman at atk.com
(435) 863-2863

-----Original Message-----
From: dmbates at gmail.com [mailto:dmbates at gmail.com] On Behalf Of Douglas
Bates
Sent: Wednesday, July 09, 2008 3:55 PM
To: Lyman, Mark
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] lme4 0.999375-20 installation error

I'm not sure exactly what is happening on the Suse system but there
should be an additional -I clause in the call to gcc.  When I compile
on a Red Hat system the transcript includes

gcc -I/unsup/R-2.7.0/lib/R/include  -I/usr/local/include
-I"/u/b/a/bates/R/i686-pc-linux-gnu-library/2.7/Matrix/include"
-fpic  -Wall -pedantic -g -O3 -std=gnu99 -c init.c -o init.o

It is the last -I clause that provides the location to find Matrix.h,
which is what you are missing here.

On Wed, Jul 9, 2008 at 4:24 PM, Mark Lyman <mark.lyman at atk.com> wrote:
> I am attempting to install lme4 0.999375-20 on a SUSE 9.1 machine. I
have
> already installed Matrix 0.999375-10. However, the lme4 install fails.
Do you
> have any suggestions on what might be wrong? My output follows:
>
> * Installing *source* package 'lme4' ...
> ** libs
> gcc -std=gnu99 -I/apps/R/R270/lib64/R/include  -I/usr/local/include
-fpic  -
> g -O2 -c init.c -o init.o
> init.c:3:20: Matrix.h: No such file or directory
> In file included from init.c:4:
> Syms.h:3: error: parse error before "lme4_ASym"
> Syms.h:3: warning: type defaults to `int' in declaration of
`lme4_ASym'
> Syms.h:4: warning: type defaults to `int' in declaration of
`lme4_CmSym'
> Syms.h:5: warning: type defaults to `int' in declaration of
`lme4_CxSym'
> Syms.h:6: warning: type defaults to `int' in declaration of
`lme4_DimSym'
> Syms.h:7: warning: type defaults to `int' in declaration of
`lme4_GpSym'
> Syms.h:8: warning: type defaults to `int' in declaration of
`lme4_LSym'
> Syms.h:9: warning: type defaults to `int' in declaration of
`lme4_RXSym'
> Syms.h:10: warning: type defaults to `int' in declaration of
`lme4_RZXSym'
> Syms.h:11: warning: type defaults to `int' in declaration of
`lme4_STSym'
> Syms.h:12: warning: type defaults to `int' in declaration of
`lme4_VSym'
> Syms.h:13: warning: type defaults to `int' in declaration of
`lme4_XSym'
> Syms.h:14: warning: type defaults to `int' in declaration of
`lme4_ZtSym'
> Syms.h:15: warning: type defaults to `int' in declaration of
`lme4_devianceSym'
> Syms.h:16: warning: type defaults to `int' in declaration of
`lme4_dimsSym'
> Syms.h:17: warning: type defaults to `int' in declaration of
`lme4_envSym'
> Syms.h:18: warning: type defaults to `int' in declaration of
`lme4_etaSym'
> Syms.h:19: warning: type defaults to `int' in declaration of
`lme4_fixefSym'
> Syms.h:20: warning: type defaults to `int' in declaration of
`lme4_flistSym'
> Syms.h:21: warning: type defaults to `int' in declaration of
`lme4_gradientSym'
> Syms.h:22: warning: type defaults to `int' in declaration of
`lme4_iSym'
> Syms.h:23: warning: type defaults to `int' in declaration of
`lme4_muEtaSym'
> Syms.h:24: warning: type defaults to `int' in declaration of
`lme4_muSym'
> Syms.h:25: warning: type defaults to `int' in declaration of
`lme4_ncSym'
> Syms.h:26: warning: type defaults to `int' in declaration of
`lme4_nlmodelSym'
> Syms.h:27: warning: type defaults to `int' in declaration of
`lme4_offsetSym'
> Syms.h:28: warning: type defaults to `int' in declaration of
`lme4_pSym'
> Syms.h:29: warning: type defaults to `int' in declaration of
`lme4_pWtSym'
> Syms.h:30: warning: type defaults to `int' in declaration of
`lme4_permSym'
> Syms.h:31: warning: type defaults to `int' in declaration of
`lme4_ranefSym'
> Syms.h:32: warning: type defaults to `int' in declaration of
`lme4_residSym'
> Syms.h:33: warning: type defaults to `int' in declaration of
`lme4_sigmaSym'
> Syms.h:34: warning: type defaults to `int' in declaration of
`lme4_sqrtXWtSym'
> Syms.h:35: warning: type defaults to `int' in declaration of
`lme4_sqrtrWtSym'
> Syms.h:36: warning: type defaults to `int' in declaration of
`lme4_uSym'
> Syms.h:37: warning: type defaults to `int' in declaration of
`lme4_varSym'
> Syms.h:38: warning: type defaults to `int' in declaration of
`lme4_xSym'
> Syms.h:39: warning: type defaults to `int' in declaration of
`lme4_ySym'
> Syms.h:39: warning: data definition has no type or storage class
> init.c:37: error: parse error before "c"
> init.c:37: warning: type defaults to `int' in declaration of `c'
> init.c:37: warning: data definition has no type or storage class
> init.c: In function `R_init_lme4':
> init.c:54: warning: implicit declaration of function
`M_R_cholmod_start'
> init.c:55: error: request for member `final_ll' in something not a
structure or
> union
> init.c:57: warning: assignment makes integer from pointer without a
cast
> init.c:58: warning: assignment makes integer from pointer without a
cast
> init.c:59: warning: assignment makes integer from pointer without a
cast
> init.c:60: warning: assignment makes integer from pointer without a
cast
> init.c:61: warning: assignment makes integer from pointer without a
cast
> init.c:62: warning: assignment makes integer from pointer without a
cast
> init.c:63: warning: assignment makes integer from pointer without a
cast
> init.c:64: warning: assignment makes integer from pointer without a
cast
> init.c:65: warning: assignment makes integer from pointer without a
cast
> init.c:66: warning: assignment makes integer from pointer without a
cast
> init.c:67: warning: assignment makes integer from pointer without a
cast
> init.c:68: warning: assignment makes integer from pointer without a
cast
> init.c:69: warning: assignment makes integer from pointer without a
cast
> init.c:70: warning: assignment makes integer from pointer without a
cast
> init.c:71: warning: assignment makes integer from pointer without a
cast
> init.c:72: warning: assignment makes integer from pointer without a
cast
> init.c:73: warning: assignment makes integer from pointer without a
cast
> init.c:74: warning: assignment makes integer from pointer without a
cast
> init.c:75: warning: assignment makes integer from pointer without a
cast
> init.c:76: warning: assignment makes integer from pointer without a
cast
> init.c:77: warning: assignment makes integer from pointer without a
cast
> init.c:78: warning: assignment makes integer from pointer without a
cast
> init.c:79: warning: assignment makes integer from pointer without a
cast
> init.c:80: warning: assignment makes integer from pointer without a
cast
> init.c:81: warning: assignment makes integer from pointer without a
cast
> init.c:82: warning: assignment makes integer from pointer without a
cast
> init.c:83: warning: assignment makes integer from pointer without a
cast
> init.c:84: warning: assignment makes integer from pointer without a
cast
> init.c:85: warning: assignment makes integer from pointer without a
cast
> init.c:86: warning: assignment makes integer from pointer without a
cast
> init.c:87: warning: assignment makes integer from pointer without a
cast
> init.c:88: warning: assignment makes integer from pointer without a
cast
> init.c:89: warning: assignment makes integer from pointer without a
cast
> init.c:90: warning: assignment makes integer from pointer without a
cast
> init.c:91: warning: assignment makes integer from pointer without a
cast
> init.c:92: warning: assignment makes integer from pointer without a
cast
> init.c:93: warning: assignment makes integer from pointer without a
cast
> init.c: In function `R_unload_lme4':
> init.c:100: warning: implicit declaration of function
`M_cholmod_finish'
> make: *** [init.o] Error 1
> ERROR: compilation failed for package 'lme4'
>
> Mark Lyman, Statistician
> ATK Launch Systems
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From HStevens at MUOhio.edu  Thu Jul 10 16:26:33 2008
From: HStevens at MUOhio.edu (Hank Stevens)
Date: Thu, 10 Jul 2008 10:26:33 -0400
Subject: [R-sig-ME] MCMC Issue relating to the State of lme4
In-Reply-To: <df33d1f90807100629x31626515p713787737ee72e2@mail.gmail.com>
References: <20080710120509.GA914@psych.upenn.edu>
	<ED7B522EE00C9A4FA515AA71724D61EE4E4C1F@DC1EXCL01.air.org>
	<df33d1f90807100629x31626515p713787737ee72e2@mail.gmail.com>
Message-ID: <9CCEDCF1-49A5-41A1-8C88-2BF09D47D19B@MUOhio.edu>

Hi folks,
Repost/summary of previous correspondence.  Thanks to Ben Bolker,  
Spencer Graves and Ken Beath for questions and comments.

I have doubts about the MCMC results with the most recent version of  
lme4 (versions ....-20 on CRAN, and -21 on R-Forge).

I recently tried analyzing a moderately sized (n=~500) unbalanced data  
set where lme and lmer gave similar F ratios.  lmer simulations gave  
the same F-tests (via simulation) as lme, but the MCMC output (using  
Helmert and sum contrasts) depended wildly on terms that were retained  
in the model. Unfortunately, it seems to depend on the data, and I  
cannot make the data (widely) publicly available, although I could  
send it to individuals.


My original question and compilation of other emails:

Are there general situations in which we might expect very different
answers from F tests vs. mcmcpvalue with orthogonal contrasts (Helmert)?

I helping someone with a normal linear model with a moderate sized,
noisy data set, and I am getting very different probabilities between
F-tests and mcmcpvalue for some interactions.

(Output appended.)

I get similar F-test results whether I use lm (and ignore the random
effect of subject), lme, and lmer with an DDF approximation and via  
simulation (lmer::simulate,  of the null hypothesis indicate that F-
stats as large (or larger) than my observed F-stats are VERY unlikely,
under the null hypothesis).

When I use mcmcpvalue, I get huge changes in P-value of a main effect
(0.6 to 0.01) when I remove its interactions. In contrast, the F-test
(using trace of the hat matrix DF's) are much more consistent when I
change the fixed effect structure.

Is mcmcpvalue much more sensitive to overfitting the model? In
some cases, removing the interactions results in a lower AIC (with ML
fits). The MCMC sample traces look (in my limited experience)  
withoutpeculiarities, and the densityplots are all quite symmetrical  
and normal-ish.


In the full model, we have 28 fixed coefs (22 continuous variables or
slope interactions) and about 500 obs of about 200 subjects (2-3  
observations per subject). The data are VERY unbalanced. The QQ plots  
look normal, but highlight the lack of balance (from one to dozens of  
reps per treatment combo).

OUTPUT:OUTPUT:OUTPUT:OUTPUT:OUTPUT:OUTPUT:OUTPUT:OUTPUT:OUTPUT:OUTPUT:

The following tables are output of factors/covariates from a function  
I wrote to use with lmer. "P.sum" is an F-test using one hack for  
denominator degrees of freedom (these agree qualitatively with lme).  
"P(H|D)" is the empirical P-value generated from Bates' function  
mcmcpvalue, with my hack for the new structure of the latest mcmcsamp  
output (NOTE that I checked that these P-values also agree with direct  
HPDinterval and quantile assessments of individual coefficients).

FULL MODEL
 > aov.Bayes(modcA, n=1000)$aov.tab
                      Df  Sum.Sq Mean.Sq F.value  P.sum P(H|D)
ageinj                1  127.00  127.00  2.6559 0.1038  0.096
sex                   1  233.32  233.32  4.8793 0.0277  0.008
race                  1  216.49  216.49  4.5273 0.0339  0.372
zses                  1  249.03  249.03  5.2078 0.0229  0.010
tsi                   1  138.05  138.05  2.8870 0.0900  0.005
tsq                   1  393.21  393.21  8.2230 0.0043  0.008
grp                   3  324.45  108.15  2.2617 0.0805  0.648
zfad                  1  988.80  988.80 20.6782 0.0000  0.051
tsi:grp               3  460.46  153.49  3.2098 0.0229  0.364
tsq:grp               3  229.16   76.39  1.5974 0.1892  0.541
grp:zfad              3   89.48   29.83  0.6238 0.5999  0.555
tsi:zfad              1   84.01   84.01  1.7568 0.1857  0.963
tsq:zfad              1   78.01   78.01  1.6314 0.2021  0.633
tsi:grp:zfad          3 1026.88  342.29  7.1582 0.0001  0.812
tsq:grp:zfad          3  123.36   41.12  0.8599 0.4618  0.884
Residuals (Tr(hat)) 469      NA   47.82      NA     NA     NA


FULL MODEL minus one three way interaction changes P(H|D) for the  
other 3-way.
 > aov.Bayes(modcB, n=1000)$aov.tab
                      Df  Sum.Sq Mean.Sq F.value  P.sum P(H|D)
ageinj                1  127.85  127.85   2.670 0.1029  0.094
sex                   1  234.75  234.75   4.902 0.0273  0.008
race                  1  217.92  217.92   4.551 0.0334  0.357
zses                  1  250.62  250.62   5.234 0.0226  0.017
tsi                   1  138.01  138.01   2.882 0.0902  0.007
tsq                   1  393.04  393.04   8.208 0.0044  0.010
grp                   3  326.62  108.87   2.274 0.0793  0.646
zfad                  1  994.61  994.61  20.771 0.0000  0.024
tsi:grp               3  460.45  153.48   3.205 0.0230  0.334
tsq:grp               3  229.34   76.45   1.596 0.1894  0.453
grp:zfad              3   90.07   30.02   0.627 0.5978  0.292
tsi:zfad              1   83.88   83.88   1.752 0.1863  0.777
tsq:zfad              1   77.92   77.92   1.627 0.2027  0.358
tsi:grp:zfad          3 1026.92  342.31   7.148 0.0001  0.015
Residuals (Tr(hat)) 472      NA   47.89      NA     NA     NA
 >

NO INTERACTIONS
 > aov.Bayes(modcD, n=1000)$aov.tab
                      Df Sum.Sq Mean.Sq F.value  P.sum P(H|D)
ageinj                1  96.66   96.66   1.847 0.1748  0.061
sex                   1 269.07  269.07   5.141 0.0238  0.005
race                  1 214.66  214.66   4.102 0.0434  0.064
zses                  1 257.90  257.90   4.928 0.0269  0.030
tsi                   1 137.91  137.91   2.635 0.1052  0.044
tsq                   1 415.80  415.80   7.945 0.0050  0.069
grp                   3 373.93  124.64   2.382 0.0688  0.001
Residuals (Tr(hat)) 490     NA   52.34      NA     NA     NA

 > sessionInfo()
R version 2.7.1 (2008-06-23)
i386-apple-darwin8.10.1

locale:
C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods
[7] base

other attached packages:
[1] foreign_0.8-26     lme4_0.999375-21   Matrix_0.999375-10
[4] lattice_0.17-8     Hmisc_3.4-3

loaded via a namespace (and not attached):
[1] cluster_1.11.11 grid_2.7.1
 >

Are there general situations in which we might expect very different
answers from F tests vs. mcmcpvalue with orthogonal contrasts (Helmert)?

I helping someone with a normal linear model with a moderate sized  
(n=500),
noisy data set, and I am getting very different probabilities between
F-tests and mcmcpvalue for some interactions.

I get similar F-test results whether I use lm (and ignore the random
effect of subject), lme, and lmer with an DDF approximation.

When I use mcmcpvalue, I get huge changes in P-value of a main effect
(0.6 to 0.01) when I remove its interactions. In contrast, the F-test
(using trace of the hat matrix DF's) are much more consistent when I
change the fixed effect structure.

I think mcmcpvalue is much more sensitive to overfitting the model. In
some cases, removing the interactions results in a lower AIC (with ML
fits).



In the full model, we have 28 fixed coefs (22 continuous variables or
slope interactions) and about 500 obs.

The data are VERY unbalanced.




On Jul 10, 2008, at 9:29 AM, Gillian Raab wrote:

> MCMC confidence intervals (strictly Bayesian credible intervals)  
> give much
> better results in some circumstances, notably for parameters that  
> are poorly
> estimated. Things are seldom a problem (In my experience) for fixed  
> effects
> unless the model is ill-specified, but the mcmc methods really score  
> for
> random effect paramters.
>
> 2008/7/10 Doran, Harold <HDoran at air.org>:
>
>> Out of curiousity, why not just use the asymptotic standards errors  
>> of
>> the fixed effects to get Cis rather than via simulation?
>>
>>> -----Original Message-----
>>> From: r-sig-mixed-models-bounces at r-project.org
>>> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf
>>> Of Jonathan Baron
>>> Sent: Thursday, July 10, 2008 8:05 AM
>>> To: Michael Kubovy
>>> Cc: r-sig-mixed-models at r-project.org
>>> Subject: Re: [R-sig-ME] State of lme4
>>>
>>> On 07/10/08 07:51, Michael Kubovy wrote:
>>>> Dear Friends,
>>>>
>>>> I have become confused as to which set-up of lme4, arm,
>>> gmodels, etc
>>>> will produce CIs on the fixed effects by simulation.
>>>
>>> In the latest version of lme4, mcmcsamp and HPDinterval work.
>>> And languageR has a new version as of yesterday, which seems
>>> to deal correctly with the current version of lme4.
>>> Specifically, pvals.fnc() works.  I don't know about arm and
>>> gmodels.  I haven't seen new versions, so I suspect they will
>>> not deal with the new format of lmer.
>>>
>>> Jon
>>> --
>>> Jonathan Baron, Professor of Psychology, University of
>>> Pennsylvania Home page: http://www.sas.upenn.edu/~baron
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>
>
> --
> Gillian M Raab
> 10 Ainslie Place EH3 6AS
> tel 0131 226 6234
> mobile 07748 678 551
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



Dr. Hank Stevens, Associate Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.cas.muohio.edu/ecology
http://www.muohio.edu/botany/

"If the stars should appear one night in a thousand years, how would men
believe and adore." -Ralph Waldo Emerson, writer and philosopher  
(1803-1882)



From bates at stat.wisc.edu  Thu Jul 10 18:49:15 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 10 Jul 2008 11:49:15 -0500
Subject: [R-sig-ME] [R] lme4 : lmer : convergence problem and other
	errors
In-Reply-To: <df33d1f90807080653m1fc38c53oedcf1f85aab05a08@mail.gmail.com>
References: <1215091816.486cd468ef63c@mail-www.oit.umass.edu>
	<df33d1f90807080040s29bd33f8paf083d6a1799aaab@mail.gmail.com>
	<1215524755.48736f9313e4d@mail-www.oit.umass.edu>
	<df33d1f90807080653m1fc38c53oedcf1f85aab05a08@mail.gmail.com>
Message-ID: <40e66e0b0807100949v108a4fa3q4f0c9a620dddb147@mail.gmail.com>

On Tue, Jul 8, 2008 at 8:53 AM, Gillian Raab
<gillian.raab at googlemail.com> wrote:
> 2008/7/8 <yufeng at nsm.umass.edu>:
>
>> Please see below:
>>
>> > 1) What version of lmer are you running? The new version post 23/6/08
>> copes
>> > with difficult likelihoods better.
>>
>> I used the most up-to-date version.

The description "most up-to-date" can become wrong very quickly.  It
is better to quote a version number.  Use

sessionInfo()

to get the version number.

>> > 2) Have you changed anything in the elements of the control parameter.
>> The
>> > pre 23/6 verion had several parameters and the later one fewer. In
>> > particular you can increase the iterations
>>
>> How could I increase the iterations?
>
> READ THE HELP FILE UNDER LMER AND THE CONTROL  PARAMETER

Well, actually, current versions of the help file should say that the
only control parameter recognized is msVerbose, which is the old way
of setting the "verbose" argument.  Martin Maechler has pointed out on
several occasions that I should allow the maximum number of iterations
to be reset (also, the maximum number of function evaluations) but I
haven't done that yet.

There is a horrible hack that can be used to provide a greater number
of iterations.  The arguments to the C function "mer_optimize" are an
mer object and a verbose flag.  If the model fit fails because the
optimizer has exceeded the number of iterations you can take the
returned object and feed it directly back into

.Call("mer_optimize", <failed_lmer_fit>, FALSE, PACKAGE = "lme4")

As I say, this is a horrible hack and I should make it possible to set
the maximum number of iterations in the call to lmer rather than doing
this.  However, doing so would involve creating the appropriate
argument names and defaults and my experience is that if this is done
hastily I end up regretting the resulting awkwardness.


>>
>>
>> > 3) Have you centred your x variables so they have means around zero. If
>> not
>> > you should always do this as it will make the fitting easier especially
>> with
>> > quadratic terms. This ought to have been my first suggestion.
>>
>> Do u mean the random effects of x's should be centered around 0? I didn't
>> do
>> that and I don't know how to do that in R? Could U tell me how? Thanks!
>
> NOTHING FANCY JUST CALCULATE NEW X VARIABLES BY SUBTRACTING THE MEAN VALUES
>
>>
>>
>>
>>
>> > 4) Having another look at your model you say it is non-linear, but it
>> looks
>> > linear to me if you set the squared terms as covariates too.
>>
>> You are right the model should be linear.
>>
>> >
>> > Good luck
>> >
>> > Gillian Raab
>> > Edinburgh
>> >
>> > On 03/07/2008, yufeng at nsm.umass.edu <yufeng at nsm.umass.edu> wrote:
>> > >
>> > > Dear R-user,
>> > >
>> > > I am trying to use the R "lmer" function in lme4 package to fit a non
>> > > linear
>> > > mixed effects model. The model I wand to fit is at an individual level
>> with
>> > > 4
>> > > parameters. For all parameters both fixed and random effects have to be
>> > > estimated, as well as their covariance matrix (see the formula bellow).
>> > > y~x1+x1^2+x2+x2^2.
>> > >
>> > >
>> > > I tried to fit the model with my data sets, but most of the time, R
>> returns
>> > > an
>> > > error message.
>> > > there are three main types of errors :
>> > >
>> > > - In mer_finalize(ans, verbose) :
>> > >   function evaluation limit reached without convergence (9)
>> > >
>> > > -  there are false convergence (8)
>> > >
>> > > -there are singular convergence (7)
>> > >
>> > > Do you know how to resolve these problems. Is there a way to modify the
>> > > parameters of the maximization algorithm to avoid these error messages?
>> > >
>> > > Thank you for your help and answers.
>> > >
>> > > Regards,
>> > >
>> > > Yufeng Zhang
>> > >
>> > > _______________________________________________
>> > > R-sig-mixed-models at r-project.org mailing list
>> > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> > >
>> >
>> >
>> >
>> > --
>> > Gillian M Raab
>> > 10 Ainslie Place EH3 6AS
>> > tel 0131 226 6234
>> > mobile 07748 678 551
>> >
>>
>>
>>
>>
>
>
> --
> Gillian M Raab
> 10 Ainslie Place EH3 6AS
> tel 0131 226 6234
> mobile 07748 678 551
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From hstevens at muohio.edu  Thu Jul 10 20:49:40 2008
From: hstevens at muohio.edu (Hank Stevens)
Date: Thu, 10 Jul 2008 14:49:40 -0400
Subject: [R-sig-ME] MCMC Issue relating to the State of lme4
In-Reply-To: <48762B59.4060303@zoology.ufl.edu>
References: <20080710120509.GA914@psych.upenn.edu>
	<ED7B522EE00C9A4FA515AA71724D61EE4E4C1F@DC1EXCL01.air.org>
	<df33d1f90807100629x31626515p713787737ee72e2@mail.gmail.com>
	<9CCEDCF1-49A5-41A1-8C88-2BF09D47D19B@MUOhio.edu>
	<48762B59.4060303@zoology.ufl.edu>
Message-ID: <E929ECBB-7770-4307-8068-B47A2CBE44D8@muohio.edu>

Hi folks,
Here are code and files for the analysis.
-------------- next part --------------


Hi folks,
Repost/summary of previous correspondence.  Thanks to Ben Bolker,
Spencer Graves and Ken Beath for questions and comments.

I have doubts about the MCMC results with the most recent version of
lme4 (versions ....-20 on CRAN, and -21 on R-Forge).

I recently tried analyzing a moderately sized (n=~500) unbalanced data
set where lme and lmer gave similar F ratios.  lmer simulations gave
the same F-tests (via simulation) as lme, but the MCMC output (using
Helmert and sum contrasts) depended wildly on terms that were retained
in the model. Unfortunately, it seems to depend on the data, and I
cannot make the data (widely) publicly available, although I could
send it to individuals.


My original question and compilation of other emails:

Are there general situations in which we might expect very different
answers from F tests vs. mcmcpvalue with orthogonal contrasts (Helmert)?

I helping someone with a normal linear model with a moderate sized,
noisy data set, and I am getting very different probabilities between
F-tests and mcmcpvalue for some interactions.

(Output appended.)

I get similar F-test results whether I use lm (and ignore the random
effect of subject), lme, and lmer with an DDF approximation and via
simulation (lmer::simulate,  of the null hypothesis indicate that F-
stats as large (or larger) than my observed F-stats are VERY unlikely,
under the null hypothesis).

When I use mcmcpvalue, I get huge changes in P-value of a main effect
(0.6 to 0.01) when I remove its interactions. In contrast, the F-test
(using trace of the hat matrix DF's) are much more consistent when I
change the fixed effect structure.

Is mcmcpvalue much more sensitive to overfitting the model? In
some cases, removing the interactions results in a lower AIC (with ML
fits). The MCMC sample traces look (in my limited experience)
withoutpeculiarities, and the densityplots are all quite symmetrical
and normal-ish.


In the full model, we have 28 fixed coefs (22 continuous variables or
slope interactions) and about 500 obs of about 200 subjects (2-3
observations per subject). The data are VERY unbalanced. The QQ plots
look normal, but highlight the lack of balance (from one to dozens of
reps per treatment combo).

OUTPUT:OUTPUT:OUTPUT:OUTPUT:OUTPUT:OUTPUT:OUTPUT:OUTPUT:OUTPUT:OUTPUT:

The following tables are output of factors/covariates from a function
I wrote to use with lmer. "P.sum" is an F-test using one hack for
denominator degrees of freedom (these agree qualitatively with lme).
"P(H|D)" is the empirical P-value generated from Bates' function
mcmcpvalue, with my hack for the new structure of the latest mcmcsamp
output (NOTE that I checked that these P-values also agree with direct
HPDinterval and quantile assessments of individual coefficients).

FULL MODEL
> aov.Bayes(modcA, n=1000)$aov.tab
                      Df  Sum.Sq Mean.Sq F.value  P.sum P(H|D)
ageinj                1  127.00  127.00  2.6559 0.1038  0.096
sex                   1  233.32  233.32  4.8793 0.0277  0.008
race                  1  216.49  216.49  4.5273 0.0339  0.372
zses                  1  249.03  249.03  5.2078 0.0229  0.010
tsi                   1  138.05  138.05  2.8870 0.0900  0.005
tsq                   1  393.21  393.21  8.2230 0.0043  0.008
grp                   3  324.45  108.15  2.2617 0.0805  0.648
zfad                  1  988.80  988.80 20.6782 0.0000  0.051
tsi:grp               3  460.46  153.49  3.2098 0.0229  0.364
tsq:grp               3  229.16   76.39  1.5974 0.1892  0.541
grp:zfad              3   89.48   29.83  0.6238 0.5999  0.555
tsi:zfad              1   84.01   84.01  1.7568 0.1857  0.963
tsq:zfad              1   78.01   78.01  1.6314 0.2021  0.633
tsi:grp:zfad          3 1026.88  342.29  7.1582 0.0001  0.812
tsq:grp:zfad          3  123.36   41.12  0.8599 0.4618  0.884
Residuals (Tr(hat)) 469      NA   47.82      NA     NA     NA


FULL MODEL minus one three way interaction changes P(H|D) for the
other 3-way.
> aov.Bayes(modcB, n=1000)$aov.tab
                      Df  Sum.Sq Mean.Sq F.value  P.sum P(H|D)
ageinj                1  127.85  127.85   2.670 0.1029  0.094
sex                   1  234.75  234.75   4.902 0.0273  0.008
race                  1  217.92  217.92   4.551 0.0334  0.357
zses                  1  250.62  250.62   5.234 0.0226  0.017
tsi                   1  138.01  138.01   2.882 0.0902  0.007
tsq                   1  393.04  393.04   8.208 0.0044  0.010
grp                   3  326.62  108.87   2.274 0.0793  0.646
zfad                  1  994.61  994.61  20.771 0.0000  0.024
tsi:grp               3  460.45  153.48   3.205 0.0230  0.334
tsq:grp               3  229.34   76.45   1.596 0.1894  0.453
grp:zfad              3   90.07   30.02   0.627 0.5978  0.292
tsi:zfad              1   83.88   83.88   1.752 0.1863  0.777
tsq:zfad              1   77.92   77.92   1.627 0.2027  0.358
tsi:grp:zfad          3 1026.92  342.31   7.148 0.0001  0.015
Residuals (Tr(hat)) 472      NA   47.89      NA     NA     NA
>

NO INTERACTIONS
> aov.Bayes(modcD, n=1000)$aov.tab
                      Df Sum.Sq Mean.Sq F.value  P.sum P(H|D)
ageinj                1  96.66   96.66   1.847 0.1748  0.061
sex                   1 269.07  269.07   5.141 0.0238  0.005
race                  1 214.66  214.66   4.102 0.0434  0.064
zses                  1 257.90  257.90   4.928 0.0269  0.030
tsi                   1 137.91  137.91   2.635 0.1052  0.044
tsq                   1 415.80  415.80   7.945 0.0050  0.069
grp                   3 373.93  124.64   2.382 0.0688  0.001
Residuals (Tr(hat)) 490     NA   52.34      NA     NA     NA

> sessionInfo()
R version 2.7.1 (2008-06-23)
i386-apple-darwin8.10.1

locale:
C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods
[7] base

other attached packages:
[1] foreign_0.8-26     lme4_0.999375-21   Matrix_0.999375-10
[4] lattice_0.17-8     Hmisc_3.4-3

loaded via a namespace (and not attached):
[1] cluster_1.11.11 grid_2.7.1
>

Are there general situations in which we might expect very different
answers from F tests vs. mcmcpvalue with orthogonal contrasts (Helmert)?

I helping someone with a normal linear model with a moderate sized
(n=500),
noisy data set, and I am getting very different probabilities between
F-tests and mcmcpvalue for some interactions.

I get similar F-test results whether I use lm (and ignore the random
effect of subject), lme, and lmer with an DDF approximation.

When I use mcmcpvalue, I get huge changes in P-value of a main effect
(0.6 to 0.01) when I remove its interactions. In contrast, the F-test
(using trace of the hat matrix DF's) are much more consistent when I
change the fixed effect structure.

I think mcmcpvalue is much more sensitive to overfitting the model. In
some cases, removing the interactions results in a lower AIC (with ML
fits).



In the full model, we have 28 fixed coefs (22 continuous variables or
slope interactions) and about 500 obs.

The data are VERY unbalanced.




On Jul 10, 2008, at 9:29 AM, Gillian Raab wrote:

> MCMC confidence intervals (strictly Bayesian credible intervals)
> give much
> better results in some circumstances, notably for parameters that
> are poorly
> estimated. Things are seldom a problem (In my experience) for fixed
> effects
> unless the model is ill-specified, but the mcmc methods really score
> for
> random effect paramters.
>
> 2008/7/10 Doran, Harold <HDoran at air.org>:
>
>> Out of curiousity, why not just use the asymptotic standards errors
>> of
>> the fixed effects to get Cis rather than via simulation?
>>
>>> -----Original Message-----
>>> From: r-sig-mixed-models-bounces at r-project.org
>>> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf
>>> Of Jonathan Baron
>>> Sent: Thursday, July 10, 2008 8:05 AM
>>> To: Michael Kubovy
>>> Cc: r-sig-mixed-models at r-project.org
>>> Subject: Re: [R-sig-ME] State of lme4
>>>
>>> On 07/10/08 07:51, Michael Kubovy wrote:
>>>> Dear Friends,
>>>>
>>>> I have become confused as to which set-up of lme4, arm,
>>> gmodels, etc
>>>> will produce CIs on the fixed effects by simulation.
>>>
>>> In the latest version of lme4, mcmcsamp and HPDinterval work.
>>> And languageR has a new version as of yesterday, which seems
>>> to deal correctly with the current version of lme4.
>>> Specifically, pvals.fnc() works.  I don't know about arm and
>>> gmodels.  I haven't seen new versions, so I suspect they will
>>> not deal with the new format of lmer.
>>>
>>> Jon
>>> --
>>> Jonathan Baron, Professor of Psychology, University of
>>> Pennsylvania Home page: http://www.sas.upenn.edu/~baron
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>
>
> --
> Gillian M Raab
> 10 Ainslie Place EH3 6AS
> tel 0131 226 6234
> mobile 07748 678 551
>
>       [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



Dr. Hank Stevens, Associate Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.cas.muohio.edu/ecology
http://www.muohio.edu/botany/

"If the stars should appear one night in a thousand years, how would men
believe and adore." -Ralph Waldo Emerson, writer and philosopher
(1803-1882)

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From bates at stat.wisc.edu  Fri Jul 11 15:28:09 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 11 Jul 2008 08:28:09 -0500
Subject: [R-sig-ME] Binomial dependent variable in a mixed-model design
In-Reply-To: <000301c8e354$982775a0$d729f889@pc04365>
References: <000001c8ddb5$f921f370$d729f889@pc04365>
	<40e66e0b0807091116g696c6929ie5308534448883fb@mail.gmail.com>
	<000301c8e354$982775a0$d729f889@pc04365>
Message-ID: <40e66e0b0807110628t5e751578i73a6f238becc28e3@mail.gmail.com>

On Fri, Jul 11, 2008 at 7:49 AM, Jan Rummel
<rummelj at staff.uni-marburg.de> wrote:
> Dear Mr. Bates,
> Thank you for your quick reply. I tried to use your lme4 package. But I
> think the problem still remains the same. If I use the comand "binomial" for
> specifying the scale of my dependent variable the program would allow binary
> data only.
> Is there any posibility to allow binomial data such as described below
> (number of hits either 0,1,2,or3)?

You can specify a binomial response by providing a matrix with columns
of # of successes and # of failures on the left hand side of the model
formula.  See

?binomial

or

library(lme4)
example(cbpp)

> Thank you very much and sorry again for the inconvenience.
> jan
>
>
> -----Urspr?ngliche Nachricht-----
> Von: dmbates at gmail.com [mailto:dmbates at gmail.com] Im Auftrag von Douglas
> Bates
> Gesendet: Mittwoch, 9. Juli 2008 20:16
> An: rummelj at staff.uni-marburg.de
> Betreff: Re: Binomial dependent variable in a mixed-model design
>
> What you are describing is called a generalized linear mixed model.
> The lme4 package for R has facilities for fitting such models.  See the
> function glmer.
>
>
> On Fri, Jul 4, 2008 at 4:11 AM, Jan Rummel <rummelj at staff.uni-marburg.de>
> wrote:
>> Dear Mr Bates,
>> I am a Phd-Student at the University of Marburg (Germany). And I have
>> a question concerning your lme-package for R.
>>
>> I have conducted an experiment with one between-subject factor "group"
>> (group1 vs. group2) and one within-subject factor "stage" (high vs. low).
>> My dependent variable is binomial. It's the number of hits witch can
>> be either 0, 1, 2, or 3.
>>
>> To analyse my data I tried your lme-implementation in R.
>> Unfortunately, lme only allows binary data under the command
>> "binomial" for the distribution of the DV. So, I got the error message
>> that only 0 and 1 are possible values of the DV.
>>
>> Is there any posbility to analyse my data in lme? And if so, how do I
>> have to modify my the R syntax for that? I am interested in the
>> between-subject main effects, the within-subject main effects, and the
> interaction as well.
>>
>> Sorry for any inconvenience.
>> Thank you very much and best wishes.
>> Jan
>>
>> --
>> Jan Rummel
>> Department of Psychology
>> University of Marburg
>> Gutenbergstr. 18
>> 35032 Marburg
>> Germany
>>
>> Phone: +49(0)6421/2823646
>> Email: jan.rummel at staff.uni-marburg.de
>>
>>
>
>



From HStevens at MUOhio.edu  Fri Jul 11 15:46:13 2008
From: HStevens at MUOhio.edu (Hank Stevens)
Date: Fri, 11 Jul 2008 09:46:13 -0400
Subject: [R-sig-ME] MCMC and F tests, take 3
Message-ID: <23A9B8C2-66F5-488A-94CF-B305BEBB5E6F@MUOhio.edu>

Hi folks,
Repost/summary of previous correspondence.  Thanks to Ben Bolker,  
Spencer Graves and Ken Beath for questions and comments.

Are there general situations in which we might expect very different
answers from F tests vs. mcmcpvalue with orthogonal contrasts  
(Helmert)? I have doubts about the MCMC results with the most recent  
version of lme4 (versions ....-20 on CRAN, and -21 on R-Forge).

If you are interested, please run the code below.

system("open http://www.cas.muohio.edu/~stevenmh/RFiles/lme4.html")
source("http://www.cas.muohio.edu/~stevenmh/RFiles/aovBayes.r")
download.file("http://www.cas.muohio.edu/~stevenmh/RFiles/datout.csv",  
"dathere.csv")
dat <- read.csv("dathere.csv")
names(dat)
options(contrasts=c("contr.helmert", "contr.poly"))
library(lme4)
sessionInfo()

modcA.sim <- lmer(y ~ ageinj + sex + race + zses +
               tsi + tsq +
                grp + grp:tsi + grp:tsq +
              zfad + zfad:grp + zfad:tsi + zfad:tsq +
              tsi:zfad:grp + tsq:zfad:grp + (1|id), dat)
attr(modcA.sim @ X, "contrasts")
aov.Bayes(modcA.sim, n=1000)
mc.out <- mcmcsamp(modcA.sim, n=1000)
densityplot(mc.out)
xyplot(mc.out)


Dr. Hank Stevens, Associate Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.cas.muohio.edu/ecology
http://www.muohio.edu/botany/

"If the stars should appear one night in a thousand years, how would men
believe and adore." -Ralph Waldo Emerson, writer and philosopher  
(1803-1882)



From bates at stat.wisc.edu  Fri Jul 11 16:19:44 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 11 Jul 2008 09:19:44 -0500
Subject: [R-sig-ME] Covariance structure and MCMC
In-Reply-To: <DE0D5C6148C2F24B9CCE3F10DE2AF00AC42AF2@ut40se02.atk.com>
References: <DE0D5C6148C2F24B9CCE3F10DE2AF00AC42AF2@ut40se02.atk.com>
Message-ID: <40e66e0b0807110719w7da068cyb7d74dde8b6dc05d@mail.gmail.com>

On Fri, Jul 11, 2008 at 9:06 AM, Remund, Todd <Todd.Remund at atk.com> wrote:
> I have been using lmer() to fit a mixed model to some experimental design
> data.  I have an unstructured covariance structure.  I would like to use
> your mcmcsamp() function to to inference using a Bayesian methods, but the
> mcmcsamp function will not take the mer class object when it has the
> covariance structure.  Is there a prototype version that I could use to
> create the MCMC samples?  I really appreciate your time, and the functions
> you have created.  They are very fast.  (My data set is rather large.)

Thank you for the kind words.  I'm sorry to say that I don't have a
prototype version of mcmcsamp for models with correlated random
effects.  I believe it would take only a day or two to implement it
but right now I don't have a day or two to work on it.       There are
simply too many other commitments at this time, including checking on
the peculiar behavior of the current mcmcsamp for models with
uncorrelated random effects.  There is really no purpose in working
out the sampling of the parameters that induce correlation if the
variance components are not being sampled correctly.



From bates at stat.wisc.edu  Fri Jul 11 16:23:36 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 11 Jul 2008 09:23:36 -0500
Subject: [R-sig-ME] hatTrace (was: no subject)
Message-ID: <40e66e0b0807110723n69f0bd7ex9e40ee2e20f72f00@mail.gmail.com>

On Mon, Jul 7, 2008 at 8:35 AM, David Paez <david.paez.1 at ulaval.ca> wrote:
> Hi all,

> In the newest version of lme4 the function hatTrace seems to be hidden away so
> that R doesn't recognize it anymore.

It is there although it is not exported in the NAMESPACE file and it begins with

 .NotYetImplemented()

so it doesn't really help much.

> Does someone know another method that gets to the trace of the hat matrix so
> that I can calculate its DF?

It is reasonably easy to write down a formula for it but providing a
calculation method that is practical for complex models fit to large
data sets is not so easy. Version 0.999875-21 of the lme4 package,
currently on R-forge and soon to be on CRAN, has a version of the
formula at the end of the PLSvGLS vignette.  The problem with
evaluating the formula is that a very large sparse system of equations
must be solved and doing that in the naive way can take a long time.



From gillian.raab at googlemail.com  Fri Jul 11 22:31:16 2008
From: gillian.raab at googlemail.com (Gillian Raab)
Date: Fri, 11 Jul 2008 21:31:16 +0100
Subject: [R-sig-ME] Covariance structure and MCMC
In-Reply-To: <40e66e0b0807110719w7da068cyb7d74dde8b6dc05d@mail.gmail.com>
References: <DE0D5C6148C2F24B9CCE3F10DE2AF00AC42AF2@ut40se02.atk.com>
	<40e66e0b0807110719w7da068cyb7d74dde8b6dc05d@mail.gmail.com>
Message-ID: <df33d1f90807111331y54ec5dc1pbef5b16106859e6d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080711/e40891a4/attachment.pl>

From thiago.cortez at uol.com.br  Mon Jul 14 12:07:16 2008
From: thiago.cortez at uol.com.br (Thiago Cortez Costa)
Date: Mon, 14 Jul 2008 07:07:16 -0300
Subject: [R-sig-ME] nlme, lme( ) convergence and selection of effects
Message-ID: <000001c8e599$66c56a70$34503f50$@cortez@uol.com.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080714/cafbc906/attachment.pl>

From marshj02 at student.uwa.edu.au  Tue Jul 15 07:43:38 2008
From: marshj02 at student.uwa.edu.au (Julie Marsh)
Date: Tue, 15 Jul 2008 13:43:38 +0800
Subject: [R-sig-ME] lmer: LRT and mcmcpvalue for fixed effects
Message-ID: <20080715134338.oi73vl4n68cgcgok@webmail-5.ucs.uwa.edu.au>

Dear lmer expeRts,

I wondered if anybody could help me reconcile these two test results ?

I have been fitting models of the form:

BASIC MODEL
fl.basic <- lmer(y.fl~(I(TIME-200) + I((TIME-200)^2))*SEX + V65_A +
                       (I(TIME-200)|STUDYNO ),
            data=fl.data[!is.na(fl.data$TIME),], method="ML",  
na.action=na.omit,
            control=list(maxIter=1000, niterEM=0, gradient=FALSE,  
msVerbose = 1))


BASIC MODEL PLUS BASIC GENETIC EFFECT
fl.gen1 <- lmer(y.fl ~(I(TIME-200) + I((TIME-200)^2))*SEX + SNP_dom + V65_A +
                       (I(TIME-200)|STUDYNO),
            data=fl.data[!is.na(fl.data$TIME),], method="ML",  
na.action=na.omit,
            control=list(maxIter=2000, niterEM=0, gradient=FALSE,  
msVerbose = 1))


I have compared the models using both (1) LRT

anova(fl.gen1, fl.basic)
0.5*(1-pchisq(2*(logLik(fl.gen1)-logLik(fl.basic)),0)) +  
0.5*(1-pchisq(2*(logLik(fl.gen1)-logLik(fl.basic)),1))

OUTPUT (1)
Models:
fl.basic: y.fl ~ (I(TIME - 200) + I((TIME - 200)^2)) * SEX + V65_A + (I(TIME -
                     200) | STUDYNO)
fl.gen  : y.fl ~ (I(TIME - 200) + I((TIME - 200)^2)) * SEX + SNP_dom +
fl.gen  :           V65_A + (I(TIME - 200) | STUDYNO)
              Df     AIC     BIC  logLik  Chisq Chi Df Pr(>Chisq)
fl.basic     10  4335.4  4396.1 -2157.7
fl.gen1      11  4291.4  4358.2 -2134.7 45.929      1  1.226e-11 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 0.5*(1-pchisq(2*(logLik(fl.gen1)-logLik(fl.basic)),0)) +  
> 0.5*(1-pchisq(2*(logLik(fl.gen1)-logLik(fl.basic)),1))
[1] 6.131984e-12
attr(,"nobs")
[1] 3189


and (2) using Prof Bates' wonderful mcmcpvalue function

mcmc = mcmcsamp(fl.gen1, n = 100000)
mcmcpvalue(as.matrix(mcmc[ ,5]))

OUTPUT (2): main effect SNP_dom: p=0.0263


Given that I am using 2 different tests for two different hypotheses I  
still would have expected these p-values to be more similar. I am so  
sorry that I can't post the data and the lmer output but I am bound by  
confidentiality. <big sigh>  I understand completely if it is not  
possible to provide any help given this lack of further information.

I have eagerly read and re-read the rwiki help page  .........

http://wiki.r-project.org/rwiki/doku.php?id=guides:lmer-tests

....... but still am unable to explain why the results should be so  
different. Much as I would love to argue against the reliance on  
p-values I'm afraid I am a resigned pragmatist when it comes to trying  
to get anything published.  <sorry!>  Needless to say I will swamp the  
article with far more informative plots and CI's.

Any help would be very much appreciated.

kindest regards,  julie marsh.



From s.blomberg1 at uq.edu.au  Tue Jul 15 09:00:22 2008
From: s.blomberg1 at uq.edu.au (Simon Blomberg)
Date: Tue, 15 Jul 2008 17:00:22 +1000
Subject: [R-sig-ME] lmer: LRT and mcmcpvalue for fixed effects
In-Reply-To: <20080715134338.oi73vl4n68cgcgok@webmail-5.ucs.uwa.edu.au>
References: <20080715134338.oi73vl4n68cgcgok@webmail-5.ucs.uwa.edu.au>
Message-ID: <1216105222.11309.77.camel@sib-sblomber01d.sib.uq.edu.au>

On Tue, 2008-07-15 at 13:43 +0800, Julie Marsh wrote:

> 
> 
> Given that I am using 2 different tests for two different hypotheses I  
> still would have expected these p-values to be more similar. 

Well, as Pinheiro and Bates say in their book (worth reading!), the LRT
for mixed effects models is anti-conservative. So your LRT p-value is
almost certainly too small. The posterior p-value might be more
accurate, if you accept the usual caveats re: priors and convergence
etc. Also, when calculating p-values by hand using pchisq, you should
probably use pchisq(..., lower.tail=FALSE) instead of 1-pchisq(...),
which is inaccurate. The log.p option might also be useful if you really
need to compare small probabilities. And why were you using pchisq with
0 df (which always == 1)? I don't understand that at all.

> I am so  
> sorry that I can't post the data and the lmer output but I am bound by  
> confidentiality. <big sigh>  I understand completely if it is not  
> possible to provide any help given this lack of further information.
> 
> I have eagerly read and re-read the rwiki help page  .........
> 
> http://wiki.r-project.org/rwiki/doku.php?id=guides:lmer-tests
> 
> ....... but still am unable to explain why the results should be so  
> different. Much as I would love to argue against the reliance on  
> p-values I'm afraid I am a resigned pragmatist when it comes to trying  
> to get anything published.  <sorry!>  Needless to say I will swamp the  
> article with far more informative plots and CI's.
> 
> Any help would be very much appreciated.
> 
> kindest regards,  julie marsh.
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
-- 
Simon Blomberg, BSc (Hons), PhD, MAppStat. 
Lecturer and Consultant Statistician 
Faculty of Biological and Chemical Sciences 
The University of Queensland 
St. Lucia Queensland 4072 
Australia
Room 320 Goddard Building (8)
T: +61 7 3365 2506
http://www.uq.edu.au/~uqsblomb
email: S.Blomberg1_at_uq.edu.au

Policies:
1.  I will NOT analyse your data for you.
2.  Your deadline is your problem.

The combination of some data and an aching desire for 
an answer does not ensure that a reasonable answer can 
be extracted from a given body of data. - John Tukey.



From rhbc at imm.dtu.dk  Tue Jul 15 10:33:18 2008
From: rhbc at imm.dtu.dk (Rune Haubo)
Date: Tue, 15 Jul 2008 10:33:18 +0200
Subject: [R-sig-ME] lmer: LRT and mcmcpvalue for fixed effects
In-Reply-To: <1216105222.11309.77.camel@sib-sblomber01d.sib.uq.edu.au>
References: <20080715134338.oi73vl4n68cgcgok@webmail-5.ucs.uwa.edu.au>
	<1216105222.11309.77.camel@sib-sblomber01d.sib.uq.edu.au>
Message-ID: <4949c7e60807150133i66780d0fxc7a0547e4f8ea7d7@mail.gmail.com>

2008/7/15 Simon Blomberg <s.blomberg1 at uq.edu.au>:
> On Tue, 2008-07-15 at 13:43 +0800, Julie Marsh wrote:
>
>>
>>
>> Given that I am using 2 different tests for two different hypotheses I
>> still would have expected these p-values to be more similar.
>
> Well, as Pinheiro and Bates say in their book (worth reading!), the LRT
> for mixed effects models is anti-conservative. So your LRT p-value is
> almost certainly too small. The posterior p-value might be more
> accurate, if you accept the usual caveats re: priors and convergence
> etc. Also, when calculating p-values by hand using pchisq, you should
> probably use pchisq(..., lower.tail=FALSE) instead of 1-pchisq(...),
> which is inaccurate. The log.p option might also be useful if you really
> need to compare small probabilities. And why were you using pchisq with
> 0 df (which always == 1)? I don't understand that at all.

Regarding the mixture of a chi-square distribution, this is more
appropriate when testing a single variance component. The ordinary
test with one df is conservative, since the test is on the boundary of
the parameter space. But Julie is not testing a variance component, so
the mixture is not appropriate here.

I can think of three reasons, that the p-values Julie obtains are
different in the likelihood ratio test and the posterior sampling.

1) the extensive use of control parameters indicates that convergence
may be an issue. If one or both the models have not reached
convergence, obviously the likelihood ratio test is based on wrong
likelihoods and will be misleading. I suppose the MCMC sampling will
also be inappropriate in this situation. The need for control
parameters could also indicate that some problems are related to the
data structure such as severe unbalance. Perhaps there is too little
information on some parameters and convergence is hard to achieve?

If the models converged nicely and problems with data or models are
unlikely, I would be inclined to trust the likelihood ratio test. It
is a test on one df with 3000+ observations, so as far as I know,
there should be no problems.

2) the MCMC sampling could be influenced by the priors or the chain
could get stuck in a specific region.

3) which version of lmer are you using? Douglas Bates recently posted
a message to this list reporting problems with MCMCsamp in the most
recent version of lme4.

/Rune

>
>> I am so
>> sorry that I can't post the data and the lmer output but I am bound by
>> confidentiality. <big sigh>  I understand completely if it is not
>> possible to provide any help given this lack of further information.
>>
>> I have eagerly read and re-read the rwiki help page  .........
>>
>> http://wiki.r-project.org/rwiki/doku.php?id=guides:lmer-tests
>>
>> ....... but still am unable to explain why the results should be so
>> different. Much as I would love to argue against the reliance on
>> p-values I'm afraid I am a resigned pragmatist when it comes to trying
>> to get anything published.  <sorry!>  Needless to say I will swamp the
>> article with far more informative plots and CI's.
>>
>> Any help would be very much appreciated.
>>
>> kindest regards,  julie marsh.
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> --
> Simon Blomberg, BSc (Hons), PhD, MAppStat.
> Lecturer and Consultant Statistician
> Faculty of Biological and Chemical Sciences
> The University of Queensland
> St. Lucia Queensland 4072
> Australia
> Room 320 Goddard Building (8)
> T: +61 7 3365 2506
> http://www.uq.edu.au/~uqsblomb
> email: S.Blomberg1_at_uq.edu.au
>
> Policies:
> 1.  I will NOT analyse your data for you.
> 2.  Your deadline is your problem.
>
> The combination of some data and an aching desire for
> an answer does not ensure that a reasonable answer can
> be extracted from a given body of data. - John Tukey.
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



-- 
Rune Haubo Bojesen Christensen

Master Student, M.Sc. Eng.
Phone: (+45) 30 26 45 54
Mail: rhbc at imm.dtu.dk, rune.haubo at gmail.com

DTU Informatics, Section for Statistics
Technical University of Denmark, Build.321, DK-2800 Kgs. Lyngby, Denmark



From bates at stat.wisc.edu  Tue Jul 15 15:02:06 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 15 Jul 2008 08:02:06 -0500
Subject: [R-sig-ME] lmer: LRT and mcmcpvalue for fixed effects
In-Reply-To: <4949c7e60807150133i66780d0fxc7a0547e4f8ea7d7@mail.gmail.com>
References: <20080715134338.oi73vl4n68cgcgok@webmail-5.ucs.uwa.edu.au>
	<1216105222.11309.77.camel@sib-sblomber01d.sib.uq.edu.au>
	<4949c7e60807150133i66780d0fxc7a0547e4f8ea7d7@mail.gmail.com>
Message-ID: <40e66e0b0807150602g34d387c4pd3e67c320d31e5f0@mail.gmail.com>

On Tue, Jul 15, 2008 at 3:33 AM, Rune Haubo <rhbc at imm.dtu.dk> wrote:
> 2008/7/15 Simon Blomberg <s.blomberg1 at uq.edu.au>:
>> On Tue, 2008-07-15 at 13:43 +0800, Julie Marsh wrote:
>>
>>>
>>>
>>> Given that I am using 2 different tests for two different hypotheses I
>>> still would have expected these p-values to be more similar.
>>
>> Well, as Pinheiro and Bates say in their book (worth reading!), the LRT
>> for mixed effects models is anti-conservative. So your LRT p-value is
>> almost certainly too small. The posterior p-value might be more
>> accurate, if you accept the usual caveats re: priors and convergence
>> etc. Also, when calculating p-values by hand using pchisq, you should
>> probably use pchisq(..., lower.tail=FALSE) instead of 1-pchisq(...),
>> which is inaccurate. The log.p option might also be useful if you really
>> need to compare small probabilities. And why were you using pchisq with
>> 0 df (which always == 1)? I don't understand that at all.
>
> Regarding the mixture of a chi-square distribution, this is more
> appropriate when testing a single variance component. The ordinary
> test with one df is conservative, since the test is on the boundary of
> the parameter space. But Julie is not testing a variance component, so
> the mixture is not appropriate here.
>
> I can think of three reasons, that the p-values Julie obtains are
> different in the likelihood ratio test and the posterior sampling.

> 1) the extensive use of control parameters indicates that convergence
> may be an issue. If one or both the models have not reached
> convergence, obviously the likelihood ratio test is based on wrong
> likelihoods and will be misleading. I suppose the MCMC sampling will
> also be inappropriate in this situation. The need for control
> parameters could also indicate that some problems are related to the
> data structure such as severe unbalance. Perhaps there is too little
> information on some parameters and convergence is hard to achieve?

I think that the use of those control parameters is based on examples
in various postings to email lists or in papers.  As you mention
below, it is important to take note of the version of the lme4 package
being used.  Most of those control parameters are, to quote Ron
Ziegler, President Nixon's press secretary, "no longer operative".  In
a way, it's a pity.  Some of the best mathematics I have ever done was
the derivation of the analytic gradient and the ECME updates for the
profiled deviance and then I found that the use of these elegant
formulas actually slowed things down on large problems.  (There's a
lesson there for those who think that statistical computing research
consists of deriving a formula and letting others worry about the
trivial implementation details.)

I guess my question would be if it really is necessary to set the
maximum number of iterations so high.  (By the way, changing the
maximum number of iterations doesn't have an effect in the current
version of the lme4 package.  I will reimplement it in the next
release.)

> If the models converged nicely and problems with data or models are
> unlikely, I would be inclined to trust the likelihood ratio test. It
> is a test on one df with 3000+ observations, so as far as I know,
> there should be no problems.

I agree.  If convergence is smooth then the likelihood ratio statistic
of 46 on 1 degree of freedom is definitive.  I imagine that the
t-statistic for that coefficient is also large in magnitude (close to
+/- 7).  In those realms, assigning a numeric p-value is less
important than the information that "it is very, very small".

> 2) the MCMC sampling could be influenced by the priors or the chain
> could get stuck in a specific region.

Yes, I would definitely want to look at the xyplot of the mcmcsamp
output, just to see if the chain managed to get itself stuck
somewhere.

> 3) which version of lmer are you using? Douglas Bates recently posted
> a message to this list reporting problems with MCMCsamp in the most
> recent version of lme4.

Indeed - although I am more concerned about the sample of the variance
component parameters than about the sample from the distribution of
the fixed-effects parameters.  If the sample from the variance
component parameters does not have the correct properties that will,
of course, affect the sample from the distribution of the
fixed-effects parameters but I am more confident of the conditional
sampling of the fixed effects (it is just a multivariate normal - I
could still manage to botch that but it is less likely than is my
getting the more complicated distributions wrong) than of the
conditional sampling of the variance component parameters.

However, given the description of the problem the mcmcsamp would need
to have been created using lme4 version 0.99875-x (because versions
0.999375-y can't yet handle vector-valued random effects).

I think the bottom line is that the likelihood ratio test and the
t-statistic are definitive and there is no need to follow up with the
mcmcsamp approach.

> /Rune
>
>>
>>> I am so
>>> sorry that I can't post the data and the lmer output but I am bound by
>>> confidentiality. <big sigh>  I understand completely if it is not
>>> possible to provide any help given this lack of further information.
>>>
>>> I have eagerly read and re-read the rwiki help page  .........
>>>
>>> http://wiki.r-project.org/rwiki/doku.php?id=guides:lmer-tests
>>>
>>> ....... but still am unable to explain why the results should be so
>>> different. Much as I would love to argue against the reliance on
>>> p-values I'm afraid I am a resigned pragmatist when it comes to trying
>>> to get anything published.  <sorry!>  Needless to say I will swamp the
>>> article with far more informative plots and CI's.
>>>
>>> Any help would be very much appreciated.
>>>
>>> kindest regards,  julie marsh.
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> --
>> Simon Blomberg, BSc (Hons), PhD, MAppStat.
>> Lecturer and Consultant Statistician
>> Faculty of Biological and Chemical Sciences
>> The University of Queensland
>> St. Lucia Queensland 4072
>> Australia
>> Room 320 Goddard Building (8)
>> T: +61 7 3365 2506
>> http://www.uq.edu.au/~uqsblomb
>> email: S.Blomberg1_at_uq.edu.au
>>
>> Policies:
>> 1.  I will NOT analyse your data for you.
>> 2.  Your deadline is your problem.
>>
>> The combination of some data and an aching desire for
>> an answer does not ensure that a reasonable answer can
>> be extracted from a given body of data. - John Tukey.
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>
>
> --
> Rune Haubo Bojesen Christensen
>
> Master Student, M.Sc. Eng.
> Phone: (+45) 30 26 45 54
> Mail: rhbc at imm.dtu.dk, rune.haubo at gmail.com
>
> DTU Informatics, Section for Statistics
> Technical University of Denmark, Build.321, DK-2800 Kgs. Lyngby, Denmark
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From gangchen at mail.nih.gov  Wed Jul 16 17:21:51 2008
From: gangchen at mail.nih.gov (Gang Chen)
Date: Wed, 16 Jul 2008 11:21:51 -0400
Subject: [R-sig-ME] Generalized least squares program
Message-ID: <ED1C7813-C437-4BB1-A1A8-73DF16DB55CD@mail.nih.gov>

I'm using function gls in nlme to run some extended linear model with  
generalized least squares many many times in a loop with the same  
design matrix X and same residual covariance structure but different  
values for the response variable. A lot of time is wasted in  
reformulating the same model structures and overhead matrix  
calculations. Such a waste makes the runtime range from one day to  
one month. Are there some slots available in gls that would allow me  
to avoid the unnecessary waste? If not, is there a counterpart of gls  
for extended linear model with generalized least squares available in  
lme4 that would give such slots?

Thanks,
Gang



From smckinney at bccrc.ca  Wed Jul 16 19:29:16 2008
From: smckinney at bccrc.ca (Steven McKinney)
Date: Wed, 16 Jul 2008 10:29:16 -0700
Subject: [R-sig-ME] Generalized least squares program
References: <ED1C7813-C437-4BB1-A1A8-73DF16DB55CD@mail.nih.gov>
Message-ID: <0BE438149FF2254DB4199E2682C8DFEB0328A39E@crcmail1.BCCRC.CA>

Hi Gang, 

Have you tried the predict() function?
You should be able to hand off new data
to your saved fitted model object.  
See 
?predict.gls
for an example.

HTH

Steve McKinney

-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org on behalf of Gang Chen
Sent: Wed 7/16/2008 8:21 AM
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Generalized least squares program
 
I'm using function gls in nlme to run some extended linear model with  
generalized least squares many many times in a loop with the same  
design matrix X and same residual covariance structure but different  
values for the response variable. A lot of time is wasted in  
reformulating the same model structures and overhead matrix  
calculations. Such a waste makes the runtime range from one day to  
one month. Are there some slots available in gls that would allow me  
to avoid the unnecessary waste? If not, is there a counterpart of gls  
for extended linear model with generalized least squares available in  
lme4 that would give such slots?

Thanks,
Gang

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From gangchen at mail.nih.gov  Wed Jul 16 20:34:32 2008
From: gangchen at mail.nih.gov (Gang Chen)
Date: Wed, 16 Jul 2008 14:34:32 -0400
Subject: [R-sig-ME] Generalized least squares program
In-Reply-To: <0BE438149FF2254DB4199E2682C8DFEB0328A39E@crcmail1.BCCRC.CA>
References: <ED1C7813-C437-4BB1-A1A8-73DF16DB55CD@mail.nih.gov>
	<0BE438149FF2254DB4199E2682C8DFEB0328A39E@crcmail1.BCCRC.CA>
Message-ID: <7889C8AE-59C6-428E-B3E7-F53BBC3BEB7E@mail.nih.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080716/22160a2b/attachment.pl>

From smckinney at bccrc.ca  Wed Jul 16 22:32:18 2008
From: smckinney at bccrc.ca (Steven McKinney)
Date: Wed, 16 Jul 2008 13:32:18 -0700
Subject: [R-sig-ME] Generalized least squares program
References: <ED1C7813-C437-4BB1-A1A8-73DF16DB55CD@mail.nih.gov>
	<0BE438149FF2254DB4199E2682C8DFEB0328A39E@crcmail1.BCCRC.CA>
	<7889C8AE-59C6-428E-B3E7-F53BBC3BEB7E@mail.nih.gov>
Message-ID: <0BE438149FF2254DB4199E2682C8DFEB0328A39F@crcmail1.BCCRC.CA>

Hi Gang,

Sorry, I should have said "update()" instead
of "predict()".

However, a timing test on a small example shows no
considerable gain to updating an existing object
compared to refitting a new object.  This may be different
for your scenario.  If not, then it's an exercise of
diving into the code base to figure out how to adapt
the update.gls() function or related fitting
functions so you can run your extra fits without
recomputing everything.


> require("nlme")
> 
> Ovary <- Ovary
> set.seed(123)
> Ovary$jitterFollicles <- Ovary$follicles + sample(((-2):2), nrow(Ovary), replace = TRUE)
> 
> fm1 <- gls(follicles ~ sin(2*pi*Time) + cos(2*pi*Time), Ovary,
+            correlation = corAR1(form = ~ 1 | Mare))
> fm2 <- update(fm1, model. = jitterFollicles ~ .)
> 
> fm3 <-  gls(jitterFollicles ~ sin(2*pi*Time) + cos(2*pi*Time), Ovary,
+            correlation = corAR1(form = ~ 1 | Mare))
> 
> all.equal(fitted(fm2), fitted(fm3))
[1] TRUE
> 
> system.time(
+             for(i in 1:100) {
+               fm2 <- update(fm1, model. = jitterFollicles ~ .)
+             }
+             )
   user  system elapsed 
 12.022   5.238  17.264 
> system.time(
+             for(i in 1:100) {
+               fm3 <-  gls(jitterFollicles ~ sin(2*pi*Time) + cos(2*pi*Time), Ovary,
+                           correlation = corAR1(form = ~ 1 | Mare))
+             }
+             )
   user  system elapsed 
 11.898   5.227  17.126 
> 

Steve McKinney


-----Original Message-----
From: Gang Chen [mailto:gangchen at mail.nih.gov]
Sent: Wed 7/16/2008 11:34 AM
To: Steven McKinney
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Generalized least squares program
 
Thanks for the suggestion!

It seems to me predict() is the opposite of what I'm looking for.  
With an extended linear model

Y = X*b + e, e ~ N(0, R s^2), R s^2 is the  covariance matrix of the  
residual term e

both X and the structure of R remain the same across all the loops in  
my case, and Y is different and so are the estimates of b and R s^2  
from one iteration to another.

Gang


On Jul 16, 2008, at 1:29 PM, Steven McKinney wrote:

> Hi Gang,
>
> Have you tried the predict() function?
> You should be able to hand off new data
> to your saved fitted model object.
> See
> ?predict.gls
> for an example.
>
> HTH
>
> Steve McKinney
>
> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org on behalf of Gang Chen
> Sent: Wed 7/16/2008 8:21 AM
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] Generalized least squares program
>
> I'm using function gls in nlme to run some extended linear model with
> generalized least squares many many times in a loop with the same
> design matrix X and same residual covariance structure but different
> values for the response variable. A lot of time is wasted in
> reformulating the same model structures and overhead matrix
> calculations. Such a waste makes the runtime range from one day to
> one month. Are there some slots available in gls that would allow me
> to avoid the unnecessary waste? If not, is there a counterpart of gls
> for extended linear model with generalized least squares available in
> lme4 that would give such slots?
>
> Thanks,
> Gang
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From gangchen at mail.nih.gov  Wed Jul 16 23:23:46 2008
From: gangchen at mail.nih.gov (Gang Chen)
Date: Wed, 16 Jul 2008 17:23:46 -0400
Subject: [R-sig-ME] Generalized least squares program
In-Reply-To: <0BE438149FF2254DB4199E2682C8DFEB0328A39F@crcmail1.BCCRC.CA>
References: <ED1C7813-C437-4BB1-A1A8-73DF16DB55CD@mail.nih.gov>
	<0BE438149FF2254DB4199E2682C8DFEB0328A39E@crcmail1.BCCRC.CA>
	<7889C8AE-59C6-428E-B3E7-F53BBC3BEB7E@mail.nih.gov>
	<0BE438149FF2254DB4199E2682C8DFEB0328A39F@crcmail1.BCCRC.CA>
Message-ID: <29C516F4-604F-4AC8-A646-93FDBE4E36A9@mail.nih.gov>

No, update() doesn't make much difference in my case either. I've  
been reluctant to dive into the source code, but I may have to do so  
if no better choices turn up.

Thanks again,
Gang


On Jul 16, 2008, at 4:32 PM, Steven McKinney wrote:

> Hi Gang,
>
> Sorry, I should have said "update()" instead
> of "predict()".
>
> However, a timing test on a small example shows no
> considerable gain to updating an existing object
> compared to refitting a new object.  This may be different
> for your scenario.  If not, then it's an exercise of
> diving into the code base to figure out how to adapt
> the update.gls() function or related fitting
> functions so you can run your extra fits without
> recomputing everything.
>
>
>> require("nlme")
>>
>> Ovary <- Ovary
>> set.seed(123)
>> Ovary$jitterFollicles <- Ovary$follicles + sample(((-2):2), nrow 
>> (Ovary), replace = TRUE)
>>
>> fm1 <- gls(follicles ~ sin(2*pi*Time) + cos(2*pi*Time), Ovary,
> +            correlation = corAR1(form = ~ 1 | Mare))
>> fm2 <- update(fm1, model. = jitterFollicles ~ .)
>>
>> fm3 <-  gls(jitterFollicles ~ sin(2*pi*Time) + cos(2*pi*Time), Ovary,
> +            correlation = corAR1(form = ~ 1 | Mare))
>>
>> all.equal(fitted(fm2), fitted(fm3))
> [1] TRUE
>>
>> system.time(
> +             for(i in 1:100) {
> +               fm2 <- update(fm1, model. = jitterFollicles ~ .)
> +             }
> +             )
>    user  system elapsed
>  12.022   5.238  17.264
>> system.time(
> +             for(i in 1:100) {
> +               fm3 <-  gls(jitterFollicles ~ sin(2*pi*Time) + cos 
> (2*pi*Time), Ovary,
> +                           correlation = corAR1(form = ~ 1 | Mare))
> +             }
> +             )
>    user  system elapsed
>  11.898   5.227  17.126
>>
>
> Steve McKinney
>
>
> -----Original Message-----
> From: Gang Chen [mailto:gangchen at mail.nih.gov]
> Sent: Wed 7/16/2008 11:34 AM
> To: Steven McKinney
> Cc: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Generalized least squares program
>
> Thanks for the suggestion!
>
> It seems to me predict() is the opposite of what I'm looking for.
> With an extended linear model
>
> Y = X*b + e, e ~ N(0, R s^2), R s^2 is the  covariance matrix of the
> residual term e
>
> both X and the structure of R remain the same across all the loops in
> my case, and Y is different and so are the estimates of b and R s^2
> from one iteration to another.
>
> Gang
>
>
> On Jul 16, 2008, at 1:29 PM, Steven McKinney wrote:
>
>> Hi Gang,
>>
>> Have you tried the predict() function?
>> You should be able to hand off new data
>> to your saved fitted model object.
>> See
>> ?predict.gls
>> for an example.
>>
>> HTH
>>
>> Steve McKinney
>>
>> -----Original Message-----
>> From: r-sig-mixed-models-bounces at r-project.org on behalf of Gang Chen
>> Sent: Wed 7/16/2008 8:21 AM
>> To: r-sig-mixed-models at r-project.org
>> Subject: [R-sig-ME] Generalized least squares program
>>
>> I'm using function gls in nlme to run some extended linear model with
>> generalized least squares many many times in a loop with the same
>> design matrix X and same residual covariance structure but different
>> values for the response variable. A lot of time is wasted in
>> reformulating the same model structures and overhead matrix
>> calculations. Such a waste makes the runtime range from one day to
>> one month. Are there some slots available in gls that would allow me
>> to avoid the unnecessary waste? If not, is there a counterpart of gls
>> for extended linear model with generalized least squares available in
>> lme4 that would give such slots?
>>
>> Thanks,
>> Gang
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>



From rhbc at imm.dtu.dk  Thu Jul 17 09:50:14 2008
From: rhbc at imm.dtu.dk (Rune Haubo)
Date: Thu, 17 Jul 2008 09:50:14 +0200
Subject: [R-sig-ME] [R] Likelihood ratio test between glm and glmer fits
In-Reply-To: <20080716202205.1vwoiebr19co0c0s@webmail4.kuleuven.be>
References: <2768A5B569B1D54EA47861B9A05422E10397AA18@jade1604.UTSARR.NET>
	<20080716202205.1vwoiebr19co0c0s@webmail4.kuleuven.be>
Message-ID: <4949c7e60807170050u1b1e9d68p5fe7b4d569364b2e@mail.gmail.com>

2008/7/16 Dimitris Rizopoulos <Dimitris.Rizopoulos at med.kuleuven.be>:
> well, for computing the p-value you need to use pchisq() and dchisq() (check
> ?dchisq for more info). For model fits with a logLik method you can directly
> use the following simple function:
>
> lrt <- function (obj1, obj2) {
>    L0 <- logLik(obj1)
>    L1 <- logLik(obj2)
>    L01 <- as.vector(- 2 * (L0 - L1))
>    df <- attr(L1, "df") - attr(L0, "df")
>    list(L01 = L01, df = df,
>        "p-value" = pchisq(L01, df, lower.tail = FALSE))
> }
>
> library(lme4)
> gm0 <- glm(cbind(incidence, size - incidence) ~ period,
>              family = binomial, data = cbpp)
> gm1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),
>              family = binomial, data = cbpp)
>
> lrt(gm0, gm1)

Yes, that seems quite natural, but then try to compare with the deviance:

logLik(gm0)
logLik(gm1)

(d0 <- deviance(gm0))
(d1 <- deviance(gm1))
(LR <- d0 - d1)
pchisq(LR, 1, lower = FALSE)

Obviously the deviance in glm is *not* twice the negative
log-likelihood as it is in glmer. The question remains which of these
two quantities is appropriate for comparison. I am not sure exactly
how the deviance and/or log-likelihood are calculated in glmer, but my
feeling is that one should trust the deviance rather than the
log-likelihoods for these purposes. This is supported by the following
comparison: Ad an arbitrary random effect with a close-to-zero
variance and note the deviance:

tmp <- rep(1:4, each = nrow(cbpp)/4)
gm2 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | tmp),
             family = binomial, data = cbpp)
(d2 <- deviance(gm2))

This deviance is very close to that obtained from the glm model.

I have included the mixed-models mailing list in the hope that someone
could explain how the deviance is computed in glmer and why deviances,
but not likelihoods are comparable to glm-fits.

Best
Rune

>
>
> However, there are some issues regarding this likelihood ratio test.
>
> 1) The null hypothesis is on the boundary of the parameter space, i.e., you
> test whether the variance for the random effect is zero. For this case the
> assumed chi-squared distribution for the LRT may *not* be totally
> appropriate and may produce conservative p-values. There is some theory
> regarding this issue, which has shown that the reference distribution for
> the LRT in this case is a mixture of a chi-squared(df = 0) and
> chi-squared(df = 1). Another option is to use simulation-based approach
> where you can approximate the reference distribution of the LRT under the
> null using simulation. You may check below for an illustration of this
> procedure (not-tested):
>
> X <- model.matrix(gm0)
> coefs <- coef(gm0)
> pr <- plogis(c(X %*% coefs))
> n <- length(pr)
> new.dat <- cbpp
> Tobs <- lrt(gm0, gm1)$L01
> B <- 200
> out.T <- numeric(B)
> for (b in 1:B) {
>    y <- rbinom(n, cbpp$size, pr)
>    new.dat$incidence <- y
>    fit0 <- glm(formula(gm0), family = binomial, data = new.dat)
>    fit1 <- glmer(formula(gm1), family = binomial, data = new.dat)
>    out.T[b] <- lrt(fit0, fit1)$L01
> }
> # estimate p-value
> (sum(out.T >= Tobs) + 1) / (B + 1)
>
>
> 2) For the glmer fit you have to note that you work with an approximation to
> the log-likelihood (obtained using numerical integration) and not the actual
> log-likelihood.
>
> I hope it helps.
>
> Best,
> Dimitris
>
> ----
> Dimitris Rizopoulos
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
>
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/(0)16/336899
> Fax: +32/(0)16/337015
> Web: http://med.kuleuven.be/biostat/
>     http://www.student.kuleuven.be/~m0390867/dimitris.htm
>
>
> Quoting COREY SPARKS <corey.sparks at UTSA.EDU>:
>
>> Dear list,
>> I am fitting a logistic multi-level regression model and need to  test the
>> difference between the ordinary logistic regression from a  glm() fit and
>> the mixed effects fit from glmer(), basically I want  to do a likelihood
>> ratio test between the two fits.
>>
>>
>> The data are like this:
>> My outcome is a (1,0) for health status, I have several (1,0) dummy
>>  variables RURAL, SMOKE, DRINK, EMPLOYED, highereduc, INDIG, male,
>>  divorced, SINGLE, chronic, vigor_d and moderat_d and AGE is  continuous (20
>> to 100).
>> My higher level is called munid and has 581 levels.
>> The data have 45243 observations.
>>
>> Here are my program statements:
>>
>> #GLM fit
>>
>> ph.fit.2<-glm(poorhealth~RURAL+SMOKE+DRINK+EMPLOYED+highereduc+INDIG+AGE+male+divorced+SINGLE+chronic+vigor_d+moderat_d,family=binomial(),
>>  data=mx.merge)
>> #GLMER fit
>>
>> ph.fit.3<-glmer(poorhealth~RURAL+SMOKE+DRINK+EMPLOYED+INSURANCE+highereduc+INDIG+AGE+male+divorced+SINGLE+chronic+vigor_d+moderat_d+(1|munid),family=binomial(),
>>  data=mx.merge)
>>
>> I cannot find a method in R that will do the LR test between a glm  and a
>> glmer fit, so I try to do it using the liklihoods from both  models
>>
>> #form the likelihood ratio test between the glm and glmer fits
>> x2<--2*(logLik(ph.fit.2)-logLik(ph.fit.3))
>>
>>>    ML
>>
>> 79.60454
>> attr(,"nobs")
>>    n
>> 45243
>> attr(,"nall")
>>    n
>> 45243
>> attr(,"df")
>> [1] 14
>> attr(,"REML")
>> [1] FALSE
>> attr(,"class")
>> [1] "logLik"
>>
>> #Get the associated p-value
>> dchisq(x2,14)
>>         ML
>>>
>>> 5.94849e-15
>>
>> Which looks like an improvement in model fit to me.  Am I seeing  this
>> correctly or are the two models even able to be compared? they  are both
>> estimated via maximum likelihood, so they should be, I think.
>> Any help would be appreciated.
>>
>> Corey
>>
>> Corey S. Sparks, Ph.D.
>>
>> Assistant Professor
>> Department of Demography and Organization Studies
>> University of Texas San Antonio
>> One UTSA Circle
>> San Antonio, TX 78249
>> email:corey.sparks at utsa.edu
>> web: https://rowdyspace.utsa.edu/users/ozd504/www/index.htm
>>
>>
>>        [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>
>
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Rune Haubo Bojesen Christensen

Master Student, M.Sc. Eng.
Phone: (+45) 30 26 45 54
Mail: rhbc at imm.dtu.dk, rune.haubo at gmail.com

DTU Informatics, Section for Statistics
Technical University of Denmark, Build.321, DK-2800 Kgs. Lyngby, Denmark



From bates at stat.wisc.edu  Thu Jul 17 13:29:41 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 17 Jul 2008 06:29:41 -0500
Subject: [R-sig-ME] [R] Likelihood ratio test between glm and glmer fits
In-Reply-To: <4949c7e60807170050u1b1e9d68p5fe7b4d569364b2e@mail.gmail.com>
References: <2768A5B569B1D54EA47861B9A05422E10397AA18@jade1604.UTSARR.NET>
	<20080716202205.1vwoiebr19co0c0s@webmail4.kuleuven.be>
	<4949c7e60807170050u1b1e9d68p5fe7b4d569364b2e@mail.gmail.com>
Message-ID: <40e66e0b0807170429kb626492qa2215811e719e0d@mail.gmail.com>

On Thu, Jul 17, 2008 at 2:50 AM, Rune Haubo <rhbc at imm.dtu.dk> wrote:
> 2008/7/16 Dimitris Rizopoulos <Dimitris.Rizopoulos at med.kuleuven.be>:
>> well, for computing the p-value you need to use pchisq() and dchisq() (check
>> ?dchisq for more info). For model fits with a logLik method you can directly
>> use the following simple function:
>>
>> lrt <- function (obj1, obj2) {
>>    L0 <- logLik(obj1)
>>    L1 <- logLik(obj2)
>>    L01 <- as.vector(- 2 * (L0 - L1))
>>    df <- attr(L1, "df") - attr(L0, "df")
>>    list(L01 = L01, df = df,
>>        "p-value" = pchisq(L01, df, lower.tail = FALSE))
>> }
>>
>> library(lme4)
>> gm0 <- glm(cbind(incidence, size - incidence) ~ period,
>>              family = binomial, data = cbpp)
>> gm1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),
>>              family = binomial, data = cbpp)
>>
>> lrt(gm0, gm1)
>
> Yes, that seems quite natural, but then try to compare with the deviance:
>
> logLik(gm0)
> logLik(gm1)
>
> (d0 <- deviance(gm0))
> (d1 <- deviance(gm1))
> (LR <- d0 - d1)
> pchisq(LR, 1, lower = FALSE)
>
> Obviously the deviance in glm is *not* twice the negative
> log-likelihood as it is in glmer. The question remains which of these
> two quantities is appropriate for comparison. I am not sure exactly
> how the deviance and/or log-likelihood are calculated in glmer, but my
> feeling is that one should trust the deviance rather than the
> log-likelihoods for these purposes. This is supported by the following
> comparison: Ad an arbitrary random effect with a close-to-zero
> variance and note the deviance:
>
> tmp <- rep(1:4, each = nrow(cbpp)/4)
> gm2 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | tmp),
>             family = binomial, data = cbpp)
> (d2 <- deviance(gm2))
>
> This deviance is very close to that obtained from the glm model.
>
> I have included the mixed-models mailing list in the hope that someone
> could explain how the deviance is computed in glmer and why deviances,
> but not likelihoods are comparable to glm-fits.

In that example I think the problem may be that I have not yet written
the code to adjust the deviance of the glmer fit for the null
deviance.

>> However, there are some issues regarding this likelihood ratio test.
>>
>> 1) The null hypothesis is on the boundary of the parameter space, i.e., you
>> test whether the variance for the random effect is zero. For this case the
>> assumed chi-squared distribution for the LRT may *not* be totally
>> appropriate and may produce conservative p-values. There is some theory
>> regarding this issue, which has shown that the reference distribution for
>> the LRT in this case is a mixture of a chi-squared(df = 0) and
>> chi-squared(df = 1). Another option is to use simulation-based approach
>> where you can approximate the reference distribution of the LRT under the
>> null using simulation. You may check below for an illustration of this
>> procedure (not-tested):
>>
>> X <- model.matrix(gm0)
>> coefs <- coef(gm0)
>> pr <- plogis(c(X %*% coefs))
>> n <- length(pr)
>> new.dat <- cbpp
>> Tobs <- lrt(gm0, gm1)$L01
>> B <- 200
>> out.T <- numeric(B)
>> for (b in 1:B) {
>>    y <- rbinom(n, cbpp$size, pr)
>>    new.dat$incidence <- y
>>    fit0 <- glm(formula(gm0), family = binomial, data = new.dat)
>>    fit1 <- glmer(formula(gm1), family = binomial, data = new.dat)
>>    out.T[b] <- lrt(fit0, fit1)$L01
>> }
>> # estimate p-value
>> (sum(out.T >= Tobs) + 1) / (B + 1)
>>
>>
>> 2) For the glmer fit you have to note that you work with an approximation to
>> the log-likelihood (obtained using numerical integration) and not the actual
>> log-likelihood.
>>
>> I hope it helps.
>>
>> Best,
>> Dimitris
>>
>> ----
>> Dimitris Rizopoulos
>> Biostatistical Centre
>> School of Public Health
>> Catholic University of Leuven
>>
>> Address: Kapucijnenvoer 35, Leuven, Belgium
>> Tel: +32/(0)16/336899
>> Fax: +32/(0)16/337015
>> Web: http://med.kuleuven.be/biostat/
>>     http://www.student.kuleuven.be/~m0390867/dimitris.htm
>>
>>
>> Quoting COREY SPARKS <corey.sparks at UTSA.EDU>:
>>
>>> Dear list,
>>> I am fitting a logistic multi-level regression model and need to  test the
>>> difference between the ordinary logistic regression from a  glm() fit and
>>> the mixed effects fit from glmer(), basically I want  to do a likelihood
>>> ratio test between the two fits.
>>>
>>>
>>> The data are like this:
>>> My outcome is a (1,0) for health status, I have several (1,0) dummy
>>>  variables RURAL, SMOKE, DRINK, EMPLOYED, highereduc, INDIG, male,
>>>  divorced, SINGLE, chronic, vigor_d and moderat_d and AGE is  continuous (20
>>> to 100).
>>> My higher level is called munid and has 581 levels.
>>> The data have 45243 observations.
>>>
>>> Here are my program statements:
>>>
>>> #GLM fit
>>>
>>> ph.fit.2<-glm(poorhealth~RURAL+SMOKE+DRINK+EMPLOYED+highereduc+INDIG+AGE+male+divorced+SINGLE+chronic+vigor_d+moderat_d,family=binomial(),
>>>  data=mx.merge)
>>> #GLMER fit
>>>
>>> ph.fit.3<-glmer(poorhealth~RURAL+SMOKE+DRINK+EMPLOYED+INSURANCE+highereduc+INDIG+AGE+male+divorced+SINGLE+chronic+vigor_d+moderat_d+(1|munid),family=binomial(),
>>>  data=mx.merge)
>>>
>>> I cannot find a method in R that will do the LR test between a glm  and a
>>> glmer fit, so I try to do it using the liklihoods from both  models
>>>
>>> #form the likelihood ratio test between the glm and glmer fits
>>> x2<--2*(logLik(ph.fit.2)-logLik(ph.fit.3))
>>>
>>>>    ML
>>>
>>> 79.60454
>>> attr(,"nobs")
>>>    n
>>> 45243
>>> attr(,"nall")
>>>    n
>>> 45243
>>> attr(,"df")
>>> [1] 14
>>> attr(,"REML")
>>> [1] FALSE
>>> attr(,"class")
>>> [1] "logLik"
>>>
>>> #Get the associated p-value
>>> dchisq(x2,14)
>>>         ML
>>>>
>>>> 5.94849e-15
>>>
>>> Which looks like an improvement in model fit to me.  Am I seeing  this
>>> correctly or are the two models even able to be compared? they  are both
>>> estimated via maximum likelihood, so they should be, I think.
>>> Any help would be appreciated.
>>>
>>> Corey
>>>
>>> Corey S. Sparks, Ph.D.
>>>
>>> Assistant Professor
>>> Department of Demography and Organization Studies
>>> University of Texas San Antonio
>>> One UTSA Circle
>>> San Antonio, TX 78249
>>> email:corey.sparks at utsa.edu
>>> web: https://rowdyspace.utsa.edu/users/ozd504/www/index.htm
>>>
>>>
>>>        [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>
>>
>>
>> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>
>
> --
> Rune Haubo Bojesen Christensen
>
> Master Student, M.Sc. Eng.
> Phone: (+45) 30 26 45 54
> Mail: rhbc at imm.dtu.dk, rune.haubo at gmail.com
>
> DTU Informatics, Section for Statistics
> Technical University of Denmark, Build.321, DK-2800 Kgs. Lyngby, Denmark
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



From bates at stat.wisc.edu  Thu Jul 17 18:40:19 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 17 Jul 2008 11:40:19 -0500
Subject: [R-sig-ME] Generalized least squares program
In-Reply-To: <29C516F4-604F-4AC8-A646-93FDBE4E36A9@mail.nih.gov>
References: <ED1C7813-C437-4BB1-A1A8-73DF16DB55CD@mail.nih.gov>
	<0BE438149FF2254DB4199E2682C8DFEB0328A39E@crcmail1.BCCRC.CA>
	<7889C8AE-59C6-428E-B3E7-F53BBC3BEB7E@mail.nih.gov>
	<0BE438149FF2254DB4199E2682C8DFEB0328A39F@crcmail1.BCCRC.CA>
	<29C516F4-604F-4AC8-A646-93FDBE4E36A9@mail.nih.gov>
Message-ID: <40e66e0b0807170940t2f4e4d78v858adf5b5340cc48@mail.gmail.com>

In general update applied to a fitted model object modifies the
original call then evaluates the modified call.  Hence there will be
no gain in speed.  The refit function in the lme4 package replaces the
response and refits the model with the new response.  If there was
such a function for a gls object that would be what you would want to
use.  However, there is no refit method for gls objects.

On Wed, Jul 16, 2008 at 4:23 PM, Gang Chen <gangchen at mail.nih.gov> wrote:
> No, update() doesn't make much difference in my case either. I've been
> reluctant to dive into the source code, but I may have to do so if no better
> choices turn up.
>
> Thanks again,
> Gang
>
>
> On Jul 16, 2008, at 4:32 PM, Steven McKinney wrote:
>
>> Hi Gang,
>>
>> Sorry, I should have said "update()" instead
>> of "predict()".
>>
>> However, a timing test on a small example shows no
>> considerable gain to updating an existing object
>> compared to refitting a new object.  This may be different
>> for your scenario.  If not, then it's an exercise of
>> diving into the code base to figure out how to adapt
>> the update.gls() function or related fitting
>> functions so you can run your extra fits without
>> recomputing everything.
>>
>>
>>> require("nlme")
>>>
>>> Ovary <- Ovary
>>> set.seed(123)
>>> Ovary$jitterFollicles <- Ovary$follicles + sample(((-2):2), nrow(Ovary),
>>> replace = TRUE)
>>>
>>> fm1 <- gls(follicles ~ sin(2*pi*Time) + cos(2*pi*Time), Ovary,
>>
>> +            correlation = corAR1(form = ~ 1 | Mare))
>>>
>>> fm2 <- update(fm1, model. = jitterFollicles ~ .)
>>>
>>> fm3 <-  gls(jitterFollicles ~ sin(2*pi*Time) + cos(2*pi*Time), Ovary,
>>
>> +            correlation = corAR1(form = ~ 1 | Mare))
>>>
>>> all.equal(fitted(fm2), fitted(fm3))
>>
>> [1] TRUE
>>>
>>> system.time(
>>
>> +             for(i in 1:100) {
>> +               fm2 <- update(fm1, model. = jitterFollicles ~ .)
>> +             }
>> +             )
>>   user  system elapsed
>>  12.022   5.238  17.264
>>>
>>> system.time(
>>
>> +             for(i in 1:100) {
>> +               fm3 <-  gls(jitterFollicles ~ sin(2*pi*Time) +
>> cos(2*pi*Time), Ovary,
>> +                           correlation = corAR1(form = ~ 1 | Mare))
>> +             }
>> +             )
>>   user  system elapsed
>>  11.898   5.227  17.126
>>>
>>
>> Steve McKinney
>>
>>
>> -----Original Message-----
>> From: Gang Chen [mailto:gangchen at mail.nih.gov]
>> Sent: Wed 7/16/2008 11:34 AM
>> To: Steven McKinney
>> Cc: r-sig-mixed-models at r-project.org
>> Subject: Re: [R-sig-ME] Generalized least squares program
>>
>> Thanks for the suggestion!
>>
>> It seems to me predict() is the opposite of what I'm looking for.
>> With an extended linear model
>>
>> Y = X*b + e, e ~ N(0, R s^2), R s^2 is the  covariance matrix of the
>> residual term e
>>
>> both X and the structure of R remain the same across all the loops in
>> my case, and Y is different and so are the estimates of b and R s^2
>> from one iteration to another.
>>
>> Gang
>>
>>
>> On Jul 16, 2008, at 1:29 PM, Steven McKinney wrote:
>>
>>> Hi Gang,
>>>
>>> Have you tried the predict() function?
>>> You should be able to hand off new data
>>> to your saved fitted model object.
>>> See
>>> ?predict.gls
>>> for an example.
>>>
>>> HTH
>>>
>>> Steve McKinney
>>>
>>> -----Original Message-----
>>> From: r-sig-mixed-models-bounces at r-project.org on behalf of Gang Chen
>>> Sent: Wed 7/16/2008 8:21 AM
>>> To: r-sig-mixed-models at r-project.org
>>> Subject: [R-sig-ME] Generalized least squares program
>>>
>>> I'm using function gls in nlme to run some extended linear model with
>>> generalized least squares many many times in a loop with the same
>>> design matrix X and same residual covariance structure but different
>>> values for the response variable. A lot of time is wasted in
>>> reformulating the same model structures and overhead matrix
>>> calculations. Such a waste makes the runtime range from one day to
>>> one month. Are there some slots available in gls that would allow me
>>> to avoid the unnecessary waste? If not, is there a counterpart of gls
>>> for extended linear model with generalized least squares available in
>>> lme4 that would give such slots?
>>>
>>> Thanks,
>>> Gang
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>
>>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From gangchen at mail.nih.gov  Thu Jul 17 23:33:20 2008
From: gangchen at mail.nih.gov (Gang Chen)
Date: Thu, 17 Jul 2008 17:33:20 -0400
Subject: [R-sig-ME] Generalized least squares program
In-Reply-To: <40e66e0b0807170940t2f4e4d78v858adf5b5340cc48@mail.gmail.com>
References: <ED1C7813-C437-4BB1-A1A8-73DF16DB55CD@mail.nih.gov>
	<0BE438149FF2254DB4199E2682C8DFEB0328A39E@crcmail1.BCCRC.CA>
	<7889C8AE-59C6-428E-B3E7-F53BBC3BEB7E@mail.nih.gov>
	<0BE438149FF2254DB4199E2682C8DFEB0328A39F@crcmail1.BCCRC.CA>
	<29C516F4-604F-4AC8-A646-93FDBE4E36A9@mail.nih.gov>
	<40e66e0b0807170940t2f4e4d78v858adf5b5340cc48@mail.gmail.com>
Message-ID: <99CB4BB1-C220-4E14-83E7-1DDF62E1B2C0@mail.nih.gov>

Dr. Bates,

Thanks a lot for the clarification!

Is there any plan to have a gls method implemented in lme4 so that I  
could take advantage of the refit function?

Also does lmer in lme4 allow for specifying covariance structure for  
the residuals? If yes, is there a way to trick lmer so that I can run  
gls by somehow zeroing the across-group random effect Z*b in model y  
= X*beta + Z*b + e?

Thanks,
Gang


On Jul 17, 2008, at 12:40 PM, Douglas Bates wrote:

> In general update applied to a fitted model object modifies the
> original call then evaluates the modified call.  Hence there will be
> no gain in speed.  The refit function in the lme4 package replaces the
> response and refits the model with the new response.  If there was
> such a function for a gls object that would be what you would want to
> use.  However, there is no refit method for gls objects.
>
> On Wed, Jul 16, 2008 at 4:23 PM, Gang Chen <gangchen at mail.nih.gov>  
> wrote:
>> No, update() doesn't make much difference in my case either. I've  
>> been
>> reluctant to dive into the source code, but I may have to do so if  
>> no better
>> choices turn up.
>>
>> Thanks again,
>> Gang
>>
>>
>> On Jul 16, 2008, at 4:32 PM, Steven McKinney wrote:
>>
>>> Hi Gang,
>>>
>>> Sorry, I should have said "update()" instead
>>> of "predict()".
>>>
>>> However, a timing test on a small example shows no
>>> considerable gain to updating an existing object
>>> compared to refitting a new object.  This may be different
>>> for your scenario.  If not, then it's an exercise of
>>> diving into the code base to figure out how to adapt
>>> the update.gls() function or related fitting
>>> functions so you can run your extra fits without
>>> recomputing everything.
>>>
>>>
>>>> require("nlme")
>>>>
>>>> Ovary <- Ovary
>>>> set.seed(123)
>>>> Ovary$jitterFollicles <- Ovary$follicles + sample(((-2):2), nrow 
>>>> (Ovary),
>>>> replace = TRUE)
>>>>
>>>> fm1 <- gls(follicles ~ sin(2*pi*Time) + cos(2*pi*Time), Ovary,
>>>
>>> +            correlation = corAR1(form = ~ 1 | Mare))
>>>>
>>>> fm2 <- update(fm1, model. = jitterFollicles ~ .)
>>>>
>>>> fm3 <-  gls(jitterFollicles ~ sin(2*pi*Time) + cos(2*pi*Time),  
>>>> Ovary,
>>>
>>> +            correlation = corAR1(form = ~ 1 | Mare))
>>>>
>>>> all.equal(fitted(fm2), fitted(fm3))
>>>
>>> [1] TRUE
>>>>
>>>> system.time(
>>>
>>> +             for(i in 1:100) {
>>> +               fm2 <- update(fm1, model. = jitterFollicles ~ .)
>>> +             }
>>> +             )
>>>   user  system elapsed
>>>  12.022   5.238  17.264
>>>>
>>>> system.time(
>>>
>>> +             for(i in 1:100) {
>>> +               fm3 <-  gls(jitterFollicles ~ sin(2*pi*Time) +
>>> cos(2*pi*Time), Ovary,
>>> +                           correlation = corAR1(form = ~ 1 | Mare))
>>> +             }
>>> +             )
>>>   user  system elapsed
>>>  11.898   5.227  17.126
>>>>
>>>
>>> Steve McKinney
>>>
>>>
>>> -----Original Message-----
>>> From: Gang Chen [mailto:gangchen at mail.nih.gov]
>>> Sent: Wed 7/16/2008 11:34 AM
>>> To: Steven McKinney
>>> Cc: r-sig-mixed-models at r-project.org
>>> Subject: Re: [R-sig-ME] Generalized least squares program
>>>
>>> Thanks for the suggestion!
>>>
>>> It seems to me predict() is the opposite of what I'm looking for.
>>> With an extended linear model
>>>
>>> Y = X*b + e, e ~ N(0, R s^2), R s^2 is the  covariance matrix of the
>>> residual term e
>>>
>>> both X and the structure of R remain the same across all the  
>>> loops in
>>> my case, and Y is different and so are the estimates of b and R s^2
>>> from one iteration to another.
>>>
>>> Gang
>>>
>>>
>>> On Jul 16, 2008, at 1:29 PM, Steven McKinney wrote:
>>>
>>>> Hi Gang,
>>>>
>>>> Have you tried the predict() function?
>>>> You should be able to hand off new data
>>>> to your saved fitted model object.
>>>> See
>>>> ?predict.gls
>>>> for an example.
>>>>
>>>> HTH
>>>>
>>>> Steve McKinney
>>>>
>>>> -----Original Message-----
>>>> From: r-sig-mixed-models-bounces at r-project.org on behalf of Gang  
>>>> Chen
>>>> Sent: Wed 7/16/2008 8:21 AM
>>>> To: r-sig-mixed-models at r-project.org
>>>> Subject: [R-sig-ME] Generalized least squares program
>>>>
>>>> I'm using function gls in nlme to run some extended linear model  
>>>> with
>>>> generalized least squares many many times in a loop with the same
>>>> design matrix X and same residual covariance structure but  
>>>> different
>>>> values for the response variable. A lot of time is wasted in
>>>> reformulating the same model structures and overhead matrix
>>>> calculations. Such a waste makes the runtime range from one day to
>>>> one month. Are there some slots available in gls that would  
>>>> allow me
>>>> to avoid the unnecessary waste? If not, is there a counterpart  
>>>> of gls
>>>> for extended linear model with generalized least squares  
>>>> available in
>>>> lme4 that would give such slots?
>>>>
>>>> Thanks,
>>>> Gang
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>
>>>
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>



From HDoran at air.org  Fri Jul 18 02:09:36 2008
From: HDoran at air.org (Doran, Harold)
Date: Thu, 17 Jul 2008 20:09:36 -0400
Subject: [R-sig-ME] Generalized least squares program
References: <ED1C7813-C437-4BB1-A1A8-73DF16DB55CD@mail.nih.gov><0BE438149FF2254DB4199E2682C8DFEB0328A39E@crcmail1.BCCRC.CA><7889C8AE-59C6-428E-B3E7-F53BBC3BEB7E@mail.nih.gov><0BE438149FF2254DB4199E2682C8DFEB0328A39F@crcmail1.BCCRC.CA><29C516F4-604F-4AC8-A646-93FDBE4E36A9@mail.nih.gov><40e66e0b0807170940t2f4e4d78v858adf5b5340cc48@mail.gmail.com>
	<99CB4BB1-C220-4E14-83E7-1DDF62E1B2C0@mail.nih.gov>
Message-ID: <ED7B522EE00C9A4FA515AA71724D61EE2718A2@DC1EXCL01.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080717/bb0dd5bf/attachment.pl>

From goran.brostrom at gmail.com  Sat Jul 19 23:51:42 2008
From: goran.brostrom at gmail.com (=?UTF-8?Q?G=C3=B6ran_Brostr=C3=B6m?=)
Date: Sat, 19 Jul 2008 23:51:42 +0200
Subject: [R-sig-ME] [R] Likelihood ratio test between glm and glmer fits
In-Reply-To: <40e66e0b0807170429kb626492qa2215811e719e0d@mail.gmail.com>
References: <2768A5B569B1D54EA47861B9A05422E10397AA18@jade1604.UTSARR.NET>
	<20080716202205.1vwoiebr19co0c0s@webmail4.kuleuven.be>
	<4949c7e60807170050u1b1e9d68p5fe7b4d569364b2e@mail.gmail.com>
	<40e66e0b0807170429kb626492qa2215811e719e0d@mail.gmail.com>
Message-ID: <148ed8180807191451r17e43ce8w420e95588a98466f@mail.gmail.com>

This particular case with a random intercept model can be handled by
glmmML, by bootstrapping the p-value.

Best, G?ran

On Thu, Jul 17, 2008 at 1:29 PM, Douglas Bates <bates at stat.wisc.edu> wrote:
> On Thu, Jul 17, 2008 at 2:50 AM, Rune Haubo <rhbc at imm.dtu.dk> wrote:
>> 2008/7/16 Dimitris Rizopoulos <Dimitris.Rizopoulos at med.kuleuven.be>:
>>> well, for computing the p-value you need to use pchisq() and dchisq() (check
>>> ?dchisq for more info). For model fits with a logLik method you can directly
>>> use the following simple function:
>>>
>>> lrt <- function (obj1, obj2) {
>>>    L0 <- logLik(obj1)
>>>    L1 <- logLik(obj2)
>>>    L01 <- as.vector(- 2 * (L0 - L1))
>>>    df <- attr(L1, "df") - attr(L0, "df")
>>>    list(L01 = L01, df = df,
>>>        "p-value" = pchisq(L01, df, lower.tail = FALSE))
>>> }
>>>
>>> library(lme4)
>>> gm0 <- glm(cbind(incidence, size - incidence) ~ period,
>>>              family = binomial, data = cbpp)
>>> gm1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),
>>>              family = binomial, data = cbpp)
>>>
>>> lrt(gm0, gm1)
>>
>> Yes, that seems quite natural, but then try to compare with the deviance:
>>
>> logLik(gm0)
>> logLik(gm1)
>>
>> (d0 <- deviance(gm0))
>> (d1 <- deviance(gm1))
>> (LR <- d0 - d1)
>> pchisq(LR, 1, lower = FALSE)
>>
>> Obviously the deviance in glm is *not* twice the negative
>> log-likelihood as it is in glmer. The question remains which of these
>> two quantities is appropriate for comparison. I am not sure exactly
>> how the deviance and/or log-likelihood are calculated in glmer, but my
>> feeling is that one should trust the deviance rather than the
>> log-likelihoods for these purposes. This is supported by the following
>> comparison: Ad an arbitrary random effect with a close-to-zero
>> variance and note the deviance:
>>
>> tmp <- rep(1:4, each = nrow(cbpp)/4)
>> gm2 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | tmp),
>>             family = binomial, data = cbpp)
>> (d2 <- deviance(gm2))
>>
>> This deviance is very close to that obtained from the glm model.
>>
>> I have included the mixed-models mailing list in the hope that someone
>> could explain how the deviance is computed in glmer and why deviances,
>> but not likelihoods are comparable to glm-fits.
>
> In that example I think the problem may be that I have not yet written
> the code to adjust the deviance of the glmer fit for the null
> deviance.
>
>>> However, there are some issues regarding this likelihood ratio test.
>>>
>>> 1) The null hypothesis is on the boundary of the parameter space, i.e., you
>>> test whether the variance for the random effect is zero. For this case the
>>> assumed chi-squared distribution for the LRT may *not* be totally
>>> appropriate and may produce conservative p-values. There is some theory
>>> regarding this issue, which has shown that the reference distribution for
>>> the LRT in this case is a mixture of a chi-squared(df = 0) and
>>> chi-squared(df = 1). Another option is to use simulation-based approach
>>> where you can approximate the reference distribution of the LRT under the
>>> null using simulation. You may check below for an illustration of this
>>> procedure (not-tested):
>>>
>>> X <- model.matrix(gm0)
>>> coefs <- coef(gm0)
>>> pr <- plogis(c(X %*% coefs))
>>> n <- length(pr)
>>> new.dat <- cbpp
>>> Tobs <- lrt(gm0, gm1)$L01
>>> B <- 200
>>> out.T <- numeric(B)
>>> for (b in 1:B) {
>>>    y <- rbinom(n, cbpp$size, pr)
>>>    new.dat$incidence <- y
>>>    fit0 <- glm(formula(gm0), family = binomial, data = new.dat)
>>>    fit1 <- glmer(formula(gm1), family = binomial, data = new.dat)
>>>    out.T[b] <- lrt(fit0, fit1)$L01
>>> }
>>> # estimate p-value
>>> (sum(out.T >= Tobs) + 1) / (B + 1)
>>>
>>>
>>> 2) For the glmer fit you have to note that you work with an approximation to
>>> the log-likelihood (obtained using numerical integration) and not the actual
>>> log-likelihood.
>>>
>>> I hope it helps.
>>>
>>> Best,
>>> Dimitris
>>>
>>> ----
>>> Dimitris Rizopoulos
>>> Biostatistical Centre
>>> School of Public Health
>>> Catholic University of Leuven
>>>
>>> Address: Kapucijnenvoer 35, Leuven, Belgium
>>> Tel: +32/(0)16/336899
>>> Fax: +32/(0)16/337015
>>> Web: http://med.kuleuven.be/biostat/
>>>     http://www.student.kuleuven.be/~m0390867/dimitris.htm
>>>
>>>
>>> Quoting COREY SPARKS <corey.sparks at UTSA.EDU>:
>>>
>>>> Dear list,
>>>> I am fitting a logistic multi-level regression model and need to  test the
>>>> difference between the ordinary logistic regression from a  glm() fit and
>>>> the mixed effects fit from glmer(), basically I want  to do a likelihood
>>>> ratio test between the two fits.
>>>>
>>>>
>>>> The data are like this:
>>>> My outcome is a (1,0) for health status, I have several (1,0) dummy
>>>>  variables RURAL, SMOKE, DRINK, EMPLOYED, highereduc, INDIG, male,
>>>>  divorced, SINGLE, chronic, vigor_d and moderat_d and AGE is  continuous (20
>>>> to 100).
>>>> My higher level is called munid and has 581 levels.
>>>> The data have 45243 observations.
>>>>
>>>> Here are my program statements:
>>>>
>>>> #GLM fit
>>>>
>>>> ph.fit.2<-glm(poorhealth~RURAL+SMOKE+DRINK+EMPLOYED+highereduc+INDIG+AGE+male+divorced+SINGLE+chronic+vigor_d+moderat_d,family=binomial(),
>>>>  data=mx.merge)
>>>> #GLMER fit
>>>>
>>>> ph.fit.3<-glmer(poorhealth~RURAL+SMOKE+DRINK+EMPLOYED+INSURANCE+highereduc+INDIG+AGE+male+divorced+SINGLE+chronic+vigor_d+moderat_d+(1|munid),family=binomial(),
>>>>  data=mx.merge)
>>>>
>>>> I cannot find a method in R that will do the LR test between a glm  and a
>>>> glmer fit, so I try to do it using the liklihoods from both  models
>>>>
>>>> #form the likelihood ratio test between the glm and glmer fits
>>>> x2<--2*(logLik(ph.fit.2)-logLik(ph.fit.3))
>>>>
>>>>>    ML
>>>>
>>>> 79.60454
>>>> attr(,"nobs")
>>>>    n
>>>> 45243
>>>> attr(,"nall")
>>>>    n
>>>> 45243
>>>> attr(,"df")
>>>> [1] 14
>>>> attr(,"REML")
>>>> [1] FALSE
>>>> attr(,"class")
>>>> [1] "logLik"
>>>>
>>>> #Get the associated p-value
>>>> dchisq(x2,14)
>>>>         ML
>>>>>
>>>>> 5.94849e-15
>>>>
>>>> Which looks like an improvement in model fit to me.  Am I seeing  this
>>>> correctly or are the two models even able to be compared? they  are both
>>>> estimated via maximum likelihood, so they should be, I think.
>>>> Any help would be appreciated.
>>>>
>>>> Corey
>>>>
>>>> Corey S. Sparks, Ph.D.
>>>>
>>>> Assistant Professor
>>>> Department of Demography and Organization Studies
>>>> University of Texas San Antonio
>>>> One UTSA Circle
>>>> San Antonio, TX 78249
>>>> email:corey.sparks at utsa.edu
>>>> web: https://rowdyspace.utsa.edu/users/ozd504/www/index.htm
>>>>
>>>>
>>>>        [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>>
>>>
>>>
>>>
>>> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>>
>> --
>> Rune Haubo Bojesen Christensen
>>
>> Master Student, M.Sc. Eng.
>> Phone: (+45) 30 26 45 54
>> Mail: rhbc at imm.dtu.dk, rune.haubo at gmail.com
>>
>> DTU Informatics, Section for Statistics
>> Technical University of Denmark, Build.321, DK-2800 Kgs. Lyngby, Denmark
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
G?ran Brostr?m

From gangchen at mail.nih.gov  Tue Jul 22 22:50:26 2008
From: gangchen at mail.nih.gov (Gang Chen)
Date: Tue, 22 Jul 2008 16:50:26 -0400
Subject: [R-sig-ME] varFixFact
Message-ID: <69669323-AD09-4898-A9B9-47DD9F76ADAF@mail.nih.gov>

I have trouble figuring out what is attribute "varFixFact" in  
component "fixDF" of a lme object. Using the data Rail in nlme as an  
example,

 > library(nlme)
 > flme <- lme(travel ~ 1, data=Rail, random = ~1 | Rail)
 > attr(flme$fixDF, "varFixFact")
          [,1]
[1,] 10.17104

I couldn't find any information how this number of 10.17104 is  
calculated through googling, but is this somehow related to the  
covariance matrix of the fixed effects?

If I run lmer from lme4 on the same data,

 > library(lme4)
 > flmer <- lmer(travel ~ 1 + (1|Rail), data=Rail)

how can I obtain this attribute in lme based on the slots of an lmer  
object, flmer?

Thank you,
Gang



From cotter.rs at gmail.com  Wed Jul 23 12:14:38 2008
From: cotter.rs at gmail.com (R.S. Cotter)
Date: Wed, 23 Jul 2008 12:14:38 +0200
Subject: [R-sig-ME] Logistisc regression (lmer) fitted by the Laplace
	approximation- references?
Message-ID: <742479270807230314g36a3cbbfx1284bad2bda457a6@mail.gmail.com>

Dear all,

I have sucessfully run Logistic regression by using generalized linear
mixed-effects model (lmer) fitted by the Laplace approximation (lme4
package).

Is there any limits that I should aware of by use of this model? I
haven't found references for this model, could somone provide me with
a reference from a article/book?

My response is Yes or No and explanatory variables is categories
(A,B,C and D), and random effect (ID, number of groups 7).

Regards RS



From cotter.rs at gmail.com  Wed Jul 23 12:09:38 2008
From: cotter.rs at gmail.com (R.S. Cotter)
Date: Wed, 23 Jul 2008 12:09:38 +0200
Subject: [R-sig-ME] Logistisc regression (lmer) fitted by the Laplace
	approximation- references?
Message-ID: <742479270807230309s50d8e5d6n6b1269cf5383157f@mail.gmail.com>

Dear all,

I have sucessfully run Logistic regression by using generalized linear
mixed-effects model (lmer) fitted by the Laplace approximation (lme4
package).

Is there any limits that I should aware of by use of this model? I
haven't found references for this model, could somone provide me with
a reference from a article/book?

My response is Yes or No and explanatory variables is categories
(A,B,C and D), and random effect (ID, number of groups 7).

Regards RS



From HDoran at air.org  Wed Jul 23 14:46:17 2008
From: HDoran at air.org (Doran, Harold)
Date: Wed, 23 Jul 2008 08:46:17 -0400
Subject: [R-sig-ME] Logistisc regression (lmer) fitted by the
	Laplaceapproximation- references?
In-Reply-To: <742479270807230309s50d8e5d6n6b1269cf5383157f@mail.gmail.com>
Message-ID: <ED7B522EE00C9A4FA515AA71724D61EEB8652B@DC1EXCL01.air.org>

Do you mean limitations of the laplace approximation or limits of a glmm
fit by lmer? If it is the former, one possible argument is that lmer
uses a second-order taylor series expansion. One might claim that going
out further in the taylor series is needed to get good approximations of
the integral. I, however, may not be one of those people. 

I know for example that the HLM software package uses a 6 order taylor
series. Just last week, I had to replicate some work. I used lmer and
HLM to compare output. The R output and the HLM output matched exactly
to the 3th decimal place for the BLUPS and the fixed effects.

Even using lmer for IRT (rasch) work gives the same point esimtates for
the items (when they are treated as fixed effects) as I get in other IRT
software packages that use estimating algorithms other than laplace. 



> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org 
> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf 
> Of R.S. Cotter
> Sent: Wednesday, July 23, 2008 6:10 AM
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] Logistisc regression (lmer) fitted by the 
> Laplaceapproximation- references?
> 
> Dear all,
> 
> I have sucessfully run Logistic regression by using 
> generalized linear mixed-effects model (lmer) fitted by the 
> Laplace approximation (lme4 package).
> 
> Is there any limits that I should aware of by use of this 
> model? I haven't found references for this model, could 
> somone provide me with a reference from a article/book?
> 
> My response is Yes or No and explanatory variables is 
> categories (A,B,C and D), and random effect (ID, number of groups 7).
> 
> Regards RS
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 



From danw at sussex.ac.uk  Wed Jul 23 15:19:02 2008
From: danw at sussex.ac.uk (Dan Wright)
Date: Wed, 23 Jul 2008 14:19:02 +0100
Subject: [R-sig-ME] Logistisc regression (lmer) fitted by the
 Laplace	approximation- references?
In-Reply-To: <742479270807230314g36a3cbbfx1284bad2bda457a6@mail.gmail.com>
References: <742479270807230314g36a3cbbfx1284bad2bda457a6@mail.gmail.com>
Message-ID: <2B9E799F5E0B0C033431CAD5@psycho934.lifesci.susx.ac.uk>

Not sure what you mean limits, but if you want a very intro description see 
<http://www.sussex.ac.uk/Users/danw/pdf/multibjmspproofs.pdf>.

Dan

ps. That website works now and will for a few weeks. I move in a few weeks, 
and my current employer may want to get rid of the space I take up on the 
server, but hopefully I will have it up at my new location soon.

--On 23 July 2008 12:14 +0200 "R.S. Cotter" <cotter.rs at gmail.com> wrote:

> Dear all,
>
> I have sucessfully run Logistic regression by using generalized linear
> mixed-effects model (lmer) fitted by the Laplace approximation (lme4
> package).
>
> Is there any limits that I should aware of by use of this model? I
> haven't found references for this model, could somone provide me with
> a reference from a article/book?
>
> My response is Yes or No and explanatory variables is categories
> (A,B,C and D), and random effect (ID, number of groups 7).
>
> Regards RS
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models




Daniel B. Wright
Until August                     After August
Psychology Department            Psychology Department
University of Sussex             Florida International University
BN1 9QH, UK                      11200 S.W. 8th Street
danw at sussex.ac.uk                Miami, FL 33199, USA
http://www.sussex.ac.uk/Users/danw/



From bates at stat.wisc.edu  Wed Jul 23 16:57:23 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 23 Jul 2008 09:57:23 -0500
Subject: [R-sig-ME] Logistisc regression (lmer) fitted by the
	Laplaceapproximation- references?
In-Reply-To: <ED7B522EE00C9A4FA515AA71724D61EEB8652B@DC1EXCL01.air.org>
References: <742479270807230309s50d8e5d6n6b1269cf5383157f@mail.gmail.com>
	<ED7B522EE00C9A4FA515AA71724D61EEB8652B@DC1EXCL01.air.org>
Message-ID: <40e66e0b0807230757w3e1e5917y9d56bb13ea60aa38@mail.gmail.com>

On Wed, Jul 23, 2008 at 7:46 AM, Doran, Harold <HDoran at air.org> wrote:
> Do you mean limitations of the laplace approximation or limits of a glmm
> fit by lmer? If it is the former, one possible argument is that lmer
> uses a second-order taylor series expansion. One might claim that going
> out further in the taylor series is needed to get good approximations of
> the integral. I, however, may not be one of those people.

> I know for example that the HLM software package uses a 6 order taylor
> series. Just last week, I had to replicate some work. I used lmer and
> HLM to compare output. The R output and the HLM output matched exactly
> to the 3th decimal place for the BLUPS and the fixed effects

I'm not sure what HLM does in terms of a 6th order Taylor series
expansion.  Before you can make sense of terms like "second-order
Taylor series expansion" versus "6th order" you should make clear what
is being approximated and, more importantly, where.

It has taken me a long time to get straight in my mind exactly what is
being approximated, and I am the co-author of papers from more than 10
years ago about this approximation.  These days the phrase that I use
is that we want to approximate the integral of "the unscaled
conditional density of the random effects given the observed data".
(See slides 61 to 65 in
http://www.stat.wisc.edu/~bates/IMPS2008/lme4D.pdf).

Given the observed data and values of the parameters, the procedure in
glmer is to determine the conditional modes of the random effects
(i.e. the values that maximize the conditional density).  The quantity
that determines the Laplace approximation used in glmer is the
second-order Taylor series approximation of the logarithm of the
conditional density at the conditional modes.

I did read the paper that Steve Raudenbush wrote about the 6th order
Taylor series approximation used in HLM but I have forgotten the
details.  At this point I would need to rephrase it in these terms
before I could compare the method to that used in glmer.  I believe
the method requires a single, simple, scalar random-effects term in
the model.  That is, it cannot be used with multiple grouping factors
or with vector-valued random effects.  Most higher-order
approximations require the simple model form because that is the only
way that you can take the integral with respect to the vector and
split it into scalar integrals.

I don't think it is betraying a confidence to say that Steve
Raudenbush told me that after he and his co-author had done
considerable work deriving the 6th order Laplacian approximation they
compared it to adaptive Gauss-Hermite quadrature (AGQ) and found that
AGQ with a small number of quadrature points (3 to 5) is more
accurate.  Bin Dai is implementing AGS in lme4 for models with a
single grouping factor.  These include  models with a single, scalar-
or vector-valued random-effects term and models with multiple
random-effects terms based on the same grouping factor.

> Even using lmer for IRT (rasch) work gives the same point esimtates for
> the items (when they are treated as fixed effects) as I get in other IRT
> software packages that use estimating algorithms other than laplace.

But the thing that lme4 can do that I don't think much other software
can do is fit an IRT model with random effects for the items and for
the subjects.  The sorts of extensions described in "Explanatory Item
Response Models", edited by Paul De Boeck and Mark Wilson (Springer,
2004) can also be fit in lme4.
>
>
>> -----Original Message-----
>> From: r-sig-mixed-models-bounces at r-project.org
>> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf
>> Of R.S. Cotter
>> Sent: Wednesday, July 23, 2008 6:10 AM
>> To: r-sig-mixed-models at r-project.org
>> Subject: [R-sig-ME] Logistisc regression (lmer) fitted by the
>> Laplaceapproximation- references?
>>
>> Dear all,
>>
>> I have sucessfully run Logistic regression by using
>> generalized linear mixed-effects model (lmer) fitted by the
>> Laplace approximation (lme4 package).
>>
>> Is there any limits that I should aware of by use of this
>> model? I haven't found references for this model, could
>> somone provide me with a reference from a article/book?
>>
>> My response is Yes or No and explanatory variables is
>> categories (A,B,C and D), and random effect (ID, number of groups 7).
>>
>> Regards RS
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From kjbeath at kagi.com  Thu Jul 24 05:36:34 2008
From: kjbeath at kagi.com (Ken Beath)
Date: Thu, 24 Jul 2008 13:36:34 +1000
Subject: [R-sig-ME] Logistisc regression (lmer) fitted by the Laplace
	approximation- references?
In-Reply-To: <742479270807230314g36a3cbbfx1284bad2bda457a6@mail.gmail.com>
References: <742479270807230314g36a3cbbfx1284bad2bda457a6@mail.gmail.com>
Message-ID: <ECFF3420-F21F-438A-828E-2508ACA23503@kagi.com>

On 23/07/2008, at 8:14 PM, R.S. Cotter wrote:

> Dear all,
>
> I have sucessfully run Logistic regression by using generalized linear
> mixed-effects model (lmer) fitted by the Laplace approximation (lme4
> package).
>
> Is there any limits that I should aware of by use of this model? I
> haven't found references for this model, could somone provide me with
> a reference from a article/book?
>
> My response is Yes or No and explanatory variables is categories
> (A,B,C and D), and random effect (ID, number of groups 7).
>

Accuracy of Laplace approximation is determined by size of clusters  
(larger is better) and variance of random effects (smaller is better).  
Unfortunately I don't know of a reference that describes when it does  
and doesn't work.

The only work I know is Pinheiro and Chao, Journal of Computational  
and Graphical Statistics vol 15 p58-81 which has some simulations for  
2-level models. My impression is that for random effect variances of 1  
or less and average cluster sizes of 6 or more then Laplace will work  
quite well for fixed effects but maybe not so well for estimates of  
the variance components, not that these are usually important. There  
may also be something on PQL in one of the papers by Rodriguez and  
Goldman, which as Laplace is better than PQL can be used as a guide.  
Generalized Latent Variable Modeling: Multilevel, Longitudinal, and  
Structural Equation Models by Skrondal and Rabe-Hesketh has an  
introduction to the integration methods.

Simulation could be used to show that the level of bias is small.

Ken



From HStevens at muohio.edu  Fri Jul 25 23:28:47 2008
From: HStevens at muohio.edu (M Henry H Stevens)
Date: Fri, 25 Jul 2008 17:28:47 -0400
Subject: [R-sig-ME] missing data in lme, lmer, PROC MIXED
Message-ID: <1217021327.8446.17.camel@stevenmh-desktop>

Hi folks,
I have colleagues who comfortably state that "missing data" are ok in
"mixed models" - because "the program (PROC MIXED) handles missing data
-- I have a hard time imagining what it does.

To those of you who use both R and SAS, I was wondering if you might
share insight into what these do. 

As far as I know, for lme:
'na.action="na.omit" ' or na.exclude, removes the rows with any missing
data.

Thanks ,
Hank





-- 
M Henry H Stevens <HStevens at muohio.edu>



From bates at stat.wisc.edu  Fri Jul 25 23:48:28 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 25 Jul 2008 16:48:28 -0500
Subject: [R-sig-ME] missing data in lme, lmer, PROC MIXED
In-Reply-To: <1217021327.8446.17.camel@stevenmh-desktop>
References: <1217021327.8446.17.camel@stevenmh-desktop>
Message-ID: <40e66e0b0807251448i6eeb74f5rc748da33c8040825@mail.gmail.com>

On Fri, Jul 25, 2008 at 4:28 PM, M Henry H Stevens <HStevens at muohio.edu> wrote:
> Hi folks,
> I have colleagues who comfortably state that "missing data" are ok in
> "mixed models" - because "the program (PROC MIXED) handles missing data
> -- I have a hard time imagining what it does.

> To those of you who use both R and SAS, I was wondering if you might
> share insight into what these do.

Perhaps they are referring to a formulation of models for repeated
measures data where you view the responses as in the form of a matrix
for subjects and occasions.  Similarly, in item response models the
data are often represented as a matrix.  In that case one can have
missing cells in the matrix but those observations are not present in
any way when you switch from the "wide" format to the "long" format.
(See ?reshape for more information on wide and long formats for
repeated measures data.)

> As far as I know, for lme:
> 'na.action="na.omit" ' or na.exclude, removes the rows with any missing
> data.

Yes.  And that's the only sensible way of handling missing cells in
the matrix if you want to view the data in the wide format.  However
some old computational methods always assumed that you had a full
matrix layout and had to fudge the case of missing cells in that
layout.



From kjbeath at kagi.com  Sun Jul 27 04:39:26 2008
From: kjbeath at kagi.com (Ken Beath)
Date: Sun, 27 Jul 2008 12:39:26 +1000
Subject: [R-sig-ME] missing data in lme, lmer, PROC MIXED
In-Reply-To: <1217021327.8446.17.camel@stevenmh-desktop>
References: <1217021327.8446.17.camel@stevenmh-desktop>
Message-ID: <13CEFB6E-06AE-4982-BD8F-CD74A3DAABA7@kagi.com>

On 26/07/2008, at 7:28 AM, M Henry H Stevens wrote:

> Hi folks,
> I have colleagues who comfortably state that "missing data" are ok in
> "mixed models" - because "the program (PROC MIXED) handles missing  
> data
> -- I have a hard time imagining what it does.
>
> To those of you who use both R and SAS, I was wondering if you might
> share insight into what these do.
>
> As far as I know, for lme:
> 'na.action="na.omit" ' or na.exclude, removes the rows with any  
> missing
> data.
>

This depends. If the missing data is the dependent and it is missing  
at random then as mixed models are fitted using maximum likelihood it  
will produce results that are optimal. Roughly (there are some really  
technical definitions for missing data and I haven't checked them) if  
we don't know the outcome and the reason it is missing isn't due to  
its value or the other data then we can simply leave it out of the  
likelihood equation it as it has no useful information. A problem is  
when data being missing provides this sort of information and is very  
difficult to model. An example is if observations above a certain  
value are more likely to be missing.

An alternative method of dealing with repeated data is to produce a  
summary for each subject or cluster, for example by averaging the last  
three visits. This doesn't correctly handle missing data although the  
loss in efficiency is usually small and it can work well, provided  
only a small proportion is missing.

What R and SAS don't deal with directly is missing data in the  
covariates. This takes a bit more work, for example using multiple  
imputation. Here the complete case method where an observation with  
any missing data is removed will result in a loss of efficiency  
compared to what can be achieved.

Ken



From HStevens at muohio.edu  Mon Jul 28 13:04:19 2008
From: HStevens at muohio.edu (M Henry H Stevens)
Date: Mon, 28 Jul 2008 07:04:19 -0400
Subject: [R-sig-ME] missing data in lme, lmer, PROC MIXED
In-Reply-To: <13CEFB6E-06AE-4982-BD8F-CD74A3DAABA7@kagi.com>
References: <1217021327.8446.17.camel@stevenmh-desktop>
	<13CEFB6E-06AE-4982-BD8F-CD74A3DAABA7@kagi.com>
Message-ID: <1217243059.5947.33.camel@stevenmh-desktop>

Thanks Ken. I have been assuming that they meant missing covariates (a
subject provided most of the predictors, but not all). So I take it that
SAS does no imputation on its own-that the user would need to do that
(if they wanted?). lme does not do anything like that.

Hank

On Sat, 2008-07-26 at 22:39 -0400, Ken Beath wrote:
> On 26/07/2008, at 7:28 AM, M Henry H Stevens wrote:
> 
> > Hi folks,
> > I have colleagues who comfortably state that "missing data" are ok in
> > "mixed models" - because "the program (PROC MIXED) handles missing
> > data
> > -- I have a hard time imagining what it does.
> >
> > To those of you who use both R and SAS, I was wondering if you might
> > share insight into what these do.
> >
> > As far as I know, for lme:
> > 'na.action="na.omit" ' or na.exclude, removes the rows with any
> > missing
> > data.
> >
> 
> This depends. If the missing data is the dependent and it is missing
> at random then as mixed models are fitted using maximum likelihood it
> will produce results that are optimal. Roughly (there are some really
> technical definitions for missing data and I haven't checked them) if
> we don't know the outcome and the reason it is missing isn't due to
> its value or the other data then we can simply leave it out of the
> likelihood equation it as it has no useful information. A problem is
> when data being missing provides this sort of information and is very
> difficult to model. An example is if observations above a certain
> value are more likely to be missing.
> 
> An alternative method of dealing with repeated data is to produce a
> summary for each subject or cluster, for example by averaging the last
> three visits. This doesn't correctly handle missing data although the
> loss in efficiency is usually small and it can work well, provided
> only a small proportion is missing.
> 
> What R and SAS don't deal with directly is missing data in the
> covariates. This takes a bit more work, for example using multiple
> imputation. Here the complete case method where an observation with
> any missing data is removed will result in a loss of efficiency
> compared to what can be achieved.
> 
> Ken
-- 
?
Dr. Hank Stevens, Associate Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.cas.muohio.edu/ecology
http://www.muohio.edu/botany/

"If the stars should appear one night in a thousand years, how would men
believe and adore." -Ralph Waldo Emerson, writer and philosopher
(1803-1882)



From kjbeath at kagi.com  Mon Jul 28 14:21:55 2008
From: kjbeath at kagi.com (Ken Beath)
Date: Mon, 28 Jul 2008 22:21:55 +1000
Subject: [R-sig-ME] missing data in lme, lmer, PROC MIXED
In-Reply-To: <1217243059.5947.33.camel@stevenmh-desktop>
References: <1217021327.8446.17.camel@stevenmh-desktop>
	<13CEFB6E-06AE-4982-BD8F-CD74A3DAABA7@kagi.com>
	<1217243059.5947.33.camel@stevenmh-desktop>
Message-ID: <68CB3A51-FA7C-4758-AE35-2DC68C077E8B@kagi.com>

On 28/07/2008, at 9:04 PM, M Henry H Stevens wrote:

> Thanks Ken. I have been assuming that they meant missing covariates (a
> subject provided most of the predictors, but not all). So I take it  
> that
> SAS does no imputation on its own-that the user would need to do that
> (if they wanted?). lme does not do anything like that.
>

Yes, neither SAS or R or most programs handle missing covariates  
automatically. The only program I know is MPlus which is a general  
latent variable modelling program. I turned off the missing data  
handling as for one model it resulted in an 11 dimensional integration.

Ken

> Hank
>
> On Sat, 2008-07-26 at 22:39 -0400, Ken Beath wrote:
>> On 26/07/2008, at 7:28 AM, M Henry H Stevens wrote:
>>
>>> Hi folks,
>>> I have colleagues who comfortably state that "missing data" are ok  
>>> in
>>> "mixed models" - because "the program (PROC MIXED) handles missing
>>> data
>>> -- I have a hard time imagining what it does.
>>>
>>> To those of you who use both R and SAS, I was wondering if you might
>>> share insight into what these do.
>>>
>>> As far as I know, for lme:
>>> 'na.action="na.omit" ' or na.exclude, removes the rows with any
>>> missing
>>> data.
>>>
>>
>> This depends. If the missing data is the dependent and it is missing
>> at random then as mixed models are fitted using maximum likelihood it
>> will produce results that are optimal. Roughly (there are some really
>> technical definitions for missing data and I haven't checked them) if
>> we don't know the outcome and the reason it is missing isn't due to
>> its value or the other data then we can simply leave it out of the
>> likelihood equation it as it has no useful information. A problem is
>> when data being missing provides this sort of information and is very
>> difficult to model. An example is if observations above a certain
>> value are more likely to be missing.
>>
>> An alternative method of dealing with repeated data is to produce a
>> summary for each subject or cluster, for example by averaging the  
>> last
>> three visits. This doesn't correctly handle missing data although the
>> loss in efficiency is usually small and it can work well, provided
>> only a small proportion is missing.
>>
>> What R and SAS don't deal with directly is missing data in the
>> covariates. This takes a bit more work, for example using multiple
>> imputation. Here the complete case method where an observation with
>> any missing data is removed will result in a loss of efficiency
>> compared to what can be achieved.
>>
>> Ken
> -- 
>
> Dr. Hank Stevens, Associate Professor
> 338 Pearson Hall
> Botany Department
> Miami University
> Oxford, OH 45056
>
> Office: (513) 529-4206
> Lab: (513) 529-4262
> FAX: (513) 529-4243
> http://www.cas.muohio.edu/~stevenmh/
> http://www.cas.muohio.edu/ecology
> http://www.muohio.edu/botany/
>
> "If the stars should appear one night in a thousand years, how would  
> men
> believe and adore." -Ralph Waldo Emerson, writer and philosopher
> (1803-1882)
>
>
>
>
>



From HDoran at air.org  Mon Jul 28 15:05:32 2008
From: HDoran at air.org (Doran, Harold)
Date: Mon, 28 Jul 2008 09:05:32 -0400
Subject: [R-sig-ME] missing data in lme, lmer, PROC MIXED
In-Reply-To: <68CB3A51-FA7C-4758-AE35-2DC68C077E8B@kagi.com>
Message-ID: <ED7B522EE00C9A4FA515AA71724D61EEB86738@DC1EXCL01.air.org>

Ken,

Does M-Plus actually impute values for the missing cells in the model
matrix for the fixed effects? Is this a default behavior of m-plus, or
does one need to be cognizant of this and implement a particular
imputation strategy?

In general, this kind of question comes up all the time on the
multilevel listserv. There are constant suggestions that many of the
multilevel software packages automagically "handle" missing data because
they use "maximum likelihood". 

> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org 
> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf 
> Of Ken Beath
> Sent: Monday, July 28, 2008 8:22 AM
> To: MHH Stevens
> Cc: R Mixed Models; Stevens,Martin Henry H. Dr.
> Subject: Re: [R-sig-ME] missing data in lme, lmer, PROC MIXED
> 
> On 28/07/2008, at 9:04 PM, M Henry H Stevens wrote:
> 
> > Thanks Ken. I have been assuming that they meant missing 
> covariates (a 
> > subject provided most of the predictors, but not all). So I take it 
> > that SAS does no imputation on its own-that the user would 
> need to do 
> > that (if they wanted?). lme does not do anything like that.
> >
> 
> Yes, neither SAS or R or most programs handle missing covariates  
> automatically. The only program I know is MPlus which is a general  
> latent variable modelling program. I turned off the missing data  
> handling as for one model it resulted in an 11 dimensional 
> integration.
> 
> Ken
> 
> > Hank
> >
> > On Sat, 2008-07-26 at 22:39 -0400, Ken Beath wrote:
> >> On 26/07/2008, at 7:28 AM, M Henry H Stevens wrote:
> >>
> >>> Hi folks,
> >>> I have colleagues who comfortably state that "missing 
> data" are ok  
> >>> in
> >>> "mixed models" - because "the program (PROC MIXED) handles missing
> >>> data
> >>> -- I have a hard time imagining what it does.
> >>>
> >>> To those of you who use both R and SAS, I was wondering 
> if you might
> >>> share insight into what these do.
> >>>
> >>> As far as I know, for lme:
> >>> 'na.action="na.omit" ' or na.exclude, removes the rows with any
> >>> missing
> >>> data.
> >>>
> >>
> >> This depends. If the missing data is the dependent and it 
> is missing
> >> at random then as mixed models are fitted using maximum 
> likelihood it
> >> will produce results that are optimal. Roughly (there are 
> some really
> >> technical definitions for missing data and I haven't 
> checked them) if
> >> we don't know the outcome and the reason it is missing isn't due to
> >> its value or the other data then we can simply leave it out of the
> >> likelihood equation it as it has no useful information. A 
> problem is
> >> when data being missing provides this sort of information 
> and is very
> >> difficult to model. An example is if observations above a certain
> >> value are more likely to be missing.
> >>
> >> An alternative method of dealing with repeated data is to produce a
> >> summary for each subject or cluster, for example by averaging the  
> >> last
> >> three visits. This doesn't correctly handle missing data 
> although the
> >> loss in efficiency is usually small and it can work well, provided
> >> only a small proportion is missing.
> >>
> >> What R and SAS don't deal with directly is missing data in the
> >> covariates. This takes a bit more work, for example using multiple
> >> imputation. Here the complete case method where an observation with
> >> any missing data is removed will result in a loss of efficiency
> >> compared to what can be achieved.
> >>
> >> Ken
> > -- 
> >
> > Dr. Hank Stevens, Associate Professor
> > 338 Pearson Hall
> > Botany Department
> > Miami University
> > Oxford, OH 45056
> >
> > Office: (513) 529-4206
> > Lab: (513) 529-4262
> > FAX: (513) 529-4243
> > http://www.cas.muohio.edu/~stevenmh/
> > http://www.cas.muohio.edu/ecology
> > http://www.muohio.edu/botany/
> >
> > "If the stars should appear one night in a thousand years, 
> how would  
> > men
> > believe and adore." -Ralph Waldo Emerson, writer and philosopher
> > (1803-1882)
> >
> >
> >
> >
> >
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 



From kjbeath at kagi.com  Tue Jul 29 13:40:58 2008
From: kjbeath at kagi.com (Ken Beath)
Date: Tue, 29 Jul 2008 21:40:58 +1000
Subject: [R-sig-ME] missing data in lme, lmer, PROC MIXED
In-Reply-To: <ED7B522EE00C9A4FA515AA71724D61EEB86738@DC1EXCL01.air.org>
References: <ED7B522EE00C9A4FA515AA71724D61EEB86738@DC1EXCL01.air.org>
Message-ID: <937E2656-9937-453A-B3BE-F72C317799A7@kagi.com>

On 28/07/2008, at 11:05 PM, Doran, Harold wrote:

> Ken,
>
> Does M-Plus actually impute values for the missing cells in the model
> matrix for the fixed effects? Is this a default behavior of m-plus, or
> does one need to be cognizant of this and implement a particular
> imputation strategy?
>

MPlus has rather poor documentation in this area. Rather than impute I  
think it assumes multivariate normality of the covariates, or for  
categorical variables an underlying latent variables. So there is an  
assumed model for the covariates, this is something that is unavoidable.

It is switched on automatically in Mplus. I've tried with some  
simulated data and it does do something and seems to work properly.   
With a linear regression on 2 covariates I set half of one covariate  
to missing. With the missing data option the standard errors are  
reduced by about 20% compared to complete case which could be quite  
useful.

> In general, this kind of question comes up all the time on the
> multilevel listserv. There are constant suggestions that many of the
> multilevel software packages automagically "handle" missing data  
> because
> they use "maximum likelihood".
>

A simplification of what actually happens.

A useful introductory paper on missing data is http://maven.smith.edu/~nhorton/muchado.pdf 
  and accompanying talk http://maven.smith.edu/~nhorton/muchado-notes.pdf

Ken


>> -----Original Message-----
>> From: r-sig-mixed-models-bounces at r-project.org
>> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf
>> Of Ken Beath
>> Sent: Monday, July 28, 2008 8:22 AM
>> To: MHH Stevens
>> Cc: R Mixed Models; Stevens,Martin Henry H. Dr.
>> Subject: Re: [R-sig-ME] missing data in lme, lmer, PROC MIXED
>>
>> On 28/07/2008, at 9:04 PM, M Henry H Stevens wrote:
>>
>>> Thanks Ken. I have been assuming that they meant missing
>> covariates (a
>>> subject provided most of the predictors, but not all). So I take it
>>> that SAS does no imputation on its own-that the user would
>> need to do
>>> that (if they wanted?). lme does not do anything like that.
>>>
>>
>> Yes, neither SAS or R or most programs handle missing covariates
>> automatically. The only program I know is MPlus which is a general
>> latent variable modelling program. I turned off the missing data
>> handling as for one model it resulted in an 11 dimensional
>> integration.
>>
>> Ken
>>
>>> Hank
>>>
>>> On Sat, 2008-07-26 at 22:39 -0400, Ken Beath wrote:
>>>> On 26/07/2008, at 7:28 AM, M Henry H Stevens wrote:
>>>>
>>>>> Hi folks,
>>>>> I have colleagues who comfortably state that "missing
>> data" are ok
>>>>> in
>>>>> "mixed models" - because "the program (PROC MIXED) handles missing
>>>>> data
>>>>> -- I have a hard time imagining what it does.
>>>>>
>>>>> To those of you who use both R and SAS, I was wondering
>> if you might
>>>>> share insight into what these do.
>>>>>
>>>>> As far as I know, for lme:
>>>>> 'na.action="na.omit" ' or na.exclude, removes the rows with any
>>>>> missing
>>>>> data.
>>>>>
>>>>
>>>> This depends. If the missing data is the dependent and it
>> is missing
>>>> at random then as mixed models are fitted using maximum
>> likelihood it
>>>> will produce results that are optimal. Roughly (there are
>> some really
>>>> technical definitions for missing data and I haven't
>> checked them) if
>>>> we don't know the outcome and the reason it is missing isn't due to
>>>> its value or the other data then we can simply leave it out of the
>>>> likelihood equation it as it has no useful information. A
>> problem is
>>>> when data being missing provides this sort of information
>> and is very
>>>> difficult to model. An example is if observations above a certain
>>>> value are more likely to be missing.
>>>>
>>>> An alternative method of dealing with repeated data is to produce a
>>>> summary for each subject or cluster, for example by averaging the
>>>> last
>>>> three visits. This doesn't correctly handle missing data
>> although the
>>>> loss in efficiency is usually small and it can work well, provided
>>>> only a small proportion is missing.
>>>>
>>>> What R and SAS don't deal with directly is missing data in the
>>>> covariates. This takes a bit more work, for example using multiple
>>>> imputation. Here the complete case method where an observation with
>>>> any missing data is removed will result in a loss of efficiency
>>>> compared to what can be achieved.
>>>>
>>>> Ken
>>> -- 
>>>
>>> Dr. Hank Stevens, Associate Professor
>>> 338 Pearson Hall
>>> Botany Department
>>> Miami University
>>> Oxford, OH 45056
>>>
>>> Office: (513) 529-4206
>>> Lab: (513) 529-4262
>>> FAX: (513) 529-4243
>>> http://www.cas.muohio.edu/~stevenmh/
>>> http://www.cas.muohio.edu/ecology
>>> http://www.muohio.edu/botany/
>>>
>>> "If the stars should appear one night in a thousand years,
>> how would
>>> men
>>> believe and adore." -Ralph Waldo Emerson, writer and philosopher
>>> (1803-1882)
>>>
>>>
>>>
>>>
>>>
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>



From byjung at gmail.com  Tue Jul 29 20:23:26 2008
From: byjung at gmail.com (Byju Govindan)
Date: Tue, 29 Jul 2008 14:23:26 -0400
Subject: [R-sig-ME] mixed effect modelling for zero inflated count data in R
Message-ID: <49d3454c0807291123m1d2b8e50m7abdcc92e1693c98@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080729/f53508fe/attachment.pl>

From rab at nauticom.net  Tue Jul 29 21:39:12 2008
From: rab at nauticom.net (Rick Bilonick)
Date: Tue, 29 Jul 2008 15:39:12 -0400
Subject: [R-sig-ME] Mixed Effects Model for Ordinal Data
Message-ID: <1217360352.6790.5.camel@eei243.eei.upmc.edu>

Is there an mixed effects R package that can be used to model ordinal
data?



From bates at stat.wisc.edu  Tue Jul 29 22:50:29 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 29 Jul 2008 15:50:29 -0500
Subject: [R-sig-ME] mixed effect modelling for zero inflated count data
	in R
In-Reply-To: <49d3454c0807291123m1d2b8e50m7abdcc92e1693c98@mail.gmail.com>
References: <49d3454c0807291123m1d2b8e50m7abdcc92e1693c98@mail.gmail.com>
Message-ID: <40e66e0b0807291350g122224c2t472fc28d8d8df09f@mail.gmail.com>

On Tue, Jul 29, 2008 at 1:23 PM, Byju Govindan <byjung at gmail.com> wrote:
> Dear R users,

>  Is gmmlAMDB the only option available to do mixed effect modelling for zero
> inflated count data in R. Or does there exist any other option.

The answer to this, and many other questions about what can be done in
R, can be found by executing

install.packages("fortunes"); library(fortunes); fortune("Yoda")

P.S. I still maintain that if this statement were to be considered a
true Yodaism the first sentence should be "R this is.".



From r.turner at auckland.ac.nz  Wed Jul 30 00:48:20 2008
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Wed, 30 Jul 2008 10:48:20 +1200
Subject: [R-sig-ME] Fitting a ``trivial'' model with lmer().
Message-ID: <5605008B-0205-4509-A586-F5ECEE71890D@auckland.ac.nz>


I sent the message, to be found below, to Doug Bates just now.
Since he is flat out preparing a number of workshops, he suggested
that I post to this SIG to see if I could get some enlightenment.

Ergo, here goes:

I wrote (to the general R-help list; 28 July 2008):

	<snip>

>> How can I (is there any way that I can)
>> tell lmer() to fit the most general possible covariance structure?
>>

and Doug Bates responded:


> It sounds like you want a model formula of
>
> lmer(y ~ tstnum + (0 + tstnum|stdnt), data=schooldat)
>
> but that model will have 21 variance-covariance terms to estimate (22
> if you count the residual variance but that one gets profiled out of
> the optimization).  I would not be surprised if the estimated
> variance-covariance matrix for the random effects turns out to be
> singular.
>

I tried the above formulation and it (as I posted yesterday) gives
a warning about a false convergence message from nlminb.  I think
there are indeed singularity problems, or rather problems with a
certain indeterminateness.

I think that the forgoing formulation translates into ``mathematical
expressions'' as

	y_ij = mu + alpha_i + s_j + e_ij

(with treatment ``contrasts'' giving the constraint that alpha_1 = 0)
where alpha_i corresponds to the i-th time, s_j corresponds to the j-th
student, and the e_ij are (Gaussian) errors with mean 0 and a covariance
structure

	Cov(e_ij, e_{i',j'}) = 0 if j != j'
			     = gamma_{i,i'} if j = j'

Translated into my ``trivial model'' thinking, this can be expressed as

	Y_j = MU + s_j + E_j

where the Y_j and E_j are 6-dimensional vectors and the addition in the
forgoing follows the S/R convention whereby the scalar s_j is added to
each component of the vectors.  And of course MU is the 6 dimensional
vector of mean values.

The vectors Y_j are independent N(MU,Sigma) where Sigma = Gamma +  
sigma^2
where Gamma = [gamma_ij] and sigma^2 is the variance of the  
(independent,
mean 0) s_j.  I believe that sigma^2 is the residual variance in the
lmer() formulation of the model.

The covariance matrix Sigma is well-determined/estimable but Gamma  
and sigma^2
are not --- we can ``sensibly'' have Gamma = Sigma - sigma^2 for any  
value
of sigma^2 such that Sigma - sigma^2 is positive definite. In general  
there
will be a wide range of such values of sigma^2.

It seems to me that the s_j are redundant in the model; we just need:

	y_ij = mu + alpha_i + e_ij

Is there a way to formulate the foregoing model in lmer()?  Effectively,
from one p.o.v., one wants to constrain the sigma^2 variance  
component to
be 0.

Of course one doesn't need lmer() to fit such a model.  I just want to
get some understanding of the modelling syntax in lmer() and perhaps
be able to fit more complicated models which are related to the  
foregoing
``trivial'' one.

Thanks.

	cheers,

		Rolf Turner

######################################################################
Attention:\ This e-mail message is privileged and confid...{{dropped:9}}



From bxc at steno.dk  Wed Jul 30 12:23:06 2008
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Wed, 30 Jul 2008 12:23:06 +0200
Subject: [R-sig-ME] zero-inflated counts
Message-ID: <2F73FA3AF524144C863B7C2C903DBC33018043A4D7@exdkmbx002.corp.novocorp.net>

Byju,

If you look at Jim Lindsay's gnlm package at:
http://popgen.unimaas.nl/~jlindsey/rcode.html
you will find possibilities there.

The piece of code I used was something like:

pmod <- fmr( help1, dist="negative binomial",
             linear = ~ if.ddur + neuro + macro + ARB + ins1d + smoke,
                 mu =~exp(linear),
                pmu = rep(0,7),
                mix = ~ imp.aw + married + insdur + hypert + if.ddur,
               pmix = rep(0,8),
             pshape = 0 )

Hope this helps.
You can of course also use the Poisson distribution.

Best regards,
Bendix
______________________________________________

Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2-4
DK-2820 Gentofte
Denmark
+45 44 43 87 38 (direct)
+45 30 75 87 38 (mobile)
bxc at steno.dk   http://www.biostat.ku.dk/~bxc



Date: Tue, 29 Jul 2008 14:23:26 -0400
From: "Byju Govindan" <byjung at gmail.com>
Subject: [R-sig-ME] mixed effect modelling for zero inflated count
        data in R
To: R-sig-mixed-models at r-project.org
Message-ID:
        <49d3454c0807291123m1d2b8e50m7abdcc92e1693c98 at mail.gmail.com>
Content-Type: text/plain

Dear R users,

 Is gmmlAMDB the only option available to do mixed effect modelling for zero
inflated count data in R. Or does there exist any other option.

Thank you
 Byju



______________________________________________

Bendix Carstensen
Senior Statistician

Steno Diabetes Center
Niels Steensens Vej 2-4
DK-2820 Gentofte
Denmark
+45 44 43 87 38 (direct)
+45 30 75 87 38 (mobile)
bxc at steno.dk   http://www.biostat.ku.dk/~bxc

This e-mail (including any attachments) is intended for ...{{dropped:8}}



From HDoran at air.org  Wed Jul 30 16:45:32 2008
From: HDoran at air.org (Doran, Harold)
Date: Wed, 30 Jul 2008 10:45:32 -0400
Subject: [R-sig-ME] Fitting a ``trivial'' model with lmer().
In-Reply-To: <5605008B-0205-4509-A586-F5ECEE71890D@auckland.ac.nz>
Message-ID: <ED7B522EE00C9A4FA515AA71724D61EEB868A2@DC1EXCL01.air.org>

Can you post str(Rolf's_data)? If I understand correctly, y is a
temporally ordered vector of the measurements on each student, correct?
I further assume that tstnum is a factor that associates each element in
the vector y with a point in time. So, your data are something like:

Student	y	tstnum
1		y1	1
1		y2	2
1		y3	3
2		y1	1
2		y2	2
2		y3	3
...

If this is your data structure and your lmer syntax is:

lmer(y ~ tstnum + (0 + tstnum|stdnt), data=schooldat)

Then you will get the marginal effects for each time point as (mu +
alpha_i) and the variance/covariance matrix of the ranodm effects will
be very large and probably unstable. I don't know what you mean by the
"most general covariance matrix possible". As someone who models
longitudinal educational data on a daily basis, I would approach this
problem differently. 

Again, I don't think I understand your data structure entirely correct,
but assume the data structure I pose above is what you have. I would
first start with a model where tstnum is an integer and not a factor in
the data frame. Because it seems you want the marginal effects for time,
I might do something like:

lmer(y ~ factor(tstnum) + (tstnum|stdnt), data=schooldat)

In this formulation, treating time in this manner accounts for the
serial correlation between scores and reduces the number of elements in
the variance/covariance matrix.

Now, I experimented quickly with the egsingle data set that is in
library(mlmRev). Here is how my thinking plays out:

> fm1 <- lmer(math ~ factor(year) + (year|schoolid), egsingle)
> summary(fm1)
Linear mixed-effects model fit by REML 
Formula: math ~ factor(year) + (year | schoolid) 
   Data: egsingle 
   AIC   BIC logLik MLdeviance REMLdeviance
 20546 20608 -10264      20503        20528
Random effects:
 Groups   Name        Variance  Std.Dev. Corr  
 schoolid (Intercept) 0.1933133 0.439674       
          year        0.0098065 0.099028 0.555 
 Residual             0.9664253 0.983069       
number of obs: 7230, groups: schoolid, 60

Fixed effects:
                 Estimate Std. Error t value
(Intercept)      -3.20081    0.10074  -31.77
factor(year)-1.5  1.18359    0.09304   12.72
factor(year)-0.5  2.19556    0.09552   22.99
factor(year)0.5   2.88863    0.09991   28.91
factor(year)1.5   3.48153    0.10660   32.66
factor(year)2.5   4.29051    0.11451   37.47

Correlation of Fixed Effects:
            (Intr) f()-1. f()-0. f()0.5 f()1.5
fctr(y)-1.5 -0.831                            
fctr(y)-0.5 -0.816  0.914                     
fctr(yr)0.5 -0.785  0.893  0.927              
fctr(yr)1.5 -0.739  0.853  0.903  0.933       
fctr(yr)2.5 -0.691  0.809  0.872  0.915  0.932
> fm2 <- lmer(math ~ factor(year) + (factor(year)|schoolid), egsingle)
Warning messages:
1: In .local(x, ..., value) :
  Estimated variance-covariance for factor 'schoolid' is singular

2: In .local(x, ..., value) :
  nlminb returned message false convergence (8)

The number of elements in the variance/covariance matrix in fm2 is large
and this model also has a singularity problem, similar to what you have
encountered. Let me stop here and let you add info or experiment with
what I pose.




> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org 
> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf 
> Of Rolf Turner
> Sent: Tuesday, July 29, 2008 6:48 PM
> To: R-Sig-mixed-models
> Subject: [R-sig-ME] Fitting a ``trivial'' model with lmer().
> 
> 
> I sent the message, to be found below, to Doug Bates just now.
> Since he is flat out preparing a number of workshops, he 
> suggested that I post to this SIG to see if I could get some 
> enlightenment.
> 
> Ergo, here goes:
> 
> I wrote (to the general R-help list; 28 July 2008):
> 
> 	<snip>
> 
> >> How can I (is there any way that I can) tell lmer() to fit 
> the most 
> >> general possible covariance structure?
> >>
> 
> and Doug Bates responded:
> 
> 
> > It sounds like you want a model formula of
> >
> > lmer(y ~ tstnum + (0 + tstnum|stdnt), data=schooldat)
> >
> > but that model will have 21 variance-covariance terms to 
> estimate (22 
> > if you count the residual variance but that one gets 
> profiled out of 
> > the optimization).  I would not be surprised if the estimated 
> > variance-covariance matrix for the random effects turns out to be 
> > singular.
> >
> 
> I tried the above formulation and it (as I posted yesterday) 
> gives a warning about a false convergence message from 
> nlminb.  I think there are indeed singularity problems, or 
> rather problems with a certain indeterminateness.
> 
> I think that the forgoing formulation translates into 
> ``mathematical expressions'' as
> 
> 	y_ij = mu + alpha_i + s_j + e_ij
> 
> (with treatment ``contrasts'' giving the constraint that 
> alpha_1 = 0) where alpha_i corresponds to the i-th time, s_j 
> corresponds to the j-th student, and the e_ij are (Gaussian) 
> errors with mean 0 and a covariance structure
> 
> 	Cov(e_ij, e_{i',j'}) = 0 if j != j'
> 			     = gamma_{i,i'} if j = j'
> 
> Translated into my ``trivial model'' thinking, this can be 
> expressed as
> 
> 	Y_j = MU + s_j + E_j
> 
> where the Y_j and E_j are 6-dimensional vectors and the 
> addition in the forgoing follows the S/R convention whereby 
> the scalar s_j is added to each component of the vectors.  
> And of course MU is the 6 dimensional vector of mean values.
> 
> The vectors Y_j are independent N(MU,Sigma) where Sigma = Gamma +
> sigma^2
> where Gamma = [gamma_ij] and sigma^2 is the variance of the 
> (independent, mean 0) s_j.  I believe that sigma^2 is the 
> residual variance in the
> lmer() formulation of the model.
> 
> The covariance matrix Sigma is well-determined/estimable but 
> Gamma and sigma^2 are not --- we can ``sensibly'' have Gamma 
> = Sigma - sigma^2 for any value of sigma^2 such that Sigma - 
> sigma^2 is positive definite. In general there will be a wide 
> range of such values of sigma^2.
> 
> It seems to me that the s_j are redundant in the model; we just need:
> 
> 	y_ij = mu + alpha_i + e_ij
> 
> Is there a way to formulate the foregoing model in lmer()?  
> Effectively, from one p.o.v., one wants to constrain the 
> sigma^2 variance component to be 0.
> 
> Of course one doesn't need lmer() to fit such a model.  I 
> just want to get some understanding of the modelling syntax 
> in lmer() and perhaps be able to fit more complicated models 
> which are related to the foregoing ``trivial'' one.
> 
> Thanks.
> 
> 	cheers,
> 
> 		Rolf Turner
> 
> ######################################################################
> Attention:\ This e-mail message is privileged and 
> confid...{{dropped:9}}
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 



From lamprianou at yahoo.com  Thu Jul 31 06:46:06 2008
From: lamprianou at yahoo.com (Iasonas Lamprianou)
Date: Wed, 30 Jul 2008 21:46:06 -0700 (PDT)
Subject: [R-sig-ME] Stats question
Message-ID: <90538.58569.qm@web54110.mail.re2.yahoo.com>

Dear friends, 
I am not sure that this is the right place?to ask,? but please feel?free to suggest an alternative discussion group.
My question is that I?want to do a comparative study in order to compare the rate of incidence in two populations. I know that a pilot study was conducted a few weeks ago and found?8/140 (around 6%) incidence in population A. Population B was not sampled. Assuming this is (about) the right proportion in the Population A what?is the sample size I need?for population A and B in the main study, in order to have power of 80% to idenitfy significant?differences? I would expect the incidence in population B to be around 10% compared to the 6% of the Population A.
Any suggestions?
Jason
Dr. Iasonas Lamprianou
Department of Education
The University of Manchester
Oxford Road, Manchester M13 9PL, UK
Tel. 0044 161 275 3485
iasonas.lamprianou at manchester.ac.uk



----- Original Message ----
From: "r-sig-mixed-models-request at r-project.org" <r-sig-mixed-models-request at r-project.org>
To: r-sig-mixed-models at r-project.org
Sent: Tuesday, 29 July, 2008 1:00:01 PM
Subject: R-sig-mixed-models Digest, Vol 19, Issue 23

Send R-sig-mixed-models mailing list submissions to
??? r-sig-mixed-models at r-project.org

To subscribe or unsubscribe via the World Wide Web, visit
??? https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
or, via email, send a message with subject or body 'help' to
??? r-sig-mixed-models-request at r-project.org

You can reach the person managing the list at
??? r-sig-mixed-models-owner at r-project.org

When replying, please edit your Subject line so it is more specific
than "Re: Contents of R-sig-mixed-models digest..."


Today's Topics:

? 1. Re: missing data in lme, lmer, PROC MIXED (M Henry H Stevens)
? 2. Re: missing data in lme, lmer, PROC MIXED (Ken Beath)
? 3. Re: missing data in lme, lmer, PROC MIXED (Doran, Harold)


----------------------------------------------------------------------

Message: 1
Date: Mon, 28 Jul 2008 07:04:19 -0400
From: M Henry H Stevens <HStevens at muohio.edu>
Subject: Re: [R-sig-ME] missing data in lme, lmer, PROC MIXED
To: Ken Beath <kjbeath at kagi.com>
Cc: R Mixed Models <r-sig-mixed-models at r-project.org>, "Stevens,
??? Martin Henry H. Dr." <stevenmh at muohio.edu>
Message-ID: <1217243059.5947.33.camel at stevenmh-desktop>
Content-Type: text/plain; charset=UTF-8

Thanks Ken. I have been assuming that they meant missing covariates (a
subject provided most of the predictors, but not all). So I take it that
SAS does no imputation on its own-that the user would need to do that
(if they wanted?). lme does not do anything like that.

Hank

On Sat, 2008-07-26 at 22:39 -0400, Ken Beath wrote:
> On 26/07/2008, at 7:28 AM, M Henry H Stevens wrote:
> 
> > Hi folks,
> > I have colleagues who comfortably state that "missing data" are ok in
> > "mixed models" - because "the program (PROC MIXED) handles missing
> > data
> > -- I have a hard time imagining what it does.
> >
> > To those of you who use both R and SAS, I was wondering if you might
> > share insight into what these do.
> >
> > As far as I know, for lme:
> > 'na.action="na.omit" ' or na.exclude, removes the rows with any
> > missing
> > data.
> >
> 
> This depends. If the missing data is the dependent and it is missing
> at random then as mixed models are fitted using maximum likelihood it
> will produce results that are optimal. Roughly (there are some really
> technical definitions for missing data and I haven't checked them) if
> we don't know the outcome and the reason it is missing isn't due to
> its value or the other data then we can simply leave it out of the
> likelihood equation it as it has no useful information. A problem is
> when data being missing provides this sort of information and is very
> difficult to model. An example is if observations above a certain
> value are more likely to be missing.
> 
> An alternative method of dealing with repeated data is to produce a
> summary for each subject or cluster, for example by averaging the last
> three visits. This doesn't correctly handle missing data although the
> loss in efficiency is usually small and it can work well, provided
> only a small proportion is missing.
> 
> What R and SAS don't deal with directly is missing data in the
> covariates. This takes a bit more work, for example using multiple
> imputation. Here the complete case method where an observation with
> any missing data is removed will result in a loss of efficiency
> compared to what can be achieved.
> 
> Ken
-- 
?
Dr. Hank Stevens, Associate Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.cas.muohio.edu/ecology
http://www.muohio.edu/botany/

"If the stars should appear one night in a thousand years, how would men
believe and adore." -Ralph Waldo Emerson, writer and philosopher
(1803-1882)



------------------------------

Message: 2
Date: Mon, 28 Jul 2008 22:21:55 +1000
From: Ken Beath <kjbeath at kagi.com>
Subject: Re: [R-sig-ME] missing data in lme, lmer, PROC MIXED
To: MHH Stevens <HStevens at muohio.edu>
Cc: R Mixed Models <r-sig-mixed-models at r-project.org>, "Stevens,
??? Martin Henry H. Dr." <stevenmh at muohio.edu>
Message-ID: <68CB3A51-FA7C-4758-AE35-2DC68C077E8B at kagi.com>
Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes

On 28/07/2008, at 9:04 PM, M Henry H Stevens wrote:

> Thanks Ken. I have been assuming that they meant missing covariates (a
> subject provided most of the predictors, but not all). So I take it? 
> that
> SAS does no imputation on its own-that the user would need to do that
> (if they wanted?). lme does not do anything like that.
>

Yes, neither SAS or R or most programs handle missing covariates? 
automatically. The only program I know is MPlus which is a general? 
latent variable modelling program. I turned off the missing data? 
handling as for one model it resulted in an 11 dimensional integration.

Ken

> Hank
>
> On Sat, 2008-07-26 at 22:39 -0400, Ken Beath wrote:
>> On 26/07/2008, at 7:28 AM, M Henry H Stevens wrote:
>>
>>> Hi folks,
>>> I have colleagues who comfortably state that "missing data" are ok? 
>>> in
>>> "mixed models" - because "the program (PROC MIXED) handles missing
>>> data
>>> -- I have a hard time imagining what it does.
>>>
>>> To those of you who use both R and SAS, I was wondering if you might
>>> share insight into what these do.
>>>
>>> As far as I know, for lme:
>>> 'na.action="na.omit" ' or na.exclude, removes the rows with any
>>> missing
>>> data.
>>>
>>
>> This depends. If the missing data is the dependent and it is missing
>> at random then as mixed models are fitted using maximum likelihood it
>> will produce results that are optimal. Roughly (there are some really
>> technical definitions for missing data and I haven't checked them) if
>> we don't know the outcome and the reason it is missing isn't due to
>> its value or the other data then we can simply leave it out of the
>> likelihood equation it as it has no useful information. A problem is
>> when data being missing provides this sort of information and is very
>> difficult to model. An example is if observations above a certain
>> value are more likely to be missing.
>>
>> An alternative method of dealing with repeated data is to produce a
>> summary for each subject or cluster, for example by averaging the? 
>> last
>> three visits. This doesn't correctly handle missing data although the
>> loss in efficiency is usually small and it can work well, provided
>> only a small proportion is missing.
>>
>> What R and SAS don't deal with directly is missing data in the
>> covariates. This takes a bit more work, for example using multiple
>> imputation. Here the complete case method where an observation with
>> any missing data is removed will result in a loss of efficiency
>> compared to what can be achieved.
>>
>> Ken
> -- 
>
> Dr. Hank Stevens, Associate Professor
> 338 Pearson Hall
> Botany Department
> Miami University
> Oxford, OH 45056
>
> Office: (513) 529-4206
> Lab: (513) 529-4262
> FAX: (513) 529-4243
> http://www.cas.muohio.edu/~stevenmh/
> http://www.cas.muohio.edu/ecology
> http://www.muohio.edu/botany/
>
> "If the stars should appear one night in a thousand years, how would? 
> men
> believe and adore." -Ralph Waldo Emerson, writer and philosopher
> (1803-1882)
>
>
>
>
>



------------------------------

Message: 3
Date: Mon, 28 Jul 2008 09:05:32 -0400
From: "Doran, Harold" <HDoran at air.org>
Subject: Re: [R-sig-ME] missing data in lme, lmer, PROC MIXED
To: "Ken Beath" <kjbeath at kagi.com>, "MHH Stevens"
??? <HStevens at muohio.edu>
Cc: R Mixed Models <r-sig-mixed-models at r-project.org>, "Stevens,
??? Martin Henry H. Dr." <stevenmh at muohio.edu>
Message-ID: <ED7B522EE00C9A4FA515AA71724D61EEB86738 at DC1EXCL01.air.org>
Content-Type: text/plain;??? charset="us-ascii"

Ken,

Does M-Plus actually impute values for the missing cells in the model
matrix for the fixed effects? Is this a default behavior of m-plus, or
does one need to be cognizant of this and implement a particular
imputation strategy?

In general, this kind of question comes up all the time on the
multilevel listserv. There are constant suggestions that many of the
multilevel software packages automagically "handle" missing data because
they use "maximum likelihood". 

> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org 
> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf 
> Of Ken Beath
> Sent: Monday, July 28, 2008 8:22 AM
> To: MHH Stevens
> Cc: R Mixed Models; Stevens,Martin Henry H. Dr.
> Subject: Re: [R-sig-ME] missing data in lme, lmer, PROC MIXED
> 
> On 28/07/2008, at 9:04 PM, M Henry H Stevens wrote:
> 
> > Thanks Ken. I have been assuming that they meant missing 
> covariates (a 
> > subject provided most of the predictors, but not all). So I take it 
> > that SAS does no imputation on its own-that the user would 
> need to do 
> > that (if they wanted?). lme does not do anything like that.
> >
> 
> Yes, neither SAS or R or most programs handle missing covariates? 
> automatically. The only program I know is MPlus which is a general? 
> latent variable modelling program. I turned off the missing data? 
> handling as for one model it resulted in an 11 dimensional 
> integration.
> 
> Ken
> 
> > Hank
> >
> > On Sat, 2008-07-26 at 22:39 -0400, Ken Beath wrote:
> >> On 26/07/2008, at 7:28 AM, M Henry H Stevens wrote:
> >>
> >>> Hi folks,
> >>> I have colleagues who comfortably state that "missing 
> data" are ok? 
> >>> in
> >>> "mixed models" - because "the program (PROC MIXED) handles missing
> >>> data
> >>> -- I have a hard time imagining what it does.
> >>>
> >>> To those of you who use both R and SAS, I was wondering 
> if you might
> >>> share insight into what these do.
> >>>
> >>> As far as I know, for lme:
> >>> 'na.action="na.omit" ' or na.exclude, removes the rows with any
> >>> missing
> >>> data.
> >>>
> >>
> >> This depends. If the missing data is the dependent and it 
> is missing
> >> at random then as mixed models are fitted using maximum 
> likelihood it
> >> will produce results that are optimal. Roughly (there are 
> some really
> >> technical definitions for missing data and I haven't 
> checked them) if
> >> we don't know the outcome and the reason it is missing isn't due to
> >> its value or the other data then we can simply leave it out of the
> >> likelihood equation it as it has no useful information. A 
> problem is
> >> when data being missing provides this sort of information 
> and is very
> >> difficult to model. An example is if observations above a certain
> >> value are more likely to be missing.
> >>
> >> An alternative method of dealing with repeated data is to produce a
> >> summary for each subject or cluster, for example by averaging the? 
> >> last
> >> three visits. This doesn't correctly handle missing data 
> although the
> >> loss in efficiency is usually small and it can work well, provided
> >> only a small proportion is missing.
> >>
> >> What R and SAS don't deal with directly is missing data in the
> >> covariates. This takes a bit more work, for example using multiple
> >> imputation. Here the complete case method where an observation with
> >> any missing data is removed will result in a loss of efficiency
> >> compared to what can be achieved.
> >>
> >> Ken
> > -- 
> >
> > Dr. Hank Stevens, Associate Professor
> > 338 Pearson Hall
> > Botany Department
> > Miami University
> > Oxford, OH 45056
> >
> > Office: (513) 529-4206
> > Lab: (513) 529-4262
> > FAX: (513) 529-4243
> > http://www.cas.muohio.edu/~stevenmh/
> > http://www.cas.muohio.edu/ecology
> > http://www.muohio.edu/botany/
> >
> > "If the stars should appear one night in a thousand years, 
> how would? 
> > men
> > believe and adore." -Ralph Waldo Emerson, writer and philosopher
> > (1803-1882)
> >
> >
> >
> >
> >
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 



------------------------------

_______________________________________________
R-sig-mixed-models mailing list
R-sig-mixed-models at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


End of R-sig-mixed-models Digest, Vol 19, Issue 23
**************************************************



      __________________________________________________________
Not happy with your email address?.
Get the one you really want - millions of new email addresses available now at Yahoo! http://uk.docs.yahoo.com/ymail/new.html



From kjbeath at kagi.com  Thu Jul 31 10:03:31 2008
From: kjbeath at kagi.com (Ken Beath)
Date: Thu, 31 Jul 2008 18:03:31 +1000
Subject: [R-sig-ME] Stats question
In-Reply-To: <90538.58569.qm@web54110.mail.re2.yahoo.com>
References: <90538.58569.qm@web54110.mail.re2.yahoo.com>
Message-ID: <644B7CF4-F9B2-4F23-80D0-B360A69F5371@kagi.com>

On 31/07/2008, at 2:46 PM, Iasonas Lamprianou wrote:

> Dear friends,
> I am not sure that this is the right place to ask,  but please feel  
> free to suggest an alternative discussion group.
> My question is that I want to do a comparative study in order to  
> compare the rate of incidence in two populations. I know that a  
> pilot study was conducted a few weeks ago and found 8/140 (around  
> 6%) incidence in population A. Population B was not sampled.  
> Assuming this is (about) the right proportion in the Population A  
> what is the sample size I need for population A and B in the main  
> study, in order to have power of 80% to idenitfy significant  
> differences? I would expect the incidence in population B to be  
> around 10% compared to the 6% of the Population A.
> Any suggestions?

This isn't really the appropriate e-mail list, but samplesize.bin in  
Hmisc should be what you need, note that it returns the total sample  
size.  It is also useful to read the chapter on sample size from a  
clinical trials book.

Ken



From gregor.gorjanc at bfro.uni-lj.si  Thu Jul 31 10:32:44 2008
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Thu, 31 Jul 2008 08:32:44 +0000 (UTC)
Subject: [R-sig-ME] Mixed Effects Model for Ordinal Data
References: <1217360352.6790.5.camel@eei243.eei.upmc.edu>
Message-ID: <loom.20080731T083105-567@post.gmane.org>

Rick Bilonick <rab at ...> writes:

> 
> Is there an mixed effects R package that can be used to model ordinal
> data?
> 

Hint: take a look at the help page for lmer i.e. type

?lmer

and read about the family argument

gg



From gregor.gorjanc at bfro.uni-lj.si  Thu Jul 31 10:35:27 2008
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Thu, 31 Jul 2008 08:35:27 +0000 (UTC)
Subject: [R-sig-ME] mixed effect modelling for zero inflated count data
	in R
References: <49d3454c0807291123m1d2b8e50m7abdcc92e1693c98@mail.gmail.com>
Message-ID: <loom.20080731T083250-142@post.gmane.org>

Byju Govindan <byjung at ...> writes:

> 
> Dear R users,
> 
>  Is gmmlAMDB the only option available to do mixed effect modelling for zero
> inflated count data in R. Or does there exist any other option.
> 
> Thank you
>  Byju
> 

Gelman and Hill (their latest book) suggest adding error at the level of linear
model for the mean (expectation) for the Poisson distribution. Take a look
into their book about the details.

gg



From HDoran at air.org  Thu Jul 31 14:27:53 2008
From: HDoran at air.org (Doran, Harold)
Date: Thu, 31 Jul 2008 08:27:53 -0400
Subject: [R-sig-ME] Mixed Effects Model for Ordinal Data
In-Reply-To: <loom.20080731T083105-567@post.gmane.org>
Message-ID: <ED7B522EE00C9A4FA515AA71724D61EEB86930@DC1EXCL01.air.org>

But, just in case it's not obvious, the current link functions for lmer
don't support ordinal data. 

> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org 
> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf 
> Of Gregor Gorjanc
> Sent: Thursday, July 31, 2008 4:33 AM
> To: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Mixed Effects Model for Ordinal Data
> 
> Rick Bilonick <rab at ...> writes:
> 
> > 
> > Is there an mixed effects R package that can be used to 
> model ordinal 
> > data?
> > 
> 
> Hint: take a look at the help page for lmer i.e. type
> 
> ?lmer
> 
> and read about the family argument
> 
> gg
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 



From rab at nauticom.net  Thu Jul 31 14:39:49 2008
From: rab at nauticom.net (Rick Bilonick)
Date: Thu, 31 Jul 2008 08:39:49 -0400
Subject: [R-sig-ME] Mixed Effects Model for Ordinal Data
In-Reply-To: <loom.20080731T083105-567@post.gmane.org>
References: <1217360352.6790.5.camel@eei243.eei.upmc.edu>
	<loom.20080731T083105-567@post.gmane.org>
Message-ID: <1217507989.3503.4.camel@eei243.eei.upmc.edu>


On Thu, 2008-07-31 at 08:32 +0000, Gregor Gorjanc wrote:
> Rick Bilonick <rab at ...> writes:
> 
> > 
> > Is there an mixed effects R package that can be used to model ordinal
> > data?
> > 
> 
> Hint: take a look at the help page for lmer i.e. type
> 
> ?lmer
> 
> and read about the family argument
> 
> gg

I'm familiar with the family argument but I don't see anything
specifically for ordinal data. I've been using several binomial's to do
(separate) cumulative models. lrm in Design will do ordinal regression
but I need a mixed effects model.

Rick B.



From rab at nauticom.net  Thu Jul 31 14:44:26 2008
From: rab at nauticom.net (Rick Bilonick)
Date: Thu, 31 Jul 2008 08:44:26 -0400
Subject: [R-sig-ME] Mixed Effects Model for Ordinal Data
In-Reply-To: <ED7B522EE00C9A4FA515AA71724D61EEB86930@DC1EXCL01.air.org>
References: <ED7B522EE00C9A4FA515AA71724D61EEB86930@DC1EXCL01.air.org>
Message-ID: <1217508266.3503.8.camel@eei243.eei.upmc.edu>


On Thu, 2008-07-31 at 08:27 -0400, Doran, Harold wrote:
> But, just in case it's not obvious, the current link functions for lmer
> don't support ordinal data. 
> 
> > -----Original Message-----
> > From: r-sig-mixed-models-bounces at r-project.org 
> > [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf 
> > Of Gregor Gorjanc
> > Sent: Thursday, July 31, 2008 4:33 AM
> > To: r-sig-mixed-models at r-project.org
> > Subject: Re: [R-sig-ME] Mixed Effects Model for Ordinal Data
> > 
> > Rick Bilonick <rab at ...> writes:
> > 
> > > 
> > > Is there an mixed effects R package that can be used to 
> > model ordinal 
> > > data?
> > > 
> > 
> > Hint: take a look at the help page for lmer i.e. type
> > 
> > ?lmer
> > 
> > and read about the family argument
> > 
> > gg
> > 
> > 

I had never noticed anything in lmer specifically for ordinal logistic
regression (like lrm in Design). I created separate logistic regressions
using lmer but it would be nice to be able to make it proportional like
lrm does.

I also looked at logitord from repeated but it is not very flexible
(only one level of nesting and one observation per time point per
subject as far as I can tell) and difficult to get to work.

Rick B.



From Gregor.Gorjanc at bfro.uni-lj.si  Thu Jul 31 15:43:48 2008
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Thu, 31 Jul 2008 15:43:48 +0200
Subject: [R-sig-ME] Mixed Effects Model for Ordinal Data
In-Reply-To: <ED7B522EE00C9A4FA515AA71724D61EEB86930@DC1EXCL01.air.org>
References: <loom.20080731T083105-567@post.gmane.org>,
	<ED7B522EE00C9A4FA515AA71724D61EEB86930@DC1EXCL01.air.org>
Message-ID: <F189E18BBAA8B6479F618B9044CF4E9C686C1337FE@owa.bfro.uni-lj.si>

Oh! You are right. I do not know how did I saw "counts" in there, which
could be fitted via Poisson link.

gg
________________________________________
From: Doran, Harold [HDoran at air.org]
Sent: Thursday, July 31, 2008 2:27 PM
To: Gorjanc Gregor; r-sig-mixed-models at r-project.org
Subject: RE: [R-sig-ME] Mixed Effects Model for Ordinal Data

But, just in case it's not obvious, the current link functions for lmer
don't support ordinal data.

> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org
> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf
> Of Gregor Gorjanc
> Sent: Thursday, July 31, 2008 4:33 AM
> To: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Mixed Effects Model for Ordinal Data
>
> Rick Bilonick <rab at ...> writes:
>
> >
> > Is there an mixed effects R package that can be used to
> model ordinal
> > data?
> >
>
> Hint: take a look at the help page for lmer i.e. type
>
> ?lmer
>
> and read about the family argument
>
> gg
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From HDoran at air.org  Fri Aug  1 17:56:44 2008
From: HDoran at air.org (Doran, Harold)
Date: Fri, 1 Aug 2008 11:56:44 -0400
Subject: [R-sig-ME] [R] Major difference in the outcome between SPSS and
	R statisticalprograms
In-Reply-To: <C222D04679FF794EA98B823041802F1CE27C1F@EXV4.ds.umcutrecht.nl>
Message-ID: <ED7B522EE00C9A4FA515AA71724D61EEB86A57@DC1EXCL01.air.org>

First off, Marc Schwartz posted this link earlier today, read it.

http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-are-p_002dvalues-not-di
splayed-when-using-lmer_0028_0029_003f

Second, your email is not really descriptive enough. I have no idea what
OR is, so I have no reaction. 

Third, you're comparing estimates from different methods of estimation.
lmer will give standard errors that account for the correlation of
individuals within similar units whereas the SPSS procedure will not.
The lmer standard errors better capture the true sampling variance of
the parameters and SPSS doesn't. 



> -----Original Message-----
> From: Draga, R. [mailto:R.Draga at umcutrecht.nl] 
> Sent: Friday, August 01, 2008 11:45 AM
> To: Doran, Harold
> Subject: RE: [R] Major difference in the outcome between SPSS 
> and R statisticalprograms
> 
> Thanks for the reaction
> 
> I know, I would not expect the outcomes to be the same.
> But, I have never before encountered such a difference in 
> outcomes between SPSS and R; mostly the OR's and p-values 
> differed a little bit.
> 
> Strange is that R shows a OR of 10,176 and 95% CI of 
> 6,295-14,056. Then the p-value must be <0.05 doesn't it?
> For age the OR's differ dramatically between SPSS and R, 
> 0.985 and 0.003.
> 
> I just can not explain it.
> 
> Ronald
> 
> -----Oorspronkelijk bericht-----
> Van: Doran, Harold [mailto:HDoran at air.org]
> Verzonden: vrijdag 1 augustus 2008 17:36
> Aan: Draga, R.; r-help at r-project.org
> Onderwerp: RE: [R] Major difference in the outcome between 
> SPSS and R statisticalprograms
> 
> 
> The biggest problem is that SPSS cannot fit a generalized linear mixed
> model but lmer does. So, why would you expect the GLM in SPSS and the
> GLMM in lmer to match anyhow? 
> 
> > -----Original Message-----
> > From: r-help-bounces at r-project.org
> > [mailto:r-help-bounces at r-project.org] On Behalf Of Draga, R.
> > Sent: Friday, August 01, 2008 10:19 AM
> > To: r-help at r-project.org
> > Subject: [R] Major difference in the outcome between SPSS and 
> > R statisticalprograms
> > 
> > Dear collegues,
> >  
> > I have used R statistical program, package 'lmer', several
> > times already.
> > I never encountered major differences in the outcome between 
> > SPSS and R.
> > ...untill my last analyses.
> >  
> > Would some know were the huge differences come from.
> >  
> > Thanks in advance, Ronald
> >  
> > In SPSS the Pearson correlation between variable 1 and
> > variable 2 is 31% p<0.001.
> > 
> >  
> > 
> > In SPSS binary logistic regression gives us an OR=4.9 (95% CI
> > 2.7-9.0), p<0.001, n=338.
> > 
> >                     OR          lower   upper
> > 
> > gender          1,120       0,565   2,221
> > 
> > age               0,985       0,956   1,015
> > 
> > variable 2         4,937   2,698   9,032
> > 
> >  
> > 
> > In R multilevel logistic regression using statistical 
> package 'lmer' 
> > gives us an OR=10.2 (95% CI 6.3-14), p=0.24, n=338, groups: 
> group 1, 
> > 98; group 2 84.
> > 
> >                    OR           lower   upper
> > 
> > gender         2,295        -2,840 7,430
> > 
> > age              0,003        -70,047           70,054
> > 
> > variable 2     10,176     6,295   14,056
> > 
> >  
> > 
> > The crosstabs gives us:
> > 
> >      variable A
> > 
> > Var B  0   1
> > 
> >     0 156 108
> > 
> >     1  17  57
> > 
> >  
> > 
> > Would somebody know how it is possible that in SPSS we get
> > p<0.001 and in R we get p=0.24?
> > 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> 



From danielezrajohnson at gmail.com  Fri Aug  1 20:38:03 2008
From: danielezrajohnson at gmail.com (Daniel Ezra Johnson)
Date: Fri, 1 Aug 2008 19:38:03 +0100
Subject: [R-sig-ME] extracting random effect standard deviation
Message-ID: <a46630750808011138x4e18b631t98d8d2e440b4ee07@mail.gmail.com>

Hello,

How can I 'extract' the value of a random effect's standard deviation
from an lmer model?
This number is (a rather important) part of the usual lmer() output
but I cannot figure out anywhere in str(lmer.model) where it can be
found!

I can get it from summary(lmer.model)@REmat[,4], but that 'extra' call
to summary is slowing down my program.

Obviously the model has this information in it somewhere because the
model listing contains it.
How can I get it out?

Thanks,
Daniel



From bates at stat.wisc.edu  Fri Aug  1 21:15:26 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 1 Aug 2008 14:15:26 -0500
Subject: [R-sig-ME] extracting random effect standard deviation
In-Reply-To: <a46630750808011138x4e18b631t98d8d2e440b4ee07@mail.gmail.com>
References: <a46630750808011138x4e18b631t98d8d2e440b4ee07@mail.gmail.com>
Message-ID: <40e66e0b0808011215nc718dceld98cb9f015f46355@mail.gmail.com>

On Fri, Aug 1, 2008 at 1:38 PM, Daniel Ezra Johnson
<danielezrajohnson at gmail.com> wrote:
> Hello,
>
> How can I 'extract' the value of a random effect's standard deviation
> from an lmer model?
> This number is (a rather important) part of the usual lmer() output
> but I cannot figure out anywhere in str(lmer.model) where it can be
> found!
>
> I can get it from summary(lmer.model)@REmat[,4], but that 'extra' call
> to summary is slowing down my program.
>
> Obviously the model has this information in it somewhere because the
> model listing contains it.
> How can I get it out?

Use the VarCorr extractor function.  This returns a list of
variance-covariance matrices so you want something like

stddevs <- function(obj)
    unlist(lapply(VarCorr(obj), function(m) sqrt(diag(m))))

I'll leave it as an exercise to the reader to decide what names should
be given to these elements and to get them in the right places.
Doing that correctly in the general case is non-trivial.

I have been playing with the idea of having an optional argument
"type" for the VarCorr extractor so that the user can specify the raw
form (the factors of the relative variance-covariance matrices as
stored internally) or the variance-covariance form or the standard
deviations and correlations form or a transformed version as the
logarithm of the standard deviations and Fisher's z-transformation
(the hyperbolic arc-tangent) of the correlations.  At different times
you want these estimates in different forms.



From bates at stat.wisc.edu  Fri Aug  1 22:44:45 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 1 Aug 2008 15:44:45 -0500
Subject: [R-sig-ME] [R] Major difference in the outcome between SPSS and
	R statisticalprograms
In-Reply-To: <ED7B522EE00C9A4FA515AA71724D61EEB86A57@DC1EXCL01.air.org>
References: <C222D04679FF794EA98B823041802F1CE27C1F@EXV4.ds.umcutrecht.nl>
	<ED7B522EE00C9A4FA515AA71724D61EEB86A57@DC1EXCL01.air.org>
Message-ID: <40e66e0b0808011344t4cdd3d0atb2d423735f582f1c@mail.gmail.com>

On Fri, Aug 1, 2008 at 10:56 AM, Doran, Harold <HDoran at air.org> wrote:
> First off, Marc Schwartz posted this link earlier today, read it.
>
> http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-are-p_002dvalues-not-di
> splayed-when-using-lmer_0028_0029_003f
>
> Second, your email is not really descriptive enough. I have no idea what
> OR is, so I have no reaction.

Perhaps OR is "odds ratio".  In a generalized linear model or a
generalized linear mixed model for binary responses and using the
logit link, the exponentials of the coefficients are scale factors for
the odds ratio.

> Third, you're comparing estimates from different methods of estimation.
> lmer will give standard errors that account for the correlation of
> individuals within similar units whereas the SPSS procedure will not.
> The lmer standard errors better capture the true sampling variance of
> the parameters and SPSS doesn't.
>
>
>
>> -----Original Message-----
>> From: Draga, R. [mailto:R.Draga at umcutrecht.nl]
>> Sent: Friday, August 01, 2008 11:45 AM
>> To: Doran, Harold
>> Subject: RE: [R] Major difference in the outcome between SPSS
>> and R statisticalprograms
>>
>> Thanks for the reaction
>>
>> I know, I would not expect the outcomes to be the same.
>> But, I have never before encountered such a difference in
>> outcomes between SPSS and R; mostly the OR's and p-values
>> differed a little bit.
>>
>> Strange is that R shows a OR of 10,176 and 95% CI of
>> 6,295-14,056. Then the p-value must be <0.05 doesn't it?
>> For age the OR's differ dramatically between SPSS and R,
>> 0.985 and 0.003.
>>
>> I just can not explain it.
>>
>> Ronald
>>
>> -----Oorspronkelijk bericht-----
>> Van: Doran, Harold [mailto:HDoran at air.org]
>> Verzonden: vrijdag 1 augustus 2008 17:36
>> Aan: Draga, R.; r-help at r-project.org
>> Onderwerp: RE: [R] Major difference in the outcome between
>> SPSS and R statisticalprograms
>>
>>
>> The biggest problem is that SPSS cannot fit a generalized linear mixed
>> model but lmer does. So, why would you expect the GLM in SPSS and the
>> GLMM in lmer to match anyhow?
>>
>> > -----Original Message-----
>> > From: r-help-bounces at r-project.org
>> > [mailto:r-help-bounces at r-project.org] On Behalf Of Draga, R.
>> > Sent: Friday, August 01, 2008 10:19 AM
>> > To: r-help at r-project.org
>> > Subject: [R] Major difference in the outcome between SPSS and
>> > R statisticalprograms
>> >
>> > Dear collegues,
>> >
>> > I have used R statistical program, package 'lmer', several
>> > times already.
>> > I never encountered major differences in the outcome between
>> > SPSS and R.
>> > ...untill my last analyses.
>> >
>> > Would some know were the huge differences come from.
>> >
>> > Thanks in advance, Ronald
>> >
>> > In SPSS the Pearson correlation between variable 1 and
>> > variable 2 is 31% p<0.001.
>> >
>> >
>> >
>> > In SPSS binary logistic regression gives us an OR=4.9 (95% CI
>> > 2.7-9.0), p<0.001, n=338.
>> >
>> >                     OR          lower   upper
>> >
>> > gender          1,120       0,565   2,221
>> >
>> > age               0,985       0,956   1,015
>> >
>> > variable 2         4,937   2,698   9,032
>> >
>> >
>> >
>> > In R multilevel logistic regression using statistical
>> package 'lmer'
>> > gives us an OR=10.2 (95% CI 6.3-14), p=0.24, n=338, groups:
>> group 1,
>> > 98; group 2 84.
>> >
>> >                    OR           lower   upper
>> >
>> > gender         2,295        -2,840 7,430
>> >
>> > age              0,003        -70,047           70,054
>> >
>> > variable 2     10,176     6,295   14,056
>> >
>> >
>> >
>> > The crosstabs gives us:
>> >
>> >      variable A
>> >
>> > Var B  0   1
>> >
>> >     0 156 108
>> >
>> >     1  17  57
>> >
>> >
>> >
>> > Would somebody know how it is possible that in SPSS we get
>> > p<0.001 and in R we get p=0.24?
>> >
>> >
>> >     [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From sundar.dorai-raj at pdf.com  Tue Aug  5 00:25:57 2008
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 04 Aug 2008 15:25:57 -0700
Subject: [R-sig-ME] weights in lmer_0.999375-23
Message-ID: <489781F5.5070003@pdf.com>

Hi, Prof. Bates (and others),

It seems that in the latest version of lme4 from CRAN or R-forge the 
weights argument does not work:

library(lme4)
set.seed(1)
w <- abs(rnorm(nrow(sleepstudy)))
fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
fm2 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy, weights = w)
Error in mer_finalize(ans, verbose) :
   Calculated PWRSS for a LMM is negative
 > sessionInfo()
R version 2.7.1 (2008-06-23)
i386-pc-mingw32

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United 
States.1252;LC_MONETARY=English_United 
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] lme4_0.999375-23   Matrix_0.999375-12 lattice_0.17-12

loaded via a namespace (and not attached):
[1] grid_2.7.1  tools_2.7.1



From dieter.menne at menne-biomed.de  Tue Aug  5 08:40:14 2008
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Tue, 5 Aug 2008 06:40:14 +0000 (UTC)
Subject: [R-sig-ME] Mixed Effects Model for Ordinal Data
References: <1217360352.6790.5.camel@eei243.eei.upmc.edu>
Message-ID: <loom.20080805T063557-978@post.gmane.org>

Rick Bilonick <rab at ...> writes:

> 
> Is there an mixed effects R package that can be used to model ordinal
> data?
> 
DPMolmm in Alejandro Jara's DPpackage will do.


And congratulations to Alejandro for finally getting top-credits for his package:


http://stat-computing.org/awards/jmc/winners.html


Dieter



From julian.pichenot at cerfe.com  Tue Aug  5 17:26:37 2008
From: julian.pichenot at cerfe.com (Julian PICHENOT)
Date: Tue, 05 Aug 2008 17:26:37 +0200
Subject: [R-sig-ME] level-dependent explanatory variables
Message-ID: <20080805172637.hxs09u3k00o0s80o@pneumatix.net-cube.net>

Dear mixed models list,

I am trying to fit models with lme4 for a data set which has an  
unbalanced and hierarchical structure.
The goal is to model the presence/absence (0/1) of a frog species in  
ponds, considering potential explanatory variables measured at three  
levels (spatial scales).

The nested structure is as follow : 1516 ponds are nested within 134  
patches and these patches are nested within 24 landscapes.

There are 3 variables measured at each level :
Pond level : A1,B1,C1
Patch level : D2,E2,F2
Landscape level : G3,H3,I3.

This is the structure of the data set :
> str(msc)
'data.frame':   1516 obs. of  13 variables:
  $ y      : int  0 1 0 1 0 0 0 1 0 0 ...
  $ A1   : num  -0.758 -0.835 -0.835 -0.757 -0.757 ...
  $ B1   : num  -1.77 -1.77 -1.77 -1.80 -1.80 ...
  $ C1   : num  -0.262 -0.189 -0.189 -0.286 -0.286 ...
  $ D2  : num  0.869 0.869 0.869 0.869 0.869 ...
  $ E2: num  -2.49 -2.49 -2.49 -2.49 -2.49 ...
  $ F2   : num  -1.09 -1.09 -1.09 -1.09 -1.09 ...
  $ G3  : num  -0.327 -0.327 -0.327 -0.327 -0.327 ...
  $ H3  : num  -1.56 -1.56 -1.56 -1.56 -1.56 ...
  $ I3 : num  1.15 1.15 1.15 1.15 1.15 ...
  $ POND   : int  1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 ...
  $ PATCH  : int  1 1 1 1 1 1 1 1 1 1 ...
  $ LAND   : int  1 1 1 1 1 1 1 1 1 1 ...

Due to the fact that they are measured at a higher level than the  
pond, the values of D2,E2,F2 are repeated for each pond in a one  
particular patch and the values of G3,H3,I3 are repeated for each pond  
of each patch in one particular landscape.

Here is the code that I use :

> glmer(y~A1+B1+C1+D2+EP2+F2+G3+H3+I3+(1|LAND/PATCH),family=binomial,data)

But I find the results a bit strange. Only the variables measured at  
the pond level are significant and I have doubts about this.

Is there another way to fit a model that takes into account the  
spatial scale of each explanatory variable ?

I use the following packages :

> sessionInfo()
R version 2.7.0 (2008-04-22)
i386-pc-mingw32

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base
other attached packages:
[1] lme4_0.999375-22   Matrix_0.999375-10 lattice_0.17-6
loaded via a namespace (and not attached):
[1] grid_2.7.0

Thanks in advance for your help and answers.
Best regards.

Julian



From HDoran at air.org  Tue Aug  5 19:19:40 2008
From: HDoran at air.org (Doran, Harold)
Date: Tue, 5 Aug 2008 13:19:40 -0400
Subject: [R-sig-ME] level-dependent explanatory variables
References: <20080805172637.hxs09u3k00o0s80o@pneumatix.net-cube.net>
Message-ID: <ED7B522EE00C9A4FA515AA71724D61EE2718D1@DC1EXCL01.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080805/4cd0d8c9/attachment.pl>

From gangchen6 at gmail.com  Tue Aug  5 21:54:41 2008
From: gangchen6 at gmail.com (Gang Chen)
Date: Tue, 5 Aug 2008 15:54:41 -0400
Subject: [R-sig-ME] Mixed model with multiple response variables?
Message-ID: <b0f0ada60808051254k6a259417v95cbb7a129321b38@mail.gmail.com>

Hi,

I have a data set collected from 10 measurements (response variables)
on two groups (healthy and patient) of subjects performing 4 different
tasks. In other words there are two fixed factors (group and task),
and 10 response variables. I could analyze the data with aov() or
lme() in package nlme for each response variable separately, but since
most likely there are correlations among the 10 response variables,
would it be more meaningful to run a MANOVA? However manova() in R
seems not to allow an error term in the formula. What else can I try
for this kind of multivariate mixed model?

Also, if I want to find out which response variables (among the 10
measurements) are statistically significant in terms of acting as
indicators for group difference, what kind of statistical analysis
would help me sort them out?

Thanks in advance,
Gang



From dafshartous at med.miami.edu  Tue Aug  5 22:13:02 2008
From: dafshartous at med.miami.edu (David Afshartous)
Date: Tue, 05 Aug 2008 16:13:02 -0400
Subject: [R-sig-ME] Mixed model with multiple response variables?
In-Reply-To: <b0f0ada60808051254k6a259417v95cbb7a129321b38@mail.gmail.com>
Message-ID: <C4BE2C8E.6D17%dafshartous@med.miami.edu>


Note sure how to fit the model in R, but two good references are:

Fieuws & Verbeke (2006). "Pairwise fitting of mixed models for the joint
modeling of multivariate longitudinal profiles," Biometrics, 62, 424-431.

Fieuws et al. (2008). "Predicting renal graft failure using multivariate
longitudinal profiles," Biostatistics, 9, 419-431.


On 8/5/08 3:54 PM, "Gang Chen" <gangchen6 at gmail.com> wrote:

> Hi,
> 
> I have a data set collected from 10 measurements (response variables)
> on two groups (healthy and patient) of subjects performing 4 different
> tasks. In other words there are two fixed factors (group and task),
> and 10 response variables. I could analyze the data with aov() or
> lme() in package nlme for each response variable separately, but since
> most likely there are correlations among the 10 response variables,
> would it be more meaningful to run a MANOVA? However manova() in R
> seems not to allow an error term in the formula. What else can I try
> for this kind of multivariate mixed model?
> 
> Also, if I want to find out which response variables (among the 10
> measurements) are statistically significant in terms of acting as
> indicators for group difference, what kind of statistical analysis
> would help me sort them out?
> 
> Thanks in advance,
> Gang
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From julian.pichenot at cerfe.com  Tue Aug  5 23:02:37 2008
From: julian.pichenot at cerfe.com (Julian PICHENOT)
Date: Tue, 05 Aug 2008 23:02:37 +0200
Subject: [R-sig-ME] level-dependent explanatory variables
In-Reply-To: <ED7B522EE00C9A4FA515AA71724D61EE2718D1@DC1EXCL01.air.org>
References: <20080805172637.hxs09u3k00o0s80o@pneumatix.net-cube.net>
	<ED7B522EE00C9A4FA515AA71724D61EE2718D1@DC1EXCL01.air.org>
Message-ID: <20080805230237.fpelxnvysskgkg88@pneumatix.net-cube.net>

Thank you for your answer.

I apologies for the confusing sense of ? levels ? that I used.

I?m going to try to clarify the meaning of ? scale of each explanatory  
variable ? in my question.

Firstly, it may be easier if I explain what the scales are. The scale  
is the resolution at which I measured the explanatory variables.

The landscape scale is a circle of 2500 meters radius in which there  
are several patches.
Each patch is a circle of 200 meters radius in which there are several ponds.

Now I give you the significance of one variable that I measured for  
each for these scales:
A1 is the water volume of the pond.
D2 is the number of ponds that I found in a patch.
G3 is the area covered by forest in a landscape.

So, I could say that A1 is a ?pond-specific? variable, D2  
?patch-specific? and G3 ?landscape-specific?.

The aim of my study is to know if these explanatory variables can  
explain the presence/absence of the species in a pond.

And to do so, I have to consider the two random effects ? patch ? and  
? landscape ?. That's the reason why I choosed to use lmer.

I hope this clarify my request.


Quoting "Doran, Harold" <HDoran at air.org>:

> Julian:
>
> When it comes to a mixed linear model, there are "levels" of the   
> variance components but there are no "levels" associated with the   
> fixed effects. In the model specification for glmer, it seems that   
> your linear predictor has 2 levels of random variation. The idea of   
> levels for fixed effects tends to come from software programs that   
> write these models out using a hierarchical notation, and I think   
> that is a bit confusing.
>
> Maybe someone else knows, but can you clarify what you mean by   
> "spatial scale of the explanatory variables"?
>
>
> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org on behalf of Julian PICHENOT
> Sent: Tue 8/5/2008 11:26 AM
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] level-dependent explanatory variables
>
> Dear mixed models list,
>
> I am trying to fit models with lme4 for a data set which has an
> unbalanced and hierarchical structure.
> The goal is to model the presence/absence (0/1) of a frog species in
> ponds, considering potential explanatory variables measured at three
> levels (spatial scales).
>
> The nested structure is as follow : 1516 ponds are nested within 134
> patches and these patches are nested within 24 landscapes.
>
> There are 3 variables measured at each level :
> Pond level : A1,B1,C1
> Patch level : D2,E2,F2
> Landscape level : G3,H3,I3.
>
> This is the structure of the data set :
>> str(msc)
> 'data.frame':   1516 obs. of  13 variables:
>   $ y      : int  0 1 0 1 0 0 0 1 0 0 ...
>   $ A1   : num  -0.758 -0.835 -0.835 -0.757 -0.757 ...
>   $ B1   : num  -1.77 -1.77 -1.77 -1.80 -1.80 ...
>   $ C1   : num  -0.262 -0.189 -0.189 -0.286 -0.286 ...
>   $ D2  : num  0.869 0.869 0.869 0.869 0.869 ...
>   $ E2: num  -2.49 -2.49 -2.49 -2.49 -2.49 ...
>   $ F2   : num  -1.09 -1.09 -1.09 -1.09 -1.09 ...
>   $ G3  : num  -0.327 -0.327 -0.327 -0.327 -0.327 ...
>   $ H3  : num  -1.56 -1.56 -1.56 -1.56 -1.56 ...
>   $ I3 : num  1.15 1.15 1.15 1.15 1.15 ...
>   $ POND   : int  1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 ...
>   $ PATCH  : int  1 1 1 1 1 1 1 1 1 1 ...
>   $ LAND   : int  1 1 1 1 1 1 1 1 1 1 ...
>
> Due to the fact that they are measured at a higher level than the
> pond, the values of D2,E2,F2 are repeated for each pond in a one
> particular patch and the values of G3,H3,I3 are repeated for each pond
> of each patch in one particular landscape.
>
> Here is the code that I use :
>
>> glmer(y~A1+B1+C1+D2+EP2+F2+G3+H3+I3+(1|LAND/PATCH),family=binomial,data)
>
> But I find the results a bit strange. Only the variables measured at
> the pond level are significant and I have doubts about this.
>
> Is there another way to fit a model that takes into account the
> spatial scale of each explanatory variable ?
>
> I use the following packages :
>
>> sessionInfo()
> R version 2.7.0 (2008-04-22)
> i386-pc-mingw32
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> other attached packages:
> [1] lme4_0.999375-22   Matrix_0.999375-10 lattice_0.17-6
> loaded via a namespace (and not attached):
> [1] grid_2.7.0
>
> Thanks in advance for your help and answers.
> Best regards.
>
> Julian
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
Julian PICHENOT

2C2A-CERFE
5, rue de la H?ronni?re
08240 BOULT-AUX-BOIS
FRANCE



From David.Duffy at qimr.edu.au  Tue Aug  5 23:31:41 2008
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Wed, 6 Aug 2008 07:31:41 +1000 (EST)
Subject: [R-sig-ME] Mixed model with multiple response variables?
In-Reply-To: <b0f0ada60808051254k6a259417v95cbb7a129321b38@mail.gmail.com>
References: <b0f0ada60808051254k6a259417v95cbb7a129321b38@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0808060730001.25909@orpheus.qimr.edu.au>

On Tue, 5 Aug 2008, Gang Chen wrote:

> Hi,
>
> I have a data set collected from 10 measurements (response variables)
> on two groups (healthy and patient) of subjects performing 4 different
> tasks. In other words there are two fixed factors (group and task),
> and 10 response variables. I could analyze the data with aov() or
> lme() in package nlme for each response variable separately, but since
> most likely there are correlations among the 10 response variables,
> would it be more meaningful to run a MANOVA? However manova() in R
> seems not to allow an error term in the formula. What else can I try
> for this kind of multivariate mixed model?
>

You might look at the Oct 2007 R-News article on the subject.  But a 
flexible approach is to use the sem package.

David Duffy.
-- 
| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v



From A.Robinson at ms.unimelb.edu.au  Wed Aug  6 02:04:19 2008
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Wed, 6 Aug 2008 10:04:19 +1000
Subject: [R-sig-ME] Mixed model with multiple response variables?
In-Reply-To: <Pine.LNX.4.64.0808060730001.25909@orpheus.qimr.edu.au>
References: <b0f0ada60808051254k6a259417v95cbb7a129321b38@mail.gmail.com>
	<Pine.LNX.4.64.0808060730001.25909@orpheus.qimr.edu.au>
Message-ID: <20080806000419.GA26356@ms.unimelb.edu.au>

Hi Gang,

I suggest that you ask yourself whether or not the correlation between
the response variables is of inferential interest in the subject
matter.

If not, then analyze them separately, correct for multiple tests
somehow, and check the correlation of the residuals.  If the residuals
are correlated then a more efficient estimate would be possible using
e.g. a relation to seemingly unrelated regression.  If the residuals
are uncorrelated then I think that you can keep the separate analyses.

If you want to try to model the correlations between the response
variables in an otherwise mixed-effects framework, some nice work was
done by Daniel Hall (U of Georgia) on forestry data, published in
Biometrics, if I recall correctly.  I also tried out some ideas in a
2004 article published in the Canadian Journal of Forest Research. 

Cheers,

Andrew

> On Tue, 5 Aug 2008, Gang Chen wrote:
> 
> Hi,
> 
> I have a data set collected from 10 measurements (response variables)
> on two groups (healthy and patient) of subjects performing 4 different
> tasks. In other words there are two fixed factors (group and task),
> and 10 response variables. I could analyze the data with aov() or
> lme() in package nlme for each response variable separately, but since
> most likely there are correlations among the 10 response variables,
> would it be more meaningful to run a MANOVA? However manova() in R
> seems not to allow an error term in the formula. What else can I try
> for this kind of multivariate mixed model?
> 

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-6410
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/



From ken at kjbeath.com.au  Wed Aug  6 10:47:44 2008
From: ken at kjbeath.com.au (Ken Beath)
Date: Wed, 6 Aug 2008 18:47:44 +1000
Subject: [R-sig-ME] Mixed model with multiple response variables?
In-Reply-To: <b0f0ada60808051254k6a259417v95cbb7a129321b38@mail.gmail.com>
References: <b0f0ada60808051254k6a259417v95cbb7a129321b38@mail.gmail.com>
Message-ID: <5E9E90BE-A9E9-416A-9B34-2512179C2210@kjbeath.com.au>

On 06/08/2008, at 5:54 AM, Gang Chen wrote:

> Hi,
>
> I have a data set collected from 10 measurements (response variables)
> on two groups (healthy and patient) of subjects performing 4 different
> tasks. In other words there are two fixed factors (group and task),
> and 10 response variables. I could analyze the data with aov() or
> lme() in package nlme for each response variable separately, but since
> most likely there are correlations among the 10 response variables,
> would it be more meaningful to run a MANOVA? However manova() in R
> seems not to allow an error term in the formula. What else can I try
> for this kind of multivariate mixed model?
>
> Also, if I want to find out which response variables (among the 10
> measurements) are statistically significant in terms of acting as
> indicators for group difference, what kind of statistical analysis
> would help me sort them out?
>

This looks like a multilevel model, with your measurements nested  
within subject. The difference to typical models is that each outcome  
will need a different variance, which I think is possible in LME.

A GEE might work (as an alternative to multilevel GEE which should)  
using the robust SE to cope with the model misspecification.

Ken



From dafshartous at med.miami.edu  Fri Aug  8 17:06:17 2008
From: dafshartous at med.miami.edu (David Afshartous)
Date: Fri, 08 Aug 2008 11:06:17 -0400
Subject: [R-sig-ME] Different versions of lme4 and covariance of random
	effects
Message-ID: <C4C1D929.6DA0%dafshartous@med.miami.edu>



All,

I recently re-estimated a model after upgrading to Rv2.7.1 that had been
estimated Rv2.6.2 previously and was surprised to see the estimated
correlation between random effects in the model change from 0.3 to 1.0.
It appears that the reason has more to do with the version of lme4 since
when I download the latest possible lme4 to Rv2.6.2 the results agree.

Below is a reproducible example where the difference in results is not as
dramatic.  Of course, for my data the initial results with the correlation
of .3 seem a lot more plausible than that of 1.0 so I'm inclined to trust
those results more than the newer ones.  It seems strange to get the
correlation of 1.  


Cheers,
David

library("lme4")
set.seed(500)
n.timepoints <- 4  ## change for shorter examples
n.subj.per.tx <- 20
sd.d <- 5;
sd.p <- 2;
sd.res <- 1.3
drug <- factor(rep(c("D", "P"), each = n.timepoints, times =
n.subj.per.tx))
drug.baseline <- rep( c(0,5), each=n.timepoints, times=n.subj.per.tx )
Patient <- rep(1:(n.subj.per.tx*2), each = n.timepoints)
Patient.baseline <- rep( rnorm( n.subj.per.tx*2, sd=c(sd.d, sd.p) ),
each=n.timepoints )
time <- factor(paste("Time-", rep(1:n.timepoints, n.subj.per.tx*2),
sep=""))
time.baseline <- rep(1:n.timepoints,n.subj.per.tx*2)*as.numeric(drug=="D")
dv <- rnorm( n.subj.per.tx*n.timepoints*2,
mean=time.baseline+Patient.baseline+drug.baseline, sd=sd.res )
dat.new <- data.frame(time, drug, dv, Patient)
dat.new$Patient.cross <- rep(1:(n.subj.per.tx), each = 2*n.timepoints)


dat.new$Dind <- as.numeric(dat.new$drug == "D")
dat.new$Pind <- as.numeric(dat.new$drug == "P")
dat.new$time.num = rep(1:n.timepoints, n.subj.per.tx*2)

##################################################################

> sessionInfo()
R version 2.6.2 (2008-02-08)
i386-pc-mingw32

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
States.1252;LC_MONETARY=English_United
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] lme4_0.99875-9    Matrix_0.999375-5 lattice_0.17-4

loaded via a namespace (and not attached):
[1] grid_2.6.2
> ( fm.het.3 <- lmer( dv ~  time.num*drug  -1 + ( 0 + Dind  + Pind |
Patient.cross ), data=dat.new ) )
Linear mixed-effects model fit by REML
Formula: dv ~ time.num * drug - 1 + (0 + Dind + Pind | Patient.cross)
   Data: dat.new
   AIC   BIC logLik MLdeviance REMLdeviance
 684.7 706.3 -335.4      669.2        670.7
Random effects:
 Groups        Name Variance Std.Dev. Corr
 Patient.cross Dind 26.8380  5.1805
               Pind  7.7623  2.7861   0.060
 Residual            1.5906  1.2612
number of obs: 160, groups: Patient.cross, 20

Fixed effects:
               Estimate Std. Error t value
time.num         0.9101     0.1261   7.216
drugD           -0.2637     1.2088  -0.218
drugP            5.0330     0.7123   7.066
time.num:drugP  -0.9476     0.1784  -5.313

Correlation of Fixed Effects:
            tim.nm drugD  drugP
drugD       -0.261
drugP        0.000  0.050
tim.nm:drgP -0.707  0.184 -0.313
>

#####################################################################

> sessionInfo()
R version 2.7.1 (2008-06-23)
i386-pc-mingw32

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
States.1252;LC_MONETARY=English_United
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] lme4_0.999375-24   Matrix_0.999375-11 lattice_0.17-8

loaded via a namespace (and not attached):
[1] grid_2.7.1
> ( fm.het.3 <- lmer( dv ~  time.num*drug  -1 + ( 0 + Dind  + Pind |
Patient.cross ), data=dat.new ) )
Linear mixed model fit by REML
Formula: dv ~ time.num * drug - 1 + (0 + Dind + Pind | Patient.cross)
   Data: dat.new
 AIC   BIC logLik deviance REMLdev
 705 729.7 -344.5    687.6     689
Random effects:
 Groups        Name Variance Std.Dev. Corr
 Patient.cross Dind 29.3378  5.4164
               Pind  4.8857  2.2104   0.169
 Residual            1.9651  1.4018
Number of obs: 160, groups: Patient.cross, 20

Fixed effects:
               Estimate Std. Error t value
time.num         0.8175     0.1402   5.832
drugD           -0.8505     1.2705  -0.669
drugP            5.9720     0.6258   9.543
time.num:drugP  -1.1262     0.1982  -5.681

Correlation of Fixed Effects:
            tim.nm drugD  drugP
drugD       -0.276
drugP        0.000  0.127
tim.nm:drgP -0.707  0.195 -0.396
>



From HDoran at air.org  Fri Aug  8 17:30:16 2008
From: HDoran at air.org (Doran, Harold)
Date: Fri, 8 Aug 2008 11:30:16 -0400
Subject: [R-sig-ME] Different versions of lme4 and covariance of
	randomeffects
In-Reply-To: <C4C1D929.6DA0%dafshartous@med.miami.edu>
Message-ID: <ED7B522EE00C9A4FA515AA71724D61EEB86D35@DC1EXCL01.air.org>

David:

I'm looking at this and am a little confused. You say the differences
are not too dramatic. But, they are. I can see that the syntax for your
model specification is the same, but the estimates of the fit statistics
differ (logLik), the estimates of the fixed effects differ, as well as
their standard errors, and the variance components differ.

Am I missing something? Was the same data set used in both cases?

> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org 
> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf 
> Of David Afshartous
> Sent: Friday, August 08, 2008 11:06 AM
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] Different versions of lme4 and covariance 
> of randomeffects
> 
> 
> 
> All,
> 
> I recently re-estimated a model after upgrading to Rv2.7.1 
> that had been estimated Rv2.6.2 previously and was surprised 
> to see the estimated correlation between random effects in 
> the model change from 0.3 to 1.0.
> It appears that the reason has more to do with the version of 
> lme4 since when I download the latest possible lme4 to 
> Rv2.6.2 the results agree.
> 
> Below is a reproducible example where the difference in 
> results is not as dramatic.  Of course, for my data the 
> initial results with the correlation of .3 seem a lot more 
> plausible than that of 1.0 so I'm inclined to trust those 
> results more than the newer ones.  It seems strange to get 
> the correlation of 1.  
> 
> 
> Cheers,
> David
> 
> library("lme4")
> set.seed(500)
> n.timepoints <- 4  ## change for shorter examples 
> n.subj.per.tx <- 20 sd.d <- 5; sd.p <- 2; sd.res <- 1.3 drug 
> <- factor(rep(c("D", "P"), each = n.timepoints, times =
> n.subj.per.tx))
> drug.baseline <- rep( c(0,5), each=n.timepoints, 
> times=n.subj.per.tx ) Patient <- rep(1:(n.subj.per.tx*2), 
> each = n.timepoints) Patient.baseline <- rep( rnorm( 
> n.subj.per.tx*2, sd=c(sd.d, sd.p) ), each=n.timepoints ) time 
> <- factor(paste("Time-", rep(1:n.timepoints, n.subj.per.tx*2),
> sep=""))
> time.baseline <- 
> rep(1:n.timepoints,n.subj.per.tx*2)*as.numeric(drug=="D")
> dv <- rnorm( n.subj.per.tx*n.timepoints*2, 
> mean=time.baseline+Patient.baseline+drug.baseline, sd=sd.res 
> ) dat.new <- data.frame(time, drug, dv, Patient) 
> dat.new$Patient.cross <- rep(1:(n.subj.per.tx), each = 2*n.timepoints)
> 
> 
> dat.new$Dind <- as.numeric(dat.new$drug == "D") dat.new$Pind 
> <- as.numeric(dat.new$drug == "P") dat.new$time.num = 
> rep(1:n.timepoints, n.subj.per.tx*2)
> 
> ##################################################################
> 
> > sessionInfo()
> R version 2.6.2 (2008-02-08)
> i386-pc-mingw32
> 
> locale:
> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
> States.1252;LC_MONETARY=English_United
> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> 
> other attached packages:
> [1] lme4_0.99875-9    Matrix_0.999375-5 lattice_0.17-4
> 
> loaded via a namespace (and not attached):
> [1] grid_2.6.2
> > ( fm.het.3 <- lmer( dv ~  time.num*drug  -1 + ( 0 + Dind  + Pind |
> Patient.cross ), data=dat.new ) )
> Linear mixed-effects model fit by REML
> Formula: dv ~ time.num * drug - 1 + (0 + Dind + Pind | Patient.cross)
>    Data: dat.new
>    AIC   BIC logLik MLdeviance REMLdeviance
>  684.7 706.3 -335.4      669.2        670.7
> Random effects:
>  Groups        Name Variance Std.Dev. Corr
>  Patient.cross Dind 26.8380  5.1805
>                Pind  7.7623  2.7861   0.060
>  Residual            1.5906  1.2612
> number of obs: 160, groups: Patient.cross, 20
> 
> Fixed effects:
>                Estimate Std. Error t value
> time.num         0.9101     0.1261   7.216
> drugD           -0.2637     1.2088  -0.218
> drugP            5.0330     0.7123   7.066
> time.num:drugP  -0.9476     0.1784  -5.313
> 
> Correlation of Fixed Effects:
>             tim.nm drugD  drugP
> drugD       -0.261
> drugP        0.000  0.050
> tim.nm:drgP -0.707  0.184 -0.313
> >
> 
> #####################################################################
> 
> > sessionInfo()
> R version 2.7.1 (2008-06-23)
> i386-pc-mingw32
> 
> locale:
> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
> States.1252;LC_MONETARY=English_United
> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> 
> other attached packages:
> [1] lme4_0.999375-24   Matrix_0.999375-11 lattice_0.17-8
> 
> loaded via a namespace (and not attached):
> [1] grid_2.7.1
> > ( fm.het.3 <- lmer( dv ~  time.num*drug  -1 + ( 0 + Dind  + Pind |
> Patient.cross ), data=dat.new ) )
> Linear mixed model fit by REML
> Formula: dv ~ time.num * drug - 1 + (0 + Dind + Pind | Patient.cross)
>    Data: dat.new
>  AIC   BIC logLik deviance REMLdev
>  705 729.7 -344.5    687.6     689
> Random effects:
>  Groups        Name Variance Std.Dev. Corr
>  Patient.cross Dind 29.3378  5.4164
>                Pind  4.8857  2.2104   0.169
>  Residual            1.9651  1.4018
> Number of obs: 160, groups: Patient.cross, 20
> 
> Fixed effects:
>                Estimate Std. Error t value
> time.num         0.8175     0.1402   5.832
> drugD           -0.8505     1.2705  -0.669
> drugP            5.9720     0.6258   9.543
> time.num:drugP  -1.1262     0.1982  -5.681
> 
> Correlation of Fixed Effects:
>             tim.nm drugD  drugP
> drugD       -0.276
> drugP        0.000  0.127
> tim.nm:drgP -0.707  0.195 -0.396
> >
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 



From dafshartous at med.miami.edu  Fri Aug  8 17:40:14 2008
From: dafshartous at med.miami.edu (David Afshartous)
Date: Fri, 08 Aug 2008 11:40:14 -0400
Subject: [R-sig-ME] Different versions of lme4 and covariance of
 randomeffects
In-Reply-To: <ED7B522EE00C9A4FA515AA71724D61EEB86D35@DC1EXCL01.air.org>
Message-ID: <C4C1E11E.6DA5%dafshartous@med.miami.edu>


Donald,

Sorry, you're right, for the reproducible example I was mainly focusing on
the correlation term but when I look closer there are other differences as
well.  And yes, the same data was used, generated via the code below with
same seed at set.seed(500).

Cheers,
David




On 8/8/08 11:30 AM, "Doran, Harold" <HDoran at air.org> wrote:

> David:
> 
> I'm looking at this and am a little confused. You say the differences
> are not too dramatic. But, they are. I can see that the syntax for your
> model specification is the same, but the estimates of the fit statistics
> differ (logLik), the estimates of the fixed effects differ, as well as
> their standard errors, and the variance components differ.
> 
> Am I missing something? Was the same data set used in both cases?
> 
>> -----Original Message-----
>> From: r-sig-mixed-models-bounces at r-project.org
>> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf
>> Of David Afshartous
>> Sent: Friday, August 08, 2008 11:06 AM
>> To: r-sig-mixed-models at r-project.org
>> Subject: [R-sig-ME] Different versions of lme4 and covariance
>> of randomeffects
>> 
>> 
>> 
>> All,
>> 
>> I recently re-estimated a model after upgrading to Rv2.7.1
>> that had been estimated Rv2.6.2 previously and was surprised
>> to see the estimated correlation between random effects in
>> the model change from 0.3 to 1.0.
>> It appears that the reason has more to do with the version of
>> lme4 since when I download the latest possible lme4 to
>> Rv2.6.2 the results agree.
>> 
>> Below is a reproducible example where the difference in
>> results is not as dramatic.  Of course, for my data the
>> initial results with the correlation of .3 seem a lot more
>> plausible than that of 1.0 so I'm inclined to trust those
>> results more than the newer ones.  It seems strange to get
>> the correlation of 1.
>> 
>> 
>> Cheers,
>> David
>> 
>> library("lme4")
>> set.seed(500)
>> n.timepoints <- 4  ## change for shorter examples
>> n.subj.per.tx <- 20 sd.d <- 5; sd.p <- 2; sd.res <- 1.3 drug
>> <- factor(rep(c("D", "P"), each = n.timepoints, times =
>> n.subj.per.tx))
>> drug.baseline <- rep( c(0,5), each=n.timepoints,
>> times=n.subj.per.tx ) Patient <- rep(1:(n.subj.per.tx*2),
>> each = n.timepoints) Patient.baseline <- rep( rnorm(
>> n.subj.per.tx*2, sd=c(sd.d, sd.p) ), each=n.timepoints ) time
>> <- factor(paste("Time-", rep(1:n.timepoints, n.subj.per.tx*2),
>> sep=""))
>> time.baseline <-
>> rep(1:n.timepoints,n.subj.per.tx*2)*as.numeric(drug=="D")
>> dv <- rnorm( n.subj.per.tx*n.timepoints*2,
>> mean=time.baseline+Patient.baseline+drug.baseline, sd=sd.res
>> ) dat.new <- data.frame(time, drug, dv, Patient)
>> dat.new$Patient.cross <- rep(1:(n.subj.per.tx), each = 2*n.timepoints)
>> 
>> 
>> dat.new$Dind <- as.numeric(dat.new$drug == "D") dat.new$Pind
>> <- as.numeric(dat.new$drug == "P") dat.new$time.num =
>> rep(1:n.timepoints, n.subj.per.tx*2)
>> 
>> ##################################################################
>> 
>>> sessionInfo()
>> R version 2.6.2 (2008-02-08)
>> i386-pc-mingw32
>> 
>> locale:
>> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
>> States.1252;LC_MONETARY=English_United
>> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
>> 
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>> 
>> other attached packages:
>> [1] lme4_0.99875-9    Matrix_0.999375-5 lattice_0.17-4
>> 
>> loaded via a namespace (and not attached):
>> [1] grid_2.6.2
>>> ( fm.het.3 <- lmer( dv ~  time.num*drug  -1 + ( 0 + Dind  + Pind |
>> Patient.cross ), data=dat.new ) )
>> Linear mixed-effects model fit by REML
>> Formula: dv ~ time.num * drug - 1 + (0 + Dind + Pind | Patient.cross)
>>    Data: dat.new
>>    AIC   BIC logLik MLdeviance REMLdeviance
>>  684.7 706.3 -335.4      669.2        670.7
>> Random effects:
>>  Groups        Name Variance Std.Dev. Corr
>>  Patient.cross Dind 26.8380  5.1805
>>                Pind  7.7623  2.7861   0.060
>>  Residual            1.5906  1.2612
>> number of obs: 160, groups: Patient.cross, 20
>> 
>> Fixed effects:
>>                Estimate Std. Error t value
>> time.num         0.9101     0.1261   7.216
>> drugD           -0.2637     1.2088  -0.218
>> drugP            5.0330     0.7123   7.066
>> time.num:drugP  -0.9476     0.1784  -5.313
>> 
>> Correlation of Fixed Effects:
>>             tim.nm drugD  drugP
>> drugD       -0.261
>> drugP        0.000  0.050
>> tim.nm:drgP -0.707  0.184 -0.313
>>> 
>> 
>> #####################################################################
>> 
>>> sessionInfo()
>> R version 2.7.1 (2008-06-23)
>> i386-pc-mingw32
>> 
>> locale:
>> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
>> States.1252;LC_MONETARY=English_United
>> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
>> 
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>> 
>> other attached packages:
>> [1] lme4_0.999375-24   Matrix_0.999375-11 lattice_0.17-8
>> 
>> loaded via a namespace (and not attached):
>> [1] grid_2.7.1
>>> ( fm.het.3 <- lmer( dv ~  time.num*drug  -1 + ( 0 + Dind  + Pind |
>> Patient.cross ), data=dat.new ) )
>> Linear mixed model fit by REML
>> Formula: dv ~ time.num * drug - 1 + (0 + Dind + Pind | Patient.cross)
>>    Data: dat.new
>>  AIC   BIC logLik deviance REMLdev
>>  705 729.7 -344.5    687.6     689
>> Random effects:
>>  Groups        Name Variance Std.Dev. Corr
>>  Patient.cross Dind 29.3378  5.4164
>>                Pind  4.8857  2.2104   0.169
>>  Residual            1.9651  1.4018
>> Number of obs: 160, groups: Patient.cross, 20
>> 
>> Fixed effects:
>>                Estimate Std. Error t value
>> time.num         0.8175     0.1402   5.832
>> drugD           -0.8505     1.2705  -0.669
>> drugP            5.9720     0.6258   9.543
>> time.num:drugP  -1.1262     0.1982  -5.681
>> 
>> Correlation of Fixed Effects:
>>             tim.nm drugD  drugP
>> drugD       -0.276
>> drugP        0.000  0.127
>> tim.nm:drgP -0.707  0.195 -0.396
>>> 
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>



From HDoran at air.org  Fri Aug  8 17:54:07 2008
From: HDoran at air.org (Doran, Harold)
Date: Fri, 8 Aug 2008 11:54:07 -0400
Subject: [R-sig-ME] Different versions of lme4 and covariance of
	randomeffects
In-Reply-To: <C4C1E11E.6DA5%dafshartous@med.miami.edu>
Message-ID: <ED7B522EE00C9A4FA515AA71724D61EEB86D3E@DC1EXCL01.air.org>

Well, I don't get this problem. Using your code, I get the same exact
output from 2.6.2 and 2.7.1. My sessionInfo shows that I have the same
packages as you in 2.7.1, but in 2.6.2 you have an older version of
Matrix. My output showing the same estimates from both R versions is
pasted below.

## output from 2.7.1
> summary(fm.het.3)
Linear mixed model fit by REML 
Formula: dv ~ time.num * drug - 1 + (0 + Dind + Pind | Patient.cross) 
   Data: dat.new 
 AIC   BIC logLik deviance REMLdev
 705 729.7 -344.5    687.6     689
Random effects:
 Groups        Name Variance Std.Dev. Corr  
 Patient.cross Dind 29.3378  5.4164         
               Pind  4.8857  2.2104   0.169 
 Residual            1.9651  1.4018         
Number of obs: 160, groups: Patient.cross, 20

Fixed effects:
               Estimate Std. Error t value
time.num         0.8175     0.1402   5.832
drugD           -0.8505     1.2705  -0.669
drugP            5.9720     0.6258   9.543
time.num:drugP  -1.1262     0.1982  -5.681

Correlation of Fixed Effects:
            tim.nm drugD  drugP 
drugD       -0.276              
drugP        0.000  0.127       
tim.nm:drgP -0.707  0.195 -0.396
> sessionInfo()
R version 2.7.1 (2008-06-23) 
i386-pc-mingw32 

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
States.1252;LC_MONETARY=English_United
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base


other attached packages:
[1] lme4_0.999375-24   Matrix_0.999375-11 lattice_0.17-13   

loaded via a namespace (and not attached):
[1] grid_2.7.1  tools_2.7.1 

### Output from 2.6.2
> summary(fm.het.3)
Linear mixed-effects model fit by REML 
Formula: dv ~ time.num * drug - 1 + (0 + Dind + Pind | Patient.cross) 
   Data: dat.new 
 AIC   BIC logLik MLdeviance REMLdeviance
 703 724.6 -344.5      687.6          689
Random effects:
 Groups        Name Variance Std.Dev. Corr  
 Patient.cross Dind 29.3339  5.4161         
               Pind  4.8854  2.2103   0.169 
 Residual            1.9651  1.4018         
number of obs: 160, groups: Patient.cross, 20

Fixed effects:
               Estimate Std. Error t value
time.num         0.8175     0.1402   5.832
drugD           -0.8505     1.2705  -0.669
drugP            5.9720     0.6258   9.543
time.num:drugP  -1.1262     0.1982  -5.681

Correlation of Fixed Effects:
            tim.nm drugD  drugP 
drugD       -0.276              
drugP        0.000  0.127       
tim.nm:drgP -0.707  0.195 -0.396
> sessionInfo()
R version 2.6.2 (2008-02-08) 
i386-pc-mingw32 

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
States.1252;LC_MONETARY=English_United
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base


other attached packages:
[1] lme4_0.99875-9    Matrix_0.999375-7 lattice_0.17-4   

loaded via a namespace (and not attached):
[1] grid_2.6.2


> -----Original Message-----
> From: David Afshartous [mailto:dafshartous at med.miami.edu] 
> Sent: Friday, August 08, 2008 11:40 AM
> To: Doran, Harold; r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Different versions of lme4 and 
> covariance of randomeffects
> 
> 
> Donald,
> 
> Sorry, you're right, for the reproducible example I was 
> mainly focusing on the correlation term but when I look 
> closer there are other differences as well.  And yes, the 
> same data was used, generated via the code below with same 
> seed at set.seed(500).
> 
> Cheers,
> David
> 
> 
> 
> 
> On 8/8/08 11:30 AM, "Doran, Harold" <HDoran at air.org> wrote:
> 
> > David:
> > 
> > I'm looking at this and am a little confused. You say the 
> differences 
> > are not too dramatic. But, they are. I can see that the syntax for 
> > your model specification is the same, but the estimates of the fit 
> > statistics differ (logLik), the estimates of the fixed 
> effects differ, 
> > as well as their standard errors, and the variance 
> components differ.
> > 
> > Am I missing something? Was the same data set used in both cases?
> > 
> >> -----Original Message-----
> >> From: r-sig-mixed-models-bounces at r-project.org
> >> [mailto:r-sig-mixed-models-bounces at r-project.org] On 
> Behalf Of David 
> >> Afshartous
> >> Sent: Friday, August 08, 2008 11:06 AM
> >> To: r-sig-mixed-models at r-project.org
> >> Subject: [R-sig-ME] Different versions of lme4 and covariance of 
> >> randomeffects
> >> 
> >> 
> >> 
> >> All,
> >> 
> >> I recently re-estimated a model after upgrading to Rv2.7.1 
> that had 
> >> been estimated Rv2.6.2 previously and was surprised to see the 
> >> estimated correlation between random effects in the model 
> change from 
> >> 0.3 to 1.0.
> >> It appears that the reason has more to do with the version of
> >> lme4 since when I download the latest possible lme4 to
> >> Rv2.6.2 the results agree.
> >> 
> >> Below is a reproducible example where the difference in results is 
> >> not as dramatic.  Of course, for my data the initial 
> results with the 
> >> correlation of .3 seem a lot more plausible than that of 
> 1.0 so I'm 
> >> inclined to trust those results more than the newer ones.  
> It seems 
> >> strange to get the correlation of 1.
> >> 
> >> 
> >> Cheers,
> >> David
> >> 
> >> library("lme4")
> >> set.seed(500)
> >> n.timepoints <- 4  ## change for shorter examples 
> n.subj.per.tx <- 20 
> >> sd.d <- 5; sd.p <- 2; sd.res <- 1.3 drug
> >> <- factor(rep(c("D", "P"), each = n.timepoints, times =
> >> n.subj.per.tx))
> >> drug.baseline <- rep( c(0,5), each=n.timepoints, 
> times=n.subj.per.tx 
> >> ) Patient <- rep(1:(n.subj.per.tx*2), each = n.timepoints) 
> >> Patient.baseline <- rep( rnorm( n.subj.per.tx*2, 
> sd=c(sd.d, sd.p) ), 
> >> each=n.timepoints ) time
> >> <- factor(paste("Time-", rep(1:n.timepoints, n.subj.per.tx*2),
> >> sep=""))
> >> time.baseline <-
> >> rep(1:n.timepoints,n.subj.per.tx*2)*as.numeric(drug=="D")
> >> dv <- rnorm( n.subj.per.tx*n.timepoints*2, 
> >> mean=time.baseline+Patient.baseline+drug.baseline, sd=sd.res
> >> ) dat.new <- data.frame(time, drug, dv, Patient) 
> >> dat.new$Patient.cross <- rep(1:(n.subj.per.tx), each = 
> >> 2*n.timepoints)
> >> 
> >> 
> >> dat.new$Dind <- as.numeric(dat.new$drug == "D") dat.new$Pind
> >> <- as.numeric(dat.new$drug == "P") dat.new$time.num = 
> >> rep(1:n.timepoints, n.subj.per.tx*2)
> >> 
> >> ##################################################################
> >> 
> >>> sessionInfo()
> >> R version 2.6.2 (2008-02-08)
> >> i386-pc-mingw32
> >> 
> >> locale:
> >> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
> >> States.1252;LC_MONETARY=English_United
> >> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
> >> 
> >> attached base packages:
> >> [1] stats     graphics  grDevices utils     datasets  
> methods   base
> >> 
> >> other attached packages:
> >> [1] lme4_0.99875-9    Matrix_0.999375-5 lattice_0.17-4
> >> 
> >> loaded via a namespace (and not attached):
> >> [1] grid_2.6.2
> >>> ( fm.het.3 <- lmer( dv ~  time.num*drug  -1 + ( 0 + Dind  + Pind |
> >> Patient.cross ), data=dat.new ) )
> >> Linear mixed-effects model fit by REML
> >> Formula: dv ~ time.num * drug - 1 + (0 + Dind + Pind | 
> Patient.cross)
> >>    Data: dat.new
> >>    AIC   BIC logLik MLdeviance REMLdeviance
> >>  684.7 706.3 -335.4      669.2        670.7
> >> Random effects:
> >>  Groups        Name Variance Std.Dev. Corr
> >>  Patient.cross Dind 26.8380  5.1805
> >>                Pind  7.7623  2.7861   0.060
> >>  Residual            1.5906  1.2612
> >> number of obs: 160, groups: Patient.cross, 20
> >> 
> >> Fixed effects:
> >>                Estimate Std. Error t value
> >> time.num         0.9101     0.1261   7.216
> >> drugD           -0.2637     1.2088  -0.218
> >> drugP            5.0330     0.7123   7.066
> >> time.num:drugP  -0.9476     0.1784  -5.313
> >> 
> >> Correlation of Fixed Effects:
> >>             tim.nm drugD  drugP
> >> drugD       -0.261
> >> drugP        0.000  0.050
> >> tim.nm:drgP -0.707  0.184 -0.313
> >>> 
> >> 
> >> 
> #####################################################################
> >> 
> >>> sessionInfo()
> >> R version 2.7.1 (2008-06-23)
> >> i386-pc-mingw32
> >> 
> >> locale:
> >> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
> >> States.1252;LC_MONETARY=English_United
> >> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
> >> 
> >> attached base packages:
> >> [1] stats     graphics  grDevices utils     datasets  
> methods   base
> >> 
> >> other attached packages:
> >> [1] lme4_0.999375-24   Matrix_0.999375-11 lattice_0.17-8
> >> 
> >> loaded via a namespace (and not attached):
> >> [1] grid_2.7.1
> >>> ( fm.het.3 <- lmer( dv ~  time.num*drug  -1 + ( 0 + Dind  + Pind |
> >> Patient.cross ), data=dat.new ) )
> >> Linear mixed model fit by REML
> >> Formula: dv ~ time.num * drug - 1 + (0 + Dind + Pind | 
> Patient.cross)
> >>    Data: dat.new
> >>  AIC   BIC logLik deviance REMLdev
> >>  705 729.7 -344.5    687.6     689
> >> Random effects:
> >>  Groups        Name Variance Std.Dev. Corr
> >>  Patient.cross Dind 29.3378  5.4164
> >>                Pind  4.8857  2.2104   0.169
> >>  Residual            1.9651  1.4018
> >> Number of obs: 160, groups: Patient.cross, 20
> >> 
> >> Fixed effects:
> >>                Estimate Std. Error t value
> >> time.num         0.8175     0.1402   5.832
> >> drugD           -0.8505     1.2705  -0.669
> >> drugP            5.9720     0.6258   9.543
> >> time.num:drugP  -1.1262     0.1982  -5.681
> >> 
> >> Correlation of Fixed Effects:
> >>             tim.nm drugD  drugP
> >> drugD       -0.276
> >> drugP        0.000  0.127
> >> tim.nm:drgP -0.707  0.195 -0.396
> >>> 
> >> 
> >> _______________________________________________
> >> R-sig-mixed-models at r-project.org mailing list 
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >> 
> 
> 



From dafshartous at med.miami.edu  Fri Aug  8 18:13:49 2008
From: dafshartous at med.miami.edu (David Afshartous)
Date: Fri, 08 Aug 2008 12:13:49 -0400
Subject: [R-sig-ME] Different versions of lme4 and covariance of
 randomeffects
In-Reply-To: <ED7B522EE00C9A4FA515AA71724D61EEB86D3E@DC1EXCL01.air.org>
Message-ID: <C4C1E8FD.6DA9%dafshartous@med.miami.edu>



I just installed the latest version of Matrix in the 2.6.2 session and also
get the same results as well.  So it appears that the version of Matrix was
the issue.  However, now that I have the newer version of Matrix how does
one re-produce the results that the older version was producing?



On 8/8/08 11:54 AM, "Doran, Harold" <HDoran at air.org> wrote:

> Well, I don't get this problem. Using your code, I get the same exact
> output from 2.6.2 and 2.7.1. My sessionInfo shows that I have the same
> packages as you in 2.7.1, but in 2.6.2 you have an older version of
> Matrix. My output showing the same estimates from both R versions is
> pasted below.
> 
> ## output from 2.7.1
>> summary(fm.het.3)
> Linear mixed model fit by REML
> Formula: dv ~ time.num * drug - 1 + (0 + Dind + Pind | Patient.cross)
>    Data: dat.new 
>  AIC   BIC logLik deviance REMLdev
>  705 729.7 -344.5    687.6     689
> Random effects:
>  Groups        Name Variance Std.Dev. Corr
>  Patient.cross Dind 29.3378  5.4164
>                Pind  4.8857  2.2104   0.169
>  Residual            1.9651  1.4018
> Number of obs: 160, groups: Patient.cross, 20
> 
> Fixed effects:
>                Estimate Std. Error t value
> time.num         0.8175     0.1402   5.832
> drugD           -0.8505     1.2705  -0.669
> drugP            5.9720     0.6258   9.543
> time.num:drugP  -1.1262     0.1982  -5.681
> 
> Correlation of Fixed Effects:
>             tim.nm drugD  drugP
> drugD       -0.276
> drugP        0.000  0.127
> tim.nm:drgP -0.707  0.195 -0.396
>> sessionInfo()
> R version 2.7.1 (2008-06-23)
> i386-pc-mingw32 
> 
> locale:
> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
> States.1252;LC_MONETARY=English_United
> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> 
> 
> other attached packages:
> [1] lme4_0.999375-24   Matrix_0.999375-11 lattice_0.17-13
> 
> loaded via a namespace (and not attached):
> [1] grid_2.7.1  tools_2.7.1
> 
> ### Output from 2.6.2
>> summary(fm.het.3)
> Linear mixed-effects model fit by REML
> Formula: dv ~ time.num * drug - 1 + (0 + Dind + Pind | Patient.cross)
>    Data: dat.new 
>  AIC   BIC logLik MLdeviance REMLdeviance
>  703 724.6 -344.5      687.6          689
> Random effects:
>  Groups        Name Variance Std.Dev. Corr
>  Patient.cross Dind 29.3339  5.4161
>                Pind  4.8854  2.2103   0.169
>  Residual            1.9651  1.4018
> number of obs: 160, groups: Patient.cross, 20
> 
> Fixed effects:
>                Estimate Std. Error t value
> time.num         0.8175     0.1402   5.832
> drugD           -0.8505     1.2705  -0.669
> drugP            5.9720     0.6258   9.543
> time.num:drugP  -1.1262     0.1982  -5.681
> 
> Correlation of Fixed Effects:
>             tim.nm drugD  drugP
> drugD       -0.276
> drugP        0.000  0.127
> tim.nm:drgP -0.707  0.195 -0.396
>> sessionInfo()
> R version 2.6.2 (2008-02-08)
> i386-pc-mingw32 
> 
> locale:
> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
> States.1252;LC_MONETARY=English_United
> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> 
> 
> other attached packages:
> [1] lme4_0.99875-9    Matrix_0.999375-7 lattice_0.17-4
> 
> loaded via a namespace (and not attached):
> [1] grid_2.6.2
> 
> 
>> -----Original Message-----
>> From: David Afshartous [mailto:dafshartous at med.miami.edu]
>> Sent: Friday, August 08, 2008 11:40 AM
>> To: Doran, Harold; r-sig-mixed-models at r-project.org
>> Subject: Re: [R-sig-ME] Different versions of lme4 and
>> covariance of randomeffects
>> 
>> 
>> Donald,
>> 
>> Sorry, you're right, for the reproducible example I was
>> mainly focusing on the correlation term but when I look
>> closer there are other differences as well.  And yes, the
>> same data was used, generated via the code below with same
>> seed at set.seed(500).
>> 
>> Cheers,
>> David
>> 
>> 
>> 
>> 
>> On 8/8/08 11:30 AM, "Doran, Harold" <HDoran at air.org> wrote:
>> 
>>> David:
>>> 
>>> I'm looking at this and am a little confused. You say the
>> differences 
>>> are not too dramatic. But, they are. I can see that the syntax for
>>> your model specification is the same, but the estimates of the fit
>>> statistics differ (logLik), the estimates of the fixed
>> effects differ, 
>>> as well as their standard errors, and the variance
>> components differ.
>>> 
>>> Am I missing something? Was the same data set used in both cases?
>>> 
>>>> -----Original Message-----
>>>> From: r-sig-mixed-models-bounces at r-project.org
>>>> [mailto:r-sig-mixed-models-bounces at r-project.org] On
>> Behalf Of David 
>>>> Afshartous
>>>> Sent: Friday, August 08, 2008 11:06 AM
>>>> To: r-sig-mixed-models at r-project.org
>>>> Subject: [R-sig-ME] Different versions of lme4 and covariance of
>>>> randomeffects
>>>> 
>>>> 
>>>> 
>>>> All,
>>>> 
>>>> I recently re-estimated a model after upgrading to Rv2.7.1
>> that had 
>>>> been estimated Rv2.6.2 previously and was surprised to see the
>>>> estimated correlation between random effects in the model
>> change from 
>>>> 0.3 to 1.0.
>>>> It appears that the reason has more to do with the version of
>>>> lme4 since when I download the latest possible lme4 to
>>>> Rv2.6.2 the results agree.
>>>> 
>>>> Below is a reproducible example where the difference in results is
>>>> not as dramatic.  Of course, for my data the initial
>> results with the
>>>> correlation of .3 seem a lot more plausible than that of
>> 1.0 so I'm 
>>>> inclined to trust those results more than the newer ones.
>> It seems 
>>>> strange to get the correlation of 1.
>>>> 
>>>> 
>>>> Cheers,
>>>> David
>>>> 
>>>> library("lme4")
>>>> set.seed(500)
>>>> n.timepoints <- 4  ## change for shorter examples
>> n.subj.per.tx <- 20
>>>> sd.d <- 5; sd.p <- 2; sd.res <- 1.3 drug
>>>> <- factor(rep(c("D", "P"), each = n.timepoints, times =
>>>> n.subj.per.tx))
>>>> drug.baseline <- rep( c(0,5), each=n.timepoints,
>> times=n.subj.per.tx
>>>> ) Patient <- rep(1:(n.subj.per.tx*2), each = n.timepoints)
>>>> Patient.baseline <- rep( rnorm( n.subj.per.tx*2,
>> sd=c(sd.d, sd.p) ),
>>>> each=n.timepoints ) time
>>>> <- factor(paste("Time-", rep(1:n.timepoints, n.subj.per.tx*2),
>>>> sep=""))
>>>> time.baseline <-
>>>> rep(1:n.timepoints,n.subj.per.tx*2)*as.numeric(drug=="D")
>>>> dv <- rnorm( n.subj.per.tx*n.timepoints*2,
>>>> mean=time.baseline+Patient.baseline+drug.baseline, sd=sd.res
>>>> ) dat.new <- data.frame(time, drug, dv, Patient)
>>>> dat.new$Patient.cross <- rep(1:(n.subj.per.tx), each =
>>>> 2*n.timepoints)
>>>> 
>>>> 
>>>> dat.new$Dind <- as.numeric(dat.new$drug == "D") dat.new$Pind
>>>> <- as.numeric(dat.new$drug == "P") dat.new$time.num =
>>>> rep(1:n.timepoints, n.subj.per.tx*2)
>>>> 
>>>> ##################################################################
>>>> 
>>>>> sessionInfo()
>>>> R version 2.6.2 (2008-02-08)
>>>> i386-pc-mingw32
>>>> 
>>>> locale:
>>>> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
>>>> States.1252;LC_MONETARY=English_United
>>>> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
>>>> 
>>>> attached base packages:
>>>> [1] stats     graphics  grDevices utils     datasets
>> methods   base
>>>> 
>>>> other attached packages:
>>>> [1] lme4_0.99875-9    Matrix_0.999375-5 lattice_0.17-4
>>>> 
>>>> loaded via a namespace (and not attached):
>>>> [1] grid_2.6.2
>>>>> ( fm.het.3 <- lmer( dv ~  time.num*drug  -1 + ( 0 + Dind  + Pind |
>>>> Patient.cross ), data=dat.new ) )
>>>> Linear mixed-effects model fit by REML
>>>> Formula: dv ~ time.num * drug - 1 + (0 + Dind + Pind |
>> Patient.cross)
>>>>    Data: dat.new
>>>>    AIC   BIC logLik MLdeviance REMLdeviance
>>>>  684.7 706.3 -335.4      669.2        670.7
>>>> Random effects:
>>>>  Groups        Name Variance Std.Dev. Corr
>>>>  Patient.cross Dind 26.8380  5.1805
>>>>                Pind  7.7623  2.7861   0.060
>>>>  Residual            1.5906  1.2612
>>>> number of obs: 160, groups: Patient.cross, 20
>>>> 
>>>> Fixed effects:
>>>>                Estimate Std. Error t value
>>>> time.num         0.9101     0.1261   7.216
>>>> drugD           -0.2637     1.2088  -0.218
>>>> drugP            5.0330     0.7123   7.066
>>>> time.num:drugP  -0.9476     0.1784  -5.313
>>>> 
>>>> Correlation of Fixed Effects:
>>>>             tim.nm drugD  drugP
>>>> drugD       -0.261
>>>> drugP        0.000  0.050
>>>> tim.nm:drgP -0.707  0.184 -0.313
>>>>> 
>>>> 
>>>> 
>> #####################################################################
>>>> 
>>>>> sessionInfo()
>>>> R version 2.7.1 (2008-06-23)
>>>> i386-pc-mingw32
>>>> 
>>>> locale:
>>>> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
>>>> States.1252;LC_MONETARY=English_United
>>>> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
>>>> 
>>>> attached base packages:
>>>> [1] stats     graphics  grDevices utils     datasets
>> methods   base
>>>> 
>>>> other attached packages:
>>>> [1] lme4_0.999375-24   Matrix_0.999375-11 lattice_0.17-8
>>>> 
>>>> loaded via a namespace (and not attached):
>>>> [1] grid_2.7.1
>>>>> ( fm.het.3 <- lmer( dv ~  time.num*drug  -1 + ( 0 + Dind  + Pind |
>>>> Patient.cross ), data=dat.new ) )
>>>> Linear mixed model fit by REML
>>>> Formula: dv ~ time.num * drug - 1 + (0 + Dind + Pind |
>> Patient.cross)
>>>>    Data: dat.new
>>>>  AIC   BIC logLik deviance REMLdev
>>>>  705 729.7 -344.5    687.6     689
>>>> Random effects:
>>>>  Groups        Name Variance Std.Dev. Corr
>>>>  Patient.cross Dind 29.3378  5.4164
>>>>                Pind  4.8857  2.2104   0.169
>>>>  Residual            1.9651  1.4018
>>>> Number of obs: 160, groups: Patient.cross, 20
>>>> 
>>>> Fixed effects:
>>>>                Estimate Std. Error t value
>>>> time.num         0.8175     0.1402   5.832
>>>> drugD           -0.8505     1.2705  -0.669
>>>> drugP            5.9720     0.6258   9.543
>>>> time.num:drugP  -1.1262     0.1982  -5.681
>>>> 
>>>> Correlation of Fixed Effects:
>>>>             tim.nm drugD  drugP
>>>> drugD       -0.276
>>>> drugP        0.000  0.127
>>>> tim.nm:drgP -0.707  0.195 -0.396
>>>>> 
>>>> 
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>> 
>> 
>>



From dafshartous at med.miami.edu  Fri Aug  8 20:32:49 2008
From: dafshartous at med.miami.edu (David Afshartous)
Date: Fri, 08 Aug 2008 14:32:49 -0400
Subject: [R-sig-ME] HPDinterval, and mcmcpvalue from R-Wiki discussion
Message-ID: <C4C20991.6DAF%dafshartous@med.miami.edu>


All,

I am trying to apply HPDinterval and mcmcpvalue as shown in the discussion
at R-Wiki  
((http://wiki.r-project.org/rwiki/doku.php?id=guides:lmer-tests&s=lme%20and%
20aov).

Three questions:
1) why am I getting the error message for HPDinterval given that I'm
following the exact steps in the discussion?
2) is it normal that my p-value (p=.72) differs that much from the one in
the discussion (p=.46)?
3) why isn't mcmcpvalue written to work on a single column? (see below)

Thanks,
David



> sessionInfo()
R version 2.7.1 (2008-06-23)
i386-apple-darwin8.10.1

locale:
en_US.UTF-8/en_US.UTF-8/C/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] SASmixed_0.4-4     coda_0.13-2        Hmisc_3.4-3        nlme_3.1-89
lme4_0.999375-24   Matrix_0.999375-11
[7] lattice_0.17-8 

loaded via a namespace (and not attached):
[1] cluster_1.11.11 grid_2.7.1      tools_2.7.1
 data(AvgDailyGain, package = "SASmixed")
 (fm1Adg <- lmer(adg ~ InitWt*Treatment - 1 + (1|Block), AvgDailyGain))

> AdgS1 <- mcmcsamp(fm1Adg, 50000)
> library(coda)
> HPDinterval(AdgS1)
Error in UseMethod("HPDinterval") :
  no applicable method for "HPDinterval"

> mcmcpvalue(as.matrix(AdgS1)[, 6:8])
[1] 0.7231
> mcmcpvalue(as.matrix(AdgS1)[, 1])
Error in base::colMeans(x, na.rm = na.rm, dims = dims) :
  'x' must be an array of at least two dimensions
> 



mcmcpvalue <- function(samp)
{
   ## elementary version that creates an empirical p-value for the
   ## hypothesis that the columns of samp have mean zero versus a
   ## general multivariate distribution with elliptical contours.

   ## differences from the mean standardized by the observed
   ## variance-covariance factor
   std <- backsolve(chol(var(samp)),
                    cbind(0, t(samp)) - colMeans(samp),
                    transpose = TRUE)
   sqdist <- colSums(std * std)
   sum(sqdist[-1] > sqdist[1])/nrow(samp)
}



From HDoran at air.org  Fri Aug  8 20:27:34 2008
From: HDoran at air.org (Doran, Harold)
Date: Fri, 8 Aug 2008 14:27:34 -0400
Subject: [R-sig-ME] [R] [lme4]Coef output with binomial lmer
In-Reply-To: <115B84D418254A4B9CC86922F325C937757F88@HERMES1.ds.leeds.ac.uk>
Message-ID: <ED7B522EE00C9A4FA515AA71724D61EEB86D5A@DC1EXCL01.air.org>

The extractor function for the fixed effects is fixef(), not coef(). Out
of curiosity, why are you using (1|ass%in%pop%in%fam)? This notation is
non-standard and does not define the nesting structure of the data.

I think you want (1|ass/pop/fam)

> -----Original Message-----
> From: r-help-bounces at r-project.org 
> [mailto:r-help-bounces at r-project.org] On Behalf Of Tom Cameron
> Sent: Friday, August 08, 2008 11:47 AM
> To: r-help at r-project.org; r-sig-mixed-models at r-project.org
> Subject: [R] [lme4]Coef output with binomial lmer
> 
> Dear R users  
>  
> I have built the following model
>  
> m1<-lmer(y~harn+foodn+(1|ass%in%pop%in%fam),family = "quasibinomial")
>  
> where y<-cbind(alive,dead)
>  
> where harn and foodn are categorical factors and the random 
> effect is a nested term to represent experimental structure 
> e.g.  Day/Block/Replicate ass= 5 level factor, pop= 2 
> populations per treatment factor in each assay, 7 reps per population
>  
> The model can be family = quasibinomial or binomial
>  
> My complete lack of understanding is in retrieving the 
> coefficients for the fixed effects to back-transform the 
> effects of my factors on proportional survival
>  
> I get the following output:
> > coef(m1)
> $`ass %in% pop %in% fam`
>       (Intercept)      harn1     harn2   foodn2
> FALSE   1.0322375 -0.1939521 0.0310434 0.810084
> TRUE    0.5997679 -0.1939521 0.0310434 0.810084
>  
> Where FALSE and TRUE refer to some attribute of the random effect 
>  
> My hunch is that it refers to the Coefficients with (=TRUE) 
> and without
> (=FALSE) the random effects?
>  
> Any help appreciated
>  
> 
> ..............................................................
> ..........
> ............
> Dr Tom C Cameron
> Genetics, Ecology and Evolution
> IICB, University of Leeds
> Leeds, UK
> Office: +44 (0)113 343 2837
> Lab:    +44 (0)113 343 2854
> Fax:    +44 (0)113 343 2835
> 
> 
> Email: t.c.cameron at leeds.ac.uk
> Webpage: click here
> <http://www.fbs.leeds.ac.uk/staff/profile.php?tag=Cameron_TC> 
> 
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



From T.C.Cameron at leeds.ac.uk  Fri Aug  8 17:47:12 2008
From: T.C.Cameron at leeds.ac.uk (Tom Cameron)
Date: Fri, 8 Aug 2008 16:47:12 +0100
Subject: [R-sig-ME] [lme4]Coef output with binomial lmer
Message-ID: <115B84D418254A4B9CC86922F325C937757F88@HERMES1.ds.leeds.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080808/e1c408f8/attachment.pl>

From T.C.Cameron at leeds.ac.uk  Fri Aug  8 23:01:26 2008
From: T.C.Cameron at leeds.ac.uk (Tom Cameron)
Date: Fri, 8 Aug 2008 22:01:26 +0100
Subject: [R-sig-ME] [R] [lme4]Coef output with binomial lmer ~ nested
	random effects
References: <ED7B522EE00C9A4FA515AA71724D61EEB86D5A@DC1EXCL01.air.org>
Message-ID: <115B84D418254A4B9CC86922F325C937757F8C@HERMES1.ds.leeds.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080808/822e24b0/attachment.pl>

From danielezrajohnson at gmail.com  Sat Aug  9 12:54:18 2008
From: danielezrajohnson at gmail.com (Daniel Ezra Johnson)
Date: Sat, 9 Aug 2008 11:54:18 +0100
Subject: [R-sig-ME] chi-square mixtures for random effects LRTs
Message-ID: <a46630750808090354s3defe632s580fc09c61f87b@mail.gmail.com>

Pinheiro & Bates (2000:86-87) discuss, but in the end do not
recommend, using mixed chi-squared distributions for random effects
likelihood ratio tests.
Their recommendation is to use the conservative 'naive' df, equal to
the difference in the number of non-redundant parameters in the model.
They discuss two examples (updated to lmer notation below):

Example 1:
fm1Machine <- lmer(score~Machine+(1|Worker),data=Machines)
fm2Machine <- lmer(score~Machine+(1|Worker/Machine),data=Machines)

Example 2:
fm1OrthF <- lmer(distance~age+(1|Subject),data=Orthodont[Orthodont$Sex=="Female",])
fm2OrthF <- lmer(distance~age+(age|Subject),data=Orthodont[Orthodont$Sex=="Female",])

In Example 1, the difference between the models is the addition of a
(nested) random intercept, therefore the number of parameters
increases by one.
In Example 2, the difference is a random slope, which also generates a
correlation parameter, so the number of parameters increases by two.

Therefore, P&B recommend using a conservative df of 1 for the test in
Example 1, and df of 2 in Example 2.

My first question is, if the difference in your model was the addition
of a crossed random effect intercept, not discussed in P&B:

Example 3:
imaginary1 <- lmer(score~sex+(1|subject))
imaginary2 <- lmer(score~sex+(1|subject)+(1|item))

No correlation term would be generated here, so would this pattern
just like Example 1? That is, would df=1 be the naive and conservative
choice?

A second question: if one did wish to employ Stram and Lee's
correction using a mixed chi-squared distribution -- between the df
given above and one less degree of freedom, e.g. Mix(0,1) in Examples
1 (and 3?), and Mix(1,2) in Example 2 -- how would this be done?

P&B implement it somewhere in plot(simulate.lme()) but I cannot find
the code for it. Is it as simple as:

mean(pchisq(LRTS,df=c(0,1),lower.tail=F))   # Example 1 (and 3?) aka
pchisq(LRTS,df=1,lower.tail=F)/2 in this special case
mean(pchisq(LRTS,df=c(1,2),lower.tail=F))   # Example 2

If this is correct, it would seem reasonably easy to use Stram & Lee's
df correction as long as the models being compared differ minimally,
as in these examples.
Figure 2.4 in P&B shows that the correction still leaves a
conservative result in the ML case for Example 1, but it still looks
better than the 'naive' df.
So I'm a bit puzzled why P&B don't in the end recommend the
chi-squared mixture adjustment.

Thanks a lot,
Daniel



From bates at stat.wisc.edu  Sun Aug 10 10:20:27 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 10 Aug 2008 10:20:27 +0200
Subject: [R-sig-ME] chi-square mixtures for random effects LRTs
In-Reply-To: <a46630750808090354s3defe632s580fc09c61f87b@mail.gmail.com>
References: <a46630750808090354s3defe632s580fc09c61f87b@mail.gmail.com>
Message-ID: <40e66e0b0808100120w399186te0f6ec8a697a4fa8@mail.gmail.com>

On Sat, Aug 9, 2008 at 12:54 PM, Daniel Ezra Johnson
<danielezrajohnson at gmail.com> wrote:
> Pinheiro & Bates (2000:86-87) discuss, but in the end do not
> recommend, using mixed chi-squared distributions for random effects
> likelihood ratio tests.
> Their recommendation is to use the conservative 'naive' df, equal to
> the difference in the number of non-redundant parameters in the model.
> They discuss two examples (updated to lmer notation below):

> Example 1:
> fm1Machine <- lmer(score~Machine+(1|Worker),data=Machines)
> fm2Machine <- lmer(score~Machine+(1|Worker/Machine),data=Machines)

> Example 2:
> fm1OrthF <- lmer(distance~age+(1|Subject),data=Orthodont[Orthodont$Sex=="Female",])
> fm2OrthF <- lmer(distance~age+(age|Subject),data=Orthodont[Orthodont$Sex=="Female",])

> In Example 1, the difference between the models is the addition of a
> (nested) random intercept, therefore the number of parameters
> increases by one.
> In Example 2, the difference is a random slope, which also generates a
> correlation parameter, so the number of parameters increases by two.

> Therefore, P&B recommend using a conservative df of 1 for the test in
> Example 1, and df of 2 in Example 2.

> My first question is, if the difference in your model was the addition
> of a crossed random effect intercept, not discussed in P&B:

> Example 3:
> imaginary1 <- lmer(score~sex+(1|subject))
> imaginary2 <- lmer(score~sex+(1|subject)+(1|item))

> No correlation term would be generated here, so would this pattern
> just like Example 1? That is, would df=1 be the naive and conservative
> choice?

Yes.

> A second question: if one did wish to employ Stram and Lee's
> correction using a mixed chi-squared distribution -- between the df
> given above and one less degree of freedom, e.g. Mix(0,1) in Examples
> 1 (and 3?), and Mix(1,2) in Example 2 -- how would this be done?

Parameters for which 0 is not the boundary value, e.g. correlations,
count for 1 degree of freedom.  Parameters for which 0 is the boundary
value, e.g. standard deviations or variances, contribute a mixture of
0 and 1 degree of freedom.  In example 3 the degrees of freedom for
reference distribution would be Mix(0,1) as you suspect.  That is
relatively easy to implement in that the effective p-value is 1/2 the
p-value that is calculated for 1 degree of freedom.  In general if you
have a mixture that is 50% chi-squared J and 50% chi-squared K then
you calculate the p-value for J and the p-value for K and average
them.  It happens that the p-value for a chi-squared with 0 degrees of
freedom is 0 for any positive likelihood ratio.
> P&B implement it somewhere in plot(simulate.lme()) but I cannot find
> the code for it. Is it as simple as:
>
> mean(pchisq(LRTS,df=c(0,1),lower.tail=F))   # Example 1 (and 3?) aka
> pchisq(LRTS,df=1,lower.tail=F)/2 in this special case
> mean(pchisq(LRTS,df=c(1,2),lower.tail=F))   # Example 2
>
> If this is correct, it would seem reasonably easy to use Stram & Lee's
> df correction as long as the models being compared differ minimally,
> as in these examples.
> Figure 2.4 in P&B shows that the correction still leaves a
> conservative result in the ML case for Example 1, but it still looks
> better than the 'naive' df.
> So I'm a bit puzzled why P&B don't in the end recommend the
> chi-squared mixture adjustment.
>
> Thanks a lot,
> Daniel
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From HDoran at air.org  Sun Aug 10 17:19:14 2008
From: HDoran at air.org (Doran, Harold)
Date: Sun, 10 Aug 2008 11:19:14 -0400
Subject: [R-sig-ME] [R] [lme4]Coef output with binomial lmer
In-Reply-To: <18904468.post@talk.nabble.com>
Message-ID: <ED7B522EE00C9A4FA515AA71724D61EEB86D9A@DC1EXCL01.air.org>

I don't think the %in% works at all. It's non-standard and I think an
incorrect model specification. Here is an example where we can see that
the transpose of the model matrix for the random effects is different
when we compare what would be the same model if it "worked". 

I don't think there is an extractor function so I pull out the slot
directly. There is some "danger" in this because extractor functions
should be used rather than directly accessing the slots, but I think it
works OK.

library(lme4)
data(egsingle, package='mlmRev')

fm1 <- lmer(math ~ 1 + (1|schoolid/childid), egsingle, control =
list(grad = 0, nit = 0, msV = 1))
fm2 <- lmer(math ~ 1 + (1|childid%in%schoolid), egsingle, control =
list(grad = 0, nit = 0, msV = 1))

z1 <- fm1 at Zt
z2 <- fm2 at Zt
identical(z1,z2)
[1] FALSE

fm1 is the correct specification as it properly denotes that children
are nested in schools. When we compare the model matrices, we can see
that they are not identical; hence the model specification is not the
same. I have no clue what %in% does and am not sure it even works with
lmer, but could be wrong. 

Note in this case

fm3 <- lmer(math ~ 1 + (1|schoolid) + (1|childid), egsingle, control =
list(grad = 0, nit = 0, msV = 1))
z3 <- fm3 at Zt
identical(z1,z3)
[1] TRUE

Is exactly the same as fm1 because the IDs are always nested.

Harold

> -----Original Message-----
> From: r-help-bounces at r-project.org 
> [mailto:r-help-bounces at r-project.org] On Behalf Of Mark Difford
> Sent: Saturday, August 09, 2008 8:32 AM
> To: r-help at r-project.org
> Subject: Re: [R] [lme4]Coef output with binomial lmer
> 
> 
> Hi Tom,
> 
> >> 1|ass%in%pop%in%fam
> 
> This is "non-standard," but as you have found, it works. The 
> correct translation is in fact
> 
> 1|fam/pop/ass
> 
> and not 1|ass/pop/fam as suggested by Harold Doran. Dropping 
> %, ass%in%pop%in%fam reads [means] as: nest ass in pop [= 
> pop/ass], and then nest this in fam == fam/pop/ass
> 
> HTH, Mark.
> 
> 
> T.C. Cameron wrote:
> > 
> > Dear R users
> >  
> > I have built the following model
> >  
> > m1<-lmer(y~harn+foodn+(1|ass%in%pop%in%fam),family = 
> "quasibinomial")
> >  
> > where y<-cbind(alive,dead)
> >  
> > where harn and foodn are categorical factors and the random 
> effect is 
> > a nested term to represent experimental structure e.g.  
> > Day/Block/Replicate ass= 5 level factor, pop= 2 populations per 
> > treatment factor in each assay, 7 reps per population
> >  
> > The model can be family = quasibinomial or binomial
> >  
> > My complete lack of understanding is in retrieving the coefficients 
> > for the fixed effects to back-transform the effects of my 
> factors on 
> > proportional survival
> >  
> > I get the following output:
> >> coef(m1)
> > $`ass %in% pop %in% fam`
> >       (Intercept)      harn1     harn2   foodn2
> > FALSE   1.0322375 -0.1939521 0.0310434 0.810084
> > TRUE    0.5997679 -0.1939521 0.0310434 0.810084
> >  
> > Where FALSE and TRUE refer to some attribute of the random effect
> >  
> > My hunch is that it refers to the Coefficients with (=TRUE) and 
> > without
> > (=FALSE) the random effects?
> >  
> > Any help appreciated
> >  
> > 
> > 
> ..............................................................
> ..........
> > ............
> > Dr Tom C Cameron
> > Genetics, Ecology and Evolution
> > IICB, University of Leeds
> > Leeds, UK
> > Office: +44 (0)113 343 2837
> > Lab:    +44 (0)113 343 2854
> > Fax:    +44 (0)113 343 2835
> > 
> > 
> > Email: t.c.cameron at leeds.ac.uk
> > Webpage: click here
> > <http://www.fbs.leeds.ac.uk/staff/profile.php?tag=Cameron_TC>
> > 
> >  
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> > 
> 
> --
> View this message in context: 
> http://www.nabble.com/-lme4-Coef-output-with-binomial-lmer-tp1
> 8894407p18904468.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



From bates at stat.wisc.edu  Sun Aug 10 21:16:24 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 10 Aug 2008 21:16:24 +0200
Subject: [R-sig-ME] HPDinterval, and mcmcpvalue from R-Wiki discussion
In-Reply-To: <C4C20991.6DAF%dafshartous@med.miami.edu>
References: <C4C20991.6DAF%dafshartous@med.miami.edu>
Message-ID: <40e66e0b0808101216x39c5db82i38f0248d9c9b82b1@mail.gmail.com>

The short answer (and it has to be short as I am typing this in a
hotel lobby in Dortmund) is that I have changed the format of the
value of the mcmcsamp function and not correspondingly changed the
function to calculate p-values.


On Fri, Aug 8, 2008 at 8:32 PM, David Afshartous
<dafshartous at med.miami.edu> wrote:
>
> All,
>
> I am trying to apply HPDinterval and mcmcpvalue as shown in the discussion
> at R-Wiki
> ((http://wiki.r-project.org/rwiki/doku.php?id=guides:lmer-tests&s=lme%20and%
> 20aov).
>
> Three questions:
> 1) why am I getting the error message for HPDinterval given that I'm
> following the exact steps in the discussion?
> 2) is it normal that my p-value (p=.72) differs that much from the one in
> the discussion (p=.46)?
> 3) why isn't mcmcpvalue written to work on a single column? (see below)
>
> Thanks,
> David
>
>
>
>> sessionInfo()
> R version 2.7.1 (2008-06-23)
> i386-apple-darwin8.10.1
>
> locale:
> en_US.UTF-8/en_US.UTF-8/C/C/en_US.UTF-8/en_US.UTF-8
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] SASmixed_0.4-4     coda_0.13-2        Hmisc_3.4-3        nlme_3.1-89
> lme4_0.999375-24   Matrix_0.999375-11
> [7] lattice_0.17-8
>
> loaded via a namespace (and not attached):
> [1] cluster_1.11.11 grid_2.7.1      tools_2.7.1
>  data(AvgDailyGain, package = "SASmixed")
>  (fm1Adg <- lmer(adg ~ InitWt*Treatment - 1 + (1|Block), AvgDailyGain))
>
>> AdgS1 <- mcmcsamp(fm1Adg, 50000)
>> library(coda)
>> HPDinterval(AdgS1)
> Error in UseMethod("HPDinterval") :
>  no applicable method for "HPDinterval"
>
>> mcmcpvalue(as.matrix(AdgS1)[, 6:8])
> [1] 0.7231
>> mcmcpvalue(as.matrix(AdgS1)[, 1])
> Error in base::colMeans(x, na.rm = na.rm, dims = dims) :
>  'x' must be an array of at least two dimensions
>>
>
>
>
> mcmcpvalue <- function(samp)
> {
>   ## elementary version that creates an empirical p-value for the
>   ## hypothesis that the columns of samp have mean zero versus a
>   ## general multivariate distribution with elliptical contours.
>
>   ## differences from the mean standardized by the observed
>   ## variance-covariance factor
>   std <- backsolve(chol(var(samp)),
>                    cbind(0, t(samp)) - colMeans(samp),
>                    transpose = TRUE)
>   sqdist <- colSums(std * std)
>   sum(sqdist[-1] > sqdist[1])/nrow(samp)
> }
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From brandon at brandoninvergo.com  Tue Aug 12 00:03:34 2008
From: brandon at brandoninvergo.com (Brandon Invergo)
Date: Tue, 12 Aug 2008 00:03:34 +0200
Subject: [R-sig-ME] Simple lme/lmer random effects questions
Message-ID: <48A0B736.8030609@brandoninvergo.com>

Hello,
(I already sent this to the R-help list before I was informed that 
there's a SIG for mixed effects models...apologies if you receive this 
twice)

I have two very rudimentary questions regarding the random effects terms 
in the lme and lmer functions. I apologize if this also partly strays 
into a general statistics question, but I'm a bit new to this all. So 
hopefully it'll be a quick problem to sort out...

Here is my experimental setup: I raised butterflies in 5 different 
testing chambers all set to different temperatures. Within the testing 
chambers, the butterflies were held in 10 different sleeves, which were 
rotated daily to compensate for microenvironmental effects. I measured 
several traits of the butterflies and I am constructing models for each 
trait (unfortunately, multivariate analysis isn't possible). In my 
models, sex and temperature are fixed factors and the sleeve is a random 
effect. Most of the response variables are normally distributed, but 
there is one with a Gamma distribution (time until an event) and another 
with poisson distribution (counts), so some models use lme while others 
use lmer. I would like to determine if, despite the daily rotation, 
there are still random effects from the individual sleeves. My two 
questions (assuming I haven't already made grave errors in my 
description of the setup) are:

1) In my data file, the "sleeve" variable is just marked with a number 1 
through 10; the temperature is noted in a different column, so the 50 
sleeves do not have unique names, but rather there are 5 instances of 
each of the 10 sleeve numbers. If sleeve is to be properly included in 
the models as a random effect, is it sufficient to leave the values as 
they are or should I generate unique names for all combinations of 
sleeve number and temperature, using something like
 > data$sleeve.in.temp <- factor(with(data, temp:sleeve)[drop=TRUE])

2) (this is the one that strays more into standard statistics territory, 
sorry) I'm a bit confused on how to actually set up the random effects 
term for the models. Given my experimental setup, using the lme syntax, 
should it be:
 > model <- lme(response ~ sex*temp, random=~temp|sleeve, data)
or
 > model <- lme(response ~ sex*temp, random=~1|sleeve, data)
or something else? I've searched and searched, but everything I find 
online seems to be significantly more advanced than what I'm doing, 
leaving me even more confused than when I started!


Thank you very much for your help!! I want to be sure I do this analysis 
right....
Cheers,
-brandon



From A.Robinson at ms.unimelb.edu.au  Tue Aug 12 00:46:33 2008
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Tue, 12 Aug 2008 08:46:33 +1000
Subject: [R-sig-ME] Simple lme/lmer random effects questions
In-Reply-To: <48A0B736.8030609@brandoninvergo.com>
References: <48A0B736.8030609@brandoninvergo.com>
Message-ID: <20080811224633.GB1337@ms.unimelb.edu.au>

Hi Brandon,

On Tue, Aug 12, 2008 at 12:03:34AM +0200, Brandon Invergo wrote:
> Hello,
> (I already sent this to the R-help list before I was informed that 
> there's a SIG for mixed effects models...apologies if you receive this 
> twice)
> 
> I have two very rudimentary questions regarding the random effects terms 
> in the lme and lmer functions. I apologize if this also partly strays 
> into a general statistics question, but I'm a bit new to this all. So 
> hopefully it'll be a quick problem to sort out...
> 
> Here is my experimental setup: I raised butterflies in 5 different 
> testing chambers all set to different temperatures. Within the testing 
> chambers, the butterflies were held in 10 different sleeves, which were 
> rotated daily to compensate for microenvironmental effects. I measured 
> several traits of the butterflies and I am constructing models for each 
> trait (unfortunately, multivariate analysis isn't possible). In my 
> models, sex and temperature are fixed factors and the sleeve is a random 
> effect. Most of the response variables are normally distributed, but 
> there is one with a Gamma distribution (time until an event) and another 
> with poisson distribution (counts), so some models use lme while others 
> use lmer. I would like to determine if, despite the daily rotation, 
> there are still random effects from the individual sleeves. My two 
> questions (assuming I haven't already made grave errors in my 
> description of the setup) are:

First, let me compliment you on the quality of the description of your
setup.  Nice work!
 
> 1) In my data file, the "sleeve" variable is just marked with a number 1 
> through 10; the temperature is noted in a different column, so the 50 
> sleeves do not have unique names, but rather there are 5 instances of 
> each of the 10 sleeve numbers. If sleeve is to be properly included in 
> the models as a random effect, is it sufficient to leave the values as 
> they are or should I generate unique names for all combinations of 
> sleeve number and temperature, using something like
> > data$sleeve.in.temp <- factor(with(data, temp:sleeve)[drop=TRUE])

You should generate unique names for all combinations.

> 2) (this is the one that strays more into standard statistics territory, 
> sorry) I'm a bit confused on how to actually set up the random effects 
> term for the models. Given my experimental setup, using the lme syntax, 
> should it be:
> > model <- lme(response ~ sex*temp, random=~temp|sleeve, data)
> or
> > model <- lme(response ~ sex*temp, random=~1|sleeve, data)
> or something else? I've searched and searched, but everything I find 
> online seems to be significantly more advanced than what I'm doing, 
> leaving me even more confused than when I started!

Based on your earlier description, I think that it should be 

model <- lme(response ~ sex*temp, random=~1|temp/sleeve.in.temp, data)

but the following would also work just fine:

model <- lme(response ~ sex*temp, random=~1|temp/sleeve, data)

For the lmer syntax, I think that you should use

model <- lmer(response ~ sex*temp + (1|temp) + (1|sleeve.in.temp), ...

I strongly recommend Pinheiro and Bates as top reading for getting the
analysis right :)

Good luck!

Andrew

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-6410
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/



From HStevens at muohio.edu  Tue Aug 12 11:58:17 2008
From: HStevens at muohio.edu (Martin Henry H. Stevens)
Date: Tue, 12 Aug 2008 05:58:17 -0400
Subject: [R-sig-ME] Simple lme/lmer random effects questions
In-Reply-To: <48A0B736.8030609@brandoninvergo.com>
References: <48A0B736.8030609@brandoninvergo.com>
Message-ID: <041355D2-9868-41EE-8F50-2E997FDFE9F9@muohio.edu>

Hi Brandon,
Yet another perspective.
On Aug 11, 2008, at 6:03 PM, Brandon Invergo wrote:
> Hello,
> (I already sent this to the R-help list before I was informed that
> there's a SIG for mixed effects models...apologies if you receive this
> twice)
>
> I have two very rudimentary questions regarding the random effects  
> terms
> in the lme and lmer functions. I apologize if this also partly strays
> into a general statistics question, but I'm a bit new to this all. So
> hopefully it'll be a quick problem to sort out...
>
> Here is my experimental setup: I raised butterflies in 5 different
> testing chambers all set to different temperatures. Within the testing
> chambers, the butterflies were held in 10 different sleeves, which  
> were
> rotated daily to compensate for microenvironmental effects. I measured
> several traits of the butterflies and I am constructing models for  
> each
> trait (unfortunately, multivariate analysis isn't possible). In my
> models, sex and temperature are fixed factors and the sleeve is a  
> random
> effect. Most of the response variables are normally distributed, but
> there is one with a Gamma distribution (time until an event) and  
> another
> with poisson distribution (counts), so some models use lme while  
> others
> use lmer. I would like to determine if, despite the daily rotation,
> there are still random effects from the individual sleeves. My two
> questions (assuming I haven't already made grave errors in my
> description of the setup) are:
>
> 1) In my data file, the "sleeve" variable is just marked with a  
> number 1
> through 10; the temperature is noted in a different column, so the 50
> sleeves do not have unique names, but rather there are 5 instances of
> each of the 10 sleeve numbers. If sleeve is to be properly included in
> the models as a random effect, is it sufficient to leave the values as
> they are or should I generate unique names for all combinations of
> sleeve number and temperature, using something like
>> data$sleeve.in.temp <- factor(with(data, temp:sleeve)[drop=TRUE])
>
> 2) (this is the one that strays more into standard statistics  
> territory,
> sorry) I'm a bit confused on how to actually set up the random effects
> term for the models. Given my experimental setup, using the lme  
> syntax,
> should it be:
>> model <- lme(response ~ sex*temp, random=~temp|sleeve, data)
> or
>> model <- lme(response ~ sex*temp, random=~1|sleeve, data)
> or something else? I've searched and searched, but everything I find
> online seems to be significantly more advanced than what I'm doing,
> leaving me even more confused than when I started!
### Assuming temp is continuous...
>
m1 <- lmer(response ~ sex*temp + (1|sleeve.in.temp) + (temp-1| 
sleeve.in.temp), data) # enforcing independence of slope and intercept
m2 <- lmer(response ~ sex*temp + (temp|sleeve.in.temp), data) #  
enforcing dependence of slope and intercept
m3 <- lmer(response ~ sex*temp + (1|sleeve.in.temp), data) # No  
difference in slopes; merely difference in intercept
anova(m1,m2,m3)
###
### Assuming temp is categorical...
m1 <- lmer(response ~ sex*temp + (1|sleeve.in.temp) + (1|temp), data)
m2 <- lmer(response ~ sex*temp + (1|sleeve.in.temp), data) #  
enforcing dependence of slope and intercept
m3 <- lmer(response ~ sex*temp + (1|temp), data) # No difference in  
slopes; merely difference in intercept
anova(m1,m2,m3)

>
> Thank you very much for your help!! I want to be sure I do this  
> analysis
> right....
> Cheers,
> -brandon
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

Dr. Hank Stevens, Associate Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.users.muohio.edu/harkesae/
http://www.cas.muohio.edu/ecology
http://www.muohio.edu/botany/
"E Pluribus Unum"

"I love deadlines. I love the whooshing noise they make as they go by."
                                             (Douglas Adams)


If you send an attachment, please try to send it in a format anyone  
can read, such as PDF, text, Open Document Format, HTML, or RTF.
Why? See:  http://www.gnu.org/philosophy/no-word-attachments.html



From mark_difford at yahoo.co.uk  Tue Aug 12 22:39:23 2008
From: mark_difford at yahoo.co.uk (Mark Difford)
Date: Tue, 12 Aug 2008 20:39:23 +0000 (GMT)
Subject: [R-sig-ME] Chapter12?
Message-ID: <430370.23458.qm@web27308.mail.ukl.yahoo.com>

In fact, though Chapter 12 still exists in the?TOC of the document, the relevant chapter (Statistical Models in R)?has mysteriously become chapter 11 (it was a long time ago that I last looked at it). The relevant section is ?11.1 Defining statistical models; formulae.
http://cran.r-project.org/doc/manuals/R-intro.html
Regards, Mark.
?Mark Difford
Ph.D. candidate, Botany Department,
Nelson Mandela Metropolitan University,
Port Elizabeth, SA.

Send instant messages to your online friends http://uk.messenger.yahoo.com



From bbh at umich.edu  Wed Aug 13 00:06:39 2008
From: bbh at umich.edu (Hansen, Ben)
Date: Tue, 12 Aug 2008 18:06:39 -0400
Subject: [R-sig-ME] segfaults from lmer on a 64-bit system
Message-ID: <E4F6F36F6B0C7B409966C9B724245814034EB5D1@ECLUST2-VS4.adsroot.itcs.umich.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080812/84d89db5/attachment.pl>

From mark_difford at yahoo.co.uk  Tue Aug 12 22:23:45 2008
From: mark_difford at yahoo.co.uk (Mark Difford)
Date: Tue, 12 Aug 2008 20:23:45 +0000 (GMT)
Subject: [R-sig-ME] [R] [lme4]Coef output with binomial lmer
Message-ID: <241237.41481.qm@web27303.mail.ukl.yahoo.com>

Hi Harold,
I wasn't suggesting to the poster (or to you)?that the model should be specified, in either lme or lmer, using the % construct. What I was doing was correcting an obvious (but not, I think, to the poster of the question) oversight in your posting, which?gave the translation of his syntax?(1|ass%in%pop%in%fam) as 1|ass/pop/fam. This is incorrect; correct is?1|fam/pop/ass.
BTW: specifying a nested model as ass %in% pop %in% fam is in fact correct formula syntax for a nested model in R (though I am not suggesting, and never have, that it be used to specify random effects structures in lme() [or in lmer(), where the symbolism is different]). If you doubt this, then set your mind at rest by please reading the relevant parts of Chapter 12 of An Introduction to R.
Regards, Mark.
?Mark Difford
Ph.D. candidate, Botany Department,
Nelson Mandela Metropolitan University,
Port Elizabeth, SA.



----- Original Message ----
From: "Doran, Harold" <HDoran at air.org>
To: Mark Difford <mark_difford at yahoo.co.uk>; r-help at r-project.org
Cc: r-sig-mixed-models at r-project.org
Sent: Sunday, 10 August, 2008 5:19:14 PM
Subject: RE: [R] [lme4]Coef output with binomial lmer

I don't think the %in% works at all. It's non-standard and I think an
incorrect model specification. Here is an example where we can see that
the transpose of the model matrix for the random effects is different
when we compare what would be the same model if it "worked". 

I don't think there is an extractor function so I pull out the slot
directly. There is some "danger" in this because extractor functions
should be used rather than directly accessing the slots, but I think it
works OK.

library(lme4)
data(egsingle, package='mlmRev')

fm1 <- lmer(math ~ 1 + (1|schoolid/childid), egsingle, control =
list(grad = 0, nit = 0, msV = 1))
fm2 <- lmer(math ~ 1 + (1|childid%in%schoolid), egsingle, control =
list(grad = 0, nit = 0, msV = 1))

z1 <- fm1 at Zt
z2 <- fm2 at Zt
identical(z1,z2)
[1] FALSE

fm1 is the correct specification as it properly denotes that children
are nested in schools. When we compare the model matrices, we can see
that they are not identical; hence the model specification is not the
same. I have no clue what %in% does and am not sure it even works with
lmer, but could be wrong. 

Note in this case

fm3 <- lmer(math ~ 1 + (1|schoolid) + (1|childid), egsingle, control =
list(grad = 0, nit = 0, msV = 1))
z3 <- fm3 at Zt
identical(z1,z3)
[1] TRUE

Is exactly the same as fm1 because the IDs are always nested.

Harold

> -----Original Message-----
> From: r-help-bounces at r-project.org 
> [mailto:r-help-bounces at r-project.org] On Behalf Of Mark Difford
> Sent: Saturday, August 09, 2008 8:32 AM
> To: r-help at r-project.org
> Subject: Re: [R] [lme4]Coef output with binomial lmer
> 
> 
> Hi Tom,
> 
> >> 1|ass%in%pop%in%fam
> 
> This is "non-standard," but as you have found, it works. The 
> correct translation is in fact
> 
> 1|fam/pop/ass
> 
> and not 1|ass/pop/fam as suggested by Harold Doran. Dropping 
> %, ass%in%pop%in%fam reads [means] as: nest ass in pop [= 
> pop/ass], and then nest this in fam == fam/pop/ass
> 
> HTH, Mark.
> 
> 
> T.C. Cameron wrote:
> > 
> > Dear R users
> >? 
> > I have built the following model
> >? 
> > m1<-lmer(y~harn+foodn+(1|ass%in%pop%in%fam),family = 
> "quasibinomial")
> >? 
> > where y<-cbind(alive,dead)
> >? 
> > where harn and foodn are categorical factors and the random 
> effect is 
> > a nested term to represent experimental structure e.g.? 
> > Day/Block/Replicate ass= 5 level factor, pop= 2 populations per 
> > treatment factor in each assay, 7 reps per population
> >? 
> > The model can be family = quasibinomial or binomial
> >? 
> > My complete lack of understanding is in retrieving the coefficients 
> > for the fixed effects to back-transform the effects of my 
> factors on 
> > proportional survival
> >? 
> > I get the following output:
> >> coef(m1)
> > $`ass %in% pop %in% fam`
> >? ? ? (Intercept)? ? ? harn1? ? harn2? foodn2
> > FALSE? 1.0322375 -0.1939521 0.0310434 0.810084
> > TRUE? ? 0.5997679 -0.1939521 0.0310434 0.810084
> >? 
> > Where FALSE and TRUE refer to some attribute of the random effect
> >? 
> > My hunch is that it refers to the Coefficients with (=TRUE) and 
> > without
> > (=FALSE) the random effects?
> >? 
> > Any help appreciated
> >? 
> > 
> > 
> ..............................................................
> ..........
> > ............
> > Dr Tom C Cameron
> > Genetics, Ecology and Evolution
> > IICB, University of Leeds
> > Leeds, UK
> > Office: +44 (0)113 343 2837
> > Lab:? ? +44 (0)113 343 2854
> > Fax:? ? +44 (0)113 343 2835
> > 
> > 
> > Email: t.c.cameron at leeds.ac.uk
> > Webpage: click here
> > <http://www.fbs.leeds.ac.uk/staff/profile.php?tag=Cameron_TC>
> > 
> >? 
> > 
> > ??? [[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> > 
> 
> --
> View this message in context: 
> http://www.nabble.com/-lme4-Coef-output-with-binomial-lmer-tp1
> 8894407p18904468.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Send instant messages to your online friends http://uk.messenger.yahoo.com



From bates at stat.wisc.edu  Wed Aug 13 08:55:25 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 13 Aug 2008 08:55:25 +0200
Subject: [R-sig-ME] segfaults from lmer on a 64-bit system
In-Reply-To: <E4F6F36F6B0C7B409966C9B724245814034EB5D1@ECLUST2-VS4.adsroot.itcs.umich.edu>
References: <E4F6F36F6B0C7B409966C9B724245814034EB5D1@ECLUST2-VS4.adsroot.itcs.umich.edu>
Message-ID: <40e66e0b0808122355n1b333cf7heb016eac2ebea2@mail.gmail.com>

I would first try later versions of both the Matrix and lme4 packages.
 Versions currently on CRAN are Matrix_0.999375-11 and
lme4_0.999375-24

If you continue to get segfaults then having a reproducible example,
along with the data - anonymized if you wish, will be important.

You can determine the versions of the packages that you are using with

sessionInfo()


On Wed, Aug 13, 2008 at 12:06 AM, Hansen, Ben <bbh at umich.edu> wrote:
> Hello all,
>
>
>
> I am encountering trouble with lme4 on a 64-bit linux system, and I'm
> wondering if any of you might have advice on how to circumvent it. Using
> recent versions of lme4 and R, I was able to compile and install the
> package, and use it with success on relatively small problems.  However,
> when I tried to scale up to work on a big problem, applying lmer() with
> thousands of observations and a hundred or so random effects, my R fails
> reporting a segmentation fault.  I don't see the same problem on a
> different 32-bit Mac OSX system, which runs the same code without
> evident difficulty.
>
>
>
> Some details of the setup where the problem occurs.  We've tried a few
> combinations of things, so I write what we tried first, with variants
> noted in parentheses:
>
>
>
> Red Hat Enterprise Linux 4 (version 5)
>
> R version 2.7.1 (2.6.1)
>
> gcc/g++ compilers for C and C++, Portland Group Fortran compilers
> (gcc/g++/gfortran, ie full GNU compiler suite)
>
> Matrix package version 0.999375-10  (installed with and without
> --with-package-versions flag)
>
> lme4 version 0.999375-16 (installed with and without
> --with-package-versions flag)
>
>
>
> Any suggestions?  I'd be very appreciative.
>
>
>
> Ben
>
>
>
>
>
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From p.dalgaard at biostat.ku.dk  Wed Aug 13 09:45:45 2008
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 13 Aug 2008 09:45:45 +0200
Subject: [R-sig-ME] Chapter12?
In-Reply-To: <430370.23458.qm@web27308.mail.ukl.yahoo.com>
References: <430370.23458.qm@web27308.mail.ukl.yahoo.com>
Message-ID: <48A29129.4010906@biostat.ku.dk>

Mark Difford wrote:
> In fact, though Chapter 12 still exists in the TOC of the document, the relevant chapter (Statistical Models in R) has mysteriously become chapter 11 (it was a long time ago that I last looked at it). The relevant section is ?11.1 Defining statistical models; formulae.
> http://cran.r-project.org/doc/manuals/R-intro.html
> Regards, Mark.
>  Mark Difford
> Ph.D. candidate, Botany Department,
> Nelson Mandela Metropolitan University,
> Port Elizabeth, SA.
>   
This look like a generic texinfo issue affecting HTML output. Numbered 
sections are off by one when the first section (here Preface) is 
unnumbered. Same thing in R-exts.html and others.

(CC to Kurt Hornik because he usually knows texinfo things.)

    -p

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark      Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)              FAX: (+45) 35327907



From kubovy at virginia.edu  Wed Aug 13 17:05:34 2008
From: kubovy at virginia.edu (Michael Kubovy)
Date: Wed, 13 Aug 2008 11:05:34 -0400
Subject: [R-sig-ME] gmodels:estimable() fails with nlme:lme()
Message-ID: <8BE219E2-DCBB-4BAD-9C7E-298F0A7B97C3@virginia.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080813/52e46bd5/attachment.pl>

From a.renwick at abdn.ac.uk  Wed Aug 13 17:17:48 2008
From: a.renwick at abdn.ac.uk (Renwick, A. R.)
Date: Wed, 13 Aug 2008 16:17:48 +0100
Subject: [R-sig-ME] Variance explained by random factor
Message-ID: <B9D1301370916C44B5874AF340C18B9B28AB99047F@VMAILB.uoa.abdn.ac.uk>


I am currently trying to run a lmer model with poisson distrubution.  I tested the model with a model without the random effect and it inferred that I should include the random effect:

ma1<-glm(RoundedOverlap~sess+breedfem,family=poisson,data=Male)
mixed<-lmer(RoundedOverlap~sess+breedfem+sess:breedfem+(1|Site),family=poisson,data=Male)

#test to see if sig difference between glm and glmm
as.numeric(2*(logLik(mixed)-logLik(ma)))
#99.16136
pchisq(99.16136,1,lower=FALSE)
#2.327441e-23  so should use a GLMM

However,the model output that I get states that the variance explained by the random factor is 0:


Generalized linear mixed model fit by the Laplace approximation
Formula: RoundedOverlap ~ sess + breedfem + sess:breedfem + (1 | Site)
   Data: Male
   AIC   BIC logLik deviance
 109.9 127.2 -45.93    91.86
Random effects:
 Groups Name        Variance Std.Dev.
 Site   (Intercept)  0        0
Number of obs: 51, groups: Site, 14

I would really appreciate if somebody could help me understand why the variance is 0.
Many thanks,
Anna




Anna Renwick
Zoology Building
School of Biological Sciences
University of Aberdeen
Aberdeen
AB24 2TZ


The University of Aberdeen is a charity registered in Scotland, No SC013683.



From kubovy at virginia.edu  Wed Aug 13 17:25:11 2008
From: kubovy at virginia.edu (Michael Kubovy)
Date: Wed, 13 Aug 2008 11:25:11 -0400
Subject: [R-sig-ME] gmodels:estimable() fails with nlme:lme() QUESTION
	WITHDRAWN
In-Reply-To: <8BE219E2-DCBB-4BAD-9C7E-298F0A7B97C3@virginia.edu>
References: <8BE219E2-DCBB-4BAD-9C7E-298F0A7B97C3@virginia.edu>
Message-ID: <8569F3FF-43E4-44CC-8207-1C4734009DC0@virginia.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080813/d012f289/attachment.pl>

From bbh at umich.edu  Thu Aug 14 05:21:03 2008
From: bbh at umich.edu (Ben Hansen)
Date: Wed, 13 Aug 2008 23:21:03 -0400
Subject: [R-sig-ME] segfaults from lmer on a 64-bit system
In-Reply-To: <40e66e0b0808122355n1b333cf7heb016eac2ebea2@mail.gmail.com>
Message-ID: <C4C91CDF.14A11%bbh@umich.edu>

Thanks for these suggestions.

I updated to the latest versions of the packages, and the segfault persists.

I have isolated an example for reproduction purposes. The data are given in
    http://www.stat.lsa.umich.edu/~bbh/segfault-bh.rda

I get segfaults when I do:
>  library(lme4) ; load("segfault-bh.rda") ; mylme <-lmer(myfmla, data=mydata)

The segfault occurs on a couple of setups:
1.  64-bit Red Hat linux; R version 2.7.1:
> sessionInfo()
R version 2.7.1 (2008-06-23)
x86_64-unknown-linux-gnu

locale:
LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8
;LC_MONETARY=C;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADD
RESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] lme4_0.999375-24   Matrix_0.999375-11 lattice_0.17-8

loaded via a namespace (and not attached):
[1] grid_2.7.1  tools_2.7.1

2. Mac OS; R version 2.7.1:
> sessionInfo()
R version 2.7.1 (2008-06-23)
powerpc-apple-darwin8.10.1

locale:
C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] lme4_0.999375-22   Matrix_0.999375-10 lattice_0.17-8

loaded via a namespace (and not attached):
[1] grid_2.7.1

3.  I also got the segfault on the 64-bit linux machine using R version
2.6.1, lme4 version 0.99875-9 (I think), and Matrix version 0.999375-9 (I
think).

4. *On the other hand,* it sort of works (without segfaulting at least)
using older R, lme4 and Matrix on my Mac:
> sessionInfo()
R version 2.6.2 (2008-02-08)
powerpc-apple-darwin8.10.1

locale:
C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] lme4_0.99875-9    Matrix_0.999375-9 lattice_0.17-6

loaded via a namespace (and not attached):
[1] grid_2.6.2

> mylme <- lmer(myfmla, data=mydata)

Warning messages:
1: In .local(x, ..., value) :
  Estimated variance-covariance for factor 'by.var' is singular

2: In .local(x, ..., value) :
  nlminb returned message false convergence (8)

--Perhaps the warnings contain a clue as to what the problem is?

Thanks for your consideration of this.

Ben
On 8/13/08 2:55 AM, "Douglas Bates" <bates at stat.wisc.edu> wrote:

> I would first try later versions of both the Matrix and lme4 packages.
>  Versions currently on CRAN are Matrix_0.999375-11 and
> lme4_0.999375-24
> 
> If you continue to get segfaults then having a reproducible example,
> along with the data - anonymized if you wish, will be important.
> 
> You can determine the versions of the packages that you are using with
> 
> sessionInfo()
> 
> 
> On Wed, Aug 13, 2008 at 12:06 AM, Hansen, Ben <bbh at umich.edu> wrote:
>> Hello all,
>> 
>> 
>> 
>> I am encountering trouble with lme4 on a 64-bit linux system, and I'm
>> wondering if any of you might have advice on how to circumvent it. Using
>> recent versions of lme4 and R, I was able to compile and install the
>> package, and use it with success on relatively small problems.  However,
>> when I tried to scale up to work on a big problem, applying lmer() with
>> thousands of observations and a hundred or so random effects, my R fails
>> reporting a segmentation fault.  I don't see the same problem on a
>> different 32-bit Mac OSX system, which runs the same code without
>> evident difficulty.
>> 
>> 
>> 
>> Some details of the setup where the problem occurs.  We've tried a few
>> combinations of things, so I write what we tried first, with variants
>> noted in parentheses:
>> 
>> 
>> 
>> Red Hat Enterprise Linux 4 (version 5)
>> 
>> R version 2.7.1 (2.6.1)
>> 
>> gcc/g++ compilers for C and C++, Portland Group Fortran compilers
>> (gcc/g++/gfortran, ie full GNU compiler suite)
>> 
>> Matrix package version 0.999375-10  (installed with and without
>> --with-package-versions flag)
>> 
>> lme4 version 0.999375-16 (installed with and without
>> --with-package-versions flag)
>> 
>> 
>> 
>> Any suggestions?  I'd be very appreciative.
>> 
>> 
>> 
>> Ben
>> 
>> 
>> 
>> 
>> 
>> 
>>        [[alternative HTML version deleted]]
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>



From gregor.gorjanc at bfro.uni-lj.si  Thu Aug 14 09:29:48 2008
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Thu, 14 Aug 2008 07:29:48 +0000 (UTC)
Subject: [R-sig-ME] Variance explained by random factor
References: <B9D1301370916C44B5874AF340C18B9B28AB99047F@VMAILB.uoa.abdn.ac.uk>
Message-ID: <loom.20080814T072650-290@post.gmane.org>

Renwick, A. R. <a.renwick at ...> writes:
...
> However,the model output that I get states that the variance explained by the
random factor is 0:
> 
> Generalized linear mixed model fit by the Laplace approximation
> Formula: RoundedOverlap ~ sess + breedfem + sess:breedfem + (1 | Site)
>    Data: Male
>    AIC   BIC logLik deviance
>  109.9 127.2 -45.93    91.86
> Random effects:
>  Groups Name        Variance Std.Dev.
>  Site   (Intercept)  0        0
> Number of obs: 51, groups: Site, 14

I smell a very flat likelihood here. You have 51 records, and you would
like to fit some "fixed" effects (you do not show the whole output so we
can not evaluate how many levels are there for sess and breedfem) and
a "random" effect of Site. I guess there is very little information in 
the data for the stimation of variance between the sites.

gg



From bates at stat.wisc.edu  Thu Aug 14 11:07:45 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 14 Aug 2008 11:07:45 +0200
Subject: [R-sig-ME] segfaults from lmer on a 64-bit system
In-Reply-To: <C4C91CDF.14A11%bbh@umich.edu>
References: <40e66e0b0808122355n1b333cf7heb016eac2ebea2@mail.gmail.com>
	<C4C91CDF.14A11%bbh@umich.edu>
Message-ID: <40e66e0b0808140207l7bab061ej1dc614519f84f707@mail.gmail.com>

Thanks for sending the data and the formula, Ben.

I have good news and bad news.  The good news is that I was able to
get consistent behavior on this example from different systems.  The
bad news is that I did so by having R segfault on a Mac.

I'm not sure exactly where the segfault occurs but the underlying
problem is your random effects specification.  You are specifying a
huge number of random effects (27 terms, some of which may correspond
to more than one random effect) for each level of the grouping factor,
by.var, which has only 4 levels.  You are trying to estimate something
like 380 variances or covariances from 4 groups.

Specifying the random effects is not simply a matter of incorporating
every fixed effect as a random effects and allowing a general
variance-covariance structure.  In this case about the best that you
can hope to include with only 4 levels of the grouping factor is
(1|by.var).


On Thu, Aug 14, 2008 at 5:21 AM, Ben Hansen <bbh at umich.edu> wrote:
> Thanks for these suggestions.
>
> I updated to the latest versions of the packages, and the segfault persists.
>
> I have isolated an example for reproduction purposes. The data are given in
>    http://www.stat.lsa.umich.edu/~bbh/segfault-bh.rda
>
> I get segfaults when I do:
>>  library(lme4) ; load("segfault-bh.rda") ; mylme <-lmer(myfmla, data=mydata)
>
> The segfault occurs on a couple of setups:
> 1.  64-bit Red Hat linux; R version 2.7.1:
>> sessionInfo()
> R version 2.7.1 (2008-06-23)
> x86_64-unknown-linux-gnu
>
> locale:
> LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8
> ;LC_MONETARY=C;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADD
> RESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] lme4_0.999375-24   Matrix_0.999375-11 lattice_0.17-8
>
> loaded via a namespace (and not attached):
> [1] grid_2.7.1  tools_2.7.1
>
> 2. Mac OS; R version 2.7.1:
>> sessionInfo()
> R version 2.7.1 (2008-06-23)
> powerpc-apple-darwin8.10.1
>
> locale:
> C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] lme4_0.999375-22   Matrix_0.999375-10 lattice_0.17-8
>
> loaded via a namespace (and not attached):
> [1] grid_2.7.1
>
> 3.  I also got the segfault on the 64-bit linux machine using R version
> 2.6.1, lme4 version 0.99875-9 (I think), and Matrix version 0.999375-9 (I
> think).
>
> 4. *On the other hand,* it sort of works (without segfaulting at least)
> using older R, lme4 and Matrix on my Mac:
>> sessionInfo()
> R version 2.6.2 (2008-02-08)
> powerpc-apple-darwin8.10.1
>
> locale:
> C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] lme4_0.99875-9    Matrix_0.999375-9 lattice_0.17-6
>
> loaded via a namespace (and not attached):
> [1] grid_2.6.2
>
>> mylme <- lmer(myfmla, data=mydata)
>
> Warning messages:
> 1: In .local(x, ..., value) :
>  Estimated variance-covariance for factor 'by.var' is singular
>
> 2: In .local(x, ..., value) :
>  nlminb returned message false convergence (8)
>
> --Perhaps the warnings contain a clue as to what the problem is?
>
> Thanks for your consideration of this.
>
> Ben
> On 8/13/08 2:55 AM, "Douglas Bates" <bates at stat.wisc.edu> wrote:
>
>> I would first try later versions of both the Matrix and lme4 packages.
>>  Versions currently on CRAN are Matrix_0.999375-11 and
>> lme4_0.999375-24
>>
>> If you continue to get segfaults then having a reproducible example,
>> along with the data - anonymized if you wish, will be important.
>>
>> You can determine the versions of the packages that you are using with
>>
>> sessionInfo()
>>
>>
>> On Wed, Aug 13, 2008 at 12:06 AM, Hansen, Ben <bbh at umich.edu> wrote:
>>> Hello all,
>>>
>>>
>>>
>>> I am encountering trouble with lme4 on a 64-bit linux system, and I'm
>>> wondering if any of you might have advice on how to circumvent it. Using
>>> recent versions of lme4 and R, I was able to compile and install the
>>> package, and use it with success on relatively small problems.  However,
>>> when I tried to scale up to work on a big problem, applying lmer() with
>>> thousands of observations and a hundred or so random effects, my R fails
>>> reporting a segmentation fault.  I don't see the same problem on a
>>> different 32-bit Mac OSX system, which runs the same code without
>>> evident difficulty.
>>>
>>>
>>>
>>> Some details of the setup where the problem occurs.  We've tried a few
>>> combinations of things, so I write what we tried first, with variants
>>> noted in parentheses:
>>>
>>>
>>>
>>> Red Hat Enterprise Linux 4 (version 5)
>>>
>>> R version 2.7.1 (2.6.1)
>>>
>>> gcc/g++ compilers for C and C++, Portland Group Fortran compilers
>>> (gcc/g++/gfortran, ie full GNU compiler suite)
>>>
>>> Matrix package version 0.999375-10  (installed with and without
>>> --with-package-versions flag)
>>>
>>> lme4 version 0.999375-16 (installed with and without
>>> --with-package-versions flag)
>>>
>>>
>>>
>>> Any suggestions?  I'd be very appreciative.
>>>
>>>
>>>
>>> Ben
>>>
>>>
>>>
>>>
>>>
>>>
>>>        [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>
>



From Kurt.Hornik at wu-wien.ac.at  Thu Aug 14 10:55:42 2008
From: Kurt.Hornik at wu-wien.ac.at (Kurt Hornik)
Date: Thu, 14 Aug 2008 10:55:42 +0200
Subject: [R-sig-ME] Chapter12?
In-Reply-To: <48A29129.4010906@biostat.ku.dk>
References: <430370.23458.qm@web27308.mail.ukl.yahoo.com>
	<48A29129.4010906@biostat.ku.dk>
Message-ID: <18595.62222.716587.862382@fangorn.hornik.net>

>>>>> Peter Dalgaard writes:

> Mark Difford wrote:
>> In fact, though Chapter 12 still exists in the TOC of the document, the relevant chapter (Statistical Models in R) has mysteriously become chapter 11 (it was a long time ago that I last looked at it). The relevant section is ?11.1 Defining statistical models; formulae.
>> http://cran.r-project.org/doc/manuals/R-intro.html
>> Regards, Mark.
>> Mark Difford
>> Ph.D. candidate, Botany Department,
>> Nelson Mandela Metropolitan University,
>> Port Elizabeth, SA.
>> 
> This look like a generic texinfo issue affecting HTML output. Numbered 
> sections are off by one when the first section (here Preface) is 
> unnumbered. Same thing in R-exts.html and others.

> (CC to Kurt Hornik because he usually knows texinfo things.)

That appears to be a bug in the way makeinfo generates the toc.

-k



From ken at kjbeath.com.au  Thu Aug 14 11:10:10 2008
From: ken at kjbeath.com.au (Ken Beath)
Date: Thu, 14 Aug 2008 19:10:10 +1000
Subject: [R-sig-ME] Variance explained by random factor
In-Reply-To: <B9D1301370916C44B5874AF340C18B9B28AB99047F@VMAILB.uoa.abdn.ac.uk>
References: <B9D1301370916C44B5874AF340C18B9B28AB99047F@VMAILB.uoa.abdn.ac.uk>
Message-ID: <E23EC85F-9F09-4319-BBAB-BFAA47862D08@kjbeath.com.au>

On 14/08/2008, at 1:17 AM, Renwick, A. R. wrote:

>
> I am currently trying to run a lmer model with poisson  
> distrubution.  I tested the model with a model without the random  
> effect and it inferred that I should include the random effect:
>
> ma1<-glm(RoundedOverlap~sess+breedfem,family=poisson,data=Male)
> mixed<-lmer(RoundedOverlap~sess+breedfem+sess:breedfem+(1| 
> Site),family=poisson,data=Male)
>
> #test to see if sig difference between glm and glmm
> as.numeric(2*(logLik(mixed)-logLik(ma)))
> #99.16136
> pchisq(99.16136,1,lower=FALSE)
> #2.327441e-23  so should use a GLMM
>

The problem may be due to the random effects model containing an  
interaction term sess:breedfem that the glm doesn't.

Ken



From bates at stat.wisc.edu  Thu Aug 14 11:21:46 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 14 Aug 2008 11:21:46 +0200
Subject: [R-sig-ME] Variance explained by random factor
In-Reply-To: <E23EC85F-9F09-4319-BBAB-BFAA47862D08@kjbeath.com.au>
References: <B9D1301370916C44B5874AF340C18B9B28AB99047F@VMAILB.uoa.abdn.ac.uk>
	<E23EC85F-9F09-4319-BBAB-BFAA47862D08@kjbeath.com.au>
Message-ID: <40e66e0b0808140221s2f2bd401h78c71672de7f58b9@mail.gmail.com>

On Thu, Aug 14, 2008 at 11:10 AM, Ken Beath <ken at kjbeath.com.au> wrote:
> On 14/08/2008, at 1:17 AM, Renwick, A. R. wrote:
>
>>
>> I am currently trying to run a lmer model with poisson distrubution.  I
>> tested the model with a model without the random effect and it inferred that
>> I should include the random effect:
>>
>> ma1<-glm(RoundedOverlap~sess+breedfem,family=poisson,data=Male)
>>
>> mixed<-lmer(RoundedOverlap~sess+breedfem+sess:breedfem+(1|Site),family=poisson,data=Male)
>>
>> #test to see if sig difference between glm and glmm
>> as.numeric(2*(logLik(mixed)-logLik(ma)))
>> #99.16136
>> pchisq(99.16136,1,lower=FALSE)
>> #2.327441e-23  so should use a GLMM
>>
>
> The problem may be due to the random effects model containing an interaction
> term sess:breedfem that the glm doesn't.

I agree.  The result from the likelihood ratio test is actually
evaluating the significance of the interaction term, not the random
effects term.

> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From a.renwick at abdn.ac.uk  Thu Aug 14 11:24:08 2008
From: a.renwick at abdn.ac.uk (Renwick, A. R.)
Date: Thu, 14 Aug 2008 10:24:08 +0100
Subject: [R-sig-ME] Variance explained by random factor
In-Reply-To: <40e66e0b0808140221s2f2bd401h78c71672de7f58b9@mail.gmail.com>
References: <B9D1301370916C44B5874AF340C18B9B28AB99047F@VMAILB.uoa.abdn.ac.uk>
	<E23EC85F-9F09-4319-BBAB-BFAA47862D08@kjbeath.com.au>
	<40e66e0b0808140221s2f2bd401h78c71672de7f58b9@mail.gmail.com>
Message-ID: <B9D1301370916C44B5874AF340C18B9B28AB990481@VMAILB.uoa.abdn.ac.uk>

Many apologise but the glm model I compared was ma not ma1 and thus did have the interaction term:

ma<-glm(RoundedOverlap~sess+breedfem+sess:breedfem ,family=poisson,data=Male)
mixed<-lmer(RoundedOverlap~sess+breedfem+sess:breedfem+(1|Site),family=poisson,data=Male)



-----Original Message-----
From: dmbates at gmail.com [mailto:dmbates at gmail.com] On Behalf Of Douglas Bates
Sent: 14 August 2008 10:22
To: Ken Beath
Cc: Renwick, A. R.; r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Variance explained by random factor

On Thu, Aug 14, 2008 at 11:10 AM, Ken Beath <ken at kjbeath.com.au> wrote:
> On 14/08/2008, at 1:17 AM, Renwick, A. R. wrote:
>
>>
>> I am currently trying to run a lmer model with poisson distrubution.
>> I tested the model with a model without the random effect and it
>> inferred that I should include the random effect:
>>
>> ma1<-glm(RoundedOverlap~sess+breedfem,family=poisson,data=Male)
>>
>> mixed<-lmer(RoundedOverlap~sess+breedfem+sess:breedfem+(1|Site),famil
>> y=poisson,data=Male)
>>
>> #test to see if sig difference between glm and glmm
>> as.numeric(2*(logLik(mixed)-logLik(ma)))
>> #99.16136
>> pchisq(99.16136,1,lower=FALSE)
>> #2.327441e-23  so should use a GLMM
>>
>
> The problem may be due to the random effects model containing an
> interaction term sess:breedfem that the glm doesn't.

I agree.  The result from the likelihood ratio test is actually evaluating the significance of the interaction term, not the random effects term.

> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


The University of Aberdeen is a charity registered in Scotland, No SC013683.



From bates at stat.wisc.edu  Thu Aug 14 11:43:39 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 14 Aug 2008 11:43:39 +0200
Subject: [R-sig-ME] Variance explained by random factor
In-Reply-To: <B9D1301370916C44B5874AF340C18B9B28AB990481@VMAILB.uoa.abdn.ac.uk>
References: <B9D1301370916C44B5874AF340C18B9B28AB99047F@VMAILB.uoa.abdn.ac.uk>
	<E23EC85F-9F09-4319-BBAB-BFAA47862D08@kjbeath.com.au>
	<40e66e0b0808140221s2f2bd401h78c71672de7f58b9@mail.gmail.com>
	<B9D1301370916C44B5874AF340C18B9B28AB990481@VMAILB.uoa.abdn.ac.uk>
Message-ID: <40e66e0b0808140243s66203400g58bf73bbc0c33233@mail.gmail.com>

On Thu, Aug 14, 2008 at 11:24 AM, Renwick, A. R. <a.renwick at abdn.ac.uk> wrote:
> Many apologise but the glm model I compared was ma not ma1 and thus did have the interaction term:

> ma<-glm(RoundedOverlap~sess+breedfem+sess:breedfem ,family=poisson,data=Male)
> mixed<-lmer(RoundedOverlap~sess+breedfem+sess:breedfem+(1|Site),family=poisson,data=Male)

In that case it could be that the deviance or log-likelihood is not
being evaluated correctly in glmer.  Look at the slot named 'deviance'
in the lmer fit.  It should be a named numeric vector.  The names of
interest are 'disc', the discrepancy for the generalized linear models
(this is the deviance without the compensation for the null deviance),
'ldL2', the logarithm of the square of the determinant of the Cholesky
factor of a second-order term, and usqr, the squared length of the
transformed random effects.  For a mixed-effects model in which the
variance of the random effects is estimated as zero, both 'ldL2' and
'usqr' should be zero.

You can check these values in

mixed at deviance
> -----Original Message-----
> From: dmbates at gmail.com [mailto:dmbates at gmail.com] On Behalf Of Douglas Bates
> Sent: 14 August 2008 10:22
> To: Ken Beath
> Cc: Renwick, A. R.; r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Variance explained by random factor
>
> On Thu, Aug 14, 2008 at 11:10 AM, Ken Beath <ken at kjbeath.com.au> wrote:
>> On 14/08/2008, at 1:17 AM, Renwick, A. R. wrote:
>>
>>>
>>> I am currently trying to run a lmer model with poisson distrubution.
>>> I tested the model with a model without the random effect and it
>>> inferred that I should include the random effect:
>>>
>>> ma1<-glm(RoundedOverlap~sess+breedfem,family=poisson,data=Male)
>>>
>>> mixed<-lmer(RoundedOverlap~sess+breedfem+sess:breedfem+(1|Site),famil
>>> y=poisson,data=Male)
>>>
>>> #test to see if sig difference between glm and glmm
>>> as.numeric(2*(logLik(mixed)-logLik(ma)))
>>> #99.16136
>>> pchisq(99.16136,1,lower=FALSE)
>>> #2.327441e-23  so should use a GLMM
>>>
>>
>> The problem may be due to the random effects model containing an
>> interaction term sess:breedfem that the glm doesn't.
>
> I agree.  The result from the likelihood ratio test is actually evaluating the significance of the interaction term, not the random effects term.
>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>
> The University of Aberdeen is a charity registered in Scotland, No SC013683.
>



From a.renwick at abdn.ac.uk  Thu Aug 14 12:00:00 2008
From: a.renwick at abdn.ac.uk (Renwick, A. R.)
Date: Thu, 14 Aug 2008 11:00:00 +0100
Subject: [R-sig-ME] Variance explained by random factor
In-Reply-To: <40e66e0b0808140243s66203400g58bf73bbc0c33233@mail.gmail.com>
References: <B9D1301370916C44B5874AF340C18B9B28AB99047F@VMAILB.uoa.abdn.ac.uk>
	<E23EC85F-9F09-4319-BBAB-BFAA47862D08@kjbeath.com.au>
	<40e66e0b0808140221s2f2bd401h78c71672de7f58b9@mail.gmail.com>
	<B9D1301370916C44B5874AF340C18B9B28AB990481@VMAILB.uoa.abdn.ac.uk>
	<40e66e0b0808140243s66203400g58bf73bbc0c33233@mail.gmail.com>
Message-ID: <B9D1301370916C44B5874AF340C18B9B28AB990484@VMAILB.uoa.abdn.ac.uk>

Fanatstic.  Thanks such a lot for that command.
The
'ldL2' = 2.178613e-11
'usqr' = 0

Is it not strange though that the test to see if sig difference between glm and glmm is so highly significant?

 as.numeric(2*(logLik(mixed)-logLik(ma)))
#99.16136
pchisq(99.16136,1,lower=FALSE)
#2.327441e-23  so should use a GLMM

-----Original Message-----
From: dmbates at gmail.com [mailto:dmbates at gmail.com] On Behalf Of Douglas Bates
Sent: 14 August 2008 10:44
To: Renwick, A. R.
Cc: Ken Beath; r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Variance explained by random factor

On Thu, Aug 14, 2008 at 11:24 AM, Renwick, A. R. <a.renwick at abdn.ac.uk> wrote:
> Many apologise but the glm model I compared was ma not ma1 and thus did have the interaction term:

> ma<-glm(RoundedOverlap~sess+breedfem+sess:breedfem
> ,family=poisson,data=Male)
> mixed<-lmer(RoundedOverlap~sess+breedfem+sess:breedfem+(1|Site),family
> =poisson,data=Male)

In that case it could be that the deviance or log-likelihood is not being evaluated correctly in glmer.  Look at the slot named 'deviance'
in the lmer fit.  It should be a named numeric vector.  The names of interest are 'disc', the discrepancy for the generalized linear models (this is the deviance without the compensation for the null deviance), 'ldL2', the logarithm of the square of the determinant of the Cholesky factor of a second-order term, and usqr, the squared length of the transformed random effects.  For a mixed-effects model in which the variance of the random effects is estimated as zero, both 'ldL2' and 'usqr' should be zero.

You can check these values in

mixed at deviance
> -----Original Message-----
> From: dmbates at gmail.com [mailto:dmbates at gmail.com] On Behalf Of
> Douglas Bates
> Sent: 14 August 2008 10:22
> To: Ken Beath
> Cc: Renwick, A. R.; r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Variance explained by random factor
>
> On Thu, Aug 14, 2008 at 11:10 AM, Ken Beath <ken at kjbeath.com.au> wrote:
>> On 14/08/2008, at 1:17 AM, Renwick, A. R. wrote:
>>
>>>
>>> I am currently trying to run a lmer model with poisson distrubution.
>>> I tested the model with a model without the random effect and it
>>> inferred that I should include the random effect:
>>>
>>> ma1<-glm(RoundedOverlap~sess+breedfem,family=poisson,data=Male)
>>>
>>> mixed<-lmer(RoundedOverlap~sess+breedfem+sess:breedfem+(1|Site),fami
>>> l
>>> y=poisson,data=Male)
>>>
>>> #test to see if sig difference between glm and glmm
>>> as.numeric(2*(logLik(mixed)-logLik(ma)))
>>> #99.16136
>>> pchisq(99.16136,1,lower=FALSE)
>>> #2.327441e-23  so should use a GLMM
>>>
>>
>> The problem may be due to the random effects model containing an
>> interaction term sess:breedfem that the glm doesn't.
>
> I agree.  The result from the likelihood ratio test is actually evaluating the significance of the interaction term, not the random effects term.
>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>
> The University of Aberdeen is a charity registered in Scotland, No SC013683.
>


The University of Aberdeen is a charity registered in Scotland, No SC013683.



From a.beckerman at sheffield.ac.uk  Thu Aug 14 12:46:48 2008
From: a.beckerman at sheffield.ac.uk (Andrew Beckerman)
Date: Thu, 14 Aug 2008 11:46:48 +0100
Subject: [R-sig-ME] Variance explained by random factor
In-Reply-To: <B9D1301370916C44B5874AF340C18B9B28AB990484@VMAILB.uoa.abdn.ac.uk>
References: <B9D1301370916C44B5874AF340C18B9B28AB99047F@VMAILB.uoa.abdn.ac.uk>
	<E23EC85F-9F09-4319-BBAB-BFAA47862D08@kjbeath.com.au>
	<40e66e0b0808140221s2f2bd401h78c71672de7f58b9@mail.gmail.com>
	<B9D1301370916C44B5874AF340C18B9B28AB990481@VMAILB.uoa.abdn.ac.uk>
	<40e66e0b0808140243s66203400g58bf73bbc0c33233@mail.gmail.com>
	<B9D1301370916C44B5874AF340C18B9B28AB990484@VMAILB.uoa.abdn.ac.uk>
Message-ID: <3FDCE02C-A713-42F1-BC3B-3C7BAAF872A2@sheffield.ac.uk>

I think i failed to copy this to the list (it went directly to Ana) -  
perhaps pertinent to the LRT result.....

ma1 and mixed differ in both fixed and random effects..... mixed has  
the interaction between fixed effects..... not sure that was intended,  
but you are unlikely to be picking up differences caused only by the  
random effects in your use of the LRT.

Andrew



On 14 Aug 2008, at 11:00, Renwick, A. R. wrote:

> Fanatstic.  Thanks such a lot for that command.
> The
> 'ldL2' = 2.178613e-11
> 'usqr' = 0
>
> Is it not strange though that the test to see if sig difference  
> between glm and glmm is so highly significant?
>
> as.numeric(2*(logLik(mixed)-logLik(ma)))
> #99.16136
> pchisq(99.16136,1,lower=FALSE)
> #2.327441e-23  so should use a GLMM
>
> -----Original Message-----
> From: dmbates at gmail.com [mailto:dmbates at gmail.com] On Behalf Of  
> Douglas Bates
> Sent: 14 August 2008 10:44
> To: Renwick, A. R.
> Cc: Ken Beath; r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Variance explained by random factor
>
> On Thu, Aug 14, 2008 at 11:24 AM, Renwick, A. R.  
> <a.renwick at abdn.ac.uk> wrote:
>> Many apologise but the glm model I compared was ma not ma1 and thus  
>> did have the interaction term:
>
>> ma<-glm(RoundedOverlap~sess+breedfem+sess:breedfem
>> ,family=poisson,data=Male)
>> mixed<-lmer(RoundedOverlap~sess+breedfem+sess:breedfem+(1| 
>> Site),family
>> =poisson,data=Male)
>
> In that case it could be that the deviance or log-likelihood is not  
> being evaluated correctly in glmer.  Look at the slot named 'deviance'
> in the lmer fit.  It should be a named numeric vector.  The names of  
> interest are 'disc', the discrepancy for the generalized linear  
> models (this is the deviance without the compensation for the null  
> deviance), 'ldL2', the logarithm of the square of the determinant of  
> the Cholesky factor of a second-order term, and usqr, the squared  
> length of the transformed random effects.  For a mixed-effects model  
> in which the variance of the random effects is estimated as zero,  
> both 'ldL2' and 'usqr' should be zero.
>
> You can check these values in
>
> mixed at deviance
>> -----Original Message-----
>> From: dmbates at gmail.com [mailto:dmbates at gmail.com] On Behalf Of
>> Douglas Bates
>> Sent: 14 August 2008 10:22
>> To: Ken Beath
>> Cc: Renwick, A. R.; r-sig-mixed-models at r-project.org
>> Subject: Re: [R-sig-ME] Variance explained by random factor
>>
>> On Thu, Aug 14, 2008 at 11:10 AM, Ken Beath <ken at kjbeath.com.au>  
>> wrote:
>>> On 14/08/2008, at 1:17 AM, Renwick, A. R. wrote:
>>>
>>>>
>>>> I am currently trying to run a lmer model with poisson  
>>>> distrubution.
>>>> I tested the model with a model without the random effect and it
>>>> inferred that I should include the random effect:
>>>>
>>>> ma1<-glm(RoundedOverlap~sess+breedfem,family=poisson,data=Male)
>>>>
>>>> mixed<-lmer(RoundedOverlap~sess+breedfem+sess:breedfem+(1| 
>>>> Site),fami
>>>> l
>>>> y=poisson,data=Male)
>>>>
>>>> #test to see if sig difference between glm and glmm
>>>> as.numeric(2*(logLik(mixed)-logLik(ma)))
>>>> #99.16136
>>>> pchisq(99.16136,1,lower=FALSE)
>>>> #2.327441e-23  so should use a GLMM
>>>>
>>>
>>> The problem may be due to the random effects model containing an
>>> interaction term sess:breedfem that the glm doesn't.
>>
>> I agree.  The result from the likelihood ratio test is actually  
>> evaluating the significance of the interaction term, not the random  
>> effects term.
>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>
>>
>> The University of Aberdeen is a charity registered in Scotland, No  
>> SC013683.
>>
>
>
> The University of Aberdeen is a charity registered in Scotland, No  
> SC013683.
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From HStevens at muohio.edu  Thu Aug 14 15:32:05 2008
From: HStevens at muohio.edu (Martin Henry H. Stevens)
Date: Thu, 14 Aug 2008 09:32:05 -0400
Subject: [R-sig-ME] extracting residuals in lmer glmm
Message-ID: <4A1AF404-7E24-4DF9-9785-BE01A3919230@muohio.edu>

Hi folks,
I seem unable to figure out how to extract residuals from a Poisson  
lmer model.

And, I am not sure what to do with the working residuals.

Any references or ideas would be appreciated.

# Example:
y <- rpois(100, lambda=10)
xz <- expand.grid(x=gl(2,5), z=gl(10,1))
m1 <- lmer(y ~ x + (1|z), xz, family="poisson")

resid(m1)
Error: 'resid' is not implemented yet

  residuals(m1)
Error: 'residuals' is not implemented yet

  summary(m1 @ wrkres)
Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
1.38    1.98    2.20    2.22    2.50    3.13


Hank Stevens



From HStevens at MUOhio.edu  Thu Aug 14 16:47:48 2008
From: HStevens at MUOhio.edu (Hank Stevens)
Date: Thu, 14 Aug 2008 10:47:48 -0400
Subject: [R-sig-ME] extracting residuals in lmer glmm
In-Reply-To: <4A1AF404-7E24-4DF9-9785-BE01A3919230@muohio.edu>
References: <4A1AF404-7E24-4DF9-9785-BE01A3919230@muohio.edu>
Message-ID: <FB8F5E5D-2CAB-4F29-B7A0-CD1D856D406E@MUOhio.edu>

In answer (sort of) to my own question, resid() now works on both mer  
and summary.mer objects.
All of these results (resid() working and not working) have occurred  
while using yesterday's CRAN version of lme4 {R package version  
0.999375-24}.

I have no idea what would have caused the problem yesterday and this  
morning, but not now. Oh well.

Hank


y <- rpois(100, lambda=10)
xz <- expand.grid(x=gl(2,5), z=gl(10,1))
m1 <- lmer(y ~ x + (1|z), xz, family="poisson")
m1s <- summary(m1)
resid(m1)
   [1] -0.04336 -0.98556  0.89885  0.27071 -0.67149 -0.14260
...
resid(m1s)
   [1] -0.04336 -0.98556  0.89885  0.27071 -0.67149 -0.14260
...

On Aug 14, 2008, at 9:32 AM, Martin Henry H. Stevens wrote:

> Hi folks,
> I seem unable to figure out how to extract residuals from a Poisson
> lmer model.
>
> And, I am not sure what to do with the working residuals.
>
> Any references or ideas would be appreciated.
>
> # Example:
> y <- rpois(100, lambda=10)
> xz <- expand.grid(x=gl(2,5), z=gl(10,1))
> m1 <- lmer(y ~ x + (1|z), xz, family="poisson")
>
> resid(m1)
> Error: 'resid' is not implemented yet
>
>  residuals(m1)
> Error: 'residuals' is not implemented yet
>
>  summary(m1 @ wrkres)
> Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
> 1.38    1.98    2.20    2.22    2.50    3.13
>
>
> Hank Stevens
>
>
>
>



Dr. Hank Stevens, Associate Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.cas.muohio.edu/ecology
http://www.muohio.edu/botany/

"If the stars should appear one night in a thousand years, how would men
believe and adore." -Ralph Waldo Emerson, writer and philosopher  
(1803-1882)



From bbh at umich.edu  Fri Aug 15 00:16:40 2008
From: bbh at umich.edu (Hansen, Ben)
Date: Thu, 14 Aug 2008 18:16:40 -0400
Subject: [R-sig-ME] segfaults from lmer on a 64-bit system
In-Reply-To: <40e66e0b0808140207l7bab061ej1dc614519f84f707@mail.gmail.com>
References: <40e66e0b0808122355n1b333cf7heb016eac2ebea2@mail.gmail.com>
	<C4C91CDF.14A11%bbh@umich.edu>
	<40e66e0b0808140207l7bab061ej1dc614519f84f707@mail.gmail.com>
Message-ID: <E4F6F36F6B0C7B409966C9B724245814034EB757@ECLUST2-VS4.adsroot.itcs.umich.edu>

Thanks much for looking into this, Doug. 

I recognize that what I'm doing here may seem odd.  My hope was to adapt
the package to a non-standard purpose of mine.  For this purpose, the
variances of the random effects are not of central interest, and it's OK
if they (those variances) are not particularly well estimated.  I found
encouragement in Gelman and Hill's regression book (\S 12.9, pp.275 ff),
which says it's a mistake to think that multilevel models require a
minimum number of groups.  As they put it, the issue is just that when
the number of groups is small then the random-effects variances will
tend to be overestimated.  For my purposes, that would have been OK. 

You suggest only modeling the intercept as random, given that I've only
got four groups.  For my particular purposes, that isn't an option.
Could lmer be expected to better handle my (admittedly ornate) random
effects specfication if there were more groups than 4 -- say, 6 or 8?

Thanks again for the attention.

Ben

Ben B. Hansen
Assistant Professor
Statistics Department and Institute for Social Research
www.stat.lsa.umich.edu/~bbh/
University of Michigan
Ann Arbor, MI 48109-1107


-----Original Message-----
From: dmbates at gmail.com [mailto:dmbates at gmail.com] On Behalf Of Douglas
Bates
Sent: Thursday, August 14, 2008 5:08 AM
To: Hansen, Ben
Cc: r-sig-mixed-models at r-project.org; Caird, Andrew J
Subject: Re: [R-sig-ME] segfaults from lmer on a 64-bit system

Thanks for sending the data and the formula, Ben.

I have good news and bad news.  The good news is that I was able to
get consistent behavior on this example from different systems.  The
bad news is that I did so by having R segfault on a Mac.

I'm not sure exactly where the segfault occurs but the underlying
problem is your random effects specification.  You are specifying a
huge number of random effects (27 terms, some of which may correspond
to more than one random effect) for each level of the grouping factor,
by.var, which has only 4 levels.  You are trying to estimate something
like 380 variances or covariances from 4 groups.

Specifying the random effects is not simply a matter of incorporating
every fixed effect as a random effects and allowing a general
variance-covariance structure.  In this case about the best that you
can hope to include with only 4 levels of the grouping factor is
(1|by.var).


On Thu, Aug 14, 2008 at 5:21 AM, Ben Hansen <bbh at umich.edu> wrote:
> Thanks for these suggestions.
>
> I updated to the latest versions of the packages, and the segfault
persists.
>
> I have isolated an example for reproduction purposes. The data are
given in
>    http://www.stat.lsa.umich.edu/~bbh/segfault-bh.rda
>
> I get segfaults when I do:
>>  library(lme4) ; load("segfault-bh.rda") ; mylme <-lmer(myfmla,
data=mydata)
>
> The segfault occurs on a couple of setups:
> 1.  64-bit Red Hat linux; R version 2.7.1:
>> sessionInfo()
> R version 2.7.1 (2008-06-23)
> x86_64-unknown-linux-gnu
>
> locale:
>
LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.U
TF-8
>
;LC_MONETARY=C;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC
_ADD
> RESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] lme4_0.999375-24   Matrix_0.999375-11 lattice_0.17-8
>
> loaded via a namespace (and not attached):
> [1] grid_2.7.1  tools_2.7.1
>
> 2. Mac OS; R version 2.7.1:
>> sessionInfo()
> R version 2.7.1 (2008-06-23)
> powerpc-apple-darwin8.10.1
>
> locale:
> C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] lme4_0.999375-22   Matrix_0.999375-10 lattice_0.17-8
>
> loaded via a namespace (and not attached):
> [1] grid_2.7.1
>
> 3.  I also got the segfault on the 64-bit linux machine using R
version
> 2.6.1, lme4 version 0.99875-9 (I think), and Matrix version 0.999375-9
(I
> think).
>
> 4. *On the other hand,* it sort of works (without segfaulting at
least)
> using older R, lme4 and Matrix on my Mac:
>> sessionInfo()
> R version 2.6.2 (2008-02-08)
> powerpc-apple-darwin8.10.1
>
> locale:
> C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] lme4_0.99875-9    Matrix_0.999375-9 lattice_0.17-6
>
> loaded via a namespace (and not attached):
> [1] grid_2.6.2
>
>> mylme <- lmer(myfmla, data=mydata)
>
> Warning messages:
> 1: In .local(x, ..., value) :
>  Estimated variance-covariance for factor 'by.var' is singular
>
> 2: In .local(x, ..., value) :
>  nlminb returned message false convergence (8)
>
> --Perhaps the warnings contain a clue as to what the problem is?
>
> Thanks for your consideration of this.
>
> Ben
> On 8/13/08 2:55 AM, "Douglas Bates" <bates at stat.wisc.edu> wrote:
>
>> I would first try later versions of both the Matrix and lme4
packages.
>>  Versions currently on CRAN are Matrix_0.999375-11 and
>> lme4_0.999375-24
>>
>> If you continue to get segfaults then having a reproducible example,
>> along with the data - anonymized if you wish, will be important.
>>
>> You can determine the versions of the packages that you are using
with
>>
>> sessionInfo()
>>
>>
>> On Wed, Aug 13, 2008 at 12:06 AM, Hansen, Ben <bbh at umich.edu> wrote:
>>> Hello all,
>>>
>>>
>>>
>>> I am encountering trouble with lme4 on a 64-bit linux system, and
I'm
>>> wondering if any of you might have advice on how to circumvent it.
Using
>>> recent versions of lme4 and R, I was able to compile and install the
>>> package, and use it with success on relatively small problems.
However,
>>> when I tried to scale up to work on a big problem, applying lmer()
with
>>> thousands of observations and a hundred or so random effects, my R
fails
>>> reporting a segmentation fault.  I don't see the same problem on a
>>> different 32-bit Mac OSX system, which runs the same code without
>>> evident difficulty.
>>>
>>>
>>>
>>> Some details of the setup where the problem occurs.  We've tried a
few
>>> combinations of things, so I write what we tried first, with
variants
>>> noted in parentheses:
>>>
>>>
>>>
>>> Red Hat Enterprise Linux 4 (version 5)
>>>
>>> R version 2.7.1 (2.6.1)
>>>
>>> gcc/g++ compilers for C and C++, Portland Group Fortran compilers
>>> (gcc/g++/gfortran, ie full GNU compiler suite)
>>>
>>> Matrix package version 0.999375-10  (installed with and without
>>> --with-package-versions flag)
>>>
>>> lme4 version 0.999375-16 (installed with and without
>>> --with-package-versions flag)
>>>
>>>
>>>
>>> Any suggestions?  I'd be very appreciative.
>>>
>>>
>>>
>>> Ben
>>>
>>>
>>>
>>>
>>>
>>>
>>>        [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>
>



From bates at stat.wisc.edu  Sat Aug 16 11:04:26 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 16 Aug 2008 04:04:26 -0500
Subject: [R-sig-ME] extracting residuals in lmer glmm
In-Reply-To: <FB8F5E5D-2CAB-4F29-B7A0-CD1D856D406E@MUOhio.edu>
References: <4A1AF404-7E24-4DF9-9785-BE01A3919230@muohio.edu>
	<FB8F5E5D-2CAB-4F29-B7A0-CD1D856D406E@MUOhio.edu>
Message-ID: <40e66e0b0808160204o5ddbd35bxdf34f2b3650071d9@mail.gmail.com>

On Thu, Aug 14, 2008 at 9:47 AM, Hank Stevens <HStevens at muohio.edu> wrote:
> In answer (sort of) to my own question, resid() now works on both mer and
> summary.mer objects.
> All of these results (resid() working and not working) have occurred while
> using yesterday's CRAN version of lme4 {R package version 0.999375-24}.

> I have no idea what would have caused the problem yesterday and this
> morning, but not now. Oh well.

I was just going to write that you must be using a very old version of
the lme4 package.  The wrkres slot was removed in the transition from
the 0.99875 to 0.999375 series.

What is returned by the resid generic applied to a generalized linear
mixed model is

(m1 at y - m1 at mu)/sqrt(m1 at var)

That is one of the many possible definitions of "residuals" for a
generalized linear model.  There is a name for those residuals but I
can't remember right now what they are callled - perhaps after I have
recovered from the trans-Atlantic flight returning from the useR!2008
conference, which was wonderful.  Thanks again to the organizers and
to the more than 400 attendees who made it such a success.  We hope to
see many of you again in Rennes, France for useR!2009 next July.


>
> Hank
>
>
> y <- rpois(100, lambda=10)
> xz <- expand.grid(x=gl(2,5), z=gl(10,1))
> m1 <- lmer(y ~ x + (1|z), xz, family="poisson")
> m1s <- summary(m1)
> resid(m1)
>  [1] -0.04336 -0.98556  0.89885  0.27071 -0.67149 -0.14260
> ...
> resid(m1s)
>  [1] -0.04336 -0.98556  0.89885  0.27071 -0.67149 -0.14260
> ...
>
> On Aug 14, 2008, at 9:32 AM, Martin Henry H. Stevens wrote:
>
>> Hi folks,
>> I seem unable to figure out how to extract residuals from a Poisson
>> lmer model.
>>
>> And, I am not sure what to do with the working residuals.
>>
>> Any references or ideas would be appreciated.
>>
>> # Example:
>> y <- rpois(100, lambda=10)
>> xz <- expand.grid(x=gl(2,5), z=gl(10,1))
>> m1 <- lmer(y ~ x + (1|z), xz, family="poisson")
>>
>> resid(m1)
>> Error: 'resid' is not implemented yet
>>
>>  residuals(m1)
>> Error: 'residuals' is not implemented yet
>>
>>  summary(m1 @ wrkres)
>> Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>> 1.38    1.98    2.20    2.22    2.50    3.13
>>
>>
>> Hank Stevens
>>
>>
>>
>>
>
>
>
> Dr. Hank Stevens, Associate Professor
> 338 Pearson Hall
> Botany Department
> Miami University
> Oxford, OH 45056
>
> Office: (513) 529-4206
> Lab: (513) 529-4262
> FAX: (513) 529-4243
> http://www.cas.muohio.edu/~stevenmh/
> http://www.cas.muohio.edu/ecology
> http://www.muohio.edu/botany/
>
> "If the stars should appear one night in a thousand years, how would men
> believe and adore." -Ralph Waldo Emerson, writer and philosopher (1803-1882)
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From a.renwick at abdn.ac.uk  Mon Aug 18 16:35:25 2008
From: a.renwick at abdn.ac.uk (Renwick, A. R.)
Date: Mon, 18 Aug 2008 15:35:25 +0100
Subject: [R-sig-ME]  Lmer  for negative binomial data
Message-ID: <B9D1301370916C44B5874AF340C18B9B28AB990492@VMAILB.uoa.abdn.ac.uk>

I am trying to run a glmm with a negative binomial distribution but am having difficulty coding it.
I ran a glm.nb to obtain a value for theta and then used in in the lmer model as below:

 mix<-lmer(realdis~width+(1|site),family=negative.binomial(theta=0.43),data=move)

However, I keep getting error messages:

Error in famType(glmFit$family) :
  unknown GLM family: 'Negative Binomial(0.43)'

If anybody could enlighten me to the best way to procede it would be much apprecaited.

Anna

Anna Renwick
Zoology Building
School of Biological Sciences
University of Aberdeen
Aberdeen
AB24 2TZ


The University of Aberdeen is a charity registered in Scotland, No SC013683.



From bates at stat.wisc.edu  Mon Aug 18 18:05:19 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 18 Aug 2008 11:05:19 -0500
Subject: [R-sig-ME] Lmer for negative binomial data
In-Reply-To: <B9D1301370916C44B5874AF340C18B9B28AB990492@VMAILB.uoa.abdn.ac.uk>
References: <B9D1301370916C44B5874AF340C18B9B28AB990492@VMAILB.uoa.abdn.ac.uk>
Message-ID: <40e66e0b0808180905u68ad4293l8f24abb0049a5985@mail.gmail.com>

As the message indicates, the negative binomial family is not defined
as a glm family.  The glm.nb function is a special-purpose function
for fitting that model.  I'm not really sure what modifications would
be needed to glmer to be able to fit such a model but I fear that they
would not be trivial.

On Mon, Aug 18, 2008 at 9:35 AM, Renwick, A. R. <a.renwick at abdn.ac.uk> wrote:
> I am trying to run a glmm with a negative binomial distribution but am having difficulty coding it.
> I ran a glm.nb to obtain a value for theta and then used in in the lmer model as below:
>
>  mix<-lmer(realdis~width+(1|site),family=negative.binomial(theta=0.43),data=move)
>
> However, I keep getting error messages:
>
> Error in famType(glmFit$family) :
>  unknown GLM family: 'Negative Binomial(0.43)'
>
> If anybody could enlighten me to the best way to procede it would be much apprecaited.
>
> Anna
>
> Anna Renwick
> Zoology Building
> School of Biological Sciences
> University of Aberdeen
> Aberdeen
> AB24 2TZ
>
>
> The University of Aberdeen is a charity registered in Scotland, No SC013683.
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Mon Aug 18 19:53:41 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 18 Aug 2008 12:53:41 -0500
Subject: [R-sig-ME] Variance explained by random factor
In-Reply-To: <B9D1301370916C44B5874AF340C18B9B28AB990484@VMAILB.uoa.abdn.ac.uk>
References: <B9D1301370916C44B5874AF340C18B9B28AB99047F@VMAILB.uoa.abdn.ac.uk>
	<E23EC85F-9F09-4319-BBAB-BFAA47862D08@kjbeath.com.au>
	<40e66e0b0808140221s2f2bd401h78c71672de7f58b9@mail.gmail.com>
	<B9D1301370916C44B5874AF340C18B9B28AB990481@VMAILB.uoa.abdn.ac.uk>
	<40e66e0b0808140243s66203400g58bf73bbc0c33233@mail.gmail.com>
	<B9D1301370916C44B5874AF340C18B9B28AB990484@VMAILB.uoa.abdn.ac.uk>
Message-ID: <40e66e0b0808181053n5c433cb9pb36b03c6462adee4@mail.gmail.com>

On Thu, Aug 14, 2008 at 5:00 AM, Renwick, A. R. <a.renwick at abdn.ac.uk> wrote:
> Fanatstic.  Thanks such a lot for that command.
> The
> 'ldL2' = 2.178613e-11
> 'usqr' = 0
>
> Is it not strange though that the test to see if sig difference between glm and glmm is so highly significant?

If the ldL2 and the usqr components are zero then the deviance for the
mixed model should be the same as the deviance for the glm model.
Does the glm model have a non-zero component named null.deviance?  If
so, that will probably be the difference in the models.  I have a
long-standing bug report that I filed to remind myself to incorporate
the null.deviance in the calculation of the deviance of a glmer model.

>  as.numeric(2*(logLik(mixed)-logLik(ma)))
> #99.16136
> pchisq(99.16136,1,lower=FALSE)
> #2.327441e-23  so should use a GLMM
>
> -----Original Message-----
> From: dmbates at gmail.com [mailto:dmbates at gmail.com] On Behalf Of Douglas Bates
> Sent: 14 August 2008 10:44
> To: Renwick, A. R.
> Cc: Ken Beath; r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Variance explained by random factor
>
> On Thu, Aug 14, 2008 at 11:24 AM, Renwick, A. R. <a.renwick at abdn.ac.uk> wrote:
>> Many apologise but the glm model I compared was ma not ma1 and thus did have the interaction term:
>
>> ma<-glm(RoundedOverlap~sess+breedfem+sess:breedfem
>> ,family=poisson,data=Male)
>> mixed<-lmer(RoundedOverlap~sess+breedfem+sess:breedfem+(1|Site),family
>> =poisson,data=Male)
>
> In that case it could be that the deviance or log-likelihood is not being evaluated correctly in glmer.  Look at the slot named 'deviance'
> in the lmer fit.  It should be a named numeric vector.  The names of interest are 'disc', the discrepancy for the generalized linear models (this is the deviance without the compensation for the null deviance), 'ldL2', the logarithm of the square of the determinant of the Cholesky factor of a second-order term, and usqr, the squared length of the transformed random effects.  For a mixed-effects model in which the variance of the random effects is estimated as zero, both 'ldL2' and 'usqr' should be zero.
>
> You can check these values in
>
> mixed at deviance
>> -----Original Message-----
>> From: dmbates at gmail.com [mailto:dmbates at gmail.com] On Behalf Of
>> Douglas Bates
>> Sent: 14 August 2008 10:22
>> To: Ken Beath
>> Cc: Renwick, A. R.; r-sig-mixed-models at r-project.org
>> Subject: Re: [R-sig-ME] Variance explained by random factor
>>
>> On Thu, Aug 14, 2008 at 11:10 AM, Ken Beath <ken at kjbeath.com.au> wrote:
>>> On 14/08/2008, at 1:17 AM, Renwick, A. R. wrote:
>>>
>>>>
>>>> I am currently trying to run a lmer model with poisson distrubution.
>>>> I tested the model with a model without the random effect and it
>>>> inferred that I should include the random effect:
>>>>
>>>> ma1<-glm(RoundedOverlap~sess+breedfem,family=poisson,data=Male)
>>>>
>>>> mixed<-lmer(RoundedOverlap~sess+breedfem+sess:breedfem+(1|Site),fami
>>>> l
>>>> y=poisson,data=Male)
>>>>
>>>> #test to see if sig difference between glm and glmm
>>>> as.numeric(2*(logLik(mixed)-logLik(ma)))
>>>> #99.16136
>>>> pchisq(99.16136,1,lower=FALSE)
>>>> #2.327441e-23  so should use a GLMM
>>>>
>>>
>>> The problem may be due to the random effects model containing an
>>> interaction term sess:breedfem that the glm doesn't.
>>
>> I agree.  The result from the likelihood ratio test is actually evaluating the significance of the interaction term, not the random effects term.
>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>
>>
>> The University of Aberdeen is a charity registered in Scotland, No SC013683.
>>
>
>
> The University of Aberdeen is a charity registered in Scotland, No SC013683.
>



From ajmackey at gmail.com  Mon Aug 18 21:41:20 2008
From: ajmackey at gmail.com (Aaron Mackey)
Date: Mon, 18 Aug 2008 15:41:20 -0400
Subject: [R-sig-ME] [R] lmer syntax, matrix of (grouped) covariates?
In-Reply-To: <40e66e0b0808181221p40183858y64a9cedfddd24da3@mail.gmail.com>
References: <24c96eca0808180920t7033f866u1287a35a4d8fe572@mail.gmail.com>
	<40e66e0b0808181221p40183858y64a9cedfddd24da3@mail.gmail.com>
Message-ID: <24c96eca0808181241xff27e26i2063193962852726@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080818/182a2e49/attachment.pl>

From maechler at stat.math.ethz.ch  Tue Aug 19 08:59:58 2008
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 19 Aug 2008 08:59:58 +0200
Subject: [R-sig-ME] [R] lmer syntax, matrix of (grouped) covariates?
In-Reply-To: <24c96eca0808181241xff27e26i2063193962852726@mail.gmail.com>
References: <24c96eca0808180920t7033f866u1287a35a4d8fe572@mail.gmail.com>
	<40e66e0b0808181221p40183858y64a9cedfddd24da3@mail.gmail.com>
	<24c96eca0808181241xff27e26i2063193962852726@mail.gmail.com>
Message-ID: <18602.28526.464291.817178@stat.math.ethz.ch>

>>>>> "AM" == Aaron Mackey <ajmackey at gmail.com>
>>>>>     on Mon, 18 Aug 2008 15:41:20 -0400 writes:

    AM> Dear Dr. Bates, Thanks for the info about the SIG on
    AM> mixed models.

    AM> A followup question: if I did not want a generalized
    AM> variance-covariance matrix, and instead wanted to assume
    AM> independence between grouped covariates, then instead of
    AM> (3n-n^2)/2 variance-covariance parameters per grouping
    AM> (of size n), I'd have only n variance parameters, yes?
    AM> Would that be specified (x1|fac) + (x2|fac) instead of
    AM> (x1 + x2 | fac)?

yes.

    AM> And either way I formulate it, my syntax question
    AM> remains: is there a way to write the formula that is
    AM> generic for a given covariate matrix of unspecified
    AM> dimension?  Said another way, how would you write a lmer
    AM> "wrapper" function that was given a matrix of
    AM> covariates?  How would you "unpack" the columns of that
    AM> matrix to generate the formula required for lmer()?
    AM> Would you build up a formula string programmatically,
    AM> and then "eval" it?

yes / "kind of" :

I would build the formula programmatically
but then use  as.formula(.)  or possibly in some cases  parse().

Package 'sfsmisc' has a utility  wrapFormula() that I had need
for at some time:

----------------------------------------

install.packages("sfsmisc")
library(sfsmisc)

myF <- wrapFormula(Fertility ~ . , data = swiss)
myF
## Fertility ~ s(Agriculture) + s(Examination) + s(Education) + 
##             s(Catholic) + s(Infant.Mortality)

------------------------------------------

and you can look at the short definition of  wrapFormula
to see how I use gsub(.) on character strings to construct the
new formula.

Martin Maechler, ETH Zurich


    AM> Thanks again,

    AM> -Aaron

    AM> On Mon, Aug 18, 2008 at 3:21 PM, Douglas Bates
    AM> <bates at stat.wisc.edu> wrote:

    >> On Mon, Aug 18, 2008 at 11:20 AM, Aaron Mackey
    >> <ajmackey at gmail.com> wrote: > I have a fairly large
    >> model:
    >> 
    >> >> length(Y) > [1] 3051 >> dim(covariates) > [1] 3051 211
    >> 
    >> > All of these 211 covariates need to be nested
    >> hierarchically within a > grouping "class", of which
    >> there are 8.  I have an accessory vector, " > cov2class"
    >> that specifies the mapping between covariates and the 8
    >> classes.
    >> 
    >> > Now, I understand I can break all this information up
    >> into individual > vectors (cov1, cov2, ..., cov211,
    >> class1, class2, ..., class8), and do > something like
    >> this:
    >> 
    >> > model <- lmer(Y ~ 1 + cov1 + cov2 + ... + cov211 + >
    >> (cov1 + cov2 + ... | class1) + > (...) + > (... + cov210
    >> + cov211 | class8)
    >> 
    >> > But I'd like keep things syntactically simpler, and use
    >> the covariates > and cov2class > variables directly.  I
    >> haven't been able to find the right syntactic sugar > to
    >> get this done.
    >> 
    >> Before considering the syntax you should consider the
    >> complexity of the model.  Even with 3000+ observations
    >> estimating fixed effects for over 200 covariates is a
    >> risky business.  To follow that up by incorporating
    >> dozens of covariates in a vector-valued random effects
    >> term with a general variance covariance matrix is not
    >> likely to be productive.  When estimating variances and
    >> covariances of random effects the complexity of the
    >> estimation problem increases according to the square of
    >> the dimension of each random-effects vector.  For a term
    >> of the form (1|fac) there is one variance-covariance
    >> parameter to estimate.  For a term of the form (1+x|fac)
    >> there are 3 such parameters to estimate, (1+x1+x2|fac)
    >> requires 6, etc.  Assuming that your 211 covariates are
    >> more-or-less equally distributed among the 8 classes you
    >> would have about 27 per class which means that you have
    >> 378 variance-covariance parameters to estimate for each
    >> of your 8 classes.  Even with 3000+ observations it would
    >> be optimistic to expect to estimate that many
    >> variance-covariance parameters.
    >> 
    >> You may want to move your question to the
    >> R-SIG-Mixed-Models mailing list and follow up on the
    >> model specification there.
    >> 

	[[alternative HTML version deleted]]

    AM> _______________________________________________
    AM> R-sig-mixed-models at r-project.org mailing list
    AM> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From bates at stat.wisc.edu  Tue Aug 19 14:04:01 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 19 Aug 2008 07:04:01 -0500
Subject: [R-sig-ME] [R] lmer syntax, matrix of (grouped) covariates?
In-Reply-To: <18602.28526.464291.817178@stat.math.ethz.ch>
References: <24c96eca0808180920t7033f866u1287a35a4d8fe572@mail.gmail.com>
	<40e66e0b0808181221p40183858y64a9cedfddd24da3@mail.gmail.com>
	<24c96eca0808181241xff27e26i2063193962852726@mail.gmail.com>
	<18602.28526.464291.817178@stat.math.ethz.ch>
Message-ID: <40e66e0b0808190504g6a6efb9bj63dae7fa5e5c5abe@mail.gmail.com>

On Tue, Aug 19, 2008 at 1:59 AM, Martin Maechler
<maechler at stat.math.ethz.ch> wrote:
>>>>>> "AM" == Aaron Mackey <ajmackey at gmail.com>
>>>>>>     on Mon, 18 Aug 2008 15:41:20 -0400 writes:
>
>    AM> Dear Dr. Bates, Thanks for the info about the SIG on
>    AM> mixed models.
>
>    AM> A followup question: if I did not want a generalized
>    AM> variance-covariance matrix, and instead wanted to assume
>    AM> independence between grouped covariates, then instead of
>    AM> (3n-n^2)/2 variance-covariance parameters per grouping
>    AM> (of size n), I'd have only n variance parameters, yes?
>    AM> Would that be specified (x1|fac) + (x2|fac) instead of
>    AM> (x1 + x2 | fac)?
>
> yes.

For lmer I think you would want to write that as (0+x1|fac) + (0+x2|fac) + ...

By default a linear model formula includes the intercept term so that
(x1|fac) is equivalent to (1+x1|fac).  If you do not suppress the
implicit intercept you end up with multiple terms including it.

>    AM> And either way I formulate it, my syntax question
>    AM> remains: is there a way to write the formula that is
>    AM> generic for a given covariate matrix of unspecified
>    AM> dimension?  Said another way, how would you write a lmer
>    AM> "wrapper" function that was given a matrix of
>    AM> covariates?  How would you "unpack" the columns of that
>    AM> matrix to generate the formula required for lmer()?
>    AM> Would you build up a formula string programmatically,
>    AM> and then "eval" it?
>
> yes / "kind of" :
>
> I would build the formula programmatically
> but then use  as.formula(.)  or possibly in some cases  parse().
>
> Package 'sfsmisc' has a utility  wrapFormula() that I had need
> for at some time:
>
> ----------------------------------------
>
> install.packages("sfsmisc")
> library(sfsmisc)
>
> myF <- wrapFormula(Fertility ~ . , data = swiss)
> myF
> ## Fertility ~ s(Agriculture) + s(Examination) + s(Education) +
> ##             s(Catholic) + s(Infant.Mortality)
>
> ------------------------------------------
>
> and you can look at the short definition of  wrapFormula
> to see how I use gsub(.) on character strings to construct the
> new formula.
>
> Martin Maechler, ETH Zurich
>
>
>    AM> Thanks again,
>
>    AM> -Aaron
>
>    AM> On Mon, Aug 18, 2008 at 3:21 PM, Douglas Bates
>    AM> <bates at stat.wisc.edu> wrote:
>
>    >> On Mon, Aug 18, 2008 at 11:20 AM, Aaron Mackey
>    >> <ajmackey at gmail.com> wrote: > I have a fairly large
>    >> model:
>    >>
>    >> >> length(Y) > [1] 3051 >> dim(covariates) > [1] 3051 211
>    >>
>    >> > All of these 211 covariates need to be nested
>    >> hierarchically within a > grouping "class", of which
>    >> there are 8.  I have an accessory vector, " > cov2class"
>    >> that specifies the mapping between covariates and the 8
>    >> classes.
>    >>
>    >> > Now, I understand I can break all this information up
>    >> into individual > vectors (cov1, cov2, ..., cov211,
>    >> class1, class2, ..., class8), and do > something like
>    >> this:
>    >>
>    >> > model <- lmer(Y ~ 1 + cov1 + cov2 + ... + cov211 + >
>    >> (cov1 + cov2 + ... | class1) + > (...) + > (... + cov210
>    >> + cov211 | class8)
>    >>
>    >> > But I'd like keep things syntactically simpler, and use
>    >> the covariates > and cov2class > variables directly.  I
>    >> haven't been able to find the right syntactic sugar > to
>    >> get this done.
>    >>
>    >> Before considering the syntax you should consider the
>    >> complexity of the model.  Even with 3000+ observations
>    >> estimating fixed effects for over 200 covariates is a
>    >> risky business.  To follow that up by incorporating
>    >> dozens of covariates in a vector-valued random effects
>    >> term with a general variance covariance matrix is not
>    >> likely to be productive.  When estimating variances and
>    >> covariances of random effects the complexity of the
>    >> estimation problem increases according to the square of
>    >> the dimension of each random-effects vector.  For a term
>    >> of the form (1|fac) there is one variance-covariance
>    >> parameter to estimate.  For a term of the form (1+x|fac)
>    >> there are 3 such parameters to estimate, (1+x1+x2|fac)
>    >> requires 6, etc.  Assuming that your 211 covariates are
>    >> more-or-less equally distributed among the 8 classes you
>    >> would have about 27 per class which means that you have
>    >> 378 variance-covariance parameters to estimate for each
>    >> of your 8 classes.  Even with 3000+ observations it would
>    >> be optimistic to expect to estimate that many
>    >> variance-covariance parameters.
>    >>
>    >> You may want to move your question to the
>    >> R-SIG-Mixed-Models mailing list and follow up on the
>    >> model specification there.
>    >>
>
>        [[alternative HTML version deleted]]
>
>    AM> _______________________________________________
>    AM> R-sig-mixed-models at r-project.org mailing list
>    AM> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From danielezrajohnson at gmail.com  Tue Aug 19 22:47:15 2008
From: danielezrajohnson at gmail.com (Daniel Ezra Johnson)
Date: Tue, 19 Aug 2008 21:47:15 +0100
Subject: [R-sig-ME] understanding log-likelihood/model fit
Message-ID: <a46630750808191347v43e2828s1c36e504d8387a78@mail.gmail.com>

Dear All,

I'm sure this is a simple question, but I haven't been able to find an
answer to it that I can understand. I'd like an answer that is pitched
somewhere between the full mathematics on the one hand, and an
oversimplified overview on the other.

One way of putting the question is, what is the difference, really,
between a fixed and a random effect (as they are fit by lmer)?
Another way of putting it involves the following example.

Suppose we have observations of a response from many subjects.
The overall average response is 500.
The subjects fall into two groups.
Half have an effect of +100 and half an effect of -100.

test1 <- data.frame(subject=rep(1:200,each=500),
	response=500+c(rep(-100,50000),rep(100,50000))+rnorm(100000,0,10),
	fixed=(rep(c("A","B"),each=50000)))

The following model treats subject as a random effect:
> null <- lmer(response~(1|subject),test1)

The following model keeps the subject effect and adds the fixed effect.
> fixed <- lmer(response~fixed+(1|subject),test1)

> null
Linear mixed model fit by REML
Formula: response ~ (1 | subject)
   Data: test1
    AIC    BIC  logLik deviance REMLdev
 746923 746951 -373458   746923  746917
Random effects:
 Groups   Name        Variance Std.Dev.
 subject  (Intercept) 10041.81 100.209
 Residual               100.46  10.023
Number of obs: 100000, groups: subject, 200
Fixed effects:
            Estimate Std. Error t value
(Intercept)  500.000      7.086   70.56

> fixed
Linear mixed model fit by REML
Formula: response ~ fixed + (1 | subject)
   Data: test1
    AIC    BIC  logLik deviance REMLdev
 743977 744015 -371984   743960  743969
Random effects:
 Groups   Name        Variance  Std.Dev.
 subject  (Intercept)  0.016654 0.12905
 Residual             99.642120 9.98209
Number of obs: 100000, groups: subject, 200
Fixed effects:
             Estimate Std. Error t value
(Intercept) 400.11806    0.04647    8610
fixedB      199.87485    0.06572    3041

The result is what one would expect, intuitively.
In the model "null" there is a large subject variance.
In the model "fixed" there is virtually no subject variance.
In both models the residuals are the same.
The logLik of the model with the fixed effect is closer to zero (by about 1500).
Therefore, we say the model with the fixed effect fits better.

This makes sense. Instead of 100 subject effects near +100 and 100
near -100, we have virtually no subject effects and the fixed effect
accounts for all the between-subject variance.

The question: why? Why does the model with the fixed effect fit better?
Why does the smaller (zero) random effect plus the fixed effect
translate into an improvement in log-likelihood?

It's not anything to do with the residuals. The models make the same
predictions:

> fitted(null)[c(1:5,50001:50005)]
 [1] 400.2807 400.2807 400.2807 400.2807 400.2807 600.2013 600.2013 600.2013
 [9] 600.2013 600.2013

> fitted(fixed)[c(1:5,50001:50005)]
 [1] 400.1282 400.1282 400.1282 400.1282 400.1282 599.9839 599.9839 599.9839
 [9] 599.9839 599.9839

And I don't think it has anything to do with the extreme non-normality
of the random effects in "null" as opposed to "fixed".

So what's the difference?

What, in terms of model fitting, makes it preferable to account for
the between-subject variation with a fixed effect (as in "fixed")
rather than with a random effect (as in "null")?

Thanks for your help,
Daniel



From john.maindonald at anu.edu.au  Wed Aug 20 02:02:16 2008
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Wed, 20 Aug 2008 10:02:16 +1000
Subject: [R-sig-ME] understanding log-likelihood/model fit
In-Reply-To: <a46630750808191347v43e2828s1c36e504d8387a78@mail.gmail.com>
References: <a46630750808191347v43e2828s1c36e504d8387a78@mail.gmail.com>
Message-ID: <3278ABDD-0E3B-4284-9266-593AB38CF043@anu.edu.au>

What you observe has everything to do with
'the extreme non-normality of the random effects in "null" as opposed  
to "fixed"'.

The distribution at the subject level is nowhere near normal.
The fixed effects account for a rather severe departure from normality.
For this reason, the between subjects components of variance
estimate is radically different between the two models.  Also the
subjects random effects are radically different.  Try

znull <- ranef(null, drop=TRUE)
zfix <- ranef(fixed, drop=TRUE)
plot(zfix$subject ~ znull$subject)

The residuals for the two models are simiiar, but not at all the same.
The residuals from the null model are surprisingly close to normal.
[Actually, a criticism of the Pinheiro and Bates book is that it relies
far too heavily on plots of residuals for its diagnostic checking.]

Think about how the results might be used.  The analyses that you
give seem to imply that the existence of the two groups is hidden
from you.  You want to generalize to other subjects (the usual
situation), so that fitting subject as a fixed effect would defeat the
whole aim of the enterprise.  So, unless you learn of the existence
of the two groups from initial exploratory analysis of the data (you
darn well should), you have to do the null analysis.

At this late stage you might examine the random subject effects
and belatedly conclude that you have been laboring under a gross
misapprehension.  Or you might charge blindly ahead and use the
model for predictive purposes.  In the latter case, the accuracy will
be terrible (high SE), but, hey, you can make predictions on what to
expect from another subject or group of subjects.

There are strong reasons for trying to ensure that models (1) are
somewhat near correct, and (2) that they model aspects of the data
that reflect the intended use of the results.

Now try
test2 <- data.frame(subject=rep(1:200,each=500),
	response=500+rep(rnorm(200), each=500)+rnorm(100000,0,10),
	fixed=(rep(c("A","B"),each=50000)))

lmer(response~ (1|subject), data=test2)
lmer(response~ fixed+(1|subject), data=test2)

Wrestling with such simulated data is a good way to gain
understanding and tune one's intuitive processes.

By the way, I believe you should be fitting by maximum likelihood
(ML), for comparing models with different fixed effects.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.


On 20 Aug 2008, at 6:47 AM, Daniel Ezra Johnson wrote:

> Dear All,
>
> I'm sure this is a simple question, but I haven't been able to find an
> answer to it that I can understand. I'd like an answer that is pitched
> somewhere between the full mathematics on the one hand, and an
> oversimplified overview on the other.
>
> One way of putting the question is, what is the difference, really,
> between a fixed and a random effect (as they are fit by lmer)?
> Another way of putting it involves the following example.
>
> Suppose we have observations of a response from many subjects.
> The overall average response is 500.
> The subjects fall into two groups.
> Half have an effect of +100 and half an effect of -100.
>
> test1 <- data.frame(subject=rep(1:200,each=500),
> 	response=500+c(rep(-100,50000),rep(100,50000))+rnorm(100000,0,10),
> 	fixed=(rep(c("A","B"),each=50000)))
>
> The following model treats subject as a random effect:
>> null <- lmer(response~(1|subject),test1)
>
> The following model keeps the subject effect and adds the fixed  
> effect.
>> fixed <- lmer(response~fixed+(1|subject),test1)
>
>> null
> Linear mixed model fit by REML
> Formula: response ~ (1 | subject)
>  Data: test1
>   AIC    BIC  logLik deviance REMLdev
> 746923 746951 -373458   746923  746917
> Random effects:
> Groups   Name        Variance Std.Dev.
> subject  (Intercept) 10041.81 100.209
> Residual               100.46  10.023
> Number of obs: 100000, groups: subject, 200
> Fixed effects:
>           Estimate Std. Error t value
> (Intercept)  500.000      7.086   70.56
>
>> fixed
> Linear mixed model fit by REML
> Formula: response ~ fixed + (1 | subject)
>  Data: test1
>   AIC    BIC  logLik deviance REMLdev
> 743977 744015 -371984   743960  743969
> Random effects:
> Groups   Name        Variance  Std.Dev.
> subject  (Intercept)  0.016654 0.12905
> Residual             99.642120 9.98209
> Number of obs: 100000, groups: subject, 200
> Fixed effects:
>            Estimate Std. Error t value
> (Intercept) 400.11806    0.04647    8610
> fixedB      199.87485    0.06572    3041
>
> The result is what one would expect, intuitively.
> In the model "null" there is a large subject variance.
> In the model "fixed" there is virtually no subject variance.
> In both models the residuals are the same.
> The logLik of the model with the fixed effect is closer to zero (by  
> about 1500).
> Therefore, we say the model with the fixed effect fits better.
>
> This makes sense. Instead of 100 subject effects near +100 and 100
> near -100, we have virtually no subject effects and the fixed effect
> accounts for all the between-subject variance.
>
> The question: why? Why does the model with the fixed effect fit  
> better?
> Why does the smaller (zero) random effect plus the fixed effect
> translate into an improvement in log-likelihood?
>
> It's not anything to do with the residuals. The models make the same
> predictions:
>
>> fitted(null)[c(1:5,50001:50005)]
> [1] 400.2807 400.2807 400.2807 400.2807 400.2807 600.2013 600.2013  
> 600.2013
> [9] 600.2013 600.2013
>
>> fitted(fixed)[c(1:5,50001:50005)]
> [1] 400.1282 400.1282 400.1282 400.1282 400.1282 599.9839 599.9839  
> 599.9839
> [9] 599.9839 599.9839
>
> And I don't think it has anything to do with the extreme non-normality
> of the random effects in "null" as opposed to "fixed".
>
> So what's the difference?
>
> What, in terms of model fitting, makes it preferable to account for
> the between-subject variation with a fixed effect (as in "fixed")
> rather than with a random effect (as in "null")?
>
> Thanks for your help,
> Daniel
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From John.Maindonald at anu.edu.au  Wed Aug 20 02:25:31 2008
From: John.Maindonald at anu.edu.au (John Maindonald)
Date: Wed, 20 Aug 2008 10:25:31 +1000
Subject: [R-sig-ME] understanding log-likelihood/model fit
In-Reply-To: <a46630750808191719h31543d6ep5a91b2ca6019062c@mail.gmail.com>
References: <a46630750808191347v43e2828s1c36e504d8387a78@mail.gmail.com>
	<3278ABDD-0E3B-4284-9266-593AB38CF043@anu.edu.au>
	<a46630750808191719h31543d6ep5a91b2ca6019062c@mail.gmail.com>
Message-ID: <D6B0DBE2-05A5-419D-B61D-43D296E64DCE@anu.edu.au>


On 20 Aug 2008, at 10:19 AM, Daniel Ezra Johnson wrote:
> Thanks for your reply.
>
>> What you observe has everything to do with
>> 'the extreme non-normality of the random effects in "null" as  
>> opposed to
>> "fixed"'.
>
> I thought so too, but it's not correct. The log-likelihood of the
> mixed-effects model does not appear to depend on how nearly normal, or
> not, the random effect BLUPs are.

Let's see your example.  If you change the data, however, you will
change the log-likelihood.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.

> That was what led to my original question. When we have a random
> effect (subject) and a between-group fixed effect (an "outer" effect),
> logically they are competing to account for the same variance.
>
> The mixed model fitting appears to "prefer" the model with the fixed
> effect, even though the model with no fixed effect appears to fit the
> data equally well (judging by residuals and the fact that the BLUPs
> are exactly what they 'should be', no shrinkage here).
>
> I am comfortable with this result, indeed my work depends on it, but I
> want to understand better why it comes out this way. This has nothing
> to do with practical research design questions.
>
> D
>>
>> The distribution at the subject level is nowhere near normal.
>> The fixed effects account for a rather severe departure from  
>> normality.
>> For this reason, the between subjects components of variance
>> estimate is radically different between the two models.  Also the
>> subjects random effects are radically different.  Try
>>
>> znull <- ranef(null, drop=TRUE)
>> zfix <- ranef(fixed, drop=TRUE)
>> plot(zfix$subject ~ znull$subject)
>>
>> The residuals for the two models are simiiar, but not at all the  
>> same.
>> The residuals from the null model are surprisingly close to normal.
>> [Actually, a criticism of the Pinheiro and Bates book is that it  
>> relies
>> far too heavily on plots of residuals for its diagnostic checking.]
>>
>> Think about how the results might be used.  The analyses that you
>> give seem to imply that the existence of the two groups is hidden
>> from you.  You want to generalize to other subjects (the usual
>> situation), so that fitting subject as a fixed effect would defeat  
>> the
>> whole aim of the enterprise.  So, unless you learn of the existence
>> of the two groups from initial exploratory analysis of the data (you
>> darn well should), you have to do the null analysis.
>>
>> At this late stage you might examine the random subject effects
>> and belatedly conclude that you have been laboring under a gross
>> misapprehension.  Or you might charge blindly ahead and use the
>> model for predictive purposes.  In the latter case, the accuracy will
>> be terrible (high SE), but, hey, you can make predictions on what to
>> expect from another subject or group of subjects.
>>
>> There are strong reasons for trying to ensure that models (1) are
>> somewhat near correct, and (2) that they model aspects of the data
>> that reflect the intended use of the results.
>>
>> Now try
>> test2 <- data.frame(subject=rep(1:200,each=500),
>>       response=500+rep(rnorm(200), each=500)+rnorm(100000,0,10),
>>       fixed=(rep(c("A","B"),each=50000)))
>>
>> lmer(response~ (1|subject), data=test2)
>> lmer(response~ fixed+(1|subject), data=test2)
>>
>> Wrestling with such simulated data is a good way to gain
>> understanding and tune one's intuitive processes.
>>
>> By the way, I believe you should be fitting by maximum likelihood
>> (ML), for comparing models with different fixed effects.
>>
>> John Maindonald             email: john.maindonald at anu.edu.au
>> phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
>> Centre for Mathematics & Its Applications, Room 1194,
>> John Dedman Mathematical Sciences Building (Building 27)
>> Australian National University, Canberra ACT 0200.
>>
>>
>> On 20 Aug 2008, at 6:47 AM, Daniel Ezra Johnson wrote:
>>
>>> Dear All,
>>>
>>> I'm sure this is a simple question, but I haven't been able to  
>>> find an
>>> answer to it that I can understand. I'd like an answer that is  
>>> pitched
>>> somewhere between the full mathematics on the one hand, and an
>>> oversimplified overview on the other.
>>>
>>> One way of putting the question is, what is the difference, really,
>>> between a fixed and a random effect (as they are fit by lmer)?
>>> Another way of putting it involves the following example.
>>>
>>> Suppose we have observations of a response from many subjects.
>>> The overall average response is 500.
>>> The subjects fall into two groups.
>>> Half have an effect of +100 and half an effect of -100.
>>>
>>> test1 <- data.frame(subject=rep(1:200,each=500),
>>>       response=500+c(rep(-100,50000),rep(100,50000)) 
>>> +rnorm(100000,0,10),
>>>       fixed=(rep(c("A","B"),each=50000)))
>>>
>>> The following model treats subject as a random effect:
>>>>
>>>> null <- lmer(response~(1|subject),test1)
>>>
>>> The following model keeps the subject effect and adds the fixed  
>>> effect.
>>>>
>>>> fixed <- lmer(response~fixed+(1|subject),test1)
>>>
>>>> null
>>>
>>> Linear mixed model fit by REML
>>> Formula: response ~ (1 | subject)
>>> Data: test1
>>> AIC    BIC  logLik deviance REMLdev
>>> 746923 746951 -373458   746923  746917
>>> Random effects:
>>> Groups   Name        Variance Std.Dev.
>>> subject  (Intercept) 10041.81 100.209
>>> Residual               100.46  10.023
>>> Number of obs: 100000, groups: subject, 200
>>> Fixed effects:
>>>         Estimate Std. Error t value
>>> (Intercept)  500.000      7.086   70.56
>>>
>>>> fixed
>>>
>>> Linear mixed model fit by REML
>>> Formula: response ~ fixed + (1 | subject)
>>> Data: test1
>>> AIC    BIC  logLik deviance REMLdev
>>> 743977 744015 -371984   743960  743969
>>> Random effects:
>>> Groups   Name        Variance  Std.Dev.
>>> subject  (Intercept)  0.016654 0.12905
>>> Residual             99.642120 9.98209
>>> Number of obs: 100000, groups: subject, 200
>>> Fixed effects:
>>>          Estimate Std. Error t value
>>> (Intercept) 400.11806    0.04647    8610
>>> fixedB      199.87485    0.06572    3041
>>>
>>> The result is what one would expect, intuitively.
>>> In the model "null" there is a large subject variance.
>>> In the model "fixed" there is virtually no subject variance.
>>> In both models the residuals are the same.
>>> The logLik of the model with the fixed effect is closer to zero  
>>> (by about
>>> 1500).
>>> Therefore, we say the model with the fixed effect fits better.
>>>
>>> This makes sense. Instead of 100 subject effects near +100 and 100
>>> near -100, we have virtually no subject effects and the fixed effect
>>> accounts for all the between-subject variance.
>>>
>>> The question: why? Why does the model with the fixed effect fit  
>>> better?
>>> Why does the smaller (zero) random effect plus the fixed effect
>>> translate into an improvement in log-likelihood?
>>>
>>> It's not anything to do with the residuals. The models make the same
>>> predictions:
>>>
>>>> fitted(null)[c(1:5,50001:50005)]
>>>
>>> [1] 400.2807 400.2807 400.2807 400.2807 400.2807 600.2013 600.2013
>>> 600.2013
>>> [9] 600.2013 600.2013
>>>
>>>> fitted(fixed)[c(1:5,50001:50005)]
>>>
>>> [1] 400.1282 400.1282 400.1282 400.1282 400.1282 599.9839 599.9839
>>> 599.9839
>>> [9] 599.9839 599.9839
>>>
>>> And I don't think it has anything to do with the extreme non- 
>>> normality
>>> of the random effects in "null" as opposed to "fixed".
>>>
>>> So what's the difference?
>>>
>>> What, in terms of model fitting, makes it preferable to account for
>>> the between-subject variation with a fixed effect (as in "fixed")
>>> rather than with a random effect (as in "null")?
>>>
>>> Thanks for your help,
>>> Daniel
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>



From A.Robinson at ms.unimelb.edu.au  Wed Aug 20 02:35:06 2008
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Wed, 20 Aug 2008 10:35:06 +1000
Subject: [R-sig-ME] Diagnostics (was: understanding log-likelihood/model fit)
In-Reply-To: <3278ABDD-0E3B-4284-9266-593AB38CF043@anu.edu.au>
References: <a46630750808191347v43e2828s1c36e504d8387a78@mail.gmail.com>
	<3278ABDD-0E3B-4284-9266-593AB38CF043@anu.edu.au>
Message-ID: <20080820003506.GF63846@ms.unimelb.edu.au>

On Wed, Aug 20, 2008 at 10:02:16AM +1000, John Maindonald wrote:

> [Actually, a criticism of the Pinheiro and Bates book is that it relies
> far too heavily on plots of residuals for its diagnostic checking.]
> 

John, can you expand on this point?  

Warm wishes,

Andrew

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-6410
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/



From smckinney at bccrc.ca  Wed Aug 20 03:43:24 2008
From: smckinney at bccrc.ca (Steven McKinney)
Date: Tue, 19 Aug 2008 18:43:24 -0700
Subject: [R-sig-ME] understanding log-likelihood/model fit
References: <a46630750808191347v43e2828s1c36e504d8387a78@mail.gmail.com><3278ABDD-0E3B-4284-9266-593AB38CF043@anu.edu.au><a46630750808191719h31543d6ep5a91b2ca6019062c@mail.gmail.com>
	<D6B0DBE2-05A5-419D-B61D-43D296E64DCE@anu.edu.au>
Message-ID: <0BE438149FF2254DB4199E2682C8DFEB0328A406@crcmail1.BCCRC.CA>


I prefer smaller data sets so I can
more readily look at all components;
here is my modification of the modeling
(along with a random seed so we
all get the same data)

You can change the parameter values
for big data sets.


Nsubj <- 10
Ngrp <- 2
NsubjRep <- 5

set.seed(123)

test1s <- data.frame(subject = rep(seq(Nsubj * Ngrp), each = NsubjRep),
        response=500+c(rep(-100,Nsubj * NsubjRep),rep(100,Nsubj * NsubjRep))+rnorm(Nsubj * Ngrp * NsubjRep, 0, 10),
        fixed=(rep(c("A","B"),each=Nsubj * NsubjRep)))

null1 <- lmer(response~(1|subject),test1s)
fixed1 <- lmer(response~fixed+(1|subject),test1s)
summary(null1)
summary(fixed1)

quartz() ## substitute your favourite plot device 
par(mfrow = c(2, 1))
hist(ranef(null1)[[1]][,1])
hist(ranef(fixed1)[[1]][,1])

quartz() ## substitute your favourite plot device 
par(mfrow = c(2, 1))
qqnorm(ranef(null1)[[1]][,1])
qqline(ranef(null1)[[1]][,1])
qqnorm(ranef(fixed1)[[1]][,1])
qqline(ranef(fixed1)[[1]][,1])

### -----

Look at the histogram of BLUPs for model null1.
The strong bimodal structure is a clue that
there may be an underlying fixed effect
(and in fact we know that this is the case,
the beauty of simulations).

The QQ plot of the null1 model also
hints that subjects are not just some
selection from a normal distribution.

The fixed effect (see summary(fixed1)
output) shows a huge t value and the
random effect estimates are near zero
(you'll see this message during the
fitting process too)

Warning message:
In .local(x, ..., value) :
  Estimated variance for factor ?subject? is effectively zero

All of this suggests that a fixed effect (covariate 'fixed')
is appropriate to explain the structure
in the data, and subject need not be modeled
as a random effect.  A linear model without
random effects would suffice here.


Now to run John Maindonald's model
with subjects being drawn from a
normal distribution:

### -----

set.seed(123)
test2s <- data.frame(subject = rep(seq(Nsubj * Ngrp), each = NsubjRep),
        response=500 + rep(rnorm(Nsubj * Ngrp), each = NsubjRep) + rnorm(Nsubj * Ngrp * NsubjRep, 0, 10),
        fixed=(rep(c("A","B"),each=Nsubj * NsubjRep)))

null2 <- lmer(response~(1|subject),test2s)
fixed2 <- lmer(response~fixed+(1|subject),test2s)
summary(null2)
summary(fixed2)

quartz() ## substitute your favourite plot device 
par(mfrow = c(2, 1))
hist(ranef(null2)[[1]][,1])
hist(ranef(fixed2)[[1]][,1])

quartz() ## substitute your favourite plot device 
par(mfrow = c(2, 1))
qqnorm(ranef(null2)[[1]][,1])
qqline(ranef(null2)[[1]][,1])
qqnorm(ranef(fixed2)[[1]][,1])
qqline(ranef(fixed2)[[1]][,1])

### -----

Now the histogram of null2 BLUPs looks
like a unimodal density, and the
QQ plot suggests BLUPs are closer
to normal.  This is much more in line
with the situation that random effects 
components were designed to handle.

The fixed effect in fixed2 (see
summary(fixed2) output) does not
contribute to the model fit, as indicated
by the small t-value, and the random effect 
estimates are not all small numbers near 
zero.  A random effects model with subject
as a random effect is appropriate
here.


So this is a partial look into the
difference between fixed and random
effects.  

A covariate that is more 'degenerate', 
i.e. takes on a few distinct values, so 
has a 'spiky' distribution, is often better
modeled as a fixed effect.

When a covariate really can be imagined
to be a draw from some large non-degenerate
or non-spiky distribution, and you are
not really interested in the individual
values it can assume, but you want to
'model out' its effect (so its effect doesn't
cause some other covariate estimate to be
dragged off course) you can save
model degrees of freedom by modeling
that covariate as a random effect. 

Comparing likelihoods and other
statistics derived therefrom will
guide you in the right direction,
as these simulations illustrate.
The likelihoods do 'depend' on
the structure in the data.  
Statistics work!
 


HTH

Steven McKinney

Statistician
Molecular Oncology and Breast Cancer Program
British Columbia Cancer Research Centre

email: smckinney +at+ bccrc +dot+ ca

tel: 604-675-8000 x7561

BCCRC
Molecular Oncology
675 West 10th Ave, Floor 4
Vancouver B.C. 
V5Z 1L3
Canada




-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org on behalf of John Maindonald
Sent: Tue 8/19/2008 5:25 PM
To: Daniel Ezra Johnson
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] understanding log-likelihood/model fit
 

On 20 Aug 2008, at 10:19 AM, Daniel Ezra Johnson wrote:
> Thanks for your reply.
>
>> What you observe has everything to do with
>> 'the extreme non-normality of the random effects in "null" as  
>> opposed to
>> "fixed"'.
>
> I thought so too, but it's not correct. The log-likelihood of the
> mixed-effects model does not appear to depend on how nearly normal, or
> not, the random effect BLUPs are.

Let's see your example.  If you change the data, however, you will
change the log-likelihood.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.

> That was what led to my original question. When we have a random
> effect (subject) and a between-group fixed effect (an "outer" effect),
> logically they are competing to account for the same variance.
>
> The mixed model fitting appears to "prefer" the model with the fixed
> effect, even though the model with no fixed effect appears to fit the
> data equally well (judging by residuals and the fact that the BLUPs
> are exactly what they 'should be', no shrinkage here).
>
> I am comfortable with this result, indeed my work depends on it, but I
> want to understand better why it comes out this way. This has nothing
> to do with practical research design questions.
>
> D
>>
>> The distribution at the subject level is nowhere near normal.
>> The fixed effects account for a rather severe departure from  
>> normality.
>> For this reason, the between subjects components of variance
>> estimate is radically different between the two models.  Also the
>> subjects random effects are radically different.  Try
>>
>> znull <- ranef(null, drop=TRUE)
>> zfix <- ranef(fixed, drop=TRUE)
>> plot(zfix$subject ~ znull$subject)
>>
>> The residuals for the two models are simiiar, but not at all the  
>> same.
>> The residuals from the null model are surprisingly close to normal.
>> [Actually, a criticism of the Pinheiro and Bates book is that it  
>> relies
>> far too heavily on plots of residuals for its diagnostic checking.]
>>
>> Think about how the results might be used.  The analyses that you
>> give seem to imply that the existence of the two groups is hidden
>> from you.  You want to generalize to other subjects (the usual
>> situation), so that fitting subject as a fixed effect would defeat  
>> the
>> whole aim of the enterprise.  So, unless you learn of the existence
>> of the two groups from initial exploratory analysis of the data (you
>> darn well should), you have to do the null analysis.
>>
>> At this late stage you might examine the random subject effects
>> and belatedly conclude that you have been laboring under a gross
>> misapprehension.  Or you might charge blindly ahead and use the
>> model for predictive purposes.  In the latter case, the accuracy will
>> be terrible (high SE), but, hey, you can make predictions on what to
>> expect from another subject or group of subjects.
>>
>> There are strong reasons for trying to ensure that models (1) are
>> somewhat near correct, and (2) that they model aspects of the data
>> that reflect the intended use of the results.
>>
>> Now try
>> test2 <- data.frame(subject=rep(1:200,each=500),
>>       response=500+rep(rnorm(200), each=500)+rnorm(100000,0,10),
>>       fixed=(rep(c("A","B"),each=50000)))
>>
>> lmer(response~ (1|subject), data=test2)
>> lmer(response~ fixed+(1|subject), data=test2)
>>
>> Wrestling with such simulated data is a good way to gain
>> understanding and tune one's intuitive processes.
>>
>> By the way, I believe you should be fitting by maximum likelihood
>> (ML), for comparing models with different fixed effects.
>>
>> John Maindonald             email: john.maindonald at anu.edu.au
>> phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
>> Centre for Mathematics & Its Applications, Room 1194,
>> John Dedman Mathematical Sciences Building (Building 27)
>> Australian National University, Canberra ACT 0200.
>>
>>
>> On 20 Aug 2008, at 6:47 AM, Daniel Ezra Johnson wrote:
>>
>>> Dear All,
>>>
>>> I'm sure this is a simple question, but I haven't been able to  
>>> find an
>>> answer to it that I can understand. I'd like an answer that is  
>>> pitched
>>> somewhere between the full mathematics on the one hand, and an
>>> oversimplified overview on the other.
>>>
>>> One way of putting the question is, what is the difference, really,
>>> between a fixed and a random effect (as they are fit by lmer)?
>>> Another way of putting it involves the following example.
>>>
>>> Suppose we have observations of a response from many subjects.
>>> The overall average response is 500.
>>> The subjects fall into two groups.
>>> Half have an effect of +100 and half an effect of -100.
>>>
>>> test1 <- data.frame(subject=rep(1:200,each=500),
>>>       response=500+c(rep(-100,50000),rep(100,50000)) 
>>> +rnorm(100000,0,10),
>>>       fixed=(rep(c("A","B"),each=50000)))
>>>
>>> The following model treats subject as a random effect:
>>>>
>>>> null <- lmer(response~(1|subject),test1)
>>>
>>> The following model keeps the subject effect and adds the fixed  
>>> effect.
>>>>
>>>> fixed <- lmer(response~fixed+(1|subject),test1)
>>>
>>>> null
>>>
>>> Linear mixed model fit by REML
>>> Formula: response ~ (1 | subject)
>>> Data: test1
>>> AIC    BIC  logLik deviance REMLdev
>>> 746923 746951 -373458   746923  746917
>>> Random effects:
>>> Groups   Name        Variance Std.Dev.
>>> subject  (Intercept) 10041.81 100.209
>>> Residual               100.46  10.023
>>> Number of obs: 100000, groups: subject, 200
>>> Fixed effects:
>>>         Estimate Std. Error t value
>>> (Intercept)  500.000      7.086   70.56
>>>
>>>> fixed
>>>
>>> Linear mixed model fit by REML
>>> Formula: response ~ fixed + (1 | subject)
>>> Data: test1
>>> AIC    BIC  logLik deviance REMLdev
>>> 743977 744015 -371984   743960  743969
>>> Random effects:
>>> Groups   Name        Variance  Std.Dev.
>>> subject  (Intercept)  0.016654 0.12905
>>> Residual             99.642120 9.98209
>>> Number of obs: 100000, groups: subject, 200
>>> Fixed effects:
>>>          Estimate Std. Error t value
>>> (Intercept) 400.11806    0.04647    8610
>>> fixedB      199.87485    0.06572    3041
>>>
>>> The result is what one would expect, intuitively.
>>> In the model "null" there is a large subject variance.
>>> In the model "fixed" there is virtually no subject variance.
>>> In both models the residuals are the same.
>>> The logLik of the model with the fixed effect is closer to zero  
>>> (by about
>>> 1500).
>>> Therefore, we say the model with the fixed effect fits better.
>>>
>>> This makes sense. Instead of 100 subject effects near +100 and 100
>>> near -100, we have virtually no subject effects and the fixed effect
>>> accounts for all the between-subject variance.
>>>
>>> The question: why? Why does the model with the fixed effect fit  
>>> better?
>>> Why does the smaller (zero) random effect plus the fixed effect
>>> translate into an improvement in log-likelihood?
>>>
>>> It's not anything to do with the residuals. The models make the same
>>> predictions:
>>>
>>>> fitted(null)[c(1:5,50001:50005)]
>>>
>>> [1] 400.2807 400.2807 400.2807 400.2807 400.2807 600.2013 600.2013
>>> 600.2013
>>> [9] 600.2013 600.2013
>>>
>>>> fitted(fixed)[c(1:5,50001:50005)]
>>>
>>> [1] 400.1282 400.1282 400.1282 400.1282 400.1282 599.9839 599.9839
>>> 599.9839
>>> [9] 599.9839 599.9839
>>>
>>> And I don't think it has anything to do with the extreme non- 
>>> normality
>>> of the random effects in "null" as opposed to "fixed".
>>>
>>> So what's the difference?
>>>
>>> What, in terms of model fitting, makes it preferable to account for
>>> the between-subject variation with a fixed effect (as in "fixed")
>>> rather than with a random effect (as in "null")?
>>>
>>> Thanks for your help,
>>> Daniel
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From OleF.Christensen at agrsci.dk  Wed Aug 20 12:45:56 2008
From: OleF.Christensen at agrsci.dk (Ole Fredslund Christensen)
Date: Wed, 20 Aug 2008 12:45:56 +0200
Subject: [R-sig-ME] Extending the specification of random effects to allow
	(1|A+B).
Message-ID: <EA09C4B2B0F16E44B8F3311629493C0D03B26F1A@DJFPOST01.djf.agrsci.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080820/1510722d/attachment.pl>

From danielezrajohnson at gmail.com  Wed Aug 20 02:19:51 2008
From: danielezrajohnson at gmail.com (Daniel Ezra Johnson)
Date: Wed, 20 Aug 2008 01:19:51 +0100
Subject: [R-sig-ME] understanding log-likelihood/model fit
In-Reply-To: <3278ABDD-0E3B-4284-9266-593AB38CF043@anu.edu.au>
References: <a46630750808191347v43e2828s1c36e504d8387a78@mail.gmail.com>
	<3278ABDD-0E3B-4284-9266-593AB38CF043@anu.edu.au>
Message-ID: <a46630750808191719h31543d6ep5a91b2ca6019062c@mail.gmail.com>

Thanks for your reply.

> What you observe has everything to do with
> 'the extreme non-normality of the random effects in "null" as opposed to
> "fixed"'.

I thought so too, but it's not correct. The log-likelihood of the
mixed-effects model does not appear to depend on how nearly normal, or
not, the random effect BLUPs are.

That was what led to my original question. When we have a random
effect (subject) and a between-group fixed effect (an "outer" effect),
logically they are competing to account for the same variance.

The mixed model fitting appears to "prefer" the model with the fixed
effect, even though the model with no fixed effect appears to fit the
data equally well (judging by residuals and the fact that the BLUPs
are exactly what they 'should be', no shrinkage here).

I am comfortable with this result, indeed my work depends on it, but I
want to understand better why it comes out this way. This has nothing
to do with practical research design questions.

D
>
> The distribution at the subject level is nowhere near normal.
> The fixed effects account for a rather severe departure from normality.
> For this reason, the between subjects components of variance
> estimate is radically different between the two models.  Also the
> subjects random effects are radically different.  Try
>
> znull <- ranef(null, drop=TRUE)
> zfix <- ranef(fixed, drop=TRUE)
> plot(zfix$subject ~ znull$subject)
>
> The residuals for the two models are simiiar, but not at all the same.
> The residuals from the null model are surprisingly close to normal.
> [Actually, a criticism of the Pinheiro and Bates book is that it relies
> far too heavily on plots of residuals for its diagnostic checking.]
>
> Think about how the results might be used.  The analyses that you
> give seem to imply that the existence of the two groups is hidden
> from you.  You want to generalize to other subjects (the usual
> situation), so that fitting subject as a fixed effect would defeat the
> whole aim of the enterprise.  So, unless you learn of the existence
> of the two groups from initial exploratory analysis of the data (you
> darn well should), you have to do the null analysis.
>
> At this late stage you might examine the random subject effects
> and belatedly conclude that you have been laboring under a gross
> misapprehension.  Or you might charge blindly ahead and use the
> model for predictive purposes.  In the latter case, the accuracy will
> be terrible (high SE), but, hey, you can make predictions on what to
> expect from another subject or group of subjects.
>
> There are strong reasons for trying to ensure that models (1) are
> somewhat near correct, and (2) that they model aspects of the data
> that reflect the intended use of the results.
>
> Now try
> test2 <- data.frame(subject=rep(1:200,each=500),
>        response=500+rep(rnorm(200), each=500)+rnorm(100000,0,10),
>        fixed=(rep(c("A","B"),each=50000)))
>
> lmer(response~ (1|subject), data=test2)
> lmer(response~ fixed+(1|subject), data=test2)
>
> Wrestling with such simulated data is a good way to gain
> understanding and tune one's intuitive processes.
>
> By the way, I believe you should be fitting by maximum likelihood
> (ML), for comparing models with different fixed effects.
>
> John Maindonald             email: john.maindonald at anu.edu.au
> phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
> Centre for Mathematics & Its Applications, Room 1194,
> John Dedman Mathematical Sciences Building (Building 27)
> Australian National University, Canberra ACT 0200.
>
>
> On 20 Aug 2008, at 6:47 AM, Daniel Ezra Johnson wrote:
>
>> Dear All,
>>
>> I'm sure this is a simple question, but I haven't been able to find an
>> answer to it that I can understand. I'd like an answer that is pitched
>> somewhere between the full mathematics on the one hand, and an
>> oversimplified overview on the other.
>>
>> One way of putting the question is, what is the difference, really,
>> between a fixed and a random effect (as they are fit by lmer)?
>> Another way of putting it involves the following example.
>>
>> Suppose we have observations of a response from many subjects.
>> The overall average response is 500.
>> The subjects fall into two groups.
>> Half have an effect of +100 and half an effect of -100.
>>
>> test1 <- data.frame(subject=rep(1:200,each=500),
>>        response=500+c(rep(-100,50000),rep(100,50000))+rnorm(100000,0,10),
>>        fixed=(rep(c("A","B"),each=50000)))
>>
>> The following model treats subject as a random effect:
>>>
>>> null <- lmer(response~(1|subject),test1)
>>
>> The following model keeps the subject effect and adds the fixed effect.
>>>
>>> fixed <- lmer(response~fixed+(1|subject),test1)
>>
>>> null
>>
>> Linear mixed model fit by REML
>> Formula: response ~ (1 | subject)
>>  Data: test1
>>  AIC    BIC  logLik deviance REMLdev
>> 746923 746951 -373458   746923  746917
>> Random effects:
>> Groups   Name        Variance Std.Dev.
>> subject  (Intercept) 10041.81 100.209
>> Residual               100.46  10.023
>> Number of obs: 100000, groups: subject, 200
>> Fixed effects:
>>          Estimate Std. Error t value
>> (Intercept)  500.000      7.086   70.56
>>
>>> fixed
>>
>> Linear mixed model fit by REML
>> Formula: response ~ fixed + (1 | subject)
>>  Data: test1
>>  AIC    BIC  logLik deviance REMLdev
>> 743977 744015 -371984   743960  743969
>> Random effects:
>> Groups   Name        Variance  Std.Dev.
>> subject  (Intercept)  0.016654 0.12905
>> Residual             99.642120 9.98209
>> Number of obs: 100000, groups: subject, 200
>> Fixed effects:
>>           Estimate Std. Error t value
>> (Intercept) 400.11806    0.04647    8610
>> fixedB      199.87485    0.06572    3041
>>
>> The result is what one would expect, intuitively.
>> In the model "null" there is a large subject variance.
>> In the model "fixed" there is virtually no subject variance.
>> In both models the residuals are the same.
>> The logLik of the model with the fixed effect is closer to zero (by about
>> 1500).
>> Therefore, we say the model with the fixed effect fits better.
>>
>> This makes sense. Instead of 100 subject effects near +100 and 100
>> near -100, we have virtually no subject effects and the fixed effect
>> accounts for all the between-subject variance.
>>
>> The question: why? Why does the model with the fixed effect fit better?
>> Why does the smaller (zero) random effect plus the fixed effect
>> translate into an improvement in log-likelihood?
>>
>> It's not anything to do with the residuals. The models make the same
>> predictions:
>>
>>> fitted(null)[c(1:5,50001:50005)]
>>
>> [1] 400.2807 400.2807 400.2807 400.2807 400.2807 600.2013 600.2013
>> 600.2013
>> [9] 600.2013 600.2013
>>
>>> fitted(fixed)[c(1:5,50001:50005)]
>>
>> [1] 400.1282 400.1282 400.1282 400.1282 400.1282 599.9839 599.9839
>> 599.9839
>> [9] 599.9839 599.9839
>>
>> And I don't think it has anything to do with the extreme non-normality
>> of the random effects in "null" as opposed to "fixed".
>>
>> So what's the difference?
>>
>> What, in terms of model fitting, makes it preferable to account for
>> the between-subject variation with a fixed effect (as in "fixed")
>> rather than with a random effect (as in "null")?
>>
>> Thanks for your help,
>> Daniel
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From Antonio.Gasparrini at lshtm.ac.uk  Wed Aug 20 03:49:10 2008
From: Antonio.Gasparrini at lshtm.ac.uk (Antonio.Gasparrini at lshtm.ac.uk)
Date: Wed, 20 Aug 2008 02:49:10 +0100
Subject: [R-sig-ME] bug?
Message-ID: <48AB8629.5572.00B2.0@lshtm.ac.uk>

Dear all,
 
I found a problem with 'lme4'. Basically, once you load the package 'aod' (Analysis of Overdispersed Data), the functions 'lmer' and 'glmer' don't work anymore:
 
library(lme4)
(fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
(gm1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),
              family = binomial, data = cbpp))
install.packages("aod")
library(aod)
(fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
(gm1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),
              family = binomial, data = cbpp))

Taking into account that this package is used to perform similar analyses, this could be a problem.

All the best



From bolker at zoology.ufl.edu  Wed Aug 20 15:00:07 2008
From: bolker at zoology.ufl.edu (Ben Bolker)
Date: Wed, 20 Aug 2008 09:00:07 -0400
Subject: [R-sig-ME] bug?
In-Reply-To: <48AB8629.5572.00B2.0@lshtm.ac.uk>
References: <48AB8629.5572.00B2.0@lshtm.ac.uk>
Message-ID: <48AC1557.7030107@zoology.ufl.edu>


Antonio.Gasparrini at lshtm.ac.uk wrote:
> Dear all,
>  
> I found a problem with 'lme4'. Basically, once you load the package 'aod' (Analysis of Overdispersed Data), the functions 'lmer' and 'glmer' don't work anymore:
>  
> library(lme4)
> (fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
> (gm1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),
>               family = binomial, data = cbpp))
> install.packages("aod")
> library(aod)
> (fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
> (gm1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),
>               family = binomial, data = cbpp))
> 
> Taking into account that this package is used to perform similar analyses, this could be a problem.
> 
> All the best
> 

  It looks like you already posted this to r-help and got some answers
(i.e., this is a problem with aod changing the S4 method for "AIC").

  For convenience, I sometimes use functions like this

load_lme4 <- function() {
  try(detach("package:nlme"),silent=TRUE)
  library(lme4)
}

load_nlme <- function() {
  try(detach("package:lme4"),silent=TRUE)
  library(nlme)
}

  to switch back and forth between lme4 and nlme (which also
don't play all that nicely with each other) -- you could do
the same sort of thing if you are going to use lme4 and aod
in the same R session.

  Ben Bolker



From danielezrajohnson at gmail.com  Wed Aug 20 15:01:29 2008
From: danielezrajohnson at gmail.com (Daniel Ezra Johnson)
Date: Wed, 20 Aug 2008 14:01:29 +0100
Subject: [R-sig-ME] understanding log-likelihood/model fit
In-Reply-To: <D6B0DBE2-05A5-419D-B61D-43D296E64DCE@anu.edu.au>
References: <a46630750808191347v43e2828s1c36e504d8387a78@mail.gmail.com>
	<3278ABDD-0E3B-4284-9266-593AB38CF043@anu.edu.au>
	<a46630750808191719h31543d6ep5a91b2ca6019062c@mail.gmail.com>
	<D6B0DBE2-05A5-419D-B61D-43D296E64DCE@anu.edu.au>
Message-ID: <a46630750808200601o702fc711wdd89b396e6231842@mail.gmail.com>

Everyone agrees about what happens here:

Nsubj <- 10
Ngrp <- 2
NsubjRep <- 5
set.seed(123)
test1s <- data.frame(subject = rep(seq(Nsubj * Ngrp), each = NsubjRep),
       response=500+c(rep(-100,Nsubj * NsubjRep),rep(100,Nsubj *
NsubjRep))+rnorm(Nsubj * Ngrp * NsubjRep, 0, 10),
       fixed=(rep(c("A","B"),each=Nsubj * NsubjRep)))
null1 <- lmer(response~(1|subject),test1s)
fixed1 <- lmer(response~fixed+(1|subject),test1s)

I still have two questions which I'll try to restate. I should note
that I have attempted to understand the mathematical details of ML
mixed effect model fitting and it's a bit beyond me. But I hope that
someone can provide an answer I can understand.

Question 1: When you have an "outer" fixed effect and a "subject"
random effect in the same model, specifically why does the model
(apparently) converge in such a way that the fixed effect is maximized
and the random effect minimized? (Not so much why should it, as why
does it? This is the 'fixed1' case.)

Question 2: Take the fixed1 model from Question 1 and compare it to
the null1 model, which has a random subject effect but no fixed
effect. The predicted values of the two models -- the ones from
fitted(), which include the ranefs -- are virtually the same. So why
does fixed1 have a lower deviance, why is it preferred to null1 in a
likelihood ratio test? (Again, I'm not asking why it's a better model.
I'm asking questions about the software, estimation procedure, and/or
theory of likelihood applied to such cases.)

D



From bates at stat.wisc.edu  Wed Aug 20 15:29:43 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 20 Aug 2008 08:29:43 -0500
Subject: [R-sig-ME] bug?
In-Reply-To: <48AC1557.7030107@zoology.ufl.edu>
References: <48AB8629.5572.00B2.0@lshtm.ac.uk>
	<48AC1557.7030107@zoology.ufl.edu>
Message-ID: <40e66e0b0808200629h60df0e82v1fa4ac1ffe221af1@mail.gmail.com>

Thanks for the suggestion, Ben.  It is unfortunate that nlme and lme4
don't play well with each other.  It was a design decision intended to
allow the same extractor function names to be used in both packages so
that users didn't need to learn new pseudonyms for "ranef", "fixef",
"VarCorr", etc.

With regard to the conflicting generic from the aod package, Martin
has suggested to me privately that we import the stats namespace
entirely and that seems to be the best solution.  It should appear on
R-forge packages tonight and on CRAN in a couple of days.

On Wed, Aug 20, 2008 at 8:00 AM, Ben Bolker <bolker at zoology.ufl.edu> wrote:
>
> Antonio.Gasparrini at lshtm.ac.uk wrote:
>> Dear all,
>>
>> I found a problem with 'lme4'. Basically, once you load the package 'aod' (Analysis of Overdispersed Data), the functions 'lmer' and 'glmer' don't work anymore:
>>
>> library(lme4)
>> (fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
>> (gm1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),
>>               family = binomial, data = cbpp))
>> install.packages("aod")
>> library(aod)
>> (fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
>> (gm1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),
>>               family = binomial, data = cbpp))
>>
>> Taking into account that this package is used to perform similar analyses, this could be a problem.
>>
>> All the best
>>
>
>  It looks like you already posted this to r-help and got some answers
> (i.e., this is a problem with aod changing the S4 method for "AIC").
>
>  For convenience, I sometimes use functions like this
>
> load_lme4 <- function() {
>  try(detach("package:nlme"),silent=TRUE)
>  library(lme4)
> }
>
> load_nlme <- function() {
>  try(detach("package:lme4"),silent=TRUE)
>  library(nlme)
> }
>
>  to switch back and forth between lme4 and nlme (which also
> don't play all that nicely with each other) -- you could do
> the same sort of thing if you are going to use lme4 and aod
> in the same R session.
>
>  Ben Bolker
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From chenlei at ibcas.ac.cn  Wed Aug 20 18:05:56 2008
From: chenlei at ibcas.ac.cn (=?gb2312?B?Y2hlbmxlaQ==?=)
Date: Thu, 21 Aug 2008 00:05:56 +0800 (CST)
Subject: [R-sig-ME] =?gb2312?b?UHJvYmxlbXMgaW4gdXNpbmcgbG1lciB0byBmaXQg?=
 =?gb2312?b?YSBtdWx0aWxldmVsIG1vZGVs?=
Message-ID: <48AC40E4.000005.11569@app-03>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080821/b3d943d6/attachment.pl>

From bates at stat.wisc.edu  Wed Aug 20 18:19:01 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 20 Aug 2008 11:19:01 -0500
Subject: [R-sig-ME] Problems in using lmer to fit a multilevel model
In-Reply-To: <48AC40E4.000005.11569@app-03>
References: <48AC40E4.000005.11569@app-03>
Message-ID: <40e66e0b0808200919m11788ff3qf1c6fc7b61fe0f9@mail.gmail.com>

2008/8/20 chenlei <chenlei at ibcas.ac.cn>:
> Dear??
>   when I fitted my logistic model(binary data),I received an error "Error in mer_finalize(ans, verbose) : q = > n = ". what's the matter?how can I do with this problem? was this tell me the observations were not enough to fit the model ? I appreciate if anyone who use lmer could give me some advice.


Hmm.  The message was supposed to be a bit more informative in that it
should have given the values of q and n.  I will repair that.

The value of q is the total number of random effects and the value of
n is the number of observations.  I included that check because it did
not make sense to me to try to fit more random effects than you have
observations.  I guess I could be persuaded that it would make sense
in some circumstances because the random effects are determined by a
penalized least squares optimization.

What is the nature of the model that would require it to have more
random effects than observations?



From brandon at brandoninvergo.com  Wed Aug 20 19:02:24 2008
From: brandon at brandoninvergo.com (Brandon Invergo)
Date: Wed, 20 Aug 2008 12:02:24 -0500
Subject: [R-sig-ME] possible lmer compatibility bug?
Message-ID: <48AC4E20.6050809@brandoninvergo.com>

Dear all,

I was just driving myself crazy trying to figure out this sudden error 
message I was getting on lines of code that were working fine the other 
day. Every time I tried to make a model with the lmer() function, it 
would return the following error: "Error in asMethod(object) : matrix is 
not symmetric [1,2]"

I realized, though, that the only thing that had changed was the version 
of R I was using, having just switched to version 2.7.1. Sure enough, 
when I loaded up v2.7.0, the script worked fine and I could successfully 
call lmer.

Is there something I'm possibly doing wrong that's preventing it from 
working in 2.7.1? Or is this a known problem?

Regards,
brandon



From bates at stat.wisc.edu  Wed Aug 20 19:19:29 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 20 Aug 2008 12:19:29 -0500
Subject: [R-sig-ME] possible lmer compatibility bug?
In-Reply-To: <48AC4E20.6050809@brandoninvergo.com>
References: <48AC4E20.6050809@brandoninvergo.com>
Message-ID: <40e66e0b0808201019g272e1405r608f1f0c3be08758@mail.gmail.com>

On Wed, Aug 20, 2008 at 12:02 PM, Brandon Invergo
<brandon at brandoninvergo.com> wrote:
> Dear all,

> I was just driving myself crazy trying to figure out this sudden error
> message I was getting on lines of code that were working fine the other day.
> Every time I tried to make a model with the lmer() function, it would return
> the following error: "Error in asMethod(object) : matrix is not symmetric
> [1,2]"

> I realized, though, that the only thing that had changed was the version of
> R I was using, having just switched to version 2.7.1. Sure enough, when I
> loaded up v2.7.0, the script worked fine and I could successfully call lmer.

> Is there something I'm possibly doing wrong that's preventing it from
> working in 2.7.1? Or is this a known problem?

It appears that you have too high a regard for the readers of this
list in that you expect them to have psychic powers :-)

More seriously, if you read your message and think of how a person
could possibly create a meaningful response based on the information
that you have given us, I think you will see that we don't have nearly
enough information to go on.

A reproducible example rather than a description of "lines of code
that were working fine" is what we will need before we can expect to
provide a useful answer to you.  Also, information like the output
from

sessionInfo()

and perhaps

traceback()

after the error message so we know what was going on at the time the
error occurred.  Most importantly, we need to know what the call was
that resulted in an error.



From brandon at brandoninvergo.com  Wed Aug 20 20:10:04 2008
From: brandon at brandoninvergo.com (Brandon Invergo)
Date: Wed, 20 Aug 2008 13:10:04 -0500
Subject: [R-sig-ME] possible lmer compatibility bug?
In-Reply-To: <40e66e0b0808201019g272e1405r608f1f0c3be08758@mail.gmail.com>
References: <48AC4E20.6050809@brandoninvergo.com>
	<40e66e0b0808201019g272e1405r608f1f0c3be08758@mail.gmail.com>
Message-ID: <48AC5DFC.30803@brandoninvergo.com>

Well, I'm not sure how to give you a reproducible example. What I can 
say is that it happens every time I use lmer() in R 2.7.1, for any model 
formula. Using the same data file, lmer() successfully completes in 
2.7.0 but it returns the error in 2.7.1. For what it's worth, here's an 
example (in the data, dev.time is an integer, sex and temp are factors 
(2 levels and 5 levels, resp.), and sleeve.in.temp is a random variable):

2.7.0:

 > dev.modelc <- lmer(dev.time ~ sex*temp + (1|sleeve.in.temp), 
data=data.clean, family=Gamma(link="log"))
 > summary(dev.modelc)
Generalized linear mixed model fit using Laplace
Formula: dev.time ~ sex * temp + (1 | sleeve.in.temp)
   Data: data.clean
 Family: Gamma(log link)
   AIC   BIC logLik deviance
 29.28 87.32 -3.639    7.277
Random effects:
 Groups         Name        Variance   Std.Dev. 
 sleeve.in.temp (Intercept) 2.5683e-12 1.6026e-06
 Residual                   5.1366e-03 7.1670e-02
number of obs: 1446, groups: sleeve.in.temp, 54

Fixed effects:
             Estimate Std. Error t value
(Intercept)  3.659500   0.005993   610.6
sexm        -0.088450   0.008956    -9.9
temp21      -0.207295   0.008132   -25.5
temp23      -0.433156   0.008363   -51.8
temp25      -0.612696   0.008491   -72.2
temp27      -0.755628   0.008297   -91.1
sexm:temp21  0.008189   0.012000     0.7
sexm:temp23 -0.003357   0.012243    -0.3
sexm:temp25 -0.014887   0.012321    -1.2
sexm:temp27  0.010674   0.012405     0.9

Correlation of Fixed Effects:
            (Intr) sexm   temp21 temp23 temp25 temp27 sxm:21 sxm:23 sxm:25
sexm        -0.669                                                       
temp21      -0.737  0.493                                                
temp23      -0.717  0.480  0.528                                         
temp25      -0.706  0.472  0.520  0.506                                  
temp27      -0.722  0.483  0.532  0.518  0.510                           
sexm:temp21  0.499 -0.746 -0.678 -0.358 -0.353 -0.361                    
sexm:temp23  0.490 -0.731 -0.361 -0.683 -0.346 -0.354  0.546             
sexm:temp25  0.486 -0.727 -0.358 -0.349 -0.689 -0.351  0.542  0.532      
sexm:temp27  0.483 -0.722 -0.356 -0.346 -0.341 -0.669  0.539  0.528  0.525
 >sessionInfo()
R version 2.7.0 (2008-04-22)
i386-pc-mingw32

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United 
States.1252;LC_MONETARY=English_United 
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base    

other attached packages:
[1] gplots_2.6.0      gdata_2.4.2       gtools_2.5.0      nlme_3.1-88     
[5] lme4_0.99875-9    Matrix_0.999375-9 lattice_0.17-6  

loaded via a namespace (and not attached):
[1] grid_2.7.0




and for 2.7.1:
 > dev.modelc <- lmer(dev.time ~ sex*temp + (1|sleeve.in.temp), 
data=data.clean, family=Gamma(link="log"))
 > summary(dev.modelc)
Error in asMethod(object) : matrix is not symmetric [1,2]
 > traceback()
15: .Call(dense_to_symmetric, from, "U", TRUE)
14: asMethod(object)
13: as(from, "symmetricMatrix")
12: .class1(object)
11: as(as(from, "symmetricMatrix"), "dMatrix")
10: .class1(object)
9: as(as(as(from, "symmetricMatrix"), "dMatrix"), "denseMatrix")
8: .class1(object)
7: as(as(as(as(from, "symmetricMatrix"), "dMatrix"), "denseMatrix"),
       "dpoMatrix")
6: asMethod(object)
5: as(sigma(object)^2 * chol2inv(object at RX, size = object at dims["p"]),
       "dpoMatrix")
4: vcov(object)
3: vcov(object)
2: summary(dev.modelc)
1: summary(dev.modelc)
 > sessionInfo()
R version 2.7.1 (2008-06-23)
i386-pc-mingw32

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United 
States.1252;LC_MONETARY=English_United 
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base    

other attached packages:
[1] gplots_2.6.0       gdata_2.4.2        gtools_2.5.0       
nlme_3.1-89      
[5] lme4_0.999375-25   Matrix_0.999375-11 lattice_0.17-8   

loaded via a namespace (and not attached):
[1] grid_2.7.1


I'm sorry, I wasn't aware of the sessionInfo() or traceback() functions 
as this was the first time I've ever encountered an error like this in 
R. I'll be sure to be more courteous and include them next time...







Douglas Bates wrote:
> On Wed, Aug 20, 2008 at 12:02 PM, Brandon Invergo
> <brandon at brandoninvergo.com> wrote:
>   
>> Dear all,
>>     
>
>   
>> I was just driving myself crazy trying to figure out this sudden error
>> message I was getting on lines of code that were working fine the other day.
>> Every time I tried to make a model with the lmer() function, it would return
>> the following error: "Error in asMethod(object) : matrix is not symmetric
>> [1,2]"
>>     
>
>   
>> I realized, though, that the only thing that had changed was the version of
>> R I was using, having just switched to version 2.7.1. Sure enough, when I
>> loaded up v2.7.0, the script worked fine and I could successfully call lmer.
>>     
>
>   
>> Is there something I'm possibly doing wrong that's preventing it from
>> working in 2.7.1? Or is this a known problem?
>>     
>
> It appears that you have too high a regard for the readers of this
> list in that you expect them to have psychic powers :-)
>
> More seriously, if you read your message and think of how a person
> could possibly create a meaningful response based on the information
> that you have given us, I think you will see that we don't have nearly
> enough information to go on.
>
> A reproducible example rather than a description of "lines of code
> that were working fine" is what we will need before we can expect to
> provide a useful answer to you.  Also, information like the output
> from
>
> sessionInfo()
>
> and perhaps
>
> traceback()
>
> after the error message so we know what was going on at the time the
> error occurred.  Most importantly, we need to know what the call was
> that resulted in an error.
>



From bates at stat.wisc.edu  Wed Aug 20 20:50:48 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 20 Aug 2008 13:50:48 -0500
Subject: [R-sig-ME] possible lmer compatibility bug?
In-Reply-To: <48AC5DFC.30803@brandoninvergo.com>
References: <48AC4E20.6050809@brandoninvergo.com>
	<40e66e0b0808201019g272e1405r608f1f0c3be08758@mail.gmail.com>
	<48AC5DFC.30803@brandoninvergo.com>
Message-ID: <40e66e0b0808201150s43babd4lb7a1507695be3abb@mail.gmail.com>

Thanks for sending the additional information.  There are two things
you can try immediately.

Don't simultaneously load the lme4 and the nlme packages.  It is
unfortunate that the packages conflict with each other but they do
because there are incompatible definitions of some of the extractor
functions.

Update your version of the lme4 package.  The version you are using is
quite out of date.   Just run

update.packages()

to get the updated versions.

On Wed, Aug 20, 2008 at 1:10 PM, Brandon Invergo
<brandon at brandoninvergo.com> wrote:
> Well, I'm not sure how to give you a reproducible example. What I can say is
> that it happens every time I use lmer() in R 2.7.1, for any model formula.
> Using the same data file, lmer() successfully completes in 2.7.0 but it
> returns the error in 2.7.1. For what it's worth, here's an example (in the
> data, dev.time is an integer, sex and temp are factors (2 levels and 5
> levels, resp.), and sleeve.in.temp is a random variable):
>
> 2.7.0:
>
>> dev.modelc <- lmer(dev.time ~ sex*temp + (1|sleeve.in.temp),
>> data=data.clean, family=Gamma(link="log"))
>> summary(dev.modelc)
> Generalized linear mixed model fit using Laplace
> Formula: dev.time ~ sex * temp + (1 | sleeve.in.temp)
>  Data: data.clean
> Family: Gamma(log link)
>  AIC   BIC logLik deviance
> 29.28 87.32 -3.639    7.277
> Random effects:
> Groups         Name        Variance   Std.Dev. sleeve.in.temp (Intercept)
> 2.5683e-12 1.6026e-06
> Residual                   5.1366e-03 7.1670e-02
> number of obs: 1446, groups: sleeve.in.temp, 54
>
> Fixed effects:
>            Estimate Std. Error t value
> (Intercept)  3.659500   0.005993   610.6
> sexm        -0.088450   0.008956    -9.9
> temp21      -0.207295   0.008132   -25.5
> temp23      -0.433156   0.008363   -51.8
> temp25      -0.612696   0.008491   -72.2
> temp27      -0.755628   0.008297   -91.1
> sexm:temp21  0.008189   0.012000     0.7
> sexm:temp23 -0.003357   0.012243    -0.3
> sexm:temp25 -0.014887   0.012321    -1.2
> sexm:temp27  0.010674   0.012405     0.9
>
> Correlation of Fixed Effects:
>           (Intr) sexm   temp21 temp23 temp25 temp27 sxm:21 sxm:23 sxm:25
> sexm        -0.669
> temp21      -0.737  0.493
>  temp23      -0.717  0.480  0.528
> temp25      -0.706  0.472  0.520  0.506
>  temp27      -0.722  0.483  0.532  0.518  0.510
> sexm:temp21  0.499 -0.746 -0.678 -0.358 -0.353 -0.361
>  sexm:temp23  0.490 -0.731 -0.361 -0.683 -0.346 -0.354  0.546
> sexm:temp25  0.486 -0.727 -0.358 -0.349 -0.689 -0.351  0.542  0.532
>  sexm:temp27  0.483 -0.722 -0.356 -0.346 -0.341 -0.669  0.539  0.528  0.525
>>sessionInfo()
> R version 2.7.0 (2008-04-22)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
> States.1252;LC_MONETARY=English_United
> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> other attached packages:
> [1] gplots_2.6.0      gdata_2.4.2       gtools_2.5.0      nlme_3.1-88
> [5] lme4_0.99875-9    Matrix_0.999375-9 lattice_0.17-6
> loaded via a namespace (and not attached):
> [1] grid_2.7.0
>
>
>
>
> and for 2.7.1:
>> dev.modelc <- lmer(dev.time ~ sex*temp + (1|sleeve.in.temp),
>> data=data.clean, family=Gamma(link="log"))
>> summary(dev.modelc)
> Error in asMethod(object) : matrix is not symmetric [1,2]
>> traceback()
> 15: .Call(dense_to_symmetric, from, "U", TRUE)
> 14: asMethod(object)
> 13: as(from, "symmetricMatrix")
> 12: .class1(object)
> 11: as(as(from, "symmetricMatrix"), "dMatrix")
> 10: .class1(object)
> 9: as(as(as(from, "symmetricMatrix"), "dMatrix"), "denseMatrix")
> 8: .class1(object)
> 7: as(as(as(as(from, "symmetricMatrix"), "dMatrix"), "denseMatrix"),
>      "dpoMatrix")
> 6: asMethod(object)
> 5: as(sigma(object)^2 * chol2inv(object at RX, size = object at dims["p"]),
>      "dpoMatrix")
> 4: vcov(object)
> 3: vcov(object)
> 2: summary(dev.modelc)
> 1: summary(dev.modelc)
>> sessionInfo()
> R version 2.7.1 (2008-06-23)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
> States.1252;LC_MONETARY=English_United
> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> other attached packages:
> [1] gplots_2.6.0       gdata_2.4.2        gtools_2.5.0       nlme_3.1-89
>  [5] lme4_0.999375-25   Matrix_0.999375-11 lattice_0.17-8
> loaded via a namespace (and not attached):
> [1] grid_2.7.1
>
>
> I'm sorry, I wasn't aware of the sessionInfo() or traceback() functions as
> this was the first time I've ever encountered an error like this in R. I'll
> be sure to be more courteous and include them next time...
>
>
>
>
>
>
>
> Douglas Bates wrote:
>>
>> On Wed, Aug 20, 2008 at 12:02 PM, Brandon Invergo
>> <brandon at brandoninvergo.com> wrote:
>>
>>>
>>> Dear all,
>>>
>>
>>
>>>
>>> I was just driving myself crazy trying to figure out this sudden error
>>> message I was getting on lines of code that were working fine the other
>>> day.
>>> Every time I tried to make a model with the lmer() function, it would
>>> return
>>> the following error: "Error in asMethod(object) : matrix is not symmetric
>>> [1,2]"
>>>
>>
>>
>>>
>>> I realized, though, that the only thing that had changed was the version
>>> of
>>> R I was using, having just switched to version 2.7.1. Sure enough, when I
>>> loaded up v2.7.0, the script worked fine and I could successfully call
>>> lmer.
>>>
>>
>>
>>>
>>> Is there something I'm possibly doing wrong that's preventing it from
>>> working in 2.7.1? Or is this a known problem?
>>>
>>
>> It appears that you have too high a regard for the readers of this
>> list in that you expect them to have psychic powers :-)
>>
>> More seriously, if you read your message and think of how a person
>> could possibly create a meaningful response based on the information
>> that you have given us, I think you will see that we don't have nearly
>> enough information to go on.
>>
>> A reproducible example rather than a description of "lines of code
>> that were working fine" is what we will need before we can expect to
>> provide a useful answer to you.  Also, information like the output
>> from
>>
>> sessionInfo()
>>
>> and perhaps
>>
>> traceback()
>>
>> after the error message so we know what was going on at the time the
>> error occurred.  Most importantly, we need to know what the call was
>> that resulted in an error.
>>
>
>



From A.Robinson at ms.unimelb.edu.au  Wed Aug 20 21:24:52 2008
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Thu, 21 Aug 2008 05:24:52 +1000
Subject: [R-sig-ME] understanding log-likelihood/model fit
In-Reply-To: <a46630750808200601o702fc711wdd89b396e6231842@mail.gmail.com>
References: <a46630750808191347v43e2828s1c36e504d8387a78@mail.gmail.com>
	<3278ABDD-0E3B-4284-9266-593AB38CF043@anu.edu.au>
	<a46630750808191719h31543d6ep5a91b2ca6019062c@mail.gmail.com>
	<D6B0DBE2-05A5-419D-B61D-43D296E64DCE@anu.edu.au>
	<a46630750808200601o702fc711wdd89b396e6231842@mail.gmail.com>
Message-ID: <20080820192452.GO84020@ms.unimelb.edu.au>

I'll take a swing at these.

On Wed, Aug 20, 2008 at 02:01:29PM +0100, Daniel Ezra Johnson wrote:
> Everyone agrees about what happens here:
> 
> Nsubj <- 10
> Ngrp <- 2
> NsubjRep <- 5
> set.seed(123)
> test1s <- data.frame(subject = rep(seq(Nsubj * Ngrp), each = NsubjRep),
>        response=500+c(rep(-100,Nsubj * NsubjRep),rep(100,Nsubj *
> NsubjRep))+rnorm(Nsubj * Ngrp * NsubjRep, 0, 10),
>        fixed=(rep(c("A","B"),each=Nsubj * NsubjRep)))
> null1 <- lmer(response~(1|subject),test1s)
> fixed1 <- lmer(response~fixed+(1|subject),test1s)
> 
> I still have two questions which I'll try to restate. I should note
> that I have attempted to understand the mathematical details of ML
> mixed effect model fitting and it's a bit beyond me. But I hope that
> someone can provide an answer I can understand.
> 
> Question 1: When you have an "outer" fixed effect and a "subject"
> random effect in the same model, specifically why does the model
> (apparently) converge in such a way that the fixed effect is maximized
> and the random effect minimized? (Not so much why should it, as why
> does it? This is the 'fixed1' case.)

A common way for hierarchical models to be fit using ML is by
profiling out the fixed effects, estimating the random effects, and
then using GLS to estimate the fixed effects conditional on the random
effects.  So, any explanatory capacity that the fixed effects offer is
deployed before the random effects are invoked.  

Likewise a popular way to applying ReML is to fit the fixed effects
using OLS, then estimate the random effects from the residuals.
Again, the net effect is that any explanatory capacity that the fixed
effects offer is deployed before the random effects are invoked.

> Question 2: Take the fixed1 model from Question 1 and compare it to
> the null1 model, which has a random subject effect but no fixed
> effect. The predicted values of the two models -- the ones from
> fitted(), which include the ranefs -- are virtually the same. So why
> does fixed1 have a lower deviance, why is it preferred to null1 in a
> likelihood ratio test? (Again, I'm not asking why it's a better model.
> I'm asking questions about the software, estimation procedure, and/or
> theory of likelihood applied to such cases.)

The deviance is computed from the log likelihood of the data,
conditional on the model.  The LL of the null model is maximized by
making the variance components big enough to cover the variation of
the data.  But, this means that the likelihood is being spread thinly,
as it were.  Eg ...

> dnorm(0, sd=1)
[1] 0.3989423
> dnorm(0, sd=2)
[1] 0.1994711

On the other hand, fixed1 uses fixed effects to explain a lot of that
variation, so that when the time comes to estimate the random effects,
they are smaller, and the LL is higher, because it doesn't have to
stretch so far.

Cheers,

Andrew

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-6410
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/



From bates at stat.wisc.edu  Wed Aug 20 21:53:06 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 20 Aug 2008 14:53:06 -0500
Subject: [R-sig-ME] understanding log-likelihood/model fit
In-Reply-To: <20080820192452.GO84020@ms.unimelb.edu.au>
References: <a46630750808191347v43e2828s1c36e504d8387a78@mail.gmail.com>
	<3278ABDD-0E3B-4284-9266-593AB38CF043@anu.edu.au>
	<a46630750808191719h31543d6ep5a91b2ca6019062c@mail.gmail.com>
	<D6B0DBE2-05A5-419D-B61D-43D296E64DCE@anu.edu.au>
	<a46630750808200601o702fc711wdd89b396e6231842@mail.gmail.com>
	<20080820192452.GO84020@ms.unimelb.edu.au>
Message-ID: <40e66e0b0808201253i1da060fdke7d84a6f278af16f@mail.gmail.com>

On Wed, Aug 20, 2008 at 2:24 PM, Andrew Robinson
<A.Robinson at ms.unimelb.edu.au> wrote:
> I'll take a swing at these.
>
> On Wed, Aug 20, 2008 at 02:01:29PM +0100, Daniel Ezra Johnson wrote:
>> Everyone agrees about what happens here:
>>
>> Nsubj <- 10
>> Ngrp <- 2
>> NsubjRep <- 5
>> set.seed(123)
>> test1s <- data.frame(subject = rep(seq(Nsubj * Ngrp), each = NsubjRep),
>>        response=500+c(rep(-100,Nsubj * NsubjRep),rep(100,Nsubj *
>> NsubjRep))+rnorm(Nsubj * Ngrp * NsubjRep, 0, 10),
>>        fixed=(rep(c("A","B"),each=Nsubj * NsubjRep)))
>> null1 <- lmer(response~(1|subject),test1s)
>> fixed1 <- lmer(response~fixed+(1|subject),test1s)
>>
>> I still have two questions which I'll try to restate. I should note
>> that I have attempted to understand the mathematical details of ML
>> mixed effect model fitting and it's a bit beyond me. But I hope that
>> someone can provide an answer I can understand.
>>
>> Question 1: When you have an "outer" fixed effect and a "subject"
>> random effect in the same model, specifically why does the model
>> (apparently) converge in such a way that the fixed effect is maximized
>> and the random effect minimized? (Not so much why should it, as why
>> does it? This is the 'fixed1' case.)

I approach this from a slightly different point of view than does
Andrew.  I prefer to think of the process of estimating the fixed
effects parameters and the random effects as a penalized estimation
problem where the penalty is applied to the random effects only.  The
magnitude of the penalty depends on the (unconditional) variance
covariance matrix of the random effects.  When the variances are small
there is a large penalty.  When the variances are large there is a
small penalty on the size of the random effects.  The measure of model
complexity, which is related to the determinant of the conditional
variance of the random effects, given the data, has the opposite
behavior.  When the variance of the random effects is small the model
is considered simpler.  The simplest possible model on this scale is
one without any random effects at all, corresponding to a variance of
zero.  The larger the variance of the random effects the more complex
the model.

The maximum likelihood criterion seeks a balance between model
complexity and fidelity to the data.  Simple models fit the data less
well than do complex models.

The point to notice, however, is that the penalty on model complexity
applies to the random effects only, not to the fixed effects.  From
this point of view if the model can explain a certain pattern in the
data either with fixed-effects parameters or with random effects there
is an advantage in explaining it with the fixed effects.

>
> A common way for hierarchical models to be fit using ML is by
> profiling out the fixed effects, estimating the random effects, and
> then using GLS to estimate the fixed effects conditional on the random
> effects.  So, any explanatory capacity that the fixed effects offer is
> deployed before the random effects are invoked.
>
> Likewise a popular way to applying ReML is to fit the fixed effects
> using OLS, then estimate the random effects from the residuals.
> Again, the net effect is that any explanatory capacity that the fixed
> effects offer is deployed before the random effects are invoked.
>
>> Question 2: Take the fixed1 model from Question 1 and compare it to
>> the null1 model, which has a random subject effect but no fixed
>> effect. The predicted values of the two models -- the ones from
>> fitted(), which include the ranefs -- are virtually the same. So why
>> does fixed1 have a lower deviance, why is it preferred to null1 in a
>> likelihood ratio test? (Again, I'm not asking why it's a better model.
>> I'm asking questions about the software, estimation procedure, and/or
>> theory of likelihood applied to such cases.)

Because the likelihood involves a penalty on the size of the variance
of the random effects.  Obtaining a similar fit (fidelity to the data)
with a much lower variance on the random effects (the penalty on model
complexity) is advantageous under the likelihood criterion.



>
> The deviance is computed from the log likelihood of the data,
> conditional on the model.  The LL of the null model is maximized by
> making the variance components big enough to cover the variation of
> the data.  But, this means that the likelihood is being spread thinly,
> as it were.  Eg ...
>
>> dnorm(0, sd=1)
> [1] 0.3989423
>> dnorm(0, sd=2)
> [1] 0.1994711
>
> On the other hand, fixed1 uses fixed effects to explain a lot of that
> variation, so that when the time comes to estimate the random effects,
> they are smaller, and the LL is higher, because it doesn't have to
> stretch so far.
>
> Cheers,
>
> Andrew
>
> --
> Andrew Robinson
> Department of Mathematics and Statistics            Tel: +61-3-8344-6410
> University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
> http://www.ms.unimelb.edu.au/~andrewpr
> http://blogs.mbs.edu/fishing-in-the-bay/
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From John.Maindonald at anu.edu.au  Thu Aug 21 02:51:22 2008
From: John.Maindonald at anu.edu.au (John Maindonald)
Date: Thu, 21 Aug 2008 10:51:22 +1000
Subject: [R-sig-ME] understanding log-likelihood/model fit
In-Reply-To: <40e66e0b0808201253i1da060fdke7d84a6f278af16f@mail.gmail.com>
References: <a46630750808191347v43e2828s1c36e504d8387a78@mail.gmail.com>
	<3278ABDD-0E3B-4284-9266-593AB38CF043@anu.edu.au>
	<a46630750808191719h31543d6ep5a91b2ca6019062c@mail.gmail.com>
	<D6B0DBE2-05A5-419D-B61D-43D296E64DCE@anu.edu.au>
	<a46630750808200601o702fc711wdd89b396e6231842@mail.gmail.com>
	<20080820192452.GO84020@ms.unimelb.edu.au>
	<40e66e0b0808201253i1da060fdke7d84a6f278af16f@mail.gmail.com>
Message-ID: <5F9FDC43-86A1-4862-9C7F-A03FC92B7A36@anu.edu.au>

I find that a very insightful way to think about results from
Daniel's simulated data.  In the case of his null1 model,
failure to adjust for the fixed effect led to a large (huge
relative to the component in fixed1) between subjects
component of variance.  There was, then, a small penalty
for random subject effects whose distribution looked
nothing like normal, and the residuals looked remarkably
normal.  It is an extreme example of a case where the
residuals have scant diagnostic usefulness.

My concern with some of the Pinheiro and Bates plots is
that they look too soon at the residuals, before doing the
grosser checks that are needed that the model is half-way
correct, and that the points are not joined up a/c subject
or whatever, so that within subject systematic departures
from the model are not evident.  I seem to recall finding
an aberrant patient when I looked at the Dialyzer data
some considerable time ago, and/or an aberrant Orange
tree.  In a quick look, I was not immediately able to
recover the state of thinking that led me to this conclusion.
Irrespective of the application to these data, the point that
residuals are one part only of the relevant diagnostic
information, and may indeed not convey much at all of
the relevant information, stands.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.


On 21 Aug 2008, at 5:53 AM, Douglas Bates wrote:

> On Wed, Aug 20, 2008 at 2:24 PM, Andrew Robinson
> <A.Robinson at ms.unimelb.edu.au> wrote:
>> I'll take a swing at these.
>>
>> On Wed, Aug 20, 2008 at 02:01:29PM +0100, Daniel Ezra Johnson wrote:
>>> Everyone agrees about what happens here:
>>>
>>> Nsubj <- 10
>>> Ngrp <- 2
>>> NsubjRep <- 5
>>> set.seed(123)
>>> test1s <- data.frame(subject = rep(seq(Nsubj * Ngrp), each =  
>>> NsubjRep),
>>>       response=500+c(rep(-100,Nsubj * NsubjRep),rep(100,Nsubj *
>>> NsubjRep))+rnorm(Nsubj * Ngrp * NsubjRep, 0, 10),
>>>       fixed=(rep(c("A","B"),each=Nsubj * NsubjRep)))
>>> null1 <- lmer(response~(1|subject),test1s)
>>> fixed1 <- lmer(response~fixed+(1|subject),test1s)
>>>
>>> I still have two questions which I'll try to restate. I should note
>>> that I have attempted to understand the mathematical details of ML
>>> mixed effect model fitting and it's a bit beyond me. But I hope that
>>> someone can provide an answer I can understand.
>>>
>>> Question 1: When you have an "outer" fixed effect and a "subject"
>>> random effect in the same model, specifically why does the model
>>> (apparently) converge in such a way that the fixed effect is  
>>> maximized
>>> and the random effect minimized? (Not so much why should it, as why
>>> does it? This is the 'fixed1' case.)
>
> I approach this from a slightly different point of view than does
> Andrew.  I prefer to think of the process of estimating the fixed
> effects parameters and the random effects as a penalized estimation
> problem where the penalty is applied to the random effects only.  The
> magnitude of the penalty depends on the (unconditional) variance
> covariance matrix of the random effects.  When the variances are small
> there is a large penalty.  When the variances are large there is a
> small penalty on the size of the random effects.  The measure of model
> complexity, which is related to the determinant of the conditional
> variance of the random effects, given the data, has the opposite
> behavior.  When the variance of the random effects is small the model
> is considered simpler.  The simplest possible model on this scale is
> one without any random effects at all, corresponding to a variance of
> zero.  The larger the variance of the random effects the more complex
> the model.
>
> The maximum likelihood criterion seeks a balance between model
> complexity and fidelity to the data.  Simple models fit the data less
> well than do complex models.
>
> The point to notice, however, is that the penalty on model complexity
> applies to the random effects only, not to the fixed effects.  From
> this point of view if the model can explain a certain pattern in the
> data either with fixed-effects parameters or with random effects there
> is an advantage in explaining it with the fixed effects.
>
>>
>> A common way for hierarchical models to be fit using ML is by
>> profiling out the fixed effects, estimating the random effects, and
>> then using GLS to estimate the fixed effects conditional on the  
>> random
>> effects.  So, any explanatory capacity that the fixed effects offer  
>> is
>> deployed before the random effects are invoked.
>>
>> Likewise a popular way to applying ReML is to fit the fixed effects
>> using OLS, then estimate the random effects from the residuals.
>> Again, the net effect is that any explanatory capacity that the fixed
>> effects offer is deployed before the random effects are invoked.
>>
>>> Question 2: Take the fixed1 model from Question 1 and compare it to
>>> the null1 model, which has a random subject effect but no fixed
>>> effect. The predicted values of the two models -- the ones from
>>> fitted(), which include the ranefs -- are virtually the same. So why
>>> does fixed1 have a lower deviance, why is it preferred to null1 in a
>>> likelihood ratio test? (Again, I'm not asking why it's a better  
>>> model.
>>> I'm asking questions about the software, estimation procedure, and/ 
>>> or
>>> theory of likelihood applied to such cases.)
>
> Because the likelihood involves a penalty on the size of the variance
> of the random effects.  Obtaining a similar fit (fidelity to the data)
> with a much lower variance on the random effects (the penalty on model
> complexity) is advantageous under the likelihood criterion.
>
>
>
>>
>> The deviance is computed from the log likelihood of the data,
>> conditional on the model.  The LL of the null model is maximized by
>> making the variance components big enough to cover the variation of
>> the data.  But, this means that the likelihood is being spread  
>> thinly,
>> as it were.  Eg ...
>>
>>> dnorm(0, sd=1)
>> [1] 0.3989423
>>> dnorm(0, sd=2)
>> [1] 0.1994711
>>
>> On the other hand, fixed1 uses fixed effects to explain a lot of that
>> variation, so that when the time comes to estimate the random  
>> effects,
>> they are smaller, and the LL is higher, because it doesn't have to
>> stretch so far.
>>
>> Cheers,
>>
>> Andrew
>>
>> --
>> Andrew Robinson
>> Department of Mathematics and Statistics            Tel:  
>> +61-3-8344-6410
>> University of Melbourne, VIC 3010 Australia         Fax:  
>> +61-3-8344-4599
>> http://www.ms.unimelb.edu.au/~andrewpr
>> http://blogs.mbs.edu/fishing-in-the-bay/
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>



From David.Duffy at qimr.edu.au  Thu Aug 21 04:48:45 2008
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Thu, 21 Aug 2008 12:48:45 +1000 (EST)
Subject: [R-sig-ME] Problems in using lmer to fit a multilevel model
In-Reply-To: <40e66e0b0808200919m11788ff3qf1c6fc7b61fe0f9@mail.gmail.com>
References: <48AC40E4.000005.11569@app-03>
	<40e66e0b0808200919m11788ff3qf1c6fc7b61fe0f9@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0808211245100.26787@orpheus.qimr.edu.au>

On Wed, 20 Aug 2008, Douglas Bates wrote:

> 2008/8/20 chenlei <chenlei at ibcas.ac.cn>:
>>   when I fitted my logistic model(binary data),
>> I received an error "Error in mer_finalize(ans, verbose) : q = > n = ". 
>
> Hmm.  The message was supposed to be a bit more informative in that it
> should have given the values of q and n.  I will repair that.
>
> The value of q is the total number of random effects and the value of
> n is the number of observations.  I included that check because it did
> not make sense to me to try to fit more random effects than you have
> observations.

> I guess I could be persuaded that it would make sense
> in some circumstances because the random effects are determined by a
> penalized least squares optimization.
>
> What is the nature of the model that would require it to have more
> random effects than observations?

Commonly, genetic models fit 2 or more random effects per individual, with 
different prespecified covariance matrices (A, D, A*A, A*D...)

David Duffy.
-- 
| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v



From ken at kjbeath.com.au  Thu Aug 21 10:58:43 2008
From: ken at kjbeath.com.au (Ken Beath)
Date: Thu, 21 Aug 2008 18:58:43 +1000
Subject: [R-sig-ME] understanding log-likelihood/model fit
In-Reply-To: <a46630750808200601o702fc711wdd89b396e6231842@mail.gmail.com>
References: <a46630750808191347v43e2828s1c36e504d8387a78@mail.gmail.com>
	<3278ABDD-0E3B-4284-9266-593AB38CF043@anu.edu.au>
	<a46630750808191719h31543d6ep5a91b2ca6019062c@mail.gmail.com>
	<D6B0DBE2-05A5-419D-B61D-43D296E64DCE@anu.edu.au>
	<a46630750808200601o702fc711wdd89b396e6231842@mail.gmail.com>
Message-ID: <C92CBD76-1CB8-4235-A5D2-FCC4BE89E25B@kjbeath.com.au>

On 20/08/2008, at 11:01 PM, Daniel Ezra Johnson wrote:

> Everyone agrees about what happens here:
>
> Nsubj <- 10
> Ngrp <- 2
> NsubjRep <- 5
> set.seed(123)
> test1s <- data.frame(subject = rep(seq(Nsubj * Ngrp), each =  
> NsubjRep),
>       response=500+c(rep(-100,Nsubj * NsubjRep),rep(100,Nsubj *
> NsubjRep))+rnorm(Nsubj * Ngrp * NsubjRep, 0, 10),
>       fixed=(rep(c("A","B"),each=Nsubj * NsubjRep)))
> null1 <- lmer(response~(1|subject),test1s)
> fixed1 <- lmer(response~fixed+(1|subject),test1s)
>
> I still have two questions which I'll try to restate. I should note
> that I have attempted to understand the mathematical details of ML
> mixed effect model fitting and it's a bit beyond me. But I hope that
> someone can provide an answer I can understand.
>
> Question 1: When you have an "outer" fixed effect and a "subject"
> random effect in the same model, specifically why does the model
> (apparently) converge in such a way that the fixed effect is maximized
> and the random effect minimized? (Not so much why should it, as why
> does it? This is the 'fixed1' case.)
>

Because your generated data has only a fixed effect, so the estimated  
random effect is zero. This is the same as if you generated data of  
the form y=x, it would be expected to fit with an intercept of close  
to zero. Someone else supplied an example of adding a random effect  
which, of course, will result in a fitted random effect of greater  
than zero.


> Question 2: Take the fixed1 model from Question 1 and compare it to
> the null1 model, which has a random subject effect but no fixed
> effect. The predicted values of the two models -- the ones from
> fitted(), which include the ranefs -- are virtually the same. So why
> does fixed1 have a lower deviance, why is it preferred to null1 in a
> likelihood ratio test? (Again, I'm not asking why it's a better model.
> I'm asking questions about the software, estimation procedure, and/or
> theory of likelihood applied to such cases.)
>

The residual variance is slightly lower for the second model  
explaining the better fit. Knowing about the fixed effects helps but  
not much.

The fitted are similar, but there is reasonable variation. It will  
always be better (in terms of likelihood and as a rule) to fit the  
correct model. What is nice is that the model with only a random  
effect gives sensible results, so in many situations I don't need to  
know why the clusters vary and departures from normality don't seem to  
matter much.

Ken



> D
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From logodall at yahoo.fr  Thu Aug 21 11:50:39 2008
From: logodall at yahoo.fr (logodall)
Date: Thu, 21 Aug 2008 09:50:39 +0000 (GMT)
Subject: [R-sig-ME] residual variance in nested mixed model with
	family=Poisson?
Message-ID: <401224.42616.qm@web26002.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080821/34853fbf/attachment.pl>

From gregor.gorjanc at bfro.uni-lj.si  Thu Aug 21 12:12:28 2008
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Thu, 21 Aug 2008 10:12:28 +0000 (UTC)
Subject: [R-sig-ME] Problems in using lmer to fit a multilevel model
References: <48AC40E4.000005.11569@app-03>
	<40e66e0b0808200919m11788ff3qf1c6fc7b61fe0f9@mail.gmail.com>
	<Pine.LNX.4.64.0808211245100.26787@orpheus.qimr.edu.au>
Message-ID: <loom.20080821T101018-33@post.gmane.org>

...
> > The value of q is the total number of random effects and the value of
> > n is the number of observations.  I included that check because it did
> > not make sense to me to try to fit more random effects than you have
> > observations.
> 
> > I guess I could be persuaded that it would make sense
> > in some circumstances because the random effects are determined by a
> > penalized least squares optimization.
> >
> > What is the nature of the model that would require it to have more
> > random effects than observations?
> 
> Commonly, genetic models fit 2 or more random effects per individual, with 
> different prespecified covariance matrices (A, D, A*A, A*D...)

Yes, and it can even happen that there is more individuals in the pedigree
than there are phenotypic records! However, this is a bit special application. 
Perhaps, the warning might be more appropriate than the error.

Gregor



From HStevens at muohio.edu  Thu Aug 21 14:19:03 2008
From: HStevens at muohio.edu (Martin Henry H. Stevens)
Date: Thu, 21 Aug 2008 08:19:03 -0400
Subject: [R-sig-ME] Re : residual variance in nested mixed model with
	family=Poisson?
In-Reply-To: <336485.30799.qm@web26004.mail.ukl.yahoo.com>
References: <336485.30799.qm@web26004.mail.ukl.yahoo.com>
Message-ID: <E80DB8AD-17CF-4498-9C00-615816658A19@muohio.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080821/e3577740/attachment.pl>

From bates at stat.wisc.edu  Thu Aug 21 14:54:00 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 21 Aug 2008 07:54:00 -0500
Subject: [R-sig-ME] residual variance in nested mixed model with
	family=Poisson?
In-Reply-To: <401224.42616.qm@web26002.mail.ukl.yahoo.com>
References: <401224.42616.qm@web26002.mail.ukl.yahoo.com>
Message-ID: <40e66e0b0808210554k35347f3bq79aadb66171a4623@mail.gmail.com>

On Thu, Aug 21, 2008 at 4:50 AM, logodall <logodall at yahoo.fr> wrote:
> Hello,
>    I would like to ask if it is possible (and if so how) to obtained an estimate of the residual variance of a mixed model having a set of nested random factors when using family=Poisson. I have searched in the help files and in the forum and I found no information in this regard. When using family=Gaussian, one obtains by default this variance but not when using Poisson. In the latest verion of lme4, one can now obtain the residuals of a fitted model when family=Poisson, and one can of course calculated the variance of these residuals. However, not being sure of how these residuals are calculated (are they standardised?, raw?, etc), I am unsure of using their variance as an estimate of the residual variance in a nested mixed model.

I don't know how the residual variance of a mixed model would be
defined for family = Poisson.  I formulate the probability model for
GLMMs in terms of the (unconditional) distribution of the random
effects, B, and the conditional distribution of the response, Y, given
the random effects.  The distribution of B is multivariate Gaussian.
The components of Y|B are independent and, for a Poisson GLMM, have a
Poisson distribution.  They are completely determined by the
conditional mean, E[Y|B].  (see slides 9 and 10 of
http://www.stat.wisc.edu/~bates/PotsdamGLMM/GLMMD.pdf).

It is not an oversight that there is no estimate of a residual
variance given for a Poisson GLMM.  Such a parameter doesn't exist in
the way that I formulate the model.



From danielezrajohnson at gmail.com  Thu Aug 21 14:35:57 2008
From: danielezrajohnson at gmail.com (Daniel Ezra Johnson)
Date: Thu, 21 Aug 2008 13:35:57 +0100
Subject: [R-sig-ME] understanding log-likelihood/model fit
In-Reply-To: <40e66e0b0808201253i1da060fdke7d84a6f278af16f@mail.gmail.com>
References: <a46630750808191347v43e2828s1c36e504d8387a78@mail.gmail.com>
	<3278ABDD-0E3B-4284-9266-593AB38CF043@anu.edu.au>
	<a46630750808191719h31543d6ep5a91b2ca6019062c@mail.gmail.com>
	<D6B0DBE2-05A5-419D-B61D-43D296E64DCE@anu.edu.au>
	<a46630750808200601o702fc711wdd89b396e6231842@mail.gmail.com>
	<20080820192452.GO84020@ms.unimelb.edu.au>
	<40e66e0b0808201253i1da060fdke7d84a6f278af16f@mail.gmail.com>
Message-ID: <a46630750808210535y26e43cd9s881e556582cc9812@mail.gmail.com>

Thank you all for your help.

I'm now referring back to the discussion in Chapter 2 of Pinheiro and
Bates and understanding this much better.
Well, a little better.

In the figures on pp. 73-74, the middle panels (log-residual norm)
seem to illustrate what Douglas Bates has described here as

> "the penalty depend[ing] on the (unconditional) variance
> covariance matrix of the random effects.  When the variances are small
> there is a large penalty.  When the variances are large there is a
> small penalty on the size of the random effects."

And the bottom panels (log-determinant ratio) seem to illustrate

> The measure of model
> complexity, which is related to the determinant of the conditional
> variance of the random effects, given the data, [which] has the opposite
> behavior.  When the variance of the random effects is small the model
> is considered simpler.  The simplest possible model on this scale is
> one without any random effects at all, corresponding to a variance of
> zero.

In these charts, as you move all the way to the right, in the limit,
the values of Delta and theta are maximized, which I believe means the
random effect variance goes to zero (with respect to the residual
variance).

As you move to the left, your model complexity gets worse, but your
model fidelity improves for a time, and that's where you get the
maximum log-likelihood (top panel).

If theta going to infinity represents zero random effects, could you
say that theta going to zero represents random effects that are no
longer distinguishable from fixed effects?

D



From bates at stat.wisc.edu  Thu Aug 21 17:41:44 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 21 Aug 2008 10:41:44 -0500
Subject: [R-sig-ME] understanding log-likelihood/model fit
In-Reply-To: <a46630750808210535y26e43cd9s881e556582cc9812@mail.gmail.com>
References: <a46630750808191347v43e2828s1c36e504d8387a78@mail.gmail.com>
	<3278ABDD-0E3B-4284-9266-593AB38CF043@anu.edu.au>
	<a46630750808191719h31543d6ep5a91b2ca6019062c@mail.gmail.com>
	<D6B0DBE2-05A5-419D-B61D-43D296E64DCE@anu.edu.au>
	<a46630750808200601o702fc711wdd89b396e6231842@mail.gmail.com>
	<20080820192452.GO84020@ms.unimelb.edu.au>
	<40e66e0b0808201253i1da060fdke7d84a6f278af16f@mail.gmail.com>
	<a46630750808210535y26e43cd9s881e556582cc9812@mail.gmail.com>
Message-ID: <40e66e0b0808210841t66cbd27ta9708f1dad46c1f9@mail.gmail.com>

On Thu, Aug 21, 2008 at 7:35 AM, Daniel Ezra Johnson
<danielezrajohnson at gmail.com> wrote:
> Thank you all for your help.
>
> I'm now referring back to the discussion in Chapter 2 of Pinheiro and
> Bates and understanding this much better.
> Well, a little better.
>
> In the figures on pp. 73-74, the middle panels (log-residual norm)
> seem to illustrate what Douglas Bates has described here as
>
>> "the penalty depend[ing] on the (unconditional) variance
>> covariance matrix of the random effects.  When the variances are small
>> there is a large penalty.  When the variances are large there is a
>> small penalty on the size of the random effects."
>
> And the bottom panels (log-determinant ratio) seem to illustrate
>
>> The measure of model
>> complexity, which is related to the determinant of the conditional
>> variance of the random effects, given the data, [which] has the opposite
>> behavior.  When the variance of the random effects is small the model
>> is considered simpler.  The simplest possible model on this scale is
>> one without any random effects at all, corresponding to a variance of
>> zero.
>
> In these charts, as you move all the way to the right, in the limit,
> the values of Delta and theta are maximized, which I believe means the
> random effect variance goes to zero (with respect to the residual
> variance).
>
> As you move to the left, your model complexity gets worse, but your
> model fidelity improves for a time, and that's where you get the
> maximum log-likelihood (top panel).
>
> If theta going to infinity represents zero random effects, could you
> say that theta going to zero represents random effects that are no
> longer distinguishable from fixed effects?

Yes, in the sense that the residual sum of squares is what you would
obtain from a fixed-effects fit where the model matrix is the [X,Z]
where X is the model matrix for the fixed-effects parameters and Z is
the model matrix for the random effects.  The parameter estimates for
this model may not be well defined because the model could be
overparameterized.  However, the residual sum of squares for this
model is well-defined, as are the predictions and the residuals.

You could produce a similar plot for an lmer model fit.  Consider the
Dyestuff data in current versions of the lme4 package. The REML
criterion is similar to the maximum likelihood but it involves another
term so let's just consider the deviance and not the REML deviance.

> options(show.signif.stars = FALSE)
> (fm1 <- lmer(Yield ~ 1 + (1|Batch), Dyestuff, REML = FALSE, verbose = TRUE))
  0:     327.33176: 0.730297
  1:     327.33082: 0.772944
  2:     327.32706: 0.753271
  3:     327.32706: 0.752556
  4:     327.32706: 0.752581
Linear mixed model fit by maximum likelihood
Formula: Yield ~ 1 + (1 | Batch)
   Data: Dyestuff
   AIC   BIC logLik deviance REMLdev
 333.3 337.5 -163.7    327.3   319.7
Random effects:
 Groups   Name        Variance Std.Dev.
 Batch    (Intercept) 1388.1   37.258
 Residual             2451.3   49.511
Number of obs: 30, groups: Batch, 6

Fixed effects:
            Estimate Std. Error t value
(Intercept)  1527.50      17.69   86.33

The verbose output gives the profiled deviance at each iteration as a
function of a single parameter, which is the ratio of the standard
deviation of the random effects to the residual standard deviation.

> 37.258/49.511
[1] 0.7525196

This parameter is stored in the ST slot.  The slot is a list with
elements corresponding to the random effects terms in the model.  In
this model there is only one random effects term, (1|Batch).  In
general the elements of the list are square matrices where the number
of rows and columns corresponds to the number of random effects from
that term for each level of the grouping factor.  In this case there
is only one random effect per level of the factor.

> fm1 at ST
[[1]]
            (Intercept)
(Intercept)   0.7525135

The deviance slot contains the deviance itself and several of its components.

> fm1 at deviance
          ML         REML         ldL2        ldRX2      sigmaML    sigmaREML
  327.327060   319.725920     8.059354     2.057972    49.510754    50.357153
       pwrss         disc         usqr         wrss
73539.442307 62669.199627 10870.242680 62669.199627

For a linear mixed model, the discrepancy (disc) at the current
parameter values is equal to the weighted residual sum of squares
(wrss).  The random effects, b, are linearly transformed to u, for
which the unconditional distribution is a "spherical"
Gaussian distribution.  (i.e. the variance-covariance matrix is
\sigma^2 times the q by q identity matrix, where \sigma^2 is the same
scalar variance as in the definition of the conditional distribution
of Y|B).  The penalty on the size of the random effects is therefore
the squared length of u (usqr).  The penalized, weighted residual sum
of squares is the sum of wrss and usqr.  The conditional estimate of
\sigma^2 is pwrss/n and sigmaML is its square root.

> d <- fm1 at deviance
> all.equal(unname(d['pwrss']), unname(d['wrss'] + d['usqr']))
[1] TRUE
> fm1 at dims['n']
 n
30
> all.equal(unname(d['sigmaML']), unname(sqrt(d['pwrss']/30)))
[1] TRUE


The log-determinant of the conditional variance-covariance of the
random effects, given the data (i.e. B|Y) is, up to the scale factor
of \sigma, ldL2.  The profiled deviance is calculated as shown in
slide 132 of http://www.stat.wisc.edu/~bates/PotsdamGLMM/LMMD.pdf

The actual code in the C function lmm_update_fixef_u in lme4/src/lmer.c is

	d[ML_POS] = d[ldL2_POS] +
	    dn * (1 + log(d[pwrss_POS]) + log(2 * PI / dn));

or, in R,

> unname(d['ldL2'] + 30 * (1 + log(2 * pi * d['pwrss']/30)))
[1] 327.3271
> all.equal(unname(d['ML']), unname(d['ldL2'] + 30 * (1 + log(2 * pi * d['pwrss']/30))))
[1] TRUE

Next I was going to show how to change the value of the ST parameter
and update the deviance slot but in the process I discovered that one
of the C functions (the aforementioned lmm_update_fixef_u) cannot be
called from R.  After changing that and making a new release I can
show how to create a similar figure in the new formulation.



From ajmackey at gmail.com  Thu Aug 21 18:10:44 2008
From: ajmackey at gmail.com (Aaron Mackey)
Date: Thu, 21 Aug 2008 12:10:44 -0400
Subject: [R-sig-ME] [R] lmer syntax, matrix of (grouped) covariates?
In-Reply-To: <40e66e0b0808190504g6a6efb9bj63dae7fa5e5c5abe@mail.gmail.com>
References: <24c96eca0808180920t7033f866u1287a35a4d8fe572@mail.gmail.com>
	<40e66e0b0808181221p40183858y64a9cedfddd24da3@mail.gmail.com>
	<24c96eca0808181241xff27e26i2063193962852726@mail.gmail.com>
	<18602.28526.464291.817178@stat.math.ethz.ch>
	<40e66e0b0808190504g6a6efb9bj63dae7fa5e5c5abe@mail.gmail.com>
Message-ID: <24c96eca0808210910v7e01ee20h95145eaec9e9cdaf@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080821/445ff472/attachment.pl>

From danielezrajohnson at gmail.com  Sat Aug 23 00:22:24 2008
From: danielezrajohnson at gmail.com (Daniel Ezra Johnson)
Date: Fri, 22 Aug 2008 23:22:24 +0100
Subject: [R-sig-ME] binomial fixed-effect p-values by simulation
Message-ID: <a46630750808221522t2840eea2m1fccee0bbe7c434e@mail.gmail.com>

Consider the common situation of modeling multiple responses from
multiple subjects, where we want to know whether a fixed-effect
between-subjects variable -- for example, gender -- has a significant
effect.

In my field (quantitative sociolinguistics) the response is usually
binomial, and for some 30 years the usual procedure has been to carry
out logistic regression, setting the subject variable aside, and
assessing fixed effect significance with a likelihood-ratio test. When
between-subject variation is high, this produces exceptionally
anti-conservative p-values (see old.p.value below).

I am writing an article and some wrapper functions designed to
introduce the use of mixed models to this audience. With subject
included as a random effect, the p-values for fixed between-subject
effects are much more conservative, even using the likelihood-ratio
test (see lrt.p.value below). In other words, this is an improvement.

But as has been discussed many times on this list, likelihood-ratio
tests are still anti-conservative for fixed effect significance. I
believe the extent of anti-conservatism depends on the number of
observations relative to the number of groups, the balance of the
data, and probably other factors too.

I have seen the MCMC method discussed, but I think this is not yet
available for binomial data with lmer(), is that correct? In addition,
that method, which operates in the parameter space, seems to be
testing a slightly different null hypothesis than the usual
frequentist one, though I may be misunderstanding this.

Would it be appropriate for me to carry out a simulation of the type
given below, to assess fixed effect significance?

The code starts by generating a dataset which happens to have a
potentially significant gender effect, according to the LRT. (In fact
the data is randomly generated and has no "real" gender effect, but
assume it was real data and we did not know that.)

A null model (no fixed effect) and an alternative model (fixed gender
effect) are fit to the data by lmer(), using a random effect for
subject in both cases.

It then takes the two parameters from the null model -- intercept and
subject s.d. -- and randomly generates new data. The simulated data is
fit with the alternative-style model and the proportion of times the
fixed effect magnitude exceeds that from the 'observed' data is the
p-value.

Does this procedure make sense? It would generalize to linear models
easily. Making it work for unbalanced data and when there are other
fixed effects in the model would not be that hard either, I think.

To my mind, this is a direct implementation of the frequentist p-value
concept via simulation. Am I missing something important?

Of course, especially with binomial data this simulation is quite slow
(i.e. compared to the unimplemented MCMC method). The code below, with
1000 simulations of a 2000-observation dataset, took 15 minutes to run
on a MacBook Pro. (The result was a p-value of 0.027, compared to
0.015 from the LRT, 0.009 from the Wald statistic... and 1.1e-56 from
the fixed-effects model!)

It's important to me to be able to give p-values that are
accurate/conservative and that I understand. So if anyone can give me
guidance on whether this simulation procedure makes sense, it would be
much appreciated.

And if there's a faster way to do something like this for binomial
data, please let me know.

Thanks,
Daniel

library(lme4)
library(boot)
set.seed(10)

observed <- data.frame(subject = rep(letters[1:20],each=100),
	gender = rep(c("M","F"),each=1000),
	response = rbinom(2000,1,rep(plogis(rnorm(20,0,3)),each=100)))

old <- glm(response~gender,binomial,observed)
old.p.value <- pchisq(old$null.deviance-old$deviance,df=1,lower.tail=F)
# 1.083e-56

null <- lmer(response ~ (1|subject),observed,binomial)
fixed <- lmer(response ~ gender + (1|subject),observed,binomial)

wald.p.value <-  summary(fixed)@coefs["genderM","Pr(>|z|)"] # 0.00894
lrt.p.value <- anova(null,fixed)[,"Pr(>Chisq)"][2] # 0.0152

null.intercept <- fixef(null) # -0.0914
null.subject.sd <- attr(VarCorr(null)$subject,"stddev") # 2.276
gender.observed <- fixef(fixed)[2] # -2.319
gender.simulated <- vector()

for (i in 1:1000){
simulated <- data.frame(subject = rep(letters[1:20],each=100),
	gender = rep(c("M","F"),each=1000),
	response = rbinom(2000,1,rep(plogis(rnorm(20,0,null.subject.sd)),each=100)))
fixed.sim <- lmer(response~gender+(1|subject),simulated,binomial)
gender.simulated[i] <- fixef(fixed.sim)[2]}

# minutes later...

simulation.p.value <- mean(abs(gender.simulated)>=abs(gender.observed)) # 0.027



From bolker at ufl.edu  Sat Aug 23 01:40:16 2008
From: bolker at ufl.edu (Ben Bolker)
Date: Fri, 22 Aug 2008 18:40:16 -0500
Subject: [R-sig-ME] binomial fixed-effect p-values by simulation
In-Reply-To: <a46630750808221522t2840eea2m1fccee0bbe7c434e@mail.gmail.com>
References: <a46630750808221522t2840eea2m1fccee0bbe7c434e@mail.gmail.com>
Message-ID: <48AF4E60.8010507@ufl.edu>

  I just wrote a "simulate" method for glmer objects, to extend the
version that's already in lme4 for lmer objects.  Given a nested pair
of models m0, m1, the simulation distribution of t-statistics would
be

replicate(1000,get.t.statistic(refit(m1,simulate(m0))))

See simulate.lme in nlme, which also has a plot method
that displays the nominal and empirical p-values for REML
and ML estimates. 

  The bad news is that it's essentially doing the same thing you
discuss below -- and therefore it's not any faster (I was able
to do about 10 replicates/minute ...)

  If you're interested let me know and I can send my
"glmersim.R"
 
  Ben Bolker

Daniel Ezra Johnson wrote:
> Consider the common situation of modeling multiple responses from
> multiple subjects, where we want to know whether a fixed-effect
> between-subjects variable -- for example, gender -- has a significant
> effect.
>
> In my field (quantitative sociolinguistics) the response is usually
> binomial, and for some 30 years the usual procedure has been to carry
> out logistic regression, setting the subject variable aside, and
> assessing fixed effect significance with a likelihood-ratio test. When
> between-subject variation is high, this produces exceptionally
> anti-conservative p-values (see old.p.value below).
>
> I am writing an article and some wrapper functions designed to
> introduce the use of mixed models to this audience. With subject
> included as a random effect, the p-values for fixed between-subject
> effects are much more conservative, even using the likelihood-ratio
> test (see lrt.p.value below). In other words, this is an improvement.
>
> But as has been discussed many times on this list, likelihood-ratio
> tests are still anti-conservative for fixed effect significance. I
> believe the extent of anti-conservatism depends on the number of
> observations relative to the number of groups, the balance of the
> data, and probably other factors too.
>
> I have seen the MCMC method discussed, but I think this is not yet
> available for binomial data with lmer(), is that correct? In addition,
> that method, which operates in the parameter space, seems to be
> testing a slightly different null hypothesis than the usual
> frequentist one, though I may be misunderstanding this.
>
> Would it be appropriate for me to carry out a simulation of the type
> given below, to assess fixed effect significance?
>
> The code starts by generating a dataset which happens to have a
> potentially significant gender effect, according to the LRT. (In fact
> the data is randomly generated and has no "real" gender effect, but
> assume it was real data and we did not know that.)
>
> A null model (no fixed effect) and an alternative model (fixed gender
> effect) are fit to the data by lmer(), using a random effect for
> subject in both cases.
>
> It then takes the two parameters from the null model -- intercept and
> subject s.d. -- and randomly generates new data. The simulated data is
> fit with the alternative-style model and the proportion of times the
> fixed effect magnitude exceeds that from the 'observed' data is the
> p-value.
>
> Does this procedure make sense? It would generalize to linear models
> easily. Making it work for unbalanced data and when there are other
> fixed effects in the model would not be that hard either, I think.
>
> To my mind, this is a direct implementation of the frequentist p-value
> concept via simulation. Am I missing something important?
>
> Of course, especially with binomial data this simulation is quite slow
> (i.e. compared to the unimplemented MCMC method). The code below, with
> 1000 simulations of a 2000-observation dataset, took 15 minutes to run
> on a MacBook Pro. (The result was a p-value of 0.027, compared to
> 0.015 from the LRT, 0.009 from the Wald statistic... and 1.1e-56 from
> the fixed-effects model!)
>
> It's important to me to be able to give p-values that are
> accurate/conservative and that I understand. So if anyone can give me
> guidance on whether this simulation procedure makes sense, it would be
> much appreciated.
>
> And if there's a faster way to do something like this for binomial
> data, please let me know.
>
> Thanks,
> Daniel
>
> library(lme4)
> library(boot)
> set.seed(10)
>
> observed <- data.frame(subject = rep(letters[1:20],each=100),
> 	gender = rep(c("M","F"),each=1000),
> 	response = rbinom(2000,1,rep(plogis(rnorm(20,0,3)),each=100)))
>
> old <- glm(response~gender,binomial,observed)
> old.p.value <- pchisq(old$null.deviance-old$deviance,df=1,lower.tail=F)
> # 1.083e-56
>
> null <- lmer(response ~ (1|subject),observed,binomial)
> fixed <- lmer(response ~ gender + (1|subject),observed,binomial)
>
> wald.p.value <-  summary(fixed)@coefs["genderM","Pr(>|z|)"] # 0.00894
> lrt.p.value <- anova(null,fixed)[,"Pr(>Chisq)"][2] # 0.0152
>
> null.intercept <- fixef(null) # -0.0914
> null.subject.sd <- attr(VarCorr(null)$subject,"stddev") # 2.276
> gender.observed <- fixef(fixed)[2] # -2.319
> gender.simulated <- vector()
>
> for (i in 1:1000){
> simulated <- data.frame(subject = rep(letters[1:20],each=100),
> 	gender = rep(c("M","F"),each=1000),
> 	response = rbinom(2000,1,rep(plogis(rnorm(20,0,null.subject.sd)),each=100)))
> fixed.sim <- lmer(response~gender+(1|subject),simulated,binomial)
> gender.simulated[i] <- fixef(fixed.sim)[2]}
>
> # minutes later...
>
> simulation.p.value <- mean(abs(gender.simulated)>=abs(gender.observed)) # 0.027
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bolker at ufl.edu  Sat Aug 23 02:17:19 2008
From: bolker at ufl.edu (Ben Bolker)
Date: Fri, 22 Aug 2008 19:17:19 -0500
Subject: [R-sig-ME] binomial fixed-effect p-values by simulation
In-Reply-To: <a46630750808221557r376a4e66udc42434e36bdd67d@mail.gmail.com>
References: <a46630750808221522t2840eea2m1fccee0bbe7c434e@mail.gmail.com>	
	<48AF4E60.8010507@ufl.edu>
	<a46630750808221557r376a4e66udc42434e36bdd67d@mail.gmail.com>
Message-ID: <48AF570F.2030709@ufl.edu>

 
  I don't quite understand your question about the t-distribution -- do you
mean naively using the t-statistic to test the null hypothesis?  A few 
issues
here -- (1) this is a Wald test, hence not generally as accurate as model
comparison; (2) assuming the scale parameter is fixed at 1, I *think* 
technically
one should just do a Z-test rather than a t-test here -- but I'm not 
sure. I have
a vague memory of a reference saying that Z- is actually better than t- in
this case (besides just being theoretically justified).  (3) if you do 
quasilikelihood,
then you should do the t-test, but then you have to decide on appropriate
degrees of freedom (oh joy).  Arguably the null-model simulations could give
quite a bit of insight into the _real_ shape of the distribution of 
(est)/(s.e.)
under the null hypothesis -- i.e., is it t?  With how many df?

  Ben

Daniel Ezra Johnson wrote:
> Yes, that would be very helpful if you'd sent your simulation function.
>
> I'm encouraged to know that I'm on the right track with this, at least
> in theory.
> For empirical p-values I can't see most people being satisfied with,
> say, 100 replications and the level of error that would bring. So the
> slowness is a problem.
>
> When would someone use the t-statistics distribution, versus the way I
> did it by comparing fixed effect sizes between the simulation and the
> data? Or are those two essentially the same thing, statistically?
>
> Thanks,
> D
>
> On Sat, Aug 23, 2008 at 12:40 AM, Ben Bolker <bolker at ufl.edu> wrote:
>   
>>  I just wrote a "simulate" method for glmer objects, to extend the
>> version that's already in lme4 for lmer objects.  Given a nested pair
>> of models m0, m1, the simulation distribution of t-statistics would
>> be
>>
>> replicate(1000,get.t.statistic(refit(m1,simulate(m0))))
>>
>> See simulate.lme in nlme, which also has a plot method
>> that displays the nominal and empirical p-values for REML
>> and ML estimates.
>>  The bad news is that it's essentially doing the same thing you
>> discuss below -- and therefore it's not any faster (I was able
>> to do about 10 replicates/minute ...)
>>
>>  If you're interested let me know and I can send my
>> "glmersim.R"
>>
>>  Ben Bolker
>>
>> Daniel Ezra Johnson wrote:
>>     
>>> Consider the common situation of modeling multiple responses from
>>> multiple subjects, where we want to know whether a fixed-effect
>>> between-subjects variable -- for example, gender -- has a significant
>>> effect.
>>>
>>> In my field (quantitative sociolinguistics) the response is usually
>>> binomial, and for some 30 years the usual procedure has been to carry
>>> out logistic regression, setting the subject variable aside, and
>>> assessing fixed effect significance with a likelihood-ratio test. When
>>> between-subject variation is high, this produces exceptionally
>>> anti-conservative p-values (see old.p.value below).
>>>
>>> I am writing an article and some wrapper functions designed to
>>> introduce the use of mixed models to this audience. With subject
>>> included as a random effect, the p-values for fixed between-subject
>>> effects are much more conservative, even using the likelihood-ratio
>>> test (see lrt.p.value below). In other words, this is an improvement.
>>>
>>> But as has been discussed many times on this list, likelihood-ratio
>>> tests are still anti-conservative for fixed effect significance. I
>>> believe the extent of anti-conservatism depends on the number of
>>> observations relative to the number of groups, the balance of the
>>> data, and probably other factors too.
>>>
>>> I have seen the MCMC method discussed, but I think this is not yet
>>> available for binomial data with lmer(), is that correct? In addition,
>>> that method, which operates in the parameter space, seems to be
>>> testing a slightly different null hypothesis than the usual
>>> frequentist one, though I may be misunderstanding this.
>>>
>>> Would it be appropriate for me to carry out a simulation of the type
>>> given below, to assess fixed effect significance?
>>>
>>> The code starts by generating a dataset which happens to have a
>>> potentially significant gender effect, according to the LRT. (In fact
>>> the data is randomly generated and has no "real" gender effect, but
>>> assume it was real data and we did not know that.)
>>>
>>> A null model (no fixed effect) and an alternative model (fixed gender
>>> effect) are fit to the data by lmer(), using a random effect for
>>> subject in both cases.
>>>
>>> It then takes the two parameters from the null model -- intercept and
>>> subject s.d. -- and randomly generates new data. The simulated data is
>>> fit with the alternative-style model and the proportion of times the
>>> fixed effect magnitude exceeds that from the 'observed' data is the
>>> p-value.
>>>
>>> Does this procedure make sense? It would generalize to linear models
>>> easily. Making it work for unbalanced data and when there are other
>>> fixed effects in the model would not be that hard either, I think.
>>>
>>> To my mind, this is a direct implementation of the frequentist p-value
>>> concept via simulation. Am I missing something important?
>>>
>>> Of course, especially with binomial data this simulation is quite slow
>>> (i.e. compared to the unimplemented MCMC method). The code below, with
>>> 1000 simulations of a 2000-observation dataset, took 15 minutes to run
>>> on a MacBook Pro. (The result was a p-value of 0.027, compared to
>>> 0.015 from the LRT, 0.009 from the Wald statistic... and 1.1e-56 from
>>> the fixed-effects model!)
>>>
>>> It's important to me to be able to give p-values that are
>>> accurate/conservative and that I understand. So if anyone can give me
>>> guidance on whether this simulation procedure makes sense, it would be
>>> much appreciated.
>>>
>>> And if there's a faster way to do something like this for binomial
>>> data, please let me know.
>>>
>>> Thanks,
>>> Daniel
>>>
>>> library(lme4)
>>> library(boot)
>>> set.seed(10)
>>>
>>> observed <- data.frame(subject = rep(letters[1:20],each=100),
>>>        gender = rep(c("M","F"),each=1000),
>>>        response = rbinom(2000,1,rep(plogis(rnorm(20,0,3)),each=100)))
>>>
>>> old <- glm(response~gender,binomial,observed)
>>> old.p.value <- pchisq(old$null.deviance-old$deviance,df=1,lower.tail=F)
>>> # 1.083e-56
>>>
>>> null <- lmer(response ~ (1|subject),observed,binomial)
>>> fixed <- lmer(response ~ gender + (1|subject),observed,binomial)
>>>
>>> wald.p.value <-  summary(fixed)@coefs["genderM","Pr(>|z|)"] # 0.00894
>>> lrt.p.value <- anova(null,fixed)[,"Pr(>Chisq)"][2] # 0.0152
>>>
>>> null.intercept <- fixef(null) # -0.0914
>>> null.subject.sd <- attr(VarCorr(null)$subject,"stddev") # 2.276
>>> gender.observed <- fixef(fixed)[2] # -2.319
>>> gender.simulated <- vector()
>>>
>>> for (i in 1:1000){
>>> simulated <- data.frame(subject = rep(letters[1:20],each=100),
>>>        gender = rep(c("M","F"),each=1000),
>>>        response =
>>> rbinom(2000,1,rep(plogis(rnorm(20,0,null.subject.sd)),each=100)))
>>> fixed.sim <- lmer(response~gender+(1|subject),simulated,binomial)
>>> gender.simulated[i] <- fixef(fixed.sim)[2]}
>>>
>>> # minutes later...
>>>
>>> simulation.p.value <- mean(abs(gender.simulated)>=abs(gender.observed)) #
>>> 0.027
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>>       
>>     

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: glmersim.R
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080822/925dc8fe/attachment.pl>

From ken at kjbeath.com.au  Sat Aug 23 02:45:47 2008
From: ken at kjbeath.com.au (Ken Beath)
Date: Sat, 23 Aug 2008 10:45:47 +1000
Subject: [R-sig-ME] binomial fixed-effect p-values by simulation
In-Reply-To: <a46630750808221522t2840eea2m1fccee0bbe7c434e@mail.gmail.com>
References: <a46630750808221522t2840eea2m1fccee0bbe7c434e@mail.gmail.com>
Message-ID: <DEFC5433-66BD-4F25-9FE8-5F6A7757BB52@kjbeath.com.au>

Daniel,

This is almost parametric bootstrapping. Rather than looking at the  
distribution of the parameter estimates it looks at the distribution  
of the test statistic (z value) and should be an improvement. I'm not  
certain of the correct way but summary(fixed.sim)@coefs will return  
them.

Unless the sample size is small I wouldn't be particularly worried  
about the distribution of the fixed effects estiamtes and I expect in  
your example there wont be much difference.

Another option is to use the sandwich estimator with logistic  
regression.

Ken


On 23/08/2008, at 8:22 AM, Daniel Ezra Johnson wrote:

> Consider the common situation of modeling multiple responses from
> multiple subjects, where we want to know whether a fixed-effect
> between-subjects variable -- for example, gender -- has a significant
> effect.
>
> In my field (quantitative sociolinguistics) the response is usually
> binomial, and for some 30 years the usual procedure has been to carry
> out logistic regression, setting the subject variable aside, and
> assessing fixed effect significance with a likelihood-ratio test. When
> between-subject variation is high, this produces exceptionally
> anti-conservative p-values (see old.p.value below).
>
> I am writing an article and some wrapper functions designed to
> introduce the use of mixed models to this audience. With subject
> included as a random effect, the p-values for fixed between-subject
> effects are much more conservative, even using the likelihood-ratio
> test (see lrt.p.value below). In other words, this is an improvement.
>
> But as has been discussed many times on this list, likelihood-ratio
> tests are still anti-conservative for fixed effect significance. I
> believe the extent of anti-conservatism depends on the number of
> observations relative to the number of groups, the balance of the
> data, and probably other factors too.
>
> I have seen the MCMC method discussed, but I think this is not yet
> available for binomial data with lmer(), is that correct? In addition,
> that method, which operates in the parameter space, seems to be
> testing a slightly different null hypothesis than the usual
> frequentist one, though I may be misunderstanding this.
>
> Would it be appropriate for me to carry out a simulation of the type
> given below, to assess fixed effect significance?
>
> The code starts by generating a dataset which happens to have a
> potentially significant gender effect, according to the LRT. (In fact
> the data is randomly generated and has no "real" gender effect, but
> assume it was real data and we did not know that.)
>
> A null model (no fixed effect) and an alternative model (fixed gender
> effect) are fit to the data by lmer(), using a random effect for
> subject in both cases.
>
> It then takes the two parameters from the null model -- intercept and
> subject s.d. -- and randomly generates new data. The simulated data is
> fit with the alternative-style model and the proportion of times the
> fixed effect magnitude exceeds that from the 'observed' data is the
> p-value.
>
> Does this procedure make sense? It would generalize to linear models
> easily. Making it work for unbalanced data and when there are other
> fixed effects in the model would not be that hard either, I think.
>
> To my mind, this is a direct implementation of the frequentist p-value
> concept via simulation. Am I missing something important?
>
> Of course, especially with binomial data this simulation is quite slow
> (i.e. compared to the unimplemented MCMC method). The code below, with
> 1000 simulations of a 2000-observation dataset, took 15 minutes to run
> on a MacBook Pro. (The result was a p-value of 0.027, compared to
> 0.015 from the LRT, 0.009 from the Wald statistic... and 1.1e-56 from
> the fixed-effects model!)
>
> It's important to me to be able to give p-values that are
> accurate/conservative and that I understand. So if anyone can give me
> guidance on whether this simulation procedure makes sense, it would be
> much appreciated.
>
> And if there's a faster way to do something like this for binomial
> data, please let me know.
>
> Thanks,
> Daniel
>
> library(lme4)
> library(boot)
> set.seed(10)
>
> observed <- data.frame(subject = rep(letters[1:20],each=100),
> 	gender = rep(c("M","F"),each=1000),
> 	response = rbinom(2000,1,rep(plogis(rnorm(20,0,3)),each=100)))
>
> old <- glm(response~gender,binomial,observed)
> old.p.value <- pchisq(old$null.deviance-old 
> $deviance,df=1,lower.tail=F)
> # 1.083e-56
>
> null <- lmer(response ~ (1|subject),observed,binomial)
> fixed <- lmer(response ~ gender + (1|subject),observed,binomial)
>
> wald.p.value <-  summary(fixed)@coefs["genderM","Pr(>|z|)"] # 0.00894
> lrt.p.value <- anova(null,fixed)[,"Pr(>Chisq)"][2] # 0.0152
>
> null.intercept <- fixef(null) # -0.0914
> null.subject.sd <- attr(VarCorr(null)$subject,"stddev") # 2.276
> gender.observed <- fixef(fixed)[2] # -2.319
> gender.simulated <- vector()
>
> for (i in 1:1000){
> simulated <- data.frame(subject = rep(letters[1:20],each=100),
> 	gender = rep(c("M","F"),each=1000),
> 	response =  
> rbinom(2000,1,rep(plogis(rnorm(20,0,null.subject.sd)),each=100)))
> fixed.sim <- lmer(response~gender+(1|subject),simulated,binomial)
> gender.simulated[i] <- fixef(fixed.sim)[2]}
>
> # minutes later...
>
> simulation.p.value <-  
> mean(abs(gender.simulated)>=abs(gender.observed)) # 0.027
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Sat Aug 23 17:13:18 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 23 Aug 2008 10:13:18 -0500
Subject: [R-sig-ME] binomial fixed-effect p-values by simulation
In-Reply-To: <DEFC5433-66BD-4F25-9FE8-5F6A7757BB52@kjbeath.com.au>
References: <a46630750808221522t2840eea2m1fccee0bbe7c434e@mail.gmail.com>
	<DEFC5433-66BD-4F25-9FE8-5F6A7757BB52@kjbeath.com.au>
Message-ID: <40e66e0b0808230813v1cb233afx975ec126f151948e@mail.gmail.com>

My thanks to Daniel for an interesting topic for discussion on this
list.  Ben had earlier sent me private email with his code for
simulation from a glmer fit and i think that the first thing i should
do is to incorporate that code, with thanks to Ben, into the lme4
package so we can do these simulations more easily and carry on this
informative discussion.

On Fri, Aug 22, 2008 at 7:45 PM, Ken Beath <ken at kjbeath.com.au> wrote:
> Daniel,

> This is almost parametric bootstrapping. Rather than looking at the
> distribution of the parameter estimates it looks at the distribution of the
> test statistic (z value) and should be an improvement. I'm not certain of
> the correct way but summary(fixed.sim)@coefs will return them.

It may be somewhat faster to take the value of fixef(fixed.sim) and
vcov(fixed.sim) and use those to create the z statistics.  The z
statistics should be

fixef(fixed.sim)/sqrt(diag(vcov(fixed.sim)))

This will avoid creating the whole summary object.

> Unless the sample size is small I wouldn't be particularly worried about the
> distribution of the fixed effects estiamtes and I expect in your example
> there wont be much difference.
>
> Another option is to use the sandwich estimator with logistic regression.
>
> Ken
>
>
> On 23/08/2008, at 8:22 AM, Daniel Ezra Johnson wrote:
>
>> Consider the common situation of modeling multiple responses from
>> multiple subjects, where we want to know whether a fixed-effect
>> between-subjects variable -- for example, gender -- has a significant
>> effect.
>>
>> In my field (quantitative sociolinguistics) the response is usually
>> binomial, and for some 30 years the usual procedure has been to carry
>> out logistic regression, setting the subject variable aside, and
>> assessing fixed effect significance with a likelihood-ratio test. When
>> between-subject variation is high, this produces exceptionally
>> anti-conservative p-values (see old.p.value below).
>>
>> I am writing an article and some wrapper functions designed to
>> introduce the use of mixed models to this audience. With subject
>> included as a random effect, the p-values for fixed between-subject
>> effects are much more conservative, even using the likelihood-ratio
>> test (see lrt.p.value below). In other words, this is an improvement.
>>
>> But as has been discussed many times on this list, likelihood-ratio
>> tests are still anti-conservative for fixed effect significance. I
>> believe the extent of anti-conservatism depends on the number of
>> observations relative to the number of groups, the balance of the
>> data, and probably other factors too.
>>
>> I have seen the MCMC method discussed, but I think this is not yet
>> available for binomial data with lmer(), is that correct? In addition,
>> that method, which operates in the parameter space, seems to be
>> testing a slightly different null hypothesis than the usual
>> frequentist one, though I may be misunderstanding this.
>>
>> Would it be appropriate for me to carry out a simulation of the type
>> given below, to assess fixed effect significance?
>>
>> The code starts by generating a dataset which happens to have a
>> potentially significant gender effect, according to the LRT. (In fact
>> the data is randomly generated and has no "real" gender effect, but
>> assume it was real data and we did not know that.)
>>
>> A null model (no fixed effect) and an alternative model (fixed gender
>> effect) are fit to the data by lmer(), using a random effect for
>> subject in both cases.
>>
>> It then takes the two parameters from the null model -- intercept and
>> subject s.d. -- and randomly generates new data. The simulated data is
>> fit with the alternative-style model and the proportion of times the
>> fixed effect magnitude exceeds that from the 'observed' data is the
>> p-value.
>>
>> Does this procedure make sense? It would generalize to linear models
>> easily. Making it work for unbalanced data and when there are other
>> fixed effects in the model would not be that hard either, I think.
>>
>> To my mind, this is a direct implementation of the frequentist p-value
>> concept via simulation. Am I missing something important?
>>
>> Of course, especially with binomial data this simulation is quite slow
>> (i.e. compared to the unimplemented MCMC method). The code below, with
>> 1000 simulations of a 2000-observation dataset, took 15 minutes to run
>> on a MacBook Pro. (The result was a p-value of 0.027, compared to
>> 0.015 from the LRT, 0.009 from the Wald statistic... and 1.1e-56 from
>> the fixed-effects model!)
>>
>> It's important to me to be able to give p-values that are
>> accurate/conservative and that I understand. So if anyone can give me
>> guidance on whether this simulation procedure makes sense, it would be
>> much appreciated.
>>
>> And if there's a faster way to do something like this for binomial
>> data, please let me know.
>>
>> Thanks,
>> Daniel
>>
>> library(lme4)
>> library(boot)
>> set.seed(10)
>>
>> observed <- data.frame(subject = rep(letters[1:20],each=100),
>>        gender = rep(c("M","F"),each=1000),
>>        response = rbinom(2000,1,rep(plogis(rnorm(20,0,3)),each=100)))
>>
>> old <- glm(response~gender,binomial,observed)
>> old.p.value <- pchisq(old$null.deviance-old$deviance,df=1,lower.tail=F)
>> # 1.083e-56
>>
>> null <- lmer(response ~ (1|subject),observed,binomial)
>> fixed <- lmer(response ~ gender + (1|subject),observed,binomial)
>>
>> wald.p.value <-  summary(fixed)@coefs["genderM","Pr(>|z|)"] # 0.00894
>> lrt.p.value <- anova(null,fixed)[,"Pr(>Chisq)"][2] # 0.0152
>>
>> null.intercept <- fixef(null) # -0.0914
>> null.subject.sd <- attr(VarCorr(null)$subject,"stddev") # 2.276
>> gender.observed <- fixef(fixed)[2] # -2.319
>> gender.simulated <- vector()
>>
>> for (i in 1:1000){
>> simulated <- data.frame(subject = rep(letters[1:20],each=100),
>>        gender = rep(c("M","F"),each=1000),
>>        response =
>> rbinom(2000,1,rep(plogis(rnorm(20,0,null.subject.sd)),each=100)))
>> fixed.sim <- lmer(response~gender+(1|subject),simulated,binomial)
>> gender.simulated[i] <- fixef(fixed.sim)[2]}
>>
>> # minutes later...
>>
>> simulation.p.value <- mean(abs(gender.simulated)>=abs(gender.observed)) #
>> 0.027
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From danielezrajohnson at gmail.com  Sat Aug 23 18:16:17 2008
From: danielezrajohnson at gmail.com (Daniel Ezra Johnson)
Date: Sat, 23 Aug 2008 17:16:17 +0100
Subject: [R-sig-ME] binomial fixed-effect p-values by simulation
In-Reply-To: <40e66e0b0808230813v1cb233afx975ec126f151948e@mail.gmail.com>
References: <a46630750808221522t2840eea2m1fccee0bbe7c434e@mail.gmail.com>
	<DEFC5433-66BD-4F25-9FE8-5F6A7757BB52@kjbeath.com.au>
	<40e66e0b0808230813v1cb233afx975ec126f151948e@mail.gmail.com>
Message-ID: <a46630750808230916h676ba0aeh2b4febd0542a3568@mail.gmail.com>

What would be the difference between simulating the z-statistic (if
I'm getting this, it would be determining what proportion of the
simulations have a z-statistic as large as the one from the observed
data) versus doing the same thing with the difference of
log-likelihoods)?

One difference I see is that with the z-statistic approach there is no
need to fit a null model, only the alternative model (to data
generated by the null model)?

D



From ken at kjbeath.com.au  Sun Aug 24 09:30:46 2008
From: ken at kjbeath.com.au (Ken Beath)
Date: Sun, 24 Aug 2008 17:30:46 +1000
Subject: [R-sig-ME] binomial fixed-effect p-values by simulation
In-Reply-To: <a46630750808230916h676ba0aeh2b4febd0542a3568@mail.gmail.com>
References: <a46630750808221522t2840eea2m1fccee0bbe7c434e@mail.gmail.com>
	<DEFC5433-66BD-4F25-9FE8-5F6A7757BB52@kjbeath.com.au>
	<40e66e0b0808230813v1cb233afx975ec126f151948e@mail.gmail.com>
	<a46630750808230916h676ba0aeh2b4febd0542a3568@mail.gmail.com>
Message-ID: <CAFCA911-9EB1-4205-886F-AA8A59048F7A@kjbeath.com.au>

On 24/08/2008, at 2:16 AM, Daniel Ezra Johnson wrote:

> What would be the difference between simulating the z-statistic (if
> I'm getting this, it would be determining what proportion of the
> simulations have a z-statistic as large as the one from the observed
> data) versus doing the same thing with the difference of
> log-likelihoods)?
>
> One difference I see is that with the z-statistic approach there is no
> need to fit a null model, only the alternative model (to data
> generated by the null model)?
>
> D
>


They should produce similar results and which is better is probably of  
theoretical rather than practical interest.

  I will try this with a commercial program I have that does both.

Ken



From danielezrajohnson at gmail.com  Sun Aug 24 11:56:47 2008
From: danielezrajohnson at gmail.com (Daniel Ezra Johnson)
Date: Sun, 24 Aug 2008 10:56:47 +0100
Subject: [R-sig-ME] binomial fixed-effect p-values by simulation
In-Reply-To: <CAFCA911-9EB1-4205-886F-AA8A59048F7A@kjbeath.com.au>
References: <a46630750808221522t2840eea2m1fccee0bbe7c434e@mail.gmail.com>
	<DEFC5433-66BD-4F25-9FE8-5F6A7757BB52@kjbeath.com.au>
	<40e66e0b0808230813v1cb233afx975ec126f151948e@mail.gmail.com>
	<a46630750808230916h676ba0aeh2b4febd0542a3568@mail.gmail.com>
	<CAFCA911-9EB1-4205-886F-AA8A59048F7A@kjbeath.com.au>
Message-ID: <a46630750808240256r7ef30105sde36268b5017e347@mail.gmail.com>

We read, e.g. in Pinheiro and Bates, that one situation where
fixed-effect LRTs are anti-conservative is when the number of fixed
effect parameters being tested is large with respect to the number of
groups.

In the tests I'm doing, there's only a single binary fixed effect
factor being tested - a between-subjects factor like gender, as noted
earlier.

I'm finding evidence for pretty serious anti-conservatism here too
(e.g. LRT chi-square p=0.0001 vs. LRT simulation p=0.016), and I'm
working on some reproducible code to demonstrate this.



From bolker at ufl.edu  Sun Aug 24 16:39:17 2008
From: bolker at ufl.edu (Ben Bolker)
Date: Sun, 24 Aug 2008 09:39:17 -0500
Subject: [R-sig-ME] binomial fixed-effect p-values by simulation
In-Reply-To: <a46630750808240256r7ef30105sde36268b5017e347@mail.gmail.com>
References: <a46630750808221522t2840eea2m1fccee0bbe7c434e@mail.gmail.com>	<DEFC5433-66BD-4F25-9FE8-5F6A7757BB52@kjbeath.com.au>	<40e66e0b0808230813v1cb233afx975ec126f151948e@mail.gmail.com>	<a46630750808230916h676ba0aeh2b4febd0542a3568@mail.gmail.com>	<CAFCA911-9EB1-4205-886F-AA8A59048F7A@kjbeath.com.au>
	<a46630750808240256r7ef30105sde36268b5017e347@mail.gmail.com>
Message-ID: <48B17295.5010105@ufl.edu>

  The other criterion is that the number of blocks has to be large.
I have seen *no* rules of thumb for how large ... in the presentation
that Doug Bates posted about recently ( 
http://www.stat.wisc.edu/~bates/PotsdamGLMM/GLMMD.pdf )
p. 32, 1934 observations, 60 groups, he uses LRT ... (you could try running
your stuff on his example -- I think all the code etc is available from 
his web site)
to see how well this works -- although here he gets p=0.796, so 
anti-conservative
would just make that larger ...

  Ben


Daniel Ezra Johnson wrote:
> We read, e.g. in Pinheiro and Bates, that one situation where
> fixed-effect LRTs are anti-conservative is when the number of fixed
> effect parameters being tested is large with respect to the number of
> groups.
>
> In the tests I'm doing, there's only a single binary fixed effect
> factor being tested - a between-subjects factor like gender, as noted
> earlier.
>
> I'm finding evidence for pretty serious anti-conservatism here too
> (e.g. LRT chi-square p=0.0001 vs. LRT simulation p=0.016), and I'm
> working on some reproducible code to demonstrate this.
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From danielezrajohnson at gmail.com  Mon Aug 25 00:30:38 2008
From: danielezrajohnson at gmail.com (Daniel Ezra Johnson)
Date: Sun, 24 Aug 2008 23:30:38 +0100
Subject: [R-sig-ME] binomial fixed-effect p-values by simulation
In-Reply-To: <48B17295.5010105@ufl.edu>
References: <a46630750808221522t2840eea2m1fccee0bbe7c434e@mail.gmail.com>
	<DEFC5433-66BD-4F25-9FE8-5F6A7757BB52@kjbeath.com.au>
	<40e66e0b0808230813v1cb233afx975ec126f151948e@mail.gmail.com>
	<a46630750808230916h676ba0aeh2b4febd0542a3568@mail.gmail.com>
	<CAFCA911-9EB1-4205-886F-AA8A59048F7A@kjbeath.com.au>
	<a46630750808240256r7ef30105sde36268b5017e347@mail.gmail.com>
	<48B17295.5010105@ufl.edu>
Message-ID: <a46630750808241530s44051253k1824bd702df49c57@mail.gmail.com>

A quick update on my progress implementing the simulation significance
test. If anyone is interested I can try to provide reproducible
examples illustrating these points.

1) I found that occasionally, refit(model.mer,response.vector)
produced a different result than glmer(response.vector ~ ...). My
solution is to use glmer() rather than refit() for each simulation,
and it doesn't appear to make it that much slower.

2) I found that the LRT approach does not seem to work as well as
simulating the z-statistic. In the LRT approach, occasionally either
the null model or the alternative model would come out with a "crazy"
log-likelihood -- e.g. one LL would be -400, the other -21000 -- and
this happened often enough in the right direction to throw off the
empirical p-values. So I'm going ahead with the z-statistic approach
(the same datasets don't give a "crazy" z-statistic).

3) There's still the problem of what to do when the simulated model
does not converge or has a false convergence - whether or not it's
biasing the empirical p-value to throw away those runs. It probably
is, in that whether or not the false convergence occurs is probably
not unrelated to the size of the fixed effect. But it doesn't appear
to be a direct relationship, so I'm comfortable tossing those runs for
now.

Probably all of these things are happening more often than they
usually would because I'm testing this on a rather extreme type of
data: binomial, 20 subjects with a large subject standard deviation
(5) and only 100 observations per subject, so there end up being a lot
of subjects with invariant responses, 0% or 100%.

And since my "observed" data has a Wald fixed effect p-value of 0.0005
(the p-value from the z-score), it's not that easy to judge whether
the simulation is slightly more conservative in a reasonable amount of
computing time!

Well, 1000 runs just finished and I got 2 where abs(z) >= z.obs, so
empirical p is 0.002.

I can provide more details if anyone is interested. Thanks again for
your help with this.

Daniel



From danielezrajohnson at gmail.com  Mon Aug 25 10:38:38 2008
From: danielezrajohnson at gmail.com (Daniel Ezra Johnson)
Date: Mon, 25 Aug 2008 09:38:38 +0100
Subject: [R-sig-ME] binomial fixed-effect p-values by simulation
In-Reply-To: <48B17295.5010105@ufl.edu>
References: <a46630750808221522t2840eea2m1fccee0bbe7c434e@mail.gmail.com>
	<DEFC5433-66BD-4F25-9FE8-5F6A7757BB52@kjbeath.com.au>
	<40e66e0b0808230813v1cb233afx975ec126f151948e@mail.gmail.com>
	<a46630750808230916h676ba0aeh2b4febd0542a3568@mail.gmail.com>
	<CAFCA911-9EB1-4205-886F-AA8A59048F7A@kjbeath.com.au>
	<a46630750808240256r7ef30105sde36268b5017e347@mail.gmail.com>
	<48B17295.5010105@ufl.edu>
Message-ID: <a46630750808250138g5d20df93i9ec11aa6354876dc@mail.gmail.com>

Sorry if this has been covered elsewhere, but if my interest is in
testing a single fixed effect _term_ (all coefficients at once) is
there an appropriate statistic to simulate for a binomial model?

In other words, if I fit a linear model "glmodel" I can simulate one
of the F-statistics from anova(glmodel). If there's only one
coefficient for the term then F = t^2...

If I have a "glmmodel" I can do anova(glmmodel) but I wanted to make
sure the F-statistic reported there was a sensible thing to look at
since it wasn't quite the square of the z-statistic in the simple
case.

Maybe it doesn't have an F-distribution but it would still work well
as a single-number stand-in for the 'size of a fixed effect' in a
simulation...

Thanks,
D



From bates at stat.wisc.edu  Mon Aug 25 15:08:11 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 25 Aug 2008 08:08:11 -0500
Subject: [R-sig-ME] binomial fixed-effect p-values by simulation
In-Reply-To: <a46630750808250138g5d20df93i9ec11aa6354876dc@mail.gmail.com>
References: <a46630750808221522t2840eea2m1fccee0bbe7c434e@mail.gmail.com>
	<DEFC5433-66BD-4F25-9FE8-5F6A7757BB52@kjbeath.com.au>
	<40e66e0b0808230813v1cb233afx975ec126f151948e@mail.gmail.com>
	<a46630750808230916h676ba0aeh2b4febd0542a3568@mail.gmail.com>
	<CAFCA911-9EB1-4205-886F-AA8A59048F7A@kjbeath.com.au>
	<a46630750808240256r7ef30105sde36268b5017e347@mail.gmail.com>
	<48B17295.5010105@ufl.edu>
	<a46630750808250138g5d20df93i9ec11aa6354876dc@mail.gmail.com>
Message-ID: <40e66e0b0808250608o4c3a4ef1l900d40b2369276a9@mail.gmail.com>

On Mon, Aug 25, 2008 at 3:38 AM, Daniel Ezra Johnson
<danielezrajohnson at gmail.com> wrote:
> Sorry if this has been covered elsewhere, but if my interest is in
> testing a single fixed effect _term_ (all coefficients at once) is
> there an appropriate statistic to simulate for a binomial model?

> In other words, if I fit a linear model "glmodel" I can simulate one
> of the F-statistics from anova(glmodel). If there's only one
> coefficient for the term then F = t^2...

Is there an F statistic reported in anova(glmmodel)?  I had a bug in
the code that allowed such a calculation in some circumstances but
Antje Nuthmann pointed out to me  that it was incorrect and could
provoke an obscure error message.

The good news is that I changed the single-argument form of the anova
method in 0.999375-25 so the wrong results are not reported.  The bad
news is that no results are reported.  For a binomial or Poisson GLMM
I think the appropriate statistic would be a chi-square but I haven't
thought of exactly how it would be calculated.

> If I have a "glmmodel" I can do anova(glmmodel) but I wanted to make
> sure the F-statistic reported there was a sensible thing to look at
> since it wasn't quite the square of the z-statistic in the simple
> case.
>
> Maybe it doesn't have an F-distribution but it would still work well
> as a single-number stand-in for the 'size of a fixed effect' in a
> simulation...
>
> Thanks,
> D
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bolker at ufl.edu  Mon Aug 25 15:56:50 2008
From: bolker at ufl.edu (Ben Bolker)
Date: Mon, 25 Aug 2008 09:56:50 -0400
Subject: [R-sig-ME] binomial fixed-effect p-values by simulation
In-Reply-To: <a46630750808250138g5d20df93i9ec11aa6354876dc@mail.gmail.com>
References: <a46630750808221522t2840eea2m1fccee0bbe7c434e@mail.gmail.com>	
	<DEFC5433-66BD-4F25-9FE8-5F6A7757BB52@kjbeath.com.au>	
	<40e66e0b0808230813v1cb233afx975ec126f151948e@mail.gmail.com>	
	<a46630750808230916h676ba0aeh2b4febd0542a3568@mail.gmail.com>	
	<CAFCA911-9EB1-4205-886F-AA8A59048F7A@kjbeath.com.au>	
	<a46630750808240256r7ef30105sde36268b5017e347@mail.gmail.com>	
	<48B17295.5010105@ufl.edu>
	<a46630750808250138g5d20df93i9ec11aa6354876dc@mail.gmail.com>
Message-ID: <48B2BA22.3020704@ufl.edu>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

  My guess, based on Littell et al 2003, would be that
something like  p %*% V^{-1} %*% p would give you a
quadratic form that would be chi-squared distributed with
rank(V) df, or (with an estimated scale parameter) F-distributed
with (rank(V), n-whatever) df -- or at least nominally
so, and if you're going to simulate anyway you're going
to find out how it's _really_ distributed ...

[I have a glmer fit named "zz", and a factor named "status"
that I want to test all levels == 0 simultaneously]

params <- grep("^status",names(fixef(zz)))
fixef(zz)[params] %*% solve(vcov(zz)[params,params]) %*% fixef(zz)[params]

   cheers
    Ben




Daniel Ezra Johnson wrote:
> Sorry if this has been covered elsewhere, but if my interest is in
> testing a single fixed effect _term_ (all coefficients at once) is
> there an appropriate statistic to simulate for a binomial model?
> 
> In other words, if I fit a linear model "glmodel" I can simulate one
> of the F-statistics from anova(glmodel). If there's only one
> coefficient for the term then F = t^2...
> 
> If I have a "glmmodel" I can do anova(glmmodel) but I wanted to make
> sure the F-statistic reported there was a sensible thing to look at
> since it wasn't quite the square of the z-statistic in the simple
> case.
> 
> Maybe it doesn't have an F-distribution but it would still work well
> as a single-number stand-in for the 'size of a fixed effect' in a
> simulation...
> 
> Thanks,
> D

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.6 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFIsroic5UpGjwzenMRAncaAJ9y+Fza6/kU3ya/Bkd699i81/mDPQCfW4DA
YNHMEw6/cFePwAdvJN6mL7s=
=x8iG
-----END PGP SIGNATURE-----



From bates at stat.wisc.edu  Mon Aug 25 17:23:05 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 25 Aug 2008 10:23:05 -0500
Subject: [R-sig-ME] binomial fixed-effect p-values by simulation
In-Reply-To: <48B2BA22.3020704@ufl.edu>
References: <a46630750808221522t2840eea2m1fccee0bbe7c434e@mail.gmail.com>
	<DEFC5433-66BD-4F25-9FE8-5F6A7757BB52@kjbeath.com.au>
	<40e66e0b0808230813v1cb233afx975ec126f151948e@mail.gmail.com>
	<a46630750808230916h676ba0aeh2b4febd0542a3568@mail.gmail.com>
	<CAFCA911-9EB1-4205-886F-AA8A59048F7A@kjbeath.com.au>
	<a46630750808240256r7ef30105sde36268b5017e347@mail.gmail.com>
	<48B17295.5010105@ufl.edu>
	<a46630750808250138g5d20df93i9ec11aa6354876dc@mail.gmail.com>
	<48B2BA22.3020704@ufl.edu>
Message-ID: <40e66e0b0808250823q57a2763bla5a69a106ea57a9b@mail.gmail.com>

On Mon, Aug 25, 2008 at 8:56 AM, Ben Bolker <bolker at ufl.edu> wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
>  My guess, based on Littell et al 2003, would be that
> something like  p %*% V^{-1} %*% p would give you a
> quadratic form that would be chi-squared distributed with
> rank(V) df, or (with an estimated scale parameter) F-distributed
> with (rank(V), n-whatever) df -- or at least nominally
> so, and if you're going to simulate anyway you're going
> to find out how it's _really_ distributed ...
>
> [I have a glmer fit named "zz", and a factor named "status"
> that I want to test all levels == 0 simultaneously]
>
> params <- grep("^status",names(fixef(zz)))
> fixef(zz)[params] %*% solve(vcov(zz)[params,params]) %*% fixef(zz)[params]

(covers face and runs away screaming).

Umm, please don't use grep on names to determine which coefficients
are associated with a given factor.  That's what the terms and assign
attributes are for.

You split the fixed effects according to assign, as in the code for
the anova method.  For example

> gm1 <- glmer(r2 ~ btype + situ + mode + Gender*Anger + (1|id) + (1|item), VerbAgg, binomial)
> str(terms(gm1))
Classes 'terms', 'formula' length 3 r2 ~ btype + situ + mode + Gender * Anger
  ..- attr(*, "variables")= language list(r2, btype, situ, mode, Gender, Anger)
  ..- attr(*, "factors")= int [1:6, 1:6] 0 1 0 0 0 0 0 0 1 0 ...
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : chr [1:6] "r2" "btype" "situ" "mode" ...
  .. .. ..$ : chr [1:6] "btype" "situ" "mode" "Gender" ...
  ..- attr(*, "term.labels")= chr [1:6] "btype" "situ" "mode" "Gender" ...
  ..- attr(*, "order")= int [1:6] 1 1 1 1 1 2
  ..- attr(*, "intercept")= int 1
  ..- attr(*, "response")= int 1
  ..- attr(*, ".Environment")=<environment: R_GlobalEnv>
  ..- attr(*, "predvars")= language list(r2, btype, situ, mode, Gender, Anger)
  ..- attr(*, "dataClasses")= Named chr [1:6] "factor" "factor"
"factor" "factor" ...
  .. ..- attr(*, "names")= chr [1:6] "r2" "btype" "situ" "mode" ...
> attr(gm1 at X, "assign")
[1] 0 1 1 2 3 4 5 6

The assign attribute indicates that the first coefficient is the
intercept, the next two are associated with the first-order term
"btype", the next is associated with the first-order term "situ", ...,
the eighth is associated with the second-order term "Gender:Anger".

> Daniel Ezra Johnson wrote:
>> Sorry if this has been covered elsewhere, but if my interest is in
>> testing a single fixed effect _term_ (all coefficients at once) is
>> there an appropriate statistic to simulate for a binomial model?
>>
>> In other words, if I fit a linear model "glmodel" I can simulate one
>> of the F-statistics from anova(glmodel). If there's only one
>> coefficient for the term then F = t^2...
>>
>> If I have a "glmmodel" I can do anova(glmmodel) but I wanted to make
>> sure the F-statistic reported there was a sensible thing to look at
>> since it wasn't quite the square of the z-statistic in the simple
>> case.
>>
>> Maybe it doesn't have an F-distribution but it would still work well
>> as a single-number stand-in for the 'size of a fixed effect' in a
>> simulation...
>>
>> Thanks,
>> D
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.6 (GNU/Linux)
> Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org
>
> iD8DBQFIsroic5UpGjwzenMRAncaAJ9y+Fza6/kU3ya/Bkd699i81/mDPQCfW4DA
> YNHMEw6/cFePwAdvJN6mL7s=
> =x8iG
> -----END PGP SIGNATURE-----
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From a.renwick at abdn.ac.uk  Mon Aug 25 17:46:56 2008
From: a.renwick at abdn.ac.uk (Renwick, A. R.)
Date: Mon, 25 Aug 2008 16:46:56 +0100
Subject: [R-sig-ME] Lmer with family = quasipoisson
In-Reply-To: <40e66e0b0808250823q57a2763bla5a69a106ea57a9b@mail.gmail.com>
References: <a46630750808221522t2840eea2m1fccee0bbe7c434e@mail.gmail.com>
	<DEFC5433-66BD-4F25-9FE8-5F6A7757BB52@kjbeath.com.au>
	<40e66e0b0808230813v1cb233afx975ec126f151948e@mail.gmail.com>
	<a46630750808230916h676ba0aeh2b4febd0542a3568@mail.gmail.com>
	<CAFCA911-9EB1-4205-886F-AA8A59048F7A@kjbeath.com.au>
	<a46630750808240256r7ef30105sde36268b5017e347@mail.gmail.com>
	<48B17295.5010105@ufl.edu>
	<a46630750808250138g5d20df93i9ec11aa6354876dc@mail.gmail.com>
	<48B2BA22.3020704@ufl.edu>
	<40e66e0b0808250823q57a2763bla5a69a106ea57a9b@mail.gmail.com>
Message-ID: <B9D1301370916C44B5874AF340C18B9B28AB9904CF@VMAILB.uoa.abdn.ac.uk>


I was wondering if anybody could explain why an AIC can be calucluated for a lmer model with a quasi poisson distriution and not a glm.
I have also found that the AIC's are extremely sensitive in the lmer model, for example removing a term with very low t-values resulted in an increase in AIC of 284.
Many thanks,
Anna

MODEL1:

mix<-lmer(realdis~sex+width+sess+sex:width+sex:sess+sess:width+(1|site),family=quasipoisson,data=move,subset=-52)
> summary(mix)
Generalized linear mixed model fit by the Laplace approximation
Formula: realdis ~ sex + width + sess + sex:width + sex:sess + sess:width +      (1 | site)
   Data: move
 Subset: -52
   AIC   BIC logLik deviance
 11019 11117  -5490    10979
Random effects:
 Groups   Name        Variance Std.Dev.
 site     (Intercept)  9.1673  3.0278
 Residual             76.4532  8.7438
Number of obs: 976, groups: site, 14

Fixed effects:
             Estimate Std. Error t value
(Intercept)   2.76589    0.91021  3.0387
sexm          0.17425    0.41856  0.4163
widthn        0.26750    0.99495  0.2689
widthw       -0.47361    0.55549 -0.8526
sess2        -0.43403    0.64479 -0.6731
sess3        -0.16228    0.85141 -0.1906
sess4        -0.40075    0.63427 -0.6318
sexm:widthn   0.78322    0.63070  1.2418
sexm:widthw   0.39836    0.44387  0.8975
sexm:sess2    0.02061    0.51386  0.0401
sexm:sess3    0.17900    0.52713  0.3396
sexm:sess4    0.36188    0.53644  0.6746
widthn:sess2  0.53589    0.83917  0.6386
widthw:sess2 -0.11500    0.65798 -0.1748
widthn:sess3 -0.19799    1.03927 -0.1905
widthw:sess3 -0.99033    0.86773 -1.1413
widthn:sess4 -0.17498    0.94033 -0.1861
widthw:sess4 -1.03199    0.75236 -1.3717

REMOVE WIDTH:SESS INTERACTION
 mix1<-lmer(realdis~sex+width+sess+sex:width+sex:sess+(1|site),family=quasipoisson,data=move,subset=-52)
> summary(mix1)
Generalized linear mixed model fit by the Laplace approximation
Formula: realdis ~ sex + width + sess + sex:width + sex:sess + (1 | site)
   Data: move
 Subset: -52
   AIC   BIC logLik deviance
 11303 11372  -5638    11275
Random effects:
 Groups   Name        Variance Std.Dev.
 site     (Intercept)  6.8207  2.6117
 Residual             84.5884  9.1972
Number of obs: 976, groups: site, 14

Fixed effects:
            Estimate Std. Error t value
(Intercept)   3.0503     0.7807   3.907
sexm          0.1780     0.4249   0.419
widthn        0.1302     0.7417   0.176
widthw       -0.7784     0.5217  -1.492
sess2        -0.4633     0.4770  -0.971
sess3        -0.9002     0.4859  -1.853
sess4        -0.9513     0.4733  -2.010
sexm:widthn   0.7431     0.6553   1.134
sexm:widthw   0.3856     0.4592   0.840
sexm:sess2    0.0963     0.5314   0.181
sexm:sess3    0.2302     0.5511   0.418
sexm:sess4    0.3524     0.5481   0.643

Change in AIC =  11303 - 11019 = an increase of 284


The University of Aberdeen is a charity registered in Scotland, No SC013683.



From bates at stat.wisc.edu  Mon Aug 25 17:57:27 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 25 Aug 2008 10:57:27 -0500
Subject: [R-sig-ME] Lmer with family = quasipoisson
In-Reply-To: <B9D1301370916C44B5874AF340C18B9B28AB9904CF@VMAILB.uoa.abdn.ac.uk>
References: <a46630750808221522t2840eea2m1fccee0bbe7c434e@mail.gmail.com>
	<40e66e0b0808230813v1cb233afx975ec126f151948e@mail.gmail.com>
	<a46630750808230916h676ba0aeh2b4febd0542a3568@mail.gmail.com>
	<CAFCA911-9EB1-4205-886F-AA8A59048F7A@kjbeath.com.au>
	<a46630750808240256r7ef30105sde36268b5017e347@mail.gmail.com>
	<48B17295.5010105@ufl.edu>
	<a46630750808250138g5d20df93i9ec11aa6354876dc@mail.gmail.com>
	<48B2BA22.3020704@ufl.edu>
	<40e66e0b0808250823q57a2763bla5a69a106ea57a9b@mail.gmail.com>
	<B9D1301370916C44B5874AF340C18B9B28AB9904CF@VMAILB.uoa.abdn.ac.uk>
Message-ID: <40e66e0b0808250857v4a686efavd939f99e5f6cd728@mail.gmail.com>

I wouldn't put too much faith in the values of the AIC etc. for the
quasipoisson family.  I included that family as a convenience and I
believe that the parameter estimates make sense but I don't know about
the deviance and log-likelihood values.  I'm still working out the
effect of the common scale parameter in that family.

On Mon, Aug 25, 2008 at 10:46 AM, Renwick, A. R. <a.renwick at abdn.ac.uk> wrote:
>
> I was wondering if anybody could explain why an AIC can be calucluated for a lmer model with a quasi poisson distriution and not a glm.
> I have also found that the AIC's are extremely sensitive in the lmer model, for example removing a term with very low t-values resulted in an increase in AIC of 284.
> Many thanks,
> Anna
>
> MODEL1:
>
> mix<-lmer(realdis~sex+width+sess+sex:width+sex:sess+sess:width+(1|site),family=quasipoisson,data=move,subset=-52)
>> summary(mix)
> Generalized linear mixed model fit by the Laplace approximation
> Formula: realdis ~ sex + width + sess + sex:width + sex:sess + sess:width +      (1 | site)
>   Data: move
>  Subset: -52
>   AIC   BIC logLik deviance
>  11019 11117  -5490    10979
> Random effects:
>  Groups   Name        Variance Std.Dev.
>  site     (Intercept)  9.1673  3.0278
>  Residual             76.4532  8.7438
> Number of obs: 976, groups: site, 14
>
> Fixed effects:
>             Estimate Std. Error t value
> (Intercept)   2.76589    0.91021  3.0387
> sexm          0.17425    0.41856  0.4163
> widthn        0.26750    0.99495  0.2689
> widthw       -0.47361    0.55549 -0.8526
> sess2        -0.43403    0.64479 -0.6731
> sess3        -0.16228    0.85141 -0.1906
> sess4        -0.40075    0.63427 -0.6318
> sexm:widthn   0.78322    0.63070  1.2418
> sexm:widthw   0.39836    0.44387  0.8975
> sexm:sess2    0.02061    0.51386  0.0401
> sexm:sess3    0.17900    0.52713  0.3396
> sexm:sess4    0.36188    0.53644  0.6746
> widthn:sess2  0.53589    0.83917  0.6386
> widthw:sess2 -0.11500    0.65798 -0.1748
> widthn:sess3 -0.19799    1.03927 -0.1905
> widthw:sess3 -0.99033    0.86773 -1.1413
> widthn:sess4 -0.17498    0.94033 -0.1861
> widthw:sess4 -1.03199    0.75236 -1.3717
>
> REMOVE WIDTH:SESS INTERACTION
>  mix1<-lmer(realdis~sex+width+sess+sex:width+sex:sess+(1|site),family=quasipoisson,data=move,subset=-52)
>> summary(mix1)
> Generalized linear mixed model fit by the Laplace approximation
> Formula: realdis ~ sex + width + sess + sex:width + sex:sess + (1 | site)
>   Data: move
>  Subset: -52
>   AIC   BIC logLik deviance
>  11303 11372  -5638    11275
> Random effects:
>  Groups   Name        Variance Std.Dev.
>  site     (Intercept)  6.8207  2.6117
>  Residual             84.5884  9.1972
> Number of obs: 976, groups: site, 14
>
> Fixed effects:
>            Estimate Std. Error t value
> (Intercept)   3.0503     0.7807   3.907
> sexm          0.1780     0.4249   0.419
> widthn        0.1302     0.7417   0.176
> widthw       -0.7784     0.5217  -1.492
> sess2        -0.4633     0.4770  -0.971
> sess3        -0.9002     0.4859  -1.853
> sess4        -0.9513     0.4733  -2.010
> sexm:widthn   0.7431     0.6553   1.134
> sexm:widthw   0.3856     0.4592   0.840
> sexm:sess2    0.0963     0.5314   0.181
> sexm:sess3    0.2302     0.5511   0.418
> sexm:sess4    0.3524     0.5481   0.643
>
> Change in AIC =  11303 - 11019 = an increase of 284
>
>
> The University of Aberdeen is a charity registered in Scotland, No SC013683.
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bolker at ufl.edu  Mon Aug 25 19:28:35 2008
From: bolker at ufl.edu (Ben Bolker)
Date: Mon, 25 Aug 2008 13:28:35 -0400
Subject: [R-sig-ME] binomial fixed-effect p-values by simulation
In-Reply-To: <40e66e0b0808250823q57a2763bla5a69a106ea57a9b@mail.gmail.com>
References: <a46630750808221522t2840eea2m1fccee0bbe7c434e@mail.gmail.com>	
	<DEFC5433-66BD-4F25-9FE8-5F6A7757BB52@kjbeath.com.au>	
	<40e66e0b0808230813v1cb233afx975ec126f151948e@mail.gmail.com>	
	<a46630750808230916h676ba0aeh2b4febd0542a3568@mail.gmail.com>	
	<CAFCA911-9EB1-4205-886F-AA8A59048F7A@kjbeath.com.au>	
	<a46630750808240256r7ef30105sde36268b5017e347@mail.gmail.com>	
	<48B17295.5010105@ufl.edu>	
	<a46630750808250138g5d20df93i9ec11aa6354876dc@mail.gmail.com>	
	<48B2BA22.3020704@ufl.edu>
	<40e66e0b0808250823q57a2763bla5a69a106ea57a9b@mail.gmail.com>
Message-ID: <48B2EBC3.60809@ufl.edu>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Douglas Bates wrote:
> On Mon, Aug 25, 2008 at 8:56 AM, Ben Bolker <bolker at ufl.edu> wrote:
>  My guess, based on Littell et al 2003, would be that
> something like  p %*% V^{-1} %*% p would give you a
> quadratic form that would be chi-squared distributed with
> rank(V) df, or (with an estimated scale parameter) F-distributed
> with (rank(V), n-whatever) df -- or at least nominally
> so, and if you're going to simulate anyway you're going
> to find out how it's _really_ distributed ...
> 
> [I have a glmer fit named "zz", and a factor named "status"
> that I want to test all levels == 0 simultaneously]
> 
> params <- grep("^status",names(fixef(zz)))
> fixef(zz)[params] %*% solve(vcov(zz)[params,params]) %*% fixef(zz)[params]
> 
>> (covers face and runs away screaming).
> 
>> Umm, please don't use grep on names to determine which coefficients
>> are associated with a given factor.  That's what the terms and assign
>> attributes are for.
> 
>> You split the fixed effects according to assign, as in the code for
>> the anova method.  For example
> 
> gm1 <- glmer(r2 ~ btype + situ + mode + Gender*Anger + (1|id) + (1|item), VerbAgg, binomial)
> str(terms(gm1))
>> Classes 'terms', 'formula' length 3 r2 ~ btype + situ + mode + Gender * Anger
>>   ..- attr(*, "variables")= language list(r2, btype, situ, mode, Gender, Anger)
>>   ..- attr(*, "factors")= int [1:6, 1:6] 0 1 0 0 0 0 0 0 1 0 ...
>>   .. ..- attr(*, "dimnames")=List of 2
>>   .. .. ..$ : chr [1:6] "r2" "btype" "situ" "mode" ...
>>   .. .. ..$ : chr [1:6] "btype" "situ" "mode" "Gender" ...
>>   ..- attr(*, "term.labels")= chr [1:6] "btype" "situ" "mode" "Gender" ...
>>   ..- attr(*, "order")= int [1:6] 1 1 1 1 1 2
>>   ..- attr(*, "intercept")= int 1
>>   ..- attr(*, "response")= int 1
>>   ..- attr(*, ".Environment")=<environment: R_GlobalEnv>
>>   ..- attr(*, "predvars")= language list(r2, btype, situ, mode, Gender, Anger)
>>   ..- attr(*, "dataClasses")= Named chr [1:6] "factor" "factor"
>> "factor" "factor" ...
>>   .. ..- attr(*, "names")= chr [1:6] "r2" "btype" "situ" "mode" ...
> attr(gm1 at X, "assign")
>> [1] 0 1 1 2 3 4 5 6
> 
>> The assign attribute indicates that the first coefficient is the
>> intercept, the next two are associated with the first-order term
>> "btype", the next is associated with the first-order term "situ", ...,
>> the eighth is associated with the second-order term "Gender:Anger".
> 

   Sorry about the running-away-screaming part.
  I'm just happy that you don't disagree that vehemently with the
statistical (as opposed to the R-programming) part of my advice ...

  thanks
   Ben
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.6 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFIsuvCc5UpGjwzenMRAi0sAKCDy5ktn0LdgUNYSUCstEbpEqMa5gCfUzlt
gOv6ckTkE8V7dSMzPildRBI=
=vTxi
-----END PGP SIGNATURE-----



From brandon at brandoninvergo.com  Tue Aug 26 02:30:30 2008
From: brandon at brandoninvergo.com (brandon at brandoninvergo.com)
Date: Mon, 25 Aug 2008 19:30:30 -0500
Subject: [R-sig-ME] Variance explained by random factor
In-Reply-To: <B9D1301370916C44B5874AF340C18B9B28AB99047F@VMAILB.uoa.abdn.ac.uk>
References: <B9D1301370916C44B5874AF340C18B9B28AB99047F@VMAILB.uoa.abdn.ac.uk>
Message-ID: <bc6ddb6be79d42eba6763b0ed4a24675@brandoninvergo.com>

On Wed, 13 Aug 2008 16:17:48 +0100, "Renwick, A. R." <a.renwick at abdn.ac.uk>
wrote:
> 
> I am currently trying to run a lmer model with poisson distrubution.  I
> tested the model with a model without the random effect and it inferred
> that I should include the random effect:
> 
> ma1<-glm(RoundedOverlap~sess+breedfem,family=poisson,data=Male)
>
mixed<-lmer(RoundedOverlap~sess+breedfem+sess:breedfem+(1|Site),family=poisson,data=Male)
> 
> #test to see if sig difference between glm and glmm
> as.numeric(2*(logLik(mixed)-logLik(ma)))
> #99.16136
> pchisq(99.16136,1,lower=FALSE)
> #2.327441e-23  so should use a GLMM
> 
> However,the model output that I get states that the variance explained by
> the random factor is 0:
> 
> 
> Generalized linear mixed model fit by the Laplace approximation
> Formula: RoundedOverlap ~ sess + breedfem + sess:breedfem + (1 | Site)
>    Data: Male
>    AIC   BIC logLik deviance
>  109.9 127.2 -45.93    91.86
> Random effects:
>  Groups Name        Variance Std.Dev.
>  Site   (Intercept)  0        0
> Number of obs: 51, groups: Site, 14
> 
> I would really appreciate if somebody could help me understand why the
> variance is 0.
> Many thanks,
> Anna


I'm having a similar problem as Anna, however the responses in this thread
don't seem to apply to my problem. In my case, when I fit a GLMM to my data
(family = Gamma), the variance of the random effect is effectively zero
(1e-12). I also fit a GLM to the data and this model has practically the
same estimates and t-values for all the parameters. However, when I check
for significant differences between the two models via the log-likelihood
ratio test, the result is highly significant. I understand that a model
without the random effects term will be interpreted as a simpler model and
thus it will have a lower log-likelihood value but I don't understand how
the addition of a random effects term with such a small variance can cause
such a large increase in the log-likelihood of the model. Am I missing
something obvious here?

Thanks for your help!
-brandon


Here's my output:

> mixed.model <- lmer(dev.time ~ sex*temp + (1|sleeve.in.temp),
data=data.clean, family=Gamma(link="log"), method="ML")
> summary(mixed.model)
Generalized linear mixed model fit using Laplace 
Formula: dev.time ~ sex * temp + (1 | sleeve.in.temp) 
   Data: data.clean 
 Family: Gamma(log link)
   AIC   BIC logLik deviance
 29.28 87.32 -3.639    7.277
Random effects:
 Groups         Name        Variance   Std.Dev.  
 sleeve.in.temp (Intercept) 2.5683e-12 1.6026e-06
 Residual                   5.1366e-03 7.1670e-02
number of obs: 1446, groups: sleeve.in.temp, 54

Fixed effects:
             Estimate Std. Error t value
(Intercept)  3.659500   0.005993   610.6
sexm        -0.088450   0.008956    -9.9
temp21      -0.207295   0.008132   -25.5
temp23      -0.433156   0.008363   -51.8
temp25      -0.612696   0.008491   -72.2
temp27      -0.755628   0.008297   -91.1
sexm:temp21  0.008189   0.012000     0.7
sexm:temp23 -0.003357   0.012243    -0.3
sexm:temp25 -0.014887   0.012321    -1.2
sexm:temp27  0.010674   0.012405     0.9

Correlation of Fixed Effects:
            (Intr) sexm   temp21 temp23 temp25 temp27 sxm:21 sxm:23 sxm:25
sexm        -0.669                                                 
temp21      -0.737  0.493                                          
temp23      -0.717  0.480  0.528                                   
temp25      -0.706  0.472  0.520  0.506                            
temp27      -0.722  0.483  0.532  0.518  0.510                     
sexm:temp21  0.499 -0.746 -0.678 -0.358 -0.353 -0.361              
sexm:temp23  0.490 -0.731 -0.361 -0.683 -0.346 -0.354  0.546       
sexm:temp25  0.486 -0.727 -0.358 -0.349 -0.689 -0.351  0.542  0.532
sexm:temp27  0.483 -0.722 -0.356 -0.346 -0.341 -0.669  0.539  0.528 0.525


> glm.model <- glm(dev.time ~ sex*temp, data=data.clean,
family=Gamma(link="log"), na.action=na.omit)
> summary(glm.model)

Call:
glm(formula = dev.time ~ sex * temp, family = Gamma(link = "log"), 
    data = data.clean, na.action = na.omit)

Deviance Residuals: 
      Min         1Q     Median         3Q        Max  
-0.217280  -0.050703  -0.004718   0.043783   0.292775  

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  3.659429   0.006014 608.474   <2e-16 ***
sexm        -0.088440   0.008987  -9.841   <2e-16 ***
temp21      -0.207203   0.008161 -25.391   <2e-16 ***
temp23      -0.433163   0.008392 -51.617   <2e-16 ***
temp25      -0.612562   0.008520 -71.895   <2e-16 ***
temp27      -0.755615   0.008326 -90.752   <2e-16 ***
sexm:temp21  0.008232   0.012041   0.684    0.494    
sexm:temp23 -0.003237   0.012285  -0.264    0.792    
sexm:temp25 -0.015077   0.012363  -1.220    0.223    
sexm:temp27  0.010813   0.012448   0.869    0.385    
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1
? ? 1 

(Dispersion parameter for Gamma family taken to be 0.005172238)

    Null deviance: 114.2564  on 1445  degrees of freedom
Residual deviance:   7.2771  on 1436  degrees of freedom
AIC: 5762.4

Number of Fisher Scoring iterations: 3
 

> as.numeric(-2*(logLik(glm.model)-logLik(mixed.model)))
[1] 5733.084
> pchisq(5733.084,1,lower=FALSE)
[1] 0

I checked the model's deviance as mentioned my Douglas Bates in this thread
and I got this:
> mixed.model at deviance
      ML     REML 
7.277151       NA



From h.wickham at gmail.com  Tue Aug 26 03:34:12 2008
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 25 Aug 2008 20:34:12 -0500
Subject: [R-sig-ME] Variance explained by random factor
In-Reply-To: <40e66e0b0808140243s66203400g58bf73bbc0c33233@mail.gmail.com>
References: <B9D1301370916C44B5874AF340C18B9B28AB99047F@VMAILB.uoa.abdn.ac.uk>
	<E23EC85F-9F09-4319-BBAB-BFAA47862D08@kjbeath.com.au>
	<40e66e0b0808140221s2f2bd401h78c71672de7f58b9@mail.gmail.com>
	<B9D1301370916C44B5874AF340C18B9B28AB990481@VMAILB.uoa.abdn.ac.uk>
	<40e66e0b0808140243s66203400g58bf73bbc0c33233@mail.gmail.com>
Message-ID: <f8e6ff050808251834i4df9517eq9941f4e7d3a27841@mail.gmail.com>

On Thu, Aug 14, 2008 at 4:43 AM, Douglas Bates <bates at stat.wisc.edu> wrote:
> On Thu, Aug 14, 2008 at 11:24 AM, Renwick, A. R. <a.renwick at abdn.ac.uk> wrote:
>> Many apologise but the glm model I compared was ma not ma1 and thus did have the interaction term:
>
>> ma<-glm(RoundedOverlap~sess+breedfem+sess:breedfem ,family=poisson,data=Male)
>> mixed<-lmer(RoundedOverlap~sess+breedfem+sess:breedfem+(1|Site),family=poisson,data=Male)
>
> In that case it could be that the deviance or log-likelihood is not
> being evaluated correctly in glmer.  Look at the slot named 'deviance'
> in the lmer fit.  It should be a named numeric vector.  The names of
> interest are 'disc', the discrepancy for the generalized linear models
> (this is the deviance without the compensation for the null deviance),
> 'ldL2', the logarithm of the square of the determinant of the Cholesky
> factor of a second-order term, and usqr, the squared length of the
> transformed random effects.  For a mixed-effects model in which the
> variance of the random effects is estimated as zero, both 'ldL2' and
> 'usqr' should be zero.
>
> You can check these values in
>
> mixed at deviance

But isn't possible that the log-likelihoods are incompatible between
glm and glmer?  Maybe one chose to drop an irrelevant constant and the
other didn't.

lmer currently doesn't let you specify a model without a random
effect, but I think this gets pretty close:

counts <- c(18,17,15,20,10,20,25,13,12)
outcome <- gl(3,1,9)
fixed <- rep(1, 9)
d.AD <- data.frame(treatment, outcome, counts)

m_glm <- glm(counts ~ outcome, family = poisson)
m_lmer <- glmer(counts ~ outcome + (1 | fixed), family = poisson)

> logLik(m_glm)
'log Lik.' -23.38066 (df=3)
> logLik(m_lmer)
'log Lik.' -2.564571 (df=4)

Although they do have the same deviance:

> deviance(m_lmer)
      ML
5.129141
> deviance(m_glm)
[1] 5.129141


Hadley

-- 
http://had.co.nz/



From ken at kjbeath.com.au  Tue Aug 26 07:29:40 2008
From: ken at kjbeath.com.au (Ken Beath)
Date: Tue, 26 Aug 2008 15:29:40 +1000
Subject: [R-sig-ME] Variance explained by random factor
In-Reply-To: <bc6ddb6be79d42eba6763b0ed4a24675@brandoninvergo.com>
References: <B9D1301370916C44B5874AF340C18B9B28AB99047F@VMAILB.uoa.abdn.ac.uk>
	<bc6ddb6be79d42eba6763b0ed4a24675@brandoninvergo.com>
Message-ID: <130563B1-6DCB-464D-92C8-CA244D6218CC@kjbeath.com.au>

On 26/08/2008, at 10:30 AM, <brandon at brandoninvergo.com> wrote:
>
> I'm having a similar problem as Anna, however the responses in this  
> thread
> don't seem to apply to my problem. In my case, when I fit a GLMM to  
> my data
> (family = Gamma), the variance of the random effect is effectively  
> zero
> (1e-12). I also fit a GLM to the data and this model has practically  
> the
> same estimates and t-values for all the parameters. However, when I  
> check
> for significant differences between the two models via the log- 
> likelihood
> ratio test, the result is highly significant. I understand that a  
> model
> without the random effects term will be interpreted as a simpler  
> model and
> thus it will have a lower log-likelihood value but I don't  
> understand how
> the addition of a random effects term with such a small variance can  
> cause
> such a large increase in the log-likelihood of the model. Am I missing
> something obvious here?
>

In addition to the problem with the missing constants in one of the  
likelihoods, there is also a scale parameter in the likelihood for  
Gamma so twice the LL difference is no longer distributed as a  
chisquare.

Ken


> Thanks for your help!
> -brandon
>
>
> Here's my output:
>
>> mixed.model <- lmer(dev.time ~ sex*temp + (1|sleeve.in.temp),
> data=data.clean, family=Gamma(link="log"), method="ML")
>> summary(mixed.model)
> Generalized linear mixed model fit using Laplace
> Formula: dev.time ~ sex * temp + (1 | sleeve.in.temp)
>   Data: data.clean
> Family: Gamma(log link)
>   AIC   BIC logLik deviance
> 29.28 87.32 -3.639    7.277
> Random effects:
> Groups         Name        Variance   Std.Dev.
> sleeve.in.temp (Intercept) 2.5683e-12 1.6026e-06
> Residual                   5.1366e-03 7.1670e-02
> number of obs: 1446, groups: sleeve.in.temp, 54
>
> Fixed effects:
>             Estimate Std. Error t value
> (Intercept)  3.659500   0.005993   610.6
> sexm        -0.088450   0.008956    -9.9
> temp21      -0.207295   0.008132   -25.5
> temp23      -0.433156   0.008363   -51.8
> temp25      -0.612696   0.008491   -72.2
> temp27      -0.755628   0.008297   -91.1
> sexm:temp21  0.008189   0.012000     0.7
> sexm:temp23 -0.003357   0.012243    -0.3
> sexm:temp25 -0.014887   0.012321    -1.2
> sexm:temp27  0.010674   0.012405     0.9
>
> Correlation of Fixed Effects:
>            (Intr) sexm   temp21 temp23 temp25 temp27 sxm:21 sxm:23  
> sxm:25
> sexm        -0.669
> temp21      -0.737  0.493
> temp23      -0.717  0.480  0.528
> temp25      -0.706  0.472  0.520  0.506
> temp27      -0.722  0.483  0.532  0.518  0.510
> sexm:temp21  0.499 -0.746 -0.678 -0.358 -0.353 -0.361
> sexm:temp23  0.490 -0.731 -0.361 -0.683 -0.346 -0.354  0.546
> sexm:temp25  0.486 -0.727 -0.358 -0.349 -0.689 -0.351  0.542  0.532
> sexm:temp27  0.483 -0.722 -0.356 -0.346 -0.341 -0.669  0.539  0.528  
> 0.525
>
>
>> glm.model <- glm(dev.time ~ sex*temp, data=data.clean,
> family=Gamma(link="log"), na.action=na.omit)
>> summary(glm.model)
>
> Call:
> glm(formula = dev.time ~ sex * temp, family = Gamma(link = "log"),
>    data = data.clean, na.action = na.omit)
>
> Deviance Residuals:
>      Min         1Q     Median         3Q        Max
> -0.217280  -0.050703  -0.004718   0.043783   0.292775
>
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept)  3.659429   0.006014 608.474   <2e-16 ***
> sexm        -0.088440   0.008987  -9.841   <2e-16 ***
> temp21      -0.207203   0.008161 -25.391   <2e-16 ***
> temp23      -0.433163   0.008392 -51.617   <2e-16 ***
> temp25      -0.612562   0.008520 -71.895   <2e-16 ***
> temp27      -0.755615   0.008326 -90.752   <2e-16 ***
> sexm:temp21  0.008232   0.012041   0.684    0.494
> sexm:temp23 -0.003237   0.012285  -0.264    0.792
> sexm:temp25 -0.015077   0.012363  -1.220    0.223
> sexm:temp27  0.010813   0.012448   0.869    0.385
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1
> ? ? 1
>
> (Dispersion parameter for Gamma family taken to be 0.005172238)
>
>    Null deviance: 114.2564  on 1445  degrees of freedom
> Residual deviance:   7.2771  on 1436  degrees of freedom
> AIC: 5762.4
>
> Number of Fisher Scoring iterations: 3
>
>
>> as.numeric(-2*(logLik(glm.model)-logLik(mixed.model)))
> [1] 5733.084
>> pchisq(5733.084,1,lower=FALSE)
> [1] 0
>
> I checked the model's deviance as mentioned my Douglas Bates in this  
> thread
> and I got this:
>> mixed.model at deviance
>      ML     REML
> 7.277151       NA
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From bates at stat.wisc.edu  Tue Aug 26 15:35:31 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 26 Aug 2008 08:35:31 -0500
Subject: [R-sig-ME] Variance explained by random factor
In-Reply-To: <130563B1-6DCB-464D-92C8-CA244D6218CC@kjbeath.com.au>
References: <B9D1301370916C44B5874AF340C18B9B28AB99047F@VMAILB.uoa.abdn.ac.uk>
	<bc6ddb6be79d42eba6763b0ed4a24675@brandoninvergo.com>
	<130563B1-6DCB-464D-92C8-CA244D6218CC@kjbeath.com.au>
Message-ID: <40e66e0b0808260635n40d85b50nc0590a82241e9833@mail.gmail.com>

On Tue, Aug 26, 2008 at 12:29 AM, Ken Beath <ken at kjbeath.com.au> wrote:
> On 26/08/2008, at 10:30 AM, <brandon at brandoninvergo.com> wrote:
>>
>> I'm having a similar problem as Anna, however the responses in this thread
>> don't seem to apply to my problem. In my case, when I fit a GLMM to my
>> data
>> (family = Gamma), the variance of the random effect is effectively zero
>> (1e-12). I also fit a GLM to the data and this model has practically the
>> same estimates and t-values for all the parameters. However, when I check
>> for significant differences between the two models via the log-likelihood
>> ratio test, the result is highly significant. I understand that a model
>> without the random effects term will be interpreted as a simpler model and
>> thus it will have a lower log-likelihood value but I don't understand how
>> the addition of a random effects term with such a small variance can cause
>> such a large increase in the log-likelihood of the model. Am I missing
>> something obvious here?

> In addition to the problem with the missing constants in one of the
> likelihoods, there is also a scale parameter in the likelihood for Gamma so
> twice the LL difference is no longer distributed as a chisquare.

Yesterday I was looking at what is done in the glm function.  Several
glm families (the non-"quasi" ones) have a member function to evaluate
the AIC then use the AIC value to determine the log-likelihood.  I'm
not sure how that will extend to the generalized linear mixed model.
I think I will be able to reconstruct a log-likelihood with the
appropriate scaling factors for the binomial, poisson, gaussian and
Gamma families.  I don't know about the inverse Gaussian and my guess
is that returning NA for the log-likelihood of the quasi families is a
sensible approach.  Ben mentioned the QAIC definition which may be
reasonable.

I was considering these questions because we wanted to check the
results from the AGQ method developed by Bin Dai as a Google Summer of
Code project.  In version 0.999375-26, which is now on CRAN, one can
specify nAGQ > 1 in glmer and nlmer but only for models with a single
grouping factor for the random effects.


>> Thanks for your help!
>> -brandon
>>
>>
>> Here's my output:
>>
>>> mixed.model <- lmer(dev.time ~ sex*temp + (1|sleeve.in.temp),
>>
>> data=data.clean, family=Gamma(link="log"), method="ML")
>>>
>>> summary(mixed.model)
>>
>> Generalized linear mixed model fit using Laplace
>> Formula: dev.time ~ sex * temp + (1 | sleeve.in.temp)
>>  Data: data.clean
>> Family: Gamma(log link)
>>  AIC   BIC logLik deviance
>> 29.28 87.32 -3.639    7.277
>> Random effects:
>> Groups         Name        Variance   Std.Dev.
>> sleeve.in.temp (Intercept) 2.5683e-12 1.6026e-06
>> Residual                   5.1366e-03 7.1670e-02
>> number of obs: 1446, groups: sleeve.in.temp, 54
>>
>> Fixed effects:
>>            Estimate Std. Error t value
>> (Intercept)  3.659500   0.005993   610.6
>> sexm        -0.088450   0.008956    -9.9
>> temp21      -0.207295   0.008132   -25.5
>> temp23      -0.433156   0.008363   -51.8
>> temp25      -0.612696   0.008491   -72.2
>> temp27      -0.755628   0.008297   -91.1
>> sexm:temp21  0.008189   0.012000     0.7
>> sexm:temp23 -0.003357   0.012243    -0.3
>> sexm:temp25 -0.014887   0.012321    -1.2
>> sexm:temp27  0.010674   0.012405     0.9
>>
>> Correlation of Fixed Effects:
>>           (Intr) sexm   temp21 temp23 temp25 temp27 sxm:21 sxm:23 sxm:25
>> sexm        -0.669
>> temp21      -0.737  0.493
>> temp23      -0.717  0.480  0.528
>> temp25      -0.706  0.472  0.520  0.506
>> temp27      -0.722  0.483  0.532  0.518  0.510
>> sexm:temp21  0.499 -0.746 -0.678 -0.358 -0.353 -0.361
>> sexm:temp23  0.490 -0.731 -0.361 -0.683 -0.346 -0.354  0.546
>> sexm:temp25  0.486 -0.727 -0.358 -0.349 -0.689 -0.351  0.542  0.532
>> sexm:temp27  0.483 -0.722 -0.356 -0.346 -0.341 -0.669  0.539  0.528 0.525
>>
>>
>>> glm.model <- glm(dev.time ~ sex*temp, data=data.clean,
>>
>> family=Gamma(link="log"), na.action=na.omit)
>>>
>>> summary(glm.model)
>>
>> Call:
>> glm(formula = dev.time ~ sex * temp, family = Gamma(link = "log"),
>>   data = data.clean, na.action = na.omit)
>>
>> Deviance Residuals:
>>     Min         1Q     Median         3Q        Max
>> -0.217280  -0.050703  -0.004718   0.043783   0.292775
>>
>> Coefficients:
>>            Estimate Std. Error t value Pr(>|t|)
>> (Intercept)  3.659429   0.006014 608.474   <2e-16 ***
>> sexm        -0.088440   0.008987  -9.841   <2e-16 ***
>> temp21      -0.207203   0.008161 -25.391   <2e-16 ***
>> temp23      -0.433163   0.008392 -51.617   <2e-16 ***
>> temp25      -0.612562   0.008520 -71.895   <2e-16 ***
>> temp27      -0.755615   0.008326 -90.752   <2e-16 ***
>> sexm:temp21  0.008232   0.012041   0.684    0.494
>> sexm:temp23 -0.003237   0.012285  -0.264    0.792
>> sexm:temp25 -0.015077   0.012363  -1.220    0.223
>> sexm:temp27  0.010813   0.012448   0.869    0.385
>> ---
>> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1
>> ' ' 1
>>
>> (Dispersion parameter for Gamma family taken to be 0.005172238)
>>
>>   Null deviance: 114.2564  on 1445  degrees of freedom
>> Residual deviance:   7.2771  on 1436  degrees of freedom
>> AIC: 5762.4
>>
>> Number of Fisher Scoring iterations: 3
>>
>>
>>> as.numeric(-2*(logLik(glm.model)-logLik(mixed.model)))
>>
>> [1] 5733.084
>>>
>>> pchisq(5733.084,1,lower=FALSE)
>>
>> [1] 0
>>
>> I checked the model's deviance as mentioned my Douglas Bates in this
>> thread
>> and I got this:
>>>
>>> mixed.model at deviance
>>
>>     ML     REML
>> 7.277151       NA
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From danielezrajohnson at gmail.com  Tue Aug 26 16:19:37 2008
From: danielezrajohnson at gmail.com (Daniel Ezra Johnson)
Date: Tue, 26 Aug 2008 15:19:37 +0100
Subject: [R-sig-ME] binomial fixed-effect p-values by simulation
In-Reply-To: <40e66e0b0808250608o4c3a4ef1l900d40b2369276a9@mail.gmail.com>
References: <a46630750808221522t2840eea2m1fccee0bbe7c434e@mail.gmail.com>
	<DEFC5433-66BD-4F25-9FE8-5F6A7757BB52@kjbeath.com.au>
	<40e66e0b0808230813v1cb233afx975ec126f151948e@mail.gmail.com>
	<a46630750808230916h676ba0aeh2b4febd0542a3568@mail.gmail.com>
	<CAFCA911-9EB1-4205-886F-AA8A59048F7A@kjbeath.com.au>
	<a46630750808240256r7ef30105sde36268b5017e347@mail.gmail.com>
	<48B17295.5010105@ufl.edu>
	<a46630750808250138g5d20df93i9ec11aa6354876dc@mail.gmail.com>
	<40e66e0b0808250608o4c3a4ef1l900d40b2369276a9@mail.gmail.com>
Message-ID: <a46630750808260719k16d141a4kf4d96bce06ffc368@mail.gmail.com>

sorry for the naive question - how can one inspect the code for a
method rather than a function - like the anova method for lmer?
D



From bates at stat.wisc.edu  Tue Aug 26 16:38:10 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 26 Aug 2008 09:38:10 -0500
Subject: [R-sig-ME] binomial fixed-effect p-values by simulation
In-Reply-To: <a46630750808260719k16d141a4kf4d96bce06ffc368@mail.gmail.com>
References: <a46630750808221522t2840eea2m1fccee0bbe7c434e@mail.gmail.com>
	<DEFC5433-66BD-4F25-9FE8-5F6A7757BB52@kjbeath.com.au>
	<40e66e0b0808230813v1cb233afx975ec126f151948e@mail.gmail.com>
	<a46630750808230916h676ba0aeh2b4febd0542a3568@mail.gmail.com>
	<CAFCA911-9EB1-4205-886F-AA8A59048F7A@kjbeath.com.au>
	<a46630750808240256r7ef30105sde36268b5017e347@mail.gmail.com>
	<48B17295.5010105@ufl.edu>
	<a46630750808250138g5d20df93i9ec11aa6354876dc@mail.gmail.com>
	<40e66e0b0808250608o4c3a4ef1l900d40b2369276a9@mail.gmail.com>
	<a46630750808260719k16d141a4kf4d96bce06ffc368@mail.gmail.com>
Message-ID: <40e66e0b0808260738k346ab9dfscd35141f56b7b17@mail.gmail.com>

On Tue, Aug 26, 2008 at 9:19 AM, Daniel Ezra Johnson
<danielezrajohnson at gmail.com> wrote:
> sorry for the naive question - how can one inspect the code for a
> method rather than a function - like the anova method for lmer?

The simple way is to check the files in the source code package,
because they include the comments as well as the code.  The source
code archive is stored at http://r-forge.r-projects.org/projects/lme4
under the SCM tab.  A direct URL is

http://r-forge.r-project.org/plugins/scmsvn/viewcvs.php/?root=lme4

The most recent version of the source code for essentially everything
associated with mer objects is in the file pkg/R/lmer.R

In general if you want to examine methods you first decide if you have
an S3 method or an S4 method.  S3 method names are returned by

methods(anova)

and S4 method names by

showMethods("anova")

To display the code for a particular S4 method use

getMethod("anova", "mer")



From bates at stat.wisc.edu  Wed Aug 27 00:02:01 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 26 Aug 2008 17:02:01 -0500
Subject: [R-sig-ME] Variance explained by random factor
In-Reply-To: <48B4192F.2070407@zoology.ufl.edu>
References: <B9D1301370916C44B5874AF340C18B9B28AB99047F@VMAILB.uoa.abdn.ac.uk>
	<bc6ddb6be79d42eba6763b0ed4a24675@brandoninvergo.com>
	<130563B1-6DCB-464D-92C8-CA244D6218CC@kjbeath.com.au>
	<40e66e0b0808260635n40d85b50nc0590a82241e9833@mail.gmail.com>
	<48B4192F.2070407@zoology.ufl.edu>
Message-ID: <40e66e0b0808261502i6107ed6fm5ad4b1b78a505f5a@mail.gmail.com>

On Tue, Aug 26, 2008 at 9:54 AM, Ben Bolker <bolker at zoology.ufl.edu> wrote:
> Douglas Bates wrote:
>> Yesterday I was looking at what is done in the glm function.  Several
>> glm families (the non-"quasi" ones) have a member function to evaluate
>> the AIC then use the AIC value to determine the log-likelihood.  I'm
>> not sure how that will extend to the generalized linear mixed model.
>> I think I will be able to reconstruct a log-likelihood with the
>> appropriate scaling factors for the binomial, poisson, gaussian and
>> Gamma families.  I don't know about the inverse Gaussian and my guess
>> is that returning NA for the log-likelihood of the quasi families is a
>> sensible approach.  Ben mentioned the QAIC definition which may be
>> reasonable.
>
>  Does it seem backwards to anyone else that GLM etc. stores the
> AIC and then reconstructs the number of parameters in order to
> calculate the log-likelihood (rather than, e.g., storing the
> log-likelihood and the number of parameters)?

It does indeed seem a bit backwards.  I'm not sure why the decision to
do it that way was made.  I haven't played around with GLMs that much
until I started working on GLMMs.



From danielezrajohnson at gmail.com  Wed Aug 27 23:08:55 2008
From: danielezrajohnson at gmail.com (Daniel Ezra Johnson)
Date: Wed, 27 Aug 2008 22:08:55 +0100
Subject: [R-sig-ME] lmer predicted values with a subset of random effects
Message-ID: <a46630750808271408s30d79d6i70ce0a5936c54d8a@mail.gmail.com>

Is there a fairly simple way to manipulate the "mer" slots to get
predicted/fitted values which include only a subset of the random
effects BLUPs? For example, the predictions at one or another level if
the effects were nested (as in nlme), or with one or the other of
crossed random effects?

Thanks,
Dan



From bates at stat.wisc.edu  Thu Aug 28 01:06:39 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 27 Aug 2008 18:06:39 -0500
Subject: [R-sig-ME] lmer predicted values with a subset of random effects
In-Reply-To: <a46630750808271408s30d79d6i70ce0a5936c54d8a@mail.gmail.com>
References: <a46630750808271408s30d79d6i70ce0a5936c54d8a@mail.gmail.com>
Message-ID: <40e66e0b0808271606i77951dbn8d924c1e44fc2d9c@mail.gmail.com>

On Wed, Aug 27, 2008 at 4:08 PM, Daniel Ezra Johnson
<danielezrajohnson at gmail.com> wrote:
> Is there a fairly simple way to manipulate the "mer" slots to get
> predicted/fitted values which include only a subset of the random
> effects BLUPs? For example, the predictions at one or another level if
> the effects were nested (as in nlme), or with one or the other of
> crossed random effects?

The important slots for this are

fixef - vector of fixed-effects parameter estimates
X - model matrix for the fixed-effects
ranef - vector of conditional modes of the random effects
Zt - transpose of the model matrix for the random effects
Gp - "group pointers" for the random effects (0-based) (see below).
flist - named list of grouping factors.  Has an "assign" attribute.
dims - an integer vector of dimensions and characteristics of the model.

Let nt be the number of random-effects terms in the model formula and
let nf be the number of grouping factors.  That is, nf =
length(flist).  Each term is mapped to an element for flist through
the assign attribute of flist.  This is to allow for models with
formulas like

Reaction ~ Days + (1|Subject) + (0+Days|Subject)

to associate the multiple random effects with the grouping factors.  Check

library(lme4)
example(lmer)
str(fm2 at flist)

It should look like

> str(fm2 at flist)
'data.frame':	180 obs. of  1 variable:
 $ Subject: Factor w/ 18 levels "308","309","310",..: 1 1 1 1 1 1 1 1 1 1 ...
 - attr(*, "assign")= int  1 1

The dims slot contains an element called "nest" indicating if the
grouping factors are nested.  I think it is properly assigned but
since I never use it in the lmer code I can't guarantee that.  Proceed
with caution.

The Gp slot has length nt + 1.  It tells you where the random effects
for each of the random-effects terms start in the ranef slot.  The
indices are 0-based so they are off-by-1 for R indices.

> fm2 at Gp
[1]  0 18 36

If you want to get predictions incorporating random-effects for
particular grouping factors then you should go through the assign
attribute of flist to associate grouping factors with terms.

After that it is just a matter of doing the matrix multiplications.


I just wrote some code to do that, which I attach.

From yuanjiayj at gmail.com  Thu Aug 28 08:59:51 2008
From: yuanjiayj at gmail.com (yuanjiayj)
Date: Thu, 28 Aug 2008 14:59:51 +0800
Subject: [R-sig-ME] air quality predict
Message-ID: <c496045b0808272359t215fb9c7y1d05afec109d0ba3@mail.gmail.com>

Hi,anyone can give me some proposal  about the "air quality predict"?
I find the "Multivariate Linear Models" is a far-fetched method.



From maechler at stat.math.ethz.ch  Thu Aug 28 10:51:35 2008
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 28 Aug 2008 10:51:35 +0200
Subject: [R-sig-ME] air quality predict
In-Reply-To: <c496045b0808272359t215fb9c7y1d05afec109d0ba3@mail.gmail.com>
References: <c496045b0808272359t215fb9c7y1d05afec109d0ba3@mail.gmail.com>
Message-ID: <18614.26391.600167.792926@stat.math.ethz.ch>

>>>>> "y" == yuanjiayj  <yuanjiayj at gmail.com>
>>>>>     on Thu, 28 Aug 2008 14:59:51 +0800 writes:

    y> Hi,anyone can give me some proposal  about the "air quality predict"?
    y> I find the "Multivariate Linear Models" is a far-fetched method.

- Your question is not precise enough ... by far !

- I'm pretty sure that even if your question was much more precise,
  this would be the wrong mailing list.

Please

  1. read (and ponder and understand!) the posting guide

     --> http://www.r-project.org/posting-guide.html

  2. *after that* ask your question on the R-help mailing list.


Regards,
Martin Maechler, ETH Zurich



From danielezrajohnson at gmail.com  Thu Aug 28 20:07:30 2008
From: danielezrajohnson at gmail.com (Daniel Ezra Johnson)
Date: Thu, 28 Aug 2008 19:07:30 +0100
Subject: [R-sig-ME] fixed effect testing again (but different)
Message-ID: <a46630750808281107u582a779fvec72e695356d1713@mail.gmail.com>

Thanks to all of your help, I have a simulation method running to
assess fixed effect significance. I have a more conceptual question
today.

Again, the fixed effects I'm usually interested in testing are
"between-subject" or "outer" to a subject random effect. I'll continue
using the example of gender since that's a simple and clear one.

Up until now my procedure is to evaluate two models, viz.

m0 <- lmer(response~(1|subject),data)
m1 <- lmer(response~gender+(1|subject),data)

The likelihood-ratio test approach directly compares these two.
The simulation approach generates data from the parameters of m0, and
compares the resulting test statistic to its value from m1.

However, the more I think about it, I think this might not be the best
way to go.
Basically the way I see the question is, "is the difference between
men and women in the data greater than what might occur by chance?"

In order to answer that, we need some estimate of how much subjects
vary within each gender group, or perhaps we could say how much they
vary, apart from gender variation. We have two obvious estimates of
this quantity: the subject random effect variance parameter of m0 and
of m1.

So far all my simulation attempts have used the parameter from m0 as
the basis for the simulation.
I'm starting to think that doesn't make sense.

In models of the form of m0, the subject random effect is large, and
it "substitutes" for the fixed effect. I mean that by accurately (and
fully, given large and balanced datasets) estimating the behavior of
individual subjects, it necessarily accounts for the between-subject
effect at the same time. (In particular, the random effects are not
constrained to a normal distribution in practice, even though they are
supposed to be normally distributed in theory.)

I'm beginning to wonder if it wouldn't make sense to do simulations
(and maybe LRT's) using, as the null model, a model with no fixed
effects but with the subject random effect parameter taken from m1,
not m0. The alternative model would still be the full m1.

Otherwise it seems as though the fixed effect (if there is one)
basically makes its way into the subject variance parameter of the
null model, increasing it, and leading eventually to an overly
conservative test.

If it seems wrong to take the parameter from m1, is there some way to
change the specification of m0 so as to obtain a separate random
effect for males and females? I've seen that done but I've forgotten
how to do it.

If I can run something like

m00 <- lmer(response~(1|subject-females)+(1|subject-males),data)   #
how is this specified?

then simulated data could be generated from the parameters of that
model (not the random-effect estimates, only the overall variance
parameters), and then that data could be fit to

m11 <- lmer(response~gender+(1|subject-females)+(1|subject-males),data)

to evaluate the fixed effect... A nice side effect is if there is
heteroscedasticity between males and females, that's built right in.

Daniel



From HDoran at air.org  Thu Aug 28 20:13:14 2008
From: HDoran at air.org (Doran, Harold)
Date: Thu, 28 Aug 2008 14:13:14 -0400
Subject: [R-sig-ME] fixed effect testing again (but different)
In-Reply-To: <a46630750808281107u582a779fvec72e695356d1713@mail.gmail.com>
Message-ID: <ED7B522EE00C9A4FA515AA71724D61EE0128C8DE@DC1EXCL01.air.org>

> Up until now my procedure is to evaluate two models, viz.
> 
> m0 <- lmer(response~(1|subject),data)
> m1 <- lmer(response~gender+(1|subject),data)
> 
> The likelihood-ratio test approach directly compares these two.

Since these models differ in their fixed effects, you need REML=FALSE
for the LRT to be meaningful.



From austin.frank at gmail.com  Fri Aug 29 06:47:19 2008
From: austin.frank at gmail.com (Austin Frank)
Date: Fri, 29 Aug 2008 00:47:19 -0400
Subject: [R-sig-ME] ML or REML for LR tests
References: <a46630750808281107u582a779fvec72e695356d1713@mail.gmail.com>
	<ED7B522EE00C9A4FA515AA71724D61EE0128C8DE@DC1EXCL01.air.org>
Message-ID: <m0k5e0h1go.fsf@gmail.com>

On Thu, Aug 28 2008, Doran, Harold wrote:

>> The likelihood-ratio test approach directly compares these two.
>
> Since these models differ in their fixed effects, you need REML=FALSE
> for the LRT to be meaningful.

This is a standard operating procedure that I picked up and accepted on
faith when I first started using lmer, before I really knew what I was
doing.  It occurs to me that this is the case for much of my
understanding of model comparison, so I'd like to check my understanding
of the use of LR tests with lmer.  If this is a case of RTFM, please
provide a pointer to the relevant Friendly Manual ;)

1) Can anyone offer a reference where the case is made for doing LR
tests on models fit by ML (as opposed to REML)?

2) Can non-nested ML models with the same number of fixed effects be
meaningfully compared with an LR test?  Something like:

--8<---------------cut here---------------start------------->8---
data(sleepstudy)
set.seed(535353)
sleepstudy$Fake <- rnorm(nrow(sleepstudy))
m1 <- lmer(Reaction ~ Days + (1 | Subject), sleepstudy, REML=FALSE)
m2 <- lmer(Reaction ~ Fake + (1 | Subject), sleepstudy, REML=FALSE)
anova(m1, m2)                          # Is this test meaningful...

## When possible, test against superset model
m12 <- lmer(Reaction ~ Days + Fake (1 | Subject),
            sleepstudy, REML=FALSE)
anova(m1, m2, m12)                     # ... or only this one?
--8<---------------cut here---------------end--------------->8---

3) Is it the case that LR tests between REML models with different
random effects are meaningful?  Does this apply to both nested and
non-nested models?

Thanks for the help,
/au

-- 
Austin Frank
http://aufrank.net
GPG Public Key (D7398C2F): http://aufrank.net/personal.asc



From ken at kjbeath.com.au  Fri Aug 29 09:53:37 2008
From: ken at kjbeath.com.au (Ken Beath)
Date: Fri, 29 Aug 2008 17:53:37 +1000
Subject: [R-sig-ME] ML or REML for LR tests
In-Reply-To: <m0k5e0h1go.fsf@gmail.com>
References: <a46630750808281107u582a779fvec72e695356d1713@mail.gmail.com>
	<ED7B522EE00C9A4FA515AA71724D61EE0128C8DE@DC1EXCL01.air.org>
	<m0k5e0h1go.fsf@gmail.com>
Message-ID: <008ED4AA-D47D-444E-B7E5-AC88DFBE4A19@kjbeath.com.au>

On 29/08/2008, at 2:47 PM, Austin Frank wrote:

> On Thu, Aug 28 2008, Doran, Harold wrote:
>
>>> The likelihood-ratio test approach directly compares these two.
>>
>> Since these models differ in their fixed effects, you need REML=FALSE
>> for the LRT to be meaningful.
>
> This is a standard operating procedure that I picked up and accepted  
> on
> faith when I first started using lmer, before I really knew what I was
> doing.  It occurs to me that this is the case for much of my
> understanding of model comparison, so I'd like to check my  
> understanding
> of the use of LR tests with lmer.  If this is a case of RTFM, please
> provide a pointer to the relevant Friendly Manual ;)
>
> 1) Can anyone offer a reference where the case is made for doing LR
> tests on models fit by ML (as opposed to REML)?
>

Any decent mixed models text. Verbeke and Molenberghs "Linear Mixed  
Models for Longitudinal Data" p63 or Pinheiro and Bates "Mixed-Effects  
Models in S and S-Plus" p76.

> 2) Can non-nested ML models with the same number of fixed effects be
> meaningfully compared with an LR test?  Something like:
>

No. General principle is for LR test models must be nested.

> --8<---------------cut here---------------start------------->8---
> data(sleepstudy)
> set.seed(535353)
> sleepstudy$Fake <- rnorm(nrow(sleepstudy))
> m1 <- lmer(Reaction ~ Days + (1 | Subject), sleepstudy, REML=FALSE)
> m2 <- lmer(Reaction ~ Fake + (1 | Subject), sleepstudy, REML=FALSE)
> anova(m1, m2)                          # Is this test meaningful...
>
> ## When possible, test against superset model
> m12 <- lmer(Reaction ~ Days + Fake (1 | Subject),
>            sleepstudy, REML=FALSE)
> anova(m1, m2, m12)                     # ... or only this one?
> --8<---------------cut here---------------end--------------->8---
>
> 3) Is it the case that LR tests between REML models with different
> random effects are meaningful?  Does this apply to both nested and
> non-nested models?
>

Maybe, but only for nested (see Q2). Supposedly it works better than  
ML. The significance tests wont be correct but if there is a huge  
significance level then there is probably a random effect. Simulation  
seems a better idea.

Ken

> Thanks for the help,
> /au
>
> -- 
> Austin Frank
> http://aufrank.net
> GPG Public Key (D7398C2F): http://aufrank.net/personal.asc
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Fri Aug 29 15:30:40 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 29 Aug 2008 08:30:40 -0500
Subject: [R-sig-ME] ML or REML for LR tests
In-Reply-To: <008ED4AA-D47D-444E-B7E5-AC88DFBE4A19@kjbeath.com.au>
References: <a46630750808281107u582a779fvec72e695356d1713@mail.gmail.com>
	<ED7B522EE00C9A4FA515AA71724D61EE0128C8DE@DC1EXCL01.air.org>
	<m0k5e0h1go.fsf@gmail.com>
	<008ED4AA-D47D-444E-B7E5-AC88DFBE4A19@kjbeath.com.au>
Message-ID: <40e66e0b0808290630l117e529bjfe2441028db42855@mail.gmail.com>

On Fri, Aug 29, 2008 at 2:53 AM, Ken Beath <ken at kjbeath.com.au> wrote:
> On 29/08/2008, at 2:47 PM, Austin Frank wrote:
>
>> On Thu, Aug 28 2008, Doran, Harold wrote:
>>
>>>> The likelihood-ratio test approach directly compares these two.
>>>
>>> Since these models differ in their fixed effects, you need REML=FALSE
>>> for the LRT to be meaningful.
>>
>> This is a standard operating procedure that I picked up and accepted on
>> faith when I first started using lmer, before I really knew what I was
>> doing.  It occurs to me that this is the case for much of my
>> understanding of model comparison, so I'd like to check my understanding
>> of the use of LR tests with lmer.  If this is a case of RTFM, please
>> provide a pointer to the relevant Friendly Manual ;)
>>
>> 1) Can anyone offer a reference where the case is made for doing LR
>> tests on models fit by ML (as opposed to REML)?
>>
>
> Any decent mixed models text. Verbeke and Molenberghs "Linear Mixed Models
> for Longitudinal Data" p63 or Pinheiro and Bates "Mixed-Effects Models in S
> and S-Plus" p76.
>
>> 2) Can non-nested ML models with the same number of fixed effects be
>> meaningfully compared with an LR test?  Something like:
>>
>
> No. General principle is for LR test models must be nested.
>
>> --8<---------------cut here---------------start------------->8---
>> data(sleepstudy)
>> set.seed(535353)
>> sleepstudy$Fake <- rnorm(nrow(sleepstudy))
>> m1 <- lmer(Reaction ~ Days + (1 | Subject), sleepstudy, REML=FALSE)
>> m2 <- lmer(Reaction ~ Fake + (1 | Subject), sleepstudy, REML=FALSE)
>> anova(m1, m2)                          # Is this test meaningful...
>>
>> ## When possible, test against superset model
>> m12 <- lmer(Reaction ~ Days + Fake (1 | Subject),
>>           sleepstudy, REML=FALSE)
>> anova(m1, m2, m12)                     # ... or only this one?
>> --8<---------------cut here---------------end--------------->8---
>>
>> 3) Is it the case that LR tests between REML models with different
>> random effects are meaningful?  Does this apply to both nested and
>> non-nested models?

> Maybe, but only for nested (see Q2). Supposedly it works better than ML. The
> significance tests wont be correct but if there is a huge significance level
> then there is probably a random effect. Simulation seems a better idea.

It may help to give a bit of background about what happens in the
anova method for mer objects in the lme4 package.

When fitting a linear mixed model, both the profiled deviance
(negative twice the log-likelihood) and the profiled REML criterion
are evaluated at each iteration during the optimization.  The word
"profiled" means that, although the likelihood or the REML criterion
are defined as functions of the fixed-effects parameters,
$\boldmath{\beta}$, the common scale parameter, $\sigma$, and the
parameters $\boldmath{\theta}$ that determine the relative covariance
matrix $\boldmath{ \Sigma}$ of the random effects, these criteria are
evaluated as a function of $\boldmath{\theta}$ alone, with
$\boldmath{\beta}$ and $\sigma$ at their conditionally optimal values.

The profiled REML criterion has a term that depends on the model
matrix X for the fixed-effects parameters.  If you change the
definition of the fixed-effects you will change the value of that
criterion in a systematic way that does not depend on how well the
respective models fit the observed data.  Thus, differences in the
REML criterion are not a meaningful way to compare two models that
differ in the definition of the fixed-effects.

If you look at the code for the anova method for the mer class you
will see that it always calls logLik with REML = FALSE.  That is, it
extracts the log-likelihood from the profiled deviance even if the
model has been fit according to the REML criterion.  In general that
would be a very bad idea if we were using the deviance and not the
profiled deviance.  You can't count on the deviance at the REML
estimates for the complete parameter vector being close to the optimal
deviance.  However, the profiled deviance at the REML estimates of
$\boldmath{\theta}$ is close to the optimal profiled deviance.

For example,

> fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
> ## profiled deviance at the REML estimate of $\theta$
> dput(d1 <- deviance(fm1, REML = FALSE))
structure(1751.98581110077, .Names = "ML")
> ## optimal value of the deviance
> dput(d2 <- deviance(update(fm1, REML = FALSE)))
structure(1751.93934480597, .Names = "ML")
> ## amount by which the profiled deviance at REML estimate
> ## exceeds the optimal value
> d1 - d2
        ML
0.04646629

As you can see, I have gravitated to using the term "REML criterion"
rather than calling it a deviance or a log-likelihood.  The print
method for mer objects still labels it as the REMLdeviance but I plan
to change that and create an additional extractor called REML to
return that value.  In that case the deviance extractor will always
return the deviance, perhaps after re-optimizing to produce the
optimal deviance.

So the bottom line is that the anova method for mer objects always
produces a likelihood ratio test based on the likelihood.  If you
compare models fit by REML that likelihood ratio statistic will be
somewhat inaccurate but only by a small amount (in all the cases that
I have seen).



From mariaevagongora at hotmail.com  Fri Aug 29 20:07:04 2008
From: mariaevagongora at hotmail.com (Maria Eva Gongora)
Date: Fri, 29 Aug 2008 15:07:04 -0300
Subject: [R-sig-ME] empty cell
Message-ID: <BAY120-W19A6C68BB2CA3AAC1424BDB6630@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080829/3a1b4c8b/attachment.pl>

From danielezrajohnson at gmail.com  Fri Aug 29 20:09:54 2008
From: danielezrajohnson at gmail.com (Daniel Ezra Johnson)
Date: Fri, 29 Aug 2008 19:09:54 +0100
Subject: [R-sig-ME] fixed effect testing again (but different)
In-Reply-To: <48B7F22B.2080600@lpl-aix.fr>
References: <a46630750808281107u582a779fvec72e695356d1713@mail.gmail.com>
	<48B7F22B.2080600@lpl-aix.fr>
Message-ID: <a46630750808291109j4a1df222h7b1894634a3fc7ed@mail.gmail.com>

>> If it seems wrong to take the parameter from m1, is there some way to
>> change the specification of m0 so as to obtain a separate random
>> effect for males and females? I've seen that done but I've forgotten
>> how to do it.
>>
>> If I can run something like
>>
>> m00 <- lmer(response~(1|subject-females)+(1|subject-males),data)   #
>> how is this specified?

Off list I was shown a way to supposedly do this, by making a pseudo
random slope using a dummy variable. It seems promising but it doesn't
really work properly, as the following example shows.

set.seed(1)
s <- rep(rnorm(200,0,1),each=100)
g <- rep(c(-3,3),each=10000)
p <- inv.logit(s+g)
obs <- data.frame(response=rbinom(20000,1,p),
	gender=rep(c("M","F"),each=10000),
	subject=rep(paste("S",1:200,sep=""),each=100))
obs$M <- ifelse(obs$gender=="M",1,0)
obs$F <- ifelse(obs$gender=="F",1,0)

test <- lmer(response~(0+M|subject)+(0+F|subject),obs,binomial)

Random effects:
 Groups  Name Variance Std.Dev.
 subject M     0.82563 0.90864       # out of whack
 subject F    37.79488 6.14775       # out of whack
Number of obs: 20000, groups: subject, 200

obs.m <- obs[obs$gender=="M",]
test.m <- lmer(response~(1|subject),obs.m,binomial)

Random effects:
 Groups  Name        Variance Std.Dev.
 subject (Intercept) 0.85413  0.9242
Number of obs: 10000, groups: subject, 100

obs.f <- obs[obs$gender=="F",]
test.f <- lmer(response~(1|subject),obs.f,binomial)

Random effects:
 Groups  Name        Variance Std.Dev.
 subject (Intercept) 0.60097  0.77522
Number of obs: 10000, groups: subject, 100

Is there, then, any way to implement heteroscedastic-by-group random
effects in lme4, as opposed to nlme?

Thanks,
D



From bates at stat.wisc.edu  Fri Aug 29 20:22:03 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 29 Aug 2008 13:22:03 -0500
Subject: [R-sig-ME] empty cell
In-Reply-To: <BAY120-W19A6C68BB2CA3AAC1424BDB6630@phx.gbl>
References: <BAY120-W19A6C68BB2CA3AAC1424BDB6630@phx.gbl>
Message-ID: <40e66e0b0808291122n42a7a2f3p32a4430626fa5116@mail.gmail.com>

On Fri, Aug 29, 2008 at 1:07 PM, Maria Eva Gongora
<mariaevagongora at hotmail.com> wrote:
>
> I am using mixed effect models (library lmer) to evaluate factors that influence the catch rate of hake (set by set) in a fishery. I have some empty cells when I include some of the interactions between the fixed factors (e.g. year:area) and interactions between fixed and random effects (e.g. year:vessel, where vessel is a random effect) . While empty cell were not the problem when all factors were treated as fixed using lm or glm, the estimation failed when I used lme4 and treated some of the factors (the vessel id) as random.
> Mar?a Eva G?ngora

I regret that we won't be able to help without more information.  As
it says at the bottom of all the messages to the R-help mailing list

PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.



From bates at stat.wisc.edu  Fri Aug 29 20:30:59 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 29 Aug 2008 13:30:59 -0500
Subject: [R-sig-ME] fixed effect testing again (but different)
In-Reply-To: <a46630750808291109j4a1df222h7b1894634a3fc7ed@mail.gmail.com>
References: <a46630750808281107u582a779fvec72e695356d1713@mail.gmail.com>
	<48B7F22B.2080600@lpl-aix.fr>
	<a46630750808291109j4a1df222h7b1894634a3fc7ed@mail.gmail.com>
Message-ID: <40e66e0b0808291130t351cf1b7nf33579129ce5fbc3@mail.gmail.com>

On Fri, Aug 29, 2008 at 1:09 PM, Daniel Ezra Johnson
<danielezrajohnson at gmail.com> wrote:
>>> If it seems wrong to take the parameter from m1, is there some way to
>>> change the specification of m0 so as to obtain a separate random
>>> effect for males and females? I've seen that done but I've forgotten
>>> how to do it.
>>>
>>> If I can run something like
>>>
>>> m00 <- lmer(response~(1|subject-females)+(1|subject-males),data)   #
>>> how is this specified?
>
> Off list I was shown a way to supposedly do this, by making a pseudo
> random slope using a dummy variable. It seems promising but it doesn't
> really work properly, as the following example shows.
>
> set.seed(1)
> s <- rep(rnorm(200,0,1),each=100)
> g <- rep(c(-3,3),each=10000)
> p <- inv.logit(s+g)
> obs <- data.frame(response=rbinom(20000,1,p),
>        gender=rep(c("M","F"),each=10000),
>        subject=rep(paste("S",1:200,sep=""),each=100))
> obs$M <- ifelse(obs$gender=="M",1,0)
> obs$F <- ifelse(obs$gender=="F",1,0)
>
> test <- lmer(response~(0+M|subject)+(0+F|subject),obs,binomial)
>
> Random effects:
>  Groups  Name Variance Std.Dev.
>  subject M     0.82563 0.90864       # out of whack
>  subject F    37.79488 6.14775       # out of whack
> Number of obs: 20000, groups: subject, 200
>
> obs.m <- obs[obs$gender=="M",]
> test.m <- lmer(response~(1|subject),obs.m,binomial)
>
> Random effects:
>  Groups  Name        Variance Std.Dev.
>  subject (Intercept) 0.85413  0.9242
> Number of obs: 10000, groups: subject, 100
>
> obs.f <- obs[obs$gender=="F",]
> test.f <- lmer(response~(1|subject),obs.f,binomial)
>
> Random effects:
>  Groups  Name        Variance Std.Dev.
>  subject (Intercept) 0.60097  0.77522
> Number of obs: 10000, groups: subject, 100
>
> Is there, then, any way to implement heteroscedastic-by-group random
> effects in lme4, as opposed to nlme?

At present, no - at least none that I know of (and I am usually
reasonably well informed regarding the capabilities of lme4).

I am currently working on abstracting the role of the parameters in
model-fitting within lme4 by redesigning the classes.  The current
design ties the parameters to a particular representation of the
relative covariance matrix of the random effects and it should be
generalized.  The trick is to generalize the approach without
sacrificing too much in the way of performance.



From bolker at zoology.ufl.edu  Fri Aug 29 21:51:02 2008
From: bolker at zoology.ufl.edu (Ben Bolker)
Date: Fri, 29 Aug 2008 15:51:02 -0400
Subject: [R-sig-ME] empty cell
In-Reply-To: <BAY120-W19A6C68BB2CA3AAC1424BDB6630@phx.gbl>
References: <BAY120-W19A6C68BB2CA3AAC1424BDB6630@phx.gbl>
Message-ID: <48B85326.9060100@zoology.ufl.edu>

Maria Eva Gongora wrote:
> I am using mixed effect models (library lmer) to evaluate factors that 

influence the catch rate of hake (set by set) in a fishery. I have some

empty cells when I include some of the interactions between the fixed

factors (e.g. year:area) and interactions between fixed and random effects

(e.g. year:vessel, where vessel is a random effect) . While empty cell were

not the problem when all factors were treated as fixed using lm or glm,

the estimation failed when I used lme4 and treated some of the factors
(the vessel id) as random.
> Mar?a Eva G?ngora
> ______________________________________________

  I suspect that the problem is the empty fixed interactions,
rather than the empty random effects levels (interactions
between fixed and random effects are random by definition).

  When the fixed effects are this badly unbalanced,
lm and glm just go ahead and spit out NA for the
unestimable parameters, whereas lme4 is a little more
finicky.

r = runif(200)
d2 = cbind(expand.grid(year=factor(1:2),site=factor(1:2),
  fac3=factor(1:2),rep=1:25),val=r,
  val2=rpois(200,exp(r)))
## unbalance the data
d2 = subset(d2,!(site==2 & year==2))
lm(val~site*year,data=d2)
glm(val2~site*year,family="poisson",data=d2)

library(lme4)
lmer(val~site*year+(1|fac3),data=d2)  ## fails
lmer(val~site+year+(1|fac3),data=d2)  ## remove interaction -- succeeds
lmer(val~site+(year|fac3),data=d2)  ## treating year as random works too

  That said, it would be good to provide more detail as Doug Bates
suggests -- sometimes we're not very good at guessing what you mean ...

  good luck
   Ben Bolker



From danielezrajohnson at gmail.com  Sat Aug 30 11:39:19 2008
From: danielezrajohnson at gmail.com (Daniel Ezra Johnson)
Date: Sat, 30 Aug 2008 10:39:19 +0100
Subject: [R-sig-ME] fixed effect testing again (but different)
In-Reply-To: <fc.004c4d1938894dce3b9aca0032bb0c9d.388951cc@umit.maine.edu>
References: <mailman.14227.1220039475.4655.r-sig-mixed-models@r-project.org>
	<fc.004c4d1938894dce3b9aca0032bb0c9d.388951cc@umit.maine.edu>
Message-ID: <a46630750808300239q75b41b24t4172f34f0ac249df@mail.gmail.com>

Note that what Alan links to is the same procedure I was reminded of,
that is, splitting (1|subject) by a dummy variable, e.g.
(0+M|subject)+(0+F|subject).

What I'm observing is that this procedure only works if the fixed
effect corresponding to the subject split is also included in the
model. Then, the M|subject and F|subject terms are both distributed
around zero and the results do seem correct.

I was interested in testing the fixed effect term, so I was trying to
run a model estimating the subject variance separately for males and
females, but without a fixed effect term for the difference between
them.

I thought that difference would get absorbed into the random effects
(e.g. the dummy slopes for M distributed around -3, those for F
distributed around +3)  but that the variances would come out
accurately.

This seems to run into trouble, maybe because it's a violation of the
assumptions that the random effects are distributed around zero. What
seems to happen is that the intercept and random effects are
accurately estimated for one stratum, and then the other stratum isn't
right.

But if hypothesis testing isn't the issue and you always include a
fixed-effect term for the same outer variable that you're doing the
heteroscedasticity 'hack' for, then yes, it does seem to work.

set.seed(1)
s <- rep(rnorm(200,0,1),each=100)
g <- rep(c(-3,3),each=10000)
p <- inv.logit(s+g)
obs <- data.frame(response=rbinom(20000,1,p),
       gender=rep(c("M","F"),each=10000),
       subject=rep(paste("S",1:200,sep=""),each=100))
obs$M <- ifelse(obs$gender=="M",1,0)
obs$F <- ifelse(obs$gender=="F",1,0)

test2 <- lmer(response~gender+(0+M|subject)+(0+F|subject),obs,binomial)
Random effects:
 Groups  Name Variance Std.Dev.
 subject M    0.85411  0.92418
 subject F    0.60095  0.77521
Fixed effects:
            Estimate...
(Intercept)  2.91419
genderM     -5.77754

testF <- lmer(response~(1|subject),obs[obs$gender=="F",],binomial)
Random effects:
 Groups  Name        Variance Std.Dev.
 subject (Intercept) 0.60097  0.77522
Fixed effects:
            Estimate...
(Intercept)  2.91411

testM <- lmer(response~(1|subject),obs[obs$gender=="M",],binomial)
Random effects:
 Groups  Name        Variance Std.Dev.
 subject (Intercept) 0.85413  0.9242
Fixed effects:
            Estimate...
(Intercept)  -2.8633

ALL THE ABOVE "MATCHES" PERFECTLY, BUT NOT:

test <- lmer(response~(0+M|subject)+(0+F|subject),obs,binomial)
Random effects:
 Groups  Name Variance Std.Dev.
 subject M     0.82563 0.90864
 subject F    37.79488 6.14775
Fixed effects:
            Estimate...
(Intercept)  -2.6939

For this last model, even though there are no other fixed effects, the
intercept is estimated off-center, close to the value for the "M"
group of subjects. The "M" subject variance is estimated correctly,
but the "F" subject ranefs are essentially estimated w/r/t/ to the
"M"-centered intercept.

Still not sure why that would throw off the _variance_ of the "F"
random effects, but it seems to have happened.

I think this dummy-slope procedure should be considered "off-label"
and used with caution!

D



From alanc at umit.maine.edu  Sat Aug 30 06:38:27 2008
From: alanc at umit.maine.edu (Alan Cobo-Lewis)
Date: Sat, 30 Aug 2008 00:38:27 -0400
Subject: [R-sig-ME] fixed effect testing again (but different)
In-Reply-To: <mailman.14227.1220039475.4655.r-sig-mixed-models@r-project.org>
References: <mailman.14227.1220039475.4655.r-sig-mixed-models@r-project.org>
Message-ID: <fc.004c4d1938894dce3b9aca0032bb0c9d.388951cc@umit.maine.edu>


Doug,

Couldn't heteroscedastic-by-group random effects be handled by the coding discussed by David Afshartous, you, and me in July 2007? David described what kind of heteroscedasticity he was looking for at
https://stat.ethz.ch/pipermail/r-sig-mixed-models/2007q3/000235.html and we worked through it at the thread beginning at https://stat.ethz.ch/pipermail/r-sig-mixed-models/2007q3/000237.html

David summarizes the outcome of the thread at https://stat.ethz.ch/pipermail/r-sig-mixed-models/2007q3/000248.html

alan


r-sig-mixed-models at r-project.org on Friday, August 29, 2008 at 3:51 PM -0500 wrote:
>Message: 5
>Date: Fri, 29 Aug 2008 13:30:59 -0500
>From: "Douglas Bates" <bates at stat.wisc.edu>
>Subject: Re: [R-sig-ME] fixed effect testing again (but different)
>To: "Daniel Ezra Johnson" <danielezrajohnson at gmail.com>
>Cc: r-sig-mixed-models at r-project.org
>Message-ID:
>	<40e66e0b0808291130t351cf1b7nf33579129ce5fbc3 at mail.gmail.com>
>Content-Type: text/plain; charset=ISO-8859-1
>
>On Fri, Aug 29, 2008 at 1:09 PM, Daniel Ezra Johnson
><danielezrajohnson at gmail.com> wrote:
>>>> If it seems wrong to take the parameter from m1, is there some way to
>>>> change the specification of m0 so as to obtain a separate random
>>>> effect for males and females? I've seen that done but I've forgotten
>>>> how to do it.
>>>>
>>>> If I can run something like
>>>>
>>>> m00 <- lmer(response~(1|subject-females)+(1|subject-males),data)   #
>>>> how is this specified?
>>
>> Off list I was shown a way to supposedly do this, by making a pseudo
>> random slope using a dummy variable. It seems promising but it doesn't
>> really work properly, as the following example shows.
>>
>> set.seed(1)
>> s <- rep(rnorm(200,0,1),each=100)
>> g <- rep(c(-3,3),each=10000)
>> p <- inv.logit(s+g)
>> obs <- data.frame(response=rbinom(20000,1,p),
>>        gender=rep(c("M","F"),each=10000),
>>        subject=rep(paste("S",1:200,sep=""),each=100))
>> obs$M <- ifelse(obs$gender=="M",1,0)
>> obs$F <- ifelse(obs$gender=="F",1,0)
>>
>> test <- lmer(response~(0+M|subject)+(0+F|subject),obs,binomial)
>>
>> Random effects:
>>  Groups  Name Variance Std.Dev.
>>  subject M     0.82563 0.90864       # out of whack
>>  subject F    37.79488 6.14775       # out of whack
>> Number of obs: 20000, groups: subject, 200
>>
>> obs.m <- obs[obs$gender=="M",]
>> test.m <- lmer(response~(1|subject),obs.m,binomial)
>>
>> Random effects:
>>  Groups  Name        Variance Std.Dev.
>>  subject (Intercept) 0.85413  0.9242
>> Number of obs: 10000, groups: subject, 100
>>
>> obs.f <- obs[obs$gender=="F",]
>> test.f <- lmer(response~(1|subject),obs.f,binomial)
>>
>> Random effects:
>>  Groups  Name        Variance Std.Dev.
>>  subject (Intercept) 0.60097  0.77522
>> Number of obs: 10000, groups: subject, 100
>>
>> Is there, then, any way to implement heteroscedastic-by-group random
>> effects in lme4, as opposed to nlme?
>
>At present, no - at least none that I know of (and I am usually
>reasonably well informed regarding the capabilities of lme4).
>
>I am currently working on abstracting the role of the parameters in
>model-fitting within lme4 by redesigning the classes.  The current
>design ties the parameters to a particular representation of the
>relative covariance matrix of the random effects and it should be
>generalized.  The trick is to generalize the approach without
>sacrificing too much in the way of performance.


--
Alan B. Cobo-Lewis, Ph.D.		(207) 581-3840 tel
Department of Psychology		(207) 581-6128 fax
University of Maine
Orono, ME 04469-5742     		alanc at maine.edu

http://www.umaine.edu/visualperception



From qwu5 at yahoo.com  Sat Aug 30 04:50:15 2008
From: qwu5 at yahoo.com (Qinglin Wu)
Date: Fri, 29 Aug 2008 19:50:15 -0700 (PDT)
Subject: [R-sig-ME] generalized linear mixed-effects model and lmer, lme
Message-ID: <494520.30026.qm@web52310.mail.re2.yahoo.com>

Dear list,

I am the new R user and just start last Friday.  I am fitting the generalized linear mixed-effects model and going to use "glmer" function.  I don?t understand the following two examples in the glmer function documentation:

Example 1:  lmer(Reaction ~ Days + (Days | Subject), sleepstudy))
Example 2:  lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject), sleepstudy))

In example 1,  i can write the model as:  
reaction = a_0 + a_1*days + b_0 + b1*days. 
The fixed effecta are a_0 + a_1*days, the random effects are b_0 + b1*days.

How can you explain example 2?  Can I write the model as:  
reaction = a_0 + a_1*days +  b_0 + c1*days.  

these two have the same fixed effect.  What is the random effects in example 2?   Is it b_0 + c1*days?  Or one random effect is b_0 and another one is c1*days?  If I use "lme" function, how can I write it in the ?random? argument.     

Their results are totally different.


Another question, I tried the function ?lme?.  I think it should have the same result with the function "lmer".  But it didn?t.

I tried the following models:   
lme(distance ~ age + Sex, data = Orthodont, random = ~ age+Sex)
lmer(distance~age+Sex+(age+Sex|Subject),Orthodont)
lmer(distance~age+Sex+((age+Sex)|Subject),Orthodont)
  
They have the almost same results except ?Corr? in the random effects.  

I don?t know how the two functions work in the R.  I checked the functions in the package. We can not look them.   Does anybody know there are some other places so that I can look the source code and then I know how they work.  

Any help would be appreciated.


Cynthia Wu











From bates at stat.wisc.edu  Sat Aug 30 12:40:47 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 30 Aug 2008 05:40:47 -0500
Subject: [R-sig-ME] generalized linear mixed-effects model and lmer, lme
In-Reply-To: <494520.30026.qm@web52310.mail.re2.yahoo.com>
References: <494520.30026.qm@web52310.mail.re2.yahoo.com>
Message-ID: <40e66e0b0808300340v7a132704qadf5323bc402449@mail.gmail.com>

On Fri, Aug 29, 2008 at 9:50 PM, Qinglin Wu <qwu5 at yahoo.com> wrote:
> Dear list,

> I am the new R user and just start last Friday.  I am fitting the generalized linear mixed-effects model and going to use "glmer" function.  I don't understand the following two examples in the glmer function documentation:

> Example 1:  lmer(Reaction ~ Days + (Days | Subject), sleepstudy))
> Example 2:  lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject), sleepstudy))

> In example 1,  i can write the model as:
> reaction = a_0 + a_1*days + b_0 + b1*days.
> The fixed effecta are a_0 + a_1*days, the random effects are b_0 + b1*days.

> How can you explain example 2?  Can I write the model as:
> reaction = a_0 + a_1*days +  b_0 + c1*days.

> these two have the same fixed effect.  What is the random effects in example 2?   Is it b_0 + c1*days?  Or one random effect is b_0 and another one is c1*days?

The two models differ in the form of the covariance matrix of the
random effects.  In the first example the random effect for the
intercept and the random effect for the slope are allowed to be
correlated within subject.  In the second example they are
independent.

In general, random effects associated with different random-effects
terms in a mixed-model formula are independent.

You may find the slides
http://www.stat.wisc.edu/~bates/PotsdamGLMM/LMMD.pdf and
http://www.stat.wisc.edu/~bates/PotsdamGLMM/GLMMD.pdf helpful in
learning about the lme4 package.

> If I use "lme" function, how can I write it in the "random" argument.

I'm not sure what "it" is here.

> Their results are totally different.
>
>
> Another question, I tried the function "lme".  I think it should have the same result with the function "lmer".  But it didn't.

> I tried the following models:
> lme(distance ~ age + Sex, data = Orthodont, random = ~ age+Sex)
> lmer(distance~age+Sex+(age+Sex|Subject),Orthodont)
> lmer(distance~age+Sex+((age+Sex)|Subject),Orthodont)

> They have the almost same results except "Corr" in the random effects.

Well the last two are equivalent.  I don't know about the first one.
However, the model doesn't make sense.  You are trying to define a
random effect for Sex by Subject but Sex doesn't vary within Subject.

The numerical methods underlying the lme function and the lmer
function are quite different.  I believe that the methods for lmer are
more reliable.

> I don't know how the two functions work in the R.  I checked the functions in the package. We can not look them.

You can always look at the functions.  R is an Open Source project and
the sources for all packages are available at CRAN.  Not only the
current sources but the entire archive of the sources for the lme4
package is available at http://r-forge.r-project.org/projects/lme4
under the SCM tab.

> Does anybody know there are some other places so that I can look the source code and then I know how they work.

Well you can look at the source code as I described above.  As for
knowing how the functions work, that may take more than a casual glace
at the sources.  There are days when I find it difficult to decide
exactly how they work, and I wrote most of them.



> Any help would be appreciated.
>
>
> Cynthia Wu
>
>
>
>
>
>
>
>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From danielezrajohnson at gmail.com  Sat Aug 30 12:53:21 2008
From: danielezrajohnson at gmail.com (Daniel Ezra Johnson)
Date: Sat, 30 Aug 2008 11:53:21 +0100
Subject: [R-sig-ME] generalized linear mixed-effects model and lmer, lme
In-Reply-To: <494520.30026.qm@web52310.mail.re2.yahoo.com>
References: <494520.30026.qm@web52310.mail.re2.yahoo.com>
Message-ID: <a46630750808300353x243e1848o3075926e5151ef31@mail.gmail.com>

> Example 1:  lmer(Reaction ~ Days + (Days | Subject), sleepstudy))
> Example 2:  lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject), sleepstudy))

Perhaps the example currently given is not the clearest one that could
appear in the ?lmer helpfile. I would suggest either replacing it with
a more elementary example, or else adding a comment line indicating
that the only difference lies in the correlation structure.

What about:
Example 1: lmer(Reaction ~ Days + (1 | Subject), sleepstudy))
Example 2: lmer(Reaction ~ Days + (Days | Subject), sleepstudy))

People looking at the helpfile might find that one more helpful.
There's something elegant about the current example, but it certainly
threw me too, until I inspected the output carefully.



From alanc at umit.maine.edu  Sat Aug 30 19:44:57 2008
From: alanc at umit.maine.edu (Alan Cobo-Lewis)
Date: Sat, 30 Aug 2008 13:44:57 -0400
Subject: [R-sig-ME] fixed effect testing again (but different)
In-Reply-To: <a46630750808300239q75b41b24t4172f34f0ac249df@mail.gmail.com>
References: <mailman.14227.1220039475.4655.r-sig-mixed-models@r-project.org>
	<	> <fc.004c4d1938894dce3b9aca0032bb0c9d.388951cc@umit.maine.edu>
	<a46630750808300239q75b41b24t4172f34f0ac249df@mail.gmail.com>
Message-ID: <fc.004c4d19388ad4cf3b9aca0032bb0c9d.388ade80@umit.maine.edu>

Daniel,

I think you're running into a problem of model misspecification (where the population has effects that are omitted from your model in lmer).

1. If there aren't any fixed effects in the population and you omit the fixed effects from lmer but include the random effects in lmer (call that lmer model A) then lmer should correctly model the heteroscedasticity.

2. If there are fixed effects in the population and you include the fixed effects as well as the random effects (call that lmer model B) in lmer then lmer should correctly model the heteroscedasticity.

3. If there are fixed effects in the population and you omit the fixed effects from lmer but include the random effects in lmer (model A again) then lmer might not correctly model the data because of model misspecification. I think this is the
problem you're running into.

Assuming that you keep the heteroscedasticity in the lmer models, testing for the presence of fixed effects amounts to comparing the fit of model B against the fit of model A. If the fixed effects are present then you'll accept model B and use that
model's coefficients to estimate the heteroscedasticity; if the fixed effects are absent then you'll accept model A and use that model's coefficients to estimate the heteroscedasticity.

alan




"Daniel Ezra Johnson" <danielezrajohnson at gmail.com> on Saturday, August 30, 2008 at 5:39 AM -0500 wrote:
>What I'm observing is that this procedure only works if the fixed
>effect corresponding to the subject split is also included in the
>model. Then, the M|subject and F|subject terms are both distributed
>around zero and the results do seem correct.
>
>I was interested in testing the fixed effect term, so I was trying to
>run a model estimating the subject variance separately for males and
>females, but without a fixed effect term for the difference between
>them.
>
>I thought that difference would get absorbed into the random effects
>(e.g. the dummy slopes for M distributed around -3, those for F
>distributed around +3)  but that the variances would come out
>accurately.
>
>This seems to run into trouble, maybe because it's a violation of the
>assumptions that the random effects are distributed around zero. What
>seems to happen is that the intercept and random effects are
>accurately estimated for one stratum, and then the other stratum isn't
>right.
>
>But if hypothesis testing isn't the issue and you always include a
>fixed-effect term for the same outer variable that you're doing the
>heteroscedasticity 'hack' for, then yes, it does seem to work.



From cat.dev.urandom at gmail.com  Sun Aug 31 04:41:52 2008
From: cat.dev.urandom at gmail.com (D Chaws)
Date: Sat, 30 Aug 2008 22:41:52 -0400
Subject: [R-sig-ME] (no subject)
Message-ID: <dcf23fb80808301941i245ac795mebdc5d63ab6ce415@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080830/982cdc45/attachment.pl>

From cat.dev.urandom at gmail.com  Sun Aug 31 04:43:52 2008
From: cat.dev.urandom at gmail.com (D Chaws)
Date: Sat, 30 Aug 2008 22:43:52 -0400
Subject: [R-sig-ME] Adjusting for initial status in growth models
Message-ID: <dcf23fb80808301943m1df0ede1k6577c6384bd56b0a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080830/ee946801/attachment.pl>

From cat.dev.urandom at gmail.com  Sun Aug 31 07:53:54 2008
From: cat.dev.urandom at gmail.com (D Chaws)
Date: Sun, 31 Aug 2008 01:53:54 -0400
Subject: [R-sig-ME] VarCorr vs ranef
Message-ID: <dcf23fb80808302253i366dae18j6a3e0652d6ec2f7d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080831/bcbcc82d/attachment.pl>

From danielezrajohnson at gmail.com  Sun Aug 31 12:13:50 2008
From: danielezrajohnson at gmail.com (Daniel Ezra Johnson)
Date: Sun, 31 Aug 2008 11:13:50 +0100
Subject: [R-sig-ME] VarCorr vs ranef
In-Reply-To: <dcf23fb80808302253i366dae18j6a3e0652d6ec2f7d@mail.gmail.com>
References: <dcf23fb80808302253i366dae18j6a3e0652d6ec2f7d@mail.gmail.com>
Message-ID: <a46630750808310313o1de77409q16d98aef0220d658@mail.gmail.com>

On Sun, Aug 31, 2008 at 6:53 AM, D Chaws <cat.dev.urandom at gmail.com> wrote:
> Can someone tell me why correlations between raw random effects are
> different from that provided in VarCorr for lme models?
> For example:
>
> fm1 = lme(distance ~ I(age-8), random = ~ 1 + I(age-8) | Subject, data =
> Orthodont)
> R# VarCorr(fm1)
> Subject = pdLogChol(1 + I(age - 8))
>            Variance StdDev Corr
> (Intercept) 3.55937  1.8866 (Intr)
> I(age - 8)  0.05127  0.2264 0.209
> Residual    1.71620  1.3100
>
> and
>
> R# cor(ranef(fm1))
>            (Intercept) I(age - 8)
> (Intercept)      1.0000     0.5764
> I(age - 8)       0.5764     1.0000
>

This isn't a complete answer, but the figures in VarCorr and the model
summary are the population estimates for the random effects (the
parameters) while everything derived from ranef() refers to the actual
Subjects in the data (the BLUPs).

Look at:

> sd(ranef(fm1))
(Intercept)  I(age - 8)
  1.7359554   0.1557322

Those figures don't match the VarCorr standard deviations either,
especially the second.

I don't know why the BLUPs pattern differently, exactly, but I did
look at plot(coefs(fm1)) which suggested Sex should be added as a
fixed effect. Once I did that, the correlation between the random
effects changed quite a lot (but was still different between VarCorr
and ranef; the population correlation was actually negative...)

D



From cat.dev.urandom at gmail.com  Sun Aug 31 14:52:30 2008
From: cat.dev.urandom at gmail.com (D Chaws)
Date: Sun, 31 Aug 2008 08:52:30 -0400
Subject: [R-sig-ME] VarCorr vs ranef
In-Reply-To: <a46630750808310313o1de77409q16d98aef0220d658@mail.gmail.com>
References: <dcf23fb80808302253i366dae18j6a3e0652d6ec2f7d@mail.gmail.com>
	<a46630750808310313o1de77409q16d98aef0220d658@mail.gmail.com>
Message-ID: <dcf23fb80808310552j54279c83u90e438dde7e8e9d1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080831/65e84323/attachment.pl>

From cotter.rs at gmail.com  Tue Sep  2 14:29:31 2008
From: cotter.rs at gmail.com (R.S. Cotter)
Date: Tue, 2 Sep 2008 14:29:31 +0200
Subject: [R-sig-ME] Error: "Cannot get confidence intervals...", with lme,
	what does it means?
Message-ID: <742479270809020529h7bfa410brbf84578354939e6a@mail.gmail.com>

Hello,

In some occasions I get this error message: "Cannot get confidence
intervals on var-cov components: Non-positive definite approximate
variance-covariance".

I have tried to figure out this by using help function, but didn't
find answer to the question. I address this question with describing
the model and the primary task that I want to solve. Sorry if the
question is clumsy formulated, I 'm not that experienced with R and
statistics.

My model is:
Response= Weight(continous)
Explanatory variables= Time (continous) and Diet (kategorical, two groups; B&C)

The primary question of interest is wheter the growth rates
(Weight/Time) differ among the two diets.

lmefit1<-lme(Weight ~ Diet*Time,random=~1|Place,data=Total)

Summary output is ok, so far so good. But I also wanted to get the
slope and confidence intervals for the growth rates for both diets
(B&C), so I ran intervals(). And I got the intercept, slope and
confidence intervals for diet B, see below.

But I also wanted the same for the diet C, to do this I renamed diet C
to A in the data sheet to force C to be the dummy variable. Is this
the right way to do it?

When running the intervals () once again, I got this message: "Cannot
get confidence intervals on var-cov components: Non-positive definite
approximate variance-covariance". What could be wrong..? Is there
other ways to get the slope and confidence intervals from a lme model?

> intervals(lmefit1)
Approximate 95% confidence intervals

 Fixed effects:
                          lower               est.                 upper
(Intercept)           66.040673     108.122242     150.203810
DietC                -175.080336   -109.638518     -44.196700
Time                     4.177387         5.434087       6.690788
DietC:Time          7.938101         11.180806     14.423511
attr(,"label")
[1] "Fixed effects:"

 Random Effects:
  Level: Place
                    lower     est.    upper
sd((Intercept)) 0.1478599 13.50651 1233.775

 Within-group standard error:
   lower     est.    upper
159.9128 174.8928 191.2761

Best regards Cotter



From gangchen at mail.nih.gov  Tue Sep  2 16:09:26 2008
From: gangchen at mail.nih.gov (Gang Chen)
Date: Tue, 2 Sep 2008 10:09:26 -0400
Subject: [R-sig-ME] Error: "Cannot get confidence intervals...", with lme,
	what does it means?
In-Reply-To: <742479270809020529h7bfa410brbf84578354939e6a@mail.gmail.com>
References: <742479270809020529h7bfa410brbf84578354939e6a@mail.gmail.com>
Message-ID: <F32C20E3-2046-468C-A9D8-87CD30A1DAD6@mail.nih.gov>

Cotter,

Check the following component

> lmefit1$apVar

If you see something like this

   [1] "Non-positive definite approximate variance-covariance"

it most likely indicates you have an inappropriate model for the  
data. Try plotting out the data, and get some idea about the feasible  
models, and then fit the data with those models.

Cheers,
Gang


On Sep 2, 2008, at 8:29 AM, R.S. Cotter wrote:

> Hello,
>
> In some occasions I get this error message: "Cannot get confidence
> intervals on var-cov components: Non-positive definite approximate
> variance-covariance".
>
> I have tried to figure out this by using help function, but didn't
> find answer to the question. I address this question with describing
> the model and the primary task that I want to solve. Sorry if the
> question is clumsy formulated, I 'm not that experienced with R and
> statistics.
>
> My model is:
> Response= Weight(continous)
> Explanatory variables= Time (continous) and Diet (kategorical, two  
> groups; B&C)
>
> The primary question of interest is wheter the growth rates
> (Weight/Time) differ among the two diets.
>
> lmefit1<-lme(Weight ~ Diet*Time,random=~1|Place,data=Total)
>
> Summary output is ok, so far so good. But I also wanted to get the
> slope and confidence intervals for the growth rates for both diets
> (B&C), so I ran intervals(). And I got the intercept, slope and
> confidence intervals for diet B, see below.
>
> But I also wanted the same for the diet C, to do this I renamed diet C
> to A in the data sheet to force C to be the dummy variable. Is this
> the right way to do it?
>
> When running the intervals () once again, I got this message: "Cannot
> get confidence intervals on var-cov components: Non-positive definite
> approximate variance-covariance". What could be wrong..? Is there
> other ways to get the slope and confidence intervals from a lme model?
>
>> intervals(lmefit1)
> Approximate 95% confidence intervals
>
>  Fixed effects:
>                           lower               est.                  
> upper
> (Intercept)           66.040673     108.122242     150.203810
> DietC                -175.080336   -109.638518     -44.196700
> Time                     4.177387         5.434087       6.690788
> DietC:Time          7.938101         11.180806     14.423511
> attr(,"label")
> [1] "Fixed effects:"
>
>  Random Effects:
>   Level: Place
>                     lower     est.    upper
> sd((Intercept)) 0.1478599 13.50651 1233.775
>
>  Within-group standard error:
>    lower     est.    upper
> 159.9128 174.8928 191.2761
>
> Best regards Cotter



From dimitris.rizopoulos at med.kuleuven.be  Tue Sep  2 16:31:53 2008
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 02 Sep 2008 16:31:53 +0200
Subject: [R-sig-ME] Error: "Cannot get confidence intervals...", with lme,
 what does it means?
In-Reply-To: <742479270809020529h7bfa410brbf84578354939e6a@mail.gmail.com>
References: <742479270809020529h7bfa410brbf84578354939e6a@mail.gmail.com>
Message-ID: <48BD4E59.9000704@med.kuleuven.be>

check whether the following solves the problem:

Total$DietC <- relevel(Total$Diet, "C")

lmefit1 <- lme(Weight ~ Diet * Time, Total, random = ~ 1 | Place)
intervals(lmefit1)

lmefit2 <- lme(Weight ~ DietC * Time, Total, random = ~ 1 | Place)
intervals(lmefit2)


I hope it helps.

Best,
Dimtris


R.S. Cotter wrote:
> Hello,
> 
> In some occasions I get this error message: "Cannot get confidence
> intervals on var-cov components: Non-positive definite approximate
> variance-covariance".
> 
> I have tried to figure out this by using help function, but didn't
> find answer to the question. I address this question with describing
> the model and the primary task that I want to solve. Sorry if the
> question is clumsy formulated, I 'm not that experienced with R and
> statistics.
> 
> My model is:
> Response= Weight(continous)
> Explanatory variables= Time (continous) and Diet (kategorical, two groups; B&C)
> 
> The primary question of interest is wheter the growth rates
> (Weight/Time) differ among the two diets.
> 
> lmefit1<-lme(Weight ~ Diet*Time,random=~1|Place,data=Total)
> 
> Summary output is ok, so far so good. But I also wanted to get the
> slope and confidence intervals for the growth rates for both diets
> (B&C), so I ran intervals(). And I got the intercept, slope and
> confidence intervals for diet B, see below.
> 
> But I also wanted the same for the diet C, to do this I renamed diet C
> to A in the data sheet to force C to be the dummy variable. Is this
> the right way to do it?
> 
> When running the intervals () once again, I got this message: "Cannot
> get confidence intervals on var-cov components: Non-positive definite
> approximate variance-covariance". What could be wrong..? Is there
> other ways to get the slope and confidence intervals from a lme model?
> 
>> intervals(lmefit1)
> Approximate 95% confidence intervals
> 
>  Fixed effects:
>                           lower               est.                 upper
> (Intercept)           66.040673     108.122242     150.203810
> DietC                -175.080336   -109.638518     -44.196700
> Time                     4.177387         5.434087       6.690788
> DietC:Time          7.938101         11.180806     14.423511
> attr(,"label")
> [1] "Fixed effects:"
> 
>  Random Effects:
>   Level: Place
>                     lower     est.    upper
> sd((Intercept)) 0.1478599 13.50651 1233.775
> 
>  Within-group standard error:
>    lower     est.    upper
> 159.9128 174.8928 191.2761
> 
> Best regards Cotter
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 

-- 
Dimitris Rizopoulos
Assistant Professor
Department of Biostatistics
Erasmus University Medical Center

Address: PO Box 2040, 3000 CA Rotterdam, the Netherlands
Tel: +31/(0)10/7043399
Fax: +31/(0)10/7044657


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From cat.dev.urandom at gmail.com  Wed Sep  3 05:34:51 2008
From: cat.dev.urandom at gmail.com (D Chaws)
Date: Tue, 2 Sep 2008 23:34:51 -0400
Subject: [R-sig-ME] Adjusting for baseline differences in growth models
Message-ID: <dcf23fb80809022034i65af6842u6392fd95711ecf5e@mail.gmail.com>

Hi all, I posted a long question for this the other day, but no one
has responded.  Here is a short version those of you who are very busy
might have time to read.  Any advice would be greatly appreciated.

I have a growth model where 2 groups differ on the outcome at
baseline.  How do I get the growth model adjusting for this difference
(or making the groups equivalent at baseline, such as in ANCOVA)?
Predict growth from a 2nd(person)-level covariate of baseline scores
(i.e., a growth:baseline covariate interaction in a mixed model)?
Does it make sense to add the baseline covariate as a predictor of the
intercepts as well as the slopes or is this completely redundant with
a model where time is coded so that 0 = baseline?

Thanks again.

- DC



From s.ruiter at maw.ru.nl  Wed Sep  3 14:14:35 2008
From: s.ruiter at maw.ru.nl (Stijn Ruiter)
Date: Wed, 03 Sep 2008 14:14:35 +0200
Subject: [R-sig-ME] Fwd: help with a cross-classified random effects
 model code in R.
Message-ID: <48BE7FAB.20605@maw.ru.nl>

Dear Dr. Bates,
You replied to a question by Violet(Shu) Xu on how to estimate
cross-classified (XC) models. In the DIGEST version however, no example
code is provided.
In general, how do we estimate XC models using lme4?

Is the following example for a null model for some dependent variable Y
for pupils who attended specific primary and secondary schools correct?
Or does lmer then estimate a nested model?

lmer(Y~(1|primaryschool)+(1|secondaryschool),data=dataname)

Kind regards,
Stijn Ruiter

-- 
Best regards,

Stijn Ruiter
Department of Sociology / ICS
Radboud University Nijmegen
P.O. Box 9104
6500 HE Nijmegen
Netherlands

Phone: + 31 24 361 2272
Fax:   + 31 24 361 2399

Visiting address:
Thomas van Aquinostraat 4.01.71
Nijmegen

website: http://oase.uci.ru.nl/~sruiter



From bates at stat.wisc.edu  Wed Sep  3 14:44:55 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 3 Sep 2008 07:44:55 -0500
Subject: [R-sig-ME] Fwd: help with a cross-classified random effects
	model code in R.
In-Reply-To: <48BE7FAB.20605@maw.ru.nl>
References: <48BE7FAB.20605@maw.ru.nl>
Message-ID: <40e66e0b0809030544mbd05508n58a4a4af9250b7a5@mail.gmail.com>

On Wed, Sep 3, 2008 at 7:14 AM, Stijn Ruiter <s.ruiter at maw.ru.nl> wrote:
> Dear Dr. Bates,
> You replied to a question by Violet(Shu) Xu on how to estimate
> cross-classified (XC) models. In the DIGEST version however, no example
> code is provided.
> In general, how do we estimate XC models using lme4?

> Is the following example for a null model for some dependent variable Y
> for pupils who attended specific primary and secondary schools correct?
> Or does lmer then estimate a nested model?

> lmer(Y~(1|primaryschool)+(1|secondaryschool),data=dataname)

That will estimate the model with crossed random effects.  An example
of exactly this type is

> data(ScotsSec, package = "mlmRev")
> library(lme4)
Loading required package: Matrix
Loading required package: lattice

Attaching package: 'Matrix'


	The following object(s) are masked from package:stats :

	 xtabs

> (fm1 <- lmer(attain ~ (1|primary) + (1|second), ScotsSec))
Linear mixed model fit by REML
Formula: attain ~ (1 | primary) + (1 | second)
   Data: ScotsSec
   AIC   BIC logLik deviance REMLdev
 17159 17183  -8575    17149   17151
Random effects:
 Groups   Name        Variance Std.Dev.
 primary  (Intercept) 1.13002  1.0630
 second   (Intercept) 0.37222  0.6101
 Residual             8.11069  2.8479
Number of obs: 3435, groups: primary, 148; second, 19

Fixed effects:
            Estimate Std. Error t value
(Intercept)   5.5017     0.1787   30.79

To see that the primary and secondary schools classifications are not
nested you can check the image of the cross classification of those
factors produced by

image(xtabs(~ primary + second, ScotsSec, sparse = TRUE))

I enclose a PDF file of that image.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: ScotsSec.pdf
Type: application/pdf
Size: 59109 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080903/14fce9d1/attachment.pdf>

From danielezrajohnson at gmail.com  Wed Sep  3 14:52:10 2008
From: danielezrajohnson at gmail.com (Daniel Ezra Johnson)
Date: Wed, 3 Sep 2008 13:52:10 +0100
Subject: [R-sig-ME] Fwd: help with a cross-classified random effects
	model code in R.
In-Reply-To: <40e66e0b0809030544mbd05508n58a4a4af9250b7a5@mail.gmail.com>
References: <48BE7FAB.20605@maw.ru.nl>
	<40e66e0b0809030544mbd05508n58a4a4af9250b7a5@mail.gmail.com>
Message-ID: <a46630750809030552w269ab0f3t6174c1abfcbc4a54@mail.gmail.com>

Am I right that if the primary schools were nested within the
secondary schools, the model would still be fit with the exact same
formula?

D

On Wed, Sep 3, 2008 at 1:44 PM, Douglas Bates <bates at stat.wisc.edu> wrote:
> On Wed, Sep 3, 2008 at 7:14 AM, Stijn Ruiter <s.ruiter at maw.ru.nl> wrote:
>> Dear Dr. Bates,
>> You replied to a question by Violet(Shu) Xu on how to estimate
>> cross-classified (XC) models. In the DIGEST version however, no example
>> code is provided.
>> In general, how do we estimate XC models using lme4?
>
>> Is the following example for a null model for some dependent variable Y
>> for pupils who attended specific primary and secondary schools correct?
>> Or does lmer then estimate a nested model?
>
>> lmer(Y~(1|primaryschool)+(1|secondaryschool),data=dataname)
>
> That will estimate the model with crossed random effects.



From bates at stat.wisc.edu  Wed Sep  3 16:32:50 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 3 Sep 2008 09:32:50 -0500
Subject: [R-sig-ME] Fwd: help with a cross-classified random effects
	model code in R.
In-Reply-To: <a46630750809030552w269ab0f3t6174c1abfcbc4a54@mail.gmail.com>
References: <48BE7FAB.20605@maw.ru.nl>
	<40e66e0b0809030544mbd05508n58a4a4af9250b7a5@mail.gmail.com>
	<a46630750809030552w269ab0f3t6174c1abfcbc4a54@mail.gmail.com>
Message-ID: <40e66e0b0809030732l4d018c14o5d4abe345e68359e@mail.gmail.com>

On Wed, Sep 3, 2008 at 7:52 AM, Daniel Ezra Johnson
<danielezrajohnson at gmail.com> wrote:
> Am I right that if the primary schools were nested within the
> secondary schools, the model would still be fit with the exact same
> formula?

The short answer is "Yes".

The somewhat longer answer is "Yes, as long as you have coded the
primary schools so that each distinct school has a distinct label".
If you can't imagine why someone would do something other than code
the distinct schools with distinct labels then you should stop reading
now.  The rest of the discussion is about why a peculiar but
commonplace practice of coding something like the primary schools
using implicitly nested factors could trip you up.

The only time that the distinction between nested and non-nested is
important is when the "inner" factor is coded using implicit nesting.
Suppose that the primary schools were nested within the secondary
schools and instead of coding the 148 primary schools as a factor
having 148 levels they were coded as a factor with, say,  20 levels
with the implicit understanding that primary school 1 feeding
secondary school 1 is different from primary school 1 feeding
secondary school 2.  It is a peculiar and error-prone way of doing
things but it is also widely used.  In that case all that is necessary
is to create a primary school factor as the interaction of the
secondary school and the "primary within secondary" factor.

> On Wed, Sep 3, 2008 at 1:44 PM, Douglas Bates <bates at stat.wisc.edu> wrote:
>> On Wed, Sep 3, 2008 at 7:14 AM, Stijn Ruiter <s.ruiter at maw.ru.nl> wrote:
>>> Dear Dr. Bates,
>>> You replied to a question by Violet(Shu) Xu on how to estimate
>>> cross-classified (XC) models. In the DIGEST version however, no example
>>> code is provided.
>>> In general, how do we estimate XC models using lme4?
>>
>>> Is the following example for a null model for some dependent variable Y
>>> for pupils who attended specific primary and secondary schools correct?
>>> Or does lmer then estimate a nested model?
>>
>>> lmer(Y~(1|primaryschool)+(1|secondaryschool),data=dataname)
>>
>> That will estimate the model with crossed random effects.
>



From ivar.herfindal at bio.ntnu.no  Thu Sep  4 17:06:38 2008
From: ivar.herfindal at bio.ntnu.no (Ivar Herfindal)
Date: Thu, 04 Sep 2008 17:06:38 +0200
Subject: [R-sig-ME] Lmer-model fails to converge
Message-ID: <48BFF97E.4010106@bio.ntnu.no>

Dear Mixed-list

I am trying to fit a mixed linear model with the lmer-function in the 
lme4-packages. After fitting the model, I get this warning:
Warning message:
In mer_finalize(ans) : iteration limit reached without convergence (9)

By searching the R-archive, I found some sort of solution posted by 
Douglas Bates 
(http://finzi.psych.upenn.edu/R/Rhelp02a/archive/138008.html) which will 
provide a greater number of iterations. I therefore tried:

(newmodel <- .Call("mer_optimize", mylmermodel, PACKAGE = "lme4"))

The "FALSE" argument in Douglas Bates suggestion caused an error 
message, but it works fine without, and the model do now converge. 
However, I cannot figure out how to get the model from the last part of 
the iterations. That is, the .Call("mer_optimize"...) only print the 
verbose from the fitting process, but does not give an mer-object that I 
can evaluate and extract random and fixed effects from. Does anyone know 
if this "horrible hack" (Bates' own words) can give a mer-object or can 
I only use it to evaluate how far my initial model was from convergence? 
I am sorry that I cannot provide any example from my own data (the 
dataset is too large to attach), but I assume that any solution should 
be independent of the model or data.

Cheers

Ivar

SessionInfo
sessionInfo()
R version 2.7.2 (2008-08-25)
i386-pc-mingw32

locale:
LC_COLLATE=Norwegian (Bokm?l)_Norway.1252;LC_CTYPE=Norwegian 
(Bokm?l)_Norway.1252;LC_MONETARY=Norwegian 
(Bokm?l)_Norway.1252;LC_NUMERIC=C;LC_TIME=Norwegian (Bokm?l)_Norway.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base    

other attached packages:
[1] mgcv_1.4-1         splancs_2.01-24    sp_0.9-26          
lme4_0.999375-26   Matrix_0.999375-13 lattice_0.17-13  

loaded via a namespace (and not attached):
[1] grid_2.7.2
 >



From bates at stat.wisc.edu  Thu Sep  4 20:17:51 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 4 Sep 2008 13:17:51 -0500
Subject: [R-sig-ME] Lmer-model fails to converge
In-Reply-To: <48BFF97E.4010106@bio.ntnu.no>
References: <48BFF97E.4010106@bio.ntnu.no>
Message-ID: <40e66e0b0809041117s5c88aaf2o5c3084c1ed5e1e02@mail.gmail.com>

On Thu, Sep 4, 2008 at 10:06 AM, Ivar Herfindal
<ivar.herfindal at bio.ntnu.no> wrote:
> Dear Mixed-list
>
> I am trying to fit a mixed linear model with the lmer-function in the
> lme4-packages. After fitting the model, I get this warning:
> Warning message:
> In mer_finalize(ans) : iteration limit reached without convergence (9)
>
> By searching the R-archive, I found some sort of solution posted by Douglas
> Bates (http://finzi.psych.upenn.edu/R/Rhelp02a/archive/138008.html) which
> will provide a greater number of iterations. I therefore tried:

> (newmodel <- .Call("mer_optimize", mylmermodel, PACKAGE = "lme4"))

> The "FALSE" argument in Douglas Bates suggestion caused an error message,
> but it works fine without, and the model do now converge. However, I cannot
> figure out how to get the model from the last part of the iterations. That
> is, the .Call("mer_optimize"...) only print the verbose from the fitting
> process, but does not give an mer-object that I can evaluate and extract
> random and fixed effects from. Does anyone know if this "horrible hack"
> (Bates' own words) can give a mer-object or can I only use it to evaluate
> how far my initial model was from convergence? I am sorry that I cannot
> provide any example from my own data (the dataset is too large to attach),
> but I assume that any solution should be independent of the model or data.

Ah, you are working with a very recent version of the lme4 package.  I
will commit another version of the lme4 package later this afternoon
with the ability to specify

lmer(..., control = list(maxIter = 500))

to increase the number of function evaluations.

However, that "horrible hack" that I described previously would have
updated the value of mylmermodel to the new parameter values so you
actually have the refitted model.  That is part of the horrible aspect
- this is not supposed to happen in an R function.  An R function
should not change the value of its arguments but this one does.  The
reason is efficiency - if the arguments were being copied to the
result at every iteration most of the time would be spent copying the
structure representing the model.  Especially with very large data
sets like yours, things would slow down to a crawl.  So some of those
C functions called with .Call, like "mer_optimize" here, do things
that they shouldn't.

> Cheers
>
> Ivar
>
> SessionInfo
> sessionInfo()
> R version 2.7.2 (2008-08-25)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=Norwegian (Bokm?l)_Norway.1252;LC_CTYPE=Norwegian
> (Bokm?l)_Norway.1252;LC_MONETARY=Norwegian
> (Bokm?l)_Norway.1252;LC_NUMERIC=C;LC_TIME=Norwegian (Bokm?l)_Norway.1252
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> other attached packages:
> [1] mgcv_1.4-1         splancs_2.01-24    sp_0.9-26
>  lme4_0.999375-26   Matrix_0.999375-13 lattice_0.17-13
> loaded via a namespace (and not attached):
> [1] grid_2.7.2
>>
>
>
>



From kyler at mail.smu.edu  Thu Sep  4 22:31:00 2008
From: kyler at mail.smu.edu (Roberts, Kyle)
Date: Thu, 4 Sep 2008 15:31:00 -0500
Subject: [R-sig-ME] lme with ECLS
In-Reply-To: <40e66e0b0809041117s5c88aaf2o5c3084c1ed5e1e02@mail.gmail.com>
References: <48BFF97E.4010106@bio.ntnu.no>
	<40e66e0b0809041117s5c88aaf2o5c3084c1ed5e1e02@mail.gmail.com>
Message-ID: <6FBE93B66C50154D9F1E55B674AFCC9D0104A607@s31xe7.systems.smu.edu>

Friends,

I am running the ECLS dataset with lme (long story on why I couldn't use lmer; mostly political) and am having trouble.  Here's the model:

m.null<-lme(MATH~TIME, random=~TIME|CHILDID, ecls, na.action=na.omit, weights=varFixed(~C1_6SC0))

When I ran the model without the weighting variable, it converged in about a minute (~17000 kids on 4 measurement occasions). But with the weights the thing has been running for about 24 hours without coming to a solution.

Any ideas????

R 2.7.1
HP workstation with 4G RAM, Windows XP

Thanks,
Kyle


*********************************************************
Dr. J. Kyle Roberts
Department of Literacy, Language, and Learning
School of Education and Human Development
Southern Methodist University
P.O. Box 750381
Dallas, TX? 75275
214-768-4494
http://www.hlm-online.com/



From queirozrafaelmv at yahoo.com.br  Fri Sep  5 00:19:19 2008
From: queirozrafaelmv at yahoo.com.br (Rafael Maia)
Date: Thu, 4 Sep 2008 19:19:19 -0300
Subject: [R-sig-ME] Error: "Cannot get confidence intervals...", with lme,
	what does it means?
In-Reply-To: <F32C20E3-2046-468C-A9D8-87CD30A1DAD6@mail.nih.gov>
References: <742479270809020529h7bfa410brbf84578354939e6a@mail.gmail.com>
	<F32C20E3-2046-468C-A9D8-87CD30A1DAD6@mail.nih.gov>
Message-ID: <DA197CED-FEBA-4D6F-8F72-5BE3BC88E847@yahoo.com.br>

Hello!

Sorry to keep this going, but could you elaborate on the meaning of  
that output? I am asking this because I am trying to model my data  
set, and with the same model, I get that output if I use REML, but I  
get a normal output if I use ML - so I'm assuming it may be related to  
the model, but also in the estimating method. The intervals calculated  
using intervals() under the ML method are very similar to the ones I  
obtain when computing them by hand.

Under my limited knowledge, I believe I am using the most appropriate  
model for my data, even though it is definitely not a "good" one for  
the data (which is inherently unbalanced).

Commands follow, below. Many thanks for any help!

Best,
Rafael

 > intervals(m1)
Error in intervals.lme(m1) :
  Cannot get confidence intervals on var-cov components: Non-positive  
definite approximate variance-covariance

 > m1<-update(m1, method="ML")

 > intervals(m1)
Approximate 95% confidence intervals

Fixed effects:
                     lower      est.      upper
(Intercept)      0.2268163  1.318894  2.4109708
factor(Status)2  0.7653039  1.812026  2.8587479
Ano             -1.7888219 -1.112453 -0.4360846
attr(,"label")
[1] "Fixed effects:"

Random Effects:
  Level: ind
                                         lower       est.       upper
sd((Intercept))                   1.692475e-02  0.9802002    56.76849
sd(factor(Status)2)               2.009503e-05  0.8586423 36689.01183
cor((Intercept),factor(Status)2) -1.000000e+00 -0.8485173     1.00000

Within-group standard error:
       lower         est.        upper
3.685400e-14 3.630675e-01 3.576762e+12



On 2 Sep 2008, at 11:09, Gang Chen wrote:

> Cotter,
>
> Check the following component
>
>> lmefit1$apVar
>
> If you see something like this
>
> [1] "Non-positive definite approximate variance-covariance"
>
> it most likely indicates you have an inappropriate model for the  
> data. Try plotting out the data, and get some idea about the  
> feasible models, and then fit the data with those models.
>
> Cheers,
> Gang
>
>
> On Sep 2, 2008, at 8:29 AM, R.S. Cotter wrote:
>
>> Hello,
>>
>> In some occasions I get this error message: "Cannot get confidence
>> intervals on var-cov components: Non-positive definite approximate
>> variance-covariance".
>>
>> I have tried to figure out this by using help function, but didn't
>> find answer to the question. I address this question with describing
>> the model and the primary task that I want to solve. Sorry if the
>> question is clumsy formulated, I 'm not that experienced with R and
>> statistics.
>>
>> My model is:
>> Response= Weight(continous)
>> Explanatory variables= Time (continous) and Diet (kategorical, two  
>> groups; B&C)
>>
>> The primary question of interest is wheter the growth rates
>> (Weight/Time) differ among the two diets.
>>
>> lmefit1<-lme(Weight ~ Diet*Time,random=~1|Place,data=Total)
>>
>> Summary output is ok, so far so good. But I also wanted to get the
>> slope and confidence intervals for the growth rates for both diets
>> (B&C), so I ran intervals(). And I got the intercept, slope and
>> confidence intervals for diet B, see below.
>>
>> But I also wanted the same for the diet C, to do this I renamed  
>> diet C
>> to A in the data sheet to force C to be the dummy variable. Is this
>> the right way to do it?
>>
>> When running the intervals () once again, I got this message: "Cannot
>> get confidence intervals on var-cov components: Non-positive definite
>> approximate variance-covariance". What could be wrong..? Is there
>> other ways to get the slope and confidence intervals from a lme  
>> model?
>>
>>> intervals(lmefit1)
>> Approximate 95% confidence intervals
>>
>> Fixed effects:
>>                         lower               est.                  
>> upper
>> (Intercept)           66.040673     108.122242     150.203810
>> DietC                -175.080336   -109.638518     -44.196700
>> Time                     4.177387         5.434087       6.690788
>> DietC:Time          7.938101         11.180806     14.423511
>> attr(,"label")
>> [1] "Fixed effects:"
>>
>> Random Effects:
>> Level: Place
>>                   lower     est.    upper
>> sd((Intercept)) 0.1478599 13.50651 1233.775
>>
>> Within-group standard error:
>>  lower     est.    upper
>> 159.9128 174.8928 191.2761
>>
>> Best regards Cotter
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From ken at kjbeath.com.au  Fri Sep  5 00:56:31 2008
From: ken at kjbeath.com.au (Ken Beath)
Date: Fri, 5 Sep 2008 08:56:31 +1000
Subject: [R-sig-ME] Adjusting for baseline differences in growth models
In-Reply-To: <dcf23fb80809022034i65af6842u6392fd95711ecf5e@mail.gmail.com>
References: <dcf23fb80809022034i65af6842u6392fd95711ecf5e@mail.gmail.com>
Message-ID: <841F7227-9C60-4BE3-BBB9-5BF677ABD7A6@kjbeath.com.au>

On 03/09/2008, at 1:34 PM, D Chaws wrote:

> Hi all, I posted a long question for this the other day, but no one
> has responded.  Here is a short version those of you who are very busy
> might have time to read.  Any advice would be greatly appreciated.
>
> I have a growth model where 2 groups differ on the outcome at
> baseline.  How do I get the growth model adjusting for this difference
> (or making the groups equivalent at baseline, such as in ANCOVA)?
> Predict growth from a 2nd(person)-level covariate of baseline scores
> (i.e., a growth:baseline covariate interaction in a mixed model)?
> Does it make sense to add the baseline covariate as a predictor of the
> intercepts as well as the slopes or is this completely redundant with
> a model where time is coded so that 0 = baseline?
>

Fitting a mixed-effects model for the post baseline outcomes only with  
the baseline as a covariate affecting only the intercept seems  
sensible. This is the same as the ANCOVA but with repeated measures.

Ken


> Thanks again.
>
> - DC
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Fri Sep  5 14:55:28 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 5 Sep 2008 07:55:28 -0500
Subject: [R-sig-ME] lme with ECLS
In-Reply-To: <6FBE93B66C50154D9F1E55B674AFCC9D0104A607@s31xe7.systems.smu.edu>
References: <48BFF97E.4010106@bio.ntnu.no>
	<40e66e0b0809041117s5c88aaf2o5c3084c1ed5e1e02@mail.gmail.com>
	<6FBE93B66C50154D9F1E55B674AFCC9D0104A607@s31xe7.systems.smu.edu>
Message-ID: <40e66e0b0809050555vd64cca7of90036940563d1fb@mail.gmail.com>

On Thu, Sep 4, 2008 at 3:31 PM, Roberts, Kyle <kyler at mail.smu.edu> wrote:
> Friends,

> I am running the ECLS dataset with lme (long story on why I couldn't use lmer; mostly political) and am having trouble.  Here's the model:

> m.null<-lme(MATH~TIME, random=~TIME|CHILDID, ecls, na.action=na.omit, weights=varFixed(~C1_6SC0))

> When I ran the model without the weighting variable, it converged in about a minute (~17000 kids on 4 measurement occasions). But with the weights the thing has been running for about 24 hours without coming to a solution.

The first thing to try is adding control = list(msVerbose = TRUE) in
the call to lme.  I would be interested in whether the difference in
running time is due to a change in the time per iteration or due to a
huge increase in the number of iterations, indicating that lme is
failing to converge.

What should be happening is that the response and the model matrices
are "pre-whitened".  That is, they are multiplied by the square root
of the weights.  That shouldn't cause such an extreme difference in
running times though.

Did you try fitting the equivalent models using lmer?  That type of
model and size of data set shouldn't take very long and it would give
you a reference fit.  One thing to watch for is whether the weighted
fit corresponds to a singular covariance matrix for the random
effects.  A big difference between lme and lmer is that lmer works
with the factor of the covariance matrix whereas lme works with the
factor of the precision matrix, which is the inverse of the covariance
matrix.  Check for lmer fits giving estimates of the correlation of
the random effects near -1 or +1.  If you use the optional argument
verbose = TRUE in lmer you will see that there are three parameters in
the optimization and you want to watch for one of the first two (most
likely the second) parameter getting close to zero.

I presume that the ECLS data are the "Early Childhood Longitudinal
Program (ECLS)" data described at http://nces.ed.gov/ECLS/index.asp
(apparently named by people who haven't quite grasped all the
subtleties of acronym construction).  Just out of interest, what is
the variable C1_6SC0?



From kyler at smu.edu  Fri Sep  5 16:51:06 2008
From: kyler at smu.edu (Kyle Roberts)
Date: Fri, 05 Sep 2008 09:51:06 -0500
Subject: [R-sig-ME] lme with ECLS
In-Reply-To: <40e66e0b0809050555vd64cca7of90036940563d1fb@mail.gmail.com>
Message-ID: <C4E6B18A.2DBB%kyler@smu.edu>

Using lmer, I got the same error that Sundar got on August 4. I am using
lmer_0.999375-24. Also, Doug, the C1_6SC0 is the ECLS weighting variable for
the 6th grade sample.

Thanks all,
Kyle

> m.null<-lmer(MATH~time + (time|CHILDID), data=ecls, na.action=na.omit,
weights=C1_6SC0, control=list(msVerbose=TRUE))
Error in mer_finalize(ans, verbose) :
  Calculated PWRSS for a LMM is negative
> 
> sessionInfo()
R version 2.7.1 (2008-06-23)
i386-apple-darwin8.10.1

locale:
en_US.UTF-8/en_US.UTF-8/C/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] car_1.2-8          foreign_0.8-26     lme4_0.999375-24
[4] Matrix_0.999375-11 lattice_0.17-8

loaded via a namespace (and not attached):
[1] grid_2.7.1  tools_2.7.1


On 9/5/08 7:55 AM, "Doug Bates" <bates at stat.wisc.edu> wrote:

> On Thu, Sep 4, 2008 at 3:31 PM, Roberts, Kyle <kyler at mail.smu.edu> wrote:
>> Friends,
> 
>> I am running the ECLS dataset with lme (long story on why I couldn't use
>> lmer; mostly political) and am having trouble.  Here's the model:
> 
>> m.null<-lme(MATH~TIME, random=~TIME|CHILDID, ecls, na.action=na.omit,
>> weights=varFixed(~C1_6SC0))
> 
>> When I ran the model without the weighting variable, it converged in about a
>> minute (~17000 kids on 4 measurement occasions). But with the weights the
>> thing has been running for about 24 hours without coming to a solution.
> 
> The first thing to try is adding control = list(msVerbose = TRUE) in
> the call to lme.  I would be interested in whether the difference in
> running time is due to a change in the time per iteration or due to a
> huge increase in the number of iterations, indicating that lme is
> failing to converge.
> 
> What should be happening is that the response and the model matrices
> are "pre-whitened".  That is, they are multiplied by the square root
> of the weights.  That shouldn't cause such an extreme difference in
> running times though.
> 
> Did you try fitting the equivalent models using lmer?  That type of
> model and size of data set shouldn't take very long and it would give
> you a reference fit.  One thing to watch for is whether the weighted
> fit corresponds to a singular covariance matrix for the random
> effects.  A big difference between lme and lmer is that lmer works
> with the factor of the covariance matrix whereas lme works with the
> factor of the precision matrix, which is the inverse of the covariance
> matrix.  Check for lmer fits giving estimates of the correlation of
> the random effects near -1 or +1.  If you use the optional argument
> verbose = TRUE in lmer you will see that there are three parameters in
> the optimization and you want to watch for one of the first two (most
> likely the second) parameter getting close to zero.
> 
> I presume that the ECLS data are the "Early Childhood Longitudinal
> Program (ECLS)" data described at http://nces.ed.gov/ECLS/index.asp
> (apparently named by people who haven't quite grasped all the
> subtleties of acronym construction).  Just out of interest, what is
> the variable C1_6SC0?



From mariaevagongora at hotmail.com  Fri Sep  5 21:03:27 2008
From: mariaevagongora at hotmail.com (Maria Eva Gongora)
Date: Fri, 5 Sep 2008 16:03:27 -0300
Subject: [R-sig-ME] empty cell
Message-ID: <BAY120-W3102D27A22830B1A0B4A70B6580@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080905/3f9cf9f5/attachment.pl>

From mareike.kohlmann at stat.uni-muenchen.de  Mon Sep  8 08:43:59 2008
From: mareike.kohlmann at stat.uni-muenchen.de (Mareike Kohlmann)
Date: Mon, 8 Sep 2008 08:43:59 +0200 (CEST)
Subject: [R-sig-ME] Kronecker Product Covariance matrix in nlme?
Message-ID: <33825.194.120.84.9.1220856239.squirrel@webmail.lrz-muenchen.de>

Hi,

I want to fit a multivariate covariance pattern model by assuming a
Kronecker product structure kronecker(U,V) for the covariance matrix
(Dimension pq x pq). This can be applied to multivariate longitudinal
data, for example, where the matrix U (with dimension qxq) is the
covariance matrix for the variables and V (with dimension p x p) the
covariance matrix to model the time dependencies.

In SAS proc mixed, this can be achieved by specifying UN at CS, UN at AR1 or
UN at UN (UN for the variables/ CS, AR1 or UN for the time structure) in the
REPEATED statement.

Is is possible to implement this model in nlme?

I suppose I have to write my own "corStruct" class and then use the weight
statement to get variable-specific variance estimates?

Thanks for your help!

Mareike


***************************************
Mareike Kohlmann
Department of Statistics
Ludwig-Maximilians-University Munich
Germany
E-Mail: mareike.kohlmann at stat.uni-muenchen.de



From dafshartous at med.miami.edu  Mon Sep  8 16:05:39 2008
From: dafshartous at med.miami.edu (David Afshartous)
Date: Mon, 08 Sep 2008 10:05:39 -0400
Subject: [R-sig-ME] VarCorr vs ranef
In-Reply-To: <dcf23fb80808310552j54279c83u90e438dde7e8e9d1@mail.gmail.com>
Message-ID: <C4EAA973.730C%dafshartous@med.miami.edu>


For a nice discussion of this topic see p.459 of Gelman & Hill (2007).
(see http://www.stat.columbia.edu/~gelman/)


On 8/31/08 8:52 AM, "D Chaws" <cat.dev.urandom at gmail.com> wrote:

> Thanks so much for the reply.  This still seems very strange.  Even if the
> differences between population and subject effects is the issue, wouldn't
> one expect a bit more similarity between the actual effects for the subjects
> and the population effects inferred from those effects?  Dr. Bates or anyone
> else, can you resolve this mystery?  Alternatively, is there a way to get
> population estimates of the random effects for subjects (contradiction in
> terms?), like fitted.lme with the level = 0 argument?
> 
> All this is in service of an attempt to gain a simple scatterplot between
> two random effects that closely reflect the estimates from VarCorr or
> summary.  I'm sure someone must have a method for this already worked out.
> pairs.lme plots the raw data from ranef, so the discrepancy is still a
> problem there.
> 
> Thanks so much for your help.
> 
> - DC
> 
> On Sun, Aug 31, 2008 at 6:13 AM, Daniel Ezra Johnson <
> danielezrajohnson at gmail.com> wrote:
> 
>> On Sun, Aug 31, 2008 at 6:53 AM, D Chaws <cat.dev.urandom at gmail.com>
>> wrote:
>>> Can someone tell me why correlations between raw random effects are
>>> different from that provided in VarCorr for lme models?
>>> For example:
>>> 
>>> fm1 = lme(distance ~ I(age-8), random = ~ 1 + I(age-8) | Subject, data =
>>> Orthodont)
>>> R# VarCorr(fm1)
>>> Subject = pdLogChol(1 + I(age - 8))
>>>            Variance StdDev Corr
>>> (Intercept) 3.55937  1.8866 (Intr)
>>> I(age - 8)  0.05127  0.2264 0.209
>>> Residual    1.71620  1.3100
>>> 
>>> and
>>> 
>>> R# cor(ranef(fm1))
>>>            (Intercept) I(age - 8)
>>> (Intercept)      1.0000     0.5764
>>> I(age - 8)       0.5764     1.0000
>>> 
>> 
>> This isn't a complete answer, but the figures in VarCorr and the model
>> summary are the population estimates for the random effects (the
>> parameters) while everything derived from ranef() refers to the actual
>> Subjects in the data (the BLUPs).
>> 
>> Look at:
>> 
>>> sd(ranef(fm1))
>> (Intercept)  I(age - 8)
>>  1.7359554   0.1557322
>> 
>> Those figures don't match the VarCorr standard deviations either,
>> especially the second.
>> 
>> I don't know why the BLUPs pattern differently, exactly, but I did
>> look at plot(coefs(fm1)) which suggested Sex should be added as a
>> fixed effect. Once I did that, the correlation between the random
>> effects changed quite a lot (but was still different between VarCorr
>> and ranef; the population correlation was actually negative...)
>> 
>> D
>> 
> 
> [[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From Thierry.ONKELINX at inbo.be  Tue Sep  9 10:15:26 2008
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 9 Sep 2008 10:15:26 +0200
Subject: [R-sig-ME] [R] correct lme syntax for this problem?
In-Reply-To: <3f547caa0809081610r11b71da6g5c2353110991bd6e@mail.gmail.com>
References: <3f547caa0809081610r11b71da6g5c2353110991bd6e@mail.gmail.com>
Message-ID: <2E9C414912813E4EB981326983E0A104056983C6@inboexch.inbo.be>

Dear Matthew,

First of all I'm forwarding this to R-SIG-Mixed, which is a more
appropriate list for your question.
Using a mixed effect with only 5 levels is a borderline situation.
Douglas Bates recommends at least 6 levels in order to get a more or
less reliable estimate. So I would consider the populations as fixed
effects. Do you have repeated measurements of individuals within your
populations? If you do you could use those as random effects.

Your anova tests whether the variances of the random slope on SPI is
zero. I think you might want this:

mod1 <- lm(height ~ SPI * population + covariate1 + covariate2)
mod2 <- lm(height ~ SPI + population + covariate1 + covariate2)
anova(mod1, mod2)

HTH,

Thierry

------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium 
tel. + 32 54/436 185
Thierry.Onkelinx at inbo.be 
www.inbo.be 

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to
say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of
data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org]
Namens Matthew Keller
Verzonden: dinsdag 9 september 2008 1:10
Aan: R Help
Onderwerp: [R] correct lme syntax for this problem?

Hello all,

I am about to send off a manuscript and, although I am fairly
confident I have used the lme function correctly, I want to be 100%
sure. Could some kind soul out there put my mind at ease?

I am simply interested in whether a predictor (SPI) is related to
height. However, there are five different populations, and each may
differ in mean level of height as well as the relationship between SPI
and height. Thus, I also want to a) account for mean level differences
in height and b) check whether the relationship between height and SPI
is different between the groups. I hope this is sufficient
information.

height, SPI, covariate1, and covariate2 are numeric. population is a
factor with 5 levels. Here are the steps I took:

summary(mod1 <- lme(height ~ SPI + covariate1 + covariate2, random = ~
SPI | population))

summary(mod2 <- lme(height ~ SPI + covariate1 + covariate2, random = ~
1 | population))

anova(mod1,mod2) #this checks whether there is evidence for IQ & SPI
being related differently between the 5 populations.

Is this correct? THANKS!

Matt


-- 
Matthew C Keller
Asst. Professor of Psychology
University of Colorado at Boulder
www.matthewckeller.com

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document. The views expressed in  this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.



From David_Hinds at perlegen.com  Tue Sep  9 19:31:17 2008
From: David_Hinds at perlegen.com (David Hinds)
Date: Tue, 9 Sep 2008 10:31:17 -0700
Subject: [R-sig-ME] Error in mer_finalize(ans, verbose): q = xxx > n = yyy
In-Reply-To: <40e66e0b0801301236u4854fd64k447fd8bae2189cf8@mail.gmail.com>
References: <ABFA245EB97DD74EA88C78E71937E78104D4A449@ilpostino.perlegen.com>
	<40e66e0b0801301236u4854fd64k447fd8bae2189cf8@mail.gmail.com>
Message-ID: <9ED53B669FD50049AE0CE1168CD3C4BD017F80F51653@mtnexmb01.perlegen.com>

Could this error be changed to just a warning, or could we have an option to skip this test?  I'm using lme4 to fit binomial mixed effects models that do have reasonable solutions but are tripped up by this test.  I'm working around the problem for now by commenting it out.

In my case part of the problem is use of a "trick" I saw on this list, to allow for subgroup-specific random effects, i.e.:

   y ~ x + (0+x.1|z) + (0+x.2|z) + ...

where x.1, x.2, etc are indicators for levels of x.  Here, most of the random effects are fixed at 0 but they still count when evaluating whether the number of random effects exceeds the number of observations.

-- David Hinds



From john.maindonald at anu.edu.au  Wed Sep 10 08:49:35 2008
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Wed, 10 Sep 2008 16:49:35 +1000
Subject: [R-sig-ME] ML or REML for LR tests
In-Reply-To: <m0k5e0h1go.fsf@gmail.com>
References: <a46630750808281107u582a779fvec72e695356d1713@mail.gmail.com>
	<ED7B522EE00C9A4FA515AA71724D61EE0128C8DE@DC1EXCL01.air.org>
	<m0k5e0h1go.fsf@gmail.com>
Message-ID: <060F9ABC-7CA1-4C36-A6F1-F3BCA9B7BDDD@anu.edu.au>

Douglas -
I am taking up your reply to the question that Austin
Frank asked on August 29.  I'd like to check that my
understanding of the reply is correct.

Looking at page 59 of Pinheiro & Bates.  \theta is a
parameterization of the ratio of the inverse of the variance-
covariance matrix of the random effects, multiplied by \sigma^2,
e.g., in a model with 3 levels of random effects,
\theta = (\sigma/\sigma_1, \sigma/\sigma_2) might do the job.

Both the profiled deviance and the profiled REML criterion
are evaluated at each step of the optimization, albeit
at the current REML estimate of \theta.  (The ML deviance
can be determined without explicitly calculating the ML
estimates of the fixed and random effects.)

It makes little difference whether one evaluates the
likelihood deviance at the REML estimate of \theta or at
the ML estimate.  More importantly for the anova table,
the difference in deviances is not much altered.

Thanks in anticipation

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.


The reply was:

(From Doug Bates - 29 August 2008)

It may help to give a bit of background about what happens in the
anova method for mer objects in the lme4 package.

When fitting a linear mixed model, both the profiled deviance
(negative twice the log-likelihood) and the profiled REML criterion
are evaluated at each iteration during the optimization.  The word
"profiled" means that, although the likelihood or the REML criterion
are defined as functions of the fixed-effects parameters,
$\boldmath{\beta}$, the common scale parameter, $\sigma$, and the
parameters $\boldmath{\theta}$ that determine the relative covariance
matrix $\boldmath{ \Sigma}$ of the random effects, these criteria are
evaluated as a function of $\boldmath{\theta}$ alone, with
$\boldmath{\beta}$ and $\sigma$ at their conditionally optimal values.

The profiled REML criterion has a term that depends on the model
matrix X for the fixed-effects parameters.  If you change the
definition of the fixed-effects you will change the value of that
criterion in a systematic way that does not depend on how well the
respective models fit the observed data.  Thus, differences in the
REML criterion are not a meaningful way to compare two models that
differ in the definition of the fixed-effects.

If you look at the code for the anova method for the mer class you
will see that it always calls logLik with REML = FALSE.  That is, it
extracts the log-likelihood from the profiled deviance even if the
model has been fit according to the REML criterion.  In general that
would be a very bad idea if we were using the deviance and not the
profiled deviance.  You can't count on the deviance at the REML
estimates for the complete parameter vector being close to the optimal
deviance.  However, the profiled deviance at the REML estimates of
$\boldmath{\theta}$ is close to the optimal profiled deviance.

For example,

fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
## profiled deviance at the REML estimate of $\theta$
dput(d1 <- deviance(fm1, REML = FALSE))
structure(1751.98581110077, .Names = "ML")
## optimal value of the deviance
dput(d2 <- deviance(update(fm1, REML = FALSE)))
structure(1751.93934480597, .Names = "ML")
## amount by which the profiled deviance at REML estimate
## exceeds the optimal value
d1 - d2
     ML
0.04646629

As you can see, I have gravitated to using the term "REML criterion"
rather than calling it a deviance or a log-likelihood.  The print
method for mer objects still labels it as the REMLdeviance but I plan
to change that and create an additional extractor called REML to
return that value.  In that case the deviance extractor will always
return the deviance, perhaps after re-optimizing to produce the
optimal deviance.

So the bottom line is that the anova method for mer objects always
produces a likelihood ratio test based on the likelihood.  If you
compare models fit by REML that likelihood ratio statistic will be
somewhat inaccurate but only by a small amount (in all the cases that
I have seen).



From Robert.Espesser at lpl-aix.fr  Wed Sep 10 12:29:29 2008
From: Robert.Espesser at lpl-aix.fr (Robert ESPESSER)
Date: Wed, 10 Sep 2008 12:29:29 +0200 (CEST)
Subject: [R-sig-ME] warnings ranef and design of a binomial model
Message-ID: <18177911.27541221042570069.JavaMail.root@frontal1>

Dear R users,

I have a question about model design.
One group of 20 subjects run the A experiment, with 17 binary responses by subject.
An other group of 20 subjects run the 2 experiments B and C, with 20 binary responses by subject, 
for both of the 2 experiment.

I want to study the effect of the  factor "expe"  
I tried this covariance model:
mod.glmer <-  glmer( vd ~ dlms*expe +(1|subject) +(0+dlms|subject) , 
              family=binomial, ....)

where:
dlms is a numeric factor (-2,-1,0, 1,2)
expe is a 3 levels factor , coding the experiment (A or B or C)

vd is the success/fail matrix.

The model with a random slope is clearly better than an intercept-only model.

the results look plausible, ranef(mod.glmer, postVar=T) was OK for the
Intercept, but failed
to compute the postvar of the random slope , with these warnings:
1: In min(x): no argument found for min; Inf is returned
It's the same for max.

I'm  suspecting  that my model is perhaps badly designed , concerning the 
unbalanced repartition of the subjects across the experiments. Is it correct ?

Thanks for any comments or suggestions.

####
sessionInfo()  is:
R version 2.7.2

lme4_0.999375-26
Matrix_0.999375-13


Robert Espesser
Laboratoire Parole et Langage  UMR 6057, CNRS
29 Av. Robert Schuman  13621 AIX    (FRANCE)
Tel: +33 (0)4 42 95 36 26   Fax: +33 (0)4 42 95 37 88
http://www.lpl-aix.fr



From bates at stat.wisc.edu  Wed Sep 10 15:37:56 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 10 Sep 2008 08:37:56 -0500
Subject: [R-sig-ME] ML or REML for LR tests
In-Reply-To: <060F9ABC-7CA1-4C36-A6F1-F3BCA9B7BDDD@anu.edu.au>
References: <a46630750808281107u582a779fvec72e695356d1713@mail.gmail.com>
	<ED7B522EE00C9A4FA515AA71724D61EE0128C8DE@DC1EXCL01.air.org>
	<m0k5e0h1go.fsf@gmail.com>
	<060F9ABC-7CA1-4C36-A6F1-F3BCA9B7BDDD@anu.edu.au>
Message-ID: <40e66e0b0809100637q3e7ed2b7ka8b0d7c2b3313a16@mail.gmail.com>

On Wed, Sep 10, 2008 at 1:49 AM, John Maindonald
<john.maindonald at anu.edu.au> wrote:
> Douglas -
> I am taking up your reply to the question that Austin
> Frank asked on August 29.  I'd like to check that my
> understanding of the reply is correct.

> Looking at page 59 of Pinheiro & Bates.  \theta is a
> parameterization of the ratio of the inverse of the variance-
> covariance matrix of the random effects, multiplied by \sigma^2,
> e.g., in a model with 3 levels of random effects,
> \theta = (\sigma/\sigma_1, \sigma/\sigma_2) might do the job.

More or less.  In lme4 the parameterization is the inverse of what you
have written.  That is, individual components of \theta are of the
form \sigma_1/\sigma. This change was made because we want to allow
for \sigma_1 to be zero whereas it is not important for \sigma to be
allowed to be zero.

> Both the profiled deviance and the profiled REML criterion
> are evaluated at each step of the optimization, albeit
> at the current REML estimate of \theta.  (The ML deviance
> can be determined without explicitly calculating the ML
> estimates of the fixed and random effects.)

I'm not sure what the part about "at the current REML estimate of
\theta" means.  For a given value of \theta the profiled deviance and
the REML criterion can both be evaluated.  The REML estimate of \theta
optimizes the profiled REML criterion.  The ML estimate of \theta
optimizes the profiled deviance.  The value of the profiled deviance
at the REML estimate of \theta will be greater than the profiled
deviance at the ML estimate but usually the difference in these
deviances is very small.

> It makes little difference whether one evaluates the
> likelihood deviance at the REML estimate of \theta or at
> the ML estimate.  More importantly for the anova table,
> the difference in deviances is not much altered.

Exactly.

> Thanks in anticipation

Thanks for the clarification.

> The reply was:
>
> (From Doug Bates - 29 August 2008)
>
> It may help to give a bit of background about what happens in the
> anova method for mer objects in the lme4 package.
>
> When fitting a linear mixed model, both the profiled deviance
> (negative twice the log-likelihood) and the profiled REML criterion
> are evaluated at each iteration during the optimization.  The word
> "profiled" means that, although the likelihood or the REML criterion
> are defined as functions of the fixed-effects parameters,
> $\boldmath{\beta}$, the common scale parameter, $\sigma$, and the
> parameters $\boldmath{\theta}$ that determine the relative covariance
> matrix $\boldmath{ \Sigma}$ of the random effects, these criteria are
> evaluated as a function of $\boldmath{\theta}$ alone, with
> $\boldmath{\beta}$ and $\sigma$ at their conditionally optimal values.
>
> The profiled REML criterion has a term that depends on the model
> matrix X for the fixed-effects parameters.  If you change the
> definition of the fixed-effects you will change the value of that
> criterion in a systematic way that does not depend on how well the
> respective models fit the observed data.  Thus, differences in the
> REML criterion are not a meaningful way to compare two models that
> differ in the definition of the fixed-effects.
>
> If you look at the code for the anova method for the mer class you
> will see that it always calls logLik with REML = FALSE.  That is, it
> extracts the log-likelihood from the profiled deviance even if the
> model has been fit according to the REML criterion.  In general that
> would be a very bad idea if we were using the deviance and not the
> profiled deviance.  You can't count on the deviance at the REML
> estimates for the complete parameter vector being close to the optimal
> deviance.  However, the profiled deviance at the REML estimates of
> $\boldmath{\theta}$ is close to the optimal profiled deviance.
>
> For example,
>
> fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
> ## profiled deviance at the REML estimate of $\theta$
> dput(d1 <- deviance(fm1, REML = FALSE))
> structure(1751.98581110077, .Names = "ML")
> ## optimal value of the deviance
> dput(d2 <- deviance(update(fm1, REML = FALSE)))
> structure(1751.93934480597, .Names = "ML")
> ## amount by which the profiled deviance at REML estimate
> ## exceeds the optimal value
> d1 - d2
>    ML
> 0.04646629
>
> As you can see, I have gravitated to using the term "REML criterion"
> rather than calling it a deviance or a log-likelihood.  The print
> method for mer objects still labels it as the REMLdeviance but I plan
> to change that and create an additional extractor called REML to
> return that value.  In that case the deviance extractor will always
> return the deviance, perhaps after re-optimizing to produce the
> optimal deviance.
>
> So the bottom line is that the anova method for mer objects always
> produces a likelihood ratio test based on the likelihood.  If you
> compare models fit by REML that likelihood ratio statistic will be
> somewhat inaccurate but only by a small amount (in all the cases that
> I have seen).
>



From bates at stat.wisc.edu  Wed Sep 10 15:43:38 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 10 Sep 2008 08:43:38 -0500
Subject: [R-sig-ME] warnings ranef and design of a binomial model
In-Reply-To: <18177911.27541221042570069.JavaMail.root@frontal1>
References: <18177911.27541221042570069.JavaMail.root@frontal1>
Message-ID: <40e66e0b0809100643q74492740j2a44db0b87d6b6df@mail.gmail.com>

I think we would need at least the output from traceback() to see
where the error is occurring.  Better yet would be to have access to
the data so we could try out the model fits.  Sometimes these errors
show up deep inside another computation.  Does

ranef(mod.glmer)

produce a result?  I'm trying to decide if the problem is in the
postVar part or in the ranef part.

On Wed, Sep 10, 2008 at 5:29 AM, Robert ESPESSER
<Robert.Espesser at lpl-aix.fr> wrote:
> Dear R users,
>
> I have a question about model design.
> One group of 20 subjects run the A experiment, with 17 binary responses by subject.
> An other group of 20 subjects run the 2 experiments B and C, with 20 binary responses by subject,
> for both of the 2 experiment.
>
> I want to study the effect of the  factor "expe"
> I tried this covariance model:
> mod.glmer <-  glmer( vd ~ dlms*expe +(1|subject) +(0+dlms|subject) ,
>              family=binomial, ....)
>
> where:
> dlms is a numeric factor (-2,-1,0, 1,2)
> expe is a 3 levels factor , coding the experiment (A or B or C)
>
> vd is the success/fail matrix.
>
> The model with a random slope is clearly better than an intercept-only model.
>
> the results look plausible, ranef(mod.glmer, postVar=T) was OK for the
> Intercept, but failed
> to compute the postvar of the random slope , with these warnings:
> 1: In min(x): no argument found for min; Inf is returned
> It's the same for max.
>
> I'm  suspecting  that my model is perhaps badly designed , concerning the
> unbalanced repartition of the subjects across the experiments. Is it correct ?
>
> Thanks for any comments or suggestions.
>
> ####
> sessionInfo()  is:
> R version 2.7.2
>
> lme4_0.999375-26
> Matrix_0.999375-13
>
>
> Robert Espesser
> Laboratoire Parole et Langage  UMR 6057, CNRS
> 29 Av. Robert Schuman  13621 AIX    (FRANCE)
> Tel: +33 (0)4 42 95 36 26   Fax: +33 (0)4 42 95 37 88
> http://www.lpl-aix.fr
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From matejus106 at googlemail.com  Wed Sep 10 16:31:29 2008
From: matejus106 at googlemail.com (jos matejus)
Date: Wed, 10 Sep 2008 15:31:29 +0100
Subject: [R-sig-ME] HPDinterval function with Rails example (P&B)
Message-ID: <d003a00f0809100731g2c6d2b95v37da79e450911dfe@mail.gmail.com>

Dear lmers

I am trying to get to grips using the functions mcmcsamp() and
HPDintervals() to generate HPD confidence intervals for my parameter
estimates for some simple mixed effects models. In order to gently
introduce myself to this subject I have been working through examples
given in Pinheiro and Bates- 'mixed-effects models in S and S-Plus
using lme4. I have a couple of questions that I have been unable to
find the answers to.

The example I am following at the moment is using the Rails dataset
(page 9, P&B). I have fitted the following model:

> library(lme4)
> data(Rail, package="nlme")
> fmRail.lme <- lmer(travel~1+(1|Rail), data=Rail)
> summary(fmRail.lme)
Linear mixed model fit by REML

Formula: travel ~ 1 + (1 | Rail)
   Data: Rail
   AIC   BIC logLik deviance REMLdev
 128.2 130.8 -61.09    128.6   122.2
Random effects:
 Groups   Name        Variance Std.Dev.
 Rail     (Intercept) 615.266  24.8046
 Residual              16.167   4.0208
Number of obs: 18, groups: Rail, 6

Fixed effects:
            Estimate Std. Error t value
(Intercept)    66.50      10.17   6.538

In order to obtain HPD intervals for the estimated parameters

> Railmcmc <- mcmcsamp(fmRail.lme, n=10000, saveb=TRUE)
> HPDinterval(Railmcmc)
$fixef
               lower    upper
(Intercept) 54.49488 78.99753
attr(,"Probability")
[1] 0.95

$ST
         lower    upper
[1,] 0.2582595 1.352534
attr(,"Probability")
[1] 0.95

$sigma
        lower    upper
[1,] 8.503474 23.07903
attr(,"Probability")
[1] 0.95

$ranef
           lower       upper
[1,] -38.8745199  0.09389242
[2,] -26.8906507  5.99597084
[3,] -24.6996428  7.63476719
[4,]  -5.3189656 27.55519181
[5,]  -4.7411818 28.25215767
[6,]  -0.3872951 35.66205005
attr(,"Probability")
[1] 0.95

  I understand the HPD intervals for the fixed effects and for sigma
(although they are quite a bit different than those obtained using the
intervals() function in nlme), however I am unclear as to what the
intervals in the  ST represent. Also, is there a way of calculating
(or accessing)  the HPD interval for the random effect associated with
rail (sigma b ) as I cant seem to see it (I remember you got it using
the function HPDinterval in the coda package). Apologies in advance if
this is a rather trivial question, but its just got me stumped and I
would really like to get to grips with this.

Best wishes
Jos



From kushler at oakland.edu  Wed Sep 10 17:04:18 2008
From: kushler at oakland.edu (Robert Kushler)
Date: Wed, 10 Sep 2008 11:04:18 -0400
Subject: [R-sig-ME] HPDinterval function with Rails example (P&B)
In-Reply-To: <d003a00f0809100731g2c6d2b95v37da79e450911dfe@mail.gmail.com>
References: <d003a00f0809100731g2c6d2b95v37da79e450911dfe@mail.gmail.com>
Message-ID: <48C7E1F2.2050106@oakland.edu>


"ST" is the ratio of "sigma-Rail" to "sigma".

Try the code below.  This approach handles simple models correctly,
but not more complex ones.  I'm sure I'll be scolded for accessing
the slots directly (and perhaps other infelicities), but others may
provide improved code.

Regards,   Rob Kushler


# code from original message:
library(lme4)
data(Rail, package="nlme")
fmRail.lme <- lmer(travel~1+(1|Rail), data=Rail)
summary(fmRail.lme)
Railmcmc <- mcmcsamp(fmRail.lme, n=10000, saveb=TRUE)
HPDinterval(Railmcmc)


# new stuff:
plot(t(Railmcmc at ST))  # inspect the chain (could be "stuck at zero")
sigmaRail <- as.vector(Railmcmc at ST*Railmcmc at sigma)
hist(sigmaRail)
plot(density(sigmaRail,adjust=2),main='Posterior distribution of "sigma-Rail"')
sort(sigmaRail)[c(50,950)]  # frequentist "equal tails" CI

# ad hoc HPD interval
hpdint <- function(x,prob=0.95) {
  xx <- sort(x)
  nn <- length(xx)
  len <- function(a)  xx[floor((a+prob)*nn)] - xx[floor(a*nn)+1]
  tail <- optim((1-prob)/2,len,method="L-BFGS-B",lower=0,upper=1-prob)$par
  c(xx[floor(tail*nn)+1],xx[floor((tail+prob)*nn)])
}
hpdint(sigmaRail,0.9)





jos matejus wrote:
> Dear lmers
> 
> I am trying to get to grips using the functions mcmcsamp() and
> HPDintervals() to generate HPD confidence intervals for my parameter
> estimates for some simple mixed effects models. In order to gently
> introduce myself to this subject I have been working through examples
> given in Pinheiro and Bates- 'mixed-effects models in S and S-Plus
> using lme4. I have a couple of questions that I have been unable to
> find the answers to.
> 
> The example I am following at the moment is using the Rails dataset
> (page 9, P&B). I have fitted the following model:
> 
>> library(lme4)
>> data(Rail, package="nlme")
>> fmRail.lme <- lmer(travel~1+(1|Rail), data=Rail)
>> summary(fmRail.lme)
> Linear mixed model fit by REML
> 
> Formula: travel ~ 1 + (1 | Rail)
>    Data: Rail
>    AIC   BIC logLik deviance REMLdev
>  128.2 130.8 -61.09    128.6   122.2
> Random effects:
>  Groups   Name        Variance Std.Dev.
>  Rail     (Intercept) 615.266  24.8046
>  Residual              16.167   4.0208
> Number of obs: 18, groups: Rail, 6
> 
> Fixed effects:
>             Estimate Std. Error t value
> (Intercept)    66.50      10.17   6.538
> 
> In order to obtain HPD intervals for the estimated parameters
> 
>> Railmcmc <- mcmcsamp(fmRail.lme, n=10000, saveb=TRUE)
>> HPDinterval(Railmcmc)
> $fixef
>                lower    upper
> (Intercept) 54.49488 78.99753
> attr(,"Probability")
> [1] 0.95
> 
> $ST
>          lower    upper
> [1,] 0.2582595 1.352534
> attr(,"Probability")
> [1] 0.95
> 
> $sigma
>         lower    upper
> [1,] 8.503474 23.07903
> attr(,"Probability")
> [1] 0.95
> 
> $ranef
>            lower       upper
> [1,] -38.8745199  0.09389242
> [2,] -26.8906507  5.99597084
> [3,] -24.6996428  7.63476719
> [4,]  -5.3189656 27.55519181
> [5,]  -4.7411818 28.25215767
> [6,]  -0.3872951 35.66205005
> attr(,"Probability")
> [1] 0.95
> 
>   I understand the HPD intervals for the fixed effects and for sigma
> (although they are quite a bit different than those obtained using the
> intervals() function in nlme), however I am unclear as to what the
> intervals in the  ST represent. Also, is there a way of calculating
> (or accessing)  the HPD interval for the random effect associated with
> rail (sigma b ) as I cant seem to see it (I remember you got it using
> the function HPDinterval in the coda package). Apologies in advance if
> this is a rather trivial question, but its just got me stumped and I
> would really like to get to grips with this.
> 
> Best wishes
> Jos
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From Robert.Espesser at lpl-aix.fr  Wed Sep 10 19:18:47 2008
From: Robert.Espesser at lpl-aix.fr (Robert ESPESSER)
Date: Wed, 10 Sep 2008 19:18:47 +0200 (CEST)
Subject: [R-sig-ME] warnings ranef and design of a binomial model
In-Reply-To: <40e66e0b0809100643q74492740j2a44db0b87d6b6df@mail.gmail.com>
References: <18177911.27541221042570069.JavaMail.root@frontal1>
	<40e66e0b0809100643q74492740j2a44db0b87d6b6df@mail.gmail.com>
Message-ID: <14867066.27091221067129458.JavaMail.root@frontal2>


> ----------------------------------------
> From: Douglas Bates <bates at stat.wisc.edu>
> Sent: Wed Sep 10 15:43:38 CEST 2008
> To: Robert ESPESSER <Robert.Espesser at lpl-aix.fr>
> Subject: Re: [R-sig-ME] warnings ranef and design of a binomial model

>I think we would need at least the output from traceback() to see
>where the error is occurring.  Better yet would be to have access to
>the data so we could try out the model fits.  Sometimes these errors
>show up deep inside another computation.  Does

>ranef(mod.glmer)

>produce a result?  I'm trying to decide if the problem is in the
>postVar part or in the ranef part.


The warning messages are output by dotplot(ranef() ).
ranef(xxx ,postVar=T) by  itself does not output any warning messages. 

The models:
modG.glmer <- glmer( vd ~ dlms*expe  +(dlms|subject) ,family=binomial, ....)
modI.glmer <- glmer( vd ~ dlms*expe  +(1|subject) ,family=binomial, ....)
modS.glmer <- glmer( vd ~ dlms*expe  +(0+dlms|subject) ,family=binomial, ....)

have their dotplot(ranef()) OK. 

The problem is only with the model:
modIS.glmer <-  glmer( vd ~ dlms*expe +(1|subject) +(0+dlms|subject) ,family=binomial, ....)

ranef(modIS.glmer)-> rr.glmer
>  dotplot( rr.glmer)
$subject

Warning messages:
1: In min(x) : aucun argument trouv? pour min ; Inf est renvoy?
2: In max(x) : aucun argument pour max ; -Inf est renvoy?

traceback() returns nothing :
> traceback()
2: bar(2)
1: foo(2)
(because I have tested it with the example in ?traceback)

In addition, how can I extract the postVar component in a ranef object ?
Dr Bates, in case you need the data , do I have attach them in a private mail to you ?

here is the complete sessionInfo():
R version 2.7.2 (2008-08-25) 
i386-pc-mingw32 

locale:
LC_COLLATE=French_France.1252;LC_CTYPE=French_France.1252;LC_MONETARY=French_France.1252;LC_NUMERIC=C;LC_TIME=French_France.1252

attached base packages:
[1] stats     graphics  grDevices datasets  utils     methods   base     

other attached packages:
[1] lme4_0.999375-26   Matrix_0.999375-13 lattice_0.17-13   

loaded via a namespace (and not attached):
[1] grid_2.7.2  nlme_3.1-89





> 
> On Wed, Sep 10, 2008 at 5:29 AM, Robert ESPESSER
> <Robert.Espesser at lpl-aix.fr> wrote:
> > Dear R users,
> >
> > I have a question about model design.
> > One group of 20 subjects run the A experiment, with 17 binary responses by subject.
> > An other group of 20 subjects run the 2 experiments B and C, with 20 binary responses by subject,
> > for both of the 2 experiment.
> >
> > I want to study the effect of the  factor "expe"
> > I tried this covariance model:
> > mod.glmer <-  glmer( vd ~ dlms*expe +(1|subject) +(0+dlms|subject) ,
> >              family=binomial, ....)
> >
> > where:
> > dlms is a numeric factor (-2,-1,0, 1,2)
> > expe is a 3 levels factor , coding the experiment (A or B or C)
> >
> > vd is the success/fail matrix.
> >
> > The model with a random slope is clearly better than an intercept-only model.
> >
> > the results look plausible, ranef(mod.glmer, postVar=T) was OK for the
> > Intercept, but failed
> > to compute the postvar of the random slope , with these warnings:
> > 1: In min(x): no argument found for min; Inf is returned
> > It's the same for max.
> >
> > I'm  suspecting  that my model is perhaps badly designed , concerning the
> > unbalanced repartition of the subjects across the experiments. Is it correct ?
> >
> > Thanks for any comments or suggestions.
> >
> > ####
> > sessionInfo()  is:
> > R version 2.7.2
> >
> > lme4_0.999375-26
> > Matrix_0.999375-13
> >
> >
> > Robert Espesser
> > Laboratoire Parole et Langage  UMR 6057, CNRS
> > 29 Av. Robert Schuman  13621 AIX    (FRANCE)
> > Tel: +33 (0)4 42 95 36 26   Fax: +33 (0)4 42 95 37 88
> > http://www.lpl-aix.fr
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From austin.frank at gmail.com  Wed Sep 10 21:01:25 2008
From: austin.frank at gmail.com (Austin Frank)
Date: Wed, 10 Sep 2008 15:01:25 -0400
Subject: [R-sig-ME] ML or REML for LR tests
References: <a46630750808281107u582a779fvec72e695356d1713@mail.gmail.com>
	<ED7B522EE00C9A4FA515AA71724D61EE0128C8DE@DC1EXCL01.air.org>
	<m0k5e0h1go.fsf@gmail.com>
	<008ED4AA-D47D-444E-B7E5-AC88DFBE4A19@kjbeath.com.au>
Message-ID: <m0zlmfakqi.fsf@urwireless-dhcp-128-151-20-16.wireless.rochester.edu>

First off, thanks to all who have responded to the series of questions I
asked!

On Fri, Aug 29 2008, Ken Beath wrote:

> On 29/08/2008, at 2:47 PM, Austin Frank wrote:
>
>> 3) Is it the case that LR tests between REML models with different
>> random effects are meaningful?  Does this apply to both nested and
>> non-nested models?
>>
>
> Maybe, but only for nested (see Q2). Supposedly it works better than
> ML. The significance tests wont be correct but if there is a huge
> significance level then there is probably a random effect. Simulation
> seems a better idea.

Ken was the only one to address this particular point, and I want to
make sure I've got it straight.  Are REML-based likelihood-ratio tests
(presumably not performed with anova.mer, as that sets REML=FALSE on the
call to logLik) an acceptable method for testing nested models with
different random effects specifications?

As a point of reference, the anova() method is called on two lmer models
that differ only in their random effects in the manuscript by Baayen,
Davidson, and Bates at
http://www.ualberta.ca/~baayen/publications/baayenDavidsonBates.pdf (pp
12-15).  The discussion of that analysis makes no mention of the
difference between REML and ML fits.  Is this because, as discussed
recently, the REML and ML estimates are so close that there is no
practical difference in which quantity is used for this test?


Thanks again!
/au

-- 
Austin Frank
http://aufrank.net
GPG Public Key (D7398C2F): http://aufrank.net/personal.asc



From adik-rhelp at ilovebacon.org  Thu Sep 11 02:22:47 2008
From: adik-rhelp at ilovebacon.org (Adam D. I. Kramer)
Date: Wed, 10 Sep 2008 17:22:47 -0700 (PDT)
Subject: [R-sig-ME] Speed estimation for lmer?
Message-ID: <Pine.LNX.4.64.0809101710270.10089@parser.ilovebacon.org>

Hi,

 	I'm about to estimate what I expect to be a fairly involved model,
like this one:

l <- lmer(y ~ x1*x2*x3 + (x1*x2*x3|grp) )

...the data set has 3,232,255 rows, for about 18000 grps, each of which has around
700 observations; x1, x2, x3 are continuous variables.

Is there any way I can estimate how long this run will take? Obviously this
depends on things like memory, processor, etc....but perhaps I could run it
on 5 groups and then multiply the amount of time it takes, or something like
that?

Also, given this information, is there some faster way to run the model? In
theory, I'd be interested in systematically checking which random effects I
could drop, but not if it would take weeks. Some prior posts to this list
(which I have only been actively reading since yesterday) suggest that lmer
is likely faster than lmer2, but there doesn't seem to be much discussion on
the speed of various modeling functions (lme, lmer, lmer2, glmer, glmmPLQ,
etc.).

Thanks in advance,
Adam Kramer



From F.DUYME at arvalisinstitutduvegetal.fr  Thu Sep 11 09:21:04 2008
From: F.DUYME at arvalisinstitutduvegetal.fr (DUYME Florent)
Date: Thu, 11 Sep 2008 09:21:04 +0200
Subject: [R-sig-ME] corr structure AR(1) and  lmer function
Message-ID: <CD2B1094373B8346885C908D1B841D3D04F635ED@srv-exch1.arvalis-fr.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080911/4f196a90/attachment.pl>

From bolker at ufl.edu  Thu Sep 11 15:49:24 2008
From: bolker at ufl.edu (Ben Bolker)
Date: Thu, 11 Sep 2008 09:49:24 -0400
Subject: [R-sig-ME] corr structure AR(1) and  lmer function
In-Reply-To: <CD2B1094373B8346885C908D1B841D3D04F635ED@srv-exch1.arvalis-fr.com>
References: <CD2B1094373B8346885C908D1B841D3D04F635ED@srv-exch1.arvalis-fr.com>
Message-ID: <48C921E4.8010606@ufl.edu>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

DUYME Florent wrote:
> Dear R users
> 
>  
> 
> I use the following design : 3 complete blocks, in each of them several
> varieties are randomly planted.
> 
> I'd like to model a gradient along each block
> 
>  
> 
> To do that, lme function is quite useful, with cor structure :
> correlation=corAR1()
> 
> The model is a crossed random effect spatial model.
> 
>  
> 
> To avoid using lme with such a model, I'd like to use lmer function. 
> 
>  
> 
> My question is : how do you model the corr structure (ie AR(1)) in the
> lmer function ?
> 
>  

  This is not currently possible with lmer, and Doug Bates has said
that it's not near the top of his "to do" list ...

  Ben Bolker
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.6 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFIySHkc5UpGjwzenMRAtSyAJ9/EemZufGn9Jw1KGoalY5NQfaN5wCcDWjI
H1qA4ypeEg2piDFQWT67EtI=
=0ux3
-----END PGP SIGNATURE-----



From bolker at ufl.edu  Thu Sep 11 17:09:19 2008
From: bolker at ufl.edu (Ben Bolker)
Date: Thu, 11 Sep 2008 11:09:19 -0400
Subject: [R-sig-ME] Speed estimation for lmer?
In-Reply-To: <Pine.LNX.4.64.0809101710270.10089@parser.ilovebacon.org>
References: <Pine.LNX.4.64.0809101710270.10089@parser.ilovebacon.org>
Message-ID: <48C9349F.9040200@ufl.edu>

Adam D. I. Kramer wrote:
> Hi,
> 
>     I'm about to estimate what I expect to be a fairly involved model,
> like this one:
> 
> l <- lmer(y ~ x1*x2*x3 + (x1*x2*x3|grp) )
> 
> ...the data set has 3,232,255 rows, for about 18000 grps, each of which
> has around
> 700 observations; x1, x2, x3 are continuous variables.
> 
> Is there any way I can estimate how long this run will take? Obviously this
> depends on things like memory, processor, etc....but perhaps I could run it
> on 5 groups and then multiply the amount of time it takes, or something
> like
> that?

  I don't know exactly how it will scale, but I would guess offhand that
it wouldn't be much worse than linear in number of points and in number
of groups (???).  I would suggest you try it for 5, 10, 20, and 100
groups and extrapolate from there ...  it does seem frighteningly big
to me.

> Also, given this information, is there some faster way to run the model? In
> theory, I'd be interested in systematically checking which random effects I
> could drop, but not if it would take weeks. Some prior posts to this list
> (which I have only been actively reading since yesterday) suggest that lmer
> is likely faster than lmer2, but there doesn't seem to be much
> discussion on
> the speed of various modeling functions (lme, lmer, lmer2, glmer, glmmPLQ,
> etc.).

  Roughly speaking: lmer and lmer2 aren't (I think) different any more,
they were different branches of the same software.  They should both
be much faster than lme.  glmer (from lme4) and glmmPQL (from nlme)
should not be necessary unless you have binomial, Poisson, etc. data
rather than normally distributed responses.

  good luck

    Ben Bolker



From farewelld at Cardiff.ac.uk  Thu Sep 11 15:03:18 2008
From: farewelld at Cardiff.ac.uk (Daniel Farewell)
Date: Thu, 11 Sep 2008 14:03:18 +0100
Subject: [R-sig-ME] models with no fixed effects
Message-ID: <48C92526020000B600039CA4@ZGRW38.uwcm.ac.uk>

I'm running into an error when using lmer to fit models with no fixed effects terms.

For example, generating some data with

df$y <- with(df <- data.frame(i = gl(5, 5), b = rep(rnorm(5), each = 5)), b + rnorm(25))

and fitting like this

fit1 <- lmer(y ~ 1 + (1 | i), df)

works fine. But fitting like this

fit0 <- lmer(y ~ 0 + (1 | i), df)

gives the following error:

CHOLMOD error: Pl?
Error in mer_finalize(ans) : 
  Cholmod error `invalid xtype' at file:../Cholesky/cholmod_solve.c, line 971

Am I missing something obvious?

Many thanks,

Daniel

Here's my sessionInfo(), in case it's useful:

R version 2.7.1 (2008-06-23) 
i386-apple-darwin8.10.1 

locale:
en_GB.UTF-8/en_GB.UTF-8/C/C/en_GB.UTF-8/en_GB.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] lme4_0.999375-26   Matrix_0.999375-15 lattice_0.17-8    

loaded via a namespace (and not attached):
[1] grid_2.7.1



From bates at stat.wisc.edu  Thu Sep 11 21:04:21 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 11 Sep 2008 14:04:21 -0500
Subject: [R-sig-ME] Speed estimation for lmer?
In-Reply-To: <48C9349F.9040200@ufl.edu>
References: <Pine.LNX.4.64.0809101710270.10089@parser.ilovebacon.org>
	<48C9349F.9040200@ufl.edu>
Message-ID: <40e66e0b0809111204u41e62785wd97c9216b1cfd05c@mail.gmail.com>

On Thu, Sep 11, 2008 at 10:09 AM, Ben Bolker <bolker at ufl.edu> wrote:
> Adam D. I. Kramer wrote:
>> Hi,
>>
>>     I'm about to estimate what I expect to be a fairly involved model,
>> like this one:
>>
>> l <- lmer(y ~ x1*x2*x3 + (x1*x2*x3|grp) )
>>
>> ...the data set has 3,232,255 rows, for about 18000 grps, each of which
>> has around 700 observations;

Are you sure?

> 3232255/18000
[1] 179.5697

>>  x1, x2, x3 are continuous variables.
>>
>> Is there any way I can estimate how long this run will take? Obviously this
>> depends on things like memory, processor, etc....but perhaps I could run it
>> on 5 groups and then multiply the amount of time it takes, or something
>> like
>> that?

I would start with a sample of groups and all observations for each of
the groups in the sample.  Something like 1000 groups should not take
a huge amount of time (easily under an hour) if you simplify the
random effects specification.  I would start with the

gsamp <- with(mydata, sample(levels(grp), 1000))
datasamp <- subset(mydata, grp %in% gsamp)
system.time(fm1 <- lmer(y ~ x1 * x2 * x3 + (1|grp), datasamp, verbose = TRUE))
system.time(fm2 <- lmer(y ~ x1 * x2 * x3 + (x1 + x2 + x3|grp),
datasamp, verbose = TRUE))
system.time(fm3 <- lmer(y ~ x1 * x2 * x3 + (x1 * x2 * x3|grp),
datasamp, verbose = TRUE))

The reason that I suggest using verbose = TRUE is because you will be
able to see the progress of the iterations and to get a feeling for
the amount of time per iteration.  Having a large number of
observations and of groups will increase the amount of time per
iteration but my guess is that it would be more-or-less linear in the
number of observations.  I wouldn't expect that it would be worse than
quadratic in the number of observations.

However, increasing the number of random effects per group could cost
you substantially because that increases the number of parameters to
be estimated as part of the nonlinear optimization.  The iteration
output for fm1 will show one parameter being optimized.  Many other
parameters are involved in the model but their estimates are
calculated directly, given the one parameter which happens to be the
ratio of the standard deviation of the random effects to the standard
deviation of the residuals.  The iteration output for fm2 will show 10
parameters in the nonlinear optimization, corresponding to four
variances and 6 covariances of the random effects.  I'm not sure
exactly how many parameters will be in the nonlinear optimization for
fm3 but it will be a large number.  I would certainly do some
graphical exploration before I embarked on trying to fit a model of
that complexity.

I would be interested in what your experiences are.
>  I don't know exactly how it will scale, but I would guess offhand that
> it wouldn't be much worse than linear in number of points and in number
> of groups (???).  I would suggest you try it for 5, 10, 20, and 100
> groups and extrapolate from there ...  it does seem frighteningly big
> to me.
>
>> Also, given this information, is there some faster way to run the model? In
>> theory, I'd be interested in systematically checking which random effects I
>> could drop, but not if it would take weeks. Some prior posts to this list
>> (which I have only been actively reading since yesterday) suggest that lmer
>> is likely faster than lmer2, but there doesn't seem to be much
>> discussion on
>> the speed of various modeling functions (lme, lmer, lmer2, glmer, glmmPLQ,
>> etc.).
>
>  Roughly speaking: lmer and lmer2 aren't (I think) different any more,
> they were different branches of the same software.  They should both
> be much faster than lme.  glmer (from lme4) and glmmPQL (from nlme)
> should not be necessary unless you have binomial, Poisson, etc. data
> rather than normally distributed responses.

Ben is correct.  There are functions lmer and lmer2 in the current
lme4 package but lmer2 is just a stub that turns around and calls
lmer.  If your response y is on a continuous scale then it should be
lmer that you use.

It will help to have a machine with a lot of memory so you don't end
up swapping.  I would recommend using a machine running 64-bit Linux
(we don't have a 64-bit Windows version of R because we are waiting on
freely available compilers).  You may also want to try with an
accelerated BLAS.  However, I would check both with and without
accelerated BLAS if you have a multi-core processor.  Sometimes a
multithreaded accelerated BLAS can make lmer run slower, not faster.
This is because the calls to the BLAS are predominantly for small
matrices and the communications overhead for multithreaded versions
more than offsets the performance gain from using multiple cores.



From bates at stat.wisc.edu  Thu Sep 11 21:15:03 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 11 Sep 2008 14:15:03 -0500
Subject: [R-sig-ME] models with no fixed effects
In-Reply-To: <48C92526020000B600039CA4@ZGRW38.uwcm.ac.uk>
References: <48C92526020000B600039CA4@ZGRW38.uwcm.ac.uk>
Message-ID: <40e66e0b0809111215t63e59548yb0d772dd59b0dd41@mail.gmail.com>

On Thu, Sep 11, 2008 at 8:03 AM, Daniel Farewell
<farewelld at cardiff.ac.uk> wrote:
> I'm running into an error when using lmer to fit models with no fixed effects terms.
>
> For example, generating some data with
>
> df$y <- with(df <- data.frame(i = gl(5, 5), b = rep(rnorm(5), each = 5)), b + rnorm(25))
>
> and fitting like this
>
> fit1 <- lmer(y ~ 1 + (1 | i), df)
>
> works fine. But fitting like this
>
> fit0 <- lmer(y ~ 0 + (1 | i), df)
>
> gives the following error:
>
> CHOLMOD error: Pl?
> Error in mer_finalize(ans) :
>  Cholmod error `invalid xtype' at file:../Cholesky/cholmod_solve.c, line 971

Admittedly that is a rather obscure error message.  It is related to
the fact, apparently not verified, that we should have p, the number
of fixed-effects, greater than zero.

I should definitely add a check on p to the validate method.  (In some
ways I'm surprised that it got as far as mer_finalize before kicking
an error).  I suppose that p = 0 could be allowed and I could add some
conditional code in the appropriate places but does it really make
sense to have p = 0?  The random effects are defined to have mean
zero.  If you have p = 0 that means that E[Y] = 0.  I would have
difficulty imagining when I would want to make that restriction.

Let me make this offer - if someone could suggest circumstances in
which such a model would make sense, I will add the appropriate
conditional code to allow for p = 0. For the time being I will just
add a requirement of  p >  0 to the validate method.



From peter.dixon at ualberta.ca  Thu Sep 11 22:09:46 2008
From: peter.dixon at ualberta.ca (Peter Dixon)
Date: Thu, 11 Sep 2008 14:09:46 -0600
Subject: [R-sig-ME] models with no fixed effects
In-Reply-To: <40e66e0b0809111215t63e59548yb0d772dd59b0dd41@mail.gmail.com>
References: <48C92526020000B600039CA4@ZGRW38.uwcm.ac.uk>
	<40e66e0b0809111215t63e59548yb0d772dd59b0dd41@mail.gmail.com>
Message-ID: <A3C0D575-253B-4E9B-8015-347CFD394F42@ualberta.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080911/b8cbd599/attachment.pl>

From a.fugard at ed.ac.uk  Thu Sep 11 22:15:21 2008
From: a.fugard at ed.ac.uk (Andy Fugard)
Date: Thu, 11 Sep 2008 21:15:21 +0100
Subject: [R-sig-ME] models with no fixed effects
In-Reply-To: <A3C0D575-253B-4E9B-8015-347CFD394F42@ualberta.ca>
References: <48C92526020000B600039CA4@ZGRW38.uwcm.ac.uk>	<40e66e0b0809111215t63e59548yb0d772dd59b0dd41@mail.gmail.com>
	<A3C0D575-253B-4E9B-8015-347CFD394F42@ualberta.ca>
Message-ID: <48C97C59.6050103@ed.ac.uk>

Peter Dixon wrote:
> On Sep 11, 2008, at 1:15 PM, Douglas Bates wrote:
> 
>> I should definitely add a check on p to the validate method.  (In some
>> ways I'm surprised that it got as far as mer_finalize before kicking
>> an error).  I suppose that p = 0 could be allowed and I could add some
>> conditional code in the appropriate places but does it really make
>> sense to have p = 0?  The random effects are defined to have mean
>> zero.  If you have p = 0 that means that E[Y] = 0.  I would have
>> difficulty imagining when I would want to make that restriction.
>>
>> Let me make this offer - if someone could suggest circumstances in
>> which such a model would make sense, I will add the appropriate
>> conditional code to allow for p = 0. For the time being I will just
>> add a requirement of  p >  0 to the validate method.
> 
> I think it would make sense to consider a model in which E[Y] = 0 when  
> the data are (either explicitly or implicitly) difference scores. (In  
> fact, I tried to fit such a model with lmer a few months ago and ran  
> into exactly this problem.)

Wouldn't you still need the intercept?  The fixed effect tells you 
whether on average the difference differs from zero.  The random effect 
estimates tell you by how much each individual's difference differs from 
the mean difference.

A

-- 
Andy Fugard, Postgraduate Research Student
Psychology (Room S6), The University of Edinburgh,
   7 George Square, Edinburgh EH8 9JZ, UK
+44 (0)78 123 87190   http://figuraleffect.googlepages.com/

The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.



From ajmackey at gmail.com  Thu Sep 11 22:17:57 2008
From: ajmackey at gmail.com (Aaron Mackey)
Date: Thu, 11 Sep 2008 16:17:57 -0400
Subject: [R-sig-ME] models with no fixed effects
In-Reply-To: <A3C0D575-253B-4E9B-8015-347CFD394F42@ualberta.ca>
References: <48C92526020000B600039CA4@ZGRW38.uwcm.ac.uk>
	<40e66e0b0809111215t63e59548yb0d772dd59b0dd41@mail.gmail.com>
	<A3C0D575-253B-4E9B-8015-347CFD394F42@ualberta.ca>
Message-ID: <24c96eca0809111317o75055031w2be12e253166f384@mail.gmail.com>

On Thu, Sep 11, 2008 at 4:09 PM, Peter Dixon <peter.dixon at ualberta.ca> wrote:

> I think it would make sense to consider a model in which E[Y] = 0 when
> the data are (either explicitly or implicitly) difference scores. (In
> fact, I tried to fit such a model with lmer a few months ago and ran
> into exactly this problem.)

I had the same initial thoughts, but for me, the intercept tells me
how much of the difference remains unexplained by the covariates.
Fixing that at 0 is the same as infinitely strong prior belief that
there are no other possible explanations for the observed differences.

-Aaron



From peter.dixon at ualberta.ca  Thu Sep 11 23:06:13 2008
From: peter.dixon at ualberta.ca (Peter Dixon)
Date: Thu, 11 Sep 2008 15:06:13 -0600
Subject: [R-sig-ME] models with no fixed effects
In-Reply-To: <48C97C59.6050103@ed.ac.uk>
References: <48C92526020000B600039CA4@ZGRW38.uwcm.ac.uk>	<40e66e0b0809111215t63e59548yb0d772dd59b0dd41@mail.gmail.com>
	<A3C0D575-253B-4E9B-8015-347CFD394F42@ualberta.ca>
	<48C97C59.6050103@ed.ac.uk>
Message-ID: <6BFC5E9F-1180-43C4-8914-E87638FA9003@ualberta.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080911/4e18f5c0/attachment.pl>

From danielezrajohnson at gmail.com  Thu Sep 11 23:06:49 2008
From: danielezrajohnson at gmail.com (Daniel Ezra Johnson)
Date: Thu, 11 Sep 2008 22:06:49 +0100
Subject: [R-sig-ME] Speed estimation for lmer?
In-Reply-To: <40e66e0b0809111204u41e62785wd97c9216b1cfd05c@mail.gmail.com>
References: <Pine.LNX.4.64.0809101710270.10089@parser.ilovebacon.org>
	<48C9349F.9040200@ufl.edu>
	<40e66e0b0809111204u41e62785wd97c9216b1cfd05c@mail.gmail.com>
Message-ID: <a46630750809111406o7a8b4b10i9b92e812697e5437@mail.gmail.com>

>>  Roughly speaking: lmer and lmer2 aren't (I think) different any more,
>> they were different branches of the same software.  They should both
>> be much faster than lme.  glmer (from lme4) and glmmPQL (from nlme)
>> should not be necessary unless you have binomial, Poisson, etc. data
>> rather than normally distributed responses.
>
> Ben is correct.  There are functions lmer and lmer2 in the current
> lme4 package but lmer2 is just a stub that turns around and calls
> lmer.  If your response y is on a continuous scale then it should be
> lmer that you use.

so if my response is on a continuous scale, is there any difference between

lmer(y~fixed+(1|random)) and
glmer(y~fixed+(1|random),family="gaussian") ?

the output is the same but since reading the above i'm wondering if
'plain' lmer is written to run faster?

d



From adik-rhelp at ilovebacon.org  Thu Sep 11 23:12:03 2008
From: adik-rhelp at ilovebacon.org (Adam D. I. Kramer)
Date: Thu, 11 Sep 2008 14:12:03 -0700 (PDT)
Subject: [R-sig-ME] Speed estimation for lmer?
In-Reply-To: <40e66e0b0809111204u41e62785wd97c9216b1cfd05c@mail.gmail.com>
References: <Pine.LNX.4.64.0809101710270.10089@parser.ilovebacon.org> 
	<48C9349F.9040200@ufl.edu>
	<40e66e0b0809111204u41e62785wd97c9216b1cfd05c@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0809111220150.10089@parser.ilovebacon.org>


On Thu, 11 Sep 2008, Douglas Bates wrote:

> On Thu, Sep 11, 2008 at 10:09 AM, Ben Bolker <bolker at ufl.edu> wrote:
>> Adam D. I. Kramer wrote:
>>> Hi,
>>>
>>>     I'm about to estimate what I expect to be a fairly involved model,
>>> like this one:
>>>
>>> l <- lmer(y ~ x1*x2*x3 + (x1*x2*x3|grp) )
>>>
>>> ...the data set has 3,232,255 rows, for about 18000 grps, each of which
>>> has around 700 observations;
>
> Are you sure?
>
>> 3232255/18000
> [1] 179.5697

Touche. By "around" I really meant "up to," and by 700 I meant 200 (I'm
doing some preliminary analysis before all of the cases are in). I apologize
for this mistake.

>>>  x1, x2, x3 are continuous variables.
>>>
>>> Is there any way I can estimate how long this run will take? Obviously
>>> this depends on things like memory, processor, etc....but perhaps I
>>> could run it on 5 groups and then multiply the amount of time it takes,
>>> or something like that?
>
> I would start with a sample of groups and all observations for each of the
> groups in the sample.  Something like 1000 groups should not take a huge
> amount of time (easily under an hour) if you simplify the random effects
> specification.  I would start with the
>
> gsamp <- with(mydata, sample(levels(grp), 1000))
> datasamp <- subset(mydata, grp %in% gsamp)
> system.time(fm1 <- lmer(y ~ x1 * x2 * x3 + (1|grp), datasamp, verbose = TRUE))
> system.time(fm2 <- lmer(y ~ x1 * x2 * x3 + (x1 + x2 + x3|grp),
> datasamp, verbose = TRUE))
> system.time(fm3 <- lmer(y ~ x1 * x2 * x3 + (x1 * x2 * x3|grp),
> datasamp, verbose = TRUE))
>
> The reason that I suggest using verbose = TRUE is because you will be able
> to see the progress of the iterations and to get a feeling for the amount
> of time per iteration.

This alone is a fantastic thing to know! It's almost like a progress bar.

And you were right about it not taking long. For fm2:
    user  system elapsed 
289.868  22.335 312.860 
...for the 1000 person subgroup on the additive random model.

...I then started estimating fm3, went to lunch, came back,
and it is still running.

> Having a large number of observations and of groups will increase the
> amount of time per iteration but my guess is that it would be more-or-less
> linear in the number of observations.  I wouldn't expect that it would be
> worse than quadratic in the number of observations.

Once fm3 is fit, I'll see how increasing the number of groups to 2k, 3k
increases the user time for fitting fm1 and fm2.

> However, increasing the number of random effects per group could cost you
> substantially because that increases the number of parameters to be
> estimated as part of the nonlinear optimization. 
> The iteration output for fm1 will show one parameter being optimized. 
> Many other parameters are involved in the model but their estimates are
> calculated directly, given the one parameter which happens to be the ratio
> of the standard deviation of the random effects to the standard deviation
> of the residuals.  The iteration output for fm2 will show 10 parameters in
> the nonlinear optimization, corresponding to four variances and 6
> covariances of the random effects.  I'm not sure exactly how many
> parameters will be in the nonlinear optimization for fm3 but it will be a
> large number.  I would certainly do some graphical exploration before I
> embarked on trying to fit a model of that complexity.

For fm3, each line looks like this:

   0:     516824.68: 0.120888 0.0364642 0.0300891 0.0361707 0.00780154
0.0103620 0.00786704 0.00195518  0.00000  0.00000  0.00000  0.00000  0.00000
0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000
0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000
0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000

...I assume that this means that for iteration 0, those are the estimated
parameters, and there are 36 of them. It appears to be completing an
iteration every 15-20 seconds. The full interaction should estimate 8
variances and sum(1:7) = 28 covariances = 36 parameters total, so that
checks out. 36 parameters is 3.6 times as many as fm2 required, so
312.860*3.6 = 1126, or about 18 minutes to fit...far less than it's taken.

36^2 / 10^2 = 13 times longer, or about 68 minutes to fit fm3...we'll see
when we get there.

>>  Roughly speaking: lmer and lmer2 aren't (I think) different any more,
>> they were different branches of the same software.  They should both be
>> much faster than lme.  glmer (from lme4) and glmmPQL (from nlme) should
>> not be necessary unless you have binomial, Poisson, etc. data rather than
>> normally distributed responses.
>
> Ben is correct.  There are functions lmer and lmer2 in the current lme4
> package but lmer2 is just a stub that turns around and calls lmer.  If
> your response y is on a continuous scale then it should be lmer that you
> use.

Indeed.

> It will help to have a machine with a lot of memory so you don't end up
> swapping.  I would recommend using a machine running 64-bit Linux (we
> don't have a 64-bit Windows version of R because we are waiting on freely
> available compilers).

R is running in 64-bit mode on a 64-bit linux machine with 16GB of memory
total. fm3 is taking about 11% of memory, and the full model (which I'm
loathe to abort) about 50% (yikes). R was compiled with -O3 and -mtune
flags.

> You may also want to try with an accelerated BLAS.

This version of R is linked to a version of ATLAS (3.8.2) that I compiled
myself before building R on this machine.

> However, I would check both with and without accelerated BLAS if you have
> a multi-core processor.  Sometimes a multithreaded accelerated BLAS can
> make lmer run slower, not faster. This is because the calls to the BLAS
> are predominantly for small matrices and the communications overhead for
> multithreaded versions more than offsets the performance gain from using
> multiple cores.

Interesting...I hadn't thought of that. The processor is 4-core, and R was
linked against the multithreaded BLAS. This may be covered in a faq
somewhere, but is there an easy way to swap out the BLAS that R is
accessing? Perhaps renaming my libraries is sufficient?

Thanks very much for your help!

--Adam



From HDoran at air.org  Thu Sep 11 23:17:24 2008
From: HDoran at air.org (Doran, Harold)
Date: Thu, 11 Sep 2008 17:17:24 -0400
Subject: [R-sig-ME] Speed estimation for lmer?
In-Reply-To: <a46630750809111406o7a8b4b10i9b92e812697e5437@mail.gmail.com>
Message-ID: <ED7B522EE00C9A4FA515AA71724D61EE0128CE42@DC1EXCL01.air.org>

Choosing the identity link in glmer is the same as a linear model. You
can test for speed using system.time(), but there may be internal code
reasons why one would prefer glmer over lmer, that I don't know. 

> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org 
> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf 
> Of Daniel Ezra Johnson
> Sent: Thursday, September 11, 2008 5:07 PM
> To: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Speed estimation for lmer?
> 
> >>  Roughly speaking: lmer and lmer2 aren't (I think) different any 
> >> more, they were different branches of the same software.  
> They should 
> >> both be much faster than lme.  glmer (from lme4) and glmmPQL (from 
> >> nlme) should not be necessary unless you have binomial, 
> Poisson, etc. 
> >> data rather than normally distributed responses.
> >
> > Ben is correct.  There are functions lmer and lmer2 in the current
> > lme4 package but lmer2 is just a stub that turns around and calls 
> > lmer.  If your response y is on a continuous scale then it 
> should be 
> > lmer that you use.
> 
> so if my response is on a continuous scale, is there any 
> difference between
> 
> lmer(y~fixed+(1|random)) and
> glmer(y~fixed+(1|random),family="gaussian") ?
> 
> the output is the same but since reading the above i'm 
> wondering if 'plain' lmer is written to run faster?
> 
> d
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 



From bates at stat.wisc.edu  Thu Sep 11 23:40:45 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 11 Sep 2008 16:40:45 -0500
Subject: [R-sig-ME] Speed estimation for lmer?
In-Reply-To: <a46630750809111406o7a8b4b10i9b92e812697e5437@mail.gmail.com>
References: <Pine.LNX.4.64.0809101710270.10089@parser.ilovebacon.org>
	<48C9349F.9040200@ufl.edu>
	<40e66e0b0809111204u41e62785wd97c9216b1cfd05c@mail.gmail.com>
	<a46630750809111406o7a8b4b10i9b92e812697e5437@mail.gmail.com>
Message-ID: <40e66e0b0809111440h410a4b19p30cbf38514ec64ff@mail.gmail.com>

On Thu, Sep 11, 2008 at 4:06 PM, Daniel Ezra Johnson
<danielezrajohnson at gmail.com> wrote:
>>>  Roughly speaking: lmer and lmer2 aren't (I think) different any more,
>>> they were different branches of the same software.  They should both
>>> be much faster than lme.  glmer (from lme4) and glmmPQL (from nlme)
>>> should not be necessary unless you have binomial, Poisson, etc. data
>>> rather than normally distributed responses.
>>
>> Ben is correct.  There are functions lmer and lmer2 in the current
>> lme4 package but lmer2 is just a stub that turns around and calls
>> lmer.  If your response y is on a continuous scale then it should be
>> lmer that you use.
>
> so if my response is on a continuous scale, is there any difference between
>
> lmer(y~fixed+(1|random)) and
> glmer(y~fixed+(1|random),family="gaussian") ?
>
> the output is the same but since reading the above i'm wondering if
> 'plain' lmer is written to run faster?

Those calls will be equivalent but if you look closely at the object
produced by the glmer call you will find that it has been quietly
replaced by a call to lmer.  There is a section in glmer that reads

    if(family$family == "gaussian" && family$link == "identity") {
        mc[[1]] <- as.name("lmer")      # use lmer not glmer
        mc$family <- NULL
        return(eval.parent(mc))
    }



From bates at stat.wisc.edu  Thu Sep 11 23:52:04 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 11 Sep 2008 16:52:04 -0500
Subject: [R-sig-ME] Speed estimation for lmer?
In-Reply-To: <Pine.LNX.4.64.0809111220150.10089@parser.ilovebacon.org>
References: <Pine.LNX.4.64.0809101710270.10089@parser.ilovebacon.org>
	<48C9349F.9040200@ufl.edu>
	<40e66e0b0809111204u41e62785wd97c9216b1cfd05c@mail.gmail.com>
	<Pine.LNX.4.64.0809111220150.10089@parser.ilovebacon.org>
Message-ID: <40e66e0b0809111452y6db6f378s1d8c2de6dc619ab9@mail.gmail.com>

On Thu, Sep 11, 2008 at 4:12 PM, Adam D. I. Kramer
<adik-rhelp at ilovebacon.org> wrote:
>
> On Thu, 11 Sep 2008, Douglas Bates wrote:
>
>> On Thu, Sep 11, 2008 at 10:09 AM, Ben Bolker <bolker at ufl.edu> wrote:
>>>
>>> Adam D. I. Kramer wrote:
>>>>
>>>> Hi,
>>>>
>>>>    I'm about to estimate what I expect to be a fairly involved model,
>>>> like this one:
>>>>
>>>> l <- lmer(y ~ x1*x2*x3 + (x1*x2*x3|grp) )
>>>>
>>>> ...the data set has 3,232,255 rows, for about 18000 grps, each of which
>>>> has around 700 observations;
>>
>> Are you sure?
>>
>>> 3232255/18000
>>
>> [1] 179.5697
>
> Touche. By "around" I really meant "up to," and by 700 I meant 200 (I'm
> doing some preliminary analysis before all of the cases are in). I apologize
> for this mistake.
>
>>>>  x1, x2, x3 are continuous variables.
>>>>
>>>> Is there any way I can estimate how long this run will take? Obviously
>>>> this depends on things like memory, processor, etc....but perhaps I
>>>> could run it on 5 groups and then multiply the amount of time it takes,
>>>> or something like that?
>>
>> I would start with a sample of groups and all observations for each of the
>> groups in the sample.  Something like 1000 groups should not take a huge
>> amount of time (easily under an hour) if you simplify the random effects
>> specification.  I would start with the
>>
>> gsamp <- with(mydata, sample(levels(grp), 1000))
>> datasamp <- subset(mydata, grp %in% gsamp)
>> system.time(fm1 <- lmer(y ~ x1 * x2 * x3 + (1|grp), datasamp, verbose =
>> TRUE))
>> system.time(fm2 <- lmer(y ~ x1 * x2 * x3 + (x1 + x2 + x3|grp),
>> datasamp, verbose = TRUE))
>> system.time(fm3 <- lmer(y ~ x1 * x2 * x3 + (x1 * x2 * x3|grp),
>> datasamp, verbose = TRUE))
>>
>> The reason that I suggest using verbose = TRUE is because you will be able
>> to see the progress of the iterations and to get a feeling for the amount
>> of time per iteration.
>
> This alone is a fantastic thing to know! It's almost like a progress bar.
>
> And you were right about it not taking long. For fm2:
>   user  system elapsed 289.868  22.335 312.860 ...for the 1000 person
> subgroup on the additive random model.
>
> ...I then started estimating fm3, went to lunch, came back,
> and it is still running.
>
>> Having a large number of observations and of groups will increase the
>> amount of time per iteration but my guess is that it would be more-or-less
>> linear in the number of observations.  I wouldn't expect that it would be
>> worse than quadratic in the number of observations.
>
> Once fm3 is fit, I'll see how increasing the number of groups to 2k, 3k
> increases the user time for fitting fm1 and fm2.
>
>> However, increasing the number of random effects per group could cost you
>> substantially because that increases the number of parameters to be
>> estimated as part of the nonlinear optimization. The iteration output for
>> fm1 will show one parameter being optimized. Many other parameters are
>> involved in the model but their estimates are
>> calculated directly, given the one parameter which happens to be the ratio
>> of the standard deviation of the random effects to the standard deviation
>> of the residuals.  The iteration output for fm2 will show 10 parameters in
>> the nonlinear optimization, corresponding to four variances and 6
>> covariances of the random effects.  I'm not sure exactly how many
>> parameters will be in the nonlinear optimization for fm3 but it will be a
>> large number.  I would certainly do some graphical exploration before I
>> embarked on trying to fit a model of that complexity.
>
> For fm3, each line looks like this:
>
>  0:     516824.68: 0.120888 0.0364642 0.0300891 0.0361707 0.00780154
> 0.0103620 0.00786704 0.00195518  0.00000  0.00000  0.00000  0.00000  0.00000
> 0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000
> 0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000
> 0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000
>
> ...I assume that this means that for iteration 0, those are the estimated
> parameters, and there are 36 of them. It appears to be completing an
> iteration every 15-20 seconds. The full interaction should estimate 8
> variances and sum(1:7) = 28 covariances = 36 parameters total, so that
> checks out. 36 parameters is 3.6 times as many as fm2 required, so
> 312.860*3.6 = 1126, or about 18 minutes to fit...far less than it's taken.

> 36^2 / 10^2 = 13 times longer, or about 68 minutes to fit fm3...we'll see
> when we get there.

There are two ways in which this will take longer.  Each iteration
will take longer and it will probably take many more iterations.  A 36
parameter optimization problem is a big problem.  Optimization usually
doesn't scale linearly.  Worst case it is exponential in the length of
the parameter vector.  Most optimization algorithms can do better than
that but the complexity is still on the order of p^3 or more.

>>>  Roughly speaking: lmer and lmer2 aren't (I think) different any more,
>>> they were different branches of the same software.  They should both be
>>> much faster than lme.  glmer (from lme4) and glmmPQL (from nlme) should
>>> not be necessary unless you have binomial, Poisson, etc. data rather than
>>> normally distributed responses.
>>
>> Ben is correct.  There are functions lmer and lmer2 in the current lme4
>> package but lmer2 is just a stub that turns around and calls lmer.  If
>> your response y is on a continuous scale then it should be lmer that you
>> use.
>
> Indeed.
>
>> It will help to have a machine with a lot of memory so you don't end up
>> swapping.  I would recommend using a machine running 64-bit Linux (we
>> don't have a 64-bit Windows version of R because we are waiting on freely
>> available compilers).
>
> R is running in 64-bit mode on a 64-bit linux machine with 16GB of memory
> total. fm3 is taking about 11% of memory, and the full model (which I'm
> loathe to abort) about 50% (yikes). R was compiled with -O3 and -mtune
> flags.
>
>> You may also want to try with an accelerated BLAS.
>
> This version of R is linked to a version of ATLAS (3.8.2) that I compiled
> myself before building R on this machine.
>
>> However, I would check both with and without accelerated BLAS if you have
>> a multi-core processor.  Sometimes a multithreaded accelerated BLAS can
>> make lmer run slower, not faster. This is because the calls to the BLAS
>> are predominantly for small matrices and the communications overhead for
>> multithreaded versions more than offsets the performance gain from using
>> multiple cores.

> Interesting...I hadn't thought of that. The processor is 4-core, and R was
> linked against the multithreaded BLAS. This may be covered in a faq
> somewhere, but is there an easy way to swap out the BLAS that R is
> accessing? Perhaps renaming my libraries is sufficient?

There is some discussion in the "Administration" manual about this.
Rather than trying to hot swap the libraries I would probably
reconfigure and recompile.  Configure using --with-blas=no and
--with-lapack=no.

> Thanks very much for your help!

You're welcome.



From jeffpowell2 at googlemail.com  Thu Sep 11 22:52:26 2008
From: jeffpowell2 at googlemail.com (Jeff Powell)
Date: Thu, 11 Sep 2008 22:52:26 +0200
Subject: [R-sig-ME] models with no fixed effects
In-Reply-To: <mailman.26072.1221164130.4655.r-sig-mixed-models@r-project.org>
References: <mailman.26072.1221164130.4655.r-sig-mixed-models@r-project.org>
Message-ID: <27E3AEC2-E3A0-40BA-A3CE-C9BBECBBFF1D@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080911/fe5e307d/attachment.pl>

From danielezrajohnson at gmail.com  Thu Sep 11 23:56:56 2008
From: danielezrajohnson at gmail.com (Daniel Ezra Johnson)
Date: Thu, 11 Sep 2008 22:56:56 +0100
Subject: [R-sig-ME] Speed estimation for lmer?
In-Reply-To: <40e66e0b0809111440h410a4b19p30cbf38514ec64ff@mail.gmail.com>
References: <Pine.LNX.4.64.0809101710270.10089@parser.ilovebacon.org>
	<48C9349F.9040200@ufl.edu>
	<40e66e0b0809111204u41e62785wd97c9216b1cfd05c@mail.gmail.com>
	<a46630750809111406o7a8b4b10i9b92e812697e5437@mail.gmail.com>
	<40e66e0b0809111440h410a4b19p30cbf38514ec64ff@mail.gmail.com>
Message-ID: <a46630750809111456p481911c8rc2ef40e661ec6f7a@mail.gmail.com>

>> so if my response is on a continuous scale, is there any difference between
>>
>> lmer(y~fixed+(1|random)) and
>> glmer(y~fixed+(1|random),family="gaussian") ?
>>
>> the output is the same but since reading the above i'm wondering if
>> 'plain' lmer is written to run faster?
>
> Those calls will be equivalent but if you look closely at the object
> produced by the glmer call you will find that it has been quietly
> replaced by a call to lmer.  There is a section in glmer that reads
>
>    if(family$family == "gaussian" && family$link == "identity") {
>        mc[[1]] <- as.name("lmer")      # use lmer not glmer
>        mc$family <- NULL
>        return(eval.parent(mc))
>    }
>

This must be a close cousin of the code that turns e.g.

d <- lmer(y~fixed(1|random),twelve,family="binomial")

into a call to glmer!



From bates at stat.wisc.edu  Fri Sep 12 00:06:32 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 11 Sep 2008 17:06:32 -0500
Subject: [R-sig-ME] Speed estimation for lmer?
In-Reply-To: <a46630750809111456p481911c8rc2ef40e661ec6f7a@mail.gmail.com>
References: <Pine.LNX.4.64.0809101710270.10089@parser.ilovebacon.org>
	<48C9349F.9040200@ufl.edu>
	<40e66e0b0809111204u41e62785wd97c9216b1cfd05c@mail.gmail.com>
	<a46630750809111406o7a8b4b10i9b92e812697e5437@mail.gmail.com>
	<40e66e0b0809111440h410a4b19p30cbf38514ec64ff@mail.gmail.com>
	<a46630750809111456p481911c8rc2ef40e661ec6f7a@mail.gmail.com>
Message-ID: <40e66e0b0809111506y42968c57q8e26294b5e30485@mail.gmail.com>

On Thu, Sep 11, 2008 at 4:56 PM, Daniel Ezra Johnson
<danielezrajohnson at gmail.com> wrote:
>>> so if my response is on a continuous scale, is there any difference between
>>>
>>> lmer(y~fixed+(1|random)) and
>>> glmer(y~fixed+(1|random),family="gaussian") ?
>>>
>>> the output is the same but since reading the above i'm wondering if
>>> 'plain' lmer is written to run faster?
>>
>> Those calls will be equivalent but if you look closely at the object
>> produced by the glmer call you will find that it has been quietly
>> replaced by a call to lmer.  There is a section in glmer that reads
>>
>>    if(family$family == "gaussian" && family$link == "identity") {
>>        mc[[1]] <- as.name("lmer")      # use lmer not glmer
>>        mc$family <- NULL
>>        return(eval.parent(mc))
>>    }
>>
>
> This must be a close cousin of the code that turns e.g.
>
> d <- lmer(y~fixed(1|random),twelve,family="binomial")
>
> into a call to glmer!

Yes.  At times I have thought that having a separate glmer function
made sense and at other times I have thought that having lmer handle
linear or generalized linear or nonlinear or generalized nonlinear
mixed models would be the best way to go.  I still don't know and
would prefer not to antagonize all those people who have written
papers and even books mentioning glmer and lmer by removing the glmer
function.  In time it is possible that glmer will become a stub, just
as lmer2 is now.



From adik-rhelp at ilovebacon.org  Fri Sep 12 00:35:09 2008
From: adik-rhelp at ilovebacon.org (Adam D. I. Kramer)
Date: Thu, 11 Sep 2008 15:35:09 -0700 (PDT)
Subject: [R-sig-ME] Speed estimation for lmer?
In-Reply-To: <40e66e0b0809111452y6db6f378s1d8c2de6dc619ab9@mail.gmail.com>
References: <Pine.LNX.4.64.0809101710270.10089@parser.ilovebacon.org> 
	<48C9349F.9040200@ufl.edu>
	<40e66e0b0809111204u41e62785wd97c9216b1cfd05c@mail.gmail.com>
	<Pine.LNX.4.64.0809111220150.10089@parser.ilovebacon.org>
	<40e66e0b0809111452y6db6f378s1d8c2de6dc619ab9@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0809111526240.10089@parser.ilovebacon.org>


On Thu, 11 Sep 2008, Douglas Bates wrote:

>> ...I assume that this means that for iteration 0, those are the estimated
>> parameters, and there are 36 of them. It appears to be completing an
>> iteration every 15-20 seconds. The full interaction should estimate 8
>> variances and sum(1:7) = 28 covariances = 36 parameters total, so that
>> checks out. 36 parameters is 3.6 times as many as fm2 required, so
>> 312.860*3.6 = 1126, or about 18 minutes to fit...far less than it's
>> taken.
>
>> 36^2 / 10^2 = 13 times longer, or about 68 minutes to fit fm3...we'll see
>> when we get there.
>
> There are two ways in which this will take longer.  Each iteration will
> take longer and it will probably take many more iterations.  A 36
> parameter optimization problem is a big problem.  Optimization usually
> doesn't scale linearly.  Worst case it is exponential in the length of the
> parameter vector.  Most optimization algorithms can do better than that
> but the complexity is still on the order of p^3 or more.

(269 iterations over 106 minutes and counting). A jump from p^10=312 seconds
to p^36 suggests that the model will probably never end up fit; p^3 suggests
243 minutes total (132 minutes from now).

>>> However, I would check both with and without accelerated BLAS if you
>>> have a multi-core processor.  Sometimes a multithreaded accelerated BLAS
>>> can make lmer run slower, not faster. This is because the calls to the
>>> BLAS are predominantly for small matrices and the communications
>>> overhead for multithreaded versions more than offsets the performance
>>> gain from using multiple cores.
>
>> Interesting...I hadn't thought of that. The processor is 4-core, and R
>> was linked against the multithreaded BLAS. This may be covered in a faq
>> somewhere, but is there an easy way to swap out the BLAS that R is
>> accessing? Perhaps renaming my libraries is sufficient?
>
> There is some discussion in the "Administration" manual about this. Rather
> than trying to hot swap the libraries I would probably reconfigure and
> recompile.  Configure using --with-blas=no and --with-lapack=no.

This section in the R-admin manual confused me.

Just to be clear, if I do this, then the BLAS library set in BLAS_LIBS *at
runtime* will be used, or at compile time? Or are you suggesting I build
another version of R which uses non-threaded ATLAS? Or a version of R that
uses its own BLAS?

--Adam



From a.fugard at ed.ac.uk  Fri Sep 12 00:57:07 2008
From: a.fugard at ed.ac.uk (Andy Fugard)
Date: Thu, 11 Sep 2008 23:57:07 +0100
Subject: [R-sig-ME] models with no fixed effects
In-Reply-To: <6BFC5E9F-1180-43C4-8914-E87638FA9003@ualberta.ca>
References: <48C92526020000B600039CA4@ZGRW38.uwcm.ac.uk>	<40e66e0b0809111215t63e59548yb0d772dd59b0dd41@mail.gmail.com>
	<A3C0D575-253B-4E9B-8015-347CFD394F42@ualberta.ca>
	<48C97C59.6050103@ed.ac.uk>
	<6BFC5E9F-1180-43C4-8914-E87638FA9003@ualberta.ca>
Message-ID: <34A320AF-33D4-45AC-ADFD-29766F5CD006@ed.ac.uk>


On 11 Sep 2008, at 22:06, Peter Dixon wrote:

>
> On Sep 11, 2008, at 2:15 PM, Andy Fugard wrote:
>
>> Peter Dixon wrote:
>>> On Sep 11, 2008, at 1:15 PM, Douglas Bates wrote:
>>>> I should definitely add a check on p to the validate method.  (In
>>>> some
>>>> ways I'm surprised that it got as far as mer_finalize before  
>>>> kicking
>>>> an error).  I suppose that p = 0 could be allowed and I could add
>>>> some
>>>> conditional code in the appropriate places but does it really make
>>>> sense to have p = 0?  The random effects are defined to have mean
>>>> zero.  If you have p = 0 that means that E[Y] = 0.  I would have
>>>> difficulty imagining when I would want to make that restriction.
>>>>
>>>> Let me make this offer - if someone could suggest circumstances in
>>>> which such a model would make sense, I will add the appropriate
>>>> conditional code to allow for p = 0. For the time being I will just
>>>> add a requirement of  p >  0 to the validate method.
>>> I think it would make sense to consider a model in which E[Y] = 0
>>> when  the data are (either explicitly or implicitly) difference
>>> scores. (In  fact, I tried to fit such a model with lmer a few
>>> months ago and ran  into exactly this problem.)
>>
>> Wouldn't you still need the intercept?  The fixed effect tells you
>> whether on average the difference differs from zero.  The random
>> effect estimates tell you by how much each individual's difference
>> differs from the mean difference.
>>
>> A
>>
>> -- 
>> Andy Fugard, Postgraduate Research Student
>> Psychology (Room S6), The University of Edinburgh,
>> 7 George Square, Edinburgh EH8 9JZ, UK
>> +44 (0)78 123 87190   http://figuraleffect.googlepages.com/
>>
>> The University of Edinburgh is a charitable body, registered in
>> Scotland, with registration number SC005336.
>>
>>
>
>
> In the context in which this arose, I was interested in assessing the
> evidence for an overall positive difference score (i.e., that E(Y)>0),
> and my strategy was to compare the fit of two models, essentially,  
> D~0+
> (1|Subject) and D~1+(1|Subject), using AIC values. To get a sensible
> assessment of the evidence for the fixed effect, it seemed to me that
> one would want to have the same random effects in the two models being
> compared. The second model is clearly more obvious, but the
> interpretation of D~0+(1|Subject) could be that subjects differ
> randomly in their response to the treatment, but that there is no
> consistent effect in the population.

I /think/ I get this, by analogy with how I use AIC/BIC/LRTs to test  
predictors.  But still a bit confused.  The two models are:

   y_ij = a + b_j + e_ij     (1)
   y_ij = c_j + e_ij         (2)

Suppose a != 0 in model 1.  Then in model 2:

    c_j = b_j + a.

(Maybe it's not as simple as this!)  But I'm not sure what effect that  
would have on the e_ij's - and my intuition says that's what's going  
to affect the fit.  Also I would have thought model 2 would give a  
better fit since having one fewer predictor is going to have less of a  
penalising effect in the AIC and BIC.

Andy

-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.



From smckinney at bccrc.ca  Fri Sep 12 02:19:04 2008
From: smckinney at bccrc.ca (Steven McKinney)
Date: Thu, 11 Sep 2008 17:19:04 -0700
Subject: [R-sig-ME] models with no fixed effects
References: <48C92526020000B600039CA4@ZGRW38.uwcm.ac.uk>	<40e66e0b0809111215t63e59548yb0d772dd59b0dd41@mail.gmail.com>
	<A3C0D575-253B-4E9B-8015-347CFD394F42@ualberta.ca>
	<48C97C59.6050103@ed.ac.uk>
	<6BFC5E9F-1180-43C4-8914-E87638FA9003@ualberta.ca>
	<34A320AF-33D4-45AC-ADFD-29766F5CD006@ed.ac.uk>
Message-ID: <0BE438149FF2254DB4199E2682C8DFEB0328A455@crcmail1.BCCRC.CA>


Not including an intercept term can indeed induce
spurious correlation in linear regression, this
issue is reviewed in Richard Kronmal's excellent paper

 Spurious Correlation and the Fallacy of the Ratio Standard Revisited
 Richard A. Kronmal
 Journal of the Royal Statistical Society. Series A 
  (Statistics in Society), Vol. 156, No. 3 (1993), pp. 379-392 

which could no doubt be readily extended to cover
mixed effect models by A Real Statistician.

The cost of including an intercept term is small
relative to the havoc that can be reaped by not
including one.

Steven McKinney




> 
> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org on behalf of Andy Fugard
> Sent: Thu 9/11/2008 3:57 PM
> To: Peter Dixon
> Cc: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] models with no fixed effects
>  
> 
> On 11 Sep 2008, at 22:06, Peter Dixon wrote:
> 
> >
> > On Sep 11, 2008, at 2:15 PM, Andy Fugard wrote:
> >
> >> Peter Dixon wrote:
> >>> On Sep 11, 2008, at 1:15 PM, Douglas Bates wrote:
> >>>> I should definitely add a check on p to the validate method.  (In
> >>>> some
> >>>> ways I'm surprised that it got as far as mer_finalize before  
> >>>> kicking
> >>>> an error).  I suppose that p = 0 could be allowed and I could add
> >>>> some
> >>>> conditional code in the appropriate places but does it really make
> >>>> sense to have p = 0?  The random effects are defined to have mean
> >>>> zero.  If you have p = 0 that means that E[Y] = 0.  I would have
> >>>> difficulty imagining when I would want to make that restriction.
> >>>>
> >>>> Let me make this offer - if someone could suggest circumstances in
> >>>> which such a model would make sense, I will add the appropriate
> >>>> conditional code to allow for p = 0. For the time being I will just
> >>>> add a requirement of  p >  0 to the validate method.
> >>> I think it would make sense to consider a model in which E[Y] = 0
> >>> when  the data are (either explicitly or implicitly) difference
> >>> scores. (In  fact, I tried to fit such a model with lmer a few
> >>> months ago and ran  into exactly this problem.)
> >>
> >> Wouldn't you still need the intercept?  The fixed effect tells you
> >> whether on average the difference differs from zero.  The random
> >> effect estimates tell you by how much each individual's difference
> >> differs from the mean difference.
> >>
> >> A
> >>
> >> -- 
> >> Andy Fugard, Postgraduate Research Student
> >> Psychology (Room S6), The University of Edinburgh,
> >> 7 George Square, Edinburgh EH8 9JZ, UK
> >> +44 (0)78 123 87190   http://figuraleffect.googlepages.com/
> >>
> >> The University of Edinburgh is a charitable body, registered in
> >> Scotland, with registration number SC005336.
> >>
> >>
> >
> >
> > In the context in which this arose, I was interested in assessing the
> > evidence for an overall positive difference score (i.e., that E(Y)>0),
> > and my strategy was to compare the fit of two models, essentially,  
> > D~0+
> > (1|Subject) and D~1+(1|Subject), using AIC values. To get a sensible
> > assessment of the evidence for the fixed effect, it seemed to me that
> > one would want to have the same random effects in the two models being
> > compared. The second model is clearly more obvious, but the
> > interpretation of D~0+(1|Subject) could be that subjects differ
> > randomly in their response to the treatment, but that there is no
> > consistent effect in the population.
> 
> I /think/ I get this, by analogy with how I use AIC/BIC/LRTs to test  
> predictors.  But still a bit confused.  The two models are:
> 
>    y_ij = a + b_j + e_ij     (1)
>    y_ij = c_j + e_ij         (2)
> 
> Suppose a != 0 in model 1.  Then in model 2:
> 
>     c_j = b_j + a.
> 
> (Maybe it's not as simple as this!)  But I'm not sure what effect that  
> would have on the e_ij's - and my intuition says that's what's going  
> to affect the fit.  Also I would have thought model 2 would give a  
> better fit since having one fewer predictor is going to have less of a  
> penalising effect in the AIC and BIC.
> 
> Andy
> 
> -- 
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 



From n.fernandez at abdn.ac.uk  Fri Sep 12 13:22:53 2008
From: n.fernandez at abdn.ac.uk (Fernandez de la pradilla, J. I.)
Date: Fri, 12 Sep 2008 12:22:53 +0100
Subject: [R-sig-ME] Random effects confidence intervals too large: infinite
 variance or blocking factor estimate is really 0?
Message-ID: <B9D1301370916C44B5874AF340C18B9B28ABB802B3@VMAILB.uoa.abdn.ac.uk>


Dear mixed-up R world,

I am fitting a lmm model in "nlme" package in R version 2.6.0 to my population growth data with the following structure:

Response:       population growth between two sequential time steps (a continuous variable)
Fixed effects (full model version):      Treat + Season + Year + Clust + Treat:Season + Treat:Year +Treat:Clust + Season:Year + Treat:Season:Year (all categorical variables)
Random effects:         1|Block or 1|Block/Plot (all categorical)

The experimental design consists of population growth rates at the plot level taken at different 3 different seasons nested within 2 different years. The spatial experimental design consists of 3 areas (Clusters) each one of them with 2 replicate Blocks each one with 4 plots (1 plot for every Treatment). So we have: 3 areas * 2 Blocks * 4 Treatments (Plots) * 6Time steps (2 years*3 seasons).

I have explored different structures of the random effects to take into account the spatial and temporal design and the best fits seems to be the ones stated above, which are a quite a good description of the experiment. I have considered Block as the basic level of replication given that the experimental set up is randomised for Blocks by design. I plan to incorporate some autoregressive component (AR1) later on and see how it performs.

The following analysis is just a subsample of the original one, with just 2 areas(Clusters) * 2 Treatments(Plots). The subsampled database looks like this:

Time TimeN Year Season Clust Block Treat Plot      Dens      Growth
1    t1     1 2006     Sp    AB     A     I   AI 0.2400000 -0.12000000
2    t1     1 2006     Sp    AB     A    IV  AIV 0.2916667 -0.01166667
3    t1     1 2006     Sp    AB     B     I   BI 0.0000000  0.12000000
4    t1     1 2006     Sp    AB     B    IV  BIV 0.2800000  0.00000000
5    t1     1 2006     Sp    CD     C     I   CI 0.0000000  0.04000000
6    t1     1 2006     Sp    CD     C    IV  CIV 0.0000000  0.04000000
7    t1     1 2006     Sp    CD     D     I   DI 0.0400000 -0.04000000
8    t1     1 2006     Sp    CD     D    IV  DIV 0.1250000  0.03500000
9    t2     2 2006     Su    AB     A     I   AI 0.1200000  0.28000000
10   t2     2 2006     Su    AB     A    IV  AIV 0.2800000  0.42833333
11   t2     2 2006     Su    AB     B     I   BI 0.1200000  0.60000000
12   t2     2 2006     Su    AB     B    IV  BIV 0.2800000  0.36000000
13   t2     2 2006     Su    CD     C     I   CI 0.0400000 -0.04000000
14   t2     2 2006     Su    CD     C    IV  CIV 0.0400000  0.08000000
15   t2     2 2006     Su    CD     D     I   DI 0.0000000  0.00000000
16   t2     2 2006     Su    CD     D    IV  DIV 0.1600000  0.12000000
17   t3     3 2006     Fa    AB     A     I   AI 0.4000000 -0.08000000
18   t3     3 2006     Fa    AB     A    IV  AIV 0.7083333  0.05166667
19   t3     3 2006     Fa    AB     B     I   BI 0.7200000 -0.36000000
20   t3     3 2006     Fa    AB     B    IV  BIV 0.6400000 -0.04000000
21   t3     3 2006     Fa    CD     C     I   CI 0.0000000  0.00000000
22   t3     3 2006     Fa    CD     C    IV  CIV 0.1200000  0.12000000
23   t3     3 2006     Fa    CD     D     I   DI 0.0000000  0.08000000
24   t3     3 2006     Fa    CD     D    IV  DIV 0.2800000  0.28000000
25   t4     4 2007     Sp    AB     A     I   AI 0.3200000  0.04000000
26   t4     4 2007     Sp    AB     A    IV  AIV 0.7600000 -0.23619048
27   t4     4 2007     Sp    AB     B     I   BI 0.3600000 -0.20000000
28   t4     4 2007     Sp    AB     B    IV  BIV 0.6000000  0.04000000
29   t4     4 2007     Sp    CD     C     I   CI 0.0000000  0.00000000
30   t4     4 2007     Sp    CD     C    IV  CIV 0.2400000  0.08000000
31   t4     4 2007     Sp    CD     D     I   DI 0.0800000  0.08000000
32   t4     4 2007     Sp    CD     D    IV  DIV 0.5600000  0.20000000
33   t5     5 2007     Su    AB     A     I   AI 0.3600000  0.28000000
34   t5     5 2007     Su    AB     A    IV  AIV 0.5238095  0.39285714
35   t5     5 2007     Su    AB     B     I   BI 0.1600000  0.60000000
36   t5     5 2007     Su    AB     B    IV  BIV 0.6400000  0.20000000
37   t5     5 2007     Su    CD     C     I   CI 0.0000000  0.28000000
38   t5     5 2007     Su    CD     C    IV  CIV 0.3200000  0.52000000
39   t5     5 2007     Su    CD     D     I   DI 0.1600000  0.28000000
40   t5     5 2007     Su    CD     D    IV  DIV 0.7600000  0.16000000
41   t6     6 2007     Fa    AB     A     I   AI 0.6400000 -0.24000000
42   t6     6 2007     Fa    AB     A    IV  AIV 0.9166667 -0.31666667
43   t6     6 2007     Fa    AB     B     I   BI 0.7600000 -0.40000000
44   t6     6 2007     Fa    AB     B    IV  BIV 0.8400000 -0.36000000
45   t6     6 2007     Fa    CD     C     I   CI 0.2800000 -0.24000000
46   t6     6 2007     Fa    CD     C    IV  CIV 0.8400000 -0.56000000
47   t6     6 2007     Fa    CD     D     I   DI 0.4400000 -0.24000000
48   t6     6 2007     Fa    CD     D    IV  DIV 0.9200000 -0.24000000


And the structure:

'data.frame':   48 obs. of  10 variables:
 $ Time  : Factor w/ 7 levels "t1","t2","t3",..: 1 1 1 1 1 1 1 1 2 2 ...
 $ TimeN : int  1 1 1 1 1 1 1 1 2 2 ...
 $ Year  : Factor w/ 3 levels "2006","2007",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ Season: Factor w/ 3 levels "Fa","Sp","Su": 2 2 2 2 2 2 2 2 3 3 ...
 $ Clust : Factor w/ 2 levels "AB","CD": 1 1 1 1 2 2 2 2 1 1 ...
 $ Block : Factor w/ 4 levels "A","B","C","D": 1 1 2 2 3 3 4 4 1 1 ...
 $ Treat : Factor w/ 2 levels "I","IV": 1 2 1 2 1 2 1 2 1 2 ...
 $ Plot  : Factor w/ 8 levels "AI","AIV","BI",..: 1 2 3 4 5 6 7 8 1 2 ...
 $ Dens  : num  0.240 0.292 0.000 0.280 0.000 ...
 $ Growth: num  -0.1200 -0.0117  0.1200  0.0000  0.0400 ...


So first we go for the most parameterized version with random effects specified as (1|Block/Plot), therefore allowing for 6 temporal replicates of every "Plot" level.

The full model output fitting a looks fine:
> Gm4c<-lme(fixed=Growth~Season+Year+Season:Year, random=~1|Block/Plot,method="ML")
> summary(Gm4c)

Linear mixed-effects model fit by maximum likelihood
 Data: NULL
      AIC       BIC logLik
  -29.808 -12.96719 23.904

Random effects:
 Formula: ~1 | Block
         (Intercept)
StdDev: 4.096235e-07

 Formula: ~1 | Plot %in% Block
        (Intercept)  Residual
StdDev: 8.33304e-08 0.1470565

 (Fixed effects omitted)

Standardized Within-Group Residuals:
       Min         Q1        Med         Q3        Max
-2.4919563 -0.5316126  0.1190019  0.5751758  2.5259568

Number of Observations: 48
Number of Groups:
          Block Plot %in% Block
              4               8

But I don't get any confidence intervals:

> intervals(Gm4c)
Error in intervals.lme(Gm4c) :
  Cannot get confidence intervals on var-cov components: Non-positive definite approximate variance-covariance


So I reduce the random effects specification to allow just estimation of effects given by the grouping "Block" factor, therefore allowing temporal replication and increasing df. However, I still obtain ridiculously large confidence intervals for this grouping factor:

> Gm3c<-lme(fixed=Growth~Season+Year+Season:Year, random=~1|Block,method="ML")
> summary(Gm3c)

Linear mixed-effects model fit by maximum likelihood
 Data: NULL
      AIC       BIC logLik
  -31.808 -16.83839 23.904

Random effects:
 Formula: ~1 | Block
         (Intercept)  Residual
StdDev: 3.352947e-07 0.1470565

(Fixed effects omitted)

Standardized Within-Group Residuals:
       Min         Q1        Med         Q3        Max
-2.4919563 -0.5316126  0.1190019  0.5751758  2.5259568

Number of Observations: 48
Number of Groups: 4

Intervals:

> intervals(Gm3c)

Approximate 95% confidence intervals

 Fixed effects:
                        lower         est.      upper
(Intercept)       -0.09870606  0.006458333  0.1116227
SeasonSp          -0.14726657  0.001458333  0.1501832
SeasonSu           0.07335843  0.222083333  0.3708082
Year2007          -0.47976657 -0.331041667 -0.1823168
SeasonSp:Year2007  0.11327241  0.323601191  0.5339300
SeasonSu:Year2007  0.23127836  0.441607143  0.6519359
attr(,"label")
[1] "Fixed effects:"

 Random Effects:
  Level: Block
                lower         est. upper
sd((Intercept))     0 3.352947e-07   Inf

 Within-group standard error:
    lower      est.     upper
0.1203951 0.1470565 0.1796221

So my question is two-fold:

1) Are the large confidence intervals obtained in the model Gm3c realistic (i.e. the variance due to Block effects is simply = 0 and Block is not explaining any variance at all) or just a computing problem (i.e. R is unable to calculate the confidence intervals due to limited replication)?

1)  When I allow temporal replication (6 time steps) for estimating variance due to "Plot" (model Gm4c), I would expect to get at least some confidence intervals (even as large as in Gm3c ), but I don't get anything. Does anybody has any idea why is that?

Thanks in advance. I look forward to your suggestions and welcome your views,

Nacho


Nacho Villar
PhD Student
ACES - Aberdeen Centre for Environmental Sustainability
University of Aberdeen-Zoology Building
Tillydrone Avenue - AB24 2TZ Aberdeen
n.fernandez at abdn.ac.uk
http://www.aces.ac.uk/people/




The University of Aberdeen is a charity registered in Scotland, No SC013683.



From a.fugard at ed.ac.uk  Fri Sep 12 13:35:38 2008
From: a.fugard at ed.ac.uk (Andy Fugard)
Date: Fri, 12 Sep 2008 12:35:38 +0100
Subject: [R-sig-ME] models with no fixed effects
In-Reply-To: <27E3AEC2-E3A0-40BA-A3CE-C9BBECBBFF1D@gmail.com>
References: <mailman.26072.1221164130.4655.r-sig-mixed-models@r-project.org>
	<27E3AEC2-E3A0-40BA-A3CE-C9BBECBBFF1D@gmail.com>
Message-ID: <48CA540A.7010403@ed.ac.uk>

I wonder do you get something analogous if you just stick in a Gaussian 
distributed x, just as a way to get p > 0 for fit0.

   > df$y <- with(df <- data.frame(i = gl(10, 10), b = rep(25 +
   >   rnorm(10), each = 10)), b + rnorm(100))
   > df$x = rnorm(100,0,5)
   >
   > fit1 <- lmer(y ~ 1 + x + (1 | i), df, REML=F)
   > fit0 <- lmer(y ~ 0 + x + (1 | i), df, REML=F)

So the intercept in fit1 != 0 (by design):

   > fixef(fit1)[1]
   (Intercept)
            25

And indeed on average the random effect estimates in fit0 are about 25 
bigger than they were in fit1:

   > summary(ranef(fit0)$i - ranef(fit1)$i)
     (Intercept)
    Min.   :24.4
    1st Qu.:24.5
    Median :24.6
    Mean   :24.6
    3rd Qu.:24.6
    Max.   :24.8

The residuals have the same variance in both fit0 and fit1, but the 
variance of i is much larger as you'd expect:

   > summary(fit1)
   ...
   Random effects:
    Groups   Name        Variance Std.Dev.
    i        (Intercept) 0.379    0.616
    Residual             0.790    0.889
   ...

   > summary(fit0)
   ...
   Random effects:
    Groups   Name        Variance Std.Dev.
    i        (Intercept) 605.18   24.600
    Residual               0.79    0.889
   ...

But fit1 seems better by AIC, BIC, and the LLR-test:

   > anova(fit0,fit1)
   ...
        Df  AIC  BIC logLik Chisq Chi Df Pr(>Chisq)
   fit0  3  356  363   -175
   fit1  4  286  296   -139  71.9      1     <2e-16 ***

So the variance of all the random effects, not only the residuals, 
affects the fit(?).  Dunno if this is an artefact of the x ~ N(0,5).

Trying again with the intercept at 0, then (most of the time!) there's 
no difference between the models:

   > df$y <- with(df <- data.frame(i = gl(10, 10), b = rep(0 + rnorm(10),
   >   each = 10)), b + rnorm(100))
   > df$x = rnorm(100,0,5)
   >
   > fit1 <- lmer(y ~ 1 + x + (1 | i), df, REML=F)
   > fit0 <- lmer(y ~ 0 + x + (1 | i), df, REML=F)
   >
   > anova(fit0,fit1)
   Data: df
   Models:
   ...
        Df  AIC  BIC logLik Chisq Chi Df Pr(>Chisq)
   fit0  3  299  306   -146
   fit1  4  300  311   -146  0.25      1       0.62

Andy


Jeff Powell wrote:
> 
> On Sep 11, 2008, at 10:15 PM, r-sig-mixed-models-request at r-project.org 
> <mailto:r-sig-mixed-models-request at r-project.org> wrote:
> ------------------------------
> ------------------------------
> 
>> Message: 7
>> Date: Thu, 11 Sep 2008 21:15:21 +0100
>> From: Andy Fugard <a.fugard at ed.ac.uk <mailto:a.fugard at ed.ac.uk>>
>> Subject: Re: [R-sig-ME] models with no fixed effects
>> To: Peter Dixon <peter.dixon at ualberta.ca <mailto:peter.dixon at ualberta.ca>>
>> Cc: r-sig-mixed-models at r-project.org 
>> <mailto:r-sig-mixed-models at r-project.org>
>> Message-ID: <48C97C59.6050103 at ed.ac.uk <mailto:48C97C59.6050103 at ed.ac.uk>>
>> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
>>
>> Peter Dixon wrote:
>>> On Sep 11, 2008, at 1:15 PM, Douglas Bates wrote:
>>>
>>>> I should definitely add a check on p to the validate method.  (In some
>>>> ways I'm surprised that it got as far as mer_finalize before kicking
>>>> an error).  I suppose that p = 0 could be allowed and I could add some
>>>> conditional code in the appropriate places but does it really make
>>>> sense to have p = 0?  The random effects are defined to have mean
>>>> zero.  If you have p = 0 that means that E[Y] = 0.  I would have
>>>> difficulty imagining when I would want to make that restriction.
>>>>
>>>> Let me make this offer - if someone could suggest circumstances in
>>>> which such a model would make sense, I will add the appropriate
>>>> conditional code to allow for p = 0. For the time being I will just
>>>> add a requirement of  p >  0 to the validate method.
>>>
>>> I think it would make sense to consider a model in which E[Y] = 0 when  
>>> the data are (either explicitly or implicitly) difference scores. (In  
>>> fact, I tried to fit such a model with lmer a few months ago and ran  
>>> into exactly this problem.)
>>
>> Wouldn't you still need the intercept?  The fixed effect tells you 
>> whether on average the difference differs from zero.  The random effect 
>> estimates tell you by how much each individual's difference differs from 
>> the mean difference.
> 
> If you are looking for correlations between difference scores, the model 
> needs to be constrained to pass through the intercept. Otherwise the fit 
> is dependent on the direction in which the differences are calculated 
> (positive/negative scores). I ran into this error earlier this week 
> while trying a mixed model to evaluate some phylogenetically-independent 
> contrasts.
> 
> Jeff
> 
> 
> 
> __________________________________
> 
> Jeff R. Powell, Ph.D.
> Postdoctoral Fellow
> Freie Universit?t Berlin
> Intitut f?r Biologie - ?kologie der Pflanzen
> Altensteinstra?e 6
> 14195 Berlin
> Germany
> 
> Ph: ++49 (0)30 838-53145
> Fx: ++49 (0)30 838-55434
> skype: jeff-powell
> jeffpowell2 at gmail.com <mailto:jeffpowell2 at gmail.com>
> 
> http://jeffpowell2.googlepages.com/
> __________________________________
> 


-- 
Andy Fugard, Postgraduate Research Student
Psychology (Room S6), The University of Edinburgh,
   7 George Square, Edinburgh EH8 9JZ, UK
+44 (0)78 123 87190   http://figuraleffect.googlepages.com/

The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.



From farewelld at Cardiff.ac.uk  Fri Sep 12 17:48:35 2008
From: farewelld at Cardiff.ac.uk (Daniel Farewell)
Date: Fri, 12 Sep 2008 16:48:35 +0100
Subject: [R-sig-ME] models with no fixed effects
Message-ID: <48CA9D63020000B600039E71@ZGRW38.uwcm.ac.uk>

I've read with interest the various arguments for and against models with no fixed effects! My
motivation for fitting such models was actually rather mundane, and hopefully uncontroversial: I
want to evaluate various fit-related quantities at fixed values of the parameters.

The kind of thing I'm after can be illustrated in an OLS context:

Generating some data with

y <- rnorm(50)

and fitting a no-fixed-effects model as

lm(y ~ 0)

works fine. Now this model isn't much use in itself, but we could use fits of this kind to plot a
profile likelihood for the mean by running

plot(b <- seq(-1, 1, 0.1), sapply(b, function(x) logLik(lm(y ~ 0, offset = rep(x, 50)))))

I hope this constitutes an example of a "useful" no-fixed-effects model?

In principle, things like logLik, predict and ranef are functions of the model, data and parameters.
In practice, they take model fits as arguments, so I guess what I really want is a way to "fake" a
model fit -- to pretend that lm or lmer has converged to a particular set of values. Using an offset
with lm, and no fixed effects, is equivalent to forcing some of the parameters to be what I want,
though the residual variance is still estimated.

In lmer (or lm, or glm, or ...), I'd love to be able to specify values for some, or all, of the
parameters, and then tell the fitting function not to update these parameters. We could then get the
quantities we want computed as a byproduct, in my case the posterior estimated random effects. Other
possible uses of this kind of functionality would be likelihood plots like the one above, or
numerical differentiation of the likelihood with respect to a parameter.

Is such a thing reasonable? Am I oversimplifying?

Very many thanks,

Daniel



From adik-rhelp at ilovebacon.org  Sat Sep 13 21:25:17 2008
From: adik-rhelp at ilovebacon.org (Adam D. I. Kramer)
Date: Sat, 13 Sep 2008 12:25:17 -0700 (PDT)
Subject: [R-sig-ME] predict.lmer or lmer to lme conversion?
Message-ID: <Pine.LNX.4.64.0809131221080.10089@parser.ilovebacon.org>

Hello,

 	I'm interested in making predictions from an lmer result:

> class(p1.lmer.additive)
[1] "mer"
attr(,"package")
[1] "lme4"

...naturally, I tried predict(p1.lmer.additive,newdata), but was told that
no acceptable method exists. The expected predict.lmer did not exist either.
I know that predict.lme exists for predictions based on the older lme()
output.

If I want predictions, should I refit my models with lme()? Is there a way
to convert an object of type "mer" to type "lme"? I'm not used to working
with S4 objects, so when names(p1.lmer.additive) is NULL, I get a bit
nervous.

Any suggestions appreciated.

--Adam



From David.Ramsey at dse.vic.gov.au  Tue Sep 16 07:44:07 2008
From: David.Ramsey at dse.vic.gov.au (David.Ramsey at dse.vic.gov.au)
Date: Tue, 16 Sep 2008 15:44:07 +1000
Subject: [R-sig-ME] glmer and overdispersed Poisson models
Message-ID: <OF69A66023.0C690D12-ONCA2574C6.001CC908-CA2574C6.001F80E1@nre.vic.gov.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080916/6323578c/attachment.pl>

From ken at kjbeath.com.au  Tue Sep 16 09:00:22 2008
From: ken at kjbeath.com.au (Ken Beath)
Date: Tue, 16 Sep 2008 17:00:22 +1000
Subject: [R-sig-ME] glmer and overdispersed Poisson models
In-Reply-To: <OF69A66023.0C690D12-ONCA2574C6.001CC908-CA2574C6.001F80E1@nre.vic.gov.au>
References: <OF69A66023.0C690D12-ONCA2574C6.001CC908-CA2574C6.001F80E1@nre.vic.gov.au>
Message-ID: <FF3FBA1D-C23C-4349-AEC1-BFA179E4FBB4@kjbeath.com.au>

On 16/09/2008, at 3:44 PM, David.Ramsey at dse.vic.gov.au wrote:

> Hi All,  I have recently upgraded my version of lme4 and redid an old
> analysis.  The data are counts and are overdispersed and an offset is
> included.  I originally fitted a mixed model in lmer() using
> quasi-likelihood
>
> lme4 version 0.99875-9   Matrix version 0.999375-4
>
>> fit1= lmer(total ~ direction + time + flow + offset(log(effort)) +
> (1|site),family=quasipoisson(link=log),data=data,method="ML")
>> summary(fit1)
> Generalized linear mixed model fit using Laplace
> Formula: total ~ direction + time + flow + offset(log(effort)) + (1 |
> site)
>   Data: data
> Family: quasipoisson(log link)
>    AIC    BIC  logLik deviance
> 222622 222637 -111305   222610
> Random effects:
> Groups   Name        Variance Std.Dev.
> site     (Intercept) 3930.7   62.695
> Residual             3544.0   59.531
> number of obs: 80, groups: site, 7
>

My suspicion is that the very high variance of the random effects is  
the problem resulting from the sites having an extreme range. This  
will give the Laplace approximation some problems so adaptive Gauss- 
Hermite may work but this data seems extreme. I'm guessing that each  
observation is either small or large.

Ken




> Fixed effects:
>             Estimate Std. Error t value
> (Intercept)   1.46436   23.70079   0.062
> directionout -0.22065    0.27874  -0.792
> timenight    -1.16554    0.28429  -4.100
> flows         0.01255    0.43394   0.029
> flowf         0.40867    0.55300   0.739
>
> Correlation of Fixed Effects:
>            (Intr) drctnt tmnght flows
> directionot -0.005
> timenight   -0.008  0.086
> flows       -0.011 -0.066  0.072
> flowf       -0.011 -0.048  0.041  0.687
>
> So far so good.  We have an estimate of the scale of 59.5 (ouch - yes
> pretty bad overdispersion!).
> I redid this analysis recently with the latest version of lme4
>
> lme4 version 0.999375-26: Matrix version 0.999375-14
>
>> fit2= glmer(total ~ direction + time + flow + offset(log(effort)) +
> (1|site),family=quasipoisson(link=log),data=data,REML=F)
>> summary(fit2)
> Generalized linear mixed model fit by the Laplace approximation
> Formula: total ~ direction + time + flow + offset(log(effort)) + (1 |
> site)
>   Data: data
>    AIC    BIC  logLik deviance
> 222624 222641 -111305   222610
> Random effects:
> Groups   Name        Variance Std.Dev.
> site     (Intercept) 4019367  2004.8
> Residual             3487254  1867.4
> Number of obs: 80, groups: site, 7
>
> Fixed effects:
>              Estimate Std. Error  t value
> (Intercept)    1.44981  757.88854  0.00191
> directionout  -0.22063    8.74381 -0.02523
> timenight     -1.16552    8.91772 -0.13070
> flows          0.01266   13.61242  0.00093
> flowf          0.40877   17.34707  0.02356
>
> Correlation of Fixed Effects:
>            (Intr) drctnt tmnght flows
> directionot -0.005
> timenight   -0.008  0.086
> flows       -0.011 -0.066  0.072
> flowf       -0.011 -0.048  0.041  0.687
>
> Yikes!! the estimate of the site random effect and scale are now  
> orders of
> magnitude larger.
> Log-likelihood and deviance is the same as are the estimates of fixed
> effects and correlations.
> Obviously the SE of the fixed effects are now a bit larger... This
> complicates
> my inferences from the previous analysis, to say the least.
>
> I guess there has been some changes in the way the scale is  
> estimated in
> the latest version of lmer (glmer).
> If I refit these models without the estimation of the scale  
> parameter (and
> close my eyes..)
>
> lme4 version 0.99875-9   Matrix version 0.999375-4
>
>> fit3<- lmer(total ~ direction + time + flow + offset(log(effort)) +
> (1|site),family=poisson(link=log),data=data,method="ML")
>> summary(fit3)
> Generalized linear mixed model fit using Laplace
> Formula: total ~ direction + time + flow + offset(log(effort)) + (1 |
> site)
>   Data: data
> Family: poisson(log link)
>    AIC    BIC  logLik deviance
> 222622 222637 -111305   222610
> Random effects:
> Groups Name        Variance Std.Dev.
> site   (Intercept) 1.1131   1.0550
> number of obs: 80, groups: site, 7
>
> Estimated scale (compare to  1 )  59.53134
>
> Fixed effects:
>              Estimate Std. Error z value Pr(>|z|)
> (Intercept)   1.464490   0.398841    3.67 0.000241 ***
> directionout -0.220652   0.004682  -47.12  < 2e-16 ***
> timenight    -1.165554   0.004775 -244.07  < 2e-16 ***
> flows         0.012577   0.007289    1.73 0.084456 .
> flowf         0.408721   0.009289   44.00  < 2e-16 ***
>
>
> lme4 version 0.999375-26: Matrix version 0.999375-14
>
>> fit4= glmer(total ~ direction + time + flow + offset(log(effort)) +
> (1|site),family=poisson(link=log),data=data,REML=F)
>> summary(fit6)
> Generalized linear mixed model fit by the Laplace approximation
> Formula: total ~ direction + time + flow + offset(log(effort)) + (1 |
> site)
>   Data: data
>    AIC    BIC  logLik deviance
> 222622 222637 -111305   222610
> Random effects:
> Groups Name        Variance Std.Dev.
> site   (Intercept) 1.1526   1.0736
> Number of obs: 80, groups: site, 7
>
> Fixed effects:
>              Estimate Std. Error z value Pr(>|z|)
> (Intercept)   1.449810   0.405848    3.57 0.000354 ***
> directionout -0.220630   0.004682  -47.12  < 2e-16 ***
> timenight    -1.165520   0.004775 -244.07  < 2e-16 ***
> flows         0.012655   0.007289    1.74 0.082547 .
> flowf         0.408774   0.009289   44.00  < 2e-16 ***
>
> The results from the two different versions now agree(to within a few
> decimal places).
> I notice the latest version of glmer() now does not output the scale  
> in
> the summary.
>
> I guess my question is "which of these is correct ?" I routinely  
> have to
> deal with
> overdispersed count data and would appreciate any advice on conducting
> these
> sorts of analyses in lme4.
>
> Thanks in advance
> Dave
>
>
> Notice:
> This email and any attachments may contain information t...{{dropped: 
> 14}}
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From contact at rensenieuwenhuis.nl  Tue Sep 16 15:03:30 2008
From: contact at rensenieuwenhuis.nl (Rense Nieuwenhuis)
Date: Tue, 16 Sep 2008 15:03:30 +0200
Subject: [R-sig-ME] Unexpected text-output using BY to plot random effects.
Message-ID: <29464F6C-E724-4906-9AEE-440872B316F0@rensenieuwenhuis.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080916/d89f06a3/attachment.pl>

From d.rizopoulos at erasmusmc.nl  Tue Sep 16 15:27:36 2008
From: d.rizopoulos at erasmusmc.nl (Dimitris Rizopoulos)
Date: Tue, 16 Sep 2008 15:27:36 +0200
Subject: [R-sig-ME] Unexpected text-output using BY to plot random
	effects.
In-Reply-To: <29464F6C-E724-4906-9AEE-440872B316F0@rensenieuwenhuis.nl>
References: <29464F6C-E724-4906-9AEE-440872B316F0@rensenieuwenhuis.nl>
Message-ID: <48CFB448.8040405@erasmusmc.nl>

abline() does not return anything, it justs adds lines to the existing 
plot; this is why you obtain NULL from the by() statement. One option to 
avoid printing is to use invisible(), e.g.,

invisible(by(sleep, sleep$resp, function(x) abline(coef=x)))


I hope it helps.

Best,
Dimitris


Rense Nieuwenhuis wrote:
> Dear Helpers,
> 
> I have been trying to visualise the random effects of a mixed effects  
> model which was fitted with the lmer function from the lme4 package.  
> Extracting the fixed and random effects works rather well with  
> 'coef', but when plotting this with 'by' and 'abline', something  
> strange happens.
> 
> I use the following syntax (example):
> 
> fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
> 
> sleep <- coef(fm1)$Subject
> sleep$resp <- 1:18
> 
> plot(c(0,10), c(200,400), type="n")
> 
> by(sleep, resp, function(x) abline(coef=x))
> 
> This results in a basic, but acceptable, plot. However, it also  
> results in quite some output at the R-Prompt. A portion of it looks  
> like this:
> 
> resp: 308
> NULL
> -----------------------------------------------------
> resp: 309
> NULL
> -----------------------------------------------------
> resp: 310
> NULL
> -----------------------------------------------------
> resp: 330
> NULL
> -----------------------------------------------------
> resp: 331
> NULL
> -----------------------------------------------------
> resp: 332
> NULL
> 
> 
> What is happening? I suspect it has something to do with my use of  
> the 'by' function, but is there a way to prevent all the unwanted  
> text-output?
> 
> Thanks in advance
> 
> Rense
> 
> 
> 
> 
> 
> - - -- --- ----- --------
> Rense Nieuwenhuis
> +31 6 481 05 683
> 
> www.rensenieuwenhuis.nl
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 

-- 
Dimitris Rizopoulos
Assistant Professor
Department of Biostatistics
Erasmus University Medical Center

Address: PO Box 2040, 3000 CA Rotterdam, the Netherlands
Tel: +31/(0)10/7043478
Fax: +31/(0)10/7043014



From spluque at gmail.com  Tue Sep 16 16:13:14 2008
From: spluque at gmail.com (Sebastian P. Luque)
Date: Tue, 16 Sep 2008 09:13:14 -0500
Subject: [R-sig-ME] Unexpected text-output using BY to plot random
	effects.
References: <29464F6C-E724-4906-9AEE-440872B316F0@rensenieuwenhuis.nl>
	<48CFB448.8040405@erasmusmc.nl>
Message-ID: <87abe8w551.fsf@patagonia.sebmags.homelinux.org>

On Tue, 16 Sep 2008 15:27:36 +0200,
Dimitris Rizopoulos <d.rizopoulos at erasmusmc.nl> wrote:

> abline() does not return anything, it justs adds lines to the existing
> plot; this is why you obtain NULL from the by() statement. One option
> to avoid printing is to use invisible(), e.g.,

> invisible(by(sleep, sleep$resp, function(x) abline(coef=x)))

Also, make sure you're using abline correctly here.  Especially, read
about the 'coef' argument.


-- 
Seb



From bates at stat.wisc.edu  Tue Sep 16 20:42:59 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 16 Sep 2008 13:42:59 -0500
Subject: [R-sig-ME] glmer and overdispersed Poisson models
In-Reply-To: <OF69A66023.0C690D12-ONCA2574C6.001CC908-CA2574C6.001F80E1@nre.vic.gov.au>
References: <OF69A66023.0C690D12-ONCA2574C6.001CC908-CA2574C6.001F80E1@nre.vic.gov.au>
Message-ID: <40e66e0b0809161142g319749a3q3db94b036cd5d532@mail.gmail.com>

On Tue, Sep 16, 2008 at 12:44 AM,  <David.Ramsey at dse.vic.gov.au> wrote:
> Hi All,  I have recently upgraded my version of lme4 and redid an old
> analysis.  The data are counts and are overdispersed and an offset is
> included.  I originally fitted a mixed model in lmer() using
> quasi-likelihood

> lme4 version 0.99875-9   Matrix version 0.999375-4

>> fit1= lmer(total ~ direction + time + flow + offset(log(effort)) +
> (1|site),family=quasipoisson(link=log),data=data,method="ML")
>> summary(fit1)
> Generalized linear mixed model fit using Laplace
> Formula: total ~ direction + time + flow + offset(log(effort)) + (1 |
> site)
>   Data: data
>  Family: quasipoisson(log link)
>    AIC    BIC  logLik deviance
>  222622 222637 -111305   222610
> Random effects:
>  Groups   Name        Variance Std.Dev.
>  site     (Intercept) 3930.7   62.695
>  Residual             3544.0   59.531
> number of obs: 80, groups: site, 7

> Fixed effects:
>             Estimate Std. Error t value
> (Intercept)   1.46436   23.70079   0.062
> directionout -0.22065    0.27874  -0.792
> timenight    -1.16554    0.28429  -4.100
> flows         0.01255    0.43394   0.029
> flowf         0.40867    0.55300   0.739

> Correlation of Fixed Effects:
>            (Intr) drctnt tmnght flows
> directionot -0.005
> timenight   -0.008  0.086
> flows       -0.011 -0.066  0.072
> flowf       -0.011 -0.048  0.041  0.687

> So far so good.  We have an estimate of the scale of 59.5 (ouch - yes
> pretty bad overdispersion!).

Yes.  It raises questions of whether it is a good idea to use the
results at all.

> I redid this analysis recently with the latest version of lme4

> lme4 version 0.999375-26: Matrix version 0.999375-14

>> fit2= glmer(total ~ direction + time + flow + offset(log(effort)) +
> (1|site),family=quasipoisson(link=log),data=data,REML=F)
>> summary(fit2)
> Generalized linear mixed model fit by the Laplace approximation
> Formula: total ~ direction + time + flow + offset(log(effort)) + (1 |
> site)
>   Data: data
>    AIC    BIC  logLik deviance
>  222624 222641 -111305   222610
> Random effects:
>  Groups   Name        Variance Std.Dev.
>  site     (Intercept) 4019367  2004.8
>  Residual             3487254  1867.4
> Number of obs: 80, groups: site, 7

That much of a difference cannot be due to changes in the optimization
method.  There must be a different formula being employed.  I suspect
that I am not taking the weights into account properly when
calculating the scale factor in one or the other implementation and
probably that the mistake is in the more recent implementation.

I have difficulty in following the calculations for the quasipoisson
and quasibinomial families because they are not really probability
distributions and you end up estimating a parameter that "isn't
there", in some sense.

If someone can suggest what the formula for the scale factor should
be, I can make a modification but I don't really have time right now
to research it or derive the theory.

> Fixed effects:
>              Estimate Std. Error  t value
> (Intercept)    1.44981  757.88854  0.00191
> directionout  -0.22063    8.74381 -0.02523
> timenight     -1.16552    8.91772 -0.13070
> flows          0.01266   13.61242  0.00093
> flowf          0.40877   17.34707  0.02356
>
> Correlation of Fixed Effects:
>            (Intr) drctnt tmnght flows
> directionot -0.005
> timenight   -0.008  0.086
> flows       -0.011 -0.066  0.072
> flowf       -0.011 -0.048  0.041  0.687
>
> Yikes!! the estimate of the site random effect and scale are now orders of
> magnitude larger.
> Log-likelihood and deviance is the same as are the estimates of fixed
> effects and correlations.
> Obviously the SE of the fixed effects are now a bit larger... This
> complicates
> my inferences from the previous analysis, to say the least.
>
> I guess there has been some changes in the way the scale is estimated in
> the latest version of lmer (glmer).
> If I refit these models without the estimation of the scale parameter (and
> close my eyes..)
>
> lme4 version 0.99875-9   Matrix version 0.999375-4
>
>>fit3<- lmer(total ~ direction + time + flow + offset(log(effort)) +
> (1|site),family=poisson(link=log),data=data,method="ML")
>> summary(fit3)
> Generalized linear mixed model fit using Laplace
> Formula: total ~ direction + time + flow + offset(log(effort)) + (1 |
> site)
>   Data: data
>  Family: poisson(log link)
>    AIC    BIC  logLik deviance
>  222622 222637 -111305   222610
> Random effects:
>  Groups Name        Variance Std.Dev.
>  site   (Intercept) 1.1131   1.0550
> number of obs: 80, groups: site, 7
>
> Estimated scale (compare to  1 )  59.53134
>
> Fixed effects:
>              Estimate Std. Error z value Pr(>|z|)
> (Intercept)   1.464490   0.398841    3.67 0.000241 ***
> directionout -0.220652   0.004682  -47.12  < 2e-16 ***
> timenight    -1.165554   0.004775 -244.07  < 2e-16 ***
> flows         0.012577   0.007289    1.73 0.084456 .
> flowf         0.408721   0.009289   44.00  < 2e-16 ***
>
>
> lme4 version 0.999375-26: Matrix version 0.999375-14
>
>> fit4= glmer(total ~ direction + time + flow + offset(log(effort)) +
> (1|site),family=poisson(link=log),data=data,REML=F)
>> summary(fit6)
> Generalized linear mixed model fit by the Laplace approximation
> Formula: total ~ direction + time + flow + offset(log(effort)) + (1 |
> site)
>   Data: data
>    AIC    BIC  logLik deviance
>  222622 222637 -111305   222610
> Random effects:
>  Groups Name        Variance Std.Dev.
>  site   (Intercept) 1.1526   1.0736
> Number of obs: 80, groups: site, 7
>
> Fixed effects:
>              Estimate Std. Error z value Pr(>|z|)
> (Intercept)   1.449810   0.405848    3.57 0.000354 ***
> directionout -0.220630   0.004682  -47.12  < 2e-16 ***
> timenight    -1.165520   0.004775 -244.07  < 2e-16 ***
> flows         0.012655   0.007289    1.74 0.082547 .
> flowf         0.408774   0.009289   44.00  < 2e-16 ***
>
> The results from the two different versions now agree(to within a few
> decimal places).
> I notice the latest version of glmer() now does not output the scale in
> the summary.
>
> I guess my question is "which of these is correct ?" I routinely have to
> deal with
> overdispersed count data and would appreciate any advice on conducting
> these
> sorts of analyses in lme4.
>
> Thanks in advance
> Dave
>
>
> Notice:
> This email and any attachments may contain information t...{{dropped:14}}
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From David.Ramsey at dse.vic.gov.au  Wed Sep 17 04:00:17 2008
From: David.Ramsey at dse.vic.gov.au (David.Ramsey at dse.vic.gov.au)
Date: Wed, 17 Sep 2008 12:00:17 +1000
Subject: [R-sig-ME] glmer and overdispersed Poisson models
In-Reply-To: <40e66e0b0809161142g319749a3q3db94b036cd5d532@mail.gmail.com>
Message-ID: <OF5701E80E.4D08E12F-ONCA2574C6.008024E3-CA2574C7.000B02C1@nre.vic.gov.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080917/8b0791dc/attachment.pl>

From a.renwick at abdn.ac.uk  Wed Sep 17 14:11:48 2008
From: a.renwick at abdn.ac.uk (Renwick, A. R.)
Date: Wed, 17 Sep 2008 13:11:48 +0100
Subject: [R-sig-ME] REML estimates not given
Message-ID: <B9D1301370916C44B5874AF340C18B9B28AB99054C@VMAILB.uoa.abdn.ac.uk>

I am running an lmer model using a poisson distribution and log link.  However, when I ask REML=TRUE I do not get a REML estimate only the ML estimate.

For example:
mix3<-lmer(RoundedOverlap~Sex+Margin+sess+Sex:Margin+(1|Site), family=poisson, data=both,REML=TRUE)
 mix3 at deviance
        ML       REML
159.158662         NA

mix4<-lmer(RoundedOverlap~Sex+Margin+sess+Sex:Margin+(1|Site), family=poisson, data=both,REML=FALSE)
 mix4 at deviance
        ML       REML
159.158662         NA

I am using both the latest R (R version 2.7.2 ) and lme4 package

I wasw ondering if anybody knows why REML does not seem to be being used and if this is an issue with poisson distributed models in general.
Many thanks
Anna


Anna Renwick
Zoology Building
School of Biological Sciences
University of Aberdeen
Aberdeen
AB24 2TZ


The University of Aberdeen is a charity registered in Scotland, No SC013683.



From bates at stat.wisc.edu  Wed Sep 17 14:37:03 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 17 Sep 2008 07:37:03 -0500
Subject: [R-sig-ME] REML estimates not given
In-Reply-To: <B9D1301370916C44B5874AF340C18B9B28AB99054C@VMAILB.uoa.abdn.ac.uk>
References: <B9D1301370916C44B5874AF340C18B9B28AB99054C@VMAILB.uoa.abdn.ac.uk>
Message-ID: <40e66e0b0809170537i57ff7ad7se3168bcac1745d9d@mail.gmail.com>

On Wed, Sep 17, 2008 at 7:11 AM, Renwick, A. R. <a.renwick at abdn.ac.uk> wrote:
> I am running an lmer model using a poisson distribution and log link.  However, when I ask REML=TRUE I do not get a REML estimate only the ML estimate.

> For example:
> mix3<-lmer(RoundedOverlap~Sex+Margin+sess+Sex:Margin+(1|Site), family=poisson, data=both,REML=TRUE)
>  mix3 at deviance
>        ML       REML
> 159.158662         NA

> mix4<-lmer(RoundedOverlap~Sex+Margin+sess+Sex:Margin+(1|Site), family=poisson, data=both,REML=FALSE)
>  mix4 at deviance
>        ML       REML
> 159.158662         NA

> I am using both the latest R (R version 2.7.2 ) and lme4 package

> I was wondering if anybody knows why REML does not seem to be being used and if this is an issue with poisson distributed models in general.

The REML criterion is not defined for generalized linear mixed models.
 Well, at least I don't know of a definition that makes sense for
GLMMs.



From lewin-koh.nicholas at gene.com  Wed Sep 17 18:05:44 2008
From: lewin-koh.nicholas at gene.com (Nicholas Lewin-Koh)
Date: Wed, 17 Sep 2008 09:05:44 -0700
Subject: [R-sig-ME] mcmcsamp can't handle random effects syntax
Message-ID: <009101c918df$3da49660$c00b2c0a@gne.windows.gene.com>


Hi,
This may be well known, but it was mucking me up.

> fm1 <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject), sleepstudy)
> ttt<-mcmcsamp(fm1,n=1000)
> fm1 <- lmer(Reaction ~ Days + (1+Days|Subject), sleepstudy)
> ttt<-mcmcsamp(fm1,n=1000)
Error in .local(object, n, verbose, ...) : 
  Code for non-trivial theta_T not yet written
> fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
> ttt<-mcmcsamp(fm1,n=1000)
Error in .local(object, n, verbose, ...) : 
  Code for non-trivial theta_T not yet written

These are all the same model, right?

Thanks
Nicholas



sessionInfo() 
R version 2.7.2 Patched (2008-08-26 r46446) 
i386-pc-mingw32 

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
States.1252;LC_MONETARY=English_United
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] splines   stats     graphics  grDevices utils     datasets  methods  
[8] base     

other attached packages:
[1] XML_1.96-0         lme4_0.999375-26   Matrix_0.999375-13 lattice_0.17-14


loaded via a namespace (and not attached):
[1] grid_2.7.2


 "Seek the company of those who seek the truth, and run away from those who
have found it." - Vaclav Havel



From bates at stat.wisc.edu  Wed Sep 17 18:10:50 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 17 Sep 2008 11:10:50 -0500
Subject: [R-sig-ME] mcmcsamp can't handle random effects syntax
In-Reply-To: <009101c918df$3da49660$c00b2c0a@gne.windows.gene.com>
References: <009101c918df$3da49660$c00b2c0a@gne.windows.gene.com>
Message-ID: <40e66e0b0809170910h72781f43v2564915320d5e57e@mail.gmail.com>

On Wed, Sep 17, 2008 at 11:05 AM, Nicholas Lewin-Koh
<lewin-koh.nicholas at gene.com> wrote:

> Hi,
> This may be well known, but it was mucking me up.

>> fm1 <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject), sleepstudy)
>> ttt<-mcmcsamp(fm1,n=1000)
>> fm1 <- lmer(Reaction ~ Days + (1+Days|Subject), sleepstudy)
>> ttt<-mcmcsamp(fm1,n=1000)
> Error in .local(object, n, verbose, ...) :
>  Code for non-trivial theta_T not yet written
>> fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
>> ttt<-mcmcsamp(fm1,n=1000)
> Error in .local(object, n, verbose, ...) :
>  Code for non-trivial theta_T not yet written

> These are all the same model, right?

Not really.  The first model has uncorrelated random effects.  The
other two have correlated random effects.  The rather cryptic and
uninformative error message is a reminder to me that I need to write
code for the MCMC update of a covariance parameter.



From lewin-koh.nicholas at gene.com  Wed Sep 17 22:08:45 2008
From: lewin-koh.nicholas at gene.com (Nicholas Lewin-Koh)
Date: Wed, 17 Sep 2008 13:08:45 -0700
Subject: [R-sig-ME] mcmcsamp can't handle random effects syntax
In-Reply-To: <40e66e0b0809170910h72781f43v2564915320d5e57e@mail.gmail.com>
References: <009101c918df$3da49660$c00b2c0a@gne.windows.gene.com>
	<40e66e0b0809170910h72781f43v2564915320d5e57e@mail.gmail.com>
Message-ID: <009601c91901$31d7d5a0$c00b2c0a@gne.windows.gene.com>

Duhh, that should have occurred to me, if I'd just looked at the model
ouput. Thanks

Nicholas

-----Original Message-----
From: dmbates at gmail.com [mailto:dmbates at gmail.com] On Behalf Of Douglas
Bates
Sent: Wednesday, September 17, 2008 9:11 AM
To: Nicholas Lewin-Koh
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] mcmcsamp can't handle random effects syntax

On Wed, Sep 17, 2008 at 11:05 AM, Nicholas Lewin-Koh
<lewin-koh.nicholas at gene.com> wrote:

> Hi,
> This may be well known, but it was mucking me up.

>> fm1 <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject), sleepstudy)
>> ttt<-mcmcsamp(fm1,n=1000)
>> fm1 <- lmer(Reaction ~ Days + (1+Days|Subject), sleepstudy)
>> ttt<-mcmcsamp(fm1,n=1000)
> Error in .local(object, n, verbose, ...) :
>  Code for non-trivial theta_T not yet written
>> fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
>> ttt<-mcmcsamp(fm1,n=1000)
> Error in .local(object, n, verbose, ...) :
>  Code for non-trivial theta_T not yet written

> These are all the same model, right?

Not really.  The first model has uncorrelated random effects.  The
other two have correlated random effects.  The rather cryptic and
uninformative error message is a reminder to me that I need to write
code for the MCMC update of a covariance parameter.



From adik-rhelp at ilovebacon.org  Fri Sep 19 00:41:40 2008
From: adik-rhelp at ilovebacon.org (Adam D. I. Kramer)
Date: Thu, 18 Sep 2008 15:41:40 -0700 (PDT)
Subject: [R-sig-ME] Heirarchical Multivariate Modeling?
Message-ID: <Pine.LNX.4.64.0809181540390.10089@parser.ilovebacon.org>

Dear colleagues,

  	I have an interest in what I would call "heirarchical multivariate
modeling." In a sense, I'm interested in extending the mixed model procedure
to an "unpredicted" multivariate case, or an analysis which would be an
extension to princomp() or prcomp() just as lmer() is an extension to lm().

  	My actual interest is in 1. estimating an aggregate PCA based on the
factor structures that exist within many individuals, each of which is based
on a different number of observations among the same set of variables, and
2. testing whether factor structures differ across people (e.g., whether
prediction improves if I model a random effect for subject). This can be
thought of as adding and testing a random effect to a PCA, or something
similar.

  	My first intuition of how to go about this would be to use the glmer
procedure, and attempt to model the entire set of variables as being
predicted by a random "intercept" for each subject, but before I undertake
this analysis, I thought it might be wise to see if anyone on this list had
any suggestions of a better way to go about this in R (or suggestions that
the above way is inappropriate).

  	Also, if anybody could recommend an article or two on the topic (I
have not seen any), I would be quite interested.

Cordially,
Adam Kramer



From Thierry.ONKELINX at inbo.be  Fri Sep 19 10:15:45 2008
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Fri, 19 Sep 2008 10:15:45 +0200
Subject: [R-sig-ME] [R] Mixed effects model with binomial errors -
	problem
In-Reply-To: <19566778.post@talk.nabble.com>
References: <19413327.post@talk.nabble.com><loom.20080910T135251-581@post.gmane.org><19414516.post@talk.nabble.com><loom.20080910T184237-496@post.gmane.org><19436083.post@talk.nabble.com>
	<19566778.post@talk.nabble.com>
Message-ID: <2E9C414912813E4EB981326983E0A104057ABDDE@inboexch.inbo.be>


First of all I'm forwarding this mail to the R-SIG-mixed-models, which
is more appropriate to your question.

Remember that family = bionomial uses by default the logit link. Hence
all parameters will be on the logit scale. So you will need to
backtransform them for comparison. Then you'll see that the parameters
are much closer to the averages. They still differ, but that is due to
the difference in model. Your averages are essentially something like
summary(model1<-glm(cbind(VisitsExpTree,TotalVisits-VisitsExpTree)~
treatment +(1|Individual), family=binomial, data=r))

> library(boot)
> intercept <- 0.37228
> treatmentb <- 0.03367
> treatmentc <- -0.60606
> treatmentd <- -0.25504
> inv.logit(intercept)
[1] 0.5920098
> inv.logit(intercept + treatmentb)
[1] 0.6001164
> inv.logit(intercept + treatmentc)
[1] 0.4418197
> inv.logit(intercept + treatmentd)
[1] 0.5292765

HTH,

Thierry

------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium 
tel. + 32 54/436 185
Thierry.Onkelinx at inbo.be 
www.inbo.be 

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to
say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of
data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org]
Namens RFTW
Verzonden: vrijdag 19 september 2008 8:16
Aan: r-help at r-project.org
Onderwerp: Re: [R] Mixed effects model with binomial errors - problem


anyone?


RFTW wrote:
> 
> ok... the model now runs properly (say, without errors). Now about the
> result.
> These are the averages per treatments
> 
> tapply(VecesArbolCo.VecesCo.C1,T2,mean)
>   a     b      c     d 
> 0.49 0.56 0.45 0.58 
> 
> 
> I run this very simple model
> 
>> summary(model1<-lmer(cbind(VisitsExpTree,TotalVisits-VisitsExpTree)~
>> treatment +(1|Individual), family=binomial, data=r))
> 
> Generalized linear mixed model fit by the Laplace approximation 
> Formula: cbind(VisitsExpTree,TotalVisits-VisitsExpTree)~ treatment
> +(1|Individual)  
>    Data: r 
>    AIC   BIC logLik deviance
>  242.3 255.9 -116.2    232.3
> Random effects:
>  Groups    Name        Variance Std.Dev.
>  Individuo (Intercept) 0.14075  0.37517 
> Number of obs: 112, groups: Individuo, 37
> 
> Fixed effects:
>             Estimate Std. Error z value Pr(>|z|)   
> (Intercept)          0.37228    0.19031  1.9562  0.05044 . 
> treatmentb          0.03367    0.24520  0.1373  0.89079   
> treatmentc         -0.60606    0.23330 -2.5978  0.00938 **
> treatmentd         -0.25504    0.22790 -1.1191  0.26311   
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> 
> Correlation of Fixed Effects:
>     (Intr) T2b    T2c   
> T2b -0.675              
> T2c -0.697  0.543       
> T2d -0.720  0.544  0.581
> 
> 
> wouldnt we expect the intercept to be roughtly the mean of treatment
a?
> and thus the estimate of treatmentb to be +0.07, c: -0.04 and d: +0.09
> roughly?
> 
> Is this model just completely not estimating well, or are the
estimates
> not the 'real values'. 
> 
> I tried to get teh predict function to give me the 4 predicted values
> based on the model, but i havent succeeded in doing so. maybe someone
can
> help me on that one too (predict(model1,type="response") doesnt work)
> 
> thnx
> 

-- 
View this message in context:
http://www.nabble.com/Mixed-effects-model-with-binomial-errors----proble
m-tp19413327p19566778.html
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document. The views expressed in  this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.



From a.renwick at abdn.ac.uk  Fri Sep 19 12:59:02 2008
From: a.renwick at abdn.ac.uk (Renwick, A. R.)
Date: Fri, 19 Sep 2008 11:59:02 +0100
Subject: [R-sig-ME] Scale parameter and within variance random factor in a
	poisson model
Message-ID: <B9D1301370916C44B5874AF340C18B9B28AB990564@VMAILB.uoa.abdn.ac.uk>

Dear All
Sorry but I have another question!
I am running a GLMM with poisson distribution and log link and I was wondering:
1) how to get the scale parameter and if there is a reason why the new version of lme4 does not report that in the model summary
2) why GLMM models do not give the within variance of the random factor.  When using a poisson family the value for sigma using the VarCorr(model) command is not estimated , probably due to the fact that REML criterion is not defined for generalized linear mixed models.  One way I thought to obtain this within variation was to calulate it using the ST value (ST=variance between/ variance within) and I was wondering if this is the correct procedure or if there is a better method.


Many thanks again,

Anna

Anna Renwick
Zoology Building
School of Biological Sciences
University of Aberdeen
Aberdeen
AB24 2TZ


The University of Aberdeen is a charity registered in Scotland, No SC013683.



From contact at rensenieuwenhuis.nl  Fri Sep 19 17:25:41 2008
From: contact at rensenieuwenhuis.nl (Rense Nieuwenhuis)
Date: Fri, 19 Sep 2008 17:25:41 +0200
Subject: [R-sig-ME] Altering the intercept-vector in lme4?
Message-ID: <913C77BC-9A52-4AB0-B331-8C3211644400@rensenieuwenhuis.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080919/a1a65ab4/attachment.pl>

From kyler at mail.smu.edu  Fri Sep 19 18:27:47 2008
From: kyler at mail.smu.edu (Roberts, Kyle)
Date: Fri, 19 Sep 2008 11:27:47 -0500
Subject: [R-sig-ME] Weights in lmer
In-Reply-To: <913C77BC-9A52-4AB0-B331-8C3211644400@rensenieuwenhuis.nl>
References: <913C77BC-9A52-4AB0-B331-8C3211644400@rensenieuwenhuis.nl>
Message-ID: <6FBE93B66C50154D9F1E55B674AFCC9D0104A65A@s31xe7.systems.smu.edu>

Dear All,

Still trying to use the weights function with the ECLS (Early Child Longitudinal Study) with lmer. I think that I have identified the problem, but I am not sure. If a person has a "0" for a weight (they don't have all years of data), how does lmer handle this case? Also, how is the weight treated when you have a stacked format (measurement occasions nested inside students)? For 4 measurement occasions, is the weight multiplied by each of the 4 time points, or does lmer somehow apply the weight at the student level (or does it matter)?

Thanks,
Kyle

*********************************************************
Dr. J. Kyle Roberts
Department of Literacy, Language, and Learning
School of Education and Human Development
Southern Methodist University
P.O. Box 750381
Dallas, TX? 75275
214-768-4494
http://www.hlm-online.com/



From bates at stat.wisc.edu  Fri Sep 19 20:08:07 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 19 Sep 2008 13:08:07 -0500
Subject: [R-sig-ME] Altering the intercept-vector in lme4?
In-Reply-To: <913C77BC-9A52-4AB0-B331-8C3211644400@rensenieuwenhuis.nl>
References: <913C77BC-9A52-4AB0-B331-8C3211644400@rensenieuwenhuis.nl>
Message-ID: <40e66e0b0809191108t1fe0e517qe135c8b5fac3d54e@mail.gmail.com>

On Fri, Sep 19, 2008 at 10:25 AM, Rense Nieuwenhuis
<contact at rensenieuwenhuis.nl> wrote:
> Dear all,
>
> while working on a project regarding a new approach at evaluating
> measures on influential 'cases' in mixed effects models, we would
> like to alter the intercept in such a way that observations nested
> within a specified grouping factor have a value 0 on the intercept
> (and all the others the normal 1).
>
> Whereas this is relatively simple in some statistical packages that
> require the manual definition of the intercept, I have difficulties
> to understand how to do this in the lme4 package. This is what I tried:
>
> require(lme4)
>
> data(bdf, package='mlmRev')
> model.1 <- lmer(IQ.perf ~ sex + schoolSES + (1 | schoolNR), data=bdf)
> data.adapted <- bdf
> data.adapted$intercept.alt <- 1
> data.adapted$intercept.alt[data.adapted$schoolNR==1] <- 0
> data.adapted$dummy <- 0
> data.adapted$dummy[data.adapted$schoolNR==1] <- 1

> model.2 <- lmer(IQ.perf ~ -1 + intercept.alt + dummy + sex +
> schoolSES + (-1 + intercept.alt | schoolNR), data=data.adapted)

> (Please note that we also add a dummy with value 1 for the specified
> grouping factor, and a 0 for all other groups.)

I think that is the cause of the problem when you combine those
indicators with the indicators for sex.  The way that a model matrix
is constructed in R, if the intercept is removed then both indicators
are included for the first factor (sex in this case).  You can
circumvent this problem by creating a factor for schoolNR == 1 as
shown below.  In general you should avoid trying to create indicator
variables explicitly and use the model formula to create them

> bdf <- within(bdf, sch1 <- factor(ifelse(schoolNR == 1, "Y", "N")))
> (model.1 <- lmer(IQ.perf ~ sex + schoolSES + (1 | schoolNR), data=bdf))
Linear mixed model fit by REML
Formula: IQ.perf ~ sex + schoolSES + (1 | schoolNR)
   Data: bdf
   AIC   BIC logLik deviance REMLdev
 10058 10087  -5024    10034   10048
Random effects:
 Groups   Name        Variance Std.Dev.
 schoolNR (Intercept) 0.17049  0.41291
 Residual             4.58623  2.14155
Number of obs: 2287, groups: schoolNR, 131

Fixed effects:
            Estimate Std. Error t value
(Intercept) 10.16620    0.26013   39.08
sex1        -0.46084    0.09063   -5.08
schoolSES    0.05676    0.01333    4.26

Correlation of Fixed Effects:
          (Intr) sex1
sex1      -0.146
schoolSES -0.960 -0.020
> (model.2 <- lmer(IQ.perf ~ 0 + sch1 + sex + schoolSES + (1 | schoolNR), bdf))
Linear mixed model fit by REML
Formula: IQ.perf ~ 0 + sch1 + sex + schoolSES + (1 | schoolNR)
   Data: bdf
   AIC   BIC logLik deviance REMLdev
 10058 10092  -5023    10033   10046
Random effects:
 Groups   Name        Variance Std.Dev.
 schoolNR (Intercept) 0.16993  0.41222
 Residual             4.58667  2.14165
Number of obs: 2287, groups: schoolNR, 131

Fixed effects:
          Estimate Std. Error t value
sch1N     10.21585    0.26491   38.56
sch1Y      9.62569    0.61365   15.69
sex1      -0.46154    0.09064   -5.09
schoolSES  0.05445    0.01353    4.03

Correlation of Fixed Effects:
          sch1N  sch1Y  sex1
sch1Y      0.242
sex1      -0.145 -0.055
schoolSES -0.961 -0.241 -0.019

> Unfortunately, model.2 does not converge**. This is the error message
> that is given:
>
> Error in mer_finalize(ans, verbose) :
>   Downdated X'X is not positive definite, 4.
>
> Does anyone know if it is possible to manually change the intercept-
> vector, giving it the value 0 for a select number of cases? If it is
> possible, how is it done? This would help us greatly in translating a
> procedure to R-Project, making it available to a larger audience.
>
> Many thanks in advance,
>
> with kind regards,
>
> Rense Nieuwenhuis
>
>
>
> ** It does when I specify the sex-variable as.numeric(). This however
> results in really messy outcomes.
>
>
> - - -- --- ----- --------
> Rense Nieuwenhuis
> +31 6 481 05 683
>
> www.rensenieuwenhuis.nl
>
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From ken at kjbeath.com.au  Tue Sep 23 00:59:39 2008
From: ken at kjbeath.com.au (Ken Beath)
Date: Tue, 23 Sep 2008 08:59:39 +1000
Subject: [R-sig-ME] ML or REML for LR tests
In-Reply-To: <m0zlmfakqi.fsf@urwireless-dhcp-128-151-20-16.wireless.rochester.edu>
References: <a46630750808281107u582a779fvec72e695356d1713@mail.gmail.com>
	<ED7B522EE00C9A4FA515AA71724D61EE0128C8DE@DC1EXCL01.air.org>
	<m0k5e0h1go.fsf@gmail.com>
	<008ED4AA-D47D-444E-B7E5-AC88DFBE4A19@kjbeath.com.au>
	<m0zlmfakqi.fsf@urwireless-dhcp-128-151-20-16.wireless.rochester.edu>
Message-ID: <AC61A56D-ECBC-494D-AF33-B429AFAF314F@kjbeath.com.au>

On 11/09/2008, at 5:01 AM, Austin Frank wrote:

> First off, thanks to all who have responded to the series of  
> questions I
> asked!
>
> On Fri, Aug 29 2008, Ken Beath wrote:
>
>> On 29/08/2008, at 2:47 PM, Austin Frank wrote:
>>
>>> 3) Is it the case that LR tests between REML models with different
>>> random effects are meaningful?  Does this apply to both nested and
>>> non-nested models?
>>>
>>
>> Maybe, but only for nested (see Q2). Supposedly it works better than
>> ML. The significance tests wont be correct but if there is a huge
>> significance level then there is probably a random effect. Simulation
>> seems a better idea.
>
> Ken was the only one to address this particular point, and I want to
> make sure I've got it straight.  Are REML-based likelihood-ratio tests
> (presumably not performed with anova.mer, as that sets REML=FALSE on  
> the
> call to logLik) an acceptable method for testing nested models with
> different random effects specifications?
>

This is discussed in Pinheiro and Bates. It is not statistically  
correct, so probably isn't a good idea for a publication. I recommend  
reading the sections in Verbeke and Molenberghs book on Linear Mixed  
Models. I have used AIC.

The easiest way to avoid choices is to decide that certain parameters  
must be modelled by random effects based on medical, biological etc  
considerations.

Ken


> As a point of reference, the anova() method is called on two lmer  
> models
> that differ only in their random effects in the manuscript by Baayen,
> Davidson, and Bates at
> http://www.ualberta.ca/~baayen/publications/baayenDavidsonBates.pdf  
> (pp
> 12-15).  The discussion of that analysis makes no mention of the
> difference between REML and ML fits.  Is this because, as discussed
> recently, the REML and ML estimates are so close that there is no
> practical difference in which quantity is used for this test?
>
>
> Thanks again!
> /au
>
> -- 
> Austin Frank
> http://aufrank.net
> GPG Public Key (D7398C2F): http://aufrank.net/personal.asc
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From ken at kjbeath.com.au  Tue Sep 23 01:53:22 2008
From: ken at kjbeath.com.au (Ken Beath)
Date: Tue, 23 Sep 2008 09:53:22 +1000
Subject: [R-sig-ME] Heirarchical Multivariate Modeling?
In-Reply-To: <Pine.LNX.4.64.0809181540390.10089@parser.ilovebacon.org>
References: <Pine.LNX.4.64.0809181540390.10089@parser.ilovebacon.org>
Message-ID: <416B8868-9306-4E22-AEF1-EFDB64C5E92E@kjbeath.com.au>

On 19/09/2008, at 8:41 AM, Adam D. I. Kramer wrote:

> Dear colleagues,
>
> 	I have an interest in what I would call "heirarchical multivariate
> modeling." In a sense, I'm interested in extending the mixed model  
> procedure
> to an "unpredicted" multivariate case, or an analysis which would be  
> an
> extension to princomp() or prcomp() just as lmer() is an extension  
> to lm().
>
> 	My actual interest is in 1. estimating an aggregate PCA based on the
> factor structures that exist within many individuals, each of which  
> is based
> on a different number of observations among the same set of  
> variables, and
> 2. testing whether factor structures differ across people (e.g.,  
> whether
> prediction improves if I model a random effect for subject). This  
> can be
> thought of as adding and testing a random effect to a PCA, or  
> something
> similar.
>
> 	My first intuition of how to go about this would be to use the glmer
> procedure, and attempt to model the entire set of variables as being
> predicted by a random "intercept" for each subject, but before I  
> undertake
> this analysis, I thought it might be wise to see if anyone on this  
> list had
> any suggestions of a better way to go about this in R (or  
> suggestions that
> the above way is inappropriate).
>
> 	Also, if anybody could recommend an article or two on the topic (I
> have not seen any), I would be quite interested.

It is possible to create multilevel versions of multivariate methods,  
maybe not PCA, but for factor analysis, yes. The sem package could  
probably be coerced into fitting them for linear models, otherwise the  
commercial programs Latent Gold and MPlus are the only solutions. The  
Mplus site has lots of modelling info.

Ken



From David.Duffy at qimr.edu.au  Tue Sep 23 04:44:15 2008
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Tue, 23 Sep 2008 12:44:15 +1000 (EST)
Subject: [R-sig-ME] Heirarchical Multivariate Modeling?
In-Reply-To: <416B8868-9306-4E22-AEF1-EFDB64C5E92E@kjbeath.com.au>
References: <Pine.LNX.4.64.0809181540390.10089@parser.ilovebacon.org>
	<416B8868-9306-4E22-AEF1-EFDB64C5E92E@kjbeath.com.au>
Message-ID: <Pine.LNX.4.64.0809231229510.2387@orpheus.qimr.edu.au>

On Tue, 23 Sep 2008, Ken Beath wrote:

> On 19/09/2008, at 8:41 AM, Adam D. I. Kramer wrote:
>
>> Dear colleagues,
>>
>> 	My actual interest is in 1. estimating an aggregate PCA based on the
>> factor structures that exist within many individuals, each of which is 
>> based
>> on a different number of observations among the same set of variables, and
>> 2. testing whether factor structures differ across people (e.g., whether
>> prediction improves if I model a random effect for subject). This can be
>> thought of as adding and testing a random effect to a PCA, or something
>> similar.
>
> It is possible to create multilevel versions of multivariate methods, maybe 
> not PCA, but for factor analysis, yes. The sem package could probably be 
> coerced into fitting them for linear models, otherwise the commercial 
> programs Latent Gold and MPlus are the only solutions. The Mplus site has 
> lots of modelling info.
>
> Ken

What he said ;)  Look at any textbook on structural equation modelling 
under measurement model etc.  If you have binary or ordinal data, we fit 
multivariate random effects models under the threshold model (see 
tetrachoric or polychoric correlations), usually these days with Mx 
(http://www.vcu.edu/mx/).  Mx can fit to summary correlation or covariance 
matrices, but also implements "raw data" ML methods that would deal with 
varying numbers and types of observations per individual such as you 
describe.


David Duffy.

-- 
| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v



From osumaria at gmail.com  Wed Sep 24 16:23:18 2008
From: osumaria at gmail.com (Maria Jose Juan Jorda)
Date: Wed, 24 Sep 2008 16:23:18 +0200
Subject: [R-sig-ME]  lmer and the weight option.
Message-ID: <e8c3f2360809240723n728f65ffr9224b63232789e92@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080924/1aa05be0/attachment.pl>

From s.ruiter at maw.ru.nl  Thu Sep 25 10:36:45 2008
From: s.ruiter at maw.ru.nl (Stijn Ruiter)
Date: Thu, 25 Sep 2008 10:36:45 +0200
Subject: [R-sig-ME] false convergence (8) in glmer
Message-ID: <48DB4D9D.3030506@maw.ru.nl>

Hi,
The suggested (though horrible) hack to provide a greater number of
iterations (see digest-list: /Thu Jul 10 18:49:15 CEST 2008/)

.Call("mer_optimize", <failed_lmer_fit>, FALSE, PACKAGE = "lme4")

does not seem to work anymore. In previous versions it did.

Any suggestions on how to continue iterating, because I have an non-converged model after glmer ends its process.
It results in the following warning message:
In mer_finalize(ans) : false convergence (8)

Thanks,
Stijn


-- 
Best regards,

Stijn Ruiter
Department of Sociology / ICS
Radboud University Nijmegen
P.O. Box 9104
6500 HE Nijmegen
Netherlands

Phone: + 31 24 361 2272
Fax:   + 31 24 361 2399

Visiting address:
Thomas van Aquinostraat 4.01.71
Nijmegen

website: http://oase.uci.ru.nl/~sruiter



From bates at stat.wisc.edu  Thu Sep 25 15:02:04 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 25 Sep 2008 08:02:04 -0500
Subject: [R-sig-ME] false convergence (8) in glmer
In-Reply-To: <48DB4D9D.3030506@maw.ru.nl>
References: <48DB4D9D.3030506@maw.ru.nl>
Message-ID: <40e66e0b0809250602i3ad72a4cn1cd051989a00f204@mail.gmail.com>

On Thu, Sep 25, 2008 at 3:36 AM, Stijn Ruiter <s.ruiter at maw.ru.nl> wrote:
> Hi,
> The suggested (though horrible) hack to provide a greater number of
> iterations (see digest-list: /Thu Jul 10 18:49:15 CEST 2008/)
>
> .Call("mer_optimize", <failed_lmer_fit>, FALSE, PACKAGE = "lme4")
>
> does not seem to work anymore. In previous versions it did.

I imagine this is a property of the model and data set and not a
property of the call.

> Any suggestions on how to continue iterating, because I have an non-converged model after glmer ends its process.
> It results in the following warning message:
> In mer_finalize(ans) : false convergence (8)

That's a warning that the parameter estimates are suspect but it's not
an error. I believe that a fitted model would have been returned.

I suggest you turn on the verbose option in the optimizer and examine
the progress of the iterations.  In the original call to glmer you add
verbose = TRUE.  If re-calling "mer_optimize" as above then change the
FALSE to TRUE.



From therneau at mayo.edu  Thu Sep 25 19:51:50 2008
From: therneau at mayo.edu (Terry Therneau)
Date: Thu, 25 Sep 2008 12:51:50 -0500 (CDT)
Subject: [R-sig-ME] 3 bugs in lmer
Message-ID: <200809251751.m8PHpoa22591@hsrnfs-101.mayo.edu>

These are of the better-user-interface variety; features that are common to all 
the other model functions that I know of, but seem to be missing in lmer.  One 
could argue that "shortfalls" would be a better label than "bugs".  

> version
               _                           
platform       sparc-sun-solaris2.10       
arch           sparc                       
os             solaris2.10                 
system         sparc, solaris2.10          
status                                     
major          2                           
minor          7.1                         
year           2008                        
month          06                          
day            23                          
svn rev        45970                       
language       R                           
version.string R version 2.7.1 (2008-06-23)


#1 - subsets
fit1c <- lmer(il2 ~ (1|plate/id), data=allsets, subset=(il2>0) )
fit1d <- lmer(il2 ~ (1|plate/id), data=allsets, subset= il2>0)

  I always put subset arguments in parenthesis for readability.  (In the actual 
code that sparked this message, the subset expression was longer, and it helps 
more).  Both models fit ok, but the first one doesn't print.
  
> all.equal(coef(fit1c), coef(fit1d)
[1] TRUE
> print(fit1c)
Linear mixed model fit by REML 
Formula: il2 ~ (1 | plate/id) 
   Data: allsets 
Error in sprintf(gettext(fmt, domain = domain), ...) : 
  object "x" not found


#2 coefficients
  Like everyone else, I almost never type the full "coefficients" method and use 
the "coef" abbreviation.  But I happened to do so today:
  
> coefficients(fit1d)
Error in object$coefficients : $ operator not defined for this S4 class


#3  character variables

  I almost never, ever, turn a character variable into a factor.  We can argue 
about the wisdom of my choice another day, but you'll likely not sway me.  
Character variables work fine in model statements, being treated as factors; for 
models other than lmer.
  Here is a fit on my original data set, before I modified it for the benefit of 
 lmer:

> fit1b <- lmer(il2 ~ (1|plate/id), data=data2)        
Error in id:plate : NA/NaN argument
In addition: Warning messages:
1: In id:plate :
  numerical expression has 3184 elements: only the first used
2: In id:plate :
  numerical expression has 3184 elements: only the first used
3: In inherits(x, "factor") : NAs introduced by coercion

By the way, there are no odd or missing values in plate or id.  

Oops, I just realized the above is wrong: plate is an integer.  So try again:

>  table(data2$plate)

 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 
80 80 70 80 80 80 46 78 80 80 80 80 80 64 80 80 80 80 80 80 76 80 80 80 80 80 
27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 
80 70 80 80 80 80 80 80 80 80 64 80 80 80 80 

> fit1b <- lmer(il2 ~ (1|factor(plate)/id), data=data2)
Error in inherits(x, "factor") : object "plate" not found

> data2$plate2 <- factor(data2$plate)
> fit1b <- lmer(il2 ~ (1|plate2/id), data=data2)   

Error in id:plate2 : NA/NaN argument
In addition: Warning messages:
1: In id:plate2 :
  numerical expression has 3184 elements: only the first used
2: In id:plate2 :
  numerical expression has 3184 elements: only the first used
3: In inherits(x, "factor") : NAs introduced by coercion

> data2$id2 <- factor(data2$id)
> fit1b <- lmer(il2 ~ (1|plate2/id2), data=data2)   
>  # Finally works


	Terry Therneau



From kmjamsen at unimelb.edu.au  Fri Sep 26 03:27:50 2008
From: kmjamsen at unimelb.edu.au (Kris Jamsen)
Date: Fri, 26 Sep 2008 11:27:50 +1000
Subject: [R-sig-ME] FW: GLMMs
Message-ID: <9C6F0C11872C98439322A20E9DB369F803450620@IS-EX-BEV2.unimelb.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080926/ba56a39e/attachment.pl>

From Wolfgang.Viechtbauer at STAT.unimaas.nl  Fri Sep 26 18:00:19 2008
From: Wolfgang.Viechtbauer at STAT.unimaas.nl (Viechtbauer Wolfgang (STAT))
Date: Fri, 26 Sep 2008 18:00:19 +0200
Subject: [R-sig-ME] GNLS Crash
Message-ID: <329A68716B57D54E8D39FD3F8A4A84DF0EB93F8E@um-mail0136.unimaas.nl>

Hi all,

I'm trying to fit a marginal (longitudinal) model with an exponential serial correlation function to the Orange tree data set. However, R crashes frequently when using the gnls() function. With the following simple example, I was able to reproduce the problem. 

gnls.exp <- gnls(circumference ~ Asym/(1 + exp(-(age-xmid)/scal)) ,
              data = Orange, correlation = corExp(form = ~1 | Tree),
              start = c(Asym=150, xmid=750, scal=300))

On my and one other computer, R usually crashes when calling the gnls() function with this code three times in a row. I stumbled across this when trying to fit this model with different starting values.

Version information:

platform       i386-pc-mingw32             
arch           i386                        
os             mingw32                     
system         i386, mingw32               
status                                     
major          2                           
minor          7.2                         
year           2008                        
month          08                          
day            25                          
svn rev        46428                       
language       R                           
version.string R version 2.7.2 (2008-08-25)

Can anybody help figure out what causes this problem?

Best,

-- 
Wolfgang Viechtbauer 
?Department of Methodology and Statistics 
?University of Maastricht, The Netherlands 
?http://www.wvbauer.com/ 



From rab at nauticom.net  Fri Sep 26 20:55:18 2008
From: rab at nauticom.net (Rick Bilonick)
Date: Fri, 26 Sep 2008 14:55:18 -0400
Subject: [R-sig-ME] Heirarchical Multivariate Modeling?
In-Reply-To: <416B8868-9306-4E22-AEF1-EFDB64C5E92E@kjbeath.com.au>
References: <Pine.LNX.4.64.0809181540390.10089@parser.ilovebacon.org>
	<416B8868-9306-4E22-AEF1-EFDB64C5E92E@kjbeath.com.au>
Message-ID: <1222455318.3337.32.camel@eei243.eei.upmc.edu>


On Tue, 2008-09-23 at 09:53 +1000, Ken Beath wrote:
> On 19/09/2008, at 8:41 AM, Adam D. I. Kramer wrote:
> 
> > Dear colleagues,
> >
> > 	I have an interest in what I would call "heirarchical multivariate
> > modeling." In a sense, I'm interested in extending the mixed model  
> > procedure
> > to an "unpredicted" multivariate case, or an analysis which would be  
> > an
> > extension to princomp() or prcomp() just as lmer() is an extension  
> > to lm().
> >
> > 	My actual interest is in 1. estimating an aggregate PCA based on the
> > factor structures that exist within many individuals, each of which  
> > is based
> > on a different number of observations among the same set of  
> > variables, and
> > 2. testing whether factor structures differ across people (e.g.,  
> > whether
> > prediction improves if I model a random effect for subject). This  
> > can be
> > thought of as adding and testing a random effect to a PCA, or  
> > something
> > similar.
> >
> > 	My first intuition of how to go about this would be to use the glmer
> > procedure, and attempt to model the entire set of variables as being
> > predicted by a random "intercept" for each subject, but before I  
> > undertake
> > this analysis, I thought it might be wise to see if anyone on this  
> > list had
> > any suggestions of a better way to go about this in R (or  
> > suggestions that
> > the above way is inappropriate).
> >
> > 	Also, if anybody could recommend an article or two on the topic (I
> > have not seen any), I would be quite interested.
> 
> It is possible to create multilevel versions of multivariate methods,  
> maybe not PCA, but for factor analysis, yes. The sem package could  
> probably be coerced into fitting them for linear models, otherwise the  
> commercial programs Latent Gold and MPlus are the only solutions. The  
> Mplus site has lots of modelling info.
> 
> Ken


Possibly you could use the Mx program (Neale, http://www.vcu.edu/mx/) to
create a structural equation model. Mx is very flexible and freely
available.

Rick B.



From b.glaser at bristol.ac.uk  Fri Sep 26 21:59:06 2008
From: b.glaser at bristol.ac.uk (Beate Glaser)
Date: Fri, 26 Sep 2008 20:59:06 +0100
Subject: [R-sig-ME] odds ratios from mixed logistic models
Message-ID: <126701062.1222462746@epi-pc32.ads.bris.ac.uk>

Dear lmer group,

I have been fitting a very simple mixed logistic regression with lmer
The outcome is educational impairment (edu.cat: 0,1) in students and the 
predictors are age and a base-line mental health measure (mh.s). I used a 
random intercept model, as none of my predictors, apart from age, is 
time-varying. From cross-sectional analysis I know that there is a 
significant change in the effect of base-line mental health on educational 
impairment across time, and this effect is included as a fixed-effect 
interaction term.
I wrote a little function to extract the OR for the mental health measure 
(increase in 1 point) given different ages of the student. I derived the SE 
of this OR as pooled SE using the SEs for the fixed effects "age" and "age 
: mental health" interaction, as well as their covariance. I am not sure 
however if this is too simple to obtain this SE, or if I need to do 
simulations (e.g. in WinBugs, unless someone knows a sneaky way to 
incorporate these combined ORs variables into the mcmcsampler within lmer) 
to obtain more accurate estimates. I would be very grateful for any 
comments and suggestions.

Kind regards,

Beate

This is the model with age centered at the mean (22.4 years):

>lmer.mh1 <- lmer(smfq.cat ~ age.c*( iq.8.s) + (0 + female|aln) + (0 + 
male|aln), data=psy.st.l, family=binomial(link=logit))

Generalized linear mixed model fit by the Laplace approximation
Formula: edu.cat ~ age.c * (mh.s) + (0 + female | aln) + (0 + male | aln)
   Data: psy.st.l
  AIC  BIC logLik deviance
 8399 8442  -4193     8387
Random effects:
 Groups Name   Variance Std.Dev.
 aln    female 3.49     1.87
 aln    male   2.18     1.48
Number of obs: 10149, groups: aln, 3438

Fixed effects:
             Estimate Std. Error z value Pr(>|z|)
(Intercept)   -0.9671     0.3113   -3.11   0.0019 **
age.c         -0.7776     0.1692   -4.60  4.3e-06 ***
mh.s          -0.1435     0.0292   -4.91  8.9e-07 ***
age.c:mh.s     0.0955     0.0161    5.93  3.1e-09 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
            (Intr) age.c  mh.s
age.c       -0.132
mh.s        -0.989  0.145
age.c:mh.s   0.143 -0.989 -0.159

#OR function to obtain ORs at different ages

orme<-function(agev,mod) {
  fe<-fixef(mod)

#Derive combined SE
  se.fe<-se.fixef(mod)
  se.mh<-se.fe[names(se.fe)=="mh.s"]
  se.ia<-se.fe[names(sefe)=="age.c:mh.s"]
  vcmat<-vcov(lmer.mh1)
  cov.mh.ia<-vcmat[4,3]
  se.or.ia<-sqrt(se.mh^2 + se.ia^2 + 2*cov.mh.ia)

#Derive OR + 95%CI
  mh.dta1<-0
  mh.dta2<-1
  or<-rep(NA,length(agev))
  ci<-matrix(nrow=2,ncol=length(agev))

for(i in 1:length(agev)) {
    dta1<-c(1,agev[i],mh.dta1,agev[i]*mh.dta1)
    dta2<-c(1,agev[i],mh.dta2,agev[i]*mh.dta2)
    logodds1<-dta1%*%fe
    logodds2<-dta2%*%fe
    or[i]<-exp(logodds2-logodds1)
    ci[1:2,i]<- or[i] + c(-1.96,1.96)*se.or.ia
    }
  return(list(or=or,ci=ci))
  }

#Mean age
print(age.m, digits=6)
22.3942

#OR for age20, age23, age24
  orme(c(-2.4,0.6,1.6),lmer.mh1)

$or
[1] 0.69 0.92 1.01

$ci
     [,1] [,2] [,3]
[1,] 0.63 0.86 0.95
[2,] 0.75 0.98 1.07



R version 2.6.2 (2008-02-08)
i386-pc-mingw32

locale:
LC_COLLATE=English_United Kingdom.1252;LC_CTYPE=English_United 
Kingdom.1252;LC_MONETARY=English_United 
Kingdom.1252;LC_NUMERIC=C;LC_TIME=English_United Kingdom.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
 [1] arm_1.1-8         R2WinBUGS_2.1-8   coda_0.13-2       lme4_0.999375-18 
Matrix_0.999375-9 xtable_1.5-2      car_1.2-8         mice_1.16
 [9] nnet_7.2-42       MASS_7.2-42       gdata_2.4.2       lattice_0.17-10 
conf.design_0.0-4 foreign_0.8-26

loaded via a namespace (and not attached):
[1] grid_2.6.2   gtools_2.4.0 tools_2.6.2


----------------------
Beate Glaser
ALSPAC
MRC CAiTE
Dept Social Medicine
Oakfield House
15-23 Oakfield Grove
Bristol BS8 2BN
UK

++44 117 33 10101



From bates at stat.wisc.edu  Fri Sep 26 23:56:42 2008
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 26 Sep 2008 16:56:42 -0500
Subject: [R-sig-ME] 3 bugs in lmer
In-Reply-To: <200809251751.m8PHpoa22591@hsrnfs-101.mayo.edu>
References: <200809251751.m8PHpoa22591@hsrnfs-101.mayo.edu>
Message-ID: <40e66e0b0809261456v5577fb7an41acf7487f250ee0@mail.gmail.com>

On Thu, Sep 25, 2008 at 12:51 PM, Terry Therneau <therneau at mayo.edu> wrote:
> These are of the better-user-interface variety; features that are common to all
> the other model functions that I know of, but seem to be missing in lmer.  One
> could argue that "shortfalls" would be a better label than "bugs".

>> version
>               _
> platform       sparc-sun-solaris2.10
> arch           sparc
> os             solaris2.10
> system         sparc, solaris2.10
> status
> major          2
> minor          7.1
> year           2008
> month          06
> day            23
> svn rev        45970
> language       R
> version.string R version 2.7.1 (2008-06-23)

> #1 - subsets
> fit1c <- lmer(il2 ~ (1|plate/id), data=allsets, subset=(il2>0) )
> fit1d <- lmer(il2 ~ (1|plate/id), data=allsets, subset= il2>0)

>  I always put subset arguments in parenthesis for readability.  (In the actual
> code that sparked this message, the subset expression was longer, and it helps
> more).  Both models fit ok, but the first one doesn't print.

Turns out that was a problem with the utility asOneSidedFormula in the
"stats" package.  I'm not sure why I used in the print method but the
problem is fixed in lme4_0.999375-27 which I will release over the
weekend if the tests run on R-forge don't show further problems.  At
least the problem is fixed in the examples that I concocted.  You did
not provide a reproducible example.

>> all.equal(coef(fit1c), coef(fit1d)
> [1] TRUE
>> print(fit1c)
> Linear mixed model fit by REML
> Formula: il2 ~ (1 | plate/id)
>   Data: allsets
> Error in sprintf(gettext(fmt, domain = domain), ...) :
>  object "x" not found
>

> #2 coefficients
>  Like everyone else, I almost never type the full "coefficients" method and use
> the "coef" abbreviation.  But I happened to do so today:

I am somewhat reluctant to add a coefficients method as a synonym for
coef because the coef method does not return what is typically meant
by "the coefficients" in a model based on a linear predictor.  The
fixef method returns the fixed effects estimates and the ranef method
returns the conditional modes of the random effects.  The current coef
method is a sort of a hodge podge that returns "subject-specific"
coefficients.  In a way I wish that I hadn't created that method
because, although those values may be meaningful for specific kinds of
mixed models, it is not clear what the result should be in general
models.  I would be more amenable to the suggestion that I remove the
coef method altogether than to the suggestion that I add
"coefficients" as a synonym. I don't feel that the coefficients <->
coef equivalence should hold in this case because the coef method
isn't what it means in other model forms.

>> coefficients(fit1d)
> Error in object$coefficients : $ operator not defined for this S4 class

> #3  character variables

>  I almost never, ever, turn a character variable into a factor.  We can argue
> about the wisdom of my choice another day, but you'll likely not sway me.
> Character variables work fine in model statements, being treated as factors; for
> models other than lmer.
>  Here is a fit on my original data set, before I modified it for the benefit of
>  lmer:

The description of the formula argument in the man page for lmer states:

  \item{formula}{a two-sided linear formula object describing the
    fixed-effects part of the model, with the response on the left of a
    \code{~} operator and the terms, separated by \code{+} operators, on
    the right.  The vertical bar character \code{"|"} separates an
    expression for a model matrix and a grouping factor.}

Notice how the description ends - the expression on the right hand
side of the vertical bar should be a factor.   I am having a little
difficulty responding in a collegial manner to a bug report of the
form "I don't like factors so you have to rewrite your code to handle
grouping factors that aren't factors."

It appears that the problem here is in the expansion of the / but
again I can't really tell because you didn't include a reproducible
example.
Do you encounter problems if you write the model as

lmer(il2 ~ (1|plate) + (1|plate:id), data=data2)

which is an equivalent specification?  If so then the underlying
problem is with the interpretation of the ':' operator.  Within a
model formula it should be interpreted as an interaction - i.e. the
definition that applies when the arguments are factors.  The error
messages you quote indicate that it is being used as the sequence
operator and that would come from numeric values, not character
values.

I suppose that I could, grudgingly, add code that would coerce the
expression on the right hand side of the | to a factor but I must
admit that I am very tempted instead to add code that would throw an
error with the error message

"The grouping factor must be a factor - and this means you too, Terry!"

Now that I have gotten that off my chest, :-) could you check if the
underlying problem is the ":" operator?


>> fit1b <- lmer(il2 ~ (1|plate/id), data=data2)
> Error in id:plate : NA/NaN argument
> In addition: Warning messages:
> 1: In id:plate :
>  numerical expression has 3184 elements: only the first used
> 2: In id:plate :
>  numerical expression has 3184 elements: only the first used
> 3: In inherits(x, "factor") : NAs introduced by coercion
>
> By the way, there are no odd or missing values in plate or id.
>
> Oops, I just realized the above is wrong: plate is an integer.  So try again:
>
>>  table(data2$plate)
>
>  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26
> 80 80 70 80 80 80 46 78 80 80 80 80 80 64 80 80 80 80 80 80 76 80 80 80 80 80
> 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41
> 80 70 80 80 80 80 80 80 80 80 64 80 80 80 80

The output from str(data2) would have been more informative.

>> fit1b <- lmer(il2 ~ (1|factor(plate)/id), data=data2)
> Error in inherits(x, "factor") : object "plate" not found

That one is a problem and it is a nontrivial problem related to what
Thomas Lumley has called "the standard, non-standard evaluation
model".  The call to model.frame in lmer is somehow not getting the
right environment in the formula under certain circumstances.  It is
very difficult to debug that one.

>> data2$plate2 <- factor(data2$plate)
>> fit1b <- lmer(il2 ~ (1|plate2/id), data=data2)
>
> Error in id:plate2 : NA/NaN argument
> In addition: Warning messages:
> 1: In id:plate2 :
>  numerical expression has 3184 elements: only the first used
> 2: In id:plate2 :
>  numerical expression has 3184 elements: only the first used
> 3: In inherits(x, "factor") : NAs introduced by coercion
>
>> data2$id2 <- factor(data2$id)
>> fit1b <- lmer(il2 ~ (1|plate2/id2), data=data2)
>>  # Finally works
>
>
>        Terry Therneau
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From maechler at stat.math.ethz.ch  Sat Sep 27 11:20:20 2008
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 27 Sep 2008 11:20:20 +0200
Subject: [R-sig-ME] seg.fault from nlme::gnls() {was " GNLS Crash"}
In-Reply-To: <329A68716B57D54E8D39FD3F8A4A84DF0EB93F8E@um-mail0136.unimaas.nl>
References: <329A68716B57D54E8D39FD3F8A4A84DF0EB93F8E@um-mail0136.unimaas.nl>
Message-ID: <18653.64212.996077.405539@cmath-5.math.ethz.ch>

>>>>> "VW" == Viechtbauer Wolfgang (STAT) <Wolfgang.Viechtbauer at STAT.unimaas.nl>
>>>>>     on Fri, 26 Sep 2008 18:00:19 +0200 writes:

    VW> Hi all, I'm trying to fit a marginal (longitudinal)
    VW> model with an exponential serial correlation function to
    VW> the Orange tree data set. However, R crashes frequently
    VW> when using the gnls() function. With the following
    VW> simple example, I was able to reproduce the problem.


    VW> gnls.exp <- gnls(circumference ~ Asym/(1 + exp(-(age-xmid)/scal)) ,
    VW> data = Orange, correlation = corExp(form = ~1 | Tree),
    VW> start = c(Asym=150, xmid=750, scal=300))

Yes, I can reproduce it (on a Linux (RHEL 5) 64bit server), using
R 2.8.0 alpha : Use this

for(i in 1:20)# for MM: ca. 8 to 10 times --> seg.fault
try(
gnls.exp <- gnls(circumference ~ Asym/(1 + exp(-(age-xmid)/scal)) ,
              data = Orange, correlation = corExp(form = ~1 | Tree),
              start = c(Asym=150, xmid=750, scal=300))
)

If I run it in the debugger and then do a backtrace
I see

Error in gnls(circumference ~ Asym/(1 + exp(-(age - xmid)/scal)), data = Orange,  : 
  Step halving factor reduced below minimum in NLS step

[............................................................]
[...................the same about 6 times...................]
[............................................................]

Error in gnls(circumference ~ Asym/(1 + exp(-(age - xmid)/scal)), data = Orange,  : 
  Step halving factor reduced below minimum in NLS step
Error in .makeMessage(..., domain = domain) : 
  incorrect number of arguments to "<-"
Error in .makeMessage(..., domain = domain) : 
  incorrect number of arguments to "<-"

Program received signal SIGABRT, Aborted.
0x0000003294e30155 in raise () from /lib64/libc.so.6
(gdb) bt
#0  0x0000003294e30155 in raise () from /lib64/libc.so.6
#1  0x0000003294e31bf0 in abort () from /lib64/libc.so.6
#2  0x00000000004184f5 in R_gc_internal (size_needed=0)
    at ../../../R/src/main/memory.c:761
#3  0x000000000041b121 in Rf_cons (car=0x1b2ec088, cdr=0x1a3436d8)
    at ../../../R/src/main/memory.c:1755
#4  0x0000000000578472 in applydefine (call=<value optimized out>, 
    op=0x1a3617a8, args=0x1ad47590, rho=0x1b2ea428)
    at ../../../R/src/include/Rinlinedfuns.h:153
#5  0x0000000000575a42 in Rf_eval (e=0x1ad47558, rho=0x1b2ea428)
    at ../../../R/src/main/eval.c:461
#6  0x0000000000576d50 in do_begin (call=0x1ad48450, op=0x1a362518, 
    args=0x1ad47520, rho=0x1b2ea428) at ../../../R/src/main/eval.c:1174
#7  0x0000000000575a42 in Rf_eval (e=0x1ad48450, rho=0x1b2ea428)
    at ../../../R/src/main/eval.c:461
#8  0x0000000000578f9c in Rf_applyClosure (call=0x1b2e9958, op=0x1ad48370, 
    arglist=0x1b2e9a70, rho=0x1b690298, suppliedenv=0x1b2e99c8)
    at ../../../R/src/main/eval.c:667
#9  0x00000000004285b9 in Rf_usemethod (generic=0x67e8d7 "[", 
    obj=<value optimized out>, call=<value optimized out>, 
    args=<value optimized out>, rho=0xffffffffffffffff, callrho=0x1b690298, 
    defrho=0x1a384f28, ans=0x7fffcaa8d5a8) at ../../../R/src/main/objects.c:311
#10 0x0000000000576a25 in Rf_DispatchOrEval (call=0x1a81d8d0, op=0x1a3622e8, 
    generic=0x67e8d7 "[", args=0x1a81d908, rho=0x1b690298, ans=0x7fffcaa8d5a8, 
    dropmissing=0, argsevald=0) at ../../../R/src/main/eval.c:1930
#11 0x00000000004afc40 in do_subset (call=0x7716, op=0x7716, 
    args=0xffffffffffffffff, rho=0x1b690298)
    at ../../../R/src/main/subset.c:577
#12 0x0000000000575a42 in Rf_eval (e=0x1a81d8d0, rho=0x1b690298)
    at ../../../R/src/main/eval.c:461
#13 0x00000000005780c7 in applydefine (call=0x1a81d748, op=0x1a3617a8, 
    args=0x6, rho=0xffffffffffffffff) at ../../../R/src/main/eval.c:1318
#14 0x0000000000575a42 in Rf_eval (e=0x1a81d748, rho=0x1b690298)
    at ../../../R/src/main/eval.c:461
#15 0x00000000005775de in do_for (call=0x1a81d668, op=0x1a360180, 
    args=0x1a81d6a0, rho=0x1b690298) at ../../../R/src/main/eval.c:1073

..... and more (downto #155 for R's main(...)

This is a bug,  probably in the C code of package nlme
which I guess corrupts memory somehere earlier, and we are
seeing the effect only later.
I'll leave this to the (memory-)debugging experts...

Martin

    VW> On my and one other computer, R usually crashes when calling the gnls() function with this code three times in a row. I stumbled across this when trying to fit this model with different starting values.

    VW> Version information:

    VW> platform       i386-pc-mingw32             
    ......
    VW> version.string R version 2.7.2 (2008-08-25)

    VW> Can anybody help figure out what causes this problem?

    VW> Best,

    VW> -- 
    VW> Wolfgang Viechtbauer 
    VW> ?Department of Methodology and Statistics 
    VW> ?University of Maastricht, The Netherlands 
    VW> ?http://www.wvbauer.com/ 

    VW> _______________________________________________
    VW> R-sig-mixed-models at r-project.org mailing list
    VW> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From bolker at zoology.ufl.edu  Mon Sep 29 21:01:20 2008
From: bolker at zoology.ufl.edu (Ben Bolker)
Date: Mon, 29 Sep 2008 15:01:20 -0400
Subject: [R-sig-ME] [Fwd: testing quasi-likelihood in glmer with simulations]
Message-ID: <48E12600.3030401@zoology.ufl.edu>



  I spent a bit of time this weekend running simulations
of quasi-likelihood estimation in glmer. The results
are posted at

http://www.zoo.ufl.edu/bolker/glmm/quasitest.pdf
http://www.zoo.ufl.edu/bolker/glmm/quasitest.Rnw

  I confirmed my suspicion that glmer is scaling the
random-effects variance-covariance matrix by the
scale parameter when it shouldn't; I also discovered
that the quasi-likelihood fit to lognormal-Poisson-distributed
data is alarmingly (?) biased, although a lognormal-Poisson
fit with individual-level random effects works nicely.

  Any comments are welcome.

  cheers
   Ben Bolker



From wdunlap at tibco.com  Mon Sep 29 21:03:11 2008
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 29 Sep 2008 12:03:11 -0700
Subject: [R-sig-ME] [Rd] seg.fault from nlme::gnls() {was " GNLS Crash"}
In-Reply-To: <18653.64212.996077.405539@cmath-5.math.ethz.ch>
References: <329A68716B57D54E8D39FD3F8A4A84DF0EB93F8E@um-mail0136.unimaas.nl>
	<18653.64212.996077.405539@cmath-5.math.ethz.ch>
Message-ID: <FBFBD35F5823FD4E97ACDEBB353DD9C608D79D@NA-PA-VBE01.na.tibco.com>

 When I run this under valgrind with gctorture(TRUE) I see memory
misuse in mat_mult (matrix.c, line 82):
% R-2.8.0-dev --debugger=valgrind --debugger-args=--db-attach=yes
... copyright blah blah ...
> data(Orange)
> gctorture(TRUE)
> gnls(circumference ~ Asym/(1 + exp(-(age-xmid)/scal)) ,
+ data = Orange, correlation = corExp(form = ~1 | Tree),
+ start = c(Asym=150, xmid=750, scal=300))
r==5894== Invalid read of size 4
==5894==    at 0x580268F: mult_mat (matrix.c:82)
==5894==    by 0x57FF1B0: corStruct_recalc (corStruct.c:70)
==5894==    by 0x5801F89: gnls_objective (gnls.c:100)
==5894==    by 0x58020FE: fit_gnls (gnls.c:152)
==5894==    by 0x81439A2: do_dotCode (dotcode.c:1758)
==5894==    by 0x8161C56: Rf_eval (eval.c:487)
==5894==    by 0x81637D5: do_set (eval.c:1422)
==5894==    by 0x8161B3B: Rf_eval (eval.c:461)
==5894==    by 0x816384B: do_begin (eval.c:1174)
==5894==    by 0x8161B3B: Rf_eval (eval.c:461)
==5894==    by 0x816397B: do_repeat (eval.c:1137)
==5894==    by 0x8161B3B: Rf_eval (eval.c:461)
==5894==  Address 0x4E4A8CC is 12 bytes inside a block of size 864 free'd
==5894==    at 0x40052A3: free (vg_replace_malloc.c:233)
==5894==    by 0x805ACFD: R_gc_internal (memory.c:767)
==5894==    by 0x805B933: Rf_cons (memory.c:1755)
==5894==    by 0x805B98E: Rf_allocList (memory.c:2094)
==5894==    by 0x815AB4C: replaceCall (Rinlinedfuns.h:84)
==5894==    by 0x8163476: applydefine (eval.c:919)
==5894==    by 0x8161B3B: Rf_eval (eval.c:461)
==5894==    by 0x816384B: do_begin (eval.c:1174)
==5894==    by 0x8161B3B: Rf_eval (eval.c:461)
==5894==    by 0x8161B3B: Rf_eval (eval.c:461)
==5894==    by 0x816384B: do_begin (eval.c:1174)
==5894==    by 0x8161B3B: Rf_eval (eval.c:461)

My copy of valgrind doesn't get along with my copy of gdb,
so when it pops into gdb the debugger doesn't get any
information from *.so files.  I don't have the time now
to put printf statements into mat_mult.

Bill Dunlap
wdunlap tibco.com 

-----Original Message-----
From: r-devel-bounces at r-project.org [mailto:r-devel-bounces at r-project.org] On Behalf Of Martin Maechler
Sent: Saturday, September 27, 2008 2:20 AM
To: Viechtbauer Wolfgang (STAT)
Cc: r-sig-mixed-models at r-project.org; R-devel at stat.math.ethz.ch
Subject: [Rd] seg.fault from nlme::gnls() {was "[R-sig-ME] GNLS Crash"}

>>>>> "VW" == Viechtbauer Wolfgang (STAT) <Wolfgang.Viechtbauer at STAT.unimaas.nl>
>>>>>     on Fri, 26 Sep 2008 18:00:19 +0200 writes:

    VW> Hi all, I'm trying to fit a marginal (longitudinal)
    VW> model with an exponential serial correlation function to
    VW> the Orange tree data set. However, R crashes frequently
    VW> when using the gnls() function. With the following
    VW> simple example, I was able to reproduce the problem.


    VW> gnls.exp <- gnls(circumference ~ Asym/(1 + exp(-(age-xmid)/scal)) ,
    VW> data = Orange, correlation = corExp(form = ~1 | Tree),
    VW> start = c(Asym=150, xmid=750, scal=300))

Yes, I can reproduce it (on a Linux (RHEL 5) 64bit server), using R 2.8.0 alpha : Use this

for(i in 1:20)# for MM: ca. 8 to 10 times --> seg.fault try( gnls.exp <- gnls(circumference ~ Asym/(1 + exp(-(age-xmid)/scal)) ,
              data = Orange, correlation = corExp(form = ~1 | Tree),
              start = c(Asym=150, xmid=750, scal=300))
)

If I run it in the debugger and then do a backtrace I see

Error in gnls(circumference ~ Asym/(1 + exp(-(age - xmid)/scal)), data = Orange,  : 
  Step halving factor reduced below minimum in NLS step

[............................................................]
[...................the same about 6 times...................] [............................................................]

Error in gnls(circumference ~ Asym/(1 + exp(-(age - xmid)/scal)), data = Orange,  : 
  Step halving factor reduced below minimum in NLS step Error in .makeMessage(..., domain = domain) : 
  incorrect number of arguments to "<-"
Error in .makeMessage(..., domain = domain) : 
  incorrect number of arguments to "<-"

Program received signal SIGABRT, Aborted.
0x0000003294e30155 in raise () from /lib64/libc.so.6
(gdb) bt
#0  0x0000003294e30155 in raise () from /lib64/libc.so.6
#1  0x0000003294e31bf0 in abort () from /lib64/libc.so.6
#2  0x00000000004184f5 in R_gc_internal (size_needed=0)
    at ../../../R/src/main/memory.c:761
#3  0x000000000041b121 in Rf_cons (car=0x1b2ec088, cdr=0x1a3436d8)
    at ../../../R/src/main/memory.c:1755
#4  0x0000000000578472 in applydefine (call=<value optimized out>, 
    op=0x1a3617a8, args=0x1ad47590, rho=0x1b2ea428)
    at ../../../R/src/include/Rinlinedfuns.h:153
#5  0x0000000000575a42 in Rf_eval (e=0x1ad47558, rho=0x1b2ea428)
    at ../../../R/src/main/eval.c:461
#6  0x0000000000576d50 in do_begin (call=0x1ad48450, op=0x1a362518, 
    args=0x1ad47520, rho=0x1b2ea428) at ../../../R/src/main/eval.c:1174
#7  0x0000000000575a42 in Rf_eval (e=0x1ad48450, rho=0x1b2ea428)
    at ../../../R/src/main/eval.c:461
#8  0x0000000000578f9c in Rf_applyClosure (call=0x1b2e9958, op=0x1ad48370, 
    arglist=0x1b2e9a70, rho=0x1b690298, suppliedenv=0x1b2e99c8)
    at ../../../R/src/main/eval.c:667
#9  0x00000000004285b9 in Rf_usemethod (generic=0x67e8d7 "[", 
    obj=<value optimized out>, call=<value optimized out>, 
    args=<value optimized out>, rho=0xffffffffffffffff, callrho=0x1b690298, 
    defrho=0x1a384f28, ans=0x7fffcaa8d5a8) at ../../../R/src/main/objects.c:311 #10 0x0000000000576a25 in Rf_DispatchOrEval (call=0x1a81d8d0, op=0x1a3622e8, 
    generic=0x67e8d7 "[", args=0x1a81d908, rho=0x1b690298, ans=0x7fffcaa8d5a8, 
    dropmissing=0, argsevald=0) at ../../../R/src/main/eval.c:1930
#11 0x00000000004afc40 in do_subset (call=0x7716, op=0x7716, 
    args=0xffffffffffffffff, rho=0x1b690298)
    at ../../../R/src/main/subset.c:577
#12 0x0000000000575a42 in Rf_eval (e=0x1a81d8d0, rho=0x1b690298)
    at ../../../R/src/main/eval.c:461
#13 0x00000000005780c7 in applydefine (call=0x1a81d748, op=0x1a3617a8, 
    args=0x6, rho=0xffffffffffffffff) at ../../../R/src/main/eval.c:1318
#14 0x0000000000575a42 in Rf_eval (e=0x1a81d748, rho=0x1b690298)
    at ../../../R/src/main/eval.c:461
#15 0x00000000005775de in do_for (call=0x1a81d668, op=0x1a360180, 
    args=0x1a81d6a0, rho=0x1b690298) at ../../../R/src/main/eval.c:1073

..... and more (downto #155 for R's main(...)

This is a bug,  probably in the C code of package nlme which I guess corrupts memory somehere earlier, and we are seeing the effect only later.
I'll leave this to the (memory-)debugging experts...

Martin

    VW> On my and one other computer, R usually crashes when calling the gnls() function with this code three times in a row. I stumbled across this when trying to fit this model with different starting values.

    VW> Version information:

    VW> platform       i386-pc-mingw32             
    ......
    VW> version.string R version 2.7.2 (2008-08-25)

    VW> Can anybody help figure out what causes this problem?

    VW> Best,

    VW> -- 
    VW> Wolfgang Viechtbauer 
    VW> ?Department of Methodology and Statistics 
    VW> ?University of Maastricht, The Netherlands 
    VW> ?http://www.wvbauer.com/ 

    VW> _______________________________________________
    VW> R-sig-mixed-models at r-project.org mailing list
    VW> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

______________________________________________
R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel



From L.teMarvelde at nioo.knaw.nl  Tue Sep 30 11:28:25 2008
From: L.teMarvelde at nioo.knaw.nl (Marvelde, Luc te)
Date: Tue, 30 Sep 2008 11:28:25 +0200
Subject: [R-sig-ME] Wrong degrees of freedom in nested model,
	what goes wrong here?
Message-ID: <1276C0564833F043AB4C85ED8DA9CFE501B74DBD@ctemail1.nioo.int>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20080930/cb90f44a/attachment.pl>

