From c@nott|ngh@m @end|ng |rom @uck|@nd@@c@nz  Wed Jul  1 03:22:03 2020
From: c@nott|ngh@m @end|ng |rom @uck|@nd@@c@nz (Christopher Nottingham)
Date: Wed, 1 Jul 2020 01:22:03 +0000
Subject: [R-sig-ME] Incorrect output from nested model with mapped pars
In-Reply-To: <71c307e350b349c1b7f25ce79ac3704f@uxcn13-tdc-b.UoA.auckland.ac.nz>
References: <71c307e350b349c1b7f25ce79ac3704f@uxcn13-tdc-b.UoA.auckland.ac.nz>
Message-ID: <7332431185f5494b95ff14f5a27d509f@uxcn13-tdc-b.UoA.auckland.ac.nz>

One more question. Why is the summary function giving the z value and associated p-value. A gaussian error structure is assumed, so shouldn't t values be used to obtain p values.

Thanks,
Chris

From: Christopher Nottingham
Sent: Tuesday, 30 June 2020 5:06 PM
To: 'r-sig-mixed-models at r-project.org' <r-sig-mixed-models at r-project.org>
Subject: Incorrect output from nested model with mapped pars

I have a dataset with a variable labelled n_comm that is not relevant to some factor level combinations. I am fitting a nested model to this data and fixing betas representing the irrelevant factor combinations to 0 using map. As shown following, the model output from the summary table does not match what should be produced.

> map_names = list(beta = factor(c(1:6, NA, 8)))
> fit = glmmTMB(log(Err) ~ model + n_surv + species + n_comm:geostatistical + intensity,
+               data = Bhat_all.df,
+               start = list(beta = ifelse(is.na(map_names$beta), 0, 1)),
+               map = map_names)
> summary(fit)
Family: gaussian  ( identity )
Formula:          log(Err) ~ model + n_surv + species + n_comm:geostatistical +      intensity
Data: Bhat_all.df

     AIC      BIC   logLik deviance df.resid
 18340.8  18394.7  -9162.4  18324.8     6212


Dispersion estimate for gaussian family (sigma^2): 1.11

Conditional model:
                                    Estimate Std. Error z value Pr(>|z|)
(Intercept)                        9.329e+00  5.461e-02  170.82   <2e-16 ***
modelbiomass-dynamics             -4.082e+00  5.089e-02  -80.22   <2e-16 ***
modelsize-structured              -4.092e+00  5.056e-02  -80.93   <2e-16 ***
n_surv                            -8.895e-04  8.146e-05  -10.92   <2e-16 ***
species$\\mathit{S. aequilatera}$  5.596e-01  2.677e-02   20.90   <2e-16 ***
intensityFishing intensity: high   3.589e-01  2.681e-02   13.39   <2e-16 ***
n_comm:geostatisticalFALSE         0.000e+00  7.450e-05    0.00    1.000
n_comm:geostatisticalTRUE         -3.245e-04  5.461e-02   -0.01    0.995
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Warning message:
In cbind(Estimate = coefs, `Std. Error` = sqrt(diag(vcov))) :
  number of rows of result is not a multiple of vector length (arg 2)
> rbind(sqrt(diag(solve(fit$obj$he()))))
           [,1]       [,2]       [,3]         [,4]       [,5]       [,6]         [,7]       [,8]
[1,] 0.05458617 0.05086506 0.05053495 8.142355e-05 0.02675588 0.02679859 7.446289e-05 0.01792267
> fit$sdr
sdreport(.) result
           Estimate   Std. Error
beta   9.3291879616 5.461347e-02
beta  -4.0822725635 5.089050e-02
beta  -4.0920631052 5.056022e-02
beta  -0.0008895195 8.146427e-05
beta   0.5595933469 2.676926e-02
beta   0.3589326271 2.681199e-02
beta  -0.0003244617 7.450013e-05
betad  0.1082286532 1.793163e-02
Maximum gradient component: 0.001913387

The output below is wrong (there should be no std err, on a mapped value and the other values are incorrect.),
n_comm:geostatisticalTRUE         -3.245e-04  5.461e-02   -0.01    0.995

The dataset is attached as a rds for reproducibility.

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Fri Jul  3 21:41:28 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Fri, 3 Jul 2020 15:41:28 -0400
Subject: [R-sig-ME] Incorrect output from nested model with mapped pars
In-Reply-To: <7332431185f5494b95ff14f5a27d509f@uxcn13-tdc-b.UoA.auckland.ac.nz>
References: <71c307e350b349c1b7f25ce79ac3704f@uxcn13-tdc-b.UoA.auckland.ac.nz>
 <7332431185f5494b95ff14f5a27d509f@uxcn13-tdc-b.UoA.auckland.ac.nz>
Message-ID: <a1e47bbc-e8ae-c8e1-b09e-000e209ac1e6@gmail.com>

 ?? Your first question looks like it could possibly be a bug, so please 
post it on the glmmTMB github issues list (ideally with a reproducible 
example!)

 ?? For your second question: using a t distribution implies knowing the 
appropriate residual df, which is very difficult (see the GLMM FAQ or 
any of the many documents floating around on the web for why the 
sampling distributions of parameters from complex models are only 
approximately t-distributed anyway, and why it is so hard to find good 
approximations ...)

On 6/30/20 9:22 PM, Christopher Nottingham wrote:
> One more question. Why is the summary function giving the z value and associated p-value. A gaussian error structure is assumed, so shouldn't t values be used to obtain p values.
>
> Thanks,
> Chris
>
> From: Christopher Nottingham
> Sent: Tuesday, 30 June 2020 5:06 PM
> To: 'r-sig-mixed-models at r-project.org' <r-sig-mixed-models at r-project.org>
> Subject: Incorrect output from nested model with mapped pars
>
> I have a dataset with a variable labelled n_comm that is not relevant to some factor level combinations. I am fitting a nested model to this data and fixing betas representing the irrelevant factor combinations to 0 using map. As shown following, the model output from the summary table does not match what should be produced.
>
>> map_names = list(beta = factor(c(1:6, NA, 8)))
>> fit = glmmTMB(log(Err) ~ model + n_surv + species + n_comm:geostatistical + intensity,
> +               data = Bhat_all.df,
> +               start = list(beta = ifelse(is.na(map_names$beta), 0, 1)),
> +               map = map_names)
>> summary(fit)
> Family: gaussian  ( identity )
> Formula:          log(Err) ~ model + n_surv + species + n_comm:geostatistical +      intensity
> Data: Bhat_all.df
>
>       AIC      BIC   logLik deviance df.resid
>   18340.8  18394.7  -9162.4  18324.8     6212
>
>
> Dispersion estimate for gaussian family (sigma^2): 1.11
>
> Conditional model:
>                                      Estimate Std. Error z value Pr(>|z|)
> (Intercept)                        9.329e+00  5.461e-02  170.82   <2e-16 ***
> modelbiomass-dynamics             -4.082e+00  5.089e-02  -80.22   <2e-16 ***
> modelsize-structured              -4.092e+00  5.056e-02  -80.93   <2e-16 ***
> n_surv                            -8.895e-04  8.146e-05  -10.92   <2e-16 ***
> species$\\mathit{S. aequilatera}$  5.596e-01  2.677e-02   20.90   <2e-16 ***
> intensityFishing intensity: high   3.589e-01  2.681e-02   13.39   <2e-16 ***
> n_comm:geostatisticalFALSE         0.000e+00  7.450e-05    0.00    1.000
> n_comm:geostatisticalTRUE         -3.245e-04  5.461e-02   -0.01    0.995
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> Warning message:
> In cbind(Estimate = coefs, `Std. Error` = sqrt(diag(vcov))) :
>    number of rows of result is not a multiple of vector length (arg 2)
>> rbind(sqrt(diag(solve(fit$obj$he()))))
>             [,1]       [,2]       [,3]         [,4]       [,5]       [,6]         [,7]       [,8]
> [1,] 0.05458617 0.05086506 0.05053495 8.142355e-05 0.02675588 0.02679859 7.446289e-05 0.01792267
>> fit$sdr
> sdreport(.) result
>             Estimate   Std. Error
> beta   9.3291879616 5.461347e-02
> beta  -4.0822725635 5.089050e-02
> beta  -4.0920631052 5.056022e-02
> beta  -0.0008895195 8.146427e-05
> beta   0.5595933469 2.676926e-02
> beta   0.3589326271 2.681199e-02
> beta  -0.0003244617 7.450013e-05
> betad  0.1082286532 1.793163e-02
> Maximum gradient component: 0.001913387
>
> The output below is wrong (there should be no std err, on a mapped value and the other values are incorrect.),
> n_comm:geostatisticalTRUE         -3.245e-04  5.461e-02   -0.01    0.995
>
> The dataset is attached as a rds for reproducibility.
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From h@nzh @end|ng |rom um|ch@edu  Sat Jul  4 00:27:12 2020
From: h@nzh @end|ng |rom um|ch@edu (Han Zhang)
Date: Fri, 3 Jul 2020 18:27:12 -0400
Subject: [R-sig-ME] Minimum detectable effect size in linear mixed model
Message-ID: <CAH-zeEF_ZOaVsAx21yWq3PjhV3=-odZ61Vkkffg541etCyEiVg@mail.gmail.com>

Hello,

I'm trying to find the minimum detectable effect size (MDES) given my
sample, alpha (.05), and desired power (90%) in a linear mixed model
setting. I'm using the simr package for a simulation-based approach. What I
did is changing the original effect size to a series of hypothetical effect
sizes and find the minimum effect size that has a 90% chance of producing a
significant result. Below is a toy code:

library(lmerTest)
library(simr)

# fit the model
model <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
summary(model)

Fixed effects:
            Estimate Std. Error      df t value Pr(>|t|)
(Intercept)  251.405      6.825  17.000  36.838  < 2e-16 ***
Days          10.467      1.546  17.000   6.771 3.26e-06 ***


Here is the code for minimum detectable effect size:

pwr <- NA

# define a set of reasonable effect sizes
es <- seq(0, 10, 2)

# loop through the effect sizes
for (i in 1:length(es)) {
  # replace the original effect size with new one
  fixef(model)['Days'] =  es[i]
  # run simulation to obtain power estimate
  pwr.summary <- summary(powerSim(
    model,
    test = fixed('Days', "t"),
    nsim = 100,
    progress = T
  ))
  # store output
  pwr[i] <- as.numeric(pwr.summary)[3]
}

# display results
cbind("Coefficient" = es,
      Power = pwr)

Output:

                           Coefficient   Power
[1,]                                     0  0.09
[2,]                                     2  0.24
[3,]                                     4  0.60
[4,]                                     6  0.99
[5,]                                     8  1.00
[6,]                                    10  1.00

My questions:

(1) Is this the right way to find the MDES?

(2) I have some trouble making sense of the output. Can I say the
following: because the estimated power when the effect = 6 is 99%, and
because the actual model has an estimate of 10.47, then the study is
sufficiently powered? Conversely, imagine that if the actual estimate was
3.0, then can I say the study is insufficiently powered?

Thank you,
Han
-- 
Han Zhang, Ph.D.
Department of Psychology
University of Michigan, Ann Arbor
https://sites.lsa.umich.edu/hanzh/

	[[alternative HTML version deleted]]


From m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com  Sat Jul  4 00:33:54 2020
From: m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com (Patrick (Malone Quantitative))
Date: Fri, 3 Jul 2020 18:33:54 -0400
Subject: [R-sig-ME] Minimum detectable effect size in linear mixed model
In-Reply-To: <CAH-zeEF_ZOaVsAx21yWq3PjhV3=-odZ61Vkkffg541etCyEiVg@mail.gmail.com>
References: <CAH-zeEF_ZOaVsAx21yWq3PjhV3=-odZ61Vkkffg541etCyEiVg@mail.gmail.com>
Message-ID: <CAJc=yOEge21JObzfBKL4Tb7tu-Tbe0g-BNKtRCVPuod7=ZMveA@mail.gmail.com>

Han,

(1) Usually, yes, but . . .

(2) If you have an actual effect, does that mean you're doing post hoc
power analysis? If so, that's a whole can of worms, for which the best
advice I have is "don't do it." Use the size of the confidence
interval of your estimate as an assessment of sample adequacy.

Pat

On Fri, Jul 3, 2020 at 6:27 PM Han Zhang <hanzh at umich.edu> wrote:
>
> Hello,
>
> I'm trying to find the minimum detectable effect size (MDES) given my
> sample, alpha (.05), and desired power (90%) in a linear mixed model
> setting. I'm using the simr package for a simulation-based approach. What I
> did is changing the original effect size to a series of hypothetical effect
> sizes and find the minimum effect size that has a 90% chance of producing a
> significant result. Below is a toy code:
>
> library(lmerTest)
> library(simr)
>
> # fit the model
> model <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
> summary(model)
>
> Fixed effects:
>             Estimate Std. Error      df t value Pr(>|t|)
> (Intercept)  251.405      6.825  17.000  36.838  < 2e-16 ***
> Days          10.467      1.546  17.000   6.771 3.26e-06 ***
>
>
> Here is the code for minimum detectable effect size:
>
> pwr <- NA
>
> # define a set of reasonable effect sizes
> es <- seq(0, 10, 2)
>
> # loop through the effect sizes
> for (i in 1:length(es)) {
>   # replace the original effect size with new one
>   fixef(model)['Days'] =  es[i]
>   # run simulation to obtain power estimate
>   pwr.summary <- summary(powerSim(
>     model,
>     test = fixed('Days', "t"),
>     nsim = 100,
>     progress = T
>   ))
>   # store output
>   pwr[i] <- as.numeric(pwr.summary)[3]
> }
>
> # display results
> cbind("Coefficient" = es,
>       Power = pwr)
>
> Output:
>
>                            Coefficient   Power
> [1,]                                     0  0.09
> [2,]                                     2  0.24
> [3,]                                     4  0.60
> [4,]                                     6  0.99
> [5,]                                     8  1.00
> [6,]                                    10  1.00
>
> My questions:
>
> (1) Is this the right way to find the MDES?
>
> (2) I have some trouble making sense of the output. Can I say the
> following: because the estimated power when the effect = 6 is 99%, and
> because the actual model has an estimate of 10.47, then the study is
> sufficiently powered? Conversely, imagine that if the actual estimate was
> 3.0, then can I say the study is insufficiently powered?
>
> Thank you,
> Han
> --
> Han Zhang, Ph.D.
> Department of Psychology
> University of Michigan, Ann Arbor
> https://sites.lsa.umich.edu/hanzh/
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



-- 
Patrick S. Malone, Ph.D., Malone Quantitative
NEW Service Models: http://malonequantitative.com

He/Him/His


From m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com  Sat Jul  4 01:03:29 2020
From: m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com (Patrick (Malone Quantitative))
Date: Fri, 3 Jul 2020 19:03:29 -0400
Subject: [R-sig-ME] Minimum detectable effect size in linear mixed model
In-Reply-To: <CAH-zeEH31ebhJM9OUDKZV+BRHDFXCPC8aRqrudqg8zm1SsQeCg@mail.gmail.com>
References: <CAH-zeEF_ZOaVsAx21yWq3PjhV3=-odZ61Vkkffg541etCyEiVg@mail.gmail.com>
 <CAJc=yOEge21JObzfBKL4Tb7tu-Tbe0g-BNKtRCVPuod7=ZMveA@mail.gmail.com>
 <CAH-zeEH31ebhJM9OUDKZV+BRHDFXCPC8aRqrudqg8zm1SsQeCg@mail.gmail.com>
Message-ID: <CAJc=yOFSaEmuixj1aBphf-=tJXYA=VMzD84r2bY1wHYTff9ASg@mail.gmail.com>

No, because I don't think it can be. That's not how power analysis works.
It's bad practice.

On Fri, Jul 3, 2020, 6:42 PM Han Zhang <hanzh at umich.edu> wrote:

> Hi Pat,
>
> Thanks for your quick reply. Yes, I already have the data and the actual
> effects, and the analysis was suggested by a reviewer. Can you elaborate on
> when do you think such an analysis might be justified?
>
> Thanks!
> Han
>
> On Fri, Jul 3, 2020 at 6:34 PM Patrick (Malone Quantitative) <
> malone at malonequantitative.com> wrote:
>
>> Han,
>>
>> (1) Usually, yes, but . . .
>>
>> (2) If you have an actual effect, does that mean you're doing post hoc
>> power analysis? If so, that's a whole can of worms, for which the best
>> advice I have is "don't do it." Use the size of the confidence
>> interval of your estimate as an assessment of sample adequacy.
>>
>> Pat
>>
>> On Fri, Jul 3, 2020 at 6:27 PM Han Zhang <hanzh at umich.edu> wrote:
>> >
>> > Hello,
>> >
>> > I'm trying to find the minimum detectable effect size (MDES) given my
>> > sample, alpha (.05), and desired power (90%) in a linear mixed model
>> > setting. I'm using the simr package for a simulation-based approach.
>> What I
>> > did is changing the original effect size to a series of hypothetical
>> effect
>> > sizes and find the minimum effect size that has a 90% chance of
>> producing a
>> > significant result. Below is a toy code:
>> >
>> > library(lmerTest)
>> > library(simr)
>> >
>> > # fit the model
>> > model <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
>> > summary(model)
>> >
>> > Fixed effects:
>> >             Estimate Std. Error      df t value Pr(>|t|)
>> > (Intercept)  251.405      6.825  17.000  36.838  < 2e-16 ***
>> > Days          10.467      1.546  17.000   6.771 3.26e-06 ***
>> >
>> >
>> > Here is the code for minimum detectable effect size:
>> >
>> > pwr <- NA
>> >
>> > # define a set of reasonable effect sizes
>> > es <- seq(0, 10, 2)
>> >
>> > # loop through the effect sizes
>> > for (i in 1:length(es)) {
>> >   # replace the original effect size with new one
>> >   fixef(model)['Days'] =  es[i]
>> >   # run simulation to obtain power estimate
>> >   pwr.summary <- summary(powerSim(
>> >     model,
>> >     test = fixed('Days', "t"),
>> >     nsim = 100,
>> >     progress = T
>> >   ))
>> >   # store output
>> >   pwr[i] <- as.numeric(pwr.summary)[3]
>> > }
>> >
>> > # display results
>> > cbind("Coefficient" = es,
>> >       Power = pwr)
>> >
>> > Output:
>> >
>> >                            Coefficient   Power
>> > [1,]                                     0  0.09
>> > [2,]                                     2  0.24
>> > [3,]                                     4  0.60
>> > [4,]                                     6  0.99
>> > [5,]                                     8  1.00
>> > [6,]                                    10  1.00
>> >
>> > My questions:
>> >
>> > (1) Is this the right way to find the MDES?
>> >
>> > (2) I have some trouble making sense of the output. Can I say the
>> > following: because the estimated power when the effect = 6 is 99%, and
>> > because the actual model has an estimate of 10.47, then the study is
>> > sufficiently powered? Conversely, imagine that if the actual estimate
>> was
>> > 3.0, then can I say the study is insufficiently powered?
>> >
>> > Thank you,
>> > Han
>> > --
>> > Han Zhang, Ph.D.
>> > Department of Psychology
>> > University of Michigan, Ann Arbor
>> > https://sites.lsa.umich.edu/hanzh/
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>>
>> --
>> Patrick S. Malone, Ph.D., Malone Quantitative
>> NEW Service Models: http://malonequantitative.com
>>
>> He/Him/His
>>
>
>
> --
> Han Zhang, Ph.D.
> Department of Psychology
> University of Michigan, Ann Arbor
> https://sites.lsa.umich.edu/hanzh/
>

	[[alternative HTML version deleted]]


From Tom_Ph|||pp| @end|ng |rom np@@gov  Sat Jul  4 01:20:12 2020
From: Tom_Ph|||pp| @end|ng |rom np@@gov (Philippi, Tom)
Date: Fri, 3 Jul 2020 23:20:12 +0000
Subject: [R-sig-ME] PCA-GLMM
Message-ID: <SA9PR09MB5357BC1A7EAF484C7D443EA7F36A0@SA9PR09MB5357.namprd09.prod.outlook.com>

Saad--

Poisson and Negative Binomial distributions are defined for counts of whole numbers.  Even if the original dependent variables were counts, by definition PCA creates continuous numeric variates.  Therefore, Poisson or Negative Binomial are not appropriate for variates from PCA.

I suggest that you shift this over to R-sig-ecology, and give background on your experimental or sampling design, and questions of interest.  At least for now, this is not a question about Mixed Effects, although once the design is specified, it may require a mixed effects analysis.

Tom Philippi
Inventory and Monitoring Program
National Park Service
---
"To do science is to search for repeated patterns, not simply to accumulate facts..."   --Robert MacArthur 1972, Geographical Ecology
"Statistical methods of analysis are intended to aid the interpretation of data that are subject to appreciable haphazard variability"    --Cox & Hinkley 1974; Theoretical Statistics
Respect science, respect nature, respect each other.

-----Original Message-----
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of Sa?d HANANE
Sent: Tuesday, June 30, 2020 1:08 PM
To: r-sig-mixed-models at r-project.org
Subject: [EXTERNAL] [R-sig-ME] PCA-GLMM



 This email has been received from outside of DOI - Use caution before clicking on links, opening attachments, or responding.



Hi,
I have performed a PCA analysis which highlighted two axes PC1 and PC2. I want to perform a GLMM analysis considering the PC1 axis as a dependent variable (variable to explain) with Poisson or negative binomial family, but it was not possible since the values of this axis are negative. In such a case what should I do?
I also need to perform glmmPQL to consider spatial autocorrelation. LME could be a solution (instead of GLMM)? If yes, how could I perform glmmPQL?
Thank you for your precious help.
Regards.

--
Sa?d Hanane, PhD
Service d'Ecologie, de Biodiversit? et de Conservation des Sols Centre de Recherche Foresti?re Chariae Omar Ibn Al Khattab, BP 763, Rabat-Agdal/Maroc.




[image: Mailtrack]
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
Sender
notified by
Mailtrack
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
30/06/20
? 21:07:24

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From v@r|n@@ch@ @end|ng |rom y@hoo@|r  Sat Jul  4 18:07:12 2020
From: v@r|n@@ch@ @end|ng |rom y@hoo@|r (varin sacha)
Date: Sat, 4 Jul 2020 18:07:12 +0200
Subject: [R-sig-ME] Minimum detectable effect size in linear mixed model
In-Reply-To: <CAJc=yOFSaEmuixj1aBphf-=tJXYA=VMzD84r2bY1wHYTff9ASg@mail.gmail.com>
References: <CAJc=yOFSaEmuixj1aBphf-=tJXYA=VMzD84r2bY1wHYTff9ASg@mail.gmail.com>
Message-ID: <E2F5114C-84F5-4E13-96E0-0973141D118A@yahoo.fr>

Hi,

Is the question about post hoc power analysis ?

Post hoc power analyses are usually not suggested. (See for example The abuse of power...hoenig & heisey).  
You should do an a priori power analysis.  If you then do the small sample study and obtain a negative result, you have no idea why ? you are stuck.
 
That is why I always tell people not to do a study where everything rides on a significant result.  It is an unnecessary gamble. 

It is always better to realize an a priori power analysis to know Type II error and the power in case of the test is not significant.

Also, it is very easy to, a priori, estimate the power of say, a medium, effect size.  So there is little reason for not doing that at the beginning.

Best,
Sacha 

Envoy? de mon iPhone

> Le 4 juil. 2020 ? 01:04, Patrick (Malone Quantitative) <malone at malonequantitative.com> a ?crit :
> 
> ?No, because I don't think it can be. That's not how power analysis works.
> It's bad practice.
> 
>> On Fri, Jul 3, 2020, 6:42 PM Han Zhang <hanzh at umich.edu> wrote:
>> 
>> Hi Pat,
>> 
>> Thanks for your quick reply. Yes, I already have the data and the actual
>> effects, and the analysis was suggested by a reviewer. Can you elaborate on
>> when do you think such an analysis might be justified?
>> 
>> Thanks!
>> Han
>> 
>> On Fri, Jul 3, 2020 at 6:34 PM Patrick (Malone Quantitative) <
>> malone at malonequantitative.com> wrote:
>> 
>>> Han,
>>> 
>>> (1) Usually, yes, but . . .
>>> 
>>> (2) If you have an actual effect, does that mean you're doing post hoc
>>> power analysis? If so, that's a whole can of worms, for which the best
>>> advice I have is "don't do it." Use the size of the confidence
>>> interval of your estimate as an assessment of sample adequacy.
>>> 
>>> Pat
>>> 
>>> On Fri, Jul 3, 2020 at 6:27 PM Han Zhang <hanzh at umich.edu> wrote:
>>>> 
>>>> Hello,
>>>> 
>>>> I'm trying to find the minimum detectable effect size (MDES) given my
>>>> sample, alpha (.05), and desired power (90%) in a linear mixed model
>>>> setting. I'm using the simr package for a simulation-based approach.
>>> What I
>>>> did is changing the original effect size to a series of hypothetical
>>> effect
>>>> sizes and find the minimum effect size that has a 90% chance of
>>> producing a
>>>> significant result. Below is a toy code:
>>>> 
>>>> library(lmerTest)
>>>> library(simr)
>>>> 
>>>> # fit the model
>>>> model <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
>>>> summary(model)
>>>> 
>>>> Fixed effects:
>>>>            Estimate Std. Error      df t value Pr(>|t|)
>>>> (Intercept)  251.405      6.825  17.000  36.838  < 2e-16 ***
>>>> Days          10.467      1.546  17.000   6.771 3.26e-06 ***
>>>> 
>>>> 
>>>> Here is the code for minimum detectable effect size:
>>>> 
>>>> pwr <- NA
>>>> 
>>>> # define a set of reasonable effect sizes
>>>> es <- seq(0, 10, 2)
>>>> 
>>>> # loop through the effect sizes
>>>> for (i in 1:length(es)) {
>>>>  # replace the original effect size with new one
>>>>  fixef(model)['Days'] =  es[i]
>>>>  # run simulation to obtain power estimate
>>>>  pwr.summary <- summary(powerSim(
>>>>    model,
>>>>    test = fixed('Days', "t"),
>>>>    nsim = 100,
>>>>    progress = T
>>>>  ))
>>>>  # store output
>>>>  pwr[i] <- as.numeric(pwr.summary)[3]
>>>> }
>>>> 
>>>> # display results
>>>> cbind("Coefficient" = es,
>>>>      Power = pwr)
>>>> 
>>>> Output:
>>>> 
>>>>                           Coefficient   Power
>>>> [1,]                                     0  0.09
>>>> [2,]                                     2  0.24
>>>> [3,]                                     4  0.60
>>>> [4,]                                     6  0.99
>>>> [5,]                                     8  1.00
>>>> [6,]                                    10  1.00
>>>> 
>>>> My questions:
>>>> 
>>>> (1) Is this the right way to find the MDES?
>>>> 
>>>> (2) I have some trouble making sense of the output. Can I say the
>>>> following: because the estimated power when the effect = 6 is 99%, and
>>>> because the actual model has an estimate of 10.47, then the study is
>>>> sufficiently powered? Conversely, imagine that if the actual estimate
>>> was
>>>> 3.0, then can I say the study is insufficiently powered?
>>>> 
>>>> Thank you,
>>>> Han
>>>> --
>>>> Han Zhang, Ph.D.
>>>> Department of Psychology
>>>> University of Michigan, Ann Arbor
>>>> https://sites.lsa.umich.edu/hanzh/
>>>> 
>>>>        [[alternative HTML version deleted]]
>>>> 
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>> 
>>> 
>>> 
>>> --
>>> Patrick S. Malone, Ph.D., Malone Quantitative
>>> NEW Service Models: http://malonequantitative.com
>>> 
>>> He/Him/His
>>> 
>> 
>> 
>> --
>> Han Zhang, Ph.D.
>> Department of Psychology
>> University of Michigan, Ann Arbor
>> https://sites.lsa.umich.edu/hanzh/
>> 
> 
>    [[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From h@nzh @end|ng |rom um|ch@edu  Sat Jul  4 23:26:28 2020
From: h@nzh @end|ng |rom um|ch@edu (Han Zhang)
Date: Sat, 4 Jul 2020 17:26:28 -0400
Subject: [R-sig-ME] Minimum detectable effect size in linear mixed model
In-Reply-To: <E2F5114C-84F5-4E13-96E0-0973141D118A@yahoo.fr>
References: <CAJc=yOFSaEmuixj1aBphf-=tJXYA=VMzD84r2bY1wHYTff9ASg@mail.gmail.com>
 <E2F5114C-84F5-4E13-96E0-0973141D118A@yahoo.fr>
Message-ID: <CAH-zeEHW17Rw-SGFWsN781fjFf3XHbhu2H=_916DHg=QuSQZrg@mail.gmail.com>

Hi Sacha,

Correct me if I'm wrong, but I tend to think this is more like a
sensitivity analysis (given alpha, power, and N, solve for the required
effect size). If the minimum detectable effect size at 80% power ends up so
large that it exceeds the typical range in the field (say,  a .6
correlation is the minimum whereas a .2 is typically expected), then we may
say the study is underpowered. So I think I made a mistake with question
(2) - the MDES should be compared to an effect size with practical
importance, not the observed effect size.

Han

On Sat, Jul 4, 2020 at 12:07 PM varin sacha <varinsacha at yahoo.fr> wrote:

> Hi,
>
> Is the question about post hoc power analysis ?
>
> Post hoc power analyses are usually not suggested. (See for example The
> abuse of power...hoenig & heisey).
> You should do an a priori power analysis.  If you then do the small sample
> study and obtain a negative result, you have no idea why ? you are stuck.
>
> That is why I always tell people not to do a study where everything rides
> on a significant result.  It is an unnecessary gamble.
>
> It is always better to realize an a priori power analysis to know Type II
> error and the power in case of the test is not significant.
>
> Also, it is very easy to, a priori, estimate the power of say, a medium,
> effect size.  So there is little reason for not doing that at the beginning.
>
> Best,
> Sacha
>
> Envoy? de mon iPhone
>
> > Le 4 juil. 2020 ? 01:04, Patrick (Malone Quantitative) <
> malone at malonequantitative.com> a ?crit :
> >
> > ?No, because I don't think it can be. That's not how power analysis
> works.
> > It's bad practice.
> >
> >> On Fri, Jul 3, 2020, 6:42 PM Han Zhang <hanzh at umich.edu> wrote:
> >>
> >> Hi Pat,
> >>
> >> Thanks for your quick reply. Yes, I already have the data and the actual
> >> effects, and the analysis was suggested by a reviewer. Can you
> elaborate on
> >> when do you think such an analysis might be justified?
> >>
> >> Thanks!
> >> Han
> >>
> >> On Fri, Jul 3, 2020 at 6:34 PM Patrick (Malone Quantitative) <
> >> malone at malonequantitative.com> wrote:
> >>
> >>> Han,
> >>>
> >>> (1) Usually, yes, but . . .
> >>>
> >>> (2) If you have an actual effect, does that mean you're doing post hoc
> >>> power analysis? If so, that's a whole can of worms, for which the best
> >>> advice I have is "don't do it." Use the size of the confidence
> >>> interval of your estimate as an assessment of sample adequacy.
> >>>
> >>> Pat
> >>>
> >>> On Fri, Jul 3, 2020 at 6:27 PM Han Zhang <hanzh at umich.edu> wrote:
> >>>>
> >>>> Hello,
> >>>>
> >>>> I'm trying to find the minimum detectable effect size (MDES) given my
> >>>> sample, alpha (.05), and desired power (90%) in a linear mixed model
> >>>> setting. I'm using the simr package for a simulation-based approach.
> >>> What I
> >>>> did is changing the original effect size to a series of hypothetical
> >>> effect
> >>>> sizes and find the minimum effect size that has a 90% chance of
> >>> producing a
> >>>> significant result. Below is a toy code:
> >>>>
> >>>> library(lmerTest)
> >>>> library(simr)
> >>>>
> >>>> # fit the model
> >>>> model <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
> >>>> summary(model)
> >>>>
> >>>> Fixed effects:
> >>>>            Estimate Std. Error      df t value Pr(>|t|)
> >>>> (Intercept)  251.405      6.825  17.000  36.838  < 2e-16 ***
> >>>> Days          10.467      1.546  17.000   6.771 3.26e-06 ***
> >>>>
> >>>>
> >>>> Here is the code for minimum detectable effect size:
> >>>>
> >>>> pwr <- NA
> >>>>
> >>>> # define a set of reasonable effect sizes
> >>>> es <- seq(0, 10, 2)
> >>>>
> >>>> # loop through the effect sizes
> >>>> for (i in 1:length(es)) {
> >>>>  # replace the original effect size with new one
> >>>>  fixef(model)['Days'] =  es[i]
> >>>>  # run simulation to obtain power estimate
> >>>>  pwr.summary <- summary(powerSim(
> >>>>    model,
> >>>>    test = fixed('Days', "t"),
> >>>>    nsim = 100,
> >>>>    progress = T
> >>>>  ))
> >>>>  # store output
> >>>>  pwr[i] <- as.numeric(pwr.summary)[3]
> >>>> }
> >>>>
> >>>> # display results
> >>>> cbind("Coefficient" = es,
> >>>>      Power = pwr)
> >>>>
> >>>> Output:
> >>>>
> >>>>                           Coefficient   Power
> >>>> [1,]                                     0  0.09
> >>>> [2,]                                     2  0.24
> >>>> [3,]                                     4  0.60
> >>>> [4,]                                     6  0.99
> >>>> [5,]                                     8  1.00
> >>>> [6,]                                    10  1.00
> >>>>
> >>>> My questions:
> >>>>
> >>>> (1) Is this the right way to find the MDES?
> >>>>
> >>>> (2) I have some trouble making sense of the output. Can I say the
> >>>> following: because the estimated power when the effect = 6 is 99%, and
> >>>> because the actual model has an estimate of 10.47, then the study is
> >>>> sufficiently powered? Conversely, imagine that if the actual estimate
> >>> was
> >>>> 3.0, then can I say the study is insufficiently powered?
> >>>>
> >>>> Thank you,
> >>>> Han
> >>>> --
> >>>> Han Zhang, Ph.D.
> >>>> Department of Psychology
> >>>> University of Michigan, Ann Arbor
> >>>> https://sites.lsa.umich.edu/hanzh/
> >>>>
> >>>>        [[alternative HTML version deleted]]
> >>>>
> >>>> _______________________________________________
> >>>> R-sig-mixed-models at r-project.org mailing list
> >>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>>
> >>>
> >>>
> >>> --
> >>> Patrick S. Malone, Ph.D., Malone Quantitative
> >>> NEW Service Models: http://malonequantitative.com
> >>>
> >>> He/Him/His
> >>>
> >>
> >>
> >> --
> >> Han Zhang, Ph.D.
> >> Department of Psychology
> >> University of Michigan, Ann Arbor
> >> https://sites.lsa.umich.edu/hanzh/
> >>
> >
> >    [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>

-- 
Han Zhang, Ph.D.
Department of Psychology
University of Michigan, Ann Arbor
https://sites.lsa.umich.edu/hanzh/

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Sat Jul  4 23:47:31 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Sat, 4 Jul 2020 23:47:31 +0200
Subject: [R-sig-ME] Minimum detectable effect size in linear mixed model
In-Reply-To: <CAH-zeEHW17Rw-SGFWsN781fjFf3XHbhu2H=_916DHg=QuSQZrg@mail.gmail.com>
References: <CAJc=yOFSaEmuixj1aBphf-=tJXYA=VMzD84r2bY1wHYTff9ASg@mail.gmail.com>
 <E2F5114C-84F5-4E13-96E0-0973141D118A@yahoo.fr>
 <CAH-zeEHW17Rw-SGFWsN781fjFf3XHbhu2H=_916DHg=QuSQZrg@mail.gmail.com>
Message-ID: <CAJuCY5z-fQKXqMOfrOw+GOmUCG+9oEgfEw0jbbGMCpQjduyHuw@mail.gmail.com>

Dear Han,

As mentioned earlier, power analysis is only relevant _before_ you do the
study. To avoid running an underpowered study. Doing a post-hoc power
analysis on an underpowered study is putting the cart before the horse.

Once you have done the analysis, look at the confidence intervals of the
estimates.
- non-significant and values in the CI small compared to the
practical range: sufficient power
- non-significant and values in the CI similar or larger than the
practical range: underpowered
- significant and values in the CI similar or larger than the practical
range: sufficient power
- significant and values in the CI small compared to the practical range:
overpowered

Note that you should not only vary the coefficient of interest. At least
also take the uncertainty of the random effect variance into account. Don't
underestimate its effect on the power. The uncertainty on these variances
can be substantial. Especially when the design has a small (<200) number of
levels for the random effect.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////


<https://www.inbo.be>


Op za 4 jul. 2020 om 23:27 schreef Han Zhang <hanzh at umich.edu>:

> Hi Sacha,
>
> Correct me if I'm wrong, but I tend to think this is more like a
> sensitivity analysis (given alpha, power, and N, solve for the required
> effect size). If the minimum detectable effect size at 80% power ends up so
> large that it exceeds the typical range in the field (say,  a .6
> correlation is the minimum whereas a .2 is typically expected), then we may
> say the study is underpowered. So I think I made a mistake with question
> (2) - the MDES should be compared to an effect size with practical
> importance, not the observed effect size.
>
> Han
>
> On Sat, Jul 4, 2020 at 12:07 PM varin sacha <varinsacha at yahoo.fr> wrote:
>
> > Hi,
> >
> > Is the question about post hoc power analysis ?
> >
> > Post hoc power analyses are usually not suggested. (See for example The
> > abuse of power...hoenig & heisey).
> > You should do an a priori power analysis.  If you then do the small
> sample
> > study and obtain a negative result, you have no idea why ? you are stuck.
> >
> > That is why I always tell people not to do a study where everything rides
> > on a significant result.  It is an unnecessary gamble.
> >
> > It is always better to realize an a priori power analysis to know Type II
> > error and the power in case of the test is not significant.
> >
> > Also, it is very easy to, a priori, estimate the power of say, a medium,
> > effect size.  So there is little reason for not doing that at the
> beginning.
> >
> > Best,
> > Sacha
> >
> > Envoy? de mon iPhone
> >
> > > Le 4 juil. 2020 ? 01:04, Patrick (Malone Quantitative) <
> > malone at malonequantitative.com> a ?crit :
> > >
> > > ?No, because I don't think it can be. That's not how power analysis
> > works.
> > > It's bad practice.
> > >
> > >> On Fri, Jul 3, 2020, 6:42 PM Han Zhang <hanzh at umich.edu> wrote:
> > >>
> > >> Hi Pat,
> > >>
> > >> Thanks for your quick reply. Yes, I already have the data and the
> actual
> > >> effects, and the analysis was suggested by a reviewer. Can you
> > elaborate on
> > >> when do you think such an analysis might be justified?
> > >>
> > >> Thanks!
> > >> Han
> > >>
> > >> On Fri, Jul 3, 2020 at 6:34 PM Patrick (Malone Quantitative) <
> > >> malone at malonequantitative.com> wrote:
> > >>
> > >>> Han,
> > >>>
> > >>> (1) Usually, yes, but . . .
> > >>>
> > >>> (2) If you have an actual effect, does that mean you're doing post
> hoc
> > >>> power analysis? If so, that's a whole can of worms, for which the
> best
> > >>> advice I have is "don't do it." Use the size of the confidence
> > >>> interval of your estimate as an assessment of sample adequacy.
> > >>>
> > >>> Pat
> > >>>
> > >>> On Fri, Jul 3, 2020 at 6:27 PM Han Zhang <hanzh at umich.edu> wrote:
> > >>>>
> > >>>> Hello,
> > >>>>
> > >>>> I'm trying to find the minimum detectable effect size (MDES) given
> my
> > >>>> sample, alpha (.05), and desired power (90%) in a linear mixed model
> > >>>> setting. I'm using the simr package for a simulation-based approach.
> > >>> What I
> > >>>> did is changing the original effect size to a series of hypothetical
> > >>> effect
> > >>>> sizes and find the minimum effect size that has a 90% chance of
> > >>> producing a
> > >>>> significant result. Below is a toy code:
> > >>>>
> > >>>> library(lmerTest)
> > >>>> library(simr)
> > >>>>
> > >>>> # fit the model
> > >>>> model <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
> > >>>> summary(model)
> > >>>>
> > >>>> Fixed effects:
> > >>>>            Estimate Std. Error      df t value Pr(>|t|)
> > >>>> (Intercept)  251.405      6.825  17.000  36.838  < 2e-16 ***
> > >>>> Days          10.467      1.546  17.000   6.771 3.26e-06 ***
> > >>>>
> > >>>>
> > >>>> Here is the code for minimum detectable effect size:
> > >>>>
> > >>>> pwr <- NA
> > >>>>
> > >>>> # define a set of reasonable effect sizes
> > >>>> es <- seq(0, 10, 2)
> > >>>>
> > >>>> # loop through the effect sizes
> > >>>> for (i in 1:length(es)) {
> > >>>>  # replace the original effect size with new one
> > >>>>  fixef(model)['Days'] =  es[i]
> > >>>>  # run simulation to obtain power estimate
> > >>>>  pwr.summary <- summary(powerSim(
> > >>>>    model,
> > >>>>    test = fixed('Days', "t"),
> > >>>>    nsim = 100,
> > >>>>    progress = T
> > >>>>  ))
> > >>>>  # store output
> > >>>>  pwr[i] <- as.numeric(pwr.summary)[3]
> > >>>> }
> > >>>>
> > >>>> # display results
> > >>>> cbind("Coefficient" = es,
> > >>>>      Power = pwr)
> > >>>>
> > >>>> Output:
> > >>>>
> > >>>>                           Coefficient   Power
> > >>>> [1,]                                     0  0.09
> > >>>> [2,]                                     2  0.24
> > >>>> [3,]                                     4  0.60
> > >>>> [4,]                                     6  0.99
> > >>>> [5,]                                     8  1.00
> > >>>> [6,]                                    10  1.00
> > >>>>
> > >>>> My questions:
> > >>>>
> > >>>> (1) Is this the right way to find the MDES?
> > >>>>
> > >>>> (2) I have some trouble making sense of the output. Can I say the
> > >>>> following: because the estimated power when the effect = 6 is 99%,
> and
> > >>>> because the actual model has an estimate of 10.47, then the study is
> > >>>> sufficiently powered? Conversely, imagine that if the actual
> estimate
> > >>> was
> > >>>> 3.0, then can I say the study is insufficiently powered?
> > >>>>
> > >>>> Thank you,
> > >>>> Han
> > >>>> --
> > >>>> Han Zhang, Ph.D.
> > >>>> Department of Psychology
> > >>>> University of Michigan, Ann Arbor
> > >>>> https://sites.lsa.umich.edu/hanzh/
> > >>>>
> > >>>>        [[alternative HTML version deleted]]
> > >>>>
> > >>>> _______________________________________________
> > >>>> R-sig-mixed-models at r-project.org mailing list
> > >>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > >>>
> > >>>
> > >>>
> > >>> --
> > >>> Patrick S. Malone, Ph.D., Malone Quantitative
> > >>> NEW Service Models: http://malonequantitative.com
> > >>>
> > >>> He/Him/His
> > >>>
> > >>
> > >>
> > >> --
> > >> Han Zhang, Ph.D.
> > >> Department of Psychology
> > >> University of Michigan, Ann Arbor
> > >> https://sites.lsa.umich.edu/hanzh/
> > >>
> > >
> > >    [[alternative HTML version deleted]]
> > >
> > > _______________________________________________
> > > R-sig-mixed-models at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
> >
>
> --
> Han Zhang, Ph.D.
> Department of Psychology
> University of Michigan, Ann Arbor
> https://sites.lsa.umich.edu/hanzh/
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Sun Jul  5 00:31:08 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sat, 4 Jul 2020 18:31:08 -0400
Subject: [R-sig-ME] Incorrect output from nested model with mapped pars
In-Reply-To: <7332431185f5494b95ff14f5a27d509f@uxcn13-tdc-b.UoA.auckland.ac.nz>
References: <71c307e350b349c1b7f25ce79ac3704f@uxcn13-tdc-b.UoA.auckland.ac.nz>
 <7332431185f5494b95ff14f5a27d509f@uxcn13-tdc-b.UoA.auckland.ac.nz>
Message-ID: <e4738eea-cf47-e96c-c8e2-b842a2c65e2b@gmail.com>

 ? PPS? We appreciate your adding the RDS for reproducibility, but ... 
most binary attachments are stripped by the mailing list.? It would be 
best if you posted this to the glmmTMB issues list, where you can attach 
files (maybe not .rds though ...)

On 6/30/20 9:22 PM, Christopher Nottingham wrote:
> One more question. Why is the summary function giving the z value and associated p-value. A gaussian error structure is assumed, so shouldn't t values be used to obtain p values.
>
> Thanks,
> Chris
>
> From: Christopher Nottingham
> Sent: Tuesday, 30 June 2020 5:06 PM
> To: 'r-sig-mixed-models at r-project.org' <r-sig-mixed-models at r-project.org>
> Subject: Incorrect output from nested model with mapped pars
>
> I have a dataset with a variable labelled n_comm that is not relevant to some factor level combinations. I am fitting a nested model to this data and fixing betas representing the irrelevant factor combinations to 0 using map. As shown following, the model output from the summary table does not match what should be produced.
>
>> map_names = list(beta = factor(c(1:6, NA, 8)))
>> fit = glmmTMB(log(Err) ~ model + n_surv + species + n_comm:geostatistical + intensity,
> +               data = Bhat_all.df,
> +               start = list(beta = ifelse(is.na(map_names$beta), 0, 1)),
> +               map = map_names)
>> summary(fit)
> Family: gaussian  ( identity )
> Formula:          log(Err) ~ model + n_surv + species + n_comm:geostatistical +      intensity
> Data: Bhat_all.df
>
>       AIC      BIC   logLik deviance df.resid
>   18340.8  18394.7  -9162.4  18324.8     6212
>
>
> Dispersion estimate for gaussian family (sigma^2): 1.11
>
> Conditional model:
>                                      Estimate Std. Error z value Pr(>|z|)
> (Intercept)                        9.329e+00  5.461e-02  170.82   <2e-16 ***
> modelbiomass-dynamics             -4.082e+00  5.089e-02  -80.22   <2e-16 ***
> modelsize-structured              -4.092e+00  5.056e-02  -80.93   <2e-16 ***
> n_surv                            -8.895e-04  8.146e-05  -10.92   <2e-16 ***
> species$\\mathit{S. aequilatera}$  5.596e-01  2.677e-02   20.90   <2e-16 ***
> intensityFishing intensity: high   3.589e-01  2.681e-02   13.39   <2e-16 ***
> n_comm:geostatisticalFALSE         0.000e+00  7.450e-05    0.00    1.000
> n_comm:geostatisticalTRUE         -3.245e-04  5.461e-02   -0.01    0.995
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> Warning message:
> In cbind(Estimate = coefs, `Std. Error` = sqrt(diag(vcov))) :
>    number of rows of result is not a multiple of vector length (arg 2)
>> rbind(sqrt(diag(solve(fit$obj$he()))))
>             [,1]       [,2]       [,3]         [,4]       [,5]       [,6]         [,7]       [,8]
> [1,] 0.05458617 0.05086506 0.05053495 8.142355e-05 0.02675588 0.02679859 7.446289e-05 0.01792267
>> fit$sdr
> sdreport(.) result
>             Estimate   Std. Error
> beta   9.3291879616 5.461347e-02
> beta  -4.0822725635 5.089050e-02
> beta  -4.0920631052 5.056022e-02
> beta  -0.0008895195 8.146427e-05
> beta   0.5595933469 2.676926e-02
> beta   0.3589326271 2.681199e-02
> beta  -0.0003244617 7.450013e-05
> betad  0.1082286532 1.793163e-02
> Maximum gradient component: 0.001913387
>
> The output below is wrong (there should be no std err, on a mapped value and the other values are incorrect.),
> n_comm:geostatisticalTRUE         -3.245e-04  5.461e-02   -0.01    0.995
>
> The dataset is attached as a rds for reproducibility.
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From v@r|n@@ch@ @end|ng |rom y@hoo@|r  Mon Jul  6 02:16:23 2020
From: v@r|n@@ch@ @end|ng |rom y@hoo@|r (varin sacha)
Date: Mon, 6 Jul 2020 02:16:23 +0200
Subject: [R-sig-ME] Minimum detectable effect size in linear mixed model
In-Reply-To: <CAH-zeEHW17Rw-SGFWsN781fjFf3XHbhu2H=_916DHg=QuSQZrg@mail.gmail.com>
References: <CAH-zeEHW17Rw-SGFWsN781fjFf3XHbhu2H=_916DHg=QuSQZrg@mail.gmail.com>
Message-ID: <77D281B1-84AA-4DEC-B1E1-3E5E04986BA5@yahoo.fr>

 Dear Han,

I agree with your interpretation of a sensitivity analysis that shows a correlation of .6 would be needed to have the desired power in a situation where .2 would be typical. To achieve desired sensitivity, we could increase sample size, or increase alpha (i.e., go to .10 instead of .05), or we could reduce our desired power (maybe be satisfied with .80 or less instead of .90 or .95), or we could try to increase the effect size, perhaps by using better measures or a more intense treatment. 

If we wish to determine an appropriate sample size, we specify alpha, power, and the effect size. Setting the effect size is tricky because we don't know the actual effect. A logical approach is to set the effect size at the smallest value that is considered to be important. If the effect size is larger, we will have even more power. If the effect size is smaller, we don't care much if the result is not statistically significant. 

I used the acronym BEAN to help people remember the four features that are involved with power analysis. 

B = beta error, where power = (1 ? Beta error)
E = effect size
A = alpha error rate
N = sample size

If you know any three, you can compute the fourth. 

Best 
Sacha

Envoy? de mon iPhone

> Le 4 juil. 2020 ? 23:26, Han Zhang <hanzh at umich.edu> a ?crit :
> 
> ?
> Hi Sacha,
> 
> Correct me if I'm wrong, but I tend to think this is more like a sensitivity analysis (given alpha, power, and N, solve for the required effect size). If the minimum detectable effect size at 80% power ends up so large that it exceeds the typical range in the field (say,  a .6 correlation is the minimum whereas a .2 is typically expected), then we may say the study is underpowered. So I think I made a mistake with question (2) - the MDES should be compared to an effect size with practical importance, not the observed effect size.
> 
> Han
> 
>> On Sat, Jul 4, 2020 at 12:07 PM varin sacha <varinsacha at yahoo.fr> wrote:
>> Hi,
>> 
>> Is the question about post hoc power analysis ?
>> 
>> Post hoc power analyses are usually not suggested. (See for example The abuse of power...hoenig & heisey).  
>> You should do an a priori power analysis.  If you then do the small sample study and obtain a negative result, you have no idea why ? you are stuck.
>> 
>> That is why I always tell people not to do a study where everything rides on a significant result.  It is an unnecessary gamble. 
>> 
>> It is always better to realize an a priori power analysis to know Type II error and the power in case of the test is not significant.
>> 
>> Also, it is very easy to, a priori, estimate the power of say, a medium, effect size.  So there is little reason for not doing that at the beginning.
>> 
>> Best,
>> Sacha 
>> 
>> Envoy? de mon iPhone
>> 
>> > Le 4 juil. 2020 ? 01:04, Patrick (Malone Quantitative) <malone at malonequantitative.com> a ?crit :
>> > 
>> > ?No, because I don't think it can be. That's not how power analysis works.
>> > It's bad practice.
>> > 
>> >> On Fri, Jul 3, 2020, 6:42 PM Han Zhang <hanzh at umich.edu> wrote:
>> >> 
>> >> Hi Pat,
>> >> 
>> >> Thanks for your quick reply. Yes, I already have the data and the actual
>> >> effects, and the analysis was suggested by a reviewer. Can you elaborate on
>> >> when do you think such an analysis might be justified?
>> >> 
>> >> Thanks!
>> >> Han
>> >> 
>> >> On Fri, Jul 3, 2020 at 6:34 PM Patrick (Malone Quantitative) <
>> >> malone at malonequantitative.com> wrote:
>> >> 
>> >>> Han,
>> >>> 
>> >>> (1) Usually, yes, but . . .
>> >>> 
>> >>> (2) If you have an actual effect, does that mean you're doing post hoc
>> >>> power analysis? If so, that's a whole can of worms, for which the best
>> >>> advice I have is "don't do it." Use the size of the confidence
>> >>> interval of your estimate as an assessment of sample adequacy.
>> >>> 
>> >>> Pat
>> >>> 
>> >>> On Fri, Jul 3, 2020 at 6:27 PM Han Zhang <hanzh at umich.edu> wrote:
>> >>>> 
>> >>>> Hello,
>> >>>> 
>> >>>> I'm trying to find the minimum detectable effect size (MDES) given my
>> >>>> sample, alpha (.05), and desired power (90%) in a linear mixed model
>> >>>> setting. I'm using the simr package for a simulation-based approach.
>> >>> What I
>> >>>> did is changing the original effect size to a series of hypothetical
>> >>> effect
>> >>>> sizes and find the minimum effect size that has a 90% chance of
>> >>> producing a
>> >>>> significant result. Below is a toy code:
>> >>>> 
>> >>>> library(lmerTest)
>> >>>> library(simr)
>> >>>> 
>> >>>> # fit the model
>> >>>> model <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
>> >>>> summary(model)
>> >>>> 
>> >>>> Fixed effects:
>> >>>>            Estimate Std. Error      df t value Pr(>|t|)
>> >>>> (Intercept)  251.405      6.825  17.000  36.838  < 2e-16 ***
>> >>>> Days          10.467      1.546  17.000   6.771 3.26e-06 ***
>> >>>> 
>> >>>> 
>> >>>> Here is the code for minimum detectable effect size:
>> >>>> 
>> >>>> pwr <- NA
>> >>>> 
>> >>>> # define a set of reasonable effect sizes
>> >>>> es <- seq(0, 10, 2)
>> >>>> 
>> >>>> # loop through the effect sizes
>> >>>> for (i in 1:length(es)) {
>> >>>>  # replace the original effect size with new one
>> >>>>  fixef(model)['Days'] =  es[i]
>> >>>>  # run simulation to obtain power estimate
>> >>>>  pwr.summary <- summary(powerSim(
>> >>>>    model,
>> >>>>    test = fixed('Days', "t"),
>> >>>>    nsim = 100,
>> >>>>    progress = T
>> >>>>  ))
>> >>>>  # store output
>> >>>>  pwr[i] <- as.numeric(pwr.summary)[3]
>> >>>> }
>> >>>> 
>> >>>> # display results
>> >>>> cbind("Coefficient" = es,
>> >>>>      Power = pwr)
>> >>>> 
>> >>>> Output:
>> >>>> 
>> >>>>                           Coefficient   Power
>> >>>> [1,]                                     0  0.09
>> >>>> [2,]                                     2  0.24
>> >>>> [3,]                                     4  0.60
>> >>>> [4,]                                     6  0.99
>> >>>> [5,]                                     8  1.00
>> >>>> [6,]                                    10  1.00
>> >>>> 
>> >>>> My questions:
>> >>>> 
>> >>>> (1) Is this the right way to find the MDES?
>> >>>> 
>> >>>> (2) I have some trouble making sense of the output. Can I say the
>> >>>> following: because the estimated power when the effect = 6 is 99%, and
>> >>>> because the actual model has an estimate of 10.47, then the study is
>> >>>> sufficiently powered? Conversely, imagine that if the actual estimate
>> >>> was
>> >>>> 3.0, then can I say the study is insufficiently powered?
>> >>>> 
>> >>>> Thank you,
>> >>>> Han
>> >>>> --
>> >>>> Han Zhang, Ph.D.
>> >>>> Department of Psychology
>> >>>> University of Michigan, Ann Arbor
>> >>>> https://sites.lsa.umich.edu/hanzh/
>> >>>> 
>> >>>>        [[alternative HTML version deleted]]
>> >>>> 
>> >>>> _______________________________________________
>> >>>> R-sig-mixed-models at r-project.org mailing list
>> >>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >>> 
>> >>> 
>> >>> 
>> >>> --
>> >>> Patrick S. Malone, Ph.D., Malone Quantitative
>> >>> NEW Service Models: http://malonequantitative.com
>> >>> 
>> >>> He/Him/His
>> >>> 
>> >> 
>> >> 
>> >> --
>> >> Han Zhang, Ph.D.
>> >> Department of Psychology
>> >> University of Michigan, Ann Arbor
>> >> https://sites.lsa.umich.edu/hanzh/
>> >> 
>> > 
>> >    [[alternative HTML version deleted]]
>> > 
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> 
> 
> 
> -- 
> Han Zhang, Ph.D.
> Department of Psychology
> University of Michigan, Ann Arbor
> https://sites.lsa.umich.edu/hanzh/

	[[alternative HTML version deleted]]


From @tevedrd @end|ng |rom y@hoo@com  Mon Jul  6 15:49:44 2020
From: @tevedrd @end|ng |rom y@hoo@com (Steve Denham)
Date: Mon, 6 Jul 2020 13:49:44 +0000 (UTC)
Subject: [R-sig-ME] Minimum detectable effect size in linear mixed model
In-Reply-To: <CAJc=yOFSaEmuixj1aBphf-=tJXYA=VMzD84r2bY1wHYTff9ASg@mail.gmail.com>
References: <CAH-zeEF_ZOaVsAx21yWq3PjhV3=-odZ61Vkkffg541etCyEiVg@mail.gmail.com>
 <CAJc=yOEge21JObzfBKL4Tb7tu-Tbe0g-BNKtRCVPuod7=ZMveA@mail.gmail.com>
 <CAH-zeEH31ebhJM9OUDKZV+BRHDFXCPC8aRqrudqg8zm1SsQeCg@mail.gmail.com>
 <CAJc=yOFSaEmuixj1aBphf-=tJXYA=VMzD84r2bY1wHYTff9ASg@mail.gmail.com>
Message-ID: <1679044206.2469139.1594043384865@mail.yahoo.com>

Power analysis is prospective, never retrospective.? You already know the results.
Steve Denham Senior Biostatistical Scientist, Charles River Laboratories
    On Friday, July 3, 2020, 07:04:00 PM EDT, Patrick (Malone Quantitative) <malone at malonequantitative.com> wrote:  
 
 No, because I don't think it can be. That's not how power analysis works.
It's bad practice.

On Fri, Jul 3, 2020, 6:42 PM Han Zhang <hanzh at umich.edu> wrote:

> Hi Pat,
>
> Thanks for your quick reply. Yes, I already have the data and the actual
> effects, and the analysis was suggested by a reviewer. Can you elaborate on
> when do you think such an analysis might be justified?
>
> Thanks!
> Han
>
> On Fri, Jul 3, 2020 at 6:34 PM Patrick (Malone Quantitative) <
> malone at malonequantitative.com> wrote:
>
>> Han,
>>
>> (1) Usually, yes, but . . .
>>
>> (2) If you have an actual effect, does that mean you're doing post hoc
>> power analysis? If so, that's a whole can of worms, for which the best
>> advice I have is "don't do it." Use the size of the confidence
>> interval of your estimate as an assessment of sample adequacy.
>>
>> Pat
>>
>> On Fri, Jul 3, 2020 at 6:27 PM Han Zhang <hanzh at umich.edu> wrote:
>> >
>> > Hello,
>> >
>> > I'm trying to find the minimum detectable effect size (MDES) given my
>> > sample, alpha (.05), and desired power (90%) in a linear mixed model
>> > setting. I'm using the simr package for a simulation-based approach.
>> What I
>> > did is changing the original effect size to a series of hypothetical
>> effect
>> > sizes and find the minimum effect size that has a 90% chance of
>> producing a
>> > significant result. Below is a toy code:
>> >
>> > library(lmerTest)
>> > library(simr)
>> >
>> > # fit the model
>> > model <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
>> > summary(model)
>> >
>> > Fixed effects:
>> >? ? ? ? ? ? Estimate Std. Error? ? ? df t value Pr(>|t|)
>> > (Intercept)? 251.405? ? ? 6.825? 17.000? 36.838? < 2e-16 ***
>> > Days? ? ? ? ? 10.467? ? ? 1.546? 17.000? 6.771 3.26e-06 ***
>> >
>> >
>> > Here is the code for minimum detectable effect size:
>> >
>> > pwr <- NA
>> >
>> > # define a set of reasonable effect sizes
>> > es <- seq(0, 10, 2)
>> >
>> > # loop through the effect sizes
>> > for (i in 1:length(es)) {
>> >? # replace the original effect size with new one
>> >? fixef(model)['Days'] =? es[i]
>> >? # run simulation to obtain power estimate
>> >? pwr.summary <- summary(powerSim(
>> >? ? model,
>> >? ? test = fixed('Days', "t"),
>> >? ? nsim = 100,
>> >? ? progress = T
>> >? ))
>> >? # store output
>> >? pwr[i] <- as.numeric(pwr.summary)[3]
>> > }
>> >
>> > # display results
>> > cbind("Coefficient" = es,
>> >? ? ? Power = pwr)
>> >
>> > Output:
>> >
>> >? ? ? ? ? ? ? ? ? ? ? ? ? ? Coefficient? Power
>> > [1,]? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 0? 0.09
>> > [2,]? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 2? 0.24
>> > [3,]? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 4? 0.60
>> > [4,]? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 6? 0.99
>> > [5,]? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 8? 1.00
>> > [6,]? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 10? 1.00
>> >
>> > My questions:
>> >
>> > (1) Is this the right way to find the MDES?
>> >
>> > (2) I have some trouble making sense of the output. Can I say the
>> > following: because the estimated power when the effect = 6 is 99%, and
>> > because the actual model has an estimate of 10.47, then the study is
>> > sufficiently powered? Conversely, imagine that if the actual estimate
>> was
>> > 3.0, then can I say the study is insufficiently powered?
>> >
>> > Thank you,
>> > Han
>> > --
>> > Han Zhang, Ph.D.
>> > Department of Psychology
>> > University of Michigan, Ann Arbor
>> > https://sites.lsa.umich.edu/hanzh/
>> >
>> >? ? ? ? [[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>>
>> --
>> Patrick S. Malone, Ph.D., Malone Quantitative
>> NEW Service Models: http://malonequantitative.com
>>
>> He/Him/His
>>
>
>
> --
> Han Zhang, Ph.D.
> Department of Psychology
> University of Michigan, Ann Arbor
> https://sites.lsa.umich.edu/hanzh/
>

??? [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
  
	[[alternative HTML version deleted]]


From m@tthew@t@boden @end|ng |rom gm@||@com  Tue Jul  7 00:19:22 2020
From: m@tthew@t@boden @end|ng |rom gm@||@com (Matthew Boden)
Date: Mon, 6 Jul 2020 15:19:22 -0700
Subject: [R-sig-ME] Level 2 outcome and 'Downdated VtV' error
Message-ID: <CAE10PCyr9ygs02h2MpXoiDKCmH+VCmQrjDrDMaw18ArwCM_k3w@mail.gmail.com>

Good afternoon,

I am looking for advice regarding a multi-level model I am trying to
implement using lme4. My two-level random-effects model won?t run, perhaps
due to one or two issues.

Background: Level 1 is patients, which are clustered in healthcare
facilities (?Station?). The outcome is a continuous variable (?PopCov?)
that is calculated at the facility-level, and is thus a Level 2 variable
that does not vary at the patient level.

The aim of this analysis is to examine whether PopCov is predicted by (a)
patient-level (e.g., race/ethnicity, age, symptom severity), and (b)
facility-level variables (e.g., overall racial/ethnic composition, average
age). It is important to examine factors such as race/ethnicity at both
patient and facility-levels because patients with different racial/ethnic
backgrounds tend to differ in terms of age, symptom severity, etc.

Each record/row in my data is a patient, with facility-level variables
(including PopCov) having identical values among patients within a given
facility.

An error is thrown when I run a basic model.

A1 <-lmer(PopCov ~ (1 | Station), data = DISP)

*Error in fn9nM$xeval()) : Downdated VtV is not positive definite

I obtain the same error when I add to the model either a patient-level or
facility level predictor.

An internet search suggested that I have complete separation of my data
and/or poorly scaled variables.

I assume this issue has to do with the fact that the outcome is a level 2
variable. Perhaps compounding the issue is the large and unbalanced nature
of the data. I have ~6 million patients clustered in ~1000 healthcare
facilities. Individual facilities have anywhere from 100 to 30000 patients
clustered in them.

I could use some advice regarding how to specify the model to predict a
facility-level variable (level 2) from both patient (level 1) and
facility-level (level 2) variables with these data.

Thank you in advance.

Matt

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Tue Jul  7 09:02:30 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Tue, 7 Jul 2020 09:02:30 +0200
Subject: [R-sig-ME] Level 2 outcome and 'Downdated VtV' error
In-Reply-To: <CAE10PCyr9ygs02h2MpXoiDKCmH+VCmQrjDrDMaw18ArwCM_k3w@mail.gmail.com>
References: <CAE10PCyr9ygs02h2MpXoiDKCmH+VCmQrjDrDMaw18ArwCM_k3w@mail.gmail.com>
Message-ID: <CAJuCY5zRth6QKv5WMkkgps-9kzAN6a9oFqdeMDEYapmYxT8jzw@mail.gmail.com>

Dear Matthew,

I recommend aggregating the data into one record per healthcare facility,
as you did when calculating the outcome variable. The aggregation removes
all variability at the patient level. Given the huge dataset, that would
force the error term close to zero.

Another option is to use an outcome variable at the patient level.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op di 7 jul. 2020 om 00:19 schreef Matthew Boden <matthew.t.boden at gmail.com
>:

> Good afternoon,
>
> I am looking for advice regarding a multi-level model I am trying to
> implement using lme4. My two-level random-effects model won?t run, perhaps
> due to one or two issues.
>
> Background: Level 1 is patients, which are clustered in healthcare
> facilities (?Station?). The outcome is a continuous variable (?PopCov?)
> that is calculated at the facility-level, and is thus a Level 2 variable
> that does not vary at the patient level.
>
> The aim of this analysis is to examine whether PopCov is predicted by (a)
> patient-level (e.g., race/ethnicity, age, symptom severity), and (b)
> facility-level variables (e.g., overall racial/ethnic composition, average
> age). It is important to examine factors such as race/ethnicity at both
> patient and facility-levels because patients with different racial/ethnic
> backgrounds tend to differ in terms of age, symptom severity, etc.
>
> Each record/row in my data is a patient, with facility-level variables
> (including PopCov) having identical values among patients within a given
> facility.
>
> An error is thrown when I run a basic model.
>
> A1 <-lmer(PopCov ~ (1 | Station), data = DISP)
>
> *Error in fn9nM$xeval()) : Downdated VtV is not positive definite
>
> I obtain the same error when I add to the model either a patient-level or
> facility level predictor.
>
> An internet search suggested that I have complete separation of my data
> and/or poorly scaled variables.
>
> I assume this issue has to do with the fact that the outcome is a level 2
> variable. Perhaps compounding the issue is the large and unbalanced nature
> of the data. I have ~6 million patients clustered in ~1000 healthcare
> facilities. Individual facilities have anywhere from 100 to 30000 patients
> clustered in them.
>
> I could use some advice regarding how to specify the model to predict a
> facility-level variable (level 2) from both patient (level 1) and
> facility-level (level 2) variables with these data.
>
> Thank you in advance.
>
> Matt
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n|  Tue Jul  7 12:39:07 2020
From: wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n| (Viechtbauer, Wolfgang (SP))
Date: Tue, 7 Jul 2020 10:39:07 +0000
Subject: [R-sig-ME] Level 2 outcome and 'Downdated VtV' error
In-Reply-To: <CAE10PCyr9ygs02h2MpXoiDKCmH+VCmQrjDrDMaw18ArwCM_k3w@mail.gmail.com>
References: <CAE10PCyr9ygs02h2MpXoiDKCmH+VCmQrjDrDMaw18ArwCM_k3w@mail.gmail.com>
Message-ID: <8141c8dc916a4e9da527223f91fcf25a@UM-MAIL3214.unimaas.nl>

Hi Matt,

What you are trying to do (i.e., use a level 2 variable as the outcome) can and should not be done. The outcome in a multilevel model needs to be measured at the lowest level.

In your model (A1), we know a priori that there is 0 within-station variability. Hence, the ICC is exactly equal to 1 in that model, but trying to fit such a model pushes the optimization routines into a situation that leads to degeneracies.

The only way to get around this is to aggregate the data to the level of the outcome (i.e., use PopCov as the outcome and aggregate all other level 1 predictors to level 2 means).

Best,
Wolfgang

>-----Original Message-----
>From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org]
>On Behalf Of Matthew Boden
>Sent: Tuesday, 07 July, 2020 0:19
>To: r-sig-mixed-models at r-project.org
>Subject: [R-sig-ME] Level 2 outcome and 'Downdated VtV' error
>
>Good afternoon,
>
>I am looking for advice regarding a multi-level model I am trying to
>implement using lme4. My two-level random-effects model won?t run, perhaps
>due to one or two issues.
>
>Background: Level 1 is patients, which are clustered in healthcare
>facilities (?Station?). The outcome is a continuous variable (?PopCov?)
>that is calculated at the facility-level, and is thus a Level 2 variable
>that does not vary at the patient level.
>
>The aim of this analysis is to examine whether PopCov is predicted by (a)
>patient-level (e.g., race/ethnicity, age, symptom severity), and (b)
>facility-level variables (e.g., overall racial/ethnic composition, average
>age). It is important to examine factors such as race/ethnicity at both
>patient and facility-levels because patients with different racial/ethnic
>backgrounds tend to differ in terms of age, symptom severity, etc.
>
>Each record/row in my data is a patient, with facility-level variables
>(including PopCov) having identical values among patients within a given
>facility.
>
>An error is thrown when I run a basic model.
>
>A1 <-lmer(PopCov ~ (1 | Station), data = DISP)
>
>*Error in fn9nM$xeval()) : Downdated VtV is not positive definite
>
>I obtain the same error when I add to the model either a patient-level or
>facility level predictor.
>
>An internet search suggested that I have complete separation of my data
>and/or poorly scaled variables.
>
>I assume this issue has to do with the fact that the outcome is a level 2
>variable. Perhaps compounding the issue is the large and unbalanced nature
>of the data. I have ~6 million patients clustered in ~1000 healthcare
>facilities. Individual facilities have anywhere from 100 to 30000 patients
>clustered in them.
>
>I could use some advice regarding how to specify the model to predict a
>facility-level variable (level 2) from both patient (level 1) and
>facility-level (level 2) variables with these data.
>
>Thank you in advance.
>
>Matt

From m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com  Tue Jul  7 15:16:04 2020
From: m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com (Patrick (Malone Quantitative))
Date: Tue, 7 Jul 2020 09:16:04 -0400
Subject: [R-sig-ME] Level 2 outcome and 'Downdated VtV' error
In-Reply-To: <8141c8dc916a4e9da527223f91fcf25a@UM-MAIL3214.unimaas.nl>
References: <CAE10PCyr9ygs02h2MpXoiDKCmH+VCmQrjDrDMaw18ArwCM_k3w@mail.gmail.com>
 <8141c8dc916a4e9da527223f91fcf25a@UM-MAIL3214.unimaas.nl>
Message-ID: <CAJc=yOE7Xo8ZKMkXchNWRi8-3+2NPOT3qLOeexOJy-r9wxCK1Q@mail.gmail.com>

Agreed with the others. Chiming in only because I've recently been
doing research on such aggregation and I can say the consensus seems
to be it doesn't introduce bias (with the possible exception of very
small clusters, which you don't have).

On Tue, Jul 7, 2020 at 6:40 AM Viechtbauer, Wolfgang (SP)
<wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
>
> Hi Matt,
>
> What you are trying to do (i.e., use a level 2 variable as the outcome) can and should not be done. The outcome in a multilevel model needs to be measured at the lowest level.
>
> In your model (A1), we know a priori that there is 0 within-station variability. Hence, the ICC is exactly equal to 1 in that model, but trying to fit such a model pushes the optimization routines into a situation that leads to degeneracies.
>
> The only way to get around this is to aggregate the data to the level of the outcome (i.e., use PopCov as the outcome and aggregate all other level 1 predictors to level 2 means).
>
> Best,
> Wolfgang
>
> >-----Original Message-----
> >From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org]
> >On Behalf Of Matthew Boden
> >Sent: Tuesday, 07 July, 2020 0:19
> >To: r-sig-mixed-models at r-project.org
> >Subject: [R-sig-ME] Level 2 outcome and 'Downdated VtV' error
> >
> >Good afternoon,
> >
> >I am looking for advice regarding a multi-level model I am trying to
> >implement using lme4. My two-level random-effects model won?t run, perhaps
> >due to one or two issues.
> >
> >Background: Level 1 is patients, which are clustered in healthcare
> >facilities (?Station?). The outcome is a continuous variable (?PopCov?)
> >that is calculated at the facility-level, and is thus a Level 2 variable
> >that does not vary at the patient level.
> >
> >The aim of this analysis is to examine whether PopCov is predicted by (a)
> >patient-level (e.g., race/ethnicity, age, symptom severity), and (b)
> >facility-level variables (e.g., overall racial/ethnic composition, average
> >age). It is important to examine factors such as race/ethnicity at both
> >patient and facility-levels because patients with different racial/ethnic
> >backgrounds tend to differ in terms of age, symptom severity, etc.
> >
> >Each record/row in my data is a patient, with facility-level variables
> >(including PopCov) having identical values among patients within a given
> >facility.
> >
> >An error is thrown when I run a basic model.
> >
> >A1 <-lmer(PopCov ~ (1 | Station), data = DISP)
> >
> >*Error in fn9nM$xeval()) : Downdated VtV is not positive definite
> >
> >I obtain the same error when I add to the model either a patient-level or
> >facility level predictor.
> >
> >An internet search suggested that I have complete separation of my data
> >and/or poorly scaled variables.
> >
> >I assume this issue has to do with the fact that the outcome is a level 2
> >variable. Perhaps compounding the issue is the large and unbalanced nature
> >of the data. I have ~6 million patients clustered in ~1000 healthcare
> >facilities. Individual facilities have anywhere from 100 to 30000 patients
> >clustered in them.
> >
> >I could use some advice regarding how to specify the model to predict a
> >facility-level variable (level 2) from both patient (level 1) and
> >facility-level (level 2) variables with these data.
> >
> >Thank you in advance.
> >
> >Matt
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



-- 
Patrick S. Malone, Ph.D., Malone Quantitative
NEW Service Models: http://malonequantitative.com

He/Him/His


From m@tthew@t@boden @end|ng |rom gm@||@com  Wed Jul  8 19:36:12 2020
From: m@tthew@t@boden @end|ng |rom gm@||@com (Matthew Boden)
Date: Wed, 8 Jul 2020 10:36:12 -0700
Subject: [R-sig-ME] Downdated VtV error for two level mixed model
Message-ID: <CAE10PCxYm4aObzXn4gz6zC13=r-X5-8h+KocSGU_DiUFRcXipA@mail.gmail.com>

Good afternoon,

I am looking for advice regarding a mixed model I am trying to implement
using lme4. My two-level random-effects model won?t run, perhaps due to one
or two issues.

Level 1 are patients clustered in healthcare facilities (?Station?). The
outcome is a continuous variable (?PopCov?) that is calculated at the
facility-level, and thus a level 2 variable that does not vary at the
patient level.

My aim is to examine the prediction of PopCov by (a) patient-level (e.g.,
race/ethnicity, age, symptom severity), and (b) facility-level variables
(e.g., overall racial/ethnic composition, average age). It is important to
examine race/ethnicity at both patient and facility-levels because patients
with different racial/ethnic backgrounds tend to differ in terms of age,
symptom severity, etc.

Each record/row in my data set is a patient, with facility-level variables
(including PopCov) having identical values among patients within a given
facility.

An error is thrown when I run a basic model.

A1 <-lmer(PopCov ~ (1 | Station), data = DISP)

*Error in fn9nM$xeval()) : Downdated VtV is not positive definite

I obtain the same error when I add to the model either a patient-level or
facility level predictor.

An internet search suggested that I have complete separation of my data
and/or poorly scaled variables.

I assume this issue has to do with the fact that the outcome is a level 2
variable. Perhaps compounding the issue is the large and unbalanced nature
of the data. I have ~6 million patients clustered in ~1000 healthcare
facilities. Individual facilities have anywhere from 100 to 30000 patients
clustered in them.

I could use some advice regarding how to specify the model to predict a
facility-level variable (level 2) from both patient (level 1) and
facility-level (level 2) variables with these data.

Thank you in advance.

Matt

	[[alternative HTML version deleted]]


From wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n|  Wed Jul  8 19:53:15 2020
From: wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n| (Viechtbauer, Wolfgang (SP))
Date: Wed, 8 Jul 2020 17:53:15 +0000
Subject: [R-sig-ME] Downdated VtV error for two level mixed model
In-Reply-To: <CAE10PCxYm4aObzXn4gz6zC13=r-X5-8h+KocSGU_DiUFRcXipA@mail.gmail.com>
References: <CAE10PCxYm4aObzXn4gz6zC13=r-X5-8h+KocSGU_DiUFRcXipA@mail.gmail.com>
Message-ID: <50b19e1188cb427f80e7ae316d4d076c@UM-MAIL3214.unimaas.nl>

Hi Matt,

You have already received some answers to your previous post:

https://stat.ethz.ch/pipermail/r-sig-mixed-models/2020q3/028806.html
https://stat.ethz.ch/pipermail/r-sig-mixed-models/2020q3/028807.html
https://stat.ethz.ch/pipermail/r-sig-mixed-models/2020q3/028808.html

Best,
Wolfgang

>-----Original Message-----
>From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org]
>On Behalf Of Matthew Boden
>Sent: Wednesday, 08 July, 2020 19:36
>To: r-sig-mixed-models at r-project.org
>Subject: [R-sig-ME] Downdated VtV error for two level mixed model
>
>Good afternoon,
>
>I am looking for advice regarding a mixed model I am trying to implement
>using lme4. My two-level random-effects model won?t run, perhaps due to one
>or two issues.
>
>Level 1 are patients clustered in healthcare facilities (?Station?). The
>outcome is a continuous variable (?PopCov?) that is calculated at the
>facility-level, and thus a level 2 variable that does not vary at the
>patient level.
>
>My aim is to examine the prediction of PopCov by (a) patient-level (e.g.,
>race/ethnicity, age, symptom severity), and (b) facility-level variables
>(e.g., overall racial/ethnic composition, average age). It is important to
>examine race/ethnicity at both patient and facility-levels because patients
>with different racial/ethnic backgrounds tend to differ in terms of age,
>symptom severity, etc.
>
>Each record/row in my data set is a patient, with facility-level variables
>(including PopCov) having identical values among patients within a given
>facility.
>
>An error is thrown when I run a basic model.
>
>A1 <-lmer(PopCov ~ (1 | Station), data = DISP)
>
>*Error in fn9nM$xeval()) : Downdated VtV is not positive definite
>
>I obtain the same error when I add to the model either a patient-level or
>facility level predictor.
>
>An internet search suggested that I have complete separation of my data
>and/or poorly scaled variables.
>
>I assume this issue has to do with the fact that the outcome is a level 2
>variable. Perhaps compounding the issue is the large and unbalanced nature
>of the data. I have ~6 million patients clustered in ~1000 healthcare
>facilities. Individual facilities have anywhere from 100 to 30000 patients
>clustered in them.
>
>I could use some advice regarding how to specify the model to predict a
>facility-level variable (level 2) from both patient (level 1) and
>facility-level (level 2) variables with these data.
>
>Thank you in advance.
>
>Matt

From m@tthew@t@boden @end|ng |rom gm@||@com  Wed Jul  8 20:55:15 2020
From: m@tthew@t@boden @end|ng |rom gm@||@com (Matthew Boden)
Date: Wed, 8 Jul 2020 11:55:15 -0700
Subject: [R-sig-ME] Level 2 outcome and 'Downdated VtV' error
In-Reply-To: <CAJc=yOE7Xo8ZKMkXchNWRi8-3+2NPOT3qLOeexOJy-r9wxCK1Q@mail.gmail.com>
References: <CAE10PCyr9ygs02h2MpXoiDKCmH+VCmQrjDrDMaw18ArwCM_k3w@mail.gmail.com>
 <8141c8dc916a4e9da527223f91fcf25a@UM-MAIL3214.unimaas.nl>
 <CAJc=yOE7Xo8ZKMkXchNWRi8-3+2NPOT3qLOeexOJy-r9wxCK1Q@mail.gmail.com>
Message-ID: <CAE10PCzQ_-rtNsMDvmrXb2KNSqSyObq+PKM8_-0Cuk0MRGgqag@mail.gmail.com>

Thank you for these responses. I figured this was the case (that you
shouldn't predict a Level 2 variable in a mixed model), but followed
contrary advice from a colleague.  Appreciate the help.

Matt

On Tue, Jul 7, 2020 at 6:16 AM Patrick (Malone Quantitative) <
malone at malonequantitative.com> wrote:

> Agreed with the others. Chiming in only because I've recently been
> doing research on such aggregation and I can say the consensus seems
> to be it doesn't introduce bias (with the possible exception of very
> small clusters, which you don't have).
>
> On Tue, Jul 7, 2020 at 6:40 AM Viechtbauer, Wolfgang (SP)
> <wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
> >
> > Hi Matt,
> >
> > What you are trying to do (i.e., use a level 2 variable as the outcome)
> can and should not be done. The outcome in a multilevel model needs to be
> measured at the lowest level.
> >
> > In your model (A1), we know a priori that there is 0 within-station
> variability. Hence, the ICC is exactly equal to 1 in that model, but trying
> to fit such a model pushes the optimization routines into a situation that
> leads to degeneracies.
> >
> > The only way to get around this is to aggregate the data to the level of
> the outcome (i.e., use PopCov as the outcome and aggregate all other level
> 1 predictors to level 2 means).
> >
> > Best,
> > Wolfgang
> >
> > >-----Original Message-----
> > >From: R-sig-mixed-models [mailto:
> r-sig-mixed-models-bounces at r-project.org]
> > >On Behalf Of Matthew Boden
> > >Sent: Tuesday, 07 July, 2020 0:19
> > >To: r-sig-mixed-models at r-project.org
> > >Subject: [R-sig-ME] Level 2 outcome and 'Downdated VtV' error
> > >
> > >Good afternoon,
> > >
> > >I am looking for advice regarding a multi-level model I am trying to
> > >implement using lme4. My two-level random-effects model won?t run,
> perhaps
> > >due to one or two issues.
> > >
> > >Background: Level 1 is patients, which are clustered in healthcare
> > >facilities (?Station?). The outcome is a continuous variable (?PopCov?)
> > >that is calculated at the facility-level, and is thus a Level 2 variable
> > >that does not vary at the patient level.
> > >
> > >The aim of this analysis is to examine whether PopCov is predicted by
> (a)
> > >patient-level (e.g., race/ethnicity, age, symptom severity), and (b)
> > >facility-level variables (e.g., overall racial/ethnic composition,
> average
> > >age). It is important to examine factors such as race/ethnicity at both
> > >patient and facility-levels because patients with different
> racial/ethnic
> > >backgrounds tend to differ in terms of age, symptom severity, etc.
> > >
> > >Each record/row in my data is a patient, with facility-level variables
> > >(including PopCov) having identical values among patients within a given
> > >facility.
> > >
> > >An error is thrown when I run a basic model.
> > >
> > >A1 <-lmer(PopCov ~ (1 | Station), data = DISP)
> > >
> > >*Error in fn9nM$xeval()) : Downdated VtV is not positive definite
> > >
> > >I obtain the same error when I add to the model either a patient-level
> or
> > >facility level predictor.
> > >
> > >An internet search suggested that I have complete separation of my data
> > >and/or poorly scaled variables.
> > >
> > >I assume this issue has to do with the fact that the outcome is a level
> 2
> > >variable. Perhaps compounding the issue is the large and unbalanced
> nature
> > >of the data. I have ~6 million patients clustered in ~1000 healthcare
> > >facilities. Individual facilities have anywhere from 100 to 30000
> patients
> > >clustered in them.
> > >
> > >I could use some advice regarding how to specify the model to predict a
> > >facility-level variable (level 2) from both patient (level 1) and
> > >facility-level (level 2) variables with these data.
> > >
> > >Thank you in advance.
> > >
> > >Matt
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>
> --
> Patrick S. Malone, Ph.D., Malone Quantitative
> NEW Service Models: http://malonequantitative.com
>
> He/Him/His
>

	[[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Thu Jul  9 00:27:19 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Wed, 8 Jul 2020 17:27:19 -0500
Subject: [R-sig-ME] A graphic for Random intercepts via distributions
Message-ID: <CACgv6yWPT+u=nxfscpHy=OpxMaDRBCLuSyrWNWsJEyDoHFhnuw@mail.gmail.com>

Good afternoon,

I came across a picture (https://github.com/hkil/m/blob/master/mlm.PNG)
that tries to show the concept of random-intercept models using
distributions.

I think, however, the picture erroneously mixes regression concepts (e.g.,
error terms) with distributional properties of those regression concepts.

I appreciate confirmation from the expert members?

Thanks,
Simon

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Thu Jul  9 00:51:36 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 8 Jul 2020 18:51:36 -0400
Subject: [R-sig-ME] A graphic for Random intercepts via distributions
In-Reply-To: <CACgv6yWPT+u=nxfscpHy=OpxMaDRBCLuSyrWNWsJEyDoHFhnuw@mail.gmail.com>
References: <CACgv6yWPT+u=nxfscpHy=OpxMaDRBCLuSyrWNWsJEyDoHFhnuw@mail.gmail.com>
Message-ID: <9f1c9e12-124f-1b7c-3a92-108fb84a2153@gmail.com>

 ? ? Can you clarify your concern?

I can see things to quibble about here (the scales of the level-2 and 
level-1 diagrams are different; I don't know why they're using e_{ij} 
for the residual error of individual i in school j but U_{0j} for the 
deviation of school j around the grand mean; it's a little confusing to 
have "level 1" above "level 2" in the text but level 2 above level 1 in 
the picture; it's potentially confusing for the arrow showing the 
deviation from the baseline to intersect with the population density 
curve [technically, the deviation doesn't have a "level", so could be 
drawn instead as an arrow between two vertical lines rather than from a 
line to a particular point ...

... but nothing that seems actively misleading.

 ? Others may have other opinions or see something I'm missing.

On 7/8/20 6:27 PM, Simon Harmel wrote:
> Good afternoon,
>
> I came across a picture (https://github.com/hkil/m/blob/master/mlm.PNG)
> that tries to show the concept of random-intercept models using
> distributions.
>
> I think, however, the picture erroneously mixes regression concepts (e.g.,
> error terms) with distributional properties of those regression concepts.
>
> I appreciate confirmation from the expert members?
>
> Thanks,
> Simon
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From @|m@h@rme| @end|ng |rom gm@||@com  Thu Jul  9 01:24:51 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Wed, 8 Jul 2020 18:24:51 -0500
Subject: [R-sig-ME] A graphic for Random intercepts via distributions
In-Reply-To: <9f1c9e12-124f-1b7c-3a92-108fb84a2153@gmail.com>
References: <CACgv6yWPT+u=nxfscpHy=OpxMaDRBCLuSyrWNWsJEyDoHFhnuw@mail.gmail.com>
 <9f1c9e12-124f-1b7c-3a92-108fb84a2153@gmail.com>
Message-ID: <CACgv6yWu07RB6+Cm2NKjDOZcipKh5rsrUPkAqpwjq+3ADHg2-Q@mail.gmail.com>

Thanks Ben. The notations e_{ij} for the residual error of individual i in
school j and U_{0j} for the deviation of school j's mean from the grand
mean is just how educational methodologists denote these concepts.

 But specifically, I thought regression concepts like e_{ij} and U_{0j} all
should be correctly shown on a scatter plot like this:
https://github.com/hkil/m/blob/master/mlm2.PNG.

So, with your suggestions is this a better picture?:
https://github.com/hkil/m/blob/master/mlm3.PNG

Is there a relationship between the scale of the fist-level distributions,
and the second-level distribution that the picture should observe?

Thanks,
Simon


On Wed, Jul 8, 2020 at 5:51 PM Ben Bolker <bbolker at gmail.com> wrote:

>      Can you clarify your concern?
>
> I can see things to quibble about here (the scales of the level-2 and
> level-1 diagrams are different; I don't know why they're using e_{ij}
> for the residual error of individual i in school j but U_{0j} for the
> deviation of school j around the grand mean; it's a little confusing to
> have "level 1" above "level 2" in the text but level 2 above level 1 in
> the picture; it's potentially confusing for the arrow showing the
> deviation from the baseline to intersect with the population density
> curve [technically, the deviation doesn't have a "level", so could be
> drawn instead as an arrow between two vertical lines rather than from a
> line to a particular point ...
>
> ... but nothing that seems actively misleading.
>
>    Others may have other opinions or see something I'm missing.
>
> On 7/8/20 6:27 PM, Simon Harmel wrote:
> > Good afternoon,
> >
> > I came across a picture (https://github.com/hkil/m/blob/master/mlm.PNG)
> > that tries to show the concept of random-intercept models using
> > distributions.
> >
> > I think, however, the picture erroneously mixes regression concepts
> (e.g.,
> > error terms) with distributional properties of those regression concepts.
> >
> > I appreciate confirmation from the expert members?
> >
> > Thanks,
> > Simon
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Thu Jul  9 01:45:10 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 8 Jul 2020 19:45:10 -0400
Subject: [R-sig-ME] A graphic for Random intercepts via distributions
In-Reply-To: <CACgv6yWu07RB6+Cm2NKjDOZcipKh5rsrUPkAqpwjq+3ADHg2-Q@mail.gmail.com>
References: <CACgv6yWPT+u=nxfscpHy=OpxMaDRBCLuSyrWNWsJEyDoHFhnuw@mail.gmail.com>
 <9f1c9e12-124f-1b7c-3a92-108fb84a2153@gmail.com>
 <CACgv6yWu07RB6+Cm2NKjDOZcipKh5rsrUPkAqpwjq+3ADHg2-Q@mail.gmail.com>
Message-ID: <CABghstQtXXrk-WE2h7oG1t2shKt4hoT+O0dvHo_TcWZvFKsZJg@mail.gmail.com>

   I agree that the second version you link to might be slightly more
technically correct, but I don't think there's anything about harmful.
   The most important correction (IMO) would be to make the red
(level-2) distribution much wider, so that it actually matched the
scale of the level-1 distribution.  (The other problem with the
picture is that for prettiness, the beta_{0x} values we can see appear
evenly spaced, which is unrealistic ...)

On Wed, Jul 8, 2020 at 7:25 PM Simon Harmel <sim.harmel at gmail.com> wrote:
>
> Thanks Ben. The notations e_{ij} for the residual error of individual i in school j and U_{0j} for the deviation of school j's mean from the grand mean is just how educational methodologists denote these concepts.
>
>  But specifically, I thought regression concepts like e_{ij} and U_{0j} all should be correctly shown on a scatter plot like this: https://github.com/hkil/m/blob/master/mlm2.PNG.
>
> So, with your suggestions is this a better picture?: https://github.com/hkil/m/blob/master/mlm3.PNG
>
> Is there a relationship between the scale of the fist-level distributions, and the second-level distribution that the picture should observe?
>
> Thanks,
> Simon
>
>
> On Wed, Jul 8, 2020 at 5:51 PM Ben Bolker <bbolker at gmail.com> wrote:
>>
>>      Can you clarify your concern?
>>
>> I can see things to quibble about here (the scales of the level-2 and
>> level-1 diagrams are different; I don't know why they're using e_{ij}
>> for the residual error of individual i in school j but U_{0j} for the
>> deviation of school j around the grand mean; it's a little confusing to
>> have "level 1" above "level 2" in the text but level 2 above level 1 in
>> the picture; it's potentially confusing for the arrow showing the
>> deviation from the baseline to intersect with the population density
>> curve [technically, the deviation doesn't have a "level", so could be
>> drawn instead as an arrow between two vertical lines rather than from a
>> line to a particular point ...
>>
>> ... but nothing that seems actively misleading.
>>
>>    Others may have other opinions or see something I'm missing.
>>
>> On 7/8/20 6:27 PM, Simon Harmel wrote:
>> > Good afternoon,
>> >
>> > I came across a picture (https://github.com/hkil/m/blob/master/mlm.PNG)
>> > that tries to show the concept of random-intercept models using
>> > distributions.
>> >
>> > I think, however, the picture erroneously mixes regression concepts (e.g.,
>> > error terms) with distributional properties of those regression concepts.
>> >
>> > I appreciate confirmation from the expert members?
>> >
>> > Thanks,
>> > Simon
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From @|m@h@rme| @end|ng |rom gm@||@com  Thu Jul  9 01:49:04 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Wed, 8 Jul 2020 18:49:04 -0500
Subject: [R-sig-ME] A graphic for Random intercepts via distributions
In-Reply-To: <CABghstQtXXrk-WE2h7oG1t2shKt4hoT+O0dvHo_TcWZvFKsZJg@mail.gmail.com>
References: <CACgv6yWPT+u=nxfscpHy=OpxMaDRBCLuSyrWNWsJEyDoHFhnuw@mail.gmail.com>
 <9f1c9e12-124f-1b7c-3a92-108fb84a2153@gmail.com>
 <CACgv6yWu07RB6+Cm2NKjDOZcipKh5rsrUPkAqpwjq+3ADHg2-Q@mail.gmail.com>
 <CABghstQtXXrk-WE2h7oG1t2shKt4hoT+O0dvHo_TcWZvFKsZJg@mail.gmail.com>
Message-ID: <CACgv6yVcOaJpQ_3VdfT_nvMeWSgOjuwSC5Y+tf1Jt-bwbzdJuw@mail.gmail.com>

Thanks so much, will do all that!

On Wed, Jul 8, 2020 at 6:45 PM Ben Bolker <bbolker at gmail.com> wrote:

>    I agree that the second version you link to might be slightly more
> technically correct, but I don't think there's anything about harmful.
>    The most important correction (IMO) would be to make the red
> (level-2) distribution much wider, so that it actually matched the
> scale of the level-1 distribution.  (The other problem with the
> picture is that for prettiness, the beta_{0x} values we can see appear
> evenly spaced, which is unrealistic ...)
>
> On Wed, Jul 8, 2020 at 7:25 PM Simon Harmel <sim.harmel at gmail.com> wrote:
> >
> > Thanks Ben. The notations e_{ij} for the residual error of individual i
> in school j and U_{0j} for the deviation of school j's mean from the grand
> mean is just how educational methodologists denote these concepts.
> >
> >  But specifically, I thought regression concepts like e_{ij} and U_{0j}
> all should be correctly shown on a scatter plot like this:
> https://github.com/hkil/m/blob/master/mlm2.PNG.
> >
> > So, with your suggestions is this a better picture?:
> https://github.com/hkil/m/blob/master/mlm3.PNG
> >
> > Is there a relationship between the scale of the fist-level
> distributions, and the second-level distribution that the picture should
> observe?
> >
> > Thanks,
> > Simon
> >
> >
> > On Wed, Jul 8, 2020 at 5:51 PM Ben Bolker <bbolker at gmail.com> wrote:
> >>
> >>      Can you clarify your concern?
> >>
> >> I can see things to quibble about here (the scales of the level-2 and
> >> level-1 diagrams are different; I don't know why they're using e_{ij}
> >> for the residual error of individual i in school j but U_{0j} for the
> >> deviation of school j around the grand mean; it's a little confusing to
> >> have "level 1" above "level 2" in the text but level 2 above level 1 in
> >> the picture; it's potentially confusing for the arrow showing the
> >> deviation from the baseline to intersect with the population density
> >> curve [technically, the deviation doesn't have a "level", so could be
> >> drawn instead as an arrow between two vertical lines rather than from a
> >> line to a particular point ...
> >>
> >> ... but nothing that seems actively misleading.
> >>
> >>    Others may have other opinions or see something I'm missing.
> >>
> >> On 7/8/20 6:27 PM, Simon Harmel wrote:
> >> > Good afternoon,
> >> >
> >> > I came across a picture (
> https://github.com/hkil/m/blob/master/mlm.PNG)
> >> > that tries to show the concept of random-intercept models using
> >> > distributions.
> >> >
> >> > I think, however, the picture erroneously mixes regression concepts
> (e.g.,
> >> > error terms) with distributional properties of those regression
> concepts.
> >> >
> >> > I appreciate confirmation from the expert members?
> >> >
> >> > Thanks,
> >> > Simon
> >> >
> >> >       [[alternative HTML version deleted]]
> >> >
> >> > _______________________________________________
> >> > R-sig-mixed-models at r-project.org mailing list
> >> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>
> >> _______________________________________________
> >> R-sig-mixed-models at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From j@v|er@@eo@ne @end|ng |rom u@m@e@  Thu Jul  9 07:19:16 2020
From: j@v|er@@eo@ne @end|ng |rom u@m@e@ (Javier Seoane)
Date: Thu, 9 Jul 2020 07:19:16 +0200
Subject: [R-sig-ME] A graphic for Random intercepts via distributions
In-Reply-To: <CACgv6yVcOaJpQ_3VdfT_nvMeWSgOjuwSC5Y+tf1Jt-bwbzdJuw@mail.gmail.com>
References: <CACgv6yWPT+u=nxfscpHy=OpxMaDRBCLuSyrWNWsJEyDoHFhnuw@mail.gmail.com>
 <9f1c9e12-124f-1b7c-3a92-108fb84a2153@gmail.com>
 <CACgv6yWu07RB6+Cm2NKjDOZcipKh5rsrUPkAqpwjq+3ADHg2-Q@mail.gmail.com>
 <CABghstQtXXrk-WE2h7oG1t2shKt4hoT+O0dvHo_TcWZvFKsZJg@mail.gmail.com>
 <CACgv6yVcOaJpQ_3VdfT_nvMeWSgOjuwSC5Y+tf1Jt-bwbzdJuw@mail.gmail.com>
Message-ID: <CABe4hFVUizyRv3H=JhTeeHKdLgiKw_GahkgnV22vv5ynXt_9hg@mail.gmail.com>

Simon, will you have those pictures available somewhere for educational
purposes? My field is ecology, where mixed models are more often presented
otherwise, not stressing that hierarchy among levels so much as in
education studies. However, I find the pictures could be useful to my
students,  along with the caveats you and Ben Bolker have made.





El jue., 9 jul. 2020 a las 2:11, Simon Harmel (<sim.harmel at gmail.com>)
escribi?:

> Thanks so much, will do all that!
>
> On Wed, Jul 8, 2020 at 6:45 PM Ben Bolker <bbolker at gmail.com> wrote:
>
> >    I agree that the second version you link to might be slightly more
> > technically correct, but I don't think there's anything about harmful.
> >    The most important correction (IMO) would be to make the red
> > (level-2) distribution much wider, so that it actually matched the
> > scale of the level-1 distribution.  (The other problem with the
> > picture is that for prettiness, the beta_{0x} values we can see appear
> > evenly spaced, which is unrealistic ...)
> >
> > On Wed, Jul 8, 2020 at 7:25 PM Simon Harmel <sim.harmel at gmail.com>
> wrote:
> > >
> > > Thanks Ben. The notations e_{ij} for the residual error of individual i
> > in school j and U_{0j} for the deviation of school j's mean from the
> grand
> > mean is just how educational methodologists denote these concepts.
> > >
> > >  But specifically, I thought regression concepts like e_{ij} and U_{0j}
> > all should be correctly shown on a scatter plot like this:
> > https://github.com/hkil/m/blob/master/mlm2.PNG.
> > >
> > > So, with your suggestions is this a better picture?:
> > https://github.com/hkil/m/blob/master/mlm3.PNG
> > >
> > > Is there a relationship between the scale of the fist-level
> > distributions, and the second-level distribution that the picture should
> > observe?
> > >
> > > Thanks,
> > > Simon
> > >
> > >
> > > On Wed, Jul 8, 2020 at 5:51 PM Ben Bolker <bbolker at gmail.com> wrote:
> > >>
> > >>      Can you clarify your concern?
> > >>
> > >> I can see things to quibble about here (the scales of the level-2 and
> > >> level-1 diagrams are different; I don't know why they're using e_{ij}
> > >> for the residual error of individual i in school j but U_{0j} for the
> > >> deviation of school j around the grand mean; it's a little confusing
> to
> > >> have "level 1" above "level 2" in the text but level 2 above level 1
> in
> > >> the picture; it's potentially confusing for the arrow showing the
> > >> deviation from the baseline to intersect with the population density
> > >> curve [technically, the deviation doesn't have a "level", so could be
> > >> drawn instead as an arrow between two vertical lines rather than from
> a
> > >> line to a particular point ...
> > >>
> > >> ... but nothing that seems actively misleading.
> > >>
> > >>    Others may have other opinions or see something I'm missing.
> > >>
> > >> On 7/8/20 6:27 PM, Simon Harmel wrote:
> > >> > Good afternoon,
> > >> >
> > >> > I came across a picture (
> > https://github.com/hkil/m/blob/master/mlm.PNG)
> > >> > that tries to show the concept of random-intercept models using
> > >> > distributions.
> > >> >
> > >> > I think, however, the picture erroneously mixes regression concepts
> > (e.g.,
> > >> > error terms) with distributional properties of those regression
> > concepts.
> > >> >
> > >> > I appreciate confirmation from the expert members?
> > >> >
> > >> > Thanks,
> > >> > Simon
> > >> >
> > >> >       [[alternative HTML version deleted]]
> > >> >
> > >> > _______________________________________________
> > >> > R-sig-mixed-models at r-project.org mailing list
> > >> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > >>
> > >> _______________________________________________
> > >> R-sig-mixed-models at r-project.org mailing list
> > >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


-- 

= = = = = = = = = = = = = = = = = = = =


Javier Seoane Pinilla
Profesor Titular
Centro de Investigaci?n en Biodiversidad y Cambio Global (CIBC-UAM)
Departamento de Ecologia
Universidad Autonoma de Madrid
Edificio de Biolog?a
Darwin, 2
E-28049 Madrid
SPAIN

e-mail: javier.seoane at uam.es
webpage: http://teguam.es/miembros/javier-seoane/
Tlf: +34 91 497 3639
= = = = = = = = = = = = = = = = = = = =

	[[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Sun Jul 12 22:18:47 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Sun, 12 Jul 2020 15:18:47 -0500
Subject: [R-sig-ME] A graphic for Random intercepts via distributions
In-Reply-To: <CABe4hFVUizyRv3H=JhTeeHKdLgiKw_GahkgnV22vv5ynXt_9hg@mail.gmail.com>
References: <CACgv6yWPT+u=nxfscpHy=OpxMaDRBCLuSyrWNWsJEyDoHFhnuw@mail.gmail.com>
 <9f1c9e12-124f-1b7c-3a92-108fb84a2153@gmail.com>
 <CACgv6yWu07RB6+Cm2NKjDOZcipKh5rsrUPkAqpwjq+3ADHg2-Q@mail.gmail.com>
 <CABghstQtXXrk-WE2h7oG1t2shKt4hoT+O0dvHo_TcWZvFKsZJg@mail.gmail.com>
 <CACgv6yVcOaJpQ_3VdfT_nvMeWSgOjuwSC5Y+tf1Jt-bwbzdJuw@mail.gmail.com>
 <CABe4hFVUizyRv3H=JhTeeHKdLgiKw_GahkgnV22vv5ynXt_9hg@mail.gmail.com>
Message-ID: <CACgv6yUCziYCzkLhMVAzRh8zr9CTAPbNrSRaQgdZszJ4-7s0nw@mail.gmail.com>

Javier,

These pictures were made by a friend of mine. Feel free to use them for
your classes. However, I wonder, how would you guys change the graphs I
shared here to represent a cross-classified (crossed-design), roughly
speaking?

Simon

On Thu, Jul 9, 2020 at 12:19 AM Javier Seoane <javier.seoane at uam.es> wrote:

> Simon, will you have those pictures available somewhere for educational
> purposes? My field is ecology, where mixed models are more often presented
> otherwise, not stressing that hierarchy among levels so much as in
> education studies. However, I find the pictures could be useful to my
> students,  along with the caveats you and Ben Bolker have made.
>
>
>
>
>
> El jue., 9 jul. 2020 a las 2:11, Simon Harmel (<sim.harmel at gmail.com>)
> escribi?:
>
>> Thanks so much, will do all that!
>>
>> On Wed, Jul 8, 2020 at 6:45 PM Ben Bolker <bbolker at gmail.com> wrote:
>>
>> >    I agree that the second version you link to might be slightly more
>> > technically correct, but I don't think there's anything about harmful.
>> >    The most important correction (IMO) would be to make the red
>> > (level-2) distribution much wider, so that it actually matched the
>> > scale of the level-1 distribution.  (The other problem with the
>> > picture is that for prettiness, the beta_{0x} values we can see appear
>> > evenly spaced, which is unrealistic ...)
>> >
>> > On Wed, Jul 8, 2020 at 7:25 PM Simon Harmel <sim.harmel at gmail.com>
>> wrote:
>> > >
>> > > Thanks Ben. The notations e_{ij} for the residual error of individual
>> i
>> > in school j and U_{0j} for the deviation of school j's mean from the
>> grand
>> > mean is just how educational methodologists denote these concepts.
>> > >
>> > >  But specifically, I thought regression concepts like e_{ij} and
>> U_{0j}
>> > all should be correctly shown on a scatter plot like this:
>> > https://github.com/hkil/m/blob/master/mlm2.PNG.
>> > >
>> > > So, with your suggestions is this a better picture?:
>> > https://github.com/hkil/m/blob/master/mlm3.PNG
>> > >
>> > > Is there a relationship between the scale of the fist-level
>> > distributions, and the second-level distribution that the picture should
>> > observe?
>> > >
>> > > Thanks,
>> > > Simon
>> > >
>> > >
>> > > On Wed, Jul 8, 2020 at 5:51 PM Ben Bolker <bbolker at gmail.com> wrote:
>> > >>
>> > >>      Can you clarify your concern?
>> > >>
>> > >> I can see things to quibble about here (the scales of the level-2 and
>> > >> level-1 diagrams are different; I don't know why they're using e_{ij}
>> > >> for the residual error of individual i in school j but U_{0j} for the
>> > >> deviation of school j around the grand mean; it's a little confusing
>> to
>> > >> have "level 1" above "level 2" in the text but level 2 above level 1
>> in
>> > >> the picture; it's potentially confusing for the arrow showing the
>> > >> deviation from the baseline to intersect with the population density
>> > >> curve [technically, the deviation doesn't have a "level", so could be
>> > >> drawn instead as an arrow between two vertical lines rather than
>> from a
>> > >> line to a particular point ...
>> > >>
>> > >> ... but nothing that seems actively misleading.
>> > >>
>> > >>    Others may have other opinions or see something I'm missing.
>> > >>
>> > >> On 7/8/20 6:27 PM, Simon Harmel wrote:
>> > >> > Good afternoon,
>> > >> >
>> > >> > I came across a picture (
>> > https://github.com/hkil/m/blob/master/mlm.PNG)
>> > >> > that tries to show the concept of random-intercept models using
>> > >> > distributions.
>> > >> >
>> > >> > I think, however, the picture erroneously mixes regression concepts
>> > (e.g.,
>> > >> > error terms) with distributional properties of those regression
>> > concepts.
>> > >> >
>> > >> > I appreciate confirmation from the expert members?
>> > >> >
>> > >> > Thanks,
>> > >> > Simon
>> > >> >
>> > >> >       [[alternative HTML version deleted]]
>> > >> >
>> > >> > _______________________________________________
>> > >> > R-sig-mixed-models at r-project.org mailing list
>> > >> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> > >>
>> > >> _______________________________________________
>> > >> R-sig-mixed-models at r-project.org mailing list
>> > >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>
> --
>
> = = = = = = = = = = = = = = = = = = = =
>
>
> Javier Seoane Pinilla
> Profesor Titular
> Centro de Investigaci?n en Biodiversidad y Cambio Global (CIBC-UAM)
> Departamento de Ecologia
> Universidad Autonoma de Madrid
> Edificio de Biolog?a
> Darwin, 2
> E-28049 Madrid
> SPAIN
>
> e-mail: javier.seoane at uam.es
> webpage: http://teguam.es/miembros/javier-seoane/
> Tlf: +34 91 497 3639
> = = = = = = = = = = = = = = = = = = = =
>

	[[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Sun Jul 12 22:44:17 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Sun, 12 Jul 2020 15:44:17 -0500
Subject: [R-sig-ME] Simulating mixed-effects longitudinal logistic data in R
Message-ID: <CACgv6yULMfqE59C5Mb8Uge93M8AmSF5Vh-Lz-c7JUpxoMUQVHQ@mail.gmail.com>

Good afternoon,

I know to simulate a linear mixed-effects longitudinal data in R, I can use
the procedure below.

But I wonder how to generate a similar design, but for a logistic model in
R?

I appreciate your suggestions,
Simon

#------------- linear mixed-effects longitudinal data in R-------

library(MASS)

growth.sim <- function(J, n.time, gammas, G, sigma = 1, seed = NULL) {

   X <- cbind(1, seq_len(n.time) - 1) # time indicators for each individual
   X <- X[rep(seq_len(n.time), J), ]  # Repeat each row n.time times

  st.id <- seq_len(J)                 # individual id
  st.id <- rep(st.id, each = n.time)  # repeat each ID n.time times


set.seed(seed) ## what should go below, possibly correlated Beta?
uj <- MASS::mvrnorm(J, mu = rep(0, 2), Sigma = G) # Generate u0 and u1
random effects for intercepts and slopes

## I think no error term is needed for logistic version?
eij <- rnorm(J * n.time, sd = sigma)              # Generate error
term for observations


betaj <- matrix(gammas, nrow = J, ncol = 2, byrow = TRUE) + uj #
Compute beta_j's

y <- rowSums(X * betaj[st.id, ]) + eij          # Compute outcome
based on Yij = sum(Xij*Bj) + eij:

dat <- data.frame(st.id, time = X[ , 2], y) # Output a data frame
return(dat)                                 # Return data}
*##==== Example of use: =====*
growth.sim(10, 4, gammas = c(300, 25),
       G = matrix(c(0.1, 0,
                    0, 0.01), nrow = 2))

	[[alternative HTML version deleted]]


From d@r|zopou|o@ @end|ng |rom er@@mu@mc@n|  Sun Jul 12 23:58:34 2020
From: d@r|zopou|o@ @end|ng |rom er@@mu@mc@n| (D. Rizopoulos)
Date: Sun, 12 Jul 2020 21:58:34 +0000
Subject: [R-sig-ME] 
 Simulating mixed-effects longitudinal logistic data in R
In-Reply-To: <CACgv6yULMfqE59C5Mb8Uge93M8AmSF5Vh-Lz-c7JUpxoMUQVHQ@mail.gmail.com>
References: <CACgv6yULMfqE59C5Mb8Uge93M8AmSF5Vh-Lz-c7JUpxoMUQVHQ@mail.gmail.com>
Message-ID: <AM6PR04MB587932C9373FC89A63D098C6E8630@AM6PR04MB5879.eurprd04.prod.outlook.com>

Have a look in this example: https://drizopoulos.github.io/GLMMadaptive/articles/GLMMadaptive_basics.html

Best,
Dimitris

??
Dimitris Rizopoulos
Professor of Biostatistics
Erasmus University Medical Center
The Netherlands
________________________________
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on behalf of Simon Harmel <sim.harmel at gmail.com>
Sent: Sunday, July 12, 2020 10:44:17 PM
To: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
Subject: [R-sig-ME] Simulating mixed-effects longitudinal logistic data in R

Good afternoon,

I know to simulate a linear mixed-effects longitudinal data in R, I can use
the procedure below.

But I wonder how to generate a similar design, but for a logistic model in
R?

I appreciate your suggestions,
Simon

#------------- linear mixed-effects longitudinal data in R-------

library(MASS)

growth.sim <- function(J, n.time, gammas, G, sigma = 1, seed = NULL) {

   X <- cbind(1, seq_len(n.time) - 1) # time indicators for each individual
   X <- X[rep(seq_len(n.time), J), ]  # Repeat each row n.time times

  st.id <- seq_len(J)                 # individual id
  st.id <- rep(st.id, each = n.time)  # repeat each ID n.time times


set.seed(seed) ## what should go below, possibly correlated Beta?
uj <- MASS::mvrnorm(J, mu = rep(0, 2), Sigma = G) # Generate u0 and u1
random effects for intercepts and slopes

## I think no error term is needed for logistic version?
eij <- rnorm(J * n.time, sd = sigma)              # Generate error
term for observations


betaj <- matrix(gammas, nrow = J, ncol = 2, byrow = TRUE) + uj #
Compute beta_j's

y <- rowSums(X * betaj[st.id, ]) + eij          # Compute outcome
based on Yij = sum(Xij*Bj) + eij:

dat <- data.frame(st.id, time = X[ , 2], y) # Output a data frame
return(dat)                                 # Return data}
*##==== Example of use: =====*
growth.sim(10, 4, gammas = c(300, 25),
       G = matrix(c(0.1, 0,
                    0, 0.01), nrow = 2))

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-mixed-models&amp;data=02%7C01%7Cd.rizopoulos%40erasmusmc.nl%7Cf46c256b636848749fc608d826a46b4e%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C0%7C637301834909767585&amp;sdata=GjOGQ%2Fv5aMwH8PK%2FuM%2FEeHkElDFwB6%2BaD8UZy2wVdx4%3D&amp;reserved=0

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Mon Jul 13 00:01:47 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sun, 12 Jul 2020 18:01:47 -0400
Subject: [R-sig-ME] 
 Simulating mixed-effects longitudinal logistic data in R
In-Reply-To: <AM6PR04MB587932C9373FC89A63D098C6E8630@AM6PR04MB5879.eurprd04.prod.outlook.com>
References: <CACgv6yULMfqE59C5Mb8Uge93M8AmSF5Vh-Lz-c7JUpxoMUQVHQ@mail.gmail.com>
 <AM6PR04MB587932C9373FC89A63D098C6E8630@AM6PR04MB5879.eurprd04.prod.outlook.com>
Message-ID: <be70be1c-1aac-7610-8d66-bfaf84c2048d@gmail.com>

 ? Or:


(1)? see ?lme4::simulate.merMod(), i.e. set up your covariates, then 
simulate(formula, newdata=data_frame, newparams=..., family=binomial, 
weights=1)

or

(2) after setting everything up as in your example, use

response <- rbinom(length(y), prob=plogis(y), size=1)

On 7/12/20 5:58 PM, D. Rizopoulos wrote:
> Have a look in this example: https://drizopoulos.github.io/GLMMadaptive/articles/GLMMadaptive_basics.html
>
> Best,
> Dimitris
>
> ??
> Dimitris Rizopoulos
> Professor of Biostatistics
> Erasmus University Medical Center
> The Netherlands
> ________________________________
> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on behalf of Simon Harmel <sim.harmel at gmail.com>
> Sent: Sunday, July 12, 2020 10:44:17 PM
> To: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
> Subject: [R-sig-ME] Simulating mixed-effects longitudinal logistic data in R
>
> Good afternoon,
>
> I know to simulate a linear mixed-effects longitudinal data in R, I can use
> the procedure below.
>
> But I wonder how to generate a similar design, but for a logistic model in
> R?
>
> I appreciate your suggestions,
> Simon
>
> #------------- linear mixed-effects longitudinal data in R-------
>
> library(MASS)
>
> growth.sim <- function(J, n.time, gammas, G, sigma = 1, seed = NULL) {
>
>     X <- cbind(1, seq_len(n.time) - 1) # time indicators for each individual
>     X <- X[rep(seq_len(n.time), J), ]  # Repeat each row n.time times
>
>    st.id <- seq_len(J)                 # individual id
>    st.id <- rep(st.id, each = n.time)  # repeat each ID n.time times
>
>
> set.seed(seed) ## what should go below, possibly correlated Beta?
> uj <- MASS::mvrnorm(J, mu = rep(0, 2), Sigma = G) # Generate u0 and u1
> random effects for intercepts and slopes
>
> ## I think no error term is needed for logistic version?
> eij <- rnorm(J * n.time, sd = sigma)              # Generate error
> term for observations
>
>
> betaj <- matrix(gammas, nrow = J, ncol = 2, byrow = TRUE) + uj #
> Compute beta_j's
>
> y <- rowSums(X * betaj[st.id, ]) + eij          # Compute outcome
> based on Yij = sum(Xij*Bj) + eij:
>
> dat <- data.frame(st.id, time = X[ , 2], y) # Output a data frame
> return(dat)                                 # Return data}
> *##==== Example of use: =====*
> growth.sim(10, 4, gammas = c(300, 25),
>         G = matrix(c(0.1, 0,
>                      0, 0.01), nrow = 2))
>
>          [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-mixed-models&amp;data=02%7C01%7Cd.rizopoulos%40erasmusmc.nl%7Cf46c256b636848749fc608d826a46b4e%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C0%7C637301834909767585&amp;sdata=GjOGQ%2Fv5aMwH8PK%2FuM%2FEeHkElDFwB6%2BaD8UZy2wVdx4%3D&amp;reserved=0
>
> 	[[alternative HTML version deleted]]
>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Mon Jul 13 00:11:52 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Sun, 12 Jul 2020 17:11:52 -0500
Subject: [R-sig-ME] 
 Simulating mixed-effects longitudinal logistic data in R
In-Reply-To: <be70be1c-1aac-7610-8d66-bfaf84c2048d@gmail.com>
References: <CACgv6yULMfqE59C5Mb8Uge93M8AmSF5Vh-Lz-c7JUpxoMUQVHQ@mail.gmail.com>
 <AM6PR04MB587932C9373FC89A63D098C6E8630@AM6PR04MB5879.eurprd04.prod.outlook.com>
 <be70be1c-1aac-7610-8d66-bfaf84c2048d@gmail.com>
Message-ID: <CACgv6yUeFYqvqboko+rK0W9vN_Z-iB+L+oSwFh5OP0c6dAbaJg@mail.gmail.com>

Many thanks all!  I basically want to generate 100 bernoulli outcomes whose
probability of being 1 is longitudinally modeled based on 4 time points
(0:3), membership to 2 groups (Control vs. treatment), and gender (male vs.
female).

Thanks a lot!

On Sun, Jul 12, 2020 at 5:02 PM Ben Bolker <bbolker at gmail.com> wrote:

>    Or:
>
>
> (1)  see ?lme4::simulate.merMod(), i.e. set up your covariates, then
> simulate(formula, newdata=data_frame, newparams=..., family=binomial,
> weights=1)
>
> or
>
> (2) after setting everything up as in your example, use
>
> response <- rbinom(length(y), prob=plogis(y), size=1)
>
> On 7/12/20 5:58 PM, D. Rizopoulos wrote:
> > Have a look in this example:
> https://drizopoulos.github.io/GLMMadaptive/articles/GLMMadaptive_basics.html
> >
> > Best,
> > Dimitris
> >
> > ??
> > Dimitris Rizopoulos
> > Professor of Biostatistics
> > Erasmus University Medical Center
> > The Netherlands
> > ________________________________
> > From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on
> behalf of Simon Harmel <sim.harmel at gmail.com>
> > Sent: Sunday, July 12, 2020 10:44:17 PM
> > To: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
> > Subject: [R-sig-ME] Simulating mixed-effects longitudinal logistic data
> in R
> >
> > Good afternoon,
> >
> > I know to simulate a linear mixed-effects longitudinal data in R, I can
> use
> > the procedure below.
> >
> > But I wonder how to generate a similar design, but for a logistic model
> in
> > R?
> >
> > I appreciate your suggestions,
> > Simon
> >
> > #------------- linear mixed-effects longitudinal data in R-------
> >
> > library(MASS)
> >
> > growth.sim <- function(J, n.time, gammas, G, sigma = 1, seed = NULL) {
> >
> >     X <- cbind(1, seq_len(n.time) - 1) # time indicators for each
> individual
> >     X <- X[rep(seq_len(n.time), J), ]  # Repeat each row n.time times
> >
> >    st.id <- seq_len(J)                 # individual id
> >    st.id <- rep(st.id, each = n.time)  # repeat each ID n.time times
> >
> >
> > set.seed(seed) ## what should go below, possibly correlated Beta?
> > uj <- MASS::mvrnorm(J, mu = rep(0, 2), Sigma = G) # Generate u0 and u1
> > random effects for intercepts and slopes
> >
> > ## I think no error term is needed for logistic version?
> > eij <- rnorm(J * n.time, sd = sigma)              # Generate error
> > term for observations
> >
> >
> > betaj <- matrix(gammas, nrow = J, ncol = 2, byrow = TRUE) + uj #
> > Compute beta_j's
> >
> > y <- rowSums(X * betaj[st.id, ]) + eij          # Compute outcome
> > based on Yij = sum(Xij*Bj) + eij:
> >
> > dat <- data.frame(st.id, time = X[ , 2], y) # Output a data frame
> > return(dat)                                 # Return data}
> > *##==== Example of use: =====*
> > growth.sim(10, 4, gammas = c(300, 25),
> >         G = matrix(c(0.1, 0,
> >                      0, 0.01), nrow = 2))
> >
> >          [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> >
> https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-mixed-models&amp;data=02%7C01%7Cd.rizopoulos%40erasmusmc.nl%7Cf46c256b636848749fc608d826a46b4e%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C0%7C637301834909767585&amp;sdata=GjOGQ%2Fv5aMwH8PK%2FuM%2FEeHkElDFwB6%2BaD8UZy2wVdx4%3D&amp;reserved=0
> >
> >       [[alternative HTML version deleted]]
> >
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From u@nhoro@1 @end|ng |rom buckeyem@||@o@u@edu  Mon Jul 13 00:25:26 2020
From: u@nhoro@1 @end|ng |rom buckeyem@||@o@u@edu (Uanhoro, James)
Date: Sun, 12 Jul 2020 22:25:26 +0000
Subject: [R-sig-ME] 
 Simulating mixed-effects longitudinal logistic data in R
In-Reply-To: <CACgv6yUeFYqvqboko+rK0W9vN_Z-iB+L+oSwFh5OP0c6dAbaJg@mail.gmail.com>
References: <CACgv6yULMfqE59C5Mb8Uge93M8AmSF5Vh-Lz-c7JUpxoMUQVHQ@mail.gmail.com>
 <AM6PR04MB587932C9373FC89A63D098C6E8630@AM6PR04MB5879.eurprd04.prod.outlook.com>
 <be70be1c-1aac-7610-8d66-bfaf84c2048d@gmail.com>
 <CACgv6yUeFYqvqboko+rK0W9vN_Z-iB+L+oSwFh5OP0c6dAbaJg@mail.gmail.com>
Message-ID: <2ea2e1ebbead74f2e45a59e727b3262518d194e8.camel@buckeyemail.osu.edu>

You can keep the same exact code with some changes:

- If you want to use logistic regression to recover the population
parameters, set: eij <- rlogis(J * n.time, scale = 1)

- If you use the normal distribution (rnorm), you will need probit
regression to recover population parameters: eij <- rnorm(J * n.time,
sd = 1)

- Note that the sd/scale has to be 1 as the estimated parameters will
be re-scaled by the value of sd(eij)

- Your y then will have to be as below to create a binary variable: y
<- ((rowSums(X * betaj[st.id, ]) + eij) > 0) + 0. Basically, you
dichotomize the generated y at 0. If you dichotomize at some other
value, the estimated intercept will shift accordingly.

- Finally, your population parameters have to make sense of the logit
scale. Intercept values of 300 and coefficients values of 25 are
unbelievable log-odds. You will generate all 1s with these population
parameters.

This is the latent variable approach to setting up the binary outcome
model.

The other answers do not use an error term to set up the model. For
these responses, you generate probabilities using either pnorm(LP)
(probit) or plogis(LP) (logistic) where LP is the linear predictor,
then pass the probabilities to rbinom() to obtain a binary variable.

James.

On Sun, 2020-07-12 at 17:11 -0500, Simon Harmel wrote:
> Many thanks all!  I basically want to generate 100 bernoulli outcomes
> whose
> probability of being 1 is longitudinally modeled based on 4 time
> points
> (0:3), membership to 2 groups (Control vs. treatment), and gender
> (male vs.
> female).
> 
> Thanks a lot!
> 
> On Sun, Jul 12, 2020 at 5:02 PM Ben Bolker <bbolker at gmail.com> wrote:
> 
> >    Or:
> > 
> > 
> > (1)  see ?lme4::simulate.merMod(), i.e. set up your covariates,
> > then
> > simulate(formula, newdata=data_frame, newparams=...,
> > family=binomial,
> > weights=1)
> > 
> > or
> > 
> > (2) after setting everything up as in your example, use
> > 
> > response <- rbinom(length(y), prob=plogis(y), size=1)
> > 
> > On 7/12/20 5:58 PM, D. Rizopoulos wrote:
> > > Have a look in this example:
> > 
> > 
https://urldefense.com/v3/__https://drizopoulos.github.io/GLMMadaptive/articles/GLMMadaptive_basics.html__;!!KGKeukY!jq0K6FHI0BtJ8LoX0xkhk9pv-4RgkiFLMeqSS8izqfz60XgQKODdgFKHcwYytJ5tqjwbaith3Ks$
> >  
> > > 
> > > Best,
> > > Dimitris
> > > 
> > > ??
> > > Dimitris Rizopoulos
> > > Professor of Biostatistics
> > > Erasmus University Medical Center
> > > The Netherlands
> > > ________________________________
> > > From: R-sig-mixed-models <
> > > r-sig-mixed-models-bounces at r-project.org> on
> > 
> > behalf of Simon Harmel <sim.harmel at gmail.com>
> > > Sent: Sunday, July 12, 2020 10:44:17 PM
> > > To: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
> > > Subject: [R-sig-ME] Simulating mixed-effects longitudinal
> > > logistic data
> > 
> > in R
> > > 
> > > Good afternoon,
> > > 
> > > I know to simulate a linear mixed-effects longitudinal data in R,
> > > I can
> > 
> > use
> > > the procedure below.
> > > 
> > > But I wonder how to generate a similar design, but for a logistic
> > > model
> > 
> > in
> > > R?
> > > 
> > > I appreciate your suggestions,
> > > Simon
> > > 
> > > #------------- linear mixed-effects longitudinal data in R-------
> > > 
> > > library(MASS)
> > > 
> > > growth.sim <- function(J, n.time, gammas, G, sigma = 1, seed =
> > > NULL) {
> > > 
> > >     X <- cbind(1, seq_len(n.time) - 1) # time indicators for each
> > 
> > individual
> > >     X <- X[rep(seq_len(n.time), J), ]  # Repeat each row n.time
> > > times
> > > 
> > >    st.id <- seq_len(J)                 # individual id
> > >    st.id <- rep(st.id, each = n.time)  # repeat each ID n.time
> > > times
> > > 
> > > 
> > > set.seed(seed) ## what should go below, possibly correlated Beta?
> > > uj <- MASS::mvrnorm(J, mu = rep(0, 2), Sigma = G) # Generate u0
> > > and u1
> > > random effects for intercepts and slopes
> > > 
> > > ## I think no error term is needed for logistic version?
> > > eij <- rnorm(J * n.time, sd = sigma)              # Generate
> > > error
> > > term for observations
> > > 
> > > 
> > > betaj <- matrix(gammas, nrow = J, ncol = 2, byrow = TRUE) + uj #
> > > Compute beta_j's
> > > 
> > > y <- rowSums(X * betaj[st.id, ]) + eij          # Compute outcome
> > > based on Yij = sum(Xij*Bj) + eij:
> > > 
> > > dat <- data.frame(st.id, time = X[ , 2], y) # Output a data frame
> > > return(dat)                                 # Return data}
> > > *##==== Example of use: =====*
> > > growth.sim(10, 4, gammas = c(300, 25),
> > >         G = matrix(c(0.1, 0,
> > >                      0, 0.01), nrow = 2))
> > > 
> > >          [[alternative HTML version deleted]]
> > > 
> > > _______________________________________________
> > > R-sig-mixed-models at r-project.org mailing list
> > > 
> > 
> > 
https://urldefense.com/v3/__https://eur01.safelinks.protection.outlook.com/?url=https*3A*2F*2Fstat.ethz.ch*2Fmailman*2Flistinfo*2Fr-sig-mixed-models&amp;data=02*7C01*7Cd.rizopoulos*40erasmusmc.nl*7Cf46c256b636848749fc608d826a46b4e*7C526638ba6af34b0fa532a1a511f4ac80*7C0*7C0*7C637301834909767585&amp;sdata=GjOGQ*2Fv5aMwH8PK*2FuM*2FEeHkElDFwB6*2BaD8UZy2wVdx4*3D&amp;reserved=0__;JSUlJSUlJSUlJSUlJSUlJSUlJQ!!KGKeukY!jq0K6FHI0BtJ8LoX0xkhk9pv-4RgkiFLMeqSS8izqfz60XgQKODdgFKHcwYytJ5tqjwbWhOO65s$
> >  
> > > 
> > >       [[alternative HTML version deleted]]
> > > 
> > > 
> > > _______________________________________________
> > > R-sig-mixed-models at r-project.org mailing list
> > > 
https://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models__;!!KGKeukY!jq0K6FHI0BtJ8LoX0xkhk9pv-4RgkiFLMeqSS8izqfz60XgQKODdgFKHcwYytJ5tqjwbOi_tX1Q$
> > >  
> > 
> >         [[alternative HTML version deleted]]
> > 
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > 
https://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models__;!!KGKeukY!jq0K6FHI0BtJ8LoX0xkhk9pv-4RgkiFLMeqSS8izqfz60XgQKODdgFKHcwYytJ5tqjwbOi_tX1Q$
> >  
> > 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> 
https://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models__;!!KGKeukY!jq0K6FHI0BtJ8LoX0xkhk9pv-4RgkiFLMeqSS8izqfz60XgQKODdgFKHcwYytJ5tqjwbOi_tX1Q$
>  

From @ch|@023 @end|ng |rom uott@w@@c@  Sun Jul 12 23:25:57 2020
From: @ch|@023 @end|ng |rom uott@w@@c@ (Sarah Chisholm)
Date: Sun, 12 Jul 2020 17:25:57 -0400
Subject: [R-sig-ME] spaMM::fitme() - a glmm for longitudinal data that
 accounts for spatial autocorrelation
Message-ID: <CAHWECf_ZO0U-BoGSPi8uYj7oBWo1xk4WqxLeHDHgRFZdTXY4bA@mail.gmail.com>

Hello,

I'm trying to fit a GLMM that accounts for spatial autocorrelation (SAC)
using the spaMM::fitme() function in R. I have a longitudinal data set
where observations were collected repeatedly from a number of sites over 13
years. I'm interested in understanding what the effect of time (year) is on
the dependent variable (y), as well as the fixed effect of a categorical
variable (class) while accounting for the random factors biome, continent,
and ID (a unique ID for each site sampled). My full data set contains ~ 180
000 rows and attached is a subset of these data ('sampleDF'). My current
fitme() model looks like this:

library(spaMM)

M1 <- fitme(y ~ year + class + (1|biome) + (1|continent) + (1|ID) +
Matern(1|long + lat), data = df, family = "gaussian", method = "REML")
I have two questions:

1) I'm uncertain if this is an appropriate way of applying the
spaMM::fitme() function to longitudinal data. I have some experience with
fitting GLS models that account for SAC to a longitudinal data set where I
had to group the data set by year using the nlme::groupedData() function
before fitting the model. Does a similar method need to be used in the case
of spaMM:fitme() and longitudinal data?

2) Is there another R package out there that can create a similar model (a
GLMM that accounts for SAC)?. I've found very few resources explaining the
use of functions in the spaMM package other than the user guide (F.
Rousset, 2020. An introduction to the spaMM package for mixed models) and
I'm not quite getting the help that I need from it. I'm wondering if
there's another approach to modeling these data that has a broader user
base and thus more easily accessible resources / online help (ex. stack
exchange / cross validated Qs and As).

Thank you!
Sarah

From @xe|@urb|z @end|ng |rom gm@||@com  Mon Jul 13 18:06:24 2020
From: @xe|@urb|z @end|ng |rom gm@||@com (Axel Urbiz)
Date: Mon, 13 Jul 2020 12:06:24 -0400
Subject: [R-sig-ME] lme4 - Mixed Model question
Message-ID: <8FAFA505-7A87-4198-B74E-8D317575A485@gmail.com>

Dear List, 

I?d appreciate any guidance on the following. 

I?m using a mixed effects logistic regression model, to allow coefficients to vary by a group variable. However, my case is not typical in the sense that I need to specify a different set of covariates for each level of the group variable. Say I have 3 covariates {x1, x2, x3} and 2 groups {g1, g2}. I want to specify a model for g1 that only depends on x1 and x2, and a model for g2 that only depends on x2 and x3. 

Note that the 2 groups is just for illustration. I actually have many more than that. 

Is this possible with lme4?

Thanks,
Axel.

From bbo|ker @end|ng |rom gm@||@com  Mon Jul 13 20:46:35 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 13 Jul 2020 14:46:35 -0400
Subject: [R-sig-ME] lme4 - Mixed Model question
In-Reply-To: <8FAFA505-7A87-4198-B74E-8D317575A485@gmail.com>
References: <8FAFA505-7A87-4198-B74E-8D317575A485@gmail.com>
Message-ID: <520332c3-8755-e0cd-620a-82ff45e64b76@gmail.com>

 ?? I *think* that if you simply set the values of the covariates you 
want to disregard for each group to 0 for that group, you'll get what 
you want.? I don't think there's a way to do that in the formula 
specification; you can obviously write code to do it, but doing it 
elegantly could be tricky.? If I were doing it I might try something 
*approximately* along the lines of

 ?? vars <- list(g1=c("x1","x2"), g2=c("x2","x3"))

 ?? all_vars <- unique(unlist(vars))

 ?? for (i in seq_along(models)) {

 ????? my_data[group==names(models)[i], setdiff(all_vars, vars[[i]])] <- 0

 ?? }


On 7/13/20 12:06 PM, Axel Urbiz wrote:
> Dear List,
>
> I?d appreciate any guidance on the following.
>
> I?m using a mixed effects logistic regression model, to allow coefficients to vary by a group variable. However, my case is not typical in the sense that I need to specify a different set of covariates for each level of the group variable. Say I have 3 covariates {x1, x2, x3} and 2 groups {g1, g2}. I want to specify a model for g1 that only depends on x1 and x2, and a model for g2 that only depends on x2 and x3.
>
> Note that the 2 groups is just for illustration. I actually have many more than that.
>
> Is this possible with lme4?
>
> Thanks,
> Axel.
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From th|erry@onke||nx @end|ng |rom |nbo@be  Mon Jul 13 20:56:04 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Mon, 13 Jul 2020 20:56:04 +0200
Subject: [R-sig-ME] spaMM::fitme() - a glmm for longitudinal data that
 accounts for spatial autocorrelation
In-Reply-To: <CAHWECf_ZO0U-BoGSPi8uYj7oBWo1xk4WqxLeHDHgRFZdTXY4bA@mail.gmail.com>
References: <CAHWECf_ZO0U-BoGSPi8uYj7oBWo1xk4WqxLeHDHgRFZdTXY4bA@mail.gmail.com>
Message-ID: <CAJuCY5xF1+iq-f8JFp0=U6hrdpRNEPVUm3wFfNXkaNph_SFx4w@mail.gmail.com>

Dear Sarah,

I don't know the spaMM package.

Have a look at the inlabru package. It has several tutorials on its website
(inlabru.org). Or the INLA package (r-inla.org). The same models but
inlabru has a more user friendly interface. I can recommend Zuur et al
(2017) Spatial, Temporal and Spatial-Temporal Ecological Data Analysis with
R-INLA

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op ma 13 jul. 2020 om 20:31 schreef Sarah Chisholm <schis023 at uottawa.ca>:

> Hello,
>
> I'm trying to fit a GLMM that accounts for spatial autocorrelation (SAC)
> using the spaMM::fitme() function in R. I have a longitudinal data set
> where observations were collected repeatedly from a number of sites over 13
> years. I'm interested in understanding what the effect of time (year) is on
> the dependent variable (y), as well as the fixed effect of a categorical
> variable (class) while accounting for the random factors biome, continent,
> and ID (a unique ID for each site sampled). My full data set contains ~ 180
> 000 rows and attached is a subset of these data ('sampleDF'). My current
> fitme() model looks like this:
>
> library(spaMM)
>
> M1 <- fitme(y ~ year + class + (1|biome) + (1|continent) + (1|ID) +
> Matern(1|long + lat), data = df, family = "gaussian", method = "REML")
> I have two questions:
>
> 1) I'm uncertain if this is an appropriate way of applying the
> spaMM::fitme() function to longitudinal data. I have some experience with
> fitting GLS models that account for SAC to a longitudinal data set where I
> had to group the data set by year using the nlme::groupedData() function
> before fitting the model. Does a similar method need to be used in the case
> of spaMM:fitme() and longitudinal data?
>
> 2) Is there another R package out there that can create a similar model (a
> GLMM that accounts for SAC)?. I've found very few resources explaining the
> use of functions in the spaMM package other than the user guide (F.
> Rousset, 2020. An introduction to the spaMM package for mixed models) and
> I'm not quite getting the help that I need from it. I'm wondering if
> there's another approach to modeling these data that has a broader user
> base and thus more easily accessible resources / online help (ex. stack
> exchange / cross validated Qs and As).
>
> Thank you!
> Sarah
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From |r@nco|@@rou@@et @end|ng |rom umontpe|||er@|r  Mon Jul 13 22:01:05 2020
From: |r@nco|@@rou@@et @end|ng |rom umontpe|||er@|r (Francois Rousset)
Date: Mon, 13 Jul 2020 22:01:05 +0200
Subject: [R-sig-ME] spaMM::fitme() - a glmm for longitudinal data that
 accounts for spatial autocorrelation
In-Reply-To: <CAHWECf_ZO0U-BoGSPi8uYj7oBWo1xk4WqxLeHDHgRFZdTXY4bA@mail.gmail.com>
References: <CAHWECf_ZO0U-BoGSPi8uYj7oBWo1xk4WqxLeHDHgRFZdTXY4bA@mail.gmail.com>
Message-ID: <5e1e1d3a-90ca-c818-6847-ba514773dbc7@umontpellier.fr>

Dear Sarah,

perhaps try to contact that package's author directly...

That being said, I am not quite sure what the question is, maybe because 
I am not familiar with constraints on the models nlme can fit and with 
its syntax. What would be the formula you would use with glmer if there 
were no spatial random effect?

Best,

F.

Le 12/07/2020 ? 23:25, Sarah Chisholm a ?crit?:
> Hello,
>
> I'm trying to fit a GLMM that accounts for spatial autocorrelation (SAC)
> using the spaMM::fitme() function in R. I have a longitudinal data set
> where observations were collected repeatedly from a number of sites over 13
> years. I'm interested in understanding what the effect of time (year) is on
> the dependent variable (y), as well as the fixed effect of a categorical
> variable (class) while accounting for the random factors biome, continent,
> and ID (a unique ID for each site sampled). My full data set contains ~ 180
> 000 rows and attached is a subset of these data ('sampleDF'). My current
> fitme() model looks like this:
>
> library(spaMM)
>
> M1 <- fitme(y ~ year + class + (1|biome) + (1|continent) + (1|ID) +
> Matern(1|long + lat), data = df, family = "gaussian", method = "REML")
> I have two questions:
>
> 1) I'm uncertain if this is an appropriate way of applying the
> spaMM::fitme() function to longitudinal data. I have some experience with
> fitting GLS models that account for SAC to a longitudinal data set where I
> had to group the data set by year using the nlme::groupedData() function
> before fitting the model. Does a similar method need to be used in the case
> of spaMM:fitme() and longitudinal data?
>
> 2) Is there another R package out there that can create a similar model (a
> GLMM that accounts for SAC)?. I've found very few resources explaining the
> use of functions in the spaMM package other than the user guide (F.
> Rousset, 2020. An introduction to the spaMM package for mixed models) and
> I'm not quite getting the help that I need from it. I'm wondering if
> there's another approach to modeling these data that has a broader user
> base and thus more easily accessible resources / online help (ex. stack
> exchange / cross validated Qs and As).
>
> Thank you!
> Sarah
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From @ch|@023 @end|ng |rom uott@w@@c@  Mon Jul 13 22:42:53 2020
From: @ch|@023 @end|ng |rom uott@w@@c@ (Sarah Chisholm)
Date: Mon, 13 Jul 2020 16:42:53 -0400
Subject: [R-sig-ME] spaMM::fitme() - a glmm for longitudinal data that
 accounts for spatial autocorrelation
In-Reply-To: <5e1e1d3a-90ca-c818-6847-ba514773dbc7@umontpellier.fr>
References: <CAHWECf_ZO0U-BoGSPi8uYj7oBWo1xk4WqxLeHDHgRFZdTXY4bA@mail.gmail.com>
 <5e1e1d3a-90ca-c818-6847-ba514773dbc7@umontpellier.fr>
Message-ID: <CAHWECf_Y8sN3ccMMoyUhJBCjTGKssZj+2uibM+HCU3-cfjzFrg@mail.gmail.com>

Thank you both for your reply.

Thierry, the inlabru package sounds interesting. However, I should have
mentioned that I'm not very familiar with Bayesian statistics and would
prefer to use other methods if possible.

Francois, I apologize for not contacting you directly first. To clarify my
question, when using the nlme::gls() function with longitudinal data, it is
necessary to group the data first. I'm *pretty sure* this is to avoid
having distances of zero in the corSpatial object, although I'm not
entirely sure of the details of fitting this model.

What I'm wondering is, will the fitme() function recognize that there are
repeated measurements through time on the same sites (and thus, duplicates
of the sites' coordinate points in the data set) to avoid calculating
distances of zero between the same site from different years. If I were to
use lme4::lmer (for a normally distributed response variable) without a
spatial random effect, the model would look like this:

M1 <- lmer(y ~ year + class + (1| biome ) + (1| continent ) + (1|ID) , data
= df, family = "gaussian" , REML = TRUE)

Thanks so much,

Sarah

On Mon, Jul 13, 2020 at 4:01 PM Francois Rousset <
francois.rousset at umontpellier.fr> wrote:

> Dear Sarah,
>
> perhaps try to contact that package's author directly...
>
> That being said, I am not quite sure what the question is, maybe because
> I am not familiar with constraints on the models nlme can fit and with
> its syntax. What would be the formula you would use with glmer if there
> were no spatial random effect?
>
> Best,
>
> F.
>
> Le 12/07/2020 ? 23:25, Sarah Chisholm a ?crit :
> > Hello,
> >
> > I'm trying to fit a GLMM that accounts for spatial autocorrelation (SAC)
> > using the spaMM::fitme() function in R. I have a longitudinal data set
> > where observations were collected repeatedly from a number of sites over
> 13
> > years. I'm interested in understanding what the effect of time (year) is
> on
> > the dependent variable (y), as well as the fixed effect of a categorical
> > variable (class) while accounting for the random factors biome,
> continent,
> > and ID (a unique ID for each site sampled). My full data set contains ~
> 180
> > 000 rows and attached is a subset of these data ('sampleDF'). My current
> > fitme() model looks like this:
> >
> > library(spaMM)
> >
> > M1 <- fitme(y ~ year + class + (1|biome) + (1|continent) + (1|ID) +
> > Matern(1|long + lat), data = df, family = "gaussian", method = "REML")
> > I have two questions:
> >
> > 1) I'm uncertain if this is an appropriate way of applying the
> > spaMM::fitme() function to longitudinal data. I have some experience with
> > fitting GLS models that account for SAC to a longitudinal data set where
> I
> > had to group the data set by year using the nlme::groupedData() function
> > before fitting the model. Does a similar method need to be used in the
> case
> > of spaMM:fitme() and longitudinal data?
> >
> > 2) Is there another R package out there that can create a similar model
> (a
> > GLMM that accounts for SAC)?. I've found very few resources explaining
> the
> > use of functions in the spaMM package other than the user guide (F.
> > Rousset, 2020. An introduction to the spaMM package for mixed models) and
> > I'm not quite getting the help that I need from it. I'm wondering if
> > there's another approach to modeling these data that has a broader user
> > base and thus more easily accessible resources / online help (ex. stack
> > exchange / cross validated Qs and As).
> >
> > Thank you!
> > Sarah
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


-- 
Sarah Chisholm
MSc Candidate
Department of Biology
University of Ottawa
Linkedin <http://www.linkedin.com/in/sarah-chisholm-422a5785>

	[[alternative HTML version deleted]]


From |r@nco|@@rou@@et @end|ng |rom umontpe|||er@|r  Tue Jul 14 00:06:50 2020
From: |r@nco|@@rou@@et @end|ng |rom umontpe|||er@|r (Francois Rousset)
Date: Tue, 14 Jul 2020 00:06:50 +0200
Subject: [R-sig-ME] spaMM::fitme() - a glmm for longitudinal data that
 accounts for spatial autocorrelation
In-Reply-To: <CAHWECf_Y8sN3ccMMoyUhJBCjTGKssZj+2uibM+HCU3-cfjzFrg@mail.gmail.com>
References: <CAHWECf_ZO0U-BoGSPi8uYj7oBWo1xk4WqxLeHDHgRFZdTXY4bA@mail.gmail.com>
 <5e1e1d3a-90ca-c818-6847-ba514773dbc7@umontpellier.fr>
 <CAHWECf_Y8sN3ccMMoyUhJBCjTGKssZj+2uibM+HCU3-cfjzFrg@mail.gmail.com>
Message-ID: <2be5ba0d-4b29-3287-f219-759088a164a0@umontpellier.fr>


Le 13/07/2020 ? 22:42, Sarah Chisholm a ?crit?:
> Thank you both?for your reply.
>
> Thierry, the inlabru package sounds interesting. However, I should 
> have mentioned that I'm not very familiar with Bayesian statistics and 
> would prefer to use other methods if possible.
>
> Francois, I apologize?for not contacting you directly first. To 
> clarify my question, when using the nlme::gls() function with 
> longitudinal data, it is necessary to group the data first. I'm 
> *pretty sure* this is to avoid having distances of zero in the 
> corSpatial object,?although I'm not entirely sure of the?details of 
> fitting this model.
>
> What I'm wondering is, will the fitme() function recognize that there 
> are repeated measurements through time on the same sites (and thus, 
> duplicates of the sites' coordinate points in the data set)
yes it does
> to avoid calculating distances of zero between the same site from 
> different years.

Internally, spaMM avoids zero distances (or rather, the singularities 
that would occur if different rows of a distance matrix represented the 
same location) by handling a distance matrix only among distinct spatial 
locations in the data.? There is no need to declare something like 
nlme::groupedData() to achieve this, and your call to spaMM::fitme() is 
OK. If there are non-zero but very close locations in the data, 
near-singularities may occur but spaMM also tries to deal with them 
automatically.

Best,

F.

> If I were to use lme4::lmer (for a normally distributed response 
> variable) without a spatial random effect, the model would look like this:
>
> M1 <- lmer(y ~ year + class + (1| biome ) + (1| continent ) + (1|ID) , 
> data = df, family = "gaussian" , REML = TRUE)
>
> Thanks so much,
>
> Sarah
>
> On Mon, Jul 13, 2020 at 4:01 PM Francois Rousset 
> <francois.rousset at umontpellier.fr 
> <mailto:francois.rousset at umontpellier.fr>> wrote:
>
>     Dear Sarah,
>
>     perhaps try to contact that package's author directly...
>
>     That being said, I am not quite sure what the question is, maybe
>     because
>     I am not familiar with constraints on the models nlme can fit and
>     with
>     its syntax. What would be the formula you would use with glmer if
>     there
>     were no spatial random effect?
>
>     Best,
>
>     F.
>
>     Le 12/07/2020 ? 23:25, Sarah Chisholm a ?crit?:
>     > Hello,
>     >
>     > I'm trying to fit a GLMM that accounts for spatial
>     autocorrelation (SAC)
>     > using the spaMM::fitme() function in R. I have a longitudinal
>     data set
>     > where observations were collected repeatedly from a number of
>     sites over 13
>     > years. I'm interested in understanding what the effect of time
>     (year) is on
>     > the dependent variable (y), as well as the fixed effect of a
>     categorical
>     > variable (class) while accounting for the random factors biome,
>     continent,
>     > and ID (a unique ID for each site sampled). My full data set
>     contains ~ 180
>     > 000 rows and attached is a subset of these data ('sampleDF'). My
>     current
>     > fitme() model looks like this:
>     >
>     > library(spaMM)
>     >
>     > M1 <- fitme(y ~ year + class + (1|biome) + (1|continent) + (1|ID) +
>     > Matern(1|long + lat), data = df, family = "gaussian", method =
>     "REML")
>     > I have two questions:
>     >
>     > 1) I'm uncertain if this is an appropriate way of applying the
>     > spaMM::fitme() function to longitudinal data. I have some
>     experience with
>     > fitting GLS models that account for SAC to a longitudinal data
>     set where I
>     > had to group the data set by year using the nlme::groupedData()
>     function
>     > before fitting the model. Does a similar method need to be used
>     in the case
>     > of spaMM:fitme() and longitudinal data?
>     >
>     > 2) Is there another R package out there that can create a
>     similar model (a
>     > GLMM that accounts for SAC)?. I've found very few resources
>     explaining the
>     > use of functions in the spaMM package other than the user guide (F.
>     > Rousset, 2020. An introduction to the spaMM package for mixed
>     models) and
>     > I'm not quite getting the help that I need from it. I'm wondering if
>     > there's another approach to modeling these data that has a
>     broader user
>     > base and thus more easily accessible resources / online help
>     (ex. stack
>     > exchange / cross validated Qs and As).
>     >
>     > Thank you!
>     > Sarah
>     > _______________________________________________
>     > R-sig-mixed-models at r-project.org
>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>
> -- 
> Sarah Chisholm
> MSc Candidate
> Department of Biology
> University of Ottawa
> Linkedin <http://www.linkedin.com/in/sarah-chisholm-422a5785>

	[[alternative HTML version deleted]]


From h|gh@t@t @end|ng |rom h|gh@t@t@com  Tue Jul 14 11:15:35 2020
From: h|gh@t@t @end|ng |rom h|gh@t@t@com (Highland Statistics Ltd)
Date: Tue, 14 Jul 2020 10:15:35 +0100
Subject: [R-sig-ME] Online course: Introduction to Regression Models with
 Spatial Correlation using R-INLA
Message-ID: <51d8adfc-797f-5a58-e71b-0afa6921045e@highstat.com>

We would like to announce the following online statistics course:

Introduction to Regression Models with Spatial Correlation using R-INLA


This is an on-demand course with around 30 videos (each is 15-60 
minutes) with live (optional) Zoom summary sessions scheduled in 2 
different time zones:

  * Time zone 1: 09.00-11.00 British Summer Time.
  * Time zone 2: 19.00-21.00 British Summer Time.

The course represents around 40 hours of work.

The course fee includes an (optional) 1-hour face-to-face video chat? 
with one or both instructors (you can discuss your own data).

Starting date: Live summary session using Zoom will run between 20 July 
- 7 August 2020.

Flyer: 
http://highstat.com/Courses/Flyers/2020/Flyer2020_07_SpatialGLM_Online.pdf

Website: http://highstat.com/index.php/courses-upcoming


Kind regards,


Alain Zuur

-- 

Dr. Alain F. Zuur
Highland Statistics Ltd.
9 St Clair Wynd
AB41 6DZ Newburgh, UK
Email:highstat at highstat.com
URL:www.highstat.com


	[[alternative HTML version deleted]]


From mo|||eebrook@ @end|ng |rom gm@||@com  Tue Jul 14 12:25:11 2020
From: mo|||eebrook@ @end|ng |rom gm@||@com (Mollie Brooks)
Date: Tue, 14 Jul 2020 12:25:11 +0200
Subject: [R-sig-ME] spaMM::fitme() - a glmm for longitudinal data that
 accounts for spatial autocorrelation
In-Reply-To: <CAHWECf_ZO0U-BoGSPi8uYj7oBWo1xk4WqxLeHDHgRFZdTXY4bA@mail.gmail.com>
References: <CAHWECf_ZO0U-BoGSPi8uYj7oBWo1xk4WqxLeHDHgRFZdTXY4bA@mail.gmail.com>
Message-ID: <CAMu=eMAVy-=hmjWaYcBMtyNS8QHS4jm_nFPp3A+hhQYfigR9bQ@mail.gmail.com>

Hi Sarah,

Sorry my reply is a bit late, but I think you could also fit matern and
other spatial correlation structures via glmmTMB. They are documented in
this vignette
https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html

I think the code might be something like

df2 <- transform(df,
    pos = numFactor(lat, long),
    group = factor(1)
)

M1 <- glmmTMB(y ~ year + class + (1|biome) + (1|continent) + (1|ID) +
matern(pos+0 | group), dispformula=~0, data = df2, REML=TRUE)

I don't have the most experience with this type of model, so maybe someone
else has more advice to give.

cheers,
Mollie

On Mon, Jul 13, 2020 at 8:31 PM Sarah Chisholm <schis023 at uottawa.ca> wrote:

> Hello,
>
> I'm trying to fit a GLMM that accounts for spatial autocorrelation (SAC)
> using the spaMM::fitme() function in R. I have a longitudinal data set
> where observations were collected repeatedly from a number of sites over 13
> years. I'm interested in understanding what the effect of time (year) is on
> the dependent variable (y), as well as the fixed effect of a categorical
> variable (class) while accounting for the random factors biome, continent,
> and ID (a unique ID for each site sampled). My full data set contains ~ 180
> 000 rows and attached is a subset of these data ('sampleDF'). My current
> fitme() model looks like this:
>
> library(spaMM)
>
> M1 <- fitme(y ~ year + class + (1|biome) + (1|continent) + (1|ID) +
> Matern(1|long + lat), data = df, family = "gaussian", method = "REML")
> I have two questions:
>
> 1) I'm uncertain if this is an appropriate way of applying the
> spaMM::fitme() function to longitudinal data. I have some experience with
> fitting GLS models that account for SAC to a longitudinal data set where I
> had to group the data set by year using the nlme::groupedData() function
> before fitting the model. Does a similar method need to be used in the case
> of spaMM:fitme() and longitudinal data?
>
> 2) Is there another R package out there that can create a similar model (a
> GLMM that accounts for SAC)?. I've found very few resources explaining the
> use of functions in the spaMM package other than the user guide (F.
> Rousset, 2020. An introduction to the spaMM package for mixed models) and
> I'm not quite getting the help that I need from it. I'm wondering if
> there's another approach to modeling these data that has a broader user
> base and thus more easily accessible resources / online help (ex. stack
> exchange / cross validated Qs and As).
>
> Thank you!
> Sarah
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From @ch|@023 @end|ng |rom uott@w@@c@  Tue Jul 14 16:55:12 2020
From: @ch|@023 @end|ng |rom uott@w@@c@ (Sarah Chisholm)
Date: Tue, 14 Jul 2020 10:55:12 -0400
Subject: [R-sig-ME] spaMM::fitme() - a glmm for longitudinal data that
 accounts for spatial autocorrelation
In-Reply-To: <CAMu=eMAVy-=hmjWaYcBMtyNS8QHS4jm_nFPp3A+hhQYfigR9bQ@mail.gmail.com>
References: <CAHWECf_ZO0U-BoGSPi8uYj7oBWo1xk4WqxLeHDHgRFZdTXY4bA@mail.gmail.com>
 <CAMu=eMAVy-=hmjWaYcBMtyNS8QHS4jm_nFPp3A+hhQYfigR9bQ@mail.gmail.com>
Message-ID: <CAHWECf9Hcn3KH7csDg337mkgQYHcDdzoxAEAx+tehuYo88iBwQ@mail.gmail.com>

Hi Mollie, thank you for your suggestion. glmmTMB seems like a good option
for my needs as well. In your sample code above, can you explain what the
term 'group' does in matern(pos+0|group)? Does this allow the spatial
correlation structure to be applied to specific groupings in the data (in
my case, for example, by 'continent')?

Francois, thank you for this very clear answer. This is a very convenient
feature of the function! May I ask you a couple of other questions about
some issues that I've had with spaMM::fitme()?

In particular, when I try fitting this model to a large data set (~14 000
rows x 7 columns, ~2 MB), the model will run for an extended period of
time, to the point where I've had to terminate the computation. I've tried
applying the suggestions that are mentioned in the user guide, i.e.
setting init=list(lambda=0.1) and init=list(lambda=NaN). Implementing
init=list(lambda=0.1) returned an error suggesting that there was a lack of
memory, while running the model with init=list(lambda=NaN) also ran for an
extended period of time without completing. Is there something else I can
do to speed up the fit of these models?

I've had a similar problem with an even larger data set (~185 000 rows x 8
columns, ~21 MB), where, when I try running the model, this error is
returned immediately:

Error in ZA %*% xmatrix : Cholmod error 'problem too large' at file ../Core/
cholmod_dense.c, line 105

I've tried running this model on two devices, both with a 64-bit OS with
Windows 10, one with 32 GB of RAM and the other with 64 GB. I've gotten the
same error from both devices. Is there a way that fitme() can accommodate
these large data sets?

Thank you,

Sarah

	[[alternative HTML version deleted]]


From |r@nco|@@rou@@et @end|ng |rom umontpe|||er@|r  Tue Jul 14 18:21:56 2020
From: |r@nco|@@rou@@et @end|ng |rom umontpe|||er@|r (Francois Rousset)
Date: Tue, 14 Jul 2020 18:21:56 +0200
Subject: [R-sig-ME] spaMM::fitme() - a glmm for longitudinal data that
 accounts for spatial autocorrelation
In-Reply-To: <CAHWECf9Hcn3KH7csDg337mkgQYHcDdzoxAEAx+tehuYo88iBwQ@mail.gmail.com>
References: <CAHWECf_ZO0U-BoGSPi8uYj7oBWo1xk4WqxLeHDHgRFZdTXY4bA@mail.gmail.com>
 <CAMu=eMAVy-=hmjWaYcBMtyNS8QHS4jm_nFPp3A+hhQYfigR9bQ@mail.gmail.com>
 <CAHWECf9Hcn3KH7csDg337mkgQYHcDdzoxAEAx+tehuYo88iBwQ@mail.gmail.com>
Message-ID: <6707c79b-c372-3364-f8ff-05827f7b27c2@umontpellier.fr>

Dear Sarah,

Le 14/07/2020 ? 16:55, Sarah Chisholm a ?crit?:
> Hi Mollie, thank you for your suggestion. glmmTMB seems like a good 
> option for my needs as well. In your sample code above, can you 
> explain what the term 'group' does in matern(pos+0|group)? Does this 
> allow the spatial correlation structure to be applied to specific 
> groupings in the data (in my case, for example, by 'continent')?
>
> Francois, thank you for this very clear answer. This is a very 
> convenient feature of the function! May I ask you a couple of other 
> questions about some issues that I've had with spaMM::fitme()?
>
> In particular, when I try fitting this model to a large data set (~14 
> 000 rows x 7 columns, ~2 MB), the model will run for an extended 
> period of time, to the point where I've had to terminate the 
> computation. I've tried applying the suggestions that are mentioned in 
> the user guide, i.e. setting?init=list(lambda=0.1) 
> and?init=list(lambda=NaN). Implementing init=list(lambda=0.1) returned 
> an error suggesting that there was a lack of memory, while running the 
> model with init=list(lambda=NaN) also ran for an extended period of 
> time without completing. Is there something else I can do to speed up 
> the fit of these models?
>
> I've had a similar problem with an even larger data set (~185 000 rows 
> x 8 columns, ~21 MB), where, when I try running the model, this error 
> is returned immediately:
>
> ErrorinZA %*%xmatrix :Cholmoderror 'problem too large'at file 
> ../Core/cholmod_dense.c,line 105
>
> I've tried running this model on two devices, both with a 64-bit OS 
> with Windows 10, one with 32 GB of RAM and the other with 64 GB. I've 
> gotten the same error from both devices. Is there a way that fitme() 
> can accommodate these large data sets?

spaMM can handle large data sets, but the first issue to consider here 
is the number of distinct locations for the spatial random effect. The 
large correlation matrices of geostatistical models will always be a 
problem, both in terms of memory requirements and of potentially huge 
computation times. My guess from past experiments is that one should 
still be able to fit models with ~ 10K locations within a few days on a 
computer with <60 Gb of RAM (given perhaps some tinkering of the 
arguments), so at least the data set of 14 000 rows should be feasible, 
particularly if the number of locations is smaller.

Anyone planning to analyze large spatial data sets should anticipate 
these problems and check by themselves whether there is any practical 
alternative suitable for their particular problem. The discussion in 
section 6.2 of the "gentle introduction" to spaMM may then be useful.

Best,

F.

>
> Thank you,
>
> Sarah

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Tue Jul 14 20:00:09 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Tue, 14 Jul 2020 20:00:09 +0200
Subject: [R-sig-ME] spaMM::fitme() - a glmm for longitudinal data that
 accounts for spatial autocorrelation
In-Reply-To: <6707c79b-c372-3364-f8ff-05827f7b27c2@umontpellier.fr>
References: <CAHWECf_ZO0U-BoGSPi8uYj7oBWo1xk4WqxLeHDHgRFZdTXY4bA@mail.gmail.com>
 <CAMu=eMAVy-=hmjWaYcBMtyNS8QHS4jm_nFPp3A+hhQYfigR9bQ@mail.gmail.com>
 <CAHWECf9Hcn3KH7csDg337mkgQYHcDdzoxAEAx+tehuYo88iBwQ@mail.gmail.com>
 <6707c79b-c372-3364-f8ff-05827f7b27c2@umontpellier.fr>
Message-ID: <CAJuCY5wrFCzisy_gnu=0ooVijCyMBhr=tS3yNGLP2S3pWAH6+Q@mail.gmail.com>

Dear Fran?ois and Sarah,

INLA seems more efficient. I ran a model with Mattern correlation structure
on 13K locations (1 observation per location) in under 10 minutes on a
laptop with 16GB RAM.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op di 14 jul. 2020 om 18:22 schreef Francois Rousset <
francois.rousset at umontpellier.fr>:

> Dear Sarah,
>
> Le 14/07/2020 ? 16:55, Sarah Chisholm a ?crit :
> > Hi Mollie, thank you for your suggestion. glmmTMB seems like a good
> > option for my needs as well. In your sample code above, can you
> > explain what the term 'group' does in matern(pos+0|group)? Does this
> > allow the spatial correlation structure to be applied to specific
> > groupings in the data (in my case, for example, by 'continent')?
> >
> > Francois, thank you for this very clear answer. This is a very
> > convenient feature of the function! May I ask you a couple of other
> > questions about some issues that I've had with spaMM::fitme()?
> >
> > In particular, when I try fitting this model to a large data set (~14
> > 000 rows x 7 columns, ~2 MB), the model will run for an extended
> > period of time, to the point where I've had to terminate the
> > computation. I've tried applying the suggestions that are mentioned in
> > the user guide, i.e. setting init=list(lambda=0.1)
> > and init=list(lambda=NaN). Implementing init=list(lambda=0.1) returned
> > an error suggesting that there was a lack of memory, while running the
> > model with init=list(lambda=NaN) also ran for an extended period of
> > time without completing. Is there something else I can do to speed up
> > the fit of these models?
> >
> > I've had a similar problem with an even larger data set (~185 000 rows
> > x 8 columns, ~21 MB), where, when I try running the model, this error
> > is returned immediately:
> >
> > ErrorinZA %*%xmatrix :Cholmoderror 'problem too large'at file
> > ../Core/cholmod_dense.c,line 105
> >
> > I've tried running this model on two devices, both with a 64-bit OS
> > with Windows 10, one with 32 GB of RAM and the other with 64 GB. I've
> > gotten the same error from both devices. Is there a way that fitme()
> > can accommodate these large data sets?
>
> spaMM can handle large data sets, but the first issue to consider here
> is the number of distinct locations for the spatial random effect. The
> large correlation matrices of geostatistical models will always be a
> problem, both in terms of memory requirements and of potentially huge
> computation times. My guess from past experiments is that one should
> still be able to fit models with ~ 10K locations within a few days on a
> computer with <60 Gb of RAM (given perhaps some tinkering of the
> arguments), so at least the data set of 14 000 rows should be feasible,
> particularly if the number of locations is smaller.
>
> Anyone planning to analyze large spatial data sets should anticipate
> these problems and check by themselves whether there is any practical
> alternative suitable for their particular problem. The discussion in
> section 6.2 of the "gentle introduction" to spaMM may then be useful.
>
> Best,
>
> F.
>
> >
> > Thank you,
> >
> > Sarah
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From |r@nco|@@rou@@et @end|ng |rom umontpe|||er@|r  Wed Jul 15 00:10:26 2020
From: |r@nco|@@rou@@et @end|ng |rom umontpe|||er@|r (Francois Rousset)
Date: Wed, 15 Jul 2020 00:10:26 +0200
Subject: [R-sig-ME] spaMM::fitme() - a glmm for longitudinal data that
 accounts for spatial autocorrelation
In-Reply-To: <CAJuCY5wrFCzisy_gnu=0ooVijCyMBhr=tS3yNGLP2S3pWAH6+Q@mail.gmail.com>
References: <CAHWECf_ZO0U-BoGSPi8uYj7oBWo1xk4WqxLeHDHgRFZdTXY4bA@mail.gmail.com>
 <CAMu=eMAVy-=hmjWaYcBMtyNS8QHS4jm_nFPp3A+hhQYfigR9bQ@mail.gmail.com>
 <CAHWECf9Hcn3KH7csDg337mkgQYHcDdzoxAEAx+tehuYo88iBwQ@mail.gmail.com>
 <6707c79b-c372-3364-f8ff-05827f7b27c2@umontpellier.fr>
 <CAJuCY5wrFCzisy_gnu=0ooVijCyMBhr=tS3yNGLP2S3pWAH6+Q@mail.gmail.com>
Message-ID: <e7c732ce-2615-23c6-874d-89afbe7d8946@umontpellier.fr>

Dear Thierry,

please provide a reproducible example so that we know what you have 
actually done.

Best,

F.

Le 14/07/2020 ? 20:00, Thierry Onkelinx a ?crit?:
> Dear Fran?ois and Sarah,
>
> INLA seems more efficient. I ran a model with Mattern correlation 
> structure on 13K locations (1 observation per location) in under 10 
> minutes on a laptop with 16GB RAM.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE 
> AND FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be <http://www.inbo.be>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no 
> more than asking him to perform a post-mortem examination: he may be 
> able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does 
> not ensure that a reasonable answer can be extracted from a given body 
> of data. ~ John Tukey
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op di 14 jul. 2020 om 18:22 schreef Francois Rousset 
> <francois.rousset at umontpellier.fr 
> <mailto:francois.rousset at umontpellier.fr>>:
>
>     Dear Sarah,
>
>     Le 14/07/2020 ? 16:55, Sarah Chisholm a ?crit?:
>     > Hi Mollie, thank you for your suggestion. glmmTMB seems like a good
>     > option for my needs as well. In your sample code above, can you
>     > explain what the term 'group' does in matern(pos+0|group)? Does
>     this
>     > allow the spatial correlation structure to be applied to specific
>     > groupings in the data (in my case, for example, by 'continent')?
>     >
>     > Francois, thank you for this very clear answer. This is a very
>     > convenient feature of the function! May I ask you a couple of other
>     > questions about some issues that I've had with spaMM::fitme()?
>     >
>     > In particular, when I try fitting this model to a large data set
>     (~14
>     > 000 rows x 7 columns, ~2 MB), the model will run for an extended
>     > period of time, to the point where I've had to terminate the
>     > computation. I've tried applying the suggestions that are
>     mentioned in
>     > the user guide, i.e. setting?init=list(lambda=0.1)
>     > and?init=list(lambda=NaN). Implementing init=list(lambda=0.1)
>     returned
>     > an error suggesting that there was a lack of memory, while
>     running the
>     > model with init=list(lambda=NaN) also ran for an extended period of
>     > time without completing. Is there something else I can do to
>     speed up
>     > the fit of these models?
>     >
>     > I've had a similar problem with an even larger data set (~185
>     000 rows
>     > x 8 columns, ~21 MB), where, when I try running the model, this
>     error
>     > is returned immediately:
>     >
>     > ErrorinZA %*%xmatrix :Cholmoderror 'problem too large'at file
>     > ../Core/cholmod_dense.c,line 105
>     >
>     > I've tried running this model on two devices, both with a 64-bit OS
>     > with Windows 10, one with 32 GB of RAM and the other with 64 GB.
>     I've
>     > gotten the same error from both devices. Is there a way that
>     fitme()
>     > can accommodate these large data sets?
>
>     spaMM can handle large data sets, but the first issue to consider
>     here
>     is the number of distinct locations for the spatial random effect.
>     The
>     large correlation matrices of geostatistical models will always be a
>     problem, both in terms of memory requirements and of potentially huge
>     computation times. My guess from past experiments is that one should
>     still be able to fit models with ~ 10K locations within a few days
>     on a
>     computer with <60 Gb of RAM (given perhaps some tinkering of the
>     arguments), so at least the data set of 14 000 rows should be
>     feasible,
>     particularly if the number of locations is smaller.
>
>     Anyone planning to analyze large spatial data sets should anticipate
>     these problems and check by themselves whether there is any practical
>     alternative suitable for their particular problem. The discussion in
>     section 6.2 of the "gentle introduction" to spaMM may then be useful.
>
>     Best,
>
>     F.
>
>     >
>     > Thank you,
>     >
>     > Sarah
>
>     ? ? ? ? [[alternative HTML version deleted]]
>
>     _______________________________________________
>     R-sig-mixed-models at r-project.org
>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>     https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Wed Jul 15 12:50:15 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Wed, 15 Jul 2020 12:50:15 +0200
Subject: [R-sig-ME] spaMM::fitme() - a glmm for longitudinal data that
 accounts for spatial autocorrelation
In-Reply-To: <e7c732ce-2615-23c6-874d-89afbe7d8946@umontpellier.fr>
References: <CAHWECf_ZO0U-BoGSPi8uYj7oBWo1xk4WqxLeHDHgRFZdTXY4bA@mail.gmail.com>
 <CAMu=eMAVy-=hmjWaYcBMtyNS8QHS4jm_nFPp3A+hhQYfigR9bQ@mail.gmail.com>
 <CAHWECf9Hcn3KH7csDg337mkgQYHcDdzoxAEAx+tehuYo88iBwQ@mail.gmail.com>
 <6707c79b-c372-3364-f8ff-05827f7b27c2@umontpellier.fr>
 <CAJuCY5wrFCzisy_gnu=0ooVijCyMBhr=tS3yNGLP2S3pWAH6+Q@mail.gmail.com>
 <e7c732ce-2615-23c6-874d-89afbe7d8946@umontpellier.fr>
Message-ID: <CAJuCY5w=jRoYUzdmynCPFc8Ec03OFXzpG+KhrvSsWJhCpy9Ocw@mail.gmail.com>

Dear Fran?ois,

Here you go:
https://drive.google.com/drive/folders/1Ocq88Yq9u_lM-loayRQlMyBS2HLy_Tio
Almost 30K locations. Fit in little over 7 min on my laptop with 16 GB RAM.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op wo 15 jul. 2020 om 00:10 schreef Francois Rousset <
francois.rousset at umontpellier.fr>:

> Dear Thierry,
>
> please provide a reproducible example so that we know what you have
> actually done.
>
> Best,
>
> F.
> Le 14/07/2020 ? 20:00, Thierry Onkelinx a ?crit :
>
> Dear Fran?ois and Sarah,
>
> INLA seems more efficient. I ran a model with Mattern correlation
> structure on 13K locations (1 observation per location) in under 10 minutes
> on a laptop with 16GB RAM.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op di 14 jul. 2020 om 18:22 schreef Francois Rousset <
> francois.rousset at umontpellier.fr>:
>
>> Dear Sarah,
>>
>> Le 14/07/2020 ? 16:55, Sarah Chisholm a ?crit :
>> > Hi Mollie, thank you for your suggestion. glmmTMB seems like a good
>> > option for my needs as well. In your sample code above, can you
>> > explain what the term 'group' does in matern(pos+0|group)? Does this
>> > allow the spatial correlation structure to be applied to specific
>> > groupings in the data (in my case, for example, by 'continent')?
>> >
>> > Francois, thank you for this very clear answer. This is a very
>> > convenient feature of the function! May I ask you a couple of other
>> > questions about some issues that I've had with spaMM::fitme()?
>> >
>> > In particular, when I try fitting this model to a large data set (~14
>> > 000 rows x 7 columns, ~2 MB), the model will run for an extended
>> > period of time, to the point where I've had to terminate the
>> > computation. I've tried applying the suggestions that are mentioned in
>> > the user guide, i.e. setting init=list(lambda=0.1)
>> > and init=list(lambda=NaN). Implementing init=list(lambda=0.1) returned
>> > an error suggesting that there was a lack of memory, while running the
>> > model with init=list(lambda=NaN) also ran for an extended period of
>> > time without completing. Is there something else I can do to speed up
>> > the fit of these models?
>> >
>> > I've had a similar problem with an even larger data set (~185 000 rows
>> > x 8 columns, ~21 MB), where, when I try running the model, this error
>> > is returned immediately:
>> >
>> > ErrorinZA %*%xmatrix :Cholmoderror 'problem too large'at file
>> > ../Core/cholmod_dense.c,line 105
>> >
>> > I've tried running this model on two devices, both with a 64-bit OS
>> > with Windows 10, one with 32 GB of RAM and the other with 64 GB. I've
>> > gotten the same error from both devices. Is there a way that fitme()
>> > can accommodate these large data sets?
>>
>> spaMM can handle large data sets, but the first issue to consider here
>> is the number of distinct locations for the spatial random effect. The
>> large correlation matrices of geostatistical models will always be a
>> problem, both in terms of memory requirements and of potentially huge
>> computation times. My guess from past experiments is that one should
>> still be able to fit models with ~ 10K locations within a few days on a
>> computer with <60 Gb of RAM (given perhaps some tinkering of the
>> arguments), so at least the data set of 14 000 rows should be feasible,
>> particularly if the number of locations is smaller.
>>
>> Anyone planning to analyze large spatial data sets should anticipate
>> these problems and check by themselves whether there is any practical
>> alternative suitable for their particular problem. The discussion in
>> section 6.2 of the "gentle introduction" to spaMM may then be useful.
>>
>> Best,
>>
>> F.
>>
>> >
>> > Thank you,
>> >
>> > Sarah
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From |r@nco|@@rou@@et @end|ng |rom umontpe|||er@|r  Wed Jul 15 16:01:03 2020
From: |r@nco|@@rou@@et @end|ng |rom umontpe|||er@|r (Francois Rousset)
Date: Wed, 15 Jul 2020 16:01:03 +0200
Subject: [R-sig-ME] spaMM::fitme() - a glmm for longitudinal data that
 accounts for spatial autocorrelation
In-Reply-To: <CAJuCY5w=jRoYUzdmynCPFc8Ec03OFXzpG+KhrvSsWJhCpy9Ocw@mail.gmail.com>
References: <CAHWECf_ZO0U-BoGSPi8uYj7oBWo1xk4WqxLeHDHgRFZdTXY4bA@mail.gmail.com>
 <CAMu=eMAVy-=hmjWaYcBMtyNS8QHS4jm_nFPp3A+hhQYfigR9bQ@mail.gmail.com>
 <CAHWECf9Hcn3KH7csDg337mkgQYHcDdzoxAEAx+tehuYo88iBwQ@mail.gmail.com>
 <6707c79b-c372-3364-f8ff-05827f7b27c2@umontpellier.fr>
 <CAJuCY5wrFCzisy_gnu=0ooVijCyMBhr=tS3yNGLP2S3pWAH6+Q@mail.gmail.com>
 <e7c732ce-2615-23c6-874d-89afbe7d8946@umontpellier.fr>
 <CAJuCY5w=jRoYUzdmynCPFc8Ec03OFXzpG+KhrvSsWJhCpy9Ocw@mail.gmail.com>
Message-ID: <5aaaa00f-8847-5d79-8d81-41868904d32c@umontpellier.fr>

Dear Thierry,

thanks. So (expectedly) this is a different issue. spaMM can fit some 
correlation models described by objects produced by 
INLA::inla.spde2.matern() and then, in my past experiments, the 
computation times were close to those of INLA, and the memory 
requirements were much smaller than what I wrote previously where this 
is not what I meant by "Matern".

Beyond general features that contribute to these computational 
differences (the use of sparse matrix methods, and to a lesser extent 
the constraint on the smoothness parameter of the approximated Matern 
model), the 'cutoff' argument in your call to inla.mesh.2d() appears 
important to reduce the number? of locations actually considered, in the 
most costly computations, below the number of locations in the data (to 
8804 rather than 30K, if I get it right), and this would also allow a 
faster fit by spaMM when called on the resulting inla.spde2 object.

Best,

F.

Le 15/07/2020 ? 12:50, Thierry Onkelinx a ?crit?:
> Dear Fran?ois,
>
> Here you go: 
> https://drive.google.com/drive/folders/1Ocq88Yq9u_lM-loayRQlMyBS2HLy_Tio
> Almost 30K locations. Fit in little over 7 min on my laptop with 16 GB 
> RAM.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE 
> AND FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be <http://www.inbo.be>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no 
> more than asking him to perform a post-mortem examination: he may be 
> able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does 
> not ensure that a reasonable answer can be extracted from a given body 
> of data. ~ John Tukey
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op wo 15 jul. 2020 om 00:10 schreef Francois Rousset 
> <francois.rousset at umontpellier.fr 
> <mailto:francois.rousset at umontpellier.fr>>:
>
>     Dear Thierry,
>
>     please provide a reproducible example so that we know what you
>     have actually done.
>
>     Best,
>
>     F.
>
>     Le 14/07/2020 ? 20:00, Thierry Onkelinx a ?crit?:
>>     Dear Fran?ois and Sarah,
>>
>>     INLA seems more efficient. I ran a model with Mattern correlation
>>     structure on 13K locations (1 observation per location) in under
>>     10 minutes on a laptop with 16GB RAM.
>>
>>     Best regards,
>>
>>     ir. Thierry Onkelinx
>>     Statisticus / Statistician
>>
>>     Vlaamse Overheid / Government of Flanders
>>     INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR
>>     NATURE AND FOREST
>>     Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality
>>     Assurance
>>     thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>
>>     Havenlaan 88 bus 73, 1000 Brussel
>>     www.inbo.be <http://www.inbo.be>
>>
>>     ///////////////////////////////////////////////////////////////////////////////////////////
>>     To call in the statistician after the experiment is done may be
>>     no more than asking him to perform a post-mortem examination: he
>>     may be able to say what the experiment died of. ~ Sir Ronald
>>     Aylmer Fisher
>>     The plural of anecdote is not data. ~ Roger Brinner
>>     The combination of some data and an aching desire for an answer
>>     does not ensure that a reasonable answer can be extracted from a
>>     given body of data. ~ John Tukey
>>     ///////////////////////////////////////////////////////////////////////////////////////////
>>
>>     <https://www.inbo.be>
>>
>>
>>     Op di 14 jul. 2020 om 18:22 schreef Francois Rousset
>>     <francois.rousset at umontpellier.fr
>>     <mailto:francois.rousset at umontpellier.fr>>:
>>
>>         Dear Sarah,
>>
>>         Le 14/07/2020 ? 16:55, Sarah Chisholm a ?crit?:
>>         > Hi Mollie, thank you for your suggestion. glmmTMB seems
>>         like a good
>>         > option for my needs as well. In your sample code above, can
>>         you
>>         > explain what the term 'group' does in matern(pos+0|group)?
>>         Does this
>>         > allow the spatial correlation structure to be applied to
>>         specific
>>         > groupings in the data (in my case, for example, by
>>         'continent')?
>>         >
>>         > Francois, thank you for this very clear answer. This is a very
>>         > convenient feature of the function! May I ask you a couple
>>         of other
>>         > questions about some issues that I've had with spaMM::fitme()?
>>         >
>>         > In particular, when I try fitting this model to a large
>>         data set (~14
>>         > 000 rows x 7 columns, ~2 MB), the model will run for an
>>         extended
>>         > period of time, to the point where I've had to terminate the
>>         > computation. I've tried applying the suggestions that are
>>         mentioned in
>>         > the user guide, i.e. setting?init=list(lambda=0.1)
>>         > and?init=list(lambda=NaN). Implementing
>>         init=list(lambda=0.1) returned
>>         > an error suggesting that there was a lack of memory, while
>>         running the
>>         > model with init=list(lambda=NaN) also ran for an extended
>>         period of
>>         > time without completing. Is there something else I can do
>>         to speed up
>>         > the fit of these models?
>>         >
>>         > I've had a similar problem with an even larger data set
>>         (~185 000 rows
>>         > x 8 columns, ~21 MB), where, when I try running the model,
>>         this error
>>         > is returned immediately:
>>         >
>>         > ErrorinZA %*%xmatrix :Cholmoderror 'problem too large'at file
>>         > ../Core/cholmod_dense.c,line 105
>>         >
>>         > I've tried running this model on two devices, both with a
>>         64-bit OS
>>         > with Windows 10, one with 32 GB of RAM and the other with
>>         64 GB. I've
>>         > gotten the same error from both devices. Is there a way
>>         that fitme()
>>         > can accommodate these large data sets?
>>
>>         spaMM can handle large data sets, but the first issue to
>>         consider here
>>         is the number of distinct locations for the spatial random
>>         effect. The
>>         large correlation matrices of geostatistical models will
>>         always be a
>>         problem, both in terms of memory requirements and of
>>         potentially huge
>>         computation times. My guess from past experiments is that one
>>         should
>>         still be able to fit models with ~ 10K locations within a few
>>         days on a
>>         computer with <60 Gb of RAM (given perhaps some tinkering of the
>>         arguments), so at least the data set of 14 000 rows should be
>>         feasible,
>>         particularly if the number of locations is smaller.
>>
>>         Anyone planning to analyze large spatial data sets should
>>         anticipate
>>         these problems and check by themselves whether there is any
>>         practical
>>         alternative suitable for their particular problem. The
>>         discussion in
>>         section 6.2 of the "gentle introduction" to spaMM may then be
>>         useful.
>>
>>         Best,
>>
>>         F.
>>
>>         >
>>         > Thank you,
>>         >
>>         > Sarah
>>
>>         ? ? ? ? [[alternative HTML version deleted]]
>>
>>         _______________________________________________
>>         R-sig-mixed-models at r-project.org
>>         <mailto:R-sig-mixed-models at r-project.org> mailing list
>>         https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>

	[[alternative HTML version deleted]]


From @ch|@023 @end|ng |rom uott@w@@c@  Wed Jul 15 16:48:38 2020
From: @ch|@023 @end|ng |rom uott@w@@c@ (Sarah Chisholm)
Date: Wed, 15 Jul 2020 10:48:38 -0400
Subject: [R-sig-ME] spaMM::fitme() - a glmm for longitudinal data that
 accounts for spatial autocorrelation
In-Reply-To: <5aaaa00f-8847-5d79-8d81-41868904d32c@umontpellier.fr>
References: <CAHWECf_ZO0U-BoGSPi8uYj7oBWo1xk4WqxLeHDHgRFZdTXY4bA@mail.gmail.com>
 <CAMu=eMAVy-=hmjWaYcBMtyNS8QHS4jm_nFPp3A+hhQYfigR9bQ@mail.gmail.com>
 <CAHWECf9Hcn3KH7csDg337mkgQYHcDdzoxAEAx+tehuYo88iBwQ@mail.gmail.com>
 <6707c79b-c372-3364-f8ff-05827f7b27c2@umontpellier.fr>
 <CAJuCY5wrFCzisy_gnu=0ooVijCyMBhr=tS3yNGLP2S3pWAH6+Q@mail.gmail.com>
 <e7c732ce-2615-23c6-874d-89afbe7d8946@umontpellier.fr>
 <CAJuCY5w=jRoYUzdmynCPFc8Ec03OFXzpG+KhrvSsWJhCpy9Ocw@mail.gmail.com>
 <5aaaa00f-8847-5d79-8d81-41868904d32c@umontpellier.fr>
Message-ID: <CAHWECf8dTV-bdwJ+jM0b-epd66FKkg8i57JsL8pLTk0a+i+92Q@mail.gmail.com>

Thanks Francois. I hadn't considered that the number of unique locations
could be the source of the problem, rather than the size of the entire data
set. It is a possibility for me to simply remove observations for a number
of locations to bring the total sample size (of unique coordinates) down.
I'll also test a lattice model using the IMRF() notation to describe the
random spatial effect - I believe this is what you referred to in your
previous email?

Sarah

On Wed, Jul 15, 2020 at 10:01 AM Francois Rousset <
francois.rousset at umontpellier.fr> wrote:

> Dear Thierry,
>
> thanks. So (expectedly) this is a different issue. spaMM can fit some
> correlation models described by objects produced by
> INLA::inla.spde2.matern() and then, in my past experiments, the computation
> times were close to those of INLA, and the memory requirements were much
> smaller than what I wrote previously where this is not what I meant by
> "Matern".
>
> Beyond general features that contribute to these computational differences
> (the use of sparse matrix methods, and to a lesser extent the constraint on
> the smoothness parameter of the approximated Matern model), the 'cutoff'
> argument in your call to inla.mesh.2d() appears important to reduce the
> number  of locations actually considered, in the most costly computations,
> below the number of locations in the data (to 8804 rather than 30K, if I
> get it right), and this would also allow a faster fit by spaMM when called
> on the resulting inla.spde2 object.
>
> Best,
>
> F.
> Le 15/07/2020 ? 12:50, Thierry Onkelinx a ?crit :
>
> Dear Fran?ois,
>
> Here you go:
> https://drive.google.com/drive/folders/1Ocq88Yq9u_lM-loayRQlMyBS2HLy_Tio
> Almost 30K locations. Fit in little over 7 min on my laptop with 16 GB RAM.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op wo 15 jul. 2020 om 00:10 schreef Francois Rousset <
> francois.rousset at umontpellier.fr>:
>
>> Dear Thierry,
>>
>> please provide a reproducible example so that we know what you have
>> actually done.
>>
>> Best,
>>
>> F.
>> Le 14/07/2020 ? 20:00, Thierry Onkelinx a ?crit :
>>
>> Dear Fran?ois and Sarah,
>>
>> INLA seems more efficient. I ran a model with Mattern correlation
>> structure on 13K locations (1 observation per location) in under 10 minutes
>> on a laptop with 16GB RAM.
>>
>> Best regards,
>>
>> ir. Thierry Onkelinx
>> Statisticus / Statistician
>>
>> Vlaamse Overheid / Government of Flanders
>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>> AND FOREST
>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>> thierry.onkelinx at inbo.be
>> Havenlaan 88 bus 73, 1000 Brussel
>> www.inbo.be
>>
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of data.
>> ~ John Tukey
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>>
>> <https://www.inbo.be>
>>
>>
>> Op di 14 jul. 2020 om 18:22 schreef Francois Rousset <
>> francois.rousset at umontpellier.fr>:
>>
>>> Dear Sarah,
>>>
>>> Le 14/07/2020 ? 16:55, Sarah Chisholm a ?crit :
>>> > Hi Mollie, thank you for your suggestion. glmmTMB seems like a good
>>> > option for my needs as well. In your sample code above, can you
>>> > explain what the term 'group' does in matern(pos+0|group)? Does this
>>> > allow the spatial correlation structure to be applied to specific
>>> > groupings in the data (in my case, for example, by 'continent')?
>>> >
>>> > Francois, thank you for this very clear answer. This is a very
>>> > convenient feature of the function! May I ask you a couple of other
>>> > questions about some issues that I've had with spaMM::fitme()?
>>> >
>>> > In particular, when I try fitting this model to a large data set (~14
>>> > 000 rows x 7 columns, ~2 MB), the model will run for an extended
>>> > period of time, to the point where I've had to terminate the
>>> > computation. I've tried applying the suggestions that are mentioned in
>>> > the user guide, i.e. setting init=list(lambda=0.1)
>>> > and init=list(lambda=NaN). Implementing init=list(lambda=0.1) returned
>>> > an error suggesting that there was a lack of memory, while running the
>>> > model with init=list(lambda=NaN) also ran for an extended period of
>>> > time without completing. Is there something else I can do to speed up
>>> > the fit of these models?
>>> >
>>> > I've had a similar problem with an even larger data set (~185 000 rows
>>> > x 8 columns, ~21 MB), where, when I try running the model, this error
>>> > is returned immediately:
>>> >
>>> > ErrorinZA %*%xmatrix :Cholmoderror 'problem too large'at file
>>> > ../Core/cholmod_dense.c,line 105
>>> >
>>> > I've tried running this model on two devices, both with a 64-bit OS
>>> > with Windows 10, one with 32 GB of RAM and the other with 64 GB. I've
>>> > gotten the same error from both devices. Is there a way that fitme()
>>> > can accommodate these large data sets?
>>>
>>> spaMM can handle large data sets, but the first issue to consider here
>>> is the number of distinct locations for the spatial random effect. The
>>> large correlation matrices of geostatistical models will always be a
>>> problem, both in terms of memory requirements and of potentially huge
>>> computation times. My guess from past experiments is that one should
>>> still be able to fit models with ~ 10K locations within a few days on a
>>> computer with <60 Gb of RAM (given perhaps some tinkering of the
>>> arguments), so at least the data set of 14 000 rows should be feasible,
>>> particularly if the number of locations is smaller.
>>>
>>> Anyone planning to analyze large spatial data sets should anticipate
>>> these problems and check by themselves whether there is any practical
>>> alternative suitable for their particular problem. The discussion in
>>> section 6.2 of the "gentle introduction" to spaMM may then be useful.
>>>
>>> Best,
>>>
>>> F.
>>>
>>> >
>>> > Thank you,
>>> >
>>> > Sarah
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>

-- 
Sarah Chisholm
MSc Candidate
Department of Biology
University of Ottawa
Linkedin <http://www.linkedin.com/in/sarah-chisholm-422a5785>

	[[alternative HTML version deleted]]


From |r@nco|@@rou@@et @end|ng |rom umontpe|||er@|r  Thu Jul 16 10:19:07 2020
From: |r@nco|@@rou@@et @end|ng |rom umontpe|||er@|r (Francois Rousset)
Date: Thu, 16 Jul 2020 10:19:07 +0200
Subject: [R-sig-ME] spaMM::fitme() - a glmm for longitudinal data that
 accounts for spatial autocorrelation
In-Reply-To: <CAHWECf8dTV-bdwJ+jM0b-epd66FKkg8i57JsL8pLTk0a+i+92Q@mail.gmail.com>
References: <CAHWECf_ZO0U-BoGSPi8uYj7oBWo1xk4WqxLeHDHgRFZdTXY4bA@mail.gmail.com>
 <CAMu=eMAVy-=hmjWaYcBMtyNS8QHS4jm_nFPp3A+hhQYfigR9bQ@mail.gmail.com>
 <CAHWECf9Hcn3KH7csDg337mkgQYHcDdzoxAEAx+tehuYo88iBwQ@mail.gmail.com>
 <6707c79b-c372-3364-f8ff-05827f7b27c2@umontpellier.fr>
 <CAJuCY5wrFCzisy_gnu=0ooVijCyMBhr=tS3yNGLP2S3pWAH6+Q@mail.gmail.com>
 <e7c732ce-2615-23c6-874d-89afbe7d8946@umontpellier.fr>
 <CAJuCY5w=jRoYUzdmynCPFc8Ec03OFXzpG+KhrvSsWJhCpy9Ocw@mail.gmail.com>
 <5aaaa00f-8847-5d79-8d81-41868904d32c@umontpellier.fr>
 <CAHWECf8dTV-bdwJ+jM0b-epd66FKkg8i57JsL8pLTk0a+i+92Q@mail.gmail.com>
Message-ID: <164dc804-478e-1963-e68f-8a4fb944bcbc@umontpellier.fr>


Le 15/07/2020 ? 16:48, Sarah Chisholm a ?crit?:
> Thanks Francois. I hadn't considered that the number of unique 
> locations could be the source of the problem, rather than the size of 
> the entire data set. It is a possibility for me to simply remove 
> observations for a number of locations to bring the total sample size 
> (of unique coordinates) down. I'll also test a lattice model using the 
> IMRF() notation to describe the random spatial effect - I believe this 
> is what you referred to in your previous email?

yes, use the IMRF formula term for this purpose.

F.
>
> Sarah
>
> On Wed, Jul 15, 2020 at 10:01 AM Francois Rousset 
> <francois.rousset at umontpellier.fr 
> <mailto:francois.rousset at umontpellier.fr>> wrote:
>
>     Dear Thierry,
>
>     thanks. So (expectedly) this is a different issue. spaMM can fit
>     some correlation models described by objects produced by
>     INLA::inla.spde2.matern() and then, in my past experiments, the
>     computation times were close to those of INLA, and the memory
>     requirements were much smaller than what I wrote previously where
>     this is not what I meant by "Matern".
>
>     Beyond general features that contribute to these computational
>     differences (the use of sparse matrix methods, and to a lesser
>     extent the constraint on the smoothness parameter of the
>     approximated Matern model), the 'cutoff' argument in your call to
>     inla.mesh.2d() appears important to reduce the number? of
>     locations actually considered, in the most costly computations,
>     below the number of locations in the data (to 8804 rather than
>     30K, if I get it right), and this would also allow a faster fit by
>     spaMM when called on the resulting inla.spde2 object.
>
>     Best,
>
>     F.
>
>     Le 15/07/2020 ? 12:50, Thierry Onkelinx a ?crit?:
>>     Dear Fran?ois,
>>
>>     Here you go:
>>     https://drive.google.com/drive/folders/1Ocq88Yq9u_lM-loayRQlMyBS2HLy_Tio
>>     Almost 30K locations. Fit in little over 7 min on my laptop with
>>     16 GB RAM.
>>
>>     Best regards,
>>
>>     ir. Thierry Onkelinx
>>     Statisticus / Statistician
>>
>>     Vlaamse Overheid / Government of Flanders
>>     INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR
>>     NATURE AND FOREST
>>     Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality
>>     Assurance
>>     thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>
>>     Havenlaan 88 bus 73, 1000 Brussel
>>     www.inbo.be <http://www.inbo.be>
>>
>>     ///////////////////////////////////////////////////////////////////////////////////////////
>>     To call in the statistician after the experiment is done may be
>>     no more than asking him to perform a post-mortem examination: he
>>     may be able to say what the experiment died of. ~ Sir Ronald
>>     Aylmer Fisher
>>     The plural of anecdote is not data. ~ Roger Brinner
>>     The combination of some data and an aching desire for an answer
>>     does not ensure that a reasonable answer can be extracted from a
>>     given body of data. ~ John Tukey
>>     ///////////////////////////////////////////////////////////////////////////////////////////
>>
>>     <https://www.inbo.be>
>>
>>
>>     Op wo 15 jul. 2020 om 00:10 schreef Francois Rousset
>>     <francois.rousset at umontpellier.fr
>>     <mailto:francois.rousset at umontpellier.fr>>:
>>
>>         Dear Thierry,
>>
>>         please provide a reproducible example so that we know what
>>         you have actually done.
>>
>>         Best,
>>
>>         F.
>>
>>         Le 14/07/2020 ? 20:00, Thierry Onkelinx a ?crit?:
>>>         Dear Fran?ois and Sarah,
>>>
>>>         INLA seems more efficient. I ran a model with Mattern
>>>         correlation structure on 13K locations (1 observation per
>>>         location) in under 10 minutes on a laptop with 16GB RAM.
>>>
>>>         Best regards,
>>>
>>>         ir. Thierry Onkelinx
>>>         Statisticus / Statistician
>>>
>>>         Vlaamse Overheid / Government of Flanders
>>>         INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE
>>>         FOR NATURE AND FOREST
>>>         Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality
>>>         Assurance
>>>         thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>
>>>         Havenlaan 88 bus 73, 1000 Brussel
>>>         www.inbo.be <http://www.inbo.be>
>>>
>>>         ///////////////////////////////////////////////////////////////////////////////////////////
>>>         To call in the statistician after the experiment is done may
>>>         be no more than asking him to perform a post-mortem
>>>         examination: he may be able to say what the experiment died
>>>         of. ~ Sir Ronald Aylmer Fisher
>>>         The plural of anecdote is not data. ~ Roger Brinner
>>>         The combination of some data and an aching desire for an
>>>         answer does not ensure that a reasonable answer can be
>>>         extracted from a given body of data. ~ John Tukey
>>>         ///////////////////////////////////////////////////////////////////////////////////////////
>>>
>>>         <https://www.inbo.be>
>>>
>>>
>>>         Op di 14 jul. 2020 om 18:22 schreef Francois Rousset
>>>         <francois.rousset at umontpellier.fr
>>>         <mailto:francois.rousset at umontpellier.fr>>:
>>>
>>>             Dear Sarah,
>>>
>>>             Le 14/07/2020 ? 16:55, Sarah Chisholm a ?crit?:
>>>             > Hi Mollie, thank you for your suggestion. glmmTMB
>>>             seems like a good
>>>             > option for my needs as well. In your sample code
>>>             above, can you
>>>             > explain what the term 'group' does in
>>>             matern(pos+0|group)? Does this
>>>             > allow the spatial correlation structure to be applied
>>>             to specific
>>>             > groupings in the data (in my case, for example, by
>>>             'continent')?
>>>             >
>>>             > Francois, thank you for this very clear answer. This
>>>             is a very
>>>             > convenient feature of the function! May I ask you a
>>>             couple of other
>>>             > questions about some issues that I've had with
>>>             spaMM::fitme()?
>>>             >
>>>             > In particular, when I try fitting this model to a
>>>             large data set (~14
>>>             > 000 rows x 7 columns, ~2 MB), the model will run for
>>>             an extended
>>>             > period of time, to the point where I've had to
>>>             terminate the
>>>             > computation. I've tried applying the suggestions that
>>>             are mentioned in
>>>             > the user guide, i.e. setting?init=list(lambda=0.1)
>>>             > and?init=list(lambda=NaN). Implementing
>>>             init=list(lambda=0.1) returned
>>>             > an error suggesting that there was a lack of memory,
>>>             while running the
>>>             > model with init=list(lambda=NaN) also ran for an
>>>             extended period of
>>>             > time without completing. Is there something else I can
>>>             do to speed up
>>>             > the fit of these models?
>>>             >
>>>             > I've had a similar problem with an even larger data
>>>             set (~185 000 rows
>>>             > x 8 columns, ~21 MB), where, when I try running the
>>>             model, this error
>>>             > is returned immediately:
>>>             >
>>>             > ErrorinZA %*%xmatrix :Cholmoderror 'problem too
>>>             large'at file
>>>             > ../Core/cholmod_dense.c,line 105
>>>             >
>>>             > I've tried running this model on two devices, both
>>>             with a 64-bit OS
>>>             > with Windows 10, one with 32 GB of RAM and the other
>>>             with 64 GB. I've
>>>             > gotten the same error from both devices. Is there a
>>>             way that fitme()
>>>             > can accommodate these large data sets?
>>>
>>>             spaMM can handle large data sets, but the first issue to
>>>             consider here
>>>             is the number of distinct locations for the spatial
>>>             random effect. The
>>>             large correlation matrices of geostatistical models will
>>>             always be a
>>>             problem, both in terms of memory requirements and of
>>>             potentially huge
>>>             computation times. My guess from past experiments is
>>>             that one should
>>>             still be able to fit models with ~ 10K locations within
>>>             a few days on a
>>>             computer with <60 Gb of RAM (given perhaps some
>>>             tinkering of the
>>>             arguments), so at least the data set of 14 000 rows
>>>             should be feasible,
>>>             particularly if the number of locations is smaller.
>>>
>>>             Anyone planning to analyze large spatial data sets
>>>             should anticipate
>>>             these problems and check by themselves whether there is
>>>             any practical
>>>             alternative suitable for their particular problem. The
>>>             discussion in
>>>             section 6.2 of the "gentle introduction" to spaMM may
>>>             then be useful.
>>>
>>>             Best,
>>>
>>>             F.
>>>
>>>             >
>>>             > Thank you,
>>>             >
>>>             > Sarah
>>>
>>>             ? ? ? ? [[alternative HTML version deleted]]
>>>
>>>             _______________________________________________
>>>             R-sig-mixed-models at r-project.org
>>>             <mailto:R-sig-mixed-models at r-project.org> mailing list
>>>             https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>
>
> -- 
> Sarah Chisholm
> MSc Candidate
> Department of Biology
> University of Ottawa
> Linkedin <http://www.linkedin.com/in/sarah-chisholm-422a5785>


From jcm6t @end|ng |rom v|rg|n|@@edu  Thu Jul 16 17:21:37 2020
From: jcm6t @end|ng |rom v|rg|n|@@edu (Mychaleckyj, Josyf C (jcm6t))
Date: Thu, 16 Jul 2020 15:21:37 +0000
Subject: [R-sig-ME] gls() results for unbalanced data with corStruct
 correlation class change if Initialize() is called first
Message-ID: <B0BDE7BD-E24A-4BF7-904D-298B5E512E08@virginia.edu>


I?ve encountered a strange situation where the results of a gls model with a corStruct correlation structure are affected by whether Initialize() is explicitly called before the gls() model is executed.

This seems to be restricted to unbalanced data sets.  I have pasted a test case using the Orthodont data set and corCompSymm below, although I first noticed it while running a custom corStruct class so it would appear to be a more general problem than this test case as you might expect. I have not tested this in lme().

I?m aware that calling Initialize() outside of gls or lme is generally discouraged but was surprised that it changed the analysis results. You could imagine calling it in an interactive modeling session to view the correlation structure without knowing that you might have changed future results. I would have expected the results to be identical.

I?d also like to know - which is the ?correct? result or are they considered practically equivalent, although the differences seem quite large for the latter.

Just to the clear, this is not about the relative merits of balanced vs unbalanced data. Its about the veracity of the results.

Thanks,
Joe.



R version 3.6.0 (Linux CentOS 7)

> library(nlme)
>
> packageVersion('nlme')
[1] ?3.1.147?
>
> #?????????????????????
> # the values are fixed for all models
> rho <- 0.3
> fixform <- distance ~ age + factor(Sex)
>
> # remove the grouping and other classes to keep it simple
> # full balanced data set
> Obal<-as.data.frame(Orthodont)
> class(Obal)
[1] "data.frame"
>
> # create unbalanced Orthodont data set with no singletons
> Ounbal <-as.data.frame(Orthodont[c(3,4,5, 7,8, 11, 12, 15, 16, 19, 20:28,31,32,35,36,39,
+ 40,41,43,44,47,48,51,52,53,55,56,59,60,63,64,67,68,71,72,75,76,
+ 79,80,83:85,87,88,91,92,95,96,99:101,103:105,107,108),])
>
> # sparser unbalanced data set with singletons
> Ounbal2<-as.data.frame(Orthodont[c(1,5,9,13,17,18,21:23,30:31,44,45:47,56:58,65:71,74:76,77,81,85,89,90,
+  93:94, 97,101:108),])
>
> Subjs<-c(paste0(c("M"),sprintf("%02d",1:16)), paste0(c("F"),sprintf("%02d",1:11)))
>
> table(Obal$Subject)[Subjs]

M01 M02 M03 M04 M05 M06 M07 M08 M09 M10 M11 M12 M13 M14 M15 M16 F01 F02 F03 F04
  4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4
F05 F06 F07 F08 F09 F10 F11
  4   4   4   4   4   4   4
> table(Ounbal$Subject)[Subjs]

M01 M02 M03 M04 M05 M06 M07 M08 M09 M10 M11 M12 M13 M14 M15 M16 F01 F02 F03 F04
  2   3   2   2   2   4   4   2   2   2   3   2   2   3   2   2   2   2   2   2
F05 F06 F07 F08 F09 F10 F11
  2   3   2   2   2   3   3
> table(Ounbal2$Subject)[Subjs]

 M01  M02  M03  M04  M05  M06 <NA>  M08 <NA> <NA>  M11  M12 <NA>  M14  M15 <NA>
   1    1    1    1    2    3         2              1    3         1    2
 F01  F02  F03  F04  F05  F06  F07  F08  F09  F10  F11
   4    3    3    1    1    1    2    2    1    4    4
>
> #----
> # test with the full balanced data set
> # pre-calling Initialize doesn't seem to make a difference
> csB <- corCompSymm(value = rho, form = ~ 1 | Subject)
> csB<-Initialize(csB, data=Obal)
>
> print(mod<-gls(fixform, correlation=csB, data=Obal))
Generalized least squares fit by REML
  Model: fixform
  Data: Obal
  Log-restricted-likelihood: -218.7563

Coefficients:
(Intercept)         age   SexFemale
 17.7067130   0.6601852  -2.3210227

Correlation Structure: Compound symmetry
 Formula: ~1 | Subject
 Parameter estimate(s):
      Rho
0.6144908
Degrees of freedom: 108 total; 105 residual
Residual standard error: 2.305696
>
> # compare with direct call
> print(mod2<-gls(fixform, correlation=corCompSymm(value = rho, form = ~ 1 | Subject), data=Obal) )
Generalized least squares fit by REML
  Model: fixform
  Data: Obal
  Log-restricted-likelihood: -218.7563

Coefficients:
(Intercept)         age   SexFemale
 17.7067130   0.6601852  -2.3210227

Correlation Structure: Compound symmetry
 Formula: ~1 | Subject
 Parameter estimate(s):
      Rho
0.6144908
Degrees of freedom: 108 total; 105 residual
Residual standard error: 2.305696
>
> #--------------
> # compare Unbalanced:
> csU <- corCompSymm(value = rho, form = ~ 1 | Subject)
> csU<-Initialize(csU, data=Ounbal)
>
> print(mod3<-gls(fixform, correlation=csU, data=Ounbal) )
Generalized least squares fit by REML
  Model: fixform
  Data: Ounbal
  Log-restricted-likelihood: -130.7703

Coefficients:
(Intercept)         age   SexFemale
  18.046228    0.659606   -3.262389

Correlation Structure: Compound symmetry
 Formula: ~1 | Subject
 Parameter estimate(s):
      Rho
0.7211814
Degrees of freedom: 64 total; 61 residual
Residual standard error: 2.378517
>
> # compare with direct call
> print(mod4<-gls(fixform, correlation=corCompSymm(value = rho, form = ~ 1 | Subject), data=Ounbal) )
Generalized least squares fit by REML
  Model: fixform
  Data: Ounbal
  Log-restricted-likelihood: -127.616

Coefficients:
(Intercept)         age   SexFemale
  18.330637    0.634451   -2.947638

Correlation Structure: Compound symmetry
 Formula: ~1 | Subject
 Parameter estimate(s):
      Rho
0.7598463
Degrees of freedom: 64 total; 61 residual
Residual standard error: 2.34705
>
> #----
> # compare with Unbalanced 2 with singletons
> csU2 <- corCompSymm(value = rho, form = ~ 1 | Subject)
> csU2<-Initialize(csU2, data=Ounbal2)
>
> print(mod5<-gls(fixform, correlation=csU2, data=Ounbal2))
Generalized least squares fit by REML
  Model: fixform
  Data: Ounbal2
  Log-restricted-likelihood: -74.02154

Coefficients:
(Intercept)         age   SexFemale
 18.5018545   0.5265219  -1.4587968

Correlation Structure: Compound symmetry
 Formula: ~1 | Subject
 Parameter estimate(s):
      Rho
0.8736732
Degrees of freedom: 44 total; 41 residual
Residual standard error: 1.94714
>
> # compare with direct call
> print(mod6<-gls(fixform, correlation=corCompSymm(value = rho, form = ~ 1 | Subject), data=Ounbal2))
Generalized least squares fit by REML
  Model: fixform
  Data: Ounbal2
  Log-restricted-likelihood: -77.29477

Coefficients:
(Intercept)         age   SexFemale
 18.4979634   0.5550817  -1.6893916

Correlation Structure: Compound symmetry
 Formula: ~1 | Subject
 Parameter estimate(s):
      Rho
0.8516777
Degrees of freedom: 44 total; 41 residual
Residual standard error: 2.024012
>


From bbo|ker @end|ng |rom gm@||@com  Thu Jul 16 18:15:42 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Thu, 16 Jul 2020 12:15:42 -0400
Subject: [R-sig-ME] gls() results for unbalanced data with corStruct
 correlation class change if Initialize() is called first
In-Reply-To: <B0BDE7BD-E24A-4BF7-904D-298B5E512E08@virginia.edu>
References: <B0BDE7BD-E24A-4BF7-904D-298B5E512E08@virginia.edu>
Message-ID: <f687bad5-e4d6-6ce0-5341-7b5bc2d3f8b2@gmail.com>

 ?My best guess is that you've found a bug in gls.? Thanks for the 
detailed test case!? I did test in lme() {as you may already know, the 
compound-symmetric GLS is equivalent, *if the estimated correlation is 
positive*, to a random-intercepts model}, and found that the lme() 
results agreed closely with the "direct call" results (standard errors 
differ slightly, but that's to be expected - they're much more sensitive 
to any numerical instability).? I put my? explorations up at:

https://gist.github.com/bbolker/501c1dc6b1ed8be5c46ef66da5a964aa

(sorry about tidyverse code, I find myself getting sucked in thiese days 
...)

On 7/16/20 11:21 AM, Mychaleckyj, Josyf C (jcm6t) wrote:
> I?ve encountered a strange situation where the results of a gls model with a corStruct correlation structure are affected by whether Initialize() is explicitly called before the gls() model is executed.
>
> This seems to be restricted to unbalanced data sets.  I have pasted a test case using the Orthodont data set and corCompSymm below, although I first noticed it while running a custom corStruct class so it would appear to be a more general problem than this test case as you might expect. I have not tested this in lme().
>
> I?m aware that calling Initialize() outside of gls or lme is generally discouraged but was surprised that it changed the analysis results. You could imagine calling it in an interactive modeling session to view the correlation structure without knowing that you might have changed future results. I would have expected the results to be identical.
>
> I?d also like to know - which is the ?correct? result or are they considered practically equivalent, although the differences seem quite large for the latter.
>
> Just to the clear, this is not about the relative merits of balanced vs unbalanced data. Its about the veracity of the results.
>
> Thanks,
> Joe.
>
>
>
> R version 3.6.0 (Linux CentOS 7)
>
>> library(nlme)
>>
>> packageVersion('nlme')
> [1] ?3.1.147?
>> #?????????????????????
>> # the values are fixed for all models
>> rho <- 0.3
>> fixform <- distance ~ age + factor(Sex)
>>
>> # remove the grouping and other classes to keep it simple
>> # full balanced data set
>> Obal<-as.data.frame(Orthodont)
>> class(Obal)
> [1] "data.frame"
>> # create unbalanced Orthodont data set with no singletons
>> Ounbal <-as.data.frame(Orthodont[c(3,4,5, 7,8, 11, 12, 15, 16, 19, 20:28,31,32,35,36,39,
> + 40,41,43,44,47,48,51,52,53,55,56,59,60,63,64,67,68,71,72,75,76,
> + 79,80,83:85,87,88,91,92,95,96,99:101,103:105,107,108),])
>> # sparser unbalanced data set with singletons
>> Ounbal2<-as.data.frame(Orthodont[c(1,5,9,13,17,18,21:23,30:31,44,45:47,56:58,65:71,74:76,77,81,85,89,90,
> +  93:94, 97,101:108),])
>> Subjs<-c(paste0(c("M"),sprintf("%02d",1:16)), paste0(c("F"),sprintf("%02d",1:11)))
>>
>> table(Obal$Subject)[Subjs]
> M01 M02 M03 M04 M05 M06 M07 M08 M09 M10 M11 M12 M13 M14 M15 M16 F01 F02 F03 F04
>    4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4
> F05 F06 F07 F08 F09 F10 F11
>    4   4   4   4   4   4   4
>> table(Ounbal$Subject)[Subjs]
> M01 M02 M03 M04 M05 M06 M07 M08 M09 M10 M11 M12 M13 M14 M15 M16 F01 F02 F03 F04
>    2   3   2   2   2   4   4   2   2   2   3   2   2   3   2   2   2   2   2   2
> F05 F06 F07 F08 F09 F10 F11
>    2   3   2   2   2   3   3
>> table(Ounbal2$Subject)[Subjs]
>   M01  M02  M03  M04  M05  M06 <NA>  M08 <NA> <NA>  M11  M12 <NA>  M14  M15 <NA>
>     1    1    1    1    2    3         2              1    3         1    2
>   F01  F02  F03  F04  F05  F06  F07  F08  F09  F10  F11
>     4    3    3    1    1    1    2    2    1    4    4
>> #----
>> # test with the full balanced data set
>> # pre-calling Initialize doesn't seem to make a difference
>> csB <- corCompSymm(value = rho, form = ~ 1 | Subject)
>> csB<-Initialize(csB, data=Obal)
>>
>> print(mod<-gls(fixform, correlation=csB, data=Obal))
> Generalized least squares fit by REML
>    Model: fixform
>    Data: Obal
>    Log-restricted-likelihood: -218.7563
>
> Coefficients:
> (Intercept)         age   SexFemale
>   17.7067130   0.6601852  -2.3210227
>
> Correlation Structure: Compound symmetry
>   Formula: ~1 | Subject
>   Parameter estimate(s):
>        Rho
> 0.6144908
> Degrees of freedom: 108 total; 105 residual
> Residual standard error: 2.305696
>> # compare with direct call
>> print(mod2<-gls(fixform, correlation=corCompSymm(value = rho, form = ~ 1 | Subject), data=Obal) )
> Generalized least squares fit by REML
>    Model: fixform
>    Data: Obal
>    Log-restricted-likelihood: -218.7563
>
> Coefficients:
> (Intercept)         age   SexFemale
>   17.7067130   0.6601852  -2.3210227
>
> Correlation Structure: Compound symmetry
>   Formula: ~1 | Subject
>   Parameter estimate(s):
>        Rho
> 0.6144908
> Degrees of freedom: 108 total; 105 residual
> Residual standard error: 2.305696
>> #--------------
>> # compare Unbalanced:
>> csU <- corCompSymm(value = rho, form = ~ 1 | Subject)
>> csU<-Initialize(csU, data=Ounbal)
>>
>> print(mod3<-gls(fixform, correlation=csU, data=Ounbal) )
> Generalized least squares fit by REML
>    Model: fixform
>    Data: Ounbal
>    Log-restricted-likelihood: -130.7703
>
> Coefficients:
> (Intercept)         age   SexFemale
>    18.046228    0.659606   -3.262389
>
> Correlation Structure: Compound symmetry
>   Formula: ~1 | Subject
>   Parameter estimate(s):
>        Rho
> 0.7211814
> Degrees of freedom: 64 total; 61 residual
> Residual standard error: 2.378517
>> # compare with direct call
>> print(mod4<-gls(fixform, correlation=corCompSymm(value = rho, form = ~ 1 | Subject), data=Ounbal) )
> Generalized least squares fit by REML
>    Model: fixform
>    Data: Ounbal
>    Log-restricted-likelihood: -127.616
>
> Coefficients:
> (Intercept)         age   SexFemale
>    18.330637    0.634451   -2.947638
>
> Correlation Structure: Compound symmetry
>   Formula: ~1 | Subject
>   Parameter estimate(s):
>        Rho
> 0.7598463
> Degrees of freedom: 64 total; 61 residual
> Residual standard error: 2.34705
>> #----
>> # compare with Unbalanced 2 with singletons
>> csU2 <- corCompSymm(value = rho, form = ~ 1 | Subject)
>> csU2<-Initialize(csU2, data=Ounbal2)
>>
>> print(mod5<-gls(fixform, correlation=csU2, data=Ounbal2))
> Generalized least squares fit by REML
>    Model: fixform
>    Data: Ounbal2
>    Log-restricted-likelihood: -74.02154
>
> Coefficients:
> (Intercept)         age   SexFemale
>   18.5018545   0.5265219  -1.4587968
>
> Correlation Structure: Compound symmetry
>   Formula: ~1 | Subject
>   Parameter estimate(s):
>        Rho
> 0.8736732
> Degrees of freedom: 44 total; 41 residual
> Residual standard error: 1.94714
>> # compare with direct call
>> print(mod6<-gls(fixform, correlation=corCompSymm(value = rho, form = ~ 1 | Subject), data=Ounbal2))
> Generalized least squares fit by REML
>    Model: fixform
>    Data: Ounbal2
>    Log-restricted-likelihood: -77.29477
>
> Coefficients:
> (Intercept)         age   SexFemale
>   18.4979634   0.5550817  -1.6893916
>
> Correlation Structure: Compound symmetry
>   Formula: ~1 | Subject
>   Parameter estimate(s):
>        Rho
> 0.8516777
> Degrees of freedom: 44 total; 41 residual
> Residual standard error: 2.024012
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From v|ctor@or|b@m|@e @end|ng |rom gm@||@com  Thu Jul 16 20:17:05 2020
From: v|ctor@or|b@m|@e @end|ng |rom gm@||@com (Victor Oribamise)
Date: Thu, 16 Jul 2020 13:17:05 -0500
Subject: [R-sig-ME] prior error in mcmcglmm multi-response model
Message-ID: <CA+iPCzY8N_yqnSyF+G5pzyzySTPApXvPT9m=pvS9g+shyshzhw@mail.gmail.com>

Hello,

I tried running this multi-response model in mcmcglmm with this prior:

priorsd <- list(R=list(V=diag(8), nu=7.001), G = list(G1 =
list(V=diag(8),nu=7.001), G2 = list(V=diag(8),nu=7.001)))

and this model:

modelTrait<-MCMCglmm(cbind(Trait1,Trait2, Trait3,Trait4,Trait5, Trait6,
Trait7, Trait8)~year+pit,random= ~ID+Dam ,ginverse=list(ID=Ainv1,
Dam=M1),family=rep("gaussian",8),prior=priorsd, , rcov=~units,
data=data,nitt=10000,burnin=1000,thin=10)


but I ran into this error:


"Error in priorformat(if (NOpriorG) { :V is the wrong dimension for some
prior$G/prior$R elements."


What am I doing wrong? How do I correct this?


And, this is a subset of my data:


Trait1 Trait2 Trait3 Trait4 Trait5 Trait6 Trait7 Trait8 year pit
1 1 1 1 1 1 1 1 1954 1
1 1 1 1 1 1 1 1 1956 2
1 1 1 1 1 1 1 1 1957 2
1 1 1 1 1 1 1 1 1957 1
1 1 1 1 1 1 1 0 1978 2
1 1 1 1 1 1 1 1 1978 2

Thanks.


Victor

	[[alternative HTML version deleted]]


From @ed|@nd @end|ng |rom he@|th@uc@d@edu  Thu Jul 16 19:31:59 2020
From: @ed|@nd @end|ng |rom he@|th@uc@d@edu (Edland, Steven)
Date: Thu, 16 Jul 2020 17:31:59 +0000
Subject: [R-sig-ME] novel correlation structure for nlme package,
 lme function
In-Reply-To: <CAO7JsnQDojxVRMRhUYVRj-CYEkH+ZcnKfWWt64cD9uHtT-0R6Q@mail.gmail.com>
References: <247527_1594899160_0QDK0508C82F8Q10_BYAPR19MB24702802968A85151DB851E7E67F0@BYAPR19MB2470.namprd19.prod.outlook.com>,
 <CAO7JsnQDojxVRMRhUYVRj-CYEkH+ZcnKfWWt64cD9uHtT-0R6Q@mail.gmail.com>
Message-ID: <BYAPR19MB24705E70926087A1554A571DE67F0@BYAPR19MB2470.namprd19.prod.outlook.com>


Hello my programmer friends.  I am wondering if anyone has written a corStruct for this lme call:

lme(y~time*group, random= ~time|id)

In this call, the covariance structure implied by (time|id) is assumed constant across groups.  We would like to model these random effects separately in the two groups.

Motivation:  In a clinical trail with an _effective_ treatment, response to treatment will  be variable (variance of the random slopes will be greater in the treatment arm).


Thank you in advance for any thoughts where I might find such a corStruct.

Steve Edland & Yu Zhao


Sincerely,
Steve Edland

Steven D. Edland, Ph.D.
Professor
Dept. of Family Medicine & Public Health
Dept. of Neurosciences
University of California, San Diego
9500 Gilman Dr. M/C 0948
La Jolla, CA 92093-0948
http://biostat.ucsd.edu/sedland.htm

	[[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Thu Jul 16 20:34:19 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Thu, 16 Jul 2020 13:34:19 -0500
Subject: [R-sig-ME] graphics to show crossed random-effects
Message-ID: <CACgv6yXpgWB0rC=r3+pL9nA8atxKCHZBTnY6AiZscFQnMa2DTA@mail.gmail.com>

Good afternoon,

Given lme4 package in R, I often show a random-intercept model (i.e., y~1 +
(1|grouping)) using a picture like:
https://github.com/hkil/m/blob/master/mlm.PNG.

I wonder how to extend this picture to show a fully crossed
random-intercepts model of the form: y~1 + (1|grouping_1) + (1|grouping_2) ?

Specifically, do I need to have 2 red, level-2 distributions (side-by-side)
each for showing the random intercepts of each grouping_j and if so, is
there any way to show the "crossing*"* of the two random-effects in the
picture?

Many thanks,
Simon

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Thu Jul 16 20:53:10 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Thu, 16 Jul 2020 20:53:10 +0200
Subject: [R-sig-ME] novel correlation structure for nlme package,
 lme function
In-Reply-To: <BYAPR19MB24705E70926087A1554A571DE67F0@BYAPR19MB2470.namprd19.prod.outlook.com>
References: <247527_1594899160_0QDK0508C82F8Q10_BYAPR19MB24702802968A85151DB851E7E67F0@BYAPR19MB2470.namprd19.prod.outlook.com>
 <CAO7JsnQDojxVRMRhUYVRj-CYEkH+ZcnKfWWt64cD9uHtT-0R6Q@mail.gmail.com>
 <BYAPR19MB24705E70926087A1554A571DE67F0@BYAPR19MB2470.namprd19.prod.outlook.com>
Message-ID: <CAJuCY5y5PuOPCYHCv7-vrsHX+F0+m17BobjzSFJSWZ=7jK26vA@mail.gmail.com>

Dear Steve,

I'd create two variables
time_treatment <- ifelse(group == "treatment", time, 0)
time_placebo <- ifelse(group != "treatment", time, 0)

And then use ~ time_treatment + time_placebo | id as random effect.

This should give you the two random slope, each with it own variance.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op do 16 jul. 2020 om 20:35 schreef Edland, Steven <sedland at health.ucsd.edu
>:

>
> Hello my programmer friends.  I am wondering if anyone has written a
> corStruct for this lme call:
>
> lme(y~time*group, random= ~time|id)
>
> In this call, the covariance structure implied by (time|id) is assumed
> constant across groups.  We would like to model these random effects
> separately in the two groups.
>
> Motivation:  In a clinical trail with an _effective_ treatment, response
> to treatment will  be variable (variance of the random slopes will be
> greater in the treatment arm).
>
>
> Thank you in advance for any thoughts where I might find such a corStruct.
>
> Steve Edland & Yu Zhao
>
>
> Sincerely,
> Steve Edland
>
> Steven D. Edland, Ph.D.
> Professor
> Dept. of Family Medicine & Public Health
> Dept. of Neurosciences
> University of California, San Diego
> 9500 Gilman Dr. M/C 0948
> La Jolla, CA 92093-0948
> http://biostat.ucsd.edu/sedland.htm
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From @ed|@nd @end|ng |rom he@|th@uc@d@edu  Thu Jul 16 21:51:12 2020
From: @ed|@nd @end|ng |rom he@|th@uc@d@edu (Edland, Steven)
Date: Thu, 16 Jul 2020 19:51:12 +0000
Subject: [R-sig-ME] novel correlation structure for nlme package,
 lme function
In-Reply-To: <CAJuCY5y5PuOPCYHCv7-vrsHX+F0+m17BobjzSFJSWZ=7jK26vA@mail.gmail.com>
References: <247527_1594899160_0QDK0508C82F8Q10_BYAPR19MB24702802968A85151DB851E7E67F0@BYAPR19MB2470.namprd19.prod.outlook.com>
 <CAO7JsnQDojxVRMRhUYVRj-CYEkH+ZcnKfWWt64cD9uHtT-0R6Q@mail.gmail.com>
 <BYAPR19MB24705E70926087A1554A571DE67F0@BYAPR19MB2470.namprd19.prod.outlook.com>,
 <CAJuCY5y5PuOPCYHCv7-vrsHX+F0+m17BobjzSFJSWZ=7jK26vA@mail.gmail.com>
Message-ID: <BYAPR19MB24703C86D378F6CF2B2D450CE67F0@BYAPR19MB2470.namprd19.prod.outlook.com>


Thanks Thierry for your reply!  We did try that, and it does not work as best as we can tell.  It becomes a problem when the allocation ratio is not 1:1.  E.g., trials often use 2:1 treatment:control allocation ratio.  In this case the s.e. of the fixed effects slope coefficients are off, and we do not meet the nominal type I error rate.

Any experiences you can share would be appreciate, and thank you again for replying.

Regards,
Steve

Steven D. Edland, Ph.D.
Professor
Dept. of Family Medicine & Public Health
Dept. of Neurosciences
University of California, San Diego
9500 Gilman Dr. M/C 0948
La Jolla, CA 92093-0948
http://biostat.ucsd.edu/sedland.htm

________________________________
From: Thierry Onkelinx <thierry.onkelinx at inbo.be>
Sent: Thursday, July 16, 2020 11:53 AM
To: Edland, Steven <sedland at health.ucsd.edu>
Cc: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>; Yu Zhao <yuz867 at ucsd.edu>
Subject: Re: [R-sig-ME] novel correlation structure for nlme package, lme function

Dear Steve,

I'd create two variables
time_treatment <- ifelse(group == "treatment", time, 0)
time_placebo <- ifelse(group != "treatment", time, 0)

And then use ~ time_treatment + time_placebo | id as random effect.

This should give you the two random slope, each with it own variance.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be<http://www.inbo.be>

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

[https://inbo-website-prd-532750756126.s3-eu-west-1.amazonaws.com/inbologoleeuw_nl.png]<https://www.inbo.be>


Op do 16 jul. 2020 om 20:35 schreef Edland, Steven <sedland at health.ucsd.edu<mailto:sedland at health.ucsd.edu>>:

Hello my programmer friends.  I am wondering if anyone has written a corStruct for this lme call:

lme(y~time*group, random= ~time|id)

In this call, the covariance structure implied by (time|id) is assumed constant across groups.  We would like to model these random effects separately in the two groups.

Motivation:  In a clinical trail with an _effective_ treatment, response to treatment will  be variable (variance of the random slopes will be greater in the treatment arm).


Thank you in advance for any thoughts where I might find such a corStruct.

Steve Edland & Yu Zhao


Sincerely,
Steve Edland

Steven D. Edland, Ph.D.
Professor
Dept. of Family Medicine & Public Health
Dept. of Neurosciences
University of California, San Diego
9500 Gilman Dr. M/C 0948
La Jolla, CA 92093-0948
http://biostat.ucsd.edu/sedland.htm

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From @ed|@nd @end|ng |rom he@|th@uc@d@edu  Thu Jul 16 23:14:32 2020
From: @ed|@nd @end|ng |rom he@|th@uc@d@edu (Edland, Steven)
Date: Thu, 16 Jul 2020 21:14:32 +0000
Subject: [R-sig-ME] novel correlation structure for nlme package,
 lme function
In-Reply-To: <BYAPR19MB24703C86D378F6CF2B2D450CE67F0@BYAPR19MB2470.namprd19.prod.outlook.com>
References: <247527_1594899160_0QDK0508C82F8Q10_BYAPR19MB24702802968A85151DB851E7E67F0@BYAPR19MB2470.namprd19.prod.outlook.com>
 <CAO7JsnQDojxVRMRhUYVRj-CYEkH+ZcnKfWWt64cD9uHtT-0R6Q@mail.gmail.com>
 <BYAPR19MB24705E70926087A1554A571DE67F0@BYAPR19MB2470.namprd19.prod.outlook.com>,
 <CAJuCY5y5PuOPCYHCv7-vrsHX+F0+m17BobjzSFJSWZ=7jK26vA@mail.gmail.com>,
 <BYAPR19MB24703C86D378F6CF2B2D450CE67F0@BYAPR19MB2470.namprd19.prod.outlook.com>
Message-ID: <BYAPR19MB247048154A78E259EF093CE9E67F0@BYAPR19MB2470.namprd19.prod.outlook.com>


Hi Thierry,  Apologies, I read your email too quickly.  This syntax for specifying the covariance structure works well for my application.  Thank you again, and sincere apologies for my hasty reply below.
Regards, Steve


Steven D. Edland, Ph.D.
Professor
Dept. of Family Medicine & Public Health
Dept. of Neurosciences
University of California, San Diego
9500 Gilman Dr. M/C 0948
La Jolla, CA 92093-0948
http://biostat.ucsd.edu/sedland.htm

________________________________
From: Edland, Steven <sedland at health.ucsd.edu>
Sent: Thursday, July 16, 2020 12:51 PM
To: Thierry Onkelinx <thierry.onkelinx at inbo.be>
Cc: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>; Yu Zhao <yuz867 at ucsd.edu>
Subject: Re: [R-sig-ME] novel correlation structure for nlme package, lme function


Thanks Thierry for your reply!  We did try that, and it does not work as best as we can tell.  It becomes a problem when the allocation ratio is not 1:1.  E.g., trials often use 2:1 treatment:control allocation ratio.  In this case the s.e. of the fixed effects slope coefficients are off, and we do not meet the nominal type I error rate.

Any experiences you can share would be appreciate, and thank you again for replying.

Regards,
Steve

Steven D. Edland, Ph.D.
Professor
Dept. of Family Medicine & Public Health
Dept. of Neurosciences
University of California, San Diego
9500 Gilman Dr. M/C 0948
La Jolla, CA 92093-0948
http://biostat.ucsd.edu/sedland.htm

________________________________
From: Thierry Onkelinx <thierry.onkelinx at inbo.be>
Sent: Thursday, July 16, 2020 11:53 AM
To: Edland, Steven <sedland at health.ucsd.edu>
Cc: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>; Yu Zhao <yuz867 at ucsd.edu>
Subject: Re: [R-sig-ME] novel correlation structure for nlme package, lme function

Dear Steve,

I'd create two variables
time_treatment <- ifelse(group == "treatment", time, 0)
time_placebo <- ifelse(group != "treatment", time, 0)

And then use ~ time_treatment + time_placebo | id as random effect.

This should give you the two random slope, each with it own variance.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be<http://www.inbo.be>

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

[https://inbo-website-prd-532750756126.s3-eu-west-1.amazonaws.com/inbologoleeuw_nl.png]<https://www.inbo.be>


Op do 16 jul. 2020 om 20:35 schreef Edland, Steven <sedland at health.ucsd.edu<mailto:sedland at health.ucsd.edu>>:

Hello my programmer friends.  I am wondering if anyone has written a corStruct for this lme call:

lme(y~time*group, random= ~time|id)

In this call, the covariance structure implied by (time|id) is assumed constant across groups.  We would like to model these random effects separately in the two groups.

Motivation:  In a clinical trail with an _effective_ treatment, response to treatment will  be variable (variance of the random slopes will be greater in the treatment arm).


Thank you in advance for any thoughts where I might find such a corStruct.

Steve Edland & Yu Zhao


Sincerely,
Steve Edland

Steven D. Edland, Ph.D.
Professor
Dept. of Family Medicine & Public Health
Dept. of Neurosciences
University of California, San Diego
9500 Gilman Dr. M/C 0948
La Jolla, CA 92093-0948
http://biostat.ucsd.edu/sedland.htm

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From w@||dm@w@@@10 @end|ng |rom gm@||@com  Fri Jul 17 00:54:03 2020
From: w@||dm@w@@@10 @end|ng |rom gm@||@com (Walid Crampton-Mawass)
Date: Thu, 16 Jul 2020 18:54:03 -0400
Subject: [R-sig-ME] prior error in mcmcglmm multi-response model
In-Reply-To: <CA+iPCzY8N_yqnSyF+G5pzyzySTPApXvPT9m=pvS9g+shyshzhw@mail.gmail.com>
References: <CA+iPCzY8N_yqnSyF+G5pzyzySTPApXvPT9m=pvS9g+shyshzhw@mail.gmail.com>
Message-ID: <CAJtCY7WjKZPL6zm82RnQXDyQKFKhf2Ef=9pWUvhzqAL=JVan8g@mail.gmail.com>

Hello Victor,

The solution to your problem is in the syntax of the formula you wrote. You
provided a multivariate prior but the syntax you wrote for the random and
residual effects were still univariate. The proper way of writing your
model would this way:

modelTrait<-MCMCglmm(cbind(Trait1,Trait2, Trait3,Trait4,Trait5, Trait6,
Trait7, Trait8)~ trait -1 + trait:year+trait:pit,random= ~us(trait):ID+
us(trait):Dam ,ginverse=list(ID=Ainv1,
Dam=M1),family=rep("gaussian",8),prior=priorsd, , rcov=~us(trait):units,
data=data,nitt=10000,burnin=1000,thin=10)

One important note here is the usage of us() which builds an unstructured
covariance matrix for your random and residual effects meaning it will
calculate the covariances between the response traits for these parameters
- which is something I presume you would want to calculate. If not, and you
already know there is no covariances between the response traits, then you
can just replace us() with idh().

Last remark would be to suggest increasing the nitt and thin as to give the
algorithm more time to estimate the multiple posterior distributions for
the included parameters in your model - 10000 iterations is a low number in
my opinion, especially if your dataset is not that large and you have a
limited number of predictors.

Cheers

Walid Mawass
Ph.D. student in Evolutionary Biology at Universit? du Qu?bec ?
Trois-Rivi?res

On Thu., Jul. 16, 2020, 2:18 p.m. Victor Oribamise, <
victor.oribamise at gmail.com> wrote:

> Hello,
>
> I tried running this multi-response model in mcmcglmm with this prior:
>
> priorsd <- list(R=list(V=diag(8), nu=7.001), G = list(G1 =
> list(V=diag(8),nu=7.001), G2 = list(V=diag(8),nu=7.001)))
>
> and this model:
>
> modelTrait<-MCMCglmm(cbind(Trait1,Trait2, Trait3,Trait4,Trait5, Trait6,
> Trait7, Trait8)~year+pit,random= ~ID+Dam ,ginverse=list(ID=Ainv1,
> Dam=M1),family=rep("gaussian",8),prior=priorsd, , rcov=~units,
> data=data,nitt=10000,burnin=1000,thin=10)
>
>
> but I ran into this error:
>
>
> "Error in priorformat(if (NOpriorG) { :V is the wrong dimension for some
> prior$G/prior$R elements."
>
>
> What am I doing wrong? How do I correct this?
>
>
> And, this is a subset of my data:
>
>
> Trait1 Trait2 Trait3 Trait4 Trait5 Trait6 Trait7 Trait8 year pit
> 1 1 1 1 1 1 1 1 1954 1
> 1 1 1 1 1 1 1 1 1956 2
> 1 1 1 1 1 1 1 1 1957 2
> 1 1 1 1 1 1 1 1 1957 1
> 1 1 1 1 1 1 1 0 1978 2
> 1 1 1 1 1 1 1 1 1978 2
>
> Thanks.
>
>
> Victor
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Fri Jul 17 15:20:34 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Fri, 17 Jul 2020 15:20:34 +0200
Subject: [R-sig-ME] spaMM::fitme() - a glmm for longitudinal data that
 accounts for spatial autocorrelation
In-Reply-To: <5aaaa00f-8847-5d79-8d81-41868904d32c@umontpellier.fr>
References: <CAHWECf_ZO0U-BoGSPi8uYj7oBWo1xk4WqxLeHDHgRFZdTXY4bA@mail.gmail.com>
 <CAMu=eMAVy-=hmjWaYcBMtyNS8QHS4jm_nFPp3A+hhQYfigR9bQ@mail.gmail.com>
 <CAHWECf9Hcn3KH7csDg337mkgQYHcDdzoxAEAx+tehuYo88iBwQ@mail.gmail.com>
 <6707c79b-c372-3364-f8ff-05827f7b27c2@umontpellier.fr>
 <CAJuCY5wrFCzisy_gnu=0ooVijCyMBhr=tS3yNGLP2S3pWAH6+Q@mail.gmail.com>
 <e7c732ce-2615-23c6-874d-89afbe7d8946@umontpellier.fr>
 <CAJuCY5w=jRoYUzdmynCPFc8Ec03OFXzpG+KhrvSsWJhCpy9Ocw@mail.gmail.com>
 <5aaaa00f-8847-5d79-8d81-41868904d32c@umontpellier.fr>
Message-ID: <CAJuCY5xrmy-7Mup2y=GFwKxdNt9gsLKqD3ABmFon6hJjLD=KRA@mail.gmail.com>

Dear Fran?ois,

Point taken about the mesh size. Halving the cutoff to 0.5 increased the
number of points on the mesh to 31k nodes. This increases the runtime to 65
min on my laptop.
https://drive.google.com/drive/folders/1Ocq88Yq9u_lM-loayRQlMyBS2HLy_Tio

I'm curious what you mean by "Matern" if it isn't similar to the spde model
of INLA.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op wo 15 jul. 2020 om 16:01 schreef Francois Rousset <
francois.rousset at umontpellier.fr>:

> Dear Thierry,
>
> thanks. So (expectedly) this is a different issue. spaMM can fit some
> correlation models described by objects produced by
> INLA::inla.spde2.matern() and then, in my past experiments, the computation
> times were close to those of INLA, and the memory requirements were much
> smaller than what I wrote previously where this is not what I meant by
> "Matern".
>
> Beyond general features that contribute to these computational differences
> (the use of sparse matrix methods, and to a lesser extent the constraint on
> the smoothness parameter of the approximated Matern model), the 'cutoff'
> argument in your call to inla.mesh.2d() appears important to reduce the
> number  of locations actually considered, in the most costly computations,
> below the number of locations in the data (to 8804 rather than 30K, if I
> get it right), and this would also allow a faster fit by spaMM when called
> on the resulting inla.spde2 object.
>
> Best,
>
> F.
> Le 15/07/2020 ? 12:50, Thierry Onkelinx a ?crit :
>
> Dear Fran?ois,
>
> Here you go:
> https://drive.google.com/drive/folders/1Ocq88Yq9u_lM-loayRQlMyBS2HLy_Tio
> Almost 30K locations. Fit in little over 7 min on my laptop with 16 GB RAM.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op wo 15 jul. 2020 om 00:10 schreef Francois Rousset <
> francois.rousset at umontpellier.fr>:
>
>> Dear Thierry,
>>
>> please provide a reproducible example so that we know what you have
>> actually done.
>>
>> Best,
>>
>> F.
>> Le 14/07/2020 ? 20:00, Thierry Onkelinx a ?crit :
>>
>> Dear Fran?ois and Sarah,
>>
>> INLA seems more efficient. I ran a model with Mattern correlation
>> structure on 13K locations (1 observation per location) in under 10 minutes
>> on a laptop with 16GB RAM.
>>
>> Best regards,
>>
>> ir. Thierry Onkelinx
>> Statisticus / Statistician
>>
>> Vlaamse Overheid / Government of Flanders
>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>> AND FOREST
>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>> thierry.onkelinx at inbo.be
>> Havenlaan 88 bus 73, 1000 Brussel
>> www.inbo.be
>>
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of data.
>> ~ John Tukey
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>>
>> <https://www.inbo.be>
>>
>>
>> Op di 14 jul. 2020 om 18:22 schreef Francois Rousset <
>> francois.rousset at umontpellier.fr>:
>>
>>> Dear Sarah,
>>>
>>> Le 14/07/2020 ? 16:55, Sarah Chisholm a ?crit :
>>> > Hi Mollie, thank you for your suggestion. glmmTMB seems like a good
>>> > option for my needs as well. In your sample code above, can you
>>> > explain what the term 'group' does in matern(pos+0|group)? Does this
>>> > allow the spatial correlation structure to be applied to specific
>>> > groupings in the data (in my case, for example, by 'continent')?
>>> >
>>> > Francois, thank you for this very clear answer. This is a very
>>> > convenient feature of the function! May I ask you a couple of other
>>> > questions about some issues that I've had with spaMM::fitme()?
>>> >
>>> > In particular, when I try fitting this model to a large data set (~14
>>> > 000 rows x 7 columns, ~2 MB), the model will run for an extended
>>> > period of time, to the point where I've had to terminate the
>>> > computation. I've tried applying the suggestions that are mentioned in
>>> > the user guide, i.e. setting init=list(lambda=0.1)
>>> > and init=list(lambda=NaN). Implementing init=list(lambda=0.1) returned
>>> > an error suggesting that there was a lack of memory, while running the
>>> > model with init=list(lambda=NaN) also ran for an extended period of
>>> > time without completing. Is there something else I can do to speed up
>>> > the fit of these models?
>>> >
>>> > I've had a similar problem with an even larger data set (~185 000 rows
>>> > x 8 columns, ~21 MB), where, when I try running the model, this error
>>> > is returned immediately:
>>> >
>>> > ErrorinZA %*%xmatrix :Cholmoderror 'problem too large'at file
>>> > ../Core/cholmod_dense.c,line 105
>>> >
>>> > I've tried running this model on two devices, both with a 64-bit OS
>>> > with Windows 10, one with 32 GB of RAM and the other with 64 GB. I've
>>> > gotten the same error from both devices. Is there a way that fitme()
>>> > can accommodate these large data sets?
>>>
>>> spaMM can handle large data sets, but the first issue to consider here
>>> is the number of distinct locations for the spatial random effect. The
>>> large correlation matrices of geostatistical models will always be a
>>> problem, both in terms of memory requirements and of potentially huge
>>> computation times. My guess from past experiments is that one should
>>> still be able to fit models with ~ 10K locations within a few days on a
>>> computer with <60 Gb of RAM (given perhaps some tinkering of the
>>> arguments), so at least the data set of 14 000 rows should be feasible,
>>> particularly if the number of locations is smaller.
>>>
>>> Anyone planning to analyze large spatial data sets should anticipate
>>> these problems and check by themselves whether there is any practical
>>> alternative suitable for their particular problem. The discussion in
>>> section 6.2 of the "gentle introduction" to spaMM may then be useful.
>>>
>>> Best,
>>>
>>> F.
>>>
>>> >
>>> > Thank you,
>>> >
>>> > Sarah
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Fri Jul 17 15:31:51 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Fri, 17 Jul 2020 15:31:51 +0200
Subject: [R-sig-ME] graphics to show crossed random-effects
In-Reply-To: <CACgv6yXpgWB0rC=r3+pL9nA8atxKCHZBTnY6AiZscFQnMa2DTA@mail.gmail.com>
References: <CACgv6yXpgWB0rC=r3+pL9nA8atxKCHZBTnY6AiZscFQnMa2DTA@mail.gmail.com>
Message-ID: <CAJuCY5yJoH=SHCY72TOAMzfOLM_y=QCii-qTPm44qKbXegzCoQ@mail.gmail.com>

Dear Simon,

I think you need to plot the crossed random effects on different axes to
illustrate their independence. E.g. the first on the x-axis, the second on
the y-axis and the density on the z-axis.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op do 16 jul. 2020 om 20:34 schreef Simon Harmel <sim.harmel at gmail.com>:

> Good afternoon,
>
> Given lme4 package in R, I often show a random-intercept model (i.e., y~1 +
> (1|grouping)) using a picture like:
> https://github.com/hkil/m/blob/master/mlm.PNG.
>
> I wonder how to extend this picture to show a fully crossed
> random-intercepts model of the form: y~1 + (1|grouping_1) + (1|grouping_2)
> ?
>
> Specifically, do I need to have 2 red, level-2 distributions (side-by-side)
> each for showing the random intercepts of each grouping_j and if so, is
> there any way to show the "crossing*"* of the two random-effects in the
> picture?
>
> Many thanks,
> Simon
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Fri Jul 17 19:37:57 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Fri, 17 Jul 2020 12:37:57 -0500
Subject: [R-sig-ME] graphics to show crossed random-effects
In-Reply-To: <CAJuCY5yJoH=SHCY72TOAMzfOLM_y=QCii-qTPm44qKbXegzCoQ@mail.gmail.com>
References: <CACgv6yXpgWB0rC=r3+pL9nA8atxKCHZBTnY6AiZscFQnMa2DTA@mail.gmail.com>
 <CAJuCY5yJoH=SHCY72TOAMzfOLM_y=QCii-qTPm44qKbXegzCoQ@mail.gmail.com>
Message-ID: <CACgv6yXcX44JFjx1p+i2EynhDhY+7M2z+KoeengTggun=MaBgQ@mail.gmail.com>

Dear Thierry,

Thanks, what do you think about this one:
https://github.com/hkil/m/blob/master/Cros.PNG

Simon

On Fri, Jul 17, 2020 at 8:32 AM Thierry Onkelinx <thierry.onkelinx at inbo.be>
wrote:

> Dear Simon,
>
> I think you need to plot the crossed random effects on different axes to
> illustrate their independence. E.g. the first on the x-axis, the second on
> the y-axis and the density on the z-axis.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op do 16 jul. 2020 om 20:34 schreef Simon Harmel <sim.harmel at gmail.com>:
>
>> Good afternoon,
>>
>> Given lme4 package in R, I often show a random-intercept model (i.e., y~1
>> +
>> (1|grouping)) using a picture like:
>> https://github.com/hkil/m/blob/master/mlm.PNG.
>>
>> I wonder how to extend this picture to show a fully crossed
>> random-intercepts model of the form: y~1 + (1|grouping_1) +
>> (1|grouping_2) ?
>>
>> Specifically, do I need to have 2 red, level-2 distributions
>> (side-by-side)
>> each for showing the random intercepts of each grouping_j and if so, is
>> there any way to show the "crossing*"* of the two random-effects in the
>> picture?
>>
>> Many thanks,
>> Simon
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Sat Jul 18 22:19:02 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sat, 18 Jul 2020 16:19:02 -0400
Subject: [R-sig-ME] gls() results for unbalanced data with corStruct
 correlation class change if Initialize() is called first
In-Reply-To: <B7C7506F-0E5F-4E39-A7BA-9DC544E6262B@virginia.edu>
References: <B0BDE7BD-E24A-4BF7-904D-298B5E512E08@virginia.edu>
 <B7C7506F-0E5F-4E39-A7BA-9DC544E6262B@virginia.edu>
Message-ID: <01be844c-f688-3de3-ccb5-6e14b4e101fe@gmail.com>

[keeping r-sig-mixed-models in Cc: in case anyone wants to follow along]

I think this is worth a bug report. From https://www.r-project.org/bugs.html

 > In order to get a bugzilla account (i.e., become ?member?), please 
send an e-mail (from the address you want to use as your login) to 
|bug-report-request at r-project.org| briefly explaining why, and a 
volunteer will add you to R?s Bugzilla members.

 ? There are lots of warnings on the page about doing your due diligence 
before reporting a bug, but I think you have ...

 ? While this is definitely worth reporting, I don't know if it will get 
fixed quickly. It's my impression that nlme is semi-orphaned - i.e. it's 
getting patched to maintain compatibility with changes in R, and some 
cleanup/technical debt payoff, but there are e.g. 14 bugs reported at 
https://bugs.r-project.org/bugzilla/buglist.cgi?quicksearch=lme&list_id=14586

See also https://svn.r-project.org/R-packages/trunk/nlme/ for 
development status/pace.

 ? cheers

 ??? Ben Bolker


On 7/18/20 10:53 AM, Mychaleckyj, Josyf C (jcm6t) wrote:
> Ben,
> Thanks for your response. This caused no end of problems in our custom corStruct class because we assumed it was in our custom code but we could not see how. It?s even more puzzling because the first statement in Initialize.corCompSymm should explicitly handle repeated calls to the function through testing the ?inf? attribute.
>
> What is the protocol now ? Should I file this ? I don?t have an R Bugzilla account.
>
> Thanks,  Joe.
>
>> On Jul 16, 2020, at 11:21 AM, Mychaleckyj, Josyf C (jcm6t) <jcm6t at virginia.edu> wrote:
>>
>>
>> I?ve encountered a strange situation where the results of a gls model with a corStruct correlation structure are affected by whether Initialize() is explicitly called before the gls() model is executed.
>>
>> This seems to be restricted to unbalanced data sets.  I have pasted a test case using the Orthodont data set and corCompSymm below, although I first noticed it while running a custom corStruct class so it would appear to be a more general problem than this test case as you might expect. I have not tested this in lme().
>>
>> I?m aware that calling Initialize() outside of gls or lme is generally discouraged but was surprised that it changed the analysis results. You could imagine calling it in an interactive modeling session to view the correlation structure without knowing that you might have changed future results. I would have expected the results to be identical.
>>
>> I?d also like to know - which is the ?correct? result or are they considered practically equivalent, although the differences seem quite large for the latter.
>>
>> Just to the clear, this is not about the relative merits of balanced vs unbalanced data. Its about the veracity of the results.
>>
>> Thanks,
>> Joe.
>>
>>
>>
>> R version 3.6.0 (Linux CentOS 7)
>>
>>> library(nlme)
>>>
>>> packageVersion('nlme')
>> [1] ?3.1.147?
>>> #?????????????????????
>>> # the values are fixed for all models
>>> rho <- 0.3
>>> fixform <- distance ~ age + factor(Sex)
>>>
>>> # remove the grouping and other classes to keep it simple
>>> # full balanced data set
>>> Obal<-as.data.frame(Orthodont)
>>> class(Obal)
>> [1] "data.frame"
>>> # create unbalanced Orthodont data set with no singletons
>>> Ounbal <-as.data.frame(Orthodont[c(3,4,5, 7,8, 11, 12, 15, 16, 19, 20:28,31,32,35,36,39,
>> + 40,41,43,44,47,48,51,52,53,55,56,59,60,63,64,67,68,71,72,75,76,
>> + 79,80,83:85,87,88,91,92,95,96,99:101,103:105,107,108),])
>>> # sparser unbalanced data set with singletons
>>> Ounbal2<-as.data.frame(Orthodont[c(1,5,9,13,17,18,21:23,30:31,44,45:47,56:58,65:71,74:76,77,81,85,89,90,
>> +  93:94, 97,101:108),])
>>> Subjs<-c(paste0(c("M"),sprintf("%02d",1:16)), paste0(c("F"),sprintf("%02d",1:11)))
>>>
>>> table(Obal$Subject)[Subjs]
>> M01 M02 M03 M04 M05 M06 M07 M08 M09 M10 M11 M12 M13 M14 M15 M16 F01 F02 F03 F04
>>   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4
>> F05 F06 F07 F08 F09 F10 F11
>>   4   4   4   4   4   4   4
>>> table(Ounbal$Subject)[Subjs]
>> M01 M02 M03 M04 M05 M06 M07 M08 M09 M10 M11 M12 M13 M14 M15 M16 F01 F02 F03 F04
>>   2   3   2   2   2   4   4   2   2   2   3   2   2   3   2   2   2   2   2   2
>> F05 F06 F07 F08 F09 F10 F11
>>   2   3   2   2   2   3   3
>>> table(Ounbal2$Subject)[Subjs]
>> M01  M02  M03  M04  M05  M06 <NA>  M08 <NA> <NA>  M11  M12 <NA>  M14  M15 <NA>
>>    1    1    1    1    2    3         2              1    3         1    2
>> F01  F02  F03  F04  F05  F06  F07  F08  F09  F10  F11
>>    4    3    3    1    1    1    2    2    1    4    4
>>> #----
>>> # test with the full balanced data set
>>> # pre-calling Initialize doesn't seem to make a difference
>>> csB <- corCompSymm(value = rho, form = ~ 1 | Subject)
>>> csB<-Initialize(csB, data=Obal)
>>>
>>> print(mod<-gls(fixform, correlation=csB, data=Obal))
>> Generalized least squares fit by REML
>>   Model: fixform
>>   Data: Obal
>>   Log-restricted-likelihood: -218.7563
>>
>> Coefficients:
>> (Intercept)         age   SexFemale
>> 17.7067130   0.6601852  -2.3210227
>>
>> Correlation Structure: Compound symmetry
>> Formula: ~1 | Subject
>> Parameter estimate(s):
>>       Rho
>> 0.6144908
>> Degrees of freedom: 108 total; 105 residual
>> Residual standard error: 2.305696
>>> # compare with direct call
>>> print(mod2<-gls(fixform, correlation=corCompSymm(value = rho, form = ~ 1 | Subject), data=Obal) )
>> Generalized least squares fit by REML
>>   Model: fixform
>>   Data: Obal
>>   Log-restricted-likelihood: -218.7563
>>
>> Coefficients:
>> (Intercept)         age   SexFemale
>> 17.7067130   0.6601852  -2.3210227
>>
>> Correlation Structure: Compound symmetry
>> Formula: ~1 | Subject
>> Parameter estimate(s):
>>       Rho
>> 0.6144908
>> Degrees of freedom: 108 total; 105 residual
>> Residual standard error: 2.305696
>>> #--------------
>>> # compare Unbalanced:
>>> csU <- corCompSymm(value = rho, form = ~ 1 | Subject)
>>> csU<-Initialize(csU, data=Ounbal)
>>>
>>> print(mod3<-gls(fixform, correlation=csU, data=Ounbal) )
>> Generalized least squares fit by REML
>>   Model: fixform
>>   Data: Ounbal
>>   Log-restricted-likelihood: -130.7703
>>
>> Coefficients:
>> (Intercept)         age   SexFemale
>>   18.046228    0.659606   -3.262389
>>
>> Correlation Structure: Compound symmetry
>> Formula: ~1 | Subject
>> Parameter estimate(s):
>>       Rho
>> 0.7211814
>> Degrees of freedom: 64 total; 61 residual
>> Residual standard error: 2.378517
>>> # compare with direct call
>>> print(mod4<-gls(fixform, correlation=corCompSymm(value = rho, form = ~ 1 | Subject), data=Ounbal) )
>> Generalized least squares fit by REML
>>   Model: fixform
>>   Data: Ounbal
>>   Log-restricted-likelihood: -127.616
>>
>> Coefficients:
>> (Intercept)         age   SexFemale
>>   18.330637    0.634451   -2.947638
>>
>> Correlation Structure: Compound symmetry
>> Formula: ~1 | Subject
>> Parameter estimate(s):
>>       Rho
>> 0.7598463
>> Degrees of freedom: 64 total; 61 residual
>> Residual standard error: 2.34705
>>> #----
>>> # compare with Unbalanced 2 with singletons
>>> csU2 <- corCompSymm(value = rho, form = ~ 1 | Subject)
>>> csU2<-Initialize(csU2, data=Ounbal2)
>>>
>>> print(mod5<-gls(fixform, correlation=csU2, data=Ounbal2))
>> Generalized least squares fit by REML
>>   Model: fixform
>>   Data: Ounbal2
>>   Log-restricted-likelihood: -74.02154
>>
>> Coefficients:
>> (Intercept)         age   SexFemale
>> 18.5018545   0.5265219  -1.4587968
>>
>> Correlation Structure: Compound symmetry
>> Formula: ~1 | Subject
>> Parameter estimate(s):
>>       Rho
>> 0.8736732
>> Degrees of freedom: 44 total; 41 residual
>> Residual standard error: 1.94714
>>> # compare with direct call
>>> print(mod6<-gls(fixform, correlation=corCompSymm(value = rho, form = ~ 1 | Subject), data=Ounbal2))
>> Generalized least squares fit by REML
>>   Model: fixform
>>   Data: Ounbal2
>>   Log-restricted-likelihood: -77.29477
>>
>> Coefficients:
>> (Intercept)         age   SexFemale
>> 18.4979634   0.5550817  -1.6893916
>>
>> Correlation Structure: Compound symmetry
>> Formula: ~1 | Subject
>> Parameter estimate(s):
>>       Rho
>> 0.8516777
>> Degrees of freedom: 44 total; 41 residual
>> Residual standard error: 2.024012

	[[alternative HTML version deleted]]


From y@@hree19 @end|ng |rom gm@||@com  Sun Jul 19 12:01:53 2020
From: y@@hree19 @end|ng |rom gm@||@com (Yashree Mehta)
Date: Sun, 19 Jul 2020 12:01:53 +0200
Subject: [R-sig-ME] Random slope with cross-level interaction
Message-ID: <CAOE=hq+V+LZQgVOvR4=Neap6OY=rCin-JCHP341Cs7Z4594evg@mail.gmail.com>

Hi,

I have the following model:

Production = seed + fertilizer + fertilizer : wheatlanduse + (1 +
fertilizer | Household)

As the formula indicates, the household level is specified as the random
intercept. Fertilizer is specified as random slope , and has also been
specified as a fixed effect.

I am interested in cross-level interaction between fertilizer and the
wheatlanduse variable. So, I have inserted "fertilizer : wheatlanduse".

My question is: Do I have to include "wheatlanduse" as a main effect in the
formula as well? Or is it acceptable to only have it as a part of the
interaction term?

Thank you,

Regards,
Yashree

	[[alternative HTML version deleted]]


From @@|@h@d|n@|ot|| @end|ng |rom gm@||@com  Sun Jul 19 17:45:49 2020
From: @@|@h@d|n@|ot|| @end|ng |rom gm@||@com (Salahadin Lotfi)
Date: Sun, 19 Jul 2020 10:45:49 -0500
Subject: [R-sig-ME] Random slope with cross-level interaction
In-Reply-To: <CAOE=hq+V+LZQgVOvR4=Neap6OY=rCin-JCHP341Cs7Z4594evg@mail.gmail.com>
References: <CAOE=hq+V+LZQgVOvR4=Neap6OY=rCin-JCHP341Cs7Z4594evg@mail.gmail.com>
Message-ID: <CAJTPX_XwL5zoxd1Suc8uS5+RXzE8KHNSXh+Q=oe5ku4kqkmHSg@mail.gmail.com>

Hi Yashree,
The interpretation of the interaction term do change whether you include
the main effect of not.
Usually having only the interaction term in the model requires a specific
hypothesis. Thus, the lower level terms (i.e., the main effects) are almost
always included.
The lmer function automatically includes the lower level terms even if you
just include an interaction term.
For example, if you setup your model as follow, the main effects of
fertilizer and wheatlanduse will be still taken into account.
Production = seed + fertilizer : wheatlanduse + (1 + fertilizer |
Household)

Thanks,
Sala

*************
Salahadin (Sala) Lotfi

PhD Candidate of Cognitive Psychology & Neuroscience

University of Wisconsin-Milwaukee

Anxiety Disorders Laboratory

President, Association of Clinical and Cognitive Neuroscience, UWM

On Sun, Jul 19, 2020 at 5:02 AM Yashree Mehta <yashree19 at gmail.com> wrote:

> Hi,
>
> I have the following model:
>
> Production = seed + fertilizer + fertilizer : wheatlanduse + (1 +
> fertilizer | Household)
>
> As the formula indicates, the household level is specified as the random
> intercept. Fertilizer is specified as random slope , and has also been
> specified as a fixed effect.
>
> I am interested in cross-level interaction between fertilizer and the
> wheatlanduse variable. So, I have inserted "fertilizer : wheatlanduse".
>
> My question is: Do I have to include "wheatlanduse" as a main effect in the
> formula as well? Or is it acceptable to only have it as a part of the
> interaction term?
>
> Thank you,
>
> Regards,
> Yashree
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From y@@hree19 @end|ng |rom gm@||@com  Sun Jul 19 18:59:50 2020
From: y@@hree19 @end|ng |rom gm@||@com (Yashree Mehta)
Date: Sun, 19 Jul 2020 18:59:50 +0200
Subject: [R-sig-ME] Random slope with cross-level interaction
In-Reply-To: <CAOE=hqJmuqvUK9=FhZEf46zbEZZjhZCDtHyYHmmTuPK8ftJ=aQ@mail.gmail.com>
References: <CAOE=hq+V+LZQgVOvR4=Neap6OY=rCin-JCHP341Cs7Z4594evg@mail.gmail.com>
 <CAJTPX_XwL5zoxd1Suc8uS5+RXzE8KHNSXh+Q=oe5ku4kqkmHSg@mail.gmail.com>
 <CAOE=hqJmuqvUK9=FhZEf46zbEZZjhZCDtHyYHmmTuPK8ftJ=aQ@mail.gmail.com>
Message-ID: <CAOE=hq+GbS83YvfLkfd6rQXXjRzK0CAF1+PH34iir-sDD2uEJw@mail.gmail.com>

Hi Salahadin,

Thanks for your reply. It is very helpful.

Then, is there a way I can extract the coefficient estimate of the fixed
effect of fertilizer as well as that of wheatlanduse as a main effect?

Regards,
Yashree

On Sun, Jul 19, 2020 at 6:59 PM Yashree Mehta <yashree19 at gmail.com> wrote:

> Hi Salahadin,
>
> Thanks for your reply. It is very helpful.
>
> Then, is there a way I can extract the coefficient estimate of the fixed
> effect of fertilizer as well as that of wheatlanduse as a main effect?
>
> Regards,
> Yashree
>
> On Sun, Jul 19, 2020 at 5:46 PM Salahadin Lotfi <salahadin.lotfi at gmail.com>
> wrote:
>
>> Hi Yashree,
>> The interpretation of the interaction term do change whether you include
>> the main effect of not.
>> Usually having only the interaction term in the model requires a specific
>> hypothesis. Thus, the lower level terms (i.e., the main effects) are almost
>> always included.
>> The lmer function automatically includes the lower level terms even if
>> you just include an interaction term.
>> For example, if you setup your model as follow, the main effects of
>> fertilizer and wheatlanduse will be still taken into account.
>> Production = seed + fertilizer : wheatlanduse + (1 + fertilizer |
>> Household)
>>
>> Thanks,
>> Sala
>>
>> *************
>> Salahadin (Sala) Lotfi
>>
>> PhD Candidate of Cognitive Psychology & Neuroscience
>>
>> University of Wisconsin-Milwaukee
>>
>> Anxiety Disorders Laboratory
>>
>> President, Association of Clinical and Cognitive Neuroscience, UWM
>>
>> On Sun, Jul 19, 2020 at 5:02 AM Yashree Mehta <yashree19 at gmail.com>
>> wrote:
>>
>>> Hi,
>>>
>>> I have the following model:
>>>
>>> Production = seed + fertilizer + fertilizer : wheatlanduse + (1 +
>>> fertilizer | Household)
>>>
>>> As the formula indicates, the household level is specified as the random
>>> intercept. Fertilizer is specified as random slope , and has also been
>>> specified as a fixed effect.
>>>
>>> I am interested in cross-level interaction between fertilizer and the
>>> wheatlanduse variable. So, I have inserted "fertilizer : wheatlanduse".
>>>
>>> My question is: Do I have to include "wheatlanduse" as a main effect in
>>> the
>>> formula as well? Or is it acceptable to only have it as a part of the
>>> interaction term?
>>>
>>> Thank you,
>>>
>>> Regards,
>>> Yashree
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Sun Jul 19 20:35:32 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sun, 19 Jul 2020 14:35:32 -0400
Subject: [R-sig-ME] Random slope with cross-level interaction
In-Reply-To: <CAOE=hq+GbS83YvfLkfd6rQXXjRzK0CAF1+PH34iir-sDD2uEJw@mail.gmail.com>
References: <CAOE=hq+V+LZQgVOvR4=Neap6OY=rCin-JCHP341Cs7Z4594evg@mail.gmail.com>
 <CAJTPX_XwL5zoxd1Suc8uS5+RXzE8KHNSXh+Q=oe5ku4kqkmHSg@mail.gmail.com>
 <CAOE=hqJmuqvUK9=FhZEf46zbEZZjhZCDtHyYHmmTuPK8ftJ=aQ@mail.gmail.com>
 <CAOE=hq+GbS83YvfLkfd6rQXXjRzK0CAF1+PH34iir-sDD2uEJw@mail.gmail.com>
Message-ID: <d87cf50b-6e7b-0cf8-f5c6-c19bdeac6dd0@gmail.com>

 ? Assuming that fertilizer is a numeric covariate and wheatlanduse is 
categorical, you might want

lmer( Production ~ seed + fertilizer *wheatlanduse + (1 + fertilizer | 
Household) , contrasts=list(wheatlanduse=contr.sum), ...)

 ? that would mean that the main effect for fertilizer would represent 
the *average* effect across levels of wheatlanduse.? The estimated main 
effect of wheatlanduse will represent the expected differences across 
levels at zero fertilizer (or, if you center fertilizer by subtracting 
the mean, at the average fertilizer level)

 ?? I'm not sure what you mean by "cross-level interaction" ...




On 7/19/20 12:59 PM, Yashree Mehta wrote:
> Hi Salahadin, Thanks for your reply. It is very helpful. Then, is 
> there a way I can extract the coefficient estimate of the fixed effect 
> of fertilizer as well as that of wheatlanduse as a main effect? 
> Regards, Yashree On Sun, Jul 19, 2020 at 6:59 PM Yashree Mehta 
> <yashree19 at gmail.com> wrote:
>> Hi Salahadin, Thanks for your reply. It is very helpful. Then, is 
>> there a way I can extract the coefficient estimate of the fixed 
>> effect of fertilizer as well as that of wheatlanduse as a main 
>> effect? Regards, Yashree On Sun, Jul 19, 2020 at 5:46 PM Salahadin 
>> Lotfi <salahadin.lotfi at gmail.com> wrote:
>>> Hi Yashree, The interpretation of the interaction term do change 
>>> whether you include the main effect of not. Usually having only the 
>>> interaction term in the model requires a specific hypothesis. Thus, 
>>> the lower level terms (i.e., the main effects) are almost always 
>>> included. The lmer function automatically includes the lower level 
>>> terms even if you just include an interaction term. For example, if 
>>> you setup your model as follow, the main effects of fertilizer and 
>>> wheatlanduse will be still taken into account. Production = seed + 
>>> fertilizer : wheatlanduse + (1 + fertilizer | Household) Thanks, 
>>> Sala ************* Salahadin (Sala) Lotfi PhD Candidate of Cognitive 
>>> Psychology & Neuroscience University of Wisconsin-Milwaukee Anxiety 
>>> Disorders Laboratory President, Association of Clinical and 
>>> Cognitive Neuroscience, UWM On Sun, Jul 19, 2020 at 5:02 AM Yashree 
>>> Mehta <yashree19 at gmail.com> wrote:
>>>> Hi, I have the following model: Production = seed + fertilizer + 
>>>> fertilizer : wheatlanduse + (1 + fertilizer | Household) As the 
>>>> formula indicates, the household level is specified as the random 
>>>> intercept. Fertilizer is specified as random slope , and has also 
>>>> been specified as a fixed effect. I am interested in cross-level 
>>>> interaction between fertilizer and the wheatlanduse variable. So, I 
>>>> have inserted "fertilizer : wheatlanduse". My question is: Do I 
>>>> have to include "wheatlanduse" as a main effect in the formula as 
>>>> well? Or is it acceptable to only have it as a part of the 
>>>> interaction term? Thank you, Regards, Yashree [[alternative HTML 
>>>> version deleted]] _______________________________________________ 
>>>> R-sig-mixed-models at r-project.org mailing list 
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models 
> [[alternative HTML version deleted]] 
> _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From @@|@h@d|n@|ot|| @end|ng |rom gm@||@com  Sun Jul 19 22:19:03 2020
From: @@|@h@d|n@|ot|| @end|ng |rom gm@||@com (Salahadin Lotfi)
Date: Sun, 19 Jul 2020 15:19:03 -0500
Subject: [R-sig-ME] Random slope with cross-level interaction
In-Reply-To: <d87cf50b-6e7b-0cf8-f5c6-c19bdeac6dd0@gmail.com>
References: <CAOE=hq+V+LZQgVOvR4=Neap6OY=rCin-JCHP341Cs7Z4594evg@mail.gmail.com>
 <CAJTPX_XwL5zoxd1Suc8uS5+RXzE8KHNSXh+Q=oe5ku4kqkmHSg@mail.gmail.com>
 <CAOE=hqJmuqvUK9=FhZEf46zbEZZjhZCDtHyYHmmTuPK8ftJ=aQ@mail.gmail.com>
 <CAOE=hq+GbS83YvfLkfd6rQXXjRzK0CAF1+PH34iir-sDD2uEJw@mail.gmail.com>
 <d87cf50b-6e7b-0cf8-f5c6-c19bdeac6dd0@gmail.com>
Message-ID: <CAJTPX_Vx6dxOrRd2U1LZS5vppnKM6tNqx_N1u5Rm=ATjoVwESw@mail.gmail.com>

I am not sure this is what you are asking, but did you try *summary**( your
model) *function? It prints out the main effect coefficients (and more
info) that you are looking for.
Also, run the library(lmerTest) to get a f-like table of type III sum of
the squares using *anova (your model) *function.
I would recommend going through this lmer vignette as well.
https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf



On Sun, Jul 19, 2020 at 1:35 PM Ben Bolker <bbolker at gmail.com> wrote:

>    Assuming that fertilizer is a numeric covariate and wheatlanduse is
> categorical, you might want
>
> lmer( Production ~ seed + fertilizer *wheatlanduse + (1 + fertilizer |
> Household) , contrasts=list(wheatlanduse=contr.sum), ...)
>
>    that would mean that the main effect for fertilizer would represent
> the *average* effect across levels of wheatlanduse.  The estimated main
> effect of wheatlanduse will represent the expected differences across
> levels at zero fertilizer (or, if you center fertilizer by subtracting
> the mean, at the average fertilizer level)
>
>     I'm not sure what you mean by "cross-level interaction" ...
>
>
>
>
> On 7/19/20 12:59 PM, Yashree Mehta wrote:
> > Hi Salahadin, Thanks for your reply. It is very helpful. Then, is
> > there a way I can extract the coefficient estimate of the fixed effect
> > of fertilizer as well as that of wheatlanduse as a main effect?
> > Regards, Yashree On Sun, Jul 19, 2020 at 6:59 PM Yashree Mehta
> > <yashree19 at gmail.com> wrote:
> >> Hi Salahadin, Thanks for your reply. It is very helpful. Then, is
> >> there a way I can extract the coefficient estimate of the fixed
> >> effect of fertilizer as well as that of wheatlanduse as a main
> >> effect? Regards, Yashree On Sun, Jul 19, 2020 at 5:46 PM Salahadin
> >> Lotfi <salahadin.lotfi at gmail.com> wrote:
> >>> Hi Yashree, The interpretation of the interaction term do change
> >>> whether you include the main effect of not. Usually having only the
> >>> interaction term in the model requires a specific hypothesis. Thus,
> >>> the lower level terms (i.e., the main effects) are almost always
> >>> included. The lmer function automatically includes the lower level
> >>> terms even if you just include an interaction term. For example, if
> >>> you setup your model as follow, the main effects of fertilizer and
> >>> wheatlanduse will be still taken into account. Production = seed +
> >>> fertilizer : wheatlanduse + (1 + fertilizer | Household) Thanks,
> >>> Sala ************* Salahadin (Sala) Lotfi PhD Candidate of Cognitive
> >>> Psychology & Neuroscience University of Wisconsin-Milwaukee Anxiety
> >>> Disorders Laboratory President, Association of Clinical and
> >>> Cognitive Neuroscience, UWM On Sun, Jul 19, 2020 at 5:02 AM Yashree
> >>> Mehta <yashree19 at gmail.com> wrote:
> >>>> Hi, I have the following model: Production = seed + fertilizer +
> >>>> fertilizer : wheatlanduse + (1 + fertilizer | Household) As the
> >>>> formula indicates, the household level is specified as the random
> >>>> intercept. Fertilizer is specified as random slope , and has also
> >>>> been specified as a fixed effect. I am interested in cross-level
> >>>> interaction between fertilizer and the wheatlanduse variable. So, I
> >>>> have inserted "fertilizer : wheatlanduse". My question is: Do I
> >>>> have to include "wheatlanduse" as a main effect in the formula as
> >>>> well? Or is it acceptable to only have it as a part of the
> >>>> interaction term? Thank you, Regards, Yashree [[alternative HTML
> >>>> version deleted]] _______________________________________________
> >>>> R-sig-mixed-models at r-project.org mailing list
> >>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > [[alternative HTML version deleted]]
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From @ed|@nd @end|ng |rom he@|th@uc@d@edu  Sun Jul 19 18:44:57 2020
From: @ed|@nd @end|ng |rom he@|th@uc@d@edu (Edland, Steven)
Date: Sun, 19 Jul 2020 16:44:57 +0000
Subject: [R-sig-ME] nlme:lme pseudo-replicate degrees of freedom
Message-ID: <BYAPR19MB2470D0B0F9258E57A13955D3E67A0@BYAPR19MB2470.namprd19.prod.outlook.com>

I notice that nlme:lme (vers. 3.1.145) still has the pseudo-replicate degrees of freedom bug when applied to the usual random slopes model of longitudinal repeated measures.

Has anyone written a lmerTest equivalent wrapper for lme, or is there any other available work-around?

Thank you in advance for any advice on this issue.

Sincerely,
Steve Edland

Steven D. Edland, Ph.D.
Professor
Dept. of Family Medicine & Public Health
Dept. of Neurosciences
University of California, San Diego
9500 Gilman Dr. M/C 0948
La Jolla, CA 92093-0948
http://biostat.ucsd.edu/sedland.htm

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Tue Jul 21 01:35:48 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 20 Jul 2020 19:35:48 -0400
Subject: [R-sig-ME] nlme:lme pseudo-replicate degrees of freedom
In-Reply-To: <BYAPR19MB2470D0B0F9258E57A13955D3E67A0@BYAPR19MB2470.namprd19.prod.outlook.com>
References: <BYAPR19MB2470D0B0F9258E57A13955D3E67A0@BYAPR19MB2470.namprd19.prod.outlook.com>
Message-ID: <f55a4922-28c0-b977-d10d-981580277cad@gmail.com>

 ?? Can you point to a reference that explains the issue (e.g. mailing 
list thread or ... something on the R bug tracker or ... ?)? I do know 
that there's an issue with df of random-slopes models which is 
illustrated under "Df alternatives" at 
https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#why-doesnt-lme4-display-denominator-degrees-of-freedomp-values-what-other-options-do-i-have

 ?I wrote some code (linked from the above discussion, available from 
https://bbolker.github.io/mixedmodels-misc/R/calcDenDF.R ) ; it uses the 
same kind of "semantic"/containment method, nothing with a quantitative 
basis like Satterthwaite/etc., but it does do a somewhat better job on 
random-slopes models. (I'm sure it could be improved.)

On 7/19/20 12:44 PM, Edland, Steven wrote:
> I notice that nlme:lme (vers. 3.1.145) still has the pseudo-replicate degrees of freedom bug when applied to the usual random slopes model of longitudinal repeated measures.
>
> Has anyone written a lmerTest equivalent wrapper for lme, or is there any other available work-around?
>
> Thank you in advance for any advice on this issue.
>
> Sincerely,
> Steve Edland
>
> Steven D. Edland, Ph.D.
> Professor
> Dept. of Family Medicine & Public Health
> Dept. of Neurosciences
> University of California, San Diego
> 9500 Gilman Dr. M/C 0948
> La Jolla, CA 92093-0948
> http://biostat.ucsd.edu/sedland.htm
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From t|mothy@@@|@u @end|ng |rom gm@||@com  Wed Jul 22 01:17:19 2020
From: t|mothy@@@|@u @end|ng |rom gm@||@com (Timothy Lau)
Date: Tue, 21 Jul 2020 17:17:19 -0600
Subject: [R-sig-ME] Non-Linear Mixed Effect Model - 4 vs. 3 parameter Sigmoid
Message-ID: <CAJM8_p=wZfid-+QV23n-7iCtvvk2zU1GMwPBHjgXBtZ3YX2YeQ@mail.gmail.com>

`?nlmer` gives a quick example of a non-linear regression model using the
datasets::Orange dataset.

`nlmer(formula = circumference ~ SSlogis(input = age, Asym = Asym, xmid =
xmid, scal = scal) ~ Asym | Tree, start = c(Asym = 200, xmid = 725, scal =
350), nAGQ = 1, data = orange2)`

I wanted to switch the 3 parameter stats::SSlogis() for the 4 parameter
FlexParamCurve::SSposnegRichards()

```
m1_start_val <- getInitial(circumference ~ SSposnegRichards(x = age, Asym =
Asym, K = K, Infl = Infl, M = M, modno = 12, pn.options = "myoptions"),
data = Orange)
m1_parm4 <- nlmer(formula = circumference ~ SSposnegRichards(x = age, Asym
= Asym, K = K, Infl = Infl, M = M,, modno = 12, pn.options = "myoptions") ~
Asym | Tree,
      start = m1_start_val,
      nAGQ = 1,
      verbose = T,
      data = Orange)
```

But I keep getting errors, with the above code I get that "Error in
mkRespMod(fr, nlenv = nlenv, nlmod = nlmod) : is.matrix(gr <- attr(val,
"gradient")) is not TRUE"

Does anyone have some advice for a better way to get this to run?



Best,
Timothy


The will to win means nothing without the will to prepare.
Enthusiasm is common. Endurance is rare.
- J. Ikangaa; A. Duckworth;

	[[alternative HTML version deleted]]


From |r@nco|@@duchenne @end|ng |rom mnhn@|r  Fri Jul 24 14:59:56 2020
From: |r@nco|@@duchenne @end|ng |rom mnhn@|r (=?utf-8?Q?Fran=C3=A7ois?= DUCHENNE)
Date: Fri, 24 Jul 2020 14:59:56 +0200 (CEST)
Subject: [R-sig-ME] glmmTMB: temporal autocorrelation and perfect fit
Message-ID: <301682899.838151.1595595596700.JavaMail.zimbra@mnhn.fr>

Dear all, 
I am a PhD student curerntly working with time series in Ecology. 
I have a question about temporal autocorrelation and the way to implement it in glmmTMB. 
Here ( [ https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html | https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html ] ), with an autoregressive process (AR1), they fit time series with one point by time step. 
However, when I am doing that I get surprisingly high r squared (r?>0.7), with simulated data and with empirical data, see for example the reproductible example attached with this email, which is from [ https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html | https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html ] . 

When I add some fixed explanatory variable I often get R?~1, I am wondering about the meaning of fitting such model? Is it normal that including the temporal autocorrelation process gives such R? and almost a perfect fit ? (largely due to the random part, fixed part often explains 10% of the variance in my data). Is the model still interpretable ? 

Thanks very much for taking time to read this, 
Best regards, 
Fran?ois Duchenne 


From @|m@h@rme| @end|ng |rom gm@||@com  Mon Jul 27 05:48:25 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Sun, 26 Jul 2020 22:48:25 -0500
Subject: [R-sig-ME] Modeling Residuals vs. Modeling Random-effects
 Covariances
Message-ID: <CACgv6yUndPixddQd73+sfo0r9TrdiLHDcKBtyWWfSBBU0=ePwA@mail.gmail.com>

Dear all,

I had a quick question. The variance-covariance structure from the
"glmmTMB" package and the "nlme" package are not the same thing?

Is there any other package capable of modeling the Level-1 residual
variance-covariance structure like in "nlme"? *-- Many Thanks, Simon *




*glmmTMB::glmmTMB(y ~ times + ar1(times | subjects), data = data) ## DON'T
RUN   nlme::lme    (y ~ times, random = ~ times | subjects, correlation =
corAR1(), data = data) ## DON'T RUN *

	[[alternative HTML version deleted]]


From @pr||m@rt|n|g @end|ng |rom hotm@||@com  Mon Jul 27 19:40:30 2020
From: @pr||m@rt|n|g @end|ng |rom hotm@||@com (April Martinig)
Date: Mon, 27 Jul 2020 11:40:30 -0600
Subject: [R-sig-ME] Is there a leverage diagnostic test for glmmTMB models?
Message-ID: <CY4PR17MB100089F146210AA5DEEB87BCAA720@CY4PR17MB1000.namprd17.prod.outlook.com>

Hello,

I've posted my question to Cross Validated, but I am also linking to it here <https://stats.stackexchange.com/questions/478727/leverage-diagnostic-test-not-supported-for-glmmtmb-models-in-r> because it might have been missed.

I am trying to use the DHARMa package to test for how much leverage specific points (on one extreme) for my response variable has on my output. 

In my question I have posted the data, what I have tried, and linked to other sources. Thank you in advance.

Take care,
April

---
April Martinig, M.Sc.
Ph.D. Candidate in Ecology
University of Alberta
martinig.weebly.com
?I do not accept the conventional order of things as a given."

https://stats.stackexchange.com/questions/478727/leverage-diagnostic-test-not-supported-for-glmmtmb-models-in-r <https://stats.stackexchange.com/questions/478727/leverage-diagnostic-test-not-supported-for-glmmtmb-models-in-r>.
	[[alternative HTML version deleted]]


From j|ox @end|ng |rom mcm@@ter@c@  Mon Jul 27 21:47:41 2020
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Mon, 27 Jul 2020 15:47:41 -0400
Subject: [R-sig-ME] 
 Is there a leverage diagnostic test for glmmTMB models?
In-Reply-To: <7778_1595877031_06RJAVii021010_CY4PR17MB100089F146210AA5DEEB87BCAA720@CY4PR17MB1000.namprd17.prod.outlook.com>
References: <7778_1595877031_06RJAVii021010_CY4PR17MB100089F146210AA5DEEB87BCAA720@CY4PR17MB1000.namprd17.prod.outlook.com>
Message-ID: <8996a99a-4957-5985-035e-6475aa10a2f7@mcmaster.ca>

Dear April,

Unfortunately, there's no hatvalues() method for "glmmTMB" objects, nor 
do functions in the car package, where the influenceIndexPlot() function 
that you tried to use originates, have "glmmTMB" methods. car does have 
methods for the "merMod" objects produced by functions in the lme4 
package and for the "lme" objects produced by the lme() function in the 
nlme package, but those probably aren't useful to you.

I'm willing to look into providing the necessary infrastructure in the 
car package to support glmmTMB() but can't promise to do that before the 
Fall.

Best,
  John

--------------------------------------------
   John Fox
   Professor Emeritus
   McMaster University
   Hamilton, Ontario, Canada
   web: https://socialsciences.mcmaster.ca/jfox/

On 7/27/2020 1:40 PM, April Martinig wrote:
> Hello,
> 
> I've posted my question to Cross Validated, but I am also linking to it here <https://stats.stackexchange.com/questions/478727/leverage-diagnostic-test-not-supported-for-glmmtmb-models-in-r> because it might have been missed.
> 
> I am trying to use the DHARMa package to test for how much leverage specific points (on one extreme) for my response variable has on my output.
> 
> In my question I have posted the data, what I have tried, and linked to other sources. Thank you in advance.
> 
> Take care,
> April
> 
> ---
> April Martinig, M.Sc.
> Ph.D. Candidate in Ecology
> University of Alberta
> martinig.weebly.com
> ?I do not accept the conventional order of things as a given."
> 
> https://stats.stackexchange.com/questions/478727/leverage-diagnostic-test-not-supported-for-glmmtmb-models-in-r <https://stats.stackexchange.com/questions/478727/leverage-diagnostic-test-not-supported-for-glmmtmb-models-in-r>.
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From bbo|ker @end|ng |rom gm@||@com  Mon Jul 27 22:25:09 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 27 Jul 2020 16:25:09 -0400
Subject: [R-sig-ME] 
 Is there a leverage diagnostic test for glmmTMB models?
In-Reply-To: <8996a99a-4957-5985-035e-6475aa10a2f7@mcmaster.ca>
References: <7778_1595877031_06RJAVii021010_CY4PR17MB100089F146210AA5DEEB87BCAA720@CY4PR17MB1000.namprd17.prod.outlook.com>
 <8996a99a-4957-5985-035e-6475aa10a2f7@mcmaster.ca>
Message-ID: <f3f6c03a-bcfe-eabf-7256-780f04c87017@gmail.com>

 ? As I mentioned in my CrossValidated answer 
https://stats.stackexchange.com/questions/478727/leverage-diagnostic-test-not-supported-for-glmmtmb-models-in-r/479298#479298 
, this does work with the add-on influence_mixed function in the glmmTMB 
package ; you just need to specify? group=".cases" (or leave the group 
blank, since ".cases" is the default value).

 ?? This doesn't use hatvalues, which as I commented there aren't 
available (yet) for glmmTMB; instead, it does brute-force leave-one-out 
computations (although it can do them in parallel).

 ?? It would be a good idea making this functionality more easily 
available, but I've messed up in the past when trying to export 'car' 
functionality, so I'd like to be careful.


 ? cheers

 ??? Ben


On 7/27/20 3:47 PM, John Fox wrote:
> Dear April,
>
> Unfortunately, there's no hatvalues() method for "glmmTMB" objects, 
> nor do functions in the car package, where the influenceIndexPlot() 
> function that you tried to use originates, have "glmmTMB" methods. car 
> does have methods for the "merMod" objects produced by functions in 
> the lme4 package and for the "lme" objects produced by the lme() 
> function in the nlme package, but those probably aren't useful to you.
>
> I'm willing to look into providing the necessary infrastructure in the 
> car package to support glmmTMB() but can't promise to do that before 
> the Fall.
>
> Best,
> ?John
>
> --------------------------------------------
> ? John Fox
> ? Professor Emeritus
> ? McMaster University
> ? Hamilton, Ontario, Canada
> ? web: https://socialsciences.mcmaster.ca/jfox/
>
> On 7/27/2020 1:40 PM, April Martinig wrote:
>> Hello,
>>
>> I've posted my question to Cross Validated, but I am also linking to 
>> it here 
>> <https://stats.stackexchange.com/questions/478727/leverage-diagnostic-test-not-supported-for-glmmtmb-models-in-r> 
>> because it might have been missed.
>>
>> I am trying to use the DHARMa package to test for how much leverage 
>> specific points (on one extreme) for my response variable has on my 
>> output.
>>
>> In my question I have posted the data, what I have tried, and linked 
>> to other sources. Thank you in advance.
>>
>> Take care,
>> April
>>
>> ---
>> April Martinig, M.Sc.
>> Ph.D. Candidate in Ecology
>> University of Alberta
>> martinig.weebly.com
>> ?I do not accept the conventional order of things as a given."
>>
>> https://stats.stackexchange.com/questions/478727/leverage-diagnostic-test-not-supported-for-glmmtmb-models-in-r 
>> <https://stats.stackexchange.com/questions/478727/leverage-diagnostic-test-not-supported-for-glmmtmb-models-in-r>. 
>>
>> ????[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From j|ox @end|ng |rom mcm@@ter@c@  Mon Jul 27 23:03:16 2020
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Mon, 27 Jul 2020 17:03:16 -0400
Subject: [R-sig-ME] 
 Is there a leverage diagnostic test for glmmTMB models?
In-Reply-To: <15531_1595881530_06RKPU4v030404_f3f6c03a-bcfe-eabf-7256-780f04c87017@gmail.com>
References: <7778_1595877031_06RJAVii021010_CY4PR17MB100089F146210AA5DEEB87BCAA720@CY4PR17MB1000.namprd17.prod.outlook.com>
 <8996a99a-4957-5985-035e-6475aa10a2f7@mcmaster.ca>
 <15531_1595881530_06RKPU4v030404_f3f6c03a-bcfe-eabf-7256-780f04c87017@gmail.com>
Message-ID: <c421dc9e-637c-10d5-9f49-329876eb81ca@mcmaster.ca>

Hi Ben,

Perhaps you, Sandy Weisberg (to whom I'm cc'ing this response), and I 
can sort this out, possibly in the Fall, so that there's more coherent 
functionality for unusual-case diagnostics across a broader range of 
mixed-modeling functions.

Best,
  John

On 2020-07-27 4:25 p.m., Ben Bolker wrote:
>  ? As I mentioned in my CrossValidated answer 
> https://stats.stackexchange.com/questions/478727/leverage-diagnostic-test-not-supported-for-glmmtmb-models-in-r/479298#479298 
> , this does work with the add-on influence_mixed function in the glmmTMB 
> package ; you just need to specify? group=".cases" (or leave the group 
> blank, since ".cases" is the default value).
> 
>  ?? This doesn't use hatvalues, which as I commented there aren't 
> available (yet) for glmmTMB; instead, it does brute-force leave-one-out 
> computations (although it can do them in parallel).
> 
>  ?? It would be a good idea making this functionality more easily 
> available, but I've messed up in the past when trying to export 'car' 
> functionality, so I'd like to be careful.
> 
> 
>  ? cheers
> 
>  ??? Ben
> 
> 
> On 7/27/20 3:47 PM, John Fox wrote:
>> Dear April,
>>
>> Unfortunately, there's no hatvalues() method for "glmmTMB" objects, 
>> nor do functions in the car package, where the influenceIndexPlot() 
>> function that you tried to use originates, have "glmmTMB" methods. car 
>> does have methods for the "merMod" objects produced by functions in 
>> the lme4 package and for the "lme" objects produced by the lme() 
>> function in the nlme package, but those probably aren't useful to you.
>>
>> I'm willing to look into providing the necessary infrastructure in the 
>> car package to support glmmTMB() but can't promise to do that before 
>> the Fall.
>>
>> Best,
>> ?John
>>
>> --------------------------------------------
>> ? John Fox
>> ? Professor Emeritus
>> ? McMaster University
>> ? Hamilton, Ontario, Canada
>> ? web: https://socialsciences.mcmaster.ca/jfox/
>>
>> On 7/27/2020 1:40 PM, April Martinig wrote:
>>> Hello,
>>>
>>> I've posted my question to Cross Validated, but I am also linking to 
>>> it here 
>>> <https://stats.stackexchange.com/questions/478727/leverage-diagnostic-test-not-supported-for-glmmtmb-models-in-r> 
>>> because it might have been missed.
>>>
>>> I am trying to use the DHARMa package to test for how much leverage 
>>> specific points (on one extreme) for my response variable has on my 
>>> output.
>>>
>>> In my question I have posted the data, what I have tried, and linked 
>>> to other sources. Thank you in advance.
>>>
>>> Take care,
>>> April
>>>
>>> ---
>>> April Martinig, M.Sc.
>>> Ph.D. Candidate in Ecology
>>> University of Alberta
>>> martinig.weebly.com
>>> ?I do not accept the conventional order of things as a given."
>>>
>>> https://stats.stackexchange.com/questions/478727/leverage-diagnostic-test-not-supported-for-glmmtmb-models-in-r 
>>> <https://stats.stackexchange.com/questions/478727/leverage-diagnostic-test-not-supported-for-glmmtmb-models-in-r>. 
>>>
>>> ????[[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From n@429 @end|ng |rom exeter@@c@uk  Wed Jul 29 01:59:16 2020
From: n@429 @end|ng |rom exeter@@c@uk (Sainsbury, Nigel)
Date: Tue, 28 Jul 2020 23:59:16 +0000
Subject: [R-sig-ME] glmmTMB dispformula
Message-ID: <CWXP265MB1302CFB3BB7A9129C253AB87DF730@CWXP265MB1302.GBRP265.PROD.OUTLOOK.COM>

Dear List,

I would greatly appreciate any guidance the community can provide relating to the way the dispformula argument works in glmmTMB.

Context:

I am running a Gaussian mixed model in glmmTMB to account for heteroscedasticity of residuals caused by two fixed effects (one continuous, one categorical). My model code therefore includes the argument, dispformula = ~ X1 + X2, where X1 and X2 are the two fixed effects in question (as per Brooks at al., 2007).

Problem:


1.      Despite searching glmmTMB package documentation and articles, email lists and Stack Overflow, I have been unable to discover how the dispformula argument manages heteroscedasticity in the model i.e. how it connects the covariates (X1 and X2 in this case) to the residual variance. For fixed effect only linear models, Zuur et al. (2009) Chapter 4 provides an excellent explanation of how the gls function in nlme can use different variance structures to control for heteroscedasticity (e.g.  VarFixed (Fixed variance), VarIdent (Different variances per stratum), VarPower (Power of the variance covariate) etc.). Can anyone explain this for me?



2.      If the specification of dispformula does explain the heteroscedasticity in the fixed effects, should the plots of model residuals against fitted values and X1 and X2 appear homoscedastic or remain heteroscedastic as when the model is run using lme4 i.e. the dispformula argument accounts for, but does not remove, the heteroscedasticity?


Thanks in advance for any guidance on this issue.

Best wishes,
Nigel

	[[alternative HTML version deleted]]


From t|m@co|e @end|ng |rom uc|@@c@uk  Wed Jul 29 19:01:35 2020
From: t|m@co|e @end|ng |rom uc|@@c@uk (Cole, Tim)
Date: Wed, 29 Jul 2020 17:01:35 +0000
Subject: [R-sig-ME] nlme bug ?
Message-ID: <50076891-C35E-49B9-AC63-19A0561F84C0@ucl.ac.uk>

My sitar package fits the Lindstrom-Beath growth curve model, which varies the slope of the fitted spline curve y by scaling x.

The model generally works well, but it fails in the simplest case, with a 1 df spline curve corresponding to a linear regression. The noddy example below fits a random intercept a and scaling factor c, with regression slope s1. The fitnlme function is called 20 times and the values hardly change, and then it produces nonsense numbers and fails.

Might this be a bug? It works fine with 2+ df for the spline curve.

Thanks for your thoughts.

Tim Cole

``` r
# install.packages('sitar')
# also uses splines and nlme
suppressMessages(library(dplyr))

dat <- sitar::heights %>%
  select(x = 'age', y = 'height', id = 'id') %>%
  mutate(x = scale(x, TRUE, FALSE))

start <- lm(y ~ splines::ns(x), data = dat)$coef
(start <- c(s1 = start[[2]], a = start[[1]], c = 0))
#>        s1         a         c
#>  56.82702 128.76694   0.00000

fitnlme <- function(x, s1, a, c) {
  print(unique(s1))
  print(unique(a))
  print(unique(c))
  a + drop(s1 * splines::ns(x * exp(c)))
}

nlme::nlme(y ~ fitnlme(x, s1, a, c), fixed = s1 + a + c ~ 1, random = a + c ~ 1 | id, start = start, data = dat)
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 1.490116e-08
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 1.490116e-08
#> Warning in nlme.formula(y ~ fitnlme(x, s1, a, c), fixed = s1 + a + c ~ 1, :
#> Iteration 1, LME step: nlminb() did not converge (code = 1). PORT message: false
#> convergence (8)
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 1.490116e-08
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 1.490116e-08
#> [1] 48.22096
#>  [1] 134.7411 130.4434 125.1878 134.2582 139.3737 132.7397 120.4782 128.5690
#>  [9] 137.1287 125.7633 136.5816 131.8558
#>  [1]  168940.88  664404.25 1336620.21  242694.18 -481809.06  256605.51
#>  [7] 1934638.21  901713.94 -251016.54 1287590.98  -99830.76  482104.58
#> Error in qr.default(t(const)): NA/NaN/Inf in foreign function call (arg 1)
```

<sup>Created on 2020-07-29 by the [reprex package](https://reprex.tidyverse.org) (v0.3.0)</sup>

--
Population Policy and Practice
UCL Great Ormond Street Institute of Child Health,
30 Guilford Street, London WC1N 1EH, UK


	[[alternative HTML version deleted]]


From v|n|c|u@@@@m@|@77 @end|ng |rom gm@||@com  Fri Jul 31 06:34:52 2020
From: v|n|c|u@@@@m@|@77 @end|ng |rom gm@||@com (Vinicius Maia)
Date: Fri, 31 Jul 2020 01:34:52 -0300
Subject: [R-sig-ME] SAC tests in mixed models
Message-ID: <CAMXcYmbbpgNQaLhdPNLWirhA7TyyCBYWnojhDUoL-pYJOXE7_g@mail.gmail.com>

Dear list,

As you may know, the inferences of Moran's I test on model's residuals
change when the model matrix or (X'X)^{-1} are taken into account. It
occurs because regression assumptions affect how we view the regression
residuals.

For example, Moran's I test value of spdep::lm.morantest (which uses the
model matrix) and spdep::moran.test are the same, however, their p-values
are different because the former takes into account the model matrix and
the latter doesn't.

As far as I know ncf::correlog and DHARMa::testSpatialAutocorrelation also
does not account for the model matrix. But I may be wrong about DHARMa.

In general, I think the results of moran.test, correlog and
testSpatialAutocorrelation are reliable for mixed models.

However, I have not found a test like spdep::lm.morantest for mixed models.

Do you also think the results of moran.test, correlog,
testSpatialAutocorrelation are reliable for mixed models? Does anyone know
a SAC test for mixed models which accounts for the models matrix, such as
lm.morantest?

Thanks in advance for any guidance on this topic.

Best,

Vinicius


 <https://rdrr.io/cran/DHARMa/man/testSpatialAutocorrelation.html>

	[[alternative HTML version deleted]]


From b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com  Sat Aug  1 20:09:40 2020
From: b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com (Alessandra Bielli)
Date: Sat, 1 Aug 2020 12:09:40 -0600
Subject: [R-sig-ME] GLMM hurdle model for continuous data -Truncated
 negative binomial family in glmmTMB?
Message-ID: <CA+6N3yU16LWsOS8L96ng8rZT1sWYLZyNrB2XzW3a7xBi3h5vMQ@mail.gmail.com>

Dear List

I have posted the question below on Cross Validated as well, where you can
find the image of the diagnostics plots.
https://stats.stackexchange.com/questions/481109/truncated-negative-binomial-hurdle-model-for-continuous-data-glmmtmb



I am running a hurdle model using the glmmTMB function. My dependent
variable is continuous and >= 0. I was looking for a function that would
allow me to model the binary response in a logistic regression (i.e the
zero inflated model) and the non-zero response in a Gamma (log) regression
(i.e the conditional model). However, the glmmTMB function does not allow
to use the gamma family.

I have two questions:

   1.

   is there a function that allows to run a GLMM with gamma family? I tried
   to write my own code to have these two models but I am not sure about how
   to predict and calculate CIs.
   2.

   Is it appropriate to use a truncated negative binomial family for the
   conditional (non-zero values) model? I ran the model:

hpm_nb <- glmmTMB(percapita_dia ~ Tipo_residuo + (1|Trip_ID),
zi=~Tipo_residuo, all, family=truncated_nbinom2)

The diagnostics plots look good and model predictions are reasonable. Is it
still wrong to have a truncated negative binomial family?

Thanks for your help,

Alessandra

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Sun Aug  2 01:05:52 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sat, 1 Aug 2020 19:05:52 -0400
Subject: [R-sig-ME] GLMM hurdle model for continuous data -Truncated
 negative binomial family in glmmTMB?
In-Reply-To: <CA+6N3yU16LWsOS8L96ng8rZT1sWYLZyNrB2XzW3a7xBi3h5vMQ@mail.gmail.com>
References: <CA+6N3yU16LWsOS8L96ng8rZT1sWYLZyNrB2XzW3a7xBi3h5vMQ@mail.gmail.com>
Message-ID: <bfe3b523-6cb5-5fae-7130-7c9e3418bb20@gmail.com>

 ? Responded on CV.? Short answer: glmmTMB does have a 'ziGamma' family 
that should allow a zero-hurdle Gamma model ...

On 8/1/20 2:09 PM, Alessandra Bielli wrote:
> Dear List
>
> I have posted the question below on Cross Validated as well, where you can
> find the image of the diagnostics plots.
> https://stats.stackexchange.com/questions/481109/truncated-negative-binomial-hurdle-model-for-continuous-data-glmmtmb
>
>
>
> I am running a hurdle model using the glmmTMB function. My dependent
> variable is continuous and >= 0. I was looking for a function that would
> allow me to model the binary response in a logistic regression (i.e the
> zero inflated model) and the non-zero response in a Gamma (log) regression
> (i.e the conditional model). However, the glmmTMB function does not allow
> to use the gamma family.
>
> I have two questions:
>
>     1.
>
>     is there a function that allows to run a GLMM with gamma family? I tried
>     to write my own code to have these two models but I am not sure about how
>     to predict and calculate CIs.
>     2.
>
>     Is it appropriate to use a truncated negative binomial family for the
>     conditional (non-zero values) model? I ran the model:
>
> hpm_nb <- glmmTMB(percapita_dia ~ Tipo_residuo + (1|Trip_ID),
> zi=~Tipo_residuo, all, family=truncated_nbinom2)
>
> The diagnostics plots look good and model predictions are reasonable. Is it
> still wrong to have a truncated negative binomial family?
>
> Thanks for your help,
>
> Alessandra
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From o||verhooker @end|ng |rom pr@t@t|@t|c@@com  Mon Aug  3 21:23:47 2020
From: o||verhooker @end|ng |rom pr@t@t|@t|c@@com (Oliver Hooker)
Date: Mon, 3 Aug 2020 20:23:47 +0100
Subject: [R-sig-ME] =?utf-8?q?ONLINE_COURSE_=E2=80=93_Introduction_to_mix?=
	=?utf-8?q?ed_models_using_R_and_Rstudio_=28IMMR02=29?=
Message-ID: <CAEsSYzyrnq6Fwp9Sk9G-9z78DDsfJgRuoNn9c9-CGkUA3_jeLA@mail.gmail.com>

ONLINE COURSE ? Introduction to mixed models using R and Rstudio
(IMMR02) This course will be delivered live

https://www.psstatistics.com/course/introduction-to-mixed-models-using-r-and-rstudio-immr02/

6 August 2020 - 7 August 2020

TIME ZONE ? Western European Time +1 ? however all sessions will be
recorded and made available allowing attendees from different time
zones to follow a day behind with an additional 1/2 days support after
the official course finish date (please email
oliverhooker at psstatistics.com for full details or to discuss how we
can accommodate you).

Course Overview:

In this two day course, we provide a comprehensive practical and
theoretical introduction to multilevel models, also known as
hierarchical or mixed effects models. We will focus primarily on
multilevel linear models, but also cover multilevel generalized linear
models. Likewise, we will also describe Bayesian approaches to
multilevel modelling. On Day 1, we will begin by focusing on random
effects multilevel models. These models make it clear how multilevel
models are in fact models of models. In addition, random effects
models serve as a solid basis for understanding mixed effects, i.e.
fixed and random effects, models. In this coverage of random effects,
we will also cover the important concepts of statistical shrinkage in
the estimation of effects, as well as intraclass correlation. We then
proceed to cover linear mixed effects models, particularly focusing on
varying intercept and/or varying slopes regresssion models. On Day 2,
we cover further aspects of linear mixed effects models, including
multilevel models for nested and crossed data data, and group level
predictor variables. On Day 2, we also cover Bayesian approaches to
multilevel levels using the brms R package.

Please email oliverhooker at psstatistics.com with any questions



-- 
Oliver Hooker PhD.
PR statistics

2020 publications;
Parallelism in eco-morphology and gene expression despite variable
evolutionary and genomic backgrounds in a Holarctic fish. PLOS
GENETICS (2020). IN PRESS

www.PRstatistics.com
facebook.com/PRstatistics/
twitter.com/PRstatistics

53 Morrison Street
Glasgow
G5 8LB
+44 (0) 7966500340
+44 (0) 7966500340


From ph||||p@@|d@y @end|ng |rom mp|@n|  Mon Aug 10 17:44:34 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Mon, 10 Aug 2020 17:44:34 +0200
Subject: [R-sig-ME] nlme bug ?
In-Reply-To: <50076891-C35E-49B9-AC63-19A0561F84C0@ucl.ac.uk>
References: <50076891-C35E-49B9-AC63-19A0561F84C0@ucl.ac.uk>
Message-ID: <e813a061-85ee-44d9-cf4a-2b8600be3ae5@mpi.nl>

This could be a bug, but I would check two related things first:

1. Have you tried plotting a profile plot of the likelihood?

2. Have you tried? different starting values?

In my limited experience, these types of models can be surprisingly
sensitive to starting values.

Phillip

On 29/7/20 7:01 pm, Cole, Tim wrote:
> My sitar package fits the Lindstrom-Beath growth curve model, which varies the slope of the fitted spline curve y by scaling x.
>
> The model generally works well, but it fails in the simplest case, with a 1 df spline curve corresponding to a linear regression. The noddy example below fits a random intercept a and scaling factor c, with regression slope s1. The fitnlme function is called 20 times and the values hardly change, and then it produces nonsense numbers and fails.
>
> Might this be a bug? It works fine with 2+ df for the spline curve.
>
> Thanks for your thoughts.
>
> Tim Cole
>
> ``` r
> # install.packages('sitar')
> # also uses splines and nlme
> suppressMessages(library(dplyr))
>
> dat <- sitar::heights %>%
>   select(x = 'age', y = 'height', id = 'id') %>%
>   mutate(x = scale(x, TRUE, FALSE))
>
> start <- lm(y ~ splines::ns(x), data = dat)$coef
> (start <- c(s1 = start[[2]], a = start[[1]], c = 0))
> #>        s1         a         c
> #>  56.82702 128.76694   0.00000
>
> fitnlme <- function(x, s1, a, c) {
>   print(unique(s1))
>   print(unique(a))
>   print(unique(c))
>   a + drop(s1 * splines::ns(x * exp(c)))
> }
>
> nlme::nlme(y ~ fitnlme(x, s1, a, c), fixed = s1 + a + c ~ 1, random = a + c ~ 1 | id, start = start, data = dat)
> #> [1] 56.82702
> #> [1] 128.7669
> #> [1] 0
> #> [1] 56.82702
> #> [1] 128.7669
> #> [1] 0
> #> [1] 56.82702
> #> [1] 128.7669
> #> [1] 0
> #> [1] 56.82702
> #> [1] 128.7669
> #> [1] 0
> #> [1] 56.82702
> #> [1] 128.7669
> #> [1] 1.490116e-08
> #> [1] 56.82702
> #> [1] 128.7669
> #> [1] 0
> #> [1] 56.82702
> #> [1] 128.7669
> #> [1] 0
> #> [1] 56.82702
> #> [1] 128.7669
> #> [1] 0
> #> [1] 56.82702
> #> [1] 128.7669
> #> [1] 0
> #> [1] 56.82702
> #> [1] 128.7669
> #> [1] 1.490116e-08
> #> Warning in nlme.formula(y ~ fitnlme(x, s1, a, c), fixed = s1 + a + c ~ 1, :
> #> Iteration 1, LME step: nlminb() did not converge (code = 1). PORT message: false
> #> convergence (8)
> #> [1] 56.82702
> #> [1] 128.7669
> #> [1] 0
> #> [1] 56.82702
> #> [1] 128.7669
> #> [1] 0
> #> [1] 56.82702
> #> [1] 128.7669
> #> [1] 0
> #> [1] 56.82702
> #> [1] 128.7669
> #> [1] 0
> #> [1] 56.82702
> #> [1] 128.7669
> #> [1] 1.490116e-08
> #> [1] 56.82702
> #> [1] 128.7669
> #> [1] 0
> #> [1] 56.82702
> #> [1] 128.7669
> #> [1] 0
> #> [1] 56.82702
> #> [1] 128.7669
> #> [1] 0
> #> [1] 56.82702
> #> [1] 128.7669
> #> [1] 0
> #> [1] 56.82702
> #> [1] 128.7669
> #> [1] 1.490116e-08
> #> [1] 48.22096
> #>  [1] 134.7411 130.4434 125.1878 134.2582 139.3737 132.7397 120.4782 128.5690
> #>  [9] 137.1287 125.7633 136.5816 131.8558
> #>  [1]  168940.88  664404.25 1336620.21  242694.18 -481809.06  256605.51
> #>  [7] 1934638.21  901713.94 -251016.54 1287590.98  -99830.76  482104.58
> #> Error in qr.default(t(const)): NA/NaN/Inf in foreign function call (arg 1)
> ```
>
> <sup>Created on 2020-07-29 by the [reprex package](https://reprex.tidyverse.org) (v0.3.0)</sup>
>
> --
> Population Policy and Practice
> UCL Great Ormond Street Institute of Child Health,
> 30 Guilford Street, London WC1N 1EH, UK
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From ph||||p@@|d@y @end|ng |rom mp|@n|  Tue Aug 11 20:58:25 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Tue, 11 Aug 2020 20:58:25 +0200
Subject: [R-sig-ME] nlme bug ?
In-Reply-To: <5B776195-64E4-4208-8ABB-EF6223A065D9@ucl.ac.uk>
References: <50076891-C35E-49B9-AC63-19A0561F84C0@ucl.ac.uk>
 <e813a061-85ee-44d9-cf4a-2b8600be3ae5@mpi.nl>
 <5B776195-64E4-4208-8ABB-EF6223A065D9@ucl.ac.uk>
Message-ID: <0d48ebbc-0b21-6c64-44e7-483f7b89f7d6@mpi.nl>

For the lme4 model fit with REML, you can get something comparable to
wait nlme calles the log-likelihood by dividing the REML criterion by
-2. (There have been a number of exchanges on this list, even in the
last year, about the problems of REML vs. _the_ likelihood, so I won't
go there, but just note that I can feel the Mathematical Powers That Be
looking disappointingly over my shoulder while I brush these problems
aside.)

On my machine that gives me:

nlme: -315.0308

lme: -313.6868

lmer: -313.6868

These are all quite similar and the corresponding coefficient values are
quite similar, so any differences are related to rounding issues and
machine-level variation in the nonlinear optimizer (which doesn't refer
to nlme here, but rather all of these tools actually fit the model).? (I
don't fully understand all the implementation details of the optimizers
themselves, but I have seen them take a different number of iterations
to fit the same model on different machines with ostensibly identical
software versions.) The slightly better fit returned by the linear
methods on my machine should generally be preferred -- but if your
inferences are sensitive to such small differences, you probably have
bigger problems. ;)

In other words, I wouldn't worry.


Phillip


On 11/8/20 3:35 pm, Cole, Tim wrote:
>
> Thanks Phillip for your thoughts.
>
> ?
>
> Thinking about it some more, I realise there is a redundancy which
> explains why it fails. The construct
>
> ?
>
> a + drop(s1 * splines::ns(x * exp(c)))
>
> ?
>
> involves three parameters a, s1 and c, whereas it is a linear model
> with intercept and slope and ought to have just two, i.e.
>
> ?
>
> a + x * b.
>
> ?
>
> This leads to another curiosity. This simpler model fits fine in nlme,
> but unexpectedly it is not identical to the equivalent model fitted in
> lme or lme4 ? see below. The mean relative difference between fitted
> values is 1000 times greater for nlme vs lme than for lme vs lme4.
>
> ?
>
> Is this simply a rounding issue, or are the nlme and lme models
> genuinely different, and if so, in what way?
>
> ?
>
> Thanks,
>
> Tim
>
> ?
>
> ``` r
>
> # install.packages('sitar')
>
> suppressMessages({library(dplyr)
>
> ? library(sitar)
>
> ? library(lme4)
>
> })
>
> ?
>
> dat <- heights %>%
>
> ? mutate(x = scale(age, scale = FALSE)) %>%
>
> ? select(x, y = height, id)
>
> ?
>
> start <- setNames(lm(y ~ x, data = dat)$coef, c('a', 'b'))
>
> ?
>
> fitnlme <- function(x, a, b) {a + x * b}
>
> ?
>
> nlme1 <- nlme(y ~ fitnlme(x, a, b), fixed = a + b ~ 1, random = a + b
> ~ 1 | id, start = start, data = dat)
>
> ?
>
> lme1 <- lme(y ~ x, random = ~ x | id, data = dat)
>
> lmer1 <- lmer(y ~ x + (x | id), data = dat)
>
> ?
>
> all.equal(fitted(nlme1), fitted(lme1), check.attributes = FALSE)
>
> #> [1] "Mean relative difference: 9.603422e-05"
>
> all.equal(fitted(lme1), fitted(lmer1), check.attributes = FALSE)
>
> #> [1] "Mean relative difference: 4.880377e-08"
>
> ```
>
> ?
>
> <sup>Created on 2020-08-11 by the [reprex
> package](https://reprex.tidyverse.org) (v0.3.0)</sup>
>
> --
>
> Population Policy and Practice
>
> UCL Great Ormond Street Institute of Child Health,
>
> 30 Guilford Street, London WC1N 1EH, UK
>
> ?
>
> ?
>
> *From: *Phillip Alday <phillip.alday at mpi.nl>
> *Date: *Monday, 10 August 2020 at 16:44
> *To: *"Cole, Tim" <tim.cole at ucl.ac.uk>,
> "r-sig-mixed-models at r-project.org" <r-sig-mixed-models at r-project.org>
> *Subject: *Re: [R-sig-ME] nlme bug ?
>
> ?
>
> This could be a bug, but I would check two related things first:
>
> ?
>
> 1. Have you tried plotting a profile plot of the likelihood?
>
> ?
>
> 2. Have you tried? different starting values?
>
> ?
>
> In my limited experience, these types of models can be surprisingly
>
> sensitive to starting values.
>
> ?
>
> Phillip
>
> ?
>
> On 29/7/20 7:01 pm, Cole, Tim wrote:
>
>     My sitar package fits the Lindstrom-Beath growth curve model,
>     which varies the slope of the fitted spline curve y by scaling x.
>
>     ?
>
>     The model generally works well, but it fails in the simplest case,
>     with a 1 df spline curve corresponding to a linear regression. The
>     noddy example below fits a random intercept a and scaling factor
>     c, with regression slope s1. The fitnlme function is called 20
>     times and the values hardly change, and then it produces nonsense
>     numbers and fails.
>
>     ?
>
>     Might this be a bug? It works fine with 2+ df for the spline curve.
>
>     ?
>
>     Thanks for your thoughts.
>
>     ?
>
>     Tim Cole
>
>     ?
>
>     ``` r
>
>     # install.packages('sitar')
>
>     # also uses splines and nlme
>
>     suppressMessages(library(dplyr))
>
>     ?
>
>     dat <- sitar::heights %>%
>
>     ?? select(x = 'age', y = 'height', id = 'id') %>%
>
>     ?? mutate(x = scale(x, TRUE, FALSE))
>
>     ?
>
>     start <- lm(y ~ splines::ns(x), data = dat)$coef
>
>     (start <- c(s1 = start[[2]], a = start[[1]], c = 0))
>
>     #>????????s1???????? a???????? c
>
>     #>??56.82702 128.76694?? 0.00000
>
>     ?
>
>     fitnlme <- function(x, s1, a, c) {
>
>     ?? print(unique(s1))
>
>     ?? print(unique(a))
>
>     ?? print(unique(c))
>
>     ?? a + drop(s1 * splines::ns(x * exp(c)))
>
>     }
>
>     ?
>
>     nlme::nlme(y ~ fitnlme(x, s1, a, c), fixed = s1 + a + c ~ 1,
>     random = a + c ~ 1 | id, start = start, data = dat)
>
>     #> [1] 56.82702
>
>     #> [1] 128.7669
>
>     #> [1] 0
>
>     #> [1] 56.82702
>
>     #> [1] 128.7669
>
>     #> [1] 0
>
>     #> [1] 56.82702
>
>     #> [1] 128.7669
>
>     #> [1] 0
>
>     #> [1] 56.82702
>
>     #> [1] 128.7669
>
>     #> [1] 0
>
>     #> [1] 56.82702
>
>     #> [1] 128.7669
>
>     #> [1] 1.490116e-08
>
>     #> [1] 56.82702
>
>     #> [1] 128.7669
>
>     #> [1] 0
>
>     #> [1] 56.82702
>
>     #> [1] 128.7669
>
>     #> [1] 0
>
>     #> [1] 56.82702
>
>     #> [1] 128.7669
>
>     #> [1] 0
>
>     #> [1] 56.82702
>
>     #> [1] 128.7669
>
>     #> [1] 0
>
>     #> [1] 56.82702
>
>     #> [1] 128.7669
>
>     #> [1] 1.490116e-08
>
>     #> Warning in nlme.formula(y ~ fitnlme(x, s1, a, c), fixed = s1 +
>     a + c ~ 1, :
>
>     #> Iteration 1, LME step: nlminb() did not converge (code = 1).
>     PORT message: false
>
>     #> convergence (8)
>
>     #> [1] 56.82702
>
>     #> [1] 128.7669
>
>     #> [1] 0
>
>     #> [1] 56.82702
>
>     #> [1] 128.7669
>
>     #> [1] 0
>
>     #> [1] 56.82702
>
>     #> [1] 128.7669
>
>     #> [1] 0
>
>     #> [1] 56.82702
>
>     #> [1] 128.7669
>
>     #> [1] 0
>
>     #> [1] 56.82702
>
>     #> [1] 128.7669
>
>     #> [1] 1.490116e-08
>
>     #> [1] 56.82702
>
>     #> [1] 128.7669
>
>     #> [1] 0
>
>     #> [1] 56.82702
>
>     #> [1] 128.7669
>
>     #> [1] 0
>
>     #> [1] 56.82702
>
>     #> [1] 128.7669
>
>     #> [1] 0
>
>     #> [1] 56.82702
>
>     #> [1] 128.7669
>
>     #> [1] 0
>
>     #> [1] 56.82702
>
>     #> [1] 128.7669
>
>     #> [1] 1.490116e-08
>
>     #> [1] 48.22096
>
>     #>??[1] 134.7411 130.4434 125.1878 134.2582 139.3737 132.7397
>     120.4782 128.5690
>
>     #>??[9] 137.1287 125.7633 136.5816 131.8558
>
>     #>??[1]??168940.88??664404.25 1336620.21??242694.18
>     -481809.06??256605.51
>
>     #>??[7] 1934638.21??901713.94 -251016.54
>     1287590.98??-99830.76??482104.58
>
>     #> Error in qr.default(t(const)): NA/NaN/Inf in foreign function
>     call (arg 1)
>
>     ```
>
>     ?
>
>     <sup>Created on 2020-07-29 by the [reprex
>     package](https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Freprex.tidyverse.org%2F&amp;data=02%7C01%7C%7Ca6498670eae541e3863a08d83d444fe4%7C1faf88fea9984c5b93c9210a11d9a5c2%7C0%7C1%7C637326710909416681&amp;sdata=9iBQs56VTomb9EAJzND9ku7%2BSgZoohwwdJWxKQ%2Bi0Pc%3D&amp;reserved=0)
>     (v0.3.0)</sup>
>
>     ?
>
>     --
>
>     Population Policy and Practice
>
>     UCL Great Ormond Street Institute of Child Health,
>
>     30 Guilford Street, London WC1N 1EH, UK
>
>     ?
>
>     ?
>
>     ?????????? [[alternative HTML version deleted]]
>
>     ?
>
>     _______________________________________________
>
>     R-sig-mixed-models at r-project.org
>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>
>     https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-mixed-models&amp;data=02%7C01%7C%7Ca6498670eae541e3863a08d83d444fe4%7C1faf88fea9984c5b93c9210a11d9a5c2%7C0%7C1%7C637326710909416681&amp;sdata=te3VRPkQ3lk8Db6%2Frsdsl1py6dbbjWA2LIO2neTsWgc%3D&amp;reserved=0
>
> ?
>

	[[alternative HTML version deleted]]


From rb@rr@nt @end|ng |rom uvm@edu  Wed Aug 12 16:56:04 2020
From: rb@rr@nt @end|ng |rom uvm@edu (Ramiro Barrantes)
Date: Wed, 12 Aug 2020 10:56:04 -0400
Subject: [R-sig-ME] Huge speed performance difference when using non-trivial
 fixed effects in NLMER vs NLME
Message-ID: <b81bd9a4-f78b-45ef-7cc8-f0a4e53f8bec@uvm.edu>

Following Ben Bolker's methodology (described here 
https://rpubs.com/bbolker/3423) I incorporated non-trivial fixed effects 
in NLMER for a four-parameter logistic. ? I placed a reproducible 
example here: https://rpubs.com/ramirob/648103

To summarize the question, if we have a dataset with individuals in 
groups where we have group-specific fixed effects, NLME's performance 
remains the same:

## [1] "NLME Time Required for data2Groups: 0.0458040237426758"

fit3Groups <- fitNLME(data3Groups,initialValues3Groups)

## [1] "NLME Time Required for data3Groups: 0.0375699996948242"

fit4Groups <- fitNLME(data4Groups,initialValues4Groups)

## [1] "NLME Time Required for data4Groups: 0.0526559352874756"

fit5Groups <- fitNLME(data5Groups,initialValues5Groups)

## [1] "NLME Time Required for data5Groups: 0.0502560138702393"

But when we do the analogous thing in NLMER, the performance increases 
with increasing number of groups:

## [1] "Time required for the data2Groups: 0.404773950576782"

fitNlmer3Groups <- fitNlmer(data3Groups, initialValues3Groups)

## [1] "Time required for the data3Groups: 0.579570055007935"

fitNlmer4Groups <- fitNlmer(data4Groups, initialValues4Groups)

## [1] "Time required for the data4Groups: 0.957509994506836"

fitNlmer5Groups <- fitNlmer(data5Groups, initialValues5Groups)

## [1] "Time required for the data5Groups: 1.68412184715271"

In addition, NLMER is much slower in general.? This is just a short 
example, but for more complicated cases the differences in performance 
are huge (minutes vs seconds).

Is NLMER "worth the wait" (e.g. less fragile, better convergence, etc) 
when trying to do non-trivial fixed effects? Is there a better 
methodology than the one described by Ben Bolker back in 2013?

Any insight appreciated.? Again, you can see a reproducible example here 
https://rpubs.com/ramirob/648103
Thank you!

-- 
Ramiro Barrantes Reynolds, Ph.D.
Bioinformatics Research Associate, Microbiology & Molecular Genetics
Vermont Integrative Genomics Resource
University of Vermont
Burlington, VT 05405
https://www.med.uvm.edu/vigr/bioinformatics


	[[alternative HTML version deleted]]


From @|dot|@23 @end|ng |rom buckeyem@||@o@u@edu  Tue Aug 18 05:32:35 2020
From: @|dot|@23 @end|ng |rom buckeyem@||@o@u@edu (Sidoti, Salvatore A.)
Date: Tue, 18 Aug 2020 03:32:35 +0000
Subject: [R-sig-ME] CLMM: Calculate ICC & Assessing Model Fit
Message-ID: <CY4PR0101MB30797B2FE57EE36D0F734BA1AB5C0@CY4PR0101MB3079.prod.exchangelabs.com>

To begin with, I'm not a fan of cross-posting. However, I posted my question on Stack Exchange more than two weeks ago, but I have yet to receive a sufficient answer:

https://stats.stackexchange.com/questions/479600/data-with-ordinal-responses-calculate-icc-assessing-model-fit
 
Here's what I've learned since then (hopefully):
 
1) ICC of a CLMM:
Computed like this:
(variance of the random effect) / (variance of the random effect + 1) If this is correct, I would love to see a reference/citation for it.
 
2) 95% Confidence Interval for the ICC from a CLMM Model To my current understanding, a confidence interval for an ICC is only obtainable via simulation. I've conducted simulations with GLMM model objects ('lme4' package) and the bootMer() function. Unfortunately, bootMer() will not accept a CLMM model ('ordinal' package).
 
3) Model Fit of a CLMM
Assuming that the model converges without incident, the model summary includes a condition number of the Hessian ('cond.H'). This value should be below 10^4 for a "good fit". This is straightforward enough. However, I am not as sure about the value for 'max.grad', which needs to be "well below 1". The question is, to what magnitude should max.grad < 1 for a decent model fit? My reference is linked below (Christensen, 2019), but it does not elaborate further on this point:
 
https://documentcloud.adobe.com/link/track?uri=urn:aaid:scds:US:b6a61fe2-b851-49ce-b8b1-cd760d290636
 
3) Effect Size of a CLMM
The random variable's effect is determined by a comparison between the full model to a model with only the fixed effects via the anova() function. I found this information on the 'rcompanion' package website:
 
https://rcompanion.org/handbook/G_12.html
 
The output of this particular anova() will include a value named 'LR.stat', the likelihood ratio statistic. The LR.stat is twice the difference of each log-likelihood (absolute value) of the respective models. Is LR.stat the mixed-model version of an "effect size"? If so, how does one determine if the effect is small, large, in-between, etc?

Cheers,
Sal

Salvatore A. Sidoti
PhD Candidate
Behavioral Ecology
The Ohio State University


From juho@kr|@t|@n@ruohonen @end|ng |rom gm@||@com  Wed Aug 19 15:35:00 2020
From: juho@kr|@t|@n@ruohonen @end|ng |rom gm@||@com (Juho Kristian Ruohonen)
Date: Wed, 19 Aug 2020 16:35:00 +0300
Subject: [R-sig-ME] CLMM: Calculate ICC & Assessing Model Fit
In-Reply-To: <CY4PR0101MB30797B2FE57EE36D0F734BA1AB5C0@CY4PR0101MB3079.prod.exchangelabs.com>
References: <CY4PR0101MB30797B2FE57EE36D0F734BA1AB5C0@CY4PR0101MB3079.prod.exchangelabs.com>
Message-ID: <CAG_dBVc_pMU31p-+JTLyu1ZxJ+r_8xt9UvbbKUNqrPhsPOQRSA@mail.gmail.com>

Just voicing two thoughts as a non-statistician:

1. Why do you feel you need a mixed model in the first place? It looks to
me like every Individual is contributing the same number of responses (i.e.
4). Thus, the individual-specific effects can be assumed to cancel each
other out, and no random effect is needed.
2. Why complicate things with a non-canonical link function? It looks to me
like a standard ordinal logistic regression model, with fixed effects only,
would do the job for you. Guides and textbooks abound on how to assess the
fit of such models.

Best,

J

ke 19. elok. 2020 klo 14.44 Sidoti, Salvatore A. (
sidoti.23 at buckeyemail.osu.edu) kirjoitti:

> To begin with, I'm not a fan of cross-posting. However, I posted my
> question on Stack Exchange more than two weeks ago, but I have yet to
> receive a sufficient answer:
>
>
> https://stats.stackexchange.com/questions/479600/data-with-ordinal-responses-calculate-icc-assessing-model-fit
>
> Here's what I've learned since then (hopefully):
>
> 1) ICC of a CLMM:
> Computed like this:
> (variance of the random effect) / (variance of the random effect + 1) If
> this is correct, I would love to see a reference/citation for it.
>
> 2) 95% Confidence Interval for the ICC from a CLMM Model To my current
> understanding, a confidence interval for an ICC is only obtainable via
> simulation. I've conducted simulations with GLMM model objects ('lme4'
> package) and the bootMer() function. Unfortunately, bootMer() will not
> accept a CLMM model ('ordinal' package).
>
> 3) Model Fit of a CLMM
> Assuming that the model converges without incident, the model summary
> includes a condition number of the Hessian ('cond.H'). This value should be
> below 10^4 for a "good fit". This is straightforward enough. However, I am
> not as sure about the value for 'max.grad', which needs to be "well below
> 1". The question is, to what magnitude should max.grad < 1 for a decent
> model fit? My reference is linked below (Christensen, 2019), but it does
> not elaborate further on this point:
>
>
> https://documentcloud.adobe.com/link/track?uri=urn:aaid:scds:US:b6a61fe2-b851-49ce-b8b1-cd760d290636
>
> 3) Effect Size of a CLMM
> The random variable's effect is determined by a comparison between the
> full model to a model with only the fixed effects via the anova() function.
> I found this information on the 'rcompanion' package website:
>
> https://rcompanion.org/handbook/G_12.html
>
> The output of this particular anova() will include a value named
> 'LR.stat', the likelihood ratio statistic. The LR.stat is twice the
> difference of each log-likelihood (absolute value) of the respective
> models. Is LR.stat the mixed-model version of an "effect size"? If so, how
> does one determine if the effect is small, large, in-between, etc?
>
> Cheers,
> Sal
>
> Salvatore A. Sidoti
> PhD Candidate
> Behavioral Ecology
> The Ohio State University
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From @|dot|@23 @end|ng |rom buckeyem@||@o@u@edu  Wed Aug 19 16:09:47 2020
From: @|dot|@23 @end|ng |rom buckeyem@||@o@u@edu (Sidoti, Salvatore A.)
Date: Wed, 19 Aug 2020 14:09:47 +0000
Subject: [R-sig-ME] CLMM: Calculate ICC & Assessing Model Fit
In-Reply-To: <CAG_dBVc_pMU31p-+JTLyu1ZxJ+r_8xt9UvbbKUNqrPhsPOQRSA@mail.gmail.com>
References: <CY4PR0101MB30797B2FE57EE36D0F734BA1AB5C0@CY4PR0101MB3079.prod.exchangelabs.com>,
 <CAG_dBVc_pMU31p-+JTLyu1ZxJ+r_8xt9UvbbKUNqrPhsPOQRSA@mail.gmail.com>
Message-ID: <DM5PR0101MB3082784A259835A88E81E2F6AB5D0@DM5PR0101MB3082.prod.exchangelabs.com>

The example you see on Stack Exchange employs a toy data set which mimics the structure of my real data, but not the distributions. It turns out that the statistical contribution of each animal in my study is quite profound, so a mixed model is essential in my case.

Salvatore Sidoti
PhD Candidate
Department of Evolution, Ecology & Organismal Biology
The Ohio State University
Columbus, OH USA

________________________________
From: Juho Kristian Ruohonen <juho.kristian.ruohonen at gmail.com>
Sent: Wednesday, August 19, 2020, 9:35 AM
To: Sidoti, Salvatore A.
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] CLMM: Calculate ICC & Assessing Model Fit

Just voicing two thoughts as a non-statistician:

1. Why do you feel you need a mixed model in the first place? It looks to me like every Individual is contributing the same number of responses (i.e. 4). Thus, the individual-specific effects can be assumed to cancel each other out, and no random effect is needed.
2. Why complicate things with a non-canonical link function? It looks to me like a standard ordinal logistic regression model, with fixed effects only, would do the job for you. Guides and textbooks abound on how to assess the fit of such models.

Best,

J

ke 19. elok. 2020 klo 14.44 Sidoti, Salvatore A. (sidoti.23 at buckeyemail.osu.edu<mailto:sidoti.23 at buckeyemail.osu.edu>) kirjoitti:
To begin with, I'm not a fan of cross-posting. However, I posted my question on Stack Exchange more than two weeks ago, but I have yet to receive a sufficient answer:

https://stats.stackexchange.com/questions/479600/data-with-ordinal-responses-calculate-icc-assessing-model-fit<https://urldefense.com/v3/__https://stats.stackexchange.com/questions/479600/data-with-ordinal-responses-calculate-icc-assessing-model-fit__;!!KGKeukY!nOrbsi1ZjRJ1NgSNVsN8cyWBHD9f41iFHEvxXTqax6B_qnbhSquQQbwMHrEvhqWEvwR6iZNEz0M$>

Here's what I've learned since then (hopefully):

1) ICC of a CLMM:
Computed like this:
(variance of the random effect) / (variance of the random effect + 1) If this is correct, I would love to see a reference/citation for it.

2) 95% Confidence Interval for the ICC from a CLMM Model To my current understanding, a confidence interval for an ICC is only obtainable via simulation. I've conducted simulations with GLMM model objects ('lme4' package) and the bootMer() function. Unfortunately, bootMer() will not accept a CLMM model ('ordinal' package).

3) Model Fit of a CLMM
Assuming that the model converges without incident, the model summary includes a condition number of the Hessian ('cond.H'). This value should be below 10^4 for a "good fit". This is straightforward enough. However, I am not as sure about the value for 'max.grad', which needs to be "well below 1". The question is, to what magnitude should max.grad < 1 for a decent model fit? My reference is linked below (Christensen, 2019), but it does not elaborate further on this point:

https://documentcloud.adobe.com/link/track?uri=urn:aaid:scds:US:b6a61fe2-b851-49ce-b8b1-cd760d290636<https://urldefense.com/v3/__https://documentcloud.adobe.com/link/track?uri=urn:aaid:scds:US:b6a61fe2-b851-49ce-b8b1-cd760d290636__;!!KGKeukY!nOrbsi1ZjRJ1NgSNVsN8cyWBHD9f41iFHEvxXTqax6B_qnbhSquQQbwMHrEvhqWEvwR65za2Jag$>

3) Effect Size of a CLMM
The random variable's effect is determined by a comparison between the full model to a model with only the fixed effects via the anova() function. I found this information on the 'rcompanion' package website:

https://rcompanion.org/handbook/G_12.html<https://urldefense.com/v3/__https://rcompanion.org/handbook/G_12.html__;!!KGKeukY!nOrbsi1ZjRJ1NgSNVsN8cyWBHD9f41iFHEvxXTqax6B_qnbhSquQQbwMHrEvhqWEvwR6uPeTzGc$>

The output of this particular anova() will include a value named 'LR.stat', the likelihood ratio statistic. The LR.stat is twice the difference of each log-likelihood (absolute value) of the respective models. Is LR.stat the mixed-model version of an "effect size"? If so, how does one determine if the effect is small, large, in-between, etc?

Cheers,
Sal

Salvatore A. Sidoti
PhD Candidate
Behavioral Ecology
The Ohio State University

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models<https://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models__;!!KGKeukY!nOrbsi1ZjRJ1NgSNVsN8cyWBHD9f41iFHEvxXTqax6B_qnbhSquQQbwMHrEvhqWEvwR6KhTBL3M$>


	[[alternative HTML version deleted]]


From juho@kr|@t|@n@ruohonen @end|ng |rom gm@||@com  Wed Aug 19 17:53:35 2020
From: juho@kr|@t|@n@ruohonen @end|ng |rom gm@||@com (Juho Kristian Ruohonen)
Date: Wed, 19 Aug 2020 18:53:35 +0300
Subject: [R-sig-ME] CLMM: Calculate ICC & Assessing Model Fit
In-Reply-To: <DM5PR0101MB3082784A259835A88E81E2F6AB5D0@DM5PR0101MB3082.prod.exchangelabs.com>
References: <CY4PR0101MB30797B2FE57EE36D0F734BA1AB5C0@CY4PR0101MB3079.prod.exchangelabs.com>
 <CAG_dBVc_pMU31p-+JTLyu1ZxJ+r_8xt9UvbbKUNqrPhsPOQRSA@mail.gmail.com>
 <DM5PR0101MB3082784A259835A88E81E2F6AB5D0@DM5PR0101MB3082.prod.exchangelabs.com>
Message-ID: <CAG_dBVfxbRzs3tw_oHjdnbMo=Q3Yjc4dGvMZYCtHZfvNHrspgw@mail.gmail.com>

And does the number of observations per animal vary? And if so, how many
observations are there per animal? If that number is sufficiently large, a
standard GLM with a fixed effect for animal ID might be enough.

As for the LRT statistic for the random effect, no it is not an effect
size. AFAIK, "effect size" is rarely talked about with random effects. What
I would view as the closest thing to the "effect size" of a random effect
is simply the random-effect SD, which reflects how much an "average"
individual/animal/cluster is estimated to deviate from the overall mean.
The ICC depends on this quantity, so the ICC could perhaps also be viewed
as a kind of "random effect size". But I couldn't tell you how the ICC is
calculated because that depends on the specifics of the cauchit link
function, with which I am not familiar. There are some link functions (such
as the probit) for which it is indeed as simple as SD^2 / (SD^2 + 1).

Best,

J



ke 19. elok. 2020 klo 17.09 Sidoti, Salvatore A. (
sidoti.23 at buckeyemail.osu.edu) kirjoitti:

> The example you see on Stack Exchange employs a toy data set which mimics
> the structure of my real data, but not the distributions. It turns out that
> the statistical contribution of each animal in my study is quite profound,
> so a mixed model is essential in my case.
>
> Salvatore Sidoti
> PhD Candidate
> Department of Evolution, Ecology & Organismal Biology
> The Ohio State University
> Columbus, OH USA
>
> ------------------------------
> *From:* Juho Kristian Ruohonen <juho.kristian.ruohonen at gmail.com>
> *Sent:* Wednesday, August 19, 2020, 9:35 AM
> *To:* Sidoti, Salvatore A.
> *Cc:* r-sig-mixed-models at r-project.org
> *Subject:* Re: [R-sig-ME] CLMM: Calculate ICC & Assessing Model Fit
>
> Just voicing two thoughts as a non-statistician:
>
> 1. Why do you feel you need a mixed model in the first place? It looks to
> me like every Individual is contributing the same number of responses (i.e.
> 4). Thus, the individual-specific effects can be assumed to cancel each
> other out, and no random effect is needed.
> 2. Why complicate things with a non-canonical link function? It looks to
> me like a standard ordinal logistic regression model, with fixed effects
> only, would do the job for you. Guides and textbooks abound on how to
> assess the fit of such models.
>
> Best,
>
> J
>
> ke 19. elok. 2020 klo 14.44 Sidoti, Salvatore A. (
> sidoti.23 at buckeyemail.osu.edu) kirjoitti:
>
>> To begin with, I'm not a fan of cross-posting. However, I posted my
>> question on Stack Exchange more than two weeks ago, but I have yet to
>> receive a sufficient answer:
>>
>>
>> https://stats.stackexchange.com/questions/479600/data-with-ordinal-responses-calculate-icc-assessing-model-fit
>> <https://urldefense.com/v3/__https://stats.stackexchange.com/questions/479600/data-with-ordinal-responses-calculate-icc-assessing-model-fit__;!!KGKeukY!nOrbsi1ZjRJ1NgSNVsN8cyWBHD9f41iFHEvxXTqax6B_qnbhSquQQbwMHrEvhqWEvwR6iZNEz0M$>
>>
>> Here's what I've learned since then (hopefully):
>>
>> 1) ICC of a CLMM:
>> Computed like this:
>> (variance of the random effect) / (variance of the random effect + 1) If
>> this is correct, I would love to see a reference/citation for it.
>>
>> 2) 95% Confidence Interval for the ICC from a CLMM Model To my current
>> understanding, a confidence interval for an ICC is only obtainable via
>> simulation. I've conducted simulations with GLMM model objects ('lme4'
>> package) and the bootMer() function. Unfortunately, bootMer() will not
>> accept a CLMM model ('ordinal' package).
>>
>> 3) Model Fit of a CLMM
>> Assuming that the model converges without incident, the model summary
>> includes a condition number of the Hessian ('cond.H'). This value should be
>> below 10^4 for a "good fit". This is straightforward enough. However, I am
>> not as sure about the value for 'max.grad', which needs to be "well below
>> 1". The question is, to what magnitude should max.grad < 1 for a decent
>> model fit? My reference is linked below (Christensen, 2019), but it does
>> not elaborate further on this point:
>>
>>
>> https://documentcloud.adobe.com/link/track?uri=urn:aaid:scds:US:b6a61fe2-b851-49ce-b8b1-cd760d290636
>> <https://urldefense.com/v3/__https://documentcloud.adobe.com/link/track?uri=urn:aaid:scds:US:b6a61fe2-b851-49ce-b8b1-cd760d290636__;!!KGKeukY!nOrbsi1ZjRJ1NgSNVsN8cyWBHD9f41iFHEvxXTqax6B_qnbhSquQQbwMHrEvhqWEvwR65za2Jag$>
>>
>> 3) Effect Size of a CLMM
>> The random variable's effect is determined by a comparison between the
>> full model to a model with only the fixed effects via the anova() function.
>> I found this information on the 'rcompanion' package website:
>>
>> https://rcompanion.org/handbook/G_12.html
>> <https://urldefense.com/v3/__https://rcompanion.org/handbook/G_12.html__;!!KGKeukY!nOrbsi1ZjRJ1NgSNVsN8cyWBHD9f41iFHEvxXTqax6B_qnbhSquQQbwMHrEvhqWEvwR6uPeTzGc$>
>>
>> The output of this particular anova() will include a value named
>> 'LR.stat', the likelihood ratio statistic. The LR.stat is twice the
>> difference of each log-likelihood (absolute value) of the respective
>> models. Is LR.stat the mixed-model version of an "effect size"? If so, how
>> does one determine if the effect is small, large, in-between, etc?
>>
>> Cheers,
>> Sal
>>
>> Salvatore A. Sidoti
>> PhD Candidate
>> Behavioral Ecology
>> The Ohio State University
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> <https://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models__;!!KGKeukY!nOrbsi1ZjRJ1NgSNVsN8cyWBHD9f41iFHEvxXTqax6B_qnbhSquQQbwMHrEvhqWEvwR6KhTBL3M$>
>>
>
>

	[[alternative HTML version deleted]]


From D@v|d@Du||y @end|ng |rom q|mrbergho|er@edu@@u  Thu Aug 20 04:58:37 2020
From: D@v|d@Du||y @end|ng |rom q|mrbergho|er@edu@@u (David Duffy)
Date: Thu, 20 Aug 2020 02:58:37 +0000
Subject: [R-sig-ME] CLMM: Calculate ICC & Assessing Model Fit
In-Reply-To: <DM5PR0101MB3082784A259835A88E81E2F6AB5D0@DM5PR0101MB3082.prod.exchangelabs.com>
References: <CY4PR0101MB30797B2FE57EE36D0F734BA1AB5C0@CY4PR0101MB3079.prod.exchangelabs.com>,
 <CAG_dBVc_pMU31p-+JTLyu1ZxJ+r_8xt9UvbbKUNqrPhsPOQRSA@mail.gmail.com>,
 <DM5PR0101MB3082784A259835A88E81E2F6AB5D0@DM5PR0101MB3082.prod.exchangelabs.com>
Message-ID: <c5b85e4f07b648b29e6b12cc59317563@qimrberghofer.edu.au>

> It turns out that the statistical contribution of each animal in my study is quite 
> profound, so a mixed model is essential in my case.

Do you really need the cauchit? What does the distribution across 9 levels of the trait
look like? We regularly fit ordinal mixed models in genetic contexts, and one rule of thumb is that if there are more than 5 levels, then treating the variable as continuous usually gives much the same answers as the ordinal model, but interpretation of effects, variance explained etc is a lot more straightforward. Aside from residuals, one can get model goodness-of-fit chi-squares from the pairwise (and higher order) contingency tables (predicted v. observed) eg Visit1 v. Visit2 summed across all subjects (where the probit model parameters are the 8 thresholds and 1 polychoric correlation, so you have 64-9 degrees of freedom for whether a latent bivariate normal fits).

Cheers, David Duffy.

From @|dot|@23 @end|ng |rom buckeyem@||@o@u@edu  Thu Aug 20 05:18:49 2020
From: @|dot|@23 @end|ng |rom buckeyem@||@o@u@edu (Sidoti, Salvatore A.)
Date: Thu, 20 Aug 2020 03:18:49 +0000
Subject: [R-sig-ME] FW:  CLMM: Calculate ICC & Assessing Model Fit
In-Reply-To: <DM5PR0101MB30825365A0DC45DFCD0D4C02AB5A0@DM5PR0101MB3082.prod.exchangelabs.com>
References: <CY4PR0101MB30797B2FE57EE36D0F734BA1AB5C0@CY4PR0101MB3079.prod.exchangelabs.com>,
 <CAG_dBVc_pMU31p-+JTLyu1ZxJ+r_8xt9UvbbKUNqrPhsPOQRSA@mail.gmail.com>,
 <DM5PR0101MB3082784A259835A88E81E2F6AB5D0@DM5PR0101MB3082.prod.exchangelabs.com>
 <c5b85e4f07b648b29e6b12cc59317563@qimrberghofer.edu.au>
 <DM5PR0101MB30825365A0DC45DFCD0D4C02AB5A0@DM5PR0101MB3082.prod.exchangelabs.com>
Message-ID: <DM5PR0101MB308296310ECEA2920A7A5B29AB5A0@DM5PR0101MB3082.prod.exchangelabs.com>

The 'cauchit' link was used in the example for stability. The other distributions that are available in the package produced warnings. My "real" data set uses the 'loglog' link for the clmm().

The distribution across the 9 levels appears rather haphazard, which dissuaded me from trying a GLMM:

https://photos.app.goo.gl/8BtBa9N6PtqbZ1fM8

Cheers,
Sal

-----Original Message-----
From: David Duffy <David.Duffy at qimrberghofer.edu.au>
Sent: Wednesday, August 19, 2020 10:59 PM
To: Sidoti, Salvatore A. <sidoti.23 at buckeyemail.osu.edu>
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] CLMM: Calculate ICC & Assessing Model Fit

> It turns out that the statistical contribution of each animal in my 
> study is quite profound, so a mixed model is essential in my case.

Do you really need the cauchit? What does the distribution across 9 levels of the trait look like? We regularly fit ordinal mixed models in genetic contexts, and one rule of thumb is that if there are more than 5 levels, then treating the variable as continuous usually gives much the same answers as the ordinal model, but interpretation of effects, variance explained etc is a lot more straightforward. Aside from residuals, one can get model goodness-of-fit chi-squares from the pairwise (and higher order) contingency tables (predicted v. observed) eg Visit1 v. Visit2 summed across all subjects (where the probit model parameters are the 8 thresholds and 1 polychoric correlation, so you have 64-9 degrees of freedom for whether a latent bivariate normal fits).

Cheers, David Duffy.


From bbo|ker @end|ng |rom gm@||@com  Sat Aug 22 21:03:49 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sat, 22 Aug 2020 15:03:49 -0400
Subject: [R-sig-ME] Better Scientific Software (BSSw) Fellowship Program
Message-ID: <879c5817-2428-cfc8-b4cc-353108e9dd64@gmail.com>

 ? I can't apply for this ("Applicants must be affiliated with a 
U.S.-based institution that is able to receive funding from the U.S. 
Department of Energy"), but maybe someone here can...(seems like 
fellowships are generally given to somewhat more technical/strategic 
approaches ("Automating testing in scientific software", "Improving the 
reliability and performance of numerical software"), but maybe they'd be 
interested in statistical computing?

https://bssw.io/blog_posts/applications-open-for-the-2021-bssw-fellowship-program


From D@v|d@Du||y @end|ng |rom q|mrbergho|er@edu@@u  Sun Aug 23 09:14:46 2020
From: D@v|d@Du||y @end|ng |rom q|mrbergho|er@edu@@u (David Duffy)
Date: Sun, 23 Aug 2020 07:14:46 +0000
Subject: [R-sig-ME] FW:  CLMM: Calculate ICC & Assessing Model Fit
In-Reply-To: <DM5PR0101MB308296310ECEA2920A7A5B29AB5A0@DM5PR0101MB3082.prod.exchangelabs.com>
References: <CY4PR0101MB30797B2FE57EE36D0F734BA1AB5C0@CY4PR0101MB3079.prod.exchangelabs.com>,
 <CAG_dBVc_pMU31p-+JTLyu1ZxJ+r_8xt9UvbbKUNqrPhsPOQRSA@mail.gmail.com>,
 <DM5PR0101MB3082784A259835A88E81E2F6AB5D0@DM5PR0101MB3082.prod.exchangelabs.com>
 <c5b85e4f07b648b29e6b12cc59317563@qimrberghofer.edu.au>
 <DM5PR0101MB30825365A0DC45DFCD0D4C02AB5A0@DM5PR0101MB3082.prod.exchangelabs.com>,
 <DM5PR0101MB308296310ECEA2920A7A5B29AB5A0@DM5PR0101MB3082.prod.exchangelabs.com>
Message-ID: <a9573a85cac442a19cee5eacb0411eee@qimrberghofer.edu.au>

> The distribution across the 9 levels appears rather haphazard, which dissuaded me from trying a GLMM:

Maybe. A certain amount of that can be soaked up by different intervals between thresholds in the probit-normal - floor and ceiling effects on your scale for the most extreme categories is one example. The equivalent of this for an ordinary LMM is an inverse-normal transformation of the scores (normit/rankit). This sometimes seems a bit lazy to me, but as I commented earlier, you may end up with similar results to the more elaborate models. Worth doing in parallel, at least.
Cheers, David Duffy.


From m|ch@|k@hn10 @end|ng |rom gm@||@com  Sun Aug 23 12:50:47 2020
From: m|ch@|k@hn10 @end|ng |rom gm@||@com (Michal Kahn)
Date: Sun, 23 Aug 2020 20:20:47 +0930
Subject: [R-sig-ME] Help with nested and crossed effects in models with 2-
 and 3-way interactions
Message-ID: <CAAU+Bjkc0DVMK8pG+4cxURF+v5p0e1a7=QJqpZwQJ1_TNbD1=g@mail.gmail.com>

Hello there! I am running a mixed model in lmer, testing the effects of
Covid restrictions on sleep, comparing 2 cohorts of individuals- one from
2019 and one from 2020, coded 0/1 (between subjects). Each individual was
measured repeatedly for ~130 consecutive nights, and each row in the
dataset represents a single night. I also have a binary Lockdown IV, where
each night is coded 0/1 to indicate if it was before/after restrictions
were imposed in 2020 (and the equivalent dates for 2019). Finally, I have a
DayOfWeek IV, where each night is coded 0/1 to indicate if it represents a
weekday/weekend night. The simplified dataset looks something like:

[image: enter image description here] <https://i.stack.imgur.com/Ouuhw.png>

My hypotheses are: (1) there will be a Cohort by Lockdown interaction
effect on sleep; and (2) there will be a Cohort by Lockdown by DayOfWeek
interaction effect on sleep.

For hypothesis 1, I ran:

mod1<- lmer(sleep ~ Cohort*Lockdown + (1|Subject) + (1|Date), data = COVID,
REML=FALSE)

Results seem reasonable, but I think I am not accounting for random slopes.
I have tried to model the slopes as follows, but the model failed to
converge.

mod2<- lmer(sleep ~ Cohort*Lockdown + (Lockdown|Subject), data = COVID,
REML=FALSE)

As for the 2nd hypothesis, if I understand correctly, nights are nested
within DayOfWeek, which are crossed with Lockdown (since each level of
Lockdown includes both weekdays and weekends). I tried the following code,
but am getting a singular fit warning (boundary (singular) fit: see
?isSingular)

mod3<- lmer(sleep ~ Cohort * Lockdown * DayOfWeek + (1|DayOfWeek/date),
data = COVID, REML=FALSE)

Could anyone direct me as to what should be changed in these models? Many
thanks in advance for your help!
Mika

	[[alternative HTML version deleted]]


From mo|||eebrook@ @end|ng |rom gm@||@com  Wed Aug 26 16:15:50 2020
From: mo|||eebrook@ @end|ng |rom gm@||@com (Mollie Brooks)
Date: Wed, 26 Aug 2020 16:15:50 +0200
Subject: [R-sig-ME] 
 Help with nested and crossed effects in models with 2-
 and 3-way interactions
In-Reply-To: <CAAU+Bjkc0DVMK8pG+4cxURF+v5p0e1a7=QJqpZwQJ1_TNbD1=g@mail.gmail.com>
References: <CAAU+Bjkc0DVMK8pG+4cxURF+v5p0e1a7=QJqpZwQJ1_TNbD1=g@mail.gmail.com>
Message-ID: <FA9D61F1-F12E-4D14-A699-8F894F788A83@gmail.com>

I?m guessing that the problem with mod3 could be another instance of confusion with nested effects. 

As originally written, the random effects in mod3 are
(1|DayOfWeek/date) = (1|DayOfWeek) + (1| DayOfWeek:date)
The second term doesn?t make sense to me when each date can only be accompanied by either 0 or 1 for DayOfWeek.

Maybe you want (1|Subject) + (1|Date) in mod3. That model could address both hypotheses.

cheers,
Mollie

> On 23Aug 2020, at 12:50, Michal Kahn <michalkahn10 at gmail.com> wrote:
> 
> Hello there! I am running a mixed model in lmer, testing the effects of
> Covid restrictions on sleep, comparing 2 cohorts of individuals- one from
> 2019 and one from 2020, coded 0/1 (between subjects). Each individual was
> measured repeatedly for ~130 consecutive nights, and each row in the
> dataset represents a single night. I also have a binary Lockdown IV, where
> each night is coded 0/1 to indicate if it was before/after restrictions
> were imposed in 2020 (and the equivalent dates for 2019). Finally, I have a
> DayOfWeek IV, where each night is coded 0/1 to indicate if it represents a
> weekday/weekend night. The simplified dataset looks something like:
> 
> [image: enter image description here] <https://i.stack.imgur.com/Ouuhw.png>
> 
> My hypotheses are: (1) there will be a Cohort by Lockdown interaction
> effect on sleep; and (2) there will be a Cohort by Lockdown by DayOfWeek
> interaction effect on sleep.
> 
> For hypothesis 1, I ran:
> 
> mod1<- lmer(sleep ~ Cohort*Lockdown + (1|Subject) + (1|Date), data = COVID,
> REML=FALSE)
> 
> Results seem reasonable, but I think I am not accounting for random slopes.
> I have tried to model the slopes as follows, but the model failed to
> converge.
> 
> mod2<- lmer(sleep ~ Cohort*Lockdown + (Lockdown|Subject), data = COVID,
> REML=FALSE)
> 
> As for the 2nd hypothesis, if I understand correctly, nights are nested
> within DayOfWeek, which are crossed with Lockdown (since each level of
> Lockdown includes both weekdays and weekends). I tried the following code,
> but am getting a singular fit warning (boundary (singular) fit: see
> ?isSingular)
> 
> mod3<- lmer(sleep ~ Cohort * Lockdown * DayOfWeek + (1|DayOfWeek/date),
> data = COVID, REML=FALSE)
> 
> Could anyone direct me as to what should be changed in these models? Many
> thanks in advance for your help!
> Mika
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From th|erry@onke||nx @end|ng |rom |nbo@be  Wed Aug 26 16:57:36 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Wed, 26 Aug 2020 16:57:36 +0200
Subject: [R-sig-ME] 
 Help with nested and crossed effects in models with 2-
 and 3-way interactions
In-Reply-To: <FA9D61F1-F12E-4D14-A699-8F894F788A83@gmail.com>
References: <CAAU+Bjkc0DVMK8pG+4cxURF+v5p0e1a7=QJqpZwQJ1_TNbD1=g@mail.gmail.com>
 <FA9D61F1-F12E-4D14-A699-8F894F788A83@gmail.com>
Message-ID: <CAJuCY5wR93gQUOmvYLOfKuiwUs_Cu8HMDaYfM_sv4fHu=jKF5g@mail.gmail.com>

Dear Mollie,

I agree that (1| DayOfWeek:date) doesn't make sense and it is better to use
(1|Date). IMHO it might be sensible to include DayOfWeek in the model.
(1|DayOfWeek) + (1|Date) or DayOfWeek + (1|Date). So either as random
effect or as fixed effect. Having a factor both as fixed and random
intercept is nonsense. Given there are only 7 days in week, I'd use
DayOfWeek rather as a fixed effect.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op wo 26 aug. 2020 om 16:16 schreef Mollie Brooks <mollieebrooks at gmail.com>:

> I?m guessing that the problem with mod3 could be another instance of
> confusion with nested effects.
>
> As originally written, the random effects in mod3 are
> (1|DayOfWeek/date) = (1|DayOfWeek) + (1| DayOfWeek:date)
> The second term doesn?t make sense to me when each date can only be
> accompanied by either 0 or 1 for DayOfWeek.
>
> Maybe you want (1|Subject) + (1|Date) in mod3. That model could address
> both hypotheses.
>
> cheers,
> Mollie
>
> > On 23Aug 2020, at 12:50, Michal Kahn <michalkahn10 at gmail.com> wrote:
> >
> > Hello there! I am running a mixed model in lmer, testing the effects of
> > Covid restrictions on sleep, comparing 2 cohorts of individuals- one from
> > 2019 and one from 2020, coded 0/1 (between subjects). Each individual was
> > measured repeatedly for ~130 consecutive nights, and each row in the
> > dataset represents a single night. I also have a binary Lockdown IV,
> where
> > each night is coded 0/1 to indicate if it was before/after restrictions
> > were imposed in 2020 (and the equivalent dates for 2019). Finally, I
> have a
> > DayOfWeek IV, where each night is coded 0/1 to indicate if it represents
> a
> > weekday/weekend night. The simplified dataset looks something like:
> >
> > [image: enter image description here] <
> https://i.stack.imgur.com/Ouuhw.png>
> >
> > My hypotheses are: (1) there will be a Cohort by Lockdown interaction
> > effect on sleep; and (2) there will be a Cohort by Lockdown by DayOfWeek
> > interaction effect on sleep.
> >
> > For hypothesis 1, I ran:
> >
> > mod1<- lmer(sleep ~ Cohort*Lockdown + (1|Subject) + (1|Date), data =
> COVID,
> > REML=FALSE)
> >
> > Results seem reasonable, but I think I am not accounting for random
> slopes.
> > I have tried to model the slopes as follows, but the model failed to
> > converge.
> >
> > mod2<- lmer(sleep ~ Cohort*Lockdown + (Lockdown|Subject), data = COVID,
> > REML=FALSE)
> >
> > As for the 2nd hypothesis, if I understand correctly, nights are nested
> > within DayOfWeek, which are crossed with Lockdown (since each level of
> > Lockdown includes both weekdays and weekends). I tried the following
> code,
> > but am getting a singular fit warning (boundary (singular) fit: see
> > ?isSingular)
> >
> > mod3<- lmer(sleep ~ Cohort * Lockdown * DayOfWeek + (1|DayOfWeek/date),
> > data = COVID, REML=FALSE)
> >
> > Could anyone direct me as to what should be changed in these models? Many
> > thanks in advance for your help!
> > Mika
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From mo|||eebrook@ @end|ng |rom gm@||@com  Wed Aug 26 17:05:00 2020
From: mo|||eebrook@ @end|ng |rom gm@||@com (Mollie Brooks)
Date: Wed, 26 Aug 2020 17:05:00 +0200
Subject: [R-sig-ME] 
 Help with nested and crossed effects in models with 2-
 and 3-way interactions
In-Reply-To: <CAJuCY5wR93gQUOmvYLOfKuiwUs_Cu8HMDaYfM_sv4fHu=jKF5g@mail.gmail.com>
References: <CAAU+Bjkc0DVMK8pG+4cxURF+v5p0e1a7=QJqpZwQJ1_TNbD1=g@mail.gmail.com>
 <FA9D61F1-F12E-4D14-A699-8F894F788A83@gmail.com>
 <CAJuCY5wR93gQUOmvYLOfKuiwUs_Cu8HMDaYfM_sv4fHu=jKF5g@mail.gmail.com>
Message-ID: <1AAE1F08-E176-4F79-AC1A-F0FDC883920D@gmail.com>

Dear Thierry,

The name "DayOfWeek" isn?t totally obvious as it means more like "weekend" and takes values of 0 or 1. So it could only be a fixed effect.

cheers,
Mollie

> On 26Aug 2020, at 16:57, Thierry Onkelinx <thierry.onkelinx at inbo.be> wrote:
> 
> Dear Mollie,
> 
> I agree that (1| DayOfWeek:date) doesn't make sense and it is better to use (1|Date). IMHO it might be sensible to include DayOfWeek in the model. (1|DayOfWeek) + (1|Date) or DayOfWeek + (1|Date). So either as random effect or as fixed effect. Having a factor both as fixed and random intercept is nonsense. Given there are only 7 days in week, I'd use DayOfWeek rather as a fixed effect.
> 
> Best regards,
> 
> ir. Thierry Onkelinx
> Statisticus / Statistician
> 
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance 
> thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be <http://www.inbo.be/>
> 
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
> ///////////////////////////////////////////////////////////////////////////////////////////
> 
>  <https://www.inbo.be/>
> 
> 
> Op wo 26 aug. 2020 om 16:16 schreef Mollie Brooks <mollieebrooks at gmail.com <mailto:mollieebrooks at gmail.com>>:
> I?m guessing that the problem with mod3 could be another instance of confusion with nested effects. 
> 
> As originally written, the random effects in mod3 are
> (1|DayOfWeek/date) = (1|DayOfWeek) + (1| DayOfWeek:date)
> The second term doesn?t make sense to me when each date can only be accompanied by either 0 or 1 for DayOfWeek.
> 
> Maybe you want (1|Subject) + (1|Date) in mod3. That model could address both hypotheses.
> 
> cheers,
> Mollie
> 
> > On 23Aug 2020, at 12:50, Michal Kahn <michalkahn10 at gmail.com <mailto:michalkahn10 at gmail.com>> wrote:
> > 
> > Hello there! I am running a mixed model in lmer, testing the effects of
> > Covid restrictions on sleep, comparing 2 cohorts of individuals- one from
> > 2019 and one from 2020, coded 0/1 (between subjects). Each individual was
> > measured repeatedly for ~130 consecutive nights, and each row in the
> > dataset represents a single night. I also have a binary Lockdown IV, where
> > each night is coded 0/1 to indicate if it was before/after restrictions
> > were imposed in 2020 (and the equivalent dates for 2019). Finally, I have a
> > DayOfWeek IV, where each night is coded 0/1 to indicate if it represents a
> > weekday/weekend night. The simplified dataset looks something like:
> > 
> > [image: enter image description here] <https://i.stack.imgur.com/Ouuhw.png <https://i.stack.imgur.com/Ouuhw.png>>
> > 
> > My hypotheses are: (1) there will be a Cohort by Lockdown interaction
> > effect on sleep; and (2) there will be a Cohort by Lockdown by DayOfWeek
> > interaction effect on sleep.
> > 
> > For hypothesis 1, I ran:
> > 
> > mod1<- lmer(sleep ~ Cohort*Lockdown + (1|Subject) + (1|Date), data = COVID,
> > REML=FALSE)
> > 
> > Results seem reasonable, but I think I am not accounting for random slopes.
> > I have tried to model the slopes as follows, but the model failed to
> > converge.
> > 
> > mod2<- lmer(sleep ~ Cohort*Lockdown + (Lockdown|Subject), data = COVID,
> > REML=FALSE)
> > 
> > As for the 2nd hypothesis, if I understand correctly, nights are nested
> > within DayOfWeek, which are crossed with Lockdown (since each level of
> > Lockdown includes both weekdays and weekends). I tried the following code,
> > but am getting a singular fit warning (boundary (singular) fit: see
> > ?isSingular)
> > 
> > mod3<- lmer(sleep ~ Cohort * Lockdown * DayOfWeek + (1|DayOfWeek/date),
> > data = COVID, REML=FALSE)
> > 
> > Could anyone direct me as to what should be changed in these models? Many
> > thanks in advance for your help!
> > Mika
> > 
> >       [[alternative HTML version deleted]]
> > 
> > _______________________________________________
> > R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-models at r-project.org> mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-models at r-project.org> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>


	[[alternative HTML version deleted]]


From gu|||@ume@|mon@@2 @end|ng |rom gm@||@com  Fri Aug 28 14:40:24 2020
From: gu|||@ume@|mon@@2 @end|ng |rom gm@||@com (Guillaume Adeux)
Date: Fri, 28 Aug 2020 14:40:24 +0200
Subject: [R-sig-ME] glmer / MuMin : Error in asMethod(object) : not a
 positive definite matrix
Message-ID: <CAENiVe9FtWLGtCYgedjL7v+8rtQ=98tDMkQH0bHtv7Y_uvhE1g@mail.gmail.com>

Hello everyone,

I am currently exploring the relationship between weed biomass during the
fallow period and cover crop productivity (in interaction with tillage
type, nitrogen fertilization, and cover crop species, as imposed by the
3-way factorial experimental design).
This results in a highly complex model which I wish to reduce to achieve
parsimony.
Hence, I fitted the full model with glmer as:

mod_full_CC=glmer(dry_bio_weeds_m2+0.001~
*block+year*scale(dry_bio_cover_m2)*tillage*N*CC*
+(1|block:tillage)+(1|block:tillage:N)+(1|block:tillage:N:CC)+(1|block:year)+(1|block:year:tillage)+(1|block:year:tillage:N)+(1|block:year:tillage:N:CC),family=gaussian(link="log"),control=glmerControl(optimizer="nloptwrap",optCtrl=list(algorithm="NLOPT_LN_NELDERMEAD")),data=biomassCC_wo_Cbis)

and fed it to MuMin::dredge() as:

options(na.action = "na.fail")
dred_CC=dredge(mod_full_CC,rank="AICc",fixed=c("block","year"))

However, I am unable to retrieve my "dred_CC" (after 8 days, arf) object
because dredge() stops after returning:
*Error in asMethod(object) : not a positive definite matrix*
Is this due to a specific problem with one given model? If that's the case,
how can I tell the function to simply skip it?
To be completely transparent, it also states "In addition: There were 50 or
more warnings (use warnings() to see the first 50)" but I don't believe the
problem comes from there.

I would greatly greatly appreciate it if someone could lend a hand.
Thanks for your time.

Guillaume ADEUX

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Fri Aug 28 16:37:43 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Fri, 28 Aug 2020 10:37:43 -0400
Subject: [R-sig-ME] glmer / MuMin : Error in asMethod(object) : not a
 positive definite matrix
In-Reply-To: <CAENiVe9FtWLGtCYgedjL7v+8rtQ=98tDMkQH0bHtv7Y_uvhE1g@mail.gmail.com>
References: <CAENiVe9FtWLGtCYgedjL7v+8rtQ=98tDMkQH0bHtv7Y_uvhE1g@mail.gmail.com>
Message-ID: <0ea3b161-eb07-276a-892f-8ad45c2153a3@gmail.com>

    Pretty hard to debug without a reproducible example (and an 8-day 
run time isn't going to help ...)

   The error message comes ultimately from the Matrix package (the error 
message is found in the "dspMatrix" ? "dppMatrix" and "dsyMatrix" ? 
"dpoMatrix" coercion methods).  If you run traceback() **immediately** 
after getting the error message, we might get a little bit more information.

   What are the warnings?

  If you've got multiple cores, using pdredge() might decrease your 
run-time.

   I don't know how to tell dredge() to use a try()-clause or equivalent 
to skip over models that have problems.



On 8/28/20 8:40 AM, Guillaume Adeux wrote:
> Hello everyone,
> 
> I am currently exploring the relationship between weed biomass during the
> fallow period and cover crop productivity (in interaction with tillage
> type, nitrogen fertilization, and cover crop species, as imposed by the
> 3-way factorial experimental design).
> This results in a highly complex model which I wish to reduce to achieve
> parsimony.
> Hence, I fitted the full model with glmer as:
> 
> mod_full_CC=glmer(dry_bio_weeds_m2+0.001~
> *block+year*scale(dry_bio_cover_m2)*tillage*N*CC*
> +(1|block:tillage)+(1|block:tillage:N)+(1|block:tillage:N:CC)+(1|block:year)+(1|block:year:tillage)+(1|block:year:tillage:N)+(1|block:year:tillage:N:CC),family=gaussian(link="log"),control=glmerControl(optimizer="nloptwrap",optCtrl=list(algorithm="NLOPT_LN_NELDERMEAD")),data=biomassCC_wo_Cbis)
> 
> and fed it to MuMin::dredge() as:
> 
> options(na.action = "na.fail")
> dred_CC=dredge(mod_full_CC,rank="AICc",fixed=c("block","year"))
> 
> However, I am unable to retrieve my "dred_CC" (after 8 days, arf) object
> because dredge() stops after returning:
> *Error in asMethod(object) : not a positive definite matrix*
> Is this due to a specific problem with one given model? If that's the case,
> how can I tell the function to simply skip it?
> To be completely transparent, it also states "In addition: There were 50 or
> more warnings (use warnings() to see the first 50)" but I don't believe the
> problem comes from there.
> 
> I would greatly greatly appreciate it if someone could lend a hand.
> Thanks for your time.
> 
> Guillaume ADEUX
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From r@|@e|@wuee@t @end|ng |rom gm@||@com  Fri Aug 28 16:55:04 2020
From: r@|@e|@wuee@t @end|ng |rom gm@||@com (=?utf-8?Q?Rafael_W=C3=BCest?=)
Date: Fri, 28 Aug 2020 16:55:04 +0200
Subject: [R-sig-ME] glmer / MuMin : Error in asMethod(object) : not a
 positive definite matrix
In-Reply-To: <0ea3b161-eb07-276a-892f-8ad45c2153a3@gmail.com>
References: <CAENiVe9FtWLGtCYgedjL7v+8rtQ=98tDMkQH0bHtv7Y_uvhE1g@mail.gmail.com>
 <0ea3b161-eb07-276a-892f-8ad45c2153a3@gmail.com>
Message-ID: <8BEE86AA-C999-46C3-8842-8B015424BC2B@gmail.com>

Hi Guillaume

You can generate a list of calls by setting evaluate = FALSE in tdredge() and use the resulting list to run the models within a try(). You will just need to extract AIC (or whatever) yourself to rank models. But it should help to pinpoint models that fail.

Something like:

dlist <- dredge(mod_full_CC,rank="AICc",fixed=c("block","year"), evaluate = FALSE)
AICs <- sapply(dlist, function(x) {
  try(AICc(eval(x)))
})

HTH,
Rafi


> On 28 Aug 2020, at 16:37, Ben Bolker <bbolker at gmail.com> wrote:
> 
>   Pretty hard to debug without a reproducible example (and an 8-day run time isn't going to help ...)
> 
>  The error message comes ultimately from the Matrix package (the error message is found in the "dspMatrix" ? "dppMatrix" and "dsyMatrix" ? "dpoMatrix" coercion methods).  If you run traceback() **immediately** after getting the error message, we might get a little bit more information.
> 
>  What are the warnings?
> 
> If you've got multiple cores, using pdredge() might decrease your run-time.
> 
>  I don't know how to tell dredge() to use a try()-clause or equivalent to skip over models that have problems.
> 
> 
> 
> On 8/28/20 8:40 AM, Guillaume Adeux wrote:
>> Hello everyone,
>> I am currently exploring the relationship between weed biomass during the
>> fallow period and cover crop productivity (in interaction with tillage
>> type, nitrogen fertilization, and cover crop species, as imposed by the
>> 3-way factorial experimental design).
>> This results in a highly complex model which I wish to reduce to achieve
>> parsimony.
>> Hence, I fitted the full model with glmer as:
>> mod_full_CC=glmer(dry_bio_weeds_m2+0.001~
>> *block+year*scale(dry_bio_cover_m2)*tillage*N*CC*
>> +(1|block:tillage)+(1|block:tillage:N)+(1|block:tillage:N:CC)+(1|block:year)+(1|block:year:tillage)+(1|block:year:tillage:N)+(1|block:year:tillage:N:CC),family=gaussian(link="log"),control=glmerControl(optimizer="nloptwrap",optCtrl=list(algorithm="NLOPT_LN_NELDERMEAD")),data=biomassCC_wo_Cbis)
>> and fed it to MuMin::dredge() as:
>> options(na.action = "na.fail")
>> dred_CC=dredge(mod_full_CC,rank="AICc",fixed=c("block","year"))
>> However, I am unable to retrieve my "dred_CC" (after 8 days, arf) object
>> because dredge() stops after returning:
>> *Error in asMethod(object) : not a positive definite matrix*
>> Is this due to a specific problem with one given model? If that's the case,
>> how can I tell the function to simply skip it?
>> To be completely transparent, it also states "In addition: There were 50 or
>> more warnings (use warnings() to see the first 50)" but I don't believe the
>> problem comes from there.
>> I would greatly greatly appreciate it if someone could lend a hand.
>> Thanks for your time.
>> Guillaume ADEUX
>> 	[[alternative HTML version deleted]]
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> 
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From bbo|ker @end|ng |rom gm@||@com  Fri Aug 28 16:57:01 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Fri, 28 Aug 2020 10:57:01 -0400
Subject: [R-sig-ME] glmer / MuMin : Error in asMethod(object) : not a
 positive definite matrix
In-Reply-To: <8BEE86AA-C999-46C3-8842-8B015424BC2B@gmail.com>
References: <CAENiVe9FtWLGtCYgedjL7v+8rtQ=98tDMkQH0bHtv7Y_uvhE1g@mail.gmail.com>
 <0ea3b161-eb07-276a-892f-8ad45c2153a3@gmail.com>
 <8BEE86AA-C999-46C3-8842-8B015424BC2B@gmail.com>
Message-ID: <65c0ead7-5c01-a0df-efec-95098f36941a@gmail.com>

    Great idea.  And, if you use parallel::mclapply (with appropriate 
use of clusterExport() to make sure the clusters have the objects they 
need in the environment), this can be done in parallel too ...

On 8/28/20 10:55 AM, Rafael W?est wrote:
> Hi Guillaume
> 
> You can generate a list of calls by setting evaluate = FALSE in tdredge() and use the resulting list to run the models within a try(). You will just need to extract AIC (or whatever) yourself to rank models. But it should help to pinpoint models that fail.
> 
> Something like:
> 
> dlist <- dredge(mod_full_CC,rank="AICc",fixed=c("block","year"), evaluate = FALSE)
> AICs <- sapply(dlist, function(x) {
>    try(AICc(eval(x)))
> })
> 
> HTH,
> Rafi
> 
> 
>> On 28 Aug 2020, at 16:37, Ben Bolker <bbolker at gmail.com> wrote:
>>
>>    Pretty hard to debug without a reproducible example (and an 8-day run time isn't going to help ...)
>>
>>   The error message comes ultimately from the Matrix package (the error message is found in the "dspMatrix" ? "dppMatrix" and "dsyMatrix" ? "dpoMatrix" coercion methods).  If you run traceback() **immediately** after getting the error message, we might get a little bit more information.
>>
>>   What are the warnings?
>>
>> If you've got multiple cores, using pdredge() might decrease your run-time.
>>
>>   I don't know how to tell dredge() to use a try()-clause or equivalent to skip over models that have problems.
>>
>>
>>
>> On 8/28/20 8:40 AM, Guillaume Adeux wrote:
>>> Hello everyone,
>>> I am currently exploring the relationship between weed biomass during the
>>> fallow period and cover crop productivity (in interaction with tillage
>>> type, nitrogen fertilization, and cover crop species, as imposed by the
>>> 3-way factorial experimental design).
>>> This results in a highly complex model which I wish to reduce to achieve
>>> parsimony.
>>> Hence, I fitted the full model with glmer as:
>>> mod_full_CC=glmer(dry_bio_weeds_m2+0.001~
>>> *block+year*scale(dry_bio_cover_m2)*tillage*N*CC*
>>> +(1|block:tillage)+(1|block:tillage:N)+(1|block:tillage:N:CC)+(1|block:year)+(1|block:year:tillage)+(1|block:year:tillage:N)+(1|block:year:tillage:N:CC),family=gaussian(link="log"),control=glmerControl(optimizer="nloptwrap",optCtrl=list(algorithm="NLOPT_LN_NELDERMEAD")),data=biomassCC_wo_Cbis)
>>> and fed it to MuMin::dredge() as:
>>> options(na.action = "na.fail")
>>> dred_CC=dredge(mod_full_CC,rank="AICc",fixed=c("block","year"))
>>> However, I am unable to retrieve my "dred_CC" (after 8 days, arf) object
>>> because dredge() stops after returning:
>>> *Error in asMethod(object) : not a positive definite matrix*
>>> Is this due to a specific problem with one given model? If that's the case,
>>> how can I tell the function to simply skip it?
>>> To be completely transparent, it also states "In addition: There were 50 or
>>> more warnings (use warnings() to see the first 50)" but I don't believe the
>>> problem comes from there.
>>> I would greatly greatly appreciate it if someone could lend a hand.
>>> Thanks for your time.
>>> Guillaume ADEUX
>>> 	[[alternative HTML version deleted]]
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From t|m@co|e @end|ng |rom uc|@@c@uk  Wed Sep  2 14:32:10 2020
From: t|m@co|e @end|ng |rom uc|@@c@uk (Cole, Tim)
Date: Wed, 2 Sep 2020 12:32:10 +0000
Subject: [R-sig-ME] nlme bug ?
In-Reply-To: <0d48ebbc-0b21-6c64-44e7-483f7b89f7d6@mpi.nl>
References: <50076891-C35E-49B9-AC63-19A0561F84C0@ucl.ac.uk>
 <e813a061-85ee-44d9-cf4a-2b8600be3ae5@mpi.nl>
 <5B776195-64E4-4208-8ABB-EF6223A065D9@ucl.ac.uk>
 <0d48ebbc-0b21-6c64-44e7-483f7b89f7d6@mpi.nl>
Message-ID: <876D1EB6-B192-42D7-95E7-82EA56CCAC66@ucl.ac.uk>

A bit ago I posted some code suggesting an nlme bug. I?ve now realised it was my error, and for info I?m recording the details here.

The code, at the end of the thread, actually contains two errors. The first is hinted at in my exchange with Phillip, that there are three fixed effects for only two parameters. So the c fixed effect should be omitted.

The second bug is more subtle ? the fitnlme function alters range(x), which is the default value for Boundary.knots in ns(.). So for fitnlme to work Boundary.knots needs to be constrained to be constant, by explicitly setting it to the initial range(x).

With these two changes the code works.

Best wishes,
Tim

``` r
suppressMessages(library(dplyr))

dat <- sitar::heights %>%
  mutate(x = scale(age, scale = FALSE),
         y = scale(height, scale = FALSE)) %>%
  select(x, y, id)

bounds <- with(dat, range(x))
start <- rev(lm(y ~ splines::ns(x), data = dat)$coef)
fitnlme <- function(x, s1, a, c) {
  a + drop(s1 * splines::ns(x * exp(c), B = bounds))
}

nlme::nlme(y ~ fitnlme(x, s1, a, c), fixed = s1 + a ~ 1, random = a + c ~ 1 | id, start = start, data = dat)
#> Nonlinear mixed-effects model fit by maximum likelihood
#>   Model: y ~ fitnlme(x, s1, a, c)
#>   Data: dat
#>   Log-likelihood: -317.4776
#>   Fixed: s1 + a ~ 1
#>        s1         a
#>  48.53801 -18.49412
#>
#> Random effects:
#>  Formula: list(a ~ 1, c ~ 1)
#>  Level: id
#>  Structure: General positive-definite, Log-Cholesky parametrization
#>          StdDev    Corr
#> a        5.5359464 a
#> c        0.2882972 -0.285
#> Residual 2.3225304
#>
#> Number of Observations: 124
#> Number of Groups: 12
```

<sup>Created on 2020-09-02 by the [reprex package](https://reprex.tidyverse.org) (v0.3.0)</sup>
--
Population Policy and Practice
UCL Great Ormond Street Institute of Child Health,
30 Guilford Street, London WC1N 1EH, UK

From: Phillip Alday <phillip.alday at mpi.nl>
Date: Tuesday, 11 August 2020 at 19:58
To: "Cole, Tim" <tim.cole at ucl.ac.uk>
Cc: "r-sig-mixed-models at r-project.org" <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] nlme bug ?


For the lme4 model fit with REML, you can get something comparable to wait nlme calles the log-likelihood by dividing the REML criterion by -2. (There have been a number of exchanges on this list, even in the last year, about the problems of REML vs. _the_ likelihood, so I won't go there, but just note that I can feel the Mathematical Powers That Be looking disappointingly over my shoulder while I brush these problems aside.)

On my machine that gives me:

nlme: -315.0308

lme: -313.6868

lmer: -313.6868

These are all quite similar and the corresponding coefficient values are quite similar, so any differences are related to rounding issues and machine-level variation in the nonlinear optimizer (which doesn't refer to nlme here, but rather all of these tools actually fit the model).  (I don't fully understand all the implementation details of the optimizers themselves, but I have seen them take a different number of iterations to fit the same model on different machines with ostensibly identical software versions.) The slightly better fit returned by the linear methods on my machine should generally be preferred -- but if your inferences are sensitive to such small differences, you probably have bigger problems. ;)

In other words, I wouldn't worry.

Phillip
On 11/8/20 3:35 pm, Cole, Tim wrote:
Thanks Phillip for your thoughts.

Thinking about it some more, I realise there is a redundancy which explains why it fails. The construct

a + drop(s1 * splines::ns(x * exp(c)))

involves three parameters a, s1 and c, whereas it is a linear model with intercept and slope and ought to have just two, i.e.

a + x * b.

This leads to another curiosity. This simpler model fits fine in nlme, but unexpectedly it is not identical to the equivalent model fitted in lme or lme4 ? see below. The mean relative difference between fitted values is 1000 times greater for nlme vs lme than for lme vs lme4.

Is this simply a rounding issue, or are the nlme and lme models genuinely different, and if so, in what way?

Thanks,
Tim

``` r
# install.packages('sitar')
suppressMessages({library(dplyr)
  library(sitar)
  library(lme4)
})

dat <- heights %>%
  mutate(x = scale(age, scale = FALSE)) %>%
  select(x, y = height, id)

start <- setNames(lm(y ~ x, data = dat)$coef, c('a', 'b'))

fitnlme <- function(x, a, b) {a + x * b}

nlme1 <- nlme(y ~ fitnlme(x, a, b), fixed = a + b ~ 1, random = a + b ~ 1 | id, start = start, data = dat)

lme1 <- lme(y ~ x, random = ~ x | id, data = dat)
lmer1 <- lmer(y ~ x + (x | id), data = dat)

all.equal(fitted(nlme1), fitted(lme1), check.attributes = FALSE)
#> [1] "Mean relative difference: 9.603422e-05"
all.equal(fitted(lme1), fitted(lmer1), check.attributes = FALSE)
#> [1] "Mean relative difference: 4.880377e-08"
```

<sup>Created on 2020-08-11 by the [reprex package](https://reprex.tidyverse.org<https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Freprex.tidyverse.org%2F&data=02%7C01%7C%7C4e3a3e85627446850a4308d83e288961%7C1faf88fea9984c5b93c9210a11d9a5c2%7C0%7C0%7C637327691137652975&sdata=RFEOtWcX5Zis44oS4zXQG%2Fgq9j9omUpwVZISuMaombI%3D&reserved=0>) (v0.3.0)</sup>
--
Population Policy and Practice
UCL Great Ormond Street Institute of Child Health,
30 Guilford Street, London WC1N 1EH, UK


From: Phillip Alday <phillip.alday at mpi.nl><mailto:phillip.alday at mpi.nl>
Date: Monday, 10 August 2020 at 16:44
To: "Cole, Tim" <tim.cole at ucl.ac.uk><mailto:tim.cole at ucl.ac.uk>, "r-sig-mixed-models at r-project.org"<mailto:r-sig-mixed-models at r-project.org> <r-sig-mixed-models at r-project.org><mailto:r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] nlme bug ?

This could be a bug, but I would check two related things first:

1. Have you tried plotting a profile plot of the likelihood?

2. Have you tried  different starting values?

In my limited experience, these types of models can be surprisingly
sensitive to starting values.

Phillip

On 29/7/20 7:01 pm, Cole, Tim wrote:
My sitar package fits the Lindstrom-Beath growth curve model, which varies the slope of the fitted spline curve y by scaling x.

The model generally works well, but it fails in the simplest case, with a 1 df spline curve corresponding to a linear regression. The noddy example below fits a random intercept a and scaling factor c, with regression slope s1. The fitnlme function is called 20 times and the values hardly change, and then it produces nonsense numbers and fails.

Might this be a bug? It works fine with 2+ df for the spline curve.

Thanks for your thoughts.

Tim Cole

``` r
# install.packages('sitar')
# also uses splines and nlme
suppressMessages(library(dplyr))

dat <- sitar::heights %>%
   select(x = 'age', y = 'height', id = 'id') %>%
   mutate(x = scale(x, TRUE, FALSE))

start <- lm(y ~ splines::ns(x), data = dat)$coef
(start <- c(s1 = start[[2]], a = start[[1]], c = 0))
#>        s1         a         c
#>  56.82702 128.76694   0.00000

fitnlme <- function(x, s1, a, c) {
   print(unique(s1))
   print(unique(a))
   print(unique(c))
   a + drop(s1 * splines::ns(x * exp(c)))
}

nlme::nlme(y ~ fitnlme(x, s1, a, c), fixed = s1 + a + c ~ 1, random = a + c ~ 1 | id, start = start, data = dat)
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 1.490116e-08
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 1.490116e-08
#> Warning in nlme.formula(y ~ fitnlme(x, s1, a, c), fixed = s1 + a + c ~ 1, :
#> Iteration 1, LME step: nlminb() did not converge (code = 1). PORT message: false
#> convergence (8)
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 1.490116e-08
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 0
#> [1] 56.82702
#> [1] 128.7669
#> [1] 1.490116e-08
#> [1] 48.22096
#>  [1] 134.7411 130.4434 125.1878 134.2582 139.3737 132.7397 120.4782 128.5690
#>  [9] 137.1287 125.7633 136.5816 131.8558
#>  [1]  168940.88  664404.25 1336620.21  242694.18 -481809.06  256605.51
#>  [7] 1934638.21  901713.94 -251016.54 1287590.98  -99830.76  482104.58
#> Error in qr.default(t(const)): NA/NaN/Inf in foreign function call (arg 1)
```
<sup>Created on 2020-07-29 by the [reprex package]


	[[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Mon Sep  7 04:38:20 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Sun, 6 Sep 2020 21:38:20 -0500
Subject: [R-sig-ME] Statistical significance of random-effects (lme4 or
 others)
Message-ID: <CACgv6yWvs25xRknXA+kEAtuurUoBgEx6HWDVuWo=U8fVXSFmAw@mail.gmail.com>

Dear All,

Most MLM packages (e.g., HLM, SPSS, SAS, STATA) provide a p-value for the
variance components.

My understanding based on (
https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#testing-significance-of-random-effects)
is that this is not possible to achieve in R, right?

If not, for my 4 models below, I assume I need to compare, using anova(),
each model against its OLS equivalent to obtain a likelihood ratio test
p-value for each model's variance component, correct?

hsb <- read.csv('
https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')

library(lme4)
m1 <- lmer(math ~ 1 + (1|sch.id), data = hsb)
m2 <- lmer(math ~ meanses + (1|sch.id), data = hsb)
m3 <- lmer(math ~ ses + (ses | sch.id), data = hsb)
m4 <- lmer(math~ ses * meanses + (ses | sch.id ), data = hsb)

ols1 <- lm(math ~ 1, data = hsb)
ols2 <- lm(math ~ meanses, data = hsb)
ols3 <- lm(math ~ ses, data = hsb)
ols4 <- lm(math ~ ses * meanses, data = hsb)

	[[alternative HTML version deleted]]


From v|ctor@or|b@m|@e @end|ng |rom gm@||@com  Mon Sep  7 04:42:32 2020
From: v|ctor@or|b@m|@e @end|ng |rom gm@||@com (Victor Oribamise)
Date: Sun, 6 Sep 2020 21:42:32 -0500
Subject: [R-sig-ME] Statistical significance of random-effects (lme4 or
 others)
In-Reply-To: <CACgv6yWvs25xRknXA+kEAtuurUoBgEx6HWDVuWo=U8fVXSFmAw@mail.gmail.com>
References: <CACgv6yWvs25xRknXA+kEAtuurUoBgEx6HWDVuWo=U8fVXSFmAw@mail.gmail.com>
Message-ID: <CA+iPCzZzbMvoh8FL06CktHdE-PCWGuGSV5VTu7thgGn=D5DhrQ@mail.gmail.com>

Hey Simon,

You can check the lsmeans package in R, you can obtain p values for your
models using the package

Victor

On Sun, Sep 6, 2020 at 9:38 PM Simon Harmel <sim.harmel at gmail.com> wrote:

> Dear All,
>
>
>
> Most MLM packages (e.g., HLM, SPSS, SAS, STATA) provide a p-value for the
>
> variance components.
>
>
>
> My understanding based on (
>
>
> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#testing-significance-of-random-effects
> )
>
> is that this is not possible to achieve in R, right?
>
>
>
> If not, for my 4 models below, I assume I need to compare, using anova(),
>
> each model against its OLS equivalent to obtain a likelihood ratio test
>
> p-value for each model's variance component, correct?
>
>
>
> hsb <- read.csv('
>
> https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')
>
>
>
> library(lme4)
>
> m1 <- lmer(math ~ 1 + (1|sch.id), data = hsb)
>
> m2 <- lmer(math ~ meanses + (1|sch.id), data = hsb)
>
> m3 <- lmer(math ~ ses + (ses | sch.id), data = hsb)
>
> m4 <- lmer(math~ ses * meanses + (ses | sch.id ), data = hsb)
>
>
>
> ols1 <- lm(math ~ 1, data = hsb)
>
> ols2 <- lm(math ~ meanses, data = hsb)
>
> ols3 <- lm(math ~ ses, data = hsb)
>
> ols4 <- lm(math ~ ses * meanses, data = hsb)
>
>
>
>         [[alternative HTML version deleted]]
>
>
>
> _______________________________________________
>
> R-sig-mixed-models at r-project.org mailing list
>
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>

	[[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Mon Sep  7 05:15:26 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Sun, 6 Sep 2020 22:15:26 -0500
Subject: [R-sig-ME] Statistical significance of random-effects (lme4 or
 others)
In-Reply-To: <CA+iPCzZzbMvoh8FL06CktHdE-PCWGuGSV5VTu7thgGn=D5DhrQ@mail.gmail.com>
References: <CACgv6yWvs25xRknXA+kEAtuurUoBgEx6HWDVuWo=U8fVXSFmAw@mail.gmail.com>
 <CA+iPCzZzbMvoh8FL06CktHdE-PCWGuGSV5VTu7thgGn=D5DhrQ@mail.gmail.com>
Message-ID: <CACgv6yVuWMRDB2b0B7BAEcM72eDRtK-Z8LtNU7bhOrQ49xzfwA@mail.gmail.com>

Hi Victor,

Thanks for your response. First, as far as I know "lsmeans" has now become
"emmeans".

Second, all my data and code is 100% reproducible, would you please let me
know how can I possibly obtain the p-value for the random-effects' variance
components in any of the 4 models I showed in my original question?

Thanks, Simon

On Sun, Sep 6, 2020 at 9:42 PM Victor Oribamise <victor.oribamise at gmail.com>
wrote:

> Hey Simon,
>
> You can check the lsmeans package in R, you can obtain p values for your
> models using the package
>
> Victor
>
> On Sun, Sep 6, 2020 at 9:38 PM Simon Harmel <sim.harmel at gmail.com> wrote:
>
>> Dear All,
>>
>>
>>
>> Most MLM packages (e.g., HLM, SPSS, SAS, STATA) provide a p-value for the
>>
>> variance components.
>>
>>
>>
>> My understanding based on (
>>
>>
>> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#testing-significance-of-random-effects
>> )
>>
>> is that this is not possible to achieve in R, right?
>>
>>
>>
>> If not, for my 4 models below, I assume I need to compare, using anova(),
>>
>> each model against its OLS equivalent to obtain a likelihood ratio test
>>
>> p-value for each model's variance component, correct?
>>
>>
>>
>> hsb <- read.csv('
>>
>> https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')
>>
>>
>>
>> library(lme4)
>>
>> m1 <- lmer(math ~ 1 + (1|sch.id), data = hsb)
>>
>> m2 <- lmer(math ~ meanses + (1|sch.id), data = hsb)
>>
>> m3 <- lmer(math ~ ses + (ses | sch.id), data = hsb)
>>
>> m4 <- lmer(math~ ses * meanses + (ses | sch.id ), data = hsb)
>>
>>
>>
>> ols1 <- lm(math ~ 1, data = hsb)
>>
>> ols2 <- lm(math ~ meanses, data = hsb)
>>
>> ols3 <- lm(math ~ ses, data = hsb)
>>
>> ols4 <- lm(math ~ ses * meanses, data = hsb)
>>
>>
>>
>>         [[alternative HTML version deleted]]
>>
>>
>>
>> _______________________________________________
>>
>> R-sig-mixed-models at r-project.org mailing list
>>
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>

	[[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Mon Sep  7 05:47:36 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Sun, 6 Sep 2020 22:47:36 -0500
Subject: [R-sig-ME] Statistical significance of random-effects (lme4 or
 others)
In-Reply-To: <CACgv6yVuWMRDB2b0B7BAEcM72eDRtK-Z8LtNU7bhOrQ49xzfwA@mail.gmail.com>
References: <CACgv6yWvs25xRknXA+kEAtuurUoBgEx6HWDVuWo=U8fVXSFmAw@mail.gmail.com>
 <CA+iPCzZzbMvoh8FL06CktHdE-PCWGuGSV5VTu7thgGn=D5DhrQ@mail.gmail.com>
 <CACgv6yVuWMRDB2b0B7BAEcM72eDRtK-Z8LtNU7bhOrQ49xzfwA@mail.gmail.com>
Message-ID: <CACgv6yWfgR5MHHEhjUvYbuAFrGNH3z=oY_MJtUEPFgnr8C2NOw@mail.gmail.com>

Dear Victor,

I'm looking for the p-value for the "variance components", the variance (or
sd) estimated for random-effects in the 4 models I showed?

For example, for m1 I'm looking for the p-value for the terms shown below.

Linear mixed model fit by REML ['lmerMod']
Formula: math ~ 1 + (1 | sch.id)
   Data: hsb

REML criterion at convergence: 47116.8

Scaled residuals:
    Min      1Q  Median      3Q     Max
-3.0631 -0.7539  0.0267  0.7606  2.7426

Random effects: *************
 Groups   Name        Variance Std.Dev.
 sch.id   (Intercept)  8.614   2.935             *****P-VALUE HERE?****
 Residual             39.148   6.257               **** P_VALUE HERE? ****
Number of obs: 7185, groups:  sch.id, 160

On Sun, Sep 6, 2020 at 10:15 PM Simon Harmel <sim.harmel at gmail.com> wrote:

> Hi Victor,
>
> Thanks for your response. First, as far as I know "lsmeans" has now become
> "emmeans".
>
> Second, all my data and code is 100% reproducible, would you please let me
> know how can I possibly obtain the p-value for the random-effects' variance
> components in any of the 4 models I showed in my original question?
>
> Thanks, Simon
>
> On Sun, Sep 6, 2020 at 9:42 PM Victor Oribamise <
> victor.oribamise at gmail.com> wrote:
>
>> Hey Simon,
>>
>> You can check the lsmeans package in R, you can obtain p values for your
>> models using the package
>>
>> Victor
>>
>> On Sun, Sep 6, 2020 at 9:38 PM Simon Harmel <sim.harmel at gmail.com> wrote:
>>
>>> Dear All,
>>>
>>>
>>>
>>> Most MLM packages (e.g., HLM, SPSS, SAS, STATA) provide a p-value for the
>>>
>>> variance components.
>>>
>>>
>>>
>>> My understanding based on (
>>>
>>>
>>> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#testing-significance-of-random-effects
>>> )
>>>
>>> is that this is not possible to achieve in R, right?
>>>
>>>
>>>
>>> If not, for my 4 models below, I assume I need to compare, using anova(),
>>>
>>> each model against its OLS equivalent to obtain a likelihood ratio test
>>>
>>> p-value for each model's variance component, correct?
>>>
>>>
>>>
>>> hsb <- read.csv('
>>>
>>> https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')
>>>
>>>
>>>
>>> library(lme4)
>>>
>>> m1 <- lmer(math ~ 1 + (1|sch.id), data = hsb)
>>>
>>> m2 <- lmer(math ~ meanses + (1|sch.id), data = hsb)
>>>
>>> m3 <- lmer(math ~ ses + (ses | sch.id), data = hsb)
>>>
>>> m4 <- lmer(math~ ses * meanses + (ses | sch.id ), data = hsb)
>>>
>>>
>>>
>>> ols1 <- lm(math ~ 1, data = hsb)
>>>
>>> ols2 <- lm(math ~ meanses, data = hsb)
>>>
>>> ols3 <- lm(math ~ ses, data = hsb)
>>>
>>> ols4 <- lm(math ~ ses * meanses, data = hsb)
>>>
>>>
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>>
>>>
>>> _______________________________________________
>>>
>>> R-sig-mixed-models at r-project.org mailing list
>>>
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>>

	[[alternative HTML version deleted]]


From juho@kr|@t|@n@ruohonen @end|ng |rom gm@||@com  Mon Sep  7 06:21:10 2020
From: juho@kr|@t|@n@ruohonen @end|ng |rom gm@||@com (Juho Kristian Ruohonen)
Date: Mon, 7 Sep 2020 07:21:10 +0300
Subject: [R-sig-ME] Statistical significance of random-effects (lme4 or
 others)
In-Reply-To: <CACgv6yWfgR5MHHEhjUvYbuAFrGNH3z=oY_MJtUEPFgnr8C2NOw@mail.gmail.com>
References: <CACgv6yWvs25xRknXA+kEAtuurUoBgEx6HWDVuWo=U8fVXSFmAw@mail.gmail.com>
 <CA+iPCzZzbMvoh8FL06CktHdE-PCWGuGSV5VTu7thgGn=D5DhrQ@mail.gmail.com>
 <CACgv6yVuWMRDB2b0B7BAEcM72eDRtK-Z8LtNU7bhOrQ49xzfwA@mail.gmail.com>
 <CACgv6yWfgR5MHHEhjUvYbuAFrGNH3z=oY_MJtUEPFgnr8C2NOw@mail.gmail.com>
Message-ID: <CAG_dBVf_MxVOOHH8WtBdP9ZzFDL+7DTTOTVZGrSq8jkrhF57aQ@mail.gmail.com>

A non-statistician's two cents:

   1. I'm not sure likelihood-ratio tests (LRTs) are valid at all for
   models fit using REML (rather than MLE). The anova() function seems to
   agree, given that its present version (4.0.2) refits the models using MLE
   in order to compare their deviances.
   2. Even when the models have been fit using MLE, likelihood-ratio tests
   for variance components are only applicable in cases of a single variance
   component. In your case, this means a LRT can only be used for *m1 vs
   ols1* and *m2 vs ols2*. There, you simply divide the p-value
reported by *anova(m1,
   ols1) *and *anova(m2, ols2)* by two. Both are obviously extremely
   statistically significant. However, models *m3 *and *m4* both have two
   random effects. The last time I checked, the default assumption of a
   chi-squared deviance is no longer applicable in such cases, so the p-values
   reported by Stata and SPSS are only approximate and tend to be too
   conservative. Perhaps you might apply an information criterion instead,
   such as the AIC
   <https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#can-i-use-aic-for-mixed-models-how-do-i-count-the-number-of-degrees-of-freedom-for-a-random-effect>
   .

Best,

J

ma 7. syysk. 2020 klo 6.48 Simon Harmel (sim.harmel at gmail.com) kirjoitti:

> Dear Victor,
>
> I'm looking for the p-value for the "variance components", the variance (or
> sd) estimated for random-effects in the 4 models I showed?
>
> For example, for m1 I'm looking for the p-value for the terms shown below.
>
> Linear mixed model fit by REML ['lmerMod']
> Formula: math ~ 1 + (1 | sch.id)
>    Data: hsb
>
> REML criterion at convergence: 47116.8
>
> Scaled residuals:
>     Min      1Q  Median      3Q     Max
> -3.0631 -0.7539  0.0267  0.7606  2.7426
>
> Random effects: *************
>  Groups   Name        Variance Std.Dev.
>  sch.id   (Intercept)  8.614   2.935             *****P-VALUE HERE?****
>  Residual             39.148   6.257               **** P_VALUE HERE? ****
> Number of obs: 7185, groups:  sch.id, 160
>
> On Sun, Sep 6, 2020 at 10:15 PM Simon Harmel <sim.harmel at gmail.com> wrote:
>
> > Hi Victor,
> >
> > Thanks for your response. First, as far as I know "lsmeans" has now
> become
> > "emmeans".
> >
> > Second, all my data and code is 100% reproducible, would you please let
> me
> > know how can I possibly obtain the p-value for the random-effects'
> variance
> > components in any of the 4 models I showed in my original question?
> >
> > Thanks, Simon
> >
> > On Sun, Sep 6, 2020 at 9:42 PM Victor Oribamise <
> > victor.oribamise at gmail.com> wrote:
> >
> >> Hey Simon,
> >>
> >> You can check the lsmeans package in R, you can obtain p values for your
> >> models using the package
> >>
> >> Victor
> >>
> >> On Sun, Sep 6, 2020 at 9:38 PM Simon Harmel <sim.harmel at gmail.com>
> wrote:
> >>
> >>> Dear All,
> >>>
> >>>
> >>>
> >>> Most MLM packages (e.g., HLM, SPSS, SAS, STATA) provide a p-value for
> the
> >>>
> >>> variance components.
> >>>
> >>>
> >>>
> >>> My understanding based on (
> >>>
> >>>
> >>>
> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#testing-significance-of-random-effects
> >>> )
> >>>
> >>> is that this is not possible to achieve in R, right?
> >>>
> >>>
> >>>
> >>> If not, for my 4 models below, I assume I need to compare, using
> anova(),
> >>>
> >>> each model against its OLS equivalent to obtain a likelihood ratio test
> >>>
> >>> p-value for each model's variance component, correct?
> >>>
> >>>
> >>>
> >>> hsb <- read.csv('
> >>>
> >>> https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')
> >>>
> >>>
> >>>
> >>> library(lme4)
> >>>
> >>> m1 <- lmer(math ~ 1 + (1|sch.id), data = hsb)
> >>>
> >>> m2 <- lmer(math ~ meanses + (1|sch.id), data = hsb)
> >>>
> >>> m3 <- lmer(math ~ ses + (ses | sch.id), data = hsb)
> >>>
> >>> m4 <- lmer(math~ ses * meanses + (ses | sch.id ), data = hsb)
> >>>
> >>>
> >>>
> >>> ols1 <- lm(math ~ 1, data = hsb)
> >>>
> >>> ols2 <- lm(math ~ meanses, data = hsb)
> >>>
> >>> ols3 <- lm(math ~ ses, data = hsb)
> >>>
> >>> ols4 <- lm(math ~ ses * meanses, data = hsb)
> >>>
> >>>
> >>>
> >>>         [[alternative HTML version deleted]]
> >>>
> >>>
> >>>
> >>> _______________________________________________
> >>>
> >>> R-sig-mixed-models at r-project.org mailing list
> >>>
> >>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>>
> >>>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Mon Sep  7 06:28:19 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Sun, 6 Sep 2020 23:28:19 -0500
Subject: [R-sig-ME] Statistical significance of random-effects (lme4 or
 others)
In-Reply-To: <CAG_dBVf_MxVOOHH8WtBdP9ZzFDL+7DTTOTVZGrSq8jkrhF57aQ@mail.gmail.com>
References: <CACgv6yWvs25xRknXA+kEAtuurUoBgEx6HWDVuWo=U8fVXSFmAw@mail.gmail.com>
 <CA+iPCzZzbMvoh8FL06CktHdE-PCWGuGSV5VTu7thgGn=D5DhrQ@mail.gmail.com>
 <CACgv6yVuWMRDB2b0B7BAEcM72eDRtK-Z8LtNU7bhOrQ49xzfwA@mail.gmail.com>
 <CACgv6yWfgR5MHHEhjUvYbuAFrGNH3z=oY_MJtUEPFgnr8C2NOw@mail.gmail.com>
 <CAG_dBVf_MxVOOHH8WtBdP9ZzFDL+7DTTOTVZGrSq8jkrhF57aQ@mail.gmail.com>
Message-ID: <CACgv6yWu3+7pJmNdiFV4Z8GdKEWgQAcNRvCSjSTmZ6UyBch45Q@mail.gmail.com>

Dear J,

My goal is not to do any comparison between any models. Rather, for each
model I want to know if the variance component is different from 0 or not.
And what is a p-value for that.

On Sun, Sep 6, 2020 at 11:21 PM Juho Kristian Ruohonen <
juho.kristian.ruohonen at gmail.com> wrote:

> A non-statistician's two cents:
>
>    1. I'm not sure likelihood-ratio tests (LRTs) are valid at all for
>    models fit using REML (rather than MLE). The anova() function seems to
>    agree, given that its present version (4.0.2) refits the models using MLE
>    in order to compare their deviances.
>    2. Even when the models have been fit using MLE, likelihood-ratio
>    tests for variance components are only applicable in cases of a single
>    variance component. In your case, this means a LRT can only be used for *m1
>    vs ols1* and *m2 vs ols2*. There, you simply divide the p-value
>    reported by *anova(m1, ols1) *and *anova(m2, ols2)* by two. Both are
>    obviously extremely statistically significant. However, models *m3 *and
>    *m4* both have two random effects. The last time I checked, the
>    default assumption of a chi-squared deviance is no longer applicable in
>    such cases, so the p-values reported by Stata and SPSS are only approximate
>    and tend to be too conservative. Perhaps you might apply an information
>    criterion instead, such as the AIC
>    <https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#can-i-use-aic-for-mixed-models-how-do-i-count-the-number-of-degrees-of-freedom-for-a-random-effect>
>    .
>
> Best,
>
> J
>
> ma 7. syysk. 2020 klo 6.48 Simon Harmel (sim.harmel at gmail.com) kirjoitti:
>
>> Dear Victor,
>>
>> I'm looking for the p-value for the "variance components", the variance
>> (or
>> sd) estimated for random-effects in the 4 models I showed?
>>
>> For example, for m1 I'm looking for the p-value for the terms shown below.
>>
>> Linear mixed model fit by REML ['lmerMod']
>> Formula: math ~ 1 + (1 | sch.id)
>>    Data: hsb
>>
>> REML criterion at convergence: 47116.8
>>
>> Scaled residuals:
>>     Min      1Q  Median      3Q     Max
>> -3.0631 -0.7539  0.0267  0.7606  2.7426
>>
>> Random effects: *************
>>  Groups   Name        Variance Std.Dev.
>>  sch.id   (Intercept)  8.614   2.935             *****P-VALUE HERE?****
>>  Residual             39.148   6.257               **** P_VALUE HERE? ****
>> Number of obs: 7185, groups:  sch.id, 160
>>
>> On Sun, Sep 6, 2020 at 10:15 PM Simon Harmel <sim.harmel at gmail.com>
>> wrote:
>>
>> > Hi Victor,
>> >
>> > Thanks for your response. First, as far as I know "lsmeans" has now
>> become
>> > "emmeans".
>> >
>> > Second, all my data and code is 100% reproducible, would you please let
>> me
>> > know how can I possibly obtain the p-value for the random-effects'
>> variance
>> > components in any of the 4 models I showed in my original question?
>> >
>> > Thanks, Simon
>> >
>> > On Sun, Sep 6, 2020 at 9:42 PM Victor Oribamise <
>> > victor.oribamise at gmail.com> wrote:
>> >
>> >> Hey Simon,
>> >>
>> >> You can check the lsmeans package in R, you can obtain p values for
>> your
>> >> models using the package
>> >>
>> >> Victor
>> >>
>> >> On Sun, Sep 6, 2020 at 9:38 PM Simon Harmel <sim.harmel at gmail.com>
>> wrote:
>> >>
>> >>> Dear All,
>> >>>
>> >>>
>> >>>
>> >>> Most MLM packages (e.g., HLM, SPSS, SAS, STATA) provide a p-value for
>> the
>> >>>
>> >>> variance components.
>> >>>
>> >>>
>> >>>
>> >>> My understanding based on (
>> >>>
>> >>>
>> >>>
>> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#testing-significance-of-random-effects
>> >>> )
>> >>>
>> >>> is that this is not possible to achieve in R, right?
>> >>>
>> >>>
>> >>>
>> >>> If not, for my 4 models below, I assume I need to compare, using
>> anova(),
>> >>>
>> >>> each model against its OLS equivalent to obtain a likelihood ratio
>> test
>> >>>
>> >>> p-value for each model's variance component, correct?
>> >>>
>> >>>
>> >>>
>> >>> hsb <- read.csv('
>> >>>
>> >>> https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')
>> >>>
>> >>>
>> >>>
>> >>> library(lme4)
>> >>>
>> >>> m1 <- lmer(math ~ 1 + (1|sch.id), data = hsb)
>> >>>
>> >>> m2 <- lmer(math ~ meanses + (1|sch.id), data = hsb)
>> >>>
>> >>> m3 <- lmer(math ~ ses + (ses | sch.id), data = hsb)
>> >>>
>> >>> m4 <- lmer(math~ ses * meanses + (ses | sch.id ), data = hsb)
>> >>>
>> >>>
>> >>>
>> >>> ols1 <- lm(math ~ 1, data = hsb)
>> >>>
>> >>> ols2 <- lm(math ~ meanses, data = hsb)
>> >>>
>> >>> ols3 <- lm(math ~ ses, data = hsb)
>> >>>
>> >>> ols4 <- lm(math ~ ses * meanses, data = hsb)
>> >>>
>> >>>
>> >>>
>> >>>         [[alternative HTML version deleted]]
>> >>>
>> >>>
>> >>>
>> >>> _______________________________________________
>> >>>
>> >>> R-sig-mixed-models at r-project.org mailing list
>> >>>
>> >>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >>>
>> >>>
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From d@|uedecke @end|ng |rom uke@de  Mon Sep  7 08:13:16 2020
From: d@|uedecke @end|ng |rom uke@de (=?iso-8859-1?Q?Daniel_L=FCdecke?=)
Date: Mon, 7 Sep 2020 08:13:16 +0200
Subject: [R-sig-ME] Statistical significance of random-effects (lme4 or
 others)
In-Reply-To: <CACgv6yWu3+7pJmNdiFV4Z8GdKEWgQAcNRvCSjSTmZ6UyBch45Q@mail.gmail.com>
References: <CACgv6yWvs25xRknXA+kEAtuurUoBgEx6HWDVuWo=U8fVXSFmAw@mail.gmail.com>
 <CA+iPCzZzbMvoh8FL06CktHdE-PCWGuGSV5VTu7thgGn=D5DhrQ@mail.gmail.com>
 <CACgv6yVuWMRDB2b0B7BAEcM72eDRtK-Z8LtNU7bhOrQ49xzfwA@mail.gmail.com>
 <CACgv6yWfgR5MHHEhjUvYbuAFrGNH3z=oY_MJtUEPFgnr8C2NOw@mail.gmail.com>
 <CAG_dBVf_MxVOOHH8WtBdP9ZzFDL+7DTTOTVZGrSq8jkrhF57aQ@mail.gmail.com>
 <CACgv6yWu3+7pJmNdiFV4Z8GdKEWgQAcNRvCSjSTmZ6UyBch45Q@mail.gmail.com>
Message-ID: <000001d684dd$fa3cdd60$eeb69820$@uke.de>

Hi Simon,
I'm not sure if this is a useful question. The variance can / should never
be negative, and it usually is always above 0 if you have some variation in
your outcome depending on the group factors (random effects).

Packages I know that do some "significance testing" or uncertainty
estimation of random effects are lmerTest::ranova() (quite well documented
what it does) or "arm::se.ranef()" resp. "parameters::standard_error(effects
= "random")". The two latter packages compute standard errors for the
conditional modes of the random effects (what you get with "ranef()").

Best
Daniel

-----Urspr?ngliche Nachricht-----
Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im
Auftrag von Simon Harmel
Gesendet: Montag, 7. September 2020 06:28
An: Juho Kristian Ruohonen <juho.kristian.ruohonen at gmail.com>
Cc: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
Betreff: Re: [R-sig-ME] Statistical significance of random-effects (lme4 or
others)

Dear J,

My goal is not to do any comparison between any models. Rather, for each
model I want to know if the variance component is different from 0 or not.
And what is a p-value for that.

On Sun, Sep 6, 2020 at 11:21 PM Juho Kristian Ruohonen <
juho.kristian.ruohonen at gmail.com> wrote:

> A non-statistician's two cents:
>
>    1. I'm not sure likelihood-ratio tests (LRTs) are valid at all for
>    models fit using REML (rather than MLE). The anova() function seems to
>    agree, given that its present version (4.0.2) refits the models using
MLE
>    in order to compare their deviances.
>    2. Even when the models have been fit using MLE, likelihood-ratio
>    tests for variance components are only applicable in cases of a single
>    variance component. In your case, this means a LRT can only be used for
*m1
>    vs ols1* and *m2 vs ols2*. There, you simply divide the p-value
>    reported by *anova(m1, ols1) *and *anova(m2, ols2)* by two. Both are
>    obviously extremely statistically significant. However, models *m3 *and
>    *m4* both have two random effects. The last time I checked, the
>    default assumption of a chi-squared deviance is no longer applicable in
>    such cases, so the p-values reported by Stata and SPSS are only
approximate
>    and tend to be too conservative. Perhaps you might apply an information
>    criterion instead, such as the AIC
>
<https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#can-i-use-aic-for-m
ixed-models-how-do-i-count-the-number-of-degrees-of-freedom-for-a-random-eff
ect>
>    .
>
> Best,
>
> J
>


--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Joachim Pr?l?, Prof. Dr. Blanche Schwappach-Pignataro, Marya Verdel
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING


From juho@kr|@t|@n@ruohonen @end|ng |rom gm@||@com  Mon Sep  7 08:29:12 2020
From: juho@kr|@t|@n@ruohonen @end|ng |rom gm@||@com (Juho Kristian Ruohonen)
Date: Mon, 7 Sep 2020 09:29:12 +0300
Subject: [R-sig-ME] Statistical significance of random-effects (lme4 or
 others)
In-Reply-To: <000001d684dd$fa3cdd60$eeb69820$@uke.de>
References: <CACgv6yWvs25xRknXA+kEAtuurUoBgEx6HWDVuWo=U8fVXSFmAw@mail.gmail.com>
 <CA+iPCzZzbMvoh8FL06CktHdE-PCWGuGSV5VTu7thgGn=D5DhrQ@mail.gmail.com>
 <CACgv6yVuWMRDB2b0B7BAEcM72eDRtK-Z8LtNU7bhOrQ49xzfwA@mail.gmail.com>
 <CACgv6yWfgR5MHHEhjUvYbuAFrGNH3z=oY_MJtUEPFgnr8C2NOw@mail.gmail.com>
 <CAG_dBVf_MxVOOHH8WtBdP9ZzFDL+7DTTOTVZGrSq8jkrhF57aQ@mail.gmail.com>
 <CACgv6yWu3+7pJmNdiFV4Z8GdKEWgQAcNRvCSjSTmZ6UyBch45Q@mail.gmail.com>
 <000001d684dd$fa3cdd60$eeb69820$@uke.de>
Message-ID: <CAG_dBVdJchGvyMj0ZN67+cNNcgYXtkXXpjTEKBdLDt-4R8Qpqg@mail.gmail.com>

While I agree with Daniel that an assumption of zero random-effect variance
doesn't make much sense, I would point out that in my understanding, a
simple LRT comparing a model with and without the random effect, where
applicable, tests exactly that. The p-value divided by 2 reflects the
probability of a deviance difference equal to or greater than the one
observed between the models, under the null hypothesis that the variance
component equals zero. A very low p/2 means a random-effect variance of 0
is very implausible.

Best,

J

ma 7. syysk. 2020 klo 9.13 Daniel L?decke (d.luedecke at uke.de) kirjoitti:

> Hi Simon,
> I'm not sure if this is a useful question. The variance can / should never
> be negative, and it usually is always above 0 if you have some variation in
> your outcome depending on the group factors (random effects).
>
> Packages I know that do some "significance testing" or uncertainty
> estimation of random effects are lmerTest::ranova() (quite well documented
> what it does) or "arm::se.ranef()" resp.
> "parameters::standard_error(effects
> = "random")". The two latter packages compute standard errors for the
> conditional modes of the random effects (what you get with "ranef()").
>
> Best
> Daniel
>
> -----Urspr?ngliche Nachricht-----
> Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im
> Auftrag von Simon Harmel
> Gesendet: Montag, 7. September 2020 06:28
> An: Juho Kristian Ruohonen <juho.kristian.ruohonen at gmail.com>
> Cc: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
> Betreff: Re: [R-sig-ME] Statistical significance of random-effects (lme4 or
> others)
>
> Dear J,
>
> My goal is not to do any comparison between any models. Rather, for each
> model I want to know if the variance component is different from 0 or not.
> And what is a p-value for that.
>
> On Sun, Sep 6, 2020 at 11:21 PM Juho Kristian Ruohonen <
> juho.kristian.ruohonen at gmail.com> wrote:
>
> > A non-statistician's two cents:
> >
> >    1. I'm not sure likelihood-ratio tests (LRTs) are valid at all for
> >    models fit using REML (rather than MLE). The anova() function seems to
> >    agree, given that its present version (4.0.2) refits the models using
> MLE
> >    in order to compare their deviances.
> >    2. Even when the models have been fit using MLE, likelihood-ratio
> >    tests for variance components are only applicable in cases of a single
> >    variance component. In your case, this means a LRT can only be used
> for
> *m1
> >    vs ols1* and *m2 vs ols2*. There, you simply divide the p-value
> >    reported by *anova(m1, ols1) *and *anova(m2, ols2)* by two. Both are
> >    obviously extremely statistically significant. However, models *m3
> *and
> >    *m4* both have two random effects. The last time I checked, the
> >    default assumption of a chi-squared deviance is no longer applicable
> in
> >    such cases, so the p-values reported by Stata and SPSS are only
> approximate
> >    and tend to be too conservative. Perhaps you might apply an
> information
> >    criterion instead, such as the AIC
> >
> <
> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#can-i-use-aic-for-m
>
> ixed-models-how-do-i-count-the-number-of-degrees-of-freedom-for-a-random-eff
> <https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#can-i-use-aic-for-mixed-models-how-do-i-count-the-number-of-degrees-of-freedom-for-a-random-eff>
> ect>
> >    .
> >
> > Best,
> >
> > J
> >
>
>
> --
>
> _____________________________________________________________________
>
> Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen
> Rechts; Gerichtsstand: Hamburg | www.uke.de
> Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Joachim
> Pr?l?, Prof. Dr. Blanche Schwappach-Pignataro, Marya Verdel
> _____________________________________________________________________
>
> SAVE PAPER - THINK BEFORE PRINTING
>
>

	[[alternative HTML version deleted]]


From Ph||||p@A|d@y @end|ng |rom mp|@n|  Mon Sep  7 09:52:21 2020
From: Ph||||p@A|d@y @end|ng |rom mp|@n| (Alday, Phillip)
Date: Mon, 7 Sep 2020 07:52:21 +0000
Subject: [R-sig-ME] Statistical significance of random-effects (lme4 or
 others)
In-Reply-To: <CAG_dBVdJchGvyMj0ZN67+cNNcgYXtkXXpjTEKBdLDt-4R8Qpqg@mail.gmail.com>
References: <CACgv6yWvs25xRknXA+kEAtuurUoBgEx6HWDVuWo=U8fVXSFmAw@mail.gmail.com>
 <CA+iPCzZzbMvoh8FL06CktHdE-PCWGuGSV5VTu7thgGn=D5DhrQ@mail.gmail.com>
 <CACgv6yVuWMRDB2b0B7BAEcM72eDRtK-Z8LtNU7bhOrQ49xzfwA@mail.gmail.com>
 <CACgv6yWfgR5MHHEhjUvYbuAFrGNH3z=oY_MJtUEPFgnr8C2NOw@mail.gmail.com>
 <CAG_dBVf_MxVOOHH8WtBdP9ZzFDL+7DTTOTVZGrSq8jkrhF57aQ@mail.gmail.com>
 <CACgv6yWu3+7pJmNdiFV4Z8GdKEWgQAcNRvCSjSTmZ6UyBch45Q@mail.gmail.com>
 <000001d684dd$fa3cdd60$eeb69820$@uke.de>
 <CAG_dBVdJchGvyMj0ZN67+cNNcgYXtkXXpjTEKBdLDt-4R8Qpqg@mail.gmail.com>
Message-ID: <27b7b328-86ed-4388-f611-4e2b5aa6f912@mpi.nl>

I'm replying to the last email in the thread, but I'm addressing several
issues raised along the way:

* I'm not sure you should call it a _likelihood_ ratio test for REML
fitted models given that REML isn't _the_ likelihood. (Every couple of
months this topic comes up, so I won't go into it here.)

* That said, you can actually compare REML fitted models *if the fixed
effects are identical*. (This is actually part of why REML is viewed
with some skepticism by some -- the computation of the objective is
dependent on the parameterization of the fixed effects.)

* In all cases, the usual tool for model comparison (LRT) require nested
models. This is why it's tricky to test different random-effects
structures against each other: it can be quite easy to have non nested
models.

* The p/2 for LRT on the random effects comes from the standard LRT
being a two-sided test, but because variances are bounded at zero, you
actually need a one-sided test.

* The variance components can actually be zero; such singular fits are
mathematically well defined. (Being able to estimate such models was one
of the advances in lme4 compared to its predecessor nlme.) A estimate of
zero doesn't mean that there is between group (e.g. here: between
school) variance, but rather that the variance between schools is not
distinguishable from the variability induced by the residual variance.
That sounds quite technical, but in more practical terms means "a zero
estimate means that you can't detect any variation between groups beyond
the residual variation; the between-group variation may be truly zero or
you may simply lack the power to detect it".

* anova() will work for comparing lme4 fits to OLS fits, but the lme4
fit has to be first:

> anova(m1, ols1)
refitting model(s) with ML (instead of REML)
Data: hsb
Models:
ols1: math ~ 1
m1: math ~ 1 + (1 | sch.id)
     npar   AIC   BIC logLik deviance  Chisq Df Pr(>Chisq)
ols1    2 48104 48117 -24050    48100
m1      3 47122 47142 -23558    47116 983.92  1  < 2.2e-16 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ?

(For the technically curious: you have to specify the lme4 object first
due to the way single-dispatch works in R.)

* I would advise against significance tests and standard errors for (the
estimates of) the random effects because their (sampling) distribution
can be highly skewed and significance tests don't capture this.

* Instead, you should use confidence intervals. Profile confidence
intervals are okay, but the gold standard are from the parametric
bootstrap. Here are two examples from the OP's original code:

> confint(m1, method="profile", oldNames=FALSE)
Computing profile confidence intervals ...
                          2.5 %    97.5 %
sd_(Intercept)|sch.id  2.594729  3.315880
sigma                  6.154803  6.361786
(Intercept)           12.156289 13.117121
> confint(m1, method="boot", oldNames=FALSE)
Computing bootstrap confidence intervals ...
                          2.5 %    97.5 %
sd_(Intercept)|sch.id  2.551532  3.272484
sigma                  6.151822  6.363059
(Intercept)           12.091033 13.123430


see ?confint.merMod for more details about other possible arguments.

* Finally, be very careful interpreting p-values. The correct
interpretation is very difficult and tricky.


Best,
Phillip




On 7/9/20 8:29 am, Juho Kristian Ruohonen wrote:
> While I agree with Daniel that an assumption of zero random-effect variance
> doesn't make much sense, I would point out that in my understanding, a
> simple LRT comparing a model with and without the random effect, where
> applicable, tests exactly that. The p-value divided by 2 reflects the
> probability of a deviance difference equal to or greater than the one
> observed between the models, under the null hypothesis that the variance
> component equals zero. A very low p/2 means a random-effect variance of 0
> is very implausible.
> 
> Best,
> 
> J
> 
> ma 7. syysk. 2020 klo 9.13 Daniel L?decke (d.luedecke at uke.de) kirjoitti:
> 
>> Hi Simon,
>> I'm not sure if this is a useful question. The variance can / should never
>> be negative, and it usually is always above 0 if you have some variation in
>> your outcome depending on the group factors (random effects).
>>
>> Packages I know that do some "significance testing" or uncertainty
>> estimation of random effects are lmerTest::ranova() (quite well documented
>> what it does) or "arm::se.ranef()" resp.
>> "parameters::standard_error(effects
>> = "random")". The two latter packages compute standard errors for the
>> conditional modes of the random effects (what you get with "ranef()").
>>
>> Best
>> Daniel
>>
>> -----Urspr?ngliche Nachricht-----
>> Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im
>> Auftrag von Simon Harmel
>> Gesendet: Montag, 7. September 2020 06:28
>> An: Juho Kristian Ruohonen <juho.kristian.ruohonen at gmail.com>
>> Cc: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
>> Betreff: Re: [R-sig-ME] Statistical significance of random-effects (lme4 or
>> others)
>>
>> Dear J,
>>
>> My goal is not to do any comparison between any models. Rather, for each
>> model I want to know if the variance component is different from 0 or not.
>> And what is a p-value for that.
>>
>> On Sun, Sep 6, 2020 at 11:21 PM Juho Kristian Ruohonen <
>> juho.kristian.ruohonen at gmail.com> wrote:
>>
>>> A non-statistician's two cents:
>>>
>>>    1. I'm not sure likelihood-ratio tests (LRTs) are valid at all for
>>>    models fit using REML (rather than MLE). The anova() function seems to
>>>    agree, given that its present version (4.0.2) refits the models using
>> MLE
>>>    in order to compare their deviances.
>>>    2. Even when the models have been fit using MLE, likelihood-ratio
>>>    tests for variance components are only applicable in cases of a single
>>>    variance component. In your case, this means a LRT can only be used
>> for
>> *m1
>>>    vs ols1* and *m2 vs ols2*. There, you simply divide the p-value
>>>    reported by *anova(m1, ols1) *and *anova(m2, ols2)* by two. Both are
>>>    obviously extremely statistically significant. However, models *m3
>> *and
>>>    *m4* both have two random effects. The last time I checked, the
>>>    default assumption of a chi-squared deviance is no longer applicable
>> in
>>>    such cases, so the p-values reported by Stata and SPSS are only
>> approximate
>>>    and tend to be too conservative. Perhaps you might apply an
>> information
>>>    criterion instead, such as the AIC
>>>
>> <
>> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#can-i-use-aic-for-m
>>
>> ixed-models-how-do-i-count-the-number-of-degrees-of-freedom-for-a-random-eff
>> <https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#can-i-use-aic-for-mixed-models-how-do-i-count-the-number-of-degrees-of-freedom-for-a-random-eff>
>> ect>
>>>    .
>>>
>>> Best,
>>>
>>> J
>>>
>>
>>
>> --
>>
>> _____________________________________________________________________
>>
>> Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen
>> Rechts; Gerichtsstand: Hamburg | www.uke.de
>> Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Joachim
>> Pr?l?, Prof. Dr. Blanche Schwappach-Pignataro, Marya Verdel
>> _____________________________________________________________________
>>
>> SAVE PAPER - THINK BEFORE PRINTING
>>
>>
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 

From meg@-x m@iii@g oii gmx@@et  Mon Sep  7 11:56:25 2020
From: meg@-x m@iii@g oii gmx@@et (meg@-x m@iii@g oii gmx@@et)
Date: Mon, 7 Sep 2020 11:56:25 +0200
Subject: [R-sig-ME] glmmTMB, OpenMP and Ubuntu
Message-ID: <003801d684fd$26c511d0$744f3570$@gmx.net>

Dear all,



I am currently fitting a complex model with glmmTMB that involves random
effects and covariance structures (spatiotemporal autocorrelation). I would
like to try the experimental parallel optimization feature
<https://cran.r-project.org/web/packages/glmmTMB/vignettes/parallel.html>
to reduce computation time. To do this I've setup a virtual machine running
Ubuntu 20.04 on 16+ cores. However, I found no information in how to setup
OpenMP, R and glmmTMB. Is there a step by step guide that I can follow?



The Vignette states the following:

"If your OS supports OpenMP parallelization and R was installed using
OpenMP, glmmTMB will automatically pick up the OpenMP flags from R's
Makevars and compile the C++ model with OpenMP support."

I was not able to find any information on how to install R using OpenMP.



Best,



Xaver


	[[alternative HTML version deleted]]


From je@n@m@r|u@@g @end|ng |rom gm@||@com  Mon Sep  7 13:57:55 2020
From: je@n@m@r|u@@g @end|ng |rom gm@||@com (J. Marius RAKOTONDRAMANGA)
Date: Mon, 7 Sep 2020 14:57:55 +0300
Subject: [R-sig-ME] glmmTMB, OpenMP and Ubuntu
In-Reply-To: <003801d684fd$26c511d0$744f3570$@gmx.net>
References: <003801d684fd$26c511d0$744f3570$@gmx.net>
Message-ID: <CAPCa8=FVMEzgsdUALjfiTuhVB5xGBxiCMPopFCAKqPg9eO0fTw@mail.gmail.com>

Dear Xaver,

Have a look on these two links, they may help you to install and configure
"openmpi-bin" on Ubuntu and "Rmpi" R packages:
1- https://zoomadmin.com/HowToInstall/UbuntuPackage/openmpi-bin
2- http://webhome.auburn.edu/~zengpen/tutorials/openMPI_with_R.pdf

Best regards,

---------
J. Marius RAKOTONDRAMANGA, MSc
PhD Student in Biostatistics/Biomathematics
Sorbonne Universit? (ex. UPMC)

Le lun. 7 sept. 2020 ? 12:56, <mega-x at gmx.net> a ?crit :

> Dear all,
>
>
>
> I am currently fitting a complex model with glmmTMB that involves random
> effects and covariance structures (spatiotemporal autocorrelation). I would
> like to try the experimental parallel optimization feature
> <https://cran.r-project.org/web/packages/glmmTMB/vignettes/parallel.html>
> to reduce computation time. To do this I've setup a virtual machine running
> Ubuntu 20.04 on 16+ cores. However, I found no information in how to setup
> OpenMP, R and glmmTMB. Is there a step by step guide that I can follow?
>
>
>
> The Vignette states the following:
>
> "If your OS supports OpenMP parallelization and R was installed using
> OpenMP, glmmTMB will automatically pick up the OpenMP flags from R's
> Makevars and compile the C++ model with OpenMP support."
>
> I was not able to find any information on how to install R using OpenMP.
>
>
>
> Best,
>
>
>
> Xaver
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From ph||||p@@|d@y @end|ng |rom mp|@n|  Mon Sep  7 15:27:33 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Mon, 7 Sep 2020 15:27:33 +0200
Subject: [R-sig-ME] Statistical significance of random-effects (lme4 or
 others)
In-Reply-To: <20200907122901.GA22771@info124.pharmacie.univ-paris5.fr>
References: <CACgv6yWvs25xRknXA+kEAtuurUoBgEx6HWDVuWo=U8fVXSFmAw@mail.gmail.com>
 <CA+iPCzZzbMvoh8FL06CktHdE-PCWGuGSV5VTu7thgGn=D5DhrQ@mail.gmail.com>
 <CACgv6yVuWMRDB2b0B7BAEcM72eDRtK-Z8LtNU7bhOrQ49xzfwA@mail.gmail.com>
 <CACgv6yWfgR5MHHEhjUvYbuAFrGNH3z=oY_MJtUEPFgnr8C2NOw@mail.gmail.com>
 <CAG_dBVf_MxVOOHH8WtBdP9ZzFDL+7DTTOTVZGrSq8jkrhF57aQ@mail.gmail.com>
 <CACgv6yWu3+7pJmNdiFV4Z8GdKEWgQAcNRvCSjSTmZ6UyBch45Q@mail.gmail.com>
 <000001d684dd$fa3cdd60$eeb69820$@uke.de>
 <CAG_dBVdJchGvyMj0ZN67+cNNcgYXtkXXpjTEKBdLDt-4R8Qpqg@mail.gmail.com>
 <27b7b328-86ed-4388-f611-4e2b5aa6f912@mpi.nl>
 <20200907122901.GA22771@info124.pharmacie.univ-paris5.fr>
Message-ID: <a3e9cb9c-acf0-3b87-38db-5d9da01f7c04@mpi.nl>

Yes, you're spot on -- I oversimplified a bit. :) The deeper issue is
indeed the edge of the parameter space. And the p/2 trick also breaks
down for non trivial cases. As do many other asymptotic results in mixed
models -- the big one is the denominator degrees of freedom. There is a
big question not just in defining what the DoF are, but also whether
it's reasonable to use the F distribution based on asymptotics.

Phillip

On 7/9/20 2:29 pm, Emmanuel Curis wrote:
> Hi,
> 
> There is a point I don't understand in your answer:
> 
> On Mon, Sep 07, 2020 at 07:52:21AM +0000, Alday, Phillip wrote:
> [...]
>> * The p/2 for LRT on the random effects comes from the standard LRT
>> being a two-sided test, but because variances are bounded at zero, you
>> actually need a one-sided test.
> 
> I thought the LRT test was always one-sided, because under the
> null-hypothesis that additional parameters are all uneeded, the two
> models have the same likelihood, hence the ratio should be 1, its log
> 0; the chi-square can only be positive by nature (which is consistent
> with the likelihood always higher for a model with more parameter),
> hence the test is by nature one-sided - that is, p = p(LRT > lrt_obs)
> and not p = p(LRT > |lrt-obs|) + p(LRT < -|lrt_obs|).
> 
> Wasn't the p/2 because the asymptotic distribution of the LRT in this
> special case is *not* a 1-df khi-square, because the special case of
> sigma?=0 is at the boundary of the parameter space and not
> ??inside??. Instead, it is a 50-50 mixture of a 1-df khi-square and a
> almost surely constant 0.  An asymptotic result that does not hold for
> more complex cases.
> 
> Am I wrong? Or may be it is just a point of what is called a 1 or
> 2-sided test?
> 
> Best regards,
>


From bbo|ker @end|ng |rom gm@||@com  Mon Sep  7 17:58:18 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 7 Sep 2020 11:58:18 -0400
Subject: [R-sig-ME] Statistical significance of random-effects (lme4 or
 others)
In-Reply-To: <000001d684dd$fa3cdd60$eeb69820$@uke.de>
References: <CACgv6yWvs25xRknXA+kEAtuurUoBgEx6HWDVuWo=U8fVXSFmAw@mail.gmail.com>
 <CA+iPCzZzbMvoh8FL06CktHdE-PCWGuGSV5VTu7thgGn=D5DhrQ@mail.gmail.com>
 <CACgv6yVuWMRDB2b0B7BAEcM72eDRtK-Z8LtNU7bhOrQ49xzfwA@mail.gmail.com>
 <CACgv6yWfgR5MHHEhjUvYbuAFrGNH3z=oY_MJtUEPFgnr8C2NOw@mail.gmail.com>
 <CAG_dBVf_MxVOOHH8WtBdP9ZzFDL+7DTTOTVZGrSq8jkrhF57aQ@mail.gmail.com>
 <CACgv6yWu3+7pJmNdiFV4Z8GdKEWgQAcNRvCSjSTmZ6UyBch45Q@mail.gmail.com>
 <000001d684dd$fa3cdd60$eeb69820$@uke.de>
Message-ID: <5842dbaa-f6c3-b7bf-b218-fc7112dc0c93@gmail.com>

     Also see RLRsim, pbkrtest.

   lmerTest::ranova() is more convenient (and sounds like what you're 
looking for), but RLRsim and pbkrtest are going to be more accurate for 
individual comparisons.

On 9/7/20 2:13 AM, Daniel L?decke wrote:
> Hi Simon,
> I'm not sure if this is a useful question. The variance can / should never
> be negative, and it usually is always above 0 if you have some variation in
> your outcome depending on the group factors (random effects).
> 
> Packages I know that do some "significance testing" or uncertainty
> estimation of random effects are lmerTest::ranova() (quite well documented
> what it does) or "arm::se.ranef()" resp. "parameters::standard_error(effects
> = "random")". The two latter packages compute standard errors for the
> conditional modes of the random effects (what you get with "ranef()").
> 
> Best
> Daniel
> 
> -----Urspr?ngliche Nachricht-----
> Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im
> Auftrag von Simon Harmel
> Gesendet: Montag, 7. September 2020 06:28
> An: Juho Kristian Ruohonen <juho.kristian.ruohonen at gmail.com>
> Cc: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
> Betreff: Re: [R-sig-ME] Statistical significance of random-effects (lme4 or
> others)
> 
> Dear J,
> 
> My goal is not to do any comparison between any models. Rather, for each
> model I want to know if the variance component is different from 0 or not.
> And what is a p-value for that.
> 
> On Sun, Sep 6, 2020 at 11:21 PM Juho Kristian Ruohonen <
> juho.kristian.ruohonen at gmail.com> wrote:
> 
>> A non-statistician's two cents:
>>
>>     1. I'm not sure likelihood-ratio tests (LRTs) are valid at all for
>>     models fit using REML (rather than MLE). The anova() function seems to
>>     agree, given that its present version (4.0.2) refits the models using
> MLE
>>     in order to compare their deviances.
>>     2. Even when the models have been fit using MLE, likelihood-ratio
>>     tests for variance components are only applicable in cases of a single
>>     variance component. In your case, this means a LRT can only be used for
> *m1
>>     vs ols1* and *m2 vs ols2*. There, you simply divide the p-value
>>     reported by *anova(m1, ols1) *and *anova(m2, ols2)* by two. Both are
>>     obviously extremely statistically significant. However, models *m3 *and
>>     *m4* both have two random effects. The last time I checked, the
>>     default assumption of a chi-squared deviance is no longer applicable in
>>     such cases, so the p-values reported by Stata and SPSS are only
> approximate
>>     and tend to be too conservative. Perhaps you might apply an information
>>     criterion instead, such as the AIC
>>
> <https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#can-i-use-aic-for-m
> ixed-models-how-do-i-count-the-number-of-degrees-of-freedom-for-a-random-eff
> ect>
>>     .
>>
>> Best,
>>
>> J
>>
> 
> 
> --
> 
> _____________________________________________________________________
> 
> Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
> Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Joachim Pr?l?, Prof. Dr. Blanche Schwappach-Pignataro, Marya Verdel
> _____________________________________________________________________
> 
> SAVE PAPER - THINK BEFORE PRINTING
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From bbo|ker @end|ng |rom gm@||@com  Mon Sep  7 20:48:09 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 7 Sep 2020 14:48:09 -0400
Subject: [R-sig-ME] glmmTMB, OpenMP and Ubuntu
In-Reply-To: <003801d684fd$26c511d0$744f3570$@gmx.net>
References: <003801d684fd$26c511d0$744f3570$@gmx.net>
Message-ID: <6a77f865-055f-28d2-6d64-2e8e9cb311c2@gmail.com>

    Try running TMB::openmp() . If you already have R installed with 
OpenMP support, that should tell you how many cores OpenMP is configured 
to run with.
    In general R automatically gets built with OpenMP support if a given 
OS/compiler combination supports it; I don't know much about how to 
troubleshoot that if it *doesn't* happen automatically, but there seems 
to be a little information  here

https://cran.r-project.org/web/packages/ProFit/vignettes/ProFit-OpenCL-OpenMP.html

   After it's installed, you could try running some fairly big models 
with glmmTMB(..., control=glmmTMBControl(parallel=xx)) where xx is 
either 1 or >1 (NULL, the default, automatically sets the max number of 
OpenMP threads to the number of available cores on my machine).  Use 
system.time() to check the elapsed time (if you want to be more 
systematic and your examples are fast enough to run several times, you 
can use one of the available R benchmarking package).  I would also 
suggest using 'top' or some system monitoring GUI to check the CPU load 
while your job is running ... the expectation is that the CPU % listed 
for your R process will reflect the number of OpenMP threads being used.

   If you encounter anything particularly interesting you could report 
it at https://github.com/glmmTMB/glmmTMB/issues/620 ...


On 9/7/20 5:56 AM, mega-x at gmx.net wrote:
> Dear all,
> 
> 
> 
> I am currently fitting a complex model with glmmTMB that involves random
> effects and covariance structures (spatiotemporal autocorrelation). I would
> like to try the experimental parallel optimization feature
> <https://cran.r-project.org/web/packages/glmmTMB/vignettes/parallel.html>
> to reduce computation time. To do this I've setup a virtual machine running
> Ubuntu 20.04 on 16+ cores. However, I found no information in how to setup
> OpenMP, R and glmmTMB. Is there a step by step guide that I can follow?
> 
> 
> 
> The Vignette states the following:
> 
> "If your OS supports OpenMP parallelization and R was installed using
> OpenMP, glmmTMB will automatically pick up the OpenMP flags from R's
> Makevars and compile the C++ model with OpenMP support."
> 
> I was not able to find any information on how to install R using OpenMP.
> 
> 
> 
> Best,
> 
> 
> 
> Xaver
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From @|m@h@rme| @end|ng |rom gm@||@com  Tue Sep  8 01:18:00 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Mon, 7 Sep 2020 18:18:00 -0500
Subject: [R-sig-ME] Statistical significance of random-effects (lme4 or
 others)
In-Reply-To: <5842dbaa-f6c3-b7bf-b218-fc7112dc0c93@gmail.com>
References: <CACgv6yWvs25xRknXA+kEAtuurUoBgEx6HWDVuWo=U8fVXSFmAw@mail.gmail.com>
 <CA+iPCzZzbMvoh8FL06CktHdE-PCWGuGSV5VTu7thgGn=D5DhrQ@mail.gmail.com>
 <CACgv6yVuWMRDB2b0B7BAEcM72eDRtK-Z8LtNU7bhOrQ49xzfwA@mail.gmail.com>
 <CACgv6yWfgR5MHHEhjUvYbuAFrGNH3z=oY_MJtUEPFgnr8C2NOw@mail.gmail.com>
 <CAG_dBVf_MxVOOHH8WtBdP9ZzFDL+7DTTOTVZGrSq8jkrhF57aQ@mail.gmail.com>
 <CACgv6yWu3+7pJmNdiFV4Z8GdKEWgQAcNRvCSjSTmZ6UyBch45Q@mail.gmail.com>
 <000001d684dd$fa3cdd60$eeb69820$@uke.de>
 <5842dbaa-f6c3-b7bf-b218-fc7112dc0c93@gmail.com>
Message-ID: <CACgv6yUW_8ruWGK7TH4KT32qa+d5Mi_j=XzKd9yiEvfO6x_D+g@mail.gmail.com>

I highly appreciate everyone's valuable input.

Thank you very much to you all,
Simon

On Mon, Sep 7, 2020 at 10:58 AM Ben Bolker <bbolker at gmail.com> wrote:

>      Also see RLRsim, pbkrtest.
>
>    lmerTest::ranova() is more convenient (and sounds like what you're
> looking for), but RLRsim and pbkrtest are going to be more accurate for
> individual comparisons.
>
> On 9/7/20 2:13 AM, Daniel L?decke wrote:
> > Hi Simon,
> > I'm not sure if this is a useful question. The variance can / should
> never
> > be negative, and it usually is always above 0 if you have some variation
> in
> > your outcome depending on the group factors (random effects).
> >
> > Packages I know that do some "significance testing" or uncertainty
> > estimation of random effects are lmerTest::ranova() (quite well
> documented
> > what it does) or "arm::se.ranef()" resp.
> "parameters::standard_error(effects
> > = "random")". The two latter packages compute standard errors for the
> > conditional modes of the random effects (what you get with "ranef()").
> >
> > Best
> > Daniel
> >
> > -----Urspr?ngliche Nachricht-----
> > Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im
> > Auftrag von Simon Harmel
> > Gesendet: Montag, 7. September 2020 06:28
> > An: Juho Kristian Ruohonen <juho.kristian.ruohonen at gmail.com>
> > Cc: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
> > Betreff: Re: [R-sig-ME] Statistical significance of random-effects (lme4
> or
> > others)
> >
> > Dear J,
> >
> > My goal is not to do any comparison between any models. Rather, for each
> > model I want to know if the variance component is different from 0 or
> not.
> > And what is a p-value for that.
> >
> > On Sun, Sep 6, 2020 at 11:21 PM Juho Kristian Ruohonen <
> > juho.kristian.ruohonen at gmail.com> wrote:
> >
> >> A non-statistician's two cents:
> >>
> >>     1. I'm not sure likelihood-ratio tests (LRTs) are valid at all for
> >>     models fit using REML (rather than MLE). The anova() function seems
> to
> >>     agree, given that its present version (4.0.2) refits the models
> using
> > MLE
> >>     in order to compare their deviances.
> >>     2. Even when the models have been fit using MLE, likelihood-ratio
> >>     tests for variance components are only applicable in cases of a
> single
> >>     variance component. In your case, this means a LRT can only be used
> for
> > *m1
> >>     vs ols1* and *m2 vs ols2*. There, you simply divide the p-value
> >>     reported by *anova(m1, ols1) *and *anova(m2, ols2)* by two. Both are
> >>     obviously extremely statistically significant. However, models *m3
> *and
> >>     *m4* both have two random effects. The last time I checked, the
> >>     default assumption of a chi-squared deviance is no longer
> applicable in
> >>     such cases, so the p-values reported by Stata and SPSS are only
> > approximate
> >>     and tend to be too conservative. Perhaps you might apply an
> information
> >>     criterion instead, such as the AIC
> >>
> > <
> https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#can-i-use-aic-for-m
> >
> ixed-models-how-do-i-count-the-number-of-degrees-of-freedom-for-a-random-eff
> > ect>
> >>     .
> >>
> >> Best,
> >>
> >> J
> >>
> >
> >
> > --
> >
> > _____________________________________________________________________
> >
> > Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen
> Rechts; Gerichtsstand: Hamburg | www.uke.de
> > Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Joachim
> Pr?l?, Prof. Dr. Blanche Schwappach-Pignataro, Marya Verdel
> > _____________________________________________________________________
> >
> > SAVE PAPER - THINK BEFORE PRINTING
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Tue Sep  8 01:22:15 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Mon, 7 Sep 2020 18:22:15 -0500
Subject: [R-sig-ME] var(ranef(Random Effect)) not the same as the variance
 component
Message-ID: <CACgv6yXiXpH8Tf06wEpXXjHAjwJ1osNL_Vc9JV+yPXqjwkNC7g@mail.gmail.com>

Hello All,

A very basic question. Generally, `var(ranef(Random Effect))` may not
necessarily be the same as the variance component reported for that Random
Effect in the model output, correct?


Thank you all,
Simon

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Tue Sep  8 01:25:20 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 7 Sep 2020 19:25:20 -0400
Subject: [R-sig-ME] 
 var(ranef(Random Effect)) not the same as the variance component
In-Reply-To: <CACgv6yXiXpH8Tf06wEpXXjHAjwJ1osNL_Vc9JV+yPXqjwkNC7g@mail.gmail.com>
References: <CACgv6yXiXpH8Tf06wEpXXjHAjwJ1osNL_Vc9JV+yPXqjwkNC7g@mail.gmail.com>
Message-ID: <f6893ea6-52db-553c-9159-96518e035332@gmail.com>

   Yes, that's correct.

 From 
https://stats.stackexchange.com/questions/392283/interpreting-blups-or-varcorr-estimates-in-mixed-models/392307#392307

 > the covariance matrix of the empirical Bayes estimates obtained from 
ranef() is related to the covariance of this posterior distribution [of 
conditional modes/BLUPs] whereas VarCorr() is giving the D matrix, which 
is the covariance matrix of the prior distribution of the random 
effects. These are not the same.

On 9/7/20 7:22 PM, Simon Harmel wrote:
> Hello All,
> 
> A very basic question. Generally, `var(ranef(Random Effect))` may not
> necessarily be the same as the variance component reported for that Random
> Effect in the model output, correct?
> 
> 
> Thank you all,
> Simon
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From @|m@h@rme| @end|ng |rom gm@||@com  Tue Sep  8 02:53:29 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Mon, 7 Sep 2020 19:53:29 -0500
Subject: [R-sig-ME] 
 var(ranef(Random Effect)) not the same as the variance component
In-Reply-To: <f6893ea6-52db-553c-9159-96518e035332@gmail.com>
References: <CACgv6yXiXpH8Tf06wEpXXjHAjwJ1osNL_Vc9JV+yPXqjwkNC7g@mail.gmail.com>
 <f6893ea6-52db-553c-9159-96518e035332@gmail.com>
Message-ID: <CACgv6yXUyiiCY3D9-RFbAG3mFtsp=Ko-oyx8+9B+5R1uLqhEgQ@mail.gmail.com>

Much appreciated, Ben. I will study those resources to better understand
the estimation process.

Thanks again,
Simon

On Mon, Sep 7, 2020 at 6:25 PM Ben Bolker <bbolker at gmail.com> wrote:

>    Yes, that's correct.
>
>  From
>
> https://stats.stackexchange.com/questions/392283/interpreting-blups-or-varcorr-estimates-in-mixed-models/392307#392307
>
>  > the covariance matrix of the empirical Bayes estimates obtained from
> ranef() is related to the covariance of this posterior distribution [of
> conditional modes/BLUPs] whereas VarCorr() is giving the D matrix, which
> is the covariance matrix of the prior distribution of the random
> effects. These are not the same.
>
> On 9/7/20 7:22 PM, Simon Harmel wrote:
> > Hello All,
> >
> > A very basic question. Generally, `var(ranef(Random Effect))` may not
> > necessarily be the same as the variance component reported for that
> Random
> > Effect in the model output, correct?
> >
> >
> > Thank you all,
> > Simon
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Tue Sep  8 03:22:01 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Mon, 7 Sep 2020 20:22:01 -0500
Subject: [R-sig-ME] 
 var(ranef(Random Effect)) not the same as the variance component
In-Reply-To: <CACgv6yXUyiiCY3D9-RFbAG3mFtsp=Ko-oyx8+9B+5R1uLqhEgQ@mail.gmail.com>
References: <CACgv6yXiXpH8Tf06wEpXXjHAjwJ1osNL_Vc9JV+yPXqjwkNC7g@mail.gmail.com>
 <f6893ea6-52db-553c-9159-96518e035332@gmail.com>
 <CACgv6yXUyiiCY3D9-RFbAG3mFtsp=Ko-oyx8+9B+5R1uLqhEgQ@mail.gmail.com>
Message-ID: <CACgv6yWbkqJxbSP5d7PtvmmewhF9h7tXyxLYv4K7y7LPjcifqA@mail.gmail.com>

Ben,

This might seem irrelevant to my previous question, but it is from the post
you linked in your previous answer. So, is it correct language to say:

By including Random-Effects (e.g., random intercepts) of some subjects, we
are **controlling/adjusting/holding constant** subjects's random variations
in that random-effect (e.g., variation in subjects' initial status)?

On Mon, Sep 7, 2020 at 7:53 PM Simon Harmel <sim.harmel at gmail.com> wrote:

> Much appreciated, Ben. I will study those resources to better understand
> the estimation process.
>
> Thanks again,
> Simon
>
> On Mon, Sep 7, 2020 at 6:25 PM Ben Bolker <bbolker at gmail.com> wrote:
>
>>    Yes, that's correct.
>>
>>  From
>>
>> https://stats.stackexchange.com/questions/392283/interpreting-blups-or-varcorr-estimates-in-mixed-models/392307#392307
>>
>>  > the covariance matrix of the empirical Bayes estimates obtained from
>> ranef() is related to the covariance of this posterior distribution [of
>> conditional modes/BLUPs] whereas VarCorr() is giving the D matrix, which
>> is the covariance matrix of the prior distribution of the random
>> effects. These are not the same.
>>
>> On 9/7/20 7:22 PM, Simon Harmel wrote:
>> > Hello All,
>> >
>> > A very basic question. Generally, `var(ranef(Random Effect))` may not
>> > necessarily be the same as the variance component reported for that
>> Random
>> > Effect in the model output, correct?
>> >
>> >
>> > Thank you all,
>> > Simon
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Tue Sep  8 03:24:34 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 7 Sep 2020 21:24:34 -0400
Subject: [R-sig-ME] 
 var(ranef(Random Effect)) not the same as the variance component
In-Reply-To: <CACgv6yWbkqJxbSP5d7PtvmmewhF9h7tXyxLYv4K7y7LPjcifqA@mail.gmail.com>
References: <CACgv6yXiXpH8Tf06wEpXXjHAjwJ1osNL_Vc9JV+yPXqjwkNC7g@mail.gmail.com>
 <f6893ea6-52db-553c-9159-96518e035332@gmail.com>
 <CACgv6yXUyiiCY3D9-RFbAG3mFtsp=Ko-oyx8+9B+5R1uLqhEgQ@mail.gmail.com>
 <CACgv6yWbkqJxbSP5d7PtvmmewhF9h7tXyxLYv4K7y7LPjcifqA@mail.gmail.com>
Message-ID: <5885bedb-33f9-0b72-c2f7-ce1a2354d613@gmail.com>



On 9/7/20 9:22 PM, Simon Harmel wrote:
> Ben,
> 
> This might seem irrelevant to my previous question, but it is from the 
> post you linked in your previous answer. So, is it correct language to say:
> 
> By including Random-Effects (e.g., random intercepts) of some 
> subjects,?we are **controlling/adjusting/holding constant** 
> subjects's?random?variations in that random-effect (e.g., variation in 
> subjects' initial?status)?

    I would probably say something like "incorporating among-subject 
variation in that term (e.g., the initial status) in the model". Or 
"accounting for".
> 
> On Mon, Sep 7, 2020 at 7:53 PM Simon Harmel <sim.harmel at gmail.com 
> <mailto:sim.harmel at gmail.com>> wrote:
> 
>     Much?appreciated, Ben. I will study those resources to better
>     understand the estimation process.
> 
>     Thanks again,
>     Simon
> 
>     On Mon, Sep 7, 2020 at 6:25 PM Ben Bolker <bbolker at gmail.com
>     <mailto:bbolker at gmail.com>> wrote:
> 
>          ? ?Yes, that's correct.
> 
>          ?From
>         https://stats.stackexchange.com/questions/392283/interpreting-blups-or-varcorr-estimates-in-mixed-models/392307#392307
> 
>          ?> the covariance matrix of the empirical Bayes estimates
>         obtained from
>         ranef() is related to the covariance of this posterior
>         distribution [of
>         conditional modes/BLUPs] whereas VarCorr() is giving the D
>         matrix, which
>         is the covariance matrix of the prior distribution of the random
>         effects. These are not the same.
> 
>         On 9/7/20 7:22 PM, Simon Harmel wrote:
>          > Hello All,
>          >
>          > A very basic question. Generally, `var(ranef(Random Effect))`
>         may not
>          > necessarily be the same as the variance component reported
>         for that Random
>          > Effect in the model output, correct?
>          >
>          >
>          > Thank you all,
>          > Simon
>          >
>          >? ? ? ?[[alternative HTML version deleted]]
>          >
>          > _______________________________________________
>          > R-sig-mixed-models at r-project.org
>         <mailto:R-sig-mixed-models at r-project.org> mailing list
>          > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>          >
> 
>         _______________________________________________
>         R-sig-mixed-models at r-project.org
>         <mailto:R-sig-mixed-models at r-project.org> mailing list
>         https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From @|m@h@rme| @end|ng |rom gm@||@com  Tue Sep  8 03:35:16 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Mon, 7 Sep 2020 20:35:16 -0500
Subject: [R-sig-ME] 
 var(ranef(Random Effect)) not the same as the variance component
In-Reply-To: <5885bedb-33f9-0b72-c2f7-ce1a2354d613@gmail.com>
References: <CACgv6yXiXpH8Tf06wEpXXjHAjwJ1osNL_Vc9JV+yPXqjwkNC7g@mail.gmail.com>
 <f6893ea6-52db-553c-9159-96518e035332@gmail.com>
 <CACgv6yXUyiiCY3D9-RFbAG3mFtsp=Ko-oyx8+9B+5R1uLqhEgQ@mail.gmail.com>
 <CACgv6yWbkqJxbSP5d7PtvmmewhF9h7tXyxLYv4K7y7LPjcifqA@mail.gmail.com>
 <5885bedb-33f9-0b72-c2f7-ce1a2354d613@gmail.com>
Message-ID: <CACgv6yVtctOEf-Ocm4QU08=Ew7Xe+0A7cxE8C8V3KFa60mTpwg@mail.gmail.com>

Sure, I felt like "accounting for" might seem a bit vague, could we also
say "after leaving aside the variation among subjects' initial status, ..."?

On Mon, Sep 7, 2020 at 8:24 PM Ben Bolker <bbolker at gmail.com> wrote:

>
>
> On 9/7/20 9:22 PM, Simon Harmel wrote:
> > Ben,
> >
> > This might seem irrelevant to my previous question, but it is from the
> > post you linked in your previous answer. So, is it correct language to
> say:
> >
> > By including Random-Effects (e.g., random intercepts) of some
> > subjects, we are **controlling/adjusting/holding constant**
> > subjects's random variations in that random-effect (e.g., variation in
> > subjects' initial status)?
>
>     I would probably say something like "incorporating among-subject
> variation in that term (e.g., the initial status) in the model". Or
> "accounting for".
> >
> > On Mon, Sep 7, 2020 at 7:53 PM Simon Harmel <sim.harmel at gmail.com
> > <mailto:sim.harmel at gmail.com>> wrote:
> >
> >     Much appreciated, Ben. I will study those resources to better
> >     understand the estimation process.
> >
> >     Thanks again,
> >     Simon
> >
> >     On Mon, Sep 7, 2020 at 6:25 PM Ben Bolker <bbolker at gmail.com
> >     <mailto:bbolker at gmail.com>> wrote:
> >
> >             Yes, that's correct.
> >
> >           From
> >
> https://stats.stackexchange.com/questions/392283/interpreting-blups-or-varcorr-estimates-in-mixed-models/392307#392307
> >
> >           > the covariance matrix of the empirical Bayes estimates
> >         obtained from
> >         ranef() is related to the covariance of this posterior
> >         distribution [of
> >         conditional modes/BLUPs] whereas VarCorr() is giving the D
> >         matrix, which
> >         is the covariance matrix of the prior distribution of the random
> >         effects. These are not the same.
> >
> >         On 9/7/20 7:22 PM, Simon Harmel wrote:
> >          > Hello All,
> >          >
> >          > A very basic question. Generally, `var(ranef(Random Effect))`
> >         may not
> >          > necessarily be the same as the variance component reported
> >         for that Random
> >          > Effect in the model output, correct?
> >          >
> >          >
> >          > Thank you all,
> >          > Simon
> >          >
> >          >       [[alternative HTML version deleted]]
> >          >
> >          > _______________________________________________
> >          > R-sig-mixed-models at r-project.org
> >         <mailto:R-sig-mixed-models at r-project.org> mailing list
> >          > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >          >
> >
> >         _______________________________________________
> >         R-sig-mixed-models at r-project.org
> >         <mailto:R-sig-mixed-models at r-project.org> mailing list
> >         https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Tue Sep  8 04:27:11 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 7 Sep 2020 22:27:11 -0400
Subject: [R-sig-ME] glmmTMB, OpenMP and Ubuntu
In-Reply-To: <007201d6855a$3636a4f0$a2a3eed0$@gmx.net>
References: <003801d684fd$26c511d0$744f3570$@gmx.net>
 <6a77f865-055f-28d2-6d64-2e8e9cb311c2@gmail.com>
 <007201d6855a$3636a4f0$a2a3eed0$@gmx.net>
Message-ID: <cea130b2-b5fd-c2e4-4773-d586bd19e115@gmail.com>


  [keeping r-sig-mixed-models in Cc]

    Don't know exactly how TMB::openmp() works ... ?
    The way I would check if it's running is to see what happens to CPU 
load when you run your model ...

On 9/7/20 5:02 PM, mega-x at gmx.net wrote:
> Dear Ben,
> 
> thank you for your quick response. I am still not sure if OpenMP is actually
> running. The command TMB::openmp() detects all 8 cores on my Linux Server.
> However, it also detects all 4 cores of my personal Windows machine that
> definitely does not support OpenMP.


> 
> The wording in the vignette sounded like you actively have to do something
> with your R installation. Your comment suggest that OpenMP is already part
> of the OS? I am still very confused here.
> 
> I am detecting a very slight improvement by switching from 1 to 8 cores on
> my Linux system (using the code of the vignette). I will do some more
> testing today including running the same model on a 96 cores cluster.
> 
> Cheers,
> 
> Xaver
> 
> -----Original Message-----
> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On
> Behalf Of Ben Bolker
> Sent: Montag, 7. September 2020 20:48
> To: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] glmmTMB, OpenMP and Ubuntu
> 
>      Try running TMB::openmp() . If you already have R installed with OpenMP
> support, that should tell you how many cores OpenMP is configured to run
> with.
>      In general R automatically gets built with OpenMP support if a given
> OS/compiler combination supports it; I don't know much about how to
> troubleshoot that if it *doesn't* happen automatically, but there seems to
> be a little information  here
> 
> https://cran.r-project.org/web/packages/ProFit/vignettes/ProFit-OpenCL-OpenM
> P.html
> 
>     After it's installed, you could try running some fairly big models with
> glmmTMB(..., control=glmmTMBControl(parallel=xx)) where xx is either 1 or >1
> (NULL, the default, automatically sets the max number of OpenMP threads to
> the number of available cores on my machine).  Use
> system.time() to check the elapsed time (if you want to be more systematic
> and your examples are fast enough to run several times, you can use one of
> the available R benchmarking package).  I would also suggest using 'top' or
> some system monitoring GUI to check the CPU load while your job is running
> ... the expectation is that the CPU % listed for your R process will reflect
> the number of OpenMP threads being used.
> 
>     If you encounter anything particularly interesting you could report it at
> https://github.com/glmmTMB/glmmTMB/issues/620 ...
> 
> 
> On 9/7/20 5:56 AM, mega-x at gmx.net wrote:
>> Dear all,
>>
>>
>>
>> I am currently fitting a complex model with glmmTMB that involves random
>> effects and covariance structures (spatiotemporal autocorrelation). I
> would
>> like to try the experimental parallel optimization feature
>> <https://cran.r-project.org/web/packages/glmmTMB/vignettes/parallel.html>
>> to reduce computation time. To do this I've setup a virtual machine
> running
>> Ubuntu 20.04 on 16+ cores. However, I found no information in how to setup
>> OpenMP, R and glmmTMB. Is there a step by step guide that I can follow?
>>
>>
>>
>> The Vignette states the following:
>>
>> "If your OS supports OpenMP parallelization and R was installed using
>> OpenMP, glmmTMB will automatically pick up the OpenMP flags from R's
>> Makevars and compile the C++ model with OpenMP support."
>>
>> I was not able to find any information on how to install R using OpenMP.
>>
>>
>>
>> Best,
>>
>>
>>
>> Xaver
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From d@r|zopou|o@ @end|ng |rom er@@mu@mc@n|  Tue Sep  8 05:02:13 2020
From: d@r|zopou|o@ @end|ng |rom er@@mu@mc@n| (D. Rizopoulos)
Date: Tue, 8 Sep 2020 03:02:13 +0000
Subject: [R-sig-ME] 
 var(ranef(Random Effect)) not the same as the variance component
In-Reply-To: <CACgv6yXiXpH8Tf06wEpXXjHAjwJ1osNL_Vc9JV+yPXqjwkNC7g@mail.gmail.com>
References: <CACgv6yXiXpH8Tf06wEpXXjHAjwJ1osNL_Vc9JV+yPXqjwkNC7g@mail.gmail.com>
Message-ID: <AM6PR04MB58794F13F051216D14720112E8290@AM6PR04MB5879.eurprd04.prod.outlook.com>

Yes, you do not expect these two be the same. The variance components are the prior variances of the random effects, whereas var(ranef(model)) is the variance of the posterior means/modes of the random effects.

Best,
Dimitris

??
Dimitris Rizopoulos
Professor of Biostatistics
Erasmus University Medical Center
The Netherlands
________________________________
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on behalf of Simon Harmel <sim.harmel at gmail.com>
Sent: Tuesday, September 8, 2020 1:22:15 AM
To: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
Subject: [R-sig-ME] var(ranef(Random Effect)) not the same as the variance component

Hello All,

A very basic question. Generally, `var(ranef(Random Effect))` may not
necessarily be the same as the variance component reported for that Random
Effect in the model output, correct?


Thank you all,
Simon

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-mixed-models&amp;data=02%7C01%7Cd.rizopoulos%40erasmusmc.nl%7Cae4132330fbd412dea9508d85384efba%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C0%7C637351177705805772&amp;sdata=agUgmCzM5ecsaoGLm8aPX0%2FuHZF1mK%2BXbP%2Fi6KX5UvI%3D&amp;reserved=0

	[[alternative HTML version deleted]]


From jd|mopou|o@21 @end|ng |rom gm@||@com  Tue Sep  8 08:48:01 2020
From: jd|mopou|o@21 @end|ng |rom gm@||@com (Yannis Dim.)
Date: Tue, 8 Sep 2020 09:48:01 +0300
Subject: [R-sig-ME] Setting priors for binary fixed effects
Message-ID: <CAEyToCXs1vHWomQebi6t=rV_EfuZX_TfDehLrk0rvqJ9o5x-jw@mail.gmail.com>

Dear everyone,

I have a comparative phylogenetic model with a binary response variable, 5
binary explanatory variables as fixed effects and the phylogeny as a random
effect. The issue I have is that with a nitt=300,000,000, I check the
heidel.diag(mcmc.list(model$Sol)) and the plot(model) and  the model does
not converge. All the variables pass the Stationarity test but 2 of them
fail the Halfwidth test. The same 2 variables also have bad trace plots.
I wonder if changing the priors will improve the convergence.Currently I
use these priors:
prior<-list(R=list(V=1, fix=1),G=list(G1=list(V=1, nu=1000, alpha.mu=0,
alpha.V=1)))

As you can see, I am using the default priors for the fixed effects.

Should I change them, due to the binary nature of my fixed effects? What's
the best priors for binary fixed effects?

I will be immensely grateful if someone could help, as this issue's been
bothering me for some time now.

Kind regards,
Yannis Dimopoulos
PhD Student - The University of Hull

	[[alternative HTML version deleted]]


From p|erre@dev|||emereu|| @end|ng |rom ephe@p@|@eu  Tue Sep  8 15:40:55 2020
From: p|erre@dev|||emereu|| @end|ng |rom ephe@p@|@eu (Pierre de Villemereuil)
Date: Tue, 08 Sep 2020 15:40:55 +0200
Subject: [R-sig-ME] Setting priors for binary fixed effects
In-Reply-To: <CAEyToCXs1vHWomQebi6t=rV_EfuZX_TfDehLrk0rvqJ9o5x-jw@mail.gmail.com>
References: <CAEyToCXs1vHWomQebi6t=rV_EfuZX_TfDehLrk0rvqJ9o5x-jw@mail.gmail.com>
Message-ID: <1986823.VlJ2TIBreC@7205pd20lnx.mnhn.fr>

Dear Yannis,

> I have a comparative phylogenetic model with a binary response variable, 5
> binary explanatory variables as fixed effects and the phylogeny as a random
> effect

Which family are you using? In my experience, the family "threshold" tend to yield the best results in terms of MCMC mixing in MCMCglmm.

>All the variables pass the Stationarity test but 2 of them fail the Halfwidth test.

The Halfwidth test is not a test of convergence. It tests whether the sampling was "large enough", but in my experience, it's not a very robust test and I tend to rely on effective sample size to evaluate whether the MCMC was long enough or not. So, if you do not see trends in your traceplots, I would say that convergence is not a problem here.

> Should I change them, due to the binary nature of my fixed effects? What's
> the best priors for binary fixed effects?

More informative priors for the fixed effect can help the mixing of the MCMC (at the cost of possible underestimation and issues with CI coverage if the priors are too informative), but as I said above, I'm unsure whether there is a problem here.

Hope this helps,
Pierre.

Le mardi 8 septembre 2020, 08:48:01 CEST Yannis Dim. a ?crit :
> Dear everyone,
> 
> I have a comparative phylogenetic model with a binary response variable, 5
> binary explanatory variables as fixed effects and the phylogeny as a random
> effect. The issue I have is that with a nitt=300,000,000, I check the
> heidel.diag(mcmc.list(model$Sol)) and the plot(model) and  the model does
> not converge. All the variables pass the Stationarity test but 2 of them
> fail the Halfwidth test. The same 2 variables also have bad trace plots.
> I wonder if changing the priors will improve the convergence.Currently I
> use these priors:
> prior<-list(R=list(V=1, fix=1),G=list(G1=list(V=1, nu=1000, alpha.mu=0,
> alpha.V=1)))
> 
> As you can see, I am using the default priors for the fixed effects.
> 
> Should I change them, due to the binary nature of my fixed effects? What's
> the best priors for binary fixed effects?
> 
> I will be immensely grateful if someone could help, as this issue's been
> bothering me for some time now.
> 
> Kind regards,
> Yannis Dimopoulos
> PhD Student - The University of Hull
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 


From jd|mopou|o@21 @end|ng |rom gm@||@com  Tue Sep  8 15:56:32 2020
From: jd|mopou|o@21 @end|ng |rom gm@||@com (Yannis Dim.)
Date: Tue, 8 Sep 2020 16:56:32 +0300
Subject: [R-sig-ME] Setting priors for binary fixed effects
In-Reply-To: <1986823.VlJ2TIBreC@7205pd20lnx.mnhn.fr>
References: <CAEyToCXs1vHWomQebi6t=rV_EfuZX_TfDehLrk0rvqJ9o5x-jw@mail.gmail.com>
 <1986823.VlJ2TIBreC@7205pd20lnx.mnhn.fr>
Message-ID: <CAEyToCWNkU4_0+Kqi4Mq73_rmTiZkw1mnH_Gt1BRFcBVARbOkg@mail.gmail.com>

Dear Pierre,

Thank you very much for your reply.
I was using family="categorical", but I will try your suggestion and I will
try family="threshold".
Thank you for explaining Halfwidth test to me. So I should only check the
traceplots to check the convergence?

Kind regards,
Yannis




???? ???, 8 ??? 2020 ???? 4:40 ?.?., ?/? Pierre de Villemereuil <
pierre.devillemereuil at ephe.psl.eu> ??????:

> Dear Yannis,
>
> > I have a comparative phylogenetic model with a binary response variable,
> 5
> > binary explanatory variables as fixed effects and the phylogeny as a
> random
> > effect
>
> Which family are you using? In my experience, the family "threshold" tend
> to yield the best results in terms of MCMC mixing in MCMCglmm.
>
> >All the variables pass the Stationarity test but 2 of them fail the
> Halfwidth test.
>
> The Halfwidth test is not a test of convergence. It tests whether the
> sampling was "large enough", but in my experience, it's not a very robust
> test and I tend to rely on effective sample size to evaluate whether the
> MCMC was long enough or not. So, if you do not see trends in your
> traceplots, I would say that convergence is not a problem here.
>
> > Should I change them, due to the binary nature of my fixed effects?
> What's
> > the best priors for binary fixed effects?
>
> More informative priors for the fixed effect can help the mixing of the
> MCMC (at the cost of possible underestimation and issues with CI coverage
> if the priors are too informative), but as I said above, I'm unsure whether
> there is a problem here.
>
> Hope this helps,
> Pierre.
>
> Le mardi 8 septembre 2020, 08:48:01 CEST Yannis Dim. a ?crit :
> > Dear everyone,
> >
> > I have a comparative phylogenetic model with a binary response variable,
> 5
> > binary explanatory variables as fixed effects and the phylogeny as a
> random
> > effect. The issue I have is that with a nitt=300,000,000, I check the
> > heidel.diag(mcmc.list(model$Sol)) and the plot(model) and  the model does
> > not converge. All the variables pass the Stationarity test but 2 of them
> > fail the Halfwidth test. The same 2 variables also have bad trace plots.
> > I wonder if changing the priors will improve the convergence.Currently I
> > use these priors:
> > prior<-list(R=list(V=1, fix=1),G=list(G1=list(V=1, nu=1000, alpha.mu=0,
> > alpha.V=1)))
> >
> > As you can see, I am using the default priors for the fixed effects.
> >
> > Should I change them, due to the binary nature of my fixed effects?
> What's
> > the best priors for binary fixed effects?
> >
> > I will be immensely grateful if someone could help, as this issue's been
> > bothering me for some time now.
> >
> > Kind regards,
> > Yannis Dimopoulos
> > PhD Student - The University of Hull
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>
>
>

	[[alternative HTML version deleted]]


From p|erre@dev|||emereu|| @end|ng |rom ephe@p@|@eu  Tue Sep  8 16:09:20 2020
From: p|erre@dev|||emereu|| @end|ng |rom ephe@p@|@eu (Pierre de Villemereuil)
Date: Tue, 08 Sep 2020 16:09:20 +0200
Subject: [R-sig-ME] Setting priors for binary fixed effects
In-Reply-To: <CAEyToCWNkU4_0+Kqi4Mq73_rmTiZkw1mnH_Gt1BRFcBVARbOkg@mail.gmail.com>
References: <CAEyToCXs1vHWomQebi6t=rV_EfuZX_TfDehLrk0rvqJ9o5x-jw@mail.gmail.com>
 <1986823.VlJ2TIBreC@7205pd20lnx.mnhn.fr>
 <CAEyToCWNkU4_0+Kqi4Mq73_rmTiZkw1mnH_Gt1BRFcBVARbOkg@mail.gmail.com>
Message-ID: <2050352.JsoBbMOpXd@7205pd20lnx.mnhn.fr>

> So I should only check the traceplots to check the convergence?

Assessing convergence in MCMC is not a very easy problem. I'd say checking the traceplot is generally a very good idea, but it lacks in reproducibility. Using the stationarity test (the first part) from heidel.diag() can help in that regard, especially for MCMCglmm were you typically have one chain. If you have more than one chain (e.g. several parallel runs of MCMCglmm), another possible approach is to use the gelman.diag() function to check for stationarity.

Cheers,
Pierre


From h@ro|d@dor@n @end|ng |rom c@mb|um@@@e@@ment@com  Tue Sep  8 22:47:35 2020
From: h@ro|d@dor@n @end|ng |rom c@mb|um@@@e@@ment@com (Harold Doran)
Date: Tue, 8 Sep 2020 20:47:35 +0000
Subject: [R-sig-ME] 
 var(ranef(Random Effect)) not the same as the variance component
In-Reply-To: <AM6PR04MB58794F13F051216D14720112E8290@AM6PR04MB5879.eurprd04.prod.outlook.com>
References: <CACgv6yXiXpH8Tf06wEpXXjHAjwJ1osNL_Vc9JV+yPXqjwkNC7g@mail.gmail.com>
 <AM6PR04MB58794F13F051216D14720112E8290@AM6PR04MB5879.eurprd04.prod.outlook.com>
Message-ID: <01e3671a7c404618b731479948673303@cambiumassessment.com>

To add a little notation to this, we can use law of total variance, var(y) = E(var(Y|X)) + var(E(Y|X)). The conditional means of the random effects are E(Y|X) and hence their variance is only one portion of the total variance. 

-----Original Message-----
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of D. Rizopoulos
Sent: Monday, September 7, 2020 11:02 PM
To: Simon Harmel <sim.harmel at gmail.com>; r-sig-mixed-models <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] var(ranef(Random Effect)) not the same as the variance component

External email alert: Be wary of links & attachments.


Yes, you do not expect these two be the same. The variance components are the prior variances of the random effects, whereas var(ranef(model)) is the variance of the posterior means/modes of the random effects.

Best,
Dimitris

  
Dimitris Rizopoulos
Professor of Biostatistics
Erasmus University Medical Center
The Netherlands
________________________________
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on behalf of Simon Harmel <sim.harmel at gmail.com>
Sent: Tuesday, September 8, 2020 1:22:15 AM
To: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
Subject: [R-sig-ME] var(ranef(Random Effect)) not the same as the variance component

Hello All,

A very basic question. Generally, `var(ranef(Random Effect))` may not necessarily be the same as the variance component reported for that Random Effect in the model output, correct?


Thank you all,
Simon

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-mixed-models&amp;data=02%7C01%7Cd.rizopoulos%40erasmusmc.nl%7Cae4132330fbd412dea9508d85384efba%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C0%7C637351177705805772&amp;sdata=agUgmCzM5ecsaoGLm8aPX0%2FuHZF1mK%2BXbP%2Fi6KX5UvI%3D&amp;reserved=0

        [[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Wed Sep  9 17:53:19 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Wed, 9 Sep 2020 10:53:19 -0500
Subject: [R-sig-ME] 
 var(ranef(Random Effect)) not the same as the variance component
In-Reply-To: <01e3671a7c404618b731479948673303@cambiumassessment.com>
References: <CACgv6yXiXpH8Tf06wEpXXjHAjwJ1osNL_Vc9JV+yPXqjwkNC7g@mail.gmail.com>
 <AM6PR04MB58794F13F051216D14720112E8290@AM6PR04MB5879.eurprd04.prod.outlook.com>
 <01e3671a7c404618b731479948673303@cambiumassessment.com>
Message-ID: <CACgv6yXuhy9AkEVj0bOari4BPpp5YYvknMDir8U5=wp0to_X3Q@mail.gmail.com>

First of all, thank you all for your valuable input.

Dimitris,

Thank you I upvoted your answer on CV as well. But please help me
understand a few things.

1- By D matrix, you mean the G matrix shown in:
https://bookdown.org/marklhc/notes/simulating-multilevel-data.html#linear-growth-model

2- When you say variance components in the output are prior values, can you
tell me how these prior values are obtained? I guess from the data itself,
but how exactly (do we run individual models first to see how much
intercepts and slopes vary & co-vary and take those as prior)?

3- Harold above noted that: "The conditional means of the random effects
are E(Y|X) and hence their variance is only one portion of the total
variance [i.e., var(y)]." I'm not sure how this directly relates to my
question in this thread?

Thank you,
Simon




On Tue, Sep 8, 2020 at 3:47 PM Harold Doran <
harold.doran at cambiumassessment.com> wrote:

> To add a little notation to this, we can use law of total variance, var(y)
> = E(var(Y|X)) + var(E(Y|X)). The conditional means of the random effects
> are E(Y|X) and hence their variance is only one portion of the total
> variance.
>
> -----Original Message-----
> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On
> Behalf Of D. Rizopoulos
> Sent: Monday, September 7, 2020 11:02 PM
> To: Simon Harmel <sim.harmel at gmail.com>; r-sig-mixed-models <
> r-sig-mixed-models at r-project.org>
> Subject: Re: [R-sig-ME] var(ranef(Random Effect)) not the same as the
> variance component
>
> External email alert: Be wary of links & attachments.
>
>
> Yes, you do not expect these two be the same. The variance components are
> the prior variances of the random effects, whereas var(ranef(model)) is the
> variance of the posterior means/modes of the random effects.
>
> Best,
> Dimitris
>
>
> Dimitris Rizopoulos
> Professor of Biostatistics
> Erasmus University Medical Center
> The Netherlands
> ________________________________
> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on
> behalf of Simon Harmel <sim.harmel at gmail.com>
> Sent: Tuesday, September 8, 2020 1:22:15 AM
> To: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
> Subject: [R-sig-ME] var(ranef(Random Effect)) not the same as the variance
> component
>
> Hello All,
>
> A very basic question. Generally, `var(ranef(Random Effect))` may not
> necessarily be the same as the variance component reported for that Random
> Effect in the model output, correct?
>
>
> Thank you all,
> Simon
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
>
> https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-mixed-models&amp;data=02%7C01%7Cd.rizopoulos%40erasmusmc.nl%7Cae4132330fbd412dea9508d85384efba%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C0%7C637351177705805772&amp;sdata=agUgmCzM5ecsaoGLm8aPX0%2FuHZF1mK%2BXbP%2Fi6KX5UvI%3D&amp;reserved=0
>
>         [[alternative HTML version deleted]]
>
>

	[[alternative HTML version deleted]]


From h@ro|d@dor@n @end|ng |rom c@mb|um@@@e@@ment@com  Wed Sep  9 20:48:17 2020
From: h@ro|d@dor@n @end|ng |rom c@mb|um@@@e@@ment@com (Harold Doran)
Date: Wed, 9 Sep 2020 18:48:17 +0000
Subject: [R-sig-ME] 
 var(ranef(Random Effect)) not the same as the variance component
In-Reply-To: <CACgv6yXuhy9AkEVj0bOari4BPpp5YYvknMDir8U5=wp0to_X3Q@mail.gmail.com>
References: <CACgv6yXiXpH8Tf06wEpXXjHAjwJ1osNL_Vc9JV+yPXqjwkNC7g@mail.gmail.com>
 <AM6PR04MB58794F13F051216D14720112E8290@AM6PR04MB5879.eurprd04.prod.outlook.com>
 <01e3671a7c404618b731479948673303@cambiumassessment.com>
 <CACgv6yXuhy9AkEVj0bOari4BPpp5YYvknMDir8U5=wp0to_X3Q@mail.gmail.com>
Message-ID: <0bbdd17ba3354b4faffc74f907134024@cambiumassessment.com>

Simon

Here is an example to show what my notation implies with respect to your question:

fm1 <- lmer(Reaction ~ 1 + (1|Subject), sleepstudy)

sqrt(var(ranef(fm1)$Subject) + mean(sapply(attr(ranef(fm1, condVar=TRUE)[[1]], "postVar"),function(x) x)))

From: Simon Harmel <sim.harmel at gmail.com>
Sent: Wednesday, September 9, 2020 11:53 AM
To: D. Rizopoulos <d.rizopoulos at erasmusmc.nl>
Cc: r-sig-mixed-models <r-sig-mixed-models at r-project.org>; Harold Doran <harold.doran at cambiumassessment.com>; Ben Bolker <bbolker at gmail.com>
Subject: Re: [R-sig-ME] var(ranef(Random Effect)) not the same as the variance component

First of all, thank you all for your valuable input.

Dimitris,

Thank you I upvoted your answer on CV as well. But please help me understand a few things.

1- By D matrix, you mean the G matrix shown in: https://bookdown.org/marklhc/notes/simulating-multilevel-data.html#linear-growth-model

2- When you say variance components in the output are prior values, can you tell me how these prior values are obtained? I guess from the data itself, but how exactly (do we run individual models first to see how much intercepts and slopes vary & co-vary and take those as prior)?

3- Harold above noted that: "The conditional means of the random effects are E(Y|X) and hence their variance is only one portion of the total variance [i.e., var(y)]." I'm not sure how this directly relates to my question in this thread?

Thank you,
Simon




On Tue, Sep 8, 2020 at 3:47 PM Harold Doran <harold.doran at cambiumassessment.com<mailto:harold.doran at cambiumassessment.com>> wrote:
To add a little notation to this, we can use law of total variance, var(y) = E(var(Y|X)) + var(E(Y|X)). The conditional means of the random effects are E(Y|X) and hence their variance is only one portion of the total variance.

-----Original Message-----
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org<mailto:r-sig-mixed-models-bounces at r-project.org>> On Behalf Of D. Rizopoulos
Sent: Monday, September 7, 2020 11:02 PM
To: Simon Harmel <sim.harmel at gmail.com<mailto:sim.harmel at gmail.com>>; r-sig-mixed-models <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
Subject: Re: [R-sig-ME] var(ranef(Random Effect)) not the same as the variance component

External email alert: Be wary of links & attachments.


Yes, you do not expect these two be the same. The variance components are the prior variances of the random effects, whereas var(ranef(model)) is the variance of the posterior means/modes of the random effects.

Best,
Dimitris


Dimitris Rizopoulos
Professor of Biostatistics
Erasmus University Medical Center
The Netherlands
________________________________
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org<mailto:r-sig-mixed-models-bounces at r-project.org>> on behalf of Simon Harmel <sim.harmel at gmail.com<mailto:sim.harmel at gmail.com>>
Sent: Tuesday, September 8, 2020 1:22:15 AM
To: r-sig-mixed-models <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
Subject: [R-sig-ME] var(ranef(Random Effect)) not the same as the variance component

Hello All,

A very basic question. Generally, `var(ranef(Random Effect))` may not necessarily be the same as the variance component reported for that Random Effect in the model output, correct?


Thank you all,
Simon

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-mixed-models&amp;data=02%7C01%7Cd.rizopoulos%40erasmusmc.nl%7Cae4132330fbd412dea9508d85384efba%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C0%7C637351177705805772&amp;sdata=agUgmCzM5ecsaoGLm8aPX0%2FuHZF1mK%2BXbP%2Fi6KX5UvI%3D&amp;reserved=0

        [[alternative HTML version deleted]]

	[[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Wed Sep  9 22:18:43 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Wed, 9 Sep 2020 15:18:43 -0500
Subject: [R-sig-ME] 
 var(ranef(Random Effect)) not the same as the variance component
In-Reply-To: <0bbdd17ba3354b4faffc74f907134024@cambiumassessment.com>
References: <CACgv6yXiXpH8Tf06wEpXXjHAjwJ1osNL_Vc9JV+yPXqjwkNC7g@mail.gmail.com>
 <AM6PR04MB58794F13F051216D14720112E8290@AM6PR04MB5879.eurprd04.prod.outlook.com>
 <01e3671a7c404618b731479948673303@cambiumassessment.com>
 <CACgv6yXuhy9AkEVj0bOari4BPpp5YYvknMDir8U5=wp0to_X3Q@mail.gmail.com>
 <0bbdd17ba3354b4faffc74f907134024@cambiumassessment.com>
Message-ID: <CACgv6yUGGrKozMZ=dWg9w1WWRtUoYAP34SYGE4-EKasz873RaA@mail.gmail.com>

Thank you, Harold.

1) `var(ranef(fm1)$Subject)` is the posterior variance of random effects or
their prior variance? (this was the point discussed so far in this thread)

2) Also, what about `mean(sapply(attr(ranef(fm1)$Subject,
"postVar"),function(x) x))`, what this expected variance represents in
words?

3) What does their sum represent? The total observed variance in random
deviations in intercepts of subjects?

Thank you very much

On Wed, Sep 9, 2020 at 1:48 PM Harold Doran <
harold.doran at cambiumassessment.com> wrote:

> Simon
>
>
>
> Here is an example to show what my notation implies with respect to your
> question:
>
>
>
> fm1 <- lmer(Reaction ~ 1 + (1|Subject), sleepstudy)
>
>
>
> sqrt(var(ranef(fm1)$Subject) + mean(sapply(attr(ranef(fm1,
> condVar=TRUE)[[1]], "postVar"),function(x) x)))
>
>
>
> *From:* Simon Harmel <sim.harmel at gmail.com>
> *Sent:* Wednesday, September 9, 2020 11:53 AM
> *To:* D. Rizopoulos <d.rizopoulos at erasmusmc.nl>
> *Cc:* r-sig-mixed-models <r-sig-mixed-models at r-project.org>; Harold Doran
> <harold.doran at cambiumassessment.com>; Ben Bolker <bbolker at gmail.com>
> *Subject:* Re: [R-sig-ME] var(ranef(Random Effect)) not the same as the
> variance component
>
>
>
> First of all, thank you all for your valuable input.
>
>
>
> Dimitris,
>
>
>
> Thank you I upvoted your answer on CV as well. But please help me
> understand a few things.
>
>
>
> 1- By D matrix, you mean the G matrix shown in:
> https://bookdown.org/marklhc/notes/simulating-multilevel-data.html#linear-growth-model
>
>
>
> 2- When you say variance components in the output are prior values, can
> you tell me how these prior values are obtained? I guess from the data
> itself, but how exactly (do we run individual models first to see how much
> intercepts and slopes vary & co-vary and take those as prior)?
>
>
>
> 3- Harold above noted that: "The conditional means of the random effects
> are E(Y|X) and hence their variance is only one portion of the total
> variance [i.e., var(y)]." I'm not sure how this directly relates to my
> question in this thread?
>
>
>
> Thank you,
>
> Simon
>
>
>
>
>
>
>
>
>
> On Tue, Sep 8, 2020 at 3:47 PM Harold Doran <
> harold.doran at cambiumassessment.com> wrote:
>
> To add a little notation to this, we can use law of total variance, var(y)
> = E(var(Y|X)) + var(E(Y|X)). The conditional means of the random effects
> are E(Y|X) and hence their variance is only one portion of the total
> variance.
>
> -----Original Message-----
> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On
> Behalf Of D. Rizopoulos
> Sent: Monday, September 7, 2020 11:02 PM
> To: Simon Harmel <sim.harmel at gmail.com>; r-sig-mixed-models <
> r-sig-mixed-models at r-project.org>
> Subject: Re: [R-sig-ME] var(ranef(Random Effect)) not the same as the
> variance component
>
> External email alert: Be wary of links & attachments.
>
>
> Yes, you do not expect these two be the same. The variance components are
> the prior variances of the random effects, whereas var(ranef(model)) is the
> variance of the posterior means/modes of the random effects.
>
> Best,
> Dimitris
>
>
> Dimitris Rizopoulos
> Professor of Biostatistics
> Erasmus University Medical Center
> The Netherlands
> ________________________________
> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on
> behalf of Simon Harmel <sim.harmel at gmail.com>
> Sent: Tuesday, September 8, 2020 1:22:15 AM
> To: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
> Subject: [R-sig-ME] var(ranef(Random Effect)) not the same as the variance
> component
>
> Hello All,
>
> A very basic question. Generally, `var(ranef(Random Effect))` may not
> necessarily be the same as the variance component reported for that Random
> Effect in the model output, correct?
>
>
> Thank you all,
> Simon
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
>
> https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-mixed-models&amp;data=02%7C01%7Cd.rizopoulos%40erasmusmc.nl%7Cae4132330fbd412dea9508d85384efba%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C0%7C637351177705805772&amp;sdata=agUgmCzM5ecsaoGLm8aPX0%2FuHZF1mK%2BXbP%2Fi6KX5UvI%3D&amp;reserved=0
>
>         [[alternative HTML version deleted]]
>
>

	[[alternative HTML version deleted]]


From h@ro|d@dor@n @end|ng |rom c@mb|um@@@e@@ment@com  Wed Sep  9 22:45:21 2020
From: h@ro|d@dor@n @end|ng |rom c@mb|um@@@e@@ment@com (Harold Doran)
Date: Wed, 9 Sep 2020 20:45:21 +0000
Subject: [R-sig-ME] 
 var(ranef(Random Effect)) not the same as the variance component
In-Reply-To: <CACgv6yUGGrKozMZ=dWg9w1WWRtUoYAP34SYGE4-EKasz873RaA@mail.gmail.com>
References: <CACgv6yXiXpH8Tf06wEpXXjHAjwJ1osNL_Vc9JV+yPXqjwkNC7g@mail.gmail.com>
 <AM6PR04MB58794F13F051216D14720112E8290@AM6PR04MB5879.eurprd04.prod.outlook.com>
 <01e3671a7c404618b731479948673303@cambiumassessment.com>
 <CACgv6yXuhy9AkEVj0bOari4BPpp5YYvknMDir8U5=wp0to_X3Q@mail.gmail.com>
 <0bbdd17ba3354b4faffc74f907134024@cambiumassessment.com>
 <CACgv6yUGGrKozMZ=dWg9w1WWRtUoYAP34SYGE4-EKasz873RaA@mail.gmail.com>
Message-ID: <978b60c9e6e447ab8d1cde7b46002a54@cambiumassessment.com>

Simon

Your original question was, ?A very basic question. Generally, `var(ranef(Random Effect))` may not necessarily be the same as the variance component reported for that Random Effect in the model output, correct??

First, let me define terms since there are a lot of variances in this model. There is the ?marginal variance? which is the variance of the random effects reported by lmer. Then there is the ?conditional variance of the random effects?, which is the variance of each individual BLUP, then you can take the ?observed variance? over all BLUPs.

Now, take the simple model used in my example,

fm1 <- lmer(Reaction ~ 1 + (1|Subject), sleepstudy)

So, this term, var(ranef(fm1)$Subject), is the observed variance of the BLUPs.

The marginal variance of the random effects in this model reported by lmer is 35.75^2 and as Dimitris pointed out, you would not expect the observed variance of the conditional means (i.e., the BLUPs) to be equal to the marginal variance of the random effects.

The random effects are conditional means, E(Y|X) and they have a conditional variance.  So:

E(var(Y|X)) = mean(sapply(attr(ranef(fm1)$Subject, "postVar"),function(x) x))

var(E(Y|X) = observed variance of the BLUPs

Combined, their sum is the marginal variance of the random effects. So, concretely, we observe that

> sqrt(var(ranef(fm1)$Subject) + mean(sapply(attr(ranef(fm1, condVar=TRUE)[[1]], "postVar"),function(x) x)))
            (Intercept)
(Intercept)    35.75385

Which is the same as the marginal SD of the random effects from lmer.

Hope this helps.

BTW, note to Ben Bolker (if he even read this far ? ), I was originally the one who suggested to Doug that there be an extractor called ?postVar?. He regretted that term and expressed that to me many times (now I understand why). But, I had to remember how to get the conditional variances of the random effects and the code above is what I came up with since they are an attribute of the ranef extractor. That code is a bit ?ugly?, so either I?m doing it wrong (big possibility) or might I suggest a new extractor function can be written that is easier to get those?

From: Simon Harmel <sim.harmel at gmail.com>
Sent: Wednesday, September 9, 2020 4:19 PM
To: Harold Doran <harold.doran at cambiumassessment.com>
Cc: D. Rizopoulos <d.rizopoulos at erasmusmc.nl>; r-sig-mixed-models <r-sig-mixed-models at r-project.org>; Ben Bolker <bbolker at gmail.com>
Subject: Re: [R-sig-ME] var(ranef(Random Effect)) not the same as the variance component

Thank you, Harold.

1) `var(ranef(fm1)$Subject)` is the posterior variance of random effects or their prior variance? (this was the point discussed so far in this thread)

2) Also, what about `mean(sapply(attr(ranef(fm1)$Subject, "postVar"),function(x) x))`, what this expected variance represents in words?

3) What does their sum represent? The total observed variance in random deviations in intercepts of subjects?

Thank you very much

On Wed, Sep 9, 2020 at 1:48 PM Harold Doran <harold.doran at cambiumassessment.com<mailto:harold.doran at cambiumassessment.com>> wrote:
Simon

Here is an example to show what my notation implies with respect to your question:

fm1 <- lmer(Reaction ~ 1 + (1|Subject), sleepstudy)

sqrt(var(ranef(fm1)$Subject) + mean(sapply(attr(ranef(fm1, condVar=TRUE)[[1]], "postVar"),function(x) x)))

From: Simon Harmel <sim.harmel at gmail.com<mailto:sim.harmel at gmail.com>>
Sent: Wednesday, September 9, 2020 11:53 AM
To: D. Rizopoulos <d.rizopoulos at erasmusmc.nl<mailto:d.rizopoulos at erasmusmc.nl>>
Cc: r-sig-mixed-models <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>; Harold Doran <harold.doran at cambiumassessment.com<mailto:harold.doran at cambiumassessment.com>>; Ben Bolker <bbolker at gmail.com<mailto:bbolker at gmail.com>>
Subject: Re: [R-sig-ME] var(ranef(Random Effect)) not the same as the variance component

First of all, thank you all for your valuable input.

Dimitris,

Thank you I upvoted your answer on CV as well. But please help me understand a few things.

1- By D matrix, you mean the G matrix shown in: https://bookdown.org/marklhc/notes/simulating-multilevel-data.html#linear-growth-model

2- When you say variance components in the output are prior values, can you tell me how these prior values are obtained? I guess from the data itself, but how exactly (do we run individual models first to see how much intercepts and slopes vary & co-vary and take those as prior)?

3- Harold above noted that: "The conditional means of the random effects are E(Y|X) and hence their variance is only one portion of the total variance [i.e., var(y)]." I'm not sure how this directly relates to my question in this thread?

Thank you,
Simon




On Tue, Sep 8, 2020 at 3:47 PM Harold Doran <harold.doran at cambiumassessment.com<mailto:harold.doran at cambiumassessment.com>> wrote:
To add a little notation to this, we can use law of total variance, var(y) = E(var(Y|X)) + var(E(Y|X)). The conditional means of the random effects are E(Y|X) and hence their variance is only one portion of the total variance.

-----Original Message-----
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org<mailto:r-sig-mixed-models-bounces at r-project.org>> On Behalf Of D. Rizopoulos
Sent: Monday, September 7, 2020 11:02 PM
To: Simon Harmel <sim.harmel at gmail.com<mailto:sim.harmel at gmail.com>>; r-sig-mixed-models <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
Subject: Re: [R-sig-ME] var(ranef(Random Effect)) not the same as the variance component

External email alert: Be wary of links & attachments.


Yes, you do not expect these two be the same. The variance components are the prior variances of the random effects, whereas var(ranef(model)) is the variance of the posterior means/modes of the random effects.

Best,
Dimitris


Dimitris Rizopoulos
Professor of Biostatistics
Erasmus University Medical Center
The Netherlands
________________________________
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org<mailto:r-sig-mixed-models-bounces at r-project.org>> on behalf of Simon Harmel <sim.harmel at gmail.com<mailto:sim.harmel at gmail.com>>
Sent: Tuesday, September 8, 2020 1:22:15 AM
To: r-sig-mixed-models <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
Subject: [R-sig-ME] var(ranef(Random Effect)) not the same as the variance component

Hello All,

A very basic question. Generally, `var(ranef(Random Effect))` may not necessarily be the same as the variance component reported for that Random Effect in the model output, correct?


Thank you all,
Simon

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-mixed-models&amp;data=02%7C01%7Cd.rizopoulos%40erasmusmc.nl%7Cae4132330fbd412dea9508d85384efba%7C526638ba6af34b0fa532a1a511f4ac80%7C0%7C0%7C637351177705805772&amp;sdata=agUgmCzM5ecsaoGLm8aPX0%2FuHZF1mK%2BXbP%2Fi6KX5UvI%3D&amp;reserved=0

        [[alternative HTML version deleted]]

	[[alternative HTML version deleted]]


From d@e@kornbrot @end|ng |rom hert@@@c@uk  Thu Sep 10 09:50:55 2020
From: d@e@kornbrot @end|ng |rom hert@@@c@uk (Kornbrot, Diana)
Date: Thu, 10 Sep 2020 07:50:55 +0000
Subject: [R-sig-ME] Bayes generalized linear binomial, logit
Message-ID: <062DBF99-3602-4DEA-9A0E-EBDBBC7C2723@herts.ac.uk>

Need Bayes generalized linear binomial, logit  on Mac running Catalina
Brms won?t compile Rstan: any suggestions?
Blme: bglmer, blmer run, but do not give posterior distributions or Bayes factor: any suggestions?
Any suggestions fro alternative Bayes packages
Help gratefully received
best
Diana
____________
University of Hertfordshire
College Lane, Hatfield, Hertfordshire AL10 9AB, UK
+44 (0) 208 444 2081
+44 (0) 7403 18 16 12
d.e.kornbrot at herts.ac.uk<mailto:d.e.kornbrot at herts.ac.uk>
http://dianakornbrot.wordpress.com/
skype:  kornbrotme
Save our in-boxes! http://emailcharter.org
 __________________








	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Thu Sep 10 09:58:07 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Thu, 10 Sep 2020 09:58:07 +0200
Subject: [R-sig-ME] Bayes generalized linear binomial, logit
In-Reply-To: <062DBF99-3602-4DEA-9A0E-EBDBBC7C2723@herts.ac.uk>
References: <062DBF99-3602-4DEA-9A0E-EBDBBC7C2723@herts.ac.uk>
Message-ID: <CAJuCY5xj4Tn0JHghkTG3zVAb6rEEgeKZoufgWcFiecEASYi2BQ@mail.gmail.com>

INLA
inlabru
MCMCglmm

https://cran.r-project.org/web/views/Bayesian.html

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op do 10 sep. 2020 om 09:51 schreef Kornbrot, Diana <
d.e.kornbrot at herts.ac.uk>:

> Need Bayes generalized linear binomial, logit  on Mac running Catalina
> Brms won?t compile Rstan: any suggestions?
> Blme: bglmer, blmer run, but do not give posterior distributions or Bayes
> factor: any suggestions?
> Any suggestions fro alternative Bayes packages
> Help gratefully received
> best
> Diana
> ____________
> University of Hertfordshire
> College Lane, Hatfield, Hertfordshire AL10 9AB, UK
> +44 (0) 208 444 2081
> +44 (0) 7403 18 16 12
> d.e.kornbrot at herts.ac.uk<mailto:d.e.kornbrot at herts.ac.uk>
> http://dianakornbrot.wordpress.com/
> skype:  kornbrotme
> Save our in-boxes! http://emailcharter.org
>  __________________
>
>
>
>
>
>
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From tor@ten@h@u||e @end|ng |rom gm@||@com  Thu Sep 10 10:11:01 2020
From: tor@ten@h@u||e @end|ng |rom gm@||@com (Torsten Hauffe)
Date: Thu, 10 Sep 2020 10:11:01 +0200
Subject: [R-sig-ME] Bayes generalized linear binomial, logit
In-Reply-To: <CAJuCY5xj4Tn0JHghkTG3zVAb6rEEgeKZoufgWcFiecEASYi2BQ@mail.gmail.com>
References: <062DBF99-3602-4DEA-9A0E-EBDBBC7C2723@herts.ac.uk>
 <CAJuCY5xj4Tn0JHghkTG3zVAb6rEEgeKZoufgWcFiecEASYi2BQ@mail.gmail.com>
Message-ID: <CAGCrCxYVmzYF5upaGXSasQYUr=XdCch-iT9XJRbRk0eMaFe+cw@mail.gmail.com>

What went wrong when following this manual for installing Rstan?

https://github.com/stan-dev/rstan/wiki/Installing-RStan-from-source-on-a-Mac

On Thu, Sep 10, 2020 at 10:02 AM Thierry Onkelinx via R-sig-mixed-models <
r-sig-mixed-models at r-project.org> wrote:

> INLA
> inlabru
> MCMCglmm
>
> https://cran.r-project.org/web/views/Bayesian.html
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op do 10 sep. 2020 om 09:51 schreef Kornbrot, Diana <
> d.e.kornbrot at herts.ac.uk>:
>
> > Need Bayes generalized linear binomial, logit  on Mac running Catalina
> > Brms won?t compile Rstan: any suggestions?
> > Blme: bglmer, blmer run, but do not give posterior distributions or Bayes
> > factor: any suggestions?
> > Any suggestions fro alternative Bayes packages
> > Help gratefully received
> > best
> > Diana
> > ____________
> > University of Hertfordshire
> > College Lane, Hatfield, Hertfordshire AL10 9AB, UK
> > +44 (0) 208 444 2081
> > +44 (0) 7403 18 16 12
> > d.e.kornbrot at herts.ac.uk<mailto:d.e.kornbrot at herts.ac.uk>
> > http://dianakornbrot.wordpress.com/
> > skype:  kornbrotme
> > Save our in-boxes! http://emailcharter.org
> >  __________________
> >
> >
> >
> >
> >
> >
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From @ndrew@r@john@on @end|ng |rom po@tgr@d@curt|n@edu@@u  Fri Sep 11 06:52:01 2020
From: @ndrew@r@john@on @end|ng |rom po@tgr@d@curt|n@edu@@u (Andrew Johnson)
Date: Fri, 11 Sep 2020 04:52:01 +0000
Subject: [R-sig-ME] Bayes generalized linear binomial, logit
In-Reply-To: <062DBF99-3602-4DEA-9A0E-EBDBBC7C2723@herts.ac.uk>
References: <062DBF99-3602-4DEA-9A0E-EBDBBC7C2723@herts.ac.uk>
Message-ID: <PSBPR01MB3703A37F8AD631648E262705A3240@PSBPR01MB3703.apcprd01.prod.exchangelabs.com>

Hi Diana,

If you post your errors/problems over on the Stan forums (discourse.mc-stan.org) we can help troubleshoot your installation. Catalina has been causing issues with Stan, but nothing we can't sort out.

Cheers,
Andrew

Dr Andrew Johnson
BPsych(Hons), MBiostat, PhD, GStat.
Research Associate | School of Psychology

Curtin University 
Email | andrew.johnson at curtin.edu.au 
Web | www.curtin.edu.au 



CRICOS Provider Code 00301J 


-----Original Message-----
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of Kornbrot, Diana
Sent: Thursday, 10 September 2020 3:51 PM
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Bayes generalized linear binomial, logit

Need Bayes generalized linear binomial, logit  on Mac running Catalina Brms won?t compile Rstan: any suggestions?
Blme: bglmer, blmer run, but do not give posterior distributions or Bayes factor: any suggestions?
Any suggestions fro alternative Bayes packages Help gratefully received best Diana ____________ University of Hertfordshire College Lane, Hatfield, Hertfordshire AL10 9AB, UK
+44 (0) 208 444 2081
+44 (0) 7403 18 16 12
d.e.kornbrot at herts.ac.uk<mailto:d.e.kornbrot at herts.ac.uk>
http://dianakornbrot.wordpress.com/
skype:  kornbrotme
Save our in-boxes! http://emailcharter.org  __________________








	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From @|m@h@rme| @end|ng |rom gm@||@com  Sat Sep 12 07:45:40 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Sat, 12 Sep 2020 00:45:40 -0500
Subject: [R-sig-ME] cross-level interaction in lme4
Message-ID: <CACgv6yVXFrP=3YhB-kjKhhrE4KGWcxLkaL6Hssgy5PZWob+dDw@mail.gmail.com>

Dear All,

In the model below, I have a cross-level interaction (SES of students x
MEAN SES of schools). However, MEAN SES of schools is made from SES of
students.

So, my question is: Is such an interaction: appropriate, meaningful, and
legitimate?

hsb <- read.csv('
https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')

library(lme4)
m1 <- lmer(math ~ ses * meanses + (1|sch.id), data = hsb)

Thank you,
Simon

	[[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Mon Sep 14 01:37:38 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Sun, 13 Sep 2020 18:37:38 -0500
Subject: [R-sig-ME] Standard Error of a coef. in a 2-level model vs. 2 OLS
 models
Message-ID: <CACgv6yU4cEznQyR_BA3HCMfRTTX9fce7R11jEaMThMOaj1UhdA@mail.gmail.com>

Dear All,

I have fit two ols models (ols1 & ols2) and an mixed-effects model (m1).
ols1 is a simple lm() model that ignores the second-level. ols2 is a simple
lm() model that ignores the first-level.

For `ols1` model,  `sigma(ols1)^2` almost equals sum of variance components
in the `m1` model: 6.68 (bet.) + 39.15 (with.)
For `ols2` model, I wonder what does `sigma(ols2)^2` represents when
compared to the `m1` model?

Here is the fully reproducible code:

library(lme4)
library(tidyverse)

hsb <- read.csv('
https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')
hsb_ave <- hsb %>% group_by(sch.id) %>% mutate(math_ave = mean(math)) %>%
slice(1) # data that only considers grouping but ignores lower level

ols1 <- lm(math ~ sector, data = hsb)
summary(ols1)

m1 <- lmer(math ~ sector + (1|sch.id), data = hsb)
summary(m1)

# `sigma(ols1)^2` almost equals 6.68 (bet.) + 39.15 (with.) from lmer

But if I fit another ols model that only considers the grouping structure
(ignoring lower level):

ols2 <- lm(math_ave ~ sector, data = hsb_ave)
summary(ols2)

Then what does `sigma(ols2)^2` should amount to when compared to the `m1`
model?

	[[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Mon Sep 14 01:50:38 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Sun, 13 Sep 2020 18:50:38 -0500
Subject: [R-sig-ME] 
 Standard Error of a coef. in a 2-level model vs. 2 OLS models
In-Reply-To: <CACgv6yU4cEznQyR_BA3HCMfRTTX9fce7R11jEaMThMOaj1UhdA@mail.gmail.com>
References: <CACgv6yU4cEznQyR_BA3HCMfRTTX9fce7R11jEaMThMOaj1UhdA@mail.gmail.com>
Message-ID: <CACgv6yUZpT_8kMVjrEWaxHhWPm_bRi+Jz7N4cwDFhb+a0A-eDQ@mail.gmail.com>

Just a clarification.

For `ols1` model, I can approximate its SE of the sector coefficient by
using the within and between variance components from the HLM model:
sqrt(( 6.68  + 39.15  )/45)/(160*.25))

BUT  For `ols2` model, how can I approximate its SE of the sector
coefficient by using the within and between variance components from the
HLM model?

On Sun, Sep 13, 2020 at 6:37 PM Simon Harmel <sim.harmel at gmail.com> wrote:

> Dear All,
>
> I have fit two ols models (ols1 & ols2) and an mixed-effects model (m1).
> ols1 is a simple lm() model that ignores the second-level. ols2 is a simple
> lm() model that ignores the first-level.
>
> For `ols1` model,  `sigma(ols1)^2` almost equals sum of variance
> components in the `m1` model: 6.68 (bet.) + 39.15 (with.)
> For `ols2` model, I wonder what does `sigma(ols2)^2` represents when
> compared to the `m1` model?
>
> Here is the fully reproducible code:
>
> library(lme4)
> library(tidyverse)
>
> hsb <- read.csv('
> https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')
> hsb_ave <- hsb %>% group_by(sch.id) %>% mutate(math_ave = mean(math)) %>%
> slice(1) # data that only considers grouping but ignores lower level
>
> ols1 <- lm(math ~ sector, data = hsb)
> summary(ols1)
>
> m1 <- lmer(math ~ sector + (1|sch.id), data = hsb)
> summary(m1)
>
> # `sigma(ols1)^2` almost equals 6.68 (bet.) + 39.15 (with.) from lmer
>
> But if I fit another ols model that only considers the grouping structure
> (ignoring lower level):
>
> ols2 <- lm(math_ave ~ sector, data = hsb_ave)
> summary(ols2)
>
> Then what does `sigma(ols2)^2` should amount to when compared to the `m1`
> model?
>

	[[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Wed Sep 16 18:41:19 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Wed, 16 Sep 2020 11:41:19 -0500
Subject: [R-sig-ME] comparison bet. mixed and ols model (using lme4)
Message-ID: <CACgv6yUTJYGLEY4f-vUJ2z1LkkNUBa7awJmng_=m_1ri3h80LQ@mail.gmail.com>

Hello all,

In my regular, non-mixed-effects model below (`ols2`), I have just used the
average of each cluster as DV and used a binary cluster-level predictor
(sector) in the model.

# Questions: Should the amount of total variation in `ols2` model equal the
between-cluster variation (tau_00) in a corresponding mixed-effects model
(`m1`)?

Thank you, Simon

library(lme4)
library(tidyverse)

hsb <- read.csv('
https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')
hsb_ave <- hsb %>% group_by(sch.id) %>% mutate(math_ave = mean(math)) %>%
slice(1)

ols2 <- lm(math_ave ~ sector, data = hsb_ave)

m1 <- lmer(math ~ sector + (1|sch.id), data = hsb)

	[[alternative HTML version deleted]]


From h@ro|d@dor@n @end|ng |rom c@mb|um@@@e@@ment@com  Wed Sep 16 23:45:27 2020
From: h@ro|d@dor@n @end|ng |rom c@mb|um@@@e@@ment@com (Harold Doran)
Date: Wed, 16 Sep 2020 21:45:27 +0000
Subject: [R-sig-ME] 
 Standard Error of a coef. in a 2-level model vs. 2 OLS models
In-Reply-To: <CACgv6yUZpT_8kMVjrEWaxHhWPm_bRi+Jz7N4cwDFhb+a0A-eDQ@mail.gmail.com>
References: <CACgv6yU4cEznQyR_BA3HCMfRTTX9fce7R11jEaMThMOaj1UhdA@mail.gmail.com>
 <CACgv6yUZpT_8kMVjrEWaxHhWPm_bRi+Jz7N4cwDFhb+a0A-eDQ@mail.gmail.com>
Message-ID: <83ba71b6f01f42b2adc245e073d8c301@cambiumassessment.com>

This is not how standard errors are computed for linear or mixed linear models. I'm not sure what you're goal is, but the SEs are the square roots of the diagonal elements of the variance/covariance matrix of the fixed effects.

See ?vcov on how to extract that matrix.

-----Original Message-----
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of Simon Harmel
Sent: Sunday, September 13, 2020 7:51 PM
To: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Standard Error of a coef. in a 2-level model vs. 2 OLS models

External email alert: Be wary of links & attachments.


Just a clarification.

For `ols1` model, I can approximate its SE of the sector coefficient by using the within and between variance components from the HLM model:
sqrt(( 6.68  + 39.15  )/45)/(160*.25))

BUT  For `ols2` model, how can I approximate its SE of the sector coefficient by using the within and between variance components from the HLM model?

On Sun, Sep 13, 2020 at 6:37 PM Simon Harmel <sim.harmel at gmail.com> wrote:

> Dear All,
>
> I have fit two ols models (ols1 & ols2) and an mixed-effects model (m1).
> ols1 is a simple lm() model that ignores the second-level. ols2 is a 
> simple
> lm() model that ignores the first-level.
>
> For `ols1` model,  `sigma(ols1)^2` almost equals sum of variance 
> components in the `m1` model: 6.68 (bet.) + 39.15 (with.) For `ols2` 
> model, I wonder what does `sigma(ols2)^2` represents when compared to 
> the `m1` model?
>
> Here is the fully reproducible code:
>
> library(lme4)
> library(tidyverse)
>
> hsb <- read.csv('
> https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')
> hsb_ave <- hsb %>% group_by(sch.id) %>% mutate(math_ave = mean(math)) 
> %>%
> slice(1) # data that only considers grouping but ignores lower level
>
> ols1 <- lm(math ~ sector, data = hsb)
> summary(ols1)
>
> m1 <- lmer(math ~ sector + (1|sch.id), data = hsb)
> summary(m1)
>
> # `sigma(ols1)^2` almost equals 6.68 (bet.) + 39.15 (with.) from lmer
>
> But if I fit another ols model that only considers the grouping 
> structure (ignoring lower level):
>
> ols2 <- lm(math_ave ~ sector, data = hsb_ave)
> summary(ols2)
>
> Then what does `sigma(ols2)^2` should amount to when compared to the 
> `m1` model?
>

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From @|m@h@rme| @end|ng |rom gm@||@com  Wed Sep 16 23:53:59 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Wed, 16 Sep 2020 16:53:59 -0500
Subject: [R-sig-ME] 
 Standard Error of a coef. in a 2-level model vs. 2 OLS models
In-Reply-To: <83ba71b6f01f42b2adc245e073d8c301@cambiumassessment.com>
References: <CACgv6yU4cEznQyR_BA3HCMfRTTX9fce7R11jEaMThMOaj1UhdA@mail.gmail.com>
 <CACgv6yUZpT_8kMVjrEWaxHhWPm_bRi+Jz7N4cwDFhb+a0A-eDQ@mail.gmail.com>
 <83ba71b6f01f42b2adc245e073d8c301@cambiumassessment.com>
Message-ID: <CACgv6yWYwhL0_8_oyv8E_SXm+psXhRXHK8YisGF+pfumjUCBhw@mail.gmail.com>

Hi Harold,

I improved my question, and asked it on CrossValidated: (
https://stats.stackexchange.com/questions/487363/using-variance-components-of-a-mixed-model-to-obtain-std-error-of-a-coef-from-a
)

I would appreciate your answer, either here or on CrossValidated.

Many thanks, Simon

On Wed, Sep 16, 2020 at 4:45 PM Harold Doran <
harold.doran at cambiumassessment.com> wrote:

> This is not how standard errors are computed for linear or mixed linear
> models. I'm not sure what you're goal is, but the SEs are the square roots
> of the diagonal elements of the variance/covariance matrix of the fixed
> effects.
>
> See ?vcov on how to extract that matrix.
>
> -----Original Message-----
> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On
> Behalf Of Simon Harmel
> Sent: Sunday, September 13, 2020 7:51 PM
> To: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
> Subject: Re: [R-sig-ME] Standard Error of a coef. in a 2-level model vs. 2
> OLS models
>
> External email alert: Be wary of links & attachments.
>
>
> Just a clarification.
>
> For `ols1` model, I can approximate its SE of the sector coefficient by
> using the within and between variance components from the HLM model:
> sqrt(( 6.68  + 39.15  )/45)/(160*.25))
>
> BUT  For `ols2` model, how can I approximate its SE of the sector
> coefficient by using the within and between variance components from the
> HLM model?
>
> On Sun, Sep 13, 2020 at 6:37 PM Simon Harmel <sim.harmel at gmail.com> wrote:
>
> > Dear All,
> >
> > I have fit two ols models (ols1 & ols2) and an mixed-effects model (m1).
> > ols1 is a simple lm() model that ignores the second-level. ols2 is a
> > simple
> > lm() model that ignores the first-level.
> >
> > For `ols1` model,  `sigma(ols1)^2` almost equals sum of variance
> > components in the `m1` model: 6.68 (bet.) + 39.15 (with.) For `ols2`
> > model, I wonder what does `sigma(ols2)^2` represents when compared to
> > the `m1` model?
> >
> > Here is the fully reproducible code:
> >
> > library(lme4)
> > library(tidyverse)
> >
> > hsb <- read.csv('
> > https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')
> > hsb_ave <- hsb %>% group_by(sch.id) %>% mutate(math_ave = mean(math))
> > %>%
> > slice(1) # data that only considers grouping but ignores lower level
> >
> > ols1 <- lm(math ~ sector, data = hsb)
> > summary(ols1)
> >
> > m1 <- lmer(math ~ sector + (1|sch.id), data = hsb)
> > summary(m1)
> >
> > # `sigma(ols1)^2` almost equals 6.68 (bet.) + 39.15 (with.) from lmer
> >
> > But if I fit another ols model that only considers the grouping
> > structure (ignoring lower level):
> >
> > ols2 <- lm(math_ave ~ sector, data = hsb_ave)
> > summary(ols2)
> >
> > Then what does `sigma(ols2)^2` should amount to when compared to the
> > `m1` model?
> >
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>

	[[alternative HTML version deleted]]


From h@ro|d@dor@n @end|ng |rom c@mb|um@@@e@@ment@com  Thu Sep 17 18:16:33 2020
From: h@ro|d@dor@n @end|ng |rom c@mb|um@@@e@@ment@com (Harold Doran)
Date: Thu, 17 Sep 2020 16:16:33 +0000
Subject: [R-sig-ME] 
 Standard Error of a coef. in a 2-level model vs. 2 OLS models
In-Reply-To: <CACgv6yWYwhL0_8_oyv8E_SXm+psXhRXHK8YisGF+pfumjUCBhw@mail.gmail.com>
References: <CACgv6yU4cEznQyR_BA3HCMfRTTX9fce7R11jEaMThMOaj1UhdA@mail.gmail.com>
 <CACgv6yUZpT_8kMVjrEWaxHhWPm_bRi+Jz7N4cwDFhb+a0A-eDQ@mail.gmail.com>
 <83ba71b6f01f42b2adc245e073d8c301@cambiumassessment.com>
 <CACgv6yWYwhL0_8_oyv8E_SXm+psXhRXHK8YisGF+pfumjUCBhw@mail.gmail.com>
Message-ID: <b8ef4f502d6b466d867deaf3f40ed984@cambiumassessment.com>

Simon

Crossposting like this is frowned on a bit, but I?ve been in your shoes trying to get an answer before. I think you might be a bit confused. I saw your questions in both places and you?re asking how to get the standard errors of the OLS model using by fitting a mixed model and using the variance components from that mixed model to get the standard errors from an OLS model.

This is what we refer to in statistics as ?bass-ackwards?. ?

If you want the standard errors of the fixed effects from an OLS model, compute them as follows:

ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
X <- model.matrix(lm.D9)

### Compute Standard errors of fixed effects
sqrt(diag(solve(crossprod(X)) * .6964^2))

### use built in extractor function to get them instead
sqrt(diag(vcov(lm.D9)))

Similarly, get the standard errors of the mixed model from its own variance/covariance matrix.


From: Simon Harmel <sim.harmel at gmail.com>
Sent: Wednesday, September 16, 2020 5:54 PM
To: Harold Doran <harold.doran at cambiumassessment.com>
Cc: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Standard Error of a coef. in a 2-level model vs. 2 OLS models

Hi Harold,

I improved my question, and asked it on CrossValidated: (https://stats.stackexchange.com/questions/487363/using-variance-components-of-a-mixed-model-to-obtain-std-error-of-a-coef-from-a)

I would appreciate your answer, either here or on CrossValidated.

Many thanks, Simon

On Wed, Sep 16, 2020 at 4:45 PM Harold Doran <harold.doran at cambiumassessment.com<mailto:harold.doran at cambiumassessment.com>> wrote:
This is not how standard errors are computed for linear or mixed linear models. I'm not sure what you're goal is, but the SEs are the square roots of the diagonal elements of the variance/covariance matrix of the fixed effects.

See ?vcov on how to extract that matrix.

-----Original Message-----
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org<mailto:r-sig-mixed-models-bounces at r-project.org>> On Behalf Of Simon Harmel
Sent: Sunday, September 13, 2020 7:51 PM
To: r-sig-mixed-models <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
Subject: Re: [R-sig-ME] Standard Error of a coef. in a 2-level model vs. 2 OLS models

External email alert: Be wary of links & attachments.


Just a clarification.

For `ols1` model, I can approximate its SE of the sector coefficient by using the within and between variance components from the HLM model:
sqrt(( 6.68  + 39.15  )/45)/(160*.25))

BUT  For `ols2` model, how can I approximate its SE of the sector coefficient by using the within and between variance components from the HLM model?

On Sun, Sep 13, 2020 at 6:37 PM Simon Harmel <sim.harmel at gmail.com<mailto:sim.harmel at gmail.com>> wrote:

> Dear All,
>
> I have fit two ols models (ols1 & ols2) and an mixed-effects model (m1).
> ols1 is a simple lm() model that ignores the second-level. ols2 is a
> simple
> lm() model that ignores the first-level.
>
> For `ols1` model,  `sigma(ols1)^2` almost equals sum of variance
> components in the `m1` model: 6.68 (bet.) + 39.15 (with.) For `ols2`
> model, I wonder what does `sigma(ols2)^2` represents when compared to
> the `m1` model?
>
> Here is the fully reproducible code:
>
> library(lme4)
> library(tidyverse)
>
> hsb <- read.csv('
> https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')
> hsb_ave <- hsb %>% group_by(sch.id<http://sch.id>) %>% mutate(math_ave = mean(math))
> %>%
> slice(1) # data that only considers grouping but ignores lower level
>
> ols1 <- lm(math ~ sector, data = hsb)
> summary(ols1)
>
> m1 <- lmer(math ~ sector + (1|sch.id<http://sch.id>), data = hsb)
> summary(m1)
>
> # `sigma(ols1)^2` almost equals 6.68 (bet.) + 39.15 (with.) from lmer
>
> But if I fit another ols model that only considers the grouping
> structure (ignoring lower level):
>
> ols2 <- lm(math_ave ~ sector, data = hsb_ave)
> summary(ols2)
>
> Then what does `sigma(ols2)^2` should amount to when compared to the
> `m1` model?
>

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From @|m@h@rme| @end|ng |rom gm@||@com  Thu Sep 17 19:15:18 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Thu, 17 Sep 2020 12:15:18 -0500
Subject: [R-sig-ME] 
 Standard Error of a coef. in a 2-level model vs. 2 OLS models
In-Reply-To: <b8ef4f502d6b466d867deaf3f40ed984@cambiumassessment.com>
References: <CACgv6yU4cEznQyR_BA3HCMfRTTX9fce7R11jEaMThMOaj1UhdA@mail.gmail.com>
 <CACgv6yUZpT_8kMVjrEWaxHhWPm_bRi+Jz7N4cwDFhb+a0A-eDQ@mail.gmail.com>
 <83ba71b6f01f42b2adc245e073d8c301@cambiumassessment.com>
 <CACgv6yWYwhL0_8_oyv8E_SXm+psXhRXHK8YisGF+pfumjUCBhw@mail.gmail.com>
 <b8ef4f502d6b466d867deaf3f40ed984@cambiumassessment.com>
Message-ID: <CACgv6yUwTC+-E4Tiq7femG-h+3yhV+wp4N-7s05gyngf+AWhMQ@mail.gmail.com>

Thanks Harold! I may have been insufficiently clear.

At its core, the question is asking: "Is the "between-cluster" variation
(i.e., ?00) in a two-level nested mixed-effects model (i.e., `m1`), the
same as the amount of "total variation" that we would otherwise get if we
only use the cluster-level data in a corresponding, NON-HLM regression
model (i.e., `ols2`)?"


library(lme4)
library(tidyverse)

hsb <- read.csv('
https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')
hsb_ave <- hsb %>% group_by(sch.id) %>% mutate(math_ave = mean(math)) %>%
slice(1)

ols2 <- lm(math_ave ~ sector, data = hsb_ave)

m1 <- lmer(math ~ sector + (1|sch.id), data = hsb)

On Thu, Sep 17, 2020 at 11:16 AM Harold Doran <
harold.doran at cambiumassessment.com> wrote:

> Simon
>
>
>
> Crossposting like this is frowned on a bit, but I?ve been in your shoes
> trying to get an answer before. I think you might be a bit confused. I saw
> your questions in both places and you?re asking how to get the standard
> errors of the OLS model using by fitting a mixed model and using the
> variance components from that mixed model to get the standard errors from
> an OLS model.
>
>
>
> This is what we refer to in statistics as ?bass-ackwards?. ?
>
>
>
> If you want the standard errors of the fixed effects from an OLS model,
> compute them as follows:
>
>
>
> ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
>
> trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
>
> group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
>
> weight <- c(ctl, trt)
>
> lm.D9 <- lm(weight ~ group)
>
> X <- model.matrix(lm.D9)
>
>
>
> ### Compute Standard errors of fixed effects
>
> sqrt(diag(solve(crossprod(X)) * .6964^2))
>
>
>
> ### use built in extractor function to get them instead
>
> sqrt(diag(vcov(lm.D9)))
>
>
>
> Similarly, get the standard errors of the mixed model from its own
> variance/covariance matrix.
>
>
>
>
>
> *From:* Simon Harmel <sim.harmel at gmail.com>
> *Sent:* Wednesday, September 16, 2020 5:54 PM
> *To:* Harold Doran <harold.doran at cambiumassessment.com>
> *Cc:* r-sig-mixed-models <r-sig-mixed-models at r-project.org>
> *Subject:* Re: [R-sig-ME] Standard Error of a coef. in a 2-level model
> vs. 2 OLS models
>
>
>
> Hi Harold,
>
>
>
> I improved my question, and asked it on CrossValidated: (
> https://stats.stackexchange.com/questions/487363/using-variance-components-of-a-mixed-model-to-obtain-std-error-of-a-coef-from-a
> )
>
>
>
> I would appreciate your answer, either here or on CrossValidated.
>
>
>
> Many thanks, Simon
>
>
>
> On Wed, Sep 16, 2020 at 4:45 PM Harold Doran <
> harold.doran at cambiumassessment.com> wrote:
>
> This is not how standard errors are computed for linear or mixed linear
> models. I'm not sure what you're goal is, but the SEs are the square roots
> of the diagonal elements of the variance/covariance matrix of the fixed
> effects.
>
> See ?vcov on how to extract that matrix.
>
> -----Original Message-----
> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On
> Behalf Of Simon Harmel
> Sent: Sunday, September 13, 2020 7:51 PM
> To: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
> Subject: Re: [R-sig-ME] Standard Error of a coef. in a 2-level model vs. 2
> OLS models
>
> External email alert: Be wary of links & attachments.
>
>
> Just a clarification.
>
> For `ols1` model, I can approximate its SE of the sector coefficient by
> using the within and between variance components from the HLM model:
> sqrt(( 6.68  + 39.15  )/45)/(160*.25))
>
> BUT  For `ols2` model, how can I approximate its SE of the sector
> coefficient by using the within and between variance components from the
> HLM model?
>
> On Sun, Sep 13, 2020 at 6:37 PM Simon Harmel <sim.harmel at gmail.com> wrote:
>
> > Dear All,
> >
> > I have fit two ols models (ols1 & ols2) and an mixed-effects model (m1).
> > ols1 is a simple lm() model that ignores the second-level. ols2 is a
> > simple
> > lm() model that ignores the first-level.
> >
> > For `ols1` model,  `sigma(ols1)^2` almost equals sum of variance
> > components in the `m1` model: 6.68 (bet.) + 39.15 (with.) For `ols2`
> > model, I wonder what does `sigma(ols2)^2` represents when compared to
> > the `m1` model?
> >
> > Here is the fully reproducible code:
> >
> > library(lme4)
> > library(tidyverse)
> >
> > hsb <- read.csv('
> > https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')
> > hsb_ave <- hsb %>% group_by(sch.id) %>% mutate(math_ave = mean(math))
> > %>%
> > slice(1) # data that only considers grouping but ignores lower level
> >
> > ols1 <- lm(math ~ sector, data = hsb)
> > summary(ols1)
> >
> > m1 <- lmer(math ~ sector + (1|sch.id), data = hsb)
> > summary(m1)
> >
> > # `sigma(ols1)^2` almost equals 6.68 (bet.) + 39.15 (with.) from lmer
> >
> > But if I fit another ols model that only considers the grouping
> > structure (ignoring lower level):
> >
> > ols2 <- lm(math_ave ~ sector, data = hsb_ave)
> > summary(ols2)
> >
> > Then what does `sigma(ols2)^2` should amount to when compared to the
> > `m1` model?
> >
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>

	[[alternative HTML version deleted]]


From ph||||p@@|d@y @end|ng |rom mp|@n|  Thu Sep 17 19:23:46 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Thu, 17 Sep 2020 19:23:46 +0200
Subject: [R-sig-ME] 
 Standard Error of a coef. in a 2-level model vs. 2 OLS models
In-Reply-To: <CACgv6yUwTC+-E4Tiq7femG-h+3yhV+wp4N-7s05gyngf+AWhMQ@mail.gmail.com>
References: <CACgv6yU4cEznQyR_BA3HCMfRTTX9fce7R11jEaMThMOaj1UhdA@mail.gmail.com>
 <CACgv6yUZpT_8kMVjrEWaxHhWPm_bRi+Jz7N4cwDFhb+a0A-eDQ@mail.gmail.com>
 <83ba71b6f01f42b2adc245e073d8c301@cambiumassessment.com>
 <CACgv6yWYwhL0_8_oyv8E_SXm+psXhRXHK8YisGF+pfumjUCBhw@mail.gmail.com>
 <b8ef4f502d6b466d867deaf3f40ed984@cambiumassessment.com>
 <CACgv6yUwTC+-E4Tiq7femG-h+3yhV+wp4N-7s05gyngf+AWhMQ@mail.gmail.com>
Message-ID: <e003752c-30f1-0310-99a6-a611bd787b34@mpi.nl>

The answer is no.

Consider the case where the between-cluster variation goes to zero (i.e.
a singular fit), but the residual variation is not zero. The
between-cluster variation is clearly not the same as the "total
variation" / residual variation in a non mixed OLS regression on the
data aggregated within clusters.

The other way to see this is that lme4 actually uses the scaled random
effects, i.e. the variance relative to the residual variance, for
fitting. (This what the entries in the theta vector reflect: the lower
Cholesky factor of the variance-covariance matrix of the scaled random
effects.)

Best,

Phillip

On 17/09/2020 19:15, Simon Harmel wrote:
> Thanks Harold! I may have been insufficiently clear.
>
> At its core, the question is asking: "Is the "between-cluster" variation
> (i.e., ?00) in a two-level nested mixed-effects model (i.e., `m1`), the
> same as the amount of "total variation" that we would otherwise get if we
> only use the cluster-level data in a corresponding, NON-HLM regression
> model (i.e., `ols2`)?"
>
>
> library(lme4)
> library(tidyverse)
>
> hsb <- read.csv('
> https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')
> hsb_ave <- hsb %>% group_by(sch.id) %>% mutate(math_ave = mean(math)) %>%
> slice(1)
>
> ols2 <- lm(math_ave ~ sector, data = hsb_ave)
>
> m1 <- lmer(math ~ sector + (1|sch.id), data = hsb)
>
> On Thu, Sep 17, 2020 at 11:16 AM Harold Doran <
> harold.doran at cambiumassessment.com> wrote:
>
>> Simon
>>
>>
>>
>> Crossposting like this is frowned on a bit, but I?ve been in your shoes
>> trying to get an answer before. I think you might be a bit confused. I saw
>> your questions in both places and you?re asking how to get the standard
>> errors of the OLS model using by fitting a mixed model and using the
>> variance components from that mixed model to get the standard errors from
>> an OLS model.
>>
>>
>>
>> This is what we refer to in statistics as ?bass-ackwards?. ?
>>
>>
>>
>> If you want the standard errors of the fixed effects from an OLS model,
>> compute them as follows:
>>
>>
>>
>> ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
>>
>> trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
>>
>> group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
>>
>> weight <- c(ctl, trt)
>>
>> lm.D9 <- lm(weight ~ group)
>>
>> X <- model.matrix(lm.D9)
>>
>>
>>
>> ### Compute Standard errors of fixed effects
>>
>> sqrt(diag(solve(crossprod(X)) * .6964^2))
>>
>>
>>
>> ### use built in extractor function to get them instead
>>
>> sqrt(diag(vcov(lm.D9)))
>>
>>
>>
>> Similarly, get the standard errors of the mixed model from its own
>> variance/covariance matrix.
>>
>>
>>
>>
>>
>> *From:* Simon Harmel <sim.harmel at gmail.com>
>> *Sent:* Wednesday, September 16, 2020 5:54 PM
>> *To:* Harold Doran <harold.doran at cambiumassessment.com>
>> *Cc:* r-sig-mixed-models <r-sig-mixed-models at r-project.org>
>> *Subject:* Re: [R-sig-ME] Standard Error of a coef. in a 2-level model
>> vs. 2 OLS models
>>
>>
>>
>> Hi Harold,
>>
>>
>>
>> I improved my question, and asked it on CrossValidated: (
>> https://stats.stackexchange.com/questions/487363/using-variance-components-of-a-mixed-model-to-obtain-std-error-of-a-coef-from-a
>> )
>>
>>
>>
>> I would appreciate your answer, either here or on CrossValidated.
>>
>>
>>
>> Many thanks, Simon
>>
>>
>>
>> On Wed, Sep 16, 2020 at 4:45 PM Harold Doran <
>> harold.doran at cambiumassessment.com> wrote:
>>
>> This is not how standard errors are computed for linear or mixed linear
>> models. I'm not sure what you're goal is, but the SEs are the square roots
>> of the diagonal elements of the variance/covariance matrix of the fixed
>> effects.
>>
>> See ?vcov on how to extract that matrix.
>>
>> -----Original Message-----
>> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On
>> Behalf Of Simon Harmel
>> Sent: Sunday, September 13, 2020 7:51 PM
>> To: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
>> Subject: Re: [R-sig-ME] Standard Error of a coef. in a 2-level model vs. 2
>> OLS models
>>
>> External email alert: Be wary of links & attachments.
>>
>>
>> Just a clarification.
>>
>> For `ols1` model, I can approximate its SE of the sector coefficient by
>> using the within and between variance components from the HLM model:
>> sqrt(( 6.68  + 39.15  )/45)/(160*.25))
>>
>> BUT  For `ols2` model, how can I approximate its SE of the sector
>> coefficient by using the within and between variance components from the
>> HLM model?
>>
>> On Sun, Sep 13, 2020 at 6:37 PM Simon Harmel <sim.harmel at gmail.com> wrote:
>>
>>> Dear All,
>>>
>>> I have fit two ols models (ols1 & ols2) and an mixed-effects model (m1).
>>> ols1 is a simple lm() model that ignores the second-level. ols2 is a
>>> simple
>>> lm() model that ignores the first-level.
>>>
>>> For `ols1` model,  `sigma(ols1)^2` almost equals sum of variance
>>> components in the `m1` model: 6.68 (bet.) + 39.15 (with.) For `ols2`
>>> model, I wonder what does `sigma(ols2)^2` represents when compared to
>>> the `m1` model?
>>>
>>> Here is the fully reproducible code:
>>>
>>> library(lme4)
>>> library(tidyverse)
>>>
>>> hsb <- read.csv('
>>> https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')
>>> hsb_ave <- hsb %>% group_by(sch.id) %>% mutate(math_ave = mean(math))
>>> %>%
>>> slice(1) # data that only considers grouping but ignores lower level
>>>
>>> ols1 <- lm(math ~ sector, data = hsb)
>>> summary(ols1)
>>>
>>> m1 <- lmer(math ~ sector + (1|sch.id), data = hsb)
>>> summary(m1)
>>>
>>> # `sigma(ols1)^2` almost equals 6.68 (bet.) + 39.15 (with.) from lmer
>>>
>>> But if I fit another ols model that only considers the grouping
>>> structure (ignoring lower level):
>>>
>>> ols2 <- lm(math_ave ~ sector, data = hsb_ave)
>>> summary(ols2)
>>>
>>> Then what does `sigma(ols2)^2` should amount to when compared to the
>>> `m1` model?
>>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From @|m@h@rme| @end|ng |rom gm@||@com  Thu Sep 17 19:37:54 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Thu, 17 Sep 2020 12:37:54 -0500
Subject: [R-sig-ME] 
 Standard Error of a coef. in a 2-level model vs. 2 OLS models
In-Reply-To: <e003752c-30f1-0310-99a6-a611bd787b34@mpi.nl>
References: <CACgv6yU4cEznQyR_BA3HCMfRTTX9fce7R11jEaMThMOaj1UhdA@mail.gmail.com>
 <CACgv6yUZpT_8kMVjrEWaxHhWPm_bRi+Jz7N4cwDFhb+a0A-eDQ@mail.gmail.com>
 <83ba71b6f01f42b2adc245e073d8c301@cambiumassessment.com>
 <CACgv6yWYwhL0_8_oyv8E_SXm+psXhRXHK8YisGF+pfumjUCBhw@mail.gmail.com>
 <b8ef4f502d6b466d867deaf3f40ed984@cambiumassessment.com>
 <CACgv6yUwTC+-E4Tiq7femG-h+3yhV+wp4N-7s05gyngf+AWhMQ@mail.gmail.com>
 <e003752c-30f1-0310-99a6-a611bd787b34@mpi.nl>
Message-ID: <CACgv6yUhu3hMCCvOuYoYgWKqSe=_qLvGdvA18Vu4o1Wf-xTzCg@mail.gmail.com>

Thanks Phillip!

In other words, when we use the full data in a regular ols model (`ols1`
below), then the residual variation [i.e.,  sigma(ols1)^2] approximately
equals the sum of within- and between-cluster variation from a
corresponding mixed model (`m1` below).

But if we use cluster-level data in a regular ols model (`ols2` below), no
correspondence between the amounts of variation between the `ols2` model
and the corresponding mixed model (`m1` below) can be found?

library(lme4)
library(tidyverse)

hsb <- read.csv('
https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')
hsb_ave <- hsb %>% group_by(sch.id) %>% mutate(math_ave = mean(math)) %>%
slice(1)

ols1 <- lm(math ~ sector, data = hsb)
summary(ols1)

m1 <- lmer(math ~ sector + (1|sch.id), data = hsb)
summary(m1)

ols2 <- lm(math_ave ~ sector, data = hsb_ave)
summary(ols2)

On Thu, Sep 17, 2020 at 12:23 PM Phillip Alday <phillip.alday at mpi.nl> wrote:

> The answer is no.
>
> Consider the case where the between-cluster variation goes to zero (i.e.
> a singular fit), but the residual variation is not zero. The
> between-cluster variation is clearly not the same as the "total
> variation" / residual variation in a non mixed OLS regression on the
> data aggregated within clusters.
>
> The other way to see this is that lme4 actually uses the scaled random
> effects, i.e. the variance relative to the residual variance, for
> fitting. (This what the entries in the theta vector reflect: the lower
> Cholesky factor of the variance-covariance matrix of the scaled random
> effects.)
>
> Best,
>
> Phillip
>
> On 17/09/2020 19:15, Simon Harmel wrote:
> > Thanks Harold! I may have been insufficiently clear.
> >
> > At its core, the question is asking: "Is the "between-cluster" variation
> > (i.e., ?00) in a two-level nested mixed-effects model (i.e., `m1`), the
> > same as the amount of "total variation" that we would otherwise get if we
> > only use the cluster-level data in a corresponding, NON-HLM regression
> > model (i.e., `ols2`)?"
> >
> >
> > library(lme4)
> > library(tidyverse)
> >
> > hsb <- read.csv('
> > https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')
> > hsb_ave <- hsb %>% group_by(sch.id) %>% mutate(math_ave = mean(math))
> %>%
> > slice(1)
> >
> > ols2 <- lm(math_ave ~ sector, data = hsb_ave)
> >
> > m1 <- lmer(math ~ sector + (1|sch.id), data = hsb)
> >
> > On Thu, Sep 17, 2020 at 11:16 AM Harold Doran <
> > harold.doran at cambiumassessment.com> wrote:
> >
> >> Simon
> >>
> >>
> >>
> >> Crossposting like this is frowned on a bit, but I?ve been in your shoes
> >> trying to get an answer before. I think you might be a bit confused. I
> saw
> >> your questions in both places and you?re asking how to get the standard
> >> errors of the OLS model using by fitting a mixed model and using the
> >> variance components from that mixed model to get the standard errors
> from
> >> an OLS model.
> >>
> >>
> >>
> >> This is what we refer to in statistics as ?bass-ackwards?. ?
> >>
> >>
> >>
> >> If you want the standard errors of the fixed effects from an OLS model,
> >> compute them as follows:
> >>
> >>
> >>
> >> ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
> >>
> >> trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
> >>
> >> group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
> >>
> >> weight <- c(ctl, trt)
> >>
> >> lm.D9 <- lm(weight ~ group)
> >>
> >> X <- model.matrix(lm.D9)
> >>
> >>
> >>
> >> ### Compute Standard errors of fixed effects
> >>
> >> sqrt(diag(solve(crossprod(X)) * .6964^2))
> >>
> >>
> >>
> >> ### use built in extractor function to get them instead
> >>
> >> sqrt(diag(vcov(lm.D9)))
> >>
> >>
> >>
> >> Similarly, get the standard errors of the mixed model from its own
> >> variance/covariance matrix.
> >>
> >>
> >>
> >>
> >>
> >> *From:* Simon Harmel <sim.harmel at gmail.com>
> >> *Sent:* Wednesday, September 16, 2020 5:54 PM
> >> *To:* Harold Doran <harold.doran at cambiumassessment.com>
> >> *Cc:* r-sig-mixed-models <r-sig-mixed-models at r-project.org>
> >> *Subject:* Re: [R-sig-ME] Standard Error of a coef. in a 2-level model
> >> vs. 2 OLS models
> >>
> >>
> >>
> >> Hi Harold,
> >>
> >>
> >>
> >> I improved my question, and asked it on CrossValidated: (
> >>
> https://stats.stackexchange.com/questions/487363/using-variance-components-of-a-mixed-model-to-obtain-std-error-of-a-coef-from-a
> >> )
> >>
> >>
> >>
> >> I would appreciate your answer, either here or on CrossValidated.
> >>
> >>
> >>
> >> Many thanks, Simon
> >>
> >>
> >>
> >> On Wed, Sep 16, 2020 at 4:45 PM Harold Doran <
> >> harold.doran at cambiumassessment.com> wrote:
> >>
> >> This is not how standard errors are computed for linear or mixed linear
> >> models. I'm not sure what you're goal is, but the SEs are the square
> roots
> >> of the diagonal elements of the variance/covariance matrix of the fixed
> >> effects.
> >>
> >> See ?vcov on how to extract that matrix.
> >>
> >> -----Original Message-----
> >> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On
> >> Behalf Of Simon Harmel
> >> Sent: Sunday, September 13, 2020 7:51 PM
> >> To: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
> >> Subject: Re: [R-sig-ME] Standard Error of a coef. in a 2-level model
> vs. 2
> >> OLS models
> >>
> >> External email alert: Be wary of links & attachments.
> >>
> >>
> >> Just a clarification.
> >>
> >> For `ols1` model, I can approximate its SE of the sector coefficient by
> >> using the within and between variance components from the HLM model:
> >> sqrt(( 6.68  + 39.15  )/45)/(160*.25))
> >>
> >> BUT  For `ols2` model, how can I approximate its SE of the sector
> >> coefficient by using the within and between variance components from the
> >> HLM model?
> >>
> >> On Sun, Sep 13, 2020 at 6:37 PM Simon Harmel <sim.harmel at gmail.com>
> wrote:
> >>
> >>> Dear All,
> >>>
> >>> I have fit two ols models (ols1 & ols2) and an mixed-effects model
> (m1).
> >>> ols1 is a simple lm() model that ignores the second-level. ols2 is a
> >>> simple
> >>> lm() model that ignores the first-level.
> >>>
> >>> For `ols1` model,  `sigma(ols1)^2` almost equals sum of variance
> >>> components in the `m1` model: 6.68 (bet.) + 39.15 (with.) For `ols2`
> >>> model, I wonder what does `sigma(ols2)^2` represents when compared to
> >>> the `m1` model?
> >>>
> >>> Here is the fully reproducible code:
> >>>
> >>> library(lme4)
> >>> library(tidyverse)
> >>>
> >>> hsb <- read.csv('
> >>> https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')
> >>> hsb_ave <- hsb %>% group_by(sch.id) %>% mutate(math_ave = mean(math))
> >>> %>%
> >>> slice(1) # data that only considers grouping but ignores lower level
> >>>
> >>> ols1 <- lm(math ~ sector, data = hsb)
> >>> summary(ols1)
> >>>
> >>> m1 <- lmer(math ~ sector + (1|sch.id), data = hsb)
> >>> summary(m1)
> >>>
> >>> # `sigma(ols1)^2` almost equals 6.68 (bet.) + 39.15 (with.) from lmer
> >>>
> >>> But if I fit another ols model that only considers the grouping
> >>> structure (ignoring lower level):
> >>>
> >>> ols2 <- lm(math_ave ~ sector, data = hsb_ave)
> >>> summary(ols2)
> >>>
> >>> Then what does `sigma(ols2)^2` should amount to when compared to the
> >>> `m1` model?
> >>>
> >>         [[alternative HTML version deleted]]
> >>
> >> _______________________________________________
> >> R-sig-mixed-models at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>
> >>
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From @@hburner @end|ng |rom gm@||@com  Sun Sep 20 19:22:22 2020
From: @@hburner @end|ng |rom gm@||@com (Michelle Ashburner)
Date: Sun, 20 Sep 2020 13:22:22 -0400
Subject: [R-sig-ME] lmer for IV by design interaction?
Message-ID: <CAME=vUBSYGgrbOAsoU-FBZaXv6bSqyU0VPN=QLobhLSZmj1c5w@mail.gmail.com>

Greetings,

Is it possible/appropriate to use lme4::lmer() to compare the effect of an
independent variable across two designs: within-subjects and
between-subjects?

Data below from Erlebacher (1977), used to illustrate his methodology:

###
dataset <- NULL

dataset$A <- c(rep.int(1, 20), rep.int(2, 20), rep.int(1, 20), rep.int(2,
20))
#A is the independent variable.
#1, 2 represent the two levels of the IV

dataset$D <- c(rep.int(1, 40), rep.int(2, 40))
#D is the design factor.
#1 represents a within-ss measurement; 2 a between-ss measurement.

dataset$S <- c(60, 73, 93, 10, 90, 80, 83, 37, 83, 70, 77, 7, 100, 70, 100,
43, 43, 83, 40, 73, 36, 53, 66, 0, 73, 43, 20, 10, 26, 40, 60, 3, 53, 26,
63, 6, 3, 30, 7, 10, 53, 77, 2, 38, 68, 92, 3, 15, 67, 53, 58, 20, 17, 40,
85, 60, 25, 3, 82, 67, 62, 0, 57, 42, 3, 55, 22, 28, 45, 47, 52, 75, 38,
45, 65, 50, 2, 0, 10, 60)

dataset <- as.data.frame(dataset)
###

Erlebacher's analysis on these data can be computed using code developed by
Merritt, Cook, and Wang (2014):
 https://www.researchgate.net/publication/264158186_Erlebacher's_Method_for_Contrasting_the_Within_and_Between-Subjects_Manipulation_of_the_Independent_Variable_using_R_and_SPSS
<https://www.researchgate.net/publication/264158186_Erlebacher's_Method_for_Contrasting_the_Within_and_Between-Subjects_Manipulation_of_the_Independent_Variable_using_R_and_SPSS>

The output of an Erlebacher's ANOVA for these data is:
Effect of A: F(1, 51) = 21.25,
Effect of D: F(1, 42) = 0.89
Effect of A x D = F(1, 51) = 7.88
(df obtained via Satterthwaite's (1946) Method)

Some have suggested a multilevel model with the IV and the design as fixed
effects; subject as a random effect, instead of the Erlebacher's ANOVA. For
example, this Stack Exchange discussion:
https://stats.stackexchange.com/questions/414995/statistically-testing-the-impact-of-a-within-subject-vs-between-subject-design

While the following gives similar results, I am unable to determine if this
is the correct approach:

###
dataset$A <- as.factor(dataset$A)
dataset$D <- as.factor(dataset$D)
dataset$subject <- c(rep(1:20, times = 2), 21:60)

library(lme4)
library(lmerTest)
anova(lmer(S ~ A + D + A*D + (1|subject),
           dataset,
           contrasts = list(A = "contr.sum", D = "contr.sum")))
###
Which outputs:
-------
Type III Analysis of Variance Table with Satterthwaite's method
     Sum Sq    Mean Sq    NumDF  DenDF    F value       Pr(>F)
A   3097.70    3097.70         1         75.004   21.7904   0.00001304 ***
D    123.74     123.74           1         53.030   0.8704     0.355067
A:D 1148.50    1148.50        1         75.004   8.0790     0.005765 **
-------

As of yet, I am unable to manage a theoretical manipulation of Erlebacher's
model to fit a multilevel model like the one above, which adds to my
confusion regarding whether one can use a MLM approach for this type of
data.

Thank you in advance for any advice.

-- 
 Michelle Ashburner, MMATH, MA (Psych), BEd

	[[alternative HTML version deleted]]


From ph||||p@@|d@y @end|ng |rom mp|@n|  Tue Sep 22 16:55:42 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Tue, 22 Sep 2020 16:55:42 +0200
Subject: [R-sig-ME] lmer for IV by design interaction?
In-Reply-To: <CAME=vUBSYGgrbOAsoU-FBZaXv6bSqyU0VPN=QLobhLSZmj1c5w@mail.gmail.com>
References: <CAME=vUBSYGgrbOAsoU-FBZaXv6bSqyU0VPN=QLobhLSZmj1c5w@mail.gmail.com>
Message-ID: <7098d11a-23af-4210-2586-c95426657589@mpi.nl>

For the fixed effects, lme4 doesn't care whether things are between-,
within- or mixed. Older software cared a lot about this because it made
it possible to make various simplifying assumptions and thus speed up
computations, but it's not necessary using modern approaches.

I'm not sure I understand what D is representing. If you have multiple
measurements from a single subject, then that should present itself
simply as multiple rows in the dataframe. Likewise, if you don't have
multiple measurements, then that will also be obvious from the data. If
it's simply a matter of which "original" experiment the data came from;
well then you can include that as factor in the analysis, but I would
expect that effect to be null (unless of course there is some
domain-specific reason why a within-subjects manipulation would yield
different results than a between-subjects manipulation). Within-subjects
designs generally provide better estimates, so I wouldn't be surprised
if the interaction effect is present but small (look at the model
coefficients, not ANOVA for this).

Regarding the random effects: you could actually fit a by-subjects slope
for A (i..e (1+A|subject) ). This may seem strange at first because "A"
would not seem to be directly estimable for subjects who only saw one
level of A. But that's where the magic of mixed models kicks in: in such
cases, the model can use the "estimates" (technically "predictions")
from the other subjects as well as the population level estimate to fill
in the gaps. The reason why this works is that uncertain estimates are
*shrunk* towards the population level estimate. John Kruschke has an
example of this shrinkage with figures here:
https://doingbayesiandataanalysis.blogspot.com/2019/07/shrinkage-in-hierarchical-models-random.html

Best,

Phillip


On 20/09/2020 19:22, Michelle Ashburner wrote:
> Greetings,
>
> Is it possible/appropriate to use lme4::lmer() to compare the effect of an
> independent variable across two designs: within-subjects and
> between-subjects?
>
> Data below from Erlebacher (1977), used to illustrate his methodology:
>
> ###
> dataset <- NULL
>
> dataset$A <- c(rep.int(1, 20), rep.int(2, 20), rep.int(1, 20), rep.int(2,
> 20))
> #A is the independent variable.
> #1, 2 represent the two levels of the IV
>
> dataset$D <- c(rep.int(1, 40), rep.int(2, 40))
> #D is the design factor.
> #1 represents a within-ss measurement; 2 a between-ss measurement.
>
> dataset$S <- c(60, 73, 93, 10, 90, 80, 83, 37, 83, 70, 77, 7, 100, 70, 100,
> 43, 43, 83, 40, 73, 36, 53, 66, 0, 73, 43, 20, 10, 26, 40, 60, 3, 53, 26,
> 63, 6, 3, 30, 7, 10, 53, 77, 2, 38, 68, 92, 3, 15, 67, 53, 58, 20, 17, 40,
> 85, 60, 25, 3, 82, 67, 62, 0, 57, 42, 3, 55, 22, 28, 45, 47, 52, 75, 38,
> 45, 65, 50, 2, 0, 10, 60)
>
> dataset <- as.data.frame(dataset)
> ###
>
> Erlebacher's analysis on these data can be computed using code developed by
> Merritt, Cook, and Wang (2014):
>  https://www.researchgate.net/publication/264158186_Erlebacher's_Method_for_Contrasting_the_Within_and_Between-Subjects_Manipulation_of_the_Independent_Variable_using_R_and_SPSS
> <https://www.researchgate.net/publication/264158186_Erlebacher's_Method_for_Contrasting_the_Within_and_Between-Subjects_Manipulation_of_the_Independent_Variable_using_R_and_SPSS>
>
> The output of an Erlebacher's ANOVA for these data is:
> Effect of A: F(1, 51) = 21.25,
> Effect of D: F(1, 42) = 0.89
> Effect of A x D = F(1, 51) = 7.88
> (df obtained via Satterthwaite's (1946) Method)
>
> Some have suggested a multilevel model with the IV and the design as fixed
> effects; subject as a random effect, instead of the Erlebacher's ANOVA. For
> example, this Stack Exchange discussion:
> https://stats.stackexchange.com/questions/414995/statistically-testing-the-impact-of-a-within-subject-vs-between-subject-design
>
> While the following gives similar results, I am unable to determine if this
> is the correct approach:
>
> ###
> dataset$A <- as.factor(dataset$A)
> dataset$D <- as.factor(dataset$D)
> dataset$subject <- c(rep(1:20, times = 2), 21:60)
>
> library(lme4)
> library(lmerTest)
> anova(lmer(S ~ A + D + A*D + (1|subject),
>            dataset,
>            contrasts = list(A = "contr.sum", D = "contr.sum")))
> ###
> Which outputs:
> -------
> Type III Analysis of Variance Table with Satterthwaite's method
>      Sum Sq    Mean Sq    NumDF  DenDF    F value       Pr(>F)
> A   3097.70    3097.70         1         75.004   21.7904   0.00001304 ***
> D    123.74     123.74           1         53.030   0.8704     0.355067
> A:D 1148.50    1148.50        1         75.004   8.0790     0.005765 **
> -------
>
> As of yet, I am unable to manage a theoretical manipulation of Erlebacher's
> model to fit a multilevel model like the one above, which adds to my
> confusion regarding whether one can use a MLM approach for this type of
> data.
>
> Thank you in advance for any advice.
>


From @m|e@FAIRS @end|ng |rom un|v-@mu@|r  Thu Sep 24 10:48:59 2020
From: @m|e@FAIRS @end|ng |rom un|v-@mu@|r (FAIRS Amie)
Date: Thu, 24 Sep 2020 08:48:59 +0000
Subject: [R-sig-ME] Calculating effect sizes of fixed effects in lmer
Message-ID: <179fc7c9b8dd40d6a389978d728f49f9@univ-amu.fr>

Dear list,

I?m hoping someone know the current practice or wisdom regarding calculating (standardised) effect sizes of fixed effects in a mixed model (I fit all mine with lmer). By effect size I mean something akin to a cohen?s d type value. I?ve followed this list for the past few years and my understanding is that there is no easy way to do this, because of working out the degrees of freedom of the random structure (I hope I?ve understood that correctly).

However, in searching the list archives for the past two years I have seen some discussion about it, in October 2019, and I have also seen that the emmeans package has a function called eff_size (though calculating the required values for the parameters seems like it could be prone to error for myself), so I thought I would ask: is there a current standard practice for calculating effect sizes of fixed effects?

I?m not doing this in response to reviewer comments, but I anticipate I will get a comment like this for something I want to submit soon ?

Best,

Amie

------------------
Dr. Amie Fairs
Post-doctorant
Aix-Marseille Universit?
Laboratoire Parole et Langage (LPL) | CNRS UMR 7309 | 5 Avenue Pasteur | 13100 Aix-en-Provence
Email : amie.fairs at univ-amu.fr<mailto:amie.fairs at univ-amu.fr>

While I may send this email outside of typical working hours, I have no expectation to receive an email outside of your typical hours.


	[[alternative HTML version deleted]]


From wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n|  Thu Sep 24 11:44:51 2020
From: wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n| (Viechtbauer, Wolfgang (SP))
Date: Thu, 24 Sep 2020 09:44:51 +0000
Subject: [R-sig-ME] Calculating effect sizes of fixed effects in lmer
In-Reply-To: <179fc7c9b8dd40d6a389978d728f49f9@univ-amu.fr>
References: <179fc7c9b8dd40d6a389978d728f49f9@univ-amu.fr>
Message-ID: <7a082444e943458faae1453ccee8cc28@UM-MAIL3213.unimaas.nl>

Dear Amie,

I would say the answer to "is there a current standard practice for calculating effect sizes of fixed effects?" is No. The difficulty is how to standardize the predictors/outcome. In standard regression models, the 'standardized coefficients' (often referred to as 'beta') can be easily obtained by standardizing the outcome and the predictor variables before fitting the model (which is equivalent to computing beta = b * sd(x) / sd(y), the equation usually shown in textbooks for computing standardized regression coefficients). An example:

x1 <- c(2,4,3,5,6,7,4,6)
x2 <- c(0,0,0,0,0,1,1,1)
y  <- c(4,3,2,4,5,4,7,4)

res <- lm(y ~ x1 + x2)
coef(res)[2] * sd(x1) / sd(y)

res <- lm(scale(y) ~ I(scale(x1)) + I(scale(x2)))
coef(res)[2]

One could in principle do the same in mixed-effects models, but such models are often used for data that have some kind of multilevel structure (e.g., repeated measurements within subjects and/or subjects nested within some higher-level grouping variable such as pupils nested within schools). We then try to account for this structure by modeling different sources of variability (e.g., variance between schools versus variance between pupils). Computing sd(y) and sd(x1) (as above) would ignore this structure and just lumps everything together. Of course there are all kinds of proposals out there how one could do this more 'correctly' in the context of such models, but I don't think there is a general agreement on how this should be done.

Indeed, reviewers often ask authors to report some kind of 'effect size'. Nothing wrong with that, but unfortunately a lot of people interpret the term 'effect size' to refer to some kind of *standardized* measure. To me, that is an overly narrow definition of what an effect size is. For example, the (unstandardized) difference in means between two groups (e.g., treated versus control) is an effect size. And so is an unstandardized regression coefficient.

Standardized effects sizes are a crutch we use for example in meta-analysis to make results from different studies more comparable to each other because unstandardized coefficients / effects are only directly comparable if the units of y and x are the same across studies.

But for interpreting the results from a single study, an unstandardized effect size is perfectly fine as long as we start to have an appreciation for the units of the scales that we work with. If I tell an experienced clinician that some treatment for depression on average leads to a 10 point reduction on the Beck Depression Inventory, they should be able to understand what that means and how clinically relevant that is. Or to use Cohen's own words (from his infamous 1994 paper 'The earth is round (p < .05)'):

"To work constructively with 'raw' regression coefficients and confidence intervals, psychologists have to start respecting the units they work with, or develop measurement units they can respect enough so that researchers in a given field or subfield can agree to use them. In this way, there can be hope that researchers' knowledge can be cumulative. (p. 1001).

I went on a bit of a rant there towards the end, but this insistence on standardized effect sizes is a bit of a pet-peeve of mine.

Best,
Wolfgang

>-----Original Message-----
>From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org]
>On Behalf Of FAIRS Amie
>Sent: Thursday, 24 September, 2020 10:49
>To: r-sig-mixed-models at r-project.org
>Subject: [R-sig-ME] Calculating effect sizes of fixed effects in lmer
>
>Dear list,
>
>I?m hoping someone know the current practice or wisdom regarding calculating
>(standardised) effect sizes of fixed effects in a mixed model (I fit all
>mine with lmer). By effect size I mean something akin to a cohen?s d type
>value. I?ve followed this list for the past few years and my understanding
>is that there is no easy way to do this, because of working out the degrees
>of freedom of the random structure (I hope I?ve understood that correctly).
>
>However, in searching the list archives for the past two years I have seen
>some discussion about it, in October 2019, and I have also seen that the
>emmeans package has a function called eff_size (though calculating the
>required values for the parameters seems like it could be prone to error for
>myself), so I thought I would ask: is there a current standard practice for
>calculating effect sizes of fixed effects?
>
>I?m not doing this in response to reviewer comments, but I anticipate I will
>get a comment like this for something I want to submit soon ?
>
>Best,
>
>Amie
>
>------------------
>Dr. Amie Fairs
>Post-doctorant
>Aix-Marseille Universit?
>Laboratoire Parole et Langage (LPL) | CNRS UMR 7309 | 5 Avenue Pasteur |
>13100 Aix-en-Provence
>Email : amie.fairs at univ-amu.fr<mailto:amie.fairs at univ-amu.fr>
>
>While I may send this email outside of typical working hours, I have no
>expectation to receive an email outside of your typical hours.

From @m|e@FAIRS @end|ng |rom un|v-@mu@|r  Thu Sep 24 15:38:19 2020
From: @m|e@FAIRS @end|ng |rom un|v-@mu@|r (FAIRS Amie)
Date: Thu, 24 Sep 2020 13:38:19 +0000
Subject: [R-sig-ME] Calculating effect sizes of fixed effects in lmer
In-Reply-To: <7a082444e943458faae1453ccee8cc28@UM-MAIL3213.unimaas.nl>
References: <179fc7c9b8dd40d6a389978d728f49f9@univ-amu.fr>
 <7a082444e943458faae1453ccee8cc28@UM-MAIL3213.unimaas.nl>
Message-ID: <471dbaa80e034e09960a50de84eeb157@univ-amu.fr>

Dear Wolfgang,

Thank you so much for your comprehensive reply! I really appreciate it.

I agree with you that interpreting unstandardised effects within a single study, like the difference between two conditions, is a good way to go for interpretation. In the past when working with RT data I've mostly thought along the lines of having an effect of a certain number of milliseconds, and I can evaluate for myself whether or not my effect is bigger, smaller, or in line with the general size of that effect in the literature. 

However, now I'm working with EEG data and I think the hang up with effect sizes is more to do with the (previously?) standard way of analysis of this data being ANOVA, where partial eta squared values are given, and so now researchers (and reviewers) in this area are used to seeing this measure of an effect that isn't a coefficient. 

Somewhat relatedly, practically all experiments I carry out are repeated measures, and I use a summary function called summarySEwithin when calculating means/SDs/CIs for descriptive purposes (I can't remember which package it is in right now), which should take into account that I have multiple trials which belong to the same individual (based on the documentation). If my mixed model only has participants and items modelled as random intercepts, with no random slopes and nothing nested (whether it is in the world is a different question but at least in the model there is this simplified structure) is it possible to then calculate SDs for standardised effects based on how it is would be done using something like summarySEwithin? I won't actually do this, I'm just curious about whether this would be a strategy.

I'm mainly reading from your email though that standardised effect sizes aren't easy to calculate or agreed upon for mixed models ?

Best,

Amie

------------------   
Dr. Amie Fairs
Post-doctorant
Aix-Marseille Universit?
Laboratoire Parole et Langage (LPL) | CNRS UMR 7309 | 5 Avenue Pasteur | 13100 Aix-en-Provence
Email?: amie.fairs at univ-amu.fr

While I may send this email outside of typical working hours, I have no expectation to receive an email outside of your typical hours.

-----Original Message-----
From: Viechtbauer, Wolfgang (SP) <wolfgang.viechtbauer at maastrichtuniversity.nl> 
Sent: 24 September 2020 11:45
To: FAIRS Amie <amie.FAIRS at univ-amu.fr>; r-sig-mixed-models at r-project.org
Subject: RE: Calculating effect sizes of fixed effects in lmer

Dear Amie,

I would say the answer to "is there a current standard practice for calculating effect sizes of fixed effects?" is No. The difficulty is how to standardize the predictors/outcome. In standard regression models, the 'standardized coefficients' (often referred to as 'beta') can be easily obtained by standardizing the outcome and the predictor variables before fitting the model (which is equivalent to computing beta = b * sd(x) / sd(y), the equation usually shown in textbooks for computing standardized regression coefficients). An example:

x1 <- c(2,4,3,5,6,7,4,6)
x2 <- c(0,0,0,0,0,1,1,1)
y  <- c(4,3,2,4,5,4,7,4)

res <- lm(y ~ x1 + x2)
coef(res)[2] * sd(x1) / sd(y)

res <- lm(scale(y) ~ I(scale(x1)) + I(scale(x2))) coef(res)[2]

One could in principle do the same in mixed-effects models, but such models are often used for data that have some kind of multilevel structure (e.g., repeated measurements within subjects and/or subjects nested within some higher-level grouping variable such as pupils nested within schools). We then try to account for this structure by modeling different sources of variability (e.g., variance between schools versus variance between pupils). Computing sd(y) and sd(x1) (as above) would ignore this structure and just lumps everything together. Of course there are all kinds of proposals out there how one could do this more 'correctly' in the context of such models, but I don't think there is a general agreement on how this should be done.

Indeed, reviewers often ask authors to report some kind of 'effect size'. Nothing wrong with that, but unfortunately a lot of people interpret the term 'effect size' to refer to some kind of *standardized* measure. To me, that is an overly narrow definition of what an effect size is. For example, the (unstandardized) difference in means between two groups (e.g., treated versus control) is an effect size. And so is an unstandardized regression coefficient.

Standardized effects sizes are a crutch we use for example in meta-analysis to make results from different studies more comparable to each other because unstandardized coefficients / effects are only directly comparable if the units of y and x are the same across studies.

But for interpreting the results from a single study, an unstandardized effect size is perfectly fine as long as we start to have an appreciation for the units of the scales that we work with. If I tell an experienced clinician that some treatment for depression on average leads to a 10 point reduction on the Beck Depression Inventory, they should be able to understand what that means and how clinically relevant that is. Or to use Cohen's own words (from his infamous 1994 paper 'The earth is round (p < .05)'):

"To work constructively with 'raw' regression coefficients and confidence intervals, psychologists have to start respecting the units they work with, or develop measurement units they can respect enough so that researchers in a given field or subfield can agree to use them. In this way, there can be hope that researchers' knowledge can be cumulative. (p. 1001).

I went on a bit of a rant there towards the end, but this insistence on standardized effect sizes is a bit of a pet-peeve of mine.

Best,
Wolfgang

>-----Original Message-----
>From: R-sig-mixed-models 
>[mailto:r-sig-mixed-models-bounces at r-project.org]
>On Behalf Of FAIRS Amie
>Sent: Thursday, 24 September, 2020 10:49
>To: r-sig-mixed-models at r-project.org
>Subject: [R-sig-ME] Calculating effect sizes of fixed effects in lmer
>
>Dear list,
>
>I?m hoping someone know the current practice or wisdom regarding 
>calculating
>(standardised) effect sizes of fixed effects in a mixed model (I fit 
>all mine with lmer). By effect size I mean something akin to a cohen?s 
>d type value. I?ve followed this list for the past few years and my 
>understanding is that there is no easy way to do this, because of 
>working out the degrees of freedom of the random structure (I hope I?ve understood that correctly).
>
>However, in searching the list archives for the past two years I have 
>seen some discussion about it, in October 2019, and I have also seen 
>that the emmeans package has a function called eff_size (though 
>calculating the required values for the parameters seems like it could 
>be prone to error for myself), so I thought I would ask: is there a 
>current standard practice for calculating effect sizes of fixed effects?
>
>I?m not doing this in response to reviewer comments, but I anticipate I 
>will get a comment like this for something I want to submit soon ?
>
>Best,
>
>Amie
>
>------------------
>Dr. Amie Fairs
>Post-doctorant
>Aix-Marseille Universit?
>Laboratoire Parole et Langage (LPL) | CNRS UMR 7309 | 5 Avenue Pasteur 
>|
>13100 Aix-en-Provence
>Email : amie.fairs at univ-amu.fr<mailto:amie.fairs at univ-amu.fr>
>
>While I may send this email outside of typical working hours, I have no 
>expectation to receive an email outside of your typical hours.

From jepu@to @end|ng |rom gm@||@com  Thu Sep 24 16:57:39 2020
From: jepu@to @end|ng |rom gm@||@com (James Pustejovsky)
Date: Thu, 24 Sep 2020 09:57:39 -0500
Subject: [R-sig-ME] Calculating effect sizes of fixed effects in lmer
In-Reply-To: <471dbaa80e034e09960a50de84eeb157@univ-amu.fr>
References: <179fc7c9b8dd40d6a389978d728f49f9@univ-amu.fr>
 <7a082444e943458faae1453ccee8cc28@UM-MAIL3213.unimaas.nl>
 <471dbaa80e034e09960a50de84eeb157@univ-amu.fr>
Message-ID: <CAFUVuJxTAPWqdSA2HmmjkZ_bJKwQY+iAf4nt9qurnMMVZHZtRg@mail.gmail.com>

Hi Amie,

I agree very much with Wolfgang's perspective that one would ideally use
outcomes such that unstandardized effects can be interpreted directly. If
one does have to fall back on standardized effect sizes, there's a further
question of what metric to use. Researchers often jump immediately to
standardized mean differences, but there are certainly other possibilities,
such as log response ratios for outcomes that are measured on ratio scales.

All that said, there has been a fair amount of work on standardized mean
difference effect sizes for certain types of research designs that would
usually be analyzed with multi-level models. A sampling (including some of
my own):

   - Hedges, L. V. (2007). Effect sizes in cluster-randomized designs. *Journal
   of Educational and Behavioral Statistics*, *32*(4), 341-370.
   - Hedges, L. V. (2011). Effect sizes in three-level cluster-randomized
   experiments. *Journal of Educational and Behavioral Statistics*, *36*(3),
   346-380.
   - Pustejovsky, J. E., Hedges, L. V., & Shadish, W. R. (2014).
   Design-comparable effect sizes in multiple baseline designs: A general
   modeling framework. *Journal of Educational and Behavioral Statistics*,
   *39*(5), 368-393.
   - Stapleton, L. M., Pituch, K. A., & Dion, E. (2015). Standardized
   effect size measures for mediation analysis in cluster-randomized
trials. *The
   Journal of Experimental Education*, *83*(4), 547-582.
   - Feingold, A. (2009). Effect sizes for growth-modeling analysis for
   controlled clinical trials in the same metric as for classical
analysis. *Psychological
   Methods*, *14*(1), 43.

One of my students and I have also developed an R package for estimating
standardized mean differences from multilevel models fitted with nlme::lme()
https://CRAN.R-project.org/package=lmeInfo
Kind Regards,
James

	[[alternative HTML version deleted]]


From @m|e@FAIRS @end|ng |rom un|v-@mu@|r  Thu Sep 24 17:00:33 2020
From: @m|e@FAIRS @end|ng |rom un|v-@mu@|r (FAIRS Amie)
Date: Thu, 24 Sep 2020 15:00:33 +0000
Subject: [R-sig-ME] Calculating effect sizes of fixed effects in lmer
In-Reply-To: <CAFUVuJxTAPWqdSA2HmmjkZ_bJKwQY+iAf4nt9qurnMMVZHZtRg@mail.gmail.com>
References: <179fc7c9b8dd40d6a389978d728f49f9@univ-amu.fr>
 <7a082444e943458faae1453ccee8cc28@UM-MAIL3213.unimaas.nl>
 <471dbaa80e034e09960a50de84eeb157@univ-amu.fr>
 <CAFUVuJxTAPWqdSA2HmmjkZ_bJKwQY+iAf4nt9qurnMMVZHZtRg@mail.gmail.com>
Message-ID: <04995d4008454a7f9432e7710f1cb521@univ-amu.fr>

Dear James,

Thank you so much ! I?ll check out all the references and your R package.

Best,

Amie

------------------
Dr. Amie Fairs
Post-doctorant
Aix-Marseille Universit?
Laboratoire Parole et Langage (LPL) | CNRS UMR 7309 | 5 Avenue Pasteur | 13100 Aix-en-Provence
Email : amie.fairs at univ-amu.fr<mailto:amie.fairs at univ-amu.fr>

While I may send this email outside of typical working hours, I have no expectation to receive an email outside of your typical hours.

From: James Pustejovsky <jepusto at gmail.com>
Sent: 24 September 2020 16:58
To: FAIRS Amie <amie.FAIRS at univ-amu.fr>
Cc: Viechtbauer, Wolfgang (SP) <wolfgang.viechtbauer at maastrichtuniversity.nl>; r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Calculating effect sizes of fixed effects in lmer

Hi Amie,

I agree very much with Wolfgang's perspective that one would ideally use outcomes such that unstandardized effects can be interpreted directly. If one does have to fall back on standardized effect sizes, there's a further question of what metric to use. Researchers often jump immediately to standardized mean differences, but there are certainly other possibilities, such as log response ratios for outcomes that are measured on ratio scales.

All that said, there has been a fair amount of work on standardized mean difference effect sizes for certain types of research designs that would usually be analyzed with multi-level models. A sampling (including some of my own):

  *   Hedges, L. V. (2007). Effect sizes in cluster-randomized designs. Journal of Educational and Behavioral Statistics, 32(4), 341-370.
  *   Hedges, L. V. (2011). Effect sizes in three-level cluster-randomized experiments. Journal of Educational and Behavioral Statistics, 36(3), 346-380.
  *   Pustejovsky, J. E., Hedges, L. V., & Shadish, W. R. (2014). Design-comparable effect sizes in multiple baseline designs: A general modeling framework. Journal of Educational and Behavioral Statistics, 39(5), 368-393.
  *   Stapleton, L. M., Pituch, K. A., & Dion, E. (2015). Standardized effect size measures for mediation analysis in cluster-randomized trials. The Journal of Experimental Education, 83(4), 547-582.
  *   Feingold, A. (2009). Effect sizes for growth-modeling analysis for controlled clinical trials in the same metric as for classical analysis. Psychological Methods, 14(1), 43.
One of my students and I have also developed an R package for estimating standardized mean differences from multilevel models fitted with nlme::lme()
https://CRAN.R-project.org/package=lmeInfo
Kind Regards,
James

	[[alternative HTML version deleted]]


From d@|uedecke @end|ng |rom uke@de  Thu Sep 24 17:30:30 2020
From: d@|uedecke @end|ng |rom uke@de (=?UTF-8?Q?Daniel_L=C3=BCdecke?=)
Date: Thu, 24 Sep 2020 17:30:30 +0200
Subject: [R-sig-ME] Calculating effect sizes of fixed effects in lmer
In-Reply-To: <04995d4008454a7f9432e7710f1cb521@univ-amu.fr>
References: <179fc7c9b8dd40d6a389978d728f49f9@univ-amu.fr>
 <7a082444e943458faae1453ccee8cc28@UM-MAIL3213.unimaas.nl>
 <471dbaa80e034e09960a50de84eeb157@univ-amu.fr>
 <CAFUVuJxTAPWqdSA2HmmjkZ_bJKwQY+iAf4nt9qurnMMVZHZtRg@mail.gmail.com>
 <04995d4008454a7f9432e7710f1cb521@univ-amu.fr>
Message-ID: <004e01d69287$a3880910$ea981b30$@uke.de>

Dear Amie,

as additional comment to what has been said so far, I'd like to point to this forum post, which describes why it is difficult to get effect sizes like eta squared etc. from mixed models: https://afex.singmann.science/forums/topic/compute-effect-sizes-for-mixed-objects#post-295

Standardized coefficients are one possibility to report some kind of "effect size". The most accurate way would be standardizing the data before fitting the model (in particular when interaction terms are involved). Although I agree that having the "raw", unstandardized coefficients may provide a more intuitive interpretation, standardizing is sometimes even required just due to problem when fitting the model (like convergence issues).

Beyond that, you can - always having the caveats (especially) for mixed models in mind! - compute effect sizes like eta squared etc., and standardized coefficients with different methods of standardizing (posthoc as described by Wolfgang, or "refitting" the model on standardized version of the data) with the "effectsize" package: https://cran.r-project.org/package=effectsize There is also a dedicated webpage: https://easystats.github.io/effectsize/

Furthermore, the package just recently implemented a function for "pseudo-standardization" of parameters in mixed models. This approach addresses the issue raised by Wolfgang that mixed models have different sources of variability, and thus sd(y) would not properly account for this.

Hope this helps.

Best wishes
Daniel

-----Urspr?ngliche Nachricht-----
Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im Auftrag von FAIRS Amie
Gesendet: Donnerstag, 24. September 2020 17:01
An: James Pustejovsky <jepusto at gmail.com>
Cc: r-sig-mixed-models at r-project.org
Betreff: Re: [R-sig-ME] Calculating effect sizes of fixed effects in lmer

Dear James,

Thank you so much ! I?ll check out all the references and your R package.

Best,

Amie

------------------
Dr. Amie Fairs
Post-doctorant
Aix-Marseille Universit?
Laboratoire Parole et Langage (LPL) | CNRS UMR 7309 | 5 Avenue Pasteur | 13100 Aix-en-Provence
Email : amie.fairs at univ-amu.fr<mailto:amie.fairs at univ-amu.fr>

While I may send this email outside of typical working hours, I have no expectation to receive an email outside of your typical hours.

From: James Pustejovsky <jepusto at gmail.com>
Sent: 24 September 2020 16:58
To: FAIRS Amie <amie.FAIRS at univ-amu.fr>
Cc: Viechtbauer, Wolfgang (SP) <wolfgang.viechtbauer at maastrichtuniversity.nl>; r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Calculating effect sizes of fixed effects in lmer

Hi Amie,

I agree very much with Wolfgang's perspective that one would ideally use outcomes such that unstandardized effects can be interpreted directly. If one does have to fall back on standardized effect sizes, there's a further question of what metric to use. Researchers often jump immediately to standardized mean differences, but there are certainly other possibilities, such as log response ratios for outcomes that are measured on ratio scales.

All that said, there has been a fair amount of work on standardized mean difference effect sizes for certain types of research designs that would usually be analyzed with multi-level models. A sampling (including some of my own):

  *   Hedges, L. V. (2007). Effect sizes in cluster-randomized designs. Journal of Educational and Behavioral Statistics, 32(4), 341-370.
  *   Hedges, L. V. (2011). Effect sizes in three-level cluster-randomized experiments. Journal of Educational and Behavioral Statistics, 36(3), 346-380.
  *   Pustejovsky, J. E., Hedges, L. V., & Shadish, W. R. (2014). Design-comparable effect sizes in multiple baseline designs: A general modeling framework. Journal of Educational and Behavioral Statistics, 39(5), 368-393.
  *   Stapleton, L. M., Pituch, K. A., & Dion, E. (2015). Standardized effect size measures for mediation analysis in cluster-randomized trials. The Journal of Experimental Education, 83(4), 547-582.
  *   Feingold, A. (2009). Effect sizes for growth-modeling analysis for controlled clinical trials in the same metric as for classical analysis. Psychological Methods, 14(1), 43.
One of my students and I have also developed an R package for estimating standardized mean differences from multilevel models fitted with nlme::lme()
https://CRAN.R-project.org/package=lmeInfo
Kind Regards,
James

	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Joachim Pr?l?, Prof. Dr. Blanche Schwappach-Pignataro, Marya Verdel
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING

From @|m@h@rme| @end|ng |rom gm@||@com  Thu Sep 24 18:38:17 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Thu, 24 Sep 2020 11:38:17 -0500
Subject: [R-sig-ME] Meaning of Corr of random-effects with a cross-level
 interaction
Message-ID: <CACgv6yXwshN5OViou9xtaDtiFdWU1NmELKsToan8kxFUw1xgFg@mail.gmail.com>

Dear All,

I had a quick question. I have a cross-level interaction in my model below
(ses*sector). My cluster-level predictor "sector" is a binary variable
(0=Public, 1=Private). My level-1 predictor is numeric.

QUESTION:  The `Corr = 1` is indicating the correlation between
intercepts and slopes across BOTH public & private sectors (like their
average) OR something else?

hsb <- read.csv('
https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')

summary(lmer(math ~ ses*sector + (ses|sch.id), data = hsb))


Random effects:
 Groups   Name        Variance Std.Dev.     Corr
 sch.id   (Intercept)  3.82107    1.9548
          ses                0.07587     0.2754        1.00
 Residual             36.78760 6.0653

	[[alternative HTML version deleted]]


From |rmont @end|ng |rom uen|@br  Thu Sep 24 22:27:58 2020
From: |rmont @end|ng |rom uen|@br (Leandro Rabello Monteiro)
Date: Thu, 24 Sep 2020 17:27:58 -0300
Subject: [R-sig-ME] mixed model with recapture data
Message-ID: <CA+xt272Kj=UXiVBLyURBse0Y7=FwCX3J1xTmkvhJqSqErGR2LA@mail.gmail.com>

Dear All
 I am trying to evaluate the body condition (SMI) of bats in a
mark-recapture study, in response to lesions caused by arm bands.
Because recapture is a matter of chance, the design is highly
unbalanced. Most individuals were recaptured twice, but there can be
up to 18 recaptures in a period of 4 years.

The data set is formatted in a way that each line is one individual at
a point in time. The head() of the data frame looks like this

  ID Sex      SMI MarkR YearMonth
1  1   M 15.10700    L0   2013-04
2  1   M 14.52348    L0   2013-06
3  1   M 15.51033    L0   2013-07
4  1   M 15.51033    L0   2013-09
5  1   M 15.26151    L0   2013-11
6  1   M 15.33953    L0   2014-08

ID is a factor to identify individuals, MarkR (response to banding) is
a factor with levels (NR =  no ring, the first capture, L0 = ringed,
no lesion, L1 = lesion type 1, L2 = lesion type 2). A single
individual can change its level in MarkR, so it is a within-subject
fixed factor. Some individuals will develop lesions and some will not.
The question of interest is whether banding itself or lesions caused
by banding can be associated with lower SMI, so the only comparisons
of interest are the levels L0-2 against the "control" NR.

 Lesions, particularly L2 are rare, occurring in ~3% of observations
(out of 2400), again with a high unbalance among levels. There is some
seasonality in body condition, but I am not particularly interested in
this aspect right now, but I am not sure about the best way to include
the temporal factor YearMonth it in the model.

I have tried the following, using individuals and YearMonth as random effects.
lm.smi<-lmer(SMI~Sex*MarkR+(1|ID)+(1|YearMonth),data=smi)

I would appreciate some guidance as to whether I might be missing
something relevant, particularly due to the highly unbalanced design.
I have searched a lot but have not managed to find similar examples in
the literature or the web. Thanks a lot for your time.


##################################################
Leandro R. Monteiro
Laboratorio de Ciencias Ambientais
Universidade Estadual do Norte Fluminense
E-mail: lrmont at uenf.br
CV Lattes: http://lattes.cnpq.br/4987216474124557
WS: https://sites.google.com/uenf.br/ecol-evolucao-de-mamiferos/
English WS: https://sites.google.com/uenf.br/mammalecologyandevolution/
##################################################


From th|erry@onke||nx @end|ng |rom |nbo@be  Fri Sep 25 09:18:03 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Fri, 25 Sep 2020 09:18:03 +0200
Subject: [R-sig-ME] mixed model with recapture data
In-Reply-To: <CA+xt272Kj=UXiVBLyURBse0Y7=FwCX3J1xTmkvhJqSqErGR2LA@mail.gmail.com>
References: <CA+xt272Kj=UXiVBLyURBse0Y7=FwCX3J1xTmkvhJqSqErGR2LA@mail.gmail.com>
Message-ID: <CAJuCY5w9DFOKKAnxSGwhu-4SE1sYoMVAgBCp+FkajmGGKUYLUw@mail.gmail.com>

Dear Leandro,

You could consider splitting the time effect into a year effect and a month
effect. This will assume that every year has the same seasonal pattern. Add
year as a fixed effect factor if your data spans only a few years.

lm.smi <- lmer(SMI ~ Sex * MarkR + Year + (1 | ID) + (1 | Month), data =
smi)

The bats in our region are hibernating. Their body condition peaks in the
early autumn and is low in early spring. You can model such a pattern with
e.g. a sine wave as fixed effect and a random effect to model the
deviations from the sine wave.
Month_rad <- 2 * pi * Month / 12
sin(Month_rad) + cos(Month_rad) + (1 | Month)

Notethataddingspacestotextmakesitmuchmorereadable.Thesamegoesforcode.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op do 24 sep. 2020 om 22:47 schreef Leandro Rabello Monteiro <lrmont at uenf.br
>:

> Dear All
>  I am trying to evaluate the body condition (SMI) of bats in a
> mark-recapture study, in response to lesions caused by arm bands.
> Because recapture is a matter of chance, the design is highly
> unbalanced. Most individuals were recaptured twice, but there can be
> up to 18 recaptures in a period of 4 years.
>
> The data set is formatted in a way that each line is one individual at
> a point in time. The head() of the data frame looks like this
>
>   ID Sex      SMI MarkR YearMonth
> 1  1   M 15.10700    L0   2013-04
> 2  1   M 14.52348    L0   2013-06
> 3  1   M 15.51033    L0   2013-07
> 4  1   M 15.51033    L0   2013-09
> 5  1   M 15.26151    L0   2013-11
> 6  1   M 15.33953    L0   2014-08
>
> ID is a factor to identify individuals, MarkR (response to banding) is
> a factor with levels (NR =  no ring, the first capture, L0 = ringed,
> no lesion, L1 = lesion type 1, L2 = lesion type 2). A single
> individual can change its level in MarkR, so it is a within-subject
> fixed factor. Some individuals will develop lesions and some will not.
> The question of interest is whether banding itself or lesions caused
> by banding can be associated with lower SMI, so the only comparisons
> of interest are the levels L0-2 against the "control" NR.
>
>  Lesions, particularly L2 are rare, occurring in ~3% of observations
> (out of 2400), again with a high unbalance among levels. There is some
> seasonality in body condition, but I am not particularly interested in
> this aspect right now, but I am not sure about the best way to include
> the temporal factor YearMonth it in the model.
>
> I have tried the following, using individuals and YearMonth as random
> effects.
> lm.smi<-lmer(SMI~Sex*MarkR+(1|ID)+(1|YearMonth),data=smi)
>
> I would appreciate some guidance as to whether I might be missing
> something relevant, particularly due to the highly unbalanced design.
> I have searched a lot but have not managed to find similar examples in
> the literature or the web. Thanks a lot for your time.
>
>
> ##################################################
> Leandro R. Monteiro
> Laboratorio de Ciencias Ambientais
> Universidade Estadual do Norte Fluminense
> E-mail: lrmont at uenf.br
> CV Lattes: http://lattes.cnpq.br/4987216474124557
> WS: https://sites.google.com/uenf.br/ecol-evolucao-de-mamiferos/
> English WS: https://sites.google.com/uenf.br/mammalecologyandevolution/
> ##################################################
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Fri Sep 25 10:03:31 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Fri, 25 Sep 2020 10:03:31 +0200
Subject: [R-sig-ME] Meaning of Corr of random-effects with a cross-level
 interaction
In-Reply-To: <CACgv6yXwshN5OViou9xtaDtiFdWU1NmELKsToan8kxFUw1xgFg@mail.gmail.com>
References: <CACgv6yXwshN5OViou9xtaDtiFdWU1NmELKsToan8kxFUw1xgFg@mail.gmail.com>
Message-ID: <CAJuCY5wvENrapzA70De+=2-b-OyP=CDDd3aDgG2-pW4_dPb_YA@mail.gmail.com>

Dear Simon,

A perfect correlation between random effect parameters indicates a problem.
Note the failed convergence warning.
Standardising ses makes things even worse: it yields a singular fit error.

Removing the random slope of ses or the sector interaction solves the
problem. i.e. the model runs and yields sensible output.

Looking at the data, it seems like both math and ses have bounds. Ses
even seems to have some data above its upper bound.
The model assumes no bounds in the response variable. Maybe this is the
cause of the problem.

ggplot(hsb, aes(x = ses, y = math, colour = factor(sector))) +
  geom_point()

Best regards,

Thierry


ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op do 24 sep. 2020 om 18:39 schreef Simon Harmel <sim.harmel at gmail.com>:

> Dear All,
>
> I had a quick question. I have a cross-level interaction in my model below
> (ses*sector). My cluster-level predictor "sector" is a binary variable
> (0=Public, 1=Private). My level-1 predictor is numeric.
>
> QUESTION:  The `Corr = 1` is indicating the correlation between
> intercepts and slopes across BOTH public & private sectors (like their
> average) OR something else?
>
> hsb <- read.csv('
> https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')
>
> summary(lmer(math ~ ses*sector + (ses|sch.id), data = hsb))
>
>
> Random effects:
>  Groups   Name        Variance Std.Dev.     Corr
>  sch.id   (Intercept)  3.82107    1.9548
>           ses                0.07587     0.2754        1.00
>  Residual             36.78760 6.0653
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From |ongrob604 @end|ng |rom gm@||@com  Fri Sep 25 10:56:27 2020
From: |ongrob604 @end|ng |rom gm@||@com (Robert Long)
Date: Fri, 25 Sep 2020 09:56:27 +0100
Subject: [R-sig-ME] Meaning of Corr of random-effects with a cross-level
 interaction
In-Reply-To: <CAJuCY5wvENrapzA70De+=2-b-OyP=CDDd3aDgG2-pW4_dPb_YA@mail.gmail.com>
References: <CACgv6yXwshN5OViou9xtaDtiFdWU1NmELKsToan8kxFUw1xgFg@mail.gmail.com>
 <CAJuCY5wvENrapzA70De+=2-b-OyP=CDDd3aDgG2-pW4_dPb_YA@mail.gmail.com>
Message-ID: <CA+3TTkPcSKozdzgsS20eUs7Pe75H80Pg+O9x3Ah_mwSwwZtwaA@mail.gmail.com>

Hi Thierry and list

This was actually cross-posted at CrossValidated yesterday:
https://stats.stackexchange.com/questions/488984/corr-of-random-effects-when-a-cross-level-interaction-in-lme4

I have the impression that Simon is experimenting with a toy dataset,
rather than analysing their own study, which is a great way to learn, in
my opinion.

As you can see from my answer and the comments to it, the real question
(actually, two questions) is this:

Suppose we have two groups of schools, with a single explanatory variable
at the student level. Suppose further that the correlation between the
random slopes for that variable and the random intercepts in the two groups
is very different.  The first question is what the overall correlation
represents ? I thought that it would probably be some kind of average of
the two.  I did some simulations that indicate that this seems to be the
case. The followup question (see the last comment to my answer) asks how to
uncover the correlations in the two groups ? From my simulations so far the
only way I can see of doing this is by splitting the data by group and
fitting two models.

Best regards
Robert Long



On Fri, Sep 25, 2020 at 9:04 AM Thierry Onkelinx via R-sig-mixed-models <
r-sig-mixed-models at r-project.org> wrote:

> Dear Simon,
>
> A perfect correlation between random effect parameters indicates a problem.
> Note the failed convergence warning.
> Standardising ses makes things even worse: it yields a singular fit error.
>
> Removing the random slope of ses or the sector interaction solves the
> problem. i.e. the model runs and yields sensible output.
>
> Looking at the data, it seems like both math and ses have bounds. Ses
> even seems to have some data above its upper bound.
> The model assumes no bounds in the response variable. Maybe this is the
> cause of the problem.
>
> ggplot(hsb, aes(x = ses, y = math, colour = factor(sector))) +
>   geom_point()
>
> Best regards,
>
> Thierry
>
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op do 24 sep. 2020 om 18:39 schreef Simon Harmel <sim.harmel at gmail.com>:
>
> > Dear All,
> >
> > I had a quick question. I have a cross-level interaction in my model
> below
> > (ses*sector). My cluster-level predictor "sector" is a binary variable
> > (0=Public, 1=Private). My level-1 predictor is numeric.
> >
> > QUESTION:  The `Corr = 1` is indicating the correlation between
> > intercepts and slopes across BOTH public & private sectors (like their
> > average) OR something else?
> >
> > hsb <- read.csv('
> > https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')
> >
> > summary(lmer(math ~ ses*sector + (ses|sch.id), data = hsb))
> >
> >
> > Random effects:
> >  Groups   Name        Variance Std.Dev.     Corr
> >  sch.id   (Intercept)  3.82107    1.9548
> >           ses                0.07587     0.2754        1.00
> >  Residual             36.78760 6.0653
> >
> >         [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From d@e@kornbrot @end|ng |rom hert@@@c@uk  Fri Sep 25 11:55:33 2020
From: d@e@kornbrot @end|ng |rom hert@@@c@uk (Kornbrot, Diana)
Date: Fri, 25 Sep 2020 09:55:33 +0000
Subject: [R-sig-ME] R-sig-mixed-models Digest,Effect sizes
In-Reply-To: <mailman.18629.5324.1601018309.1128.r-sig-mixed-models@r-project.org>
References: <mailman.18629.5324.1601018309.1128.r-sig-mixed-models@r-project.org>
Message-ID: <DCBE4366-4D4C-40B6-993F-0F261F767C33@herts.ac.uk>

Global effect sizes are ALWAYS dodgy for measuring strength because they rely on ASSUMPTIONS about pooled variance.
At the end of the day investigators want to know about specific binary comparisons.
These may be a priori if investigators already have a prediction or post hoc if they follow from considering obtained means.

An alternative, and IMHO better, approach is to use global fit measures to choose best model of all possible  pairs after ordering means, and then make relevant pairwise comparisons,
Simple e.g. only repeated measures: create a single between repeated factor R  consider models
M1 r1>r2>r3
M2 r1>r2=r3
M3 r1=r2>r3
M4 r1=r2=r3
See which model is best using global fit WAIC preferred, but AIC or BIC will almost certainly give same ordering  of models. Only consider effect size for relevant pairwise contrast in best models.  The other are irrelevan.

Recalculatee postdoc and only the sd relevant to that poor will be used. Do NOT use contrast form factorial analysis as that will bring in all the hairy pooled variance assumption. As it is pairwise repeated effectively a single difference SD is relevant. So no assumptions

If its a single between factor then then I would recommend heterogeneous variance pairwise test.
Its more complicated if both between and repeated. Consider order
B1r1, B2r2, B2r1, B1r2 giving models

M1: B1r1>B2r2, between comparisons
       B2r2>B2r1 repeated comparisons
       B2r1>B1r2 between comparisons
M2: B1r1=B2r2, no comparisons
       B2r2>B2r1 repeated comparison
       B2r1>B1r2 between comparisons
Etc.

Keselman, H. J., Cribbie, R. A., & Holland, B. (2004). Pairwise multiple comparison test procedures: An update for clinical child and adolescent psychologists [Article]. Journal of Clinical Child and Adolescent Psychology, 33(3), 623-645. https://doi.org/10.1207/s15374424jccp3303_19
Cribbie, R. A., & Keselman, H. J. (2003). The effects of nonnormality on parametric, nonparametric, and model comparison approaches to pairwise comparisons [Article]. Educational and Psychological Measurement, 63(4), 615-635. https://doi.org/10.1177/0013164403251283
Cribbie, R. A., & Keselman, H. J. (2003). Pairwise multiple comparisons: A model comparison approach versus stepwise procedures. British Journal of Mathematical and Statistical Psychology, 56(1), 167-182. https://doi.org/10.1348/000711003321645412

IN other words if design compares UK , US, Australia why should sd of US effect difference between UK and Australia?
If design gives coffee , orange juice and whisky to same people [appropriately counterbalanced] why should sd on whisky be involved in comparison of orange juice and coffee.
Work of Keeselman and Cribbie not getting attention it deserves, IMHO.
best
Diana

On 25 Sep 2020, at 08:18, r-sig-mixed-models-request at r-project.org<mailto:r-sig-mixed-models-request at r-project.org> wrote:

Send R-sig-mixed-models mailing list submissions to
r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>

To subscribe or unsubscribe via the World Wide Web, visit
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
or, via email, send a message with subject or body 'help' to
r-sig-mixed-models-request at r-project.org

You can reach the person managing the list at
r-sig-mixed-models-owner at r-project.org

When replying, please edit your Subject line so it is more specific
than "Re: Contents of R-sig-mixed-models digest..."


Today's Topics:

  1. Re: Calculating effect sizes of fixed effects in lmer
     (=?UTF-8?Q?Daniel_L=C3=BCdecke?=)
  2. Meaning of Corr of random-effects with a cross-level
     interaction (Simon Harmel)
  3. mixed model with recapture data (Leandro Rabello Monteiro)
  4. Re: mixed model with recapture data (Thierry Onkelinx)

----------------------------------------------------------------------

Message: 1
Date: Thu, 24 Sep 2020 17:30:30 +0200
From: =?UTF-8?Q?Daniel_L=C3=BCdecke?= <d.luedecke at uke.de>
To: 'FAIRS Amie' <amie.FAIRS at univ-amu.fr>, 'James Pustejovsky'
<jepusto at gmail.com>
Cc: <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Calculating effect sizes of fixed effects in
lmer
Message-ID: <004e01d69287$a3880910$ea981b30$@uke.de>
Content-Type: text/plain; charset="utf-8"

Dear Amie,

as additional comment to what has been said so far, I'd like to point to this forum post, which describes why it is difficult to get effect sizes like eta squared etc. from mixed models: https://afex.singmann.science/forums/topic/compute-effect-sizes-for-mixed-objects#post-295

Standardized coefficients are one possibility to report some kind of "effect size". The most accurate way would be standardizing the data before fitting the model (in particular when interaction terms are involved). Although I agree that having the "raw", unstandardized coefficients may provide a more intuitive interpretation, standardizing is sometimes even required just due to problem when fitting the model (like convergence issues).

Beyond that, you can - always having the caveats (especially) for mixed models in mind! - compute effect sizes like eta squared etc., and standardized coefficients with different methods of standardizing (posthoc as described by Wolfgang, or "refitting" the model on standardized version of the data) with the "effectsize" package: https://cran.r-project.org/package=effectsize There is also a dedicated webpage: https://easystats.github.io/effectsize/

Furthermore, the package just recently implemented a function for "pseudo-standardization" of parameters in mixed models. This approach addresses the issue raised by Wolfgang that mixed models have different sources of variability, and thus sd(y) would not properly account for this.

Hope this helps.

Best wishes
Daniel

-----Urspr?ngliche Nachricht-----
Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im Auftrag von FAIRS Amie
Gesendet: Donnerstag, 24. September 2020 17:01
An: James Pustejovsky <jepusto at gmail.com>
Cc: r-sig-mixed-models at r-project.org
Betreff: Re: [R-sig-ME] Calculating effect sizes of fixed effects in lmer

Dear James,

Thank you so much ! I?ll check out all the references and your R package.

Best,

Amie

------------------
Dr. Amie Fairs
Post-doctorant
Aix-Marseille Universit?
Laboratoire Parole et Langage (LPL) | CNRS UMR 7309 | 5 Avenue Pasteur | 13100 Aix-en-Provence
Email : amie.fairs at univ-amu.fr<mailto:amie.fairs at univ-amu.fr>

While I may send this email outside of typical working hours, I have no expectation to receive an email outside of your typical hours.

From: James Pustejovsky <jepusto at gmail.com>
Sent: 24 September 2020 16:58
To: FAIRS Amie <amie.FAIRS at univ-amu.fr>
Cc: Viechtbauer, Wolfgang (SP) <wolfgang.viechtbauer at maastrichtuniversity.nl>; r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Calculating effect sizes of fixed effects in lmer

Hi Amie,

I agree very much with Wolfgang's perspective that one would ideally use outcomes such that unstandardized effects can be interpreted directly. If one does have to fall back on standardized effect sizes, there's a further question of what metric to use. Researchers often jump immediately to standardized mean differences, but there are certainly other possibilities, such as log response ratios for outcomes that are measured on ratio scales.

All that said, there has been a fair amount of work on standardized mean difference effect sizes for certain types of research designs that would usually be analyzed with multi-level models. A sampling (including some of my own):

 *   Hedges, L. V. (2007). Effect sizes in cluster-randomized designs. Journal of Educational and Behavioral Statistics, 32(4), 341-370.
 *   Hedges, L. V. (2011). Effect sizes in three-level cluster-randomized experiments. Journal of Educational and Behavioral Statistics, 36(3), 346-380.
 *   Pustejovsky, J. E., Hedges, L. V., & Shadish, W. R. (2014). Design-comparable effect sizes in multiple baseline designs: A general modeling framework. Journal of Educational and Behavioral Statistics, 39(5), 368-393.
 *   Stapleton, L. M., Pituch, K. A., & Dion, E. (2015). Standardized effect size measures for mediation analysis in cluster-randomized trials. The Journal of Experimental Education, 83(4), 547-582.
 *   Feingold, A. (2009). Effect sizes for growth-modeling analysis for controlled clinical trials in the same metric as for classical analysis. Psychological Methods, 14(1), 43.
One of my students and I have also developed an R package for estimating standardized mean differences from multilevel models fitted with nlme::lme()
https://CRAN.R-project.org/package=lmeInfo
Kind Regards,
James

[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Joachim Pr?l?, Prof. Dr. Blanche Schwappach-Pignataro, Marya Verdel
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING


------------------------------

Message: 2
Date: Thu, 24 Sep 2020 11:38:17 -0500
From: Simon Harmel <sim.harmel at gmail.com>
To: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
Subject: [R-sig-ME] Meaning of Corr of random-effects with a
cross-level interaction
Message-ID:
<CACgv6yXwshN5OViou9xtaDtiFdWU1NmELKsToan8kxFUw1xgFg at mail.gmail.com>
Content-Type: text/plain; charset="utf-8"

Dear All,

I had a quick question. I have a cross-level interaction in my model below
(ses*sector). My cluster-level predictor "sector" is a binary variable
(0=Public, 1=Private). My level-1 predictor is numeric.

QUESTION:  The `Corr = 1` is indicating the correlation between
intercepts and slopes across BOTH public & private sectors (like their
average) OR something else?

hsb <- read.csv('
https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')

summary(lmer(math ~ ses*sector + (ses|sch.id), data = hsb))


Random effects:
Groups   Name        Variance Std.Dev.     Corr
sch.id   (Intercept)  3.82107    1.9548
         ses                0.07587     0.2754        1.00
Residual             36.78760 6.0653

[[alternative HTML version deleted]]




------------------------------

Message: 3
Date: Thu, 24 Sep 2020 17:27:58 -0300
From: Leandro Rabello Monteiro <lrmont at uenf.br>
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] mixed model with recapture data
Message-ID:
<CA+xt272Kj=UXiVBLyURBse0Y7=FwCX3J1xTmkvhJqSqErGR2LA at mail.gmail.com>
Content-Type: text/plain; charset="utf-8"

Dear All
I am trying to evaluate the body condition (SMI) of bats in a
mark-recapture study, in response to lesions caused by arm bands.
Because recapture is a matter of chance, the design is highly
unbalanced. Most individuals were recaptured twice, but there can be
up to 18 recaptures in a period of 4 years.

The data set is formatted in a way that each line is one individual at
a point in time. The head() of the data frame looks like this

 ID Sex      SMI MarkR YearMonth
1  1   M 15.10700    L0   2013-04
2  1   M 14.52348    L0   2013-06
3  1   M 15.51033    L0   2013-07
4  1   M 15.51033    L0   2013-09
5  1   M 15.26151    L0   2013-11
6  1   M 15.33953    L0   2014-08

ID is a factor to identify individuals, MarkR (response to banding) is
a factor with levels (NR =  no ring, the first capture, L0 = ringed,
no lesion, L1 = lesion type 1, L2 = lesion type 2). A single
individual can change its level in MarkR, so it is a within-subject
fixed factor. Some individuals will develop lesions and some will not.
The question of interest is whether banding itself or lesions caused
by banding can be associated with lower SMI, so the only comparisons
of interest are the levels L0-2 against the "control" NR.

Lesions, particularly L2 are rare, occurring in ~3% of observations
(out of 2400), again with a high unbalance among levels. There is some
seasonality in body condition, but I am not particularly interested in
this aspect right now, but I am not sure about the best way to include
the temporal factor YearMonth it in the model.

I have tried the following, using individuals and YearMonth as random effects.
lm.smi<-lmer(SMI~Sex*MarkR+(1|ID)+(1|YearMonth),data=smi)

I would appreciate some guidance as to whether I might be missing
something relevant, particularly due to the highly unbalanced design.
I have searched a lot but have not managed to find similar examples in
the literature or the web. Thanks a lot for your time.


##################################################
Leandro R. Monteiro
Laboratorio de Ciencias Ambientais
Universidade Estadual do Norte Fluminense
E-mail: lrmont at uenf.br
CV Lattes: http://lattes.cnpq.br/4987216474124557
WS: https://sites.google.com/uenf.br/ecol-evolucao-de-mamiferos/
English WS: https://sites.google.com/uenf.br/mammalecologyandevolution/
##################################################




------------------------------

Message: 4
Date: Fri, 25 Sep 2020 09:18:03 +0200
From: Thierry Onkelinx <thierry.onkelinx at inbo.be>
To: Leandro Rabello Monteiro <lrmont at uenf.br>
Cc: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] mixed model with recapture data
Message-ID:
<CAJuCY5w9DFOKKAnxSGwhu-4SE1sYoMVAgBCp+FkajmGGKUYLUw at mail.gmail.com>
Content-Type: text/plain; charset="utf-8"

Dear Leandro,

You could consider splitting the time effect into a year effect and a month
effect. This will assume that every year has the same seasonal pattern. Add
year as a fixed effect factor if your data spans only a few years.

lm.smi <- lmer(SMI ~ Sex * MarkR + Year + (1 | ID) + (1 | Month), data =
smi)

The bats in our region are hibernating. Their body condition peaks in the
early autumn and is low in early spring. You can model such a pattern with
e.g. a sine wave as fixed effect and a random effect to model the
deviations from the sine wave.
Month_rad <- 2 * pi * Month / 12
sin(Month_rad) + cos(Month_rad) + (1 | Month)

Notethataddingspacestotextmakesitmuchmorereadable.Thesamegoesforcode.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op do 24 sep. 2020 om 22:47 schreef Leandro Rabello Monteiro <lrmont at uenf.br
:

Dear All
I am trying to evaluate the body condition (SMI) of bats in a
mark-recapture study, in response to lesions caused by arm bands.
Because recapture is a matter of chance, the design is highly
unbalanced. Most individuals were recaptured twice, but there can be
up to 18 recaptures in a period of 4 years.

The data set is formatted in a way that each line is one individual at
a point in time. The head() of the data frame looks like this

 ID Sex      SMI MarkR YearMonth
1  1   M 15.10700    L0   2013-04
2  1   M 14.52348    L0   2013-06
3  1   M 15.51033    L0   2013-07
4  1   M 15.51033    L0   2013-09
5  1   M 15.26151    L0   2013-11
6  1   M 15.33953    L0   2014-08

ID is a factor to identify individuals, MarkR (response to banding) is
a factor with levels (NR =  no ring, the first capture, L0 = ringed,
no lesion, L1 = lesion type 1, L2 = lesion type 2). A single
individual can change its level in MarkR, so it is a within-subject
fixed factor. Some individuals will develop lesions and some will not.
The question of interest is whether banding itself or lesions caused
by banding can be associated with lower SMI, so the only comparisons
of interest are the levels L0-2 against the "control" NR.

Lesions, particularly L2 are rare, occurring in ~3% of observations
(out of 2400), again with a high unbalance among levels. There is some
seasonality in body condition, but I am not particularly interested in
this aspect right now, but I am not sure about the best way to include
the temporal factor YearMonth it in the model.

I have tried the following, using individuals and YearMonth as random
effects.
lm.smi<-lmer(SMI~Sex*MarkR+(1|ID)+(1|YearMonth),data=smi)

I would appreciate some guidance as to whether I might be missing
something relevant, particularly due to the highly unbalanced design.
I have searched a lot but have not managed to find similar examples in
the literature or the web. Thanks a lot for your time.


##################################################
Leandro R. Monteiro
Laboratorio de Ciencias Ambientais
Universidade Estadual do Norte Fluminense
E-mail: lrmont at uenf.br
CV Lattes: http://lattes.cnpq.br/4987216474124557
WS: https://sites.google.com/uenf.br/ecol-evolucao-de-mamiferos/
English WS: https://sites.google.com/uenf.br/mammalecologyandevolution/
##################################################

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


[[alternative HTML version deleted]]




------------------------------

Subject: Digest Footer

_______________________________________________
R-sig-mixed-models mailing list
R-sig-mixed-models at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


------------------------------

End of R-sig-mixed-models Digest, Vol 165, Issue 22
***************************************************

____________
University of Hertfordshire
College Lane, Hatfield, Hertfordshire AL10 9AB, UK
+44 (0) 208 444 2081
+44 (0) 7403 18 16 12
d.e.kornbrot at herts.ac.uk<mailto:d.e.kornbrot at herts.ac.uk>
http://dianakornbrot.wordpress.com/
skype:  kornbrotme
Save our in-boxes! http://emailcharter.org
 __________________








	[[alternative HTML version deleted]]


From |rmont @end|ng |rom uen|@br  Fri Sep 25 15:03:47 2020
From: |rmont @end|ng |rom uen|@br (Leandro Rabello Monteiro)
Date: Fri, 25 Sep 2020 10:03:47 -0300
Subject: [R-sig-ME] mixed model with recapture data
In-Reply-To: <CAJuCY5w9DFOKKAnxSGwhu-4SE1sYoMVAgBCp+FkajmGGKUYLUw@mail.gmail.com>
References: <CA+xt272Kj=UXiVBLyURBse0Y7=FwCX3J1xTmkvhJqSqErGR2LA@mail.gmail.com>
 <CAJuCY5w9DFOKKAnxSGwhu-4SE1sYoMVAgBCp+FkajmGGKUYLUw@mail.gmail.com>
Message-ID: <CA+xt2731vjhqBxa8s1izUM_r3Yd4YP9mg8Dqzc7NAqMOhq1rpg@mail.gmail.com>

Dear Thierry
Thanks a lot for the advice. Our bats (Carollia perspicillata) do not
hibernate, but their body condition also presents a seasonal pattern of
change, with a decrease after the breeding season (middle of the year, dry
season here), particularly strong for females. I implemented the month and
year factors as you suggested, but also added an interaction with sex. The
dimorphic seasonality in condition helped explain the main result, that
females are more sensitive than males to lesions caused by arm bands.
Best regards,
Leandro

##################################################
Leandro R. Monteiro
Laboratorio de Ciencias Ambientais
Universidade Estadual do Norte Fluminense
E-mail: lrmont at uenf.br
CV Lattes: http://lattes.cnpq.br/4987216474124557
WS: https://sites.google.com/uenf.br/ecol-evolucao-de-mamiferos/
<http://sites.google.com/site/morphogroup/>
English WS: https://sites.google.com/uenf.br/mammalecologyandevolution/
##################################################


Em sex., 25 de set. de 2020 ?s 04:18, Thierry Onkelinx <
thierry.onkelinx at inbo.be> escreveu:

> Dear Leandro,
>
> You could consider splitting the time effect into a year effect and a
> month effect. This will assume that every year has the same seasonal
> pattern. Add year as a fixed effect factor if your data spans only a few
> years.
>
> lm.smi <- lmer(SMI ~ Sex * MarkR + Year + (1 | ID) + (1 | Month), data =
> smi)
>
> The bats in our region are hibernating. Their body condition peaks in the
> early autumn and is low in early spring. You can model such a pattern with
> e.g. a sine wave as fixed effect and a random effect to model the
> deviations from the sine wave.
> Month_rad <- 2 * pi * Month / 12
> sin(Month_rad) + cos(Month_rad) + (1 | Month)
>
> Notethataddingspacestotextmakesitmuchmorereadable.Thesamegoesforcode.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op do 24 sep. 2020 om 22:47 schreef Leandro Rabello Monteiro <
> lrmont at uenf.br>:
>
>> Dear All
>>  I am trying to evaluate the body condition (SMI) of bats in a
>> mark-recapture study, in response to lesions caused by arm bands.
>> Because recapture is a matter of chance, the design is highly
>> unbalanced. Most individuals were recaptured twice, but there can be
>> up to 18 recaptures in a period of 4 years.
>>
>> The data set is formatted in a way that each line is one individual at
>> a point in time. The head() of the data frame looks like this
>>
>>   ID Sex      SMI MarkR YearMonth
>> 1  1   M 15.10700    L0   2013-04
>> 2  1   M 14.52348    L0   2013-06
>> 3  1   M 15.51033    L0   2013-07
>> 4  1   M 15.51033    L0   2013-09
>> 5  1   M 15.26151    L0   2013-11
>> 6  1   M 15.33953    L0   2014-08
>>
>> ID is a factor to identify individuals, MarkR (response to banding) is
>> a factor with levels (NR =  no ring, the first capture, L0 = ringed,
>> no lesion, L1 = lesion type 1, L2 = lesion type 2). A single
>> individual can change its level in MarkR, so it is a within-subject
>> fixed factor. Some individuals will develop lesions and some will not.
>> The question of interest is whether banding itself or lesions caused
>> by banding can be associated with lower SMI, so the only comparisons
>> of interest are the levels L0-2 against the "control" NR.
>>
>>  Lesions, particularly L2 are rare, occurring in ~3% of observations
>> (out of 2400), again with a high unbalance among levels. There is some
>> seasonality in body condition, but I am not particularly interested in
>> this aspect right now, but I am not sure about the best way to include
>> the temporal factor YearMonth it in the model.
>>
>> I have tried the following, using individuals and YearMonth as random
>> effects.
>> lm.smi<-lmer(SMI~Sex*MarkR+(1|ID)+(1|YearMonth),data=smi)
>>
>> I would appreciate some guidance as to whether I might be missing
>> something relevant, particularly due to the highly unbalanced design.
>> I have searched a lot but have not managed to find similar examples in
>> the literature or the web. Thanks a lot for your time.
>>
>>
>> ##################################################
>> Leandro R. Monteiro
>> Laboratorio de Ciencias Ambientais
>> Universidade Estadual do Norte Fluminense
>> E-mail: lrmont at uenf.br
>> CV Lattes: http://lattes.cnpq.br/4987216474124557
>> WS: https://sites.google.com/uenf.br/ecol-evolucao-de-mamiferos/
>> English WS: https://sites.google.com/uenf.br/mammalecologyandevolution/
>> ##################################################
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From Tom_Ph|||pp| @end|ng |rom np@@gov  Fri Sep 25 18:12:37 2020
From: Tom_Ph|||pp| @end|ng |rom np@@gov (Philippi, Tom)
Date: Fri, 25 Sep 2020 16:12:37 +0000
Subject: [R-sig-ME] [EXTERNAL]  mixed model with recapture data
In-Reply-To: <CA+xt272Kj=UXiVBLyURBse0Y7=FwCX3J1xTmkvhJqSqErGR2LA@mail.gmail.com>
References: <CA+xt272Kj=UXiVBLyURBse0Y7=FwCX3J1xTmkvhJqSqErGR2LA@mail.gmail.com>
Message-ID: <SA9PR09MB5357A5C1D65540FA6F0603C9F3360@SA9PR09MB5357.namprd09.prod.outlook.com>

Leandro--
2 more things I would think about:

1:  Are lesion types ordered with L2 being larger or more serious lesions than L1: L0 < L1 < L2, or are L1 and L2 different lesion types without one being stronger than the other?  And, can (do) L1 or L2 individuals sometimes heal or revert to L0 over time?  

What are the possible transitions of MarkR for any individual between recaptures?  My thinking is that causality could go in either direction: high SMI individual might be less likely to develop lesions, or lesions could lead to reductions in SMI.  Therefore, I would run a second analysis asking if SMI at recapture I predicts transitions toward lesions or more sever lesions at recapture i+1.  That might have to be restricted to a subset where the 2 recaptures are only 1 or a few months apart, and, since your exchange with Thierry past this draft reply by, you may need to consider seasonality..

2: I suspect that you want to use SMI when MarkR is NR (the first capture) as a "baseline" for that individual.  If so, you may want to make sure NR is the first rather than last level of MarkR.

Tom

-----Original Message-----
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of Leandro Rabello Monteiro
Sent: Thursday, September 24, 2020 1:28 PM
To: r-sig-mixed-models at r-project.org
Subject: [EXTERNAL] [R-sig-ME] mixed model with recapture data



 This email has been received from outside of DOI - Use caution before clicking on links, opening attachments, or responding.



Dear All
 I am trying to evaluate the body condition (SMI) of bats in a mark-recapture study, in response to lesions caused by arm bands.
Because recapture is a matter of chance, the design is highly unbalanced. Most individuals were recaptured twice, but there can be up to 18 recaptures in a period of 4 years.

The data set is formatted in a way that each line is one individual at a point in time. The head() of the data frame looks like this

  ID Sex      SMI MarkR YearMonth
1  1   M 15.10700    L0   2013-04
2  1   M 14.52348    L0   2013-06
3  1   M 15.51033    L0   2013-07
4  1   M 15.51033    L0   2013-09
5  1   M 15.26151    L0   2013-11
6  1   M 15.33953    L0   2014-08

ID is a factor to identify individuals, MarkR (response to banding) is a factor with levels (NR =  no ring, the first capture, L0 = ringed, no lesion, L1 = lesion type 1, L2 = lesion type 2). A single individual can change its level in MarkR, so it is a within-subject fixed factor. Some individuals will develop lesions and some will not.
The question of interest is whether banding itself or lesions caused by banding can be associated with lower SMI, so the only comparisons of interest are the levels L0-2 against the "control" NR.

 Lesions, particularly L2 are rare, occurring in ~3% of observations (out of 2400), again with a high unbalance among levels. There is some seasonality in body condition, but I am not particularly interested in this aspect right now, but I am not sure about the best way to include the temporal factor YearMonth it in the model.

I have tried the following, using individuals and YearMonth as random effects.
lm.smi<-lmer(SMI~Sex*MarkR+(1|ID)+(1|YearMonth),data=smi)

I would appreciate some guidance as to whether I might be missing something relevant, particularly due to the highly unbalanced design.
I have searched a lot but have not managed to find similar examples in the literature or the web. Thanks a lot for your time.


##################################################
Leandro R. Monteiro
Laboratorio de Ciencias Ambientais
Universidade Estadual do Norte Fluminense
E-mail: lrmont at uenf.br
CV Lattes: http://lattes.cnpq.br/4987216474124557
WS: https://sites.google.com/uenf.br/ecol-evolucao-de-mamiferos/
English WS: https://sites.google.com/uenf.br/mammalecologyandevolution/
##################################################

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From @|m@h@rme| @end|ng |rom gm@||@com  Fri Sep 25 19:34:54 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Fri, 25 Sep 2020 12:34:54 -0500
Subject: [R-sig-ME] Meaning of Corr of random-effects with a cross-level
 interaction
In-Reply-To: <CAJuCY5wvENrapzA70De+=2-b-OyP=CDDd3aDgG2-pW4_dPb_YA@mail.gmail.com>
References: <CACgv6yXwshN5OViou9xtaDtiFdWU1NmELKsToan8kxFUw1xgFg@mail.gmail.com>
 <CAJuCY5wvENrapzA70De+=2-b-OyP=CDDd3aDgG2-pW4_dPb_YA@mail.gmail.com>
Message-ID: <CACgv6yX-ZT=OangrPKKuzHgkSubOcvkPENjHGcn184kTmqT7EQ@mail.gmail.com>

Thank you Thierry! Would you please clarify one of your sentences: "both
math and ses have bounds. Ses even seems to have some data above its upper
bound."

Specifically, would please clarify what you mean by "ses has some data
above its upper bound"?(you mean the couple of outlying ses values in red
as shown in your plot?)

Of course, real world data always have some lower and upper bound based on
the instrument (e.g., a math test) used to collect the data. But my
question is what are the relative required lower and upper bounds on
NUMERIC OUTCOME & NUMERIC PREDICTORS so we don't face convergence issues
of the type I have shown in my question?

Thank you,
Simon

On Fri, Sep 25, 2020 at 3:03 AM Thierry Onkelinx <thierry.onkelinx at inbo.be>
wrote:

> Dear Simon,
>
> A perfect correlation between random effect parameters indicates a
> problem. Note the failed convergence warning.
> Standardising ses makes things even worse: it yields a singular fit error.
>
> Removing the random slope of ses or the sector interaction solves the
> problem. i.e. the model runs and yields sensible output.
>
> Looking at the data, it seems like both math and ses have bounds. Ses
> even seems to have some data above its upper bound.
> The model assumes no bounds in the response variable. Maybe this is the
> cause of the problem.
>
> ggplot(hsb, aes(x = ses, y = math, colour = factor(sector))) +
>   geom_point()
>
> Best regards,
>
> Thierry
>
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op do 24 sep. 2020 om 18:39 schreef Simon Harmel <sim.harmel at gmail.com>:
>
>> Dear All,
>>
>> I had a quick question. I have a cross-level interaction in my model below
>> (ses*sector). My cluster-level predictor "sector" is a binary variable
>> (0=Public, 1=Private). My level-1 predictor is numeric.
>>
>> QUESTION:  The `Corr = 1` is indicating the correlation between
>> intercepts and slopes across BOTH public & private sectors (like their
>> average) OR something else?
>>
>> hsb <- read.csv('
>> https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')
>>
>> summary(lmer(math ~ ses*sector + (ses|sch.id), data = hsb))
>>
>>
>> Random effects:
>>  Groups   Name        Variance Std.Dev.     Corr
>>  sch.id   (Intercept)  3.82107    1.9548
>>           ses                0.07587     0.2754        1.00
>>  Residual             36.78760 6.0653
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From |rmont @end|ng |rom uen|@br  Fri Sep 25 19:38:21 2020
From: |rmont @end|ng |rom uen|@br (Leandro Rabello Monteiro)
Date: Fri, 25 Sep 2020 14:38:21 -0300
Subject: [R-sig-ME] [EXTERNAL]  mixed model with recapture data
In-Reply-To: <SA9PR09MB5357A5C1D65540FA6F0603C9F3360@SA9PR09MB5357.namprd09.prod.outlook.com>
References: <CA+xt272Kj=UXiVBLyURBse0Y7=FwCX3J1xTmkvhJqSqErGR2LA@mail.gmail.com>
 <SA9PR09MB5357A5C1D65540FA6F0603C9F3360@SA9PR09MB5357.namprd09.prod.outlook.com>
Message-ID: <CA+xt273rmAVw5NQ-JSdo84m3FAYzv9JWN4PJAVQzpJ=1uuAO1Q@mail.gmail.com>

Dear Tom
Thanks a lot for your comments.
1:The lesion types are indeed ordered from L0 (no lesion) to L2 (the
most severe). They can sometimes revert from one state to another, but
mostly between L0 and L1, and it was common in L2 for the arm band to
be removed, after which they would heal (we know because we had double
marking with necklaces in some of the individuals).

Maybe a multistate mark recapture model would allow an estimate of the
transition probabilities between lesion states and the effect of SMI
as a covariate, but the approach you suggested might be easier to
implement. Will look into it, thanks.

2: Yes, NR is working as a baseline and I made sure that it is the first level.

Best regards,
Leandro

##################################################
Leandro R. Monteiro
Laboratorio de Ciencias Ambientais
Universidade Estadual do Norte Fluminense
E-mail: lrmont at uenf.br
CV Lattes: http://lattes.cnpq.br/4987216474124557
WS: https://sites.google.com/uenf.br/ecol-evolucao-de-mamiferos/
English WS: https://sites.google.com/uenf.br/mammalecologyandevolution/
##################################################

Em sex., 25 de set. de 2020 ?s 13:12, Philippi, Tom
<Tom_Philippi at nps.gov> escreveu:
>
> Leandro--
> 2 more things I would think about:
>
> 1:  Are lesion types ordered with L2 being larger or more serious lesions than L1: L0 < L1 < L2, or are L1 and L2 different lesion types without one being stronger than the other?  And, can (do) L1 or L2 individuals sometimes heal or revert to L0 over time?
>
> What are the possible transitions of MarkR for any individual between recaptures?  My thinking is that causality could go in either direction: high SMI individual might be less likely to develop lesions, or lesions could lead to reductions in SMI.  Therefore, I would run a second analysis asking if SMI at recapture I predicts transitions toward lesions or more sever lesions at recapture i+1.  That might have to be restricted to a subset where the 2 recaptures are only 1 or a few months apart, and, since your exchange with Thierry past this draft reply by, you may need to consider seasonality..
>
> 2: I suspect that you want to use SMI when MarkR is NR (the first capture) as a "baseline" for that individual.  If so, you may want to make sure NR is the first rather than last level of MarkR.
>
> Tom
>
> -----Original Message-----
> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of Leandro Rabello Monteiro
> Sent: Thursday, September 24, 2020 1:28 PM
> To: r-sig-mixed-models at r-project.org
> Subject: [EXTERNAL] [R-sig-ME] mixed model with recapture data
>
>
>
>  This email has been received from outside of DOI - Use caution before clicking on links, opening attachments, or responding.
>
>
>
> Dear All
>  I am trying to evaluate the body condition (SMI) of bats in a mark-recapture study, in response to lesions caused by arm bands.
> Because recapture is a matter of chance, the design is highly unbalanced. Most individuals were recaptured twice, but there can be up to 18 recaptures in a period of 4 years.
>
> The data set is formatted in a way that each line is one individual at a point in time. The head() of the data frame looks like this
>
>   ID Sex      SMI MarkR YearMonth
> 1  1   M 15.10700    L0   2013-04
> 2  1   M 14.52348    L0   2013-06
> 3  1   M 15.51033    L0   2013-07
> 4  1   M 15.51033    L0   2013-09
> 5  1   M 15.26151    L0   2013-11
> 6  1   M 15.33953    L0   2014-08
>
> ID is a factor to identify individuals, MarkR (response to banding) is a factor with levels (NR =  no ring, the first capture, L0 = ringed, no lesion, L1 = lesion type 1, L2 = lesion type 2). A single individual can change its level in MarkR, so it is a within-subject fixed factor. Some individuals will develop lesions and some will not.
> The question of interest is whether banding itself or lesions caused by banding can be associated with lower SMI, so the only comparisons of interest are the levels L0-2 against the "control" NR.
>
>  Lesions, particularly L2 are rare, occurring in ~3% of observations (out of 2400), again with a high unbalance among levels. There is some seasonality in body condition, but I am not particularly interested in this aspect right now, but I am not sure about the best way to include the temporal factor YearMonth it in the model.
>
> I have tried the following, using individuals and YearMonth as random effects.
> lm.smi<-lmer(SMI~Sex*MarkR+(1|ID)+(1|YearMonth),data=smi)
>
> I would appreciate some guidance as to whether I might be missing something relevant, particularly due to the highly unbalanced design.
> I have searched a lot but have not managed to find similar examples in the literature or the web. Thanks a lot for your time.
>
>
> ##################################################
> Leandro R. Monteiro
> Laboratorio de Ciencias Ambientais
> Universidade Estadual do Norte Fluminense
> E-mail: lrmont at uenf.br
> CV Lattes: http://lattes.cnpq.br/4987216474124557
> WS: https://sites.google.com/uenf.br/ecol-evolucao-de-mamiferos/
> English WS: https://sites.google.com/uenf.br/mammalecologyandevolution/
> ##################################################
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From |ongrob604 @end|ng |rom gm@||@com  Sat Sep 26 21:35:52 2020
From: |ongrob604 @end|ng |rom gm@||@com (Robert Long)
Date: Sat, 26 Sep 2020 20:35:52 +0100
Subject: [R-sig-ME] Meaning of Corr of random-effects with a cross-level
 interaction
In-Reply-To: <CA+3TTkPcSKozdzgsS20eUs7Pe75H80Pg+O9x3Ah_mwSwwZtwaA@mail.gmail.com>
References: <CACgv6yXwshN5OViou9xtaDtiFdWU1NmELKsToan8kxFUw1xgFg@mail.gmail.com>
 <CAJuCY5wvENrapzA70De+=2-b-OyP=CDDd3aDgG2-pW4_dPb_YA@mail.gmail.com>
 <CA+3TTkPcSKozdzgsS20eUs7Pe75H80Pg+O9x3Ah_mwSwwZtwaA@mail.gmail.com>
Message-ID: <CA+3TTkMbQiN--Lzq7WFrX33nb_jWoDTiiXpsHrwEGc+y7HY2aQ@mail.gmail.com>

Dear all

After some further thought and simulations I found that the objective, to
uncover different correlations between slopes and intercepts in different
groups, can be achieved simply by fitting the cross-level interaction as a
random slope. In lme4 this fits separate random slopes for each group along
with the correlations. Simulations seem to confirm that it works, at least
for the limited simulation scenarios that I tried:

https://stats.stackexchange.com/questions/489059/obtaining-correlation-between-random-effects-separately-for-2-groups/489181#489181

Best wishes
Robert Long


On Fri, Sep 25, 2020 at 9:56 AM Robert Long <longrob604 at gmail.com> wrote:

> Hi Thierry and list
>
> This was actually cross-posted at CrossValidated yesterday:
>
> https://stats.stackexchange.com/questions/488984/corr-of-random-effects-when-a-cross-level-interaction-in-lme4
>
> I have the impression that Simon is experimenting with a toy dataset,
> rather than analysing their own study, which is a great way to learn, in
> my opinion.
>
> As you can see from my answer and the comments to it, the real question
> (actually, two questions) is this:
>
> Suppose we have two groups of schools, with a single explanatory variable
> at the student level. Suppose further that the correlation between the
> random slopes for that variable and the random intercepts in the two groups
> is very different.  The first question is what the overall correlation
> represents ? I thought that it would probably be some kind of average of
> the two.  I did some simulations that indicate that this seems to be the
> case. The followup question (see the last comment to my answer) asks how to
> uncover the correlations in the two groups ? From my simulations so far the
> only way I can see of doing this is by splitting the data by group and
> fitting two models.
>
> Best regards
> Robert Long
>
>
>
> On Fri, Sep 25, 2020 at 9:04 AM Thierry Onkelinx via R-sig-mixed-models <
> r-sig-mixed-models at r-project.org> wrote:
>
>> Dear Simon,
>>
>> A perfect correlation between random effect parameters indicates a
>> problem.
>> Note the failed convergence warning.
>> Standardising ses makes things even worse: it yields a singular fit error.
>>
>> Removing the random slope of ses or the sector interaction solves the
>> problem. i.e. the model runs and yields sensible output.
>>
>> Looking at the data, it seems like both math and ses have bounds. Ses
>> even seems to have some data above its upper bound.
>> The model assumes no bounds in the response variable. Maybe this is the
>> cause of the problem.
>>
>> ggplot(hsb, aes(x = ses, y = math, colour = factor(sector))) +
>>   geom_point()
>>
>> Best regards,
>>
>> Thierry
>>
>>
>> ir. Thierry Onkelinx
>> Statisticus / Statistician
>>
>> Vlaamse Overheid / Government of Flanders
>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
>> FOREST
>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>> thierry.onkelinx at inbo.be
>> Havenlaan 88 bus 73, 1000 Brussel
>> www.inbo.be
>>
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to
>> say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of
>> data.
>> ~ John Tukey
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>>
>> <https://www.inbo.be>
>>
>>
>> Op do 24 sep. 2020 om 18:39 schreef Simon Harmel <sim.harmel at gmail.com>:
>>
>> > Dear All,
>> >
>> > I had a quick question. I have a cross-level interaction in my model
>> below
>> > (ses*sector). My cluster-level predictor "sector" is a binary variable
>> > (0=Public, 1=Private). My level-1 predictor is numeric.
>> >
>> > QUESTION:  The `Corr = 1` is indicating the correlation between
>> > intercepts and slopes across BOTH public & private sectors (like their
>> > average) OR something else?
>> >
>> > hsb <- read.csv('
>> > https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')
>> >
>> > summary(lmer(math ~ ses*sector + (ses|sch.id), data = hsb))
>> >
>> >
>> > Random effects:
>> >  Groups   Name        Variance Std.Dev.     Corr
>> >  sch.id   (Intercept)  3.82107    1.9548
>> >           ses                0.07587     0.2754        1.00
>> >  Residual             36.78760 6.0653
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From m@rc_grt @end|ng |rom y@hoo@|r  Sun Sep 27 10:52:51 2020
From: m@rc_grt @end|ng |rom y@hoo@|r (Marc Girondot)
Date: Sun, 27 Sep 2020 10:52:51 +0200
Subject: [R-sig-ME] Predict for glmer
References: <5bef0880-7917-692a-c89e-101df83770f8.ref@yahoo.fr>
Message-ID: <5bef0880-7917-692a-c89e-101df83770f8@yahoo.fr>

Hi,

I try to understand exactly how mixed model is working. Let do this 
simple model with fake data:


library(lme4)

invlogit <- function(n) {1/(1 + exp(-n))}

d <- data.frame(A=c(10, 20, 10, 20, 30, 15, 17, 19, 20, 30, 20, 30, 21, 
23),
 ??????????????? B=c(2, 3, 7, 9, 10, 8, 5, 7, 9, 10, 2, 3, 7, 8),
 ??????????????? D=c(10, 11, 12, 13, 14, 10, 11, 12, 13, 14, 13, 12, 12, 
13),
 ??????????????? R1=c("A", "A", "A", "A", "A", "A", "A", "B", "B", "B", 
"B", "B", "B", "B"),
 ??????????????? R2=c("C", "C", "C", "D", "D", "D", "D", "E", "E", "E", 
"F", "F", "F", "F"))

m <- glmer(formula = cbind(A, B) ~ D + (1 | R1 / R2),
 ????? data=d,
 ????? family = binomial(link = "logit"))

It seems to work well (except the warning for boundary because probably 
I don't introduce enough data).

Let do a predict:

predict(m, type="response")

 ??????? 1???????? 2???????? 3???????? 4???????? 5 6???????? 7???????? 
8???????? 9??????? 10??????? 11
0.7706127 0.7665437 0.7624248 0.7502806 0.7459699 0.7629174 0.7587547 
0.7572462 0.7530161 0.7487368 0.7696229
 ?????? 12??????? 13??????? 14
0.7736541 0.7736541 0.7696229

It is ok. Now I try to do the predict by hand to be sure that I 
understand how it works:

ef <- fixef(m)
er <- ranef(m)

fixed <- model.matrix(~ D, data = d) %*% ef
random <- rowSums(model.matrix(~ R1, data = d) * er$R2[d$R2, ])
invlogit(fixed[, 1] + random)

 ??????? 1???????? 2???????? 3???????? 4???????? 5???????? 6 7???????? 
8???????? 9??????? 10??????? 11

0.7706127 0.7665437 0.7624248 0.7502806 0.7459699 0.7629174 0.7587547 
0.7523144 0.7480270 0.7436906 0.7809063
 ?????? 12??????? 13??????? 14
0.7847953 0.7847953 0.7809063

The first 7 estimates are ok but not the last 7. So it is related to R1 
factor... but I don't understand why it is badly estimated.


If someone can help me...

Thanks a lot

Marc


From th|erry@onke||nx @end|ng |rom |nbo@be  Mon Sep 28 09:14:01 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Mon, 28 Sep 2020 09:14:01 +0200
Subject: [R-sig-ME] Meaning of Corr of random-effects with a cross-level
 interaction
In-Reply-To: <CACgv6yX-ZT=OangrPKKuzHgkSubOcvkPENjHGcn184kTmqT7EQ@mail.gmail.com>
References: <CACgv6yXwshN5OViou9xtaDtiFdWU1NmELKsToan8kxFUw1xgFg@mail.gmail.com>
 <CAJuCY5wvENrapzA70De+=2-b-OyP=CDDd3aDgG2-pW4_dPb_YA@mail.gmail.com>
 <CACgv6yX-ZT=OangrPKKuzHgkSubOcvkPENjHGcn184kTmqT7EQ@mail.gmail.com>
Message-ID: <CAJuCY5xcafMMmrzCrXZPWkSWzCLNU0atck++j7HpGTaSKNYMVQ@mail.gmail.com>

Dear Simon,

The plot shows a set of dots on a vertical line indicating an upper bound
on ses. The few points outside the bound need investigating in a real
dataset. They might be wrong measurements.

If you have a response variable with a boundary, you need to use a
distribution that copes with that. Using a Gaussian distribution with
please of data close to a boundary, might lead to predictions and
confidence intervals outside of the boundary. I've seen people give a talk
on mortality with confidence intervals like (80%; 120%)...

I've no idea WHY you are getting the false convergence, just WHEN it starts
to kick in.

If you are cross posting you question, please do mention that.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////


<https://www.inbo.be>


Op vr 25 sep. 2020 om 19:35 schreef Simon Harmel <sim.harmel at gmail.com>:

> Thank you Thierry! Would you please clarify one of your sentences: "both
> math and ses have bounds. Ses even seems to have some data above its
> upper bound."
>
> Specifically, would please clarify what you mean by "ses has some data
> above its upper bound"?(you mean the couple of outlying ses values in red
> as shown in your plot?)
>
> Of course, real world data always have some lower and upper bound based on
> the instrument (e.g., a math test) used to collect the data. But my
> question is what are the relative required lower and upper bounds on
> NUMERIC OUTCOME & NUMERIC PREDICTORS so we don't face convergence issues
> of the type I have shown in my question?
>
> Thank you,
> Simon
>
> On Fri, Sep 25, 2020 at 3:03 AM Thierry Onkelinx <thierry.onkelinx at inbo.be>
> wrote:
>
>> Dear Simon,
>>
>> A perfect correlation between random effect parameters indicates a
>> problem. Note the failed convergence warning.
>> Standardising ses makes things even worse: it yields a singular fit error.
>>
>> Removing the random slope of ses or the sector interaction solves the
>> problem. i.e. the model runs and yields sensible output.
>>
>> Looking at the data, it seems like both math and ses have bounds. Ses
>> even seems to have some data above its upper bound.
>> The model assumes no bounds in the response variable. Maybe this is the
>> cause of the problem.
>>
>> ggplot(hsb, aes(x = ses, y = math, colour = factor(sector))) +
>>   geom_point()
>>
>> Best regards,
>>
>> Thierry
>>
>>
>> ir. Thierry Onkelinx
>> Statisticus / Statistician
>>
>> Vlaamse Overheid / Government of Flanders
>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>> AND FOREST
>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>> thierry.onkelinx at inbo.be
>> Havenlaan 88 bus 73, 1000 Brussel
>> www.inbo.be
>>
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of data.
>> ~ John Tukey
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>>
>> <https://www.inbo.be>
>>
>>
>> Op do 24 sep. 2020 om 18:39 schreef Simon Harmel <sim.harmel at gmail.com>:
>>
>>> Dear All,
>>>
>>> I had a quick question. I have a cross-level interaction in my model
>>> below
>>> (ses*sector). My cluster-level predictor "sector" is a binary variable
>>> (0=Public, 1=Private). My level-1 predictor is numeric.
>>>
>>> QUESTION:  The `Corr = 1` is indicating the correlation between
>>> intercepts and slopes across BOTH public & private sectors (like their
>>> average) OR something else?
>>>
>>> hsb <- read.csv('
>>> https://raw.githubusercontent.com/rnorouzian/e/master/hsb.csv')
>>>
>>> summary(lmer(math ~ ses*sector + (ses|sch.id), data = hsb))
>>>
>>>
>>> Random effects:
>>>  Groups   Name        Variance Std.Dev.     Corr
>>>  sch.id   (Intercept)  3.82107    1.9548
>>>           ses                0.07587     0.2754        1.00
>>>  Residual             36.78760 6.0653
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Mon Sep 28 13:25:04 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Mon, 28 Sep 2020 13:25:04 +0200
Subject: [R-sig-ME] Predict for glmer
In-Reply-To: <5bef0880-7917-692a-c89e-101df83770f8@yahoo.fr>
References: <5bef0880-7917-692a-c89e-101df83770f8.ref@yahoo.fr>
 <5bef0880-7917-692a-c89e-101df83770f8@yahoo.fr>
Message-ID: <CAJuCY5wk4ZOkxk0nc37bM+eAapwhu3rN48esgmORBgu0uGyzdg@mail.gmail.com>

Dear Marc,

You probably want random <- er$R1[d$R1, ] + er$R2[d$R2, ]

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op zo 27 sep. 2020 om 10:53 schreef Marc Girondot via R-sig-mixed-models <
r-sig-mixed-models at r-project.org>:

> Hi,
>
> I try to understand exactly how mixed model is working. Let do this
> simple model with fake data:
>
>
> library(lme4)
>
> invlogit <- function(n) {1/(1 + exp(-n))}
>
> d <- data.frame(A=c(10, 20, 10, 20, 30, 15, 17, 19, 20, 30, 20, 30, 21,
> 23),
>                  B=c(2, 3, 7, 9, 10, 8, 5, 7, 9, 10, 2, 3, 7, 8),
>                  D=c(10, 11, 12, 13, 14, 10, 11, 12, 13, 14, 13, 12, 12,
> 13),
>                  R1=c("A", "A", "A", "A", "A", "A", "A", "B", "B", "B",
> "B", "B", "B", "B"),
>                  R2=c("C", "C", "C", "D", "D", "D", "D", "E", "E", "E",
> "F", "F", "F", "F"))
>
> m <- glmer(formula = cbind(A, B) ~ D + (1 | R1 / R2),
>        data=d,
>        family = binomial(link = "logit"))
>
> It seems to work well (except the warning for boundary because probably
> I don't introduce enough data).
>
> Let do a predict:
>
> predict(m, type="response")
>
>          1         2         3         4         5 6         7
> 8         9        10        11
> 0.7706127 0.7665437 0.7624248 0.7502806 0.7459699 0.7629174 0.7587547
> 0.7572462 0.7530161 0.7487368 0.7696229
>         12        13        14
> 0.7736541 0.7736541 0.7696229
>
> It is ok. Now I try to do the predict by hand to be sure that I
> understand how it works:
>
> ef <- fixef(m)
> er <- ranef(m)
>
> fixed <- model.matrix(~ D, data = d) %*% ef
> random <- rowSums(model.matrix(~ R1, data = d) * er$R2[d$R2, ])
> invlogit(fixed[, 1] + random)
>
>          1         2         3         4         5         6 7
> 8         9        10        11
>
> 0.7706127 0.7665437 0.7624248 0.7502806 0.7459699 0.7629174 0.7587547
> 0.7523144 0.7480270 0.7436906 0.7809063
>         12        13        14
> 0.7847953 0.7847953 0.7809063
>
> The first 7 estimates are ok but not the last 7. So it is related to R1
> factor... but I don't understand why it is badly estimated.
>
>
> If someone can help me...
>
> Thanks a lot
>
> Marc
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From m@rc_grt @end|ng |rom y@hoo@|r  Mon Sep 28 22:25:18 2020
From: m@rc_grt @end|ng |rom y@hoo@|r (Marc Girondot)
Date: Mon, 28 Sep 2020 22:25:18 +0200
Subject: [R-sig-ME] Loglik of model with maxfun = 0 is not the one expected
References: <11def03b-8c56-d5aa-b86a-baf1510dc81a.ref@yahoo.fr>
Message-ID: <11def03b-8c56-d5aa-b86a-baf1510dc81a@yahoo.fr>

Thanks a lot to Thierry Onkelinx <thierry.onkelinx at inbo.be> to answer to 
my previous demand. It works now !

Another point that I don't understand.

I fit a model. I get -log likelihood
I extract the fitted parameters.
I inject these fitted parameters to estimate the likelihood of the model 
and I obtain a different -log likelihood.

So I suppose that I don't extract all the fitted parameters... but which 
one ?

Thanks if you can help me to understand what's happened here.

Marc

___________________

library(lme4)

# Ei is simply data that will be used to fit the model

Ei <- structure(list(Ma = c(15, 14, 28, 5, 26, 7, 25, 4, 9, 25,
 ???????????????????????? 8, 11, 2, 3, 1, 1, 0, 0, 10, 5, 0, 0, 9, 1, 2, 
0, 10, 0, 0, 0,
 ???????????????????????? 8, 1, 0, 0, 12, 3, 29, 10, 26, 9, 18, 5, 14, 
6, 1, 2, 0, 0),
 ?????????????? Fa = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 4, 5, 18, 2,
 ?????????????????????????? 11, 2, 10, 0, 3, 7, 3, 0, 9, 8, 8, 0, 10, 9, 
9, 0, 8, 5,
 ?????????????????????????? 4, 0, 0, 0, 0, 5, 1, 2, 0, 8, 0, 8, 3, 10, 3),
 ?????????????? IT = c(26.4,
 ????????????????????????????????????????? 26.9, 27.9, 27.9, 28.4, 28.4, 
29, 29, 29, 29.5, 29.5, 29.5,
 ????????????????????????????????????????? 30, 30, 30.3, 30.3, 30.8, 
30.8, 28, 29.5, 31, 32.5, 28, 29.5,
 ????????????????????????????????????????? 31, 32.5, 28, 29.5, 31, 32.5, 
28, 29.5, 31, 32.5, 28.15,
 ????????????????????????????????????????? 28.15, 28.65, 28.65, 29.15, 
29.15, 29.55, 29.55, 29.75, 29.75,
 ????????????????????????????????????????? 30.05, 30.05, 30.65, 30.65),
 ?????????????? RMU = structure(c(2L, 2L, 2L,
 ???????????????????????????????? 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L,
 ???????????????????????????????? 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L,
 ???????????????????????????????? 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L),
 ?????????????????????????????? .Label = c("ASW", "AW",
 ????????????????????????????????????????? "PSW"), class = "factor"),
 ?????????????? C = structure(c(4L,
 ??????????????????????????????????? 4L, 4L, 3L, 4L, 3L, 4L, 7L, 8L, 4L, 
7L, 8L, 7L, 8L, 7L, 8L,
 ??????????????????????????????????? 7L, 8L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 
6L, 10L, 10L, 10L, 10L,
 ??????????????????????????????????? 9L, 9L, 9L, 9L, 1L, 2L, 1L, 2L, 1L, 
2L, 1L, 2L, 1L, 2L, 1L,
 ??????????????????????????????????? 2L, 1L, 2L),
 ????????????????????????????????? .Label = c("A", "B", "C", "D",
 ???????????????????????????????????????????? "E", "F", "G", "H", "I", 
"J"), class = "factor")),
 ????????? class = "data.frame", row.names = 302:349)

# m1 if the fitted model

m1 <- glmer(formula = cbind(Ma, Fa) ~ 0 + IT + (1 | RMU / C) + (0 + IT | 
RMU),
 ??????????? data=Ei,
 ??????????? control = glmerControl(optimizer="bobyqa", optCtrl = 
list(maxfun = 1000000)),
 ??????????? family = binomial(link = "logit"))

# I check that all is ok.
logLik(m1)
ranef(m1)
fixef(m1)

theta <- getME(m1, "theta")
beta <- getME(m1, "beta")

# use the fitted values of m1 that I introduce in m2. Note that the 
model is not fitted because maxfun = 0. Log likelihood is just calculated
m2 <- glmer(formula = cbind(Ma, Fa) ~ 0 + IT + (1 | RMU / C) + (0 + IT | 
RMU),
 ??????????? data=Ei,
 ??????????? control = glmerControl(optimizer="bobyqa", optCtrl = 
list(maxfun = 0)),
 ??????????? start = list(theta = theta,
 ???????????????????????? fixef = beta),
 ??????????? family = binomial(link = "logit"))

# Both likelihood should be identical... they are not
logLik(m1)
logLik(m2)


From bbo|ker @end|ng |rom gm@||@com  Mon Sep 28 22:54:29 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 28 Sep 2020 16:54:29 -0400
Subject: [R-sig-ME] 
 Loglik of model with maxfun = 0 is not the one expected
In-Reply-To: <11def03b-8c56-d5aa-b86a-baf1510dc81a@yahoo.fr>
References: <11def03b-8c56-d5aa-b86a-baf1510dc81a.ref@yahoo.fr>
 <11def03b-8c56-d5aa-b86a-baf1510dc81a@yahoo.fr>
Message-ID: <8b2efd45-ce75-083a-93fd-54b509239e4d@gmail.com>

   This is a perfectly reasonable, and an unexpectedly difficult, 
problem.  The glmer-fitting machinery has a lot of internal state that 
is hard to set exactly right.

   In this particular case, adding 
nAGQ0initStep=FALSE to glmerControl() works [it disables the first-stage 
optimization step; presumably at some point during this step, or the 
pre- and post-processing surrounding it, the GLMM likelihood evaluation 
is called and messes up the internal state)

Here is another way to reproduce the log-likelihood:

m3 <- update(m1, devFunOnly=TRUE)
-0.5*m3(c(theta,beta))

The development version of lme4 will also allow

-0.5*getME(m1,"devfun")(c(theta,beta))

(the internal code is more efficient and self-contained than the 
update(., "devFunOnly") version)

On 9/28/20 4:25 PM, Marc Girondot via R-sig-mixed-models wrote:
> Thanks a lot to Thierry Onkelinx <thierry.onkelinx at inbo.be> to answer to 
> my previous demand. It works now !
> 
> Another point that I don't understand.
> 
> I fit a model. I get -log likelihood
> I extract the fitted parameters.
> I inject these fitted parameters to estimate the likelihood of the model 
> and I obtain a different -log likelihood.
> 
> So I suppose that I don't extract all the fitted parameters... but which 
> one ?
> 
> Thanks if you can help me to understand what's happened here.
> 
> Marc
> 
> ___________________
> 
> library(lme4)
> 
> # Ei is simply data that will be used to fit the model
> 
> Ei <- structure(list(Ma = c(15, 14, 28, 5, 26, 7, 25, 4, 9, 25,
>  ???????????????????????? 8, 11, 2, 3, 1, 1, 0, 0, 10, 5, 0, 0, 9, 1, 2, 
> 0, 10, 0, 0, 0,
>  ???????????????????????? 8, 1, 0, 0, 12, 3, 29, 10, 26, 9, 18, 5, 14, 
> 6, 1, 2, 0, 0),
>  ?????????????? Fa = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 4, 5, 18, 2,
>  ?????????????????????????? 11, 2, 10, 0, 3, 7, 3, 0, 9, 8, 8, 0, 10, 9, 
> 9, 0, 8, 5,
>  ?????????????????????????? 4, 0, 0, 0, 0, 5, 1, 2, 0, 8, 0, 8, 3, 10, 3),
>  ?????????????? IT = c(26.4,
>  ????????????????????????????????????????? 26.9, 27.9, 27.9, 28.4, 28.4, 
> 29, 29, 29, 29.5, 29.5, 29.5,
>  ????????????????????????????????????????? 30, 30, 30.3, 30.3, 30.8, 
> 30.8, 28, 29.5, 31, 32.5, 28, 29.5,
>  ????????????????????????????????????????? 31, 32.5, 28, 29.5, 31, 32.5, 
> 28, 29.5, 31, 32.5, 28.15,
>  ????????????????????????????????????????? 28.15, 28.65, 28.65, 29.15, 
> 29.15, 29.55, 29.55, 29.75, 29.75,
>  ????????????????????????????????????????? 30.05, 30.05, 30.65, 30.65),
>  ?????????????? RMU = structure(c(2L, 2L, 2L,
>  ???????????????????????????????? 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
> 2L, 2L, 2L, 2L, 2L, 2L,
>  ???????????????????????????????? 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
> 3L, 3L, 3L, 3L, 3L, 3L,
>  ???????????????????????????????? 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
> 1L, 1L, 1L, 1L, 1L, 1L),
>  ?????????????????????????????? .Label = c("ASW", "AW",
>  ????????????????????????????????????????? "PSW"), class = "factor"),
>  ?????????????? C = structure(c(4L,
>  ??????????????????????????????????? 4L, 4L, 3L, 4L, 3L, 4L, 7L, 8L, 4L, 
> 7L, 8L, 7L, 8L, 7L, 8L,
>  ??????????????????????????????????? 7L, 8L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 
> 6L, 10L, 10L, 10L, 10L,
>  ??????????????????????????????????? 9L, 9L, 9L, 9L, 1L, 2L, 1L, 2L, 1L, 
> 2L, 1L, 2L, 1L, 2L, 1L,
>  ??????????????????????????????????? 2L, 1L, 2L),
>  ????????????????????????????????? .Label = c("A", "B", "C", "D",
>  ???????????????????????????????????????????? "E", "F", "G", "H", "I", 
> "J"), class = "factor")),
>  ????????? class = "data.frame", row.names = 302:349)
> 
> # m1 if the fitted model
> 
> m1 <- glmer(formula = cbind(Ma, Fa) ~ 0 + IT + (1 | RMU / C) + (0 + IT | 
> RMU),
>  ??????????? data=Ei,
>  ??????????? control = glmerControl(optimizer="bobyqa", optCtrl = 
> list(maxfun = 1000000)),
>  ??????????? family = binomial(link = "logit"))
> 
> # I check that all is ok.
> logLik(m1)
> ranef(m1)
> fixef(m1)
> 
> theta <- getME(m1, "theta")
> beta <- getME(m1, "beta")
> 
> # use the fitted values of m1 that I introduce in m2. Note that the 
> model is not fitted because maxfun = 0. Log likelihood is just calculated
> m2 <- glmer(formula = cbind(Ma, Fa) ~ 0 + IT + (1 | RMU / C) + (0 + IT | 
> RMU),
>  ??????????? data=Ei,
>  ??????????? control = glmerControl(optimizer="bobyqa", optCtrl = 
> list(maxfun = 0)),
>  ??????????? start = list(theta = theta,
>  ???????????????????????? fixef = beta),
>  ??????????? family = binomial(link = "logit"))
> 
> # Both likelihood should be identical... they are not
> logLik(m1)
> logLik(m2)
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From bz117 @end|ng |rom georgetown@edu  Tue Sep 29 21:21:06 2020
From: bz117 @end|ng |rom georgetown@edu (Bingsong Zhang)
Date: Tue, 29 Sep 2020 15:21:06 -0400
Subject: [R-sig-ME] Questions about lme4 package
Message-ID: <62E16B4D-AF80-48EE-9A0D-2E0A9284BA7F@georgetown.edu>

Dear there,

Thank you for reading my email. My name is Bingsong Zhang, a third year PhD student of Biostatistics at Georgetown University. 

I am doing my thesis on generalized mixed model, which may rely on lme4 package. I know from the documentation that current version lme4 is not allow for specifying flexible correlation structure, so I am wondering if it possible to specify some popular structures like those available in nlmn? And how possible it is if I want to improve based on the current lme4?

Best,
Bingsong

From d@|uedecke @end|ng |rom uke@de  Tue Sep 29 21:47:04 2020
From: d@|uedecke @end|ng |rom uke@de (=?iso-8859-1?Q?Daniel_L=FCdecke?=)
Date: Tue, 29 Sep 2020 21:47:04 +0200
Subject: [R-sig-ME] Questions about lme4 package
In-Reply-To: <62E16B4D-AF80-48EE-9A0D-2E0A9284BA7F@georgetown.edu>
References: <62E16B4D-AF80-48EE-9A0D-2E0A9284BA7F@georgetown.edu>
Message-ID: <000001d69699$4ead6f00$ec084d00$@uke.de>

Hi Bingsong,
you may want to look at the glmmTMB package
(https://cran.r-project.org/package=glmmTMB), in a way a kind of successor
of the lme4 package, which provides different correlation structures, and
allows for many distributional families (Gaussian, binomial, poisson, ...),
including zero-inflated and hurdle models.

Does this help?

Best
Daniel

-----Urspr?ngliche Nachricht-----
Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im
Auftrag von Bingsong Zhang
Gesendet: Dienstag, 29. September 2020 21:21
An: r-sig-mixed-models at r-project.org
Betreff: [R-sig-ME] Questions about lme4 package

Dear there,

Thank you for reading my email. My name is Bingsong Zhang, a third year PhD
student of Biostatistics at Georgetown University. 

I am doing my thesis on generalized mixed model, which may rely on lme4
package. I know from the documentation that current version lme4 is not
allow for specifying flexible correlation structure, so I am wondering if it
possible to specify some popular structures like those available in nlmn?
And how possible it is if I want to improve based on the current lme4?

Best,
Bingsong
_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Joachim Pr?l?, Prof. Dr. Blanche Schwappach-Pignataro, Marya Verdel
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING


From jd|mopou|o@21 @end|ng |rom gm@||@com  Wed Sep 30 10:57:16 2020
From: jd|mopou|o@21 @end|ng |rom gm@||@com (Yannis Dim.)
Date: Wed, 30 Sep 2020 11:57:16 +0300
Subject: [R-sig-ME] Interpretation of Posterior distribution produced by
 MCMCglmm
Message-ID: <CAEyToCU3zURJ0Wd2T2wU27utjf0kLX1OSDh2-ZjUB1cRB3Qy3Q@mail.gmail.com>

Dear list,

I have a comparative phylogenetic model with a binary response variable, 4
binary explanatory variables as fixed effects and the phylogeny as a random
effect. I would greatly appreciate it if someone could help me interpret
the output of the model, particularly the density plots of (Intercept),
location_selection and material_transport, which I have attached. As you
can see these three variables have a triangular posterior distribution, and
although I have found a few papers of triangular distribution, they talk
about the prior distribution. I haven't found anything about a triangular
posterior distribution. Following is also the output of the model, in case
that helps:

Iterations = 120000001:1319999501
 Thinning interval  = 500
 Sample size  = 2400000

 DIC: 71.74413

 G-structure:  ~phylo

      post.mean  l-95% CI u-95% CI eff.samp
phylo     3.841 1.215e-11    9.991  1567107

 R-structure:  ~units

      post.mean l-95% CI u-95% CI eff.samp
units         1        1        1        0

 Location effects: care01 ~ location_selection +
substrate_modification + material_transport + laying_location_is_food

                        post.mean l-95% CI u-95% CI eff.samp    pMCMC
(Intercept)              -250.852 -568.741   -5.698     1648  < 4e-07 ***
location_selection        246.618    1.747  564.414     1648 0.000358 ***
substrate_modification      6.168    3.514    9.248  1651692  < 4e-07 ***
material_transport        475.747  165.100  706.887     1079    1e-05 ***
laying_location_is_food     0.187   -2.019    2.408  2393680 0.858883
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1


Many thanks in advance,
Yannis Dimopoulos
PhD Student - The University of Hull

From @|m@h@rme| @end|ng |rom gm@||@com  Wed Sep 30 21:57:23 2020
From: @|m@h@rme| @end|ng |rom gm@||@com (Simon Harmel)
Date: Wed, 30 Sep 2020 14:57:23 -0500
Subject: [R-sig-ME] Drop the correlation bet. random effects to find those
 with small variance
Message-ID: <CACgv6yV-miMOoDWshwnvJxxY5e-+YXaB5NBRkT43eQzVb0zYXw@mail.gmail.com>

Dear All,

Bates, et al. (2015) <https://arxiv.org/pdf/1506.04967.pdf> mention that to
identify a mixed-model with a singular variance-covariance matrix we can:

Fit a zero correlation parameter which will identify random effects with
zero, or very small, variance

That is, going from `m0` to `m1` (see below). BUT, how come dropping all
correlations between slopes and intercepts can lead to identifying random
effects with zero, or very small, variance?

library(lme4)

dat <- read.csv('
https://raw.githubusercontent.com/WRobertLong/Stackexchange/master/data/singular.csv
')

m0 <- lmer(y ~ A * B * C + (A * B * C  | group), data = dat)
m1 <- lmer(y ~ A * B * C + (A * B * C || group), data = dat)

	[[alternative HTML version deleted]]


